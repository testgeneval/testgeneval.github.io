{"raw_generations": {"sample_200": ["def test_invalid_attachment_filename(self):\n    \"\"\"\n    Regression test for #14964\n    \"\"\"\n    headers = {\"Date\": \"Fri, 09 Nov 2001 01:08:47 -0000\", \"Message-ID\": \"foo\"}\n    subject, from_email, to = 'hello', 'from@example.com', 'to@example.com'\n    content = 'This is the message.'\n    msg = EmailMessage(subject, content, from_email, [to], headers=headers)\n    # Unicode in file name\n    with self.assertRaisesMessage(ValueError, 'Invalid address; address parts cannot contain newlines.'):\n        msg.attach(\"une pi\u00e8ce jointe\\n.pdf\", b\"%PDF-1.4.%...\", mimetype=\"application/pdf\")\n", "    def test_get_connection_kwargs(self):\n        \"\"\"Test that get_connection() kwargs are passed to the backend.\"\"\"\n        backend = mail.get_connection(\n            'django.core.mail.backends.smtp.EmailBackend',\n            timeout=30,\n            use_ssl=True,\n            ssl_keyfile='/path/to/keyfile',\n            ssl_certfile='/path/to/certfile',\n        )\n        self.assertEqual(backend.timeout, 30)\n        self.assertTrue(backend.use_ssl)\n        self.assertEqual(backend.ssl_keyfile, '/path/to/keyfile')\n        self.assertEqual(backend.ssl_certfile, '/path/to/certfile')\n", "def test_email_multi_alternatives(self):\n    \"\"\"\n    Test EmailMultiAlternatives send_mail with html_message parameter\n    \"\"\"\n    subject = 'Hello there'\n    message = 'This is a test message'\n    from_email = 'from@example.com'\n    recipient_list = ['nobody@example.com']\n    html_message = '<p>This is a <strong>html</strong> message.</p>'\n    msg = EmailMultiAlternatives(subject, message, from_email, recipient_list)\n    msg.attach_alternative(html_message, \"text/html\")\n    self.assertEqual(msg.mixed_subtype, 'alternative')\n    self.assertEqual(len(msg.alternatives), 1)\n    self.assertEqual(msg.alternatives[0][0], html_message)\n    self.assertEqual(msg.alternatives[0][1], \"text/html\")\n\n    sent_num = msg.send()\n    self.assertEqual(sent_num, 1)\n    message = self.get_the_message()\n    self.assertEqual(message.get('subject'), 'Hello there')\n    self.assertEqual(message.get_all('to'), ['nobody@example.com'])\n    self.assertTrue(message.is_multipart())\n    self.assertEqual(len(message.get_payload()), 2)\n    self.assertEqual(message.get_payload(0).get_payload(), 'This is a test message')\n    self.assertEqual(message.get_payload(0).get_content_type(), 'text/plain')\n    self.assertEqual(message.get_payload(1).get_payload(), '<p>This is a <strong>html</strong> message.</p>')\n    self.assertEqual(message.get_payload(1).get_content_type(), 'text/html')\n", "def test_email_attachment_filename_encoding(self):\n    \"\"\"\n    The attachment filename is properly encoded when it contains non-ASCII characters.\n    \"\"\"\n    subject = \"Subject\"\n    body = \"Content\"\n    from_email = \"from@example.com\"\n    to = [\"to@example.com\"]\n    filename = \"une pi\u00e8ce jointe.pdf\"\n\n    email = EmailMessage(subject, body, from_email, to)\n    email.attach(filename, b\"%PDF-1.4.%...\", mimetype=\"application/pdf\")\n    message = email.message()\n\n    # The filename is encoded in the attachment header.\n    self.assertEqual(\n        message.get_payload(1)['Content-Disposition'],\n        \"attachment; filename*=%3D%3Futf-8%3F%3DB%3Fune%2520pi%C3%A8ce%2520joint%3F%3D.pdf%3F%3D\"\n    )\n", "def test_attachment_filename_encoding(self):\n    \"\"\"\n    Regression test for #29268 - Ensure that attachment filenames are properly\n    encoded.\n    \"\"\"\n    msg = EmailMessage('Subject', 'Content', 'from@example.com', ['to@example.com'])\n    msg.attach('attachment.txt', 'Content', 'text/plain')\n    msg.attach('attachment_with_non_ascii_name.txt', 'Content', 'text/plain')\n    message = msg.message()\n    payload = message.get_payload()\n    self.assertEqual(len(payload), 2)\n    self.assertEqual(payload[0].get_filename(), 'attachment.txt')\n    self.assertEqual(payload[1].get_filename(), 'attachment_with_non_ascii_name.txt')\n    self.assertEqual(payload[0].get_payload(), 'Content')\n    self.assertEqual(payload[1].get_payload(), 'Content')\n\n    # Test with a file name containing non-ASCII characters\n    msg = EmailMessage('Subject', 'Content', 'from@example.com', ['to@example.com'])\n    msg.attach('attachment_\u00e9.txt', 'Content', 'text/plain')\n    message = msg.message()\n    payload = message.get_payload()\n    self.assertEqual(len(payload), 1)\n    self.assertEqual(payload[0].get_filename(), 'attachment_=C3=A9.txt')\n    self.assertEqual(payload[0].get_payload(), 'Content')\n"], "sample_1007": ["def test_factorial2_simplify():\n    n = Symbol('n', integer=True)\n    assert simplify(factorial2(n + 1) - n * factorial2(n)) == 0\n    assert simplify(factorial2(2*n + 1) - (2*n + 1) * factorial2(2*n)) == 0\n    assert simplify(factorial2(n + 1) - (n + 1) * factorial2(n - 1)) == 0\n    assert simplify(factorial2(2*n + 2) - (2*n + 2) * factorial2(2*n)) == 0\n", "def test_factorial2_edge_cases():\n    n = Symbol('n', integer=True)\n    assert factorial2(-5) == 1/3\n    assert factorial2(-3) == 2\n    assert factorial2(-1) == 1\n    assert factorial2(1) == 1\n    assert factorial2(0) == 1\n    assert factorial2(3) == 3\n    assert factorial2(4) == 8\n\n    # Test some edge cases\n    assert factorial2(-2**1024) == 1\n    assert factorial2(-2**1023) == 2**1023\n    assert factorial2(2**1024 - 1) == 2**(2**1023 - 1)\n", "def test_subfactorial_edge_cases():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True)\n    assert subfactorial(1).func == subfactorial\n    assert subfactorial(n, evaluate=False).func == subfactorial\n    assert subfactorial(n).is_integer is None\n    assert subfactorial(k).is_nonnegative is None\n    assert subfactorial(n).is_even is None\n    assert subfactorial(n).is_odd is None\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(-1) == 0\n    assert subfactorial(-2) == 0\n    assert subfactorial(-oo) == 0\n    assert subfactorial(oo) == oo\n    assert subfactorial(nan) == nan\n", "def test_factorial_mod():\n    x, y = symbols('x,y')\n    n, k = symbols('n k', integer=True)\n    m = Symbol('m', integer=True, nonnegative=True)\n\n    assert factorial(5, 7) == 0\n    assert factorial(5, 11) == 1\n    assert factorial(7, 11) == 0\n    assert factorial(10, 7) == 6\n\n    assert factorial(oo, 5) == 0\n    assert factorial(oo, 11) == 0\n    assert factorial(-oo, 5) == 0\n    assert factorial(-oo, 11) == 0\n\n    assert factorial(x, 5) == factorial(x, 5)\n    assert factorial(n, 5) == factorial(n, 5)\n\n    assert factorial(7, 5).func == Mod\n    assert factorial(10, 5).func == Mod\n    assert factorial(oo, 5).func == Mod\n    assert factorial(-oo, 5).func == Mod\n\n    assert factorial(7, 11).func == Mod\n    assert factorial(10, 11).func == Mod\n    assert factorial(oo, 11).func == Mod\n    assert factorial(-oo, 11).func == Mod\n", "def test_factorial_swing():\n    n = Symbol('n', integer=True)\n    assert factorial._swing(0) == 1\n    assert factorial._swing(1) == 1\n    assert factorial._swing(2) == 2\n    assert factorial._swing(3) == 3\n    assert factorial._swing(4) == 3\n    assert factorial._swing(5) == 15\n    assert factorial._swing(6) == 15\n    assert factorial._swing(7) == 315\n    assert factorial._swing(8) == 105\n    assert factorial._swing(9) == 945\n    assert factorial._swing(10) == 945\n    assert factorial._swing(11) == 10395\n    assert factorial._swing(12) == 10395\n    assert factorial._swing(13) == 135135\n    assert factorial._swing(14) == 135135\n    assert factorial._swing(15) == 2028117\n    assert factorial._swing(16) == 2028117\n    assert factorial._swing(17) == 32332319\n    assert factorial._swing(18) == 32332319\n    assert factorial._swing(19) == 620448401\n    assert factorial._swing(20) == 2432902008176640000\n    assert factorial._swing(21) == 51090942171709440000\n    assert factorial._swing(22) == 1067828639758138112000\n    assert factorial._swing(23) == 25862324151116818063360\n    assert factorial._swing(24) == 67044257280014755193600\n    assert factorial._swing(25) == 121645100408832000000000\n    assert factorial._swing(26) == 121645100408832000000000\n    assert factorial._swing(27) == 2755789564473600000000000\n    assert factorial._swing(28) == 2755789564473600000000000\n    assert factorial._swing(29) == 82228386541779228177255628800000000\n    assert factorial._swing(30) == 82228386541779228177255628800000000\n    assert factorial._swing(31) == 229383715603072000000000000000000"], "sample_55": ["    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.a1 = Answer.objects.create(question='What?', answer='Yes.')\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.section = Section.objects.create(name='Test section')\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.s1 = Section.objects.create(name='Test section')\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n"], "sample_744": ["def test_power_transformer_copy():\n    pt = PowerTransformer(method='box-cox', copy=False)\n    X = np.abs(X_2d)\n\n    X_trans_class = pt.fit_transform(X)\n    assert_true(X_trans_class is not X)\n\n    X_trans_class = pt.fit(X).transform(X)\n    assert_true(X_trans_class is not X)\n\n    pt = PowerTransformer(method='box-cox', copy=True)\n    X_trans_class = pt.fit_transform(X)\n    assert_false(X_trans_class is X)\n\n    X_trans_class = pt.fit(X).transform(X)\n    assert_false(X_trans_class is X)\n\n    X_trans_func = power_transform(X, copy=False)\n    assert_true(X_trans_func is not X)\n\n    X_trans_func = power_transform(X, copy=True)\n    assert_false(X_trans_func is X)\n", "def test_power_transformer_inverse_of_transform():\n    # Test that applying transform and then inverse_transform\n    # results in the original values\n    X = np.abs(X_2d)\n    for standardize in [True, False]:\n        pt = PowerTransformer(method='box-cox', standardize=standardize)\n        X_trans = pt.fit_transform(X)\n        X_inv = pt.inverse_transform(X_trans)\n        assert_array_almost_equal(X_inv, X)\n", "def test_quantile_transform_output_distribution():\n    # Check the output_distribution parameter\n    X = np.array([[0, 2, 2.6],\n                  [25, 4, 4.1],\n                  [50, 6, 2.3],\n                  [75, 8, 9.5],\n                  [100, 10, 0.1]])\n\n    transformer_uniform = QuantileTransformer(n_quantiles=5,\n                                              output_distribution='uniform')\n    transformer_uniform.fit(X)\n\n    # using the a uniform output, each entry of X should be map between 0 and 1\n    # and equally spaced\n    X_trans_uniform = transformer_uniform.fit_transform(X)\n    X_expected_uniform = np.tile(np.linspace(0, 1, num=5), (3, 1)).T\n    assert_almost_equal(np.sort(X_trans_uniform, axis=0), X_expected_uniform)\n\n    transformer_normal = QuantileTransformer(n_quantiles=5,\n                                             output_distribution='normal')\n    transformer_normal.fit(X)\n\n    # using the a normal output, each entry of X should be map to a normal\n    # distribution\n    X_trans_normal = transformer_normal.fit_transform(X)\n    X_test = np.array([\n        [-1, 1, 0],\n        [101, 11, 10],\n    ])\n    X_expected_normal = np.array([\n        [-1.26491106, -1.15034948, -1.34164079],\n        [1.34164079,  1.15034948, 1.26491106],\n    ])\n    assert_array_almost_equal(transformer_normal.transform(X_test),\n                              X_expected_normal, decimal=3)\n", "def test_power_transformer_output_distribution():\n    pt = PowerTransformer(method='box-cox', output_distribution='normal')\n\n    # Exceptions should be raised for non-supported distributions\n    with_distribution_message = \"'output_distribution' has to be either 'normal' or 'uniform'. Got 'rnd' instead.\"\n    assert_raise_message(ValueError, with_distribution_message,\n                         pt.fit, np.abs(X_2d))\n\n    pt = PowerTransformer(method='box-cox', output_distribution='uniform')\n\n    X_trans = pt.fit_transform(np.abs(X_2d))\n\n    # Check that the output distribution is uniform\n    assert_allclose(stats.kstest(X_trans.flatten(), 'uniform')[1], 0.5, atol=0.15)\n", "def test_quantile_transform_sparse_subsample():\n    # Test that the subsampling algorithm behaves correctly\n    # when we have a large sparse matrix and a small number of\n    # quantiles\n\n    rng = np.random.RandomState(0)\n    n_samples = 100000\n    n_features = 5\n    X = sparse.rand(n_samples, n_features, density=0.01, format='csc', random_state=rng)\n\n    transformer = QuantileTransformer(n_quantiles=10,\n                                      subsample=n_samples // 10,\n                                      random_state=rng)\n    X_trans = transformer.fit_transform(X)\n\n    # Check that X_trans is a sparse matrix with the same shape\n    assert sparse.isspmatrix_csc(X_trans)\n    assert X_trans.shape == (n_samples, n_features)\n\n    # Check that X_trans is a float64 matrix\n    assert X_trans.dtype == np.float64\n\n    # Check that the sparse matrix was not converted to dense\n    assert X_trans.nnz == X.nnz\n"], "sample_908": ["def test_unparse_arguments(source, expected):\n    tree = ast.parse(source)\n    function_def = tree.body[0]\n    assert ast.unparse_arguments(function_def.args) == expected\n", "def test_unparse_arguments(source, expected):\n    module = ast.parse(source)\n    if source.startswith(\"def\"):\n        assert ast.unparse(module.body[0].args) == expected\n    else:\n        assert ast.unparse(module.body[0].targets[0]) == expected\n", "def test_unparse_arguments(source, expected):\n    tree = ast.parse(source)\n    function_def = tree.body[0]\n    assert ast.unparse(function_def.args) == expected\n", "def test_unparse_arguments():\n    source = \"def test(a, b=1, *, c, d=2, **kwargs): pass\"\n    expected = \"a, b=1, *, c, d=2, **kwargs\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args) == expected\n", "def test_unparse_complex_expressions(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value) == expected\n\n"], "sample_1060": ["def test_AbstractPythonCodePrinter_relational():\n    p = AbstractPythonCodePrinter()\n    x, y = symbols('x y')\n    assert p.doprint(Eq(x, y)) == '(x == y)'\n    assert p.doprint(Le(x, y)) == '(x <= y)'\n    assert p.doprint(Gt(x, y)) == '(x > y)'\n    assert p.doprint(x > y) == '(x > y)'\n    assert p.doprint(x < y) == '(x < y)'\n", "def test_CodegenArrayTensorProduct():\n    n = NumPyPrinter()\n    from sympy.codegen.array_utils import CodegenArrayTensorProduct\n    expr = CodegenArrayTensorProduct([MatrixSymbol('A', 2, 2), MatrixSymbol('B', 2, 2)])\n    assert n.doprint(expr) == \"numpy.einsum(A, [0, 1], B, [0, 1])\"\n", "def test_AbstractPythonCodePrinter_print_FunctionDefinition():\n    prntr = PythonCodePrinter()\n    x = symbols('x')\n    fd = Assignment(x, 2)\n    assert prntr.doprint(fd) == 'def (x):\\n    x = 2'\n", "def test_print_CodegenArrayTensorProduct():\n    from sympy.tensor import IndexedBase, Idx\n    p1 = IndexedBase('p1')\n    i1, i2 = Idx('i1'), Idx('i2')\n    p2 = IndexedBase('p2')\n    j1, j2 = Idx('j1'), Idx('j2')\n    expr = p1[i1, i2] * p2[j1, j2]\n    n = NumPyPrinter()\n    assert n.doprint(expr) == 'numpy.einsum(p1, [0, 1], p2, [0, 1])'\n", "def test_AbstractPythonCodePrinter():\n    p = AbstractPythonCodePrinter()\n    assert p._get_statement(\"x = 2\") == \"x = 2\"\n    assert p._get_comment(\"This is a comment\") == \"  # This is a comment\"\n    assert p._indent_codestring(\"x = 2\\ny = 3\") == \"    x = 2\\n    y = 3\"\n"], "sample_693": ["def test_addCleanup_with_exception_in_setup_and_teardown(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n        class MyTestCase(unittest.TestCase):\n                self.addCleanup(lambda: 1/0)\n                assert False\n                self.addCleanup(lambda: 1/0)\n                pass\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    passed, skipped, failed = reprec.countoutcomes()\n    assert failed == 2\n    assert passed == 0\n", "def test_collecting_tests_from_subclasses(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class BaseTestCase(unittest.TestCase):\n                pass\n\n        class SubClassTestCase(BaseTestCase):\n                pass\n    \"\"\"\n    )\n    reprec = pytester.inline_run()\n    reprec.assertoutcome(passed=2)\n", "def test_unittest_subclassing(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class BaseTestCase(unittest.TestCase):\n                self.base_setup = True\n\n                pass\n\n        class MyTestCase(BaseTestCase):\n                super().setUp()\n                self.my_setup = True\n\n                assert self.base_setup\n                assert self.my_setup\n\n                assert self.base_setup\n                assert not hasattr(self, 'my_setup')\n        \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    reprec.assertoutcome(passed=3)\n", "def test_unittest_cleanups_teardown_interaction(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n        class MyTestCase(unittest.TestCase):\n            values = []\n\n                    self.values.append(1)\n                self.addCleanup(cleanup1)\n\n                    self.values.append(2)\n                    raise Exception(\"Cleanup exception\")\n\n                self.addCleanup(cleanup2)\n\n                self.values.append(3)\n                raise Exception(\"Teardown exception\")\n\n                pass\n            assert MyTestCase.values == [1, 2, 3]\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    passed, skipped, failed = reprec.countoutcomes()\n    assert failed == 1\n    assert passed == 1\n", "def test_unittest_teardown_not_called_when_test_is_interrupted(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n        import pytest\n\n        class MyTestCase(unittest.TestCase):\n                pytest.fail(\"tearDown should not be called\")\n\n                pytest.exit(\"test exit\")\n        \"\"\"\n    )\n    result = pytester.runpytest(testpath)\n    assert result.ret == 1\n    result.stdout.no_fnmatch_line(\"*tearDown should not be called*\")\n"], "sample_488": ["def test_lazy_model_operation_exception(self):\n    \"\"\"\n    Test that lazy_model_operation raises an exception when the models are not\n    registered in the correct order.\n    \"\"\"\n    apps = Apps([\"apps\"])\n\n        return model_a, model_b\n\n    with self.assertRaises(LookupError):\n        apps.lazy_model_operation(test_func, (\"apps\", \"model_b\"), (\"apps\", \"model_a\"))\n\n    class ModelA(models.Model):\n        pass\n\n    class ModelB(models.Model):\n        pass\n\n    apps.register_model(\"apps\", ModelB)\n    apps.register_model(\"apps\", ModelA)\n\n    model_a, model_b = test_func(ModelA, ModelB)\n    self.assertEqual(model_a, ModelA)\n    self.assertEqual(model_b, ModelB)\n", "    def test_populate(self):\n        \"\"\"\n        Tests apps.populate().\n        \"\"\"\n        apps_ready = False\n        models_ready = False\n        with self.settings(INSTALLED_APPS=[\"apps.apps.MyAdmin\", \"apps.apps.MyAuth\"]):\n            apps.populate()\n            apps_ready = apps.apps_ready\n            models_ready = apps.models_ready\n        self.assertIs(apps_ready, True)\n        self.assertIs(models_ready, True)\n\n        with self.assertRaises(RuntimeError):\n            apps.populate()\n\n        with self.assertRaisesMessage(AppRegistryNotReady, \"Apps aren't loaded yet\"):\n            apps.populate(installed_apps=[\"apps.apps.MyAdmin\", \"apps.apps.MyAuth\"])\n", "def test_lazy_model_operation_exception(self):\n    \"\"\"\n    Tests apps.lazy_model_operation() when one of the operations raises an exception.\n    \"\"\"\n    @isolate_apps(\"apps\", kwarg_name=\"apps\")\n        model_classes = []\n        initial_pending = set(apps._pending_operations)\n\n            if len(models) > 1:\n                raise ValueError(\"Test exception\")\n            model_classes[:] = models\n\n        class LazyA(models.Model):\n            pass\n\n        model_keys = [\n            (\"apps\", model_name)\n            for model_name in [\"lazya\", \"lazyb\"]\n        ]\n        apps.lazy_model_operation(test_func, *model_keys)\n\n        class LazyB(models.Model):\n            pass\n\n        with self.assertRaises(ValueError):\n            apps.get_model(\"apps\", \"LazyB\")\n\n        # Ensure that pending operations are cleared after an exception.\n        self.assertEqual(set(apps._pending_operations), initial_pending)\n\n    inner_test()\n", "def test_lazy_model_operation_more_than_two_models(self):\n    \"\"\"\n    Tests apps.lazy_model_operation() with more than two models.\n    \"\"\"\n    model_classes = []\n    initial_pending = set(apps._pending_operations)\n\n        model_classes[:] = models\n\n    class LazyA(models.Model):\n        pass\n\n    class LazyB(models.Model):\n        pass\n\n    class LazyC(models.Model):\n        pass\n\n    class LazyD(models.Model):\n        pass\n\n    # Test models appearing in a different order\n    model_keys = [\n        (\"apps\", model_name)\n        for model_name in [\"lazyd\", \"lazyb\", \"lazyc\", \"lazya\"]\n    ]\n    apps.lazy_model_operation(test_func, *model_keys)\n\n    # Make sure all models are waited on\n    self.assertEqual(\n        set(apps._pending_operations) - initial_pending,\n        {(\"apps\", \"lazya\"), (\"apps\", \"lazyb\"), (\"apps\", \"lazyc\"), (\"apps\", \"lazyd\")},\n    )\n\n    # Define the models and check if the callback was executed properly\n    LazyA.__module__ = \"apps\"\n    LazyB.__module__ = \"apps\"\n    LazyC.__module__ = \"apps\"\n    LazyD.__module__ = \"apps\"\n\n    apps.register_model(\"apps\", LazyA)\n    apps.register_model(\"apps\", LazyB)\n    apps.register_model(\"apps\", LazyC)\n    apps.register_model(\"apps\", LazyD)\n\n    self.assertEqual(model_classes, [LazyA, LazyB, LazyC, LazyD])\n", "    def test_pending_operations_are_executed_in_correct_order(self):\n        \"\"\"\n        Test that pending operations are executed in the correct order.\n\n        This test ensures that the pending operations are executed in the order they were registered.\n        \"\"\"\n        # Create a new app registry\n        apps = Apps([\"apps\"])\n\n        # Create a list to store the execution order\n        execution_order = []\n\n        # Define a function to register a model and execute a pending operation\n                execution_order.append(model_name)\n\n            class Model(models.Model):\n                class Meta:\n                    app_label = \"apps\"\n                    apps = apps\n\n            apps.lazy_model_operation(test_func, (\"apps\", model_name))\n            apps.register_model(\"apps\", Model)\n\n        # Register models and pending operations in a specific order\n        register_model_and_execute(apps, \"model1\", execution_order)\n        register_model_and_execute(apps, \"model2\", execution_order)\n        register_model_and_execute(apps, \"model3\", execution_order)\n\n        # Check that the execution order is correct\n        self.assertEqual(execution_order, [\"model1\", \"model2\", \"model3\"])\n"], "sample_572": ["    def test_cached_support(self, rng):\n        x = rng.normal(0, 3, 100)\n        kde = KDE()\n        kde.define_support(x)\n        _, support1 = kde(x[(x > -1) & (x < 1)])\n        _, support2 = kde(x[(x > -2) & (x < 2)])\n        assert_array_equal(support1, kde.support)\n        assert_array_equal(support2, kde.support)\n", "def test_cached_bin_params(self, x, y):\n\n    h = Histogram()\n    bin_kws1 = h.define_bin_params(x, y, cache=True)\n    bin_kws2 = h.define_bin_params(x, y, cache=False)\n    assert bin_kws1 == bin_kws2\n\n    bin_kws1 = h.bin_kws\n    bin_kws2 = h.define_bin_params(x, y, cache=False)\n    assert bin_kws1 == bin_kws2\n", "    def test_bivariate_support_cache(self, rng):\n\n        x, y = rng.normal(0, 3, (2, 100))\n        kde = KDE(gridsize=100)\n        kde.define_support(x, y)\n        _, support1 = kde(x[(x > -1) & (x < 1)], y[(y > -1) & (y < 1)])\n        assert_array_equal(support1[0], kde.support[0])\n        assert_array_equal(support1[1], kde.support[1])\n", "    def test_bivariate_clip(self, rng):\n\n        x, y = rng.normal(0, 3, (2, 100))\n        clip = (-1, 1), (-5, 5)\n        kde = KDE(clip=clip)\n        _, (xx, yy) = kde(x, y)\n\n        assert xx.min() >= clip[0][0]\n        assert xx.max() <= clip[0][1]\n        assert yy.min() >= clip[1][0]\n        assert yy.max() <= clip[1][1]\n", "def test_define_support_univariate_weights(self, rng):\n\n    x = rng.normal(0, 3, 100)\n    weights = rng.uniform(0, 5, 100)\n    kde = KDE()\n    kde.define_support(x, weights)\n    _, support = kde(x, weights)\n    assert_array_equal(support, kde.support)\n"], "sample_416": ["def test_default_database(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({}),\n        ([\"psql\", \"postgres\"], None),\n    )\n", "def test_default_database(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env(\n            {\n                \"USER\": \"someuser\",\n                \"PASSWORD\": \"somepassword\",\n                \"HOST\": \"somehost\",\n                \"PORT\": \"444\",\n            }\n        ),\n        (\n            [\"psql\", \"-U\", \"someuser\", \"-h\", \"somehost\", \"-p\", \"444\", \"postgres\"],\n            {\"PGPASSWORD\": \"somepassword\"},\n        ),\n    )\n", "def test_default_database(self):\n    \"\"\"Test connecting to the default 'postgres' database.\"\"\"\n    self.assertEqual(\n        self.settings_to_cmd_args_env(\n            {\n                \"USER\": \"someuser\",\n                \"HOST\": \"somehost\",\n                \"PORT\": \"444\",\n            }\n        ),\n        (\n            [\"psql\", \"-U\", \"someuser\", \"-h\", \"somehost\", \"-p\", \"444\", \"postgres\"],\n            None,\n        ),\n    )\n", "    def test_empty_database_name(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env(\n                {\n                    \"USER\": \"someuser\",\n                    \"PASSWORD\": \"somepassword\",\n                    \"HOST\": \"somehost\",\n                    \"PORT\": \"444\",\n                }\n            ),\n            (\n                [\"psql\", \"-U\", \"someuser\", \"-h\", \"somehost\", \"-p\", \"444\", \"postgres\"],\n                {\"PGPASSWORD\": \"somepassword\"},\n            ),\n        )\n", "def test_defaults_to_postgres_db(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({\"USER\": \"someuser\", \"PASSWORD\": \"somepassword\"}),\n        (\n            [\"psql\", \"-U\", \"someuser\", \"postgres\"],\n            {\"PGPASSWORD\": \"somepassword\"},\n        ),\n    )\n"], "sample_1114": ["def test_ComplexRegion_complex_intersect():\n    # Polar form\n    X_axis = ComplexRegion(Interval(0, oo)*FiniteSet(0, S.Pi), polar=True)\n\n    unit_disk = ComplexRegion(Interval(0, 1)*Interval(0, 2*S.Pi), polar=True)\n    upper_half_unit_disk = ComplexRegion(Interval(0, 1)*Interval(0, S.Pi), polar=True)\n    upper_half_disk = ComplexRegion(Interval(0, oo)*Interval(0, S.Pi), polar=True)\n    lower_half_disk = ComplexRegion(Interval(0, oo)*Interval(S.Pi, 2*S.Pi), polar=True)\n    right_half_disk = ComplexRegion(Interval(0, oo)*Interval(-S.Pi/2, S.Pi/2), polar=True)\n    first_quad_disk = ComplexRegion(Interval(0, oo)*Interval(0, S.Pi/2), polar=True)\n\n    assert upper_half_disk.intersect(unit_disk) == upper_half_unit_disk\n    assert right_half_disk.intersect(first_quad_disk) == first_quad_disk\n    assert upper_half_disk.intersect(right_half_disk) == first_quad_disk\n    assert upper_half_disk.intersect(lower_half_disk) == X_axis\n\n    c1 = ComplexRegion(Interval(0, 4)*Interval(0, 2*S.Pi), polar=True)\n    assert c1.intersect(Interval(1, 5)) == Interval(1, 4)\n    assert c1.intersect(Interval(4, 9)) == FiniteSet(4)\n    assert c1.intersect(Interval(5, 12)) is S.EmptySet\n\n    # Rectangular form\n    X_axis = ComplexRegion(Interval(-oo, oo)*FiniteSet(0))\n\n    unit_square = ComplexRegion(Interval(-1, 1)*Interval(-1, 1))\n    upper_half_unit_square = ComplexRegion(Interval(-1, 1)*Interval(0, 1))\n    upper_half_plane = ComplexRegion(Interval(-oo, oo)*Interval(0, oo))\n    lower_half_plane = ComplexRegion(Interval(-oo, oo)*Interval(-oo, 0))\n    right_half_plane = ComplexRegion(Interval(0, oo)*Interval(-oo, oo))\n    first_quad_plane = ComplexRegion(Interval(0, oo)*Interval(0, oo))\n\n    assert upper_half_plane.inter", "def test_ComplexRegion_boundary():\n    # Polar form\n    c1 = ComplexRegion(Interval(0, 1)*Interval(0, 2*S.Pi), polar=True)\n    c2 = ComplexRegion(Interval(0, 1)*Interval(0, S.Pi), polar=True)\n    c3 = ComplexRegion(Interval(0, oo)*Interval(0, S.Pi), polar=True)\n    c4 = ComplexRegion(Interval(0, oo)*Interval(S.Pi, 2*S.Pi), polar=True)\n\n    assert c1.boundary == c1\n    assert c2.boundary == c2\n    assert c3.boundary == ComplexRegion(Interval(0, oo)*FiniteSet(0), polar=True)\n    assert c4.boundary == ComplexRegion(Interval(0, oo)*FiniteSet(S.Pi), polar=True)\n\n    # Rectangular form\n    c5 = ComplexRegion(Interval(2, 5)*Interval(6, 9))\n    c6 = ComplexRegion(Interval(4, 6)*Interval(10, 12))\n    c7 = ComplexRegion(Interval(0, 10)*Interval(-10, 0))\n    c8 = ComplexRegion(Interval(12, 16)*Interval(14, 20))\n\n    assert c5.boundary == c5\n    assert c6.boundary == c6\n    assert c7.boundary == ComplexRegion(Interval(0, 10)*FiniteSet(0))\n    assert c8.boundary == ComplexRegion(Interval(12, 16)*FiniteSet(14))\n", "def test_normalize_theta_set_non_pi_coefficient():\n    raises(NotImplementedError, lambda: normalize_theta_set(Interval(0, 2)))\n    raises(NotImplementedError, lambda: normalize_theta_set(FiniteSet(0, 3, 3)))\n", "def test_Rationals_iterate():\n    r = iter(S.Rationals)\n    assert next(r) == 0\n    assert next(r) == 1\n    assert next(r) == -1\n    assert next(r) == S.Half\n    assert next(r) == 2\n    assert next(r) == Rational(-1, 2)\n    assert next(r) == -2\n    assert next(r) == Rational(1, 3)\n    assert next(r) == 3\n    assert next(r) == Rational(-1, 3)\n    assert next(r) == -3\n    assert next(r) == Rational(2, 3)\n    assert next(r) == 4\n    assert next(r) == Rational(-2, 3)\n    assert next(r) == Rational(-3, 2)\n    assert next(r) == Rational(-4, 3)\n    assert next(r) == Rational(3, 2)\n    assert next(r) == Rational(4, 3)\n    assert next(r) == 5\n", "def test_ComplexRegion_polar_boundary():\n    r1 = Interval(1, 3)\n    theta1 = Interval(0, 2*S.Pi)\n    c1 = ComplexRegion(r1*theta1, polar=True)\n    assert c1.boundary == ComplexRegion(Interval(1, 3)*FiniteSet(0, 2*S.Pi), polar=True)\n\n    r2 = Interval(0, 3)\n    theta2 = Interval(0, S.Pi)\n    c2 = ComplexRegion(r2*theta2, polar=True)\n    assert c2.boundary == ComplexRegion(Interval(0, 3)*FiniteSet(0, S.Pi), polar=True)\n\n    r3 = Interval(0, 1)\n    theta3 = Interval(S.Pi/2, 3*S.Pi/2)\n    c3 = ComplexRegion(r3*theta3, polar=True)\n    assert c3.boundary == ComplexRegion(Interval(0, 1)*FiniteSet(S.Pi/2, 3*S.Pi/2), polar=True)\n"], "sample_5": ["def test_parameter_copy(model):\n    if not HAS_SCIPY and model['class'] in SCIPY_MODELS:\n        pytest.skip()\n\n    m = model['class'](**model['parameters'])\n    param_name = list(model['parameters'].keys())[0]\n\n    param_copy = getattr(m, param_name).copy()\n\n    assert param_copy.name == getattr(m, param_name).name\n    assert param_copy.default == getattr(m, param_name).default\n    assert param_copy.unit == getattr(m, param_name).unit\n    assert param_copy.fixed == getattr(m, param_name).fixed\n    assert param_copy.tied == getattr(m, param_name).tied\n    assert param_copy.bounds == getattr(m, param_name).bounds\n", "def test_parameter_model_copy(model):\n    if not HAS_SCIPY and model['class'] in SCIPY_MODELS:\n        pytest.skip()\n\n    m = model['class'](**model['parameters'])\n    m_copy = m.copy()\n\n    # Check that the model and its copy have the same parameters\n    for param_name in m.param_names:\n        assert getattr(m, param_name) == getattr(m_copy, param_name)\n\n    # Check that the model and its copy are not the same instance\n    assert m is not m_copy\n\n    # Check that the model and its copy have the same parameters after changing a parameter\n    setattr(m, m.param_names[0], 2)\n    assert getattr(m, m.param_names[0]) != getattr(m_copy, m.param_names[0])\n", "def test_Parameter_bounds():\n    param = Parameter('test', 'test', 1.0)\n    assert param.bounds == (None, None)\n    param.bounds = (1.0, 2.0)\n    assert param.bounds == (1.0, 2.0)\n    param.min = 1.5\n    assert param.bounds == (1.5, 2.0)\n    param.max = 2.5\n    assert param.bounds == (1.5, 2.5)\n    param.bounds = (None, None)\n    assert param.bounds == (None, None)\n    param.min = 1.0\n    assert param.bounds == (1.0, None)\n    param.max = 2.0\n    assert param.bounds == (1.0, 2.0)\n    with pytest.raises(TypeError):\n        param.bounds = 'test'\n", "def test_Parameter_quantity_property(mag, default, value):\n    param = Parameter(name='test', default=default, mag=mag)\n    if mag and not isinstance(value, u.Quantity):\n        with pytest.raises(TypeError):\n            param.quantity = value\n    elif mag and value.unit not in (u.mag, u.ABmag):\n        with pytest.raises(u.UnitsError):\n            param.quantity = value\n    else:\n        param.quantity = value\n        assert param.quantity == value\n        if isinstance(value, u.Quantity):\n            assert param.unit == value.unit\n        else:\n            assert param.unit is None\n", "def test_Parameter_unit_magnitude(name, default, unit, mag):\n    p = Parameter(name=name, default=default, unit=unit, mag=mag)\n\n    if mag and unit is not None and not unit.is_equivalent(u.mag):\n        with pytest.raises(ValueError):\n            p.quantity = 3 * u.m\n    elif mag and unit is None:\n        with pytest.raises(ValueError):\n            p.quantity = 3 * u.m\n    else:\n        if unit is not None:\n            assert p.quantity.unit.is_equivalent(unit)\n        else:\n            assert p.quantity is None\n\n    if unit is not None:\n        if mag:\n            assert p.unit.is_equivalent(u.mag)\n        else:\n            assert p.unit.is_equivalent(unit)\n    else:\n        assert p.unit is None\n\n    if mag:\n        assert p.mag\n    else:\n        assert not p.mag\n"], "sample_1029": ["def test_Cycle():\n    sT(Cycle(x, y), \"Cycle(Symbol('x'), Symbol('y'))\")\n", "def test_Predicate():\n    sT(S.is_even, \"Predicate('is_even')\")\n    sT(S.is_odd, \"Predicate('is_odd')\")\n    sT(S.is_integer, \"Predicate('is_integer')\")\n    sT(S.is_rational, \"Predicate('is_rational')\")\n    sT(S.is_complex, \"Predicate('is_complex')\")\n    sT(S.is_real, \"Predicate('is_real')\")\n", "def test_Predicate():\n    p = symbols('p', cls=Function)\n    sT(p(x, y), \"Predicate('p')(Symbol('x'), Symbol('y'))\")\n    sT(p(x, y, z=1), \"Predicate('p')(Symbol('x'), Symbol('y'), z=Integer(1))\")\n", "def test_Predicate():\n    sT(symbols('x', positive=True), \"Symbol('x', positive=True)\")\n    assert srepr(symbols('x', positive=False)) == \"Symbol('x', positive=False)\"\n    assert srepr(symbols('x', positive=None)) == \"Symbol('x', positive=None)\"\n\n    sT(symbols('x', integer=True), \"Symbol('x', integer=True)\")\n    assert srepr(symbols('x', integer=False)) == \"Symbol('x', integer=False)\"\n    assert srepr(symbols('x', integer=None)) == \"Symbol('x', integer=None)\"\n\n    sT(symbols('x', real=True), \"Symbol('x', real=True)\")\n    assert srepr(symbols('x', real=False)) == \"Symbol('x', real=False)\"\n    assert srepr(symbols('x', real=None)) == \"Symbol('x', real=None)\"\n\n    sT(symbols('x', complex=True), \"Symbol('x', complex=True)\")\n    assert srepr(symbols('x', complex=False)) == \"Symbol('x', complex=False)\"\n    assert srepr(symbols('x', complex=None)) == \"Symbol('x', complex=None)\"\n\n    sT(symbols('x', hermitian=True), \"Symbol('x', hermitian=True)\")\n    assert srepr(symbols('x', hermitian=False)) == \"Symbol('x', hermitian=False)\"\n    assert srepr(symbols('x', hermitian=None)) == \"Symbol('x', hermitian=None)\"\n\n    sT(symbols('x', odd=True), \"Symbol('x', odd=True)\")\n    assert srepr(symbols('x', odd=False)) == \"Symbol('x', odd=False)\"\n    assert srepr(symbols('x', odd=None)) == \"Symbol('x', odd=None)\"\n\n    sT(symbols('x', even=True), \"Symbol('x', even=True)\")\n    assert srepr(symbols('x', even=False)) == \"Symbol('x', even=False)\"\n    assert srepr(symbols('x', even=None)) == \"Symbol('x', even=None)\"\n\n    sT(symbols('x', prime=True), \"Symbol('x', prime=True)\")\n    assert srepr(symbols('x', prime=False)) == \"Symbol('x', prime=False)\"\n    assert srepr(symbols('x', prime=None)) == \"Symbol('x', prime=None)\"\n\n    sT(s", "def test_AppliedPredicate():\n    from sympy.logic.boolalg import Implies\n    assert srepr(Implies(x > 2, x < 3)) == \\\n        \"AppliedPredicate(Implies, (GreaterThan(Symbol('x'), Integer(2)), LessThan(Symbol('x'), Integer(3))))\"\n"], "sample_738": ["def test_tfidf_vectorizer_with_stop_words():\n    # Test TfidfVectorizer with stop words\n    vect = TfidfVectorizer(stop_words='english')\n    X = vect.fit_transform(ALL_FOOD_DOCS)\n    assert_equal(X.shape[1], 9)\n    feature_names = vect.get_feature_names()\n    assert_false('the' in feature_names)\n    assert_false('and' in feature_names)\n    assert_false('of' in feature_names)\n    assert_in('pizza', feature_names)\n    assert_in('burger', feature_names)\n    assert_in('beer', feature_names)\n", "def test_hashingvectorizer_non_negative_deprecation():\n    # Non-regression test: ensure non_negative parameter of HashingVectorizer\n    # raises a DeprecationWarning\n\n    message = (\"The 'non_negative' parameter in HashingVectorizer has been \"\n               \"deprecated in 0.19 and will be removed in 0.21.\")\n    exception = DeprecationWarning\n\n        hv = HashingVectorizer(non_negative=True)\n\n    assert_warns_message(exception, message, func)\n", "def test_countvectorizer_vocabulary_frozen_after_fit():\n    # Non regression test: CountVectorizer vocabulary should be frozen after fit\n    cv = CountVectorizer(vocabulary=None)\n    cv.fit(JUNK_FOOD_DOCS)\n    vocab_before = cv.vocabulary_\n    cv.fit(NOTJUNK_FOOD_DOCS)\n    vocab_after = cv.vocabulary_\n    assert_not_equal(vocab_before, vocab_after)\n    assert_greater(len(vocab_before), 0)\n    assert_greater(len(vocab_after), 0)\n\n    cv = CountVectorizer(vocabulary=cv.vocabulary_)\n    cv.fit(JUNK_FOOD_DOCS)\n    vocab_before = cv.vocabulary_\n    cv.fit(NOTJUNK_FOOD_DOCS)\n    vocab_after = cv.vocabulary_\n    assert_equal(vocab_before, vocab_after)\n    assert_greater(len(vocab_before), 0)\n    assert_greater(len(vocab_after), 0)\n", "def test_tfidfvectorizer_idf_calculation():\n    # Test that idf calculation is correct for TfidfVectorizer\n    X = [[1, 1, 1],\n         [1, 1, 0],\n         [1, 0, 0]]\n    tv = TfidfVectorizer(norm='l2', use_idf=True, smooth_idf=True)\n    tv.fit_transform(X)\n    idf = tv._tfidf.idf_\n    expected_idf = np.array([0.69314718, 0.40546511, 0.])\n    assert_array_almost_equal(idf, expected_idf)\n\n    tv = TfidfVectorizer(norm='l2', use_idf=True, smooth_idf=False)\n    tv.fit_transform(X)\n    idf = tv._tfidf.idf_\n    expected_idf = np.array([1., 0.69314718, -np.inf])\n    assert_array_almost_equal(idf, expected_idf)\n", "def test_vectorizer_strip_accents():\n    # Test the `strip_accents` parameter of vectorizers\n    text = \"J'ai mang\u00e9 du kangourou  ce midi, c'\u00e9tait pas tr\u00e8s bon.\"\n    docs = [text, text.lower()]\n    expected_features = ['ai', 'mange', 'du', 'kangourou', 'ce', 'midi',\n                         'etait', 'pas', 'tres', 'bon']\n\n    # Test CountVectorizer\n    for analyzer in ['word', 'char']:\n        for strip_accents in ['ascii', 'unicode']:\n            vect = CountVectorizer(strip_accents=strip_accents, analyzer=analyzer)\n            X = vect.fit_transform(docs)\n            assert_equal(vect.get_feature_names(), expected_features)\n\n    # Test HashingVectorizer\n    for analyzer in ['word', 'char']:\n        for strip_accents in ['ascii', 'unicode']:\n            vect = HashingVectorizer(strip_accents=strip_accents, analyzer=analyzer)\n            X = vect.transform(docs)\n            assert_greater(X.shape[1], 0)\n\n    # Test TfidfVectorizer\n    for analyzer in ['word', 'char']:\n        for strip_accents in ['ascii', 'unicode']:\n            vect = TfidfVectorizer(strip_accents=strip_accents, analyzer=analyzer)\n            X = vect.fit_transform(docs)\n            assert_equal(vect.get_feature_names(), expected_features)\n"], "sample_272": ["def test_custom_contenttypes_model(self):\n    \"\"\"\n    Tests detection of initial migrations already having been applied.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    self.assertTableNotExists(\"migrations_author\")\n    self.assertTableNotExists(\"migrations_tribble\")\n    self.assertTableNotExists(\"django_content_type\")\n    # Migrate forwards\n    executor.migrate([\n        (\"contenttypes\", \"0001_initial\"),\n        (\"auth\", \"0001_initial\"),\n        (\"migrations\", \"0001_initial\")\n    ])\n    self.assertTableExists(\"migrations_author\")\n    self.assertTableExists(\"migrations_tribble\")\n    self.assertTableExists(\"django_content_type\")\n    # Make sure the soft-application detection works (#23093)\n    # Change table_names to not return auth_user during this as\n    # it wouldn't be there in a normal run, and ensure migrations.Author\n    # exists in the global app registry temporarily.\n    old_table_names = connection.introspection.table_names\n    connection.introspection.table_names = lambda c: [x for x in old_table_names(c) if x != \"auth_user\"]\n    migrations_apps = executor.loader.project_state((\"migrations\", \"0001_initial\")).apps\n    global_apps.get_app_config(\"migrations\").models[\"author\"] = migrations_apps.get_model(\"migrations\", \"author\")\n    try:\n        migration = executor.loader.get_migration(\"auth\", \"0001_initial\")\n        self.assertIs(executor.detect_soft_applied(None, migration)[0], True)\n    finally:\n        connection.introspection.table_names = old_table_names\n        del global_apps.get_app_config(\"migrations\").models[\"author\"]\n    # And migrate back to clean up the database\n    executor.loader.build_graph()\n    executor.migrate([\n        (\"migrations\", None),\n        (\"auth\", None),\n        (\"contenttypes\", None)\n    ])\n    self.assertTableNotExists(\"migrations_author\")\n    self.assertTableNotExists(\"migrations_tribble\")\n    self.assertTableNotExists(\"django_content_type\")\n", "    def test_migration_plan_clean_start(self):\n        \"\"\"\n        Test the migration_plan function with clean_start=True\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        plan = executor.migration_plan(self.available_apps, clean_start=True)\n        self.assertEqual(len(plan), 0)\n        # Create some initial migrations\n        executor.migrate([(\"migrations\", \"0001_initial\")])\n        executor.loader.build_graph()\n        # Rebuild the plan\n        plan = executor.migration_plan(self.available_apps, clean_start=True)\n        self.assertEqual(\n            plan,\n            [\n                (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], True),\n            ],\n        )\n        executor.migrate([(\"migrations\", \"0002_second\")])\n        executor.loader.build_graph()\n        plan = executor.migration_plan(self.available_apps, clean_start=True)\n        self.assertEqual(\n            plan,\n            [\n                (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], True),\n                (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], True),\n            ],\n        )\n", "def test_detect_soft_applied_add_field_with_renamed_model(self):\n    \"\"\"\n    Test detecting a ManyToManyField table from an AddField operation\n    when the model has been renamed.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    # Create the tables for 0001 but make it look like the migration hasn't\n    # been applied.\n    executor.migrate([(\"migrations\", \"0001_initial\")])\n    executor.migrate([(\"migrations\", None)], fake=True)\n    # Table detection sees 0001 is applied but not 0002.\n    migration = executor.loader.get_migration(\"migrations\", \"0001_initial\")\n    self.assertIs(executor.detect_soft_applied(None, migration)[0], True)\n    # Rename the model and check if detection still works.\n    old_table_names = connection.introspection.table_names\n    connection.introspection.table_names = lambda c: [\"migrations_newauthor\"] + [x for x in old_table_names(c) if x != \"migrations_author\"]\n    self.assertIs(executor.detect_soft_applied(None, migration)[0], True)\n    # Revert the table name change.\n    connection.introspection.table_names = old_table_names\n\n    # Create the tables for both migrations but make it look like neither\n    # has been applied.\n    executor.loader.build_graph()\n    executor.migrate([(\"migrations\", \"0001_initial\")], fake=True)\n    executor.migrate([(\"migrations\", \"0002_second\")])\n    executor.loader.build_graph()\n    executor.migrate([(\"migrations\", None)], fake=True)\n    # Table detection sees 0002 is applied.\n    migration = executor.loader.get_migration(\"migrations\", \"0002_second\")\n    self.assertIs(executor.detect_soft_applied(None, migration)[0], True)\n\n    # Leave the tables for 0001 except the many-to-many table. That missing\n    # table should cause detect_soft_applied() to return False.\n    with connection.schema_editor() as editor:\n        editor.execute(editor.sql_delete_table % {\"table\": \"migrations_book\"})\n    migration = executor.loader.get_migration(\"migrations\", \"0001_initial\")\n    self.assertIs(executor.detect_soft_applied(None, migration)[0], False)\n\n    # Cleanup by removing the remaining tables.\n    with connection.schema_editor() as editor:\n        editor.execute(editor.sql_delete_table % {\"table\":", "def test_reorder_operations(self):\n    \"\"\"\n    Regression test for #24692 - reorder operations.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    # Let's look at the plan first and make sure it's up to scratch\n    plan = executor.migration_plan([(\"migrations\", \"0002_second\")])\n    self.assertEqual(\n        plan,\n        [\n            (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n            (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False),\n        ],\n    )\n    # Were the tables there before?\n    self.assertTableNotExists(\"migrations_author\")\n    self.assertTableNotExists(\"migrations_book\")\n    # Alright, let's try running it\n    executor.migrate([(\"migrations\", \"0002_second\")])\n    # Are the tables there now?\n    self.assertTableExists(\"migrations_author\")\n    self.assertTableExists(\"migrations_book\")\n    # Rebuild the graph to reflect the new DB state\n    executor.loader.build_graph()\n    # Alright, let's undo what we did\n    plan = executor.migration_plan([(\"migrations\", None)])\n    self.assertEqual(\n        plan,\n        [\n            (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], True),\n            (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], True),\n        ],\n    )\n    executor.migrate([(\"migrations\", None)])\n    # Are the tables gone?\n    self.assertTableNotExists(\"migrations_author\")\n    self.assertTableNotExists(\"migrations_book\")\n    # Rebuild the graph to reflect the new DB state\n    executor.loader.build_graph()\n    # Let's reorder some operations in the migration plan\n    migration = executor.loader.get_migration(\"migrations\", \"0002_second\")\n    migration.operations = [migration.operations[1], migration.operations[0]]\n    # Alright, let's try running it again\n    executor.migrate([(\"migrations\", \"0002_second\")])\n    # Are the tables there now?\n    self.assertTableExists(\"migrations_author\")\n    self.assertTableExists(\"migrations_book\")\n    # Rebuild the graph to reflect the new DB state\n    executor.loader.build_graph()\n    # Alright, let's undo what we", "    def test_detect_soft_applied_create_model_with_swappable_models(self):\n        \"\"\"\n        executor.detect_soft_applied() detects tables for models that are\n        swapped at the app level.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        # Create the tables for the swapped models but make it look like the\n        # migration hasn't been applied.\n        executor.migrate([(\"migrations\", \"0001_initial\")])\n        executor.migrate([(\"migrations\", None)], fake=True)\n        # Swap Author model in the migrations app.\n        with self.modify_settings(INSTALLED_APPS={\n            'append': 'migrations.test_migrations_swap_author',\n        }):\n            # Table detection sees 0001 is applied.\n            migration = executor.loader.get_migration(\"migrations\", \"0001_initial\")\n            self.assertIs(executor.detect_soft_applied(None, migration)[0], True)\n            # Make sure the model is correctly swapped back.\n            migration = executor.loader.get_migration(\"migrations\", \"0002_second\")\n            self.assertIs(executor.detect_soft_applied(None, migration)[0], False)\n"], "sample_234": ["def test_union_with_empty_qs_and_distinct_fields(self):\n    qs1 = Number.objects.all()\n    qs2 = Number.objects.none()\n    self.assertEqual(len(qs1.union(qs2).distinct('num')), 10)\n", "def test_union_with_deferred_models(self):\n    class DeferredNumber(Number):\n        class Meta:\n            proxy = True\n\n    DeferredNumber.objects.bulk_create([DeferredNumber(num=i, other_num=10 - i) for i in range(10)])\n\n    qs1 = Number.objects.filter(num__lte=1).defer('other_num')\n    qs2 = DeferredNumber.objects.filter(num__gte=8).defer('other_num')\n    self.assertNumbersEqual(qs1.union(qs2), [0, 1, 8, 9], ordered=False)\n", "def test_union_with_nested_annotation(self):\n    qs1 = Number.objects.annotate(count=Value(0, IntegerField())).values('num', 'count')\n    qs2 = Number.objects.annotate(count=Value(1, IntegerField())).values('num', 'count')\n    self.assertCountEqual(qs1.union(qs2), [(0, 0), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1)])\n", "def test_union_with_aggregate(self):\n    qs1 = Number.objects.all()\n    qs2 = Number.objects.all()\n    self.assertEqual(qs1.union(qs2).aggregate(avg_num=Avg('num'))['avg_num'], 4.5)\n    self.assertEqual(qs1.union(qs2).aggregate(count=Count('num'))['count'], 20)\n    self.assertEqual(qs1.union(qs2).aggregate(sum_num=Sum('num'))['sum_num'], 90)\n", "def test_union_with_related_model(self):\n    ReservedName.objects.create(name='a', order=2)\n    qs1 = ReservedName.objects.all().select_related('number')\n    qs2 = ReservedName.objects.all().select_related('number')\n    self.assertQuerysetEqual(\n        qs1.union(qs2),\n        [{'name': 'a', 'order': 2, 'number__num': 2, 'number__other_num': 8}],\n        transform=lambda x: {\n            'name': x.name,\n            'order': x.order,\n            'number__num': getattr(x.number, 'num', None),\n            'number__other_num': getattr(x.number, 'other_num', None),\n        },\n    )\n"], "sample_312": ["def test_add_squash(self):\n    node = Node([('a', 1), ('b', 2)], connector='AND')\n    new_node = Node([('c', 3), ('d', 4)], connector='AND')\n    node.add(new_node, 'AND')\n    expected_node = Node([('a', 1), ('b', 2), ('c', 3), ('d', 4)], connector='AND')\n    self.assertEqual(node, expected_node)\n", "def test_add_squash(self):\n    # Test that squashing works correctly when adding a new node with the same connector\n    node = Node([('a', 1), ('b', 2)])\n    new_node = Node([('c', 3), ('d', 4)])\n    node.add(new_node, Node.default)\n    self.assertEqual(str(node), \"(DEFAULT: ('a', 1), ('b', 2), ('c', 3), ('d', 4))\")\n", "def test_add_squash(self):\n    # Squash a node that has a single child\n    node = Node([('a', 1)])\n    child = Node([('b', 2)])\n    result = node.add(child, 'DEFAULT')\n    self.assertEqual(len(result.children), 0)\n    self.assertEqual(result, node)\n    self.assertEqual(str(node), \"(DEFAULT: ('a', 1), ('b', 2))\")\n\n    # Don't squash a node that has multiple children\n    node = Node([('a', 1)])\n    child = Node([('b', 2), ('c', 3)])\n    result = node.add(child, 'DEFAULT')\n    self.assertEqual(len(result.children), 2)\n    self.assertEqual(result, node)\n    self.assertEqual(str(node), \"(DEFAULT: ('a', 1), (DEFAULT: ('b', 2), ('c', 3)))\")\n\n    # Don't squash a node that is negated\n    node = Node([('a', 1)])\n    child = Node([('b', 2)], negated=True)\n    result = node.add(child, 'DEFAULT')\n    self.assertEqual(len(result.children), 2)\n    self.assertEqual(result, node)\n    self.assertEqual(str(node), \"(DEFAULT: ('a', 1), (NOT (DEFAULT: ('b', 2)))\")\n\n    # Don't squash a node that has a different connector\n    node = Node([('a', 1)])\n    child = Node([('b', 2)], connector='OR')\n    result = node.add(child, 'DEFAULT')\n    self.assertEqual(len(result.children), 2)\n    self.assertEqual(result, node)\n    self.assertEqual(str(node), \"(DEFAULT: ('a', 1), (OR: ('b', 2)))\")\n", "def test_add_squashing(self):\n    # Create a node with two children\n    node1 = Node([('a', 1), ('b', 2)])\n    # Create another node with two children\n    node2 = Node([('c', 3), ('d', 4)])\n    # Add node2 to node1 with the same connector, this should squash node2's children\n    node1.add(node2, Node.default)\n    # node1 should now have four children\n    self.assertEqual(len(node1), 4)\n    # node1's children should be the original children plus node2's children\n    self.assertEqual(node1.children, [('a', 1), ('b', 2), ('c', 3), ('d', 4)])\n", "def test_add_squash(self):\n    # Test that we can squash the other node's children directly into this node.\n    node = Node([('a', 1), ('b', 2)])\n    other = Node([('c', 3), ('d', 4)])\n    node.add(other, Node.default)\n    self.assertEqual(str(node), \"(DEFAULT: ('a', 1), ('b', 2), ('c', 3), ('d', 4))\")\n"], "sample_584": ["    def test_check_for_impossible_ordering_with_bystander_dim(self):\n        # Check non-monotonic bystander dimension coord doesn't raise\n        # ValueError on combine (https://github.com/pydata/xarray/issues/3150)\n        ycoord = ['a', 'c', 'b']\n\n        data = np.random.rand(7, 3)\n\n        ds1 = Dataset(data_vars=dict(data=(['x', 'y'], data[:3, :])),\n                      coords=dict(x=[1, 2, 3], y=ycoord))\n\n        ds2 = Dataset(data_vars=dict(data=(['x', 'y'], data[3:, :])),\n                      coords=dict(x=[4, 5, 6, 7], y=ycoord))\n\n        expected = Dataset(data_vars=dict(data=(['x', 'y'], data)),\n                           coords=dict(x=[1, 2, 3, 4, 5, 6, 7], y=ycoord))\n\n        actual = combine_by_coords((ds1, ds2))\n        assert_identical(expected, actual)\n", "    def test_coordinates_must_be_monotonic(self):\n        # Check non-monotonic bystander dimension coord doesn't raise\n        # ValueError on combine (https://github.com/pydata/xarray/issues/3150)\n        ycoord = ['a', 'c', 'b']\n\n        data = np.random.rand(7, 3)\n\n        ds1 = Dataset(data_vars=dict(data=(['x', 'y'], data[:3, :])),\n                      coords=dict(x=[1, 2, 3], y=ycoord))\n\n        ds2 = Dataset(data_vars=dict(data=(['x', 'y'], data[3:, :])),\n                      coords=dict(x=[4, 5, 6, 7], y=ycoord))\n\n        expected = Dataset(data_vars=dict(data=(['x', 'y'], data)),\n                           coords=dict(x=[1, 2, 3, 4, 5, 6, 7], y=ycoord))\n\n        with pytest.raises(ValueError, match=\"Resulting object does not have \"\n                                             \"monotonic global indexes\"):\n            combine_by_coords((ds1, ds2))\n\n        # Check for non-monotonic dimension coord doesn't raise\n        # ValueError on combine\n        xcoord = [5, 3, 1]\n\n        data = np.random.rand(7, 3)\n\n        ds1 = Dataset(data_vars=dict(data=(['x', 'y'], data[:3, :])),\n                      coords=dict(x=xcoord, y=ycoord))\n\n        ds2 = Dataset(data_vars=dict(data=(['x', 'y'], data[3:, :])),\n                      coords=dict(x=[2, 4, 6, 8], y=ycoord))\n\n        with pytest.raises(ValueError, match=\"Resulting object does not have \"\n                                             \"monotonic global indexes\"):\n            combine_by_coords((ds1, ds2))\n", "    def test_check_ordering(self):\n        # Check that if the user supplies datasets in the wrong order, then\n        # the combination is still done correctly.\n        data = create_test_data()\n        data1 = data.isel(dim2=slice(4, 9))\n        data2 = data.isel(dim2=slice(4))\n        actual = combine_by_coords([data2, data1])\n        expected = data\n        assert expected.broadcast_equals(actual)\n", "    def test_no_dimension_coords_but_variables(self):\n        ds0 = Dataset({'foo': ('x', [0, 1])})\n        ds1 = Dataset({'foo': ('x', [1, 2])})\n\n        expected = Dataset({'foo': ('x', [0, 1, 2])})\n        actual = combine_by_coords([ds0, ds1])\n        assert_identical(expected, actual)\n", "    def test_combine_by_coords_with_non_monotonic_bystander_dimension(self):\n        # Check non-monotonic bystander dimension coord doesn't raise\n        # ValueError on combine (https://github.com/pydata/xarray/issues/3150)\n        ycoord = ['a', 'c', 'b']\n\n        data = np.random.rand(7, 3)\n\n        ds1 = Dataset(data_vars=dict(data=(['x', 'y'], data[:3, :])),\n                      coords=dict(x=[1, 2, 3], y=ycoord))\n\n        ds2 = Dataset(data_vars=dict(data=(['x', 'y'], data[3:, :])),\n                      coords=dict(x=[4, 5, 6, 7], y=ycoord))\n\n        expected = Dataset(data_vars=dict(data=(['x', 'y'], data)),\n                           coords=dict(x=[1, 2, 3, 4, 5, 6, 7], y=ycoord))\n\n        actual = combine_by_coords((ds1, ds2))\n        assert_identical(expected, actual)\n"], "sample_1138": ["def test_TR22():\n    assert TR22(tan(x)**2) == sec(x)**2 - 1\n    assert TR22(1 + tan(x)**2) == sec(x)**2\n    assert TR22(cot(x)**2) == csc(x)**2 - 1\n    assert TR22(1 + cot(x)**2) == csc(x)**2\n    assert TR22(tan(x)**3) == tan(x)**3\n    assert TR22(tan(x)**4) == (sec(x)**2 - 1)**2\n", "def test_fu_unhandled_functions():\n    assert fu(sec(x)) == sec(x)\n    assert fu(csc(x)) == csc(x)\n    assert fu(sinh(x)) == sinh(x)\n    assert fu(cosh(x)) == cosh(x)\n    assert fu(tanh(x)) == tanh(x)\n    assert fu(coth(x)) == coth(x)\n    assert fu(sech(x)) == sech(x)\n    assert fu(csch(x)) == csch(x)\n", "def test_fu_handle_pow():\n    expr = sin(x)**2 - sin(x)**2\n    assert fu(expr) == 0\n\n    expr = sin(x)**2 + cos(x)**2\n    assert fu(expr) == 1\n\n    expr = sin(x)**2 + cos(x)**3\n    assert fu(expr) == sin(x)**2 + cos(x)**3\n\n    expr = 2*sin(x)**2 + 2*cos(x)**2\n    assert fu(expr) == 2\n\n    expr = 2*sin(x)**3 + 2*cos(x)**3\n    assert fu(expr) == 2*sin(x)**3 + 2*cos(x)**3\n", "def test_hyper_as_trig_trig():\n    from sympy.simplify.fu import _osborne as o, _osbornei as i\n    x = Symbol('x')\n    # testing issue 17413\n    t = i(o(x, x), x)\n    assert t == x\n    t = i(o(cos(x), x), x)\n    assert t == cos(x)\n    t = i(o(cos(x/2), x), x)\n    assert t == cos(x/2)\n    # testing issue 17571\n    t = i(o(sin(x)**2, x), x)\n    assert t == sin(x)**2\n    t = i(o(sin(x)**2 + 1, x), x)\n    assert t == sin(x)**2 + 1\n    t = i(o(sin(x)**2 + cos(x)**2, x), x)\n    assert t == sin(x)**2 + cos(x)**2\n    # hyperbolic identities\n    t = i(o(sinh(x)**2 + cosh(x)**2, x), x)\n    assert t == sinh(x)**2 + cosh(x)**2\n    t = i(o(sinh(x)**2 - cosh(x)**2, x), x)\n    assert t == sinh(x)**2 - cosh(x)**2\n    t = i(o(sinh(x)**2 - cosh(x)**2 + 1, x), x)\n    assert t == sinh(x)**2 - cosh(x)**2 + 1\n", "def test_FU():\n    # test application of FU transformations one at a time\n    from sympy.simplify.fu import FU, hyper_as_trig\n    from sympy import pi\n    f = [s for s in dir(FU) if s.isalpha()]\n    for s in f:\n        fu = FU[s]\n        for x in (pi/7, pi/6, pi/5, pi/4, pi/3, pi/2, 1):\n            for y in (pi/7, pi/6, pi/5, pi/4, pi/3, pi/2, 1):\n                if s in ('TR10', 'TR11', 'TR12', 'TR13', 'TR2', 'TR3'):\n                    e = fu(cos(x + y))\n                    assert e.is_real\n                elif s in ('TR1', 'TR14', 'TR6', 'TR7', 'TR8', 'TR9'):\n                    e = fu(cos(x)*cos(y))\n                    assert e.is_real\n                elif s in ('TR4', 'TRpower', 'TR15', 'TR16', 'TR22', 'TR5'):\n                    e = fu(sin(x)**2)\n                    assert e.is_real\n                elif s in ('TR10i',):\n                    e = fu(sin(x)*cos(y) + cos(x)*sin(y))\n                    assert e.is_real\n                elif s in ('TR111',):\n                    e = fu(1 - 1/tan(x)**2)\n                    assert e.is_real\n                elif s in ('L',):\n                    e = fu(cos(x))\n                    assert e.is_integer\n                elif s in ('TRmorrie',):\n                    e = fu(cos(x)*cos(2*x))\n                    assert e.is_real\n                elif s in ('TR2i',):\n                    e = fu(sin(x)/cos(x))\n                    assert e.is_real\n                elif s in ('TR12i',):\n                    e = fu((tan(x) + tan(y))/(tan(x)*tan(y) - 1))\n                    assert e.is_real\n                elif s == 'TR0':\n                    e = fu(sin(x) + 2*sin(x))\n                    assert e.is_real\n                elif s == 'CTR1':\n                    e = fu(sin(x)**2)\n                    assert e.is_real\n                elif s == 'CTR2':\n                    e = fu(cos(2*x))\n                    assert e"], "sample_329": ["def test_serialize_lazy_object_with_exception(self):\n        raise ValueError('Test exception')\n    lazy_object = SimpleLazyObject(lazy_func)\n    with self.assertRaisesMessage(ValueError, 'Test exception'):\n        self.serialize_round_trip(lazy_object)\n", "def test_serialize_function_type(self):\n        pass\n    self.assertSerializedResultEqual(\n        some_function,\n        (\"migrations.test_writer.some_function\", {'import migrations.test_writer'}),\n    )\n    self.serialize_round_trip(some_function)\n\n    class SomeClass:\n            pass\n    instance = SomeClass()\n    self.assertSerializedResultEqual(\n        instance.some_method,\n        (\"migrations.test_writer.SomeClass().some_method\", {'import migrations.test_writer'}),\n    )\n    self.serialize_round_trip(instance.some_method)\n", "def test_serialize_settings_reference(self):\n    settings.AUTH_USER_MODEL = 'auth.User'\n    self.assertSerializedResultEqual(\n        SettingsReference('AUTH_USER_MODEL', 'AUTH_USER_MODEL'),\n        (\"settings.AUTH_USER_MODEL\", {\"from django.conf import settings\"})\n    )\n    self.assertSerializedResultEqual(\n        SettingsReference('someapp.model', 'AUTH_USER_MODEL'),\n        (\"settings.AUTH_USER_MODEL\", {\"from django.conf import settings\"})\n    )\n    self.assertSerializedResultEqual(\n        SettingsReference('someapp.model', 'SOME_SETTING'),\n        (\"'someapp.model'\", set())\n    )\n", "    def test_serialize_lazy_object(self):\n        lazy_object = SimpleLazyObject(lambda: datetime.datetime.now())\n        with self.assertRaises(ValueError):\n            self.assertSerializedEqual(lazy_object)\n\n        lazy_object = SimpleLazyObject(lambda: datetime.datetime.now(), str)\n        with self.assertRaises(ValueError):\n            self.assertSerializedEqual(lazy_object)\n\n        lazy_object = SimpleLazyObject(lambda: datetime.datetime.now(), str, str)\n        with self.assertRaises(ValueError):\n            self.assertSerializedEqual(lazy_object)\n\n        lazy_object = SimpleLazyObject(lambda: datetime.datetime.now(), str, str, str)\n        with self.assertRaises(ValueError):\n            self.assertSerializedEqual(lazy_object)\n\n        lazy_object = SimpleLazyObject(lambda: datetime.datetime.now(), str, str, str, str)\n        with self.assertRaises(ValueError):\n            self.assertSerializedEqual(lazy_object)\n\n        lazy_object = SimpleLazyObject(lambda: datetime.datetime.now(), datetime.datetime)\n        string, imports = serializer_factory(lazy_object).serialize()\n        self.assertEqual(string, \"datetime.datetime.now()\")\n        self.assertEqual(imports, {'import datetime'})\n", "    def test_serialize_pathlib_os_path_like(self):\n        \"\"\"\n        Test pathlib.PurePath and os.PathLike when serialized.\n        \"\"\"\n        path = pathlib.PurePosixPath('/home/user')\n        self.assertSerializedResultEqual(\n            path,\n            (\"pathlib.PurePosixPath('/home/user')\", {'import pathlib'})\n        )\n\n        path = pathlib.PureWindowsPath('C:\\\\Users\\\\User')\n        self.assertSerializedResultEqual(\n            path,\n            (\"pathlib.PureWindowsPath('C:/Users/User')\", {'import pathlib'})\n        )\n\n        path = pathlib.Path('/home/user')\n        self.assertSerializedResultEqual(\n            path,\n            (\"pathlib.PurePosixPath('/home/user')\", {'import pathlib'})\n        )\n\n        path = os.PathLike()\n        self.assertSerializedResultEqual(\n            path,\n            (repr(path.path), {})\n        )\n"], "sample_1170": ["def test_Pow_with_rational_exponent():\n    assert str(x**Rational(3, 2)) == \"x**(3/2)\"\n    assert str(x**Rational(-3, 2)) == \"x**(-3/2)\"\n    assert str(x**Rational(-3, -2)) == \"x**(3/2)\"\n    assert str((x + 1)**Rational(3, 2)) == \"(x + 1)**(3/2)\"\n    assert str((x + 1)**Rational(-3, 2)) == \"(x + 1)**(-3/2)\"\n    assert str((x + 1)**Rational(-3, -2)) == \"(x + 1)**(3/2)\"\n", "def test_ConditionSet():\n    from sympy.sets.sets import ConditionSet\n    from sympy import symbols\n    x = symbols('x')\n    condset = ConditionSet(x, x > 1)\n    assert str(condset) == \"ConditionSet(x, x > 1)\"\n    assert sstr(condset) == \"ConditionSet(x, x > 1)\"\n", "def test_lattice_operations():\n    e = x + y\n    f = x - y\n    g = x + z\n    assert str(e.join(f, g)) == \"join(x + y, x - y, x + z)\"\n    assert str(e.meet(f, g)) == \"meet(x + y, x - y, x + z)\"\n", "def test_Pow_special_cases():\n    assert sstr(Pow(x, 1, evaluate=False)) == 'x**1'\n    assert sstr(Pow(x, 0, evaluate=False)) == 'x**0'\n    assert sstr(Pow(x, -1, evaluate=False)) == 'x**(-1)'\n    assert sstr(Pow(x, -0, evaluate=False)) == 'x**0'\n    assert sstr(Pow(x, 1, 0, evaluate=False)) == 'root(x, 0)'\n    assert sstr(Pow(x, 0, 0, evaluate=False)) == 'root(x, 0)'\n", "def test_StrPrinter_settings():\n    assert sstr(x**2, order='lex') == 'x**2'\n    assert sstr(x**2, order='none') == 'x**2'\n    assert sstr(x + y, order='lex') == 'x + y'\n    assert sstr(x + y, order='none') == 'y + x'\n    assert sstr(x*y, order='lex') == 'x*y'\n    assert sstr(x*y, order='none') == 'y*x'\n    assert sstr(x*y + 1, order='none') == 'y*x + 1'\n    assert sstr(x*y + 1, order='lex') == 'x*y + 1'\n    assert sstr(x*y + 1, abbrev=True) == 'x*y + 1'\n\n    # test multiple settings\n    assert sstr(x*y + 1, order='none', abbrev=True) == 'y*x + 1'\n    assert sstr(x*y + 1, order='lex', abbrev=True) == 'x*y + 1'\n    assert sstr(x*y + 1, order='none', sympy_integers=True) == 'y*x + 1'\n    assert sstr(x*y + 1, order='lex', sympy_integers=True) == 'x*y + 1'\n    assert sstr(x*y + 1, order='none', abbrev=True, sympy_integers=True) == 'y*x + 1'\n    assert sstr(x*y + 1, order='lex', abbrev=True, sympy_integers=True) == 'x*y + 1'\n\n    # check that perm_cyclic is used\n    from sympy.combinatorics.permutations import Permutation\n    p = Permutation([0, 3, 2, 1])\n    assert sstr(p, perm_cyclic=True) == \"(0 1 3)\"\n    assert sstr(p, perm_cyclic=False) == \"Permutation([0, 3, 2, 1])\"\n"], "sample_18": ["    def setup_class(self):\n        self.q = u.Quantity(np.arange(1.0, 5.0), \"m/s\")\n        self.q.info.name = \"v\"\n        self.q.info.description = \"air speed of a african swallow\"\n", "    def test_to_string(self):\n        q = u.Quantity(np.arange(1.0, 5.0), \"m/s\")\n        q.info.name = \"v\"\n        q.info.description = \"air speed of a african swallow\"\n\n        q_str = q.to_string()\n        assert_info_equal(q_str, q)\n\n        q_latex = q.to_string(format=\"latex\")\n        assert_info_equal(q_latex, q)\n\n        q_latex_inline = q.to_string(format=\"latex_inline\")\n        assert_info_equal(q_latex_inline, q)\n", "    def setup_class(self):\n        self.q = u.Quantity(np.arange(1.0, 5.0), \"m/s\")\n", "    def setup_class(self):\n        self.q1 = u.Quantity(1, \"m\")\n        self.q2 = u.Quantity(1, \"m\")\n", "    def setup_class(self):\n        self.q1 = u.Quantity(np.array([1.0, 2.0, 3.0]), \"m\")\n        self.q1.info.name = \"x\"\n        self.q1.info.description = \"some length\"\n\n        self.q2 = u.Quantity(np.array([4.0, 5.0, 6.0]), \"m\")\n        self.q2.info.name = \"y\"\n        self.q2.info.description = \"some other length\"\n\n        self.q3 = u.Quantity(np.array([7.0, 8.0, 9.0]), \"cm\")\n        self.q3.info.name = \"z\"\n        self.q3.info.description = \"some other length\"\n"], "sample_184": ["    def test_check_constraint_pointing_to_generic_relation(self):\n        class Tag(models.Model):\n            pass\n\n        class Item(models.Model):\n            tag = models.GenericForeignKey('tag', for_concrete_model=True)\n            content_type = models.ForeignKey(ContentType, on_delete=models.CASCADE)\n            object_id = models.PositiveIntegerField()\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(\n                        name='name',\n                        check=models.Q(tag__isnull=False) & models.Q(content_type__isnull=False) & models.Q(object_id__isnull=False)\n                    ),\n                ]\n\n        self.assertEqual(Item.check(databases=self.databases), [\n            Error(\n                \"'constraints' refers to the joined field 'tag'.\",\n                obj=Item,\n                id='models.E041',\n            ),\n        ])\n", "    def test_unique_constraint_with_non_unique_fields(self):\n        class Model(models.Model):\n            age = models.IntegerField(unique=True)\n            age2 = models.IntegerField(unique=False)\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(fields=['age', 'age2'], name='unique_age_age2'),\n                ]\n\n        errors = Model.check(databases=self.databases)\n        expected = [\n            Error(\n                \"All fields of a unique constraint must be unique.\",\n                obj=Model._meta.get_field('age2'),\n                id='fields.E325',\n            ),\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_check_unique_constraint_with_function(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        name='unique_age',\n                        fields=['age'],\n                        include=['age'],\n                        function=models.F('age')\n                    ),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [\n            Error(\n                \"Invalid constraint. 'function' can't be used with 'fields'.\",\n                obj=Model,\n                id='models.E042',\n            ),\n        ])\n", "    def test_property_name_clash(self):\n        class Model(models.Model):\n            fk = models.ForeignKey('self', models.CASCADE)\n\n            @property\n                pass\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"The property 'fk_id' clashes with a related field accessor.\",\n                obj=Model,\n                id='models.E025',\n            )\n        ])\n", "    def test_index_with_function_expression(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                indexes = [\n                    models.Index(\n                        models.functions.Upper('age'),\n                        name='index_age_upper',\n                    ),\n                ]\n\n        errors = Model.check(databases=self.databases)\n        expected = [] if connection.features.supports_covering_indexes else [\n            Warning(\n                '%s does not support indexes with non-key columns.'\n                % connection.display_name,\n                hint=(\n                    \"Non-key columns will be ignored. Silence this warning if \"\n                    \"you don't care about it.\"\n                ),\n                obj=Model,\n                id='models.W040',\n            )\n        ]\n        self.assertEqual(errors, expected)\n"], "sample_39": ["def test_sip_with_altkey2():\n    \"\"\"\n    Test that when creating a WCS object using a key, CTYPE with\n    that key is looked at and not the primary CTYPE, even if\n    relax=False.\n    fix for #5443.\n    \"\"\"\n    with fits.open(get_pkg_data_filename('data/sip.fits')) as f:\n        w = wcs.WCS(f[0].header)\n    # create a header with two WCSs.\n    h1 = w.to_header(relax=False, key='A')\n    h2 = w.to_header(relax=False)\n    h1['CTYPE1A'] = \"RA---SIN-SIP\"\n    h1['CTYPE2A'] = \"DEC--SIN-SIP\"\n    h1.update(h2)\n    w = wcs.WCS(h1, key='A', relax=False)\n    assert (w.wcs.ctype == np.array(['RA---SIN-SIP', 'DEC--SIN-SIP'])).all()\n", "def test_pixel_bounds():\n    \"\"\"\n    Test pixel_bounds property.\n    \"\"\"\n    w = wcs.WCS(naxis=2)\n    w._naxis = [1000, 500]\n    assert w.pixel_bounds == [(0, 1000), (0, 500)]\n    assert w.pixel_shape == (1000, 500)\n\n    w.pixel_bounds = [(10, 1010), (20, 520)]\n    assert w._naxis == [1000, 500]\n\n    w.pixel_bounds = [(10, 1010), (20, 520), (0, 1000)]\n    assert w._naxis == [1000, 500, 1000]\n\n    w = wcs.WCS(naxis=2)\n    w.pixel_bounds = None\n    assert w._naxis == [0, 0]\n", "def test_crpix_update():\n    \"\"\"\n    Test that WCS object's pixel shape is updated when WCS's crpix is updated.\n    \"\"\"\n    w = wcs.WCS(naxis=2)\n    w._naxis = [100, 200]\n    w.wcs.crpix = [50, 100]\n    w.wcs.cdelt = [0.1, 0.1]\n    w.wcs.crval = [1, 1]\n\n    assert w.pixel_shape == (100, 200)\n    assert w.array_shape == (200, 100)\n\n    w.wcs.crpix = [10, 20]\n    assert w.pixel_shape == (100, 200)\n    assert w.array_shape == (200, 100)\n\n    w.wcs.crpix = [150, 250]\n    assert w.pixel_shape == (100, 200)\n    assert w.array_shape == (200, 100)\n\n    w.wcs.crpix = [10, 20]\n    w.wcs.cdelt = [0.01, 0.01]\n    assert w.pixel_shape == (100, 200)\n    assert w.array_shape == (200, 100)\n", "def test_sip_a_b_order():\n    \"\"\"\n    Test that WCS raises an InvalidTransformError when SIP A_ORDER or B_ORDER\n    keyword values are out of the valid range.\n    \"\"\"\n\n    header = fits.Header()\n    header['CTYPE1'] = \"RA---SIN\"\n    header['CTYPE2'] = \"DEC--SIN\"\n    header['CRVAL1'] = 12.5\n    header['CRVAL2'] = 34.2\n    header['CRPIX1'] = 100\n    header['CRPIX2'] = 200\n    header['CDELT1'] = -0.01\n    header['CDELT2'] = 0.01\n    header['A_ORDER'] = 0\n    header['B_ORDER'] = 1\n\n    with pytest.raises(wcs.InvalidTransformError):\n        wcs.WCS(header)\n", "def test_pixel_bounds():\n    \"\"\"\n    Test for #6331\n    \"\"\"\n    w = wcs.WCS(naxis=2)\n    w._naxis = [100, 100]\n    assert w.pixel_bounds == [(0.5, 99.5), (0.5, 99.5)]\n    assert w.pixel_bounds == w.wcs.pixel_bounds\n    w.wcs.pixel_bounds = [(0, 100), (0, 100)]\n    assert w.pixel_bounds == w.wcs.pixel_bounds\n\n    # Make sure WCS object attributes are updated correctly.\n    w.pixel_bounds = [(0, 100), (0, 100)]\n    assert w._naxis == [100, 100]\n\n    w.pixel_bounds = None\n    assert w._naxis == [None, None]\n"], "sample_45": ["def test_trunc_subquery_with_timezone(self):\n    author_1 = Author.objects.create(name='J. R. R. Tolkien')\n    author_2 = Author.objects.create(name='G. R. R. Martin')\n    melb = pytz.timezone('Australia/Melbourne')\n    sao = pytz.timezone('America/Sao_Paulo')\n    fan_since_1 = datetime(2016, 2, 3, 15, 0, 0)\n    fan_since_2 = datetime(2015, 2, 3, 15, 0, 0)\n    fan_since_3 = datetime(2017, 2, 3, 15, 0, 0)\n    if settings.USE_TZ:\n        fan_since_1 = timezone.make_aware(fan_since_1, is_dst=False)\n        fan_since_2 = timezone.make_aware(fan_since_2, is_dst=False)\n        fan_since_3 = timezone.make_aware(fan_since_3, is_dst=False)\n    Fan.objects.create(author=author_1, name='Tom', fan_since=fan_since_1)\n    Fan.objects.create(author=author_1, name='Emma', fan_since=fan_since_2)\n    Fan.objects.create(author=author_2, name='Isabella', fan_since=fan_since_3)\n\n    inner = Fan.objects.filter(\n        author=OuterRef('pk'),\n        name__in=('Emma', 'Isabella', 'Tom')\n    ).values('author').annotate(newest_fan=Max('fan_since')).values('newest_fan')\n    outer = Author.objects.annotate(\n        newest_fan_year=TruncYear(Subquery(inner, output_field=DateTimeField()), tzinfo=melb)\n    )\n    tz = melb if settings.USE_TZ else None\n    self.assertSequenceEqual(\n        outer.order_by('name').values('name', 'newest_fan_year'),\n        [\n            {'name': 'G. R. R. Martin', 'newest_fan_year': datetime(2017, 1, 1, 0, 0, tzinfo=tz)},\n            {'name': 'J. R. R. Tolkien', 'newest_fan_year': datetime(2016, 1, 1, 0, 0, tzinfo=tz)},\n        ]\n    )\n\n    outer = Author.objects.annotate(\n       ", "def test_now_func(self):\n    with self.subTest('without timezone'):\n        start_datetime = datetime(2015, 6, 15, 14, 10, 50, 321)\n        self.create_model(start_datetime, start_datetime)\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(now=Now()).order_by('start_datetime'),\n            [(start_datetime, datetime.now())],\n            lambda m: (m.start_datetime, m.now)\n        )\n\n    with self.subTest('with timezone'):\n        with timezone.override(pytz.timezone('Australia/Melbourne')):\n            start_datetime = datetime(2015, 6, 15, 14, 10, 50, 321)\n            self.create_model(start_datetime, start_datetime)\n            self.assertQuerysetEqual(\n                DTModel.objects.annotate(now=Now()).order_by('start_datetime'),\n                [(start_datetime, datetime.now(pytz.timezone('Australia/Melbourne')))],\n                lambda m: (m.start_datetime, m.now)\n            )\n", "def test_trunc_timezone_awareness(self):\n    start_datetime = datetime(2016, 1, 1, 12, 0, 0)\n    end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n    self.create_model(end_datetime, start_datetime)\n\n    melb = pytz.timezone('Australia/Melbourne')\n\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated=Trunc('start_datetime', kind, output_field=DateTimeField(), tzinfo=melb)\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime.astimezone(melb), kind, melb)),\n                (end_datetime, truncate_to(end_datetime.astimezone(melb), kind, melb))\n            ],\n            lambda m: (m.start_datetime, m.truncated)\n        )\n\n    test_datetime_kind('year')\n    test_datetime_kind('quarter')\n    test_datetime_kind('month')\n    test_datetime_kind('week')\n    test_datetime_kind('day')\n    test_datetime_kind('hour')\n    test_datetime_kind('minute')\n    test_datetime_kind('second')\n", "def test_now_func(self):\n    now = timezone.now()\n    if settings.USE_TZ:\n        now = timezone.make_aware(now, is_dst=False)\n\n    self.create_model(now, None)\n    self.create_model(None, now)\n\n    self.assertQuerysetEqual(\n        DTModel.objects.annotate(now=Now()).order_by('start_datetime'),\n        [\n            (None, now),\n            (now, now),\n        ],\n        lambda m: (m.start_datetime, m.now)\n    )\n\n    self.assertEqual(DTModel.objects.filter(start_datetime=Now()).count(), 1)\n\n    msg = 'output_field must be either DateField, TimeField, or DateTimeField'\n    with self.assertRaisesMessage(ValueError, msg):\n        list(DTModel.objects.annotate(now=Now(output_field=IntegerField())))\n\n    with self.assertRaisesMessage(AssertionError, \"'name' isn't a DateField, TimeField, or DateTimeField.\"):\n        list(DTModel.objects.annotate(now=Now('name')))\n\n    with self.assertRaisesMessage(ValueError, \"Cannot truncate DateField 'start_date' to DateTimeField\"):\n        list(DTModel.objects.annotate(now=Now('start_date')))\n\n    with self.assertRaisesMessage(ValueError, \"Cannot truncate TimeField 'start_time' to DateTimeField\"):\n        list(DTModel.objects.annotate(now=Now('start_time')))\n", "def test_now_func(self):\n    now = timezone.now()\n    tz = timezone.get_default_timezone() if settings.USE_TZ else None\n    dt = datetime.now(tz)\n    with self.assertNumQueries(2):\n        self.assertLessEqual((now - dt).total_seconds(), 1)\n        self.assertEqual(DTModel.objects.annotate(now=Now()).first().now, now)\n\n    self.assertEqual(DTModel.objects.filter(start_datetime__lte=Now()).count(), 2)\n    self.assertEqual(DTModel.objects.filter(start_datetime__lt=Now()).count(), 2)\n    self.assertEqual(DTModel.objects.filter(start_datetime__gt=Now()).count(), 0)\n    self.assertEqual(DTModel.objects.filter(start_datetime__gte=Now()).count(), 0)\n    self.assertEqual(DTModel.objects.filter(end_datetime__lte=Now()).count(), 2)\n    self.assertEqual(DTModel.objects.filter(end_datetime__lt=Now()).count(), 2)\n    self.assertEqual(DTModel.objects.filter(end_datetime__gt=Now()).count(), 0)\n    self.assertEqual(DTModel.objects.filter(end_datetime__gte=Now()).count(), 0)\n\n    msg = \"lookup type must be 'exact', 'lt', 'lte', 'gt', or 'gte'\"\n    with self.assertRaisesMessage(ValueError, msg):\n        DTModel.objects.filter(start_datetime__startswith=Now())\n"], "sample_686": ["def test_node_use_from_parent_with_name(testdir) -> None:\n    class MockConfig:\n        pass\n\n    ms = MockConfig()\n    node_name = \"MockNode\"\n    with pytest.warns(\n        DeprecationWarning,\n        match=f\"Direct construction of {node_name} has been deprecated, please use {node_name}.from_parent.*\",\n    ) as w:\n        nodes.Node(name=node_name, config=ms, session=ms, nodeid=\"None\")  # type: ignore\n    assert w[0].lineno == inspect.currentframe().f_lineno - 1  # type: ignore\n    assert w[0].filename == __file__\n", "def test_pytest_collect_module_renamed() -> None:\n    \"\"\"Test deprecation of pytest.collect.{name} -> pytest.{name}\"\"\"\n    with pytest.warns(pytest.PytestDeprecationWarning, match=\"pytest.collect.{name} was moved to pytest.{name}\"):\n        deprecated.PYTEST_COLLECT_MODULE(name=\"collect\")  # type: ignore\n", "def test_fixture_positional_arguments_are_deprecated(fixture_function, expected_warning):\n    if expected_warning:\n        with pytest.warns(pytest.PytestDeprecationWarning, match=expected_warning.args[0]):\n            fixture_function()\n    else:\n        with warnings.catch_warnings(record=True) as w:\n            fixture_function()\n            assert len(w) == 0\n", "def test_pytest_collect_module_warning_formatting() -> None:\n    \"\"\"Check the warning format for pytest.collect module deprecation\"\"\"\n    with pytest.warns(pytest.PytestDeprecationWarning, match=r\"pytest\\.collect\\..* was moved to pytest\\..*\") as w:\n        pytest.PYTEST_COLLECT_MODULE.format(name=\"example\")\n    assert len(w) == 1\n    assert w[0].lineno == inspect.currentframe().f_lineno - 1\n    assert w[0].filename == __file__\n", "def test_deprecated_warnings(fixture, testdir):\n    testdir.makepyfile(\n        f\"\"\"\n        def test_{fixture}(pytestconfig):\n            with pytest.warns(pytest.PytestDeprecationWarning):\n                {fixture} = getattr(pytestconfig, '{fixture}')\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    assert result.ret == 0\n    result.stdout.fnmatch_lines(\n        [f\"*PytestDeprecationWarning: {getattr(deprecated, fixture).message}*\"]\n    )\n"], "sample_391": ["def test_create_model_alter_model_options(self):\n    \"\"\"\n    AlterModelOptions should optimize into CreateModel.\n    \"\"\"\n    managers = [(\"objects\", EmptyManager())]\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[(\"name\", models.CharField(max_length=255))],\n                options={\"verbose_name\": \"Foo\"},\n                bases=(UnicodeModel,),\n                managers=managers,\n            ),\n            migrations.AlterModelOptions(\n                name=\"Foo\", options={\"verbose_name_plural\": \"Foos\"}\n            ),\n        ],\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[(\"name\", models.CharField(max_length=255))],\n                options={\"verbose_name\": \"Foo\", \"verbose_name_plural\": \"Foos\"},\n                bases=(UnicodeModel,),\n                managers=managers,\n            ),\n        ],\n    )\n", "def test_add_constraint_remove_constraint(self):\n    \"\"\"\n    RemoveConstraint should cancel AddConstraint\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.AddConstraint(\n                model_name=\"Foo\",\n                constraint=models.UniqueConstraint(fields=(\"name\",), name=\"unique_name\"),\n            ),\n            migrations.RemoveConstraint(\n                model_name=\"Foo\",\n                name=\"unique_name\",\n            ),\n        ],\n        [],\n    )\n", "def test_create_model_alter_together(self):\n    \"\"\"\n    AlterTogether should optimize into CreateModel.\n    \"\"\"\n    managers = [(\"objects\", EmptyManager())]\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[\n                    (\"name\", models.CharField(max_length=255)),\n                    (\"age\", models.IntegerField()),\n                ],\n                options={\"verbose_name\": \"Foo\"},\n                bases=(UnicodeModel,),\n                managers=managers,\n            ),\n            migrations.AlterUniqueTogether(\"Foo\", [[\"name\", \"age\"]]),\n        ],\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[\n                    (\"name\", models.CharField(max_length=255)),\n                    (\"age\", models.IntegerField()),\n                ],\n                options={\"verbose_name\": \"Foo\", \"unique_together\": {(\"name\", \"age\")}},\n                bases=(UnicodeModel,),\n                managers=managers,\n            ),\n        ],\n    )\n", "def test_alter_model_table_rename_model(self):\n    \"\"\"\n    AlterModelTable should optimize into RenameModel.\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                \"Foo\", [(\"name\", models.CharField(max_length=255))]\n            ),\n            migrations.AlterModelTable(\"Foo\", \"woohoo\"),\n            migrations.RenameModel(\"Foo\", \"Bar\"),\n        ],\n        [\n            migrations.CreateModel(\n                \"Bar\", [(\"name\", models.CharField(max_length=255))]\n            ),\n            migrations.AlterModelTable(\"Bar\", \"woohoo\"),\n        ],\n    )\n", "def test_remove_index(self):\n    \"\"\"\n    RemoveIndex should cancel AddIndex\n    \"\"\"\n    index = models.Index(fields=['name'])\n    self.assertOptimizesTo(\n        [\n            migrations.AddIndex(\"Foo\", index),\n            migrations.RemoveIndex(\"Foo\", \"Foo_name_0c5f8478_idx\"),\n        ],\n        [],\n    )\n"], "sample_688": ["def test_ensure_extended_length_path(testdir):\n    if not sys.platform.startswith(\"win32\"):\n        pytest.skip(\"Test only applicable on Windows\")\n\n    long_path = \"a\" * 250 + \".txt\"\n    p = testdir.tmpdir.join(long_path)\n    p.write(\"content\")\n    assert not p.exists()\n    ep = ensure_extended_length_path(p)\n    assert ep.exists()\n    assert ep.read_text(\"utf-8\") == \"content\"\n", "def test_resolve_from_str(testdir):\n    p = testdir.makepyfile(\"def test_foo(): pass\")\n    path = Path(str(p))\n    assert resolve_from_str(str(path), path.parent) == path\n\n    # Test with relative path\n    rel_path = os.path.relpath(str(path), start=str(path.parent))\n    assert resolve_from_str(rel_path, path.parent) == path\n\n    # Test with user-expanded path\n    user_path = os.path.expanduser(str(path))\n    assert resolve_from_str(user_path, path.parent) == path\n\n    # Test with variable-expanded path\n    var_path = os.path.expandvars(str(path))\n    assert resolve_from_str(var_path, path.parent) == path\n\n    # Test with invalid path\n    invalid_path = \"invalid_path\"\n    with pytest.raises(FileNotFoundError):\n        resolve_from_str(invalid_path, path.parent)\n", "    def test_cleanup_numbered_dir(self, testdir):\n        prefix = \"test-\"\n        root = testdir.mkdir(prefix)\n        root.ensure(\"test-1\").mkdir()\n        root.ensure(\"test-2\").mkdir()\n        root.ensure(\"test-3\").mkdir()\n        root.ensure(\"test-4\").mkdir()\n\n        # Create a lock for each numbered dir.\n        for dir in root.listdir():\n            create_cleanup_lock(dir)\n\n        cleanup_numbered_dir(root, prefix, 2, 0)\n\n        # Ensure only the two most recent numbered dirs remain.\n        remaining_dirs = [d.basename for d in root.listdir()]\n        assert remaining_dirs == [\"test-3\", \"test-4\"]\n", "    def test_empty_lock_file(self, testdir):\n        lock_path = testdir.makefile(\".txt\", \"123\")\n        assert not ensure_deletable(lock_path, consider_lock_dead_if_created_before=0)\n", "def test_import_path_mode_importlib_reproducible_hashing(testdir):\n    \"\"\"Check that --import-mode=importlib uses reproducible hashing for test node IDs (#7251).\"\"\"\n    testdir.makepyfile(\n        **{\n            \"tests/test_foo.py\": \"\"\"\n                    pass\n            \"\"\"\n        }\n    )\n    result1 = testdir.runpytest(\"-v\", \"--import-mode=importlib\")\n    result1.stdout.fnmatch_lines([\"tests/test_foo.py::test_foo *\", \"* 1 passed in *\"])\n\n    # Check that test node IDs are reproducible\n    node_id1 = result1.stdout.str().split(\"tests/test_foo.py::test_foo\")[1].strip().split()[0]\n    result2 = testdir.runpytest(\"-v\", \"--import-mode=importlib\")\n    result2.stdout.fnmatch_lines([\"tests/test_foo.py::test_foo *\", \"* 1 passed in *\"])\n    node_id2 = result2.stdout.str().split(\"tests/test_foo.py::test_foo\")[1].strip().split()[0]\n    assert node_id1 == node_id2\n"], "sample_888": ["def test_iforest_warm_start_with_contamination(global_random_seed):\n    \"\"\"Test iterative addition of iTrees to an iForest with contamination parameter\"\"\"\n    \n    rng = check_random_state(global_random_seed)\n    X = rng.randn(20, 2)\n\n    # fit first 10 trees\n    clf = IsolationForest(\n        n_estimators=10, max_samples=20, random_state=rng, warm_start=True, contamination=0.1\n    )\n    clf.fit(X)\n    # remember the 1st tree\n    tree_1 = clf.estimators_[0]\n    # fit another 10 trees\n    clf.set_params(n_estimators=20)\n    clf.fit(X)\n    # expecting 20 fitted trees and no overwritten trees\n    assert len(clf.estimators_) == 20\n    assert clf.estimators_[0] is tree_1\n\n    # test offset_ calculation\n    assert clf.offset_ == np.percentile(clf._score_samples(X), 100.0 * 0.1)\n", "def test_iforest_verbose_output(caplog):\n    \"\"\"Check that IsolationForest produces the correct output with verbose.\"\"\"\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([1, 0])\n    model = IsolationForest(verbose=1)\n    model.fit(X, y)\n    assert \"Building estimator\" in caplog.text\n", "def test_iforest_subsampled_features_random_state(global_random_seed):\n    \"\"\"Check IsolationForest when max_features is a float and random_state is set.\"\"\"\n    rng = check_random_state(global_random_seed)\n    X_train, X_test, y_train, y_test = train_test_split(\n        diabetes.data[:50], diabetes.target[:50], random_state=rng\n    )\n    clf = IsolationForest(max_features=0.8, random_state=global_random_seed)\n    clf.fit(X_train, y_train)\n    clf.predict(X_test)\n    clf.set_params(random_state=global_random_seed).fit(X_train, y_train)\n    assert_array_almost_equal(clf.predict(X_test), clf.predict(X_test))\n", "def test_iforest_feature_names():\n    \"\"\"Check that feature names are handled correctly in IsolationForest.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    rng = np.random.RandomState(0)\n\n    X = pd.DataFrame(data=rng.randn(4, 2), columns=[\"a\", \"b\"])\n    model = IsolationForest(random_state=0)\n\n    model.fit(X)\n    assert_array_equal(model.feature_names_in_, X.columns)\n\n    X_list = X.values.tolist()\n    model.fit(X_list)\n    assert model.feature_names_in_ is None\n\n    X_with_non_string_columns = pd.DataFrame(data=rng.randn(4, 2), columns=[1, 2])\n    model.fit(X_with_non_string_columns)\n    assert model.feature_names_in_ is None\n", "def test_iforest_contamination_threshold(global_random_seed):\n    \"\"\"Test if contamination threshold is correctly applied\"\"\"\n    rng = check_random_state(global_random_seed)\n    X = np.vstack((rng.randn(100, 2), rng.randn(1, 2) * 10))\n    y = np.zeros(101)\n\n    contamination = 0.01\n    clf = IsolationForest(contamination=contamination, random_state=rng).fit(X)\n    y_pred = clf.predict(X)\n    outliers = np.sum(y_pred == -1)\n    assert np.isclose(outliers / len(y), contamination, atol=0.05)\n\n    contamination = \"auto\"\n    clf = IsolationForest(contamination=contamination, random_state=rng).fit(X)\n    y_pred = clf.predict(X)\n    outliers = np.sum(y_pred == -1)\n    assert outliers > 0\n"], "sample_1148": ["def test_matrixelement_as_coeff_Mul():\n    assert MatrixElement(A, 1, 2).as_coeff_Mul() == (S.One, A[1, 2])\n    assert MatrixElement(2*A, 1, 2).as_coeff_Mul() == (2, A[1, 2])\n", "def test_MatrixElement_with_Slice():\n    x, y, z, w = symbols(\"x y z w\")\n    M = Matrix([[x, y], [z, w]])\n    i, j = symbols(\"i, j\")\n    Mij = M[i, j]\n    assert isinstance(Mij, MatrixElement)\n    Ms = SparseMatrix([[2, 3], [4, 5]])\n    msij = Ms[i, j]\n    assert isinstance(msij, MatrixElement)\n    for oi, oj in [(0, 0), (0, 1), (1, 0), (1, 1)]:\n        assert Mij.subs({i: oi, j: oj}) == M[oi, oj]\n        assert msij.subs({i: oi, j: oj}) == Ms[oi, oj]\n    A = MatrixSymbol(\"A\", 2, 2)\n    assert A[0, 0].subs(A, M) == x\n    assert A[i, j].subs(A, M) == M[i, j]\n    assert M[i, j].subs(M, A) == A[i, j]\n\n    A = MatrixSymbol(\"A\", 3, 3)\n    assert isinstance(A[1:3, 1:3], MatrixSlice)\n    assert isinstance(A[1:3, 1], MatrixSlice)\n    assert isinstance(A[1, 1:3], MatrixSlice)\n    assert isinstance(A[1, 1], MatrixElement)\n    raises(IndexError, lambda: A[3:5, 1:3])\n    raises(IndexError, lambda: A[1:3, 3:5])\n    raises(IndexError, lambda: A[3, 1:3])\n    raises(IndexError, lambda: A[1:3, 3])\n\n    A = MatrixSymbol(\"A\", n, m)\n    assert isinstance(A[1:3, 1:3], MatrixSlice)\n    assert isinstance(A[1:3, 1], MatrixSlice)\n    assert isinstance(A[1, 1:3], MatrixSlice)\n    assert isinstance(A[1, 1], MatrixElement)\n    raises(IndexError, lambda: A[3:5, 1:3])\n    raises(IndexError, lambda: A[1:3, 3:5])\n    raises(IndexError, lambda: A[3", "def test_derivative_matrix_lines():\n    M = MatrixSymbol('M', 2, 2)\n    N = MatrixSymbol('N', 2, 3)\n    i, j = symbols('i, j')\n    A = MatrixSymbol('A', 2, 2)\n    x = symbols('x')\n\n    assert A._eval_derivative_matrix_lines(x) == [_LeftRightArgs(\n        [Identity(2), Identity(2)],\n    )]\n    assert M._eval_derivative_matrix_lines(x) == [_LeftRightArgs(\n        [ZeroMatrix(2, 2), ZeroMatrix(2, 2)],\n    )]\n    assert N._eval_derivative_matrix_lines(x) == [_LeftRightArgs(\n        [ZeroMatrix(2, 2), ZeroMatrix(3, 3)],\n    )]\n    assert (M*N)._eval_derivative_matrix_lines(x) == [\n        _LeftRightArgs(\n            [Identity(2), ZeroMatrix(2, 3)],\n        ),\n        _LeftRightArgs(\n            [ZeroMatrix(2, 2), Identity(3)],\n        )\n    ]\n    assert (A*N)._eval_derivative_matrix_lines(x) == [\n        _LeftRightArgs(\n            [Identity(2), ZeroMatrix(2, 3)],\n        ),\n        _LeftRightArgs(\n            [ZeroMatrix(2, 2), Identity(3)],\n        )\n    ]\n    assert (A*N).T._eval_derivative_matrix_lines(x) == [\n        _LeftRightArgs(\n            [ZeroMatrix(3, 3), Identity(2)],\n        ),\n        _LeftRightArgs(\n            [Identity(3), ZeroMatrix(2, 2)],\n        )\n    ]\n", "def test_matrix_expr_from_index_summation():\n    from sympy import MatrixSymbol, Sum, symbols\n    from sympy.tensor import IndexedBase\n    i, j, k, l = symbols('i j k l', integer=True)\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    C = MatrixSymbol('C', 3, 3)\n    D = MatrixSymbol('D', 3, 3)\n    X = IndexedBase('X')\n\n    assert MatrixExpr.from_index_summation(Sum(A[i, j]*B[j, k], (j, 0, 2))) == A*B\n    assert MatrixExpr.from_index_summation(Sum(A[i, j]*B[k, j], (j, 0, 2))) == A.T*B\n    assert MatrixExpr.from_index_summation(Sum(A[i, j]*B[j, i], (i, 0, 2), (j, 0, 2))) == A*B.T\n    assert MatrixExpr.from_index_summation(Sum(A[i, i], (i, 0, 2))) == A.trace()\n    assert MatrixExpr.from_index_summation(Sum(X[i]*A[i, j]*B[j, k], (i, 0, 2), (j, 0, 2))) == X.as_coeff_Mul()[0]*A*B\n    assert MatrixExpr.from_index_summation(Sum(A[i, j]*B[j, k]*C[l, k], (j, 0, 2), (k, 0, 2))) == A*B*C\n    assert MatrixExpr.from_index_summation(Sum(A[i, j]*B[j, k]*C[i, l], (j, 0, 2))) == A*B*C.T\n    assert MatrixExpr.from_index_summation(Sum(A[i, j]*B[j, k]*C[l, i], (j, 0, 2))) == A.T*B*C\n    assert MatrixExpr.from_index_summation(Sum(A[i, j]*B[k, j]*C[i, l], (j, 0, 2))) == A.T*B*C\n    assert MatrixExpr.from_index_summation(Sum(A[i, j]*B[j, k]*C[l, k]*D", "def test_matrix_derivative():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    w = MatrixSymbol('w', n, 1)\n    D = MatrixSymbol('D', n, 1)\n\n    assert w.diff(w).equals(Identity(n))\n\n    expr1 = (A*w)[k, 0]\n    expr2 = (B*w)[k, 0]\n    assert expr1.diff(w) == D\n    assert expr2.diff(w) == D\n    assert (expr1 + expr2).diff(w).shape == (n, n)\n\n    expr1 = (A*B)[k, 0]\n    assert expr1.diff(A).shape == (n, n, n)\n\n    expr = A*A.T\n    assert expr.diff(A).shape == (n, n, n)\n\n    expr = A*A.T*A\n    assert expr.diff(A).shape == (n, n, n)\n"], "sample_802": ["def test_feature_union_nested_pipeline():\n    # test feature union with nested pipeline\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    pca = PCA(n_components=2, svd_solver='randomized', random_state=0)\n    pipe = Pipeline([('pca', pca)])\n    select = SelectKBest(k=1)\n    fu = FeatureUnion([(\"pipe\", pipe), (\"select\", select)])\n    fu.fit(X, y)\n    X_transformed = fu.transform(X)\n    assert_equal(X_transformed.shape, (X.shape[0], 3))\n\n    # check if it does the expected thing\n    assert_array_almost_equal(X_transformed[:, :-1], pca.fit_transform(X))\n    assert_array_equal(X_transformed[:, -1],\n                       select.fit_transform(X, y).ravel())\n", "def test_feature_union_transformer_weights():\n    # test feature union with transformer weights\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    pca = PCA(n_components=2, svd_solver='randomized', random_state=0)\n    select = SelectKBest(k=1)\n    # test using fit followed by transform\n    fs = FeatureUnion([(\"pca\", pca), (\"select\", select)],\n                      transformer_weights={\"pca\": 10})\n    fs.fit(X, y)\n    X_transformed = fs.transform(X)\n    # test using fit_transform\n    fs = FeatureUnion([(\"pca\", pca), (\"select\", select)],\n                      transformer_weights={\"pca\": 10})\n    X_fit_transformed = fs.fit_transform(X, y)\n    # check against expected result\n\n    # We use a different pca object to control the random_state stream\n    assert_array_almost_equal(X_transformed[:, :-1], 10 * pca.fit_transform(X))\n    assert_array_equal(X_transformed[:, -1],\n                       select.fit_transform(X, y).ravel())\n    assert_array_almost_equal(X_fit_transformed[:, :-1],\n                              10 * pca.fit_transform(X))\n    assert_array_equal(X_fit_transformed[:, -1],\n                       select.fit_transform(X, y).ravel())\n\n    # test with invalid transformer weights\n    with assert_raises(ValueError):\n        FeatureUnion([(\"pca\", pca), (\"select\", select)],\n                     transformer_weights={\"wrong_key\": 10})\n", "def test_pipeline_memory_cachedir_as_string():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n    try:\n        if LooseVersion(joblib_version) < LooseVersion('0.12'):\n            # Deal with change of API in joblib\n            memory = Memory(cachedir=cachedir, verbose=10)\n        else:\n            memory = Memory(location=cachedir, verbose=10)\n        # Test with Transformer + SVC\n        clf = SVC(gamma='scale', probability=True, random_state=0)\n        transf = DummyTransf()\n        pipe = Pipeline([('transf', clone(transf)), ('svc', clf)],\n                        memory=memory)\n        pipe.fit(X, y)\n        # Create a new pipeline with cachedir as string\n        cached_pipe = Pipeline([('transf', transf), ('svc', clf)],\n                               memory=cachedir)\n        cached_pipe.fit(X, y)\n        # Check that cached_pipe and pipe yield identical results\n        assert_array_equal(pipe.predict(X), cached_pipe.predict(X))\n        assert_array_equal(pipe.predict_proba(X), cached_pipe.predict_proba(X))\n        assert_array_equal(pipe.predict_log_proba(X),\n                           cached_pipe.predict_log_proba(X))\n        assert_array_equal(pipe.score(X, y), cached_pipe.score(X, y))\n        assert_array_equal(pipe.named_steps['transf'].means_,\n                           cached_pipe.named_steps['transf'].means_)\n    finally:\n        shutil.rmtree(cachedir)\n", "def test_pipeline_fit_predict_nans():\n    # Test that pipeline.fit_predict raises an error when y contains NaNs\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    y_with_nan = y.copy()\n    y_with_nan[0] = np.nan\n\n    pipe = Pipeline([('transf', Transf()), ('clf', KMeans())])\n    assert_raise_message(\n        ValueError,\n        \"Input contains NaN, infinity or a value too large for dtype('float64').\",\n        pipe.fit_predict, X, y_with_nan\n    )\n", "def test_pipeline_with_sparse_input_and_transformer():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    transf = Transf()\n    clf = SVC(gamma='scale', probability=True, random_state=0)\n    pipe = Pipeline([('transf', transf), ('svc', clf)])\n    X_sparse = sparse.csr_matrix(X)\n\n    # Test fit\n    pipe.fit(X_sparse, y)\n    pipe.fit(X_sparse, y=None)\n\n    # Test predict\n    assert_array_equal(pipe.predict(X_sparse), pipe.predict(X))\n    assert_array_equal(pipe.predict_proba(X_sparse), pipe.predict_proba(X))\n    assert_array_equal(pipe.predict_log_proba(X_sparse), pipe.predict_log_proba(X))\n    assert_array_equal(pipe.decision_function(X_sparse), pipe.decision_function(X))\n\n    # Test transform\n    assert_array_equal(pipe.transform(X_sparse).toarray(), pipe.transform(X))\n\n    # Test inverse_transform\n    assert_array_equal(pipe.inverse_transform(pipe.transform(X_sparse)).toarray(),\n                       pipe.inverse_transform(pipe.transform(X)))\n\n    # Test fit_predict\n    assert_array_equal(pipe.fit_predict(X_sparse), pipe.fit_predict(X))\n"], "sample_1089": ["def test_monotonic_sign_issue_18662():\n    F = _monotonic_sign\n    x = symbols('x')\n    assert F(x**2 - 1) is None\n    p = Dummy(positive=True)\n    assert F(p**2 - 1) == 1\n    n = Dummy(negative=True)\n    assert F(n**2 - 1) == -1\n", "def test__monotonic_sign():\n    F = _monotonic_sign\n    assert F(2) == 2\n    assert F(-2) == -2\n    assert F(3.5) == 2\n    assert F(-3.5) == -2\n    assert F(I) is None\n    assert F(-I) is None\n    assert F(Dummy(integer=True)) == S.Zero\n    assert F(Dummy(nonnegative=True)) == 0\n    assert F(Dummy(nonpositive=True)) == 0\n    assert F(Dummy(prime=True)) == 2\n    assert F(Dummy(prime=True, odd=True)) == 3\n    assert F(Dummy(composite=True)) == 4\n    assert F(Dummy(composite=True, odd=True)) == 9\n    assert F(Dummy(positive=True, integer=True)) == 1\n    assert F(Dummy(positive=True, even=True)) == 2\n    assert F(Dummy(positive=True, even=True, prime=False)) == 4\n    assert F(Dummy(negative=True, integer=True)) == -1\n    assert F(Dummy(negative=True, even=True)) == -2\n    assert F(Dummy(zero=True)) == 0\n    assert F(Dummy(nonnegative=True)) == 0\n    assert F(Dummy(nonpositive=True)) == 0\n    assert F(Dummy(real=True)) is None\n    assert F(Dummy(extended_real=True)) is None\n    assert F(Dummy(positive=True, extended_real=True)) == 2\n    assert F(Dummy(negative=True, extended_real=True)) == -2\n", "def test_mask_nc():\n    from sympy import symbols\n    from sympy.physics.secondquant import Commutator\n    from sympy.core.exprtools import _mask_nc\n\n    # Multiple nc-symbols:\n    A, B = symbols('A:B', commutative=False)\n    eq = A**2 - B**2\n    assert _mask_nc(eq) == (A**2 - B**2, {}, [A, B])\n\n    # An nc-object with nc-symbols but no others outside of it:\n    eq = 1 + symbols('x')*Commutator(A, B)\n    assert _mask_nc(eq) == (_mask_nc._mask[0]**2 + symbols('x')*_mask_nc._mask[0], {_mask_nc._mask[0]: Commutator(A, B)}, [])\n\n    # Multiple nc-objects:\n    eq = symbols('x')*Commutator(A, B) + symbols('x')*Commutator(A, symbols('C', commutative=False))\n    assert _mask_nc(eq) == (symbols('x')*_mask_nc._mask[0] + symbols('x')*_mask_nc._mask[1]*_mask_nc._mask[0], {_mask_nc._mask[0]: Commutator(A, B), _mask_nc._mask[1]: Commutator(A, symbols('C'))}, [_mask_nc._mask[0], _mask_nc._mask[1]])\n\n    # Multiple nc-objects and nc-symbols:\n    eq = A*Commutator(A, B) + symbols('B', commutative=False)*Commutator(A, symbols('C'))\n    assert _mask_nc(eq) == (A*_mask_nc._mask[0] + B*_mask_nc._mask[1], {_mask_nc._mask[0]: Commutator(A, B), _mask_nc._mask[1]: Commutator(A, symbols('C'))}, [A, B])\n\n    # issue 10355\n    eq = 1/(Commutator(A, B)*Commutator(symbols('C'), symbols('D')))\n    assert _mask_nc(eq) == (1/(_mask_nc._mask[0]*_mask_nc._mask[1]), {_mask_nc._mask[0]: Commutator(A, B), _mask_nc._mask[1]: Commutator(symbols('C'), symbols('D'))}, [])\n\n", "def test_issue_factor_terms_powers():\n    # Test factor_terms with powers\n    x, y = symbols('x y', real=True)\n    A, B = symbols('A B', commutative=False)\n\n    # Test simple cases\n    assert factor_terms(x**2 + x**2*y) == x**2*(1 + y)\n    assert factor_terms(x**3 + x**3*y) == x**3*(1 + y)\n    assert factor_terms(x**2*y + y) == y*(x**2 + 1)\n\n    # Test with negative powers\n    assert factor_terms(1/x**2 + 1/x**2*y) == 1/x**2*(1 + y)\n    assert factor_terms(1/x**3 + 1/x**3*y) == 1/x**3*(1 + y)\n    assert factor_terms(1/y*x**2 + 1/y) == 1/y*(x**2 + 1)\n\n    # Test with non-commutative symbols\n    assert factor_terms(A**2 + A**2*B) == A**2*(1 + B)\n    assert factor_terms(A**3 + A**3*B) == A**3*(1 + B)\n    assert factor_terms(B*A**2 + B) == B*(A**2 + 1)\n\n    # Test with mixed terms\n    assert factor_terms(x**2 + y**2 + x**2*y + y**2*x) == (x + y)**2*(x + y)\n    assert factor_terms(x**2*A + x**2*A*B + y**2*A + y**2*A*B) == (x**2 + y**2)*A*(1 + B)\n", "def test_factor_nc_univariate_polynomial():\n    x = symbols('x')\n    p = symbols('p', positive=True, integer=True)\n    n = symbols('n', negative=True, integer=True)\n    i = symbols('i', integer=True)\n\n    # issue #17664\n    assert factor_nc(x + p*x**3) == x*(1 + p*x**2)\n    assert factor_nc(x + n*x**3) == x*(1 + n*x**2)\n    assert factor_nc(x + i*x**3) == x + i*x**3\n\n    # issue #17725\n    assert factor_nc(x**4 + x**3 + x**2) == x**2*(x**2 + x + 1)\n    assert factor_nc(x**4 + x**2) == x**2*(x**2 + 1)\n    assert factor_nc(x**5 + x**4 + x**3) == x**3*(x**2 + x + 1)\n    assert factor_nc(x**4 + x**2 + 1) == x**2*(x**2 + 1) + 1\n\n    # additional tests\n    assert factor_nc(x**3 + x**2 + x) == x*(x**2 + x + 1)\n    assert factor_nc(x**6 + x**3) == x**3*(x**3 + 1)\n    assert factor_nc(x**6 + x**2) == x**2*(x**4 + 1)\n\n    assert factor_nc(x**7 + x**5 + x**3 + x) == x*(x**6 + x**4 + x**2 + 1)\n    assert factor_nc(x**8 + x**6 + x**4 + x**2) == x**2*(x**6 + x**4 + x**2 + 1)\n    assert factor_nc(x**9 + x**7 + x**5 + x**3) == x**3*(x**6 + x**4 + x**2 + 1)\n"], "sample_647": ["def test_unformatted_warning_format():\n    warning = warning_types.UnformattedWarning(\n        category=warning_types.PytestWarning, template=\"Test warning with {key}\"\n    )\n    formatted_warning = warning.format(key=\"value\")\n    assert isinstance(formatted_warning, warning_types.PytestWarning)\n    assert str(formatted_warning) == \"Test warning with value\"\n\n", "def test_unformatted_warning_format():\n    \"\"\"Test the UnformattedWarning class.\"\"\"\n    warning = warning_types.UnformattedWarning(\n        category=warning_types.PytestWarning, template=\"Test warning: {message}\"\n    )\n    formatted_warning = warning.format(message=\"This is a test warning\")\n    assert isinstance(formatted_warning, warning_types.PytestWarning)\n    assert str(formatted_warning) == \"Test warning: This is a test warning\"\n\n", "def test_UnformattedWarning_format():\n    \"\"\"Test UnformattedWarning class and its format method.\"\"\"\n    warning = warning_types.UnformattedWarning(\n        category=warning_types.PytestWarning, template=\"Test warning: {message}\"\n    )\n    formatted_warning = warning.format(message=\"Test message\")\n    assert isinstance(formatted_warning, warning_types.PytestWarning)\n    assert str(formatted_warning) == \"Test warning: Test message\"\n\n", "def test_pytest_deprecation_warnings(warning_class: UserWarning) -> None:\n    \"\"\"Make sure all deprecation warnings declared in _pytest.warning_types are displayed as coming\n    from 'pytest' instead of the internal module (#5452).\n    \"\"\"\n    assert warning_class.__module__ == \"pytest\"\n    with pytest.warns(warning_class) as record:\n        warning_types.warn_explicit_for(\n            pytest.raises, warning_types.PytestWarning(\"test\")  # type: ignore\n        )\n    assert len(record) == 1\n    assert record[0].category == warning_class\n\n", "def test_unformatted_warning_formatting(\n    category: Type[PytestWarning], template: str, kwargs: dict, expected_message: str"], "sample_359": ["def test_rename_field_with_expression_index(self):\n    app_label = 'test_rename_with_expr_index'\n    index_name = f'{app_label}_pony_idx'\n    table_name = f'{app_label}_pony'\n    project_state = self.set_up_test_model(\n        app_label,\n        indexes=[models.Index(Abs('pink'), name=index_name)],\n    )\n    self.assertTableExists(table_name)\n    self.assertIndexNameExists(table_name, index_name)\n    new_state = project_state.clone()\n    operation = migrations.RenameField('Pony', 'pink', 'blue')\n    operation.state_forwards(app_label, new_state)\n    with connection.schema_editor() as editor:\n        operation.database_forwards(app_label, editor, project_state, new_state)\n    if connection.features.supports_expression_indexes:\n        self.assertIndexNameExists(table_name, index_name)\n    else:\n        self.assertIndexNameNotExists(table_name, index_name)\n    with connection.schema_editor() as editor:\n        operation.database_backwards(app_label, editor, new_state, project_state)\n    if connection.features.supports_expression_indexes:\n        self.assertIndexNameExists(table_name, index_name)\n    else:\n        self.assertIndexNameNotExists(table_name, index_name)\n", "    def test_alter_field_type_choices(self):\n        \"\"\"\n        Tests the AlterField operation with field type choices.\n        \"\"\"\n        project_state = self.set_up_test_model(\"test_alfltc\")\n        # Test the state alteration\n        operation = migrations.AlterField(\"Pony\", \"pink\", models.IntegerField(choices=[(1, \"Pink\"), (2, \"Not Pink\")]))\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_alfltc\", new_state)\n        self.assertIsInstance(\n            new_state.models[\"test_alfltc\", \"pony\"].fields[\"pink\"].choices,\n            list,\n        )\n        self.assertEqual(new_state.models[\"test_alfltc\", \"pony\"].fields[\"pink\"].choices, [(1, \"Pink\"), (2, \"Not Pink\")])\n        # Test the database alteration\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_alfltc\", editor, project_state, new_state)\n        # Test deconstruction\n        definition = operation.deconstruct()\n        self.assertEqual(definition[0], \"AlterField\")\n        self.assertEqual(definition[1], [])\n        self.assertEqual(sorted(definition[2]), [\"field\", \"model_name\", \"name\"])\n", "def test_alter_field_unique_constraint(self):\n    \"\"\"\n    Tests that AlterField keeps the unique constraint after changes.\n    \"\"\"\n    project_state = self.set_up_test_model(\"test_alfluc\")\n\n    project_state = self.apply_operations(\"test_alfluc\", project_state, operations=[\n        migrations.CreateModel(\"Pony\", fields=[\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"unique_field\", models.CharField(max_length=100, unique=True)),\n        ]),\n    ])\n\n    operation = migrations.AlterField(\"Pony\", \"unique_field\", models.CharField(max_length=200, unique=True))\n\n    new_state = project_state.clone()\n    operation.state_forwards(\"test_alfluc\", new_state)\n\n    Pony = new_state.apps.get_model(\"test_alfluc\", \"Pony\")\n    self.assertEqual(len(Pony._meta.get_field(\"unique_field\").constraints), 1)\n\n    with connection.schema_editor() as editor:\n        operation.database_forwards(\"test_alfluc\", editor, project_state, new_state)\n\n    Pony = new_state.apps.get_model(\"test_alfluc\", \"Pony\")\n    self.assertEqual(len(Pony._meta.get_field(\"unique_field\").constraints), 1)\n    with self.assertRaises(IntegrityError), transaction.atomic():\n        Pony.objects.create(unique_field=\"Ponies\")\n        Pony.objects.create(unique_field=\"Ponies\")\n\n    with connection.schema_editor() as editor:\n        operation.database_backwards(\"test_alfluc\", editor, new_state, project_state)\n\n    Pony = project_state.apps.get_model(\"test_alfluc\", \"Pony\")\n    self.assertEqual(len(Pony._meta.get_field(\"unique_field\").constraints), 1)\n    with self.assertRaises(IntegrityError), transaction.atomic():\n        Pony.objects.create(unique_field=\"Ponies\")\n        Pony.objects.create(unique_field=\"Ponies\")\n", "def test_reduceRenameModelOnCreateModelWithFK(self):\n    \"\"\"\n    Renaming a model with a CreateModel operation that contains a ForeignKey\n    to it should correctly replace the old name with the new name.\n    \"\"\"\n    project_state = ProjectState()\n    operation1 = migrations.CreateModel(\n        \"Pony\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"pink\", models.IntegerField(default=1)),\n            (\"friend\", models.ForeignKey(\"Pony\", models.CASCADE)),\n        ],\n    )\n    operation2 = migrations.RenameModel(\"Pony\", \"Horse\")\n    new_state = project_state.clone()\n    operation1.state_forwards(\"test_rmcponfk\", new_state)\n    reduced_operations = operation2.reduce(operation1, \"test_rmcponfk\")\n    self.assertEqual(len(reduced_operations), 1)\n    self.assertIsInstance(reduced_operations[0], migrations.CreateModel)\n    self.assertEqual(reduced_operations[0].name, \"Horse\")\n    self.assertEqual(reduced_operations[0].fields[2][1].remote_field.model, \"test_rmcponfk.Horse\")\n", "def test_create_model_with_functional_unique_constraint(self):\n    app_label = 'test_create_model_with_functional_unique_constraint'\n    table_name = f'{app_label}_pony'\n    unique_constraint_name = f'{app_label}_pony_unique'\n    unique_constraint_name_lower = f'{app_label}_pony_unique'.lower()\n    project_state = self.set_up_test_model(app_label)\n    constraint = models.UniqueConstraint(models.F('weight').desc(nulls_last=True), name=unique_constraint_name)\n    operation = migrations.CreateModel(\n        \"Pony\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"weight\", models.IntegerField()),\n        ],\n        options={'constraints': [constraint]},\n    )\n    self.assertEqual(\n        operation.describe(),\n        'Create model Pony with constraint %s' % unique_constraint_name,\n    )\n    new_state = project_state.clone()\n    operation.state_forwards(app_label, new_state)\n    self.assertEqual(len(new_state.models[app_label, 'pony'].options['constraints']), 1)\n    Pony = new_state.apps.get_model(app_label, 'Pony')\n    self.assertEqual(len(Pony._meta.constraints), 1)\n    # Create table.\n    with connection.schema_editor() as editor:\n        operation.database_forwards(app_label, editor, project_state, new_state)\n    Pony.objects.create(weight=4, pink=False)\n    Pony.objects.create(weight=5, pink=True)\n    # Test constraint works\n    with self.assertRaises(IntegrityError):\n        Pony.objects.create(weight=5, pink=True)\n    # Reversal.\n    with connection.schema_editor() as editor:\n        operation.database_backwards(app_label, editor, new_state, project_state)\n    # Deconstruction.\n    definition = operation.deconstruct()\n    self.assertEqual(definition[0], 'CreateModel')\n    self.assertEqual(definition[1], [])\n    self.assertEqual(definition[2]['options']['constraints'], [constraint])\n    # Test name of the constraint has been stored in the state and apps.\n    self.assertEqual(\n        new_state.models[app_label, 'pony'].options['constraints'][0].name,\n        unique_constraint_name_lower,\n    )\n    self.assertEqual(\n        new_state.apps.get_model(app_label, 'Pony')._meta.constraints[0].name,\n        unique_constraint_name_lower,\n    )\n    # Index name is named after the constraint name\n    self.assertIndexNameExists(table_name, unique_constraint_name_lower)\n"], "sample_14": ["def test_longitude_wrap_angle_edge_cases():\n    # Test edge cases for Longitude wrapping\n    lon = Longitude(180*u.deg)\n    lon.wrap_angle = 0*u.deg\n    assert lon.value == 0*u.deg\n\n    lon = Longitude(360*u.deg)\n    lon.wrap_angle = 0*u.deg\n    assert lon.value == 0*u.deg\n\n    lon = Longitude(-180*u.deg)\n    lon.wrap_angle = 0*u.deg\n    assert lon.value == 0*u.deg\n\n    lon = Longitude(-360*u.deg)\n    lon.wrap_angle = 0*u.deg\n    assert lon.value == 0*u.deg\n\n    lon = Longitude(180*u.deg, wrap_angle=0*u.deg)\n    assert lon.value == 0*u.deg\n\n    lon = Longitude(360*u.deg, wrap_angle=0*u.deg)\n    assert lon.value == 0*u.deg\n\n    lon = Longitude(-180*u.deg, wrap_angle=0*u.deg)\n    assert lon.value == 0*u.deg\n\n    lon = Longitude(-360*u.deg, wrap_angle=0*u.deg)\n    assert lon.value == 0*u.deg\n", "def test_longitude_wrap_angle_type():\n    \"\"\"\n    Test that Longitude wrap_angle always stays an Angle\n    \"\"\"\n\n    lon = Longitude(0 * u.deg)\n    assert type(lon.wrap_angle) is Angle\n\n    lon.wrap_angle = '180d'\n    assert type(lon.wrap_angle) is Angle\n\n    lon.wrap_angle = 180 * u.deg\n    assert type(lon.wrap_angle) is Angle\n\n    lon.wrap_angle = Longitude(180 * u.deg)\n    assert type(lon.wrap_angle) is Angle\n    assert not isinstance(lon.wrap_angle, Longitude)\n", "def test_longitude_wrap_at_limits():\n    \"\"\"\n    Test that Longitude wrapping works correctly at the limits (e.g. just\n    below or above 360 degrees).\n    \"\"\"\n    lon = Longitude(np.array([359.999, 360, 360.001]) * u.deg)\n    wrapped = lon.wrap_at(360 * u.deg)\n    assert np.allclose(wrapped.degree, np.array([-0.001, 0, 0.001]))\n\n    lon = Longitude(np.array([359.999, 360, 360.001]) * u.deg)\n    wrapped = lon.wrap_at('180d')\n    assert np.allclose(wrapped.degree, np.array([-180.001, -180, -179.999]))\n", "def test_latitude_wrap_at():\n    \"\"\"\n    Test wrapping Latitude angles to a specified wrap angle\n    \"\"\"\n    lat = Latitude([89.9, -89.9] * u.deg)\n    wrapped_lat = lat.wrap_at(90 * u.deg)\n    assert np.allclose(wrapped_lat.degree, [-89.9, 89.9])\n\n    lat = Latitude([90.1, -90.1] * u.deg)\n    with pytest.raises(ValueError):\n        lat.wrap_at(90 * u.deg)\n\n    lat = Latitude([89.9, -89.9] * u.deg)\n    wrapped_lat = lat.wrap_at(360 * u.deg)\n    assert np.allclose(wrapped_lat.degree, [-89.9, -270.1])\n\n    with pytest.raises(ValueError):\n        lat.wrap_at(-90 * u.deg)\n\n    with pytest.raises(ValueError):\n        lat.wrap_at(91 * u.deg)\n", "def test_longitude_wrap_angle_setting(cls, input, expected, exception):\n    \"\"\"\n    Test that setting Longitude.wrap_angle to an invalid value raises an exception\n    \"\"\"\n    if exception:\n        with pytest.raises(exception):\n            input.wrap_angle = expected\n    else:\n        input.wrap_angle = expected\n        assert input.wrap_angle == expected\n"], "sample_465": ["    def test_to_field_allowed(self):\n        class BandAdmin(ModelAdmin):\n            pass\n\n        ma = BandAdmin(Band, self.site)\n        self.assertIs(ma.to_field_allowed(request, \"id\"), True)\n        self.assertIs(ma.to_field_allowed(request, \"name\"), True)\n        self.assertIs(ma.to_field_allowed(request, \"bio\"), False)\n", "def test_to_field_allowed(self):\n    \"\"\"\n    to_field_allowed() should allow referencing the primary key by default.\n    \"\"\"\n\n    ma = ModelAdmin(Band, self.site)\n    self.assertIs(ma.to_field_allowed(request, \"id\"), True)\n\n    class BandAdmin(ModelAdmin):\n            return to_field == \"name\"\n\n    ma = BandAdmin(Band, self.site)\n    self.assertIs(ma.to_field_allowed(request, \"name\"), True)\n    self.assertIs(ma.to_field_allowed(request, \"id\"), False)\n\n    class BandAdmin(ModelAdmin):\n            return False\n\n    ma = BandAdmin(Band, self.site)\n    self.assertIs(ma.to_field_allowed(request, \"name\"), False)\n    self.assertIs(ma.to_field_allowed(request, \"id\"), False)\n", "def test_to_field_allowed(self):\n    \"\"\"\n    to_field_allowed returns True for fields that the model associated with\n    this admin should be allowed to be referenced by the specified field.\n    \"\"\"\n    ma = ModelAdmin(Band, self.site)\n    self.assertIs(ma.to_field_allowed(request, \"id\"), True)\n    self.assertIs(ma.to_field_allowed(request, \"name\"), True)\n    self.assertIs(ma.to_field_allowed(request, \"bio\"), True)\n    self.assertIs(ma.to_field_allowed(request, \"sign_date\"), True)\n    self.assertIs(ma.to_field_allowed(request, \"nonexistent\"), False)\n\n    # Reverse OneToOneField\n    class SongAdmin(ModelAdmin):\n        model = Song\n\n    ma = SongAdmin(Song, self.site)\n    self.assertIs(ma.to_field_allowed(request, \"featuring\"), True)\n    self.assertIs(ma.to_field_allowed(request, \"band\"), False)\n\n    # Forward OneToOneField\n    class SongAdmin(ModelAdmin):\n        model = Band\n\n    ma = SongAdmin(Band, self.site)\n    self.assertIs(ma.to_field_allowed(request, \"featuring\"), False)\n\n    # ForeignKey\n    class ConcertAdmin(ModelAdmin):\n        model = Concert\n\n    ma = ConcertAdmin(Concert, self.site)\n    self.assertIs(ma.to_field_allowed(request, \"main_band\"), True)\n    self.assertIs(ma.to_field_allowed(request, \"opening_band\"), True)\n    self.assertIs(ma.to_field_allowed(request, \"day\"), False)\n    self.assertIs(ma.to_field_allowed(request, \"transport\"), False)\n\n    # Forward ForeignKey\n    class BandAdmin(ModelAdmin):\n        model = Band\n\n    ma = BandAdmin(Band, self.site)\n    self.assertIs(ma.to_field_allowed(request, \"main_band\"), False)\n    self.assertIs(ma.to_field_allowed(request, \"opening_band\"), False)\n\n    # ManyToManyField\n    class BandAdmin(ModelAdmin):\n        model = Band\n\n    ma = BandAdmin(Band, self.site)\n    self.assertIs(ma.to_field_allowed(request, \"members\"), False)\n\n    # Reverse OneToMany\n    class ConcertAdmin(ModelAdmin):\n        model = Concert\n\n    ma = ConcertAdmin(Concert, self.site)\n    self.assertIs(ma.to_field_allowed(request, \"members\"), False)\n", "    def test_get_field_queryset(self):\n        # Test that get_field_queryset returns a queryset ordered by the related\n        # model's get_ordering() when specified.\n        class BandAdmin(ModelAdmin):\n            pass\n\n        self.site.register(Band, BandAdmin)\n\n        class SongAdmin(ModelAdmin):\n            list_display = [\"band\"]\n\n        self.site.register(Song, SongAdmin)\n\n        ma = SongAdmin(Song, self.site)\n        db_field = Song._meta.get_field(\"band\")\n        qs = ma.get_field_queryset(\"default\", db_field, request)\n        self.assertEqual(qs.query.order_by, (\"name\",))\n\n        # Test that get_field_queryset returns None when the related model\n        # doesn't have a ModelAdmin.\n        class NonAdminModel(models.Model):\n            pass\n\n        class NonAdminModelAdmin(ModelAdmin):\n            pass\n\n        self.site.register(NonAdminModel, NonAdminModelAdmin)\n\n        class NonAdminModel2(models.Model):\n            fk = models.ForeignKey(NonAdminModel, models.CASCADE)\n\n        ma = ModelAdmin(NonAdminModel2, self.site)\n        db_field = NonAdminModel2._meta.get_field(\"fk\")\n        qs = ma.get_field_queryset(\"default\", db_field, request)\n        self.assertIsNone(qs)\n\n        # Test that get_field_queryset returns None when the related model's\n        # ModelAdmin doesn't specify get_ordering().\n        class ModelWithoutOrderingAdmin(ModelAdmin):\n            pass\n\n        self.site.register(NonAdminModel, ModelWithoutOrderingAdmin)\n\n        ma = ModelAdmin(NonAdminModel2, self.site)\n        db_field = NonAdminModel2._meta.get_field(\"fk\")\n        qs = ma.get_field_queryset(\"default\", db_field, request)\n        self.assertIsNone(qs)\n", "def test_render_change_form(self):\n    ma = ModelAdmin(Band, self.site)\n    # Check that render_change_form() doesn't overwrite the 'adminform' context\n    # variable (#12206).\n    context = {\n        \"adminform\": \"Custom admin form\",\n        \"inline_admin_formsets\": [],\n    }\n    with self.assertRaises(KeyError):\n        ma.render_change_form(request, context, add=True)\n    context[\"adminform\"] = helpers.AdminForm(\n        ma.get_form(request)(), [(None, {\"fields\": [\"name\"]})], {}, [], ma\n    )\n    rendered = ma.render_change_form(request, context, add=True)\n    self.assertIn(\"Custom admin form\", rendered.rendered_content)\n    self.assertIn(\"name\", rendered.rendered_content)\n"], "sample_273": ["    def test_field_name_clash(self):\n        class Parent(models.Model):\n            field1 = models.CharField(max_length=10)\n\n        class Child(Parent):\n            field1 = models.CharField(max_length=10)\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Error(\n                \"The field 'field1' from parent model 'check_framework.Parent' \"\n                \"clashes with the field 'field1' from parent model \"\n                \"'check_framework.Parent'.\",\n                obj=Child,\n                id='models.E005',\n            )\n        ])\n", "    def test_unique_error_message(self):\n        class Model(models.Model):\n            field1 = models.CharField(max_length=10, unique=True)\n            field2 = models.CharField(max_length=10)\n\n            class Meta:\n                unique_together = (('field1', 'field2'),)\n\n                if len(unique_check) == 1:\n                    return 'Unique error message for field %s' % unique_check[0]\n                else:\n                    return 'Unique error message for fields %s and %s' % tuple(unique_check)\n\n        instance = Model(field1='value1', field2='value2')\n        with self.assertRaises(ValidationError) as e:\n            instance.validate_unique()\n        self.assertEqual(len(e.exception.error_list), 1)\n        self.assertEqual(len(e.exception.error_list[0]), 1)\n        self.assertEqual(e.exception.error_list[0][0].message, 'Unique error message for field field1')\n\n        instance = Model(field1='value1')\n        with self.assertRaises(ValidationError) as e:\n            instance.validate_unique(exclude=['field1'])\n        self.assertEqual(len(e.exception.error_list), 1)\n        self.assertEqual(len(e.exception.error_list[0]), 1)\n        self.assertEqual(e.exception.error_list[0][0].message, 'Unique error message for fields field1 and field2')\n\n        instance = Model(field1='value1', field2='value2')\n        with self.assertRaises(ValidationError) as e:\n            instance.validate_unique(exclude=['field2'])\n        self.assertEqual(len(e.exception.error_list), 1)\n        self.assertEqual(len(e.exception.error_list[0]), 1)\n        self.assertEqual(e.exception.error_list[0][0].message, 'Unique error message for fields field1 and field2')\n", "    def test_model_name_db_lookup_clashes(self):\n        class Model(models.Model):\n            class Meta:\n                db_table = 'test_table'\n        Model._meta.model_name = '_Model'\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Error(\n                \"The model name '_Model' cannot start or end with an underscore \"\n                \"as it collides with the query lookup syntax.\",\n                obj=Model,\n                id='models.E023'\n            )\n        ])\n\n        Model._meta.model_name = 'Model_'\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Error(\n                \"The model name 'Model_' cannot start or end with an underscore \"\n                \"as it collides with the query lookup syntax.\",\n                obj=Model,\n                id='models.E023'\n            )\n        ])\n\n        Model._meta.model_name = 'Model_Model'\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Error(\n                \"The model name 'Model_Model' cannot contain double underscores as \"\n                \"it collides with the query lookup syntax.\",\n                obj=Model,\n                id='models.E024'\n            )\n        ])\n\n        Model._meta.model_name = 'MyModel'\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [])\n", "    def test_ordering_with_respect_to_and_ordering(self):\n        class Model(models.Model):\n            class Meta:\n                ordering = ['id']\n                order_with_respect_to = 'field'\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Error(\n                \"'ordering' and 'order_with_respect_to' cannot be used together.\",\n                obj=Model,\n                id='models.E021',\n            )\n        ])\n", "    def test_abstract_models(self):\n        class AbstractModel(models.Model):\n            class Meta:\n                abstract = True\n\n        class ChildModel(AbstractModel):\n            pass\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [])\n"], "sample_1050": ["def test_NumPyPrinter_print_relatational():\n    n = NumPyPrinter()\n    assert n.doprint(Eq(x, y)) == 'numpy.equal(x, y)'\n    assert n.doprint(x < y) == 'numpy.less(x, y)'\n    assert n.doprint(x <= y) == 'numpy.less_equal(x, y)'\n    assert n.doprint(x > y) == 'numpy.greater(x, y)'\n    assert n.doprint(x >= y) == 'numpy.greater_equal(x, y)'\n", "def test_AbstractPythonCodePrinter_relational():\n    prntr = AbstractPythonCodePrinter()\n    assert prntr._print_Relational(Eq(x, y)) == '(x == y)'\n    assert prntr._print_Relational(Le(x, y)) == '(x <= y)'\n    assert prntr._print_Relational(Gt(x, y)) == '(x > y)'\n    assert prntr._print_Relational(Eq(x, y + 2)) == '(x == (y + 2))'\n    assert prntr._print_Relational(Le(x**2, y)) == '((x**2) <= y)'\n", "def test_NumPyPrinter_print_Piecewise():\n    n = NumPyPrinter()\n\n    pw = Piecewise((x, Eq(x, 0)), (y, Eq(x, 1)), (z, Eq(x, 2)), (None, True))\n    expected = \"numpy.select([x == 0, x == 1, x == 2], [x, y, z], default=numpy.nan)\"\n    assert n.doprint(pw) == expected\n\n    pw = Piecewise((x, Eq(x, 0)), (y, Eq(x, 1)))\n    expected = \"numpy.select([x == 0, x == 1], [x, y], default=numpy.nan)\"\n    assert n.doprint(pw) == expected\n\n    pw = Piecewise((x, Eq(x, 0)))\n    expected = \"numpy.select([x == 0], [x], default=numpy.nan)\"\n    assert n.doprint(pw) == expected\n", "def test_Piecewise_printing():\n    x = symbols('x')\n    expr = Piecewise((x**2, x < 0), (x, Eq(x, 0)), (x**3, x > 0), (x**4, True))\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == '((x**2) if (x < 0) else (x) if (x == 0) else (x**3) if (x > 0) else (x**4))'\n", "def test_PythonCodePrinter_piecewise_defaults():\n    prntr = PythonCodePrinter()\n    expr = Piecewise((1, Eq(x, 0)), (2, True))\n    assert prntr.doprint(expr) == '((1) if (x == 0) else (2))'\n"], "sample_793": ["def test_iforest_predict():\n    \"\"\"Check Isolation Forest predict method\"\"\"\n    X_train = np.array([[0, 1], [1, 2]])\n    X_test = np.array([[2, 1], [1, 1]])\n\n    clf = IsolationForest(random_state=rng).fit(X_train)\n    y_pred = clf.predict(X_test)\n    assert_array_equal(y_pred, [1, 1])\n\n    clf = IsolationForest(contamination=0.5, random_state=rng).fit(X_train)\n    y_pred = clf.predict(X_test)\n    assert_array_equal(y_pred, [-1, -1])\n\n    clf = IsolationForest(contamination=\"auto\", random_state=rng).fit(X_train)\n    y_pred = clf.predict(X_test)\n    assert_array_equal(y_pred, [1, 1])\n", "def test_iforest_n_estimators(n_estimators):\n    \"\"\"Test Isolation Forest with varying number of estimators.\"\"\"\n    X = iris.data\n    clf = IsolationForest(n_estimators=n_estimators)\n    clf.fit(X)\n    assert len(clf.estimators_) == n_estimators\n    assert_array_almost_equal(clf.score_samples(X), clf.decision_function(X) + clf.offset_)\n", "def test_iforest_contamination_parameter():\n    X_train = [[1, 1], [1, 2], [2, 1], [100, 100]]\n    contamination_values = [0.1, 0.3, 0.5, 'auto']\n\n    for contamination in contamination_values:\n        clf = IsolationForest(contamination=contamination).fit(X_train)\n        assert clf._contamination == contamination if contamination != 'auto' else 0.1\n", "def test_iforest_offset_calculation():\n    \"\"\"Check that offset_ is correctly calculated.\"\"\"\n    X_train = [[1, 1], [1, 2], [2, 1]]\n    X_test = [[2., 2.]]\n    \n    # Test with contamination as a float\n    clf1 = IsolationForest(contamination=0.1).fit(X_train)\n    scores = clf1.score_samples(X_train)\n    assert_allclose(clf1.offset_, np.percentile(scores, 10), atol=1e-6)\n    \n    # Test with contamination as 'auto'\n    clf2 = IsolationForest(contamination='auto').fit(X_train)\n    assert_allclose(clf2.offset_, -0.5, atol=1e-6)\n    \n    # Test with behaviour='old'\n    clf3 = IsolationForest(contamination=0.1, behaviour='old').fit(X_train)\n    assert_allclose(clf3.offset_, -0.5, atol=1e-6)\n", "def test_iforest_n_features_1(contamination):\n    \"\"\"Test if n_features_ is set correctly when n_features is 1.\"\"\"\n    X = [[1], [1], [2], [6]]\n    clf = IsolationForest(contamination=contamination).fit(X)\n    assert clf.n_features_ == 1\n"], "sample_52": ["    def setUp(self):\n        self.author = Writer.objects.create(name='Test writer')\n", "def test_deepcopy_modelchoicefield(self):\n    class ModelChoiceForm(forms.Form):\n        category = forms.ModelChoiceField(Category.objects.all())\n\n    form1 = ModelChoiceForm()\n    field1 = form1.fields['category']\n\n    # Test that deep copying the form doesn't share the same queryset\n    # instance as the original form.\n    form2 = copy.deepcopy(form1)\n    field2 = form2.fields['category']\n    self.assertIsNot(field1, field2)\n    self.assertIsNot(field1.queryset, field2.queryset)\n\n    # Test that a queryset change on one form does not affect the other.\n    field2.queryset = Category.objects.exclude(name='Entertainment')\n    self.assertCountEqual(field1.queryset, [self.c1, self.c2, self.c3])\n    self.assertCountEqual(field2.queryset, [self.c2, self.c3])\n", "def test_model_form_instance_gets_cleaned_data_from_model_fields(self):\n    class TestModelForm(forms.ModelForm):\n        class Meta:\n            model = Book\n            fields = ('title', 'author')\n\n    class TestModel:\n            self.title = title\n            self.author = author\n\n            return self.title == other.title and self.author == other.author\n\n    test_model = TestModel('Test Title', Writer.objects.create(name='Test Author'))\n    form = TestModelForm(instance=test_model)\n    self.assertTrue(form.is_valid())\n    self.assertEqual(form.cleaned_data['title'], 'Test Title')\n    self.assertEqual(form.cleaned_data['author'], test_model.author)\n", "    def test_model_multiple_choice_field_with_limit_choices_to(self):\n        class ModelMultipleChoiceForm(forms.Form):\n            categories = forms.ModelMultipleChoiceField(\n                Category.objects.all(),\n                limit_choices_to={'name__startswith': 'T'}\n            )\n\n        self.assertEqual(len(ModelMultipleChoiceForm().fields['categories'].queryset), 2)\n        self.assertEqual(len(ModelMultipleChoiceForm().fields['categories'].choices), 3)  # Including the empty label\n        self.assertEqual(\n            list(ModelMultipleChoiceForm().fields['categories'].queryset.values_list('name', flat=True)),\n            ['A test', 'Third']\n        )\n\n        # The limit_choices_to attribute is used to filter the queryset.\n        self.assertEqual(len(ModelMultipleChoiceForm().fields['categories'].choices), 3)\n        self.assertEqual(\n            list(ModelMultipleChoiceForm().fields['categories'].queryset.values_list('name', flat=True)),\n            ['A test', 'Third']\n        )\n\n        # If the limit_choices_to attribute is a callable, it's called.\n            return {'name__startswith': 'T'}\n\n        class ModelMultipleChoiceForm(forms.Form):\n            categories = forms.ModelMultipleChoiceField(\n                Category.objects.all(),\n                limit_choices_to=limit_choices\n            )\n\n        self.assertEqual(len(ModelMultipleChoiceForm().fields['categories'].queryset), 2)\n        self.assertEqual(len(ModelMultipleChoiceForm().fields['categories'].choices), 3)\n        self.assertEqual(\n            list(ModelMultipleChoiceForm().fields['categories'].queryset.values_list('name', flat=True)),\n            ['A test', 'Third']\n        )\n", "    def test_save_formset(self):\n        # test save formset method\n        class BookForm(forms.ModelForm):\n            class Meta:\n                model = Book\n                fields = ('title', 'author')\n\n        class BookFormSet(forms.BaseModelFormSet):\n                if commit:\n                    # If committing, save the instance and the m2m data immediately.\n                    return super().save_new(form, commit=commit)\n                else:\n                    return super().save_new(form, commit=commit)\n\n                if commit:\n                    # If committing, save the instance and the m2m data immediately.\n                    return super().save_existing(form, instance, commit=commit)\n                else:\n                    return super().save_existing(form, instance, commit=commit)\n\n        book_formset = modelformset_factory(Book, formset=BookFormSet, form=BookForm, extra=1)\n\n        # Create a book\n        book = Book.objects.create(title='Test Book', author=Writer.objects.create(name='Test writer'))\n\n        # Create a formset for the book\n        formset = book_formset(queryset=Book.objects.filter(title='Test Book'))\n\n        # Check that the formset is valid\n        self.assertTrue(formset.is_valid())\n\n        # Save the formset\n        saved_books = formset.save(commit=False)\n\n        # Check that the book instance was saved\n        self.assertEqual(len(saved_books), 1)\n\n        # Check that the formset's saved forms is empty when commit is False\n        self.assertEqual(len(formset.saved_forms), 0)\n\n        # Check that the book instance was saved correctly\n        self.assertEqual(saved_books[0].title, 'Test Book')\n        self.assertEqual(saved_books[0].author.name, 'Test writer')\n\n        # Save the book instance\n        saved_books[0].save()\n\n        # Check that the formset's saved forms is populated when commit is False\n        self.assertEqual(len(formset.saved_forms), 1)\n"], "sample_726": ["def test_label_binarize_multiclass_sparse():\n    y = [0, 1, 2]\n    classes = [0, 1, 2]\n    pos_label = 1\n    neg_label = 0\n    expected = np.eye(3)\n\n    # check label_binarize\n    binarized = label_binarize(y, classes, neg_label=neg_label,\n                               pos_label=pos_label,\n                               sparse_output=True)\n    assert_array_equal(binarized.toarray(), expected)\n    assert_true(issparse(binarized))\n\n    # check inverse\n    inversed = _inverse_binarize_multiclass(binarized, classes=classes)\n    assert_array_equal(inversed, y)\n\n    # Check label binarizer\n    lb = LabelBinarizer(neg_label=neg_label, pos_label=pos_label,\n                        sparse_output=True)\n    binarized = lb.fit_transform(y)\n    assert_array_equal(binarized.toarray(), expected)\n    assert_true(issparse(binarized))\n    inverse_output = lb.inverse_transform(binarized)\n    assert_array_equal(inverse_output, y)\n    assert_true(issparse(inverse_output))\n\n    # Check LabelBinarizer with single class\n    y = [1, 1, 1]\n    expected = np.zeros((3, 1))\n    binarized = lb.fit_transform(y)\n    assert_array_equal(binarized.toarray(), expected)\n    assert_true(issparse(binarized))\n    inverse_output = lb.inverse_transform(binarized)\n    assert_array_equal(inverse_output, y)\n    assert_true(issparse(inverse_output))\n", "def test_label_binarizer_non_numeric_labels():\n    # Test that LabelBinarizer works with non-numeric labels\n    y = ['a', 'b', 'c', 'a']\n    lb = LabelBinarizer()\n    expected = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0]])\n    assert_array_equal(lb.fit_transform(y), expected)\n    assert_array_equal(lb.inverse_transform(lb.transform(y)), y)\n\n    # Test that LabelBinarizer works with non-numeric labels and sparse_output=True\n    lb = LabelBinarizer(sparse_output=True)\n    assert_array_equal(lb.fit_transform(y).toarray(), expected)\n    assert_array_equal(lb.inverse_transform(lb.transform(y)), y)\n\n    # Test that LabelBinarizer works with non-numeric labels and neg_label, pos_label\n    lb = LabelBinarizer(neg_label=-1, pos_label=2)\n    expected = np.array([[-1, 0, 0], [0, -1, 0], [0, 0, -1], [-1, 0, 0]])\n    assert_array_equal(lb.fit_transform(y), expected)\n    assert_array_equal(lb.inverse_transform(lb.transform(y)), y)\n\n    # Test that LabelBinarizer works with non-numeric labels and neg_label, pos_label, sparse_output=True\n    lb = LabelBinarizer(neg_label=-1, pos_label=2, sparse_output=True)\n    assert_array_equal(lb.fit_transform(y).toarray(), expected)\n    assert_array_equal(lb.inverse_transform(lb.transform(y)), y)\n", "def test_label_binarizer_sparse_threshold():\n    # Test sparse thresholding\n    for sparse_matrix in [coo_matrix, csc_matrix, csr_matrix, dok_matrix,\n                          lil_matrix]:\n        y = np.array([[0.5, 0.3], [0.1, 0.9]])\n        lb = LabelBinarizer(threshold=0.6)\n        y_sparse = sparse_matrix(y)\n        got = lb.fit_transform(y_sparse)\n        assert_array_equal(got.toarray(), lb.fit_transform(y))\n        assert_equal(issparse(got), True)\n", "def test_label_binarizer_with_missing_values():\n    # Test that missing values are handled correctly\n    lb = LabelBinarizer(sparse_output=False)\n    inp = [0, 1, np.nan, 2, 2, np.nan]\n    expected = np.array([[0, 1, 0],\n                         [0, 0, 1],\n                         [0, 0, 0],\n                         [0, 0, 1],\n                         [0, 0, 1],\n                         [0, 0, 0]])\n    got = lb.fit_transform(inp)\n    assert_array_equal(expected, got)\n    assert_array_equal(lb.inverse_transform(got), inp)\n\n    # Test with sparse_output=True\n    lb = LabelBinarizer(sparse_output=True)\n    inp = [0, 1, np.nan, 2, 2, np.nan]\n    expected = np.array([[0, 1, 0],\n                         [0, 0, 1],\n                         [0, 0, 0],\n                         [0, 0, 1],\n                         [0, 0, 1],\n                         [0, 0, 0]])\n    got = lb.fit_transform(inp)\n    assert_array_equal(expected, got.toarray())\n    assert_array_equal(lb.inverse_transform(got.toarray()), inp)\n\n    # Test that missing values are handled correctly in inverse_transform\n    lb = LabelBinarizer(sparse_output=False)\n    inp = [0, 1, 2, 2]\n    expected = np.array([[0, 1, 0],\n                         [0, 0, 1],\n                         [0, 0, 1],\n                         [0, 0, 1]])\n    got = lb.fit_transform(inp)\n    assert_array_equal(lb.inverse_transform(got), inp)\n\n    # Test with sparse_output=True\n    lb = LabelBinarizer(sparse_output=True)\n    inp = [0, 1, 2, 2]\n    expected = np.array([[0, 1, 0],\n                         [0, 0, 1],\n                         [0, 0, 1],\n                         [0, 0, 1]])\n    got = lb.fit_transform(inp)\n    assert_array_equal(lb.inverse_transform(got.toarray()), inp)\n", "def test_label_binarize_non_integer_labels():\n    classes = ['a', 'b', 'c']\n    pos_label = 2\n    neg_label = 0\n\n    # List of single labels\n    y = ['a', 'b', 'a', 'c']\n    expected = np.array([[2, 0, 0],\n                         [0, 2, 0],\n                         [2, 0, 0],\n                         [0, 0, 2]])\n    yield check_binarized_results, y, classes, pos_label, neg_label, expected\n\n    # List of tuples\n    y = [('a',), ('b',), ('a',), ('c',)]\n    expected = np.array([[2, 0, 0],\n                         [0, 2, 0],\n                         [2, 0, 0],\n                         [0, 0, 2]])\n    yield check_binarized_results, y, classes, pos_label, neg_label, expected\n\n    # List of lists\n    y = [['a'], ['b'], ['a'], ['c']]\n    expected = np.array([[2, 0, 0],\n                         [0, 2, 0],\n                         [2, 0, 0],\n                         [0, 0, 2]])\n    yield check_binarized_results, y, classes, pos_label, neg_label, expected\n\n    # Multilabel indicator\n    y = [[1, 0, 0],\n          [0, 1, 0],\n          [1, 0, 0],\n          [0, 0, 1]]\n    y_sparse = [sparse_matrix(y)\n                for sparse_matrix in [coo_matrix, csc_matrix, csr_matrix,\n                                      dok_matrix, lil_matrix]]\n\n    expected = np.array([[2, 0, 0],\n                         [0, 2, 0],\n                         [2, 0, 0],\n                         [0, 0, 2]])\n    for y in [y] + y_sparse:\n        yield check_binarized_results, y, classes, pos_label, neg_label, expected\n"], "sample_1028": ["def test_Mod_is_imaginary():\n    x = Symbol('x')\n    i = Symbol('i', imaginary=True)\n    p = Symbol('p', positive=True)\n    n = Symbol('n', negative=True)\n\n    assert Mod(x, y).is_imaginary is None\n    assert Mod(i, p).is_imaginary is True\n    assert Mod(i, n).is_imaginary is True\n    assert Mod(i, i).is_imaginary is False\n    assert Mod(i, -i).is_imaginary is False\n", "def test_Mod_is_nonzero():\n    p = Symbol('p', integer=True)\n    q1 = Symbol('q1', integer=True)\n    q2 = Symbol('q2', integer=True, nonzero=True)\n    assert Mod(x, y).is_zero is None\n    assert Mod(p, q1).is_zero is None\n    assert Mod(x, q2).is_zero is None\n    assert Mod(p, q2).is_zero is None\n    assert Mod(0, x).is_zero is True\n    assert Mod(0, q2).is_zero is True\n    assert Mod(p, 0).is_zero is False  # issue 10727\n    assert Mod(0, 0).is_zero is False\n    assert Mod(nan, 1).is_zero is False\n    assert Mod(1, nan).is_zero is False\n    assert Mod(nan, nan).is_zero is False\n", "def test_Mod_simplify():\n    x = Symbol('x')\n    y = Symbol('y')\n    p = Symbol('p', positive=True)\n    n = Symbol('n', negative=True)\n\n    # Test simplification of Mod with same base\n    assert Mod(x**2 + x, x) == Mod(x, x)\n    assert Mod(x**2 + x + 1, x) == Mod(1, x)\n    assert Mod(x**2 - x, x) == Mod(0, x)\n    assert Mod(x**2 - x - 1, x) == Mod(-1, x)\n\n    # Test simplification of Mod with different base\n    assert Mod(x**2 + y, x) == Mod(y, x)\n    assert Mod(x**2 + y + 1, x) == Mod(y + 1, x)\n    assert Mod(x**2 - y, x) == Mod(-y, x)\n    assert Mod(x**2 - y - 1, x) == Mod(-y - 1, x)\n\n    # Test simplification of Mod with symbolic divisor\n    assert Mod(x**2 + p, p) == Mod(p, p)\n    assert Mod(x**2 + p + 1, p) == Mod(1, p)\n    assert Mod(x**2 - p, p) == Mod(0, p)\n    assert Mod(x**2 - p - 1, p) == Mod(-1, p)\n\n    # Test simplification of Mod with negative divisor\n    assert Mod(x**2 + n, n) == Mod(n, n)\n    assert Mod(x**2 + n + 1, n) == Mod(1, n)\n    assert Mod(x**2 - n, n) == Mod(0, n)\n    assert Mod(x**2 - n - 1, n) == Mod(-1, n)\n", "def test_Mod_periodic():\n    a, b = symbols('a b')\n    p = symbols('p', positive=True, integer=True)\n    assert (p * a) % p == 0\n    assert (p * a + b) % p == b % p\n    assert (p * a + p * b) % p == 0\n    assert (p * a + p * b + 1) % p == 1\n    assert (-p * a) % p == 0\n    assert (-p * a + b) % p == b % p\n    assert (-p * a + p * b) % p == 0\n    assert (-p * a + p * b + 1) % p == 1\n    assert (a + p * b) % p == a % p\n    assert (a + p * b + 1) % p == (a + 1) % p\n    assert (-a + p * b) % p == -a % p\n    assert (-a + p * b + 1) % p == (-a + 1) % p\n", "def test_Mod_denest_add():\n    assert Mod(x + 3, 2) == Mod(x, 2)\n    assert Mod(x + 3.0, 1) == Mod(1.*x, 1)\n    assert Mod(x - S(33)/10, 1) == Mod(x + S(7)/10, 1)\n    assert (x + 1) % x == 1 % x\n    assert (x + y) % x == y % x\n    assert (x + y + 2) % x == (y + 2) % x\n    assert (a + 3*x + 1) % (2*x) == Mod(a + x + 1, 2*x)\n    assert (12*x + 18*y) % (3*x) == 3*Mod(6*y, x)\n    assert (x**2 + y**2 + 3) % 3 == Mod(x**2 + y**2, 3)\n    assert (2*x**2 + 2*y**2 + 6) % 2 == 2*Mod(x**2 + y**2 + 3, 1)\n    assert (x**2 + y**2 + 3.0) % 3 == Mod(1.*x**2 + 1.*y**2, 3)\n    assert (2*x**2 + 2*y**2 + 6.0) % 2 == 2*Mod(1.*x**2 + 1.*y**2 + 3, 1)\n"], "sample_441": ["    def test_normalize(self):\n        field = UsernameField()\n        self.assertEqual(field.to_python(\"  test  \"), \"test\")\n        self.assertEqual(field.to_python(\"test\"), \"test\")\n", "    def test_normalize_username(self):\n        # The normalization happens in the to_python() method.\n        ohm_username = \"test\u2126\"  # U+2126 OHM SIGN\n        field = UsernameField()\n        self.assertEqual(field.to_python(ohm_username), \"test\u03a9\")  # U+03A9 GREEK CAPITAL LETTER OMEGA\n", "    def test_unicodedata_normalize(self):\n        username = \"iamthe\u03a9\"  # U+03A9 GREEK CAPITAL LETTER OMEGA\n        field = UsernameField()\n        normalized_username = field.to_python(username)\n        self.assertEqual(normalized_username, \"iamthe\u03a9\")  # U+03A9 GREEK CAPITAL LETTER OMEGA\n", "    def test_normalize(self):\n        field = UsernameField()\n        self.assertEqual(field.to_python(\"test\"), \"test\")\n        self.assertEqual(field.to_python(\"test\"), \"test\")\n        self.assertEqual(field.to_python(\" Test \"), \"Test\")\n        self.assertEqual(field.to_python(\"test\u03a3\"), \"test\u03a3\")\n\n        self.assertEqual(field.to_python(\"test\"), \"test\")\n        self.assertEqual(field.to_python(\"test\u03a9\"), \"test\u03a9\")  # U+03A9 GREEK CAPITAL LETTER OMEGA\n        self.assertEqual(field.to_python(\"test\u2126\"), \"test\u03a9\")  # U+2126 OHM SIGN\n", "    def test_normalize_input(self):\n        username = UsernameField()\n        self.assertEqual(\n            username.to_python(\"   \\u2126   \"),\n            \"\\u03A9\",\n        )\n"], "sample_521": ["def test_text3d_rotation(fig_test, fig_ref):\n    ax = fig_test.add_subplot(projection=\"3d\")\n    txt = Text(0.5, 0.5, 1, 'Hello $\\int$', rotation=45, zdir='x')\n    art3d.text_2d_to_3d(txt, z=1, zdir='x')\n    ax.add_artist(txt)\n\n    ax = fig_ref.add_subplot(projection=\"3d\")\n    txt = art3d.Text3D(0.5, 0.5, 1, 'Hello $\\int$', rotation=45, zdir='x')\n    ax.add_artist(txt)\n", "def test_line3d_zdir(fig_test, fig_ref):\n    # Test that changing zdir works as expected\n    ax_test = fig_test.add_subplot(projection='3d')\n    line, = ax_test.plot([0, 1], [0, 1], [0, 1])\n    line.set_3d_properties(zdir='y')\n    ax_test.set_xlim3d(-1, 2)\n    ax_test.set_ylim3d(-1, 2)\n    ax_test.set_zlim3d(-1, 2)\n    \n    ax_ref = fig_ref.add_subplot(projection='3d')\n    ax_ref.plot([0, 1], [1, 1], [0, 1])\n    ax_ref.set_xlim3d(-1, 2)\n    ax_ref.set_ylim3d(-1, 2)\n    ax_ref.set_zlim3d(-1, 2)\n", "def test_text3d_rotation(fig_test, fig_ref):\n    \"\"\"Test that text3d rotates with the plot.\"\"\"\n    ax = fig_test.add_subplot(projection=\"3d\")\n    t3d = art3d.Text3D(0.5, 0.5, 1, r'Foo bar $\\int$')\n    ax.add_artist(t3d)\n    ax.view_init(elev=30, azim=45, roll=30)\n\n    ax = fig_ref.add_subplot(projection=\"3d\")\n    t3d = art3d.Text3D(0.5, 0.5, 1, r'Foo bar $\\int$')\n    ax.add_artist(t3d)\n    fig_test.canvas.draw()\n    fig_ref.canvas.draw()\n    ax.view_init(elev=30, azim=45, roll=30)\n", "def test_path3dcollection_set_sizes():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    path = mpath.Path(np.array([[0, 0], [1, 1], [1, 0]]))\n    c = art3d.Path3DCollection([path], sizes=100)\n    assert c._sizes3d == [100]\n    assert c.get_sizes() == [100]\n    c.set_sizes([200])\n    assert c._sizes3d == [200]\n    assert c.get_sizes() == [200]\n    assert len(c._sizes3d) == 1\n    assert len(c.get_sizes()) == 1\n    c.set_sizes(50)\n    assert c._sizes3d == [50]\n    assert c.get_sizes() == [50]\n    assert len(c._sizes3d) == 1\n    assert len(c.get_sizes()) == 1\n    with pytest.raises(ValueError):\n        c.set_sizes([100, 200])\n", "def test_plot_surface_3d_interpolation():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    X = np.linspace(-3, 3, 256)\n    Y = np.linspace(-3, 3, 256)\n    X, Y = np.meshgrid(X, Y)\n    R = np.sqrt(X**2 + Y**2)\n    Z = np.cos(R)\n    surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, antialiased=True,\n                           edgecolor='k', linewidth=0.1)\n    fig.colorbar(surf, shrink=0.5, aspect=5)\n\n    X = np.linspace(-3, 3, 256)\n    Y = np.linspace(-3, 3, 256)\n    X, Y = np.meshgrid(X, Y)\n    R = np.sqrt(X**2 + Y**2)\n    Z = np.cos(R)\n    ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, antialiased=False,\n                    edgecolor='k', linewidth=0.1, rstride=40, cstride=40)\n"], "sample_490": ["def test_validate_with_violation_error_code(self):\n    constraint = UniqueConstraint(\n        fields=[\"name\"],\n        name=\"name_uniq\",\n        violation_error_code=\"custom_code\",\n    )\n    non_unique_product = UniqueConstraintProduct(\n        name=self.p1.name, color=self.p1.color\n    )\n    msg = \"Constraint \u201cname_uniq\u201d is violated.\"\n    with self.assertRaisesMessage(ValidationError, msg) as cm:\n        constraint.validate(UniqueConstraintProduct, non_unique_product)\n    self.assertEqual(cm.exception.code, \"custom_code\")\n", "def test_validate_nested_expression(self):\n    constraint = models.UniqueConstraint(\n        Lower(F(\"author__name\")), name=\"author_name_lower_uniq\"\n    )\n    msg = \"Constraint \u201cauthor_name_lower_uniq\u201d is violated.\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        constraint.validate(\n            UniqueConstraintProduct,\n            UniqueConstraintProduct(author__name=self.p1.author.name.upper()),\n        )\n    constraint.validate(\n        UniqueConstraintProduct,\n        UniqueConstraintProduct(author__name=\"another-author\"),\n    )\n    # Existing instances have their existing row excluded.\n    constraint.validate(UniqueConstraintProduct, self.p1)\n    # Unique field is excluded.\n    constraint.validate(\n        UniqueConstraintProduct,\n        UniqueConstraintProduct(author__name=self.p1.author.name.upper()),\n        exclude={\"author__name\"},\n    )\n", "def test_delete_database_constraint(self):\n    constraint_name = \"price_gt_discounted_price\"\n    constraints = get_constraints(Product._meta.db_table)\n    self.assertIn(constraint_name, constraints)\n    with atomic(), connection.cursor() as cursor:\n        cursor.execute(\n            \"ALTER TABLE {} DROP CONSTRAINT {};\".format(\n                connection.ops.quote_name(Product._meta.db_table),\n                connection.ops.quote_name(constraint_name),\n            )\n        )\n    constraints = get_constraints(Product._meta.db_table)\n    self.assertNotIn(constraint_name, constraints)\n    Product.objects.create(price=10, discounted_price=20)\n", "    def test_nulls_distinct_database_constraint(self):\n        obj_1 = UniqueConstraintProduct.objects.create(name=\"p1\", color=\"red\")\n        UniqueConstraintProduct.objects.create(name=None, color=\"red\")\n        UniqueConstraintProduct.objects.create(name=\"p1\", color=None)\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintProduct.objects.create(name=\"p1\", color=\"red\")\n\n        class NullsNotDistinctModel(models.Model):\n            name = models.CharField(max_length=255)\n            color = models.CharField(max_length=255)\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=[\"name\", \"color\"],\n                        name=\"name_color_uniq\",\n                        nulls_distinct=False,\n                    )\n                ]\n\n        NullsNotDistinctModel.objects.create(name=\"p1\", color=\"red\")\n        with self.assertRaises(IntegrityError):\n            NullsNotDistinctModel.objects.create(name=\"p1\", color=None)\n", "    def setUpTestData(cls):\n        cls.p1 = UniqueConstraintProduct.objects.create(name=\"p1\", color=\"red\")\n        cls.p2 = UniqueConstraintProduct.objects.create(name=\"p2\", color=None)\n"], "sample_141": ["    def test_progress_bar(self):\n        output = StringIO()\n        progress_bar = ProgressBar(output, 10)\n        progress_bar.update(5)\n        self.assertIn('[.......       ]', output.getvalue())\n", "def test_deferred_fields_deserialization(self):\n    \"\"\"\n    Test deserialization of deferred fields.\n    \"\"\"\n    deferred_str = \"\"\"[\n    {\n        \"pk\": 1,\n        \"model\": \"serializers.player\",\n        \"fields\": {\n            \"name\": \"Bob\",\n            \"rank\": 1,\n            \"team\": \"Team\"\n        }\n    },\n    {\n        \"pk\": 1,\n        \"model\": \"serializers.team\",\n        \"fields\": {\n            \"name\": \"Team\"\n        }\n    }]\"\"\"\n\n    with self.assertRaises(DeserializationError):\n        for obj in serializers.deserialize('json', deferred_str, ignore=False):\n            obj.save_deferred_fields()\n\n    deferred_str = \"\"\"[\n    {\n        \"pk\": 1,\n        \"model\": \"serializers.player\",\n        \"fields\": {\n            \"name\": \"Bob\",\n            \"rank\": 1,\n            \"team\": 1\n        }\n    },\n    {\n        \"pk\": 1,\n        \"model\": \"serializers.team\",\n        \"fields\": {\n            \"name\": \"Team\"\n        }\n    }]\"\"\"\n\n    for obj in serializers.deserialize('json', deferred_str, ignore=False):\n        obj.save_deferred_fields()\n", "    def test_deserialization_deferred_fields(self):\n        \"\"\"\n        Test deserialization with deferred fields.\n        \"\"\"\n        test_string = \"\"\"[{\n            \"pk\": 1,\n            \"model\": \"serializers.m2mdata\",\n            \"fields\": {\n                \"data\": [1, 2, 3]\n            }\n        }, {\n            \"pk\": 1,\n            \"model\": \"serializers.m2mrelated\",\n            \"fields\": {\n                \"name\": \"Test\"\n            }\n        }]\"\"\"\n        deserialized_objects = list(serializers.deserialize('json', test_string, ignore=False))\n        self.assertEqual(len(deserialized_objects), 2)\n        m2m_data_obj = deserialized_objects[0]\n        m2m_related_obj = deserialized_objects[1]\n        self.assertIsNone(m2m_data_obj.object.data_id)\n        m2m_data_obj.save_deferred_fields()\n        m2m_related_obj.save_deferred_fields()\n        self.assertIsNotNone(m2m_data_obj.object.data_id)\n", "def test_deserialize_m2m_values_invalid_input(self):\n    \"\"\"\n    Test that deserialize_m2m_values raises M2MDeserializationError for invalid input.\n    \"\"\"\n    field = models.ManyToManyField(models.Model)\n    field.remote_field.model = models.Model\n    field.remote_field.model._default_manager.get_by_natural_key = lambda *args: None\n\n    with self.assertRaises(M2MDeserializationError):\n        deserialize_m2m_values(field, None, 'default', True)\n\n    with self.assertRaises(M2MDeserializationError):\n        deserialize_m2m_values(field, 'non-iterable', 'default', True)\n", "    def test_progress_bar_no_output(self):\n        output = StringIO()\n        progress_bar = ProgressBar(output, 100)\n        progress_bar.update(50)\n        self.assertEqual(output.getvalue(), '')\n"], "sample_626": ["def test_merge_sel_results():\n    # Test merging two IndexSelResult objects\n    result1 = IndexSelResult(dim_indexers={\"x\": 0}, indexes={\"x\": 0}, variables={\"x\": 0})\n    result2 = IndexSelResult(dim_indexers={\"y\": 1}, indexes={\"y\": 1}, variables={\"y\": 1})\n    merged = merge_sel_results([result1, result2])\n    assert merged.dim_indexers == {\"x\": 0, \"y\": 1}\n    assert merged.indexes == {\"x\": 0, \"y\": 1}\n    assert merged.variables == {\"x\": 0, \"y\": 1}\n\n    # Test merging two IndexSelResult objects with duplicate dimensions\n    result1 = IndexSelResult(dim_indexers={\"x\": 0})\n    result2 = IndexSelResult(dim_indexers={\"x\": 1})\n    with pytest.raises(ValueError, match=\"Xarray does not support label-based selection with more than one index over the following dimension\"):\n        merge_sel_results([result1, result2])\n", "def test_explicit_indexing_adapter():\n    array = np.arange(12).reshape(3, 4)\n    key = VectorizedIndexer((np.array([0, 2]), np.array([0, 3])))\n    result = explicit_indexing_adapter(key, array.shape, IndexingSupport.VECTORIZED, array.__getitem__)\n    np.testing.assert_array_equal(result, np.array([[0, 3], [8, 11]]))\n\n    result = explicit_indexing_adapter(\n        key, array.shape, IndexingSupport.OUTER, array.__getitem__\n    )\n    np.testing.assert_array_equal(result, np.array([[0, 3], [8, 11]]))\n\n    result = explicit_indexing_adapter(\n        key, array.shape, IndexingSupport.BASIC, array.__getitem__\n    )\n    np.testing.assert_array_equal(result, np.array([[0, 3], [8, 11]]))\n\n    key = BasicIndexer((slice(1, 3), slice(1, 3)))\n    result = explicit_indexing_adapter(key, array.shape, IndexingSupport.VECTORIZED, array.__getitem__)\n    np.testing.assert_array_equal(result, np.array([[4, 5], [8, 9]]))\n\n    result = explicit_indexing_adapter(key, array.shape, IndexingSupport.OUTER, array.__getitem__)\n    np.testing.assert_array_equal(result, np.array([[4, 5], [8, 9]]))\n\n    result = explicit_indexing_adapter(key, array.shape, IndexingSupport.BASIC, array.__getitem__)\n    np.testing.assert_array_equal(result, np.array([[4, 5], [8, 9]]))\n", "def test_map_index_queries() -> None:\n    data = xr.DataArray(np.arange(12).reshape(3, 4), dims=(\"x\", \"y\"))\n    results = map_index_queries(data, {\"x\": 1})\n    assert results.dim_indexers == {\"x\": 1}\n    assert results.indexes == {}\n\n    results = map_index_queries(data, {\"x\": slice(1, 2), \"y\": 2})\n    assert results.dim_indexers == {\"x\": slice(1, 2), \"y\": 2}\n    assert results.indexes == {}\n\n    with pytest.raises(ValueError, match=r\"no index found for coordinate 'z'\"):\n        map_index_queries(data, {\"z\": 1})\n\n    with pytest.raises(ValueError, match=r\"'w' is not a valid dimension or coordinate\"):\n        map_index_queries(data, {\"w\": 1})\n\n    with pytest.raises(ValueError, match=r\"cannot supply selection options\"):\n        map_index_queries(data, {\"x\": 1}, method=\"nearest\")\n", "def test_create_mask(indexer: Any, shape: tuple[int, ...], data=None):\n    # Test that a mask is created correctly when the indexer has -1 values\n    # and the data is a dask array.\n    indexer = OuterIndexer((np.array([0, -1, 2]), np.array([0, 1])))\n    shape = (3, 3)\n    data = np.arange(9).reshape(shape)\n    mask = create_mask(indexer, shape, data)\n    assert np.all(mask == np.array([[False, True, False]]))\n\n    # Test that a mask is created correctly when the indexer has -1 values\n    # and the data is a sparse array.\n    import sparse\n    data = sparse.COO.from_numpy(np.arange(9).reshape(shape))\n    mask = create_mask(indexer, shape, data)\n    assert np.all(mask == np.array([[False, True, False]]))\n\n    # Test that a mask is not created when the indexer does not have -1 values.\n    indexer = OuterIndexer((np.array([0, 1, 2]), np.array([0, 1])))\n    mask = create_mask(indexer, shape, data)\n    assert mask is None\n\n    # Test that a mask is created correctly when the indexer is a VectorizedIndexer\n    indexer = VectorizedIndexer((np.array([0, -1, 2]), np.array([0, 1])))\n    mask = create_mask(indexer, shape, data)\n    assert np.all(mask == np.array([[False, True, False]]))\n", "def test_indexing_adapter_round_trip(dtype) -> None:\n    values = np.array([\"a\", \"b\", \"c\"], dtype=dtype)\n    expected = values[1]\n    actual = PandasIndexingAdapter(values)[1].item()\n    assert actual == expected\n    assert isinstance(actual, str) or isinstance(actual, bytes)\n"], "sample_204": ["def test_loading_replacing_migration(self):\n    \"\"\"\n    Tests loading a migration that replaces another migration.\n    \"\"\"\n    loader = MigrationLoader(connection)\n    recorder = MigrationRecorder(connection)\n    self.addCleanup(recorder.flush)\n\n    # Load with nothing applied: replacing migration should be used.\n    loader.build_graph()\n    plan = set(loader.graph.forwards_plan(('migrations', '0003_replacement')))\n    plan = plan - loader.applied_migrations.keys()\n    expected_plan = {\n        ('migrations', '0003_replacement'),\n    }\n    self.assertEqual(plan, expected_plan)\n\n    # Fake-apply one of the replaced migrations: replacing migration shouldn't be used.\n    recorder.record_applied('migrations', '0001_initial')\n    loader.build_graph()\n    plan = set(loader.graph.forwards_plan(('migrations', '0003_replacement')))\n    plan = plan - loader.applied_migrations.keys()\n    expected_plan = {\n        ('migrations', '0002_second'),\n        ('migrations', '0003_replacement'),\n    }\n    self.assertEqual(plan, expected_plan)\n\n    # Fake-apply the other replaced migration: replacing migration should be used again.\n    recorder.record_applied('migrations', '0002_second')\n    loader.build_graph()\n    plan = set(loader.graph.forwards_plan(('migrations', '0003_replacement')))\n    plan = plan - loader.applied_migrations.keys()\n    expected_plan = {\n        ('migrations', '0003_replacement'),\n    }\n    self.assertEqual(plan, expected_plan)\n", "    def test_cyclic_dependencies(self):\n        \"\"\"\n        A cyclic dependency in the migration graph should raise an exception.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        msg = (\n            \"Migration migrations.0002_second depends on migrations.0001_initial, \"\n            \"which in turn depends on it, creating a cycle.\"\n        )\n        with self.assertRaisesMessage(NodeNotFoundError, msg):\n            loader.build_graph()\n", "def test_detect_conflicts(self):\n    \"\"\"\n    Test that detect_conflicts method correctly identifies conflicting apps.\n    \"\"\"\n    with override_settings(MIGRATION_MODULES={\n        \"app1\": \"migrations.test_migrations_conflict\",\n        \"app2\": \"migrations.test_migrations_conflict\",\n    }) as override_settings:\n        @modify_settings(INSTALLED_APPS={'append': [\n            \"migrations.test_migrations_conflict.app1\",\n            \"migrations.test_migrations_conflict.app2\",\n        ]})\n            loader = MigrationLoader(connection)\n            loader.build_graph()\n            conflicts = loader.detect_conflicts()\n            expected_conflicts = {\n                'app1': ['0001_initial', '0002_conflict'],\n                'app2': ['0001_initial', '0002_conflict'],\n            }\n            self.assertEqual(conflicts, expected_conflicts)\n\n        inner_test()\n", "def test_loading_replacing_replaced(self):\n    \"\"\"\n    Tests loading a migration that replaces a replaced migration\n    \"\"\"\n    loader = MigrationLoader(connection)\n    recorder = MigrationRecorder(connection)\n    self.addCleanup(recorder.flush)\n\n    # Loading with nothing applied should use the squashed migration\n    loader.build_graph()\n    plan = set(loader.graph.forwards_plan(('migrations', '0004_auto')))\n    plan = plan - loader.applied_migrations.keys()\n    expected_plan = {\n        ('migrations', '0001_auto'),\n        ('migrations', '0003_squashed'),\n        ('migrations', '0004_auto'),\n    }\n    self.assertEqual(plan, expected_plan)\n\n    # Fake-apply one migration and it should now use the old two\n    recorder.record_applied(\"migrations\", \"0001_initial\")\n    recorder.record_applied(\"migrations\", \"0002_second\")\n    loader.build_graph()\n    plan = set(loader.graph.forwards_plan(('migrations', '0004_auto')))\n    plan = plan - loader.applied_migrations.keys()\n    expected_plan = {\n        ('migrations', '0003_replacing_replaced'),\n        ('migrations', '0004_auto'),\n    }\n    self.assertEqual(plan, expected_plan)\n\n    # Fake-apply one more migration and it should use the replacing migration\n    recorder.record_applied(\"migrations\", \"0001_initial\")\n    recorder.record_applied(\"migrations\", \"0002_second\")\n    recorder.record_applied(\"migrations\", \"0003_replacing\")\n    loader.build_graph()\n    plan = set(loader.graph.forwards_plan(('migrations', '0004_auto')))\n    plan = plan - loader.applied_migrations.keys()\n    expected_plan = {\n        ('migrations', '0004_auto'),\n    }\n    self.assertEqual(plan, expected_plan)\n", "    def test_replacing_migrations(self):\n        \"\"\"\n        Tests the logic for replacing migrations with new ones.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        recorder = MigrationRecorder(connection)\n        self.addCleanup(recorder.flush)\n\n        # Loading with nothing applied should just give us the replacing node\n        loader.build_graph()\n        self.assertEqual(\n            len([x for x in loader.graph.nodes if x[0] == \"migrations\"]),\n            1,\n        )\n\n        # However, fake-apply one migration and it should now use the old two\n        recorder.record_applied(\"migrations\", \"0001_initial\")\n        loader.build_graph()\n        self.assertEqual(\n            len([x for x in loader.graph.nodes if x[0] == \"migrations\"]),\n            2,\n        )\n\n        # The replacing migration should be marked as applied if all of its\n        # replacement targets are applied.\n        recorder.record_applied(\"migrations\", \"0002_second\")\n        loader.build_graph()\n        self.assertIn((\"migrations\", \"0003_replacing\"), loader.applied_migrations)\n\n        # The replacing migration shouldn't be marked as applied if not all of\n        # its replacement targets are applied.\n        recorder.record_unapplied(\"migrations\", \"0001_initial\")\n        loader.build_graph()\n        self.assertNotIn((\"migrations\", \"0003_replacing\"), loader.applied_migrations)\n"], "sample_984": ["def test_Inverse():\n    assert str(~x) == \"~x\"\n    assert str(~(x*y)) == \"~(x*y)\"\n    assert str(~(x + y)) == \"~(x + y)\"\n", "def test_str_printer_settings():\n    printer = StrPrinter()\n    assert printer._settings == {\n        \"order\": None,\n        \"full_prec\": \"auto\",\n        \"sympy_integers\": False,\n    }\n\n    printer._settings[\"order\"] = \"lex\"\n    assert printer._settings == {\n        \"order\": \"lex\",\n        \"full_prec\": \"auto\",\n        \"sympy_integers\": False,\n    }\n\n    printer._settings[\"full_prec\"] = True\n    assert printer._settings == {\n        \"order\": \"lex\",\n        \"full_prec\": True,\n        \"sympy_integers\": False,\n    }\n\n    printer._settings[\"sympy_integers\"] = True\n    assert printer._settings == {\n        \"order\": \"lex\",\n        \"full_prec\": True,\n        \"sympy_integers\": True,\n    }\n", "def test_print_PolyElement_rational_exponent():\n    Ruv, u,v = ring(\"u,v\", ZZ)\n    Rxyz, x,y,z = ring(\"x,y,z\", Ruv)\n    assert str(x**(QQ(1, 2))) == \"x**(1/2)\"\n    assert str((-x)**(QQ(1, 2))) == \"(-x)**(1/2)\"\n    assert str((-x)**(QQ(3, 2))) == \"(-x)**(3/2)\"\n    assert str((-x)**(QQ(1, 3))) == \"(-x)**(1/3)\"\n", "def test_MatrixSymbol_printing():\n    # test cases for issue #11821\n    A = MatrixSymbol(\"A\", 1, 3)\n    B = MatrixSymbol(\"B\", 1, 3)\n    C = MatrixSymbol(\"C\", 1, 3)\n    M = MatrixSymbol(\"M\", 3, 3)\n    assert(str(A) == \"A\")\n    assert(str(M) == \"M\")\n    assert(str(A*A) == \"A*A\")\n    assert(str(A*A*A) == \"A*A*A\")\n    assert(str(A*B) == \"A*B\")\n    assert(str(B*A) == \"B*A\")\n    assert(str(A+B) == \"A + B\")\n    assert(str(A-B) == \"A - B\")\n    assert(str(A+B*C) == \"A + B*C\")\n    assert(str(A*C+B) == \"A*C + B\")\n    assert(str(M*A) == \"M*A\")\n    assert(str(A*M) == \"A*M\")\n    assert(str(M*A*M) == \"M*A*M\")\n    assert(str(A**2) == \"A**2\")\n    assert(str(A**3) == \"A**3\")\n    assert(str(-A) == \"-A\")\n    assert(str(-M) == \"-M\")\n", "compilation error"], "sample_422": ["    def setUpTestData(cls):\n        cls.book1 = Book.objects.create(title=\"Poems\")\n        cls.author1 = Author.objects.create(name=\"Charlotte\", first_book=cls.book1)\n", "    def setUpTestData(cls):\n        cls.book1 = Book.objects.create(title=\"Poems\")\n        cls.book2 = Book.objects.create(title=\"Jane Eyre\")\n\n        cls.author1 = Author.objects.create(name=\"Charlotte\", first_book=cls.book1)\n        cls.author2 = Author.objects.create(name=\"Anne\", first_book=cls.book1)\n        cls.author3 = Author.objects.create(name=\"Emily\", first_book=cls.book2)\n", "    def setUpTestData(cls):\n        cls.book1 = Book.objects.create(title=\"Poems\")\n        cls.author1 = Author.objects.create(name=\"Charlotte\", first_book=cls.book1)\n", "    def setUpTestData(cls):\n        cls.book1 = Book.objects.create(title=\"Poems\")\n        cls.book2 = Book.objects.create(title=\"Jane Eyre\")\n        cls.author1 = Author.objects.create(name=\"Charlotte\", first_book=cls.book1)\n        cls.author2 = Author.objects.create(name=\"Anne\", first_book=cls.book1)\n", "    def setUpTestData(cls):\n        cls.restaurant = Restaurant.objects.create(name=\"Restaurant\")\n        cls.place = Place.objects.create(name=\"Place\")\n        cls.restaurant.place = cls.place\n        cls.restaurant.save()\n"], "sample_1100": ["def test_pow_as_content_primitive():\n    assert (2*x + 2)**y.as_content_primitive() == \\\n        (1, (2*(x + 1), y))\n    assert (2*x + 2)**3.as_content_primitive() == (8, (x + 1)**3)\n", "def test_Pow_is_algebraic():\n    x = Symbol('x', algebraic=True)\n    y = Symbol('y', algebraic=True, positive=True)\n\n    assert (x**2).is_algebraic is True\n    assert (x**3).is_algebraic is True\n    assert (x**x).is_algebraic is None\n    assert (y**x).is_algebraic is True\n\n    assert (x**Rational(1, 3)).is_algebraic is None\n    assert (y**Rational(1, 3)).is_algebraic is True\n\n    assert sqrt(-1 - sqrt(2)).is_algebraic is False\n\n    i = Symbol('i', imaginary=True)\n    assert (i**i).is_algebraic is False\n    assert (I**i).is_algebraic is True\n    assert ((-I)**i).is_algebraic is True\n    assert (2**i).is_algebraic is False  # (2**(pi/log(2) * I)) is algebraic, 2**I is not\n    assert (2**I).is_algebraic is False\n    assert (2**-I).is_algebraic is False\n    assert (i**2).is_algebraic is True\n    assert (i**3).is_algebraic is False\n    assert (i**x).is_algebraic is False  # could be (-I)**(2/3)\n    e = Symbol('e', even=True)\n    o = Symbol('o', odd=True)\n    k = Symbol('k', integer=True)\n    assert (i**e).is_algebraic is True\n    assert (i**o).is_algebraic is False\n    assert (i**k).is_algebraic is False\n    assert (i**(4*k)).is_algebraic is True\n\n    x = Symbol(\"x\", nonnegative=True)\n    y = Symbol(\"y\", nonnegative=True)\n    assert im(x**y).expand(complex=True) is S.Zero\n    assert (x**y).is_algebraic is False\n    i = Symbol('i', imaginary=True)\n    assert (exp(i)**I).is_algebraic is True\n    assert log(exp(i)).is_imaginary is None  # i could be 2*pi*I\n    c = Symbol('", "def test_isqrt():\n    assert isqrt(0) == 0\n    assert isqrt(1) == 1\n    assert isqrt(4) == 2\n    assert isqrt(10) == 3\n    assert isqrt(20) == 4\n    assert isqrt(100) == 10\n    assert isqrt(1000) == 31\n    assert isqrt(1000000) == 1000\n    assert isqrt(1000000000) == 31622\n    assert isqrt(1234567890) == 35136\n    assert isqrt(1234567890123456789) == 351364182\n    assert isqrt(12345678901234567890) == 351364183\n", "def test_Pow_zero_to_power():\n    # Test that 0**x always evaluates to 0, except when x is 0, where it returns 1.\n    assert Pow(0, 1, evaluate=False) == 0\n    assert Pow(0, 2, evaluate=False) == 0\n    assert Pow(0, -1, evaluate=False) == 0\n    assert Pow(0, 0, evaluate=False) == 1\n    assert Pow(0, S.Zero, evaluate=False) == 1\n    assert Pow(0, Rational(0), evaluate=False) == 1\n", "def test_integer_nthroot():\n    assert integer_nthroot(16, 2) == (4, True)\n    assert integer_nthroot(26, 2) == (5, False)\n    assert integer_nthroot(27, 3) == (3, True)\n    assert integer_nthroot(64, 4) == (4, True)\n    assert integer_nthroot(125, 3) == (5, True)\n    assert integer_nthroot(1000, 3) == (10, True)\n    assert integer_nthroot(-27, 3) == (-3, True)\n    assert integer_nthroot(-1000, 3) == (-10, True)\n    assert integer_nthroot(-1000, -3) == (-0.1, False)\n    assert integer_nthroot(-27, -3) == (-0.333333333333333, False)\n    assert integer_nthroot(-1, -1) == (-1, True)\n\n    assert integer_nthroot(100, 0.5) == (10, True)\n    assert integer_nthroot(100, 0.3) == (0, False)\n\n    assert integer_nthroot(2**100, 100) == (2, True)\n    assert integer_nthroot(2**100 + 1, 100) == (2, False)\n\n    assert integer_nthroot(-2**100, 100) == (-2, True)\n    assert integer_nthroot(-2**100 - 1, 100) == (-2, False)\n\n    assert integer_nthroot(0, 5) == (0, True)\n    assert integer_nthroot(1, 5) == (1, True)\n    assert integer_nthroot(-1, 5) == (-1, True)\n    assert integer_nthroot(-1, -5) == (-1, True)\n\n    assert integer_nthroot(-2, 5) == (0, False)\n\n    x = symbols('x')\n    assert integer_nthroot(2**x, x) == (2, True)\n\n    assert integer_nthroot(2**(2*x), 2*x) == (2, True)\n    assert integer_nthroot(2**(2*x + 1), 2*x) == (2, False)\n"], "sample_226": ["    def test_destroy_test_db(self):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        test_database_name = creation._get_test_db_name()\n        creation.connection.settings_dict['NAME'] = test_database_name\n\n        with mock.patch.object(creation, '_destroy_test_db') as mocked_destroy_db:\n            creation.destroy_test_db(old_database_name, verbosity=0)\n            mocked_destroy_db.assert_called_once_with(test_database_name, 0)\n", "    def test_clone_test_db(self, mocked_sync_apps, mocked_migrate, *mocked_objects):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = True\n        creation = test_connection.creation_class(test_connection)\n        if connection.vendor == 'oracle':\n            # Don't close connection on Oracle.\n            creation.connection.close = mock.Mock()\n        old_database_name = test_connection.settings_dict['NAME']\n        suffix = 'clone'\n        try:\n            with mock.patch.object(creation, '_clone_test_db'):\n                creation.clone_test_db(suffix, verbosity=0, autoclobber=True, keepdb=False)\n            # Ensure the clone name is correctly generated\n            cloned_database_name = creation.get_test_db_clone_settings(suffix)['NAME']\n            self.assertEqual(cloned_database_name, f\"{old_database_name}_{suffix}\")\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def test_create_test_db_with_serialize(self):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['NAME'] = 'test_database'\n        creation = test_connection.creation_class(test_connection)\n        if connection.vendor == 'oracle':\n            # Don't close connection on Oracle.\n            creation.connection.close = mock.Mock()\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                with mock.patch.object(creation, 'serialize_db_to_string') as mocked_serialize:\n                    creation.create_test_db(verbosity=0, autoclobber=True, serialize=True)\n                    mocked_serialize.assert_called_once()\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def test_clone_test_db(self, mock_clone):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        creation.clone_test_db(suffix='clone', verbosity=1, autoclobber=True, keepdb=False)\n        mock_clone.assert_called_once_with('clone', 1, autoclobber=True, keepdb=False)\n", "    def test_set_as_test_mirror(self):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        primary_settings_dict = test_connection.settings_dict\n        test_settings_dict = test_connection.settings_dict.copy()\n        test_settings_dict['NAME'] = 'test_database_name'\n        creation.set_as_test_mirror(primary_settings_dict)\n        self.assertEqual(creation.connection.settings_dict['NAME'], primary_settings_dict['NAME'])\n"], "sample_727": ["def test_imputation_most_frequent_sparse():\n    # Test imputation using the most-frequent strategy with sparse matrices.\n    X = sparse.csr_matrix(np.array([\n        [-1, -1, 0, 5],\n        [-1, 2, -1, 3],\n        [-1, 1, 3, -1],\n        [-1, 2, 3, 7],\n    ]))\n\n    X_true = np.array([\n        [2, 0, 5],\n        [2, 3, 3],\n        [1, 3, 3],\n        [2, 3, 7],\n    ])\n\n    # scipy.stats.mode, used in Imputer, doesn't return the first most\n    # frequent as promised in the doc but the lowest most frequent. When this\n    # test will fail after an update of scipy, Imputer will need to be updated\n    # to be consistent with the new (correct) behaviour\n    _check_statistics(X, X_true, \"most_frequent\", [np.nan, 2, 3, 3], -1)\n", "def test_imputation_strategy_parameter():\n    # Test that imputation works with strategy parameter as a string\n    X = np.array([\n        [1, 2, np.nan],\n        [4, np.nan, 6],\n        [7, 8, 9]\n    ])\n\n    imputer = Imputer(strategy=\"mean\")\n    imputer.fit(X)\n    assert_array_almost_equal(imputer.statistics_, np.array([4., 5., 7.]))\n\n    imputer = Imputer(strategy=\"median\")\n    imputer.fit(X)\n    assert_array_almost_equal(imputer.statistics_, np.array([4., 2., 7.]))\n\n    imputer = Imputer(strategy=\"most_frequent\")\n    imputer.fit(X)\n    assert_array_almost_equal(imputer.statistics_, np.array([7., 2., 9.]))\n\n    # Test that imputation raises an error with invalid strategy\n    imputer = Imputer(strategy=\"invalid\")\n    assert_raises(ValueError, imputer.fit, X)\n", "def test_imputation_edge_cases():\n    # Test edge cases\n    # Test imputation for a matrix with a single row.\n    X = np.array([[np.nan, 1, np.nan]])\n    imputer = Imputer(strategy=\"mean\")\n    X_imputed = imputer.fit_transform(X)\n    assert_array_almost_equal(X_imputed, np.array([[1., 1., 1.]]))\n\n    # Test imputation for a matrix with a single column.\n    X = np.array([[1.], [np.nan], [1.]])\n    imputer = Imputer(strategy=\"mean\")\n    X_imputed = imputer.fit_transform(X)\n    assert_array_almost_equal(X_imputed, np.array([[1.], [1.], [1.]]))\n\n    # Test imputation for a matrix with a single element.\n    X = np.array([[np.nan]])\n    imputer = Imputer(strategy=\"mean\")\n    X_imputed = imputer.fit_transform(X)\n    assert_array_almost_equal(X_imputed, np.array([[0.]]))\n\n    # Test imputation for an empty matrix.\n    X = np.array([])\n    imputer = Imputer(strategy=\"mean\")\n    X_imputed = imputer.fit_transform(X)\n    assert_array_almost_equal(X_imputed, np.array([]))\n", "def test_imputer_edge_cases():\n    # Test imputation on edge cases\n    imputer = Imputer(strategy=\"mean\")\n\n    # Test on empty array\n    X = np.array([])\n    X_true = np.array([])\n    assert_array_equal(imputer.fit_transform(X), X_true)\n\n    # Test on single element array\n    X = np.array([1])\n    X_true = np.array([1])\n    assert_array_equal(imputer.fit_transform(X), X_true)\n\n    # Test on single column array with all missing values\n    X = np.array([[np.nan]])\n    assert_raises(ValueError, imputer.fit_transform, X)\n\n    # Test on single row array with all missing values\n    X = np.array([[np.nan, np.nan]])\n    assert_array_equal(imputer.fit_transform(X), np.array([[np.nan, np.nan]]))\n", "def test_imputation_raise_warning_on_non_numeric_dtype():\n    # Test that a warning is raised when fitting an imputer with non-numeric data.\n    X = np.array([[1, 2], [3, 4], [np.nan, 6]])\n\n    # Make X non-numeric by adding a string column\n    X_non_numeric = np.hstack((X, np.array([['a'], ['b'], ['c']]))) \n\n    imputer = Imputer()\n    with assert_warns_message(UserWarning, \n                             \"Imputer does not support non-numeric data\"):\n        imputer.fit(X_non_numeric)\n"], "sample_855": ["def test_dummy_regressor_on_sparse_X():\n    X = sp.csc_matrix(np.array([[1, 0, 0],\n                                [0, 2, 0],\n                                [0, 0, 3]]))\n    y = np.array([2, 2, 2])\n    y_expected = np.array([2, 2, 2])\n    cls = DummyRegressor()\n    cls.fit(X, y)\n    y_pred = cls.predict(X)\n    assert_array_equal(y_pred, y_expected)\n", "def test_constant_strategy_multioutput_with_sparse_target():\n    X = [[0], [0], [0], [0]]  # ignored\n    y = sp.csc_matrix(np.array([[2, 3],\n                                [1, 3],\n                                [2, 3],\n                                [2, 0]]))\n\n    n_samples = len(X)\n\n    clf = DummyClassifier(strategy=\"constant\", random_state=0,\n                          constant=[1, 0])\n    clf.fit(X, y)\n    y_pred = clf.predict(X)\n    assert sp.issparse(y_pred)\n    assert_array_equal(y_pred.toarray(),\n                       np.hstack([np.ones((n_samples, 1)),\n                                  np.zeros((n_samples, 1))]))\n    _check_predict_proba(clf, X, y)\n", "def test_classifier_score(strategy):\n    y = [0, 2, 1, 1]\n    X = [[0]] * 4\n    clf = DummyClassifier(strategy=strategy, random_state=0, constant=0)\n    clf.fit(X, y)\n    assert clf.score(X, y) == 0.75\n", "def test_regressor_prediction_on_empty_input(strategy):\n    X = []\n    y = np.array([])\n\n    reg = DummyRegressor(strategy=strategy)\n    assert_raises(ValueError, reg.fit, X, y)\n", "def test_constant_strategy_regressor_with_array_constant():\n    X = [[0]] * 5  # ignored\n    y = [1, 2, 4, 6, 8]\n\n    # Test with 1D array constant\n    est = DummyRegressor(strategy=\"constant\", constant=np.array([43]))\n    est.fit(X, y)\n    assert_array_equal(est.predict(X), [43] * len(X))\n\n    # Test with 2D array constant\n    est = DummyRegressor(strategy=\"constant\", constant=np.array([[43]]))\n    est.fit(X, y)\n    assert_array_equal(est.predict(X), [43] * len(X))\n"], "sample_953": ["def test_quickstart_with_non_ascii_path(tempdir):\n    answers = {\n        'Root path': str(tempdir / '\\u30c9\\u30a4\\u30c4\\u30c9\\u30a4\\u30c4'),\n        'Project name': 'Sphinx Test',\n        'Author name': 'Georg Brandl',\n        'Project version': '0.1',\n    }\n    qs.term_input = mock_input(answers)\n    d = {}\n    qs.ask_user(d)\n    qs.generate(d)\n\n    conffile = tempdir / '\\u30c9\\u30a4\\u30c4\\u30c9\\u30a4\\u30c4' / 'conf.py'\n    assert conffile.isfile()\n    ns = {}\n    exec(conffile.read_text(), ns)\n    assert ns['extensions'] == []\n    assert ns['templates_path'] == ['_templates']\n    assert ns['project'] == 'Sphinx Test'\n    assert ns['copyright'] == '%s, Georg Brandl' % time.strftime('%Y')\n    assert ns['version'] == '0.1'\n    assert ns['release'] == '0.1'\n    assert ns['html_static_path'] == ['_static']\n\n    assert (tempdir / '\\u30c9\\u30a4\\u30c4\\u30c9\\u30a4\\u30c4' / '_static').isdir()\n    assert (tempdir / '\\u30c9\\u30a4\\u30c4\\u30c9\\u30a4\\u30c4' / '_templates').isdir()\n    assert (tempdir / '\\u30c9\\u30a4\\u30c4\\u30c9\\u30a4\\u30c4' / 'index.rst').isfile()\n    assert (tempdir / '\\u30c9\\u30a4\\u30c4\\u30c9\\u30a4\\u30c4' / 'Makefile').isfile()\n    assert (tempdir / '\\u30c9\\u30a4\\u30c4\\u30c9\\u30a4\\u30c4' / 'make.bat').isfile()\n", "def test_quickstart_valid_dir(tempdir):\n    answers = {\n        'Root path': tempdir,\n        'Project name': 'Sphinx Test',\n        'Author name': 'Georg Brandl',\n        'Project version': '0.1',\n    }\n    qs.term_input = mock_input(answers)\n    d = {}\n\n    # Create a directory with a Makefile\n    (tempdir / 'Makefile').write_text('')\n\n    with pytest.raises(SystemExit):\n        qs.ask_user(d)\n\n    # Create a directory with a non-empty source directory\n    (tempdir / 'source').mkdir()\n    (tempdir / 'source' / 'conf.py').write_text('')\n    with pytest.raises(SystemExit):\n        qs.ask_user(d)\n\n    # Create a directory with a reserved name\n    (tempdir / 'conf.py').write_text('')\n    with pytest.raises(SystemExit):\n        qs.ask_user(d)\n\n    # Create a valid directory\n    tempdir.mkdir(exist_ok=True)\n    (tempdir / 'source').mkdir(exist_ok=True)\n    qs.ask_user(d)\n", "def test_quickstart_valid_dir(tempdir):\n    answers = {\n        'Root path': tempdir,\n        'Project name': 'Sphinx Test',\n        'Author name': 'Georg Brandl',\n        'Project version': '0.1',\n    }\n    qs.term_input = mock_input(answers)\n    d = {}\n\n    # Create a file in the root directory to test that it is not a valid directory\n    with open(tempdir / 'conf.py', 'w') as f:\n        f.write('')\n\n    with pytest.raises(SystemExit):\n        qs.ask_user(d)\n\n    # Create a file in the source directory to test that it is not a valid directory\n    (tempdir / 'source').mkdir()\n    with open(tempdir / 'source' / 'conf.py', 'w') as f:\n        f.write('')\n\n    with pytest.raises(SystemExit):\n        qs.ask_user(d)\n\n    # Create a reserved file in the source directory to test that it is not a valid directory\n    (tempdir / 'source').mkdir()\n    with open(tempdir / 'source' / 'index.rst', 'w') as f:\n        f.write('')\n\n    with pytest.raises(SystemExit):\n        qs.ask_user(d)\n", "def test_valid_dir(tempdir):\n    # test for empty directory\n    d = {'path': str(tempdir)}\n    assert qs.valid_dir(d) is True\n\n    # test for non-existent directory\n    d = {'path': str(tempdir / 'non-existent')}\n    assert qs.valid_dir(d) is True\n\n    # test for existing directory with Makefile\n    d = {'path': str(tempdir)}\n    (tempdir / 'Makefile').touch()\n    assert qs.valid_dir(d) is False\n\n    # test for existing directory with make.bat\n    d = {'path': str(tempdir)}\n    (tempdir / 'make.bat').touch()\n    assert qs.valid_dir(d) is False\n\n    # test for existing directory with conf.py\n    d = {'path': str(tempdir)}\n    (tempdir / 'conf.py').touch()\n    assert qs.valid_dir(d) is False\n\n    # test for existing directory with _static\n    d = {'path': str(tempdir)}\n    (tempdir / '_static').mkdir()\n    assert qs.valid_dir(d) is False\n\n    # test for existing directory with _templates\n    d = {'path': str(tempdir)}\n    (tempdir / '_templates').mkdir()\n    assert qs.valid_dir(d) is False\n\n    # test for existing directory with master document\n    d = {'path': str(tempdir), 'master': 'index', 'suffix': '.rst'}\n    (tempdir / 'index.rst').touch()\n    assert qs.valid_dir(d) is False\n\n    # test for existing directory with separate source and build dirs\n    d = {'path': str(tempdir), 'sep': True}\n    (tempdir / 'source').mkdir()\n    assert qs.valid_dir(d) is True\n\n    # test for existing directory with separate source and build dirs and Makefile\n    d = {'path': str(tempdir), 'sep': True}\n    (tempdir / 'source').mkdir()\n    (tempdir / 'Makefile').touch()\n    assert qs.valid_dir(d) is False\n\n    # test for existing directory with separate source and build dirs and make.bat\n    d = {'path': str(tempdir), 'sep': True}\n    (tempdir / 'source').mkdir()\n    (tempdir / 'make.bat').touch()\n    assert qs.valid_dir(d) is False\n", "def test_valid_dir(tempdir):\n    # Test with a directory that does not exist\n    d = {'path': str(tempdir / 'nonexistent')}\n    assert qs.valid_dir(d) is True\n\n    # Test with a directory that is a file\n    with (tempdir / 'file.txt').open('w') as f:\n        d = {'path': str(tempdir / 'file.txt')}\n    assert qs.valid_dir(d) is False\n\n    # Test with a directory that contains a Makefile or make.bat\n    d = {'path': str(tempdir)}\n    (tempdir / 'Makefile').touch()\n    assert qs.valid_dir(d) is False\n    (tempdir / 'Makefile').unlink()\n    (tempdir / 'make.bat').touch()\n    assert qs.valid_dir(d) is False\n    (tempdir / 'make.bat').unlink()\n\n    # Test with a directory that contains a conf.py file\n    d = {'path': str(tempdir)}\n    (tempdir / 'conf.py').touch()\n    assert qs.valid_dir(d) is False\n    (tempdir / 'conf.py').unlink()\n\n    # Test with a directory that contains a _static or _templates directory\n    d = {'dot': '_'}\n    (tempdir / '_static').mkdir()\n    assert qs.valid_dir(d) is False\n    (tempdir / '_static').rmdir()\n    (tempdir / '_templates').mkdir()\n    assert qs.valid_dir(d) is False\n    (tempdir / '_templates').rmdir()\n\n    # Test with a directory that contains a master file\n    d = {'master': 'master'}\n    (tempdir / 'master.rst').touch()\n    assert qs.valid_dir(d) is False\n    (tempdir / 'master.rst').unlink()\n\n    # Test with a directory that does not contain any reserved names\n    d = {'path': str(tempdir)}\n    assert qs.valid_dir(d) is True\n"], "sample_1062": ["def test_TR22():\n    assert TR22(1 + tan(x)**2) == sec(x)**2\n    assert TR22(1 + tan(x)**-2) == tan(x)**-2\n    assert TR22(1 + tan(x)**4) == (sec(x)**2)**2\n    assert TR22(1 + cot(x)**2) == csc(x)**2\n    assert TR22(1 + cot(x)**-2) == cot(x)**-2\n    assert TR22(1 + cot(x)**4) == (csc(x)**2)**2\n", "def test_TR22():\n    assert TR22(tan(x)**2) == sec(x)**2 - 1\n    assert TR22(cot(x)**2) == csc(x)**2 - 1\n    assert TR22(tan(x)**-2) == tan(x)**-2\n    assert TR22(cot(x)**-2) == cot(x)**-2\n    assert TR22(tan(x)**6) == (sec(x)**2 - 1)**3\n    assert TR22(cot(x)**6) == (csc(x)**2 - 1)**3\n", "def test_TR14_sin():\n    eq = (sin(x) - 1)*(sin(x) + 1)\n    ans = -cos(x)**2\n    assert TR14(eq) == ans\n    assert TR14(1/eq) == 1/ans\n    assert TR14((sin(x) - 1)**2*(sin(x) + 1)**2) == ans**2\n    assert TR14((sin(x) - 1)**2*(sin(x) + 1)**3) == ans**2*(sin(x) + 1)\n    assert TR14((sin(x) - 1)**3*(sin(x) + 1)**2) == ans**2*(sin(x) - 1)\n    eq = (sin(x) - 1)**y*(sin(x) + 1)**y\n    assert TR14(eq) == eq\n    eq = (sin(x) - 2)**y*(sin(x) + 1)\n    assert TR14(eq) == eq\n    eq = (tan(x) - 2)**2*(sin(x) + 1)\n    assert TR14(eq) == eq\n    i = symbols('i', integer=True)\n    assert TR14((sin(x) - 1)**i*(sin(x) + 1)**i) == ans**i\n    assert TR14((cos(x) - 1)**i*(cos(x) + 1)**i) == (sin(x)**2)**i\n    # could use extraction in this case\n    eq = (sin(x) - 1)**(i + 1)*(sin(x) + 1)**i\n    assert TR14(eq) in [(sin(x) - 1)*ans**i, eq]\n", "def test_TR22():\n    assert TR22(tan(x)**2) == sec(x)**2 - 1\n    assert TR22(cot(x)**2) == csc(x)**2 - 1\n    assert TR22(1 + tan(x)**2) == sec(x)**2\n    assert TR22(1 + cot(x)**2) == csc(x)**2\n    assert TR22(tan(x)**4) == (sec(x)**2 - 1)**2\n    assert TR22(cot(x)**4) == (csc(x)**2 - 1)**2\n    assert TR22(tan(x)**-2) == 1/(sec(x)**2 - 1)\n    assert TR22(cot(x)**-2) == 1/(csc(x)**2 - 1)\n    assert TR22(tan(x)*tan(y)) == tan(x)*tan(y)\n", "def test_fu_hyperbolic():\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n    assert fu(sinh(x), measure=lambda x: x.count_ops()) == tanh(x*I)/I\n    assert fu(cosh(x), measure=lambda x: x.count_ops()) == cos(x*I)\n    assert fu(tanh(x), measure=lambda x: x.count_ops()) == tanh(x)\n    assert fu(coth(x), measure=lambda x: x.count_ops()) == coth(x)\n    assert fu(sech(x), measure=lambda x: x.count_ops()) == sec(x*I)\n    assert fu(csch(x), measure=lambda x: x.count_ops()) == csc(x*I)/I\n\n    assert fu(sinh(x)*sinh(y), measure=lambda x: x.count_ops()) == \\\n        sinh(x)*sinh(y)\n    assert fu(cosh(x)*cosh(y), measure=lambda x: x.count_ops()) == \\\n        cosh(x)*cosh(y)\n    assert fu(tanh(x)*tanh(y), measure=lambda x: x.count_ops()) == \\\n        tanh(x)*tanh(y)\n    assert fu(coth(x)*coth(y), measure=lambda x: x.count_ops()) == \\\n        coth(x)*coth(y)\n    assert fu(sech(x)*sech(y), measure=lambda x: x.count_ops()) == \\\n        sech(x)*sech(y)\n    assert fu(csch(x)*csch(y), measure=lambda x: x.count_ops()) == \\\n        csch(x)*csch(y)\n\n    assert fu(sinh(x)*sinh(y)*cosh(z), measure=lambda x: x.count_ops()) == \\\n        sinh(x)*sinh(y)*cosh(z)\n    assert fu(cosh(x)*tanh(y)*sech(z), measure=lambda x: x.count_ops()) == \\\n        cosh(x)*tanh(y)*sech(z)\n    assert fu(sinh(x)*tanh(y)*coth(z), measure=lambda x: x.count_ops()) == \\\n        sinh(x)*tanh(y)*coth(z)\n    assert fu(tanh(x)*coth(y)*csch(z), measure=lambda x: x.count_ops()) == \\\n        tanh(x)*coth(y)*csch(z)\n"], "sample_300": ["def test_filterable(self):\n    query = Query(Item)\n    msg = 'F is disallowed in the filter clause.'\n    with self.assertRaisesMessage(NotSupportedError, msg):\n        query.build_where(F('name'))\n", "def test_filter_annotation(self):\n    query = Query(Item)\n    query.add_annotation(Func('name', function='UPPER'), 'name_upper', select=False)\n    where = query.build_where(Q(name_upper='FOO'))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, Exact)\n    self.assertEqual(lookup.rhs, 'FOO')\n    self.assertIsInstance(lookup.lhs, Func)\n    self.assertEqual(lookup.lhs.target, Func('name', function='UPPER', output_field=CharField()))\n", "def test_exists(self):\n    query = Query(Item)\n    q = query.exists(using='default', limit=False)\n    self.assertEqual(q.low_mark, 0)\n    self.assertEqual(q.high_mark, 1)\n    self.assertEqual(q.select, (Col(alias='a', target=Item._meta.get_field('id'), output_field=Item._meta.get_field('id')),))\n\n    query = Query(Item)\n    q = query.exists(using='default', limit=True)\n    self.assertEqual(q.low_mark, 0)\n    self.assertEqual(q.high_mark, 1)\n    self.assertEqual(q.select, (Col(alias='a', target=Item._meta.get_field('id'), output_field=Item._meta.get_field('id')),))\n", "def test_add_ordering(self):\n    query = Query(Author)\n    query.add_ordering('num')\n    query.add_ordering('-name')\n    self.assertEqual(query.order_by, ('num', ('-name',)))\n    query.clear_ordering(True)\n    query.add_ordering('name')\n    query.add_ordering('?')\n    self.assertEqual(query.order_by, ('?','name',))\n", "def test_add_annotation(self):\n    query = Query(Author)\n    query.add_annotation(F('num') + F('num'), alias='num_squared', is_summary=True)\n    self.assertEqual(len(query.annotation_select), 1)\n    annotation = query.annotation_select['num_squared']\n    self.assertIsInstance(annotation, Func)\n    self.assertEqual(annotation.function, 'num + num')\n    self.assertEqual(annotation.output_field, IntegerField())\n\n    query = Query(Author)\n    query.add_annotation(F('num') + F('num'), alias='num_squared', is_summary=False)\n    self.assertEqual(len(query.annotation_select), 1)\n    annotation = query.annotation_select['num_squared']\n    self.assertIsInstance(annotation, Func)\n    self.assertEqual(annotation.function, 'num + num')\n    self.assertEqual(annotation.output_field, IntegerField())\n\n    query = Query(Author)\n    query.add_annotation(F('num') + F('num'), alias='num_squared', is_summary=True, select=False)\n    self.assertEqual(len(query.annotation_select), 0)\n    self.assertEqual(len(query.annotations), 1)\n    annotation = query.annotations['num_squared']\n    self.assertIsInstance(annotation, Func)\n    self.assertEqual(annotation.function, 'num + num')\n    self.assertEqual(annotation.output_field, IntegerField())\n"], "sample_1045": ["def test_Float_issue_14996():\n    assert same_and_same_prec(Float('1.12345678901234567890', 22), Float(1.12345678901234567890, 22))\n", "def test_issue_11341():\n    from sympy.core.numbers import comp\n    assert comp(1, 1.00000000000000000000000000000000001) is True\n    assert comp(1, 1.0000000000000000000000000000000001, 1e-30) is True\n    assert comp(1, 1.0000000000000000000000000000000001, 1e-31) is False\n", "def test_float_context():\n    from mpmath import mp\n    mp.dps = 5\n    assert float(Float(1)) == 1.0\n    assert str(Float(1)) == '1.00000'\n    mp.dps = 50\n    assert float(Float(1)) == 1.0\n    assert str(Float(1)) == '1.00000000000000000000000000000000000000000000000000'\n    mp.dps = 15\n", "def test_comp_with_nan():\n    assert comp(sqrt(2).n(10), S.NaN, tol=1) is False\n    assert comp(S(3), S.NaN, tol=1) is False\n    assert comp(S(0), S.NaN, tol=1) is False\n    assert comp(S.NaN, sqrt(2).n(10), tol=1) is False\n    assert comp(S.NaN, S(3), tol=1) is False\n    assert comp(S.NaN, S(0), tol=1) is False\n    assert comp(S.NaN, S.NaN, tol=1) is False\n", "def test_Rational_repr_latex():\n    # issue 10434\n    p = Rational(1, 2)\n    assert repr(p) == \"Rational(1, 2)\"\n    assert srepr(p) == \"Rational(1, 2)\"\n\n    assert latex(p) == r\"\\frac{1}{2}\"\n    assert latex(Rational(1, 1)) == \"1\"\n    assert latex(Rational(1, 2)) == r\"\\frac{1}{2}\"\n    assert latex(Rational(-1, 2)) == r\"-\\frac{1}{2}\"\n    assert latex(Rational(1, -2)) == r\"-\\frac{1}{2}\"\n    assert latex(Rational(-1, -2)) == r\"\\frac{1}{2}\"\n    assert latex(Rational(2, 2)) == \"1\"\n    assert latex(Rational(-2, 2)) == \"-1\"\n    assert latex(Rational(0, 2)) == \"0\"\n    assert latex(Rational(2, 1)) == \"2\"\n    assert latex(Rational(-2, 1)) == \"-2\"\n    assert latex(Rational(0, 1)) == \"0\"\n    assert latex(Rational(1, 3)) == r\"\\frac{1}{3}\"\n    assert latex(Rational(2, 3)) == r\"\\frac{2}{3}\"\n    assert latex(Rational(1, -3)) == r\"-\\frac{1}{3}\"\n    assert latex(Rational(2, -3)) == r\"-\\frac{2}{3}\"\n    assert latex(Rational(-1, 3)) == r\"-\\frac{1}{3}\"\n    assert latex(Rational(-2, 3)) == r\"- \\frac{2}{3}\"\n    assert latex(Rational(2, -3)) == r\"-\\frac{2}{3}\"\n    assert latex(Rational(-1, -3)) == r\"\\frac{1}{3}\"\n    assert latex(Rational(-2, -3)) == r\"\\frac{2}{3}\"\n"], "sample_1071": ["def test_check_dimensions_with_pow():\n    x = symbols('x')\n    assert check_dimensions(inch**2 + x**2) == inch**2 + x**2\n    raises(ValueError, lambda: check_dimensions(inch**2 + 1))\n    raises(ValueError, lambda: check_dimensions(length**2 + 1))\n    raises(ValueError, lambda: check_dimensions(length**2 + time**2))\n    raises(ValueError, lambda: check_dimensions(meter**2 + second**2))\n    raises(ValueError, lambda: check_dimensions(2 * meter**2 + second**2))\n    raises(ValueError, lambda: check_dimensions(2 * meter**2 + 3 * second**2))\n    raises(ValueError, lambda: check_dimensions(1 / second**2 + 1 / meter**2))\n    raises(ValueError, lambda: check_dimensions(2 * meter**2*(mile + centimeter)**2 + km**2))\n", "def test_convert_to_dimensionless():\n    # Test conversion of dimensionless quantities\n    assert convert_to(pi, meter) == pi\n    assert convert_to(pi, second) == pi\n    assert convert_to(pi, kilogram) == pi\n    assert convert_to(pi, joule) == pi\n    assert convert_to(pi, speed_of_light) == pi\n    assert convert_to(pi, planck) == pi\n    assert convert_to(pi, steradian) == pi\n    assert convert_to(pi, radians) == pi\n    assert convert_to(pi, degree) == pi\n\n    # Test conversion of dimensionless quantities to multiple units\n    assert convert_to(pi, [meter, second]) == pi\n    assert convert_to(pi, [meter, kilogram]) == pi\n    assert convert_to(pi, [joule, speed_of_light]) == pi\n    assert convert_to(pi, [steradian, radians]) == pi\n    assert convert_to(pi, [degree, pi]) == pi\n", "def test_convert_to_non_quantities():\n    raises(TypeError, lambda: convert_to(1, meter))\n    raises(TypeError, lambda: convert_to(x, meter))\n    raises(TypeError, lambda: convert_to(pi, meter))\n    raises(TypeError, lambda: convert_to(sin(x), meter))\n", "def test_check_dimensions_add_with_prefixes():\n    assert check_dimensions(kilo*inch + inch) == kilo*inch + inch\n    assert check_dimensions(kilo*inch + kilo*inch) == 2*kilo*inch\n    assert check_dimensions(centimeter + kilo*centimeter) == centimeter + kilo*centimeter\n    assert check_dimensions(centimeter + kilo*centimeter + meter) == centimeter + kilo*centimeter + 100*centimeter\n    raises(ValueError, lambda: check_dimensions(kilo*inch + 1))\n    raises(ValueError, lambda: check_dimensions(kilo*inch + 2*kilo))\n    raises(ValueError, lambda: check_dimensions(kilo*inch + kilo*second))\n", "def test_convert_to_edge_cases():\n    assert convert_to(speed_of_light, []) == speed_of_light\n    assert convert_to(speed_of_light, [speed_of_light]) == speed_of_light\n    assert convert_to(0, meter) == 0\n    assert convert_to(0, [meter, second]) == 0\n    assert convert_to(speed_of_light, 0) == speed_of_light\n    assert convert_to(speed_of_light, [0, 0]) == speed_of_light\n\n    # Test with unsupported target units\n    raises(ValueError, lambda: convert_to(speed_of_light, sin(speed_of_light)))\n    raises(ValueError, lambda: convert_to(speed_of_light, [sin(speed_of_light), speed_of_light]))\n\n    # Test with incompatible target units\n    raises(ValueError, lambda: convert_to(meter, joule))\n    raises(ValueError, lambda: convert_to(meter, [joule, speed_of_light]))\n\n    # Test with incompatible target units ( Pow )\n    raises(ValueError, lambda: convert_to(meter, speed_of_light**2))\n    raises(ValueError, lambda: convert_to(meter, [speed_of_light**2, speed_of_light]))\n"], "sample_467": ["def test_value_from_datadict_with_invalid_year(self):\n    tests = [\n        ((str(sys.maxsize + 1), \"12\", \"1\"), \"0-0-0\"),\n        ((-1, \"12\", \"1\"), \"0-0-0\"),\n    ]\n    for values, expected in tests:\n        with self.subTest(values=values):\n            data = {}\n            for field_name, value in zip((\"year\", \"month\", \"day\"), values):\n                if value is not None:\n                    data[\"field_%s\" % field_name] = value\n            self.assertEqual(\n                self.widget.value_from_datadict(data, {}, \"field\"), expected\n            )\n", "def test_value_from_datadict_with_invalid_year(self):\n    data = {\n        \"field_year\": \"10000\",\n        \"field_month\": \"12\",\n        \"field_day\": \"1\",\n    }\n    self.assertEqual(self.widget.value_from_datadict(data, {}, \"field\"), \"0-0-0\")\n", "def test_format_value_unparsable(self):\n    # Regression test for #25817\n    value = \"some invalid date string\"\n    with self.assertRaises(ValueError):\n        self.widget.format_value(value)\n", "def test_date_input_format(self):\n    w = SelectDateWidget(\n        years=(\"2007\", \"2008\", \"2009\", \"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\"),\n    )\n    with self.subTest():\n        self.assertEqual(w.format_value(\"2023-02-20\"), {\"day\": 20, \"month\": 2, \"year\": 2023})\n\n    with self.subTest():\n        self.assertEqual(w.format_value(\"2023-02-\"), {\"day\": \"\", \"month\": 2, \"year\": 2023})\n\n    with self.subTest():\n        self.assertEqual(w.format_value(\"2023--20\"), {\"day\": 20, \"month\": \"\", \"year\": 2023})\n\n    with self.subTest():\n        self.assertEqual(w.format_value(\"-02-20\"), {\"day\": 20, \"month\": 2, \"year\": \"\"})\n\n    with self.subTest():\n        self.assertEqual(w.format_value(\"2023-02-\"), {\"day\": \"\", \"month\": 2, \"year\": 2023})\n\n    with self.subTest():\n        self.assertEqual(w.format_value(\"2023--\"), {\"day\": \"\", \"month\": \"\", \"year\": 2023})\n\n    with self.subTest():\n        self.assertEqual(w.format_value(\"-02-\"), {\"day\": \"\", \"month\": 2, \"year\": \"\"})\n\n    with self.subTest():\n        self.assertEqual(w.format_value(\"-02\"), {\"day\": \"\", \"month\": 2, \"year\": \"\"})\n\n    with self.subTest():\n        self.assertEqual(w.format_value(\"2023-\"), {\"day\": \"\", \"month\": \"\", \"year\": 2023})\n\n    with self.subTest():\n        self.assertEqual(w.format_value(\"2023\"), {\"day\": \"\", \"month\": \"\", \"year\": 2023})\n\n    with self.subTest():\n        self.assertEqual(w.format_value(\"\"), {\"day\": \"\", \"month\": \"\", \"year\": \"\"})\n\n    with self.subTest():\n        self.assertEqual(w.format_value(None), {\"day\": None, \"month\": None, \"year\": None})\n", "def test_render_invalid_input_types(self):\n    \"\"\"\n    Test rendering the widget with invalid input types.\n    \"\"\"\n    # Test rendering with invalid input type\n    with self.assertRaises(ValueError):\n        SelectDateWidget(years=(\"2007\",), months=\"Invalid\")\n\n    # Test rendering with invalid input type\n    with self.assertRaises(ValueError):\n        SelectDateWidget(years=(\"2007\",), empty_label=(\"Invalid\", \"tuple\"))\n\n    # Test rendering with empty input type\n    w = SelectDateWidget(years=())\n    self.assertHTMLEqual(\n        w.render(\"mydate\", \"\"),\n        \"\"\"\n        <select name=\"mydate_month\" id=\"id_mydate_month\">\n            <option selected value=\"\">---</option>\n        </select>\n\n        <select name=\"mydate_day\" id=\"id_mydate_day\">\n            <option selected value=\"\">---</option>\n        </select>\n\n        <select name=\"mydate_year\" id=\"id_mydate_year\">\n            <option selected value=\"\">---</option>\n        </select>\n        \"\"\",\n    )\n"], "sample_593": ["def test_collapsible_section():\n    name = \"Test Section\"\n    inline_details = \"Inline details\"\n    details = \"Details\"\n    n_items = 5\n    enabled = True\n    collapsed = False\n\n    section = fh.collapsible_section(\n        name, inline_details, details, n_items, enabled, collapsed\n    )\n\n    assert name in section\n    assert inline_details in section\n    assert details in section\n    assert str(n_items) in section\n    assert \"disabled\" not in section\n    assert \"checked\" not in section\n\n    section = fh.collapsible_section(name, inline_details, details, n_items, False, True)\n    assert \"disabled\" in section\n    assert \"checked\" in section\n", "def test_collapsible_section():\n    # Test the collapsible section with different parameters\n    assert fh.collapsible_section(\"Test\", n_items=5).count(\"class='xr-section-summary-in' type='checkbox' \") == 1\n    assert fh.collapsible_section(\"Test\", n_items=0).count(\"class='xr-section-summary-in' type='checkbox' disabled\") == 1\n    assert fh.collapsible_section(\"Test\", n_items=5, collapsed=True).count(\"class='xr-section-summary-in' type='checkbox' checked\") == 1\n    assert fh.collapsible_section(\"Test\", n_items=5, enabled=False).count(\"class='xr-section-summary-in' type='checkbox' disabled\") == 1\n", "def test_collapsible_section_enabled_collapsed():\n    name = \"Test Section\"\n    enabled = True\n    collapsed = True\n    formatted = fh.collapsible_section(name, enabled=enabled, collapsed=collapsed)\n    assert f\"<input id='section-{uuid.UUID('')\" in formatted\n    assert \"class='xr-section-summary-in' type='checkbox'  checked>\" not in formatted\n    assert \"class='xr-section-summary-in' type='checkbox' >\" in formatted\n", "def test_collapsible_section_enabled_with_items():\n    name = \"Test Section\"\n    details = \"Some details\"\n    n_items = 5\n    enabled = True\n    collapsed = False\n    section = fh.collapsible_section(\n        name, inline_details=\"\", details=details, n_items=n_items, enabled=enabled, collapsed=collapsed\n    )\n    assert name in section\n    assert details in section\n    assert str(n_items) in section\n    assert \"checked\" not in section\n    assert \"disabled\" not in section\n\n", "def test_collapsible_section_enabled():\n    formatted = fh.collapsible_section(\"Test Section\", \"\", \"\", enabled=True)\n    assert \"disabled\" not in formatted\n\n"], "sample_712": ["def test_ordinal_encoder_unsorted_categories():\n    X = np.array([[1, 2], [3, 4]])\n    enc = OrdinalEncoder(categories=[[2, 1], [4, 3]])\n    exp = np.array([[1, 0], [0, 1]])\n    assert_array_equal(enc.fit(X).transform(X), exp.astype('float64'))\n    enc = OrdinalEncoder(dtype='int64')\n    assert_array_equal(enc.fit(X).transform(X), exp)\n\n    # unsorted passed categories still raise for numerical values\n    X = np.array([[1, 2], [3, 4]])\n    enc = OrdinalEncoder(categories=[[2, 1, 4], [4, 3]])\n    msg = 'Unsorted categories are not supported'\n    with pytest.raises(ValueError, match=msg):\n        enc.fit(X)\n", "def test_ordinal_encoder_handle_unknown():\n    X = [['a', 2, 1], ['b', 3, 2], ['a', 3, 2]]\n    X2 = [['c', 3, 1]]\n\n    # Test that OrdinalEncoder raises error for unknown features present\n    # during transform.\n    oe = OrdinalEncoder()\n    oe.fit(X)\n    assert_raises(ValueError, oe.transform, X2)\n\n    # Test the ignore option, ignores unknown features (giving all 0's)\n    oe = OrdinalEncoder(handle_unknown='ignore')\n    oe.fit(X)\n    X2_passed = X2.copy()\n    assert_array_equal(\n        oe.transform(X2_passed),\n        np.array([[None, 2., 1.]]))\n    # ensure transformed data was not modified in place\n    assert_allclose(X2, X2_passed)\n\n    # Raise error if handle_unknown is neither ignore or error.\n    oe = OrdinalEncoder(handle_unknown='42')\n    assert_raises(ValueError, oe.fit, X)\n", "def test_ordinal_encoder_handle_unknown(X):\n    enc = OrdinalEncoder(handle_unknown='error')\n    assert_warns(FutureWarning, enc.fit, X)\n    assert_raises(ValueError, enc.transform, [['unknown', 1, 1]])\n    enc = OrdinalEncoder(handle_unknown='ignore')\n    enc.fit(X)\n    assert_array_equal(enc.transform([['unknown', 1, 1]]), np.array([[0, 0, 0]]))\n    assert_array_equal(enc.inverse_transform(enc.transform(X)), np.array(X, dtype=object))\n    assert_array_equal(enc.inverse_transform(enc.transform([['unknown', 1, 1]])), np.array([['unknown', 1, 1]], dtype=object))\n", "def test_ordinal_encoder_categories():\n    X = np.array([['a', 'b'], ['c', 'd']])\n\n    # test categories are not sorted\n    enc = OrdinalEncoder(categories=[['b', 'a']])\n    msg = 'Unsorted categories are not supported for numerical categories'\n    with pytest.raises(ValueError, match=msg):\n        enc.fit(X)\n\n    # test categories are not sorted for multiple columns\n    enc = OrdinalEncoder(categories=[['b', 'a'], ['d', 'c']])\n    with pytest.raises(ValueError, match=msg):\n        enc.fit(X)\n\n    # test categories do not match input data\n    enc = OrdinalEncoder(categories=[['a', 'c']])\n    msg = 'Found unknown categories {0} in column {1} during fit'\n    with pytest.raises(ValueError, match=msg.format('b', 0)):\n        enc.fit(X)\n\n    # test categories do not match input data for multiple columns\n    enc = OrdinalEncoder(categories=[['a', 'c'], ['d']])\n    with pytest.raises(ValueError, match=msg.format('b', 0)):\n        enc.fit(X)\n\n    # test categories match input data\n    enc = OrdinalEncoder(categories=[['a', 'b'], ['c', 'd']])\n    X_tr = enc.fit_transform(X)\n    assert_array_equal(X_tr, np.array([[0, 0], [1, 1]]))\n    assert_array_equal(enc.inverse_transform(X_tr), X)\n", "def test_ordinal_encoder_input_types():\n    # test support for input types and correct dtype inference\n    for X in [np.array([['a', 'b'], ['b', 'c']]),\n              np.array([[1, 2], [2, 3]], dtype='int64'),\n              np.array([[1., 2.], [2., 3.]], dtype='float64'),\n              pd.DataFrame({'A': ['a', 'b'], 'B': ['b', 'c']}),\n              pd.DataFrame({'A': [1, 2], 'B': [2, 3]}, dtype='int64')]:\n        enc = OrdinalEncoder(dtype='int64')\n        assert_array_equal(enc.fit_transform(X).dtype, 'int64')\n        assert_array_equal(enc.inverse_transform(enc.transform(X)).dtype,\n                           X.dtype)\n\n    # check dtype casting\n    X = np.array([[1, 2], [2, 3]], dtype='float32')\n    enc = OrdinalEncoder(dtype='int64')\n    assert_array_equal(enc.fit_transform(X).dtype, 'int64')\n    assert_array_equal(enc.inverse_transform(enc.transform(X)).dtype,\n                       X.dtype)\n\n    # check error is raised for invalid input type\n    X = np.array([[1, 2], [2, 3]], dtype='float64')\n    enc = OrdinalEncoder(dtype='int32')\n    assert_raises(ValueError, enc.fit_transform, X)\n"], "sample_108": ["    def test_locale_prefix_match(self):\n        locale_prefix = LocalePrefixPattern()\n        match = locale_prefix.match('/en/')\n        self.assertEqual(match, ('', (), {}))\n", "    def test_get_descriptor_returns_regex(self):\n        pattern = RegexPattern(r'hello')\n        self.assertIsInstance(pattern.regex, re.Pattern)\n", "    def test_resolver_match(self):\n        match = resolve('/articles/2003/')\n        resolver_match = ResolverMatch(\n            match.func,\n            match.args,\n            match.kwargs,\n            match.url_name,\n            match.app_names,\n            match.namespaces,\n            match.route,\n        )\n        self.assertEqual(resolver_match.func, match.func)\n        self.assertEqual(resolver_match.args, match.args)\n        self.assertEqual(resolver_match.kwargs, match.kwargs)\n        self.assertEqual(resolver_match.url_name, match.url_name)\n        self.assertEqual(resolver_match.app_name, match.app_name)\n        self.assertEqual(resolver_match.namespace, match.namespace)\n        self.assertEqual(resolver_match.route, match.route)\n        self.assertEqual(resolver_match[0], match.func)\n        self.assertEqual(resolver_match[1], match.args)\n        self.assertEqual(resolver_match[2], match.kwargs)\n        self.assertEqual(repr(resolver_match), repr(match))\n", "    def test_compile_regex_for_each_language(self):\n        # Create a regex pattern with a lazily-translated string.\n        from django.utils.translation import gettext_lazy as _\n        pattern = RegexPattern(_('^/hello/'), name='hello')\n        \n        # Compile regex for the default language.\n        language_code = settings.LANGUAGE_CODE\n        self.assertIn(language_code, pattern._regex_dict)\n        self.assertEqual(pattern.regex.pattern, r'^/hello/')\n        \n        # Compile regex for a different language.\n        with override_settings(LANGUAGE_CODE='fr'):\n            self.assertEqual(pattern.regex.pattern, r'^/bonjour/')\n        \n        # Check that the original regex is restored when the language changes back.\n        with override_settings(LANGUAGE_CODE=settings.LANGUAGE_CODE):\n            self.assertEqual(pattern.regex.pattern, r'^/hello/')    \n", "    def test_locale_prefix(self):\n        pattern = LocalePrefixPattern()\n        self.assertEqual(pattern.match('/en/').pattern, '/en/')\n"], "sample_531": ["def test_subfigure_subplotspec():\n    fig = plt.figure()\n    gs = fig.add_gridspec(2, 2)\n    subfig = fig.add_subfigure(gs[1, 1])\n\n    assert subfig.get_subplotspec() is gs[1, 1]\n\n    gs2 = subfig.add_gridspec(2, 2)\n    subfig2 = subfig.add_subfigure(gs2[0, 0])\n\n    assert subfig2.get_subplotspec() is gs2[0, 0]\n", "def test_subfigure_layout_engine():\n    # test that subfigures can be assigned their own layout engine\n    fig = Figure(layout='constrained')\n    subfig = fig.add_subfigure((0, 0, 1, 1), layout='tight')\n    assert isinstance(fig.get_layout_engine(), ConstrainedLayoutEngine)\n    assert isinstance(subfig.get_layout_engine(), TightLayoutEngine)\n\n    fig = Figure(layout='tight')\n    subfig = fig.add_subfigure((0, 0, 1, 1), layout='constrained')\n    assert isinstance(fig.get_layout_engine(), TightLayoutEngine)\n    assert isinstance(subfig.get_layout_engine(), ConstrainedLayoutEngine)\n\n    fig = Figure(layout=None)\n    subfig = fig.add_subfigure((0, 0, 1, 1), layout='compressed')\n    assert isinstance(fig.get_layout_engine(), PlaceHolderLayoutEngine)\n    assert isinstance(subfig.get_layout_engine(), ConstrainedLayoutEngine)\n", "def test_get_tightbbox_with_subplotspec(fig_test, fig_ref):\n    fig_test, ax_test = fig_test.subplots(layout='constrained')\n    fig_ref, ax_ref = fig_ref.subplots(layout='constrained')\n    ax_test.set_subplotspec(gridspec.GridSpec(1, 2)[0, 1])\n    ax_ref.set_subplotspec(gridspec.GridSpec(1, 2)[0, 1])\n", "def test_subfigure_dpi():\n    fig = plt.figure(layout='constrained')\n    sub = fig.subfigures(1, 2)\n\n    axs = sub[0].subplots(2, 2)\n    for ax in axs.flat:\n        pc = ax.pcolormesh(np.random.randn(30, 30), vmin=-2, vmax=2)\n    sub[0].colorbar(pc, ax=axs)\n    sub[0].suptitle('Left Side')\n\n    axs = sub[1].subplots(1, 3)\n    for ax in axs.flat:\n        pc = ax.pcolormesh(np.random.randn(30, 30), vmin=-2, vmax=2)\n    sub[1].colorbar(pc, ax=axs, location='bottom')\n    sub[1].suptitle('Right Side')\n    fig.dpi = 300\n\n    fig2 = plt.figure(layout='constrained')\n    sub2 = fig2.subfigures(1, 2)\n\n    axs2 = sub2[0].subplots(2, 2)\n    for ax in axs2.flat:\n        pc = ax.pcolormesh(np.random.randn(30, 30), vmin=-2, vmax=2)\n    sub2[0].colorbar(pc, ax=axs2)\n    sub2[0].suptitle('Left Side')\n\n    axs2 = sub2[1].subplots(1, 3)\n    for ax in axs2.flat:\n        pc = ax.pcolormesh(np.random.randn(30, 30), vmin=-2, vmax=2)\n    sub2[1].colorbar(pc, ax=axs2, location='bottom')\n    sub2[1].suptitle('Right Side')\n\n    fig2.dpi = 100\n", "def test_tightbbox_image():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[0, 1], [0, 1]])\n    ax.set_xlim(-.1, 1.1)\n    ax.set_ylim(-.1, 1.1)\n    tbb = ax.get_tightbbox(fig.canvas.get_renderer())\n    np.testing.assert_allclose(tbb.width, 122., atol=2)\n    np.testing.assert_allclose(tbb.height, 122., atol=2)\n    np.testing.assert_allclose(tbb.x0, 77., atol=2)\n    np.testing.assert_allclose(tbb.y0, 77., atol=2)\n    im.set_in_layout(False)\n    tbb = ax.get_tightbbox(fig.canvas.get_renderer())\n    np.testing.assert_allclose(tbb.width, 122., atol=2)\n    np.testing.assert_allclose(tbb.height, 122., atol=2)\n    np.testing.assert_allclose(tbb.x0, 77., atol=2)\n    np.testing.assert_allclose(tbb.y0, 77., atol=2)\n    fig.set_size_inches(6, 6)\n    tbb = ax.get_tightbbox(fig.canvas.get_renderer())\n    np.testing.assert_allclose(tbb.width, 366., atol=2)\n    np.testing.assert_allclose(tbb.height, 366., atol=2)\n    np.testing.assert_allclose(tbb.x0, 77., atol=2)\n    np.testing.assert_allclose(tbb.y0, 77., atol=2)\n    im.set_in_layout(False)\n    tbb = ax.get_tightbbox(fig.canvas.get_renderer())\n    np.testing.assert_allclose(tbb.width, 366., atol=2)\n    np.testing.assert_allclose(tbb.height, 366., atol=2)\n    np.testing.assert_allclose(tbb.x0, 77., atol=2)\n    np.testing.assert_allclose(tbb.y0, 77., atol=2)\n    fig.set_size_inches(3, 3)\n    tbb = ax.get_tightbbox(fig.canvas.get_renderer())\n    np.testing.assert_allclose(tbb.width, 122., atol=2)\n    np.testing.assert_allclose(tbb.height, 122., atol=2)\n    np.testing.assert_allclose(tbb.x0, 77., atol=2)\n    np.testing.assert_allclose(tbb.y0, 77"], "sample_928": ["def test_default_role():\n    with default_role('dummy.rst', 'emphasis'):\n        assert docutils.get_role('emphasis') is not None\n    assert docutils.get_role('emphasis') is None\n", "def test_default_role():\n    with default_role('dummy.rst', 'emphasis'):\n        assert docutils.get_role('') == roles.emphasis\n    with default_role('dummy.rst', 'nonexistent'):\n        assert docutils.get_role('') == roles.emphasis  # still the previous role\n    with default_role('dummy.rst', None):\n        assert docutils.get_role('') == None\n    with default_role('dummy.rst', ''):\n        assert docutils.get_role('') == None\n", "def test_default_role(app):\n    docname = 'test_document'\n    name = 'emphasis'\n\n    # test a role that exists\n    with default_role(docname, name):\n        assert docutils.get_role('') == roles.emphasis\n\n    # test a role that does not exist\n    with default_role(docname, 'nonexistent_role'):\n        assert docutils.get_role('') is None\n\n    # test no role\n    with default_role(docname, None):\n        assert docutils.get_role('') is None\n", "def test_default_role():\n    docname = 'test_document'\n    role_name = 'emphasis'\n    with default_role(docname, role_name):\n        assert roles.get_role('')(None, 'emphasis', 'some_text', 0, None) == ('some_text', ['emphasis'])\n\n    with default_role(docname, 'nonexistent_role'):\n        assert roles.get_role('')(None, 'nonexistent_role', 'some_text', 0, None) is None\n\n    with default_role(docname, None):\n        assert roles.get_role('')(None, None, 'some_text', 0, None) is None\n", "def test_default_role(app):\n    docname = 'test'\n    name = 'emphasis'\n\n    with default_role(docname, name):\n        assert docutils.get_role('') == roles.emphasis\n\n    with default_role(docname, 'unknown'):\n        assert docutils.get_role('') is None\n\n    # Test that roles are unregistered after use\n    assert docutils.get_role('') is None\n\n    # Test that registering a role that does not exist results in a warning\n    with default_role(docname, 'unknown'):\n        pass\n"], "sample_590": ["def test_concat_empty_dataset():\n    # test that concatenating with an empty dataset works\n    ds1 = Dataset({\"a\": (\"x\", [1, 2])})\n    ds2 = Dataset()\n    actual = concat([ds1, ds2], \"x\")\n    expected = Dataset({\"a\": (\"x\", [1, 2, np.nan])})\n    assert_identical(actual, expected)\n", "    def test_concat_with_empty_dataset(self):\n        data = create_test_data()\n        split_data = [data.isel(dim1=slice(3)), data.isel(dim1=slice(3, None))]\n\n        empty_dataset = Dataset()\n\n        # Test concatenating an empty dataset to a non-empty dataset\n        actual = concat([data, empty_dataset], dim=\"dim1\")\n        assert_identical(data, actual)\n\n        # Test concatenating a non-empty dataset to an empty dataset\n        actual = concat([empty_dataset, data], dim=\"dim1\")\n        assert_identical(data, actual)\n\n        # Test concatenating two empty datasets\n        actual = concat([empty_dataset, empty_dataset], dim=\"dim1\")\n        assert_identical(empty_dataset, actual)\n\n        # Test concatenating an empty dataset to a list of non-empty datasets\n        actual = concat([data, empty_dataset, data], dim=\"dim1\")\n        assert_identical(concat([data, data], dim=\"dim1\"), actual)\n", "    def test_concat_dataset_with_variable_dim(self):\n        # Test concatenating a dataset where the dimension to concatenate along\n        # is a variable in one of the datasets\n        ds1 = Dataset({\"x\": (\"y\", [1, 2])})\n        ds2 = Dataset({\"x\": (\"z\", [3, 4])})\n        dim = Variable(\"y\", [0, 1])\n        expected = Dataset({\"x\": ((\"y\", \"z\"), [[1, 2], [3, 4]])}, {\"y\": [0, 1]})\n        actual = concat([ds1, ds2], dim, coords=\"all\")\n        assert_identical(expected, actual)\n", "def test_concat_dim_coord_length_mismatch():\n    # Test that an error is raised when the length of the concatenation\n    # dimension does not match the length of the objects being concatenated.\n    ds1 = Dataset({\"a\": ((\"x\", \"y\"), np.random.randn(2, 3))}, {\"x\": [0, 1]})\n    ds2 = Dataset({\"a\": ((\"x\", \"y\"), np.random.randn(3, 3))}, {\"x\": [0, 1, 2]})\n    with raises_regex(ValueError, \"concat dimension lengths are not equal\"):\n        concat([ds1, ds2], dim=(\"x\", [0, 1, 2, 3]))\n", "    def test_concat_invalid_compat(self):\n        ds1 = Dataset({\"a\": 1}, {\"b\": 1})\n        ds2 = Dataset({\"a\": 2}, {\"b\": 1})\n        with raises_regex(ValueError, \"compat.* invalid\"):\n            concat([ds1, ds2], \"c\", compat=\"invalid\")\n"], "sample_550": ["def test_axes_sharex_sharey():\n    fig, axs = plt.subplots(2, 2, sharex=True, sharey=True)\n    assert axs[0, 0].get_shared_x_axes() == axs[0, 0].get_shared_y_axes()\n    axs[0, 0].set_xlim(1, 2)\n    axs[0, 0].set_ylim(1, 2)\n    for ax in axs.flat:\n        assert ax.get_xlim() == (1, 2)\n        assert ax.get_ylim() == (1, 2)\n    axs[0, 0].set_xlim(3, 4)\n    axs[0, 0].set_ylim(3, 4)\n    for ax in axs.flat:\n        assert ax.get_xlim() == (3, 4)\n        assert ax.get_ylim() == (3, 4)\n", "def test_axes_init():\n    fig, ax = plt.subplots()\n    assert ax._unit_change_handler(\"x\") is not None\n    assert ax._unit_change_handler(\"y\") is not None\n\n    ax.callbacks.disconnect(\"units\", ax._unit_change_handler(\"x\"))\n    ax.callbacks.disconnect(\"units\", ax._unit_change_handler(\"y\"))\n\n    fig, ax = plt.subplots()\n    assert ax._unit_change_handler(\"x\") is not None\n    assert ax._unit_change_handler(\"y\") is not None\n", "def test_Axes_set_aspect():\n    fig, ax = plt.subplots()\n    ax.set_aspect('auto')\n    assert ax.get_aspect() == 'auto'\n\n    ax.set_aspect('equal')\n    assert ax.get_aspect() == 1\n\n    ax.set_aspect(2)\n    assert ax.get_aspect() == 2\n\n    ax.set_aspect('foo')\n    assert ax.get_aspect() == 'auto'  # Default to auto\n", "def test_axes_set_aspect():\n    fig, ax = plt.subplots()\n    ax.set_aspect('auto')\n    assert ax.get_aspect() == 'auto'\n\n    ax.set_aspect(1)\n    assert ax.get_aspect() == 1\n\n    ax.set_aspect('equal')\n    assert ax.get_aspect() == 1\n\n    ax.set_aspect('auto', adjustable='box', anchor='C')\n    assert ax.get_aspect() == 'auto'\n    assert ax.get_adjustable() == 'box'\n    assert ax.get_anchor() == 'C'\n\n    ax.set_aspect(2, adjustable='datalim', anchor='NW')\n    assert ax.get_aspect() == 2\n    assert ax.get_adjustable() == 'datalim'\n    assert ax.get_anchor() == 'NW'\n\n    ax.set_aspect('equal', adjustable='box', anchor='SE')\n    assert ax.get_aspect() == 1\n    assert ax.get_adjustable() == 'box'\n    assert ax.get_anchor() == 'SE'\n\n    ax.set_aspect('auto')\n    assert ax.get_aspect() == 'auto'\n\n    ax.set_aspect(None)\n    assert ax.get_aspect() == None\n", "def test_axis_limits():\n    fig, ax = plt.subplots()\n    # Test the case where x, y limits are both auto\n    ax.set_xlim(auto=True)\n    ax.set_ylim(auto=True)\n    assert ax.get_xlim() == (0, 1)\n    assert ax.get_ylim() == (0, 1)\n\n    # Test the case where x limit is set, but y limit is auto\n    ax.set_xlim(1, 2)\n    ax.set_ylim(auto=True)\n    assert ax.get_xlim() == (1, 2)\n    assert ax.get_ylim() == (0, 1)\n\n    # Test the case where y limit is set, but x limit is auto\n    ax.set_xlim(auto=True)\n    ax.set_ylim(1, 2)\n    assert ax.get_xlim() == (0, 1)\n    assert ax.get_ylim() == (1, 2)\n\n    # Test the case where both x and y limits are set\n    ax.set_xlim(1, 2)\n    ax.set_ylim(1, 2)\n    assert ax.get_xlim() == (1, 2)\n    assert ax.get_ylim() == (1, 2)\n\n    # Test the case where x limit is set to be inverted\n    ax.set_xlim(2, 1)\n    ax.set_ylim(1, 2)\n    assert ax.get_xlim() == (2, 1)\n    assert ax.get_ylim() == (1, 2)\n\n    # Test the case where y limit is set to be inverted\n    ax.set_xlim(1, 2)\n    ax.set_ylim(2, 1)\n    assert ax.get_xlim() == (1, 2)\n    assert ax.get_ylim() == (2, 1)\n"], "sample_1151": ["def test_Mod_is_imaginary_real():\n    i = Symbol('i', imaginary=True)\n    r = Symbol('r', real=True)\n    assert Mod(i, 1).is_imaginary is None\n    assert Mod(i, r).is_imaginary is None\n    assert Mod(r, i).is_imaginary is None\n    assert Mod(r, r).is_imaginary is False\n    assert Mod(r, 1).is_imaginary is False\n    assert Mod(i, i).is_imaginary is False\n    assert Mod(i, -1).is_imaginary is False\n", "def test_Mod_with_unbounded_symbols():\n    x, y, z = symbols('x y z', unbounded=True)\n    assert (x % y).is_finite is False\n    assert (x % z).is_finite is False\n    assert (y % z).is_finite is False\n    assert (y % x).is_finite is False\n    assert (z % x).is_finite is False\n    assert (z % y).is_finite is False\n", "def test_modulo_handling():\n    x, y = symbols('x y', real=True)\n    assert (Mod(x, y) + 3).is_nonnegative is None\n    assert (Mod(x, y) - 3).is_nonpositive is None\n    assert (Mod(x, y) * 3).is_nonnegative is None\n    assert (Mod(x, y) / 3).is_nonnegative is None\n", "def test_Mul_with_incompatible_properties():\n    c = Symbol('c', complex=True)\n    r = Symbol('r', real=True)\n    p = Symbol('p', positive=True)\n    n = Symbol('n', negative=True)\n    assert (c*r).is_real is False\n    assert (c*p).is_real is False\n    assert (c*n).is_real is False\n    assert (c*p).is_positive is False\n    assert (c*n).is_positive is False\n    assert (c*p).is_negative is False\n    assert (c*n).is_negative is False\n    assert (c*p).is_nonpositive is None\n    assert (c*n).is_nonpositive is None\n    assert (c*p).is_nonnegative is None\n    assert (c*n).is_nonnegative is None\n", "def test_Mod_is_finite():\n    p = Symbol('p', integer=True, positive=True)\n    n = Symbol('n', integer=True, nonnegative=True)\n    r = Symbol('r', real=True)\n    z = Symbol('z', zero=True)\n\n    assert (x % p).is_finite is None\n    assert (r % p).is_finite is True\n    assert (z % p).is_finite is True\n    assert (n % p).is_finite is True\n    assert (z % n).is_finite is None\n    assert (n % r).is_finite is None\n    assert (r % r).is_finite is None\n    assert (r % z).is_finite is True\n    assert (z % z).is_finite is True\n"], "sample_1099": ["def test_eval_partial_derivative_divergence_type_with_different_tensors():\n    expr1a = PartialDerivative(A(i), B(i))\n    expr1b = PartialDerivative(A(i), B(k))\n    expr1c = PartialDerivative(L.delta(-i, k) * A(i), B(k))\n\n    assert (expr1a._perform_derivative()\n            - (L.delta(-i, k) * expr1b._perform_derivative())).contract_delta(L.delta) == 0\n\n    assert (expr1a._perform_derivative()\n            - expr1c._perform_derivative()).contract_delta(L.delta) == 0\n\n    expr2a = PartialDerivative(H(i, j), H(i, j))\n    expr2b = PartialDerivative(H(i, j), B(m))\n    expr2c = PartialDerivative(L.delta(-i, k) * L.delta(-j, m) * H(i, j), B(m))\n\n    assert (expr2a._perform_derivative()\n            - (L.delta(-i, k) * L.delta(-j, m) * expr2b._perform_derivative())).contract_delta(L.delta) == 0\n\n    assert (expr2a._perform_derivative()\n            - expr2c._perform_derivative()).contract_delta(L.delta) == 0\n", "def test_eval_partial_derivative_single_1st_rank_tensors_by_tensor_array_indices():\n    expr1 = PartialDerivative(A(1), A(1))\n    assert expr1._perform_derivative() == 1\n\n    expr2 = PartialDerivative(A(1), A(2))\n    assert expr2._perform_derivative() == 0\n\n    expr3 = PartialDerivative(A(2), A(1))\n    assert expr3._perform_derivative() == 0\n\n    expr4 = PartialDerivative(A(2), A(2))\n    assert expr4._perform_derivative() == 1\n\n    expr5 = PartialDerivative(A(-1), A(1))\n    assert expr5._perform_derivative() == -1\n\n    expr6 = PartialDerivative(A(-1), A(-1))\n    assert expr6._perform_derivative() == 1\n\n    expr7 = PartialDerivative(A(-2), A(1))\n    assert expr7._perform_derivative() == 0\n\n    expr8 = PartialDerivative(A(-2), A(-2))\n    assert expr8._perform_derivative() == 1\n", "def test_eval_partial_derivative_nested_derivatives():\n    tau, alpha = symbols(\"tau alpha\")\n\n    # test nested partial derivatives\n    expr1 = PartialDerivative(PartialDerivative(A(i), A(j)), A(k))\n    assert expr1._perform_derivative() == 0\n\n    expr2 = PartialDerivative(PartialDerivative(A(i), A(j)), H(k, m))\n    assert expr2._perform_derivative() == 0\n\n    expr3 = PartialDerivative(PartialDerivative(H(i, j), A(k)), A(m))\n    assert expr3._perform_derivative() - L.delta(i, -m) * L.delta(j, -L_0) * L.delta(-k, L_0) == 0\n\n    expr4 = PartialDerivative(PartialDerivative(H(i, j), A(k)), H(m, n))\n    assert expr4._perform_derivative() - L.delta(i, -m) * L.delta(j, -L_0) * L.delta(-k, L_1) * L.delta(L_0, -n) == 0\n\n    expr5 = PartialDerivative(PartialDerivative(A(i), tau), A(j))\n    assert expr5._perform_derivative() == 0\n\n    expr6 = PartialDerivative(PartialDerivative(A(i), tau), tau)\n    assert expr6._perform_derivative() == 0\n", "def test_eval_partial_derivative_mixed_tensor_expr():\n    tau, alpha = symbols(\"tau alpha\")\n\n    base_expr = H(i, j)*H(-i, -j) + H(i, j)*A(-i)*A(j) + tau**alpha*H(i, j)\n\n    tensor_derivative = PartialDerivative(base_expr, H(k, m))._perform_derivative()\n    vector_derivative = PartialDerivative(base_expr, A(k))._perform_derivative()\n    scalar_derivative = PartialDerivative(base_expr, tau)._perform_derivative()\n\n    assert (tensor_derivative - (L.delta(i, -L_0)*L.delta(j, -L_1)*H(-L_0, -L_1)*H(L_2, -L_2)*L.delta(-L_2, -k)*L.delta(L_2, -m) +\n               L.delta(i, -L_0)*L.delta(j, -L_1)*H(-L_0, -L_1)*A(-L_2)*A(L_2)*L.delta(L_2, -k)*L.delta(L_2, -m) +\n               L.delta(i, -L_0)*L.delta(j, -L_1)*H(-L_0, -L_1)*tau**alpha*L.delta(L_2, -k)*L.delta(L_2, -m) +\n               L.delta(i, -k)*L.delta(j, -L_0)*H(-i, -L_0)*H(L_1, -L_1)*L.delta(L_1, -m) +\n               L.delta(i, -k)*L.delta(j, -L_0)*H(-i, -L_0)*A(-L_1)*A(L_1)*L.delta(L_1, -m) +\n               L.delta(i, -k)*L.delta(j, -L_0)*H(-i, -L_0)*tau**alpha*L.delta(L_1, -m) +\n               L.delta(j, -m)*H(L_0, -L_0)*H(-i, k) +\n               L.delta(j, -m)*H(L_0, -L_0)*A(-i)*A(k) +\n               L.delta(j, -m)*H(L_0, -L_0)*tau**alpha)).expand() == 0\n\n    assert (vector_derivative - (L.delta(L", "def test_eval_partial_derivative_mixed_tensor_tensor_expr():\n    tau, alpha = symbols(\"tau alpha\")\n\n    base_expr3 = A(i)*H(-i, j) + H(i, k)*H(-i, -j)\n\n    vector_tensor_derivative = PartialDerivative(base_expr3, H(k, m))._perform_derivative()\n    vector_tensor_derivative_expected = (\n        A(L_0)*L.metric(-L_0, -L_1)*L.delta(L_1, -k)*L.delta(j, -m) +\n        L.delta(i, -k)*L.metric(j, -L_0)*L.delta(L_0, -m)*H(i, k) +\n        H(i, k)*L.metric(-i, -L_0)*L.delta(L_0, -k)*L.metric(j, -L_1)*L.delta(L_1, -m)\n    )\n    assert (vector_tensor_derivative - vector_tensor_derivative_expected) == 0\n\n    vector_vector_tensor_derivative = PartialDerivative(base_expr3, A(k))._perform_derivative()\n    vector_vector_tensor_derivative_expected = (\n        L.delta(L_0, -k)*A(-L_0)*H(-L_0, j) +\n        A(L_0)*L.metric(-L_0, -L_1)*L.delta(L_1, -k)*H(-L_0, j) +\n        A(L_0)*H(-L_0, L_1)*L.metric(L_1, -L_2)*L.delta(L_2, -k)*L.delta(j, -L_3)*L.metric(-L_3, -L_4)*L.delta(L_4, -L_5)*L.delta(L_5, -j) +\n        L.delta(L_0, -k)*H(L_0, j)*H(i, -j)\n    )\n    assert (vector_vector_tensor_derivative.contract_metric(L.metric).contract_delta(L.delta) -\n            (vector_vector_tensor_derivative_expected.contract_metric(L.metric).contract_delta(L.delta))).expand() == 0\n"], "sample_863": ["def test_pipeline_memory_with_transformer_weights():\n    # test that pipeline's memory works with transformer weights\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n    if LooseVersion(joblib.__version__) < LooseVersion('0.12'):\n        # Deal with change of API in joblib\n        memory = joblib.Memory(cachedir=cachedir, verbose=10)\n    else:\n        memory = joblib.Memory(location=cachedir, verbose=10)\n    pca = PCA(n_components=2, svd_solver='randomized', random_state=0)\n    select = SelectKBest(k=1)\n    # test using fit followed by transform\n    fs = FeatureUnion([(\"pca\", pca), (\"select\", select)],\n                      transformer_weights={\"pca\": 10}, n_jobs=1)\n    fs_pipeline = Pipeline([('fs', fs)])\n    fs_pipeline.memory = memory\n    fs_pipeline.fit(X, y)\n    X_transformed = fs_pipeline.transform(X)\n    # test using fit_transform\n    fs_pipeline = Pipeline([('fs', fs)])\n    fs_pipeline.memory = memory\n    X_fit_transformed = fs_pipeline.fit_transform(X, y)\n    # check against expected result\n    assert_array_almost_equal(X_transformed[:, :-1], 10 * pca.fit_transform(X))\n    assert_array_equal(X_transformed[:, -1],\n                       select.fit_transform(X, y).ravel())\n    assert_array_almost_equal(X_fit_transformed[:, :-1],\n                              10 * pca.fit_transform(X))\n    assert_array_equal(X_fit_transformed[:, -1],\n                       select.fit_transform(X, y).ravel())\n    shutil.rmtree(cachedir)\n", "def test_pipeline_set_params_steps_tuple():\n    # Test that set_params accepts a tuple for the 'steps' parameter\n    pipe = Pipeline([('transf', Transf()), ('clf', FitParamT())])\n    pipe.set_params(steps=[('transf', Transf()), ('clf', FitParamT())])\n    assert pipe.steps == [('transf', Transf()), ('clf', FitParamT())]\n\n    # Test that set_params accepts a tuple for the 'steps' parameter\n    pipe.set_params(steps=(('transf', Transf()), ('clf', FitParamT())))\n    assert pipe.steps == [('transf', Transf()), ('clf', FitParamT())]\n\n    # Test that set_params raises an error if the 'steps' parameter is not a list or tuple\n    assert_raise_message(\n        TypeError,\n        \"'steps' must be a list of (name, transform) tuples.\",\n        pipe.set_params,\n        steps='invalid'\n    )\n", "def test_pipeline_steps_with_step_name_conflict():\n    mult2 = Mult(2)\n    mult3 = Mult(3)\n    # test that pipeline raises error when step name is the same as the\n    # parameter name 'steps'\n    assert_raise_message(\n        ValueError,\n        \"Estimator names conflict with constructor arguments: ['steps']\",\n        Pipeline,\n        steps=[('steps', mult2), ('mult', mult3)],\n        memory=None,\n        verbose=False\n    )\n", "def test_pipeline_fit_with_memory_and_X_as_sparse_matrix():\n    X = sparse.csr_matrix(np.array([[1, 2], [3, 4]]))\n    y = np.array([1, 2])\n\n    cachedir = mkdtemp()\n    if LooseVersion(joblib.__version__) < LooseVersion('0.12'):\n        # Deal with change of API in joblib\n        memory = joblib.Memory(cachedir=cachedir, verbose=10)\n    else:\n        memory = joblib.Memory(location=cachedir, verbose=10)\n    pipe = Pipeline([('transf', Transf()), ('clf', Mult())], memory=memory)\n\n    pipe.fit(X, y)\n\n    try:\n        # Check that we can predict with a sparse matrix as input\n        pipe.predict(X)\n\n        # Check that we can transform with a sparse matrix as input\n        pipe.transform(X)\n    finally:\n        shutil.rmtree(cachedir)\n", "def test_pipeline_with_transformer_weights():\n    # test that pipeline works with transformer weights\n    X = iris.data\n    y = iris.target\n    pca = PCA(n_components=2, svd_solver='randomized', random_state=0)\n    select = SelectKBest(k=1)\n    pipeline = Pipeline([('pca', pca), ('select', select)])\n    pipeline.fit(X, y)\n    X_transformed = pipeline.transform(X)\n\n    # with weights\n    weighted_pipeline = Pipeline([('pca', pca), ('select', select)],\n                                 transformer_weights={'select': 2})\n    weighted_pipeline.fit(X, y)\n    X_weighted_transformed = weighted_pipeline.transform(X)\n    assert X_weighted_transformed.shape == (X.shape[0], 3)\n    assert_array_almost_equal(X_weighted_transformed[:, :-1], pca.transform(X))\n    assert_array_equal(X_weighted_transformed[:, -1],\n                       2 * select.fit_transform(X, y).ravel())\n\n    # with default weights\n    weighted_pipeline = Pipeline([('pca', pca), ('select', select)],\n                                 transformer_weights=None)\n    weighted_pipeline.fit(X, y)\n    X_weighted_transformed = weighted_pipeline.transform(X)\n    assert X_weighted_transformed.shape == (X.shape[0], 3)\n    assert_array_almost_equal(X_weighted_transformed[:, :-1], pca.transform(X))\n    assert_array_equal(X_weighted_transformed[:, -1],\n                       select.fit_transform(X, y).ravel())\n"], "sample_206": ["def test_filefield_deletion_on_model_delete(self):\n    \"\"\"\n    Deletion of a FileField should be cascaded when the model instance is deleted.\n    \"\"\"\n    document = Document.objects.create(myfile='something.txt')\n    file_path = document.myfile.path\n    self.assertTrue(os.path.exists(file_path))\n    document.delete()\n    self.assertFalse(os.path.exists(file_path))\n", "def test_filefield_update_dimension_fields(self):\n    \"\"\"\n    Test that ImageField's update_dimension_fields method updates width_field and height_field correctly.\n    \"\"\"\n    class MyDocument(models.Model):\n        image = models.ImageField(width_field='width', height_field='height', upload_to='unused')\n        width = models.IntegerField(null=True)\n        height = models.IntegerField(null=True)\n\n    with override_settings(MEDIA_ROOT=temp.gettempdir()):\n        with open(__file__, 'rb') as fp:\n            file = File(fp, name='test_image.png')\n            document = MyDocument(image=file)\n            document.save()\n            self.assertIsNone(document.width)\n            self.assertIsNone(document.height)\n            document.image = file\n            document.save()\n            document.refresh_from_db()\n            self.assertIsNotNone(document.width)\n            self.assertIsNotNone(document.height)\n", "def test_file_descriptor___set___image_field(self):\n    \"\"\"\n    ImageField descriptor updates the width and height fields on assignment.\n    \"\"\"\n    class MyModel(models.Model):\n        image = models.ImageField(width_field='width', height_field='height')\n        width = models.IntegerField()\n        height = models.IntegerField()\n\n    m = MyModel()\n    with open(__file__, 'rb') as fp:\n        file1 = File(fp, name='test_file.py')\n        m.image = file1\n    self.assertIsNotNone(m.width)\n    self.assertIsNotNone(m.height)\n", "def test_fieldfile_generate_filename(self):\n    \"\"\"\n    Test that FieldFile generates the correct filename when calling save.\n    \"\"\"\n    d = Document(myfile='test_file.py')\n    with override_settings(MEDIA_ROOT=tempfile.gettempdir()):\n        filename = d.myfile.generate_filename(d, 'test_file.py')\n        self.assertEqual(filename, 'test_file.py')\n        with self.settings(MEDIA_ROOT=tempfile.gettempdir(), DATETIME_FORMAT='%Y-%m-%d'):\n            filename = d.myfile.generate_filename(d, 'test_file.py')\n            today = datetime.date.today()\n            self.assertEqual(filename, os.path.join(today.strftime('%Y-%m-%d'), 'test_file.py'))\n", "def test_generate_filename(self):\n    \"\"\"\n    FileField generates unique filenames.\n    \"\"\"\n    with override_settings(MEDIA_ROOT=tempfile.gettempdir()):\n        class MyDocument(models.Model):\n            myfile = models.FileField(upload_to='unused')\n        document = MyDocument()\n        filename = 'something.txt'\n        generated_filename = document.myfile.field.generate_filename(document, filename)\n        self.assertNotEqual(filename, generated_filename)\n        self.assertTrue(generated_filename.startswith('unused/'))\n\n        # Multiple saves of the same filename should generate different filenames.\n        generated_filename2 = document.myfile.field.generate_filename(document, filename)\n        self.assertNotEqual(generated_filename, generated_filename2)\n        self.assertTrue(generated_filename2.startswith('unused/'))\n\n        # Ensure that filename is properly escaped.\n        filename = 'something.txt#'\n        generated_filename = document.myfile.field.generate_filename(document, filename)\n        self.assertNotEqual(filename, generated_filename)\n        self.assertTrue(generated_filename.startswith('unused/'))\n"], "sample_532": ["def test_contour_zorder():\n    fig, ax = plt.subplots()\n    cs1 = ax.contour(np.arange(16).reshape((4, 4)), zorder=10)\n    cs2 = ax.contour(np.arange(16).reshape((4, 4)), zorder=5)\n    assert cs1._contour_zorder == 10\n    assert cs2._contour_zorder == 5\n    for coll in cs1.collections:\n        assert coll.get_zorder() == 10\n    for coll in cs2.collections:\n        assert coll.get_zorder() == 5\n", "def test_find_nearest_contour_at_path_edge():\n    x = np.arange(0, 10)\n    y = np.arange(0, 10)\n    xg, yg = np.meshgrid(x, y)\n    z = np.random.random((10, 10))\n\n    fig, ax = plt.subplots()\n    cs = ax.contour(xg, yg, z)\n\n    # test at the start of a path\n    nearest_contour = cs.find_nearest_contour(0, 0, pixel=False)\n    expected_nearest = (1, 0, 0, 0, 0, 0)\n    assert_array_almost_equal(nearest_contour, expected_nearest)\n\n    # test at the end of a path\n    nearest_contour = cs.find_nearest_contour(9, 9, pixel=False)\n    expected_nearest = (1, 0, 99, 9, 9, 0)\n    assert_array_almost_equal(nearest_contour, expected_nearest)\n\n    # test on a vertical edge\n    nearest_contour = cs.find_nearest_contour(9.5, 5, pixel=False)\n    expected_nearest = (1, 0, 49, 9.5, 5, 0.25)\n    assert_array_almost_equal(nearest_contour, expected_nearest)\n\n    # test on a horizontal edge\n    nearest_contour = cs.find_nearest_contour(5, 9.5, pixel=False)\n    expected_nearest = (1, 0, 49, 5, 9.5, 0.25)\n    assert_array_almost_equal(nearest_contour, expected_nearest)\n", "def test_contour_labeler_event_handler():\n    cs = ContourSet(None, [1, 2, 3], [[np.array([[0, 0], [1, 0], [1, 1]]), np.array([[1, 1], [0, 1]])]])\n    inline = True\n    inline_spacing = 5\n\n    event = mpl.backend_bases.Event('button_press_event', x=1, y=1, button=MouseButton.LEFT, xdata=1, ydata=1)\n    _contour_labeler_event_handler(cs, inline, inline_spacing, event)\n    assert len(cs.labelTexts) == 1\n\n    event = mpl.backend_bases.Event('button_press_event', x=2, y=2, button=MouseButton.LEFT, xdata=2, ydata=2)\n    _contour_labeler_event_handler(cs, inline, inline_spacing, event)\n    assert len(cs.labelTexts) == 2\n\n    event = mpl.backend_bases.Event('key_press_event', x=3, y=3, key='enter', xdata=3, ydata=3)\n    _contour_labeler_event_handler(cs, inline, inline_spacing, event)\n\n    event = mpl.backend_bases.Event('button_press_event', x=0, y=0, button=MouseButton.MIDDLE, xdata=0, ydata=0)\n    _contour_labeler_event_handler(cs, inline, inline_spacing, event)\n\n    event = mpl.backend_bases.Event('key_press_event', x=0, y=0, key='escape', xdata=0, ydata=0)\n    _contour_labeler_event_handler(cs, inline, inline_spacing, event)\n\n    event = mpl.backend_bases.Event('button_press_event', x=1, y=1, button=MouseButton.RIGHT, xdata=1, ydata=1)\n    _contour_labeler_event_handler(cs, inline, inline_spacing, event)\n    assert len(cs.labelTexts) == 1\n\n    event = mpl.backend_bases.Event('key_press_event', x=1, y=1, key='backspace', xdata=1, ydata=1)\n    _contour_labeler_event_handler(cs, inline, inline_spacing, event)\n    assert len(cs.labelTexts) == 0\n", "def test_contour_labeler_event_handler():\n    from unittest.mock import MagicMock\n    x = 10\n    y = 20\n    inline = True\n    inline_spacing = 5\n    event = MagicMock(name=\"button_press_event\", button=MouseButton.LEFT, \n                     x=x, y=y, inaxes=True)\n    cs = MagicMock(add_label_near=MagicMock(), axes=MagicMock())\n    canvas = cs.axes.figure.canvas\n    canvas.stop_event_loop = MagicMock()\n\n    _contour_labeler_event_handler(cs, inline, inline_spacing, event)\n    cs.add_label_near.assert_called_once_with(x, y, transform=False, \n                                             inline=True, inline_spacing=5)\n    canvas.draw.assert_called_once()\n    canvas.stop_event_loop.assert_not_called()\n\n    event.button = MouseButton.MIDDLE\n    _contour_labeler_event_handler(cs, inline, inline_spacing, event)\n    canvas.stop_event_loop.assert_called_once()\n\n    event.button = MouseButton.RIGHT\n    cs.add_label_near.reset_mock()\n    cs.pop_label = MagicMock()\n    _contour_labeler_event_handler(cs, inline, inline_spacing, event)\n    cs.pop_label.assert_called_once()\n    canvas.draw.assert_called_once()\n\n    inline = False\n    event.button = MouseButton.LEFT\n    _contour_labeler_event_handler(cs, inline, inline_spacing, event)\n    cs.add_label_near.assert_called_once_with(x, y, transform=False, \n                                             inline=False, inline_spacing=5)\n", "def test_clabel_labels_outside_data():\n    delta = 0.025\n    x = np.arange(-3.0, 3.0, delta)\n    y = np.arange(-2.0, 2.0, delta)\n    X, Y = np.meshgrid(x, y)\n    Z1 = np.exp(-(X**2) - Y**2)\n    Z2 = np.exp(-((X - 1) ** 2) - (Y - 1) ** 2)\n    Z = (Z1 - Z2) * 2\n    fig, ax = plt.subplots()\n    CS = ax.contour(X, Y, Z, levels=[-1, -0.5, 0, 0.5, 1])\n    CS.clabel(inline=False, inline_spacing=1, fmt='%1.1f')\n    labels = [t.get_text() for t in ax.texts]\n    assert np.all([l in labels for l in ['-1.0', '-0.5', '0.0', '0.5', '1.0']])\n    assert np.all([l in labels for l in ['-1.0', '-0.5', '0.0', '0.5', '1.0']])\n"], "sample_566": ["def test_subfigure_subplotspec():\n    fig = plt.figure()\n    gs = fig.add_gridspec(1, 2)\n    sfig = fig.add_subfigure(gs[0])\n    assert sfig._subplotspec == gs[0]\n    assert gs[0].get_topmost_subplotspec() == sfig._subplotspec\n", "def test_add_artist_to_subfigure():\n    fig = plt.figure()\n    subfig = fig.add_subfigure()\n    ax = subfig.add_subplot()\n    assert ax in fig.axes\n    assert ax in subfig.axes\n    assert fig.get_children() == [fig.patch, subfig.patch, subfig, ax]\n    assert subfig.get_children() == [subfig.patch, ax]\n", "def test_subfigure_gridspec_passed_twice():\n    # test that passing gridspec_kw to subfigures and again to add_subfigure\n    # doesn't raise an error\n    fig = plt.figure()\n    gs = fig.add_gridspec(1, 2)\n    fig.subfigures(1, 2, gridspec_kw={'wspace': 0.1})\n    fig.add_subfigure(gs[0], gridspec_kw={'wspace': 0.1})\n", "def test_add_subplot_subplotspec():\n    # Test that the subplotspec's bbox is updated correctly\n    fig = Figure(layout='constrained')\n    gs = gridspec.GridSpec(2, 2, left=0.05, right=0.95, bottom=0.05, top=0.95)\n    fig.add_subplot(gs[0, 0])\n    subplotspec = gridspec.GridSpec(2, 2, left=0.1, right=0.9, bottom=0.1, top=0.9)[0, 0]\n    ax = fig.add_subplot(subplotspec)\n    assert ax.get_subplotspec().get_position(fig) == subplotspec.get_position(fig)\n    assert ax.get_subplotspec().get_position(fig).x0 == 0.1\n    assert ax.get_subplotspec().get_position(fig).x1 == 0.45\n    assert ax.get_subplotspec().get_position(fig).y0 == 0.1\n    assert ax.get_subplotspec().get_position(fig).y1 == 0.45\n", "def test_subfigure_is_figure():\n    # Subfigures should act as figures in many respects\n    fig = plt.figure()\n    subfig = fig.subfigures(1, 1)[0]\n\n    # Test the following methods\n    methods = [\n        'align_xlabels',\n        'align_ylabels',\n        'align_labels',\n        'colorbar',\n        'gca',\n        'legend',\n        'subplots',\n        'subplots_adjust',\n        'suptitle',\n        'supxlabel',\n        'supylabel',\n        'text',\n    ]\n\n    for method in methods:\n        assert hasattr(fig, method) == hasattr(subfig, method)\n\n    for method in methods:\n        if method not in [\"colorbar\", \"legend\", \"subplots\", \"text\"]:\n            # the following methods don't need arguments\n            getattr(subfig, method)()\n            getattr(fig, method)()\n\n    # these methods need arguments\n    getattr(subfig, \"colorbar\")(None)\n    getattr(fig, \"colorbar\")(None)\n    getattr(subfig, \"legend\")()\n    getattr(fig, \"legend\")()\n    getattr(subfig, \"subplots\")()\n    getattr(fig, \"subplots\")()\n    getattr(subfig, \"text\")(0, 0, \"\")\n    getattr(fig, \"text\")(0, 0, \"\")\n\n    # The following methods should not be available\n    methods = [\n        'add_artist',\n        'add_axes',\n        'add_gridspec',\n        'add_subplot',\n        'add_subfigure',\n        'clear',\n        'clf',\n        'delaxes',\n        'figimage',\n        'savefig',\n        'set_canvas',\n        'set_dpi',\n        'set_facecolor',\n        'set_figheight',\n        'set_figwidth',\n        'set_frameon',\n        'set_size_inches',\n        'set_tight_layout',\n        'show',\n        'tight_layout'\n    ]\n\n    for method in methods:\n        assert not hasattr(subfig, method)\n        assert hasattr(fig, method)\n"], "sample_990": ["def test_hyperbolic_conjugate():\n    x, y = symbols('x,y', real=True)\n    z = x + y*I\n    assert sinh(z).conjugate() == sinh(z.conjugate())\n    assert cosh(z).conjugate() == cosh(z.conjugate())\n    assert tanh(z).conjugate() == tanh(z.conjugate())\n    assert coth(z).conjugate() == coth(z.conjugate())\n    assert csch(z).conjugate() == csch(z.conjugate())\n    assert sech(z).conjugate() == sech(z.conjugate())\n    assert asinh(z).conjugate() == asinh(z.conjugate())\n    assert acosh(z).conjugate() == acosh(z.conjugate())\n    assert atanh(z).conjugate() == atanh(z.conjugate())\n    assert acoth(z).conjugate() == acoth(z.conjugate())\n    assert asech(z).conjugate() == asech(z.conjugate())\n    assert acsch(z).conjugate() == acsch(z.conjugate())\n", "def test_hyperbolic_functinos_as_real_imag():\n    x = Symbol('x', real=True)\n    a, b = symbols('a b', real=True)\n    for func in [sinh, cosh, tanh, coth, sech, csch]:\n        z = a + b*I\n        assert func(z).as_real_imag() == (func(x).as_real_imag(deep=True, complex=True),\n                                         func(x).as_real_imag(deep=True, complex=True))\n", "def test_hyperbolic_function():\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n    \n    # Test sinh, cosh, tanh and coth functions with complex arguments\n    assert sinh(x + y*I).is_real == (x.is_real and y.is_zero)\n    assert cosh(x + y*I).is_real == (x.is_real and y.is_zero)\n    assert tanh(x + y*I).is_real == (x.is_real and y.is_zero)\n    assert coth(x + y*I).is_real == (x.is_real and y.is_zero)\n    \n    # Test csch, sech, asinh, acosh, atanh and acoth functions with complex arguments\n    assert csch(x + y*I).is_real == (x.is_real and y.is_zero)\n    assert sech(x + y*I).is_real == (x.is_real and y.is_zero)\n    assert asinh(x + y*I).is_real == (x.is_real and y.is_zero)\n    assert acosh(x + y*I).is_real == (x.is_real and y.is_zero)\n    assert atanh(x + y*I).is_real == (x.is_real and y.is_zero)\n    assert acoth(x + y*I).is_real == (x.is_real and y.is_zero)\n    \n    # Test HyperbolicFunction with multiple arguments\n    assert HyperbolicFunction(x, y, z).args == (x, y, z)\n    \n    # Test ReciprocalHyperbolicFunction with multiple arguments\n    assert ReciprocalHyperbolicFunction(x, y, z).args == (x, y, z)\n    \n    # Test InverseHyperbolicFunction with multiple arguments\n    assert InverseHyperbolicFunction(x, y, z).args == (x, y, z)\n", "def test_asinh_atanh_acoth_acsch():\n    x, y = symbols('x y')\n\n    assert asinh(log(x)) == log(x + sqrt(x**2 - 1))\n    assert atanh(log(x)) == log(1 + x) / 2 - log(1 - x) / 2\n    assert acoth(log(x)) == log(1 + 1/x) / 2 - log(1 - 1/x) / 2\n    assert acsch(log(x)) == log(x + sqrt(x**2 + 1))\n\n    # property: asinh(x) + atanh(x) = atanh(x / (1 + sqrt(1 + x**2)))\n    assert asinh(x) + atanh(x) == atanh(x / (1 + sqrt(1 + x**2)))\n    assert asinh(x) - atanh(x) == -atanh(x / (1 - sqrt(1 + x**2)))\n\n    # property: asinh(x) + acoth(x) = acoth(x / (1 + sqrt(1 + x**2)))\n    assert asinh(x) + acoth(x) == acoth(x / (1 + sqrt(1 + x**2)))\n    assert asinh(x) - acoth(x) == -acoth(x / (1 - sqrt(1 + x**2)))\n\n    # property: atanh(x) + acsch(x) = acsch(x / (1 + sqrt(1 - x**2)))\n    assert atanh(x) + acsch(x) == acsch(x / (1 + sqrt(1 - x**2)))\n    assert atanh(x) - acsch(x) == -acsch(x / (1 - sqrt(1 - x**2)))\n\n    # property: acoth(x) + acsch(x) = acsch(x / (x**2 - 1))\n    assert acoth(x) + acsch(x) == acsch(x / (x**2 - 1))\n    assert acoth(x) - acsch(x) == -acsch(x / (x**2 + 1))\n\n    assert asinh(0).evalf() == 0\n    assert atanh(0).evalf() == 0\n    assert acoth(0).evalf() == zoo\n    assert acsch(0).evalf() == zoo", "def test_hyperbolic_functions_series():\n    x = Symbol('x')\n    assert sinh(x).series(x, 0, 20) == x + x**3/6 + 3*x**5/40 + 15*x**7/336 + 103*x**9/6720 + 8733*x**11/322560 + 207783*x**13/5806080 + 5718753*x**15/116121600 + 190553167*x**17/322560000 + 6613626103*x**19/9289728000 + O(x**20)\n    assert cosh(x).series(x, 0, 20) == 1 + x**2/2 + x**4/24 + x**6/720 + x**8/40320 + x**10/3628800 + x**12/479001600 + x**14/87178291200 + x**16/20922789888000 + x**18/6402373705728000 + O(x**20)\n    assert tanh(x).series(x, 0, 20) == x - x**3/3 + 2*x**5/15 - 17*x**7/315 + 62*x**9/2835 - 1382*x**11/155925 + 21844*x**13/6081075 - 929569*x**15/638512875 + 293974323*x**17/22907909275 - 10349477504*x**19/131681894375 + O(x**20)\n    assert coth(x).series(x, 0, 20) == 1/x + x/3 - x**3/45 + 2*x**5/945 - 2*x**7/4725 + 1382*x**9/155925 - 21844*x**11/6081075 + 929569*x**13/638512875 - 293974323*x**15/22907909275 + 10349477504*x**17/131681894375 - 311999450413*x**19/5128662085625 + O(x**20)\n    assert csch(x).series(x, 0, 20) == 1/x - x/6 + 7*x**3/360 - 31*x**5/15120 + 127*x**7/604"], "sample_831": ["def test_export_text_with_feature_names_and_class_names():\n    clf = DecisionTreeClassifier(max_depth=2, random_state=0)\n    clf.fit(X, y)\n\n    expected_report = dedent(\"\"\"\n    |--- feature0 <= 0.00\n    |   |--- class: -1\n    |--- feature0 >  0.00\n    |   |--- class: 1\n    \"\"\").lstrip()\n\n    feature_names = ['feature0', 'feature1']\n    class_names = ['class0', 'class1']\n    assert export_text(clf, feature_names=feature_names, class_names=class_names) == expected_report\n\n    feature_names = ['feature0', 'feature1']\n    class_names = True\n    assert export_text(clf, feature_names=feature_names, class_names=class_names) == dedent(\"\"\"\n    |--- feature0 <= 0.00\n    |   |--- class: y[0]\n    |--- feature0 >  0.00\n    |   |--- class: y[1]\n    \"\"\").lstrip()\n\n    feature_names = ['feature0', 'feature1']\n    class_names = ['class0', 'class1', 'class2']\n    message = \"Length of class_names, 3 does not match number of classes, 2\"\n    assert_raise_message(ValueError, message, export_text, clf, feature_names=feature_names, class_names=class_names)\n", "def test_color_brew():\n    # Test color generation function\n    colors = _color_brew(5)\n    assert len(colors) == 5\n    for color in colors:\n        assert len(color) == 3\n        for val in color:\n            assert 0 <= val <= 255\n\n    # Test color generation with equally spaced hues\n    colors = _color_brew(5)\n    hues = [color[0] / 255.0 for color in colors]\n    hue_diff = hues[1] - hues[0]\n    for i in range(2, len(hues)):\n        assert_almost_equal(hues[i] - hues[i - 1], hue_diff)\n\n    # Test edge cases\n    assert len(_color_brew(1)) == 1\n    assert len(_color_brew(0)) == 0\n", "def test_export_text_regressor_weights():\n    # testing that the weights of a regressor are exported\n    clf = DecisionTreeRegressor(max_depth=2, random_state=0)\n    clf.fit(X, y, sample_weight=w)\n\n    expected_report = dedent(\"\"\"\n    |--- feature_0 <= 0.0\n    |   |--- value: -1.0\n    |--- feature_0 >  0.0\n    |   |--- value: 1.0\n    \"\"\").lstrip()\n    assert export_text(clf) == expected_report\n\n    expected_report = dedent(\"\"\"\n    |--- feature_0 <= 0.0\n    |   |--- value: -1.0\n    |--- feature_0 >  0.0\n    |   |--- weights: [1.5, 1.5] value: 1.0\n    \"\"\").lstrip()\n    assert export_text(clf, show_weights=True) == expected_report\n", "def test_plot_tree_classifier_non_numeric_class_names(pyplot):\n    # mostly smoke tests\n    # Check correctness of export_graphviz for classifier with non-numeric class names\n    clf = DecisionTreeClassifier(max_depth=3,\n                                 min_samples_split=2,\n                                 random_state=2)\n    clf.fit(X, y)\n\n    # Test export code\n    feature_names = ['first feat', 'sepal_width']\n    class_names = ['class_a', 'class_b']\n    nodes = plot_tree(clf, feature_names=feature_names, class_names=class_names)\n    assert len(nodes) == 3\n    assert nodes[0].get_text() == (\"first feat <= 0.0\\ngini = 0.5\\n\"\n                                   \"samples = 6\\nvalue = [3, 3]\")\n    assert nodes[1].get_text() == \"gini = 0.0\\nsamples = 3\\nvalue = [3, 0]\\nclass: class_a\"\n    assert nodes[2].get_text() == \"gini = 0.0\\nsamples = 3\\nvalue = [0, 3]\\nclass: class_b\"\n\n", "def test_export_text_with_feature_names_undefined():\n    clf = DecisionTreeClassifier(max_depth=2, random_state=0)\n    clf.fit(X, y)\n\n    expected_report = dedent(\"\"\"\n    |--- feature_0 <= 0.00\n    |   |--- class: -1\n    |--- feature_0 >  0.00\n    |   |--- class: 1\n    \"\"\").lstrip()\n    assert export_text(clf) == expected_report\n"], "sample_8": ["    def test_unsupported_functions(self):\n        for func_name in UNSUPPORTED_FUNCTIONS:\n            func = getattr(np, func_name)\n            with pytest.raises(NotImplementedError, match=func_name):\n                func(self.ma)\n", "def test_masked_formatting_empty_mask():\n    ma = Masked(np.arange(5))\n    with np.printoptions(formatter={'bool': lambda x: '\u2014' if x else ''}):\n        assert repr(ma.mask) == \"MaskedNDArray([\u2014\u2014, \u2014\u2014, \u2014\u2014, \u2014\u2014, \u2014\u2014], dtype=bool)\"\n", "    def test_median(self):\n        data = np.array([1, 2, np.nan, 4])\n        mask = np.array([False, False, True, False])\n        ma = Masked(data, mask)\n        median = np.median(ma)\n        assert median.unmasked == 2\n        assert not median.mask\n\n        data = np.array([1, 2, np.nan, 4])\n        mask = np.array([False, True, True, False])\n        ma = Masked(data, mask)\n        median = np.median(ma)\n        assert median.unmasked == 2.5\n        assert not median.mask\n\n        data = np.array([1, 2, np.nan, np.nan])\n        mask = np.array([False, False, True, True])\n        ma = Masked(data, mask)\n        median = np.median(ma)\n        assert median.unmasked == 1.5\n        assert not median.mask\n", "    def test_mean_Quantity_raise(self):\n        ma = self.ma\n        with pytest.raises(u.UnitsError):\n            ma.mean(axis=1, keepdims=True)\n        ", "    def test_mask_propagation_on_binary_ops(self, op):\n        data = self.ma.unmasked\n        mask = self.ma.mask\n        # Masked when either operand is masked.\n        for shape in [self.ma.shape, (1,)+self.ma.shape, self.ma.shape+(1,)]:\n            other = np.ones(shape)\n            masked_other = Masked(other, mask=np.ones(other.shape, dtype=bool))\n            expected_mask = np.ones(data.shape, dtype=bool)\n            if shape == self.ma.shape:\n                expected_data = eval(f'data {op} other')\n            else:\n                expected_data = eval(f'np.broadcast_to(data, shape) {op} np.broadcast_to(other, shape)')\n            result = eval(f'self.ma {op} masked_other')\n            assert_array_equal(result.unmasked, expected_data)\n            assert_array_equal(result.mask, expected_mask)\n\n            # Masked when both operands are masked but not both unmasked.\n            other = Masked(other, mask=np.zeros(other.shape, dtype=bool))\n            if shape == self.ma.shape:\n                expected_data = eval(f'data {op} other.unmasked')\n            else:\n                expected_data = eval(f'np.broadcast_to(data, shape) {op} np.broadcast_to(other.unmasked, shape)')\n            expected_mask = mask | np.zeros(data.shape, dtype=bool)\n            result = eval(f'self.ma {op} other')\n            assert_array_equal(result.unmasked, expected_data)\n            assert_array_equal(result.mask, expected_mask)\n"], "sample_914": ["def test_unparse_arguments():\n    source = \"def func(a, b=1, *, c=2, d, **kwargs): pass\"\n    expected = \"a, b=1, *, c=2, d, **kwargs\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args) == expected\n", "def test_unparse_function_definition():\n    source = \"\"\"", "def test_unparse_arguments(source, expected):\n    module = ast.parse(source)\n    if isinstance(module.body[0], ast.FunctionDef):\n        assert ast.unparse(module.body[0].args) == expected\n    elif isinstance(module.body[0], ast.Assign):\n        assert ast.unparse(module.body[0].targets[0]) == expected\n    else:\n        assert ast.unparse(module.body[0].value) == expected\n", "def test_unparse_arguments():\n    source = \"def test(a: int = 1, b: str, *, c: int, d: str = 'test'): pass\"\n    expected = \"a: int = 1, b: str, *, c: int, d: str = 'test'\"\n    tree = ast.parse(source)\n    assert ast.unparse(tree.body[0].args) == expected\n", "def test_unparse_arguments(source, expected):\n    tree = ast.parse(source)\n    function_def = tree.body[0]\n    assert ast.unparse(function_def.args) == expected\n"], "sample_161": ["    def test_foreign_object_to_unique_fields_with_condition(self):\n        class Person(models.Model):\n            country_id = models.IntegerField()\n            city_id = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['country_id', 'city_id'],\n                        name='tfotufwc_unique',\n                        condition=models.Q(city_id__gt=10),\n                    ),\n                ]\n\n        class MMembership(models.Model):\n            person_country_id = models.IntegerField()\n            person_city_id = models.IntegerField()\n            person = models.ForeignObject(\n                Person,\n                on_delete=models.CASCADE,\n                from_fields=['person_country_id', 'person_city_id'],\n                to_fields=['country_id', 'city_id'],\n            )\n\n        field = MMembership._meta.get_field('person')\n        self.assertEqual(field.check(), [\n            Error(\n                \"No subset of the fields 'country_id', 'city_id' on model 'Person' is unique.\",\n                hint=(\n                    'Mark a single field as unique=True or add a set of '\n                    'fields to a unique constraint (via unique_together or a '\n                    'UniqueConstraint (without condition) in the model '\n                    'Meta.constraints).'\n                ),\n                obj=field,\n                id='fields.E310',\n            ),\n        ])\n", "    def test_foreign_key_with_through_model_on_non_unique_field(self):\n        class Parent(models.Model):\n            pass\n\n        class Child(models.Model):\n            name = models.CharField(max_length=20)\n            parent = models.ForeignKey(\n                'Parent',\n                models.CASCADE,\n                to_field='id',\n                through='ChildParent',\n                through_fields=('child', 'parent'),\n            )\n\n        class ChildParent(models.Model):\n            child = models.ForeignKey(Child, models.CASCADE)\n            parent = models.ForeignKey(Parent, models.CASCADE)\n\n        field = Child._meta.get_field('parent')\n        self.assertEqual(field.check(from_model=Child), [\n            Error(\n                \"'Parent.id' must be unique because it is referenced by a foreign key.\",\n                hint=(\n                    'Add unique=True to this field or add a UniqueConstraint '\n                    '(without condition) in the model Meta.constraints.'\n                ),\n                obj=field,\n                id='fields.E311',\n            ),\n        ])\n", "    def test_through_model_symmetrical_warning(self):\n        class Model(models.Model):\n            rel = models.ManyToManyField('self', through='Through', symmetrical=True)\n\n        class Through(models.Model):\n            from_model = models.ForeignKey(Model, models.CASCADE)\n            to_model = models.ForeignKey(Model, models.CASCADE)\n\n        with self.assertWarnsMessage(DjangoWarning, 'symmetrical has no effect on ManyToManyField with a through model'):\n            Model.check()\n", "    def test_to_field_name_is_valid(self):\n        class Parent(models.Model):\n            good_field = models.IntegerField(unique=True)\n            bad_field = models.IntegerField()\n\n        class Child(models.Model):\n            parent = models.ForeignObject(\n                Parent,\n                on_delete=models.CASCADE,\n                from_fields=['field'],\n                to_fields=['good_field'],\n            )\n\n        field = Child._meta.get_field('parent')\n        self.assertEqual(field.check(), [\n            Error(\n                \"The to_field 'field' doesn't exist on the related model 'invalid_models_tests.Parent'.\",\n                obj=field,\n                id='fields.E312',\n            ),\n        ])\n\n        class Child(models.Model):\n            parent = models.ForeignObject(\n                Parent,\n                on_delete=models.CASCADE,\n                from_fields=['field'],\n                to_fields=['bad_field'],\n            )\n\n        field = Child._meta.get_field('parent')\n        self.assertEqual(field.check(), [\n            Error(\n                \"The to_field 'field' doesn't exist on the related model 'invalid_models_tests.Parent'.\",\n                obj=field,\n                id='fields.E312',\n            ),\n            Error(\n                \"'Parent.bad_field' must be unique because it is referenced by a foreign key.\",\n                hint=(\n                    'Add unique=True to this field or add a UniqueConstraint '\n                    '(without condition) in the model Meta.constraints.'\n                ),\n                obj=field,\n                id='fields.E311',\n            ),\n        ])\n", "    def test_unique_table_with_model_proxy(self):\n        class Group(models.Model):\n            members = models.ManyToManyField('Person', through='Membership')\n\n        class Person(models.Model):\n            pass\n\n        class Membership(models.Model):\n            group = models.ForeignKey(Group, models.CASCADE)\n            person = models.ForeignKey(Person, models.CASCADE)\n\n        class GroupProxy(Group):\n            class Meta:\n                proxy = True\n\n        # The second model proxy doesn't have the same concrete model.\n        self.assertEqual(Group.check(), [])\n"], "sample_504": ["def test_colorbar_remove_after_set_norm():\n    fig, ax = plt.subplots()\n    x, y = np.ogrid[-4:4:31j, -4:4:31j]\n    z = 120000*np.exp(-x**2 - y**2)\n\n    im = ax.imshow(z)\n    cbar = fig.colorbar(im)\n    im.set_norm(Normalize(vmin=-1000, vmax=1000))\n    cbar.remove()\n", "def test_colorbar_alpha():\n    \"\"\"\n    Test alpha is correctly set for colorbar.\n    \"\"\"\n    fig, ax = plt.subplots()\n    sm = cm.ScalarMappable(norm=mcolors.Normalize(), cmap='viridis', alpha=0.5)\n    fig.colorbar(sm)\n\n    assert sm.get_alpha() == 0.5\n\n    sm.set_alpha(0.8)\n    assert sm.get_alpha() == 0.8\n\n    sm.set_alpha(None)\n    assert sm.get_alpha() is None\n", "def test_colorbar_mappable_alpha():\n    # Test that the alpha value is properly updated when the mappable's\n    # alpha changes.\n    fig, ax = plt.subplots()\n    im = ax.imshow(np.random.rand(10, 10), cmap='viridis', alpha=0.5)\n    cbar = fig.colorbar(im)\n    assert cbar.alpha == 0.5\n\n    im.set_alpha(0.7)\n    assert cbar.alpha == 0.7\n\n    im.set_alpha(None)\n    assert cbar.alpha is None\n\n    im.set_alpha([0.5, 0.7])\n    assert cbar.alpha is None\n", "def test_colorbar_alpha_array():\n    \"\"\"\n    Test the alpha value of the colorbar.\n    \"\"\"\n    fig, ax = plt.subplots()\n    sm = cm.ScalarMappable(norm=mcolors.Normalize(), cmap='viridis')\n    sm.set_alpha([0.5, 0.7, 0.9])\n    fig.colorbar(sm)\n    assert fig.axes[-1].collections[0].get_alpha() is None\n    fig.draw_without_rendering()\n\n    sm.set_alpha(0.5)\n    fig.colorbar(sm)\n    np.testing.assert_almost_equal(fig.axes[-1].collections[0].get_alpha(), 0.5)\n    fig.draw_without_rendering()\n\n    fig, ax = plt.subplots()\n    x = np.arange(1, 5).reshape(2, 2)/4\n    pc = ax.pcolormesh(x, alpha=x)\n    cb = fig.colorbar(pc, ax=ax)\n    # The colorbar's alpha should be None and the mappable should still have\n    # the original alpha array\n    assert cb.alpha is None\n    assert pc.get_alpha() is x\n    fig.draw_without_rendering()\n\n    pc.set_alpha(0.5)\n    cb = fig.colorbar(pc, ax=ax)\n    np.testing.assert_almost_equal(cb.ax.collections[0].get_alpha(), 0.5)\n    fig.draw_without_rendering()\n", "def test_cbar_clip_path(fig_ref, fig_test):\n    fig_test, ax = fig_test.subplots()\n    x = np.arange(1, 5).reshape(2, 2)\n    pc = ax.pcolormesh(x, alpha=x)\n    cbar = fig_test.colorbar(pc, ax=ax)\n\n    # The colorbar's alpha should be None and the mappable should still have\n    # the original alpha array\n    assert cbar.alpha is None\n    assert pc.get_alpha() is x\n    fig_test.draw_without_rendering()\n\n    # test the clip path\n    clip_path = cbar.ax.patch.get_clip_path()\n    assert clip_path.get_extents().bounds == (0.0, 0.0, 1.0, 1.0)\n\n    # test the clip path when the aspect is changed\n    cbar.ax.set_aspect('equal')\n    fig_test.draw_without_rendering()\n    clip_path = cbar.ax.patch.get_clip_path()\n    assert clip_path.get_extents().bounds == (0.0, 0.0, 1.0, 1.0)\n"], "sample_1171": ["def test_issue_18556():\n    assert ImageSet(Lambda(x, x**2), S.Integers).is_subset(S.Reals)\n    assert ImageSet(Lambda(x, x**2), S.Naturals0).is_subset(S.Reals)\n    assert ImageSet(Lambda(x, x), S.Integers).is_subset(S.Rationals)\n    assert ImageSet(Lambda(x, x/2), S.Integers).is_subset(S.Rationals)\n    assert ImageSet(Lambda(x, x/2), S.Naturals0).is_subset(S.Rationals)\n", "def test_ImageSet_parameters():\n    from sympy.abc import n\n    # Test ImageSet parameters\n    lam = Lambda(n, 2*n)\n    img = ImageSet(lam, S.Integers)\n    assert img.parameters == (n,)\n\n    lam = Lambda(n, (2*n, n**2))\n    img = ImageSet(lam, S.Integers)\n    assert img.parameters == (n,)\n\n    lam = Lambda((n, n), (2*n, n**2))\n    raises(ValueError, lambda: ImageSet(lam, S.Integers))\n", "def test_ImageSet_symbols_clash():\n    from sympy.abc import n\n    s = ImageSet(Lambda(n, n), S.Integers)\n    raises(ValueError, lambda: s.intersect(s))\n    t = symbols('t')\n    s = ImageSet(Lambda(n, n), S.Integers)\n    s2 = ImageSet(Lambda(t, t), S.Integers)\n    s3 = ImageSet(Lambda(n, t), S.Integers)\n    assert s.intersect(s2) == ImageSet(Lambda(n, n), S.Integers)\n    assert s.intersect(s3) == Intersection(s, s3, evaluate=False)\n", "def test_ComplexRegion_as_relational():\n    x, y = symbols('x y', real=True)\n    z = x + y*I\n    r = symbols('r', positive=True)\n    theta = symbols('theta', real=True)\n    z_polar = r * (cos(theta) + sin(theta)*I)\n\n    assert ComplexRegion(S.Reals*S.Reals).as_relational(z) == S.true\n\n    c1 = ComplexRegion(Interval(1, 2)*Interval(2, 3))\n    assert c1.as_relational(z) == And(And(x >= 1, x <= 2), And(y >= 2, y <= 3))\n\n    c2 = ComplexRegion(Interval(0, 1)*Interval(0, 2*pi), polar=True)\n    assert c2.as_relational(z_polar) == And(r >= 0, r <= 1, theta >= 0, theta < 2*pi)\n\n    c3 = ComplexRegion(Union(Interval(0, 1)*Interval(0, 2*pi),\n                            Interval(1, 2)*Interval(0, 2*pi)), polar=True)\n    assert c3.as_relational(z_polar) == Or(\n        And(r >= 0, r <= 1, theta >= 0, theta < 2*pi),\n        And(r >= 1, r <= 2, theta >= 0, theta < 2*pi))\n\n    c4 = ComplexRegion(Interval(1, 2)*FiniteSet(0, pi), polar=True)\n    assert c4.as_relational(z_polar) == And(r >= 1, r <= 2, theta == 0) | And(r >= 1, r <= 2, theta == pi)\n", "def test_issue_18291():\n    n = symbols('n', integer=True)\n    assert Range(n, 0, -1).reversed == Range(0, n, 1)\n\n    # Check the edge cases.\n    assert Range(1, 1).reversed == Range(1, 1)\n    assert Range(1, 2).reversed == Range(2, 1, -1)\n    assert Range(-1, -2).reversed == Range(-2, -1, 1)\n    assert Range(-2, -1).reversed == Range(-1, -2, -1)\n\n    # Check that the start and stop are switched\n    assert Range(10, 20, 2).reversed.start == Range(18, 10, -2).start\n    assert Range(10, 20, 2).reversed.stop == Range(18, 10, -2).stop\n    assert Range(10, 20, 2).reversed.step == Range(18, 10, -2).step\n\n    # Check that the iterator still works correctly.\n    assert list(Range(1, 11, 2).reversed) == [9, 7, 5, 3, 1]\n\n    # Check that the size still works correctly.\n    assert Range(1, 11, 2).reversed.size == Range(1, 11, 2).size\n\n    # Check the error messages.\n    raises(ValueError, lambda: Range(n).reversed)\n    raises(ValueError, lambda: Range(1, 2, 1).reversed.start)\n    raises(ValueError, lambda: Range(1, 2, 1).reversed.stop)\n"], "sample_472": ["def test_paginator_iter__invalid_page_number(self):\n    paginator = Paginator([1, 2, 3], 2)\n    paginator.page = unittest.mock.Mock(side_effect=InvalidPage)\n    with self.assertRaises(InvalidPage):\n        list(paginator)\n", "def testPaginatorWithWarningWhenObjectListOrderedIsNone(self):\n    \"\"\"\n    Paginator warning when object_list.ordered is None.\n    \"\"\"\n    class ObjectList:\n        pass\n\n    object_list = ObjectList()\n    msg = (\n        \"Pagination may yield inconsistent results with an unordered \"\n        \"object_list: {!r}.\".format(object_list)\n    )\n    with self.assertWarnsMessage(UnorderedObjectListWarning, msg):\n        Paginator(object_list, 5)\n", "    def testPaginator_init_invalid_input(self):\n        with self.assertRaises(ValueError):\n            Paginator([1, 2, 3], 'a', 5)\n        with self.assertRaises(ValueError):\n            Paginator([1, 2, 3], 5, 'a')\n        with self.assertRaises(ValueError):\n            Paginator([1, 2, 3], 5, 1, 'a')\n        with self.assertRaises(ValueError):\n            Paginator([1, 2, 3], 5, 1, False, error_messages=None)\n", "def test_page_range_generator_exhaustion(self):\n    \"\"\"\n    Test that get_elided_page_range generator exhaustion works correctly.\n    \"\"\"\n    paginator = Paginator(range(10), 1)\n    page_range = paginator.get_elided_page_range(1)\n    page_range_list = list(page_range)\n    self.assertEqual(len(page_range_list), paginator.num_pages)\n    self.assertEqual(page_range_list, list(paginator.page_range))\n\n    # Test exhaustion with a large number of pages\n    paginator = Paginator(range(10000), 1)\n    page_range = paginator.get_elided_page_range(1)\n    page_range_list = list(page_range)\n    self.assertEqual(len(page_range_list), paginator.num_pages)\n    self.assertEqual(page_range_list, list(paginator.page_range))\n", "def test_paginator_warnings(self):\n    \"\"\"\n    Paginator warnings for unordered object_list.\n    \"\"\"\n    # A custom object list that has an ordered attribute.\n    class OrderedObjectList:\n        ordered = True\n\n    # A custom object list that has a model attribute but is unordered.\n    class UnorderedObjectListModel:\n        model = \"Model\"\n        ordered = False\n\n    # A custom object list that is unordered and has no model attribute.\n    class UnorderedObjectList:\n        ordered = False\n\n    # No warning for an ordered object_list.\n    with warnings.catch_warnings(record=True) as recorded:\n        Paginator(OrderedObjectList(), 5)\n    self.assertEqual(len(recorded), 0)\n\n    # Warning for an unordered object_list with a model.\n    msg = (\n        \"Pagination may yield inconsistent results with an unordered \"\n        \"object_list: <class 'pagination.models.UnorderedObjectListModel'> QuerySet.\"\n    )\n    with self.assertWarnsMessage(UnorderedObjectListWarning, msg):\n        Paginator(UnorderedObjectListModel(), 5)\n\n    # Warning for an unordered object_list without a model.\n    msg = \"Pagination may yield inconsistent results with an unordered object_list: {!r}.\".format(\n        UnorderedObjectList()\n    )\n    with self.assertWarnsMessage(UnorderedObjectListWarning, msg):\n        Paginator(UnorderedObjectList(), 5)\n"], "sample_898": ["def test_label_ranking_average_precision_score_multilabel_invariance():\n    # Generate some data\n    n_classes = 4\n    n_samples = 50\n\n    _, y1 = make_multilabel_classification(n_features=1, n_classes=n_classes,\n                                           random_state=0, n_samples=n_samples,\n                                           allow_unlabeled=True)\n    _, y2 = make_multilabel_classification(n_features=1, n_classes=n_classes,\n                                           random_state=1, n_samples=n_samples,\n                                           allow_unlabeled=True)\n\n    # To make sure at least one empty label is present\n    y1 = np.vstack([y1, [[0] * n_classes]])\n    y2 = np.vstack([y2, [[0] * n_classes]])\n\n    y1_sparse_indicator = sp.coo_matrix(y1)\n    y2_sparse_indicator = sp.coo_matrix(y2)\n\n    y1_score = np.random.normal(size=(y1.shape[0], y1.shape[1]))\n    y2_score = np.random.normal(size=(y2.shape[0], y2.shape[1]))\n\n    # Check representation invariance\n    assert_almost_equal(\n        label_ranking_average_precision_score(y1, y1_score),\n        label_ranking_average_precision_score(y1_sparse_indicator,\n                                              y1_score),\n        err_msg=\"label_ranking_average_precision_score failed representation \"\n                \"invariance between dense and sparse indicator formats.\")\n\n    # Check representation invariance\n    assert_almost_equal(\n        label_ranking_average_precision_score(y1, y1_score),\n        label_ranking_average_precision_score(y1_sparse_indicator,\n                                              y1_score),\n        err_msg=\"label_ranking_average_precision_score failed representation \"\n                \"invariance between dense and sparse indicator formats.\")\n\n    # Check representation invariance\n    assert_almost_equal(\n        label_ranking_average_precision_score(y1, y1_score),\n        label_ranking_average_precision_score(y1_sparse_indicator,\n                                              y1_score),\n        err_msg=\"label_ranking_average_precision_score failed representation \"\n                \"invariance between dense and sparse indicator formats.\")\n", "def test_auc_reorder():\n    # test auc with reorder=False\n    x = np.array([0.1, 0.2, 0.3, 0.4])\n    y = np.array([0.3, 0.2, 0.1, 0.4])\n    # x is neither increasing nor decreasing\n    assert_raises(ValueError, auc, x, y, reorder=False)\n    # x is increasing\n    x = np.array([0.1, 0.2, 0.3, 0.4])\n    y = np.array([0.1, 0.2, 0.3, 0.4])\n    area = auc(x, y, reorder=False)\n    assert_almost_equal(area, 0.5)\n    # x is decreasing\n    x = np.array([0.4, 0.3, 0.2, 0.1])\n    y = np.array([0.1, 0.2, 0.3, 0.4])\n    area = auc(x, y, reorder=False)\n    assert_almost_equal(area, 0.5)\n", "def test_auc():\n    random_state = check_random_state(0)\n    x = random_state.rand(100)\n    y = random_state.rand(100)\n\n    # test auc with x values that are not sorted\n    with np.errstate(invalid='ignore'):\n        assert_almost_equal(auc(x, y, reorder=True), auc(np.sort(x), y))\n\n    # test auc with x values that are already sorted\n    assert_almost_equal(auc(x, y, reorder=True), auc(np.sort(x), y))\n\n    # test auc with x values that are monotonic increasing\n    assert_almost_equal(auc(x, y, reorder=True), auc(np.sort(x), y))\n\n    # test auc with x values that are monotonic decreasing\n    assert_almost_equal(auc(x, y, reorder=True), auc(np.sort(x), y))\n\n    # test auc with equal x values\n    x = np.array([1, 1, 1, 1])\n    y = np.array([0.2, 0.3, 0.1, 0.4])\n    assert_almost_equal(auc(x, y, reorder=True), 0.5)\n\n    # test auc with equal x values and y values\n    x = np.array([1, 1, 1, 1])\n    y = np.array([0.2, 0.2, 0.2, 0.2])\n    assert_almost_equal(auc(x, y, reorder=True), 0.5)\n\n    # test auc with one point\n    x = np.array([1])\n    y = np.array([1])\n    assert_almost_equal(auc(x, y, reorder=True), 0)\n\n    # test auc with two points\n    x = np.array([1, 2])\n    y = np.array([0.2, 0.8])\n    assert_almost_equal(auc(x, y, reorder=True), 0.5)\n\n    # test auc with negative values\n    x = np.array([-1, 0, 1])\n    y = np.array([0.2, 0.3, 0.1])\n    assert_almost_equal(auc(x, y, reorder=True), 0.5)\n\n    # test auc with x values that are not finite\n    x = np.array([np.nan, 0, 1])\n    y = np.array([0.2, 0.", "def test_auc_multiclass():\n    # Test area under curve in multiclass case\n\n    # Regression test for #5915\n    y_true = [0, 1, 2]\n    y_pred = [[0.1, 0.9, 0.1],\n              [0.2, 0.7, 0.1],\n              [0.1, 0.1, 0.8]]\n    score = roc_auc_score(y_true, y_pred, multi_class='ovr')\n    assert_greater(score, 0.95,\n                   msg=\"We failed to test correctly area under curve \"\n                       \"multiclass one vs. rest\")\n\n    # Test multiclass one vs. one\n    score = roc_auc_score(y_true, y_pred, multi_class='ovo')\n    assert_greater(score, 0.95,\n                   msg=\"We failed to test correctly area under curve \"\n                       \"multiclass one vs. one\")\n\n    # Test multiclass is not supported\n    assert_raises(ValueError, roc_auc_score, y_true, y_pred, multi_class='abc')\n    assert_raises(ValueError, roc_auc_score, y_true, y_pred, multi_class='ovr',\n                  average='samples')\n", "def test_label_ranking_average_precision_score():\n    # Generate some data\n    n_samples = 50\n    n_labels = 20\n    random_state = check_random_state(0)\n    y_true = random_state.randint(0, 2, size=(n_samples, n_labels))\n    y_score = random_state.normal(size=(n_samples, n_labels))\n\n    # Test that the average is 1. when there is only one label\n    assert_almost_equal(label_ranking_average_precision_score(\n        y_true[:, :1], y_score[:, :1]), 1)\n\n    # Test that the average is 1. when the score is perfect\n    perfect_score = np.argsort(y_true, axis=1)\n    assert_almost_equal(label_ranking_average_precision_score(\n        y_true, perfect_score), 1)\n\n    # Test that the average is 0. when the score is the reverse of the truth\n    reverse_score = np.argsort(-y_true, axis=1)\n    assert_almost_equal(label_ranking_average_precision_score(\n        y_true, reverse_score), 0)\n\n    # Test that the average is less than or equal to 1.\n    assert_less_equal(label_ranking_average_precision_score(y_true, y_score), 1)\n\n    # Test that the average is always greater than or equal to 0.\n    assert_greater_equal(label_ranking_average_precision_score(y_true, y_score), 0)\n\n    # Test the case in which an instance has no label\n    y_true[0, :] = 0\n    y_score[0, :] = 0\n    assert_almost_equal(label_ranking_average_precision_score(y_true, y_score),\n                        (label_ranking_average_precision_score(\n                            y_true[1:], y_score[1:])))\n\n    # Test the case in which a label is not present in the training data\n    # but present in the test data\n    y_true_with_label = y_true.copy()\n    y_score_with_label = y_score.copy()\n    y_true_with_label[0, 0] = 1\n    y_score_with_label[0, 0] = 1\n    y_true[0, 0] = 0\n    y_score[0, 0] = 0\n    assert_almost_equal(label_ranking_average_precision_score(y_true, y_score),\n                        label_ranking_average_precision_score(\n                            y_true_with_label, y_score_with_label))\n\n    #"], "sample_985": ["def test_max_min():\n    x, y, z = symbols('x y z')\n    a, b = symbols('a b')\n    assert Max(x, y, z).subs(x, 1) == Max(1, y, z)\n    assert Max(x, y, z).subs({x: 1, y: 2}) == Max(1, 2, z)\n    assert Max(x, y, z).subs({x: 1, y: 2, z: 3}) == 3\n    assert Min(x, y, z).subs(x, 1) == Min(1, y, z)\n    assert Min(x, y, z).subs({x: 1, y: 2}) == Min(1, 2, z)\n    assert Min(x, y, z).subs({x: 1, y: 2, z: 3}) == 1\n\n    # test rewrite as Heaviside\n    assert Max(x, y).rewrite(Heaviside) == Heaviside(x-y)*x + Heaviside(y-x)*y\n    assert Max(x, y, z).rewrite(Heaviside) == Heaviside(x-z)*Heaviside(x-y)*x + Heaviside(x-z)*Heaviside(y-x)*Heaviside(y-z)*y + Heaviside(z-x)*Heaviside(z-y)*z\n    assert Min(x, y).rewrite(Heaviside) == Heaviside(x-y)*y + Heaviside(y-x)*x\n    assert Min(x, y, z).rewrite(Heaviside) == Heaviside(x-z)*Heaviside(x-y)*z + Heaviside(x-z)*Heaviside(y-x)*Heaviside(y-z)*y + Heaviside(z-x)*Heaviside(z-y)*x\n\n    # test rewrite as Piecewise\n    assert Max(x, y).rewrite(Piecewise) == Piecewise((x, x >= y), (y, True))\n    assert Max(x, y, z).rewrite(Piecewise) == Piecewise((x, (x >= y) & (x >= z)), (y, y >= z), (z, True))\n    assert Max(x, y, a, b).rewrite(Piecewise) == Piecewise((a, (a >= b) & (a >= x) & (a", "def test_identities():\n    x = symbols('x')\n    assert Id(x) == x\n    assert Id(x**2) == x**2\n    assert Id(S.Half) == S.Half\n    assert Id(1) == 1\n    assert Id(IdentityFunction(x)) == x\n", "def test_real_root():\n    from sympy import real_root, root, Symbol, Rational, sqrt\n\n    x = Symbol('x', real=True)\n    assert real_root(x**2) == x\n    assert real_root(x**3) == x\n    assert real_root(-x**2) == -x\n\n    n = Symbol('n', positive=True, integer=True)\n    assert real_root(x**n) == x\n\n    # test issue 14223\n    assert real_root(x**Rational(1, 2)) == sqrt(x)\n    assert real_root(x**Rational(1, 3)) == root(x, 3)\n    assert real_root(-x**Rational(1, 2)) == -sqrt(x)\n    assert real_root(-x**Rational(1, 3)) == -root(x, 3)\n", "def test_root():\n    from sympy import root, sqrt, cbrt, Rational, Symbol, I\n    x = Symbol('x')\n    n = Symbol('n', positive=True)\n    assert root(x, 2) == sqrt(x)\n    assert root(x, 3) == cbrt(x)\n    assert root(x, n) == x**(1/n)\n    assert root(x, -Rational(2, 3)) == x**(-3/2)\n    assert root(-2, 3, 2) == -(-1)**(2/3)*2**(1/3)\n    assert root(-8, 3) == 2*(-1)**(1/3)\n    assert root(-8, 3, 1) == -2\n    assert root(-32, 5, 3) == -2\n    assert root(-32, 5, 3) == root(-32, 5, 2)\n", "def test_minmaxbase():\n    from sympy import Min, Max, Symbol, oo, I\n    from sympy.abc import x, y, z\n    p = Symbol('p', positive=True)\n    n = Symbol('n', negative=True)\n\n    # Test MinMaxBase._new_args_filter\n    raises(ValueError, lambda: Min(oo))\n    raises(ValueError, lambda: Min(n, x, I))\n\n    # Test MinMaxBase._find_localzeros\n    assert Min._find_localzeros([x, y, z]) == {x, y, z}\n    assert Min._find_localzeros([x, y, x]) == {x, y}\n    assert Min._find_localzeros([n, p, n]) == {n}\n    assert Min._find_localzeros([oo, n, p]) == {n, p}\n\n    # Test MinMaxBase._is_connected\n    assert Min._is_connected(n, p) == Min\n    assert Min._is_connected(n, x) is None\n    assert Min._is_connected(x, x) == True\n\n    # Test MinMaxBase._eval_derivative\n    assert Min(x, y).diff(x) == Heaviside(y - x)\n\n    # Test MinMaxBase.evalf\n    assert Min(n, p).evalf() == -1\n    assert Min(n, p).evalf(2) == -1.00\n\n    # Test MinMaxBase._eval_is_comparable\n    assert Min(n, p).is_comparable is False\n    assert Min(n, p).evalf().is_comparable\n"], "sample_942": ["def test_pyvariable_signature(app):\n    text = (\".. py:data:: var\\n\"\n            \"   :type: int\\n\"\n            \"   :value: 1\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"var\"],\n                                                    [desc_annotation, (\": \",\n                                                                       [pending_xref, \"int\"])],\n                                                    [desc_annotation, \" = 1\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"data\",\n                domain=\"py\", objtype=\"data\", noindex=False)\n    assert 'var' in domain.objects\n    assert domain.objects['var'] == ('index', 'var', 'data', False)\n\n    text = (\".. py:data:: var\\n\"\n            \"   :annotation: int\\n\"\n            \"   :value: 1\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"var\"],\n                                                    [desc_annotation, \"int\"],\n                                                    [desc_annotation, \" = 1\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"data\",\n                domain=\"py\", objtype=\"data\", noindex=False)\n    assert 'var' in domain.objects\n    assert domain.objects['var'] == ('index', 'var', 'data', False)\n", "def test_pyvariable_signature(app):\n    text = (\".. py:variable:: MY_CONSTANT\\n\"\n            \"   :type: int\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"MY_CONSTANT\"],\n                                                    [desc_annotation, (\": \",\n                                                                       [pending_xref, \"int\"])],\n                                                    [desc_annotation, \"\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"data\",\n                domain=\"py\", objtype=\"data\", noindex=False)\n", "def test_domain_py_module_synopsis(app, status, warning):\n    app.builder.build_all()\n\n    domain = app.env.get_domain('py')\n    modules = domain.modules\n    assert modules['module_a.submodule'][2] == 'Module synopsis'\n    assert modules['module_b.submodule'][2] == 'Module synopsis'\n    assert modules['module_a.submodule.submodule'][2] == 'Submodule synopsis'\n", "def test_domain_py_get_full_qualified_name(app, status, warning):\n    app.builder.build_all()\n\n        domain = app.env.get_domain('py')\n        return domain.get_full_qualified_name(node)\n\n    doctree = app.env.get_doctree('roles')\n    refnodes = list(doctree.traverse(pending_xref))\n    for node in refnodes:\n        print(node)\n        print(get_full_qualified_name(node))\n\n    assert get_full_qualified_name(refnodes[0]) == 'TopLevel'\n    assert get_full_qualified_name(refnodes[1]) == 'top_level'\n    assert get_full_qualified_name(refnodes[2]) == 'NestedParentA.child_1'\n    assert get_full_qualified_name(refnodes[3]) == 'NestedParentA.NestedChildA.subchild_2'\n    assert get_full_qualified_name(refnodes[4]) == 'NestedParentA.child_2'\n    assert get_full_qualified_name(refnodes[5]) is None\n    assert get_full_qualified_name(refnodes[6]) == 'NestedParentA.NestedChildA'\n    assert get_full_qualified_name(refnodes[7]) == 'NestedParentA.NestedChildA.subchild_2'\n    assert get_full_qualified_name(refnodes[8]) == 'NestedParentA.child_1'\n    assert get_full_qualified_name(refnodes[9]) == 'NestedParentA.NestedChildA.subchild_1'\n    assert get_full_qualified_name(refnodes[10]) == 'NestedParentB.child_1'\n    assert get_full_qualified_name(refnodes[11]) == 'NestedParentB'\n    assert get_full_qualified_name(refnodes[12]) == 'NestedParentA.NestedChildA'\n\n    doctree = app.env.get_doctree('module')\n    refnodes = list(doctree.traverse(pending_xref))\n    assert get_full_qualified_name(refnodes[0]) == 'module_a.submodule.ModTopLevel'\n    assert get_full_qualified_name(refnodes[1]) == 'module_a.submodule.ModTopLevel.mod_child_1'\n    assert get_full_qualified_name(refnodes[2]) == 'module_a.submodule.ModTopLevel.mod_child_1'\n    assert get_full_qualified_name(refnodes[3]) == 'module_a.submodule.ModTop", "def test_domain_py_xrefs_with_submodule(app, status, warning):\n    app.builder.build_all()\n\n    doctree = app.env.get_doctree('module')\n    refnodes = list(doctree.traverse(pending_xref))\n\n    assert len(refnodes) == 16\n\n    # Check submodule references\n    assert refnodes[0].astext() == 'module_a.submodule'\n    assert refnodes[0]['reftarget'] == 'module_a.submodule'\n\n    assert refnodes[7].astext() == 'module_b.submodule'\n    assert refnodes[7]['reftarget'] == 'module_b.submodule'\n\n    # Check submodule member references\n    assert refnodes[1].astext() == 'ModTopLevel'\n    assert refnodes[1]['reftarget'] == 'ModTopLevel'\n    assert refnodes[1]['py:module'] == 'module_a.submodule'\n\n    assert refnodes[2].astext() == 'ModTopLevel.mod_child_1'\n    assert refnodes[2]['reftarget'] == 'ModTopLevel.mod_child_1'\n    assert refnodes[2]['py:module'] == 'module_a.submodule'\n\n    assert refnodes[3].astext() == 'ModTopLevel'\n    assert refnodes[3]['reftarget'] == 'ModTopLevel'\n    assert refnodes[3]['py:module'] == 'module_a.submodule'\n\n    assert refnodes[4].astext() == 'ModTopLevel.mod_child_1'\n    assert refnodes[4]['reftarget'] == 'ModTopLevel.mod_child_1'\n    assert refnodes[4]['py:module'] == 'module_a.submodule'\n\n    assert refnodes[5].astext() == 'prop'\n    assert refnodes[5]['reftarget'] == 'prop'\n    assert refnodes[5]['py:module'] == 'module_a.submodule'\n\n    assert refnodes[6].astext() == 'prop'\n    assert refnodes[6]['reftarget'] == 'prop'\n    assert refnodes[6]['py:module'] == 'module_a.submodule'\n\n    assert refnodes[8].astext() == 'ModTopLevel'\n    assert refnodes[8]['reftarget'] == 'ModTopLevel'\n    assert refnodes["], "sample_818": ["def test_spectral_clustering_empty_cluster():\n    # Test that spectral_clustering raises an error when a cluster is empty\n    S = np.array([[1.0, 1.0, 0.0, 0.0],\n                  [1.0, 1.0, 0.0, 0.0],\n                  [0.0, 0.0, 1.0, 1.0],\n                  [0.0, 0.0, 1.0, 1.0]])\n\n    assert_raises(ValueError, spectral_clustering, S, n_clusters=3,\n                  random_state=0, assign_labels='kmeans')\n\n    assert_raises(ValueError, spectral_clustering, S, n_clusters=3,\n                  random_state=0, assign_labels='discretize')\n", "def test_spectral_clustering_on_precomputed_affinity_matrix():\n    # Test that spectral clustering works with a precomputed affinity matrix\n    X, y = make_blobs(n_samples=20, random_state=0,\n                      centers=[[1, 1], [-1, -1]], cluster_std=0.01)\n    S = rbf_kernel(X, gamma=1)\n    S = np.maximum(S - 1e-4, 0)\n    \n    # Test with a dense matrix\n    sp = SpectralClustering(n_clusters=2, affinity='precomputed',\n                            random_state=0).fit(S)\n    assert adjusted_rand_score(y, sp.labels_) == 1\n    \n    # Test with a sparse matrix\n    S_sparse = sparse.csr_matrix(S)\n    sp = SpectralClustering(n_clusters=2, affinity='precomputed',\n                            random_state=0).fit(S_sparse)\n    assert adjusted_rand_score(y, sp.labels_) == 1\n", "def test_spectral_clustering_edge_cases():\n    # Test that spectral_clustering handles edge cases correctly\n\n    # Test that spectral_clustering raises an error when n_clusters is 0\n    assert_raises(ValueError, spectral_clustering,\n                 np.array([[1.0, 1.0], [1.0, 1.0]]), n_clusters=0,\n                 random_state=0, eigen_solver='arpack')\n\n    # Test that spectral_clustering raises an error when n_clusters is negative\n    assert_raises(ValueError, spectral_clustering,\n                 np.array([[1.0, 1.0], [1.0, 1.0]]), n_clusters=-1,\n                 random_state=0, eigen_solver='arpack')\n\n    # Test that spectral_clustering raises an error when n_components is 0\n    assert_raises(ValueError, spectral_clustering,\n                 np.array([[1.0, 1.0], [1.0, 1.0]]), n_clusters=2,\n                 n_components=0, random_state=0, eigen_solver='arpack')\n\n    # Test that spectral_clustering raises an error when n_components is negative\n    assert_raises(ValueError, spectral_clustering,\n                 np.array([[1.0, 1.0], [1.0, 1.0]]), n_clusters=2,\n                 n_components=-1, random_state=0, eigen_solver='arpack')\n\n    # Test that spectral_clustering raises an error when affinity is not symmetric\n    assert_raises(ValueError, spectral_clustering,\n                 np.array([[1.0, 0.0], [1.0, 1.0]]), n_clusters=2,\n                 random_state=0, eigen_solver='arpack')\n\n    # Test that spectral_clustering raises an error when affinity is not square\n    assert_raises(ValueError, spectral_clustering,\n                 np.array([[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]),\n                 n_clusters=2, random_state=0, eigen_solver='arpack')\n\n    # Test that spectral_clustering raises an error when affinity is not positive\n    # semi-definite\n    assert_raises(ValueError, spectral_clustering,\n                 np.array([[1.0, -1.0], [-1.0, 1.0]]), n_clusters=2,\n                 random_state=0, eigen_solver='ar", "def test_spectral_clustering_with_inconsistent_affinity():\n    # Test that spectral_clustering raises a ValueError if affinity matrix is not symmetric\n    X, y = make_blobs(n_samples=20, random_state=0,\n                      centers=[[1, 1], [-1, -1]], cluster_std=0.01)\n    S = rbf_kernel(X, gamma=1)\n    S[0, 1] = 0.5  # Make the matrix non-symmetric\n    assert_raises(ValueError, spectral_clustering, S, n_clusters=2,\n                  random_state=0, eigen_solver='arpack', assign_labels='kmeans')\n", "def test_spectral_clustering_n_init():\n    # Test that n_init parameter affects the output\n    centers = np.array([\n        [0., 0., 0.],\n        [10., 10., 10.],\n        [20., 20., 20.],\n    ])\n    X, true_labels = make_blobs(n_samples=100, centers=centers,\n                                cluster_std=1., random_state=42)\n    D = pairwise_distances(X)  # Distance matrix\n    S = np.max(D) - D  # Similarity matrix\n    S = sparse.coo_matrix(S)\n    \n    # Test that different n_init values give different results\n    labels_n_init_10 = spectral_clustering(S, n_clusters=2, random_state=0, n_init=10)\n    labels_n_init_5 = spectral_clustering(S, n_clusters=2, random_state=0, n_init=5)\n    assert not np.array_equal(labels_n_init_10, labels_n_init_5)\n\n    # Test that n_init=1 is handled correctly\n    labels_n_init_1 = spectral_clustering(S, n_clusters=2, random_state=0, n_init=1)\n    assert (X.shape[0],) == labels_n_init_1.shape\n\n    # Test that n_init < 1 raises an error\n    assert_raises(ValueError, spectral_clustering, S, n_clusters=2,\n                  random_state=0, n_init=-1)\n"], "sample_435": ["    def test_render(self):\n        field = ReadOnlyPasswordHashField()\n        html = field.widget.render(\"password\", None, {})\n        self.assertIn(_(\"No password set.\"), html)\n", "    def test_template_name(self):\n        widget = ReadOnlyPasswordHashWidget()\n        self.assertEqual(widget.template_name, \"auth/widgets/read_only_password_hash.html\")\n", "    def test_ascii_strings(self):\n        s1 = \"hello\"\n        s2 = \"hello\"\n        self.assertTrue(_unicode_ci_compare(s1, s2))\n", "    def test_integer_username_password_reset(self):\n        \"\"\"Password reset for user with integer username.\"\"\"\n        username = 123\n        email = \"test@example.com\"\n        IntegerUsernameUser.objects.create_user(username, email, \"password\")\n        form = PasswordResetForm({\"email\": email})\n        self.assertTrue(form.is_valid())\n        form.save()\n        self.assertEqual(len(mail.outbox), 1)\n        self.assertEqual(mail.outbox[0].to, [email])\n", "    def test_get_bound_field(self):\n        class TestForm(forms.Form):\n            hash_field = ReadOnlyPasswordHashField()\n\n        form = TestForm()\n        bound_field = form[\"hash_field\"]\n        self.assertEqual(bound_field.field.widget.read_only, True)\n"], "sample_1136": ["def test_Poly_from_domain_element_field_extension():\n    u = Symbol('u')\n\n    K = QQ.algebraic_field(sqrt(2))\n\n    f = K(u + sqrt(2))\n\n    assert Poly(f, u, domain=K).rep == DMP([K(1), K(sqrt(2))], K)\n\n    f = K(u + sqrt(3))\n\n    raises(CoercionFailed, lambda: Poly(f, u, domain=K))\n", "def test_issue_19579():\n    p = Poly(2+3*I, x, domain=ZZ_I)\n    q = Poly(1-I, x, domain=ZZ_I)\n    assert p.div(q, auto=True) == (Poly(-S(1)/2 + 5*I/2, x, domain='QQ_I'), Poly(0, x, domain='QQ_I'))\n    assert p.div(q, auto=False) == (Poly(0, x, domain='ZZ_I'), Poly(2 + 3*I, x, domain='ZZ_I'))\n\n    p = Poly(0.2*x + 0.1, x)\n    q = Poly(x**2 + 1, x)\n    assert p.div(q) == (Poly(0.2*x - 0.2, x), Poly(0.41, x))\n    assert p.div(q, auto=True) == (Poly(0.2*x - 0.2, x), Poly(0.41, x))\n    assert p.div(q, auto=False) == (Poly(0.2*x - 0.2, x), Poly(0.41, x))\n\n    p = Poly(2+3*I, x, domain=ZZ_I)\n    q = Poly(1-I, x, domain=ZZ_I)\n    assert p.rem(q, auto=True) == Poly(0, x, domain='QQ_I')\n    assert p.rem(q, auto=False) == Poly(2 + 3*I, x, domain='ZZ_I')\n\n    p = Poly(0.2*x + 0.1, x)\n    q = Poly(x**2 + 1, x)\n    assert p.rem(q) == Poly(0.41, x)\n    assert p.rem(q, auto=True) == Poly(0.41, x)\n    assert p.rem(q, auto=False) == Poly(0.41, x)\n\n    p = Poly(2+3*I, x, domain=ZZ_I)\n    q = Poly(1-I, x, domain=ZZ_I)\n    assert p.quo(q, auto=True) == Poly(-S(1)/2 + 5*I/2, x, domain='QQ_I')\n    assert p.quo(q, auto=False) == Poly(0, x, domain='ZZ_I')\n\n    p = Poly(0.2*x + 0.1, x)\n    q", "def test_Poly_abs_and_neg_with_expression_domain():\n    # Check to ensure that the abs and neg methods are correctly implemented for ExpressionDomain\n    p = Poly(sin(x) + 1, domain='EX')\n    assert p.abs() == Poly(abs(sin(x) + 1), domain='EX')\n    assert -p == Poly(-sin(x) - 1, domain='EX')\n", "def test_Poly_from_ExpressionDomain_edge_cases():\n    ex = ExpressionDomain()\n    a = ex.Expression(1)\n    b = ex.Expression(2)\n\n    assert a.as_expr() == 1\n    assert b.as_expr() == 2\n\n    assert a == a\n    assert a != b\n\n    c = ex.simplify(b.ex + a.ex)\n    assert c.as_expr() == 3\n\n    assert ex.from_sympy(a.as_expr()) == a\n    assert ex.from_ZZ_python(3, ex) == ex.Expression(3)\n    assert ex.from_QQ_python(Rational(3, 2), ex) == ex.Expression(Rational(3, 2))\n    assert ex.from_ZZ_gmpy(3, ex) == ex.Expression(3)\n    assert ex.from_QQ_gmpy(Rational(3, 2), ex) == ex.Expression(Rational(3, 2))\n    assert ex.from_GaussianIntegerRing(a.as_expr(), ex) == ex.Expression(1)\n    assert ex.from_GaussianRationalField(a.as_expr(), ex) == ex.Expression(1)\n    assert ex.from_RealField(a.as_expr(), ex) == ex.Expression(1)\n    assert ex.from_PolynomialRing(a.as_expr(), ex) == ex.Expression(1)\n    assert ex.from_FractionField(a.as_expr(), ex) == ex.Expression(1)\n    assert ex.from_ExpressionDomain(a, ex) == a\n", "def test_expression_domain():\n    EX = ExpressionDomain()\n    e = EX(x+1)\n    e1 = EX(x+2)\n    e2 = EX(x**2+1)\n    e3 = EX((x+1)*(x+1))\n\n    assert EX.is_positive(e1) == True\n    assert EX.is_positive(e2) == True\n    assert EX.is_positive(-e1) == False\n    assert EX.is_negative(e1) == False\n    assert EX.is_negative(-e1) == True\n    assert EX.is_nonpositive(e1) == False\n    assert EX.is_nonpositive(-e1) == True\n    assert EX.is_nonnegative(e1) == True\n    assert EX.is_nonnegative(-e1) == False\n\n    assert e1.numer() == e1\n    assert e1.denom() == EX.one\n\n    assert e1.gcd(e2) == e1\n    assert e2.gcd(e1) == e1\n\n    assert e1.lcm(e2) == e1\n    assert e2.lcm(e1) == e1\n\n    assert e3 == e\n"], "sample_705": ["def test_pytester_runitem(pytester: Pytester) -> None:\n    pytester.makepyfile(\"def test_func(): pass\")\n    item = pytester.getitem(\"test_func\")\n    result = pytester.runitem(item.node.name)\n    assert result is None\n", "def test_pytester_runresult_str(pytester: Pytester) -> None:\n    result = pytester.runpytest()\n    assert str(result) == result.stdout.str()\n", "def test_pytester_syspathinsert(pytester: Pytester) -> None:\n    \"\"\"Verify the syspathinsert method prepends the directory to sys.path.\"\"\"\n    original_syspath = sys.path[:]\n    pytester.syspathinsert()\n    assert sys.path[0] == str(pytester.path)\n    pytester._finalize()\n    assert sys.path == original_syspath\n", "def test_pytester_linecomp(pytester: Pytester) -> None:\n    lc = pytester.LineComp()\n    lc.stringio.write(\"hello\\n\")\n    lc.stringio.write(\"world\\n\")\n    lc.assert_contains_lines([\"hello\", \"world\"])\n\n    with pytest.raises(pytest.fail.Exception):\n        lc.assert_contains_lines([\"hello\", \"world\", \"not present\"])\n\n    lc.stringio.seek(0)\n    lc.assert_contains_lines([\"hello\", \"world\"])\n", "def test_pytester_copy_example(pytester: Pytester) -> None:\n    pytester.makepyfile(\"def test_example(): pass\")\n\n    # Make sure the example is copied by name.\n    p = pytester.copy_example(\"test_example.py\")\n    assert (p / \"test_example.py\").exists()\n\n    # Make sure the example is copied by module name.\n    p = pytester.copy_example(\"test_example\")\n    assert (p / \"test_example.py\").exists()\n\n    # Make sure the example is copied by package name.\n    pytester.makepyfile(\"example/__init__.py\")\n    pytester.makepyfile(\"example/test_example.py\")\n    p = pytester.copy_example(\"example\")\n    assert (p / \"example\" / \"__init__.py\").exists()\n    assert (p / \"example\" / \"test_example.py\").exists()\n\n    # Make sure it raises when the example does not exist.\n    with pytest.raises(LookupError):\n        pytester.copy_example(\"nonexistent\")\n\n    # Make sure it raises when the example is not a file or a package.\n    pytester.makepyfile(\"example/nonexistent\", \"nonexistent content\")\n    with pytest.raises(LookupError):\n        pytester.copy_example(\"nonexistent\")\n"], "sample_1047": ["def test_issue_16313_imaginary():\n    x = Symbol('x', imaginary=True)\n    k = Symbol('k', real=True)\n    l = Symbol('l', real=True, zero=False)\n    assert (-x).is_imaginary is True\n    assert (k*x).is_imaginary is True\n    assert (l*x).is_imaginary is True\n    assert (l*x*x).is_imaginary is False  # since x*x can be a real number\n    assert (-x).is_positive is False\n", "def test_issue_imaginary_unit_power():\n    # Test the powers of the imaginary unit\n    assert (S.ImaginaryUnit**0).is_real is True\n    assert (S.ImaginaryUnit**1).is_real is False\n    assert (S.ImaginaryUnit**2).is_real is True\n    assert (S.ImaginaryUnit**3).is_real is False\n    assert (S.ImaginaryUnit**4).is_real is True\n    assert (S.ImaginaryUnit**5).is_real is False\n    assert (S.ImaginaryUnit**6).is_real is True\n    assert (S.ImaginaryUnit**7).is_real is False\n    assert (S.ImaginaryUnit**8).is_real is True\n    assert (S.ImaginaryUnit**-1).is_real is False\n    assert (S.ImaginaryUnit**-2).is_real is True\n    assert (S.ImaginaryUnit**-3).is_real is False\n    assert (S.ImaginaryUnit**-4).is_real is True\n    assert (S.ImaginaryUnit**-5).is_real is False\n    assert (S.ImaginaryUnit**-6).is_real is True\n    assert (S.ImaginaryUnit**-7).is_real is False\n    assert (S.ImaginaryUnit**-8).is_real is True\n", "def test_Pow_is_nonzero():\n    x = Symbol('x', nonzero=True)\n    y = Symbol('y', zero=True)\n    z = Symbol('z', nonzero=True)\n    p = Symbol('p', positive=True)\n    n = Symbol('n', negative=True)\n    assert (x**y).is_nonzero is False\n    assert (x**z).is_nonzero is True\n    assert (y**z).is_nonzero is True\n    assert (x**p).is_nonzero is True\n    assert (x**n).is_nonzero is None\n    assert (y**p).is_nonzero is True\n    assert (y**n).is_nonzero is True\n    assert (z**p).is_nonzero is True\n    assert (z**n).is_nonzero is None\n", "def test_issue_sqrt_transcendental():\n    x = Symbol('x', real=True, transcendental=True)\n    y = Symbol('y', real=True, algebraic=True)\n    assert sqrt(x).is_transcendental is None\n    assert sqrt(y).is_algebraic\n    assert sqrt(x*y).is_transcendental is None\n    assert sqrt(x + 1).is_transcendental is None\n    assert sqrt(y + 1).is_algebraic is None\n", "def test_issue_assumption_deduction():\n    # Test that the assumptions are correctly deduced\n    x = Symbol('x', real=True, integer=True)\n    y = Symbol('y', real=True, integer=False)\n    assert (x + y).is_integer is False\n    assert (x * y).is_integer is False\n    assert (x * y).is_rational is False\n    assert (x + y).is_rational is False\n    assert (x * y).is_real is True\n    assert (x + y).is_real is True\n"], "sample_1193": ["def test_are_similar():\n    c1, c2 = Circle(Point(0, 0), 4), Circle(Point(1, 4), 3)\n    t1 = Triangle(Point(0, 0), Point(1, 0), Point(0, 1))\n    t2 = Triangle(Point(0, 0), Point(2, 0), Point(0, 2))\n    t3 = Triangle(Point(0, 0), Point(3, 0), Point(0, 1))\n    assert are_similar(t1, t2)\n    assert not are_similar(t1, t3)\n    raises(GeometryError, lambda: are_similar(c1, t1))\n    assert are_similar(c1, c1)\n", "def test_are_similar():\n    p1 = Point(0, 0)\n    p2 = Point(1, 1)\n    c1 = Circle(p1, 1)\n    c2 = Circle(p1, 2)\n    c3 = Circle(p2, 2)\n    t1 = Triangle(p1, p2, Point(1, 0))\n    t2 = Triangle(p1, p2, Point(2, 0))\n    t3 = Triangle(p2, Point(3, 1), Point(2, 0))\n    raises(GeometryError, lambda: are_similar(c1, t1))\n    assert are_similar(c1, c1) == True\n    assert are_similar(c1, c2) == True\n    assert are_similar(c2, c3) == True\n    assert are_similar(t1, t1) == True\n    assert are_similar(t1, t2) == True\n    assert are_similar(t2, t3) == False\n", "def test_util_functions():\n    # test find function\n    from sympy import Symbol\n    x = Symbol('x')\n    assert find('x', x) == x\n    raises(ValueError, lambda: find('x', Symbol('y')))\n    raises(ValueError, lambda: find(x, Symbol('x')))\n\n    # test _ordered_points function\n    p1, p2 = Point2D(1, 2), Point2D(0, 0)\n    assert _ordered_points((p1, p2)) == (p2, p1)\n    assert _ordered_points((p2, p1)) == (p2, p1)\n\n    # test are_coplanar function\n    p1, p2, p3, p4 = Point3D(0, 0, 0), Point3D(1, 0, 0), Point3D(0, 1, 0), Point3D(1, 1, 0)\n    assert are_coplanar(p1, p2, p3, p4) == True\n    p5 = Point3D(0, 0, 1)\n    assert are_coplanar(p1, p2, p3, p5) == False\n", "def test_ordered_points():\n    # Test that _ordered_points returns a tuple of points sorted numerically according to args\n    points = [Point2D(3, 2), Point2D(1, 1), Point2D(2, 3)]\n    expected = (Point2D(1, 1), Point2D(2, 3), Point2D(3, 2))\n    assert _ordered_points(points) == expected\n\n    # Test that _ordered_points works with points in different quadrants\n    points = [Point2D(-1, -2), Point2D(-3, -1), Point2D(2, 3)]\n    expected = (Point2D(-3, -1), Point2D(-1, -2), Point2D(2, 3))\n    assert _ordered_points(points) == expected\n\n    # Test that _ordered_points works with points on the same line\n    points = [Point2D(1, 1), Point2D(2, 2), Point2D(3, 3)]\n    expected = (Point2D(1, 1), Point2D(2, 2), Point2D(3, 3))\n    assert _ordered_points(points) == expected\n", "def test_convex_hull_edge_cases():\n    # test with single point\n    assert convex_hull(Point(0, 0), **dict(polygon=False)) == (Point2D(0, 0), None)\n\n    # test with two points\n    assert convex_hull(Point(0, 0), Point(1, 0), **dict(polygon=False)) == (\n        Segment2D(Point2D(0, 0), Point2D(1, 0)), None)\n\n    # test with three collinear points\n    assert convex_hull(Point(0, 0), Point(1, 0), Point(2, 0), **dict(polygon=False)) == (\n        Segment2D(Point2D(0, 0), Point2D(2, 0)), None)\n\n    # test with three points that form a triangle\n    assert convex_hull(Point(0, 0), Point(1, 0), Point(0, 1), **dict(polygon=False)) == (\n        [Point2D(0, 0), Point2D(1, 0), Point2D(0, 1)],\n        [Point2D(0, 0), Point2D(0, 1)])\n\n    # test with points that are not ordered\n    raises(ValueError, lambda: convex_hull(Point(1, 0), Point('a', 0), Point(0, 0), **dict(polygon=False)))\n\n    # test with points of different dimensions\n    raises(ValueError, lambda: convex_hull(Point2D(0, 0), Point3D(1, 1, 1), **dict(polygon=False)))\n"], "sample_666": ["def test_log_format_with_no_color(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\\\n        import logging\n            logging.warning(\"hello\")\n            assert 0\n    \"\"\"\n    )\n    result = testdir.runpytest_subprocess(p, \"--log-format\", \"%(message)s\", \"--log-cli-level\", \"WARNING\")\n    result.stdout.fnmatch_lines([\"*hello*\"])\n", "def test_log_capture_fixture_with_logging(testdir):\n    testdir.makeconftest(\n        \"\"\"\n        import logging\n        import pytest\n\n        logging.basicConfig()\n\n        @pytest.fixture\n            logging.info(\"setup logger\")\n            yield\n            logging.info(\"teardown logger\")\n        \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.info(\"test log\")\n            assert 0\n        \"\"\"\n    )\n    result = testdir.runpytest_subprocess(\"--log-cli-level=INFO\")\n    assert result.ret != 0\n    result.stdout.fnmatch_lines(\n        [\n            \"*INFO*setup logger*\",\n            \"*INFO*test log*\",\n            \"*INFO*teardown logger*\",\n        ]\n    )\n", "def test_capturing_non_ascii_characters_in_error_message(testdir, method):\n    testdir.makepyfile(\n        \"\"\"\\\n            raise Exception(\"message with non-ascii characters: \u00e4\u00f6\u00fc\")\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--capture=%s\" % method)\n    result.stdout.fnmatch_lines(\n        [\n            \"*Exception: message with non-ascii characters: \u00e4\u00f6\u00fc*\",\n            \"*1 failed*\",\n        ]\n    )\n", "def test_capture_closes_file_descriptor_after_use(testdir, fixture):\n    testdir.makepyfile(\n        \"\"\"\\\n        import os\n            captured = {fixture}.readouterr()\n            assert captured.out == ''\n            assert captured.err == ''\n            # The file descriptor should be closed now\n            assert {fixture}.file_descriptor.closed\n        \"\"\".format(\n            fixture=fixture\n        )\n    )\n    result = testdir.runpytest_subprocess()\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n", "    def test_simple(self):\n        formatter = PercentStyleMultiline(\"%(message)s\", auto_indent=True)\n        record = logging.LogRecord(\"test\", logging.INFO, \"\", 1, \"message\", None, None)\n        assert formatter.format(record) == \"message\"\n"], "sample_1115": ["def test_tensorhead_construction_with_symmetry():\n    L = TensorIndexType(\"L\")\n    A1 = TensorHead('A', [L, L], TensorSymmetry.fully_symmetric(2))\n    A2 = TensorHead('A', [L, L], TensorSymmetry.direct_product(2))\n    assert A1 == A2\n    A3 = TensorHead('A', [L, L], TensorSymmetry.fully_symmetric(-2))  # Antisymmetric\n    assert A1 != A3\n", "def test_tensor_replacement_with_metric():\n    L = TensorIndexType(\"L\", dim=2)\n    i, j, k, l = tensor_indices(\"i j k l\", L)\n    A, B, C, D = tensor_heads(\"A B C D\", [L])\n    H = TensorHead(\"H\", [L, L])\n    K = TensorHead(\"K\", [L]*4)\n\n    expr = H(i, j)\n    repl = {H(i,j): [[1,2],[3,4]], L: diag(1, -1)}\n    assert expr._extract_data(repl) == ([i, j], Array([[1, -2], [3, -4]]))\n\n    assert expr.replace_with_arrays(repl) == Array([[1, -2], [3, -4]])\n    assert expr.replace_with_arrays(repl, [i, j]) == Array([[1, -2], [3, -4]])\n    assert expr.replace_with_arrays(repl, [i, -j]) == Array([[1, 2], [3, 4]])\n    assert expr.replace_with_arrays(repl, [-i, j]) == Array([[1, -2], [-3, 4]])\n    assert expr.replace_with_arrays(repl, [-i, -j]) == Array([[1, 2], [-3, -4]])\n    assert expr.replace_with_arrays(repl, [j, i]) == Array([[1, 3], [-2, -4]])\n    assert expr.replace_with_arrays(repl, [j, -i]) == Array([[1, -3], [-2, 4]])\n    assert expr.replace_with_arrays(repl, [-j, i]) == Array([[1, 3], [2, 4]])\n    assert expr.replace_with_arrays(repl, [-j, -i]) == Array([[1, -3], [2, -4]])\n    # Test stability of optional parameter 'indices'\n    assert expr.replace_with_arrays(repl) == Array([[1, -2], [3, -4]])\n\n    expr = H(i,j)\n    repl = {H(i,j): [[1,2],[3,4]], L: diag(1, -1)}\n    assert expr._extract_data(repl) == ([i, j], Array([[1, 2], [3, 4]]))\n\n    assert expr.replace_with_arrays(repl) == Array([[1,", "def test_valued_tensor_expressions_with_commuting_components():\n    (A, B, AB, BA, C, Lorentz, E, px, py, pz, LorentzD, mu0, mu1, mu2, ndm, n0, n1,\n     n2, NA, NB, NC, minkowski, ba_matrix, ndm_matrix, i0, i1, i2, i3, i4) = _get_valued_base_test_variables()\n\n    expr = A(i0) * B(i0) + B(i0) * A(i0)\n    assert expr.canon_bp() == 2 * A(i0) * B(i0)\n\n    expr = A(i0) * B(i0) - B(i0) * A(i0)\n    assert expr.canon_bp() == 0\n\n    expr = A(i0) * B(i1) + B(i0) * A(i1)\n    assert expr.canon_bp() == A(i0) * B(i1) + A(i1) * B(i0)\n", "def test_components_data_accessors():\n    Lorentz = TensorIndexType('Lorentz', dummy_name='L')\n    i0, i1 = tensor_indices('i0:2', Lorentz)\n    E, px, py, pz = symbols('E p_x p_y p_z', positive=True)\n    c = symbols('c', positive=True)\n    P = TensorHead('P', [Lorentz])\n    P.data = [E, px, py, pz]\n\n    assert P.data == P(i0).data\n    assert P.data == P(-i0).data\n\n    assert P.data[0] == P(i0).data[0]\n    assert P.data[1] == P(i0).data[1]\n    assert P.data[2] == P(i0).data[2]\n    assert P.data[3] == P(i0).data[3]\n\n    assert P.data[0] == P(-i0).data[0]\n    assert P.data[1] == -P(-i0).data[1]\n    assert P.data[2] == -P(-i0).data[2]\n    assert P.data[3] == -P(-i0).data[3]\n\n    P(i0).data = [1, 2, 3, 4]\n    assert P.data[0] == P(i0).data[0]\n    assert P.data[1] == P(i0).data[1]\n    assert P.data[2] == P(i0).data[2]\n    assert P.data[3] == P(i0).data[3]\n\n    assert P.data[0] == P(-i0).data[0]\n    assert P.data[1] == -P(-i0).data[1]\n    assert P.data[2] == -P(-i0).data[2]\n    assert P.data[3] == -P(-i0).data[3]\n\n    assert P.data == P(i0).data\n    assert P.data == P(-i0).data\n", "def test_issue_21026_TensMul_substitution():\n    L = TensorIndexType(\"L\")\n    a, b, c, d, i = tensor_indices(\"a b c d i\", L)\n    A, B, C = tensor_heads(\"A B C\", [L, L])\n    expr = A(a, b) * B(b, c) * C(c, d)\n    expr = expr.substitute_indices((-c, -a))\n    assert expr == A(a, b) * B(b, -a) * C(-a, d)\n    expr = expr.substitute_indices((-d, -b))\n    assert expr == A(a, b) * B(b, -a) * C(-a, -b)\n"], "sample_466": ["def test_serialize_timezone(self):\n    tz = datetime.timezone.utc\n    string, imports = MigrationWriter.serialize(tz)\n    self.assertEqual(\n        string, \"datetime.timezone.utc\"\n    )\n    self.assertEqual(imports, {\"import datetime\"})\n\n    tz = get_default_timezone()\n    string, imports = MigrationWriter.serialize(tz)\n    self.assertEqual(\n        string, \"django.utils.timezone.get_default_timezone()\"\n    )\n    self.assertEqual(imports, {\"import django.utils.timezone\"})\n\n    tz = get_fixed_timezone(180)\n    string, imports = MigrationWriter.serialize(tz)\n    self.assertEqual(\n        string, \"django.utils.timezone.get_fixed_timezone(180)\"\n    )\n    self.assertEqual(imports, {\"import django.utils.timezone\"})\n\n    tz = zoneinfo.ZoneInfo(\"Europe/Paris\")\n    string, imports = MigrationWriter.serialize(tz)\n    self.assertEqual(\n        string, \"zoneinfo.ZoneInfo('Europe/Paris')\"\n    )\n    self.assertEqual(imports, {\"import zoneinfo\"})\n", "def test_serialize_timezone(self):\n    \"\"\"\n    Make sure timezone objects are properly serialized.\n    \"\"\"\n    self.assertSerializedResultEqual(\n        datetime.timezone.utc,\n        (\"datetime.timezone.utc\", {\"import datetime\"}),\n    )\n    self.assertSerializedResultEqual(\n        get_default_timezone(),\n        (\"django.utils.timezone.get_default_timezone()\", {\"from django.utils import timezone\"}),\n    )\n    self.assertSerializedResultEqual(\n        get_fixed_timezone(180),\n        (\"django.utils.timezone.get_fixed_timezone(180)\", {\"from django.utils import timezone\"}),\n    )\n    self.assertSerializedResultEqual(\n        zoneinfo.ZoneInfo(\"America/New_York\"),\n        (\n            \"zoneinfo.ZoneInfo('America/New_York')\",\n            {\"import zoneinfo\"},\n        ),\n    )\n", "def test_serialize_simple_lazy_object(self):\n    class TestModel:\n            self.value = value\n\n    obj = SimpleLazyObject(lambda: TestModel(\"test_value\"))\n    self.assertEqual(self.serialize_round_trip(obj).value, \"test_value\")\n", "def test_serialize_migrations_dependencies(self):\n    migration = type(\n        \"Migration\",\n        (migrations.Migration,),\n        {\n            \"dependencies\": [\n                (\"app\", \"0001_initial\"),\n                migrations.swappable_dependency(\"auth.User\"),\n            ]\n        },\n    )\n    writer = MigrationWriter(migration)\n    output = writer.as_string()\n    self.assertIn(\n        \"dependencies = [\\n\"\n        \"    ('app', '0001_initial'),\\n\"\n        \"    migrations.swappable_dependency(settings.AUTH_USER_MODEL),\\n\"\n        \"]\\n\",\n        output,\n    )\n\n    migration = type(\n        \"Migration\",\n        (migrations.Migration,),\n        {\n            \"dependencies\": [migrations.swappable_dependency(\"auth.User\")]\n        },\n    )\n    writer = MigrationWriter(migration)\n    output = writer.as_string()\n    self.assertIn(\n        \"dependencies = [\\n\"\n        \"    migrations.swappable_dependency(settings.AUTH_USER_MODEL),\\n\"\n        \"]\\n\",\n        output,\n    )\n\n    migration = type(\n        \"Migration\",\n        (migrations.Migration,),\n        {\n            \"dependencies\": []\n        },\n    )\n    writer = MigrationWriter(migration)\n    output = writer.as_string()\n    self.assertIn(\"dependencies = [\\n]\\n\", output)\n", "def test_serialize_timezone(self):\n    \"\"\"Ticket #27845 - Tests serializing timezone.\"\"\"\n    tz = datetime.timezone.utc\n    self.assertSerializedResultEqual(\n        tz,\n        (\n            \"datetime.timezone.utc\",\n            {\"import datetime\"},\n        ),\n    )\n    tz = datetime.timezone(datetime.timedelta(hours=2))\n    self.assertSerializedResultEqual(\n        tz,\n        (\n            \"datetime.timezone(datetime.timedelta(hours=2))\",\n            {\"import datetime\"},\n        ),\n    )\n    tz = get_fixed_timezone(180)\n    self.assertSerializedResultEqual(\n        tz,\n        (\n            \"django.utils.timezone.get_fixed_timezone(180)\",\n            {\"from django.utils import timezone\"},\n        ),\n    )\n    tz = zoneinfo.ZoneInfo(\"Europe/Paris\")\n    self.assertSerializedResultEqual(\n        tz,\n        (\n            \"zoneinfo.ZoneInfo('Europe/Paris')\",\n            {\"import zoneinfo\"},\n        ),\n    )\n    tz = datetime.datetime.now(tz).tzinfo\n    self.assertSerializedResultEqual(\n        tz,\n        (\n            \"zoneinfo.ZoneInfo('Europe/Paris')\",\n            {\"import zoneinfo\"},\n        ),\n    )\n"], "sample_486": ["def test_inlineformset_factory_saves_foreign_key_on_submit(self):\n    \"\"\"\n    Test that the foreign key is saved correctly on a valid form submission.\n    \"\"\"\n    parent = UUIDPKParent.objects.create()\n    FormSet = inlineformset_factory(UUIDPKParent, UUIDPKChild, fields=\"__all__\")\n    formset = FormSet(\n        {\n            \"uuidpkchild_set-TOTAL_FORMS\": 1,\n            \"uuidpkchild_set-INITIAL_FORMS\": 0,\n            \"uuidpkchild_set-MAX_NUM_FORMS\": \"\",\n            \"uuidpkchild_set-0-name\": \"Foo\",\n        },\n        instance=parent,\n    )\n    self.assertTrue(formset.is_valid())\n    formset.save()\n    self.assertEqual(UUIDPKChild.objects.count(), 1)\n    child = UUIDPKChild.objects.get()\n    self.assertEqual(child.parent, parent)\n", "def test_inlineformset_factory_foreign_key_to_field(self):\n    \"\"\"\n    Tests that foreign key to_field is correctly used when creating an inline formset.\n    \"\"\"\n    FormSet = inlineformset_factory(\n        ParentWithUUIDAlternateKey,\n        ChildRelatedViaAK,\n        fields=\"__all__\",\n        fk_name=\"parent\",\n        to_field=\"uuid\",\n    )\n    formset = FormSet()\n    self.assertEqual(formset.forms[0].fields[\"parent\"].to_field, \"uuid\")\n", "def test_inlineformset_factory_properly_initializes_foreign_key_field(self):\n    \"\"\"\n    Test that the foreign key field in the inline formset is properly initialized\n    with the parent instance's pk when the parent instance is not a new instance.\n    \"\"\"\n    parent = UUIDPKParent.objects.create()\n    FormSet = inlineformset_factory(UUIDPKParent, UUIDPKChild, fields=\"__all__\")\n    formset = FormSet(instance=parent)\n    self.assertEqual(formset.forms[0].fields[\"parent\"].initial, parent.pk)\n", "    def test_inlineformset_factory_nulls_default_pks_non_primary_foreign_key(self):\n        \"\"\"\n        #24958 - Variant of test_inlineformset_factory_nulls_default_pks for\n        the case of a parent object with a UUID primary key and a child\n        object that relates to a non-primary key on the parent.\n        \"\"\"\n        FormSet = inlineformset_factory(UUIDPKParent, ChildRelatedViaAK, fields=\"__all__\")\n        formset = FormSet()\n        self.assertIsNone(formset.forms[0].fields[\"parent\"].initial)\n        self.assertIsNotNone(formset.forms[0].fields[\"parent\"].to_field)\n", "    def test_inlineformset_factory_cannot_delete_extra_if_max_num_one(self):\n        \"\"\"\n        #28482 - Inlines with a max_num of 1 should not be able to delete extra\n        forms when the foreign key to the parent model is unique.\n        \"\"\"\n        FormSet = inlineformset_factory(\n            UUIDPKParent, UUIDPKChild, fields=\"__all__\", max_num=1\n        )\n        formset = FormSet(\n            {\n                \"uuidpkchild_set-TOTAL_FORMS\": 2,\n                \"uuidpkchild_set-INITIAL_FORMS\": 0,\n                \"uuidpkchild_set-MAX_NUM_FORMS\": \"\",\n                \"uuidpkchild_set-0-name\": \"Test\",\n                \"uuidpkchild_set-1-DELETE\": \"on\",\n            }\n        )\n        self.assertFalse(formset.is_valid())\n"], "sample_403": ["def test_rename_model_with_mti_and_through_model(self):\n    app_label = \"test_rename_mti_through\"\n    project_state = self.apply_operations(\n        app_label,\n        ProjectState(),\n        operations=[\n            migrations.CreateModel(\n                \"Rider\",\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                ],\n            ),\n            migrations.CreateModel(\n                \"Pony\",\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                ],\n            ),\n            migrations.CreateModel(\n                \"PonyRider\",\n                fields=[\n                    (\n                        \"rider\",\n                        models.ForeignKey(\n                            \"test_rename_mti_through.Rider\", models.CASCADE\n                        ),\n                    ),\n                    (\n                        \"pony\",\n                        models.ForeignKey(\n                            \"test_rename_mti_through.Pony\", models.CASCADE\n                        ),\n                    ),\n                ],\n            ),\n            migrations.AddField(\n                \"Pony\",\n                \"riders\",\n                models.ManyToManyField(\n                    \"test_rename_mti_through.Rider\",\n                    through=\"test_rename_mti_through.PonyRider\",\n                ),\n            ),\n            migrations.CreateModel(\n                \"ShetlandPony\",\n                fields=[\n                    (\n                        \"pony_ptr\",\n                        models.OneToOneField(\n                            \"test_rename_mti_through.Pony\",\n                            models.CASCADE,\n                            parent_link=True,\n                            primary_key=True,\n                            serialize=False,\n                        ),\n                    ),\n                ],\n            ),\n        ],\n    )\n    Pony = project_state.apps.get_model(app_label, \"Pony\")\n    Rider = project_state.apps.get_model(app_label, \"Rider\")\n    PonyRider = project_state.apps.get_model(app_label, \"PonyRider\")\n    ShetlandPony = project_state.apps.get_model(app_label, \"ShetlandPony\")\n    pony = Pony.objects.create()\n    rider = Rider.objects.create()\n    PonyRider.objects.create(pony=pony, rider=rider)\n    shetland_pony = ShetlandPony.objects.create()\n\n    project_state = self.apply_operations(\n        app_label,\n        project_state,\n        operations=[\n            migrations.RenameModel(\"Pony\", \"Pony2\"),\n        ],\n        atomic=connection.features.supports_atomic_references_rename,\n    )\n    Pony = project_state.apps.get_model(app_label, \"Pony2\")\n    Rider = project_state.apps.get_model(app_label, \"Rider\")\n    PonyRider = project_state.apps.get_model(app_label, \"PonyRider\")\n    ShetlandPony = project", "def test_alter_unique_together_remove_when_unique_constraints_exist(self):\n    app_label = \"test_aluntoremove_wuc\"\n    table_name = \"%s_pony\" % app_label\n    project_state = self.set_up_test_model(\n        app_label, unique_together=True, unique_constraints=True\n    )\n    self.assertUniqueConstraintExists(table_name, [\"pink\", \"weight\"])\n    # Remove unique together.\n    new_state = project_state.clone()\n    operation = migrations.AlterUniqueTogether(\"Pony\", set())\n    operation.state_forwards(app_label, new_state)\n    with connection.schema_editor() as editor:\n        operation.database_forwards(app_label, editor, project_state, new_state)\n    self.assertUniqueConstraintExists(table_name, [\"pink\", \"weight\"])\n    # Unique constraint wasn't removed.\n", "def test_rename_model_managers(self):\n    \"\"\"\n    RenameModel operation shouldn't remove managers.\n    \"\"\"\n    project_state = self.set_up_test_model(\"test_rnmoma\", manager_model=True)\n    # Test the state alteration\n    operation = migrations.RenameModel(\"Food\", \"Pony\")\n    self.assertEqual(operation.describe(), \"Rename model Food to Pony\")\n    self.assertEqual(operation.migration_name_fragment, \"rename_food_pony\")\n    # Test initial state and database\n    self.assertIn((\"test_rnmoma\", \"food\"), project_state.models)\n    self.assertNotIn((\"test_rnmoma\", \"pony\"), project_state.models)\n    # Migrate forwards\n    new_state = project_state.clone()\n    atomic_rename = connection.features.supports_atomic_references_rename\n    new_state = self.apply_operations(\n        \"test_rnmoma\", new_state, [operation], atomic=atomic_rename\n    )\n    # Test new state and database\n    self.assertNotIn((\"test_rnmoma\", \"food\"), new_state.models)\n    self.assertIn((\"test_rnmoma\", \"pony\"), new_state.models)\n    # RenameModel also repoints all incoming FKs and M2Ms\n    model = new_state.apps.get_model(\"test_rnmoma\", \"Pony\")\n    self.assertIsInstance(model.food_qs, models.Manager)\n    self.assertIsInstance(model.food_mgr, FoodManager)\n    self.assertEqual(model.food_mgr.args, (\"a\", \"b\", 1, 2))\n    self.assertIsInstance(model.food_mgr_kwargs, FoodManager)\n    self.assertEqual(model.food_mgr_kwargs.args, (\"x\", \"y\", 3, 4))\n    # Migrate backwards\n    original_state = self.unapply_operations(\n        \"test_rnmoma\", project_state, [operation], atomic=atomic_rename\n    )\n    # Test original state and database\n    self.assertIn((\"test_rnmoma\", \"food\"), original_state.models)\n    self.assertNotIn((\"test_rnmoma\", \"pony\"), original_state.models)\n    model = original_state.apps.get_model(\"test_rnmoma\", \"Food\")\n    self.assertIsInstance(model.food_qs, models.Manager)\n    self.assertIsInstance(model.food_mgr, FoodManager)\n    self.assertEqual(model.food_mgr.args, (\"a\", \"b\", 1, 2))\n    self.assertIsInstance(model.food_mgr_kwargs, FoodManager)\n    self.assertEqual(model.food_mgr_kwargs.args, (\"x\", \"y\", 3, 4))\n", "def test_alter_model_options_m2m(self):\n    \"\"\"\n    Tests the AlterModelOptions operation on a model with a ManyToManyField.\n    \"\"\"\n    project_state = self.set_up_test_model(\"test_almoopmm\", second_model=True)\n    # Test the state alteration\n    operation = migrations.AlterModelOptions(\n        \"Pony\",\n        options={\n            \"base_manager_name\": \"ponies\",\n        },\n    )\n    self.assertEqual(operation.describe(), \"Change Meta options on Pony\")\n    new_state = project_state.clone()\n    operation.state_forwards(\"test_almoopmm\", new_state)\n    # Ensure the M2M field manager name has been updated.\n    self.assertEqual(\n        new_state.models[\"test_almoopmm\", \"stable\"].options.get(\n            \"base_manager_name\", None\n        ),\n        \"ponies\",\n    )\n    # Test the database alteration\n    with connection.schema_editor() as editor:\n        operation.database_forwards(\"test_almoopmm\", editor, project_state, new_state)\n    # Test reversal\n    with connection.schema_editor() as editor:\n        operation.database_backwards(\"test_almoopmm\", editor, new_state, project_state)\n    # And deconstruction\n    definition = operation.deconstruct()\n    self.assertEqual(definition[0], \"AlterModelOptions\")\n    self.assertEqual(definition[1], [])\n    self.assertEqual(\n        definition[2],\n        {\n            \"name\": \"Pony\",\n            \"options\": {\"base_manager_name\": \"ponies\"},\n        },\n    )\n", "    def test_alter_unique_together_remove_to_unique_constraint(self):\n        app_label = \"test_alunto_wunct\"\n        table_name = \"%s_pony\" % app_label\n        project_state = self.set_up_test_model(\n            app_label, unique_together=True, index_together=True\n        )\n        self.assertUniqueConstraintExists(table_name, [\"pink\", \"weight\"])\n        # Add index together.\n        new_state = project_state.clone()\n        operation = migrations.AlterUniqueTogether(\"Pony\", set())\n        operation.state_forwards(app_label, new_state)\n        with connection.schema_editor() as editor:\n            operation.database_forwards(app_label, editor, project_state, new_state)\n        self.assertUniqueConstraintNotExists(table_name, [\"pink\", \"weight\"])\n        if connection.features.supports_unique_constraints:\n            self.assertIndexNotExists(table_name, [\"pink\", \"weight\"])\n        # Remove index together.\n        project_state = new_state\n        new_state = project_state.clone()\n        operation = migrations.AlterIndexTogether(\"Pony\", [(\"pink\", \"weight\")])\n        operation.state_forwards(app_label, new_state)\n        with connection.schema_editor() as editor:\n            operation.database_forwards(app_label, editor, project_state, new_state)\n        self.assertIndexExists(table_name, [\"pink\", \"weight\"])\n        # Add unique constraint.\n        project_state = new_state\n        new_state = project_state.clone()\n        constraint = models.UniqueConstraint(fields=[\"pink\"], name=\"pony_pink_key\")\n        operation = migrations.AddConstraint(\"Pony\", constraint)\n        operation.state_forwards(app_label, new_state)\n        with connection.schema_editor() as editor:\n            operation.database_forwards(app_label, editor, project_state, new_state)\n        if connection.features.supports_unique_constraints:\n            self.assertUniqueConstraintExists(table_name, [\"pink\"])\n        # Remove unique constraint.\n        project_state = new_state\n        new_state = project_state.clone()\n        operation = migrations.RemoveConstraint(\n            \"Pony\", constraint.name\n        )\n        operation.state_forwards(app_label, new_state)\n        with connection.schema_editor() as editor:\n            operation.database_forwards(app_label, editor, project_state, new_state)\n        if connection.features.supports_unique_constraints:\n            self.assertUniqueConstraintNotExists(table_name, [\"pink\"])\n        # Add unique together.\n        project_state = new_state\n        new_state = project_state.clone()\n        operation = migrations.AlterUniqueTogether(\"Pony\", [(\""], "sample_1140": ["def test_pretty_denom():\n    from sympy import symbols, sin, cos\n    x = symbols('x')\n    ascii_str = \\", "def test_pretty_SingularityFunction_notation():\n    assert xpretty(SingularityFunction(x, 0, n), use_unicode=True) == (", "def test_pretty_polynomial_gcd():\n    from sympy import Poly\n    x = Symbol('x')\n    p1 = Poly(x**2 - 3*x - 2, x)\n    p2 = Poly(x**2 - x - 2, x)\n    p3 = Poly(x**2 + x + 1, x)\n    p4 = Poly(x**2 + x - 2, x)\n    p5 = Poly(x**2 - 2*x - 2, x)\n    p6 = Poly(x**2 - 2*x + 2, x)\n    assert upretty(p1.gcd(p2)) == \"x + 1\"\n    assert upretty(p1.gcd(p3)) == \"1\"\n    assert upretty(p1.gcd(p4)) == \"x + 1\"\n    assert upretty(p2.gcd(p3)) == \"1\"\n    assert upretty(p2.gcd(p4)) == \"x - 1\"\n    assert upretty(p2.gcd(p5)) == \"1\"\n    assert upretty(p2.gcd(p6)) == \"x - 1\"\n    assert upretty(p3.gcd(p4)) == \"1\"\n    assert upretty(p3.gcd(p5)) == \"1\"\n    assert upretty(p3.gcd(p6)) == \"1\"\n    assert upretty(p4.gcd(p5)) == \"1\"\n    assert upretty(p4.gcd(p6)) == \"x + 1\"\n    assert upretty(p5.gcd(p6)) == \"1\"\n", "def test_pretty_vector_derivatives():\n    from sympy.vector import CoordSys3D\n    C = CoordSys3D('C')\n    x, y, z = C.base_vectors()\n    f = C.cartesian_to_curvilinear(x + y)\n    u = C.cartesian_to_curvilinear(x)\n    v = C.cartesian_to_curvilinear(y)\n    w = C.cartesian_to_curvilinear(z)\n    assert upretty(f.diff(x)) == '\u2202\u20d7(x_A + y_A)/\u2202x_A = 1'\n    assert upretty(x.diff(x)) == '\u2202\u20d7x_A/\u2202x_A = 1'\n    assert upretty(y.diff(x)) == '\u2202\u20d7y_A/\u2202x_A = 0'\n    assert upretty(x.diff(y)) == '\u2202\u20d7x_A/\u2202y_A = 0'\n    assert upretty(y.diff(y)) == '\u2202\u20d7y_A/\u2202y_A = 1'\n    assert upretty(z.diff(z)) == '\u2202\u20d7z_A/\u2202z_A = 1'\n    assert upretty(x.diff(z)) == '\u2202\u20d7x_A/\u2202z_A = 0'\n    assert upretty(y.diff(z)) == '\u2202\u20d7y_A/\u2202z_A = 0'\n    assert upretty(z.diff(x)) == '\u2202\u20d7z_A/\u2202x_A = 0'\n    assert upretty(z.diff(y)) == '\u2202\u20d7z_A/\u2202y_A = 0'\n    assert upretty(u.diff(v)) == '\u2202\u20d7u_A/\u2202v_A = 0'\n    assert upretty(v.diff(w)) == '\u2202\u20d7v_A/\u2202w_A = 0'\n    assert upretty(w.diff(u)) == '\u2202\u20d7w_A/\u2202u_A = 0'\n    assert upretty(u.diff(u)) == '\u2202\u20d7u_A/\u2202u_A = 1'\n    assert upretty(v.diff(v)) == '\u2202\u20d7v_A/\u2202v_A = 1'\n    assert upretty(w.diff(w)) == '\u2202\u20d7w_A/\u2202w_A = 1", "def test_unimplemented():\n    from sympy import Function, Rational\n    from sympy.diffgeom import CoordSys3D\n    v = CoordSys3D('v')\n    p = Rational(1,2)\n    f = Function('f')(v.x)\n\n    # Test that the unimplemented error is raised for CoordSys3D\n    with raises(NotImplementedError):\n        pretty(p*f)\n"], "sample_682": ["def test_xfail_condition_with_invalid_syntax(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(condition=\" invalid syntax \")\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-rsx\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*ERROR*test_func*\",\n            \"*evaluating*xfail*condition*\",\n            \"*invalid syntax*\",\n            \"*^*\",\n            \"*SyntaxError: invalid syntax*\",\n            \"*1 error*\",\n        ]\n    )\n", "def test_xfail_unicode_reason(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(reason=\"\u00e9\")\n            assert 0\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-rsxX\")\n    result.stdout.fnmatch_lines(\n        [\"*XFAIL*test_func*\u00e9*\"]\n    )\n", "def test_xfail_with_multiple_conditions(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(\"sys.platform.startswith('win')\")\n        @pytest.mark.xfail(\"sys.platform.startswith('linux')\")\n            assert 0\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-rxs\")\n    result.stdout.fnmatch_lines(\n        [\"*XFAIL*\", \"*condition: sys.platform.startswith('win')*\"]\n    )\n", "def test_skipif_conditional_with_namespace(testdir):\n    testdir.makepyfile(\n        test_foo=\"\"\"\n            import pytest\n            import mymodule\n            @pytest.mark.skipif(\"mymodule.has_feature\")\n                assert 0\n        \"\"\",\n        mymodule=\"\"\"\n            has_feature = True\n        \"\"\",\n    )\n    result = testdir.runpytest(\"-rs\")\n    result.stdout.fnmatch_lines([\"*SKIP*condition: mymodule.has_feature*\"])\n    assert result.ret == 0\n", "    def test_skipif_marker_order(self, testdir):\n        testdir.makepyfile(\n            test_foo=\"\"\"\n            import pytest\n            @pytest.mark.skipif(\"False\", reason='first_condition')\n            @pytest.mark.skipif(\"True\", reason='second_condition')\n                assert 1\n        \"\"\"\n        )\n        result = testdir.runpytest(\"-rs\")\n        result.stdout.fnmatch_lines(\n            [\n                \"*SKIP*1*test_foo.py*second_condition*\",\n                \"*1 skipped*\"\n            ]\n        )\n        assert result.ret == 0\n"], "sample_679": ["def test_mark_evaluator_invalidraise(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.skipif(\"1/0\")\n            pass\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"Error evaluating 'skipif' expression\",\n            \"    1/0\",\n            \"  File \\\"<string>\\\", line 1, in <module>\",\n            \"    ZeroDivisionError: division by zero\",\n        ]\n    )\n    assert result.ret == 0\n", "def test_mark_option_with_evaluated_expression(expr: str, expected_passed: str, testdir) -> None:\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        xyz = True\n        xyz2 = False\n        @pytest.mark.eval(expr)\n            pass\n        @pytest.mark.eval(\"xyz2\")\n            pass\n    \"\"\"\n    )\n    rec = testdir.inline_run(\"-m\", expr)\n    passed, skipped, failed = rec.listoutcomes()\n    passed = [x.nodeid.split(\"::\")[-1] for x in passed]\n    assert passed == expected_passed\n", "def test_mark_evaluator_invalidraise(testdir):\n    testdir.makeconftest(\n        \"\"\"\n        import pytest\n        pytestmark = pytest.mark.eval\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n            assert 1 / 0\n    \"\"\"\n    )\n    reprec = testdir.inline_run()\n    passed, skipped, failed = reprec.listoutcomes()\n    assert len(failed) == 1\n    assert failed[0].nodeid.endswith(\"test_func\")\n    dlist = reprec.getcalls(\"pytest_deselected\")\n    assert len(dlist) == 0\n\n    testdir.makeconftest(\n        \"\"\"\n        import pytest\n        pytestmark = pytest.mark.eval(raises=ZeroDivisionError)\n    \"\"\"\n    )\n    reprec = testdir.inline_run()\n    passed, skipped, failed = reprec.listoutcomes()\n    assert len(passed) == 1\n    assert passed[0].nodeid.endswith(\"test_func\")\n    dlist = reprec.getcalls(\"pytest_deselected\")\n    assert len(dlist) == 0\n", "def test_mark_evaluator_istrue(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.my_marker(condition=\"xyz\")\n            assert True\n\n        @pytest.mark.my_marker(condition=\"xyz2\")\n            assert False\n\n        xyz = True\n        xyz2 = False\n        \"\"\"\n    )\n    reprec = testdir.inline_run(\"-m\", \"my_marker\")\n    passed, skipped, failed = reprec.countoutcomes()\n    assert passed == 1\n    assert skipped == failed == 0\n", "def test_mark_evaluator_invalid_raise(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.skipif(\"1 + 1 == 2\", reason=\"skip because of math\")\n            assert 0\n    \"\"\"\n    )\n    reprec = testdir.inline_run()\n    passed, skipped, failed = reprec.listoutcomes()\n    assert len(skipped) == 1\n    assert skipped[0].nodeid.split(\"::\")[-1] == \"test_skip\"\n    assert skipped[0].keyword == \"skipif\"\n\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.xfail(\"1 + 1 == 2\", reason=\"xfail because of math\")\n            assert 0\n    \"\"\"\n    )\n    reprec = testdir.inline_run()\n    passed, skipped, failed = reprec.listoutcomes()\n    assert len(failed) == 1\n    assert failed[0].nodeid.split(\"::\")[-1] == \"test_xfail\"\n    assert failed[0].keyword == \"xfail\"\n"], "sample_343": ["    def test_bulk_related_objects(self):\n        question = Question.objects.create(text='test')\n        answer1 = Answer.objects.create(question=question)\n        answer2 = Answer.objects.create(question=question)\n\n        related_objects = question.answer_set.bulk_related_objects([question])\n        self.assertCountEqual([obj.pk for obj in related_objects], [answer1.pk, answer2.pk])\n", "    def test_contribute_to_class(self):\n        class Model(models.Model):\n            answer_set = GenericRelation(Answer, content_type_field='content_type', object_id_field='object_id')\n        \n        self.assertIsInstance(Model.answer_set, ReverseGenericManyToOneDescriptor)\n        self.assertEqual(Model.answer_set.field, Model._meta.get_field('answer_set'))\n", "    def test_generic_relation_filter(self):\n        question = Question.objects.create(text='test')\n        answer1 = Answer.objects.create(question=question)\n        answer2 = Answer.objects.create(question=question)\n\n        self.assertEqual(Answer.question.get_forward_related_filter(question), {'object_id': question.pk, 'content_type': question.content_type_id})\n\n        self.assertEqual(Answer.question.get_filter_kwargs_for_object(answer1), {'object_id': answer1.object_id, 'content_type': answer1.content_type})\n", "    def test_add_bulk_with_multiple_objects(self):\n        question = Question.objects.create(text='test')\n        answer1 = Answer(question=question)\n        answer2 = Answer(question=question)\n        question.answer_set.add(answer1, answer2, bulk=True)\n        self.assertEqual(Answer.objects.count(), 2)\n        self.assertEqual(question.answer_set.count(), 2)\n", "    def test_get_queryset(self):\n        question = Question.objects.create(text='test')\n        answer1 = Answer.objects.create(question=question)\n        answer2 = Answer.objects.create(question=question)\n\n        related_manager = question.answer_set\n        with self.assertNumQueries(1):\n            self.assertCountEqual(related_manager.get_queryset(), [answer1, answer2])\n"], "sample_1059": ["def test_chebyshev_roots():\n    assert chebyshevt_root(1, 0) == 0\n    assert chebyshevu_root(1, 0) == 0\n    assert chebyshevt_root(2, 0) == -S(1)/2\n    assert chebyshevu_root(2, 0) == -S(1)/2\n    assert chebyshevt_root(2, 1) == S(1)/2\n    assert chebyshevu_root(2, 1) == S(1)/2\n    assert chebyshevt_root(3, 0) == -S.sqrt(3)/2\n    assert chebyshevu_root(3, 0) == -S.sqrt(2)/2\n    assert chebyshevt_root(3, 1) == 0\n    assert chebyshevu_root(3, 1) == S.sqrt(2)/2\n    assert chebyshevt_root(3, 2) == S.sqrt(3)/2\n    assert chebyshevu_root(3, 2) == 0\n    assert chebyshevt_root(4, 0) == -S.sqrt(2 + S.sqrt(2))/2\n    assert chebyshevu_root(4, 0) == -S.sqrt(3 + 2*S.sqrt(2))/2\n    assert chebyshevt_root(4, 1) == -S.sqrt(2 - S.sqrt(2))/2\n    assert chebyshevu_root(4, 1) == -S.sqrt(3 - 2*S.sqrt(2))/2\n    assert chebyshevt_root(4, 2) == S.sqrt(2 - S.sqrt(2))/2\n    assert chebyshevu_root(4, 2) == S.sqrt(3 - 2*S.sqrt(2))/2\n    assert chebyshevt_root(4, 3) == S.sqrt(2 + S.sqrt(2))/2\n    assert chebyshevu_root(4, 3) == S.sqrt(3 + 2*S.sqrt(2))/2\n\n    n = Symbol('n')\n    k = Symbol('k')\n    assert chebyshevt_root(n,", "def test_chebyshev_roots():\n    n = Symbol(\"n\")\n    k = Symbol(\"k\")\n    X = chebyshevt_root(n, k)\n    assert isinstance(X, chebyshevt_root)\n    assert chebyshevt_root(1, 0) == 0\n    assert chebyshevt_root(2, 1) == -sqrt(3)/2\n    assert chebyshevt_root(2, 0) == sqrt(3)/2\n    assert chebyshevt_root(3, 2) == -sqrt(3)/2\n    assert chebyshevt_root(3, 0) == sqrt(3)/2\n    assert chebyshevt_root(3, 1) == 0\n    assert chebyshevt_root(4, 3) == -sqrt(2 + sqrt(2))/2\n    assert chebyshevt_root(4, 2) == sqrt(2 + sqrt(2))/2\n    assert chebyshevt_root(4, 1) == sqrt(2 - sqrt(2))/2\n    assert chebyshevt_root(4, 0) == -sqrt(2 - sqrt(2))/2\n    assert chebyshevt_root(5, 0) == sqrt(5 - 2*sqrt(5))/2\n    assert chebyshevt_root(5, 2) == 0\n    assert chebyshevt_root(5, 4) == -sqrt(5 - 2*sqrt(5))/2\n\n    assert chebyshevu_root(1, 0) == 0\n    assert chebyshevu_root(2, 1) == -sqrt(2)/2\n    assert chebyshevu_root(2, 0) == sqrt(2)/2\n    assert chebyshevu_root(3, 2) == -sqrt(2)/2\n    assert chebyshevu_root(3, 0) == sqrt(2)/2\n    assert chebyshevu_root(3, 1) == 0\n    assert chebyshevu_root(4, 3) == -sqrt(3 - sqrt(3))/2\n    assert chebyshevu_root(4, 2)", "def test_extreme_values():\n    n = Symbol(\"n\")\n    a = Symbol(\"a\")\n    b = Symbol(\"b\")\n    alpha = Symbol(\"alpha\")\n\n    # Test extreme values\n    assert jacobi(n, a, b, zoo) == oo\n    assert jacobi(n, a, b, -oo) == oo\n    assert gegenbauer(n, alpha, zoo) == oo\n    assert gegenbauer(n, alpha, -oo) == oo\n    assert chebyshevt(n, zoo) == oo\n    assert chebyshevt(n, -oo) == oo\n    assert chebyshevu(n, zoo) == oo\n    assert chebyshevu(n, -oo) == oo\n    assert legendre(n, zoo) == oo\n    assert legendre(n, -oo) == oo\n    assert assoc_legendre(n, a, zoo) == oo\n    assert assoc_legendre(n, a, -oo) == oo\n    assert hermite(n, zoo) == oo\n    assert hermite(n, -oo) == oo\n    assert laguerre(n, zoo) == (-1)**n*oo\n    assert laguerre(n, -oo) == oo\n    assert assoc_laguerre(n, alpha, zoo) == (-1)**n*oo\n    assert assoc_laguerre(n, alpha, -oo) == oo\n\n    # Test edge cases for x=1, x=-1, x=0, n=0, a=0, b=0, alpha=0\n    assert jacobi(n, a, b, 1) == RisingFactorial(a + 1, n)/factorial(n)\n    assert jacobi(n, a, b, -1) == (-1)**n*RisingFactorial(b + 1, n)/factorial(n)\n    assert jacobi(n, a, b, 0) == 2**(-n)*gamma(a + n + 1)*hyper((-b - n, -n), (a + 1,), -1)/(factorial(n)*gamma(a + 1))\n    assert jacobi(0, a, b, x) == 1\n    assert jacobi(n, 0, 0, x) == legendre(n, x)\n    assert jacobi(n, a, 0, x) == (2**(-n", "def test_orthogonal_polynomial():\n    n = Symbol(\"n\")\n    x = Symbol(\"x\")\n    a = Symbol(\"a\")\n    b = Symbol(\"b\")\n    alpha = Symbol(\"alpha\")\n\n    assert OrthogonalPolynomial._eval_at_order(jacobi, n, x) == jacobi(n, x)\n\n    raises(ArgumentIndexError, lambda: OrthogonalPolynomial._eval_conjugate(\n        jacobi(n, a, b, x), 0))\n    raises(ArgumentIndexError, lambda: OrthogonalPolynomial._eval_conjugate(\n        jacobi(n, a, b, x), 2))\n    raises(ArgumentIndexError, lambda: OrthogonalPolynomial._eval_conjugate(\n        jacobi(n, a, b, x), 3))\n    raises(ArgumentIndexError, lambda: OrthogonalPolynomial._eval_conjugate(\n        jacobi(n, a, b, x), 4))\n    raises(ArgumentIndexError, lambda: OrthogonalPolynomial._eval_conjugate(\n        jacobi(n, a, b, x), 5))\n    raises(ArgumentIndexError, lambda: OrthogonalPolynomial._eval_conjugate(\n        jacobi(n, a, b, x), 6))\n\n    X = OrthogonalPolynomial(n, a, b, x, alpha)\n    assert isinstance(X, OrthogonalPolynomial)\n\n    assert OrthogonalPolynomial._eval_conjugate(X) == X.func(\n        n.conjugate(), a.conjugate(), b.conjugate(), x.conjugate(), alpha.conjugate())\n", "def test_jacobi_diff():\n    n = Symbol(\"n\")\n    a = Symbol(\"a\")\n    b = Symbol(\"b\")\n\n    # Test jacobi.diff(n) raises error\n    raises(ArgumentIndexError, lambda: jacobi(n, a, b, x).fdiff(1))\n\n    # Test jacobi.diff(a) and jacobi.diff(b)\n    assert jacobi(n, a, b, x).fdiff(2).dummy_eq(Sum((jacobi(n, a, b, x) +\n            (2*_k + a + b + 1)*RisingFactorial(_k + b + 1, n - _k)*jacobi(_k, a,\n            b, x)/((-_k + n)*RisingFactorial(_k + a + b + 1, n - _k)))/(_k +\n            a + b + n + 1), (_k, 0, n - 1)))\n    assert jacobi(n, a, b, x).fdiff(3).dummy_eq(Sum(((-1)**(-_k + n)*(2*_k +\n            a + b + 1)*RisingFactorial(_k + a + 1, n - _k)*jacobi(_k, a, b, x)/\n            ((-_k + n)*RisingFactorial(_k + a + b + 1, n - _k)) + jacobi(n, a,\n            b, x))/(_k + a + b + n + 1), (_k, 0, n - 1)))\n\n    # Test jacobi.diff(x)\n    assert jacobi(n, a, b, x).fdiff(4) == (a/2 + b/2 + n/2 + S(1)/2) * jacobi(n - 1, a + 1, b + 1, x)\n\n    # Test jacobi.diff(x) when a + b + 2*n is an integer\n    raises(ValueError, lambda: jacobi(2, 1, 1, x).fdiff(4))\n"], "sample_142": ["def test_inline_with_multiple_fk(self):\n    class TwoAlbumFKAndAnEInline(admin.TabularInline):\n        model = TwoAlbumFKAndAnE\n        fk_name = \"album1\"\n\n    class TwoAlbumFKAndAnEInlineWithNoFKName(admin.TabularInline):\n        model = TwoAlbumFKAndAnE\n\n    class MyAdmin(admin.ModelAdmin):\n        inlines = [TwoAlbumFKAndAnEInline, TwoAlbumFKAndAnEInlineWithNoFKName]\n\n    errors = MyAdmin(Album, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"'admin_checks.TwoAlbumFKAndAnE' has more than one ForeignKey to \"\n            \"'admin_checks.Album'. You must specify a 'fk_name' attribute.\",\n            obj=TwoAlbumFKAndAnEInlineWithNoFKName,\n            id='admin.E202',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_construct_instance_with_empty_required_fields(self):\n    class TestForm(forms.ModelForm):\n        class Meta:\n            model = Song\n            fields = ['title', 'album']\n\n    form = TestForm(data={'title': '', 'album': ''})\n    self.assertFalse(form.is_valid())\n    instance = Album()\n    with self.assertRaises(ValueError):\n        construct_instance(form, instance)\n", "def test_model_form_metaclass_validates_fields(self):\n    class ModelFormMetaclassTestCase(forms.ModelForm):\n        class Meta:\n            fields = '__all__'\n            model = Song\n\n    with self.assertRaises(ImproperlyConfigured):\n        class ModelFormMetaclassTestCase(forms.ModelForm, metaclass=ModelFormMetaclass):\n            pass\n\n    class ModelFormMetaclassTestCase(forms.ModelForm, metaclass=ModelFormMetaclass):\n        class Meta:\n            fields = '__all__'\n            model = Song\n            fields = ('title',)\n\n    class ModelFormMetaclassTestCase(forms.ModelForm, metaclass=ModelFormMetaclass):\n        class Meta:\n            exclude = '__all__'\n            model = Song\n\n    with self.assertRaises(ImproperlyConfigured):\n        class ModelFormMetaclassTestCase(forms.ModelForm, metaclass=ModelFormMetaclass):\n            class Meta:\n                fields = 'title'\n                model = Song\n\n    with self.assertRaises(ImproperlyConfigured):\n        class ModelFormMetaclassTestCase(forms.ModelForm, metaclass=ModelFormMetaclass):\n            class Meta:\n                exclude = 'title'\n                model = Song\n", "def test_custom_fields_in_fieldsets(self):\n    \"\"\"\n    Regression test for #29266 - custom fields in fieldsets are not checked\n    correctly.\n    \"\"\"\n    class SongForm(forms.ModelForm):\n        extra_data = forms.CharField()\n\n        class Meta:\n            model = Song\n            fields = '__all__'\n\n    class FieldsOnFormOnlyAdmin(admin.ModelAdmin):\n        form = SongForm\n        fieldsets = (\n            (None, {\n                'fields': ['title', 'extra_data']\n            }),\n        )\n\n    errors = FieldsOnFormOnlyAdmin(Song, AdminSite()).check()\n    self.assertEqual(errors, [])\n\n    class FieldsOnFormOnlyAdmin(admin.ModelAdmin):\n        form = SongForm\n        fieldsets = (\n            (None, {\n                'fields': ['extra_data', 'non_model_field']\n            }),\n        )\n\n    errors = FieldsOnFormOnlyAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'fieldsets[0][1]['fields']' contains both model and \"\n            \"non-model field(s) ('non_model_field').\",\n            obj=FieldsOnFormOnlyAdmin,\n            id='admin.E016'\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_model_to_dict(self):\n    album = Album(title='Test Album')\n    album.save()\n    song = Song(title='Test Song', album=album, original_release=1990)\n    song.save()\n    data = model_to_dict(song, fields=['title', 'album', 'original_release'])\n    self.assertEqual(data, {\n        'title': 'Test Song',\n        'album': album,\n        'original_release': 1990,\n    })\n"], "sample_124": ["def test_field_deep_copy_validators(self):\n    class CustomCharField(CharField):\n            kwargs['validators'] = [RegexValidator(regex='^a.*')]\n            super().__init__(**kwargs)\n\n    field = CustomCharField()\n    field_copy = copy.deepcopy(field)\n    self.assertIsInstance(field_copy, CustomCharField)\n    self.assertIsNot(field_copy.validators, field.validators)\n    self.assertEqual(field_copy.validators, field.validators)\n", "def test_boundfield_html(self):\n    class MyForm(Form):\n        field = CharField()\n\n    boundfield = MyForm()['field']\n\n    self.assertIsInstance(boundfield.__html__(), str)\n    self.assertEqual(boundfield.__html__(), str(boundfield))\n", "def test_regexfield_regex_validation(self):\n    # Test that the RegexField validates against the regular expression.\n    class RegexTestForm(Form):\n        regex_field = RegexField(regex=r'^[0-9]+$')\n\n    form = RegexTestForm({'regex_field': '123'})\n    self.assertTrue(form.is_valid())\n\n    form = RegexTestForm({'regex_field': 'abc'})\n    self.assertFalse(form.is_valid())\n    self.assertEqual(form.errors['regex_field'], ['Enter a valid value.'])\n", "def test_field_invalid_initial_data(self):\n    class CustomField(CharField):\n            if value == 'invalid':\n                raise ValidationError('Invalid data.')\n            return value\n\n    class CustomForm(Form):\n        custom = CustomField(initial='invalid')\n\n    f = CustomForm()\n    self.assertHTMLEqual(f.as_table(), '<tr><th>Custom:</th><td>'\n                         '<ul class=\"errorlist\"><li>Invalid data.</li></ul>'\n                         '<input type=\"text\" name=\"custom\" required></td></tr>')\n", "def test_multivaluefield_optional_subfields_with_initial(self):\n    class PhoneField(MultiValueField):\n            fields = (\n                CharField(label='Country Code', validators=[\n                    RegexValidator(r'^\\+[0-9]{1,2}$', message='Enter a valid country code.')]),\n                CharField(label='Phone Number'),\n                CharField(label='Extension', error_messages={'incomplete': 'Enter an extension.'}),\n                CharField(label='Label', required=False, help_text='E.g. home, work.'),\n            )\n            super().__init__(fields, *args, **kwargs)\n\n            if data_list:\n                return '%s.%s ext. %s (label: %s)' % tuple(data_list)\n            return None\n\n    # An optional MultiValueField with require_all_fields=False and initial data\n    # should not raise an incomplete error.\n    f = PhoneField(required=False, require_all_fields=False, initial=['+61', '287654321', '123'])\n    self.assertEqual(f.clean('+61'), '+61.287654321 ext. 123 (label: )')\n\n    # An optional MultiValueField with require_all_fields=False and initial data\n    # should raise a validation error if the initial data is invalid.\n    f = PhoneField(required=False, require_all_fields=False, initial=['61', '287654321', '123'])\n    with self.assertRaisesMessage(ValidationError, \"'Enter a valid country code.'\"):\n        f.clean('+61')\n"], "sample_1011": ["def test_hankel():\n    assert octave_code(hankel1(1, x)) == 'besselh(1, 1, x)'\n    assert octave_code(hankel2(1, x)) == 'besselh(1, 2, x)'\n", "def test_MatrixSlice_printing():\n    # test cases for issue #11821\n    A = MatrixSymbol(\"A\", 3, 3)\n    B = MatrixSymbol(\"B\", 3, 3)\n    C = MatrixSymbol(\"C\", 3, 3)\n    assert mcode(A[:2, 0]) == \"A(1:2, 1)\"\n    assert mcode(A[0, 1:]) == \"A(1, 2:3)\"\n    assert mcode(A[1:, 1:]) == \"A(2:3, 2:3)\"\n    assert mcode(A[1:, :]) == \"A(2:3, :)\"\n    assert mcode(A[:, 1:]) == \"A(:, 2:3)\"\n    assert mcode(A[:, :]) == \"A(:, :)\"\n\n    F = C[:2, 1:].subs(C, A - B)\n    assert mcode(F) == \"(-B + A)(1:2, 2:3)\"\n", "def test_octave_matrix_hadamard():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    C = HadamardProduct(A, B)\n    assert mcode(A + C) == \"(A + A.*B)\"\n    assert mcode(C + A) == \"(A.*B + A)\"\n    assert mcode(A + 2*C) == \"(A + 2*(A.*B))\"\n    assert mcode(2*C + A) == \"(2*(A.*B) + A)\"\n    assert mcode(C + C) == \"(A.*B + A.*B)\"\n    assert mcode(C + C + A) == \"(A + 2*(A.*B))\"\n    assert mcode(A + A + C) == \"(2*A + A.*B)\"\n    assert mcode(2*A + 3*C) == \"(2*A + 3*(A.*B))\"\n", "def test_Assignment():\n    x, y, z = symbols('x, y, z')\n    assert mcode(Assignment(x, y)) == \"x = y;\"\n    assert mcode(Assignment(x, y+z)) == \"x = y + z;\"\n    A = MatrixSymbol('A', 1, 3)\n    B = MatrixSymbol('B', 1, 3)\n    assert mcode(Assignment(A, B)) == \"A = B;\"\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    assert mcode(Assignment(A, B)) == \"A = B;\"\n    C = MatrixSymbol('C', 1, 3)\n    assert mcode(Assignment(C, x+y+z)) == \"C = x + y + z;\"\n    assert mcode(Assignment(x, C)) == \"x = C;\"\n    assert mcode(Assignment(C, x)) == \"C = x;\"\n", "def test_octave_exp():\n    # Check exponential with symbolic base\n    assert mcode(exp(x)) == \"exp(x)\"\n    # Check exponential with constant base\n    assert mcode(exp(2)) == \"exp(2)\"\n    # Check exponential with constant base (non-integer)\n    assert mcode(exp(2.5)) == \"exp(2.5)\"\n    # Check exponential with negative constant base\n    assert mcode(exp(-2)) == \"exp(-2)\"\n    # Check exponential with a non-constant base\n    assert mcode(exp(x + y)) == \"exp(x + y)\"\n    # Check exponential with non-constant base including numbers\n    assert mcode(exp(x + 3)) == \"exp(x + 3)\"\n    # Check exponential with non-constant base including fractions\n    assert mcode(exp(x + Rational(1, 2))) == \"exp(x + 1/2)\"\n    # Check exponential with non-constant base including pi\n    assert mcode(exp(x + pi)) == \"exp(x + pi)\"\n"], "sample_186": ["def test_check_inlines_property_with_non_iterable(self):\n    class CitiesInline(admin.TabularInline):\n        model = City\n\n    class StateAdmin(admin.ModelAdmin):\n        @property\n            return 'not a list'\n\n    errors = StateAdmin(State, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'inlines' must be a list or tuple.\",\n            obj=StateAdmin,\n            id='admin.E103',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_radio_fields_key_not_editable(self):\n    class SongAdmin(admin.ModelAdmin):\n        radio_fields = {'original_release': admin.HORIZONTAL}\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'radio_fields' refers to 'original_release', which \"\n            \"does not refer to a field that can be edited via the admin.\",\n            obj=SongAdmin,\n            id='admin.E023',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_check_ordering(self):\n    class SongAdmin(admin.ModelAdmin):\n        ordering = ['title', '-rating']\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    self.assertEqual(errors, [])\n\n    class SongAdmin(admin.ModelAdmin):\n        ordering = ['nonexistent']\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'ordering[0]' refers to 'nonexistent', which is not an attribute of 'admin_checks.Song'.\",\n            obj=SongAdmin,\n            id='admin.E033',\n        )\n    ]\n    self.assertEqual(errors, expected)\n\n    class SongAdmin(admin.ModelAdmin):\n        ordering = ['rating', '?', 'title']\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'ordering' has the random ordering marker '?', but contains other fields as well.\",\n            hint='Either remove the \"?\", or remove the other fields.',\n            obj=SongAdmin,\n            id='admin.E032',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_ordering_random_with_fields(self):\n    \"\"\"\n    Regression test for #26192 - admin's ModelAdmin.ordering with '?' and other fields\n    \"\"\"\n    class SongAdmin(admin.ModelAdmin):\n        ordering = ['?', 'title']\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'ordering' has the random ordering marker '?', \"\n            \"but contains other fields as well.\",\n            hint='Either remove the \"?\", or remove the other fields.',\n            obj=SongAdmin,\n            id='admin.E032',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_autocomplete_fields_through(self):\n    \"\"\"\n    Regression test for #26240: ensure autocomplete_fields can handle field names\n    with a 'through' option.\n    \"\"\"\n    class BookAdmin(admin.ModelAdmin):\n        autocomplete_fields = ['authors']\n\n    errors = BookAdmin(Book, AdminSite()).check()\n    self.assertEqual(errors, [])\n"], "sample_409": ["def do_get_language_info(parser, token):\n    \"\"\"\n    Store the language information dictionary for the given language code in a\n    context variable.\n\n    Usage::\n\n        {% get_language_info for LANGUAGE_CODE as l %}\n        {{ l.code }}\n        {{ l.name }}\n        {{ l.name_translated }}\n        {{ l.name_local }}\n        {{ l.bidi|yesno:\"bi-directional,uni-directional\" }}\n    \"\"\"\n    args = token.split_contents()\n    if len(args) != 5 or args[1] != \"for\" or args[3] != \"as\":\n        raise TemplateSyntaxError(\n            \"'%s' requires 'for string as variable' (got %r)\" % (args[0], args[1:])\n        )\n    return GetLanguageInfoNode(parser.compile_filter(args[2]), args[4])\n\n", "    def test_blocktrans_render_value(self):\n        output = self.engine.render_to_string(\"template\")\n        self.assertEqual(output, \"\")\n", "def test_blocktranslate_with_asvar_no_space(self):\n    with translation.override(\"de\"):\n        output = self.engine.render_to_string(\"i18n42\")\n    self.assertEqual(output, \">text<\")\n", "    def test_blocktranslate_assign_to_same_name(self):\n        with self.assertRaisesMessage(\n            TemplateSyntaxError,\n            \"The 'with' option in '{}' tag can't assign to the same name more than once.\".format(\n                self.tag_name\n            ),\n        ):\n            self.engine.render_to_string(\"template\", {\"a\": \"value\"})\n", "    def test_blocktranslate_asvar_context(self):\n        \"\"\"\n        Verify that the 'asvar' option and 'context' option can be used together.\n        \"\"\"\n        template_string = \"\"\"\n        {% load i18n %}\n        {% blocktranslate asvar my_var context \"greeting\" %}Hello{% endblocktranslate %}\n        {{ my_var }}\n        \"\"\"\n        template = self.engine.from_string(template_string)\n        with translation.override(\"de\"):\n            output = template.render(Context())\n        self.assertEqual(output, \"Hallo\")\n"], "sample_709": ["def test_pytester_getpublicnames(pytester: Pytester) -> None:\n    assert get_public_names([\"foo\", \"_bar\", \"baz\"]) == [\"foo\", \"baz\"]\n", "def test_pytester_runresult_stdout_stderr(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n            print(\"stdout\")\n\n            import sys\n            print(\"stderr\", file=sys.stderr)\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    assert \"stdout\" in result.stdout.str()\n    assert \"stderr\" in result.stderr.str()\n", "def test_pytester_run_with_env(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import os\n\n            assert os.environ[\"PYTEST_DEBUG_TEMPROOT\"] == pytester._test_tmproot\n            assert os.environ[\"HOME\"] == str(pytester.path)\n            assert os.environ[\"USERPROFILE\"] == str(pytester.path)\n            assert os.environ[\"PY_COLORS\"] == \"0\"\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-p\", \"pytester\")\n    assert result.ret == ExitCode.OK\n", "def test_pytester_run_KeyboardInterrupt(pytester: Pytester) -> None:\n    testfile = pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.xfail\n            raise KeyboardInterrupt\n    \"\"\"\n    )\n    result = pytester.runpytest_inprocess(str(testfile))\n    result.assert_outcomes(xfailed=1)\n    assert result.ret == ExitCode.INTERRUPTED\n", "def test_pytester_getpublicnames(pytester: Pytester) -> None:\n    \"\"\"Test that get_public_names function works correctly.\"\"\"\n    names = [\"test_foo\", \"_test_bar\", \"test_baz\"]\n    assert pytester_mod.get_public_names(names) == [\"test_foo\", \"test_baz\"]\n\n    names = [\"_test_foo\", \"_test_bar\", \"_test_baz\"]\n    assert pytester_mod.get_public_names(names) == []\n"], "sample_362": ["def test_add_unique_constraint(self):\n    \"\"\"Tests the creation of a unique constraint.\"\"\"\n    changes = self.get_changes([self.author_empty, self.book], [\n        self.author_empty, self.book_with_foo_together_2,\n    ])\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, [\n        'AlterUniqueTogether',\n    ])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='book', unique_together={('title', 'author')})\n", "def test_renamed_field_with_unique_constraint(self):\n    \"\"\"\n    Renaming a field that is part of a unique constraint should remove\n    the constraint with the old field name, then rename the field, and\n    then add a new constraint with the new field name.\n    \"\"\"\n    book = ModelState(\"otherapp\", \"Book\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n        (\"title\", models.CharField(max_length=200)),\n    ], {\n        \"unique_together\": {(\"author\", \"title\")},\n    })\n    book_with_renamed_title = ModelState(\"otherapp\", \"Book\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n        (\"new_title\", models.CharField(max_length=200)),\n    ], {\n        \"unique_together\": {(\"author\", \"new_title\")},\n    })\n    changes = self.get_changes([self.author_empty, book], [self.author_empty, book_with_renamed_title], MigrationQuestioner({\"ask_rename\": True}))\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, [\n        'AlterUniqueTogether',\n        'RenameField',\n        'AlterUniqueTogether',\n    ])\n    self.assertOperationAttributes(\n        changes, 'otherapp', 0, 0, name='book', unique_together=set(),\n    )\n    self.assertOperationAttributes(\n        changes, 'otherapp', 0, 1, model_name='book', old_name='title', new_name='new_title',\n    )\n    self.assertOperationAttributes(\n        changes, 'otherapp', 0, 2, name='book',\n        unique_together={('author', 'new_title')},\n    )\n", "def test_renamed_field_with_through_model(self):\n    \"\"\"Renaming a field used as a through model should be detected.\"\"\"\n    author = ModelState(\"otherapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"book\", models.ManyToManyField(\"otherapp.Book\", through=\"otherapp.Attribution\")),\n    ])\n    book = ModelState(\"otherapp\", \"Book\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"author\", models.ForeignKey(\"otherapp.Author\", models.CASCADE)),\n        (\"title\", models.CharField(max_length=200)),\n    ])\n    attribution = ModelState(\"otherapp\", \"Attribution\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"author\", models.ForeignKey(\"otherapp.Author\", models.CASCADE)),\n        (\"book\", models.ForeignKey(\"otherapp.Book\", models.CASCADE)),\n    ])\n    author_renamed = ModelState(\"otherapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"book\", models.ManyToManyField(\"otherapp.Book\", through=\"otherapp.Attribution\")),\n    ])\n    attribution_renamed = ModelState(\"otherapp\", \"Attribution\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"writer\", models.ForeignKey(\"otherapp.Author\", models.CASCADE)),\n        (\"book\", models.ForeignKey(\"otherapp.Book\", models.CASCADE)),\n    ])\n    book_renamed = ModelState(\"otherapp\", \"Book\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"author\", models.ForeignKey(\"otherapp.Author\", models.CASCADE)),\n        (\"title\", models.CharField(max_length=200)),\n    ])\n    changes = self.get_changes([author, book, attribution], [author_renamed, book_renamed, attribution_renamed],\n                               MigrationQuestioner({\"ask_rename\": True}))\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, ['RenameField', 'RenameField'])\n    self.assertOperationAttributes(\n        changes, 'otherapp', 0, 0, model_name='attribution', old_name='author', new_name='writer',\n    )\n    self.assertOperationAttributes(\n        changes, 'otherapp', 0, 1, model_name='book', old_name='author', new_name='author',\n    )\n", "def test_through_field_dependencies(self):\n    \"\"\"\n    When a model is removed, any field that refers to it through\n    'through' must be removed first.\n    \"\"\"\n    before = [\n        ModelState('testapp', 'Author', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('testapp', 'Publisher', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('testapp', 'Contract', [\n            ('id', models.AutoField(primary_key=True)),\n            ('author', models.ForeignKey('testapp.Author', models.CASCADE)),\n            ('publisher', models.ForeignKey('testapp.Publisher', models.CASCADE)),\n        ]),\n        ModelState('testapp', 'Book', [\n            ('id', models.AutoField(primary_key=True)),\n            ('author', models.ForeignKey('testapp.Author', models.CASCADE)),\n            ('publisher', models.ManyToManyField('testapp.Publisher', through='testapp.Contract')),\n        ]),\n    ]\n    after = [\n        ModelState('testapp', 'Author', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('testapp', 'Book', [\n            ('id', models.AutoField(primary_key=True)),\n            ('author', models.ForeignKey('testapp.Author', models.CASCADE)),\n        ]),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\n        'RemoveField', 'RemoveField', 'RemoveField', 'DeleteModel',\n    ])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='publisher', model_name='book')\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, name='author', model_name='contract')\n    self.assertOperationAttributes(changes, 'testapp', 0, 2, name='publisher', model_name='contract')\n    self.assertOperationAttributes(changes, 'testapp', 0, 3, name='Contract')\n", "def test_alter_field_multiple_bases(self):\n    \"\"\"\n    #24016 - AlterField does not create an additional field for every\n    base model.\n    \"\"\"\n    app = \"app\"\n    model = \"model\"\n    field = \"field\"\n    # Create a model with two bases.\n    before = [\n        ModelState(\n            app, model, [\n                (\"id\", models.AutoField(primary_key=True)),\n            ],\n            bases=(\"app.base1\", \"app.base2\"),\n        ),\n        ModelState(\"app\", \"base1\", []),\n        ModelState(\"app\", \"base2\", []),\n    ]\n    # Alter a field in one of the bases.\n    after = [\n        ModelState(\n            app, model, [\n                (\"id\", models.AutoField(primary_key=True)),\n            ],\n            bases=(\"app.base1\", \"app.base2\"),\n        ),\n        ModelState(\"app\", \"base1\", [\n            (field, models.CharField(max_length=255)),\n        ]),\n        ModelState(\"app\", \"base2\", []),\n    ]\n    changes = self.get_changes(before, after)\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, app, 1)\n    self.assertOperationTypes(changes, app, 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, app, 0, 0, model_name=model, name=field)\n"], "sample_659": ["def test_repr_entry_native(self):\n    entry = ReprEntryNative([\"line1\\n\", \"line2\\n\"])\n    s = str(entry)\n    assert s == \"line1\\nline2\\n\"\n", "def test_getrawcode():\n        pass\n\n    code = getrawcode(foo)\n    assert code.co_name == \"foo\"\n\n    class Bar:\n            pass\n\n    code = getrawcode(Bar().baz)\n    assert code.co_name == \"baz\"\n\n    code = getrawcode(compile(\"x = 5\", \"\", \"exec\"))\n    assert code.co_name == \"<module>\"\n\n    with pytest.raises(TypeError):\n        getrawcode(\"not a code object\")\n", "    def test_getrepr_crash(self):\n        with pytest.raises(ValueError, match=\"invalid literal for int() with base 10: 'qwe'\") as excinfo:\n            int('qwe')\n        reprcrash = excinfo._getreprcrash()\n        assert isinstance(reprcrash, ReprFileLocation)\n        assert reprcrash.lineno == 1\n        assert reprcrash.message.startswith(\"ValueError: invalid literal for int() with base 10:\")\n", "def test_getreprcrash(self):\n    excinfo = ExceptionInfo.from_current()\n    crash = excinfo._getreprcrash()\n    assert str(crash).startswith(\"ReprFileLocation(\")\n\n    # Test with a non-existent file path\n    excinfo = ExceptionInfo((Exception, Exception(\"Error\"), None))\n    crash = excinfo._getreprcrash()\n    assert str(crash).startswith(\"ReprFileLocation('???', 1, \")\n\n    # Test with a non-existent file path and no exception message\n    excinfo = ExceptionInfo((Exception, Exception(), None))\n    crash = excinfo._getreprcrash()\n    assert str(crash).startswith(\"ReprFileLocation('???', 1, \")\n", "def test_getargs_with_varargs_and_kwargs():\n        pass\n\n    code = Code(foo.__code__)\n    assert code.getargs() == ('a', 'b')\n    assert code.getargs(var=True) == ('a', 'b', 'args', 'kwargs')\n"], "sample_74": ["def test_exception_handling(self):\n    \"\"\"Test that the original SIGINT handler is restored even if an exception occurs.\"\"\"\n        raise Exception('Mocked subprocess.run exception')\n\n    sigint_handler = signal.getsignal(signal.SIGINT)\n    with mock.patch('subprocess.run', new=_mock_subprocess_run):\n        try:\n            DatabaseClient.runshell_db({})\n        except Exception:\n            pass\n    self.assertEqual(sigint_handler, signal.getsignal(signal.SIGINT))\n", "def test_sigint_handler_restoration_on_exception(self):\n    \"\"\"The original SIGINT handler is restored even if subprocess.run raises an exception.\"\"\"\n        handler = signal.getsignal(signal.SIGINT)\n        self.assertEqual(handler, signal.SIG_IGN)\n        raise subprocess.SubprocessError\n\n    sigint_handler = signal.getsignal(signal.SIGINT)\n    self.assertNotEqual(sigint_handler, signal.SIG_IGN)\n    try:\n        with mock.patch('subprocess.run', new=_mock_subprocess_run):\n            DatabaseClient.runshell_db({})\n    except subprocess.SubprocessError:\n        pass\n    self.assertEqual(sigint_handler, signal.getsignal(signal.SIGINT))\n", "def test_sigint_handler_restore(self):\n    \"\"\"Test that the original SIGINT handler is restored after subprocess.run() fails.\"\"\"\n        raise subprocess.CalledProcessError(1, args)\n    sigint_handler = signal.getsignal(signal.SIGINT)\n    with mock.patch('subprocess.run', new=_mock_subprocess_run):\n        with self.assertRaises(subprocess.CalledProcessError):\n            DatabaseClient.runshell_db({})\n    # dbshell restores the original handler.\n    self.assertEqual(sigint_handler, signal.getsignal(signal.SIGINT))\n", "    def test_empty_conn_params(self):\n        \"\"\"runshell_db with an empty connection parameters dictionary.\"\"\"\n        self.assertEqual(\n            self._run_it({}),\n            (\n                ['psql'],\n                {},\n            )\n        )\n", "def test_runshell(self):\n    \"\"\"The runshell method calls runshell_db with the connection params.\"\"\"\n    db_params = {\n        'database': 'mydb',\n        'user': 'myuser',\n        'host': 'localhost',\n        'port': '5432',\n    }\n    client = DatabaseClient()\n    client.connection = mock.Mock()\n    client.connection.get_connection_params.return_value = db_params\n\n        self.mock_db_params = params\n\n    with mock.patch.object(DatabaseClient, 'runshell_db', new=_mock_runshell_db):\n        client.runshell()\n    self.assertEqual(self.mock_db_params, db_params)\n"], "sample_1180": ["def test_orthogonal_direction():\n    p = Point(0, 1)\n    assert p.orthogonal_direction == Point(1, 0)\n    p = Point(1, 0)\n    assert p.orthogonal_direction == Point(0, 1)\n    p = Point(1, 1)\n    assert p.orthogonal_direction == Point(-1, 1)\n", "def test_dotted_line_distance():\n    p = Point(1, 1)\n    raises(TypeError, lambda: p.dot(p))\n    p1 = Point(1, 1)\n    p2 = Point(2, 2)\n    assert p1.dot(p2) == 5\n    assert p1.dot((2, 2)) == 5\n    assert p1.dot([2, 2]) == 5\n", "def test_orthogonal_direction():\n    # test that it works for higher dimensional points\n    p = Point(0, 1, 0, 0)\n    expected = Point(0, 0, 1, 0)\n    assert p.orthogonal_direction == expected\n\n    # test that it works for points with multiple zero coordinates\n    p = Point(0, 0, 1, 2)\n    expected = Point(0, 1, 0, 0)\n    assert p.orthogonal_direction == expected\n\n    # test that it works for points with no zero coordinates\n    p = Point(1, 2, 3, 4)\n    expected = Point(-2, 1, 0, 0)\n    assert p.orthogonal_direction == expected\n", "def test_issue_13191():\n    assert Point._normalize_dimension(Point(1, 2, 3), Point(4, 5)) == [\n        Point(1, 2, 3), Point(4, 5, 0)]\n    assert Point._normalize_dimension(Point(1, 2), Point(4, 5, 6), on_morph='ignore') == [\n        Point(1, 2, 0), Point(4, 5, 6)]\n    with warns(UserWarning):\n        assert Point._normalize_dimension(Point(1, 2), Point(4, 5, 6), on_morph='warn') == [\n            Point(1, 2, 0), Point(4, 5, 6)]\n    raises(ValueError, lambda: Point._normalize_dimension(\n        Point(1, 2), Point(4, 5, 6), on_morph='error'))\n    raises(ValueError, lambda: Point._normalize_dimension(\n        Point(1, 2), Point(4, 5, 6), on_morph='unknown'))\n    with warns(UserWarning):\n        assert Point._normalize_dimension(Point(1, 2, 3, 4), Point(4, 5, 6)) == [\n            Point(1, 2, 3, 4), Point(4, 5, 6, 0)]\n    with warns(UserWarning):\n        assert Point._normalize_dimension(Point(1, 2, 3, 4, 5), Point(4, 5, 6)) == [\n            Point(1, 2, 3, 4, 5), Point(4, 5, 6, 0, 0)]\n", "def test_orthogonal_direction():\n    # Test that orthogonal_direction returns a point orthogonal to\n    # the line through the point and the origin\n    p = Point3D(1, 2, 3)\n    orth_dir = p.orthogonal_direction\n    assert orth_dir.dot(p) == 0\n    # Test that a point in a higher dimension still returns an orthogonal point\n    # in that dimension\n    q = Point(1, 2, 3, 4)\n    orth_dir = q.orthogonal_direction\n    assert orth_dir.dot(q) == 0\n    # Test that a point at the origin returns a default value\n    p = Point(0, 0)\n    orth_dir = p.orthogonal_direction\n    assert orth_dir == Point(1, 0)\n    p = Point3D(0, 0, 0)\n    orth_dir = p.orthogonal_direction\n    assert orth_dir == Point3D(1, 0, 0)\n"], "sample_385": ["def test_get_select2_language(self):\n    self.assertEqual(get_select2_language(), \"en\")\n    with translation.override(\"fr\"):\n        self.assertEqual(get_select2_language(), \"fr\")\n    with translation.override(\"sr-latn\"):\n        self.assertEqual(get_select2_language(), \"sr\")\n    with translation.override(\"zh-hant-tw\"):\n        self.assertEqual(get_select2_language(), \"zh-TW\")\n", "def test_get_select2_language(self):\n    self.assertEqual(get_select2_language(), \"en\")\n\n    with translation.override(\"fr\"):\n        self.assertEqual(get_select2_language(), \"fr\")\n\n    with translation.override(\"es-ar\"):\n        self.assertEqual(get_select2_language(), \"es\")\n\n    with translation.override(\"zh-hant-tw\"):\n        self.assertEqual(get_select2_language(), \"zh-TW\")\n\n    with translation.override(\"invalid-language\"):\n        self.assertIsNone(get_select2_language())\n", "def test_optgroups(self):\n    beatles = Band.objects.create(name=\"The Beatles\", style=\"rock\")\n    who = Band.objects.create(name=\"The Who\", style=\"rock\")\n    Album.objects.create(name=\"Sgt. Pepper's Lonely Hearts Club Band\", band=beatles)\n    Album.objects.create(name=\"Tommy\", band=who)\n    rel = Album._meta.get_field(\"band\").remote_field\n    w = AutocompleteSelect(rel, admin.site)\n    groups = w.optgroups(\"name\", [beatles.pk])\n    self.assertEqual(len(groups), 1)\n    self.assertEqual(groups[0][0], None)\n    self.assertEqual(len(groups[0][1]), 2)\n    selected_option = next(\n        option for option in groups[0][1] if option[\"value\"] == str(beatles.pk)\n    )\n    self.assertEqual(selected_option[\"selected\"], True)\n", "def test_build_attrs_with_overridden_attrs(self):\n    rel = Album._meta.get_field(\"band\")\n    w = AutocompleteSelect(rel, admin.site, attrs={\"data-placeholder\": \"Select a band\"})\n    attrs = w.get_context(\"name\", None, {}).get(\"widget\", {}).get(\"attrs\")\n    self.assertEqual(attrs[\"data-placeholder\"], \"Select a band\")\n", "    def test_optgroups_no_selected_options(self):\n        \"\"\"Selected options are empty if no value is provided.\"\"\"\n        form = AlbumForm()\n        widget = form[\"band\"].field.widget\n        optgroups = widget.optgroups(\"name\", None)\n        self.assertEqual(len(optgroups), 1)\n        self.assertEqual(len(optgroups[0][1]), 1 if not widget.is_required else 0)\n"], "sample_631": ["    def test_custom_dummy_variable_regex(self):\n        \"\"\"Test the --dummy-variables-rgx option works.\"\"\"\n        node = astroid.parse(\n            \"\"\"\n            pass\n        \"\"\"\n        )\n        with self.assertNoMessages():\n            self.walk(node)\n\n        node = astroid.parse(\n            \"\"\"\n            pass\n        \"\"\"\n        )\n        with self.assertAddsMessages(\n            Message(\"unused-argument\", node=node[\"x\"], args=\"x\")\n        ):\n            self.walk(node)\n", "    def test_defined_before_assignment_in_nested_scopes(self):\n        \"\"\"Make sure that we can correctly detect when a variable is defined before it is assigned\n        in nested scopes.\"\"\"\n        node = astroid.parse(\n            \"\"\"\n            x = 1\n                print(x)\n                x = 2\n            inner()\n            print(x)\n        \"\"\"\n        )\n        with self.assertNoMessages():\n            self.walk(node)\n", "def test_type_annotations_with_class_attributes(self):\n    \"\"\"Test that class attributes are recognized as used when used in type annotations\"\"\"\n    module = astroid.parse(\n        \"\"\"\n        class MyClass:\n            attr: 'MyClass'\n                pass\n        \"\"\"\n    )\n    with self.assertNoMessages():\n        self.walk(module)\n", "def test_late_binding_closure(self):\n    \"\"\"Check that self.late_binding_closure handles lambda correctly\"\"\"\n    node = astroid.parse(\n        \"\"\"\n        a = 1\n            b = 2\n            inner = lambda: b + 1\n            return inner\n        outer()\n    \"\"\"\n    )\n    with self.assertAddsMessages(\n        Message(\"cell-var-from-loop\", node=node.body[0].body[1], args=\"b\")\n    ):\n        self.walk(node)\n", "    def test_cell_var_from_loop_function(self):\n        \"\"\"Ensure a variable defined in a loop is correctly detected\n        when it's used in the closure of a function inside the loop.\n\n        https://github.com/PyCQA/pylint/issues/3463\n        \"\"\"\n        node = astroid.parse(\n            \"\"\"\n            for i in range(10):\n                if True:\n                    outer_var = i\n                    return outer_var\n            return inner\n\n        outer()\n        \"\"\"\n        )\n        with self.assertAddsMessages(\n            Message(\"cell-var-from-loop\", node=node.body[0].body[0].body[1].body[0], args=\"outer_var\")\n        ):\n            self.walk(node)\n"], "sample_919": ["def test_pointer_to_member_functions():\n    check('function', 'int C::* f(int, double)',\n          {2: '1fM1CFidE'})\n    check('function', 'int (C::* f)(int, double)',\n          {2: '1fM1CFidE'})\n    check('function', 'int C::*f(int, double)',\n          {2: '1fM1CFidE'})\n    check('function', 'int (C::*f)(int, double)',\n          {2: '1fM1CFidE'})\n\n    check('function', 'void f(int C::*f())',\n          {2: '1fM1CFivE'})\n    check('function', 'void f(int (C::*f)())',\n          {2: '1fM1CFivE'})\n    check('function', 'void f(int C::*f)()',\n          {2: '1fM1CFivE'})\n    check('function', 'void f(int (C::*f))()',\n          {2: '1fM1CFivE'})\n\n    check('function', 'void f(int C::*const f())',\n          {2: '1fKM1CFivE'})\n    check('function', 'void f(int (C::*const f)())',\n          {2: '1fKM1CFivE'})\n    check('function', 'void f(int C::*const f)()',\n          {2: '1fKM1CFivE'})\n    check('function', 'void f(int (C::*const f))()',\n          {2: '1fKM1CFivE'})\n\n    check('function', 'void f(int C::*volatile f())',\n          {2: '1fVM1CFivE'})\n    check('function', 'void f(int (C::*volatile f)())',\n          {2: '1fVM1CFivE'})\n    check('function', 'void f(int C::*volatile f)()',\n          {2: '1fVM1CFivE'})\n    check('function', 'void f(int (C::*volatile f))()',\n          {2: '1fVM1CFivE'})\n\n    check('function', 'void f(int C::*const volatile f())',\n          {2: '1fVKM1CFivE'})\n    check('function', 'void f(int (C::*const", "def test_operator_definitions():\n    check('function', 'void operator+(int a)', {1: \"add-operator\", 2: \"plv\"})\n    check('function', 'void operator*(int a)', {1: \"mul-operator\", 2: \"mlv\"})\n    check('function', 'void operator+(int a, int b)', {1: \"add-operator\", 2: \"plv\"})\n    check('function', 'void operator+(const int a, int b)', {1: \"add-operator\", 2: \"plv\"})\n    check('function', 'void operator+(int a, const int b)', {1: \"add-operator\", 2: \"plv\"})\n    check('function', 'void operator+(const int a, const int b)', {1: \"add-operator\", 2: \"plv\"})\n    check('function', 'void operator+(int a, int b, int c)', {1: None, 2: None})\n    check('function', 'void operator+(int a)', {1: \"add-operator\", 2: \"plv\"},\n          output='void operator+(int a)')\n    check('function', 'void operator+(volatile int a)', {1: \"add-operator\", 2: \"plv\"},\n          output='void operator+(volatile int a)')\n    check('function', 'void operator+(const int a)', {1: \"add-operator\", 2: \"plv\"},\n          output='void operator+(const int a)')\n    check('function', 'void operator+(const volatile int a)', {1: \"add-operator\", 2: \"plv\"},\n          output='void operator+(const volatile int a)')\n    check('function', 'void operator+(const volatile int a, int b)', {1: \"add-operator\", 2: \"plv\"},\n          output='void operator+(const volatile int a, int b)')\n    check('function', 'void operator+(int a, const volatile int b)', {1: \"add-operator\", 2: \"plv\"},\n          output='void operator+(int a, const volatile int b)')\n    check('function', 'void operator+(const volatile int a, const volatile int b)', {1: \"add-operator\", 2: \"plv\"},\n          output='void operator+(const volatile int a, const volatile int b)')\n\n    check('function', 'A::operator+(int a)', {1:", "def test_template_parameter_lists():\n    check('class', \"template<> {key}A\", {2: \"IE1A\"}, output='template<> {key}A')\n    check('class', \"template<>{key}A\", {2: \"IE1A\"}, output='template<>{key}A')\n    check('class', \"template<>{key}A<T>\", {2: \"IE1AI1TE\"}, output='template<> {key}A<T>')\n    check('class', \"template<>{key}A<1>\", {2: \"IE_iE1A\"}, output='template<> {key}A<1>')\n    check('class', \"template<>{key}A<1>\", {2: \"IE_iE1A\"}, output='template<> {key}A<1>')\n    check('class', \"template<>{key}A<1>\", {2: \"IE_iE1A\"}, output='template<> {key}A<1>')\n    check('class', \"template<>{key}A<int>\", {2: \"IE1AIiE\"}, output='template<> {key}A<int>')\n    check('class', \"template<>{key}A<int&>\", {2: \"IE1ARiE\"}, output='template<> {key}A<int&>')\n    check('class', \"template<>{key}A<int*>\", {2: \"IE1APiE\"}, output='template<> {key}A<int*>')\n    check('class', \"template<>{key}A<int const>\", {2: \"IE1AiE\"}, output='template<> {key}A<int const>')\n    check('class', \"template<>{key}A<int volatile>\", {2: \"IE1AViE\"}, output='template<> {key}A<int volatile>')\n    check('class', \"template<>{key}A<int const volatile>\", {2: \"IE1AVKiE\"}, output='template<> {key}A<int const volatile>')\n    check('class', \"template<>{key}A<int volatile const>\", {2: \"IE1AVKiE\"}, output='template<> {key}A<int volatile const>')\n    check('class', \"template<>{key}A<42>\", {2: \"IE_iE1A\"}, output='template<> {key}A<42>')\n    check('class', \"template<>{key}A", "def test_template_args():\n    # TODO: this is probably already covered by the attributes tests,\n    # but this tests that we don't add parens on the types\n    check('type', \"template<typename T> {key}A<T>::B<T>\", {2: \"I0E1AN1A1B1TE\"})\n    check('type', \"template<typename T> {key}A<T>::B<T, T>\", {2: \"I0E1AN1A1B2T1TE\"})\n    check('type', \"template<typename T> {key}A<T>::B<T, T, T>\", {2: \"I0E1AN1A1B3T1TE\"})\n    check('type', \"template<typename T> {key}A<T>::B<3>\", {2: \"I0E1AN1A1B1iE\"})\n    check('type', \"template<typename T> {key}A<T>::B<T, 42>\", {2: \"I0E1AN1A1B1T1iE\"})\n    check('type', \"template<typename T> {key}A<T>::B<T, 3>\", {2: \"I0E1AN1A1B1T1iE\"})\n    check('type', \"template<typename T> {key}A<T>::B<42, T>\", {2: \"I0E1AN1A1B1i1TE\"})\n    check('type', \"template<typename T> {key}A<T>::B<3, T>\", {2: \"I0E1AN1A1B1i1TE\"})\n    check('type', \"template<typename T> {key}A<T>::B<T, T, 42>\", {2: \"I0E1AN1A1B2T1iE\"})\n    check('type', \"template<typename T> {key}A<T>::B<T, T, 3>\", {2: \"I0E1AN1A1B2T1iE\"})\n    check('type', \"template<typename T> {key}A<T>::B<42, T, T>\", {2: \"I0E1AN1A1B1i2T1TE\"})\n    check('type', \"template<typename T> {key}A<T>::B<3, T, T>\", {2: \"I0", "def test_template_concepts():\n    # from #1549\n    check('class', 'template<template<typename T> class C> {key}A', {2: 'I0E1A'})\n    check('class', 'template<template<typename T> typename C> {key}A', {2: 'I0E1A'})\n    check('class', 'template<template<typename T> class C> {key}A<int>', {2: 'I0E1A'})\n    check('class', 'template<template<typename T> typename C> {key}A<int>', {2: 'I0E1A'})\n\n    check('function', 'template<template<typename T> class C> void A()',\n          {2: 'I0E1Av'})\n    check('function', 'template<template<typename T> typename C> void A()',\n          {2: 'I0E1Av'})\n    check('function', 'template<template<typename T> class C> void A()',\n          {2: 'I0E1Av'})\n    check('function', 'template<template<typename T> typename C> void A()',\n          {2: 'I0E1Av'})\n\n    # from #3451\n    check('type', \"template<template<int> class C> {key}MyTemplate<C<1> >\",\n          {2: \"I0E15MyTemplateI0I1EE\"})\n    check('type', \"template<template<int> typename C> {key}MyTemplate<C<1> >\",\n          {2: \"I0E15MyTemplateI0I1EE\"})\n\n    # from #3882\n    check('type', 'template<template<typename T> class C, template<typename> class D> '\n          '{key}MyTemplate<C<int>, D<char> >',\n          {2: 'I00E15MyTemplateI0IiEEI1IcEE'})\n\n    # from #4438\n    check('class', 'template<template<typename> class C> {key}A',\n          {2: 'I0E1A'})\n    check('class', 'template<template<> class C> {key}A',\n          {2: 'I0E1A'})\n\n    check('function', 'template<template<typename> class C> void f()',\n          {2: 'I0E1Av'})\n    check"], "sample_967": ["def test_mathjax_custom_delimiters(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    html = (r'<span class=\"math notranslate nohighlight\">'\n            r'<inline>a\\^2\\+b\\^2=c\\^2</inline></span>')\n    assert re.search(html, content, re.S)\n\n    html = (r'<div class=\"math notranslate nohighlight\">'\n            r'<display>a\\^2\\+b\\^2=c\\^2</display></div>')\n    assert re.search(html, content, re.S)\n", "def test_mathjax_inline_display_config(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    html = (r'<span class=\"math notranslate nohighlight\">\\[a\\^2\\+b\\^2=c\\^2\\]</span>')\n    assert re.search(html, content, re.S)\n    html = (r'<div class=\"math notranslate nohighlight\">\\(\\[a\\^2\\+b\\^2=c\\^2\\]\\)')\n    assert re.search(html, content, re.S)\n", "def test_custom_mathjax_inline_display(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    html = (r'<span class=\"math notranslate nohighlight\">$$a\\^2\\+b\\^2=c\\^2$$</span>')\n    assert html in content\n\n    html = (r'<div class=\"math notranslate nohighlight\">\\begin{equation}a\\^2\\+b\\^2=c\\^2\\end{equation}</div>')\n    assert html in content\n", "def test_mathjax_custom_delimiters(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    html = (r'<span class=\"math notranslate nohighlight\">\\$\\$\\s*'\n            r'E=mc\\^2\\s*\\$\\$</span>')\n    assert re.search(html, content, re.S)\n\n    html = (r'<div class=\"math notranslate nohighlight\">\\s*'\n            r'\\$\\$\\s*\\\\\\[ \\\\begin\\{align\\}\\\\begin\\{aligned\\}S \\&amp;= \\\\pi r\\^2\\\\\\\\'\n            r'V \\&amp;= \\\\frac\\{4\\}\\{3\\} \\\\pi r\\^3\\\\end\\{aligned\\}\\\\end\\{align\\} \\\\\\]\\s*\\$\\$</div>')\n    assert re.search(html, content, re.S)\n", "def test_mathjax_inline_delimiters(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    html = (r'<span class=\"math notranslate nohighlight\">$$E=mc^2$$</span>')\n    assert html in content\n\n"], "sample_318": ["    def test_language_prefix(self):\n        \"\"\"\n        The language prefix is not added to the URL when it matches the default\n        language and prefix_default_language is False.\n        \"\"\"\n        resolver = get_resolver('urlpatterns_reverse.locale_urls')\n        self.assertEqual(resolver.reverse('test', language_code='en'), '/test/')\n", "    def test_regex_pattern_compile(self):\n        pattern = RegexPattern(r'^/test/(\\d+)/')\n        self.assertEqual(pattern.regex.pattern, '^/test/(?P<1>\\\\d+)/')\n", "    def test_url_pattern_repr(self):\n        url_pattern = URLPattern(\n            RegexPattern('test/'),\n            views.empty_view,\n            name='test-pattern',\n        )\n        self.assertEqual(repr(url_pattern), '<URLPattern test/>')\n", "    def test_regex_pattern(self):\n        test_urls = [\n            ('^normal$', '/normal/', '', {}, views.empty_view),\n            ('^normal/$', '/normal/', '', {}, views.empty_view),\n            ('^normal$/', '/normal/', '', {}, views.empty_view),\n            ('^normal/$', '/normal/', '', {}, views.empty_view),\n            ('^normal/(?P<arg1>[0-9]+)/$', '/normal/42/', '42', {}, views.empty_view),\n            ('^normal/(?P<arg1>[0-9]+)/$', '/normal/42/', '', {'arg1': '42'}, views.empty_view),\n            ('^normal/(?P<arg1>[0-9]+)/(?P<arg2>[0-9]+)/$', '/normal/42/37/', ('42', '37'), {}, views.empty_view),\n            ('^normal/(?P<arg1>[0-9]+)/(?P<arg2>[0-9]+)/$', '/normal/42/37/', '', {'arg1': '42', 'arg2': '37'}, views.empty_view),\n        ]\n        for pattern, path, args, kwargs, view in test_urls:\n            with self.subTest(pattern=pattern):\n                resolver = URLResolver(RegexPattern(pattern), 'urlpatterns_reverse.regex_pattern_tests')\n                match = resolver.resolve(path)\n                self.assertEqual(match.func, view)\n                self.assertEqual(match.args, args)\n                self.assertEqual(match.kwargs, kwargs)\n", "    def test_empty_string(self):\n        pattern = RegexPattern('', 'test_name')\n        self.assertEqual(pattern.describe(), \"'test_name'\")\n"], "sample_555": ["def test_default_hatch_color():\n    patch = Patch()\n    assert patch.get_hatch_color() == mcolors.to_rgba(mpl.rcParams['hatch.color'])\n", "def test_annulus_getters():\n    a = Annulus((1, 1), 1, 0.5)\n    assert a.center == (1, 1)\n    assert a.width == 0.5\n    assert a.angle == 0\n    assert a.a == 1\n    assert a.b == 1\n    assert a.radii == (1, 1)\n", "def test_ellipse_aspect_ratio_correction():\n    ellipse = Ellipse((0, 0), 1, 2)\n\n    assert ellipse._aspect_ratio_correction == 1.0\n\n    ellipse = Ellipse((0, 0), 1, 2)\n    ellipse.set_aspect('equal')\n    assert ellipse._aspect_ratio_correction == 2.0\n\n    ellipse = Ellipse((0, 0), 2, 1)\n    ellipse.set_aspect('equal')\n    assert ellipse._aspect_ratio_correction == 0.5\n", "def test_arc_setters():\n    a = Arc((1, 1), 2, 2, theta1=0, theta2=90, angle=45)\n\n    a.set_center((0, 0))\n    a.set_width(3)\n    a.set_height(4)\n    a.set_angle(30)\n    a.set_theta1(45)\n    a.set_theta2(135)\n\n    a2 = Arc((0, 0), 3, 4, theta1=45, theta2=135, angle=30)\n\n    assert a.get_path().extents == a2.get_path().extents\n", "def test_arc_style():\n    # Test that different styles are rendered correctly for arcs\n    fig, ax = plt.subplots()\n    ax.set_aspect('equal')\n    arc1 = Arc((0.5, 0.5), 0.5, 1, theta1=0, theta2=60, angle=20, \n               fill=True, edgecolor='none', facecolor='C0')\n    arc2 = Arc((0.5, 0.5), 1.5, 1, theta1=0, theta2=60, angle=10, \n               fill=True, edgecolor='C1', facecolor='none')\n    arc3 = Arc((0.5, 0.5), 2.5, 1, theta1=0, theta2=60, angle=0, \n               fill=True, edgecolor='C2', facecolor='C2', hatch='//')\n    ax.add_patch(arc1)\n    ax.add_patch(arc2)\n    ax.add_patch(arc3)\n    ax.set_xlim(0, 3)\n    ax.set_ylim(0, 3)\n"], "sample_975": ["def test_nsolve_unevaluated_expression():\n    x = Symbol('x')\n    assert nsolve(Pow(1/x, -1, evaluate=False), x, 2) == 0.5\n", "def test_nsolve_nonzero_denominator():\n    x = Symbol('x')\n    # Test that nsolve uses the full expression (numerator and denominator).\n    # Test with a non-zero denominator.\n    ans = nsolve((x**2 + 3*x + 2)/(x + 3), -2.1)\n    # The root -3 was divided out, so make sure we don't find it.\n    assert ans == -1.0\n", "def test_nsolve_high_dimensional():\n    x, y, z = symbols('x y z')\n    f1 = x**2 - y\n    f2 = x - y*z\n    f3 = y - z\n    f = Matrix((f1, f2, f3)).T\n    F = lambdify((x, y, z), f.T, modules='mpmath')\n\n        root = nsolve(f, (x, y, z), x0)\n        assert mnorm(F(*root), 1) <= 1.e-8\n        return root\n    assert list(map(round, getroot((1, 1, 1)))) == [1.0, 1.0, 1.0]\n    assert nsolve([Eq(f1), Eq(f2), Eq(f3)], [x, y, z], (1, 1, 1))  # just see that it works\n", "def test_nsolve_matrix_with_prec():\n    import mpmath\n    mpmath.mp.dps = 15\n    x, y = symbols('x y')\n    f1 = 3 * x**2 - 2 * y**2 - 1\n    f2 = x**2 - 2 * x + y**2 + 2 * y - 8\n    f = Matrix((f1, f2)).T\n    sol = nsolve(f, (x, y), (-1, 1), prec=20)\n    assert mpmath.mp.dps == 20\n    assert sol.shape == (2, 1)\n    assert isinstance(sol, Matrix)\n    assert all(isinstance(i, Float) for i in sol)\n", "def test_unrad():\n    from sympy.solvers import unrad\n    x = Symbol('x')\n    y = Symbol('y')\n    eq = sqrt(x) - 2*sqrt(y) - 3\n    assert unrad(eq, x, y) == (_p**2 - 2*_p*sqrt(y) + y - 9,\n        [_p, _p**2 - x]) \n\n    # from issue 2461\n    eq = sqrt(x) + y - 5\n    assert unrad(eq) == (_p - 5, [_p, _p**2 - x])\n"], "sample_194": ["    def test_abstract_constraint_name_with_condition(self):\n        constraints = get_constraints(UniqueConstraintConditionProduct._meta.db_table)\n        self.assertIn('name_uniq', constraints)\n", "    def test_clone(self):\n        constraint = models.UniqueConstraint(\n            fields=['name'],\n            name='name_unique',\n            deferrable=models.Deferrable.IMMEDIATE,\n        )\n        cloned_constraint = constraint.clone()\n        self.assertEqual(constraint, cloned_constraint)\n        self.assertIsNot(constraint, cloned_constraint)\n", "    def test_include_fields_as_tuple(self):\n        fields = ['name', 'color']\n        name = 'include_fields'\n        include = ('baz_1', 'baz_2')\n        constraint = models.UniqueConstraint(fields=fields, name=name, include=include)\n        path, args, kwargs = constraint.deconstruct()\n        self.assertEqual(path, 'django.db.models.UniqueConstraint')\n        self.assertEqual(args, ())\n        self.assertEqual(kwargs, {\n            'fields': tuple(fields),\n            'name': name,\n            'include': include,\n        })\n", "    def test_empty_fields(self):\n        message = 'At least one field is required to define a unique constraint.'\n        with self.assertRaisesMessage(ValueError, message):\n            models.UniqueConstraint(name='uniq', fields=[])\n", "    def test_check_constraint_multiple_expressions(self):\n        class Product(models.Model):\n            name = models.CharField(max_length=255)\n            price = models.DecimalField(max_digits=5, decimal_places=2)\n            discounted_price = models.DecimalField(max_digits=5, decimal_places=2)\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(\n                        check=(models.Q(price__gt=models.F('discounted_price')) | models.Q(discounted_price__isnull=True)),\n                        name='price_discounted_price',\n                    ),\n                ]\n\n        Product.objects.create(name='Product 1', price=10, discounted_price=5)\n        Product.objects.create(name='Product 2', price=10, discounted_price=None)\n        with self.assertRaises(IntegrityError):\n            Product.objects.create(name='Product 3', price=10, discounted_price=15)\n"], "sample_236": ["def test_collector_add_dependency(self):\n    collector = Collector(using='default')\n    model1 = R\n    model2 = S\n    collector.add_dependency(model1, model2)\n    self.assertIn(model2._meta.concrete_model, collector.dependencies[model1._meta.concrete_model])\n    self.assertIn(model2, collector.data)\n", "def test_cannot_fast_delete_due_to_signal_listeners(self):\n        pass\n\n    models.signals.pre_delete.connect(listener, sender=A)\n\n    a = A.objects.create()\n    collector = Collector(using='default')\n    self.assertFalse(collector.can_fast_delete(a))\n\n    models.signals.pre_delete.disconnect(listener, sender=A)\n", "def test_collector_clear_restricted_objects_from_queryset(self):\n    r = R.objects.create()\n    a1 = A.objects.create(restrict=r)\n    a2 = A.objects.create(restrict=r)\n    collector = Collector(using='default')\n    collector.collect([a1, a2], source=None, nullable=False, collect_related=True,\n                     source_attr=None, reverse_dependency=False, keep_parents=False,\n                     fail_on_restricted=False)\n    self.assertEqual(len(collector.restricted_objects[R]), 1)\n    qs = A.objects.filter(name__in=[a1.name, a2.name])\n    collector.clear_restricted_objects_from_queryset(R, qs)\n    self.assertEqual(len(collector.restricted_objects[R]), 0)\n", "    def test_can_fast_delete_instance_with_reverse_one_to_one(self):\n        a = A.objects.create()\n        r = R.objects.create(o2o_setnull=a)\n        a.o2o_setnull = r\n        a.save()\n        collector = Collector(using='default')\n        self.assertTrue(collector.can_fast_delete(r))\n        self.assertTrue(collector.can_fast_delete(a))\n", "    def test_can_fast_delete_checks_reverse_one_to_one_relationships(self):\n        \"\"\"\n        Ensure that can_fast_delete() checks reverse one-to-one relationships.\n        \"\"\"\n        child = Child.objects.create()\n        parent = Parent.objects.get(id=child.id)\n        with self.assertNumQueries(2):\n            parent.delete()\n        self.assertFalse(Child.objects.exists())\n        self.assertFalse(Parent.objects.exists())\n"], "sample_443": ["    def setUp(self):\n        super().setUp()\n        self.dirname = settings.CACHES[\"default\"][\"LOCATION\"]\n", "    def test_incr_decr_version_zero_value(self):\n        \"\"\"incr_version() and decr_version() handle zero version correctly\"\"\"\n        cache.set(\"answer\", 42, version=0)\n        self.assertEqual(cache.get(\"answer\", version=0), 42)\n        self.assertEqual(cache.incr_version(\"answer\", version=0), 1)\n        self.assertEqual(cache.get(\"answer\", version=1), 42)\n        self.assertIsNone(cache.get(\"answer\", version=0))\n        self.assertEqual(cache.decr_version(\"answer\", version=1), 0)\n        self.assertIsNone(cache.get(\"answer\", version=1))\n        self.assertEqual(cache.get(\"answer\", version=0), 42)\n", "    def setUp(self):\n        cache.clear()\n", "    def setUp(self):\n        super().setUp()\n        self.dirname = self.mkdtemp()\n        # Caches location cannot be modified through override_settings /\n        # modify_settings, hence settings are manipulated directly here and the\n        # setting_changed signal is triggered manually.\n        for cache_params in settings.CACHES.values():\n            cache_params[\"LOCATION\"] = self.dirname\n        setting_changed.send(self.__class__, setting=\"CACHES\", enter=False)\n", "    def setUp(self):\n        super().setUp()\n        self.dirname = self.mkdtemp()\n        # Caches location cannot be modified through override_settings /\n        # modify_settings, hence settings are manipulated directly here and the\n        # setting_changed signal is triggered manually.\n        for cache_params in settings.CACHES.values():\n            cache_params[\"LOCATION\"] = self.dirname\n        setting_changed.send(self.__class__, setting=\"CACHES\", enter=False)\n"], "sample_212": ["def test_session_middleware(self):\n    request = HttpRequest()\n    response = HttpResponse()\n    session_middleware = SessionMiddleware(get_response=lambda r: response)\n\n    # Test process_request\n    session_middleware.process_request(request)\n    self.assertIsNotNone(request.session)\n\n    # Test process_response\n    response = session_middleware.process_response(request, response)\n    self.assertEqual(response.status_code, 200)\n\n    # Test process_response with modified session\n    request.session['key'] = 'value'\n    response = session_middleware.process_response(request, response)\n    self.assertEqual(response.cookies[settings.SESSION_COOKIE_NAME], request.session.session_key)\n\n    # Test process_response with expired session\n    request.session.set_expiry(0)\n    response = session_middleware.process_response(request, response)\n    self.assertNotIn(settings.SESSION_COOKIE_NAME, response.cookies)\n\n    # Test process_response with deleted session\n    request.session.delete()\n    response = session_middleware.process_response(request, response)\n    self.assertNotIn(settings.SESSION_COOKIE_NAME, response.cookies)\n\n    # Test process_response with UpdateError\n    class MockSession:\n            raise UpdateError\n    request.session = MockSession()\n    with self.assertRaises(SuspiciousOperation):\n        session_middleware.process_response(request, response)\n", "def test_session_middleware_process_request(self):\n    request = HttpRequest()\n    middleware = SessionMiddleware()\n\n    # Test with no session key in cookies\n    request.COOKIES = {}\n    middleware.process_request(request)\n    self.assertIsNotNone(request.session)\n\n    # Test with a session key in cookies\n    request.COOKIES = {settings.SESSION_COOKIE_NAME: 'session_key'}\n    middleware.process_request(request)\n    self.assertIsNotNone(request.session)\n", "    def test_process_response_empty_session(self):\n        middleware = SessionMiddleware()\n        request = HttpRequest()\n        request.session = middleware.SessionStore('session_key')\n        request.session.clear()\n\n        response = HttpResponse()\n        response = middleware.process_response(request, response)\n\n        self.assertNotIn(settings.SESSION_COOKIE_NAME, response.cookies)\n        self.assertEqual(response['Vary'], 'Cookie')\n", "def test_session_middleware_process_response(self):\n    # Create a request and response\n    request = HttpRequest()\n    response = HttpResponse()\n\n    # Create a SessionMiddleware instance\n    session_middleware = SessionMiddleware()\n\n    # Simulate a session being set\n    session_key = 'test_session_key'\n    request.session = session_middleware.SessionStore(session_key)\n\n    # Test that the session cookie is set in the response\n    session_middleware.process_response(request, response)\n    self.assertIn(settings.SESSION_COOKIE_NAME, response.cookies)\n\n    # Test that the session cookie is deleted when the session is empty\n    request.session.delete()\n    session_middleware.process_response(request, response)\n    self.assertNotIn(settings.SESSION_COOKIE_NAME, response.cookies)\n\n    # Test that an UpdateError is raised if the session is modified after it's been saved\n    request.session['test'] = 'value'\n    session_middleware.process_response(request, response)\n    request.session.modified = True\n    with self.assertRaises(SuspiciousOperation):\n        session_middleware.process_response(request, response)\n\n    # Test that the session cookie is not saved if the response is a 500\n    request.session['test'] = 'value'\n    response.status_code = 500\n    session_middleware.process_response(request, response)\n    self.assertNotIn(settings.SESSION_COOKIE_NAME, response.cookies)\n", "def test_session_middleware_save_session(self):\n    session_key = 'test_session_key'\n    request = HttpRequest()\n    request.COOKIES[settings.SESSION_COOKIE_NAME] = session_key\n    request.session = self.SessionStore(session_key)\n\n    middleware = SessionMiddleware()\n\n    # Test saving the session\n    request.session['test_key'] = 'test_value'\n    response = middleware.process_response(request, HttpResponse())\n    self.assertEqual(response.cookies[settings.SESSION_COOKIE_NAME].value, session_key)\n\n    # Test deleting the session cookie\n    request.session.clear()\n    response = middleware.process_response(request, HttpResponse())\n    self.assertNotIn(settings.SESSION_COOKIE_NAME, response.cookies)\n\n    # Test that the session is not saved on a 500 response\n    request.session['test_key'] = 'test_value'\n    response = HttpResponse(status_code=500)\n    middleware.process_response(request, response)\n    self.assertNotIn(settings.SESSION_COOKIE_NAME, response.cookies)\n\n    # Test saving the session with expire at browser close\n    request.session.set_expiry(0)\n    request.session['test_key'] = 'test_value'\n    response = middleware.process_response(request, HttpResponse())\n    self.assertEqual(response.cookies[settings.SESSION_COOKIE_NAME]['max-age'], '0')\n\n    # Test that an UpdateError raises a SuspiciousOperation\n    class MockSessionStore:\n            raise UpdateError\n\n    middleware.SessionStore = MockSessionStore\n    request.session = MockSessionStore(session_key)\n    request.session['test_key'] = 'test_value'\n    with self.assertRaises(SuspiciousOperation):\n        middleware.process_response(request, HttpResponse())\n"], "sample_297": ["    def setUpTestData(cls):\n        cls.a1 = Author.objects.create(name='a1', num=1001)\n        cls.a2 = Author.objects.create(name='a2', num=2002)\n        cls.a3 = Author.objects.create(name='a3', num=3003)\n        cls.e1 = ExtraInfo.objects.create(info='e1', note=Note.objects.create(note='n1', misc='foo'))\n        cls.e2 = ExtraInfo.objects.create(info='e2', note=Note.objects.create(note='n2', misc='bar'))\n        cls.a1.extra = cls.e1\n        cls.a2.extra = cls.e1\n        cls.a3.extra = cls.e2\n        cls.a1.save()\n        cls.a2.save()\n        cls.a3.save()\n", "    def test_ticket_25533(self):\n        a1 = Author.objects.create(name='a1', num=1001)\n        a2 = Author.objects.create(name='a2', num=2002)\n        a3 = Author.objects.create(name='a3', num=3003)\n        r1 = Report.objects.create(name='r1', creator=a1)\n        r2 = Report.objects.create(name='r2', creator=a3)\n        r3 = Report.objects.create(name='r3')\n\n        # Each filter call is processed \"at once\" against a single table, so this is\n        # different from the previous example as it tries to find tags that are two\n        # things at once (rather than two tags).\n        self.assertSequenceEqual(\n            Report.objects.filter(Q(creator__num__gte=2002, name='r1')),\n            []\n        )\n        self.assertSequenceEqual(\n            Report.objects.filter(Q(creator__num__gte=2002) & Q(name='r1')),\n            []\n        )\n\n        qs = Author.objects.filter(ranking__rank=2, ranking__id=1)\n        self.assertSequenceEqual(list(qs), [])\n        self.assertEqual(2, qs.query.count_active_tables(), 2)\n        qs = Author.objects.filter(ranking__rank=2).filter(ranking__id=1)\n        self.assertEqual(qs.query.count_active_tables(), 3)\n", "    def test_ticket_21221(self):\n        \"\"\"\n        A test for ticket 21221. \n        This test checks that the QuerySet.order_by() method does not \n        crash when given a field that is not present in the model.\n        \"\"\"\n        try:\n            Item.objects.order_by('non_existent_field')\n        except FieldError as e:\n            self.assertEqual(str(e), \"Cannot resolve keyword 'non_existent_field' into field. \"\n                                     \"Choices are: creator, created, id, modified, name, note, tags\")\n", "    def test_ticket_24915(self):\n        # Make sure isnull=False works correctly on empty strings.\n        note = Note.objects.create(note='')\n        self.assertSequenceEqual(Note.objects.filter(note__isnull=False), [note])\n        self.assertSequenceEqual(Note.objects.filter(note__isnull=True), [])\n\n        # If isnull=False is used in an exclude clause, it should not exclude\n        # empty strings.\n        self.assertSequenceEqual(\n            Note.objects.exclude(note__isnull=False),\n            []\n        )\n\n        self.assertSequenceEqual(\n            Note.objects.exclude(note__isnull=True),\n            [note]\n        )\n", "    def test_ticket_25314(self):\n        # Test that model with custom Meta class still works with default ordering.\n        class CustomMeta(type):\n                class Meta:\n                    ordering = ['name']\n                dct['Meta'] = Meta\n                return super().__new__(cls, name, bases, dct)\n        class MyModel(metaclass=CustomMeta):\n            name = models.CharField(max_length=255)\n\n        with self.settings(INSTALLED_APPS=['tests.test_models', 'tests.test_models.mymodel']):\n            from .mymodel.models import MyModel\n            MyModel.objects.create(name='Alice')\n            MyModel.objects.create(name='Bob')\n            self.assertEqual(list(MyModel.objects.values_list('name', flat=True)), ['Alice', 'Bob'])\n"], "sample_156": ["def test_default_renderer_class_gets_instantiated(self):\n    class CustomRendererClass:\n            self.instantiated = True\n\n    class CustomForm(Form):\n        default_renderer = CustomRendererClass\n\n    form = CustomForm()\n    self.assertTrue(form.renderer.instantiated)\n", "def test_field_order_with_duplicates(self):\n    class TestForm(Form):\n        field1 = CharField()\n        field2 = CharField()\n        field3 = CharField()\n        field_order = ['field3', 'field2', 'field1', 'field1']\n\n    with self.assertRaisesMessage(ValueError, 'field_order must not contain duplicates'):\n        TestForm()\n", "def test_cleaned_data_after_add_error(self):\n    class UserForm(Form):\n        username = CharField(max_length=10)\n        password = CharField(widget=PasswordInput)\n\n            self.add_error('username', 'Invalid username')\n            return self.cleaned_data\n\n    f = UserForm({'username': 'sirrobin', 'password': 'blue'})\n    self.assertTrue(f.is_valid())\n    self.assertEqual(f.cleaned_data['username'], 'sirrobin')\n\n    f = UserForm({'username': 'sirrobin', 'password': 'blue'})\n    f.is_valid()\n    self.assertEqual(f.errors, {'username': ['Invalid username']})\n    self.assertEqual(f.cleaned_data, {'password': 'blue'})\n", "def test_empty_string_choice(self):\n    class MyForm(Form):\n        my_choice = ChoiceField(choices=[('a', 'a'), ('', 'b'), ('', 'c')])\n\n    form = MyForm()\n    self.assertHTMLEqual(\n        form.as_p(),\n        \"\"\"<p><label for=\"id_my_choice\">My choice:</label> ", "    def test_form_mixin(self):\n        class RequiredFormMixin:\n                super().__init__(*args, **kwargs)\n                for field_name, field in self.fields.items():\n                    field.required = True\n\n        class TestForm(Form, RequiredFormMixin):\n            pass\n\n        class TestFormWithFields(Form, RequiredFormMixin):\n            field1 = CharField()\n            field2 = CharField(required=False)\n\n        form = TestForm({'field1': 'value'})\n        self.assertEqual(form.errors, {'field1': ['This field cannot be blank.']})\n\n        form = TestFormWithFields({'field1': 'value'})\n        self.assertEqual(form.errors, {'field2': ['This field cannot be blank.']})\n"], "sample_452": ["def test_alter_field_mti_and_fk_to_base_with_swappable_model(self):\n    app_label = \"test_alflpkmtiftbswm\"\n    project_state = self.set_up_test_model(\n        app_label,\n        mti_model=True,\n        related_model=True,\n        swappable_model=True,\n    )\n    operation = migrations.AlterField(\n        \"Pony\",\n        \"id\",\n        models.BigAutoField(primary_key=True),\n    )\n    new_state = project_state.clone()\n    operation.state_forwards(app_label, new_state)\n    self.assertIsInstance(\n        new_state.models[app_label, \"pony\"].fields[\"id\"],\n        models.BigAutoField,\n    )\n\n        return [\n            c.type_code\n            for c in connection.introspection.get_table_description(\n                cursor,\n                f\"{app_label}_{table}\",\n            )\n            if c.name == column\n        ][0]\n\n        with connection.cursor() as cursor:\n            parent_id_type = _get_column_id_type(cursor, \"pony\", \"id\")\n            fk_id_type = _get_column_id_type(cursor, \"rider\", \"pony_id\")\n            child_id_type = _get_column_id_type(\n                cursor, \"shetlandpony\", \"pony_ptr_id\"\n            )\n        self.assertEqual(parent_id_type, child_id_type)\n        self.assertEqual(parent_id_type, fk_id_type)\n\n    assertIdTypeEqualsMTIFkType()\n    # Alter primary key.\n    with connection.schema_editor() as editor:\n        operation.database_forwards(app_label, editor, project_state, new_state)\n    assertIdTypeEqualsMTIFkType()\n    if connection.features.supports_foreign_keys:\n        self.assertFKExists(\n            f\"{app_label}_shetlandpony\",\n            [\"pony_ptr_id\"],\n            (f\"{app_label}_pony\", \"id\"),\n        )\n        self.assertFKExists(\n            f\"{app_label}_rider\",\n            [\"pony_id\"],\n            (f\"{app_label}_pony\", \"id\"),\n        )\n    # Reversal.\n    with connection.schema_editor() as editor:\n        operation.database_backwards(app_label, editor, new_state, project_state)\n    assertIdTypeEqualsMTIFkType()\n    if connection.features.supports_foreign_keys:\n        self.assertFKExists(\n            f\"{app_label}_shetlandpony\",\n", "def test_rename_field_with_functional_default(self):\n    \"\"\"\n    Renaming a field that uses a functional default doesn't cause issues.\n    \"\"\"\n    project_state = self.set_up_test_model(\"test_rnflfd\")\n    operation = migrations.RenameField(\"Pony\", \"pink\", \"blue\")\n    self.assertEqual(operation.describe(), \"Rename field pink on Pony to blue\")\n    self.assertEqual(operation.migration_name_fragment, \"rename_pink_pony_blue\")\n    new_state = project_state.clone()\n    operation.state_forwards(\"test_rnflfd\", new_state)\n    self.assertIn(\"blue\", new_state.models[\"test_rnflfd\", \"pony\"].fields)\n    self.assertNotIn(\"pink\", new_state.models[\"test_rnflfd\", \"pony\"].fields)\n    # Rename field.\n    self.assertColumnExists(\"test_rnflfd_pony\", \"pink\")\n    self.assertColumnNotExists(\"test_rnflfd_pony\", \"blue\")\n    with connection.schema_editor() as editor:\n        operation.database_forwards(\"test_rnflfd\", editor, project_state, new_state)\n    self.assertColumnExists(\"test_rnflfd_pony\", \"blue\")\n    self.assertColumnNotExists(\"test_rnflfd_pony\", \"pink\")\n    # Reversal.\n    with connection.schema_editor() as editor:\n        operation.database_backwards(\"test_rnflfd\", editor, new_state, project_state)\n    self.assertColumnExists(\"test_rnflfd_pony\", \"pink\")\n    self.assertColumnNotExists(\"test_rnflfd_pony\", \"blue\")\n    # Deconstruction.\n    definition = operation.deconstruct()\n    self.assertEqual(definition[0], \"RenameField\")\n    self.assertEqual(definition[1], [])\n    self.assertEqual(\n        definition[2],\n        {\"model_name\": \"Pony\", \"old_name\": \"pink\", \"new_name\": \"blue\"},\n    )\n", "def test_repoint_field_m2m_with_through_fields(self):\n    project_state = self.set_up_test_model(\n        \"test_alflmm\", second_model=True, third_model=True\n    )\n\n    project_state = self.apply_operations(\n        \"test_alflmm\",\n        project_state,\n        operations=[\n            migrations.CreateModel(\n                \"Pony\",\n                fields=[\n                    (\n                        \"places\",\n                        models.ManyToManyField(\n                            \"Stable\", related_name=\"ponies\", through=\"PonyStable\"\n                        ),\n                    ),\n                ],\n            ),\n            migrations.CreateModel(\n                \"PonyStable\",\n                fields=[\n                    (\n                        \"pony\",\n                        models.ForeignKey(\"test_alflmm.Pony\", models.CASCADE),\n                    ),\n                    (\n                        \"stable\",\n                        models.ForeignKey(\"test_alflmm.Stable\", models.CASCADE),\n                    ),\n                ],\n            ),\n        ],\n    )\n    Pony = project_state.apps.get_model(\"test_alflmm\", \"Pony\")\n\n    project_state = self.apply_operations(\n        \"test_alflmm\",\n        project_state,\n        operations=[\n            migrations.AlterField(\n                \"Pony\",\n                \"places\",\n                models.ManyToManyField(\n                    to=\"Van\",\n                    related_name=\"ponies\",\n                    through=\"PonyStable\",\n                    through_fields=(\"van\", \"pony\"),\n                ),\n            ),\n            migrations.CreateModel(\n                \"Van\",\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=100)),\n                ],\n            ),\n        ],\n    )\n    Pony = project_state.apps.get_model(\"test_alflmm\", \"Pony\")\n    PonyStable = project_state.apps.get_model(\"test_alflmm\", \"PonyStable\")\n    Van = project_state.apps.get_model(\"test_alflmm\", \"Van\")\n    p = Pony.objects.create(pink=False, weight=4.55)\n    PonyStable.objects.create(pony=p, stable_id=1)\n    PonyStable.objects.create(pony=p, van=Van.objects.create(name=\"Van1\"))\n    self.assertEqual(p.places.count(), 2)\n    p.places.all().delete()\n", "def test_repoint_field_many_to_many_field(self):\n    app_label = \"test_rfieldmtm\"\n    project_state = self.apply_operations(\n        app_label,\n        ProjectState(),\n        operations=[\n            migrations.CreateModel(\n                \"Pony\",\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=20)),\n                ],\n            ),\n            migrations.CreateModel(\n                \"Rider\",\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\n                        \"ponies\",\n                        models.ManyToManyField(\"Pony\", related_name=\"riders\"),\n                    ),\n                ],\n            ),\n        ],\n    )\n\n    Rider = project_state.apps.get_model(app_label, \"Rider\")\n    Pony = project_state.apps.get_model(app_label, \"Pony\")\n    rider = Rider.objects.create()\n    pony = Pony.objects.create()\n    rider.ponies.add(pony)\n\n    # Point the M2M field to self.\n    new_state = project_state.clone()\n    operations = [\n        migrations.AlterField(\n            model_name=\"Rider\",\n            name=\"ponies\",\n            field=models.ManyToManyField(\"self\", related_name=\"riders\"),\n        )\n    ]\n    project_state = self.apply_operations(\n        app_label, project_state, operations=operations\n    )\n    rider = project_state.apps.get_model(app_label, \"Rider\").objects.get(id=rider.id)\n    pony = project_state.apps.get_model(app_label, \"Pony\").objects.get(id=pony.id)\n    with self.assertRaises(ValueError):\n        rider.ponies.add(pony)\n\n    # Reversal.\n    self.unapply_operations(app_label, new_state, operations=operations)\n    rider = new_state.apps.get_model(app_label, \"Rider\").objects.get(id=rider.id)\n    pony = new_state.apps.get_model(app_label, \"Pony\").objects.get(id=pony.id)\n    rider.ponies.add(pony)\n", "def test_reduce_renamed_field_in_indexes(self):\n    \"\"\"\n    Reduces AlterField after RenameField in indexes.\n    \"\"\"\n    app_label = \"test_rifri\"\n    project_state = self.set_up_test_model(app_label, index=True)\n    new_state = project_state.clone()\n    operations = [\n        migrations.RenameField(model_name=\"pony\", old_name=\"pink\", new_name=\"red\"),\n        migrations.AlterField(\n            model_name=\"pony\", name=\"red\", field=models.IntegerField(null=True)\n        ),\n    ]\n    project_state = self.apply_operations(app_label, project_state, operations)\n    self.assertEqual(\n        len(project_state.models[app_label, \"pony\"].options[\"indexes\"]), 1\n    )\n    self.assertEqual(project_state.models[app_label, \"pony\"].options[\"indexes\"][0][1], 1)\n"], "sample_1120": ["def test_matrix_expr_from_index_summation():\n    from sympy import MatrixSymbol, Sum, Symbol\n    from sympy.abc import i, j, k, l, N\n    A = MatrixSymbol(\"A\", N, N)\n    B = MatrixSymbol(\"B\", N, N)\n    expr = Sum(A[i, j]*B[j, k], (j, 0, N-1))\n    assert MatrixExpr.from_index_summation(expr) == A*B\n    expr = Sum(A[j, i]*B[j, k], (j, 0, N-1))\n    assert MatrixExpr.from_index_summation(expr) == A.T*B\n    expr = Sum(A[i, i], (i, 0, N-1))\n    assert MatrixExpr.from_index_summation(expr) == A.trace()\n    expr = Sum(A[i, j]*B[k, j]*A[l, k], (j, 0, N-1), (k, 0, N-1))\n    assert MatrixExpr.from_index_summation(expr) == A*B.T*A.T\n\n    n = Symbol('n', integer=True, nonnegative=True)\n    A = MatrixSymbol(\"A\", n, n)\n    B = MatrixSymbol(\"B\", n, n)\n    expr = Sum(A[i, j]*B[j, k], (j, 0, n-1))\n    assert MatrixExpr.from_index_summation(expr) == A*B\n    expr = Sum(A[j, i]*B[j, k], (j, 0, n-1))\n    assert MatrixExpr.from_index_summation(expr) == A.T*B\n    expr = Sum(A[i, i], (i, 0, n-1))\n    assert MatrixExpr.from_index_summation(expr) == A.trace()\n    expr = Sum(A[i, j]*B[k, j]*A[l, k], (j, 0, n-1), (k, 0, n-1))\n    assert MatrixExpr.from_index_summation(expr) == A*B.T*A.T\n", "def test_matrix_expression_canonicalize():\n    from sympy import MatrixSymbol\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    assert A.canonicalize() == A\n    assert (A*B).canonicalize() == A*B\n    assert (A + B).canonicalize() == A + B\n    assert (A.T).canonicalize() == A.T\n    assert (A**2).canonicalize() == A**2\n    assert (A.I).canonicalize() == A.I\n", "def test_matrix_expr_as_explicit():\n    n = symbols('n', integer=True)\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    C = MatrixSymbol('C', n, n)\n    assert A.as_explicit().shape == (n, n)\n    assert (A + B).as_explicit().shape == (n, n)\n    assert (A*B).as_explicit().shape == (n, n)\n    assert (A*B*C).as_explicit().shape == (n, n)\n    assert (A + B*C).as_explicit().shape == (n, n)\n    assert (A*B + C).as_explicit().shape == (n, n)\n    assert (A**2).as_explicit().shape == (n, n)\n    assert (A**-1).as_explicit().shape == (n, n)\n    assert (A.T).as_explicit().shape == (n, n)\n    assert (A.I).as_explicit().shape == (n, n)\n", "def test_MatAdd_postprocessor_error_handling():\n    # Check that MatAdd doesn't try to create MatAdd with mismatched shapes\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 3, 3)\n    with raises(ShapeError):\n        MatAdd(A, B).doit()\n\n    # Check that MatAdd doesn't try to create MatAdd with mismatched shapes\n    # in the case where one of the matrices is a scalar\n    C = MatrixSymbol(\"C\", 2, 2)\n    with raises(TypeError):\n        MatAdd(5, C).doit()\n\n    # Check that MatAdd doesn't try to create MatAdd with mismatched shapes\n    # in the case where one of the matrices is a matrix expression\n    D = MatrixSymbol(\"D\", 2, 2)\n    E = MatrixSymbol(\"E\", 2, 2)\n    F = MatMul(D, E)\n    with raises(ShapeError):\n        MatAdd(F, C).doit()\n", "def test_issue_9023():\n    # Test if the function MatrixSymbol raises TypeError when \n    # input is not an integer\n    raises(TypeError, lambda: MatrixSymbol('A', 2, 3.5))\n    raises(TypeError, lambda: MatrixSymbol('A', 2, 3 + 0j))\n    raises(TypeError, lambda: MatrixSymbol('A', 2, '3'))\n"], "sample_34": ["def test_enabled_units_context():\n    # Issue #2420\n    with u.set_enabled_units([]):\n        with pytest.raises(u.UnitsError):\n            u.m.find_equivalent_units()\n    with u.set_enabled_units([u.m]):\n        assert u.m.find_equivalent_units() == u.EquivalentUnitsList([u.m])\n", "def test_normalized_equivalencies():\n    equivalencies = u._normalize_equivalencies([(u.m, u.cm, lambda x: x*100, lambda x: x/100)])\n    assert equivalencies == [(u.m, u.cm, lambda x: x*100, lambda x: x/100),\n                           (u.m, u.cm, lambda x: x/100, lambda x: x*100)]\n\n    equivalencies = u._normalize_equivalencies([(u.m, None, lambda x: x, None)])\n    assert equivalencies == [(u.m, None, lambda x: x, None),\n                           (u.m, None, lambda x: x, None)]\n\n    with pytest.raises(ValueError):\n        u._normalize_equivalencies([(u.m, u.cm)])\n", "def test_to_string_powers():\n    \"\"\"\n    Test that powers of units are handled correctly by to_string.\n    \"\"\"\n    assert (u.m ** 2).to_string() == \"m**2\"\n    assert (u.m ** -1).to_string() == \"m**-1\"\n    assert (u.m ** 3.5).to_string() == \"m**3.5\"\n    assert (u.m ** Fraction(1, 2)).to_string() == \"m**0.5\"\n    assert (u.m ** -Fraction(3, 2)).to_string() == \"m**-1.5\"\n", "def test_compose_with_enabled_units_context_manager():\n    \"\"\"\n    Test that units can be composed into units enabled within a context manager.\n    Regression test for https://github.com/astropy/astropy/issues/4326\n    \"\"\"\n    with u.add_enabled_units(u.km):\n        assert u.m.compose(units=u.km)[0] == u.km\n", "def test_base_composition():\n    # Test the composition of base units\n    base_composition = u.m.decompose()\n    assert base_composition.scale == 1.0\n    assert base_composition.bases == [u.m]\n    assert base_composition.powers == [1]\n\n    # Test the composition of a derived unit\n    derived_composition = (u.m / u.s).decompose()\n    assert derived_composition.scale == 1.0\n    assert derived_composition.bases == [u.m, u.s]\n    assert derived_composition.powers == [1, -1]\n\n    # Test the composition of a prefixed unit\n    prefixed_composition = u.km.decompose()\n    assert prefixed_composition.scale == 1000.0\n    assert prefixed_composition.bases == [u.m]\n    assert prefixed_composition.powers == [1]\n\n    # Test the composition of a unit with multiple bases\n    multiple_bases_composition = (u.m * u.s).decompose()\n    assert multiple_bases_composition.scale == 1.0\n    assert multiple_bases_composition.bases == [u.m, u.s]\n    assert multiple_bases_composition.powers == [1, 1]\n\n    # Test the composition of a unit with negative powers\n    negative_powers_composition = (u.m / (u.s ** 2)).decompose()\n    assert negative_powers_composition.scale == 1.0\n    assert negative_powers_composition.bases == [u.m, u.s]\n    assert negative_powers_composition.powers == [1, -2]\n\n    # Test the composition of a unit with fractional powers\n    fractional_powers_composition = (u.m ** 0.5).decompose()\n    assert fractional_powers_composition.scale == 1.0\n    assert fractional_powers_composition.bases == [u.m]\n    assert fractional_powers_composition.powers == [Fraction(1, 2)]\n\n    # Test the composition of a unit with complex powers\n    complex_powers_composition = (u.m ** (1 + 2j)).decompose()\n    assert complex_powers_composition.scale == 1.0\n    assert complex_powers_composition.bases == [u.m]\n    assert complex_powers_composition.powers == [complex(1, 2)]\n"], "sample_368": ["def test_detect_soft_applied_with_unmanaged_models(self):\n    \"\"\"\n    Test that unmanaged models are excluded from soft applied detection.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    # Were the tables there before?\n    self.assertTableNotExists(\"migrations_author\")\n    self.assertTableNotExists(\"migrations_unmanaged\")\n    # Run it normally\n    self.assertEqual(\n        executor.migration_plan([(\"migrations\", \"0001_initial\")]),\n        [\n            (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n        ],\n    )\n    executor.migrate([(\"migrations\", \"0001_initial\")])\n    # Are the tables there now?\n    self.assertTableExists(\"migrations_author\")\n    self.assertTableNotExists(\"migrations_unmanaged\")\n    # Make sure that the unmanaged model is not detected as soft applied\n    migration = executor.loader.get_migration(\"migrations\", \"0001_initial\")\n    self.assertIs(executor.detect_soft_applied(None, migration)[0], False)\n    # Rebuild the graph to reflect the new DB state\n    executor.loader.build_graph()\n    # Fake-reverse that\n    executor.migrate([(\"migrations\", None)], fake=True)\n    # Are the tables still there?\n    self.assertTableExists(\"migrations_author\")\n    self.assertTableNotExists(\"migrations_unmanaged\")\n    # Make sure that was faked\n    executor.loader.build_graph()\n    # Finally, migrate forwards; this should fake-apply our initial migration\n    executor.migrate([(\"migrations\", \"0001_initial\")], fake_initial=True)\n    self.assertTableExists(\"migrations_author\")\n    self.assertTableNotExists(\"migrations_unmanaged\")\n    # And migrate back to clean up the database\n    executor.loader.build_graph()\n    executor.migrate([(\"migrations\", None)])\n    self.assertTableNotExists(\"migrations_author\")\n    self.assertTableNotExists(\"migrations_unmanaged\")\n", "def test_migration_plan_with_replacements(self):\n    \"\"\"\n    Test migration_plan with replacement migrations.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    # Create a replacement migration for \"0001_initial\"\n    executor.loader.replace_migrations = True\n    executor.loader.build_graph()\n    plan = executor.migration_plan([(\"migrations\", \"0002_second\")])\n    self.assertEqual(\n        plan,\n        [\n            (executor.loader.graph.nodes[\"migrations\", \"0001_replaced\"], False),\n            (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False),\n        ],\n    )\n    # Make sure the replaced migration is not in the plan\n    self.assertNotIn(\"0001_initial\", [node[0][1] for node in plan])\n    # Now try migrating to the replaced migration\n    plan = executor.migration_plan([(\"migrations\", \"0001_replaced\")])\n    self.assertEqual(\n        plan,\n        [\n            (executor.loader.graph.nodes[\"migrations\", \"0001_replaced\"], False),\n        ],\n    )\n    # Make sure the original migration is not in the plan\n    self.assertNotIn(\"0001_initial\", [node[0][1] for node in plan])\n", "    def test_migration_plan_with_replaced_migration(self):\n        \"\"\"\n        Tests the migration plan generation when a migration replaces another.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        # Make sure the replaced migration is applied\n        executor.migrate([(\"migrations\", \"0001_initial\")])\n        executor.loader.build_graph()\n        # Replaced migration should not be in the plan\n        plan = executor.migration_plan([(\"migrations\", \"0002_second\")])\n        self.assertEqual(\n            plan,\n            [\n                (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False),\n            ],\n        )\n        # Replaced migration should be unapplied if it's in the way\n        plan = executor.migration_plan([(\"migrations\", None)])\n        self.assertEqual(\n            plan,\n            [\n                (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], True),\n                (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], True),\n            ],\n        )\n        # If the replaced migration is not applied, it should be in the plan\n        executor.loader.build_graph()\n        executor.recorder.record_unapplied(\"migrations\", \"0001_initial\")\n        plan = executor.migration_plan([(\"migrations\", \"0002_second\")])\n        self.assertEqual(\n            plan,\n            [\n                (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n                (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False),\n            ],\n        )\n", "def test_detect_soft_applied_ignores_unmanaged_models(self):\n    \"\"\"\n    executor.detect_soft_applied() ignores unmanaged models.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    # Create the table for the managed model but make it look like the\n    # migration hasn't been applied.\n    executor.migrate([(\"migrations\", \"0001_initial\")])\n    executor.migrate([(\"migrations\", None)], fake=True)\n    # Table detection sees the migration is applied but should ignore the\n    # unmanaged model.\n    migration = executor.loader.get_migration(\"migrations\", \"0001_initial\")\n    self.assertIs(executor.detect_soft_applied(None, migration)[0], True)\n    # Leave the table for the managed model. That table should not cause\n    # detect_soft_applied() to return False.\n    with connection.schema_editor() as editor:\n        editor.execute(editor.sql_delete_table % {\"table\": \"migrations_unmanaged\"})\n    migration = executor.loader.get_migration(\"migrations\", \"0001_initial\")\n    self.assertIs(executor.detect_soft_applied(None, migration)[0], True)\n\n    # Cleanup by removing the remaining tables.\n    with connection.schema_editor() as editor:\n        editor.execute(editor.sql_delete_table % {\"table\": \"migrations_author\"})\n    self.assertTableNotExists(\"migrations_author\")\n    self.assertTableNotExists(\"migrations_unmanaged\")\n", "def test_migrate_marks_replacement_applied_with_replaced(self):\n    \"\"\"\n    A new squash migration will be marked as applied even if all its replaced\n    migrations were previously already applied, including the replaced ones.\n    \"\"\"\n    recorder = MigrationRecorder(connection)\n    # Record all replaced migrations as applied\n    recorder.record_applied(\"migrations\", \"0001_initial\")\n    recorder.record_applied(\"migrations\", \"0002_second\")\n    recorder.record_applied(\"migrations\", \"0003_third\")\n    recorder.record_applied(\"migrations\", \"0004_fourth\")\n    executor = MigrationExecutor(connection)\n    executor.migrate([(\"migrations\", \"0001_squashed_0002_0003_0004\")])\n\n    # Because 0001 and 0002 are both applied, even though this migrate run\n    # didn't apply anything new, their squashed replacement should be marked as\n    # applied.\n    self.assertIn(\n        (\"migrations\", \"0001_squashed_0002_0003_0004\"),\n        recorder.applied_migrations(),\n    )\n\n    # Also the original migrations should be marked as unapplied\n    self.assertNotIn(\n        (\"migrations\", \"0002_second\"),\n        recorder.applied_migrations(),\n    )\n    self.assertNotIn(\n        (\"migrations\", \"0003_third\"),\n        recorder.applied_migrations(),\n    )\n    self.assertNotIn(\n        (\"migrations\", \"0004_fourth\"),\n        recorder.applied_migrations(),\n    )\n"], "sample_994": ["def test_Catalan_rewrite_as_sqrt():\n    assert Catalan.rewrite(sqrt) == S.Half + sqrt(5)*S.Half\n", "def test_issue_14383():\n    assert S(3.5).factors() == {}\n", "def test_Rational_hash():\n    assert hash(Rational(0.5)) == hash(Rational(1, 2))\n    assert hash(Rational(-0.5)) == hash(Rational(-1, 2))\n    assert hash(Rational(0.5)) != hash(Rational(0.6))\n    assert hash(Rational(-0.5)) != hash(Rational(0.5))\n    raises(TypeError, lambda: hash(Rational(0.5, 3)))\n", "def test_RationalFloatIssue():\n    x = Rational(1, 2)\n    assert x.evalf() == 0.5\n    x = Rational(3, 2)\n    assert x.evalf() == 1.5\n    x = Rational(2, 3)\n    assert x.evalf() == 0.666666666666667\n    x = Rational(5, 4)\n    assert x.evalf() == 1.25\n    x = Rational(1, 10)\n    assert x.evalf() == 0.1\n    x = Rational(1, 11)\n    assert x.evalf() == 0.090909090909091\n    x = Rational(1, 100)\n    assert x.evalf() == 0.01\n    x = Rational(3, 7)\n    assert x.evalf() == 0.428571428571429\n    x = Rational(1, 7)\n    assert x.evalf() == 0.142857142857143\n", "def test_issue_13470():\n    # Integer truncates trailing L's in Python 3\n    # Integer should not remove trailing L's in Python 2\n    from mpmath import mpf\n    import struct\n    assert Integer(mpf('1.2', 100)._mpf_[1]) == Integer(long(1200000000000000000000000000000000000000000000000000000000000))\n    assert Integer(mpf('1.2', 100)._mpf_[1]) == Integer(long(1200000000000000000000000000000000000000000000000000000000000L))\n    i = Integer(mpf('1.2', 100)._mpf_[1])\n    assert Integer(i) == i\n    assert Integer(long(i)) == i\n"], "sample_339": ["def test_custom_formfield_callback(self):\n    class PoetForm(forms.ModelForm):\n        class Meta:\n            model = Poet\n            fields = \"__all__\"\n\n        if f.name == 'name':\n            return forms.CharField(min_length=2, max_length=100)\n        return f.formfield(**kwargs)\n\n    PoetFormSet = modelformset_factory(Poet, fields=\"__all__\", form=PoetForm, formfield_callback=formfield_callback)\n    formset = PoetFormSet()\n    self.assertEqual(formset.forms[0].fields['name'].min_length, 2)\n    self.assertEqual(formset.forms[0].fields['name'].max_length, 100)\n", "def test_inline_formsets_with_custom_save_method_and_commit_false(self):\n    class PoemForm(forms.ModelForm):\n            # change the name to \"Brooklyn Bridge\" just to be a jerk.\n            poem = super().save(commit=False)\n            poem.name = \"Brooklyn Bridge\"\n            if commit:\n                poem.save()\n            return poem\n\n    PoemFormSet = inlineformset_factory(Poet, Poem, form=PoemForm, fields=\"__all__\")\n\n    data = {\n        'poem_set-TOTAL_FORMS': '3',  # the number of forms rendered\n        'poem_set-INITIAL_FORMS': '0',  # the number of forms with initial data\n        'poem_set-MAX_NUM_FORMS': '',  # the max number of forms\n        'poem_set-0-name': 'The Cloud in Trousers',\n        'poem_set-1-name': 'I',\n        'poem_set-2-name': '',\n    }\n\n    poet = Poet.objects.create(name='Vladimir Mayakovsky')\n    formset = PoemFormSet(data=data, instance=poet)\n    self.assertTrue(formset.is_valid())\n\n    saved = formset.save(commit=False)\n    self.assertEqual(len(saved), 2)\n    poem1, poem2 = saved\n    self.assertEqual(poem1.name, 'Brooklyn Bridge')\n    self.assertEqual(poem2.name, 'Brooklyn Bridge')\n\n    self.assertEqual(Poem.objects.count(), 0)\n\n    for poem in saved:\n        poem.save()\n\n    self.assertEqual(Poem.objects.count(), 2)\n    poem1, poem2 = Poem.objects.order_by('name')\n    self.assertEqual(poem1.name, 'Brooklyn Bridge')\n    self.assertEqual(poem2.name, 'Brooklyn Bridge')\n", "    def test_inline_formsets_with_multi_table_inheritance_and_proxy_model(self):\n        # Test inline formsets where the inline-edited object uses multi-table\n        # inheritance and the model is a proxy.\n\n        class BookProxy(Book):\n            class Meta:\n                proxy = True\n\n        AuthorBooksFormSet = inlineformset_factory(Author, BookProxy, can_delete=False, extra=1, fields=\"__all__\")\n        author = Author.objects.create(pk=1, name='Charles Baudelaire')\n\n        formset = AuthorBooksFormSet(instance=author)\n        self.assertEqual(len(formset.forms), 1)\n        self.assertHTMLEqual(\n            formset.forms[0].as_p(),\n            '<p><label for=\"id_book_set-0-title\">Title:</label>'\n            '<input id=\"id_book_set-0-title\" type=\"text\" name=\"book_set-0-title\" maxlength=\"100\"></p>'\n            '<p><label for=\"id_book_set-0-notes\">Notes:</label>'\n            '<input id=\"id_book_set-0-notes\" type=\"text\" name=\"book_set-0-notes\" maxlength=\"100\">'\n            '<input type=\"hidden\" name=\"book_set-0-author\" value=\"1\" id=\"id_book_set-0-author\">'\n            '<input type=\"hidden\" name=\"book_set-0-id\" id=\"id_book_set-0-id\"></p>'\n        )\n\n        data = {\n            'book_set-TOTAL_FORMS': '1',  # the number of forms rendered\n            'book_set-INITIAL_FORMS': '0',  # the number of forms with initial data\n            'book_set-MAX_NUM_FORMS': '',  # the max number of forms\n            'book_set-0-title': 'Flowers of Evil',\n            'book_set-0-notes': 'English translation of Les Fleurs du Mal'\n        }\n\n        formset = AuthorBooksFormSet(data, instance=author)\n        self.assertTrue(formset.is_valid())\n\n        saved = formset.save()\n        self.assertEqual(len(saved), 1)\n        book1, = saved\n        self.assertEqual(book1.title, 'Flowers of Evil')\n        self.assertEqual(book1.notes, 'English translation of Les Fleurs du Mal')\n", "def test_inline_formset_with_custom_save_method_parent_instance(self):\n    # Test that the ModelForm.save() method can access the parent instance if\n    # it exists in the database (#24395).\n\n    class BookForm(forms.ModelForm):\n            book = super().save(commit=False)\n            book.notes = \"Written by %s\" % book.author.name\n            if commit:\n                book.save()\n            return book\n\n    BookFormSet = inlineformset_factory(Author, Book, form=BookForm, fields=\"__all__\")\n    data = {\n        'book_set-TOTAL_FORMS': '1',\n        'book_set-INITIAL_FORMS': '0',\n        'book_set-MAX_NUM_FORMS': '',\n        'book_set-0-title': 'Le Lac',\n    }\n    author = Author(name=\"Charles Baudelaire\")\n    author.save()\n    formset = BookFormSet(data=data, instance=author)\n    self.assertTrue(formset.is_valid())\n\n    book = formset.save()[0]\n    self.assertEqual(book.notes, 'Written by Charles Baudelaire')\n", "    def test_save_calls_model_save_with_args_kwargs(self):\n        class TestForm(forms.ModelForm):\n            class Meta:\n                model = Author\n                fields = ('name',)\n\n        form = TestForm({'name': 'Test Author'})\n        self.assertTrue(form.is_valid())\n        author = form.save(commit=False)\n        author.save(force_update=True)\n        self.assertEqual(Author.objects.get().name, 'Test Author')\n"], "sample_598": ["def test_inline_variable_array_repr(self):\n    class CustomArray:\n            self.value = value\n\n            return NotImplemented\n\n        @property\n            return self.value.shape\n\n        @property\n            return self.value.dtype\n\n        @property\n            return self.value.ndim\n\n    value = CustomArray(np.array([20, 40]))\n    variable = xr.Variable(\"x\", value)\n\n    max_width = 10\n    actual = formatting.inline_variable_array_repr(variable, max_width=10)\n\n    expected = maybe_truncate(repr(value.value).replace(\"\\n\", \" \"), max_width)\n    assert actual == expected\n", "def test_unindexed_dims_repr(self):\n    cases = [\n        (((\"x\", \"y\"), {\"x\": [1, 2], \"y\": [1, 2]}), None),\n        (((\"x\", \"y\", \"z\"), {\"x\": [1, 2], \"y\": [1, 2]}), \"Dimensions without coordinates: z\"),\n    ]\n\n    for dims, coords, expected in cases:\n        result = formatting.unindexed_dims_repr(dims, coords)\n        assert result == expected\n", "def test_inline_variable_array_repr(self):\n    cases = [\n        (xr.DataArray([1, 2, 3]), \"1 2 3\"),\n        (xr.DataArray([1, 2, 3], dims=[\"x\"]), \"1 2 3\"),\n        (xr.DataArray([1, 2, 3], dims=[\"x\"], coords={\"x\": [0, 1, 2]}), \"1 2 3\"),\n        (\n            xr.DataArray(\n                [1, 2, 3], dims=[\"x\"], coords={\"x\": [0, 1, 2]}, attrs={\"units\": \"m\"}\n            ),\n            \"1 2 3\",\n        ),\n    ]\n    for var, expected in cases:\n        max_width = 10\n        actual = formatting.inline_variable_array_repr(var, max_width)\n        assert actual == expected\n", "def test_summarize_variable(self):\n    variable = xr.Variable(\"x\", np.arange(4), {\"units\": \"meters\"})\n    expected = \"  x        (x) int64 0 1 2 3\"\n    actual = formatting.summarize_variable(\"x\", variable, 7)\n    assert expected == actual\n\n    # with index\n    variable = xr.Variable(\"x\", np.arange(4), {\"units\": \"meters\"})\n    expected = \"* x        (x) int64 0 1 2 3\"\n    actual = formatting.summarize_variable(\"x\", variable, 7, \"*\")\n    assert expected == actual\n", "    def test_inline_sparse_repr(self):\n        sparse_array = xr.DataArray(\n            sparse.COO(coords=[[0, 1, 2], [0, 1, 2]], data=[1, 2, 3], shape=(3, 3))\n        )\n        actual = formatting.inline_sparse_repr(sparse_array.data)\n        expected = \"<sparse.COO: nnz=3, fill_value=0>\"\n        assert actual == expected\n"], "sample_396": ["    def setUpTestData(cls):\n        cls.a1 = Author.objects.create(name=\"a1\", num=1001)\n        cls.a2 = Author.objects.create(name=\"a2\", num=2002)\n        cls.r1 = Report.objects.create(name=\"r1\", creator=cls.a1)\n        cls.r2 = Report.objects.create(name=\"r2\", creator=cls.a2)\n        cls.r3 = Report.objects.create(name=\"r3\")\n", "    def setUpTestData(cls):\n        cls.n1 = Note.objects.create(note=\"n1\", misc=\"foo\", id=1)\n        cls.n2 = Note.objects.create(note=\"n2\", misc=\"bar\", id=2)\n        e1 = ExtraInfo.objects.create(info=\"e1\", note=cls.n1)\n        e2 = ExtraInfo.objects.create(info=\"e2\", note=cls.n2)\n        cls.a1 = Author.objects.create(name=\"a1\", num=1001, extra=e1)\n        cls.a2 = Author.objects.create(name=\"a2\", num=2002, extra=e1)\n        cls.a3 = Author.objects.create(name=\"a3\", num=3003, extra=e2)\n", "    def test_reuse_with_different_aliases(self):\n        # Joins can be reused if they have different aliases but the same\n        # connections.\n        qs1 = School.objects.filter(pk__in=Student.objects.values_list(\"school_id\", flat=True))\n        qs2 = School.objects.filter(pk__in=Classroom.objects.values_list(\"school_id\", flat=True))\n        self.assertEqual(str((qs1 & qs2).query).count(\"JOIN\"), 2)\n        self.assertEqual(len(qs1 & qs2), 1)\n        self.assertEqual((qs1 & qs2)[0].pk, self.school.pk)\n", "    def test_ticket_24671(self):\n        \"\"\"\n        The \"id__in\" lookup should be correctly optimized for subqueries.\n        \"\"\"\n        # Create a subquery with an aggregate.\n        qs = (\n            Author.objects.filter(extra__in=ExtraInfo.objects.annotate(\n                max_note_id=Max('note_id'\n            ).filter(max_note_id__gt=0))\n        )\n        # Verify that the subquery is correctly optimized.\n        self.assertIn('max', str(qs.query))\n        self.assertSequenceEqual(qs, [self.a1, self.a2, self.a3, self.a4])\n", "    def setUpTestData(cls):\n        cls.a1 = Author.objects.create(name=\"a1\", num=1001)\n        cls.a2 = Author.objects.create(name=\"a2\", num=2002)\n        cls.a3 = Author.objects.create(name=\"a3\", num=3003)\n        cls.i1 = Item.objects.create(name=\"one\", created=datetime.datetime.now(), creator=cls.a1)\n        cls.i2 = Item.objects.create(name=\"two\", created=datetime.datetime.now(), creator=cls.a2)\n        cls.i3 = Item.objects.create(name=\"three\", created=datetime.datetime.now(), creator=cls.a2)\n        cls.i4 = Item.objects.create(name=\"four\", created=datetime.datetime.now(), creator=cls.a3)\n"], "sample_998": ["def test_latex_DifferentialForms():\n    from sympy.diffgeom import Manifold, Patch, CoordSystem, CoordChart\n    M = Manifold(\"M\", 2)\n    P = Patch(\"P\", M)\n    cs = CoordSystem(\"cs\", P)\n    c_coor1 = CoordChart(cs, (Symbol('x'), Symbol('y')))\n    d = c_coor1.diff_form(1)\n    assert latex(d) == r\"\\mathrm{d}x \\wedge \\mathrm{d}y\"\n", "def test_latex_super_subscripted_mathring():\n    s = Symbol('mathringx')\n    assert latex(s) == r\"\\mathring{x}\"\n    assert latex(s**2) == r\"{\\mathring{x}}^{2}\"\n    assert latex(s**-2) == r\"{\\mathring{x}}^{-2}\"\n    assert latex(-s) == r\"-\\mathring{x}\"\n    s = Symbol('xmathring')\n    assert latex(s) == \"x_{\\mathring}\"\n    assert latex(s**2) == \"x_{\\mathring}^{2}\"\n    assert latex(s**-2) == \"x_{\\mathring}^{-2}\"\n    assert latex(-s) == r\"-x_{\\mathring}\"\n    s = Symbol('mathringxmathring')\n    assert latex(s) == r\"\\mathring{x}_{\\mathring}\"\n    assert latex(s**2) == r\"{\\mathring{x}}_{\\mathring}^{2}\"\n    assert latex(s**-2) == r\"{\\mathring{x}}_{\\mathring}^{-2}\"\n    assert latex(-s) == r\"-{\\mathring{x}}_{\\mathring}\"\n", "def test_latex_tensor():\n    from sympy.tensor import TensorType, TensorIndexType, tensors\n    from sympy import symbols\n    L = TensorIndexType('L', dim=2)\n    i, j, k, l = symbols('i j k l', tensor_index_types=(L, L, L, L))\n    A = TensorType((L, L), 'A')\n    B = TensorType((L,), 'B')\n    a, b = A(i, j), B(k)\n    t = tensors._TensorHead('t', [L]*4)\n    expr = t(i, j, k, l)\n    assert latex(expr) == \"t^{i}_{jkl}\"\n    assert latex(a) == \"A^{ij}\"\n    assert latex(b) == \"B^{k}\"\n    assert latex(a*b) == \"A^{ij} B^{k}\"\n    assert latex(a*b + expr) == \"t^{i}_{jkl} + A^{ij} B^{k}\"\n", "def test_latex_folding_in_Mul():\n    # Test folding of short fractions in the coefficients of Mul objects\n    assert latex(x*y) == r\"x y\"\n    assert latex(1/x*y) == r\"\\frac{y}{x}\"\n    assert latex(1/x*y, fold_short_frac=True) == r\"\\frac{y}{x}\"\n    assert latex(1/x*y, fold_short_frac=False) == r\"\\frac{y}{x}\"\n    assert latex(2/x*y) == r\"\\frac{2 y}{x}\"\n    assert latex(2/x*y, fold_short_frac=True) == r\"\\frac{2 y}{x}\"\n    assert latex(2/x*y, fold_short_frac=False) == r\"\\frac{2 y}{x}\"\n    assert latex(2/x*y, fold_frac_powers=True) == r\"\\frac{2 y}{x}\"\n    assert latex(2/x*y, fold_frac_powers=False) == r\"\\frac{2 y}{x}\"\n    assert latex(2*y/x) == r\"\\frac{2 y}{x}\"\n    assert latex(2*y/x, fold_short_frac=True) == r\"\\frac{2 y}{x}\"\n    assert latex(2*y/x, fold_short_frac=False) == r\"\\frac{2 y}{x}\"\n    assert latex(x/2*y) == r\"\\frac{x y}{2}\"\n    assert latex(x/2*y, fold_short_frac=True) == r\"x y / 2\"\n    assert latex(x/2*y, fold_short_frac=False) == r\"\\frac{x y}{2}\"\n    assert latex(x/2*y, fold_frac_powers=True) == r\"x y / 2\"\n    assert latex(x/2*y, fold_frac_powers=False) == r\"\\frac{x y}{2}\"\n    assert latex(x*y/2) == r\"\\frac{x y}{2}\"\n    assert latex(x*y/2, fold_short_frac=True) == r\"x y / 2\"\n    assert latex(x*y/2, fold_short_frac=False) == r\"\\frac{x y}{2}\"\n    assert latex(x*y/2, fold_frac_powers=True) == r\"x y / 2\"\n    assert latex(x*y/2, fold_frac_powers=False) == r\"\\frac{x y}{2}\"\n    assert latex(2*(x + 1)/y) == r\"\\frac{2 \\left(x + 1\\right", "def test_latex_printing_with_difficult_expressions():\n    # Test the latex printing of more complex expressions that cover a wide range\n    # of the available functions and special variables\n    from sympy import symbols, Function, sin, cos, tan, log, sqrt, Rational, pi, I, oo, Matrix, diff\n    x, y, z = symbols('x y z')\n    f = Function('f')\n    latex_expression = \"f{\\left (x + y \\right )} + \\frac{1}{2} \\left(2 \\log{\\left (x + 1 \\right )} + \\log{\\left (x + 1 \\right )}\\right) - \\sqrt{2} \\sin{\\left (2 \\sqrt{2} x + \\pi \\right )} - \\cos{\\left (2 \\sqrt{2} x + \\pi \\right )} + \\tan{\\left (2 \\sqrt{2} x + \\pi \\right )} + \\frac{3 \\left(x + y \\right) \\left(x - y \\right)}{2 \\left(2 \\left(x + y \\right) \\left(x - y \\right) + 1 \\right)} - \\frac{2 \\left(x + y \\right) \\left(x - y \\right)}{2 \\left(x + y \\right) \\left(x - y \\right) + 1} + 1 + i\"\n    assert latex(f(x + y) + Rational(1, 2) * (2 * log(x + 1) + log(x + 1)) - sqrt(2) * sin(2 * sqrt(2) * x + pi) - cos(2 * sqrt(2) * x + pi) + tan(2 * sqrt(2) * x + pi) + (3 * (x + y) * (x - y) / (2 * (2 * (x + y) * (x - y) + 1))) - (2 * (x + y) * (x - y) / (2 * (x + y) * (x - y) + 1)) + 1 + I) == latex_expression\n"], "sample_1195": ["def test_simplify_gpgp():\n    i0,i1,i2,i3,i4,i5 = tensor_indices('i0:6', LorentzIndex)\n    p, q = tensor_heads('p,q', [LorentzIndex])\n    \n    t = p(i0)*G(-i0)*q(i1)*G(-i1)\n    t1 = simplify_gpgp(t)\n    assert _is_tensor_eq(t1, G(-i0)*p(i0)*q(i1)*q(-i1))\n\n    t = p(i0)*G(-i0)*q(i1)*G(-i1)*p(i2)*G(-i2)\n    t1 = simplify_gpgp(t)\n    assert _is_tensor_eq(t1, G(-i0)*p(i0)*q(i1)*q(-i1)*p(i2)*G(-i2))\n\n    t = p(i0)*G(-i0)*q(i1)*G(-i1)*p(i2)*G(-i2)*q(i3)*G(-i3)\n    t1 = simplify_gpgp(t)\n    assert _is_tensor_eq(t1, G(-i0)*p(i0)*q(i1)*q(-i1)*p(i2)*q(i3)*q(-i3)*G(-i2))\n\n    t = p(i0)*G(-i0)*q(i1)*G(-i1)*p(i2)*G(-i2)*q(i3)*G(-i3)*p(i4)*G(-i4)\n    t1 = simplify_gpgp(t)\n    assert _is_tensor_eq(t1, G(-i0)*p(i0)*q(i1)*q(-i1)*p(i2)*q(i3)*q(-i3)*p(i4)*G(-i4)*G(-i2))\n\n    t = p(i0)*G(-i0)*q(i1)*G(-i1)*p(i2)*G(-i2)*q(i3)*G(-i3)*p(i4)*G(-i4)*q(i5)*G(-i5)\n    t1 = simplify_gpgp(t)\n    assert _is_tensor_eq(t1, G(-i0)*p(i0)*q(i1)*q(-i1)*p(i2)*q(i3)*q(-i3)*p(i4)*q(i5)*q", "def test_gamma_matrix_simplify_gpgp():\n    i0,i1,i2,i3,i4,i5,i6 = tensor_indices('i0:7', LorentzIndex)\n    p, q = tensor_heads('p,q', [LorentzIndex])\n    ps = p(i0)*G(-i0)\n    qs = q(i0)*G(-i0)\n\n    t = simplify_gpgp(ps*qs)\n    assert _is_tensor_eq(t, G(-i0)*p(i0)*q(-i0))\n\n    t = simplify_gpgp(ps*qs*qs)\n    assert _is_tensor_eq(t, G(-i0)*p(i0)*q(i1)*q(-i1))\n\n    t = simplify_gpgp(ps*qs*qs*ps*qs)\n    assert _is_tensor_eq(t, (G(-i0)*p(i0)*q(i1)*q(-i1)) + (G(-i0)*p(i0)*q(i1)*q(i2)*G(-i2)*q(-i1)))\n\n    t = simplify_gpgp(ps*qs*qs*ps*qs*ps*qs)\n    assert _is_tensor_eq(t, (G(-i0)*p(i0)*q(i1)*q(-i1)) + (G(-i0)*p(i0)*q(i1)*q(i2)*G(-i2)*q(-i1)) + (G(-i0)*p(i0)*q(i1)*q(i2)*G(-i2)*q(i3)*G(-i3)*q(-i1)))\n\n    t = simplify_gpgp(ps*qs*qs*ps*qs*ps*qs*ps*qs)\n    assert _is_tensor_eq(t, (G(-i0)*p(i0)*q(i1)*q(-i1)) + (G(-i0)*p(i0)*q(i1)*q(i2)*G(-i2)*q(-i1)) + (G(-i0)*p(i0)*q(i1)*q(i2)*G(-i2)*q(i3)*G(-i3)*q(-i1)) + (G(-i0)*p(i0)*q(i1)*q(i2)*G(-i2)*q(i3)*G(-i3)*q(i4)*G(-i4)*q(-i1)))\n", "def test_gamma_trace_gpgp():\n    g = LorentzIndex.metric\n    m0, m1, m2, m3, m4, m5, m6 = tensor_indices('m0:7', LorentzIndex)\n    p, q = tensor_heads('p,q', [LorentzIndex])\n\n    # Test gamma_trace and simplify_gpgp on products of G(i)*p(-i)*G(j)*p(-j)\n    t = G(m0)*p(-m0)*G(m1)*p(-m1)\n    t1 = gamma_trace(simplify_gpgp(t, sort=False))\n    assert _is_tensor_eq(t1, 4*p(m0)*p(-m0))\n\n    t = G(m0)*p(-m0)*G(m1)*q(-m1)\n    t1 = gamma_trace(simplify_gpgp(t, sort=False))\n    assert _is_tensor_eq(t1, 4*p(m0)*q(-m0))\n\n    t = G(m0)*p(-m0)*G(m1)*q(-m1)*G(m2)*p(-m2)\n    t1 = gamma_trace(simplify_gpgp(t, sort=False))\n    assert _is_tensor_eq(t1, 16*p(m0)*q(-m0)*p(m2)*p(-m2) - 4*p(m0)*p(-m0)*q(m2)*q(-m2))\n", "def test_gamma_matrix_trace_with_spinor_indices():\n    g = LorentzIndex.metric\n\n    m0, m1, m2, m3, m4, m5, m6 = tensor_indices('m0:7', LorentzIndex)\n    n0, n1, n2, n3, n4, n5 = tensor_indices('n0:6', LorentzIndex)\n\n    # working in D=4 dimensions\n    D = 4\n\n    # traces with spinor indices are not implied:\n    t = G(m0)*G(m1)*G(m2)*G(m3)\n    t1 = gamma_trace(t)\n    assert t1.equals(0)\n\n    # test that no contraction happens with DiracSpinorIndex.auto_left/auto_right:\n    from sympy.physics.hep.gamma_matrices import DiracSpinorIndex\n    from sympy.physics.hep.gamma_matrices import GammaMatrix as G\n    from sympy.physics.hep.gamma_matrices import gamma_trace\n    from sympy.tensor.tensor import tensor_indices\n    from sympy.tensor.tensor import tensor_mul\n\n    i0,i1,i2,i3,i4,i5 = tensor_indices('i0:6', LorentzIndex)\n    p = DiracSpinorIndex.auto_right\n    q = DiracSpinorIndex.auto_left\n    t = G(i0)*G(i1)*G(i2)*G(i3)*DiracSpinorIndex.delta(p, -q)\n    t1 = gamma_trace(t)\n    assert t1.equals(t)\n    t2 = gamma_trace(t, DiracSpinorIndex.auto_left, DiracSpinorIndex.auto_right)\n    assert t2.equals(4*g(i0, i2)*DiracSpinorIndex.delta(p, -q) - 4*g(i0, i1)*g(i2, i3)*DiracSpinorIndex.delta(p, -q) + 4*g(i0, i3)*g(i1, i2)*DiracSpinorIndex.delta(p, -q))\n", "def test_gamma_trace_with_other_tensors():\n    g = LorentzIndex.metric\n\n    m0, m1, m2, m3, m4, m5, m6 = tensor_indices('m0:7', LorentzIndex)\n    n0, n1, n2, n3, n4, n5 = tensor_indices('n0:6', LorentzIndex)\n\n    p, q = tensor_heads('p,q', [LorentzIndex])\n    ps = p(m0)*G(-m0)\n    qs = q(m0)*G(-m0)\n\n    t = ps*qs*G(m1)*G(m2)*G(m3)*G(-m1)*G(-m2)*G(-m3)\n    t1 = gamma_trace(t)\n    assert t1.equals(4*pq*(D - 4)*g(m1, m3)*p(m2)*q(-m2) + 4*pq*(D - 4)*p(m1)*q(-m1)*g(m2, m3) + 8*pq*p(m1)*q(-m1)*p(m2)*q(-m2))\n\n    t = G(m0)*G(m1)*G(-m0)*G(m3)*G(m4)*G(-m3)\n    t1 = gamma_trace(t)\n    assert _is_tensor_eq(t1, 4*(-4 + D)*g(m1, m4)*g(m0, m4)*G(m3)*G(-m3) + (-4 + D)*(D - 4)*g(m0, m4)*g(m1, m3)*G(m3)*G(-m3) + (D - 4)*G(m0)*G(-m0)*g(m1, m4)*G(m3)*G(-m3))\n\n    t = G(m0)*G(m1)*G(m2)*G(m3)*G(m4)*G(m5)*G(-m0)*G(-m1)*G(-m2)*G(-m3)*G(-m4)\n    t1 = gamma_trace(t)\n    t2 = (D - 4)**3*G(m5)*G(-m5) + 8*D*(D - 4)**2*g(m5, m0)*G(m1)*G(m2)*G(m3)*G(-m1)*G(-m"], "sample_49": ["    def test_clearable_file_input(self):\n        class MyForm(Form):\n            file_field = CharField(max_length=20, widget=ClearableFileInput())\n        f = MyForm(files={'file_field': 'file.txt'})\n        self.assertEqual(\n            f.fields['file_field'].widget.value_from_datadict({'file_field-clear': 'on'}, f.files, 'file_field'),\n            False\n        )\n", "def test_media_inheritance_from_meta(self):\n    ###############################################################\n    # Inheritance of media from Meta classes\n    ###############################################################\n\n    class MyWidget1(TextInput):\n        class Meta:\n            media = Media(css={'all': ('path/to/css1', '/path/to/css2')}, js=('/path/to/js1',))\n\n    class MyWidget2(MyWidget1):\n        class Media:\n            css = {'all': ('/path/to/css3', 'path/to/css1')}\n            js = ('/path/to/js1', '/path/to/js4')\n\n    w2 = MyWidget2()\n    self.assertEqual(\n        str(w2.media),\n        \"\"\"<link href=\"/path/to/css3\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">", "def test_splitdatetimewidget(self):\n    ###############################################################\n    # SplitDateTimeWidget\n    ###############################################################\n\n    class MyForm(Form):\n        datetime_field = DateTimeField(widget=SplitDateTimeWidget())\n\n    form = MyForm({'datetime_field_0': '2022-07-25', 'datetime_field_1': '14:30'})\n\n    self.assertEqual(form.is_valid(), True)\n    self.assertEqual(form.cleaned_data['datetime_field'], datetime.datetime(2022, 7, 25, 14, 30))\n\n    # Test invalid date\n    form = MyForm({'datetime_field_0': '2022-13-25', 'datetime_field_1': '14:30'})\n\n    self.assertEqual(form.is_valid(), False)\n\n    # Test invalid time\n    form = MyForm({'datetime_field_0': '2022-07-25', 'datetime_field_1': '25:30'})\n\n    self.assertEqual(form.is_valid(), False)\n\n    # Test both invalid\n    form = MyForm({'datetime_field_0': '2022-13-32', 'datetime_field_1': '25:30'})\n\n    self.assertEqual(form.is_valid(), False)\n", "def test_widget_subwidgets(self):\n    ###############################################################\n    # Widget subwidgets\n    ###############################################################\n\n    # Test that the subwidgets method of a widget yields subwidgets\n    class SubWidget(TextInput):\n            super().__init__(attrs)\n            self.choice = ChoiceWidget(choices=[(1, 'one'), (2, 'two')])\n\n            yield from super().subwidgets(name, value, attrs)\n            yield from self.choice.subwidgets(name, value, attrs)\n\n    sw = SubWidget()\n    subwidgets = list(sw.subwidgets('subwidget', 1))\n    self.assertEqual(len(subwidgets), 1)\n    self.assertIsInstance(subwidgets[0], TextInput)\n    self.assertEqual(subwidgets[0]['name'], 'subwidget')\n    self.assertEqual(subwidgets[0]['value'], '1')\n\n    # Test that the subwidgets method of a ChoiceWidget yields its subwidgets\n    cw = ChoiceWidget(choices=[(1, 'one'), (2, 'two')])\n    subwidgets = list(cw.subwidgets('choice', [1, 2]))\n    self.assertEqual(len(subwidgets), 2)\n    self.assertIsInstance(subwidgets[0], ChoiceWidget)\n    self.assertEqual(subwidgets[0]['name'], 'choice')\n    self.assertEqual(subwidgets[0]['value'], [1, 2])\n\n    # Test that the subwidgets method of a MultiWidget yields its subwidgets\n    mw = MultiWidget([TextInput, TextInput])\n    subwidgets = list(mw.subwidgets('multi', [1, 2]))\n    self.assertEqual(len(subwidgets), 2)\n    self.assertIsInstance(subwidgets[0], TextInput)\n    self.assertEqual(subwidgets[0]['name'], 'multi_0')\n    self.assertEqual(subwidgets[0]['value'], '1')\n    self.assertIsInstance(subwidgets[1], TextInput)\n    self.assertEqual(subwidgets[1]['name'], 'multi_1')\n    self.assertEqual(subwidgets[1]['value'], '2')\n\n    # Test that the subwidgets method of a SelectDateWidget yields its subwidgets\n    sdw = SelectDateWidget()\n    subwidgets = list(sdw.subwidgets('selectdate', datetime.date(2022, 1, 1)))\n    self.assertEqual(len(subwidgets), 3)\n    self.assertIsInstance(subwidgets[0], Select)\n    self.assertEqual(subwidgets[0]['name'], 'selectdate_year')\n", "def test_media_property_caching(self):\n    ###############################################################\n    # Media properties are cached on the class\n    ###############################################################\n\n    class MyWidget(TextInput):\n            return Media(css={'all': ('/some/path',)}, js=('/some/js',))\n        media = property(_media)\n\n    # The first time media is accessed, it's computed and cached.\n    w1 = MyWidget()\n    self.assertEqual(str(w1.media), \"\"\"<link href=\"/some/path\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">"], "sample_987": ["def test_evalf_acos():\n    # Test that acos is correctly handled in evalf\n    from sympy import acos\n    assert NS(acos(1), 15) == '0.000000000000000'\n    assert NS(acos(-1), 15) == '3.14159265358979'\n    assert NS(acos(0.5), 15) == '1.04719755119660'\n    assert NS(acos(-0.5), 15) == '2.09439510239320'\n    assert NS(acos(2), 15) == '0.000000000000000 + 1.31695789692482*I'\n", "def test_issue_10825_evalf():\n    assert NS(\"I**-100000\", 15) == '1.00000000000000'\n    assert NS(\"I**100000\", 15) == '1.00000000000000'\n    assert NS(\"(-I)**-100000\", 15) == '1.00000000000000'\n    assert NS(\"(-I)**100000\", 15) == '1.00000000000000'\n", "def test_evalf_add_combinations():\n    from sympy import symbols\n    x, y = symbols('x y')\n    # Test evalf of Add with different combinations of real and complex arguments\n    assert NS(x + y, subs={x: 1, y: 2}) == '3.00000000000000'\n    assert NS(x + y, subs={x: 1 + 2*I, y: 2}) == '3.00000000000000 + 2.00000000000000*I'\n    assert NS(x + y, subs={x: 1, y: 2 + 3*I}) == '3.00000000000000 + 3.00000000000000*I'\n    assert NS(x + y, subs={x: 1 + 2*I, y: 2 + 3*I}) == '3.00000000000000 + 5.00000000000000*I'\n    # Test evalf of Add with a combination of real and complex arguments and a float\n    assert NS(x + y + 2.5, subs={x: 1, y: 2}) == '5.50000000000000'\n    assert NS(x + y + 2.5, subs={x: 1 + 2*I, y: 2}) == '5.50000000000000 + 2.00000000000000*I'\n    assert NS(x + y + 2.5, subs={x: 1, y: 2 + 3*I}) == '5.50000000000000 + 3.00000000000000*I'\n    assert NS(x + y + 2.5, subs={x: 1 + 2*I, y: 2 + 3*I}) == '5.50000000000000 + 5.00000000000000*I'\n", "def test_evalf_piecewise_chop():\n    # see issue 10638\n    expr = Piecewise((1 + 10**-13*I, Eq(x, 1)), (x, True))\n    assert NS(expr.subs(x, 1), chop=True) == '1.00000000000000'\n", "def test_evalf_accuracy_independence():\n    # This test checks that the accuracy of the result is independent\n    # of the order of the terms\n    from sympy import symbols, sin, cos\n    x, y = symbols('x y')\n    expr = (sin(x) + sin(y) + cos(x) + cos(y))**100\n    for _ in range(100):\n        args = expr.as_coeff_Mul()[1].args\n        args = list(args)\n        # Randomly shuffle the arguments\n        import random\n        random.shuffle(args)\n        # Check that the numerical result is the same\n        assert NS(expr.func(*args)) == NS(expr)\n"], "sample_542": ["def test_get_window_extent_rotated():\n    fig, ax = plt.subplots()\n    text = fig.text(0.5, 0.5, \"test\", rotation=45)\n    renderer = fig.canvas.get_renderer()\n    extent = text.get_window_extent(renderer)\n    assert extent.height > extent.width\n    text.set_rotation(0)\n    extent = text.get_window_extent(renderer)\n    assert extent.width > extent.height\n", "def test_update_bbox_position_size():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, \"Test\", bbox=dict(facecolor='red'))\n    fig.canvas.draw()\n    bbox = text.get_window_extent(fig.canvas.renderer)\n    assert bbox.height > 0\n\n    text.set_position((0.5, 0.6))\n    fig.canvas.draw()\n    new_bbox = text.get_window_extent(fig.canvas.renderer)\n    assert new_bbox.height == bbox.height\n    assert new_bbox.y0 > bbox.y0\n", "def test_bbox_transforms():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'test')\n    text.set_bbox(dict(boxstyle='round, pad=0.1'))\n    text.set_transform(ax.transAxes)\n    fig.canvas.draw()\n    assert text._bbox_patch.get_window_extent().transformed(fig.dpi_scale_trans.inverted()).bounds == (0.0, 0.0, 1.0, 1.0)\n", "def test_get_wrap_line_width():\n    fig, ax = plt.subplots()\n    renderer = fig.canvas.get_renderer()\n\n    text = fig.text(0.5, 0.5, \"Test text\", ha='left')\n    ax.set_xlim([0, 1])\n    ax.set_ylim([0, 1])\n    line_width = text._get_wrap_line_width()\n\n    assert line_width > 0\n\n    ax.set_xlim([0.1, 0.2])\n    ax.set_ylim([0, 1])\n    line_width = text._get_wrap_line_width()\n\n    assert line_width > 0\n\n    ax.set_xlim([0.5, 0.6])\n    ax.set_ylim([0, 1])\n    line_width = text._get_wrap_line_width()\n\n    assert line_width > 0\n", "def test_annotation_with_bbox():\n    # Test that the bounding box of an annotation is correctly calculated when\n    # the annotation has a bbox patch.\n    fig, ax = plt.subplots()\n    ann = ax.annotate(\n        \"Annotation with bbox\", xy=(0.5, 0.5), xytext=(0.5, 0.5),\n        textcoords=\"data\", xycoords=\"data\", bbox=dict(facecolor=\"red\"))\n    fig.canvas.draw()\n    ann_bbox = ann.get_window_extent()\n    text_bbox = ann.get_window_extent(fig.canvas.get_renderer(), bbox=None)\n    arrow_bbox = ann.arrow_patch.get_window_extent(fig.canvas.get_renderer())\n    assert np.allclose(ann_bbox.get_points(), \n                       Bbox.union([text_bbox, arrow_bbox]).get_points())\n"], "sample_334": ["def test_order_fields_with_custom_order(self):\n    class MyForm(Form):\n        field1 = CharField()\n        field2 = CharField()\n        field3 = CharField()\n        field4 = CharField()\n        field5 = CharField()\n\n    form = MyForm(field_order=['field3', 'field2', 'field1'])\n    expected_order = ['field3', 'field2', 'field1', 'field4', 'field5']\n    self.assertEqual(list(form.fields), expected_order)\n", "def test_boundfield_iterable_context(self):\n    class MyForm(Form):\n        name = ChoiceField(choices=[('a', 'A'), ('b', 'B')])\n\n    form = MyForm(auto_id=False)\n    boundfield = form['name']\n    context = Context({'boundfield': boundfield})\n    template = Template('{% for subwidget in boundfield %}{{ subwidget.choice_value }}{% endfor %}')\n    self.assertEqual(template.render(context), 'ab')\n", "    def test_order_fields(self):\n        class TestForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n            field3 = CharField()\n\n        form = TestForm(field_order=['field1', 'field2', 'field3'])\n        self.assertEqual(list(form.fields), ['field1', 'field2', 'field3'])\n\n        form = TestForm(field_order=['field3', 'field1', 'field2'])\n        self.assertEqual(list(form.fields), ['field3', 'field1', 'field2'])\n\n        form = TestForm(field_order=['field1'])\n        self.assertEqual(list(form.fields), ['field1', 'field2', 'field3'])\n\n        form = TestForm(field_order=[])\n        self.assertEqual(list(form.fields), ['field1', 'field2', 'field3'])\n\n        class TestFormWithFieldOrderAttribute(Form):\n            field_order = ['field3', 'field1', 'field2']\n            field1 = CharField()\n            field2 = CharField()\n            field3 = CharField()\n\n        form = TestFormWithFieldOrderAttribute()\n        self.assertEqual(list(form.fields), ['field3', 'field1', 'field2'])\n\n        form = TestFormWithFieldOrderAttribute(field_order=['field1', 'field2', 'field3'])\n        self.assertEqual(list(form.fields), ['field1', 'field2', 'field3'])\n\n        class TestFormWithFieldOrderAttributeNone(Form):\n            field_order = None\n            field1 = CharField()\n            field2 = CharField()\n            field3 = CharField()\n\n        form = TestFormWithFieldOrderAttributeNone()\n        self.assertEqual(list(form.fields), ['field1', 'field2', 'field3'])\n\n        form = TestFormWithFieldOrderAttributeNone(field_order=['field1', 'field2', 'field3'])\n        self.assertEqual(list(form.fields), ['field1', 'field2', 'field3'])\n", "def test_added_error_field_key(self):\n    class UserForm(Form):\n        username = CharField(max_length=10)\n        password = CharField(widget=PasswordInput)\n\n            if (self.cleaned_data.get('password1') and self.cleaned_data.get('password2') and\n                    self.cleaned_data['password1'] != self.cleaned_data['password2']):\n                self.add_error('nonexistentfield', 'Passwords do not match')\n\n            return self.cleaned_data\n\n    f = UserForm({'username': 'adrian', 'password': 'foo'})\n    self.assertEqual(f.errors['nonexistentfield'], ['Passwords do not match'])\n", "def test_cached_property_as_expected_result(self):\n    class FormWithCachedProperty(Form):\n        @cached_property\n            return 'result'\n\n    form = FormWithCachedProperty()\n    self.assertEqual(form.my_property, 'result')\n    # Call it again to check it's cached\n    self.assertEqual(form.my_property, 'result')\n    # Add an attribute with the same name to check it doesn't interfere\n    form.my_property = 'other result'\n    self.assertEqual(form.my_property, 'other result')\n"], "sample_835": ["def test_adaboost_feature_importances():\n    # Test that feature importances are correctly calculated.\n    X, y = datasets.make_classification(n_samples=2000, n_features=10,\n                                        n_informative=3, n_redundant=0,\n                                        n_repeated=0, shuffle=False,\n                                        random_state=1)\n\n    for alg in ['SAMME', 'SAMME.R']:\n        clf = AdaBoostClassifier(algorithm=alg, random_state=1)\n        clf.fit(X, y)\n        importances = clf.feature_importances_\n\n        assert importances.shape[0] == 10\n        assert (importances[:3, np.newaxis] >= importances[3:]).all()\n\n    reg = AdaBoostRegressor(random_state=1)\n    reg.fit(X, y)\n    importances = reg.feature_importances_\n\n    assert importances.shape[0] == 10\n    assert (importances[:3, np.newaxis] >= importances[3:]).all()\n", "def test_adaboost_feature_importances():\n    # Check feature importances.\n    X, y = datasets.make_classification(n_samples=2000,\n                                        n_features=10,\n                                        n_informative=5,\n                                        n_redundant=0,\n                                        n_repeated=0,\n                                        shuffle=False,\n                                        random_state=1)\n\n    for alg in ['SAMME', 'SAMME.R']:\n        clf = AdaBoostClassifier(algorithm=alg, random_state=1)\n\n        clf.fit(X, y)\n        importances = clf.feature_importances_\n\n        assert importances.shape[0] == 10\n        assert np.all(importances[:5] > importances[5:])\n\n    # AdaBoost regression\n    X, y = datasets.make_regression(n_samples=2000,\n                                    n_features=10,\n                                    n_informative=5,\n                                    random_state=1)\n\n    clf = AdaBoostRegressor(random_state=1)\n    clf.fit(X, y)\n    importances = clf.feature_importances_\n\n    assert importances.shape[0] == 10\n    assert np.all(importances[:5] > importances[5:])\n", "def test_adaboost_error_on_empty_estimator_list():\n    \"\"\"\n    Test that AdaBoostClassifier and AdaBoostRegressor raise an error when\n    predict or predict_proba is called before fit.\n    \"\"\"\n    boost_clf = AdaBoostClassifier()\n    boost_reg = AdaBoostRegressor()\n\n    assert_raises(ValueError, boost_clf.predict, X)\n    assert_raises(ValueError, boost_clf.predict_proba, X)\n    assert_raises(ValueError, boost_clf.decision_function, X)\n    assert_raises(ValueError, boost_clf.score, X, y_class)\n    assert_raises(ValueError, boost_reg.predict, X)\n    assert_raises(ValueError, boost_reg.score, X, y_regr)\n", "def test_adaboost_n_estimators():\n    # Check that AdaBoost estimators raise an error when n_estimators is too small\n\n    # AdaBoost classification\n    clf = AdaBoostClassifier(n_estimators=0)\n    assert_raises(ValueError, clf.fit, X, y_class)\n\n    clf = AdaBoostClassifier(n_estimators=1)\n    assert_raises(ValueError, clf.fit, X, y_class)\n\n    # AdaBoost regression\n    reg = AdaBoostRegressor(n_estimators=0)\n    assert_raises(ValueError, reg.fit, X, y_regr)\n\n    reg = AdaBoostRegressor(n_estimators=1)\n    assert_raises(ValueError, reg.fit, X, y_regr)\n\n    # Check that a single estimator still raises an error when sample weights\n    # are provided\n    clf = AdaBoostClassifier(n_estimators=1)\n    assert_raises(ValueError, clf.fit, X, y_class, sample_weight=[1, 2, 3, 4, 5, 6])\n\n    reg = AdaBoostRegressor(n_estimators=1)\n    assert_raises(ValueError, reg.fit, X, y_regr, sample_weight=[1, 2, 3, 4, 5, 6])\n", "def test_adaboost_finite_weight_update():\n    # Check that AdaBoost updates weights with finite values.\n    # Non-regression test for #18566\n\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n    w = np.array([1, 1e-10])\n\n    clf = AdaBoostClassifier(n_estimators=1, random_state=0)\n    clf.fit(X, y, sample_weight=w)\n\n    assert np.all(np.isfinite(clf.estimator_weights_))\n    assert np.all(np.isfinite(clf.estimator_errors_))\n"], "sample_305": ["def test_exact_lookup_with_none_rhs(self):\n    # Regression test for #22558: Exact lookup with None as rhs should not\n    # raise ValueError\n    self.assertEqual(\n        Book.objects.filter(name__exact=None).count(),\n        0\n    )\n", "def test_multiple_annotations_with_same_name(self):\n    \"\"\"Regression test for #29399: Using multiple annotations with the same name should raise a ValueError.\"\"\"\n    msg = \"The annotation 'avg' conflicts with a field on the model.\"\n    with self.assertRaisesMessage(ValueError, msg):\n        Book.objects.annotate(avg=Avg('authors__age'), avg=Avg('price'))\n", "def test_lookup_transforms(self):\n    # Test lookup transforms on annotations\n    book = Book.objects.annotate(\n        rating_rounded=Round(Func(F('rating'), function='round', template=\"%(function)s(%(expressions)s, 0)\"),\n                             output_field=IntegerField())\n    ).get(pk=self.b1.pk)\n    self.assertEqual(book.rating_rounded, 4)\n\n    # Test transforms with multiple fields\n    book = Book.objects.annotate(\n        rating_price=Func(F('rating'), F('price'), function='my_custom_func',\n                          output_field=DecimalField())\n    ).get(pk=self.b1.pk)\n    self.assertEqual(book.rating_price, 135.0)\n", "def test_lookup_input_validation(self):\n    # Test that lookup input is properly validated.\n    with self.assertRaises(TypeError):\n        Book.objects.filter(name__exact=123)\n    with self.assertRaises(TypeError):\n        Book.objects.filter(name__iexact=123)\n    with self.assertRaises(ValueError):\n        Book.objects.filter(name__exact=None)\n    with self.assertRaises(ValueError):\n        Book.objects.filter(name__iexact=None)\n\n    # Test that the lookups with invalid inputs don't fail when they're not\n    # actually used in the query.\n    qs = Book.objects.filter(Q(name__exact=123) | Q(name='Python Web Development with Django'))\n    self.assertQuerysetEqual(qs, ['Python Web Development with Django'], attrgetter('name'))\n\n    qs = Book.objects.filter(Q(name__iexact=123) | Q(name='Python Web Development with Django'))\n    self.assertQuerysetEqual(qs, ['Python Web Development with Django'], attrgetter('name'))\n\n    qs = Book.objects.filter(Q(name__exact=None) | Q(name='Python Web Development with Django'))\n    self.assertQuerysetEqual(qs, ['Python Web Development with Django'], attrgetter('name'))\n\n    qs = Book.objects.filter(Q(name__iexact=None) | Q(name='Python Web Development with Django'))\n    self.assertQuerysetEqual(qs, ['Python Web Development with Django'], attrgetter('name'))\n", "    def test_annotation_on_reverse_one_to_one(self):\n        \"\"\"\n        Annotations are allowed on reverse one-to-one fields.\n        \"\"\"\n        class AuthorWithOneToOne(models.Model):\n            name = models.CharField(max_length=255)\n            bio = models.OneToOneField('AuthorBio', on_delete=models.CASCADE)\n\n        class AuthorBio(models.Model):\n            bio = models.TextField()\n\n        with self.settings(INSTALLED_APPS=self.settings.INSTALLED_APPS + ['tests.aggregation_regress']):\n            with models.schema.SchemaChecker() as schema_checker:\n                self.assertFieldOutput(AuthorWithOneToOne, 'bio', ['authorbio_ptr', 'bio'])\n                self.assertFieldOutput(AuthorWithOneToOne, 'bio__bio', ['bio'])\n"], "sample_964": ["def test_pydecorator_method_signature(app, status, warning):\n    text = (\".. py:class:: MyClass\\n\"\n            \"\\n\"\n            \"   .. py:method:: my_method\\n\"\n            \"   .. py:decoratormethod:: my_method\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, (\"class\", desc_sig_space)],\n                                                    [desc_name, \"MyClass\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc,\n                                                  addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'my_method() (MyClass method)', 'MyClass.my_method', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_addname, \"@\"],\n                                                     [desc_name, \"my_method\"])],\n                                   [desc_content, ()]))\n    assert_node(doctree[1][1][2], addnodes.index,\n                entries=[('single', 'my_method() (MyClass method)', 'MyClass.my_method', '', None)])\n    assert_node(doctree[1][1][3], ([desc_signature, ([desc_addname, \"@\"],\n                                                     [desc_name, \"my_method\"])],\n                                   [desc_content, ()]))\n    assert 'MyClass.my_method' in app.env.domains['py'].objects\n    assert app.env.domains['py'].objects['MyClass.my_method'] == ('index', 'MyClass.my_method', 'method', False)\n", "def test_pythonmodule_index_entry(app):\n    text = (\".. py:module:: sphinx\\n\"\n            \".. py:module:: sphinx.config\\n\"\n            \".. py:module:: sphinx.builders\\n\"\n            \".. py:module:: sphinx.builders.html\\n\")\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('s', [IndexEntry('sphinx', 1, '', '', '', '', ''),\n                IndexEntry('sphinx.builders', 2, '', '', '', '', ''),  # NOQA\n                IndexEntry('sphinx.builders.html', 2, '', '', '', '', ''),  # NOQA\n                IndexEntry('sphinx.config', 2, '', '', '', '', '')])],\n        False\n    )\n", "def test_python_domain_note_object(app):\n    domain = app.env.get_domain('py')\n    domain.note_object('obj1', 'function', 'id1')\n    domain.note_object('obj1', 'function', 'id1', aliased=True)\n    domain.note_object('obj2', 'class', 'id2')\n    domain.note_object('obj2', 'class', 'id3', aliased=True)\n\n    assert len(domain.objects) == 2\n    assert domain.objects['obj1'] == ('', 'id1', 'function', False)\n    assert domain.objects['obj2'] == ('', 'id2', 'class', False)\n", "def test_domain_py_find_obj_attribute(app):\n    \"\"\"Test domain objects find_obj method with attribute objects\"\"\"\n        return app.env.domains['py'].find_obj(\n            app.env, modname, prefix, obj_name, obj_type, searchmode)\n\n    app.builder.build_all()\n\n    assert (find_obj(None, None, 'NONEXISTANT', 'attr') == [])\n    assert (find_obj(None, None, 'NestedParentA.NestedChildA.subchild_1', 'attr') == [])\n    assert (find_obj(None, 'NestedParentA.NestedChildA', 'subchild_1', 'attr') == [])\n\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\")\n    restructuredtext.parse(app, text)\n\n    assert (find_obj(None, None, 'Class.attr', 'attr') ==\n            [('Class.attr', ('index', 'Class.attr', 'attribute', False))])\n    assert (find_obj(None, 'Class', 'attr', 'attr') ==\n            [('Class.attr', ('index', 'Class.attr', 'attribute', False))])\n    assert (find_obj(None, None, 'attr', 'attr') == [])\n    assert (find_obj(None, 'Class', 'attr', 'attr', 1) ==\n            [('Class.attr', ('index', 'Class.attr', 'attribute', False))])\n\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"   .. py:attribute:: Class.attr\")\n    restructuredtext.parse(app, text)\n\n    assert (find_obj(None, None, 'Class.attr', 'attr') ==\n            [('Class.attr', ('index', 'Class.attr', 'attribute', False))])\n    assert (find_obj(None, 'Class', 'attr', 'attr') ==\n            [('Class.attr', ('index', 'Class.attr', 'attribute', False))])\n    assert (find_obj(None, None, 'attr', 'attr') == [])\n    assert (find_obj(None, 'Class', 'attr', 'attr', 1) ==\n            [('Class.attr', ('index', 'Class.attr', 'attribute', False))])\n", "def test_domain_py_xref_prefixes(app):\n    \"\"\"Test that py:xref directives respect prefix directives\"\"\"\n    text = (\".. py:class:: Class\\n\"\n            \".. py:module:: module\\n\"\n            \".. py:currentmodule:: module\\n\"\n            \"\\n\"\n            \"   .. py:method:: method\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"   .. py:property:: prop\\n\"\n            \"\\n\"\n            \"   :mod:`module`\\n\"\n            \"   :class:`module.Class`\\n\"\n            \"   :meth:`method`\\n\"\n            \"   :attr:`attr`\\n\"\n            \"   :prop:`prop`\\n\"\n            \"\\n\"\n            \".. py:currentmodule:: None\\n\"\n            \"\\n\"\n            \"   :class:`Class`\\n\"\n            \"   :mod:`module`\\n\"\n            \"   :meth:`module.Class.method`\\n\"\n            \"   :attr:`module.Class.attr`\\n\"\n            \"   :prop:`module.Class.prop`\")\n    doctree = restructuredtext.parse(app, text)\n\n    domain = app.env.get_domain('py')\n    assert 'module.Class' in domain.objects\n    assert 'module.Class.method' in domain.objects\n    assert 'module.Class.attr' in domain.objects\n    assert 'module.Class.prop' in domain.objects\n\n    # Test module prefix\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, (\"class\", desc_sig_space)],\n                                                    [desc_addname, \"module.\"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc,\n                                                  addnodes.index,\n                                                  desc,\n                                                  addnodes.index,\n                                                  desc,\n                                                  addnodes.index,\n                                                  paragraph,\n                                                  paragraph,\n                                                  paragraph,\n                                                  paragraph,\n                                                  paragraph,\n                                                  paragraph,\n                                                  paragraph,\n                                                  paragraph,\n                                                  paragraph)])]))\n\n    # Test class prefix\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'method() (module.Class method)', 'module.Class.method', '', None)])\n    assert_node(doctree[1][1][1], addnodes.index,\n                entries=[('single', 'attr (module.Class attribute)', 'module.Class.attr', '', None)])\n    assert_node(doctree[1][1][2], addnodes.index,\n                entries=[('single', 'prop"], "sample_774": ["def test_ordinal_encoder_dtypes_pandas():\n    # check dtype (similar to test_categorical_encoder_dtypes for dataframes)\n    pd = pytest.importorskip('pandas')\n\n    enc = OrdinalEncoder()\n    exp = np.array([[0, 1], [1, 0]], dtype='float64')\n\n    X = pd.DataFrame({'A': [1, 2], 'B': [3, 4]}, dtype='int64')\n    enc.fit(X)\n    assert all([enc.categories_[i].dtype == 'int64' for i in range(2)])\n    assert_array_equal(enc.transform(X), exp)\n\n    X = pd.DataFrame({'A': [1, 2], 'B': ['a', 'b']})\n    enc.fit(X)\n    assert all([enc.categories_[i].dtype == 'object' for i in range(2)])\n    assert_array_equal(enc.transform(X), exp)\n", "def test_ordinal_encoder_categories_dtype_mismatch():\n    # Test OrdinalEncoder with categories that have different dtypes.\n    X = [['a', 'b'], ['c', 'd']]\n    enc = OrdinalEncoder(categories=[['a', 'b', 'c'], [1, 2, 3]])\n    msg = \"The categories should not mix strings and numeric values\"\n    with pytest.raises(ValueError, match=msg):\n        enc.fit_transform(X)\n", "def test_ordinal_encoder_inverse_transform(sparse_, drop, categories):\n    X = [['cat1', 'a'], ['cat2', 'b']]\n    enc = OrdinalEncoder(categories=categories, dtype=float)\n    X_tr = enc.fit_transform(X)\n    exp = np.array(X, dtype=object)\n    assert_array_equal(enc.inverse_transform(X_tr), exp)\n\n    if categories is not None:\n        # test inverse transform with categories specified\n        enc = OrdinalEncoder(categories=categories, dtype=float)\n        X_tr = enc.fit_transform(X)\n        exp = np.array(X, dtype=object)\n        assert_array_equal(enc.inverse_transform(X_tr), exp)\n", "def test_ordinal_encoder_handle_unknown():\n    X = [['a', 'b', 'c'], ['b', 'a', 'c']]\n    X2 = [['d', 'b', 'c']]\n\n    enc = OrdinalEncoder(handle_unknown='error')\n    enc.fit(X)\n    assert_raises(ValueError, enc.transform, X2)\n\n    enc = OrdinalEncoder(handle_unknown='use_encoded_value')\n    enc.fit(X)\n    assert_raises(ValueError, enc.transform, X2)\n\n    enc = OrdinalEncoder(handle_unknown='ignore')\n    enc.fit(X)\n    X2_tr = enc.transform(X2)\n    assert_array_equal(X2_tr, [[2., 0., 0.]])\n\n    enc = OrdinalEncoder(handle_unknown='ignore')\n    enc.fit(X)\n    assert_array_equal(enc.inverse_transform(X2_tr), [['None', 'b', 'c']])\n", "def test_ordinal_encoder_feature_range():\n    enc = OrdinalEncoder()\n    X = np.array([[2, 1], [0, 2]])\n    with pytest.raises_regex(\n            ValueError,\n            'Shape mismatch: if categories is an array, it has to be of shape '\n            '\\(n_features,\\).'):\n        enc.fit(X, categories=[[1, 2]])\n"], "sample_946": ["def test_type_to_xref(app):\n    doctree = PythonDomain.type_to_xref(\"int\", app.env)\n    assert_node(doctree, ([pending_xref, \"int\"],))\n    assert_node(doctree[0], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"int\")\n\n    doctree = PythonDomain.type_to_xref(\"None\", app.env)\n    assert_node(doctree, ([pending_xref, \"None\"],))\n    assert_node(doctree[0], pending_xref, refdomain=\"py\", reftype=\"obj\", reftarget=\"None\")\n\n    doctree = PythonDomain.type_to_xref(\"List[int]\", app.env)\n    assert_node(doctree, ([pending_xref, \"List\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \"]\"]))\n\n    doctree = PythonDomain.type_to_xref(\"List[int]\", env=None)\n    assert_node(doctree, ([pending_xref, \"List\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \"]\"]))\n\n    doctree = PythonDomain.type_to_xref(\"int\", app.env, False)\n    assert_node(doctree, ([pending_xref, \"int\"],))\n    assert_node(doctree[0], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"int\")\n\n    doctree = PythonDomain.type_to_xref(\"List[int]\", app.env, False)\n    assert_node(doctree, ([pending_xref, \"List\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \"]\"]))\n\n    doctree = PythonDomain.type_to_xref(\"List[int]\", env=None, False)\n    assert_node(doctree, ([pending_xref, \"List\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \"]\"]))\n\n    with pytest.raises(RemovedInSphinx50Warning):\n        PythonDomain.type_to_xref(\"int\", env=None, False)\n", "def test_python_domain_get_full_qualified_name():\n    env = Mock(domaindata={})\n    domain = PythonDomain(env)\n\n    # case: node without py:module and py:class\n    node = nodes.reference(reftarget='func')\n    assert domain.get_full_qualified_name(node) == 'func'\n\n    # case: node with py:module\n    kwargs = {'py:module': 'module1'}\n    node = nodes.reference(reftarget='func', **kwargs)\n    assert domain.get_full_qualified_name(node) == 'module1.func'\n\n    # case: node with py:class\n    kwargs = {'py:class': 'Class'}\n    node = nodes.reference(reftarget='func', **kwargs)\n    assert domain.get_full_qualified_name(node) == 'Class.func'\n\n    # case: node with both py:module and py:class\n    kwargs = {'py:module': 'module1', 'py:class': 'Class'}\n    node = nodes.reference(reftarget='func', **kwargs)\n    assert domain.get_full_qualified_name(node) == 'module1.Class.func'\n\n    # case: node without reftarget\n    node = nodes.reference()\n    assert domain.get_full_qualified_name(node) is None\n\n    # case: node without py:module and py:class and reftarget contains dots\n    node = nodes.reference(reftarget='module1.Class.func')\n    assert domain.get_full_qualified_name(node) == 'module1.Class.func'\n\n    # case: node with py:module and reftarget contains dots\n    kwargs = {'py:module': 'module1'}\n    node = nodes.reference(reftarget='Class.func', **kwargs)\n    assert domain.get_full_qualified_name(node) == 'module1.Class.func'\n\n    # case: node with py:class and reftarget contains dots\n    kwargs = {'py:class': 'Class'}\n    node = nodes.reference(reftarget='module1.func', **kwargs)\n    assert domain.get_full_qualified_name(node) == 'module1.Class.func'\n\n    # case: node with both py:module and py:class and reftarget contains dots\n    kwargs = {'py:module': 'module1', 'py:class': 'Class'}\n    node = nodes.reference(reftarget='func', **kwargs)\n    assert domain.get_full_qualified_name(node) == 'module1.Class.func'\n", "def test_pymodule(app):\n    text = (\".. py:module:: example\\n\"\n            \"   :platform: win32\\n\"\n            \"   :synopsis: A test module\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (nodes.target,\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"module \"],\n                                                    [desc_name, \"example\"])],\n                                  [desc_content, ()])]))\n    assert_node(doctree[2], addnodes.index,\n                entries=[('pair', 'module; example', 'module-example', '', 'win32', 'A test module')])\n    assert 'example' in app.env.domains['py'].modules\n    assert app.env.domains['py'].modules['example'] == ('index', 'module-example', 'A test module', 'win32', False)\n\n    text = (\".. py:module:: example\\n\"\n            \"   :deprecated:\\n\")\n    restructuredtext.parse(app, text)\n    assert 'example' in app.env.domains['py'].modules\n    assert app.env.domains['py'].modules['example'] == ('index', 'module-example', '', '', True)\n\n    text = (\".. py:module:: example\\n\"\n            \"   :synopsis: A test module\\n\"\n            \"   :platform: win32\\n\"\n            \"   :deprecated:\\n\")\n    restructuredtext.parse(app, text)\n    assert 'example' in app.env.domains['py'].modules\n    assert app.env.domains['py'].modules['example'] == ('index', 'module-example', 'A test module', 'win32', True)\n", "def test_parse_annotation_multi_line(app):\n    text = \"\"\".. py:function:: test() -> List[\n        Union[\n            Tuple[str, int],\n            Dict[str, int]\n        ]\n    ]\"\"\"\n    doctree = _parse_annotation(text[11:], app.env)\n    assert_node(doctree, ([pending_xref, \"List\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"Union\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"Tuple\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"str\"],\n                          [desc_sig_punctuation, \", \"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \", \"],\n                          [pending_xref, \"Dict\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"str\"],\n                          [desc_sig_punctuation, \", \"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"]\"]))\n    assert_node(doctree[0], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"List\")\n    assert_node(doctree[2], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"Union\")\n    assert_node(doctree[4], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"Tuple\")\n    assert_node(doctree[5], desc_sig_punctuation, \"\", \"[\")\n    assert_node(doctree[6], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"str\")\n    assert_node(doctree[7], desc_sig_punctuation, \"\", \", \")\n    assert_node(doctree[8], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"int\")\n    assert_node(doctree[9], desc_sig_punctuation, \"\", \"]\")\n    assert_node(doctree[10], desc_sig_punctuation, \"\", \", \")\n    assert_node(doctree[11], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"Dict\")\n    assert_node(doctree[12], desc_sig_punctuation, \"\", \"[\")\n    assert_node(doctree[13], pending_xref, ref", "def test_python_domain_merge_domaindata(app):\n    # Create an environment with two documents\n    text1 = (\".. py:class:: Class1\\n\"\n             \".. py:class:: Class2\\n\")\n    text2 = (\".. py:class:: Class3\\n\"\n             \".. py:class:: Class4\\n\")\n\n    restructuredtext.parse(app, text1, 'doc1')\n    restructuredtext.parse(app, text2, 'doc2')\n\n    domain = app.env.get_domain('py')\n    domaindata = domain.get_domaindata()\n\n    # Merge data from 'doc2' into the current environment\n    domain.merge_domaindata(['doc2'], domaindata)\n\n    # Test that objects from 'doc2' have been added\n    assert 'Class3' in domain.objects\n    assert 'Class4' in domain.objects\n\n    # Test that objects from 'doc1' are still present\n    assert 'Class1' in domain.objects\n    assert 'Class2' in domain.objects\n\n    # Test that the number of objects is correct\n    assert len(domain.objects) == 4\n"], "sample_962": ["def test_undecorate():\n    with mock(['unknown']):\n        import unknown\n        class Class:\n            pass\n        decorated_class = unknown.secret.Class()\n        decorated_class.__sphinx_decorator_args__ = (Class,)\n        assert undecorate(decorated_class) == Class\n        undecorated_class = unknown.secret.Class\n        assert undecorate(undecorated_class) == undecorated_class\n", "def test_undecorate():\n    with mock(['unknown']):\n        import unknown\n        assert undecorate(unknown.secret.Class) == unknown.secret.Class\n        assert undecorate(unknown.secret.Class()) == unknown.secret.Class()\n\n    class Decorator:\n            self.func = func\n\n            return self.func(*args, **kwargs)\n\n    @Decorator\n        pass\n\n    assert undecorate(my_function) == my_function\n    assert undecorate(my_function()) == my_function()\n", "def test_ismock():\n    assert not ismock(int)\n    assert not ismock(str)\n    assert not ismock(1)\n    assert not ismock('a')\n\n    with mock(['mocked_module']):\n        import mocked_module\n        assert ismock(mocked_module)\n\n        class MyClass:\n            pass\n\n        assert not ismock(MyClass)\n        assert not ismock(MyClass())\n\n    class MyMockedObject:\n        __sphinx_mock__ = True\n\n    assert ismock(MyMockedObject())\n\n    class MyMockedModule(ModuleType):\n        __sphinx_mock__ = True\n\n    assert ismock(MyMockedModule('name'))\n", "def test_undecorate():\n    with mock(['unknown']):\n        import unknown\n        decorated = unknown.secret.Class()\n        assert undecorate(decorated) == decorated\n\n        decorated = unknown.secret.Class(unknown.secret.Class())\n        assert undecorate(decorated) == unknown.secret.Class()\n\n    class MyClass:\n        pass\n\n    obj = MyClass()\n    assert undecorate(obj) == obj\n\n    class MockedClass:\n        __sphinx_mock__ = True\n        __sphinx_decorator_args__ = (MyClass(),)\n\n    mocked = MockedClass()\n    assert undecorate(mocked) == MyClass()\n", "def test_mock():\n    with mock(['unknown.mock']):\n        import unknown.mock\n        assert ismock(unknown.mock) is True\n        assert ismock(unknown.mock.secret) is True\n        assert ismock(unknown.mock.secret.Class) is True\n\n    assert ismock(unknown.mock) is False\n"], "sample_1013": ["def test_lambdify_kwargs():\n    f = lambdify((x, y), x + y)\n    assert f(x=1, y=2) == 3\n    raises(TypeError, lambda: f(x=1, y=2, z=3))\n", "def test_lambdify_vectorized_functions_with_tuples():\n    if not numpy:\n        skip(\"numpy not installed.\")\n\n    f = lambdify((x, y), (sin(x), cos(y)), 'numpy')\n    a, b = numpy.random.randn(2, 10)\n    fa, fb = f(a, b)\n    assert fa.shape == (10,)\n    assert fb.shape == (10,)\n    assert numpy.allclose(fa, numpy.sin(a))\n    assert numpy.allclose(fb, numpy.cos(b))\n", "def test_lambdify_special_function():\n    if not numexpr:\n        skip(\"numexpr not installed.\")\n    if not numpy:\n        skip(\"numpy not installed.\")\n    from sympy import sign\n    expr = sign(x)\n    f_numexpr = lambdify(x, expr, modules='numexpr')\n    f_numpy = lambdify(x, expr, modules='numpy')\n    arr = numpy.array([-1.5, -0.5, 0, 0.5, 1.5])\n    numpy.testing.assert_array_equal(f_numexpr(arr), f_numpy(arr))\n    numpy.testing.assert_array_equal(f_numexpr(arr), numpy.sign(arr))\n", "def test_issue_15603():\n    # Test for issue 15603: lambdify does not handle functions with variable\n    # number of arguments\n    from sympy.functions.elementary.piecewise import ExprCondPair\n    f = lambdify(x, Piecewise(ExprCondPair(sin(x), x < 0), (cos(x), True)), modules='sympy')\n    assert f(1) == cos(1)\n    assert f(-1) == sin(-1)\n", "def test_issue_17659():\n    #Test for issue 17659.  test that lambdify works with a non-ASCII symbol.\n    from sympy import symbols\n    lambda_ = symbols('\u03bb')\n    f = lambdify(lambda_, lambda_ ** 2)\n    assert f(3) == 9\n"], "sample_459": ["    def test_lookup_exact(self):\n        instance = IntegerModel.objects.create(value=1)\n        self.assertEqual(IntegerModel.objects.get(value__exact=1), instance)\n        with self.assertRaises(IntegerModel.DoesNotExist):\n            IntegerModel.objects.get(value__exact=2)\n", "    def test_lookup_identity(self):\n        lookup1 = Exact('field', 'value')\n        lookup2 = Exact('field', 'value')\n        self.assertEqual(lookup1, lookup2)\n        self.assertEqual(hash(lookup1), hash(lookup2))\n\n        lookup3 = Exact('field', 'different_value')\n        self.assertNotEqual(lookup1, lookup3)\n        self.assertNotEqual(hash(lookup1), hash(lookup3))\n", "    def test_Lookup(self):\n        from django.db.models import CharField\n        lookup = Lookup(lhs=CharField(), rhs='value')\n        self.assertEqual(lookup.lookup_name, None)\n        self.assertEqual(lookup.prepare_rhs, True)\n        self.assertEqual(lookup.can_use_none_as_rhs, False)\n", "    def test_lookup_class(self):\n        lookup = Lookup(Value(1), Value(2))\n        self.assertEqual(lookup.__class__, Lookup)\n        self.assertEqual(lookup.identity, (Lookup, Value(1), Value(2)))\n        self.assertEqual(hash(lookup), hash(make_hashable(lookup.identity)))\n", "    def test_lookup_name(self):\n        class TestLookup(Lookup):\n            lookup_name = \"test_lookup\"\n\n        lookup = TestLookup(\"field\", \"value\")\n        self.assertEqual(lookup.lookup_name, \"test_lookup\")\n"], "sample_527": ["def test_subplots_adjust(kwargs, expected):\n    fig = plt.figure()\n    fig.subplots_adjust(**kwargs)\n    subplotpars = fig.subplotpars\n    for key, value in expected.items():\n        assert getattr(subplotpars, key) == value\n", "def test_subfigure_add_subfigure():\n    fig = plt.figure()\n    subfig = fig.add_subfigure((1, 1), 1, 1)\n    with pytest.warns(UserWarning, match=\"The subfigure method is deprecated\"):\n        new_subfig = subfig.subfigure((1, 1), 1, 1)\n    assert isinstance(new_subfig, FigureBase)\n    assert new_subfig.get_figure() is subfig\n", "def test_subfigure_draw():\n    fig = plt.figure()\n    sfig = fig.add_subfigure((0.1, 0.1, 0.5, 0.5))\n    ax = sfig.subplots()\n    ax.plot([1, 2, 3])\n    renderer = fig._get_renderer()\n    fig.draw(renderer)\n    # Check that the subfigure's bounding box has been correctly set.\n    assert sfig.bbox_relative == Bbox.from_bounds(0.1, 0.1, 0.5, 0.5)\n", "def test_figure_subfigure():\n    fig = plt.figure()\n    sf = fig.add_subfigure([0.1, 0.1, 0.4, 0.4])\n    assert sf.bbox_inches.p0 == (0.1, 0.1)\n    assert sf.bbox_inches.p1 == (0.5, 0.5)\n    ax1 = sf.add_subplot()\n    ax1.plot([1, 2, 3])\n    assert ax1 in sf.axes\n    assert ax1 not in fig.axes\n\n    sf2 = fig.add_subfigure([0.6, 0.6, 0.4, 0.4])\n    ax2 = sf2.add_subplot()\n    ax2.plot([1, 2, 3])\n    assert ax2 in fig.axes\n    assert ax2 in sf2.axes\n    assert len(fig.axes) == 2\n    assert len(sf.axes) == 1\n    assert len(sf2.axes) == 1\n", "def test_add_subplot_properties():\n    fig, ax = plt.subplots()\n    assert ax._projection_init == (Axes, {'sharex': None, 'sharey': None, 'label': None})\n    ax2 = fig.add_subplot(1, 1, 1, projection='polar')\n    assert ax2._projection_init == (projections.get_projection_class('polar'), {'sharex': None, 'sharey': None, 'label': None})\n    ax3 = fig.add_subplot(1, 1, 1, projection='custom', sharex=ax2, sharey=ax)\n    assert ax3._projection_init == (projections.get_projection_class('custom'), {'sharex': ax2, 'sharey': ax, 'label': None})\n    ax4 = fig.add_subplot(1, 1, 1, axes_class=Axes, sharex=ax2, sharey=ax)\n    assert ax4._projection_init == (Axes, {'sharex': ax2, 'sharey': ax, 'label': None})\n    ax5 = fig.add_subplot(1, 1, 1, axes_class=Axes, polar=True, sharex=ax2, sharey=ax)\n    assert ax5._projection_init == (projections.get_projection_class('polar'), {'sharex': ax2, 'sharey': ax, 'label': None})\n"], "sample_786": ["def test_transform_with_sparse_input():\n    X = sp.csr_matrix(X)\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal')\n    Xt = est.fit_transform(X)\n    assert_array_equal(Xt, est.transform(X.toarray()))\n", "def test_invalid_input():\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal')\n    X_invalid = np.array([[1, 2], [3, 4], [5, 6], [7, 8]], dtype=object)\n    X_invalid[0, 0] = 'a'\n    assert_raise_message(ValueError, \"Input X contains non-numeric data.\",\n                       est.fit, X_invalid)\n", "def test_transform_nans(strategy):\n    X = np.array([[1, 2, np.nan, 4],\n                  [5, np.nan, 7, 8],\n                  [np.nan, 10, 11, 12],\n                  [13, 14, 15, np.nan]])\n    est = KBinsDiscretizer(n_bins=3, strategy=strategy, encode='ordinal')\n    Xt = est.fit_transform(X)\n    assert_array_equal(Xt.shape, X.shape)\n    assert_array_equal(np.isnan(X), np.isnan(Xt))\n", "def test_inverse_transform_constant_features(strategy):\n    X = np.random.RandomState(0).randn(100, 3)\n    X[:, 0] = 1  # constant feature\n    X[:, 2] = 2  # constant feature\n    kbd = KBinsDiscretizer(n_bins=3, strategy=strategy, encode='ordinal')\n    Xt = kbd.fit_transform(X)\n    X2 = kbd.inverse_transform(Xt)\n    assert_array_equal(X[:, [1]], X2[:, [1]])\n    assert_array_equal(Xt.max(axis=0) + 1, kbd.n_bins_)\n    assert_array_equal(X2[:, 0], 0)\n    assert_array_equal(X2[:, 2], 0)\n", "def test_inverse_transform_edge_cases(strategy):\n    X = np.array([[-2, -1, 0, 1], [-1, 0, 1, 2], [0, 1, 2, 3], [1, 2, 3, 4]])\n    kbd = KBinsDiscretizer(n_bins=3, strategy=strategy, encode='ordinal')\n    Xt = kbd.fit_transform(X)\n    X2 = kbd.inverse_transform(Xt)\n    X2t = kbd.fit_transform(X2)\n    assert_array_equal(Xt, X2t)\n\n    # Test with min and max values\n    X = np.array([[-2, -2, -2, -2], [-1, -1, -1, -1], [0, 0, 0, 0], [1, 1, 1, 1]])\n    Xt = kbd.fit_transform(X)\n    X2 = kbd.inverse_transform(Xt)\n    X2t = kbd.fit_transform(X2)\n    assert_array_equal(Xt, X2t)\n\n    # Test with values just below and above min and max\n    X = np.array([[-2.1, -1.1, 0.1, 1.1], [-2, -1, 0, 1], [-1.9, -0.9, 0.9, 1.9], [-2, -1, 0, 1]])\n    Xt = kbd.fit_transform(X)\n    X2 = kbd.inverse_transform(Xt)\n    X2t = kbd.fit_transform(X2)\n    assert_array_equal(Xt, X2t)\n"], "sample_387": ["    def test_render_readonly_filefield(self):\n        self.client.force_login(self.superuser)\n        response = self.client.get(\n            reverse(\"admin:admin_widgets_album_change\", args=(1,))\n        )\n        self.assertContains(\n            response,\n            '<div class=\"readonly\"><a href=\"%(STORAGE_URL)salbums/cover_art.jpg\">'\n            r\"albums\\cover_art.jpg</a></div>\"\n            % {\"STORAGE_URL\": default_storage.url(\"\")},\n            html=True,\n        )\n        self.assertNotContains(\n            response,\n            '<input type=\"file\" name=\"cover_art\" id=\"id_cover_art\">',\n            html=True,\n        )\n", "    def test_get_fieldsets(self):\n        class BandAdmin(admin.ModelAdmin):\n            fieldsets = [\n                (None, {\"fields\": [\"name\"]}),\n                (\"Advanced\", {\"fields\": [\"style\"]}),\n            ]\n\n        band_admin = BandAdmin(Band, admin.site)\n        self.assertEqual(\n            band_admin.get_fieldsets(request=None, obj=None),\n            band_admin.fieldsets,\n        )\n", "    def test_base_model_admin_inheritance(self):\n        class MyModel(models.Model):\n            field1 = models.CharField(max_length=10)\n            field2 = models.CharField(max_length=10)\n\n        class BaseAdmin(BaseModelAdmin):\n            pass\n\n        class ModelAdmin(BaseAdmin):\n            pass\n\n        base_admin = BaseAdmin(MyModel, admin.site)\n        model_admin = ModelAdmin(MyModel, admin.site)\n        self.assertEqual(base_admin.get_fieldsets(None), [(None, {\"fields\": [\"field1\", \"field2\"]})])\n        self.assertEqual(model_admin.get_fieldsets(None), [(None, {\"fields\": [\"field1\", \"field2\"]})])\n", "    def test_view_on_site_url(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            view_on_site = True\n\n        ma = MyModelAdmin(Member, admin.site)\n        obj = Member(name=\"John Doe\", birthdate=datetime(1980, 1, 1))\n        self.assertIsNone(ma.get_view_on_site_url(obj=None))\n        self.assertIsNotNone(ma.get_view_on_site_url(obj=obj))\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n        cls.band = Band.objects.create(name=\"Bogey Blues\")\n"], "sample_669": ["def test_capturing_bytes_in_non_utf8_encoding(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import sys\n            sys.stdout.encoding = 'iso-8859-1'\n            print('b\\\\u00f6y')\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--capture=fd\")\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n", "def test_capture_and_interactive_debugger(testdir):\n    testdir.makeconftest(\n        \"\"\"\n            import pdb\n            pdb.set_trace()\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n            print(\"hello\")\n    \"\"\"\n    )\n    result = testdir.runpytest_subprocess(\"-s\")\n    result.stdout.fnmatch_lines([\"*(Pdb)*\", \"*hello*\"])\n", "    def test_capturing_stdin(self, testdir):\n        p = testdir.makepyfile(\n            \"\"\"\n                import sys\n                sys.stdin.write(\"hello\")\n                assert 0\n            \"\"\"\n        )\n        result = testdir.runpytest_subprocess(p)\n        result.stdout.fnmatch_lines(\n            [\n                \"*test_func*\",\n                \"*assert 0*\",\n                \"*Captured stderr*\",\n                \"*IOError: reading from stdin while output is captured!\",\n            ]\n        )\n", "def test_capture_of_sys_stdout_flush(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import sys\n\n            sys.stdout.write(\"foo\")\n            sys.stdout.flush()\n            assert 0\n    \"\"\"\n    )\n    result = testdir.runpytest(str(p))\n    result.stdout.fnmatch_lines([\"*test_capture*\", \"*foo*\", \"*1 failed*\"])\n", "def test_encode_error_behavior(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n            import sys\n            sys.stdout.buffer.write(b'\\xff')\n            sys.stderr.buffer.write(b'\\xff')\n            out, err = capsysbinary.readouterr()\n        \"\"\"\n    )\n    result = testdir.runpytest(str(p))\n    result.stdout.no_fnmatch_line(\"*UnicodeDecodeError*\")\n"], "sample_27": ["def test_fitsdiff_hdu_name_and_ver(tmp_path):\n    \"\"\"Make sure diff report reports HDU name and ver if same in files\"\"\"\n    path1 = tmp_path / \"test1.fits\"\n    path2 = tmp_path / \"test2.fits\"\n\n    hdulist = HDUList([PrimaryHDU(), ImageHDU(data=np.zeros(5), name=\"SCI\", ver=2)])\n    hdulist.writeto(path1)\n    hdulist[1].data[0] = 1\n    hdulist.writeto(path2)\n\n    diff = FITSDiff(path1, path2)\n    assert \"Extension HDU 1 (SCI, 2):\" in diff.report()\n", "def test_fitsdiff_hdu_name_with_version(tmp_path):\n    \"\"\"Make sure diff report reports HDU name and ver if same in files\"\"\"\n    path1 = tmp_path / \"test1.fits\"\n    path2 = tmp_path / \"test2.fits\"\n\n    hdulist = HDUList([PrimaryHDU(), ImageHDU(data=np.zeros(5), name=\"SCI\", ver=2)])\n    hdulist.writeto(path1)\n    hdulist[1].data[0] = 1\n    hdulist.writeto(path2)\n\n    diff = FITSDiff(path1, path2)\n    assert \"Extension HDU 1 (SCI, 2):\" in diff.report()\n", "def test_fitsdiff_hdu_extension_types(tmp_path):\n    \"\"\"Make sure diff report correctly reports HDU extension types\"\"\"\n    path1 = tmp_path / \"test1.fits\"\n    path2 = tmp_path / \"test2.fits\"\n\n    hdulist = HDUList([PrimaryHDU(), ImageHDU(data=np.zeros(5), name=\"SCI\")])\n    hdulist.writeto(path1)\n    hdulist = HDUList([PrimaryHDU(), BinTableHDU(data=np.zeros(5), name=\"SCI\")])\n    hdulist.writeto(path2)\n\n    diff = FITSDiff(path1, path2)\n    report = diff.report()\n    assert \"Extension types differ:\" in report\n    assert \"  a: IMAGE\" in report\n    assert \"  b: BINTABLE\" in report\n", "def test_fitsdiff_report_to_string(tmp_path):\n    \"\"\"Make sure FITSDiff report can be generated as a string\"\"\"\n    path1 = tmp_path / \"test1.fits\"\n    path2 = tmp_path / \"test2.fits\"\n\n    hdulist = HDUList([PrimaryHDU(), ImageHDU(data=np.zeros(5))])\n    hdulist.writeto(path1)\n    hdulist[1].data[0] = 1\n    hdulist.writeto(path2)\n\n    diff = FITSDiff(path1, path2)\n    report_string = diff.report(fileobj=None)\n    assert isinstance(report_string, str)\n    assert \"Extension HDU 1:\" in report_string\n", "def test_fitsdiff_ignore_hdu_patterns(tmp_path):\n    \"\"\"Test FITSDiff ignores HDUs with matching names when requested\"\"\"\n    path1 = tmp_path / \"test1.fits\"\n    path2 = tmp_path / \"test2.fits\"\n\n    hdulist = HDUList(\n        [\n            PrimaryHDU(),\n            ImageHDU(data=np.zeros(5), name=\"SCI\"),\n            ImageHDU(data=np.zeros(5), name=\"SCI\"),\n        ]\n    )\n    hdulist.writeto(path1)\n    hdulist[1].data[0] = 1\n    hdulist.writeto(path2)\n\n    diff = FITSDiff(path1, path2, ignore_hdus=[\"SCI\"])\n    assert diff.identical\n\n    diff = FITSDiff(path1, path2, ignore_hdus=[\"SCI*\"])\n    assert diff.identical\n\n    diff = FITSDiff(path1, path2)\n    assert not diff.identical\n"], "sample_673": ["    def test_repr_failure(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n                '''\n                >>> 1 + 1\n                3\n                '''\n                pass\n            \"\"\"\n        )\n        p = testdir.makepyfile(\n            \"\"\"\n                '''\n                >>> 1 + 1\n                3\n                '''\n                pass\n        \"\"\"\n        )\n        reprec = testdir.inline_run(p, \"--doctest-modules\")\n        reprec.assertoutcome(failed=1)\n\n        excinfo = reprec.getcalls(\"pytest_runtest_logreport\")[0].item._excinfo\n        item = reprec.getcalls(\"pytest_runtest_logreport\")[0].item\n        failure_repr = item.repr_failure(excinfo)\n        assert failure_repr is not None\n        assert isinstance(failure_repr, ReprFailDoctest)\n        assert len(failure_repr.reprlocation_lines) == 1\n", "def test_doctest_linedata_on_property_lambda(testdir):\n    \"\"\"\n    Test that a property defined with a lambda function can still be tested.\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        class Sample(object):\n            some_property = property(lambda self: 'something', doc=\"\"\"\n                >>> Sample().some_property\n                'something'\n            \"\"\")\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--doctest-modules\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*= FAILURES =*\",\n            \"*_ [[]doctest[]] test_doctest_linedata_on_property_lambda.Sample.some_property _*\",\n            \"EXAMPLE LOCATION UNKNOWN, not showing all tests of that example\",\n            \"[?][?][?] >>> Sample().some_property\",\n            \"Expected:\",\n            \"    'something'\",\n            \"Got:\",\n            \"    'something'\",\n            \"\",\n            \"*/test_doctest_linedata_on_property_lambda.py:None: DocTestFailure\",\n            \"*= 1 failed in *\",\n        ]\n    )\n", "def test_doctest_unicode_encoding_error(testdir):\n    \"\"\"Test that doctests raise a UnicodeDecodeError if they contain non-ASCII\n    characters and the doctest_encoding is set to an encoding that can't\n    handle those characters.\n    \"\"\"\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        doctest_encoding = ascii\n    \"\"\"\n    )\n    testdir.maketxtfile(\n        test_doctest_unicode_encoding_error=\"\"\"\n        >>> print('\u00e5\u00e9\u00ee\u00f8\u00fc')\n        \u00e5\u00e9\u00ee\u00f8\u00fc\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*UnicodeDecodeError*\", \"*1 failed*\"])\n", "    def test_doctest_runner_report_failure(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            '''\n            >>> i = 5\n            >>> i + 1\n            7\n            '''\n        \"\"\"\n        )\n        reprec = testdir.inline_run(\"--doctest-modules\")\n        reprec.assertoutcome(failed=1)\n", "    def test_doctest_unicode_reporting(self, testdir):\n        \"\"\"Test that doctests with unicode characters in the expected output report correctly.\"\"\"\n        testdir.makepyfile(\n            \"\"\"\n                '''\n                >>> '\u00f6\u00e4\u00fc'\n                '\u00f6\u00e4\u00fc'\n                >>> '\u00f6\u00e4\u00fc'.encode('utf-8')\n                b'\u00f6\u00e4\u00fc'\n                >>> b'\u00f6\u00e4\u00fc'.decode('utf-8')\n                '\u00f6\u00e4\u00fc'\n                '''\n        \"\"\"\n        )\n        result = testdir.runpytest(\"--doctest-modules\")\n        result.stdout.fnmatch_lines([\"* 3 passed *\"])\n"], "sample_710": ["def test_setup_method_with_coroutine(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n        import pytest\n\n        class MyTestCase(unittest.IsolatedAsyncioTestCase):\n            async def setup_method(self):\n                pass\n\n            async def test_method1(self):\n                pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n", "def test_unittest_subclassing_setupclass_teardownclass(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n        class MyTestCase(unittest.TestCase):\n            @classmethod\n                cls.x = 1\n\n                assert self.x == 1\n\n            @classmethod\n                cls.x = 2\n\n        class MySubTestCase(MyTestCase):\n            @classmethod\n                super().setUpClass()\n                cls.y = 1\n\n                assert self.x == 1\n                assert self.y == 1\n\n            @classmethod\n                cls.y = 2\n                super().tearDownClass()\n        \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    reprec.assertoutcome(passed=2)\n", "def test_runTest_with_unittest_skip_issue1559(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n            @unittest.skip(\"skip message\")\n                pass\n    \"\"\"\n    )\n    result = pytester.runpytest(\"-rs\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*SKIP*[1]*skip message*\",\n            \"*1 skipped*\",\n        ]\n    )\n", "def test_prunetraceback_for_unittest_teardowns(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class Test(unittest.TestCase):\n                pass\n\n                raise Exception(\"teardown error\")\n\n                assert 1 == 1\n    \"\"\"\n    )\n    result = pytester.runpytest(\"-v\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_hello*\",\n            \"*tearDown*\",\n            \"*teardown error*\",\n            \"*ERROR at teardown of Test.test_hello*\",\n        ]\n    )\n", "def test_setup_class_fixture_order(pytester: Pytester) -> None:\n    \"\"\"Check that setup class fixtures are called in the correct order (#3887).\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n        import pytest\n\n        class MyTestCase(unittest.TestCase):\n            x = 0\n\n            @classmethod\n                cls.x += 1\n                assert cls.x == 2  # setup fixture should run before setUpClass\n\n                assert self.x == 2\n\n                assert self.x == 2\n    \"\"\"\n    )\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(scope=\"class\", autouse=True)\n            MyTestCase.x += 1\n    \"\"\"\n    )\n    reprec = pytester.inline_run()\n    reprec.assertoutcome(passed=2)\n"], "sample_834": ["def test_fit_transform():\n    \"\"\"Test fit_transform method.\"\"\"\n    X = iris_data\n    y = iris_target\n    nca = NeighborhoodComponentsAnalysis(max_iter=30)\n    X_transformed = nca.fit_transform(X, y)\n    assert_array_equal(X_transformed, nca.transform(X))\n", "def test_components_as_attributes():\n    nca = NeighborhoodComponentsAnalysis()\n    X, y = make_classification(n_samples=30, n_features=5, n_classes=4,\n                               n_redundant=0, n_informative=5, random_state=0)\n    nca.fit(X, y)\n    assert hasattr(nca, 'components_')\n    assert hasattr(nca, 'n_iter_')\n", "def test_transform_with_2d_array_input():\n    \"\"\"Test transform method with a 2D array as input\"\"\"\n    X = iris_data\n    y = iris_target\n\n    nca = NeighborhoodComponentsAnalysis(max_iter=30)\n    nca.fit(X, y)\n\n    # test with a 2D array\n    X_new = np.array([[1.0, 2.0], [3.0, 4.0]])\n    X_new_transformed = nca.transform(X_new)\n    assert X_new_transformed.shape == (2, nca.components_.shape[0])\n\n    # test with a 2D array with multiple columns but one row\n    X_new = np.array([[1.0, 2.0]])\n    X_new_transformed = nca.transform(X_new)\n    assert X_new_transformed.shape == (1, nca.components_.shape[0])\n", "def test_nca_clone():\n    # Test that cloning NeighborhoodComponentsAnalysis produces a fully independent\n    # object\n    rng = np.random.RandomState(42)\n    X, y = make_classification(n_samples=30, n_features=5, n_classes=4,\n                               n_redundant=0, n_informative=5, random_state=0)\n    nca = NeighborhoodComponentsAnalysis(warm_start=True, max_iter=5)\n    nca.fit(X, y)\n    transformation = nca.components_\n\n    nca_clone = clone(nca)\n    nca_clone.fit(X, y)\n\n    assert_array_almost_equal(transformation, nca.components_)\n    assert nca_clone.components_ is not nca.components_\n\n    # Check that changing one does not change the other\n    nca.max_iter = 10\n    nca.fit(X, y)\n    assert not np.array_equal(transformation, nca.components_)\n    assert_array_almost_equal(nca_clone.components_, transformation)\n", "def test_transform_input_validation():\n    \"\"\"Test that transform input validation works correctly.\n\n    Test that the transform method correctly raises an error when the input\n    data is not valid (i.e., not a numpy array, not 2D, not float type).\n    \"\"\"\n    nca = NeighborhoodComponentsAnalysis()\n    nca.components_ = np.arange(12).reshape(4, 3)\n\n    # Fail if input data is not a numpy array\n    assert_raise_message(ValueError,\n                         'Input must be a numpy array.',\n                         nca.transform, np.array([]))\n\n    # Fail if input data is not 2D\n    assert_raise_message(ValueError,\n                         'Reshape your data either using array.reshape(-1, 1) '\n                         'if your data has a single feature or '\n                         'array.reshape(1, -1) if it contains a single '\n                         'sample.',\n                         nca.transform, np.array([1, 2, 3]))\n\n    # Fail if input data is not float type\n    assert_raise_message(ValueError,\n                         'Input must be a float array.',\n                         nca.transform, np.array([[1, 2], [3, 4]], dtype=int))\n\n    # Fail if input data has a different number of features than the\n    # components\n    X = np.arange(8).reshape(2, 4)\n    assert_raise_message(ValueError,\n                         'Input has {} features, but NeighborhoodComponentsAnalysis '\n                         'is expecting {} features as input.'\n                         .format(X.shape[1], nca.components_.shape[1]),\n                         nca.transform, X)\n\n    # Pass otherwise\n    X = np.arange(12).reshape(4, 3)\n    nca.transform(X)\n"], "sample_678": ["def test_parts(tmp_path):\n    \"\"\"Test the parts function.\"\"\"\n    # Create a path with multiple components\n    path = tmp_path / \"a\" / \"b\" / \"c\"\n    path.mkdir(parents=True)\n\n    expected_parts = {str(tmp_path), str(tmp_path / \"a\"), str(tmp_path / \"a\" / \"b\"), str(path)}\n    assert parts(str(path)) == expected_parts\n", "def test_cleanup_numbered_dir(tmp_path):\n    \"\"\"Ensure that cleanup_numbered_dir removes old directories.\"\"\"\n    root = tmp_path\n    prefix = \"test-\"\n    keep = 2\n    for i in range(5):\n        (root / (prefix + str(i))).mkdir()\n\n    cleanup_numbered_dir(root, prefix, keep, 0)\n\n    remaining_dirs = [d for d in root.iterdir() if d.name.startswith(prefix)]\n    assert len(remaining_dirs) == keep\n    assert [int(d.name[len(prefix):]) for d in remaining_dirs] == [3, 4]\n", "def test_cleanup_candidates(tmp_path):\n    \"\"\"Test cleanup_candidates returns the correct paths for cleanup\"\"\"\n    prefix = \"test-\"\n    keep = 3\n\n    # Create test directories\n    for i in range(10):\n        (tmp_path / f\"{prefix}{i}\").mkdir()\n\n    # Test cleanup_candidates\n    candidates = list(cleanup_candidates(tmp_path, prefix, keep))\n    assert len(candidates) == 7\n    for i, path in enumerate(candidates):\n        assert path.name == f\"{prefix}{i}\"\n\n    # Test cleanup_candidates with no deletable directories\n    for path in cleanup_candidates(tmp_path, prefix, 10):\n        try_cleanup(path, 0)\n    candidates = list(cleanup_candidates(tmp_path, prefix, keep))\n    assert len(candidates) == 0\n\n    # Test cleanup_candidates with a non-existent directory\n    with pytest.raises(FileNotFoundError):\n        cleanup_candidates(tmp_path / \"non-existent\", prefix, keep)\n", "def test_cleanup_lock_removal(tmp_path):\n    \"\"\"Ensure that the cleanup lock removal function works correctly.\"\"\"\n    lock_path = tmp_path / \"lock\"\n    lock_path.touch()\n    assert lock_path.is_file()\n\n        lock_path.unlink()\n\n    atexit.register(cleanup_on_exit)\n    atexit.unregister(cleanup_on_exit)\n\n    # Simulate a fork\n        pass\n\n    atexit.register(cleanup_on_exit_fork)\n    atexit.unregister(cleanup_on_exit_fork)\n\n    # Test that the cleanup_on_exit function still exists in the atexit registry\n    assert cleanup_on_exit in atexit._exithandlers\n    assert cleanup_on_exit_fork in atexit._exithandlers\n\n    # Test that the cleanup_on_exit function removes the lock file\n    atexit.register(cleanup_on_exit)\n    cleanup_on_exit()\n    assert not lock_path.is_file()\n\n    # Test that the cleanup_on_exit_fork function does not remove the lock file\n    lock_path.touch()\n    assert lock_path.is_file()\n    atexit.register(cleanup_on_exit_fork)\n    cleanup_on_exit_fork()\n    assert lock_path.is_file()\n", "def test_lock_timeout(tmp_path, monkeypatch):\n    \"\"\"Ensure that lock timeouts are respected.\"\"\"\n    path = tmp_path / \"temp-1\"\n    path.mkdir()\n    lock_path = get_lock_path(path)\n    lock_path.touch()\n\n    mtime = lock_path.stat().st_mtime\n\n    # Mock time to make the lock appear older than it really is.\n    with unittest.mock.patch(\"time.time\", return_value=mtime + LOCK_TIMEOUT + 1):\n        assert ensure_deletable(path, consider_lock_dead_if_created_before=mtime)\n\n    # Test that the lock was deleted\n    assert not lock_path.is_file()\n\n    # Test that the lock is not deleted if it's still within the timeout.\n    lock_path.touch()\n    mtime = lock_path.stat().st_mtime\n    with unittest.mock.patch(\"time.time\", return_value=mtime + LOCK_TIMEOUT / 2):\n        assert not ensure_deletable(path, consider_lock_dead_if_created_before=mtime)\n\n    # Test that the lock was not deleted\n    assert lock_path.is_file()\n"], "sample_635": ["def test_property_with_multiple_docstrings(self) -> None:\n    \"\"\"Example of a property having multiple docstrings\"\"\"\n    node = astroid.extract_node(\n        \"\"\"\n    class Foo(object):\n        @property\n            '''docstring ...\n\n            :type: int\n            '''\n            return 10\n\n        @foo.setter\n            '''setter docstring ...\n\n            :type: None\n            '''\n            pass\n\n        @foo.deleter\n            '''deleter docstring ...\n\n            :type: None\n            '''\n            pass\n    \"\"\"\n    )\n    with self.assertNoMessages():\n        self._visit_methods_of_class(node)\n", "def test_finds_missing_yield_type_in_generator(self) -> None:\n    \"\"\"Example of a generator missing yield type documentation in\n    the Google style docstring\n    \"\"\"\n    node = astroid.extract_node(\n        \"\"\"\n        \"\"\"docstring ...\n\n        Yields:\n            int\n        \"\"\"\n        yield 1\n    \"\"\"\n    )\n    with self.assertAddsMessages(\n        MessageTest(msg_id=\"missing-yield-type-doc\", node=node),\n    ):\n        self.checker.visit_functiondef(node)\n", "def test_property_returns_annotation(self) -> None:\n    \"\"\"Test case to test the annotation of a property return value\"\"\"\n    property_node, node = astroid.extract_node(\n        \"\"\"\n    class Foo(object):\n        @property\n            '''docstring ...\n\n            :raises RuntimeError: Always\n            '''\n            raise RuntimeError()\n            return 10 #@+\n        \"\"\"\n    )\n    with self.assertNoMessages():\n        self.checker.visit_return(node)\n", "    def test_docstringify_calls_necessary_subclass(self) -> None:\n        \"\"\"Check that the docstringify function correctly calls the necessary subclass.\"\"\"\n        docstring = \"\"\"\n        Args:\n            my_arg (int): My argument.\n        \"\"\"\n        subclass = docstringify(docstring, default_type=\"google\")\n        assert isinstance(subclass, GoogleDocstring)\n\n        docstring = \"\"\"\n        :param my_arg: My argument.\n        \"\"\"\n        subclass = docstringify(docstring, default_type=\"sphinx\")\n        assert isinstance(subclass, SphinxDocstring)\n\n        docstring = \"\"\"\n        Parameters\n        ----------\n        my_arg : int\n            My argument.\n        \"\"\"\n        subclass = docstringify(docstring, default_type=\"numpy\")\n        assert isinstance(subclass, NumpyDocstring)\n", "    def test_setter_property_name(self) -> None:\n        \"\"\"Test that we can correctly determine the property name for a setter.\"\"\"\n        node = astroid.extract_node(\n            \"\"\"\n        class Foo(object):\n            @property\n                return 1\n\n            @foo.setter\n                pass\n        \"\"\"\n        )\n        setter_node = node.body[1]\n        self.assertEqual(get_setters_property_name(setter_node), \"foo\")\n"], "sample_1156": ["def test_hyperbolic_function_derivatives():\n    x = Symbol('x')\n    assert sinh(x).diff(x) == cosh(x)\n    assert cosh(x).diff(x) == sinh(x)\n    assert tanh(x).diff(x) == 1 - tanh(x)**2\n    assert coth(x).diff(x) == -1/sinh(x)**2\n    assert csch(x).diff(x) == -coth(x)*csch(x)\n    assert sech(x).diff(x) == -tanh(x)*sech(x)\n    assert asinh(x).diff(x) == 1/sqrt(x**2 + 1)\n    assert acosh(x).diff(x) == 1/sqrt(x**2 - 1)\n    assert atanh(x).diff(x) == 1/(1 - x**2)\n    assert acoth(x).diff(x) == 1/(1 - x**2)\n    assert asech(x).diff(x) == -1/(x*sqrt(1 - x**2))\n    assert acsch(x).diff(x) == -1/(x**2*sqrt(1 + 1/x**2))\n", "def test_hyperbolic_limits():\n    x = Symbol('x')\n    assert cosh(x).limit(x, 0) == 1\n    assert tanh(x).limit(x, 0) == 0\n    assert coth(x).limit(x, 0) is zoo\n    assert csch(x).limit(x, 0) is zoo\n    assert sech(x).limit(x, 0) == 1\n", "def test_issue_12345():\n    from sympy import I\n    assert sinh(I).is_real is False\n    assert cosh(I).is_real is True\n    assert tanh(I).is_real is False\n    assert coth(I).is_real is False\n    assert sech(I).is_real is True\n    assert csch(I).is_real is False\n    assert asinh(I).is_real is False\n    assert acosh(I).is_real is False\n    assert atanh(I).is_real is False\n    assert acoth(I).is_real is False\n    assert asech(I).is_real is False\n    assert acsch(I).is_real is False\n", "def test_hyperbolic_as_leading_term():\n    x, y = symbols('x,y')\n\n    for func in [sinh, cosh, tanh]:\n        assert func(x + y).as_leading_term(x) == func(x)\n        assert func(x + y).as_leading_term(y) == func(y)\n    for func in [csch, sech, coth]:\n        assert func(x + y).as_leading_term(x) == func(x)\n        assert func(x + y).as_leading_term(y) == func(y)\n    for func in [asinh, acosh, atanh]:\n        assert func(x + y).as_leading_term(x) == func(x)\n        assert func(x + y).as_leading_term(y) == func(y)\n    for func in [asech, acsch, acoth]:\n        assert func(x + y).as_leading_term(x) == func(x)\n        assert func(x + y).as_leading_term(y) == func(y)\n", "def test_hyperbolic_functions_finite():\n    x = Symbol('x')\n    k = symbols('k', real=True, finite=True)\n    n = symbols('n', integer=True, finite=True)\n\n    assert sinh(n).is_finite is True\n    assert cosh(n).is_finite is True\n    assert tanh(n).is_finite is True\n    assert coth(n).is_finite is False\n    assert csch(n).is_finite is False\n    assert sech(n).is_finite is True\n\n    assert sinh(n + k).is_finite is True\n    assert cosh(n + k).is_finite is True\n    assert tanh(n + k).is_finite is True\n    assert coth(n + k).is_finite is False\n    assert csch(n + k).is_finite is False\n    assert sech(n + k).is_finite is True\n\n    assert sinh(n + k + I).is_finite is False\n    assert cosh(n + k + I).is_finite is False\n    assert tanh(n + k + I).is_finite is False\n    assert coth(n + k + I).is_finite is False\n    assert csch(n + k + I).is_finite is False\n    assert sech(n + k + I).is_finite is False\n\n    assert sinh(I).is_finite is False\n    assert cosh(I).is_finite is False\n    assert tanh(I).is_finite is False\n    assert coth(I).is_finite is False\n    assert csch(I).is_finite is False\n    assert sech(I).is_finite is False\n\n    assert sinh(n * I).is_finite is False\n    assert cosh(n * I).is_finite is False\n    assert tanh(n * I).is_finite is False\n    assert coth(n * I).is_finite is False\n    assert csch(n * I).is_finite is False\n    assert sech(n * I).is_finite is False\n\n    assert sinh(k * I).is_finite is False\n    assert cosh(k * I).is_finite is False\n    assert tanh(k * I).is_finite is False\n    assert coth(k * I).is_finite is False\n    assert csch(k * I).is"], "sample_741": ["def test_grid_search_error_on_empty_grid():\n    # Test that an error is raised when the grid is empty\n    X, y = make_classification(n_samples=200, n_features=100, random_state=0)\n\n    clf = LinearSVC()\n    cv = GridSearchCV(clf, param_grid={})\n    assert_raise_message(ValueError, \"Parameter values for parameter (C) need \"\n                         \"to be a non-empty sequence.\",\n                         cv.fit, X, y)\n", "def test_grid_search_masked_arrays():\n    # Test that grid search works with masked arrays\n    X = np.ma.array(np.arange(100).reshape(10, 10))\n    X[:, 2] = np.ma.masked\n    y = np.array([0] * 5 + [1] * 5)\n\n    clf = LinearSVC()\n    grid_search = GridSearchCV(clf, {'C': [0.1, 1.0]})\n    grid_search.fit(X, y)\n    assert_true(grid_search.best_score_ >= 0)\n    y_pred = grid_search.predict(X.filled())\n    assert_true(np.mean(y_pred == y) >= 0.9)\n\n    X_ = sp.csr_matrix(X.filled())\n    clf = LinearSVC()\n    grid_search = GridSearchCV(clf, {'C': [0.1, 1.0]})\n    grid_search.fit(X_, y)\n    y_pred2 = grid_search.predict(X_)\n    assert_array_equal(y_pred, y_pred2)\n    assert_true(np.mean(y_pred == y) >= 0.9)\n", "def test_grid_search_cv_results_degenerate_cv():\n    # Test GridSearchCV when the cv object is degenerate, e.g. when\n    # n_splits=1\n    X, y = make_classification(n_samples=10, random_state=42)\n\n    n_splits = 1\n    params = [dict(kernel=['rbf', ], C=[1, 10], gamma=[0.1, 1]),\n              dict(kernel=['poly', ], degree=[1, 2])]\n\n    param_keys = ('param_C', 'param_degree', 'param_gamma', 'param_kernel')\n    score_keys = ('mean_test_score', 'mean_train_score',\n                  'rank_test_score',\n                  'split0_test_score', 'split0_train_score',\n                  'std_test_score', 'std_train_score',\n                  'mean_fit_time', 'std_fit_time',\n                  'mean_score_time', 'std_score_time')\n    n_candidates = 6\n\n    search = GridSearchCV(SVC(gamma='scale'), cv=n_splits, param_grid=params)\n    search.fit(X, y)\n    cv_results = search.cv_results_\n    # Check if score and timing are reasonable\n    assert_true(all(cv_results['rank_test_score'] >= 1))\n    assert_true(all(cv_results[k] >= 0) for k in score_keys\n                if k is not 'rank_test_score')\n    assert_true(all(cv_results[k] <= 1) for k in score_keys\n                if 'time' not in k and\n                k is not 'rank_test_score')\n\n    # Check cv_results structure\n    check_cv_results_array_types(search, param_keys, score_keys)\n    check_cv_results_keys(cv_results, param_keys, score_keys, n_candidates)\n    check_cv_results_grid_scores_consistency(search)\n", "def test_grid_search_refit_attribute_access():\n    # Test attribute access on GridSearchCV with `refit` as an attribute name\n    clf = DecisionTreeClassifier(random_state=0)\n    grid_search = GridSearchCV(clf, {'max_depth': [1, 2, 3]},\n                               cv=2, refit='refit')\n    grid_search.fit(X, y)\n    assert_equal(grid_search.refit, 'refit')\n    assert_equal(grid_search.best_estimator_.refit, False)\n", "def test_grid_search_cv_parameter_iterator():\n    # Test the `param_grid` is regenerated on each `fit`\n    X, y = make_classification(n_samples=100, random_state=42)\n    param_grid = {'C': [1, 10], 'kernel': ['rbf', 'poly']}\n    grid_search = GridSearchCV(SVC(gamma='scale'), param_grid=param_grid, cv=2)\n\n    first_param_grid = grid_search._get_param_iterator()\n\n    # First check that all the param settings are generated correctly\n    expected_params = [{'C': 1, 'kernel': 'rbf'},\n                       {'C': 10, 'kernel': 'rbf'},\n                       {'C': 1, 'kernel': 'poly'},\n                       {'C': 10, 'kernel': 'poly'}]\n    assert_equal(list(first_param_grid), expected_params)\n\n    # Now call `fit` and check the `param_grid` is regenerated\n    grid_search.fit(X, y)\n    second_param_grid = grid_search._get_param_iterator()\n\n    # `second_param_grid` should be a new object but same contents\n    assert_not_equal(id(first_param_grid), id(second_param_grid))\n    assert_equal(list(second_param_grid), expected_params)\n\n    # Now check that the param_grid is re-generated for every `fit`\n    grid_search.fit(X, y)\n    third_param_grid = grid_search._get_param_iterator()\n\n    # `third_param_grid` should be a new object but same contents\n    assert_not_equal(id(second_param_grid), id(third_param_grid))\n    assert_equal(list(third_param_grid), expected_params)\n"], "sample_434": ["    def test_get_context_data(self):\n        class MyView(TemplateView):\n            extra_context = {\"foo\": \"bar\"}\n\n        view = MyView()\n        context = view.get_context_data(baz=\"qux\")\n        self.assertEqual(context, {\"view\": view, \"foo\": \"bar\", \"baz\": \"qux\"})\n", "    def test_get(self):\n        class TestTemplateView(TemplateView):\n            template_name = \"template.html\"\n\n        request_factory = RequestFactory()\n        request = request_factory.get(\"/\")\n\n        view = TestTemplateView()\n        view.setup(request)\n\n        response = view.get(request)\n        self.assertIsInstance(response, TemplateResponse)\n", "    def test_template_view_renders_template(self):\n        template_name = \"test_template.html\"\n        request_factory = RequestFactory()\n        request = request_factory.get(\"/\")\n        view = TemplateView.as_view(template_name=template_name)\n        response = view(request)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.template_name, [template_name])\n", "    def test_get_context_data(self):\n        class MyTemplateView(TemplateView):\n            extra_context = {\"foo\": \"bar\"}\n\n        request = RequestFactory().get(\"/\")\n        view = MyTemplateView()\n        view.setup(request)\n        context = view.get_context_data()\n        self.assertEqual(context, {\"foo\": \"bar\", \"view\": view})\n", "    def test_template_view_renders_template(self):\n        template_name = \"template.html\"\n        template_content = \"Hello, world!\"\n\n        # Mock the render_to_response method to check that it's called correctly.\n        with mock.patch.object(TemplateResponseMixin, \"render_to_response\") as render_mock:\n            render_mock.return_value = HttpResponse(template_content)\n\n            view = TemplateView()\n            view.template_name = template_name\n            view.request = RequestFactory().get(\"/\")\n\n            response = view.get(view.request)\n\n            self.assertIsInstance(response, HttpResponse)\n            self.assertEqual(response.content.decode(\"utf-8\"), template_content)\n            render_mock.assert_called_once()\n"], "sample_529": ["def test_legend_draggable():\n    fig, ax = plt.subplots()\n    ax.plot(np.arange(10), label='test')\n    leg = ax.legend(draggable=True)\n    assert leg.get_draggable()\n", "def test_set_bbox_to_anchor_invalid_input():\n    # test that set_bbox_to_anchor raises an exception for invalid input\n    fig, ax = plt.subplots()\n    ax.plot(range(10))\n    leg = ax.legend()\n    with pytest.raises(ValueError):\n        leg.set_bbox_to_anchor((1, 2, 3, 4, 5))\n", "def test_legend_framealpha_0():\n    # test framealpha=0.0\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label='foo')\n    ax.legend(framealpha=0.0)\n    ax.set_xlim([0.5, 3.5])\n    ax.set_ylim([0.5, 3.5])\n", "def testegend_custom_labelcolor():\n    fig, ax = plt.subplots()\n    ax.plot(np.arange(10), label='line1')\n    ax.plot(np.arange(10), label='line2')\n\n    # Define custom colors\n    colors = ['red', 'blue']\n\n    # Set the labelcolor to the custom colors\n    legend = ax.legend(labelcolor=colors)\n\n    # Check if the labelcolor is set correctly\n    for i, text in enumerate(legend.get_texts()):\n        assert text.get_color() == colors[i]\n", "def test_draggable_legend():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label='line')\n    legend = ax.legend(draggable=True)\n    assert legend.get_draggable()\n    fig.canvas.draw()\n    assert legend.stale\n    # Drag the legend to a new location\n    x, y = legend.get_window_extent().x0, legend.get_window_extent().y0\n    event = mpl.backend_bases.LocationEvent('button_press_event', fig.canvas, x, y)\n    legend._draggable.drag_start(event)\n    dx, dy = 10, 10\n    event = mpl.backend_bases.LocationEvent('motion_notify_event', fig.canvas, x + dx, y + dy)\n    legend._draggable.drag_update(event)\n    event = mpl.backend_bases.LocationEvent('button_release_event', fig.canvas, x + dx, y + dy)\n    legend._draggable.drag_stop(event)\n    assert legend.stale\n    fig.canvas.draw()\n    assert not legend.stale\n    assert legend.get_window_extent().x0 == x + dx\n"], "sample_1145": ["def test_refine_nested_abs():\n    assert refine(Abs(Abs(x)), Q.real(x)) == Abs(x)\n    assert refine(Abs(Abs(x)), Q.positive(x)) == x\n    assert refine(Abs(Abs(x)), Q.negative(x)) == -x\n    assert refine(Abs(Abs(x*y)), Q.positive(x)) == Abs(y)*x\n    assert refine(Abs(Abs(x*y)), Q.positive(y)) == Abs(x)*y\n    assert refine(Abs(Abs(x*y)), Q.negative(x)) == Abs(y)*(-x)\n    assert refine(Abs(Abs(x*y)), Q.negative(y)) == Abs(x)*(-y)\n", "def test_refine_commutative_args():\n    assert refine(Abs(x + y), Q.positive(x) & Q.real(y)) == x + Abs(y)\n    assert refine(Abs(x + y), Q.real(x) & Q.positive(y)) == Abs(x) + y\n    assert refine(Abs(x + y), Q.negative(x) & Q.real(y)) == -x + Abs(y)\n    assert refine(Abs(x + y), Q.real(x) & Q.negative(y)) == Abs(x) - y\n", "def test_refine_add_and_mul():\n    x = Symbol('x', real = True)\n    y = Symbol('y', real = True)\n    assert refine(x + Abs(y), Q.negative(y)) == x - y\n    assert refine(Abs(x) + y, Q.negative(x)) == -x + y\n    assert refine(Abs(x) + Abs(y), Q.negative(x) & Q.negative(y)) == -x - y\n    assert refine(x * Abs(y), Q.negative(y)) == x * -y\n    assert refine(Abs(x) * y, Q.negative(x)) == -x * y\n    assert refine(Abs(x) * Abs(y), Q.negative(x) & Q.negative(y)) == x * y\n\n    assert refine(x + Abs(y), Q.positive(y)) == x + y\n    assert refine(Abs(x) + y, Q.positive(x)) == x + y\n    assert refine(Abs(x) + Abs(y), Q.positive(x) & Q.positive(y)) == x + y\n    assert refine(x * Abs(y), Q.positive(y)) == x * y\n    assert refine(Abs(x) * y, Q.positive(x)) == x * y\n    assert refine(Abs(x) * Abs(y), Q.positive(x) & Q.positive(y)) == x * y\n", "def test_refine_with_more_boolean_expressions():\n    assert refine(Piecewise((1, Q.is_true(x < 0) & Q.positive(y)), (3, True)),\n                   Q.is_true(x < 0) & Q.positive(y)) == 1\n    assert refine(Piecewise((1, Q.is_true(x < 0) & Q.positive(y)), (3, True)),\n                   ~Q.is_true(x < 0) & Q.positive(y)) == 3\n    assert refine(Piecewise((1, Q.is_true(x < 0) & Q.positive(y)), (3, True)),\n                   Q.is_true(x < 0) & ~Q.positive(y)) == 3\n    assert refine(Piecewise((1, Q.is_true(x < 0) & Q.positive(y)), (3, True)),\n                   ~Q.is_true(x < 0) & ~Q.positive(y)) == 3\n    assert refine(Piecewise((1, Q.is_true(x < 0) | Q.positive(y)), (3, True)),\n                   Q.is_true(x < 0) | Q.positive(y)) == 1\n    assert refine(Piecewise((1, Q.is_true(x < 0) | Q.positive(y)), (3, True)),\n                   ~(Q.is_true(x < 0) | Q.positive(y))) == 3\n    assert refine(Piecewise((1, Q.is_true(x < 0) & Q.positive(y)), (3, True)),\n                   Q.is_true(x < 0)) == Piecewise((1, Q.positive(y)), (3, True))\n    assert refine(Piecewise((1, Q.is_true(x < 0) & Q.positive(y)), (3, True)),\n                   Q.positive(y)) == Piecewise((1, Q.is_true(x < 0)), (3, True))\n", "def test_refine_coverage():\n    # Test refinement with mixed assumptions\n    assert refine(Abs(x + I*y), Q.real(x) & Q.imaginary(y)) == Abs(x + I*y)\n    assert refine(re(x + I*y), Q.real(x) & Q.imaginary(y)) == x\n    assert refine(im(x + I*y), Q.real(x) & Q.imaginary(y)) == -y\n\n    # Test refinement with multiple variables\n    assert refine(Abs(x + y), Q.positive(x) & Q.negative(y)) == x + y\n    assert refine(Abs(x + y), Q.negative(x) & Q.positive(y)) == y - x\n    assert refine(Abs(x + y), Q.real(x) & Q.real(y)) == Abs(x + y)\n\n    # Test refinement with complex expressions\n    assert refine(Abs(x + y*z), Q.positive(x) & Q.real(y) & Q.real(z)) == x + y*z\n    assert refine(Abs(x + y*z), Q.negative(x) & Q.real(y) & Q.real(z)) == y*z - x\n    assert refine(Abs(x + y*z), Q.real(x) & Q.real(y) & Q.real(z)) == Abs(x + y*z)\n"], "sample_602": ["def test_open_dataset_with_file_like_object():\n    expected = xr.Dataset(\n        dict(a=2 * np.arange(5)), coords=dict(x=(\"x\", np.arange(5), dict(units=\"s\")))\n    )\n\n    with open(\"example.nc\", \"wb\") as f:\n        expected.to_netcdf(f)\n\n    with open(\"example.nc\", \"rb\") as f:\n        actual = xr.open_dataset(f)\n\n    assert_identical(expected, actual)\n\n    import os\n\n    os.remove(\"example.nc\")\n", "def test__validate_dataset_names():\n    dataset = xr.Dataset()\n    dataset.attrs[\"key\"] = \"\"\n\n    with pytest.raises(ValueError):\n        _validate_dataset_names(dataset)\n\n    dataset = xr.Dataset()\n    dataset.attrs[\"key\"] = 123\n\n    with pytest.raises(TypeError):\n        _validate_dataset_names(dataset)\n\n    dataset = xr.Dataset()\n    dataset[\"var\"] = xr.DataArray([1, 2, 3])\n\n    with pytest.raises(ValueError):\n        dataset[\"var\"].name = \"\"\n        _validate_dataset_names(dataset)\n\n    with pytest.raises(TypeError):\n        dataset[\"var\"].name = 123\n        _validate_dataset_names(dataset)\n", "def test_to_zarr_append():\n    dataset = xr.Dataset(dict(a=(\"x\", np.arange(5))))\n    store = \"test.zarr\"\n    dataset.to_zarr(store)\n    new_dataset = xr.Dataset(dict(a=(\"x\", np.arange(5, 10))))\n    new_dataset.to_zarr(store, append_dim=\"x\")\n    combined = xr.open_zarr(store)\n    expected = xr.Dataset(dict(a=(\"x\", np.arange(10))))\n    assert_identical(combined, expected)\n", "def test_open_dataset():\n    # Create a test dataset\n    expected = xr.Dataset(\n        dict(a=np.arange(5)), coords=dict(x=(\"x\", np.arange(5), dict(units=\"s\")))\n    )\n\n    # Save the dataset to a netCDF file\n    with xr.open_dataset(None, engine=\"scipy\") as ds:\n        ds = expected.copy(deep=True)\n        with open(\"test.nc\", \"wb\") as f:\n            ds.to_netcdf(f, engine=\"scipy\")\n\n    # Open the dataset from the netCDF file\n    actual = xr.open_dataset(\"test.nc\", engine=\"scipy\")\n\n    # Check if the opened dataset is identical to the expected dataset\n    assert_identical(expected, actual)\n\n    # Remove the test file\n    import os\n    os.remove(\"test.nc\")\n", "def test_open_dataset_with_data_vars():\n    data_vars = {\"a\": (\"x\", [1, 2, 3]), \"b\": (\"x\", [4, 5, 6])}\n    coords = {\"x\": [1, 2, 3]}\n    expected = xr.Dataset(data_vars, coords)\n\n    class CustomBackend(xr.backends.BackendEntrypoint):\n            filename_or_obj,\n            drop_variables=None,\n            data_vars=\"all\",\n            coords=\"different\",\n            **kwargs,\n        ):\n            if data_vars == \"all\":\n                return expected.copy(deep=True)\n            elif data_vars == \"minimal\":\n                return xr.Dataset({\"a\": (\"x\", [1, 2, 3])}, coords)\n            else:\n                raise ValueError(\"Invalid data_vars option\")\n\n    actual = xr.open_dataset(\n        \"fake_filename\", engine=CustomBackend, data_vars=\"all\", coords=\"different\"\n    )\n    assert_identical(expected, actual)\n\n    actual = xr.open_dataset(\n        \"fake_filename\", engine=CustomBackend, data_vars=\"minimal\", coords=\"different\"\n    )\n    assert_identical(xr.Dataset({\"a\": (\"x\", [1, 2, 3])}, coords), actual)\n"], "sample_1161": ["def test_stringify():\n    p = StrPrinter()\n    assert p.stringify([1, 2, 3], \", \") == \"1, 2, 3\"\n    assert p.stringify([], \", \") == \"\"\n    assert p.stringify([1], \", \") == \"1\"\n", "def test_MatPow_precedence():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert str(A**2) == \"A**2\"\n    assert str(A**(-1)) == \"A**(-1)\"\n    assert str(A**(2*x)) == \"A**(2*x)\"\n    assert str(A**B) == \"A**B\"\n    assert str(A**(B**2)) == \"A**(B**2)\"\n    assert str(A**(B**(-2))) == \"A**(B**(-2))\"\n    assert str(A**(B**(2*x))) == \"A**(B**(2*x))\"\n", "def test_issue_25253():\n    from sympy.physics.units.definitions import second\n    assert sstr(second**(-2), abbrev=True) == \"1/s**2\"\n    assert sstr(second**(-2), abbrev=False) == \"1/second**2\"\n    assert sstr(second**(-1), abbrev=True) == \"1/s\"\n    assert sstr(second**(-1), abbrev=False) == \"1/second\"\n    assert sstr(second**2, abbrev=True) == \"s**2\"\n    assert sstr(second**2, abbrev=False) == \"second**2\"\n    assert sstr(second, abbrev=True) == \"s\"\n    assert sstr(second, abbrev=False) == \"second\"\n", "def test_StrPrinter():\n    printer = StrPrinter()\n\n        assert printer.doprint(expr) == expected_str\n\n    check(x**2 + 3*x + 1, 'x**2 + 3*x + 1')\n    check(1 + x**2 + 3*x, 'x**2 + 3*x + 1')\n    check(x**2 + x + 3, 'x**2 + x + 3')\n    check(x**3 + x**2 + x + 1, 'x**3 + x**2 + x + 1')\n    check(x**3 + 3*x**2 + x + 1, 'x**3 + 3*x**2 + x + 1')\n    check(x**3 + x**2 + 3*x + 1, 'x**3 + x**2 + 3*x + 1')\n    check(x**3 + x**2 + x + 3, 'x**3 + x**2 + x + 3')\n    check(3*x**3 + x**2 + x + 1, '3*x**3 + x**2 + x + 1')\n    check(x**3 + 3*x**2 + 3*x + 1, 'x**3 + 3*x**2 + 3*x + 1')\n    check(x**3 + x**2 + 3*x + 3, 'x**3 + x**2 + 3*x + 3')\n    check(3*x**3 + 3*x**2 + x + 1, '3*x**3 + 3*x**2 + x + 1')\n    check(3*x**3 + x**2 + 3*x + 3, '3*x**3 + x**2 + 3*x + 3')\n    check(3*x**3 + 3*x**2 + 3*x + 3, '3*x**3 + 3*x**2 + 3*x + 3')\n", "def test_Rational_function():\n    x = Symbol('x')\n    f = x/(x + 1)\n    assert sstr(f) == \"(x + 1)**(-1)*x\"\n\n    # Test printing rational functions with order specified\n    f = Poly(x, x)/Poly(x + 1, x)\n    assert sstr(f, order=\"lex\") == \"(x + 1)**(-1)*x\"\n\n    f = Poly(x, x)/Poly(x + 1, x, order=\"deglex\")\n    assert sstr(f, order=\"deglex\") == \"(x + 1)**(-1)*x\"\n"], "sample_70": ["def test_can_fast_delete_with_multiple_objects(self):\n    u1 = User.objects.create()\n    u2 = User.objects.create()\n    collector = Collector(using='default')\n    self.assertTrue(collector.can_fast_delete([u1, u2]))\n    User.objects.filter(pk__in=[u1.pk, u2.pk]).delete()\n    self.assertIsNone(u1.pk)\n    self.assertIsNone(u2.pk)\n", "    def test_collector_add(self):\n        collector = Collector(using='default')\n        a1 = A.objects.create(name='a1')\n        a2 = A.objects.create(name='a2')\n        new_objs = collector.add([a1, a2])\n        self.assertEqual(new_objs, [a1, a2])\n        self.assertEqual(collector.data, {A: {a1, a2}})\n\n        # Add the same objects again\n        new_objs = collector.add([a1, a2])\n        self.assertEqual(new_objs, [])\n\n        # Add new objects\n        a3 = A.objects.create(name='a3')\n        a4 = A.objects.create(name='a4')\n        new_objs = collector.add([a3, a4])\n        self.assertEqual(new_objs, [a3, a4])\n        self.assertEqual(collector.data, {A: {a1, a2, a3, a4}})\n", "def test_can_fast_delete_with_protected_reverse_relationship(self):\n    a = create_a('child_protected')\n    # A can be fast-deleted.\n    collector = Collector(using='default')\n    self.assertTrue(collector.can_fast_delete(a.child_protected))\n    # Even though A is being fast-deleted, child_protected won't be fast deleted\n    # since it has a protected reverse relationship to A.\n    self.assertFalse(collector.can_fast_delete(a.child_protected, from_field=a._meta.get_field('child_protected')))\n", "def test_can_fast_delete_with_signal_listeners(self):\n    \"\"\"\n    When signal listeners are present for an object, but the object's delete\n    operation can still be performed as a fast delete, can_fast_delete should\n    return True.\n    \"\"\"\n    a = create_a('o2o_setnull')\n    a.o2o_setnull.delete()\n    a = A.objects.get(pk=a.pk)\n    self.assertIsNone(a.o2o_setnull)\n\n    calls = []\n\n        calls.append(sender)\n        self.assertTrue(sender.can_fast_delete(a.o2o_setnull))\n\n    signals.pre_delete.connect(check_can_fast_delete, sender=A)\n    a.o2o_setnull.delete()\n    signals.pre_delete.disconnect(check_can_fast_delete, sender=A)\n\n    self.assertEqual(len(calls), 1)\n", "def test_multiple_cascades(self):\n    a1 = create_a('cascade')\n    a2 = create_a('cascade_nullable')\n    a3 = create_a('cascade')\n    r1 = a1.cascade\n    r2 = a2.cascade_nullable\n    r3 = a3.cascade\n    r1.m2m.add(r2)\n    r3.m2m.add(r1)\n    # Collecting these two A's and their related R's, and the R's related\n    # M2M's will cascade to 4 R's, 3 M2M's, and 2 A's.\n    with self.assertNumQueries(7):\n        A.objects.filter(name__in=['cascade', 'cascade_nullable']).delete()\n    self.assertFalse(A.objects.exists())\n    self.assertFalse(R.objects.exists())\n    self.assertFalse(MR.objects.exists())\n"], "sample_811": ["def test_check_pairwise_arrays_input_order():\n    # Ensures that check_pairwise_arrays returns X and Y in the correct order\n    XA = np.array([[1, 2], [3, 4]])\n    XB = np.array([[5, 6], [7, 8]])\n    XA_checked, XB_checked = check_pairwise_arrays(XA, XB)\n    assert_array_equal(XA, XA_checked)\n    assert_array_equal(XB, XB_checked)\n\n    XB_checked, XA_checked = check_pairwise_arrays(XB, XA)\n    assert_array_equal(XA, XA_checked)\n    assert_array_equal(XB, XB_checked)\n", "def test_cosine_similarity_dtype(dtype):\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((10, 10)).astype(dtype)\n    Y = rng.random_sample((10, 10)).astype(dtype)\n\n    K = cosine_similarity(X, Y)\n    assert_equal(K.dtype, np.float)\n", "def test_check_non_negative_arrays():\n    # Ensure an error is raised on negative arrays for additive_chi2_kernel.\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    X[X < 0.5] = -X[X < 0.5]\n    XB = rng.random_sample((5, 4))\n    XB[XB < 0.5] = -XB[XB < 0.5]\n    assert_raises(ValueError, check_pairwise_arrays, X, XB,\n                 precomputed=False, dtype=None)\n", "def test_check_pairwise_arrays_with_precomputed_metric():\n    # Test that check_pairwise_arrays with a precomputed metric raises an error\n    # when the shape of X is not (n_samples, n_samples) or (n_queries, n_indexed)\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 3))  # wrong shape\n    assert_raises(ValueError, check_pairwise_arrays, X, None, precomputed=True)\n    X = rng.random_sample((5, 5))  # correct shape\n    check_pairwise_arrays(X, None, precomputed=True)\n    X = rng.random_sample((3, 5))  # correct shape\n    check_pairwise_arrays(X, None, precomputed=True)\n", "def test_check_pairwise_arrays_negative_dtype():\n    # Ensures that checks return valid arrays with float dtype for pairwise\n    # arrays even if the input array has negative values.\n\n    rng = np.random.RandomState(0)\n    XA = rng.random_sample((5, 4))\n    XA[0, 0] = -0.5\n    XB = rng.random_sample((5, 4))\n\n    XA_checked, XB_checked = check_pairwise_arrays(XA, XB)\n    assert_array_equal(XA, XA_checked)\n    assert_array_equal(XB, XB_checked)\n    assert XA_checked.dtype == np.float\n    assert XB_checked.dtype == np.float\n"], "sample_483": ["def test_check_ordering_with_empty_string(self):\n    class SongAdmin(admin.ModelAdmin):\n        ordering = [\"\"]\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'ordering[0]' refers to '', which is not a field of \"\n            \"'admin_checks.Song'.\",\n            obj=SongAdmin,\n            id=\"admin.E033\",\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_check_radio_fields_key(self):\n    class SongAdmin(admin.ModelAdmin):\n        radio_fields = {\"nonexistent\": admin.HORIZONTAL}\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'radio_fields' refers to 'nonexistent', which does not \"\n            \"refer to a field of 'admin_checks.Song', and does not have a \"\n            \"'choices' definition.\",\n            obj=SongAdmin,\n            id=\"admin.E023\",\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "    def test_check_ordering_item(self):\n        class SongAdmin(admin.ModelAdmin):\n            ordering = [\"album__id\"]\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        self.assertEqual(errors, [])\n", "    def test_check_ordering_with_f_expression(self):\n        \"\"\"Check that ordering with F expression is valid.\"\"\"\n\n        class SongAdmin(admin.ModelAdmin):\n            ordering = (models.F(\"title\"),)\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        self.assertEqual(errors, [])\n", "    def test_check_readonly_fields_item_for_list_display(self):\n        \"\"\"\n        Regression test for #32583 - readonly_fields should allow for fields\n        present in list_display even if not in the model.\n        \"\"\"\n\n        class SongAdmin(admin.ModelAdmin):\n            list_display = [\"title\", \"original_release\"]\n            readonly_fields = [\"original_release\"]\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        self.assertEqual(errors, [])\n"], "sample_10": ["def test_add_column_with_index():\n    t = Table()\n    idx = Index([Column([1, 2, 3])], engine=None, unique=False)\n    with pytest.raises(ValueError):\n        t.add_column(idx, index=0)\n", "def test_table_html_readwrite():\n    t = table.Table([[1, 2], [3, 4]], names=('a', 'b'))\n    out = StringIO()\n    t.write(out, format='html')\n    out.seek(0)\n    t2 = table.Table.read(out, format='html')\n    assert np.all(t2['a'] == t['a'])\n    assert np.all(t2['b'] == t['b'])\n    assert t2.colnames == t.colnames\n    assert t2.dtype == t.dtype\n", "def test_masked_table_creation_from_unmasked_table():\n    t = Table([[1, 2, 3], [4, 5, 6]], names=['a', 'b'])\n    mt = Table(t, masked=True)\n    assert mt.colnames == ['a', 'b']\n    assert np.all(mt['a'].mask == False)\n    assert np.all(mt['b'].mask == False)\n    assert isinstance(mt['a'], MaskedColumn)\n    assert isinstance(mt['b'], MaskedColumn)\n    assert mt['a'].dtype == np.dtype('int64')\n    assert mt['b'].dtype == np.dtype('int64')\n    assert mt.masked is True\n", "def test_init_with_list_of_dtypes():\n    \"\"\"\n    Test the fix for #9137\n    \"\"\"\n    t = Table(data=np.array([1, 2], dtype=[('a', 'i4'), ('b', 'f4')]))\n    assert t.colnames == ['a', 'b']\n    assert np.all(t['a'] == [1, 2])\n    assert np.all(t['b'] == [1., 2.])\n", "def test_table_attribute_pickling_round_trip():\n    \"\"\"Test round-trip of table attributes through pickling/unpickling\"\"\"\n    t = MyTable()\n    t.foo = [1, 2, 3]\n    t.bar = 'bar'\n    t.baz = 1\n    assert t.foo == [1, 2, 3]\n    assert t.bar == 'bar'\n    assert t.baz == 1\n    t_pickled = pickle.loads(pickle.dumps(t))\n    assert t_pickled.foo == [1, 2, 3]\n    assert t_pickled.bar == 'bar'\n    assert t_pickled.baz == 1\n"], "sample_717": ["def test_load_lfw_pairs_subset():\n    lfw_pairs_train = fetch_lfw_pairs(subset='train', data_home=SCIKIT_LEARN_DATA,\n                                      download_if_missing=True)\n    lfw_pairs_test = fetch_lfw_pairs(subset='test', data_home=SCIKIT_LEARN_DATA,\n                                     download_if_missing=True)\n    lfw_pairs_10_folds = fetch_lfw_pairs(subset='10_folds',\n                                         data_home=SCIKIT_LEARN_DATA,\n                                         download_if_missing=True)\n\n    assert lfw_pairs_train.pairs.shape != lfw_pairs_test.pairs.shape\n    assert lfw_pairs_train.pairs.shape != lfw_pairs_10_folds.pairs.shape\n    assert lfw_pairs_test.pairs.shape != lfw_pairs_10_folds.pairs.shape\n\n    # the target is whether the person is the same or not\n    assert_array_equal(lfw_pairs_train.target_names,\n                       ['Different persons', 'Same person'])\n    assert_array_equal(lfw_pairs_test.target_names,\n                       ['Different persons', 'Same person'])\n    assert_array_equal(lfw_pairs_10_folds.target_names,\n                       ['Different persons', 'Same person'])\n\n    assert_raises(ValueError, fetch_lfw_pairs, subset='invalid_subset',\n                  data_home=SCIKIT_LEARN_DATA, download_if_missing=True)\n", "def test_fetch_lfw_pairs_subsets():\n    for subset in ['train', 'test', '10_folds']:\n        lfw_pairs_subset = fetch_lfw_pairs(subset=subset, download_if_missing=False)\n        assert lfw_pairs_subset.data is not None\n        assert lfw_pairs_subset.pairs is not None\n        assert lfw_pairs_subset.target is not None\n        assert lfw_pairs_subset.target_names is not None\n        assert lfw_pairs_subset.DESCR is not None\n", "def test_fetch_lfw_pairs_subset():\n    lfw_pairs_train = fetch_lfw_pairs(subset='train', data_home=SCIKIT_LEARN_DATA,\n                                      download_if_missing=False)\n    lfw_pairs_test = fetch_lfw_pairs(subset='test', data_home=SCIKIT_LEARN_DATA,\n                                     download_if_missing=False)\n    lfw_pairs_10folds = fetch_lfw_pairs(subset='10_folds',\n                                        data_home=SCIKIT_LEARN_DATA,\n                                        download_if_missing=False)\n\n    assert lfw_pairs_train.pairs.shape[0] != lfw_pairs_test.pairs.shape[0]\n    assert lfw_pairs_train.pairs.shape[0] != lfw_pairs_10folds.pairs.shape[0]\n    assert lfw_pairs_test.pairs.shape[0] != lfw_pairs_10folds.pairs.shape[0]\n\n    assert lfw_pairs_train.target_names.tolist() == lfw_pairs_test.target_names.tolist()\n    assert lfw_pairs_train.target_names.tolist() == lfw_pairs_10folds.target_names.tolist()\n", "def test_load_lfw_pairs_subset():\n    lfw_pairs_train = fetch_lfw_pairs(subset='train',\n                                      data_home=SCIKIT_LEARN_DATA,\n                                      download_if_missing=False)\n    lfw_pairs_test = fetch_lfw_pairs(subset='test',\n                                     data_home=SCIKIT_LEARN_DATA,\n                                     download_if_missing=False)\n    lfw_pairs_10_folds = fetch_lfw_pairs(subset='10_folds',\n                                         data_home=SCIKIT_LEARN_DATA,\n                                         download_if_missing=False)\n\n    assert_equal(lfw_pairs_train.pairs.shape[0], 10)\n    assert_equal(lfw_pairs_test.pairs.shape[0], 10)\n    assert_equal(lfw_pairs_10_folds.pairs.shape[0], 10)\n\n    # the target is whether the person is the same or not\n    assert_array_equal(lfw_pairs_train.target_names,\n                       ['Different persons', 'Same person'])\n    assert_array_equal(lfw_pairs_test.target_names,\n                       ['Different persons', 'Same person'])\n    assert_array_equal(lfw_pairs_10_folds.target_names,\n                       ['Different persons', 'Same person'])\n\n    # test invalid subset value\n    assert_raises(ValueError, fetch_lfw_pairs, subset='invalid',\n                  data_home=SCIKIT_LEARN_DATA, download_if_missing=False)\n", "def test_fetch_lfw_pairs_subset():\n    # Test that the different subsets ('train', 'test', '10_folds') return\n    # different numbers of pairs\n    lfw_pairs_train = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA,\n                                      download_if_missing=False,\n                                      subset='train')\n    lfw_pairs_test = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA,\n                                     download_if_missing=False,\n                                     subset='test')\n    lfw_pairs_10_folds = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA,\n                                         download_if_missing=False,\n                                         subset='10_folds')\n    assert lfw_pairs_train.pairs.shape[0] != lfw_pairs_test.pairs.shape[0]\n    assert lfw_pairs_train.pairs.shape[0] != lfw_pairs_10_folds.pairs.shape[0]\n    assert lfw_pairs_test.pairs.shape[0] != lfw_pairs_10_folds.pairs.shape[0]\n\n    # Test that an invalid subset raises a ValueError\n    assert_raises(ValueError, fetch_lfw_pairs, data_home=SCIKIT_LEARN_DATA,\n                  download_if_missing=False, subset='invalid')\n"], "sample_140": ["    def test_sensitive_variables_with_all(self):\n        @sensitive_variables()\n            return password, user\n\n        self.assertEqual(test_func.sensitive_variables, '__ALL__')\n", "    def test_sensitive_variables_called_without_args(self):\n        @sensitive_variables()\n            pass\n        self.assertEqual(test_func.sensitive_variables, '__ALL__')\n", "    def test_sensitive_variables_empty(self):\n        @sensitive_variables()\n            pass\n        self.assertEqual(test_func.sensitive_variables, '__ALL__')\n", "    def test_sensitive_variables_called_with_args(self):\n        @sensitive_variables('user', 'password')\n            return user, password\n\n        self.assertEqual(test_func.sensitive_variables, ('user', 'password'))\n", "    def test_sensitive_variables_empty(self):\n        @sensitive_variables()\n            pass\n        self.assertEqual(test_func.sensitive_variables, '__ALL__')\n"], "sample_971": ["def test_get_node_location(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    node = nodes.Node()\n    node.source, node.line = ('index.txt', 10)\n    location = logging.get_node_location(node)\n    assert location == 'index.txt:10'\n\n    node.source, node.line = ('index.txt', None)\n    location = logging.get_node_location(node)\n    assert location == 'index.txt:'\n\n    node.source, node.line = (None, 10)\n    location = logging.get_node_location(node)\n    assert location == '<unknown>:10'\n\n    node.source, node.line = (None, None)\n    location = logging.get_node_location(node)\n    assert location is None\n", "def test_get_node_location():\n    node = nodes.Node()\n    node.source, node.line = ('source.txt', 10)\n    location = logging.get_node_location(node)\n    assert location == \"source.txt:10\"\n\n    node.source, node.line = ('source.txt', None)\n    location = logging.get_node_location(node)\n    assert location == \"source.txt:\"\n\n    node.source, node.line = (None, 10)\n    location = logging.get_node_location(node)\n    assert location == \"<unknown>:10\"\n\n    node.source, node.line = (None, None)\n    location = logging.get_node_location(node)\n    assert location is None\n", "def test_log_collector(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    log_collector = logging.LogCollector()\n    with log_collector.collect():\n        logger.info('message1')\n        logger.warning('message2')\n        logger.error('message3')\n\n    assert len(log_collector.logs) == 3\n    assert log_collector.logs[0].levelname == 'INFO'\n    assert log_collector.logs[0].message == 'message1'\n    assert log_collector.logs[1].levelname == 'WARNING'\n    assert log_collector.logs[1].message == 'message2'\n    assert log_collector.logs[2].levelname == 'ERROR'\n    assert log_collector.logs[2].message == 'message3'\n", "def test_get_node_location():\n    node1 = nodes.Node()\n    node1.source, node1.line = ('file1.txt', 10)\n    assert logging.get_node_location(node1) == 'file1.txt:10'\n\n    node2 = nodes.Node()\n    node2.source, node2.line = ('file2.txt', None)\n    assert logging.get_node_location(node2) == 'file2.txt:'\n\n    node3 = nodes.Node()\n    node3.source, node3.line = (None, 20)\n    assert logging.get_node_location(node3) == '<unknown>:20'\n\n    node4 = nodes.Node()\n    node4.source, node4.line = (None, None)\n    assert logging.get_node_location(node4) is None\n", "def test_collector(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    collector = logging.LogCollector()\n\n    with collector.collect():\n        logger.warning('message1')\n        logger.info('message2')\n        logger.critical('message3')\n\n    assert len(collector.logs) == 3\n\n    for record in collector.logs:\n        assert isinstance(record, logging.LogRecord)\n\n    assert collector.logs[0].getMessage() == 'message1'\n    assert collector.logs[1].getMessage() == 'message2'\n    assert collector.logs[2].getMessage() == 'message3'\n"], "sample_382": ["def test_watch_for_template_changes_empty_directories(self, mock_get_template_directories):\n    mock_get_template_directories.return_value = set()\n    mock_reloader = mock.MagicMock()\n    autoreload.watch_for_template_changes(mock_reloader)\n    mock_reloader.watch_dir.assert_not_called()\n", "    def test_template_changed_with_python_file(self, mock_reset):\n        template_path = Path(__file__).parent / 'test.py'\n        self.assertIsNone(autoreload.template_changed(None, template_path))\n        mock_reset.assert_not_called()\n", "def test_get_template_directories_app_dirs(self):\n    self.assertSetEqual(\n        autoreload.get_template_directories(),\n        {ROOT / 'templates'}\n    )\n", "    def test_get_template_directories_no_template_engines(self):\n        engines.all = mock.Mock(return_value=[])\n        self.assertSetEqual(autoreload.get_template_directories(), set())\n        engines.all.assert_called_once()\n", "    def test_template_changed_no_template_dirs(self, mock_get_template_directories):\n        mock_get_template_directories.return_value = set()\n        self.assertIsNone(autoreload.template_changed(None, Path(__file__)))\n        mock_get_template_directories.assert_called_once()\n"], "sample_642": ["def test_parse_rich_type_value() -> None:\n    \"\"\"Test the _parse_rich_type_value function.\"\"\"\n    assert _parse_rich_type_value(\"hello\") == \"hello\"\n    assert _parse_rich_type_value(123) == \"123\"\n    assert _parse_rich_type_value([1, 2, 3]) == \"1,2,3\"\n    assert _parse_rich_type_value((1, 2, 3)) == \"1,2,3\"\n    assert _parse_rich_type_value({\"a\": 1, \"b\": 2}) == \"a:1,b:2\"\n    assert _parse_rich_type_value(re.compile(\"hello\")) == \"hello\"\n    with pytest.raises(TypeError):\n        _parse_rich_type_value(None)\n", "def test_preprocess_options_takearg_value() -> None:\n    \"\"\"Test that _preprocess_options correctly handles takearg options with values.\"\"\"\n    run = mock.Mock(spec=Run)\n    args = [\"--rcfile\", \"path/to/rcfile\", \"--init-hook\", \"print('Hello World')\"]\n    result = _preprocess_options(run, args)\n    assert result == []\n    run._set_rcfile.assert_called_once_with(\"path/to/rcfile\")\n    run._init_hook.assert_called_once_with(\"print('Hello World')\")\n\n", "def test_preprocessable_options() -> None:\n    \"\"\"Test that pre-processable options are correctly handled.\"\"\"\n    with mock.patch(\"pylint.lint.run.Run._preprocess_options\") as mock_preprocess_options:\n        run = Run([\"--init-hook\", \"import sys\", \"--rcfile\", \"path/to/rcfile\"])\n        mock_preprocess_options.assert_called_once_with(run, [\"--init-hook\", \"import sys\", \"--rcfile\", \"path/to/rcfile\"])\n        assert run._rcfile == \"path/to/rcfile\"\n        assert run._plugins == []\n\n        run = Run([\"--init-hook=import sys\", \"--load-plugins=plugin1,plugin2\"])\n        mock_preprocess_options.assert_called_with(run, [\"--init-hook=import sys\", \"--load-plugins=plugin1,plugin2\"])\n        assert run._plugins == [\"plugin1\", \"plugin2\"]\n\n        run = Run([\"-v\"])\n        mock_preprocess_options.assert_called_with(run, [\"-v\"])\n        assert run.verbose is True\n\n        run = Run([\"--enable-all-extensions\"])\n        mock_preprocess_options.assert_called_with(run, [\"--enable-all-extensions\"])\n        assert len(run._plugins) > 0\n\n        with pytest.raises(config.exceptions.ArgumentPreprocessingError):\n            Run([\"--init-hook\"])\n\n        with pytest.raises(config.exceptions.ArgumentPreprocessingError):\n            Run([\"--init-hook=\", \"import sys\"])\n\n        with pytest.raises(config.exceptions.ArgumentPreprocessingError):\n            Run([\"-v\", \"value\"])\n", "def test_preprocess_options_without_value() -> None:\n    \"\"\"Test that the _preprocess_options function raises an error when a pre-processable option is missing a value.\"\"\"\n    run = mock.MagicMock()\n    args = [\"--init-hook\"]\n\n    with pytest.raises(config.exceptions.ArgumentPreprocessingError):\n        _preprocess_options(run, args)\n\n", "def test_init_hook(capsys: CaptureFixture, option_name: str, input_value: str, expected_value: str) -> None:\n    \"\"\"Test the init_hook function.\"\"\"\n    run = mock.Mock(spec=Run)\n    _init_hook(run, input_value)\n    out = capsys.readouterr()\n    assert expected_value in out.out\n\n"], "sample_420": ["    def test_fields_from_abstract_model(self):\n        class AbstractModel(models.Model):\n            abstract = True\n            field1 = models.CharField(max_length=10)\n\n        class ConcreteModel(AbstractModel):\n            field2 = models.CharField(max_length=10)\n\n        class ConcreteForm(forms.ModelForm):\n            class Meta:\n                model = ConcreteModel\n                fields = \"__all__\"\n\n        self.assertEqual(list(ConcreteForm().fields), [\"field1\", \"field2\"])\n", "    def test_model_form_with_non_existent_model_field(self):\n        class NonExistentFieldModelForm(forms.ModelForm):\n            non_existent_field = forms.CharField()\n\n            class Meta:\n                model = Category\n                fields = \"__all__\"\n\n        with self.assertRaisesMessage(FieldError, \"Unknown field(s) (non_existent_field) specified for Category\"):\n            NonExistentFieldModelForm()\n", "    def test_fields_order(self):\n        class BaseForm(forms.ModelForm):\n            class Meta:\n                model = Category\n                fields = (\"name\", \"slug\", \"url\")\n\n        class ChildForm(BaseForm):\n            pass\n\n        self.assertEqual(\n            list(BaseForm.base_fields),\n            [\"name\", \"slug\", \"url\"],\n        )\n\n        self.assertEqual(\n            list(ChildForm.base_fields),\n            [\"name\", \"slug\", \"url\"],\n        )\n\n        class ChildForm(BaseForm):\n            class Meta:\n                model = Category\n                fields = (\"slug\", \"name\", \"url\")\n\n        self.assertEqual(\n            list(BaseForm.base_fields),\n            [\"name\", \"slug\", \"url\"],\n        )\n\n        self.assertEqual(\n            list(ChildForm.base_fields),\n            [\"slug\", \"name\", \"url\"],\n        )\n\n        class ChildForm(BaseForm):\n            class Meta:\n                model = Category\n                fields = (\"slug\", \"url\", \"name\")\n\n        self.assertEqual(\n            list(BaseForm.base_fields),\n            [\"name\", \"slug\", \"url\"],\n        )\n\n        self.assertEqual(\n            list(ChildForm.base_fields),\n            [\"slug\", \"url\", \"name\"],\n        )\n", "    def test_fields_for_model(self):\n        expected_fields = {\n            \"name\": forms.CharField,\n            \"slug\": forms.CharField,\n            \"url\": forms.CharField,\n        }\n        self.assertEqual(\n            fields_for_model(Category, fields=\"__all__\"), expected_fields\n        )\n", "    def setUpTestData(cls):\n        cls.writer = Writer.objects.create(name=\"Mike Royko\")\n"], "sample_31": ["    def test_write_latex_unit_conversion(self, write, tmp_path, format):\n        \"\"\"Test unit conversion for columns\"\"\"\n        fp = tmp_path / \"test_write_latex_unit_conversion.tex\"\n        write(fp, format=format)\n        tbl = QTable.read(fp)\n        # asserts each column has the correct unit\n        for column_name in tbl.colnames[2:]:\n            param_name = next((k for k, v in _FORMAT_TABLE.items() if v == column_name), None)\n            if param_name:\n                param = getattr(self.cosmology.__class__, param_name, None)\n                assert param is not None and param.unit is not None and param.unit == tbl[column_name].unit\n", "    def test_write_latex_unit_conversion(self, write, tmp_path, format):\n        \"\"\"Test unit conversion during write operation\"\"\"\n        fp = tmp_path / \"test_write_latex_unit_conversion.tex\"\n        write(fp, format=format)\n        tbl = QTable.read(fp)\n        for column_name in tbl.colnames[2:]:\n            if column_name in _FORMAT_TABLE.values():\n                param = [p for p in type(self.cosmology).__parameters__ if _FORMAT_TABLE.get(p) == column_name]\n                if param:\n                    param = param[0]\n                    assert tbl[column_name].unit == getattr(type(self.cosmology), param).unit\n", "    def test_write_latex_with_kwargs(self, write, tmp_path, format):\n        \"\"\"Test passing kwargs to write_latex\"\"\"\n        fp = tmp_path / \"test_write_latex_with_kwargs.tex\"\n        write(fp, format=format, latex_names=True)\n        tbl = QTable.read(fp)\n        \n        # Now try with kwargs\n        write(fp, format=format, latex_names=True, comment='#')\n        tbl = QTable.read(fp, format='latex', comment='#')\n        for column_name in tbl.colnames[2:]:\n            # for now, Cosmology as metadata and name is stored in first 2 slots\n            assert column_name in _FORMAT_TABLE.values()\n", "def test_rename_latex_columns_with_cls(self, write, tmp_path, latex_names):\n    \"\"\"Test renaming columns when latex_names is True or False.\"\"\"\n    fp = tmp_path / \"test_rename_latex_columns_with_cls.tex\"\n    write(fp, format=\"latex\", latex_names=latex_names)\n    tbl = QTable.read(fp)\n    if latex_names:\n        for column_name in tbl.colnames[2:]:\n            assert column_name in _FORMAT_TABLE.values()\n    else:\n        for column_name in tbl.colnames[2:]:\n            assert column_name in _FORMAT_TABLE.keys()\n", "    def test_write_latex_cls_with_custom_kwargs(self, write, tmp_path, format):\n        \"\"\"Test writing with custom kwargs for table class\"\"\"\n        fp = tmp_path / \"test_write_latex_cls_with_custom_kwargs.tex\"\n        write(fp, format=format, cls=QTable, latex_names=True, meta={'name': 'custom_name'})\n        tbl = QTable.read(fp, format=\"latex\")\n        assert tbl.meta['name'] == 'custom_name'\n"], "sample_64": ["    def test_set_signed_cookie(self):\n        response = HttpResponse()\n        response.set_signed_cookie('test', 'value')\n        self.assertIn('Set-Cookie', response.serialize_headers())\n", "    def test_setdefault(self):\n        r = HttpResponseBase()\n        r.setdefault('header', 'value')\n        self.assertEqual(r['header'], 'value')\n        r['header'] = 'new_value'\n        r.setdefault('header', 'another_value')\n        self.assertEqual(r['header'], 'new_value')\n", "    def test_init_content_type(self):\n        response = HttpResponseBase(content_type='text/plain')\n        self.assertEqual(response['Content-Type'], 'text/plain')\n", "    def test_default_headers(self):\n        r = HttpResponseBase()\n        self.assertIn('Content-Type', r)\n        self.assertEqual(r['Content-Type'], 'text/html; charset=utf-8')\n", "    def test_status_code_out_of_range(self):\n        with self.assertRaises(ValueError):\n            HttpResponseBase(status=600)\n"], "sample_694": ["def test_argument_type_str_choice_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n            parser.addoption(\n                \"--example\",\n                action=\"store\",\n                type=\"example\",\n                choices=[\"one\", \"two\"],\n            )\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-Wdefault::pytest.PytestRemovedIn8Warning\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestRemovedIn8Warning: `type` argument to addoption() is the string 'example'.\"\n            \" For choices this is optional and can be omitted, \"\n            \" but when supplied should be a type (for example `str` or `int`).\"\n            \" (options: ['one', 'two'])\",\n        ]\n    )\n", "def test_argument_percent_default_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            parser.addoption(\"--foo\", type=\"int\", default=0, help=\"foo value (%default)\")\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-Wdefault::pytest.PytestRemovedIn8Warning\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestRemovedIn8Warning: pytest now uses argparse. '%default' should be changed to '%(default)s'\",\n        ]\n    )\n\n", "def test_argument_type_str_is_deprecated(pytester: Pytester, typ: str) -> None:\n    pytester.makeconftest(\n        f\"\"\"\n        import pytest\n\n            parser.addoption(\"--arg\", type=\"{typ}\")\n        \"\"\"\n    )\n    with pytest.warns(\n        PytestRemovedIn8Warning,\n        match=f\"`type` argument to addoption() is the string '{typ!r}'\",\n    ):\n        pytester.parseconfig(\"-Wdefault::pytest.PytestRemovedIn8Warning\")\n", "def test_argument_percent_default_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        addopts = --help\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--help\", \"-Wdefault::pytest.PytestRemovedIn8Warning\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestRemovedIn8Warning: pytest now uses argparse. '%default' should be changed to '%(default)s'\",\n        ]\n    )\n\n", "def test_argument_type_str_choice_is_deprecated(pytester: Pytester) -> None:\n    \"\"\"Test deprecation warning for using string in type argument for addoption.\"\"\"\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n            parser.addoption(\"--foo\", type=\"str\", choices=[\"bar\", \"baz\"])\n        \"\"\"\n    )\n    with pytest.warns(\n        PytestRemovedIn8Warning,\n        match=r\"`type` argument to addoption() is the string 'str'. \"\n        r\"For choices this is optional and can be omitted, \"\n        r\"but when supplied should be a type\",\n    ):\n        pytester.parseconfig(\"-Wdefault::pytest.PytestRemovedIn8Warning\")\n"], "sample_159": ["    def test_builtin_permission_codename_length(self):\n        class Checked(models.Model):\n            class Meta:\n                verbose_name = 'x' * 201\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The verbose_name of model 'auth_tests.Checked' must be at most 244 \"\n                \"characters for its builtin permission names to be at most 255 characters.\",\n                obj=Checked,\n                id='auth.E007',\n            ),\n        ])\n", "    def test_mixed_user_model_and_permissions_checks(self):\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('my_custom_permission', 'Some permission'),\n                    ('other_one', 'Some other permission'),\n                ]\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"'CustomUserNonUniqueUsername.username' must be \"\n                \"unique because it is named as the 'USERNAME_FIELD'.\",\n                obj=CustomUserNonUniqueUsername,\n                id='auth.E003',\n            ),\n        ])\n", "    def test_permission_name_and_codename_length_with_long_model_name(self):\n        model_name = 'X' * 90\n        custom_permission_name = 'some ridiculously long verbose name that is out of control' * 3\n        custom_permission_codename = 'x' * 10\n\n        model = type(model_name, (models.Model,), {\n            '__module__': self.__module__,\n            'Meta': type('Meta', (), {\n                'verbose_name': 'some verbose name',\n                'permissions': [\n                    (custom_permission_codename, custom_permission_name),\n                ]\n            })\n        })\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(len(errors), 2)\n        self.assertEqual(errors[0].id, 'auth.E008')\n        self.assertEqual(errors[1].id, 'auth.E011')\n", "    def test_builtin_permission_name_length_with_custom_model_name(self):\n        class LongName(models.Model):\n            class Meta:\n                verbose_name = 'some ridiculously long verbose name that is out of control' * 3\n                permissions = [\n                    ('change_longname', 'Change LongName permission'),\n                ]\n\n        class ShortName(models.Model):\n            class Meta:\n                permissions = [\n                    ('change_shortname', 'Change ShortName permission'),\n                ]\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'auth.E007')\n        self.assertEqual(errors[0].obj, LongName)\n", "    def test_builtin_permission_name_length_with_custom_permission(self):\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('change_checked', 'Can edit permission (builtin)')\n                ]\n                default_permissions = ('change_checked',)\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [])\n"], "sample_1082": ["def test_hyperbolic_unbranched():\n    x = Symbol('x')\n    assert sinh(x).unbranched == True\n    assert cosh(x).unbranched == True\n    assert tanh(x).unbranched == True\n    assert coth(x).unbranched == True\n    assert csch(x).unbranched == True\n    assert sech(x).unbranched == True\n", "def test_hyperbolic_rewrite_as_exp():\n    x = Symbol('x')\n    assert sinh(x).rewrite(exp) == (exp(x) - exp(-x))/2\n    assert cosh(x).rewrite(exp) == (exp(x) + exp(-x))/2\n    assert tanh(x).rewrite(exp) == (exp(x) - exp(-x))/(exp(x) + exp(-x))\n    assert coth(x).rewrite(exp) == (exp(x) + exp(-x))/(exp(x) - exp(-x))\n    assert csch(x).rewrite(exp) == 2/(exp(x) - exp(-x))\n    assert sech(x).rewrite(exp) == 2/(exp(x) + exp(-x))\n    assert asinh(x).rewrite(exp) == log(x + sqrt(x**2 + 1))\n    assert acosh(x).rewrite(exp) == log(x + sqrt(x - 1)*sqrt(x + 1))\n    assert atanh(x).rewrite(exp) == log((1 + x)/(1 - x))/2\n    assert acoth(x).rewrite(exp) == log((x + 1)/(x - 1))/2\n    assert asech(x).rewrite(exp) == log(1/x + sqrt(1/x - 1) * sqrt(1/x + 1))\n    assert acsch(x).rewrite(exp) == log(1/x + sqrt(1/x**2 + 1))\n", "def test_peeloff_ipi():\n    from sympy import I, pi\n    from sympy.functions.elementary.hyperbolic import _peeloff_ipi as peel\n    assert peel(0) == (0, 0)\n    assert peel(I*pi/2) == (0, I*pi/2)\n    assert peel(I*pi) == (0, I*pi)\n    assert peel(3*I*pi/2) == (0, 3*I*pi/2)\n    assert peel(-I*pi/2) == (0, -I*pi/2)\n    assert peel(-I*pi) == (0, -I*pi)\n    assert peel(-3*I*pi/2) == (0, -3*I*pi/2)\n    assert peel(I*pi/2 + 3) == (3, I*pi/2)\n    assert peel(I*pi + 3) == (3, I*pi)\n    assert peel(3*I*pi/2 + 3) == (3, 3*I*pi/2)\n    assert peel(-I*pi/2 + 3) == (3, -I*pi/2)\n    assert peel(-I*pi + 3) == (3, -I*pi)\n    assert peel(-3*I*pi/2 + 3) == (3, -3*I*pi/2)\n    assert peel(I*pi/2 - 3) == (-3, I*pi/2)\n    assert peel(I*pi - 3) == (-3, I*pi)\n    assert peel(3*I*pi/2 - 3) == (-3, 3*I*pi/2)\n    assert peel(-I*pi/2 - 3) == (-3, -I*pi/2)\n    assert peel(-I*pi - 3) == (-3, -I*pi)\n    assert peel(-3*I*pi/2 - 3) == (-3, -3*I*pi/2)\n", "def test_ReciprocalHyperbolicFunction():\n    x = Symbol('x')\n    assert sech(x).is_even is True\n    assert csch(x).is_odd is True\n    assert sech(x)._reciprocal_of(x) == cosh(x)\n    assert csch(x)._reciprocal_of(x) == sinh(x)\n\n    assert sech(-x) == sech(x)\n    assert csch(-x) == -csch(x)\n    assert sech(x)._calculate_reciprocal('_eval_conjugate') == cosh(x).conjugate()\n    assert sech(x)._rewrite_reciprocal('_eval_rewrite_as_exp', x) == 1/cosh(x).rewrite('exp')\n", "def test_hyperbolic_taylor_terms():\n    x = Symbol('x')\n    for func in [sinh, cosh, tanh, coth, csch, sech]:\n        series = func(x).series(x, 0, 10)\n        for n in range(10):\n            t = func.taylor_term(n, x)\n            assert t.equals(series.coeff(x, n))\n        for n in range(-10, 0):\n            t = func.taylor_term(n, x)\n            assert t == 0\n"], "sample_848": ["def test_regressor_chain_predict():\n    # test if regressor chain initializes correctly with base estimator and fit\n    # assert predictions work as expected for predict\n\n    ridge = Ridge(random_state=0)\n    chain = RegressorChain(ridge)\n\n    # train the chain and also get the predictions.\n    chain.fit(X, y)\n\n    predictions = chain.predict(X)\n    assert (n_samples, n_outputs) == predictions.shape\n\n    # train the ridge with each column and assert that predictions are equal\n    for i in range(3):\n        ridge_ = clone(ridge)  # create a clone with the same state\n        ridge_.fit(X, y[:, i])\n        assert_array_almost_equal(ridge_.predict(X), predictions[:, i])\n", "def test_regressor_chain_predict_proba():\n    # test predict_proba is not supported by RegressorChain\n    rc = RegressorChain(Ridge())\n    assert_raises_regex(NotImplementedError, \"predict_proba is not \"\n                                            \"available when the base \"\n                                            \"estimator does not have it.\",\n                        rc.predict_proba, X)\n", "def test_regressor_chain_partial_fit():\n    # test if regressor chain initializes correctly with base estimator and \n    # fit assert predictions work as expected for predict\n\n    X, y = datasets.make_regression(n_targets=3)\n    X_train, y_train = X[:50], y[:50]\n    X_test, y_test = X[50:], y[50:]\n\n    half_index = X_train.shape[0] // 2\n    rgr_chain = RegressorChain(GradientBoostingRegressor(random_state=0))\n    rgr_chain.partial_fit(X_train[:half_index], y_train[:half_index])\n    rgr_chain.partial_fit(X_train[half_index:], y_train[half_index:])\n\n    y_pred_chain = rgr_chain.predict(X_test)\n\n    for i in range(3):\n        rgr = GradientBoostingRegressor(random_state=0)\n        rgr.fit(X_train, y_train[:, i])\n        rgr_pred = rgr.predict(X_test)\n        assert_array_almost_equal(y_pred_chain[:, i], rgr_pred)\n", "def test_regressor_chain_fit_and_predict_with_cross_val():\n    # Fit regressor chain and verify predict performance using cross_val_predict\n    X, Y = datasets.make_regression(n_targets=5)\n    regressor_chain = RegressorChain(Ridge(), cv=3)\n    regressor_chain.fit(X, Y)\n\n    Y_pred = regressor_chain.predict(X)\n    assert Y_pred.shape == Y.shape\n\n    # Compare with the predictions obtained from a chain without cross_val_predict\n    regressor_chain_no_cv = RegressorChain(Ridge())\n    regressor_chain_no_cv.fit(X, Y)\n    Y_pred_no_cv = regressor_chain_no_cv.predict(X)\n    assert not np.all(Y_pred == Y_pred_no_cv)\n\n    # Check that the predictions of the chain with cross_val_predict are better\n    assert mean_squared_error(Y, Y_pred) < mean_squared_error(Y, Y_pred_no_cv)\n", "def test_multi_output_regressor_score():\n    # test if multi target regression's score method works as expected\n    # for multioutput='raw_values' and multioutput='uniform_average'\n\n    X, y = datasets.make_regression(n_targets=3)\n    X_train, y_train = X[:50], y[:50]\n    X_test, y_test = X[50:], y[50:]\n\n    rgr = MultiOutputRegressor(GradientBoostingRegressor(random_state=0))\n    rgr.fit(X_train, y_train)\n\n    multioutput = 'raw_values'\n    scores = rgr.score(X_test, y_test, multioutput=multioutput)\n    assert_array_almost_equal(scores, np.array([\n        mean_squared_error(y_test[:, 0], rgr.predict(X_test)[:, 0]),\n        mean_squared_error(y_test[:, 1], rgr.predict(X_test)[:, 1]),\n        mean_squared_error(y_test[:, 2], rgr.predict(X_test)[:, 2])\n    ]))\n\n    multioutput = 'uniform_average'\n    score = rgr.score(X_test, y_test, multioutput=multioutput)\n    assert_almost_equal(score, np.mean(scores))\n\n    # test if the method raises an error for invalid multioutput\n    invalid_multioutput = 'invalid_value'\n    with pytest.raises(ValueError,\n                      match=f\"multioutput must be 'raw_values' or \"\n                            f\"'uniform_average', but got '{invalid_multioutput}'\"):\n        rgr.score(X_test, y_test, multioutput=invalid_multioutput)\n"], "sample_473": ["compilation error", "compilation error", "compilation error", "    def _mock_receive(self):\n        async def receive():\n            return {\"type\": \"http.disconnect\"}\n\n        return receive\n", "compilation error"], "sample_745": ["def test_inverse_transform_with_sparse_input():\n    X_dense = np.array([1, 4, 9, 16]).reshape((2, 2))\n    X_sparse = sparse.csr_matrix(X_dense)\n\n    F = FunctionTransformer(\n        func=np.sqrt,\n        inverse_func=np.square,\n        accept_sparse=True,\n        validate=True\n    )\n\n    Xt_sparse = F.fit_transform(X_sparse)\n    assert_array_equal(\n        F.inverse_transform(Xt_sparse).toarray(),\n        X_dense,\n    )\n\n    Xt_dense = F.fit_transform(X_dense)\n    assert_array_equal(\n        F.inverse_transform(Xt_dense),\n        X_dense,\n    )\n", "def test_inverse_func_with_kwargs():\n    X = np.array([1, 2, 3, 4]).reshape((2, 2))\n\n    # Test that inverse_transform works correctly with keyword arguments\n    F = FunctionTransformer(\n        func=np.exp,\n        inverse_func=np.log,\n        inv_kw_args=dict(out=np.zeros_like(X)),\n    )\n    Xt = F.fit_transform(X)\n    assert_array_equal(\n        F.inverse_transform(Xt),\n        np.log(Xt, out=np.zeros_like(X)),\n    )\n", "def test_function_transformer_inverse_func_warning():\n    # Test that a warning is raised when inverse_func does not invert func.\n    X = np.array([1, 4, 9, 16]).reshape((2, 2))\n    F = FunctionTransformer(func=np.sqrt, inverse_func=lambda x: x**2, check_inverse=True)\n    with pytest.warns(UserWarning):\n        F.fit(X)\n", "def test_function_transformer_inverse_func_with_kwargs():\n    X = np.array([1, 2, 3, 4]).reshape((2, 2))\n\n    # Test that inverse_func works correctly with keyword arguments\n    F = FunctionTransformer(\n        func=np.around,\n        inverse_func=np.around, inv_kw_args=dict(decimals=1),\n    )\n    assert_array_equal(\n        F.inverse_transform(F.transform(X)),\n        np.around(np.around(X), decimals=1),\n    )\n\n    # Test that changing inv_kw_args updates the inverse transformation\n    F.inv_kw_args['decimals'] = 2\n    assert_array_equal(\n        F.inverse_transform(F.transform(X)),\n        np.around(np.around(X), decimals=2),\n    )\n\n    # Test that setting inv_kw_args to None resets to default\n    F.inv_kw_args = None\n    assert_array_equal(\n        F.inverse_transform(F.transform(X)),\n        np.around(np.around(X)),\n    )\n", "def test_function_transformer_inverse_func_deprecation():\n    X = np.array([1, 4, 9, 16]).reshape((2, 2))\n    func = np.sqrt\n    inverse_func = np.square\n\n    # Test that inverse_func is checked correctly when validate=True\n    transformer = FunctionTransformer(func=func, inverse_func=inverse_func,\n                                     validate=True, check_inverse=True)\n    with pytest.warns(UserWarning):\n        transformer.fit(X)\n\n    # Test that inverse_func is checked correctly when validate=False\n    transformer = FunctionTransformer(func=func, inverse_func=inverse_func,\n                                     validate=False, check_inverse=True)\n    assert_no_warnings(transformer.fit, X)\n\n    # Test that inverse_func is not checked when check_inverse=False\n    transformer = FunctionTransformer(func=func, inverse_func=inverse_func,\n                                     validate=True, check_inverse=False)\n    assert_no_warnings(transformer.fit, X)\n"], "sample_1184": ["compilation error", "compilation error", "def test_beam_parameter():\n    l, w_0, z_r = symbols('l w_0 z_r')\n    p = BeamParameter(l, 1, w=w_0)\n    assert p.z_r == z_r\n    assert p.wavelen == l\n    assert p.n == 1\n\n    p = BeamParameter(l, 1, z_r=z_r)\n    assert p.w == sqrt(w_0**2*(l**2*z_r**2/(pi**2*w_0**4) + 1))\n    assert p.w_0 == sqrt(l*z_r/(pi))\n\n    p = BeamParameter(l, 1, z_r=z_r, n=2)\n    assert p.z_r == z_r\n    assert p.wavelen == l\n    assert p.n == 2\n", "compilation error", "compilation error"], "sample_360": ["    def test_long_key(self):\n        cache = caches['default']\n        key = 'a' * 250\n        cache.set(key, 'value')\n        self.assertEqual(cache.get(key), 'value')\n", "    def tearDown(self):\n        cache.clear()\n", "    def test_vary_by_accept_language(self):\n        request = self.factory.get('/view/', HTTP_ACCEPT_LANGUAGE='en-US')\n        response = HttpResponse()\n        key_prefix = 'localprefix'\n        learn_cache_key(request, response, key_prefix=key_prefix)\n        self.assertIn('en-us', get_cache_key(request, key_prefix=key_prefix))\n        self.assertNotIn('en', get_cache_key(request, key_prefix=key_prefix))\n", "def test_cache_middleware_head_request_with_vary_header(SimpleTestCase):\n    \"\"\"\n    Test that HEAD requests with a Vary header are properly cached.\n\n    This test ensures that the cache middleware correctly handles HEAD requests\n    that include a Vary header. It should cache the response based on the\n    headers specified in the Vary header.\n    \"\"\"\n    middleware = CacheMiddleware(empty_response)\n    request = self.factory.head('/path/to/resource', HTTP_ACCEPT_ENCODING='gzip')\n    response = HttpResponse()\n    response.headers['Vary'] = 'Accept-Encoding'\n\n    # First request, cache should be empty\n    self.assertIsNone(middleware.process_request(request))\n\n    # Set cache response\n    response = middleware.process_response(request, response)\n\n    # Second request, cache should return response\n    request = self.factory.head('/path/to/resource', HTTP_ACCEPT_ENCODING='gzip')\n    self.assertIsNotNone(middleware.process_request(request))\n\n    # Third request with different Accept-Encoding, cache should not return response\n    request = self.factory.head('/path/to/resource', HTTP_ACCEPT_ENCODING='deflate')\n    self.assertIsNone(middleware.process_request(request))\n", "    def setUp(self):\n        super().setUp()\n        self.factory = RequestFactory()\n"], "sample_1143": ["def test_issue_5098():\n    assert S.Half.is_Rational\n    assert S.Half.is_rational\n    assert S.Pi.is_real\n    assert S.Pi.is_irrational\n", "def test_issue_12345():\n    # Test for issue 12345: Float precision handling\n    assert same_and_same_prec(Float(1.0, dps=Integer(15)), Float(1.0, precision=Integer(53)))\n    assert same_and_same_prec(Float(1.0, dps=Integer(20)), Float(1.0, precision=Integer(66)))\n    assert same_and_same_prec(Float(1.0, dps=Integer(25)), Float(1.0, precision=Integer(79)))\n", "def test_evalf():\n    p = S.Pi.evalf(10)\n    assert isinstance(p, Float) is True\n    assert p._prec == 10\n\n    e = S.E.evalf(10)\n    assert isinstance(e, Float) is True\n    assert e._prec == 10\n\n    g = S.GoldenRatio.evalf(10)\n    assert isinstance(g, Float) is True\n    assert g._prec == 10\n\n    c = S.Catalan.evalf(10)\n    assert isinstance(c, Float) is True\n    assert c._prec == 10\n\n    eg = S.EulerGamma.evalf(10)\n    assert isinstance(eg, Float) is True\n    assert eg._prec == 10\n\n    tc = S.TribonacciConstant.evalf(10)\n    assert isinstance(tc, Float) is True\n    assert tc._prec == 10\n", "def test_issue_6966():\n    # Test that Float instances are not considered equal to themselves when they\n    # are created with a different precision\n    f1 = Float('1.2', 5)\n    f2 = Float('1.2', 10)\n    assert f1 == f2\n    assert f1 is not f2\n", "def test_issue_16942():\n    # Check that Float handles pickled mpmath numbers\n    # issue #16942\n    import pickle\n\n    # Create an mpmath number\n    mpf = mpmath.mpf('0.123456789')\n\n    # Pickle it\n    pickled_mpf = pickle.dumps(mpf)\n\n    # Unpickle it into a SymPy Float\n    unpickled_float = pickle.loads(pickled_mpf)\n\n    # Create another SymPy Float with the same value\n    expected_float = Float('0.123456789')\n\n    # Check that they are the same\n    assert unpickled_float == expected_float\n"], "sample_1009": ["def test_vector_magnitude():\n    x, y, z = symbols('x, y, z')\n    N = ReferenceFrame('N')\n    \n    v = x * N.x + y * N.y + z * N.z\n    assert v.magnitude() == sqrt(x**2 + y**2 + z**2)\n\n    v = N.x + N.y + N.z\n    assert v.magnitude() == sqrt(3)\n\n    v = 0 * N.x\n    assert v.magnitude() == 0\n", "def test_vectorNormalize():\n    x, y, z = symbols('x y z')\n    N = ReferenceFrame('N')\n    v1 = x * N.x + y * N.y + z * N.z\n    assert v1.normalize() == (x * N.x + y * N.y + z * N.z) / sqrt(x**2 + y**2 + z**2)\n    v2 = Vector(0)\n    assert v2.normalize() == Vector(0)\n    raises(ZeroDivisionError, lambda: v2.normalize())\n", "def test_vector_normalize():\n    x, y, z = symbols('x, y, z')\n    N = ReferenceFrame('N')\n\n    # Test normalization of a simple vector\n    v = 3 * N.x + 4 * N.y\n    assert v.normalize() == v / 5\n\n    # Test normalization of a vector with a symbolic magnitude\n    v = x * N.x + y * N.y + z * N.z\n    mag = v.magnitude()\n    assert v.normalize() == v / mag\n\n    # Test normalization of a zero vector\n    v = Vector(0)\n    assert v.normalize() == v\n\n    # Test that the normalized vector has the correct magnitude\n    v = x * N.x + y * N.y + z * N.z\n    assert v.normalize().magnitude() == 1\n\n    # Test that the normalized vector is the same as the original vector for unit vectors\n    v = N.x\n    assert v.normalize() == v\n", "def test_vector_to_matrix():\n    x, y, z = symbols('x y z')\n    N = ReferenceFrame('N')\n    A = ReferenceFrame('A')\n    v = x * N.x + y * N.y + z * N.z\n    v_A = v.express(A)\n    assert v.to_matrix(N) == Matrix([[x], [y], [z]])\n    assert v_A.to_matrix(A) == Matrix([[x], [y], [z]])\n    assert v_A.to_matrix(N) == Matrix([[x], [y], [z]])\n    assert v.to_matrix(A) == Matrix([[x], [y], [z]])\n", "def test_vector_cross_product():\n    q1, q2 = dynamicsymbols('q1 q2')\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Axis', [q1, N.x])\n    B = A.orientnew('B', 'Axis', [q2, A.y])\n\n    assert N.x ^ N.x == Vector(0)\n    assert N.x ^ N.y == N.z\n    assert N.y ^ N.x == -N.z\n    assert N.y ^ N.z == N.x\n    assert N.z ^ N.y == -N.x\n    assert N.z ^ N.z == Vector(0)\n\n    assert A.x ^ A.x == Vector(0)\n    assert A.x ^ A.y == A.z\n    assert A.y ^ A.x == -A.z\n    assert A.y ^ A.z == A.x\n    assert A.z ^ A.y == -A.x\n    assert A.z ^ A.z == Vector(0)\n\n    assert B.x ^ B.x == Vector(0)\n    assert B.x ^ B.y == B.z\n    assert B.y ^ B.x == -B.z\n    assert B.y ^ B.z == B.x\n    assert B.z ^ B.y == -B.x\n    assert B.z ^ B.z == Vector(0)\n\n    assert N.x ^ A.x == Vector(0)\n    assert N.y ^ A.y == -q1 * A.z\n    assert N.z ^ A.z == Vector(0)\n    assert N.x ^ B.x == (sin(q2) * sin(q1) * A.y - cos(q2) * sin(q1) * A.z)\n    assert N.y ^ B.y == sin(q2) * A.x\n    assert N.z ^ B.z == -sin(q1) * A.x\n"], "sample_250": ["def test_timezone_with_dst_transition(self):\n    # Create a datetime object during a DST transition\n    tz = get_default_timezone()\n    dt = datetime(2023, 3, 26, 1, 30, tzinfo=tz)\n\n    # Test that the timezone offset is correct\n    self.assertEqual(dateformat.format(dt, 'O'), '+0200')\n\n    # Test that the timezone name is correct\n    self.assertEqual(dateformat.format(dt, 'T'), 'CEST')\n\n    # Test that the timezone offset in seconds is correct\n    self.assertEqual(dateformat.format(dt, 'Z'), '7200')\n\n    # Test that the ISO 8601 year number is correct\n    self.assertEqual(dateformat.format(dt, 'o'), '2023')\n", "def test_format_z(self):\n    # Test 'z' format with a date that doesn't fall on January 1\n    dt = datetime(2000, 1, 2)\n    self.assertEqual(dateformat.format(dt, 'z'), '2')\n\n    # Test 'z' format with a date that falls on December 31\n    dt = datetime(2000, 12, 31)\n    self.assertEqual(dateformat.format(dt, 'z'), '366')\n\n    # Test 'z' format with a date that falls on February 29 in a leap year\n    dt = datetime(2000, 2, 29)\n    self.assertEqual(dateformat.format(dt, 'z'), '60')\n\n    # Test 'z' format with a date that falls on February 28 in a non-leap year\n    dt = datetime(2001, 2, 28)\n    self.assertEqual(dateformat.format(dt, 'z'), '59')\n", "def test_format_with_invalid_characters(self):\n    dt = datetime(2022, 1, 1)\n    with self.assertRaises(ValueError):\n        dateformat.format(dt, 'Invalid % character')\n", "    def test_time_format_with_tzinfo(self):\n        tz = get_fixed_timezone(-210)\n        dt = datetime(2009, 5, 16, 5, 30, 30, tzinfo=tz)\n        expected_date = datetime(2009, 5, 16, 8, 30, 30)\n\n        self.assertEqual(dateformat.format(dt, 'G'), '08')\n        self.assertEqual(dateformat.format(dt, 'g'), '8')\n        self.assertEqual(dateformat.format(dt, 'h'), '08')\n        self.assertEqual(dateformat.format(dt, 'H'), '08')\n        self.assertEqual(dateformat.format(dt, 'i'), '30')\n        self.assertEqual(dateformat.format(dt, 's'), '30')\n        self.assertEqual(dateformat.format(dt, 'f'), '8:30')\n        self.assertEqual(dateformat.format(dt, 'P'), '8:30 a.m.')\n\n        # Test time format specifiers with tzinfo\n        self.assertEqual(dateformat.format(dt, 'e'), '-0330')\n        self.assertEqual(dateformat.format(dt, 'O'), '-0330')\n        self.assertEqual(dateformat.format(dt, 'T'), 'EET')\n        self.assertEqual(dateformat.format(dt, 'Z'), '-19800')\n\n        # Test timezone offset\n        dt = datetime(2009, 5, 16, 5, 30, 30, tzinfo=get_fixed_timezone(-210))\n        self.assertEqual(dateformat.format(dt, 'e'), '-0330')\n        self.assertEqual(dateformat.format(dt, 'O'), '-0330')\n        self.assertEqual(dateformat.format(dt, 'T'), 'EET')\n        self.assertEqual(dateformat.format(dt, 'Z'), '-19800')\n", "def test_timezone_edge_cases(self):\n    # Test the edge cases around DST transitions.\n    # Europe/London has a transition from +0100 to +0000 on the last Sunday in October.\n    # This is 2005-10-30 02:00 in the Europe/London timezone, but 2005-10-30 01:00 UTC.\n    naive_dt = datetime(2005, 10, 30, 1, 0)\n    tz = get_fixed_timezone(0)  # UTC\n    aware_dt = make_aware(naive_dt, tz)\n    \n    # Test that we correctly handle the naive datetime in the ambiguous hour.\n    self.assertEqual(dateformat.format(naive_dt, 'T'), '')\n    \n    # Test that we correctly handle the aware datetime in the ambiguous hour.\n    self.assertEqual(dateformat.format(aware_dt, 'T'), 'UTC')\n    \n    # Test that we correctly handle the aware datetime in the non-ambiguous hour.\n    aware_dt = make_aware(datetime(2005, 10, 30, 3, 0), tz)\n    self.assertEqual(dateformat.format(aware_dt, 'T'), 'UTC')\n"], "sample_3": ["def test_specialized_columns_multidim():\n    \"\"\"Test writing and reading of multidimensional columns\"\"\"\n    t = Table()\n    t['1-d'] = np.array([np.array([1, 2], dtype=np.uint8),\n                         np.array([3, 4, 5], dtype=np.uint8)], dtype=object)\n    t['2-d'] = np.arange(8, dtype=np.float16).reshape(2, 2, 2)\n    t['3-d'] = np.arange(24, dtype=np.float16).reshape(2, 3, 4)\n    out = StringIO()\n    t.write(out, format='ascii.ecsv')\n    t2 = Table.read(out.getvalue(), format='ascii.ecsv')\n    assert t2.colnames == t.colnames\n    for name in t2.colnames:\n        assert t2[name].dtype == t[name].dtype\n        for val1, val2 in zip(t2[name], t[name]):\n            if isinstance(val1, np.ndarray):\n                assert val1.dtype == val2.dtype\n            assert np.all(val1 == val2)\n", "def test_ecsv_round_trip_masked_string_column(tmpdir):\n    \"\"\"Test round-trip of masked string column\"\"\"\n    filename = str(tmpdir.join('test.ecsv'))\n    t = Table()\n    t['a'] = MaskedColumn(['a', 'b', 'c', ''], dtype=str)\n    t['a'].mask[0] = True\n    t.write(filename, format='ascii.ecsv', serialize_method='data_mask')\n    t2 = Table.read(filename, format='ascii.ecsv')\n    assert np.all(t2['a'].mask == t['a'].mask)\n    assert np.all(t2['a'] == t['a'])\n    assert t2['a'].dtype == t['a'].dtype\n", "def test_multidim_array_with_missing_values(tmpdir):\n    \"\"\"\n    Test multidim array with missing values in ECSV.\n    \"\"\"\n    t = Table()\n    t['a'] = np.arange(24, dtype=np.int64).reshape(2, 3, 4)\n    t['a'][0, 0, 0] = np.ma.masked\n\n    filename = str(tmpdir.join('test.ecsv'))\n    t.write(filename, format='ascii.ecsv')\n\n    t2 = Table.read(filename, format='ascii.ecsv')\n    assert np.all(t2['a'].data == t['a'].data)\n    assert np.all(t2['a'].mask == t['a'].mask)\n", "def test_ecsv_read_multiline_columns(tmpdir):\n    \"\"\"Test that multi-line columns (e.g. caused by long descriptions or units)\n    in the ECSV header are read correctly.\"\"\"\n    filename = str(tmpdir.join('test.ecsv'))\n\n    t = Table()\n    t['a'] = np.arange(3)\n    t['a'].info.description = 'This is a very long description that needs '\n    t['a'].info.description += 'to be split over multiple lines.'\n    t.write(filename, format='ascii.ecsv')\n\n    t2 = Table.read(filename)\n    assert t2['a'].info.description == t['a'].info.description\n", "def test_ecsv_round_trip_mixin_serialization():\n    \"\"\"\n    Test that ECSV round-trips mixin serialization using data_mask and null_value\n    methods.  This tests the new functionality where mixin columns are stored in\n    the YAML header.\n    \"\"\"\n    from astropy.table.column import Time, SkyCoord\n    t = QTable()\n    t['a'] = np.arange(2) * u.km\n    t['b'] = np.arange(2)\n    t['c'] = np.arange(2) * u.deg\n    t['d'] = np.arange(2)\n    t['e'] = np.arange(2) * u.km\n    t['f'] = np.arange(2)\n    t['g'] = np.arange(2) * u.deg\n    t['h'] = np.arange(2)\n    t['i'] = np.arange(2) * u.km\n    t['j'] = np.arange(2)\n    t['k'] = Time(['1999-01-01T00:00:00', '1999-01-02T00:00:00'], format='isot')\n    t['l'] = SkyCoord(ra=[10, 20]*u.deg, dec=[30, 40]*u.deg, distance=[10, 20]*u.pc)\n    t.meta['test'] = 'round-trip test'\n    t['a'].meta['test'] = 'round-trip test'\n\n    out = StringIO()\n    t.write(out, format='ascii.ecsv', serialize_method='data_mask')\n    t2 = QTable.read(out.getvalue(), format='ascii.ecsv')\n\n    assert t.colnames == t2.colnames\n    for name in t.colnames:\n        assert t[name].dtype == t2[name].dtype\n        assert np.all(t[name] == t2[name])\n        if hasattr(t[name], 'mask'):\n            assert np.all(t[name].mask == t2[name].mask)\n"], "sample_570": ["    def test_clip(self, rng):\n\n        x = rng.normal(0, 3, 100)\n        clip = -1, 1\n        kde = KDE(clip=clip, cumulative=True)\n        _, support = kde(x)\n\n        assert support.min() >= clip[0]\n        assert support.max() <= clip[1]\n", "    def test_complementary_not_implemented(self, rng):\n        x = rng.normal(0, 3, 100)\n        kde = KDE(cumulative=True)\n        with pytest.raises(NotImplementedError):\n            kde(x)\n", "    def test_support_assignment(self, rng):\n\n        x = rng.normal(0, 3, 100)\n        kde = KDE()\n        kde.define_support(x)\n        assert kde.support is not None\n", "    def test_errorbar_none(self, long_df):\n\n        agg = EstimateAggregator(\"mean\", errorbar=None)\n        out = agg(long_df, \"y\")\n        assert \"ymin\" not in out\n        assert \"ymax\" not in out\n", "    def test_invalid_estimator(self, long_df):\n\n        with pytest.raises(ValueError, match=\"Estimator must be a callable or string\"):\n            agg = EstimateAggregator(5)\n            agg(long_df, \"x\")\n"], "sample_797": ["def test_power_transformer_yeo_johnson_negative_lambda():\n    # Make sure negative lambdas work for Yeo-Johnson\n    X = np.abs(X_1col)\n    pt = PowerTransformer(method='yeo-johnson', standardize=False)\n\n    pt.lambdas_ = np.array([-1.5])\n    X_trans = pt.transform(X)\n    X_inv = pt.inverse_transform(X_trans)\n    assert_array_almost_equal(X, X_inv)\n", "def test_power_transformer_transform_negative_values():\n    # Test transform method with negative values\n    X = np.array([[-1.0, 0.5], [0.0, 1.0], [1.0, -0.5]])\n    pt = PowerTransformer(method='yeo-johnson')\n    pt.fit(X)\n\n    X_trans = pt.transform(X)\n    assert_array_less(X_trans, np.inf)\n    assert_array_less(-np.inf, X_trans)\n\n    pt = PowerTransformer(method='box-cox')\n    assert_raise_message(ValueError, \"The Box-Cox transformation can only be \"\n                         \"applied to strictly positive data\", pt.fit, X)\n", "def test_power_transformer_boundaries():\n    # Make sure that PowerTransformer handles boundaries correctly.\n\n    # Test the boundary 0. for both methods\n    pt_boxcox = PowerTransformer(method='box-cox')\n    pt_yeojohnson = PowerTransformer(method='yeo-johnson')\n\n    X = np.array([BOUNDS_THRESHOLD, 1., 2.]).reshape(-1, 1)\n    assert_array_almost_equal(pt_boxcox.fit_transform(X), X)\n    assert_array_almost_equal(pt_yeojohnson.fit_transform(X), X)\n\n    # Test the boundaries for yeo-johnson\n    X = np.array([BOUNDS_THRESHOLD, 1., 2., 0.]).reshape(-1, 1)\n    assert_array_almost_equal(pt_yeojohnson.fit_transform(X), X)\n\n    # Test that inverse transform does not break on these boundaries\n    assert_array_almost_equal(pt_boxcox.inverse_transform(pt_boxcox.fit_transform(X)), X)\n    assert_array_almost_equal(pt_yeojohnson.inverse_transform(pt_yeojohnson.fit_transform(X)), X)\n", "def test_power_transformer_output_distribution(method):\n    # Check that output_distribution parameter correctly transforms data\n    rng = np.random.RandomState(0)\n    n_samples = 100\n    n_features = 2\n    X = rng.randn(n_samples, n_features)\n\n    pt_normal = PowerTransformer(method=method, output_distribution='normal')\n    X_trans_normal = pt_normal.fit_transform(X)\n\n    pt_uniform = PowerTransformer(method=method, output_distribution='uniform')\n    X_trans_uniform = pt_uniform.fit_transform(X)\n\n    assert not np.allclose(X_trans_normal, X_trans_uniform)\n    assert pt_normal.quantiles_.shape[0] == pt_uniform.quantiles_.shape[0]\n    assert pt_normal.quantiles_.shape[1] == pt_uniform.quantiles_.shape[1]\n\n    # Make sure that the first two moments are approximately 0 and 1 for\n    # normal output distribution\n    assert np.allclose(X_trans_normal.mean(axis=0), np.zeros(n_features))\n    assert np.allclose(X_trans_normal.std(axis=0), np.ones(n_features))\n\n    # Make sure that the min and max of the uniform output are approximately\n    # 0 and 1\n    assert np.allclose(X_trans_uniform.min(axis=0), np.zeros(n_features))\n    assert np.allclose(X_trans_uniform.max(axis=0), np.ones(n_features))\n", "def test_power_transformer_quantile_range_edge_cases():\n    X = np.abs(X_2d)\n\n    edge_cases = [\n        (0, 100),\n        (0, 100.5),\n        (100, 100),\n        (0, 0.00001),\n        (99.999, 100.001),\n        (50.000, 50.001),\n    ]\n\n    for quantile_range in edge_cases:\n        pt = PowerTransformer(quantile_range=quantile_range)\n        pt.fit(X)\n\n        # fit should not fail and the output should be equal to the input\n        X_trans = pt.transform(X)\n        assert_array_almost_equal(X, X_trans)\n\n        # inverse transform should also return the original input\n        X_inv_trans = pt.inverse_transform(X_trans)\n        assert_array_almost_equal(X, X_inv_trans)\n"], "sample_530": ["def test_AnnotationBbox_extents():\n    plt.rcParams.update(plt.rcParamsDefault)\n    fig, ax = plt.subplots(figsize=(4, 3), dpi=100)\n\n    ax.axis([0, 1, 0, 1])\n\n    # Test AnnotationBbox\n    da = DrawingArea(20, 20, 0, 0, clip=True)\n    p = mpatches.Circle((-10, 30), 32)\n    da.add_artist(p)\n\n    ab3 = AnnotationBbox(da, [.5, .5], xybox=(-0.2, 0.5), xycoords='data',\n                         boxcoords=\"axes fraction\", box_alignment=(0., .5),\n                         arrowprops=dict(arrowstyle=\"->\"))\n    ax.add_artist(ab3)\n\n    fig.canvas.draw()\n    renderer = fig.canvas.get_renderer()\n\n    bb3w = ab3.get_window_extent(renderer)\n    bb3e = ab3.get_tightbbox(renderer)\n\n    target3 = [-17.6, 129.0, 200.7, 167.9]\n    assert_allclose(bb3w.extents, target3, atol=2)\n    assert_allclose(bb3e.extents, target3, atol=2)\n", "def test_offsetimage():\n    fig, ax = plt.subplots()\n    im = np.random.rand(10, 10)\n    offsetbox = OffsetImage(im, zoom=3)\n    ab = AnnotationBbox(offsetbox, (0.5, 0.5), xybox=(0, 75),\n                        xycoords='axes fraction', boxcoords=\"offset points\",\n                        pad=0.3, arrowprops=dict(arrowstyle=\"->\"))\n    ax.add_artist(ab)\n    ax.axis([0, 1, 0, 1])\n\n    fig.canvas.draw()\n    renderer = fig.canvas.get_renderer()\n\n    # Test AnnotationBbox\n    bb = ab.get_window_extent(renderer)\n\n    target = [180.0, -32.0, 230.0, 92.9]\n    assert_allclose(bb.extents, target, atol=2)\n\n    offsetbox.set_zoom(4)\n    fig.canvas.draw()\n    renderer = fig.canvas.get_renderer()\n\n    bb = ab.get_window_extent(renderer)\n    target = [240.0, -42.7, 305.0, 123.4]\n    assert_allclose(bb.extents, target, atol=2)\n\n    offsetbox.set_data(np.random.rand(10, 10))\n    fig.canvas.draw()\n    renderer = fig.canvas.get_renderer()\n\n    bb = ab.get_window_extent(renderer)\n    target = [240.0, -42.7, 305.0, 123.4]\n    assert_allclose(bb.extents, target, atol=2)\n", "def test_aux_transform_box():\n    # Test AuxTransformBox by checking its transform\n    import matplotlib.transforms as mtransforms\n\n    # Create a figure and an axes\n    fig, ax = plt.subplots()\n\n    # Create a transform\n    aux_transform = mtransforms.Affine2D().rotate_deg(45)\n\n    # Create an AuxTransformBox\n    aux_box = AuxTransformBox(aux_transform)\n\n    # Add some children to the AuxTransformBox\n    aux_box.add_artist(mpatches.Circle((0, 0), 1, edgecolor='black'))\n    aux_box.add_artist(mpatches.Rectangle((-1, -1), 2, 2, edgecolor='red'))\n\n    # Add the AuxTransformBox to the axes\n    ax.add_artist(aux_box)\n\n    # Set the limits of the axes\n    ax.set_xlim(-5, 5)\n    ax.set_ylim(-5, 5)\n\n    # Draw the figure\n    fig.canvas.draw()\n\n    # Check that the transform of the AuxTransformBox is correct\n    assert_allclose(aux_box.get_transform().get_matrix(),\n                    mtransforms.Affine2D().rotate_deg(45).get_matrix())\n", "def test_auxtransformbox():\n    fig, ax = plt.subplots()\n\n    aux_transform = mtransforms.Affine2D().scale(2).rotate_deg(45)\n\n    aux_box = AuxTransformBox(aux_transform)\n\n    offsetbox = TextArea(\"test\")\n    aux_box.add_artist(offsetbox)\n\n    ax.add_artist(aux_box)\n\n    ax.set_xlim(-5, 5)\n    ax.set_ylim(-5, 5)\n\n    fig.canvas.draw()\n\n    # Check the text is rotated and scaled correctly.\n    renderer = fig.canvas.get_renderer()\n    bb = offsetbox.get_window_extent(renderer)\n    assert_allclose(bb.center, (0, 0), atol=1)\n\n    # Check that aux_box.get_extent is correct\n    w, h, xd, yd = aux_box.get_extent(renderer)\n    assert_allclose(w, h * np.sqrt(2), atol=1)\n    assert_allclose(xd, yd, atol=1)\n", "def test_aux_transform_box():\n    fig, ax = plt.subplots()\n    aux_transform = mtransforms.Affine2D().scale(2).rotate_deg(45)\n    atb = AuxTransformBox(aux_transform)\n    da = DrawingArea(10, 10)\n    atb.add_artist(da)\n    ax.add_artist(atb)\n    ax.set_xlim(0, 20)\n    ax.set_ylim(0, 20)\n    fig.canvas.draw()\n\n    atb.set_offset((5, 5))\n    fig.canvas.draw()\n\n    assert_allclose(atb.get_offset(), (5, 5))\n\n    da = DrawingArea(5, 5)\n    atb.add_artist(da)\n    fig.canvas.draw()\n\n    atb.set_offset((10, 10))\n    fig.canvas.draw()\n\n    assert_allclose(atb.get_offset(), (10, 10))\n\n    # Test window extent\n    bb1 = atb.get_window_extent()\n    assert_allclose(bb1.extents, [5, 5, 15, 15], atol=1)\n\n    aux_transform = mtransforms.Affine2D().scale(4)\n    atb = AuxTransformBox(aux_transform)\n    da = DrawingArea(5, 5)\n    atb.add_artist(da)\n    ax.add_artist(atb)\n    fig.canvas.draw()\n\n    # Test window extent\n    bb2 = atb.get_window_extent()\n    assert_allclose(bb2.extents, [5, 5, 25, 25], atol=1)\n"], "sample_996": ["def test_doit_with_hints():\n    assert Product(x, (x, 1, n)).doit(deep=False) == Product(x, (x, 1, n))\n    assert Product(x, (x, 1, n)).doit(deep=True) == x**n\n", "def test__eval_is_zero():\n    i = Symbol('i', integer=True)\n    assert Product(0, (i, 1, 10)).doit().is_zero\n    assert Product(i, (i, 0, 10)).doit().is_zero\n    assert not Product(1, (i, 1, 10)).doit().is_zero\n    assert not Product(i + 1, (i, 0, 10)).doit().is_zero\n", "def test_infinite_product_polynomial():\n    assert Product(k**2 + 1, (k, 1, oo)).is_convergent() is S.false\n    assert Product(1/(k**2 + 1), (k, 1, oo)).is_convergent() is S.true\n    assert Product(k/(k + 1), (k, 1, oo)).is_convergent() is S.false\n    assert Product((k + 1)/k, (k, 1, oo)).is_convergent() is S.false\n    assert Product(k/(k**2 + 1), (k, 1, oo)).is_convergent() is S.true\n    assert Product((k**2 + 1)/k, (k, 1, oo)).is_convergent() is S.false\n", "def test_reverse_order_on_non_integer_limits():\n    x, y, a, b, c, d = symbols('x, y, a, b, c, d')\n\n    assert Product(x, (x, 1, 3)).reverse_order(x) == Product(1/x, (x, 4, 0))\n    assert Product(x*y, (x, 1, 5), (y, 0, 6)).reverse_order(x, 1) == \\\n           Product(x*y, (x, 6, 0), (y, 7, -1))\n    assert Product(x, (x, a, b)).reverse_order(x) == Product(1/x, (x, b + 1, a - 1))\n    assert Product(x*y, (x, a, b), (y, c, d)).reverse_order(x, y) == \\\n           Product(x*y, (y, d + 1, c - 1), (x, b + 1, a - 1))\n\n    assert Product(x, (x, 1, 3)).reverse_order(0) == Product(1/x, (x, 4, 0))\n    assert Product(x*y, (x, 1, 5), (y, 0, 6)).reverse_order(0, 1) == \\\n           Product(x*y, (x, 6, 0), (y, 7, -1))\n    assert Product(x, (x, a, b)).reverse_order(0) == Product(1/x, (x, b + 1, a - 1))\n    assert Product(x*y, (x, a, b), (y, c, d)).reverse_order(0, 1) == \\\n           Product(x*y, (y, d + 1, c - 1), (x, b + 1, a - 1))\n", "def test_eval_product_with_polynomial():\n    # Test _eval_product with a polynomial term\n    assert Product(k**2 + 1, (k, 1, n)).doit() == Product(k**2 + 1, (k, 1, n))\n    assert Product(k**2 + 1, (k, 1, 3)).doit() == 10\n    assert Product(k**2 + 2*k + 1, (k, 1, n)).doit() == Product(k**2 + 2*k + 1, (k, 1, n))\n    assert Product(k**2 + 2*k + 1, (k, 1, 3)).doit() == 208\n    assert Product(k**2 - 4, (k, -2, 2)).doit() == 0\n    assert Product(k**2 - 4, (k, -2, 1)).doit() == -15\n    assert Product(k**2 - 4, (k, 0, 2)).doit() == -24\n    assert Product(k**2 - 4, (k, 1, 2)).doit() == -12\n    assert Product(k**2 - 4, (k, 2, 2)).doit() == 0\n"], "sample_901": ["def test_k_means_negative_sample_weight():\n    # Check that an error is raised when passing negative sample weights\n    km = KMeans(n_clusters=n_clusters, random_state=42)\n    sample_weight = np.ones(n_samples) - 1\n    msg = \"sample_weight cannot contain negative weights\"\n    assert_raises_regex(ValueError, msg, km.fit, X, sample_weight=sample_weight)\n", "def test_k_means_init_on_subsample():\n    # Test that k-means++ initialization works correctly on a subsample of data\n    km = KMeans(n_clusters=n_clusters, init=\"k-means++\", random_state=42,\n                n_init=1, init_size=n_samples // 2)\n    km.fit(X)\n    _check_fitted_model(km)\n", "def test_k_means_premature_stop():\n    # test that the algorithm stops after n_init=1 even if max_iter > 1\n    n_init = 1\n    max_iter = 5\n    km = KMeans(n_clusters=n_clusters, init=\"k-means++\", n_init=n_init,\n                max_iter=max_iter, random_state=42)\n    km.fit(X)\n    assert km.n_iter_ == 1\n", "def test_k_means_random_state():\n    # Test if KMeans provides the same results with different seeds\n    rng1 = np.random.RandomState(0)\n    rng2 = np.random.RandomState(0)\n\n    X = rng1.normal(size=(100, 10))\n    X_csr = sp.csr_matrix(X)\n\n    km1 = KMeans(n_clusters=5, random_state=rng1, n_init=1)\n    km1.fit(X)\n    labels1 = km1.labels_\n    centers1 = km1.cluster_centers_\n\n    km2 = KMeans(n_clusters=5, random_state=rng2, n_init=1)\n    km2.fit(X)\n    labels2 = km2.labels_\n    centers2 = km2.cluster_centers_\n\n    km1_csr = KMeans(n_clusters=5, random_state=rng1, n_init=1)\n    km1_csr.fit(X_csr)\n    labels1_csr = km1_csr.labels_\n    centers1_csr = km1_csr.cluster_centers_\n\n    km2_csr = KMeans(n_clusters=5, random_state=rng2, n_init=1)\n    km2_csr.fit(X_csr)\n    labels2_csr = km2_csr.labels_\n    centers2_csr = km2_csr.cluster_centers_\n\n    assert_array_equal(labels1, labels2)\n    assert_array_almost_equal(centers1, centers2)\n    assert_array_equal(labels1_csr, labels2_csr)\n    assert_array_almost_equal(centers1_csr, centers2_csr)\n", "def test_k_means_on_large_sparse_matrix():\n    # Issue GH #13801\n    data = sp.csr_matrix(np.random.randint(0, 2, size=(2000, 10000)))\n    kmeans = KMeans(n_clusters=5, n_init=1, init='k-means++', random_state=0,\n                    n_jobs=-1)\n    kmeans.fit(data)\n"], "sample_1137": ["def test_convert_to_with_Pow():\n    expr = speed_of_light**2\n    target_units = [meter, second]\n    converted_expr = convert_to(expr, target_units)\n    assert converted_expr == 89875517873681764 * meter**2 / second**2\n", "def test_convert_to_dimensionless():\n    assert convert_to(gravitational_constant, joule * meter**3 / kilogram**2) == gravitational_constant\n    assert convert_to(gravitational_constant, joule * meter**3 / kilogram**2) == 6.67430e-11*joule*meter**3/kilogram**2\n", "def test_convert_to_with_quantity_simplify():\n    q1 = Quantity('q1')\n    q2 = Quantity('q2')\n\n    q1.set_global_relative_scale_factor(S(10), meter)\n    q2.set_global_relative_scale_factor(S(5), meter)\n\n    expr = q1 + q2\n\n    conv = convert_to(expr, meter)\n    assert conv == 15*meter\n\n    q_simplified = quantity_simplify(conv)\n    assert q_simplified == 15*meter\n\n", "def test_quantity_simplify():\n    assert quantity_simplify(kilo*meter) == 1000*meter\n    assert quantity_simplify(kilo*meter/second) == 1000*meter/second\n    assert quantity_simplify(meter/second) == meter/second\n    assert quantity_simplify(kilo*meter**2) == 1000000*meter**2\n    assert quantity_simplify(meter**2) == meter**2\n\n    # test with multiple units\n    assert quantity_simplify(kilo*meter*second) == 1000*meter*second\n    assert quantity_simplify(meter*second) == meter*second\n\n    # test with prefixes\n    from sympy.physics.units.prefixes import kilo, centi\n    assert quantity_simplify(kilo*centi*meter) == 0.1*meter\n    assert quantity_simplify(kilo*centi*meter**2) == 0.01*meter**2\n\n    # test with quantities\n    from sympy.physics.units.definitions import coulomb\n    assert quantity_simplify(kilo*coulomb) == 1000*coulomb\n    assert quantity_simplify(coulomb) == coulomb\n\n    # test with expressions\n    assert quantity_simplify(kilo*meter + meter) == 1001*meter\n    assert quantity_simplify(kilo*meter - meter) == 999*meter\n    assert quantity_simplify(kilo*meter**2 + meter**2) == 1000001*meter**2\n    assert quantity_simplify(kilo*meter**2 - meter**2) == 999999*meter**2\n", "def test_convert_to_edge_cases():\n    # Test conversion of a pure number\n    assert convert_to(5, meter) == 5\n\n    # Test conversion of a unit with no dimension\n    assert convert_to(Quantity('dimensionless'), meter) == Quantity('dimensionless')\n\n    # Test conversion of a unit with incompatible dimension\n    assert convert_to(Quantity('incompatible'), meter) == Quantity('incompatible')\n\n    # Test conversion of a unit with no scale factor\n    q = Quantity('no_scale_factor')\n    assert convert_to(q, meter) == q\n\n    # Test conversion of a unit with a scale factor that is not a number\n    q = Quantity('non_numeric_scale_factor')\n    q.set_global_relative_scale_factor(Symbol('x'), meter)\n    assert convert_to(q, meter) == q\n\n    # Test conversion of a unit with multiple scale factors\n    q = Quantity('multiple_scale_factors')\n    q.set_global_relative_scale_factor(2 * meter, meter)\n    q.set_global_relative_scale_factor(3 * kilo, kilo)\n    assert convert_to(q, meter) == 6000 * meter\n\n    # Test conversion of a unit with a scale factor that is a prefix\n    q = Quantity('prefix_scale_factor')\n    q.set_global_relative_scale_factor(kilo, meter)\n    assert convert_to(q, meter) == 1000 * meter\n\n    # Test conversion of a unit with a scale factor that is a unit\n    q = Quantity('unit_scale_factor')\n    q.set_global_relative_scale_factor(meter, meter)\n    assert convert_to(q, meter) == meter**2\n"], "sample_285": ["def test_file_system_finder_find(self):\n    static_dir = Path(TEST_ROOT) / 'project' / 'static'\n    with self.settings(STATICFILES_DIRS=[static_dir]):\n        finder = get_finder('django.contrib.staticfiles.finders.FileSystemFinder')\n        self.assertEqual(finder.find('file.txt'), str(static_dir / 'file.txt'))\n        self.assertEqual(finder.find('non_existent_file.txt'), None)\n        self.assertEqual(finder.find('file.txt', all=True), [str(static_dir / 'file.txt')])\n        self.assertEqual(finder.find('non_existent_file.txt', all=True), [])\n", "    def test_find(self):\n        \"\"\"find() returns the first found file.\"\"\"\n        static_dir = Path(TEST_ROOT) / 'project' / 'documents'\n        static_file = static_dir / 'test.txt'\n        static_file.touch()\n        finder = get_finder('django.contrib.staticfiles.finders.FileSystemFinder')\n        finder.locations = [(None, static_dir)]\n        finder.storages = {static_dir: FileSystemStorage(location=static_dir)}\n        self.assertEqual(finder.find('test.txt'), str(static_file))\n", "def test_base_storage_finder_find(self):\n    storage = mock.Mock()\n    storage.exists.return_value = True\n    storage.path.return_value = '/path/to/file'\n    finder = BaseStorageFinder(storage=storage)\n    self.assertEqual(finder.find('path/to/file'), '/path/to/file')\n", "def test_find_all(self):\n    \"\"\"\n    Test the find() function with all=True.\n    \"\"\"\n    finder = get_finder('django.contrib.staticfiles.finders.FileSystemFinder')\n    static_dir = Path(TEST_ROOT) / 'project' / 'documents'\n    with self.settings(STATICFILES_DIRS=[(static_dir, static_dir)]):\n        matches = finder.find('file.txt', all=True)\n        self.assertEqual(len(matches), 1)\n        self.assertEqual(matches[0], str(static_dir / 'file.txt'))\n", "def test_app_directories_finder_list(self):\n    with self.settings(STATICFILES_FINDERS=[\n        'django.contrib.staticfiles.finders.AppDirectoriesFinder',\n    ], INSTALLED_APPS=[\n        'tests.contrib.staticfiles.finders.test_finders.test_app',\n    ]):\n        finder = get_finder('django.contrib.staticfiles.finders.AppDirectoriesFinder')\n        files = list(finder.list(None))\n        self.assertEqual(len(files), 1)\n        file_path, storage = files[0]\n        self.assertEqual(file_path, 'test.txt')\n        self.assertIsInstance(storage, finder.storage_class)\n"], "sample_1150": ["def test_issue_18146b():\n    from sympy.abc import x\n    assert Range(0, 8, 2).contains(x) == Contains(x, Range(0, 8, 2))\n    assert Range(0, 8, 2).contains(x, evaluate=False) == Contains(x, Range(0, 8, 2), evaluate=False)\n    assert Range(0, 8, 2).contains(2*x) == Contains(2*x, Range(0, 8, 2))\n    assert Range(0, 8, 2).contains(2*x, evaluate=False) == Contains(2*x, Range(0, 8, 2), evaluate=False)\n", "def test_ComplexRegion_polar_intersect():\n    # Polar form\n    unit_disk = ComplexRegion(Interval(0, 1)*Interval(0, 2*S.Pi), polar=True)\n    upper_half_unit_disk = ComplexRegion(Interval(0, 1)*Interval(0, S.Pi), polar=True)\n    upper_half_disk = ComplexRegion(Interval(0, oo)*Interval(0, S.Pi), polar=True)\n    lower_half_disk = ComplexRegion(Interval(0, oo)*Interval(S.Pi, 2*S.Pi), polar=True)\n    right_half_disk = ComplexRegion(Interval(0, oo)*Interval(-S.Pi/2, S.Pi/2), polar=True)\n    first_quad_disk = ComplexRegion(Interval(0, oo)*Interval(0, S.Pi/2), polar=True)\n\n    assert upper_half_disk.intersect(unit_disk) == upper_half_unit_disk\n    assert right_half_disk.intersect(first_quad_disk) == first_quad_disk\n    assert upper_half_disk.intersect(right_half_disk) == first_quad_disk\n    assert upper_half_disk.intersect(lower_half_disk) == ComplexRegion(Interval(0, oo)*FiniteSet(0), polar=True)\n\n    c1 = ComplexRegion(Interval(0, 4)*Interval(0, 2*S.Pi), polar=True)\n    assert c1.intersect(Interval(1, 5)) == Interval(1, 4)\n    assert c1.intersect(Interval(4, 9)) == FiniteSet(4)\n    assert c1.intersect(Interval(5, 12)) is S.EmptySet\n\n    c2 = ComplexRegion(Interval(0, 4)*Interval(S.Pi, 2*S.Pi), polar=True)\n    assert c2.intersect(Interval(1, 5)) == Interval(1, 4)\n    assert c2.intersect(Interval(4, 9)) == FiniteSet(4)\n    assert c2.intersect(Interval(5, 12)) is S.EmptySet\n\n    c3 = ComplexRegion(Interval(0, 4)*Interval(0, S.Pi), polar=True)\n    c4 = ComplexRegion(Interval(0, 4)*Interval(S.Pi, 2*S.Pi), polar=True)\n    assert c3.intersect(c4) is S.EmptySet\n    assert c3.intersect(c4,", "def test_imageset_subset_real():\n    from sympy.abc import n\n    assert ImageSet(Lambda(n, n), S.Naturals).is_subset(S.Reals) == True\n    assert ImageSet(Lambda(n, n), S.Integers).is_subset(S.Reals) == True\n    assert ImageSet(Lambda(n, 2*n), S.Integers).is_subset(S.Reals) == True\n    assert ImageSet(Lambda(n, I*n), S.Integers).is_subset(S.Reals) == False\n", "def test_range_polar_interval_intersection():\n    p = symbols('p', positive=True)\n    assert isinstance(Range(3).intersect(Interval(p, p + 2)), Intersection)\n    assert Range(4).intersect(Interval(0, 3)) == Range(4)\n    assert Range(4).intersect(Interval(-oo, oo)) == Range(4)\n    assert Range(4).intersect(Interval(1, oo)) == Range(1, 4)\n    assert Range(4).intersect(Interval(1.1, oo)) == Range(2, 4)\n    assert Range(4).intersect(Interval(0.1, 3)) == Range(1, 4)\n    assert Range(4).intersect(Interval(0.1, 3.1)) == Range(1, 4)\n    assert Range(4).intersect(Interval.open(0, 3)) == Range(1, 3)\n    assert Range(4).intersect(Interval.open(0.1, 0.5)) is S.EmptySet\n\n    # Null Range intersections\n    assert Range(0).intersect(Interval(0.2, 0.8)) is S.EmptySet\n    assert Range(0).intersect(Interval(-oo, oo)) is S.EmptySet\n\n    # test intersection of Range with polar interval\n    r = symbols('r', real=True)\n    theta = symbols('theta', real=True)\n    c1 = ComplexRegion(Interval(0, 1)*Interval(0, 2*S.Pi), polar=True)\n    c2 = ComplexRegion(Interval(0, 1)*Interval(0, S.Pi), polar=True)\n\n    assert Range(0, 1).intersect(c1) == Interval(0, 1)\n    assert Range(0, 1).intersect(c2) == Interval(0, 1)\n    assert Range(1, 2).intersect(c1) == FiniteSet(1)\n    assert Range(1, 2).intersect(c2) == FiniteSet(1)\n    assert Range(2, 3).intersect(c1) is S.EmptySet\n    assert Range(2, 3).intersect(c2) is S.EmptySet\n", "def test_issue_18146b():\n    a = Symbol('a', integer=True)\n    r = Range(-oo, oo)\n    assert Range(oo).contains(a) == True\n    assert Range(-oo, oo).contains(a) == True\n    assert Range(-oo, 2).contains(a) == (a <= 1)\n    assert Range(1, oo).contains(a) == (a >= 1)\n    assert Range(2, -2).contains(a) == (a >= 2) & (a <= -2)  # issue #18999\n    assert Range(1).contains(a) == Eq(a, 0)\n    assert Range(-2, 2, 2).contains(a) == (a >= -2) & (a <= 0) & Eq(Mod(a, 2), 0)\n    assert Range(1, 9, 2).contains(a) == (a >= 1) & (a <= 7) & Eq(Mod(a, 2), 1)\n    assert Range(1, 9, 3).contains(a) == (a >= 1) & (a <= 7) & Eq(Mod(a - 1, 3), 0)\n    assert Range(9, 1, -3).contains(a) == (a >= 3) & (a <= 9) & Eq(Mod(a - 9, -3), 0)\n    assert Range(1, -3, -3).contains(a) == (a >= -2) & (a <= 1) & Eq(Mod(a + 2, -3), 0)\n"], "sample_492": ["def test_serialize_timezone(self):\n    self.assertSerializedResultEqual(\n        get_default_timezone(),\n        (\n            \"django.utils.timezone.get_default_timezone()\",\n            {\"from django.utils import timezone\"},\n        ),\n    )\n    self.assertSerializedResultEqual(\n        get_fixed_timezone(60),\n        (\n            \"django.utils.timezone.get_fixed_timezone(60)\",\n            {\"from django.utils import timezone\"},\n        ),\n    )\n    self.assertSerializedResultEqual(\n        datetime.timezone.utc,\n        (\n            \"datetime.timezone.utc\",\n            {\"import datetime\"},\n        ),\n    )\n    self.assertSerializedResultEqual(\n        zoneinfo.ZoneInfo(\"Europe/Paris\"),\n        (\n            \"zoneinfo.ZoneInfo('Europe/Paris')\",\n            {\"import zoneinfo\"},\n        ),\n    )\n", "def test_serialize_timezone(self):\n    self.assertSerializedResultEqual(\n        datetime.timezone.utc,\n        (\"datetime.timezone.utc\", {\"import datetime\"}),\n    )\n    self.assertSerializedResultEqual(\n        get_default_timezone(),\n        (\"datetime.timezone.utc\", {\"import datetime\"}),\n    )\n    self.assertSerializedResultEqual(\n        get_fixed_timezone(180),\n        (\"datetime.timezone(datetime.timedelta(seconds=180))\", {\"import datetime\"}),\n    )\n    self.assertSerializedResultEqual(\n        zoneinfo.ZoneInfo(\"Europe/Paris\"),\n        (\"zoneinfo.ZoneInfo('Europe/Paris')\", {\"import zoneinfo\"}),\n    )\n    field = models.DateTimeField(default=datetime.datetime.now, tzinfo=get_default_timezone())\n    string = MigrationWriter.serialize(field)[0]\n    self.assertEqual(\n        string,\n        \"models.DateTimeField(default=datetime.datetime.now, tzinfo=datetime.timezone.utc)\",\n    )\n", "def test_serialize_timezone(self):\n    self.assertSerializedResultEqual(\n        get_default_timezone(),\n        (\n            \"django.utils.timezone.get_default_timezone()\",\n            {\"from django.utils import timezone\"},\n        ),\n    )\n    self.assertSerializedResultEqual(\n        get_fixed_timezone(180),\n        (\n            \"django.utils.timezone.get_fixed_timezone(180)\",\n            {\"from django.utils import timezone\"},\n        ),\n    )\n    self.assertSerializedResultEqual(\n        datetime.timezone.utc,\n        (\n            \"datetime.timezone.utc\",\n            {\"import datetime\"},\n        ),\n    )\n    self.assertSerializedResultEqual(\n        zoneinfo.ZoneInfo(\"US/Pacific\"),\n        (\n            \"zoneinfo.ZoneInfo('US/Pacific')\",\n            {\"import zoneinfo\"},\n        ),\n    )\n    self.assertSerializedResultEqual(\n        datetime.datetime(2022, 1, 1, tzinfo=get_default_timezone()),\n        (\n            \"datetime.datetime(2022, 1, 1, tzinfo=django.utils.timezone.get_default_timezone())\",\n            {\"from django.utils import timezone\", \"import datetime\"},\n        ),\n    )\n", "def test_serialize_timezone(self):\n    \"\"\"\n    Test serialization of timezone objects.\n    \"\"\"\n    self.assertSerializedResultEqual(\n        get_default_timezone(),\n        (\"django.utils.timezone.get_default_timezone()\", {\"from django.utils import timezone\"}),\n    )\n    self.assertSerializedResultEqual(\n        get_fixed_timezone(180),\n        (\"django.utils.timezone.get_fixed_timezone(180)\", {\"from django.utils import timezone\"}),\n    )\n    self.assertSerializedResultEqual(\n        zoneinfo.ZoneInfo(\"America/New_York\"),\n        (\"zoneinfo.ZoneInfo('America/New_York')\", {\"import zoneinfo\"}),\n    )\n    self.assertSerializedResultEqual(\n        datetime.timezone.utc,\n        (\"datetime.timezone.utc\", {\"import datetime\"}),\n    )\n    dt = datetime.datetime(2022, 1, 1, tzinfo=get_default_timezone())\n    self.serialize_round_trip(dt)\n    self.serialize_round_trip(datetime.datetime(2022, 1, 1, tzinfo=zoneinfo.ZoneInfo(\"America/New_York\")))\n    self.serialize_round_trip(datetime.datetime(2022, 1, 1, tzinfo=datetime.timezone.utc))\n", "def test_serialize_zone_info(self):\n    self.assertSerializedResultEqual(\n        datetime.datetime(2014, 1, 1, 1, 1, tzinfo=zoneinfo.ZoneInfo(\"Europe/Paris\")),\n        (\n            \"datetime.datetime(2014, 1, 1, 1, 1, tzinfo=zoneinfo.ZoneInfo('Europe/Paris'))\",\n            {\"import datetime\", \"import zoneinfo\"},\n        ),\n    )\n    with self.assertRaisesMessage(\n        ValueError, \"Cannot serialize: zoneinfo.ZoneInfo('America/New_York') because it can't be found\"\n    ):\n        MigrationWriter.serialize(zoneinfo.ZoneInfo(\"America/New_York\"))\n"], "sample_940": ["def test_signature_from_str_with_trailing_comma():\n    signature = '(a, b, *args, c=0, d=\"blah\", **kwargs,)'\n    sig = inspect.signature_from_str(signature)\n    assert list(sig.parameters.keys()) == ['a', 'b', 'args', 'c', 'd', 'kwargs']\n    assert sig.parameters['a'].name == 'a'\n    assert sig.parameters['a'].kind == Parameter.POSITIONAL_OR_KEYWORD\n    assert sig.parameters['a'].default == Parameter.empty\n    assert sig.parameters['a'].annotation == Parameter.empty\n    assert sig.parameters['b'].name == 'b'\n    assert sig.parameters['b'].kind == Parameter.POSITIONAL_OR_KEYWORD\n    assert sig.parameters['b'].default == Parameter.empty\n    assert sig.parameters['b'].annotation == Parameter.empty\n    assert sig.parameters['args'].name == 'args'\n    assert sig.parameters['args'].kind == Parameter.VAR_POSITIONAL\n    assert sig.parameters['args'].default == Parameter.empty\n    assert sig.parameters['args'].annotation == Parameter.empty\n    assert sig.parameters['c'].name == 'c'\n    assert sig.parameters['c'].kind == Parameter.KEYWORD_ONLY\n    assert sig.parameters['c'].default == '0'\n    assert sig.parameters['c'].annotation == Parameter.empty\n    assert sig.parameters['d'].name == 'd'\n    assert sig.parameters['d'].kind == Parameter.KEYWORD_ONLY\n    assert sig.parameters['d'].default == \"'blah'\"\n    assert sig.parameters['d'].annotation == Parameter.empty\n    assert sig.parameters['kwargs'].name == 'kwargs'\n    assert sig.parameters['kwargs'].kind == Parameter.VAR_KEYWORD\n    assert sig.parameters['kwargs'].default == Parameter.empty\n    assert sig.parameters['kwargs'].annotation == Parameter.empty\n    assert sig.return_annotation == Parameter.empty\n", "def test_is_singledispatch_function_and_method():\n    from functools import singledispatch, singledispatchmethod\n\n    @singledispatch\n        pass\n\n    @singledispatchmethod\n    class MyClass:\n            pass\n\n    assert inspect.is_singledispatch_function(func) is True\n    assert inspect.is_singledispatch_function(func.register(int)) is True\n    assert inspect.is_singledispatch_method(MyClass().method) is True\n    assert inspect.is_singledispatch_function(MyClass().method) is False\n    assert inspect.is_singledispatch_method(func) is False\n", "def test_isabstractmethod():\n    class AbstractClass:\n            pass\n\n            raise NotImplementedError\n\n            pass\n\n    class SubClass(AbstractClass):\n            return \"implemented\"\n\n    assert inspect.isabstractmethod(AbstractClass().abstract_method) is True\n    assert inspect.isabstractmethod(SubClass().abstract_method) is False\n    assert inspect.isabstractmethod(AbstractClass().non_abstract_method) is False\n    assert inspect.isabstractmethod(SubClass().non_abstract_method) is False\n", "def test_is_singledispatch_function_and_method():\n    from functools import singledispatch\n\n    @singledispatch\n        pass\n\n    @singledispatch\n    class bar:\n            pass\n\n    assert inspect.is_singledispatch_function(foo) is True\n    assert inspect.is_singledispatch_function(bar) is False\n\n    if sys.version_info >= (3, 8):\n        from functools import singledispatchmethod\n\n        class Foo:\n            @singledispatchmethod\n                pass\n\n        assert inspect.is_singledispatch_method(Foo.bar) is True\n        assert inspect.is_singledispatch_method(Foo().bar) is True\n    else:\n        pass\n", "def test_is_singledispatch_function():\n    @functools.singledispatch\n        pass\n\n    assert inspect.is_singledispatch_function(foo) is True\n\n        pass\n\n    assert inspect.is_singledispatch_function(bar) is False\n\n    class Baz:\n        @staticmethod\n        @functools.singledispatch\n            pass\n\n    assert inspect.is_singledispatch_function(Baz.qux) is True\n\n"], "sample_1176": ["def test_issue_13515():\n    x = Rational(3, 4)\n    assert x._hashable_content() == (3, 4)\n    y = Rational(3, 4)\n    assert x._hashable_content() == y._hashable_content()\n    assert hash(x) == hash(y)\n    assert x == y\n    assert hash(Rational(6, 8)) != hash(x)\n    assert Rational(6, 8) != x\n    assert hash(x) == hash(S.Half)\n    assert Rational(3, 4) != S.Half\n", "def test_Approximation():\n    from sympy.core.numbers import NumberSymbol\n    from sympy.abc import x\n    class TestApproximateClass(NumberSymbol):\n            return (1, 2)\n\n    test_approx = TestApproximateClass()\n    assert test_approx.approximation(Integer) == (1, 2)\n    assert test_approx.approximation(Rational) == (1, 2)\n    assert test_approx.approximation(Float) == (1, 2)\n\n    assert GoldenRatio.approximation(Integer) == (1, 2)\n    assert GoldenRatio.approximation(Rational) == (Rational(1, 1), Rational(2, 1))\n    assert GoldenRatio.approximation(Float) == (1, 2)\n\n    assert pi.approximation(Integer) == (3, 4)\n    assert pi.approximation(Rational) == (Rational(223, 71), Rational(22, 7))\n    assert pi.approximation(Float) == (3, 4)\n\n    assert E.approximation(Integer) == (2, 3)\n    assert E.approximation(Rational) == (Rational(8, 3), Rational(11, 4))\n    assert E.approximation(Float) == (2, 3)\n\n    assert EulerGamma.approximation(Integer) == (0, 1)\n    assert EulerGamma.approximation(Rational) == (Rational(1, 2), Rational(3, 5))\n    assert EulerGamma.approximation(Float) == (0, 1)\n\n    assert Catalan.approximation(Integer) == (0, 1)\n    assert Catalan.approximation(Rational) == (Rational(9, 10), Rational(1, 1))\n    assert Catalan.approximation(Float) == (0, 1)\n\n    assert TribonacciConstant.approximation(Integer) == (1, 2)\n    assert TribonacciConstant.approximation(Rational) == (Rational(1, 1), Rational(2, 1))\n    assert TribonacciConstant.approximation(Float) == (1, 2)\n\n    assert x.approximation(Integer) is None\n", "def test_comp_with_simple_rational_expressions():\n    # issue 12361\n    assert comp(S.Half + S.Half, 1)\n    assert comp(S.Half + S.One, 1.5)\n    assert comp(S.Half + 1.0, 1.5)\n    assert comp(1 + S.Half, 1.5)\n    assert comp(S.Half - S.Half, 0)\n    assert comp(S.Half - S.One, -0.5)\n    assert comp(S.Half - 1.0, -0.5)\n    assert comp(1 - S.Half, 0.5)\n    assert comp(S.Half*S.Half, 0.25)\n    assert comp(S.Half*S.One, 0.5)\n    assert comp(S.Half*1.0, 0.5)\n    assert comp(1*S.Half, 0.5)\n    assert comp(S.Half/S.Half, 1)\n    assert comp(S.Half/S.One, 0.5)\n    assert comp(S.Half/1.0, 0.5)\n    assert comp(1/S.Half, 2)\n    assert comp(2.0/S.Half, 4)\n    assert comp(S.One/S.Half, 2)\n", "def test_issue_19443():\n    assert (-oo).as_powers_dict() == {S.NegativeOne: 1, S.Infinity: 1}\n    assert (-1)**oo == -oo\n    assert (-1)**(-oo) == -oo\n    assert 1**oo == 1\n    assert 1**(-oo) == 1\n    assert (-1)**(-oo).as_powers_dict() == {S.NegativeOne: 1, S.Infinity: 1}\n    assert oo**oo == oo\n    assert oo**(-oo) == 0\n    assert (-oo)**oo == oo\n    assert (-oo)**(-oo) == 0\n    assert (-oo)**oo == S.NegativeOne**oo*S.Infinity**oo\n    assert (-oo)**(-oo) == S.NegativeOne**(-oo)*S.Infinity**(-oo)\n    assert (S.NegativeOne*S.Infinity)**oo == S.NegativeOne**oo*S.Infinity**oo\n    assert (S.NegativeOne*S.Infinity)**(-oo) == S.NegativeOne**(-oo)*S.Infinity**(-oo)\n    assert (S.NegativeOne*S.Infinity)**oo == oo\n    assert (S.NegativeOne*S.Infinity)**(-oo) == 0\n    assert oo**(-S.NegativeOne) == oo\n    assert oo**S.NegativeOne == 0\n    assert (-oo)**(-S.NegativeOne) == oo\n    assert (-oo)**S.NegativeOne == 0\n    assert (oo**oo)**oo == oo\n    assert (oo**(-oo))**oo == 0\n    assert ((-oo)**oo)**oo == oo\n    assert ((-oo)**(-oo))**oo == 0\n    assert (-oo).is_number is True\n    assert S.NegativeOne.is_number is True\n    assert S.Infinity.is_number is True\n", "def test_evalf():\n    assert Float(1, 10).evalf(5) == Float(1, 5)\n    assert Float('1.234', 5).evalf(10) == Float(1.234, 10)\n    assert Float('1.234', '').evalf(5) == Float('1.234', 5)\n    assert Float(1.234, '').evalf(5) == Float(1.234, 5)\n    assert Float(1.234, 15).evalf(5) == Float(1.234, 5)\n    assert Float(1.234, 15)._prec == 15\n\n    # from Number\n    assert Integer(1).evalf(10) == Float(1, 10)\n    assert Rational(1, 2).evalf(10) == Float(0.5, 10)\n    assert Rational(1, 2).evalf() == Float(0.5, 15)\n\n    # from Float\n    assert Float(1, 10)._evalf(5) == Float(1, 5)\n    assert Float('1.234', 5)._evalf(10) == Float(1.234, 10)\n    assert Float('1.234', '')._evalf(5) == Float('1.234', 5)\n    assert Float(1.234, '')._evalf(5) == Float(1.234, 5)\n    assert Float(1.234, 15)._evalf(5) == Float(1.234, 5)\n    assert Float(1.234, 15)._prec == 15\n\n    # from Rational\n    assert Rational(1, 2)._evalf(10) == Float(0.5, 10)\n    assert Rational(1, 2)._evalf() == Float(0.5, 15)\n\n    # from NumberSymbol\n    assert pi.evalf(50) == Float(pi.n(50), 50)\n    assert E.evalf(50) == Float(E.n(50), 50)\n\n    # issue 16836\n    assert Integer(10).evalf(10) == Float(10, 10)\n\n    # from Rational\n    assert Rational(1, 2)._evalf(10) == Float(0.5, 10)\n    assert Rational"], "sample_254": ["    def setUp(self):\n        self.client.force_login(self.superuser)\n        self.model_admin = ModelAdmin(Holder, admin_site)\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n        self.holder = Holder.objects.create(dummy=13)\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n", "    def test_custom_inline(self):\n        \"\"\"\n        Test that a custom inline can be defined and will be rendered correctly.\n        \"\"\"\n        from django.contrib.admin import TabularInline\n        from .models import CustomInlineModel, ParentModel\n\n        class CustomInline(TabularInline):\n            model = CustomInlineModel\n\n        class ParentModelAdmin(admin.ModelAdmin):\n            inlines = [CustomInline]\n\n        parent_model = ParentModel.objects.create(name='Parent')\n        CustomInlineModel.objects.create(parent=parent_model)\n\n        url = reverse('admin:admin_inlines_parentmodel_change', args=(parent_model.id,))\n        response = self.client.get(url)\n        self.assertContains(response, 'Custom inline')\n\n        self.assertIn('custom_inline', response.context['inline_admin_formsets'][0].opts.verbose_name)\n", "    def setUp(self):\n        self.factory = RequestFactory()\n        self.superuser = User.objects.create_superuser(username='super', email='super@example.com', password='secret')\n        self.client.force_login(self.superuser)\n"], "sample_665": ["def test_collect_with_mark_on_test_function_and_class(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.marker\n        class TestClass(object):\n                pass\n\n                pass\n\n        @pytest.mark.marker\n            pass\n\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-m\", \"marker\")\n    result.stdout.fnmatch_lines(\n        [\n            \"test_collect_with_mark_on_test_function_and_class.py::TestClass::test_method1 PASSED\",\n            \"test_collect_with_mark_on_test_function_and_class.py::TestClass::test_method2 PASSED\",\n            \"test_collect_with_mark_on_test_function_and_class.py::test_function PASSED\",\n        ]\n    )\n    assert \"test_function2\" not in result.stdout.str()\n", "def test_collector_handles_overridden_python_functions(testdir):\n    testdir.makeconftest(\n        \"\"\"\n            return MyModule(path, parent=parent)\n        class MyModule(pytest.Module):\n                return __import__(\"__builtins__\")\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\"collected 0 items, 1 error in *\"]\n    )\n    assert result.ret == ExitCode.NO_TESTS_COLLECTED\n", "    def test_pytest_pycollect_makemodule(self, testdir):\n        p = testdir.makepyfile(\n            \"\"\"\n            class TestClass:\n                    pass\n        \"\"\"\n        )\n        package = testdir.mkpydir(\"package\")\n        package.join(\"__init__.py\").write(\"\")\n\n        items, hookrec = testdir.inline_genitems(package.join(\"test_module.py\"))\n        assert len(items) == 0\n\n        package.join(\"test_module.py\").write(p.read())\n        items, hookrec = testdir.inline_genitems(package.join(\"test_module.py\"))\n        assert len(items) == 2\n\n        assert hookrec.calls[1].report.node == items[0].parent\n        assert hookrec.calls[2].report.node == items[1].parent.parent\n        assert hookrec.calls[3].report.node == items[1]\n", "    def test_collect_nodes_are_sorted_by_nodeid(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            class TestX(object):\n                    pass\n\n            class TestY(object):\n                    pass\n        \"\"\"\n        )\n        session = testdir.getnode(testdir.parseconfigure())\n        nodes = session.perform_collect([session.nodeid], genitems=False)\n        nodeids = [node.nodeid for node in nodes]\n        assert nodeids == sorted(nodeids)\n", "    def test_pyobj_property(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            class TestClass(object):\n                    pass\n        \"\"\"\n        )\n        p = testdir.collect_by_name(\"TestClass\")\n        assert p.cls == p.obj\n        assert p.obj == TestClass\n        assert p.module == p.parent.obj\n"], "sample_57": ["    def test_bound_data(self):\n        field = ReadOnlyPasswordHashField()\n        self.assertEqual(field.bound_data('value1', 'value2'), 'value2')\n", "    def test_render_empty_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        html = widget.render(name='password', value='', attrs={})\n        self.assertIn(_(\"No password set.\"), html)\n", "    def test_render_with_empty_string(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = ''\n        self.assertHTMLEqual(\n            widget.render('name', value, {'id': 'id_password'}),\n            '<div id=\"id_password\"><strong>No password set.</strong></div>'\n        )\n", "    def test_bound_data(self):\n        field = ReadOnlyPasswordHashField()\n        self.assertEqual(field.bound_data(None, 'initial_value'), 'initial_value')\n", "    def test_to_python(self):\n        # to_python() should return the initial value.\n        field = ReadOnlyPasswordHashField()\n        initial = 'initial value'\n        self.assertEqual(field.to_python(initial), initial)\n        self.assertEqual(field.to_python(None), None)\n"], "sample_569": ["def test_lmplot_sharey_warning(self):\n\n    df = pd.DataFrame(dict(\n        x=[0, 1, 2, 0, 1, 2],\n        y=[1, -1, 0, -100, 200, 0],\n        z=[\"a\", \"a\", \"a\", \"b\", \"b\", \"b\"],\n    ))\n\n    with pytest.warns(UserWarning):\n        g = lm.lmplot(data=df, x=\"x\", y=\"y\", row=\"z\", sharey=False)\n    ax1, ax2 = g.axes.flat\n    assert ax1.get_ylim()[0] > ax2.get_ylim()[0]\n    assert ax1.get_ylim()[1] < ax2.get_ylim()[1]\n", "def test_lmplot_x_bins(self):\n\n    g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, x_bins=3)\n    ax = g.axes[0, 0]\n\n    assert len(ax.lines) == 4\n    assert len(ax.collections) == 2\n\n    x, y = ax.collections[0].get_offsets().T\n    npt.assert_array_equal(x, self.df.x)\n    npt.assert_array_equal(y, self.df.y)\n", "def test_residplot_partial(self):\n\n    x = self.rs.randn(100)\n    y = x + self.rs.randn(100)\n    z = x + self.rs.randn(100)\n\n    ax = lm.residplot(x=x, y=y)\n    resid = y - np.polyval(np.polyfit(x, y, 1), x)\n    x_plot, y_plot = ax.collections[0].get_offsets().T\n    npt.assert_array_equal(x, x_plot)\n    npt.assert_array_almost_equal(resid, y_plot)\n\n    ax = lm.residplot(x=x, y=y, x_partial=z)\n    resid = y - np.polyval(np.polyfit(x, y, 1), x)\n    resid -= np.polyval(np.polyfit(z, resid, 1), z)\n    x_plot, y_plot = ax.collections[0].get_offsets().T\n    npt.assert_array_equal(x, x_plot)\n    npt.assert_array_almost_equal(resid, y_plot)\n\n    ax = lm.residplot(x=x, y=y, y_partial=z)\n    resid = y - np.polyval(np.polyfit(x, y, 1), x)\n    resid -= np.polyval(np.polyfit(z, y, 1), z)\n    x_plot, y_plot = ax.collections[0].get_offsets().T\n    npt.assert_array_equal(x, x_plot)\n    npt.assert_array_almost_equal(resid, y_plot)\n\n    ax = lm.residplot(x=x, y=y, x_partial=z, y_partial=z)\n    resid = y - np.polyval(np.polyfit(x, y, 1), x)\n    resid -= np.polyval(np.polyfit(z, resid, 1), z)\n    resid -= np.polyval(np.polyfit(z, y, 1), z)\n    x_plot, y_plot = ax.collections[0].get_offsets().T\n    npt.assert_array_equal(x, x_plot)\n    npt.assert_array_almost_equal(resid, y_plot)\n", "    def test_lmplot_facet_kws_deprecation(self):\n\n        msg = (\"`sharex` is deprecated from the `lmplot` function signature. \"\n               \"Please update your code to pass it using `facet_kws`.\")\n        with pytest.warns(UserWarning, match=msg):\n            lm.lmplot(data=self.df, x=\"x\", y=\"y\", col=\"h\", sharex=True)\n\n        msg = (\"`sharey` is deprecated from the `lmplot` function signature. \"\n               \"Please update your code to pass it using `facet_kws`.\")\n        with pytest.warns(UserWarning, match=msg):\n            lm.lmplot(data=self.df, x=\"x\", y=\"y\", col=\"h\", sharey=True)\n\n        msg = (\"`legend_out` is deprecated from the `lmplot` function signature. \"\n               \"Please update your code to pass it using `facet_kws`.\")\n        with pytest.warns(UserWarning, match=msg):\n            lm.lmplot(data=self.df, x=\"x\", y=\"y\", col=\"h\", legend_out=True)\n", "def test_regplot_partial(self):\n\n    x, y = self.rs.randn(2, 100)\n    z = self.rs.randn(100)\n\n    ax = lm.regplot(x=x, y=y)\n    resid1 = ax.collections[0].get_offsets()[:, 1]\n\n    ax.clear()\n    ax = lm.regplot(x=x, y=y, x_partial=z)\n    resid2 = ax.collections[0].get_offsets()[:, 1]\n\n    ax.clear()\n    ax = lm.regplot(x=x, y=y, y_partial=z)\n    resid3 = ax.collections[0].get_offsets()[:, 1]\n\n    ax.clear()\n    ax = lm.regplot(x=x, y=y, x_partial=z, y_partial=z)\n    resid4 = ax.collections[0].get_offsets()[:, 1]\n\n    npt.assert_array_almost_equal(resid1, resid2 + resid3 - resid4)\n"], "sample_482": ["    def test_no_arg(self):\n        output = self.engine.render_to_string(\"floatformat_no_arg\", {\"a\": 34.23234})\n        self.assertEqual(output, \"34.2\")\n        output = self.engine.render_to_string(\"floatformat_no_arg\", {\"a\": 34.00000})\n        self.assertEqual(output, \"34\")\n        output = self.engine.render_to_string(\"floatformat_no_arg\", {\"a\": 34.26000})\n        self.assertEqual(output, \"34.3\")\n", "    def test_basic(self):\n        output = self.engine.render_to_string(\n            \"floatformat_basic\",\n            {\"num1\": 34.23234, \"num2\": 34.00000, \"num3\": 34.26000},\n        )\n        self.assertEqual(output, \"34.2 -- 34 -- 34.3\")\n", "    def test_file_size_formatting(self):\n        output = self.engine.render_to_string(\n            \"filesizeformat\",\n            {\n                \"a\": 123,\n                \"b\": 1234567,\n            },\n        )\n        self.assertEqual(output, \"123 bytes -- 1.2 MB\")\n", "    def test_dictsort(self):\n        output = self.engine.render_to_string(\n            \"dictsort\",\n            {\n                \"a\": [{\"name\": \"John\"}, {\"name\": \"Alice\"}, {\"name\": \"Bob\"}],\n            },\n        )\n        self.assertEqual(output, \"Alice Bob John \")\n\n        output = self.engine.render_to_string(\n            \"dictsort_reversed\",\n            {\n                \"a\": [{\"name\": \"John\"}, {\"name\": \"Alice\"}, {\"name\": \"Bob\"}],\n            },\n        )\n        self.assertEqual(output, \"John Bob Alice \")\n", "    def test_truncatechars(self):\n        output = self.engine.render_to_string(\n            \"truncatechars\",\n            {\"a\": \"truncatechars test\", \"b\": \"hello\"},\n        )\n        self.assertEqual(output, \"trunc -- hel\u2026\")\n\n"], "sample_852": ["def test_make_gaussian_quantiles():\n    X, y = make_gaussian_quantiles(mean=[1.0, 2.0], cov=2.0, n_samples=100,\n                                   n_features=2, n_classes=3, random_state=0)\n\n    assert X.shape == (100, 2), \"X shape mismatch\"\n    assert y.shape == (100,), \"y shape mismatch\"\n    assert np.unique(y).shape == (3,), \"Unexpected number of classes\"\n", "def test_make_gaussian_quantiles():\n    X, y = make_gaussian_quantiles(n_samples=100, n_features=3, n_classes=4,\n                                    shuffle=True, random_state=0)\n\n    assert X.shape == (100, 3), \"X shape mismatch\"\n    assert y.shape == (100,), \"y shape mismatch\"\n    assert np.unique(y).shape == (4,), \"Unexpected number of classes\"\n    assert np.all(y >= 0) and np.all(y <= 3), \"Unexpected class values\"\n", "def test_make_classification_unequal_class_weights():\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=5,\n                               n_redundant=1, n_repeated=1, n_classes=3,\n                               n_clusters_per_class=1, weights=[1, 1, 3],\n                               hypercube=False, shift=None, scale=None,\n                               random_state=0)\n\n    assert X.shape == (100, 20), \"X shape mismatch\"\n    assert y.shape == (100,), \"y shape mismatch\"\n    assert np.unique(y).shape == (3,), \"Unexpected number of classes\"\n    assert sum(y == 0) == 25, \"Unexpected number of samples in class #0\"\n    assert sum(y == 1) == 25, \"Unexpected number of samples in class #1\"\n    assert sum(y == 2) == 50, \"Unexpected number of samples in class #2\"\n\n    # Test for n_features > 30\n    X, y = make_classification(n_samples=2000, n_features=31, n_informative=31,\n                               n_redundant=0, n_repeated=0, hypercube=True,\n                               scale=0.5, weights=[1, 1, 3], random_state=0)\n\n    assert X.shape == (2000, 31), \"X shape mismatch\"\n    assert y.shape == (2000,), \"y shape mismatch\"\n    assert (np.unique(X.view([('', X.dtype)]*X.shape[1])).view(X.dtype)\n            .reshape(-1, X.shape[1]).shape[0] == 2000), (\n                \"Unexpected number of unique rows\")\n", "def test_make_gaussian_quantiles():\n    X, y = make_gaussian_quantiles(n_samples=100, n_features=2,\n                                    n_classes=3, n_samples_info=10,\n                                    random_state=0)\n    assert X.shape == (100, 2), \"X shape mismatch\"\n    assert y.shape == (100,), \"y shape mismatch\"\n    assert np.unique(y).shape == (3,), \"Unexpected number of classes\"\n\n    X, y = make_gaussian_quantiles(n_samples=100, n_features=2,\n                                    n_classes=3, n_samples_info=10,\n                                    random_state=0, shuffle=False)\n    for class_ in np.unique(y):\n        assert_array_almost_equal(X[y == class_].mean(axis=0),\n                                  np.zeros(2), decimal=2,\n                                  err_msg=\"Not a standard normal\")\n", "def test_make_gaussian_quantiles():\n    X, y = make_gaussian_quantiles(n_samples=50, n_features=5,\n                                   n_classes=3, shuffle=True, random_state=0)\n\n    assert X.shape == (50, 5), \"X shape mismatch\"\n    assert y.shape == (50,), \"y shape mismatch\"\n\n    assert len(np.unique(y)) == 3, \"Unexpected number of classes\"\n\n    # Check that the classes are in order of increasing distance from the origin\n    y_order = np.argsort(y)\n    assert np.all(np.linalg.norm(X[y_order], axis=1) >=\n                  np.roll(np.linalg.norm(X[y_order], axis=1), 1))\n"], "sample_436": ["    def setUp(self):\n            return\n\n        self.output = StringIO()\n        self.cmd = RunserverCommand(stdout=self.output)\n        self.cmd.run = monkey_run\n", "    def test_ipv6_addrport(self):\n        self.write_settings(\n            \"settings.py\",\n            sdict={\n                \"ALLOWED_HOSTS\": [\"::1\"],\n                \"DEBUG\": True,\n            },\n        )\n        out, err = self.run_manage([\"runserver\", \"0:8000\", \"--use-ipv6\"])\n        self.assertNoOutput(err)\n        self.assertIn(\"Starting development server at http://[::]:8000/\", out)\n", "    def test_runner_addrport_ipv6_with_zero_port(self, run_mock):\n        call_command(\n            \"runserver\",\n            addrport=\"::1:0\",\n            use_ipv6=True,\n            skip_checks=True,\n            use_reloader=False,\n            stdout=self.stdout,\n        )\n        self.assertIn(\n            \"Starting development server at http://[::1:0]/\",\n            self.stdout.getvalue(),\n        )\n        run_mock.assert_called_once_with(\n            \"::1\",\n            0,\n            mock.ANY,\n            ipv6=True,\n            threading=mock.ANY,\n            server_cls=mock.ANY,\n        )\n", "    def setUp(self):\n        self.settings = {\n            \"DEBUG\": True,\n            \"ALLOWED_HOSTS\": [],\n        }\n        self.stdout = StringIO()\n        self.cmd = RunserverCommand(stdout=self.stdout)\n", "    def setUp(self):\n        self.cmd = RunserverCommand(stdout=self.stdout, stderr=self.stderr)\n"], "sample_15": ["    def test_creation(self):\n        q = np.arange(5) * u.m\n        qiter = q.flat\n        assert isinstance(qiter, QuantityIterator)\n        assert qiter._quantity is q\n        assert qiter._dataiter is q.view(np.ndarray).flat\n", "    def test_clip(self):\n        q = np.array([-1.0, 0.0, 1.0]) * u.m\n        result = np.clip(q, -0.5 * u.m, 0.5 * u.m)\n        assert np.all(result == np.clip(q.value, -0.5, 0.5) * q.unit)\n\n        result = np.clip(q, -0.5 * u.km, 0.5 * u.m)\n        assert np.all(result == np.clip(q.value, -0.5, 0.5) * q.unit)\n\n        result = np.clip(q, -0.5, 0.5)\n        assert np.all(result == np.clip(q.value, -0.5, 0.5) * q.unit)\n\n        result = np.clip(q, -0.5 * u.dimensionless_unscaled, 0.5)\n        assert np.all(result == np.clip(q.value, -0.5, 0.5) * q.unit)\n\n        result = np.clip(q, -0.5, 0.5 * u.dimensionless_unscaled)\n        assert np.all(result == np.clip(q.value, -0.5, 0.5) * q.unit)\n\n        result = np.clip(q, -0.5 * u.dimensionless_unscaled, 0.5 * u.dimensionless_unscaled)\n        assert np.all(result == np.clip(q.value, -0.5, 0.5) * q.unit)\n\n        with pytest.raises(u.UnitsError):\n            np.clip(q, -0.5 * u.m, 0.5 * u.s)\n\n        with pytest.raises(u.UnitsError):\n            np.clip(q, -0.5 * u.s, 0.5 * u.m)\n", "    def test_format(self):\n        q = 12345.6789 * u.m\n        assert q.to_string() == \"12345.6789 m\"\n        assert q.to_string(format=\"latex\") == \"$12345.6789\\\\ m$\"\n        assert q.to_string(format=\"latex_inline\") == \"$12345.6789\\\\ m$\"\n        assert q.to_string(format=\"latex\", subfmt=\"inline\") == \"$12345.6789\\\\ m$\"\n        assert q.to_string(format=\"latex\", subfmt=\"display\") == \"$\\\\displaystyle 12345.6789\\\\ m$\"\n", "    def test_repr_latex(self):\n        q = u.Quantity(\"1.23456789\", u.m)\n        assert q._repr_latex_() == \"$1.23456789\\\\ \\\\mathrm{m}$\"\n", "    def test_constructor_none_unit(self):\n        assert u.Quantity(value=4.0).unit == u.dimensionless_unscaled\n        assert u.Quantity(value=4.0, unit=None).unit == u.dimensionless_unscaled\n"], "sample_534": ["def test_clabel_text():\n    x, y = np.meshgrid(np.arange(0, 10), np.arange(0, 10))\n    z = np.max(np.dstack([abs(x), abs(y)]), 2)\n\n    fig, ax = plt.subplots()\n    cs = ax.contour(x, y, z)\n    pts = np.array([(1.0, 3.0), (1.0, 4.4), (1.0, 6.0)])\n    clabels = ax.clabel(cs, manual=pts)\n    for clabel in clabels:\n        assert isinstance(clabel, mpl.text.Text)\n\n    fig, ax = plt.subplots()\n    cs = ax.contour(x, y, z)\n    pts = np.array([(1.0, 3.0), (1.0, 4.4), (1.0, 6.0)])\n    clabels = ax.clabel(cs, manual=pts, use_clabeltext=True)\n    for clabel in clabels:\n        assert isinstance(clabel, ClabelText)\n", "def test_circular_contour_warning_logscale():\n    # Check that almost circular contours don't throw a warning with logscale\n    x, y = np.meshgrid(np.linspace(-2, 2, 4), np.linspace(-2, 2, 4))\n    r = np.hypot(x, y)\n    r[r < 1e-10] = 1e-10  # avoid zeros to use log scale\n\n    plt.figure()\n    cs = plt.contour(x, y, r, norm=LogNorm())\n    plt.clabel(cs)\n", "def test_contour_labeler_event_handler():\n    cs = ContourSet(None, None, None)\n    event1 = type(\"Event\", (), {\"name\": \"button_press_event\", \"button\": \"MouseButton.LEFT\"})\n    event2 = type(\"Event\", (), {\"name\": \"key_press_event\", \"key\": \"enter\"})\n    event3 = type(\"Event\", (), {\"name\": \"key_press_event\", \"key\": \"delete\"})\n\n    _contour_labeler_event_handler(cs, True, 5, event1)\n    _contour_labeler_event_handler(cs, True, 5, event2)\n    _contour_labeler_event_handler(cs, True, 5, event3)\n", "def test_add_label_near_negative_index():\n    # Test that negative index works in add_label_near\n    x, y = np.meshgrid(np.arange(0, 10), np.arange(0, 10))\n    z = np.max(np.dstack([abs(x), abs(y)]), 2)\n\n    fig, ax = plt.subplots()\n    cs = ax.contour(x, y, z)\n    cs.clabel()\n\n    # Add a label near the point (5, 5) with negative index\n    cs.add_label_near(5, 5, inline=True, transform=None, index=-1)\n\n    # Add a label near the point (5, 5) with negative index and transform\n    cs.add_label_near(5, 5, inline=True, transform=ax.transAxes, index=-1)\n", "def test_contour_labeler_event_handler():\n    fig, ax = plt.subplots()\n    x, y = np.meshgrid(np.arange(0, 10), np.arange(0, 10))\n    z = np.max(np.dstack([abs(x), abs(y)]), 2)\n    cs = ax.contour(x, y, z)\n\n    # Mock event\n    event = type('Event', (object,), {\n        'name': 'button_press_event',\n        'button': MouseButton.MIDDLE,\n        'key': None,\n        'x': 5,\n        'y': 5,\n        'inaxes': ax\n    })\n\n    # Test middle mouse button press\n    canvas = fig.canvas\n    _contour_labeler_event_handler(cs, True, 5, event)\n    assert len(cs.labelTexts) == 0\n    canvas.stop_event_loop = lambda: None  # disable\n    _contour_labeler_event_handler(cs, True, 5, event)\n    assert len(cs.labelTexts) == 0\n\n    # Test right mouse button press\n    event.button = MouseButton.RIGHT\n    event.inaxes = ax\n    _contour_labeler_event_handler(cs, True, 5, event)\n    assert len(cs.labelTexts) == 0\n\n    # Test left mouse button press\n    event.button = MouseButton.LEFT\n    event.inaxes = ax\n    _contour_labeler_event_handler(cs, True, 5, event)\n    assert len(cs.labelTexts) == 1\n    _contour_labeler_event_handler(cs, True, 5, event)\n    assert len(cs.labelTexts) == 2\n\n    # Test key press\n    event.name = 'key_press_event'\n    event.button = None\n    event.key = 'escape'\n    _contour_labeler_event_handler(cs, True, 5, event)\n    assert len(cs.labelTexts) == 2\n    canvas.stop_event_loop = lambda: None  # disable\n    event.key = 'enter'\n    _contour_labeler_event_handler(cs, True, 5, event)\n    assert len(cs.labelTexts) == 2\n\n    # Test backspace press\n    event.key = 'backspace'\n    event.inaxes = ax\n    _contour_labeler_event_handler(cs, True, 5, event)\n    assert len(cs.labelTexts) == 1\n    event"], "sample_271": ["    def test_common_roots(self):\n        paths = (\n            Path('/first/second'),\n            Path('/first/second/third'),\n            Path('/first/'),\n            Path('/root/first/'),\n        )\n        results = autoreload.common_roots(paths)\n        self.assertCountEqual(results, [Path('/first/'), Path('/root/first/')])\n", "        def __init__(self, msg, extra_context):\n            super().__init__(msg)\n            self.extra_context = extra_context\n", "    def test_request_processed_calls_update_watches(self):\n        with mock.patch.object(self.reloader, 'update_watches') as mocked_update_watches:\n            self.reloader.request_processed()\n        self.assertEqual(mocked_update_watches.call_count, 1)\n", "    def test_subscribe_dir(self, mocked_watch_root):\n        reloader = autoreload.WatchmanReloader()\n        mocked_watch_root.return_value = ('watch', '')\n        reloader._subscribe_dir(self.tempdir, ['test.py'])\n        mocked_watch_root.assert_called_once_with(self.tempdir)\n        reloader.client.query.assert_called_once_with(\n            'subscribe', 'watch', 'files:{}'.format(self.tempdir),\n            {\n                'expression': ['anyof', ['type', 'f'], ['type', 'l']],\n                'fields': ['name'],\n                'since': reloader._get_clock('watch'),\n                'dedup_results': True,\n            }\n        )\n", "    def test_tick_updates_mtimes(self, mock_notify_file_changed, mock_snapshot_files, mock_sleep):\n        mock_snapshot_files.side_effect = [\n            {(self.existing_file, 1)},\n            {(self.existing_file, 2)},\n        ]\n        with mock.patch.object(autoreload.StatReloader, 'watched_files', return_value=[self.existing_file]):\n            ticker = autoreload.StatReloader().tick()\n            next(ticker)\n            next(ticker)\n            mock_sleep.assert_called_once()\n            self.assertEqual(mock_snapshot_files.call_count, 2)\n            mock_notify_file_changed.assert_called_once_with(self.existing_file)\n"], "sample_427": ["def test_absolute_max_zero(self):\n    data = {\n        \"form-TOTAL_FORMS\": \"1\",\n        \"form-INITIAL_FORMS\": \"0\",\n    }\n    AbsoluteMaxFavoriteDrinksFormSet = formset_factory(\n        FavoriteDrinkForm,\n        absolute_max=0,\n    )\n    formset = AbsoluteMaxFavoriteDrinksFormSet(data=data)\n    self.assertIs(formset.is_valid(), False)\n    self.assertEqual(len(formset.forms), 0)\n    self.assertEqual(\n        formset.non_form_errors(),\n        [\"Please submit at most 0 forms.\"],\n    )\n", "def test_absolute_max_with_max_num_and_min_num(self):\n    data = {\n        \"form-TOTAL_FORMS\": \"3\",\n        \"form-INITIAL_FORMS\": \"0\",\n        \"form-MAX_NUM_FORMS\": \"2\",\n    }\n    LimitedFavoriteDrinksFormSet = formset_factory(\n        FavoriteDrinkForm,\n        max_num=1,\n        min_num=3,\n        absolute_max=1000,\n    )\n    formset = LimitedFavoriteDrinksFormSet(data=data)\n    self.assertIs(formset.is_valid(), False)\n    self.assertEqual(len(formset.forms), 3)\n    self.assertEqual(\n        formset.non_form_errors(),\n        [\"Please submit at most 1 form.\"],\n    )\n", "def test_formset_absolute_max_property(self):\n    ChoiceFormSet = formset_factory(Choice)\n    formset = ChoiceFormSet()\n    self.assertEqual(formset.absolute_max, 1000)\n\n    ChoiceFormSet = formset_factory(Choice, max_num=50)\n    formset = ChoiceFormSet()\n    self.assertEqual(formset.absolute_max, 50)\n\n    ChoiceFormSet = formset_factory(Choice, max_num=2000)\n    formset = ChoiceFormSet()\n    self.assertEqual(formset.absolute_max, 2000)\n", "def test_can_delete_extra_with_initial_data(self):\n    ChoiceFormFormset = formset_factory(\n        form=Choice,\n        can_delete=True,\n        can_delete_extra=False,\n        extra=2,\n        initial=[{\"choice\": \"Zero\", \"votes\": \"1\"}, {\"choice\": \"One\", \"votes\": \"1\"}],\n    )\n    formset = ChoiceFormFormset(\n        data={\n            \"form-0-choice\": \"Zero\",\n            \"form-0-votes\": \"0\",\n            \"form-0-DELETE\": \"on\",\n            \"form-1-choice\": \"One\",\n            \"form-1-votes\": \"1\",\n            \"form-2-choice\": \"\",\n            \"form-2-votes\": \"\",\n            \"form-TOTAL_FORMS\": \"3\",\n            \"form-INITIAL_FORMS\": \"2\",\n        }\n    )\n    self.assertEqual(\n        formset.cleaned_data,\n        [{\"choice\": \"Zero\", \"votes\": 0, \"DELETE\": True}, {\"choice\": \"One\", \"votes\": 1}],\n    )\n    self.assertEqual(len(formset.deleted_forms), 1)\n    self.assertIs(formset._should_delete_form(formset.forms[0]), True)\n    self.assertIs(formset._should_delete_form(formset.forms[1]), False)\n    self.assertIs(formset._should_delete_form(formset.forms[2]), False)\n", "def test_total_error_count_includes_non_form_errors(self):\n    \"\"\"total_error_count includes non form errors.\"\"\"\n    class TestFormSet(BaseFormSet):\n            raise ValidationError(\"Non form error\")\n\n    data = {\n        \"form-TOTAL_FORMS\": \"1\",\n        \"form-INITIAL_FORMS\": \"0\",\n        \"form-0-name\": \"Test\",\n        \"form-0-votes\": \"\",\n    }\n    TestFormSet = formset_factory(FavoriteDrinkForm, formset=TestFormSet)\n    formset = TestFormSet(data, prefix=\"form\")\n    self.assertEqual(formset.total_error_count(), 2)\n"], "sample_672": ["def test_ellipsize():\n    s = \"x\" * 50\n    assert _ellipsize(s, 25) == \"x\" * 10 + \"...\" + \"x\" * 10\n    assert _ellipsize(s, 50) == s\n    assert _ellipsize(s, 100) == s\n    assert _ellipsize(\"\", 10) == \"\"\n", "def test_ellipsize():\n    s = \"x\" * 100\n    assert _ellipsize(s, 10) == \"xxxxx...xxxxx\"\n    assert _ellipsize(s, 100) == s\n    assert _ellipsize(s, 1000) == s\n    assert _ellipsize(\"\", 10) == \"\"\n", "def test_safeformat():\n    class BrokenRepr:\n            raise ValueError(\"broken\")\n\n    obj = BrokenRepr()\n    assert \"ValueError\" in safeformat(obj)\n\n    class BrokenPformat:\n            return \"fine\"\n\n            raise AttributeError\n\n    obj = BrokenPformat()\n    assert \"AttributeError\" in safeformat(obj)\n\n    class FineObject:\n            return \"fine\"\n\n    obj = FineObject()\n    assert safeformat(obj) == \"fine\"\n", "def test_safeformat():\n    class BrokenFormat:\n            raise ValueError(\"Broken format\")\n\n    assert \"ValueError\" in safeformat(BrokenFormat())\n\n    class BrokenPPrint:\n            raise ValueError(\"Broken pprint\")\n\n    assert \"ValueError\" in safeformat(BrokenPPrint())\n\n    class BrokenRepr:\n            raise ValueError(\"Broken repr\")\n\n    assert \"ValueError\" in safeformat(BrokenRepr())\n\n    assert safeformat(\"hello\") == \"'hello'\"\n    assert safeformat(123) == \"123\"\n    assert safeformat(None) == \"None\"\n", "def test_saferepr_edge_cases():\n    # Test saferepr() with various edge cases\n    assert saferepr(float('inf')) == 'inf'\n    assert saferepr(float('-inf')) == '-inf'\n    assert saferepr(float('nan')) == 'nan'\n    assert saferepr(complex(1, 2)) == '(1+2j)'\n    assert saferepr(bytearray([1, 2, 3])) == 'bytearray(b\\'\\\\x01\\\\x02\\\\x03\\')'\n    assert saferepr(memoryview(b'123')) == '<memory at 0x%x>' % id(memoryview(b'123'))\n"], "sample_1066": ["def test_print_SingularityFunction():\n    assert mp.doprint(SingularityFunction(x, 4, 5)) == \\\n        '<apply><power/><apply><root/><ci>x</ci><ci>4</ci></apply><cn>5</cn></apply>'\n    assert mp.doprint(SingularityFunction(x, -3, 4)) == \\\n        '<apply><power/><apply><root/><ci>x</ci><ci>3</ci></apply><cn>4</cn></apply>'\n    assert mp.doprint(SingularityFunction(x, 0, 4)) == \\\n        '<apply><power/><ci>x</ci><cn>4</cn></apply>'\n    assert mp.doprint(SingularityFunction(x, a, n)) == \\\n        '<apply><power/><apply><root/><apply><minus/><ci>a</ci><ci>x</ci></apply></apply><ci>n</ci></apply>'\n    assert mp.doprint(SingularityFunction(x, 4, -2)) == \\\n        '<apply><power/><apply><root/><ci>x</ci><ci>4</ci></apply><cn>-2</cn></apply>'\n    assert mp.doprint(SingularityFunction(x, 4, -1)) == \\\n        '<apply><power/><apply><root/><ci>x</ci><ci>4</ci></apply><cn>-1</cn></apply>'\n", "def test_print_SingularityFunction_with_Symbol():\n    x = Symbol('x')\n    n = Symbol('n')\n    a = Symbol('a')\n    assert mathml(SingularityFunction(x, 4, 5), printer='presentation') == \\\n        '<msup><mfenced close=\"&#10217;\" open=\"&#10216;\"><mrow><mi>x</mi>' \\\n        '<mo>-</mo><mn>4</mn></mrow></mfenced><mn>5</mn></msup>'\n    assert mathml(SingularityFunction(x, -3, 4), printer='presentation') == \\\n        '<msup><mfenced close=\"&#10217;\" open=\"&#10216;\"><mrow><mi>x</mi>' \\\n        '<mo>+</mo><mn>3</mn></mrow></mfenced><mn>4</mn></msup>'\n    assert mathml(SingularityFunction(x, 0, 4), printer='presentation') == \\\n        '<msup><mfenced close=\"&#10217;\" open=\"&#10216;\"><mi>x</mi></mfenced>' \\\n        '<mn>4</mn></msup>'\n    assert mathml(SingularityFunction(x, a, n), printer='presentation') == \\\n        '<msup><mfenced close=\"&#10217;\" open=\"&#10216;\"><mrow><mrow>' \\\n        '<mo>-</mo><mi>a</mi></mrow><mo>+</mo><mi>x</mi></mrow></mfenced>' \\\n        '<mi>n</mi></msup>'\n    assert mathml(SingularityFunction(x, 4, -2), printer='presentation') == \\\n        '<msup><mfenced close=\"&#10217;\" open=\"&#10216;\"><mrow><mi>x</mi>' \\\n        '<mo>-</mo><mn>4</mn></mrow></mfenced><mn>-2</mn></msup>'\n    assert mathml(SingularityFunction(x, 4, -1), printer='presentation') == \\\n        '<msup><mfenced close=\"&#10217;\" open=\"&#10216;\"><mrow><mi>x</mi>' \\\n        '<mo>-</mo><mn>4</mn></mrow></mfenced><mn>-1</mn></msup>'\n", "def test_print_matrix_operations():\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n    assert mathml(X*Y, printer='presentation') == \\\n        '<mrow><mi>X</mi><mo>&InvisibleTimes;</mo><mi>Y</mi></mrow>'\n    assert mathml(X**2, printer='presentation') == \\\n        '<msup><mi>X</mi><mn>2</mn></msup>'\n    assert mathml(X**-1, printer='presentation') == \\\n        '<msup><mi>X</mi><mn>-1</mn></msup>'\n    assert mathml(X + Y, printer='presentation') == \\\n        '<mrow><mi>X</mi><mo>+</mo><mi>Y</mi></mrow>'\n    assert mathml(X - Y, printer='presentation') == \\\n        '<mrow><mi>X</mi><mo>-</mo><mi>Y</mi></mrow>'\n    assert mathml(-X, printer='presentation') == \\\n        '<mrow><mo>-</mo><mi>X</mi></mrow>'\n", "def test_print_AccumBounds_html():\n    a = Symbol('a', real=True)\n    assert mpp.doprint(AccumBounds(0, a)) == '<mrow><mfenced close=\"&#10217;\" open=\"&#10216;\"><mn>0</mn><mi>a</mi></mfenced></mrow>'\n    assert mpp.doprint(AccumBounds(0, a)) == '<mrow><mfenced close=\"\u2016\" open=\"\u2016\"><mn>0</mn><mi>a</mi></mfenced></mrow>'\n    assert mpp.doprint(AccumBounds(a + 1, a + 2)) == '<mrow><mfenced close=\"\u2016\" open=\"\u2016\"><mrow><mi>a</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>a</mi><mo>+</mo><mn>2</mn></mrow></mfenced></mrow>'\n", "def test_print_HadamardProduct():\n    from sympy.matrices.expressions import HadamardProduct\n    from sympy import symbols, Matrix\n\n    x, y, z = symbols('x y z')\n\n    assert mathml(HadamardProduct(x*y, z), printer='presentation') == \\\n        '<mrow><mrow><mi>x</mi><mo>&InvisibleTimes;</mo><mi>y</mi></mrow>'\\\n        '<mo>&#x2218;</mo><mi>z</mi></mrow>'\n    assert mathml(HadamardProduct(x*y, Matrix([[1, 2], [3, 4]])), printer='presentation') == \\\n        '<mrow><mrow><mi>x</mi><mo>&InvisibleTimes;</mo><mi>y</mi></mrow><mo>&#x2218;</mo>'\\\n        '<mfenced><mtable><mtr><mtd><mn>1</mn></mtd><mtd><mn>2</mn></mtd></mtr><mtr><mtd><mn>3</mn></mtd><mtd><mn>4</mn></mtd></mtr></mtable></mfenced></mrow>'\n"], "sample_1042": ["def test_IndexedBase_offset():\n    i, j = symbols('i j', integer=True)\n    o = Symbol('o', integer=True)\n    A = IndexedBase('A', strides=(1, 2), offset=o)\n    assert A.offset == o\n    assert A.strides == (1, 2)\n    assert A[i, j].shape == A.shape\n    assert A[i, j].offset == o\n    assert A[i, j].strides == (1, 2)\n", "def test_IndexedBase_strides():\n    i, j, k = symbols('i j k', integer=True)\n    o, p, q = symbols('o p q', integer=True)\n    A = IndexedBase('A', strides=(o, p, q))\n    assert A.offset == 0\n    assert A.strides == Tuple(o, p, q)\n    B = IndexedBase('B', strides='C')\n    assert B.offset == 0\n    assert B.strides == 'C'\n    C = IndexedBase('C', strides='F')\n    assert C.offset == 0\n    assert C.strides == 'F'\n    D = IndexedBase('D', strides=(o, p, q), offset=10)\n    assert D.offset == 10\n    assert D.strides == Tuple(o, p, q)\n    raises(TypeError, lambda: IndexedBase('E', strides='Invalid'))\n", "def test_indexed_roundtrip():\n    i, j = symbols('i j', integer=True)\n    A = IndexedBase('A', shape=(10, 10))\n    a = A[i, j]\n    assert a == Indexed(a.base, *a.indices)\n    assert a == Indexed(IndexedBase(a.base.label), *a.indices)\n    assert a == Indexed(IndexedBase(a.base.label, shape=a.base.shape), *a.indices)\n", "def test_IndexedBase_shape_inference():\n    i, j, k = symbols('i j k', integer=True)\n    m, n, o = symbols('m n o', integer=True)\n\n    A = IndexedBase('A', shape=(m, n, o))\n    assert A.shape == (m, n, o)\n    assert A[i, j, k].shape == (m, n, o)\n    assert A[i, j, 5].shape == (m, n, o)\n    assert A[i, 5, k].shape == (m, n, o)\n    assert A[5, j, k].shape == (m, n, o)\n\n    B = IndexedBase('B')\n    assert B[i, j, k].shape == (m, n, o)\n    assert B[i, j, 5].shape == (m, 5)\n    assert B[i, 5, k].shape == (m, n)\n    assert B[5, j, k].shape == (n, o)\n", "def test_indexed_derivative_with_piecewise():\n    i, j, k, n = symbols(\"i,j,k,n\")\n    x = IndexedBase(\"x\")\n    y = IndexedBase(\"y\")\n    f = Function(\"f\")\n    pw = Piecewise((x[i], Eq(i, j)), (y[i], True))\n    assert pw.diff(y[i]) == KroneckerDelta(i, i)\n    assert pw.diff(x[j]) == KroneckerDelta(i, j)\n    assert pw.diff(y[k]) == KroneckerDelta(i, k)\n    assert pw.diff(f(x[i])) == 0\n    assert pw.diff(f(y[i])) == f(x[i]).diff(y[i])*KroneckerDelta(i, i) + f(y[i]).diff(y[i])*KroneckerDelta(i, i)\n    assert pw.diff(x[j]).subs(j, i) == 1\n    assert pw.diff(x[j]).subs(j, k) == 0\n    assert pw.diff(y[j]).subs(j, i) == 1\n    assert pw.diff(y[j]).subs(j, k) == 0\n    assert Sum(pw.diff(y[j]), (i, 0, n)).subs(j, i) == n\n    assert Sum(pw.diff(y[j]), (i, 0, n)).subs(j, n+1) == 0\n"], "sample_1073": ["def test_sqrt_depth():\n    from sympy import sqrt\n    from sympy.simplify.sqrtdenest import sqrt_depth\n    assert sqrt_depth(0) == 0\n    assert sqrt_depth(1) == 0\n    assert sqrt_depth(sqrt(2)) == 1\n    assert sqrt_depth(sqrt(2 + sqrt(3))) == 2\n    assert sqrt_depth(sqrt(2 + sqrt(3 + sqrt(4)))) == 3\n    assert sqrt_depth(sqrt(1 + sqrt(2)*sqrt(1 + sqrt(3)))) == 2\n    assert sqrt_depth(sqrt(1 + 2*sqrt(6))) == 1\n    assert sqrt_depth(1 + sqrt(2)) == 1\n    assert sqrt_depth(1 + sqrt(2 + sqrt(3))) == 2\n", "def test_sqrt_depth():\n    from sympy import sqrt\n    assert sqrt_depth(1 + sqrt(2)*(1 + sqrt(3))) == 1\n    assert sqrt_depth(1 + sqrt(2)*sqrt(1 + sqrt(3))) == 2\n    assert sqrt_depth(sqrt(2 + sqrt(5))) == 1\n    assert sqrt_depth(sqrt(2 + sqrt(sqrt(5)))) == 2\n    assert sqrt_depth(sqrt(2 + sqrt(sqrt(sqrt(5))))) == 3\n    assert sqrt_depth(sqrt(1)) == 0\n    assert sqrt_depth(sqrt(1 + sqrt(2))) == 1\n    assert sqrt_depth(1 + sqrt(1 + sqrt(2))) == 1\n", "def test_sqrt_depth():\n    from sympy import sqrt\n    assert sqrt_depth(sqrt(1)) == 1\n    assert sqrt_depth(sqrt(2)) == 1\n    assert sqrt_depth(sqrt(2) + 1) == 1\n    assert sqrt_depth(sqrt(2) + sqrt(3)) == 1\n    assert sqrt_depth(sqrt(2)*sqrt(3)) == 2\n    assert sqrt_depth(sqrt(1 + sqrt(2))) == 2\n    assert sqrt_depth(sqrt(1 + sqrt(2 + sqrt(3)))) == 3\n    assert sqrt_depth(sqrt(1 + sqrt(2 + sqrt(3 + sqrt(4))))) == 4\n", "def test_sqrtdenest_coverage():\n    # Test the sqrt_biquadratic_denest function with different values\n    assert sqrtdenest(sqrt(5*r3 + 6*r2)) == sqrt(2)*root(3, 4) + root(3, 4)**3\n    assert sqrtdenest(sqrt(8*r2 + 2*r5 - 18)) == sqrt(-1)*(-r10 + 1 + r2 + r5)\n    assert sqrtdenest(sqrt(8*r2/3 + 14*r5/3 + S(154)/9)) == -r10/3 + r2 + r5 + 3\n    assert sqrtdenest(sqrt(4*r15 + 8*r5 + 12*r3 + 24)) == 1 + r3 + r5 + r15\n\n    # Test the sqrt_depth function with different values\n    assert sqrt_depth(1 + sqrt(2)*(1 + sqrt(3))) == 1\n    assert sqrt_depth(1 + sqrt(2)*sqrt(1 + sqrt(3))) == 2\n\n    # Test the is_algebraic function with different values\n    assert is_algebraic(sqrt(2)*(3/(sqrt(7) + sqrt(5)*sqrt(2))))\n    assert not is_algebraic(sqrt(2)*(3/(sqrt(7) + sqrt(5)*cos(2))))\n\n    # Test the _sqrtdenest1 function with different values\n    assert _sqrtdenest1(sqrt(5 + 2*sqrt(6))) == r2 + r3\n    assert _sqrtdenest1(sqrt(5 + 4*sqrt(5 + 2*sqrt(6)))) == sqrt(5.0 + 4*r2 + 4*r3)\n\n    # Test the _denester function with different values\n    assert _denester([sqrt(5 + 2*sqrt(6))], [None, None, None, None], 0, 1) == (r2 + r3, [1])\n", "def test_sqrt_depth():\n    from sympy import sqrt\n    r2, r3, r5, r6, r7, r10, r15, r29 = [sqrt(x) for x in [2, 3, 5, 6, 7, 10, 15, 29]]\n\n    assert sqrt_depth(r2) == 1\n    assert sqrt_depth(r2 + 1) == 1\n    assert sqrt_depth(r2 + r3) == 1\n    assert sqrt_depth(sqrt(2) + r3) == 2\n    assert sqrt_depth(r2 * sqrt(3)) == 2\n    assert sqrt_depth(r2 + sqrt(r3)) == 2\n    assert sqrt_depth(sqrt(r2) + r3) == 2\n    assert sqrt_depth(r2 * r3) == 1\n    assert sqrt_depth(r2 + sqrt(r3 + r5)) == 3\n    assert sqrt_depth(r2 * sqrt(r3 + r5)) == 3\n    assert sqrt_depth(r2 * (r3 + sqrt(r5))) == 2\n    assert sqrt_depth(sqrt(r2 + r3)) == 2\n    assert sqrt_depth(r2 * sqrt(r3 + sqrt(r5))) == 3\n    assert sqrt_depth(r2 * (r3 + sqrt(r5 + r7))) == 3\n    assert sqrt_depth(sqrt(r2 + sqrt(r3 + r5))) == 3\n    assert sqrt_depth(r2 * (r3 + sqrt(r5 + sqrt(r7)))) == 4\n    assert sqrt_depth(sqrt(r2 + sqrt(r3 + sqrt(r5 + r7)))) == 4\n    assert sqrt_depth(r2 + sqrt(r3 + sqrt(r5 + sqrt(r7 + r10)))) == 5\n    assert sqrt_depth(sqrt(r2 + sqrt(r3 + sqrt(r5 + sqrt(r7 + r10))))) == 5\n    assert sqrt_depth(sqrt(r2 + sqrt(r3 + sqrt(r5 + sqrt(r7 + sqrt(r10 + r15))))) == 6\n    assert sqrt_depth(sqrt(r2 + sqrt(r3 + sqrt(r5 + sqrt(r7 + sqrt(r10 + sqrt(r15 + r29)))))) == 7\n    assert sqrt_depth(r2 * (r3 + sqrt(r5 + sqrt(r7 + sqrt(r10 + sqrt(r15 + sqrt"], "sample_1027": ["def test_issue_15000():\n    f = Poly((2*x + 2)**10000000000)\n\n    assert f.factor_list() == (1, [(Poly(2*x + 2), 10000000000)])\n", "def test_Poly_pow_zero():\n    assert Poly(x).pow(0) == Poly(1, x, domain=ZZ)\n    assert Poly(x/2).pow(0) == Poly(1, x, domain=QQ)\n\n    assert Poly(x**2 + 1).pow(0) == Poly(1, x, domain=ZZ)\n    assert Poly(x**2 + 1, domain='QQ').pow(0) == Poly(1, x, domain='QQ')\n\n    raises(ValueError, lambda: Poly(x**2 + 1).pow(-1))\n    raises(ValueError, lambda: Poly(x**2 + 1).pow(1/2))\n", "def test_Poly_from_expr_domain_conversion():\n    assert Poly.from_expr(x + 1, domain='RR').rep == DMP([1.0, 1.0], RR)\n    assert Poly.from_expr(x + 1, domain='QQ').rep == DMP([1, 1], QQ)\n    assert Poly.from_expr(x + 1, domain='ZZ').rep == DMP([1, 1], ZZ)\n    assert Poly.from_expr(0.5*x + 1, domain='QQ').rep == DMP([1, S(1)/2], QQ)\n    assert Poly.from_expr(0.5*x + 1, domain='ZZ').rep == DMP([1, 1], ZZ)\n    assert Poly.from_expr(x + 1.0, domain='ZZ').rep == DMP([1, 1], ZZ)\n    assert Poly.from_expr(x + 1.0, domain='QQ').rep == DMP([1, 1], QQ)\n    assert Poly.from_expr(0.5*x + 1.0, domain='QQ').rep == DMP([1, S(1)/2], QQ)\n    assert Poly.from_expr(0.5*x + 1.0, domain='ZZ').rep == DMP([1, 1], ZZ)\n", "def test_Poly_divmod_with_fractions():\n    \"\"\"\n    Test that polynomial division with fractions is handled correctly.\n    \"\"\"\n    assert Poly(x + 1, x).divmod(S(1)/2*x) == (Poly(x + 1, x).quo(S(1)/2*x), Poly(1, x))\n    assert Poly(x + 1, x).divmod(x*S(1)/2) == (Poly(x + 1, x).quo(x*S(1)/2), Poly(1, x))\n", "def test_Poly___mul__():\n    F = Poly(x, x)\n    G = Poly(y, y)\n    H = Poly(z, z)\n    \n    assert _strict_eq(F * G, Poly(x*y, x, y))\n    assert _strict_eq(G * F, Poly(x*y, x, y))\n    assert _strict_eq(F * H, Poly(x*z, x, y, z))\n    assert _strict_eq(H * F, Poly(x*z, x, y, z))\n\n    assert _strict_eq(F * (G + H), Poly(x*y + x*z, x, y, z))\n    assert _strict_eq((F + G) * H, Poly(x*z + y*z, x, y, z))\n    assert _strict_eq(F * (G - H), Poly(x*y - x*z, x, y, z))\n    assert _strict_eq((F - G) * H, Poly(x*z - y*z, x, y, z))\n\n    assert _strict_eq(F * 0, Poly(0, x, y))\n    assert _strict_eq(F * 1, F)\n    assert _strict_eq(F * S.Half, Poly(x/2, x, y))\n    assert _strict_eq(F * S.Half, Poly(x/2, x, y))\n    assert _strict_eq(F * 0, Poly(0, x, y))\n    assert _strict_eq(F * 1, F)\n    assert _strict_eq(F * S.Half, Poly(x/2, x, y))\n    assert _strict_eq(F * S.Half, Poly(x/2, x, y))\n    assert _strict_eq(F * S.Half, Poly(x/2, x, y))\n    assert _strict_eq(F * S.Half, Poly(x/2, x, y))\n    assert _strict_eq(F * S.Half, Poly(x/2, x, y))\n    assert _strict_eq(F * S.Half, Poly(x/2, x, y))\n    assert _strict_eq(F * S.Half, Poly(x/2, x, y))\n    assert _strict_eq(F * S.Half, Poly(x/2, x, y))\n    assert _strict_eq(F * S.Half, Poly(x/2, x, y))\n    assert _strict_eq(F * S.Half, Poly(x/2, x, y))\n    assert"], "sample_394": ["    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n        cls.s1 = Section.objects.create(name=\"Test section\")\n        cls.a1 = Article.objects.create(\n            content=\"<p>Middle content</p>\",\n            date=datetime.datetime(2008, 3, 18, 11, 54, 58),\n            section=cls.s1,\n        )\n        cls.a2 = Article.objects.create(\n            content=\"<p>Oldest content</p>\",\n            date=datetime.datetime(2000, 3, 18, 11, 54, 58),\n            section=cls.s1,\n        )\n        cls.a3 = Article.objects.create(\n            content=\"<p>Newest content</p>\",\n            date=datetime.datetime(2009, 3, 18, 11, 54, 58),\n            section=cls.s1,\n        )\n        cls.p1 = PrePopulatedPost.objects.create(\n            title=\"A Long Title\", published=True, slug=\"a-long-title\"\n        )\n        cls.color1 = Color.objects.create(value=\"Red\", warm=True)\n        cls.color2 = Color.objects.create(value=\"Orange\", warm=True)\n        cls.color3 = Color.objects.create(value=\"Blue\", warm=False)\n        cls.color4 = Color.objects.create(value=\"Green\", warm=False)\n        cls.fab1 = Fabric.objects.create(surface=\"x\")\n        cls.fab2 = Fabric.objects.create(surface=\"y\")\n        cls.fab3 = Fabric.objects.create(surface=\"plain\")\n        cls.b1 = Book.objects.create(name=\"Book 1\")\n        cls.b2 = Book.objects.create(name=\"Book 2\")\n        cls.pro1 = Promo.objects.create(name=\"Promo 1\", book=cls.b1)\n        cls.pro1 = Promo.objects.create(name=\"Promo 2\", book=cls.b2)\n        cls.chap1 = Chapter.objects.create(\n            title=\"Chapter 1\", content=\"[ insert contents here ]\", book=cls.b1\n        )\n        cls.chap2 = Chapter.objects.create(\n            title=\"Chapter 2\", content=\"[ insert contents here ]\", book=cls.b1\n        )\n        cls.chap3 = Chapter.objects.create(\n            title=\"Chapter 1\", content=\"[ insert contents here ]\",", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n        cls.s1 = Section.objects.create(name=\"Test section\")\n        cls.a1 = Article.objects.create(\n            content=\"<p>Middle content</p>\",\n            date=datetime.datetime(2008, 3, 18, 11, 54, 58),\n            section=cls.s1,\n        )\n        cls.a2 = Article.objects.create(\n            content=\"<p>Oldest content</p>\",\n            date=datetime.datetime(2000, 3, 18, 11, 54, 58),\n            section=cls.s1,\n        )\n        cls.a3 = Article.objects.create(\n            content=\"<p>Newest content</p>\",\n            date=datetime.datetime(2009, 3, 18, 11, 54, 58),\n            section=cls.s1,\n        )\n        cls.p1 = PrePopulatedPost.objects.create(\n            title=\"A Long Title\", published=True, slug=\"a-long-title\"\n        )\n        cls.color1 = Color.objects.create(value=\"Red\", warm=True)\n        cls.color2 = Color.objects.create(value=\"Orange\", warm=True)\n        cls.color3 = Color.objects.create(value=\"Blue\", warm=False)\n        cls.color4 = Color.objects.create(value=\"Green\", warm=False)\n        cls.fab1 = Fabric.objects.create(surface=\"x\")\n        cls.fab2 = Fabric.objects.create(surface=\"y\")\n        cls.fab3 = Fabric.objects.create(surface=\"plain\")\n        cls.b1 = Book.objects.create(name=\"Book 1\")\n        cls.b2 = Book.objects.create(name=\"Book 2\")\n        cls.pro1 = Promo.objects.create(name=\"Promo 1\", book=cls.b1)\n        cls.pro1 = Promo.objects.create(name=\"Promo 2\", book=cls.b2)\n        cls.chap1 = Chapter.objects.create(\n            title=\"Chapter 1\", content=\"[ insert contents here ]\", book=cls.b1\n        )\n        cls.chap2 = Chapter.objects.create(\n            title=\"Chapter 2\", content=\"[ insert contents here ]\", book=cls.b1\n        )\n        cls.chap3 = Chapter.objects.create(\n            title=\"Chapter 1\", content=\"[ insert contents here ]", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n        cls.s1 = Section.objects.create(name=\"Test section\")\n        cls.a1 = Article.objects.create(\n            content=\"<p>Middle content</p>\",\n            date=datetime.datetime(2008, 3, 18, 11, 54, 58),\n            section=cls.s1,\n        )\n        cls.a2 = Article.objects.create(\n            content=\"<p>Oldest content</p>\",\n            date=datetime.datetime(2000, 3, 18, 11, 54, 58),\n            section=cls.s1,\n        )\n        cls.a3 = Article.objects.create(\n            content=\"<p>Newest content</p>\",\n            date=datetime.datetime(2009, 3, 18, 11, 54, 58),\n            section=cls.s1,\n        )\n        cls.p1 = PrePopulatedPost.objects.create(\n            title=\"A Long Title\", published=True, slug=\"a-long-title\"\n        )\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n        cls.actor1 = Actor.objects.create(name=\"Harrison Ford\", age=60)\n        cls.actor2 = Actor.objects.create(name=\"Carrie Fisher\", age=55)\n        cls.actor3 = Actor.objects.create(name=\"Mark Hamill\", age=65)\n        cls.actor4 = Actor.objects.create(name=\"Alec Guinness\", age=80)\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n        cls.color1 = Color.objects.create(value=\"Red\", warm=True)\n        cls.color2 = Color.objects.create(value=\"Orange\", warm=True)\n        cls.color3 = Color.objects.create(value=\"Blue\", warm=False)\n        cls.color4 = Color.objects.create(value=\"Green\", warm=False)\n"], "sample_84": ["    def test_valid_date(self):\n        date = 'Sun, 06 Nov 1994 08:49:37 GMT'\n        self.assertEqual(parse_http_date_safe(date), parse_http_date(date))\n", "    def test_parsing_rfc1123(self):\n        parsed = parse_http_date_safe('Sun, 06 Nov 1994 08:49:37 GMT')\n        self.assertEqual(datetime.utcfromtimestamp(parsed), datetime(1994, 11, 6, 8, 49, 37))\n", "    def test_valid_dates(self):\n        t = 1167616461.0\n        self.assertEqual(parse_http_date_safe('Mon, 01 Jan 2007 01:54:21 GMT'), t)\n        self.assertEqual(parse_http_date_safe('Sunday, 06-Nov-94 08:49:37 GMT'), 784111317)\n        self.assertEqual(parse_http_date_safe('Sun Nov  6 08:49:37 1994'), 784111317)\n", "    def test_default_fields_limit(self):\n        self.assertEqual(limited_parse_qsl('a=1&b=2&c=3'), [('a', '1'), ('b', '2'), ('c', '3')])\n", "    def test_parse_qs_with_default_args(self):\n        query_string = \"a=1&b=2&c=3\"\n        expected_result = [('a', '1'), ('b', '2'), ('c', '3')]\n        self.assertEqual(limited_parse_qsl(query_string), expected_result)\n"], "sample_192": ["def test_absolute_max_with_can_order(self):\n    data = {\n        'form-TOTAL_FORMS': '1001',\n        'form-INITIAL_FORMS': '0',\n        'form-MIN_NUM_FORMS': '0',\n        'form-MAX_NUM_FORMS': '0',\n        'form-0-ORDER': '1',\n    }\n    LimitedFavoriteDrinksFormSet = formset_factory(\n        FavoriteDrinkForm,\n        max_num=30,\n        absolute_max=1000,\n        can_order=True,\n    )\n    formset = LimitedFavoriteDrinksFormSet(data=data)\n    self.assertIs(formset.is_valid(), False)\n    self.assertEqual(len(formset.forms), 1000)\n    self.assertEqual(\n        formset.non_form_errors(),\n        ['Please submit 30 or fewer forms.'],\n    )\n", "def test_non_form_errors_after_is_valid(self):\n    \"\"\"\n    If non_form_errors() is called after is_valid() on a valid formset,\n    it should return an empty ErrorList.\n    \"\"\"\n    data = {\n        'drinks-TOTAL_FORMS': '1',  # the number of forms rendered\n        'drinks-INITIAL_FORMS': '0',  # the number of forms with initial data\n        'drinks-MIN_NUM_FORMS': '0',  # min number of forms\n        'drinks-MAX_NUM_FORMS': '0',  # max number of forms\n        'drinks-0-name': 'Gin and Tonic',\n    }\n    formset = FavoriteDrinksFormSet(data, prefix='drinks')\n    self.assertTrue(formset.is_valid())\n    self.assertEqual(formset.non_form_errors(), [])\n", "def test_absolute_max_with_min_num(self):\n    \"\"\"\n    A formset's min_num and absolute_max can be greater than 0.\n    \"\"\"\n    data = {\n        'form-TOTAL_FORMS': '3',  # the number of forms rendered\n        'form-INITIAL_FORMS': '0',  # the number of forms with initial data\n        'form-MIN_NUM_FORMS': '2',  # min number of forms\n        'form-MAX_NUM_FORMS': '0',  # max number of forms\n        'form-0-name': 'Gin Tonic',\n        'form-1-name': 'Bloody Mary',\n        'form-2-name': 'Jack and Coke',\n    }\n    LimitedFavoriteDrinksFormSet = formset_factory(\n        FavoriteDrinkForm,\n        min_num=2,\n        absolute_max=3,\n    )\n    formset = LimitedFavoriteDrinksFormSet(data=data)\n    self.assertTrue(formset.is_valid())\n    self.assertEqual(len(formset.forms), 3)\n", "def test_max_num_zero_with_initial_and_can_order(self):\n    \"\"\"\n    If max_num is 0, the formset can still display forms with initial data\n    when can_order is True.\n    \"\"\"\n    initial = [{'choice': 'Fernet and Coke'}]\n    LimitedFavoriteDrinkFormSet = formset_factory(FavoriteDrinkForm, extra=1, max_num=0, can_order=True)\n    formset = LimitedFavoriteDrinkFormSet(initial=initial)\n    self.assertHTMLEqual(\n        '\\n'.join(str(form) for form in formset.forms),\n        \"\"\"<tr><th><label for=\"id_form-0-name\">Name:</label></th>", "def test_formset_absolute_max_with_management_form_data(self):\n    \"\"\"\n    If management form's data contains absolute_max and it's lower than\n    DEFAULT_MAX_NUM, it will be used as the hard limit.\n    \"\"\"\n    data = {\n        'form-TOTAL_FORMS': '2',\n        'form-INITIAL_FORMS': '0',\n        'form-MIN_NUM_FORMS': '0',\n        'form-MAX_NUM_FORMS': '0',\n        'form-0-name': 'Test',\n        'form-1-name': 'Test',\n    }\n    LimitedFavoriteDrinksFormSet = formset_factory(\n        FavoriteDrinkForm,\n        extra=2,\n        max_num=DEFAULT_MAX_NUM,\n    )\n    formset = LimitedFavoriteDrinksFormSet(data, prefix='form')\n    self.assertEqual(len(formset.forms), 2)\n\n    # management form's data contains absolute_max\n    data['form-ABSOLUTE_MAX'] = 1\n    formset = LimitedFavoriteDrinksFormSet(data, prefix='form')\n    self.assertEqual(len(formset.forms), 1)\n"], "sample_643": ["def test_colorized_text_reporter_with_custom_color_mapping() -> None:\n    \"\"\"Test the colorized text reporter with a custom color mapping.\"\"\"\n    output = StringIO()\n    linter = PyLinter(reporter=ColorizedTextReporter(output, color_mapping={\n        \"I\": MessageStyle(\"red\"),\n        \"C\": MessageStyle(\"green\", (\"bold\",)),\n    }))\n    linter.open()\n    linter.set_current_module(\"my_module\")\n    linter.add_message(\"C0301\", line=1, args=(1, 2))\n    linter.add_message(\"line-too-long\", line=2, args=(3, 4))\n    out_lines = output.getvalue().split(\"\\n\")\n    assert out_lines[1].startswith(\"\\033[32m************* Module my_module\\033[0m\")\n    assert out_lines[2].startswith(\"\\033[32;1mmy_module:1:0: C0301: Line too long (1/2) (line-too-long)\\033[0m\")\n    assert out_lines[3].startswith(\"\\033[32;1mmy_module:2:0: C0301: Line too long (3/4) (line-too-long)\\033[0m\")\n", "def test_colorized_reporter(linter: PyLinter) -> None:\n    output = StringIO()\n    linter.reporter = ColorizedTextReporter(output)\n    linter.set_current_module(\"test_module\")\n    linter.add_message(\"line-too-long\", line=1, args=(1, 2))\n    assert \"Line too long (1/2)\" in output.getvalue()\n", "def test_colorized_text_reporter() -> None:\n    output = StringIO()\n    linter = PyLinter(reporter=ColorizedTextReporter(output))\n    linter.set_option(\"persistent\", False)\n    linter.set_option(\"reports\", True)\n    linter.set_option(\"score\", True)\n\n    linter.open()\n    linter.set_current_module(\"my_mod\")\n    linter.add_message(\"C0301\", line=1, args=(1, 2))\n    linter.add_message(\"line-too-long\", line=2, args=(3, 4))\n    linter.add_message(\"C0114\", line=1, args=(1, 2))\n\n    expected_output = (\n        \"\\x1b[35;1m************* Module my_mod\\x1b[0m\\n\"\n        \"my_mod:1:0: \\x1b[35;1mC0301\\x1b[0m: Line too long (1/2) (\\x1b[35;1mline-too-long\\x1b[0m)\\n\"\n        \"my_mod:2:0: \\x1b[35;1mC0301\\x1b[0m: Line too long (3/4) (\\x1b[35;1mline-too-long\\x1b[0m)\\n\"\n        \"my_mod:1:0: \\x1b[31;1mC0114\\x1b[0m: \\x1b[31;1mMissing module docstring\\x1b[0m (\\x1b[31;1mmissing-module-docstring\\x1b[0m)\\n\"\n    )\n\n    assert output.getvalue() == expected_output\n", "def test_colorized_reporter_custom_color_mapping(tmp_path):\n    output = StringIO()\n    linter = PyLinter(reporter=ColorizedTextReporter(output=output))\n    checkers.initialize(linter)\n    linter.config.persistent = 0\n    source_file = tmp_path / \"somemodule.py\"\n    source_file.write_text('NOT_EMPTY = \"This module is not empty\"\\n')\n    linter.check_single_file_item(FileItem(\"somemodule\", source_file, \"somemodule\"))\n    linter.add_message(\"line-too-long\", line=1, args=(1, 2))\n    linter.generate_reports()\n\n    expected_output = r\"\\x1b\\[32m************* Module somemodule\\x1b\\[0m\\n\"\n    assert output.getvalue().startswith(expected_output)\n\n    # Test custom color mapping\n    custom_color_mapping = {\n        \"I\": MessageStyle(\"blue\"),\n        \"C\": MessageStyle(None, (\"bold\",)),\n        \"R\": MessageStyle(\"magenta\", (\"bold\", \"italic\")),\n        \"W\": MessageStyle(\"magenta\"),\n        \"E\": MessageStyle(\"red\", (\"bold\",)),\n        \"F\": MessageStyle(\"red\", (\"bold\", \"underline\")),\n        \"S\": MessageStyle(\"yellow\", (\"inverse\",)),  # S stands for module Separator\n    }\n    output = StringIO()\n    linter = PyLinter(\n        reporter=ColorizedTextReporter(output=output, color_mapping=custom_color_mapping)\n    )\n    checkers.initialize(linter)\n    linter.config.persistent = 0\n    source_file = tmp_path / \"somemodule.py\"\n    source_file.write_text('NOT_EMPTY = \"This module is not empty\"\\n')\n    linter.check_single_file_item(FileItem(\"somemodule\", source_file, \"somemodule\"))\n    linter.add_message(\"line-too-long\", line=1, args=(1, 2))\n    linter.generate_reports()\n\n    expected_output = r\"\\x1b\\[32m************* Module somemodule\\x1b\\[0m\\n\"\n    assert not output.getvalue().startswith(expected_output)\n    assert r\"\\x1b\\[34m************* Module somemodule\\x1b\\[0m\" in output.getvalue()\n", "def test_colorized_text_reporter_color_mapping(tmp_path):\n    output = StringIO()\n    colorized_reporter = ColorizedTextReporter(output)\n    colorized_reporter.linter = PyLinter()\n\n    # Add a message with color mapping\n    colorized_reporter.add_message(\n        Message(\n            symbol=\"line-too-long\",\n            msg_id=\"C0301\",\n            location=MessageLocationTuple(\"abspath\", \"path\", \"module\", \"obj\", 1, 2, 1, 3),\n            msg=\"Line too long (1/2)\",\n            confidence=HIGH,\n        )\n    )\n\n    # Check the output\n    out_lines = output.getvalue().split(\"\\n\")\n    assert out_lines[0].startswith(\"\\033[35m************* Module \\033[0m\")\n    assert out_lines[1].startswith(\"\\033[35mLine too long (1/2) \\033[0m\")\n\n    # Test with custom color mapping\n    custom_color_mapping = {\n        \"W\": MessageStyle(\"red\", (\"bold\", \"underline\")),\n        \"E\": MessageStyle(\"blue\", (\"italic\",)),\n    }\n    colorized_reporter.color_mapping = custom_color_mapping\n\n    # Add a message with custom color mapping\n    colorized_reporter.add_message(\n        Message(\n            symbol=\"error\",\n            msg_id=\"E0001\",\n            location=MessageLocationTuple(\"abspath\", \"path\", \"module\", \"obj\", 1, 2, 1, 3),\n            msg=\"Error message\",\n            confidence=HIGH,\n        )\n    )\n\n    # Check the output\n    out_lines = output.getvalue().split(\"\\n\")\n    assert out_lines[2].startswith(\"\\033[34;3mError message \\033[0m\")\n\n    # Test with invalid color mapping\n    invalid_color_mapping = {\n        \"W\": MessageStyle(\"invalid_color\", (\"bold\", \"underline\")),\n    }\n    colorized_reporter.color_mapping = invalid_color_mapping\n\n    # Add a message with invalid color mapping\n    colorized_reporter.add_message(\n        Message(\n            symbol=\"warning\",\n            msg_id=\"W0001\",\n            location=MessageLocationTuple(\"abspath\", \"path\", \"module\", \"obj\", 1, 2, 1, 3),\n            msg=\"Warning message\",\n            confidence=HIGH,\n        )\n    )\n\n    # Check the output\n    out_lines = output.getvalue().split(\"\\n"], "sample_1040": ["def test_print_function():\n    f = Function('f')\n    assert mpp.doprint(f(x, y)) == '<mrow><mi>f</mi><mfenced><mi>x</mi><mi>y</mi></mfenced></mrow>'\n    assert mp.doprint(f(x, y)) == '<apply><f/><ci>x</ci><ci>y</ci></apply>'\n", "def test_print_function():\n    f = Function('f')\n    assert mpp.doprint(f(x, y)) == '<mrow><mi>f</mi><mfenced><mi>x</mi><mi>y</mi></mfenced></mrow>'\n    assert mp.doprint(f(x, y)) == '<apply><f/><ci>x</ci><ci>y</ci></apply>'\n    assert mathml(f(x, y), printer='presentation', mul_symbol=\"*\") == '<mrow><mi>f</mi><mfenced><mi>x</mi><mi>y</mi></mfenced></mrow>'\n    assert mathml(f(x, y), mul_symbol=\"*\") == '<apply><f/><ci>x</ci><ci>y</ci></apply>'\n", "def test_print_function():\n    f = Function('f')\n    assert mp.doprint(f(x, 1)) == '<apply><f/><ci>x</ci><cn>1</cn></apply>'\n    assert mpp.doprint(f(x, 1)) == '<mrow><mi>f</mi><mfenced><mi>x</mi><mn>1</mn></mfenced></mrow>'\n", "def test_print_function():\n    f = Function('f')\n    assert mpp.doprint(f(x)) == '<mrow><mi>f</mi><mfenced><mi>x</mi></mfenced></mrow>'\n    assert mp.doprint(f(x)) == '<apply><ci>f</ci><ci>x</ci></apply>'\n    assert mathml(f(x, y), printer='presentation') == '<mrow><mi>f</mi><mfenced><mi>x</mi><mi>y</mi></mfenced></mrow>'\n    assert mathml(f(x, y)) == '<apply><ci>f</ci><ci>x</ci><ci>y</ci></apply>'\n", "def test_print_function_with_symbol_name():\n    f = Function(Symbol('f'))\n    assert mpp.doprint(f(x, y)) == '<mrow><mi>f</mi><mfenced><mi>x</mi><mi>y</mi></mfenced></mrow>'\n    assert mp.doprint(f(x, y)) == '<apply><f/><ci>x</ci><ci>y</ci></apply>'\n"], "sample_581": ["def test_blueprint_setup_state(app, client):\n    class CustomBlueprintSetupState(BlueprintSetupState):\n            super().__init__(*args, **kwargs)\n            self.custom_attribute = \"custom_value\"\n\n    class CustomBlueprint(Blueprint):\n            return CustomBlueprintSetupState(self, app, options, first_registration)\n\n    bp = CustomBlueprint(\"test\", __name__)\n\n        assert state.custom_attribute == \"custom_value\"\n\n    bp.record(custom_setup)\n\n    app.register_blueprint(bp)\n\n    client.get(\"/\")\n", "def test_blueprint_url_defaults_with_nested_blueprints(app, client):\n    parent = flask.Blueprint(\"parent\", __name__)\n    child = flask.Blueprint(\"child\", __name__)\n\n    @parent.url_defaults\n        values.setdefault(\"parent\", \"parent\")\n\n    @child.url_defaults\n        values.setdefault(\"child\", \"child\")\n\n    @parent.route(\"/parent/<parent>\")\n        return flask.url_for(\".child_route\", child=child)\n\n    @child.route(\"/child/<child>\")\n        return f\"{parent}/{child}\"\n\n    parent.register_blueprint(child, url_prefix=\"/child\")\n    app.register_blueprint(parent, url_prefix=\"/parent\")\n\n    assert client.get(\"/parent/parent/child/child\").data == b\"parent/child\"\n", "def test_blueprint_setup_state_properties(app, client):\n    bp = flask.Blueprint(\"test\", __name__)\n\n    with app.test_request_context(\"/\"):\n        state = bp.make_setup_state(app, {})\n\n        assert state.app is app\n        assert state.blueprint is bp\n        assert state.options == {}\n        assert state.first_registration is False\n        assert state.subdomain is None\n        assert state.url_prefix is None\n        assert state.name == \"test\"\n        assert state.name_prefix == \"\"\n\n        bp2 = flask.Blueprint(\"test2\", __name__, subdomain=\"sub\", url_prefix=\"/prefix\")\n        state = bp2.make_setup_state(app, {\"subdomain\": \"new_sub\", \"url_prefix\": \"/new_prefix\"})\n        assert state.subdomain == \"new_sub.sub\"\n        assert state.url_prefix == \"/prefix/new_prefix\"\n", "def test_blueprint_name_prefix(app, client):\n    parent = flask.Blueprint(\"parent\", __name__)\n\n    @parent.route(\"/\")\n        return \"Parent\"\n\n    child = flask.Blueprint(\"child\", __name__)\n\n    @child.route(\"/\")\n        return \"Child\"\n\n    app.register_blueprint(parent)\n    parent.register_blueprint(child, name_prefix=\"grand\")\n\n    assert client.get(\"/\").data == b\"Parent\"\n    assert client.get(\"/grand.child/\").data == b\"Child\"\n\n    @app.route(\"/\")\n        return flask.url_for(\"parent_index\")\n\n    assert client.get(\"/\").data == b\"/\"\n    app.register_blueprint(parent, name_prefix=\"great\")\n    assert client.get(\"/\").data == b\"/great.parent_index\"\n\n    parent.register_blueprint(child, name=\"other\")\n    assert client.get(\"/other/\").data == b\"Child\"\n    assert client.get(\"/grand.child/\").status_code == 404\n", "def test_blueprint_url_defaults_with_nested_blueprint(app, client):\n    parent = flask.Blueprint(\"parent\", __name__)\n\n    @parent.route(\"/\")\n        return \"\"\n\n    child = flask.Blueprint(\"child\", __name__)\n\n    @child.route(\"/\")\n        return \"\"\n\n    @child.route(\"/bar\")\n        return \"\"\n\n    parent.register_blueprint(child, url_prefix=\"/child\")\n\n    @parent.url_defaults\n        values.setdefault(\"lang_code\", \"en\")\n\n    @parent.url_value_preprocessor\n        flask.g.lang_code = values.pop(\"lang_code\")\n\n    app.register_blueprint(parent, url_prefix=\"/<lang_code>\")\n\n    assert client.get(\"/en/\").data == b\"\"\n    assert client.get(\"/en/child/\").data == b\"\"\n    assert client.get(\"/en/child/bar\").data == b\"\"\n\n    # Make sure url defaults are not applied to parent blueprint\n    @app.route(\"/test\")\n        return \"\"\n\n    assert client.get(\"/en/test\").data == b\"\"\n\n    # Test with url prefix and defaults\n    bp = flask.Blueprint(\"bp\", __name__, url_prefix=\"/<lang_code>\")\n\n    @bp.route(\"/\")\n        return \"\"\n\n    @bp.route(\"/bar\")\n        return \"\"\n\n    @bp.url_defaults\n        values.setdefault(\"lang_code\", \"en\")\n\n    @bp.url_value_preprocessor\n        flask.g.lang_code = values.pop(\"lang_code\")\n\n    app.register_blueprint(bp, url_prefix=\"/api\")\n\n    assert client.get(\"/api/en/\").data == b\"\"\n    assert client.get(\"/api/en/bar\").data == b\"\"\n\n    # Test with url defaults and subdomain\n    bp = flask.Blueprint(\"bp\", __name__, subdomain=\"api\")\n\n    @bp.route(\"/\")\n        return \"\"\n\n    @bp.route(\"/bar\")\n        return \"\"\n\n    @bp.url_defaults\n        values.setdefault(\"lang_code\", \"en\")\n\n    @bp.url_value_preprocessor\n        flask.g.lang_code = values.pop(\"lang_code\")\n\n    app.register_blueprint(bp, url_prefix=\"/<lang_code>"], "sample_993": ["def test_FreeGroupElm_cyclic_reduction():\n    w1 = x**2*y**2*x**-1\n    w2 = x*y**2\n    assert w1.is_cyclically_reduced() == False\n    assert w1.cyclic_reduction() == w2\n    assert w2.is_cyclically_reduced() == True\n\n    w1 = x**-3*y**-1*x**5\n    w2 = x**2*y**-1\n    assert w1.is_cyclically_reduced() == False\n    assert w1.cyclic_reduction() == w2\n    assert w2.is_cyclically_reduced() == True\n\n    w1 = x**2*y**2*x**-1\n    w2 = x*y**2\n    w3 = x**-1\n    assert w1.is_cyclically_reduced() == False\n    assert w1.cyclic_reduction(removed=True) == (w2, w3)\n    assert w2.is_cyclically_reduced() == True\n\n    w1 = x**-3*y**-1*x**5\n    w2 = y**-1*x**2\n    w3 = x**-3\n    assert w1.is_cyclically_reduced() == False\n    assert w1.cyclic_reduction(removed=True) == (w2, w3)\n    assert w2.is_cyclically_reduced() == True\n", "def test_FreeGroupElm_cyclic_reduction():\n    assert (x**2*y**2*x**-1).cyclic_reduction() == x*y**2\n    assert (x**-3*y**-1*x**5).cyclic_reduction() == x**2*y**-1\n    assert (x**-2*y**-1*x).cyclic_reduction() == y*x\n    assert (x*y*x**2*y**2).cyclic_reduction() == y*x**2*y**2\n\n    assert (x**2*y**2*x**-1).cyclic_reduction(removed=True) == (x*y**2, x)\n    assert (x**-3*y**-1*x**5).cyclic_reduction(removed=True) == (x**2*y**-1, x**-3)\n    assert (x**-2*y**-1*x).cyclic_reduction(removed=True) == (y*x, x**-2)\n    assert (x*y*x**2*y**2).cyclic_reduction(removed=True) == (y*x**2*y**2, x)\n", "def test_FreeGroupElm_cyclic_reduction():\n    assert (x**2*y**2*x**-1).cyclic_reduction() == x*y**2\n    assert (x**-3*y**-1*x**5).cyclic_reduction() == x**2*y**-1\n\n    w1 = x**3*y**3*x**-3*y**-3\n    assert w1.cyclic_reduction() == F.identity\n\n    w2 = x**-3*y**3*x**3*y**-3\n    assert w2.cyclic_reduction() == F.identity\n\n    w3 = x**2*y*x**-2*y\n    assert w3.cyclic_reduction() == x*y*x**-1*y\n\n    w4 = x**2*y*x*y**-1*x**-2\n    assert w4.cyclic_reduction() == F.identity\n", "def test_FreeGroupElm_cyclic_reduction():\n    assert (x**2*y**2*x**-1).cyclic_reduction() == x*y**2\n    assert (x**-3*y**-1*x**5).cyclic_reduction() == x**2*y**-1\n    assert (x**2*y*x**-2*y*x**2*y**-2).cyclic_reduction() == y\n    assert (x*y*x**-1*y**2*x**-1*y**-1).cyclic_reduction() == y**2\n\n    # Test removed option\n    w, r = (x**2*y**2*x**-1).cyclic_reduction(removed=True)\n    assert w == x*y**2\n    assert r == x**2\n", "def test_FreeGroupElm_cyclic_reduction():\n    w = x**2*y**2*x**-1\n    assert w.cyclic_reduction() == x*y**2\n    w = x**-3*y**-1*x**5\n    assert w.cyclic_reduction() == x**2*y**-1\n    w = x**2*y**2*x**-1*y**2\n    assert w.cyclic_reduction() == y**2*x*y**2\n    w = x*y**-1*x**2*y**-1*x\n    assert w.cyclic_reduction() == x**2*y**-1*x*y**-1\n"], "sample_187": ["def test_camel_case_to_spaces(self):\n    self.assertEqual(text.camel_case_to_spaces('HelloWorld'), 'hello world')\n    self.assertEqual(text.camel_case_to_spaces('helloWorld'), 'hello world')\n    self.assertEqual(text.camel_case_to_spaces('hello'), 'hello')\n    self.assertEqual(text.camel_case_to_spaces('HelloWorldTest'), 'hello world test')\n    self.assertEqual(text.camel_case_to_spaces(''), '')\n    self.assertEqual(text.camel_case_to_spaces(lazystr('HelloWorld')), 'hello world')\n", "def test_camel_case_to_spaces(self):\n    self.assertEqual(text.camel_case_to_spaces('helloWorld'), 'hello world')\n    self.assertEqual(text.camel_case_to_spaces('HelloWorld'), 'hello world')\n    self.assertEqual(text.camel_case_to_spaces('hello'), 'hello')\n    self.assertEqual(text.camel_case_to_spaces(''), '')\n    self.assertEqual(text.camel_case_to_spaces(lazystr('helloWorld')), 'hello world')\n    self.assertEqual(text.camel_case_to_spaces('helloWorldOfDjango'), 'hello world of django')\n", "def test_camel_case_to_spaces(self):\n    self.assertEqual(text.camel_case_to_spaces('helloWorld'), 'hello world')\n    self.assertEqual(text.camel_case_to_spaces('HelloWorld'), 'hello world')\n    self.assertEqual(text.camel_case_to_spaces('Hello'), 'hello')\n    self.assertEqual(text.camel_case_to_spaces('hello'), 'hello')\n    self.assertEqual(text.camel_case_to_spaces('helloWorldFoo'), 'hello world foo')\n    self.assertEqual(text.camel_case_to_spaces('HelloWorldFoo'), 'hello world foo')\n    self.assertEqual(text.camel_case_to_spaces('HelloWORLD'), 'hello world')\n    self.assertEqual(text.camel_case_to_spaces(''), '')\n    self.assertEqual(text.camel_case_to_spaces(lazystr('helloWorld')), 'hello world')\n", "def test_camel_case_to_spaces(self):\n    items = (\n        ('HelloWorld', 'hello world'),\n        ('Hello', 'hello'),\n        ('helloWorld', 'hello world'),\n        ('', ''),\n        ('aLongStringWithNumbers123', 'a long string with numbers123'),\n    )\n    for value, output in items:\n        self.assertEqual(text.camel_case_to_spaces(value), output)\n        self.assertEqual(text.camel_case_to_spaces(lazystr(value)), output)\n", "def test_camel_case_to_spaces(self):\n    items = [\n        ('helloWorld', 'hello world'),\n        ('HelloWorld', 'hello world'),\n        ('helloHTMLWorld', 'hello html world'),\n        ('helloHTML5World', 'hello html5 world'),\n        ('hello123World', 'hello123 world'),\n        ('helloWorld123', 'hello world123'),\n        ('hello-world', 'hello-world'),  # already has a space\n        ('helloWorld123helloWorld', 'hello world123hello world'),\n    ]\n    for value, output in items:\n        self.assertEqual(text.camel_case_to_spaces(value), output)\n        self.assertEqual(text.camel_case_to_spaces(lazystr(value)), output)\n"], "sample_103": ["def test_default_alias_collision(self):\n    with self.assertRaisesMessage(FieldError, \"The named annotation 'rating' conflicts with a field on the model\"):\n        Book.objects.annotate(rating=Avg('authors__age'))\n\n    with self.assertRaisesMessage(FieldError, \"The named annotation 'rating' conflicts with a field on the model\"):\n        Book.objects.aggregate(rating=Avg('authors__age'))\n", "def test_std_dev_with_negative_numbers(self):\n    \"\"\"Test that the standard deviation can handle negative numbers.\"\"\"\n    Book.objects.filter(pk=self.b1.pk).delete()\n    Book.objects.filter(pk=self.b2.pk).delete()\n    Book.objects.create(\n        isbn='159059725', name='The Definitive Guide to Django: Web Development Done Right',\n        pages=447, rating=-4.5, price=Decimal('30.00'), contact=self.a1, publisher=self.p1,\n        pubdate=datetime.date(2007, 12, 6)\n    )\n    Book.objects.create(\n        isbn='067232959', name='Sams Teach Yourself Django in 24 Hours',\n        pages=528, rating=-3.0, price=Decimal('23.09'), contact=self.a3, publisher=self.p2,\n        pubdate=datetime.date(2008, 3, 3)\n    )\n    self.assertEqual(\n        Book.objects.aggregate(std_dev=StdDev('rating'))['std_dev'],\n        Approximate(0.75, places=2)\n    )\n", "def test_aggregate_over_annotate_with_filter(self):\n    \"\"\"\n    Test that aggregates over annotations with a filter work correctly.\n    \"\"\"\n    qs = Author.objects.annotate(\n        total_friends=Count(\"friends__id\")\n    ).filter(total_friends__gt=1).aggregate(avg_age=Avg(\"age\"))\n    self.assertEqual(qs[\"avg_age\"], Approximate(36.6, places=1))\n\n    qs = Author.objects.annotate(\n        total_friends=Count(\"friends__id\")\n    ).filter(total_friends__gt=1).annotate(avg_age=Avg(\"age\")).aggregate(total_age=Sum(\"avg_age\"))\n    self.assertEqual(qs[\"total_age\"], 109.8)\n", "def test_aggregate_filter_with_empty_source(self):\n    vals = Book.objects.aggregate(Avg(\"rating\", filter=Q(pages__gt=100)))\n    self.assertEqual(vals, {\"rating__avg\": Approximate(4.24, places=2)})\n\n    vals = Book.objects.aggregate(Sum(\"price\", filter=Q(pages__gt=100)))\n    self.assertEqual(vals, {\"price__sum\": Approximate(Decimal(\"220.48\"), places=2)})\n\n    vals = Book.objects.aggregate(Max(\"pages\", filter=Q(pages__gt=100)))\n    self.assertEqual(vals, {\"pages__max\": 1132})\n\n    vals = Book.objects.aggregate(Min(\"pages\", filter=Q(pages__gt=100)))\n    self.assertEqual(vals, {\"pages__min\": 300})\n\n    vals = Book.objects.aggregate(Count(\"pages\", filter=Q(pages__gt=100)))\n    self.assertEqual(vals, {\"pages__count\": 5})\n", "def test_aggregate_with_empty_query(self):\n    # Create a Book with an empty ManyToMany field (authors)\n    Book.objects.create(\n        isbn='123456789', name='Empty Authors Book',\n        pages=0, rating=0.0, price=Decimal('0.00'), contact=None, publisher=self.p1,\n        pubdate=datetime.date(2000, 1, 1)\n    )\n\n    # Test that aggregates over an empty ManyToMany field don't crash\n    vals = Book.objects.aggregate(Avg(\"authors__age\"))\n    self.assertIsNone(vals[\"authors__age__avg\"])\n\n    vals = Book.objects.aggregate(Sum(\"authors__age\"))\n    self.assertIsNone(vals[\"authors__age__sum\"])\n\n    vals = Book.objects.aggregate(Count(\"authors__age\"))\n    self.assertEqual(vals[\"authors__age__count\"], 0)\n\n    vals = Book.objects.aggregate(Max(\"authors__age\"))\n    self.assertIsNone(vals[\"authors__age__max\"])\n\n    vals = Book.objects.aggregate(Min(\"authors__age\"))\n    self.assertIsNone(vals[\"authors__age__min\"])\n"], "sample_983": ["def test_row_structure_symbolic_cholesky():\n    M = SparseMatrix([\n        [1, 0, 3, 2],\n        [0, 0, 1, 0],\n        [4, 0, 0, 5],\n        [0, 6, 7, 0]])\n    assert M.row_structure_symbolic_cholesky() == [[0], [], [0], [1, 2]]\n", "def test_sparse_matrix_applyfunc():\n    m = SparseMatrix(3, 3, lambda i, j: i + j)\n    assert m.applyfunc(lambda x: x**2) == SparseMatrix(3, 3, lambda i, j: (i + j)**2)\n    assert m.applyfunc(lambda x: x + 1) == SparseMatrix(3, 3, lambda i, j: i + j + 1)\n    assert m.applyfunc(lambda x: x % 2) == SparseMatrix(3, 3, lambda i, j: (i + j) % 2)\n", "def test_sparse_matrix_fill():\n    # Test fill method\n    s = SparseMatrix.eye(3)\n    s.fill(2)\n    assert s == SparseMatrix(3, 3, lambda i, j: 2)\n    \n    # Test fill method with 0\n    s = SparseMatrix.eye(3)\n    s.fill(0)\n    assert s == SparseMatrix.zeros(3)\n    \n    # Test fill method with Symbol\n    x = Symbol('x')\n    s = SparseMatrix.eye(3)\n    s.fill(x)\n    assert s == SparseMatrix(3, 3, lambda i, j: x)\n", "def test_sparse_matrix_operations():\n    # Test LDL decomposition\n    A = SparseMatrix(((25, 15, -5), (15, 18, 0), (-5, 0, 11)))\n    L, D = A.LDLdecomposition()\n    assert L.is_lower\n    assert D.is_diagonal\n    assert L * D * L.T == A\n\n    # Test Cholesky decomposition\n    A = SparseMatrix(((25, 15, -5), (15, 18, 0), (-5, 0, 11)))\n    L = A.cholesky()\n    assert L.is_lower\n    assert L * L.T == A\n\n    # Test matrix inverse\n    A = SparseMatrix(((2, -1, 0), (-1, 2, -1), (0, 0, 2)))\n    A_inv = A.inv()\n    assert A * A_inv == SparseMatrix.eye(3)\n\n    # Test matrix multiplication\n    A = SparseMatrix(((1, 2), (3, 4)))\n    B = SparseMatrix(((5, 6), (7, 8)))\n    C = A * B\n    assert C == SparseMatrix(((19, 22), (43, 50)))\n\n    # Test matrix addition\n    A = SparseMatrix(((1, 2), (3, 4)))\n    B = SparseMatrix(((5, 6), (7, 8)))\n    C = A + B\n    assert C == SparseMatrix(((6, 8), (10, 12)))\n\n    # Test matrix subtraction\n    A = SparseMatrix(((1, 2), (3, 4)))\n    B = SparseMatrix(((5, 6), (7, 8)))\n    C = A - B\n    assert C == SparseMatrix(((-4, -4), (-4, -4)))\n\n    # Test scalar multiplication\n    A = SparseMatrix(((1, 2), (3, 4)))\n    B = 2 * A\n    assert B == SparseMatrix(((2, 4), (6, 8)))\n\n    # Test element-wise multiplication\n    A = SparseMatrix(((1, 2), (3, 4)))\n    B = SparseMatrix(((5, 6), (7, 8)))\n    C = A.elementwise(B)\n    assert C == SparseMatrix(((5, 12), (21", "def test_svd():\n    # test the svd decomposition\n    M = SparseMatrix([[1, 0, 0], [0, 0, 0], [0, 0, 0]])\n    U, s, Vh = M.singular_values()\n    assert U == Matrix([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    assert s == Matrix([1, 0, 0])\n    assert Vh == Matrix([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n\n    M = SparseMatrix([[1, 1, 1], [1, 1, 1]])\n    U, s, Vh = M.singular_values()\n    assert U == Matrix([[1, 0], [0, 1]])\n    assert s == Matrix([3, 0])\n    assert Vh == Matrix([[R(1, 3), R(1, 3), R(1, 3)], [R(-1, 2)*R(2, 3), -R(1, 3), R(1, 3)]])\n\n    M = SparseMatrix([[1, 0, 0], [0, 2, 0], [0, 0, 3]])\n    U, s, Vh = M.singular_values()\n    assert U == Matrix([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    assert s == Matrix([3, 2, 1])\n    assert Vh == Matrix([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n\n    M = SparseMatrix([[1, 1], [1, 1]])\n    U, s, Vh = M.singular_values()\n    assert U == Matrix([[R(1, 2)*R(2, 3), R(1, 2)*R(-2, 3)], [R(1, 2)*R(2, 3), R(1, 2)*R(2, 3)]])\n    assert s == Matrix([2, 0])\n    assert Vh == Matrix([[R(1, 2), R(1, 2"], "sample_60": ["    def setUp(self):\n        self.site = AdminSite()\n", "    def setUp(self):\n        self.site = AdminSite()\n", "    def test_get_formfield_callback(self):\n        \"\"\"\n        The formfield_callback passed to formset_factory is correctly set to\n        the inline's formfield_for_dbfield.\n        \"\"\"\n        class MediaForm(ModelForm):\n            class Meta:\n                model = Media\n                fields = ['description', 'url', 'keywords']\n\n        class MediaInline(GenericTabularInline):\n            model = Media\n            form = MediaForm\n\n        class EpisodeAdmin(admin.ModelAdmin):\n            inlines = [MediaInline]\n\n        ma = EpisodeAdmin(Episode, self.site)\n\n            pass\n\n        formset = ma.get_formsets_with_inlines(request)[0][0]()\n        self.assertEqual(formset.formfield_callback, ma.formfield_for_dbfield)\n\n        ma.formfield_callback = custom_formfield_for_dbfield\n        formset = ma.get_formsets_with_inlines(request)[0][0]()\n        self.assertEqual(formset.formfield_callback, custom_formfield_for_dbfield)\n", "    def test_get_actions_returns_actions_from_site(self):\n        actions = ['some_action', 'another_action']\n        site = AdminSite(name='test')\n        site.actions = actions\n        admin = ModelAdmin(Episode, site)\n        request = MockRequest()\n        request.user = MockSuperUser()\n        self.assertEqual(admin.get_actions(request), {'some_action': (None, 'some_action', 'Some action'), 'another_action': (None, 'another_action', 'Another action')})\n", "    def setUp(self):\n        self.site = AdminSite()\n        self.model_admin = ModelAdmin(Episode, self.site)\n"], "sample_1204": ["def test_abelian_invariants():\n    G = AbelianGroup(2, 3, 4)\n    assert G.abelian_invariants() == [2, 3, 4]\n    G=PermutationGroup([Permutation(1, 2, 3, 4), Permutation(1, 2), Permutation(5, 6)])\n    assert G.abelian_invariants() == [2, 2]\n    G = AlternatingGroup(7)\n    assert G.abelian_invariants() == []\n    G = AlternatingGroup(4)\n    assert G.abelian_invariants() == [3]\n    G = DihedralGroup(4)\n    assert G.abelian_invariants() == [2, 2]\n\n    G = PermutationGroup([Permutation(1, 2, 3, 4, 5, 6, 7)])\n    assert G.abelian_invariants() == [7]\n    G = DihedralGroup(12)\n    S = G.sylow_subgroup(3)\n    assert S.abelian_invariants() == [3]\n    G = PermutationGroup([Permutation(0, 1), Permutation(0, 2, 4, 6)(1, 3, 5, 7)])\n    assert G.abelian_invariants() == [2, 4]\n    G = SymmetricGroup(30)\n    S = G.sylow_subgroup(2)\n    assert S.abelian_invariants() == [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n    S = G.sylow_subgroup(3)\n    assert S.abelian_invariants() == [3, 3, 3, 3]\n    S = G.sylow_subgroup(5)\n    assert S.abelian_invariants() == [5, 5, 5]\n\n    G = PermutationGroup([Permutation(0, 1), Permutation(2, 3)])\n    assert G.abelian_invariants() == [2, 2]\n\n    G = AbelianGroup(2, 3, 5)\n    assert G.abelian_invariants() == [2, 3, 5]\n\n    G = PermutationGroup([Permutation(0, ", "def test_strong_presentation():\n    # Check that presentations of various groups have the correct order\n    P = PermutationGroup(Permutation(0,1,5,2)(3,7,4,6), Permutation(0,3,5,4)(1,6,2,7))\n    assert P.strong_presentation().order() == P.order()\n    P = AlternatingGroup(5)\n    assert P.strong_presentation().order() == P.order()\n    P = SymmetricGroup(5)\n    assert P.strong_presentation().order() == P.order()\n", "def test_composition_series_of_symmetric():\n    # Test composition series of SymmetricGroup\n    S = SymmetricGroup(12)\n    G = S.sylow_subgroup(2)\n    comp_series = G.composition_series()\n    assert comp_series == G.derived_series()\n    # The first group in the composition series is always the group itself and\n    # the last group in the series is the trivial group.\n    assert G.composition_series()[0] == G\n    assert len(G.composition_series()) == 7\n    # Test composition series for other cases\n    G = S.sylow_subgroup(3)\n    comp_series = G.composition_series()\n    assert comp_series == G.derived_series()\n    assert G.composition_series()[0] == G\n    assert len(G.composition_series()) == 3\n", "def test_pointwise_stabilizer():\n    S = SymmetricGroup(2)\n    stab = S.pointwise_stabilizer([0, 1])\n    assert stab.generators == [Permutation([0, 1])]\n    a = Permutation(0, 2, 1, 3, 4, 5)\n    b = Permutation(2, 1, 3, 4, 5, 0)\n    G = PermutationGroup([a, b])\n    G23 = G.pointwise_stabilizer([2, 3])\n    assert G23.order() == 10\n    assert G23.is_subgroup(G)\n    assert G23.is_subgroup(G.stabilizer(2).stabilizer(3))\n    a = Permutation(0, 2, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19)\n    b = Permutation(0, 1, 2, 3, 4, 5, 19, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 7, 17, 18)\n    c = Permutation(0, 1, 2, 3, 4, 5, 6, 7, 9, 18, 16, 11, 12, 13, 14, 15, 8, 17, 10, 19)\n    G = PermutationGroup([a, b, c])\n    G2 = G.pointwise_stabilizer([2])\n    assert G2.order() == 181440\n    assert G2.is_subgroup(G)\n    assert G2.is_subgroup(G.stabilizer(2))\n    S = SymmetricGroup(5)\n    points = []\n    stab = S\n    for point in (2, 0, 3, 4, 1):\n        stab = stab.stabilizer(point)\n        points.append(point)\n        assert S.pointwise_stabilizer(points).is_subgroup(stab)\n    points = [1, 3, 5]\n    assert S.pointwise_stabilizer(points).is_subgroup(S.pointwise", "def test_is_dihedral():\n    # Test for large group order\n    G = PermutationGroup(\n        Permutation(1, 5)(2, 4),\n        Permutation(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19),\n        Permutation(20, 21)(22, 23)(24, 25)(26, 27)(28, 29)(30, 31)(32, 33)(34, 35),\n        Permutation(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35)\n    )\n    assert G.is_dihedral is False\n\n    # Test for no 2-Sylow subgroup\n    G = PermutationGroup(\n        Permutation(1, 5)(2, 4),\n        Permutation(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19),\n        Permutation(20, 21)(22, 23)(24, 25)(26, 27)(28, 29)(30, 31)(32, 33)(34, 35),\n        Permutation(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, "], "sample_432": ["def test_view_on_site_url(self):\n    child = Child.objects.create(name=\"child\", parent=None)\n    m = ChildAdmin(Child, custom_site)\n    self.assertIsNone(m.get_view_on_site_url(child))\n\n    parent = Parent.objects.create(name=\"parent\")\n    child = Child.objects.create(name=\"child\", parent=parent)\n    m = ChildAdmin(Child, custom_site)\n    url = reverse(\n        \"admin:view_on_site\",\n        kwargs={\n            \"content_type_id\": get_content_type_for_model(child).pk,\n            \"object_id\": child.pk,\n        },\n        current_app=custom_site.name,\n    )\n    self.assertEqual(m.get_view_on_site_url(child), url)\n", "def test_to_field_allowed(self):\n    parent = Parent.objects.create(name=\"parent\")\n    child = Child.objects.create(name=\"child\", parent=parent)\n\n    request = self.factory.get(\"/child/\", data={TO_FIELD_VAR: \"name\"})\n    request.user = self.superuser\n\n    model_admin = ChildAdmin(Child, custom_site)\n    self.assertFalse(model_admin.to_field_allowed(request, \"name\"))\n\n    request = self.factory.get(\"/child/\", data={TO_FIELD_VAR: \"parent\"})\n    request.user = self.superuser\n\n    model_admin = ChildAdmin(Child, custom_site)\n    self.assertTrue(model_admin.to_field_allowed(request, \"parent\"))\n\n    request = self.factory.get(\"/child/\", data={TO_FIELD_VAR: \"pk\"})\n    request.user = self.superuser\n\n    model_admin = ChildAdmin(Child, custom_site)\n    self.assertTrue(model_admin.to_field_allowed(request, \"pk\"))\n\n    request = self.factory.get(\"/child/\", data={TO_FIELD_VAR: \"nonexistent\"})\n    request.user = self.superuser\n\n    model_admin = ChildAdmin(Child, custom_site)\n    self.assertFalse(model_admin.to_field_allowed(request, \"nonexistent\"))\n", "def test_empty_value_display_html(self):\n    \"\"\"\n    Regression test for #14982: EMPTY_CHANGELIST_VALUE should be honored\n    for relationship fields.\n    \"\"\"\n    new_child = Child.objects.create(name=\"name\", parent=None)\n    request = self.factory.get(\"/child/\")\n    request.user = self.superuser\n    m = ChildAdmin(Child, custom_site)\n    cl = m.get_changelist_instance(request)\n    cl.formset = None\n    template = Template(\n        \"{% load admin_list %}{% spaceless %}{% result_list cl %}{% endspaceless %}\"\n    )\n    context = Context({\"cl\": cl, \"opts\": Child._meta})\n    table_output = template.render(context)\n    link = reverse(\"admin:admin_changelist_child_change\", args=(new_child.id,))\n    # Set EMPTY_CHANGELIST_VALUE to a custom string\n    admin.site.empty_value_display = \"---\"\n    table_output = template.render(context)\n    row_html = build_tbody_html(\n        new_child.id, link, '<td class=\"field-parent nowrap\">---</td>'\n    )\n    self.assertNotEqual(\n        table_output.find(row_html),\n        -1,\n        \"Failed to find expected row element: %s\" % table_output,\n    )\n    # Revert EMPTY_CHANGELIST_VALUE\n    del admin.site.empty_value_display\n    table_output = template.render(context)\n    row_html = build_tbody_html(\n        new_child.id, link, '<td class=\"field-parent nowrap\">-</td>'\n    )\n    self.assertNotEqual(\n        table_output.find(row_html),\n        -1,\n        \"Failed to find expected row element: %s\" % table_output,\n    )\n", "def test_list_editable_and_list_display_links(self):\n    \"\"\"\n    list_editable must be a subset of list_display_links.\n    \"\"\"\n    a = Swallow.objects.create(origin=\"Swallow A\", load=4, speed=1)\n    b = Swallow.objects.create(origin=\"Swallow B\", load=2, speed=2)\n    superuser = self._create_superuser(\"superuser\")\n    self.client.force_login(superuser)\n    changelist_url = reverse(\"admin:admin_changelist_swallow_changelist\")\n    m = SwallowAdmin(Swallow, custom_site)\n    # Make list_editable a superset of list_display_links.\n    m.list_display_links = [\"id\"]\n    m.list_editable = [\"id\", \"origin\"]\n    data = {\n        \"form-TOTAL_FORMS\": \"2\",\n        \"form-INITIAL_FORMS\": \"2\",\n        \"form-MIN_NUM_FORMS\": \"0\",\n        \"form-MAX_NUM_FORMS\": \"1000\",\n        \"form-0-uuid\": str(a.pk),\n        \"form-1-uuid\": str(b.pk),\n        \"form-0-id\": \"1\",\n        \"form-0-origin\": \"Swallow A edited\",\n        \"form-1-origin\": \"Swallow B edited\",\n        \"_save\": \"Save\",\n    }\n    with self.assertRaises(IncorrectLookupParameters):\n        self.client.post(changelist_url, data)\n", "def test_duplicate_filter_params(self):\n    \"\"\"\n    Regression tests for #24297: duplicate filter parameters don't affect the\n    results or the filter links.\n    \"\"\"\n    parent = Parent.objects.create(name=\"parent\")\n    Child.objects.create(parent=parent, name=\"child1\")\n    Child.objects.create(parent=parent, name=\"child2\")\n\n    m = ParentAdmin(Parent, custom_site)\n    request = self.factory.get(\"/parent/\", data={\"child__name\": \"child1\", \"child__name\": \"child1\"})\n    request.user = self.superuser\n\n    cl = m.get_changelist_instance(request)\n    self.assertEqual(cl.queryset.count(), 1)\n\n    # Clear filters link.\n    self.assertContains(request, '<a href=\"?\">&#10006; Clear all filters</a>')\n\n    # Filter link.\n    filter_url = reverse(\"admin:admin_changelist_parent_changelist\") + \"?child__name=child1\"\n    self.assertContains(request, '<a href=\"%s\" class=\"selected\">%s</a>' % (filter_url, \"child1\"))\n"], "sample_762": ["def test_clone_transformer_mixin():\n    # Test that clone works correctly with TransformerMixin\n    from sklearn.preprocessing import StandardScaler\n\n    scaler = StandardScaler()\n    scaler.fit(np.array([[1, 2], [3, 4]]))\n\n    cloned_scaler = clone(scaler)\n\n    assert_array_equal(scaler.mean_, cloned_scaler.mean_)\n    assert_array_equal(scaler.scale_, cloned_scaler.scale_)\n    assert_array_equal(scaler.transform(np.array([[1, 2]])), \n                       cloned_scaler.transform(np.array([[1, 2]])))\n", "def test_is_regressor():\n    # Test that is_regressor correctly identifies regressors\n    from sklearn.linear_model import LinearRegression\n    assert is_regressor(LinearRegression())\n    assert is_regressor(GridSearchCV(LinearRegression(), {}))\n    assert is_regressor(Pipeline([('lr', LinearRegression())]))\n\n    # Test that is_regressor correctly identifies non-regressors\n    from sklearn.svm import SVC\n    assert not is_regressor(SVC())\n    assert not is_regressor(GridSearchCV(SVC(), {}))\n    assert not is_regressor(Pipeline([('svc', SVC())]))\n\n    # Test that is_regressor raises an error for non-estimators\n    assert_raises(AttributeError, is_regressor, \"not an estimator\")\n", "def test_clone_nested_estimators():\n    # Test cloning of nested estimators\n    class NestedEstimator(BaseEstimator):\n            self.estimator = estimator\n\n    class InnerEstimator(BaseEstimator):\n            self.param = param\n\n    inner_est = InnerEstimator(param=1)\n    nested_est = NestedEstimator(estimator=inner_est)\n\n    cloned_nested_est = clone(nested_est)\n    assert cloned_nested_est.estimator.param == 1\n    assert cloned_nested_est.estimator is not inner_est\n\n    # Test cloning of nested estimators with deep=False\n    cloned_nested_est = clone(nested_est, safe=False)\n    assert cloned_nested_est.estimator.param == 1\n    assert cloned_nested_est.estimator is inner_est\n", "def test_clone_mixin():\n    class EstimatorWithMixin(BaseEstimator, TransformerMixin):\n            self.param = param\n\n            return self\n\n            return X\n\n    estimator = EstimatorWithMixin()\n    estimator.param = 1\n    cloned_estimator = clone(estimator)\n\n    assert estimator is not cloned_estimator\n    assert estimator.param == cloned_estimator.param\n    assert hasattr(cloned_estimator, 'fit')\n    assert hasattr(cloned_estimator, 'transform')\n", "def test_clone_sparse_matrices_edge_cases():\n    # Test cloning sparse matrices with zero and zero-like arrays\n    sparse_matrix_classes = [\n        getattr(sp, name)\n        for name in dir(sp) if name.endswith('_matrix')]\n\n    for cls in sparse_matrix_classes:\n        # Test cloning with zero arrays\n        sparse_matrix = cls(np.zeros((0, 0)))\n        clf = MyEstimator(empty=sparse_matrix)\n        clf_cloned = clone(clf)\n        assert clf.empty.__class__ is clf_cloned.empty.__class__\n        assert_array_equal(clf.empty.toarray(), clf_cloned.empty.toarray())\n\n        # Test cloning with zero-like arrays\n        sparse_matrix = cls(np.zeros((0, 5)))\n        clf = MyEstimator(empty=sparse_matrix)\n        clf_cloned = clone(clf)\n        assert clf.empty.__class__ is clf_cloned.empty.__class__\n        assert_array_equal(clf.empty.toarray(), clf_cloned.empty.toarray())\n\n        sparse_matrix = cls(np.zeros((5, 0)))\n        clf = MyEstimator(empty=sparse_matrix)\n        clf_cloned = clone(clf)\n        assert clf.empty.__class__ is clf_cloned.empty.__class__\n        assert_array_equal(clf.empty.toarray(), clf_cloned.empty.toarray())\n"], "sample_536": ["def test_rectangle_selector_rotation_point():\n    tool = widgets.RectangleSelector(ax, onselect=noop, interactive=True)\n    tool.extents = (100, 150, 100, 150)\n    assert tool._selection_artist.rotation_point == 'center'\n\n    with pytest.raises(ValueError):\n        tool._selection_artist.rotation_point = 'unvalid_value'\n\n    tool._selection_artist.rotation_point = 'lower left'\n    assert tool._selection_artist.rotation_point == 'lower left'\n", "def test_range_slider_out_of_bounds(orientation):\n    if orientation == \"vertical\":\n        idx = [1, 0, 3, 2]\n    else:\n        idx = [0, 1, 2, 3]\n\n    fig, ax = plt.subplots()\n\n    slider = widgets.RangeSlider(\n        ax=ax, label=\"\", valmin=0.0, valmax=1.0, orientation=orientation,\n        valinit=[-1, 2]\n    )\n    box = slider.poly.get_extents().transformed(ax.transAxes.inverted())\n    assert_allclose(box.get_points().flatten()[idx], [0.0, 0.25, 1.0, 0.75])\n\n    slider = widgets.RangeSlider(\n        ax=ax, label=\"\", valmin=0.0, valmax=1.0, orientation=orientation,\n        valinit=[1.5, -0.5]\n    )\n    box = slider.poly.get_extents().transformed(ax.transAxes.inverted())\n    assert_allclose(box.get_points().flatten()[idx], [0.0, 0.25, 1.0, 0.75])\n", "def test_rectangle_selector_set_rotation(ax):\n    tool = widgets.RectangleSelector(ax, onselect=noop,\n                                     interactive=True,\n                                     props={'visible': False})\n    tool.extents = (0.2, 0.8, 0.3, 0.7)\n\n    assert tool.rotation == 0\n\n    tool.rotation = 45\n    assert tool.rotation == 45\n    assert tool.extents == (0.2, 0.8, 0.3, 0.7)\n\n    tool.rotation = 45.0\n    assert tool.rotation == 45\n    assert tool.extents == (0.2, 0.8, 0.3, 0.7)\n\n    tool.rotation = -45\n    assert tool.rotation == -45\n    assert tool.extents == (0.2, 0.8, 0.3, 0.7)\n\n    with pytest.raises(ValueError):\n        tool.rotation = 60\n", "def test_range_slider_snap(orientation, valinit):\n    snap_values = np.linspace(0, 1, 11)\n    slider = widgets.RangeSlider(\n        ax=get_ax(), label=\"\", valmin=0, valmax=1, valinit=valinit,\n        orientation=orientation, valstep=snap_values)\n\n    min_value, max_value = slider._min_in_bounds(valinit[0]), slider._max_in_bounds(valinit[1])\n    assert_allclose(slider.val, (min_value, max_value))\n\n    min_value, max_value = slider._min_in_bounds(min_value - 0.1), slider._max_in_bounds(max_value + 0.1)\n    slider.set_val((min_value, max_value))\n    assert_allclose(slider.val, (min_value, max_value))\n", "def test_selector_multiple_state(ax):\n    # Test that adding multiple states to the RectangleSelector\n    # and the PolygonSelector results in the correct behavior.\n    for selector in [\n        widgets.RectangleSelector(ax, onselect=lambda e1, e2: None),\n        widgets.PolygonSelector(ax, onselect=lambda e1, e2: None)\n    ]:\n        selector.add_state('square')\n        selector.add_state('center')\n\n        assert len(selector._state) == 2\n        assert 'square' in selector._state\n        assert 'center' in selector._state\n\n        selector.remove_state('square')\n        assert len(selector._state) == 1\n        assert 'center' in selector._state\n        assert 'square' not in selector._state\n\n        selector.add_state('square')\n        assert len(selector._state) == 2\n        assert 'square' in selector._state\n        assert 'center' in selector._state\n"], "sample_619": ["def test_decode_cf_datetime_invalid_calendar() -> None:\n    num_dates = np.arange(10)\n    units = \"days since 2000-01-01\"\n    calendar = \"invalid_calendar\"\n\n    with pytest.raises(ValueError, match=\"Array does not contain datetime objects.\"):\n        decode_cf_datetime(num_dates, units, calendar=calendar)\n", "def test_decode_cf_datetime_invalid_use_cftime_argument():\n    units = \"days since 2000-01-01\"\n    with pytest.raises(\n        ValueError, match=\"use_cftime must be None or a boolean value.\"\n    ):\n        decode_cf_datetime(0, units, use_cftime=\"cftime\")\n", "def test_decode_cf_datetime_over_2262(units) -> None:\n    # GH 14068\n    num_dates = np.arange(95795, 95798)\n    expected = pd.date_range(\"2262-04-11\", periods=3, freq=\"D\")\n    actual = decode_cf_datetime(num_dates, units)\n    assert_array_equal(actual, expected)\n", "def test_cftime_to_nptime_datetime_types() -> None:\n    import cftime\n\n    datetime_types = _all_cftime_date_types()\n    for calendar_name, date_type in datetime_types.items():\n        cftime_date = date_type(2000, 1, 1)\n        expected = pd.Timestamp(2000, 1, 1).to_numpy()\n        actual = coding.times.cftime_to_nptime(cftime_date)\n\n        assert_array_equal(actual, expected)\n", "def test_decode_cf_datetime_with_non_integer_time_variable() -> None:\n    dates = pd.date_range(\"2000\", periods=4)\n    var = Variable([\"time\"], dates, {\"units\": \"days since 2000-01-01 00:00:00\"})\n    var.encoding = {\"dtype\": \"int64\"}\n    expected = Variable([\"time\"], dates, {\"units\": \"days since 2000-01-01 00:00:00\"})\n\n    result = coding.times.CFDatetimeCoder().decode(var)\n    assert_equal(result, expected)\n"], "sample_819": ["def test_voting_regressor_transform():\n    \"\"\"Check transform method of VotingRegressor on toy dataset.\"\"\"\n    reg1 = DummyRegressor(strategy='mean')\n    reg2 = DummyRegressor(strategy='median')\n    reg3 = DummyRegressor(strategy='quantile', quantile=.2)\n    X = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]])\n    y = np.array([2, 6, 12, 20, 30, 42])\n\n    ereg1 = VotingRegressor([('mean', reg1), ('median', reg2),\n                             ('quantile', reg3)]).fit(X, y)\n    ereg2 = VotingRegressor([('mean', reg1), ('median', reg2),\n                             ('quantile', reg3)], weights=[1, 2, 3]).fit(X, y)\n\n    assert_array_equal(ereg1.transform(X).shape, (6, 3))\n    assert_array_equal(ereg2.transform(X).shape, (6, 3))\n\n    reg1_pred = reg1.fit(X, y).predict(X)\n    reg2_pred = reg2.fit(X, y).predict(X)\n    reg3_pred = reg3.fit(X, y).predict(X)\n\n    assert_array_almost_equal(ereg1.transform(X),\n                              np.array([reg1_pred, reg2_pred, reg3_pred]).T)\n    assert_array_almost_equal(ereg2.transform(X),\n                              np.array([reg1_pred, reg2_pred, reg3_pred]).T)\n", "def test_voting_regressor_weights():\n    \"\"\"Check weighted average regression prediction on boston dataset.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(n_estimators=10, random_state=1)\n    reg3 = LinearRegression()\n    ereg = VotingRegressor([('lr1', reg1), ('rf', reg2), ('lr2', reg3)],\n                           weights=[0.4, 0.3, 0.3])\n\n    X_r_train, X_r_test, y_r_train, y_r_test = \\\n        train_test_split(X_r, y_r, test_size=.25)\n\n    reg1_pred = reg1.fit(X_r_train, y_r_train).predict(X_r_test)\n    reg2_pred = reg2.fit(X_r_train, y_r_train).predict(X_r_test)\n    reg3_pred = reg3.fit(X_r_train, y_r_train).predict(X_r_test)\n    ereg_pred = ereg.fit(X_r_train, y_r_train).predict(X_r_test)\n\n    avg = np.average(np.asarray([reg1_pred, reg2_pred, reg3_pred]), axis=0,\n                     weights=[0.4, 0.3, 0.3])\n    assert_almost_equal(ereg_pred, avg, decimal=2)\n", "def test_voting_classifier_empty_weights():\n    \"\"\"Check VotingClassifier with empty weights\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)],\n                            voting='soft',\n                            weights=[])\n    eclf.fit(X, y)\n    assert eclf.weights is None\n    assert_array_equal(eclf.predict(X), eclf.estimators_[0].predict(X))\n    assert_array_almost_equal(eclf.predict_proba(X),\n                              eclf.estimators_[0].predict_proba(X))\n\n", "def test_voting_regressor_set_params():\n    \"\"\"Check that VotingRegressor set_params updates estimators correctly.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(n_estimators=10, random_state=1)\n    ereg1 = VotingRegressor([('lr', reg1), ('rf', reg2)]).fit(X_r, y_r)\n    ereg2 = VotingRegressor([('lr', reg1), ('rf', reg2)]).set_params(\n        rf__n_estimators=5).fit(X_r, y_r)\n\n    assert ereg1.estimators_[1].n_estimators == 10\n    assert ereg2.estimators_[1].n_estimators == 5\n    assert ereg1.estimators_[0].coef_ is ereg2.estimators_[0].coef_\n", "def test_voting_classifier_with_custom_transformer():\n    \"\"\"Test VotingClassifier with a custom transformer.\"\"\"\n    class CustomTransformer(BaseEstimator, TransformerMixin):\n            return self\n\n            return X * 2\n\n    clf1 = LogisticRegression(random_state=123)\n    transformer = CustomTransformer()\n    eclf = VotingClassifier(estimators=[\n        ('lr', clf1), ('trans', transformer)],\n        voting='soft')\n\n    eclf.fit(X, y)\n    assert_array_almost_equal(eclf.transform(X), np.array([clf1.predict_proba(X * 2),\n                                                           [[1., 0.],\n                                                            [0., 1.],\n                                                            [1., 0.],\n                                                            [0., 1.],\n                                                            [1., 0.],\n                                                            [0., 1.]]]))\n"], "sample_446": ["    def test_safeseq(self):\n        value = [\"a\", \"b\", \"<c>\", \"d\"]\n        result = self.engine.render_to_string(\"safeseq\", {\"value\": value})\n        self.assertEqual(result, \"a b &lt;c&gt; d\")\n        result = self.engine.render_to_string(\"safeseq\", {\"value\": mark_safe(value)})\n        self.assertEqual(result, \"a b <c> d\")\n", "    def test_stringfilter01(self):\n        output = self.engine.render_to_string(\"stringfilter01\", {\"a\": \"HELLO\"})\n        self.assertEqual(output, \"hello\")\n", "    def test_dictsort01(self):\n        values = [{\"name\": \"John\", \"age\": 25}, {\"name\": \"Alice\", \"age\": 30}, {\"name\": \"Bob\", \"age\": 20}]\n        output = self.engine.render_to_string(\"dictsort01\", {\"values\": values})\n        self.assertEqual(output, \"AliceBobJohn\")\n", "def test_make_list(self):\n    self.assertEqual(make_list(\"hello\"), [\"h\", \"e\", \"l\", \"l\", \"o\"])\n    self.assertEqual(make_list(123), [\"1\", \"2\", \"3\"])\n    self.assertEqual(make_list([1, 2, 3]), [1, 2, 3])\n    self.assertEqual(make_list(\"hello\"[1]), [\"e\"])\n    self.assertEqual(make_list(\"\"), [])\n\n    self.assertEqual(make_list(None), [None])\n\n    self.assertEqual(make_list(1.0), [\"1\", \".\", \"0\"])\n    self.assertEqual(make_list(True), [\"T\", \"r\", \"u\", \"e\"])\n    self.assertEqual(make_list(False), [\"F\", \"a\", \"l\", \"s\", \"e\"])\n", "    def test_addslashes(self):\n        self.assertEqual(addslashes(\"Hello, 'world'!\"), \"Hello, \\\\'world\\'!\")\n"], "sample_350": ["def test_get_with_filters_and_ordering_unsupported_on_combined_qs(self):\n    qs = Number.objects.all()\n    msg = 'Calling QuerySet.get(...) with filters after %s() is not supported.'\n    combinators = ['union']\n    if connection.features.supports_select_difference:\n        combinators.append('difference')\n    if connection.features.supports_select_intersection:\n        combinators.append('intersection')\n    for combinator in combinators:\n        with self.subTest(combinator=combinator):\n            with self.assertRaisesMessage(NotSupportedError, msg % combinator):\n                getattr(qs, combinator)(qs).order_by('num').get(num=2)\n", "def test_union_with_values_and_select_for_update(self):\n    ReservedName.objects.create(name='a', order=2)\n    qs1 = ReservedName.objects.select_for_update().values('name', 'order', 'id')\n    qs2 = ReservedName.objects.values('name', 'order', 'id')\n    reserved_name = qs1.union(qs2).get()\n    self.assertEqual(reserved_name['name'], 'a')\n    self.assertEqual(reserved_name['order'], 2)\n    reserved_name = qs1.union(qs2).values_list('name', 'order', 'id').get()\n    self.assertEqual(reserved_name[:2], ('a', 2))\n    # List of columns can be changed.\n    reserved_name = qs1.union(qs2).values_list('order').get()\n    self.assertEqual(reserved_name, (2,))\n", "def test_union_with_empty_values_queryset(self):\n    qs1 = Number.objects.all()\n    qs2 = Number.objects.none().values('num')\n    qs3 = Number.objects.none().values_list('num')\n    self.assertEqual(qs1.union(qs2).count(), 10)\n    self.assertEqual(qs1.union(qs3).count(), 10)\n    self.assertEqual(qs2.union(qs1).count(), 10)\n    self.assertEqual(qs3.union(qs1).count(), 10)\n    self.assertEqual(qs2.union(qs3).count(), 0)\n", "def test_union_with_values_list_and_order_by_fk(self):\n    ReservedName.objects.create(name='rn1', order=0)\n    Celebrity.objects.create(name='Celeb1', reserved_name=ReservedName.objects.get(order=0))\n    qs1 = Celebrity.objects.filter(name='Celeb1').values_list('reserved_name__order', flat=True)\n    qs2 = ReservedName.objects.filter(order=0).values_list('order', flat=True)\n    self.assertSequenceEqual(\n        qs1.union(qs2).order_by('reserved_name__order', 'order'),\n        [0, 0]\n    )\n", "def test_union_with_order_by_on_heterogeneous_models_with_defer(self):\n    # The following code tests union on two different models with order_by and defer\n    Celebrity.objects.create(name='John Doe', age=30, occupation='Actor')\n    ReservedName.objects.create(name='Jane Doe', order=1)\n    qs1 = Celebrity.objects.defer('age').order_by('name')\n    qs2 = ReservedName.objects.defer('order').order_by('name')\n    self.assertSequenceEqual(\n        qs1.union(qs2).order_by('name').values_list('name', flat=True),\n        ['Jane Doe', 'John Doe'],\n    )\n"], "sample_845": ["def test_vectorizer_set_stop_words():\n    vect = CountVectorizer()\n    vect.set_params(stop_words='english')\n    assert vect.get_stop_words() == ENGLISH_STOP_WORDS\n    stoplist = ['some', 'other', 'words']\n    vect.set_params(stop_words=stoplist)\n    assert vect.get_stop_words() == set(stoplist)\n", "def test_vectorizer_invalid_input_type(Estimator):\n    message = \"Invalid value for 'input': 'invalid'. Supported 'input' types \"\n    message += \"are 'filename', 'file', or 'content'.\"\n    assert_raise_message(\n        ValueError, message, Estimator, input='invalid')\n", "def test_custom_stop_words_consistency():\n    stop_words = ['will', 'be', 'tokenized', 'differently']\n    tokenize = CountVectorizer().build_tokenizer()\n    preprocess = CountVectorizer(lowercase=True).build_preprocessor()\n\n    inconsistent = set()\n    for w in stop_words or ():\n        tokens = list(tokenize(preprocess(w)))\n        for token in tokens:\n            if token not in stop_words:\n                inconsistent.add(token)\n\n    assert inconsistent == {'be', 'will'}\n", "def test_vectorizer_strip_accents():\n    vect = CountVectorizer()\n    doc = \"J'ai mang\u00e9 du kangourou  ce midi, c'\u00e9tait pas tr\u00e8s bon.\"\n    X1 = vect.fit_transform([doc]).toarray()\n    expected = ['ai', 'mange', 'du', 'kangourou', 'ce', 'midi',\n                'etait', 'pas', 'tres', 'bon']\n    assert_array_equal(vect.get_feature_names(), expected)\n\n    vect.set_params(strip_accents='unicode')\n    X2 = vect.fit_transform([doc]).toarray()\n    assert_array_equal(vect.get_feature_names(), expected)\n\n    vect.set_params(strip_accents='ascii')\n    X3 = vect.fit_transform([doc]).toarray()\n    assert_array_equal(vect.get_feature_names(), expected)\n\n    # check that the results are the same\n    assert_array_almost_equal(X1, X2)\n    assert_array_almost_equal(X1, X3)\n", "def test_vectorizer_attribute_assignment(Vectorizer):\n    # test the attribute assignment behavior\n    vect = Vectorizer()\n    vect.vocabulary_ = {'a': 0}\n    vect.fixed_vocabulary_ = True\n    vect.stop_words_ = set(['stop'])\n\n    vect_copy = clone(vect)\n\n    assert vect_copy.vocabulary_ == vect.vocabulary_\n    assert vect_copy.fixed_vocabulary_ == vect.fixed_vocabulary_\n    assert vect_copy.stop_words_ == vect.stop_words_\n"], "sample_484": ["    def setUpTestData(cls):\n        Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        Author.objects.create(name=\"Rhonda\")\n", "    def setUpTestData(cls):\n        Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        Author.objects.create(name=\"Rhonda\")\n", "    def setUpTestData(cls):\n        Author.objects.create(name=\"John Smith\")\n        Author.objects.create(name=\"Rhonda\")\n", "    def setUpTestData(cls):\n        Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        Author.objects.create(name=\"Rhonda\")\n", "    def setUpTestData(cls):\n        Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        Author.objects.create(name=\"Rhonda\", alias=\"rhonda\")\n"], "sample_81": ["    def test_match(self):\n        pattern = URLPattern(RegexPattern(r'^test/$'), lambda x: x)\n        match = pattern.resolve('/test/')\n        self.assertIsInstance(match, ResolverMatch)\n        self.assertEqual(match.func, pattern.callback)\n        self.assertEqual(match.args, ())\n        self.assertEqual(match.kwargs, {})\n", "    def test_reverse_dict(self):\n        resolver = get_resolver()\n        self.assertIsInstance(resolver.reverse_dict, MultiValueDict)\n", "    def test_resolve(self):\n        pattern = RegexPattern('^test/$')\n        url_pattern = URLPattern(pattern, lambda x: x)\n        match = url_pattern.resolve('/test/')\n        self.assertIsInstance(match, ResolverMatch)\n        self.assertEqual(match.func, url_pattern.callback)\n        self.assertEqual(match.args, ())\n        self.assertEqual(match.kwargs, {})\n        self.assertEqual(match.url_name, None)\n        self.assertEqual(match.app_names, [])\n        self.assertEqual(match.namespaces, [])\n        self.assertEqual(match.route, '^test/$')\n", "    def test_resolve(self):\n        pattern = RegexPattern(r'^test/$', name='test')\n        url_pattern = URLPattern(pattern, lambda request: 'Test view')\n        match = url_pattern.resolve('/test/')\n        self.assertEqual(match.func.__name__, '<lambda>')\n        self.assertEqual(match.url_name, 'test')\n", "    def test_str(self):\n        regex = RegexPattern('^test/$')\n        url_pattern = URLPattern(regex, lambda x: None)\n        self.assertEqual(str(url_pattern), '<URLPattern ^test/$>')\n"], "sample_418": ["    def test_date_format(self):\n        date_obj = datetime.date(2022, 1, 1)\n        output = self.engine.render_to_string(\"date01\", {\"date\": date_obj})\n        self.assertEqual(output, \"2022-01-01\")\n", "    def test_stringformat01(self):\n        output = self.engine.render_to_string(\"stringformat01\", {\"var\": 5.12})\n        self.assertEqual(output, \"05.12\")\n", "    def test_addslashes(self):\n        output01 = self.engine.render_to_string(\"addslashes01\")\n        self.assertEqual(output01, \"Hello World\")\n\n        output02 = self.engine.render_to_string(\"addslashes02\")\n        self.assertEqual(output02, 'Hello \\\\\"World\\\"')\n\n        output03 = self.engine.render_to_string(\"addslashes03\")\n        self.assertEqual(output03, 'Hello \\\\\\'World\\\\\\\\')\n", "    def test_timesince(self):\n        from datetime import datetime, timedelta\n\n        now = datetime.now()\n        delta = timedelta(days=1)\n        yesterday = now - delta\n\n        self.assertEqual(timesince(yesterday, now), \"1 day, 0 minutes\")\n", "    def test_addslashes(self):\n        output = self.engine.render_to_string(\"addslashes01\")\n        self.assertEqual(output, 'test\\\\\\'ing')\n\n        output = self.engine.render_to_string(\"addslashes02\")\n        self.assertEqual(output, 'test\\\\\\\\\\\\\"ing')\n"], "sample_748": ["def test_grid_search_with_sparse_param_grid():\n    X, y = make_classification(n_samples=20, n_features=10, random_state=0)\n    param_grid = [{'C': np.logspace(-10, 1, 5)}]\n    gs = GridSearchCV(SVC(gamma='scale'), param_grid=param_grid, cv=2)\n    gs.fit(X, y)\n", "def test_grid_search_refit_estimator_clone():\n    X, y = make_classification(random_state=0)\n    grid = GridSearchCV(SVC(gamma='scale'), param_grid={'C': [1]}, cv=3)\n    grid.fit(X, y)\n\n    # Check if the best_estimator_ is a clone of the estimator\n    assert_true(hasattr(grid, \"best_estimator_\"))\n    assert_false(grid.best_estimator_ is grid.estimator)\n    assert_true(isinstance(grid.best_estimator_, SVC))\n", "def test_randomized_search_cv_results_grid_points_less_than_n_iter():\n    # Test RandomizedSearchCV with n_iter > grid_size\n    X, y = make_classification(n_samples=50, n_features=4, random_state=42)\n\n    n_splits = 3\n    n_search_iter = 10\n    param_grid = {'C': np.logspace(-10, 1), 'gamma': np.logspace(-5, 0, base=0.1)}\n    param_keys = ('param_C', 'param_gamma')\n    score_keys = ('mean_test_score', 'mean_train_score',\n                  'rank_test_score',\n                  'split0_test_score', 'split1_test_score',\n                  'split2_test_score',\n                  'split0_train_score', 'split1_train_score',\n                  'split2_train_score',\n                  'std_test_score', 'std_train_score',\n                  'mean_fit_time', 'std_fit_time',\n                  'mean_score_time', 'std_score_time')\n    n_cand = 6  # Number of unique param settings\n\n    search = RandomizedSearchCV(SVC(gamma='scale'), n_iter=n_search_iter,\n                                cv=n_splits, param_distributions=param_grid)\n    search.fit(X, y)\n    cv_results = search.cv_results_\n    # Check results structure\n    check_cv_results_array_types(search, param_keys, score_keys)\n    check_cv_results_keys(cv_results, param_keys, score_keys, n_cand)\n    # For random_search, all the param array vals should be unmasked\n    assert_false(any(cv_results['param_C'].mask) or\n                 any(cv_results['param_gamma'].mask))\n    check_cv_results_grid_scores_consistency(search)\n", "def test_grid_search_multimetric():\n    # Test GridSearchCV with multi-metric scoring\n    scoring = ['accuracy', 'precision']\n    X, y = make_classification(n_samples=50, n_features=4, random_state=42)\n    clf = LinearSVC(random_state=42)\n    grid_search = GridSearchCV(clf, {}, cv=3, iid=True, scoring=scoring)\n    grid_search.fit(X, y)\n    assert_true(grid_search.multimetric_)\n    assert_array_equal(sorted(grid_search.scorer_.keys()),\n                       ['accuracy', 'precision'])\n    cv_results = grid_search.cv_results_\n    for score_key in ['mean_test_accuracy', 'mean_test_precision',\n                      'mean_train_accuracy', 'mean_train_precision',\n                      'rank_test_accuracy', 'rank_test_precision',\n                      'split0_test_accuracy', 'split1_test_accuracy',\n                      'split2_test_accuracy', 'split0_test_precision',\n                      'split1_test_precision', 'split2_test_precision',\n                      'split0_train_accuracy', 'split1_train_accuracy',\n                      'split2_train_accuracy', 'split0_train_precision',\n                      'split1_train_precision', 'split2_train_precision']:\n        assert_true(score_key in cv_results.keys())\n        if score_key.startswith('rank'):\n            assert_true(cv_results[score_key].dtype == np.int32)\n        else:\n            assert_true(cv_results[score_key].dtype == np.float64)\n", "def test_grid_search_with_no_candidates():\n    # Test if GridSearchCV correctly handles the case when there are no\n    # candidates to try.\n\n    # This can happen if the parameter grid is empty or all candidates\n    # have been filtered out.\n\n    param_grid = {}\n    grid_search = GridSearchCV(MockClassifier(), param_grid, cv=2)\n    assert_no_warnings(grid_search.fit, X, y)\n\n    param_grid = [{'foo_param': [1]}, {}]\n    grid_search = GridSearchCV(MockClassifier(), param_grid, cv=2)\n    assert_no_warnings(grid_search.fit, X, y)\n\n    param_grid = [{'foo_param': [1]}, {'foo_param': [1]}]\n    grid_search = GridSearchCV(MockClassifier(), param_grid, cv=2)\n    assert_no_warnings(grid_search.fit, X, y)\n"], "sample_753": ["def test_logistic_regression_empty_classes():\n    # Test that an exception is raised when one of the classes is empty\n    X, y = make_classification(n_samples=100, n_features=20, random_state=0,\n                               n_informative=10, n_classes=3)\n\n    # Make one of the classes empty\n    y[y == 2] = 1\n\n    # Test for LogisticRegression\n    lr = LogisticRegression(solver='lbfgs')\n    msg = 'One of the classes is empty'\n    assert_raise_message(ValueError, msg, lr.fit, X, y)\n\n    # Test for LogisticRegressionCV\n    lr = LogisticRegressionCV(solver='lbfgs')\n    msg = 'One of the classes is empty'\n    assert_raise_message(ValueError, msg, lr.fit, X, y)\n", "def test_penalty():\n    # Test that the penalty is applied correctly\n\n    # Create a dataset with a strong feature and a weak feature\n    rng = np.random.RandomState(0)\n    X = np.concatenate([rng.randn(100, 2) + [1, 1], rng.randn(100, 2)])\n    y = np.array([1] * 100 + [-1] * 100)\n\n    # Fit a model with L2 penalty\n    clf_l2 = LogisticRegression(C=1, penalty='l2', solver='liblinear')\n    clf_l2.fit(X, y)\n    coef_l2 = clf_l2.coef_[0]\n\n    # Fit a model with L1 penalty\n    clf_l1 = LogisticRegression(C=1, penalty='l1', solver='liblinear')\n    clf_l1.fit(X, y)\n    coef_l1 = clf_l1.coef_[0]\n\n    # The L1 penalty should result in a sparse model\n    assert np.count_nonzero(coef_l1) < np.count_nonzero(coef_l2)\n", "def test_logistic_regressioncv_newton_cg_multinomial():\n    # Test for LogisticRegressionCV with solver newton-cg and multiclass option.\n    X, y = make_classification(n_samples=100, random_state=0, n_classes=3,\n                               n_informative=6)\n    train, test = np.arange(80), np.arange(80, 100)\n    lr = LogisticRegressionCV(solver='newton-cg', Cs=[1.], cv=3,\n                              multi_class='multinomial', tol=1e-7,\n                              max_iter=100)\n    # we use lbfgs to support multinomial\n    params = lr.get_params()\n    # we store the params to set them further in _log_reg_scoring_path\n    for key in ['C', 'n_jobs', 'warm_start']:\n        del params[key]\n    lr.fit(X, y)\n    for averaging in ['', '_macro', '_weighted']:\n        scorer = get_scorer('f1' + averaging)\n        assert_array_almost_equal(\n            _log_reg_scoring_path(X, y, train, test, Cs=[1.],\n                                  scoring=scorer, **params)[2][0],\n            scorer(lr, X[test], y[test]))\n", "def test_multiclass_shape():\n    # Test that the shape of the various attributes are correct\n    # when multi_class is set to multinomial.\n\n    # First check the case where fit_intercept is False.\n    X, y = make_classification(n_samples=100, n_features=5,\n                               n_informative=5, n_classes=3, random_state=0)\n\n    lr = LogisticRegression(solver='lbfgs', multi_class='multinomial',\n                            fit_intercept=False)\n    lr.fit(X, y)\n    assert_equal(lr.coef_.shape, (3, 5))\n    assert_array_equal(lr.classes_, [0, 1, 2])\n    assert_equal(lr.intercept_.shape, (3,))\n\n    # Next check the case where fit_intercept is True.\n    lr = LogisticRegression(solver='lbfgs', multi_class='multinomial',\n                            fit_intercept=True)\n    lr.fit(X, y)\n    assert_equal(lr.coef_.shape, (3, 5))\n    assert_array_equal(lr.classes_, [0, 1, 2])\n    assert_equal(lr.intercept_.shape, (3,))\n\n    # Finally check the case for LogisticRegressionCV.\n    lr = LogisticRegressionCV(solver='lbfgs', multi_class='multinomial', Cs=1)\n    lr.fit(X, y)\n    assert_equal(lr.coef_.shape, (3, 5))\n    assert_array_equal(lr.classes_, [0, 1, 2])\n    assert_equal(lr.intercept_.shape, (3,))\n    assert_equal(lr.C_.shape, (3,))\n    coefs_paths = np.asarray(list(lr.coefs_paths_.values()))\n    assert_equal(coefs_paths.shape, (3, 3, 1, 6))\n    assert_equal(lr.Cs_.shape, (1,))\n    scores = np.asarray(list(lr.scores_.values()))\n    assert_equal(scores.shape, (3, 3, 1))\n", "def test_multinomial_binary_edge_cases(solver):\n    # Test that multinomial logistic regression behaves well on some\n    # edge cases in binary classification problems\n    n_samples = 10\n    X = np.array([[1], [1]])  # duplicate features\n    y = np.array([0, 1])\n\n    clf = LogisticRegression(solver=solver, multi_class='multinomial',\n                             random_state=42)\n    clf.fit(X, y)\n    assert_equal(clf.coef_.shape, (1, X.shape[1]))\n    assert_array_equal(clf.predict(X), y)\n\n    X = np.array([[1, 1]])  # single sample\n    y = np.array([0])\n\n    clf = LogisticRegression(solver=solver, multi_class='multinomial',\n                             random_state=42)\n    clf.fit(X, y)\n    assert_equal(clf.coef_.shape, (1, X.shape[1]))\n    assert_array_equal(clf.predict(X), y)\n\n    X = np.array([[1, 1, 1]])  # single feature\n    y = np.array([0])\n\n    clf = LogisticRegression(solver=solver, multi_class='multinomial',\n                             random_state=42)\n    clf.fit(X, y)\n    assert_equal(clf.coef_.shape, (1, X.shape[1]))\n    assert_array_equal(clf.predict(X), y)\n"], "sample_1207": ["def test_recursive_parentheses():\n    transformations = standard_transformations + (implicit_multiplication,)\n    assert parse_expr(\"(2*(x+1))\", transformations=transformations) == 2*(x+1)\n    assert parse_expr(\"((2*(x+1))\", transformations=transformations) == 2*(x+1)\n    assert parse_expr(\"((2*(x+1)))\", transformations=transformations) == 2*(x+1)\n    assert parse_expr(\"2*((x+1))\", transformations=transformations) == 2*(x+1)\n    assert parse_expr(\"2*(((x+1)))\", transformations=transformations) == 2*(x+1)\n", "def test_issue_undefined_variables():\n    assert parse_expr('x + y', evaluate=False).free_symbols == {Symbol('x'), Symbol('y')}\n\n    # Local dict\n    local_dict = {'x': Symbol('x')}\n\n    assert parse_expr('x + y', local_dict=local_dict, evaluate=False).free_symbols == {Symbol('y')}\n\n    # Global dict\n    global_dict = {'y': Symbol('y')}\n\n    assert parse_expr('x + y', global_dict=global_dict, evaluate=False).free_symbols == {Symbol('x')}\n\n    # Both local and global dict\n    assert parse_expr('x + y', local_dict=local_dict, global_dict=global_dict, evaluate=False).free_symbols == set()\n", "def test_repeated_decimals_with_unicode():\n    inputs = {\n        '.1[\u2081\u2081\u2081]': Rational(11, 99),\n        '1.\u2082\u2083[\u2083\u2083\u2083]': Rational(1003, 990),\n    }\n    for text, result in inputs.items():\n        assert parse_expr(text) == result\n", "def test_transformations_T():\n    transformations = standard_transformations + (implicit_multiplication,)\n    x = Symbol('x')\n    y = Symbol('y')\n    assert parse_expr(\"xy\", transformations=transformations) == x*y\n    assert parse_expr(\"x y\", transformations=T[:8]) == x*y\n    assert parse_expr(\"x y\", transformations=T[:7]) == x * y\n    assert parse_expr(\"x y\", transformations=T[:6]) == x * y\n    assert parse_expr(\"x y\", transformations=T[:5]) == x  # y\n    assert parse_expr(\"2x\", transformations=T[:5]) == 2*x\n    assert parse_expr(\"2x\", transformations=T[:6]) == 2*x\n    assert parse_expr(\"2x\", transformations=T[:7]) == 2*x\n    assert parse_expr(\"2x\", transformations=T[:8]) == 2*x\n", "def test_evaluate_false_and_auto_symbol():\n    x = Symbol('x')\n    inputs = {\n        \"3x\": 3*x,\n        \"x+1\": x+1,\n        \"x + 1\": x + 1,\n        \"x-1\": x-1,\n        \"-1\": -1,\n        \"3\": 3\n    }\n    transformations = standard_transformations + (implicit_multiplication,)\n    for text, result in inputs.items():\n        assert parse_expr(text, transformations=transformations, evaluate=False) == result\n"], "sample_761": ["def test_iterative_imputer_fit_twice():\n    rng = np.random.RandomState(0)\n    n = 100\n    d = 10\n    X = sparse_random_matrix(n, d, density=0.10, random_state=rng).toarray()\n\n    imputer = IterativeImputer(missing_values=0,\n                               max_iter=1,\n                               random_state=rng)\n    imputer.fit(X)\n\n    # fit again\n    imputer.fit(X)\n\n    # transform\n    X_trans = imputer.transform(X)\n    assert_allclose(X_trans, imputer.initial_imputer_.transform(X))\n", "def test_iterative_imputer_initial_imputation():\n    # Test that initial imputation is correct for different initial strategies\n    rng = np.random.RandomState(0)\n\n    n = 100\n    d = 10\n    X = sparse_random_matrix(n, d, density=0.10, random_state=rng).toarray()\n    missing_flag = X == 0\n    X[missing_flag] = np.nan\n\n    for strategy in [\"mean\", \"median\", \"most_frequent\", \"constant\"]:\n        imputer = IterativeImputer(missing_values=0,\n                                   max_iter=0,\n                                   initial_strategy=strategy,\n                                   random_state=rng)\n        imputer.fit(X)\n        assert_array_equal(imputer.initial_imputer_.transform(X),\n                          imputer.transform(X))\n", "def test_iterative_imputer_transform_add_indicator():\n    rng = np.random.RandomState(0)\n    X = rng.randn(100, 10)\n    X[0, 0] = np.nan\n\n    imputer = IterativeImputer(missing_values=np.nan).fit(X)\n    imputer_add_indicator = IterativeImputer(missing_values=np.nan,\n                                             add_indicator=True).fit(X)\n\n    assert_allclose(imputer.transform(X)[:, :-1], imputer_add_indicator.transform(X)[:, :-1])\n    assert_array_equal(imputer_add_indicator.transform(X)[:, -1], _get_mask(X, np.nan)[:, 0])\n", "def test_imputation_constant_fill_value_error(array_constructor, missing_values):\n    # Test imputation using the constant strategy with invalid fill_value\n    X = np.array([\n        [np.nan, 1.1, 0, np.nan],\n        [1.2, np.nan, 1.3, np.nan],\n        [0, 0, np.nan, np.nan],\n        [1.4, 1.5, 0, np.nan]\n    ])\n\n    X = array_constructor(X)\n\n    with pytest.raises(ValueError,\n                      match=\"'fill_value'={} is invalid. Expected a \"\n                            \"numerical value when imputing numerical \"\n                            \"data\".format(\"x\")):\n        imputer = SimpleImputer(strategy=\"constant\",\n                               fill_value=\"x\",\n                               missing_values=missing_values)\n        imputer.fit_transform(X)\n", "def test_simple_imputer_empty_columns(missing_values, imputation_strategy, expected_statistics):\n    # Test that empty columns are handled correctly when fitting SimpleImputer\n    X = np.array([\n        [1, 2, np.nan],\n        [2, np.nan, 3],\n        [np.nan, 4, np.nan],\n        [np.nan, 5, 5]\n    ])\n    \n    imputer = SimpleImputer(missing_values=missing_values, strategy=imputation_strategy)\n    imputer.fit(X)\n    \n    assert_array_almost_equal(imputer.statistics_, expected_statistics)\n"], "sample_675": ["def test_log_capture_handler_reset(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logger = logging.getLogger(__name__)\n\n            logger.info('message 1')\n            assert len(caplog.records) == 1\n            caplog.clear()\n            assert len(caplog.records) == 0\n\n            logger.info('message 2')\n            assert len(caplog.records) == 1\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=INFO\")\n    result.assert_outcomes(passed=1)\n", "def test_log_records_attribute(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('text going to logger from call')\n            assert len(caplog.records) == 1\n            assert caplog.records[0].name == __name__\n            assert caplog.records[0].levelno == logging.INFO\n            assert caplog.records[0].getMessage() == 'text going to logger from call'\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=INFO\")\n    assert result.ret == 0\n", "def test_log_auto_indent(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logger = logging.getLogger(__name__)\n            logger.info(\"This is a short message\")\n            logger.info(\"This is a very long message that spans multiple lines\\\\n\"\n                        \"and has a lot of content\")\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=INFO\", \"--log-auto-indent=true\")\n    assert result.ret == 0\n    result.stdout.fnmatch_lines(\n        [\n            \"*This is a short message*\",\n            \"*This is a very long message that spans multiple lines*\",\n            \"*and has a lot of content*\",\n        ]\n    )\n\n", "def test_log_auto_indent(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('This is a\\\\n'\n                        'multi-line\\\\n'\n                        'message.')\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=INFO\", \"--log-auto-indent=true\")\n    assert result.ret == 0\n    result.stdout.fnmatch_lines(\n        [\"*- Captured log call -*\", \"*INFO    test_log_auto_indent.py:*This is a*\",\n         \"*                  multi-line*\", \"*                  message.*\"]\n    )\n\n", "def test_log_capture_fixture_set_level(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            with caplog.at_level(logging.INFO):\n                logger.info(\"This log message should be captured\")\n                logger.debug(\"This log message shouldn't be captured\")\n\n            logger.info(\"This log message should be captured\")\n            logger.warning(\"This log message should be captured\")\n\n            caplog.set_level(logging.DEBUG)\n            logger.info(\"This log message should be captured\")\n            logger.debug(\"This log message should be captured\")\n            logger.warning(\"This log message should be captured\")\n\n            caplog.set_level(logging.INFO)\n            logger.info(\"This log message should be captured\")\n            logger.debug(\"This log message shouldn't be captured\")\n            logger.warning(\"This log message should be captured\")\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=WARNING\")\n    result.assert_outcomes(passed=2)\n    result.stdout.fnmatch_lines(\n        [\n            \"*- Captured log call -*\",\n            \"*This log message should be captured*\",\n            \"*This log message shouldn't be captured\",\n        ]\n    )\n    result.stdout.fnmatch_lines(\n        [\n            \"*- Captured log call -*\",\n            \"*This log message should be captured*\",\n            \"*This log message should be captured*\",\n            \"*This log message should be captured*\",\n        ]\n    )\n"], "sample_701": ["def test_argument_type_str_choice_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            parser.addoption(\"--foo\", type='str', choices=['bar', 'baz'])\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: `type` argument to addoption() is the string 'str'. \"\n            \"For choices this is optional and can be omitted, but when supplied should be a type \"\n            \"(for example `str` or `int`). (options: ['bar', 'baz'])\",\n        ]\n    )\n\n", "def test_argument_type_str_choice_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            group = parser.getgroup(\"custom options\")\n            group.addoption(\"--foo\", action=\"store\", type=\"int\", choices=[\"bar\", \"baz\"])\n        \"\"\"\n    )\n    with pytest.warns(\n        PytestDeprecationWarning,\n        match=re.escape(\n            \"`type` argument to addoption() is the string 'int'. \"\n            \" For choices this is optional and can be omitted, \"\n            \" but when supplied should be a type (for example `str` or `int`).\"\n            \" (options: ['bar', 'baz'])\"\n        ),\n    ):\n        pytester.runpytest(\"-p\", \"no:pytestconfig\")\n", "def test_argument_type_str_choice_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n            parser.addoption(\"--foo\", type='str', choices=['a', 'b'])\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: `type` argument to addoption() is the string 'str'. \"\n            \"For choices this is optional and can be omitted, but when supplied should be a type \"\n            \"(for example `str` or `int`). (options: ['a', 'b'])\",\n        ]\n    )\n\n", "def test_argument_percent_default_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n            group = parser.getgroup(\"general\")\n            group.addoption(\n                \"--foo\",\n                dest=\"foo\",\n                type=int,\n                default=1,\n                help=\"the foo option (%default)\",\n            )\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-h\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: pytest now uses argparse.*\",\n        ]\n    )\n\n", "def test_argument_percent_default_is_deprecated() -> None:\n    with pytest.warns(\n        PytestDeprecationWarning,\n        match=re.escape(\n            'pytest now uses argparse. \"%default\" should be changed to \"%(default)s\"'\n        ),\n    ):\n        pytest.addoption(\"--foo\", type=str, default=\"bar\", help=\"%default\")\n\n"], "sample_1061": ["def test_pow_as_real_imag():\n    assert Pow(2, 0).as_real_imag() == (1, 0)\n    assert Pow(-2, 0).as_real_imag() == (1, 0)\n    assert Pow(0, 1).as_real_imag() == (0, 0)\n    assert Pow(1, 1).as_real_imag() == (1, 0)\n    assert Pow(-1, 1).as_real_imag() == (-1, 0)\n    assert Pow(0, -1).as_real_imag() == (zoo, 0)\n    assert Pow(1, -1).as_real_imag() == (1, 0)\n    assert Pow(-1, -1).as_real_imag() == (-1, 0)\n    assert Pow(2, 2).as_real_imag() == (4, 0)\n    assert Pow(-2, 2).as_real_imag() == (4, 0)\n    assert Pow(2, -2).as_real_imag() == (S(1)/4, 0)\n    assert Pow(-2, -2).as_real_imag() == (S(1)/4, 0)\n    assert Pow(0, 2).as_real_imag() == (0, 0)\n    assert Pow(1, 2).as_real_imag() == (1, 0)\n    assert Pow(-1, 2).as_real_imag() == (1, 0)\n    assert Pow(0, -2).as_real_imag() == (zoo, 0)\n    assert Pow(1, -2).as_real_imag() == (1, 0)\n    assert Pow(-1, -2).as_real_imag() == (1, 0)\n    assert Pow(S(1)/2, 2).as_real_imag() == (S(1)/4, 0)\n    assert Pow(-S(1)/2, 2).as_real_imag() == (S(1)/4, 0)\n    assert Pow(S(1)/2, -2).as_real_imag() == (4, 0)\n    assert Pow(-S(1)/2, -2).as_real_imag() == (4, 0)\n    assert Pow(2, S(1)/2).as_real_imag() == (sqrt(2), 0)\n    assert Pow(-2", "def test_infinite_integer_power():\n    x = Symbol('x', positive=True)\n    y = Symbol('y', real=True)\n    assert S.One**oo == S.One\n    assert S.One**(-oo) == S.One\n    assert (S.Zero**oo).is_zerofunc\n    assert (S.Zero**(-oo)).is_polefunc\n    assert (S.NegativeOne**oo).is_real\n    assert (S.NegativeOne**(-oo)).is_real\n    assert (x**oo).is_positive\n    assert (x**(-oo)).is_positive\n    assert (y**oo).is_real\n    assert (y**(-oo)).is_real\n", "def test_Pow_as_real_imag():\n    # test issue 10393\n    from sympy import Symbol\n    b, e = Symbol('b', real=True), Symbol('e', positive=True)\n    p = Pow(b, e)\n    a = p.as_real_imag()\n    assert all(i.free_symbols == {b, e} for i in a)\n    assert a[0].is_real and a[1].is_real\n", "def test_Pow_as_real_imag():\n    # positive integer\n    p = Pow(2, 3)\n    r, i = p.as_real_imag()\n    assert r == p\n    assert i == 0\n    # negative integer\n    p = Pow(-2, 3)\n    r, i = p.as_real_imag()\n    assert r == p\n    assert i == 0\n    # even root of positive number\n    p = Pow(2, S(1)/2)\n    r, i = p.as_real_imag()\n    assert r == p\n    assert i == 0\n    # odd root of positive number\n    p = Pow(2, S(1)/3)\n    r, i = p.as_real_imag()\n    assert r == p\n    assert i == 0\n    # even root of negative number\n    p = Pow(-2, S(1)/2)\n    r, i = p.as_real_imag()\n    assert i == p\n    assert r == 0\n    # odd root of negative number\n    p = Pow(-2, S(1)/3)\n    r, i = p.as_real_imag()\n    assert r == p\n    assert i == 0\n    # rational power of positive number\n    p = Pow(2, S(3)/4)\n    r, i = p.as_real_imag()\n    assert r == p\n    assert i == 0\n    # rational power of negative number\n    p = Pow(-2, S(3)/4)\n    r, i = p.as_real_imag()\n    assert i == p\n    assert r == 0\n    # irrational power of positive number\n    p = Pow(2, sqrt(2))\n    r, i = p.as_real_imag()\n    assert r == p\n    assert i == 0\n    # irrational power of negative number\n    p = Pow(-2, sqrt(2))\n    r, i = p.as_real_imag()\n    assert i == p\n    assert r == 0\n    # real power of complex number\n    p = Pow(1 + I, 2)\n    r, i = p.as_real_imag()\n    assert r == 0\n    assert i == 2\n    # complex power of real number\n    p = Pow(2, 1 + I)\n    r, i = p.as_real_imag()\n    assert r", "def test_integer_nthroot_with_exact_power():\n    assert integer_nthroot(64, 2)[0] == 8\n    assert integer_nthroot(729, 3)[0] == 9\n    assert integer_nthroot(1000000, 6)[0] == 10\n    assert integer_nthroot(59049, 10)[0] == 3\n    assert integer_nthroot(125, 3)[0] == 5\n    assert integer_nthroot(4096, 6)[0] == 4\n    assert integer_nthroot(823543, 10)[0] == 3\n    assert integer_nthroot(1061208, 10)[0] == 4\n    assert integer_nthroot(1000000000, 10)[0] == 10\n    assert integer_nthroot(1000000000000, 12)[0] == 10\n"], "sample_1133": ["def test_critical_angle_edge_cases():\n    assert critical_angle(1, 1) == 0\n    raises(ValueError, lambda: critical_angle(1, 2))  # n2 > n1\n    raises(TypeError, lambda: critical_angle('a', 1))\n    raises(TypeError, lambda: critical_angle(1, 'a'))\n    raises(TypeError, lambda: critical_angle([1, 2], 1))\n    raises(TypeError, lambda: critical_angle(1, [1, 2]))\n", "def test_refraction_angle_edge_cases():\n    r1 = Ray3D(Point3D(-1, -1, 1), Point3D(0, 0, 0))\n    n = Matrix([0, 0, 1])\n    P = Plane(Point3D(0, 0, 0), normal_vector=[0, 0, 1])\n\n    raises(ValueError, lambda: refraction_angle(r1, 1, 1, normal=n, plane=P))\n\n    raises(TypeError, lambda: refraction_angle(r1, 1, 1, normal=\"invalid\"))\n\n    raises(TypeError, lambda: refraction_angle(r1, 1, 1, plane=\"invalid\"))\n\n    raises(ValueError, lambda: refraction_angle(2*pi, 1, 1))\n\n    raises(ValueError, lambda: refraction_angle(r1, 1, 1, normal=n, angle_of_incidence=2*pi))\n\n    raises(TypeError, lambda: refraction_angle(\"invalid\", 1, 1))\n\n    raises(TypeError, lambda: refraction_angle(r1, \"invalid\", 1))\n\n    raises(TypeError, lambda: refraction_angle(r1, 1, \"invalid\"))\n", "def test_refraction_angle_edge_cases():\n    # Test for the case when the normal is parallel to the incident ray\n    n1, n2 = symbols('n1, n2')\n    r1 = Ray3D(Point3D(-1, -1, 1), Point3D(0, 0, 0))\n    n = Matrix([1, 1, 1])\n    P = Plane(Point3D(0, 0, 0), normal_vector=[1, 1, 1])\n    assert refraction_angle(r1, 1, 1, plane=P) == Ray3D(Point3D(0, 0, 0), Point3D(-1, -1, -1))\n\n    # Test for the case when the incident ray is parallel to the normal\n    r1 = Ray3D(Point3D(-1, -1, 1), Point3D(0, 0, 0))\n    n = Matrix([0, 0, 1])\n    P = Plane(Point3D(0, 0, 0), normal_vector=[0, 0, 1])\n    assert refraction_angle(r1, 1, 1, plane=P) == Ray3D(Point3D(0, 0, 0), Point3D(0, 0, -1))\n\n    # Test for the case when the refractive indices are equal\n    r1 = Ray3D(Point3D(-1, -1, 1), Point3D(0, 0, 0))\n    n = Matrix([0, 0, 1])\n    P = Plane(Point3D(0, 0, 0), normal_vector=[0, 0, 1])\n    assert refraction_angle(r1, 1, 1, plane=P) == Ray3D(Point3D(0, 0, 0), Point3D(1, 1, -1))\n", "def test_refraction_angle_edge_cases():\n    n1, n2 = symbols('n1, n2')\n    r1 = Ray3D(Point3D(-1, -1, 1), Point3D(0, 0, 0))\n    i = Matrix([1, 1, 1])\n    n = Matrix([0, 0, 1])\n    P = Plane(Point3D(0, 0, 0), normal_vector=[0, 0, 1])\n    raises(ValueError, lambda: refraction_angle(r1, 1, 1, n, P))\n    raises(ValueError, lambda: refraction_angle(r1, 1, 1, normal=None, plane=None))\n    raises(ValueError, lambda: refraction_angle(r1, 1, 1, plane=None))\n    raises(ValueError, lambda: refraction_angle(r1, 1, 1, normal=None))\n    raises(ValueError, lambda: refraction_angle(r1, 1.33, 1, plane=P))\n    raises(ValueError, lambda: refraction_angle(r1, 1, 1, n, plane=P, normal=n))\n    raises(ValueError, lambda: refraction_angle(r1, 1, 1, plane=P, normal=n))\n    raises(ValueError, lambda: refraction_angle(r1, 1, 1.33, normal=n))\n    raises(ValueError, lambda: refraction_angle(r1, 1, 1.33, plane=P))\n    raises(ValueError, lambda: refraction_angle(1.57, 1, 1, normal=n, plane=P))\n    raises(ValueError, lambda: refraction_angle(1.57, 1.33, 1, normal=n, plane=P))\n    raises(ValueError, lambda: refraction_angle(r1, 1, 1.33, normal=n))\n    raises(ValueError, lambda: refraction_angle(r1, 1.33, 1, plane=P))\n", "def test_refraction_angle_edge_cases():\n    from sympy import symbols, Matrix, oo\n    n1, n2 = symbols('n1, n2')\n    r1 = Matrix([0, 0, 1])\n    n = Matrix([0, 0, 1])\n    raises(ValueError, lambda: refraction_angle(1.5, 1, 1))\n    raises(ValueError, lambda: refraction_angle(r1, n1, n2, plane=None, normal=None))\n    raises(ValueError, lambda: refraction_angle(r1, 1, 1, plane=n))\n    raises(ValueError, lambda: refraction_angle(r1, 1, 1, normal=n, plane=n))\n    raises(TypeError, lambda: refraction_angle(None, n1, n2, normal=n))\n    raises(TypeError, lambda: refraction_angle(r1, n1, n2, normal=None, plane=\"Plane\"))\n    raises(ValueError, lambda: refraction_angle(r1, 1.33, 1, plane=n))\n    assert refraction_angle(r1, 1.33, 1, plane=n) == 0\n    assert refraction_angle(0.5, 1.33, 1.33) == 0.5\n    assert refraction_angle(r1, 1.33, 1.33, normal=n) == Matrix([0, 0, 1])\n    assert refraction_angle(r1, 1, 1, plane=None, normal=n) == Matrix([\n        [0],\n        [0],\n        [-1]])\n"], "sample_252": ["    def test_empty_list(self):\n        field = models.JSONField()\n        self.assertEqual(field.get_prep_value([]), '[]')\n", "    def test_key_transform_with_nested_key_transform(self):\n        nested_key_transform = KeyTransform('f', KeyTransform('1', KeyTransform('d', 'value')))\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__d__1__has_key=nested_key_transform),\n            [self.objs[4]],\n        )\n", "    def test_from_db_value(self):\n        field = models.JSONField()\n        self.assertIsNone(field.from_db_value(None, None, connection))\n        self.assertEqual(field.from_db_value('null', None, connection), None)\n        self.assertEqual(field.from_db_value('{\"a\": \"b\"}', None, connection), {'a': 'b'})\n        with self.assertRaises(json.JSONDecodeError):\n            field.from_db_value('invalid json', None, connection)\n", "def test_key_transform_with_list_target(self):\n    tests = [\n        ('value__0__1__0', 'a'),\n        ('value__1__0', 'a'),\n        ('value__d__0', 'e'),\n        ('value__d__1__f', 'g'),\n        ('value__d__1', {'f': 'g'}),\n        ('value__d', [{'f': 'g'}]),\n    ]\n    for lookup, value in tests:\n        with self.subTest(lookup=lookup):\n            self.assertIs(NullableJSONModel.objects.filter(**{lookup: value}).exists(), True)\n", "    def test_key_regex(self):\n        field = models.JSONField()\n        transform = KeyTransform('key')\n        self.assertIsInstance(transform.get_lookup('regex'), KeyTransformRegex)\n        self.assertIsInstance(transform.get_lookup('iregex'), KeyTransformIRegex)\n"], "sample_357": ["def test_remove_model_with_field_removed_from_base_model(self):\n    \"\"\"\n    Removing a model that has a field removed from its base model\n    should generate the correct operations.\n    \"\"\"\n    before = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ]),\n        ModelState('app', 'book', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ], bases=('app.readable',)),\n    ]\n    after = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'DeleteModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='title', model_name='readable')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='book')\n", "    def test_alter_unique_constraint(self):\n        before = ModelState(\n            \"testapp\", \"model\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"email\", models.EmailField(max_length=254)),\n            ],\n            options={\"unique_together\": {(\"name\", \"email\")}},\n        )\n        after = ModelState(\n            \"testapp\", \"model\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"email\", models.EmailField(max_length=254)),\n            ],\n            options={\"unique_together\": {(\"email\", \"name\")}},\n        )\n        changes = self.get_changes([before], [after])\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterUniqueTogether\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"model\", unique_together={(\"email\", \"name\")})\n", "    def test_non_circular_foreignkey_dependency_removal_different_apps(self):\n        \"\"\"\n        #27049 - If two models with a ForeignKey from one to the other are removed\n        at the same time, the autodetector should remove them in the correct\n        order, even if the models belong to different apps.\n        \"\"\"\n        changes = self.get_changes([self.author_with_publisher, self.publisher_with_author], [])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertNumberMigrations(changes, \"otherapp\", 0)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RemoveField\", \"DeleteModel\", \"DeleteModel\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", model_name='publisher')\n        self.assertOperationAttributes(changes, \"testapp\", 0, 1, name=\"Author\")\n        self.assertOperationAttributes(changes, \"testapp\", 0, 2, name=\"Publisher\")\n", "def test_deconstructible_objects_in_model_fields(self):\n    before_state = ModelState(\n        \"app\",\n        \"model\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"field\", models.Field(deconstruct=lambda x: (\"django.db.models.Field\", [], {\"some\": \"value\"}))),\n        ],\n    )\n    after_state = ModelState(\n        \"app\",\n        \"model\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"field\", models.Field(deconstruct=lambda x: (\"django.db.models.Field\", [], {\"some\": \"value\"}))),\n        ],\n    )\n    changes = self.get_changes([before_state], [after_state])\n    self.assertEqual(changes, {})\n", "def test_renamed_model_with_field_rename_and_unique_together(self):\n    \"\"\"\n    #26079 - Renaming a model and simultaneously renaming one of the fields that\n    are referenced in unique_together should not result in incorrect operations\n    being generated.\n    \"\"\"\n    book_with_unique_together = ModelState('otherapp', 'Book', [\n        ('id', models.AutoField(primary_key=True)),\n        ('author', models.ForeignKey('testapp.Author', models.CASCADE)),\n        ('title', models.CharField(max_length=200)),\n    ], {\n        'unique_together': {('author', 'title')},\n    })\n    book_with_field_and_model_renamed = ModelState('otherapp', 'NewBook', [\n        ('id', models.AutoField(primary_key=True)),\n        ('writer', models.ForeignKey('testapp.Writer', models.CASCADE)),\n        ('title', models.CharField(max_length=200)),\n    ], {\n        'unique_together': {('writer', 'title')},\n    })\n    changes = self.get_changes(\n        [self.author_with_book, book_with_unique_together],\n        [self.author_renamed_with_book, book_with_field_and_model_renamed],\n        MigrationQuestioner({\"ask_rename\": True, \"ask_rename_model\": True}),\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"RenameModel\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name=\"Author\", new_name=\"Writer\")\n    # Right number/type of migrations for related field rename and model rename?\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, [\"RenameModel\", \"RenameField\"])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, old_name=\"Book\", new_name=\"NewBook\")\n    self.assertOperationAttributes(changes, 'otherapp', 0, 1, old_name=\"author\", new_name=\"writer\")\n"], "sample_266": ["def test_check_key(self):\n    \"\"\"\n    Test the MigrationLoader.check_key() method.\n    \"\"\"\n    migration_loader = MigrationLoader(connection)\n    migration_loader.load_disk()\n\n    # Test normal key\n    key = (\"migrations\", \"0001_initial\")\n    self.assertEqual(migration_loader.check_key(key, \"migrations\"), key)\n\n    # Test __first__ key\n    key = (\"migrations\", \"__first__\")\n    self.assertIsNone(migration_loader.check_key(key, \"migrations\"))\n\n    # Test __first__ key with other app\n    key = (\"other_app\", \"__first__\")\n    with self.assertRaises(ValueError):\n        migration_loader.check_key(key, \"migrations\")\n\n    # Test __latest__ key\n    key = (\"migrations\", \"__latest__\")\n    self.assertEqual(migration_loader.check_key(key, \"migrations\"), (\"migrations\", \"0002_second\"))\n\n    # Test __latest__ key with other app\n    key = (\"other_app\", \"__latest__\")\n    with self.assertRaises(ValueError):\n        migration_loader.check_key(key, \"migrations\")\n\n    # Test unknown app\n    key = (\"unknown_app\", \"0001_initial\")\n    with self.assertRaises(ValueError):\n        migration_loader.check_key(key, \"migrations\")\n\n    # Test unknown migration\n    key = (\"migrations\", \"unknown_migration\")\n    with self.assertRaises(KeyError):\n        migration_loader.check_key(key, \"migrations\")\n", "    def test_detect_conflicts(self):\n        \"\"\"\n        Makes sure detect_conflicts identifies apps with more than one leaf migration.\n        \"\"\"\n        migration_loader = MigrationLoader(connection)\n        migration_loader.build_graph()\n        conflicts = migration_loader.detect_conflicts()\n        self.assertEqual(conflicts, {})\n", "    def test_detect_conflicts(self):\n        \"\"\"\n        Test MigrationLoader's ability to detect conflicts in apps with more than\n        one leaf migration.\n        \"\"\"\n        migration_loader = MigrationLoader(connection)\n        conflicts = migration_loader.detect_conflicts()\n        self.assertEqual(conflicts, {})\n        \n        # Introduce a conflict by adding a new migration with the same app label\n        with self.temporary_migration_module(module='migrations.test_migrations') as migration_dir:\n            with open(os.path.join(migration_dir, '0003_third.py'), 'w') as f:\n                f.write('''", "def test_loading_squashed_replace_conflict(self):\n    \"Tests loading a squashed migration with a conflict in the replaces list\"\n\n    loader = MigrationLoader(connection)\n    recorder = MigrationRecorder(connection)\n    self.addCleanup(recorder.flush)\n\n    # Empty database: use squashed migration\n    loader.build_graph()\n    plan = set(loader.graph.forwards_plan(('migrations', '5_auto')))\n    expected_plan = {\n        ('migrations', '0001_initial'),\n        ('migrations', '3_squashed_5'),\n        ('migrations', '4_auto'),\n        ('migrations', '5_auto'),\n    }\n    self.assertEqual(plan, expected_plan)\n\n    # Fake-apply the first migration: still use squashed migration\n    recorder.record_applied('migrations', '0001_initial')\n    loader.build_graph()\n    plan = set(loader.graph.forwards_plan(('migrations', '5_auto')))\n    expected_plan = {\n        ('migrations', '2_squashed_3'),\n        ('migrations', '3_squashed_5'),\n        ('migrations', '4_auto'),\n        ('migrations', '5_auto'),\n    }\n    self.assertEqual(plan, expected_plan)\n\n    # Fake-apply the first squashed migration: still use the second squashed migration\n    recorder.record_applied('migrations', '2_squashed_3')\n    loader.build_graph()\n    plan = set(loader.graph.forwards_plan(('migrations', '5_auto')))\n    expected_plan = {\n        ('migrations', '3_squashed_5'),\n        ('migrations', '4_auto'),\n        ('migrations', '5_auto'),\n    }\n    self.assertEqual(plan, expected_plan)\n\n    # Fake-apply the second squashed migration: conflict\n    recorder.record_applied('migrations', '3_squashed_5')\n    with self.assertRaises(NodeNotFoundError):\n        loader.build_graph()\n", "def test_check_key__first__(self):\n    \"\"\"\n    Tests MigrationLoader.check_key method for '__first__' dependency.\n    \"\"\"\n    migration_loader = MigrationLoader(connection)\n    migration_loader.load_disk()\n\n    # Test when migration with dependency on __first__ exists in graph\n    self.assertEqual(\n        migration_loader.check_key((\"migrations\", \"__first__\"), \"migrations\"),\n        (\"migrations\", \"0001_initial\"),\n    )\n\n    # Test when migration with dependency on __first__ does not exist in graph\n    with self.assertRaises(ValueError):\n        migration_loader.check_key((\"nonexistent_app\", \"__first__\"), \"migrations\")\n\n    # Test when migration with dependency on __first__ has been applied\n    recorder = MigrationRecorder(connection)\n    recorder.record_applied(\"migrations\", \"0001_initial\")\n    self.assertEqual(\n        migration_loader.check_key((\"migrations\", \"__first__\"), \"migrations\"),\n        (\"migrations\", \"0001_initial\"),\n    )\n\n    # Test when migration with dependency on __first__ has not been applied\n    recorder.record_unapplied(\"migrations\", \"0001_initial\")\n    self.assertEqual(\n        migration_loader.check_key((\"migrations\", \"__first__\"), \"migrations\"),\n        (\"migrations\", \"0001_initial\"),\n    )\n"], "sample_687": ["def test_clear_does_not_affect_previous_records(caplog):\n    caplog.set_level(logging.INFO)\n    logger.info(\"before clear\")\n    caplog.clear()\n    logger.info(\"after clear\")\n    assert len(caplog.records) == 2\n    assert caplog.records[0].getMessage() == \"before clear\"\n    assert caplog.records[1].getMessage() == \"after clear\"\n", "def test_log_auto_indent(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n            logger = logging.getLogger('catchlog')\n\n            logger.info('log message\\nwith multiple lines')\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_auto_indent=True\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*log message*\"])\n    assert result.ret == 0\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_auto_indent=4\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*    log message*\"])\n    assert result.ret == 0\n", "def test_live_logging_is_disabled_by_default(testdir):\n    \"\"\"Check that live logging is not active when not configured.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            logging.warning(\"WARNING message\")\n            assert not caplog.text\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.no_fnmatch_line(\"*WARNING message*\")\n    assert result.ret == 0\n", "def test_log_file(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_file_level == logging.INFO\n            logger = logging.getLogger('catchlog')\n            logger.info(\"INFO message will be shown\")\n            logger.debug(\"DEBUG message won't be shown\")\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_level=DEBUG\n        log_file=log_file.txt\n        log_file_level=INFO\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.stdout.no_fnmatch_line(\"*DEBUG message won't be shown*\")\n    result.stdout.fnmatch_lines([\"*INFO message will be shown*\"])\n    assert result.ret == 0\n    with open(\"log_file.txt\", \"r\") as log_file:\n        log_file_lines = log_file.readlines()\n        assert len(log_file_lines) == 2  # 1 line for log message and 1 for newline character\n        assert \"INFO message will be shown\" in log_file_lines[0]\n", "def test_log_auto_indent(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n            logger = logging.getLogger('catchlog')\n            logger.info(\"INFO message with \\nnew line\")\n            logger.info(\"INFO message without new line\")\n            logger.info(\"INFO message with\\nmany\\nnew\\nlines\")\n            caplog.set_level(logging.INFO)\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_auto_indent=true\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    assert result.ret == 0\n\n    # Check that each line is correctly indented\n    info_message_lines = result.stdout.lines[1].splitlines()\n    assert info_message_lines[0].startswith('INFO ')\n    assert info_message_lines[1].startswith('    ')\n    assert info_message_lines[2].startswith('INFO ')\n    assert info_message_lines[3].startswith('    ')\n    assert info_message_lines[4].startswith('INFO ')\n    assert info_message_lines[5].startswith('    ')\n    assert info_message_lines[6].startswith('    ')\n    assert info_message_lines[7].startswith('    ')\n"], "sample_274": ["    def test_model_form_with_unique_fields(self):\n        # Test that ModelForm correctly raises a ValidationError when there are\n        # duplicate values for unique fields.\n        class TestModel(models.Model):\n            field1 = models.CharField(max_length=10, unique=True)\n            field2 = models.CharField(max_length=10)\n\n        class TestForm(ModelForm):\n            class Meta:\n                model = TestModel\n                fields = ('field1', 'field2')\n\n        formset = formset_factory(TestForm, extra=2)\n        data = {\n            'form-0-field1': 'value1',\n            'form-0-field2': 'value2',\n            'form-1-field1': 'value1',  # duplicate value for unique field\n            'form-1-field2': 'value3',\n        }\n        formset = formset(data)\n        with self.assertRaises(ValidationError) as cm:\n            formset.clean()\n        self.assertEqual(len(cm.exception.messages), 1)\n        self.assertIn('Please correct the duplicate data for field1.', str(cm.exception))\n", "    def test_model_form_with_model_instance(self):\n        from ..models import Book\n        book = Book.objects.create(title='Book Title', author='Book Author')\n        data = {'title': 'New Title', 'author': 'New Author'}\n        form = modelform_factory(Book, fields=('title', 'author'))(data, instance=book)\n        self.assertTrue(form.is_valid())\n        book = form.save()\n        self.assertEqual(book.title, 'New Title')\n        self.assertEqual(book.author, 'New Author')\n", "    def test_modelform_required_fields(self):\n        from ..models import Book\n\n        class BookForm(ModelForm):\n            class Meta:\n                model = Book\n                fields = ['title', 'author']\n                error_messages = {\n                    'title': {\n                        'required': 'TITLE REQUIRED',\n                    },\n                    'author': {\n                        'required': 'AUTHOR REQUIRED',\n                    },\n                }\n\n        # Test that the form raises the correct error message for a missing field.\n        form = BookForm({})\n        self.assertFormErrors({\n            'title': ['TITLE REQUIRED'],\n            'author': ['AUTHOR REQUIRED'],\n        }, form.is_valid)\n", "    def test_validate_unique(self):\n        from ..models import Author\n        class AuthorForm(ModelForm):\n            class Meta:\n                model = Author\n                fields = ('name', 'email')\n\n        form = AuthorForm({'name': 'John', 'email': 'john@example.com'})\n        self.assertTrue(form.is_valid())\n\n        # Make sure the form raises a ValidationError for duplicate values\n        Author.objects.create(name='John', email='john@example.com')\n        form = AuthorForm({'name': 'John', 'email': 'john@example.com'})\n        self.assertFalse(form.is_valid())\n", "    def test_model_form_metaclass(self):\n        # Test ModelFormMetaclass.\n        class TestModelForm(ModelForm):\n            class Meta:\n                model = ChoiceModel\n                fields = ['name']\n\n        self.assertEqual(len(TestModelForm._meta.fields), 1)\n        self.assertIn('name', TestModelForm._meta.fields)\n"], "sample_616": ["def test_cross_with_missing_values() -> None:\n    da_a = xr.DataArray([1, 2, 3], dims=[\"cartesian\"], coords=dict(cartesian=([\"cartesian\"], [1, 2, 3])))\n    da_b = xr.DataArray([4, 5, np.nan], dims=[\"cartesian\"], coords=dict(cartesian=([\"cartesian\"], [1, 2, 3])))\n    expected = np.array([(-3, 6, np.nan)])\n    actual = xr.cross(da_a, da_b, dim=\"cartesian\")\n    xr.testing.assert_duckarray_allclose(expected, actual)\n", "def test_cross_no_vectorized_dims() -> None:\n    array = np.array([1, 2, 3])\n    variable = xr.Variable(\"x\", array)\n    data_array = xr.DataArray(variable, [(\"x\", -array)])\n    dataset = xr.Dataset({\"y\": variable})\n\n    with pytest.raises(ValueError):\n        xr.cross(array, variable)\n\n    with pytest.raises(ValueError):\n        xr.cross(array, data_array)\n\n    with pytest.raises(ValueError):\n        xr.cross(array, dataset)\n\n    with pytest.raises(ValueError):\n        xr.cross(variable, data_array)\n\n    with pytest.raises(ValueError):\n        xr.cross(variable, dataset)\n\n    with pytest.raises(ValueError):\n        xr.cross(data_array, dataset)\n\n    with pytest.raises(ValueError):\n        xr.cross(dataset, dataset)\n", "def test_cross_input_size_mismatch() -> None:\n    # test that error is raised if size of dimension does not match between inputs\n    a = xr.DataArray(np.arange(3), dims=\"x\")\n    b = xr.DataArray(np.arange(4), dims=\"x\")\n    with pytest.raises(ValueError, match=r\"The size of 'x' on a must be 1, 2, or 3\"):\n        xr.cross(a, b, dim=\"x\")\n", "def test_cross_coordinates():\n    # Test cross product with different coordinates\n    a = xr.DataArray(\n        np.array([[1, 2], [3, 4]]),\n        dims=[\"lat\", \"lon\"],\n        coords={\"lat\": np.linspace(0, 1, 2), \"lon\": np.linspace(0, 1, 2)},\n    )\n    b = xr.DataArray(\n        np.array([[5, 6], [7, 8]]),\n        dims=[\"lat\", \"lon\"],\n        coords={\"lat\": np.linspace(0, 1, 2), \"lon\": np.linspace(0, 1, 2)},\n    )\n\n    actual = xr.cross(a, b, dim=\"lon\")\n    expected = xr.DataArray(\n        np.array([[1.0, 1.0], [1.0, 1.0]]),\n        dims=[\"lat\"],\n        coords={\"lat\": np.linspace(0, 1, 2)},\n    )\n    assert_identical(actual, expected)\n\n    # Test cross product with different coordinates that need to be broadcast\n    a = xr.DataArray(\n        np.array([1, 2]),\n        dims=[\"lon\"],\n        coords={\"lon\": np.linspace(0, 1, 2), \"lat\": 0.5},\n    )\n    b = xr.DataArray(\n        np.array([[5, 6], [7, 8]]),\n        dims=[\"lat\", \"lon\"],\n        coords={\"lat\": np.linspace(0, 1, 2), \"lon\": np.linspace(0, 1, 2)},\n    )\n\n    actual = xr.cross(a, b, dim=\"lon\")\n    expected = xr.DataArray(\n        np.array([[1.0, 1.0], [1.0, 1.0]]),\n        dims=[\"lat\"],\n        coords={\"lat\": np.linspace(0, 1, 2)},\n    )\n    assert_identical(actual, expected)\n", "def test_where_with_missing_dims(use_dask, fill_value, a, b, expected):\n    if use_dask:\n        a = a.chunk()\n        b = b.chunk()\n\n    actual = xr.where(a > 2, a, b, fill_value=fill_value)\n    xr.testing.assert_allclose(actual.data, expected)\n"], "sample_758": ["def test_check_X_y_force_all_finite_invalid(value, force_all_finite,\n                                             match_msg, retype):\n    X = retype(np.arange(4).reshape(2, 2).astype(np.float))\n    X[0, 0] = value\n    y = np.arange(2)\n    with pytest.raises(ValueError, match=match_msg):\n        check_X_y(X, y, force_all_finite=force_all_finite,\n                  accept_sparse=True)\n", "def test_check_X_y_ensure_min_features_message():\n    # Simulate a model that would require at least 2 features\n    X = np.ones((10, 1))\n    y = np.ones(10)\n    msg = \"1 feature(s) (shape=(10, 1)) while a minimum of 2 is required.\"\n    assert_raise_message(ValueError, msg, check_X_y, X, y,\n                         ensure_min_features=2)\n\n    # The same message is raised if the data has more than 2 dimensions even if\n    # this is not mandatory\n    X = np.ones((10, 1, 28, 28))\n    assert_raise_message(ValueError, msg, check_X_y, X, y,\n                         ensure_min_features=2, allow_nd=True)\n", "def test_check_array_on_sparse_dataframe():\n    try:\n        import pandas as pd\n        import scipy.sparse as sp\n    except ImportError:\n        raise SkipTest(\"Pandas not found\")\n\n    X = sp.csc_matrix([[1, 2], [0, 0]])\n    df = pd.DataFrame.sparse.from_spmatrix(X)\n    checked_X = check_array(df, dtype=np.float64, accept_sparse=True)\n    assert sp.isspmatrix_csr(checked_X)\n    assert checked_X.dtype == np.float64\n    assert_array_equal(checked_X.toarray(), X.toarray())\n", "def test_check_X_y_force_all_finite_valid():\n    X = np.arange(4).reshape(2, 2).astype(np.float)\n    y = np.arange(2)\n    X[0, 0] = np.nan\n    X_checked, y_checked = check_X_y(X, y, force_all_finite=False)\n    assert_allclose_dense_sparse(X_checked, X)\n    assert_array_equal(y_checked, y)\n\n    X[0, 0] = np.inf\n    X_checked, y_checked = check_X_y(X, y, force_all_finite='allow-nan')\n    assert_allclose_dense_sparse(X_checked, X)\n    assert_array_equal(y_checked, y)\n\n", "def test_check_X_y_consistent_length():\n    X = np.ones((2, 2))\n    y = np.ones(1)\n    assert_raises_regex(ValueError, 'Found input variables with inconsistent '\n                        'numbers of samples: \\[2, 1\\]', check_X_y, X, y)\n    y = np.ones(3)\n    assert_raises_regex(ValueError, 'Found input variables with inconsistent '\n                        'numbers of samples: \\[2, 3\\]', check_X_y, X, y)\n    assert_no_warnings(check_X_y, X, np.ones(2))\n\n    # Test that it raises an error when input arrays are not allowed to have\n    # more than 2 dimensions.\n    X = np.ones((2, 2, 2))\n    y = np.ones(2)\n    assert_raises_regex(ValueError, 'Found array with dim 3. Estimator '\n                        'expected <= 2.', check_X_y, X, y)\n    check_X_y(X, y, allow_nd=True)  # doesn't raise\n"], "sample_122": ["    def tearDown(self):\n        cache.clear()\n", "def test_get_conditional_response(self):\n    # Test get_conditional_response with various ETag and last_modified values\n    request = self.factory.get('/test/')\n    etag = '1234567890'\n    last_modified = 'Wed, 21-Jan-2015 07:28:00 GMT'\n    response = HttpResponse()\n\n    # Test 200 status code with ETag and last_modified\n    response.status_code = 200\n    cached_response = get_conditional_response(request, etag, last_modified, response)\n    self.assertEqual(cached_response, response)\n\n    # Test 200 status code with ETag and no last_modified\n    response.status_code = 200\n    cached_response = get_conditional_response(request, etag, None, response)\n    self.assertEqual(cached_response, response)\n\n    # Test 200 status code with last_modified and no ETag\n    response.status_code = 200\n    cached_response = get_conditional_response(request, None, last_modified, response)\n    self.assertEqual(cached_response, response)\n\n    # Test 200 status code with no ETag and no last_modified\n    response.status_code = 200\n    cached_response = get_conditional_response(request, None, None, response)\n    self.assertEqual(cached_response, response)\n\n    # Test non-200 status code with ETag and last_modified\n    response.status_code = 404\n    cached_response = get_conditional_response(request, etag, last_modified, response)\n    self.assertEqual(cached_response, response)\n\n    # Test non-200 status code with ETag and no last_modified\n    response.status_code = 404\n    cached_response = get_conditional_response(request, etag, None, response)\n    self.assertEqual(cached_response, response)\n\n    # Test non-200 status code with last_modified and no ETag\n    response.status_code = 404\n    cached_response = get_conditional_response(request, None, last_modified, response)\n    self.assertEqual(cached_response, response)\n\n    # Test non-200 status code with no ETag and no last_modified\n    response.status_code = 404\n    cached_response = get_conditional_response(request, None, None, response)\n    self.assertEqual(cached_response, response)\n\n    # Test with If-Match header\n    request.META['HTTP_IF_MATCH'] = '*'\n    response.status_code = 200\n    cached_response = get_conditional_response(request, etag, last_modified", "    def test_get_conditional_response(self):\n        request = self.factory.get('/')\n        response = HttpResponse()\n        response['ETag'] = '12345'\n        last_modified = timezone.now()\n        response['Last-Modified'] = http_date(last_modified.timestamp())\n        result = get_conditional_response(request, etag=response['ETag'], last_modified=last_modified, response=response)\n        self.assertIsInstance(result, HttpResponseNotModified)\n\n        request = self.factory.get('/')\n        request.META['HTTP_IF_MATCH'] = '12345'\n        request.META['HTTP_IF_MODIFIED_SINCE'] = http_date(last_modified.timestamp())\n        response = HttpResponse()\n        result = get_conditional_response(request, etag='12345', last_modified=last_modified, response=response)\n        self.assertIsInstance(result, HttpResponseNotModified)\n\n        request = self.factory.get('/')\n        request.META['HTTP_IF_NONE_MATCH'] = '12345'\n        request.META['HTTP_IF_MODIFIED_SINCE'] = http_date(last_modified.timestamp())\n        response = HttpResponse()\n        result = get_conditional_response(request, etag='12345', last_modified=last_modified, response=response)\n        self.assertIsInstance(result, HttpResponseNotModified)\n", "    def test_get_conditional_response(self):\n        request = HttpRequest()\n        request.META['HTTP_IF_MODIFIED_SINCE'] = 'Wed, 21 Jan 2015 12:53:33 GMT'\n        request.META['HTTP_IF_NONE_MATCH'] = '\"1234567890\"'\n\n        response = HttpResponse()\n        response['Last-Modified'] = 'Wed, 21 Jan 2015 12:53:33 GMT'\n        response['ETag'] = '\"1234567890\"'\n\n        conditional_response = get_conditional_response(request, last_modified=response['Last-Modified'], etag=response['ETag'], response=response)\n        self.assertIsInstance(conditional_response, HttpResponseNotModified)\n\n        request.META['HTTP_IF_MODIFIED_SINCE'] = 'Wed, 21 Jan 2015 12:53:32 GMT'\n        request.META['HTTP_IF_NONE_MATCH'] = '\"1234567891\"'\n\n        conditional_response = get_conditional_response(request, last_modified=response['Last-Modified'], etag=response['ETag'], response=response)\n        self.assertEqual(conditional_response, response)\n", "    def test_get_cache_key_with_vary(self):\n        request = self.factory.get(self.path)\n        response = HttpResponse()\n        response['Vary'] = 'Accept-Encoding'\n        # Expect None if no headers have been set yet.\n        self.assertIsNone(get_cache_key(request))\n        # Set headers to an empty list.\n        learn_cache_key(request, response)\n\n        self.assertEqual(\n            get_cache_key(request),\n            'views.decorators.cache.cache_page.settingsprefix.GET.'\n            '9a3e9e0f9a8f7d3f14f2fbb1eb4b22f.d41d8cd98f00b204e9800998ecf8427e'\n        )\n\n        # Same vary header should return the same cache key.\n        request = self.factory.get(self.path)\n        response = HttpResponse()\n        response['Vary'] = 'Accept-Encoding'\n        learn_cache_key(request, response)\n        self.assertEqual(\n            get_cache_key(request),\n            'views.decorators.cache.cache_page.settingsprefix.GET.'\n            '9a3e9e0f9a8f7d3f14f2fbb1eb4b22f.d41d8cd98f00b204e9800998ecf8427e'\n        )\n\n        # Different vary header should return a different cache key.\n        request = self.factory.get(self.path)\n        response = HttpResponse()\n        response['Vary'] = 'Cookie'\n        learn_cache_key(request, response)\n        self.assertNotEqual(\n            get_cache_key(request),\n            'views.decorators.cache.cache_page.settingsprefix.GET.'\n            '9a3e9e0f9a8f7d3f14f2fbb1eb4b22f.d41d8cd98f00b204e9800998ecf8427e'\n        )\n"], "sample_1012": ["def test_SymPyPrinter():\n    p = SymPyPrinter()\n    expr = acos(x)\n    assert 'sympy' not in p.module_imports\n    assert p.doprint(expr) == 'sympy.acos(x)'\n    assert 'sympy' in p.module_imports\n    assert p.doprint(pi) == 'sympy.pi'\n    assert p.module_imports['sympy'] == {'acos', 'pi'}\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    assert p.doprint(acos(x)) == 'sympy.acos(x)'\n    assert p.doprint(Piecewise((1, Eq(x, 0)), (2, x>6))) == '((1) if (x == 0) else (2) if (x > 6) else None)'\n    assert p.doprint(sign(x)) == '(0.0 if x == 0 else sympy.copysign(1, x))'\n", "def test_PythonCodePrinter_with_user_functions():\n    prntr = PythonCodePrinter(user_functions={'sin': 'my_sin'})\n    assert prntr.doprint(x**y) == 'x**y'\n    assert prntr.doprint(Mod(x, 2)) == 'x % 2'\n    assert prntr.doprint(And(x, y)) == 'x and y'\n    assert prntr.doprint(Or(x, y)) == 'x or y'\n    assert not prntr.module_imports\n    assert prntr.doprint(pi) == 'math.pi'\n    assert prntr.module_imports == {'math': {'pi'}}\n    assert prntr.doprint(acos(x)) == 'my_sin(x)'\n    assert prntr.doprint(Assignment(x, 2)) == 'x = 2'\n    assert prntr.doprint(Piecewise((1, Eq(x, 0)),\n                        (2, x>6))) == '((1) if (x == 0) else (2) if (x > 6) else None)'\n    assert prntr.doprint(Piecewise((2, Le(x, 0)),\n                        (3, Gt(x, 0)), evaluate=False)) == '((2) if (x <= 0) else'\\\n                                                        ' (3) if (x > 0) else None)'\n    assert prntr.doprint(sign(x)) == '(0.0 if x == 0 else math.copysign(1, x))'\n", "def test_function_definition():\n    x = symbols('x')\n    expr = Assignment(x, x**2)\n    f = PythonCodePrinter().doprint(expr)\n    assert f == \"x = x**2\"\n\n    func_def = Eq(x, x**2)\n    f = PythonCodePrinter().doprint(func_def)\n    assert f == \"def x(x):\\n    x = x**2\"\n\n    func_def = Eq(x, x**2 + y)\n    f = PythonCodePrinter().doprint(func_def)\n    assert f == \"def x(x, y):\\n    x = x**2 + y\"\n\n    func_def = Eq(x, x**2 + y**2)\n    f = PythonCodePrinter().doprint(func_def)\n    assert f == \"def x(x, y):\\n    x = x**2 + y**2\"\n", "def test_PythonCodePrinter_function_definition():\n    x, y, z = symbols('x y z')\n    f = Function('f')\n    fd = f.func(x, y, z)\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(fd) == 'f(x, y, z)'\n    assert prntr.doprint(Assignment(x, fd)) == 'x = f(x, y, z)'\n    assert prntr.doprint(fd.diff(x)) == 'f.diff(x)(x, y, z)'\n    assert prntr.doprint(fd.diff(x, y)) == 'f.diff(x, y)(x, y, z)'\n"], "sample_696": ["def test_argument_percent_default_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n            group = parser.getgroup(\"general\")\n            group.addoption(\"--foo\", action=\"store\", default=\"bar\", help=\"foo help %default\")\n    \"\"\"\n    )\n    result = pytester.runpytest(\"--help\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: pytest now uses argparse. '%default' should be changed to '%(default)s'\",\n        ]\n    )\n", "def test_argument_percent_default_deprecated(pytester: Pytester) -> None:\n    pytester.makeini(\"[pytest]\n        addopts = --help\n        markers =\n            slow: this is a slow test\n        [tool:pytest]\n        addopts =\n            --help\n            --strict-markers\n    \"\"\")\n    result = pytester.runpytest(\"-h\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: pytest now uses argparse. '%default' should be changed to '%(default)s'\",\n        ]\n    )\n\n", "def test_argument_percent_default_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        addopts = --myopt=%default\n    \"\"\"\n    )\n    with pytest.warns(\n        pytest.PytestDeprecationWarning,\n        match=re.escape(\n            'pytest now uses argparse. \"%default\" should be changed to \"%(default)s\"',\n        ),\n    ):\n        pytester.runpytest()\n\n", "def test_argument_percent_default_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            pass\n\n            group = parser.getgroup(\"my_group\")\n            group.addoption(\n                \"--my-option\",\n                action=\"store\",\n                default=\"default_value\",\n                help=\"my help, default: %default\",\n            )\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: pytest now uses argparse. %default should be changed to %(default)s\",\n        ]\n    )\n\n", "def test_argument_percent_default_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            return 10\n\n            assert fix == 10\n    \"\"\"\n    )\n    pytester.makefile(\n        \".ini\",\n        \"\"\"\n        [pytest]\n        addopts = --help\n    \"\"\"\n    )\n\n    pytester.makefile(\n        \".ini\",\n        \"\"\"\n        [pytest]\n        addopts = --help\n        [tool:pytest]\n        addopts = --foo=%default\n    \"\"\"\n    )\n\n    result = pytester.runpytest(\"--foo=bar\", \"--help\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: pytest now uses argparse.*\"\n        ]\n    )\n\n"], "sample_317": ["def test_feed_generator_string_representation(self):\n    \"\"\"\n    Test the string representation of the feed generator.\n    \"\"\"\n    feed = Rss201rev2Feed(\n        title=\"Poynter E-Media Tidbits\",\n        link=\"http://www.poynter.org/column.asp?id=31\",\n        description=\"A group Weblog by the sharpest minds in online media/journalism/publishing.\",\n        language=\"en\",\n    )\n    feed.add_item(\n        title=\"Hello\",\n        link=\"http://www.holovaty.com/test/\",\n        description=\"Testing.\",\n    )\n    self.assertIn(\"Rss201rev2Feed\", str(feed))\n    self.assertIn(\"title=Poynter E-Media Tidbits\", str(feed))\n    self.assertIn(\"link=http://www.poynter.org/column.asp?id=31\", str(feed))\n    self.assertIn(\"description=A group Weblog by the sharpest minds in online media/journalism/publishing.\", str(feed))\n    self.assertIn(\"language=en\", str(feed))\n\n    atom_feed = Atom1Feed(\n        title=\"Poynter E-Media Tidbits\",\n        link=\"http://www.poynter.org/column.asp?id=31\",\n        description=\"A group Weblog by the sharpest minds in online media/journalism/publishing.\",\n        language=\"en\",\n    )\n    atom_feed.add_item(\n        title=\"Hello\",\n        link=\"http://www.holovaty.com/test/\",\n        description=\"Testing.\",\n    )\n    self.assertIn(\"Atom1Feed\", str(atom_feed))\n    self.assertIn(\"title=Poynter E-Media Tidbits\", str(atom_feed))\n    self.assertIn(\"link=http://www.poynter.org/column.asp?id=31\", str(atom_feed))\n    self.assertIn(\"description=A group Weblog by the sharpest minds in online media/journalism/publishing.\", str(atom_feed))\n    self.assertIn(\"language=en\", str(atom_feed))\n", "def test_syndication_feed_add_root_elements(self):\n    \"\"\"\n    Test the add_root_elements() method of SyndicationFeed.\n    \"\"\"\n    class TestFeed(SyndicationFeed):\n            handler.addQuickElement(\"test_element\", \"Test content\")\n\n    feed = TestFeed(\n        title=\"Test Feed\",\n        link=\"https://example.com\",\n        description=\"Test feed description\",\n    )\n    feed.add_item(\n        title=\"Test Item\",\n        link=\"https://example.com/test-item/\",\n        description=\"Test item description\",\n    )\n    doc = feed.writeString('utf-8')\n    self.assertIn('<test_element>Test content</test_element>', doc)\n", "def test_root_attributes(self):\n    \"\"\"\n    Test the root_attributes method of SyndicationFeed.\n    \"\"\"\n    feed = SyndicationFeed(\n        title=\"Poynter E-Media Tidbits\",\n        link=\"http://www.poynter.org/column.asp?id=31\",\n        description=\"A group Weblog by the sharpest minds in online media/journalism/publishing.\",\n        language=\"en\",\n    )\n    self.assertEqual(feed.root_attributes(), {})\n\n    class TestFeed(SyndicationFeed):\n            return {\"xmlns\": \"http://example.com\"}\n\n    test_feed = TestFeed(\n        title=\"Poynter E-Media Tidbits\",\n        link=\"http://www.poynter.org/column.asp?id=31\",\n        description=\"A group Weblog by the sharpest minds in online media/journalism/publishing.\",\n        language=\"en\",\n    )\n    self.assertEqual(test_feed.root_attributes(), {\"xmlns\": \"http://example.com\"})\n\n    class TestFeed2(SyndicationFeed):\n            return {\"xmlns\": \"http://example.com\", \"lang\": \"en\"}\n\n    test_feed2 = TestFeed2(\n        title=\"Poynter E-Media Tidbits\",\n        link=\"http://www.poynter.org/column.asp?id=31\",\n        description=\"A group Weblog by the sharpest minds in online media/journalism/publishing.\",\n        language=\"en\",\n    )\n    self.assertEqual(test_feed2.root_attributes(), {\"xmlns\": \"http://example.com\", \"lang\": \"en\"})\n", "    def test_add_item_elements_date_in_feed(self):\n        \"\"\"\n        Tests that dates are added to items in the feed.\n        \"\"\"\n        response = self.client.get('/syndication/rss2/')\n        doc = minidom.parseString(response.content)\n        chan = doc.getElementsByTagName('channel')[0]\n        items = chan.getElementsByTagName('item')\n\n        d = Entry.objects.get(pk=self.e1.pk).published\n        pub_date = rfc2822_date(timezone.make_aware(d, TZ))\n\n        self.assertChildNodeContent(items[0], {\n            'pubDate': pub_date,\n        })\n\n        d = Entry.objects.get(pk=self.e2.pk).updated\n        updated_date = rfc2822_date(timezone.make_aware(d, TZ))\n\n        self.assertChildNodeContent(items[1], {\n            'pubDate': updated_date,\n        })\n", "def test_rss201rev2feed_with_item_copyright(self):\n    \"\"\"\n    Test if an item-level copyright is correctly added to the RSS feed.\n    \"\"\"\n    response = self.client.get('/syndication/rss2/item_copyright/')\n    doc = minidom.parseString(response.content)\n    chan = doc.getElementsByTagName('channel')[0]\n    items = chan.getElementsByTagName('item')\n    for item in items:\n        self.assertChildNodes(item, [\n            'title',\n            'link',\n            'description',\n            'guid',\n            'category',\n            'pubDate',\n            'author',\n            'comments',\n            'copyright',\n        ])\n        self.assertEqual(\n            item.getElementsByTagName('copyright')[0].firstChild.wholeText,\n            'Copyright (c) 2007, Item author'\n        )\n"], "sample_597": ["    def test_merge_coords(self):\n        coord1 = xr.DataArray([1, 2], dims=\"x\", coords={\"x\": [10, 20]})\n        coord2 = xr.DataArray([3, 4], dims=\"x\", coords={\"x\": [10, 30]})\n\n        actual, _ = merge.merge_coords([coord1, coord2])\n        expected = {\"x\": xr.DataArray([10, 20, 30], dims=\"x\")}\n        assert actual == expected\n\n        actual, _ = merge.merge_coords([coord1, coord2], join=\"inner\")\n        expected = {\"x\": xr.DataArray([10], dims=\"x\")}\n        assert actual == expected\n\n        actual, _ = merge.merge_coords([coord1, coord2], join=\"left\")\n        expected = {\"x\": xr.DataArray([10, 20], dims=\"x\")}\n        assert actual == expected\n\n        actual, _ = merge.merge_coords([coord1, coord2], join=\"right\")\n        expected = {\"x\": xr.DataArray([10, 30], dims=\"x\")}\n        assert actual == expected\n\n        with pytest.raises(ValueError):\n            merge.merge_coords([coord1, coord2], join=\"exact\")\n", "    def test_merge_with_pandas_dataframe(self):\n        pandas_df = pd.DataFrame({\"x\": [1, 2, 3]})\n        xr_ds = xr.Dataset({\"x\": [1, 2, 3]})\n        actual = xr.merge([xr_ds, pandas_df])\n        expected = xr.Dataset({\"x\": [1, 2, 3]})\n        assert actual.identical(expected)\n", "    def test_unique_variable_broadcast_equals(self):\n        variables = [xr.Variable(\"x\", [1, 2, 3]), xr.Variable(\"x\", [1, 2, 3])]\n        actual = merge.unique_variable(\"x\", variables, compat=\"broadcast_equals\")\n        expected = variables[0]\n        assert_identical(actual, expected)\n", "    def test_merge_compat_override(self):\n        data = create_test_data()\n        ds1 = data[[\"var1\"]]\n        ds2 = data[[\"var2\"]]\n        ds2[\"var2\"] *= 2\n        expected = data[[\"var1\", \"var2\"]].copy()\n        expected[\"var2\"] *= 2\n        assert expected.identical(ds1.merge(ds2, compat=\"override\"))\n\n        assert ds1.identical(ds1.merge(ds2, compat=\"override\", join=\"left\"))\n        assert expected.identical(ds1.merge(ds2, compat=\"override\", join=\"right\"))\n", "    def test_merge_coords_override_dims(self):\n        ds1 = xr.Dataset({\"x\": [1, 2]})\n        ds2 = xr.Dataset({\"x\": [3, 4]})\n        with pytest.raises(MergeError):\n            xr.merge([ds1, ds2], join=\"override\")\n"], "sample_644": ["def test_check_import_as_rename(self) -> None:\n    module = astroid.MANAGER.ast_from_module_name(\"rename\", REGR_DATA)\n    import_node = module.body[0]\n    import_from_node = module.body[1]\n\n    msg = MessageTest(\n        msg_id=\"useless-import-alias\",\n        node=import_node,\n        line=1,\n        col_offset=0,\n        end_line=1,\n        end_col_offset=27,\n    )\n    with self.assertAddsMessages(msg):\n        self.checker.visit_import(import_node)\n    with self.assertNoMessages():\n        self.checker.visit_importfrom(import_from_node)\n\n    msg = MessageTest(\n        msg_id=\"consider-using-from-import\",\n        node=import_from_node,\n        args=(\"package\", \"module\"),\n        line=2,\n        col_offset=0,\n        end_line=2,\n        end_col_offset=33,\n    )\n    with self.assertAddsMessages(msg):\n        self.checker.visit_importfrom(import_from_node)\n", "def test_import_outside_toplevel(self) -> None:\n    module = astroid.MANAGER.ast_from_module_name(\"import_outside_toplevel\", REGR_DATA)\n    import_node = module.body[0]\n    function_node = module.body[1]\n\n    msg = MessageTest(\n        msg_id=\"import-outside-toplevel\",\n        node=import_node,\n        args=\"a,b\",\n        line=3,\n        col_offset=4,\n        end_line=3,\n        end_col_offset=13,\n    )\n    with self.assertAddsMessages(msg):\n        self.checker.visit_import(import_node)\n        self.checker.visit_functiondef(function_node)\n\n    msg = MessageTest(\n        msg_id=\"import-outside-toplevel\",\n        node=import_node,\n        args=\"c\",\n        line=6,\n        col_offset=4,\n        end_line=6,\n        end_col_offset=9,\n    )\n    with self.assertAddsMessages(msg):\n        self.checker.visit_import(import_node)\n        self.checker.visit_classdef(function_node)\n", "    def test_import_outside_toplevel(self) -> None:\n        module = astroid.MANAGER.ast_from_module_name(\"import_outside_toplevel\", REGR_DATA)\n        import_node = module.body[0].body[0].body[0]\n        import_from_node = module.body[1].body[0].body[0]\n\n        msg_import = MessageTest(\n            msg_id=\"import-outside-toplevel\",\n            node=import_node,\n            args=\"import_outside_toplevel\",\n            line=3,\n            col_offset=8,\n            end_line=3,\n            end_col_offset=25,\n        )\n        msg_import_from = MessageTest(\n            msg_id=\"import-outside-toplevel\",\n            node=import_from_node,\n            args=\"import_outside_toplevel\",\n            line=6,\n            col_offset=8,\n            end_line=6,\n            end_col_offset=27,\n        )\n        with self.assertAddsMessages(msg_import, msg_import_from):\n            self.checker.visit_functiondef(module.body[0])\n            self.checker.visit_classdef(module.body[1])\n        with self.assertNoMessages():\n            self.checker.visit_functiondef(module.body[2])\n            self.checker.visit_classdef(module.body[3])\n", "    def test_import_outside_toplevel(self) -> None:\n        module = astroid.MANAGER.ast_from_module_name(\"import_outside_toplevel\", REGR_DATA)\n        import_from = module.body[0].body[0]\n\n        msg = MessageTest(\n            msg_id=\"import-outside-toplevel\",\n            node=import_from,\n            args=\"imported_module\",\n            line=4,\n            col_offset=4,\n            end_line=4,\n            end_col_offset=19,\n        )\n        with self.assertAddsMessages(msg):\n            self.checker.visit_importfrom(import_from)\n\n        # Test that imports are allowed inside nested scopes when the module is allowed\n        module = astroid.MANAGER.ast_from_module_name(\"import_outside_toplevel_allowed\", REGR_DATA)\n        import_from = module.body[0].body[0]\n\n        with self.assertNoMessages():\n            self.checker.visit_importfrom(import_from)\n", "    def test_check_import_position(self) -> None:\n        module = astroid.parse(\n            \"\"\""], "sample_629": ["def test_expand_modules_ignore(\n    files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re, expected", "def test_expand_modules_ignores(\n    files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re, expected", "def test_expand_modules_with_ignore_list(\n    files_or_modules, ignore_list, ignore_list_re, expected", "def test_expand_modules_ignore(files_or_modules, ignore_list, expected):\n    ignore_list_re, ignore_list_paths_re = [], []\n    modules, errors = expand_modules(\n        files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re\n    )\n    modules.sort(key=lambda d: d[\"name\"])\n    assert modules == expected\n    assert not errors\n\n", "def test_expand_modules_nonpackage_files(files_or_modules, expected):\n    ignore_list, ignore_list_re, ignore_list_paths_re = [], [], []\n    modules, errors = expand_modules(\n        files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re\n    )\n    modules.sort(key=lambda d: d[\"name\"])\n    assert modules == expected\n    assert not errors\n\n"], "sample_180": ["    def test_save_force_insert_with_unsaved_related_object(self):\n        class Author(models.Model):\n            pass\n\n        class Book(models.Model):\n            author = models.ForeignKey(Author, on_delete=models.CASCADE)\n\n        author = Author()\n        book = Book(author=author)\n\n        with self.assertRaises(ValueError):\n            book.save(force_insert=True)\n\n        author.save()\n        book.save()\n", "    def test_model_with_order_with_respect_to(self):\n        class Question(models.Model):\n            pass\n\n        class Answer(models.Model):\n            question = models.ForeignKey(Question, models.CASCADE)\n\n            class Meta:\n                order_with_respect_to = 'question'\n\n        question = Question.objects.create()\n        answer1 = Answer.objects.create(question=question)\n        answer2 = Answer.objects.create(question=question)\n\n        self.assertEqual(Answer.objects.count(), 2)\n\n        answer1.delete()\n        self.assertEqual(Answer.objects.count(), 1)\n\n        answer3 = Answer.objects.create(question=question)\n        self.assertEqual(Answer.objects.count(), 2)\n\n        answer3.delete()\n        self.assertEqual(Answer.objects.count(), 1)\n\n        answer4 = Answer.objects.create(question=question)\n        self.assertEqual(Answer.objects.count(), 2)\n\n        answer4.delete()\n        self.assertEqual(Answer.objects.count(), 1)\n", "    def test_check_db_column_clashes(self):\n        class Model(models.Model):\n            field1 = models.IntegerField(db_column='my_column')\n            field2 = models.IntegerField(db_column='my_column')\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"Field 'field2' has column name 'my_column' that is used by \"\n                \"another field.\",\n                hint=\"Specify a 'db_column' for the field.\",\n                obj=Model,\n                id='models.E007',\n            )\n        ])\n", "    def test_model_check_id_clash_with_abstract_parent(self):\n        class AbstractParent(models.Model):\n            id = models.IntegerField()\n\n        class Child(AbstractParent):\n            pass\n\n        self.assertEqual(Child.check(), [\n            Error(\n                \"'id' can only be used as a field name if the field also sets \"\n                \"'primary_key=True'.\",\n                obj=Child._meta.get_field('id'),\n                id='models.E004',\n            ),\n        ])\n", "    def test_order_with_respect_to_on_proxy_model(self):\n        class Question(models.Model):\n            pass\n\n        class Answer(Question):\n            class Meta:\n                proxy = True\n                order_with_respect_to = 'question'\n\n        self.assertEqual(Answer.check(), [\n            Error(\n                \"Proxy model cannot have 'order_with_respect_to'.\",\n                obj=Answer,\n                id='models.E031',\n            ),\n        ])\n"], "sample_337": ["def test_process_view_token_too_short(self):\n    \"\"\"\n    If the token is shorter than expected, it is ignored and a new token is\n    created.\n    \"\"\"\n    req = self._get_request()\n    self._set_csrf_cookie(req, 'x' * (CSRF_TOKEN_LENGTH - 1))\n    mw = CsrfViewMiddleware(token_view)\n    mw.process_view(req, token_view, (), {})\n    resp = mw(req)\n    csrf_cookie = self._read_csrf_cookie(req, resp)\n    self.assertEqual(len(csrf_cookie), CSRF_TOKEN_LENGTH)\n", "def test_origin_verified_with_trusted_origins(self):\n    \"\"\"\n    A POST HTTPS request with an origin that is in CSRF_TRUSTED_ORIGINS is\n    accepted.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req._is_secure_override = True\n    req.META['HTTP_HOST'] = 'www.example.com'\n    req.META['HTTP_ORIGIN'] = 'https://www.example.com'\n    mw = CsrfViewMiddleware(post_form_view)\n    with self.settings(CSRF_TRUSTED_ORIGINS=['https://www.example.com']):\n        mw.process_request(req)\n        resp = mw.process_view(req, post_form_view, (), {})\n        self.assertIsNone(resp)\n", "def test_process_response_csrf_cookie_secure(self):\n    \"\"\"\n    The CSRF cookie is secure if the request is secure and\n    CSRF_COOKIE_SECURE is True.\n    \"\"\"\n    req = self._get_request()\n    req._is_secure_override = True\n    with self.settings(CSRF_COOKIE_SECURE=True):\n        mw = CsrfViewMiddleware(token_view)\n        mw.process_view(req, token_view, (), {})\n        resp = mw(req)\n        self.assertTrue(resp.cookies[settings.CSRF_COOKIE_NAME]['secure'])\n", "def test_origin_verified_host_with_port(self):\n    \"\"\"Test _origin_verified() with host and port.\"\"\"\n    req = self._get_POST_request_with_token()\n    req.META['HTTP_HOST'] = 'www.example.com:8080'\n    req.META['HTTP_ORIGIN'] = 'https://www.example.com:8080'\n    mw = CsrfViewMiddleware(post_form_view)\n    self.assertIs(mw._origin_verified(req), True)\n", "def test_process_view_rotate_token_called(self):\n    \"\"\"\n    If rotate_token() is called by a view that has already been processed by\n    CsrfViewMiddleware, the token is updated.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    mw = CsrfViewMiddleware(sandwiched_rotate_token_view)\n    mw.process_request(req)\n    mw.process_view(req, sandwiched_rotate_token_view, (), {})\n    response = mw(req)\n    self.assertContains(response, 'OK')\n    csrf_cookie = self._read_csrf_cookie(req, response)\n    actual_secret = _unmask_cipher_token(csrf_cookie)\n    # set_cookie() was called a second time with a different secret.\n    self.assertCookiesSet(req, response, [TEST_SECRET, actual_secret])\n    self.assertNotEqual(actual_secret, TEST_SECRET)\n"], "sample_75": ["    def test_model(self):\n        model = resolve_relation(Author, 'self')\n        self.assertEqual(model, Author)\n", "    def setUpTestData(cls):\n        cls.author1 = Author.objects.create(name='Jane')\n        cls.author2 = Author.objects.create(name='Tom')\n        Bio.objects.create(author=cls.author1)\n        Bio.objects.create(author=cls.author2)\n", "    def test_traversal_foreignkey_attribute(self):\n        book1 = Book.objects.create(title='Poems')\n        book2 = Book.objects.create(title='Jane Eyre')\n        book3 = Book.objects.create(title='Wuthering Heights')\n        book4 = Book.objects.create(title='Sense and Sensibility')\n\n        author1 = Author.objects.create(name='Charlotte', first_book=book1)\n        author2 = Author.objects.create(name='Anne', first_book=book1)\n        author3 = Author.objects.create(name='Emily', first_book=book1)\n        author4 = Author.objects.create(name='Jane', first_book=book4)\n\n        book1.authors.add(author1, author2, author3)\n        book2.authors.add(author1)\n        book3.authors.add(author3)\n        book4.authors.add(author4)\n\n        with self.assertNumQueries(2):\n            authors = Author.objects.prefetch_related('books', 'first_book__authors')\n            authors_list = [[str(a.first_book.title), [str(b) for b in a.books.all()]] for a in authors]\n        self.assertEqual(authors_list, [\n            [\"Poems\", [\"Poems\", \"Jane Eyre\"]],\n            [\"Poems\", [\"Poems\"]],\n            [\"Poems\", [\"Poems\", \"Wuthering Heights\"]],\n            [\"Sense and Sensibility\", [\"Sense and Sensibility\"]],\n        ])\n\n        with self.assertNumQueries(2):\n            authors = Author.objects.prefetch_related('first_book__authors__books')\n            authors_list = [[str(a.first_book.title), [str(b) for b in a.first_book.authors.all()], [str(b) for b in a.books.all()]] for a in authors]\n        self.assertEqual(authors_list, [\n            [\"Poems\", [\"Charlotte\", \"Anne\", \"Emily\"], [\"Poems\", \"Jane Eyre\"]],\n            [\"Poems\", [\"Charlotte\", \"Anne\", \"Emily\"], [\"Poems\"]],\n            [\"Poems\", [\"Charlotte\", \"Anne\", \"Emily\"], [\"Poems\", \"Wuthering Heights\"]],\n            [\"Sense and Sensibility\", [\"Jane\"], [\"Sense and Sensibility\"]],\n        ])\n", "    def setUpTestData(cls):\n        cls.book = Book.objects.create(title='Poems')\n        cls.author = Author.objects.create(first_book=cls.book)\n        cls.author_address = AuthorAddress.objects.create(author=cls.author)\n", "    def setUpTestData(cls):\n        cls.book = Book.objects.create(title='Wuthering Heights')\n        cls.author = Author.objects.create(name='Emily', first_book=cls.book)\n"], "sample_1179": ["def test_Inverse():\n    M = MatrixSymbol(\"M\", 3, 3)\n    N = MatrixSymbol(\"N\", 3, 3)\n    assert str(M**(-1)) == \"M**(-1)\"\n    assert str(N**(-1)) == \"N**(-1)\"\n    assert str(3 * M**(-1)) == \"3*M**(-1)\"\n    assert str(M**(-1) * 3) == \"3*M**(-1)\"\n    assert str(-M**(-1)) == \"-M**(-1)\"\n", "def test_Pow_unexpected_inputs():\n    assert str(Pow(x, None)) == \"x**None\"\n    assert str(Pow(x, nan)) == \"x**nan\"\n    assert str(Pow(x, zoo)) == \"x**zoo\"\n    assert str(Pow(x, oo)) == \"x**oo\"\n", "def test_Pow_Rational():\n    assert str(x**(Rational(1, 2))) == 'x**(1/2)'\n    assert str(x**(Rational(-1, 2))) == 'x**(-1/2)'\n    assert str(x**2) == 'x**2'\n    assert str(x**(-1)) == '1/x'\n    assert str(x**(Rational(2, 3))) == 'x**(2/3)'\n    assert str(x**(Rational(-2, 3))) == 'x**(-2/3)'\n    assert str(2**(Rational(1, 2))) == 'sqrt(2)'\n    assert str(2**(Rational(-1, 2))) == '1/sqrt(2)'\n", "def test_issue_25780():\n    from sympy.physics.control.lti import MIMOFreqTrans\n    from sympy.physics.control.lti import TransferFunctionMatrix\n    tf1 = TransferFunction(1, 1, s)\n    tf2 = TransferFunction(2, 2, s)\n    tfm = TransferFunctionMatrix([[tf1, tf2], [tf2, tf1]])\n    assert str(MIMOFreqTrans(tfm)) == \\\n        \"MIMOFreqTrans(TransferFunctionMatrix(((TransferFunction(1, 1, s), TransferFunction(2, 2, s)), (TransferFunction(2, 2, s), TransferFunction(1, 1, s)))))\"\n", "def test_EmptySequence():\n    assert str(S.EmptySequence) == \"EmptySequence\"\n"], "sample_121": ["    def test_long_field_name(self):\n        \"\"\"\n        #13711 -- Model check for long field names\n        when database does not support long names.\n        \"\"\"\n        allowed_len, db_alias = get_max_column_name_length()\n\n        class ModelWithLongField(models.Model):\n            title = models.CharField(max_length=11)\n\n        long_field_name = 'a' * (self.max_column_name_length + 1)\n        models.CharField(max_length=11).contribute_to_class(ModelWithLongField, long_field_name)\n        self.assertEqual(ModelWithLongField.check(), [\n            Error(\n                'Autogenerated column name too long for field \"%s\". '\n                'Maximum length is \"%s\" for database \"%s\".'\n                % (long_field_name, self.max_column_name_length, self.column_limit_db_alias),\n                hint=\"Set the column name manually using 'db_column'.\",\n                obj=ModelWithLongField,\n                id='models.E018',\n            )\n        ])\n", "    def test_proxy_model_cannot_have_fields(self):\n        class AbstractModel(models.Model):\n            class Meta:\n                abstract = True\n\n            field1 = models.IntegerField()\n            field2 = models.CharField(max_length=10)\n\n        class ProxyModel(AbstractModel):\n            class Meta:\n                proxy = True\n\n        self.assertEqual(ProxyModel.check(), [\n            Error(\n                \"Proxy model 'ProxyModel' contains model fields.\",\n                obj=ProxyModel,\n                id='models.E017',\n            ),\n        ])\n", "    def test_ordering_on_multiple_inherited_fields(self):\n        class Parent(models.Model):\n            field1 = models.CharField(max_length=10)\n\n        class Child(Parent):\n            field2 = models.CharField(max_length=10)\n\n            class Meta:\n                ordering = ['field1', 'field2']\n\n        self.assertEqual(Child.check(), [])\n", "    def test_multiple_inheritance_check_id_fields(self):\n        class Parent(models.Model):\n            id = models.IntegerField()\n\n        class Child(Parent):\n            pass\n\n        self.assertEqual(Child.check(), [\n            Error(\n                \"'id' can only be used as a field name if the field also sets \"\n                \"'primary_key=True'.\",\n                obj=Child._meta.get_field('id'),\n                id='models.E004',\n            ),\n        ])\n", "    def test_ordering_is_empty_list(self):\n        class Model(models.Model):\n            field1 = models.IntegerField()\n            field2 = models.IntegerField()\n\n            class Meta:\n                ordering = []\n\n        self.assertEqual(Model.check(), [])\n"], "sample_21": ["def test_read_write_with_error_specs(tmp_path):\n    test_file = tmp_path / \"test.qdp\"\n    t1 = Table()\n    t1.add_column(Column(name=\"a\", data=[1, 2, 3]))\n    t1.add_column(Column(name=\"a_err\", data=[0.1, 0.2, 0.3]))\n    t1.add_column(Column(name=\"b\", data=[4.0, 5.0, 6.0]))\n    t1.add_column(Column(name=\"b_perr\", data=[0.4, 0.5, 0.6]))\n    t1.add_column(Column(name=\"b_nerr\", data=[-0.4, -0.5, -0.6]))\n    t1.write(test_file, format=\"ascii.qdp\")\n    t2 = Table.read(test_file, table_id=0, format=\"ascii.qdp\", names=[\"a\", \"b\"])\n    assert np.allclose(t2[\"a\"], t1[\"a\"])\n    assert np.allclose(t2[\"a_err\"], t1[\"a_err\"])\n    assert np.allclose(t2[\"b\"], t1[\"b\"])\n    assert np.allclose(t2[\"b_perr\"], t1[\"b_perr\"])\n    assert np.allclose(t2[\"b_nerr\"], t1[\"b_nerr\"])\n", "def test_understand_err_col():\n    colnames = ['a', 'a_err', 'b', 'b_perr', 'b_nerr']\n    serr, terr = _understand_err_col(colnames)\n    assert np.allclose(serr, [1])\n    assert np.allclose(terr, [2])\n    \n    colnames = ['a', 'a_nerr']\n    with pytest.raises(ValueError) as exc:\n        serr, terr = _understand_err_col(colnames)\n    assert \"Missing positive error\" in str(exc.value)\n    \n    colnames = ['a', 'a_perr']\n    with pytest.raises(ValueError) as exc:\n        serr, terr = _understand_err_col(colnames)\n    assert \"Missing negative error\" in str(exc.value)\n    \n    colnames = ['a', 'b', 'c']\n    serr, terr = _understand_err_col(colnames)\n    assert np.allclose(serr, [])\n    assert np.allclose(terr, [])\n    \n    colnames = ['a_err']\n    serr, terr = _understand_err_col(colnames)\n    assert np.allclose(serr, [0])\n    assert np.allclose(terr, [])\n", "def test_read_table_qdp_with_errors(tmp_path):\n    example_qdp = \"\"\"\n    ! Initial comment line 1\n    ! Initial comment line 2\n    READ TERR 1\n    READ SERR 3\n    ! Table 0 comment\n    !a a(pos) a(neg) b c ce d\n    53000.5   0.25  -0.5   1  1.5  3.5 2\n    54000.5   1.25  -1.5   2  2.5  4.5 3\n    NO NO NO NO NO\n    ! Table 1 comment\n    !a a(pos) a(neg) b c ce d\n    54000.5   2.25  -2.5   NO  3.5  5.5 5\n    55000.5   3.25  -3.5   4  4.5  6.5 nan\n    \"\"\"\n\n    path = str(tmp_path / \"test.qdp\")\n    with open(path, \"w\") as fp:\n        print(example_qdp, file=fp)\n\n    table = _read_table_qdp(path, names=[\"a\", \"b\", \"c\", \"d\"], table_id=0)\n\n    assert \"initial_comments\" in table.meta\n    assert \"comments\" in table.meta\n\n    assert np.allclose(table[\"a\"][0], 53000.5)\n    assert np.allclose(table[\"a_perr\"][0], 0.25)\n    assert np.allclose(table[\"a_nerr\"][0], -0.5)\n    assert np.allclose(table[\"b\"][0], 1)\n    assert np.allclose(table[\"c\"][0], 1.5)\n    assert np.allclose(table[\"c_err\"][0], 3.5)\n    assert np.allclose(table[\"d\"][0], 2)\n\n    assert np.allclose(table[\"a\"][1], 54000.5)\n    assert np.allclose(table[\"a_perr\"][1], 1.25)\n    assert np.allclose(table[\"a_nerr\"][1], -1.5)\n    assert np.allclose(table[\"b\"][1], 2)\n    assert np.allclose(table[\"c\"][1], 2.5)\n    assert np.allclose(table", "def test_understand_err_col():\n    colnames1 = ['a', 'a_err', 'b', 'b_perr', 'b_nerr']\n    serr, terr = _understand_err_col(colnames1)\n    assert np.allclose(serr, [1])\n    assert np.allclose(terr, [2])\n\n    colnames2 = ['a', 'b', 'a_err', 'c', 'b_perr', 'b_nerr']\n    serr, terr = _understand_err_col(colnames2)\n    assert np.allclose(serr, [2])\n    assert np.allclose(terr, [4])\n\n    colnames3 = ['a', 'a_err']\n    serr, terr = _understand_err_col(colnames3)\n    assert np.allclose(serr, [1])\n    assert np.allclose(terr, [])\n\n    colnames4 = ['a', 'b', 'a_perr', 'a_nerr']\n    serr, terr = _understand_err_col(colnames4)\n    assert np.allclose(serr, [])\n    assert np.allclose(terr, [1])\n\n    with pytest.raises(ValueError) as exc:\n        _understand_err_col(['a', 'a_nerr'])\n    assert str(exc.value) == \"Missing positive error...\"\n\n    with pytest.raises(ValueError) as exc:\n        _understand_err_col(['a', 'a_perr'])\n    assert str(exc.value) == \"Missing negative error...\"\n", "def test_interpret_err_lines():\n    with pytest.raises(ValueError, match=\"Inconsistent number of input colnames\"):\n        _interpret_err_lines({\"serr\": [2], \"terr\": [1]}, 6, names=[\"a\", \"b\"])\n    with pytest.raises(ValueError, match=\"Inconsistent number of input colnames\"):\n        _interpret_err_lines({\"serr\": [1], \"terr\": [3]}, 6, names=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n\n    colnames = _interpret_err_lines({\"serr\": [2], \"terr\": [1]}, 7, names=[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\"])\n    assert colnames == [\"a\", \"a_perr\", \"a_nerr\", \"b_err\", \"c\", \"d\", \"e\"]\n\n    colnames = _interpret_err_lines({\"serr\": [1], \"terr\": [3]}, 7, names=[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\"])\n    assert colnames == [\"a_err\", \"b\", \"c\", \"d_perr\", \"d_nerr\", \"e\", \"f\"]\n\n    colnames = _interpret_err_lines(None, 2, names=[\"a\", \"b\"])\n    assert colnames == [\"a\", \"b\"]\n"], "sample_468": ["def test_render_context_pop(self):\n    test_context = RenderContext({\"fruit\": \"papaya\"})\n    test_context.push()\n    test_context[\"vegetable\"] = \"artichoke\"\n    with self.assertRaises(ContextPopException):\n        test_context.pop()\n        test_context.pop()\n", "    def test_render_context_push_state(self):\n        test_context = RenderContext({\"fruit\": \"papaya\"})\n\n        with test_context.push_state(Template(\"fruit\")) as state:\n            test_context[\"vegetable\"] = \"artichoke\"\n            self.assertEqual(list(test_context), [\"vegetable\"])\n\n            self.assertNotIn(\"fruit\", test_context)\n            with self.assertRaises(KeyError):\n                test_context[\"fruit\"]\n            self.assertIsNone(test_context.get(\"fruit\"))\n\n        self.assertEqual(list(test_context), [\"fruit\"])\n        self.assertIn(\"fruit\", test_context)\n        self.assertEqual(test_context[\"fruit\"], \"papaya\")\n", "def test_render_context_push_state(self):\n    \"\"\"\n    Test RenderContext.push_state() with a template.\n    \"\"\"\n    test_context = RenderContext({\"fruit\": \"papaya\"})\n    template = Template(\"\")\n    initial_template = test_context.template\n\n    with test_context.push_state(template):\n        test_context[\"vegetable\"] = \"artichoke\"\n        self.assertEqual(list(test_context), [\"vegetable\"])\n        self.assertEqual(test_context.template, template)\n\n    self.assertEqual(test_context.template, initial_template)\n    self.assertNotIn(\"vegetable\", test_context)\n    self.assertEqual(list(test_context), [\"fruit\"])\n", "def test_request_context_new_with_processors(self):\n    \"\"\"#29018 -- RequestContext.new() preserves context processors.\"\"\"\n    request = self.request_factory.get(\"/\")\n    context_processors = [\"django.template.context_processors.request\"]\n    original_context = RequestContext(request, processors=context_processors)\n    new_context = original_context.new()\n    self.assertEqual(new_context._processors, context_processors)\n", "def test_context_pop_exception_on_empty_context(self):\n    c = Context()\n    with self.assertRaises(ContextPopException):\n        c.pop()\n"], "sample_535": ["def test_table_positioning():\n    fig, axs = plt.subplots(3, 3, figsize=(10, 10))\n    positions = ['upper right', 'upper left', 'lower left',\n                 'lower right', 'center left', 'center right',\n                 'lower center', 'upper center', 'center']\n    for ax, position in zip(axs.flat, positions):\n        ax.axis('off')\n        ax.table(cellText=[['1', '2'], ['3', '4']], loc=position)\n", "def test_table_loc():\n    # Check all the different locations\n    locations = ['top left', 'top right', 'bottom left', 'bottom right',\n                 'top', 'bottom', 'left', 'right', 'center', 'center left',\n                 'center right', 'lower center', 'upper center', 'lower right',\n                 'upper right', 'lower left', 'upper left']\n    fig, axs = plt.subplots(nrows=len(locations), figsize=(4, len(locations)+1))\n    for ax, loc in zip(axs, locations):\n        ax.table(cellText=[['1', '2'], ['3', '4']], loc=loc)\n        ax.axis('off')\n    plt.tight_layout()\n", "def test_table_set_fontsize(fig_test, fig_ref):\n    fig_test, ax_test = fig_test.subplots()\n    fig_ref, ax_ref = fig_ref.subplots()\n\n    cell_text = [[\"Hello\", \"World\"], [\"Foo\", \"Bar\"]]\n    table_test = ax_test.table(cellText=cell_text, loc='center')\n    table_test.auto_set_font_size(False)\n    table_test.set_fontsize(12)\n\n    table_ref = ax_ref.table(cellText=cell_text, loc='center')\n    table_ref.auto_set_font_size(False)\n    for cell in table_ref.get_celld().values():\n        cell.set_fontsize(12)\n\n    assert fig_test.savefig(\"_test_table_set_fontsize.png\") is None\n    assert fig_ref.savefig(\"_ref_table_set_fontsize.png\") is None\n", "def test_set_text_props():\n    fig, ax = plt.subplots()\n\n    table = ax.table(cellText=[['Test'], ['Text']], loc='center')\n\n    # Test setting text properties for all cells\n    for cell in table.get_celld().values():\n        cell.set_text_props(fontsize=14, fontweight='bold', color='red')\n\n    # Test setting text properties for a single cell\n    cell = table[0, 0]\n    cell.set_text_props(fontstyle='italic', fontfamily='monospace')\n    cell = table[1, 0]\n    cell.set_text_props(va='top', ha='center', rotation=45)\n\n    ax.axis('off')\n", "def test_text_rotation():\n    # Check that text rotation works in table\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n\n    cell_text = [[f\"{x}\" for x in range(4)] for _ in range(4)]\n    for i in range(4):\n        ax.table(cellText=cell_text,\n                 rowLabels=[f\"{x}\" for x in range(4)],\n                 colLabels=[f\"{x}\" for x in range(4)],\n                 loc='center',\n                 bbox=[0.1 + i * 0.2, 0.2, 0.1, 0.6],\n                 fontproperties={'rotation': i * 45},\n                 )\n    ax.axis('off')\n"], "sample_451": ["def test_replace_metacharacters(self):\n    self.assertEqual(replace_metacharacters(\"Hello, world!\"), \"Hello, world!\")\n    self.assertEqual(replace_metacharacters(\"Hello, (world)?\"), \"Hello, (world)\")\n    self.assertEqual(replace_metacharacters(\"Hello, *world*\"), \"Hello, *world\")\n    self.assertEqual(replace_metacharacters(\"Hello, +world+\"), \"Hello, +world\")\n    self.assertEqual(replace_metacharacters(\"Hello, ^world^\"), \"Hello, ^world\")\n", "def test_replace_metacharacters(self):\n    self.assertEqual(replace_metacharacters(r\"^hello$\"), r\"^hello$\")\n    self.assertEqual(replace_metacharacters(r\"\\^hello\\$\"), r\"\\^hello\\$\")\n    self.assertEqual(replace_metacharacters(r\"\\^hel\\*lo\\$\"), r\"\\^hel\\*lo\\$\")\n    self.assertEqual(replace_metacharacters(r\"\\^hel\\\\*lo\\$\"), r\"\\^hel\\*lo\\$\")\n", "    def test_replace_metacharacters(self):\n        \"\"\"\n        Test that unescaped metacharacters are correctly replaced.\n        \"\"\"\n        pattern = \"Hello? World*+\"\n        expected = \"Hello World*+\"\n        self.assertEqual(replace_metacharacters(pattern), expected)\n", "def test_replace_metacharacters(self):\n    self.assertEqual(replace_metacharacters(\"a*b+c?\"), \"a*b+c?\")\n    self.assertEqual(replace_metacharacters(\"\\\\*a+\\\\+b\\\\?c\\\\\\\\\"), \"*a+ b c\\\\\")\n    self.assertEqual(replace_metacharacters(\"\\\\\\\\*a+\\\\+b\\\\?c\\\\\\\\\"), \"\\\\*a+ b c\\\\\")\n    self.assertEqual(replace_metacharacters(\"a\\\\*b\\\\+c\\\\?\"), \"a\\\\*b c \")\n", "    def test_get_view_name(self):\n        from django.views.generic import View\n        from django.urls import path\n        from django.urls.resolvers import get_resolver\n\n        class TestView(View):\n            pass\n\n        resolver = get_resolver([path('test/', TestView.as_view())])\n\n        view_func = resolver.resolve('/test/')\n        self.assertEqual(get_view_name(view_func), \"django.views.generic.base.View\")\n\n            pass\n\n        view_func = resolver.resolve('/test/')\n        view_func.func = test_func\n        self.assertEqual(get_view_name(view_func), \"test_func\")\n\n        class TestView2(View):\n                cls.__module__ = 'mymodule'\n                super().__init_subclass__(**kwargs)\n\n        self.assertEqual(get_view_name(TestView2.as_view()), \"mymodule.TestView2\")\n"], "sample_1200": ["def test_UnitSystem_get_units_non_prefixed():\n    assert SI.get_units_non_prefixed() == set([meter, kilogram, second, ampere, kelvin, mole, candela])\n    us = SI.extend([], name=\"us\")\n    assert us.get_units_non_prefixed() == set([meter, kilogram, second, ampere, kelvin, mole, candela])\n    us = SI.extend([Quantity('new_unit')], name=\"us\")\n    assert us.get_units_non_prefixed() == set([meter, kilogram, second, ampere, kelvin, mole, candela, Quantity('new_unit')])\n    us = SI.extend([], units=(Quantity('new_unit'),), name=\"us\")\n    assert us.get_units_non_prefixed() == set([meter, kilogram, second, ampere, kelvin, mole, candela, Quantity('new_unit')])\n", "def test_unit_system_extension():\n    new_system = SI.extend((kilo*second,), name=\"New SI\", description=\"SI with a new base unit\")\n    assert new_system.name == \"New SI\"\n    assert new_system.descr == \"SI with a new base unit\"\n    assert new_system._base_units == (second,) + SI._base_units\n    assert new_system._units == (kilo*second,) + SI._units\n", "def test_get_units_non_prefixed():\n    # Test the method with the SI unit system\n    si_system = UnitSystem.get_unit_system(\"SI\")\n    non_prefixed_units = si_system.get_units_non_prefixed()\n    assert all(not unit.is_prefixed for unit in non_prefixed_units)\n\n    # Create a custom unit system with prefixed units\n    custom_system = UnitSystem(base_units=(meter, kilogram, second), units=(centimeter, kilogram, joule))\n    custom_non_prefixed_units = custom_system.get_units_non_prefixed()\n    assert all(not unit.is_prefixed for unit in custom_non_prefixed_units)\n\n    # Create a custom unit system with only prefixed units\n    custom_system_prefixed = UnitSystem(base_units=(kilometer, kilogram, second), units=(centimeter, joule))\n    custom_non_prefixed_units_prefixed = custom_system_prefixed.get_units_non_prefixed()\n    assert len(custom_non_prefixed_units_prefixed) == 0\n", "def test_derived_units():\n    # create a unit system\n    us = UnitSystem([meter, second])\n    \n    # create derived units\n    derived_units = {\n        energy: joule,\n        force: newton,\n        pressure: pascal,\n        velocity: meter/second,\n    }\n    \n    # set derived units\n    us._derived_units = derived_units\n    \n    # test derived units\n    assert us.derived_units == derived_units\n    assert us.get_quantity_dimension(joule) == energy\n    assert us.get_quantity_scale_factor(joule) == 1\n    assert us.get_quantity_dimension(newton) == force\n    assert us.get_quantity_scale_factor(newton) == 1\n    assert us.get_quantity_dimension(pascal) == pressure\n    assert us.get_quantity_scale_factor(pascal) == 1\n    assert us.get_quantity_dimension(meter/second) == velocity\n    assert us.get_quantity_scale_factor(meter/second) == 1\n", "def test_UnitSystem_extend():\n    u1 = Quantity(\"u1\")\n    u2 = Quantity(\"u2\")\n    u1.set_global_relative_scale_factor(S(10), meter)\n    u2.set_global_relative_scale_factor(S(5), second)\n\n    us = UnitSystem((u1, u2))\n    us1 = us.extend((u2, ), name=\"new_system\")\n\n    assert us1._base_units == (u1, u2)\n    assert us1._units == (u1, u2)\n    assert us1.name == \"new_system\"\n\n    us2 = us.extend((), name=\"another_system\", description=\"test system\")\n\n    assert us2._base_units == (u1, u2)\n    assert us2._units == (u1, u2)\n    assert us2.name == \"another_system\"\n    assert us2.descr == \"test system\"\n\n    us3 = us.extend((u1, ), derived_units={length: Quantity(\"new_unit\")})\n\n    assert us3._base_units == (u1, u2, u1)\n    assert us3._units == (u1, u2, u1)\n    assert us3.derived_units == {length: Quantity(\"new_unit\")}\n"], "sample_931": ["def test_pyvariable_options(app):\n    text = (\".. py:variable:: var\\n\"\n            \"   :type: int\\n\"\n            \"   :value: 1\\n\"\n            \"   :final:\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"var\"],\n                                                    [desc_annotation, (\": \",\n                                                                       [pending_xref, \"int\"])],\n                                                    [desc_annotation, \" = 1\"])],\n                                  [desc_content, ()])]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"data\",\n                domain=\"py\", objtype=\"data\", noindex=False)\n    assert_node(doctree[1][0][1][1], pending_xref, **{\"py:module\": None})\n    assert_node(doctree[1][0][1][3], pending_xref, **{\"py:module\": None})\n    assert 'var' in domain.objects\n    assert domain.objects['var'] == ('index', 'var', 'data')\n", "def test_python_domain_clear_doc(app):\n    text = (\".. py:function:: f1()\\n\"\n            \".. py:function:: f2()\\n\")\n    restructuredtext.parse(app, text)\n    domain = app.env.get_domain('py')\n    assert len(domain.objects) == 2\n\n    domain.clear_doc(app.env.docnames[0])\n    assert len(domain.objects) == 1\n", "def test_python_domain_clear_doc(app):\n    text = (\".. py:function:: f()\\n\"\n            \".. py:module:: example\\n\"\n            \".. py:function:: g()\\n\")\n    domain = app.env.get_domain('py')\n    restructuredtext.parse(app, text)\n    assert 'f' in domain.objects\n    assert 'example.g' in domain.objects\n    assert 'example' in domain.modules\n\n    domain.clear_doc('index')\n    assert 'f' not in domain.objects\n    assert 'example.g' in domain.objects\n    assert 'example' in domain.modules\n", "def test_pyclass_inheritance(app, status, warning):\n    text = (\".. py:class:: Parent\\n\"\n            \"\\n\"\n            \"   .. py:method:: method\\n\"\n            \"\\n\"\n            \".. py:class:: Child(Parent)\\n\"\n            \"\\n\"\n            \"   .. py:method:: method\\n\"\n            \"   .. py:method:: child_method\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Parent\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])],\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Child\"],\n                                                    [desc_addname, \"(Parent)\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc,\n                                                  addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'method() (Parent method)', 'Parent.method', '', None)])\n    assert_node(doctree[3][1][0], addnodes.index,\n                entries=[('single', 'method() (Child method)', 'Child.method', '', None)])\n    assert_node(doctree[3][1][2], addnodes.index,\n                entries=[('single', 'child_method() (Child method)', 'Child.child_method', '', None)])\n    assert doctree[3][0][1][1].astext() == \"(Parent)\"  # check inheritance\n", "def test_python_domain_merge_domaindata(app):\n    text1 = (\".. py:module:: mod1\\n\"\n             \".. py:function:: func1\\n\")\n    text2 = (\".. py:module:: mod1\\n\"\n             \".. py:function:: func2\\n\")\n    restructuredtext.parse(app, text1, 'file1')\n    restructuredtext.parse(app, text2, 'file2')\n\n    domain = app.env.get_domain('py')\n    domain.merge_domaindata(['file1', 'file2'], {\n        'objects': {\n            'mod1.func1': ('file1', 'mod1.func1', 'function'),\n            'mod1.func2': ('file2', 'mod1.func2', 'function'),\n        },\n        'modules': {\n            'mod1': ('file1', 'module-mod1', 'synopsis', '', False),\n        }\n    })\n\n    assert 'mod1.func1' in domain.objects\n    assert 'mod1.func2' in domain.objects\n    assert 'mod1' in domain.modules\n    assert domain.objects['mod1.func1'] == ('file1', 'mod1.func1', 'function')\n    assert domain.objects['mod1.func2'] == ('file2', 'mod1.func2', 'function')\n    assert domain.modules['mod1'] == ('file1', 'module-mod1', 'synopsis', '', False)\n"], "sample_245": ["    def test_invalid_domain(self):\n        with self.assertRaisesMessage(CommandError, \"currently makemessages only supports domains 'django' and 'djangojs'\"):\n            management.call_command('makemessages', locale=[LOCALE], domain='invalid', verbosity=0)\n", "    def test_invalid_extension(self):\n        out, _ = self._run_makemessages(extensions=['invalid_extension'])\n        self.assertIn(\"examining files with the extensions: invalid_extension\", out)\n", "    def test_empty_file(self):\n        # An empty file should not raise any errors.\n        open('empty_file.py', 'a').close()\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n        os.remove('empty_file.py')\n        self.assertTrue(os.path.exists(self.PO_FILE))\n", "    def test_build_file_cleanup(self):\n        tmp_file = tempfile.NamedTemporaryFile(suffix='.py')\n        tmp_file.write(b'content')\n        tmp_file.flush()\n\n        build_file = self.build_file_class(self, 'django', self.translatable_file_class('.', tmp_file.name, '/tmp'))\n        build_file.preprocess()\n        build_file.cleanup()\n\n        self.assertFalse(os.path.exists(tmp_file.name + '.c'))\n", "    def test_preprocess_non_templatized_file(self):\n        cmd = mock.Mock(spec=MakeMessagesCommand)\n        build_file = MakeMessagesCommand.build_file_class(cmd, 'django', mock.Mock(file='test.py', dirpath='/path/to/file'))\n        build_file.preprocess()\n        self.assertFalse(os.path.exists(os.path.join(build_file.translatable.dirpath, 'test.c')))\n"], "sample_296": ["def test_update_cookie(self):\n    \"\"\"\n    The cookie's 'max-age' attribute is properly set when updating.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n    storage.add(constants.INFO, 'test')\n    storage.update(response)\n    self.assertEqual(response.cookies['messages']['max-age'], None)\n\n    # Delete the cookie by storing an empty value.\n    storage = self.get_storage()\n    response = self.get_response()\n    for m in storage:\n        pass  # Iterate through the storage to simulate consumption of messages.\n    storage.update(response)\n    self.assertEqual(response.cookies['messages']['max-age'], 0)\n", "def test_process_messages_recursive(self):\n    \"\"\"\n    Ensure MessageDecoder.process_messages correctly processes nested messages.\n    \"\"\"\n    storage = self.storage_class(self.get_request())\n    messages = [\n        Message(constants.INFO, 'message %s'),\n        {'message_list': [Message(constants.INFO, 'message %s'), Message(constants.INFO, 'message %s')]},\n        [Message(constants.INFO, 'message %s'), Message(constants.INFO, 'message %s')],\n        {'nested_list': [Message(constants.INFO, 'message %s'), {'message_list': [Message(constants.INFO, 'message %s')]}]}\n    ]\n    encoded_messages = storage._encode(messages)\n    decoded_messages = storage._decode(encoded_messages)\n    self.assertEqual(len(decoded_messages), 4)\n    self.assertIsInstance(decoded_messages[0], Message)\n    self.assertIsInstance(decoded_messages[1]['message_list'][0], Message)\n    self.assertIsInstance(decoded_messages[1]['message_list'][1], Message)\n    self.assertIsInstance(decoded_messages[2][0], Message)\n    self.assertIsInstance(decoded_messages[2][1], Message)\n    self.assertIsInstance(decoded_messages[3]['nested_list'][0], Message)\n    self.assertIsInstance(decoded_messages[3]['nested_list'][1]['message_list'][0], Message)\n", "def test_not_finished_sentinel(self):\n    \"\"\"\n    Test that the 'not_finished' sentinel value is correctly handled.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # Add more messages than can fit in the cookie.\n    msg_size = int((CookieStorage.max_cookie_size - 54) / 4.5 - 37)\n    random.seed(42)\n    messages = [get_random_string(msg_size) for _ in range(5)]\n    for msg in messages:\n        storage.add(constants.INFO, msg)\n\n    # Store messages in the cookie.\n    storage.update(response)\n\n    # Verify that the 'not_finished' sentinel is added to the cookie.\n    data = storage._decode(response.cookies['messages'].value)\n    self.assertIn(CookieStorage.not_finished, data)\n\n    # Verify that the 'not_finished' sentinel is removed when retrieving messages.\n    retrieved_messages = list(storage)\n    self.assertNotIn(CookieStorage.not_finished, retrieved_messages)\n\n    # Verify that the messages that didn't fit in the cookie are returned.\n    self.assertEqual(len(retrieved_messages), 4)\n\n    # Verify that the messages are correctly ordered.\n    self.assertEqual(retrieved_messages, messages[:4])\n", "def test_not_finished_sentinel(self):\n    \"\"\"\n    The not_finished sentinel is used to indicate when not all messages\n    were stored in the cookie due to size constraints.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # Create a list of messages that will be split due to size constraints.\n    messages = ['a'] * (CookieStorage.max_cookie_size // 5)\n\n    # Add an extra message to trigger the not_finished sentinel.\n    messages.append('extra message')\n\n    # Store the messages and get the ones that were not stored.\n    unstored_messages = storage._store(messages, response, remove_oldest=False)\n    self.assertEqual(len(unstored_messages), 1)\n\n    # Check that the not_finished sentinel was added to the stored messages.\n    stored_messages = storage._decode(response.cookies['messages'].value)\n    self.assertEqual(stored_messages[-1], CookieStorage.not_finished)\n\n    # Remove the not_finished sentinel and check that the stored messages are correct.\n    stored_messages.pop()\n    self.assertEqual(len(stored_messages), len(messages) - 1)\n\n    # Check that the not stored message is the correct one.\n    self.assertEqual(unstored_messages[0].message, messages[-1])\n", "def test_not_finished_sentinel(self):\n    \"\"\"\n    The 'not_finished' sentinel value is correctly handled by the storage.\n    \"\"\"\n    storage = self.get_storage()\n    messages = [Message(constants.INFO, 'message1'), Message(constants.INFO, 'message2')]\n    set_cookie_data(storage, messages + [CookieStorage.not_finished])\n    retrieved_messages, all_retrieved = storage._get()\n    self.assertEqual(retrieved_messages, messages)\n    self.assertFalse(all_retrieved)\n"], "sample_683": ["def test_capturing_and_logging_fundamentals_on_teardown(testdir):\n    # here we check a fundamental feature\n    p = testdir.makepyfile(\n        \"\"\"\n        import sys, os\n        import py, logging\n        from _pytest import capture\n        cap = capture.MultiCapture(\n            in_=None,\n            out=None,\n            err=capture.SysCapture(2),\n        )\n        cap.start_capturing()\n\n        logging.warning(\"hello1\")\n        outerr = cap.readouterr()\n        print(\"suspend, captured %%s\" %%(outerr,))\n        logging.warning(\"hello2\")\n\n        cap.pop_outerr_to_orig()\n        logging.warning(\"hello3\")\n\n        outerr = cap.readouterr()\n        print(\"suspend2, captured %%s\" %% (outerr,))\n            logging.warning(\"teardown\")\n        \"\"\"\n    )\n    result = testdir.runpython(p)\n    result.stdout.fnmatch_lines(\n        \"\"\"\n        suspend, captured*hello1*\n        suspend2, captured*WARNING:root:hello3*\n    \"\"\"\n    )\n    result.stderr.fnmatch_lines(\n        \"\"\"\n        WARNING:root:hello2\n        WARNING:root:teardown\n    \"\"\"\n    )\n    assert \"atexit\" not in result.stderr.str()\n", "def test_readouterr_after_stop_capturing(testdir):\n    testdir.makepyfile(\n        \"\"\"\\\n            print(\"hello\")\n            capfd.stop_capturing()\n            captured = capfd.readouterr()\n            assert captured.out == \"hello\\\\n\"\n            assert captured.err == \"\"\n        \"\"\"\n    )\n    result = testdir.runpytest_subprocess()\n    result.assert_outcomes(passed=1)\n", "def test_fdcapture_invalid_fd_test_failure(testdir):\n    testdir.makepyfile(\n        \"\"\"\\\n        import os\n\n            os.close(2)\n            with pytest.raises(OSError):\n                os.write(2, b\"error\")\n    \"\"\"\n    )\n    result = testdir.runpytest_subprocess(\"--capture=fd\")\n    assert result.ret == 0\n    assert result.parseoutcomes()[\"passed\"] == 1\n", "def test_readouterr_multiple_times(capsysbinary):\n    \"\"\"Test that readouterr() can be called multiple times without crashing (#2995).\"\"\"\n    print(\"hello\")\n    out, err = capsysbinary.readouterr()\n    out, err = capsysbinary.readouterr()\n    out, err = capsysbinary.readouterr()\n    assert out == b\"hello\\n\"\n    assert err == b\"\"\n", "def test_suspend_and_resume_multiple_times(testdir):\n    testdir.makepyfile(\n        \"\"\"\\\n            import sys\n            cap = None\n            with pytest.raises(ValueError):\n                cap.suspend()\n            with pytest.raises(ValueError):\n                cap.resume()\n            cap = pytest.capsys()\n            cap.suspend()\n            cap.suspend()\n            cap.resume()\n            cap.resume()\n            cap.resume()\n            assert len(cap.readouterr()[0]) == 0\n            print(\"Hello, world!\")\n            cap.suspend()\n            cap.resume()\n            assert cap.readouterr()[0] == \"Hello, world!\\n\"\n        \"\"\"\n    )\n    reprec = testdir.inline_run()\n    reprec.assertoutcome(passed=1)\n"], "sample_625": ["def test_cross_multidim() -> None:\n    # test multiple vector cross-products. Note that the direction of the\n    # cross product vector is defined by the right-hand rule:\n    a = xr.DataArray(\n        [[1, 2, 3], [4, 5, 6]],\n        dims=(\"time\", \"cartesian\"),\n        coords=dict(\n            time=([\"time\"], [0, 1]),\n            cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"]),\n        ),\n    )\n    b = xr.DataArray(\n        [[4, 5, 6], [1, 2, 3]],\n        dims=(\"time\", \"cartesian\"),\n        coords=dict(\n            time=([\"time\"], [0, 1]),\n            cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"]),\n        ),\n    )\n    expected = xr.DataArray(\n        [[-3, 6, -3], [3, -6, 3]],\n        dims=(\"time\", \"cartesian\"),\n        coords=dict(\n            time=([\"time\"], [0, 1]),\n            cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"]),\n        ),\n    )\n    actual = xr.cross(a, b, dim=\"cartesian\")\n    assert_identical(expected, actual)\n", "def test_apply_dask_parallelized_vectorized_func() -> None:\n    # regression test for GH4015\n    import dask.array as da\n\n    data_array = xr.DataArray([[0, 1, 2], [1, 2, 3]], dims=(\"x\", \"y\"))\n    expected = xr.DataArray([1, 2], dims=[\"x\"])\n\n        return np.median(x, axis=-1)\n\n    actual = apply_ufunc(\n        median_along_y,\n        data_array.chunk({\"x\": 1}),\n        input_core_dims=[[\"y\"]],\n        output_core_dims=[[]],\n        vectorize=True,\n        dask=\"parallelized\",\n        output_dtypes=[float],\n    )\n    assert_identical(expected, actual)\n", "def test_cross_two_vectors_of_different_size_with_coords_undefined_on_one_vector() -> None:\n    # Vector cross-product with 3 dimensions:\n\n    a = xr.DataArray([1, 2], dims=[\"cartesian\"], coords={\"cartesian\": [\"x\", \"y\"]})\n    b = xr.DataArray([4, 5, 6], dims=[\"cartesian\"])\n\n    expected = xr.DataArray([12, -6, -3], dims=[\"cartesian\"], coords={\"cartesian\": [\"x\", \"y\", \"z\"]})\n    actual = xr.cross(a, b, dim=\"cartesian\")\n    assert_identical(expected, actual)\n", "def test_cross_dask_broadcasting() -> None:\n    # GH 4483\n    # Test broadcasting when using cross product\n\n    a = xr.DataArray(\n        np.array([1, 2, 3]),\n        dims=[\"cartesian\"],\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])),\n    )\n\n    b = xr.DataArray(\n        np.array([4, 5, 6]),\n        dims=[\"cartesian\"],\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])),\n    )\n\n    a = a.chunk({\"cartesian\": 2})\n    b = b.chunk({\"cartesian\": 2})\n\n    c = xr.cross(a, b, dim=\"cartesian\")\n    expected = np.cross(a, b, axis=-1)\n\n    xr.testing.assert_duckarray_allclose(c, expected)\n", "def test_cross_variable(use_dask: bool) -> None:\n    # Test cross product on variables\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"test for dask.\")\n\n    # Test with arrays of size 3\n    a = xr.Variable((\"x\",), np.arange(3))\n    b = xr.Variable((\"x\",), np.arange(3))\n    expected = np.cross(a, b)\n    actual = xr.cross(a, b, dim=\"x\")\n    assert_array_equal(actual, expected)\n\n    # Test with arrays of size 2\n    a = xr.Variable((\"x\",), np.arange(2))\n    b = xr.Variable((\"x\",), np.arange(3))\n    expected = np.cross(a, b, axis=0)\n    actual = xr.cross(a, b, dim=\"x\")\n    assert_array_equal(actual, expected)\n\n    # Test with different variable sizes\n    a = xr.Variable((\"x\",), np.arange(2))\n    b = xr.Variable((\"x\",), np.arange(3))\n    expected = np.cross(np.append(a, 0), b, axis=0)\n    actual = xr.cross(a, b, dim=\"x\")\n    assert_array_equal(actual, expected)\n\n    # Test with dask arrays\n    if use_dask:\n        a = a.chunk()\n        b = b.chunk()\n        actual = xr.cross(a, b, dim=\"x\")\n        assert_array_equal(actual.compute(), expected)\n\n    # Test with arrays of size 1\n    a = xr.Variable((\"x\",), [1])\n    b = xr.Variable((\"x\",), [2, 3, 4])\n    expected = np.cross(np.array([1, 0, 0]), b, axis=0)\n    actual = xr.cross(a, b, dim=\"x\")\n    assert_array_equal(actual, expected)\n\n    # Test with dask arrays\n    if use_dask:\n        a = a.chunk()\n        b = b.chunk()\n        actual = xr.cross(a, b, dim=\"x\")\n        assert_array_equal(actual.compute(), expected)\n\n    # Test with 2D array\n    a = xr.Variable((\"x\", \"y\"), [[1, 2], [3, 4]])\n    b = xr.Variable((\"x\", \"y"], "sample_445": ["def test_reversed_with_depth(self):\n    \"\"\"\n    Test timesince with reversed=True and depth > 1.\n    \"\"\"\n    t = self.t\n    tests = [\n        (t + self.oneyear + self.onemonth + self.oneweek + self.oneday + self.onehour, 2, \"1\\xa0year, 1\\xa0month\"),\n        (t + self.oneyear + self.onemonth + self.oneweek + self.oneday + self.onehour, 3, \"1\\xa0year, 1\\xa0month, 1\\xa0week\"),\n        (t + self.oneyear + self.onemonth + self.oneweek + self.oneday + self.onehour, 4, \"1\\xa0year, 1\\xa0month, 1\\xa0week, 1\\xa0day\"),\n        (t + self.oneyear + self.onemonth + self.oneweek + self.oneday + self.onehour, 5, \"1\\xa0year, 1\\xa0month, 1\\xa0week, 1\\xa0day, 1\\xa0hour\"),\n        (t + self.oneyear + self.onemonth + self.oneweek + self.oneday + self.onehour, 6, \"1\\xa0year, 1\\xa0month, 1\\xa0week, 1\\xa0day, 1\\xa0hour\"),\n    ]\n    for value, depth, expected in tests:\n        with self.subTest():\n            self.assertEqual(timesince(t, value, reversed=True, depth=depth), expected)\n            self.assertEqual(timeuntil(t, value, depth=depth), expected)\n", "def test_timeuntil(self):\n    \"\"\"\n    Test the timeuntil function which is an alias for timesince with reversed=True.\n    \"\"\"\n    t = self.t\n    now = datetime.datetime(2007, 8, 14, 13, 46, 0)\n    self.assertEqual(timeuntil(t, now), \"0\\xa0minutes\")\n    self.assertEqual(timeuntil(t + self.oneminute, now), \"1\\xa0minute\")\n    self.assertEqual(timeuntil(t + self.onehour, now), \"1\\xa0hour\")\n    self.assertEqual(timeuntil(t + self.oneday, now), \"1\\xa0day\")\n    self.assertEqual(timeuntil(t + self.oneweek, now), \"1\\xa0week\")\n    self.assertEqual(timeuntil(t + self.onemonth, now), \"1\\xa0month\")\n    self.assertEqual(timeuntil(t + self.oneyear, now), \"1\\xa0year\")\n    self.assertEqual(timeuntil(t + 2 * self.oneday + 6 * self.onehour, now), \"2\\xa0days, 6\\xa0hours\")\n    self.assertEqual(timeuntil(t + 2 * self.oneweek + 2 * self.oneday, now), \"2\\xa0weeks, 2\\xa0days\")\n", "def test_timesince_future_datetime_with_microsecond(self):\n    \"\"\"Test timesince with a future datetime object with a microsecond value.\"\"\"\n    future = self.t + self.onemicrosecond\n    future_with_microsecond = future.replace(microsecond=123456)\n    self.assertEqual(timesince(future_with_microsecond), \"0\\xa0minutes\")\n", "    def test_month_roll_over(self):\n        \"\"\"Months should roll over correctly.\"\"\"\n        tests = [\n            (datetime.datetime(2022, 1, 31), datetime.datetime(2022, 3, 1), \"2 months\"),\n            (datetime.datetime(2022, 1, 31), datetime.datetime(2022, 3, 2), \"2 months, 1 day\"),\n            (datetime.datetime(2022, 1, 31), datetime.datetime(2022, 3, 3), \"2 months, 2 days\"),\n            (datetime.datetime(2022, 1, 31), datetime.datetime(2022, 3, 28), \"2 months, 27 days\"),\n            (datetime.datetime(2022, 1, 31), datetime.datetime(2022, 3, 29), \"2 months, 28 days\"),\n            (datetime.datetime(2022, 1, 31), datetime.datetime(2022, 3, 30), \"2 months, 29 days\"),\n            (datetime.datetime(2022, 1, 31), datetime.datetime(2022, 3, 31), \"3 months\"),\n        ]\n        for value, expected_value, expected_result in tests:\n            with self.subTest():\n                self.assertEqual(timesince(value, expected_value), expected_result)\n                self.assertEqual(timeuntil(expected_value, value), expected_result)\n", "def test_non_integer_depth(self):\n    \"\"\"Test that timesince raises an error with a non-integer depth.\"\"\"\n    msg = \"depth must be an integer.\"\n    with self.assertRaisesMessage(TypeError, msg):\n        timesince(self.t, self.t, depth=2.5)\n"], "sample_871": ["def test_silhouette_reduce_sparse():\n    \"\"\"Check for sparse input to private method `_silhouette_reduce`.\"\"\"\n    X = np.array([[0.2, 0.1, 0.1, 0.2, 0.1, 1.6, 0.2, 0.1]], dtype=np.float32).T\n    pdist_dense = pairwise_distances(X)\n    pdist_sparse = csr_matrix(pdist_dense)\n    y = [0, 0, 0, 0, 1, 1, 1, 1]\n    label_freqs = np.bincount(y)\n    intra_clust_dists, inter_clust_dists = _silhouette_reduce(\n        pdist_sparse, start=0, labels=y, label_freqs=label_freqs\n    )\n    intra_clust_dists_dense, inter_clust_dists_dense = _silhouette_reduce(\n        pdist_dense, start=0, labels=y, label_freqs=label_freqs\n    )\n    assert_allclose(intra_clust_dists, intra_clust_dists_dense)\n    assert_allclose(inter_clust_dists, inter_clust_dists_dense)\n", "def test_calinski_harabasz_score_random_state():\n    \"\"\"Test the Calinski-Harabasz score with a random state.\"\"\"\n    np.random.seed(0)\n    X = np.random.rand(100, 5)\n    labels = np.random.randint(0, 5, size=100)\n    score1 = calinski_harabasz_score(X, labels)\n    score2 = calinski_harabasz_score(X, labels)\n    assert score1 != score2\n\n    np.random.seed(0)\n    X = np.random.rand(100, 5)\n    labels = np.random.randint(0, 5, size=100)\n    score3 = calinski_harabasz_score(X, labels)\n    assert score1 == score3\n", "def test_silhouette_reduce_non_csr_sparse():\n    \"\"\"Check for non-CSR input to private method `_silhouette_reduce`.\"\"\"\n    X = np.array([[0.2, 0.1, 0.1, 0.2, 0.1, 1.6, 0.2, 0.1]], dtype=np.float32).T\n    pdist_dense = pairwise_distances(X)\n    pdist_csr = csr_matrix(pdist_dense)\n    pdist_csc = csc_matrix(pdist_dense)\n    pdist_dok = dok_matrix(pdist_dense)\n    pdist_lil = lil_matrix(pdist_dense)\n    y = [0, 0, 0, 0, 1, 1, 1, 1]\n    label_freqs = np.bincount(y)\n    \n    # Check CSR matrix is accepted\n    _silhouette_reduce(pdist_csr, start=0, labels=y, label_freqs=label_freqs)\n    \n    # Check other sparse matrices raise an error\n    for to_sparse in [csc_matrix, dok_matrix, lil_matrix]:\n        pdist_sparse = to_sparse(pdist_dense)\n        assert issparse(pdist_sparse)\n        with pytest.raises(\n            TypeError,\n            match=\"Expected CSR matrix. Please pass sparse matrix in CSR format.\",\n        ):\n            _silhouette_reduce(pdist_sparse, start=0, labels=y, label_freqs=label_freqs)\n", "def test_silhouette_reduce_csr_sparse():\n    \"\"\"Check for CSR input to private method `_silhouette_reduce`.\"\"\"\n    X = np.array([[0.2, 0.1, 0.1, 0.2, 0.1, 1.6, 0.2, 0.1]], dtype=np.float32).T\n    pdist_dense = pairwise_distances(X)\n    pdist_sparse = csr_matrix(pdist_dense)\n    y = [0, 0, 0, 0, 1, 1, 1, 1]\n    label_freqs = np.bincount(y)\n    intra_clust_dists, inter_clust_dists = _silhouette_reduce(\n        pdist_sparse, start=0, labels=y, label_freqs=label_freqs\n    )\n    assert intra_clust_dists.shape == (8,)\n    assert inter_clust_dists.shape == (8,)\n", "def test_silhouette_score_all_equal():\n    # Assert the value is 0. when all samples are equals\n    X = [[0, 0], [0, 0], [0, 0]]\n    labels = [0, 1, 2]\n    assert silhouette_score(X, labels) == pytest.approx(0.0)\n\n    # Assert the value is 0. when all samples have equal distances\n    X = [[0, 0], [1, 0], [0, 1]]\n    labels = [0, 1, 1]\n    assert silhouette_score(X, labels) == pytest.approx(0.0)\n\n    # Assert the value is 1. when all samples in the same cluster have zero distance\n    # and samples in different clusters have distance 1.\n    X = [[0, 0], [0, 0], [1, 1]]\n    labels = [0, 0, 1]\n    assert silhouette_score(X, labels) == pytest.approx(1.0)\n"], "sample_42": ["def test_pixel_scale_negative():\n    pix = 75*u.pix\n    asec = -30*u.arcsec\n\n    pixscale = 0.4*u.arcsec/u.pix\n    pixscale2 = 2.5*u.pix/u.arcsec\n\n    assert_quantity_allclose(pix.to(u.arcsec, u.pixel_scale(pixscale)), asec)\n    assert_quantity_allclose(pix.to(u.arcmin, u.pixel_scale(pixscale)), asec)\n\n    assert_quantity_allclose(pix.to(u.arcsec, u.pixel_scale(pixscale2)), asec)\n    assert_quantity_allclose(pix.to(u.arcmin, u.pixel_scale(pixscale2)), asec)\n\n    assert_quantity_allclose(asec.to(u.pix, u.pixel_scale(pixscale)), pix)\n    assert_quantity_allclose(asec.to(u.pix, u.pixel_scale(pixscale2)), pix)\n", "def test_dimensionless_angles_context():\n    with u.set_enabled_equivalencies(u.dimensionless_angles()):\n        phase = u.Quantity(1., u.cycle)\n        assert_allclose(np.exp(1j*phase), 1.)\n        Omega = u.cycle / (1.*u.minute)\n        assert_allclose(np.exp(1j*Omega*60.*u.second), 1.)\n        # ensure we can turn off equivalencies even within the scope\n        with pytest.raises(u.UnitsError):\n            phase.to(1, equivalencies=None)\n\n    with u.set_enabled_equivalencies(u.dimensionless_angles()):\n        u.cycle.to(u.radian)\n        eq_on = u.cycle.find_equivalent_units()\n        with pytest.raises(u.UnitsError):\n            u.cycle.to(u.radian, equivalencies=None)\n\n    # without equivalencies, we should find a smaller (sub)set\n    eq_off = u.cycle.find_equivalent_units()\n    assert all(eq in set(eq_on) for eq in eq_off)\n    assert set(eq_off) < set(eq_on)\n", "def test_doppler_optical_errors():\n    with pytest.raises(u.UnitsError):\n        u.doppler_optical(u.Quantity(1, u.m))\n\n    with pytest.raises(u.UnitsError):\n        u.doppler_optical(u.Quantity(1, u.m/s))\n", "def test_doppler_relativistic_edge_cases():\n    rest = 1.0501 * u.eV\n    shifted = 1.0503 * u.eV\n\n    # test low velocity\n    velo = shifted.to(u.km / u.s, equivalencies=u.doppler_relativistic(rest))\n    np.testing.assert_almost_equal(velo.value, 0, decimal=5)\n\n    # test high velocity\n    shifted = 0.0004342864612223407 * u.eV\n    velo = shifted.to(u.km / u.s, equivalencies=u.doppler_relativistic(rest))\n    np.testing.assert_almost_equal(velo.value, 0, decimal=5)\n", "def test_with_H0():\n    # check that we can convert to/from littleh\n    H0 = 70*u.km/u.s/u.Mpc\n    assert_quantity_allclose(H0.to(u.littleh/u.Mpc, u.with_H0(H0)), u.littleh/u.Mpc)\n\n    # check that using the default cosmology works\n    cosmo = cosmology.default_cosmology.get()\n    cosmodist = cosmo.H0 * u.Mpc/u.littleh\n    assert_quantity_allclose(cosmodist.to(u.Mpc, u.with_H0()), cosmo.H0*u.Mpc)\n\n    # check that we can convert to/from littleh with different powers\n    assert_quantity_allclose((H0**2).to(u.littleh**2/u.Mpc**2, u.with_H0(H0)),\n                            u.littleh**2/u.Mpc**2)\n\n    # check that we can convert to/from littleh with different units\n    assert_quantity_allclose(H0.to(u.littleh/u.kpc, u.with_H0(H0)),\n                            u.littleh/u.kpc)\n\n    # check that we can't convert to/from non-compatible units\n    with pytest.raises(u.UnitsError):\n        H0.to(u.kg/u.Mpc, u.with_H0(H0))\n\n    # check that littleh units are composed correctly\n    assert (u.Mpc/u.littleh).compose(units=(u.Mpc, u.dimensionless_unscaled),\n                                     equivalencies=u.with_H0(H0))[0] == H0\n"], "sample_475": ["    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = \"not an iterable\"\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_callable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = (lambda obj: obj.name,)\n\n        self.assertIsValid(TestModelAdmin, ValidationTestModel)\n", "    def test_check_dependencies(self):\n        class TestAdminSite(admin.AdminSite):\n            enable_nav_sidebar = True\n\n        errors = TestAdminSite().check(app_configs=[])\n        self.assertEqual(len(errors), 5)\n"], "sample_129": ["    def test_stringfilter_decorator(self):\n        @register.filter(is_safe=True)\n        @stringfilter\n            return value.upper()\n\n        self.assertEqual(test_filter('hello'), 'HELLO')\n        self.assertEqual(test_filter(123), '123')\n", "    def test_stringformat01(self):\n        output = self.engine.render_to_string('stringformat01', {\"a\": \"hello\"})\n        self.assertEqual(output, \"hello\")\n", "    def test_phone2numeric_filter(self):\n        tests = [\n            ('(123) 456-7890', '1234567890'),\n            ('123-456-7890', '1234567890'),\n            ('123 456 7890', '1234567890'),\n            ('123.456.7890', '1234567890'),\n            ('1234567890', '1234567890'),\n            ('not a phone number', 'not a phone number'),\n            ('', ''),\n        ]\n        for input_value, expected_output in tests:\n            with self.subTest(input_value=input_value):\n                output = self.engine.render_to_string('phone2numeric_filter', {\"value\": input_value})\n                self.assertEqual(output, expected_output)\n", "    def test_truncatechars(self):\n        self.assertEqual(truncatechars(\"Hello, world!\", 5), \"Hello\")\n", "    def test_underflow(self):\n        self.assertEqual(floatformat(1e-309), '0')\n"], "sample_507": ["def test_StrCategoryConverter_update():\n    data = ['a', 'd']\n    locs = [0, 1]\n\n    data_update = ['b', 'd', 'e']\n    unique_data = ['a', 'd', 'b', 'e']\n    updated_locs = [0, 1, 2, 3]\n\n    unit = cat.UnitData(data)\n    cc = cat.StrCategoryConverter()\n    np.testing.assert_allclose(cc.convert(data, unit, None), locs)\n\n    unit.update(data_update)\n    np.testing.assert_allclose(cc.convert(unique_data, unit, None), updated_locs)\n", "def test_str_is_convertible():\n    unit_data = cat.UnitData()\n    assert unit_data._str_is_convertible(\"42\") is True\n    assert unit_data._str_is_convertible(\"3.14\") is True\n    assert unit_data._str_is_convertible(\"2022-01-01\") is True\n    assert unit_data._str_is_convertible(\"invalid\") is False\n    assert unit_data._str_is_convertible(\"NaN\") is False\n    assert unit_data._str_is_convertible(np.nan) is False\n    assert unit_data._str_is_convertible(None) is False\n", "    def test_validate_unit_error(self):\n        class FakeUnit:\n            pass\n        with pytest.raises(ValueError):\n            cat.StrCategoryConverter._validate_unit(FakeUnit())\n", "    def test_convert_empty(self):\n        cc = cat.StrCategoryConverter()\n        unit = cat.UnitData()\n        with pytest.raises(ValueError):\n            cc.convert([], unit, FakeAxis(unit))\n", "def test_strcategoryformatter_edge_cases():\n    # Test that the formatter raises an IndexError when the input is out of range\n    formatter = cat.StrCategoryFormatter({\"a\": 0, \"b\": 1})\n    with pytest.raises(KeyError):\n        formatter.format_ticks([2])\n\n    # Test that the formatter returns an empty string when the input is not an integer\n    assert formatter.format_ticks([1.5])[0] == \"\"\n    assert formatter.format_ticks([\"a\"])[0] == \"\"\n\n    # Test that the formatter returns an empty string when the input is None\n    assert formatter.format_ticks([None])[0] == \"\"\n"], "sample_388": ["    def setUp(self):\n        self.backend = BaseBackend()\n", "    def setUp(self):\n        self.backend = ModelBackend()\n", "    def test_base_backend_authenticate(self):\n        backend = BaseBackend()\n        self.assertIsNone(backend.authenticate(None))\n", "    def setUp(self):\n        self.user = User.objects.create_user(username=\"testuser\", password=\"password\")\n", "    def test_inactive_user(self):\n        user = UserModel.objects.create(username=\"knownuser\", is_active=False)\n        user.set_password(\"foo\")\n        user.save()\n        self.client.login(username=\"knownuser\", password=\"foo\")\n        response = self.client.get(\"/login/\")\n        self.assertEqual(response.context[\"user\"].username, user.username)\n"], "sample_313": ["def test_reset_loaders_with_cached_loader(self):\n    mock_reset = mock.Mock()\n    with mock.patch('django.template.loaders.cached.Loader.reset', mock_reset):\n        autoreload.reset_loaders()\n        self.assertEqual(mock_reset.call_count, 1)\n", "def test_watch_for_template_changes_empty_template_dirs(self, mock_get_template_dirs):\n    mock_get_template_dirs.return_value = set()\n    mock_reloader = mock.MagicMock()\n    autoreload.watch_for_template_changes(mock_reloader)\n    mock_reloader.watch_dir.assert_not_called()\n", "def test_get_template_directories_ignores_django_templates(self, mock_is_django_path):\n    mock_is_django_path.return_value = True\n    self.assertSetEqual(\n        autoreload.get_template_directories(),\n        set()\n    )\n\n    mock_is_django_path.return_value = False\n    self.assertSetEqual(\n        autoreload.get_template_directories(),\n        {\n            ROOT / 'templates',\n            ROOT / 'templates_extra',\n        }\n    )\n", "def test_get_template_directories_multiple_backends(self, mock_all):\n    mock_backend1 = mock.MagicMock(spec=DjangoTemplates)\n    mock_backend2 = mock.MagicMock(spec=DjangoTemplates)\n    mock_loader1 = mock.MagicMock()\n    mock_loader2 = mock.MagicMock()\n    mock_loader1.get_dirs.return_value = ['dir1', 'dir2']\n    mock_loader2.get_dirs.return_value = ['dir3', 'dir4']\n    mock_backend1.engine.template_loaders = [mock_loader1, mock_loader2]\n    mock_all.return_value = [mock_backend1, mock_backend2]\n    self.assertSetEqual(\n        autoreload.get_template_directories(),\n        {\n            Path.cwd() / 'dir1',\n            Path.cwd() / 'dir2',\n            Path.cwd() / 'dir3',\n            Path.cwd() / 'dir4',\n        }\n    )\n", "    def test_reset_loaders_with_nested_loader(self, mock_reset):\n        autoreload.reset_loaders()\n        self.assertEqual(mock_reset.call_count, 2)\n"], "sample_188": ["    def setUpTestData(cls):\n        Time.objects.bulk_create([\n            Time(time=datetime.time(9, 0)),\n            Time(time=datetime.time(12, 0)),\n            Time(time=datetime.time(13, 0)),\n            Time(time=datetime.time(17, 0)),\n        ])\n", "    def setUp(self):\n        self.t1 = Time.objects.create(time=datetime.time(12, 0))\n        self.t2 = Time.objects.create(time=datetime.time(14, 0))\n        self.s1 = SimulationRun.objects.create(start=self.t1, end=self.t2, midpoint='12:00')\n        self.s2 = SimulationRun.objects.create(start=self.t1, end=self.t2, midpoint='13:00')\n        self.s3 = SimulationRun.objects.create(start=self.t1, end=self.t2, midpoint='14:00')\n", "    def setUpTestData(cls):\n        cls.t1 = Time.objects.create(time=datetime.time(12, 0))\n        cls.t2 = Time.objects.create(time=datetime.time(13, 0))\n        cls.t3 = Time.objects.create(time=datetime.time(14, 0))\n        cls.t4 = Time.objects.create(time=datetime.time(15, 0))\n        cls.sr1 = SimulationRun.objects.create(start=cls.t1, end=cls.t2, midpoint='12:30')\n        cls.sr2 = SimulationRun.objects.create(start=cls.t2, end=cls.t3, midpoint='13:30')\n        cls.sr3 = SimulationRun.objects.create(start=cls.t3, end=cls.t4, midpoint='14:30')\n", "    def test_addition_with_float_on_decimal_field(self):\n        # Test for ticket #29432\n        self.example_inc.num_chairs = Decimal('0.4')\n        self.example_inc.save()\n        self.assertQuerysetEqual(\n            Company.objects.filter(num_chairs=F('num_chairs') + 0.1),\n            ['<Company: Example Inc.>'],\n            lambda c: c.name,\n            ordered=False,\n        )\n", "    def setUpTestData(cls):\n        # Create 3 employees\n        cls.employee1 = Employee.objects.create(firstname='John', lastname='Doe', salary=50000)\n        cls.employee2 = Employee.objects.create(firstname='Jane', lastname='Doe', salary=60000)\n        cls.employee3 = Employee.objects.create(firstname='Jim', lastname='Smith', salary=70000)\n\n        # Create 2 companies, each having 2 employees\n        cls.company1 = Company.objects.create(name='Company 1', num_employees=3, num_chairs=5, ceo=cls.employee1)\n        cls.company2 = Company.objects.create(name='Company 2', num_employees=2, num_chairs=3, ceo=cls.employee2)\n        cls.company1.employees.add(cls.employee1)\n        cls.company1.employees.add(cls.employee2)\n        cls.company2.employees.add(cls.employee2)\n        cls.company2.employees.add(cls.employee3)\n"], "sample_301": ["    def test_raises_runtime_error(self):\n        try:\n            raise RuntimeError\n        except RuntimeError:\n            exc_info = sys.exc_info()\n\n        with mock.patch('django.utils.autoreload._exception', exc_info):\n            with self.assertRaisesMessage(RuntimeError, None):\n                autoreload.raise_last_exception()\n", "    def test_common_roots(self):\n        reloader = autoreload.WatchmanReloader()\n        paths = (\n            Path('/first/second'),\n            Path('/first/second/third'),\n            Path('/first/'),\n            Path('/root/first/'),\n        )\n        roots = reloader.watched_roots(paths)\n        self.assertCountEqual(roots, [Path('/first/'), Path('/root/first/')])\n", "    def test_common_roots_empty(self):\n        self.assertEqual(autoreload.common_roots(()), ())\n", "    def test_file_changed_signal_called_once(self):\n        filename = self.temporary_file('test.py')\n        filename.touch()\n\n        with extend_sys_path(str(filename.parent)):\n            with mock.patch('django.utils.autoreload.trigger_reload') as mocked_trigger:\n                autoreload.file_changed.send(sender=None, file_path=filename)\n                self.assertEqual(mocked_trigger.call_count, 1)\n                self.assertCountEqual(mocked_trigger.call_args[0], [filename])\n", "    def setUp(self):\n        self.reloader = autoreload.WatchmanReloader()\n        self.root = Path(tempfile.mkdtemp())\n"], "sample_697": ["def test_tmp_path_factory_fixes_up_world_readable_permissions_in_subdir(\n    tmp_path: Path, monkeypatch: MonkeyPatch", "def test_tmppath_relative_basetemp_absolute_invalid_input(tmp_path, monkeypatch):\n    \"\"\"#4425\"\"\"\n    monkeypatch.chdir(tmp_path)\n    config = cast(Config, FakeConfig(\"hello\"))\n    t = TempPathFactory.from_config(config, _ispytest=True)\n    with pytest.raises(ValueError):\n        t.mktemp(basename=\"\")\n\n    with pytest.raises(ValueError):\n        t.mktemp(basename=\"/\")\n", "def test_tmp_path_factory_getbasetemp_custom_cleanup(pytester: Pytester) -> None:\n    \"\"\"Integration test for TempPathFactory.getbasetemp() with custom cleanup.\"\"\"\n    mytemp = pytester.path.joinpath(\"xyz\")\n    p = pytester.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    pytester.runpytest(p, \"--basetemp=%s\" % mytemp)\n    assert mytemp.exists()\n    pytester.runpytest(p, \"--basetemp=%s\" % mytemp)\n    assert mytemp.exists()\n    assert len(list(mytemp.iterdir())) == 1  # only one directory remains after cleanup\n", "def test_getbasetemp_configured_custom_directory(pytester: Pytester) -> None:\n    custom_basetemp = pytester.mkdir(\"custom-basetemp\")\n    custom_basetemp.joinpath(\"custom-basetemp-0\").mkdir()\n    pytester.runpytest(\"--basetemp=\" + str(custom_basetemp))\n    assert custom_basetemp.joinpath(\"custom-basetemp-0\").exists()\n    assert custom_basetemp.joinpath(\"custom-basetemp-1\").exists()\n    pytester.runpytest(\"--basetemp=\" + str(custom_basetemp))\n    assert custom_basetemp.joinpath(\"custom-basetemp-1\").exists()\n    assert not custom_basetemp.joinpath(\"custom-basetemp-0\").exists()\n    assert custom_basetemp.joinpath(\"custom-basetemp-2\").exists()\n", "def test_tmp_path_factory_handles_unable_to_delete_lock_file(tmp_path_factory: TempPathFactory, monkeypatch: MonkeyPatch) -> None:\n    # _basetemp / _given_basetemp are cached / set in parallel runs, patch them\n    monkeypatch.setattr(tmp_path_factory, \"_basetemp\", None)\n    monkeypatch.setattr(tmp_path_factory, \"_given_basetemp\", None)\n\n    # Simulate the scenario where the lock file cannot be deleted\n    original_remove = Path.rmdir\n        raise OSError(\"Mock error: unable to delete lock file\")\n    monkeypatch.setattr(Path, \"rmdir\", mock_remove)\n\n    # Verify that an error is raised when trying to delete the lock file\n    with pytest.raises(OSError, match=\"Mock error: unable to delete lock file\"):\n        tmp_path_factory.getbasetemp()\n\n    # Restore the original remove method\n    monkeypatch.setattr(Path, \"rmdir\", original_remove)\n"], "sample_179": ["    def test_autogenerated_through_name_clash(self):\n        class Model(models.Model):\n            other_model = models.ManyToManyField('self')\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"The field's intermediary table 'model_model' clashes with the \"\n                \"table name of 'invalid_models_tests.Model'.\",\n                obj=Model._meta.get_field('other_model'),\n                id='fields.E340',\n            ),\n        ])\n", "    def test_ordering_pointing_to_invalid_field(self):\n        class Model(models.Model):\n            field = models.IntegerField()\n\n            class Meta:\n                ordering = ['invalid_field']\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"'ordering' refers to the nonexistent field, related field, \"\n                \"or lookup 'invalid_field'.\",\n                obj=Model,\n                id='models.E015',\n            ),\n        ])\n", "    def test_fk_to_abstract_model(self):\n        class AbstractModel(models.Model):\n            class Meta:\n                abstract = True\n\n        class Model(models.Model):\n            fk = models.ForeignKey(AbstractModel, models.CASCADE)\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"The field 'fk' clashes with the field 'fk' from model \"\n                \"'invalid_models_tests.AbstractModel'.\",\n                obj=Model._meta.get_field('fk'),\n                id='models.E006',\n            ),\n        ])\n", "    def test_non_string_model_name(self):\n        class Model(models.Model):\n            class Meta:\n                app_label = 123\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"Model 'Meta.app_label' must be a string, not 'int'.\",\n                obj=Model._meta,\n                id='models.E020',\n            ),\n        ])\n", "    def test_model_with_unrelated_fields(self):\n        class Model(models.Model):\n            class Meta:\n                ordering = ['field1', 'field2']\n\n        class ChildModel(Model):\n            field1 = models.IntegerField()\n            field2 = models.IntegerField()\n\n        self.assertEqual(ChildModel.check(), [])\n"], "sample_395": ["    def test_template_dirs_non_existent_path(self):\n        self.assertEqual(autoreload.get_template_directories(), set())\n", "    def test_template_changed_with_no_template_dirs(self, mock_get_template_dirs, mock_reset):\n        mock_get_template_dirs.return_value = set()\n        template_path = Path(__file__).parent / \"templates\" / \"index.html\"\n        self.assertIsNone(autoreload.template_changed(None, template_path))\n        mock_reset.assert_not_called()\n", "def test_template_changed_non_template_suffix(self, mock_reset):\n    template_path = Path(__file__).parent / \"templates\" / \"index.py\"\n    self.assertIsNone(autoreload.template_changed(None, template_path))\n    mock_reset.assert_not_called()\n", "def test_get_template_directories_no_django_templates(self, mock_all):\n    mock_backend = mock.MagicMock(spec=DjangoTemplates)\n    mock_backend.engine.dirs = []\n    mock_backend.engine.template_loaders = []\n    mock_all.return_value = [mock_backend]\n    self.assertEqual(autoreload.get_template_directories(), set())\n", "    def test_template_changed_on_directory_change(self, mock_reset, mock_get_template_directories):\n        mock_get_template_directories.return_value = {Path(\"/path/to/template_dir\")}\n        template_dir_path = Path(\"/path/to/template_dir\")\n        file_path = template_dir_path / \"subdir\" / \"template.html\"\n        autoreload.template_changed(None, file_path)\n        mock_reset.assert_called_once()\n        mock_get_template_directories.assert_called_once()\n"], "sample_648": ["def test_parameter_set_extract_from_force_tuple(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.parametrize('x', [1, 2])\n            pass\n    \"\"\"\n    )\n\n    from _pytest.mark.structures import ParameterSet\n\n    parameterset = ParameterSet.extract_from((1, 2), force_tuple=True)\n    assert isinstance(parameterset.values, tuple)\n    assert len(parameterset.values) == 2\n\n    parameterset = ParameterSet.extract_from([1, 2], force_tuple=True)\n    assert isinstance(parameterset.values, tuple)\n    assert len(parameterset.values) == 2\n\n    parameterset = ParameterSet.extract_from(1, force_tuple=True)\n    assert isinstance(parameterset.values, tuple)\n    assert len(parameterset.values) == 1\n", "def test_combine_marks(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.slow\n        @pytest.mark.webtest\n            pass\n    \"\"\"\n    )\n    rec = pytester.inline_run()\n    passed, skipped, failed = rec.listoutcomes()\n    assert len(passed) == 1\n    assert len(skipped) == 0\n    assert len(failed) == 0\n    assert list(passed[0].iter_markers(name=\"slow\"))\n    assert list(passed[0].iter_markers(name=\"webtest\"))\n", "def test_marked_class_without_test_function(pytester: Pytester) -> None:\n    \"\"\"Test fails file is run when marked class contains no test functions.\"\"\"\n    py_file = pytester.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.slowtest\n        class Test1(object):\n            pass\n    \"\"\"\n    )\n    file_name = os.path.basename(py_file)\n    rec = pytester.inline_run(file_name)\n    rec.assertoutcome()\n", "def test_combined_with_mark_for_parametrize_marks(pytester: Pytester) -> None:\n    pytester.makeini(\n        \"\"\"\n    [pytest]\n    {}=skip\n    \"\"\".format(\n            EMPTY_PARAMETERSET_OPTION\n        )\n    )\n\n    config = pytester.parseconfig()\n    from _pytest.mark import get_empty_parameterset_mark\n\n    mark1 = get_empty_parameterset_mark(config, [\"a\"], all)\n    mark2 = get_empty_parameterset_mark(config, [\"b\"], all)\n    mark_combined = mark1.combined_with(mark2)\n    assert mark_combined.name == \"skip\"\n    assert mark_combined.args == ()\n    assert mark_combined.kwargs[\"reason\"].startswith(\"got empty parameter set \")\n", "def test_get_unpacked_marks(marks, expected):\n    class TestClass:\n        pytestmark = marks\n\n    assert get_unpacked_marks(TestClass) == expected\n\n"], "sample_671": ["    def test_skipif_with_invalid_condition(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            @pytest.mark.skipif(\"invalid_condition\")\n                pass\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines(\n            [\n                \"*ERROR*test_func*\",\n                \"*evaluating*skipif*expression*\",\n                \"*invalid_condition*\",\n                \"NameError: name 'invalid_condition' is not defined\",\n                \"*1 error*\",\n            ]\n        )\n", "def test_xfail_run_anyway_with_strict(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(strict=True)\n            assert 0\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--runxfail\")\n    result.stdout.fnmatch_lines(\n        [\"*1 failed*\", \"*def test_func():*\", \"*assert 0*\", \"*1 fail*\"]\n    )\n", "    def test_xfail_reason_with_non_string(self, testdir):\n        p = testdir.makepyfile(\n            test_one=\"\"\"\n            import pytest\n            @pytest.mark.xfail(reason=123)\n                assert 0\n        \"\"\"\n        )\n        result = testdir.runpytest(p, \"-rxs\")\n        result.stdout.fnmatch_lines([\"*1 error*\"])\n", "def test_xfail_not_run_mark_not_report(testdir):\n    p = testdir.makepyfile(\n        test_one=\"\"\"\n            import pytest\n            @pytest.mark.xfail(run=False, reason=\"noway\")\n                assert 0\n        \"\"\"\n    )\n    result = testdir.runpytest(p, \"-v\")\n    # result.stdout.fnmatch_lines([\n    #     \"*HINT*use*-r*\"\n    # ])\n    result.stdout.fnmatch_lines(\n        [\n            \"*1 skipped*\"\n        ]\n    )\n", "def test_xfail_strict_with_nodeid(testdir):\n    \"\"\"\n    Verify that xfail(strict=True) with a nodeid does not interfere with the\n    test node's own nodeid.\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        class TestClass(object):\n            @pytest.mark.xfail(strict=True, reason=\"unsupported feature\", nodeid=\"nodeid_1\")\n                pass\n        \"\"\"\n    )\n    result = testdir.runpytest(\"-rxX\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*TestClass::test_foo (nodeid='nodeid_1')*XPASS(strict)*unsupported feature*\"\n        ]\n    )\n"], "sample_622": ["def test_decode_cf_variable_with_invalid_fill_value() -> None:\n    variable = Variable([\"t\"], np.arange(3), {\"_FillValue\": np.array([0, 1])})\n    with pytest.raises(ValueError):\n        conventions.decode_cf_variable(\"t\", variable)\n", "def test_encode_cf_variable_non_native_dtype():\n    v = Variable([\"x\"], np.arange(5, dtype=\">i2\"))\n    v.encoding = {\"dtype\": \"int32\"}\n    expected = Variable([\"x\"], np.arange(5, dtype=\"int32\"))\n    actual = conventions.encode_cf_variable(v)\n    assert_identical(expected, actual)\n", "def test_decode_cf_variable_with_encoding_dtype():\n    variable = Variable([\"time\"], np.arange(3), encoding={\"dtype\": \"float64\"})\n    decoded = conventions.decode_cf_variable(\"time\", variable)\n    assert decoded.encoding[\"dtype\"] == \"float64\"\n    assert_identical(decoded, variable)\n", "    def test_non_multiindex(self) -> None:\n        var = Variable([\"x\"], [1, 2, 3])\n        conventions.ensure_not_multiindex(var)\n", "def test_decode_cf_variables_error_handling() -> None:\n    variables = {\n        \"t\": Variable([\"t\"], [0, 1, 2], {\"units\": \"days since 2000-01-01\"}),\n        \"x\": Variable([\"x\"], [9, 8, 7], {\"units\": \"km\"}),\n        \"foo\": (\n            (\"t\", \"x\"),\n            [[0, 0, 0], [1, 1, 1], [2, 2, 2]],\n            {\"units\": \"bar\"},\n        ),\n        \"y\": (\"t\", [5, 10, -999], {\"_FillValue\": -999}),\n    }\n    attributes = {}\n    with pytest.raises(Exception, match=\"Failed to decode variable 't'\"):\n        conventions.decode_cf_variables(variables, attributes, decode_times=False)\n"], "sample_918": ["def test_get_objects(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:method:: meth1\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"   .. py:staticmethod:: smeth\\n\"\n            \"   .. py:classmethod:: cmeth\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    objects = domain.get_objects()\n    expected_objects = [\n        ('Class', 'Class', 'class', 'index', 'Class', 0),\n        ('Class.meth1', 'meth1', 'method', 'index', 'Class.meth1', 1),\n        ('Class.attr', 'attr', 'attribute', 'index', 'Class.attr', 1),\n        ('Class.smeth', 'smeth', 'method', 'index', 'Class.smeth', 1),\n        ('Class.cmeth', 'cmeth', 'method', 'index', 'Class.cmeth', 1),\n    ]\n    assert list(objects) == expected_objects\n", "def test_pyclasslike_prefix(app):\n    text = (\".. py:class:: Foo\\n\"\n            \"\\n\"\n            \"   .. py:class:: Bar\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Foo\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'Bar (class in Foo)', 'Foo.Bar', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_annotation, \"class \"],\n                                                     [desc_addname, \"Foo.\"],\n                                                     [desc_name, \"Bar\"])],\n                                   [desc_content, ()]))\n    assert 'Foo.Bar' in domain.objects\n    assert domain.objects['Foo.Bar'] == ('index', 'Foo.Bar', 'class')\n\n", "def test_pymodule(app):\n    text = (\".. py:module:: mymodule\\n\"\n            \"\\n\"\n            \"   This is my module.\\n\"\n            \"\\n\"\n            \"   .. py:currentmodule::\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [nodes.target, nodes.Text(), addnodes.index]))\n    assert_node(doctree[0], nodes.target, ids=['module-mymodule'])\n    assert_node(doctree[1], addnodes.index, entries=[('pair', 'module; mymodule', 'module-mymodule', '', None)])\n    assert 'mymodule' in domain.modules\n    assert domain.modules['mymodule'] == ('index', 'module-mymodule', 'This is my module.', '', False)\n\n    text = (\".. py:module:: mymodule\\n\"\n            \"\\n\"\n            \"   This is my module.\\n\"\n            \"\\n\"\n            \"   .. py:currentmodule:: mymodule2\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert 'mymodule' in domain.modules\n    assert domain.modules['mymodule'] == ('index', 'module-mymodule', 'This is my module.', '', False)\n    assert domain.env.ref_context['py:module'] == 'mymodule2'\n", "def test_handle_signature(app):\n    env = Mock()\n    signode = desc_signature()\n\n    domain = PythonDomain(env)\n    obj = PyObject()\n    obj.env = env\n    obj.signode = signode\n\n    # Test with module and class name\n    obj.options = {'module': 'module_name'}\n    env.ref_context = {'py:class': 'class_name'}\n    fullname, prefix = obj.handle_signature('class_name.method_name()', signode)\n    assert fullname == 'module_name.class_name.method_name'\n    assert prefix == 'class_name'\n\n    # Test without module name but with class name\n    obj.options = {}\n    env.ref_context = {'py:class': 'class_name'}\n    fullname, prefix = obj.handle_signature('class_name.method_name()', signode)\n    assert fullname == 'class_name.method_name'\n    assert prefix == 'class_name'\n\n    # Test without class name\n    obj.options = {}\n    env.ref_context = {}\n    fullname, prefix = obj.handle_signature('method_name()', signode)\n    assert fullname == 'method_name'\n    assert prefix == ''\n\n    # Test with module name\n    obj.options = {'module': 'module_name'}\n    env.ref_context = {}\n    fullname, prefix = obj.handle_signature('method_name()', signode)\n    assert fullname == 'module_name.method_name'\n    assert prefix == ''\n\n    # Test with return annotation\n    obj.options = {}\n    env.ref_context = {}\n    fullname, prefix = obj.handle_signature('method_name() -> int', signode)\n    assert fullname == 'method_name'\n    assert prefix == ''\n    assert signode.astext() == 'method_name() -> int'\n\n    # Test with annotation\n    obj.options = {'annotation': 'annotation'}\n    env.ref_context = {}\n    fullname, prefix = obj.handle_signature('method_name()', signode)\n    assert fullname == 'method_name'\n    assert prefix == ''\n    assert signode.astext() == ' annotation method_name()'\n", "def test_pydomain_find_obj_with_various_prefixes(app):\n    text = \"\"\""], "sample_750": ["def test_omp_cv_multitarget():\n    ompcv = OrthogonalMatchingPursuitCV(normalize=True, fit_intercept=False,\n                                        max_iter=10, cv=5)\n    ompcv.fit(X, y)\n    assert_equal(ompcv.n_nonzero_coefs_, n_nonzero_coefs)\n    omp = OrthogonalMatchingPursuit(normalize=True, fit_intercept=False,\n                                    n_nonzero_coefs=ompcv.n_nonzero_coefs_)\n    omp.fit(X, y)\n    assert_array_almost_equal(ompcv.coef_, omp.coef_)\n", "def test_omp_precompute_error():\n    # Test that precompute raises an error when n_targets > n_samples\n    X = np.random.randn(10, 20)\n    y = np.random.randn(10, 25)\n    assert_raises(ValueError, orthogonal_mp, X, y, precompute=True)\n", "def test_omp_path_with_n_jobs():\n    path_single = orthogonal_mp(X, y, n_nonzero_coefs=5, return_path=True)\n    path_parallel = orthogonal_mp(X, y, n_nonzero_coefs=5, return_path=True,\n                                  precompute=True, n_jobs=-1)\n    assert_array_almost_equal(path_single, path_parallel)\n    path_single = orthogonal_mp_gram(G, Xy, n_nonzero_coefs=5, return_path=True)\n    path_parallel = orthogonal_mp_gram(G, Xy, n_nonzero_coefs=5, return_path=True,\n                                       n_jobs=-1)\n    assert_array_almost_equal(path_single, path_parallel)\n", "def test_omp_path_early_stopping():\n    path = orthogonal_mp(X, y, n_nonzero_coefs=10, tol=1e-6, return_path=True)\n    last = orthogonal_mp(X, y, n_nonzero_coefs=10, tol=1e-6, return_path=False)\n    assert_equal(path.shape, (n_features, n_targets, 10))\n    assert_array_almost_equal(path[:, :, -1], last)\n\n    path = orthogonal_mp_gram(G, Xy, n_nonzero_coefs=10, tol=1e-6, return_path=True)\n    last = orthogonal_mp_gram(G, Xy, n_nonzero_coefs=10, tol=1e-6, return_path=False)\n    assert_equal(path.shape, (n_features, n_targets, 10))\n    assert_array_almost_equal(path[:, :, -1], last)\n\n    # Test early stopping when n_nonzero_coefs is larger than the number of features\n    path = orthogonal_mp(X, y, n_nonzero_coefs=n_features + 1, tol=1e-6, return_path=True)\n    last = orthogonal_mp(X, y, n_nonzero_coefs=n_features + 1, tol=1e-6, return_path=False)\n    assert_equal(path.shape, (n_features, n_targets, n_features))\n    assert_array_almost_equal(path[:, :, -1], last)\n\n    path = orthogonal_mp_gram(G, Xy, n_nonzero_coefs=n_features + 1, tol=1e-6, return_path=True)\n    last = orthogonal_mp_gram(G, Xy, n_nonzero_coefs=n_features + 1, tol=1e-6, return_path=False)\n    assert_equal(path.shape, (n_features, n_targets, n_features))\n    assert_array_almost_equal(path[:, :, -1], last)\n", "def test_omp_reaches_least_squares_on_edge_cases():\n    # Use edge cases where the true number of nonzero coefficients is 0 or 1\n    rng = check_random_state(0)\n    n_samples, n_features = (10, 8)\n    n_targets = 3\n    X = rng.randn(n_samples, n_features)\n    # Case with 0 nonzero coefficients\n    Y_zero = np.zeros((n_samples, n_targets))\n    omp_zero = OrthogonalMatchingPursuit(n_nonzero_coefs=0)\n    omp_zero.fit(X, Y_zero)\n    assert_array_almost_equal(omp_zero.coef_, np.zeros((n_features, n_targets)))\n    \n    # Case with 1 nonzero coefficient\n    Y_one = np.zeros((n_samples, n_targets))\n    Y_one[:, 0] = X[:, 0]\n    omp_one = OrthogonalMatchingPursuit(n_nonzero_coefs=1)\n    omp_one.fit(X, Y_one)\n    assert_array_almost_equal(omp_one.coef_[:, 0], [1, 0, 0, 0, 0, 0, 0, 0])\n    \n    # Case with 1 nonzero coefficient with multiple targets\n    Y_one_multi = np.zeros((n_samples, n_targets))\n    Y_one_multi[:, :] = X[:, 0].reshape(-1, 1)\n    omp_one_multi = OrthogonalMatchingPursuit(n_nonzero_coefs=1)\n    omp_one_multi.fit(X, Y_one_multi)\n    assert_array_almost_equal(omp_one_multi.coef_, [[1, 0, 0, 0, 0, 0, 0, 0]] * n_targets)\n"], "sample_1030": ["def test_are_coplanar():\n    from sympy import Point3D, Line3D\n    a = Line3D(Point3D(5, 0, 0), Point3D(1, -1, 1))\n    b = Line3D(Point3D(0, -2, 0), Point3D(3, 1, 1))\n    c = Line3D(Point3D(0, -1, 0), Point3D(5, -1, 9))\n    assert are_coplanar(a, b, c) == False\n    assert are_coplanar(a, a) == True\n    assert are_coplanar(a, a, a) == True\n    assert are_coplanar(Point3D(0, 0, 0), Point3D(1, 0, 0), Point3D(0, 1, 0)) == False\n    assert are_coplanar(Point3D(0, 0, 0), Point3D(1, 0, 0), Point3D(0, 0, 1)) == False\n    assert are_coplanar(Point3D(0, 0, 0), Point3D(1, 0, 0), Point3D(1, 0, 0)) == True\n", "def test_are_coplanar():\n    from sympy.geometry import Point3D, Line3D\n    p1, p2, p3, p4 = Point3D(1, 2, 3), Point3D(4, 5, 6), Point3D(7, 8, 9), Point3D(10, 11, 12)\n    l1, l2, l3 = Line3D(p1, p2), Line3D(p3, p4), Line3D(p1, p3)\n    assert are_coplanar(l1, l2) == False\n    assert are_coplanar(l1, l3) == True\n    assert are_coplanar(p1, p2, p3) == True\n    assert are_coplanar(p1, p2, p3, p4) == True\n    assert are_coplanar(p1, p2, p3, p4, l1) == True\n    raises(ValueError, lambda: are_coplanar(p1, 2))\n", "def test_are_coplanar():\n    from sympy.geometry import Point3D, Line3D\n\n    p1, p2, p3, p4 = Point3D(1, 1, 1), Point3D(2, 2, 2), Point3D(3, 3, 3), Point3D(4, 4, 4)\n    assert are_coplanar(p1, p2, p3, p4) is True\n    p1, p2, p3, p4 = Point3D(1, 1, 1), Point3D(2, 2, 2), Point3D(3, 3, 3), Point3D(4, 4, 5)\n    assert are_coplanar(p1, p2, p3, p4) is False\n\n    l1, l2, l3 = Line3D(p1, p2), Line3D(p2, p3), Line3D(p3, p4)\n    assert are_coplanar(l1, l2, l3) is True\n    l1, l2, l3 = Line3D(p1, p2), Line3D(p2, p3), Line3D(p3, Point3D(4, 4, 5))\n    assert are_coplanar(l1, l2, l3) is False\n\n    raises(TypeError, lambda: are_coplanar(p1, 'a'))\n    raises(TypeError, lambda: are_coplanar(p1, p2, 'a'))\n    raises(TypeError, lambda: are_coplanar(l1, 'a'))\n    raises(TypeError, lambda: are_coplanar(l1, l2, 'a'))\n", "def test_are_coplanar():\n    p1 = Point3D(1, 2, 3)\n    p2 = Point3D(4, 5, 6)\n    p3 = Point3D(7, 8, 9)\n    p4 = Point3D(10, 11, 12)\n    assert are_coplanar(p1, p2, p3, p4) == True\n    p4 = Point3D(10, 11, 13)\n    assert are_coplanar(p1, p2, p3, p4) == False\n    line1 = Line3D(p1, p2)\n    line2 = Line3D(p3, p4)\n    assert are_coplanar(line1, line2) == False\n    plane = Plane(p1, p2, p3)\n    assert are_coplanar(plane, p1, p2, p3) == True\n    assert are_coplanar(plane, p1, p2, p4) == False\n    from sympy.geometry import Line, Point\n    p1, p2, p3 = Point(0, 0), Point(1, 1), Point(1, 0)\n    l1, l2 = Line(p1, p2), Line(p2, p3)\n    assert are_coplanar(l1, l2) == True\n    p3 = Point(1, 1, 0)\n    l2 = Line(p2, p3)\n    assert are_coplanar(l1, l2) == False\n", "def test_are_coplanar():\n    from sympy.geometry import Point3D, Line3D, Plane\n    from sympy.geometry.util import are_coplanar\n\n    # Test with three lines\n    a = Line3D(Point3D(5, 0, 0), Point3D(1, -1, 1))\n    b = Line3D(Point3D(0, -2, 0), Point3D(3, 1, 1))\n    c = Line3D(Point3D(0, -1, 0), Point3D(5, -1, 9))\n    assert not are_coplanar(a, b, c)\n\n    # Test with a plane and a point\n    p = Plane(Point3D(0, 0, 0), normal_vector=(1, 1, 1))\n    pt = Point3D(1, 1, 1)\n    assert are_coplanar(p, pt)\n\n    # Test with a plane and a line\n    assert are_coplanar(p, a)\n\n    # Test with a plane and a plane\n    q = Plane(Point3D(0, 0, 0), normal_vector=(1, 1, 1))\n    assert are_coplanar(p, q)\n\n    # Test with a line and a point\n    assert are_coplanar(a, pt)\n\n    # Test with three points\n    pt1 = Point3D(0, 0, 0)\n    pt2 = Point3D(1, 0, 0)\n    pt3 = Point3D(0, 1, 0)\n    assert are_coplanar(pt1, pt2, pt3)\n\n    # Test with two points\n    assert are_coplanar(pt1, pt2)\n\n    # Test with two points and a line\n    assert are_coplanar(pt1, pt2, a)\n\n    # Test with two points and a plane\n    assert are_coplanar(pt1, pt2, p)\n\n    # Test with a point, a line and a plane\n    assert are_coplanar(pt, a, p)\n\n    # Test with two lines and a plane\n    assert are_coplanar(a, b, p)\n"], "sample_1198": ["def test_parser_mathematica_function():\n    parser = MathematicaParser()\n\n    convert_chain = lambda expr: parser.parse(expr)\n\n    # Basic function definitions\n    assert convert_chain(\"f[x_] := x^3\") == Lambda(sympy.Symbol(\"x\"), sympy.Symbol(\"x\")**3)\n    assert convert_chain(\"f[x_, y_] := x^2 + y^2\") == Lambda((sympy.Symbol(\"x\"), sympy.Symbol(\"y\")), sympy.Symbol(\"x\")**2 + sympy.Symbol(\"y\")**2)\n    assert convert_chain(\"f[x_, y_, z_] := x^3 + y^2 + z\") == Lambda((sympy.Symbol(\"x\"), sympy.Symbol(\"y\"), sympy.Symbol(\"z\")), sympy.Symbol(\"x\")**3 + sympy.Symbol(\"y\")**2 + sympy.Symbol(\"z\"))\n\n    # Function definitions with default values\n    assert convert_chain(\"f[x_:0] := x^3\") == Lambda(sympy.Symbol(\"x\"), sympy.Symbol(\"x\")**3).subs(sympy.Symbol(\"x\"), 0)\n    assert convert_chain(\"f[x_:0, y_:1] := x^2 + y^2\") == Lambda((sympy.Symbol(\"x\"), sympy.Symbol(\"y\")), sympy.Symbol(\"x\")**2 + sympy.Symbol(\"y\")**2).subs({sympy.Symbol(\"x\"): 0, sympy.Symbol(\"y\"): 1})\n\n    # Function definitions with conditions\n    assert convert_chain(\"f[x_] := x^3 /; x > 0\") == Lambda(sympy.Symbol(\"x\"), sympy.Symbol(\"x\")**3).subs(sympy.Symbol(\"x\"), sympy.Symbol(\"x\", positive=True))\n    assert convert_chain(\"f[x_, y_] := x^2 + y^2 /; x > 0 && y < 0\") == Lambda((sympy.Symbol(\"x\"), sympy.Symbol(\"y\")), sympy.Symbol(\"x\")**2 + sympy.Symbol(\"y\")**2).subs({sympy.Symbol(\"x\"): sympy.Symbol(\"x\", positive=True), sympy.Symbol(\"y\"): sympy.Symbol(\"y\", negative=True)})\n\n    # Function definitions with multiple conditions\n    assert convert_chain(\"f[x_] := x^3 /; x > 0 && x < ", "def test_parser_mathematica_functions():\n    parser = MathematicaParser()\n\n    chain = lambda expr: parser.parse(expr)\n\n    # Additional conversions\n    assert chain(\"ArcCos[x]\") == acos(x)\n    assert chain(\"ArcCot[x]\") == acot(x)\n    assert chain(\"ArcCsc[x]\") == acsc(x)\n    assert chain(\"ArcSec[x]\") == asec(x)\n    assert chain(\"ArcSin[x]\") == asin(x)\n    assert chain(\"ArcTan[x]\") == atan(x)\n    assert chain(\"Cosh[x]\") == cosh(x)\n    assert chain(\"Coth[x]\") == coth(x)\n    assert chain(\"Cot[x]\") == cot(x)\n    assert chain(\"Csch[x]\") == csch(x)\n    assert chain(\"Sec[x]\") == sec(x)\n    assert chain(\"Sech[x]\") == sech(x)\n    assert chain(\"Sinh[x]\") == sinh(x)\n    assert chain(\"Tan[x]\") == tan(x)\n\n    # Logarithmic functions\n    assert chain(\"Log[x]\") == log(x)\n    assert chain(\"Log[x, y]\") == log(y, x)\n    assert chain(\"Log2[x]\") == log(x, 2)\n    assert chain(\"Log10[x]\") == log(x, 10)\n\n    # Trigonometric functions\n    assert chain(\"Cos[x]\") == cos(x)\n    assert chain(\"Sin[x]\") == sin(x)\n\n    # Hyperbolic functions\n    assert chain(\"ArcSinh[x]\") == asinh(x)\n    assert chain(\"ArcCosh[x]\") == acosh(x)\n    assert chain(\"ArcTanh[x]\") == atanh(x)\n    assert chain(\"ArcCoth[x]\") == acoth(x)\n    assert chain(\"ArcSech[x]\") == asech(x)\n    assert chain(\"ArcCsch[x]\") == acsch(x)\n\n    # Exponential function\n    assert chain(\"Exp[x]\") == exp(x)\n\n    # Root functions\n    assert chain(\"Sqrt[x]\") == sqrt(x)\n\n    # Airy functions\n    assert chain(\"AiryAi[x]\") == airyai(x)\n    assert chain(\"AiryAiPrime[x]\") == airyaiprime(x)\n    assert chain(\"AiryBi[x]\") == airybi(x)\n    assert chain(\"AiryBiPrime[x]\") == airybiprime(x)\n\n    # Error functions\n    assert chain(\"ExpIntegralEi[x", "def test_mathematica_parser_edge_cases():\n    parser = MathematicaParser()\n\n    # test for raising an exception when input is not a string\n    raises(TypeError, lambda: parser.parse(123))\n\n    # test for raising an exception when input is not a valid Mathematica expression\n    raises(SyntaxError, lambda: parser.parse(\"a +\"))\n\n    # test for raising an exception when input contains an invalid character\n    raises(SyntaxError, lambda: parser.parse(\"a @ b\"))\n\n    # test for raising an exception when input contains an invalid function name\n    raises(ValueError, lambda: MathematicaParser({\"InvalidFunction[x]\": \"x\"}))\n\n    # test for raising an exception when input contains an invalid translation\n    raises(ValueError, lambda: MathematicaParser({\"Sin[x]\": \"invalid_translation\"}))\n\n    # test for handling of None input\n    assert parser.parse(None) is None\n\n    # test for handling of empty string input\n    assert parser.parse(\"\") == \"\"\n\n    # test for handling of whitespace-only input\n    assert parser.parse(\"   \") == \"\"\n\n    # test for handling of comments in input\n    assert parser.parse(\"a (* comment *) b\") == \"a b\"\n", "def test_parser_mathematica_additional_translations():\n    parser = MathematicaParser()\n\n    additional_translations = {\n        'MyFunction[x]': 'my_function(x)',\n        'MyFunction[x, y]': 'my_function(x, y)',\n        'MyFunction[*x]': 'my_function(*x)'\n    }\n\n    d = {\n        'MyFunction[x]': 'my_function(x)',\n        'MyFunction[2, 3]': 'my_function(2, 3)',\n        'MyFunction[1, 2, 3]': 'my_function(1, 2, 3)',\n        '-MyFunction[x]': '-my_function(x)',\n        'MyFunction[x] + 1': 'my_function(x) + 1'\n    }\n\n    for e in d:\n        assert parse_mathematica(e, additional_translations) == sympify(d[e])\n\n    # Test that the cache is working properly\n    assert parser.cache_original == additional_translations\n    assert parser.cache_compiled != {}\n\n    # Test that the cache is cleared when a new parser is created\n    parser2 = MathematicaParser()\n    assert parser2.cache_original == {}\n    assert parser2.cache_compiled == {}\n\n    # Test that the cache is updated when a new additional_translations is provided\n    parser2 = MathematicaParser({'NewFunction[x]': 'new_function(x)'})\n    assert parser2.cache_original != parser.cache_original\n    assert parser2.cache_compiled != parser.cache_compiled\n\n    # Test that the cache is not updated when the same additional_translations is provided\n    parser3 = MathematicaParser(additional_translations)\n    assert parser3.cache_original == parser.cache_original\n    assert parser3.cache_compiled == parser.cache_compiled\n\n    # Test that the parser raises a ValueError when additional_translations is not a dictionary\n    raises(ValueError, lambda: MathematicaParser('not a dictionary'))\n", "def test_mathematica_corner_cases():\n    parser = MathematicaParser()\n\n    chain = lambda expr: parser._from_tokens_to_fullformlist(parser._from_mathematica_to_tokens(expr))\n\n    # Test edge case with multiple consecutive operators:\n    assert chain(\"a++b\") == [\"Plus\", \"a\", \"b\"]\n\n    # Test edge case with multiple consecutive unary minus:\n    assert chain(\"- - a\") == [\"Times\", \"-1\", [\"Times\", \"-1\", \"a\"]]\n    assert chain(\"a - -b\") == [\"Plus\", \"a\", [\"Times\", \"-1\", [\"Times\", \"-1\", \"b\"]]]\n    assert chain(\"-a--b\") == [\"Plus\", [\"Times\", \"-1\", \"a\"], [\"Times\", \"-1\", \"b\"]]\n\n    # Test edge case with no whitespace between operators and operands:\n    assert chain(\"a+b+c+d\") == [\"Plus\", \"a\", \"b\", \"c\", \"d\"]\n    assert chain(\"a-b-c-d\") == [\"Plus\", \"a\", [\"Times\", \"-1\", \"b\"], [\"Times\", \"-1\", \"c\"], [\"Times\", \"-1\", \"d\"]]\n    assert chain(\"a*b*c*d\") == [\"Times\", \"a\", \"b\", \"c\", \"d\"]\n    assert chain(\"a/b/c/d\") == [\"Times\", \"a\", [\"Power\", \"b\", \"-1\"], [\"Power\", \"c\", \"-1\"], [\"Power\", \"d\", \"-1\"]]\n\n    # Test edge case with parentheses around an entire expression:\n    assert chain(\"(a + b) + c\") == [\"Plus\", [\"Plus\", \"a\", \"b\"], \"c\"]\n    assert chain(\"(a + b - c) + d\") == [\"Plus\", [\"Plus\", [\"Plus\", \"a\", \"b\"], [\"Times\", \"-1\", \"c\"]], \"d\"]\n\n    # Test edge case with a single character variable name:\n    assert chain(\"x^2\") == [\"Power\", \"x\", \"2\"]\n    assert chain(\"y_\") == [\"Pattern\", \"y\", [\"Blank\"]]\n"], "sample_525": ["def test_subfigure_repr():\n    fig = Figure()\n    subfig = fig.add_subfigure([0, 0, 1, 1])\n    assert repr(subfig) == f\"<SubFigure size {subfig.bbox.width}x{subfig.bbox.height} with 0 Axes>\"\n", "def test_subfigure_warning():\n    fig = plt.figure()\n    subfig = fig.subfigures()\n    with pytest.warns(UserWarning):\n        fig.add_subplot(1, 1, 1)\n", "def test_subfigure_pickle():\n    fig = plt.figure(layout='constrained')\n    sub_fig = fig.add_subfigure(gridspec.GridSpec(1, 1)[0, 0])\n    assert pickle.loads(pickle.dumps(fig)).subfigs[0].get_label() == ''\n    assert pickle.loads(pickle.dumps(sub_fig)).get_label() == ''\n", "def test_add_gridspec():\n    fig = plt.figure()\n    gs = fig.add_gridspec(1, 1)\n    ax = fig.add_subplot(gs[0])\n    assert gs == ax.get_subplotspec().get_gridspec()\n\n    gs = fig.add_gridspec(2, 2)\n    axs = gs.subplots()\n    for ax in axs.flat:\n        assert gs == ax.get_subplotspec().get_gridspec()\n\n    with pytest.raises(ValueError):\n        fig.add_gridspec(0, 1)\n\n    with pytest.raises(ValueError):\n        fig.add_gridspec(1, 0)\n\n    gs = fig.add_gridspec(1, 1)\n    assert gs.figure is fig\n", "def test_layout_engine_change():\n    # test that changing the layout engine\n    fig, ax = plt.subplots(layout='constrained')\n    pc = ax.pcolormesh(np.random.randn(2, 2))\n    fig.colorbar(pc)\n    assert isinstance(fig.get_layout_engine(), ConstrainedLayoutEngine)\n    with pytest.raises(RuntimeError, match='Colorbar layout of new layout'):\n        fig.set_layout_engine(TightLayoutEngine())\n    assert isinstance(fig.get_layout_engine(), ConstrainedLayoutEngine)\n    fig.set_layout_engine(None)\n    assert isinstance(fig.get_layout_engine(), PlaceHolderLayoutEngine)\n    with pytest.raises(RuntimeError, match='Colorbar layout of new layout'):\n        fig.set_layout_engine(TightLayoutEngine())\n    fig.set_layout_engine(None)\n    assert isinstance(fig.get_layout_engine(), PlaceHolderLayoutEngine)\n    assert fig.get_layout_engine().adjust_compatible\n    assert fig.get_layout_engine().colorbar_gridspec\n    with pytest.raises(RuntimeError, match='Colorbar layout of new layout'):\n        fig.set_layout_engine(TightLayoutEngine())\n    fig.set_layout_engine(None)\n    assert isinstance(fig.get_layout_engine(), PlaceHolderLayoutEngine)\n    assert not fig.get_layout_engine().adjust_compatible\n    assert fig.get_layout_engine().colorbar_gridspec\n    with pytest.raises(RuntimeError, match='Colorbar layout of new layout'):\n        fig.set_layout_engine(TightLayoutEngine())\n    assert isinstance(fig.get_layout_engine(), PlaceHolderLayoutEngine)\n"], "sample_691": ["def test_pytest_unconfigure(pytester: Pytester) -> None:\n    \"\"\"Test that pytest_unconfigure correctly disables and re-enables faulthandler.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n    import faulthandler\n        pass\n    \"\"\"\n    )\n    result = pytester.runpytest_subprocess()\n    assert result.ret == 0\n\n    # Check that faulthandler is enabled after pytest has finished\n    assert faulthandler.is_enabled()\n", "def test_pytest_unconfigure(pytester: Pytester) -> None:\n    \"\"\"Test that the fault handler is correctly disabled and re-enabled during pytest_unconfigure.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import faulthandler\n            pass\n    \"\"\"\n    )\n    result = pytester.runpytest_subprocess()\n    assert result.ret == 0\n    # check that the fault handler is still enabled after pytest_unconfigure\n    result = pytester.run(sys.executable, \"-c\", \"import faulthandler; print(faulthandler.is_enabled())\")\n    assert result.stdout.strip() == \"True\"\n", "def test_faulthandler_stderr_key(pytester: Pytester) -> None:\n    \"\"\"Test that the fault handler stderr key is properly set and used.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n            pass\n        \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        faulthandler_timeout = 0.01\n        \"\"\"\n    )\n    result = pytester.runpytest_subprocess()\n    assert result.ret == 0\n\n    from _pytest.faulthandler import FaultHandlerHooks\n    from _pytest.store import StoreKey\n    assert isinstance(FaultHandlerHooks._get_stderr_fileno(), int)\n    assert isinstance(FaultHandlerHooks.fault_handler_stderr_key, StoreKey)\n", "def test_pytest_unconfigure(pytester: Pytester, monkeypatch) -> None:\n    \"\"\"Test that pytest_unconfigure correctly disables and re-enables faulthandler.\"\"\"\n    import faulthandler\n\n    was_enabled = faulthandler.is_enabled()\n\n        nonlocal was_enabled\n        was_enabled = False\n\n        nonlocal was_enabled\n        was_enabled = True\n\n    monkeypatch.setattr(faulthandler, \"disable\", mock_disable)\n    monkeypatch.setattr(faulthandler, \"enable\", mock_enable)\n\n    pytester.makepyfile(\n        \"\"\"\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest_subprocess()\n    assert result.ret == 0\n\n    # Make sure faulthandler was initially enabled\n    assert was_enabled\n\n    # Make sure faulthandler was disabled during pytest_unconfigure\n    assert not was_enabled\n\n    # Make sure faulthandler was re-enabled after pytest_unconfigure\n    assert was_enabled\n", "def test_fault_handler_stderr_key_removed_after_unconfigure(pytester: Pytester) -> None:\n    \"\"\"Test that fault_handler_stderr_key is removed from config after unconfigure.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest_subprocess()\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n    assert result.ret == 0\n\n    # check if fault_handler_stderr_key is removed\n    from _pytest.faulthandler import FaultHandlerHooks, fault_handler_stderr_key\n    from _pytest.config import Config\n    config = Config()\n    config.pluginmanager.register(FaultHandlerHooks(), \"faulthandler-hooks\")\n    hooks = config.pluginmanager.getplugin(\"faulthandler-hooks\")\n    hooks.pytest_configure(config)\n    assert fault_handler_stderr_key in config._store\n    hooks.pytest_unconfigure(config)\n    assert fault_handler_stderr_key not in config._store\n"], "sample_378": ["    def test_values(self):\n        notes = [\n            Note.objects.create(note='test-%s' % i, misc='misc-%s' % i)\n            for i in range(10)\n        ]\n        with self.assertNumQueries(1):\n            values = list(Note.objects.values('note', 'misc').order_by('note'))\n        self.assertEqual(len(values), 10)\n        self.assertEqual(values[0], {'note': 'test-0', 'misc': 'misc-0'})\n", "    def test_values_list_flat_with_single_field(self):\n        Note.objects.bulk_create([Note(note='test-%s' % i) for i in range(10)])\n        notes = Note.objects.values_list('note', flat=True)\n        self.assertIsInstance(notes, FlatValuesListIterable)\n        self.assertEqual(list(notes), ['test-0', 'test-1', 'test-2', 'test-3', 'test-4', 'test-5', 'test-6', 'test-7', 'test-8', 'test-9'])\n", "    def test_filter_after_union(self):\n        notes = Note.objects.bulk_create([Note(note='test-%s' % i, misc='misc-%s' % i) for i in range(10)])\n        qs = Note.objects.filter(note='test-1')\n        union_qs = Note.objects.filter(misc='misc-1').union(qs)\n        self.assertEqual(union_qs.filter(note='test-1').count(), 1)\n", "    def test_len_on_annotated_queryset(self):\n        objects = Individual.objects.create(alive=True)\n        annotated_queryset = Individual.objects.annotate(count=1)\n        annotated_queryset._fetch_all()\n        self.assertEqual(len(annotated_queryset), 1)\n", "    def test_get_or_create_with_defaults(self):\n        defaults = {'misc': 'default'}\n        note, created = Note.objects.get_or_create(note='test', defaults=defaults)\n        self.assertTrue(created)\n        self.assertEqual(note.note, 'test')\n        self.assertEqual(note.misc, 'default')\n"], "sample_176": ["def test_order_with_respect_to_alteration(self):\n    \"\"\"Tests altering order_with_respect_to option.\"\"\"\n    changes = self.get_changes([self.author_with_book_order_wrt], [self.author_with_book])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterOrderWithRespectTo\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"author\", order_with_respect_to=None)\n", "    def test_add_model_with_field_removed_from_proxy_model(self):\n        \"\"\"\n        Removing a field from a proxy model takes place before adding a new proxy\n        model that inherits from the same base model.\n        \"\"\"\n        proxy = ModelState('app', 'proxy', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ], options={'proxy': True})\n        before = [proxy]\n        after = [\n            ModelState('app', 'proxy', [\n                ('id', models.AutoField(primary_key=True)),\n            ], options={'proxy': True}),\n            ModelState('app', 'proxy2', [\n                ('title', models.CharField(max_length=200)),\n            ], bases=('app.proxy',), options={'proxy': True}),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'CreateModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='title', model_name='proxy')\n        self.assertOperationAttributes(changes, 'app', 0, 1, name='proxy2')\n", "def test_remove_order_with_respect_to_with_unmanaged_models(self):\n    \"\"\"Tests removal of order_with_respect_to option with unmanaged models.\"\"\"\n    before = [\n        ModelState(\"app\", \"model\", [\n            (\"id\", models.AutoField(primary_key=True)),\n        ], options={\"order_with_respect_to\": \"othermodel\"}),\n    ]\n    after = [\n        ModelState(\"app\", \"model\", [\n            (\"id\", models.AutoField(primary_key=True)),\n        ], options={}, managed=False),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 0)\n", "def test_alter_custom_through_model_with_m2m(self):\n    \"\"\"\n    Altering a custom through model used in M2M relations.\n    \"\"\"\n    before = [\n        ModelState('testapp', 'Author', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('testapp', 'Publisher', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('testapp', 'Contract', [\n            ('id', models.AutoField(primary_key=True)),\n            ('author', models.ForeignKey('testapp.Author', models.CASCADE)),\n            ('publisher', models.ForeignKey('testapp.Publisher', models.CASCADE)),\n        ]),\n        ModelState('testapp', 'Author', [\n            ('id', models.AutoField(primary_key=True)),\n            ('publishers', models.ManyToManyField('testapp.Publisher', through='testapp.Contract')),\n        ])\n    ]\n    after = [\n        ModelState('testapp', 'Author', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('testapp', 'Publisher', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('testapp', 'Contract', [\n            ('id', models.AutoField(primary_key=True)),\n            ('author', models.ForeignKey('testapp.Author', models.CASCADE)),\n            ('publisher', models.ForeignKey('testapp.Publisher', models.CASCADE)),\n            ('year', models.IntegerField()),\n        ]),\n        ModelState('testapp', 'Author', [\n            ('id', models.AutoField(primary_key=True)),\n            ('publishers', models.ManyToManyField('testapp.Publisher', through='testapp.Contract')),\n        ])\n    ]\n    changes = self.get_changes(before, after)\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\", \"AddField\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", model_name=\"contract\")\n    self.assertOperationAttributes(changes, \"testapp\", 0, 1, name=\"year\", model_name=\"contract\")\n", "def test_model_inherited_fields(self):\n    \"\"\"\n    Inherited fields are included when generating migrations for a new model.\n    \"\"\"\n    before = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ]),\n    ]\n    after = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ]),\n        ModelState('app', 'book', [\n            ('author', models.CharField(max_length=200)),\n        ], bases=('app.readable',)),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, [\"CreateModel\"])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name=\"book\")\n    book_fields = [field.name for field, field_instance in changes['app'][0].operations[0].fields]\n    self.assertIn('title', book_fields)\n    self.assertIn('author', book_fields)\n"], "sample_270": ["    def test_check_unique_constraint_deferrable_immediate(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['age'],\n                        name='unique_age_deferrable',\n                        deferrable=models.Deferrable.IMMEDIATE,\n                    ),\n                ]\n\n        errors = Model.check(databases=self.databases)\n        expected = [] if connection.features.supports_deferrable_unique_constraints else [\n            Warning(\n                '%s does not support deferrable unique constraints.'\n                % connection.display_name,\n                hint=(\n                    \"A constraint won't be created. Silence this warning if \"\n                    \"you don't care about it.\"\n                ),\n                obj=Model,\n                id='models.W038',\n            ),\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_unique_constraint_with_multiple_deferrable_values(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['age'],\n                        name='unique_age_deferrable',\n                        deferrable=models.Deferrable.DEFERRED,\n                    ),\n                    models.UniqueConstraint(\n                        fields=['age'],\n                        name='unique_age_deferrable_immediate',\n                        deferrable=models.Deferrable.IMMEDIATE,\n                    ),\n                ]\n\n        errors = Model.check(databases=self.databases)\n        expected = [] if connection.features.supports_deferrable_unique_constraints else [\n            Warning(\n                '%s does not support deferrable unique constraints.'\n                % connection.display_name,\n                hint=(\n                    \"A constraint won't be created. Silence this warning if \"\n                    \"you don't care about it.\"\n                ),\n                obj=Model,\n                id='models.W038',\n            ),\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_index_include_pointing_to_fk(self):\n        class Target(models.Model):\n            pass\n\n        class Model(models.Model):\n            fk_1 = models.ForeignKey(Target, models.CASCADE, related_name='target_1')\n            fk_2 = models.ForeignKey(Target, models.CASCADE, related_name='target_2')\n\n            class Meta:\n                indexes = [\n                    models.Index(\n                        fields=['id'],\n                        include=['fk_1_id', 'fk_2'],\n                        name='name',\n                    ),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [])\n", "    def test_check_constraint_with_expression(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(\n                        name='age_gt_18',\n                        check=models.Q(age__gt=models.F('age') - 18),\n                    ),\n                ]\n\n        errors = Model.check(databases=self.databases)\n        expected_errors = [\n            Error(\n                \"'constraints' refers to the joined field 'age'.\",\n                obj=Model,\n                id='models.E041',\n            ),\n        ]\n        self.assertCountEqual(errors, expected_errors)\n", "    def test_valid(self):\n        class Question(models.Model):\n            pass\n\n        class Answer(models.Model):\n            question = models.ForeignKey(Question, models.CASCADE)\n\n            class Meta:\n                order_with_respect_to = 'question'\n\n        self.assertEqual(Answer.check(), [])\n"], "sample_511": ["def test_xkcd_mode():\n    with plt.xkcd():\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3])\n        assert rcParams['text.usetex'] is False\n    assert rcParams['text.usetex'] is True\n\n    with pytest.raises(RuntimeError):\n        with plt.xkcd():\n            plt.rcParams['text.usetex'] = True\n", "def test_xkcd_context_manager():\n    with plt.xkcd(scale=2, length=100, randomness=2):\n        assert rcParams['path.sketch'] == (2, 100, 2)\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n        ax.plot([1, 2, 3])\n        plt.close(fig)\n    assert rcParams['path.sketch'] != (2, 100, 2)\n", "def test_fallback_backend():\n    original_backend = mpl.get_backend()\n    mpl.use('nonexistent', force=True)\n    assert mpl.get_backend() != 'nonexistent'\n    assert mpl.get_backend() in ['agg', 'cairo', 'qt4agg', 'qt5agg']\n    mpl.use(original_backend, force=True)\n    assert mpl.get_backend() == original_backend\n", "def test_xkcd_context_manager():\n    # Test xkcd() as context manager.\n    fig = plt.figure()\n    with plt.xkcd():\n        assert rcParams['text.usetex'] is False\n        assert rcParams['font.family'] == ['xkcd', 'xkcd Script', 'Humor Sans', 'Comic Neue', 'Comic Sans MS']\n        assert rcParams['font.size'] == 14.0\n        assert rcParams['path.sketch'] == (1, 100, 2)\n        assert rcParams['axes.linewidth'] == 1.5\n        assert rcParams['lines.linewidth'] == 2.0\n        assert rcParams['figure.facecolor'] == 'white'\n        assert rcParams['grid.linewidth'] == 0.0\n        assert rcParams['axes.grid'] is False\n        assert rcParams['axes.unicode_minus'] is False\n        assert rcParams['axes.edgecolor'] == 'black'\n        assert rcParams['xtick.major.size'] == 8\n        assert rcParams['xtick.major.width'] == 3\n        assert rcParams['ytick.major.size'] == 8\n        assert rcParams['ytick.major.width'] == 3\n    assert rcParams['text.usetex'] is True  # Reset to original value.\n    assert rcParams['font.family'] != ['xkcd', 'xkcd Script', 'Humor Sans', 'Comic Neue', 'Comic Sans MS']\n    assert rcParams['font.size'] != 14.0\n    assert rcParams['path.sketch'] != (1, 100, 2)\n    assert rcParams['axes.linewidth'] != 1.5\n    assert rcParams['lines.linewidth'] != 2.0\n    assert rcParams['figure.facecolor'] != 'white'\n    assert rcParams['grid.linewidth'] != 0.0\n    assert rcParams['axes.grid'] is not False\n    assert rcParams['axes.unicode_minus'] is not False\n    assert rcParams['axes.edgecolor'] != 'black'\n    assert rcParams['xtick.major.size'] != 8\n    assert rcParams['xtick.major.width'] != 3\n    assert rcParams['ytick.major.size'] != 8\n    assert rcParams['ytick.major.width'] != 3\n", "def test_subplot_mosaic():\n    fig, ax_dict = plt.subplot_mosaic([\n        ['A', 'A', 'C'],\n        ['D', '.',  'C']\n    ], sharex=True, sharey=True)\n\n    assert len(ax_dict) == 3\n    assert 'A' in ax_dict\n    assert 'C' in ax_dict\n    assert 'D' in ax_dict\n    for ax in ax_dict.values():\n        assert ax is not None\n\n    # Test that the subplot sharex and sharey arguments work\n    assert ax_dict['A'].get_shared_x_axes().get_siblings(ax_dict['A']) == [ax_dict['D']]\n    assert ax_dict['A'].get_shared_y_axes().get_siblings(ax_dict['A']) == [ax_dict['C'], ax_dict['D']]\n\n    # Test the empty_sentinel argument\n    fig, ax_dict = plt.subplot_mosaic([\n        ['A', 'A', 'C'],\n        ['D', '.',  'C']\n    ], sharex=True, sharey=True, empty_sentinel='.')\n\n    # Test that the subplot_mosaic can handle a single character string\n    fig, ax_dict = plt.subplot_mosaic([\n        '''\n        AAA\n        D.C\n        '''\n    ], sharex=True, sharey=True)\n\n    # Test that the subplot_mosaic can handle a single label\n    fig, ax_dict = plt.subplot_mosaic([\n        ['A']\n    ], sharex=True, sharey=True)\n\n    # Test that the subplot_mosaic can handle a list of lists of different lengths\n    fig, ax_dict = plt.subplot_mosaic([\n        ['A', 'A', 'C'],\n        ['D',  'C']\n    ], sharex=True, sharey=True)\n\n    # Test that the subplot_mosaic raises an error when the layout is invalid\n    with pytest.raises(ValueError):\n        plt.subplot_mosaic([\n            ['A', 'A', 'C'],\n            ['D',  'C', 'E']\n        ], sharex=True, sharey=True, empty_sentinel='F')\n\n    # Test that the subplot_mosaic raises an error when the layout is not a list\n    with pytest.raises(TypeError):\n        plt.subplot_mosaic('Invalid layout', sharex=True, sharey=True)\n\n    # Test that the subplot_mosaic raises an error when the layout is not a list of lists\n    with pytest.raises(TypeError):\n        plt.subplot_mosaic"], "sample_97": ["def test_setting_timeout_from_environment_variable_invalid(self):\n    with self.assertRaises(ValueError):\n        self.RELOADER_CLS()\n", "    def test_executable_without_py_extension(self):\n        original_argv = sys.argv.copy()\n        sys.argv = ['foo.exe', '-m', 'django']\n        args = autoreload.get_child_arguments()\n        self.assertEqual(args, [sys.executable, '-W%s' % o for o in sys.warnoptions] + ['foo.exe.py', '-m', 'django'])\n        sys.argv = original_argv\n", "    def test_calls_tcsetattr_if_termios_is_available(self, mocked_isatty, mocked_termios):\n        mocked_termios.tcgetattr.return_value = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n        autoreload.ensure_echo_on()\n        mocked_termios.tcsetattr.assert_called_once()\n", "    def test_terminal_mode(self, mocked_termios):\n        mocked_termios.tcgetattr.return_value = [0, 1, 2, 4]\n        autoreload.ensure_echo_on()\n        self.assertEqual(mocked_termios.tcsetattr.call_count, 1)\n        self.assertEqual(mocked_termios.tcsetattr.call_args[0][1], 1)\n        self.assertEqual(mocked_termios.tcsetattr.call_args[0][2][3], 6)\n", "    def test_echo_on_called_when_terminal(self, mocked_termios):\n        mocked_termios.tcgetattr.return_value = [0, 1, 2, 0]\n        sys.stdin = mock.MagicMock(isatty=mock.MagicMock(return_value=True))\n        autoreload.ensure_echo_on()\n        mocked_termios.tcsetattr.assert_called_once_with(sys.stdin, mocked_termios.TCSANOW, [0, 1, 2, mocked_termios.ECHO])\n"], "sample_256": ["    def test_bug_19349_has_changed(self):\n        field = ReadOnlyPasswordHashField()\n        self.assertFalse(field.has_changed('aaa', 'bbb'))\n", "    def test_invalid_password_format_or_unknown_hashing_algorithm(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = 'invalid$format'\n        html = widget.render('password', value, {'id': 'id_password'})\n        self.assertIn(_(\"Invalid password format or unknown hashing algorithm.\"), html)\n", "    def test_render_with_invalid_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = 'invalid$hash'\n        with self.assertRaises(ValueError):\n            widget.render('password', value, {'id': 'id_password'})\n", "    def test_render_without_value(self):\n        widget = ReadOnlyPasswordHashWidget()\n        html = widget.render('password', '', {})\n        self.assertIn(_(\"No password set.\"), html)\n", "    def test_send_mail_with_extra_email_context(self):\n        (user, username, email) = self.create_dummy_user()\n        form = PasswordResetForm({'email': email})\n        self.assertTrue(form.is_valid())\n        form.save(domain_override='example.com', extra_email_context={'custom_context': 'custom value'})\n        self.assertEqual(len(mail.outbox), 1)\n        self.assertEqual(mail.outbox[0].to, [email])\n        self.assertIn('custom value', mail.outbox[0].body)\n"], "sample_290": ["def test_unmanaged_custom_pk_fk_dependency(self):\n    \"\"\"\n    #23415 - The autodetector must correctly deal with custom FK on\n    unmanaged models when the unmanaged model is referenced by a FK.\n    \"\"\"\n    # First, we test the default pk field name\n    changes = self.get_changes([], [self.author_unmanaged_default_pk, self.book])\n    # The field name the FK on the book model points to\n    self.assertEqual(changes['otherapp'][0].operations[0].fields[2][1].remote_field.field_name, 'id')\n    # Now, we test the custom pk field name\n    changes = self.get_changes([], [self.author_unmanaged_custom_pk, self.book])\n    # The field name the FK on the book model points to\n    self.assertEqual(changes['otherapp'][0].operations[0].fields[2][1].remote_field.field_name, 'pk_field')\n", "def test_multiple_bases_and_proxies(self):\n    \"\"\"\n    #23956 - Inheriting models doesn't move *_ptr fields into AddField operations.\n    \"\"\"\n    A = ModelState(\"app\", \"A\", [(\"a_id\", models.AutoField(primary_key=True))])\n    B = ModelState(\"app\", \"B\", [(\"b_id\", models.AutoField(primary_key=True))])\n    C = ModelState(\"app\", \"C\", [], bases=(\"app.A\", \"app.B\"))\n    P = ModelState(\"app\", \"P\", [], bases=(\"app.C\",), options={\"proxy\": True})\n    D = ModelState(\"app\", \"D\", [], bases=(\"app.A\", \"app.B\"))\n    E = ModelState(\"app\", \"E\", [], bases=(\"app.A\", \"app.B\"))\n    changes = self.get_changes([], [A, B, C, P, D, E])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"app\", 1)\n    self.assertOperationTypes(changes, \"app\", 0, [\n        \"CreateModel\", \"CreateModel\", \"CreateModel\", \"CreateModel\", \"CreateModel\", \"CreateModel\"\n    ])\n    self.assertOperationAttributes(changes, \"app\", 0, 0, name=\"A\")\n    self.assertOperationAttributes(changes, \"app\", 0, 1, name=\"B\")\n    self.assertOperationAttributes(changes, \"app\", 0, 2, name=\"C\")\n    self.assertOperationAttributes(changes, \"app\", 0, 3, name=\"P\", options={\"proxy\": True})\n    self.assertOperationAttributes(changes, \"app\", 0, 4, name=\"D\")\n    self.assertOperationAttributes(changes, \"app\", 0, 5, name=\"E\")\n", "def test_alter_field_with_deconstructible_args(self):\n    \"\"\"Tests autodetection of fields with deconstructible args.\"\"\"\n    changes = self.get_changes(\n        [self.author_name_deconstructible_1], [self.author_name_deconstructible_list_1]\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"name\")\n", "def test_alter_foreign_key_to_proxy(self):\n    # First, test the default pk field name\n    before = [\n        ModelState('app', 'Foo', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('app', 'Bar', [\n            ('id', models.AutoField(primary_key=True)),\n            ('foo', models.ForeignKey('app.Foo', models.CASCADE)),\n        ]),\n    ]\n    after = [\n        ModelState('app', 'Foo', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('app', 'Bar', [\n            ('id', models.AutoField(primary_key=True)),\n            ('foo', models.ForeignKey('app.FooProxy', models.CASCADE)),\n        ]),\n        ModelState('app', 'FooProxy', [\n            ('id', models.AutoField(primary_key=True)),\n        ], options={'proxy': True}, bases=('app.Foo',)),\n    ]\n    changes = self.get_changes(before, after)\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, [\"CreateModel\", \"AlterField\"])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name=\"FooProxy\", options={'proxy': True})\n    self.assertEqual(changes['app'][0].operations[1].field.remote_field.model._meta.db_table, 'app_foo')\n    self.assertEqual(changes['app'][0].operations[1].field.remote_field.field_name, 'id')\n", "def test_alter_model_options_to_none(self):\n    \"\"\"Changing model's options to None should remove them.\"\"\"\n    changes = self.get_changes([self.author_with_options], [self.author_empty])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelOptions\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", options=None)\n\n    # Also test that AlterModelOptions doesn't produce a migration when options are None.\n    changes = self.get_changes([self.author_empty], [self.author_empty])\n    self.assertEqual(len(changes), 0)\n"], "sample_881": ["def test_ndcg_tie_handling():\n    y_true = np.array([[3, 0, 0, 1], [0, 3, 0, 1], [0, 0, 3, 1]])\n    y_score = np.array([[0.25, 0.5, 0.5, 0.5], [0.5, 0.25, 0.5, 0.5], [0.5, 0.5, 0.25, 0.5]])\n    assert_allclose(ndcg_score(y_true, y_score), np.array([1, 1, 1]))\n    assert_allclose(ndcg_score(y_true, y_score, ignore_ties=True), np.array([1, 1, 1]))\n", "def test_roc_auc_score_with_positive_class_probability():\n    # Test case where class probability is provided as y_score\n    y_true = np.array([1, 1, 2, 2])\n    y_score = np.array([[0.1, 0.4, 0.5], [0.2, 0.3, 0.5], [0.6, 0.2, 0.2], [0.3, 0.4, 0.3]])\n    with pytest.raises(ValueError):\n        roc_auc_score(y_true, y_score)\n", "def test_average_precision_score_multilabel():\n    # Test that average_precision_score is working as expected for multilabel\n    # classification problems.\n\n    rng = np.random.RandomState(0)\n    y_true = rng.randint(2, size=(10, 5))\n    y_pred = rng.rand(10, 5)\n    assert_almost_equal(\n        average_precision_score(y_true, y_pred, average=\"samples\"),\n        np.mean(\n            [\n                average_precision_score(y_true[i], y_pred[i], average=\"macro\")\n                for i in range(len(y_true))\n            ]\n        ),\n    )\n\n    assert_almost_equal(\n        average_precision_score(y_true, y_pred, average=\"macro\"),\n        average_precision_score(y_true, y_pred, average=\"macro\", zero_division=0),\n    )\n    assert_almost_equal(\n        average_precision_score(y_true, y_pred, average=\"weighted\"),\n        average_precision_score(y_true, y_pred, average=\"weighted\", zero_division=0),\n    )\n", "def test_roc_auc_score_with_sample_weight():\n    y_true = [0, 0, 1, 1, 1]\n    y_score = [0.2, 0.3, 0.4, 0.6, 0.8]\n    sample_weight = [1, 1, 1, 1, 0.1]\n    assert_almost_equal(roc_auc_score(y_true, y_score, sample_weight=sample_weight), 0.9)\n\n    # test with binary classification\n    y_true = [0, 0, 1, 1]\n    y_score = [[0.1, 0.9], [0.3, 0.7], [0.6, 0.4], [0.8, 0.2]]\n    sample_weight = [1, 1, 1, 0.1]\n    assert_almost_equal(roc_auc_score(y_true, y_score, sample_weight=sample_weight), 0.9)\n\n    # test with multiclass classification\n    y_true = [0, 1, 2, 2]\n    y_score = [[0.1, 0.3, 0.6], [0.2, 0.6, 0.2], [0.4, 0.3, 0.3], [0.1, 0.2, 0.7]]\n    sample_weight = [1, 1, 1, 0.1]\n    assert_almost_equal(roc_auc_score(y_true, y_score, sample_weight=sample_weight, average='macro'), 0.8)\n", "def test_roc_auc_score_input_shape_error():\n    y_true = np.array([0, 1, 1, 0])\n    y_pred = np.array([0.1, 0.4, 0.35, 0.8])\n    y_pred_2d = np.array([[0.1, 0.9], [0.4, 0.6], [0.35, 0.65], [0.8, 0.2]])\n    y_pred_3d = np.array(\n        [\n            [[0.1, 0.9], [0.9, 0.1]],\n            [[0.4, 0.6], [0.6, 0.4]],\n            [[0.35, 0.65], [0.65, 0.35]],\n            [[0.8, 0.2], [0.2, 0.8]],\n        ]\n    )\n\n    with pytest.raises(ValueError, match=\"Found input variables with inconsistent\"):\n        roc_auc_score(y_true, y_pred_3d)\n    with pytest.raises(ValueError, match=\"multiclass format is not supported\"):\n        roc_auc_score(y_true, y_pred_2d)\n\n    # Check that roc_auc_score function returns an error when trying\n    # to compute AUC for non-binary class values.\n    y_true_multiclass = np.array([0, 1, 2, 2])\n    y_pred_multiclass = np.array([[0.1, 0.4, 0.35], [0.4, 0.3, 0.3], [0.35, 0.5, 0.15], [0, 0.2, 0.8]])\n    err_msg = \"ROC AUC score is not defined\"\n    with pytest.raises(ValueError, match=err_msg):\n        roc_auc_score(y_true_multiclass, y_pred_multiclass)\n"], "sample_846": ["def test_column_transformer_transformer_weights_remainder():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).T\n\n    X_res_both = X_array.copy()\n\n    # second and third columns are doubled when remainder = DoubleTrans\n    X_res_both[:, 1:3] *= 2\n\n    # weights\n    X_res_both[:, 1:3] *= 10\n\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           remainder=DoubleTrans(),\n                           transformer_weights={'remainder': 10})\n\n    assert_array_equal(ct.fit_transform(X_array), X_res_both)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert isinstance(ct.transformers_[-1][1], DoubleTrans)\n    assert_array_equal(ct.transformers_[-1][2], [1, 2])\n", "def test_column_transformer_multiple_remainder_transformer_calls():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).T\n    X_df = pd.DataFrame(X_array, columns=['first', 'second', 'third'])\n\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           remainder=DoubleTrans())\n\n    assert_array_equal(ct.fit(X_array).transform(X_array), ct.fit_transform(X_array))\n    assert_array_equal(ct.fit(X_df).transform(X_df), ct.fit_transform(X_df))\n    assert_array_equal(ct.fit(X_array).transform(X_df), ct.fit_transform(X_df))\n    assert_array_equal(ct.fit(X_df).transform(X_array), ct.fit_transform(X_array))\n\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert isinstance(ct.transformers_[-1][1], DoubleTrans)\n    assert_array_equal(ct.transformers_[-1][2], [1, 2])\n", "def test_column_transformer_sparse_remainder_transformer_dtype():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).T\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           remainder=SparseMatrixTrans(),\n                           sparse_threshold=0.8)\n\n    X_trans = ct.fit_transform(X_array)\n    assert X_trans.dtype == np.float64\n\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).T.astype(np.int32)\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           remainder=SparseMatrixTrans(),\n                           sparse_threshold=0.8)\n    X_trans = ct.fit_transform(X_array)\n    assert X_trans.dtype == np.int32\n\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).T.astype(object)\n    with pytest.raises(ValueError,\n                       match=\"For a sparse output, all columns should\"):\n        ColumnTransformer([('trans1', Trans(), [0])],\n                          remainder=SparseMatrixTrans(),\n                          sparse_threshold=0.8).fit_transform(X_array)\n", "def test_column_transformer_get_feature_names_pandas_with_remainder(columns):\n    pd = pytest.importorskip(\"pandas\")\n    X = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6], \"c\": [7, 8, 9]})\n    ct = ColumnTransformer([(\"trans\", Trans(), columns)], remainder=\"passthrough\")\n    ct.fit(X)\n    assert ct.get_feature_names() == [f\"trans__{col}\" for col in columns] + [\n        col for col in X.columns if col not in columns\n    ]\n", "def test_column_transformer_boolean_indexing_on_pandas():\n    pd = pytest.importorskip('pandas')\n    X_df = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))\n\n    # Boolean indexing with a pandas boolean series\n    boolean_series = pd.Series([True, False, True])\n    ct = ColumnTransformer([('trans', Trans(), boolean_series)],\n                           remainder='passthrough')\n    X_trans = ct.fit_transform(X_df)\n    assert X_trans.shape == (3, 4)\n\n    # Boolean indexing with a boolean numpy array\n    boolean_array = np.array([True, False, True])\n    ct = ColumnTransformer([('trans', Trans(), boolean_array)],\n                           remainder='passthrough')\n    X_trans = ct.fit_transform(X_df)\n    assert X_trans.shape == (3, 4)\n"], "sample_1048": ["def test_parabola_axis_of_symmetry():\n    p1 = Point(0, 0)\n    p2 = Point(0, 4)\n    p3 = Point(6, 0)\n    d1 = Line(Point(4, 0), Point(4, 9))\n    d2 = Line(Point(7, 6), Point(3, 6))\n    d3 = Line(Point(4, 0), slope=oo)\n    d4 = Line(Point(7, 6), slope=0)\n    \n    pa1 = Parabola(p1, d1)\n    pa2 = Parabola(p2, d2)\n    pa3 = Parabola(p3, d3)\n    pa4 = Parabola(p1, d4)\n    \n    assert pa1.axis_of_symmetry == Line2D(Point(0, 0), Point(0, 1))\n    assert pa2.axis_of_symmetry == Line2D(Point(0, 4), Point(0, 1))\n    assert pa3.axis_of_symmetry == Line2D(Point(6, 0), Point(1, 0))\n    assert pa4.axis_of_symmetry == Line2D(Point(0, 0), Point(1, 0))\n", "def test_parabola_equation():\n    x, y = symbols('x y')\n    p1 = Point(0, 0)\n    p2 = Point(0, 4)\n    d1 = Line(Point(0, 6), Point(1, 6))\n    d2 = Line(Point(-2, 0), Point(-2, 1))\n    pa1 = Parabola(p1, d1)\n    pa2 = Parabola(p2, d2)\n\n    assert pa1.equation() == -x**2 + 8*y - 24\n    assert pa2.equation() == -y**2 + 8*x - 16\n    assert pa1.equation('z') == -z**2 + 8*y - 24\n    assert pa1.equation(x='z', y='w') == -z**2 + 8*w - 24\n    assert pa2.equation('z', 'w') == -w**2 + 8*z - 16\n", "def test_parabola_properties():\n    p1 = Point(0, 0)\n    p2 = Point(3, 0)\n    p3 = Point(0, 3)\n    d1 = Line(Point(0, 4), Point(1, 4))\n    d2 = Line(Point(4, 0), Point(4, 1))\n    parabola1 = Parabola(p1, d1)\n    parabola2 = Parabola(p2, d1)\n    parabola3 = Parabola(p1, d2)\n\n    # Test axis of symmetry\n    assert parabola1.axis_of_symmetry == Line(Point(0, 0), Point(0, 1))\n    assert parabola2.axis_of_symmetry == Line(Point(3, 0), Point(3, 1))\n    assert parabola3.axis_of_symmetry == Line(Point(0, 0), Point(1, 0))\n\n    # Test ambient dimension\n    assert parabola1.ambient_dimension == 2\n    assert parabola2.ambient_dimension == 2\n    assert parabola3.ambient_dimension == 2\n\n    # Test equation\n    assert parabola1.equation() == -16*x**2 + y + 64\n    assert parabola2.equation() == -16*(x - 3)**2 + y + 64\n    assert parabola3.equation() == x**2 + 16*y - 64\n\n    # Test equation with custom x and y\n    assert parabola1.equation('t') == -16*t**2 + y + 64\n    assert parabola2.equation(y='s') == -16*(x - 3)**2 + s + 64\n", "def test_parabola_equation():\n    p1 = Point(0, 0)\n    p2 = Point(3, 7)\n    d1 = Line(Point(4, 0), Point(4, 9))\n    d2 = Line(Point(7, 6), Point(3, 6))\n    d3 = Line(Point(4, 0), slope=oo)\n    d4 = Line(Point(7, 6), slope=0)\n\n    parabola1 = Parabola(p1, d1)\n    parabola2 = Parabola(p2, d2)\n    parabola3 = Parabola(p2, d4)\n    parabola4 = Parabola(p1, d3)\n\n    assert parabola1.equation() == -parabola1.equation('x', 'y')\n    assert parabola2.equation('x') == -parabola2.equation()\n    assert parabola3.equation('f', 'g') == -parabola3.equation()\n    assert parabola4.equation('a', 'b') == -parabola4.equation()\n\n    # Test with different variable names\n    assert parabola1.equation('t', 's') == -parabola1.equation()\n    assert parabola2.equation('m', 'n') == -parabola2.equation()\n    assert parabola3.equation('u', 'v') == -parabola3.equation()\n    assert parabola4.equation('p', 'q') == -parabola4.equation()\n\n    # Test with non-standard variable names\n    assert parabola1.equation('\u03c6', '\u03c8') == -parabola1.equation()\n    assert parabola2.equation('\u03b1', '\u03b2') == -parabola2.equation()\n    assert parabola3.equation('\u03b3', '\u03b4') == -parabola3.equation()\n    assert parabola4.equation('\u03b5', '\u03b6') == -parabola4.equation()\n\n    # Test with Greek letters\n    assert parabola1.equation('\u03bc', '\u03bd') == -parabola1.equation()\n    assert parabola2.equation('\u03be', '\u03bf') == -parabola2.equation()\n    assert parabola3", "def test_parabola_equation():\n    p1 = Point(0, 0)\n    p2 = Point(3, 7)\n    p3 = Point(0, 4)\n    p4 = Point(6, 0)\n    d1 = Line(Point(4, 0), Point(4, 9))\n    d2 = Line(Point(7, 6), Point(3, 6))\n    d3 = Line(Point(4, 0), slope=oo)\n    d4 = Line(Point(7, 6), slope=0)\n\n    pa1 = Parabola(p1, d2)\n    pa2 = Parabola(p1, d1)\n    pa3 = Parabola(p2, d2)\n    pa4 = Parabola(p3, d2)\n    pa5 = Parabola(p2, d1)\n    pa6 = Parabola(p4, d1)\n\n    # Test equation with default x, y\n    assert pa1.equation() == -pa1.p_parameter*pa1.axis_of_symmetry.equation()\n    assert pa2.equation() == -pa2.p_parameter*pa2.axis_of_symmetry.equation()\n    assert pa3.equation() == -pa3.p_parameter*pa3.axis_of_symmetry.equation()\n    assert pa4.equation() == -pa4.p_parameter*pa4.axis_of_symmetry.equation()\n    assert pa5.equation() == -pa5.p_parameter*pa5.axis_of_symmetry.equation()\n    assert pa6.equation() == -pa6.p_parameter*pa6.axis_of_symmetry.equation()\n\n    # Test equation with custom x, y\n    x, y = symbols('x y')\n    assert pa1.equation(x, y) == -pa1.p_parameter*x**2 + 8*y - 32\n    assert pa2.equation('u', 'v') == -4*u - 16*v + 64\n    assert pa3.equation(x, y) == -x**2/2 + 7*y - 49/2\n    assert pa4.equation('a', 'b') == -a**2 + 12*b - 48\n    assert pa5.equation(x, y) == -x**2/2 - 16*y + 56\n    assert pa6.equation('c', 'd') =="], "sample_719": ["def test_tfidf_vectorizer_min_max_df():\n    # Test the effect of min_df and max_df on vocabulary size\n    X = ['This is the first document.', 'This document is the second document.',\n         'And this is the third one.', 'Is this the first document?']\n\n    tfidf = TfidfVectorizer()\n    X_tfidf = tfidf.fit_transform(X)\n    vocab_size = len(tfidf.vocabulary_)\n    assert vocab_size == 9\n\n    tfidf = TfidfVectorizer(min_df=0.5)\n    X_tfidf = tfidf.fit_transform(X)\n    vocab_size = len(tfidf.vocabulary_)\n    assert vocab_size == 5\n\n    tfidf = TfidfVectorizer(max_df=0.5)\n    X_tfidf = tfidf.fit_transform(X)\n    vocab_size = len(tfidf.vocabulary_)\n    assert vocab_size == 6\n", "def test_vectorizer_inconsistent_input_type():\n    # Test that the vectorizer raises an error when the input type is inconsistent\n    vectorizer = CountVectorizer(input='filename')\n    documents = ['this is a text document', 'another text document']\n    assert_raise_message(\n        ValueError, \"Iterable over raw text documents expected, \"\n        \"string object received.\", vectorizer.fit_transform, documents)\n\n    vectorizer = CountVectorizer(input='content')\n    documents = ['filename.txt', 'another_filename.txt']\n    assert_raise_message(\n        ValueError, \"Iterable over raw text documents expected, \"\n        \"string object received.\", vectorizer.fit_transform, documents)\n\n    vectorizer = CountVectorizer(input='file')\n    documents = ['this is a text document', 'another text document']\n    assert_raise_message(\n        ValueError, \"Iterable over raw text documents expected, \"\n        \"string object received.\", vectorizer.fit_transform, documents)\n", "def test_hashing_vectorizer_non_negative_deprecation():\n    # Ensure the non_negative argument of HashingVectorizer raises a\n    # deprecation warning\n    msg = \"The 'non_negative' parameter is deprecated and will be removed in \"\n    msg += \"0.21. It will be set to False by default.\"\n    with pytest.warns(FutureWarning, match=msg):\n        HashingVectorizer(non_negative=True)\n", "def test_hashing_vectorizer_with_custom_hasher():\n    class CustomHasher:\n            self.n_features = n_features\n            self.dtype = dtype\n\n            return self\n\n            data = np.zeros(len(X), dtype=self.dtype)\n            indices = np.zeros(len(X), dtype=np.int32)\n            indptr = np.arange(len(X) + 1, dtype=np.int32)\n            return sp.csr_matrix((data, indices, indptr),\n                                shape=(len(X), self.n_features))\n\n    vectorizer = HashingVectorizer(n_features=2 ** 20, dtype=np.float64)\n    vectorizer._get_hasher = lambda: CustomHasher(n_features=vectorizer.n_features,\n                                                  dtype=vectorizer.dtype)\n    X = vectorizer.transform(ALL_FOOD_DOCS)\n    assert_equal(X.shape, (len(ALL_FOOD_DOCS), vectorizer.n_features))\n    assert_equal(X.dtype, vectorizer.dtype)\n", "def test_vectorizer_max_df_int_value():\n    # Test max_df parameter with an int value\n    test_data = ['abc', 'dea', 'eat']\n    vect = CountVectorizer(analyzer='char', max_df=1)\n    vect.fit(test_data)\n    assert_true('a' not in vect.vocabulary_.keys())\n    assert_true('c' not in vect.vocabulary_.keys())  # ignored by min_df=1\n    assert_true('d' not in vect.vocabulary_.keys())  # ignored by max_df=1\n    assert_true('t' in vect.vocabulary_.keys())\n    assert_true('e' in vect.vocabulary_.keys())\n    assert_equal(len(vect.vocabulary_.keys()), 2)\n    assert_true('a' in vect.stop_words_)\n    assert_true('c' in vect.stop_words_)\n    assert_true('d' in vect.stop_words_)\n    assert_equal(len(vect.stop_words_), 3)\n\n    vect = CountVectorizer(analyzer='char', max_df=2)\n    vect.fit(test_data)\n    assert_true('a' in vect.vocabulary_.keys())\n    assert_true('c' not in vect.vocabulary_.keys())  # ignored by min_df=1\n    assert_true('d' in vect.vocabulary_.keys())\n    assert_true('t' in vect.vocabulary_.keys())\n    assert_true('e' in vect.vocabulary_.keys())\n    assert_equal(len(vect.vocabulary_.keys()), 3)\n    assert_true('a' not in vect.stop_words_)\n    assert_true('c' in vect.stop_words_)\n    assert_true('d' not in vect.stop_words_)\n    assert_equal(len(vect.stop_words_), 1)\n\n    vect = CountVectorizer(analyzer='char', max_df=3)\n    vect.fit(test_data)\n    assert_true('a' in vect.vocabulary_.keys())\n    assert_true('c' not in vect.vocabulary_.keys())  # ignored by min_df=1\n    assert_true('d' in vect.vocabulary_.keys())\n    assert_true('t' in vect.vocabulary_.keys())\n    assert_true('e' in vect.vocabulary_.keys())\n    assert_equal(len(vect.vocabulary_.keys()), 3)\n    assert_true('a' not in vect.stop_words_)\n    assert_true('c' in vect.stop_words_)\n    assert_true('d' not in vect.stop_words_)\n    assert_equal(len(vect.stop_words"], "sample_1149": ["def test_SingletonRegistry():\n    # Test SingletonRegistry.__call__\n    assert S(1) == 1\n    assert S(1) is not 1\n    assert isinstance(S(1), Basic)\n\n    # Test SingletonRegistry.register\n    class TestClass(Basic):\n        pass\n    S.register(TestClass)\n    assert hasattr(S, 'TestClass')\n    assert S.TestClass is TestClass()\n\n    # Test SingletonRegistry.__getattr__\n    delattr(S, 'TestClass')\n    S._classes_to_install['TestClass'] = TestClass\n    assert hasattr(S, 'TestClass')\n    assert S.TestClass is TestClass()\n\n    # Test SingletonRegistry.__repr__\n    assert repr(S) == 'S'\n", "def test_S_call():\n    # Test the __call__ method of S\n    assert S(5) == 5\n    assert S(1.5) == 1.5\n    assert S('x') == Basic.Symbol('x')\n", "def test_SingletonRegistry():\n    registry = SingletonRegistry()\n\n    # Test that the __call__ method of the SingletonRegistry instance\n    # behaves like sympify\n    assert registry(1) == sympify(1)\n\n    # Test that __getattr__ raises an AttributeError if the class is not\n    # registered\n    try:\n        registry.NonExistingAttribute\n        assert False\n    except AttributeError:\n        pass\n\n    # Test that __getattr__ returns the instance of the class if it is\n    # registered\n    class TestClass(Basic, metaclass=Singleton):\n        pass\n\n    registry.register(TestClass)\n    assert registry.TestClass is TestClass()\n", "def test_S_class_access_before_installation():\n    class TestClass(Basic, metaclass=Singleton):\n        pass\n\n    assert hasattr(S, 'TestClass') is False\n    assert S._classes_to_install['TestClass'].__name__ == 'TestClass'\n    assert hasattr(S, 'TestClass') is True\n    assert S.TestClass is TestClass()\n", "def test_singleton_repr():\n    assert repr(S) == \"S\"\n"], "sample_377": ["    def test_exception_reporter_with_threading(self):\n        \"\"\"An exception report can be generated for a request with a Thread.\"\"\"\n        request = self.rf.get(\"/test_view/\")\n        thread = threading.Thread(target=lambda: None)\n\n        try:\n            raise ValueError(\"Oops\")\n        except ValueError:\n            exc_type, exc_value, tb = sys.exc_info()\n\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n        html = reporter.get_traceback_html()\n        self.assertInHTML(\"<h1>ValueError at /test_view/</h1>\", html)\n        self.assertIn('<pre class=\"exception_value\">Oops</pre>', html)\n        self.assertIn(\"<th>Request Method:</th>\", html)\n        self.assertIn(\"<th>Request URL:</th>\", html)\n        self.assertIn('<h3 id=\"user-info\">USER</h3>', html)\n        self.assertIn(\"<p>[unable to retrieve the current user]</p>\", html)\n        self.assertIn(\"<h2>Request information</h2>\", html)\n        self.assertIn(\"<p>No POST data</p>\", html)\n\n        text = reporter.get_traceback_text()\n        self.assertIn(\"ValueError at /test_view/\", text)\n        self.assertIn(\"Oops\", text)\n        self.assertIn(\"Request Method:\", text)\n        self.assertIn(\"Request URL:\", text)\n        self.assertIn(\"USER: [unable to retrieve the current user]\", text)\n        self.assertIn(\"Request information:\", text)\n        self.assertIn(\"No POST data\", text)\n", "    def test_unicode_in_exception_chain(self):\n        \"\"\"\n        Unicode characters in the exception chain are handled correctly.\n        \"\"\"\n        try:\n            try:\n                raise ValueError(\"Oops\")\n            except Exception:\n                raise RuntimeError(\"R\u00e9sum\u00e9\") from None\n        except Exception:\n            exc_type, exc_value, tb = sys.exc_info()\n\n        reporter = ExceptionReporter(None, exc_type, exc_value, tb)\n        html = reporter.get_traceback_html()\n        self.assertInHTML(\"<h1>RuntimeError</h1>\", html)\n        self.assertIn(\n            '<pre class=\"exception_value\">R\u00e9sum\u00e9</pre>', html\n        )\n        self.assertIn(\"<th>Exception Type:</th>\", html)\n        self.assertIn(\"<th>Exception Value:</th>\", html)\n        self.assertIn(\"<h2>Traceback \", html)\n        self.assertIn(\"<h2>Request information</h2>\", html)\n        self.assertIn(\"<p>Request data not supplied</p>\", html)\n        self.assertIn(\n            \"During handling of the above exception (Oops), another exception \"\n            \"occurred\",\n            html,\n        )\n", "    def test_setting_allows_custom_subclass(self):\n        self.assertIsInstance(\n            get_default_exception_reporter_filter(),\n            CustomExceptionReporterFilter,\n        )\n", "    def test_encoding_error_non_ascii_filename(self):\n        \"\"\"\n        A UnicodeError displays a portion of the problematic string. HTML in\n        safe strings is escaped. Non-ASCII filename is handled.\n        \"\"\"\n        try:\n            mark_safe(\"abcdefghijkl<p>mn\u1f40p</p>qrstuwxyz\").encode(\"ascii\")\n        except Exception:\n            exc_type, exc_value, tb = sys.exc_info()\n        reporter = ExceptionReporter(None, exc_type, exc_value, tb)\n        with tempfile.NamedTemporaryFile(prefix=\"r\u00e9sum\u00e9\") as tmpfile:\n            with open(tmpfile.name, \"w\", encoding=\"utf-8\") as f:\n                f.write(\"print('Hello, World!')\")\n            with mock.patch(\n                \"django.views.debug.ExceptionReporter._get_source\",\n                return_value=[\"print('Hello, World!')\"],\n            ):\n                html = reporter.get_traceback_html()\n        self.assertIn(\"<h2>Unicode error hint</h2>\", html)\n        self.assertIn(\"The string that could not be encoded/decoded was: \", html)\n        self.assertIn(\"<strong>&lt;p&gt;mn\u1f40p&lt;/p&gt;</strong>\", html)\n", "    def test_unhandled_exception_filtering(self):\n        \"\"\"\n        An unhandled exception should not be filtered by the SafeExceptionReporterFilter.\n        \"\"\"\n        try:\n            raise SystemExit\n        except SystemExit:\n            exc_type, exc_value, tb = sys.exc_info()\n\n        reporter = ExceptionReporter(None, exc_type, exc_value, tb)\n        html = reporter.get_traceback_html()\n        self.assertIn(\"SystemExit\", html)\n        self.assertIn(\"<pre class='exception_value'></pre>\", html)\n"], "sample_640": ["def test_is_overload_stub() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n    from typing import overload\n\n    @overload\n    @overload\n    \"\"\"\n    )\n    assert utils.is_overload_stub(code[0])\n    assert utils.is_overload_stub(code[1])\n    assert not utils.is_overload_stub(code[2])\n", "def test_returns_bool() -> None:\n    node = astroid.extract_node(\"return True\")\n    assert utils.returns_bool(node)\n    node = astroid.extract_node(\"return False\")\n    assert utils.returns_bool(node)\n    node = astroid.extract_node(\"return 42\")\n    assert not utils.returns_bool(node)\n    node = astroid.extract_node(\"print(42)\")\n    assert not utils.returns_bool(node)\n", "def test_returns_bool() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n        return True\n\n        return False\n\n        return 1\n    \"\"\"\n    )\n    assert isinstance(code, list) and len(code) == 3\n\n    assert isinstance(code[0], nodes.FunctionDef)\n    assert utils.returns_bool(code[0].returns) is True\n    assert isinstance(code[1], nodes.FunctionDef)\n    assert utils.returns_bool(code[1].returns) is True\n\n    assert isinstance(code[2], nodes.FunctionDef)\n    assert utils.returns_bool(code[2].returns) is False\n", "def test_is_subscriptable_pep585_with_postponed_evaluation_enabled() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n    from __future__ import annotations\n    class Test:\n            return cls\n\n    test = Test[1]  #@\n    \"\"\"\n    )\n    assert utils.is_class_subscriptable_pep585_with_postponed_evaluation_enabled(\n        code[-2], code[-1]\n    )\n", "def test_parse_format_string() -> None:\n    samples = [\n        (\"%s\", ({\"\": {}}, 1, {}, [\"s\"])),\n        (\"%s %s\", ({\"\": {}, \"\" : {}}, 2, {}, [\"s\", \"s\"])),\n        (\"%(name)s\", ({\"name\": {}}, 1, {\"name\": \"s\"}, [\"s\"])),\n        (\"%(name)s %(name)s\", ({\"name\": {}}, 2, {\"name\": \"s\"}, [\"s\", \"s\"])),\n        (\"%d %s\", ({\"\": {}}, 2, {}, [\"d\", \"s\"])),\n        (\"%(name)d %(name)s\", ({\"name\": {}}, 2, {\"name\": \"d\"}, [\"d\", \"s\"])),\n        (\"%c %s\", ({\"\": {}}, 2, {}, [\"c\", \"s\"])),\n        (\"%(name)c %(name)s\", ({\"name\": {}}, 2, {\"name\": \"c\"}, [\"c\", \"s\"])),\n        (\"%.2f %s\", ({\"\": {}}, 2, {}, [\"f\", \"s\"])),\n        (\"%(name).2f %(name)s\", ({\"name\": {}}, 2, {\"name\": \"f\"}, [\"f\", \"s\"])),\n        (\"%i %s\", ({\"\": {}}, 2, {}, [\"i\", \"s\"])),\n        (\"%(name)i %(name)s\", ({\"name\": {}}, 2, {\"name\": \"i\"}, [\"i\", \"s\"])),\n        (\"%o %s\", ({\"\": {}}, 2, {}, [\"o\", \"s\"])),\n        (\"%(name)o %(name)s\", ({\"name\": {}}, 2, {\"name\": \"o\"}, [\"o\", \"s\"])),\n        (\"%u %s\", ({\"\": {}}, 2, {}, [\"u\", \"s\"])),\n        (\"%(name)u %(name)s\", ({\"name\": {}}, 2, {\"name\": \"u\"}, [\"u\", \"s\"])),\n        (\"%x %s\", ({\"\": {}}, 2, {}, [\"x\", \"s\"])),\n        (\"%(name)x %(name)s\", ({\"name\": {}}, 2, {\"name\": \"x\"}, [\"x\", \"s\"])),\n        (\"%(name)#x %(name)s\", ({\"name\": {}}, 2, {\"name\":"], "sample_634": ["    def test_expand_modules_with_errors(self, files_or_modules, expected_errors):\n        \"\"\"Test expand_modules with non-existent modules/files\"\"\"\n        ignore_list, ignore_list_re = [], []\n        modules, errors = expand_modules(\n            files_or_modules,\n            ignore_list,\n            ignore_list_re,\n            get_global_option(self, \"ignore-paths\"),\n        )\n        assert modules == []\n        assert len(errors) == len(expected_errors)\n        for error in errors:\n            assert \"key\" in error and error[\"key\"] == \"fatal\"\n            assert \"mod\" in error and error[\"mod\"] in [f[\"mod\"] for f in expected_errors]\n            assert isinstance(error[\"ex\"], type(next(f[\"ex\"] for f in expected_errors if f[\"mod\"] == error[\"mod\"])))\n", "    def test_expand_modules_with_import_errors(self, files_or_modules, expected_errors):\n        \"\"\"Test expand_modules with files or modules that cause import errors\"\"\"\n        ignore_list, ignore_list_re = [], []\n        modules, errors = expand_modules(\n            files_or_modules,\n            ignore_list,\n            ignore_list_re,\n            get_global_option(self, \"ignore-paths\"),\n        )\n        assert modules == []\n        assert errors == expected_errors\n", "def test_expand_modules_import_error(self, files_or_modules, expected_errors):\n    \"\"\"Test expand_modules with modules that cause an ImportError\"\"\"\n    ignore_list, ignore_list_re = [], []\n    _, errors = expand_modules(\n        files_or_modules,\n        ignore_list,\n        ignore_list_re,\n        get_global_option(self, \"ignore-paths\"),\n    )\n    assert len(errors) == 1\n    assert all(expected_error in str(errors[0].get(\"ex\", \"\")) for expected_error in expected_errors)\n", "    def test_expand_modules_with_nonexistent_module(self, files_or_modules, expected):\n        \"\"\"Test expand_modules with a nonexistent module or package\"\"\"\n        ignore_list, ignore_list_re = [], []\n        modules, errors = expand_modules(\n            files_or_modules,\n            ignore_list,\n            ignore_list_re,\n            get_global_option(self, \"ignore-paths\"),\n        )\n        assert modules == expected\n        assert len(errors) == 1\n", "    def test_expand_modules_errors(self, files_or_modules, expected_errors, expected_modules):\n        \"\"\"Test expand_modules with errors\"\"\"\n        ignore_list, ignore_list_re = [], []\n        modules, errors = expand_modules(\n            files_or_modules,\n            ignore_list,\n            ignore_list_re,\n            get_global_option(self, \"ignore-paths\"),\n        )\n        assert len(errors) == expected_errors\n        assert modules == expected_modules\n"], "sample_331": ["    def test_parse_duration_with_trailing_whitespace(self):\n        test_values = (\n            ('4 15:30 ', timedelta(days=4, minutes=15, seconds=30)),\n            ('-172800 ', timedelta(days=-2)),\n            ('-15:30 ', timedelta(minutes=-15, seconds=-30)),\n            ('-1:15:30 ', timedelta(hours=-1, minutes=-15, seconds=-30)),\n            ('-30.1 ', timedelta(seconds=-30, milliseconds=-100)),\n            ('-30,1 ', timedelta(seconds=-30, milliseconds=-100)),\n            ('-00:01:01 ', timedelta(minutes=-1, seconds=-1)),\n            ('-01:01 ', timedelta(seconds=-61)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "def test_parse_duration_unsupported_formats(self):\n    # Test unsupported formats\n    self.assertIsNone(parse_duration('1 month'))\n    self.assertIsNone(parse_duration('1 year'))\n    self.assertIsNone(parse_duration('P1Y'))\n    self.assertIsNone(parse_duration('P1M'))\n    self.assertIsNone(parse_duration('P1W'))\n    self.assertIsNone(parse_duration('PT1F'))  # Fractional seconds\n    self.assertIsNone(parse_duration('1:1:1.1.1'))\n    self.assertIsNone(parse_duration('1:1:1.1,1'))\n    self.assertIsNone(parse_duration('1 1:1:1.1.1'))\n    self.assertIsNone(parse_duration('1 1:1:1.1,1'))\n", "    def test_parse_duration_edge_cases(self):\n        # Test large values\n        self.assertEqual(parse_duration('1000000 days'), timedelta(days=1000000))\n        self.assertEqual(parse_duration('1000000:59:59'), timedelta(days=1000000, hours=23, minutes=59, seconds=59))\n        self.assertEqual(parse_duration('1000000 days 1000000:59:59'), timedelta(days=2000000, hours=23, minutes=59, seconds=59))\n\n        # Test negative values\n        self.assertEqual(parse_duration('-1000000 days'), timedelta(days=-1000000))\n        self.assertEqual(parse_duration('-1000000:59:59'), timedelta(days=-1000000, hours=-1, minutes=-1, seconds=-1))\n        self.assertEqual(parse_duration('-1000000 days -1000000:59:59'), timedelta(days=-2000000, hours=-1, minutes=-1, seconds=-1))\n\n        # Test invalid values\n        self.assertIsNone(parse_duration('1000000:60:59'))\n        self.assertIsNone(parse_duration('1000000:59:60'))\n        self.assertIsNone(parse_duration('1000000 days 1000000:60:59'))\n        self.assertIsNone(parse_duration('1000000 days 1000000:59:60'))\n\n        # Test extreme fraction of seconds\n        self.assertEqual(parse_duration('1:1:1.999999'), timedelta(hours=1, minutes=1, seconds=1, microseconds=999999))\n        self.assertEqual(parse_duration('1:1:1.000001'), timedelta(hours=1, minutes=1, seconds=1, microseconds=1))\n        self.assertEqual(parse_duration('1:1:1.0000001'), timedelta(hours=1, minutes=1, seconds=1, microseconds=1))\n", "    def test_empty_string(self):\n        self.assertIsNone(parse_duration(''))\n", "    def test_invalid_input(self):\n        test_values = (\n            '  10:15:30',  # leading whitespace\n            '10:15:30  ',  # trailing whitespace\n            '10:15:30.1.1',  # multiple decimal points\n            '10:15:30.1,1',  # both decimal point and comma\n            '10:15:30,1.1',  # both comma and decimal point in wrong order\n            '10:15:30:40',  # extra colon\n            '10:15:30::40',  # extra colon\n            '10:15.30',  # missing second colon\n            ':15:30',  # missing hour\n            '10::30',  # missing minute\n            '10:15:',  # missing seconds\n            '10:15:30.1.1.1',  # multiple decimal points and commas\n            '10:15:30,1,1',  # multiple commas\n            '10:15:30.1,.1',  # decimal point and comma in wrong order\n            '10:15:30,1.1,1',  # decimal point and comma in wrong order\n        )\n        for source in test_values:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_duration(source))\n"], "sample_275": ["    def test_delete_with_prefetch_related(self):\n        \"\"\"\n        Test that delete() works correctly when prefetch_related() is used.\n        \"\"\"\n        book1 = Book.objects.create(pagecount=100)\n        book2 = Book.objects.create(pagecount=200)\n        award1 = Award.objects.create(name='Nobel', content_object=book1)\n        award2 = Award.objects.create(name='Pulitzer', content_object=book2)\n        Award.objects.prefetch_related('content_object').filter(name='Nobel').delete()\n        self.assertEqual(Award.objects.count(), 1)\n        self.assertEqual(Book.objects.count(), 1)\n", "    def test_none(self):\n        qs = Book.objects.none()\n        self.assertIsInstance(qs, EmptyQuerySet)\n        self.assertEqual(len(qs), 0)\n        self.assertEqual(bool(qs), False)\n        with self.assertRaises(TypeError):\n            qs.filter()\n        with self.assertRaises(TypeError):\n            qs.exclude()\n        with self.assertRaises(TypeError):\n            qs.order_by()\n        with self.assertRaises(TypeError):\n            qs.reverse()\n        with self.assertRaises(TypeError):\n            qs.distinct()\n        with self.assertRaisesMessage(TypeError, 'Cannot call delete() after .none().'):\n            qs.delete()\n        self.assertEqual(repr(qs), '<EmptyQuerySet []>')\n", "    def test_prefetch_related_on_to_field(self):\n        \"\"\"\n        Test that prefetch_related works with ForeignKey.to_field set to non-PK.\n        \"\"\"\n        food = Food.objects.create(name=\"Food\")\n        meal = Eaten.objects.create(food=food, meal=\"Breakfast\")\n        Eaten.objects.create(food=food, meal=\"Dinner\")\n\n        # Prefetch the related objects.\n        meals = Eaten.objects.prefetch_related('food').all()\n\n        # Access the related objects.\n        for m in meals:\n            self.assertEqual(m.food.name, \"Food\")\n", "    def test_filtered_relation(self):\n        book1 = Book.objects.create(pagecount=100)\n        book2 = Book.objects.create(pagecount=200)\n        note1 = AwardNote.objects.create(note='note 1', award=book1)\n        note2 = AwardNote.objects.create(note='note 2', award=book2)\n\n        filtered_qs = Award.objects.filter(note__note='note 1')\n        self.assertEqual(filtered_qs.count(), 1)\n        self.assertEqual(filtered_qs[0].id, book1.id)\n", "    def test_empty_queryset_iteration(self):\n        # Test iterating over an empty queryset\n        qs = Book.objects.none()\n        self.assertEqual(list(qs), [])\n"], "sample_88": ["def test_forbid_multi_line_headers(self):\n    with self.assertRaises(BadHeaderError):\n        forbid_multi_line_headers('Subject', 'Test\\nHeader', 'utf-8')\n\n    with self.assertRaises(BadHeaderError):\n        forbid_multi_line_headers('Subject', 'Test\\rHeader', 'utf-8')\n\n    name, val = forbid_multi_line_headers('Subject', 'Test Header', 'utf-8')\n    self.assertEqual(name, 'Subject')\n    self.assertEqual(val, 'Test Header')\n\n    # Test with a lazy translation.\n    from django.utils.translation import gettext_lazy\n    lazy_val = gettext_lazy('Test\\nHeader')\n    with self.assertRaises(BadHeaderError):\n        forbid_multi_line_headers('Subject', lazy_val, 'utf-8')\n\n    lazy_val = gettext_lazy('Test Header')\n    name, val = forbid_multi_line_headers('Subject', lazy_val, 'utf-8')\n    self.assertEqual(name, 'Subject')\n    self.assertEqual(val, 'Test Header')\n", "    def test_attachment_filename_encoding(self):\n        \"\"\"\n        Regression test for #15042\n        \"\"\"\n        email = EmailMessage('Subject', 'Content', 'from@example.com', ['to@example.com'])\n        email.attach(\"une pi\u00e8ce jointe.pdf\", b\"%PDF-1.4.%...\", mimetype=\"application/pdf\")\n        email.attach(\"une pi\u00e8ce jointe2.txt\", b\"Text file content\", mimetype=\"text/plain\")\n        filename1, content1, mimetype1 = self.get_decoded_attachments(email)[0]\n        filename2, content2, mimetype2 = self.get_decoded_attachments(email)[1]\n        self.assertEqual(filename1, 'une pi\u00e8ce jointe.pdf')\n        self.assertEqual(content1, b\"%PDF-1.4.%...\")\n        self.assertEqual(mimetype1, 'application/pdf')\n        self.assertEqual(filename2, 'une pi\u00e8ce jointe2.txt')\n        self.assertEqual(content2, b'Text file content')\n        self.assertEqual(mimetype2, 'text/plain')\n", "def test_mime_attachment_with_bytes_content(self):\n    msg = EmailMessage('Subject', 'Content', 'from@example.com', ['to@example.com'])\n    msg.attach('file.txt', b'file content', 'text/plain')\n    message = msg.message()\n    self.assertIsInstance(message, MIMEMultipart)\n    self.assertEqual(len(message.get_payload()), 1)\n    payload = message.get_payload()[0]\n    self.assertIsInstance(payload, MIMEText)\n    self.assertEqual(payload.get_payload(decode=True), b'file content')\n    self.assertEqual(payload.get_content_type(), 'text/plain')\n    self.assertEqual(payload.get('Content-Disposition'), 'attachment; filename=\"file.txt\"')\n", "def test_attachment_with_message_rfc822(self):\n    \"\"\"\n    Attachment with mimetype 'message/rfc822' is sent correctly.\n    \"\"\"\n    # Create a child message\n    child_msg = EmailMessage(\n        'Child Subject', 'Some body of child message', 'bounce@example.com', ['to@example.com'],\n        headers={'From': 'from@example.com'},\n    )\n\n    # Now create a parent\n    parent_msg = EmailMessage(\n        'Parent Subject', 'Some parent body', 'bounce@example.com', ['to@example.com'],\n        headers={'From': 'from@example.com'},\n    )\n\n    # Attach to parent as an EmailMessage object\n    parent_msg.attach(content=child_msg, mimetype='message/rfc822')\n    parent_msg.send()\n\n    message = self.get_the_message()\n    self.assertEqual(message.get_content_type(), 'multipart/mixed')\n    self.assertEqual(len(message.get_payload()), 2)\n    self.assertEqual(message.get_payload(0).get_payload(), 'Some parent body')\n    self.assertEqual(message.get_payload(1).get_content_type(), 'message/rfc822')\n    self.assertEqual(message.get_payload(1).get_payload()[0].get('Subject'), 'Child Subject')\n", "def test_custom_mime_attachment(self):\n    \"\"\"\n    Regression test for #27563 - Custom mime attachment types.\n    \"\"\"\n    subject, from_email, to = 'hello', 'from@example.com', 'to@example.com'\n    content = '<p>This is an <strong>important</strong> message.</p>'\n    msg = EmailMessage(subject, content, from_email, [to])\n    msg.attach('example.xhtml', content, mimetype='application/xhtml+xml')\n    msg_bytes = msg.message().as_bytes()\n    message = message_from_bytes(msg_bytes)\n    payload = message.get_payload()\n    self.assertEqual(payload[1].get_content_type(), 'application/xhtml+xml')\n    self.assertEqual(payload[1].get_payload(), content)\n"], "sample_228": ["def test_absolute_max_with_zero_total_forms(self):\n    data = {\n        'form-TOTAL_FORMS': '0',\n        'form-INITIAL_FORMS': '0',\n        'form-MAX_NUM_FORMS': '0',\n    }\n    LimitedFavoriteDrinksFormSet = formset_factory(\n        FavoriteDrinkForm,\n        max_num=30,\n        absolute_max=1000,\n    )\n    formset = LimitedFavoriteDrinksFormSet(data=data)\n    self.assertIs(formset.is_valid(), True)\n    self.assertEqual(len(formset.forms), 0)\n    self.assertEqual(\n        formset.non_form_errors(),\n        []\n    )\n", "    def test_absolute_max_with_min_num(self):\n        data = {\n            'form-TOTAL_FORMS': '2',  # the number of forms rendered\n            'form-INITIAL_FORMS': '0',  # the number of forms with initial data\n            'form-MIN_NUM_FORMS': '3',  # min number of forms\n            'form-MAX_NUM_FORMS': '0',  # max number of forms - should be ignored\n        }\n        LimitedFavoriteDrinksFormSet = formset_factory(\n            FavoriteDrinkForm,\n            absolute_max=3,\n            min_num=3,\n        )\n        formset = LimitedFavoriteDrinksFormSet(data=data)\n        self.assertIs(formset.is_valid(), False)\n        self.assertEqual(\n            formset.non_form_errors(),\n            ['Please submit at least 3 forms.'],\n        )\n        self.assertEqual(formset.absolute_max, 3)\n\n        data['form-TOTAL_FORMS'] = '3'\n        formset = LimitedFavoriteDrinksFormSet(data=data)\n        self.assertIs(formset.is_valid(), True)\n", "    def test_total_form_count_edge_case(self):\n        # Test edge case where absolute_max is lower than total form count from data\n        data = {\n            'choices-TOTAL_FORMS': '1001',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-MAX_NUM_FORMS': '0',\n        }\n        ChoiceFormSet = formset_factory(Choice, extra=1, max_num=1000)\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertEqual(formset.total_form_count(), 1000)\n        self.assertEqual(len(formset.forms), 1000)\n", "    def test_formset_factory_invalid_params(self):\n        with self.assertRaisesMessage(ValueError, \"'absolute_max' must be greater or equal to 'max_num'.\"):\n            formset_factory(FavoriteDrinkForm, max_num=1, absolute_max=0)\n\n        with self.assertRaisesMessage(ValueError, \"'absolute_max' must be greater or equal to 'max_num'.\"):\n            formset_factory(FavoriteDrinkForm, max_num=2001, absolute_max=2000)\n\n        with self.assertRaisesMessage(ValueError, \"'absolute_max' must be greater or equal to 'max_num'.\"):\n            formset_factory(FavoriteDrinkForm, max_num=None, absolute_max=1000)\n\n        with self.assertRaisesMessage(ValueError, \"'absolute_max' must be greater or equal to 'max_num'.\"):\n            formset_factory(FavoriteDrinkForm, max_num=-1, absolute_max=1000)\n\n        with self.assertRaisesMessage(ValueError, \"'absolute_max' must be greater or equal to 'max_num'.\"):\n            formset_factory(FavoriteDrinkForm, max_num=0, absolute_max=-1)\n", "def test_total_form_count(self):\n    \"\"\"\n    Test the total_form_count() method under various scenarios.\n    \"\"\"\n    # When the formset is unbound, the total form count is determined by the\n    # extra and initial parameters.\n    ChoiceFormSet = formset_factory(Choice)\n    formset = ChoiceFormSet()\n    self.assertEqual(formset.total_form_count(), 1)\n\n    ChoiceFormSet = formset_factory(Choice, extra=5)\n    formset = ChoiceFormSet()\n    self.assertEqual(formset.total_form_count(), 5)\n\n    ChoiceFormSet = formset_factory(Choice, extra=5, min_num=3)\n    formset = ChoiceFormSet()\n    self.assertEqual(formset.total_form_count(), 5)\n\n    ChoiceFormSet = formset_factory(Choice, extra=0, min_num=3)\n    formset = ChoiceFormSet()\n    self.assertEqual(formset.total_form_count(), 3)\n\n    # When the formset is bound, the total form count is determined by the\n    # TOTAL_FORMS value in the data, unless this value exceeds the absolute\n    # maximum.\n    ChoiceFormSet = formset_factory(Choice)\n    formset = ChoiceFormSet({\n        'choices-TOTAL_FORMS': '5',\n        'choices-INITIAL_FORMS': '0',\n    })\n    self.assertEqual(formset.total_form_count(), 5)\n\n    ChoiceFormSet = formset_factory(Choice)\n    formset = ChoiceFormSet({\n        'choices-TOTAL_FORMS': '1001',\n        'choices-INITIAL_FORMS': '0',\n    })\n    self.assertEqual(formset.total_form_count(), 1000)\n\n    # If the formset has initial data, this is taken into account when\n    # determining the total form count.\n    ChoiceFormSet = formset_factory(Choice, extra=0, min_num=0)\n    formset = ChoiceFormSet({\n        'choices-TOTAL_FORMS': '1',\n        'choices-INITIAL_FORMS': '2',\n    }, initial=[{'choice': 'Calexico', 'votes': 100}, {'choice': 'The Decemberists', 'votes': 500}])\n    self.assertEqual(formset.total_form_count(), 2)\n\n    ChoiceFormSet = formset_factory(Choice, extra=2, min_num=0)\n    formset = ChoiceFormSet({\n        'choices-TOTAL_FORMS': '1',\n       "], "sample_641": ["def test_load_result_non_existent_file(path: str) -> None:\n    loaded = load_results(path)\n    assert loaded is None\n", "def test_load_result_file_not_found(path: str) -> None:\n    loaded = load_results(path)\n    assert loaded is None\n\n", "def test_load_results_non_existent_file(tmp_path: Path) -> None:\n    assert load_results(tmp_path / \"non_existent_file\") is None\n\n", "def test_load_results_non_existent_file(tmp_path: Path) -> None:\n    with pytest.raises(FileNotFoundError):\n        load_results(tmp_path)\n\n", "def test_load_result_non_existent_file(tmp_path: Path) -> None:\n    non_existent_path = tmp_path / \"non_existent_file.txt\"\n    assert load_results(non_existent_path) is None\n\n"], "sample_372": ["    def test_prefix_default_language(self):\n        language_code = 'en'\n        prefix = LocalePrefixPattern(prefix_default_language=True)\n        self.assertEqual(prefix.language_prefix, '%s/' % language_code)\n        prefix = LocalePrefixPattern(prefix_default_language=False)\n        self.assertEqual(prefix.language_prefix, '')\n", "    def test_get(self):\n        class TestClass:\n            regex = LocaleRegexDescriptor('regex')\n\n                self.regex = 'test'\n\n        obj = TestClass()\n        self.assertEqual(obj.regex, re.compile('test'))\n", "    def test_language_prefix(self):\n        from django.conf import settings\n        settings.LANGUAGE_CODE = 'en'\n        settings.USE_I18N = True\n        resolver = URLResolver(RegexPattern(r'^/'), 'urlpatterns_reverse.urls')\n        prefix = LocalePrefixPattern()\n        resolver.pattern = prefix\n        self.assertEqual(prefix.regex.pattern, '^en/')\n", "    def test_locale_url_resolving(self):\n        # Test that URLs with locales can be resolved\n        url = '/en/test/'\n        match = resolve(url)\n        self.assertEqual(match.func.__name__, 'empty_view')\n", "    def test_regex_pattern_match(self):\n        pattern = RegexPattern(r'^normal/(?P<arg1>[0-9]+)/(?P<arg2>[0-9]+)/$')\n        self.assertEqual(pattern.match('/normal/42/37/'), ('', ('42', '37'), {'arg1': '42', 'arg2': '37'}))\n        self.assertIsNone(pattern.match('/normal/42/'))\n"], "sample_932": ["def test_qualified_template_parameters():\n    check('function', 'template<typename T> void f(T T::*)',\n          {2: '1fM1TKi'})\n    check('function', 'template<typename T> void f(T T::*const)',\n          {2: '1fKM1TKi'})\n    check('function', 'template<typename T> void f(T T::*volatile)',\n          {2: '1fVM1TKi'})\n    check('function', 'template<typename T> void f(T T::*const volatile)',\n          {2: '1fVKM1TKi'})\n    check('function', 'template<typename T> void f(T T::*const volatile)',\n          {2: '1fVKM1TKi'},\n          output='template<typename T> void f(T T::*volatile const)')\n    check('function', 'template<typename T> void f(int T::*p)',\n          {2: '1fM1TKi'})\n    check('function', 'template<typename T> void f(int T::*const)',\n          {2: '1fKM1TKi'})\n    check('function', 'template<typename T> void f(int T::*volatile)',\n          {2: '1fVM1TKi'})\n    check('function', 'template<typename T> void f(int T::*const volatile)',\n          {2: '1fVKM1TKi'})\n    check('function', 'template<typename T> void f(int T::*const volatile)',\n          {2: '1fVKM1TKi'},\n          output='template<typename T> void f(int T::*volatile const)')\n    check('function', 'template<typename T> void f(int (T::*)(float, double))',\n          {2: '1fM1TKFifdE'})\n    check('function', 'template<typename T> void f(int (T::* p)(float, double))',\n          {2: '1fM1TKFifdE'})\n    check('function', 'template<typename T> void f(int (::T::* p)(float, double))',\n          {2: '1fM1TKFifdE'})\n    check('function', 'template<typename T> void f(void (T::*)() const &)',\n          {2: '1fM1TKRFvvE'})\n    check('function', 'template<typename T> int T::* f(int, double)',\n          {2: '1fM", "def test_enum_template_scoped():\n    check('enum', 'template<template<int T> class U> {key}enum struct A',\n          {2: 'I0E7A'})\n    check('enum', 'template<template<int T> class U> {key}enum class A',\n          {2: 'I0E7A'})\n    check('enum', 'template<typename T> {key}enum struct A',\n          {2: 'I0E7A'})\n    check('enum', 'template<typename T> {key}enum class A',\n          {2: 'I0E7A'})\n", "def test_id_generation_with_template():\n    # check id generation for template template parameters\n    check('function', 'template<template<typename> typename T> void f(T)',\n          {2: 'I0E1f', 4: 'I0E1fv'})\n    check('function', 'template<template<typename> class T> void f(T)',\n          {2: 'I0E1f', 4: 'I0E1fv'})\n    check('function', 'template<template<typename> typename... T> void f(T)',\n          {2: 'IDpE1f', 4: 'IDpE1fv'})\n    check('function', 'template<template<typename> typename... T> void f(T)',\n          {2: 'IDpE1f', 4: 'IDpE1fv'})\n", "def test_template_parameter_defaults():\n    # with default values\n    check('function', 'template <typename T = int> void f(T)', {2: 'I0E1fi'})\n    check('function', 'template <typename T = int> void f(T v = 0)', {2: 'I0E1fi'})\n    check('function', 'template <typename T = int, typename U = double> void f(T, U)',\n          {2: 'I00E1fi1U'})\n    check('function', 'template <typename T = int, typename U = double> void f(T, U v = 42)',\n          {2: 'I00E1fi1U'})\n    # with template parameter defaults\n    check('function', 'template <typename T = decltype(0)> void f(T)', {2: 'I0DcE1fi'})\n    check('function', 'template <typename T = T> void f(T)', {2: 'I0E1fi'})\n    check('function', 'template <typename T = T::type> void f(T)', {2: 'I0_N1T3typeE1fi'})\n    check('function', 'template <typename T = T::type, typename U = T::type> void f(T, U)',\n          {2: 'I0_N1T3typeE1fi_N1T3typeE'})\n    # with a default function parameter\n    check('function', 'template <typename T> void f(T, typename T::type = 42)', {2: 'I0E1fi_N1T3typeE'})\n    check('function', 'template <typename T> void f(T, typename T::type = T::value)', {2: 'I0E1fi_N1T5valueE'})\n    # with template parameter defaults and a default function parameter\n    check('function', 'template <typename T = int> void f(T, typename T::type = 42)', {2: 'I0E1fi_N1T3typeE'})\n    check('function', 'template <typename T = int> void f(T, typename T::type = T::value)', {2: 'I0E1fi_N1T5valueE'})\n", "def test_template_parameter_list_parens():\n    # test that the template parameter list parsing is correct\n    # for the different versions of the mangle scheme\n    # this is what makes the parsing of template parameter lists work correctly\n    # in v1\n    id_v1 = \"1v\"\n    # in v2, as it should be\n    id_v2 = \"I0E1v\"\n    # in v2, with an extra closing paren\n    id_v2_extra_paren = \"I0E1vE\"\n    # in v3, with the anchor\n    id_v3 = \"N1A1vE\"\n    # in v4, with the anchor\n    id_v4 = \"N1A1vE\"\n    check('class', 'template<({key}v)> A', {1: id_v1, 2: id_v2, 3: id_v3, 4: id_v4},\n          output='template<> {key}A')\n    check('class', 'template<({key}v)> A', {1: id_v1, 2: id_v2, 3: id_v3, 4: id_v4},\n          output='template<> {key}A')\n    check('class', 'template<({key}v)> > A', {1: id_v1, 2: id_v2_extra_paren, 3: id_v3, 4: id_v4},\n          output='template<> {key}A')\n    check('class', 'template<({key}v)> > A', {1: id_v1, 2: id_v2_extra_paren, 3: id_v3, 4: id_v4},\n          output='template<> {key}A')\n"], "sample_1201": ["def test_cgs_gauss_base_and_derived_units():\n    # Test that base units have a scale factor of 1\n    assert cgs_gauss.get_quantity_scale_factor(centimeter) == 1\n    assert cgs_gauss.get_quantity_scale_factor(gram) == 1\n    assert cgs_gauss.get_quantity_scale_factor(second) == 1\n\n    # Test that derived units have correct scale factors\n    assert cgs_gauss.get_quantity_scale_factor(statcoulomb) == centimeter**(S(3)/2)*gram**(S.Half)/second\n    assert cgs_gauss.get_quantity_scale_factor(statampere) == statcoulomb/second\n    assert cgs_gauss.get_quantity_scale_factor(statvolt) == erg/statcoulomb\n    assert cgs_gauss.get_quantity_scale_factor(gauss) == sqrt(gram/centimeter)/second\n    assert cgs_gauss.get_quantity_scale_factor(maxwell) == sqrt(centimeter**3*gram)/second\n\n    # Test that SI units have correct scale factors\n    assert cgs_gauss.get_quantity_scale_factor(coulomb) == 10*speed_of_light*statcoulomb\n    assert cgs_gauss.get_quantity_scale_factor(ampere) == 10*speed_of_light*statcoulomb/second\n    assert cgs_gauss.get_quantity_scale_factor(volt) == 10**6/speed_of_light*statvolt\n    assert cgs_gauss.get_quantity_scale_factor(weber) == 10**8*maxwell\n    assert cgs_gauss.get_quantity_scale_factor(tesla) == 10**4*gauss\n    assert cgs_gauss.get_quantity_scale_factor(debye) == One/10**18*statcoulomb*centimeter\n    assert cgs_gauss.get_quantity_scale_factor(oersted) == sqrt(gram/centimeter)/second\n    assert cgs_gauss.get_quantity_scale_factor(ohm) == 10**5/speed_of_light**2*second/centimeter\n    assert cgs_gauss.get_quantity_scale_factor(farad) == One/10**5*speed_of_light**2*centimeter\n    assert cgs_gauss.get_quantity_scale_factor(henry) == 10**5/speed_of_light**2/centimeter*second**2\n", "def test_cgs_gauss_base_units():\n    assert convert_to(centimeter, meter, cgs_gauss) == meter/100\n    assert convert_to(meter, centimeter, cgs_gauss) == 100*centimeter\n    assert convert_to(gram, kilogram, cgs_gauss) == kilogram/1000\n    assert convert_to(kilogram, gram, cgs_gauss) == 1000*gram\n    assert convert_to(second, second, cgs_gauss) == second\n    assert convert_to(second, minute, cgs_gauss) == minute/60\n", "def test_cgs_gauss_conversions():\n    # Test conversions between cgs_gauss units\n    assert convert_to(statampere, statcoulomb/second, cgs_gauss) == statampere\n    assert convert_to(statcoulomb/second, statampere, cgs_gauss) == statcoulomb/second\n    assert convert_to(statvolt, erg/statcoulomb, cgs_gauss) == statvolt\n    assert convert_to(erg/statcoulomb, statvolt, cgs_gauss) == erg/statcoulomb\n    assert convert_to(gauss, sqrt(gram/centimeter)/second, cgs_gauss) == gauss\n    assert convert_to(sqrt(gram/centimeter)/second, gauss, cgs_gauss) == sqrt(gram/centimeter)/second\n    assert convert_to(maxwell, sqrt(centimeter**3*gram)/second, cgs_gauss) == maxwell\n    assert convert_to(sqrt(centimeter**3*gram)/second, maxwell, cgs_gauss) == sqrt(centimeter**3*gram)/second\n    assert convert_to(statampere, ampere, cgs_gauss) == statampere/(2997924580*ampere)\n    assert convert_to(ampere, statampere, cgs_gauss) == 2997924580*statampere\n    assert convert_to(statvolt, volt, cgs_gauss) == statvolt/(10**6*299792458*volt)\n    assert convert_to(volt, statvolt, cgs_gauss) == 10**6*299792458*statvolt\n    assert convert_to(gauss, tesla, cgs_gauss) == gauss/(10**4*tesla)\n    assert convert_to(tesla, gauss, cgs_gauss) == 10**4*gauss\n    assert convert_to(maxwell, weber, cgs_gauss) == maxwell/(10**8*weber)\n    assert convert_to(weber, maxwell, cgs_gauss) == 10**8*maxwell\n    assert convert_to(debye, statcoulomb*centimeter, cgs_gauss) == debye/(10**18*statcoulomb*centimeter)\n    assert convert_to(statcoulomb*centimeter, debye, cgs_gauss) == 10", "def test_cgs_gauss_unit_conversions():\n    # Test conversion between cgs_gauss units\n    assert convert_to(statcoulomb, statampere*second, cgs_gauss) == statampere*second\n    assert convert_to(statampere, statcoulomb/second, cgs_gauss) == statcoulomb/second\n    assert convert_to(statvolt, erg/statcoulomb, cgs_gauss) == erg/statcoulomb\n    assert convert_to(gauss, sqrt(gram/centimeter)/second, cgs_gauss) == sqrt(gram/centimeter)/second\n    assert convert_to(maxwell, sqrt(centimeter**3*gram)/second, cgs_gauss) == sqrt(centimeter**3*gram)/second\n\n    # Test conversion between cgs_gauss and SI units\n    assert convert_to(statcoulomb, coulomb, cgs_gauss) == coulomb/2997924580\n    assert convert_to(coulomb, statcoulomb, cgs_gauss) == 2997924580*statcoulomb\n    assert convert_to(statampere, ampere, cgs_gauss) == ampere/2997924580\n    assert convert_to(ampere, statampere, cgs_gauss) == 2997924580*statampere\n    assert convert_to(statvolt, volt, cgs_gauss) == 10**6*statvolt/299792458\n    assert convert_to(volt, statvolt, cgs_gauss) == 299792458*statvolt/10**6\n\n    # Test conversion between cgs_gauss and other units\n    assert convert_to(gauss, oersted, cgs_gauss) == oersted\n    assert convert_to(oersted, gauss, cgs_gauss) == gauss\n    assert convert_to(maxwell, weber/10**8, cgs_gauss) == weber/10**8\n    assert convert_to(weber/10**8, maxwell, cgs_gauss) == maxwell\n    assert convert_to(statcoulomb, debye/10**18, cgs_gauss) == debye/10**18\n    assert convert_to(debye/10**18, statcoulomb, cgs_gauss) == statcoulomb\n", "def test_cgs_gauss_base_units():\n    assert convert_to(centimeter, meter, cgs_gauss) == meter/100\n    assert convert_to(gram, kilogram, cgs_gauss) == kilogram/1000\n    assert convert_to(second, second, cgs_gauss) == second\n\n    assert convert_to(meter, centimeter, cgs_gauss) == 100*centimeter\n    assert convert_to(kilogram, gram, cgs_gauss) == 1000*gram\n    assert convert_to(second, second, cgs_gauss) == second\n"], "sample_80": ["def test_check_related_objects(self):\n    query = Query(Author)\n    with self.assertRaisesMessage(ValueError, 'Cannot query \"Item\": Must be \"Author\" instance.'):\n        query.check_related_objects(Author._meta.get_field('num'), Item(), Author._meta)\n    with self.assertRaisesMessage(ValueError, 'Cannot use QuerySet for \"Item\": Use a QuerySet for \"Author\".'):\n        query.check_related_objects(Author._meta.get_field('num'), Item.objects.all(), Author._meta)\n", "def test_add_annotation(self):\n    query = Query(Author)\n    annotation = Count('id', distinct=True)\n    query.add_annotation(annotation, alias='count')\n    self.assertIn('count', query.annotations)\n    self.assertEqual(query.annotations['count'], annotation)\n    self.assertEqual(query.annotation_select_mask, {'count'})\n", "def test_split_exclude(self):\n    query = Query(Author)\n    with self.assertRaises(FieldError):\n        query.build_filter(('friends__name', 'John'))\n\n    query = Query(Item)\n    where = query.build_where(~Q(creator__num__gt=2))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, GreaterThan)\n    self.assertEqual(lookup.rhs, 2)\n    self.assertEqual(lookup.lhs.target, Author._meta.get_field('num'))\n    lookup = where.children[1]\n    self.assertIsInstance(lookup, IsNull)\n    self.assertEqual(lookup.lhs.target, Author._meta.get_field('num'))\n    self.assertEqual(where.connector, OR)\n", "def test_names_to_path(self):\n    query = Query(Author)\n    path, final_field, targets, names = query.names_to_path(['num'], Author._meta)\n    self.assertEqual(path, [])\n    self.assertEqual(final_field, Author._meta.get_field('num'))\n    self.assertEqual(targets, (Author._meta.get_field('num'),))\n    self.assertEqual(names, [])\n\n    query = Query(Item)\n    path, final_field, targets, names = query.names_to_path(['creator__num'], Item._meta)\n    self.assertEqual(path, [query.setup_joins(['creator'], Item._meta, query.get_initial_alias())[0]])\n    self.assertEqual(final_field, Author._meta.get_field('num'))\n    self.assertEqual(targets, (Author._meta.get_field('num'),))\n    self.assertEqual(names, [])\n\n    query = Query(Item)\n    path, final_field, targets, names = query.names_to_path(['creator__does_not_exist'], Item._meta)\n    with self.assertRaisesMessage(FieldError, \"Cannot resolve keyword 'does_not_exist' into field\"):\n        query.setup_joins(['creator__does_not_exist'], Item._meta, query.get_initial_alias())\n\n    query = Query(Item)\n    path, final_field, targets, names = query.names_to_path(['does_not_exist'], Item._meta, fail_on_missing=True)\n    with self.assertRaisesMessage(FieldError, \"Cannot resolve keyword 'does_not_exist' into field\"):\n        query.setup_joins(['does_not_exist'], Item._meta, query.get_initial_alias())\n\n    query = Query(Author)\n    path, final_field, targets, names = query.names_to_path(['author__num'], Author._meta)\n    with self.assertRaisesMessage(FieldError, \"Cannot resolve keyword 'author' into field\"):\n        query.setup_joins(['author__num'], Author._meta, query.get_initial_alias())\n", "def test_add_annotation(self):\n    query = Query(Item)\n    query.add_annotation(SimpleCol(Item._meta.get_field('num')), alias='num')\n    self.assertIn('num', query.annotation_select)\n    self.assertIsInstance(query.annotation_select['num'], SimpleCol)\n    self.assertEqual(query.annotation_select['num'].target, Item._meta.get_field('num'))\n\n    query.add_annotation(Lower(Item._meta.get_field('name')), alias='name_lower')\n    self.assertIn('name_lower', query.annotation_select)\n    self.assertIsInstance(query.annotation_select['name_lower'], Lower)\n    self.assertEqual(query.annotation_select['name_lower'].lhs.target, Item._meta.get_field('name'))\n\n    with self.assertRaises(ValueError):\n        query.add_annotation(5, alias='not_an_expression')\n\n    with self.assertRaises(FieldError):\n        query.add_annotation(SimpleCol(Item._meta.get_field('modified')), alias='invalid_alias')\n"], "sample_856": ["def test_predefinedsplit_repr():\n    # Check that PredefinedSplit has a good repr\n    ps = PredefinedSplit(test_fold=np.array([1, 1, 1, 1, 1, 2, 2, 2]))\n    ps_repr = \"PredefinedSplit(test_fold=array([1, 1, 1, 1, 1, 2, 2, 2]))\"\n    assert ps_repr == repr(ps)\n", "def test_leave_one_p_group_out_empty_trainset():\n    # LeaveOneGroup out expect at least 2 groups so no need to check\n    cv = LeavePGroupsOut(n_groups=2)\n    X, y = [[1], [2]], [0, 3]  # 2 samples\n    with pytest.raises(\n            ValueError,\n            match='p=2 must be strictly less than the number of samples=2'):\n        next(cv.split(X, y, groups=[1, 2]))\n\n    # Test n_groups is greater than number of groups\n    cv = LeavePGroupsOut(n_groups=3)\n    X, y = [[1], [2]], [0, 3]  # 2 samples\n    with pytest.raises(\n            ValueError,\n            match='The groups parameter contains fewer than .* n_groups'):\n        next(cv.split(X, y, groups=[1, 2]))\n\n    # Test n_groups is equal to number of groups\n    cv = LeavePGroupsOut(n_groups=2)\n    X, y = [[1], [2]], [0, 3]  # 2 samples\n    with pytest.raises(\n            ValueError,\n            match='The groups parameter contains fewer than .* n_groups'):\n        next(cv.split(X, y, groups=[1, 2]))\n", "def test_leave_one_group_out_no_groups():\n    groups = np.array([1, 1, 1, 1, 1, 1])\n    logo = LeaveOneGroupOut()\n    assert 1 == logo.get_n_splits(groups=groups)\n    train, test = next(logo.split(y=np.ones(len(groups)), groups=groups))\n    assert len(train) == len(groups)\n    assert len(test) == 0\n    assert_array_equal(train, np.arange(len(groups)))\n    assert_array_equal(test, [])\n", "def test_group_kfold_index_dtypes():\n    # Ensure GroupKFold correctly handles groups with different dtypes.\n    groups = [np.arange(10, dtype=np.int32), np.arange(10, dtype=np.float32),\n              np.arange(10, dtype=np.int64), np.arange(10, dtype=np.float64)]\n    X = np.ones((10, 2))\n    y = np.arange(10)\n\n    for group in groups:\n        group_kfold = GroupKFold(n_splits=3)\n        list(group_kfold.split(X, y, group))\n", "def test_check_cv_with_stratify():\n    # Test that check_cv with stratify=True and a list of string labels\n    # raises an informative error message\n    y = ['a'] * 4 + ['b'] * 4 + ['c'] * 4\n    with pytest.raises(ValueError,\n                       match=r'stratify should be a array-like, not list'):\n        check_cv(3, y, classifier=True)\n"], "sample_716": ["def test_ridge_regression_sample_weights_sparse():\n    # Test sample weights with sparse matrices\n\n    rng = np.random.RandomState(0)\n\n    for solver in (\"cholesky\", ):\n        for n_samples, n_features in ((6, 5), (5, 10)):\n            for alpha in (1.0, 1e-2):\n                y = rng.randn(n_samples)\n                X = sp.csr_matrix(rng.randn(n_samples, n_features))\n                sample_weight = 1.0 + rng.rand(n_samples)\n\n                coefs = ridge_regression(X, y,\n                                         alpha=alpha,\n                                         sample_weight=sample_weight,\n                                         solver=solver)\n\n                # Sample weight can be implemented via a simple rescaling\n                # for the square loss.\n                coefs2 = ridge_regression(\n                    X.multiply(np.sqrt(sample_weight)[:, np.newaxis]),\n                    y * np.sqrt(sample_weight),\n                    alpha=alpha, solver=solver)\n                assert_array_almost_equal(coefs, coefs2)\n", "def test_ridgecv_store_cv_values_multitarget():\n    # Test _RidgeCV's store_cv_values attribute for multi-target case.\n    rng = np.random.RandomState(42)\n\n    n_samples = 8\n    n_features = 5\n    n_targets = 2\n    x = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples, n_targets)\n    alphas = [1e-1, 1e0, 1e1]\n    n_alphas = len(alphas)\n\n    r = RidgeCV(alphas=alphas, store_cv_values=True)\n\n    r.fit(x, y)\n    assert_equal(r.cv_values_.shape, (n_samples, n_targets, n_alphas))\n", "def test_ridgecv_store_cv_values_multi_response():\n    # Test _RidgeCV's store_cv_values attribute for multi-response.\n    rng = np.random.RandomState(42)\n\n    n_samples = 8\n    n_features = 5\n    n_responses = 3\n    X = rng.randn(n_samples, n_features)\n    Y = rng.randn(n_samples, n_responses)\n    alphas = [1e-1, 1e0, 1e1]\n    n_alphas = len(alphas)\n\n    r = RidgeCV(alphas=alphas, store_cv_values=True)\n\n    r.fit(X, Y)\n    assert_equal(r.cv_values_.shape, (n_samples, n_responses, n_alphas))\n", "def test_ridge_sparse_design():\n    # Test Ridge regression on sparse design\n    rng = np.random.RandomState(0)\n    alpha = 1.0\n\n    n_samples, n_features = 6, 5\n    X_64 = rng.randn(n_samples, n_features)\n    X_32 = X_64.astype(np.float32)\n\n    X_sparse_64 = sp.csr_matrix(X_64)\n    X_sparse_32 = X_sparse_64.astype(np.float32)\n\n    y_64 = rng.randn(n_samples)\n    y_32 = y_64.astype(np.float32)\n\n    solvers = [\"sag\", \"saga\"]\n    for solver in solvers:\n        # Check type consistency 32bits\n        ridge_32 = Ridge(alpha=alpha, solver=solver)\n        ridge_32.fit(X_sparse_32, y_32)\n        coef_32 = ridge_32.coef_\n\n        # Check type consistency 64 bits\n        ridge_64 = Ridge(alpha=alpha, solver=solver)\n        ridge_64.fit(X_sparse_64, y_64)\n        coef_64 = ridge_64.coef_\n\n        # Do the actual checks at once for easier debug\n        assert coef_32.dtype == X_32.dtype\n        assert coef_64.dtype == X_64.dtype\n        assert ridge_32.predict(X_sparse_32).dtype == X_32.dtype\n        assert ridge_64.predict(X_sparse_64).dtype == X_64.dtype\n        assert_almost_equal(ridge_32.coef_, ridge_64.coef_, decimal=5)\n", "def test_ridgecv_random_state():\n    # Test that RidgeCV's random_state parameter is correctly set and used.\n    np.random.seed(0)\n    X, y = make_regression(n_samples=100, n_features=10, random_state=42)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n    cv = 5\n    alphas = [1.0, 0.1, 0.01]\n    ridgecv1 = RidgeCV(alphas=alphas, cv=cv, random_state=42)\n    ridgecv1.fit(X_train, y_train)\n\n    ridgecv2 = RidgeCV(alphas=alphas, cv=cv, random_state=42)\n    ridgecv2.fit(X_train, y_train)\n\n    assert_array_almost_equal(ridgecv1.coef_, ridgecv2.coef_)\n    assert_array_almost_equal(ridgecv1.intercept_, ridgecv2.intercept_)\n\n    ridgecv3 = RidgeCV(alphas=alphas, cv=cv, random_state=43)\n    ridgecv3.fit(X_train, y_train)\n    assert_raises(AssertionError, assert_array_almost_equal,\n                  ridgecv1.coef_, ridgecv3.coef_)\n    assert_raises(AssertionError, assert_array_almost_equal,\n                  ridgecv1.intercept_, ridgecv3.intercept_)\n"], "sample_715": ["def test_learning_curve_incremental_learning_with_shuffle():\n    X, y = make_classification(n_samples=30, n_features=1, n_informative=1,\n                               n_redundant=0, n_classes=2,\n                               n_clusters_per_class=1, random_state=0)\n    estimator = MockIncrementalImprovingEstimator(20)\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=3, exploit_incremental_learning=True,\n        train_sizes=np.linspace(0.1, 1.0, 10), shuffle=True, random_state=0)\n    assert_array_equal(train_sizes, np.linspace(2, 20, 10))\n    assert_array_almost_equal(train_scores.mean(axis=1),\n                              np.linspace(1.9, 1.0, 10))\n    assert_array_almost_equal(test_scores.mean(axis=1),\n                              np.linspace(0.1, 1.0, 10))\n", "def test_cross_val_predict_invalid_method():\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    clf = LogisticRegression()\n\n    assert_raise_message(ValueError,\n                         \"Invalid method 'invalid_method' for estimator \"\n                         \"LogisticRegression. \"\n                         \"Available methods are: ['decision_function', \"\n                         \"'predict', 'predict_log_proba', 'predict_proba']\",\n                         cross_val_predict, clf, X, y, method='invalid_method')\n", "def test_cross_val_score_sparse_X_and_y():\n    # test that cross_val_score gives same result for sparse X and sparse y\n    X, y = make_classification(n_samples=30, n_features=20, n_informative=5,\n                               random_state=0, n_classes=2)\n    X_sparse = coo_matrix(X)\n    y_sparse = coo_matrix(np.array([y, y]).T)\n\n    clf = LogisticRegression(random_state=42)\n    scores = cross_val_score(clf, X, y, cv=3)\n    scores_sparse_X = cross_val_score(clf, X_sparse, y, cv=3)\n    scores_sparse_y = cross_val_score(clf, X, y_sparse, cv=3)\n    scores_sparse_X_and_y = cross_val_score(clf, X_sparse, y_sparse, cv=3)\n\n    assert_array_almost_equal(scores, scores_sparse_X)\n    assert_array_almost_equal(scores, scores_sparse_y)\n    assert_array_almost_equal(scores, scores_sparse_X_and_y)\n", "def test_cross_val_score_sample_weights():\n    # Check if sample weights are correctly passed to scorer object.\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    sample_weight = np.ones(len(X))\n    clf = SVC(kernel='linear', probability=True)\n    scoring = make_scorer(precision_score, average='macro')\n    # Test with sample weights\n    sample_weight = np.ones(len(X))\n    with warnings.catch_warnings(record=True):\n        scores = cross_val_score(clf, X, y, cv=5, scoring=scoring,\n                                 fit_params={'sample_weight': sample_weight})\n    assert_array_almost_equal(scores, [0.98, 0.98, 0.96, 1.00, 0.96])\n", "def test_learning_curve_error_handle():\n    # Regression test for issue #10059. Test that error is properly handled\n    # when the learning curve is computed in parallel with n_jobs > 1.\n    X, y = make_classification(n_samples=100, n_features=1, n_informative=1,\n                               n_redundant=0, n_classes=2,\n                               n_clusters_per_class=1, random_state=0)\n    estimator = MockImprovingEstimator(100)\n    assert_warns_message(RuntimeWarning, \"Failed to reach tolerance\",\n                         learning_curve, estimator, X, y, cv=3,\n                         train_sizes=np.linspace(0.1, 1.0, 10),\n                         n_jobs=2, verbose=0)\n"], "sample_592": ["def test_inline_variable_array_repr(self):\n    da = xr.DataArray(np.random.randn(100, 5, 1), dims=(\"x\", \"y\", \"z\"))\n    actual = formatting.inline_variable_array_repr(da.variable, max_width=100)\n    assert len(actual) <= 100\n    assert \"...\" in actual\n\n    ds = xr.Dataset({\"var\": da})\n    actual = formatting.inline_variable_array_repr(ds[\"var\"].variable, max_width=100)\n    assert len(actual) <= 100\n    assert \"...\" in actual\n\n    va = xr.Variable((\"x\", \"y\"), np.random.randn(100, 5), {\"title\": \"test Variable\"})\n    actual = formatting.inline_variable_array_repr(va, max_width=100)\n    assert len(actual) <= 100\n    assert \"...\" in actual\n", "def test_format_array_flat_with_dask_array(self):\n    import dask.array as da\n    array = da.random.random((10, 5, 2), chunks=(2, 5, 2))\n    actual = formatting.format_array_flat(array, 20)\n    expected = \"...\"\n    assert expected == actual\n\n    array = da.random.random((10, 5, 2), chunks=(2, 5, 2))\n    actual = formatting.format_array_flat(array, 30)\n    assert \"...\" in actual\n\n    array = da.random.random((10, 5, 2), chunks=(2, 5, 2))\n    actual = formatting.format_array_flat(array, 100)\n    assert len(actual) <= 100\n", "def test_inline_variable_array_repr(self):\n    cases = [\n        (np.arange(100), \"0 ... 99\"),\n        (np.random.randn(100), \"0.0000 ... 0.0000\"),\n        (pd.to_datetime([\"NaT\", \"0s\", \"1s\", \"NaT\"]), \"NaT ... NaT\"),\n        (xr.DataArray(np.arange(100)), \"dask.array<chunksize=(100,)>\"),\n        (xr.DataArray(sparse.COO(np.arange(100))), \"<COO: nnz=100, fill_value=0>\"),\n    ]\n    for array, expected in cases:\n        actual = formatting.inline_variable_array_repr(array, 80)\n        assert expected == actual\n", "    def test_inline_variable_array_repr(self):\n        da = xr.DataArray(np.random.randn(100, 5, 1))\n        actual = formatting.inline_variable_array_repr(da, max_width=80)\n        assert len(actual) <= 80\n\n        expected = \"dask.array<chunksize=(10, 5, 1)>\"\n        actual = formatting.inline_variable_array_repr(da.chunk(), max_width=80)\n        assert actual == expected\n\n        da = xr.DataArray(np.random.randn(100, 5, 1), chunks=(10, 5, 1))\n        actual = formatting.inline_variable_array_repr(da, max_width=80)\n        assert actual == expected\n\n        actual = formatting.inline_variable_array_repr(da.variable, max_width=80)\n        assert actual == expected\n\n        sparse_da = xr.DataArray(np.random.randn(100, 5, 1), chunks=(10, 5, 1), dtype=\"float64\")\n        sparse_da.data = sparse_da.data.tocsr()\n        actual = formatting.inline_variable_array_repr(sparse_da, max_width=80)\n        expected = \"<COO: nnz=100, fill_value=0.0>\"\n        assert actual == expected\n\n        actual = formatting.inline_variable_array_repr(sparse_da.variable, max_width=80)\n        assert actual == expected\n", "    def test_inline_variable_array_repr(self):\n        data = np.random.randn(10, 20)\n        da = xr.DataArray(data, dims=(\"x\", \"y\"))\n\n        # in-memory numpy array\n        expected = formatting.format_array_flat(data, 80)\n        actual = formatting.inline_variable_array_repr(da, 80)\n        assert actual == expected\n\n        # dask array\n        da = xr.DataArray(data, chunks=(5, 10))\n        expected = formatting.inline_dask_repr(da.data)\n        actual = formatting.inline_variable_array_repr(da, 80)\n        assert actual == expected\n\n        # sparse array\n        from scipy.sparse import csr_matrix\n        data_sparse = csr_matrix(np.random.randn(10, 20))\n        da = xr.DataArray(data_sparse, dims=(\"x\", \"y\"))\n        expected = formatting.inline_sparse_repr(da.data)\n        actual = formatting.inline_variable_array_repr(da, 80)\n        assert actual == expected\n\n        # out-of-memory numpy array\n        data = np.random.randn(10, 20)\n        da = xr.DataArray(data, dims=(\"x\", \"y\"))\n        da.data = da.data.astype(\"int32\")\n        expected = \"[400 values with dtype=int32]\"\n        actual = formatting.inline_variable_array_repr(da, 80)\n        assert actual == expected\n"], "sample_795": ["def test_check_classifiers_regression_target():\n    # Check if classifier throws an exception when fed regression targets\n\n    boston = load_boston()\n    X, y = boston.data, boston.target\n    e = SVC(probability=True)\n    msg = 'Unknown label type: '\n    assert_raises_regex(ValueError, msg, e.fit, X, y)\n\n    msg = 'Unknown label type: '\n    assert_raises_regex(ValueError, msg, check_classifiers_regression_target,\n                        'estimator', e)\n", "def test_check_estimator_dtypes():\n    # check that check_estimator works with different dtypes\n    class MyEstimator(BaseEstimator):\n            return self\n            return np.ones(X.shape[0])\n            return X.astype(np.float64)\n\n    est = MyEstimator()\n    check_estimator(est)\n\n    est = MyEstimator()\n    est.fit(np.ones((10, 2), dtype=np.float32), np.ones(10))\n    check_estimator(est)\n", "def test_check_regressor_data_not_an_array():\n    # Check that an error is raised when y is not an array\n    X, y = make_blobs(n_samples=30, random_state=0, n_features=2,\n                      cluster_std=0.1)\n    X -= X.min()\n    y = NotAnArray(y)\n    estimator = Ridge()\n\n    with assert_raises_regex(\n            ValueError, \"y should be an array\"):\n        check_regressor_data_not_an_array(\n            'estimator_name', estimator)\n", "def test_check_class_weight_classifiers():\n    # check that ill-computed balanced weights raises an exception\n    assert_raises_regex(AssertionError,\n                        \"Classifier estimator_name is not computing\"\n                        \" class_weight=balanced properly.\",\n                        check_class_weight_balanced_linear_classifier,\n                        'estimator_name',\n                        BadBalancedWeightsClassifier)\n\n", "def test_check_estimator_empty_data():\n    # Test that check_estimator works when training data is empty.\n\n    class EmptyDataEstimator(BaseEstimator):\n            return self\n\n            return np.ones(X.shape[0])\n\n    # Test that check_estimator works with an empty data\n    with ignore_warnings(category=FutureWarning):\n        check_estimator(EmptyDataEstimator())\n\n    # Test that check_estimator raises an error when training data is empty\n    class EmptyDataErrorEstimator(BaseEstimator):\n            if X.shape[0] == 0:\n                raise ValueError(\"Empty data.\")\n            return self\n\n            return np.ones(X.shape[0])\n\n    with ignore_warnings(category=FutureWarning):\n        assert_raises_regex(\n            AssertionError, \"The estimator .* does not raise an error when an \"\n                            \"empty data is used to train.\",\n            check_estimator, EmptyDataErrorEstimator())\n"], "sample_440": ["    def test_update_conflicts_unique_fields_both(self):\n        UpsertConflict.objects.bulk_create(\n            [\n                UpsertConflict(number=1, rank=1, name=\"John\"),\n                UpsertConflict(number=2, rank=2, name=\"Mary\"),\n                UpsertConflict(number=3, rank=3, name=\"Hannah\"),\n            ]\n        )\n        self.assertEqual(UpsertConflict.objects.count(), 3)\n\n        conflicting_objects = [\n            UpsertConflict(number=1, rank=4, name=\"Steve\"),\n            UpsertConflict(number=2, rank=2, name=\"Olivia\"),\n            UpsertConflict(number=3, rank=1, name=\"Hannah\"),\n        ]\n        UpsertConflict.objects.bulk_create(\n            conflicting_objects,\n            update_conflicts=True,\n            update_fields=[\"name\"],\n            unique_fields=[\"number\", \"rank\"],\n        )\n        self.assertEqual(UpsertConflict.objects.count(), 3)\n        self.assertCountEqual(\n            UpsertConflict.objects.values(\"number\", \"rank\", \"name\"),\n            [\n                {\"number\": 1, \"rank\": 1, \"name\": \"Steve\"},\n                {\"number\": 2, \"rank\": 2, \"name\": \"Olivia\"},\n                {\"number\": 3, \"rank\": 3, \"name\": \"Hannah\"},\n            ],\n        )\n\n        UpsertConflict.objects.bulk_create(\n            conflicting_objects + [UpsertConflict(number=4, rank=4, name=\"Mark\")],\n            update_conflicts=True,\n            update_fields=[\"name\"],\n            unique_fields=[\"number\", \"rank\"],\n        )\n        self.assertEqual(UpsertConflict.objects.count(), 4)\n        self.assertCountEqual(\n            UpsertConflict.objects.values(\"number\", \"rank\", \"name\"),\n            [\n                {\"number\": 1, \"rank\": 1, \"name\": \"Steve\"},\n                {\"number\": 2, \"rank\": 2, \"name\": \"Olivia\"},\n                {\"number\": 3, \"rank\": 3, \"name\": \"Hannah\"},\n                {\"number\": 4, \"rank\": 4, \"name\": \"Mark\"},\n            ],\n        )\n", "def test_bulk_create_unique_fields_primary_key(self):\n    UpsertConflict.objects.bulk_create(\n        [\n            UpsertConflict(number=1, rank=1, name=\"John\"),\n            UpsertConflict(number=2, rank=2, name=\"Mary\"),\n            UpsertConflict(number=3, rank=3, name=\"Hannah\"),\n        ]\n    )\n    self.assertEqual(UpsertConflict.objects.count(), 3)\n\n    conflicting_objects = [\n        UpsertConflict(number=1, rank=4, name=\"Steve\"),\n        UpsertConflict(number=2, rank=2, name=\"Olivia\"),\n        UpsertConflict(number=3, rank=1, name=\"Hannah\"),\n    ]\n    UpsertConflict.objects.bulk_create(\n        conflicting_objects,\n        update_conflicts=True,\n        update_fields=[\"name\", \"rank\"],\n        unique_fields=[\"number\", \"rank\"],\n    )\n    self.assertEqual(UpsertConflict.objects.count(), 3)\n    self.assertCountEqual(\n        UpsertConflict.objects.values(\"number\", \"rank\", \"name\"),\n        [\n            {\"number\": 1, \"rank\": 4, \"name\": \"Steve\"},\n            {\"number\": 2, \"rank\": 2, \"name\": \"Olivia\"},\n            {\"number\": 3, \"rank\": 1, \"name\": \"Hannah\"},\n        ],\n    )\n\n    UpsertConflict.objects.bulk_create(\n        conflicting_objects + [UpsertConflict(number=4, rank=4, name=\"Mark\")],\n        update_conflicts=True,\n        update_fields=[\"name\", \"rank\"],\n        unique_fields=[\"number\", \"rank\"],\n    )\n    self.assertEqual(UpsertConflict.objects.count(), 4)\n    self.assertCountEqual(\n        UpsertConflict.objects.values(\"number\", \"rank\", \"name\"),\n        [\n            {\"number\": 1, \"rank\": 4, \"name\": \"Steve\"},\n            {\"number\": 2, \"rank\": 2, \"name\": \"Olivia\"},\n            {\"number\": 3, \"rank\": 1, \"name\": \"Hannah\"},\n            {\"number\": 4, \"rank\": 4, \"name\": \"Mark\"},\n        ],\n    )\n", "def test_update_conflicts_invalid_values(self):\n    fields = [\"name\"]\n    unique_fields = [\"f1\"]\n    TwoFields.objects.bulk_create(\n        [\n            TwoFields(f1=1, f2=1),\n            TwoFields(f1=2, f2=2),\n        ]\n    )\n    conflicting_objects = [\n        TwoFields(f1=1, f2=1, name=\"c\"),\n        TwoFields(f1=2, f2=2, name=\"d\"),\n    ]\n    with self.assertRaises(IntegrityError):\n        TwoFields.objects.bulk_create(\n            conflicting_objects,\n            update_conflicts=True,\n            update_fields=fields,\n            unique_fields=unique_fields,\n        )\n", "    def test_update_conflicts_long_field_name(self):\n        # Test updating conflicts with a field having a long name.\n        long_field_name = \"x\" * 50\n        with connection.schema_editor(atomic=False) as schema_editor:\n            schema_editor.add_field(\n                UpsertConflict._meta.model, models.CharField(long_field_name, max_length=50)\n            )\n        try:\n            UpsertConflict.objects.bulk_create(\n                [\n                    UpsertConflict(number=1, rank=1, name=\"John\", **{long_field_name: \"long_value\"}),\n                    UpsertConflict(number=2, rank=2, name=\"Mary\", **{long_field_name: \"long_value\"}),\n                ]\n            )\n            self.assertEqual(UpsertConflict.objects.count(), 2)\n\n            conflicting_objects = [\n                UpsertConflict(number=1, rank=4, name=\"Steve\", **{long_field_name: \"long_value\"}),\n                UpsertConflict(number=2, rank=2, name=\"Olivia\", **{long_field_name: \"long_value\"}),\n            ]\n            UpsertConflict.objects.bulk_create(\n                conflicting_objects,\n                update_conflicts=True,\n                update_fields=[\"name\", \"rank\", long_field_name],\n                unique_fields=[\"number\"],\n            )\n            self.assertEqual(UpsertConflict.objects.count(), 2)\n            self.assertCountEqual(\n                UpsertConflict.objects.values(\"number\", \"rank\", \"name\", long_field_name),\n                [\n                    {\"number\": 1, \"rank\": 4, \"name\": \"Steve\", long_field_name: \"long_value\"},\n                    {\"number\": 2, \"rank\": 2, \"name\": \"Olivia\", long_field_name: \"long_value\"},\n                ],\n            )\n        finally:\n            with connection.schema_editor(atomic=False) as schema_editor:\n                schema_editor.remove_field(UpsertConflict._meta.model, long_field_name)\n", "def test_bulk_create_deferred_fields(self):\n    obj1 = RelatedModel(country=self.data[0], name=\"John\")\n    obj2 = RelatedModel(country=self.data[1], name=\"Mary\")\n    obj3 = RelatedModel(country=self.data[2], name=\"Hannah\")\n    RelatedModel.objects.bulk_create([obj1, obj2, obj3])\n    self.assertEqual(RelatedModel.objects.count(), 3)\n    # Ensure the country field is deferred correctly\n    self.assertEqual(obj1.country._deferred_fields, {\"description\"})\n    self.assertEqual(obj2.country._deferred_fields, {\"description\"})\n    self.assertEqual(obj3.country._deferred_fields, {\"description\"})\n    self.assertEqual(RelatedModel.objects.defer(\"name\").values_list(\"country\", flat=True), [1, 2, 3])\n"], "sample_423": ["def test_order_with_respect_to_removed(self):\n    \"\"\"\n    Removing order_with_respect_to should generate a migration.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_with_book_order_wrt], [self.author_with_book]\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterOrderWithRespectTo\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, name=\"author\", order_with_respect_to=None\n    )\n", "def test_add_model_order_with_respect_to_index_constraint_with_name(self):\n    tests = [\n        (\n            \"AddIndex\",\n            {\n                \"indexes\": [\n                    models.Index(fields=[\"_order\"], name=\"book_order_idx\"),\n                ]\n            },\n        ),\n        (\n            \"AddConstraint\",\n            {\n                \"constraints\": [\n                    models.CheckConstraint(\n                        check=models.Q(_order__gt=1),\n                        name=\"book_order_gt_1\",\n                    ),\n                ]\n            },\n        ),\n    ]\n    for operation, extra_option in tests:\n        with self.subTest(operation=operation):\n            after = ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=200)),\n                    (\"book\", models.ForeignKey(\"otherapp.Book\", models.CASCADE)),\n                ],\n                options={\n                    \"order_with_respect_to\": \"book\",\n                    **extra_option,\n                },\n            )\n            changes = self.get_changes(\n                [self.book, self.author_with_book],\n                [self.book, after],\n                MigrationQuestioner({\"ask_initial\": True}),\n            )\n            self.assertNumberMigrations(changes, \"testapp\", 1)\n            self.assertOperationTypes(\n                changes,\n                \"testapp\",\n                0,\n                [\n                    \"AlterOrderWithRespectTo\",\n                    operation,\n                ],\n            )\n            self.assertOperationAttributes(\n                changes,\n                \"testapp\",\n                0,\n                0,\n                name=\"author\",\n                order_with_respect_to=\"book\",\n            )\n", "def test_add_model_options_with_indexes(self):\n    \"\"\"\n    Test creation of new model with indexes already defined.\n    \"\"\"\n    model_state = ModelState(\n        \"otherapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ],\n        {\n            \"indexes\": [\n                models.Index(fields=[\"name\"], name=\"create_model_with_indexes_idx\"),\n                models.Index(fields=[\"name\"], name=\"create_model_with_indexes_idx2\"),\n            ]\n        },\n    )\n    changes = self.get_changes([], [model_state])\n    added_index1 = models.Index(fields=[\"name\"], name=\"create_model_with_indexes_idx\")\n    added_index2 = models.Index(fields=[\"name\"], name=\"create_model_with_indexes_idx2\")\n    # Right number of migrations?\n    self.assertEqual(len(changes[\"otherapp\"]), 1)\n    # Right number of actions?\n    migration = changes[\"otherapp\"][0]\n    self.assertEqual(len(migration.operations), 3)\n    # Right actions order?\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"CreateModel\", \"AddIndex\", \"AddIndex\"])\n    self.assertOperationAttributes(changes, \"otherapp\", 0, 0, name=\"Author\")\n    self.assertOperationAttributes(changes, \"otherapp\", 0, 1, model_name=\"author\", index=added_index1)\n    self.assertOperationAttributes(changes, \"otherapp\", 0, 2, model_name=\"author\", index=added_index2)\n", "def test_custom_fk_to_field(self):\n    before = [\n        ModelState(\n            \"app\",\n            \"Foo\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"field\", models.IntegerField(unique=True)),\n            ],\n        ),\n        ModelState(\n            \"app\",\n            \"Bar\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\n                    \"foo\",\n                    models.ForeignKey(\"app.Foo\", models.CASCADE, to_field=\"field\"),\n                ),\n            ],\n        ),\n    ]\n    after = [\n        ModelState(\n            \"app\",\n            \"Foo\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"renamed_field\", models.IntegerField(unique=True)),\n            ],\n        ),\n        ModelState(\n            \"app\",\n            \"Bar\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\n                    \"foo\",\n                    models.ForeignKey(\n                        \"app.Foo\",\n                        models.CASCADE,\n                        to_field=\"renamed_field\",\n                    ),\n                ),\n            ],\n        ),\n    ]\n    changes = self.get_changes(before, after, MigrationQuestioner({\"ask_rename\": True}))\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"app\", 1)\n    self.assertOperationTypes(changes, \"app\", 0, [\"RenameField\"])\n    self.assertOperationAttributes(\n        changes, \"app\", 0, 0, old_name=\"field\", new_name=\"renamed_field\"\n    )\n\n    # Check that AlterField is not present.\n    for operation in changes[\"app\"][0].operations:\n        self.assertNotIsInstance(operation, migrations.AlterField)\n", "def test_add_and_alter_unique_together(self):\n    \"\"\"Added fields will be created before using them in unique_together.\"\"\"\n    changes = self.get_changes(\n        [self.author_empty, self.book],\n        [self.author_empty, self.book_unique_together_3],\n        MigrationQuestioner({\"ask_rename\": True}),\n    )\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(\n        changes,\n        \"otherapp\",\n        0,\n        [\"AddField\", \"AlterUniqueTogether\"],\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"otherapp\",\n        0,\n        1,\n        name=\"book\",\n        unique_together={(\"title\", \"newfield\")},\n    )\n    # Renaming one of the fields of unique_together\n    after = ModelState(\n        \"otherapp\",\n        \"Book\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n            (\"newfield2\", models.IntegerField()),\n            (\"title\", models.CharField(max_length=200)),\n        ],\n        {\n            \"unique_together\": {(\"newfield2\", \"title\")},\n        },\n    )\n    changes = self.get_changes(\n        [self.author_empty, self.book_unique_together_3],\n        [self.author_empty, after],\n        MigrationQuestioner({\"ask_rename\": True}),\n    )\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(\n        changes,\n        \"otherapp\",\n        0,\n        [\"RenameField\", \"AlterUniqueTogether\"],\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"otherapp\",\n        0,\n        1,\n        name=\"book\",\n        unique_together={(\"newfield2\", \"title\")},\n    )\n"], "sample_689": ["def test_warning_captured_hook_deprecated() -> None:\n    with pytest.warns(pytest.PytestDeprecationWarning, match=\"pytest_warning_captured\"):\n        pytest.hook.pytest_warning_captured()\n", "def test_warning_captured_hook_is_deprecated(testdir: Testdir) -> None:\n    \"\"\"The pytest_warning_captured hook is deprecated and will be removed in a future release.\"\"\"\n    testdir.makeconftest(\n        \"\"\"\n            pass\n        \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import warnings\n\n            warnings.warn(\"test warning\")\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\"*PytestDeprecationWarning: The pytest_warning_captured is deprecated and will be removed in a future release.*\"]\n    )\n", "def test_warning_captured_hook_is_deprecated(testdir: Testdir) -> None:\n    testdir.makepyfile(\n        test_foo=\"\"\"\n            pass\n        \"\"\"\n    )\n    result = testdir.parseconfig()\n    with pytest.warns(pytest.PytestDeprecationWarning, match=\"pytest_warning_captured\"):\n        result.gethookwrapper(\"pytest_warning_captured\")\n", "def test_pytest_warning_captured_is_deprecated(testdir: Testdir) -> None:\n    module = testdir.getmodulecol(\n        \"\"\"\n        import pytest\n\n            pytest.warns(pytest.PytestWarning, \"some warning message\")\n        \"\"\",\n        withinit=True,\n    )\n    assert isinstance(module, pytest.Module)\n\n    with pytest.warns(pytest.PytestDeprecationWarning, match=\"pytest_warning_captured\"):\n        with pytest.warns(pytest.PytestWarning, match=\"some warning message\"):\n            pass\n", "def test_warning_captured_hook_is_deprecated(testdir: Testdir) -> None:\n    module = testdir.getmodulecol(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            pytest.warn(\"Something went wrong\", category=RuntimeWarning)\n            return 1\n\n            assert myfixture == 1\n        \"\"\",\n        withinit=True,\n    )\n    assert isinstance(module, pytest.Module)\n\n    # The pytest_warning_captured hook is deprecated.\n    with pytest.warns(pytest.PytestDeprecationWarning, match=\"pytest_warning_captured\"):\n        testdir.runpytest(\"-p\", \"no:pytest-warnings\")\n"], "sample_695": ["def test_iterparentnodeids_with_trailing_slash() -> None:\n    nodeid = \"a/b/c/\"\n    expected = [\"\", \"a\", \"a/b\", \"a/b/c\"]\n    result = list(nodes.iterparentnodeids(nodeid))\n    assert result == expected\n\n", "def test_item_init_subclass_warning_for_diamond_inheritance() -> None:\n    \"\"\"Verifies that a warning is raised when a diamond inheritance occurs.\"\"\"\n\n    with pytest.warns(PytestWarning, match=\"is an Item subclass and should not be a collector\"):\n\n        class GrandParent(nodes.Collector, nodes.Item):\n            pass\n\n        class Parent(GrandParent):\n            pass\n\n        class Child(Parent):\n            pass\n", "def test_repr_failure_with_fulltrace(pytester: Pytester) -> None:\n    p = pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            a = 1\n            b = 1 / (1 - 1)\n    \"\"\"\n    )\n    result = pytester.runpytest([\"--fulltrace\", p])\n    result.stdout.fnmatch_lines([\"*1 failed in *\"])\n    assert \"self.session._node_location_to_relpath\" in result.stdout.str()\n    assert \"File \\\"test_repr_failure_with_fulltrace.py\\\", line 5, in test_fail\" in result.stdout.str()\n", "def test_get_fslocation_from_item(pytester: Pytester) -> None:\n    items = pytester.getitems(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    item = items[0]\n    assert nodes.get_fslocation_from_item(item) == (item.fspath, 1)\n\n    class FakeItem:\n            self.location = location\n\n    fake_item = FakeItem((\"/path/to/file\", 10, \"some_name\"))\n    assert nodes.get_fslocation_from_item(fake_item) == (fake_item.location[0], fake_item.location[1])\n\n    class FakeObj:\n            self.__code__ = type(\"code\", (), {\"co_filename\": \"fake_file\", \"co_firstlineno\": lineno})\n\n    fake_item = nodes.Item(\"fake_name\", obj=FakeObj(5))\n    assert nodes.get_fslocation_from_item(fake_item) == (\"fake_file\", 5)\n\n    fake_item = nodes.Item(\"fake_name\", fspath=\"fake_fspath\")\n    assert nodes.get_fslocation_from_item(fake_item) == (\"fake_fspath\", -1)\n", "def test_addfinalizer_called_after_teardown(pytester: Pytester) -> None:\n    \"\"\"Test that the addfinalizer callback is called after teardown.\"\"\"\n    p = pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            return \"value\"\n\n            finalizer_called = []\n\n                finalizer_called.append(True)\n                assert test_addfinalizer_teardown_called[0]\n\n            test_addfinalizer_teardown_called = []\n\n            request = pytest.config.getoption(\"request\")\n            item = request.node\n            item.addfinalizer(finalizer)\n\n            item.teardown()\n            test_addfinalizer_teardown_called.append(True)\n\n            assert finalizer_called\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\".*\"])\n\n"], "sample_1086": ["def test_Not():\n    assert str(~x) == \"~x\"\n    assert str(~(x + y)) == \"~(x + y)\"\n    assert str(~(x * y)) == \"~(x*y)\"\n", "def test_str_relational():\n    assert str(Eq(x, y)) == \"Eq(x, y)\"\n    assert str(Ne(x, y)) == \"Ne(x, y)\"\n    assert str(Lt(x, y)) == \"x < y\"\n    assert str(Le(x, y)) == \"x <= y\"\n    assert str(Gt(x, y)) == \"x > y\"\n    assert str(Ge(x, y)) == \"x >= y\"\n    assert str(Eq(x, 1) | Eq(x, 2)) == \"Eq(x, 1) | Eq(x, 2)\"\n    assert str(Ne(x, 1) & Ne(x, 2)) == \"Ne(x, 1) & Ne(x, 2)\"\n", "def test_Rational_sqrt():\n    n1 = Rational(1, 4)\n    n2 = Rational(1, 3)\n    n3 = Rational(2, 4)\n    n4 = Rational(2, -4)\n    n5 = Rational(0)\n    n7 = Rational(3)\n    n8 = Rational(-3)\n    assert str(sqrt(n1*n2)) == \"sqrt(1/12)\"\n    assert str(sqrt(n1*n2)) == \"sqrt(1/12)\"\n    assert str(sqrt(n3)) == \"1/2\"\n    assert str(sqrt(n1*n3)) == \"1/4\"\n    assert str(sqrt(n1 + n3)) == \"sqrt(3/4)\"\n    assert str(sqrt(n1 + n2)) == \"sqrt(7/12)\"\n    assert str(sqrt(n1 + n4)) == \"I*sqrt(1/4)\"\n    assert str(sqrt(n4*n4)) == \"1/2\"\n    assert str(sqrt(n4 + n2)) == \"I*sqrt(1/6)\"\n    assert str(sqrt(n4 + n5)) == \"I/2\"\n    assert str(sqrt(n4*n5)) == \"0\"\n    assert str(sqrt(n3 + n4)) == \"0\"\n    assert str(sqrt(n1**n7)) == \"1/16\"\n    assert str(sqrt(n2**n7)) == \"1/27\"\n    assert str(sqrt(n2**n8)) == \"27\"\n    assert str(sqrt(n7**n8)) == \"1/27\"\n    assert str(sqrt(Rational(\"-25\"))) == \"5*I\"\n    assert str(sqrt(Rational(\"1.25\"))) == \"5/4\"\n    assert str(sqrt(Rational(\"-2.6e-2\"))) == \"I*sqrt(13/500)\"\n    assert str(sqrt(S(\"25/7\"))) == \"5*sqrt(7)/7\"\n    assert str(sqrt(S(\"-123/569\"))) == \"I*sqrt(123/569)\"\n    assert str(sqrt(S(\"0.1[23]\", rational=1))) == \"sqrt(61/495)\"\n    assert str(sqrt(S(\"5.1[666]\", rational=1))) == \"sqrt(31/6)\"\n    assert str(sqrt(S(\"-5.1[666]\", rational=1))) == \"I*sqrt(31/6)\"\n    assert str(sqrt(S(\"0.[", "def test_issue_17097():\n    from sympy.physics.units import meter, second\n    expr = meter / second\n    assert sstr(expr) == \"meter/second\"\n    assert sstr(expr, abbrev=True) == \"m/s\"\n\n    expr = meter / second**2\n    assert sstr(expr) == \"meter/second**2\"\n    assert sstr(expr, abbrev=True) == \"m/s**2\"\n\n    expr = meter / (second**2 * meter)\n    assert sstr(expr) == \"1/second**2\"\n    assert sstr(expr, abbrev=True) == \"1/s**2\"\n", "def test_ConditionSet():\n    x, y = symbols('x y')\n    assert str(ConditionSet(x, Eq(x**2 + 1, 0), S.Reals)) == 'ConditionSet(x, Eq(x**2 + 1, 0), Reals)'\n    assert str(ConditionSet(x, Eq(x**2 + 1, 0), S.Integers)) == 'ConditionSet(x, Eq(x**2 + 1, 0), Integers)'\n    assert str(ConditionSet(x, Eq(x**2 + 1, 0), S.Complexes)) == 'ConditionSet(x, Eq(x**2 + 1, 0), Complexes)'\n    assert str(ConditionSet(x, Eq(x, y), S.Integers)) == 'ConditionSet(x, Eq(x, y), Integers)'\n    assert str(ConditionSet(x, Eq(x, 0), S.Integers)) == 'ConditionSet(x, Eq(x, 0), Integers)'\n    assert str(ConditionSet(x, Eq(x**2 + y, 0), S.Integers)) == 'ConditionSet(x, Eq(x**2 + y, 0), Integers)'\n    assert str(ConditionSet(x, Eq(x, 0), S.UniversalSet)) == 'ConditionSet(x, Eq(x, 0))'\n"], "sample_823": ["def test_pairwise_kernels_sparse_output():\n    # Test pairwise_kernels function with sparse output.\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    Y = rng.random_sample((3, 4))\n    Xcsr = csr_matrix(X)\n    Ycsr = csr_matrix(Y)\n\n    # should be sparse\n    K1 = pairwise_kernels(Xcsr, Ycsr, metric=\"linear\", dense_output=False)\n    assert issparse(K1)\n\n    # should be dense, and equal to K1\n    K2 = pairwise_kernels(X, Y, metric=\"linear\", dense_output=True)\n    assert not issparse(K2)\n    assert_array_almost_equal(K1.todense(), K2)\n\n    # show the kernel output equal to the sparse.todense()\n    K3 = pairwise_kernels(X, Y, metric=\"linear\")\n    assert_array_almost_equal(K1.todense(), K3)\n", "def test_paired_distances_invalid_metric():\n    # Test the paired_distance helper function\n    # with an invalid metric.\n    rng = np.random.RandomState(0)\n    # Euclidean distance should be equivalent to calling the function.\n    X = rng.random_sample((5, 4))\n    # Euclidean distance, with Y != X.\n    Y = rng.random_sample((5, 4))\n    assert_raises(ValueError, paired_distances, X, Y, metric=\"unknown\")\n", "def test_chi_square_kernel_invalid_input(metric):\n    # Test that chi2 kernel raises ValueError on sparse input\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    Y = csr_matrix(X)\n    assert_raises(ValueError, PAIRWISE_KERNEL_FUNCTIONS[metric], X, Y)\n\n    # Test that chi2 kernel raises ValueError on non-positive input\n    X = rng.random_sample((5, 4))\n    X[X < 0] = -1\n    assert_raises(ValueError, PAIRWISE_KERNEL_FUNCTIONS[metric], X)\n\n    # Test that chi2 kernel raises ValueError on non-positive input\n    X = rng.random_sample((5, 4))\n    Y = rng.random_sample((10, 4))\n    Y[Y < 0] = -1\n    assert_raises(ValueError, PAIRWISE_KERNEL_FUNCTIONS[metric], X, Y)\n", "def test_check_non_negative_precomputed():\n    # Test check_non_negative function for precomputed distance matrices\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 5))\n    X[X < 0.5] = -1\n    assert_raises_regexp(ValueError, '.*non-negative values.*',\n                         check_pairwise_arrays, X, None, precomputed=True)\n\n    X = np.random.rand(5, 5)\n    X[0, 1] = np.nan\n    assert_raises_regexp(ValueError, '.*non-negative values.*',\n                         check_pairwise_arrays, X, None, precomputed=True)\n", "def test_paired_distances_callable_nonstrict_metric_edge_case():\n    # Test that paired_distances accepts a callable metric\n    # that returns 0 when comparing a point to itself\n    # and nonzero when comparing different points\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    # Euclidean distance should be equivalent to calling the function.\n    Y = X.copy()\n    Y[0, :] = X[1, :]\n    Y[1, :] = X[0, :]\n\n    S = paired_distances(X, Y, metric='manhattan')\n    S2 = paired_distances(X, Y, metric=lambda x, y: np.abs(x - y).sum(axis=0))\n    assert_array_almost_equal(S, S2)\n"], "sample_12": ["def test_longitude_wrap_at_non_angle():\n    \"\"\"\n    Test that Longitude.wrap_at raises an exception when given a non-angle-like\n    object\n    \"\"\"\n    with pytest.raises(TypeError):\n        Longitude(10*u.deg).wrap_at(10)\n", "def test_longitude_wrap_at_limit():\n    \"\"\"\n    Regression test for issue #1786\n    \"\"\"\n    lon = Longitude('359.9999999999999d')\n    assert lon.degree == 360.\n    assert lon.wrap_at('360d').degree == 0.\n\n    lon = Longitude('-359.9999999999999d')\n    assert lon.degree == -360.\n    assert lon.wrap_at('-360d').degree == 0.\n", "def test_latitude_range_edge_cases():\n    \"\"\"\n    Test edge cases for Latitude validation\n    \"\"\"\n\n    # Test that angles exactly at the limits are allowed\n    Latitude(90*u.deg)\n    Latitude(-90*u.deg)\n\n    # Test that angles just beyond the limits are not allowed\n    with pytest.raises(ValueError):\n        Latitude(90.0001*u.deg)\n    with pytest.raises(ValueError):\n        Latitude(-90.0001*u.deg)\n\n    # Test that NaN and infinity are allowed (as they are handled separately)\n    Latitude(np.nan*u.deg)\n    Latitude(np.inf*u.deg)\n    Latitude(-np.inf*u.deg)\n", "def test_longitude_nans_in_array():\n    \"\"\"Test Longitude handling of NaN values in arrays.\"\"\"\n    lon = Longitude([1, np.nan, 2] * u.deg)\n    assert lon.wrap_angle == 360 * u.deg\n    assert np.isnan(lon.degree[1])\n    assert lon[1] == Angle(np.nan * u.deg)\n    lon[1] = 3 * u.deg\n    assert lon.degree[1] == 3\n    with pytest.raises(TypeError):\n        lon[1] = Latitude(3 * u.deg)\n    with pytest.raises(TypeError):\n        lon = Longitude(Latitude([1, 2, 3] * u.deg))\n", "def test_longitude_wrap_angle_change():\n    \"\"\"\n    Tests changing the wrap_angle of Longitude objects\n    \"\"\"\n\n    lon = Longitude(np.array([1, 2, 3]) * u.deg)\n    assert lon.wrap_angle == 360 * u.deg\n\n    lon.wrap_angle = 180 * u.deg\n    assert lon.wrap_angle == 180 * u.deg\n\n    # Test that changing the wrap_angle doesn't affect the value of the Longitude object\n    assert np.all(lon.value == np.array([1, 2, 3]))\n\n    # Test that setting wrap_angle to None raises a ValueError\n    with pytest.raises(ValueError):\n        lon.wrap_angle = None\n\n    # Test that setting wrap_angle to a Quantity with a non-angular unit raises a UnitsError\n    with pytest.raises(u.UnitsError):\n        lon.wrap_angle = 1 * u.m\n\n    # Test that setting wrap_angle to an Angle object with a non-angular unit raises a UnitsError\n    with pytest.raises(u.UnitsError):\n        lon.wrap_angle = Angle(1 * u.m)\n"], "sample_963": ["def test_is_system_TypeVar():\n    T = TypeVar('T')\n    assert is_system_TypeVar(T) is True\n    assert is_system_TypeVar(int) is False\n    assert is_system_TypeVar(MyClass1) is False\n", "def test_get_type_hints():\n        pass\n\n        pass\n\n    assert get_type_hints(func_with_hints) == {'a': int, 'b': str, 'return': None}\n    assert get_type_hints(func_without_hints) == {}\n\n    class ClassWithHInts:\n            pass\n\n    class ClassWithoutHints:\n            pass\n\n    assert get_type_hints(ClassWithHInts.__init__) == {'a': int, 'b': str, 'return': None}\n    assert get_type_hints(ClassWithoutHints.__init__) == {}\n\n    class ClassWithHIntsInClass:\n        a: int\n        b: str\n\n    assert get_type_hints(ClassWithHIntsInClass) == {'a': int, 'b': str}\n\n    assert get_type_hints(1) == {}\n    assert get_type_hints(\"string\") == {}\n    assert get_type_hints(None) == {}\n", "def test_restify_type_hints_invalid_builtin_classes():\n    assert restify(Struct) == \":py:class:`struct.Struct`\"\n    assert restify(TracebackType) == \":py:class:`types.TracebackType`\"\n\n", "def test_get_type_hints():\n        pass\n\n    annotations = get_type_hints(my_function)\n    assert annotations == {'a': int, 'b': str, 'return': NoneType}\n\n    class MyClass:\n            pass\n\n    annotations = get_type_hints(MyClass)\n    assert annotations == {'a': int, 'b': str, 'return': NoneType}\n\n    annotations = get_type_hints(MyClass.__init__)\n    assert annotations == {'a': int, 'b': str, 'return': NoneType}\n\n        pass\n\n    annotations = get_type_hints(my_function_with_defaults)\n    assert annotations == {'a': int, 'b': str, 'return': NoneType}\n\n        \"\"\"My function\n\n        :param a: int\n        :param b: str\n        :return: None\n        \"\"\"\n        pass\n\n    annotations = get_type_hints(my_function_with_type_hints_in_docstring)\n    assert annotations == {'a': int, 'b': str, 'return': NoneType}\n", "def test_restify_invalid_module():\n    class InvalidModule:\n        __module__ = '__invalid__'\n\n    assert restify(InvalidModule) == \":py:class:`tests.test_util_typing.InvalidModule`\"\n\n"], "sample_730": ["def test_multioutput_enetcv_l1_ratio_list():\n    X, y, _, _ = build_dataset(n_features=10, n_targets=3)\n    clf = ElasticNetCV(n_alphas=10, eps=1e-3, max_iter=100,\n                       l1_ratio=[0.3, 0.5], tol=1e-3)\n    clf.fit(X, y[:, 0])\n    assert_equal(0.5, clf.l1_ratio_)\n    assert_equal((X.shape[1],), clf.coef_.shape)\n    assert_equal((1,), clf.intercept_.shape)\n    assert_equal((2, 10, 3), clf.mse_path_.shape)\n    assert_equal((2, 10), clf.alphas_.shape)\n", "def test_enet_n_jobs():\n    X, y, X_test, y_test = build_dataset(n_samples=50, n_features=20)\n    clf = ElasticNetCV(n_jobs=1, cv=3)\n    clf.fit(X, y)\n    assert_greater(clf.score(X_test, y_test), 0.99)\n\n    clf = ElasticNetCV(n_jobs=-1, cv=3)\n    clf.fit(X, y)\n    assert_greater(clf.score(X_test, y_test), 0.99)\n", "def test_enet_alpha_grid():\n    X, y, _, _ = build_dataset(n_samples=20, n_features=10)\n    for l1_ratio in np.linspace(0, 1, 10):\n        alphas = _alpha_grid(X, y, l1_ratio=l1_ratio)\n        # check that there are no duplicates in the grid\n        assert_array_equal(alphas, np.unique(alphas))\n        # check that all the alphas are positive\n        assert np.all(alphas > 0)\n        # check that the grid is sorted in descending order\n        assert_array_equal(alphas, np.sort(alphas)[::-1])\n", "def test_lasso_cv_with_custom_alphas():\n    # Test that the LassoCV can handle custom alphas\n\n    X, y, X_test, y_test = build_dataset(n_samples=50, n_features=20)\n\n    # Generate custom alphas\n    custom_alphas = np.logspace(-4, 0, 10)\n\n    # Fit the model with custom alphas\n    lasso_cv = LassoCV(n_jobs=1, max_iter=5000, alphas=custom_alphas, cv=3)\n    lasso_cv.fit(X, y)\n\n    # Check that the model uses the custom alphas\n    assert_array_almost_equal(lasso_cv.alphas_, custom_alphas)\n    assert_greater(lasso_cv.score(X_test, y_test), 0.99)\n", "def test_enet_selection_parameter():\n    # Test that an error message is raised if selection is not in cyclic or random.\n    # Test that selection random results in different coefficient values than selection cyclic\n    X, y, _, _ = build_dataset(n_samples=50, n_features=20)\n    rng = np.random.RandomState(42)\n    X_cyclic = X.copy()\n    y_cyclic = y.copy()\n    X_random = X.copy()\n    y_random = y.copy()\n    rng.shuffle(y_random)\n\n    est_cyclic = ElasticNet(selection='cyclic', tol=1e-8, random_state=rng)\n    est_cyclic.fit(X_cyclic, y_cyclic)\n    est_random = ElasticNet(selection='random', tol=1e-8, random_state=rng)\n    est_random.fit(X_random, y_random)\n\n    assert_raises(ValueError, ElasticNet(selection='invalid').fit, X, y)\n\n    assert_array_almost_equal(est_cyclic.coef_, est_random.coef_, decimal=5)\n"], "sample_1208": ["def test_sample_numpy():\n    distribs_numpy = [\n        MatrixNormal('M', [[5, 6]], [4], [[2, 1], [1, 2]]),\n        Wishart('W', 5, [[1, 0], [0, 1]])\n    ]\n\n    size = 5\n    numpy = import_module('numpy')\n    if not numpy:\n        skip('Numpy not installed. Abort tests for _sample_numpy.')\n    else:\n        for X in distribs_numpy:\n            samps = sample(X, size=size, library='numpy')\n            for sam in samps:\n                assert Matrix(sam) in X.pspace.distribution.set\n        M = MatrixGamma('M', 1, 2, [[1, 0], [0, 1]])\n        raises(NotImplementedError, lambda: sample(M, size=3))\n", "def test_sample_numpy():\n    numpy = import_module('numpy')\n    if not numpy:\n        skip('Numpy not installed. Abort tests for _sample_numpy.')\n    else:\n        # No distributions are currently supported by numpy\n        M = MatrixGamma('M', 1, 2, [[1, 0], [0, 1]])\n        raises(NotImplementedError, lambda: sample(M, size=3, library='numpy'))\n", "def test_sample_numpy():\n    numpy = import_module('numpy')\n    if not numpy:\n        skip('Numpy is installed by default, but abort tests for _sample_numpy.')\n    else:\n        distribs_numpy = [\n            MatrixNormal('M', [[5, 6]], [4], [[2, 1], [1, 2]]),\n            Wishart('W', 5, [[1, 0], [0, 1]])\n        ]\n        size = 5\n        for X in distribs_numpy:\n            samps = sample(X, size=size, library='numpy')\n            for sam in samps:\n                assert Matrix(sam) in X.pspace.distribution.set\n        M = MatrixGamma('M', 1, 2, [[1, 0], [0, 1]])\n        raises(NotImplementedError, lambda: sample(M, size=3))\n", "def test_MatrixStudentT_unequal_matrices():\n    M = MatrixStudentT('M', 2, [[1, 2]], [[2, 1], [1, 2]], [[4]])\n    raises(ValueError, lambda: MatrixStudentT('M', 2, [[1, 2]], [[2, 1], [1, 2]], [[4, 5]]))\n    raises(ValueError, lambda: MatrixStudentT('M', 2, [[1, 2]], [[2, 1], [1, 2]], [[4], [5]]))\n    raises(ValueError, lambda: MatrixStudentT('M', 2, [[1, 2]], [[2, 1], [1]], [[4]]))\n    raises(ValueError, lambda: MatrixStudentT('M', 2, [[1, 2]], [[2], [1, 2]], [[4]]))\n    raises(ValueError, lambda: MatrixStudentT('M', 2, [[1, 2]], [[2, 1], [1, 2]], [[4, 5], [4]]))\n", "def test_MatrixStudentT_edge_cases():\n    # Test MatrixStudentT with numeric values for nu, location_matrix, scale_matrix_1, scale_matrix_2\n    M = MatrixStudentT('M', 2, [[1, 2], [3, 4]], [[1, 0], [0, 1]], [[1, 0], [0, 1]])\n    X = MatrixSymbol('X', 2, 2)\n    D = 2**(-2) * Determinant(Matrix([[1, 0], [0, 1]])) ** (-1) * Determinant(Matrix([[1, 0], [0, 1]])) \\\n        ** (-1) / Determinant(Matrix([[1, 0], [0, 1]]) + Matrix([[1, 2], [3, 4]]) * Matrix([[1, 0], [0, 1]]) \\\n        * Matrix([[1, 2], [3, 4]].T) + Matrix([[1, 0], [0, 1]])) ** 3\n    assert density(M)(X) == D\n\n    # Test MatrixStudentT with symbolic values for nu, location_matrix, scale_matrix_1, scale_matrix_2\n    v = symbols('v', positive=True)\n    n, p = symbols('n p', positive=True, integer=True)\n    Omega = MatrixSymbol('Omega', p, p)\n    Sigma = MatrixSymbol('Sigma', n, n)\n    Location = MatrixSymbol('Location', n, p)\n    Y = MatrixSymbol('Y', n, p)\n    M = MatrixStudentT('M', v, Location, Omega, Sigma)\n    exprd = gamma(v/2 + 1)*Determinant(Matrix([[1]]) + Sigma**(-1)*(-Location + Y)*Omega**(-1)*(-Location.T + Y.T))**(-v/2 - 1) / \\\n            (pi*gamma(v/2)*sqrt(Determinant(Omega))*Determinant(Sigma))\n    assert density(M)(Y) == exprd\n    raises(ValueError, lambda: density(M)(1))\n    raises(ValueError, lambda: MatrixStudentT('M', 1, [1, 2], [[1, 0], [0, 1]], [[1, 0], [2, 1]]))\n    raises(ValueError, lambda: MatrixStudentT('M', "], "sample_867": ["def test_grid_search_predict_proba_disabled_after_fitting():\n    # Test predict_proba when disabled on estimator after fitting.\n    X = np.arange(20).reshape(5, -1)\n    y = [0, 0, 1, 1, 1]\n    clf = SVC(probability=False, random_state=42)\n    gs = GridSearchCV(clf, {}, cv=2).fit(X, y)\n    assert not hasattr(gs, \"predict_proba\")\n    assert not hasattr(gs.best_estimator_, \"predict_proba\")\n", "def test_search_cv_results_dataframe_conversion():\n    # Test conversion of cv_results_ to a pandas dataframe\n    from pandas import DataFrame\n\n    X, y = make_classification(n_samples=50, n_features=4, random_state=42)\n    search = GridSearchCV(SVC(), cv=3, param_grid={'C': [1, 10]})\n    search.fit(X, y)\n\n    try:\n        df = DataFrame(search.cv_results_)\n    except Exception as e:\n        assert False, (\"Conversion to pandas DataFrame failed with error: %s\" % e)\n\n    expected_columns = ['mean_fit_time', 'std_fit_time', 'mean_score_time',\n                       'std_score_time', 'param_C', 'params', 'split0_test_score',\n                       'split1_test_score', 'split2_test_score', 'mean_test_score',\n                       'std_test_score', 'rank_test_score']\n\n    assert_array_equal(df.columns, expected_columns)\n    assert len(df) == len(search.cv_results_['params'])\n", "def test_grid_search_estimator_init_params():\n    # test that init parameters are not changed during grid search\n    class EstimatorWithInitParams(BaseEstimator):\n            self.param1 = param1\n            self.param2 = param2\n\n            pass\n\n            return 1\n\n    estimator = EstimatorWithInitParams(param1='value1', param2='value2')\n    init_params = estimator.get_params()\n    param_grid = {'param1': ['value3', 'value4'], 'param2': ['value5', 'value6']}\n    GridSearchCV(estimator, param_grid).fit(X, y)\n    assert estimator.get_params() == init_params\n", "def test_search_cv_refit_callable_returning_best_params():\n    \"\"\"\n    Test implementation catches the errors when 'refit' returns the params\n    instead of index.\n    \"\"\"\n        \"\"\"\n        A dummy function tests when 'best_params_' is returned.\n        \"\"\"\n        return cv_results['params'][0]\n\n    X, y = make_classification(n_samples=100, n_features=4,\n                               random_state=42)\n\n    clf = GridSearchCV(LinearSVC(random_state=42), {'C': [0.1, 1]},\n                       scoring='precision', refit=refit_callable_returning_best_params)\n    with pytest.raises(TypeError,\n                       match='best_index_ returned is not an integer'):\n        clf.fit(X, y)\n\n", "def test_refit_with_callable_and_multi_metric():\n    \"\"\"Test GridSearchCV with refit=callable and multiple metrics.\"\"\"\n    # Test that refit=callable can be used with multiple metrics\n    X, y = make_classification(n_samples=100, n_features=4, random_state=0)\n\n        # return index of the model with the least 'mean_test_accuracy'\n        return cv_results['mean_test_accuracy'].argmin()\n\n    param_grid = {'C': [0.1, 1, 10]}\n    scoring = {'Accuracy': make_scorer(accuracy_score), 'Precision': 'precision'}\n\n    clf = GridSearchCV(LinearSVC(random_state=0), param_grid, cv=3,\n                       scoring=scoring, refit=refit_callable)\n    clf.fit(X, y)\n    assert clf.best_index_ == 0\n    # Ensure `best_score_` is disabled when using `refit=callable`\n    assert not hasattr(clf, 'best_score_')\n"], "sample_883": ["def test_bayesian_ridge_ard_no_convergence(Estimator):\n    \"\"\"Check that the model handles non-convergence.\"\"\"\n    X = np.array([[1, 1], [1, 1], [1, 1], [1, 1], [1, 1]])\n    y = np.array([1, 1, 1, 1, 2])\n\n    model = Estimator(max_iter=1)\n    model.fit(X, y)\n\n    # Check that the model did not converge\n    assert model.n_iter_ == 1\n", "def test_bayesian_ridge_ard_intercept(Estimator):\n    \"\"\"Check that BayesianRidge and ARDRegression can handle intercept correctly.\"\"\"\n    X = np.array([[1, 1], [3, 4], [5, 7], [4, 1], [2, 6], [3, 10], [3, 2]])\n    y = np.array([2, 3, 4, 3, 1, 5, 6]).T  # add 1 to all y values\n\n    # Fit with intercept\n    model_with_intercept = Estimator(fit_intercept=True)\n    model_with_intercept.fit(X, y)\n\n    # Fit without intercept\n    model_without_intercept = Estimator(fit_intercept=False)\n    X_without_intercept = np.hstack((np.ones((X.shape[0], 1)), X))\n    model_without_intercept.fit(X_without_intercept, y)\n\n    # Compare the predictions\n    assert_array_almost_equal(\n        model_with_intercept.predict(X), model_without_intercept.predict(X_without_intercept)\n    )\n", "def test_bayesian_ridge_ard_invalid_parameters(Estimator):\n    \"\"\"Check that errors are raised when using invalid parameters.\"\"\"\n    X, y = diabetes.data, diabetes.target\n\n    # Test invalid tol\n    with pytest.raises(ValueError, match=\"tol must be greater than or equal to 0\"):\n        Estimator(tol=-1).fit(X, y)\n\n    # Test invalid max_iter\n    with pytest.raises(ValueError, match=\"max_iter must be greater than or equal to 1\"):\n        Estimator(max_iter=-1).fit(X, y)\n\n    # Test invalid alpha_1\n    with pytest.raises(ValueError, match=\"alpha_1 must be greater than 0\"):\n        Estimator(alpha_1=-1).fit(X, y)\n\n    # Test invalid alpha_2\n    with pytest.raises(ValueError, match=\"alpha_2 must be greater than 0\"):\n        Estimator(alpha_2=-1).fit(X, y)\n\n    # Test invalid lambda_1\n    with pytest.raises(ValueError, match=\"lambda_1 must be greater than 0\"):\n        Estimator(lambda_1=-1).fit(X, y)\n\n    # Test invalid lambda_2\n    with pytest.raises(ValueError, match=\"lambda_2 must be greater than 0\"):\n        Estimator(lambda_2=-1).fit(X, y)\n\n    # Test invalid threshold_lambda\n    with pytest.raises(ValueError, match=\"threshold_lambda must be greater than 0\"):\n        ARDRegression(threshold_lambda=-1).fit(X, y)\n\n    # Test invalid fit_intercept\n    with pytest.raises(ValueError, match=\"fit_intercept must be a boolean\"):\n        Estimator(fit_intercept=\"not a boolean\").fit(X, y)\n\n    # Test invalid copy_X\n    with pytest.raises(ValueError, match=\"copy_X must be a boolean\"):\n        Estimator(copy_X=\"not a boolean\").fit(X, y)\n\n    # Test invalid compute_score\n    with pytest.raises(ValueError, match=\"compute_score must be a boolean\"):\n        Estimator(compute_score=\"not a boolean\").fit(X, y)\n", "def test_bayesian_ridge_ard_regression_on_non_invertible_design_matrix():\n    # Test BayesianRidge and ARDRegression on a design matrix that is not\n    # invertible (i.e., it has linearly dependent rows).\n    # This should not raise any errors.\n    X = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]])\n    y = np.array([1, 2, 3])\n\n    for clf in [BayesianRidge(), ARDRegression()]:\n        clf.fit(X, y)\n        y_pred = clf.predict(X)\n        assert y_pred.shape == (X.shape[0],)\n", "def test_bayesian_ridge_ard_alpha_1_and_alpha_2_range(Estimator):\n    \"\"\"Check that `alpha_1` and `alpha_2` parameters are validated.\"\"\"\n    # Test with values on the bounds of the interval\n    X, y = diabetes.data, diabetes.target\n    model = Estimator(alpha_1=0.0, alpha_2=0.0)\n    with pytest.raises(ValueError, match=\"alpha_1 must be strictly greater than 0\"):\n        model.fit(X, y)\n\n    model = Estimator(alpha_1=1e-6, alpha_2=0.0)\n    with pytest.raises(ValueError, match=\"alpha_2 must be strictly greater than 0\"):\n        model.fit(X, y)\n\n    model = Estimator(alpha_1=1e-6, alpha_2=1e-6)\n    model.fit(X, y)\n"], "sample_1006": ["def test_multi_factorial():\n    assert MultiFactorial(5) == MultiFactorial(5)\n    assert MultiFactorial(3).is_integer is None\n    assert MultiFactorial(2).is_integer is None\n    assert MultiFactorial(0).is_integer is True\n    assert MultiFactorial(-1).is_integer is None\n    assert MultiFactorial(-2).is_integer is None\n\n    assert MultiFactorial(5).is_positive is None\n    assert MultiFactorial(3).is_positive is None\n    assert MultiFactorial(2).is_positive is None\n    assert MultiFactorial(0).is_positive is True\n    assert MultiFactorial(-1).is_positive is None\n    assert MultiFactorial(-2).is_positive is None\n\n    assert MultiFactorial(5).is_real is None\n    assert MultiFactorial(3).is_real is None\n    assert MultiFactorial(2).is_real is None\n    assert MultiFactorial(0).is_real is True\n    assert MultiFactorial(-1).is_real is None\n    assert MultiFactorial(-2).is_real is None\n\n    assert MultiFactorial(5).is_composite is None\n    assert MultiFactorial(3).is_composite is None\n    assert MultiFactorial(2).is_composite is None\n    assert MultiFactorial(0).is_composite is None\n    assert MultiFactorial(-1).is_composite is None\n    assert MultiFactorial(-2).is_composite is None\n", "def test_subfactorial2():\n    x, y = symbols('x,y')\n    n, k = symbols('n k', integer=True)\n    m = Symbol('m', integer=True, nonnegative=True)\n\n    assert subfactorial(nan) == nan\n\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(5) == 44\n\n    assert subfactorial(x).is_integer is None\n    assert subfactorial(n).is_integer is None\n    assert subfactorial(m).is_integer is True\n\n    assert subfactorial(x).is_even is None\n    assert subfactorial(n).is_even is None\n    assert subfactorial(m).is_even is None\n    assert subfactorial(m + 1).is_odd is True\n    assert subfactorial(m + 2).is_even is True\n\n    assert subfactorial(x).is_odd is None\n    assert subfactorial(n).is_odd is None\n    assert subfactorial(m).is_odd is None\n    assert subfactorial(m + 1).is_odd is True\n    assert subfactorial(m + 2).is_even is True\n\n    assert subfactorial(x).is_nonnegative is None\n    assert subfactorial(n).is_nonnegative is None\n    assert subfactorial(m).is_nonnegative is True\n\n    assert subfactorial(x).rewrite(uppergamma) == uppergamma(x + 1, -1)/S.Exp1\n    assert subfactorial(5).rewrite(uppergamma) == 44\n", "def test_factorial_edge_cases():\n    x = Symbol('x')\n    assert factorial(0) == 1\n    assert factorial(-0) == 1\n    assert factorial(1) == 1\n    assert factorial(-1) == zoo\n    assert factorial(-2) == zoo\n    assert factorial(oo) == oo\n    assert factorial(-oo) == zoo\n    assert factorial(zoo) == zoo\n    assert factorial(-zoo) == zoo\n    assert factorial(nan) == nan\n\n    assert factorial(x).func == factorial\n    assert factorial(0).func == factorial\n    assert factorial(1).func == factorial\n    assert factorial(-1).func == factorial\n    assert factorial(oo).func == factorial\n    assert factorial(-oo).func == factorial\n    assert factorial(zoo).func == factorial\n    assert factorial(-zoo).func == factorial\n    assert factorial(nan).func == factorial\n", "def test_factorial2_eval():\n    n = Symbol('n', integer=True)\n    assert factorial2(n).evalf() == 2**(n/2)*gamma(n/2 + 1)*Piecewise((1, Eq(Mod(n, 2), 0)), (sqrt(2)/sqrt(pi), Eq(Mod(n, 2), 1)))\n    assert factorial2(n + 1).evalf() == 2**((n+1)/2)*gamma((n+1)/2 + 1)*Piecewise((1, Eq(Mod(n+1, 2), 0)), (sqrt(2)/sqrt(pi), Eq(Mod(n+1, 2), 1)))\n    assert factorial2(2*n).evalf() == 2**n*gamma(n + 1)\n    assert factorial2(2*n + 1).evalf() == sqrt(2)*2**(n + 1/2)*gamma(n + 3/2)/sqrt(pi)\n", "def test_rising_and_falling_factorials_with_special_numbers():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True, positive=True)\n\n    assert rf(1, k) == factorial(k)\n    assert ff(1, k) == factorial(k)\n    assert rf(-1, k) == (-1)**(k - 1)*factorial(k - 1)\n    assert ff(-1, k) == (-1)**k*factorial(k)\n\n    assert rf(oo, k) == oo\n    assert ff(oo, k) == oo\n    assert rf(-oo, k) == (-1)**(k - 1)*oo if k.is_odd else oo\n    assert ff(-oo, k) == (-1)**k*oo\n\n    assert rf(0, k) == 0**k\n    assert ff(0, k) == 0**k\n    assert rf(n, 0) == 1\n    assert ff(n, 0) == 1\n\n    # Test Rising and Falling Factorial with special numbers (pi, E, GoldenRatio, EulerGamma)\n    from sympy import pi, E, GoldenRatio, EulerGamma\n    assert rf(pi, 2) == pi*(pi + 1)\n    assert ff(pi, 2) == pi*(pi - 1)\n    assert rf(E, 2) == E*(E + 1)\n    assert ff(E, 2) == E*(E - 1)\n    assert rf(GoldenRatio, 2) == GoldenRatio*(GoldenRatio + 1)\n    assert ff(GoldenRatio, 2) == GoldenRatio*(GoldenRatio - 1)\n    assert rf(EulerGamma, 2) == EulerGamma*(EulerGamma + 1)\n    assert ff(EulerGamma, 2) == EulerGamma*(EulerGamma - 1)\n"], "sample_1163": ["def test_polar_lift():\n    from sympy import pi, I, exp_polar, oo\n    x = Symbol('x')\n    assert polar_lift(4) == 4*exp_polar(0)\n    assert polar_lift(-4) == 4*exp_polar(I*pi)\n    assert polar_lift(-I) == exp_polar(-I*pi/2)\n    assert polar_lift(I + 2) == polar_lift(2 + I)\n    assert polar_lift(4*x) == 4*polar_lift(x)\n    assert polar_lift(4*polar_lift(x)) == 4*polar_lift(x)\n", "def test_polar_lift():\n    assert polar_lift(-4) == 4*exp_polar(I*pi)\n    assert polar_lift(-I) == exp_polar(-I*pi/2)\n    assert polar_lift(I + 2) != polar_lift(2 + I)\n\n    x = Symbol('x')\n    assert polar_lift(4*x) == 4*polar_lift(x)\n    assert polar_lift(4*polar_lift(x)) == 4*polar_lift(x)\n\n    a = Symbol('a', algebraic=True)\n    t = Symbol('t', transcendental=True)\n    x = Symbol('x')\n    assert polar_lift(a).is_algebraic is None\n    assert polar_lift(x).is_algebraic is None\n    assert polar_lift(t).is_algebraic is False\n\n    assert polar_lift(0).is_polar is True\n    assert polar_lift(1).is_polar is True\n    assert polar_lift(-1).is_polar is True\n\n    assert polar_lift(S.ComplexInfinity).is_polar is None\n", "def test_periodic_argument_edge_cases():\n    from sympy import periodic_argument, oo, nan, zoo, S, I, pi\n\n    assert periodic_argument(nan, oo) is nan\n    assert periodic_argument(nan, 2*pi) is nan\n    assert periodic_argument(zoo, oo) is nan\n    assert periodic_argument(zoo, 2*pi) is nan\n    assert periodic_argument(oo, oo) is nan\n    assert periodic_argument(oo, 2*pi) is nan\n    assert periodic_argument(-oo, oo) is nan\n    assert periodic_argument(-oo, 2*pi) is nan\n    assert periodic_argument(I*oo, oo) is nan\n    assert periodic_argument(I*oo, 2*pi) is nan\n    assert periodic_argument(I*oo, S.Infinity) is nan\n    assert periodic_argument(-I*oo, oo) is nan\n    assert periodic_argument(-I*oo, 2*pi) is nan\n    assert periodic_argument(-I*oo, S.Infinity) is nan\n    assert periodic_argument(I*S.Infinity, oo) is nan\n    assert periodic_argument(I*S.Infinity, 2*pi) is nan\n    assert periodic_argument(I*S.Infinity, S.Infinity) is nan\n    assert periodic_argument(-I*S.Infinity, oo) is nan\n    assert periodic_argument(-I*S.Infinity, 2*pi) is nan\n    assert periodic_argument(-I*S.Infinity, S.Infinity) is nan\n\n    assert periodic_argument(S.Zero, oo) is S.Zero\n    assert periodic_argument(S.Zero, 2*pi) is S.Zero\n    assert periodic_argument(S.Zero, S.Infinity) is S.Zero\n\n    assert periodic_argument(S.One, oo) is S.Zero\n    assert periodic_argument(S.One, 2*pi) is S.Zero\n    assert periodic_argument(S.One, pi) is S.Zero\n    assert periodic_argument(S.One, S.Infinity) is S.Zero\n\n    assert periodic_argument(-S.One, oo) is pi\n    assert periodic_argument(-S.One, 2*pi) is pi\n    assert periodic_argument(-S.One, pi) is pi\n    assert periodic_argument(-S.One, S.Infinity) is pi\n\n    assert periodic_argument(I, oo) is pi/2\n    assert periodic_argument(I, 2*pi) is pi/2\n    assert periodic_argument(I, pi) is pi", "def test_periodic_argument_and_principal_branch():\n    from sympy import exp_polar, periodic_argument, principal_branch, pi, I\n    # issue #14692\n    assert periodic_argument(exp_polar(I*pi/2)/exp_polar(I*pi), pi) == pi/2\n    assert principal_branch(exp_polar(I*pi/2)/exp_polar(I*pi), pi) == exp_polar(-I*pi/2)\n    # issue #14859\n    assert principal_branch(-I, pi) == exp_polar(-I*pi/2)\n    assert principal_branch(-I, 2*pi) == exp_polar(I*3*pi/2)\n    assert periodic_argument(-I, pi) == -pi/2\n    assert periodic_argument(-I, 2*pi) == -pi/2\n    assert periodic_argument(-I, pi) == periodic_argument(-I, 2*pi)\n    assert principal_branch(-I, pi) != principal_branch(-I, 2*pi)\n    assert periodic_argument(exp_polar(I*pi/2), 2*pi) == pi/2\n    assert periodic_argument(exp_polar(-I*pi/2), 2*pi) == -pi/2\n    assert principal_branch(-I, pi) == principal_branch(exp_polar(-I*pi/2), pi)\n", "def test_polar_lift():\n    from sympy import pi, oo, I, exp_polar, exp, zoo, polar_lift, S\n    assert polar_lift(0) == 0\n    assert polar_lift(-1) == exp_polar(I*pi)\n    assert polar_lift(I) == exp_polar(-I*pi/2)\n    assert polar_lift(-I) == exp_polar(I*pi/2)\n    assert polar_lift(oo) is oo\n    assert polar_lift(-oo) is -oo\n    assert polar_lift(S.ComplexInfinity) is S.ComplexInfinity\n    assert polar_lift(zoo) is S.ComplexInfinity\n    assert polar_lift(S.Half) == S.Half\n    assert polar_lift(3*I) == 3*exp_polar(-I*pi/2)\n    assert polar_lift(pi*I) == pi*exp_polar(-I*pi/2)\n\n    assert polar_lift(S.One + I) == (1 + I)\n    assert polar_lift(-S.One + I) == (I + exp_polar(I*pi))\n\n    assert polar_lift(exp_polar(2*pi*I)) == exp_polar(2*pi*I)\n    assert polar_lift(exp_polar(2*pi*I)*2) == 2*exp_polar(2*pi*I)\n    assert polar_lift(exp_polar(3*pi*I)*2) == 2*exp_polar(pi*I)\n\n    assert polar_lift(exp_polar(I*pi/2)*2) == 2*I\n    assert polar_lift(exp_polar(3*pi*I/2)*2) == -2*I\n\n    assert polar_lift(3*exp_polar(I*pi/4)) == 3*exp_polar(I*pi/4)\n\n    assert polar_lift(exp_polar(5*I*pi)/4) == exp_polar(I*pi/4)/4\n\n    assert polar_lift(5*exp_polar(I*pi/4)) == 5*exp_polar(I*pi/4)\n    assert polar_lift(I*exp_polar(I*pi/4)) == I*exp_polar(I*pi/4)\n\n    assert polar_lift(-5*exp_polar(I*pi/4)) == 5*exp_polar(3*I*pi/4)\n    assert polar_lift(-I*exp_polar(I*pi/4)) == I*exp_polar(3*I*pi/4)\n\n    x = S.Half\n    assert polar_lift(x)"], "sample_419": ["def test_absolute_max_with_min_num(self):\n    data = {\n        \"form-TOTAL_FORMS\": \"1001\",\n        \"form-INITIAL_FORMS\": \"0\",\n        \"form-MAX_NUM_FORMS\": \"0\",\n    }\n    LimitedFavoriteDrinksFormSet = formset_factory(\n        FavoriteDrinkForm,\n        min_num=30,\n        absolute_max=1000,\n    )\n    formset = LimitedFavoriteDrinksFormSet(data=data)\n    self.assertIs(formset.is_valid(), False)\n    self.assertEqual(len(formset.forms), 1000)\n    self.assertEqual(\n        formset.non_form_errors(),\n        [\"Please submit at least 30 forms.\"],\n    )\n", "def test_management_form_fields_order(self):\n    \"\"\"\n    The management form fields are ordered correctly.\n    \"\"\"\n    formset = FavoriteDrinksFormSet()\n    self.assertEqual(\n        list(formset.management_form.fields),\n        [TOTAL_FORM_COUNT, INITIAL_FORM_COUNT, MIN_NUM_FORM_COUNT, MAX_NUM_FORM_COUNT],\n    )\n", "def test_deletion_widget_custom_widget_attrs(self):\n    class CustomDeletionWidget(CheckboxInput):\n            kwargs[\"attrs\"] = {\"class\": \"deletion\"}\n            super().__init__(*args, **kwargs)\n\n    class CustomDeletionFormSet(BaseFormSet):\n        deletion_widget = CustomDeletionWidget\n\n    ChoiceFormSet = formset_factory(Choice, formset=CustomDeletionFormSet, can_delete=True)\n    formset = ChoiceFormSet(auto_id=False)\n    self.assertHTMLEqual(\n        \"\\n\".join(form.as_ul() for form in formset.forms),\n        '<li>Choice: <input type=\"text\" name=\"form-0-choice\"></li>'\n        '<li>Votes: <input type=\"number\" name=\"form-0-votes\"></li>'\n        '<li>Delete: <input class=\"deletion\" type=\"checkbox\" name=\"form-0-DELETE\"></li>'\n    )\n", "    def test_formset_absolute_max_with_min_num(self):\n        \"\"\"\n        absolute_max is also taken into account when min_num is used to\n        require extra forms.\n        \"\"\"\n        data = {\n            \"form-TOTAL_FORMS\": \"2001\",\n            \"form-INITIAL_FORMS\": \"0\",\n            \"form-MAX_NUM_FORMS\": \"0\",\n        }\n        AbsoluteMaxFavoriteDrinksFormSet = formset_factory(\n            FavoriteDrinkForm, min_num=3000, absolute_max=3000\n        )\n        formset = AbsoluteMaxFavoriteDrinksFormSet(data=data)\n        self.assertIs(formset.is_valid(), False)\n        self.assertEqual(len(formset.forms), 3000)\n        # absolute_max provides a hard limit.\n        data[\"form-TOTAL_FORMS\"] = \"3001\"\n        formset = AbsoluteMaxFavoriteDrinksFormSet(data=data)\n        self.assertIs(formset.is_valid(), False)\n        self.assertEqual(len(formset.forms), 3000)\n        self.assertEqual(\n            formset.non_form_errors(),\n            [\"Please submit at most 1000 forms.\"],\n        )\n", "def test_management_form_rendered_with_management_form_template(self):\n    \"\"\"Management form uses a template from the formset's renderer.\"\"\"\n    class CustomRenderer(TemplatesSetting):\n        formset_template_name = \"a/custom/formset/template.html\"\n        management_form_template_name = \"a/custom/management_form/template.html\"\n\n    ChoiceFormSet = formset_factory(Choice, renderer=CustomRenderer)\n\n    formset = ChoiceFormSet()\n\n    # The management form is rendered with the custom template.\n    self.assertEqual(formset.management_form.template_name, \"a/custom/management_form/template.html\")\n\n    # The rest of the formset uses the formset template.\n    self.assertEqual(formset.template_name, \"a/custom/formset/template.html\")\n"], "sample_1090": ["def test_comp():\n    assert S.comp(S(1.000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "def test_power():\n    with evaluate(False):\n        assert isinstance(x**x, Pow)\n        assert isinstance(x**(x**x), Pow)\n        assert isinstance(x**y, Pow)\n        assert isinstance(x**(y**y), Pow)\n        assert isinstance(x**(2**y), Pow)\n\n        assert isinstance(S.One**x, Pow)\n        assert isinstance(S.One**(x**x), Pow)\n        assert isinstance(S.One**y, Pow)\n        assert isinstance(S.One**(y**y), Pow)\n        assert isinstance(S.One**(2**y), Pow)\n\n        assert isinstance((2*x)**x, Pow)\n        assert isinstance((2*x)**(x**x), Pow)\n        assert isinstance((2*x)**y, Pow)\n        assert isinstance((2*x)**(y**y), Pow)\n        assert isinstance((2*x)**(2**y), Pow)\n\n    with evaluate(True):\n        assert (x**x).func is Pow\n        assert (x**(x**x)).func is Pow\n        assert (x**y).func is Pow\n        assert (x**(y**y)).func is Pow\n        assert (x**(2**y)).func is Pow\n\n        assert (S.One**x).func is Pow\n        assert (S.One**(x**x)).func is Pow\n        assert (S.One**y).func is Pow\n        assert (S.One**(y**y)).func is Pow\n        assert (S.One**(2**y)).func is Pow\n\n        assert ((2*x)**x).func is Pow\n        assert ((2*x)**(x**x)).func is Pow\n        assert ((2*x)**y).func is Pow\n        assert ((2*x)**(y**y)).func is Pow\n        assert ((2*x)**(2**y)).func is Pow\n\n    with evaluate(False):\n        assert (x + 1)**2 == Pow(x + 1, 2)\n        assert (x + 1)**(2 + 1) == Pow(x + 1, 2 + 1)\n        assert (x + y)**2 == Pow(x + y, 2)\n        assert (x + y)**(2 + 1) == Pow(x + y, 2 + 1)\n\n    with evaluate(True):\n        assert (x + 1)**2 == x**2 + 2*x + 1\n        assert (x + 1)**(", "def test_floating_point_representation():\n    with evaluate(False):\n        assert S('0.1').is_Float\n        assert S('1e-2').is_Float\n        assert S('.1').is_Float\n        assert S('1e-2/3.2').is_Float\n        assert S('12345678901234567890.1234567890123456789').is_Float\n\n    with evaluate(False):\n        assert S(1e-20).is_Float\n        assert S('1e-20').is_Float\n\n    with evaluate(False):\n        assert S(-0.1).is_Float\n        assert S(-1e-2).is_Float\n        assert S(-.1).is_Float\n\n    with evaluate(False):\n        assert S('0.1') != S(0.1)\n        assert S('0.1') == S(1)/10\n        assert S('0.1') == S(1)/S(10)\n        assert S('0.1') == S.Rational(1, 10)\n        assert S('0.1') != S.Float(0.1, 20)\n        assert S('0.1') == S.Float(0.1, 1)\n", "def test_numbers():\n    assert isinstance(S(0.5), S.Half)\n    assert isinstance(S(0.5 + 0.5), S.One)\n    assert isinstance(S(1/2), S.Half)\n    assert isinstance(S(1/2 + 1/2), S.One)\n    assert isinstance(S(2)/2, S.One)\n    assert isinstance(S(2)/S(2), S.One)\n    assert isinstance(S(2)/S(4), S.Half)\n    assert isinstance(S(0.5)/S(2), S.Quarter)\n    assert isinstance(S(0.25)/S(2), S.Eighth)\n    assert isinstance(S(1)/S(2) + S(1)/S(2), S.One)\n    assert isinstance(S(1)/S(2) + S(0.5), S.One)\n    assert isinstance(S(1)/S(2) + 0.5, S.One)\n    assert isinstance(S(0.5) + 0.5, S.One)\n    assert isinstance(S(0.5) + S(0.5), S.One)\n    assert isinstance(S(1)/2, S.Half)\n    assert isinstance(S(1)/S(2), S.Half)\n    assert isinstance(1/S(2), S.Half)\n    assert isinstance(1/2, S.Half)\n    assert isinstance(S(2)/S(4), S.Half)\n    assert isinstance(S(1)/2, S.Half)\n    assert isinstance(S(1)/S(2), S.Half)\n    assert isinstance(S(2)/S(4), S.Half)\n    assert isinstance(S(1)/S(2) + S(1)/S(4), S.ThreeQuarters)\n    assert isinstance(S(1)/S(2) + 0.25, S.ThreeQuarters)\n    assert isinstance(S(1)/S(2) + 0.25, S.ThreeQuarters)\n    assert isinstance(S(0.75), S.ThreeQuarters)\n    assert isinstance(S(0.75 + 0.25), S.One)\n    assert isinstance(S(0.75) + 0.25, S.One)\n    assert isinstance(S(0.75) + S(0.25), S.One)\n    assert isinstance(0.75 + S(0.25), S.One)\n", "def test_evalf():\n    from sympy.core.numbers import Float\n    from sympy import S, sqrt, pi, E, sin, cos\n    from sympy.abc import x, y, z\n\n    assert S(1.1).evalf(3) == 1.10\n    assert S('1.1').evalf(3) == 1.10\n    assert S('1.1').evalf(20) == 1.1000000000000000888\n    assert S('1.1').evalf(15) == 1.0999999999999999\n\n    # issue #10649\n    assert S('1e-17').evalf(100) == 1.00000000000000000000000000000e-17\n\n    assert S('1.0001').evalf(5) == 1.0001\n    assert S(1.0001).evalf(5) == 1.0001\n\n    assert S(1.0001).evalf(10) == 1.0001000000\n\n    assert S(1.1).evalf(3) == 1.10\n    assert S(1.1).evalf(20) == 1.099999999999999991\n    assert S(1.1).evalf(15) == 1.0999999999999999\n\n    assert S(1.0001).evalf(10) == 1.0001000000\n\n    assert S(1).evalf(10) == 1.0000000000\n\n    assert S(12345678901234567890).evalf() == 1.2345678901234568e+16\n    assert S(12345678901234567890).evalf(50) == \\\n        1.23456789012345678901234567890e+16\n\n    assert S(0.1).evalf(3) == 0.100\n    assert S(0.1).evalf(20) == 0.0999999999999999956\n    assert S(0.1).evalf(15) == 0.09999999999999999\n\n    assert S(1e-10).evalf(20) == 9.9999999999999997e-11\n   "], "sample_46": ["    def test_exact(self):\n        lookup = Exact('field', '550e8400e29b41d4a716446655440000')\n        self.assertEqual(lookup.as_sql(None, None), ('field exact %s', ('550e8400e29b41d4a716446655440000',)))\n", "    def setUpTestData(cls):\n        cls.obj = UUIDModel.objects.create(field=uuid.uuid4())\n        cls.related_obj = RelatedToUUIDModel.objects.create(uuid_fk=cls.obj)\n", "    def setUpTestData(cls):\n        cls.objs = [\n            UUIDModel.objects.create(field=uuid.uuid4()),\n            UUIDModel.objects.create(field='550e8400e29b41d4a716446655440000'),\n            UUIDModel.objects.create(field=None),\n        ]\n", "    def setUpTestData(cls):\n        cls.obj = UUIDModel.objects.create(field=uuid.uuid4())\n", "    def test_exact_lookup_with_uuid_instance(self):\n        instance = UUIDModel.objects.create(field=uuid.uuid4())\n        self.assertEqual(UUIDModel.objects.get(field__exact=instance.field), instance)\n"], "sample_177": ["def test_related_models_tuples(self):\n    class Author(models.Model):\n        pass\n\n    class Book(models.Model):\n        author = models.ForeignKey(Author, models.CASCADE)\n        contributors = models.ManyToManyField(Author)\n\n    class Publisher(models.Model):\n        book = models.OneToOneField(Book, models.CASCADE)\n\n    self.assertEqual(\n        get_related_models_tuples(Author),\n        {('tests', 'book'), ('tests', 'publisher'), ('tests', 'book_contributors')},\n    )\n    self.assertEqual(\n        get_related_models_tuples(Book),\n        {('tests', 'author'), ('tests', 'publisher')},\n    )\n    self.assertEqual(\n        get_related_models_tuples(Publisher),\n        {('tests', 'book')},\n    )\n", "    def test_get_related_models_tuples(self):\n        new_apps = Apps()\n\n        class A(models.Model):\n            class Meta:\n                app_label = \"migrations\"\n                apps = new_apps\n\n        class B(A):\n            class Meta:\n                app_label = \"migrations\"\n                apps = new_apps\n\n        class C(models.Model):\n            a = models.ForeignKey(A, models.CASCADE)\n\n            class Meta:\n                app_label = \"migrations\"\n                apps = new_apps\n\n        class D(models.Model):\n            c = models.ForeignKey(C, models.CASCADE)\n\n            class Meta:\n                app_label = \"migrations\"\n                apps = new_apps\n\n        project_state = ProjectState()\n        project_state.add_model(ModelState.from_model(A))\n        project_state.add_model(ModelState.from_model(B))\n        project_state.add_model(ModelState.from_model(C))\n        project_state.add_model(ModelState.from_model(D))\n\n        A = project_state.apps.get_model('migrations', 'A')\n        B = project_state.apps.get_model('migrations', 'B')\n        C = project_state.apps.get_model('migrations', 'C')\n        D = project_state.apps.get_model('migrations', 'D')\n\n        self.assertEqual(get_related_models_tuples(A), {('migrations', 'b')})\n        self.assertEqual(get_related_models_tuples(B), set())\n        self.assertEqual(get_related_models_tuples(C), {('migrations', 'a')})\n        self.assertEqual(get_related_models_tuples(D), {('migrations', 'c')})\n", "def test_get_related_models_tuples(self):\n    class Author(models.Model):\n        name = models.CharField(max_length=255)\n\n        class Meta:\n            app_label = \"migrations\"\n\n    class Book(models.Model):\n        author = models.ForeignKey(Author, models.CASCADE)\n        contributors = models.ManyToManyField(Author)\n\n        class Meta:\n            app_label = \"migrations\"\n\n    class Magazine(models.Model):\n        authors = models.ManyToManyField(Author)\n\n        class Meta:\n            app_label = \"migrations\"\n\n    new_apps = Apps(['migrations'])\n    Author._meta.apps = new_apps\n    Book._meta.apps = new_apps\n    Magazine._meta.apps = new_apps\n    related_models = get_related_models_tuples(Book)\n    self.assertEqual(related_models, {(\"migrations\", \"author\"), (\"migrations\", \"magazine\")})\n    related_models = get_related_models_tuples(Magazine)\n    self.assertEqual(related_models, {(\"migrations\", \"author\")})\n", "def test_get_related_models_tuples(self):\n    \"\"\"\n    Tests get_related_models_tuples returns the correct related models.\n    \"\"\"\n    new_apps = Apps(['migrations'])\n\n    class A(models.Model):\n        class Meta:\n            app_label = \"migrations\"\n            apps = new_apps\n\n    class B(models.Model):\n        a = models.ForeignKey(A, models.CASCADE)\n\n        class Meta:\n            app_label = \"migrations\"\n            apps = new_apps\n\n    class C(models.Model):\n        b = models.ForeignKey(B, models.CASCADE)\n\n        class Meta:\n            app_label = \"migrations\"\n            apps = new_apps\n\n    class D(models.Model):\n        a = models.ForeignKey(A, models.CASCADE)\n\n        class Meta:\n            app_label = \"migrations\"\n            apps = new_apps\n\n    class E(models.Model):\n        b = models.ForeignKey(B, models.CASCADE)\n\n        class Meta:\n            app_label = \"migrations\"\n            apps = new_apps\n\n    # Make a ProjectState and render it\n    project_state = ProjectState()\n    project_state.add_model(ModelState.from_model(A))\n    project_state.add_model(ModelState.from_model(B))\n    project_state.add_model(ModelState.from_model(C))\n    project_state.add_model(ModelState.from_model(D))\n    project_state.add_model(ModelState.from_model(E))\n    project_state.apps  # Fill the apps cached property\n\n    A = project_state.apps.get_model(\"migrations\", \"A\")\n    B = project_state.apps.get_model(\"migrations\", \"B\")\n    C = project_state.apps.get_model(\"migrations\", \"C\")\n    D = project_state.apps.get_model(\"migrations\", \"D\")\n    E = project_state.apps.get_model(\"migrations\", \"E\")\n\n    self.assertEqual(\n        get_related_models_tuples(A),\n        {(\"migrations\", \"b\"), (\"migrations\", \"c\"), (\"migrations\", \"d\")},\n    )\n    self.assertEqual(\n        get_related_models_tuples(B),\n        {(\"migrations\", \"c\"), (\"migrations\", \"e\")},\n    )\n    self.assertEqual(\n        get_related_models_tuples(C),\n        set(),\n    )\n    self.assertEqual(\n        get_related_models_tuples(D),\n        set(),\n    )\n    self.assertEqual(\n        get_related_models_tuples(E),\n        set(),\n    )\n", "def test_clone_project_state(self):\n    \"\"\"\n    ProjectState and StateApps should be properly cloned, including\n    all the models.\n    \"\"\"\n    project_state = ProjectState()\n    project_state.add_model(ModelState(\n        app_label=\"migrations\",\n        name=\"Tag\",\n        fields=[\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=100)),\n            (\"hidden\", models.BooleanField()),\n        ],\n        managers=[\n            ('food_mgr', FoodManager('a', 'b')),\n            ('food_qs', FoodQuerySet.as_manager()),\n        ]\n    ))\n\n    cloned_state = project_state.clone()\n\n    # The original model should not be the same as the cloned one\n    self.assertIsNot(\n        project_state.apps.get_model('migrations', 'tag'),\n        cloned_state.apps.get_model('migrations', 'tag')\n    )\n\n    # The managers should be different but should be pointing to the correct\n    # model version.\n    model = project_state.apps.get_model('migrations', 'tag')\n    cloned_model = cloned_state.apps.get_model('migrations', 'tag')\n    self.assertIsNot(model.food_mgr, cloned_model.food_mgr)\n    self.assertIs(model, model.food_mgr.model)\n    self.assertIs(cloned_model, cloned_model.food_mgr.model)\n"], "sample_911": ["def test_class_definitions():\n    check('class', 'A final public : B', {1: 'A', 2: '1A'}, output='A final : public B')\n", "def test_template_specializations():\n    check('function', 'template<> void f<int>', {2: 'I_iE1fv', 4: 'I_iE1fvv'})\n    check('function', 'template void f<int>', {2: 'I_iE1fv', 4: 'I_iE1fvv'},\n          output='template<> void f<int>')\n    check('member', 'template<> int A<int>::a', {2: 'IEN1AIiE1aE'})\n    check('member', 'template int A<int>::a', {2: 'IEN1AIiE1aE'},\n          output='template<> int A<int>::a')\n    check('member', 'template<> template<> int A<int>::B<int>::b', {2: 'IEIEN1AIiE1BIiE1bE'})\n    check('member', 'template template<> int A<int>::B<int>::b', {2: 'IEIEN1AIiE1BIiE1bE'},\n          output='template<> template<> int A<int>::B<int>::b')  # same as above\n    check('member', 'template<> template int A<int>::B<int>::b', {2: 'IEIEN1AIiE1BIiE1bE'},\n          output='template<> template<> int A<int>::B<int>::b')  # same as above\n    check('function', 'template<> void f<int>(int)', {2: 'I_iE1fvi', 4: 'I_iE1fvvi'})\n    check('function', 'template void f<int>(int)', {2: 'I_iE1fvi', 4: 'I_iE1fvvi'},\n          output='template<> void f<int>(int)')\n", "def test_stringify():\n        ast = parse('type', decl)\n        res = str(ast.declaration)\n        if res != string:\n            print(\"\")\n            print(\"Input:    \", decl)\n            print(\"Result:   \", res)\n            print(\"Expected: \", string)\n            raise DefinitionError(\"\")\n        assert ast.declaration.get_id(version) is not None\n\n    check('std::vector<std::pair<std::string, long long>>', 2, 'std::vector<std::pair<std::string, long long>>')\n    check('std::vector<std::pair<std::string, long long>>', 3, 'std::vector<std::pair<std::string, long long>>')\n    check('std::vector<std::pair<std::string, long long>>', 4, 'std::vector<std::pair<std::string, long long>>')\n    check('std::vector<std::pair<std::string, long long>, long>', 2, 'std::vector<std::pair<std::string, long long>, long>')\n    check('std::vector<std::pair<std::string, long long>, long>', 3, 'std::vector<std::pair<std::string, long long>, long>')\n    check('std::vector<std::pair<std::string, long long>, long>', 4, 'std::vector<std::pair<std::string, long long>, long>')\n", "def test_template_template_parameters():\n    check('function',\n          \"template<template <typename T> typename F> \"\n          \"void foo(F<int>)\",\n          {2: \"I0E3fooI0I1T1FEiEEE\",\n           3: \"I0E3fooI0I1T1FEiEEE\",\n           4: \"I0E3foovI0I1T1FEiEEE\"})\n    check('function',\n          \"template<template <typename... Ts> typename F> \"\n          \"void foo(F<int>)\",\n          {2: \"IDpE3fooI0I_Dp1T1FEiEEE\",\n           3: \"IDpE3fooI0I_Dp1T1FEiEEE\",\n           4: \"IDpE3foovI0I_Dp1T1FEiEEE\"})\n    check('function',\n          \"template<template <typename T> typename... Fs> \"\n          \"void foo(Fs<int>...)\",\n          {2: \"IDpE3fooIDpI0I1T1FEiEEE\",\n           3: \"IDpE3fooIDpI0I1T1FEiEEE\",\n           4: \"IDpE3foovIDpI0I1T1FEiEEE\"})\n    check('function',\n          \"template<template <typename... Ts> typename... Fs> \"\n          \"void foo(Fs<int>...)\",\n          {2: \"II_DpE3fooI_DpI_Dp1T1FEiEEE\",\n           3: \"II_DpE3fooI_DpI_Dp1T1FEiEEE\",\n           4: \"II_DpE3foovI_DpI_Dp1T1FEiEEE\"})\n", "def test_template_ids():\n    # check template id's (i.e., type alias) in various positions\n\n    # in global scope\n    check('type', 'template<typename T> using A = B<T>', {2: 'I0E1A'})\n    check('type', 'template<int> using A = B<42>', {2: 'I_iE1A'})\n\n    # in namespace scope\n    check('type', 'namespace N { template<typename T> using A = B<T>; }',\n          {2: 'N1A'})\n\n    # in class scope\n    check('class', 'template<typename T> struct C { using A = B<T>; };',\n          {2: 'I0E1C'})\n    check('class', 'template<int> struct C { using A = B<42>; };',\n          {2: 'I_iE1C'})\n\n    # in template class\n    check('class', 'template<typename T> template<typename U> struct C { using A = B<T>; };',\n          {2: 'I0E0E1C'})\n    check('class', 'template<typename T> template<int> struct C { using A = B<42>; };',\n          {2: 'I0E_iE1C'})\n\n    # in function scope\n    check('function', 'template<typename T> void f() { using A = B<T>; }',\n          {2: 'I0E1fv'})\n\n    # in template function\n    check('function', 'template<typename T> template<typename U> void f() { using A = B<T>; }',\n          {2: 'I0E0E1fv'})\n\n    # in template parameter list\n    check('type', 'template<typename T, template<typename> typename B> using A = B<T>;',\n          {2: 'I0II0E0E1A'})\n\n    # from #4490\n    check('type', 'template<auto T> using type = T;', {2: 'I_DaE4typeE'})\n\n    # from #4565\n    check('type', 'template<int> using type = int;', {2: 'I_iE4typeE'})\n    check('type', 'template<int> using type = int[5];', {2: 'I_iE4typeE'})\n"], "sample_915": ["def test_isenumclass_and_isenumattribute():\n    from enum import Enum\n\n    class Color(Enum):\n        RED = 1\n        GREEN = 2\n        BLUE = 3\n\n    assert inspect.isenumclass(Color) is True\n    assert inspect.isenumattribute(Color.RED) is True\n    assert inspect.isenumattribute(Color) is False\n    assert inspect.isenumclass(\"enum\") is False\n", "def test_isabstractmethod():\n    class AbstractClass:\n            pass\n\n            \"\"\"This is an abstract method\"\"\"\n            raise NotImplementedError\n\n    class ConcreteClass(AbstractClass):\n            \"\"\"This is a concrete method\"\"\"\n            pass\n\n    assert inspect.isabstractmethod(AbstractClass().abstract_method) is True\n    assert inspect.isabstractmethod(ConcreteClass().abstract_method) is False\n    assert inspect.isabstractmethod(AbstractClass.abstract_method) is True\n    assert inspect.isabstractmethod(ConcreteClass.abstract_method) is False\n", "def test_isdescriptor_with_mock_object(mocker):\n    mock_descriptor = mocker.Mock()\n    mock_descriptor.__get__ = mocker.Mock(return_value=None)\n    assert inspect.isdescriptor(mock_descriptor) is True\n\n    mock_descriptor = mocker.Mock()\n    mock_descriptor.__set__ = mocker.Mock(return_value=None)\n    assert inspect.isdescriptor(mock_descriptor) is True\n\n    mock_descriptor = mocker.Mock()\n    mock_descriptor.__delete__ = mocker.Mock(return_value=None)\n    assert inspect.isdescriptor(mock_descriptor) is True\n\n    mock_descriptor = mocker.Mock()\n    assert inspect.isdescriptor(mock_descriptor) is False\n", "def test_getdoc_inherited_docstring():\n    class Foo:\n            \"\"\"docstring.\"\"\"\n\n    class Bar(Foo):\n            # inherited method with no docstring\n            pass\n\n    assert inspect.getdoc(Bar.meth, getattr, False, Bar, \"meth\") == \"docstring.\"\n\n", "def test_isabstractmethod():\n    class AbstractBaseClass:\n        @abstractmethod\n            pass\n\n    class ConcreteClass(AbstractBaseClass):\n            pass\n\n    assert inspect.isabstractmethod(AbstractBaseClass.abstract_method) is True\n    assert inspect.isabstractmethod(ConcreteClass.abstract_method) is False\n"], "sample_363": ["    def test_AutocompleteMixin_get_url(self):\n        field = forms.ModelChoiceField(Album.objects.all())\n        admin_site = admin.AdminSite()\n        widget = widgets.AutocompleteMixin(field, admin_site)\n        self.assertEqual(widget.get_url(), '/admin:autocomplete')\n", "    def test_get_url(self):\n        w = widgets.AutocompleteSelect(\n            field=models.Field(), admin_site=widget_admin_site\n        )\n        self.assertEqual(\n            w.get_url(),\n            reverse('admin:widgetadmin_autocomplete'),\n        )\n", "    def test_url_name(self):\n        class TestWidget(AutocompleteMixin, forms.Select):\n            url_name = 'autocomplete_test'\n\n        widget = TestWidget(field=None, admin_site=None)\n        self.assertEqual(widget.get_url(), '/admin/autocomplete_test/')\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n", "    def test_get_url(self):\n        from django.urls import reverse\n        class MockAdminSite:\n            name = 'myadmin'\n        class MockField:\n            model = MockAdminSite\n        w = widgets.AutocompleteSelect(MockField, MockAdminSite())\n        self.assertEqual(w.get_url(), reverse('myadmin:autocomplete'))\n"], "sample_1052": ["def test_julia_code_reserved_words():\n    x, y, z = symbols('if, while, z')\n    expr = (x + y) * z\n    routine = make_routine(\"test\", expr)\n    code_gen = JuliaCodeGen()\n    source = get_string(code_gen.dump_jl, [routine])\n    expected = (\n        \"function test(if_, while_, z)\\n\"\n        \"    test_result = z*(if_ + while_)\\n\"\n        \"    return test_result\\n\"\n        \"end\\n\"\n    )\n    assert source == expected\n", "def test_julia_output_arg():\n    from sympy import sin, cos, Equality\n    x, y, z = symbols(\"x,y,z\")\n    r = make_routine(\"foo\", [Equality(y, sin(x)), cos(x)])\n    c = JuliaCodeGen()\n    result = c.write([r], \"test\", header=False, empty=False)\n    assert result[0][0] == \"test.jl\"\n    expected = (\n        \"function foo(x)\\n\"\n        \"   y = sin(x)\\n\"\n        \"   return cos(x), y\\n\"\n        \"end\\n\"\n    )\n    assert result[0][1] == expected\n", "def test_fortran_module():\n    x = symbols('x')\n    result = codegen(('test',x), 'F95', 'test', header=False, empty=False)\n    source = (result[0][1])\n    expected = (\n        \"REAL*8 function test(x)\\n\"\n        \"implicit none\\n\"\n        \"REAL*8, intent(in) :: x\\n\"\n        \"test = x\\n\"\n        \"end function\\n\")\n    assert source == expected\n\n    result = codegen(('test',x), 'F95', 'test', header=False, empty=False, standard=\"F95\")\n    source = (result[0][1])\n    expected = (\n        \"module test_module\\n\"\n        \"contains\\n\"\n        \"REAL*8 function test(x)\\n\"\n        \"implicit none\\n\"\n        \"REAL*8, intent(in) :: x\\n\"\n        \"test = x\\n\"\n        \"end function\\n\"\n        \"end module\\n\")\n    assert source == expected\n", "def test_if_function():\n    # Issue #11855\n    from sympy import symbols, If\n    x = symbols('x')\n    expr = If(x > 0, x, -x)\n    result = codegen(('test', expr), \"c\", \"test\", header=False, empty=False)\n    source = result[0][1]\n    expected = (\n        '#include \"test.h\"\\n'\n        '#include <math.h>\\n'\n        'double test(double x) {\\n'\n        '   double test_result;\\n'\n        '   if (x > 0) {\\n'\n        '      test_result = x;\\n'\n        '   }\\n'\n        '   else {\\n'\n        '      test_result = -x;\\n'\n        '   }\\n'\n        '   return test_result;\\n'\n        '}\\n'\n    )\n    assert source == expected\n", "def test_octave_loops():\n    from sympy.tensor import IndexedBase, Idx\n    from sympy import symbols\n\n    n, m = symbols('n,m', integer=True)\n    A, x, y = map(IndexedBase, 'Axy')\n    i = Idx('i', m)\n    j = Idx('j', n)\n\n    (f1, code), (f2, interface) = codegen(\n        ('matrix_vector', Eq(y[i], A[i, j]*x[j])), \"Octave\", \"file\", header=False, empty=False)\n\n    assert f1 == 'file.m'\n    expected = (\n        'function [%(y)s] = matrix_vector(A, n, m, x)\\n'\n        '  %(y)s = zeros(m, 1);\\n'\n        '  for i = 1:m\\n'\n        '    %(y)s(i) = 0;\\n'\n        '    for j = 1:n\\n'\n        '      %(y)s(i) = %(y)s(i) + A(i, j)*x(j);\\n'\n        '    end\\n'\n        '  end\\n'\n        'end\\n'\n    )\n\n    assert code == expected % {'y': 'y'} or code == expected % {'y': 'Y'}\n\n    assert f2 == 'file.m'\n    assert interface == (\n        '%   file.m  Autogenerated by sympy\\n'\n        '%\\n'\n        'function [%(y)s] = matrix_vector(A, n, m, x)\\n'\n        '%   %(y)s = zeros(m, 1);\\n'\n        '%   for i = 1:m\\n'\n        '%     %(y)s(i) = 0;\\n'\n        '%     for j = 1:n\\n'\n        '%       %(y)s(i) = %(y)s(i) + A(i, j)*x(j);\\n'\n        '%     end\\n'\n        '%   end\\n'\n        'end\\n'\n    ) % {'y': 'y'} or interface == expected % {'y': 'Y'}\n"], "sample_497": ["    def test_get_view_interval(self):\n        fig, ax = plt.subplots()\n        axis = ax.xaxis\n        assert axis.get_view_interval() == (0, 1)\n", "    def test_get_minpos(self):\n        fig, ax = plt.subplots()\n        assert ax.yaxis.get_minpos() == 0.0\n        assert ax.xaxis.get_minpos() == 0.0\n        ax.set_ylim(2, 3)\n        ax.set_xlim(2, 3)\n        assert ax.yaxis.get_minpos() == 2.0\n        assert ax.xaxis.get_minpos() == 2.0\n", "    def test_contains(self):\n        fig, ax = plt.subplots()\n        assert ax.yaxis.contains((0, 0)) == (True, {})\n        assert ax.yaxis.contains((1, 1)) == (False, {})\n        assert ax.xaxis.contains((0, 0)) == (True, {})\n        assert ax.xaxis.contains((1, 1)) == (False, {})\n", "def test_axis_date():\n    import datetime\n    fig, ax = plt.subplots()\n    x = [datetime.datetime(2000, 1, 1), datetime.datetime(2001, 1, 1)]\n    y = [1, 2]\n    ax.plot(x, y)\n    ax.xaxis.axis_date()\n    ticks = ax.xaxis.get_major_ticks()\n    assert len(ticks) == 2\n    assert ax.xaxis.get_major_formatter().__class__.__name__ == 'DateFormatter'\n    assert ax.xaxis.get_major_locator().__class__.__name__ == 'AutoDateLocator'\n", "    def test_set_default_intervals(self):\n        fig, ax = plt.subplots()\n        ax.set_xlim(0, 1)\n        ax.set_ylim(0, 1)\n        ax.xaxis.set_major_locator(mticker.MaxNLocator(nbins=5))\n        ax.yaxis.set_major_locator(mticker.MaxNLocator(nbins=5))\n        ax.xaxis.set_major_formatter(mticker.ScalarFormatter())\n        ax.yaxis.set_major_formatter(mticker.ScalarFormatter())\n\n        # axis data and view limits\n        assert_array_equal(ax.get_xlim(), (0, 1))\n        assert_array_equal(ax.get_ylim(), (0, 1))\n\n        # set axis data and view limits using set_default_intervals\n        ax.xaxis.set_default_intervals()\n        ax.yaxis.set_default_intervals()\n\n        # check limits set correctly\n        assert_array_equal(ax.get_xlim(), (0, 1))\n        assert_array_equal(ax.get_ylim(), (0, 1))\n"], "sample_37": ["def test_ctype_priority():\n    header = \"\"\"", "def test_to_fits_2():\n    \"\"\"\n    Test to_fits() with SIP distortion.\n    \"\"\"\n    fits_name = get_pkg_data_filename('data/sip.fits')\n    w = wcs.WCS(fits_name)\n    wfits = w.to_fits()\n    assert isinstance(wfits, fits.HDUList)\n    assert isinstance(wfits[0], fits.PrimaryHDU)\n    assert isinstance(wfits[1], fits.ImageHDU)\n    assert wfits[1].name == \"D2IMARR\"\n    # SIP distortion is present\n    assert w.sip is not None\n    # Check that SIP distortion keywords are written to the header\n    assert \"A_ORDER\" in wfits[0].header\n    assert \"A_0_0\" in wfits[0].header\n", "def test_copy():\n    \"\"\"\n    Regression test for #5845\n    \"\"\"\n    w = wcs.WCS(naxis=2)\n    w.wcs.ctype = ['RA---CAR', 'DEC--CAR']\n    w.wcs.crval = [12.5, 34.2]\n    w.wcs.cdelt = [0.1, 0.1]\n    w.wcs.crpix = [100, 200]\n    w2 = w.copy()\n    w2.wcs.crpix[0] = 1\n    assert_array_equal(w.wcs.crpix, [100, 200])\n", "def test_single_pixel_input():\n    \"\"\"\n    Test that WCS methods can handle single pixel input\n\n    Fixes #5993.\n    \"\"\"\n    w = wcs.WCS(naxis=2)\n    w.wcs.ctype = [\"RA---CAR\", \"DEC--CAR\"]\n    w.wcs.cdelt = [10, 10]\n    w.wcs.crval = [-90, 90]\n    w.wcs.crpix = [1, 1]\n\n    result = w.wcs_pix2world(1, 1, 0)\n    assert_array_almost_equal_nulp(result, [-89, 90])\n\n    result = w.wcs_world2pix(-89, 90, 0)\n    assert_array_almost_equal_nulp(result, [1, 1])\n\n    result = w.all_pix2world(1, 1, 0)\n    assert_array_almost_equal_nulp(result, [-89, 90])\n\n    result = w.all_world2pix(-89, 90, 0)\n    assert_array_almost_equal_nulp(result, [1, 1])\n", "def test_reject_sip_pj():\n    \"\"\"\n    Test that when relax=None or relax=False, SIP coefficients are not\n    written to the header if they were not present in the original header.\n    \"\"\"\n    header_string = \"\"\""], "sample_373": ["    def test_get_view_func(self):\n        view = 'django.contrib.admindocs.views.BaseAdminDocsView'\n        view_func = views.ViewDetailView._get_view_func(view)\n        self.assertEqual(view_func.__name__, 'BaseAdminDocsView')\n", "    def test_get_view_func_from_callback(self):\n        url = reverse('django-admindocs-views-detail', args=['django.contrib.admindocs.views.BaseAdminDocsView'])\n        view = ViewDetailView()\n        view.kwargs = {'view': 'django.contrib.admindocs.views.BaseAdminDocsView'}\n        view_func = view._get_view_func(view.kwargs['view'])\n        self.assertIsNotNone(view_func)\n", "    def setUp(self):\n        self.model = models.Model()\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n", "    def setUp(self):\n        self.url_resolver = get_resolver(get_urlconf())\n"], "sample_41": ["def test_compose_no_duplicates_with_equivalencies():\n    \"\"\"\n    Issue #1438\n    \"\"\"\n    from astropy.constants import G\n    G.decompose(equivalencies=[(u.cm, u.m, lambda x: x, lambda x: x)])\n", "def test_dimensionless_unscaled():\n    assert u.dimensionless_unscaled.is_unity()\n    assert u.dimensionless_unscaled.scale == 1.0\n    assert u.dimensionless_unscaled.bases == []\n    assert u.dimensionless_unscaled.powers == []\n    assert u.dimensionless_unscaled.to_string() == ''\n    assert u.dimensionless_unscaled.to_string('latex') == ''\n    assert u.dimensionless_unscaled.physical_type == 'dimensionless'\n    assert u.dimensionless_unscaled._get_physical_type_id() == ()\n    assert u.dimensionless_unscaled._compose() == [u.dimensionless_unscaled]\n", "def test_dimensionless_unit_compose():\n    \"\"\"\n    Test that dimensionless units can be composed and decomposed.\n\n    Regression test for https://github.com/astropy/astropy/issues/4451\n    \"\"\"\n    dimensionless_unit = u.dimensionless_unscaled\n    composed = dimensionless_unit.compose()\n    assert len(composed) == 1\n    assert composed[0] == dimensionless_unit\n    assert composed[0].decompose() == dimensionless_unit\n", "def test_unit_context_stack():\n    \"\"\"Test that unit registries are correctly stacked and popped.\"\"\"\n    registry = u.get_current_unit_registry()\n\n    with u.set_enabled_units([u.pc]):\n        assert len(u.get_current_unit_registry().all_units) > len(registry.all_units)\n\n    assert len(u.get_current_unit_registry().all_units) == len(registry.all_units)\n\n    with u.add_enabled_units([u.pc]):\n        assert len(u.get_current_unit_registry().all_units) > len(registry.all_units)\n\n    assert len(u.get_current_unit_registry().all_units) == len(registry.all_units)\n\n    with u.set_enabled_equivalencies([(u.m, u.pc, lambda x: x, lambda x: x)]):\n        assert len(u.get_current_unit_registry().equivalencies) > len(registry.equivalencies)\n\n    assert len(u.get_current_unit_registry().equivalencies) == len(registry.equivalencies)\n\n    with u.add_enabled_equivalencies([(u.m, u.pc, lambda x: x, lambda x: x)]):\n        assert len(u.get_current_unit_registry().equivalencies) > len(registry.equivalencies)\n\n    assert len(u.get_current_unit_registry().equivalencies) == len(registry.equivalencies)\n", "def test_invalid_unit_name():\n    \"\"\"\n    Regression test for https://github.com/astropy/astropy/issues/7283\n    \"\"\"\n\n    with pytest.raises(ValueError):\n        u.IrreducibleUnit(\"\")\n    with pytest.raises(ValueError):\n        u.IrreducibleUnit([])\n    with pytest.raises(ValueError):\n        u.IrreducibleUnit([(\"\",)])\n    with pytest.raises(ValueError):\n        u.IrreducibleUnit([(\"\", \"alias\")])\n    with pytest.raises(ValueError):\n        u.IrreducibleUnit([\"alias\", \"\"])\n"], "sample_1096": ["def test_IndexedBase_strides():\n    i, j = symbols('i j', integer=True)\n    A = IndexedBase('A', shape=(3, 3), strides=(2, 1), offset=5)\n    assert A.shape == (3, 3)\n    assert A.strides == (2, 1)\n    assert A.offset == 5\n    assert A[i, j].shape == (3, 3)\n    assert A[i, j].strides == (2, 1)\n    assert A[i, j].offset == 5\n", "def test_IndexedBase_strides():\n    i, j = symbols('i j', integer=True)\n    m, n = symbols('m n', integer=True)\n    a = IndexedBase('a', strides=(m, n))\n    assert a.strides == (m, n)\n    assert a[i, j].offset is S.Zero\n    assert a[i, j].strides == (m, n)\n    a = IndexedBase('a', strides=(m, n), offset=2)\n    assert a.offset == 2\n    assert a[i, j].offset == 2\n    a = IndexedBase('a', strides='C')\n    assert a.strides == 'C'\n    assert a[i, j].strides == 'C'\n    a = IndexedBase('a', strides='F')\n    assert a.strides == 'F'\n    assert a[i, j].strides == 'F'\n", "def test_IndexedBase_with_functions():\n    i, j = symbols('i j', integer=True)\n    f = Function('f')\n    A = IndexedBase(f)\n    assert A[i].base == f\n    assert A[i, j].base == f\n    assert A[i].args == (f, i)\n    assert A[i, j].args == (f, i, j)\n    assert A[i, j].func(*A[i, j].args) == A[i, j]\n    assert A[i].is_Indexed\n    assert not A.is_Indexed\n", "def test_Indexed_derivative():\n    i, j = symbols('i j', integer=True, cls=Idx)\n    A = IndexedBase('A')\n    a, b = symbols('a b')\n    B = IndexedBase('B')\n    C = IndexedBase('C', shape=(10, 10))\n    assert A[i, j].diff(B[i, j]) == 0\n    assert A[i, j].diff(B[i, 1]) == 0\n    assert A[i, j].diff(B[1, j]) == 0\n    assert A[i, j].diff(B[1, 1]) == 0\n    assert A[i, j].diff(a) == 0\n    assert A[i, j].diff(b) == 0\n    assert A[i, j].diff(A[i, j]) == 1\n    assert A[i, j].diff(A[1, j]) == KroneckerDelta(i, 1)\n    assert A[i, j].diff(A[i, 1]) == KroneckerDelta(j, 1)\n    assert A[i, j].diff(A[1, 1]) == KroneckerDelta(i, 1)*KroneckerDelta(j, 1)\n    assert C[i, j].diff(C[i, j]) == 1\n    assert C[i, j].diff(C[1, j]) == KroneckerDelta(i, 1)\n    assert C[i, j].diff(C[i, 1]) == KroneckerDelta(j, 1)\n    assert C[i, j].diff(C[1, 1]) == KroneckerDelta(i, 1)*KroneckerDelta(j, 1)\n", "def test_IndexedBase_derivative():\n    i, j, k, l = symbols('i j k l', cls=Idx)\n    x, y = symbols('x y')\n    A, B = symbols('A B', cls=IndexedBase)\n\n    expr = A[i, j] * B[k, l]\n    assert expr.diff(A[i, j]) == B[k, l]\n    assert expr.diff(A[k, l]) == KroneckerDelta(i, k) * KroneckerDelta(j, l) * B[k, l]\n    assert expr.diff(B[i, j]) == A[i, j]\n    assert expr.diff(B[k, l]) == KroneckerDelta(i, k) * KroneckerDelta(j, l) * A[i, j]\n\n    expr = sin(A[i, j])\n    assert expr.diff(A[i, j]) == cos(A[i, j])\n    assert expr.diff(A[k, l]) == KroneckerDelta(i, k) * KroneckerDelta(j, l) * cos(A[k, l])\n\n    expr = exp(A[i, j])\n    assert expr.diff(A[i, j]) == exp(A[i, j])\n    assert expr.diff(A[k, l]) == KroneckerDelta(i, k) * KroneckerDelta(j, l) * exp(A[k, l])\n\n    expr = x * A[i, j]\n    assert expr.diff(A[i, j]) == x\n    assert expr.diff(x) == A[i, j]\n    assert expr.diff(y) == 0\n\n    expr = x + A[i, j]\n    assert expr.diff(A[i, j]) == 1\n    assert expr.diff(x) == 1\n    assert expr.diff(y) == 0\n"], "sample_523": ["def test_legend_set_draggable_state_twice():\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='shabnams')\n    leg = ax.legend(draggable=True)\n    assert leg.get_draggable()\n    leg.set_draggable(False)\n    assert not leg.get_draggable()\n    leg.set_draggable(True)\n    assert leg.get_draggable()\n", "def test_legend_remove_twice():\n    \"\"\"Test that removing a legend twice doesn't raise an error.\"\"\"\n    fig, ax = plt.subplots()\n    lines = ax.plot(range(10))\n    leg = fig.legend(lines, \"test\")\n    leg.remove()\n    leg.remove()\n    assert fig.legends == []\n\n    leg = ax.legend(\"test\")\n    leg.remove()\n    leg.remove()\n    assert ax.get_legend() is None\n", "def test_draggable_legend_remove():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [1, 2], label='test')\n    leg = ax.legend()\n    leg.draggable(True)\n    assert leg.get_draggable()\n    leg.remove()\n    assert not leg.get_draggable()\n    assert not leg.get_visible()\n", "def test_empty_legend_box():\n    fig, ax = plt.subplots()\n    legend = ax.legend()\n    assert len(legend.legendHandles) == 0\n    assert len(legend.get_texts()) == 0\n    assert legend.get_window_extent(fig.canvas.get_renderer()).width == 0\n", "def test_legend_scatteryoffsets():\n    fig, ax = plt.subplots()\n    ax.scatter([1, 2, 3], [1, 2, 3], label='test')\n    ax.legend(scatteryoffsets=[0.0, 0.5, 1.0])\n    legend = ax.get_legend()\n    assert np.allclose(legend._scatteryoffsets, [0.0, 0.5, 1.0])\n    # test that nans in scatteryoffsets doesn't raise an exception\n    ax.legend(scatteryoffsets=[np.nan, 0.5, 1.0])\n"], "sample_759": ["def test_ordinal_encoder_unsorted_categories(X):\n    enc = OrdinalEncoder(categories=[['b', 'a', 'c']])\n    exp = np.array([[1.], [0.]])\n    assert_array_equal(enc.fit_transform(X), exp)\n    assert list(enc.categories[0]) == list(['b', 'a', 'c'])\n    assert enc.categories_[0].tolist() == ['b', 'a', 'c']\n    assert np.issubdtype(enc.categories_[0].dtype, np.object_)\n\n    # unsorted passed categories still raise for numerical values\n    X = np.array([[1, 2]]).T\n    enc = OrdinalEncoder(categories=[[2, 1, 3]])\n    msg = 'Unsorted categories are not supported'\n    with pytest.raises(ValueError, match=msg):\n        enc.fit_transform(X)\n", "def test_one_hot_encoder_get_feature_names():\n    enc = OneHotEncoder()\n    X = [['abc', 2, 55], ['def', 1, 55], ['abc', 3, 55]]\n    enc.fit(X)\n    feature_names = enc.get_feature_names(input_features=['A', 'B', 'C'])\n    assert_array_equal(feature_names,\n                      ['A_abc', 'A_def', 'B_1', 'B_2', 'B_3', 'C_55'])\n    feature_names = enc.get_feature_names(input_features=['X', 'Y', 'Z'])\n    assert_array_equal(feature_names,\n                      ['X_abc', 'X_def', 'Y_1', 'Y_2', 'Y_3', 'Z_55'])\n", "def test_ordinal_encoder_inverse_handle_unknown():\n    X = [['abc', 2, 55], ['def', 1, 55]]\n    enc = OrdinalEncoder(handle_unknown='use_encoded_value')\n    X_tr = enc.fit_transform(X)\n    exp = np.array(X, dtype=object)\n    assert_array_equal(enc.inverse_transform(X_tr), exp)\n\n    # Test unknown categories\n    X = [['abc', 4, 55], ['def', 1, 55]]\n    enc = OrdinalEncoder(handle_unknown='use_encoded_value')\n    X_tr = enc.fit_transform(X)\n    exp = np.array([['abc', 3, 55], ['def', 1, 55]], dtype=object)\n    assert_array_equal(enc.inverse_transform(X_tr), exp)\n\n    # Test unknown categories with object dtype\n    X = [['abc', 4, 55], ['def', 1, 55]]\n    enc = OrdinalEncoder(handle_unknown='use_encoded_value')\n    X_tr = enc.fit_transform(X)\n    exp = np.array([['abc', 3, 55], ['def', 1, 55]], dtype=object)\n    assert_array_equal(enc.inverse_transform(X_tr), exp)\n", "def test_one_hot_encoder_handle_unknown_with_duplicate_categories():\n    X = np.array([['A', 'B'], ['A', 'B'], ['C', 'D']])\n    enc = OneHotEncoder(handle_unknown='ignore')\n    enc.fit(X)\n    X2 = np.array([['A', 'B'], ['E', 'F']])\n    assert_array_equal(enc.transform(X2).toarray(), [[1., 0., 1., 0.], [0., 0., 0., 0.]])\n\n    X = np.array([['A', 'B'], ['A', 'B'], ['C', 'D']])\n    enc = OneHotEncoder(handle_unknown='error')\n    enc.fit(X)\n    X2 = np.array([['A', 'B'], ['E', 'F']])\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        enc.transform(X2)\n", "def test_ordinal_encoder_with_just_one_category():\n    enc = OrdinalEncoder()\n    X = np.array([[1], [1], [1]])\n    X2 = np.array([[1], [1], [1]])\n    exp = np.array([[0.], [0.], [0.]])\n    assert_array_equal(enc.fit_transform(X), exp)\n    assert_array_equal(enc.fit(X).transform(X2), exp)\n\n    enc = OrdinalEncoder()\n    X = np.array([['a'], ['a'], ['a']], dtype=object)\n    X2 = np.array([['a'], ['a'], ['a']], dtype=object)\n    exp = np.array([[0.], [0.], [0.]])\n    assert_array_equal(enc.fit_transform(X), exp)\n    assert_array_equal(enc.fit(X).transform(X2), exp)\n\n    enc = OrdinalEncoder()\n    X = np.array([[1, 1], [1, 1], [1, 1]])\n    X2 = np.array([[1, 1], [1, 1], [1, 1]])\n    exp = np.array([[0., 0.], [0., 0.], [0., 0.]])\n    assert_array_equal(enc.fit_transform(X), exp)\n    assert_array_equal(enc.fit(X).transform(X2), exp)\n\n    enc = OrdinalEncoder()\n    X = np.array([['a', 'a'], ['a', 'a'], ['a', 'a']], dtype=object)\n    X2 = np.array([['a', 'a'], ['a', 'a'], ['a', 'a']], dtype=object)\n    exp = np.array([[0., 0.], [0., 0.], [0., 0.]])\n    assert_array_equal(enc.fit_transform(X), exp)\n    assert_array_equal(enc.fit(X).transform(X2), exp)\n"], "sample_817": ["def test_variance_threshold_with_all_constant_features():\n    # Test VarianceThreshold with a dataset where all features have zero variance.\n    data = [[1, 2, 3, 4, 5],\n            [1, 2, 3, 4, 5],\n            [1, 2, 3, 4, 5]]\n\n    for X in [data, csr_matrix(data), csc_matrix(data), bsr_matrix(data)]:\n        with pytest.raises(ValueError, match=\"No feature in X meets the variance threshold\"):\n            VarianceThreshold(threshold=1e-5).fit(X)\n", "def test_variance_threshold_with_negative_values():\n    # Test VarianceThreshold with custom variance and negative values.\n    data = [[-1, 2, -3, 4],\n            [0, 2, -3, 5],\n            [1, 1, -2, 0]]\n\n    for X in [data, csr_matrix(data)]:\n        X = VarianceThreshold(threshold=2).fit_transform(X)\n        assert_equal((len(data), 2), X.shape)\n\n    for X in [data, csr_matrix(data)]:\n        X = VarianceThreshold(threshold=10).fit_transform(X)\n        assert_equal((len(data), 0), X.shape)\n\n    for X in [data, csr_matrix(data)]:\n        X = VarianceThreshold(threshold=-1).fit_transform(X)\n        assert_equal((len(data), 4), X.shape)\n", "def test_variance_threshold_with_sparse_and_dense():\n    # Test that VarianceThreshold handles sparse and dense matrices\n    # with the same result.\n    data = [[0, 1, 2, 3, 4],\n            [0, 2, 2, 3, 5],\n            [1, 1, 2, 4, 0]]\n    X_dense = np.array(data)\n    X_csr = csr_matrix(data)\n\n    selector_dense = VarianceThreshold(threshold=0.5).fit(X_dense)\n    selector_csr = VarianceThreshold(threshold=0.5).fit(X_csr)\n\n    assert_array_equal(selector_dense.get_support(indices=True),\n                      selector_csr.get_support(indices=True))\n    assert_array_equal(selector_dense.transform(X_dense),\n                      selector_csr.transform(X_csr).toarray())\n", "def test_variance_threshold_edge_cases():\n    # Test VarianceThreshold with edge case inputs.\n    # Test with a single feature that just meets the threshold.\n    data = [[0.5, 0.5, 0.5], [0.5, 0.5, 0.5], [0.5, 0.5, 0.6]]\n    for X in [data, csr_matrix(data), csc_matrix(data), bsr_matrix(data)]:\n        sel = VarianceThreshold(threshold=0.001).fit(X)\n        assert_array_equal([0, 2], sel.get_support(indices=True))\n\n    # Test with a single feature that just doesn't meet the threshold.\n    data = [[0.5, 0.5, 0.5], [0.5, 0.5, 0.5], [0.5, 0.5, 0.499]]\n    for X in [data, csr_matrix(data), csc_matrix(data), bsr_matrix(data)]:\n        sel = VarianceThreshold(threshold=0.001).fit(X)\n        assert_array_equal([0], sel.get_support(indices=True))\n\n    # Test with all features meeting the threshold.\n    data = [[0.5, 0.5, 0.5], [0.5, 0.6, 0.5], [0.5, 0.5, 0.6]]\n    for X in [data, csr_matrix(data), csc_matrix(data), bsr_matrix(data)]:\n        sel = VarianceThreshold(threshold=0.001).fit(X)\n        assert_array_equal([0, 1, 2], sel.get_support(indices=True))\n\n    # Test with no features meeting the threshold.\n    data = [[0.5, 0.5, 0.5], [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]]\n    for X in [data, csr_matrix(data), csc_matrix(data), bsr_matrix(data)]:\n        msg = \"No feature in X meets the variance threshold 0.00100\"\n        with pytest.raises(ValueError, match=msg):\n            VarianceThreshold(threshold=0.001).fit(X)\n", "def test_variance_threshold_edge_cases():\n    # Test VarianceThreshold with custom variance on edge cases.\n    # Test with a matrix with only one feature\n    X = [[1, 1, 1]]\n    with pytest.raises(ValueError, match=\"No feature in X meets the variance threshold\"):\n        VarianceThreshold(threshold=0.1).fit(X)\n\n    # Test with a matrix with all zero variance features\n    X = [[0, 0], [0, 0]]\n    with pytest.raises(ValueError, match=\"No feature in X meets the variance threshold\"):\n        VarianceThreshold(threshold=0.1).fit(X)\n\n    # Test with a matrix with a single sample and non-zero variance\n    X = [[1, 2, 3]]\n    X = VarianceThreshold(threshold=0).fit_transform(X)\n    assert_equal((1, 3), X.shape)\n\n    # Test with a matrix with a single feature and non-zero variance\n    X = [[1], [2], [3]]\n    X = VarianceThreshold(threshold=0).fit_transform(X)\n    assert_equal((3, 1), X.shape)\n"], "sample_1032": ["def test_identity_function():\n    from sympy.abc import x\n    f = IdentityFunction()\n    assert f(x) == x\n    assert f(1) == 1\n    assert f(S.Infinity) == S.Infinity\n    assert f(S.NegativeInfinity) == S.NegativeInfinity\n    assert f(S.ComplexInfinity) == S.ComplexInfinity\n    assert f(S.NaN) == S.NaN\n    assert f(S.One) == S.One\n    assert f(S.Zero) == S.Zero\n    assert f(S.Half) == S.Half\n    assert f(S.Pi) == S.Pi\n    assert f(S.Exp1) == S.Exp1\n    assert f(S.ImaginaryUnit) == S.ImaginaryUnit\n    assert f(S.EulerGamma) == S.EulerGamma\n    assert f(S.Catalan) == S.Catalan\n    assert f(S.GoldenRatio) == S.GoldenRatio\n\n    assert f(f(x)) == x\n    assert f(f(1)) == 1\n    assert f(f(S.Infinity)) == S.Infinity\n    assert f(f(S.NegativeInfinity)) == S.NegativeInfinity\n    assert f(f(S.ComplexInfinity)) == S.ComplexInfinity\n    assert f(f(S.NaN)) == S.NaN\n    assert f(f(S.One)) == S.One\n    assert f(f(S.Zero)) == S.Zero\n    assert f(f(S.Half)) == S.Half\n    assert f(f(S.Pi)) == S.Pi\n    assert f(f(S.Exp1)) == S.Exp1\n    assert f(f(S.ImaginaryUnit)) == S.ImaginaryUnit\n    assert f(f(S.EulerGamma)) == S.EulerGamma\n    assert f(f(S.Catalan)) == S.Catalan\n    assert f(f(S.GoldenRatio)) == S.GoldenRatio\n", "def test_power():\n    from sympy.abc import x, y\n    assert root(x**2, 2) == Abs(x)\n    assert root(x**3, 3) == Abs(x)\n    assert root(x**3, -3) == 1/Abs(x)\n    assert root(x**y, 2) == Abs(x)**(y/2)\n    assert root(x**y, -2) == 1/Abs(x)**(y/2)\n    assert root(x**y, 3) == Abs(x)**(y/3)\n    assert root(x**y, -3) == 1/Abs(x)**(y/3)\n    assert real_root(x**2, 2) == x\n    assert real_root(x**3, 3) == x\n    assert real_root(x**3, -3) == 1/x\n    assert real_root(x**y, 2) == x**(y/2)\n    assert real_root(x**y, -2) == 1/x**(y/2)\n    assert real_root(x**y, 3) == x**(y/3)\n    assert real_root(x**y, -3) == 1/x**(y/3)\n", "def test_MinMaxBase():\n    from sympy.abc import x, y\n    p = Symbol('p', positive=True)\n    n = Symbol('n', negative=True)\n\n    assert MinMaxBase._is_connected(x, y) is None\n    assert MinMaxBase._is_connected(x, x) is True\n    assert MinMaxBase._is_connected(x, 1) is True\n    assert MinMaxBase._is_connected(1, x) is True\n    assert MinMaxBase._is_connected(x, -oo) is True\n    assert MinMaxBase._is_connected(-oo, x) is True\n    assert MinMaxBase._is_connected(x, oo) is True\n    assert MinMaxBase._is_connected(oo, x) is True\n    assert MinMaxBase._is_connected(p, n) is True\n    assert MinMaxBase._is_connected(n, p) is True\n\n    assert MinMaxBase._is_connected(oo, -oo) is True\n    assert MinMaxBase._is_connected(-oo, oo) is True\n    assert MinMaxBase._is_connected(p, oo) is True\n    assert MinMaxBase._is_connected(oo, p) is True\n    assert MinMaxBase._is_connected(n, -oo) is True\n    assert MinMaxBase._is_connected(-oo, n) is True\n", "def test_rewrite_MaxMin_as_Abs():\n    from sympy.abc import x, y, z, a, b\n    assert (Max(x, y) - Min(x, y)).rewrite(Abs) == Abs(x - y)\n    assert (Max(x, y) + Min(x, y)).rewrite(Abs) == x + y\n    assert (Max(x, Max(y, z)) - Min(x, Min(y, z))).rewrite(Abs) == Abs(x - z)\n    assert (Max(x, Max(y, z)) + Min(x, Min(y, z))).rewrite(Abs) == x + z\n    assert (Max(x, y) - Max(x, z)).rewrite(Abs) == Abs(y - z)\n    assert (Min(x, y) - Min(x, z)).rewrite(Abs) == Abs(y - z)\n    assert (Max(x, y) + Max(x, z)).rewrite(Abs) == x + Max(y, z)\n    assert (Min(x, y) + Min(x, z)).rewrite(Abs) == x + Min(y, z)\n    assert (Max(x, y) - Min(y, z)).rewrite(Abs) == Abs(x - z)\n    assert (Max(x, y) + Min(y, z)).rewrite(Abs) == Abs(x + z)\n    assert (Max(x, y) - Max(y, z)).rewrite(Abs) == Abs(x - z)\n    assert (Min(x, y) - Min(y, z)).rewrite(Abs) == Abs(x - z)\n    assert (Max(a, b) - Min(a, b)).rewrite(Abs) == Abs(a - b)\n    assert (Max(a, b) + Min(a, b)).rewrite(Abs) == a + b\n", "def test_issue_14400():\n    from sympy.abc import x, y, z\n    assert sqrt(x**2).diff(x) == x/sqrt(x**2)\n    assert cbrt(x**3).diff(x) == 1/(3*x**(2/3))\n    assert root(x**3, 3).diff(x) == 1/(3*x**(2/3))\n    assert real_root(x**3, 3).diff(x) == 1/(3*x**(2/3))\n\n    assert sqrt(x**2).diff(x).subs(x, -1) == -1\n    assert cbrt(x**3).diff(x).subs(x, -1) == -1/3\n    assert root(x**3, 3).diff(x).subs(x, -1) == 1/3\n    assert real_root(x**3, 3).diff(x).subs(x, -1) == -1/3\n\n    assert sqrt(x**2).diff(x).subs(x, 0) == zoo\n    assert cbrt(x**3).diff(x).subs(x, 0) == zoo\n    assert root(x**3, 3).diff(x).subs(x, 0) == zoo\n    assert real_root(x**3, 3).diff(x).subs(x, 0) == zoo\n\n    # Check the derivative for the case where the argument is not a power\n    assert sqrt(x).diff(x) == 1/(2*sqrt(x))\n    assert cbrt(x).diff(x) == 1/(3*x**(2/3))\n    assert root(x, 3).diff(x) == 1/(3*x**(2/3))\n    assert real_root(x, 3).diff(x) == 1/(3*x**(2/3))\n\n    # Check the derivative for the case where the base is not x\n    assert sqrt(y**2).diff(x) == 0\n    assert cbrt(z**3).diff(x) == 0\n    assert root(y**3, 3).diff(x) == 0\n    assert real_root(z**3, 3).diff(x) == 0\n"], "sample_338": ["def test_mti_inheritance_with_proxy_models(self):\n    Animal = ModelState('app', 'Animal', [\n        (\"id\", models.AutoField(primary_key=True)),\n    ])\n    Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n    DogProxy = ModelState('app', 'DogProxy', [], bases=('app.Dog',), options={'proxy': True})\n    changes = self.get_changes([Animal, Dog], [Animal, DogProxy])\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['CreateModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name=\"DogProxy\")\n", "def test_renamed_model_with_order_wrt_change(self):\n    \"\"\"\n    Tests autodetection of renamed models while simultaneously changing\n    order_with_respect_to.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_with_book_order_wrt, self.book],\n        [self.author_renamed_with_book, self.book],\n        MigrationQuestioner({\"ask_rename_model\": True}),\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"RenameModel\", \"AlterOrderWithRespectTo\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name=\"Author\", new_name=\"Writer\")\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, name=\"Writer\", order_with_respect_to=None)\n", "def test_alter_model_options_remove_default_related_name(self):\n    \"\"\"Removing a model's default_related_name should make a change.\"\"\"\n    model_state_with_default_related_name = ModelState(\n        'app', 'model', [\n            ('id', models.AutoField(primary_key=True)),\n        ],\n        options={'default_related_name': 'related_name'},\n    )\n    model_state_without_default_related_name = ModelState('app', 'model', [\n        ('id', models.AutoField(primary_key=True)),\n    ])\n    changes = self.get_changes([model_state_with_default_related_name], [model_state_without_default_related_name])\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['AlterModelOptions'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='model', options={})\n", "    def test_add_field_with_unique_together(self):\n        \"\"\"Tests adding a field to a model with an existing unique_together.\"\"\"\n        before = [\n            ModelState(\"testapp\", \"Author\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n            ], options={\"unique_together\": {(\"name\",)}}),\n        ]\n        after = [\n            ModelState(\"testapp\", \"Author\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"age\", models.IntegerField()),\n            ], options={\"unique_together\": {(\"name\", \"age\")}}),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\", \"AlterUniqueTogether\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"age\")\n        self.assertOperationAttributes(changes, 'testapp', 0, 1, name=\"author\", unique_together={('name', 'age')})\n", "def test_custom_default(self):\n    \"\"\"\n    #24362 - Tests autodetection of new fields with custom default\n    \"\"\"\n    class CustomDefault:\n            return \"Test\"\n\n    custom_default = CustomDefault()\n\n    before = ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n    ])\n    after = ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=200, default=custom_default)),\n    ])\n\n    changes = self.get_changes([before], [after])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"name\")\n    self.assertIsInstance(changes['testapp'][0].operations[0].field.default, CustomDefault)\n"], "sample_421": ["    def test_resolve_combined_type(self):\n        self.assertEqual(\n            _resolve_combined_type(Combinable.ADD, IntegerField, IntegerField),\n            IntegerField,\n        )\n        self.assertEqual(\n            _resolve_combined_type(Combinable.ADD, IntegerField, DecimalField),\n            DecimalField,\n        )\n        self.assertEqual(\n            _resolve_combined_type(Combinable.ADD, NoneType, IntegerField),\n            IntegerField,\n        )\n        self.assertIsNone(\n            _resolve_combined_type(Combinable.ADD, IntegerField, TextField)\n        )\n", "def test_nested_combined_expression(self):\n    self.assertQuerysetEqual(\n        CaseTestModel.objects.annotate(\n            test=(Case(\n                When(integer=1, then=Value(2)),\n                When(integer=2, then=Value(1)),\n                default=Value(3),\n            ) + 1) + 1,\n        ).order_by(\"pk\"),\n        [(1, 4), (2, 3), (3, 5), (2, 3), (3, 5), (3, 5), (4, 5)],\n        transform=attrgetter(\"integer\", \"test\"),\n    )\n", "    def test_combined_expression_in_case(self):\n        self.assertQuerysetEqual(\n            CaseTestModel.objects.annotate(\n                test=Case(\n                    When(integer=F(\"integer2\"), then=F(\"integer\") + 1),\n                    When(integer=2, then=F(\"integer\") + 3),\n                    default=F(\"integer\"),\n                )\n            ).order_by(\"pk\"),\n            [(1, 2), (2, 4), (3, 5), (2, 3), (3, 5), (3, 4), (4, 4)],\n            transform=attrgetter(\"integer\", \"test\"),\n        )\n", "    def test_window_function(self):\n        self.assertQuerysetEqual(\n            CaseTestModel.objects.annotate(\n                test=Case(\n                    When(integer=1, then=Window(Sum('integer'), partition_by=F('string'))),\n                    When(integer=2, then=Window(Sum('integer'), partition_by=F('string'))),\n                    default=Window(Sum('integer'), partition_by=F('string')),\n                )\n            ).order_by(\"pk\"),\n            [(1, 1), (2, 2), (3, 9), (2, 2), (3, 9), (3, 9), (4, 4)],\n            transform=attrgetter(\"integer\", \"test\"),\n        )\n", "    def test_combined_expression_with_duration(self):\n        self.assertQuerysetEqual(\n            CaseTestModel.objects.annotate(\n                test=F(\"duration\") + F(\"duration\")\n            ).order_by(\"pk\"),\n            [(None, None), (None, None), (None, None), (None, None), (None, None), (None, None), (None, None)],\n            transform=lambda o: (o.duration, o.test),\n        )\n"], "sample_794": ["def test_ridge_regression_empty_data():\n    X = np.array([])\n    y = np.array([])\n    alpha = 1.0\n    assert_raises(ValueError, ridge_regression, X, y, alpha=alpha, solver='svd')\n", "def test_ridge_regression_with_sparse_input_and_individual_penalties():\n    # Test ridge regression with sparse input and individual penalties\n\n    rng = np.random.RandomState(42)\n\n    n_samples, n_features, n_targets = 20, 10, 5\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples, n_targets)\n\n    X_sparse = sp.csr_matrix(X)\n\n    penalties = np.arange(n_targets)\n\n    for solver in ['cholesky', 'sag', 'saga']:\n        coefs_dense = np.array([\n            Ridge(alpha=alpha, solver=solver).fit(X, target).coef_\n            for alpha, target in zip(penalties, y.T)])\n\n        coefs_sparse = np.array([\n            Ridge(alpha=alpha, solver=solver).fit(X_sparse, target).coef_\n            for alpha, target in zip(penalties, y.T)])\n\n        assert_array_almost_equal(coefs_dense, coefs_sparse)\n", "def test_ridge_regression_random_state(solver):\n    rng = np.random.RandomState(0)\n    alpha = 1.0\n\n    n_samples, n_features = 6, 5\n    X_32 = rng.randn(n_samples, n_features).astype(np.float32)\n    y_32 = rng.randn(n_samples).astype(np.float32)\n    X_64 = X_32.astype(np.float64)\n    y_64 = y_32.astype(np.float64)\n\n    # Check type consistency 32bits\n    ridge_32 = Ridge(alpha=alpha, solver=solver, random_state=42, max_iter=500, tol=1e-10,)\n    ridge_32.fit(X_32, y_32)\n    coef_32 = ridge_32.coef_\n\n    # Check type consistency 64 bits\n    ridge_64 = Ridge(alpha=alpha, solver=solver, random_state=42, max_iter=500, tol=1e-10,)\n    ridge_64.fit(X_64, y_64)\n    coef_64 = ridge_64.coef_\n\n    # Do the actual checks at once for easier debug\n    assert coef_32.dtype == X_32.dtype\n    assert coef_64.dtype == X_64.dtype\n    assert ridge_32.predict(X_32).dtype == X_32.dtype\n    assert ridge_64.predict(X_64).dtype == X_64.dtype\n    assert_allclose(ridge_32.coef_, ridge_64.coef_, rtol=1e-4)\n\n    # Check if random_state produces identical results\n    ridge_32_2 = Ridge(alpha=alpha, solver=solver, random_state=42, max_iter=500, tol=1e-10,)\n    ridge_32_2.fit(X_32, y_32)\n    assert_allclose(ridge_32.coef_, ridge_32_2.coef_, rtol=1e-10)\n", "def test_ridge_regression_raise_on_non_numeric_input():\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array(['a', 'b'])\n    alpha = 1.0\n\n        ridge_regression(X, y, alpha)\n\n    assert_raises_regex(ValueError,\n                        \"y should be an array-like of shape (n_samples,) or \"\n                        \"\\(n_samples, n_targets\\), got (2,)\",\n                        fit_ridge_regression)\n", "def test_ridge_regression_intercept_shape(solver):\n    X = np.random.rand(1000, 3)\n    y = np.random.rand(1000)\n    intercept = 5.0\n    y += intercept\n\n    # No intercept\n    ridge = Ridge(alpha=1.0, fit_intercept=False, solver=solver)\n    ridge.fit(X, y)\n    assert_equal(ridge.intercept_, 0.0)\n    assert_equal(ridge.coef_.shape, (X.shape[1],))\n\n    # With intercept\n    ridge = Ridge(alpha=1.0, fit_intercept=True, solver=solver)\n    ridge.fit(X, y)\n    assert_array_almost_equal(ridge.coef_, np.zeros(X.shape[1]))\n    assert_almost_equal(ridge.intercept_, intercept)\n\n    # Multi target\n    y = np.c_[y, y + 1.0]\n    ridge = Ridge(alpha=1.0, fit_intercept=True, solver=solver)\n    ridge.fit(X, y)\n    assert_array_almost_equal(ridge.coef_, np.zeros((2, X.shape[1])))\n    assert_array_almost_equal(ridge.intercept_, np.array([intercept, intercept + 1.0]))\n"], "sample_539": ["def test_polygon_selector_add_state(ax):\n    tool = widgets.PolygonSelector(ax, onselect=noop)\n    with pytest.raises(ValueError):\n        tool.add_state('unsupported_state')\n    tool.add_state('move_vertex')\n    tool.add_state('move_all')\n    tool.add_state('clear')\n", "def test_polygon_selector_clear(ax, draw_bounding_box):\n    tool = widgets.PolygonSelector(ax, onselect=noop,\n                                   draw_bounding_box=draw_bounding_box)\n    verts = [(50, 50), (150, 50), (50, 150)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    assert tool._selection_completed\n\n    tool.clear()\n\n    assert not tool._selection_completed\n    assert not tool._visible\n", "def test_polygon_selector_set_verts(ax, draw_bounding_box):\n    tool = widgets.PolygonSelector(ax, onselect=noop,\n                                   draw_bounding_box=draw_bounding_box)\n\n    verts = [(50, 50), (150, 50), (50, 150), (150, 150), (50, 50)]\n    tool.verts = verts\n\n    assert tool.verts == verts\n\n    verts = [(100, 100), (120, 100), (110, 120), (100, 100)]\n    tool.verts = verts\n\n    assert tool.verts == verts\n", "def test_SubplotTool(ax):\n    targetfig, toolfig = plt.subplots(2)\n    subplot_tool = widgets.SubplotTool(targetfig, toolfig)\n    assert subplot_tool.targetfig == targetfig\n    assert subplot_tool.figure == toolfig\n\n    targetfig.subplots_adjust(left=0.5, bottom=0.6, right=0.7, top=0.8,\n                              wspace=0.9, hspace=0.10)\n    slider_values = [slider.val for slider in subplot_tool._sliders]\n    assert slider_values == [0.5, 0.6, 0.7, 0.8, 0.9, 0.10]\n", "def test_subplots_tool(ax):\n    tool = widgets.SubplotTool(ax.figure, ax.figure)\n    assert tool.sliderleft.val == 0.1\n    assert tool.sliderbottom.val == 0.1\n    assert tool.sliderright.val == 0.9\n    assert tool.slidertop.val == 0.9\n    assert tool.sliderwspace.val == 0.2\n    assert tool.sliderhspace.val == 0.2\n\n    # Trigger 'on_slider_changed'\n    do_event(tool.sliderleft, 'on_changed', 0.2)\n\n    assert tool.sliderleft.val == 0.2\n    assert ax.figure.subplotpars.left == 0.2\n\n    # Trigger 'on_reset'\n    do_event(tool.buttonreset, 'on_clicked', mock.Mock())\n\n    assert tool.sliderleft.val == 0.1\n    assert ax.figure.subplotpars.left == 0.1\n"], "sample_559": ["def test_axes_locatable_position_with_negative_size():\n    fig, ax = plt.subplots()\n    divider = make_axes_locatable(ax)\n    with mpl.rc_context({\"figure.subplot.wspace\": 0.02}):\n        cax = divider.append_axes('right', size='-5%')\n    fig.canvas.draw()\n    assert cax.get_position(original=False).width < 0\n", "def test_anchored_artists_3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.plot([0, 1], [0, 1], [0, 1])\n    ax.set_xlim(-1, 2)\n    ax.set_ylim(-1, 2)\n    ax.set_zlim(-1, 2)\n\n    ada = AnchoredDrawingArea(40, 20, 0, 0, loc='upper right', pad=0.,\n                              frameon=False)\n    p1 = Circle((10, 10), 10)\n    ada.drawing_area.add_artist(p1)\n    p2 = Circle((30, 10), 5, fc=\"r\")\n    ada.drawing_area.add_artist(p2)\n    ax.add_artist(ada)\n\n    box = AnchoredAuxTransformBox(ax.transData, loc='upper left')\n    el = Ellipse((0, 0), width=0.1, height=0.4, angle=30, color='cyan')\n    box.drawing_area.add_artist(el)\n    ax.add_artist(box)\n\n    asb = AnchoredSizeBar(ax.transData, 0.2, r\"0.2 units\", loc='lower right',\n                          pad=0.3, borderpad=0.4, sep=4, fill_bar=True,\n                          frameon=False, label_top=True, prop={'size': 20},\n                          size_vertical=0.05, color='green')\n    ax.add_artist(asb)\n", "def test_anchored_direction_arrows_inset():\n    fig, ax = plt.subplots()\n    ax.imshow(np.ones((10, 10)))\n\n    inset_ax = inset_axes(ax, width=\"30%\", height=\"30%\",\n                         loc=4, borderpad=1)\n\n    simple_arrow = AnchoredDirectionArrows(\n            inset_ax.transAxes, 'X', 'Y')\n    inset_ax.add_artist(simple_arrow)\n\n    direction_arrows = AnchoredDirectionArrows(\n            inset_ax.transAxes, 'A', 'B', loc='upper right', color='red',\n            aspect_ratio=-0.5, pad=0.6, borderpad=2, frameon=True, alpha=0.7,\n            sep_x=-0.06, sep_y=-0.08, back_length=0.1, head_width=9,\n            head_length=10, tail_width=5)\n    inset_ax.add_artist(direction_arrows)\n", "def test_secondary_axis_vertical():\n    fig, ax = plt.subplots()\n    ax2 = ax.secondary_xaxis('top', functions=(lambda x: x**2, lambda x: x**0.5))\n    ax2.set_xlabel('x^2')\n    assert ax2.get_xaxis().get_label().get_text() == 'x^2'\n    ax.plot([1, 2, 3], [1, 2, 3])\n    ax2.plot([1, 2, 3], [1, 4, 9])\n    assert_array_equal(ax.get_xticks(), [1, 2, 3])\n    assert_array_equal(ax2.get_xticks(), [1, 4, 9])\n", "def test_gridspec():\n    fig = plt.figure()\n    ax1 = fig.add_subplot(3, 3, 1)\n    ax2 = fig.add_subplot(3, 3, (2, 3))\n    ax3 = fig.add_subplot(3, 3, 5)\n    ax4 = fig.add_subplot(3, 3, (6, 8))\n    ax5 = fig.add_subplot(3, 3, (7, 9))\n    grid = mpl.gridspec.GridSpec(3, 3)\n    grid[1, 1].set_visible(False)\n    ax6 = fig.add_subplot(grid[1, 1])\n    fig.canvas.draw()\n    assert ax6.get_position().x0 == pytest.approx(0.35)\n    assert ax6.get_position().x1 == pytest.approx(0.65)\n    assert ax6.get_position().y0 == pytest.approx(0.35)\n    assert ax6.get_position().y1 == pytest.approx(0.65)\n    assert ax6.get_position().width == pytest.approx(0.3)\n    assert ax6.get_position().height == pytest.approx(0.3)\n"], "sample_402": ["    def test_prepend_www_with_port(self):\n        request = self.rf.get(\"/path/\", HTTP_HOST=\"example.com:8080\")\n        r = CommonMiddleware(get_response_empty).process_request(request)\n        self.assertEqual(r.status_code, 301)\n        self.assertEqual(r.url, \"http://www.example.com:8080/path/\")\n", "    def test_prepend_www_append_slash_redirect_querystring(self):\n        \"\"\"\n        PREPEND_WWW and APPEND_SLASH should preserve querystrings when redirecting.\n        \"\"\"\n        request = self.rf.get(\"/slash?test=1\")\n        resp = CommonMiddleware(get_response_404)(request)\n        self.assertEqual(resp.url, \"http://www.testserver/slash/?test=1\")\n", "def test_prepend_www_append_slash_with_port(self):\n    \"\"\"\n    PREPEND_WWW should work when the URL has a port specified.\n    \"\"\"\n    request = self.rf.get(\"http://testserver:8000/slash\")\n    r = CommonMiddleware(get_response_empty).process_request(request)\n    self.assertEqual(r.status_code, 301)\n    self.assertEqual(r.url, \"http://www.testserver:8000/slash/\")\n\n    request = self.rf.get(\"http://testserver:8000/slash\")\n    r = CommonMiddleware(get_response_404)(request)\n    self.assertEqual(r.status_code, 301)\n    self.assertEqual(r.url, \"http://www.testserver:8000/slash/\")\n", "def test_prepend_www_append_slash_with_non_ascii_chars(self):\n    request = self.rf.get(\"/path_with_non_ascii_chars\")\n    request.META[\"HTTP_HOST\"] = \"testserver\"\n    request.path = \"/path_with_non_ascii_chars\"\n    request.path_info = \"/path_with_non_ascii_chars\"\n    r = CommonMiddleware(get_response_empty).process_request(request)\n    self.assertEqual(r.status_code, 301)\n    self.assertEqual(r.url, \"http://www.testserver/path_with_non_ascii_chars/\")\n\n    request = self.rf.get(\"/path_with_non_ascii_chars/\")\n    request.META[\"HTTP_HOST\"] = \"testserver\"\n    request.path = \"/path_with_non_ascii_chars/\"\n    request.path_info = \"/path_with_non_ascii_chars/\"\n    r = CommonMiddleware(get_response_empty).process_request(request)\n    self.assertIsNone(r)\n", "def test_prepend_www_append_slash_no_content_length(self):\n    \"\"\"\n    PREPEND_WWW and APPEND_SLASH should not result in a redirect loop when the \n    response does not have a Content-Length header.\n    \"\"\"\n    request = self.rf.get(\"/path\")\n    response = HttpResponse()\n    r = CommonMiddleware(get_response_empty).process_response(request, response)\n    self.assertEqual(r.status_code, 301)\n    self.assertEqual(r.url, \"http://www.testserver/path/\")\n    self.assertNotIn(\"Content-Length\", r)\n"], "sample_857": ["def test_feature_importances_on_constant_feature(name):\n    TreeEstimator = ALL_TREES[name]\n    X = np.zeros((10, 20))\n    X[:, 0] = 1\n    y = np.random.randint(0, 2, (10, ))\n\n    est = TreeEstimator(random_state=0)\n    est.fit(X, y)\n    assert_array_almost_equal(est.feature_importances_, np.array([1] + [0]*19))\n", "def test_prune_tree_raises_invalid_ccp_alpha(Cls, ccp_alpha):\n    # Test pruning with invalid ccp_alpha values\n    X = iris.data\n    y = iris.target\n\n    clf = Cls(ccp_alpha=ccp_alpha)\n    with pytest.raises(ValueError):\n        clf.fit(X, y)\n", "def test_depth_first_best_first_equivalence():\n    # Test that DepthFirstTreeBuilder and BestFirstTreeBuilder return the same\n    # trees.\n    X, y = datasets.make_classification(n_samples=10000, random_state=42)\n\n    for name, TreeEstimator in ALL_TREES.items():\n        est_depth_first = TreeEstimator(random_state=0).fit(X, y)\n        est_best_first = TreeEstimator(max_leaf_nodes=X.shape[0],\n                                       random_state=0).fit(X, y)\n\n        assert_tree_equal(est_depth_first.tree_, est_best_first.tree_,\n                          \"{0}: inequal trees\".format(name))\n", "def test_ccp_pruning_path_on_constant_input():\n    # Check that cost complexity pruning path raises an error when\n    # used on constant input data.\n\n    X = np.zeros((10, 2))\n    y = np.zeros(10)\n\n    for tree_cls in [DecisionTreeClassifier, DecisionTreeRegressor]:\n        clf = tree_cls(random_state=0)\n        with pytest.raises(ValueError):\n            clf.cost_complexity_pruning_path(X, y)\n", "def test_feature_importances_on_anisotropic_data():\n    # Ensure feature importances are not affected by feature scales.\n    X, y = datasets.make_classification(n_samples=1000, n_features=2,\n                                        n_informative=2, n_redundant=0,\n                                        n_repeated=0, shuffle=False,\n                                        random_state=0)\n\n    X[:, 1] *= 1e6  # Make one feature much more prominent\n\n    clf = DecisionTreeClassifier(random_state=0)\n    clf.fit(X, y)\n\n    # Check that both features have similar importance\n    importance = clf.feature_importances_\n    assert_almost_equal(importance[0], importance[1], decimal=2)\n"], "sample_474": ["def test_length_transform(self):\n    authors = Author.objects.annotate(name_length=Length(\"name\"))\n    self.assertEqual(authors.get(name=\"John Smith\").name_length, 9)\n    self.assertEqual(authors.get(name=\"\u00c9lena Jordan\").name_length, 12)\n    self.assertEqual(authors.get(name=\"Rhonda\").name_length, 6)\n", "    def setUpTestData(cls):\n        cls.john = Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        cls.elena = Author.objects.create(name=\"\u00c9lena Jordan\", alias=\"elena\")\n        cls.rhonda = Author.objects.create(name=\"Rhonda\")\n", "    def setUpTestData(cls):\n        cls.john = Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        cls.elena = Author.objects.create(name=\"\u00c9lena Jordan\", alias=\"elena\")\n        cls.rhonda = Author.objects.create(name=\"Rhonda\")\n", "    def setUpTestData(cls):\n        cls.author1 = Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        cls.author2 = Author.objects.create(name=\"\u00c9lena Jordan\", alias=\"elena\")\n        cls.author3 = Author.objects.create(name=\"Rhonda\")\n", "    def setUpTestData(cls):\n        cls.john = Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        cls.elena = Author.objects.create(name=\"\u00c9lena Jordan\", alias=\"elena\")\n        cls.rhonda = Author.objects.create(name=\"Rhonda\")\n"], "sample_506": ["def test_spines_edgecolor():\n    fig, ax = plt.subplots()\n    ax.spines.left.set_color('red')\n    ax.spines.right.set_color('green')\n    ax.spines.top.set_color('blue')\n    ax.spines.bottom.set_color('yellow')\n    ax.set_xlim([0, 1])\n    ax.set_ylim([0, 1])\n", "def test_spine_clear():\n    fig, ax = plt.subplots()\n    spine = ax.spines.left\n    spine.set_color('red')\n    spine.clear()\n    assert spine.get_color() == (0.0, 0.0, 0.0, 1.0)  # default edge color\n\n    spine = ax.spines.left\n    spine.set_bounds((0, 1))\n    spine.clear()\n    assert spine.get_bounds() is None\n", "def test_spine_get_bounds():\n    fig, ax = plt.subplots()\n    spine = ax.spines['left']\n    assert spine.get_bounds() is None\n    spine.set_bounds((0, 1))\n    assert spine.get_bounds() == (0, 1)\n    spine.set_bounds(high=2)\n    assert spine.get_bounds() == (0, 2)\n    spine.set_bounds(low=3)\n    assert spine.get_bounds() == (3, 2)\n    spine.set_bounds(None, None)\n    assert spine.get_bounds() == (3, 2)\n    spine.set_bounds((4, 5))\n    assert spine.get_bounds() == (4, 5)\n\n    with pytest.raises(ValueError, match='set_bounds() method incompatible'):\n        ax.spines['circle'].set_bounds((0, 1))\n", "def test_spine_set_bounds(fig_test, fig_ref):\n    plt.style.use(\"default\")\n\n    ax_test = fig_test.add_subplot()\n    ax_test.set_xlim([-1, 1])\n    ax_test.set_ylim([-1, 1])\n    ax_test.spines.left.set_bounds(-0.5, 0.5)\n    ax_test.spines.right.set_bounds(-0.5, 0.5)\n    ax_test.spines.top.set_bounds(-0.5, 0.5)\n    ax_test.spines.bottom.set_bounds(-0.5, 0.5)\n\n    ax_ref = fig_ref.add_subplot()\n    ax_ref.set_xlim([-0.5, 0.5])\n    ax_ref.set_ylim([-0.5, 0.5])\n    ax_ref.spines.left.set_visible(False)\n    ax_ref.spines.right.set_visible(False)\n    ax_ref.spines.top.set_visible(False)\n    ax_ref.spines.bottom.set_visible(False)\n", "def test_spine_set_bounds():\n    fig, ax = plt.subplots()\n    ax.spines.left.set_bounds(0, 1)\n    ax.spines.right.set_bounds(-1, 2)\n    ax.spines.top.set_bounds(-2, 3)\n    ax.spines.bottom.set_bounds(4, 5)\n\n    assert ax.spines.left.get_bounds() == (0, 1)\n    assert ax.spines.right.get_bounds() == (-1, 2)\n    assert ax.spines.top.get_bounds() == (-2, 3)\n    assert ax.spines.bottom.get_bounds() == (4, 5)\n\n    ax.spines.left.set_bounds((0.5, 1.5))\n    ax.spines.right.set_bounds(low=-0.5, high=2.5)\n    ax.spines.top.set_bounds(high=3.5)\n    ax.spines.bottom.set_bounds(low=4.5)\n\n    assert ax.spines.left.get_bounds() == (0.5, 1.5)\n    assert ax.spines.right.get_bounds() == (-0.5, 2.5)\n    assert ax.spines.top.get_bounds() == (-2, 3.5)\n    assert ax.spines.bottom.get_bounds() == (4.5, 5)\n\n    with pytest.raises(ValueError, match='set_bounds() method incompatible'):\n        ax.spines.bottom.set_patch_circle((0, 0), 1)\n        ax.spines.bottom.set_bounds(0, 1)\n"], "sample_605": ["def test_groupby_map_with_uneven_group_sizes():\n        if group.sizes[\"x\"] == 1:\n            result = group.isel(x=[0, 0])\n        else:\n            result = group.isel(x=[0])\n        return result\n\n    array = xr.DataArray([1, 2, 3, 4, 5, 6], [(\"x\", [1, 2, 2, 1, 1, 1])])\n    expected = xr.DataArray([1, 2, 2, 3, 4, 5], [(\"x\", [1, 1, 2, 1, 1, 1])])\n    actual = array.groupby(\"x\").map(func)\n    assert_identical(expected, actual)\n", "def test_da_groupby_datetime_index_with_grouper():\n    times = pd.date_range(\"2000-01-01\", periods=4)\n    foo = xr.DataArray([1, 2, 3, 4], coords=dict(time=times), dims=\"time\")\n    grouper = pd.Grouper(freq=\"2D\")\n    g = foo.groupby(foo.time.dt.month).groupby(time=grouper)\n    expected = xr.DataArray(\n        [3, 7],\n        coords=dict(time=pd.date_range(\"2000-01-02\", periods=2), month=[1, 2]),\n        dims=\"time\",\n    )\n    actual = g.sum(dim=\"time\")\n    assert_identical(expected, actual)\n", "def test_da_groupby_quantile_with_interpolation():\n    # Test quantile with interpolation\n    array = xr.DataArray(\n        data=[1, 2, 3, 4, 5, 6], coords={\"x\": [1, 1, 1, 2, 2, 2]}, dims=\"x\"\n    )\n\n    # Test with a single quantile\n    expected = xr.DataArray(\n        data=[2, 5], coords={\"x\": [1, 2], \"quantile\": 0.5}, dims=\"x\"\n    )\n    actual = array.groupby(\"x\").quantile(0.5, interpolation=\"nearest\")\n    assert_identical(expected, actual)\n\n    actual = array.groupby(\"x\").quantile(0.5, interpolation=\"midpoint\")\n    expected = xr.DataArray(\n        data=[2, 5], coords={\"x\": [1, 2], \"quantile\": 0.5}, dims=\"x\"\n    )\n    assert_identical(expected, actual)\n\n    # Test with a vector of quantiles\n    expected = xr.DataArray(\n        data=[[1, 3], [4, 6]],\n        coords={\"x\": [1, 2], \"quantile\": [0, 1]},\n        dims=(\"x\", \"quantile\"),\n    )\n    actual = array.groupby(\"x\").quantile([0, 1], interpolation=\"midpoint\")\n    assert_identical(expected, actual)\n\n    # Test with a single quantile and a different interpolation method\n    expected = xr.DataArray(\n        data=[2, 5], coords={\"x\": [1, 2], \"quantile\": 0.5}, dims=\"x\"\n    )\n    actual = array.groupby(\"x\").quantile(0.5, interpolation=\"linear\")\n    assert_identical(expected, actual)\n\n    # Test with a vector of quantiles and a different interpolation method\n    expected = xr.DataArray(\n        data=[[1, 3], [4, 6]],\n        coords={\"x\": [1, 2], \"quantile\": [0, 1]},\n        dims=(\"x\", \"quantile\"),\n    )\n    actual = array.groupby(\"x\").quantile([0, 1], interpolation=\"linear\")\n    assert_identical(expected, actual)\n\n    # Test with an invalid interpolation method\n    with raises_regex(ValueError, \"interpolation must be one of", "def test_groupby_reduce_with_grouper():\n    # create test data\n    times = pd.date_range(\"2000-01-01\", periods=365)\n    x = [0, 1]\n    foo = xr.DataArray(\n        np.reshape(np.arange(365 * 2), (365, 2)),\n        coords={\"time\": times, \"x\": x},\n        dims=(\"time\", \"x\"),\n    )\n    grouper = pd.Grouper(freq=\"M\")\n    g = foo.groupby(foo.time.dt.month)\n\n    actual = g.reduce(np.mean, dim=..., skipna=True)\n    expected = xr.DataArray(\n        data=[\n            14.5,\n            45.5,\n            76.5,\n            107.5,\n            138.5,\n            169.5,\n            200.5,\n            231.5,\n            262.5,\n            293.5,\n            324.5,\n            355.5,\n        ],\n        coords={\"month\": np.arange(1, 13), \"x\": x},\n        dims=(\"month\", \"x\"),\n    )\n    assert_identical(expected, actual)\n\n    actual = g.reduce(np.mean, dim=\"time\", skipna=True)\n    expected = xr.DataArray(\n        data=[\n            183.0,\n            183.0,\n        ],\n        coords={\"month\": np.arange(1, 13), \"x\": x},\n        dims=(\"month\", \"x\"),\n    )\n    assert_identical(expected, actual)\n\n    actual = g.reduce(np.mean, dim=\"time\", skipna=False)\n    expected = xr.DataArray(\n        data=[\n            183.0,\n            183.0,\n        ],\n        coords={\"month\": np.arange(1, 13), \"x\": x},\n        dims=(\"month\", \"x\"),\n    )\n    assert_identical(expected, actual)\n", "def test_groupby_dtypes():\n    # test for different dtypes in groupby\n    array1 = xr.DataArray(\n        [1, 2, 3], [(\"x\", [1, 2, 2])], dtype=np.int64\n    )\n    array2 = xr.DataArray(\n        [1, 2, 3], [(\"x\", [1, 2, 2])], dtype=np.float64\n    )\n    array3 = xr.DataArray(\n        [1, 2, 3], [(\"x\", [1, 2, 2])], dtype=np.float32\n    )\n    array4 = xr.DataArray(\n        [1, 2, 3], [(\"x\", [1, 2, 2])], dtype=np.complex128\n    )\n    array5 = xr.DataArray(\n        [1, 2, 3], [(\"x\", [1, 2, 2])], dtype=np.string_\n    )\n\n    # Check that each groupby produces the correct dtype\n    assert array1.groupby(\"x\").sum().dtype == np.int64\n    assert array2.groupby(\"x\").sum().dtype == np.float64\n    assert array3.groupby(\"x\").sum().dtype == np.float32\n    assert array4.groupby(\"x\").sum().dtype == np.complex128\n    assert array5.groupby(\"x\").sum().dtype == np.string_\n"], "sample_879": ["def test_ordinal_encoder_handle_unknowns_non_finite():\n    \"\"\"Test OrdinalEncoder with non-finite unknown values.\"\"\"\n    enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-2)\n    X_fit = np.array([[\"a\", \"x\"], [\"b\", \"y\"], [\"c\", \"z\"]], dtype=object)\n    X_trans = np.array([[\"c\", \"xy\"], [\"bla\", \"y\"], [\"a\", \"x\"]], dtype=object)\n    enc.fit(X_fit)\n\n    X_trans_enc = enc.transform(X_trans)\n    exp = np.array([[2, -2], [-2, 1], [0, 0]], dtype=\"int64\")\n    assert_array_equal(X_trans_enc, exp)\n\n    X_trans_inv = enc.inverse_transform(X_trans_enc)\n    inv_exp = np.array([[\"c\", None], [None, \"y\"], [\"a\", \"x\"]], dtype=object)\n    assert_array_equal(X_trans_inv, inv_exp)\n", "def test_one_hot_encoder_infrequent_categories_with_passed_categories():\n    \"\"\"Check that the order of categories provided by a user is respected.\"\"\"\n    X_train = np.array([[\"a\"] * 5 + [\"b\"] * 20 + [\"c\"] * 10 + [\"d\"] * 3]).T\n    categories = [[\"b\", \"d\", \"a\", \"c\"]]\n    ohe = OneHotEncoder(\n        categories=categories,\n        max_categories=2,\n        handle_unknown=\"infrequent_if_exist\",\n        sparse_output=False,\n    ).fit(X_train)\n\n    assert_array_equal(ohe.infrequent_categories_, [[\"b\", \"d\", \"a\"]])\n\n    X_test = [[\"b\"], [\"a\"], [\"c\"], [\"d\"], [\"e\"]]\n    expected = np.array([[1, 0], [0, 1], [0, 1], [0, 1], [0, 1]])\n\n    X_trans = ohe.transform(X_test)\n    assert_allclose(expected, X_trans)\n\n    # 'infrequent' is used to denote the infrequent categories for\n    # `inverse_transform`\n    expected_inv = [[col] for col in [\"b\"] + [\"infrequent_sklearn\"] * 4]\n    X_inv = ohe.inverse_transform(X_trans)\n    assert_array_equal(expected_inv, X_inv)\n\n    feature_names = ohe.get_feature_names_out()\n    assert_array_equal([\"x0_b\", \"x0_infrequent_sklearn\"], feature_names)\n", "def test_one_hot_encoder_infrequent_categories_with_missing_values():\n    \"\"\"Check that the infrequent categories and missing values are handled correctly.\"\"\"\n    # X[:, 0] 'a', 'b', 'c' have the same frequency. 'a' and 'b' will be\n    # considered infrequent because they are greater\n\n    # X[:, 1] 0, 3, 5, 10 has frequency 2 and 12 has frequency 1.\n    # 0, 3, 12 will be considered infrequent\n    X_train = np.c_[\n        [0, 1, 3, 3, 3, 3, 2, 0, 3, None],\n        [0, 0, 0, 0, 1, 1, 1, 1, 1, 0],\n    ]\n\n    ohe = OneHotEncoder(max_categories=3, drop=\"if_binary\", sparse_output=False)\n    ohe.fit(X_train)\n\n    X_test = [[3, 0], [1, 1]]\n    X_trans = ohe.transform(X_test)\n\n    # feature 1 is binary so it drops a category 0\n    assert_allclose(X_trans, [[0, 1, 0, 0], [0, 0, 1, 1]])\n\n    X_test = [[1, 12], [1, None]]\n    X_trans = ohe.transform(X_test)\n\n    # feature 1 is binary so it drops a category 0\n    assert_allclose(X_trans, [[0, 0, 1, 0], [0, 0, 1, 0]])\n\n    X_inv = ohe.inverse_transform(X_trans)\n    expected_inv = np.array(\n        [[3, \"infrequent_sklearn\"], [3, \"infrequent_sklearn\"]], dtype=object\n    )\n    assert_array_equal(expected_inv, X_inv)\n", "def test_ordinal_encoder_handle_unknown_with_infrequent():\n    \"\"\"Check that `OrdinalEncoder` with `handle_unknown='use_encoded_value'` and\n    `infrequent` categories works as expected.\"\"\"\n    X_train = np.array([[\"a\"] * 5 + [\"b\"] * 20 + [\"c\"] * 10 + [\"d\"] * 3]).T\n    ohe = OneHotEncoder(\n        categories=\"auto\",\n        handle_unknown=\"infrequent_if_exist\",\n        sparse_output=False,\n        max_categories=2,\n    ).fit(X_train)\n\n    oe = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=2).fit(\n        ohe.transform(X_train)\n    )\n\n    # \"infrequent_sklearn\" is unknown\n    X_test = [[\"infrequent_sklearn\"], [\"c\"], [\"d\"], [\"e\"]]\n    expected = np.array([[2], [1], [0], [2]])\n    X_test_trans = oe.transform(X_test)\n    assert_allclose(X_test_trans, expected)\n\n    # inverse_transform maps unknown to None\n    X_inv = oe.inverse_transform(X_test_trans)\n    expected_inv = np.array([[\"infrequent_sklearn\"], [\"c\"], [\"d\"], [None]], dtype=object)\n    assert_array_equal(expected_inv, X_inv)\n\n    # Check that encoded_missing_value is used for missing categories\n    X_train = np.array([[\"a\"] * 5 + [\"b\"] * 20 + [\"c\"] * 10 + [\"d\"] * 3], dtype=object).T\n    oe = OrdinalEncoder(\n        handle_unknown=\"use_encoded_value\",\n        unknown_value=2,\n        encoded_missing_value=-1,\n    ).fit(ohe.transform(X_train))\n\n    X_test = [[\"infrequent_sklearn\"], [\"c\"], [\"d\"], [\"e\"], [\"None\"]]\n    expected = np.array([[2], [1], [0], [2], [-1]])\n    X_test_trans = oe.transform(X_test)\n    assert_allclose(X_test_trans, expected)\n\n    # inverse_transform maps unknown to None and encoded missing to None\n    X_inv = oe.inverse_transform(X_test_trans)\n    expected_inv = np.array(\n        [[\"infrequent_sklearn\"], [\"c\"], [\"d\"], [None], [None]], dtype=object\n    )\n    assert_array_equal(expected_inv, X_inv)\n", "def test_get_feature_names_out_handle_input_features_with_spaces_and_numbers(Encoder):\n    \"\"\"Check that get_feature_names_out is not broken by spaces or numbers in the\n    input feature names.\n\n    Non-regression test for:\n    https://github.com/scikit-learn/scikit-learn/issues/26842\n    \"\"\"\n    feature_names_in = [\"col1\", \"feature2 with space\", \"col3_with_underscore_4\"]\n    X = np.array([[1, 2, 3]])\n    encoder = Encoder()\n    X_trans = encoder.fit_transform(X)\n\n    if Encoder == OneHotEncoder:\n        expected_feature_names_out = [\n            \"col1_1\",\n            \"feature2 with space_2\",\n            \"col3_with_underscore_4_3\",\n        ]\n    else:\n        expected_feature_names_out = [\"col1\", \"feature2 with space\", \"col3_with_underscore_4\"]\n\n    assert_array_equal(encoder.get_feature_names_out(input_features=feature_names_in), expected_feature_names_out)\n"], "sample_1101": ["def test_schur_subsets_number():\n    raises(ValueError, lambda: _schur_subsets_number(S.Infinity))\n    raises(ValueError, lambda: _schur_subsets_number(-1))\n    raises(ValueError, lambda: _schur_subsets_number(0))\n    assert _schur_subsets_number(1) == 1\n    assert _schur_subsets_number(2) == 1\n    assert _schur_subsets_number(3) == 1\n    assert _schur_subsets_number(4) == 2\n    assert _schur_subsets_number(5) == 2\n    random_number_generator = _randint(1000)\n    for _ in range(5):\n        n = random_number_generator(1, 1000)\n        assert _schur_subsets_number(n) == math.ceil(math.log(2*n + 1, 3))\n", "def test__schur_subsets_number():\n    raises(ValueError, lambda: _schur_subsets_number(S.Infinity))\n    raises(ValueError, lambda: _schur_subsets_number(-1))\n    raises(ValueError, lambda: _schur_subsets_number(0))\n    assert _schur_subsets_number(1) == 1\n    assert _schur_subsets_number(2) == 1\n    assert _schur_subsets_number(3) == 1\n    assert _schur_subsets_number(4) == 2\n    assert _schur_subsets_number(5) == 2\n    assert _schur_subsets_number(6) == 2\n    assert _schur_subsets_number(7) == 2\n    assert _schur_subsets_number(8) == 2\n    assert _schur_subsets_number(9) == 3\n\n    random_number_generator = _randint(1000)\n    for _ in range(5):\n        n = random_number_generator(1, 1000)\n        min_k = _schur_subsets_number(n)\n        assert isinstance(min_k, int)\n        assert min_k > 0\n", "def test_schur_subsets_number():\n    raises(ValueError, lambda: _schur_subsets_number(S.Infinity))\n    raises(ValueError, lambda: _schur_subsets_number(-1))\n    raises(ValueError, lambda: _schur_subsets_number(0))\n\n    assert _schur_subsets_number(1) == 1\n    assert _schur_subsets_number(2) == 1\n    assert _schur_subsets_number(3) == 1\n    for i in range(4, 1000):\n        assert _schur_subsets_number(i) == math.ceil(math.log(2*i + 1, 3))\n", "def test_generate_next_list():\n    test_cases = [\n        ([[1, 4], [2, 3]], 8, [[[3, 12], [9, 8]], [[6, 5], [7, 4]], [1, 4, 7]]),\n        ([[1, 4], [2, 3]], 9, [[[3, 12], [9, 8]], [[6, 5], [7, 4]], [1, 4, 7, 10]]),\n        ([[1], [4], [2, 3]], 10, [[[3], [12]], [[9], [8]], [[6, 5], [7, 4]], [1, 4, 7, 10]]),\n    ]\n    for input_list, n, expected_output in test_cases:\n        assert _generate_next_list(input_list, n) == expected_output\n", "def test_schur_subsets_number():\n    assert _schur_subsets_number(1) == 1\n    assert _schur_subsets_number(2) == 1\n    assert _schur_subsets_number(3) == 1\n    assert _schur_subsets_number(4) == 2\n    assert _schur_subsets_number(5) == 2\n    assert _schur_subsets_number(8) == 2\n    assert _schur_subsets_number(9) == 3\n\n    raises(ValueError, lambda: _schur_subsets_number(S.Infinity))\n    raises(ValueError, lambda: _schur_subsets_number(-1))\n    raises(ValueError, lambda: _schur_subsets_number(0))\n"], "sample_937": ["def test_unparse_function_definition(source, expected):\n    tree = ast.parse(source)\n    assert ast.unparse(tree.body[0]) == expected\n", "def test_unparse_function_signature(source, expected):\n    tree = ast.parse(source)\n    assert ast.unparse(tree.body[0].args) == expected\n", "def test_unparse_arguments():\n    source = \"def func(a: int, b=2, *, c: str, d=4, **kwargs): pass\"\n    expected = \"a: int, b=2, *, c: str, d=4, **kwargs\"\n    tree = ast.parse(source)\n    assert ast.unparse(tree.body[0].args) == expected\n", "def test_unparse_visit_arguments():\n    source = \"\"\"", "def test_unparse_arg_with_annotation():\n    source = \"def func(a: int, b: str) -> None: pass\"\n    expected = \"a: int, b: str\"\n    tree = ast.parse(source)\n    assert ast.unparse(tree.body[0].args) == expected\n\n"], "sample_1092": ["def test_opt_cse_order():\n    e = x*y**2 + x*y + 2*x*y\n    opt_subs = cse_main.opt_cse([e], 'canonical')\n    assert opt_subs == {(x*y**2): (x0, x*y)}\n    opt_subs = cse_main.opt_cse([e], 'none')\n    assert opt_subs == {(x*y): (x0, x*y)}\n", "def test_nested_cse_postprocess():\n    exprs = [exp(x**2) + exp(x**2)*y, x**2]\n    substs, reduced = cse(exprs, postprocess=cse_main.cse_separate)\n    ans = ([(x0, x**2), (x1, exp(x0))], [x1*(x1 + y), x0])\n    assert substs == ans[0]\n    assert reduced == ans[1]\n", "def test_issue_21516():\n    from sympy import symbols\n    x, y = symbols('x y')\n    expr = x + y\n    exprs = [expr, expr]\n    substitutions, reduced_exprs = cse(exprs, symbols('x:10'))\n    assert len(substitutions) == 1\n    assert reduced_exprs[0] == reduced_exprs[1]\n    assert expr.subs(reversed(substitutions)) == expr\n", "def test_issue_18204():\n    # Check if cse() is working correctly with non-commutative functions\n    A = Symbol('A', commutative=False)\n    B = Symbol('B', commutative=False)\n    C = Symbol('C', commutative=False)\n    assert cse(A*B*C) == ([], [A*B*C])\n    assert cse(B*C*A) == ([], [B*C*A])\n    assert cse([A*B*C, B*C*A]) == ([], [A*B*C, B*C*A])\n    assert cse([A*B*C, B*C]) == ([], [A*B*C, B*C])\n", "def test_cse_large_optimizations():\n    x = Symbol('x')\n    y = Symbol('y')\n\n    # A large expression with many repeated subexpressions.\n    expr = ((x + y)**10).expand()\n\n    # CSE on the expression without any optimizations.\n    # This should find some repeated subexpressions.\n    _, reduced_expr = cse(expr)\n\n    # CSE on the expression with some optimizations.\n    # This should find more repeated subexpressions.\n    _, reduced_expr_optimized = cse(expr, optimizations='basic')\n\n    # Check that the optimized CSE found more repeated subexpressions.\n    assert len(reduced_expr) < len(reduced_expr_optimized)\n\n    # Check that the optimized CSE did not change the expression.\n    assert reduced_expr.subs({x: 1, y: 2}) == reduced_expr_optimized.subs({x: 1, y: 2})\n"], "sample_201": ["def test_legacy_hash_decode_invalid(self):\n    # RemovedInDjango40Warning: pre-Django 3.1 hashes will be invalid.\n    storage = self.storage_class(self.get_request())\n    messages = ['this', 'that']\n    # Encode/decode a message using the pre-Django 3.1 hash, but with an invalid hash.\n    encoder = MessageEncoder(separators=(',', ':'))\n    value = encoder.encode(messages)\n    encoded_messages = '%s$%s' % ('invalid_hash', value)\n    decoded_messages = storage._decode(encoded_messages)\n    self.assertIsNone(decoded_messages)\n", "def test_legacy_hash_decode_invalid(self):\n    # RemovedInDjango40Warning: pre-Django 3.1 hashes will be invalid.\n    storage = self.storage_class(self.get_request())\n    messages = ['this', 'that']\n    # Encode/decode a message using the pre-Django 3.1 hash, but with an invalid hash.\n    encoder = MessageEncoder(separators=(',', ':'))\n    value = encoder.encode(messages)\n    encoded_messages = '%s$%s' % ('invalid_hash', value)\n    decoded_messages = storage._decode(encoded_messages)\n    self.assertIsNone(decoded_messages)\n", "def test_legacy_hash_decode_invalid(self):\n    # RemovedInDjango40Warning: pre-Django 3.1 hashes will be invalid.\n    storage = self.storage_class(self.get_request())\n    messages = ['this', 'that']\n    # Encode/decode a message using the pre-Django 3.1 hash.\n    encoder = MessageEncoder(separators=(',', ':'))\n    value = encoder.encode(messages)\n    # Intentionally corrupt the hash to test invalid hash handling.\n    encoded_messages = '%s$%s' % (storage._legacy_hash(value)[:-1], value)\n    decoded_messages = storage._decode(encoded_messages)\n    self.assertIsNone(decoded_messages)\n", "def test_max_cookie_length_with_empty_messages(self):\n    \"\"\"\n    If the data exceeds what is allowed in a cookie and the messages are empty,\n    the storage does not raise an exception and an empty list is returned by\n    the ``update`` method.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    msg_size = int((CookieStorage.max_cookie_size - 54) / 4.5 - 37)\n    for i in range(5):\n        storage.add(constants.INFO, '')\n    unstored_messages = storage.update(response)\n\n    cookie_storing = self.stored_messages_count(storage, response)\n    self.assertEqual(cookie_storing, 0)\n\n    self.assertEqual(unstored_messages, [])\n", "def test_cookie_not_finished_sentinel(self):\n    \"\"\"\n    Verify the 'not_finished' sentinel is added to the cookie when\n    the encoded data is larger than max_cookie_size.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # Create messages that will exceed max_cookie_size\n    msg_size = int((CookieStorage.max_cookie_size - 54) / 4.5 - 37)\n    for i in range(5):\n        storage.add(constants.INFO, str(i) * msg_size)\n    storage.update(response)\n\n    # Verify the 'not_finished' sentinel is in the cookie\n    cookie = response.cookies.get(storage.cookie_name)\n    data = storage._decode(cookie.value)\n    self.assertEqual(data[-1], CookieStorage.not_finished)\n\n    # Verify the 'not_finished' sentinel is removed after retrieval\n    storage = self.get_storage()\n    set_cookie_data(storage, data)\n    messages = list(storage)\n    self.assertNotIn(CookieStorage.not_finished, messages)\n"], "sample_606": ["def test_keep_attrs_strategies_dataarray_variables_missing(\n    variant, strategy, attrs, expected, error", "def test_cross_unequal_size() -> None:\n    with pytest.raises(ValueError):\n        xr.cross(xr.DataArray([1, 2, 3, 4]), xr.DataArray([4, 5, 6]), \"dim_0\")\n", "def test_cross_over_1d_array_with_coords() -> None:\n    a = xr.DataArray(\n        [1, 2],\n        dims=[\"cartesian\"],\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"z\"])),\n    )\n    b = xr.DataArray(\n        [4, 5, 6],\n        dims=[\"cartesian\"],\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])),\n    )\n\n    expected = np.array([12, -6, -3])\n    actual = xr.cross(a, b, dim=\"cartesian\")\n    assert_allclose(actual, expected)\n", "def test_cross_1d_and_2d_variable() -> None:\n    a = xr.Variable(\"cartesian\", np.array([1, 2]))\n    b = xr.Variable(\"cartesian\", np.array([4, 5, 6]))\n\n    expected = np.cross(np.array([1, 2, 0]), np.array([4, 5, 6]))\n    actual = xr.cross(a, b, dim=\"cartesian\")\n    assert_identical(expected, actual.data)\n\n    # reverse the order of the cross product\n    expected = np.cross(np.array([4, 5, 6]), np.array([1, 2, 0]))\n    actual = xr.cross(b, a, dim=\"cartesian\")\n    assert_identical(expected, actual.data)\n", "def test_crossBroadcasting(use_dask) -> None:\n    # Test for broadcasting over multiple dimensions\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"test for dask.\")\n\n    a = np.arange(3 * 2 * 4).reshape((3, 2, 4))\n    b = np.arange(3 * 2 * 4).reshape((3, 2, 4))\n    da = xr.DataArray(a, dims=(\"x\", \"y\", \"cartesian\"))\n    db = xr.DataArray(b, dims=(\"x\", \"y\", \"cartesian\"))\n    if use_dask:\n        da = da.chunk()\n        db = db.chunk()\n    actual = xr.cross(da, db, dim=\"cartesian\")\n\n    expected = np.cross(a, b, axis=-1)\n    xr.testing.assert_duckarray_allclose(expected, actual)\n"], "sample_1065": ["def test_factorial2_Mod():\n    pr = Symbol('pr', prime=True)\n    p, q = 10**9 + 9, 10**9 + 33 # prime modulo\n    r, s = 10**7 + 5, 33333333 # composite modulo\n    assert Mod(factorial2(pr - 1), pr) == pr - 1\n    assert Mod(factorial2(pr - 1), -pr) == -1\n    assert Mod(factorial2(r - 1, evaluate=False), r) == 0\n    assert Mod(factorial2(s - 1, evaluate=False), s) == 0\n    assert Mod(factorial2(p - 1, evaluate=False), p) == p - 1\n    assert Mod(factorial2(q - 1, evaluate=False), q) == q - 1\n    assert Mod(factorial2(p - 50, evaluate=False), p) == 854928834\n    assert Mod(factorial2(q - 1800, evaluate=False), q) == 905504050\n    assert Mod(factorial2(153, evaluate=False), r) == Mod(factorial2(153), r)\n    assert Mod(factorial2(255, evaluate=False), s) == Mod(factorial2(255), s)\n\n    n = Symbol('n', integer=True)\n    assert factorial2(n).is_integer is None\n    assert factorial2(n).is_nonnegative is None\n    assert factorial2(n).is_real is None\n    assert factorial2(n).is_even is None\n    assert factorial2(n).is_odd is None\n", "def test_multifactorial():\n    n = Symbol('n', integer=True)\n    x = Symbol('x')\n\n    assert MultiFactorial(n).diff(n) == Derivative(MultiFactorial(n), n)\n    assert MultiFactorial(x).diff(x) == Derivative(MultiFactorial(x), x)\n    raises(ArgumentIndexError, lambda: MultiFactorial(n**2).fdiff(2))\n\n    assert MultiFactorial(n).is_integer is None\n    assert MultiFactorial(n).is_positive is None\n    assert MultiFactorial(n).is_real is None\n    assert MultiFactorial(n).is_composite is None\n    assert MultiFactorial(n).is_even is None\n    assert MultiFactorial(n).is_odd is None\n", "def test_subfactorial_diff():\n    n = Symbol('n', integer=True)\n    assert subfactorial(n).diff(n) == subfactorial(n)*(1 - 1/(n + 1) - 1/n)\n    assert subfactorial(n**2).diff(n) == subfactorial(n**2)*(2*n - 2*n/(n**2 + 1) - 2*n/(n**2))\n    raises(ArgumentIndexError, lambda: subfactorial(n**2).fdiff(2))\n\n", "def test_rf_ff_polynomial_input():\n    x, y = symbols('x,y')\n    n, k = symbols('n k', integer=True)\n    m = Symbol('m', integer=True, nonnegative=True)\n\n    # Test with polynomials of degree > 1\n    assert rf(Poly(x**3 + 2*x**2 + 3*x + 1, x), 2) == Poly(x**6 + 8*x**5 + 35*x**4 + 50*x**3 + 42*x**2 + 18*x + 3, x)\n    assert ff(Poly(x**3 + 2*x**2 + 3*x + 1, x), 2) == Poly(x**6 + 4*x**5 + 5*x**4 - 2*x**3 - 3*x**2, x)\n\n    # Test with polynomial with negative coefficients\n    assert rf(Poly(-x**3 + 2*x**2 - 3*x - 1, x), 2) == Poly(x**6 - 8*x**5 + 35*x**4 - 50*x**3 + 42*x**2 - 18*x + 3, x)\n    assert ff(Poly(-x**3 + 2*x**2 - 3*x - 1, x), 2) == Poly(x**6 - 4*x**5 + 5*x**4 + 2*x**3 - 3*x**2, x)\n\n    # Test with polynomial with fractional coefficients\n    assert rf(Poly(x**3/2 + 2*x**2/3 + 3*x/4 + 1, x), 2) == Poly(x**6/4 + 8*x**5/3 + 35*x**4/2 + 50*x**3/3 + 42*x**2/2 + 18*x/2 + 3/4, x)\n    assert ff(Poly(x**3/2 + 2*x**2/3 + 3*x/4 + 1, x), 2) == Poly(x**6/4 + 4*x**5/3 + 5*x**4/2 - 2*x**3/3 - 3*x**2/4, x)\n\n    # Test with polynomial with symbolic coefficients\n    a, b, c, d = symbols('a", "def test_combinatorial_edge_cases():\n    x = Symbol('x')\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True, nonnegative=True)\n\n    assert factorial(1/2) == gamma(3/2)\n    assert factorial(oo) == oo\n    assert factorial(nan) == nan\n\n    assert factorial2(1/2) == sqrt(2*pi)/2\n    assert factorial2(oo) is None\n    assert factorial2(nan) == nan\n\n    assert subfactorial(nan) == nan\n    assert subfactorial(-oo) == oo\n    assert subfactorial(oo) == oo\n    assert subfactorial(S.Half) == oo\n    assert subfactorial(x).func == subfactorial\n\n    assert binomial(oo, 2) == oo\n    assert binomial(nan, 2) == nan\n    assert binomial(x, oo) == oo\n    assert binomial(x, 2).is_integer is None\n    assert binomial(nan, x) == nan\n    assert binomial(n, x).func == binomial\n\n    assert RisingFactorial(nan, 2) == nan\n    assert RisingFactorial(oo, 2) == oo\n    assert RisingFactorial(x, oo) == oo\n    assert RisingFactorial(x, 2).func == RisingFactorial\n    assert RisingFactorial(nan, x) == nan\n    assert RisingFactorial(n, x).func == RisingFactorial\n\n    assert FallingFactorial(nan, 2) == nan\n    assert FallingFactorial(oo, 2) == oo\n    assert FallingFactorial(x, oo) == oo\n    assert FallingFactorial(x, 2).func == FallingFactorial\n    assert FallingFactorial(nan, x) == nan\n    assert FallingFactorial(n, x).func == FallingFactorial\n"], "sample_127": ["def test_empty_model_empty_field_values(self):\n    NoFields.objects.bulk_create([NoFields() for i in range(2)])\n    self.assertEqual(NoFields.objects.count(), 2)\n    self.assertEqual(NoFields.objects.all().count(), 2)\n", "def test_bulk_create_with_expression_primary_key(self):\n    from django.db.models.functions import Now\n    from django.db.models import DateTimeField\n    class TestModel(models.Model):\n        id = DateTimeField(primary_key=True)\n    TestModel.objects.bulk_create([\n        TestModel(id=Now()),\n        TestModel(id=Now()),\n        TestModel(id=Now()),\n    ])\n    self.assertEqual(TestModel.objects.count(), 3)\n", "def test_bulk_update_efficiency(self):\n    # Create objects to update.\n    for i in range(0, 1001):\n        TwoFields.objects.create(f1=i, f2=i + 1)\n\n    # Retrieve the objects to update.\n    objects = list(TwoFields.objects.all())\n\n    # Update the objects and measure the number of database queries.\n    with self.assertNumQueries(1):\n        TwoFields.objects.bulk_update(objects, ['f1', 'f2'])\n\n    # Test that the objects have been updated.\n    self.assertEqual(TwoFields.objects.filter(f1__gte=450, f1__lte=550).count(), 101)\n    self.assertEqual(TwoFields.objects.filter(f2__gte=901).count(), 101)\n", "def test_large_batch_mixed_efficiency_multiple_fields(self):\n    \"\"\"\n    Test inserting a large batch with objects having primary key set\n    mixed together with objects without PK set. Also, the objects have\n    multiple fields with different values.\n    \"\"\"\n    with override_settings(DEBUG=True):\n        connection.queries_log.clear()\n        TwoFields.objects.bulk_create([\n            TwoFields(id=i if i % 2 == 0 else None, f1=i, f2=i + 1)\n            for i in range(100000, 101000)\n        ])\n        self.assertLess(len(connection.queries), 10)\n        # Check if the data was correctly saved\n        self.assertEqual(TwoFields.objects.count(), 1000)\n        for obj in TwoFields.objects.all():\n            if obj.id % 2 == 0:\n                self.assertEqual(obj.f1, obj.id)\n                self.assertEqual(obj.f2, obj.id + 1)\n            else:\n                self.assertIsNotNone(obj.f1)\n                self.assertIsNotNone(obj.f2)\n", "    def test_bulk_create_with_raw_id_fields(self):\n        # Regression test for #25071\n        pizzeria = Pizzeria.objects.create(name=\"Test\")\n        restaurant = Restaurant.objects.create(name=\"Test\", pizzeria=pizzeria)\n        with self.assertNumQueries(1):\n            Pizzeria.objects.bulk_create([\n                Pizzeria(id=2, name=\"Pizzeria 1\", raw_id_fields={'restaurant': restaurant.id}),\n                Pizzeria(id=3, name=\"Pizzeria 2\", raw_id_fields={'restaurant': restaurant.id}),\n            ])\n        pizzeria1, pizzeria2 = Pizzeria.objects.all()[1:]\n        self.assertEqual(pizzeria1.name, \"Pizzeria 1\")\n        self.assertEqual(pizzeria2.name, \"Pizzeria 2\")\n        self.assertEqual(pizzeria1.restaurant_id, restaurant.id)\n        self.assertEqual(pizzeria2.restaurant_id, restaurant.id)\n"], "sample_358": ["    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'], lambda column: column.upper(),\n            col_suffixes=('ASC', 'DESC'), opclasses=('opclass1', 'opclass2')\n        )\n", "    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'], lambda column: column.upper(),\n            col_suffixes=['ASC', 'DESC'], opclasses=['varchar_pattern_ops', 'text_pattern_ops']\n        )\n", "    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'], lambda column: column.upper(), col_suffixes=('ASC', 'DESC')\n        )\n", "    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'],\n            lambda column: column.upper(), col_suffixes=('ASC', 'DESC'),\n            opclasses=('opclass1', 'opclass2'),\n        )\n", "    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'], lambda column: column.upper(), \n            ('ASC', 'DESC'), ('btree', 'brin')\n        )\n"], "sample_335": ["    def test_decimalfield_min_value_zero(self):\n        f = DecimalField(min_value=decimal.Decimal('0'))\n        self.assertEqual(f.clean('0'), decimal.Decimal(\"0\"))\n        self.assertEqual(f.clean('-0'), decimal.Decimal(\"0\"))\n        with self.assertRaisesMessage(ValidationError, \"'Ensure this value is greater than or equal to 0.'\"):\n            f.clean('-0.1')\n", "def test_decimalfield_negative_leading_zeros(self):\n    f = DecimalField(max_digits=5, decimal_places=2)\n    self.assertEqual(f.clean('-0000000.10'), decimal.Decimal(\"-0.10\"))\n    # But a leading 0 before the . doesn't count towards max_digits\n    self.assertEqual(f.clean('-0000000.100'), decimal.Decimal(\"-0.100\"))\n    # Only leading whole zeros \"collapse\" to one digit.\n    self.assertEqual(f.clean('-000000.02'), decimal.Decimal(\"-0.02\"))\n    with self.assertRaisesMessage(ValidationError, \"'Ensure that there are no more than 5 digits in total.'\"):\n        f.clean('-000000.0002')\n    self.assertEqual(f.clean('-.002'), decimal.Decimal(\"-0.002\"))\n", "    def test_decimalfield_non_finite(self):\n        f = DecimalField(max_digits=4, decimal_places=2)\n        values = (\n            'Infinity', '-Infinity', 'inf', '-inf', 'INFINITY', '-INFINITY',\n        )\n        for value in values:\n            with self.subTest(value=value), self.assertRaisesMessage(ValidationError, \"'Enter a number.'\"):\n                f.clean(value)\n\n        values = (\n            'NaN', '-NaN', 'nan', '-nan', 'NAN', '-NAN',\n        )\n        for value in values:\n            with self.subTest(value=value), self.assertRaisesMessage(ValidationError, \"'Enter a number.'\"):\n                f.clean(value)\n", "def test_decimalfield_empty_string_with_step(self):\n    f = DecimalField(max_digits=4, decimal_places=2, widget=NumberInput(attrs={'step': '0.01'}))\n    self.assertWidgetRendersTo(f, '<input step=\"0.01\" type=\"number\" name=\"f\" id=\"id_f\" required>')\n    with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n        f.clean('')\n    with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n        f.clean(None)\n", "def test_decimalfield_edge_cases(self):\n    f = DecimalField(max_digits=1, decimal_places=1)\n    with self.assertRaisesMessage(ValidationError, \"'Ensure that there are no more than 1 digit before the decimal point.'\"):\n        f.clean('1.1')\n    f = DecimalField(max_digits=0, decimal_places=1)\n    with self.assertRaisesMessage(ValidationError, \"'Ensure that there are no more than 0 digits in total.'\"):\n        f.clean('0.1')\n    f = DecimalField(max_digits=1, decimal_places=0)\n    self.assertEqual(f.clean('1'), decimal.Decimal(\"1\"))\n    with self.assertRaisesMessage(ValidationError, \"'Ensure that there are no more than 1 digit in total.'\"):\n        f.clean('10')\n    f = DecimalField(max_digits=0, decimal_places=0)\n    self.assertEqual(f.clean('0'), decimal.Decimal(\"0\"))\n    with self.assertRaisesMessage(ValidationError, \"'Ensure that there are no more than 0 digits in total.'\"):\n        f.clean('1')\n"], "sample_353": ["def test_environment_variable_priority_non_interactive(self):\n    # Command line arguments have higher priority than environment variables in non-interactive mode.\n    call_command(\n        'createsuperuser',\n        interactive=False,\n        username='cmd_superuser',\n        email='cmd@somewhere.org',\n        password='cmd_password',\n        verbosity=0,\n    )\n    user = User.objects.get(username='cmd_superuser')\n    self.assertEqual(user.email, 'cmd@somewhere.org')\n    self.assertTrue(user.check_password('cmd_password'))\n", "    def test_command_init(self):\n        command = createsuperuser.Command()\n        self.assertIsNone(command.stdin)\n", "def test_get_input_data_custom_validation(self):\n    \"\"\"\n    Test custom validation in get_input_data.\n    \"\"\"\n    new_io = StringIO()\n    entered_passwords = ['nopasswd', 'nopasswd']\n    entered_date_of_births = ['1976-04-01', '1976-04-01']\n\n        return entered_passwords.pop(0)\n\n        return entered_date_of_births.pop(0)\n\n    @mock_inputs({\n        'password': return_passwords,\n        'date_of_birth': return_date_of_births,\n        'email': 'joe@somewhere.org',\n        'first_name': 'Joe',\n    })\n        call_command(\n            'createsuperuser',\n            interactive=True,\n            stdin=MockTTY(),\n            stdout=new_io,\n            stderr=new_io,\n        )\n        command_output = new_io.getvalue().strip()\n        self.assertEqual(command_output, 'Superuser created successfully.')\n        u = CustomUser._default_manager.get(email=\"joe@somewhere.org\")\n        self.assertEqual(u.date_of_birth, date(1976, 4, 1))\n\n    test(self)\n", "def test_handle_m2m_required_field(self):\n    \"\"\"Creation should fail if the user doesn't provide a value for an M2M required field.\"\"\"\n    @override_settings(AUTH_USER_MODEL='auth_tests.CustomUserWithM2M')\n        org_id = Organization.objects.create(name='Organization').pk\n        call_command('createsuperuser', interactive=False, username='joe', stdout=self.stdout)\n        msg = 'You must use --orgs with --noinput.'\n        self.assertIn(msg, self.stdout.getvalue().strip())\n        self.assertEqual(CustomUserWithM2M._default_manager.count(), 0)\n\n    test(self)\n", "    def setUp(self):\n        self.command = createsuperuser.Command()\n        self.command.UserModel = User\n        self.command.username_field = self.command.UserModel._meta.get_field(self.command.UserModel.USERNAME_FIELD)\n"], "sample_515": ["def test_colorbar_orientation_vertical_horizontal():\n    \"\"\"\n    Test that the colorbar orientation is correctly set to\n    'vertical' and 'horizontal'\n    \"\"\"\n    fig, ax = plt.subplots(2)\n    pc = ax[0].pcolormesh(np.arange(100).reshape(10, 10))\n    cb = fig.colorbar(pc, ax=ax[0])\n    assert cb.orientation == 'vertical'\n\n    pc = ax[1].pcolormesh(np.arange(100).reshape(10, 10))\n    cb = fig.colorbar(pc, ax=ax[1], orientation='horizontal')\n    assert cb.orientation == 'horizontal'\n", "def test_colorbar_extension_trianglematch_inverted_axis(orientation, extend, expected):\n    \"\"\"Test extension color with an inverted axis\"\"\"\n    data = np.arange(12).reshape(3, 4)\n    fig, ax = plt.subplots()\n    cmap = mpl.colormaps[\"viridis\"].with_extremes(under=(0, 0, 0, 1),\n                                                  over=(1, 1, 1, 1))\n    im = ax.imshow(data, cmap=cmap)\n    cbar = fig.colorbar(im, orientation=orientation, extend=extend)\n    if orientation == \"horizontal\":\n        cbar.ax.invert_xaxis()\n    else:\n        cbar.ax.invert_yaxis()\n    assert cbar._extend_patches[0].get_facecolor() == expected\n    if extend == \"both\":\n        assert len(cbar._extend_patches) == 2\n        assert cbar._extend_patches[1].get_facecolor() == (1, 1, 1, 1)\n    else:\n        assert len(cbar._extend_patches) == 1\n", "def test_colorbar_mappable_noraise():\n    # github issue #21158\n    fig, ax = plt.subplots()\n    mappable = cm.ScalarMappable(cmap='viridis')\n    mappable.set_array([])  # set an empty array\n    fig.colorbar(mappable, ax=ax)\n    # check that no error is raised\n    assert True\n", "def test_colorbar_tickformatter():\n    # test for #16030\n    fig, ax = plt.subplots()\n    im = ax.pcolormesh(np.random.rand(10, 10), norm=LogNorm())\n    cbar = fig.colorbar(im)\n    assert isinstance(cbar.ax.yaxis.get_major_formatter(), ticker.LogFormatter)\n    assert isinstance(cbar.ax.yaxis.get_minor_formatter(), ticker.LogFormatter)\n    cbar.formatter = ticker.FormatStrFormatter('%1.0f')\n    fig.canvas.draw()\n    assert not isinstance(cbar.ax.yaxis.get_major_formatter(), ticker.LogFormatter)\n    assert not isinstance(cbar.ax.yaxis.get_minor_formatter(), ticker.LogFormatter)\n", "def test_colorbar_set_ticks_lognorm():\n    # test set_ticks for lognorm\n    x, y = np.ogrid[-4:4:31j, -4:4:31j]\n    z = 120000*np.exp(-x**2 - y**2)\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(z)\n    cbar = fig.colorbar(im)\n    np.testing.assert_allclose(cbar.ax.yaxis.get_majorticklocs(),\n                               np.arange(0, 120000.1, 20000))\n\n    # check we don't break the log scale:\n    cbar.set_ticks([1, 2, 3, 4])\n    np.testing.assert_allclose(cbar.ax.yaxis.get_majorticklocs(),\n                               np.logspace(0, np.log10(4), 4))\n\n    # check that setting a locator (e.g. a loglocator) preserves the ticks\n    from matplotlib.ticker import LogLocator\n    cbar.locator = LogLocator(base=10, numticks=5)\n    np.testing.assert_allclose(cbar.ax.yaxis.get_majorticklocs(),\n                               np.logspace(0, np.log10(120000), 5))\n\n    # check setting the locator to a different kind of locator breaks the ticks:\n    cbar.locator = ticker.MaxNLocator(4)\n    assert cbar.ax.yaxis.get_major_locator()._integer == True\n    assert np.any(cbar.ax.yaxis.get_majorticklocs() < 0)\n"], "sample_992": ["def test_MpmathPrinter():\n    p = MpmathPrinter()\n    assert p.doprint(pi) == 'mpmath.pi'\n    assert p.module_imports == {'mpmath': {'pi'}}\n    from sympy.functions.elementary.exponential import log\n    assert p.doprint(log(2)) == 'mpmath.log(2)'\n    assert p.module_imports == {'mpmath': {'pi', 'log'}}\n    assert p.doprint(log(2, 2)) == 'mpmath.log(2)/mpmath.log(2)'\n    assert p.module_imports == {'mpmath': {'pi', 'log'}}\n    assert p.doprint(x**y) == 'x**y'\n    assert p.module_imports == {'mpmath': {'pi', 'log'}}\n", "def test_NumericCodePrinter():\n    p = NumPyPrinter()\n    assert p._print_uppergamma(sympy.uppergamma(1, x)) == \"mpmath.gammainc(1, x, mpmath.inf)\"\n    assert p._print_lowergamma(sympy.lowergamma(1, x)) == \"mpmath.gammainc(1, 0, x)\"\n    assert p._print_log1p(sympy.log1p(x)) == \"mpmath.log(x+1)\"\n    assert p._print_log2(sympy.log2(x)) == \"mpmath.log(x)/mpmath.log(2)\"\n", "def test_NumPyPrinter():\n    p = NumPyPrinter()\n    expr = x**2\n    assert p.doprint(expr) == 'x**2'\n    assert not p.module_imports\n    expr = expr + 2\n    assert p.doprint(expr) == 'x**2 + 2'\n    assert not p.module_imports\n    expr = x**2 + 2*pi\n    assert p.doprint(expr) == 'x**2 + 2*numpy.pi'\n    assert 'numpy' in p.module_imports\n    expr = x**2 + 2*pi + 3*x\n    assert p.doprint(expr) == 'x**2 + 2*numpy.pi + 3*x'\n    assert 'numpy' in p.module_imports\n", "def test_NaN_Infinity():\n    p = PythonCodePrinter()\n    assert p.doprint(float('nan')) == 'float(\\'nan\\')'\n    assert p.doprint(float('inf')) == 'float(\\'inf\\')'\n", "def test_NumericPrinter_class_hierarchy():\n    class CustomPrinter(PythonCodePrinter):\n        _kf = dict(chain(\n            _known_functions.items(),\n            [(k, 'custom_module.' + v) for k, v in _known_functions_math.items()]\n        ))\n\n    class SubPrinter(CustomPrinter):\n        pass\n\n    expr = acos(x)\n\n    printer = CustomPrinter()\n    assert printer.doprint(expr) == 'custom_module.acos(x)'\n    assert printer.module_imports == {'custom_module': {'acos'}}\n\n    sub_printer = SubPrinter()\n    assert sub_printer.doprint(expr) == 'custom_module.acos(x)'\n    assert sub_printer.module_imports == {'custom_module': {'acos'}}\n"], "sample_668": ["def test_funcargnames_deprecated(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: The `funcargnames` attribute was an alias for `fixturenames`, since pytest 2.3 - use the newer attribute instead.*\"\n        ]\n    )\n\n", "def test_funcargnames_deprecation():\n    with pytest.warns(pytest.PytestDeprecationWarning, match=\"The `funcargnames` attribute was an alias for `fixturenames`, since pytest 2.3 - use the newer attribute instead.\"):\n        deprecated.FUNCARGNAMES.emit_warning()\n", "def test_fixture_positional_arguments_deprecation(testdir, arg_type):\n    testdir.makepyfile(\n        f\"\"\"\n        import pytest\n\n        @pytest.fixture\n            return {arg_type}\n\n            pass\n    \"\"\"\n    )\n    if arg_type == \"positional\":\n        with pytest.warns(pytest.PytestDeprecationWarning, match=\"Passing arguments to pytest.fixture\\\\(\\\\) as positional arguments is deprecated - pass them as a keyword argument instead.\"):\n            testdir.runpytest()\n    else:\n        testdir.runpytest()\n", "def test_fixtures_positional_args(testdir, fixture_name, expected_warning):\n    if expected_warning:\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.fixture\n            def {}(arg1, arg2):\n                pass\n\n                pass\n        \"\"\".format(fixture_name, fixture_name)\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines([\"*{}*\".format(expected_warning)])\n    else:\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.fixture\n            def {}(**kwargs):\n                pass\n\n                pass\n        \"\"\".format(fixture_name, fixture_name)\n        )\n        result = testdir.runpytest()\n        result.stdout.no_fnmatch_line(\"*PytestDeprecationWarning:*\")\n", "def test_deprecated_warnings(testdir, warning, message):\n    testdir.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-W\", \"error::pytest.PytestDeprecationWarning\")\n    result.stdout.no_fnmatch_line(message)\n    \n    with pytest.warns(warning):\n        # trigger the warning here\n        # the exact trigger logic may vary based on the specific warning\n        # for this example, we'll assume it's triggered when a test is run\n        testdir.runpytest()\n\n    result = testdir.runpytest(\"-W\", \"error::pytest.PytestDeprecationWarning\")\n    result.stdout.fnmatch_lines([message])\n"], "sample_499": ["def test_set_bbox_to_anchor():\n    fig, ax = plt.subplots()\n    leg = ax.legend()\n    bbox_to_anchor = [1.0, 0.5]\n    leg.set_bbox_to_anchor(bbox_to_anchor)\n    assert leg.get_bbox_to_anchor().x0 == bbox_to_anchor[0]\n    assert leg.get_bbox_to_anchor().y0 == bbox_to_anchor[1]\n    assert leg.get_bbox_to_anchor().width == 0\n    assert leg.get_bbox_to_anchor().height == 0\n\n    bbox_to_anchor = [1.0, 0.5, 0.1, 0.1]\n    leg.set_bbox_to_anchor(bbox_to_anchor)\n    assert leg.get_bbox_to_anchor().x0 == bbox_to_anchor[0]\n    assert leg.get_bbox_to_anchor().y0 == bbox_to_anchor[1]\n    assert leg.get_bbox_to_anchor().width == bbox_to_anchor[2]\n    assert leg.get_bbox_to_anchor().height == bbox_to_anchor[3]\n", "def test_auto_legend_empty_collections():\n    # Test automatic legend placement with empty collections.\n    fig, ax = plt.subplots()\n    ax.scatter([1, 2, 3], [1, 2, 3], label='scatter')\n    ax.add_collection(mcollections.LineCollection([]))\n    ax.legend(loc='best')\n", "def test_legend_mode_with_ncol():\n    # test mode='expand' with ncol>1\n    fig, axs = plt.subplots(2, 1)\n    x = np.arange(100)\n    for ax in axs:\n        ax.plot(x, 50 - x, 'o', label='y=1')\n        ax.plot(x, x - 50, 'o', label='y=-1')\n        ax.legend(loc='upper left', mode='expand', ncol=2)\n", "def test_legend_draggable():\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='Aardvark')\n    leg = ax.legend()\n    leg.set_draggable(True)\n    assert leg.get_draggable()\n    leg.set_draggable(False)\n    assert not leg.get_draggable()\n\n    fig.canvas.draw()\n    event = mpl.backend_bases.LocationEvent('mouse motion', fig.canvas, 10, 10)\n    assert not leg.contains(event)\n\n    leg.set_draggable(True)\n    event = mpl.backend_bases.LocationEvent('mouse motion', fig.canvas, 10, 10)\n    assert leg.contains(event)\n\n    with pytest.raises(ValueError):\n        leg.set_draggable('invalid')\n", "def test_legend_custom_handler():\n    \"\"\"Test that the legend's custom handler is used\"\"\"\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label='line1')\n    ax.plot([4, 5, 6], label='line2')\n\n    class CustomHandler(mlegend.HandlerLine2D):\n                           width, height, fontsize, trans):\n            # Create a custom line handle\n            line = mlines.Line2D([xdescent, xdescent + width], [height / 2, height / 2],\n                                 color='red', linestyle='--')\n            return [line]\n\n    custom_handler_map = {mlines.Line2D: CustomHandler()}\n    ax.legend(handler_map=custom_handler_map)\n\n    for handle in ax.get_legend().legendHandles:\n        assert handle.get_color() == 'red'\n        assert handle.get_linestyle() == '--'\n"], "sample_860": ["def test_check_scalar_type_error():\n    with pytest.raises(TypeError, match=\"`test_name` must be an instance of\"):\n        check_scalar(\"a\", \"test_name\", int)\n        check_scalar(1, \"test_name\", str)\n", "def test_check_X_y_no_samples_y():\n    X = np.ones((0, 2))\n    y = None\n    assert_raise_message(ValueError, \"y cannot be None\", check_X_y, X, y)\n", "def test_check_scalar_with_unexpected_type():\n    # Test that check_scalar returns a TypeError if x has an unexpected type\n    with pytest.raises(TypeError) as raised_error:\n        check_scalar('string', \"test_name\", int)\n    assert str(raised_error.value) == \"`test_name` must be an instance of <class 'int'>, not <class 'str'>.\"\n", "def test_check_scalar_multiple_target_types():\n    # Test that check_scalar returns no error/warning if valid inputs are\n    # provided when multiple target types are given\n    with pytest.warns(None) as record:\n        check_scalar(3, \"test_name\", (int, float), 2, 5)\n    assert len(record) == 0\n\n    # Test that check_scalar raises the right error if a wrong input is given\n    # when multiple target types are provided\n    with pytest.raises(Exception) as raised_error:\n        check_scalar(\"three\", \"test_name\", (int, float), 2, 5)\n    assert str(raised_error.value) == \"`test_name` must be an instance of \"\n    assert type(raised_error.value) == TypeError\n", "def test_check_X_y_validate_input_ensure_min_features():\n    # Test that a warning is raised when the input is validated and \n    # ensure_min_features is greater than the number of features in the data\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n    msg = \"2 feature(s) (shape=(2, 2)) while a minimum of 3 is required.\"\n    with pytest.warns(UserWarning, match=msg):\n        check_X_y(X, y, ensure_min_features=3)\n"], "sample_752": ["def test_iforest_no_samples():\n    \"\"\"Test Isolation Forest with no samples.\"\"\"\n    X = np.array([[]])  # empty feature matrix\n    assert_raises(ValueError, IsolationForest().fit, X)\n", "def test_iforest_predict_on_train_data():\n    # Test predict on training data\n    X = iris.data\n    clf = IsolationForest().fit(X)\n    y_pred = clf.predict(X)\n    assert_array_equal(y_pred, np.ones(X.shape[0]))\n", "def test_iforest_on_high_dimensional_data():\n    \"\"\"Test Isolation Forest performs well on high dimensional data\"\"\"\n    rng = check_random_state(42)\n    n_samples = 100\n    n_features = 1000\n    X_train = rng.randn(n_samples, n_features)\n    X_test = rng.randn(n_samples, n_features)\n\n    # Generate some abnormal novel observations\n    X_outliers = rng.uniform(low=-4, high=4, size=(20, n_features))\n    X_test = np.r_[X_test, X_outliers]\n    y_test = np.array([0] * n_samples + [1] * 20)\n\n    # fit the model\n    clf = IsolationForest(max_samples=100, random_state=rng).fit(X_train)\n\n    # predict scores (the lower, the more normal)\n    y_pred = - clf.decision_function(X_test)\n\n    # check that there is at most 6 errors (false positive or false negative)\n    assert_greater(roc_auc_score(y_test, y_pred), 0.98)\n", "def test_iforest_prediction_on_empty_input():\n    \"\"\"Test that predict and decision_function raise ValueError on empty input.\"\"\"\n    clf = IsolationForest(random_state=rng).fit(iris.data)\n    assert_raises(ValueError, clf.predict, np.empty((0, iris.data.shape[1])))\n    assert_raises(ValueError, clf.decision_function, np.empty((0, iris.data.shape[1])))\n", "def test_iforest_max_depth():\n    \"\"\"Test that max_depth is recalculated correctly\"\"\"\n    X = iris.data\n    for max_samples in [10, 100, 150]:\n        clf = IsolationForest(max_samples=max_samples, random_state=rng).fit(X)\n        expected_max_depth = int(np.ceil(np.log2(clf.max_samples_)))\n        for est in clf.estimators_:\n            assert_equal(est.max_depth, expected_max_depth)\n\n    clf = IsolationForest(random_state=rng).fit(X)\n    expected_max_depth = int(np.ceil(np.log2(clf.max_samples_)))\n    for est in clf.estimators_:\n        assert_equal(est.max_depth, expected_max_depth)\n"], "sample_899": ["def test_check_estimator_pairwise_metric():\n    # check that check_estimator() works on estimator with _pairwise\n    # metric\n\n    class Estimator(BaseEstimator):\n            self.metric = metric\n\n            return self\n\n            return X\n\n    # test precomputed metric\n    est = Estimator(metric='precomputed')\n    check_estimator(est)\n", "def test_check_estimator_check_class_weight_classifiers():\n    # Check class_weight check for LinearSVC with class_weight='auto' and 'balanced'\n    est = LinearSVC(class_weight='auto')\n    check_class_weight_classifiers('LinearSVC', est)\n\n    # Check class_weight check for LinearSVC with class_weight='balanced'\n    est = LinearSVC(class_weight='balanced')\n    check_class_weight_classifiers('LinearSVC', est)\n", "def test_check_estimatorSparseDataInconsistent():\n    # non-regression test for estimators handling of sparse data\n    class EstimatorWithInconsistentSparseData(BaseEstimator):\n            if sp.issparse(X):\n                if X.shape[0] != 5:\n                    raise ValueError(\"Sparse data should have 5 samples\")\n            else:\n                if X.shape[0] != 10:\n                    raise ValueError(\"Dense data should have 10 samples\")\n\n            return self\n\n            return np.ones(X.shape[0])\n\n    assert_raises_regex(AssertionError,\n                        r\"Estimator doesn't seem to fail gracefully on sparse \"\n                        \"data: it should raise a TypeError if sparse input \"\n                        \"is explicitly not supported.\",\n                        check_estimator, EstimatorWithInconsistentSparseData)\n", "def test_check_estimator_partial_fit_n_features():\n    # test that check_estimators_partial_fit_n_features test case is working\n    # correctly by checking a classifier that implements partial_fit\n\n    class ClassifierWithPartialFit(BaseEstimator):\n            self.partial_fit_called_ = False\n            self.X_ = None\n\n            self.X_ = X\n            return self\n\n            self.partial_fit_called_ = True\n            # this should not be called if number of features has changed\n            if self.X_ is not None and X.shape[1] != self.X_.shape[1]:\n                raise ValueError(\"Number of features has changed\")\n\n    est = ClassifierWithPartialFit()\n    check_estimators_partial_fit_n_features(\"estimator\", est)\n\n    class ClassifierWithoutPartialFit(BaseEstimator):\n            return self\n\n    assert_raises_regex(AttributeError, \".*partial_fit.*\",\n                        check_estimators_partial_fit_n_features,\n                        \"estimator\", ClassifierWithoutPartialFit())\n\n    class ClassifierWithPartialFitButRaisesNotImplementedError(BaseEstimator):\n            return self\n\n            raise NotImplementedError\n\n    assert_raises_regex(AssertionError, \".*partial_fit.*\",\n                        check_estimators_partial_fit_n_features,\n                        \"estimator\",\n                        ClassifierWithPartialFitButRaisesNotImplementedError())\n", "def test_check_estimators_sparse_data():\n    # test that check_estimators_sparse_data works for estimators\n    # that are not transformers\n    class DenseEstimator(BaseEstimator):\n            X = check_array(X, accept_sparse=False)\n            return self\n\n            X = check_array(X, accept_sparse=False)\n            return np.ones(X.shape[0])\n\n    class SparseEstimator(BaseEstimator):\n            X = check_array(X, accept_sparse=True)\n            return self\n\n            X = check_array(X, accept_sparse=True)\n            return np.ones(X.shape[0])\n\n    class UnsupportingSparseEstimator(BaseEstimator):\n            X = check_array(X, accept_sparse=False)\n            return self\n\n            X = check_array(X, accept_sparse=False)\n            return np.ones(X.shape[0])\n\n            X = check_array(X, accept_sparse=False)\n            return X\n\n    # Estimator that is not a transformer, but accepts sparse input\n    # should pass\n    check_estimators_sparse_data('SparseEstimator', SparseEstimator())\n\n    # Estimator that is not a transformer, but does not accept sparse input\n    # should pass\n    check_estimators_sparse_data('DenseEstimator', DenseEstimator())\n\n    # Estimator that is a transformer but does not support sparse data\n    # should pass if it raises an error\n    check_estimators_sparse_data('UnsupportingSparseEstimator',\n                                UnsupportingSparseEstimator())\n\n    # Estimator that is a transformer but does not support sparse data\n    # should fail if it does not raise an error\n    class UnsupportingSparseEstimatorNoError(BaseEstimator):\n            X = check_array(X, accept_sparse=False)\n            return self\n\n            # does not check X\n            return X\n\n    msg = (\"Estimator UnsupportingSparseEstimatorNoError doesn't seem to \"\n           \"fail gracefully on sparse data\")\n    assert_raises_regex(AssertionError, msg,\n                        check_estimators_sparse_data,\n                        'UnsupportingSparseEstimatorNoError',\n                        UnsupportingSparseEstimatorNoError())\n"], "sample_7": ["def test_masked_column_insert_masked_value():\n    c = table.MaskedColumn([1, 2, 3])\n    c2 = c.insert(1, np.ma.masked)\n    assert np.all(c2 == [1, np.ma.masked, 2, 3])\n    assert np.all(c2.mask == [False, True, False, False])\n    assert c2.dtype == 'O'\n", "def test_column_repr_with_unit():\n    c = table.Column([1, 2], name='a', dtype=float, unit='m')\n    assert repr(c).startswith('<Column name=\"a\" dtype=\"float64\" unit=\"m\"')\n", "def test_column_copy_indices():\n    \"\"\"\n    Test copy_indices argument in Column copy() method.\n    \"\"\"\n    c1 = table.Column(data=[1, 2], name='a')\n    c1.indices.append(object())\n    c1_copy = c1.copy(copy_indices=True)\n    assert c1.indices is c1_copy.indices\n    c1_copy = c1.copy(copy_indices=False)\n    assert c1.indices is not c1_copy.indices\n", "def test_column_insert_masked_non_bool():\n    \"\"\"Test that inserting non-boolean values into the mask of a MaskedColumn raises a TypeError\"\"\"\n    c = table.MaskedColumn([1, 2, 3], mask=[False, False, False])\n\n    with pytest.raises(TypeError, match='mask must be a boolean array'):\n        c.insert(1, [100, 200, 300], mask=0)\n\n    with pytest.raises(TypeError, match='mask must be a boolean array'):\n        c.insert(1, [100, 200, 300], mask='a')\n", "def test_quantity_conversion_edge_cases():\n    \"\"\"\n    Test cases that were added to handle some edge cases in quantity conversion.\n\n    Test cases added to handle when column has no unit, or when the\n    input is already a quantity.\n    \"\"\"\n\n    c1 = table.Column([1, 2, 3])\n    assert c1.quantity == table.Column([1, 2, 3], unit='')\n\n    c2 = table.Column([1, 2, 3], unit='m')\n    q2 = c2.quantity\n    assert c2.quantity == q2\n\n    c3 = table.Column([1, 2, 3], unit='m')\n    assert c3.quantity == u.Quantity([1, 2, 3], unit='m')\n"], "sample_949": ["def test_custom_man_pages(app, status, warning):\n    app.builder.build_all()\n    assert (app.outdir / 'customname.2').exists()\n\n    content = (app.outdir / 'customname.2').read_text()\n    assert 'Custom Title' in content\n    assert 'Custom Author' in content\n", "def test_empty_man_pages(app, status, warning):\n    with pytest.warns(UserWarning, match='no \"man_pages\" config value found; no manual pages will be written'):\n        app.build()\n    assert not any((app.outdir / file).exists() for file in ['python.1', 'man1/python.1'])\n", "def test_empty_man_pages_config(app, status, warning):\n    app.config.man_pages = []\n    app.builder.build_all()\n    assert not any((app.outdir / f).exists() for f in app.outdir.iterdir())\n\n    # test that no error is raised when man_pages config is empty\n    assert not status.getvalue().strip()\n", "def test_default_man_pages_empty_author():\n    config = Config({'project': 'STASI\u2122 Documentation',\n                     'author': \"\",\n                     'release': '1.0'})\n    config.init_values()\n    expected = [('index', 'stasi', 'STASI\u2122 Documentation 1.0', [], 1)]\n    assert default_man_pages(config) == expected\n\n", "def test_get_outdated_docs(app):\n    builder = app.builder\n    assert builder.get_outdated_docs() == 'all manpages'\n\n"], "sample_866": ["def test_affinity_propagation_empty_input():\n    # Test AffinityPropagation with empty input\n    with pytest.raises(ValueError):\n        AffinityPropagation().fit(np.array([]))\n", "def test_affinity_propagation_input_validation():\n    # Test input validation of the affinity_propagation function\n    with pytest.raises(ValueError):\n        affinity_propagation(np.array([[1, 2, 3], [4, 5, 6]]))\n\n    with pytest.raises(ValueError):\n        affinity_propagation(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]),\n                            damping=1.5)\n\n    with pytest.raises(ValueError):\n        affinity_propagation(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]),\n                            damping=0.4)\n\n    with pytest.raises(ValueError):\n        affinity_propagation(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]),\n                            convergence_iter=0)\n\n    with pytest.raises(ValueError):\n        affinity_propagation(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]),\n                            max_iter=0)\n", "def test_affinity_propagation_preference_array():\n    # Test that preference array is broadcasted correctly\n    X = np.array([[0, 0], [1, 1], [-2, -2]])\n    preference = np.array([10])\n    af = AffinityPropagation(preference=preference)\n    labels = af.fit(X).labels_\n    assert_array_equal(labels, af.fit(X).labels_)\n\n    # Test that preference array with shape (n_samples,) is handled correctly\n    preference = np.array([10, 20, 30])\n    af = AffinityPropagation(preference=preference)\n    labels = af.fit(X).labels_\n    assert_array_equal(labels, af.fit(X).labels_)\n\n    # Test that preference array with shape (n_samples, 1) is handled correctly\n    preference = np.array([[10], [20], [30]])\n    af = AffinityPropagation(preference=preference)\n    labels = af.fit(X).labels_\n    assert_array_equal(labels, af.fit(X).labels_)\n", "def test_affinity_propagation_copy_data():\n    # Test if the copy parameter of the fit method is used\n    X = np.array([[0, 0], [1, 1], [-2, -2]])\n    S = -euclidean_distances(X, squared=True)\n    af_copy = AffinityPropagation()\n    af_nocopy = AffinityPropagation(copy=False)\n    af_copy.fit(S)\n    af_nocopy.fit(S)\n    assert_array_equal(af_copy.affinity_matrix_, S)\n    assert not np.may_share_memory(S, af_nocopy.affinity_matrix_)\n", "def test_affinity_propagation_input_validations():\n    # Test input validation for damping\n    with pytest.raises(ValueError):\n        AffinityPropagation(damping=0.4)\n\n    with pytest.raises(ValueError):\n        AffinityPropagation(damping=1)\n\n    with pytest.raises(ValueError):\n        AffinityPropagation(damping=1.1)\n\n    # Test input validation for max_iter\n    with pytest.raises(ValueError):\n        AffinityPropagation(max_iter=0)\n\n    # Test input validation for convergence_iter\n    with pytest.raises(ValueError):\n        AffinityPropagation(convergence_iter=0)\n\n    # Test input validation for preference\n    with pytest.raises(ValueError):\n        AffinityPropagation(preference=np.array([1, 2]))\n\n    with pytest.raises(ValueError):\n        AffinityPropagation(preference=[1, 2])\n\n    # Test input validation for affinity\n    with pytest.raises(ValueError):\n        AffinityPropagation(affinity=\"other\")\n\n    # Test input validation for copy\n    with pytest.raises(ValueError):\n        AffinityPropagation(copy=\"yes\")\n\n    # Test input validation for verbose\n    with pytest.raises(ValueError):\n        AffinityPropagation(verbose=\"yes\")\n\n    # Test input validation for n_iter_\n    with pytest.raises(ValueError):\n        af = AffinityPropagation()\n        af.n_iter_ = \"1\"\n"], "sample_461": ["    def test_urlfield_clean_ipv6(self):\n        f = URLField()\n        tests = [\n            (\"http://[::1]\", \"http://[::1]\"),\n            (\"http://[::1]/\", \"http://[::1]/\"),\n            (\"http://[::1]:8080\", \"http://[::1]:8080\"),\n            (\"http://[::1]:8080/\", \"http://[::1]:8080/\"),\n            (\"http://[2001:0db8:85a3:0000:0000:8a2e:0370:7334]\", \"http://[2001:db8:85a3::8a2e:370:7334]\"),\n            (\"http://[2001:0db8:85a3:0000:0000:8a2e:0370:7334]/\", \"http://[2001:db8:85a3::8a2e:370:7334]/\"),\n            (\"http://[2001:0db8:85a3:0000:0000:8a2e:0370:7334]:8080\", \"http://[2001:db8:85a3::8a2e:370:7334]:8080\"),\n            (\"http://[2001:0db8:85a3:0000:0000:8a2e:0370:7334]:8080/\", \"http://[2001:db8:85a3::8a2e:370:7334]:8080/\"),\n        ]\n        for url, expected in tests:\n            with self.subTest(url=url):\n                self.assertEqual(f.clean(url), expected)\n", "    def test_uuidfield_widget(self):\n        f = URLField()\n        self.assertWidgetRendersTo(f, '<input type=\"text\" name=\"f\" id=\"id_f\" required>')\n", "    def test_generic_ip_address_field_widget(self):\n        f = GenericIPAddressField()\n        self.assertWidgetRendersTo(f, '<input type=\"text\" name=\"f\" id=\"id_f\" required>')\n", "    def test_booleanfield_widget(self):\n        f = BooleanField()\n        self.assertWidgetRendersTo(f, '<input type=\"checkbox\" name=\"f\" id=\"id_f\">')\n", "    def test_jsonfield_clean(self):\n        f = JSONField()\n        tests = [\n            (None, None),\n            (\"null\", None),\n            (\"true\", True),\n            (\"false\", False),\n            (\"\\\"hello\\\"\", \"hello\"),\n            (\"1\", 1),\n            (\"1.5\", 1.5),\n            (\"[]\", []),\n            (\"{}\", {}),\n            (\"[1, 2, 3]\", [1, 2, 3]),\n            (\"{\\\"a\\\": 1, \\\"b\\\": 2}\", {\"a\": 1, \"b\": 2}),\n        ]\n        for value, expected in tests:\n            with self.subTest(value=value):\n                self.assertEqual(f.clean(value), expected)\n"], "sample_324": ["def test_csrf_token_in_header_with_customized_name_and_origin(self):\n    \"\"\"\n    The token may be passed in a header instead of in the form when\n    settings.CSRF_HEADER_NAME is customized and HTTP_ORIGIN is set.\n    \"\"\"\n    @override_settings(CSRF_HEADER_NAME='HTTP_X_CSRFTOKEN_CUSTOMIZED', CSRF_TRUSTED_ORIGINS=['https://dashboard.example.com'])\n        req = self._get_POST_csrf_cookie_request(\n            meta_token=self._csrf_id,\n            token_header='HTTP_X_CSRFTOKEN_CUSTOMIZED',\n        )\n        req.META['HTTP_ORIGIN'] = 'https://dashboard.example.com'\n        mw = CsrfViewMiddleware(post_form_view)\n        mw.process_request(req)\n        resp = mw.process_view(req, post_form_view, (), {})\n        self.assertIsNone(resp)\n    inner_test(self)\n", "def test_csrf_view_middleware_rotated_token(self):\n    \"\"\"\n    Test that rotating the token updates the session or cookie.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    mw = CsrfViewMiddleware(post_form_view)\n    mw.process_request(req)\n    mw.process_view(req, post_form_view, (), {})\n    original_token = req.META[\"CSRF_COOKIE\"]\n    rotate_token(req)\n    mw = CsrfViewMiddleware(post_form_view)\n    mw.process_request(req)\n    mw.process_view(req, post_form_view, (), {})\n    new_token = req.META[\"CSRF_COOKIE\"]\n    self.assertNotEqual(original_token, new_token)\n", "    def test_csrf_header_name_customization(self):\n        \"\"\"\n        Ensure that CsrfViewMiddleware can be configured with a custom\n        CSRF_HEADER_NAME.\n        \"\"\"\n        class CsrfPostRequest(HttpRequest):\n                super().__init__()\n                self.method = 'POST'\n                self.COOKIES[settings.CSRF_COOKIE_NAME] = self._csrf_id_cookie\n                self.session = SessionStore()\n                self.session[CSRF_SESSION_KEY] = self._csrf_id_cookie\n\n        req = CsrfPostRequest()\n        req.META['HTTP_X_CSRFTOKEN_CUSTOMIZED'] = self._csrf_id\n        mw = CsrfViewMiddleware(post_form_view)\n        mw.process_request(req)\n        resp = mw.process_view(req, post_form_view, (), {})\n        self.assertIsNone(resp)\n\n        req = CsrfPostRequest()\n        req.META['HTTP_X_CSRFTOKEN'] = self._csrf_id\n        mw = CsrfViewMiddleware(post_form_view)\n        with self.assertLogs('django.security.csrf', 'WARNING') as cm:\n            resp = mw.process_view(req, post_form_view, (), {})\n        self.assertEqual(resp.status_code, 403)\n        self.assertEqual(cm.records[0].getMessage(), 'Forbidden (%s): ' % REASON_CSRF_TOKEN_MISSING)\n", "def test_secure_and_httponly_flags(self):\n    \"\"\"\n    Ensure the CSRF cookie's secure and httponly flags are set correctly.\n    \"\"\"\n    req = self._get_GET_no_csrf_cookie_request()\n    mw = CsrfViewMiddleware(token_view)\n    mw.process_view(req, token_view, (), {})\n    resp = mw(req)\n    csrf_cookie = resp.cookies.get(settings.CSRF_COOKIE_NAME, False)\n    self.assertFalse(csrf_cookie.get('secure', False))\n    self.assertFalse(csrf_cookie.get('httponly', False))\n", "def test_csrf_cookie_used_when_csrf_use_sessions_is_true(self):\n    \"\"\"\n    When CSRF_USE_SESSIONS=True, the CSRF token is always used from the session.\n    \"\"\"\n    req = self._get_GET_no_csrf_cookie_request()\n    mw = CsrfViewMiddleware(token_view)\n    mw.process_request(req)\n    req.session[CSRF_SESSION_KEY] = _get_new_csrf_string()\n    mw.process_view(req, token_view, (), {})\n    resp = mw(req)\n    self.assertEqual(len(req.session[CSRF_SESSION_KEY]), CSRF_TOKEN_LENGTH)\n    self.assertEqual(len(resp.cookies.get(settings.CSRF_COOKIE_NAME, {}).get('value')), CSRF_TOKEN_LENGTH)\n"], "sample_1084": ["def test_intersection_sets():\n    assert intersection_sets(S.Integers, S.Naturals) == S.Naturals\n    assert intersection_sets(S.Naturals, S.Integers) == S.Naturals\n    assert intersection_sets(S.Integers, S.Naturals0) == S.Naturals0\n    assert intersection_sets(S.Naturals0, S.Integers) == S.Naturals0\n    assert intersection_sets(S.Naturals, S.Naturals0) == S.Naturals0\n    assert intersection_sets(S.Naturals0, S.Naturals) == S.Naturals0\n    assert intersection_sets(S.Rationals, S.Naturals) == S.Naturals\n    assert intersection_sets(S.Naturals, S.Rationals) == S.Naturals\n    assert intersection_sets(S.Rationals, S.Integers) == S.Integers\n    assert intersection_sets(S.Integers, S.Rationals) == S.Integers\n    assert intersection_sets(S.Naturals0, S.Rationals) == S.Naturals0\n    assert intersection_sets(S.Rationals, S.Naturals0) == S.Naturals0\n", "def test_product_set_intersection():\n    interval1 = Interval(1, 5)\n    interval2 = Interval(2, 6)\n    interval3 = Interval(3, 4)\n    interval4 = Interval(2, 7)\n\n    product_set1 = ProductSet(interval1, interval2)\n    product_set2 = ProductSet(interval3, interval4)\n\n    assert product_set1.intersect(product_set2) == ProductSet(Intersection(interval1, interval3), Intersection(interval2, interval4))\n\n    interval5 = Interval(6, 8)\n    interval6 = Interval(3, 5)\n\n    product_set3 = ProductSet(interval5, interval6)\n    assert product_set1.intersect(product_set3) == S.EmptySet\n", "def test_intersection_sets():\n    # Test intersection between ConditionSet and ConditionSet\n    x = symbols('x', real=True)\n    cond1 = ConditionSet(x, Eq(x**2, 1), S.Reals)\n    cond2 = ConditionSet(x, Eq(x**2, 4), S.Reals)\n    assert intersection_sets(cond1, cond2) is None\n\n    # Test intersection between ConditionSet and Set\n    cond = ConditionSet(x, Eq(x**2, 1), S.Reals)\n    set_ = Interval(0, 1)\n    assert intersection_sets(cond, set_) == ConditionSet(x, And(Eq(x**2, 1), (x >= 0) & (x <= 1)), S.Reals)\n\n    # Test intersection between Naturals and Integers\n    assert intersection_sets(S.Naturals, S.Integers) == S.Naturals\n\n    # Test intersection between Naturals and Naturals\n    assert intersection_sets(S.Naturals, S.Naturals) == S.Naturals\n\n    # Test intersection between Interval and Naturals\n    interval = Interval(1, 10)\n    assert intersection_sets(interval, S.Naturals) == Range(1, 11)\n\n    # Test intersection between ComplexRegion and Set\n    region = ComplexRegion(Interval(0, 1)*Interval(0, 2*S.Pi), polar=True)\n    set_ = Interval(0, 1)\n    assert intersection_sets(region, set_) == Interval(0, 1)\n\n    # Test intersection between Integers and Reals\n    assert intersection_sets(S.Integers, S.Reals) == S.Integers\n\n    # Test intersection between Range and Interval\n    range_ = Range(1, 10)\n    interval = Interval(5, 15)\n    assert intersection_sets(range_, interval) == Range(5, 10)\n\n    # Test intersection between Range and Naturals\n    assert intersection_sets(range_, S.Naturals) == range_\n\n    # Test intersection between Range and Range\n    range1 = Range(1, 10, 2)\n    range2 = Range(5, 15, 3)\n    assert intersection_sets(range1, range2) == Range(5, 10, 6)\n\n    # Test intersection between Range and Integers\n    assert intersection_sets(range1, S.Integers) == range1\n\n    # Test intersection between Image", "def test_intersection_sets():\n    # Test intersection of ConditionSet with different sets\n    x = symbols('x', real=True)\n    cond_set1 = ConditionSet(x, x > 0, S.Reals)\n    cond_set2 = ConditionSet(x, x < 0, S.Reals)\n\n    assert intersection_sets(cond_set1, S.Reals) == cond_set1\n    assert intersection_sets(cond_set1, S.Naturals) == Range(1, S.Infinity)\n    assert intersection_sets(cond_set1, S.Integers) == Range(1, S.Infinity)\n    assert intersection_sets(cond_set1, Interval(0, 1)) == Interval.Ropen(0, 1)\n    assert intersection_sets(cond_set1, Interval(-1, 1)) == Interval.Ropen(0, 1)\n\n    assert intersection_sets(cond_set2, S.Reals) == cond_set2\n    assert intersection_sets(cond_set2, S.Naturals) == S.EmptySet\n    assert intersection_sets(cond_set2, S.Integers) == Range(-S.Infinity, -1)\n    assert intersection_sets(cond_set2, Interval(-1, 0)) == Interval.Lopen(-1, 0)\n    assert intersection_sets(cond_set2, Interval(-1, 1)) == Interval.Lopen(-1, 0)\n\n    # Test intersection of different sets with ImageSet\n    lamda = Lambda(x, x**2)\n    img_set = ImageSet(lamda, S.Naturals)\n\n    assert intersection_sets(img_set, S.Reals) == img_set\n    assert intersection_sets(img_set, S.Naturals) == img_set\n    assert intersection_sets(img_set, S.Integers) == img_set\n    assert intersection_sets(img_set, Interval(0, 10)) == FiniteSet(1, 4, 9)\n    assert intersection_sets(img_set, Interval(1, 10)) == FiniteSet(1, 4, 9)\n\n    # Test intersection of ProductSet with different sets\n    set1 = ProductSet(S.Naturals, S.Naturals)\n    set2 = ProductSet(S.Integers, S.Integers)\n\n    assert intersection_sets(set1, S.Reals*S.Reals) == set1\n    assert intersection_sets(set1, S.Naturals*S.Naturals) == set1\n    assert intersection_sets(set1, S.Integers*S.Integers) == set", "def test_product_intersection():\n    a = Interval(1, 2)\n    b = Interval(2, 3)\n    c = Interval(3, 4)\n    d = Interval(4, 5)\n    e = Interval(5, 6)\n\n    assert ProductSet(a, b).intersect(ProductSet(c, d)) == S.EmptySet\n    assert ProductSet(a, b).intersect(ProductSet(a, b)) == ProductSet(a, b)\n    assert ProductSet(a, b).intersect(ProductSet(c, c)) == S.EmptySet\n    assert ProductSet(a, b).intersect(ProductSet(a, c)) == ProductSet(a, b.intersect(c))\n    assert ProductSet(a, b).intersect(ProductSet(c, b)) == ProductSet(a.intersect(c), b)\n    assert ProductSet(a, b).intersect(ProductSet(a, b).intersect(ProductSet(c, d))) == S.EmptySet\n    assert ProductSet(a, b).intersect(ProductSet(a, b).intersect(ProductSet(a, c))) == ProductSet(a, b.intersect(c))\n    assert ProductSet(a, b).intersect(ProductSet(c, d).intersect(ProductSet(a, b))) == S.EmptySet\n    assert ProductSet(a, b).intersect(ProductSet(c, d).intersect(ProductSet(a, c))) == S.EmptySet\n    assert ProductSet(a, b).intersect(ProductSet(c, d).intersect(ProductSet(c, b))) == S.EmptySet\n    assert ProductSet(a, b).intersect(ProductSet(a, c).intersect(ProductSet(c, d))) == ProductSet(a.intersect(c), b.intersect(d))\n    assert ProductSet(a, b).intersect(ProductSet(a, c).intersect(ProductSet(c, b))) == ProductSet(a, b.intersect(c))\n    assert ProductSet(a, b).intersect(ProductSet(c, b).intersect(ProductSet(a, c))) == ProductSet(a.intersect(c), b)\n    assert ProductSet(a, b).intersect(ProductSet(c, b).intersect(ProductSet(c, d))) == S.EmptySet\n    assert ProductSet(a, b).intersect(ProductSet(a, b).intersect(ProductSet(c, d)).intersect(ProductSet(e, e))) == S.EmptySet\n    assert ProductSet(a, b).intersect(ProductSet(a, b).intersect(ProductSet(a, c)).intersect(ProductSet(e, e))) == ProductSet(a, b.intersect(c))\n    assert Product"], "sample_549": ["def test_get_nonzero_slices():\n    assert cbook._get_nonzero_slices(np.array([[1, 2], [0, 0]])) == (slice(0, 1), slice(0, 2))\n    assert cbook._get_nonzero_slices(np.array([[0, 0], [1, 2]])) == (slice(1, 2), slice(0, 2))\n    assert cbook._get_nonzero_slices(np.array([[1, 2], [1, 2]])) == (slice(0, 2), slice(0, 2))\n    assert cbook._get_nonzero_slices(np.array([[0, 0], [0, 0]])) == (slice(0, 0), slice(0, 0))\n", "def test_safe_masked_invalid():\n    a = np.array([1, 2, 3, np.nan, np.nan, 6])\n    am = np.ma.array([1, 2, 3, np.nan, np.nan, 6], mask=[False, False, True, True, False, False])\n    assert np.array_equal(cbook.safe_masked_invalid(a), a)\n    assert np.array_equal(cbook.safe_masked_invalid(am), np.ma.array([1, 2, np.nan, np.nan, np.nan, 6], mask=[False, False, True, True, True, False]))\n    assert np.array_equal(cbook.safe_masked_invalid(a, copy=False), a)\n    assert np.array_equal(cbook.safe_masked_invalid(am, copy=False), np.ma.array([1, 2, np.nan, np.nan, np.nan, 6], mask=[False, False, True, True, True, False]))\n", "def test_safe_masked_invalid():\n    a = np.array([1, 2, 3])\n    b = np.ma.masked_greater(a, 2)\n    c = cbook.safe_masked_invalid(b)\n    assert_array_equal(c, b)\n\n    a = np.array([1, 2, np.nan])\n    c = cbook.safe_masked_invalid(a)\n    assert_array_equal(c, np.ma.masked_invalid(a))\n\n    a = np.array([1, 2, np.inf])\n    c = cbook.safe_masked_invalid(a)\n    assert_array_equal(c, np.ma.masked_invalid(a))\n\n    a = np.array([1, 2, np.nan + np.inf])\n    c = cbook.safe_masked_invalid(a)\n    assert_array_equal(c, np.ma.masked_invalid(a))\n\n    a = np.array([1, 2, 3], dtype=np.uint8)\n    c = cbook.safe_masked_invalid(a)\n    assert_array_equal(c, a)\n", "def test_safe_masked_invalid():\n    a = np.ma.array([1, 2, 3, np.nan, np.nan, 6],\n                    mask=[False, False, True, True, False, False])\n    actual = cbook.safe_masked_invalid(a, copy=False)\n    assert_array_equal(actual.data, a.data)\n    assert_array_equal(actual.mask, a.mask)\n    assert actual.flags.writeable\n\n    actual = cbook.safe_masked_invalid(a, copy=True)\n    assert_array_equal(actual.data, a.data)\n    assert_array_equal(actual.mask, a.mask)\n    assert actual.flags.writeable\n\n    actual = cbook.safe_masked_invalid(np.nan, copy=False)\n    assert np.isnan(actual)\n\n    actual = cbook.safe_masked_invalid(np.array([1, np.nan]), copy=True)\n    assert_array_equal(actual, np.array([1, np.nan]))\n", "def test_violin_stats():\n    np.random.seed(0)\n    data = np.random.normal(size=(10, 4))\n    n_repeats = 100\n        return np.repeat(np.mean(data), len(coords))\n    stats = cbook.violin_stats(data, kernel, n_repeats)\n    assert len(stats) == 4\n    assert all(isinstance(s, dict) for s in stats)\n    for key in ['coords', 'vals', 'mean', 'median', 'min', 'max', 'quantiles']:\n        assert all(key in s for s in stats)\n    assert all(len(s['coords']) == len(s['vals']) for s in stats)\n    assert all(s['mean'] == np.mean(data[:, i]) for i, s in enumerate(stats))\n    assert all(s['median'] == np.median(data[:, i]) for i, s in enumerate(stats))\n    assert all(s['min'] == np.min(data[:, i]) for i, s in enumerate(stats))\n    assert all(s['max'] == np.max(data[:, i]) for i, s in enumerate(stats))\n    assert all(s['quantiles'] == np.percentile(data[:, i], [25, 50, 75]) for i, s in enumerate(stats))\n"], "sample_685": ["def test_at_level_with_multiple_loggers(caplog):\n    logger1 = logging.getLogger(\"logger1\")\n    logger2 = logging.getLogger(\"logger2\")\n\n    with caplog.at_level(logging.INFO, logger=\"logger1\"):\n        logger1.info(\"logger1 INFO message\")\n        logger2.info(\"logger2 INFO message\")\n        assert \"logger1 INFO message\" in caplog.text\n        assert \"logger2 INFO message\" not in caplog.text\n\n    with caplog.at_level(logging.INFO, logger=\"logger2\"):\n        logger1.info(\"logger1 INFO message\")\n        logger2.info(\"logger2 INFO message\")\n        assert \"logger1 INFO message\" not in caplog.text\n        assert \"logger2 INFO message\" in caplog.text\n\n    with caplog.at_level(logging.INFO):\n        logger1.info(\"logger1 INFO message\")\n        logger2.info(\"logger2 INFO message\")\n        assert \"logger1 INFO message\" in caplog.text\n        assert \"logger2 INFO message\" in caplog.text\n", "def test_log_messages_multiline(caplog):\n    caplog.set_level(logging.INFO)\n    logger.info(\"boo %s\\nbaz %s\", \"arg1\", \"arg2\")\n    logger.info(\"foo\\nbar\\nbaz %s\", \"arg3\")\n\n    assert \"boo arg1\\nbaz arg2\" == caplog.messages[0]\n    assert \"foo\\nbar\\nbaz arg3\" == caplog.messages[1]\n    assert len(caplog.text.splitlines()) > len(caplog.messages)\n", "def test_get_records_for_all_stages(caplog, logging_during_setup_and_teardown):\n    assert [x.message for x in caplog.get_records(\"setup\")] == [\"a_setup_log\"]\n    logger.info(\"a_call_log\")\n    assert [x.message for x in caplog.get_records(\"call\")] == [\"a_call_log\"]\n    assert not caplog.get_records(\"teardown\")\n\n    logger.info(\"another_call_log\")\n    logger.info(\"a_teardown_log\")\n    assert [x.message for x in caplog.get_records(\"call\")] == [\"a_call_log\", \"another_call_log\"]\n    assert [x.message for x in caplog.get_records(\"teardown\")] == [\"a_teardown_log\"]\n", "def test_caplog_respects_log_cli_level_option(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_cli_level == logging.INFO\n            logger = logging.getLogger('catchlog')\n            logger.debug(\"DEBUG message won't be shown on CLI\")\n            logger.info(\"INFO message will be shown on CLI\")\n            logger.warning(\"WARNING message will be shown on CLI\")\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--log-cli-level=INFO\")\n    result.stdout.fnmatch_lines(\n        [\"*INFO message will be shown on CLI*\", \"*WARNING message will be shown on CLI*\"]\n    )\n    result.stdout.no_fnmatch_line(\"*DEBUG message won't be shown on CLI*\")\n    assert result.ret == 0\n", "def test_log_auto_indent(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import pytest\n\n            logger = logging.getLogger('log-auto-indent')\n            caplog.set_level(logging.INFO)\n\n            long_message = (\"This is a very long message that should be \"\n                            \"auto-indented because it has a newline.\")\n            logger.info(long_message)\n\n            assert \"This is a very long message that should be\" in caplog.text\n            assert \"  auto-indented because it has a newline.\" in caplog.text\n\n            caplog.set_level(logging.INFO, logger='log-auto-indent')\n            logger.info(long_message)\n            assert \"This is a very long message that should be\" in caplog.text\n            assert \"  auto-indented because it has a newline.\" in caplog.text\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_auto_indent=True\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    assert result.ret == 0\n"], "sample_332": ["def test_html_safe_property(self):\n    formset = self.make_choiceformset()\n    self.assertIsInstance(formset.__html__(), str)\n    self.assertEqual(str(formset), formset.__html__())\n", "def test_management_form_absolute_max(self):\n    ChoiceFormSet = formset_factory(Choice)\n    data = {\n        'choices-TOTAL_FORMS': '1001',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '0',\n        'choices-MAX_NUM_FORMS': '0',\n    }\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.non_form_errors(), ['Please submit at most 1000 forms.'])\n    self.assertEqual(\n        str(formset.non_form_errors()),\n        '<ul class=\"errorlist nonform\"><li>Please submit at most 1000 forms.</li></ul>',\n    )\n", "def test_absolute_max_zero(self):\n    \"\"\"\n    If absolute_max is 0 then no form is rendered at all, regardless of extra,\n    unless initial data is present.\n    \"\"\"\n    data = {\n        'form-TOTAL_FORMS': '2001',\n        'form-INITIAL_FORMS': '0',\n        'form-MAX_NUM_FORMS': '0',\n    }\n    AbsoluteMaxFavoriteDrinksFormSet = formset_factory(\n        FavoriteDrinkForm,\n        absolute_max=0,\n    )\n    formset = AbsoluteMaxFavoriteDrinksFormSet(data=data)\n    self.assertIs(formset.is_valid(), False)\n    self.assertEqual(len(formset.forms), 0)\n    # absolute_max provides a hard limit.\n    data['form-TOTAL_FORMS'] = '1'\n    formset = AbsoluteMaxFavoriteDrinksFormSet(data=data)\n    self.assertIs(formset.is_valid(), False)\n    self.assertEqual(len(formset.forms), 0)\n    self.assertEqual(\n        formset.non_form_errors(),\n        ['Please submit at most 1000 forms.'],\n    )\n", "def test_min_num_with_initial_forms_and_validate_min(self):\n    \"\"\"min_num with initial forms and validate_min.\"\"\"\n    LimitedFavoriteDrinkFormSet = formset_factory(\n        FavoriteDrinkForm, min_num=2, validate_min=True)\n    formset = LimitedFavoriteDrinkFormSet(initial=[{'name': 'Fernet and Coke'}])\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.non_form_errors(), ['Please submit at least 2 forms.'])\n", "def test_formset_empty_permitted(self):\n    \"\"\"empty_permitted attribute is properly set.\"\"\"\n    class CustomChoice(Choice):\n            super().__init__(*args, **kwargs)\n            self.empty_permitted = False\n\n    ChoiceFormSet = formset_factory(CustomChoice)\n    data = {\n        'choices-TOTAL_FORMS': '2',\n        'choices-INITIAL_FORMS': '1',\n        'choices-0-choice': 'Zero',\n        'choices-0-votes': '0',\n    }\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertTrue(formset.forms[0].empty_permitted)  # initial form\n    self.assertFalse(formset.forms[1].empty_permitted)  # extra form\n\n    # Ensure that the empty_permitted attribute on the form is used.\n    class CustomBaseFormSet(BaseFormSet):\n            form = super()._construct_form(i, **kwargs)\n            form.empty_permitted = False\n            return form\n\n    CustomChoiceFormSet = formset_factory(CustomChoice, formset=CustomBaseFormSet)\n    formset = CustomChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertFalse(formset.forms[0].empty_permitted)  # initial form\n    self.assertFalse(formset.forms[1].empty_permitted)  # extra form\n"], "sample_563": ["def test_offsetbox_set_offset():\n    fig, ax = plt.subplots()\n    offsetbox = OffsetBox()\n    offsetbox.set_offset((10, 20))\n    assert offsetbox.get_offset() == (10, 20)\n    offsetbox.set_offset(lambda width, height, xdescent, ydescent, renderer: (30, 40))\n    assert offsetbox.get_offset(Bbox.from_bounds(0, 0, 100, 100), ax.transData) == (30, 40)\n", "def test_drawing_area_add_artist():\n    # Test adding an artist to a DrawingArea\n    fig, ax = plt.subplots()\n    da = DrawingArea(100, 100)\n    text = TextArea(\"test\")\n    da.add_artist(text)\n    ax.add_artist(da)\n    fig.canvas.draw()\n\n    # Check that the text's transform is set correctly\n    assert text.get_transform() == da.get_transform()\n\n    # Check that the text's axes is set correctly\n    assert text.axes == ax\n", "def test_annotationbbox_save_offset():\n    fig, ax = plt.subplots()\n    da = DrawingArea(20, 20, 0, 0, clip=True)\n    ab = AnnotationBbox(da, [.5, .5], xybox=(-0.2, 0.5), xycoords='data',\n                        boxcoords=\"axes fraction\", box_alignment=(0., .5),\n                        arrowprops=dict(arrowstyle=\"->\"))\n    ax.add_artist(ab)\n\n    ab.save_offset()\n    assert ab.offsetbox_x == ab.offsetbox_y == 0\n    ab.update_offset(10, 10)\n    assert ab.offsetbox_x == 10 and ab.offsetbox_y == 10\n    ab.finalize_offset()\n    assert ab.xyann == (0.5, 0.5)\n", "def test_auxtransformbox():\n    # Create a plot with some axes\n    fig, ax = plt.subplots()\n\n    # Create a FancyBboxPatch artist\n    patch = mpatches.FancyBboxPatch((0.1, 0.1), 0.5, 0.5, mutation_scale=10)\n\n    # Create an AuxTransformBox with the patch and a transform\n    aux_transform = mtransforms.Affine2D().rotate_deg(45)\n    aux_box = OffsetBox.aux_transform(aux_transform, child=patch)\n\n    # Add the AuxTransformBox to the axes\n    ax.add_artist(aux_box)\n\n    # Draw the plot\n    fig.canvas.draw()\n\n    # Check that the AuxTransformBox is drawn correctly\n    assert aux_box.get_window_extent().extents == pytest.approx((160, 70, 320, 230))\n", "def test_annotationbbox_text_placement():\n    fig, ax = plt.subplots()\n    da = DrawingArea(50, 30)\n    bg = mpatches.Rectangle((0, 0), 50, 30, facecolor='#CCCCCC',\n                            edgecolor='None', linewidth=0)\n    da.add_artist(bg)\n\n    textprops = {'ha': 'left'}\n    expected_coords = [(0, 0.5), (25, 0.5), (50, 0.5)]\n\n    for i, (loc, (x, y)) in enumerate(zip(['left', 'center', 'right'],\n                                           expected_coords)):\n        ab = AnnotationBbox(da, (0.5, 0.5), xybox=(0.1, 0.1), xycoords='data',\n                            boxcoords=\"axes fraction\", box_alignment=(0, 0.5),\n                            arrowprops=dict(arrowstyle=\"->\"))\n        ab.offsetbox.set_child(TextArea(\"Test\", textprops=textprops))\n        ax.add_artist(ab)\n        ax.draw_artist(ab)\n        # Convert data coordinates to display space\n        display_coords = ab.offsetbox.get_window_extent(ax.figure.canvas.get_renderer())\n        assert_allclose((display_coords.x0, display_coords.y0), (x, y), atol=1)\n"], "sample_568": ["def test_text_3d_properties(fig_test, fig_ref):\n    ax_test = fig_test.add_subplot(projection='3d')\n    t_test = art3d.Text3D(0, 0, 0, \"Test\")\n    t_test.set_3d_properties(1, 'x')\n    ax_test.add_artist(t_test)\n\n    ax_ref = fig_ref.add_subplot(projection='3d')\n    t_ref = art3d.Text3D(0, 0, 1, \"Test\", zdir='x')\n    ax_ref.add_artist(t_ref)\n\n    assert t_test.get_position_3d() == t_ref.get_position_3d()\n", "def test_line3d_set_3d_properties():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    x, y, z = [0, 1], [2, 3], [4, 5]\n    line = art3d.Line3D(x, y, z)\n    line.set_3d_properties(zs=[6, 7], zdir='y')\n    np.testing.assert_array_equal((x, y, [6, 7]), line.get_data_3d())\n    line.set_3d_properties(zs=[8, 9], zdir='x')\n    np.testing.assert_array_equal(([8, 9], y, [6, 7]), line.get_data_3d())\n    line.set_3d_properties(zs=[10, 11], zdir='z')\n    np.testing.assert_array_equal(([8, 9], y, [10, 11]), line.get_data_3d())\n", "def test_patch_collection_2d_to_3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    patch = mpatch.RegularPolygon((0.5, 0.5), numVertices=5,\n                                  radius=0.1, facecolor='r')\n    collection = mpatch.PatchCollection([patch])\n    art3d.patch_collection_2d_to_3d(collection)\n    ax.add_collection3d(collection)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_zlim(0, 1)\n", "def test_collection3d_depthshade(fig_test, fig_ref):\n    fig_test.add_subplot(projection='3d').add_collection3d(\n        art3d.Collection3D([np.array([[0, 0, 0], [1, 0, 0], [0, 1, 0]])],\n                            facecolors=[(1, 0, 0, 1), (0, 1, 0, 1)]),\n        depthshade=False)\n    fig_ref.add_subplot(projection='3d').add_collection3d(\n        art3d.Collection3D([np.array([[0, 0, 0], [1, 0, 0], [0, 1, 0]])],\n                            facecolors=[(1, 0, 0, 1), (0, 1, 0, 1)]),\n        depthshade=True)\n", "def test_collection_2d_to_3d(fig_test, fig_ref):\n    ax_test = fig_test.add_subplot(projection='3d')\n    ax_ref = fig_ref.add_subplot(projection='3d')\n\n    # Test converting a PolyCollection with 3D vertices\n    poly_ref = np.array([\n        [[0, 0, 1], [0, 1, 1], [1, 1, 1], [1, 0, 1]],\n        [[0, 0, 0], [0, 1, 0], [1, 1, 0], [1, 0, 0]]\n    ])\n    c_ref = Poly3DCollection(poly_ref)\n    ax_ref.add_collection3d(c_ref)\n\n    poly_test = np.array([\n        [[0, 0], [0, 1], [1, 1], [1, 0]],\n        [[0, 0], [0, 1], [1, 1], [1, 0]]\n    ])\n    c_test = PolyCollection(poly_test)\n    art3d.collection_2d_to_3d(c_test, zdir='z', zs=1)\n    ax_test.add_collection3d(c_test)\n\n    # Test converting a PolyCollection with 2D vertices\n    poly_ref = np.array([\n        [[0, 0], [0, 1], [1, 1], [1, 0]],\n        [[0, 0], [0, 1], [1, 1], [1, 0]]\n    ])\n    c_ref = Poly3DCollection(poly_ref, zs=0)\n    ax_ref.add_collection3d(c_ref)\n\n    poly_test = np.array([\n        [[0, 0], [0, 1], [1, 1], [1, 0]],\n        [[0, 0], [0, 1], [1, 1], [1, 0]]\n    ])\n    c_test = PolyCollection(poly_test)\n    art3d.collection_2d_to_3d(c_test, zdir='z', zs=0)\n    ax_test.add_collection3d(c_test)\n\n    # Test converting a PolyCollection with no vertices\n    c_ref = Poly3DCollection([])\n    ax_ref.add_collection3d(c_ref)\n\n    c_test = PolyCollection([])\n"], "sample_138": ["    def test_hashed_name(self):\n        with self.assertRaises(NotImplementedError):\n            storage.staticfiles_storage.hashed_name('test.txt')\n", "    def test_manifest_strict_mode_disabled(self):\n        storage.staticfiles_storage.manifest_strict = False\n        relpath = storage.staticfiles_storage.hashed_name(\"nonexistent_file.css\")\n        self.assertIsNotNone(relpath)\n", "    def setUp(self):\n        self.storage = storage.ManifestStaticFilesStorage()\n", "    def test_hash_key_collision(self):\n        \"\"\"\n        Two files with different paths but the same hash key don't overwrite\n        each other (#28828).\n        \"\"\"\n        content = b'file content'\n        first_path = 'test/file.txt'\n        second_path = 'test/subdir/../file.txt'\n        storage.staticfiles_storage.save(first_path, ContentFile(content))\n        storage.staticfiles_storage.save(second_path, ContentFile(content))\n        self.assertEqual(storage.staticfiles_storage.hashed_name(first_path), storage.staticfiles_storage.hashed_name(second_path))\n        first_hashed_path = storage.staticfiles_storage.stored_name(first_path)\n        second_hashed_path = storage.staticfiles_storage.stored_name(second_path)\n        self.assertNotEqual(first_hashed_path, second_hashed_path)\n", "    def setUp(self):\n        self.storage = storage.HashedFilesMixin()\n"], "sample_904": ["def test_option_xref_role(app):\n    text = (\".. program:: ls\\n\"\n            \"\\n\"\n            \".. option:: -l\\n\"\n            \"\\n\"\n            \"The option :option:`-l` is very useful.\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, ([nodes.paragraph, ([pending_xref, nodes.inline, \"The option \"],\n                                             [nodes.inline, \"-l\"],\n                                             [nodes.inline, \" is very useful.\"])]))\n    refnode = app.env.get_domain(\"std\").resolve_xref(app.env, 'index', app.builder, 'option', '-l',\n                                                     pending_xref(), nodes.paragraph())\n    assert_node(refnode, nodes.reference, refid=\"cmdoption-ls-l\")\n", "def test_resolve_xref_with_option_xref_role(app):\n    text = (\".. program:: ls\\n\"\n            \"\\n\"\n            \".. option:: -l\\n\"\n            \"\\n\"\n            \"The :option:`-l` option is very useful.\\n\")\n    domain = app.env.get_domain('std')\n    doctree = restructuredtext.parse(app, text)\n    refnode = doctree.findall(addnodes.pending_xref)[0]\n    resolved_node = domain.resolve_xref(app.env, 'index', app.builder, 'option', '-l', refnode, nodes.paragraph())\n    assert_node(resolved_node, nodes.reference, refid='cmdoption-ls-l')\n", "def test_productionlist_xref(app):\n    text = (\".. productionlist:: P\\n\"\n            \"   A: `A` `~A` `:A` `~:A` `A:` `~A:`\\n\"\n            \"   B: `B` `~B` `:B` `~:B` `B:` `~B:`\\n\")\n    restructuredtext.parse(app, text)\n\n    domain = app.env.get_domain(\"std\")\n    refnodes = list(domain.resolve_xref(app.env, 'index', app.builder, 'token', 'A',\n                                        pending_xref(), nodes.paragraph()))\n    assert len(refnodes) == 1\n    assert_node(refnodes[0], nodes.reference, internal=True, refuri='#grammar-token-P-A')\n\n    refnodes = list(domain.resolve_xref(app.env, 'index', app.builder, 'token', '~A',\n                                        pending_xref(), nodes.paragraph()))\n    assert len(refnodes) == 1\n    assert_node(refnodes[0], nodes.reference, internal=True, refuri='#grammar-token-P-A')\n\n    refnodes = list(domain.resolve_xref(app.env, 'index', app.builder, 'token', ':A',\n                                        pending_xref(), nodes.paragraph()))\n    assert len(refnodes) == 1\n    assert_node(refnodes[0], nodes.reference, internal=True, refuri='#grammar-token-P-A')\n\n    refnodes = list(domain.resolve_xref(app.env, 'index', app.builder, 'token', '~:A',\n                                        pending_xref(), nodes.paragraph()))\n    assert len(refnodes) == 1\n    assert_node(refnodes[0], nodes.reference, internal=True, refuri='#grammar-token-P-A')\n\n    refnodes = list(domain.resolve_xref(app.env, 'index', app.builder, 'token', 'A:',\n                                        pending_xref(), nodes.paragraph()))\n    assert len(refnodes) == 1\n    assert_node(refnodes[0], nodes.reference, internal=True, refuri='#grammar-token-P-A')\n\n    refnodes = list(domain.resolve_xref(app.env, 'index', app.builder, 'token', '~A:',\n                                        pending_xref(), nodes.paragraph()))\n    assert len(refnodes) == 1\n    assert_node(refnodes[0], nodes.reference, internal=True, refuri='#grammar-token-P-A')\n\n    refnodes = list(domain.resolve_xref(app.env, 'index', app.builder,", "def test_program_option_dangling_warning(app, status, warning):\n    text = (\".. program:: ls\\n\"\n            \"\\n\"\n            \".. option:: -l\\n\"\n            \".. option:: -m\")\n\n    restructuredtext.parse(app, text, \"case1\")\n    assert (\"case1.rst:5: WARNING: unknown option: -m\" in warning.getvalue())\n\n    text = (\".. option:: -l\\n\"\n            \"\\n\"\n            \".. program:: ls\\n\")\n\n    restructuredtext.parse(app, text, \"case2\")\n    assert (\"case2.rst:2: WARNING: unknown option: -l\" in warning.getvalue())\n", "def test_productionlist_with_empty_line(app, status, warning):\n    text = (\".. productionlist::\\n\"\n            \"\\n\"\n            \"   A: `A`\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.productionlist,\n                          [addnodes.production, nodes.Text, \"A: \", \n                           nodes.pending_xref, nodes.literal, \"A\"]))\n\n    warnings = warning.getvalue().split(\"\\n\")\n    assert len(warnings) == 1\n    assert \"WARNING: missing production name\" in warnings[0]\n"], "sample_50": ["def test_sigint_handler_restoration_on_exception(self):\n    \"\"\"SIGINT handler is restored even if an exception occurs.\"\"\"\n        raise subprocess.SubprocessError('Mocked subprocess error')\n\n    sigint_handler = signal.getsignal(signal.SIGINT)\n    # The default handler isn't SIG_IGN.\n    self.assertNotEqual(sigint_handler, signal.SIG_IGN)\n    try:\n        with mock.patch('subprocess.run', new=_mock_subprocess_run):\n            DatabaseClient.runshell_db({})\n    except subprocess.SubprocessError:\n        pass\n    # dbshell restores the original handler.\n    self.assertEqual(sigint_handler, signal.getsignal(signal.SIGINT))\n", "def test_runshell(self):\n    \"\"\"runshell method calls runshell_db with the connection parameters.\"\"\"\n    conn_params = {\n        'database': 'dbname',\n        'user': 'someuser',\n        'password': 'somepassword',\n        'host': 'somehost',\n        'port': '444',\n    }\n    db_client = DatabaseClient()\n    db_client.connection = mock.Mock()\n    db_client.connection.get_connection_params.return_value = conn_params\n\n    with mock.patch.object(DatabaseClient, 'runshell_db') as mock_runshell_db:\n        db_client.runshell()\n        mock_runshell_db.assert_called_once_with(conn_params)\n", "def test_sigint_handler_restored_after_exception(self):\n    \"\"\"The original SIGINT handler is restored even if subprocess.run() raises an exception.\"\"\"\n        raise subprocess.SubprocessError('Mocked subprocess error')\n    sigint_handler = signal.getsignal(signal.SIGINT)\n    with self.assertRaises(subprocess.SubprocessError):\n        with mock.patch('subprocess.run', new=_mock_subprocess_run):\n            DatabaseClient.runshell_db({})\n    self.assertEqual(sigint_handler, signal.getsignal(signal.SIGINT))\n", "def test_runshell(self):\n    \"\"\"runshell() delegates to runshell_db() with the connection parameters.\"\"\"\n    conn_params = mock.Mock()\n    db_client = DatabaseClient()\n    db_client.connection = mock.Mock()\n    db_client.connection.get_connection_params.return_value = conn_params\n    with mock.patch.object(DatabaseClient, 'runshell_db') as runshell_db:\n        db_client.runshell()\n        runshell_db.assert_called_once_with(conn_params)\n", "def test_signal_restoration_after_subprocess_failure(self):\n    \"\"\"SIGINT handler is restored after subprocess.run raises an exception.\"\"\"\n        raise subprocess.CalledProcessError(1, 'Mocked error')\n\n    sigint_handler = signal.getsignal(signal.SIGINT)\n    # The default handler isn't SIG_IGN.\n    self.assertNotEqual(sigint_handler, signal.SIG_IGN)\n    with mock.patch('subprocess.run', new=_mock_subprocess_run):\n        with self.assertRaises(subprocess.CalledProcessError):\n            DatabaseClient.runshell_db({})\n    # dbshell restores the original handler.\n    self.assertEqual(sigint_handler, signal.getsignal(signal.SIGINT))\n"], "sample_1078": ["def test_IndexedBase_strides():\n    i, j, k, l, m, n, o, p = symbols('i j k l m n o p', integer=True)\n    a = IndexedBase('a', strides=(l, m, n), offset=o)\n    assert a.strides == (l, m, n)\n    assert a.offset == o\n    assert a[i, j, k].strides == (l, m, n)\n    assert a[i, j, k].offset == o\n    assert a[i, j, k].shape == (p,) if a.shape else (m, n, p)\n    assert a[i, j, k, p].strides == (l, m, n, p) if a.shape else (l, m, n, p)\n    assert a[i, j, k, p].offset == o\n    b = IndexedBase('b', strides='C', offset=o)\n    assert b.strides == 'C'\n    assert b.offset == o\n    c = IndexedBase('c', strides='F', offset=o)\n    assert c.strides == 'F'\n    assert c.offset == o\n", "def test_IndexedBase_shape_property():\n    i, j, m, n = symbols('i j m n', integer=True)\n    a = IndexedBase('a')\n    assert a.shape is None\n    raises(IndexException, lambda: a[i, j].shape)\n    a_shape = IndexedBase('a', shape=(m, n))\n    assert a_shape[i, j].shape == Tuple(m, n)\n    a_shape[i, j, 1].shape = Tuple(m, n, 1)\n    assert a_shape[i, j, 1].shape == Tuple(m, n, 1)\n    raises(IndexException, lambda: a_shape[i, j, 1].shape = Tuple(m, n))\n", "def test_Indexed_derivative():\n    i, j = symbols('i j', cls=Idx)\n    A = IndexedBase('A')\n    B = IndexedBase('B')\n    expr1 = A[i]*B[j]\n    expr2 = A[i]**2\n    expr3 = sin(A[i])\n    \n    assert expr1.diff(A[i]) == B[j]\n    assert expr1.diff(B[i]) == A[i]\n    assert expr2.diff(A[i]) == 2*A[i]\n    assert expr3.diff(A[i]) == cos(A[i])\n    raises(TypeError, lambda: expr1.diff(A[i, j]))\n    raises(TypeError, lambda: expr1.diff(A))\n", "def test_IndexedBase_as_list():\n    i = symbols('i', integer=True)\n    A = IndexedBase('A')\n    assert A[i] == A.__getitem__((i,))\n    assert A[i] == A.__getitem__([i])\n    assert A[i] == A.__getitem__(Tuple(i,))\n    raises(IndexException, lambda: A.__getitem__())\n    raises(IndexException, lambda: A.__getitem__('a'))\n    raises(IndexException, lambda: A.__getitem__((i, i)))\n    A = IndexedBase('A', shape=(3,))\n    raises(IndexException, lambda: A.__getitem__((i, i)))\n    raises(IndexException, lambda: A.__getitem__((i, i, i)))\n", "def test_Indexed_shape_edge_cases():\n    i, j, k = symbols('i j k', integer=True)\n    n, m, o = symbols('n m o', integer=True)\n    A = IndexedBase('A', shape=(n, m))\n    assert A[i, j].shape == Tuple(n, m)\n    assert A[i, j, k].shape is None\n    B = IndexedBase('A', shape=(n, m, o))\n    assert B[i, j, k].shape == Tuple(n, m, o)\n    C = IndexedBase('A')\n    assert C[i, j, k].shape is None\n    D = IndexedBase('A', shape=())\n    assert D[i].shape is None\n    E = IndexedBase('A', shape=(n,))\n    assert E[i, j].shape is None\n    raises(IndexException, lambda: E[i, j].ranges)\n"], "sample_221": ["def test_in_lookup_values_list_queryset_evaluation(self):\n    \"\"\"\n    Neither pickling nor unpickling a QuerySet.query with an __in=inner_qs\n    lookup should evaluate inner_qs when the inner_qs uses values_list().\n    \"\"\"\n    groups = Group.objects.values_list('id')\n    events = Event.objects.filter(group__in=groups)\n\n    with self.assertNumQueries(0):\n        dumped = pickle.dumps(events.query)\n\n    with self.assertNumQueries(0):\n        reloaded = pickle.loads(dumped)\n        reloaded_events = Event.objects.none()\n        reloaded_events.query = reloaded\n\n    self.assertSequenceEqual(reloaded_events, [self.e1])\n", "    def setUpTestData(cls):\n        cls.event1 = Event.objects.create(title='Event 1')\n        cls.event2 = Event.objects.create(title='Event 2')\n", "    def test_union_with_empty_querysets(self):\n        qs1 = Event.objects.all()\n        qs2 = Event.objects.none()\n        self.assertSequenceEqual(list(qs1.union(qs2)), list(qs1))\n", "    def setUpTestData(cls):\n        cls.event1 = Event.objects.create(title='Event 1', group_id=1)\n        cls.event2 = Event.objects.create(title='Event 2', group_id=1)\n        cls.event3 = Event.objects.create(title='Event 3', group_id=2)\n        cls.event4 = Event.objects.create(title='Event 4', group_id=2)\n", "def test_model_pickle_prefetch_related_with_deferred(self):\n    \"\"\"\n    Regression for #28986: Test that a deferred model can be pickled after\n    prefetch_related.\n    \"\"\"\n    m1 = M2MModel.objects.create()\n    g1 = Group.objects.create(name='foof')\n    m1.groups.add(g1)\n\n    # Create a deferred model\n    m2 = M2MModel.objects.defer('groups').get()\n\n    # Prefetch related, then pickle and unpickle\n    m2 = pickle.loads(pickle.dumps(m2))\n    m2.groups.all()\n\n    # Create a deferred model with a prefetched related object\n    m3 = M2MModel.objects.prefetch_related('groups').defer('groups').get()\n\n    # Pickle and unpickle\n    m3 = pickle.loads(pickle.dumps(m3))\n    self.assertEqual(m3.groups.all()[0].name, 'foof')\n"], "sample_965": ["def test_isabstractmethod():\n    class Foo:\n        @abstractmethod\n            pass\n\n    class Bar(Foo):\n            pass\n\n    class Baz(Foo):\n        pass\n\n    assert inspect.isabstractmethod(Foo.meth) is True\n    assert inspect.isabstractmethod(Bar.meth) is False\n    assert inspect.isabstractmethod(Baz.meth) is True\n", "def test_signature_from_str_positional_only_with_defaults():\n    sig = inspect.signature_from_str('(a, b=0, /, c, d=1)')\n    assert list(sig.parameters.keys()) == ['a', 'b', 'c', 'd']\n    assert sig.parameters['a'].kind == Parameter.POSITIONAL_ONLY\n    assert sig.parameters['a'].default == Parameter.empty\n    assert sig.parameters['b'].kind == Parameter.POSITIONAL_ONLY\n    assert sig.parameters['b'].default == '0'\n    assert sig.parameters['c'].kind == Parameter.POSITIONAL_ONLY\n    assert sig.parameters['c'].default == Parameter.empty\n    assert sig.parameters['d'].kind == Parameter.POSITIONAL_ONLY\n    assert sig.parameters['d'].default == '1'\n", "def test_isabstractmethod():\n    class AbstractClass:\n        @abstractmethod\n            pass\n\n    class ConcreteClass:\n            pass\n\n    assert inspect.isabstractmethod(AbstractClass.abstract_method) is True\n    assert inspect.isabstractmethod(ConcreteClass.concrete_method) is False\n\n    abstract_instance = AbstractClass()\n    concrete_instance = ConcreteClass()\n\n    with pytest.raises(NotImplementedError):\n        abstract_instance.abstract_method()\n\n    concrete_instance.concrete_method()\n", "def test_getorigbases():\n    class A:\n        pass\n\n    class B:\n        pass\n\n    class C(A, B):\n        pass\n\n    assert inspect.getorigbases(C) == (A, B)\n\n    class D(C):\n        pass\n\n    assert inspect.getorigbases(D) == (C,)\n\n    class E:\n        __orig_bases__ = (A, B)\n\n    assert inspect.getorigbases(E) == (A, B)\n\n    class F:\n        pass\n\n    assert inspect.getorigbases(F) is None\n\n    obj = \"not a class\"\n\n    with pytest.raises(TypeError):\n        inspect.getorigbases(obj)\n", "def test_unwrap_all():\n        pass\n\n    func2 = functools.partial(func1, 1)\n    func3 = functools.partial(func2, 2)  # nested partial object\n    func4 = functools.wraps(func1)(func3)\n\n    assert inspect.unwrap_all(func4) is func1\n\n    class ClassMethod:\n            self.__func__ = method\n\n    class StaticMethod:\n            self.__func__ = method\n\n    class_method = ClassMethod(func1)\n    static_method = StaticMethod(func1)\n\n    assert inspect.unwrap_all(class_method) is func1\n    assert inspect.unwrap_all(static_method) is func1\n\n    # Test stop parameter\n        return isinstance(obj, functools.partial)\n\n    assert inspect.unwrap_all(func4, stop=stop) is func2\n"], "sample_53": ["def test_media_property(self):\n    class MediaDefiningWidget(Widget, metaclass=MediaDefiningClass):\n        class Media:\n            css = {'screen': ['my_css.css']}\n            js = ['my_js.js']\n\n    widget = MediaDefiningWidget()\n    self.assertEqual(widget.media._css, {'screen': ['my_css.css']})\n    self.assertEqual(widget.media._js, ['my_js.js'])\n\n    class MediaDefiningWidgetWithExtend(MediaDefiningWidget):\n        class Media:\n            extend = False\n            css = {'print': ['my_print_css.css']}\n            js = ['my_print_js.js']\n\n    widget = MediaDefiningWidgetWithExtend()\n    self.assertEqual(widget.media._css, {'print': ['my_print_css.css']})\n    self.assertEqual(widget.media._js, ['my_print_js.js'])\n\n    class MediaDefiningWidgetWithExtendTrue(MediaDefiningWidget):\n        class Media:\n            extend = True\n            css = {'print': ['my_print_css.css']}\n            js = ['my_print_js.js']\n\n    widget = MediaDefiningWidgetWithExtendTrue()\n    self.assertEqual(widget.media._css, {'screen': ['my_css.css'], 'print': ['my_print_css.css']})\n    self.assertEqual(widget.media._js, ['my_js.js', 'my_print_js.js'])\n", "    def test_clearable_file_input(self):\n        class ClearableFileInputForm(forms.Form):\n            file = forms.FileField(required=False, widget=forms.ClearableFileInput)\n\n        form = ClearableFileInputForm(files={'file': SimpleUploadedFile('test.txt', b'Hello, world!')})\n        form.is_valid()\n        widget = form.fields['file'].widget\n        context = widget.get_context('file', form.data['file'], attrs={})\n        self.assertIn('checked', context['widget']['attrs'])\n        self.assertEqual(context['widget']['value'], form.data['file'])\n\n        form = ClearableFileInputForm(files={'file': SimpleUploadedFile('test.txt', b'Hello, world!')}, data={'file-clear': 'on'})\n        form.is_valid()\n        widget = form.fields['file'].widget\n        context = widget.get_context('file', form.data['file'], attrs={})\n        self.assertEqual(context['widget']['value'], False)\n", "def test_media_order_conflict_warning(self):\n    class CustomWidget(Widget):\n        class Media:\n            extend = True\n\n            super().__init__()\n            self.Media.js = ['js/a.js', 'js/b.js']\n\n    class AnotherWidget(Widget):\n        class Media:\n            extend = True\n\n            super().__init__()\n            self.Media.js = ['js/b.js', 'js/a.js']\n\n    widget1 = CustomWidget()\n    widget2 = AnotherWidget()\n\n    with self.assertWarns(MediaOrderConflictWarning):\n        widget1.media + widget2.media\n", "def test_value_from_datadict_file_input(self):\n    form = forms.Form()\n    widget = FileInput()\n    file_data = {'file': 'file_data'}\n    self.assertEqual(widget.value_from_datadict({}, file_data, 'file'), 'file_data')\n", "def test_select_date_widget(self):\n    widget = SelectDateWidget()\n    date_value = datetime.date(2022, 1, 15)\n    context = widget.get_context('date_field', date_value, {})\n    self.assertEqual(context['widget']['value'], {'year': 2022, 'month': 1, 'day': 15})\n    self.assertEqual(len(context['widget']['subwidgets']), 3)\n\n    # Test that the widget correctly handles invalid dates\n    invalid_date = '2022-02-31'\n    match = SelectDateWidget.date_re.match(invalid_date)\n    self.assertIsNotNone(match)\n    self.assertEqual([int(val) or '' for val in match.groups()], [2022, 2, 31])\n\n    # Test the _parse_date_fmt method\n    self.assertEqual(list(SelectDateWidget._parse_date_fmt()), ['year', 'month', 'day'])\n\n    # Test the format_value method with a string input\n    formatted_value = widget.format_value('2022-01-15')\n    self.assertEqual(formatted_value, {'year': 2022, 'month': 1, 'day': 15})\n"], "sample_344": ["def test_resolve_fields_and_relations(self):\n    \"\"\"\n    Resolving fields and relations in ProjectState works as expected.\n    \"\"\"\n\n    project_state = ProjectState()\n    project_state.add_model(ModelState(\n        app_label=\"migrations\",\n        name=\"Tag\",\n        fields=[\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=100)),\n            (\"hidden\", models.BooleanField()),\n        ],\n    ))\n    project_state.add_model(ModelState(\n        app_label=\"migrations\",\n        name=\"SubTag\",\n        fields=[\n            ('tag_ptr', models.OneToOneField(\n                'migrations.Tag',\n                models.CASCADE,\n                auto_created=True,\n                parent_link=True,\n                primary_key=True,\n                to_field='id',\n                serialize=False,\n            )),\n            (\"awesome\", models.BooleanField()),\n        ],\n        bases=(\"migrations.Tag\",),\n    ))\n\n    project_state.resolve_fields_and_relations()\n\n    self.assertIn((\"migrations\", \"subtag\"), project_state.relations[(\"migrations\", \"tag\")])\n    self.assertIn((\"migrations\", \"tag\"), project_state.relations[(\"migrations\", \"subtag\")])\n", "    def test_related_models_recursive_with_self_referential_fields(self):\n        \"\"\"\n        Tests that the get_related_models_recursive function works correctly\n        when a model has self-referential fields.\n        \"\"\"\n        class SelfReferential(models.Model):\n            self_referential_fk = models.ForeignKey('self', models.CASCADE)\n            self_referential_m2m = models.ManyToManyField('self')\n\n            class Meta:\n                app_label = 'migrations'\n\n        self_referential_state = ModelState.from_model(SelfReferential)\n        project_state = ProjectState()\n        project_state.add_model(self_referential_state)\n\n        related_models = get_related_models_recursive(SelfReferential)\n        self.assertEqual(len(related_models), 2)\n        self.assertIn((SelfReferential._meta.app_label, SelfReferential._meta.model_name), related_models)\n        self.assertIn((SelfReferential._meta.app_label, SelfReferential._meta.model_name + '_self_referential_m2m'), related_models)\n", "def test_remove_model_with_referential_integrity(self):\n    project_state = ProjectState()\n    project_state.add_model(ModelState(\n        app_label=\"migrations\",\n        name=\"A\",\n        fields=[\n            (\"id\", models.AutoField(primary_key=True)),\n        ],\n    ))\n    project_state.add_model(ModelState(\n        app_label=\"migrations\",\n        name=\"B\",\n        fields=[\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"a\", models.ForeignKey(\"migrations.A\", models.CASCADE)),\n        ],\n    ))\n\n    old_state = project_state.clone()\n\n    project_state.remove_model(\"migrations\", \"a\")\n\n    A = old_state.apps.get_model('migrations', 'A')\n    B = old_state.apps.get_model('migrations', 'B')\n    with self.assertRaises(LookupError):\n        project_state.apps.get_model('migrations', 'A')\n\n    # Check that B's referential integrity is still intact\n    self.assertIs(A, B._meta.get_field('a').related_model)\n", "def test_unrendered_models_not_reloaded(self):\n    project_state = ProjectState()\n    project_state.add_model(ModelState('migrations', 'A', []))\n    project_state.add_model(ModelState('migrations', 'B', [], bases=('migrations.A',)))\n\n    project_state.apps  # Render project state.\n\n    # The models aren't loaded yet\n    with self.assertRaises(InvalidBasesError):\n        project_state.apps.get_model('migrations', 'B')\n\n    old_state = project_state.clone()\n\n    # Now reload the models.\n    project_state.reload_models([('migrations', 'A')])\n\n    # Unrendered models are not reloaded.\n    self.assertIs(old_state.apps.get_model('migrations', 'B'), project_state.apps.get_model('migrations', 'B'))\n", "    def test_reload_model_inheritance(self):\n        \"\"\"\n        Models with a base that's a concrete model, should be reloaded when\n        the base model is changed.\n        \"\"\"\n        project_state = ProjectState()\n        project_state.add_model(ModelState('migrations', 'A', []))\n        project_state.add_model(ModelState('migrations', 'B', [], bases=('migrations.A',)))\n        project_state.apps  # Render project state.\n        A = project_state.apps.get_model('migrations', 'A')\n        B = project_state.apps.get_model('migrations', 'B')\n        self.assertEqual(A._meta.get_field('b_ptr').remote_field.model, B)\n        self.assertEqual(B._meta.get_field('a_ptr').remote_field.model, A)\n\n        # Create a new ProjectState with the same models, but with A having an\n        # additional field.\n        new_state = project_state.clone()\n        new_state.add_field('migrations', 'A', 'new_field', models.IntegerField())\n        new_state.apps  # Render project state.\n        new_A = new_state.apps.get_model('migrations', 'A')\n        new_B = new_state.apps.get_model('migrations', 'B')\n        self.assertEqual(new_A._meta.get_field('new_field').model, new_A)\n        self.assertEqual(new_A._meta.get_field('b_ptr').remote_field.model, new_B)\n        self.assertEqual(new_B._meta.get_field('a_ptr').remote_field.model, new_A)\n\n        # A and B are now different models.\n        self.assertIsNot(A, new_A)\n        self.assertIsNot(B, new_B)\n\n        # But they still refer to the correct model.\n        self.assertIs(new_B._meta.get_field('a_ptr').remote_field.model, new_A)\n"], "sample_865": ["def test_cost_complexity_pruning_path_on_unfitted_tree():\n    # Test that `cost_complexity_pruning_path` raises NotFittedError on\n    # an unfitted tree\n    clf = DecisionTreeClassifier()\n\n    X = iris.data\n    y = iris.target\n\n    with pytest.raises(NotFittedError):\n        clf.cost_complexity_pruning_path(X, y)\n", "def test_prune_tree_max_depth():\n    X, y = datasets.make_classification(n_samples=100, n_features=10,\n                                        n_informative=5, random_state=42)\n    clf = DecisionTreeClassifier(ccp_alpha=0.5, random_state=0)\n    clf.fit(X, y)\n    assert clf.tree_.max_depth < X.shape[0]\n", "def test_feature_importances():\n    # Check feature importances.\n    # Test case from issue #4358\n    X = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],\n                  [1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]])\n    y = np.array([0, 1, 1, 0, 1, 0, 0, 1])\n    est = DecisionTreeClassifier(random_state=0)\n    est.fit(X, y)\n    assert np.all(est.feature_importances_ >= 0.)\n    assert np.isclose(np.sum(est.feature_importances_), 1.)\n", "def test_prune_tree_no_impurity_decrease():\n    # Test pruning for trees that don't have an impurity decrease.\n    X = np.array([[0.], [1.], [0.], [1.]])\n    y = np.array([0., 0., 1., 1.])\n\n    for name, TreeEstimator in ALL_TREES.items():\n        est = TreeEstimator(random_state=0)\n        est.fit(X, y)\n        assert_array_equal(est.predict(X), y)\n\n        est = TreeEstimator(random_state=0, min_impurity_decrease=1.)\n        est.fit(X, y)\n        assert_array_equal(est.predict(X), y)\n", "def check_max_depth_prune_tree(name):\n    # Test if max_depth works with prune_tree\n    TreeEstimator = ALL_TREES[name]\n\n    X = iris.data\n    y = iris.target\n\n    est = TreeEstimator(random_state=0, max_depth=3)\n    est.fit(X, y)\n\n    # if max_depth is set, tree should not be pruned by prune_tree\n    assert est.tree_.max_depth == est.get_depth()\n\n    est2 = TreeEstimator(random_state=0, max_depth=3, ccp_alpha=0.01)\n    est2.fit(X, y)\n\n    assert est.tree_.node_count == est2.tree_.node_count\n    assert est.tree_.max_depth == est2.get_depth()\n\n"], "sample_540": ["def test_animation_repr_html_null_writer(caplog, anim):\n    caplog.set_level(\"WARNING\")\n    with pytest.raises(AttributeError):\n        anim._repr_html_()\n    assert len(caplog.records) == 1\n    record, = caplog.records\n    assert (record.name == \"matplotlib.animation\"\n            and record.levelname == \"WARNING\")\n", "def test_adjusted_figsize():\n    w, h = 10.0, 10.0\n    dpi = 100\n    n = 2\n    wnew, hnew = animation.adjusted_figsize(w, h, dpi, n)\n\n    assert wnew == 10.0\n    assert hnew == 10.0\n\n    w, h = 9.9, 9.9\n    wnew, hnew = animation.adjusted_figsize(w, h, dpi, n)\n\n    assert wnew < 10.0\n    assert hnew < 10.0\n", "def test_movie_writer_extra_args(anim):\n    writer = animation.MovieWriter(extra_args=['-loglevel', 'error'])\n    with pytest.warns(None) as record:\n        anim.save('unused.null', writer=writer)\n    assert len(record) == 0\n    assert writer.extra_args == ['-loglevel', 'error']\n\n    writer = animation.MovieWriter(extra_args=['-loglevel'])\n    with pytest.warns(None) as record:\n        anim.save('unused.null', writer=writer)\n    assert len(record) == 1\n    assert record[0].category == UserWarning\n", "def test_adjusted_figsize(anim):\n    # Test that adjusted_figsize is working as expected.\n    fig = anim._fig\n    w, h = fig.get_size_inches()\n    dpi = 100\n    n = 2\n    wnew, hnew = animation.adjusted_figsize(w, h, dpi, n)\n\n    assert int(w*dpi) % n != 0\n    assert int(wnew*dpi) % n == 0\n\n    wnext = np.nextafter(w, np.inf)\n    assert int(wnext*dpi) % n == 0\n    assert wnext > w\n    assert wnew == wnext\n\n    hnext = np.nextafter(h, -np.inf)\n    assert int(hnext*dpi) % n == 0\n    assert hnext < h\n    assert hnew == hnext\n", "def test_animation_embed_frames(anim):\n    # test saving with embedded frames\n    anim = animation.FuncAnimation(**anim)\n    filename = 'movie.html'\n    writer = animation.HTMLWriter(fps=30, embed_frames=True)\n    with open(filename, 'w') as of:\n        of.write(anim.to_jshtml(fps=30, embed_frames=True, default_mode='once'))\n    with open(filename, 'r') as of:\n        assert of.read().find('frames = []') == -1\n"], "sample_259": ["def test_prefetch_object_with_nested_lookups(self):\n    authors = list(Author.objects.all())\n    with self.assertNumQueries(2):\n        prefetch_related_objects(authors, Prefetch('books__read_by', queryset=Reader.objects.all()))\n\n    with self.assertNumQueries(0):\n        self.assertEqual(\n            [\n                [[str(r) for r in b.read_by.all()] for b in a.books.all()]\n                for a in authors\n            ],\n            [\n                [['Amy'], ['Belinda']],  # Charlotte - Poems, Jane Eyre\n                [['Amy']],               # Anne - Poems\n                [['Amy'], []],           # Emily - Poems, Wuthering Heights\n                [['Amy', 'Belinda']],    # Jane - Sense and Sense\n            ]\n        )\n", "def test_deferred_model(self):\n    book1 = Book.objects.get(id=self.book1.id)\n    authors = list(book1.authors.all())\n    with self.assertNumQueries(1):\n        prefetch_related_objects(authors, 'first_book')\n\n    with self.assertNumQueries(0):\n        [author.first_book.defer('id').pk for author in authors]\n", "def test_prefetch_object_with_nested_lookups(self):\n    author1 = Author.objects.get(id=self.author1.id)\n    with self.assertNumQueries(1):\n        prefetch_related_objects([author1], Prefetch('books__read_by'))\n\n    with self.assertNumQueries(0):\n        read_by_list = [reader.name for book in author1.books.all() for reader in book.read_by.all()]\n        self.assertCountEqual(read_by_list, ['Amy', 'Belinda'])\n", "def test_prefetch_object_with_through_model(self):\n    book1 = Book.objects.get(id=self.book1.id)\n    with self.assertNumQueries(1):\n        prefetch_related_objects([book1], Prefetch('authors__books'))\n\n    with self.assertNumQueries(0):\n        expected_books = [\n            {self.author1: [self.book1, self.book2]},\n            {self.author2: [self.book1]},\n            {self.author3: [self.book1, self.book3]},\n        ]\n        actual_books = [\n            {a: list(a.books.all()) for a in b.authors.all()}\n            for b in [book1]\n        ]\n        self.assertEqual(expected_books, actual_books)\n", "def test_deferred_filter(self):\n    authors = Author.objects.defer('name')\n    with self.assertNumQueries(1):\n        authors = list(authors.filter(first_book=self.book1))\n\n    with self.assertNumQueries(1):\n        # deferred fields don't interfere with filter\n        self.assertEqual([a.name for a in authors], ['Charlotte', 'Anne', 'Emily'])\n"], "sample_567": ["def test_annotation_update_xy():\n    fig, ax = plt.subplots(1, 1)\n    an = ax.annotate('annotation', xy=(0.5, 0.5))\n    extent1 = an.get_window_extent(fig.canvas.get_renderer())\n    an.xy = (0.7, 0.7)\n    extent2 = an.get_window_extent(fig.canvas.get_renderer())\n    assert not np.allclose(extent1.get_points(), extent2.get_points(), rtol=1e-6)\n", "def test_wrap_text_rotation():\n    fig = plt.figure(figsize=(6, 6))\n    s = 'This is a very long text that should be wrapped multiple times.'\n    text = fig.text(0.5, 0.7, s, wrap=True, rotation=45)\n    fig.canvas.draw()\n    assert len(text._get_wrapped_text().split('\\n')) == 3\n\n    text = fig.text(0.5, 0.3, s, wrap=True, rotation=90)\n    fig.canvas.draw()\n    assert len(text._get_wrapped_text().split('\\n')) == 3\n\n    text = fig.text(0.5, 0.1, s, wrap=True, rotation=-45)\n    fig.canvas.draw()\n    assert len(text._get_wrapped_text().split('\\n')) == 3\n", "def test_annotate_textclip():\n    fig, ax = plt.subplots()\n    annot = ax.annotate('Annotation', xy=(.5, .5), xycoords='data',\n                        xytext=(0, 50), textcoords='offset points')\n    annot.set_clip_on(True)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_aspect('equal')\n    ax.set_xticks([])\n    ax.set_yticks([])\n", "def test_annotation_clip():\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n\n    ann1 = ax.annotate(\"annotation\", xy=(0.5, 0.5), xycoords=\"data\",\n                       annotation_clip=False)\n    ann2 = ax.annotate(\"annotation\", xy=(1.5, 0.5), xycoords=\"data\")\n    ann3 = ax.annotate(\"annotation\", xy=(1.5, 0.5), xycoords=\"data\",\n                       annotation_clip=False)\n    ann4 = ax.annotate(\"annotation\", xy=(1.5, 1.5), xycoords=\"data\",\n                       annotation_clip=False)\n\n    fig.canvas.draw()\n    assert ann1._check_xy(fig.canvas.get_renderer())\n    assert not ann2._check_xy(fig.canvas.get_renderer())\n    assert ann3._check_xy(fig.canvas.get_renderer())\n    assert ann4._check_xy(fig.canvas.get_renderer())\n\n    assert ann1.get_visible()\n    assert not ann2.get_visible()\n    assert ann3.get_visible()\n    assert ann4.get_visible()\n", "def test_xytext_textcoords():\n    fig, ax = plt.subplots()\n    t = ax.text(0.5, 0.5, 'test')\n    ann = ax.annotate(t, xy=(0.5, 0.5), xytext=(0.2, 0.2), textcoords='axes fraction')\n    assert t.get_window_extent(fig.canvas.get_renderer()).overlaps(\n        ann.get_window_extent(fig.canvas.get_renderer()))\n\n    # Swap coordinates\n    ann = ax.annotate(t, xy=(0.2, 0.2), xytext=(0.5, 0.5), textcoords='axes fraction')\n    assert t.get_window_extent(fig.canvas.get_renderer()).overlaps(\n        ann.get_window_extent(fig.canvas.get_renderer()))\n"], "sample_720": ["def test_quantile_transform_subsample_sparse():\n    # Test that subsampling of input yield to a consistent results We check\n    # that the computed quantiles are almost mapped to a [0, 1] vector where\n    # values are equally spaced. The infinite norm is checked to be smaller\n    # than a given threshold. This is repeated 5 times.\n\n    # sparse support\n    # TODO: rng should be seeded once we drop support for older versions of\n    # scipy (< 0.13) that don't support seeding.\n    n_samples = 1000000\n    n_quantiles = 1000\n    ROUND = 5\n    X = sparse.rand(n_samples, 1, density=.99, format='csc')\n    inf_norm_arr = []\n    for random_state in range(ROUND):\n        transformer = QuantileTransformer(random_state=random_state,\n                                          n_quantiles=n_quantiles,\n                                          subsample=n_samples // 10,\n                                          ignore_implicit_zeros=True)\n        transformer.fit(X)\n        diff = (np.linspace(0, 1, n_quantiles) -\n                np.ravel(transformer.quantiles_))\n        inf_norm = np.max(np.abs(diff))\n        assert_true(inf_norm < 1e-1)\n        inf_norm_arr.append(inf_norm)\n    # each random subsampling yield a unique approximation to the expected\n    # linspace CDF\n    assert_equal(len(np.unique(inf_norm_arr)), len(inf_norm_arr))\n", "def test_quantile_transform_output_distribution():\n    X = np.random.RandomState(37).randn(10, 3)\n    transformer = QuantileTransformer(n_quantiles=10,\n                                      output_distribution='normal')\n    X_trans = transformer.fit_transform(X)\n    assert_almost_equal(stats.normal.ppf(np.linspace(0, 1, 10)), \n                        np.sort(X_trans, axis=0), 3)\n\n    transformer = QuantileTransformer(n_quantiles=100,\n                                      output_distribution='normal')\n    X_trans = transformer.fit_transform(X)\n    assert_almost_equal(stats.normal.ppf(np.linspace(0, 1, 100)), \n                        np.sort(X_trans, axis=0), 3)\n\n    transformer = QuantileTransformer(n_quantiles=1000,\n                                      output_distribution='normal')\n    X_trans = transformer.fit_transform(X)\n    assert_almost_equal(stats.normal.ppf(np.linspace(0, 1, 1000)), \n                        np.sort(X_trans, axis=0), 3)\n", "def test_power_transformer_copy_exception():\n    pt = PowerTransformer(method='box-cox', standardize=False, copy=False)\n    X = np.abs(X_2d)\n\n    # An exception should not be raised if copy=False is used\n    pt.fit(X)\n\n    # Copy should be disabled\n    assert pt.copy == False\n\n    X_copy = X.copy()\n    pt.fit(X)\n    assert_array_equal(X, X_copy)\n\n    # An exception should not be raised if copy=True is used\n    pt = PowerTransformer(method='box-cox', standardize=False, copy=True)\n    pt.fit(X)\n\n    # Copy should be enabled\n    assert pt.copy == True\n\n    X_copy = X.copy()\n    pt.fit(X)\n    assert_array_not_equal(X, X_copy)\n", "def test_quantile_transform_edge_cases():\n    # Test edge cases with repeated values\n    X = np.array([[0, 1, 1, 1],\n                  [1, 1, 1, 2]])\n    transformer = QuantileTransformer(n_quantiles=2, output_distribution='normal')\n    transformer.fit(X)\n    X_trans = transformer.transform(X)\n    assert_array_almost_equal(X_trans, [[-1.0, -1.0, -1.0, -1.0],\n                                        [1.0, 1.0, 1.0, 1.0]])\n    # Test edge cases with repeated values and output uniform distribution\n    transformer = QuantileTransformer(n_quantiles=2, output_distribution='uniform')\n    transformer.fit(X)\n    X_trans = transformer.transform(X)\n    assert_array_almost_equal(X_trans, [[0, 0, 0, 0],\n                                        [1, 1, 1, 1]])\n    # Test edge cases with repeated values and output uniform distribution\n    X = np.array([[0, 0, 0, 1],\n                  [1, 1, 1, 2]])\n    transformer = QuantileTransformer(n_quantiles=2, output_distribution='uniform')\n    transformer.fit(X)\n    X_trans = transformer.transform(X)\n    assert_array_almost_equal(X_trans, [[0, 0, 0, 1],\n                                        [1, 1, 1, 1]])\n", "def test_quantile_transform_fitted_and_transformed():\n    # Test that quantile transformer behaves as expected after both fit and transform\n    X = np.arange(100).reshape((20, 5))\n    transformer = QuantileTransformer(n_quantiles=10)\n    X_trans = transformer.fit_transform(X)\n    X_trans2 = transformer.transform(X)\n\n    # Test that fit_transform and transform give the same result\n    assert_array_almost_equal(X_trans, X_trans2)\n\n    # Test that the resulting transformed array is a float\n    assert_true(isinstance(X_trans2[0, 0], float))\n"], "sample_936": ["def test_stringify_ForwardRef():\n    if sys.version_info >= (3, 7):\n        from typing import ForwardRef\n        assert stringify(ForwardRef('int')) == \"int\"\n        assert stringify(ForwardRef('unknown')) == \"unknown\"\n    else:\n        from sphinx.util.typing import ForwardRef\n        assert stringify(ForwardRef('int')) == \"int\"\n        assert stringify(ForwardRef('unknown')) == \"unknown\"\n", "def test_stringify_forward_ref():\n    if sys.version_info >= (3, 7):\n        from typing import ForwardRef\n        assert stringify(ForwardRef(\"int\")) == \"int\"\n        assert stringify(ForwardRef(\"unknown\")) == \"unknown\"\n    else:\n        from sphinx.util.typing import ForwardRef\n        assert stringify(ForwardRef(\"int\")) == \"int\"\n        assert stringify(ForwardRef(\"unknown\")) == \"unknown\"\n", "def test_stringify_ForwardRef():\n    assert stringify(ForwardRef(\"int\")) == \"int\"\n    assert stringify(ForwardRef(\"str\")) == \"str\"\n    assert stringify(ForwardRef(\"List[int]\")) == \"List[int]\"\n    assert stringify(ForwardRef(\"Tuple[str, str]\")) == \"Tuple[str, str]\"\n    assert stringify(ForwardRef(\"Dict[str, int]\")) == \"Dict[str, int]\"\n", "def test_stringify_type_hints_ForwardRef():\n    from sphinx.util.typing import ForwardRef\n\n    T = TypeVar('T')\n    F = ForwardRef('T')\n    assert stringify(F) == \"T\"\n\n    F = ForwardRef('List[T]')\n    assert stringify(F) == \"List[T]\"\n\n    F = ForwardRef('Union[int, str]')\n    assert stringify(F) == \"Union[int, str]\"\n", "def test_stringify_type_hints_ForwardRef():\n    from typing import ForwardRef\n\n    if sys.version_info >= (3, 7):\n        assert stringify(ForwardRef(\"int\")) == \"int\"\n        assert stringify(ForwardRef(\"str\")) == \"str\"\n    else:\n        assert stringify(ForwardRef(\"int\")) == \"'int'\"\n        assert stringify(ForwardRef(\"str\")) == \"'str'\"\n\n    assert stringify(ForwardRef(\"unknown\")) == \"unknown\"\n\n    # Test nested ForwardRef\n    if sys.version_info >= (3, 7):\n        assert stringify(ForwardRef(\"List[int]\")) == \"List[int]\"\n        assert stringify(ForwardRef(\"Dict[str, Tuple]\")) == \"Dict[str, Tuple]\"\n    else:\n        assert stringify(ForwardRef(\"List[int]\")) == \"List[int]\"\n        assert stringify(ForwardRef(\"Dict[str, Tuple]\")) == \"Dict[str, Tuple]\"\n\n    # Test ForwardRef with arguments\n    if sys.version_info >= (3, 7):\n        assert stringify(ForwardRef(\"Callable[[str], int]\")) == \"Callable[[str], int]\"\n        assert stringify(ForwardRef(\"Union[str, int]\")) == \"Union[str, int]\"\n    else:\n        assert stringify(ForwardRef(\"Callable[[str], int]\")) == \"Callable[str, int]\"\n        assert stringify(ForwardRef(\"Union[str, int]\")) == \"Union[str, int]\"\n"], "sample_1103": ["def test_integer_nthroot_edge_cases():\n    # Test integer_nthroot for edge cases\n    assert integer_nthroot(0, 0) == (0, True)\n    assert integer_nthroot(0, 1) == (0, True)\n    assert integer_nthroot(1, 0) == (1, True)\n    assert integer_nthroot(1, 1) == (1, True)\n    assert integer_nthroot(-1, 0) == (1, True)\n    assert integer_nthroot(-1, 1) == (-1, True)\n    assert integer_nthroot(0, -1) == (0, True)\n    assert integer_nthroot(1, -1) == (1, True)\n    assert integer_nthroot(-1, -1) == (-1, True)\n", "def test_Pow_integer_nthroot():\n    # issue 15340\n    assert integer_nthroot(16, 2) == (4, True)\n    assert integer_nthroot(26, 2) == (5, False)\n    assert integer_nthroot(8, 3) == (2, True)\n    assert integer_nthroot(27, 3) == (3, True)\n    assert integer_nthroot(28, 3) == (3, False)\n    assert integer_nthroot(125, 5) == (3, False)\n    assert isqrt(16) == 4\n    assert isqrt(26) == 5\n", "def test_integer_nthroot():\n    assert integer_nthroot(16, 2) == (4, True)\n    assert integer_nthroot(26, 2) == (5, False)\n    assert integer_nthroot(16, 3) == (2, True)\n    assert integer_nthroot(27, 3) == (3, True)\n    assert integer_nthroot(16, 4) == (2, True)\n    assert integer_nthroot(27, 4) == (2, False)\n    assert integer_nthroot(2, 1) == (2, True)\n    assert integer_nthroot(4, 1) == (4, True)\n    assert integer_nthroot(9, 1) == (9, True)\n    assert integer_nthroot(2, -1) == (1/2, True)\n    assert integer_nthroot(4, -1) == (1/4, True)\n    assert integer_nthroot(9, -1) == (1/9, True)\n    assert integer_nthroot(1, 10) == (1, True)\n    assert integer_nthroot(1, 0) == (1, True)\n    assert integer_nthroot(0, 10) == (0, True)\n    assert integer_nthroot(-1, 3) == (-1, True)\n    assert integer_nthroot(-27, 3) == (-3, True)\n    assert integer_nthroot(-16, 4) == (2, False)\n    assert integer_nthroot(-27, 4) == (3, False)\n    assert integer_nthroot(-2, 1) == (-2, True)\n    assert integer_nthroot(-4, 1) == (-4, True)\n    assert integer_nthroot(-9, 1) == (-9, True)\n    assert integer_nthroot(-2, -1) == (-1/2, True)\n    assert integer_nthroot(-4, -1) == (-1/4, True)\n    assert integer_nthroot(-9, -1) == (-1/9, True)\n", "def test_Pow_is_integer():\n    i = Symbol('i', integer=True)\n    n = Symbol('n', integer=True, nonnegative=True)\n    m = Symbol('m', integer=True, positive=True)\n    p = Symbol('p', positive=True)\n    x = Symbol('x', extended_real=True)\n    assert (x**p).is_integer is None\n    assert (x**m).is_integer is None\n    assert (x**n).is_integer is None\n    assert (x**i).is_integer is None\n    assert (2**x).is_integer is None\n    assert (2**m).is_integer is True\n    assert (2**i).is_integer is None\n    assert (p**m).is_integer is True\n    assert (p**i).is_integer is None\n    assert (m**n).is_integer is True\n    assert (m**i).is_integer is None\n    assert (i**m).is_integer is True\n    assert (i**n).is_integer is True\n    assert (i**p).is_integer is True\n    assert (i**i).is_integer is True\n    assert (n**m).is_integer is True\n    assert (n**n).is_integer is True\n    assert (n**i).is_integer is True\n    assert (n**p).is_integer is True\n    assert (m**m).is_integer is True\n    assert (m**p).is_integer is True\n    assert ((-1)**n).is_integer is True\n    assert ((-1)**i).is_integer is True\n    assert ((-1)**m).is_integer is True\n    assert ((-1)**p).is_integer is True\n", "def test_Pow_infinite():\n    x = Symbol('x')\n\n    # issue 10866\n    assert Pow(oo, oo, evaluate=False).is_finite is False\n\n    # issue 15611\n    assert Pow(x, zoo, evaluate=False).is_finite is False\n    assert Pow(zoo, x, evaluate=False).is_finite is False\n\n    # issue 16485\n    assert Pow(oo, -oo, evaluate=False).is_finite is False\n    assert Pow(-oo, oo, evaluate=False).is_finite is False\n    assert Pow(-oo, -oo, evaluate=False).is_finite is False\n\n    # issue 17665\n    assert Pow(oo, S.Pi, evaluate=False).is_finite is False\n    assert Pow(oo, oo * I, evaluate=False).is_finite is False\n    assert Pow(oo, -oo * I, evaluate=False).is_finite is False\n\n    # issue 17666\n    assert Pow(oo, S.Half, evaluate=False).is_finite is False\n    assert Pow(-oo, S.Half, evaluate=False).is_finite is False\n    assert Pow(-oo, -S.Half, evaluate=False).is_finite is False\n    assert Pow(-oo, I, evaluate=False).is_finite is False\n    assert Pow(oo, -I, evaluate=False).is_finite is False\n    assert Pow(-oo, I, evaluate=False).is_finite is False\n    assert Pow(-oo, -I, evaluate=False).is_finite is False\n\n    # issue 17667\n    assert Pow(x, oo, evaluate=False).is_finite is None\n    assert Pow(-x, oo, evaluate=False).is_finite is None\n    assert Pow(x, -oo, evaluate=False).is_finite is None\n    assert Pow(-x, -oo, evaluate=False).is_finite is None\n    assert Pow(x, oo * I, evaluate=False).is_finite is None\n    assert Pow(-x, oo * I, evaluate=False).is_finite is None\n    assert Pow(x, -oo * I, evaluate=False).is_finite is None\n    assert Pow(-x, -oo * I, evaluate=False).is_finite is None\n"], "sample_458": ["    def test_ljust_rjust_center(self):\n        self.assertEqual(ljust(\"hello\", 10), \"hello     \")\n        self.assertEqual(rjust(\"hello\", 10), \"     hello\")\n        self.assertEqual(center(\"hello\", 10), \"   hello  \")\n", "def test_non_ascii_input(self):\n    self.assertEqual(floatformat(\"123,456.789\"), \"123,456.789\")\n    self.assertEqual(floatformat(\"123 456,789\"), \"123 456,789\")\n    self.assertEqual(floatformat(\"123.456,789\"), \"123.456,789\")\n    self.assertEqual(floatformat(\"123 456.789\"), \"123 456.789\")\n\n    self.assertEqual(floatformat(\"123,456.789\", 2), \"123,456.79\")\n    self.assertEqual(floatformat(\"123 456,789\", 2), \"123 456,79\")\n    self.assertEqual(floatformat(\"123.456,789\", 2), \"123.456,79\")\n    self.assertEqual(floatformat(\"123 456.789\", 2), \"123 456.79\")\n", "    def test_truncate(self):\n        output = self.engine.render_to_string(\"truncate01\", {\"value\": \"Hello, world!\"})\n        self.assertEqual(output, \"Hello, wo\u2026\")\n", "def test_stringfilter_decorator(self):\n    @stringfilter\n        return value + arg\n\n    self.assertEqual(filter_string(\"Hello\", \" world\"), \"Hello world\")\n    self.assertEqual(filter_string(\"Hello\", 42), \"Hello42\")\n    self.assertEqual(filter_string(123, \"abc\"), \"123abc\")\n    with self.assertRaises(TypeError):\n        filter_string(None, \"abc\")\n    with self.assertRaises(TypeError):\n        filter_string(\"Hello\", None)\n", "def test_floatformat_with_large_exponents(self):\n    self.assertEqual(floatformat(1.5e+100, 2), \"1.50e+100\")\n    self.assertEqual(floatformat(1.5e-100, 2), \"1.50e-100\")\n    self.assertEqual(floatformat(1.5e+100, -2), \"1.50e+100\")\n    self.assertEqual(floatformat(1.5e-100, -2), \"1.50e-100\")\n"], "sample_393": ["    def test_domain_django(self):\n        management.call_command(\"makemessages\", locale=[LOCALE], domain=\"django\", verbosity=0)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertMsgId(\"Translatable literal #6b\", po_contents)\n", "    def test_invalid_domain(self):\n        msg = \"currently makemessages only supports domains 'django' and 'djangojs'\"\n        with self.assertRaisesMessage(CommandError, msg):\n            management.call_command(\n                \"makemessages\", locale=[LOCALE], verbosity=0, domain=\"invalid\"\n            )\n", "    def test_postprocess_messages(self):\n        build_file = MakeMessagesCommand.build_file_class(\n            MakeMessagesCommand(), \"django\", MakeMessagesCommand.translatable_file_class(\n                \"/path/to/file\", \"file.py\", \"/path/to/locale\"\n            )\n        )\n        msgs = \"#: /path/to/file/file.py.py:123\\nmsgid \\\"test\\\"\\nmsgstr \\\"\\\"\"\n        self.assertEqual(\n            build_file.postprocess_messages(msgs),\n            \"#: /path/to/file/file.py:123\\nmsgid \\\"test\\\"\\nmsgstr \\\"\\\"\",\n        )\n", "    def test_symlink_to_non_existent_file(self):\n        if symlinks_supported():\n            # Create a symlink to a file that does not exist.\n            os.symlink(\"non_existent_file.txt\", \"templates/symlink.txt\")\n        else:\n            self.skipTest(\n                \"os.symlink() not available on this OS + Python version combination.\"\n            )\n        management.call_command(\"makemessages\", locale=[LOCALE], verbosity=0)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertNotMsgId(\"This literal should be ignored.\", po_contents)\n", "    def test_locale_dir_not_found(self):\n        \"\"\"\n        makemessages raises an exception if it can't find a locale directory\n        and a file from which it should extract messages.\n        \"\"\"\n        self.default_locale_path = None\n        msg = (\n            \"Unable to find a locale path to store translations for file \"\n            \"__init__.py. Make sure the 'locale' directory exists in an app \"\n            \"or LOCALE_PATHS setting is set.\"\n        )\n        with self.assertRaisesMessage(management.CommandError, msg):\n            management.call_command(\"makemessages\", locale=[LOCALE], verbosity=0)\n        # Working files are cleaned up on an error.\n        self.assertFalse(os.path.exists(\"./__init__.py.py\"))\n"], "sample_397": ["def test_invalid_template_loader(self):\n    \"\"\"\n    Test that an invalid template loader configuration raises an ImproperlyConfigured exception.\n    \"\"\"\n    with self.assertRaisesMessage(\n        ImproperlyConfigured,\n        \"Invalid value in template loaders configuration: 42\",\n    ):\n        DjangoTemplates(\n            {\n                \"DIRS\": [],\n                \"APP_DIRS\": False,\n                \"NAME\": \"django\",\n                \"OPTIONS\": {\n                    \"loaders\": [42],\n                },\n            }\n        )\n", "def test_repr(self):\n    engine = DjangoTemplates(\n        {\n            \"DIRS\": [\"/path/to/templates\"],\n            \"APP_DIRS\": True,\n            \"NAME\": \"django\",\n            \"OPTIONS\": {\n                \"context_processors\": [test_processor_name],\n                \"builtins\": [\"template_backends.apps.good.templatetags.good_tags\"],\n                \"autoescape\": False,\n            },\n        }\n    )\n    expected_repr = (\n        \"<DjangoTemplates: dirs=['/path/to/templates'] app_dirs=True context_processors=['template_tests.test_response.test_processor_name'] debug=False \"\n        \"loaders=[('django.template.loaders.cached.Loader', ['django.template.loaders.filesystem.Loader', 'django.template.loaders.app_directories.Loader'])] \"\n        \"string_if_invalid='' file_charset='utf-8' libraries={} builtins=['django.template.defaulttags', 'django.template.defaultfilters', \"\n        \"'django.template.loader_tags', 'template_backends.apps.good.templatetags.good_tags'] autoescape=False>\"\n    )\n    self.assertEqual(repr(engine.engine), expected_repr)\n", "def test_find_template_loader_invalid_config(self):\n    with self.assertRaisesMessage(\n        ImproperlyConfigured,\n        \"Invalid value in template loaders configuration: 123\",\n    ):\n        DjangoTemplates(\n            {\n                \"DIRS\": [],\n                \"APP_DIRS\": False,\n                \"NAME\": \"django\",\n                \"OPTIONS\": {\"loaders\": [\"django.template.loaders.cached.Loader\", 123]},\n            }\n        )\n\n    with self.assertRaisesMessage(\n        ImproperlyConfigured,\n        \"Invalid value in template loaders configuration: None\",\n    ):\n        DjangoTemplates(\n            {\n                \"DIRS\": [],\n                \"APP_DIRS\": False,\n                \"NAME\": \"django\",\n                \"OPTIONS\": {\"loaders\": [\"django.template.loaders.cached.Loader\", None]},\n            }\n        )\n\n    with self.assertRaisesMessage(\n        ImproperlyConfigured,\n        \"Invalid value in template loaders configuration: {}\",\n    ):\n        DjangoTemplates(\n            {\n                \"DIRS\": [],\n                \"APP_DIRS\": False,\n                \"NAME\": \"django\",\n                \"OPTIONS\": {\"loaders\": [\"django.template.loaders.cached.Loader\", {}]},\n            }\n        )\n", "    def test_find_template_loader_invalid_config(self):\n        engine = DjangoTemplates(\n            {\n                \"DIRS\": [],\n                \"APP_DIRS\": False,\n                \"NAME\": \"django\",\n                \"OPTIONS\": {},\n            }\n        )\n\n        with self.assertRaisesMessage(\n            ImproperlyConfigured,\n            \"Invalid value in template loaders configuration: %r\" % \"invalid_loader\",\n        ):\n            engine.find_template_loader(\"invalid_loader\")\n\n        with self.assertRaisesMessage(\n            ImproperlyConfigured,\n            \"Invalid value in template loaders configuration: %r\" % [\"invalid_loader\"],\n        ):\n            engine.find_template_loader([\"invalid_loader\"])\n\n        with self.assertRaisesMessage(\n            ImproperlyConfigured,\n            \"Invalid value in template loaders configuration: %r\" % (\"invalid_loader\",),\n        ):\n            engine.find_template_loader((\"invalid_loader\",))\n\n        engine = DjangoTemplates(\n            {\n                \"DIRS\": [],\n                \"APP_DIRS\": True,\n                \"NAME\": \"django\",\n                \"OPTIONS\": {\n                    \"loaders\": [\"django.template.loaders.filesystem.Loader\"],\n                },\n            }\n        )\n        with self.assertRaisesMessage(\n            ImproperlyConfigured,\n            \"app_dirs must not be set when loaders is defined.\",\n        ):\n            engine.get_template_loaders(engine.loaders)\n", "    def test_default(self):\n        \"\"\"\n        Test that the get_default() method returns the first DjangoTemplates backend.\n        \"\"\"\n        templates = [\n            {\n                \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n                \"NAME\": \"django\",\n                \"OPTIONS\": {},\n            },\n            {\n                \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n                \"NAME\": \"other\",\n                \"OPTIONS\": {},\n            },\n        ]\n        engines = EngineHandler(templates=templates)\n        self.assertEqual(engines[\"django\"].engine, Engine.get_default())\n"], "sample_968": ["def test_python_domain_clear_doc(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:method:: meth1\\n\"\n            \"\\n\"\n            \"   .. py:method:: meth2\\n\")\n    domain = app.env.get_domain('py')\n    restructuredtext.parse(app, text)\n    assert len(domain.data['objects']) == 3\n    domain.clear_doc('index')\n    assert len(domain.data['objects']) == 0\n", "def test_pyfield_options(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :type: Optional[str]\\n\"\n            \"      :value: ''\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr2\\n\"\n            \"      :type: int\\n\"\n            \"      :value: 42\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr3\\n\"\n            \"      :type: List[int]\\n\"\n            \"      :value: [1, 2, 3]\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, (\"class\", desc_sig_space)],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc,\n                                                  addnodes.index,\n                                                  desc,\n                                                  addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'attr (Class attribute)', 'Class.attr', '', None)])\n    assert_node(doctree[1][1][1], addnodes.index,\n                entries=[('single', 'attr2 (Class attribute)', 'Class.attr2', '', None)])\n    assert_node(doctree[1][1][2], addnodes.index,\n                entries=[('single', 'attr3 (Class attribute)', 'Class.attr3', '', None)])\n    assert_node(doctree[1][1][0][0][1][1],\n                ([desc_annotation, ([desc_sig_punctuation, ':'],\n                                    desc_sig_space,\n                                    [pending_xref, \"Optional\"],\n                                    [desc_sig_punctuation, \"[\"],\n                                    [pending_xref, \"str\"],\n                                    [desc_sig_punctuation, \"]\"])],\n                 [desc_annotation, (desc_sig_space,\n                                    [desc_sig_punctuation, '='],\n                                    desc_sig_space,\n                                    \"''\")]))\n    assert_node(doctree[1][1][1][0][1][1],\n                ([desc_annotation, ([desc_sig_punctuation, ':'],\n                                    desc_sig_space,\n                                    [pending_xref, \"int\"])],\n                 [desc_annotation, (desc_sig_space,\n                                    [desc_sig_punctuation, '='],\n                                    desc", "def test_get_full_qualified_name_edge_cases(app):\n    env = Mock(domaindata={})\n    domain = PythonDomain(env)\n\n    # None reftarget\n    node = nodes.reference(reftarget=None)\n    assert domain.get_full_qualified_name(node) is None\n\n    # invalid reftarget\n    node = nodes.reference(reftarget=' invalid')\n    assert domain.get_full_qualified_name(node) is None\n\n    # py:module context without reftarget\n    kwargs = {'py:module': 'module1'}\n    node = nodes.reference(**kwargs)\n    assert domain.get_full_qualified_name(node) is None\n\n    # py:class context without reftarget\n    kwargs = {'py:class': 'Class'}\n    node = nodes.reference(**kwargs)\n    assert domain.get_full_qualified_name(node) is None\n\n    # both py:module and py:class context without reftarget\n    kwargs = {'py:module': 'module1', 'py:class': 'Class'}\n    node = nodes.reference(**kwargs)\n    assert domain.get_full_qualified_name(node) is None\n", "def test_python_module_index_with_submodule(app):\n    text = (\".. py:module:: sphinx\\n\"\n            \".. py:module:: sphinx.config\\n\"\n            \".. py:module:: sphinx.builders\\n\"\n            \".. py:module:: sphinx.builders.html\\n\"\n            \".. py:module:: sphinx_intl\\n\")\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    result = index.generate()\n    assert len(result[0]) == 3\n    assert result[0][0][0] == 'b'\n    assert result[0][1][0] == 's'\n    assert result[0][2][0] == 's'\n    assert len(result[0][0][1]) == 0\n    assert len(result[0][1][1]) == 2\n    assert result[0][1][1][0][0] == 'sphinx'\n    assert result[0][1][1][1][0] == 'sphinx_intl'\n    assert len(result[0][2][1]) == 2\n    assert result[0][2][1][0][0] == 'sphinx.builders'\n    assert result[0][2][1][1][0] == 'sphinx.builders.html'\n    assert result[0][2][1][0][2] == 'index'\n    assert result[0][2][1][0][3] == 'module-sphinx.builders'\n    assert result[0][2][1][0][4] == ''\n    assert result[0][2][1][0][5] == ''\n    assert result[0][2][1][0][6] == ''\n    assert result[0][2][1][1][2] == 'index'\n    assert result[0][2][1][1][3] == 'module-sphinx.builders.html'\n    assert result[0][2][1][1][4] == ''\n    assert result[0][2][1][1][5] == ''\n    assert result[0][2][1][1][6] == ''\n    assert result[1] == False\n", "def test_type_to_xref():\n    env = Mock()\n    env.config.python_use_unqualified_type_names = False\n\n    # test class reference\n    target = 'List'\n    assert_node(type_to_xref(target, env),\n                (pending_xref, ([pending_xref],),\n                 refdomain=\"py\", reftype=\"class\", reftarget=\"List\"))\n\n    # test object reference\n    target = 'None'\n    assert_node(type_to_xref(target, env),\n                (pending_xref, ([pending_xref],),\n                 refdomain=\"py\", reftype=\"obj\", reftarget=\"None\"))\n\n    # test unqualified type names\n    env.config.python_use_unqualified_type_names = True\n    target = 'List'\n    assert_node(type_to_xref(target, env),\n                (pending_xref, ([pending_xref_condition, ([nodes.Text, \"List\"],),\n                                 pending_xref_condition, ([pending_xref, \"List\"],)],\n                                refdomain=\"py\", reftype=\"class\", reftarget=\"List\"))\n\n    # test unqualified type names (shortname)\n    target = 'List.List'\n    assert_node(type_to_xref(target, env),\n                (pending_xref, ([pending_xref_condition, ([nodes.Text, \"List\"],),\n                                 pending_xref_condition, ([pending_xref, \"List.List\"],)],\n                                refdomain=\"py\", reftype=\"class\", reftarget=\"List.List\"))\n\n    # test unqualified type names (module)\n    env.ref_context['py:module'] = 'mymodule'\n    target = 'List'\n    assert_node(type_to_xref(target, env),\n                (pending_xref, ([pending_xref_condition, ([nodes.Text, \"List\"],),\n                                 pending_xref_condition, ([pending_xref, \"mymodule.List\"],)],\n                                refdomain=\"py\", reftype=\"class\", reftarget=\"List\",\n                                py_module=\"mymodule\"))\n\n    # test unqualified type names (class)\n    env.ref_context['py:class'] = 'MyClass'\n    target = 'List'\n    assert_node(type_to_xref(target, env),\n                (pending_xref, ([pending_xref_condition, ([nodes.Text, \"List\"],),\n                                 pending_xref_condition, ([pending_xref, \"MyClass.List\"],)],\n                                refdomain=\"py\", reftype=\"class\", reftarget=\"List\",\n                                py_module=None, py_class=\"MyClass"], "sample_83": ["    def test_init(self):\n        func = lambda: None\n        takes_context = True\n        args = []\n        kwargs = {}\n        node = TagHelperNode(func, takes_context, args, kwargs)\n        self.assertEqual(node.func, func)\n        self.assertEqual(node.takes_context, takes_context)\n        self.assertEqual(node.args, args)\n        self.assertEqual(node.kwargs, kwargs)\n", "    def test_init(self):\n        library = Library()\n        self.assertEqual(library.filters, {})\n        self.assertEqual(library.tags, {})\n", "    def setUp(self):\n        self.parser = object()  # Mock parser object\n", "    def setUp(self):\n        self.library = Library()\n        self.context = {}\n", "    def test_get_resolved_arguments(self):\n            pass\n        node = TagHelperNode(func, False, [1, 2, 3], {'a': 4, 'b': 5})\n        resolved_args, resolved_kwargs = node.get_resolved_arguments({1: 'one', 2: 'two', 3: 'three', 4: 'four', 5: 'five'})\n        self.assertEqual(resolved_args, ['one', 'two', 'three'])\n        self.assertEqual(resolved_kwargs, {'a': 'four', 'b': 'five'})\n"], "sample_102": ["def test_union_with_distinct_fields(self):\n    qs1 = Number.objects.filter(num__lte=1).values('num').distinct('num')\n    qs2 = Number.objects.filter(num__gte=2, num__lte=3).values('num')\n    self.assertCountEqual(qs1.union(qs2).values_list('num', flat=True), [0, 1, 2, 3])\n", "def test_union_with_values_list_and_values(self):\n    ReservedName.objects.create(name='rn1', order=1)\n    ReservedName.objects.create(name='rn2', order=2)\n    qs1 = ReservedName.objects.values_list('name', flat=True)\n    qs2 = ReservedName.objects.values('name')\n    self.assertCountEqual(qs1.union(qs2), ['rn1', 'rn2'])\n", "def test_union_with_values_list_and_select_for_update(self):\n    ReservedName.objects.bulk_create([\n        ReservedName(name='rn1', order=7),\n        ReservedName(name='rn2', order=5),\n        ReservedName(name='rn0', order=6),\n        ReservedName(name='rn9', order=-1),\n    ])\n    qs1 = ReservedName.objects.filter(order__gte=6).select_for_update()\n    qs2 = ReservedName.objects.filter(order__lte=5)\n    union_qs = qs1.union(qs2)\n    with self.assertRaisesMessage(DatabaseError, \"FOR UPDATE clause is not allowed in subqueries\"):\n        list(union_qs.values_list('order', flat=True))\n\n    qs1 = ReservedName.objects.filter(order__gte=6)\n    qs2 = ReservedName.objects.filter(order__lte=5).select_for_update()\n    union_qs = qs1.union(qs2)\n    with self.assertRaisesMessage(DatabaseError, \"FOR UPDATE clause is not allowed in subqueries\"):\n        list(union_qs.values_list('order', flat=True))\n", "def test_values_list_with_renamed_annotations(self):\n    qs1 = Number.objects.annotate(\n        num_alias=F('num'),\n    ).values_list('num_alias')\n    qs2 = Number.objects.annotate(\n        num_alias=F('other_num'),\n    ).values_list('num_alias')\n    self.assertCountEqual(qs1.union(qs2).order_by('num_alias'), [(i,) for i in range(10, -1, -1)])\n", "    def test_union_with_values_and_distinct(self):\n        ReservedName.objects.create(name='a', order=2)\n        qs1 = ReservedName.objects.filter(order=2).values('name', 'order', 'id')\n        qs2 = ReservedName.objects.all()\n        reserved_name = qs1.union(qs2).distinct().get()\n        self.assertEqual(reserved_name['name'], 'a')\n        self.assertEqual(reserved_name['order'], 2)\n        reserved_name = qs1.union(qs2).distinct().values_list('name', 'order', 'id').get()\n        self.assertEqual(reserved_name[:2], ('a', 2))\n"], "sample_1107": ["def test_signed_permutations():\n    assert list(signed_permutations((1, 2))) == [\n        (1, 2), (-1, 2), (1, -2), (-1, -2),\n        (2, 1), (-2, 1), (2, -1), (-2, -1)]\n    assert list(signed_permutations((1, 1, 1))) == [\n        (1, 1, 1), (-1, 1, 1), (1, -1, 1), (-1, -1, 1),\n        (1, 1, -1), (-1, 1, -1), (1, -1, -1), (-1, -1, -1)]\n", "def test_permute_signs():\n    # Test tuples\n    assert list(permute_signs((0, 1, 2))) == [(0, 1, 2), (0, -1, 2), (0, 1, -2), (0, -1, -2)]\n    # Test lists\n    assert list(permute_signs([0, 1, 2])) == [[0, 1, 2], [0, -1, 2], [0, 1, -2], [0, -1, -2]]\n    # Test other iterable\n    assert list(permute_signs({1, 2, 3})) == [[1, 2, 3], [1, 2, -3], [1, -2, 3], [1, -2, -3],\n                                               [-1, 2, 3], [-1, 2, -3], [-1, -2, 3], [-1, -2, -3]]\n", "def test_permute_signs():\n    assert list(permute_signs((0, 1, 2))) == [\n        (0, 1, 2), (0, -1, 2), (0, 1, -2), (0, -1, -2)]\n    assert list(permute_signs((-1, -2, -3))) == [\n        (-1, -2, -3), (-1, -2, 3), (-1, 2, -3), (-1, 2, 3),\n        (1, -2, -3), (1, -2, 3), (1, 2, -3), (1, 2, 3)]\n    assert list(permute_signs((0, 1, -2, 0))) == [\n        (0, 1, -2, 0), (0, 1, -2, 0), (0, 1, 2, 0), (0, 1, 2, 0),\n        (0, -1, -2, 0), (0, -1, -2, 0), (0, -1, 2, 0), (0, -1, 2, 0)]\n    assert list(permute_signs((-1, 1, 1, 1, 1))) == [\n        (-1, 1, 1, 1, 1), (-1, 1, 1, 1, -1), (-1, 1, 1, -1, 1),\n        (-1, 1, 1, -1, -1), (-1, 1, -1, 1, 1), (-1, 1, -1, 1, -1),\n        (-1, 1, -1, -1, 1), (-1, 1, -1, -1, -1), (-1, -1, 1, 1, 1),\n        (-1, -1, 1, 1, -1), (-1, -1, 1, -1, 1), (-1, -1, 1, -1, -1),\n        (-1, -1, -1, 1, 1), (-1, -1, -1", "def test_permute_signs():\n    assert list(permute_signs((1, 0, 2))) == [(1, 0, 2), (-1, 0, 2), (1, 0, -2),\n        (-1, 0, -2), (1, 0, 2), (-1, 0, 2), (1, 0, -2), (-1, 0, -2)]\n    assert list(permute_signs((1, 2, 3))) == [(1, 2, 3), (-1, 2, 3), (1, -2, 3),\n        (-1, -2, 3), (1, 2, -3), (-1, 2, -3), (1, -2, -3), (-1, -2, -3)]\n    assert list(permute_signs((0, 0))) == [(0, 0)]\n    assert list(permute_signs((0, 1))) == [(0, 1), (0, -1)]\n    assert list(permute_signs((0, 0, 1, 2, 3))) == [\n        (0, 0, 1, 2, 3), (0, 0, -1, 2, 3), (0, 0, 1, -2, 3), (0, 0, -1, -2, 3),\n        (0, 0, 1, 2, -3), (0, 0, -1, 2, -3), (0, 0, 1, -2, -3), (0, 0, -1, -2, -3)]\n", "def test_permute_signs():\n    assert list(permute_signs((1, 2, 3))) == [\n        (1, 2, 3), (-1, 2, 3), (1, -2, 3), (1, 2, -3), (-1, -2, 3), (-1, 2, -3),\n        (-1, -2, -3), (1, -2, -3), (-1, 1, -2, 3), (1, -1, 2, 3), (1, 1, -2, -3),\n        (1, 1, -2, 3), (1, 1, 2, -3), (1, 1, 2, 3), (-1, -1, 2, 3), (-1, 1, -2, 3),\n        (-1, 1, 2, -3), (-1, 1, 2, 3), (-1, -1, -2, -3), (-1, -1, -2, 3),\n        (-1, -1, 2, -3), (-1, -1, 2, 3)]\n    assert list(permute_signs([1, 2, 3])) == [\n        (1, 2, 3), (-1, 2, 3), (1, -2, 3), (1, 2, -3), (-1, -2, 3), (-1, 2, -3),\n        (-1, -2, -3), (1, -2, -3), (-1, 1, -2, 3), (1, -1, 2, 3), (1, 1, -2, -3),\n        (1, 1, -2, 3), (1, 1, 2, -3), (1, 1, 2, 3), (-1, -1, 2, 3), (-1, 1, -2, 3),\n        (-1, 1, 2, -3), (-1, 1, 2, 3), (-1, -1, -2, -3), (-1, -1, -2, 3"], "sample_345": ["    def test_watch_dir_with_non_existent_directory(self):\n        reloader = autoreload.WatchmanReloader()\n        with mock.patch.object(reloader, '_watch_root') as mocked_watch_root:\n            reloader._watch_glob(self.tempdir / 'does_not_exist', ['*'])\n            self.assertIn('glob-parent-does_not_exist:%s' % self.tempdir, mocked_watch_root.call_args[0][1])\n", "    def setUp(self):\n        self.reloader = autoreload.StatReloader()\n", "    def test_update_watches_no_subscriptions(self, mocked_update_watches):\n        # If the WatchmanReloader.update_watches() method is called with no subscriptions,\n        # it should not raise any errors.\n        self.reloader.update_watches()\n        mocked_update_watches.assert_called_once()\n", "    def test_subscribed_to_subscription_errors(self, mocked_client):\n        class MockSubscriptionError(Exception):\n            pass\n        mocked_client.return_value.__enter__.return_value.subscription.side_effect = MockSubscriptionError()\n        with self.assertRaises(autoreload.WatchmanUnavailable):\n            autoreload.WatchmanReloader().tick()\n", "    def test_multiple_subscriptions(self, mocked_update_watches, mocked_client, mocked_subscription):\n        mocked_client.subs = {\n            'files:/test': [{'subscription': 'files:/test'}],\n            'glob:/test': [{'subscription': 'glob:/test'}],\n        }\n        reloader = autoreload.WatchmanReloader()\n        reloader._subscribe_dir('/test', ['test_file.py'])\n        reloader._subscribe('/test', 'files:/test', ['anyof', ['type', 'f']])\n        reloader._subscribe('/test', 'glob:/test', ['anyof', ['match', '*.py', 'wholename']])\n        reloader.tick()\n        self.assertEqual(mocked_subscription.call_count, 2)\n"], "sample_248": ["    def test_shell_with_no_interfaces_installed(self, select):\n        select.return_value = ([], [], [])\n        with self.assertRaisesMessage(CommandError, \"Couldn't import python interface.\"):\n            call_command('shell')\n", "def test_shell_with_invalid_interface(self):\n    with self.assertRaisesMessage(CommandError, \"Couldn't import invalid interface.\"):\n        call_command('shell', interface='invalid')\n", "def test_shell_with_no_interfaces(self, select):\n    select.return_value = ([], [], [])\n    with self.assertRaisesMessage(CommandError, \"Couldn't import python interface.\"):\n        call_command('shell')\n", "def test_shell_with_no_interface(self, select):\n    select.return_value = ([], [], [])\n    with self.assertRaisesMessage(CommandError, \"Couldn't import python interface.\"):\n        call_command('shell', interface='unknown')\n", "    def test_shell_with_all_interfaces_not_installed(self, select):\n        select.return_value = ([], [], [])\n        with self.assertRaisesMessage(CommandError, \"Couldn't import python interface.\"):\n            call_command('shell', interface='python')\n"], "sample_979": ["def test_adjoint():\n    A = MatrixSymbol('A', n, m)\n    Ad = Adjoint(A)\n    assert Ad.shape == (m, n)\n    assert Ad.base == A\n    assert Ad.conjugate() == Transpose(A)\n    assert Ad.transpose() == Adjoint(A.conjugate())\n", "def test_MatrixElement_simplify():\n    A = MatrixSymbol('A', 1, 1)\n    assert simplify(A[0, 0] + ImmutableMatrix([[sin(x)**2 + cos(x)**2]])[0, 0]) == \\\n        A[0, 0] + 1\n    assert simplify(A[0, 0] * ImmutableMatrix([[sin(x)**2 + cos(x)**2]])[0, 0]) == \\\n        A[0, 0]\n", "def test_MatrixElement_properties():\n    i, j = symbols('i j')\n    assert MatrixElement.parent.is_MatrixExpr\n    assert MatrixElement.parent.is_Matrix\n    assert not MatrixElement.parent.is_commutative\n    assert MatrixElement.i.is_integer\n    assert MatrixElement.j.is_integer\n    assert MatrixElement._diff_wrt\n    assert MatrixElement.is_symbol\n    assert MatrixElement.is_commutative\n", "def test_matrixelement_complex():\n    A = MatrixSymbol('A', 2, 2)\n    assert A[1, 0].conjugate() == A[1, 0].conjugate().conjugate()\n    assert A[1, 0].conjugate() != A[1, 0]\n    assert A[0, 0].conjugate() == A[0, 0]\n    assert (2*A[1, 1] + A[1, 1].conjugate()).conjugate() == 2*A[1, 1].conjugate() + A[1, 1]\n", "def test_MatrixElement_subs():\n    A = MatrixSymbol('A', 3, 3)\n    i, j, k, l = symbols('i j k l')\n    assert A[i, j].subs({A: ImmutableMatrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])}) == ImmutableMatrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])[i, j]\n    assert A[i, j].subs({i: k, j: l}) == A[k, l]\n    assert A[i, j].subs({k: i, l: j}) == A[k, l]\n    assert A[i, j].subs({i: 0, j: 1}) == A[0, 1]\n    assert A[i, j].subs({i: 0, j: 0}) == A[0, 0]\n    raises(TypeError, lambda: A[i, j].subs({A[i, j]: 0}))\n"], "sample_1199": ["def test_tensor_product_anticommutator():\n    assert tensor_product_simp(AntiCommutator(TP(A, B), TP(B, C)).doit()) == \\\n        TP(A*B, B*C) + TP(B*A, C*B)\n    assert AntiCommutator(TP(A, B), TP(B, C)).doit().expand(tensorproduct=True) == \\\n        TP(A*B, B*C) + TP(B*A, C*B)\n", "def test_tensor_product_anticommutator():\n    assert tensor_product_simp(AntiCommutator(TP(A, B), TP(B, C)).doit()) == \\\n        TP(A, B)*TP(B, C) + TP(B, C)*TP(A, B)\n", "def test_tensor_product_latex():\n    assert TensorProduct(A, B)._latex() == '{A}\\\\otimes {B}'\n    assert TensorProduct(A, B, C)._latex() == '{A}\\\\otimes {B}\\\\otimes {C}'\n    assert TensorProduct(A**2, B)._latex() == '{A^{2}}\\\\otimes {B}'\n    assert TensorProduct(A, B + C)._latex() == '{A}\\\\otimes \\\\left({B} + {C}\\\\right)'\n    assert TensorProduct(A + B, C)._latex() == '{\\\\left({A} + {B}\\\\right)}\\\\otimes {C}'\n", "def test_tensor_product_simp_with_commutator():\n    # tests for Commutators\n    assert tensor_product_simp(Comm(TP(A, B), TP(C, D))) == \\\n        TP(A*B, C*D) - TP(A*D, B*C)\n    assert tensor_product_simp(TP(A, B)*Comm(TP(C, D), TP(E, F))) == \\\n        TP(A*C, B*E)*TP(I, F) - TP(A*D, B*F)*TP(I, E)\n    assert tensor_product_simp(Comm(TP(A, B), TP(C, D))*TP(E, F)) == \\\n        TP(I, C)*TP(A, B)*TP(I, E)*TP(F, D) - TP(I, D)*TP(A, B)*TP(I, F)*TP(E, C)\n", "def test_tensor_product_simp_Mul():\n    # test that tensor_product_simp_Mul can handle complex expressions\n    A, B, C, D, E, F = symbols('A B C D E F', commutative=False)\n    assert tensor_product_simp_Mul(A * TP(B, C) * TP(D, E)) == \\\n        TP(A*B*D, C*E)\n    assert tensor_product_simp_Mul(TP(A, B) * TP(C, D) * TP(E, F)) == \\\n        TP(A*C*E, B*D*F)\n    # Test that tensor_product_simp_Mul can handle powers\n    assert tensor_product_simp_Mul(TP(A, B)**2 * TP(C, D)) == \\\n        TP(A**2*C, B**2*D)\n    # Test that tensor_product_simp_Mul can handle negative numbers\n    assert tensor_product_simp_Mul(-TP(A, B) * TP(C, D)) == \\\n        -TP(A*C, B*D)\n"], "sample_837": ["def test_get_blas_info():\n    try:\n        blas_info = _get_blas_info()\n        assert 'macros' in blas_info\n        assert 'lib_dirs' in blas_info\n        assert 'cblas_libs' in blas_info\n    except ImportError:\n        pass\n", "def test_get_blas_info():\n    try:\n        blas_info = _get_blas_info()\n        assert 'macros' in blas_info\n        assert 'lib_dirs' in blas_info\n        assert 'cblas_libs' in blas_info\n    except ImportError:\n        # If sklearn is not installed, _get_blas_info will fail\n        pass\n    except Exception as e:\n        # Any other exception should be reported as a test failure\n        assert False, f\"Unexpected exception: {e}\"\n", "def test_get_blas_info(monkeypatch):\n    import sklearn.utils._show_versions as show_versions\n    from unittest.mock import MagicMock\n    from sklearn.utils._build_utils import get_blas_info\n\n    # mock the get_blas_info function\n    mock_get_blas_info = MagicMock(return_value=(['lib1', 'lib2'], {'define_macros': [('macro1', 'value1'), ('macro2', 'value2')], 'library_dirs': ['/lib/dir1', '/lib/dir2']}))\n    monkeypatch.setattr(get_blas_info, 'get_blas_info', mock_get_blas_info)\n\n    blas_info = show_versions._get_blas_info()\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n    assert blas_info['macros'] == 'macro1=value1, macro2=value2'\n    assert blas_info['lib_dirs'] == '/lib/dir1:/lib/dir2'\n    assert blas_info['cblas_libs'] == 'lib1, lib2'\n", "def test_get_blas_info():\n    try:\n        blas_info = _get_blas_info()\n        assert 'macros' in blas_info\n        assert 'lib_dirs' in blas_info\n        assert 'cblas_libs' in blas_info\n    except ImportError:\n        # _get_blas_info relies on scikit-learn's _build_utils which may not be available in all environments\n        pass\n\n", "def test_get_blas_info():\n    try:\n        blas_info = _get_blas_info()\n\n        assert 'macros' in blas_info\n        assert 'lib_dirs' in blas_info\n        assert 'cblas_libs' in blas_info\n    except ImportError:\n        # If scikit-learn is not installed, the test will fail.\n        # We need to install scikit-learn for the test to pass.\n        pass\n"], "sample_2": ["def test_write_read_multiextensionfits_metadata(tmpdir):\n    # Test that if a metadata is present it is saved and loaded by default.\n    ccd_data = create_ccd_data()\n    meta = {'author': 'John Doe', 'date': '2022-01-01'}\n    ccd_data.meta = meta\n    filename = tmpdir.join('afile.fits').strpath\n    ccd_data.write(filename)\n    ccd_after = CCDData.read(filename)\n    assert ccd_after.meta == meta\n", "def test_mask_init_with_non_boolean_array(tmpdir):\n    ccd_data = create_ccd_data()\n    ccd_data.mask = np.ones_like(ccd_data)\n    with pytest.raises(ValueError):\n        ccd_data.to_hdu()\n", "def test_uncertainty_unit_equivalence():\n    unit = u.adu\n    std_dev_uncertainty_unit = unit\n    variance_uncertainty_unit = (unit ** 2)\n    inverse_variance_uncertainty_unit = (1 / (unit ** 2))\n\n    assert _uncertainty_unit_equivalent_to_parent(StdDevUncertainty, std_dev_uncertainty_unit, unit)\n    assert not _uncertainty_unit_equivalent_to_parent(StdDevUncertainty, variance_uncertainty_unit, unit)\n    assert not _uncertainty_unit_equivalent_to_parent(StdDevUncertainty, inverse_variance_uncertainty_unit, unit)\n\n    assert not _uncertainty_unit_equivalent_to_parent(VarianceUncertainty, std_dev_uncertainty_unit, unit)\n    assert _uncertainty_unit_equivalent_to_parent(VarianceUncertainty, variance_uncertainty_unit, unit)\n    assert not _uncertainty_unit_equivalent_to_parent(VarianceUncertainty, inverse_variance_uncertainty_unit, unit)\n\n    assert not _uncertainty_unit_equivalent_to_parent(InverseVariance, std_dev_uncertainty_unit, unit)\n    assert not _uncertainty_unit_equivalent_to_parent(InverseVariance, variance_uncertainty_unit, unit)\n    assert _uncertainty_unit_equivalent_to_parent(InverseVariance, inverse_variance_uncertainty_unit, unit)\n", "def test_insert_in_metadata_fits_safe():\n    ccd_data = create_ccd_data()\n    long_key = 'this-is-a-long-key-that-should-be-shortened'\n    long_value = 'this-is-a-long-value-that-should-be-shortened-too'\n    ccd_data._insert_in_metadata_fits_safe(long_key, long_value)\n    short_key = long_key[:8]\n    assert short_key in ccd_data.meta\n    assert ccd_data.meta[short_key] == long_value\n    assert f'HIERARCH {long_key.upper()}' in ccd_data.meta\n    assert ccd_data.meta[f'HIERARCH {long_key.upper()}'] == (short_key,\n                                                            f'Shortened name for {long_key}')\n", "def test_read_fits_with_oddly_ordered_keywords(tmpdir):\n    # https://github.com/astropy/astropy/issues/9379\n    # Test that a FITS file with WCS keywords in nonstandard order can still\n    # be read correctly.\n\n    header = textwrap.dedent(\"\"\"\n        SIMPLE  =                    T\n        PC1_2   =           0.00000000\n        PC2_2   =           0.00000000\n        CTYPE1  = 'RA---TAN'\n        CTYPE2  = 'DEC--TAN'\n        CDELT1  =           0.00250000\n        CDELT2  =           0.00250000\n        PC2_1   =           0.00000000\n        CRVAL1  =        121.93141941\n        CRVAL2  =         21.76541844\n        PC1_1   =           0.00000000\n        CRPIX1  =           500.00000000\n        CRPIX2  =           500.00000000\n        CUNIT1  = 'deg     '\n        CUNIT2  = 'deg     '\n    \"\"\")\n\n    hdu = fits.PrimaryHDU(np.ones((10, 10)), header=fits.Header.fromstring(header))\n    filename = tmpdir.join('oddly_ordered.fits').strpath\n    hdu.writeto(filename)\n\n    ccd = CCDData.read(filename, unit='adu')\n    assert ccd.wcs is not None\n"], "sample_518": ["def test_fancyarrowpatch_units(fig_test, fig_ref):\n    # Smoke test to check that FancyArrowPatch works with units\n    from datetime import datetime\n    fig_test.subplots()\n    patch = FancyArrowPatch((0, 0), (0.01, 0.01), mutation_scale=20)\n    fig_test.gca().add_patch(patch)\n    fig_test.subplots()\n    patch = FancyArrowPatch((0, datetime(2000, 1, 1)), (0.01, datetime(2000, 1, 1)), mutation_scale=20)\n    fig_ref.gca().add_patch(patch)\n", "def test_boxstyle_register():\n    class TestBoxStyle(BoxStyle._Base):\n            return Path.unit_rectangle()\n\n    BoxStyle.register(\"Test\", TestBoxStyle)\n    style = BoxStyle(\"Test\")\n    assert style(x0=0, y0=0, width=1, height=1, mutation_size=1).codes == [Path.MOVETO,\n                                                                          Path.LINETO,\n                                                                          Path.LINETO,\n                                                                          Path.LINETO,\n                                                                          Path.CLOSEPOLY]\n", "def test_connection_patch_arrows():\n    fig, ax1 = plt.subplots()\n    fig, ax2 = plt.subplots()\n    con = mpatches.ConnectionPatch(xyA=(0.1, 0.1), xyB=(0.9, 0.9),\n                                   coordsA='data', coordsB='data',\n                                   axesA=ax2, axesB=ax1,\n                                   arrowstyle=\"-|>\", shrinkA=0, shrinkB=0)\n    ax2.add_artist(con)\n\n    ax1.set_xlim(0, 1)\n    ax1.set_ylim(0, 1)\n    ax2.set_xlim(0, 1)\n    ax2.set_ylim(0, 1)\n", "def test_boxstyle():\n    \"\"\"\n    Check that BoxStyle classes work as expected.\n    \"\"\"\n    # Test Round4\n    boxstyle = BoxStyle.Round4()\n    path = boxstyle(0, 0, 1, 1, 1)\n    assert np.allclose(path.vertices[0], [0, 0])\n    assert np.allclose(path.vertices[-1], [0, 0])\n\n    # Test Sawtooth\n    boxstyle = BoxStyle.Sawtooth()\n    path = boxstyle(0, 0, 1, 1, 1)\n    assert np.allclose(path.vertices[0], [0, 0])\n    assert np.allclose(path.vertices[-1], [0, 0])\n", "def test_boxstyle_setters():\n    fig, ax = plt.subplots()\n    patch = FancyBboxPatch((0.5, 0.5), 1, 1)\n    ax.add_patch(patch)\n\n    with pytest.raises(ValueError, match=\"boxstyle='custom' is deprecated\"):\n        patch.set_boxstyle(\"custom\")\n\n    with pytest.warns(UserWarning, match=\"boxstyles must be callable without \"\n                                      \"the 'mutation_aspect' parameter\"):\n        patch.set_boxstyle(\"Round\", pad=0.2)\n\n    patch.set_boxstyle(\"Round\", pad=0.2)\n    assert isinstance(patch.get_boxstyle(), BoxStyle.Round)\n\n    assert patch.get_boxstyle().pad == 0.2\n"], "sample_480": ["    def test_key_text_transform_from_lookup_nested(self):\n        qs = NullableJSONModel.objects.annotate(\n            b=KT(\"value__baz__a\"),\n        ).filter(b__iexact=\"B\")\n        self.assertSequenceEqual(qs, [self.objs[7]])\n        qs = NullableJSONModel.objects.annotate(\n            c=KT(\"value__bax__foo\"),\n        ).filter(c__contains=\"ar\")\n        self.assertSequenceEqual(qs, [self.objs[7]])\n", "    def test_create_key_transform(self):\n        factory = KeyTransformFactory(\"key\")\n        key_transform = factory(\"value\")\n        self.assertIsInstance(key_transform, KeyTransform)\n        self.assertEqual(key_transform.key_name, \"key\")\n        self.assertEqual(key_transform.lhs, \"value\")\n", "    def test_text_lookup_mixin(self):\n        obj = NullableJSONModel.objects.create(value={\"foo\": \"bar\"})\n        qs = NullableJSONModel.objects.annotate(\n            char_value=KeyTextTransform(\"foo\", \"value\"),\n        ).filter(char_value__icontains=\"BaR\")\n        self.assertSequenceEqual(qs, [obj])\n", "    def test_list_value(self):\n        value = [{\"a\": 1}, {\"b\": \"x\"}]\n        obj = NullableJSONModel.objects.create(value=value)\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__1__in=[{\"b\": \"x\"}, {\"c\": \"y\"}]),\n            [obj],\n        )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__1__in=[{\"b\": \"y\"}, {\"c\": \"y\"}]),\n            [],\n        )\n", "    def test_key_transform_raw_sql_with_value(self):\n        obj = NullableJSONModel.objects.create(value={\"d\": [\"e\", {\"f\": \"g\"}]})\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__d=RawSQL(\n                    \"%s\",\n                    [json.dumps({\"f\": \"g\"})],\n                    output_field=models.JSONField(),\n                )\n            ),\n            [obj],\n        )\n"], "sample_384": ["    def test_prefetch_related_cannot_use_raw_querysets(self):\n        with self.assertRaises(ValueError):\n            Note.objects.prefetch_related(\"tag__note_set__raw_note_set\")\n", "    def test_iterate_over_queryset_twice(self):\n        # Test that a QuerySet can be iterated over twice.\n        notes = [Note.objects.create(note=str(i)) for i in range(10)]\n        queryset = Note.objects.all()\n        self.assertEqual(list(queryset), notes)\n        self.assertEqual(list(queryset), notes)\n", "    def test_query_set_repr(self):\n        notes = Note.objects.bulk_create([Note(note=\"test-note\") for _ in range(20)])\n        qs = Note.objects.all()\n        repr_qs = repr(qs)\n        self.assertEqual(len(repr_qs), 130)\n", "    def test_aggregate_with_field_name_collision(self):\n        articles = [\n            Article.objects.create(name=str(i), created=datetime.datetime.today())\n            for i in range(10)\n        ]\n        queryset = Article.objects.annotate(avg_name=Avg(\"name\"))\n        msg = (\n            \"The annotation 'avg_name' conflicts with a field on the model.\"\n        )\n        with self.assertRaisesMessage(ValueError, msg):\n            queryset.aggregate(avg_name=Avg(\"avg_name\"))\n", "    def test_query_cache_cleared_on_delete(self):\n        for note in Note.objects.bulk_create([Note(note=str(i)) for i in range(10)]):\n            note.note = \"test\"\n        qs = Note.objects.all()\n        self.assertEqual(len(qs._result_cache), 10)\n        qs.delete()\n        self.assertIsNone(qs._result_cache)\n"], "sample_681": ["def test_log_set_path_relative(testdir):\n    report_dir_base = testdir.tmpdir.strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file_level = DEBUG\n        log_cli=true\n        \"\"\"\n    )\n    testdir.makeconftest(\n        \"\"\"\n            import os\n            import pytest\n            @pytest.hookimpl(hookwrapper=True, tryfirst=True)\n                config = item.config\n                logging_plugin = config.pluginmanager.get_plugin(\"logging-plugin\")\n                report_file = os.path.join(\"subdir\", item._request.node.name)\n                logging_plugin.set_log_path(report_file)\n                yield\n        \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n            import logging\n            logger = logging.getLogger(\"testcase-logger\")\n                logger.info(\"message from test 1\")\n                assert True\n\n                logger.debug(\"message from test 2\")\n                assert True\n        \"\"\"\n    )\n    testdir.runpytest()\n    expected = os.path.join(report_dir_base, \"subdir\")\n    assert os.path.isdir(expected)\n    with open(os.path.join(expected, \"test_first\")) as rfh:\n        content = rfh.read()\n        assert \"message from test 1\" in content\n\n    with open(os.path.join(expected, \"test_second\")) as rfh:\n        content = rfh.read()\n        assert \"message from test 2\" in content\n", "def test_log_file_path_non_ascii(testdir):\n    log_file = testdir.tmpdir.join(\"pytest-\ud83e\udd2f.log\").strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level = INFO\n        \"\"\".format(\n            log_file\n        )\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n        logging.getLogger().info(\"Normal message\")\n\n            logging.getLogger().debug(\"debug message in test_simple\")\n            logging.getLogger().info(\"info message in test_simple\")\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n\n    result.stdout.no_fnmatch_line(\"*--- live log collection ---*\")\n\n    assert result.ret == 0\n    assert os.path.isfile(log_file)\n    with open(log_file, encoding=\"utf-8\") as rfh:\n        contents = rfh.read()\n        assert \"Normal message\" in contents\n        assert \"debug message in test_simple\" not in contents\n        assert \"info message in test_simple\" in contents\n", "def test_log_capture_fixture_changing_log_level(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n            logger = logging.getLogger(__name__)\n            logger.setLevel(logging.INFO)\n            logger.info('text going to logger from call at INFO')\n            caplog.set_level(logging.DEBUG)\n            logger.setLevel(logging.DEBUG)\n            logger.info('text going to logger from call at DEBUG')\n            logger.debug('text going to logger from call at DEBUG')\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=DEBUG\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*-- Captured log call --*\",\n            \"*INFO*text going to logger from call at INFO*\",\n            \"*INFO*text going to logger from call at DEBUG*\",\n            \"*DEBUG*text going to logger from call at DEBUG*\",\n        ]\n    )\n", "def test_log_capture_fixture(testdir):\n    \"\"\"Test the LogCaptureFixture class.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('info message')\n            assert 'info message' in caplog.text\n            caplog.clear()\n            assert caplog.text == ''\n\n            logger.warning('warning message')\n            assert 'warning message' in caplog.text\n            caplog.handler.setLevel(logging.ERROR)\n            logger.info('info message')\n            assert 'info message' not in caplog.text\n\n            with caplog.at_level(logging.DEBUG):\n                logger.debug('debug message')\n                assert 'debug message' in caplog.text\n\n            assert 'debug message' not in caplog.text\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=DEBUG\")\n    assert result.ret == 0\n", "def test_log_capture_fixture(caplog):\n    \"\"\"\n    Test that the caplog fixture can capture log messages.\n    \"\"\"\n    import logging\n\n    logger = logging.getLogger(__name__)\n    logger.info(\"test log message\")\n\n    assert caplog.messages == [\"test log message\"]\n\n"], "sample_109": ["def test_autocomplete_select_multiple(self):\n    beatles = Band.objects.create(name='The Beatles', style='rock')\n    who = Band.objects.create(name='The Who', style='rock')\n    stones = Band.objects.create(name='The Rolling Stones', style='rock')\n    \n    form = AlbumForm(initial={'featuring': [beatles.pk, who.pk]})\n    output = form.as_table()\n    selected_option_beatles = '<option value=\"%s\" selected>The Beatles</option>' % beatles.pk\n    selected_option_who = '<option value=\"%s\" selected>The Who</option>' % who.pk\n    option_stones = '<option value=\"%s\">The Rolling Stones</option>' % stones.pk\n    self.assertIn(selected_option_beatles, output)\n    self.assertIn(selected_option_who, output)\n    self.assertNotIn(option_stones, output)\n", "def test_optgroups(self):\n    beatles = Band.objects.create(name='The Beatles', style='rock')\n    who = Band.objects.create(name='The Who', style='rock')\n    rel = Album._meta.get_field('band').remote_field\n    w = AutocompleteSelect(rel, admin.site)\n    value = beatles.pk\n    name = 'band'\n    optgroups = w.optgroups(name, value)\n    expected_optgroups = [\n        (None, [\n            ('<option value=\"%s\">%s</option>' % (beatles.pk, beatles.name), True, 0),\n        ], 0),\n    ]\n    self.assertEqual(len(optgroups), len(expected_optgroups))\n    for i in range(len(optgroups)):\n        self.assertEqual(len(optgroups[i]), len(expected_optgroups[i]))\n        self.assertEqual(optgroups[i][0], expected_optgroups[i][0])\n        self.assertEqual(len(optgroups[i][1]), len(expected_optgroups[i][1]))\n        for j in range(len(optgroups[i][1])):\n            self.assertEqual(optgroups[i][1][j][0], expected_optgroups[i][1][j][0])\n            self.assertEqual(optgroups[i][1][j][1], expected_optgroups[i][1][j][1])\n            self.assertEqual(optgroups[i][1][j][2], expected_optgroups[i][1][j][2])\n", "def test_optgroups(self):\n    beatles = Band.objects.create(name='The Beatles', style='rock')\n    who = Band.objects.create(name='The Who', style='rock')\n    rel = Album._meta.get_field('band').remote_field\n    w = AutocompleteSelect(rel, admin.site)\n    queryset = Album.objects.all()\n    field = ModelChoiceField(queryset=queryset)\n    widget = AutocompleteSelect(rel, admin.site)\n    name = 'my_field'\n    value = beatles.pk\n    optgroups = widget.optgroups(name, value)\n    self.assertEqual(len(optgroups), 1)\n    self.assertEqual(len(optgroups[0][1]), 1)\n    self.assertEqual(optgroups[0][1][0]['name'], name)\n    self.assertEqual(optgroups[0][1][0]['value'], str(value))\n    self.assertEqual(optgroups[0][1][0]['label'], 'The Beatles')\n    self.assertEqual(optgroups[0][1][0]['selected'], True)\n\n    w = AutocompleteSelectMultiple(rel, admin.site)\n    field = ModelMultipleChoiceField(queryset=queryset)\n    value = [beatles.pk, who.pk]\n    optgroups = w.optgroups(name, value)\n    self.assertEqual(len(optgroups), 1)\n    self.assertEqual(len(optgroups[0][1]), 2)\n    self.assertEqual(optgroups[0][1][0]['name'], name)\n    self.assertEqual(optgroups[0][1][0]['value'], str(value[0]))\n    self.assertEqual(optgroups[0][1][0]['label'], 'The Beatles')\n    self.assertEqual(optgroups[0][1][0]['selected'], True)\n    self.assertEqual(optgroups[0][1][1]['name'], name)\n    self.assertEqual(optgroups[0][1][1]['value'], str(value[1]))\n    self.assertEqual(optgroups[0][1][1]['label'], 'The Who')\n    self.assertEqual(optgroups[0][1][1]['selected'], True)\n", "def test_optgroups(self):\n    beatles = Band.objects.create(name='The Beatles', style='rock')\n    who = Band.objects.create(name='The Who', style='rock')\n    album = Album.objects.create(name='Abbey Road', band=beatles)\n\n    rel = Album._meta.get_field('band').remote_field\n    w = AutocompleteSelect(rel, admin.site)\n    optgroups = w.optgroups(name='band', value=[beatles.pk])\n\n    self.assertEqual(len(optgroups), 1)\n    self.assertEqual(len(optgroups[0][1]), 1)\n    self.assertEqual(optgroups[0][1][0]['value'], beatles.pk)\n    self.assertEqual(optgroups[0][1][0]['label'], 'The Beatles')\n    self.assertEqual(optgroups[0][1][0]['selected'], True)\n\n    # Test multiple values with a ManyToMany field.\n    w = AutocompleteSelect(Album._meta.get_field('featuring').remote_field, admin.site)\n    optgroups = w.optgroups(name='featuring', value=[beatles.pk, who.pk])\n\n    self.assertEqual(len(optgroups), 1)\n    self.assertEqual(len(optgroups[0][1]), 2)\n    self.assertEqual(optgroups[0][1][0]['value'], beatles.pk)\n    self.assertEqual(optgroups[0][1][0]['label'], 'The Beatles')\n    self.assertEqual(optgroups[0][1][0]['selected'], True)\n    self.assertEqual(optgroups[0][1][1]['value'], who.pk)\n    self.assertEqual(optgroups[0][1][1]['label'], 'The Who')\n    self.assertEqual(optgroups[0][1][1]['selected'], True)\n\n    # Test no values.\n    optgroups = w.optgroups(name='featuring', value=None)\n\n    self.assertEqual(len(optgroups), 1)\n    self.assertEqual(len(optgroups[0][1]), 0)\n", "def test_optgroups(self):\n    rel = Album._meta.get_field('band').remote_field\n    w = AutocompleteSelect(rel, admin.site)\n    choices = ((1, 'choice1'), (2, 'choice2'), (3, 'choice3'))\n    field = forms.ModelChoiceField(queryset=[Band.objects.create(name='Test band %d' % i) for i in range(3)])\n    field.widget = w\n    field.choices = choices\n    optgroups = w.optgroups('field_name', [2])\n    self.assertEqual(len(optgroups), 1)\n    self.assertEqual(len(optgroups[0][1]), 3)\n    self.assertEqual(optgroups[0][1][1]['value'], '2')\n    self.assertEqual(optgroups[0][1][1]['label'], 'choice2')\n    self.assertTrue(optgroups[0][1][1]['selected'])\n"], "sample_650": ["def test_log_formatter_multiline(pytester: Pytester) -> None:\n    \"\"\"Test log formatter handles multiline messages.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('text going to logger from call\\\\nwith multiple lines')\n            assert False\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--log-level=INFO\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\n            \"*-- Captured log call --*\",\n            \"*INFO*text going to logger from call*\",\n            \"*with multiple lines\",\n        ]\n    )\n", "def test_log_file_handler_is_closed(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('text')\n            assert False\n        \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_file=log.txt\n        log_file_level = INFO\n    \"\"\"\n    )\n    pytester.runpytest()\n    assert not pytester.path.joinpath(\"log.txt\").open().closed\n    pytester.runpytest_subprocess()\n    assert pytester.path.joinpath(\"log.txt\").open().closed\n", "def test_log_indentation(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('This is a multi-\\\\nline log message')\n            logger.info('Another log message with some indentation:\\\\n    some text')\n            assert False\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--log-auto-indent=true\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\n            \"*INFO*This is a multi-\",\n            \"*line log message\",\n            \"*INFO*Another log message with some indentation:\",\n            \"*    some text\",\n        ]\n    )\n", "def test_log_cli_format_multiline(pytester: Pytester) -> None:\n    \"\"\"Check that log-cli format supports multiline messages.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('This is a multiline\\nlog message')\n            assert False\n        \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_cli=true\n        log_format=%(asctime)s; %(levelname)s; %(message)s\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\n            \"*This is a multiline\",\n            \"          log message\"\n        ]\n    )\n", "def test_log_auto_indent(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            with caplog.at_level(logging.INFO):\n                logger.info(\"This is a\\\\nmultiline message\")\n                logger.info(\"This is another\\\\nmultiline message\")\n            assert len(caplog.messages) == 2\n            for message in caplog.messages:\n                assert message.startswith(\"This is a multiline message\")\n        \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_format=%(levelname)s %(message)s\n        log_auto_indent=true\n    \"\"\"\n    )\n\n    result = pytester.runpytest(\"--log-level=INFO\")\n    assert result.ret == 0\n    result.stdout.fnmatch_lines([\"*- Captured log call -*\", \"*INFO*This is a\", \"*INFO*This is another*\"])\n"], "sample_96": ["    def test_expression_with_function(self):\n        class TestModelAdmin(ModelAdmin):\n            ordering = (F('name__startswith').asc(),)\n\n        self.assertIsValid(TestModelAdmin, ValidationTestModel)\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            'admin.E034'\n        )\n", "    def test_not_list_or_tuple(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            'admin.E034'\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            'admin.E034'\n        )\n", "    def test_valid_case_with_multiple_fields(self):\n        class TestModelAdmin(ModelAdmin):\n            ordering = ('band__name', '-name',)\n\n        self.assertIsValid(TestModelAdmin, Song)\n"], "sample_1049": ["def test_arbitrary_point_parameter():\n    t = symbols('t')\n    p = Plane((1, 2, 3), normal_vector=(0, 0, 1))\n    assert p.parameter_value(p.arbitrary_point(t), t) == {t: t}\n    assert p.parameter_value(p.arbitrary_point(t)*2, t) == {t: t}\n    p = Plane((0, 0, 0), (1, 0, 0), (0, 1, 0))\n    u, v = symbols('u v')\n    assert p.parameter_value(p.arbitrary_point(u, v), u, v) == {u: u, v: v}\n    assert p.parameter_value(p.arbitrary_point(u, v)*2, u, v) == {u: 2*u, v: 2*v}\n", "def test_angle_between():\n    p1 = Plane(Point3D(1, 2, 2), normal_vector=(1, 2, 3))\n    p2 = Plane(Point3D(2, 2, 2), normal_vector=(-1, 2, -1))\n    l1 = Line3D(Point3D(1, 3, 4), Point3D(2, 2, 2))\n\n    assert p1.angle_between(p2).simplify() == pi/2\n    assert p1.angle_between(p1).simplify() == 0\n    assert p2.angle_between(p1).simplify() == pi/2\n    assert p1.angle_between(l1).simplify() == -asin(sqrt(21)/6)\n    assert p2.angle_between(l1).simplify() == -asin(sqrt(6)/6)\n", "def test_arbitrary_point_and_parameter_value():\n    t, u, v = symbols(\"t, u v\")\n    p = Plane((0, 0, 0), (0, 0, 1), (0, 1, 0))\n    ap_1 = p.arbitrary_point(t)\n    assert ap_1 == Point3D(0, cos(t), sin(t))\n    ap_2 = p.arbitrary_point(u, v)\n    assert ap_2 == Point3D(u, v, 0)\n    assert p.parameter_value(ap_1, t) == {t: t}\n    assert p.parameter_value(ap_2, u, v) == {u: u, v: v}\n\n    p = Plane((1, 1, 1), normal_vector=(1, 1, 1))\n    ap_1 = p.arbitrary_point(t)\n    assert ap_1 == Point3D(-sqrt(30)*sin(t)/30 + 2*sqrt(5)*cos(t)/5 + 1, \n                           sqrt(30)*sin(t)/15 + sqrt(5)*cos(t)/5 + 1, \n                           sqrt(30)*sin(t)/6 + 1)\n    ap_2 = p.arbitrary_point(u, v)\n    assert ap_2 == Point3D(2*u - v + 1, u + 2*v + 1, 5*v + 1)\n    assert p.parameter_value(ap_1, t) == {t: t}\n    assert p.parameter_value(ap_2, u, v) == {u: u, v: v}\n", "def test_plane_edge_cases():\n    # test equation method with symbolic variables\n    x, y, z = symbols('x y z')\n    p = Plane((0, 0, 0), normal_vector=(1, 1, 1))\n    assert p.equation(x, y, z) == x + y + z\n\n    # test arbitrary_point method with symbolic variables\n    u, v = symbols('u v')\n    p = Plane((0, 0, 0), normal_vector=(1, 0, 0))\n    assert p.arbitrary_point(u, v) == Point3D(0, u, v)\n\n    # test arbitrary_point method with non-zero p1\n    p = Plane((1, 2, 3), normal_vector=(1, 0, 0))\n    assert p.arbitrary_point(u, v) == Point3D(1, 2 + u, 3 + v)\n\n    # test projection method with Point and Point3D\n    p = Plane((0, 0, 0), normal_vector=(1, 0, 0))\n    assert p.projection((1, 2)) == Point3D(0, 2, 0)\n    assert p.projection(Point3D(1, 2, 3)) == Point3D(0, 2, 3)\n\n    # test projection_line method with Line and Line3D\n    p = Plane((0, 0, 0), normal_vector=(1, 0, 0))\n    assert p.projection_line(Line((1, 2), (3, 4))) == Line3D(Point3D(0, 2, 0), Point3D(0, 4, 0))\n    assert p.projection_line(Line3D(Point3D(1, 2, 3), Point3D(3, 4, 5))) == Line3D(Point3D(0, 2, 3), Point3D(0, 4, 5))\n\n    # test distance method with Point, Line, Ray, Segment, Plane\n    p = Plane((0, 0, 0), normal_vector=(1, 0, 0))\n    assert p.distance((1, 2, 3)) == 1\n    assert p.distance(Line3D((1, 2, 3), (4,", "def test_arbitrary_point():\n    t = Dummy('t')\n    u, v = symbols('u v', real=True)\n    pl1 = Plane((0, 0, 0), normal_vector=(1, 0, 0))\n    pl2 = Plane((0, 0, 0), normal_vector=(1, 1, 1))\n    pl3 = Plane((1, 2, 3), normal_vector=(2, 3, 4))\n    \n    assert pl1.arbitrary_point() == Point3D(0, cos(t), sin(t))\n    assert pl1.arbitrary_point(u, v) == Point3D(0, u, v)\n    assert pl2.arbitrary_point() == Point3D(cos(t), sin(t), 0)\n    assert pl2.arbitrary_point(u, v) == Point3D(u, v, -u - v)\n    assert pl3.arbitrary_point() == Point3D(3 - (30)**(1/2)*sin(t)/15, \n                                           (10 + (30)**(1/2)*sin(t)/5)/3, \n                                           2 + (30)**(1/2)*sin(t)/6)\n    assert pl3.arbitrary_point(u, v) == Point3D(1 - 3*u + 2*v, 2 + u + 3*v, 3 + 2*u - v)\n"], "sample_913": ["def test_pydomain_clear_doc(app, status, warning):\n    app.builder.build_all()\n\n    domain = app.env.get_domain('py')\n    assert len(domain.objects) > 0\n    assert len(domain.modules) > 0\n\n    domain.clear_doc('roles')\n    assert len(domain.objects) > 0\n    assert len(domain.modules) > 0\n\n    domain.clear_doc('module')\n    assert len(domain.objects) > 0\n    assert len(domain.modules) == 0\n\n    domain.clear_doc('module_option')\n    assert len(domain.objects) > 0\n    assert len(domain.modules) == 0\n\n    domain.clear_doc('roles')\n    assert len(domain.objects) == 0\n    assert len(domain.modules) == 0\n", "def test_domain_py_module_levels(app, status, warning):\n    app.builder.build_all()\n\n    modules = app.env.domains['py'].data['modules']\n    objects = app.env.domains['py'].data['objects']\n\n    assert 'module_a' in modules\n    assert 'module_a.submodule' in modules\n    assert 'module_b' in modules\n\n    assert objects['module_a.top_level'][2] == 'function'\n    assert objects['module_a.submodule.ModTopLevel'][2] == 'class'\n    assert objects['module_a.submodule.ModTopLevel.mod_child_1'][2] == 'method'\n    assert objects['module_a.submodule.ModTopLevel.mod_child_2'][2] == 'method'\n    assert objects['module_b.top_level'][2] == 'function'\n", "def test_resolve_xref_for_inner_module(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'module.html').read_text()\n    assert ('Link to <a class=\"reference internal\" href=\"#module_a.submodule.ModTopLevel\"'\n            ' title=\"module_a.submodule.ModTopLevel\">'\n            '<code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">'\n            'ModTopLevel</span></code></a>' in content)\n    assert ('Link to <a class=\"reference internal\" href=\"#module_a.submodule.ModTopLevel.mod_child_1\"'\n            ' title=\"module_a.submodule.ModTopLevel.mod_child_1\">'\n            '<code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">'\n            'mod_child_1</span></code></a>' in content)\n    assert ('Link to <a class=\"reference internal\" href=\"#module_b.submodule.ModTopLevel\"'\n            ' title=\"module_b.submodule.ModTopLevel\">'\n            '<code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">'\n            'ModTopLevel</span></code></a>' in content)\n", "def test_pyclass_signature(app):\n    text = (\".. py:class:: MyClass\\n\"\n            \"\\n\"\n            \"   .. py:method:: __init__(self, param: int)\\n\"\n            \"      :param param: description\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :type: str\\n\"\n            \"      :value: ''\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"MyClass\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc,\n                                                  addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'MyClass (class)', 'MyClass', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_annotation, \"method \"],\n                                                     [desc_name, \"__init__\"],\n                                                     [desc_parameterlist,\n                                                      ([desc_parameter, ([desc_sig_name, \"self\"],\n                                                                           [desc_sig_punctuation, \",\"],\n                                                                           [desc_parameter, ([desc_sig_name, \"param\"],\n                                                                                             [desc_sig_punctuation, \":\"],\n                                                                                             \" \",\n                                                                                             [nodes.inline, pending_xref, \"int\"])])])],\n                                                     [desc_returns, nodes.inline, \"None\"])],\n                                   [desc_content, (nodes.paragraph,\n                                                   [nodes.field_list,\n                                                    ([nodes.field, ([nodes.field_name, \"param\"],\n                                                                     [nodes.field_body, \"description\"])])])]))\n    assert 'MyClass' in domain.objects\n    assert domain.objects['MyClass'] == ('index', 'MyClass', 'class')\n    assert 'MyClass.__init__' in domain.objects\n    assert domain.objects['MyClass.__init__'] == ('index', 'MyClass.__init__', 'method')\n    assert 'MyClass.attr' in domain.objects\n    assert domain.objects['MyClass.attr'] == ('index', 'MyClass.attr', 'attribute')\n", "def test_domain_py_resolve_any_xref(app, status, warning):\n    app.builder.build_all()\n\n    domain = app.env.get_domain('py')\n\n        node = nodes.reference(reftarget=target, refdomain='py', reftype=reftype)\n        return domain.resolve_any_xref(app.env, 'index', app.builder, target, node, nodes.inline())\n\n    # test case: not found\n    assert resolve_xref('nonexist') == []\n\n    # test case: exact match\n    assert resolve_xref('TopLevel') == [('py:class', addnodes.pending_xref())]\n\n    # test case: fuzzy match\n    assert resolve_xref('subchild_1') == [\n        ('py:meth', addnodes.pending_xref()),\n        ('py:meth', addnodes.pending_xref())\n    ]\n\n    # test case: with reftype\n    assert resolve_xref('TopLevel', 'class') == [('py:class', addnodes.pending_xref())]\n    assert resolve_xref('TopLevel', 'meth') == []\n    assert resolve_xref('subchild_1', 'meth') == [\n        ('py:meth', addnodes.pending_xref()),\n        ('py:meth', addnodes.pending_xref())\n    ]\n"], "sample_1205": ["def test_PolyElement_primitive():\n    R, x, y = ring(\"x,y\", QQ)\n\n    f = 3*x**2 + 6*x + 9\n    g = x**2 + 2*x + 3\n\n    assert f.primitive() == (3, g)\n\n    f = x**2 + 2*x + 1\n    g = x**2 + 2*x + 1\n\n    assert f.primitive() == (1, g)\n", "def test_PolyElement_compose_ring():\n    Ruv, u,v = ring(\"u,v\", ZZ)\n    Rxyz, x,y,z = ring(\"x,y,z\", Ruv)\n\n    assert Rxyz.compose(Ruv) == Rxyz\n    assert Rxyz.compose(\"u,v\") == Rxyz\n\n    R, x = ring(\"x\", ZZ)\n    raises(CoercionFailed, lambda: R.compose(Ruv))\n    raises(CoercionFailed, lambda: R.compose(\"u,v\"))\n    raises(CoercionFailed, lambda: R.compose(\"x,y,z\"))\n", "def test_PolyElement_drop_to_ground():\n    R, x,y,z = ring(\"x,y,z\", ZZ)\n\n    assert R(1).drop_to_ground(0).ring == PolyRing(\"y,z\", ZZ[x], lex)\n    assert R(1).drop_to_ground(0).drop_to_ground(0).ring == PolyRing(\"z\", ZZ[x,y], lex)\n    assert R(1).drop_to_ground(0).drop_to_ground(0).drop_to_ground(0).ring == ZZ[x,y,z]\n\n    assert (3*z).drop_to_ground(0).ring == PolyRing(\"y,z\", ZZ[x], lex)\n    assert (3*z).drop_to_ground(0) == 3*z\n\n    raises(ValueError, lambda: x.drop_to_ground(0))\n    raises(ValueError, lambda: x.drop_to_ground(1))\n    raises(ValueError, lambda: x.drop_to_ground(2))\n    raises(ValueError, lambda: (x*y).drop_to_ground(1))\n    raises(ValueError, lambda: (x*y).drop_to_ground(0).drop_to_ground(0))\n\n    f = x**4*y**2 + x**2*y + 1\n    g = x**2*y**3 + x**2*y + 1\n\n    J, (f, g) = f.deflate(g)\n    h, cff, cfg = f.cofactors(g)\n\n    assert f.drop_to_ground(0).deflate(g.drop_to_ground(0)) == (J, [f.drop_to_ground(0), g.drop_to_ground(0)])\n    assert f.drop_to_ground(0).cofactors(g.drop_to_ground(0)) == (h.drop_to_ground(0), cff.drop_to_ground(0), cfg.drop_to_ground(0))\n", "def test_PolyElement_monic():\n    R, x,y,z = ring(\"x,y,z\", QQ)\n\n    assert (x*y*z).monic() == x*y*z\n    assert (2*x*y*z).monic() == x*y*z\n    assert (3*x*y*z).monic() == x*y*z\n    assert (-x*y*z).monic() == -x*y*z\n    assert (-2*x*y*z).monic() == -x*y*z\n    assert (-3*x*y*z).monic() == -x*y*z\n    assert (2*x + 3*y + 4*z).monic() == x + 1.5*y + 2*z\n    assert (3*x + 4*y + 5*z).monic() == x + 1.3333333333333333*y + 1.6666666666666667*z\n    assert (-2*x - 3*y - 4*z).monic() == -x - 1.5*y - 2*z\n    assert (-3*x - 4*y - 5*z).monic() == -x - 1.3333333333333333*y - 1.6666666666666667*z\n    assert (x*y + z).monic() == x*y + z\n    assert (x*y - z).monic() == x*y - z\n    assert (x + y*z).monic() == x + y*z\n    assert (x - y*z).monic() == x - y*z\n\n    R, x = ring(\"x\", ZZ)\n\n    assert (x**2).monic() == x**2\n    assert (2*x**2).monic() == x**2\n    assert (3*x**2).monic() == x**2\n    assert (-x**2).monic() == -x**2\n    assert (-2*x**2).monic() == -x**2\n    assert (-3*x**2).monic() == -x**2\n    assert (x + 2).monic() == x + 2\n    assert (x + 3).monic() == x + 3\n    assert (-x - 2).monic() == -x - 2\n    assert (-x - 3).monic() == -x - 3\n    assert (x**2 + 1).monic() == x**2 + 1\n   ", "def test_PolyElement___iadd___iadd_monom():\n    R, x = ring(\"x\", ZZ)\n    p = R.zero\n\n    assert p._iadd_monom((1, 2)) == 2*x\n    assert p._iadd_monom((1, 2)).is_monomial == True\n\n    p = x\n    assert p._iadd_monom((1, 2)) == 3*x\n    assert p._iadd_monom((1, 2)).is_monomial == True\n\n    p = x + 2\n    assert p._iadd_monom((1, 2)) == 3*x + 2\n    assert p._iadd_monom((1, 2)).is_monomial == False\n\n    p = x**2 + x + 2\n    assert p._iadd_monom((1, 2)) == x**2 + 3*x + 2\n    assert p._iadd_monom((1, 2)).is_monomial == False\n"], "sample_950": ["def test_type_to_xref(app):\n    doctree = type_to_xref(\"int\", app.env)\n    assert_node(doctree, ([pending_xref, \"int\"],))\n    assert_node(doctree, pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"int\")\n\n    doctree = type_to_xref(\"List[int]\", app.env)\n    assert_node(doctree, ([pending_xref, \"List\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \"]\"]))\n\n    doctree = type_to_xref(\"Tuple[int, int]\", app.env)\n    assert_node(doctree, ([pending_xref, \"Tuple\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \", \"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \"]\"]))\n\n    doctree = type_to_xref(\"Tuple[()]\", app.env)\n    assert_node(doctree, ([pending_xref, \"Tuple\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [desc_sig_punctuation, \"(\"],\n                          [desc_sig_punctuation, \")\"],\n                          [desc_sig_punctuation, \"]\"]))\n\n    doctree = type_to_xref(\"Tuple[int, ...]\", app.env)\n    assert_node(doctree, ([pending_xref, \"Tuple\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \", \"],\n                          [desc_sig_punctuation, \"...\"],\n                          [desc_sig_punctuation, \"]\"]))\n\n    doctree = type_to_xref(\"Callable[[int, int], int]\", app.env)\n    assert_node(doctree, ([pending_xref, \"Callable\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \", \"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \", \"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \"]\"]))\n\n    doctree = type_to_xref(\"List[None]\", app.env)\n    assert_node(doctree, ([pending_xref, \"List\"],\n                          [desc", "def test_get_full_qualified_name_with_prefix(app):\n    env = Mock(domaindata={})\n    domain = PythonDomain(env)\n\n    # with py:module and py:class context and prefix\n    kwargs = {'py:module': 'module1', 'py:class': 'Class'}\n    node = nodes.reference(reftarget='~func', **kwargs)\n    assert domain.get_full_qualified_name(node) == 'module1.Class.func'\n\n    # with py:module context and prefix\n    kwargs = {'py:module': 'module1'}\n    node = nodes.reference(reftarget='~func', **kwargs)\n    assert domain.get_full_qualified_name(node) == 'module1.func'\n\n    # with py:class context and prefix\n    kwargs = {'py:class': 'Class'}\n    node = nodes.reference(reftarget='~func', **kwargs)\n    assert domain.get_full_qualified_name(node) == 'Class.func'\n", "def test_type_to_xref(app):\n    text = \"Union[int, str]\"\n    doctree = PythonDomain.type_to_xref(text, app.env)\n    assert_node(doctree, pending_xref)\n    assert_node(doctree[0][0], pending_xref_condition, condition=\"resolved\")\n    assert_node(doctree[0][0][0], nodes.Text, text=\"Union\")\n    assert_node(doctree[0][1], pending_xref_condition, condition=\"*\")\n    assert_node(doctree[0][1][0], nodes.Text, text=\"Union[int, str]\")\n\n    text = \"List[int]\"\n    doctree = PythonDomain.type_to_xref(text, app.env)\n    assert_node(doctree, pending_xref)\n    assert_node(doctree[0][0], pending_xref_condition, condition=\"resolved\")\n    assert_node(doctree[0][0][0], nodes.Text, text=\"List\")\n    assert_node(doctree[0][1], pending_xref_condition, condition=\"*\")\n    assert_node(doctree[0][1][0], nodes.Text, text=\"List[int]\")\n\n    text = \"Tuple[int, ...]\"\n    doctree = PythonDomain.type_to_xref(text, app.env)\n    assert_node(doctree, pending_xref)\n    assert_node(doctree[0][0], pending_xref_condition, condition=\"resolved\")\n    assert_node(doctree[0][0][0], nodes.Text, text=\"Tuple\")\n    assert_node(doctree[0][1], pending_xref_condition, condition=\"*\")\n    assert_node(doctree[0][1][0], nodes.Text, text=\"Tuple[int, ...]\")\n\n    text = \"Callable[[int, str], int]\"\n    doctree = PythonDomain.type_to_xref(text, app.env)\n    assert_node(doctree, pending_xref)\n    assert_node(doctree[0][0], pending_xref_condition, condition=\"resolved\")\n    assert_node(doctree[0][0][0], nodes.Text, text=\"Callable\")\n    assert_node(doctree[0][1], pending_xref_condition, condition=\"*\")\n    assert_node(doctree[0][1][0], nodes.Text, text=\"Callable[[int, str], int]\")\n\n    text = \"None\"\n    doctree = PythonDomain.type_to_xref(text,", "def test_domain_py_xref_for_data(app, status, warning):\n    app.builder.build_all()\n\n    text = (\".. py:data:: MY_CONSTANT\\n\"\n            \"   :type: int\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"MY_CONSTANT\"],\n                                                    [desc_annotation, (\": \",\n                                                                       [pending_xref, \"int\"])])],\n                                  desc_content)]))\n\n    text = (\".. py:data:: MY_CONSTANT\\n\"\n            \"   :type: int\\n\"\n            \"   :value: 42\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"MY_CONSTANT\"],\n                                                    [desc_annotation, (\": \",\n                                                                       [pending_xref, \"int\"])],\n                                                    [desc_annotation, \" = 42\"])],\n                                  desc_content)]))\n\n    text = (\".. py:data:: MY_CONSTANT\\n\"\n            \"   :type: typing.List[int]\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"MY_CONSTANT\"],\n                                                    [desc_annotation, (\": \",\n                                                                       [pending_xref, \"List\"],\n                                                                       [desc_sig_punctuation, \"[\"],\n                                                                       [pending_xref, \"int\"],\n                                                                       [desc_sig_punctuation, \"]\"])])],\n                                  desc_content)]))\n\n    text = (\".. py:data:: MY_CONSTANT\\n\"\n            \"   :type: typing.List[int] | str\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"MY_CONSTANT\"],\n                                                    [desc_annotation, (\": \",\n                                                                       [pending_xref, \"List\"],\n                                                                       [desc_sig_punctuation, \"[\"],\n                                                                       [pending_xref, \"int\"],\n                                                                       [desc_sig_punctuation, \"]\"])],\n                                                    [desc_sig_punctuation, \" | \"],\n                                                    [pending_xref, \"str\"])],\n                                  desc_content)]))\n", "def test_pymodule_index_with_common_prefix(app, status, warning):\n    text = (\".. py:module:: sphinx\\n\"\n            \".. py:module:: sphinx.domains\\n\"\n            \".. py:module:: sphinx.domains.python\\n\"\n            \".. py:module:: sphinx.domains.c\\n\"\n            \".. py:module:: sphinx.domains.cpp\\n\")\n    doctree = restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('s', [IndexEntry('sphinx', 1, '', '', '', '', ''),\n                IndexEntry('sphinx.domains', 2, 'index', 'module-sphinx.domains', '', '', ''),\n                IndexEntry('sphinx.domains.python', 2, 'index', 'module-sphinx.domains.python', '', '', '')]),\n         ('sphinx.domains.c', 2, 'index', 'module-sphinx.domains.c', '', '', ''),\n         IndexEntry('sphinx.domains.cpp', 2, 'index', 'module-sphinx.domains.cpp', '', '', '')])],\n        True\n    )\n\n    app.builder.build_all()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<a class=\"reference internal\" href=\"#module-sphinx.domains.python\" title=\"sphinx.domains.python\">'\n            '<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">domains.python</span></code></a>'\n            in content)\n"], "sample_321": ["    def test_rotate_token(self):\n        \"\"\"\n        Rotate the CSRF token on request.\n        \"\"\"\n        req = self._get_GET_csrf_cookie_request()\n        mw = CsrfViewMiddleware(post_form_view)\n        mw.process_request(req)\n        old_token = req.META['CSRF_COOKIE']\n        rotate_token(req)\n        mw.process_request(req)\n        new_token = req.META['CSRF_COOKIE']\n        self.assertNotEqual(old_token, new_token)\n", "    def test_https_good_referer_matches_host(self):\n        \"\"\"\n        A POST HTTPS request with a good referer should be accepted from a\n        host that matches ALLOWED_HOSTS.\n        \"\"\"\n        req = self._get_POST_request_with_token()\n        req._is_secure_override = True\n        req.META['HTTP_REFERER'] = 'https://www.example.com/'\n        req.META['SERVER_PORT'] = '443'\n        mw = CsrfViewMiddleware(post_form_view)\n        mw.process_request(req)\n        response = mw.process_view(req, post_form_view, (), {})\n        self.assertIsNone(response)\n", "def test_csrf_view_middleware_process_response_set_secure_cookie(self):\n    req = self._get_GET_csrf_cookie_request()\n    resp = HttpResponse()\n    mw = CsrfViewMiddleware(post_form_view)\n    mw.process_request(req)\n    mw.process_view(req, post_form_view, (), {})\n    mw.process_response(req, resp)\n    self.assertTrue(resp.cookies[settings.CSRF_COOKIE_NAME]['secure'])\n", "def test_csrf_cookie_secure_and_samesite_false(self):\n    \"\"\"\n    Test CsrfViewMiddleware doesn't set 'Secure' and 'SameSite' flags for the\n    CSRF cookie when they are set to False in settings.\n    \"\"\"\n    req = self._get_GET_no_csrf_cookie_request()\n    mw = CsrfViewMiddleware(token_view)\n    mw.process_view(req, token_view, (), {})\n    resp = mw(req)\n    csrf_cookie = resp.cookies[settings.CSRF_COOKIE_NAME]\n    self.assertFalse(csrf_cookie.get('secure'))\n    self.assertIsNone(csrf_cookie.get('samesite'))\n", "def test_csrf_cookie_renewal(self):\n    \"\"\"\n    The CSRF cookie is renewed after a request, regardless of whether the\n    response is from a view that uses the CsrfViewMiddleware or not.\n    \"\"\"\n    req = self._get_GET_no_csrf_cookie_request()\n    mw = CsrfViewMiddleware(token_view)\n    mw.process_view(req, token_view, (), {})\n    resp = mw(req)\n    csrf_cookie = resp.cookies.get(settings.CSRF_COOKIE_NAME, False)\n    self.assertIsNotNone(csrf_cookie)\n\n    # Make a second request to a view that doesn't use CsrfViewMiddleware.\n    req = self._get_GET_no_csrf_cookie_request()\n    req.COOKIES[settings.CSRF_COOKIE_NAME] = csrf_cookie.value\n    view_resp = HttpResponse()\n    resp = mw.process_response(req, view_resp)\n    new_csrf_cookie = resp.cookies.get(settings.CSRF_COOKIE_NAME, False)\n    self.assertIsNotNone(new_csrf_cookie)\n    self.assertNotEqual(new_csrf_cookie.value, csrf_cookie.value)\n"], "sample_1": ["def test_separable_mapping():\n    # Test separability of Mapping model\n    mapping = Mapping((0, 1, 0, 1))\n    assert_allclose(is_separable(mapping), np.array([True, True]))\n    assert_allclose(separability_matrix(mapping), np.array([[True, False], [False, True]]))\n\n    mapping = Mapping((0, 0, 1, 1))\n    assert_allclose(is_separable(mapping), np.array([False, False]))\n    assert_allclose(separability_matrix(mapping), np.array([[True, True], [True, True]]))\n", "def test_separable_model_with_single_input_and_multiple_outputs():\n    @custom_model\n        return x, x**2, x**3\n\n    separable_array = is_separable(model_b())\n    assert np.all(separable_array == [False, False, False])\n\n    separability_matrix_array = separability_matrix(model_b())\n    assert np.all(separability_matrix_array == np.ones((3, 1), dtype=np.bool_))\n", "def test_separable_mapping():\n    # Test Mapping model with different input and output dimensions\n    map_model = Mapping((0, 1, 2), name='map_model')\n    assert_allclose(separability_matrix(map_model), np.array([[True, False, False],\n                                                             [False, True, False],\n                                                             [False, False, True]]))\n\n    map_model = Mapping((0, 0, 1), name='map_model')\n    assert_allclose(separability_matrix(map_model), np.array([[True, False, False],\n                                                             [True, False, False],\n                                                             [False, True, False]]))\n\n    map_model = Mapping((0, 1), name='map_model')\n    assert_allclose(separability_matrix(map_model), np.array([[True, False],\n                                                             [False, True]]))\n", "def test_separable_mapping_model():\n    # Test Mapping model\n    mapping_model = Mapping((0, 1, 0, 1))\n    expected_separable = np.array([True, True, True, True])\n    expected_separability_matrix = np.array([[True, False], [False, True], [True, False], [False, True]])\n    assert_allclose(is_separable(mapping_model), expected_separable)\n    assert_allclose(separability_matrix(mapping_model), expected_separability_matrix)\n\n    # Test Mapping model with different inputs and outputs\n    mapping_model = Mapping((0, 1, 0, 1, 2, 3))\n    expected_separable = np.array([True, True, True, True, True, True])\n    expected_separability_matrix = np.array([[True, False, False], [False, True, False], [True, False, False], [False, True, False], [False, False, True], [False, False, True]])\n    assert_allclose(is_separable(mapping_model), expected_separable)\n    assert_allclose(separability_matrix(mapping_model), expected_separability_matrix)\n\n    # Test Mapping model with all inputs mapped to the same output\n    mapping_model = Mapping((0, 0, 0, 0))\n    expected_separable = np.array([False, False, False, False])\n    expected_separability_matrix = np.array([[True, True, True, True], [True, True, True, True], [True, True, True, True], [True, True, True, True]])\n    assert_allclose(is_separable(mapping_model), expected_separable)\n    assert_allclose(separability_matrix(mapping_model), expected_separability_matrix)\n", "def test_separable_matrix_model_inputs():\n    # Test separability matrix with model inputs\n    result = _separable(sh1 & sh2)\n    assert_allclose(result, np.array([[1, 0], [0, 1]]))\n    result = _separable(sh1 & sh2 | scl1 & scl2)\n    assert_allclose(result, np.array([[1, 0], [0, 1]]))\n    result = _separable(sh1 & sh2 | rot)\n    assert_allclose(result, np.array([[1, 1], [1, 1]]))\n    result = _separable(sh1 | sh2)\n    assert_allclose(result, np.array([[1], [0]]))\n    result = _separable(sh1 | scl1)\n    assert_allclose(result, np.array([[1]]))\n    result = _separable(sh1 | scl1 | sh2)\n    assert_allclose(result, np.array([[1]]))\n    result = _separable(map3)\n    assert_allclose(result, np.array([[1], [1]]))\n    result = _separable(map3 | sh1)\n    assert_allclose(result, np.array([[1]]))\n    result = _separable(sh1 | map3)\n    assert_allclose(result, np.array([[1]]))\n    result = _separable(sh1 & map3)\n    assert_allclose(result, np.array([[1, 0], [1, 0]]))\n    result = _separable(map3 & sh1)\n    assert_allclose(result, np.array([[1, 0], [1, 0]]))\n"], "sample_859": ["def test_sparse_input_convergence_warning_precompute():\n    X, y, _, _ = build_dataset(n_samples=1000, n_features=500)\n\n    with pytest.warns(ConvergenceWarning):\n        ElasticNet(max_iter=1, tol=0, precompute=True).fit(\n            sparse.csr_matrix(X, dtype=np.float32), y)\n\n    # check that the model converges w/o warnings\n    with pytest.warns(None) as record:\n        Lasso(max_iter=1000, precompute=True).fit(sparse.csr_matrix(X, dtype=np.float32), y)\n\n    assert not record.list\n", "def test_l1_ratio_validation():\n    # Test that an error message is raised if l1_ratio is not in [0, 1]\n    msg = \"l1_ratio must be between 0 and 1; got\"\n    X = np.array([[1, 2, 4, 5, 8], [3, 5, 7, 7, 8]]).T\n    y = np.array([12, 10, 11, 21, 5])\n\n    for model in (ElasticNet, ElasticNetCV, MultiTaskElasticNet,\n                  MultiTaskElasticNetCV):\n        assert_raise_message(ValueError, msg, model(l1_ratio=1.1).fit, X, y)\n        assert_raise_message(ValueError, msg, model(l1_ratio=-0.1).fit, X, y)\n", "def test_lassoCV_non_negative_alpha():\n    X, y, _, _ = build_dataset()\n    alphas = [-1]\n    clf = LassoCV(alphas=alphas)\n    assert_raise_message(ValueError, \"Penalty term must be positive; got (array([-1.]),)\",\n                        clf.fit, X, y)\n", "def test_enet_coordinate_descent_bounds():\n    \"\"\"Test enet_coordinate_descent function with bounds.\"\"\"\n    rng = np.random.RandomState(0)\n    X = rng.randn(100, 50)\n    y = rng.randn(100)\n    alpha = 1.0\n    l1_ratio = 0.5\n\n    model = cd_fast.enet_coordinate_descent(np.zeros(X.shape[1]), alpha * l1_ratio,\n                                            alpha * (1.0 - l1_ratio), X, y,\n                                            max_iter=1, tol=0.0,\n                                            random_state=rng, positive=False,\n                                            check_input=False)\n\n    assert_array_almost_equal(model.coef_, np.zeros(X.shape[1]))\n", "def test_lassoCV_separate_intercept_and_slope():\n    X, y, _, _ = build_dataset()\n    X = X[:, :5]  # Make X smaller to speed up test\n\n    # Fit model with intercept\n    clf_intercept = LassoCV(cv=2).fit(X, y)\n    intercept_coef_intercept = clf_intercept.intercept_\n    slope_coef_intercept = clf_intercept.coef_\n\n    # Fit model without intercept\n    clf_no_intercept = LassoCV(fit_intercept=False, cv=2).fit(X, y)\n    intercept_coef_no_intercept = clf_no_intercept.intercept_\n    slope_coef_no_intercept = clf_no_intercept.coef_\n\n    assert intercept_coef_no_intercept == 0\n    assert_array_almost_equal(intercept_coef_intercept + np.dot(X, slope_coef_no_intercept), np.dot(X, slope_coef_intercept) + intercept_coef_intercept)\n"], "sample_1165": ["def test_quaternion_pow_cos_sin_edge_cases():\n    q1 = Quaternion(0, 1, 0, 0)\n    assert q1.pow_cos_sin(0) == Quaternion(1, 0, 0, 0)\n    assert q1.pow_cos_sin(1) == q1\n    assert q1.pow_cos_sin(-1) == Quaternion(1, 0, 0, 0)\n    q2 = Quaternion(1, 0, 0, 0)\n    assert q2.pow_cos_sin(0) == Quaternion(1, 0, 0, 0)\n    assert q2.pow_cos_sin(1) == q2\n    assert q2.pow_cos_sin(-1) == Quaternion(1, 0, 0, 0)\n", "def test_quaternion_power():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n    q0 = Quaternion(0, 0, 0, 0)\n\n    assert q1.pow(0) == Quaternion(1, 0, 0, 0)\n    assert q2.pow(0) == Quaternion(1, 0, 0, 0)\n    assert q0.pow(0) == Quaternion(1, 0, 0, 0)\n\n    assert q1.pow(1) == q1\n    assert q2.pow(1) == q2\n    assert q0.pow(1) == q0\n\n    assert q1.pow(2) == q1 * q1\n    assert q2.pow(2) == q2 * q2\n    assert q0.pow(2) == q0 * q0\n\n    assert q1.pow(-2) == (q1 ** -1) ** 2\n    assert q2.pow(-2) == (q2 ** -1) ** 2\n    raises(ValueError, lambda: q0.pow(-2))\n", "def test_quaternion_power_cases():\n    q = Quaternion(1, 2, 3, 4)\n    n = Symbol('n')\n    raises(TypeError, lambda: q**n)\n    n = Symbol('n', real=True)\n    raises(TypeError, lambda: q**n)\n    n = Symbol('n', integer=True, positive=True)\n    assert q**n == q.pow(n)\n    n = Symbol('n', integer=True, negative=True)\n    assert q**n == q.pow(n)\n    n = Symbol('n', integer=True)\n    assert q**n == q.pow(n)\n    q1 = Quaternion(1, 0, 0, 0)\n    assert q1.pow(0) == Quaternion(1, 0, 0, 0)\n    q1 = Quaternion(0, 1, 0, 0)\n    assert q1.pow(0) == Quaternion(1, 0, 0, 0)\n    q1 = Quaternion(0, 0, 1, 0)\n    assert q1.pow(0) == Quaternion(1, 0, 0, 0)\n    q1 = Quaternion(0, 0, 0, 1)\n    assert q1.pow(0) == Quaternion(1, 0, 0, 0)\n", "def test_quaternion_pow_cos_sin():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(cos(phi/2), 0, 0, sin(phi/2))\n    q3 = Quaternion(1, 1, 1, 1)\n    q4 = Quaternion(0, 0, 0, 0)\n\n    assert q1.pow_cos_sin(2) == Quaternion(900*cos(2*acos(sqrt(30)/30)),\n                                    1800*sqrt(29)*sin(2*acos(sqrt(30)/30))/29,\n                                    2700*sqrt(29)*sin(2*acos(sqrt(30)/30))/29,\n                                    3600*sqrt(29)*sin(2*acos(sqrt(30)/30))/29)\n    assert q2.pow_cos_sin(2) == Quaternion(cos(phi),\n                                    0,\n                                    0,\n                                    sin(phi))\n    assert q3.pow_cos_sin(3) == Quaternion(S.One, S.One, S.One, S.One)\n    raises(ValueError, lambda: q4.pow_cos_sin(2))\n\n    # Test with a complex quaternion\n    q5 = Quaternion(1 + 2*I, 3 + 4*I, 0, 0, real_field=False)\n    assert q5.pow_cos_sin(2) == Quaternion((1 + 2*I)**2 - (3 + 4*I)**2,\n                                          2*(1 + 2*I)*(3 + 4*I),\n                                          0,\n                                          0)\n", "def test_quaternion_trigonometric_rotation():\n    \"\"\"\n    Testing rotation using Quaternion class for some common angles.\n    \"\"\"\n    from math import sqrt\n\n    # Testing 90 degree rotation about x-axis\n    q = Quaternion(cos(pi/4), sin(pi/4), 0, 0)\n    assert trigsimp(q.to_rotation_matrix()) == Matrix([\n                [1,        0,         0],\n                [0,  0,        -1],\n                [0,  1,         0]])\n\n    # Testing 180 degree rotation about y-axis\n    q = Quaternion(cos(pi/2), 0, sin(pi/2), 0)\n    assert trigsimp(q.to_rotation_matrix()) == Matrix([\n                [-1,        0,         0],\n                [0,  1,         0],\n                [0,  0,        -1]])\n\n    # Testing 45 degree rotation about z-axis\n    q = Quaternion(sqrt(2)/2, 0, 0, sqrt(2)/2)\n    assert trigsimp(q.to_rotation_matrix()) == Matrix([\n                [1/sqrt(2), -1/sqrt(2), 0],\n                [1/sqrt(2),  1/sqrt(2), 0],\n                [0,           0,        1]])\n\n    # Testing 90 degree rotation about an axis other than the principal axes.\n    # For example the axis (1/sqrt(3), 1/sqrt(3), 1/sqrt(3))\n    q = Quaternion(1/2, 1/2, 1/2, 1/2)\n    rotation_matrix = q.to_rotation_matrix()\n    assert trigsimp(rotation_matrix * Matrix([1, 1, 1])) == \\\n        trigsimp(Matrix([1, 1, 1]) - 2 * Matrix([1/3, 1/3, 1/3]))\n\n    # Testing 180 degree rotation about an axis other than the principal axes.\n    # For example the axis (1/sqrt(3), 1/sqrt(3), 1/sqrt(3))\n    q = Quaternion(0, 1/sqrt(3), 1/sqrt(3), 1/sqrt(3))\n    rotation_matrix = q.to_rotation_matrix()\n    assert trigsimp(rotation_matrix * Matrix([1, 1"], "sample_257": ["    def test_key_transform_decimal(self):\n        obj = NullableJSONModel.objects.create(value={'pi': 3.14159265358979323846})\n        self.assertAlmostEqual(\n            NullableJSONModel.objects.annotate(pi=KeyTransform('pi', 'value')).get().pi,\n            3.14159265358979323846,\n        )\n", "    def test_key_transform_factory(self):\n        key_transform = JSONField().get_transform('key_name')\n        self.assertIsInstance(key_transform, KeyTransformFactory)\n", "    def test_key_transform_icontains(self):\n        self.assertIs(NullableJSONModel.objects.filter(value__foo__icontains='Ar').exists(), True)\n        self.assertIs(NullableJSONModel.objects.filter(value__foo__icontains='ar').exists(), True)\n", "    def test_process_lhs(self):\n        key_transform = KeyTransform('a', 'value')\n        compiler = object()\n        connection = object()\n        lhs, lhs_params = key_transform.process_lhs(compiler, connection)\n        self.assertEqual(lhs, 'value')\n        self.assertEqual(lhs_params, ())\n", "    def test_key_text_lookup_mixin_integer_key(self):\n        transform = KeyTransformTextLookupMixin(KeyTransform(1))\n        msg = 'Transform should be an instance of KeyTransform in order to use this lookup.'\n        with self.assertRaisesMessage(TypeError, msg):\n            KeyTransformTextLookupMixin(KeyTransform(1))\n"], "sample_1130": ["def test_point_a1pt_theory_pos_not_defined():\n    q, q2 = dynamicsymbols('q q2')\n    qd, q2d = dynamicsymbols('q q2', 1)\n    qdd, q2dd = dynamicsymbols('q q2', 2)\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    B.set_ang_vel(N, qd * B.z)\n    O = Point('O')\n    P = Point('P')\n    P.set_vel(B, q2d * B.z)\n    O.set_vel(N, 0)\n    raises(ValueError, lambda: P.a1pt_theory(O, N, B)) # P.pos_from(O) not defined\n", "def test_point_acc():\n    q, q2 = dynamicsymbols('q q2')\n    qd, q2d = dynamicsymbols('q q2', 1)\n    qdd, q2dd = dynamicsymbols('q q2', 2)\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    B.set_ang_vel(N, qd * B.z)\n    O = Point('O')\n    P = O.locatenew('P', B.x)\n    P.set_vel(B, 0)\n    O.set_vel(N, 0)\n    assert P.acc(N) == 0\n    P.set_acc(B, qdd * B.y)\n    assert P.acc(B) == qdd * B.y\n    O.set_acc(N, q2d * N.x)\n    assert P.a1pt_theory(O, N, B) == ((-qd**2) * B.x + qdd * B.y +\n                               (-q2d) * B.x + q2dd * N.x)\n    assert P.acc(N) == ((-qd**2) * B.x + qdd * B.y +\n                        (-q2d) * B.x + q2dd * N.x)\n", "def test_point_acc():\n    q, q2 = dynamicsymbols('q q2')\n    qd, q2d = dynamicsymbols('q q2', 1)\n    qdd, q2dd = dynamicsymbols('q q2', 2)\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    B.set_ang_vel(N, 5 * B.y)\n    O = Point('O')\n    P = O.locatenew('P', q * B.x)\n    P.set_vel(B, qd * B.x + q2d * B.y)\n    O.set_vel(N, 0)\n    assert P.acc(B) == (qdd * B.x + q2dd * B.y)\n    assert P.a1pt_theory(O, N, B) == ((-25 * q + qdd) * B.x + (q2dd) * B.y +\n                               (-10 * qd) * B.z)\n    O.set_acc(N, 10 * N.x)\n    assert P.acc(N) == ((-25 * q + qdd) * B.x + (q2dd) * B.y +\n                        (-10 * qd) * B.z + 10 * N.x)\n    O.set_vel(N, q2d * B.x)\n    assert P.acc(N) == ((q2dd - qd**2) * B.x + (q2d * qd + qdd) * B.y +\n                        (-10 * qd) * B.z + 10 * N.x)\n", "def test_auto_point_vel_pos_vector_multiple_paths():\n    t = dynamicsymbols._t\n    q1, q2, u1 = dynamicsymbols('q1, q2, u1')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    O = Point('O')\n    O.set_vel(B, u1 * B.x)\n    P = Point('P')\n    P.set_pos(O, q1 * N.x)\n    P1 = Point('P1')\n    P1.set_pos(P, q2 * B.y)\n    P1.set_vel(B, q1.diff(t) * N.x)\n    P2 = Point('P2')\n    P2.set_pos(P, q2 * N.y)\n    P2.set_vel(B, q2.diff(t) * N.y)\n    P1.set_pos(P2, q1 * B.z)\n    with warnings.catch_warnings(): #There are two possible paths in this point tree, thus a warning is raised\n        warnings.simplefilter('error')\n        with ignore_warnings(UserWarning):\n            assert P1.vel(B) == (q1.diff(t) + q2.diff(t)) * N.x + q2.diff(t) * B.y + u1 * B.x\n", "def test_auto_point_vel_edge_case():\n    q1, q2 = dynamicsymbols('q1 q2')\n    N = ReferenceFrame('N')\n    P = Point('P')\n    P1 = Point('P1')\n    P1.set_pos(P, q1 * N.x)\n    P2 = Point('P2')\n    P2.set_pos(P1, q2 * N.x)\n    P3 = Point('P3')\n    P3.set_pos(P2, q1 * N.x)\n    P4 = Point('P4')\n    P4.set_pos(P3, q2 * N.x)\n    P4.set_vel(N, q2.diff(dynamicsymbols._t) * N.x + q1.diff(dynamicsymbols._t) * N.y)\n    P2.set_vel(N, q1.diff(dynamicsymbols._t) * N.x)\n    assert P1.vel(N) == (q1.diff(dynamicsymbols._t)) * N.x\n    P2.set_vel(N, q1.diff(dynamicsymbols._t) * N.x + q2.diff(dynamicsymbols._t) * N.y)\n    assert P1.vel(N) == (q1.diff(dynamicsymbols._t)) * N.x\n"], "sample_1131": ["def test_PythonCodePrinter_reserved_words():\n    prntr = PythonCodePrinter()\n    reserved_words = prntr.reserved_words\n    for word in reserved_words:\n        expr = symbols(word)\n        py_str = prntr.doprint(expr)\n        assert word not in py_str\n        assert py_str.startswith(word + '_')\n", "def test_SciPyPrinter_lowergamma_uppergamma():\n    p = SciPyPrinter()\n\n    assert p.doprint(lowergamma(x, y)) == 'scipy.special.gamma(x)*scipy.special.gammainc(x, y)'\n    assert p.doprint(uppergamma(x, y)) == 'scipy.special.gamma(x)*scipy.special.gammaincc(x, y)'\n    assert p.doprint(lowergamma(1, y)) == 'scipy.special.exp(-y)'\n    assert p.doprint(uppergamma(1, y)) == 'scipy.special.exp(-y)'\n", "def test_unsupported_functions():\n    from sympy import zeta, polygamma\n\n    expr1 = zeta(x)\n    expr2 = polygamma(0, x)\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr1) == '  # Not supported in Python:\\n  # zeta\\nzeta(x)'\n    assert prntr.doprint(expr2) == '  # Not supported in Python:\\n  # polygamma\\npolygamma(0, x)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr1) == 'mpmath.zeta(x)'\n    assert prntr.doprint(expr2) == 'mpmath.psi(x, 0)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr1) == '  # Not supported in Python with NumPy:\\n  # zeta\\nzeta(x)'\n    assert prntr.doprint(expr2) == '  # Not supported in Python with NumPy:\\n  # polygamma\\npolygamma(0, x)'\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr1) == '  # Not supported in Python with SciPy:\\n  # zeta\\nzeta(x)'\n    assert prntr.doprint(expr2) == '  # Not supported in Python with SciPy:\\n  # polygamma\\npolygamma(0, x)'\n", "def test_MatrixBase_printing():\n    from sympy import MatrixSymbol, ShapeError\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 3, 3)\n    C = MatrixSymbol('C', 2, 3)\n    \n    # NumPyPrinter\n    nprntr = NumPyPrinter()\n    assert nprntr.doprint(A) == 'numpy.array([[A[0, 0], A[0, 1]], [A[1, 0], A[1, 1]])'\n    assert nprntr.doprint(B) == 'numpy.array([[B[0, 0], B[0, 1], B[0, 2]], [B[1, 0], B[1, 1], B[1, 2]], [B[2, 0], B[2, 1], B[2, 2]])'\n    assert nprntr.doprint(C) == 'numpy.array([[C[0, 0], C[0, 1], C[0, 2]], [C[1, 0], C[1, 1], C[1, 2]])'\n    \n    # SciPyPrinter\n    sprntr = SciPyPrinter()\n    assert sprntr.doprint(A) == 'numpy.array([[A[0, 0], A[0, 1]], [A[1, 0], A[1, 1]])'\n    assert sprntr.doprint(B) == 'numpy.array([[B[0, 0], B[0, 1], B[0, 2]], [B[1, 0], B[1, 1], B[1, 2]], [B[2, 0], B[2, 1], B[2, 2]])'\n    assert sprntr.doprint(C) == 'numpy.array([[C[0, 0], C[0, 1], C[0, 2]], [C[1, 0], C[1, 1], C[1, 2]])'\n    \n    # MpmathPrinter\n    mprntr = MpmathPrinter()\n    assert mprntr.doprint(A) == 'numpy.array([[A[0, 0], A", "def test_issue_21019():\n    expr = 2 ** (1 / 3)\n    p = NumPyPrinter()\n    assert p.doprint(expr) == 'numpy.power(2, 1/3)'\n    p = PythonCodePrinter()\n    assert p.doprint(expr) == '(2.0 ** (1.0 / 3.0))'\n"], "sample_804": ["def test_ordinal_encoder_dtypes():\n    # check that dtypes are preserved when determining categories\n    enc = OrdinalEncoder()\n    exp = np.array([[0., 1., 0.],\n                    [1., 0., 0.]], dtype='float64')\n\n    for X in [np.array([[1, 2], [3, 4]], dtype='int64'),\n              np.array([[1, 2], [3, 4]], dtype='float64'),\n              np.array([['a', 'b'], ['c', 'd']]),  # string dtype\n              np.array([[1, 'a'], [3, 'b']], dtype='object')]:\n        enc.fit(X)\n        assert all([enc.categories_[i].dtype == X.dtype for i in range(2)])\n        assert_array_equal(enc.transform(X), exp)\n\n    X = [[1, 2], [3, 4]]\n    enc.fit(X)\n    assert all([np.issubdtype(enc.categories_[i].dtype, np.integer)\n                for i in range(2)])\n    assert_array_equal(enc.transform(X), exp)\n\n    X = [[1, 'a'], [3, 'b']]\n    enc.fit(X)\n    assert all([enc.categories_[i].dtype == 'object' for i in range(2)])\n    assert_array_equal(enc.transform(X), exp)\n", "def test_one_hot_encoder_drop_not_fitted():\n    enc = OneHotEncoder(drop='first')\n    X = [['Male', 1], ['Female', 3]]\n    msg = (\"This OneHotEncoder instance is not fitted yet. \"\n           \"Call 'fit' with appropriate arguments before using this method.\")\n    with pytest.raises(NotFittedError, match=msg):\n        enc.transform(X)\n", "def test_one_hot_encoder_handle_unknown_error():\n    X = np.array([['a', 2, 55], ['b', 1, 55], ['c', 3, 55]]).T\n    X2 = np.array([['a', 2, 55], ['b', 1, 55], ['d', 3, 55]]).T\n    enc = OneHotEncoder(handle_unknown='error')\n    enc.fit(X)\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        enc.transform(X2)\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        enc.fit_transform(X2)\n", "def test_one_hot_encoder_handle_unknown_with_drop():\n    # test handle unknown with drop\n    X = [['abc', 2, 55], ['def', 1, 55], ['def', 3, 55]]\n    enc = OneHotEncoder(handle_unknown='ignore', categories=[['a', 'b'], [1, 2, 3], [54, 55]])\n    exp = np.array([[0., 0., 0., 0., 0.],\n                    [0., 0., 0., 0., 0.],\n                    [0., 0., 0., 0., 0.]])\n    assert_array_equal(enc.fit_transform(X).toarray(), exp)\n\n    enc = OneHotEncoder(handle_unknown='ignore', drop='first', categories=[['a', 'b'], [1, 2, 3], [54, 55]])\n    exp = np.array([[0., 0., 0., 0.],\n                    [0., 0., 0., 0.],\n                    [0., 0., 0., 0.]])\n    assert_array_equal(enc.fit_transform(X).toarray(), exp)\n", "def test_one_hot_encoder_drop_string():\n    enc = OneHotEncoder(drop='first')\n    X = [['cat1', 1], ['cat2', 2], ['cat1', 2], ['cat3', 3]]\n    X_test = [['cat1', 1], ['cat2', 2], ['cat1', 3]]\n    enc.fit(X)\n    X_tr = enc.transform(X_test)\n    exp = np.array([[0., 1., 1.], [1., 0., 1.], [0., 0., 1.]])\n    assert_array_equal(X_tr.toarray(), exp)\n    assert_array_equal(enc.inverse_transform(X_tr), X_test)\n"], "sample_505": ["def test_daterange_microsecond():\n    \"\"\"\n    Test drange works as expected for microsecond resolution\n    \"\"\"\n    start = datetime.datetime(2011, 1, 1, tzinfo=mdates.UTC)\n    end = start + datetime.timedelta(microseconds=1)\n    delta = datetime.timedelta(microseconds=1)\n    # We expect 2 values in drange(start, end, delta), because drange returns\n    # dates from an half open interval [start, end)\n    assert len(mdates.drange(start, end, delta)) == 2\n", "def test_RRuleLocator_repr():\n    rrule = mdates.rrulewrapper(dateutil.rrule.YEARLY)\n    locator = mdates.RRuleLocator(rrule)\n    assert repr(locator) == f\"RRuleLocator(rrule={rrule!r}, tz=None)\"\n    tz = dateutil.tz.gettz('US/Eastern')\n    locator = mdates.RRuleLocator(rrule, tz=tz)\n    assert repr(locator) == f\"RRuleLocator(rrule={rrule!r}, tz={tz!r})\"\n", "def test_zero_dates():\n    dates = np.array([np.datetime64('1970-01-01'),\n                      np.datetime64('1970-01-02'),\n                      np.datetime64('1970-01-03')])\n    with pytest.raises(ValueError):\n        mdates.drange(dates[0], dates[0], datetime.timedelta(days=1))\n    with pytest.raises(ValueError):\n        mdates.drange(dates[1], dates[1], datetime.timedelta(days=1))\n    with pytest.raises(ValueError):\n        mdates.drange(dates[2], dates[2], datetime.timedelta(days=1))\n    with pytest.raises(ValueError):\n        mdates.drange(dates[0], dates[2], datetime.timedelta(days=0))\n", "def test_date2num_object_array():\n    # Test that date2num works with object arrays that contain a mix of\n    # datetime objects and numpy datetime objects.\n    dates = np.array([datetime.datetime(2001, 1, 1), np.datetime64('2001-01-02'),\n                      datetime.datetime(2001, 1, 3), np.datetime64('2001-01-04')],\n                     dtype=object)\n    date2num_dates = mdates.date2num(dates)\n    expected = [mdates.date2num(dates[i]) for i in range(len(dates))]\n    np.testing.assert_array_equal(date2num_dates, expected)\n", "def test_drange_rapidly():\n    \"\"\"\n    Test drange for rapidly increasing dates\n    \"\"\"\n    start = datetime.datetime(2011, 1, 1, tzinfo=mdates.UTC)\n    end = datetime.datetime(2011, 1, 1, tzinfo=mdates.UTC)\n    end += datetime.timedelta(days=2**30)  # end is 35 years after start\n    delta = datetime.timedelta(hours=1)\n\n    # We expect the number of values in drange(start, end, delta) to be\n    # 2**30 * 24\n    assert len(mdates.drange(start, end, delta)) == 2**30 * 24\n"], "sample_175": ["    def test_restrict_cascade_indirect_diamond_delete_top(self):\n        delete_top = DeleteTop.objects.create()\n        b1 = B1.objects.create(delete_top=delete_top)\n        b2 = B2.objects.create(delete_top=delete_top)\n        DeleteBottom.objects.create(b1=b1, b2=b2)\n        msg = (\n            \"Cannot delete some instances of model 'B1' because they are \"\n            \"referenced through restricted foreign keys: 'DeleteBottom.b1'.\"\n        )\n        with self.assertRaisesMessage(RestrictedError, msg):\n            b1.delete()\n        self.assertTrue(DeleteTop.objects.exists())\n        self.assertTrue(B1.objects.exists())\n        self.assertTrue(B2.objects.exists())\n        self.assertTrue(DeleteBottom.objects.exists())\n        # Object referenced also with CASCADE relationship can be deleted.\n        delete_top.delete()\n        self.assertFalse(DeleteTop.objects.exists())\n        self.assertFalse(B1.objects.exists())\n        self.assertFalse(B2.objects.exists())\n        self.assertFalse(DeleteBottom.objects.exists())\n", "def test_fast_delete_deferred_fields(self):\n    # Create an instance with deferred fields.\n    user = User.objects.defer('name', 'email').create(avatar=Avatar.objects.create())\n    # Fast delete should work even when fields are deferred.\n    self.assertNumQueries(2, user.delete)\n    self.assertFalse(User.objects.exists())\n    self.assertFalse(Avatar.objects.exists())\n", "    def test_can_fast_delete_with_nullable_field(self):\n        a = create_a('setvalue')\n        collector = Collector(using='default')\n        self.assertTrue(collector.can_fast_delete(a.setvalue))\n", "    def test_add_dependency(self):\n        collector = Collector(using='default')\n        model = R\n        dependency = A\n        collector.add_dependency(model, dependency)\n        self.assertIn(model._meta.concrete_model, collector.dependencies)\n        self.assertIn(dependency._meta.concrete_model, collector.dependencies[model._meta.concrete_model])\n", "def test_restricted_deletion_with_multiple_cascades(self):\n    \"\"\"\n    Test that the RestrictedError is raised when trying to delete an object\n    with multiple cascades to the same model.\n    \"\"\"\n    # Create a DeleteTop instance\n    delete_top = DeleteTop.objects.create()\n\n    # Create two B1 instances that reference the same DeleteTop instance\n    b1_1 = B1.objects.create(delete_top=delete_top)\n    b1_2 = B1.objects.create(delete_top=delete_top)\n\n    # Create a DeleteBottom instance that references both B1 instances\n    DeleteBottom.objects.create(b1=b1_1, b2=b1_2)\n\n    # Try to delete the DeleteTop instance, which should raise a RestrictedError\n    msg = (\n        \"Cannot delete some instances of model 'B1' because they are \"\n        \"referenced through restricted foreign keys: 'DeleteBottom.b1', \"\n        \"'DeleteBottom.b2'.\"\n    )\n    with self.assertRaisesMessage(RestrictedError, msg):\n        delete_top.delete()\n"], "sample_547": ["def test_offsetimage():\n    fig, ax = plt.subplots()\n    im = np.random.rand(10, 10)\n    oi = OffsetImage(im, zoom=3)\n    ab = AnnotationBbox(oi, (.5, .5), xycoords='data',\n                        boxcoords=\"axes fraction\")\n    ax.add_artist(ab)\n    ax.set_xlim((0, 1))\n    ax.set_ylim((0, 1))\n", "def test_offsetimage():\n    fig, ax = plt.subplots()\n    arr = np.random.rand(10, 10)\n    oi = OffsetImage(arr, zoom=10)\n    ab = AnnotationBbox(oi, (0.5, 0.5), xycoords='data',\n                        boxcoords=\"axes fraction\", box_alignment=(0.5, 0.5),\n                        arrowprops=dict(arrowstyle=\"->\"))\n    ax.add_artist(ab)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n", "def test_drawingarea_add_artist():\n    fig, ax = plt.subplots()\n    da = DrawingArea(100, 100)\n    ax.add_artist(da)\n    p = mpatches.Rectangle((0, 0), 100, 100, facecolor='red', edgecolor='black')\n    da.add_artist(p)\n    assert p.axes == ax\n    assert p.get_transform() == da.get_transform()\n    assert p.figure == fig\n", "def test_auxtransformbox():\n    # Test AuxTransformBox with a simple example\n    fig, ax = plt.subplots()\n    da = DrawingArea(20, 20)\n    aux_transform = mtransforms.Affine2D().rotate_deg(45)\n    aux_transform_box = AuxTransformBox(aux_transform)\n    aux_transform_box.add_artist(da)\n    ax.add_artist(aux_transform_box)\n    ax.set_xlim(0, 30)\n    ax.set_ylim(0, 30)\n    fig.canvas.draw()\n    # No assert, just a smoke test for AuxTransformBox\n", "def test_offsetbox_draggable():\n    fig, ax = plt.subplots()\n    offsetbox = AnchoredOffsetbox(\n        loc='center',\n        child=DrawingArea(100, 100),\n        pad=0.,\n        frameon=False,\n        bbox_to_anchor=(.5, .5),\n        bbox_transform=ax.transAxes,\n        borderpad=0.\n    )\n    ax.add_artist(offsetbox)\n    draggable = DraggableOffsetBox(offsetbox, offsetbox)\n    draggable.disconnect()\n\n    # Test that the offsetbox is draggable and can be disconnected\n    assert draggable._check_still_parented()\n    offsetbox.set_offset((10, 10))\n    draggable.save_offset()\n    draggable.update_offset(10, 10)\n    assert offsetbox.get_offset() == (20, 20)\n    draggable.finalize_offset()\n    assert offsetbox.get_offset() == (20, 20)\n\n    # Test that the offsetbox is no longer draggable after being disconnected\n    draggable.disconnect()\n    assert not draggable._check_still_parented()\n    draggable.update_offset(10, 10)\n    assert offsetbox.get_offset() == (20, 20)\n"], "sample_322": ["def test_migrate_forwards_with_interleaved_dependencies(self):\n    \"\"\"\n    Migrate forwards with interleaved dependencies.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    plan = executor.migration_plan([\n        (\"migrations\", \"0002_second\"),\n        (\"migrations2\", \"0001_initial\"),\n    ])\n    self.assertEqual(\n        plan,\n        [\n            (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n            (executor.loader.graph.nodes[\"migrations2\", \"0001_initial\"], False),\n            (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False),\n        ],\n    )\n    executor.migrate([\n        (\"migrations\", \"0002_second\"),\n        (\"migrations2\", \"0001_initial\")\n    ])\n    # Rebuild the graph to reflect the new DB state\n    executor.loader.build_graph()\n    # Now plan a second time and make sure it's empty\n    plan = executor.migration_plan([\n        (\"migrations\", \"0002_second\"),\n        (\"migrations2\", \"0001_initial\"),\n    ])\n    self.assertEqual(plan, [])\n    # The resulting state should include applied migrations.\n    state = executor.migrate([\n        (\"migrations\", \"0002_second\"),\n        (\"migrations2\", \"0001_initial\"),\n    ])\n    self.assertIn(('migrations', 'book'), state.models)\n    self.assertIn(('migrations', 'author'), state.models)\n    self.assertIn(('migrations2', 'otherauthor'), state.models)\n    # Erase all the fake records\n    executor.recorder.record_unapplied(\"migrations2\", \"0001_initial\")\n    executor.recorder.record_unapplied(\"migrations\", \"0002_second\")\n    executor.recorder.record_unapplied(\"migrations\", \"0001_initial\")\n", "    def test_detect_soft_applied_empty_initial_migration(self):\n        \"\"\"\n        Test that detect_soft_applied correctly handles empty initial migrations.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        migration = executor.loader.get_migration(\"migrations\", \"0001_initial\")\n        migration.initial = False\n        migration.operations = []\n        self.assertIs(executor.detect_soft_applied(None, migration)[0], True)\n", "def test_migration_plan_with_clean_start(self):\n    \"\"\"\n    Test that migration_plan with clean_start=True correctly calculates the plan.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    # Create a state with all migrations applied\n    state = self._create_project_state(with_applied_migrations=True)\n    # Now get the plan to clean start\n    plan = executor.migration_plan([(\"migrations\", None)], clean_start=True)\n    # It should include all migrations in reverse order\n    expected_plan = [\n        (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], True),\n        (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], True),\n    ]\n    self.assertEqual(plan, expected_plan)\n", "    def test_migrate_with_replaced_migrations(self):\n        \"\"\"\n        Regression test for #25657 - Replaced migrations should be applied\n        when their replacements are applied.\n        \"\"\"\n        recorder = MigrationRecorder(connection)\n        # Place the database in a state where the replaced migrations are\n        # partially applied: 0001 is applied, 0002 is not.\n        recorder.record_applied(\"migrations\", \"0001_initial\")\n        executor = MigrationExecutor(connection)\n        # Use fake because we don't actually have the first migration\n        # applied, so the second will fail. And there's no need to actually\n        # create/modify tables here, we're just testing the\n        # MigrationRecord, which works the same with or without fake.\n        executor.migrate([(\"migrations\", \"0001_squashed_0002\")], fake=True)\n\n        # Because we've now applied 0001 and 0002 both, their squashed\n        # replacement should be marked as applied.\n        self.assertIn(\n            (\"migrations\", \"0001_squashed_0002\"),\n            recorder.applied_migrations(),\n        )\n\n        # Test that replaced migrations are applied when their replacements\n        # are applied.\n        recorder = MigrationRecorder(connection)\n        recorder.record_unapplied(\"migrations\", \"0001_initial\")\n        executor = MigrationExecutor(connection)\n        executor.migrate([(\"migrations\", \"0001_squashed_0002\")])\n\n        # Check that replaced migrations are applied.\n        self.assertIn(\n            (\"migrations\", \"0001_initial\"),\n            recorder.applied_migrations(),\n        )\n        self.assertIn(\n            (\"migrations\", \"0002_second\"),\n            recorder.applied_migrations(),\n        )\n\n        # Check that the replacement is also applied.\n        self.assertIn(\n            (\"migrations\", \"0001_squashed_0002\"),\n            recorder.applied_migrations(),\n        )\n", "    def test_migrate_with_replacement_conflicts(self):\n        \"\"\"\n        Test that replacing a migration that has conflicts with other migrations\n        works as expected.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n\n        # Create a conflicting migration in the database\n        recorder = MigrationRecorder(connection)\n        recorder.record_applied(\"migrations\", \"0002_second\")\n\n        # Try to migrate with a replacement migration\n        with self.assertRaisesMessage(DatabaseError, \"Replacing migrations with conflicts is not supported\"):\n            executor.migrate([(\"migrations\", \"0001_squashed_0002\")])\n\n        # Try to migrate with a replacement migration that replaces the conflicting migration\n        executor.migrate([(\"migrations\", \"0001_squashed_0002_replace_conflict\")])\n\n        # Check that the replacement migration was applied\n        self.assertIn(\n            (\"migrations\", \"0001_squashed_0002_replace_conflict\"),\n            recorder.applied_migrations(),\n        )\n\n        # Check that the replaced migration is marked as unapplied\n        self.assertNotIn(\n            (\"migrations\", \"0002_second\"),\n            recorder.applied_migrations(),\n        )\n\n        # Try to migrate backwards\n        executor.migrate([(\"migrations\", None)])\n\n        # Check that the replacement migration is marked as unapplied\n        self.assertNotIn(\n            (\"migrations\", \"0001_squashed_0002_replace_conflict\"),\n            recorder.applied_migrations(),\n        )\n"], "sample_342": ["def test_process_request_invalid_app_label(self):\n    request = self.factory.get(self.url, {'term': 'is', 'app_label': 'does_not_exist', 'model_name': Answer._meta.model_name, 'field_name': 'question'})\n    request.user = self.superuser\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "def test_process_request_permission_denied(self):\n    \"\"\"\n    Test process_request raises PermissionDenied when model admin is not configured\n    with search_fields.\n    \"\"\"\n    class SearchFieldlessQuestionAdmin(admin.ModelAdmin):\n        pass\n\n    with model_admin(Question, SearchFieldlessQuestionAdmin):\n        request = self.factory.get(self.url, {'term': '', **self.opts})\n        request.user = self.superuser\n        with self.assertRaises(Http404):\n            AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "    def test_process_request_invalid_app_label(self):\n        request = self.factory.get(self.url, {'term': 'is', 'app_label': 'does_not_exist', 'model_name': Answer._meta.model_name, 'field_name': 'question'})\n        request.user = self.superuser\n        with self.assertRaises(PermissionDenied):\n            AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "def test_has_perm_denies_permission_for_model_if_not_view_permission(self):\n    \"\"\"\n    If a user does not have view permission for the model, they should not be\n    able to access the autocomplete view.\n    \"\"\"\n    request = self.factory.get(self.url, {'term': 'is', **self.opts})\n    request.user = self.user\n    self.user.user_permissions.clear()\n    p = Permission.objects.get(\n        content_type=ContentType.objects.get_for_model(Question),\n        codename='delete_question',\n    )\n    self.user.user_permissions.add(p)\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "def test_process_request_invalid_model(self):\n    request = self.factory.get(self.url, {'term': 'is', 'app_label': 'non_existent_app', 'model_name': 'Model', 'field_name': 'question'})\n    request.user = self.superuser\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n"], "sample_654": ["def test_fixture_parametrization_inheritance(testdir):\n    testdir.makeconftest(\n        \"\"\"\n        import pytest\n        @pytest.fixture(params=[1, 2])\n            return request.param\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            return parent_fixture\n\n            assert child_fixture in [1, 2]\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=2)\n", "def test_setup_function_collision(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(scope=\"module\", autouse=True)\n            pass\n\n            pass\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*pytest.fixture and setup_function are not allowed to have the same name*\",\n            \"*1 error*\",\n        ]\n    )\n", "    def test_finalizer_order(self, testdir):\n        testdir.makeconftest(\n            \"\"\"\n            import pytest\n            values = []\n            @pytest.fixture(autouse=True, scope='function')\n                values.append(1)\n                request.addfinalizer(lambda: values.append(4))\n            @pytest.fixture(autouse=True, scope='function')\n                values.append(2)\n                request.addfinalizer(lambda: values.append(3))\n        \"\"\"\n        )\n        testdir.makepyfile(\n            \"\"\"\n                pass\n        \"\"\"\n        )\n        result = testdir.runpytest(\"-s\")\n        result.stdout.fnmatch_lines(\n            \"\"\"\n            *1    1\n            *2    2\n            *4    3\n            *5    4\n        \"\"\"\n        )\n", "def test_scope_module_teardown_on_failure(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(scope=\"module\")\n            request.addfinalizer(lambda: values.append(\"teardown\"))\n            values.append(\"setup\")\n            return 42\n\n        values = []\n\n            values.append(\"test1\")\n            assert myfixture == 42\n\n            values.append(\"test2\")\n            assert myfixture == 43\n        \"\"\"\n    )\n    result = testdir.runpytest(\"-s\")\n    result.stdout.fnmatch_lines(\n        \"\"\"\n        *1 failed*1 passed*\n    \"\"\"\n    )\n    values = result.getcalls(\"pytest_runtest_call\")[0].item.module.values\n    assert values == [\"setup\", \"test1\", \"teardown\", \"test2\"]\n", "    def test_finalization_order_independent_fixtures(self, testdir):\n        testdir.makeconftest(\n            \"\"\"\n            import pytest\n            from collections import defaultdict\n            import pytest\n\n            values = []\n\n            @pytest.fixture(scope='function', autouse=True)\n                values.append('setup 1')\n                request.addfinalizer(lambda: values.append('teardown 1'))\n\n            @pytest.fixture(scope='function', autouse=True)\n                values.append('setup 2')\n                request.addfinalizer(lambda: values.append('teardown 2'))\n        \"\"\"\n        )\n        testdir.makepyfile(\n            \"\"\"\n                pass\n        \"\"\"\n        )\n        reprec = testdir.inline_run(\"-v\")\n        reprec.assertoutcome(passed=1)\n        values = reprec.getcalls(\"pytest_runtest_call\")[0].item.session.config._conftestmodules[\n            \"conftest.py\"\n        ].values\n        assert values == [\n            \"setup 1\",\n            \"setup 2\",\n            \"teardown 2\",\n            \"teardown 1\",\n        ]\n"], "sample_551": ["def test_patch_2d_to_3d():\n    from matplotlib.patches import Rectangle\n    fig, ax = plt.subplots(subplot_kw=dict(projection='3d'))\n    patch = Rectangle((0.5, 0.5), 1, 1, color='r')\n    ax.add_patch(patch)\n    art3d.patch_2d_to_3d(patch, z=1, zdir='z')\n    ax.add_patch(patch)\n    assert patch.get_z() == 1\n    assert patch._dir_vec.tolist() == [0, 0, 1]\n", "def test_text_2d_to_3d():\n    fig, ax = plt.subplots(subplot_kw=dict(projection='3d'))\n    text = ax.text(0, 0, 0, 'Hello, world!')\n    art3d.text_2d_to_3d(text, z=5, zdir='y')\n    assert text.get_position_3d() == (0, 0, 5)\n    assert np.allclose(text.get_dir_vector(), np.array([0, 1, 0]))\n\n", "def test_patch_collection_2d_to_3d():\n    # Create a 2D patch collection\n    patches = [plt.Circle((0.5, 0.5), 0.2, edgecolor='black', facecolor='blue'),\n               plt.Circle((1.5, 1.5), 0.2, edgecolor='red', facecolor='green')]\n\n    fig, ax = plt.subplots(subplot_kw=dict(projection='3d'))\n    pc = art3d.PatchCollection(patches)\n    ax.add_collection(pc)\n\n    # Convert to 3D\n    art3d.patch_collection_2d_to_3d(pc, zs=0, zdir='z', depthshade=True)\n\n    # Check if the conversion was successful\n    assert isinstance(pc, art3d.Patch3DCollection)\n    assert pc.get_depthshade()\n", "def test_text_2d_to_3d():\n    # Test conversion of 2D Text to 3D Text\n    from matplotlib.text import Text\n\n    fig, ax = plt.subplots(subplot_kw=dict(projection='3d'))\n    text = Text(0, 0, 'Hello, World!')\n    text_2d_to_3d(text, z=5, zdir='z')\n    ax.add_artist(text)\n\n    assert isinstance(text, art3d.Text3D)\n    assert text.get_position_3d() == (0, 0, 5)\n    assert text.get_3d_properties() == (5, (0, 0, 1))\n\n    # Test conversion with direction vector\n    fig, ax = plt.subplots(subplot_kw=dict(projection='3d'))\n    text = Text(0, 0, 'Hello, World!')\n    text_2d_to_3d(text, z=5, zdir=(1, 1, 1))\n    ax.add_artist(text)\n\n    assert isinstance(text, art3d.Text3D)\n    assert text.get_position_3d() == (0, 0, 5)\n    assert np.allclose(text.get_3d_properties()[1], (1/np.sqrt(3), 1/np.sqrt(3), 1/np.sqrt(3)))\n", "def test_text_2d_to_3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    text = mtext.Text(0, 0, 'Test text')\n    art3d.text_2d_to_3d(text, z=1, zdir='x')\n    ax.add_artist(text)\n    assert text.get_position_3d() == (0, 0, 1)\n    text.set_position_3d((1, 1, 1), zdir='y')\n    assert text.get_position_3d() == (1, 1, 1)\n    assert text.get_dir_vector() == np.array((0, 1, 0))\n"], "sample_1109": ["def test_frac_edge_cases():\n    assert frac(0) == 0\n    assert frac(1) == 0\n    assert frac(-1) == 0\n    assert frac(oo) == AccumBounds(0, 1)\n    assert frac(-oo) == AccumBounds(0, 1)\n    assert frac(zoo) is nan\n\n    assert frac(S.Half) == S.Half\n    assert frac(-S.Half) == S.Half\n    assert frac(S.Quarter) == S.Quarter\n    assert frac(-S.Quarter) == S.ThreeQuarters\n\n    assert frac(Rational(1, 2)) == Rational(1, 2)\n    assert frac(-Rational(1, 2)) == Rational(1, 2)\n    assert frac(Rational(3, 4)) == Rational(3, 4)\n    assert frac(-Rational(3, 4)) == Rational(1, 4)\n\n    assert frac(Float(0.5)) == Float(0.5)\n    assert frac(-Float(0.5)) == Float(0.5)\n    assert frac(Float(0.75)) == Float(0.75)\n    assert frac(-Float(0.75)) == Float(0.25)\n", "def test_frac_corner_cases():\n    assert frac(0) == 0\n    assert frac(-0) == 0\n    assert frac(S.Infinity) == AccumBounds(0, 1)\n    assert frac(-S.Infinity) == AccumBounds(0, 1)\n    assert frac(S.ComplexInfinity) is nan\n    assert frac(S.NaN) is nan\n\n    assert frac(S.Half) == S.Half\n    assert frac(-S.Half) == S.Half\n\n    assert frac(Rational(1, 2)) == Rational(1, 2)\n    assert frac(-Rational(1, 2)) == Rational(1, 2)\n\n    assert frac(Float(0.5)) == Float(0.5)\n    assert frac(-Float(0.5)) == Float(0.5)\n\n    assert frac(I) == I\n    assert frac(-I) == I\n", "def test_frac_comparison():\n    # Test frac comparisons with non-integer real numbers\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    p_r = Symbol('p_r', real=True, positive=True)\n    n_r = Symbol('n_r', real=True, negative=True)\n    np_r = Symbol('n_r', real=True, nonpositive=True)\n    nn_r = Symbol('n_r', real=True, nonnegative=True)\n\n    # Test frac with real numbers\n    assert (frac(x) < y).is_Relational\n    assert (frac(x) <= y).is_Relational\n    assert (frac(x) > y).is_Relational\n    assert (frac(x) >= y).is_Relational\n    assert (frac(x) == y).is_Relational\n    assert (frac(x) != y).is_Relational\n\n    # Test frac with positive real numbers\n    assert (frac(x) < p_r).is_Relational\n    assert (frac(x) <= p_r).is_Relational\n    assert (frac(x) > p_r).is_Relational\n    assert (frac(x) >= p_r).is_Relational\n    assert (frac(x) == p_r).is_Relational\n    assert (frac(x) != p_r).is_Relational\n\n    # Test frac with negative real numbers\n    assert (frac(x) < n_r).is_Relational\n    assert (frac(x) <= n_r).is_Relational\n    assert (frac(x) > n_r).is_Relational\n    assert (frac(x) >= n_r).is_Relational\n    assert (frac(x) == n_r).is_Relational\n    assert (frac(x) != n_r).is_Relational\n\n    # Test frac with nonpositive real numbers\n    assert (frac(x) < np_r).is_Relational\n    assert (frac(x) <= np_r).is_Relational\n    assert (frac(x) > np_r).is_Relational\n    assert (frac(x) >= np_r).is_Relational\n    assert (frac(x) == np_r).is_Relational\n    assert (frac(x) != np_r).is_Relational\n\n    # Test frac with nonnegative real numbers\n    assert (frac(x) < nn_r).is_Relational\n    assert (frac(x) <= nn_r).is_Relational\n    assert (frac(x)", "def test_frac_extremes():\n    x = Symbol('x')\n    y = Symbol('y', real=True)\n    assert frac(x, evaluate=False).is_nonpositive is None\n    assert frac(x, evaluate=False).is_negative is None\n    assert frac(y).is_nonpositive is False\n    assert frac(y).is_negative is False\n    assert frac(y, evaluate=False).is_nonpositive is False\n    assert frac(y, evaluate=False).is_negative is False\n    assert frac(x, evaluate=False).is_nonnegative is None\n    assert frac(x, evaluate=False).is_positive is None\n    assert frac(y).is_nonnegative is True\n    assert frac(y).is_positive is False\n    assert frac(y, evaluate=False).is_nonnegative is True\n    assert frac(y, evaluate=False).is_positive is False\n    assert frac(x, evaluate=False).is_zero is None\n    assert frac(y).is_zero is None\n    assert frac(y, evaluate=False).is_zero is None\n    assert frac(0).is_zero is True\n    assert frac(0, evaluate=False).is_zero is True\n    assert frac(1).is_zero is True\n    assert frac(1, evaluate=False).is_zero is True\n    assert frac(-1).is_zero is True\n    assert frac(-1, evaluate=False).is_zero is True\n", "def test_frac_boundaries():\n    assert frac(0) == 0\n    assert frac(1) == 0\n    assert frac(-1) == 0\n    assert frac(Rational(1, 2)) == Rational(1, 2)\n    assert frac(Rational(-1, 2)) == Rational(1, 2)\n    assert frac(Float(0.5)) == Float(0.5)\n    assert frac(Float(-0.5)) == Float(0.5)\n    assert frac(E) - E + 2 == floor(E)\n    assert frac(-E) - (-E) + 2 == floor(-E)\n\n    assert frac(x).is_finite\n    assert frac(y).is_finite\n    assert frac(r).is_finite\n    assert frac(i).is_finite\n\n    assert frac(x).is_real == x.is_real\n    assert frac(y).is_real\n    assert frac(r).is_real\n    assert frac(i).is_real\n\n    assert frac(x).is_imaginary == x.is_imaginary\n    assert frac(y).is_imaginary == False\n    assert frac(r).is_imaginary == False\n    assert frac(i).is_imaginary\n\n    assert frac(x).is_integer == (x.is_integer or x.is_imaginary)\n    assert frac(y).is_integer == False\n    assert frac(r).is_integer == False\n    assert frac(i).is_integer == False\n\n    assert frac(x).is_zero == (x.is_zero or x.is_integer)\n    assert frac(y).is_zero == (y.is_zero or y.is_integer)\n    assert frac(r).is_zero == (r.is_zero or r.is_integer)\n    assert frac(i).is_zero == (i.is_zero or i.is_integer)\n\n    assert frac(x).is_negative is None\n    assert frac(y).is_negative == False\n    assert frac(r).is_negative is None\n    assert frac(i).is_negative is None\n"], "sample_327": ["def test_prepare_value_invalid_input(self):\n    field = JSONField()\n    with self.assertRaisesMessage(ValueError, 'Invalid JSON input'):\n        field.prepare_value(object())\n", "def test_bound_data(self):\n    class JSONForm(Form):\n        json_field = JSONField(required=False)\n\n    field = JSONForm()['json_field']\n    self.assertEqual(field.bound_data('[\"a\", \"b\"]', None), '[\"a\", \"b\"]')\n    self.assertEqual(field.bound_data(None, '[\"a\", \"b\"]'), '[\"a\", \"b\"]')\n    self.assertIsNone(field.bound_data(None, None))\n", "def test_generic_ip_address_field(self):\n    field = GenericIPAddressField(protocol='both')\n    valid_ipv4 = '192.168.1.1'\n    valid_ipv6 = '2001:0db8:85a3:0000:0000:8a2e:0370:7334'\n    invalid_ipv4 = '256.1.1.1'\n    invalid_ipv6 = '2001:0db8:85a3:0000:0000:8a2e:0370:7335:7334'\n\n    self.assertEqual(field.clean(valid_ipv4), valid_ipv4)\n    self.assertEqual(field.clean(valid_ipv6), valid_ipv6)\n\n    with self.assertRaisesMessage(ValidationError, 'Enter a valid IPv4 or IPv6 address.'):\n        field.clean(invalid_ipv4)\n\n    with self.assertRaisesMessage(ValidationError, 'Enter a valid IPv4 or IPv6 address.'):\n        field.clean(invalid_ipv6)\n\n    field = GenericIPAddressField(protocol='ipv4')\n    self.assertEqual(field.clean(valid_ipv4), valid_ipv4)\n\n    with self.assertRaisesMessage(ValidationError, 'Enter a valid IPv4 address.'):\n        field.clean(valid_ipv6)\n\n    with self.assertRaisesMessage(ValidationError, 'Enter a valid IPv4 address.'):\n        field.clean(invalid_ipv4)\n\n    with self.assertRaisesMessage(ValidationError, 'Enter a valid IPv4 address.'):\n        field.clean(invalid_ipv6)\n\n    field = GenericIPAddressField(protocol='ipv6')\n    self.assertEqual(field.clean(valid_ipv6), valid_ipv6)\n\n    with self.assertRaisesMessage(ValidationError, 'Enter a valid IPv6 address.'):\n        field.clean(valid_ipv4)\n\n    with self.assertRaisesMessage(ValidationError, 'Enter a valid IPv6 address.'):\n        field.clean(invalid_ipv4)\n\n    with self.assertRaisesMessage(ValidationError, 'Enter a valid IPv6 address.'):\n        field.clean(invalid_ipv6)\n", "def test_bounded_json_field(self):\n    class JSONForm(Form):\n        json_field = JSONField()\n\n    form = JSONForm({'json_field': '{\"key\": \"value\"}'})\n    bound_field = form['json_field']\n    self.assertIsInstance(bound_field, BoundField)\n    self.assertEqual(bound_field.value(), '{\"key\": \"value\"}')\n    self.assertEqual(bound_field.form.data['json_field'], '{\"key\": \"value\"}')\n    self.assertEqual(bound_field.field.prepare_value({'key': 'value'}), '{\"key\": \"value\"}')\n", "def test_uuidfield_prepare_value(self):\n    field = UUIDField()\n    self.assertEqual(field.prepare_value(uuid.UUID('{c141e152-6550-4172-a784-05448d98204b}')), 'c141e152-6550-4172-a784-05448d98204b')\n    self.assertEqual(field.prepare_value('c141e152-6550-4172-a784-05448d98204b'), 'c141e152-6550-4172-a784-05448d98204b')\n    self.assertEqual(field.prepare_value(None), 'null')\n"], "sample_207": ["    def test_call(self):\n        factory = KeyTransformFactory('key')\n        transform = factory()\n        self.assertIsInstance(transform, KeyTransform)\n        self.assertEqual(transform.key_name, 'key')\n", "    def test_key_transform_null_value(self):\n        obj = NullableJSONModel.objects.create(value={'key': None})\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__key=None),\n            [obj],\n        )\n", "    def test_register_lookup(self):\n        @models.JSONField.register_lookup\n        class MyLookup(lookups.Lookup):\n            lookup_name = 'my_lookup'\n        field = models.JSONField()\n        lookup = field.get_lookup('my_lookup')\n        self.assertIs(lookup, MyLookup)\n        models.JSONField._unregister_lookup(MyLookup)\n", "    def setUpTestData(cls):\n        cls.objs = [\n            NullableJSONModel.objects.create(value={'a': 1, 'b': 2}),\n            NullableJSONModel.objects.create(value={'a': 3, 'c': 4}),\n            NullableJSONModel.objects.create(value={'b': 5, 'd': 6}),\n        ]\n", "    def test_get_transform(self):\n        field = models.JSONField()\n        transform = field.get_transform('test')\n        self.assertIsInstance(transform, KeyTransformFactory)\n"], "sample_778": ["def test_nmf_max_iter_reached():\n    # Test that a warning is raised when max_iter is reached\n    n_samples = 20\n    n_features = 15\n    n_components = 10\n    alpha = 0.1\n    l1_ratio = 0.5\n    tol = 1e-10\n    max_iter = 1\n\n    # initialization\n    rng = np.random.mtrand.RandomState(42)\n    X = rng.randn(n_samples, n_features)\n    np.abs(X, X)\n\n    for solver in ('cd', 'mu'):\n        msg = (\"Maximum number of iteration %d reached. Increase it to\"\n               \" improve convergence.\" % max_iter)\n        assert_warns_message(ConvergenceWarning, msg,\n                            NMF(n_components=n_components, solver=solver,\n                                alpha=alpha, l1_ratio=l1_ratio,\n                                max_iter=max_iter, tol=tol, random_state=42\n                                ).fit, X)\n", "def test_nmf_decomposition_with_zeros(solver):\n    # Test that the decomposition does not contain negative values\n    # when the input matrix contains zeros.\n    A = np.c_[np.zeros(5), np.arange(1, 6)]\n    model = NMF(n_components=2, solver=solver, init='random',\n                random_state=0)\n    transf = model.fit_transform(A)\n    assert not((model.components_ < 0).any() or\n               (transf < 0).any())\n", "def test_nmf_multiplicative_update_zero_beta_loss():\n    # Test that multiplicative update NMF can handle beta_loss=0.\n    n_samples = 20\n    n_features = 10\n    n_components = 5\n    rng = np.random.mtrand.RandomState(42)\n    X = rng.randn(n_samples, n_features)\n    np.clip(X, 0, None, out=X)\n    X_csr = sp.csr_matrix(X)\n    W0, H0 = nmf._initialize_nmf(X, n_components, init='random',\n                                 random_state=42)\n\n    W, H, _ = non_negative_factorization(\n        X_csr, W0, H0, n_components, init='custom', update_H=True,\n        solver='mu', beta_loss=0., max_iter=100, alpha=0., l1_ratio=0.,\n        regularization='both', random_state=42)\n\n    assert not np.any(np.isnan(W))\n    assert not np.any(np.isnan(H))\n\n    # Test that the output values are non-negative\n    assert not ((W < 0).any() or (H < 0).any())\n", "def test_nmf_beta_divergence_input_validation():\n    # Test that _beta_divergence raises an error for incorrect input parameters\n    rng = np.random.mtrand.RandomState(42)\n    n_samples = 10\n    n_features = 5\n    n_components = 3\n    X = np.abs(rng.randn(n_samples, n_features))\n    W = np.abs(rng.randn(n_samples, n_components))\n    H = np.abs(rng.randn(n_components, n_features))\n\n    msg = \"Invalid beta_loss parameter: got 'spam' instead of one of\"\n    assert_raise_message(ValueError, msg, nmf._beta_divergence, X, W, H, 'spam')\n\n    msg = \"Invalid beta_loss parameter: got -2 instead of one of\"\n    assert_raise_message(ValueError, msg, nmf._beta_divergence, X, W, H, -2)\n\n    msg = \"Invalid beta_loss parameter: got 2.5 instead of one of\"\n    assert_raise_message(ValueError, msg, nmf._beta_divergence, X, W, H, 2.5)\n\n    msg = \"beta must be in (0, 2] but is 2.5\"\n    assert_raise_message(ValueError, msg, nmf._beta_divergence, X, W, H, 2.5,\n                        square_root=True)\n", "def test_nmf_update_coordinate_descent_zeros_in_H():\n    # Test the case when H contains zeros and Coordinate Descent solver is used\n    rng = np.random.mtrand.RandomState(42)\n    n_samples, n_features, n_components = 10, 2, 2\n    X = np.abs(rng.randn(n_samples, n_features))\n\n    # H contains zeros\n    H = np.array([[1, 0], [0, 1]])\n\n    W = np.abs(rng.randn(n_samples, n_components))\n\n    # the expected behavior is to set the corresponding elements to zero in W\n    W_cdnmf = nmf._update_coordinate_descent(X, W.copy(), H.T, 0, 0, False, rng)\n    assert_array_almost_equal(W_cdnmf[:, 1], np.zeros(n_samples))\n\n    # test that the first column of W is updated\n    assert not np.array_equal(W_cdnmf[:, 0], W[:, 0])\n"], "sample_405": ["def test_rename_model_mti(self):\n    \"\"\"\n    Tests the RenameModel operation on a multi-table inheritance model.\n    \"\"\"\n    project_state = self.set_up_test_model(\"test_rnmtimo\", mti_model=True)\n    # Test the state alteration\n    operation = migrations.RenameModel(\"Pony\", \"Horse\")\n    self.assertEqual(operation.describe(), \"Rename model Pony to Horse\")\n    new_state = project_state.clone()\n    operation.state_forwards(\"test_rnmtimo\", new_state)\n    self.assertNotIn((\"test_rnmtimo\", \"pony\"), new_state.models)\n    self.assertIn((\"test_rnmtimo\", \"horse\"), new_state.models)\n    # RenameModel also repoints all incoming FKs and M2Ms\n    self.assertEqual(\n        \"test_rnmtimo.ShetlandHorse\",\n        new_state.models[\"test_rnmtimo\", \"shetlandhorse\"]\n        .fields[\"horse\"]\n        .remote_field.model,\n    )\n    Horse = new_state.apps.get_model(\"test_rnmtimo\", \"Horse\")\n    self.assertIs(\n        Horse._meta.get_field(\"horse\").remote_field.model, Horse\n    )\n    # Test the database alteration\n    self.assertTableExists(\"test_rnmtimo_pony\")\n    self.assertTableExists(\"test_rnmtimo_shetlandpony\")\n    self.assertColumnExists(\"test_rnmtimo_shetlandpony\", \"pony_ptr_id\")\n    with connection.schema_editor(\n        atomic=connection.features.supports_atomic_references_rename\n    ) as editor:\n        operation.database_forwards(\"test_rnmtimo\", editor, project_state, new_state)\n    self.assertTableNotExists(\"test_rnmtimo_pony\")\n    self.assertTableExists(\"test_rnmtimo_horse\")\n    self.assertTableExists(\"test_rnmtimo_shetlandpony\")\n    self.assertColumnExists(\"test_rnmtimo_shetlandpony\", \"horse_ptr_id\")\n    # And test reversal\n    with connection.schema_editor(\n        atomic=connection.features.supports_atomic_references_rename\n    ) as editor:\n        operation.database_backwards(\n            \"test_rnmtimo\", editor, new_state, project_state\n        )\n    self.assertTableExists(\"test_rnmtimo_pony\")\n    self.assertTableNotExists(\"test_rnmtimo_horse\")\n    self.assertTableExists(\"test_rnmt", "def test_rename_model_with_fks_and_self_referential_fields(self):\n    \"\"\"\n    RenameModel operation correctly handles models with foreign keys to\n    themselves and other models.\n    \"\"\"\n    app_label = \"test_rename_model_with_fks_and_self_referential_fields\"\n    project_state = self.apply_operations(\n        app_label,\n        ProjectState(),\n        operations=[\n            migrations.CreateModel(\n                \"Pony\",\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=20)),\n                ],\n            ),\n            migrations.CreateModel(\n                \"Rider\",\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"pony\", models.ForeignKey(\"Pony\", models.CASCADE)),\n                    (\"friend\", models.ForeignKey(\"self\", models.CASCADE)),\n                ],\n            ),\n        ],\n    )\n    Pony = project_state.apps.get_model(app_label, \"Pony\")\n    Rider = project_state.apps.get_model(app_label, \"Rider\")\n    pony = Pony.objects.create(name=\"a good name\")\n    rider = Rider.objects.create(pony=pony, friend=rider)\n\n    project_state = self.apply_operations(\n        app_label,\n        project_state,\n        operations=[\n            migrations.RenameModel(\"Pony\", \"Pony2\"),\n        ],\n        atomic=connection.features.supports_atomic_references_rename,\n    )\n    Pony = project_state.apps.get_model(app_label, \"Pony2\")\n    Rider = project_state.apps.get_model(app_label, \"Rider\")\n    self.assertEqual(Rider._meta.get_field(\"pony\").remote_field.model, Pony)\n    pony = Pony.objects.create(name=\"a good name\")\n    rider = Rider.objects.create(pony=pony, friend=rider)\n    self.assertEqual(Rider._meta.get_field(\"friend\").remote_field.model, Rider)\n", "def test_add_model_with_functional_unique_constraint(self):\n    app_label = \"test_add_model_with_functional_unique_constraint\"\n    constraint_name = f\"{app_label}_pony_uq\"\n    table_name = f\"{app_label}_pony\"\n    project_state = self.apply_operations(\n        app_label,\n        ProjectState(),\n        operations=[\n            migrations.CreateModel(\n                \"Pony\",\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"weight\", models.FloatField()),\n                ],\n                constraints=[\n                    models.UniqueConstraint(\n                        \"weight\",\n                        name=constraint_name,\n                    ),\n                ],\n            ),\n        ],\n    )\n    new_state = project_state.clone()\n    operation = migrations.AddField(\n        \"Pony\",\n        \"pink\",\n        models.IntegerField(unique=True),\n    )\n    operation.state_forwards(app_label, new_state)\n    self.assertEqual(\n        len(new_state.models[app_label, \"pony\"].options[\"constraints\"]), 2\n    )\n    Pony = new_state.apps.get_model(app_label, \"Pony\")\n    self.assertEqual(len(Pony._meta.constraints), 2)\n    with connection.schema_editor() as editor:\n        operation.database_forwards(app_label, editor, project_state, new_state)\n    Pony.objects.create(pink=1, weight=4.0)\n    if connection.features.supports_expression_indexes:\n        with self.assertRaises(IntegrityError):\n            Pony.objects.create(weight=4.0)\n    else:\n        Pony.objects.create(weight=4.0)\n    Pony.objects.create(pink=2, weight=5.0)\n    Pony.objects.create(pink=3, weight=6.0)\n    Pony.objects.create(pink=4, weight=7.0)\n    Pony.objects.create(pink=5, weight=8.0)\n    Pony.objects.create(pink=6, weight=9.0)\n    Pony.objects.create(pink=7, weight=10.0)\n", "def test_reduce(self):\n    # Test ModelOperation.reduce\n    operation = migrations.CreateModel(\n        \"Pony\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"pink\", models.IntegerField(default=1)),\n        ],\n    )\n    other_operation = migrations.DeleteModel(\"Pony\")\n    self.assertEqual(operation.reduce(other_operation, \"test_crmo\"), [])\n\n    # Test ModelOperation.reduce with AlterField\n    alter_field_operation = migrations.AlterField(\n        \"Pony\",\n        \"pink\",\n        models.IntegerField(null=True),\n    )\n    self.assertEqual(\n        operation.reduce(alter_field_operation, \"test_crmo\"),\n        [\n            migrations.CreateModel(\n                \"Pony\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"pink\", models.IntegerField(null=True, default=1)),\n                ],\n            ),\n        ],\n    )\n\n    # Test ModelOperation.reduce with RenameField\n    rename_field_operation = migrations.RenameField(\"Pony\", \"pink\", \"blue\")\n    self.assertEqual(\n        operation.reduce(rename_field_operation, \"test_crmo\"),\n        [\n            migrations.CreateModel(\n                \"Pony\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"blue\", models.IntegerField(default=1)),\n                ],\n            ),\n        ],\n    )\n\n    # Test ModelOperation.reduce with AddConstraint\n    check_constraint = models.CheckConstraint(\n        check=models.Q(pink__gt=2), name=\"test_constraint_pony_pink_gt_2\"\n    )\n    add_constraint_operation = migrations.AddConstraint(\"Pony\", check_constraint)\n    self.assertEqual(\n        operation.reduce(add_constraint_operation, \"test_crmo\"),\n        [\n            migrations.CreateModel(\n                \"Pony\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"pink\", models.IntegerField(default=1)),\n                ],\n                options={\"constraints\": [check_constraint]},\n            ),\n        ],\n    )\n\n    # Test ModelOperation.reduce with AddIndex\n    index = models.Index(fields=[\"pink\"], name=\"test_index\")\n    add_index_operation = migrations.AddIndex(\"Pony\", index)\n    self.assertEqual(\n        operation.reduce(add_index_operation, \"test_crmo\"),\n        [\n            migrations.CreateModel(\n                \"Pony\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"pink\", models.IntegerField(default=1)),\n                ],\n                options={\"indexes\": [index]},\n            ),\n        ],\n    )\n\n    # Test ModelOperation.reduce with AlterModelTable\n    alter_model_table_operation = migrations.Alter", "def test_rename_model_with_multiple_tables(self):\n    project_state = self.set_up_test_model(\"test_rnmmut\")\n    # Create a model with multiple tables.\n    project_state = self.apply_operations(\n        \"test_rnmmut\",\n        project_state,\n        [\n            migrations.CreateModel(\n                \"MultiTable\",\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"text\", models.TextField()),\n                ],\n                options={\"db_table\": \"multitable\"},\n            ),\n            migrations.CreateModel(\n                \"MultiTableChild\",\n                fields=[\n                    (\"multitable_ptr\", models.OneToOneField(\n                        \"MultiTable\", models.CASCADE, parent_link=True, primary_key=True\n                    )),\n                    (\"number\", models.IntegerField(default=0)),\n                ],\n                options={\"db_table\": \"multitablechild\"},\n            ),\n            migrations.AddField(\n                \"MultiTableChild\",\n                \"mt\",\n                models.ForeignKey(\"MultiTable\", models.CASCADE),\n            ),\n        ],\n    )\n    # Rename the model.\n    operation = migrations.RenameModel(\"MultiTable\", \"SingleTable\")\n    project_state = self.apply_operations(\n        \"test_rnmmut\",\n        project_state,\n        [operation],\n        atomic=connection.features.supports_atomic_references_rename,\n    )\n    # Verify the model was renamed correctly.\n    SingleTable = project_state.apps.get_model(\"test_rnmmut\", \"SingleTable\")\n    MultiTableChild = project_state.apps.get_model(\"test_rnmmut\", \"MultiTableChild\")\n    self.assertEqual(SingleTable._meta.db_table, \"multitable\")\n    self.assertEqual(MultiTableChild._meta.db_table, \"multitablechild\")\n    self.assertEqual(MultiTableChild._meta.get_field(\"multitable_ptr\").related_model, SingleTable)\n    self.assertEqual(MultiTableChild._meta.get_field(\"mt\").remote_field.model, SingleTable)\n"], "sample_545": ["def test_mosaic_mixed_types():\n    fig = plt.figure()\n    with pytest.raises(ValueError, match=\"List mosaic specification must be 2D\"):\n        fig.subplot_mosaic([[\"A\", \"B\", 1]])\n", "def test_set_layout_engine_at_create():\n    fig = Figure(layout=\"constrained\")\n    assert isinstance(fig.get_layout_engine(), ConstrainedLayoutEngine)\n    fig.set_layout_engine(layout=\"tight\")\n    assert isinstance(fig.get_layout_engine(), TightLayoutEngine)\n    fig.set_layout_engine(layout=\"constrained\")\n    assert isinstance(fig.get_layout_engine(), ConstrainedLayoutEngine)\n", "def test_subfigure_add_subplot_kwargs(fig_test, fig_ref):\n    fig_test.subfigures(1, 2)\n    fig_test.add_subplot(1, 1, 1)\n    fig_ref.add_subplot(1, 1, 1)\n    fig_ref.add_subplot(1, 1, 1, facecolor='r')\n", "def test_subfigure_suptitle_and_supxlabel():\n    fig = plt.figure()\n    sfig = fig.add_subfigure()\n    ax = sfig.add_subplot(111)\n    ax.plot([1, 2, 3], [1, 2, 3])\n    ax.set_xlabel('x-label')\n    ax.set_ylabel('y-label')\n    ax.set_title('axes title')\n    fig.suptitle('figure title')\n    sfig.supxlabel('subfigure label x')\n    sfig.supylabel('subfigure label y')\n    sfig.suptitle('subfigure title')\n    fig.draw_without_rendering()\n    assert fig._suptitle.get_text() == 'figure title'\n    assert sfig._suptitle.get_text() == 'subfigure title'\n    assert sfig._supxlabel.get_text() == 'subfigure label x'\n    assert sfig._supylabel.get_text() == 'subfigure label y'\n", "def test_mosaic_sharex():\n    fig = plt.figure()\n    axs = fig.subplot_mosaic(\n        [['A', 'B'],\n         ['C', 'D'],\n         ['C', 'D']],\n        sharex=True\n    )\n    for ax in axs.values():\n        ax.plot([0, 1])\n        ax.set_xticklabels(['0', '1'])\n    axs['C'].set_xlabel('shared')\n"], "sample_390": ["def test_directory_index_with_non_ascii_characters(self):\n    response = self.client.get(\"/%s/subdir/%C3%A9/\" % self.prefix)\n    self.assertContains(response, \"Index of subdir/\u00e9/\")\n    self.assertEqual(response.context[\"file_list\"], [\"visible\"])\n", "    def test_directory_index_trailing_slash(self):\n        \"The directory index view handles paths with a trailing slash\"\n        response = self.client.get(\"/%s/subdir/\" % self.prefix)\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, \"Index of subdir/\")\n        self.assertEqual(response.context[\"directory\"], \"subdir/\")\n        response = self.client.get(\"/%s/subdir\" % self.prefix)\n        self.assertEqual(response.status_code, 404)\n", "def test_directory_index_with_show_indexes(self):\n    \"The static view can serve directory indexes\"\n    url_pattern = static(self.prefix, document_root=media_dir, show_indexes=True)\n    with self.settings(ROOT_URLCONF=\"view_tests.urls\"):\n        response = self.client.get(\"/%s/\" % self.prefix)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.templates[0].name, \"static/directory_index.html\")\n        self.assertIn(\"file.txt\", response.context[\"file_list\"])\n        self.assertIn(\"subdir/\", response.context[\"file_list\"])\n", "def test_directory_index_with_unicode(self):\n    # Test that the directory index view handles directories and files with non-ASCII names\n    unicode_dir_name = \"uni\u00e7od\u00e9\"\n    unicode_file_name = \"uni\u00e7od\u00e9_file.txt\"\n    dir_path = path.join(media_dir, unicode_dir_name)\n    file_path = path.join(dir_path, unicode_file_name)\n    Path(dir_path).mkdir(exist_ok=True)\n    with open(file_path, \"w\") as fp:\n        fp.write(\"Hello, world!\")\n\n    response = self.client.get(\"/%s/%s/\" % (self.prefix, unicode_dir_name))\n    self.assertEqual(response.status_code, 200)\n    self.assertContains(response, \"Index of uni\u00e7od\u00e9/\")\n    self.assertContains(response, \"uni\u00e7od\u00e9_file.txt\")\n", "def test_was_modified_since_unparsable_length(self):\n    \"\"\"Handle unparsable Content-Length in If-Modified-Since header\"\"\"\n    header = \"Thu, 01 Jan 1970 00:00:00 GMT; length=abc\"\n    self.assertTrue(was_modified_since(header, 1))\n"], "sample_639": ["def test_base_checker_consistency() -> None:\n    \"\"\"Test consistency of msgid in BaseChecker.\"\"\"\n    class InconsistentChecker(BaseChecker):\n        name = \"inconsistent\"\n        msgs = {\n            \"W0001\": (\n                \"Inconsistent checker has an example.\",\n                \"inconsistent-checker-example\",\n                \"Used nowhere and serves no purpose.\",\n            ),\n            \"X0002\": (\n                \"Different prefix.\",\n                \"different-prefix\",\n                \"This msgid should raise an exception.\",\n            ),\n        }\n\n    inconsistent_checker = InconsistentChecker()\n    with pytest.raises(InvalidMessageError):\n        inconsistent_checker.check_consistency()\n\n    class ConsistentChecker(BaseChecker):\n        name = \"consistent\"\n        msgs = {\n            \"W0001\": (\n                \"Consistent checker has an example.\",\n                \"consistent-checker-example\",\n                \"Used nowhere and serves no purpose.\",\n            ),\n            \"W0002\": (\n                \"Another example.\",\n                \"another-example\",\n                \"This msgid should not raise an exception.\",\n            ),\n        }\n\n    consistent_checker = ConsistentChecker()\n    consistent_checker.check_consistency()\n", "def test_base_checker_add_message() -> None:\n    \"\"\"Test the add_message method of the BaseChecker class.\"\"\"\n    basic = OtherBasicChecker()\n    class FakeLinter:\n            self.msgid = msgid\n            self.line = line\n            self.node = node\n            self.args = args\n            self.confidence = confidence\n            self.col_offset = col_offset\n            self.end_lineno = end_lineno\n            self.end_col_offset = end_col_offset\n\n    basic.linter = FakeLinter()\n    basic.add_message(\"W0001\", line=10, node=None, args=(\"example\",), confidence=\"HIGH\", col_offset=5, end_lineno=10, end_col_offset=15)\n    assert basic.linter.msgid == \"W0001\"\n    assert basic.linter.line == 10\n    assert basic.linter.node is None\n    assert basic.linter.args == (\"example\",)\n    assert basic.linter.confidence == \"HIGH\"\n    assert basic.linter.col_offset == 5\n    assert basic.linter.end_lineno == 10\n    assert basic.linter.end_col_offset == 15\n\n", "def test_base_checker_add_message() -> None:\n    \"\"\"Test adding a message to the linter.\"\"\"\n    linter = Mock()\n    checker = OtherBasicChecker(linter=linter)\n    msgid = \"W0001\"\n    line = 10\n    node = Mock()\n    args = [\"arg1\", \"arg2\"]\n    confidence = Confidence.HIGH\n    col_offset = 5\n    end_lineno = 11\n    end_col_offset = 10\n    checker.add_message(\n        msgid,\n        line,\n        node,\n        args,\n        confidence,\n        col_offset,\n        end_lineno,\n        end_col_offset,\n    )\n    linter.add_message.assert_called_once_with(\n        msgid,\n        line,\n        node,\n        args,\n        confidence,\n        col_offset,\n        end_lineno,\n        end_col_offset,\n    )\n\n", "def test_base_checker_message_definitions() -> None:\n    \"\"\"Test creation of message definitions.\"\"\"\n    basic = OtherBasicChecker()\n    messages = basic.messages\n    assert len(messages) == 1\n    message = messages[0]\n    assert message.msgid == \"W0001\"\n    assert message.msg == \"Basic checker has an example.\"\n    assert message.symbol == \"basic-checker-example\"\n    assert message.confidence == Confidence.HIGH\n", "def test_base_checker_hash() -> None:\n    \"\"\"Test the hash functionality of BaseChecker.\"\"\"\n    fake_checker_1 = OtherBasicChecker()\n    fake_checker_2 = OtherBasicChecker()\n    assert fake_checker_1 == fake_checker_2\n    assert hash(fake_checker_1) == hash(fake_checker_2)\n    fake_checker_3 = DifferentBasicChecker()\n    assert fake_checker_1 != fake_checker_3\n    assert hash(fake_checker_1) != hash(fake_checker_3)\n\n"], "sample_315": ["    def test_process_request_with_language_in_path(self):\n        request = RequestFactory().get('/en/test/')\n        middleware = LocaleMiddleware(lambda req: HttpResponse())\n        middleware.process_request(request)\n        self.assertEqual(request.LANGUAGE_CODE, 'en')\n", "    def test_language_from_path(self):\n        request = RequestFactory().get('/nl/prefixed/')\n        middleware = LocaleMiddleware(lambda req: HttpResponse())\n        middleware.process_request(request)\n        self.assertEqual(request.LANGUAGE_CODE, 'nl')\n", "    def test_process_request_language_from_path(self):\n        request = RequestFactory().get('/nl/prefixed/')\n        middleware = LocaleMiddleware(lambda req: HttpResponse())\n        middleware.process_request(request)\n        self.assertEqual(request.LANGUAGE_CODE, 'nl')\n", "    def test_language_activation(self):\n        request = RequestFactory().get('/nl/')\n        middleware = LocaleMiddleware(lambda req: HttpResponse())\n        with translation.override('en'):\n            middleware.process_request(request)\n        self.assertEqual(translation.get_language(), 'nl')\n        self.assertEqual(request.LANGUAGE_CODE, 'nl')\n", "    def test_process_request(self):\n        request = RequestFactory().get('/nl/')\n        middleware = LocaleMiddleware(lambda req: HttpResponse())\n        with translation.override('nl'):\n            middleware.process_request(request)\n        self.assertEqual(request.LANGUAGE_CODE, 'nl')\n"], "sample_501": ["def test_legend_set_frame_on():\n    # test that set_frame_on works as expected\n    fig, ax = plt.subplots()\n    ax.plot(range(10))\n    leg = ax.legend()\n    assert leg.get_frame_on() is True\n    leg.set_frame_on(False)\n    assert leg.get_frame_on() is False\n    leg.set_frame_on(True)\n    assert leg.get_frame_on() is True\n", "def test_legend_title_fontprop_update():\n    # test that title_fontproperties is updated\n    fig, ax = plt.subplots()\n    ax.plot(range(10))\n    leg = ax.legend(title='Aardvark', title_fontproperties={'family': 'serif'})\n    leg.set_title('Aardvark', prop={'family': 'monospace'})\n    assert leg.get_title().get_fontname() == 'monospace'\n", "def test_legend_handler_map():\n    # test that the legend_handler_map kwarg is used\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label='test')\n    handles, labels = ax.get_legend_handles_labels()\n    handler_map = {Line2D: mlegend.HandlerTuple(ndivide=None)}\n    with mock.patch('matplotlib.legend.Legend') as Legend:\n        ax.legend(handler_map=handler_map)\n    Legend.assert_called_with(ax, handles, labels, handler_map=handler_map)\n", "def test_draggable_legend():\n    # test that the legend can be made draggable\n    fig, ax = plt.subplots()\n    x = np.arange(10)\n    ax.plot(x, x, label='y=x')\n    leg = ax.legend()\n    assert not leg.get_draggable()\n\n    # test that set_draggable makes the legend draggable\n    leg.set_draggable(True)\n    assert leg.get_draggable()\n\n    # test that set_draggable makes the legend non-draggable\n    leg.set_draggable(False)\n    assert not leg.get_draggable()\n\n    # test that an exception is raised when an invalid update value is given\n    with pytest.raises(ValueError):\n        leg.set_draggable(True, update='invalid')\n", "def test_legend_draggable():\n    fig, ax = plt.subplots()\n    line1, = ax.plot([1, 2, 3], label='line1')\n    line2, = ax.plot([3, 2, 1], label='line2')\n    legend = ax.legend([line1, line2], ['A', 'B'], loc='upper right')\n\n    # Make sure the legend is initially not draggable\n    assert not legend.get_draggable()\n\n    # Make the legend draggable\n    legend.set_draggable(True)\n\n    # Make sure the legend is now draggable\n    assert legend.get_draggable()\n\n    # Check that the legend has a DraggableLegend object\n    assert isinstance(legend._draggable, mlegend.DraggableLegend)\n\n    # Check that the DraggableLegend object has the right legend\n    assert legend._draggable.legend is legend\n\n    # Make sure the legend is still draggable after drawing\n    fig.canvas.draw()\n    assert legend.get_draggable()\n\n    # Turn off the draggable functionality\n    legend.set_draggable(False)\n\n    # Check that the legend is no longer draggable\n    assert not legend.get_draggable()\n\n    # Check that the DraggableLegend object has been removed\n    assert legend._draggable is None\n"], "sample_1070": ["def test_exp_polar():\n    assert exp_polar(0).as_numer_denom() == (exp_polar(0), 1)\n    assert exp_polar(-x).as_numer_denom() == (1, exp_polar(x))\n    assert exp_polar(-2*x).as_numer_denom() == (1, exp_polar(2*x))\n    assert exp_polar(-2).as_numer_denom() == (1, exp_polar(2))\n    assert exp_polar(n).as_numer_denom() == (1, exp_polar(-n))\n    assert exp_polar(-n).as_numer_denom() == (exp_polar(-n), 1)\n    assert exp_polar(-I*x).as_numer_denom() == (1, exp_polar(I*x))\n    assert exp_polar(-I*n).as_numer_denom() == (1, exp_polar(I*n))\n    assert exp_polar(-n).as_numer_denom() == (exp_polar(-n), 1)\n\n    x, y = symbols('x y', polar=True)\n    assert exp_polar(x + y).as_numer_denom() == (exp_polar(x + y), 1)\n    assert exp_polar(x + y).as_numer_denom() == (exp_polar(x + y), 1)\n    assert exp_polar(2 + 3*I).as_numer_denom() == (exp_polar(2 + 3*I), 1)\n\n    assert exp_polar(I*10).n() == exp_polar(I*10)\n\n    assert log(exp_polar(z)) == z\n    assert log(x*y).expand() == log(x) + log(y)\n    assert log(x**z).expand() == z*log(x)\n\n    assert exp_polar(3).exp == 3\n\n    assert exp_polar(0).is_rational is True\n", "def test_exp_derivative():\n    x, y, z = symbols('x y z')\n    f = Function('f')\n    assert exp(x).diff(x) == exp(x)\n    assert exp(x).diff(y) == 0\n    assert exp(f(x)).diff(x) == exp(f(x))*f(x).diff(x)\n    assert exp(x).diff(x, 2) == exp(x)\n    assert exp(x*y).diff(x) == y*exp(x*y)\n    assert exp(x*y).diff(y) == x*exp(x*y)\n    assert exp(f(x)).diff(x, 2) == exp(f(x))*f(x).diff(x, 2) + f(x).diff(x)**2*exp(f(x))\n    assert exp(x).diff(x, 3) == exp(x)\n    assert exp(x).diff(x, 4) == exp(x)\n", "def test_exp_polar_assumptions():\n    x = symbols('x', positive=True)\n    i = symbols('i', imaginary=True)\n    r = symbols('r', real=True)\n    p = symbols('p', polar=True)\n\n    assert exp_polar(x).is_real is True\n    assert exp_polar(i).is_real is None\n    assert exp_polar(r).is_real is True\n    assert exp_polar(p).is_real is True\n    assert exp_polar(0).is_real is True\n    assert exp_polar(0).is_imaginary is False\n    assert exp_polar(-x).is_real is True\n    assert exp_polar(-i).is_real is None\n    assert exp_polar(-r).is_real is True\n    assert exp_polar(-p).is_real is True\n\n    assert exp_polar(1).is_extended_real is True\n    assert exp_polar(I).is_extended_real is False\n\n    assert exp_polar(x).is_extended_positive is True\n    assert exp_polar(-x).is_extended_positive is True\n    assert exp_polar(0).is_extended_positive is True\n    assert exp_polar(I).is_extended_positive is None\n    assert exp_polar(-I).is_extended_positive is None\n", "def test_exp_polar_fdiff():\n    x = Symbol('x')\n    raises(ArgumentIndexError, lambda: exp_polar(x).fdiff(2))\n", "def test_exp_polar_is_real():\n    x, y = symbols('x y')\n    assert exp_polar(x).is_real is None\n    assert exp_polar(y).is_real is None\n    assert exp_polar(2*x).is_real is None\n    assert exp_polar(2*y).is_real is None\n    assert exp_polar(x + I*y).is_real is None\n    assert exp_polar(I).is_real is False\n    assert exp_polar(2*I).is_real is False\n    assert exp_polar(I*pi/2).is_real is False\n    assert exp_polar(I*pi).is_real is False\n    assert exp_polar(3*I*pi/2).is_real is False\n    assert exp_polar(2*I*pi).is_real is False\n    assert exp_polar(x*I*pi/2).is_real is None\n    assert exp_polar(2*x*I*pi).is_real is None\n    assert exp_polar(3*y*I*pi/2).is_real is None\n    assert exp_polar(4*y*I*pi).is_real is None\n    assert exp_polar(x + I*pi/2).is_real is None\n    assert exp_polar(y + 2*I*pi).is_real is None\n"], "sample_73": ["    def test_url_without_base_url(self):\n        storage = storage.StaticFilesStorage(location='/path/to/static/files')\n        with self.assertRaisesMessage(ImproperlyConfigured, 'STATIC_URL cannot be empty'):\n            storage.url('test.txt')\n", "    def test_empty_path(self):\n        configured_storage = storage.staticfiles_storage\n        self.assertEqual(configured_storage.hashed_name(\"\", filename=\"\"), \"\")\n        self.assertEqual(configured_storage.stored_name(\"\"), \"\")\n        self.assertEqual(configured_storage.hashed_name(\"\", content=b\"\"), \"\")\n        self.assertEqual(configured_storage.stored_name(\"\", content=b\"\"), \"\")\n", "    def setUp(self):\n        super().setUp()\n        # Create files with non-normalized paths\n        self._test_dir = os.path.join(settings.STATIC_ROOT, 'test')\n        os.makedirs(self._test_dir, exist_ok=True)\n        self._non_normalized_path = os.path.join(self._test_dir, 'path/to/../file.txt')\n        with open(self._non_normalized_path, 'w') as f:\n            f.write('content')\n", "    def test_hashing_empty_file(self):\n        relpath = self.hashed_file_path(\"test/empty.txt\")\n        self.assertEqual(relpath, \"test/empty.d41d8cd98f00.txt\")\n        with storage.staticfiles_storage.open(relpath) as relfile:\n            content = relfile.read()\n            self.assertEqual(content, b\"\")\n", "    def test_cache_invalidation(self):\n        storage.staticfiles_storage.hashed_files.clear()\n        relpath = self.hashed_file_path(\"cached/styles.css\")\n        self.assertEqual(relpath, \"cached/styles.5e0040571e1a.css\")\n\n        # Update the file\n        with open(os.path.join(TEST_ROOT, 'project', 'static', 'cached', 'styles.css'), 'w') as f:\n            f.write('/* changed */')\n\n        # Make sure the cache is invalidated\n        relpath = self.hashed_file_path(\"cached/styles.css\")\n        self.assertNotEqual(relpath, \"cached/styles.5e0040571e1a.css\")\n\n        # Make sure the new file is used\n        with storage.staticfiles_storage.open(relpath) as relfile:\n            content = relfile.read()\n            self.assertIn(b'/* changed */', content)\n"], "sample_62": ["    def test_admin_site_attributes(self):\n        site = admin.AdminSite()\n        self.assertEqual(site.site_title, 'Django site admin')\n        self.assertEqual(site.site_header, 'Django administration')\n        self.assertEqual(site.index_title, 'Site administration')\n        self.assertEqual(site.site_url, '/')\n        self.assertEqual(site.empty_value_display, '-')\n", "    def setUp(self):\n        self.site = admin.AdminSite()\n", "    def setUp(self):\n        self.site = admin.AdminSite()\n", "    def setUp(self):\n        self.site = admin.AdminSite()\n", "    def test_admin_site_urls(self):\n        admin_site = admin.AdminSite()\n        admin_site.register(Person)\n        urls = admin_site.get_urls()\n        self.assertEqual(len(urls), 10)\n"], "sample_447": ["def test_nested_combined_expression_annotation_with_aggregation(self):\n    book = (\n        Book.objects.annotate(\n            combined=ExpressionWrapper(\n                Value(3) * (F(\"pages\") + F(\"rating\")), output_field=FloatField()\n            ),\n            rating_count=Count(\"rating\"),\n        )\n        .filter(isbn=\"159059725\")\n        .first()\n    )\n    self.assertEqual(book.combined, 1341.5)\n    self.assertEqual(book.rating_count, 1)\n", "def test_case_when_annotation(self):\n    books = Book.objects.annotate(\n        rating_label=Case(\n            When(rating__lt=3, then=Value(\"low\")),\n            When(rating__lt=4, then=Value(\"medium\")),\n            default=Value(\"high\"),\n            output_field=CharField(),\n        )\n    )\n    self.assertCountEqual(\n        books.values_list(\"name\", \"rating_label\"),\n        [\n            (\"The Definitive Guide to Django: Web Development Done Right\", \"high\"),\n            (\"Sams Teach Yourself Django in 24 Hours\", \"medium\"),\n            (\"Practical Django Projects\", \"medium\"),\n            (\"Python Web Development with Django\", \"medium\"),\n            (\"Artificial Intelligence: A Modern Approach\", \"medium\"),\n            (\"Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp\", \"high\"),\n        ],\n    )\n", "def test_window_frame_annotation(self):\n    Store.objects.bulk_create(\n        [\n            Store(\n                name=\"Store1\",\n                original_opening=datetime.datetime(2022, 1, 1, 9, 0, 0),\n                friday_night_closing=datetime.time(22, 0),\n                sales=100,\n            ),\n            Store(\n                name=\"Store2\",\n                original_opening=datetime.datetime(2022, 1, 1, 10, 0, 0),\n                friday_night_closing=datetime.time(22, 0),\n                sales=150,\n            ),\n            Store(\n                name=\"Store3\",\n                original_opening=datetime.datetime(2022, 1, 1, 11, 0, 0),\n                friday_night_closing=datetime.time(22, 0),\n                sales=200,\n            ),\n        ]\n    )\n    stores = (\n        Store.objects.annotate(\n            total_sales=Window(\n                expression=Sum(\"sales\"),\n                partition_by=F(\"friday_night_closing\"),\n                order_by=F(\"original_opening\").asc(),\n                frame=RowRange(start=None, end=0),\n            )\n        )\n        .order_by(\"original_opening\")\n    )\n    self.assertEqual(stores[0].total_sales, 100)\n    self.assertEqual(stores[1].total_sales, 250)\n    self.assertEqual(stores[2].total_sales, 450)\n", "def test_combined_expression_with_function(self):\n    book = Book.objects.annotate(\n        total=ExpressionWrapper(\n            Func(F(\"price\"), function=\"ROUND\", arg_joiner=\",\"), output_field=FloatField()\n        )\n    ).first()\n    self.assertAlmostEqual(book.total, round(book.price, 0))\n", "def test_coalesce_annotation(self):\n    # Test that Coalesce can handle None values\n    books = Book.objects.annotate(\n        is_book=Value(True, output_field=BooleanField()),\n        is_pony=Value(False, output_field=BooleanField()),\n        is_none=Value(None, output_field=BooleanField(null=True)),\n    )\n    self.assertGreater(len(books), 0)\n    for book in books:\n        self.assertIs(book.is_book, True)\n        self.assertIs(book.is_pony, False)\n        self.assertIsNone(book.is_none)\n\n    # Test that Coalesce can handle F() values\n    books = Book.objects.annotate(\n        is_book=Value(True, output_field=BooleanField()),\n        is_pony=Value(False, output_field=BooleanField()),\n        is_none=F(\"is_none\"),\n    )\n    self.assertGreater(len(books), 0)\n    for book in books:\n        self.assertIs(book.is_book, True)\n        self.assertIs(book.is_pony, False)\n        self.assertIsNone(book.is_none)\n\n    # Test that Coalesce can handle multiple F() values\n    books = Book.objects.annotate(\n        is_book=Value(True, output_field=BooleanField()),\n        is_pony=Value(False, output_field=BooleanField()),\n        is_none=F(\"is_none\"),\n        is_maybe=F(\"is_book\"),\n    )\n    self.assertGreater(len(books), 0)\n    for book in books:\n        self.assertIs(book.is_book, True)\n        self.assertIs(book.is_pony, False)\n        self.assertIsNone(book.is_none)\n        self.assertIs(book.is_maybe, True)\n\n    # Test that Coalesce can handle multiple F() values with different types\n    books = Book.objects.annotate(\n        is_book=Value(True, output_field=BooleanField()),\n        is_pony=Value(False, output_field=BooleanField()),\n        is_none=F(\"is_none\"),\n        is_maybe=F(\"rating\"),\n    )\n    self.assertGreater(len(books), 0)\n    for book in books:\n        self.assertIs(book.is_book, True)\n        self.assertIs(book.is_pony, False)\n        self.assertIsNone(book.is_none)\n        self.assertIsInstance(book.is_maybe, float)\n\n    # Test that Coalesce can handle Q() values\n    books = Book.objects.annotate(\n        is_book=Value(True, output_field=BooleanField()),\n        is_pony=Value(False, output_field=BooleanField()),\n        is_none=F(\"is_none\"),\n        is"], "sample_218": ["def test_trunc_duration(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n    end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n    if settings.USE_TZ:\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n    self.create_model(end_datetime, start_datetime)\n\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated=Trunc('duration', kind, output_field=DurationField())\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(end_datetime - start_datetime, kind)),\n                (end_datetime, truncate_to(start_datetime - end_datetime, kind))\n            ],\n            lambda m: (m.start_datetime, m.truncated)\n        )\n\n    test_duration_kind('second')\n    test_duration_kind('minute')\n    test_duration_kind('hour')\n    test_duration_kind('day')\n\n    with self.assertRaisesMessage(ValueError, 'Trunc only valid on DateField, TimeField, or DateTimeField.'):\n        list(DTModel.objects.annotate(truncated=Trunc('duration', 'second')))\n\n    with self.assertRaisesMessage(ValueError, 'Trunc only valid on DateField, TimeField, or DateTimeField.'):\n        list(DTModel.objects.annotate(truncated=Trunc('duration', 'second', output_field=DurationField())))\n", "def test_trunc_func_with_timezone_and_expr(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n    end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n    if settings.USE_TZ:\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n    self.create_model(end_datetime, start_datetime)\n\n    melb = pytz.timezone('Australia/Melbourne')\n\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated=Trunc(F('start_datetime'), kind, output_field=DateTimeField(), tzinfo=melb)\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime.astimezone(melb), kind, melb)),\n                (end_datetime, truncate_to(end_datetime.astimezone(melb), kind, melb))\n            ],\n            lambda m: (m.start_datetime, m.truncated)\n        )\n\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated=Trunc(F('start_date'), kind, output_field=DateField(), tzinfo=melb)\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime.date(), kind)),\n                (end_datetime, truncate_to(end_datetime.date(), kind))\n            ],\n            lambda m: (m.start_datetime, m.truncated)\n        )\n\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated=Trunc(F('start_time'), kind, output_field=TimeField(), tzinfo=melb)\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime.time(), kind)),\n                (end_datetime, truncate_to(end_datetime.time(), kind))\n            ],\n            lambda m: (m.start_datetime, m.truncated)\n        )\n\n    test_date_kind('year')\n    test_date_kind('quarter')\n    test_date_kind('month')\n    test_date_kind('week')\n    test_date_kind('day')\n    test_time_kind('hour')\n    test_time_kind('minute')\n    test_time_kind('second')\n    test_datetime_kind('year')\n    test_datetime", "def test_trunc_timezone_with_aware_input_datetime(self):\n    start_datetime = datetime(2016, 1, 1, 1, 30, 50, 321, tzinfo=timezone.utc)\n    end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123, tzinfo=timezone.utc)\n    self.create_model(start_datetime, end_datetime)\n\n    melb = pytz.timezone('Australia/Melbourne')\n\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated=Trunc('start_datetime', kind, output_field=DateTimeField(), tzinfo=melb)\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime.astimezone(melb), kind, melb)),\n                (end_datetime, truncate_to(end_datetime.astimezone(melb), kind, melb))\n            ],\n            lambda m: (m.start_datetime, m.truncated)\n        )\n\n    test_datetime_kind('year')\n    test_datetime_kind('quarter')\n    test_datetime_kind('month')\n    test_datetime_kind('week')\n    test_datetime_kind('day')\n    test_datetime_kind('hour')\n    test_datetime_kind('minute')\n    test_datetime_kind('second')\n", "def test_trunc_timezone_mismatch(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n    end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n    if settings.USE_TZ:\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n    self.create_model(end_datetime, start_datetime)\n\n    with timezone.override('Australia/Sydney'):\n        with self.assertRaisesMessage(ValueError, \"Database returned an invalid datetime value. Are time zone definitions for your database installed?\"):\n            DTModel.objects.annotate(truncated=Trunc('start_datetime', 'day', tzinfo=pytz.timezone('America/New_York'))).get()\n", "def test_now(self):\n    tzinfo = timezone.get_current_timezone()\n    now = timezone.now()\n    now_date = now.date()\n    now_time = now.time()\n    expected = {\n        'now_datetime': now,\n        'now_date': now_date,\n        'now_time': now_time,\n    }\n    if settings.USE_TZ:\n        expected['now_datetime'] = now.astimezone(tzinfo)\n        expected['now_date'] = now.astimezone(tzinfo).date()\n        expected['now_time'] = now.astimezone(tzinfo).time()\n    self.assertQuerysetEqual(\n        DTModel.objects.annotate(\n            now_datetime=Now(),\n            now_date=TruncDate(Now()),\n            now_time=TruncTime(Now()),\n        ).values('now_datetime', 'now_date', 'now_time'),\n        [expected],\n        lambda x: x,\n    )\n"], "sample_1135": ["def test_Mul_hermitian_antihermitian():\n    a = Symbol('a', hermitian=True)\n    b = Symbol('b', antihermitian=True)\n    c = Symbol('c', hermitian=False)\n    d = Symbol('d', antihermitian=False)\n    assert (a * a).is_hermitian is True\n    assert (a * b).is_hermitian is False\n    assert (a * c).is_hermitian is None\n    assert (a * d).is_hermitian is None\n    assert (b * b).is_antihermitian is True\n    assert (b * a).is_antihermitian is False\n    assert (b * c).is_antihermitian is None\n    assert (b * d).is_antihermitian is None\n    assert (c * c).is_hermitian is None\n    assert (c * d).is_hermitian is None\n    assert (d * d).is_hermitian is None\n", "def test_Mul_is_integer():\n    i1 = Symbol('i1', integer=True, zero=False)\n    i2 = Symbol('i2', integer=True, zero=False)\n    nr = Symbol('nr', rational=False)\n    x = Symbol('x')\n    assert Mul(i1, i2, evaluate=False).is_integer is True\n    assert Mul(i1, nr, evaluate=False).is_integer is False\n    assert Mul(i1, x, evaluate=False).is_integer is None\n    assert Mul(nr, x, evaluate=False).is_integer is False\n    assert Mul(x, x, evaluate=False).is_integer is None\n", "def test_mul_as_coeff_Mul():\n    expr1 = 2 + 3*x\n    expr2 = x + 1\n    result = (expr1 * expr2).as_coeff_Mul()\n    assert result == (2, (x + 1)*(3*x + 2))\n", "def test_Mul_with_repeated_noncommutative_args():\n    from sympy import sqrt\n    x, y = symbols('x y', commutative=False)\n    assert Mul(x, y, x, y, evaluate=False) == Mul(x, x, y, y, evaluate=False)\n    assert Mul(x, sqrt(y), sqrt(y), x, evaluate=False) == Mul(x, sqrt(y**2), x, evaluate=False)\n", "def test_Mul_matches():\n    x, y = symbols('x y')\n    a, b = symbols('a b')\n\n    assert Mul(a, b).matches(Mul(a, b)) == {}\n    assert Mul(a, b).matches(Mul(b, a)) is None\n    assert Mul(x, y).matches(Mul(x, y)) == {}\n\n    assert Mul(a, x).matches(Mul(a, y)) is None\n    assert Mul(a, b).matches(Mul(a, x)) is None\n\n    assert Mul(a, b).matches(Mul(x, y)) is None\n    assert Mul(a, x).matches(Mul(b, y)) is None\n\n    assert Mul(a, b).matches(Mul(a, x, y)) is None\n    assert Mul(a, b, x).matches(Mul(a, x, y)) is None\n"], "sample_352": ["        def as_sql(self, compiler, connection):\n            return 'dummy', []\n", "    def test_ticket_25780(self):\n        q1 = Author.objects.filter(name='a1')\n        q2 = Author.objects.filter(name='a2')\n        q3 = q1 | q2\n        self.assertEqual(str(q3.query).count('JOIN'), 0)\n        q4 = q1 | q2\n        self.assertEqual(str(q4.query).count('JOIN'), 0)\n", "    def test_split_having(self):\n        node = WhereNode(\n            children=[NothingNode(), WhereNode(children=[NothingNode()])],\n            connector=AND,\n            negated=False,\n        )\n        where_node, having_node = node.split_having()\n        self.assertIsNone(where_node)\n        self.assertIsNone(having_node)\n\n        node = WhereNode(\n            children=[NothingNode(), WhereNode(children=[WhereNode(children=[NothingNode()])])],\n            connector=OR,\n            negated=False,\n        )\n        where_node, having_node = node.split_having()\n        self.assertIsNone(where_node)\n        self.assertIsNotNone(having_node)\n\n        node = WhereNode(\n            children=[NothingNode(), WhereNode(children=[WhereNode(children=[NothingNode()])])],\n            connector=AND,\n            negated=True,\n        )\n        where_node, having_node = node.split_having()\n        self.assertIsNotNone(where_node)\n        self.assertIsNone(having_node)\n\n        node = WhereNode(\n            children=[\n                WhereNode(children=[NothingNode()]),\n                WhereNode(children=[WhereNode(children=[NothingNode()])]),\n            ],\n            connector=OR,\n            negated=False,\n        )\n        where_node, having_node = node.split_having()\n        self.assertIsNotNone(where_node)\n        self.assertIsNone(having_node)\n", "    def test_split_having_empty_query(self):\n        node = WhereNode(children=[], connector=AND, negated=False)\n        where_part, having_part = node.split_having()\n        self.assertIsNone(where_part)\n        self.assertIsNone(having_part)\n", "    def test_and_conjunction(self):\n        # Test that AND conjunctions work correctly\n        w = WhereNode(children=[WhereNode(children=[WhereNode()] * 2, connector='AND', negated=False),\n                         WhereNode(children=[WhereNode()] * 2, connector='AND', negated=False)],\n                      connector='AND', negated=False)\n        self.assertEqual(w.as_sql(None, None), ('', []))\n"], "sample_291": ["    def test_get_context_data(self):\n        view = ContextMixin()\n        context = view.get_context_data(test_key='test_value')\n        self.assertIn('test_key', context)\n        self.assertEqual(context['test_key'], 'test_value')\n        self.assertIn('view', context)\n        self.assertEqual(context['view'], view)\n", "    def test_extra_context(self):\n        mixin = ContextMixin()\n        mixin.extra_context = {'foo': 'bar'}\n        context = mixin.get_context_data()\n        self.assertEqual(context['foo'], 'bar')\n        self.assertEqual(context['view'], mixin)\n", "    def test_extra_context(self):\n        class TestView(ContextMixin, View):\n            extra_context = {'foo': 'bar'}\n\n                return HttpResponse('')\n\n        view = TestView()\n        context = view.get_context_data()\n        self.assertIn('foo', context)\n        self.assertEqual(context['foo'], 'bar')\n        self.assertIn('view', context)\n        self.assertEqual(context['view'], view)\n", "    def test_template_view_context_mixin(self):\n        \"\"\"\n        Test that TemplateView uses ContextMixin.\n        \"\"\"\n        class TestTemplateView(TemplateView):\n            extra_context = {'key': 'value'}\n\n                return self.render_to_response({})\n\n        response = TestTemplateView.as_view()(self.rf.get('/'))\n        self.assertEqual(response.context['key'], 'value')\n        self.assertIsInstance(response.context['view'], View)\n", "    def test_render_to_response_with_context(self):\n        class TestView(TemplateView):\n            template_name = 'template.html'\n\n        view = TestView.as_view()\n        response = view(self.rf.get('/'))\n        response.render()\n        self.assertEqual(response.status_code, 200)\n        self.assertIn('view', response.context)\n"], "sample_1197": ["def test_unit_system():\n    us = UnitSystem((meter, kilogram, second), name=\"SI\", descr=\"International System of Units\")\n    assert us.name == \"SI\"\n    assert us.descr == \"International System of Units\"\n    assert len(us._base_units) == 3\n    assert us.dim == 3\n    assert us.is_consistent\n\n    new_us = us.extend((coulomb,), name=\"SIc\", description=\"SI with charge unit\")\n    assert new_us.name == \"SIc\"\n    assert new_us.descr == \"SI with charge unit\"\n    assert len(new_us._base_units) == 4\n    assert new_us.dim == 4\n    assert new_us.is_consistent\n", "def test_extend_unit_system():\n    us = UnitSystem((kilogram, meter, second), name=\"SI\")\n    us2 = us.extend((kelvin,), name=\"SI extended\")\n\n    assert us2.dim == 4\n    assert us2._base_units == (kilogram, meter, second, kelvin)\n    assert us2._units == (kilogram, meter, second, kelvin)\n\n    us3 = us2.extend((), name=\"SI extended again\")\n    assert us3._base_units == (kilogram, meter, second, kelvin)\n    assert us3._units == (kilogram, meter, second, kelvin)\n\n    us4 = us2.extend((kelvin, kelvin), name=\"SI extended again\")\n    assert us4._base_units == (kilogram, meter, second, kelvin, kelvin)\n    assert us4._units == (kilogram, meter, second, kelvin, kelvin)\n\n    assert us.is_consistent\n    assert us2.is_consistent\n    assert us3.is_consistent\n    assert us4.is_consistent\n", "def test_unit_system_creation():\n    units = (meter, gram, second, joule)\n    dimension_system = Dimension(length=1, mass=1, time=1, energy=1)\n    derived_units = {energy: joule}\n    \n    unit_system = UnitSystem(units, name=\"Custom System\", dimension_system=dimension_system, derived_units=derived_units)\n    \n    assert unit_system.name == \"Custom System\"\n    assert unit_system._base_units == units\n    assert unit_system._dimension_system == dimension_system\n    assert unit_system._derived_units == derived_units\n", "def test_unit_system_extension():\n    # Test extending the SI system with new units\n    new_system = SI.extend((Quantity(\"new_base\"),), name=\"New SI\")\n    assert new_system.name == \"New SI\"\n    assert len(new_system._base_units) == len(SI._base_units) + 1\n    assert new_system.is_consistent\n\n    # Test extending the new system with another unit\n    another_system = new_system.extend((Quantity(\"another_base\"),), name=\"Another SI\")\n    assert another_system.name == \"Another SI\"\n    assert len(another_system._base_units) == len(new_system._base_units) + 1\n    assert another_system.is_consistent\n\n    # Test extending with non-prefixed units\n    non_prefixed_system = another_system.extend((Quantity(\"non_prefixed\"),), name=\"Non Prefixed SI\")\n    assert non_prefixed_system.name == \"Non Prefixed SI\"\n    assert len(non_prefixed_system._base_units) == len(another_system._base_units) + 1\n    assert non_prefixed_system.is_consistent\n    assert not non_prefixed_system._units[-1].is_prefixed\n", "def test_unit_system():\n    unit_system = UnitSystem.get_unit_system(\"SI\")\n    assert unit_system.name == \"SI\"\n    assert unit_system.get_dimension_system() is not None\n\n    assert unit_system.dim == 7\n    assert unit_system.is_consistent\n\n    assert len(unit_system.get_units_non_prefixed()) > 0\n    assert len(unit_system.derived_units) > 0\n\n    assert str(unit_system) == \"SI\"\n\n    assert repr(unit_system).startswith('<UnitSystem: ')\n\n    new_unit_system = unit_system.extend(base=(Quantity(\"new_unit\"),), units=(Quantity(\"another_unit\"),))\n    assert new_unit_system.name == \"\"\n    assert len(new_unit_system._base_units) == 8\n    assert len(new_unit_system._units) > len(unit_system._units)\n\n    with raises(ValueError):\n        UnitSystem.get_unit_system(\"Unknown Unit System\")\n\n    assert UnitSystem.get_default_unit_system() is unit_system\n"], "sample_54": ["def test_file_from_disk_response_with_filename(self):\n    with tempfile.NamedTemporaryFile(suffix='.txt') as tmp:\n        response = FileResponse(tmp, filename='custom_filename.txt')\n    self.assertEqual(response['Content-Disposition'], 'inline; filename=\"custom_filename.txt\"')\n", "def test_set_signed_cookie(self):\n    response = HttpResponse()\n    response.set_signed_cookie('my_cookie', 'my_value', salt='my_salt')\n    self.assertIn('Set-Cookie', response.serialize_headers().decode('utf-8'))\n    self.assertEqual(response.cookies['my_cookie']['httponly'], True)\n", "    def test_file_from_disk_with_filename(self):\n        response = FileResponse(open(__file__, 'rb'), filename='custom_filename.py')\n        self.assertEqual(response['Content-Length'], str(os.path.getsize(__file__)))\n        self.assertIn(response['Content-Type'], ['text/x-python', 'text/plain'])\n        self.assertEqual(response['Content-Disposition'], 'inline; filename=\"custom_filename.py\"')\n        response.close()\n", "def test_file_from_disk_with_explicit_filename(self):\n    response = FileResponse(open(__file__, 'rb'), filename='explicit_filename.txt')\n    self.assertEqual(response['Content-Length'], str(os.path.getsize(__file__)))\n    self.assertIn(response['Content-Type'], ['text/x-python', 'text/plain'])\n    self.assertEqual(response['Content-Disposition'], 'inline; filename=\"explicit_filename.txt\"')\n    response.close()\n", "def test_file_response_custom_filename(self):\n    with tempfile.NamedTemporaryFile(suffix='.txt') as tmp:\n        tmp.write(b'Hello World')\n        tmp.flush()\n        response = FileResponse(tmp, filename='custom_filename.txt')\n    self.assertEqual(response['Content-Disposition'], 'inline; filename=\"custom_filename.txt\"')\n"], "sample_966": ["def test_python_domain_clear_doc(app):\n    text = (\".. py:function:: f1()\\n\"\n            \".. py:function:: f2()\\n\")\n    doctree = restructuredtext.parse(app, text)\n    domain = app.env.get_domain('py')\n    assert 'f1' in domain.objects\n    assert 'f2' in domain.objects\n\n    domain.clear_doc('index')\n    assert 'f1' not in domain.objects\n    assert 'f2' not in domain.objects\n", "def test_pyclass_nodocstring(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:method:: method\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, (\"class\", desc_sig_space)],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'method() (Class method)', 'Class.method', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_annotation, (\"method\", desc_sig_space)],\n                                                     [desc_name, \"method\"],\n                                                     [desc_parameterlist, ()])],\n                                   [desc_content, ()]))\n    assert 'Class.method' in app.env.get_domain('py').objects\n    assert app.env.get_domain('py').objects['Class.method'] == ('index', 'Class.method', 'method', False)\n", "def test_python_python_use_unqualified_type_names_with_literal(app, status, warning):\n    text = (\".. py:class:: Foo\\n\"\n            \"\\n\"\n            \"   :var Literal['a', 'b', 'c'] attr: blah blah\\n\")\n    app.env.config.python_use_unqualified_type_names = True\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree,\n                (addnodes.index,\n                 [desc, ([desc_signature, ([desc_annotation, (\"class\", desc_sig_space)],\n                                           [desc_name, \"Foo\"])],\n                         [desc_content, nodes.field_list, nodes.field])]))\n    assert_node(doctree[1][1][0][0], ([nodes.field_name, \"Variables\"],\n                                      [nodes.field_body, nodes.paragraph]))\n\n    # :var int attr:\n    assert_node(doctree[1][1][0][0][1][0],\n                ([addnodes.literal_strong, \"attr\"],\n                 \" (\",\n                 [pending_xref, addnodes.literal_emphasis, \"Literal\"],\n                 [addnodes.literal_emphasis, \"[\"],\n                 [addnodes.literal_emphasis, \"'a'\"],\n                 [addnodes.literal_emphasis, \", \"],\n                 [addnodes.literal_emphasis, \"'b'\"],\n                 [addnodes.literal_emphasis, \", \"],\n                 [addnodes.literal_emphasis, \"'c'\"],\n                 [addnodes.literal_emphasis, \"]\"],\n                 \")\",\n                 \" -- \",\n                 \"blah blah\"))\n    assert_node(doctree[1][1][0][0][1][0][2], pending_xref,\n                refdomain=\"py\", reftype=\"class\", reftarget=\"Literal\", **{\"py:class\": \"Foo\"})\n", "def test_parse_annotation_mixed_type(app):\n    doctree = _parse_annotation(\"List[Union[int, str]]\", app.env)\n    assert_node(doctree, ([pending_xref, \"List\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"Union\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \",\"],\n                          desc_sig_space,\n                          [pending_xref, \"str\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"]\"]))\n\n    doctree = _parse_annotation(\"Dict[str, Union[int, str]]\", app.env)\n    assert_node(doctree, ([pending_xref, \"Dict\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"str\"],\n                          [desc_sig_punctuation, \",\"],\n                          desc_sig_space,\n                          [pending_xref, \"Union\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \",\"],\n                          desc_sig_space,\n                          [pending_xref, \"str\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"]\"]))\n\n    doctree = _parse_annotation(\"Tuple[Union[int, str], ...]\", app.env)\n    assert_node(doctree, ([pending_xref, \"Tuple\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"Union\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \",\"],\n                          desc_sig_space,\n                          [pending_xref, \"str\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \",\"],\n                          desc_sig_space,\n                          [desc_sig_punctuation, \"...\"],\n                          [desc_sig_punctuation, \"]\"]))\n\n    doctree = _parse_annotation(\"Union[int, str, Tuple[...]]\", app.env)\n    assert_node(doctree, ([pending_xref, \"Union\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \",\"],\n                          desc_sig_space,\n                          [pending_xref, \"str\"],\n                          [desc_sig_punctuation, \",\"],\n                          desc_sig_space,\n                          [pending_xref, \"Tuple\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [desc", "def test_python_domain_xrefs_with_spaces(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<a class=\"reference internal\" href=\"#module_a.submodule.ModTopLevel\" title=\"module_a.submodule.ModTopLevel\">'\n            '<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">module_a.submodule.ModTopLevel</span></code></a>'\n            in content)\n    assert ('<a class=\"reference internal\" href=\"#module_a.submodule.ModTopLevel.mod_child_1\" title=\"module_a.submodule.ModTopLevel.mod_child_1\">'\n            '<code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">module_a.submodule.ModTopLevel.mod_child_1</span></code></a>'\n            in content)\n    assert ('<a class=\"reference internal\" href=\"#module_a.submodule.ModTopLevel.mod_child_1\" title=\"module_a.submodule.ModTopLevel.mod_child_1\">'\n            '<code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">module_a.submodule.ModTopLevel.mod_child_1</span></code></a>'\n            in content)\n    assert ('<a class=\"reference internal\" href=\"#module_a.submodule.ModTopLevel.mod_child_1\" title=\"module_a.submodule.ModTopLevel.mod_child_1\">'\n            '<code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">module_a.submodule.ModTopLevel.mod_child_1</span></code></a>'\n            in content)\n    assert ('<a class=\"reference internal\" href=\"#module_a.submodule.ModTopLevel.mod_child_2\" title=\"module_a.submodule.ModTopLevel.mod_child_2\">'\n            '<code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">module_a.submodule.ModTopLevel.mod_child_2</span></code></a>'\n            in content)\n    assert ('<a class=\"reference internal\" href=\"#module_a.submodule.ModTopLevel.mod_child_2\" title=\"module_a.submodule.ModTopLevel.mod_child_2\">'\n            '<code class=\"xref py py-meth docutils literal notranslate\"><span class"], "sample_760": ["def test_make_scorer_custom_function():\n    # Test that make_scorer works with a custom scoring function.\n        return np.mean(y_true == y_pred)\n\n    scorer = make_scorer(custom_scorer)\n\n    X, y = make_classification(random_state=0)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n    clf = LogisticRegression(random_state=0)\n    clf.fit(X_train, y_train)\n\n    score1 = scorer(clf, X_test, y_test)\n    score2 = custom_scorer(y_test, clf.predict(X_test))\n    assert_almost_equal(score1, score2)\n", "def test_make_scorer_with_callable():\n    # Test that make_scorer works with callable objects\n        return np.mean(y_true == y_pred)\n\n    scorer = make_scorer(scorer_func)\n\n    X, y = make_classification(random_state=0)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n    clf = DecisionTreeClassifier(random_state=0)\n    clf.fit(X_train, y_train)\n\n    score1 = scorer(clf, X_test, y_test)\n    score2 = scorer_func(y_test, clf.predict(X_test))\n    assert_almost_equal(score1, score2)\n", "def test_make_scorer_for_multilabel_scorers():\n    # Test that make_scorer correctly handles multilabel scorers\n    X, y = make_multilabel_classification(n_samples=30, random_state=0)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n    for scorer_name in MULTILABEL_ONLY_SCORERS:\n        scorer = make_scorer(getattr(cluster_module, scorer_name))\n        estimator = OneVsRestClassifier(DecisionTreeClassifier())\n        estimator.fit(X_train, y_train)\n\n        score1 = scorer(estimator, X_test, y_test)\n        score2 = getattr(cluster_module, scorer_name)(y_test,\n                                                      estimator.predict(X_test))\n        assert_almost_equal(score1, score2)\n", "def test_multimetric_scorers_memmap_input():\n    # Test that multimetric scorers return scalars when computed on memmap data\n    scoring = ['precision', 'recall', 'f1']\n    estimator = ESTIMATORS['f1']\n    X, y, y_ml = X_mm, y_mm, y_ml_mm\n    scorers, is_multi = _check_multimetric_scoring(estimator, scoring=scoring)\n    assert is_multi\n    for scorer_name, scorer in scorers.items():\n        score = scorer(estimator, X, y)\n        assert isinstance(score, numbers.Number), scorer_name\n", "def test_make_scorer_on_invalid_inputs():\n    # Test make_scorer on invalid inputs\n    with pytest.raises(ValueError, match=\"Set either needs_proba or needs_threshold to True, but not both\"):\n        make_scorer(f1_score, needs_proba=True, needs_threshold=True)\n\n    with pytest.raises(ValueError, match=\"needs_proba or needs_threshold must be True when scoring is a loss function\"):\n        make_scorer(f1_score, greater_is_better=False, needs_proba=False, needs_threshold=False)\n\n    with pytest.raises(TypeError, match=\"score_func must be a callable\"):\n        make_scorer(\"not a function\", greater_is_better=True)\n\n    with pytest.raises(TypeError, match=\"needs_proba must be a boolean\"):\n        make_scorer(f1_score, needs_proba=\"not a boolean\")\n\n    with pytest.raises(TypeError, match=\"needs_threshold must be a boolean\"):\n        make_scorer(f1_score, needs_threshold=\"not a boolean\")\n\n    with pytest.raises(TypeError, match=\"greater_is_better must be a boolean\"):\n        make_scorer(f1_score, greater_is_better=\"not a boolean\")\n"], "sample_25": ["def test_record_valued_keyword_cards_case_sensitivity(self):\n    \"\"\"\n    Regression test for https://github.com/astropy/astropy/issues/4964\n\n    Ensures that record-valued keyword cards are correctly handled when\n    accessed by keyword/field-specifier in a case-insensitive manner.\n    \"\"\"\n\n    header = fits.Header([(\"DP1\", \"NAXIS: 2\", \"Comment\")])\n    assert header[\"DP1.NAXIS\"] == 2.0\n    assert header[\"dp1.naxis\"] == 2.0\n    assert header[\"Dp1.NaXiS\"] == 2.0\n\n    header[\"dp1.naxis\"] = 3.0\n    assert header[\"DP1.NAXIS\"] == 3.0\n    assert header[\"dp1.naxis\"] == 3.0\n    assert header[\"Dp1.NaXiS\"] == 3.0\n\n    del header[\"dp1.naxis\"]\n    assert \"DP1.NAXIS\" not in header\n    assert \"dp1.naxis\" not in header\n    assert \"Dp1.NaXiS\" not in header\n", "def test_rvkc_in_header_slice(self):\n    \"\"\"\n    A slice of a header that includes RVKCs should include the RVKCs.\n    \"\"\"\n\n    h = fits.Header()\n    h[\"FOO\"] = \"BAR\"\n    h[\"DP1.NAXIS\"] = 2\n    h[\"DP1.AXIS.1\"] = 1\n    h[\"DP1.AXIS.2\"] = 2\n    h[\"BAZ\"] = \"QUX\"\n    h[\"DP1.NAUX\"] = 2\n    h[\"DP1.AUX.1.COEFF.0\"] = 0\n    h[\"DP1.AUX.1.POWER.0\"] = 1\n    h[\"DP1.AUX.1.COEFF.1\"] = 0.00048828125\n    h[\"DP1.AUX.1.POWER.1\"] = 1\n\n    hslice = h[2:8]\n    assert len(hslice) == 6\n    assert \"DP1.NAXIS\" in hslice\n    assert \"DP1.AXIS.1\" in hslice\n    assert \"DP1.AXIS.2\" in hslice\n    assert \"DP1.NAUX\" in hslice\n    assert \"DP1.AUX.1.COEFF.0\" in hslice\n    assert \"DP1.AUX.1.POWER.0\" in hslice\n    assert \"DP1.AUX.1.COEFF.1\" in hslice\n    assert \"DP1.AUX.1.POWER.1\" in hslice\n    assert len(hslice.cards) == 6\n", "def test_record_valued_keyword_cards_in_subheaders(self):\n    \"\"\"\n    Regression test for https://github.com/astropy/astropy/issues/6613\n\n    Ensure that record-valued keyword cards are properly handled in subheaders\n    of headers.\n    \"\"\"\n\n    hdu = fits.ImageHDU()\n    hdu.header[\"FOO\"] = 1\n    hdu.header[\"BAR\"] = 2\n    hdu.header[\"DP1\"] = \"NAXIS: 2\"\n    hdu.header[\"DP1\"] = \"AXIS.1: 1\"\n    hdu.header[\"DP1\"] = \"AXIS.2: 2\"\n    hdu.header[\"DP1\"] = \"NAUX: 2\"\n    hdu.header[\"DP1\"] = \"AUX.1.COEFF.0: 0\"\n    hdu.header[\"DP1\"] = \"AUX.1.POWER.0: 1\"\n    hdu.header[\"DP1\"] = \"AUX.1.COEFF.1: 0.00048828125\"\n    hdu.header[\"DP1\"] = \"AUX.1.POWER.1: 1\"\n\n    subheader = hdu.header[\"DP1...\"]\n    assert len(subheader) == 8\n    assert subheader[0] == 2.0\n    assert subheader[1] == 1.0\n    assert subheader[2] == 2.0\n    assert subheader[3] == 2.0\n    assert subheader[4] == 0.0\n    assert subheader[5] == 1.0\n    assert subheader[6] == 0.00048828125\n    assert subheader[7] == 1.0\n", "def test_hierarch_long_value(self):\n    \"\"\"Regression test for https://github.com/astropy/astropy/issues/11305\"\"\"\n\n    h = fits.Header()\n    value = \"A\" * 67\n    h[\"HIERARCH LONG KEY\"] = value\n    assert str(h.cards[\"HIERARCH LONG KEY\"]) == f\"HIERARCH LONG KEY= '{value}'\"\n    assert h[\"HIERARCH LONG KEY\"] == value\n\n    value = \"A\" * 68\n    h[\"HIERARCH LONG KEY\"] = value\n    assert str(h.cards[\"HIERARCH LONG KEY\"]) == f\"HIERARCH LONG KEY= 'A'  \" f\"CONTINUE  '{'A' * 67}'\"\n    assert h[\"HIERARCH LONG KEY\"] == value\n", "def test_rvkc_update_by_keyword(self):\n    \"\"\"\n    Regression test for https://github.com/astropy/astropy/issues/11174\n\n    Test updating a record-valued keyword card by keyword only, and ensure\n    that the existing value is replaced and that new field-specifiers are not\n    created.\n    \"\"\"\n\n    h = fits.Header()\n    h.set(\"DP1\", \"AXIS.1: 2\")\n    h.set(\"DP1\", \"AXIS.2: 4\")\n    h[\"DP1\"] = \"AXIS.1: 3\"\n    assert h[\"DP1.AXIS.1\"] == 3.0\n    assert h[\"DP1.AXIS.2\"] == 4.0\n    assert len(h) == 2\n"], "sample_401": ["def test_formset_can_order_custom_template_name(self):\n    class CustomFormSet(BaseFormSet):\n        template_name = \"custom_template.html\"\n\n    class OrderingFormSet(CustomFormSet):\n            return HiddenInput(attrs={\"class\": \"ordering\"})\n\n    ChoiceFormSet = formset_factory(Choice, formset=OrderingFormSet, can_order=True)\n    formset = ChoiceFormSet(auto_id=False)\n    self.assertEqual(formset.template_name, \"custom_template.html\")\n", "def test_formset_absolute_max_with_can_delete(self):\n    # Test that absolute_max works with can_delete\n    data = {\n        \"form-TOTAL_FORMS\": \"2001\",\n        \"form-INITIAL_FORMS\": \"0\",\n        \"form-MAX_NUM_FORMS\": \"0\",\n        \"form-0-DELETE\": \"on\",  # delete this form\n    }\n    AbsoluteMaxFavoriteDrinksFormSet = formset_factory(\n        FavoriteDrinkForm, absolute_max=3000, can_delete=True\n    )\n    formset = AbsoluteMaxFavoriteDrinksFormSet(data=data)\n    self.assertIs(formset.is_valid(), True)\n    self.assertEqual(len(formset.forms), 2001)\n    self.assertEqual(len(formset.deleted_forms), 1)\n", "def test_management_form_data_validation(self):\n    # Test management form data validation\n    data = {\n        \"choices-TOTAL_FORMS\": \"abc\",\n        \"choices-INITIAL_FORMS\": \"0\",\n    }\n    ChoiceFormSet = formset_factory(Choice)\n    formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(\n        formset.non_form_errors(),\n        [\n            \"ManagementForm data is missing or has been tampered with. \"\n            \"Missing fields: form-TOTAL_FORMS, form-INITIAL_FORMS. \"\n            \"You may need to file a bug report if the issue persists.\",\n        ],\n    )\n    self.assertEqual(formset.errors, [])\n\n    data = {\n        \"choices-TOTAL_FORMS\": \"-1\",\n        \"choices-INITIAL_FORMS\": \"0\",\n    }\n    formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(\n        formset.non_form_errors(),\n        [\n            \"ManagementForm data is missing or has been tampered with. \"\n            \"Missing fields: choices-TOTAL_FORMS. You may need to file a bug report \"\n            \"if the issue persists.\",\n        ],\n    )\n    self.assertEqual(formset.errors, [])\n", "def test_add_fields_called_on_all_forms(self):\n    \"\"\"\n    The add_fields() method is called on all forms, including initial and extra.\n    \"\"\"\n    class AddFieldsFormSet(BaseFormSet):\n            form.fields[\"added\"] = CharField()\n\n    class TestForm(Form):\n        pass\n\n    AddFieldsFormSet = formset_factory(TestForm, formset=AddFieldsFormSet, extra=2)\n    formset = AddFieldsFormSet(initial=[{\"added\": \"test\"}])\n\n    # The field was added to all forms.\n    for form in formset:\n        self.assertIn(\"added\", form.fields)\n", "    def test_management_form_cleaned_data_total_forms(self):\n        \"\"\"The management form's cleaned_data for TOTAL_FORMS is always set to 0 if the formset is unbound.\"\"\"\n        formset = self.make_choiceformset()\n        self.assertEqual(formset.management_form.cleaned_data[TOTAL_FORM_COUNT], 0)\n        formset = self.make_choiceformset([(\"test\", 1)])\n        self.assertEqual(formset.management_form.cleaned_data[TOTAL_FORM_COUNT], 1)\n        data = {\n            \"choices-TOTAL_FORMS\": \"5\",\n            \"choices-INITIAL_FORMS\": \"0\",\n        }\n        formset = self.make_choiceformset(data)\n        self.assertEqual(formset.management_form.cleaned_data[TOTAL_FORM_COUNT], 5)\n        formset = self.make_choiceformset(formset_class=formset_factory(Choice, extra=10))\n        self.assertEqual(formset.management_form.cleaned_data[TOTAL_FORM_COUNT], 10)\n"], "sample_197": ["def test_reversed_with_depth(self):\n    \"\"\" Test the reversed parameter with depth. \"\"\"\n    t = self.t + self.oneyear + self.onemonth + self.oneweek + self.oneday + self.onehour\n    tests = [\n        (t, 1, '1\\xa0year'),\n        (t, 2, '1\\xa0year, 1\\xa0month'),\n        (t, 3, '1\\xa0year, 1\\xa0month, 1\\xa0week'),\n        (t, 4, '1\\xa0year, 1\\xa0month, 1\\xa0week, 1\\xa0day'),\n        (t, 5, '1\\xa0year, 1\\xa0month, 1\\xa0week, 1\\xa0day, 1\\xa0hour'),\n        (t, 6, '1\\xa0year, 1\\xa0month, 1\\xa0week, 1\\xa0day, 1\\xa0hour'),\n        (self.t + self.onehour, 5, '1\\xa0hour'),\n        (self.t + (4 * self.oneminute), 3, '4\\xa0minutes'),\n        (self.t + self.onehour + self.oneminute, 1, '1\\xa0hour'),\n        (self.t + self.oneday + self.onehour, 1, '1\\xa0day'),\n        (self.t + self.oneweek + self.oneday, 1, '1\\xa0week'),\n        (self.t + self.onemonth + self.oneweek, 1, '1\\xa0month'),\n        (self.t + self.oneyear + self.onemonth, 1, '1\\xa0year'),\n        (self.t + self.oneyear + self.oneweek + self.oneday, 3, '1\\xa0year'),\n    ]\n    for value, depth, expected in tests:\n        with self.subTest():\n            self.assertEqual(timesince(self.t, value, reversed=True, depth=depth), expected)\n", "def test_non_english_translations(self):\n    time_strings = {\n        'year': npgettext_lazy('naturaltime-future', '%d year', '%d years'),\n        'month': npgettext_lazy('naturaltime-future', '%d month', '%d months'),\n        'week': npgettext_lazy('naturaltime-future', '%d week', '%d weeks'),\n        'day': npgettext_lazy('naturaltime-future', '%d day', '%d days'),\n        'hour': npgettext_lazy('naturaltime-future', '%d hour', '%d hours'),\n        'minute': npgettext_lazy('naturaltime-future', '%d minute', '%d minutes'),\n    }\n    with translation.override('fr'):\n        for now in [self.t, self.t - self.onemicrosecond, self.t - self.oneday]:\n            with self.subTest(now):\n                self.assertEqual(\n                    timesince(self.t, now, time_strings=time_strings),\n                    '0\\xa0minutes',\n                )\n    with translation.override('de'):\n        for now in [self.t, self.t - self.onemicrosecond, self.t - self.oneday]:\n            with self.subTest(now):\n                self.assertEqual(\n                    timesince(self.t, now, time_strings=time_strings),\n                    '0\\xa0Minuten',\n                )\n", "def test_time_strings(self):\n    \"\"\" Test using custom time strings. \"\"\"\n    custom_time_strings = {\n        'year': npgettext_lazy('custom-time-strings', '%d year', '%d years'),\n        'month': npgettext_lazy('custom-time-strings', '%d month', '%d months'),\n        'week': npgettext_lazy('custom-time-strings', '%d week', '%d weeks'),\n        'day': npgettext_lazy('custom-time-strings', '%d day', '%d days'),\n        'hour': npgettext_lazy('custom-time-strings', '%d hour', '%d hours'),\n        'minute': npgettext_lazy('custom-time-strings', '%d minute', '%d minutes'),\n    }\n    with translation.override('en'):\n        self.assertEqual(timesince(self.t, self.t + self.oneday, time_strings=custom_time_strings), '1\\xa0day')\n        self.assertEqual(timesince(self.t, self.t + self.oneweek, time_strings=custom_time_strings), '1\\xa0week')\n        self.assertEqual(timesince(self.t, self.t + self.onemonth, time_strings=custom_time_strings), '1\\xa0month')\n        self.assertEqual(timesince(self.t, self.t + self.oneyear, time_strings=custom_time_strings), '1\\xa0year')\n", "def test_custom_time_strings(self):\n    \"\"\"\n    Test using custom time strings.\n    \"\"\"\n    custom_time_strings = {\n        'year': ngettext_lazy('%d year', '%d years'),\n        'month': ngettext_lazy('%d month', '%d months'),\n        'week': ngettext_lazy('%d week', '%d weeks'),\n        'day': ngettext_lazy('%d day', '%d days'),\n        'hour': ngettext_lazy('%d hour', '%d hours'),\n        'minute': ngettext_lazy('%d minute', '%d minutes'),\n    }\n    custom_time_strings['year'] = ngettext_lazy('%d anno', '%d anni')\n    self.assertEqual(timesince(self.t, self.t + self.oneyear, time_strings=custom_time_strings), '1\\xa0anno')\n    self.assertEqual(timesince(self.t, self.t + 2 * self.oneyear, time_strings=custom_time_strings), '2\\xa0anni')\n    self.assertEqual(timesince(self.t, self.t + self.onemonth, time_strings=custom_time_strings), '1\\xa0month')\n    self.assertEqual(timesince(self.t, self.t + 2 * self.onemonth, time_strings=custom_time_strings), '2\\xa0months')\n", "def test_future_and_past_with_non_default_depth(self):\n    \"\"\" Test future and past with non default depth \"\"\"\n    t = self.t\n    now = t + self.oneyear + self.onemonth + self.oneweek + self.oneday + self.onehour\n    future_depth_tests = [\n        (now, 1, '1\\xa0year'),\n        (now, 2, '1\\xa0year, 1\\xa0month'),\n        (now, 3, '1\\xa0year, 1\\xa0month, 1\\xa0week'),\n        (now, 4, '1\\xa0year, 1\\xa0month, 1\\xa0week, 1\\xa0day'),\n        (now, 5, '1\\xa0year, 1\\xa0month, 1\\xa0week, 1\\xa0day, 1\\xa0hour'),\n        (now, 6, '1\\xa0year, 1\\xa0month, 1\\xa0week, 1\\xa0day, 1\\xa0hour'),\n    ]\n    for value, depth, expected in future_depth_tests:\n        with self.subTest():\n            self.assertEqual(timesince(t, value, depth=depth), expected)\n    \n    past = t - self.oneyear - self.onemonth - self.oneweek - self.oneday - self.onehour\n    past_depth_tests = [\n        (past, 1, '0\\xa0minutes'),\n        (past, 2, '0\\xa0minutes'),\n        (past, 3, '0\\xa0minutes'),\n        (past, 4, '0\\xa0minutes'),\n        (past, 5, '0\\xa0minutes'),\n        (past, 6, '0\\xa0minutes'),\n    ]\n    for value, depth, expected in past_depth_tests:\n        with self.subTest():\n            self.assertEqual(timesince(t, value, depth=depth), expected)\n"], "sample_281": ["def test_process_request_invalid_app_label(self):\n    request = self.factory.get(self.url, {'term': 'is', 'app_label': 'invalid_app', 'model_name': Answer._meta.model_name, 'field_name': 'question'})\n    request.user = self.superuser\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "def test_process_request_invalid_model(self):\n    \"\"\"\n    Invalid model name in the request parameters raises PermissionDenied.\n    \"\"\"\n    invalid_opts = {\n        'app_label': Answer._meta.app_label,\n        'model_name': 'InvalidModel',\n        'field_name': 'question'\n    }\n    request = self.factory.get(self.url, {'term': 'is', **invalid_opts})\n    request.user = self.superuser\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "def test_invalid_page_parameter(self):\n    request = self.factory.get(self.url, {'term': '', 'page': 'invalid', **self.opts})\n    request.user = self.superuser\n    with self.assertRaises(Http404):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "def test_process_request_invalid_app_label(self):\n    request = self.factory.get(self.url, {'term': 'is', 'app_label': 'invalid', **self.opts})\n    request.user = self.superuser\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "def test_invalid_term(self):\n    request = self.factory.get(self.url, {'term': '', **self.opts})\n    request.user = self.superuser\n    with self.assertRaises(Http404):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n"], "sample_766": ["def test_minibatch_dictionary_learning_partial_fit():\n    n_components = 12\n    rng = np.random.RandomState(0)\n    V = rng.randn(n_components, n_features)  # random init\n    V /= np.sum(V ** 2, axis=1)[:, np.newaxis]\n    dict1 = MiniBatchDictionaryLearning(n_components, n_iter=10 * len(X),\n                                        batch_size=1,\n                                        alpha=1, shuffle=False, dict_init=V,\n                                        random_state=0).fit(X)\n    dict2 = MiniBatchDictionaryLearning(n_components, alpha=1,\n                                        n_iter=1, dict_init=V,\n                                        random_state=0)\n    for i in range(10):\n        for sample in X:\n            dict2.partial_fit(sample[np.newaxis, :], y=None)\n\n    assert not np.all(sparse_encode(X, dict1.components_, alpha=1) == 0)\n    assert_array_almost_equal(dict1.components_, dict2.components_,\n                              decimal=2)\n", "def test_dict_learning_online_partial_fit_multiple_calls():\n    n_components = 12\n    rng = np.random.RandomState(0)\n    V = rng.randn(n_components, n_features)  # random init\n    V /= np.sum(V ** 2, axis=1)[:, np.newaxis]\n    dict1 = MiniBatchDictionaryLearning(n_components, n_iter=10 * len(X),\n                                        batch_size=1,\n                                        alpha=1, shuffle=False, dict_init=V,\n                                        random_state=0).fit(X)\n    dict2 = MiniBatchDictionaryLearning(n_components, alpha=1,\n                                        n_iter=1, dict_init=V,\n                                        random_state=0)\n    dict2.partial_fit(X[:3])\n    dict2.partial_fit(X[3:6])\n    dict2.partial_fit(X[6:])\n    assert not np.all(sparse_encode(X, dict1.components_, alpha=1) == 0)\n    assert_array_almost_equal(dict1.components_, dict2.components_,\n                              decimal=2)\n", "def test_sparse_encode_copies_input(algo):\n    n_components = 12\n    rng = np.random.RandomState(0)\n    V = rng.randn(n_components, n_features)  # random init\n    V /= np.sum(V ** 2, axis=1)[:, np.newaxis]\n    Xf = check_array(X, order='F')\n    Xf.flags.writeable = False\n    sparse_encode(Xf, V, algorithm=algo)\n    assert not Xf.flags.writeable\n", "def test_dict_learning_init_by_svd():\n    n_components = 5\n    dico = DictionaryLearning(n_components, random_state=0).fit(X)\n    components_svd = dico.components_\n    dico = DictionaryLearning(n_components, dict_init=components_svd).fit(X)\n    assert_array_almost_equal(dico.components_, components_svd)\n", "def test_dict_learning_online_batch_size():\n    n_components = 12\n    rng = np.random.RandomState(0)\n    V = rng.randn(n_components, n_features)  # random init\n    V /= np.sum(V ** 2, axis=1)[:, np.newaxis]\n\n    # Test that batch_size=1, batch_size=n_samples and batch_size a divisor\n    # of n_samples give the same results\n    dico1 = MiniBatchDictionaryLearning(\n        n_components, n_iter=1, batch_size=1, alpha=1, shuffle=False,\n        dict_init=V, random_state=0).fit(X)\n    dico2 = MiniBatchDictionaryLearning(\n        n_components, n_iter=n_samples, batch_size=n_samples, alpha=1,\n        shuffle=False, dict_init=V, random_state=0).fit(X)\n    dico3 = MiniBatchDictionaryLearning(\n        n_components, n_iter=n_samples // 2, batch_size=n_samples // 2, alpha=1,\n        shuffle=False, dict_init=V, random_state=0).fit(X)\n\n    assert_array_almost_equal(dico1.components_, dico2.components_, decimal=2)\n    assert_array_almost_equal(dico2.components_, dico3.components_, decimal=2)\n"], "sample_265": ["def test_get_template_raise_template_does_not_exist(self):\n    \"\"\"\n    get_template raises TemplateDoesNotExist when template does not exist.\n    \"\"\"\n    engine = DjangoTemplates({\n        'DIRS': [],\n        'APP_DIRS': False,\n        'NAME': 'django',\n        'OPTIONS': {},\n    })\n\n    with self.assertRaises(TemplateDoesNotExist):\n        engine.get_template('non_existent_template.html')\n", "def test_get_template(self):\n    \"\"\"\n    Test the get_template method of the DjangoTemplates class.\n    \"\"\"\n    engine = DjangoTemplates({\n        'DIRS': [],\n        'APP_DIRS': True,\n        'NAME': 'django',\n        'OPTIONS': {},\n    })\n\n    # Test with a template that exists in an app directory\n    template_name = 'template_tests/test.html'\n    template = engine.get_template(template_name)\n    self.assertIsInstance(template, Template)\n    self.assertEqual(template.origin.name, template_name)\n\n    # Test with a template that does not exist\n    template_name = 'non_existent_template.html'\n    with self.assertRaises(TemplateDoesNotExist):\n        engine.get_template(template_name)\n", "def test_get_templatetag_libraries_custom_libraries_priority(self):\n    \"\"\"\n    Custom template tag libraries take priority over discovered ones.\n    \"\"\"\n    custom_library_name = 'good_tags'\n    discovered_library_name = 'subpackage.tags'\n    engine = DjangoTemplates({\n        'DIRS': [],\n        'APP_DIRS': True,\n        'NAME': 'django',\n        'OPTIONS': {\n            'libraries': {\n                custom_library_name: 'template_backends.apps.good.templatetags.custom_tags',\n                discovered_library_name: 'template_backends.apps.good.templatetags.discovered_tags',\n            },\n        },\n    })\n\n    self.assertEqual(\n        engine.engine.libraries[custom_library_name],\n        'template_backends.apps.good.templatetags.custom_tags',\n    )\n    self.assertEqual(\n        engine.engine.libraries[discovered_library_name],\n        'template_backends.apps.good.templatetags.discovered_tags',\n    )\n", "def test_get_template_with_exception(self):\n    \"\"\"\n    Test that TemplateDoesNotExist is raised with the correct information when\n    get_template() fails to find a template.\n    \"\"\"\n    engine = self.engine_class({\n        'DIRS': [],\n        'APP_DIRS': False,\n        'NAME': 'django',\n        'OPTIONS': {},\n    })\n    with self.assertRaises(TemplateDoesNotExist) as cm:\n        engine.get_template('nonexistent_template.html')\n    exc = cm.exception\n    self.assertEqual(exc.backend, engine)\n    self.assertIsInstance(exc, TemplateDoesNotExist)\n    self.assertIn('nonexistent_template.html', exc.args[0])\n", "def test_render_with_request(self):\n    \"\"\"Test rendering a template with a request.\"\"\"\n    engine = DjangoTemplates({\n        'DIRS': [],\n        'APP_DIRS': False,\n        'NAME': 'django',\n        'OPTIONS': {},\n    })\n    template = engine.from_string('{{ request.path }}')\n    request = self.request_factory.get('/')\n    context = {'request': request}\n    self.assertEqual(template.render(context), '/')\n"], "sample_870": ["def test_gpr_predict_input_not_modified_return_cov():\n    \"\"\"\n    Check that the input X is not modified by the predict method of the\n    GaussianProcessRegressor when setting return_cov=True.\n\n    Non-regression test for:\n    https://github.com/scikit-learn/scikit-learn/issues/24340\n    \"\"\"\n    gpr = GaussianProcessRegressor(kernel=CustomKernel()).fit(X, y)\n\n    X2_copy = np.copy(X2)\n    _, _ = gpr.predict(X2, return_cov=True)\n\n    assert_allclose(X2, X2_copy)\n", "def test_gpr_alpha(kernel):\n    # Test that alpha is used in the log marginal likelihood computation.\n    gpr = GaussianProcessRegressor(kernel=kernel, alpha=0.1)\n    gpr.fit(X, y)\n    lml = gpr.log_marginal_likelihood(gpr.kernel_.theta)\n    gpr.alpha = 0.2\n    lml_alpha = gpr.log_marginal_likelihood(gpr.kernel_.theta, clone_kernel=False)\n    assert lml != lml_alpha\n", "def test_y_std_is_positive(n_targets):\n    \"\"\"Check that standard deviations in y_std are always non-negative.\"\"\"\n    rng = np.random.RandomState(0)\n\n    n_samples_train, n_samples_test = 10, 7\n    X_train = rng.randn(n_samples_train, 3)\n    X_test = rng.randn(n_samples_test, 3)\n    y_train = rng.randn(n_samples_train, n_targets)\n\n    model = GaussianProcessRegressor()\n    model.fit(X_train, y_train)\n\n    _, y_std = model.predict(X_test, return_std=True)\n\n    assert np.all(y_std >= 0)\n", "def test_gpr_sample_y_consistency(kernel):\n    # Test that samples drawn from GP are consistent with predictive\n    # distribution.\n    rng = np.random.RandomState(1234)\n\n    n_samples_train = 10\n    n_samples_test = 7\n    n_samples_y_test = 300000\n    y_train_shape = (n_samples_train,)\n    X_train = rng.randn(n_samples_train, 3)\n    y_train = rng.randn(*y_train_shape)\n\n    model = GaussianProcessRegressor(kernel=kernel).fit(X_train, y_train)\n\n    y_samples = model.sample_y(X_train, n_samples=n_samples_y_test)\n\n    # More digits accuracy would require many more samples\n    y_mean, y_cov = model.predict(X_train, return_cov=True)\n    assert_almost_equal(y_mean, np.mean(y_samples, 1), 1)\n    assert_almost_equal(\n        np.diag(y_cov) / np.diag(y_cov).max(),\n        np.var(y_samples, 1) / np.diag(y_cov).max(),\n        1,\n    )\n", "def test_gpr_optimizer_restarts():\n    \"\"\"\n    Test that the optimizer is restarted the correct number of times and that\n    each restart uses a different initial hyperparameter setting.\n    \"\"\"\n    n_restarts = 5\n    kernel = RBF(length_scale=1.0)\n    gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=n_restarts)\n\n    # Capture the initial theta values for each restart\n    initial_thetas = []\n\n    # Monkey patch the optimizer to capture the initial theta values\n        initial_thetas.append(initial_theta)\n        # Use a dummy optimizer that simply returns the initial theta\n        return initial_theta, obj_func(initial_theta, eval_gradient=False)\n\n    gpr.optimizer = custom_optimizer\n\n    gpr.fit(X, y)\n\n    # Check that the optimizer was restarted the correct number of times\n    assert len(initial_thetas) == n_restarts + 1  # +1 for the initial fit\n\n    # Check that each restart used a different initial hyperparameter setting\n    assert len(set(tuple(theta) for theta in initial_thetas)) == len(initial_thetas)\n"], "sample_68": ["    def test_technical_500_response_status_code(self):\n        try:\n            raise ValueError(\"Can't find my keys\")\n        except ValueError:\n            exc_type, exc_value, tb = sys.exc_info()\n        request = self.rf.get('/test_view/')\n        response = technical_500_response(request, exc_type, exc_value, tb)\n        self.assertEqual(response.status_code, 500)\n", "    def test_technical_500_response_ajax(self):\n        \"\"\"\n        Test that technical_500_response returns a plain text response for AJAX\n        requests.\n        \"\"\"\n        try:\n            raise ValueError(\"Can't find my keys\")\n        except ValueError:\n            exc_type, exc_value, tb = sys.exc_info()\n        request = self.rf.get('/test_view/', HTTP_X_REQUESTED_WITH='XMLHttpRequest')\n        response = technical_500_response(request, exc_type, exc_value, tb)\n        self.assertEqual(response['Content-Type'], 'text/plain; charset=utf-8')\n", "    def test_technical_404_response(self):\n        \"\"\"\n        Test that technical_404_response returns a HttpResponseNotFound instance.\n        \"\"\"\n        request = RequestFactory().get('/technical404/')\n        exception = Exception('Test Exception')\n        response = technical_404_response(request, exception)\n        self.assertIsInstance(response, HttpResponseNotFound)\n", "    def test_request_path(self):\n        request = self.rf.get('/path/to/view/')\n        exception = Http404({'path': '/path/to/view/'})\n        response = technical_404_response(request, exception)\n        self.assertContains(response, '<code>path/to/view/</code>', status_code=404)\n", "    def test_is_active(self):\n        \"Check that the filter is only active when DEBUG is False.\"\n        request = mock.Mock(DEBUG=True)\n        filter = SafeExceptionReporterFilter()\n        self.assertFalse(filter.is_active(request))\n\n        request = mock.Mock(DEBUG=False)\n        self.assertTrue(filter.is_active(request))\n"], "sample_890": ["def test_n_features_to_select_auto_with_different_estimators(estimator):\n    \"\"\"Check the behaviour of `n_features_to_select=\"auto\"` with different estimators\"\"\"\n    n_features = 10\n    tol = 1e-3\n    X, y = make_regression(n_features=n_features, random_state=0)\n    sfs = SequentialFeatureSelector(\n        estimator,\n        n_features_to_select=\"auto\",\n        tol=tol,\n        cv=2,\n    )\n    sfs.fit(X, y)\n\n    max_features_to_select = n_features - 1\n\n    assert sfs.get_support(indices=True).shape[0] <= max_features_to_select\n    assert sfs.n_features_to_select_ <= max_features_to_select\n    assert sfs.transform(X).shape[1] <= max_features_to_select\n    assert sfs.get_support(indices=True).shape[0] == sfs.n_features_to_select_\n", "def test_cross_val_score_error():\n    \"\"\"Check that an error is raised when cross_val_score fails\"\"\"\n    n_samples, n_features = 50, 3\n    X, y = make_regression(n_samples, n_features, random_state=0)\n\n    sfs = SequentialFeatureSelector(\n        LinearRegression(), n_features_to_select=\"auto\", cv=2, scoring=\"bad_scoring\"\n    )\n\n    with pytest.raises(ValueError, match=\"scoring value 'bad_scoring' is not valid\"):\n        sfs.fit(X, y)\n", "def test_get_support_mask():\n    # Test that the private _get_support_mask method returns the same as the\n    # public get_support method\n\n    X, y = make_regression(n_features=10)\n    sfs = SequentialFeatureSelector(\n        LinearRegression(), n_features_to_select=\"auto\", cv=2\n    )\n    sfs.fit(X, y)\n\n    public_support = sfs.get_support()\n    private_support = sfs._get_support_mask()\n\n    assert_array_equal(public_support, private_support)\n", "def test_n_features_to_select_edge_cases(estimator):\n    # Test edge cases of n_features_to_select\n\n    n_features = 1\n    X, y = make_regression(n_features=n_features)\n    sfs = SequentialFeatureSelector(estimator, n_features_to_select=n_features)\n    sfs.fit(X, y)\n    assert sfs.get_support(indices=True).shape[0] == n_features\n\n    sfs = SequentialFeatureSelector(estimator, n_features_to_select=0)\n    with pytest.raises(ValueError, match=\"n_features_to_select must be either\"):\n        sfs.fit(X, y)\n\n    sfs = SequentialFeatureSelector(estimator, n_features_to_select=-1)\n    with pytest.raises(ValueError, match=\"n_features_to_select must be either\"):\n        sfs.fit(X, y)\n\n    sfs = SequentialFeatureSelector(estimator, n_features_to_select=1.5)\n    with pytest.raises(ValueError, match=\"n_features_to_select must be either\"):\n        sfs.fit(X, y)\n", "def test_direction():\n    \"\"\"Check the behaviour of the direction parameter\n    with `direction='forward'` and `direction='backward'`\n    \"\"\"\n\n    n_features = 10\n    X, y = make_regression(n_features=n_features, random_state=0)\n\n    sfs_forward = SequentialFeatureSelector(\n        LinearRegression(),\n        n_features_to_select=\"auto\",\n        direction=\"forward\",\n        cv=2,\n    )\n    sfs_backward = SequentialFeatureSelector(\n        LinearRegression(),\n        n_features_to_select=\"auto\",\n        direction=\"backward\",\n        cv=2,\n    )\n\n    sfs_forward.fit(X, y)\n    sfs_backward.fit(X, y)\n\n    assert not np.array_equal(sfs_forward.support_, sfs_backward.support_)\n    assert sfs_forward.get_support().sum() == sfs_backward.get_support().sum()\n    assert sfs_forward.transform(X).shape[1] == sfs_backward.transform(X).shape[1]\n"], "sample_1162": ["def test_Function_kind():\n    class MyFunction(Function):\n        pass\n    assert MyFunction(comm_x).kind is NumberKind\n    assert MyFunction(noncomm_x).kind is UndefinedKind\n", "def test_function_kind():\n    assert Function('f')(comm_x).kind is UndefinedKind\n    assert Function('f')(noncomm_x).kind is UndefinedKind\n", "def test_Function_kind():\n    class my_function(Function):\n        pass\n    assert my_function(comm_x).kind is NumberKind\n    assert my_function(noncomm_x).kind is UndefinedKind\n", "def test_UndefinedFunction_kind():\n    assert UndefinedFunction('f').kind is UndefinedKind\n    assert UndefinedFunction('g', real=True).kind is UndefinedKind\n    f = UndefinedFunction('f', real=True)\n    assert f(comm_x).kind is UndefinedKind\n    assert f(noncomm_x).kind is UndefinedKind\n", "def test_Function_kind():\n    f_comm = Function('f')\n    f_noncomm = Function('f', commutative=False)\n\n    assert f_comm(kind=NumberKind).kind is NumberKind\n    assert f_noncomm(kind=NumberKind).kind is UndefinedKind\n    assert f_comm(kind=NumberKind).kind is NumberKind\n    assert f_noncomm(kind=UndefinedKind).kind is UndefinedKind\n\n    assert f_comm(comm_x).kind is NumberKind\n    assert f_noncomm(comm_x).kind is NumberKind\n    assert f_comm(noncomm_x).kind is UndefinedKind\n    assert f_noncomm(noncomm_x).kind is UndefinedKind\n"], "sample_193": ["    def test_deconstruct_swappable_model(self):\n        class SwappableModel(models.Model):\n            pass\n\n        class TestModel(models.Model):\n            fk = models.ForeignKey('swappable_model', on_delete=models.CASCADE)\n\n            class Meta:\n                app_label = 'migrations'\n                apps = Apps(['migrations'])\n\n        with self.settings(SWAPPABLE_MODEL='migrations.SwappableModel'):\n            state = ModelState.from_model(TestModel)\n            self.assertEqual(state.fields['fk'].remote_field.model, 'migrations.SwappableModel')\n", "    def test_related_field_check(self):\n        new_apps = Apps(['migrations'])\n\n        class Author(models.Model):\n            class Meta:\n                app_label = 'migrations'\n                apps = new_apps\n\n        class Book(models.Model):\n            author = models.ForeignKey(Author, models.CASCADE, related_name='books')\n            author2 = models.ForeignKey(Author, models.CASCADE, related_name='books')\n\n            class Meta:\n                app_label = 'migrations'\n                apps = new_apps\n\n        project_state = ProjectState()\n        project_state.add_model(ModelState.from_model(Author))\n        project_state.add_model(ModelState.from_model(Book))\n        self.assertEqual(len(project_state.apps.get_models()), 2)\n\n        book_state = project_state.models['migrations', 'book']\n        errors = book_state.check()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].msg, \"Reverse accessor for 'migrations.Book.author' clashes with field name 'migrations.Author.books'.\")\n\n        # test field with '+' as related_name\n        class Publisher(models.Model):\n            books = models.ManyToManyField(Book, related_name='+')\n\n            class Meta:\n                app_label = 'migrations'\n                apps = new_apps\n\n        project_state.add_model(ModelState.from_model(Publisher))\n        self.assertEqual(len(project_state.apps.get_models()), 3)\n\n        publisher_state = project_state.models['migrations', 'publisher']\n        errors = publisher_state.check()\n        self.assertEqual(len(errors), 0)\n", "def test_field_model_is_swapped(self):\n    \"\"\"\n    When a field is pointing to a model that is swapped, the model\n    should be swapped in the rendered state too.\n    \"\"\"\n    new_apps = Apps(['migrations'])\n\n    class Author(models.Model):\n        name = models.CharField(max_length=255)\n\n        class Meta:\n            app_label = 'migrations'\n            apps = new_apps\n            swappable = 'Author'\n\n    class Book(models.Model):\n        author = models.ForeignKey(Author, models.CASCADE)\n\n        class Meta:\n            app_label = 'migrations'\n            apps = new_apps\n\n    project_state = ProjectState()\n    project_state.add_model(ModelState.from_model(Book))\n    project_state.apps  # Render project state\n\n    # Swap the author model\n    project_state.apps.swappable_setting('AUTHOR', 'migrations.NewAuthor')\n\n    new_apps = Apps(['migrations'])\n    class NewAuthor(models.Model):\n        name = models.CharField(max_length=255)\n\n        class Meta:\n            app_label = 'migrations'\n            apps = new_apps\n\n    project_state.add_model(ModelState.from_model(NewAuthor))\n\n    book = project_state.apps.get_model('migrations', 'Book')\n    self.assertEqual(book._meta.get_field('author').remote_field.model._meta.label, 'migrations.newauthor')\n", "    def test_resolve_relation(self):\n        model = ModelState('migrations', 'Model', fields=[], bases=(models.Model,))\n        self.assertEqual(resolve_relation(model, RECURSIVE_RELATIONSHIP_CONSTANT), model)\n        self.assertEqual(resolve_relation(model, 'Model'), 'migrations.Model')\n        self.assertEqual(resolve_relation(model, 'migrations.Model'), 'migrations.Model')\n        self.assertEqual(resolve_relation(model, models.Model), models.Model)\n", "    def test_check_for_conflicting_fields(self):\n        \"\"\"\n        Tests for #23453, #27033 - Ensure that RelatedField's\n        check method correctly checks for field clashes.\n        \"\"\"\n        new_apps = Apps(['migrations'])\n\n        class Author(models.Model):\n            food_set = models.IntegerField()\n            food = models.ForeignKey('Food', models.CASCADE)\n\n            class Meta:\n                app_label = 'migrations'\n                apps = new_apps\n\n        class Food(models.Model):\n            food = models.IntegerField()\n\n            class Meta:\n                app_label = 'migrations'\n                apps = new_apps\n\n        project_state = ProjectState()\n        project_state.add_model(ModelState.from_model(Author))\n        project_state.add_model(ModelState.from_model(Food))\n\n        errors = project_state.apps.get_model('migrations', 'Author')._meta.get_field('food').check()\n        self.assertEqual(len(errors), 3)\n"], "sample_1077": ["def test_ComplexRegion_boundary():\n    a, b = Interval(2, 5), Interval(4, 8)\n    c1 = ComplexRegion(a*b)\n    c2 = ComplexRegion(a*b, polar=False)\n    assert c1.boundary == c1\n    assert c2.boundary == c2\n", "def test_ImageSet_multivariate_iterator_not_injective():\n    L = Lambda((x, y), (x - x % 2, y - y % 2))  # produces (0, 0), (2, 0), (0, 2), (2, 2), (0, 4), ...\n    evens = ImageSet(L, S.Naturals * S.Naturals)\n    i = iter(evens)\n    # No repeats here\n    assert (next(i), next(i), next(i), next(i)) == ((0, 0), (2, 0), (0, 2), (2, 2))\n", "def test_ComplexRegion_invalid_input():\n    raises(ValueError, lambda: ComplexRegion(\"abc\"))\n    raises(ValueError, lambda: ComplexRegion(FiniteSet(1, 2, 3)))\n    raises(ValueError, lambda: ComplexRegion(ProductSet(Interval(1, 2), Interval(3, 4), Interval(5, 6))))\n    raises(ValueError, lambda: ComplexRegion(Union(Interval(1, 2), Interval(3, 4), Interval(5, 6))))\n    raises(ValueError, lambda: ComplexRegion(Interval(1, 2), polar=\"True\"))\n", "def test_ComplexRegion_contains_multiple_tuples():\n    c1 = ComplexRegion(ProductSet(Interval(1, 3), Interval(1, 3)))\n    assert all(Tuple(i, j) in c1 for i, j in cartes((1, 2, 3), (1, 2, 3)))\n    assert not any(Tuple(i, j) in c1 for i, j in cartes((-1, 4), (1, 2, 3)))\n\n    c2 = ComplexRegion(ProductSet(Interval(1, 3), Interval(1, 3)), polar=True)\n    assert all(Tuple(i, j) in c2 for i, j in cartes((1, 2, 3), (0, pi/2, pi, 3*pi/2, 2*pi)))\n    assert not any(Tuple(i, j) in c2 for i, j in cartes((-1, 0), (0, pi/2, pi, 3*pi/2, 2*pi)))\n", "def test_ComplexRegion_iterator():\n    # Test the iterator of ComplexRegion\n    # Rectangular form\n    c1 = ComplexRegion(Interval(1, 3)*Interval(1, 3))\n    c1_iter = iter(c1)\n    # expected = [1 + I, 2 + I, 3 + I, 1 + 2*I, 2 + 2*I, 3 + 2*I, \n    #             1 + 3*I, 2 + 3*I, 3 + 3*I]\n    expected = [1 + I, 1 + 2*I, 1 + 3*I, 2 + I, 2 + 2*I, 2 + 3*I, \n                3 + I, 3 + 2*I, 3 + 3*I]\n    assert [next(c1_iter) for _ in range(9)] == expected\n\n    # Polar form\n    c2 = ComplexRegion(Interval(1, 3)*Interval(0, 2*S.Pi), polar=True)\n    c2_iter = iter(c2)\n    # expected = [1, 1.54044006211683, 2.54044006211683, 2.44929359829471 + 1.11022302462516*I,\n    #             3.54044006211683, 2.44929359829471 - 1.11022302462516*I, 1.44929359829471 + 2.22044604925031*I,\n    #             3.44929359829471 + 2.22044604925031*I, 1.44929359829471 - 2.22044604925031*I]\n    expected = [1, 1.54044006211683, 2.54044006211683, 2.44929359829471 + 1.11022302462516*I,\n                3.54044006211683, 2.44929359829471 - 1.11022302462516*I, 1.44929359829471 + 2.22044604925031*I,\n                3.44929359829471 + 2.22044604925031*I, 1.44929359829471 - 2.22044604925031*I]\n    assert [next(c2"], "sample_152": ["def test_can_fast_delete_avoids_cascading_foreign_keys(self):\n    child = RChild.objects.create()\n    parent = child.r_ptr\n    self.assertTrue(Collector(using='default').can_fast_delete(child))\n    self.assertFalse(Collector(using='default').can_fast_delete(parent))\n", "    def test_add_dependency(self):\n        collector = Collector(using='default')\n        model = A\n        dependency = R\n        collector.add_dependency(model, dependency)\n        self.assertIn(model, collector.dependencies)\n        self.assertIn(dependency, collector.dependencies[model])\n", "    def test_can_fast_delete(self):\n        r = R.objects.create()\n        collector = Collector(using='default')\n        self.assertFalse(collector.can_fast_delete(r, from_field=A._meta.get_field('r')))\n\n        a = create_a('setnull')\n        collector = Collector(using='default')\n        self.assertTrue(collector.can_fast_delete(a.setnull, from_field=A._meta.get_field('setnull')))\n", "def test_fast_delete_double_delete(self):\n    \"\"\"Regression for #31998: fast delete with double delete.\"\"\"\n    m = M.objects.create()\n    M2.objects.create(m=m)\n    M2.objects.create(m=m)\n    with self.assertNumQueries(2):\n        m.delete()\n", "    def test_clear_restricted_objects_from_set(self):\n        collector = Collector(using='default')\n        model = R\n        field_name = 'related'\n        obj = R.objects.create()\n        collector.add_restricted_objects(model._meta.get_field(field_name), [obj])\n        self.assertEqual(len(collector.restricted_objects[model]), 1)\n        collector.clear_restricted_objects_from_set(model, {obj})\n        self.assertEqual(len(collector.restricted_objects[model]), 0)\n"], "sample_1019": ["def test_monotonic_sign_noncommutative():\n    from sympy import Symbol\n    from sympy.abc import x\n    A = Symbol('A', commutative=False)\n    F = _monotonic_sign\n    assert F(A) is None\n    assert F(A + 1).is_positive\n    assert F(A - 1).is_nonpositive\n    assert F(x*A + 1).is_nonnegative\n    assert F(x*A - 1).is_nonpositive\n    assert F(A*x + 1).is_nonnegative\n    assert F(A*x - 1).is_nonpositive\n", "def test_monotonic_sign_with_powers():\n    F = _monotonic_sign\n    x = symbols('x')\n    p = Dummy(positive=True)\n    assert F(p**2) is None\n    assert F(p**2 + 1).is_positive\n    assert F(p**3).is_nonnegative\n    assert F(-p**2).is_nonpositive\n    assert F(-p**3).is_negative\n    assert F(x**2) is None\n    assert F(x**3) is None\n    assert F(x**2 + 1).is_positive\n    assert F(x**3 + 1) is None\n    assert F(-x**2).is_nonpositive\n    assert F(-x**3).is_negative\n    assert F(-x**2 - 1).is_negative\n    assert F(-x**3 - 1).is_negative\n", "def test_factor_terms_coverage():\n    # issue 12235\n    assert factor_terms((x + x*y + x*y**2) * (x + y + y**2)) == \\\n        x*y*(y + 1)*(x + y)*(x + 1)\n", "def test_mask_nc():\n    # Check non-commutative objects and symbols\n    A, B, C = symbols('A, B, C', commutative=False)\n    assert _mask_nc(A) == (A, {}, [A])\n    assert _mask_nc(A**2) == (A**2, {}, [A])\n    assert _mask_nc(A + B) == (A + B, None, [A, B])\n    assert _mask_nc(A + B + C) == (A + B + C, None, [A, B, C])\n    assert _mask_nc(A*B) == (A*B, {}, [A, B])\n    assert _mask_nc(A*B + A*C) == (A*B + A*C, {}, [A, B, C])\n    assert _mask_nc(A*(B + C)) == (A*(B + C), {}, [A, B, C])\n\n    # Check commutative objects\n    x, y, z = symbols('x, y, z')\n    assert _mask_nc(x) == (x, {}, [])\n    assert _mask_nc(x + y) == (x + y, {}, [])\n    assert _mask_nc(x*y) == (x*y, {}, [])\n    assert _mask_nc(x*y + z) == (x*y + z, {}, [])\n\n    # Check mixed commutative and non-commutative objects\n    assert _mask_nc(x*A) == (x*A, {}, [A])\n    assert _mask_nc(x*(A + B)) == (x*(A + B), {}, [A, B])\n    assert _mask_nc(A + x) == (A + x, {}, [A])\n    assert _mask_nc(A + x + y) == (A + x + y, {}, [A])\n    assert _mask_nc(A*B + x) == (A*B + x, {}, [A, B])\n    assert _mask_nc(A*B + x + y) == (A*B + x + y, {}, [A, B])\n\n    # Check non-commutative objects in a commutative expression\n    assert _mask_nc(x*(A + B)) == (x*_mask_nc(A + B)[0], _mask_nc(A + B)[1], _mask_nc(A + B)[2])\n    assert _mask_nc(x*(A*B + C)) == (x*_mask_nc(A*B + C)[0],", "def test_issue_9876():\n    A, B = symbols('A B', commutative=False)\n    assert factor_nc(A*(A + B) + (A + B)*B) == (A + B)**2\n    assert factor_nc((A + B)*(A + B) + (A + B)**2) == (A + B)**3\n    assert factor_nc(A*(A + B)**2 + (A + B)**2*B) == (A + B)**2*(A + B)\n    assert factor_nc(A*(B + A)**2 + (B + A)**2*B) == (A + B)**2*(B + A)\n    assert factor_nc(A**3 + A**2*B + A*B**2 + B**3) == (A + B)**3\n    assert factor_nc(A**3 + A**2*B + A*B**2 + B**2*A) == (A + B)**3\n    assert factor_nc(A**2*B + B*A**2 + B**2*A + B**3) == B*(A + B)**3\n    assert factor_nc(B*A**3 + A*B**3 + A**2*B**2 + B*A**2*B) == B*(A + B)**3\n    assert factor_nc(A**2*B + B*A**2 + B**3 + A*B**2) == (A + B)**3\n    assert factor_nc(A**2*B + B*A**2 + B**3 + B**2*A) == (A + B)**3\n    assert factor_nc(A*B + B*A) == (A + B)**2\n    assert factor_nc(B*A + A*B + A**2) == A*(A + B)\n    assert factor_nc(B*A + A*B + B**2) == B*(A + B)\n"], "sample_213": ["    def test_hash(self):\n        file = FieldFile(None, None, 'test.txt')\n        self.assertEqual(hash(file), hash('test.txt'))\n", "    def test_fieldfile_hash_equality(self):\n        f1 = FieldFile(None, None, 'test.txt')\n        f2 = FieldFile(None, None, 'test.txt')\n        f3 = FieldFile(None, None, 'test2.txt')\n        self.assertEqual(hash(f1), hash(f2))\n        self.assertNotEqual(hash(f1), hash(f3))\n        self.assertEqual(f1, f2)\n        self.assertNotEqual(f1, f3)\n", "    def test_file_descriptor_property(self):\n        instance = object()\n        field = FileField()\n        file = FieldFile(instance, field, 'file_name')\n        self.assertEqual(file.name, 'file_name')\n        self.assertIs(file.instance, instance)\n        self.assertIs(file.field, field)\n        self.assertIs(file.storage, field.storage)\n", "    def test_fieldfile_repr(self):\n        fieldfile = FieldFile(None, None, 'name')\n        self.assertEqual(repr(fieldfile), \"FieldFile: name\")\n", "    def test_descriptor_delete(self):\n        \"\"\"\n        Test that the descriptor for the file attribute correctly deletes the\n        file when the instance is deleted.\n        \"\"\"\n        obj = Storage()\n        obj.normal.save(\"test_file.txt\", ContentFile(\"test content\"))\n        obj.delete()\n        self.assertFalse(self.storage.exists(obj.normal.name))\n"], "sample_406": ["    def test_empty_manager(self):\n        manager = EmptyManager(Article)\n        self.assertEqual(manager.get_queryset().count(), 0)\n", "    def test_manager_deconstruct(self):\n        manager = BaseManager()\n        manager.model = 'TestModel'\n        manager.name = 'test_manager'\n        deconstructed = manager.deconstruct()\n        self.assertEqual(deconstructed[0], False)\n        self.assertIsInstance(deconstructed[1], str)\n        self.assertIsNone(deconstructed[2])\n        self.assertEqual(deconstructed[3], ())\n        self.assertEqual(deconstructed[4], {})\n", "    def test_manager_creation_counter(self):\n        manager1 = BaseManager()\n        manager2 = BaseManager()\n        self.assertEqual(manager1.creation_counter, 0)\n        self.assertEqual(manager2.creation_counter, 1)\n", "    def test_base_manager_from_queryset_class_creation_counter(self):\n        BaseManager.creation_counter = 0\n        class CustomQuerySet(models.QuerySet):\n            pass\n\n        class CustomManager(BaseManager.from_queryset(CustomQuerySet)):\n            pass\n\n        self.assertEqual(BaseManager.creation_counter, 2)\n", "    def test_str_representation(self):\n        class TestManager(BaseManager):\n            model = mock.Mock(_meta=mock.Mock(label=\"myapp.mymodel\"))\n            name = \"my_manager\"\n\n        manager = TestManager()\n        self.assertEqual(str(manager), \"myapp.mymodel.my_manager\")\n"], "sample_736": ["def test_logreg_intercept_scaling_non_default():\n    # Test that the intercept_scaling parameter is used correctly\n    X, y = make_classification(n_samples=20, n_features=5, random_state=0)\n    clf = LogisticRegression(intercept_scaling=10.0, random_state=0)\n    clf.fit(X, y)\n\n    X_intercept = np.hstack((X, 10.0 * np.ones((X.shape[0], 1))))\n    clf_no_intercept = LogisticRegression(fit_intercept=False, random_state=0)\n    clf_no_intercept.fit(X_intercept, y)\n\n    assert_array_almost_equal(clf.intercept_, 10.0 * clf_no_intercept.coef_[-1])\n    assert_array_almost_equal(clf.coef_, clf_no_intercept.coef_[:-1])\n", "def test_logreg_predict_proba_multiclass():\n    # Test that predict_proba in multiclass case returns the same result\n    # when using 'ovr' and 'multinomial' methods with 3 or more classes.\n    X, y = make_classification(n_samples=10, n_features=20, random_state=0,\n                               n_classes=3, n_informative=10)\n\n    # Predicted probabilities using the true-entropy loss should give a\n    # smaller loss than those using the ovr method.\n    clf_multi = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\")\n    clf_multi.fit(X, y)\n    clf_ovr = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\")\n    clf_ovr.fit(X, y)\n    assert_array_almost_equal(clf_multi.predict_proba(X), clf_ovr.predict_proba(X))\n", "def test_logistic_regression_ill_conditional_data():\n    # Test that logistic regression can handle ill-conditioned data\n    X, y = make_classification(n_samples=100, n_features=10,\n                               n_informative=5, n_repeated=0,\n                               n_redundant=5, random_state=0)\n\n    # Make the data ill-conditioned by adding a very large constant to one\n    # of the features\n    X[:, 0] += 1e10\n\n    lr = LogisticRegression()\n    with ignore_warnings(category=RuntimeWarning):\n        lr.fit(X, y)\n", "def test_logistic_regression_X_dtype():\n    # Test that the input X data type is checked for LogisticRegression\n\n    # Test with unsupported dtype\n    X = np.array([[1, 2], [3, 4]], dtype=object)\n    y = np.array([0, 1])\n    msg = \"X has dtype object, expected float or int.\"\n    assert_raise_message(TypeError, msg, LogisticRegression().fit, X, y)\n\n    # Test with supported dtype\n    X = np.array([[1, 2], [3, 4]], dtype=np.float64)\n    y = np.array([0, 1])\n    LogisticRegression().fit(X, y)\n\n    # Test with supported dtype for sparse X\n    X = sp.csr_matrix(np.array([[1, 2], [3, 4]], dtype=np.float64))\n    y = np.array([0, 1])\n    LogisticRegression().fit(X, y)\n", "def test_logistic_regression_random_state():\n    # Test that logistic regression is reproducible using the random_state\n    # parameter\n    X, y = make_classification(n_samples=20, n_features=5, random_state=0)\n    solvers = [\"lbfgs\", \"newton-cg\", \"liblinear\", \"sag\", \"saga\"]\n    for solver in solvers:\n        if solver == \"liblinear\":\n            # LIBLINEAR is not reproducible\n            continue\n\n        clf1 = LogisticRegression(solver=solver, random_state=42)\n        clf2 = LogisticRegression(solver=solver, random_state=42)\n        clf1.fit(X, y)\n        clf2.fit(X, y)\n        assert_array_almost_equal(clf1.coef_, clf2.coef_, decimal=3)\n"], "sample_513": ["def test_legend_labelcolor_mfc_short():\n    # test the labelcolor for labelcolor='mfc'\n    fig, ax = plt.subplots()\n    ax.plot(np.arange(10), np.arange(10)*1, label='#1', markerfacecolor='r')\n    ax.plot(np.arange(10), np.arange(10)*2, label='#2', markerfacecolor='g')\n    ax.plot(np.arange(10), np.arange(10)*3, label='#3', markerfacecolor='b')\n\n    mpl.rcParams['legend.labelcolor'] = 'mfc'\n    leg = ax.legend()\n    for text, color in zip(leg.get_texts(), ['r', 'g', 'b']):\n        assert mpl.colors.same_color(text.get_color(), color)\n", "def test_draggable_legend_remove():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [1, 2], label='test')\n    leg = ax.legend()\n    leg.set_draggable(True)\n    assert leg.get_draggable()\n    leg.remove()\n    assert not leg.get_draggable()\n", "def test_legend_remove_from_axes_and_figure():\n    fig, ax = plt.subplots()\n    lines = ax.plot(range(10))\n    leg = ax.legend(lines, \"test\")\n    assert ax.get_legend() is not None\n    assert fig.legends == [leg]\n    leg.remove()\n    assert ax.get_legend() is None\n    assert fig.legends == []\n\n    fig, ax = plt.subplots()\n    lines = ax.plot(range(10))\n    leg = fig.legend(lines, \"test\")\n    assert ax.get_legend() is None\n    assert fig.legends == [leg]\n    leg.remove()\n    assert ax.get_legend() is None\n    assert fig.legends == []\n", "def test_legend_framealpha_none():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [1, 2], label='test')\n    legend = ax.legend(framealpha=None)\n    assert legend.legendPatch.get_alpha() == 1\n", "def test_legend_title_text_prop():\n    # test the title text properties\n    fig, ax = plt.subplots()\n    ax.plot(range(10))\n    leg = ax.legend(title='Aardvark', title_fontproperties={'family': 'serif'})\n    assert leg.get_title().get_family()[0] == 'serif'\n    leg = ax.legend(title='Aardvark', title_fontproperties={'size': 15})\n    assert leg.get_title().get_size() == 15\n    leg = ax.legend(title='Aardvark', title_fontproperties={'weight': 'bold'})\n    assert leg.get_title().get_weight() == 700\n"], "sample_1144": ["def test_requires_partial_empty_expr():\n    assert requires_partial(Derivative(0, symbols('x'))) is False\n    assert requires_partial(Derivative(0, symbols('x', integer=True))) is False\n    assert requires_partial(Derivative(0, symbols('x'), symbols('y'))) is False\n    assert requires_partial(Derivative(0, symbols('x', integer=True), symbols('y'))) is False\n", "def test_requires_partial_nested_derivatives():\n    x, y, z = symbols('x y z')\n    f = symbols('f', cls=Function)\n\n    assert requires_partial(Derivative(Derivative(f(x, y), x), y)) is True\n    assert requires_partial(Derivative(Derivative(f(x, y), x, y), x)) is True\n    assert requires_partial(Derivative(Derivative(f(x, y, z), x, y, z), x, y)) is True\n    assert requires_partial(Derivative(Derivative(f(x, y, z), x, y, z), x, y, z)) is True\n    assert requires_partial(Derivative(Derivative(f(x, y, z), (x, 2), (y, 2)), x)) is True\n    assert requires_partial(Derivative(Derivative(f(x, y, z), (x, 2), y), (x, 2))) is True\n", "def test_requires_partial_with_nested_derivatives():\n    x, y, z = symbols('x y z')\n    f = x * y * z\n\n    # Test with nested derivatives\n    assert requires_partial(Derivative(Derivative(f, x), y)) is True\n    assert requires_partial(Derivative(Derivative(f, x, y), z)) is True\n    assert requires_partial(Derivative(Derivative(f, x, y, z), x)) is True\n\n    # Test with nested derivatives and an integral\n    assert requires_partial(Derivative(Integral(Derivative(f, x), (x, 0, 1)), y)) is True\n    assert requires_partial(Derivative(Derivative(Integral(f, (x, 0, 1)), y), z)) is True\n\n    # Test with nested derivatives and a function with an unspecified number of variables\n    f = symbols('f', cls=Function)\n    assert requires_partial(Derivative(Derivative(f(x, y), x), y)) is True\n    assert requires_partial(Derivative(Derivative(f(x, y, z), x, y), z)) is True\n", "def test_requires_partial_edge_cases():\n    # Test requires_partial on an expression with no free symbols\n    assert requires_partial(42) is False\n    assert requires_partial(3.14) is False\n    assert requires_partial(\"hello\") is False\n\n    # Test requires_partial on a derivative with no free symbols\n    from sympy import symbols, Derivative\n    x = symbols('x', integer=True)\n    f = x ** 2\n    assert requires_partial(Derivative(f, x)) is False\n    assert requires_partial(Derivative(f, x, x)) is False\n\n    # Test requires_partial on a derivative of an Integral object\n    from sympy import Integral\n    f = Integral(x ** 2, (x, 0, 1))\n    assert requires_partial(Derivative(f, x)) is False\n    assert requires_partial(Derivative(f, x, x)) is False\n\n    # Test requires_partial on a derivative of a Function object\n    from sympy import Function\n    f = Function('f')\n    assert requires_partial(Derivative(f(x), x)) is False\n    assert requires_partial(Derivative(f(x, y), x)) is True\n", "def test_requires_partial_with_named_symbols():\n    a, b, c = symbols('a b c', cls=symbols('x').__class__)\n    assert requires_partial(Derivative(a * b, a)) is True\n    assert requires_partial(Derivative(a * b, b)) is True\n\n    assert requires_partial(Derivative(a * b, c)) is True\n    assert requires_partial(Derivative(Derivative(a * b, a), b)) is False\n\n    # Issue with non-integer free_symbols\n    assert requires_partial(Derivative(a * b + c, a)) is True\n"], "sample_16": ["    def test_diag_indices(self):\n        self.check(np.diag_indices)\n", "    def test_diag_indices(self):\n        o = np.diag_indices(3)\n        assert o[0].dtype == int\n        assert o[1].dtype == int\n        assert np.all(o[0] == np.diag_indices(3)[0])\n        assert np.all(o[1] == np.diag_indices(3)[1])\n", "    def test_logspace(self):\n        self.check(np.logspace, 1.0, 3.0)\n", "    def test_sum(self):\n        q = np.array([[1, 2], [3, 4]]) * u.m\n        assert_array_equal(np.sum(q, axis=0), np.sum(q.value, axis=0) * u.m)\n        assert_array_equal(np.sum(q, axis=1), np.sum(q.value, axis=1) * u.m)\n", "    def setup_method(self):\n        self.q = np.arange(9.0).reshape(3, 3) * u.m\n"], "sample_131": ["def test_serialize_db_to_string(self):\n    creation = connection.creation_class(connection)\n    with mock.patch.object(creation, '_get_test_db_name') as mocked_get_test_db_name:\n        with mock.patch.object(creation, '_get_database_display_str') as mocked_get_database_display_str:\n            with mock.patch.object(creation, 'log') as mocked_log:\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=True)\n                mocked_get_test_db_name.assert_called_once()\n                mocked_get_database_display_str.assert_called_once()\n                mocked_log.assert_called_once()\n", "    def test_log_call(self, mocked_log):\n        creation = BaseDatabaseCreation(self.get_connection_copy())\n        creation.log('Test message')\n        mocked_log.assert_called_once_with('Test message')\n", "    def test_serialize_db_to_string(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        with mock.patch.object(creation, '_create_test_db'):\n            with mock.patch.object(creation, 'connection') as mocked_connection:\n                with mock.patch.object(creation, 'log') as mocked_log:\n                    creation.create_test_db(verbosity=0, autoclobber=True, serialize=True)\n                    mocked_log.assert_called_once()\n                    mocked_connection.ops.quote_name.assert_called_once()\n", "    def test_create_test_db(self, mocked_createcachetable, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        test_database_name = 'my_test_db'\n        with mock.patch.object(creation, '_create_test_db', return_value=test_database_name):\n            test_db_name = creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n            self.assertEqual(test_db_name, test_database_name)\n            mocked_createcachetable.assert_called_once_with(database=connection.alias)\n            mocked_migrate.assert_called_once()\n            mocked_ensure_connection.assert_called_once()\n", "    def test_serialize_and_deserialize_db(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        # Build list of all apps to serialize\n        from django.db.migrations.loader import MigrationLoader\n        loader = MigrationLoader(connection)\n        app_list = []\n        for app_config in apps.get_app_configs():\n            if (\n                app_config.models_module is not None and\n                app_config.label in loader.migrated_apps and\n                app_config.name not in settings.TEST_NON_SERIALIZED_APPS\n            ):\n                app_list.append((app_config, None))\n        # Make a function to iteratively return every object\n            for model in serializers.sort_dependencies(app_list):\n                if (model._meta.can_migrate(connection) and\n                        router.allow_migrate_model(connection.alias, model)):\n                    queryset = model._default_manager.using(connection.alias).order_by(model._meta.pk.name)\n                    yield from queryset.iterator()\n        # Serialize to a string\n        serialized_data = serializers.serialize(\"json\", get_objects(), indent=None)\n        # Deserialize data\n        deserialized_objects = list(serializers.deserialize(\"json\", serialized_data, using=connection.alias))\n        self.assertEqual(len(deserialized_objects), len(list(get_objects())))\n"], "sample_90": ["    def test_model_form_with_localized_char_fields(self):\n        class LocalizedCharModelForm(forms.ModelForm):\n            class Meta:\n                model = Category\n                localized_fields = ('name', 'slug')\n                fields = '__all__'\n\n        form = LocalizedCharModelForm({'name': 'Fran\u00e7ais', 'slug': 'francais'})\n        self.assertTrue(form.is_valid())\n        category = form.save()\n        self.assertEqual(category.name, 'Fran\u00e7ais')\n        self.assertEqual(category.slug, 'francais')\n", "    def test_fields_for_model_calls_get_limit_choices_to(self):\n        \"\"\"fields_for_model should call get_limit_choices_to to apply limit_choices_to.\"\"\"\n        field = mock.Mock()\n        field.get_limit_choices_to.return_value = {'a': 1}\n        formfield_callback = lambda f, **kwargs: f.formfield()\n        model = mock.Mock()\n        model._meta.get_fields.return_value = [field]\n        fields_for_model(model, fields=['field'], formfield_callback=formfield_callback)\n        field.get_limit_choices_to.assert_called_once()\n", "    def test_validate_unique(self):\n        class TestModel(models.Model):\n            field = models.CharField(max_length=10, unique=True)\n\n        form = modelform_factory(TestModel, fields=\"__all__\")\n        obj = TestModel.objects.create(field=\"test\")\n        data = {\"field\": \"test\"}\n        form_instance = form(data)\n        self.assertFalse(form_instance.is_valid())\n        self.assertEqual(len(form_instance.errors), 1)\n        self.assertEqual(form_instance.errors['field'], ['Testmodel with this Field already exists.'])\n", "    def test_save_with_existing_file(self):\n        # Test that saving a model with an existing file doesn't change the file.\n        file = SimpleUploadedFile('test.txt', b'content')\n        form = TextFileForm({'description': 'Test'}, files={'file': file})\n        instance = form.save()\n        instance.refresh_from_db()\n        self.assertEqual(instance.file.name, 'tests/test.txt')\n        self.assertEqual(instance.file.read(), b'content')\n        # If the file is present in the form, it should be updated\n        form = TextFileForm({'description': 'Test'}, files={'file': SimpleUploadedFile('test2.txt', b'new content')}, instance=instance)\n        instance = form.save()\n        instance.refresh_from_db()\n        self.assertEqual(instance.file.name, 'tests/test2.txt')\n        self.assertEqual(instance.file.read(), b'new content')\n", "    def test_base_model_form_set(self):\n        \"\"\"\n        This test ensures that BaseModelFormSet's save_new method uses commit=False\n        for ModelForms without a primary key field.\n\n        This was broken in Django 1.6 due to the introduction of Model._meta.pk\n        and was fixed in Django 1.7.\n        \"\"\"\n        class ExplicitPKForm(forms.ModelForm):\n            class Meta:\n                model = ExplicitPK\n                fields = ('desc',)\n\n        class MyModelFormSet(BaseModelFormSet):\n            model = ExplicitPK\n            form = ExplicitPKForm\n\n        formset = MyModelFormSet(data={'form-0-desc': 'desc1'})\n        self.assertTrue(formset.is_valid())\n        instances = formset.save()\n        self.assertEqual(len(instances), 1)\n        self.assertIsNone(instances[0].pk)\n"], "sample_519": ["def test_set_layout_engine_warning():\n    fig = Figure(layout=\"constrained\")\n    with pytest.warns(UserWarning):\n        fig.set_layout_engine(layout=\"tight\")\n    assert isinstance(fig.get_layout_engine(), TightLayoutEngine)\n\n    with pytest.warns(UserWarning):\n        fig.set_layout_engine(layout=\"constrained\")\n    assert isinstance(fig.get_layout_engine(), ConstrainedLayoutEngine)\n\n    with pytest.warns(UserWarning):\n        fig.set_layout_engine(layout=\"compressed\")\n    assert isinstance(fig.get_layout_engine(), ConstrainedLayoutEngine)\n", "def test_subfigure_suptitle_kwargs():\n    fig = plt.figure()\n    subfig = fig.add_subfigure(gs[0])\n    subfig.suptitle('title', fontsize='x-large', fontweight='bold')\n    subfig.suptitle('title', fontsize=12, fontweight='light')\n", "def test_subfigure_subplots_adjust():\n    fig = plt.figure(layout='constrained')\n    sub_fig = fig.subfigures()\n    axs = sub_fig.subplots(2, 2)\n    axs[0, 0].plot([0, 1, 2])\n    axs[0, 1].plot([0, 1, 2])\n    axs[1, 0].plot([0, 1, 2])\n    axs[1, 1].plot([0, 1, 2])\n    fig.draw_without_rendering()\n    axs[0, 0].get_window_extent().x0\n    sub_fig.subplots_adjust(left=0.1)\n    fig.draw_without_rendering()\n    axs[0, 0].get_window_extent().x0\n", "def test_subfigure_tight_layout(fig_test, fig_ref):\n    fig_test.set_layout_engine(\"tight\")\n    fig_ref.set_layout_engine(\"tight\")\n\n    gs = fig_test.add_gridspec(1, 2)\n    ax1 = fig_test.add_subplot(gs[0, 0])\n    ax2 = fig_test.add_subplot(gs[0, 1])\n\n    subfig = fig_test.add_subfigure(gs[1, :])\n    ax3 = subfig.add_subplot(111)\n\n    ax1.plot([1, 2, 3])\n    ax2.plot([4, 5, 6])\n    ax3.plot([7, 8, 9])\n\n    ax1.set_title(\"Title 1\")\n    ax2.set_title(\"Title 2\")\n    ax3.set_title(\"Title 3\")\n\n    fig_test.tight_layout()\n\n    gs = fig_ref.add_gridspec(3, 2)\n    ax1 = fig_ref.add_subplot(gs[0, 0])\n    ax2 = fig_ref.add_subplot(gs[0, 1])\n    ax3 = fig_ref.add_subplot(gs[1:, :])\n\n    ax1.plot([1, 2, 3])\n    ax2.plot([4, 5, 6])\n    ax3.plot([7, 8, 9])\n\n    ax1.set_title(\"Title 1\")\n    ax2.set_title(\"Title 2\")\n    ax3.set_title(\"Title 3\")\n\n    fig_ref.tight_layout()\n", "def test_subfigure_add_artist():\n    # test that artists can be added to a subfigure\n    fig = plt.figure(layout='constrained')\n    sub = fig.subfigures(1, 2)\n\n    axs = sub[0].subplots(2, 2)\n    for ax in axs.flat:\n        pc = ax.pcolormesh(np.random.randn(30, 30), vmin=-2, vmax=2)\n        ax.set_title('subfig[0]')\n    sub[0].colorbar(pc, ax=axs)\n\n    axs = sub[1].subplots(1, 2)\n    for ax in axs:\n        ax.plot(np.arange(10))\n        ax.set_title('subfig[1]')\n    axs[0].set_facecolor('r')\n    axs[1].set_facecolor('g')\n    sub[1].add_artist(mpl.patches.Circle((0.5, 0.5), 0.1, transform=sub[1].transSubfigure,\n                                         facecolor='r', alpha=0.5))\n    sub[1].add_artist(mpl.patches.Rectangle((0.7, 0.2), 0.1, 0.1, transform=sub[1].transSubfigure,\n                                             facecolor='g', alpha=0.5))\n"], "sample_493": ["def test_annotation_with_arithmetic_expression(self):\n    authors = Author.objects.annotate(avg_age=F(\"age\") + 1).order_by(\"name\")\n    self.assertQuerySetEqual(\n        authors,\n        [\n            (\"Adrian Holovaty\", 35),\n            (\"Brad Dayley\", 46),\n            (\"Jacob Kaplan-Moss\", 36),\n            (\"James Bennett\", 30),\n            (\"Jeffrey Forcier\", 38),\n            (\"Paul Bissex\", 30),\n            (\"Peter Norvig\", 58),\n            (\"Stuart Russell\", 47),\n            (\"Wesley J. Chun\", 26),\n        ],\n        lambda a: (a.name, a.avg_age),\n    )\n\n    authors = Author.objects.annotate(avg_age=F(\"age\") - 1).order_by(\"name\")\n    self.assertQuerySetEqual(\n        authors,\n        [\n            (\"Adrian Holovaty\", 33),\n            (\"Brad Dayley\", 44),\n            (\"Jacob Kaplan-Moss\", 34),\n            (\"James Bennett\", 28),\n            (\"Jeffrey Forcier\", 36),\n            (\"Paul Bissex\", 28),\n            (\"Peter Norvig\", 56),\n            (\"Stuart Russell\", 45),\n            (\"Wesley J. Chun\", 24),\n        ],\n        lambda a: (a.name, a.avg_age),\n    )\n\n    authors = Author.objects.annotate(avg_age=F(\"age\") * 2).order_by(\"name\")\n    self.assertQuerySetEqual(\n        authors,\n        [\n            (\"Adrian Holovaty\", 68),\n            (\"Brad Dayley\", 90),\n            (\"Jacob Kaplan-Moss\", 70),\n            (\"James Bennett\", 58),\n            (\"Jeffrey Forcier\", 74),\n            (\"Paul Bissex\", 58),\n            (\"Peter Norvig\", 114),\n            (\"Stuart Russell\", 92),\n            (\"Wesley J. Chun\", 50),\n        ],\n        lambda a: (a.name, a.avg_age),\n    )\n\n    authors = Author.objects.annotate(avg_age=F(\"age\") / 2).order_by(\"name\")\n    self.assertQuerySetEqual(\n        authors,\n        [\n            (\"Adrian Holovaty\", 17.0),\n            (\"Brad Dayley\", 22.5),\n            (\"Jacob Kaplan-Moss\", 17.5),\n            (\"James Bennett\", 14.5),\n            (\"Jeffrey Forcier\", 18", "def test_aggregation_with_union(self):\n    qs1 = Author.objects.annotate(num_books=Count(\"book\")).filter(num_books__gt=1)\n    qs2 = Author.objects.annotate(num_books=Count(\"book\")).filter(num_books__lte=1)\n    qs = qs1.union(qs2)\n    with self.assertNumQueries(1):\n        result = qs.aggregate(total_books=Sum(\"num_books\"))\n    self.assertEqual(result[\"total_books\"], 10)\n", "def test_annotation_over_window_function(self):\n    authors = (\n        Author.objects.annotate(\n            rank=Window(\n                RowNumber(),\n                partition_by=F(\"age\"),\n                order_by=F(\"id\").asc(),\n            )\n        )\n        .annotate(total_age=F(\"age\") + F(\"rank\"))\n        .order_by(\"age\")\n    )\n    self.assertEqual(\n        list(authors.values_list(\"age\", \"rank\", \"total_age\")),\n        [\n            (25, 1, 26),\n            (29, 1, 30),\n            (29, 2, 31),\n            (29, 3, 32),\n            (34, 1, 35),\n            (34, 2, 36),\n            (35, 1, 36),\n            (37, 1, 38),\n            (45, 1, 46),\n            (46, 1, 47),\n            (57, 1, 58),\n        ],\n    )\n", "def test_aggregation_subquery_annotation_distinct(self):\n    authors_qs = Author.objects.values(\"name\").annotate(\n        books=Count(\"book\", distinct=True),\n    )\n    publishers_qs = Publisher.objects.annotate(\n        authors=Subquery(authors_qs.values(\"books\")),\n    )\n    self.assertEqual(\n        publishers_qs.values_list(\"name\", \"authors\"),\n        [\n            (\"Apress\", 1),\n            (\"Apress\", 1),\n            (\"Morgan Kaufmann\", 1),\n            (\"Morgan Kaufmann\", 1),\n            (\"Prentice Hall\", 1),\n            (\"Prentice Hall\", 1),\n            (\"Prentice Hall\", 1),\n            (\"Sams\", 1),\n            (\"Jonno's House of Books\", 0),\n        ],\n    )\n", "    def test_aggregation_window_function_collision(self):\n        \"\"\"\n        Window functions are included in the GROUP BY if they are\n        grouped against.\n        \"\"\"\n        b1 = Book.objects.create(\n            name=\"b1\",\n            publisher=self.p1,\n            pages=100,\n            rating=4.5,\n            price=10,\n            contact=self.a1,\n            pubdate=datetime.date.today(),\n        )\n        b1.authors.add(self.a1)\n        b2 = Book.objects.create(\n            name=\"b2\",\n            publisher=self.p2,\n            pages=1000,\n            rating=3.2,\n            price=50,\n            contact=self.a2,\n            pubdate=datetime.date.today(),\n        )\n        b2.authors.add(self.a1, self.a2)\n\n        # window annotations must be included in the GROUP BY\n        with self.assertNumQueries(1) as ctx:\n            aggs = (\n                Book.objects.annotate(\n                    row_number=Window(expression=RowNumber(), partition_by=F(\"publisher\"))\n                )\n                .values_list(\"row_number\", flat=True)\n                .annotate(count=Count(\"*\"))\n            )\n        sql = ctx.captured_queries[0][\"sql\"].lower()\n        self.assertEqual(sql.count(\"select\"), 2, \"Subquery wrapping required\")\n        self.assertEqual(aggs.count(), 2)\n\n        # window annotations with alias collision must be included in the GROUP BY\n        with self.assertNumQueries(1) as ctx:\n            aggs = (\n                Book.objects.annotate(row_number=Window(expression=RowNumber(), partition_by=F(\"publisher\")))\n                .values(\"row_number\")\n                .annotate(row_number=Count(\"*\"))\n            )\n        sql = ctx.captured_queries[0][\"sql\"].lower()\n        self.assertEqual(sql.count(\"select\"), 2, \"Subquery wrapping required\")\n        self.assertEqual(dict(aggs), {1: 2})\n"], "sample_537": ["def test_cohere_detrend():\n    N = 1024\n    np.random.seed(19680801)\n    x = np.random.randn(N)\n    # phase offset\n    y = np.roll(x, 20)\n    # high-freq roll-off\n    y = np.convolve(y, np.ones(20) / 20., mode='same')\n\n    cohsq1, f = mlab.cohere(x, y, NFFT=256, Fs=2, noverlap=128, detrend=mlab.detrend_none)\n    cohsq2, f = mlab.cohere(x, y, NFFT=256, Fs=2, noverlap=128, detrend=mlab.detrend_mean)\n\n    assert_allclose(np.mean(cohsq1), 0.837, atol=1.e-3)\n    assert np.isreal(np.mean(cohsq1))\n    assert_allclose(np.mean(cohsq2), 0.837, atol=1.e-3)\n    assert np.isreal(np.mean(cohsq2))\n", "def test_psd_onesided_with_negative_frequency():\n    u = np.array([0, 1, 2, 3, 1, 2, 1])\n    dt = 1.0\n    Su = np.abs(np.fft.fft(u) * dt)**2 / (dt * u.size)\n    P, f = mlab.psd(u, NFFT=u.size, Fs=1/dt, window=mlab.window_none,\n                    detrend=mlab.detrend_none, noverlap=0, pad_to=None,\n                    scale_by_freq=None,\n                    sides='twosided')\n    Su_1side = np.append([Su[0]], Su[1:4] + Su[4:][::-1])\n    assert_allclose(P[:len(Su_1side)], Su_1side, atol=1e-06)\n    assert_allclose(P[len(Su_1side):], Su_1side[::-1], atol=1e-06)\n", "def test_stride_windows_n32_noverlap0_unflatten_2d_x(self):\n    n = 32\n    x = np.arange(n)[np.newaxis]\n    x2 = np.tile(x, (21, 1))\n    x1 = x2.T\n    x2 = x2.flatten()\n    y = mlab.stride_windows(x2, n)\n\n    assert y.shape == x1.shape\n    assert_array_equal(y, x1)\n", "def test_single_spectrum_windows(mode):\n    freqs = TestSpectral.freqs_spectrum\n    win = mlab.window_hanning(np.ones(TestSpectral.NFFT_spectrum_real))\n    spec, fsp = getattr(mlab, f\"{mode}_spectrum\")(\n        x=TestSpectral.y,\n        Fs=TestSpectral.Fs, sides=TestSpectral.sides, pad_to=TestSpectral.pad_to_spectrum,\n        window=win)\n    spec_n, fsp_n = getattr(mlab, f\"{mode}_spectrum\")(\n        x=TestSpectral.y,\n        Fs=TestSpectral.Fs, sides=TestSpectral.sides, pad_to=TestSpectral.pad_to_spectrum,\n        window=mlab.window_none)\n    assert_allclose(spec*win.sum(), spec_n, atol=1e-08)\n    assert_allclose(fsp, fsp_n, atol=1e-06)\n    assert spec.shape == fsp.shape\n", "    def test_covariance_factor_callable(self):\n        \"\"\"Test the covariance_factor when using a callable.\"\"\"\n            return 0.5\n        x1 = np.array([-7, -5, 1, 4, 5])\n        kde = mlab.GaussianKDE(x1, bw_method=bw_method)\n        y_expected = 0.5\n        assert kde.covariance_factor() == y_expected\n"], "sample_429": ["def test_email_validator_with_punycode(self):\n    validator = EmailValidator()\n    test_cases = [\n        (\"test@b\u00fccher.de\", True),\n        (\"test@xn--bcher-kva.de\", True),\n        (\"test@invalid-punycode.de\", False),\n    ]\n    for email, expected in test_cases:\n        try:\n            validator(email)\n            self.assertTrue(expected)\n        except ValidationError:\n            self.assertFalse(expected)\n", "def test_max_length_validator_clean(self):\n    v = MaxLengthValidator(10)\n    self.assertEqual(v.clean(\"example\"), 7)\n    self.assertEqual(v.clean(10), 10)\n    self.assertEqual(v.clean([1, 2, 3]), 3)\n    self.assertEqual(v.clean({\"a\": 1, \"b\": 2}), 2)\n", "    def test_prohibit_null_characters(self):\n        validator = ProhibitNullCharactersValidator()\n        self.assertIsNone(validator(\"hello\"))\n        with self.assertRaisesMessage(ValidationError, \"Null characters are not allowed.\"):\n            validator(\"\\x00hello\")\n        with self.assertRaisesMessage(ValidationError, \"Null characters are not allowed.\"):\n            validator(\"hello\\x00\")\n        with self.assertRaisesMessage(ValidationError, \"Null characters are not allowed.\"):\n            validator(\"he\\x00llo\")\n        with self.assertRaisesMessage(ValidationError, \"Null characters are not allowed.\"):\n            validator(\"\\x00\\x00\\x00\")\n", "def test_step_value_validator_with_zero_step(self):\n    with self.assertRaisesMessage(\n        ValueError,\n        \"Step size must be greater than zero.\",\n    ):\n        StepValueValidator(0)(10)\n", "def test_ipv46_address_validator(self):\n    for validator in ip_address_validator_map.values():\n        validator_func = validator[0][0]\n        with self.subTest(validator=validator_func):\n            try:\n                # Test a valid IPv4 address.\n                validator_func(\"192.168.0.1\")\n            except ValidationError:\n                self.fail(\"A valid IPv4 address raised a ValidationError\")\n\n            try:\n                # Test a valid IPv6 address.\n                validator_func(\"2001:0db8:85a3:0000:0000:8a2e:0370:7334\")\n            except ValidationError:\n                self.fail(\"A valid IPv6 address raised a ValidationError\")\n\n            try:\n                # Test an invalid IP address.\n                validator_func(\"256.1.1.1\")\n            except ValidationError:\n                pass\n            else:\n                self.fail(\"An invalid IP address did not raise a ValidationError\")\n\n            try:\n                # Test an invalid IP address.\n                validator_func(\"2001:0db8:85a3:0000:0000:8a2e:0370:733g\")\n            except ValidationError:\n                pass\n            else:\n                self.fail(\"An invalid IP address did not raise a ValidationError\")\n"], "sample_955": ["def test_unparse_lambda(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value, source) == expected\n", "def test_unparse_various_statements(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0], source) == expected\n", "def test_unparse_arguments(source, expected):\n    tree = ast.parse(\"def f(%s): pass\" % source)\n    assert ast.unparse(tree.body[0].args, source) == expected\n", "def test_unparse_argument_lists(source, expected):\n    tree = ast.parse(source)\n    assert ast.unparse(tree.body[0].args, source) == expected\n\n", "def test_unparse_with_constants(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value, source) == expected\n\n"], "sample_1166": ["def test_monomial_ldiv():\n    assert monomial_ldiv((3, 4, 1), (1, 2, 0)) == (2, 2, 1)\n    assert monomial_ldiv((3, 4, 1), (1, 2, 2)) == (2, 2, -1)\n    assert monomial_ldiv((1, 2, 3), (1, 2, 3)) == (0, 0, 0)\n    assert monomial_ldiv((1, 2, 3), (0, 0, 0)) == (1, 2, 3)\n", "def test_MonomialOps():\n    ops = MonomialOps(3)\n    assert ops.mul()((1, 2, 3), (4, 5, 6)) == (5, 7, 9)\n    assert ops.pow()( (1, 2, 3), 2) == (2, 4, 6)\n    assert ops.mulpow()( (1, 2, 3), (4, 5, 6), 2) == (9, 12, 15)\n    assert ops.ldiv()((4, 5, 6), (1, 2, 3)) == (3, 3, 3)\n    assert ops.div()((4, 5, 6), (1, 2, 3)) == (3, 3, 3)\n    assert ops.lcm()((1, 2, 3), (4, 5, 6)) == (4, 5, 6)\n    assert ops.gcd()((1, 2, 3), (4, 5, 6)) == (1, 2, 3)\n", "def test_monomial_ldiv():\n    assert monomial_ldiv((3, 4, 1), (1, 2, 0)) == (2, 2, 1)\n    assert monomial_ldiv((3, 4, 1), (2, 2, 2)) == (1, 2, -1)\n    assert monomial_ldiv((3, 4, 1), (4, 4, 1)) == (-1, 0, 0)\n", "def test_monomial_ops():\n    ops = MonomialOps(3)\n    assert ops.mul()( (1, 2, 3), (4, 5, 6) ) == (5, 7, 9)\n    assert ops.pow()( (1, 2, 3), 4 ) == (4, 8, 12)\n    assert ops.mulpow()( (1, 2, 3), (4, 5, 6), 2 ) == (9, 14, 18)\n    assert ops.ldiv()( (6, 8, 10), (3, 4, 5) ) == (3, 4, 5)\n    assert ops.div()( (6, 8, 10), (3, 4, 5) ) == (3, 4, 5)\n    assert ops.lcm()( (3, 4, 5), (6, 7, 8) ) == (6, 7, 8)\n    assert ops.gcd()( (6, 8, 10), (3, 4, 5) ) == (3, 4, 5)\n", "def test_monomial_ldiv():\n    assert monomial_ldiv((3, 4, 1), (1, 2, 0)) == (2, 2, 1)\n    assert monomial_ldiv((3, 4, 1), (1, 2, 2)) == (2, 2, -1)\n"], "sample_1203": ["def test_group_homomorphism_properties():\n    F, a, b = free_group(\"a, b\")\n    G = FpGroup(F, [a**3, b**3, (a*b)**2])\n    c = Permutation(3)(0, 1, 2)\n    d = Permutation(3)(1, 2, 3)\n    A = AlternatingGroup(4)\n    T = homomorphism(G, A, [a, b], [c, d])\n    assert T.is_injective()\n    assert T.is_surjective()\n    assert T.is_isomorphism()\n    assert not T.is_trivial()\n\n    E, e = free_group(\"e\")\n    H = FpGroup(E, [e**8])\n    P = PermutationGroup([Permutation(0, 1, 2, 3), Permutation(0, 2)])\n    T = homomorphism(H, P, [e], [Permutation(0, 1, 2, 3)])\n    assert not T.is_injective()\n    assert not T.is_surjective()\n    assert not T.is_isomorphism()\n    assert not T.is_trivial()\n\n    D = DihedralGroup(8)\n    p = Permutation(0, 1, 2, 3, 4, 5, 6, 7)\n    P = PermutationGroup(p)\n    T = homomorphism(P, D, [p], [p])\n    assert T.is_injective()\n    assert not T.is_surjective()\n    assert not T.is_isomorphism()\n    assert not T.is_trivial()\n\n    T2 = homomorphism(F, P, [F.generators[0]], P.generators)\n    T = T.compose(T2)\n    assert not T.is_injective()\n    assert not T.is_surjective()\n    assert not T.is_isomorphism()\n    assert not T.is_trivial()\n", "def test_homomorphism_properties():\n    # Test the properties of a homomorphism\n    F, a, b = free_group(\"a, b\")\n    G = FpGroup(F, [a**3, b**3, (a*b)**2])\n    c = Permutation(3)(0, 1, 2)\n    d = Permutation(3)(1, 2, 3)\n    A = AlternatingGroup(4)\n    T = homomorphism(G, A, [a, b], [c, d])\n    \n    assert T.is_injective() == (T.kernel().order() == 1)\n    assert T.is_surjective() == (T.image().order() == A.order())\n    assert T.is_isomorphism() == (T.is_injective() and T.is_surjective())\n    assert T.is_trivial() == (T.image().order() == 1)\n\n    assert T.compose(T).domain == G\n    assert T.compose(T).codomain == A\n\n    # Test the invert_subgroup method\n    H = T.image()\n    assert T.invert_subgroup(H).order() == G.order()\n    assert T.invert_subgroup(H).is_subgroup(G)\n\n    # Test the restrict_to method\n    S = G.subgroup([a])\n    assert T.restrict_to(S).domain == S\n    assert T.restrict_to(S).codomain == A\n", "def test_homomorphism_restrict_to():\n    # Restricting to subgroup\n    D = DihedralGroup(8)\n    p = Permutation(0, 1, 2, 3, 4, 5, 6, 7)\n    P = PermutationGroup(p)\n    T = homomorphism(P, D, [p], [p])\n    H = P.subgroup([p**2])\n    T_restricted = T.restrict_to(H)\n    assert T_restricted(a) == T(a) for a in H.generators\n    raises(ValueError, lambda: T.restrict_to(PermutationGroup(Permutation(1, 2))))\n", "def test_block_homomorphism():\n    # Test block homomorphism\n    D = DihedralGroup(8)\n    blocks = D.minimal_block([0, 3, 4], [1, 2, 5, 6, 7])\n    H = block_homomorphism(D, blocks)\n    assert H.domain == D\n    assert H.codomain.order() == 2\n    assert H.kernel().order() == 8\n    assert H.image().order() == 2\n\n    # Test block homomorphism for a non-minimal block\n    blocks = D.minimal_block([0, 3], [1, 2, 4, 5, 6, 7])\n    H = block_homomorphism(D, blocks)\n    assert H.domain == D\n    assert H.codomain.order() == 2\n    assert H.kernel().order() == 8\n    assert H.image().order() == 2\n", "def test_group_homomorphism_composition():\n    F, a, b = free_group(\"a, b\")\n    G = FpGroup(F, [a**2, b**3])\n    H = FpGroup(F, [a**3, b**2])\n    I = FpGroup(F, [a**5, b**4])\n\n    # Test composition with itself\n    T = homomorphism(G, H, G.generators, H.generators)\n    T2 = T.compose(T)\n    assert T2.domain == G\n    assert T2.codomain == H\n\n    # Test composition with a different homomorphism\n    S = homomorphism(H, I, H.generators, I.generators)\n    T3 = T.compose(S)\n    assert T3.domain == G\n    assert T3.codomain == I\n\n    # Test composition with a homomorphism that is not surjective\n    D = DihedralGroup(8)\n    p = Permutation(0, 1, 2, 3, 4, 5, 6, 7)\n    P = PermutationGroup(p)\n    T4 = homomorphism(G, P, G.generators, P.generators)\n    raises(ValueError, lambda: T4.compose(T2))\n\n    # Test composition with a homomorphism that is not injective\n    T5 = homomorphism(F, G, F.generators, G.generators)\n    T5.images = {a: G.identity, b: G.identity}\n    raises(ValueError, lambda: T.compose(T5))\n"], "sample_711": ["def test_iterparentnodeids_with_empty_string() -> None:\n    \"\"\"Test iterparentnodeids with an empty string as input.\"\"\"\n    result = list(nodes.iterparentnodeids(\"\"))\n    assert result == [\"\"]\n", "def test_add_marker_with_invalid_type(pytester: Pytester) -> None:\n    items = pytester.getitems(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    item = items[0]\n\n    with pytest.raises(ValueError, match=\"is not a string or pytest.mark.* Marker\"):\n        item.add_marker(123)  # type: ignore[arg-type]\n", "def test_iterparentnodeids_with_colons_in_test_name() -> None:\n    \"\"\"Test that the function iterparentnodeids correctly handles nodeids with colons in the test name.\"\"\"\n    nodeid = \"a/b::c::d:e:f\"\n    expected = [\"\", \"a\", \"a/b\", \"a/b::c\", \"a/b::c::d:e:f\"]\n    result = list(nodes.iterparentnodeids(nodeid))\n    assert result == expected\n", "def test_get_closest_marker(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.xyz\n            pass\n    \"\"\"\n    )\n    items = pytester.getitems()\n    marker = items[0].get_closest_marker(\"xyz\")\n    assert marker.name == \"xyz\"\n\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.parametrize(\"x\", [1])\n            pass\n    \"\"\"\n    )\n    items = pytester.getitems()\n    marker = items[0].get_closest_marker(\"parametrize\")\n    assert marker.name == \"parametrize\"\n\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.xyz\n        class TestXyz:\n            @pytest.mark.qwe\n                pass\n    \"\"\"\n    )\n    items = pytester.getitems()\n    marker1 = items[0].get_closest_marker(\"xyz\")\n    assert marker1.name == \"xyz\"\n    marker2 = items[0].get_closest_marker(\"qwe\")\n    assert marker2.name == \"qwe\"\n    marker3 = items[0].get_closest_marker(\"nonexist\")\n    assert marker3 is None\n", "def test_iter_markers_with_node() -> None:\n    class CustomNode(nodes.Node):\n            super().__init__(name, parent, config, session, nodeid, path)\n\n            pass\n\n    config = pytest.config.get_config()\n    session = pytest.Session(config)\n    parent_node = CustomNode(\"parent\", None, config, session, \"parent\", Path(\"/parent\"))\n    node = CustomNode(\"node\", parent_node, config, session, \"parent::node\", Path(\"/parent/node\"))\n\n    node.add_marker(\"custom\")\n    parent_node.add_marker(\"parent_custom\")\n\n    expected = [(parent_node, nodes.Mark(\"parent_custom\")), (node, nodes.Mark(\"custom\"))]\n    assert list(node.iter_markers_with_node()) == expected\n    assert list(node.iter_markers_with_node(\"custom\")) == [(node, nodes.Mark(\"custom\"))]\n    assert list(parent_node.iter_markers_with_node()) == [(parent_node, nodes.Mark(\"parent_custom\"))]\n    assert list(parent_node.iter_markers_with_node(\"custom\")) == []\n"], "sample_28": ["def test_record_valued_keyword_cards_in_lowercase(self):\n    \"\"\"\n    Test Record Valued Keyword Cards when keyword and field-specifier are in\n    lowercase.\n    \"\"\"\n\n    # Test different methods for initializing a card that should be\n    # recognized as a RVKC\n    c = fits.Card.fromstring(\"dp1     = 'naxis: 2' / A comment\")\n    assert c.keyword == \"DP1.NAXIS\"\n    assert c.value == 2.0\n    assert c.field_specifier == \"NAXIS\"\n    assert c.comment == \"A comment\"\n\n    c = fits.Card.fromstring(\"dp1     = 'naxis:  2.1'\")\n    assert c.keyword == \"DP1.NAXIS\"\n    assert c.value == 2.1\n    assert c.field_specifier == \"NAXIS\"\n\n    c = fits.Card.fromstring(\"dp1     = 'naxis: a'\")\n    assert c.keyword == \"DP1\"\n    assert c.value == \"NAXIS: a\"\n    assert c.field_specifier is None\n\n    c = fits.Card(\"dp1\", \"naxis: 2\")\n    assert c.keyword == \"DP1.NAXIS\"\n    assert c.value == 2.0\n    assert c.field_specifier == \"NAXIS\"\n\n    c = fits.Card(\"dp1\", \"naxis:  2.0\")\n    assert c.keyword == \"DP1.NAXIS\"\n    assert c.value == 2.0\n    assert c.field_specifier == \"NAXIS\"\n\n    c = fits.Card(\"dp1\", \"naxis: a\")\n    assert c.keyword == \"DP1\"\n    assert c.value == \"NAXIS: a\"\n    assert c.field_specifier is None\n\n    c = fits.Card(\"dp1.naxis\", 2)\n    assert c.keyword == \"DP1.NAXIS\"\n    assert c.value == 2.0\n    assert c.field_specifier == \"NAXIS\"\n\n    c = fits.Card(\"dp1.naxis\", 2.0)\n    assert c.keyword == \"DP1.NAXIS\"\n    assert c.value == 2.0\n    assert c.field_specifier == \"NAXIS\"\n\n    c = fits.Card(\"dp1.naxis\", \"a\")\n    assert c.keyword == \"DP1.NAXIS\"\n    assert c.value == \"a\"\n    assert c.field", "def test_set_rvkc(self):\n    \"\"\"\n    Test setting RVKCs directly\n    \"\"\"\n\n    header = fits.Header()\n    header[\"DP1.NAXIS\"] = 2\n    assert header[\"DP1.NAXIS\"] == 2.0\n    assert header[\"DP1\"] == \"NAXIS: 2\"\n\n    header[\"DP1.NAUX\"] = 2\n    assert header[\"DP1.NAUX\"] == 2.0\n    assert header[\"DP1\"] == \"NAXIS: 2\"\n    assert list(header)[1] == \"DP1.NAUX\"\n\n    header[\"DP1.AXIS.1\"] = 1.1\n    assert header[\"DP1.AXIS.1\"] == 1.1\n    assert header[\"DP1\"] == \"NAXIS: 2\"\n\n    header[\"DP1.AUX.1.COEFF.1\"] = 0.00048828125\n    assert header[\"DP1.AUX.1.COEFF.1\"] == 0.00048828125\n    assert header[\"DP1\"] == \"NAXIS: 2\"\n", "def test_update_from_header_with_record_valued_keyword_cards(self):\n    \"\"\"\n    Test updating a header with a header that contains record-valued keyword\n    cards.\n    \"\"\"\n    h1 = fits.Header([(\"A\", 1)])\n    h1[\"D2IM1.EXTVER\"] = 1\n    h2 = fits.Header([(\"B\", 2)])\n    h2[\"D2IM1.EXTVER\"] = 2\n    h2[\"D2IM1.EXTVER\"] = 3\n    h1.update(h2)\n    assert len(h1) == 3\n    assert h1[\"D2IM1.EXTVER\"] == [1.0, 2.0, 3.0]\n    assert h1[\"D2IM1.EXTVER\", 0] == 1.0\n    assert h1[\"D2IM1.EXTVER\", 1] == 2.0\n    assert h1[\"D2IM1.EXTVER\", 2] == 3.0\n    assert h1[\"D2IM1\"] == \"EXTVER: 1\"\n    assert h1[(\"D2IM1\", 1)] == \"EXTVER: 2\"\n    assert h1[(\"D2IM1\", 2)] == \"EXTVER: 3\"\n", "def test_record_valued_keyword_cards_with_hierarch_keywords(self):\n    \"\"\"\n    Regression test for https://github.com/astropy/astropy/issues/12655\n\n    Ensures that the addition of RVKCs with HIERARCH keywords works correctly.\n    \"\"\"\n    c1 = fits.Card.fromstring(\"HIERARCH DP1     = 'NAXIS: 2' / A comment\")\n    c2 = fits.Card.fromstring(\"HIERARCH DP1     = 'AXIS.1: 1' / A comment\")\n    c3 = fits.Card.fromstring(\"HIERARCH DP1     = 'AXIS.2: 2' / A comment\")\n\n    h = fits.Header([c1, c2, c3])\n\n    assert h[\"DP1.NAXIS\"] == 2.0\n    assert h[\"DP1.AXIS.1\"] == 1.0\n    assert h[\"DP1.AXIS.2\"] == 2.0\n\n    h[\"DP1.AXIS.1\"] = 3.0\n\n    assert h[\"DP1.AXIS.1\"] == 3.0\n", "def test_record_valued_keyword_card_long_string(self):\n    \"\"\"Test record-valued keyword card with a long string value.\"\"\"\n    header = fits.Header()\n    header[\"DP1\"] = \"AXIS.1: 'This is a long string value ' * 30\"\n    assert len(header.cards) == 3\n    assert header[\"DP1.AXIS.1\"] == 1.0\n    assert header.cards[1].keyword == \"DP1.AXIS.1\"\n    assert header.cards[1].field_specifier == \"AXIS.1\"\n    assert header.cards[1].value == 1.0\n    assert header.cards[1].comment == \"\"\n    assert str(header.cards[1]).rstrip() == \"DP1     = 'AXIS.1: This is a long string value This is a long string &'  \"\n    assert str(header.cards[2]).rstrip() == \"CONTINUE  'value This is a long string value This is a long string &'  \"\n    assert str(header.cards[3]).rstrip() == \"CONTINUE  '&' / \"\n\n    header = fits.Header()\n    header[\"DP1\"] = \"AXIS.1: 'This is a long string value ' * 40\"\n    assert len(header.cards) == 4\n    assert header[\"DP1.AXIS.1\"] == 1.0\n    assert header.cards[1].keyword == \"DP1.AXIS.1\"\n    assert header.cards[1].field_specifier == \"AXIS.1\"\n    assert header.cards[1].value == 1.0\n    assert header.cards[1].comment == \"\"\n    assert str(header.cards[1]).rstrip() == \"DP1     = 'AXIS.1: This is a long string value This is a long string &'  \"\n    assert str(header.cards[2]).rstrip() == \"CONTINUE  'value This is a long string value This is a long string &'  \"\n    assert str(header.cards[3]).rstrip() == \"CONTINUE  '&' / \"\n\n    header = fits.Header()\n    header[\"DP1\"] = \"AXIS.1: 'This is a long string value ' * 50\"\n    assert len(header.cards) == 5\n    assert header[\"DP1.AXIS.1\"] == 1.0\n    assert header.cards[1].keyword == \"DP1.AXIS.1\"\n   "], "sample_404": ["def test_render_variable_node_with_non_ascii_chars(self):\n    template = self._engine().from_string(\"{% for char in chars %}{{ char }}{% endfor %}\")\n    context = Context({\"chars\": [\"\u00e9\", \"\u00e8\", \"\u00e0\"]})\n    rendered = template.render(context)\n    self.assertEqual(rendered, \"\u00e9\u00e8\u00e0\")\n", "def test_template_syntax_error_origin(self):\n    \"\"\"\n    Errors in templates should include the template origin.\n    \"\"\"\n    engine = self._engine()\n    template = engine.from_string(\"lala{% foobar %}\")\n    msg = (\n        \"Invalid block tag on line 1: 'foobar'. Did you forget to \"\n        \"register or load this tag?\"\n    )\n    with self.assertRaisesMessage(TemplateSyntaxError, msg) as e:\n        template.render(Context())\n\n    if self.debug_engine:\n        debug = e.exception.template_debug\n        self.assertEqual(debug[\"name\"], UNKNOWN_SOURCE)\n", "def test_node_render_annotated(self):\n    \"\"\"\n    Test that Node.render_annotated() calls Node.render() when an exception\n    occurs and template engine debug mode is enabled.\n    \"\"\"\n    class TestNode(Node):\n            raise Exception(\"Test exception\")\n\n    node = TestNode()\n    with self.settings(DEBUG=True):\n        with self.assertRaises(Exception) as e:\n            node.render_annotated(Context())\n        self.assertEqual(str(e.exception), \"Test exception\")\n        if self.debug_engine:\n            self.assertEqual(e.exception._culprit_node, node)\n", "def test_debug_template_origin(self):\n    \"\"\"\n    Test that template origin is properly set on the template and its nodes\n    when debug is enabled.\n    \"\"\"\n    template = self._engine().from_string(\"content\")\n    self.assertIsInstance(template.origin, Origin)\n    self.assertEqual(template.origin.name, UNKNOWN_SOURCE)\n    for node in template.nodelist:\n        self.assertIsInstance(node.origin, Origin)\n        self.assertEqual(node.origin.name, UNKNOWN_SOURCE)\n", "def test_variable_attribute_separator(self):\n    \"\"\"\n    Test that VARIABLE_ATTRIBUTE_SEPARATOR is used correctly in variable\n    lookups.\n    \"\"\"\n    engine = self._engine()\n    t = engine.from_string(\"{% for item in items %}{{ item.name }}{% endfor %}\")\n    c = Context({\"items\": [{\"name\": \"Item 1\"}, {\"name\": \"Item 2\"}]})\n    self.assertEqual(t.render(c), \"Item 1 Item 2\")\n\n    # Test with a custom separator.\n    with override_settings(TEMPLATE_STRING_IF_INVALID=\"\",\n                           TEMPLATE_CONTEXT_PROCESSORS=[],\n                           TEMPLATE_DIRS=[]):\n        engine = self._engine(\n            string_if_invalid=\"\",\n            context_processors=[],\n            dirs=[],\n            builtins=[],\n            template_dirs=[],\n            origin=None,\n            autoescape=True,\n            loaders=[],\n            app_dirs=True,\n        )\n        engine.template_libraries = {}\n        engine.builtins = {}\n        engine.template_builtins = {}\n        engine.template_libraries = {}\n\n        # set a custom attribute separator\n        engine.variable_attribute_separator = \":\"\n\n        # now the template should use the custom separator\n        t = engine.from_string(\"{% for item in items %}{{ item:name }}{% endfor %}\")\n        c = Context({\"items\": [{\"name\": \"Item 1\"}, {\"name\": \"Item 2\"}]})\n        self.assertEqual(t.render(c), \"Item 1 Item 2\")\n"], "sample_453": ["    def test_prepopulated_fields_js(self):\n        request = self.request_factory.get(reverse(\"admin:auth_user_change\", args=[self.superuser.pk]))\n        request.user = self.superuser\n        admin = UserAdmin(User, site)\n        response = admin.change_view(request, str(self.superuser.pk))\n        template_context = prepopulated_fields_js(response.context_data)\n        self.assertIn(\"prepopulated_fields\", template_context)\n        self.assertIn(\"prepopulated_fields_json\", template_context)\n", "def test_prepopulated_fields_js(self):\n    \"\"\"\n    prepopulated_fields_js template tag should return prepopulated fields as JSON.\n    \"\"\"\n    article = Article.objects.all()[0]\n    request = self.request_factory.get(\n        reverse(\"admin:admin_views_article_change\", args=[article.pk])\n    )\n    request.user = self.superuser\n    admin = ArticleAdmin(Article, site)\n    response = admin.change_view(request, str(article.pk))\n    template_context = prepopulated_fields_js(response.context_data)\n    self.assertIn(\"prepopulated_fields_json\", template_context)\n    prepopulated_fields_json = json.loads(template_context[\"prepopulated_fields_json\"])\n    self.assertIsInstance(prepopulated_fields_json, list)\n", "    def test_prepopulated_fields_js(self):\n        \"\"\"\n        prepopulated_fields_js template tag should generate correct JSON.\n        \"\"\"\n        request = self.request_factory.get(\n            reverse(\"admin:auth_user_add\")\n        )\n        request.user = self.superuser\n        admin = UserAdmin(User, site)\n        admin.prepopulated_fields = {\"username\": (\"first_name\", \"last_name\")}\n        response = admin.add_view(request)\n        template_context = prepopulated_fields_js(response.context_data)\n        expected_prepopulated_fields_json = [\n            {\n                \"id\": \"#id_username\",\n                \"name\": \"username\",\n                \"dependency_ids\": [\"#id_first_name\", \"#id_last_name\"],\n                \"dependency_list\": [\"first_name\", \"last_name\"],\n                \"maxLength\": 150,\n                \"allowUnicode\": True,\n            }\n        ]\n        self.assertEqual(\n            template_context[\"prepopulated_fields_json\"],\n            json.dumps(expected_prepopulated_fields_json),\n        )\n", "    def test_cell_count(self):\n        \"\"\"\n        Test cell_count filter.\n        \"\"\"\n        # Create a simple inline admin form.\n        class QuestionInlineAdmin(admin.TabularInline):\n            model = Question\n\n        question = Question.objects.create(question=\"q\")\n        question_admin = QuestionInlineAdmin(Question, site)\n        inline_admin_form = question_admin.get_formset(request=self.request_factory.get(\"/\")).form\n        inline_admin_form.instance = question\n        inline_admin_form.fields[\"question\"].is_hidden = False\n\n        # Test with a visible field.\n        self.assertEqual(cell_count(inline_admin_form), 2)\n\n        # Test with a hidden field.\n        inline_admin_form.fields[\"question\"].is_hidden = True\n        self.assertEqual(cell_count(inline_admin_form), 1)\n\n        # Test with a fieldset.\n        class QuestionInlineAdmin(admin.TabularInline):\n            model = Question\n            fieldsets = [(None, {\"fields\": [\"question\"]})]\n\n        question_admin = QuestionInlineAdmin(Question, site)\n        inline_admin_form = question_admin.get_formset(request=self.request_factory.get(\"/\")).form\n        inline_admin_form.instance = question\n        inline_admin_form.fields[\"question\"].is_hidden = False\n        self.assertEqual(cell_count(inline_admin_form), 2)\n\n        # Test with multiple fieldsets.\n        class QuestionInlineAdmin(admin.TabularInline):\n            model = Question\n            fieldsets = [(None, {\"fields\": [\"question\"]}), (None, {\"fields\": [\"id\"]})]\n\n        question_admin = QuestionInlineAdmin(Question, site)\n        inline_admin_form = question_admin.get_formset(request=self.request_factory.get(\"/\")).form\n        inline_admin_form.instance = question\n        inline_admin_form.fields[\"question\"].is_hidden = False\n        inline_admin_form.fields[\"id\"].is_hidden = True\n        self.assertEqual(cell_count(inline_admin_form), 2)\n", "def test_prepopulated_fields_js_tag(self):\n    \"\"\"\n    Test that prepopulated_fields_js template tag updates the context correctly.\n    \"\"\"\n    request = self.request_factory.get(reverse(\"admin:auth_user_change\", args=[self.superuser.pk]))\n    request.user = self.superuser\n    admin = UserAdmin(User, site)\n    extra_context = {\"adminform\": self.MockAdminForm(prepopulated_fields=[{\"field\": \"field1\"}])}\n    response = admin.change_view(request, str(self.superuser.pk), extra_context=extra_context)\n    template_context = prepopulated_fields_js(response.context_data)\n    self.assertIn(\"prepopulated_fields\", template_context)\n    self.assertIn(\"prepopulated_fields_json\", template_context)\n    self.assertEqual(len(template_context[\"prepopulated_fields\"]), 1)\n    self.assertEqual(len(template_context[\"prepopulated_fields_json\"]), 1)\n"], "sample_93": ["def test_combinable(self):\n    msg = (\n        \"Expression contains mixed types: FloatField, IntegerField. \"\n        \"You must set output_field.\"\n    )\n    qs = Book.objects.annotate(sums=Sum(F('rating')) + Sum(F('pages')) + Sum(F('price')))\n    with self.assertRaisesMessage(FieldError, msg):\n        qs.first()\n\n    b1 = Book.objects.annotate(sums=Sum(F('rating') + F('pages') + F('price'),\n                               output_field=IntegerField())).get(pk=self.b4.pk)\n    self.assertEqual(b1.sums, 383)\n\n    b2 = Book.objects.annotate(sums=Sum(F('rating') + F('pages') + F('price'),\n                               output_field=FloatField())).get(pk=self.b4.pk)\n    self.assertEqual(b2.sums, 383.69)\n\n    b3 = Book.objects.annotate(sums=Sum(F('rating') + F('pages') + F('price'),\n                               output_field=DecimalField())).get(pk=self.b4.pk)\n    self.assertEqual(b3.sums, Approximate(Decimal(\"383.69\"), places=2))\n", "def test_window_function(self):\n    # Test basic window function usage.\n    publishers = Publisher.objects.annotate(\n        rank=Window(expression=Avg('num_awards'), order_by=F('num_awards').desc())\n    ).order_by('num_awards')\n\n    self.assertEqual([p.num_awards for p in publishers], [9, 7, 3, 1, 0])\n    self.assertEqual([p.rank for p in publishers], [6.2, 6.2, 6.2, 6.2, 6.2])\n\n    # Test window function with partition by.\n    publishers = Publisher.objects.annotate(\n        rank=Window(expression=Avg('num_awards'), partition_by=F('num_awards') % 2, order_by=F('num_awards').desc())\n    ).order_by('num_awards')\n\n    self.assertEqual([p.num_awards for p in publishers], [9, 7, 3, 1, 0])\n    self.assertEqual([p.rank for p in publishers], [8.0, 7.0, 2.0, 1.0, 0.0])\n\n    # Test window function with frame clause.\n    publishers = Publisher.objects.annotate(\n        sum=Window(expression=Sum('num_awards'), frame=RowRange(start=None, end=0))\n    ).order_by('num_awards')\n\n    self.assertEqual([p.num_awards for p in publishers], [9, 7, 3, 1, 0])\n    self.assertEqual([p.sum for p in publishers], [9, 16, 19, 20, 20])\n", "def test_bitwise_aggregates(self):\n    class MyMax(Func):\n        function = 'MAX'\n        output_field = IntegerField()\n        template = '%(function)s(%(expressions)s)'\n\n    class MySum(Func):\n        function = 'SUM'\n        output_field = IntegerField()\n        template = '%(function)s(%(expressions)s)'\n\n    books = Book.objects.annotate(\n        bitwise_or=MyMax('authors__age', output_field=IntegerField()),\n    )\n    self.assertEqual(books.get(pk=self.b1.pk).bitwise_or, 35)\n\n    books = Book.objects.annotate(\n        bitwise_or=MySum('authors__age', output_field=IntegerField()),\n    )\n    self.assertEqual(books.get(pk=self.b1.pk).bitwise_or, 69)\n\n    books = Book.objects.annotate(\n        bitwise_or=F('authors__age').bitor(1),\n    )\n    self.assertEqual(books.get(pk=self.b1.pk).bitwise_or, 35)\n\n    books = Book.objects.annotate(\n        bitwise_and=F('authors__age').bitand(1),\n    )\n    self.assertEqual(books.get(pk=self.b1.pk).bitwise_and, 1)\n\n    books = Book.objects.annotate(\n        bitwise_left_shift=F('authors__age').bitleftshift(1),\n    )\n    self.assertEqual(books.get(pk=self.b1.pk).bitwise_left_shift, 70)\n\n    books = Book.objects.annotate(\n        bitwise_right_shift=F('authors__age').bitrightshift(1),\n    )\n    self.assertEqual(books.get(pk=self.b1.pk).bitwise_right_shift, 17)\n", "def test_window_function(self):\n    \"\"\"\n    Test the use of window functions with different types of expressions.\n    \"\"\"\n    from django.db.models.functions import RowNumber, Rank, DenseRank\n    from django.db.models import Window\n\n    # Test with a plain expression\n    books = (\n        Book.objects\n        .annotate(\n            row_num=Window(RowNumber()),\n            rank=Window(Rank()),\n            dense_rank=Window(DenseRank()),\n        )\n        .order_by('rating')\n    )\n    self.assertEqual(books[0].row_num, 1)\n    self.assertEqual(books[0].rank, 1)\n    self.assertEqual(books[0].dense_rank, 1)\n    self.assertEqual(books[5].row_num, 6)\n    self.assertEqual(books[5].rank, 6)\n    self.assertEqual(books[5].dense_rank, 4)\n\n    # Test with a combined expression\n    books = (\n        Book.objects\n        .annotate(\n            rating_sum=Sum('rating'),\n            row_num=Window(RowNumber()),\n            rank=Window(Rank()),\n            dense_rank=Window(DenseRank()),\n        )\n        .order_by('rating')\n    )\n    self.assertEqual(books[0].rating_sum, 4.5)\n    self.assertEqual(books[0].row_num, 1)\n    self.assertEqual(books[0].rank, 1)\n    self.assertEqual(books[0].dense_rank, 1)\n    self.assertEqual(books[5].rating_sum, 4.0)\n    self.assertEqual(books[5].row_num, 6)\n    self.assertEqual(books[5].rank, 6)\n    self.assertEqual(books[5].dense_rank, 4)\n\n    # Test with a subquery expression\n    books = (\n        Book.objects\n        .annotate(\n            rating_sum=Sum('rating'),\n            row_num=Window(RowNumber()),\n            rank=Window(Rank()),\n            dense_rank=Window(DenseRank()),\n        )\n        .annotate(\n            count=Subquery(\n                Publisher.objects.filter(id=OuterRef('publisher_id'))\n                .values('id')\n                .annotate(count=Count('id'))\n                .values('count')\n            )\n        )\n        .order_by('rating')\n    )\n    self.assertEqual(books[0].count, 3)\n    self.assertEqual(books[0].row_num, 1)\n    self.assertEqual(books[0].rank, ", "def test_annotation_over_aggregate_over_annotation(self):\n    # Aggregate over an annotation that aggregates over another annotation\n    qs = (\n        Author.objects\n        .annotate(age_times_two=ExpressionWrapper(F('age') * 2, output_field=IntegerField()))\n        .annotate(avg_age_times_two=Avg('age_times_two'))\n    )\n    self.assertEqual(qs.aggregate(avg=Avg('avg_age_times_two'))['avg'], Approximate(75, places=1))\n\n    # Aggregate over an annotation that aggregates over another annotation\n    # with a complex expression\n    qs = (\n        Author.objects\n        .annotate(\n            friends_age=ExpressionWrapper(\n                F('friends__age') + F('age'),\n                output_field=IntegerField()\n            )\n        )\n        .annotate(avg_friends_age=Avg('friends_age'))\n    )\n    self.assertEqual(qs.aggregate(avg=Avg('avg_friends_age'))['avg'], Approximate(69.8, places=1))\n\n    # Aggregate over an annotation that aggregates over another annotation\n    # with a complex expression with multiple fields\n    qs = (\n        Author.objects\n        .annotate(\n            friends_age=ExpressionWrapper(\n                F('friends__age') + F('age') + 1,\n                output_field=IntegerField()\n            )\n        )\n        .annotate(avg_friends_age=Avg('friends_age'))\n    )\n    self.assertEqual(qs.aggregate(avg=Avg('avg_friends_age'))['avg'], Approximate(70, places=1))\n"], "sample_66": ["    def test_get_signed_cookie(self):\n        request = HttpRequest()\n        request.COOKIES = {\n            'signed_key': signing.get_cookie_signer(salt='salt').sign('value'),\n        }\n        self.assertEqual(request.get_signed_cookie('signed_key', salt='salt'), 'value')\n        self.assertIsNone(request.get_signed_cookie('unsigned_key', default=None))\n        with self.assertRaises(signing.BadSignature):\n            request.get_signed_cookie('signed_key', salt='wrong_salt')\n        with self.assertRaises(signing.BadSignature):\n            request.get_signed_cookie('tampered_key', salt='salt')\n", "def test_build_absolute_uri_with_port(self):\n    request = HttpRequest()\n    request.get_host = lambda: 'www.example.com:8080'\n    request.path = ''\n    self.assertEqual(\n        request.build_absolute_uri(location='/path/with:colons'),\n        'http://www.example.com:8080/path/with:colons'\n    )\n", "    def test_bytes_to_text(self):\n        self.assertEqual(bytes_to_text('hello', 'utf-8'), 'hello')\n        self.assertEqual(bytes_to_text(b'hello', 'utf-8'), 'hello')\n        self.assertEqual(bytes_to_text('hello', 'latin-1'), 'hello')\n        self.assertEqual(bytes_to_text(b'hello', 'latin-1'), 'hello')\n", "def test_get_signed_cookie(self):\n    request = HttpRequest()\n    request.COOKIES = {'signed_cookie': 'test_signed_cookie'}\n    # Test with default salt\n    self.assertEqual(request.get_signed_cookie('signed_cookie'), None)\n    # Test with provided salt\n    self.assertEqual(request.get_signed_cookie('signed_cookie', salt='test_salt'), None)\n    # Test with provided default value\n    self.assertEqual(request.get_signed_cookie('signed_cookie', default='default_value'), 'default_value')\n    # Test with provided max_age\n    self.assertEqual(request.get_signed_cookie('signed_cookie', max_age=3600), None)\n\n    # Test with a valid signed cookie\n    request.COOKIES = {'signed_cookie': signing.get_cookie_signer(salt='test_salt').sign('signed_cookie_value')}\n    self.assertEqual(request.get_signed_cookie('signed_cookie', salt='test_salt'), 'signed_cookie_value')\n", "def test_querydict_urlencode(self):\n    query_dict = QueryDict(mutable=True)\n    query_dict['key'] = ['value']\n    self.assertEqual(query_dict.urlencode(), 'key=value')\n    query_dict.setlist('key', ['value1', 'value2'])\n    self.assertEqual(query_dict.urlencode(), 'key=value1&key=value2')\n    query_dict.setlist('key2', ['value3'])\n    self.assertEqual(query_dict.urlencode(), 'key=value1&key=value2&key2=value3')\n    query_dict['key3'] = 'value4'\n    self.assertEqual(query_dict.urlencode(), 'key=value1&key=value2&key2=value3&key3=value4')\n    query_dict['key4'] = ''\n    self.assertEqual(query_dict.urlencode(), 'key=value1&key=value2&key2=value3&key3=value4&key4=')\n    query_dict['key5'] = None\n    self.assertEqual(query_dict.urlencode(), 'key=value1&key=value2&key2=value3&key3=value4&key4=&key5=None')\n    query_dict.clear()\n    self.assertEqual(query_dict.urlencode(), '')\n"], "sample_580": ["def test_variable_type_edge_cases():\n\n    # Test with a Series containing a mix of numeric and string values\n    s = pd.Series([1, 2, \"3\", np.nan], dtype=object)\n    assert variable_type(s) == \"categorical\"\n\n    # Test with a Series containing a mix of datetime and numeric values\n    s = pd.Series([pd.Timestamp(1), 2, np.nan], dtype=object)\n    assert variable_type(s) == \"categorical\"\n\n    # Test with a Series containing a mix of numeric and datetime values\n    s = pd.Series([1, pd.Timestamp(2), np.nan], dtype=object)\n    assert variable_type(s) == \"categorical\"\n\n    # Test with an empty Series\n    s = pd.Series([], dtype=object)\n    assert variable_type(s) == \"categorical\"\n\n    # Test with a Series containing a single value\n    s = pd.Series([1], dtype=object)\n    assert variable_type(s) == \"numeric\"\n\n    # Test with a Series containing a single datetime value\n    s = pd.Series([pd.Timestamp(1)], dtype=object)\n    assert variable_type(s) == \"datetime\"\n\n    # Test with a Series containing a single string value\n    s = pd.Series([\"a\"], dtype=object)\n    assert variable_type(s) == \"categorical\"\n\n    # Test with a Series containing a single NaN value\n    s = pd.Series([np.nan], dtype=object)\n    assert variable_type(s) == \"numeric\"\n\n    # Test with a Series containing a single NA value\n    s = pd.Series([pd.NA], dtype=object)\n    assert variable_type(s) == \"numeric\"\n\n    # Test with a Series containing a single None value\n    s = pd.Series([None], dtype=object)\n    assert variable_type(s) == \"categorical\"\n", "def test_variable_type_edge_cases():\n\n    # Test that a datetime object is considered datetime\n    s = pd.Series([datetime(2022, 1, 1)])\n    assert variable_type(s) == \"datetime\"\n\n    # Test that a numpy datetime object is considered datetime\n    s = pd.Series([np.datetime64('2022-01-01')])\n    assert variable_type(s) == \"datetime\"\n\n    # Test that a collection of datetime objects is considered datetime\n    s = pd.Series([datetime(2022, 1, 1), datetime(2022, 1, 2)])\n    assert variable_type(s) == \"datetime\"\n\n    # Test that a collection of numpy datetime objects is considered datetime\n    s = pd.Series([np.datetime64('2022-01-01'), np.datetime64('2022-01-02')])\n    assert variable_type(s) == \"datetime\"\n\n    # Test that a collection of mixed datetime and numpy datetime objects is considered datetime\n    s = pd.Series([datetime(2022, 1, 1), np.datetime64('2022-01-02')])\n    assert variable_type(s) == \"datetime\"\n\n    # Test that a collection of numbers and non-numbers is considered categorical\n    s = pd.Series([1, 2, \"a\"])\n    assert variable_type(s) == \"categorical\"\n\n    # Test that a collection of numbers and NaN is considered numeric\n    s = pd.Series([1, 2, np.nan])\n    assert variable_type(s) == \"numeric\"\n\n    # Test that a collection of datetimes and NaN is considered datetime\n    s = pd.Series([datetime(2022, 1, 1), np.nan])\n    assert variable_type(s) == \"datetime\"\n\n    # Test that a collection of booleans and NaN is considered numeric by default\n    s = pd.Series([True, False, np.nan])\n    assert variable_type(s) == \"numeric\"\n\n    # Test that a collection of booleans and NaN is considered boolean when strict_boolean=False\n    s = pd.Series([True, False, np.nan])\n    assert variable_type(s, boolean_type=\"boolean\") == \"boolean\"\n\n    # Test that a collection of booleans and NaN is considered numeric when strict_boolean=True\n    s = pd.Series([True, False, np.nan])\n    assert variable_type(s, boolean_type=\"boolean\", strict_boolean=True) == \"numeric\"\n", "def test_variable_type_edge_cases():\n\n    # Test mixed-type Series\n    s = pd.Series([1, \"a\", np.nan])\n    assert variable_type(s) == \"categorical\"\n\n    # Test datetime Series with different dtypes\n    s = pd.Series([pd.Timestamp(1), pd.Timestamp(2)], dtype=object)\n    assert variable_type(s) == \"datetime\"\n\n    s = pd.Series([np.datetime64(1), np.datetime64(2)])\n    assert variable_type(s) == \"datetime\"\n\n    # Test Series with all unique values\n    s = pd.Series(range(1000))\n    assert variable_type(s) == \"numeric\"\n\n    # Test Series with all unique values and nans\n    s = pd.Series(range(1000)).append(pd.Series([np.nan]))\n    assert variable_type(s) == \"numeric\"\n\n    # Test Series with all nans and a single non-null value\n    s = pd.Series([np.nan]*1000).append(pd.Series([1]))\n    assert variable_type(s) == \"numeric\"\n\n    # Test empty Series\n    s = pd.Series([])\n    assert variable_type(s) == \"numeric\"\n\n    # Test categorical Series with empty categories\n    s = pd.Series([], dtype=\"category\")\n    assert variable_type(s) == \"categorical\"\n", "def test_variable_type_datetime_string():\n\n    s = pd.Series([\"2022-01-01\", \"2022-01-02\"])\n    assert variable_type(s) == \"categorical\"\n\n    s = pd.Series(pd.to_datetime(s))\n    assert variable_type(s) == \"datetime\"\n\n    s = pd.Series([datetime(2022, 1, 1), datetime(2022, 1, 2)])\n    assert variable_type(s) == \"datetime\"\n\n    s = pd.Series([np.datetime64(\"2022-01-01\"), np.datetime64(\"2022-01-02\")])\n    assert variable_type(s) == \"datetime\"\n\n    s = pd.Series([pd.Timestamp(2022, 1, 1), pd.Timestamp(2022, 1, 2)])\n    assert variable_type(s) == \"datetime\"\n\n    s = pd.Series([datetime(2022, 1, 1), pd.NA])\n    assert variable_type(s) == \"datetime\"\n\n    s = pd.Series([np.datetime64(\"2022-01-01\"), pd.NA])\n    assert variable_type(s) == \"datetime\"\n\n    s = pd.Series([pd.Timestamp(2022, 1, 1), pd.NA])\n    assert variable_type(s) == \"datetime\"\n\n", "def test_variable_type_edge_cases():\n\n    # Test with a list of numbers as input\n    assert variable_type([1, 2, 3]) == \"numeric\"\n\n    # Test with a numpy array as input\n    assert variable_type(np.array([1, 2, 3])) == \"numeric\"\n\n    # Test with a pandas Series of datetime objects\n    s = pd.Series([datetime(2022, 1, 1), datetime(2022, 1, 2)])\n    assert variable_type(s) == \"datetime\"\n\n    # Test with a pandas Series of mixed data types\n    s = pd.Series([1, \"a\", 2.0, None])\n    assert variable_type(s) == \"categorical\"\n\n    # Test with a pandas Series of all null values\n    s = pd.Series([None, None, None])\n    assert variable_type(s) == \"numeric\"\n\n    # Test with an empty pandas Series\n    s = pd.Series()\n    assert variable_type(s) == \"numeric\"\n\n    # Test with a pandas Series of NaN values\n    s = pd.Series([np.nan, np.nan, np.nan])\n    assert variable_type(s) == \"numeric\"\n\n    # Test with a pandas Series of NA values\n    s = pd.Series([pd.NA, pd.NA, pd.NA])\n    assert variable_type(s) == \"numeric\"\n\n    # Test with a pandas Series of boolean values with object dtype\n    s = pd.Series([True, False, True], dtype=object)\n    assert variable_type(s) == \"numeric\"\n\n    # Test with a pandas Series of boolean values with bool dtype\n    s = pd.Series([True, False, True], dtype=bool)\n    assert variable_type(s) == \"boolean\"\n\n    # Test with a pandas Series of boolean values with BooleanDtype dtype\n    s = pd.Series([True, False, True], dtype=\"boolean\")\n    assert variable_type(s) == \"boolean\"\n\n    # Test with a pandas Series of boolean values with strict_boolean=True\n    s = pd.Series([True, False, True], dtype=bool)\n    assert variable_type(s, boolean_type=\"numeric\", strict_boolean=True) == \"boolean\"\n\n    # Test with a pandas Series of boolean values with boolean_type=\"categorical\"\n    s = pd.Series([True, False, True], dtype=bool)\n    assert variable_type(s, boolean_type=\"categorical\") == \"categorical\"\n\n    #"], "sample_470": ["    def test_partition(self):\n        \"\"\"Test the partition function splits values correctly.\"\"\"\n        is_even = lambda x: x % 2 == 0\n        numbers = [1, 2, 3, 4, 5, 6]\n        even, odd = partition(is_even, numbers)\n        self.assertEqual(even, [2, 4, 6])\n        self.assertEqual(odd, [1, 3, 5])\n", "    def test_simple_lazy_object(self):\n        class MyClass:\n                self.value = 42\n\n                return self.value\n\n            return MyClass()\n\n        lazy_obj = SimpleLazyObject(create_instance)\n        self.assertIsNone(lazy_obj._wrapped)\n        self.assertEqual(lazy_obj.method(), 42)\n        self.assertIsNotNone(lazy_obj._wrapped)\n        self.assertEqual(lazy_obj.value, 42)\n", "def test_lazy_object_pickling(self):\n    \"\"\"lazy objects can be pickled.\"\"\"\n    lazy_obj = lazy(lambda: \"test\", str)()\n    pickled_obj = self.serializer.dumps(lazy_obj)\n    loaded_obj = self.serializer.loads(pickled_obj)\n    self.assertEqual(str(lazy_obj), str(loaded_obj))\n", "def test_simple_lazy_object(self):\n    \"\"\"\n    Test SimpleLazyObject class.\n    \"\"\"\n        return \"Lazy Init\"\n\n    lazy_object = SimpleLazyObject(lazy_init)\n    self.assertEqual(str(lazy_object), \"<SimpleLazyObject: <function lazy_init at 0x...>>\")\n\n    lazy_object._setup()\n    self.assertEqual(str(lazy_object), \"Lazy Init\")\n\n    lazy_object._setup()\n    self.assertEqual(str(lazy_object), \"Lazy Init\")\n\n    lazy_object_copy = copy.copy(lazy_object)\n    self.assertEqual(str(lazy_object_copy), \"Lazy Init\")\n\n    lazy_object_deepcopy = copy.deepcopy(lazy_object)\n    self.assertEqual(str(lazy_object_deepcopy), \"Lazy Init\")\n\n    lazy_object_dir = dir(lazy_object)\n    self.assertIn(\"__class__\", lazy_object_dir)\n    self.assertIn(\"__str__\", lazy_object_dir)\n    self.assertIn(\"__repr__\", lazy_object_dir)\n\n    lazy_object_hash = hash(lazy_object)\n    self.assertIsInstance(lazy_object_hash, int)\n\n    lazy_object_bool = bool(lazy_object)\n    self.assertIsInstance(lazy_object_bool, bool)\n\n    lazy_object_eq = lazy_object == lazy_object_copy\n    self.assertIsInstance(lazy_object_eq, bool)\n\n    lazy_object_lt = lazy_object < lazy_object_copy\n    self.assertIsInstance(lazy_object_lt, bool)\n\n    lazy_object_gt = lazy_object > lazy_object_copy\n    self.assertIsInstance(lazy_object_gt, bool)\n\n    lazy_object_ne = lazy_object != lazy_object_copy\n    self.assertIsInstance(lazy_object_ne, bool)\n\n    lazy_object_getitem = lazy_object[0]\n    self.assertEqual(lazy_object_getitem, \"L\")\n\n    lazy_object_setitem = lazy_object.__setitem__(0, \"x\")\n    self.assertEqual(lazy_object[0], \"x\")\n\n    lazy_object_delitem = lazy_object.__delitem__(0)\n    self.assertEqual(lazy_object, \"azy Init\")\n\n    lazy_object_iter = iter(lazy_object)\n    self.assertIsInstance(lazy_object_iter, iterator)\n\n    lazy_object_len = len(lazy_object)\n    self.assertIsInstance(lazy_object_len, int)\n\n    lazy_object_contains = \"x\" in lazy_object\n    self.assertIsInstance(lazy_object_contains, bool)\n", "def test_lazy_object(self):\n    \"\"\"LazyObject works as expected.\"\"\"\n    class MyObject:\n            self.value = value\n\n            return self.value == other.value\n\n            return str(self.value)\n\n            return hash(self.value)\n\n    lazy_obj = SimpleLazyObject(lambda: MyObject(42))\n\n    # Test that lazy object isn't initialized on creation.\n    self.assertIsNone(lazy_obj._wrapped)\n\n    # Test that lazy object is initialized on attribute access.\n    self.assertEqual(lazy_obj.value, 42)\n    self.assertIsNotNone(lazy_obj._wrapped)\n\n    # Test that lazy object is initialized only once.\n    lazy_obj.value = 24\n    self.assertEqual(lazy_obj.value, 24)\n    self.assertEqual(lazy_obj._wrapped.value, 24)\n\n    # Test equality.\n    self.assertEqual(lazy_obj, MyObject(24))\n    self.assertNotEqual(lazy_obj, MyObject(42))\n\n    # Test hash.\n    self.assertEqual(hash(lazy_obj), hash(MyObject(24)))\n\n    # Test string representation.\n    self.assertEqual(str(lazy_obj), \"24\")\n\n    # Test copy.\n    copied_obj = copy.copy(lazy_obj)\n    self.assertEqual(copied_obj.value, 24)\n    self.assertIsNot(copied_obj, lazy_obj)\n    self.assertIsNot(copied_obj._wrapped, lazy_obj._wrapped)\n\n    # Test deep copy.\n    deep_copied_obj = copy.deepcopy(lazy_obj)\n    self.assertEqual(deep_copied_obj.value, 24)\n    self.assertIsNot(deep_copied_obj, lazy_obj)\n    self.assertIsNot(deep_copied_obj._wrapped, lazy_obj._wrapped)\n\n    # Test dir.\n    self.assertIn(\"value\", dir(lazy_obj))\n\n    # Test getitem.\n    lazy_list = SimpleLazyObject(lambda: [1, 2, 3])\n    self.assertEqual(lazy_list[0], 1)\n\n    # Test setitem.\n    lazy_list[0] = 42\n    self.assertEqual(lazy_list[0], 42)\n\n    # Test delitem.\n    del lazy_list[0]\n    self.assertEqual(lazy_list, [2, 3])\n\n    # Test contains.\n    self.assertIn(2, lazy_list)\n\n    # Test len.\n    self.assertEqual(len(lazy_list), 2)\n\n    #"], "sample_1039": ["def test_print_imaginary_unit():\n    assert mp.doprint(I) == '<imaginaryi/>'\n    assert mpp.doprint(I) == '<mi>&ImaginaryI;</mi>'\n    assert mp.doprint(-I) == '<apply><minus/><imaginaryi/></apply>'\n    assert mpp.doprint(-I) == '<mrow><mo>-</mo><mi>&ImaginaryI;</mi></mrow>'\n", "def test_print_random_symbol_with_params():\n    R = RandomSymbol(Symbol('R'), Integer(1), Integer(5))\n    assert mpp.doprint(R) == '<mi>R</mi>'\n    assert mp.doprint(R) == '<ci>R</ci>'\n", "def test_presentation_mathml_matrices_symbols():\n    A = Matrix([[x, y], [x*y, y**2]])\n    B = MatrixSymbol('B', 2, 3)\n    mll_1 = mpp._print(A)\n    assert mll_1.childNodes[0].nodeName == 'mtable'\n    assert mll_1.childNodes[0].childNodes[0].nodeName == 'mtr'\n    assert len(mll_1.childNodes[0].childNodes) == 2\n    assert mll_1.childNodes[0].childNodes[0].childNodes[0].nodeName == 'mtd'\n    assert len(mll_1.childNodes[0].childNodes[0].childNodes) == 2\n    assert mll_1.childNodes[0].childNodes[0].childNodes[0].childNodes[0].nodeName == 'mi'\n    assert mll_1.childNodes[0].childNodes[0].childNodes[0].childNodes[0].childNodes[0].nodeValue == 'x'\n    assert mll_1.childNodes[0].childNodes[0].childNodes[1].childNodes[0].nodeName == 'mi'\n    assert mll_1.childNodes[0].childNodes[0].childNodes[1].childNodes[0].childNodes[0].nodeValue == 'y'\n    assert mll_1.childNodes[0].childNodes[1].childNodes[0].nodeName == 'mtd'\n    assert len(mll_1.childNodes[0].childNodes[1].childNodes) == 2\n    assert mll_1.childNodes[0].childNodes[1].childNodes[0].childNodes[0].nodeName == 'mi'\n    assert mll_1.childNodes[0].childNodes[1].childNodes[0].childNodes[0].childNodes[0].nodeValue == 'x'\n    assert mll_1.childNodes[0].childNodes[1].childNodes[0].childNodes[1].nodeName == 'mo'\n    assert mll_1.childNodes[0].childNodes[1].childNodes[0].childNodes[1].childNodes[0].nodeValue == '*'\n    assert mll_1.childNodes[0].childNodes[1].childNodes[0].childNodes[2].nodeName == 'mi'\n    assert mll_1.childNodes[0].childNodes[1].childNodes[0].childNodes[2].childNodes[0].nodeValue == 'y'\n    assert mll_1.childNodes[0].childNodes[1].childNodes[1].childNodes[0].nodeName == '", "def test_mathml_printer_with_settings():\n    mp = MathMLPrinter({'long_frac_ratio': 1})\n    mml_1 = mp._print(Rational(2, 5))\n    assert mml_1.childNodes[0].nodeName == 'cn'\n\n    mml_2 = mp._print(Rational(10, 5))\n    assert mml_2.childNodes[0].nodeName == 'divide'\n", "def test_presentation_mathml_matrices_symbol():\n    A = MatrixSymbol('A', 2, 3)\n    mll_1 = mpp._print(A)\n    assert mll_1.nodeName == 'mi'\n    assert mll_1.childNodes[0].nodeValue == 'A'\n    del mll_1\n\n    mll_2 = mp._print(A)\n    assert mll_2.nodeName == 'ci'\n    assert mll_2.childNodes[0].nodeValue == 'A'\n    del mll_2\n\n    A = MatrixSymbol('A_1_2', 1, 1)\n    mll_1 = mpp._print(A)\n    assert mll_1.nodeName == 'msub'\n    assert mll_1.childNodes[0].nodeName == 'mi'\n    assert mll_1.childNodes[0].childNodes[0].nodeValue == 'A'\n    assert mll_1.childNodes[1].nodeName == 'mrow'\n    assert mll_1.childNodes[1].childNodes[0].nodeName == 'mi'\n    assert mll_1.childNodes[1].childNodes[0].childNodes[0].nodeValue == '1'\n    assert mll_1.childNodes[1].childNodes[1].nodeName == 'mo'\n    assert mll_1.childNodes[1].childNodes[1].childNodes[0].nodeValue == ' '\n    assert mll_1.childNodes[1].childNodes[2].nodeName == 'mi'\n    assert mll_1.childNodes[1].childNodes[2].childNodes[0].nodeValue == '2'\n    del mll_1\n\n    mll_2 = mp._print(A)\n    assert mll_2.nodeName == 'ci'\n    assert mll_2.childNodes[0].nodeName == 'mml:msub'\n    assert mll_2.childNodes[0].childNodes[0].nodeName == 'mml:mi'\n    assert mll_2.childNodes[0].childNodes[0].childNodes[0].nodeValue == 'A'\n    assert mll_2.childNodes[0].childNodes[1].nodeName == 'mml:mrow'\n    assert mll_2.childNodes[0].childNodes[1].childNodes[0].nodeName == 'mml:mi'\n    assert mll_2.childNodes[0].childNodes[1].childNodes[0].childNodes[\n        0].nodeValue == '1'\n    assert mll_2.childNodes[0]."], "sample_145": ["    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            'admin.E034'\n        )\n", "    def test_not_callable_or_boolean(self):\n        class TestModelAdmin(ModelAdmin):\n            view_on_site = 'not callable nor boolean'\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'view_on_site' must be a callable or a boolean value.\",\n            'admin.E025'\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            exclude = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'exclude' must be a list or tuple.\",\n            'admin.E014'\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            'admin.E034'\n        )\n", "    def test_ordering_with_negative_string(self):\n        class TestModelAdmin(ModelAdmin):\n            ordering = ('-non_existent_field',)\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'ordering[0]' refers to 'non_existent_field', \"\n            \"which is not an attribute of 'modeladmin.ValidationTestModel'.\",\n            'admin.E033'\n        )\n"], "sample_99": ["    def test_trunc_with_unaware_datetime(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        self.create_model(start_datetime, end_datetime)\n        self.create_model(end_datetime, start_datetime)\n\n            self.assertQuerysetEqual(\n                DTModel.objects.annotate(\n                    truncated=Trunc('start_datetime', kind, output_field=DateTimeField())\n                ).order_by('start_datetime'),\n                [\n                    (start_datetime, truncate_to(start_datetime, kind)),\n                    (end_datetime, truncate_to(end_datetime, kind))\n                ],\n                lambda m: (m.start_datetime, m.truncated)\n            )\n\n            self.assertQuerysetEqual(\n                DTModel.objects.annotate(\n                    truncated=Trunc('start_date', kind, output_field=DateField())\n                ).order_by('start_datetime'),\n                [\n                    (start_datetime, truncate_to(start_datetime.date(), kind)),\n                    (end_datetime, truncate_to(end_datetime.date(), kind))\n                ],\n                lambda m: (m.start_datetime, m.truncated)\n            )\n\n            self.assertQuerysetEqual(\n                DTModel.objects.annotate(\n                    truncated=Trunc('start_time', kind, output_field=TimeField())\n                ).order_by('start_datetime'),\n                [\n                    (start_datetime, truncate_to(start_datetime.time(), kind)),\n                    (end_datetime, truncate_to(end_datetime.time(), kind))\n                ],\n                lambda m: (m.start_datetime, m.truncated)\n            )\n\n        test_date_kind('year')\n        test_date_kind('quarter')\n        test_date_kind('month')\n        test_date_kind('week')\n        test_date_kind('day')\n        test_time_kind('hour')\n        test_time_kind('minute')\n        test_time_kind('second')\n        test_datetime_kind('year')\n        test_datetime_kind('quarter')\n        test_datetime_kind('month')\n        test_datetime_kind('week')\n        test_datetime_kind('day')\n        test_datetime_kind('hour')\n        test_datetime_kind('minute')\n        test_datetime_kind('second')\n", "def test_now_func(self):\n    start_datetime = datetime(2022, 6, 15, 14, 30, 50, 321)\n    end_datetime = datetime(2022, 6, 15, 14, 10, 50, 123)\n    if settings.USE_TZ:\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n    self.create_model(end_datetime, start_datetime)\n\n    msg = 'output_field must be a DateTimeField.'\n    with self.assertRaisesMessage(ValueError, msg):\n        list(DTModel.objects.annotate(truncated=Now(output_field=IntegerField())))\n\n    with self.assertRaisesMessage(AssertionError, \"'name' isn't a DateTimeField\"):\n        list(DTModel.objects.annotate(truncated=Now('name')))\n\n    self.assertQuerysetEqual(\n        DTModel.objects.annotate(now=Now()).order_by('start_datetime'),\n        [\n            (start_datetime, datetime.now()),\n            (end_datetime, datetime.now()),\n        ],\n        lambda m: (m.start_datetime, m.now)\n    )\n", "    def test_trunc_timezone_transition(self):\n        sao = pytz.timezone('America/Sao_Paulo')\n        start_datetime = datetime(2015, 10, 17, 23, 30, 50, 321)\n        end_datetime = datetime(2016, 2, 20, 13, 11, 27, 123)\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n\n        with timezone.override(sao):\n            model = DTModel.objects.annotate(\n                truncated_start=TruncMinute('start_datetime'),\n                truncated_end=TruncMinute('end_datetime'),\n            ).get()\n\n            self.assertEqual(model.truncated_start.tzinfo.utcoffset(model.truncated_start), timedelta(hours=-3))\n            self.assertEqual(model.truncated_end.tzinfo.utcoffset(model.truncated_end), timedelta(hours=-2))\n", "    def test_trunc_timezone_applied_before_truncation_for_truncweek(self):\n        start_datetime = datetime(2016, 1, 1, 1, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n\n        sao = pytz.timezone('America/Sao_Paulo')\n\n        # TruncWeek for dates in the southern hemisphere where the first Monday of the year is in the previous year.\n        model = DTModel.objects.annotate(\n            week_truncated=TruncWeek('start_datetime', tzinfo=sao),\n        ).order_by('start_datetime').get()\n\n        self.assertEqual(model.start_datetime, start_datetime)\n        self.assertEqual(model.week_truncated, truncate_to(start_datetime.astimezone(sao), 'week', sao))\n        self.assertEqual(model.start_datetime.year, 2016)\n        self.assertEqual(model.week_truncated.year, 2015)\n", "    def test_trunc_postgresql_timezone_awareness(self):\n        # PostgreSQL only\n        if not connection.vendor == 'postgresql':\n            self.skipTest('PostgreSQL only')\n\n        if not connection.features.has_native_timezone_support:\n            self.skipTest('Requires PostgreSQL with native timezone support')\n\n        # The value is selected to be the same in both UTC and Australia/Melbourne.\n        # There's a 1 hour difference in their offsets during DST.\n        melb = pytz.timezone('Australia/Melbourne')\n        utc = pytz.timezone('UTC')\n        # Select a date during DST in Australia/Melbourne, but not in UTC.\n        start_datetime = utc.localize(datetime(2016, 10, 1, 1, 0, 0))\n        end_datetime = utc.localize(datetime(2016, 10, 1, 13, 0, 0))\n        start_melb = start_datetime.astimezone(melb)\n        end_melb = end_datetime.astimezone(melb)\n        self.create_model(start_datetime, end_datetime)\n        self.create_model(end_datetime, start_datetime)\n\n        # 'trunc' should work correctly even if the field's timezone is different.\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated_start=Trunc('start_datetime', 'hour', output_field=DateTimeField(), tzinfo=melb),\n                truncated_end=Trunc('end_datetime', 'hour', output_field=DateTimeField(), tzinfo=melb),\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_melb, 'hour', melb)),\n                (end_datetime, truncate_to(end_melb, 'hour', melb)),\n            ],\n            lambda m: (m.start_datetime, m.truncated_start, m.truncated_end)\n        )\n\n        # 'trunc' should also work correctly if the server timezone is different.\n        with timezone.override(melb):\n            self.assertQuerysetEqual(\n                DTModel.objects.annotate(\n                    truncated_start=Trunc('start_datetime', 'hour', output_field=DateTimeField()),\n                    truncated_end=Trunc('end_datetime', 'hour', output_field=DateTimeField()),\n                ).order_by('start_datetime'),\n                [\n                    (start_datetime, truncate_to(start_melb, 'hour', melb)),\n                    (end_datetime, truncate_to(end_melb, 'hour', melb)),\n               "], "sample_67": ["    def test_model_form_metaclass_inheritance(self):\n        class ModelFormSubclass(forms.ModelForm, metaclass=ModelFormMetaclass):\n            pass\n\n        class SubclassMeta(ModelFormSubclass):\n            pass\n\n        self.assertEqual(type(SubclassMeta), ModelFormMetaclass)\n", "    def test_base_model_formset_instantiation(self):\n        class MyModelFormSet(BaseModelFormSet):\n            pass\n\n        with self.assertRaises(ImproperlyConfigured):\n            MyModelFormSet()\n\n        class MyModelFormSet(BaseModelFormSet):\n            model = Category\n\n        with self.assertRaises(ImproperlyConfigured):\n            MyModelFormSet()\n\n        class MyModelFormSet(BaseModelFormSet):\n            model = Category\n            form = BaseCategoryForm\n\n        MyModelFormSet()\n", "    def test_add_fields(self):\n        class CategoryForm(forms.ModelForm):\n            class Meta:\n                model = Category\n                fields = '__all__'\n\n        formset = modelformset_factory(Category, form=CategoryForm, extra=1)\n        form = formset.empty_form\n        formset.add_fields(form, None)\n        self.assertEqual(list(form.fields), ['name', 'slug', 'url', 'id'])\n", "    def test_base_modelformset(self):\n        class CategoryForm(forms.ModelForm):\n            class Meta:\n                model = Category\n                fields = '__all__'\n\n        class CategoryFormSet(BaseModelFormSet):\n            model = Category\n\n        formset = CategoryFormSet()\n\n        self.assertIsInstance(formset, forms.BaseFormSet)\n        self.assertEqual(formset.model, Category)\n", "    def test_save_empty_data(self):\n        # Saving with empty data should raise a ValueError\n        form = ProductForm(data={})\n        self.assertFalse(form.is_valid())\n        msg = \"The Product could not be created because the data didn't validate.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            form.save()\n"], "sample_1113": ["def test_block_collapse():\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n    Z = MatrixSymbol('Z', 2, 2)\n    B = BlockMatrix([[X, Z], [ZeroMatrix(2, 2), Y]])\n    C = BlockMatrix([[Identity(2), Z]])\n    assert block_collapse(C*B) == BlockMatrix([[X, Z + Z*Y]])\n", "def test_block_collapse():\n    from sympy import symbols, MatrixSymbol, Identity, ZeroMatrix, block_collapse\n    n, m, l = symbols('n m l', positive=True)\n    X = MatrixSymbol('X', n, n)\n    Y = MatrixSymbol('Y', m, m)\n    Z = MatrixSymbol('Z', n, m)\n    B = BlockMatrix([[X, Z], [ZeroMatrix(m, n), Y]])\n    C = BlockMatrix([[Identity(n), Z]])\n    assert block_collapse(C*B) == BlockMatrix([[X, Z + Z*Y]])\n", "def test_block_diagonal_matrix_inverse():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 3, 3)\n    M = BlockDiagMatrix(A, B)\n    M_inv = M.inv()\n    assert M_inv.diag == [A.inv(), B.inv()]\n    assert M_inv.blocks == ImmutableDenseMatrix([\n        [A.inv(), ZeroMatrix(2, 3)],\n        [ZeroMatrix(3, 2), B.inv()]\n    ])\n", "def test_block_collapse():\n    # Test block_collapse with BlockDiagMatrix\n    X = MatrixSymbol('X', n, n)\n    Y = MatrixSymbol('Y', m ,m)\n    B = BlockDiagMatrix(X, Y)\n    C = BlockDiagMatrix(Y, X)\n    assert block_collapse(B*C) == BlockDiagMatrix(X*Y, Y*X)\n\n    # Test block_collapse with BlockMatrix\n    Z = MatrixSymbol('Z', n, m)\n    B = BlockMatrix([[X, Z], [ZeroMatrix(m,n), Y]])\n    C = BlockMatrix([[Identity(n), Z]])\n    assert block_collapse(C*B) == BlockMatrix([[X, Z + Z*Y]])\n    assert block_collapse(B*C) == BlockMatrix([[X*Identity(n) + Z*Z, X*Z + Z*Y], [Y*Z, Y*Z*Y]])\n\n    # Test block_collapse with mixed block sizes\n    A1 = MatrixSymbol('A1', n, k)\n    A2 = MatrixSymbol('A2', n, l)\n    A3 = MatrixSymbol('A3', m, k)\n    A4 = MatrixSymbol('A4', m, l)\n    B = BlockMatrix([[A1, A2], [A3, A4]])\n    C = BlockMatrix([[Identity(n), A2], [A3, Identity(m)]])\n    assert block_collapse(C*B) == BlockMatrix([[A1, A2 + A2*Identity(m)], [A3 + Identity(n)*A3, A3*A2 + Identity(m)*A4]])\n\n    # Test block_collapse with numeric matrices\n    M1 = ImmutableMatrix([[1, 2], [3, 4]])\n    M2 = ImmutableMatrix([[5, 6], [7, 8]])\n    B = BlockMatrix([[M1, M2]])\n    C = BlockMatrix([[M1]])\n    assert block_collapse(C*B) == BlockMatrix([[M1**2, M1*M2]])\n\n    # Test block_collapse with transpose\n    X = MatrixSymbol('X', n, n)\n    Y = MatrixSymbol('Y', n, n)\n    B = BlockMatrix([[X, Y], [Y, X]])\n    assert block_collapse(B.T*B) == BlockMatrix([[X.T*X + Y.T*Y, X.T*", "def test_block_collapse():\n    X = MatrixSymbol('X', n, m)\n    Y = MatrixSymbol('Y', m, n)\n    B = BlockMatrix([[Identity(n), X], [ZeroMatrix(m, n), Y]])\n    assert block_collapse(B).shape == (n, m + n)\n    assert block_collapse(B*B.T).shape == (n, n)\n\n    B = BlockMatrix([[X, Y], [X, Y]])\n    assert block_collapse(B) == B\n\n    # Test rectangular matrix\n    X = MatrixSymbol('X', n, m)\n    Y = MatrixSymbol('Y', k, n)\n    B = BlockMatrix([[Identity(n), X, ZeroMatrix(n, k)], [ZeroMatrix(m, n), Y, ZeroMatrix(m, k)]])\n    assert block_collapse(B).shape == (n + m, m + k)\n"], "sample_333": ["def test_empty_permitted_with_files(self):\n    class FileForm(Form):\n        file = FileField()\n\n    f = FileForm(files={}, empty_permitted=True)\n    self.assertTrue(f.is_valid())\n    self.assertEqual(f.errors, {})\n    self.assertEqual(f.cleaned_data, {})\n", "def test_baseform_order_fields_with_invalid_field(self):\n    class TestForm(BaseForm):\n        field1 = CharField()\n        field2 = CharField()\n\n    form = TestForm()\n    with self.assertRaisesMessage(ValueError, \"has no field named\"):\n        form.order_fields(['field1', 'field3'])\n", "def test_empty_list_choices(self):\n    class SomeForm(Form):\n        field = ChoiceField(choices=[])\n\n    f = SomeForm()\n    self.assertHTMLEqual(\n        f.as_p(),\n        '<p><label for=\"id_field\">Field:</label> <select name=\"field\" required></select></p>'\n    )\n", "def test_order_fields_with_nested_fields(self):\n    class PersonForm(Form):\n        name = CharField()\n        address = CharField()\n        contact = CharField()\n        order_fields = ['name', 'contact', 'address']\n\n    class UserForm(PersonForm):\n        email = CharField()\n\n    form = UserForm()\n    self.assertEqual(list(form.fields), ['name', 'contact', 'address', 'email'])\n", "def test_invalid_form_initial_data(self):\n    class TestForm(Form):\n        test = CharField()\n\n    form = TestForm(initial={'test': object()})\n    with self.assertRaisesMessage(TypeError, \"Invalid initial data\"):\n        form.full_clean()\n"], "sample_952": ["def test_is_new_type():\n    from typing import NewType\n\n    MyInt = NewType('MyInt', int)\n\n    assert inspect.isNewType(MyInt) is True\n    assert inspect.isNewType(int) is False\n    assert inspect.isNewType('MyInt') is False\n", "def test_isabstractmethod():\n    class AbstractClass:\n            pass\n\n    AbstractClass.abstract_method.__isabstractmethod__ = True\n\n    assert inspect.isabstractmethod(AbstractClass.abstract_method) is True\n\n    class ConcreteClass:\n            pass\n\n    assert inspect.isabstractmethod(ConcreteClass.concrete_method) is False\n", "def test_isabstractmethod():\n    class AbstractClass:\n            pass\n\n        abstract_method.__isabstractmethod__ = True\n\n    class ConcreteClass:\n            pass\n\n    abstract_obj = AbstractClass()\n    concrete_obj = ConcreteClass()\n\n    assert inspect.isabstractmethod(AbstractClass.abstract_method) is True\n    assert inspect.isabstractmethod(abstract_obj.abstract_method) is True\n    assert inspect.isabstractmethod(ConcreteClass.concrete_method) is False\n    assert inspect.isabstractmethod(concrete_obj.concrete_method) is False\n", "def test_isabstractmethod():\n    class AbstractBaseClass:\n            pass\n\n            pass\n\n        abstract_method.__isabstractmethod__ = True\n\n    class ConcreteSubClass(AbstractBaseClass):\n            pass\n\n    assert inspect.isabstractmethod(AbstractBaseClass.abstract_method) is True\n    assert inspect.isabstractmethod(ConcreteSubClass.abstract_method) is False\n    assert inspect.isabstractmethod(AbstractBaseClass.__init__) is False\n    assert inspect.isabstractmethod(ConcreteSubClass.__init__) is False\n", "def test_isabstractmethod():\n    class Foo:\n            pass\n\n            raise NotImplementedError\n\n            raise Exception\n\n    class Bar(Foo):\n            pass\n\n    assert inspect.isabstractmethod(Foo.meth1) is False\n    assert inspect.isabstractmethod(Foo.meth2) is True\n    assert inspect.isabstractmethod(Foo.meth3) is False\n    assert inspect.isabstractmethod(Bar.meth2) is False\n"], "sample_836": ["def test_check_partial_fit_first_call():\n    class MockEstimator:\n            pass\n\n    estimator = MockEstimator()\n\n    # Test first call with classes provided\n    assert _check_partial_fit_first_call(estimator, classes=[1, 2, 3])\n\n    # Test first call without classes provided\n    assert_raises(ValueError, _check_partial_fit_first_call, estimator)\n\n    # Test subsequent call with consistent classes\n    estimator.classes_ = np.array([1, 2, 3])\n    _check_partial_fit_first_call(estimator, classes=[1, 2, 3])\n\n    # Test subsequent call with inconsistent classes\n    assert_raises(ValueError, _check_partial_fit_first_call, estimator,\n                 classes=[1, 2, 4])\n\n    # Test subsequent call without classes provided\n    _check_partial_fit_first_call(estimator)\n", "def test_check_partial_fit_first_call():\n    class MockClassifier:\n        pass\n\n    clf = MockClassifier()\n    classes = [0, 1, 2]\n\n    # Test first call to partial_fit with classes\n    assert _check_partial_fit_first_call(clf, classes)\n    assert hasattr(clf, 'classes_')\n    assert_array_equal(clf.classes_, unique_labels(classes))\n\n    # Test subsequent calls to partial_fit with classes\n    assert not _check_partial_fit_first_call(clf, classes)\n    assert_array_equal(clf.classes_, unique_labels(classes))\n\n    # Test call to partial_fit with different classes\n    classes_different = [1, 2, 3]\n    assert_raises(ValueError, _check_partial_fit_first_call, clf, classes_different)\n\n    # Test call to partial_fit without classes\n    assert_raises(ValueError, _check_partial_fit_first_call, clf)\n\n    # Test first call to partial_fit without classes\n    clf = MockClassifier()\n    assert_raises(ValueError, _check_partial_fit_first_call, clf)\n", "def test_check_partial_fit_first_call():\n    # Test _check_partial_fit_first_call with correct input\n    class TestClassifier:\n            self.classes_ = None\n\n    clf = TestClassifier()\n    classes = [1, 2, 3]\n    assert _check_partial_fit_first_call(clf, classes)\n    assert clf.classes_.tolist() == [1, 2, 3]\n    assert not _check_partial_fit_first_call(clf, classes)\n\n    # Test _check_partial_fit_first_call with inconsistent classes\n    classes = [1, 2, 4]\n    assert_raises(ValueError, _check_partial_fit_first_call, clf, classes)\n\n    # Test _check_partial_fit_first_call without classes\n    clf = TestClassifier()\n    assert_raises(ValueError, _check_partial_fit_first_call, clf)\n", "def test_check_partial_fit_first_call():\n    # Test that the function raises an error when classes is not provided\n    # at the first call to partial_fit\n    class DummyClassifier:\n        pass\n\n    clf = DummyClassifier()\n    assert_raises(ValueError, _check_partial_fit_first_call, clf)\n\n    # Test that the function raises an error when classes provided is not\n    # consistent with previous classes\n    clf = DummyClassifier()\n    _check_partial_fit_first_call(clf, classes=[1, 2, 3])\n    assert_raises(ValueError, _check_partial_fit_first_call, clf, classes=[1, 2])\n\n    # Test that the function does not raise an error when classes is provided\n    # and consistent with previous classes\n    clf = DummyClassifier()\n    _check_partial_fit_first_call(clf, classes=[1, 2, 3])\n    assert _check_partial_fit_first_call(clf, classes=[1, 2, 3]) is False\n", "def test_check_partial_fit_first_call():\n    # Test that the function correctly handles the first call to partial_fit\n\n    # Create an instance of a classifier\n    class DummyClassifier:\n            pass\n\n    clf = DummyClassifier()\n\n    # Test that an error is raised when no classes are provided\n    assert_raises(ValueError, _check_partial_fit_first_call, clf)\n\n    # Test that the classes_ attribute is set correctly on the first call\n    classes = [0, 1, 2]\n    assert _check_partial_fit_first_call(clf, classes=classes)\n    assert_array_equal(clf.classes_, unique_labels(classes))\n\n    # Test that an error is raised if the classes are different on a subsequent call\n    classes = [0, 1, 3]\n    assert_raises(ValueError, _check_partial_fit_first_call, clf, classes=classes)\n\n    # Test that the function returns False on a subsequent call with the same classes\n    assert not _check_partial_fit_first_call(clf, classes=[0, 1, 2])\n\n    # Test that the function returns False on a subsequent call with no classes provided\n    assert not _check_partial_fit_first_call(clf)\n"], "sample_960": ["def test_pydecorator_class(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:method:: meth\\n\"\n            \"   .. py:decorator:: deco\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc,\n                                                  addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'meth() (Class method)', 'Class.meth', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"meth\"],\n                                                     [desc_parameterlist, ()])],\n                                   [desc_content, ()]))\n    assert 'Class.meth' in domain.objects\n    assert domain.objects['Class.meth'] == ('index', 'Class.meth', 'method', False)\n    assert_node(doctree[1][1][2], addnodes.index,\n                entries=[('single', 'deco', 'deco', '', None)])\n    assert_node(doctree[1][1][3], ([desc_signature, ([desc_addname, \"@\"],\n                                                     [desc_name, \"deco\"])],\n                                   [desc_content, ()]))\n    assert 'deco' in domain.objects\n    assert domain.objects['deco'] == ('index', 'deco', 'function', False)\n", "def test_note_object_with_canonical(app):\n    domain = app.env.get_domain('py')\n    canonical_name = 'canonical.Class'\n    name = 'Class'\n    node_id = 'node_id'\n    domain.note_object(canonical_name, 'class', node_id, location=None)\n    domain.note_object(name, 'class', node_id, canonical=canonical_name, location=None)\n    assert len(domain.objects) == 2\n    assert domain.objects[canonical_name] == ('index', node_id, 'class', False)\n    assert domain.objects[name] == ('index', node_id, 'class', True)\n", "def test_pymodule_with_synopsis(app):\n    text = (\".. py:module:: example\\n\"\n            \"   :synopsis: This is a test module.\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [nodes.target,\n                           addnodes.index,\n                           [desc, ([desc_signature, ([desc_addname, \"example\"])],\n                                   [desc_content, ()])]]))\n    assert_node(doctree[1], addnodes.index,\n                entries=[('pair', 'module; example', 'module-example', '', None)])\n    assert_node(doctree[2], desc, desctype=\"module\",\n                domain=\"py\", objtype=\"module\", noindex=False)\n    assert_node(doctree[2][0], desc_signature,\n                [desc_addname, \"example\"],\n                domain=\"py\", objtype=\"module\", noindex=False)\n    assert 'example' in domain.modules\n    assert domain.modules['example'] == ('index', 'module-example', 'This is a test module.', '', False)\n\n", "def test_domain_py_canonical_long_names(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'canonical_long_names.html').read_text()\n    assert ('<a class=\"reference internal\" href=\"#very_long_name_for_a_module_a_very_long_name_for_a_class_with_a_very_long_name_for_a_method\" '\n            'title=\"very_long_name_for_a_module_a_very_long_name_for_a_class_with_a_very_long_name_for_a_method\">'\n            '<code class=\"xref py py-meth docutils literal notranslate\">'\n            '<span class=\"pre\">very_long_name_for_a_method</span>'\n            '</code></a>' in content)\n", "def test_python_domain_resolve_xref_unqualified_type_names(app):\n    text = (\".. py:function:: hello(name: int) -> str\\n\"\n            \".. py:data:: age = 30\\n\"\n            \":type: int\")\n    doctree = restructuredtext.parse(app, text)\n    index = app.env.get_domain('py')\n\n    # int -> qualified name\n    node = pending_xref(reftarget=\"int\", refdomain=\"py\", reftype=\"class\")\n    result = index.resolve_xref(app.env, \"index\", app.builder, \"class\", \"int\", node, nodes.Text(\"int\"))\n    assert result is not None\n\n    # int -> unqualified name\n    node = pending_xref(reftarget=\"int\", refdomain=\"py\", reftype=\"class\")\n    app.config.python_use_unqualified_type_names = True\n    result = index.resolve_xref(app.env, \"index\", app.builder, \"class\", \"int\", node, nodes.Text(\"int\"))\n    assert result is not None\n\n    # foo.Name -> qualified name\n    node = pending_xref(reftarget=\"foo.Name\", refdomain=\"py\", reftype=\"class\")\n    result = index.resolve_xref(app.env, \"index\", app.builder, \"class\", \"foo.Name\", node, nodes.Text(\"Name\"))\n    assert result is not None\n\n    # foo.Name -> unqualified name\n    node = pending_xref(reftarget=\"foo.Name\", refdomain=\"py\", reftype=\"class\")\n    app.config.python_use_unqualified_type_names = True\n    result = index.resolve_xref(app.env, \"index\", app.builder, \"class\", \"foo.Name\", node, nodes.Text(\"Name\"))\n    assert result is not None\n\n    # Name -> unqualified name\n    node = pending_xref(reftarget=\"Name\", refdomain=\"py\", reftype=\"class\")\n    app.config.python_use_unqualified_type_names = True\n    result = index.resolve_xref(app.env, \"index\", app.builder, \"class\", \"Name\", node, nodes.Text(\"Name\"))\n    assert result is not None\n\n    # Name -> qualified name\n    node = pending_xref(reftarget=\"Name\", refdomain=\"py\", reftype=\"class\")\n    app.config.python_use_unqualified_type_names = False\n    result = index.resolve_xref(app.env, \"index\", app.builder, \"class\", \"Name\", node, nodes.Text(\"Name"], "sample_86": ["    def test_lazy_hash(self):\n        \"\"\"\n        hash() works correctly for Promises.\n        \"\"\"\n        lazy_a = lazy(lambda: 4, int)\n        lazy_b = lazy(lambda: 4, int)\n        lazy_c = lazy(lambda: 5, int)\n\n        self.assertEqual(hash(lazy_a()), hash(lazy_b()))\n        self.assertNotEqual(hash(lazy_b()), hash(lazy_c()))\n        self.assertEqual(len({lazy_a(), lazy_b()}), 1)\n        self.assertEqual(len({lazy_b(), lazy_c()}), 2)\n", "def test_lazy_object_pickling(self):\n    \"\"\"Test that lazy objects can be pickled and unpickled correctly.\"\"\"\n    import pickle\n    class Klazz:\n            self.value = value\n\n            return self.value == other.value\n\n    obj = lazy(lambda: Klazz(42), Klazz)()\n    pickled_obj = pickle.dumps(obj)\n    unpickled_obj = pickle.loads(pickled_obj)\n    self.assertEqual(unpickled_obj._setupfunc(), obj._setupfunc())\n", "    def test_lazy_hash(self):\n        \"\"\"\n        hash() works correctly for Promises.\n        \"\"\"\n        lazy_a = lazy(lambda: 4, int)\n        lazy_b = lazy(lambda: 4, int)\n        lazy_c = lazy(lambda: 5, int)\n\n        self.assertEqual(hash(lazy_a()), hash(lazy_b()))\n        self.assertNotEqual(hash(lazy_b()), hash(lazy_c()))\n", "    def test_lazy_text(self):\n        \"\"\"lazystr returns a lazy evaluated callable that returns a string.\"\"\"\n        lazy_str = lazystr('Hello, World!')\n        self.assertEqual(lazy_str, 'Hello, World!')\n", "def test_lazy_object(self):\n    \"\"\"Test the lazy object functionality.\"\"\"\n    # Test that the object is not initialized until it's used\n    class LazyObjectTest:\n            self.init_called = False\n            self.init_called = True\n\n    lazy_obj = LazyObjectTest()\n    self.assertFalse(lazy_obj._wrapped.init_called)\n\n    # Test that the object is initialized when an attribute is accessed\n    lazy_obj.__getattr__('__class__')\n    self.assertTrue(lazy_obj._wrapped.init_called)\n\n    # Test that the object is not reinitialized after it's been initialized\n    lazy_obj._wrapped.init_called = False\n    lazy_obj.__getattr__('__class__')\n    self.assertTrue(lazy_obj._wrapped.init_called)\n\n    # Test the __eq__ method\n    lazy_obj2 = LazyObjectTest()\n    lazy_obj2._setup()\n    self.assertNotEqual(lazy_obj, lazy_obj2)\n\n    # Test the __lt__ method\n    self.assertLess(lazy_obj, lazy_obj2)\n\n    # Test the __hash__ method\n    self.assertNotEqual(hash(lazy_obj), hash(lazy_obj2))\n\n    # Test the __mod__ method\n    lazy_obj_str = LazyObjectTest()\n    lazy_obj_str._setup()\n    lazy_obj_str._wrapped = \"Hello\"\n    self.assertEqual(lazy_obj_str % \" World\", \"Hello World\")\n\n    # Test the __deepcopy__ method\n    import copy\n    lazy_obj_copy = copy.deepcopy(lazy_obj)\n    self.assertEqual(lazy_obj._wrapped, lazy_obj_copy._wrapped)\n\n    # Test the __dir__ method\n    self.assertEqual(dir(lazy_obj), dir(lazy_obj._wrapped))\n\n    # Test the __class__ property\n    self.assertEqual(lazy_obj.__class__, LazyObjectTest)\n\n    # Test the __getitem__, __setitem__, and __delitem__ methods\n    lazy_obj_list = LazyObjectTest()\n    lazy_obj_list._setup()\n    lazy_obj_list._wrapped = [1, 2, 3]\n    self.assertEqual(lazy_obj_list[0], 1)\n    lazy_obj_list[0] = 10\n    self.assertEqual(lazy_obj_list._wrapped[0], 10)\n    del lazy_obj_list[0]\n    self.assertEqual(lazy_obj_list._wrapped, [2, 3])\n\n    # Test the __iter__ method\n    self.assertEqual(list(lazy_obj"], "sample_941": ["def test_is_system_TypeVar():\n    T = TypeVar('T')\n    assert is_system_TypeVar(T) is True\n    assert is_system_TypeVar(int) is False\n    assert is_system_TypeVar(str) is False\n", "def test_is_system_TypeVar():\n    T = TypeVar('T')\n    assert is_system_TypeVar(T) is True\n    assert is_system_TypeVar(str) is False\n    assert is_system_TypeVar(int) is False\n    assert is_system_TypeVar(MyInt) is False\n", "def test_restify_type_hints_NewType():\n    assert restify(MyInt) == \":class:`MyInt`\"\n    assert restify(MyInt[int]) == \":class:`MyInt`\\\\ [:class:`int`]\"\n    assert restify(MyInt[str]) == \":class:`MyInt`\\\\ [:class:`str`]\"\n\n", "def test_get_type_hints():\n        return a + b\n\n    type_hints = get_type_hints(func)\n    assert type_hints == {'a': int, 'b': str, 'return': float}\n\n        return a + b\n\n    type_hints = get_type_hints(func_without_return_type)\n    assert type_hints == {'a': int, 'b': str}\n\n    class ClassWithAnnotations:\n            pass\n\n            return c + d\n\n    type_hints = get_type_hints(ClassWithAnnotations.__init__)\n    assert type_hints == {'a': int, 'b': str, 'return': None}\n\n    type_hints = get_type_hints(ClassWithAnnotations.method)\n    assert type_hints == {'c': float, 'd': int, 'return': str}\n\n    # test for broken class\n    class BrokenClass:\n        __annotations__ = None\n\n    type_hints = get_type_hints(BrokenClass)\n    assert type_hints == {}\n", "def test_restify_is_system_TypeVar():\n    T = TypeVar('T')\n    assert restify(T) == \":obj:`tests.test_util_typing.T`\"\n    assert restify(T) == is_system_TypeVar(T)\n"], "sample_71": ["def test_edge_cases(self):\n    # Test edge cases for decimal_pos\n    self.assertEqual(nformat(1234, '.', decimal_pos=0), '1234')\n    self.assertEqual(nformat(1234.2, '.', decimal_pos=0), '1234')\n    self.assertEqual(nformat(1234, '.', decimal_pos=-1), '1234')\n\n    # Test edge cases for grouping\n    self.assertEqual(nformat(1234, '.', grouping=0, thousand_sep=','), '1234')\n    self.assertEqual(nformat(1234, '.', grouping=-1, thousand_sep=','), '1234')\n    self.assertEqual(nformat(1234, '.', grouping=(0, 0), thousand_sep=','), '1234')\n\n    # Test edge cases for force_grouping\n    self.assertEqual(nformat(1234, '.', grouping=3, thousand_sep=',', force_grouping=False), '1234')\n    self.assertEqual(nformat(1234, '.', grouping=3, thousand_sep=',', force_grouping=True), '1,234')\n\n    # Test edge cases for use_l10n\n    with self.settings(USE_THOUSAND_SEPARATOR=True, USE_L10N=True):\n        self.assertEqual(nformat(1234, '.', grouping=3, thousand_sep=',', use_l10n=False), '1234')\n        self.assertEqual(nformat(1234, '.', grouping=3, thousand_sep=',', use_l10n=True), '1,234')\n", "def test_error_cases(self):\n    with self.assertRaises(TypeError):\n        nformat('a', '.')\n    with self.assertRaises(TypeError):\n        nformat(None, '.')\n    with self.assertRaises(ValueError):\n        nformat(Decimal('1234'), '.', decimal_pos=-1)\n    with self.assertRaises(ValueError):\n        nformat(Decimal('1234'), '.', grouping=-1)\n    with self.assertRaises(ValueError):\n        nformat(Decimal('1234'), '.', grouping='abc')\n    with self.assertRaises(ValueError):\n        nformat(Decimal('1234'), '.', decimal_pos='abc')\n", "def test_non_integer_decimal_pos(self):\n    self.assertEqual(nformat(1234, '.', decimal_pos=1.5), '1234.0')\n    self.assertEqual(nformat(1234, '.', decimal_pos=-1), '1234')\n    self.assertEqual(nformat(1234, '.', decimal_pos=0.1), '1234')\n    self.assertEqual(nformat(Decimal('1234'), '.', decimal_pos=1.5), '1234.0')\n    self.assertEqual(nformat(Decimal('1234'), '.', decimal_pos=-1), '1234')\n    self.assertEqual(nformat(Decimal('1234'), '.', decimal_pos=0.1), '1234')\n", "def test_decimal_subclass_edge_cases(self):\n    class EuroDecimal(Decimal):\n        \"\"\"\n        Wrapper for Decimal which prefixes each amount with the \u20ac symbol.\n        \"\"\"\n            amount = super().__format__(specifier, **kwargs)\n            return '\u20ac {}'.format(amount)\n\n    # Test the edge case where the decimal subclass instance is negative.\n    price = EuroDecimal('-1.23')\n    self.assertEqual(nformat(price, ','), '\u20ac -1,23')\n\n    # Test the edge case where the decimal subclass instance is zero.\n    price = EuroDecimal('0')\n    self.assertEqual(nformat(price, ','), '\u20ac 0')\n\n    # Test the edge case where the decimal subclass instance is an integer.\n    price = EuroDecimal('1')\n    self.assertEqual(nformat(price, ','), '\u20ac 1')\n\n    # Test the edge case where the decimal subclass instance has a high number of decimal places.\n    price = EuroDecimal('1.23456789')\n    self.assertEqual(nformat(price, ',', decimal_pos=4), '\u20ac 1,2346')\n", "def test_invalid_inputs(self):\n    with self.assertRaises(TypeError):\n        nformat(None, '.')\n    with self.assertRaises(ValueError):\n        nformat('1234', ',')\n    with self.assertRaises(ValueError):\n        nformat('1234.2', ',')\n    with self.assertRaises(ValueError):\n        nformat(1234, ',')\n    with self.assertRaises(ValueError):\n        nformat(1234.2, ',')\n    with self.assertRaises(ValueError):\n        nformat(Decimal('1234'), ',')\n    with self.assertRaises(ValueError):\n        nformat(Decimal('1234.2'), ',')\n"], "sample_692": ["def test_temp_path_factory_from_config(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n            assert tmp_path_factory.getbasetemp().is_dir()\n    \"\"\"\n    )\n    config = pytest.config.getconfig()\n    tmppath_factory = TempPathFactory.from_config(config, _ispytest=True)\n    assert tmppath_factory.getbasetemp().is_dir()\n", "def test_temp_path_factory_from_config(pytester: Pytester) -> None:\n    \"\"\"Test TempPathFactory.from_config with a pytest Config object.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n            assert tmp_path_factory.getbasetemp().exists()\n    \"\"\"\n    )\n    config = pytester.config\n    tmp_path_factory = TempPathFactory.from_config(config, _ispytest=True)\n    assert tmp_path_factory.getbasetemp().exists()\n", "def test_getbasetemp_subdir_with_existing_file(pytester: Pytester) -> None:\n    \"\"\"Integration test for #4427\"\"\"\n    mytemp = pytester.mkdir(\"mytemp\")\n    file_path = mytemp.joinpath(\"hello.txt\")\n    file_path.touch()\n    pytester.runpytest(\"--basetemp=%s\" % mytemp)\n    assert mytemp.exists()\n    assert file_path.exists()\n", "def test_tmppathfactory_from_config_with_basetemp(tmp_path, monkeypatch: MonkeyPatch) -> None:\n    \"\"\"Test that TempPathFactory.from_config() works correctly when given a basetemp option.\"\"\"\n    monkeypatch.setattr(sys, \"platform\", \"win32\")  # Test the else branch\n    config = cast(Config, FakeConfig(tmp_path))\n    t = TempPathFactory.from_config(config, _ispytest=True)\n    assert t.getbasetemp().resolve() == tmp_path.resolve()\n\n    monkeypatch.setattr(sys, \"platform\", \"linux\")  # Test the if branch\n    t = TempPathFactory.from_config(config, _ispytest=True)\n    assert t.getbasetemp().resolve() == (tmp_path / \"pytest-of-unknown\").resolve()\n", "def test_getbasetemp_without_basetemp_option(tmp_path_factory, monkeypatch: MonkeyPatch) -> None:\n    \"\"\"Test that getbasetemp() works correctly when no basetemp option is provided.\"\"\"\n    monkeypatch.setenv(\"PYTEST_DEBUG_TEMPROOT\", str(tmp_path_factory.mktemp(\"pytest-debug-root\")))\n    tmp_path_factory._basetemp = None\n    tmp_path_factory._given_basetemp = None\n    basetemp = tmp_path_factory.getbasetemp()\n    assert basetemp.exists()\n    assert basetemp.is_dir()\n\n"], "sample_157": ["    def test_destroy_test_db(self):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        test_database_name = creation._get_test_db_name()\n        test_connection.settings_dict['NAME'] = test_database_name\n\n        with mock.patch.object(creation, '_destroy_test_db') as mocked_destroy_test_db:\n            creation.destroy_test_db(old_database_name, verbosity=0, keepdb=False)\n            mocked_destroy_test_db.assert_called_once_with(test_database_name, verbosity=0)\n\n        with mock.patch.object(creation, '_destroy_test_db') as mocked_destroy_test_db:\n            creation.destroy_test_db(old_database_name, verbosity=0, keepdb=True)\n            mocked_destroy_test_db.assert_not_called()\n", "    def test_autoclobber_true(self, mocked_migrate, mocked_ensure_connection):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db') as mocked_create_test_db:\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n                mocked_create_test_db.assert_called_once()\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def test_create_test_db(self):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = True\n        test_connection.settings_dict['TEST']['NAME'] = 'test_db_name'\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db') as mocked_create_test_db:\n                test_database_name = creation.create_test_db(verbosity=1, autoclobber=True, serialize=True, keepdb=False)\n                self.assertEqual(test_database_name, 'test_db_name')\n                mocked_create_test_db.assert_called_once()\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def test_serialize_db_to_string(self, mocked_ensure_connection):\n        # Check that serialize_db_to_string() returns a string.\n        connection.creation._test_serialized_contents = None\n        data = connection.creation.serialize_db_to_string()\n        self.assertIsInstance(data, str)\n        self.assertGreater(len(data), 0)\n", "    def test_database_creation(self):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = True\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n\n        with mock.patch.object(creation, '_execute_create_test_db') as mock_execute, \\\n             mock.patch.object(creation, '_clone_test_db'):\n            creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n            self.assertTrue(mock_execute.called)\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n"], "sample_775": ["def test_large_estimator():\n    # Render a large estimator with many parameters\n    pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True)\n\n    class LargeEstimator(BaseEstimator):\n            for k, v in kwargs.items():\n                setattr(self, k, v)\n\n    params = {f'param_{i}': i for i in range(100)}\n    estimator = LargeEstimator(**params)\n    repr_ = pp.pformat(estimator)\n    assert '...' in repr_\n", "def test_key_val_tuple():\n    # Test rendering of KeyValTuple and KeyValTupleParam\n    pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True)\n    key_val_tuple = KeyValTuple((\"key\", \"value\"))\n    key_val_param = KeyValTupleParam((\"key\", \"value\"))\n\n    expected = \"key: value\"\n    assert pp.pformat(key_val_tuple) == expected\n\n    expected = \"key=value\"\n    assert pp.pformat(key_val_param) == expected\n\n    # Test with long values\n    key_val_tuple = KeyValTuple((\"key\", \"value\" * 100))\n    key_val_param = KeyValTupleParam((\"key\", \"value\" * 100))\n\n    expected = \"key: ...\\n\" + \" \" * 5 + \"value\" * 100\n    assert pp.pformat(key_val_tuple) == expected\n\n    expected = \"key=...\\n\" + \" \" * 5 + \"value\" * 100\n    assert pp.pformat(key_val_param) == expected\n", "def test_estimator_repr_with_long_param_names():\n    # Test that estimators with long parameter names are correctly formatted.\n    class DummyEstimator(BaseEstimator):\n            self.very_long_parameter_name_1 = very_long_parameter_name_1\n            self.very_long_parameter_name_2 = very_long_parameter_name_2\n\n    de = DummyEstimator(very_long_parameter_name_1=10, very_long_parameter_name_2=20)\n    expected = \"\"\"", "def test_format_params_or_dict_items():\n    # Test the _format_params_or_dict_items method directly\n    pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True)\n\n    # Test with a small number of items\n    items = [('a', 1), ('b', 2), ('c', 3)]\n    expected = \"a=1, b=2, c=3\"\n    assert pp._format_params_or_dict_items(items, io.StringIO(), 0, 0, {}, 0, False) is None\n    assert pp.stream.getvalue() == expected\n\n    # Test with a large number of items\n    pp.stream = io.StringIO()\n    items = [('a', 1) for _ in range(50)]\n    expected = \"a=1, a=1, a=1, a=1, a=1, a=1, a=1, a=1, a=1, a=1, a=1, a=1, a=1, a=1, a=1, a=1, a=1, a=1, a=1, a=1, a=1, a=1, a=1, a=1, a=1, a=1, a=1, a=1, a=1, a=1, a=1, a=1, a=1, ..., \"\n    assert pp._format_params_or_dict_items(items, pp.stream, 0, 0, {}, 0, False) is None\n    assert pp.stream.getvalue() == expected\n\n    # Test with a dictionary\n    pp.stream = io.StringIO()\n    items = [('a', 1), ('b', 2), ('c', 3)]\n    expected = \"a: 1, b: 2, c: 3\"\n    assert pp._format_params_or_dict_items(items, pp.stream, 0, 0, {}, 0, True) is None\n    assert pp.stream.getvalue() == expected\n", "def test_max_width_constraint():\n    # When repr exceeds max width, use newline and indent\n    pp = _EstimatorPrettyPrinter(indent=1, width=40, compact=False)\n    pipeline = make_pipeline(StandardScaler(), LogisticRegression(C=999))\n    expected = \"\"\"\\"], "sample_815": ["def test_jaccard_similarity_score_binary():\n    # Binary case\n    y_true = np.array([0, 1, 1, 0])\n    y_pred = np.array([0, 1, 0, 1])\n    assert_almost_equal(jaccard_score(y_true, y_pred), 0.5)\n    assert_warns(DeprecationWarning, jaccard_similarity_score, y_true, y_pred)\n", "def test_balanced_accuracy_score_adjusted():\n    y_true = [0, 1, 1, 0, 0]\n    y_pred = [0, 1, 0, 0, 1]\n\n    unadjusted = balanced_accuracy_score(y_true, y_pred)\n    adjusted = balanced_accuracy_score(y_true, y_pred, adjusted=True)\n\n    assert adjusted < unadjusted\n", "def test_log_loss_pandas_input_empty_class():\n    # case when input is a pandas series and dataframe with empty class gh-12423\n    y_tr = np.array([\"ham\", \"spam\", \"spam\", \"\"])\n    y_pr = np.array([[0.2, 0.7], [0.6, 0.5], [0.4, 0.1], [0.7, 0.2]])\n    types = [(MockDataFrame, MockDataFrame)]\n    try:\n        from pandas import Series, DataFrame\n        types.append((Series, DataFrame))\n    except ImportError:\n        pass\n    for TrueInputType, PredInputType in types:\n        # y_pred dataframe, y_true series\n        y_true, y_pred = TrueInputType(y_tr), PredInputType(y_pr)\n        assert_raises(ValueError, log_loss, y_true, y_pred)\n", "def test_log_loss_with_sample_weight():\n    # test sample_weight\n    y_true = np.array([0, 1, 0, 1])\n    y_pred = np.array([[0.7, 0.3], [0.2, 0.8], [0.4, 0.6], [0.1, 0.9]])\n    sample_weight = np.array([1.0, 0.0, 1.0, 0.0])\n\n    # if sample_weight is all zeros, the loss should be zero\n    loss = log_loss(y_true, y_pred, sample_weight=sample_weight)\n    assert_almost_equal(loss, 0.0)\n\n    # test the case when sample_weight is not all zeros\n    y_true = np.array([0, 1, 0, 1])\n    y_pred = np.array([[0.7, 0.3], [0.2, 0.8], [0.4, 0.6], [0.1, 0.9]])\n    sample_weight = np.array([0.0, 1.0, 0.0, 1.0])\n\n    # if sample_weight is not all zeros, the loss should not be zero\n    loss = log_loss(y_true, y_pred, sample_weight=sample_weight)\n    assert_not_equal(loss, 0.0)\n", "def test_matthews_corrcoef_underflow():\n    # https://github.com/scikit-learn/scikit-learn/issues/9622\n    rng = np.random.RandomState(20170906)\n\n        conf_matrix = confusion_matrix(y_true, y_pred)\n        true_pos = conf_matrix[1, 1]\n        false_pos = conf_matrix[1, 0]\n        false_neg = conf_matrix[0, 1]\n        n_points = len(y_true)\n        pos_rate = (true_pos + false_neg) / n_points\n        activity = (true_pos + false_pos) / n_points\n        mcc_numerator = true_pos / n_points - pos_rate * activity\n        mcc_denominator = activity * pos_rate * (1 - activity) * (1 - pos_rate)\n        return mcc_numerator / np.sqrt(mcc_denominator)\n\n        x_true = rng.random_sample(n_points)\n        x_pred = x_true + 0.2 * (rng.random_sample(n_points) - 0.5)\n        y_true = (x_true > 0.5)\n        y_pred = (x_pred > 0.5)\n        return y_true, y_pred\n\n    arr = np.repeat([0., 1.], n_points)  # binary\n    assert_almost_equal(matthews_corrcoef(arr, arr), 1.0)\n    arr = np.repeat([0., 1., 2.], n_points)  # multiclass\n    assert_almost_equal(matthews_corrcoef(arr, arr), 1.0)\n\n    y_true, y_pred = random_ys(n_points)\n    assert_almost_equal(matthews_corrcoef(y_true, y_true), 1.0)\n    assert_almost_equal(matthews_corrcoef(y_true, y_pred),\n                        mcc_safe(y_true, y_pred))\n    assert_almost_equal(matthews_corrcoef(y_true, y_pred),\n                        matthews_corrcoef(y_true.astype(np.int32), y_pred.astype(np.int32)))\n\n    # Check if a copy of the input arrays is made, avoiding modifications in-place.\n    y_true_32, y_pred_32 = y_true.astype(np.int32), y_pred.astype(np.int32)\n    y_true_32[0], y_pred_32[0] = 3, 3\n    assert_almost_equal(matthew"], "sample_147": ["def test_distinct_with_values(self):\n    ReservedName.objects.create(name='a', order=2)\n    qs1 = ReservedName.objects.all()\n    qs2 = ReservedName.objects.all()\n    reserved_name = qs1.union(qs2).distinct('order').values('name', 'order', 'id').get()\n    self.assertEqual(reserved_name['name'], 'a')\n    self.assertEqual(reserved_name['order'], 2)\n    reserved_name = qs1.union(qs2).distinct('order').values_list('name', 'order', 'id').get()\n    self.assertEqual(reserved_name[:2], ('a', 2))\n", "def test_union_with_in_bulk(self):\n    qs1 = Number.objects.in_bulk([1, 2, 3])\n    qs2 = Number.objects.in_bulk([3, 4, 5])\n    union_qs = list(qs1.union(qs2))\n    self.assertEqual(len(union_qs), 5)\n    self.assertEqual([obj.num for obj in union_qs], [1, 2, 3, 4, 5])\n", "def test_get_from_combined_qs(self):\n    qs1 = Number.objects.filter(num__lte=1)\n    qs2 = Number.objects.filter(num__gte=8)\n    qs = qs1.union(qs2)\n    self.assertNumbersEqual(qs, [0, 1, 8, 9])\n    with self.assertRaises(Number.DoesNotExist):\n        qs.get(num=5)\n    with self.assertRaises(Number.MultipleObjectsReturned):\n        qs.get(num=0)\n", "def test_union_with_ordered_and_annotated_querysets(self):\n    qs1 = Number.objects.order_by('num').filter(num__lte=1)\n    qs2 = Number.objects.annotate(count=Value(0, IntegerField())).filter(num__gte=2, num__lte=3)\n    union = qs1.union(qs2)\n    self.assertNumbersEqual(union, [0, 1, 2, 3])\n    self.assertNumbersEqual(union.order_by('-num'), [3, 2, 1, 0])\n", "def test_union_with_values_list_on_related(self):\n    ReservedName.objects.create(name='rn1', order=1)\n    ReservedName.objects.create(name='rn2', order=2)\n    qs1 = Number.objects.filter(num=1).values_list('pk', 'num')\n    qs2 = Number.objects.filter(num=2).values_list('pk', 'num')\n    qs3 = ReservedName.objects.values_list('pk', 'order')\n    union_qs = qs1.union(qs2, qs3)\n    self.assertEqual(union_qs[0], (1, 1))\n    self.assertEqual(union_qs[1], (2, 2))\n    self.assertEqual(union_qs[2], (1, 1))\n    self.assertEqual(union_qs[3], (2, 2))\n"], "sample_19": ["compilation error", "def test_array_like_input(tmp_path):\n    # Test that WCS supports ArrayLike inputs\n\n    class ArrayLike:\n            self.data = data\n\n            return self.data\n\n    w = wcs.WCS(naxis=2)\n    w.wcs.crpix = [1.5, 5.5]\n    w.wcs.cdelt = [-0.1, 0.1]\n\n    arr = ArrayLike(np.array([1, 2]))\n    ra, dec = w.wcs_pix2world(arr, 1, 0)\n    assert_array_almost_equal(ra, 0.4)\n    assert_array_almost_equal(dec, 4.5)\n\n    arr = ArrayLike(np.array([[1, 2], [3, 4]]))\n    ra, dec = w.wcs_pix2world(arr, 0)\n    assert_array_almost_equal(ra, [0.4, 2.7])\n    assert_array_almost_equal(dec, [4.5, 2.5])\n", "def test_sliced_wcs_pixel_area(tmp_path):\n    \"\"\"\n    Test that WCSUtils.proj_plane_pixel_area is correctly called when WCS is sliced.\n    \"\"\"\n    w = wcs.WCS(naxis=3)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\", \"FREQ\"]\n    w.wcs.crpix = [1, 1, 1]\n    w.wcs.crval = [5.63, -72.05, 1.0]\n    w.wcs.cdelt = [1.0, 1.0, 1.0]\n    w.wcs.set()\n    ws = w.sub([1, 2])\n\n    # Test pixel area calculation\n    area = ws.proj_plane_pixel_area()\n    assert area.unit == u.deg ** 2\n    assert np.isclose(area.value, ws.wcs.cd[0, 0] * ws.wcs.cd[1, 1])\n", "def test_cunit_from_header(tmp_path):\n    # Regression test for #13419\n    header = fits.Header()\n    header[\"CUNIT1\"] = \"km/s\"\n    header[\"CUNIT2\"] = \"m/s\"\n    header[\"CTYPE1\"] = \"VELO_LSR\"\n    header[\"CTYPE2\"] = \"FREQ\"\n    header[\"CRVAL1\"] = 100\n    header[\"CRVAL2\"] = 5000\n    header[\"CRPIX1\"] = 1\n    header[\"CRPIX2\"] = 1\n    header[\"CDELT1\"] = 10\n    header[\"CDELT2\"] = 0.01\n\n    header = fits.Header(header, copy=True)\n\n    with pytest.warns(wcs.FITSFixedWarning):\n        w = wcs.WCS(header)\n\n    w.wcs.cunit = [\"m/s\", \"Hz\"]\n\n    filename = tmp_path / \"test_cunit_from_header.fits\"\n    w.to_fits(filename)\n\n    with fits.open(filename) as hdulist:\n        with pytest.warns(wcs.FITSFixedWarning):\n            w2 = wcs.WCS(hdulist[0].header)\n\n    assert w2.wcs.cunit == [\"m/s\", \"Hz\"]\n", "def test_pv_non_linear_coordinate_axis_type():\n    \"\"\"\n    Test PV non-linear coordinate axis type. Regression test for #11543.\n    \"\"\"\n    header_dict = {\n        \"CTYPE1\": \"RA---TPV\",\n        \"CTYPE2\": \"DEC--TPV\",\n        \"CRVAL1\": 12.5,\n        \"CRVAL2\": 34.2,\n        \"CRPIX1\": 2048,\n        \"CRPIX2\": 1024,\n        \"CDELT1\": -0.00055555556,\n        \"CDELT2\": 0.00055555556,\n        \"PV1_1\": 1,\n        \"PV1_2\": 0,\n        \"PV2_1\": 0,\n        \"PV2_2\": 1,\n        \"PV1_3\": 0,\n        \"PV1_4\": 0,\n        \"PV1_5\": 0,\n        \"PV1_6\": 0,\n        \"PV1_7\": 0,\n        \"PV1_8\": 0,\n        \"PV1_9\": 0,\n        \"PV1_10\": 0,\n        \"PV1_11\": 0,\n        \"PV1_12\": 0,\n        \"PV1_13\": 0,\n        \"PV1_14\": 0,\n        \"PV1_15\": 0,\n        \"PV1_16\": 0,\n        \"PV1_17\": 0,\n        \"PV1_18\": 0,\n        \"PV1_19\": 0,\n        \"PV1_20\": 0,\n    }\n    w = wcs.WCS(header_dict)\n    assert w.wcs.axis_types[0] == 1314\n    assert w.wcs.axis_types[1] == 1314\n    assert np.allclose(w.wcs.get_pv(), [(1, 1, 1), (1, 2, 0)])\n"], "sample_199": ["def test_window_expression(self):\n    book = Book.objects.annotate(\n        pages_rank=Window(\n            expression=Rank(),\n            order_by=F('pages').desc(),\n        )\n    ).get(isbn='155860191')\n    self.assertEqual(book.pages_rank, 1)\n\n    book = Book.objects.annotate(\n        pages_dense_rank=Window(\n            expression=DenseRank(),\n            order_by=F('pages').desc(),\n        )\n    ).get(isbn='155860191')\n    self.assertEqual(book.pages_dense_rank, 1)\n\n    book = Book.objects.annotate(\n        pages_row_number=Window(\n            expression=RowNumber(),\n            order_by=F('pages').desc(),\n        )\n    ).get(isbn='155860191')\n    self.assertEqual(book.pages_row_number, 1)\n\n    book = Book.objects.annotate(\n        pages_ntile=Window(\n            expression=Ntile(2),\n            order_by=F('pages').desc(),\n        )\n    ).get(isbn='155860191')\n    self.assertEqual(book.pages_ntile, 1)\n\n    book = Book.objects.annotate(\n        pages_lag=Window(\n            expression=Lag('pages'),\n            order_by=F('pages').desc(),\n        )\n    ).get(isbn='155860191')\n    self.assertIsNone(book.pages_lag)\n\n    book = Book.objects.annotate(\n        pages_lead=Window(\n            expression=Lead('pages'),\n            order_by=F('pages').desc(),\n        )\n    ).get(isbn='155860191')\n    self.assertEqual(book.pages_lead, 447)\n\n    book = Book.objects.annotate(\n        pages_first_value=Window(\n            expression=FirstValue('pages'),\n            order_by=F('pages').desc(),\n        )\n    ).get(isbn='155860191')\n    self.assertEqual(book.pages_first_value, 1132)\n\n    book = Book.objects.annotate(\n        pages_last_value=Window(\n            expression=LastValue('pages'),\n            order_by=F('pages').desc(),\n        )\n    ).get(isbn='155860191')\n    self.assertEqual(book.pages_last_value, 300)\n\n    book = Book.objects.annotate(\n        pages_nth_value=Window(\n            expression=NthValue('pages', 2),\n            order_by=F('pages').desc(),\n        )\n    ).get(isbn='155860191')\n    self.assertEqual(book.pages_nth_value, 946)\n", "def test_combined_annotation_with_datetime(self):\n    active = datetime.datetime(2015, 3, 20, 14, 0, 0)\n    duration = datetime.timedelta(hours=1)\n    expires = active + duration\n    Ticket.objects.create(active_at=active, duration=duration)\n    t = Ticket.objects.annotate(\n        expires=ExpressionWrapper(F('active_at') + F('duration'), output_field=DateTimeField())\n    ).first()\n    self.assertEqual(t.expires, expires)\n\n    t = Ticket.objects.annotate(\n        expires=ExpressionWrapper(F('duration') + F('active_at'), output_field=DateTimeField())\n    ).first()\n    self.assertEqual(t.expires, expires)\n\n    t = Ticket.objects.annotate(\n        expires=ExpressionWrapper(F('active_at') + Value(duration), output_field=DateTimeField())\n    ).first()\n    self.assertEqual(t.expires, expires)\n\n    t = Ticket.objects.annotate(\n        expires=ExpressionWrapper(Value(duration) + F('active_at'), output_field=DateTimeField())\n    ).first()\n    self.assertEqual(t.expires, expires)\n\n    t = Ticket.objects.annotate(\n        expires=ExpressionWrapper(F('active_at') + F('duration'), output_field=DateTimeField())\n    ).first()\n    self.assertEqual(t.expires, expires)\n\n    t = Ticket.objects.annotate(\n        expires=ExpressionWrapper(F('duration') + F('active_at'), output_field=DateTimeField())\n    ).first()\n    self.assertEqual(t.expires, expires)\n\n    t = Ticket.objects.annotate(\n        expires=ExpressionWrapper(F('active_at') - F('duration'), output_field=DateTimeField())\n    ).first()\n    self.assertEqual(t.expires, active - duration)\n\n    t = Ticket.objects.annotate(\n        expires=ExpressionWrapper(F('duration') - F('active_at'), output_field=DateTimeField())\n    ).first()\n    self.assertEqual(t.expires, duration - active)\n\n    t = Ticket.objects.annotate(\n        expires=ExpressionWrapper(F('active_at') - Value(duration), output_field=DateTimeField())\n    ).first()\n    self.assertEqual(t.expires, active - duration)\n\n    t = Ticket.objects.annotate(\n        expires=ExpressionWrapper(Value(duration) - F('active_at'), output_field=DateTimeField())\n    ).first()\n    self.assertEqual(t.expires, duration - active)\n", "def test_nested_annotation_with_m2m(self):\n    qs = Author.objects.annotate(\n        friend_count=Count('friends'),\n        max_friend_age=Max('friends__age')\n    ).annotate(\n        max_friend_age_plus_one=F('max_friend_age') + 1\n    ).values('name', 'friend_count', 'max_friend_age', 'max_friend_age_plus_one')\n    self.assertCountEqual(qs, [\n        {'name': 'Adrian Holovaty', 'friend_count': 2, 'max_friend_age': 35, 'max_friend_age_plus_one': 36},\n        {'name': 'Jacob Kaplan-Moss', 'friend_count': 2, 'max_friend_age': 35, 'max_friend_age_plus_one': 36},\n        {'name': 'James Bennett', 'friend_count': 1, 'max_friend_age': 34, 'max_friend_age_plus_one': 35},\n        {'name': 'Brad Dayley', 'friend_count': 0, 'max_friend_age': 0, 'max_friend_age_plus_one': 1},\n        {'name': 'Paul Bissex', 'friend_count': 1, 'max_friend_age': 37, 'max_friend_age_plus_one': 38},\n        {'name': 'Wesley J. Chun', 'friend_count': 2, 'max_friend_age': 37, 'max_friend_age_plus_one': 38},\n        {'name': 'Peter Norvig', 'friend_count': 1, 'max_friend_age': 46, 'max_friend_age_plus_one': 47},\n        {'name': 'Stuart Russell', 'friend_count': 1, 'max_friend_age': 57, 'max_friend_age_plus_one': 58},\n    ])\n", "def test_outer_ref_annotation(self):\n    store = Store.objects.first()\n    Employee.objects.create(id=1, first_name='Max', manager=True, last_name='Paine', store=store, age=23, salary=Decimal(50000.00))\n    Employee.objects.create(id=2, first_name='Buffy', manager=False, last_name='Summers', store=store, age=18, salary=Decimal(40000.00))\n    inner_qs = Employee.objects.filter(manager=True).values_list('pk', flat=True)\n    qs = Employee.objects.annotate(is_manager=Case(\n        When(pk__in=Subquery(inner_qs), then=Value(True)),\n        default=Value(False),\n        output_field=BooleanField(),\n    ))\n    for employee in qs:\n        if employee.pk == 1:\n            self.assertIs(employee.is_manager, True)\n        else:\n            self.assertIs(employee.is_manager, False)\n", "def test_annotation_with_window_function(self):\n    \"\"\"\n    Test using a Window function to compute the row number.\n    \"\"\"\n    class RowNumber(Window):\n            super().__init__(\n                expression,\n                partition_by=[F('name')],\n                order_by=F('name').asc(),\n            )\n            return '0 ROWS', 'UNBOUNDED FOLLOWING'\n\n    # Create a model to test against.\n    Book.objects.create(name='Book1', price=10)\n    Book.objects.create(name='Book2', price=20)\n    Book.objects.create(name='Book3', price=30)\n\n    qs = Book.objects.annotate(row_number=RowNumber(F('id'))).order_by('name')\n\n    self.assertQuerysetEqual(\n        qs,\n        [\n            ('Book1', 1),\n            ('Book2', 2),\n            ('Book3', 3)\n        ],\n        lambda b: (b.name, b.row_number),\n    )\n"], "sample_162": ["    def test_is_templatized(self):\n        cmd = MakeMessagesCommand()\n        cmd.domain = 'djangojs'\n        cmd.gettext_version = (0, 18, 3)\n        build_file = cmd.build_file_class(cmd, cmd.domain, self.translatable_file_class('.', 'test.js', '.'))\n        self.assertFalse(build_file.is_templatized)\n\n        cmd.gettext_version = (0, 18, 2)\n        build_file = cmd.build_file_class(cmd, cmd.domain, self.translatable_file_class('.', 'test.js', '.'))\n        self.assertTrue(build_file.is_templatized)\n\n        cmd.domain = 'django'\n        build_file = cmd.build_file_class(cmd, cmd.domain, self.translatable_file_class('.', 'test.py', '.'))\n        self.assertFalse(build_file.is_templatized)\n\n        build_file = cmd.build_file_class(cmd, cmd.domain, self.translatable_file_class('.', 'test.html', '.'))\n        self.assertTrue(build_file.is_templatized)\n", "    def test_makemessages_invalid_domain(self):\n        \"\"\"Behavior is correct if domain is invalid.\"\"\"\n        msg = \"currently makemessages only supports domains 'django' and 'djangojs'\"\n        with self.assertRaisesMessage(CommandError, msg):\n            management.call_command('makemessages', locale=[LOCALE], domain='invalid', verbosity=0)\n", "    def setUp(self):\n        super().setUp()\n        self.symlinked_locale_dir = os.path.join(self.test_dir, 'locale_symlinked')\n        if symlinks_supported():\n            os.symlink(os.path.join(self.test_dir, 'locale'), self.symlinked_locale_dir)\n        else:\n            self.skipTest(\"os.symlink() not available on this OS + Python version combination.\")\n", "    def setUp(self):\n        super().setUp()\n        self.source_file = os.path.join(self.test_dir, 'single_file.py')\n        with open(self.source_file, 'w') as f:\n            f.write('from django.utils.translation import gettext as _\\n')\n            f.write('_(\"Hello, World!\")\\n')\n", "    def test_handle_method_error_message(self):\n        \"\"\"\n        Test that the handle method raises the correct error message when\n        there are no locales, exclude, or all options specified.\n        \"\"\"\n        with self.assertRaisesMessage(CommandError, \"Type 'manage.py help makemessages' for usage information.\"):\n            management.call_command('makemessages')\n"], "sample_957": ["def test_get_type_hints():\n        pass\n\n    assert get_type_hints(func) == {'a': int, 'b': str, 'return': type(None)}\n\n    class MyClass:\n            pass\n\n    assert get_type_hints(MyClass) == {'a': int, 'b': str, 'return': type(None)}\n\n    class MyClass2:\n            self.x: int = 0\n\n    assert get_type_hints(MyClass2) == {'a': int, 'b': str, 'return': type(None)}\n\n    assert get_type_hints(123) == {}\n\n        pass\n\n    assert get_type_hints(func2) == {'a': 'MyClass', 'return': type(None)}\n", "def test_get_type_hints():\n        pass\n\n        return True\n\n    assert get_type_hints(foo) == {'a': int, 'b': str, 'return': bool}\n    assert get_type_hints(bar) == {'a': int, 'b': str, 'return': bool}\n\n    class TestClass:\n            pass\n\n    assert get_type_hints(TestClass) == {'a': int, 'b': str, 'return': None}\n\n    class TestClassWithNoAnnotations:\n            pass\n\n    assert get_type_hints(TestClassWithNoAnnotations) == {}\n\n    class TestClassWithBrokenAnnotations:\n            pass\n\n    assert get_type_hints(TestClassWithBrokenAnnotations) == {}\n", "def test_get_type_hints():\n        pass\n\n    assert get_type_hints(func) == {'a': int, 'b': str, 'return': NoneType}\n\n    class MyClass:\n            pass\n\n    assert get_type_hints(MyClass.__init__) == {'x': int, 'y': str, 'return': NoneType}\n\n    assert get_type_hints(None) == {}\n\n    class BrokenClass:\n        __annotations__ = 'not a dict'\n\n    assert get_type_hints(BrokenClass) == {}\n\n    class AnotherBrokenClass:\n            pass\n\n    assert get_type_hints(AnotherBrokenClass.__init__) == {}\n\n    class YetAnotherBrokenClass:\n            pass\n\n    assert get_type_hints(YetAnotherBrokenClass.__init__) == {'x': int, 'return': NoneType}\n", "def test_restify_type_hints_broken_types():\n    class BrokenType:\n        __module__ = '__broken__'\n        __qualname__ = 'BrokenType'\n\n    assert restify(BrokenType) == ':class:`__broken__.BrokenType`'\n\n    class AnotherBrokenType:\n        __module__ = 'builtins'\n\n    assert restify(AnotherBrokenType) == ':class:`AnotherBrokenType`'\n", "def test_get_type_hints():\n        pass\n\n    assert get_type_hints(my_function) == {'a': int, 'b': str, 'return': NoneType}\n\n        pass\n\n    assert get_type_hints(my_function_with_defaults) == {'a': int, 'b': str, 'return': NoneType}\n\n        pass\n\n    assert get_type_hints(my_function_with_typevars) == {'T': TypeVar('T'), 'return': TypeVar('T')}\n\n        pass\n\n    assert get_type_hints(my_function_with_invalid_type_hints) == {}\n\n    class MyClass:\n            pass\n\n    assert get_type_hints(MyClass().my_method) == {'a': int, 'b': str, 'return': NoneType}\n\n    # Test with broken class\n    class BrokenClass:\n        __annotations__ = {\"a\": 123}\n\n    assert get_type_hints(BrokenClass()) == {}\n\n    # Test with invalid object\n    assert get_type_hints(123) == {}\n\n    # Test with None\n    assert get_type_hints(None) == {}\n\n    # Test with ForwardRef\n    from typing import ForwardRef\n    my_forward_ref = ForwardRef('int')\n    assert get_type_hints(my_forward_ref) == {}\n"], "sample_541": ["def test_tool_handles(ax):\n    line_handles = widgets.ToolHandles(ax, [10, 20, 30], [10, 20, 30])\n    assert line_handles.x == [10, 20, 30]\n    assert line_handles.y == [10, 20, 30]\n    assert len(line_handles.artists) == 1\n\n    # Move the line handles\n    line_handles.set_data([15, 25, 35], [15, 25, 35])\n    assert line_handles.x == [15, 25, 35]\n    assert line_handles.y == [15, 25, 35]\n\n    # Test the closest function\n    closest_index, closest_distance = line_handles.closest(20, 20)\n    assert closest_index == 1\n    assert closest_distance < 5\n\n    # Test the set_visible function\n    for artist in line_handles.artists:\n        assert artist.get_visible()\n    line_handles.set_visible(False)\n    for artist in line_handles.artists:\n        assert not artist.get_visible()\n\n    # Test the set_animated function\n    for artist in line_handles.artists:\n        assert artist.get_animated()\n    line_handles.set_animated(False)\n    for artist in line_handles.artists:\n        assert not artist.get_animated()\n\n    # Test the remove function\n    line_handles.remove()\n    assert line_handles.artists[0] not in ax.artists\n", "def test_polygon_selector_bounding_box_remove(ax):\n    verts = [(20, 0), (0, 20), (20, 40), (40, 20)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[3]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    # Create selector\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=True)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    # In order to trigger the correct callbacks, trigger events on the canvas\n    # instead of the individual tools\n    t = ax.transData\n    canvas = ax.figure.canvas\n\n    # Remove the bounding box\n    tool._box.remove()\n    tool._box = None\n    assert tool._box is None\n    assert tool._get_bbox() == tool._selection_artist.get_bbox()\n\n    # Remove the bounding box\n    tool._box = widgets.RectangleSelector(\n        ax, onselect=lambda *args, **kwargs: None, interactive=True,\n        props=tool._box_props, handle_props=tool._box_handle_props)\n    tool._box._allow_creation = False\n    tool._box._selection_completed = True\n    tool._update_box()\n\n    # Remove a point from the polygon and check that the box extents update\n    np.testing.assert_allclose(tool._box.extents, (20.0, 40.0, 20.0, 40.0))\n\n    MouseEvent(\"button_press_event\", canvas, *t.transform((30, 20)), 3)._process()\n    MouseEvent(\"button_release_event\", canvas, *t.transform((30, 20)), 3)._process()\n    np.testing.assert_allclose(tool.verts, [(20, 30), (30, 40), (40, 30)])\n    np.testing.assert_allclose(tool._box.extents, (20.0, 40.0, 30.0, 40.0))\n", "def test_range_slider_bounds(ax):\n    # Test that the RangeSlider valmin and valmax are not overritten by\n    # slidermin and slidermax values.\n\n    slidermin = widgets.Slider(ax, \"\", 0, 10, valinit=2, slidermax=10)\n    slidermax = widgets.Slider(ax, \"\", 0, 10, valinit=8, slidermin=0)\n\n    rs = widgets.RangeSlider(ax, \"\", 0, 10, valinit=[2, 8],\n                             slidermin=slidermin, slidermax=slidermax)\n\n    assert rs.valmin == 0\n    assert rs.valmax == 10\n", "def test_range_slider_minmax_values(ax):\n    slider = widgets.RangeSlider(ax, label=\"\", valmin=0.0, valmax=1.0,\n                                 valinit=[0.2, 0.8], orientation='horizontal')\n    slider.set_max(0.7)\n    assert_allclose(slider.val, (0.2, 0.7))\n\n    slider.set_min(0.1)\n    assert_allclose(slider.val, (0.1, 0.7))\n\n    slider.set_val((0.3, 0.6))\n    assert_allclose(slider.val, (0.3, 0.6))\n\n    slider.set_min(0.4)\n    assert_allclose(slider.val, (0.4, 0.6))\n\n    slider.set_max(0.9)\n    assert_allclose(slider.val, (0.4, 0.7))\n\n    slider.set_val((0.2, 0.9))\n    assert_allclose(slider.val, (0.4, 0.7))\n\n    slider.set_val((0.2, 0.3))\n    assert_allclose(slider.val, (0.3, 0.4))\n\n    slider.set_val((0.7, 0.8))\n    assert_allclose(slider.val, (0.7, 0.8))\n\n    slider.set_val((0.7, 0.2))\n    assert_allclose(slider.val, (0.2, 0.7))\n", "def test_selector_multiple_selectors(ax, selector):\n    kwargs = dict(ax=ax, onselect=noop, interactive=True)\n    if selector == 'span':\n        Selector = widgets.SpanSelector\n        kwargs['direction'] = 'horizontal'\n    else:\n        Selector = widgets.RectangleSelector\n\n    # Create a selector\n    selector1 = Selector(**kwargs)\n    click_and_drag(selector1, start=(10, 10), end=(100, 120))\n\n    # Create another selector\n    selector2 = Selector(**kwargs)\n    click_and_drag(selector2, start=(20, 20), end=(80, 100))\n\n    # Create a third selector\n    selector3 = Selector(**kwargs)\n    click_and_drag(selector3, start=(15, 15), end=(75, 95))\n\n    # Check that both selectors respond to input\n    click_and_drag(selector1, start=(70, 70), end=(70, 140))\n    click_and_drag(selector2, start=(50, 50), end=(50, 150))\n    click_and_drag(selector3, start=(50, 50), end=(50, 150))\n"], "sample_111": ["def test_get_ordering_field_columns_with_duplicate_fields(self):\n    \"\"\"\n    Test get_ordering_field_columns() when there are duplicate fields in\n    list_display.\n    \"\"\"\n    class DupFieldBandAdmin(admin.ModelAdmin):\n        list_display = ['name', 'name', 'nr_of_members']\n\n    m = DupFieldBandAdmin(Band, custom_site)\n    request = self.factory.get('/band/')\n    request.user = self.superuser\n    cl = m.get_changelist_instance(request)\n    self.assertEqual(cl.get_ordering_field_columns(), {})\n", "def test_get_ordering_field_columns_with_F_expression(self):\n    \"\"\"\n    Test get_ordering_field_columns when the ordering field is an F expression.\n    \"\"\"\n    class OrderedByFBandAdmin(admin.ModelAdmin):\n        list_display = ['name', 'genres', 'nr_of_members']\n        ordering = (\n            F('nr_of_members').desc(nulls_last=True),\n            Upper(F('name')).asc(),\n            F('genres').asc(),\n        )\n\n    m = OrderedByFBandAdmin(Band, custom_site)\n    request = self.factory.get('/band/')\n    request.user = self.superuser\n    cl = m.get_changelist_instance(request)\n    self.assertEqual(cl.get_ordering_field_columns(), {3: 'desc', 2: 'asc'})\n\n    m = OrderedByFBandAdmin(Band, custom_site)\n    request = self.factory.get('/band/', {'o': '3'})\n    request.user = self.superuser\n    cl = m.get_changelist_instance(request)\n    self.assertEqual(cl.get_ordering_field_columns(), {3: 'asc'})\n\n    m = OrderedByFBandAdmin(Band, custom_site)\n    request = self.factory.get('/band/', {'o': '-3'})\n    request.user = self.superuser\n    cl = m.get_changelist_instance(request)\n    self.assertEqual(cl.get_ordering_field_columns(), {3: 'desc'})\n\n    m = OrderedByFBandAdmin(Band, custom_site)\n    request = self.factory.get('/band/', {'o': '3.3'})\n    request.user = self.superuser\n    cl = m.get_changelist_instance(request)\n    self.assertEqual(cl.get_ordering_field_columns(), {3: 'asc'})\n\n    m = OrderedByFBandAdmin(Band, custom_site)\n    request = self.factory.get('/band/', {'o': '-3.3'})\n    request.user = self.superuser\n    cl = m.get_changelist_instance(request)\n    self.assertEqual(cl.get_ordering_field_columns(), {3: 'desc'})\n", "def test_apply_select_related_when_list_select_related_is_a_list_with_repeated_fields(self):\n    class GetListSelectRelatedAdmin(admin.ModelAdmin):\n        list_display = ('band', 'player', 'band')\n        list_select_related = ['band', 'band']\n\n    ia = GetListSelectRelatedAdmin(Invitation, custom_site)\n    request = self.factory.get('/invitation/')\n    request.user = self.superuser\n    cl = ia.get_changelist_instance(request)\n    self.assertEqual(cl.queryset.query.select_related, {'player': {}, 'band': {}})\n", "def test_get_ordering_field_columns_with_related_fields(self):\n    \"\"\"\n    Regression tests for #21944: get_ordering_field_columns() should handle\n    related fields correctly.\n    \"\"\"\n    parent = Parent.objects.create(name='parent')\n    child = Child.objects.create(parent=parent, name='child')\n    request = self.factory.get('/child/')\n    request.user = self.superuser\n    m = ChildAdmin(Child, custom_site)\n    cl = m.get_changelist_instance(request)\n    ordering = {'parent': 'desc'}\n    ordering_fields = cl.get_ordering_field_columns()\n    self.assertEqual(ordering_fields, {1: 'desc'})\n", "def test_get_ordering_field_columns_with_model_field(self):\n    \"\"\"\n    Regression test for #24323: ChangeList.get_ordering_field_columns()\n    should work with model fields.\n    \"\"\"\n    class MyModel(models.Model):\n        name = models.CharField(max_length=255)\n        age = models.IntegerField()\n\n            return self.name\n\n    class MyModelAdmin(admin.ModelAdmin):\n        list_display = ['name', 'age']\n        ordering = ['name', '-age']\n\n    m = MyModelAdmin(MyModel, admin.site)\n    request = self.factory.get('/mymodel/')\n    request.user = self.superuser\n    cl = m.get_changelist_instance(request)\n    self.assertEqual(cl.get_ordering_field_columns(), {1: 'asc', 2: 'desc'})\n"], "sample_366": ["    def test_parse_duration_edge_cases(self):\n        test_values = (\n            ('-0 days', timedelta(0)),\n            ('-0:0:0', timedelta(0)),\n            ('-0:0:0.0', timedelta(0)),\n            ('-00:00:00', timedelta(0)),\n            ('-00:00:00.00', timedelta(0)),\n            ('-0 days 0:0:0', timedelta(0)),\n            ('-0 days 0:0:0.0', timedelta(0)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "def test_parse_duration_decimal_separator(self):\n    test_values = (\n        ('0.5 15:30', timedelta(days=0.5, minutes=15, seconds=30)),\n        ('0,5 15:30', timedelta(days=0.5, minutes=15, seconds=30)),\n        ('-0.5 15:30', timedelta(days=-0.5, minutes=15, seconds=30)),\n        ('-0,5 15:30', timedelta(days=-0.5, minutes=15, seconds=30)),\n        ('0.5:30', timedelta(minutes=30, milliseconds=500)),\n        ('0,5:30', timedelta(minutes=30, milliseconds=500)),\n        ('-0.5:30', timedelta(minutes=-30, milliseconds=-500)),\n        ('-0,5:30', timedelta(minutes=-30, milliseconds=-500)),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n", "    def test_parse_date_with_whitespace(self):\n        # Test parsing dates with leading and trailing whitespace\n        self.assertEqual(parse_date('  2012-04-23  '), date(2012, 4, 23))\n        # Test parsing dates with whitespace in the date string\n        self.assertIsNone(parse_date('2012 -04- 23'))\n", "def test_parse_duration_invalid_inputs(self):\n    invalid_inputs = (\n        'abcd',  # Not a valid duration\n        '1234567890',  # Too many digits\n        ' 15:30',  # Leading whitespace\n        '15:30 ',  # Trailing whitespace\n        '-15:30.abc',  # Invalid character in seconds\n        '15:30.123abc',  # Invalid character in microseconds\n        '15:60:30',  # Invalid minute value\n        '15:30:60',  # Invalid second value\n        '-01:01:01.123abc',  # Invalid character in microseconds\n    )\n    for source in invalid_inputs:\n        with self.subTest(source=source):\n            self.assertIsNone(parse_duration(source))\n", "    def test_parse_duration_invalid_inputs(self):\n        test_values = (\n            ('', None),\n            ('a', None),\n            ('1', None),  # No units specified\n            ('1 days 1', None),  # Missing time component\n            ('1 1:1:1:1', None),  # Too many time components\n            ('- 1:1:1', None),  # Negative sign not at the start\n            ('1:1:1:', None),  # Empty time component\n            ('1:1:1.1.1', None),  # Two decimal separators\n            ('1:1:1:1.1', None),  # Decimal separator in hours\n            ('1:1:1.1:', None),  # Decimal separator in minutes\n            ('1:1:1:1,1', None),  # Comma and colon used together\n            ('1 days 25:1:1', None),  # Invalid hour\n            ('1 days 1:61:1', None),  # Invalid minute\n            ('1 days 1:1:61', None),  # Invalid second\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n"], "sample_677": ["def test_evaluate_with_custom_matcher(expr: str, matcher: Callable[[str], bool], expected: bool) -> None:\n    assert evaluate(expr, matcher) is expected\n", "def test_evaluate_with_custom_matcher(expr: str, matcher: Callable[[str], bool], expected: bool) -> None:\n    assert evaluate(expr, matcher) is expected\n", "def test_evaluate_with_custom_matcher(expr: str, matcher: Callable[[str], bool], expected: bool) -> None:\n    assert evaluate(expr, matcher) is expected\n", "def test_matcher_functions(expr: str = 'ident', matcher: Callable[[str], bool], expected: bool) -> None:\n    assert evaluate(expr, matcher) is expected\n", "def test_complex_matches(expr: str, ident: str, expected: bool) -> None:\n    matcher = {ident: True, \"a\": True, \"b\": True, \"c\": True}.__getitem__\n    assert evaluate(expr, matcher) is expected\n"], "sample_674": ["def test_repr_failure(testdir):\n    \"\"\"Test that repr_failure returns a string for a collection failure.\"\"\"\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        pytestmark = pytest.mark.xfail(reason=\"collection failure\")\n    \"\"\"\n    )\n    result = testdir.runpytest(p, \"--collect-only\")\n    assert result.ret == 0\n    collector = result.parseconfig.getnode(p)\n    excinfo = ExceptionInfo(ValueError(\"some collection error\"))\n    repr_failure = collector.repr_failure(excinfo)\n    assert isinstance(repr_failure, str)\n    assert \"some collection error\" in repr_failure\n", "def test_repr_failure_py_truncates_locals(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n            very_long_variable_name = \"x\" * 1000\n            assert False\n    \"\"\"\n    )\n    result = testdir.runpytest(p, \"--showlocals\", \"--fulltrace\")\n    result.stdout.fnmatch_lines([\"*very_long_variable_name = 'x' * 1000*\"])\n\n    result = testdir.runpytest(p, \"--showlocals\")\n    result.stdout.fnmatch_lines([\"*very_long_variable_name = 'x' * 1000*\", \"*  ...*\"])\n\n", "def test_repr_failure_py_custom_tbstyle(testdir):\n    \"\"\"\n    Test that repr_failure_py's tbstyle parameter takes precedence over the tbstyle option.\n    \"\"\"\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            raise ValueError('fixture error')\n\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-v\", \"--tb=long\", p)\n    result.stdout.fnmatch_lines([\"*_ERROR at *_\"])\n\n    result = testdir.runpytest(\"-v\", \"--tb=short\", p)\n    result.stdout.fnmatch_lines([\"*_ERROR at *_\"], consecutive=False)\n\n    result = testdir.runpytest(\"-v\", \"--tb=auto\", p)\n    result.stdout.fnmatch_lines([\"*_ERROR at *_\"], consecutive=False)\n\n    result = testdir.runpytest(\"-v\", \"--tb=long\", \"--fulltrace\", p)\n    result.stdout.fnmatch_lines([\"*_ERROR at *_\"], consecutive=False)\n", "def test_item_repr_failure_with_fulltrace(testdir):\n    \"\"\"\n    Test that Item.repr_failure returns a long traceback with fulltrace option.\n    \"\"\"\n    p = testdir.makepyfile(\n        \"\"\"\n            assert False\n    \"\"\"\n    )\n    item = testdir.getitems(p)[0]\n    excinfo = ExceptionInfo(value=Exception(\"test exception\"))\n    result = item.repr_failure(excinfo, style=None)\n    item.config._config.option.fulltrace = True\n    assert \"full traceback\" in str(item.repr_failure(excinfo, style=None))\n\n", "def test_prunetraceback(testdir):\n    \"\"\"Test that _prunetraceback cuts paths from the traceback\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n            raise Exception(\"Something went wrong\")\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*File \\\"test_prunetraceback.py\\\", line 2*\"])\n\n    # Create a node and call _prunetraceback to cut the path from the traceback\n    node = nodes.Collector(\"test_prunetraceback.py\")\n    excinfo = pytest.raises(Exception, test_func)\n    node._prunetraceback(excinfo)\n\n    # Check that the path to the test file is no longer in the traceback\n    assert \"test_prunetraceback.py\" not in str(excinfo.traceback)\n"], "sample_477": ["    def test_filesizeformat01(self):\n        output = self.engine.render_to_string(\"filesizeformat01\", {\"size\": 1024})\n        self.assertEqual(output, \"1.0\\xa0KB\")\n", "def get_digit_tests(SimpleTestCase):\n    @setup({\"get_digit01\": \"{{ a|get_digit:b }}\"})\n        output = self.engine.render_to_string(\"get_digit01\", {\"a\": \"123\", \"b\": \"1\"})\n        self.assertEqual(output, \"3\")\n\n    @setup({\"get_digit02\": \"{{ a|get_digit:b }}\"})\n        output = self.engine.render_to_string(\"get_digit02\", {\"a\": \"123\", \"b\": \"2\"})\n        self.assertEqual(output, \"2\")\n\n    @setup({\"get_digit03\": \"{{ a|get_digit:b }}\"})\n        output = self.engine.render_to_string(\"get_digit03\", {\"a\": \"123\", \"b\": \"3\"})\n        self.assertEqual(output, \"1\")\n\n    @setup({\"get_digit04\": \"{{ a|get_digit:b }}\"})\n        output = self.engine.render_to_string(\"get_digit04\", {\"a\": \"123\", \"b\": \"4\"})\n        self.assertEqual(output, \"0\")\n\n    @setup({\"get_digit05\": \"{{ a|get_digit:b }}\"})\n        output = self.engine.render_to_string(\"get_digit05\", {\"a\": \"123\", \"b\": \"a\"})\n        self.assertEqual(output, \"123\")\n\n    @setup({\"get_digit06\": \"{{ a|get_digit:b }}\"})\n        output = self.engine.render_to_string(\"get_digit06\", {\"a\": \"123\", \"b\": \"0\"})\n        self.assertEqual(output, \"123\")\n\n    @setup({\"get_digit07\": \"{{ a|get_digit:b }}\"})\n        output = self.engine.render_to_string(\"get_digit07\", {\"a\": \"123\", \"b\": \"-1\"})\n        self.assertEqual(output, \"123\")\n", "def test_dictsort01(self):\n    output = self.engine.render_to_string(\n        \"dictsort01\",\n        {\n            \"a\": [{\"b\": 2}, {\"b\": 1}, {\"b\": 3}],\n            \"b\": [{\"b\": \"c\"}, {\"b\": \"a\"}, {\"b\": \"b\"}],\n        },\n    )\n    self.assertEqual(output, \"[{'b': 1}, {'b': 2}, {'b': 3}] [{'b': 'a'}, {'b': 'b'}, {'b': 'c'}]\")\n", "    def test_default01(self):\n        output = self.engine.render_to_string(\"default01\", {\"a\": None})\n        self.assertEqual(output, \"default\")\n", "def length_is(value, arg):\n    \"\"\"\n    Return a boolean of whether the value's length is the argument.\n\n    This filter is deprecated and will be removed in Django 5.1.\n    \"\"\"\n    warnings.warn(\n        \"The length_is template filter is deprecated in favor of the length template \"\n        \"filter and the == operator within an {% if %} tag.\",\n        RemovedInDjango51Warning,\n    )\n    try:\n        return len(value) == int(arg)\n    except (ValueError, TypeError):\n        return \"\"\n"], "sample_894": ["def test_read_only_buffer_csr_matrix(monkeypatch):\n    \"\"\"RandomForestClassifier must work on readonly sparse data.\n\n    Non-regression test for: https://github.com/scikit-learn/scikit-learn/issues/25333\n    \"\"\"\n    monkeypatch.setattr(\n        sklearn.ensemble._forest,\n        \"Parallel\",\n        partial(Parallel, max_nbytes=100),\n    )\n    rng = np.random.RandomState(seed=0)\n\n    X, y = make_classification(n_samples=100, n_features=200, random_state=rng)\n    X = csr_matrix(X, copy=True)\n\n    X.flags.writeable = False\n\n    clf = RandomForestClassifier(n_jobs=2, random_state=rng)\n    cross_val_score(clf, X, y, cv=2)\n", "def test_n_estimators_is_none():\n    \"\"\"Check that setting `n_estimators` to None will raise an error.\"\"\"\n    with pytest.raises(ValueError):\n        RandomForestClassifier(n_estimators=None)\n    with pytest.raises(ValueError):\n        RandomForestRegressor(n_estimators=None)\n    with pytest.raises(ValueError):\n        ExtraTreesClassifier(n_estimators=None)\n    with pytest.raises(ValueError):\n        ExtraTreesRegressor(n_estimators=None)\n", "def test_compute_oob_predictions(ForestEstimator, y_type, bootstrap):\n    \"\"\"Test OOB predictions when the y variable is continuous or\n    continuous-multioutput.\n    \"\"\"\n    n_samples = 100\n    n_estimators = 5\n\n    if y_type == \"continuous\":\n        X, y = datasets.make_regression(\n            n_samples=n_samples, n_features=10, n_informative=3, random_state=0\n        )\n    else:\n        X, y = datasets.make_regression(\n            n_samples=n_samples,\n            n_features=10,\n            n_informative=3,\n            n_targets=3,\n            random_state=0,\n        )\n\n    est = ForestEstimator(n_estimators=n_estimators, bootstrap=bootstrap)\n    with pytest.raises(ValueError, match=\"OOB prediction is not supported\"):\n        est.fit(X, y)\n\n    with pytest.raises(ValueError, match=\"OOB prediction is not supported\"):\n        est._compute_oob_predictions(X, y)\n", "def test_oob_score_custom_scoring_function():\n    \"\"\"Check that a custom scoring function is properly used when computing oob score.\"\"\"\n    X, y = datasets.make_regression(n_samples=500, n_features=10, random_state=1)\n    scoring_function = mean_squared_error\n\n    regressor = RandomForestRegressor(\n        n_estimators=50, bootstrap=True, oob_score=scoring_function, random_state=0\n    )\n    regressor.fit(X, y)\n    assert hasattr(regressor, \"oob_score_\")\n    assert regressor.oob_score_ > 0.0\n\n    # check that oob_score_ is correct\n    oob_pred = np.array(\n        [\n            tree.predict(X)[:, np.newaxis]\n            for tree in regressor.estimators_\n            if hasattr(tree, \"n_outputs_\") and tree.n_outputs_ == 1\n        ]\n    )\n    oob_pred = np.nanmean(oob_pred, axis=0)\n    assert oob_pred.shape[0] == X.shape[0]\n    assert oob_pred.shape[1] == 1\n\n    assert np.isclose(\n        regressor.oob_score_, scoring_function(y, oob_pred), atol=1e-5\n    ), \"OOB scores are not equal\"\n", "def test_forest_random_state_different_implementation():\n    \"\"\"Check that different forest estimators have same feature importances when\n    given the same random state, even though the trees are not the same\"\"\"\n    X = np.array([[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]])\n    y = np.array([-1, -1, -1, 1, 1, 1])\n\n    # ExtraTrees is expected to have different tree structures than RandomForest\n    forest_estimators = [ExtraTreesClassifier, RandomForestClassifier]\n\n    feature_importances = []\n    for estimator in forest_estimators:\n        clf = estimator(random_state=42)\n        clf.fit(X, y)\n        feature_importances.append(clf.feature_importances_)\n\n    assert_array_almost_equal(feature_importances[0], feature_importances[1])\n"], "sample_822": ["def test_pairwise_distances_argmin():\n    # Check pairwise minimum distances computation for any metric\n    X = [[0], [1]]\n    Y = [[-2], [3]]\n\n    Xsp = dok_matrix(X)\n    Ysp = csr_matrix(Y, dtype=np.float32)\n\n    expected_idx = [0, 1]\n    expected_vals = [2, 2]\n\n    # euclidean metric\n    idx = pairwise_distances_argmin(X, Y, metric=\"euclidean\")\n    idx2 = pairwise_distances_argmin(X, Y, metric=\"euclidean\", axis=0)\n    assert_array_almost_equal(idx, expected_idx)\n    assert_array_almost_equal(idx2, expected_idx)\n\n    # sparse matrix case\n    idxsp = pairwise_distances_argmin(Xsp, Ysp, metric=\"euclidean\")\n    assert_array_almost_equal(idxsp, expected_idx)\n    # We don't want np.matrix here\n    assert_equal(type(idxsp), np.ndarray)\n\n    # Non-euclidean scikit-learn metric\n    idx = pairwise_distances_argmin(X, Y, metric=\"manhattan\")\n    assert_array_almost_equal(idx, expected_idx)\n    # sparse matrix case\n    idxsp = pairwise_distances_argmin(Xsp, Ysp, metric=\"manhattan\")\n    assert_array_almost_equal(idxsp, expected_idx)\n\n    # Non-euclidean Scipy distance (callable)\n    idx = pairwise_distances_argmin(X, Y, metric=minkowski,\n                                    metric_kwargs={\"p\": 2})\n    assert_array_almost_equal(idx, expected_idx)\n\n    # Non-euclidean Scipy distance (string)\n    idx = pairwise_distances_argmin(X, Y, metric=\"minkowski\",\n                                    metric_kwargs={\"p\": 2})\n    assert_array_almost_equal(idx, expected_idx)\n\n    # Compare with naive implementation\n    rng = np.random.RandomState(0)\n    X = rng.randn(97, 149)\n    Y = rng.randn(111, 149)\n\n    dist = pairwise_distances(X, Y, metric=\"manhattan\")\n    dist_orig_ind = dist.argmin(axis=1)\n    assert_array_almost_equal(dist_orig_ind, pairwise_distances_argmin(X, Y,\n                                                                     axis=1,\n                                                                     metric=\"manhattan\"))\n\n    dist = pairwise_distances(X, Y, metric=\"manhattan\")\n    dist_orig_ind = dist.argmin(axis=0)\n    assert_array_almost_equal(dist_orig_ind, pairwise_distances_argmin(X, Y,\n                                                                     axis=0,\n", "def test_check_pairwise_arrays_invalid_input():\n    # Ensures that check_pairwise_arrays raises an error on invalid input.\n    rng = np.random.RandomState(0)\n    XA = rng.random_sample((5, 4))\n    XB = rng.random_sample((4, 4))\n\n    assert_raises(ValueError, check_pairwise_arrays, XA, XB, precomputed=True)\n    assert_raises(ValueError, check_pairwise_arrays, XA, XB, dtype=int)\n    assert_raises(ValueError, check_pairwise_arrays, XA, XB, dtype=float, precomputed=True)\n", "def test_cosine_similarity_non_negative():\n    # Test that the cosine similarity of a matrix with non-negative values\n    # results in a similarity matrix with values in [0, 1]\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((100, 5000))\n    D = cosine_similarity(X)\n    assert np.all(D >= 0)\n    assert np.all(D <= 1)\n    assert_array_almost_equal(D[np.diag_indices_from(D)], [1.] * D.shape[0])\n", "def test_cosine_similarity_sparse_output_dtype():\n    # Test that the cosine_similarity function returns a matrix with the correct\n    # dtype when the input is sparse and dense_output is set to False\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    X_sparse = csr_matrix(X)\n    Y = rng.random_sample((3, 4))\n    Y_sparse = csr_matrix(Y)\n    \n    S1 = cosine_similarity(X_sparse, Y_sparse, dense_output=False)\n    assert issparse(S1)\n    assert_equal(S1.dtype, np.float)\n    \n    S2 = cosine_similarity(X_sparse, Y_sparse, dense_output=True)\n    assert not issparse(S2)\n    assert_equal(S2.dtype, np.float)\n    \n    S3 = cosine_similarity(X, Y_sparse, dense_output=False)\n    assert issparse(S3)\n    assert_equal(S3.dtype, np.float)\n    \n    S4 = cosine_similarity(X_sparse, Y, dense_output=False)\n    assert issparse(S4)\n    assert_equal(S4.dtype, np.float)\n", "def test_check_pairwise_arrays_precomputed():\n    # Ensure that checks return valid pairwise arrays for precomputed metric.\n    # Check that if Y is None, Y is returned as a pointer to X.\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 5))\n    X_checked, Y_checked = check_pairwise_arrays(X, None, precomputed=True)\n    assert_array_equal(X, X_checked)\n    assert X_checked is Y_checked\n\n    XB = rng.random_sample((5, 10))\n    assert_raises(ValueError, check_pairwise_arrays, XB, None,\n                  precomputed=True)\n\n    XB = rng.random_sample((10, 5))\n    assert_raises(ValueError, check_pairwise_arrays, X, XB,\n                  precomputed=True)\n\n    # test with sparse precomputed distance matrix\n    XB_sparse = csr_matrix(rng.random_sample((5, 5)))\n    XB_sparse_checked, _ = check_pairwise_arrays(XB_sparse, None,\n                                                precomputed=True)\n    assert_array_equal(XB_sparse.todense(), XB_sparse_checked.todense())\n\n    XB_sparse = csr_matrix(rng.random_sample((10, 10)))\n    assert_raises(ValueError, check_pairwise_arrays, XB_sparse, None,\n                  precomputed=True)\n"], "sample_704": ["def test_node_repr_failure_with_different_tbstyles(pytester: Pytester) -> None:\n    p = pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            assert False\n    \"\"\"\n    )\n    result = pytester.runpytest(\"-v\", \"--tb=short\", p)\n    result.stdout.fnmatch_lines([str(p) + \":*: AssertionError\", \"*1 failed in *\"])\n\n    result = pytester.runpytest(\"-v\", \"--tb=long\", p)\n    result.stdout.fnmatch_lines([str(p) + \":*: AssertionError\", \"*1 failed in *\"])\n\n    result = pytester.runpytest(\"-v\", \"--fulltrace\", p)\n    result.stdout.fnmatch_lines([str(p) + \":*: AssertionError\", \"*1 failed in *\"])\n", "def test_fs_collector_from_parent(pytester: Pytester) -> None:\n    fs_collector = pytester.getitems(\n        \"\"\"\n            pass\n    \"\"\"\n    )[0].parent\n\n    assert fs_collector.path.is_file()\n\n    fs_collector_from_parent = nodes.FSCollector.from_parent(\n        fs_collector.parent, path=fs_collector.path\n    )\n    assert fs_collector_from_parent.path == fs_collector.path\n    assert fs_collector_from_parent.parent == fs_collector.parent\n", "def test_node_repr_failure_with_fulltrace(pytester: Pytester) -> None:\n    p = pytester.makepyfile(\n        \"\"\"\n            assert False\n    \"\"\"\n    )\n    result = pytester.runpytest([\"-v\", \"--fulltrace\", p])\n    result.stdout.fnmatch_lines([str(p) + \":*AssertionError:*\", \"*1 failed in *\"])\n\n", "def test_node_getparent() -> None:\n    # Arrange\n    root = nodes.Node.from_parent(None, name=\"root\")\n    parent = nodes.Node.from_parent(root, name=\"parent\")\n    child = nodes.Node.from_parent(parent, name=\"child\")\n\n    # Act\n    node = child.getparent(nodes.Node)\n    assert node is parent\n\n    node = child.getparent(type(root))\n    assert node is root\n\n    node = child.getparent(nodes.Collector)\n    assert node is parent\n\n    node = parent.getparent(nodes.Collector)\n    assert node is root\n\n    node = root.getparent(nodes.Collector)\n    assert node is root\n\n    node = root.getparent(nodes.Item)\n    assert node is None\n", "def test_node_repr_failure_cwd_changed(pytester: Pytester) -> None:\n    \"\"\"Test failure lines should use absolute paths if cwd has changed since\n    invocation, so the path is correct (#6428).\n\n    This test verifies that a fixture changing the current working directory\n    doesn't prevent pytest from showing the correct path in the failure message.\n    \"\"\"\n    p = pytester.makepyfile(\n        \"\"\"\n        import os\n        import pytest\n\n        @pytest.fixture\n            out_dir = tmp_path / 'ddd'\n            out_dir.mkdir()\n            old_dir = os.getcwd()\n            os.chdir(out_dir)\n            yield out_dir\n            os.chdir(old_dir)\n\n            assert False\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([str(p) + \":*: AssertionError\", \"*1 failed in *\"])\n    assert Path.cwd() not in result.stdout.str()\n"], "sample_113": ["    def test_get_view_name(self):\n            pass\n        self.assertEqual(utils.get_view_name(test_view), 'tests.test_view')\n", "    def test_get_view_name(self):\n        class View:\n                self.__module__ = 'test_module'\n                self.__qualname__ = 'test_view'\n\n        self.assertEqual(utils.get_view_name(View()), 'test_module.test_view')\n", "    def test_get_view_name(self):\n            pass\n        self.assertEqual(utils.get_view_name(test_view), 'tests.test_view')\n", "    def test_get_view_name(self):\n            pass\n        self.assertEqual(utils.get_view_name(my_view), '%s.my_view' % __name__)\n", "    def test_trim_docstring(self):\n        docstring = \"\"\"\n        This is a docstring\n            with leading and trailing whitespace.\n\n        And it has multiple lines.\n        \"\"\"\n        trimmed_docstring = utils.trim_docstring(docstring)\n        self.assertEqual(trimmed_docstring, \"This is a docstring\\n    with leading and trailing whitespace.\\n\\nAnd it has multiple lines.\")\n"], "sample_729": ["def test_warm_start_enetcv():\n    # Test warm start of ElasticNetCV\n    X, y, _, _ = build_dataset()\n    n_alphas = 50\n\n    # Train a model to converge on a lightly regularized problem\n    final_alpha = 1e-5\n    low_reg_model = ElasticNetCV(n_alphas=n_alphas, alpha=final_alpha).fit(X, y)\n\n    # Fitting a new model on a more regularized version of the same problem.\n    # Fitting with high regularization is easier it should converge faster\n    # in general.\n    high_reg_model = ElasticNetCV(n_alphas=n_alphas, alpha=final_alpha * 10).fit(X, y)\n    assert_greater(low_reg_model.n_iter_, high_reg_model.n_iter_)\n\n    # Fit the solution to the original, less regularized version of the\n    # problem but from the solution of the highly regularized variant of\n    # the problem as a better starting point. This should also converge\n    # faster than the original model that starts from zero.\n    warm_low_reg_model = deepcopy(high_reg_model)\n    warm_low_reg_model.set_params(warm_start=True, alpha=final_alpha)\n    warm_low_reg_model.fit(X, y)\n    assert_greater(low_reg_model.n_iter_, warm_low_reg_model.n_iter_)\n\n    assert_equal(low_reg_model.alpha_, warm_low_reg_model.alpha_)\n", "def test_enet_warning_on_non_fitting():\n    # Test that a warning is raised when fit is called on an ElasticNet object\n    # that has already been fitted, and warm_start is False.\n\n    # Generate dataset\n    X, y, X_test, y_test = build_dataset(n_samples=20, n_features=10)\n\n    # Create ElasticNet object\n    enet = ElasticNet(alpha=0.5, max_iter=100, precompute=False)\n\n    # Fit the object to the data\n    ignore_warnings(enet.fit)(X, y)\n\n    # Try to fit the object again to the data with warm_start=False\n    with pytest.warns(UserWarning, match=\"calling fit with warm_start=False\"):\n        enet.fit(X, y)\n", "def test_enet_warm_start_with_precompute():\n    X, y, _, _ = build_dataset()\n    X = X.copy(order='F')\n\n    original_X = X.copy()\n\n    # Warm start test with precompute\n    enet = ElasticNet(alpha=1e-3, max_iter=2, tol=1e-3, precompute=True)\n    enet.fit(X, y)\n    assert_array_almost_equal(original_X, X)\n\n    enet.set_params(warm_start=True)\n    assert_warns_message(UserWarning, \"Warm restart can only be used in \"\n                         \"combination with precompute='auto' or \"\n                         \"False\", enet.fit, X, y)\n", "def test_enet_check_input():\n    X, y, _, _ = build_dataset(n_samples=20, n_features=10)\n    X = np.asfortranarray(X)\n    y = np.asfortranarray(y)\n    X_order = 'F'\n    y_order = 'F'\n    X_dtype = X.dtype\n    y_dtype = y.dtype\n\n    # Test that no error is raised if data is provided in the right format\n    clf = ElasticNet(selection='cyclic', tol=1e-8)\n    clf.fit(X, y, check_input=False)\n\n    # Test that an error is raised if X has not the right order\n    X = np.ascontiguousarray(X)\n    X_order = 'C'\n    assert_raises_regex(ValueError, \"X should be a 'F' or 'C' contiguous\"\n                        \" array, got 'C' non-contiguous\", clf.fit,\n                        X, y, check_input=False)\n\n    # Test that an error is raised if y has not the right order\n    X = np.asfortranarray(X)\n    y = np.ascontiguousarray(y)\n    y_order = 'C'\n    assert_raises_regex(ValueError, \"y should be a 'F' or 'C' contiguous\"\n                        \" array, got 'C' non-contiguous\", clf.fit,\n                        X, y, check_input=False)\n\n    # Test that an error is raised if X has not the right dtype\n    X = X.astype('float16')\n    X_dtype = 'float16'\n    assert_raises_regex(ValueError, \"X should be a float64 or float32 \"\n                        \"array, got float16\", clf.fit, X, y,\n                        check_input=False)\n\n    # Test that an error is raised if y has not the right dtype\n    X = X.astype(X_dtype)\n    y = y.astype('float16')\n    y_dtype = 'float16'\n    assert_raises_regex(ValueError, \"y should be a float64 or float32 \"\n                        \"array, got float16\", clf.fit, X, y,\n                        check_input=False)\n", "def test_enet_lasso_corner_cases():\n    # Check corner cases that were previously untested:\n    # - X = y = 0\n    # - X = [1]\n    # - X = [1, 1, 1] and y = [0, 1, 2] (all constant features)\n    # - X = [1, 1, 1] and y = [1, 2, 3] (constant features and constant target)\n    X = np.array([[0], [0], [0]])\n    y = np.array([0, 0, 0])\n    clf = ElasticNet(fit_intercept=True).fit(X, y)\n    assert_array_almost_equal(clf.coef_, [0])\n    assert_array_almost_equal(clf.intercept_, [0])\n    assert_almost_equal(clf.dual_gap_, 0)\n\n    X = np.array([[1]])\n    y = np.array([0])\n    clf = ElasticNet(fit_intercept=True).fit(X, y)\n    assert_array_almost_equal(clf.coef_, [0])\n    assert_array_almost_equal(clf.intercept_, [0])\n    assert_almost_equal(clf.dual_gap_, 0)\n\n    X = np.array([[1], [1], [1]])\n    y = np.array([0, 1, 2])\n    clf = ElasticNet(fit_intercept=True).fit(X, y)\n    assert_array_almost_equal(clf.coef_, [0])\n    assert_array_almost_equal(clf.intercept_, [1])\n    assert_almost_equal(clf.dual_gap_, 0)\n\n    X = np.array([[1], [1], [1]])\n    y = np.array([1, 2, 3])\n    clf = ElasticNet(fit_intercept=True).fit(X, y)\n    assert_array_almost_equal(clf.coef_, [0])\n    assert_array_almost_equal(clf.intercept_, [2])\n    assert_almost_equal(clf.dual_gap_, 0)\n"], "sample_809": ["def test_mutual_info_classif_mixed_noise():\n    # Here the target is discrete and there are two continuous and one\n    # discrete feature. The idea of this test is clear from the code.\n    rng = check_random_state(0)\n    X = rng.rand(1000, 3)\n    X[:, 1] += X[:, 0]\n    y = ((0.5 * X[:, 0] + X[:, 2]) > 0.5).astype(int)\n    X[:, 2] = X[:, 2] > 0.5\n\n    # Add noise to the continuous features\n    X[:, 0] += rng.randn(1000) / 1000\n    X[:, 1] += rng.randn(1000) / 1000\n\n    mi = mutual_info_classif(X, y, discrete_features=[2], n_neighbors=3,\n                             random_state=0, copy=False)\n    mi_copy = mutual_info_classif(X, y, discrete_features=[2], n_neighbors=3,\n                                  random_state=0, copy=True)\n    assert_array_equal(mi, mi_copy)\n\n    assert_array_equal(np.argsort(-mi), [2, 0, 1])\n    for n_neighbors in [5, 7, 9]:\n        mi_nn = mutual_info_classif(X, y, discrete_features=[2],\n                                    n_neighbors=n_neighbors, random_state=0)\n        # Check that the continuous values have an higher MI with greater\n        # n_neighbors\n        assert_greater(mi_nn[0], mi[0])\n        assert_greater(mi_nn[1], mi[1])\n        # The n_neighbors should not have any effect on the discrete value\n        # The MI should be the same\n        assert_equal(mi_nn[2], mi[2])\n", "def test_mutual_info_regression_sparse():\n    # Test that mutual information regression works correctly for sparse matrices\n    rng = check_random_state(0)\n    X = rng.rand(1000, 3)\n    X[:, 1] += X[:, 0]\n    y = ((0.5 * X[:, 0] + X[:, 2]) > 0.5).astype(int)\n\n    X_sparse = csr_matrix(X)\n\n    mi_dense = mutual_info_regression(X, y, discrete_features=[2], n_neighbors=3,\n                                     random_state=0)\n    mi_sparse = mutual_info_regression(X_sparse, y, discrete_features=[2],\n                                      n_neighbors=3, random_state=0)\n\n    # Check that the results are the same for dense and sparse matrices\n    assert_array_equal(mi_dense, mi_sparse)\n", "def test_estimate_mi_copy():\n    # Test that the input data is not modified in place when copy=True\n    # and is modified in place when copy=False.\n\n    X = np.array([[0, 0, 0],\n                  [1, 1, 0],\n                  [2, 0, 1],\n                  [2, 0, 1],\n                  [2, 0, 1]], dtype=float, copy=True)\n    y = np.array([0, 1, 2, 2, 1], dtype=float)\n    X_csr = csr_matrix(X)\n\n    X_copy = X.copy()\n    mutual_info_regression(X, y, copy=True, random_state=0)\n    assert_array_equal(X, X_copy)\n\n    mutual_info_regression(X, y, copy=False, random_state=0)\n    assert_raises(AssertionError, assert_array_equal, X, X_copy)\n", "def test_mutual_info_classif_sparse():\n    # Test that mutual_info_classif works with sparse matrices\n    rng = check_random_state(0)\n    X = csr_matrix(rng.rand(100, 10))\n    X.data[X.data < 0.7] = 0  # make it sparse\n    X.eliminate_zeros()\n    y = rng.randint(0, 2, size=100)\n\n    mi = mutual_info_classif(X, y, discrete_features=True, random_state=0)\n    assert_equal(mi.shape, (10,))\n\n    # check that the MI is zero for constant columns\n    assert_almost_equal(mi[0], 0)\n", "def test_mutual_info_very_high_correlation():\n    # Check that in case of very high correlation the MI is still close to\n    # the theoretical value.\n    rng = check_random_state(0)\n    n_samples = 1000\n    x = rng.randn(n_samples)\n    y = 0.001 * rng.randn(n_samples) + x\n\n    mi_theory = np.log(1 / (0.001 * np.sqrt(2 * np.pi)))\n    mi_computed = mutual_info_regression(x.reshape(-1, 1), y,\n                                        discrete_features=False,\n                                        random_state=0)[0]\n    assert_almost_equal(mi_computed, mi_theory, 2)\n"], "sample_1033": ["def test_primitive():\n    a, b, c = symbols('a b c')\n    expr = 2*a + 4*b + 6*c\n    p, prim = expr.primitive()\n    assert p == 2\n    assert prim == a + 2*b + 3*c\n\n    expr = 2*a + b + 3*c\n    p, prim = expr.primitive()\n    assert p == 1\n    assert prim == 2*a + b + 3*c\n\n    expr = 3*a + 6*b + 9*c\n    p, prim = expr.primitive()\n    assert p == 3\n    assert prim == a + 2*b + 3*c\n\n    expr = 0*a + 0*b + 0*c\n    p, prim = expr.primitive()\n    assert p == 0\n    assert prim == 0\n", "def test_Mul_with_zero_infinite_2():\n    # Check for issue 11283\n    assert (S.Infinity*0).is_real is True\n    assert (S.NegativeInfinity*0).is_real is True\n    assert (S.ComplexInfinity*0).is_real is True\n    assert (-S.Infinity*0).is_real is True\n    assert (-S.NegativeInfinity*0).is_real is True\n    assert (-S.ComplexInfinity*0).is_real is True\n", "def test_issue_18986():\n    # Test that sympy's Add can correctly sort its arguments when there are many of them\n    import random\n    random.seed(42)\n    args = [random.randint(0, 100) for _ in range(20)]\n    add = Add(*args, evaluate=False)\n    sorted_args = sorted(args)\n    assert add.args == tuple(sorted_args)\n", "def test_issue_000():\n    # test the primitive method in Add\n    # make sure that a complex rational factor is correctly extracted\n    # from the terms in the sum\n    a = (2 + 3*I)*(x + y)\n    b = (3 + 2*I)*(x + y)\n    c = a + b\n    assert c.primitive() == (5 + I, x + y)\n", "def test_as_content_primitive_with_floating_point_coefficient():\n    a = symbols('a', real=True)\n    b = symbols('b', real=True)\n    c = symbols('c', real=True)\n    \n    # Test with floating point coefficients\n    assert (3.5*a + 2.0*b).as_content_primitive() == (1.0, 3.5*a + 2.0*b)\n    assert (3.5*a + 3.5*b).as_content_primitive() == (3.5, a + b)\n    assert (3.5*a + 7.0*b).as_content_primitive() == (1.0, 3.5*a + 7.0*b)\n    \n    # Test with negative floating point coefficients\n    assert (-3.5*a - 2.0*b).as_content_primitive() == (-1.0, 3.5*a + 2.0*b)\n    assert (-3.5*a - 3.5*b).as_content_primitive() == (-3.5, a + b)\n    assert (-3.5*a - 7.0*b).as_content_primitive() == (-1.0, 3.5*a + 7.0*b)\n    \n    # Test with negative coefficients that can be combined\n    assert (3.5*a - 3.5*b - 3.5*c).as_content_primitive() == (3.5, a - b - c)\n    assert (3.5*a + 3.5*b + 3.5*c).as_content_primitive() == (3.5, a + b + c)\n    \n    # Test with coefficients that are not numbers\n    assert (a + b).as_content_primitive() == (1, a + b)\n"], "sample_351": ["    def test_modelchoicefield_prepare_value_empty(self):\n        field = forms.ModelChoiceField(Category.objects.all())\n        self.assertIsNone(field.prepare_value(''))\n", "def test_model_to_dict(self):\n    \"\"\"\n    Test model_to_dict() functionality.\n    \"\"\"\n    article = Article.objects.create(pub_date=datetime.date(1988, 1, 4), writer=Writer.objects.create(name='Test writer'))\n    data = model_to_dict(article)\n    self.assertEqual(data['pub_date'], datetime.date(1988, 1, 4))\n    self.assertEqual(data['writer'], article.writer.pk)\n\n    # Test that non-editable fields are excluded.\n    self.assertNotIn('id', data)\n\n    # Test that fields can be specified.\n    data = model_to_dict(article, fields=['pub_date'])\n    self.assertEqual(data, {'pub_date': datetime.date(1988, 1, 4)})\n\n    # Test that excluded fields are actually excluded.\n    data = model_to_dict(article, exclude=['pub_date'])\n    self.assertNotIn('pub_date', data)\n\n    # Test that non-existent fields are ignored.\n    data = model_to_dict(article, fields=['pub_date', 'nonexistent'])\n    self.assertEqual(data, {'pub_date': datetime.date(1988, 1, 4)})\n", "def test_model_multiple_choice_field(self):\n    f = forms.ModelMultipleChoiceField(Category.objects.all())\n    choices = list(f.choices)\n    self.assertEqual(len(choices), 4)\n    self.assertEqual(choices, [\n        ('', '---------'),\n        (self.c1.pk, 'Entertainment'),\n        (self.c2.pk, 'A test'),\n        (self.c3.pk, 'Third'),\n    ])\n\n    with self.assertRaises(ValidationError):\n        f.clean('')\n\n    with self.assertRaises(ValidationError):\n        f.clean(None)\n\n    with self.assertRaises(ValidationError):\n        f.clean([0])\n\n    self.assertEqual(len(f.clean([self.c2.pk, self.c3.pk])), 2)\n    self.assertEqual([x.pk for x in f.clean([self.c2.pk, self.c3.pk])], [self.c2.pk, self.c3.pk])\n\n    c4 = Category.objects.create(name='Fourth', slug='4th', url='4th')\n    self.assertEqual(len(f.clean([self.c2.pk, self.c3.pk, c4.pk])), 3)\n    self.assertEqual([x.pk for x in f.clean([self.c2.pk, self.c3.pk, c4.pk])], [self.c2.pk, self.c3.pk, c4.pk])\n\n    Category.objects.get(url='4th').delete()\n    msg = \"['Select a valid choice. That choice is not one of the available choices.']\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        f.clean([self.c2.pk, self.c3.pk, c4.pk])\n\n    self.assertEqual(f.clean([]), [])\n\n    f.required = True\n    with self.assertRaises(ValidationError):\n        f.clean([])\n\n    f.required = False\n    self.assertEqual(f.clean([]), Category.objects.none())\n\n    f.to_field_name = 'slug'\n    self.assertEqual(len(f.clean([self.c2.slug, self.c3.slug])), 2)\n    self.assertEqual([x.pk for x in f.clean([self.c2.slug, self.c3.slug])], [self.c2.pk, self.c3.pk])\n\n    f = forms.ModelMultipleChoiceField(Category.objects.all(), required=False)\n    self.assertEqual(f.clean(''), Category.objects.none())\n\n    f = forms.ModelMultipleChoiceField(Category.objects.all(), required=False)\n    self.assertEqual(f.clean([]), Category.objects.none())\n\n    f = forms.ModelMultipleChoiceField(Category.objects.all(), required=True)\n    with self.assertRaises(ValidationError):\n        f.clean('')\n\n    f = forms", "def test_modelform_factory(self):\n    class AuthorForm(forms.ModelForm):\n        class Meta:\n            model = Author\n            fields = ['name']\n\n    # Test that the model's Meta fields and exclude are respected.\n    self.assertEqual(modelform_factory(Author, fields=('name',)).base_fields.keys(), {'name'})\n    self.assertEqual(modelform_factory(Author, exclude=('name',)).base_fields.keys(), {'age', 'friends'})\n\n    # Test that fields and exclude passed to modelform_factory override the model's Meta.\n    self.assertEqual(modelform_factory(Author, fields=('name', 'age'), exclude=('friends',)).base_fields.keys(), {'name', 'age'})\n    self.assertEqual(modelform_factory(Author, fields=('name', 'age', 'friends'), exclude=('name',)).base_fields.keys(), {'age', 'friends'})\n\n    # Test that widgets passed to modelform_factory are applied.\n    self.assertIsInstance(modelform_factory(Author, widgets={'name': forms.Textarea}).base_fields['name'].widget, forms.Textarea)\n\n    # Test that localized_fields passed to modelform_factory are applied.\n    self.assertTrue(modelform_factory(Author, localized_fields=('age',)).base_fields['age'].localize)\n\n    # Test that labels passed to modelform_factory are applied.\n    self.assertEqual(modelform_factory(Author, labels={'name': 'Author Name'}).base_fields['name'].label, 'Author Name')\n\n    # Test that help_texts passed to modelform_factory are applied.\n    self.assertEqual(modelform_factory(Author, help_texts={'name': 'Enter the author name'}).base_fields['name'].help_text, 'Enter the author name')\n\n    # Test that error_messages passed to modelform_factory are applied.\n    self.assertEqual(modelform_factory(Author, error_messages={'name': {'required': 'Enter your name!'}}).base_fields['name'].error_messages['required'], 'Enter your name!')\n\n    # Test that field_classes passed to modelform_factory are applied.\n    self.assertIsInstance(modelform_factory(Author, field_classes={'name': forms.CharField}).base_fields['name'], forms.CharField)\n\n    # Test that formfield_callback passed to modelform_factory is applied.\n        return forms.CharField(**kwargs)\n    self.assertIsInstance(modelform_factory(Author, formfield_callback=custom_formfield_callback).base_fields['name'], forms.CharField)\n\n", "def test_construct_instance(self):\n    # Create a model instance from the bound form's cleaned_data\n    author = Writer.objects.create(name='Test Author')\n    article_data = {\n        'title': 'Test Article',\n        'pub_date': datetime.date(2008, 1, 1),\n        'author': author.pk,\n    }\n    form = ArticleForm(data=article_data)\n    self.assertTrue(form.is_valid())\n    article = construct_instance(form, Article())\n    self.assertEqual(article.title, article_data['title'])\n    self.assertEqual(article.pub_date, article_data['pub_date'])\n    self.assertEqual(article.author, author)\n\n    # Test that file fields are saved after other fields\n    # to allow callable upload_to to use the values from other fields\n    article_data['image'] = SimpleUploadedFile('test_image.jpg', b'test content')\n    form = ArticleForm(data=article_data, files={'image': article_data['image']})\n    self.assertTrue(form.is_valid())\n    article = construct_instance(form, Article())\n    self.assertEqual(article.image, article_data['image'])\n\n    # Test that form fields that aren't on the model are ignored\n    article_data['extra_field'] = 'extra value'\n    form = ArticleForm(data=article_data)\n    self.assertTrue(form.is_valid())\n    article = construct_instance(form, Article())\n    self.assertEqual(article.title, article_data['title'])\n    self.assertEqual(article.pub_date, article_data['pub_date'])\n    self.assertEqual(article.author, author)\n\n    # Test that fields with default values are ignored if not present in the form\n    article_data = {}\n    form = ArticleForm(data=article_data)\n    self.assertTrue(form.is_valid())\n    article = construct_instance(form, Article())\n    self.assertEqual(article.title, '')\n    self.assertEqual(article.pub_date, datetime.date.today())\n\n    # Test that fields with default values are used if present in the form\n    # but the value is empty\n    article_data = {'title': ''}\n    form = ArticleForm(data=article_data)\n    self.assertTrue(form.is_valid())\n    article = construct_instance(form, Article())\n    self.assertEqual(article.title, '')\n    self.assertEqual(article.pub_date, datetime.date.today())\n\n    # Test that fields with default values are not used if present in the form\n    # and the value is not empty\n    article_data = {'title': 'Test Article', 'pub_date': datetime.date(2008, 1, 1)}\n    form = ArticleForm(data="], "sample_365": ["def test_lazy_object_proxy_methods(self):\n    \"\"\"Test methods of the lazy object proxy.\"\"\"\n    lazy_obj = lazy(lambda: [1, 2, 3], list)()\n\n    # Test __getitem__\n    self.assertEqual(lazy_obj[0], 1)\n\n    # Test __setitem__\n    lazy_obj[0] = 10\n    self.assertEqual(lazy_obj[0], 10)\n\n    # Test __delitem__\n    del lazy_obj[0]\n    self.assertEqual(lazy_obj, [2, 3])\n\n    # Test __iter__\n    self.assertEqual(list(iter(lazy_obj)), [2, 3])\n\n    # Test __len__\n    self.assertEqual(len(lazy_obj), 2)\n\n    # Test __contains__\n    self.assertTrue(2 in lazy_obj)\n    self.assertFalse(1 in lazy_obj)\n\n    # Test __mod__\n    self.assertEqual(\"Hello %s\" % lazy_obj[0], \"Hello 2\")\n\n    # Test __add__\n    self.assertEqual(lazy_obj + [4, 5], [2, 3, 4, 5])\n\n    # Test __radd__\n    self.assertEqual([4, 5] + lazy_obj, [4, 5, 2, 3])\n\n    # Test __hash__\n    with self.assertRaises(TypeError):\n        hash(lazy_obj)\n", "def test_simple_lazy_object(self):\n    \"\"\"Test SimpleLazyObject's __reduce__, __copy__, and __deepcopy__.\"\"\"\n        return 'initialized'\n\n    lazy_obj = SimpleLazyObject(lazy_init)\n    self.assertEqual(lazy_obj._wrapped, empty)\n\n    # __reduce__\n    reduced_lazy_obj = pickle.loads(pickle.dumps(lazy_obj))\n    self.assertEqual(reduced_lazy_obj._wrapped, 'initialized')\n\n    # __copy__\n    copied_lazy_obj = copy.copy(lazy_obj)\n    self.assertEqual(copied_lazy_obj._wrapped, empty)\n    copied_lazy_obj._setup()\n    self.assertEqual(copied_lazy_obj._wrapped, 'initialized')\n\n    # __deepcopy__\n    deepcopied_lazy_obj = copy.deepcopy(lazy_obj)\n    self.assertEqual(deepcopied_lazy_obj._wrapped, empty)\n    deepcopied_lazy_obj._setup()\n    self.assertEqual(deepcopied_lazy_obj._wrapped, 'initialized')\n\n    # Test __repr__\n    self.assertEqual(repr(lazy_obj), '<SimpleLazyObject: <function lazy_init at ...>>')\n    lazy_obj._setup()\n    self.assertEqual(repr(lazy_obj), \"<SimpleLazyObject: 'initialized'>\")\n", "def test_partition(self):\n    \"\"\"The partition function splits a list based on a predicate.\"\"\"\n    is_even = lambda x: x % 2 == 0\n    numbers = [1, 2, 3, 4, 5, 6]\n    evens, odds = partition(is_even, numbers)\n    self.assertEqual(evens, [2, 4, 6])\n    self.assertEqual(odds, [1, 3, 5])\n\n    # Test partition with an empty list.\n    empty_list = partition(is_even, [])\n    self.assertEqual(empty_list, ([], []))\n\n    # Test partition with a list containing one element.\n    single_element_list = partition(is_even, [5])\n    self.assertEqual(single_element_list, ([], [5]))\n\n    # Test partition with a list containing two elements.\n    two_elements_list = partition(is_even, [4, 5])\n    self.assertEqual(two_elements_list, ([4], [5]))\n\n    # Test partition with a list containing duplicate elements.\n    duplicate_elements_list = partition(is_even, [2, 4, 2, 4, 1])\n    self.assertEqual(duplicate_elements_list, ([2, 4, 2, 4], [1]))\n", "def test_lazy_hash(self):\n    \"\"\"\n    hash() works correctly for Promises.\n    \"\"\"\n    lazy_a = lazy(lambda: 4, int)\n    lazy_b = lazy(lambda: 4, int)\n    lazy_c = lazy(lambda: 5, int)\n\n    self.assertEqual(hash(lazy_a()), hash(lazy_b()))\n    self.assertNotEqual(hash(lazy_b()), hash(lazy_c()))\n", "    def test_lazy_object_hash(self):\n        \"\"\"\n        hash() works correctly for Promises.\n        \"\"\"\n        lazy_a = lazy(lambda: 4, int)\n        lazy_b = lazy(lambda: 4, int)\n        lazy_c = lazy(lambda: 5, int)\n\n        self.assertEqual(hash(lazy_a()), hash(lazy_b()))\n        self.assertNotEqual(hash(lazy_b()), hash(lazy_c()))\n"], "sample_261": ["    def test_parse_date_with_leading_or_trailing_whitespace(self):\n        self.assertIsNone(parse_date(' 2012-04-23 '))\n        self.assertIsNone(parse_date('2012-04-23 '))\n", "    def test_parse_empty_string(self):\n        self.assertIsNone(parse_date(''))\n        self.assertIsNone(parse_time(''))\n        self.assertIsNone(parse_datetime(''))\n        self.assertIsNone(parse_duration(''))\n", "    def test_parse_datetime_with_microsecond_overflow(self):\n        # Test that parsing a datetime with a microsecond value greater than 999999\n        # returns None instead of raising an error.\n        self.assertIsNone(parse_datetime('2012-04-23T10:20:30.1234567'))\n", "    def test_empty_strings(self):\n        self.assertIsNone(parse_date(''))\n        self.assertIsNone(parse_time(''))\n        self.assertIsNone(parse_datetime(''))\n        self.assertIsNone(parse_duration(''))\n", "    def test_parse_duration_empty_string(self):\n        self.assertIsNone(parse_duration(''))\n"], "sample_944": ["def test_is_system_TypeVar():\n    T = TypeVar('T')\n    T_co = TypeVar('T_co', covariant=True)\n    T_contra = TypeVar('T_contra', contravariant=True)\n\n    assert is_system_TypeVar(T)\n    assert is_system_TypeVar(T_co)\n    assert is_system_TypeVar(T_contra)\n\n    class MyTypeVar(TypeVar):\n        pass\n\n    MY_T = MyTypeVar('MY_T')\n\n    assert not is_system_TypeVar(MY_T)\n", "def test_is_system_TypeVar():\n    from typing import TypeVar\n    T = TypeVar('T')\n    assert is_system_TypeVar(T) is False\n\n    class MyTypeVar(TypeVar):\n        pass\n    MyT = MyTypeVar('MyT')\n    assert is_system_TypeVar(MyT) is False\n\n    from typing import Generic\n    class MyGeneric(Generic[T]):\n        pass\n    assert is_system_TypeVar(T) is False\n", "def test_get_type_hints():\n        pass\n\n    assert get_type_hints(func) == {'a': int, 'b': str, 'return': bool}\n\n    class MyClass:\n            pass\n\n    assert get_type_hints(MyClass) == {'x': int, 'y': str}\n\n    class MyGenericClass(Generic[T]):\n        pass\n\n    assert get_type_hints(MyGenericClass) == {}\n\n    class MyBrokenClass:\n        __annotations__ = {'broken': 'attribute'}\n\n    assert get_type_hints(MyBrokenClass) == {}\n\n    assert get_type_hints(None) == {}\n    assert get_type_hints('string') == {}\n", "def test_get_type_hints():\n        pass\n\n    type_hints = get_type_hints(test_function)\n    assert type_hints == {'a': int, 'b': str, 'return': type(None)}\n\n    class TestClass:\n            pass\n\n    type_hints = get_type_hints(TestClass.__init__)\n    assert type_hints == {'a': int, 'b': str, 'return': type(None)}\n\n    with pytest.raises(TypeError):\n        get_type_hints(\"not a function or class\")\n\n    with pytest.raises(KeyError):\n        get_type_hints(test_function, globalns={}, localns={\"a\": \"not a type\"})\n\n    # Test with TYPE_CHECKING\n    try:\n        from typing import TYPE_CHECKING\n        TYPE_CHECKING = True\n        type_hints = get_type_hints(test_function)\n        assert type_hints == {}\n    finally:\n        TYPE_CHECKING = False\n", "def test_get_type_hints():\n        pass\n\n    assert get_type_hints(my_function) == {'x': int, 'y': str, 'return': type(None)}\n\n    class MyClass:\n            pass\n\n    assert get_type_hints(MyClass.my_method) == {'x': int, 'y': str, 'return': type(None)}\n\n        pass\n\n    assert get_type_hints(my_function_with_defaults) == {'x': int, 'y': str, 'return': type(None)}\n\n    class MySubClass(MyClass):\n            pass\n\n    assert get_type_hints(MySubClass.my_method) == {'x': int, 'y': str, 'return': type(None)}\n\n    # Test with a function that has no annotations\n        pass\n\n    assert get_type_hints(my_function_without_annotations) == {}\n\n    # Test with a method that has no annotations\n    class MyClassWithoutAnnotations:\n            pass\n\n    assert get_type_hints(MyClassWithoutAnnotations.my_method) == {}\n\n    # Test with a class\n    class MyClassWithAnnotations:\n        x: int\n        y: str\n\n    assert get_type_hints(MyClassWithAnnotations) == {'x': int, 'y': str}\n\n    # Test with a module\n    import math\n    assert get_type_hints(math) == {}\n"], "sample_120": ["def test_serialize_lazy_objects_from_settings(self):\n    lazy_setting = SimpleLazyObject(lambda: settings.AUTH_USER_MODEL)\n    self.assertEqual(self.serialize_round_trip(lazy_setting), settings.AUTH_USER_MODEL)\n", "def test_serialize_custom_enum(self):\n    \"\"\"\n    Test serialization of custom enum classes.\n    \"\"\"\n    class CustomEnum(enum.Enum):\n        A = 1\n        B = 2\n\n    MigrationWriter.register_serializer(CustomEnum, EnumSerializer)\n    self.assertSerializedResultEqual(\n        CustomEnum.A,\n        (\"migrations.test_writer.CustomEnum['A']\", {'import migrations.test_writer'})\n    )\n    MigrationWriter.unregister_serializer(CustomEnum)\n    with self.assertRaisesMessage(ValueError, 'Cannot serialize: CustomEnum.A'):\n        self.assertSerializedEqual(CustomEnum.A)\n", "def test_serialize_function_type_serializer(self):\n        pass\n    self.assertSerializedResultEqual(\n        test_func,\n        (\"migrations.test_writer.test_func\", {'import migrations.test_writer'})\n    )\n    test_method = test_func.__get__(self, self.__class__)\n    self.assertSerializedResultEqual(\n        test_method,\n        (\"migrations.test_writer.TestWriterTests.test_func\", {'import migrations.test_writer'})\n    )\n", "def test_serialize_type(self):\n    class TypeSerializerTest(models.Model):\n        pass\n\n    self.assertSerializedResultEqual(\n        TypeSerializerTest,\n        (\"migrations.test_writer.TypeSerializerTest\", {'import migrations.test_writer'})\n    )\n    self.assertSerializedResultEqual(\n        type,\n        (\"type\", set())\n    )\n    self.assertSerializedResultEqual(\n        int,\n        (\"int\", set())\n    )\n    self.assertSerializedResultEqual(\n        str,\n        (\"str\", set())\n    )\n    self.assertSerializedResultEqual(\n        float,\n        (\"float\", set())\n    )\n    self.assertSerializedResultEqual(\n        dict,\n        (\"dict\", set())\n    )\n    self.assertSerializedResultEqual(\n        list,\n        (\"list\", set())\n    )\n    self.assertSerializedResultEqual(\n        tuple,\n        (\"tuple\", set())\n    )\n    self.assertSerializedResultEqual(\n        set,\n        (\"set\", set())\n    )\n    self.assertSerializedResultEqual(\n        frozenset,\n        (\"frozenset\", set())\n    )\n", "def test_serialize_settings_reference_with_path(self):\n    class CustomSettings:\n        AUTH_USER_MODEL = 'auth.User'\n\n    custom_settings = CustomSettings()\n    settings_configured = SettingsReference('AUTH_USER_MODEL', custom_settings)\n    self.assertSerializedResultEqual(\n        settings_configured,\n        (\"settings.AUTH_USER_MODEL\", {\"from django.conf import settings\"})\n    )\n\n    custom_settings.AUTH_USER_MODEL = 'auth2.User'\n    settings_configured = SettingsReference('AUTH_USER_MODEL', custom_settings)\n    self.assertSerializedResultEqual(\n        settings_configured,\n        (\"settings.AUTH_USER_MODEL\", {\"from django.conf import settings\"})\n    )\n"], "sample_706": ["def test_evaluate_with_none() -> None:\n    assert not evaluate(\"None\", lambda ident: True)\n    assert not evaluate(\"None\", lambda ident: False)\n    assert evaluate(\"None\", lambda ident: ident is None)\n    assert not evaluate(\"not None\", lambda ident: ident is None)\n    assert evaluate(\"not None\", lambda ident: True)\n", "def test_evaluate_with_dynamic_matcher(input: str, matcher: Callable[[str], bool], expected: bool) -> None:\n    assert evaluate(input, matcher) is expected\n", "def test_evaluate_with_different_matchers(matcher: dict, expected: bool) -> None:\n    expression_str = \"not (a or b or c or d or e or f)\"\n    assert evaluate(expression_str, matcher.__getitem__) is expected\n", "def test_matcher_adapter_evaluate_identifier_out_of_matcher_scope() -> None:\n    \"\"\"Test that MatcherAdapter properly handles identifiers outside of its scope.\"\"\"\n    matcher = {\"true\": True}.__getitem__\n    assert not evaluate(\"false\", matcher)\n    with pytest.raises(KeyError):\n        matcher(\"$false\")\n", "def test_matcher_function_called(matcher_input: str, matcher_output: bool, input: str, expected: bool) -> None:\n    matcher_calls = []\n    matcher = lambda ident: matcher_calls.append(ident) or matcher_output\n    result = evaluate(input, matcher)\n    assert result is expected\n    if input.startswith(\"not\"):\n        assert len(matcher_calls) == 1\n        assert matcher_calls[0] == input[4:]\n    elif \" and \" in input or \" or \" in input:\n        assert len(matcher_calls) == 2\n        assert matcher_calls[0] in input.split()\n        assert matcher_calls[1] in input.split()\n    else:\n        assert len(matcher_calls) == 1\n        assert matcher_calls[0] == input\n"], "sample_386": ["def test_mark_safe_object_implementing_both_dunder_str_and_dunder_html(self):\n    class Obj:\n            return \"<obj>\"\n\n            return \"<<obj>>\"\n\n    s = mark_safe(Obj())\n\n    self.assertRenderEqual(\"{{ s }}\", \"<<obj>>\", s=s)\n    self.assertRenderEqual(\"{{ s|force_escape }}\", \"&lt;&lt;obj&gt;&gt;\", s=s)\n", "def test_mark_safe_object_implementing_dunder_html_and_dunder_str(self):\n    class Obj:\n            return \"<<obj>>\"\n            return \"<obj>\"\n\n    s = mark_safe(Obj())\n\n    self.assertRenderEqual(\"{{ s }}\", \"<<obj>>\", s=s)\n    self.assertRenderEqual(\"{{ s|force_escape }}\", \"&lt;&lt;obj&gt;&gt;\", s=s)\n", "def test_mark_safe_decorator_on_callable_with_non_string_return(self):\n    \"\"\"\n    mark_safe used as a decorator on a function that returns a non-string value.\n    \"\"\"\n\n        return 123\n\n    self.assertEqual(mark_safe(return_non_string)(), return_non_string())\n\n        return SafeString(\"a&b\")\n\n    self.assertEqual(mark_safe(return_non_string_safe_data)(), return_non_string_safe_data())\n", "def test_mark_safe_with_non_string_input(self):\n    \"\"\"\n    Test that mark_safe handles non-string inputs by calling their __str__ method.\n    \"\"\"\n    class NonStringObj:\n            return \"<non-string>\"\n\n    s = mark_safe(NonStringObj())\n    self.assertRenderEqual(\"{{ s }}\", \"&lt;non-string&gt;\", s=s)\n\n    s = mark_safe(123)\n    self.assertRenderEqual(\"{{ s }}\", \"123\", s=s)\n\n    s = mark_safe(None)\n    self.assertRenderEqual(\"{{ s }}\", \"\", s=s)\n", "def test_mark_safe_marked_safe_already(self):\n    \"\"\"\n    Test that calling mark_safe() on a SafeString instance returns the original instance.\n    \"\"\"\n    s = mark_safe(\"a&b\")\n    self.assertIs(mark_safe(s), s)\n"], "sample_713": ["def test_ridge_regression_reflects_input_dtype():\n    rng = np.random.RandomState(0)\n    alpha = 1.0\n\n    n_samples, n_features = 6, 5\n    X_64 = rng.randn(n_samples, n_features)\n    y_64 = rng.randn(n_samples)\n    X_32 = X_64.astype(np.float32)\n    y_32 = y_64.astype(np.float32)\n\n    solvers = [\"svd\", \"sparse_cg\", \"cholesky\", \"lsqr\", \"sag\", \"saga\"]\n    for solver in solvers:\n\n        # Check type consistency 32bits\n        coef_32 = ridge_regression(X_32, y_32, alpha=alpha, solver=solver)\n        assert coef_32.dtype == X_32.dtype\n\n        # Check type consistency 64 bits\n        coef_64 = ridge_regression(X_64, y_64, alpha=alpha, solver=solver)\n        assert coef_64.dtype == X_64.dtype\n", "def test_ridge_classifier_cv_multi_class():\n    X, y = make_multilabel_classification(n_samples=10, n_features=5, \n                                          n_classes=3, n_labels=1, \n                                          allow_unlabeled=False, \n                                          random_state=0)\n    assert_raises(ValueError, RidgeClassifierCV().fit, X, y)\n", "def test_ridge_solver_choice():\n    # Test that the solver choice works as expected\n    X, y = make_regression(n_samples=100, n_features=10, random_state=42)\n\n    # Test that 'auto' solver chooses the correct solver\n    ridge_auto = Ridge(alpha=1.0, solver='auto')\n    ridge_auto.fit(X, y)\n    assert isinstance(ridge_auto, Ridge)\n\n    # Test that 'svd' solver works as expected\n    ridge_svd = Ridge(alpha=1.0, solver='svd')\n    ridge_svd.fit(X, y)\n    assert isinstance(ridge_svd, Ridge)\n\n    # Test that 'cholesky' solver works as expected\n    ridge_cholesky = Ridge(alpha=1.0, solver='cholesky')\n    ridge_cholesky.fit(X, y)\n    assert isinstance(ridge_cholesky, Ridge)\n\n    # Test that 'lsqr' solver works as expected\n    ridge_lsqr = Ridge(alpha=1.0, solver='lsqr')\n    ridge_lsqr.fit(X, y)\n    assert isinstance(ridge_lsqr, Ridge)\n\n    # Test that 'sparse_cg' solver works as expected\n    ridge_sparse_cg = Ridge(alpha=1.0, solver='sparse_cg')\n    ridge_sparse_cg.fit(X, y)\n    assert isinstance(ridge_sparse_cg, Ridge)\n\n    # Test that 'sag' solver works as expected\n    ridge_sag = Ridge(alpha=1.0, solver='sag')\n    ridge_sag.fit(X, y)\n    assert isinstance(ridge_sag, Ridge)\n\n    # Test that 'saga' solver works as expected\n    ridge_saga = Ridge(alpha=1.0, solver='saga')\n    ridge_saga.fit(X, y)\n    assert isinstance(ridge_saga, Ridge)\n", "def test_ridge_regression_multi_target_sparse():\n    # Test multi-target regression using sparse input\n    rng = np.random.RandomState(42)\n    n_samples, n_features, n_targets = 10, 5, 3\n    X = sp.csr_matrix(rng.randn(n_samples, n_features))\n    y = rng.randn(n_samples, n_targets)\n\n    # Test using different solvers\n    solvers = ['sag', 'saga']\n    for solver in solvers:\n        ridge = Ridge(alpha=1.0, solver=solver)\n        ridge.fit(X, y)\n        assert_equal(ridge.coef_.shape, (n_targets, n_features))\n        assert_array_almost_equal(ridge.predict(X), ridge.predict(X.toarray()))\n", "def test_ridge_regression_multi_alpha():\n    rng = np.random.RandomState(0)\n    n_samples, n_features, n_targets = 10, 5, 3\n    X = rng.randn(n_samples, n_features)\n    Y = rng.randn(n_samples, n_targets)\n    alpha1 = 1.0\n    alpha2 = np.array([1.0, 2.0, 3.0])\n\n    # Test that multi-alpha yields same results as individual fits\n    ridge1 = Ridge(alpha=alpha1, solver='svd')\n    ridge1.fit(X, Y)\n    ridge2 = Ridge(alpha=alpha2, solver='svd')\n    ridge2.fit(X, Y)\n    assert_array_almost_equal(ridge1.coef_, ridge2.coef_[:1])\n\n    # Test that using an array of len 1 alpha is the same as a single alpha\n    ridge3 = Ridge(alpha=[alpha1], solver='svd')\n    ridge3.fit(X, Y)\n    assert_array_almost_equal(ridge1.coef_, ridge3.coef_)\n\n    # Test that using an array of alpha with one value is the same as single\n    ridge4 = Ridge(alpha=np.array([alpha1]), solver='svd')\n    ridge4.fit(X, Y)\n    assert_array_almost_equal(ridge1.coef_, ridge4.coef_)\n\n    # Test that multi-alpha yields same results as individual fits\n    ridge5 = Ridge(alpha=alpha2, solver='cholesky')\n    ridge5.fit(X, Y)\n    assert_array_almost_equal(ridge2.coef_, ridge5.coef_)\n"], "sample_858": ["def test_flatten_transform_with_single_estimator():\n    \"\"\"Check transform method of VotingClassifier with single estimator.\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    eclf1 = VotingClassifier(estimators=[\n        ('lr', clf1)],\n        voting='soft',\n        flatten_transform=True).fit(X, y)\n    eclf2 = VotingClassifier(estimators=[\n        ('lr', clf1)],\n        voting='soft',\n        flatten_transform=False).fit(X, y)\n    \n    assert_array_equal(eclf1.transform(X).shape, (len(X), 3))\n    assert_array_equal(eclf2.transform(X).shape, (1, len(X), 3))\n    assert_array_almost_equal(eclf1.transform(X), eclf2.transform(X)[0])\n", "def test_voting_regressor_single_estimator():\n    # Test that VotingRegressor works with a single estimator\n    reg1 = LinearRegression()\n    ereg = VotingRegressor(estimators=[('lr', reg1)]).fit(X_r, y_r)\n    assert_array_almost_equal(ereg.predict(X_r), reg1.fit(X_r, y_r).predict(X_r))\n", "def test_voting_classifier_with_grid_search_and_sample_weight():\n    \"\"\"Check GridSearchCV with sample_weight parameter.\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    clf3 = GaussianNB()\n    eclf = VotingClassifier(estimators=[\n                ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n                voting='soft')\n\n    params = {'lr__C': [1.0, 100.0],\n              'voting': ['soft', 'hard'],\n              'weights': [[0.5, 0.5, 0.5], [1.0, 0.5, 0.5]]}\n\n    sample_weight = np.random.RandomState(123).uniform(size=(len(y),))\n\n    grid = GridSearchCV(estimator=eclf, param_grid=params)\n    grid.fit(iris.data, iris.target, sample_weight=sample_weight)\n\n    assert hasattr(grid, 'best_estimator_')\n    assert hasattr(grid, 'best_score_')\n    assert hasattr(grid, 'best_params_')\n    assert hasattr(grid, 'cv_results_')\n", "def test_voting_regressor_prediction_with_zero_weights():\n    \"\"\"Check regression prediction with zero weights.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(n_estimators=10)\n    ereg = VotingRegressor([('lr', reg1), ('rf', reg2)], weights=[1, 0])\n\n    ereg.fit(X_r, y_r)\n    reg1.fit(X_r, y_r)\n    y_pred_ereg = ereg.predict(X_r)\n    y_pred_reg1 = reg1.predict(X_r)\n    assert_array_almost_equal(y_pred_ereg, y_pred_reg1)\n", "def test_voting_regressor_weights():\n    \"\"\"Check weighted average regression prediction on boston dataset.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(n_estimators=10, random_state=123)\n    reg3 = DecisionTreeRegressor(random_state=123)\n\n    # Test when weights are all the same\n    ereg = VotingRegressor([('lr', reg1), ('rf', reg2), ('tree', reg3)],\n                           weights=[1, 1, 1])\n    X_r_train, X_r_test, y_r_train, y_r_test = \\\n        train_test_split(X_r, y_r, test_size=.25)\n    ereg.fit(X_r_train, y_r_train)\n    y_pred = ereg.predict(X_r_test)\n    assert_array_almost_equal(y_pred, reg1.fit(X_r_train, y_r_train).predict(X_r_test), decimal=2)\n    assert_array_almost_equal(y_pred, reg2.fit(X_r_train, y_r_train).predict(X_r_test), decimal=2)\n    assert_array_almost_equal(y_pred, reg3.fit(X_r_train, y_r_train).predict(X_r_test), decimal=2)\n\n    # Test when weights are all different\n    ereg = VotingRegressor([('lr', reg1), ('rf', reg2), ('tree', reg3)],\n                           weights=[10, 5, 1])\n    ereg.fit(X_r_train, y_r_train)\n    y_pred = ereg.predict(X_r_test)\n    assert_array_almost_equal(y_pred, (10 * reg1.fit(X_r_train, y_r_train).predict(X_r_test) +\n                                      5 * reg2.fit(X_r_train, y_r_train).predict(X_r_test) +\n                                      1 * reg3.fit(X_r_train, y_r_train).predict(X_r_test)) / 16, decimal=2)\n\n    # Test when weights are all zeros\n    ereg = VotingRegressor([('lr', reg1), ('rf', reg2), ('tree', reg3)],\n                           weights=[0, 0, 0])\n    msg = \"Input contains NaN, infinity or a value too large for dtype('float64').\"\n    assert_raise_message(ValueError, msg, ereg.fit, X_r_train, y_r_train)\n"], "sample_469": ["def test_alias_annotation_filter_with_m2m(self):\n    qs = (\n        Author.objects.alias(\n            jacob_name=F(\"friends__name\"),\n        )\n        .filter(\n            friends__age=35,\n        )\n        .annotate(\n            james_name=F(\"jacob_name\"),\n        )\n        .filter(\n            friends__age=29,\n        )\n        .annotate(\n            paul_name=F(\"james_name\"),\n        )\n        .values(\"jacob_name\", \"james_name\", \"paul_name\")\n    )\n    self.assertCountEqual(\n        qs,\n        [{\"jacob_name\": \"Jacob Kaplan-Moss\", \"james_name\": \"James Bennett\", \"paul_name\": \"James Bennett\"}],\n    )\n", "def test_annotation_with_relational_fields(self):\n    publisher_books = (\n        Publisher.objects.annotate(\n            book_name=F(\"book__name\"),\n            book_rating=F(\"book__rating\"),\n        )\n        .values(\"name\", \"book_name\", \"book_rating\")\n    )\n    self.assertEqual(\n        list(publisher_books),\n        [\n            {\"name\": \"Apress\", \"book_name\": \"The Definitive Guide to Django: Web Development Done Right\", \"book_rating\": 4.5},\n            {\"name\": \"Apress\", \"book_name\": \"Practical Django Projects\", \"book_rating\": 4.0},\n            {\"name\": \"Apress\", \"book_name\": \"Artificial Intelligence: A Modern Approach\", \"book_rating\": 4.0},\n            {\"name\": \"Sams\", \"book_name\": \"Sams Teach Yourself Django in 24 Hours\", \"book_rating\": 3.0},\n            {\"name\": \"Prentice Hall\", \"book_name\": \"Python Web Development with Django\", \"book_rating\": 4.0},\n            {\"name\": \"Morgan Kaufmann\", \"book_name\": \"Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp\", \"book_rating\": 5.0},\n            {\"name\": \"Jonno's House of Books\", \"book_name\": None, \"book_rating\": None},\n        ],\n    )\n\n    publisher_books = (\n        Publisher.objects.annotate(\n            book_name=F(\"book__name\"),\n            book_rating=F(\"book__rating\"),\n        )\n        .values(\"name\", \"book_name\", \"book_rating\")\n        .distinct()\n    )\n    self.assertEqual(\n        list(publisher_books),\n        [\n            {\"name\": \"Apress\", \"book_name\": \"Artificial Intelligence: A Modern Approach\", \"book_rating\": 4.0},\n            {\"name\": \"Apress\", \"book_name\": \"Practical Django Projects\", \"book_rating\": 4.0},\n            {\"name\": \"Apress\", \"book_name\": \"The Definitive Guide to Django: Web Development Done Right\", \"book_rating\": 4.5},\n            {\"name\": \"Jonno's House of Books\", \"book_name\": None, \"book_rating\": None},\n            {\"name\": \"Morgan Kaufmann\", \"book_name\": \"Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp\", \"book_rating\": 5.0},\n            {\"name\":", "def test_combine_queries(self):\n    qs1 = Author.objects.annotate(other_age=F(\"age\")).filter(other_age=34)\n    qs2 = Author.objects.annotate(other_age=F(\"age\")).filter(other_age=35)\n\n    qs = qs1.union(qs2)\n    self.assertQuerySetEqual(\n        qs.order_by(\"other_age\"),\n        [34, 34, 35],\n        lambda a: a.other_age,\n    )\n\n    qs = qs1.union(qs2, all=True)\n    self.assertQuerySetEqual(\n        qs.order_by(\"other_age\"),\n        [34, 34, 35],\n        lambda a: a.other_age,\n    )\n\n    qs = qs1.intersection(qs2)\n    self.assertQuerySetEqual(\n        qs.order_by(\"other_age\"),\n        [],\n        lambda a: a.other_age,\n    )\n\n    qs = qs1.difference(qs2)\n    self.assertQuerySetEqual(\n        qs.order_by(\"other_age\"),\n        [34, 34],\n        lambda a: a.other_age,\n    )\n\n    qs = qs1.union(qs2).annotate(new_age=F(\"other_age\"))\n    self.assertQuerySetEqual(\n        qs.order_by(\"other_age\"),\n        [34, 34, 35],\n        lambda a: a.other_age,\n    )\n", "def test_is_nullable(self):\n    self.assertTrue(\n        Book.objects.annotate(\n            check_empty_string=ExpressionWrapper(\n                F(\"pages\") == \"\", output_field=BooleanField()\n            )\n        ).get(pages=\"\").check_empty_string\n    )\n    self.assertTrue(\n        Book.objects.annotate(\n            check_none_value=ExpressionWrapper(\n                F(\"rating__isnull\"), output_field=BooleanField()\n            )\n        ).get(rating=None).check_none_value\n    )\n    self.assertFalse(\n        Book.objects.annotate(\n            check_empty_string=ExpressionWrapper(\n                F(\"pages\") == \"\", output_field=BooleanField()\n            )\n        ).get(pages=\"1\").check_empty_string\n    )\n    self.assertFalse(\n        Book.objects.annotate(\n            check_none_value=ExpressionWrapper(\n                F(\"rating__isnull\"), output_field=BooleanField()\n            )\n        ).get(rating=1.0).check_none_value\n    )\n", "    def test_multi_table_annotation(self):\n        # annotate over two tables (without joining)\n        # and use the resulting annotations in a filter\n        store_books = Store.objects.annotate(\n            book_count=Count(\"books\", distinct=True),\n            book_isbn=Max(\"books__isbn\"),\n        )\n        self.assertCountEqual(\n            store_books.filter(book_count=6).values_list(\"name\", flat=True),\n            [\"Amazon.com\"],\n        )\n        self.assertCountEqual(\n            store_books.filter(book_isbn=\"159059996\").values_list(\"name\", flat=True),\n            [\"Amazon.com\", \"Books.com\", \"Mamma and Pappa's Books\"],\n        )\n"], "sample_1031": ["def test_check_scale_factors():\n    # Check if scale factors are the right SI dimensions\n    for scale_factor, dimension in zip(\n            Quantity.SI_quantity_scale_factors.values(),\n            Quantity.SI_quantity_dimension_map.values()):\n        dimex = Quantity.get_dimensional_expr(scale_factor)\n        if dimex != 1:\n            if not dimsys_default.equivalent_dims(dimension, Dimension(dimex)):\n                assert False, f\"quantity value and dimension mismatch: {dimension}, {dimex}\"\n", "def test_scale_factors():\n    # Check that the scale factors for all units are correctly defined\n    for unit in Quantity.SI_quantity_scale_factors.keys():\n        scale_factor = Quantity.SI_quantity_scale_factors[unit]\n        dim = Quantity.SI_quantity_dimension_map[unit]\n        assert Quantity.get_dimensional_expr(scale_factor) == dim\n\n    # Check that some specific units have the correct scale factors\n    assert Quantity.SI_quantity_scale_factors[meter] == 1\n    assert Quantity.SI_quantity_scale_factors[kilogram] == 1\n    assert Quantity.SI_quantity_scale_factors[second] == 1\n    assert Quantity.SI_quantity_scale_factors[ampere] == 1\n    assert Quantity.SI_quantity_scale_factors[kelvin] == 1\n    assert Quantity.SI_quantity_scale_factors[mole] == 1\n    assert Quantity.SI_quantity_scale_factors[candela] == 1\n\n    # Check that some derived units have the correct scale factors\n    assert Quantity.SI_quantity_scale_factors[newton] == kg*m/s**2\n    assert Quantity.SI_quantity_scale_factors[joule] == N*m\n    assert Quantity.SI_quantity_scale_factors[watt] == J/s\n    assert Quantity.SI_quantity_scale_factors[pascal] == N/m**2\n    assert Quantity.SI_quantity_scale_factors[hertz] == 1/s\n", "def test_dimensional_equivalence():\n    dm = Quantity(\"dm\")\n    dm.set_dimension(length)\n    dm.set_scale_factor(Rational(1, 10))\n\n    base = (m, s)\n    base_dim = (m.dimension, s.dimension)\n    ms = UnitSystem(base, (c, dm), \"MS\", \"MS system\")\n\n    assert ms._system.equivalent_dims(m.dimension, dm.dimension)\n\n    kg = Quantity(\"kg\")\n    kg.set_dimension(mass)\n    kg.set_scale_factor(1)\n\n    with raises(ValueError):\n        ms._system.equivalent_dims(m.dimension, kg.dimension)\n", "def test_get_dimensional_expr():\n    assert Quantity.get_dimensional_expr(kg) == mass\n    assert Quantity.get_dimensional_expr(m / s) == velocity\n    assert Quantity.get_dimensional_expr(m * kg / s**2) == force\n\n    assert Quantity.get_dimensional_expr(m / s**2) == acceleration\n    assert Quantity.get_dimensional_expr(kg * m**2 / s**2) == energy\n    assert Quantity.get_dimensional_expr(kg / m / s**2) == pressure\n\n    assert Quantity.get_dimensional_expr(m * kg / s**3) == power\n    assert Quantity.get_dimensional_expr(m**2 / s**2) == velocity**2\n    assert Quantity.get_dimensional_expr(m**2 * kg / s**2) == energy\n", "def test_dimensional_expr():\n    # test that dimension of quantity value is the same as quantity dimension\n    for q in Quantity.SI_quantity_scale_factors.keys():\n        dimex = Quantity.get_dimensional_expr(q)\n        if dimex != 1:\n            if not dimsys_default.equivalent_dims(q.dimension, Dimension(dimex)):\n                raise ValueError(\"quantity value and dimension mismatch\")\n"], "sample_349": ["def test_url_with_custom_admin_site(self):\n    custom_admin_site = admin.AdminSite('custom')\n    rel = Album._meta.get_field('band')\n    w = AutocompleteSelect(rel, custom_admin_site)\n    url = w.get_url()\n    self.assertEqual(url, '/custom/autocomplete/')\n", "def test_optgroups(self):\n    beatles = Band.objects.create(name='The Beatles', style='rock')\n    who = Band.objects.create(name='The Who', style='rock')\n    album = Album.objects.create(name='Test Album', band=beatles)\n    album.featured_bands.add(who)\n    form = AlbumForm(instance=album)\n    widget = form['featuring'].field.widget\n    optgroups = widget.optgroups(name='featuring', value=[who.pk])\n    self.assertEqual(len(optgroups), 1)\n    self.assertEqual(len(optgroups[0][1]), 1)\n    self.assertEqual(optgroups[0][1][0]['value'], str(who.pk))\n    self.assertEqual(optgroups[0][1][0]['label'], 'The Who')\n", "    def test_optgroups(self):\n        beatles = Band.objects.create(name='The Beatles', style='rock')\n        who = Band.objects.create(name='The Who', style='rock')\n        album = Album.objects.create(name='Rubber Soul', band=beatles)\n        album.featuring.add(beatles, who)\n        form = AlbumForm(instance=album)\n        opts = form['band'].field.widget.optgroups('band', album.band_id)\n        self.assertEqual(len(opts), 1)\n        self.assertEqual(opts[0][1][0]['value'], str(album.band_id))\n        self.assertEqual(opts[0][1][0]['label'], 'The Beatles')\n        self.assertTrue(opts[0][1][0]['selected'])\n", "def test_optgroups_with_no_selected_choices(self):\n    beatles = Band.objects.create(name='The Beatles', style='rock')\n    who = Band.objects.create(name='The Who', style='rock')\n    form = AlbumForm(initial={'band': None})\n    output = form['band'].field.widget.optgroups('my_field', None, None)\n    self.assertEqual(len(output), 1)\n    self.assertEqual(len(output[0][1]), 0)\n", "    def test_optgroups(self):\n        beatles = Band.objects.create(name='The Beatles', style='rock')\n        who = Band.objects.create(name='The Who', style='rock')\n        rel = Album._meta.get_field('band')\n        w = AutocompleteSelect(rel, admin.site)\n        groups = w.optgroups('field_name', [beatles.uuid])\n        self.assertEqual(len(groups), 1)\n        self.assertEqual(len(groups[0][1]), 1)\n        option = groups[0][1][0]\n        self.assertEqual(option['value'], beatles.uuid)\n        self.assertEqual(option['label'], 'The Beatles')\n        self.assertTrue(option['selected'])\n"], "sample_768": ["def test_time_series_split_edge_cases():\n    # Test that TimeSeriesSplit behaves correctly when\n    # n_samples equals n_splits or n_samples is less than n_splits\n    n_samples = 5\n    X = np.ones(n_samples)\n\n    # n_samples equals n_splits\n    tscv = TimeSeriesSplit(n_splits=n_samples)\n    splits = tscv.split(X)\n    for i, (train, test) in enumerate(splits):\n        assert_array_equal(train, np.arange(i + 1))\n        assert_array_equal(test, np.array([i + 1]))\n\n    # n_samples is less than n_splits\n    tscv = TimeSeriesSplit(n_splits=n_samples + 1)\n    assert_raises(ValueError, next, tscv.split(X))\n\n    # n_samples is 1\n    X = np.ones(1)\n    tscv = TimeSeriesSplit(n_splits=2)\n    assert_raises(ValueError, next, tscv.split(X))\n\n    # max_train_size is larger than n_samples\n    tscv = TimeSeriesSplit(n_splits=2, max_train_size=n_samples + 1)\n    assert_raises(ValueError, next, tscv.split(X))\n", "def test_leave_one_p_group_out_overlapping_groups():\n    groups = np.array([1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3])\n    X = y = np.ones(len(groups))\n    n_groups = len(np.unique(groups))\n    n_splits = comb(n_groups, p=2)\n    cv = LeavePGroupsOut(n_groups=2)\n    splits = list(cv.split(X, y, groups))\n    assert len(splits) == n_splits\n\n    for train, test in splits:\n        assert len(train) > 0\n        assert len(test) > 0\n        assert len(np.intersect1d(train, test)) == 0\n        assert len(np.union1d(train, test)) == len(groups)\n\n        test_groups = np.unique(groups[test])\n        assert len(test_groups) == 2\n        assert len(np.intersect1d(test_groups, np.unique(groups[train]))) == 0\n", "def test_stratified_kfold_on_y_with_unhashable_types():\n    # Test that StratifiedKFold works with y containing unhashable types.\n    X = np.arange(10)\n    y = [set([1, 2]), set([2, 3]), set([3, 4]), set([4, 5]), set([5, 6]),\n         set([6, 7]), set([7, 8]), set([8, 9]), set([9, 10]), set([10, 11])]\n    # Check that StratifiedKFold fails with unhashable types\n    with pytest.raises(TypeError):\n        StratifiedKFold().split(X, y)\n    # Check that StratifiedShuffleSplit fails with unhashable types\n    with pytest.raises(TypeError):\n        StratifiedShuffleSplit().split(X, y)\n", "def test_leave_one_out():\n    # Test that LeaveOneOut cross-validation object returns the correct\n    # train/test folds.\n    for i in range(1, 10):\n        loo = LeaveOneOut()\n        X = np.ones(i)\n        for j, (train, test) in enumerate(loo.split(X)):\n            assert_array_equal(test, np.array([j]))\n            assert_array_equal(train, np.delete(np.arange(i), j))\n\n    # Test that LeaveOneOut can handle arrays with more than one feature.\n    X = np.ones((10, 5))\n    for train, test in loo.split(X):\n        assert_array_equal(train, np.delete(np.arange(10), test))\n        assert_array_equal(test, np.array([0]))\n\n    # Test that LeaveOneOut raises an error when given an empty array.\n    X = np.ones(0)\n    assert_raise_message(ValueError, \"Found array with 0 sample\\(s\\).\",\n                         next, LeaveOneOut().split(X))\n\n    # Test that LeaveOneOut raises an error when given an array with only one\n    # element.\n    X = np.ones(1)\n    assert_raise_message(ValueError, \"Found array with 1 sample\\(s\\).\",\n                         next, LeaveOneOut().split(X))\n", "def test_leave_one_out():\n    # Make sure LOO can handle a single sample\n    loo = LeaveOneOut()\n    assert_equal(loo.get_n_splits(X=np.ones(1)), 1)\n\n    # Check that the returned splits are indeed a single sample\n    for train, test in loo.split(X=np.ones(10)):\n        assert_equal(len(train), 9)\n        assert_equal(len(test), 1)\n\n    # Check that all samples are used in a test set\n    collected_test_samples = set()\n    for _, test in loo.split(X=np.arange(10)):\n        collected_test_samples.update(test)\n    assert_equal(collected_test_samples, set(range(10)))\n\n    # Check that the test sample is always different\n    previous_test = None\n    for _, test in loo.split(X=np.arange(10)):\n        assert_not_equal(previous_test, test)\n        previous_test = test\n\n    # Check that the splits are non-overlapping\n    for train, test in loo.split(X=np.arange(10)):\n        assert_array_equal(np.intersect1d(train, test), [])\n"], "sample_1178": ["def test_Element():\n    e1 = Element('x', (1, 2, 3))\n    e2 = Element('x', (1, 2, 3), strides=(4, 5, 6), offset=7)\n    assert e1.symbol == 'x'\n    assert e1.indices == (1, 2, 3)\n    assert e1.strides is None\n    assert e1.offset is None\n    assert e1.args == ('x', (1, 2, 3), None, None)\n    assert e1.func(*e1.args) == e1\n\n    assert e2.strides == (4, 5, 6)\n    assert e2.offset == 7\n    assert e2.args == ('x', (1, 2, 3), (4, 5, 6), 7)\n    assert e2.func(*e2.args) == e2\n\n    e3 = Element('x', 'ijk')\n    assert e3.indices == (Symbol('i'), Symbol('j'), Symbol('k'))\n    assert e3.strides is None\n    assert e3.offset is None\n    assert e3.args == ('x', (Symbol('i'), Symbol('j'), Symbol('k')), None, None)\n    assert e3.func(*e3.args) == e3\n", "def test_Element():\n    e = Element('x')\n    assert e.symbol == 'x'\n    assert e.indices == ()\n    assert e.strides == None\n    assert e.offset == None\n\n    e = Element('x', indices='i')\n    assert e.indices == ('i',)\n\n    e = Element('x', strides='l')\n    assert e.strides == ('l',)\n\n    e = Element('x', offset='o')\n    assert e.offset == 'o'\n\n    e = Element('x', indices='ijk', strides='lmn', offset='o')\n    assert e.indices == ('i', 'j', 'k')\n    assert e.strides == ('l', 'm', 'n')\n    assert e.offset == 'o'\n\n    raises(TypeError, lambda: Element('x', strides='i', offset='o'))\n    raises(TypeError, lambda: Element('x', indices='i', offset='o'))\n", "def test_Element():\n    elem = Element(x, 'ijk')\n    assert elem.symbol == x\n    assert elem.indices == (symbols('i j k'))\n    elem2 = Element(x, ('i', 'j', 'k'))\n    assert elem2.indices == (symbols('i j k'))\n    elem3 = Element(x, (i, j, k))\n    assert elem3.indices == (i, j, k)\n    assert elem == elem2 == elem3\n\n    elem4 = Element(x, 'i')\n    assert elem4.indices == (symbols('i'),)\n    elem5 = Element(x, ('i',))\n    assert elem5.indices == (symbols('i'),)\n    elem6 = Element(x, i)\n    assert elem6.indices == (i,)\n    assert elem4 == elem5 == elem6\n\n    elem7 = Element(x, 'i', strides='lmn', offset='o')\n    assert elem7.strides == (symbols('l m n'))\n    assert elem7.offset == symbols('o')\n    elem8 = Element(x, 'i', strides=('l', 'm', 'n'), offset='o')\n    assert elem8.strides == (symbols('l m n'))\n    assert elem8.offset == symbols('o')\n    elem9 = Element(x, 'i', strides=(l, m, n), offset='o')\n    assert elem9.strides == (l, m, n)\n    assert elem9.offset == symbols('o')\n    elem10 = Element(x, 'i', strides=(l, m, n), offset=o)\n    assert elem10.strides == (l, m, n)\n    assert elem10.offset == o\n    assert elem7 == elem8 == elem9 == elem10\n", "def test_Element():\n    # Testing Element\n    elem1 = Element('x', 'ijk')\n    assert elem1.symbol == 'x'\n    assert elem1.indices == ('i', 'j', 'k')\n    assert elem1.strides == None\n    assert elem1.offset == None\n    assert str(elem1) == 'Element(x, indices=(\"i\", \"j\", \"k\"))'\n    assert elem1.func(*elem1.args) == elem1\n    elem1b = Element('x', 'ijk')\n    assert elem1 == elem1b\n    elem2 = Element('x', 'ijk', strides='lmn', offset='o')\n    assert elem2.indices == ('i', 'j', 'k')\n    assert elem2.strides == ('l', 'm', 'n')\n    assert elem2.offset == 'o'\n    assert str(elem2) == 'Element(x, indices=(\"i\", \"j\", \"k\"), strides=(\"l\", \"m\", \"n\"), offset=\"o\")'\n\n    # Testing Element with Pointer\n    p = Pointer('x')\n    elem3 = p[1, 2, 3]\n    assert elem3.symbol == 'x'\n    assert elem3.indices == (1, 2, 3)\n    assert elem3.strides == None\n    assert elem3.offset == None\n    assert str(elem3) == 'Element(x, indices=(1, 2, 3))'\n    assert elem3.func(*elem3.args) == elem3\n    elem3b = p[1, 2, 3]\n    assert elem3 == elem3b\n", "def test_FunctionCall_with_tuple_input():\n    # Test that FunctionCall can be instantiated with a tuple as input\n    # https://github.com/sympy/sympy/issues/22195\n    fc = FunctionCall('power', (x, 3))\n    assert fc.args == ('power', (x, 3))\n\n    # Test that FunctionCall can be instantiated with a list as input\n    fc = FunctionCall('power', [x, 3])\n    assert fc.args == ('power', (x, 3))\n\n    # Test that FunctionCall can be instantiated with multiple arguments\n    fc = FunctionCall('power', x, 3)\n    assert fc.args == ('power', (x, 3))\n"], "sample_448": ["def test_validate_expression_with_q(self):\n    constraint = models.UniqueConstraint(\n        Lower(\"name\"),\n        name=\"name_lower_uniq\",\n        condition=models.Q(color=\"red\"),\n    )\n    msg = \"Constraint \u201cname_lower_uniq\u201d is violated.\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        constraint.validate(\n            UniqueConstraintProduct,\n            UniqueConstraintProduct(name=self.p1.name.upper(), color=\"red\"),\n        )\n    constraint.validate(\n        UniqueConstraintProduct,\n        UniqueConstraintProduct(name=self.p1.name.upper(), color=\"blue\"),\n    )\n    constraint.validate(\n        UniqueConstraintProduct,\n        UniqueConstraintProduct(name=\"another-name\", color=\"red\"),\n    )\n    # Existing instances have their existing row excluded.\n    constraint.validate(UniqueConstraintProduct, self.p1)\n    # Unique field is excluded.\n    constraint.validate(\n        UniqueConstraintProduct,\n        UniqueConstraintProduct(name=self.p1.name.upper(), color=\"red\"),\n        exclude={\"name\"},\n    )\n    # Field from a condition is excluded.\n    constraint.validate(\n        UniqueConstraintProduct,\n        UniqueConstraintProduct(name=self.p1.name.upper(), color=\"red\"),\n        exclude={\"color\"},\n    )\n", "def test_custom_violation_error_message(self):\n    constraint = UniqueConstraintProduct._meta.constraints[0]\n    constraint.violation_error_message = \"Custom violation error message.\"\n    msg = \"Custom violation error message.\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        constraint.validate(UniqueConstraintProduct, self.p1)\n", "def test_clone(self):\n    fields = [\"foo\", \"bar\"]\n    name = \"unique_fields\"\n    constraint = models.UniqueConstraint(fields=fields, name=name)\n    cloned_constraint = constraint.clone()\n    self.assertEqual(cloned_constraint.fields, fields)\n    self.assertEqual(cloned_constraint.name, name)\n\n    fields = [\"foo\", \"bar\"]\n    name = \"unique_fields\"\n    condition = models.Q(foo=models.F(\"bar\"))\n    constraint = models.UniqueConstraint(\n        fields=fields, name=name, condition=condition\n    )\n    cloned_constraint = constraint.clone()\n    self.assertEqual(cloned_constraint.fields, fields)\n    self.assertEqual(cloned_constraint.name, name)\n    self.assertEqual(cloned_constraint.condition, condition)\n\n    fields = [\"foo\"]\n    name = \"unique_fields\"\n    constraint = models.UniqueConstraint(\n        fields=fields,\n        name=name,\n        deferrable=models.Deferrable.DEFERRED,\n    )\n    cloned_constraint = constraint.clone()\n    self.assertEqual(cloned_constraint.fields, fields)\n    self.assertEqual(cloned_constraint.name, name)\n    self.assertEqual(\n        cloned_constraint.deferrable, models.Deferrable.DEFERRED\n    )\n\n    fields = [\"foo\", \"bar\"]\n    name = \"unique_fields\"\n    include = [\"baz_1\", \"baz_2\"]\n    constraint = models.UniqueConstraint(\n        fields=fields, name=name, include=include\n    )\n    cloned_constraint = constraint.clone()\n    self.assertEqual(cloned_constraint.fields, fields)\n    self.assertEqual(cloned_constraint.name, name)\n    self.assertEqual(cloned_constraint.include, tuple(include))\n\n    fields = [\"foo\", \"bar\"]\n    name = \"unique_fields\"\n    opclasses = [\"varchar_pattern_ops\", \"text_pattern_ops\"]\n    constraint = models.UniqueConstraint(\n        fields=fields, name=name, opclasses=opclasses\n    )\n    cloned_constraint = constraint.clone()\n    self.assertEqual(cloned_constraint.fields, fields)\n    self.assertEqual(cloned_constraint.name, name)\n    self.assertEqual(cloned_constraint.opclasses, opclasses)\n\n    name = \"unique_fields\"\n    constraint = models.UniqueConstraint(\n        Lower(\"title\"),\n        name=name,\n    )\n    cloned_constraint = constraint.clone()\n    self.assertEqual(cloned_constraint.name, name)\n    self.assertEqual(cloned_constraint.expressions, constraint.expressions)\n", "    def test_deferrable_immediate(self):\n        constraint = UniqueConstraintDeferrable._meta.constraints[1]\n        obj = UniqueConstraintDeferrable(name=\"product\", shelf=\"front\")\n        with self.assertRaises(ValidationError):\n            constraint.validate(UniqueConstraintDeferrable, obj)\n", "    def test_include_model_validation(self):\n        obj_1 = UniqueConstraintInclude.objects.create(name=\"p1\", color=\"red\")\n        obj_2 = UniqueConstraintInclude.objects.create(name=\"p2\", color=\"blue\")\n        obj_3 = UniqueConstraintInclude(name=\"p1\", color=\"blue\")\n        msg = \"Unique constraint uniqueconstraintinclude with this Name already exists.\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            obj_3.validate_constraints()\n        # Fields not in the include are excluded.\n        obj_3.color = \"red\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            obj_3.validate_constraints()\n        # Existing instances have their existing row excluded.\n        obj_1.validate_constraints()\n        # Unique field is excluded.\n        obj_3.validate_constraints(exclude={\"name\"})\n"], "sample_263": ["def test_dumpdata_progressbar_output_file(self):\n    \"\"\"\n    Dumpdata shows no progress bar when --output is set.\n    \"\"\"\n    management.call_command('loaddata', 'fixture1.json', verbosity=0)\n    new_io = StringIO()\n    with NamedTemporaryFile() as file:\n        options = {\n            'format': 'json',\n            'stdout': new_io,\n            'stderr': new_io,\n            'output': file.name,\n            'verbosity': 1,\n        }\n        management.call_command('dumpdata', 'fixtures', **options)\n        self.assertEqual(new_io.getvalue(), '')\n", "def test_dumpdata_with_non_ascii_model_name(self):\n    management.call_command('loaddata', 'fixture1.json', verbosity=0)\n    self._dumpdata_assert(\n        ['fixtures.Article'],\n        '[{\"pk\": 2, \"model\": \"fixtures.article\", \"fields\": {\"headline\": \"Poker has no place on ESPN\", '\n        '\"pub_date\": \"2006-06-16T12:00:00\"}}, {\"pk\": 3, \"model\": \"fixtures.article\", \"fields\": {\"headline\": '\n        '\"Time to reform copyright\", \"pub_date\": \"2006-06-16T13:00:00\"}}]',\n        filename='dumpdata_\u00e9.json'\n    )\n", "def test_dumpdata_with_concrete_proxy_models(self):\n    \"\"\"\n    If a concrete model and its proxy model are both dumped, they should be\n    serialized separately.\n    \"\"\"\n    spy = Spy.objects.create(name='Paul')\n    proxy_spy = ProxySpy.objects.create(name='Paul')\n    self._dumpdata_assert(\n        ['fixtures.Spy', 'fixtures.ProxySpy'],\n        '[{\"pk\": %d, \"model\": \"fixtures.spy\", \"fields\": {\"cover_blown\": false}}, {\"pk\": %d, \"model\": '\n        '\"fixtures.spy\", \"fields\": {\"cover_blown\": false}}]' % (spy.pk, proxy_spy.pk)\n    )\n", "def test_dumpdata_with_exclude_models_from_app(self):\n    # Load fixture 1. Single JSON file, with two objects.\n    management.call_command('loaddata', 'fixture1.json', verbosity=0)\n\n    # Try to exclude some models from the fixtures app\n    self._dumpdata_assert(\n        ['fixtures'],\n        '[{\"pk\": 1, \"model\": \"fixtures.category\", \"fields\": {\"description\": \"Latest news stories\", \"title\": '\n        '\"News Stories\"}}]',\n        exclude_list=['fixtures.Article']\n    )\n", "def test_dumpdata_with_progress_output_and_error(self):\n    \"\"\"Test dumpdata with progress output and an error.\"\"\"\n    management.call_command('loaddata', 'fixture1.json', verbosity=0)\n    new_io = StringIO()\n    new_io.isatty = lambda: True\n    with NamedTemporaryFile() as file:\n        options = {\n            'format': 'json',\n            'stdout': new_io,\n            'stderr': new_io,\n            'output': file.name,\n        }\n        with mock.patch('django.core.management.commands.dumpdata serializers.serialize', side_effect=Exception('Mock error')):\n            management.call_command('dumpdata', 'fixtures', **options)\n            command_output = new_io.getvalue()\n            self.assertIn('[' + '.' * ProgressBar.progress_width + ']', command_output)\n            self.assertIn('Mock error', command_output)\n"], "sample_512": ["def test_subplots_adjust():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3])\n    plt.subplots_adjust(left=0.5, bottom=0.5, right=0.5, top=0.5)\n    assert fig.subplotpars.left == 0.5\n    assert fig.subplotpars.right == 0.5\n    assert fig.subplotpars.bottom == 0.5\n    assert fig.subplotpars.top == 0.5\n    plt.subplots_adjust(left=None, bottom=None, right=None, top=None)\n    assert fig.subplotpars.left != 0.5\n    assert fig.subplotpars.right != 0.5\n    assert fig.subplotpars.bottom != 0.5\n    assert fig.subplotpars.top != 0.5\n", "def test_fignum_exists():\n    plt.figure()\n    assert plt.fignum_exists(1)\n    assert not plt.fignum_exists(2)\n    plt.close('all')\n    assert not plt.fignum_exists(1)\n\n    plt.figure(\"test\")\n    assert plt.fignum_exists(\"test\")\n    assert not plt.fignum_exists(\"not_test\")\n    plt.close('all')\n    assert not plt.fignum_exists(\"test\")\n\n    fig = plt.figure()\n    plt.close(fig)\n    assert not plt.fignum_exists(fig.number)\n", "def test_subplot2grid_positioning():\n    fig = plt.figure()\n    ax = plt.subplot2grid((2, 3), (0, 0), colspan=2)\n    ax1 = plt.subplot2grid((2, 3), (0, 0), colspan=2)\n    assert ax is ax1\n\n    ax2 = plt.subplot2grid((2, 3), (1, 1))\n    assert ax is not ax2\n\n    ax3 = plt.subplot2grid((2, 3), (0, 0), colspan=2, rowspan=1)\n    assert ax is ax3\n\n    ax4 = plt.subplot2grid((2, 3), (0, 0), colspan=2, rowspan=2)\n    assert ax is not ax4\n\n    ax5 = plt.subplot2grid((2, 3), (0, 1), colspan=2)\n    assert ax is not ax5\n\n    ax6 = plt.subplot2grid((2, 3), (1, 0), colspan=2)\n    assert ax is not ax6\n", "def test_rcParamsOrig_sanity():\n    # Ensure rcParamsOrig is a deep copy of rcParamsDefault at import time.\n    assert rcParamsOrig == rcParamsDefault\n    rcParams['backend'] = 'different backend'\n    assert rcParams['backend'] != rcParamsOrig['backend']\n", "def test_subplot_mosaic():\n    fig, ax_dict = plt.subplot_mosaic(\n        [\n            ['A', 'B'],\n            ['C', 'D']\n        ],\n        figsize=(8, 6),\n        gridspec_kw=dict(width_ratios=[1, 1], height_ratios=[2, 1]),\n        empty_sentinel='.'\n    )\n    assert len(ax_dict) == 3\n    assert ax_dict['A'].get_position().x0 < ax_dict['B'].get_position().x0\n    assert ax_dict['A'].get_position().y0 > ax_dict['C'].get_position().y0\n    plt.close(fig)\n\n    fig, ax_dict = plt.subplot_mosaic(\n        [\n            ['A', '.'],\n            ['C', 'D']\n        ],\n        figsize=(8, 6),\n        gridspec_kw=dict(width_ratios=[1, 1], height_ratios=[2, 1])\n    )\n    assert len(ax_dict) == 3\n    assert ax_dict['A'].get_position().x0 < ax_dict['C'].get_position().x0\n    assert ax_dict['A'].get_position().y0 > ax_dict['C'].get_position().y0\n    plt.close(fig)\n\n    fig, ax_dict = plt.subplot_mosaic(\n        [\n            ['A', 'B'],\n            ['C', 'D']\n        ],\n        figsize=(8, 6),\n        gridspec_kw=dict(width_ratios=[1, 2], height_ratios=[2, 1])\n    )\n    assert len(ax_dict) == 4\n    assert ax_dict['A'].get_position().width < ax_dict['B'].get_position().width\n    plt.close(fig)\n"], "sample_737": ["def test_vectorizer_lowercase():\n    # raw documents\n    data = [\"This IS a Test.\", \"This is a test.\"]\n\n    for vectorizer in (TfidfVectorizer(), CountVectorizer()):\n        vectorizer.set_params(lowercase=True)\n        transformed_data = vectorizer.fit_transform(data)\n        assert_array_equal(transformed_data.toarray()[0],\n                          transformed_data.toarray()[1])\n\n        vectorizer.set_params(lowercase=False)\n        transformed_data = vectorizer.fit_transform(data)\n        assert_not_equal(transformed_data.toarray()[0],\n                        transformed_data.toarray()[1])\n", "def test_tfidfvectorizer_with_numpy_array():\n    # Test that TfidfVectorizer works with a numpy array as input\n    data = np.array(ALL_FOOD_DOCS)\n    tv = TfidfVectorizer()\n    tv.fit(data)\n    X = tv.transform(data)\n    assert_equal(X.shape, (len(ALL_FOOD_DOCS), len(tv.vocabulary_)))\n", "def test_vectorizer_input_validation():\n    # Test input validation for different types of vectorizers\n    for vec in [HashingVectorizer(), CountVectorizer(), TfidfVectorizer()]:\n        # Test with single string input\n        assert_raise_message(\n            ValueError,\n            \"Iterable over raw text documents expected, string object received.\",\n            vec.fit_transform, \"hello world!\"\n        )\n\n        # Test with non-iterable input\n        assert_raise_message(\n            ValueError,\n            \"Iterable over raw text documents expected, string object received.\",\n            vec.fit_transform, 123\n        )\n\n        # Test with list of non-string inputs\n        assert_raise_message(\n            ValueError,\n            \"Iterable over raw text documents expected, string object received.\",\n            vec.fit_transform, [1, 2, 3]\n        )\n\n        # Test with generator of non-string inputs\n            yield 1\n            yield 2\n            yield 3\n        assert_raise_message(\n            ValueError,\n            \"Iterable over raw text documents expected, string object received.\",\n            vec.fit_transform, gen_non_string()\n        )\n", "def test_vectorizer_stop_words():\n    # test that stop_words is properly set and retrieved.\n    vect = CountVectorizer()\n    vect.stop_words = ENGLISH_STOP_WORDS\n    assert_equal(vect.get_stop_words(), ENGLISH_STOP_WORDS)\n\n    # test that stop_words cannot be set to an invalid value\n    message = \"not a built-in stop list: _bad_stop_list_\"\n    exception = ValueError\n        vect.stop_words = \"_bad_stop_list_\"\n    assert_raise_message(exception, message, func)\n", "def test_vectorizer_input_type():\n    # Test the input type check\n    for vectorizer in (CountVectorizer(), TfidfVectorizer(),\n                        HashingVectorizer()):\n        data = [\"hello world\", b\"hello world\", [b\"hello world\"], \"hello world\"]\n        for x in data:\n            assert_raises(ValueError, vectorizer.fit_transform, x)\n        assert_warns_message(\n            ValueError, \"Iterable over raw text documents expected, \"\n            \"string object received.\",\n            vectorizer.fit_transform, [\"hello world\"])\n        assert_warns_message(\n            ValueError, \"Iterable over raw text documents expected, \"\n            \"string object received.\",\n            vectorizer.fit, [\"hello world\"])\n        assert_warns_message(\n            ValueError, \"Iterable over raw text documents expected, \"\n            \"string object received.\",\n            vectorizer.transform, [\"hello world\"])\n"], "sample_1188": ["def test_pretty_printing_symbols():\n    assert pretty(a) == 'a'\n    assert pretty(b) == 'b'\n    assert pretty(c) == 'c'\n", "def test_issue_23058_generalized():\n    from sympy import symbols, sin, cos, pi, UnevaluatedExpr\n\n    delop = Del()\n    CC_   = CoordSys3D(\"C\")\n    y     = CC_.y\n    xhat  = CC_.i\n\n    t = symbols(\"t\")\n    eps, mu = symbols(\"eps mu\", positive=True)\n    Bx = 2 * eps**(-11) * cos(eps**5 * t) * sin(eps**(-3) * y)\n    vecB = Bx * xhat\n    vecE = (1/eps) * Integral(delop.cross(vecB/mu).doit(), t)\n    vecE = vecE.doit()\n\n    vecB_str = \"\"\"\\", "def test_pow_printing():\n    from sympy import symbols\n    x = symbols('x')\n    assert pretty(x**2) == ' 2\\nx '\n    assert pretty(2**x) == 'x\\n2 '\n    assert pretty(x**(-2)) == '  -2\\nx '\n    assert pretty((-x)**2) == ' 2\\nx '\n    assert pretty((-2)**x) == 'x\\n-2 '\n    assert pretty((-x)**(-2)) == ' -2\\n x '\n    assert pretty(2**(x**2)) == ' x 2\\n2   '\n    assert pretty(x**(2*x)) == '2\u22c5x\\nx '\n", "def test_tensor_printing():\n    from sympy.tensor import IndexedBase\n    from sympy import symbols, tensorcontraction, tensorproduct\n    i, j, k, l, m, n = symbols('i j k l m n')\n    A = IndexedBase('A')\n    B = IndexedBase('B')\n    C = IndexedBase('C')\n    D = IndexedBase('D')\n\n    t = A(i, j)*B(k, l)\n    pform = upretty(t)\n    assert pform == \"\"\"\\", "def test_pretty_printing_unicode():\n    from sympy import symbols, sin, cos, pi\n    from sympy.vector import CoordSys3D, Del, Vector, express\n\n    C = CoordSys3D(\"C\")\n    v1 = 3*C.x*C.y\n    v2 = C.x**2*C.k\n    v3 = C.x*C.y*C.z\n    v4 = 2*C.i - 3*C.j\n    v5 = 2*C.i + 3*C.j\n\n    upretty_v1 = \"\"\"\\\n    2\\n\\"], "sample_104": ["    def test_hashed_name_with_file_hash_none(self):\n        relpath = \"cached/ignored.css\"\n        parsed_name = urlsplit(unquote(relpath))\n        clean_name = parsed_name.path.strip()\n        file_hash = storage.staticfiles_storage.file_hash(clean_name)\n        self.assertIsNone(file_hash)\n        hashed_name = storage.staticfiles_storage.hashed_name(relpath, content=None)\n        self.assertEqual(hashed_name, relpath)\n", "    def test_hash_key_unique(self):\n        # Test that different files have different hash keys.\n        file1 = \"test/file1.txt\"\n        file2 = \"test/file2.txt\"\n        self.assertNotEqual(storage.staticfiles_storage.hash_key(file1), storage.staticfiles_storage.hash_key(file2))\n", "    def setUp(self):\n        super().setUp()\n        self._temp_dir = temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(temp_dir, 'test'))\n        self.addCleanup(shutil.rmtree, temp_dir)\n", "    def setUp(self):\n        super().setUp()\n        storage.staticfiles_storage.hashed_files.clear()  # avoid cache interference\n", "    def setUp(self):\n        super().setUp()\n\n        temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(temp_dir, 'test'))\n        self._temp_dir = temp_dir\n        self.addCleanup(shutil.rmtree, temp_dir)\n\n        self._get_filename_path = lambda filename: os.path.join(temp_dir, 'test', filename)\n\n        self._write_file('test/invalid.css', b'url(\"missing.png\")')\n\n        with self.modify_settings(STATICFILES_DIRS={'append': temp_dir}):\n            finders.get_finder.cache_clear()\n            err = StringIO()\n            call_command('collectstatic', interactive=False, verbosity=0, stderr=err)\n"], "sample_700": ["def test_xfail_with_strict_option(pytester: Pytester) -> None:\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        xfail_strict = true\n    \"\"\"\n    )\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.xfail\n            pass\n    \"\"\"\n    )\n    result = pytester.runpytest(\"-rxX\")\n    result.stdout.fnmatch_lines([\"*XPASS*test_func*\"])\n    assert result.ret == 0\n", "def test_xfail_in_setup_item(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n            pytest.xfail(\"setup xfail\")\n            pass\n    \"\"\"\n    )\n    result = pytester.runpytest(\"-rX\")\n    result.stdout.fnmatch_lines(\n        [\n            \"XPASS test_xfail_in_setup_item.py::test_pass\",\n            \"setup xfail\",\n        ]\n    )\n", "def test_dynamic_skipif_markers_with_custom_namespace(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n            return {\"my_var\": True}\n    \"\"\"\n    )\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.skipif(\"my_var\")\n            pass\n\n        @pytest.mark.skipif(\"not my_var\")\n            assert True\n    \"\"\"\n    )\n    result = pytester.runpytest(\"-rs\")\n    result.stdout.fnmatch_lines([\"*1 passed*1 skipped*\"])\n", "    def test_id_duplicates(self, pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n            @pytest.mark.parametrize('arg', ['value1', 'value1'])\n                assert arg\n        \"\"\"\n        )\n        result = pytester.runpytest()\n        result.stdout.fnmatch_lines(\n            [\n                \"*1 failed*1 error*\",\n                \"*test_param.py:3: in test_param*\",\n                \"*ValueError: Duplicate id: 'value1' for arg values: ('value1',)*\",\n            ]\n        )\n", "def test_parametrize_fails_on_missing_param_ids(pytester: Pytester) -> None:\n    p = pytester.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.parametrize(\"a\", [1])\n            pass\n    \"\"\"\n    )\n    result = pytester.runpytest(p)\n    result.stdout.fnmatch_lines(\n        [\n            \"*1 failed*\",\n            \"*_parametrize_missing_param_ids*\",\n            \"*test_func*parameter 'b' not specified in call funcargs*\",\n        ]\n    )\n"], "sample_772": ["def test_empty_feature_importances(estimator):\n    # Test that feature importances are returned as an empty array when there\n    # are no features.\n    est = estimator()\n    assert_array_equal(est.feature_importances_, np.array([]))\n", "def test_feature_importances_dtype(name):\n    ForestEstimator = FOREST_ESTIMATORS[name]\n    est = ForestEstimator(random_state=0)\n    X = np.array([[0, 1], [1, 2]], dtype=np.float32)\n    y = np.array([0, 1], dtype=np.int32)\n    est.fit(X, y)\n    assert_equal(est.feature_importances_.dtype, np.float64)\n", "def test_feature_importances_max_depth(name):\n    # Test if feature importances are correctly computed when max_depth\n    # is set\n\n    X, y = datasets.make_classification(n_samples=200, n_features=10,\n                                       n_informative=3, n_redundant=0,\n                                       n_repeated=0, random_state=0)\n\n    ForestEstimator = FOREST_ESTIMATORS[name]\n    clf = ForestEstimator(random_state=0, max_depth=1).fit(X, y)\n\n    # The first feature is the most informative. We expect the feature\n    # importances to be higher for this feature than for the others.\n    importances = clf.feature_importances_\n    assert_greater(importances[0], importances[1:].max())\n\n", "def test_sample_weight_in_inconsistent_target(name):\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n    rng = np.random.RandomState(0)\n    X = iris.data\n    y = iris.target\n    sample_weight = rng.rand(X.shape[0])\n\n    # classify with one-hot encoded targets\n    y_one_hot = np.array([np.where(y == i, 1, 0) for i in range(3)]).T\n    clf = ForestClassifier(random_state=0)\n    clf.fit(X, y_one_hot, sample_weight=sample_weight)\n    pred = clf.predict(X)\n    assert_array_equal(pred, y)\n\n    # classify with non one-hot encoded targets\n    clf = ForestClassifier(random_state=0)\n    clf.fit(X, y, sample_weight=sample_weight)\n    pred = clf.predict(X)\n    assert_array_equal(pred, y)\n", "def check_max_features(name):\n    X, y = iris.data, iris.target\n\n    ForestEstimator = FOREST_ESTIMATORS[name]\n\n    # Smoke test for max_features=None, 'auto', 'sqrt' and 'log2'\n    est = ForestEstimator(n_estimators=1, max_features='auto', random_state=0)\n    est.fit(X, y)\n    assert_equal(est.n_features_, X.shape[1])\n    assert_greater(len(est.estimators_[0].feature_importances_.nonzero()[0]), 0)\n\n    est = ForestEstimator(n_estimators=1, max_features='sqrt', random_state=0)\n    est.fit(X, y)\n    assert_greater(len(est.estimators_[0].feature_importances_.nonzero()[0]), 0)\n\n    est = ForestEstimator(n_estimators=1, max_features='log2', random_state=0)\n    est.fit(X, y)\n    assert_greater(len(est.estimators_[0].feature_importances_.nonzero()[0]), 0)\n\n    est = ForestEstimator(n_estimators=1, max_features=None, random_state=0)\n    est.fit(X, y)\n    assert_greater(len(est.estimators_[0].feature_importances_.nonzero()[0]), 0)\n\n    # Test if max_features is a list\n    est = ForestEstimator(n_estimators=1, max_features=[1, 2, 3], random_state=0)\n    assert_raises(ValueError, est.fit, X, y)\n\n    # Test if max_features is a tuple\n    est = ForestEstimator(n_estimators=1, max_features=(1, 2, 3), random_state=0)\n    assert_raises(ValueError, est.fit, X, y)\n\n    # Test if max_features is a string\n    est = ForestEstimator(n_estimators=1, max_features=\"not a valid string\",\n                          random_state=0)\n    assert_raises(ValueError, est.fit, X, y)\n\n"], "sample_526": ["def test_DateFormatter_usetex():\n    time = mdates.date2num(datetime.datetime(2011, 1, 1, 0, 0, tzinfo=mdates.UTC))\n    formatter = mdates.DateFormatter('%Y-%b-%d %H:%M', usetex=False)\n    assert formatter(time) == '2011-Jan-01 00:00'\n    formatter = mdates.DateFormatter('%Y-%b-%d %H:%M', usetex=True)\n    assert formatter(time) == '$\\\\mathdefault{2011{-}Jan{-}01\\;00{:}00}$'\n", "def test_MicrosecondLocator():\n    locator = mdates.MicrosecondLocator(interval=5)\n    locator.create_dummy_axis()\n    locator.axis.set_view_interval(mdates.date2num(datetime.datetime(2011, 1, 1)),\n                                   mdates.date2num(datetime.datetime(2011, 1, 1)))\n    ticks = locator()\n    assert np.allclose(ticks, np.linspace(733717.0, 733718.0, 200000))\n", "def test_num2timedelta_non_zero_microseconds():\n    # Test conversion of non-zero microseconds\n    timedelta = mdates.num2timedelta(0.0000001)\n    assert timedelta == datetime.timedelta(microseconds=1)\n", "def test_AutoDateFormatter_callable_with_dateutil_timezones():\n    # Test that AutoDateFormatter handles dateutil timezones\n    import dateutil.tz\n    import pytz\n\n    tz = pytz.timezone('America/New_York')\n    dateutil_tz = dateutil.tz.gettz('America/New_York')\n\n        return [dt.strftime('%d-%m//%Y') for dt in dates]\n\n    formatter = mdates.AutoDateFormatter(mdates.AutoDateLocator())\n    formatter.scaled[-10] = callable_formatting_function\n\n    time = mdates.date2num(datetime.datetime(2014, 12, 25, tzinfo=tz))\n    assert formatter(time, tz=tz) == ['25-12//2014']\n\n    time = mdates.date2num(datetime.datetime(2014, 12, 25, tzinfo=dateutil_tz))\n    assert formatter(time, tz=dateutil_tz) == ['25-12//2014']\n", "def test_DateConverter():\n    converter = mdates.DateConverter()\n    date1 = datetime.datetime(2020, 1, 1, tzinfo=mdates.UTC)\n    date2 = datetime.datetime(2020, 1, 2, tzinfo=mdates.UTC)\n    axis = 'test_axis'\n    assert converter.axisinfo(date1.tzinfo, axis) == units.AxisInfo(\n        majloc=mdates.AutoDateLocator(tz=date1.tzinfo, interval_multiples=True),\n        majfmt=mdates.AutoDateFormatter(\n            mdates.AutoDateLocator(tz=date1.tzinfo, interval_multiples=True)),\n        label='',\n        default_limits=(datetime.date(1970, 1, 1), datetime.date(1970, 1, 2)))\n\n    assert converter.convert(date1, date1.tzinfo, axis) == mdates.date2num(date1)\n    assert converter.default_units(date1, axis) == date1.tzinfo\n    assert converter.default_units([date1, date2], axis) == date1.tzinfo\n\n    assert converter.default_units(datetime.date(2020, 1, 1), axis) is None\n    assert converter.default_units([datetime.date(2020, 1, 1),\n                                   datetime.date(2020, 1, 2)], axis) is None\n    assert converter.default_units('2020-01-01', axis) is None\n    assert converter.default_units(['2020-01-01', '2020-01-02'], axis) is None\n"], "sample_933": ["def test_gettext_pot_header(app):\n    app.builder.build_all()\n    pot_file = app.outdir / 'sphinx.pot'\n    assert pot_file.isfile()\n\n    result = pot_file.read_text()\n    assert 'Translator: Translator Name <translator@example.com>' in result\n    assert 'Language-Team: LANGUAGE <LL@li.org>' in result\n", "def test_gettext_catalogs_writeable(app):\n    app.builder.build_all()\n    catalog = (app.outdir / 'extapi.pot').read_text()\n    assert 'msgid \"\"' in catalog\n\n    # test writing non-empty catalog\n    with open(app.outdir / 'extapi.pot', 'r+', encoding='utf-8') as f:\n        content = f.read()\n        f.seek(0)\n        f.write('# dummy comment\\n')\n        f.write(content)\n        f.truncate()\n\n    app.builder.build_all()\n    catalog = (app.outdir / 'extapi.pot').read_text()\n    assert 'msgid \"\"' in catalog\n", "def test_gettext_uuid_origin(app):\n    app.builder.build_all()\n\n    result = (app.outdir / 'extapi.pot').read_text()\n    assert ': ' in result\n\n    uuid_pattern = r'[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}'\n\n        for line in pot.splitlines():\n            if ': ' in line:\n                match = re.search(uuid_pattern, line)\n                if match:\n                    yield match.group()\n\n    uuids = list(get_uuids(result))\n\n    # Ensure uniqueness of UUIDs in the .pot file\n    assert len(uuids) == len(set(uuids))\n", "def test_catalog_getting(app):\n    app.builder.build_all()\n\n    catalog = app.builder.catalogs['extapi']\n\n    # check that catalog has some messages\n    assert len(list(catalog)) > 0\n\n    # check Message object structure\n    message = next(iter(catalog))\n    assert isinstance(message, Message)\n    assert isinstance(message.text, str)\n    assert isinstance(message.locations, list)\n    assert isinstance(message.locations[0], tuple)\n    assert len(message.locations[0]) == 2\n    assert isinstance(message.uuids, list)\n    assert len(message.uuids) == len(message.locations)\n\n    # check that messages have locations and uuids\n    for msg in catalog:\n        assert len(msg.locations) > 0\n        assert len(msg.uuids) > 0\n", "def test_gettext_no_location_info_in_pot(app):\n    app.builder.build_all()\n\n    pot = (app.outdir / 'sphinx.pot').read_text()\n    assert '#: ' not in pot\n\n"], "sample_791": ["def test_ordinal_encoder_warning():\n    enc = OrdinalEncoder()\n    X = [['Male', 1], ['Female', 3]]\n    np.testing.assert_no_warnings(enc.fit_transform, X)\n", "def test_one_hot_encoder_feature_names_with_drop():\n    enc = OneHotEncoder(drop='first')\n    X = [['Male', 1, 'girl', 2, 3],\n         ['Female', 41, 'girl', 1, 10],\n         ['Male', 51, 'boy', 12, 3],\n         ['Male', 91, 'girl', 21, 30]]\n\n    enc.fit(X)\n    feature_names = enc.get_feature_names()\n    assert isinstance(feature_names, np.ndarray)\n\n    assert_array_equal(['x0_Female',\n                        'x1_41', 'x1_51', 'x1_91',\n                        'x2_boy', 'x2_girl',\n                        'x3_1', 'x3_2', 'x3_12', 'x3_21',\n                        'x4_10', 'x4_30'], feature_names)\n\n    feature_names2 = enc.get_feature_names(['one', 'two',\n                                            'three', 'four', 'five'])\n\n    assert_array_equal(['one_Female',\n                        'two_41', 'two_51', 'two_91',\n                        'three_boy', 'three_girl',\n                        'four_1', 'four_2', 'four_12', 'four_21',\n                        'five_10', 'five_30'], feature_names2)\n\n    with pytest.raises(ValueError, match=\"input_features should have length\"):\n        enc.get_feature_names(['one', 'two'])\n", "def test_ordinal_encoder_pandas():\n    pd = pytest.importorskip('pandas')\n\n    X_df = pd.DataFrame({'A': ['a', 'b'], 'B': [1, 2]})\n\n    Xtr = OrdinalEncoder().fit_transform(X_df)\n    assert_allclose(Xtr, [[0., 0.], [1., 1.]])\n\n    X_df = pd.DataFrame({'A': ['a', 'b', 'b', 'a'], 'B': [1, 2, 2, 1]})\n    enc = OrdinalEncoder(dtype='int64')\n    Xtr = enc.fit_transform(X_df)\n    assert_array_equal(Xtr, [[0, 0], [1, 1], [1, 1], [0, 0]])\n\n    X_df = pd.DataFrame({'A': [1, 2, 2, 1], 'B': ['a', 'b', 'b', 'a']})\n    enc = OrdinalEncoder(dtype='int64')\n    Xtr = enc.fit_transform(X_df)\n    assert_array_equal(Xtr, [[0, 0], [1, 1], [1, 1], [0, 0]])\n", "def test_one_hot_encoder_categories_dtype_preservation(drop, handle_unknown):\n    # Test that categories dtype is preserved when specifying categories\n    # manually\n    X = [['a', 'b'], ['b', 'c']]\n    cats = [['a', 'b', 'c'], ['d', 'e']]\n\n    enc = OneHotEncoder(categories=cats, drop=drop, handle_unknown=handle_unknown)\n    enc.fit(X)\n    for i, cat in enumerate(enc.categories_):\n        assert cat.dtype == np.array(cats[i]).dtype\n\n    # Test that categories dtype is preserved when determining categories\n    # automatically\n    enc = OneHotEncoder(drop=drop, handle_unknown=handle_unknown)\n    enc.fit(X)\n    for i, cat in enumerate(enc.categories_):\n        assert cat.dtype == X[:, i].dtype\n", "def test_ordinal_encoder_partial_fit(categories):\n    enc = OrdinalEncoder(categories=categories)\n    X = [['a', 1], ['b', 2], ['a', 3]]\n    X2 = [['b', 2], ['a', 3], ['c', 1]]\n    exp = np.array([[0, 0], [1, 0], [0, 2]], dtype='int64')\n    assert_array_equal(enc.fit_transform(X).astype('int64'), exp)\n    assert_array_equal(enc.fit(X).transform(X).astype('int64'), exp)\n    assert_array_equal(enc.fit_transform(X2).astype('int64'), exp)\n    assert_array_equal(enc.fit(X2).transform(X2).astype('int64'), exp)\n"], "sample_807": ["def test_calibrated_classifier_cv_with_sample_weight():\n    \"\"\"Test CalibratedClassifierCV with sample weights and prefit\"\"\"\n    n_samples = 50\n    X, y = make_classification(n_samples=3 * n_samples, n_features=6,\n                               random_state=42)\n    sample_weight = np.random.RandomState(seed=42).uniform(size=y.size)\n\n    X -= X.min()  # MultinomialNB only allows positive X\n\n    # split train and test\n    X_train, y_train, sw_train = \\\n        X[:n_samples], y[:n_samples], sample_weight[:n_samples]\n    X_calib, y_calib, sw_calib = \\\n        X[n_samples:2 * n_samples], y[n_samples:2 * n_samples], \\\n        sample_weight[n_samples:2 * n_samples]\n    X_test, y_test = X[2 * n_samples:], y[2 * n_samples:]\n\n    # Naive-Bayes\n    clf = MultinomialNB()\n    clf.fit(X_train, y_train, sw_train)\n    prob_pos_clf = clf.predict_proba(X_test)[:, 1]\n\n    # Naive Bayes with calibration\n    for this_X_calib, this_X_test in [(X_calib, X_test),\n                                      (sparse.csr_matrix(X_calib),\n                                       sparse.csr_matrix(X_test))]:\n        for method in ['isotonic', 'sigmoid']:\n            pc_clf = CalibratedClassifierCV(clf, method=method, cv=\"prefit\")\n\n            pc_clf.fit(this_X_calib, y_calib, sample_weight=sw_calib)\n            y_prob = pc_clf.predict_proba(this_X_test)\n            y_pred = pc_clf.predict(this_X_test)\n            prob_pos_pc_clf = y_prob[:, 1]\n            assert_array_equal(y_pred,\n                               np.array([0, 1])[np.argmax(y_prob, axis=1)])\n\n            # Check that brier score has improved after calibration\n            assert_greater(brier_score_loss(y_test, prob_pos_clf),\n                           brier_score_loss(y_test, prob_pos_pc_clf))\n\n            # Check invariance against relabeling [0, 1] -> [1, 2]\n            pc_clf.fit(this_X_calib, y_calib + 1, sample_weight=sw_calib)\n            prob_pos_pc_clf_relabeled = pc_clf.predict_proba(this_X_test)[:, 1", "def test_calibration_curve_invariance():\n    \"\"\"Test that calibration_curve is invariant to class labels\"\"\"\n    y_true = np.array([0, 0, 0, 1, 1, 1])\n    y_pred = np.array([0., 0.1, 0.2, 0.8, 0.9, 1.])\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=2)\n\n    # test that relabeling [0, 1] -> [1, 0] does not change the result\n    prob_true_relabeled, prob_pred_relabeled = calibration_curve(1 - y_true, y_pred, n_bins=2)\n    assert_array_almost_equal(prob_true, prob_true_relabeled)\n    assert_array_almost_equal(prob_pred, prob_pred_relabeled)\n\n    # test that relabeling [0, 1] -> [-1, 1] does not change the result\n    prob_true_relabeled, prob_pred_relabeled = calibration_curve(y_true * 2 - 1, y_pred, n_bins=2)\n    assert_array_almost_equal(prob_true, prob_true_relabeled)\n    assert_array_almost_equal(prob_pred, prob_pred_relabeled)\n", "def test_calibrated_classifier_predict_proba_with_prefit():\n    \"\"\"Test predict_proba for CalibratedClassifierCV with prefit\"\"\"\n    X, y = make_classification(n_samples=10, n_features=5, n_classes=3,\n                               random_state=42)\n    clf = LinearSVC(random_state=42)\n    clf.fit(X, y)\n    calibrated_clf = CalibratedClassifierCV(clf, cv='prefit', method='sigmoid')\n    calibrated_clf.fit(X, y)\n    proba = calibrated_clf.predict_proba(X)\n    assert_array_almost_equal(np.sum(proba, axis=1), np.ones(proba.shape[0]))\n", "def test_calibration_with_inconsistent_sample_weights():\n    # Test that calibration raises an error when sample weights do not match\n    # between fit and predict\n    X, y = make_classification(n_samples=10, n_features=5)\n    sample_weight = np.random.RandomState(42).uniform(size=len(y))\n    clf = LinearSVC(random_state=42)\n    calibrated_clf = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=2)\n    calibrated_clf.fit(X, y, sample_weight=sample_weight)\n    assert_raises(ValueError, calibrated_clf.predict_proba, X[:5], sample_weight[:4])\n", "def test_calibrated_classifier_cv_cv_iterator():\n    \"\"\"Test CalibratedClassifierCV with a CV iterator\"\"\"\n    n_samples = 100\n    X, y = make_classification(n_samples=n_samples, n_features=6,\n                               random_state=42)\n    X_train, y_train = X[:n_samples // 2], y[:n_samples // 2]\n    X_test = X[n_samples // 2:]\n\n    cv = [(np.arange(n_samples // 2), np.array([], dtype=int))]\n\n    clf = LinearSVC()\n    calibrated_clf = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=cv)\n    calibrated_clf.fit(X_train, y_train)\n\n    y_pred = calibrated_clf.predict_proba(X_test)\n    assert_equal(y_pred.shape, (n_samples // 2, 2))\n\n    assert_greater(brier_score_loss(y_train, calibrated_clf.predict_proba(X_train)),\n                   brier_score_loss(y_train, clf.fit(X_train, y_train).predict_proba(X_train)))\n"], "sample_32": ["    def test_de_density_scale_non_scalar(self, cosmo):\n        \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.de_density_scale` with non-scalar redshift.\"\"\"\n        z = np.array([0.1, 0.2, 0.5, 1.5, 2.5])\n        expected = np.array([1.00705953, 1.02687239, 1.15234885, 2.40022841, 6.49384982])\n        assert u.allclose(cosmo.de_density_scale(z), expected, rtol=1e-4)\n", "    def test_w_outside_range(self, cosmo):\n        \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.w` outside the recommended range.\"\"\"\n        with pytest.warns(\n            RuntimeWarning, match=\"The w0wzCDM model is not recommended for z > 1.\"\n        ):\n            cosmo.w(z)\n", "def test_de_densityscale_edge_cases():\n    \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.de_density_scale` for edge cases.\"\"\"\n    cosmo = w0wzCDM(H0=70, Om0=0.3, Ode0=0.50, w0=-1, wz=0.5)\n\n    # Check that de_density_scale(0) returns 1\n    assert u.allclose(cosmo.de_density_scale(0), 1)\n\n    # Check that de_density_scale(-1) raises a ValueError\n    with pytest.raises(ValueError):\n        cosmo.de_density_scale(-1)\n\n    # Check that de_density_scale(inf) returns inf\n    assert np.isinf(cosmo.de_density_scale(np.inf))\n\n    # Check that de_density_scale(-inf) raises a ValueError\n    with pytest.raises(ValueError):\n        cosmo.de_density_scale(-np.inf)\n\n    # Check that de_density_scale(nan) raises a ValueError\n    with pytest.raises(ValueError):\n        cosmo.de_density_scale(np.nan)\n", "def test_de_density_scale_edge_cases(cosmo_cls):\n    \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.de_density_scale` edge cases.\"\"\"\n    cosmo = cosmo_cls(H0=70, Om0=0.3, Ode0=0.7, w0=-1, wz=0)\n\n    # Test for z=0\n    assert u.allclose(cosmo.de_density_scale(0), 1.0)\n\n    # Test for w0=-1, wz=0 (i.e. Lambda)\n    cosmo = cosmo_cls(H0=70, Om0=0.3, Ode0=0.7, w0=-1, wz=0)\n    assert u.allclose(\n        cosmo.de_density_scale([0.1, 1.0, 10.0]), 1.0, atol=1e-10\n    )\n\n    # Test for w0=0, wz=0 (i.e. matter only)\n    cosmo = cosmo_cls(H0=70, Om0=0.3, Ode0=0.7, w0=0, wz=0)\n    assert u.allclose(\n        cosmo.de_density_scale([0.1, 1.0, 10.0]),\n        [(1 + 0.1) ** 3, (1 + 1.0) ** 3, (1 + 10.0) ** 3],\n    )\n", "    def test_de_density_scale_at_zero(self, cosmo):\n        \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.de_density_scale` at z=0.\"\"\"\n        assert u.allclose(cosmo.de_density_scale(0), 1.0, rtol=1e-4)\n"], "sample_892": ["def test_adaboost_empty_training_set():\n    # Test that AdaBoost can handle an empty training set.\n    X = np.array([[]])\n    y = np.array([])\n    clf = AdaBoostClassifier(random_state=0)\n    msg = \"Cannot fit with empty training set\"\n    with pytest.raises(ValueError, match=msg):\n        clf.fit(X, y)\n\n    regr = AdaBoostRegressor(random_state=0)\n    with pytest.raises(ValueError, match=msg):\n        regr.fit(X, y)\n", "def test_adaboost_regressor_feature_importance():\n    \"\"\"Check that the feature importance of AdaBoostRegressor is correct.\"\"\"\n    X, y = datasets.make_regression(\n        n_samples=100, n_features=10, n_informative=3, random_state=42\n    )\n\n    # Test with a DecisionTreeRegressor\n    regr = AdaBoostRegressor(DecisionTreeRegressor(random_state=42))\n    regr.fit(X, y)\n    assert regr.feature_importances_.shape[0] == 10\n    assert (regr.feature_importances_[:3] >= regr.feature_importances_[3:]).all()\n\n    # Test with an estimator that does not support feature_importances_\n    class DummyRegressor(BaseEstimator):\n            pass\n\n            return np.zeros(X.shape[0])\n\n    with pytest.raises(AttributeError, match=\"Unable to compute feature importances\"):\n        regr = AdaBoostRegressor(DummyRegressor())\n        regr.fit(X, y)\n        regr.feature_importances_\n", "def test_adaboostclassifier_multiclass_support():\n    # Test that AdaBoostClassifier supports multi-class classification with\n    # string labels.\n    X, y = datasets.make_classification(\n        n_samples=100, n_features=10, n_informative=5, n_redundant=0, n_classes=3\n    )\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n    # test with numeric labels\n    model = AdaBoostClassifier(random_state=0)\n    model.fit(X_train, y_train)\n    assert_array_equal(model.predict(X_test), y_test)\n\n    # test with string labels\n    y_train_str = [\"class_{}\".format(i) for i in y_train]\n    y_test_str = [\"class_{}\".format(i) for i in y_test]\n    model.fit(X_train, y_train_str)\n    assert_array_equal(model.predict(X_test), y_test_str)\n", "def test_adaboostregressor_staged_predict_with_variance(loss):\n    # Test that staged predictions of AdaBoostRegressor are correct\n    # and that their variance decreases as the number of estimators increases.\n\n    # Data\n    X, y = datasets.make_regression(n_samples=100, n_features=2, n_informative=1, random_state=0)\n\n    # Fit model\n    model = AdaBoostRegressor(loss=loss, n_estimators=50, random_state=0)\n    model.fit(X, y)\n\n    # Predict\n    y_pred_staged = np.array(list(model.staged_predict(X)))\n\n    # Check that variance decreases as the number of estimators increases\n    variances = np.var(y_pred_staged, axis=0)\n    assert_array_less(variances[:-1], variances[1:])\n\n    # Check that staged predictions are correct\n    assert_array_almost_equal(y_pred_staged[-1], model.predict(X))\n", "def test_adaboostclassifier_different_initializations():\n    # Check that the initialization of the AdaBoostClassifier is correct\n    # for different cases.\n    rng = np.random.RandomState(0)\n    X, y = datasets.make_classification(\n        n_samples=100, n_features=10, n_informative=3, random_state=rng\n    )\n    algorithms = [\"SAMME\", \"SAMME.R\"]\n\n    # Initialize with a random estimator\n    for algorithm in algorithms:\n        model = AdaBoostClassifier(algorithm=algorithm, random_state=rng)\n        model.fit(X, y)\n        assert model.estimator_ is not None\n\n    # Initialize with a custom estimator\n    from sklearn.linear_model import LogisticRegression\n\n    for algorithm in algorithms:\n        model = AdaBoostClassifier(algorithm=algorithm, estimator=LogisticRegression())\n        model.fit(X, y)\n        assert isinstance(model.estimator_, LogisticRegression)\n\n    # Initialize with a custom estimator and random_state\n    for algorithm in algorithms:\n        model = AdaBoostClassifier(\n            algorithm=algorithm,\n            estimator=LogisticRegression(),\n            random_state=rng,\n        )\n        model.fit(X, y)\n        assert isinstance(model.estimator_, LogisticRegression)\n"], "sample_600": ["def test_unsigned_integer_encode():\n    original = xr.Variable((\"x\",), np.array([255], dtype=np.uint8), encoding={\"_Unsigned\": \"true\"})\n    coder = variables.UnsignedIntegerCoder()\n    encoded = coder.encode(original)\n    assert encoded.dtype == np.int8\n    assert encoded.values == np.array([-1])\n\n    original = xr.Variable((\"x\",), np.array([-1], dtype=np.int8), encoding={\"_Unsigned\": \"false\"})\n    coder = variables.UnsignedIntegerCoder()\n    encoded = coder.encode(original)\n    assert encoded.dtype == np.uint8\n    assert encoded.values == np.array([255])\n", "def test_unsigned_coder_encode():\n    original = xr.Variable((\"x\",), np.arange(10, dtype=np.uint8), encoding=dict(_Unsigned=\"true\"))\n    coder = variables.UnsignedIntegerCoder()\n    encoded = coder.encode(original)\n    assert encoded.dtype == np.int8\n    assert encoded.attrs[\"_Unsigned\"] == \"true\"\n    roundtripped = coder.decode(encoded)\n    assert_identical(original, roundtripped)\n", "def test_UnsignedIntegerCoder_encode():\n    original = xr.Variable((\"x\",), np.array([1, 2, 3], dtype=np.uint8), {\"_Unsigned\": \"true\"})\n    coder = variables.UnsignedIntegerCoder()\n    encoded = coder.encode(original)\n    expected = xr.Variable((\"x\",), np.array([-1, -2, -3], dtype=np.int8))\n    assert_identical(encoded, expected)\n\n    original = xr.Variable((\"x\",), np.array([1, 2, 3], dtype=np.int8), {\"_Unsigned\": \"false\"})\n    coder = variables.UnsignedIntegerCoder()\n    encoded = coder.encode(original)\n    expected = xr.Variable((\"x\",), np.array([1, 2, 3], dtype=np.uint8))\n    assert_identical(encoded, expected)\n", "def test_CFScaleOffsetCoder_encode_conflicting_scale_offset():\n    original = xr.Variable(\n        (\"x\",), np.arange(10.0), encoding=dict(scale_factor=10, add_offset=0.1)\n    )\n    coder = variables.CFScaleOffsetCoder()\n    encoded = coder.encode(original)\n\n    # create a variable with conflicting scale_factor and add_offset\n    conflicting = xr.Variable(\n        (\"x\",), np.arange(10.0), encoding=dict(scale_factor=10, add_offset=1.0)\n    )\n    conflicting.encoding[\"scale_factor\"] = 5\n    conflicting.encoding[\"add_offset\"] = 0.2\n\n    with pytest.raises(ValueError):\n        coder.decode(conflicting)\n", "def test_unsigned_converts_to_signed(dtype):\n    original = xr.Variable((\"x\",), np.arange(10, dtype=dtype), attrs={\"_Unsigned\": \"true\"})\n    coder = variables.UnsignedIntegerCoder()\n    encoded = coder.encode(original)\n    assert encoded.dtype == np.dtype(\"i%s\" % dtype[-1:])\n    roundtripped = coder.decode(encoded)\n    assert_identical(original, roundtripped)\n    assert roundtripped.dtype == np.dtype(\"u%s\" % dtype[-1:])\n\n"], "sample_1172": ["def test_solve_biquadratic_failures():\n    x0, y0, x1, y1 = symbols('x0 y0 x1 y1')\n\n    f_1 = (x - 1)**2 + (y - 1)**2\n    f_2 = (x - 2)**2 + (y - 2)**2\n    (f, g), opt = parallel_poly_from_expr((f_1, f_2), x, y)\n    raises(SolveFailed, lambda: solve_biquadratic(f, g, opt))\n\n    f_1 = (x - x0)**2 + (y - y0)**2\n    f_2 = (x - x1)**2 + (y - y1)**2\n    (f, g), opt = parallel_poly_from_expr((f_1, f_2), x, y)\n    raises(SolveFailed, lambda: solve_biquadratic(f, g, opt))\n\n    f_1 = x**2 + y**2\n    f_2 = x**2 + y**2\n    (f, g), opt = parallel_poly_from_expr((f_1, f_2), x, y)\n    raises(SolveFailed, lambda: solve_biquadratic(f, g, opt))\n\n    f_1 = x**2 + y**2\n    f_2 = x + y\n    (f, g), opt = parallel_poly_from_expr((f_1, f_2), x, y)\n    raises(SolveFailed, lambda: solve_biquadratic(f, g, opt))\n", "def test_solve_biquadratic_error_handling():\n    x0, y0, x1, y1, r = symbols('x0 y0 x1 y1 r')\n\n    f_1 = (x - 1)**2 + (y - 1)**2 - r**2\n    f_2 = (x - 2)**2 + (y - 2)**2 - r**2\n\n    raises(PolynomialError, lambda: solve_biquadratic(Poly(f_1, x), f_2, None))\n    raises(PolynomialError, lambda: solve_biquadratic(f_1, Poly(f_2, y), None))\n    raises(PolynomialError, lambda: solve_biquadratic(f_1, f_2, \"Invalid Option\"))\n    raises(SolveFailed, lambda: solve_biquadratic(f_1, f_2, None))\n", "def test_solve_poly_system_exceptions():\n    # Test that an exception is raised when polys is empty\n    raises(ValueError, lambda: solve_poly_system([], x, y))\n\n    # Test that an exception is raised when there are more generators than polys\n    raises(NotImplementedError, lambda: solve_poly_system([x - 1], x, y))\n\n    # Test that an exception is raised when there are no generators\n    raises(NotImplementedError, lambda: solve_poly_system([x - 1]))\n\n    # Test that an exception is raised when polys is not a list/tuple/set\n    raises(TypeError, lambda: solve_poly_system(\"x - 1\", x, y))\n\n    # Test that an exception is raised when a generator is not a Symbol\n    raises(TypeError, lambda: solve_poly_system([x - 1], 1, y))\n", "def test_solve_poly_system_errors():\n    # Test that SolveFailed is raised when the input is not a list/tuple/set\n    raises(TypeError, lambda: solve_poly_system(\"x - 1\", x))\n\n    # Test that SolveFailed is raised when the input list is empty\n    raises(ValueError, lambda: solve_poly_system([], x, y))\n\n    # Test that SolveFailed is raised when the number of generators is not equal to the number of variables in the polynomials\n    raises(NotImplementedError, lambda: solve_poly_system([x - 1, y - 1], x))\n\n    # Test that SolveFailed is raised when the generators are not symbols\n    raises(NotImplementedError, lambda: solve_poly_system([x - 1, y - 1], 1, 2))\n\n    # Test that SolveFailed is raised when the input polynomials are not valid\n    raises(PolynomialError, lambda: solve_poly_system([x - 1, 1/x], x, y))\n", "def test_solve_poly_system_special_cases():\n    # Test case with no solutions\n    assert solve_poly_system([x**2 + y**2, x**2 + y**2 + 1], x, y) is None\n\n    # Test case with one equation and two variables\n    assert solve_poly_system([x - 1], x, y) == [(1, S.Zero)]\n\n    # Test case with two equations and one variable\n    assert solve_poly_system([x - 1, x - 2], x) is None\n\n    # Test case with complex roots\n    assert solve_poly_system([x**2 + y**2, x + y - 1], x, y) == \\\n        [(S.Half - sqrt(3)*I/2, S.Half + sqrt(3)*I/2),\n         (S.Half + sqrt(3)*I/2, S.Half - sqrt(3)*I/2)]\n\n    # Test case with non-polynomial equation\n    raises(PolynomialError, lambda: solve_poly_system([sin(x)], x))\n\n    # Test case with non-polynomial equation and two variables\n    raises(PolynomialError, lambda: solve_poly_system([sin(x), y - 1], x, y))\n\n    # Test case with invalid generator\n    raises(NotImplementedError, lambda: solve_poly_system([x - 1], 1))\n\n    # Test case with invalid generator and two variables\n    raises(NotImplementedError, lambda: solve_poly_system([x - 1, y - 1], x, 1))\n"], "sample_295": ["    def test_order_by_invalid(self):\n        with self.assertRaises(ValueError):\n            OrderBy('invalid_expression')\n", "    def test___getstate___raises_FieldError(self):\n        expr = BaseExpression(output_field=None)\n        with self.assertRaisesMessage(FieldError, 'Cannot resolve expression type, unknown output_field'):\n            expr.__getstate__()\n", "    def test_decimal_cast(self):\n        sql, params = SQLiteNumericMixin().as_sqlite(compiler=None, connection='connection', sql='sql')\n        self.assertEqual(sql, 'CAST(sql AS NUMERIC)')\n", "    def test_window_function_annotation(self):\n        Employee.objects.bulk_create([\n            Employee(firstname='John', lastname='Doe', salary=100),\n            Employee(firstname='Jane', lastname='Doe', salary=200),\n            Employee(firstname='Jim', lastname='Doe', salary=300),\n        ])\n\n        qs = Employee.objects.annotate(\n            avg_salary=Window(\n                Func(F('salary'), function='AVG', output_field=FloatField()),\n                partition_by=F('lastname'),\n            )\n        )\n\n        self.assertSequenceEqual(\n            qs.values_list('salary', 'avg_salary'),\n            [\n                (100, 200.0),\n                (200, 200.0),\n                (300, 200.0),\n            ],\n        )\n", "    def test_window_expression_with_function(self):\n        ExpressionList = ExpressionList\n        Window = Window\n        Func = Func\n        F = F\n        with self.subTest('Func in the expression'):\n            Window(Func(F('field')), frame=Func(Func(F('field'))))\n        with self.subTest('Func in the frame'):\n            Window(F('field'), frame=Func(F('field')))\n        with self.subTest('Func in the partition by'):\n            Window(F('field'), partition_by=(Func(F('field')),))\n        with self.subTest('Func in the order by'):\n            Window(F('field'), order_by=(Func(F('field')),))\n"], "sample_1043": ["def test_user_functions():\n    from sympy import sin, cos\n    assert mcode(sin(x), user_functions={'sin': 'mysin'}) == \"mysin[x]\"\n    assert mcode(cos(x), user_functions={'cos': lambda *x: 'mycos'}) == \"mycos[x]\"\n    assert mcode(sin(x) + cos(x), user_functions={'sin': 'mysin', 'cos': 'mycos'}) == \"mysin[x] + mycos[x]\"\n", "def test_list():\n    assert mcode([sin(x), cos(x), [tan(x), cot(x)]]) == \\\n        \"{Sin[x], Cos[x], {Tan[x], Cot[x]}}\"\n    assert mcode([]) == \"{}\"\n", "def test_UserDefinedFunctions():\n    from sympy import Function\n    f = Function('f')\n    assert mcode(f(x), user_functions={'f': 'my_f'}) == \"my_f[x]\"\n    assert mcode(f(x, y, z), user_functions={'f': 'my_f'}) == \"my_f[x, y, z]\"\n    g = Function('g')\n    assert mcode(g(x, y, z), user_functions={'f': 'my_f'}) == \"g[x, y, z]\"\n    assert mcode(g(x, y, z), user_functions={'f': 'my_f', 'g': 'my_g'}) == \"my_g[x, y, z]\"\n", "def test_Function_no_condition():\n    class CustomFunction(Function):\n        pass\n    custom_function = CustomFunction('f')\n    assert mcode(custom_function(x, y, z)) == \"f[x, y, z]\"\n    assert mcode(custom_function(x, y, z)) != \"f(x, y, z)\"\n    assert mcode(custom_function(x, y, z, evaluate=False)) == \"f[x, y, z]\"\n    assert mcode(custom_function(x, y, z, evaluate=False)) != \"f(x, y, z)\"\n", "def test_known_functions():\n    # Testing user-defined functions\n    assert mcode(Function('f')(x), user_functions={'f': 'g'}) == \"g[x]\"\n    # Testing user-defined functions with multiple patterns\n    assert mcode(Function('f')(x, y), user_functions={'f': ['h', lambda a, b: a.is_even, 'k']}) == \"f[x, y]\"\n    assert mcode(Function('f')(x, 2), user_functions={'f': ['h', lambda a, b: a.is_even, 'k']}) == \"k[x, 2]\"\n\n    # Testing function name which is not in the known_functions dict\n    assert mcode(Function('F')(x)) == \"F[x]\"\n\n    # Testing function with 0 arguments\n    assert mcode(Function('F')()) == \"F[]\"\n\n    # Testing function with one argument\n    assert mcode(Function('F')(x)) == \"F[x]\"\n\n    # Testing function with multiple arguments\n    assert mcode(Function('F')(x, y, z)) == \"F[x, y, z]\"\n\n    # Testing a function with a string argument\n    assert mcode(Function('F')('a')) == \"F[a]\"\n"], "sample_4": ["def test_read_html_table_bad_format(self, read, write, tmp_path):\n    \"\"\"Test if the format argument is incorrect\"\"\"\n    fp = tmp_path / \"test_read_html_table_bad_format.html\"\n\n    write(fp, format=\"ascii.html\")\n\n    # incorrect format\n    with pytest.raises(ValueError, match=\"format must be 'ascii.html', not\"):\n        read(fp, format=\"ascii.ecsv\")\n", "    def test_read_html_table_invalid_format(self, tmp_path):\n        \"\"\"Test read_html_table with invalid format.\"\"\"\n        fp = tmp_path / \"test_read_html_table_invalid_format.html\"\n\n        # Write a table with invalid format\n        table = QTable({\"H0\": [70]})\n        table.write(fp, format=\"ascii.ecsv\", overwrite=True)\n\n        # Try to read the table with read_html_table\n        with pytest.raises(ValueError, match=\"format must be 'ascii.html', not 'ascii.ecsv'\"):\n            read_html_table(fp, format=\"ascii.ecsv\")\n", "def test_read_html_table_latex_names_conflict(self, read, write, tmp_path):\n    \"\"\"Test if latex names conflict with actual names.\"\"\"\n    fp = tmp_path / \"test_read_html_table_latex_names_conflict.html\"\n\n    write(fp, format=\"ascii.html\", latex_names=True)\n\n    tbl = QTable.read(fp)\n\n    # add an actual name that conflicts with a latex name\n    tbl['H0'] = [1]\n\n    tbl.write(fp, format=\"ascii.html\", overwrite=True)\n\n    # this should error\n    with pytest.raises(KeyError, match=\"Conflicting column names\"):\n        read(fp, format=\"ascii.html\")\n\n    # but not if we disable latex names\n    read(fp, format=\"ascii.html\", latex_names=False)\n", "def test_read_html_table_unit_conversion(self, read, write, tmp_path, add_cu):\n    \"\"\"Test unit conversion when reading HTML table.\"\"\"\n    fp = tmp_path / \"test_read_html_table_unit_conversion.html\"\n\n    # Write with unitless columns\n    write(fp, format=\"ascii.html\")\n\n    # Add unit to columns\n    table = QTable.read(fp)\n    for name, col in table.columns.items():\n        param = getattr(type(self.cosmo), name, None)\n        if isinstance(param, Parameter) and param.unit is not None and param.unit != u.one:\n            table[name] = col * param.unit\n\n    # Re-write the table\n    table.write(fp, format=\"ascii.html\", overwrite=True)\n\n    # Read the table back\n    read_cosmo = read(fp, format=\"ascii.html\")\n\n    # Check if units are correctly converted\n    for name in self.cosmo.__parameters__:\n        param = getattr(type(self.cosmo), name, None)\n        if isinstance(param, Parameter) and param.unit is not None and param.unit != u.one:\n            assert getattr(read_cosmo, name).unit == param.unit\n", "    def test_read_html_table_format_error(self, read, tmp_path):\n        \"\"\"Test if the format argument is not \"ascii.html\".\"\"\"\n        fp = tmp_path / \"test_read_html_table_format_error.html\"\n\n        # Create a table with a single row\n        table = QTable(rows=[[1, 2], [3, 4]], names=(\"a\", \"b\"))\n        table.write(fp, format=\"ascii.html\")\n\n        with pytest.raises(ValueError, match=\"format must be 'ascii.html'\"):\n            read(fp, format=\"csv\")\n"], "sample_1160": ["def test_intersection_sets():\n    x, y = symbols('x y')\n    assert intersection_sets(S.EmptySet, S.EmptySet) == S.EmptySet\n    assert intersection_sets(S.EmptySet, S.Naturals) == S.EmptySet\n    assert intersection_sets(S.EmptySet, S.Integers) == S.EmptySet\n    assert intersection_sets(S.EmptySet, S.Rationals) == S.EmptySet\n    assert intersection_sets(S.EmptySet, S.Reals) == S.EmptySet\n    assert intersection_sets(S.EmptySet, S.Complexes) == S.EmptySet\n    assert intersection_sets(S.EmptySet, S.UniversalSet) == S.EmptySet\n\n    assert intersection_sets(S.UniversalSet, S.EmptySet) == S.EmptySet\n    assert intersection_sets(S.UniversalSet, S.Naturals) == S.Naturals\n    assert intersection_sets(S.UniversalSet, S.Integers) == S.Integers\n    assert intersection_sets(S.UniversalSet, S.Rationals) == S.Rationals\n    assert intersection_sets(S.UniversalSet, S.Reals) == S.Reals\n    assert intersection_sets(S.UniversalSet, S.Complexes) == S.Complexes\n    assert intersection_sets(S.UniversalSet, S.UniversalSet) == S.UniversalSet\n\n    assert intersection_sets(S.Naturals, S.EmptySet) == S.EmptySet\n    assert intersection_sets(S.Naturals, S.Naturals) == S.Naturals\n    assert intersection_sets(S.Naturals, S.Integers) == S.Naturals\n    assert intersection_sets(S.Naturals, S.Rationals) == S.Naturals\n    assert intersection_sets(S.Naturals, S.Reals) == S.Naturals\n    assert intersection_sets(S.Naturals, S.Complexes) == S.Naturals\n    assert intersection_sets(S.Naturals, S.UniversalSet) == S.Naturals\n\n    assert intersection_sets(S.Integers, S.EmptySet) == S.EmptySet\n    assert intersection_sets(S.Integers, S.Naturals) == S.Naturals\n    assert intersection_sets(S.Integers, S.Integers) == S.Integers\n    assert intersection_sets(S.Integers, S.Rationals) == S.Integers\n    assert intersection_sets(S.Integers, S.Reals) == S.Integers\n    assert intersection_sets(S.Integers, S.Complexes)", "def test_issue_20203():\n    s = ImageSet(Lambda(x, x), S.Integers)\n    assert s.is_subset(S.Integers)\n    assert S.Integers.is_subset(s) is False\n", "def test_ComplexRegion_repr():\n    assert repr(S.Complexes) == \"S.Complexes\"\n    a, b = Interval(1, 2), Interval(3, 4)\n    assert repr(ComplexRegion(a*b)) == \"ComplexRegion(\" + repr(a*b) + \")\"\n    a, b = Interval(1, 2), Interval(3, 4)\n    assert repr(ComplexRegion(a*b, polar=True)) == \"ComplexRegion(\" + repr(a*b) + \", polar=True)\"\n", "def test_image_set_intersection_image_set():\n    # Test the intersection of two image sets with condition\n    x, y = symbols('x y')\n    s = ImageSet(Lambda(x, x**2), Interval(0, 10, True, True))\n    t = ImageSet(Lambda(y, y + 1), Interval(1, 11, True, True))\n    assert s.intersect(t) == Interval.open(1, 10)\n", "def test_imageset_intersection_interval_edge_cases():\n    from sympy.abc import n\n    from sympy import oo\n    # Test intersection with a non-real interval\n    f1 = ImageSet(Lambda(n, n), S.Integers)\n    f2 = ImageSet(Lambda(n, n), Interval(1, oo))\n    assert f1.intersect(f2) == f2\n    f2 = ImageSet(Lambda(n, n), Interval(-oo, 1))\n    assert f1.intersect(f2) == f2\n    f2 = ImageSet(Lambda(n, n), Interval(1, 1))\n    assert f1.intersect(f2) is S.EmptySet\n    # Test intersection with an interval containing only one integer\n    f2 = ImageSet(Lambda(n, n), Interval(1, 2, True, True))\n    assert f1.intersect(f2) is S.EmptySet\n"], "sample_1147": ["def test_latex_NamedMorphism():\n    from sympy.categories import NamedMorphism\n    m = NamedMorphism(S.Zero, S.One, \"f\")\n    assert latex(m) == r\"f:\\mathbb{0} \\rightarrow \\mathbb{1}\"\n", "def test_latex_unexpected_input():\n    # Test that unexpected input will not crash the function.\n    assert latex('string') == r\"\\mathtt{\\text{string}}\"\n    assert latex(1) == \"1\"\n    assert latex(None) == r\"\\text{None}\"\n    assert latex([]) == r\"\\left[ \\right]\"\n    assert latex({}) == r\"\\left\\{ \\right\\}\"\n    assert latex(lambda x: x) == r\"\\mathtt{\\text{<lambda>}}\"\n", "def test_latex_Symbol_with_modifiers():\n    from sympy import Symbol\n\n    # Test a Symbol with a modifier\n    s = Symbol('x_tilde')\n    assert latex(s) == r'\\tilde{x}'\n\n    # Test a Symbol with multiple modifiers\n    s = Symbol('x_tilde_bar')\n    assert latex(s) == r'\\overline{\\tilde{x}}'\n\n    # Test a Symbol with a modifier that is not a valid LaTeX command\n    s = Symbol('x_invalid_modifier')\n    assert latex(s) == r'x_{invalid\\_modifier}'\n\n    # Test a Symbol with a modifier that is a valid LaTeX command\n    # but is not in the modifier_dict\n    s = Symbol('x_mathbf')\n    assert latex(s) == r'x_{mathbf}'\n", "compilation error", "def test_latex_summation():\n    i = symbols('i')\n    f = symbols('f', cls=Function)\n    assert latex(Sum(f(i), (i, 0, n))) == r'\\sum_{i=0}^{n} f{\\left(i \\right)}'\n    assert latex(Sum(f(i), (i, 0, n), (i, 2, 3))) == r'\\sum_{i=0}^{n} \\sum_{i=2}^{3} f{\\left(i \\right)}'\n    assert latex(Sum(f(i), (i, 0, oo))) == r'\\sum_{i=0}^{\\infty} f{\\left(i \\right)}'\n    assert latex(Sum(f(i), (i, -oo, 3))) == r'\\sum_{i=-\\infty}^{3} f{\\left(i \\right)}'\n    assert latex(Sum(f(i), (i, 0, oo), (i, 1, oo))) == r'\\sum_{i=0}^{\\infty} \\sum_{i=1}^{\\infty} f{\\left(i \\right)}'\n    assert latex(Sum(f(i), (i, 1, 5), (i, 1, n))) == r'\\sum_{i=1}^{5} \\sum_{i=1}^{n} f{\\left(i \\right)}'\n    assert latex(Sum(f(i), (i, 1, 2), (i, 1, n))) == r'\\sum_{i=1}^{2} \\sum_{i=1}^{n} f{\\left(i \\right)}'\n    assert latex(Sum(f(i), (i, 1, oo), (i, 1, 5))) == r'\\sum_{i=1}^{\\infty} \\sum_{i=1}^{5} f{\\left(i \\right)}'\n    assert latex(Sum(f(i), (i, -oo, 1), (i, 1, 3))) == r'\\sum_{i=-\\infty}^{1} \\sum_{i=1}^{3} f{\\left(i \\right)}'\n    assert latex(Sum(f(i), (i, 1, oo), (i, 1, oo))) == r'\\sum_{i=1"], "sample_164": ["    def test_configure_logging_no_config(self):\n        configure_logging(None, None)\n        self.assertEqual(logging.getLogger('django').handlers, [])\n", "    def test_format_with_server_time(self):\n        formatter = ServerFormatter()\n        server_time = '2023-12-31 23:59:59'\n        log_msg = 'log message'\n        record = logging.makeLogRecord({'msg': log_msg})\n        record.server_time = server_time\n        self.assertEqual(formatter.format(record), '[%s] %s' % (server_time, log_msg))\n", "    def test_configure_logging_no_config(self):\n        \"\"\"\n        Test that configure_logging doesn't raise an error when no logging_config\n        is provided.\n        \"\"\"\n        configure_logging(None, None)\n        self.assertEqual(logging.config.dictConfig.called, True)\n", "    def test_configure_logging_with_none(self):\n        configure_logging(None, None)\n        self.assertEqual(logging.config.dictConfig.call_count, 1)\n        self.assertEqual(logging.config.dictConfig.call_args[1]['config'], DEFAULT_LOGGING)\n", "    def test_uses_server_time(self):\n        formatter = ServerFormatter()\n        self.assertTrue(formatter.uses_server_time())\n        formatter._fmt = '%(asctime)s %(message)s'\n        self.assertFalse(formatter.uses_server_time())\n"], "sample_707": ["def test_node_repr_failure_with_long_tbstyle(pytester: Pytester) -> None:\n    p = pytester.makepyfile(\n        \"\"\"\n            assert False\n    \"\"\"\n    )\n    result = pytester.runpytest([p, \"--fulltrace\"])\n    result.stdout.fnmatch_lines([str(p) + \":*: AssertionError\", \"*1 failed in *\"])\n    assert \"test_node_repr_failure_with_long_tbstyle\" in result.stdout.str()\n    assert \"self.fail(\" not in result.stdout.str()\n", "def test_iter_markers() -> None:\n    class Marker1:\n        name = \"marker1\"\n\n    class Marker2:\n        name = \"marker2\"\n\n    class NodeSubclass(nodes.Node):\n            super().__init__(name, parent)\n\n    parent = NodeSubclass(\"parent\")\n    parent.add_marker(Marker1())\n    parent.add_marker(Marker2(), append=False)\n\n    child = NodeSubclass(\"child\", parent)\n    child.add_marker(Marker2())\n\n    markers = list(parent.iter_markers())\n    assert markers == [Marker2(), Marker1()]\n\n    markers = list(child.iter_markers())\n    assert markers == [Marker2(), Marker2(), Marker1()]\n\n    markers = list(parent.iter_markers(\"marker2\"))\n    assert markers == [Marker2()]\n\n    markers = list(child.iter_markers(\"marker1\"))\n    assert markers == [Marker1()]\n", "def test_node_repr_failure_short_style(pytester: Pytester) -> None:\n    \"\"\"Ensure that repr_failure with style='short' produces a short traceback.\"\"\"\n    p = pytester.makepyfile(\n        \"\"\"\n            raise ValueError(\"test\")\n    \"\"\"\n    )\n    items = pytester.getitems(p)\n    excinfo = ExceptionInfo.from_exc_info((ValueError, ValueError(\"test\"), None))\n    report = items[0].repr_failure(excinfo, style=\"short\")\n    assert isinstance(report, str)\n    assert \"ValueError: test\" in report\n    assert \"1 failed in\" in report\n", "def test_repr_failure_short_traceback(pytester: Pytester) -> None:\n    p = pytester.makepyfile(\n        \"\"\"\n            x = 1 / 0\n    \"\"\"\n    )\n    items = pytester.getitems()\n    assert len(items) == 1\n    item = items[0]\n    result = pytester.runpytest(\"-v\", \"--tb=short\", p)\n    assert result.ret == 1\n    excinfo = pytest._code.ExceptionInfo.from_exc_info(\n        (ZeroDivisionError, ZeroDivisionError(\"division by zero\"), None)\n    )\n    assert item._repr_failure_py(excinfo, style=\"short\") == result.stdout.str()\n", "def test_node_repr_failure_repr() -> None:\n    class FakeException(Exception):\n            return \"FakeException()\"\n\n    exc = FakeException()\n    excinfo = pytest._code.ExceptionInfo.from_exc_info((type(exc), exc, None))\n    node = nodes.Node(\"test_node\", nodeid=\"::test_node\")\n    result = node.repr_failure(excinfo)\n    assert isinstance(result, str)\n    assert \"FakeException()\" in result\n\n"], "sample_721": ["def test_check_X_y():\n    X = np.random.rand(10, 2)\n    y = np.random.randint(0, 2, 10)\n    X_checked, y_checked = check_X_y(X, y)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, y)\n\n    X_sparse = sp.csr_matrix(X)\n    y_checked = check_X_y(X_sparse, y)[1]\n    assert_array_equal(y_checked, y)\n\n    X = np.random.rand(10, 2, 3)\n    y = np.random.randint(0, 2, 10)\n    X_checked, y_checked = check_X_y(X, y, allow_nd=True)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, y)\n\n    assert_raises(ValueError, check_X_y, X, y)\n\n    X = np.random.rand(10, 2)\n    y = np.random.rand(9)\n    assert_raises(ValueError, check_X_y, X, y)\n", "def test_check_X_y():\n    # Test that check_X_y validates input shapes and converts to numpy arrays\n    X = np.arange(10).reshape(5, 2)\n    y = np.arange(5)\n\n    X_converted, y_converted = check_X_y(X, y)\n    assert_true(isinstance(X_converted, np.ndarray))\n    assert_true(isinstance(y_converted, np.ndarray))\n    assert_equal(X_converted.shape, (5, 2))\n    assert_equal(y_converted.shape, (5,))\n\n    # Test that check_X_y raises an error for inconsistent lengths\n    assert_raises_regex(ValueError, 'Found input variables with inconsistent '\n                        'numbers of samples', check_X_y, X, np.arange(3))\n\n    # Test that check_X_y raises an error for multi-output y with nan or inf\n    y_multi = np.array([[1, 2], [3, 4], [5, np.nan]])\n    assert_raises_regex(ValueError, 'y contains NaN or infinity', check_X_y,\n                        X, y_multi, multi_output=True)\n\n    # Test that check_X_y raises an error for non-numeric y with numeric dtype\n    y_object = np.array([1, 2, 3], dtype=object)\n    y_object[0] = 'a'\n    assert_raises_regex(ValueError, 'y contains non-numerical values', check_X_y,\n                        X, y_object, y_numeric=True)\n", "def test_check_X_y():\n    # Test check_X_y\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n\n    # Test standard usage\n    X_checked, y_checked = check_X_y(X, y)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, y)\n\n    # Test with sparse X\n    X_sparse = sp.csr_matrix(X)\n    X_checked, y_checked = check_X_y(X_sparse, y)\n    assert_array_equal(X_checked.toarray(), X)\n    assert_array_equal(y_checked, y)\n\n    # Test with sparse y\n    y_sparse = sp.csr_matrix(y.reshape(-1, 1))\n    X_checked, y_checked = check_X_y(X, y_sparse)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, y)\n\n    # Test with estimator\n    estimator = KNeighborsClassifier()\n    X_checked, y_checked = check_X_y(X, y, estimator=estimator)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, y)\n\n    # Test with non-numeric y\n    y_non_numeric = np.array(['a', 'b'])\n    X_checked, y_checked = check_X_y(X, y_non_numeric, y_numeric=True)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, np.array([0., 0.]))\n\n    # Test with multi-output y\n    y_multi = np.array([[0, 1], [1, 0]])\n    X_checked, y_checked = check_X_y(X, y_multi, multi_output=True)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, y_multi)\n\n    # Test with invalid input\n    assert_raises(ValueError, check_X_y, X, np.array([0, 1, 2]))\n    assert_raises(ValueError, check_X_y, X, np.array([0]))\n    assert_raises(ValueError, check_X_y, np.array([1, 2]), y)\n\n    # Test that data type of y is preserved\n    y_int = np.array([0, 1], dtype=np.int32)\n    X_checked, y_checked = check_X_y(X, y_int)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, y_int)\n    assert_equal(y_checked.dtype, np.int32)\n\n    # Test that", "def test_check_X_y():\n    # Test check_X_y for inconsistent numbers of samples\n    X = np.array([1, 2, 3, 4])\n    y = np.array([1, 2, 3])\n    assert_raise_message(\n        ValueError, \"Found input variables with inconsistent numbers of \"\n                     \"samples: \\[4, 3\\]\", check_X_y, X, y)\n\n    # Test check_X_y with inconsistent numbers of samples in 2D arrays\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([[1], [2]])\n    assert_raise_message(\n        ValueError, \"Found input variables with inconsistent numbers of \"\n                     \"samples: \\[3, 2\\]\", check_X_y, X, y)\n\n    # Test check_X_y with multi-output y\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([[1, 2], [3, 4]])\n    check_X_y(X, y, multi_output=True)\n\n    # Test check_X_y with multi-output y and sparse X\n    X = sp.csr_matrix(np.array([[1, 2], [3, 4], [5, 6]]))\n    y = np.array([[1, 2], [3, 4]])\n    check_X_y(X, y, multi_output=True, accept_sparse=True)\n\n    # Test check_X_y with multi-output y and sparse y\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = sp.csr_matrix(np.array([[1, 2], [3, 4]]))\n    check_X_y(X, y, multi_output=True, accept_sparse=True)\n\n    # Test check_X_y with y having invalid shape\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([[[1, 2], [3, 4]]])  # 3D y\n    assert_raise_message(\n        ValueError, \"bad input shape \\(3, 2, 1\\)\", check_X_y, X, y)\n\n    # Test check_X_y with y having invalid shape and sparse X\n    X = sp.csr_matrix(np.array([[1, 2], [", "def test_check_X_y():\n    # test that check_X_y enforces 2D on X, 1D on y and consistent lengths\n    X = np.arange(10).reshape(2, 5)\n    y = np.arange(5)\n    X_checked, y_checked = check_X_y(X, y)\n    assert_equal(X_checked.shape, X.shape)\n    assert_equal(y_checked.shape, y.shape)\n\n    # test that it converts X to a numpy array\n    X = [[1, 2, 3], [4, 5, 6]]\n    y = np.arange(2)\n    X_checked, y_checked = check_X_y(X, y)\n    assert_true(isinstance(X_checked, np.ndarray))\n\n    # test that it ensures X is at least 2D\n    X = np.arange(5)\n    y = np.arange(5)\n    X_checked, y_checked = check_X_y(X, y)\n    assert_equal(X_checked.shape, (5, 1))\n\n    # test that y is squeezed to 1D\n    X = np.arange(10).reshape(2, 5)\n    y = np.arange(2).reshape(2, 1)\n    X_checked, y_checked = check_X_y(X, y)\n    assert_equal(y_checked.shape, (2,))\n\n    # test that it raises an error for inconsistent lengths\n    X = np.arange(10).reshape(2, 5)\n    y = np.arange(3)\n    assert_raise_message(ValueError, 'Found input variables with '\n                         'inconsistent numbers of samples: \\[2, 3\\]',\n                         check_X_y, X, y)\n\n    # test that it raises an error for a scalar X\n    X = 5\n    y = np.arange(2)\n    assert_raise_message(ValueError, 'Expected 2D array, got scalar '\n                         'array instead:array=5. Reshape your data either '\n                         'using array.reshape(-1, 1) if your data has a '\n                         'single feature or array.reshape(1, -1) if it '\n                         'contains a single sample.', check_X_y, X, y)\n\n    # test that it raises an error for an array with ndim > 2\n    X = np.arange(8).reshape(2, 2, 2)\n    y = np.arange(2)\n    assert_raise_message(ValueError, 'Found array with dim"], "sample_356": ["def test_custom_base(self):\n    before = [\n        ModelState(\"app\", \"Parent\", [(\"id\", models.AutoField(primary_key=True))]),\n        ModelState(\"app\", \"Child\", [], bases=(\"app.Parent\",)),\n    ]\n    after = [\n        ModelState(\"app\", \"Grandparent\", [(\"id\", models.AutoField(primary_key=True))]),\n        ModelState(\"app\", \"Parent\", [], bases=(\"app.Grandparent\",)),\n        ModelState(\"app\", \"Child\", [], bases=(\"app.Parent\",)),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, \"app\", 1)\n    self.assertOperationTypes(changes, \"app\", 0, [\"CreateModel\", \"AlterModelOptions\", \"AlterModelOptions\"])\n    self.assertOperationAttributes(changes, \"app\", 0, 0, name=\"Grandparent\")\n    self.assertOperationAttributes(changes, \"app\", 0, 1, name=\"Parent\", options={})\n    self.assertOperationAttributes(changes, \"app\", 0, 2, name=\"Parent\", options={})\n", "def test_add_field_with_deconstructible_default(self):\n    \"\"\"Tests that adding a field with a deconstructible default works.\"\"\"\n    author = ModelState(\n        \"otherapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ]\n    )\n    author_with_deconstructible_default = ModelState(\n        \"otherapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n            (\"date_of_birth\", models.DateField(default=DeconstructibleObject())),\n        ]\n    )\n    changes = self.get_changes([author], [author_with_deconstructible_default])\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, ['AddField'])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name=\"date_of_birth\")\n", "def test_alter_unique_constraint_with_check_constraint(self):\n    \"\"\"\n    Altering a unique constraint on a model with check constraints works correctly.\n    \"\"\"\n    book = ModelState(\"otherapp\", \"Book\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n        (\"title\", models.CharField(max_length=200)),\n    ], {\n        \"constraints\": [models.CheckConstraint(check=models.Q(author__isnull=False), name='author_not_null')],\n        \"unique_together\": {(\"author\", \"title\")},\n    })\n    book_altered = ModelState(\"otherapp\", \"Book\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n        (\"title\", models.CharField(max_length=200)),\n    ], {\n        \"constraints\": [models.CheckConstraint(check=models.Q(author__isnull=False), name='author_not_null')],\n        \"unique_together\": {(\"title\", \"author\")},\n    })\n    changes = self.get_changes([book], [book_altered])\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, [\"AlterUniqueTogether\"])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name=\"book\", unique_together={(\"title\", \"author\")})\n", "def test_add_unique_together_withFK_same_app(self):\n    \"\"\"Tests unique_together with FK constraint in the same app.\"\"\"\n    author = ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=200)),\n    ])\n    book = ModelState(\"testapp\", \"Book\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n        (\"title\", models.CharField(max_length=200)),\n    ], {\n        \"unique_together\": {(\"author\", \"title\")},\n    })\n    changes = self.get_changes([author], [author, book])\n    # Right number of migrations?\n    self.assertEqual(len(changes['testapp']), 1)\n    # Right number of actions?\n    migration = changes['testapp'][0]\n    self.assertEqual(len(migration.operations), 3)\n    # Right actions order?\n    self.assertOperationTypes(changes, 'testapp', 0, ['CreateModel', 'AddField', 'AlterUniqueTogether'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='Book')\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, model_name='book', name='author')\n    self.assertOperationAttributes(changes, 'testapp', 0, 2, name='book', unique_together={('author', 'title')})\n", "def test_alter_model_with_single_through_unique_together(self):\n    \"\"\"\n    Tests that changing the model used in a ManyToManyField's through model\n    generates a migration that updates the unique_together on that through model.\n    \"\"\"\n    # Model before changes\n    model_state_before = ModelState(\n        \"testapp\",\n        \"ModelA\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n        ],\n    )\n    # Model after changes\n    model_state_after = ModelState(\n        \"testapp\",\n        \"ModelA\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n        ],\n        options={\n            \"unique_together\": {\n                (\"modelb__field1\", \"modelb__field2\"),\n            }\n        },\n    )\n    # ModelB before changes\n    modelb_state_before = ModelState(\n        \"testapp\",\n        \"ModelB\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"field1\", models.IntegerField()),\n        ],\n    )\n    # ModelB after changes\n    modelb_state_after = ModelState(\n        \"testapp\",\n        \"ModelB\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"field1\", models.IntegerField()),\n            (\"field2\", models.IntegerField()),\n        ],\n    )\n    # ModelC before changes\n    modelc_state_before = ModelState(\n        \"testapp\",\n        \"ModelC\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"modela\", models.ForeignKey(\"testapp.ModelA\", models.CASCADE)),\n            (\"modelb\", models.ForeignKey(\"testapp.ModelB\", models.CASCADE)),\n        ],\n        options={\n            \"unique_together\": {\n                (\"modela\", \"modelb\"),\n            }\n        },\n    )\n    # ModelC after changes\n    modelc_state_after = ModelState(\n        \"testapp\",\n        \"ModelC\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"modela\", models.ForeignKey(\"testapp.ModelA\", models.CASCADE)),\n            (\"modelb\", models.ForeignKey(\"testapp.ModelB\", models.CASCADE)),\n        ],\n        options={\n            \"unique_together\": {\n                (\"modela\", \"modelb\"),\n                (\"modela\", \"modelb__field1\", \"modelb__field2\"),\n            }\n        },\n    )\n    # Detect changes\n    changes = self.get_changes(\n        [model_state_before, modelb_state_before, modelc_state_before],\n        [model_state_after, modelb_state"], "sample_813": ["def test_bayesian_ridge_intercept():\n    # Test BayesianRidge with intercept\n    X = np.array([[1, 1], [2, 2], [3, 3]])\n    y = np.array([1, 2, 3])\n\n    clf = BayesianRidge(fit_intercept=True)\n    clf.fit(X, y)\n\n    assert_array_almost_equal(clf.predict([[1, 1]]), [1.0])\n    assert_array_almost_equal(clf.predict([[2, 2]]), [2.0])\n    assert_array_almost_equal(clf.predict([[3, 3]]), [3.0])\n\n    clf = BayesianRidge(fit_intercept=False)\n    clf.fit(X, y)\n\n    assert_array_almost_equal(clf.predict([[1, 1]]), [0.0])\n    assert_array_almost_equal(clf.predict([[2, 2]]), [0.0])\n    assert_array_almost_equal(clf.predict([[3, 3]]), [0.0])\n", "def test_bayesian_ridge_alpha_init_too_small():\n    # Test that BayesianRidge can handle small alpha_init values\n    X = np.array([[1], [2], [3]])\n    y = np.array([1, 2, 3])\n    alpha_init = 1e-9\n    clf = BayesianRidge(alpha_init=alpha_init)\n    clf.fit(X, y)\n    assert clf.alpha_ >= alpha_init\n", "def test_bayesian_ridge_with_multiple_features():\n    # Test BayesianRidge with multiple features\n    X = np.random.rand(100, 10)\n    y = np.random.rand(100)\n    clf = BayesianRidge()\n    clf.fit(X, y)\n\n    # Test predict with multiple features\n    X_test = np.random.rand(10, 10)\n    y_pred = clf.predict(X_test)\n    assert y_pred.shape == (10,)\n\n    # Test predict with return_std=True\n    y_pred, y_std = clf.predict(X_test, return_std=True)\n    assert y_pred.shape == (10,)\n    assert y_std.shape == (10,)\n\n    # Test that the coefficients are not all zero\n    assert np.any(clf.coef_ != 0)\n\n    # Test that the intercept is not zero\n    assert clf.intercept_ != 0\n\n    # Test that the scores_ attribute is not empty\n    assert len(clf.scores_) > 0\n", "def test_bayesian_ridge_alpha_and_lambda():\n    \"\"\"Test the update of alpha and lambda in BayesianRidge\"\"\"\n    X = np.array([[1, 1], [3, 4], [5, 7], [4, 1], [2, 6], [3, 10], [3, 2]])\n    y = np.array([1, 2, 3, 2, 0, 4, 5]).T\n\n    clf = BayesianRidge(compute_score=True, n_iter=2, tol=1e-6)\n    clf.fit(X, y)\n\n    # check that the values of alpha and lambda have been updated\n    assert clf.alpha_ != 1. / (np.var(y) + np.finfo(np.float64).eps)\n    assert clf.lambda_ != 1.\n\n    # check that the values of alpha and lambda are positive\n    assert clf.alpha_ > 0\n    assert clf.lambda_ > 0\n", "def test_ard_regression_convergence():\n    # Test ARDRegression convergence on a noisy dataset.\n    random_state = check_random_state(42)\n    X = random_state.random_sample((100, 10))\n    y = 3 * X[:, 0] + 2 * X[:, 1] + random_state.randn(100)\n\n    ard = ARDRegression(n_iter=1000, tol=1e-6)\n    ard.fit(X, y)\n\n    # Check that the coefficients corresponding to the relevant features are\n    # non-zero and the others are zero.\n    assert np.count_nonzero(ard.coef_) == 2\n\n    # Check that the model has converged\n    assert ard.n_iter_ < ard.n_iter\n"], "sample_690": ["def test_evaluate_condition_with_invalid_code(pytester: Pytester) -> None:\n    item = pytester.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.skipif(\"invalid_code\")\n            pass\n    \"\"\"\n    )\n    with pytest.raises(pytest.fail.Exception) as excinfo:\n        evaluate_condition(item, item.iter_markers(name=\"skipif\").__next__(), \"invalid_code\")\n    assert excinfo.value.msg is not None\n    assert \"SyntaxError: invalid syntax\" in excinfo.value.msg\n", "def test_xfail_imperative_in_setup_class(self, pytester: Pytester) -> None:\n    p = pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        class TestClass(object):\n                pytest.xfail(\"hello\")\n\n                assert 0\n\n                assert 1\n    \"\"\"\n    )\n    result = pytester.runpytest(p)\n    result.stdout.fnmatch_lines([\"*1 xfailed*\"])\n    result = pytester.runpytest(p, \"-rx\")\n    result.stdout.fnmatch_lines([\"*XFAIL*TestClass::test_1*\", \"*reason:*hello*\"])\n    result = pytester.runpytest(p, \"--runxfail\")\n    result.stdout.fnmatch_lines(\n        \"\"\"\n        *TestClass::test_1*\n        *1 fail*\n    \"\"\"\n    )\n", "def test_xfail_with_empty_reason(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        test_one=\"\"\"\n            import pytest\n            @pytest.mark.xfail(reason=\"\")\n                assert 0\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-rxs\")\n    result.stdout.fnmatch_lines([\"*XFAIL*test_one.py::*condition: ''*\"])\n", "def test_skipif_with_nested_markers(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.skipif(\"hasattr(os, 'sep')\")\n        class TestClass(object):\n            @pytest.mark.skip\n                pass\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-rs\")\n    result.stdout.fnmatch_lines([\"*unconditional skip*\", \"*1 skipped*\"])\n", "def test_xfail_subclass_of_baseexception(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        class MyError(Exception):\n            pass\n\n        @pytest.mark.xfail(raises=MyError)\n            raise MyError(\"My error\")\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(xfailed=1)\n"], "sample_708": ["def test_getstatementrange_with_nested_functions() -> None:\n    source = Source(\n        \"\"\"\\\n                pass\n            inner()\n        outer()\n        \"\"\"\n    )\n    assert source.getstatementrange(1) == (0, 4)\n    assert source.getstatementrange(2) == (1, 3)\n    assert source.getstatementrange(3) == (1, 3)\n    assert source.getstatementrange(4) == (0, 4)\n", "def test_getstatementrange_with_multiline_string() -> None:\n    source = Source(\n        \"\"\"\\\n    s = '''hello\n    world'''\n    x = 3", "def test_getstatementrange_on_empty_lines():\n    source = Source(\n        \"\"\"\\", "def test_getstatementrange_nested_if_else() -> None:\n    source = Source(\n        \"\"\"\\", "    def setup_class(self) -> None:\n        self.source = Source(\n            \"\"\"\\"], "sample_22": ["def test_matrix_transpose():\n    \"\"\"Test the matrix transpose function.\"\"\"\n    # Normal rotation matrix\n    m1 = rotation_matrix(35 * u.deg, \"x\")\n    m1_t = matrix_transpose(m1)\n    assert_allclose(m1_t, m1.T)\n\n    # Stack of rotation matrices\n    n1 = np.tile(m1, (2, 1, 1))\n    n1_t = matrix_transpose(n1)\n    assert_allclose(n1_t, n1.swapaxes(-2, -1))\n\n    # Non-square matrix\n    m2 = np.array([[1, 2, 3], [4, 5, 6]])\n    with pytest.raises(ValueError):\n        matrix_transpose(m2)\n", "def test_matrix_transpose():\n    \"\"\"Test the matrix transpose function.\"\"\"\n    # Test transpose of a single matrix\n    m = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    assert_array_equal(matrix_transpose(m), m.swapaxes(-2, -1))\n\n    # Test transpose of a stack of matrices\n    m = np.stack((m, m))\n    assert_array_equal(matrix_transpose(m), m.swapaxes(-2, -1))\n\n    # Test transpose of a matrix with non-standard shape\n    m = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]])\n    assert_array_equal(matrix_transpose(m), m.swapaxes(-2, -1))\n", "def test_matrix_transpose():\n    \"\"\"Test the matrix transpose function.\"\"\"\n    m1 = np.array([[1, 2], [3, 4]])\n    assert_array_equal(matrix_transpose(m1), np.array([[1, 3], [2, 4]]))\n\n    m2 = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n    expected_m2 = np.array([[[1, 3], [2, 4]], [[5, 7], [6, 8]]])\n    assert_array_equal(matrix_transpose(m2), expected_m2)\n\n    m3 = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]], [[10, 11, 12], [13, 14, 15], [16, 17, 18]]])\n    expected_m3 = np.array([[[1, 4, 7], [2, 5, 8], [3, 6, 9]], [[10, 13, 16], [11, 14, 17], [12, 15, 18]]])\n    assert_array_equal(matrix_transpose(m3), expected_m3)\n", "def test_matrix_transpose():\n    \"\"\"Test matrix transpose.\"\"\"\n    m = rotation_matrix(45 * u.deg, \"x\")\n    mt = matrix_transpose(m)\n    assert_allclose(mt, m.T)\n\n    m2 = np.tile(m, (2, 1, 1))\n    mt2 = matrix_transpose(m2)\n    assert_allclose(mt2, m2.swapaxes(-2, -1))\n", "def test_matrix_transpose():\n    \"\"\"Test the matrix transpose function.\"\"\"\n    m = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    assert_array_equal(matrix_transpose(m), np.array([[1, 4, 7], [2, 5, 8], [3, 6, 9]]))\n\n    # Test with a stack of matrices\n    n = np.stack((m, m))\n    expected = np.stack((np.array([[1, 4, 7], [2, 5, 8], [3, 6, 9]]), np.array([[1, 4, 7], [2, 5, 8], [3, 6, 9]])))\n    assert_array_equal(matrix_transpose(n), expected)\n\n    # Test with a 1D array (should raise an error)\n    with pytest.raises(ValueError):\n        matrix_transpose(np.array([1, 2, 3]))\n"], "sample_929": ["def test_type_to_xref(app):\n    env = app.env\n    assert type_to_xref(\"int\", env).astext() == \"int\"\n    assert type_to_xref(\"List[int]\", env).astext() == \"List[int]\"\n    assert type_to_xref(\"Tuple[int, int]\", env).astext() == \"Tuple[int, int]\"\n    assert type_to_xref(\"Callable[[int, int], int]\", env).astext() == \"Callable[[int, int], int]\"\n    assert type_to_xref(\"None\", env).astext() == \"None\"\n", "def test_pyvariable_signature(app):\n    text = (\".. py:variable:: var\\n\"\n            \"   :type: str\\n\"\n            \"   :value: 'hello'\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"var\"],\n                                                    [desc_annotation, (\": \",\n                                                                       [pending_xref, \"str\"])],\n                                                    [desc_annotation, \" = 'hello'\"])],\n                                  [desc_content, ()])]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"data\",\n                domain=\"py\", objtype=\"data\", noindex=False)\n    assert 'var' in domain.objects\n    assert domain.objects['var'] == ('index', 'var', 'data')\n", "def test_pyattribute_with_annotation(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :annotation: List[int]\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'attr (Class attribute)', 'Class.attr', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"attr\"],\n                                                     [desc_annotation, (\": \",\n                                                                        [pending_xref, \"List\"],\n                                                                        [desc_sig_punctuation, \"[\"],\n                                                                        [pending_xref, \"int\"],\n                                                                        [desc_sig_punctuation, \"]\"])])],\n                                   [desc_content, ()]))\n    assert_node(doctree[1][1][1][0][1][1], pending_xref, **{\"py:class\": \"Class\"})\n    assert_node(doctree[1][1][1][0][1][3], pending_xref, **{\"py:class\": \"Class\"})\n\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :type: List[int]\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'attr (Class attribute)', 'Class.attr', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"attr\"],\n                                                     [desc_annotation, (\": \",\n                                                                        [pending_xref, \"List\"],\n                                                                        [desc_sig_punctuation, \"[\"],\n                                                                        [pending_xref, \"int\"],\n                                                                        [desc_sig_punctuation, \"]\"])])],\n                                   [desc_content, ()]))\n   ", "def test_parse_annotation_with_invalid_syntax(app):\n    doctree = _parse_annotation(\"int[int\", app.env)\n    assert_node(doctree, ([pending_xref, \"int[int\"],]))\n    assert_node(doctree[0], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"int[int\")\n\n    doctree = _parse_annotation(\"List[int, \", app.env)\n    assert_node(doctree, ([pending_xref, \"List[int, \"],))\n    assert_node(doctree[0], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"List[int, \")\n\n    doctree = _parse_annotation(\"Tuple[int int]\", app.env)\n    assert_node(doctree, ([pending_xref, \"Tuple[int int\"],]))\n    assert_node(doctree[0], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"Tuple[int int]\")\n", "def test_python_domain_clear_doc(app, status, warning):\n    app.builder.build_all()\n\n    domain = app.env.get_domain('py')\n\n    # Create a dummy document with some python objects\n    text = (\".. py:module:: test_module\\n\"\n            \".. py:class:: TestClass\\n\"\n            \"   .. py:method:: test_method\\n\")\n    restructuredtext.parse(app, text)\n\n    # Check that the objects are in the domain\n    assert 'test_module' in domain.modules\n    assert 'test_module.TestClass' in domain.objects\n    assert 'test_module.TestClass.test_method' in domain.objects\n\n    # Clear the document\n    domain.clear_doc('index')\n\n    # Check that the objects are removed from the domain\n    assert 'test_module' not in domain.modules\n    assert 'test_module.TestClass' not in domain.objects\n    assert 'test_module.TestClass.test_method' not in domain.objects\n"], "sample_951": ["def test_isabstractmethod():\n    class AbstractClass:\n        @abstractmethod\n            pass\n\n    class ConcreteClass(AbstractClass):\n            pass\n\n    assert inspect.isabstractmethod(AbstractClass.abstract_method) is True\n    assert inspect.isabstractmethod(ConcreteClass.abstract_method) is False\n", "def test_isabstractmethod():\n    class AbstractClass:\n        @abstractmethod\n            pass\n\n            pass\n\n    assert inspect.isabstractmethod(AbstractClass.abstract_method) is True\n    assert inspect.isabstractmethod(AbstractClass.concrete_method) is False\n    assert inspect.isabstractmethod(AbstractClass) is False\n    assert inspect.isabstractmethod(None) is False\n", "def test_isNewType():\n    class MyNewType(typing.NewType('MyNewType', int)):\n        pass\n\n    assert inspect.isNewType(MyNewType) is True\n    assert inspect.isNewType(int) is False\n    assert inspect.isNewType(object) is False\n    assert inspect.isNewType(None) is False\n", "def test_unwrap_all():\n        pass\n\n    func2 = functools.partial(func1, 1)\n    func2.__doc__ = \"func2\"\n    func3 = functools.partial(func2, 2)  # nested partial object\n\n    assert inspect.unwrap_all(func2) is func1\n    assert inspect.unwrap_all(func3) is func1\n\n        return obj.__name__ == 'func1'\n\n    assert inspect.unwrap_all(func3, stop=stop) is func2\n", "def test_isenumclass():\n    class EnumClassExample(enum.Enum):\n        A = 1\n        B = 2\n\n    class NonEnumClassExample:\n        pass\n\n    assert inspect.isenumclass(EnumClassExample) is True\n    assert inspect.isenumclass(NonEnumClassExample) is False\n\n"], "sample_1190": ["def test_UnitSystem_get_units_non_prefixed():\n    assert SI.get_units_non_prefixed() == {meter, gram, second, ampere, kelvin, mole, candela}\n", "def test_UnitSystem_extension():\n    new_system = SI.extend((Quantity(\"test_unit\"), name=\"Test System\")\n    assert new_system.name == \"Test System\"\n    assert len(new_system._base_units) == len(SI._base_units) + 1\n    assert new_system.is_consistent == SI.is_consistent\n", "def test_unit_system():\n    SI = UnitSystem.get_unit_system(\"SI\")\n    assert isinstance(SI, UnitSystem)\n    assert SI.name == \"SI\"\n    assert SI.dim == 7\n    assert SI.is_consistent\n\n    assert len(SI._base_units) == 7\n    assert len(SI._units) > len(SI._base_units)\n\n    assert SI.get_dimension_system() is not None\n    assert SI.get_quantity_dimension(meter) == length\n\n    assert SI.get_quantity_scale_factor(meter) == 1\n\n    assert SI.get_default_unit_system() is SI\n\n    assert SI.get_dimensional_expr(meter) == length.name\n    assert SI.get_dimensional_expr(meter/second) == length.name + '/' + time.name\n\n    assert SI._collect_factor_and_dimension(meter) == (1, length)\n\n    assert len(SI.get_units_non_prefixed()) > 0\n    assert meter in SI.get_units_non_prefixed()\n    assert kilometer not in SI.get_units_non_prefixed()\n\n    new_SI = SI.extend([Quantity(\"new_unit\")], name=\"new_SI\")\n    assert new_SI.name == \"new_SI\"\n    assert new_SI.dim == 8\n    assert len(new_SI._base_units) == 8\n    assert len(new_SI._units) > len(new_SI._base_units)\n", "def test_UnitSystem():\n    unit_system = UnitSystem(name=\"Custom\", base_units=(length, time, mass), \n                             units=(energy, pressure), \n                             dimension_system=Dimension(length=1, time=1, mass=1),\n                             derived_units={energy: joule})\n    \n    assert unit_system.name == \"Custom\"\n    assert unit_system.dim == 3\n    assert unit_system.is_consistent == True\n\n    assert len(unit_system._base_units) == 3\n    assert len(unit_system._units) == 5\n    assert len(unit_system._derived_units) == 1\n\n    assert str(unit_system) == \"Custom\"\n    assert repr(unit_system) == '<UnitSystem: (length, time, mass)>'\n\n    new_unit_system = unit_system.extend(base=(temperature, ), \n                                         units=(area, ), \n                                         name=\"CustomExtended\", \n                                         description=\"Extended Custom Unit System\",\n                                         derived_units={area: meter**2})\n\n    assert new_unit_system.name == \"CustomExtended\"\n    assert new_unit_system.dim == 4\n    assert new_unit_system.is_consistent == True\n\n    assert len(new_unit_system._base_units) == 4\n    assert len(new_unit_system._units) == 7\n    assert len(new_unit_system._derived_units) == 2\n\n    assert str(new_unit_system) == \"CustomExtended\"\n    assert repr(new_unit_system) == '<UnitSystem: (length, time, mass, temperature)>'\n\n    unit_system = UnitSystem.get_unit_system(\"SI\")\n    assert unit_system.name == \"SI\"\n\n    unit_system = UnitSystem.get_default_unit_system()\n    assert unit_system.name == \"SI\"\n\n    units = unit_system.get_units_non_prefixed()\n    assert meter in units\n    assert joule in units\n    assert second in units\n", "def test_unit_system_extension():\n    new_unit_system = SI.extend([Quantity(\"new_base_unit\")])\n    assert new_unit_system.dim == 8\n    assert new_unit_system.is_consistent\n    assert new_unit_system._base_units == (\n        Quantity(\"meter\"), Quantity(\"gram\"), Quantity(\"second\"),\n        Quantity(\"kelvin\"), Quantity(\"ampere\"), Quantity(\"mole\"),\n        Quantity(\"candela\"), Quantity(\"new_base_unit\")\n    )\n    assert new_unit_system._units == (\n        Quantity(\"meter\"), Quantity(\"kilogram\"), Quantity(\"gram\"),\n        Quantity(\"second\"), Quantity(\"minute\"), Quantity(\"hour\"),\n        Quantity(\"day\"), Quantity(\"kelvin\"), Quantity(\"ampere\"),\n        Quantity(\"mole\"), Quantity(\"candela\"), Quantity(\"new_base_unit\"),\n        Quantity(\"newton\"), Quantity(\"pascal\"), Quantity(\"joule\"),\n        Quantity(\"watt\"), Quantity(\"volt\"), Quantity(\"farad\"),\n        Quantity(\"ohm\"), Quantity(\"siemens\"), Quantity(\"weber\"),\n        Quantity(\"tesla\"), Quantity(\"henry\"), Quantity(\"coulomb\"),\n        Quantity(\"lumen\"), Quantity(\"lux\"), Quantity(\"becquerel\"),\n        Quantity(\"sievert\"), Quantity(\"gray\"), Quantity(\"radian\"),\n        Quantity(\"steradian\"), Quantity(\"hertz\"), Quantity(\"degree\"),\n        Quantity(\"dalton\"), Quantity(\"katal\"), Quantity(\"joule\"),\n        Quantity(\"katal\"), Quantity(\"degree_celsius\"), Quantity(\"byte\")\n    )\n    assert new_unit_system._derived_units == {\n        Dimension(length**2): Quantity(\"newton\"),\n        Dimension(mass): Quantity(\"kilogram\"),\n        Dimension(time**-1): Quantity(\"hertz\"),\n        Dimension(length**-2): Quantity(\"pascal\"),\n        Dimension(length**-2*mass*time**-2): Quantity(\"joule\"),\n        Dimension(length**2*time**-3): Quantity(\"watt\"),\n        Dimension(length**2*time**-3*mass**-1): Quantity(\"volt\"),\n        Dimension(time**-1*mass**-1): Quantity(\"farad\"),\n        Dimension(time*mass*length**-2): Quantity(\"ohm\"),\n        Dimension(time**-1*mass**-1*length**2): Quantity(\"siemens\"),\n        Dimension(length**2*mass*time**-2*current**-1): Quantity(\"weber\"),\n        Dimension(mass*time**-2*current**-1): Quantity(\""], "sample_36": ["def test_biweight_location_with_nan():\n    \"\"\"\n    Test that biweight_location raises a ValueError when input data contains NaN.\n    \"\"\"\n    data = np.array([1, 2, np.nan, 4, 5])\n    with pytest.raises(ValueError):\n        biweight_location(data)\n", "def test_biweight_midcorrelation_edge_cases():\n    \"\"\"\n    Test edge cases for biweight_midcorrelation.\n    \"\"\"\n\n    # Test correlation of two identical arrays\n    x = np.array([1, 2, 3, 4, 5])\n    y = np.array([1, 2, 3, 4, 5])\n    assert_allclose(biweight_midcorrelation(x, y), 1.0)\n\n    # Test correlation of two arrays with zero variance\n    x = np.array([1, 1, 1, 1, 1])\n    y = np.array([1, 1, 1, 1, 1])\n    assert_allclose(biweight_midcorrelation(x, y), 1.0)\n\n    # Test correlation of two arrays with one element\n    x = np.array([1])\n    y = np.array([1])\n    assert_allclose(biweight_midcorrelation(x, y), 1.0)\n\n    # Test correlation of two arrays with NaNs\n    x = np.array([1, 2, np.nan, 4, 5])\n    y = np.array([1, 2, np.nan, 4, 5])\n    with pytest.raises(ValueError) as e:\n        biweight_midcorrelation(x, y)\n    assert 'Input arrays cannot contain NaNs' in str(e.value)\n\n    # Test correlation of two arrays with infinite values\n    x = np.array([1, 2, np.inf, 4, 5])\n    y = np.array([1, 2, np.inf, 4, 5])\n    with pytest.raises(ValueError) as e:\n        biweight_midcorrelation(x, y)\n    assert 'Input arrays cannot contain infinite values' in str(e.value)\n", "def test_biweight_location_c():\n    \"\"\"Test the biweight location with different values of c.\"\"\"\n    with NumpyRNGContext(12345):\n        data = randn(1000)\n        c_values = [3.0, 5.0, 6.0, 9.0]\n        biweight_locations = [biweight_location(data, c=c) for c in c_values]\n        assert len(biweight_locations) == len(c_values)\n        assert all(isinstance(loc, float) for loc in biweight_locations)\n", "def test_biweight_location_c_parameter():\n    \"\"\"\n    Test that the biweight_location function raises a ValueError when\n    the c parameter is less than 0.\n    \"\"\"\n\n    data = [1, 3, 5, 500, 2]\n    with pytest.raises(ValueError) as e:\n        biweight_location(data, c=-5)\n    assert 'The c parameter must be a positive value.' not in str(e.value)\n\n    with pytest.raises(ValueError) as e:\n        biweight_location(data, c=0)\n    assert 'The c parameter must be a positive value.' in str(e.value)\n", "def test_biweight_midvariance_with_M():\n    \"\"\"\n    Test that biweight_midvariance works with the M parameter.\n    \"\"\"\n\n    data = [1, 3, 5, 500, 2]\n    M = 3\n    var = biweight_midvariance(data, M=M)\n    assert_allclose(var, 2.9238456)    # verified with R\n\n    data = [[1, 3, 5], [500, 2, 100]]\n    M = [3, 2]\n    cov = biweight_midcovariance(data, M=M)\n    assert_allclose(cov, [[2.9238456, -1.4619244],    # verified with R\n                          [-1.4619244, 2.3390765]])\n\n    M = 3\n    cov = biweight_midcovariance(data, M=M)\n    assert_allclose(cov, [[2.9238456, -1.4619244],    # verified with R\n                          [-1.4619244, 2.3390765]])\n"], "sample_227": ["def test_genericforeignkey(self):\n    modeladmin = BookmarkAdminGenericRelation(Bookmark, site)\n\n    request = self.request_factory.get('/')\n    request.user = self.alfred\n    changelist = modeladmin.get_changelist_instance(request)\n\n    request = self.request_factory.get('/', {'tags__content_type__pk': ContentType.objects.get_for_model(Book).pk})\n    request.user = self.alfred\n    changelist = modeladmin.get_changelist_instance(request)\n\n    # Make sure the correct queryset is returned\n    queryset = changelist.get_queryset(request)\n    self.assertEqual(list(queryset), [])\n\n    # Make sure the correct choice is selected\n    filterspec = changelist.get_filters(request)[0][0]\n    self.assertEqual(filterspec.title, 'tags')\n    choices = list(filterspec.choices(changelist))\n    self.assertEqual(len(choices), 1)\n\n    self.assertEqual(choices[0]['display'], 'All')\n    self.assertIs(choices[0]['selected'], True)\n    self.assertEqual(choices[0]['query_string'], '?')\n\n    request = self.request_factory.get('/', {'tags__object_id': self.bio_book.pk})\n    request.user = self.alfred\n    changelist = modeladmin.get_changelist_instance(request)\n\n    # Make sure the correct queryset is returned\n    queryset = changelist.get_queryset(request)\n    self.assertEqual(list(queryset), [])\n\n    # Make sure the correct choice is selected\n    filterspec = changelist.get_filters(request)[0][0]\n    self.assertEqual(filterspec.title, 'tags')\n    choices = list(filterspec.choices(changelist))\n    self.assertEqual(len(choices), 1)\n\n    self.assertEqual(choices[0]['display'], 'All')\n    self.assertIs(choices[0]['selected'], True)\n    self.assertEqual(choices[0]['query_string'], '?')\n\n    request = self.request_factory.get('/', {'tags__content_type__pk': ContentType.objects.get_for_model(Bookmark).pk})\n    request.user = self.alfred\n    changelist = modeladmin.get_changelist_instance(request)\n\n    # Make sure the correct queryset is returned\n    queryset = changelist.get_queryset(request)\n    self.assertEqual(list(queryset), [])\n\n    # Make sure the correct choice is selected\n    filterspec = changelist.get_filters(request)[0][0]\n    self.assertEqual(filterspec.title, 'tags')\n    choices = list(filterspec.choices(changelist))\n    self.assertEqual(len(choices), 1)\n\n    self.assertEqual(choices[0]['display'], '", "def test_genericforeignkey(self):\n    modeladmin = BookmarkAdminGenericRelation(Bookmark, site)\n\n    request = self.request_factory.get('/', {})\n    request.user = self.alfred\n    changelist = modeladmin.get_changelist_instance(request)\n\n    # Make sure the correct queryset is returned\n    queryset = changelist.get_queryset(request)\n    self.assertEqual(list(queryset), list(Bookmark.objects.all()))\n\n    # Make sure the correct choice is selected\n    filterspec = changelist.get_filters(request)[0][0]\n    self.assertEqual(filterspec.title, 'tags')\n    choices = list(filterspec.choices(changelist))\n\n    self.assertEqual(choices[0]['display'], 'All')\n    self.assertIs(choices[0]['selected'], True)\n    self.assertEqual(choices[0]['query_string'], '?')\n\n    # Filter by TaggedItem tag=='python' -------------------------------------\n\n    request = self.request_factory.get('/', {'tags__tag': 'python'})\n    request.user = self.alfred\n    changelist = modeladmin.get_changelist_instance(request)\n\n    # Make sure the correct queryset is returned\n    queryset = changelist.get_queryset(request)\n    self.assertEqual(list(queryset), [python_bookmark, django_bookmark])\n\n    filterspec = changelist.get_filters(request)[0][0]\n    self.assertEqual(filterspec.title, 'tags')\n    choices = list(filterspec.choices(changelist))\n\n    self.assertEqual(choices[0]['display'], 'All')\n    self.assertIs(choices[0]['selected'], False)\n    self.assertEqual(choices[0]['query_string'], '?')\n\n    self.assertEqual(choices[1]['display'], 'python')\n    self.assertIs(choices[1]['selected'], True)\n    self.assertEqual(choices[1]['query_string'], '?tags__tag=python')\n", "def test_genericforeignkey(self):\n    \"\"\"\n    GenericForeignKey can be used in a model's list_filter.\n    \"\"\"\n    class TaggedItemAdmin(ModelAdmin):\n        list_filter = ('content_object',)\n\n    site.register(TaggedItem, TaggedItemAdmin)\n    self.addCleanup(lambda: site.unregister(TaggedItem))\n\n    django_bookmark = Bookmark.objects.create(url='https://www.djangoproject.com/')\n    python_bookmark = Bookmark.objects.create(url='https://www.python.org/')\n    TaggedItem.objects.create(content_object=django_bookmark, tag='python')\n    TaggedItem.objects.create(content_object=python_bookmark, tag='python')\n\n    modeladmin = TaggedItemAdmin(TaggedItem, site)\n    request = self.request_factory.get('/', {})\n    request.user = self.alfred\n    changelist = modeladmin.get_changelist_instance(request)\n    queryset = changelist.get_queryset(request)\n    self.assertEqual(list(queryset), list(TaggedItem.objects.all()))\n\n    request = self.request_factory.get('/', {'content_object__id__exact': django_bookmark.pk})\n    request.user = self.alfred\n    changelist = modeladmin.get_changelist_instance(request)\n    queryset = changelist.get_queryset(request)\n    self.assertEqual(list(queryset), [TaggedItem.objects.get(content_object=django_bookmark)])\n\n    request = self.request_factory.get('/', {'content_object__id__exact': python_bookmark.pk})\n    request.user = self.alfred\n    changelist = modeladmin.get_changelist_instance(request)\n    queryset = changelist.get_queryset(request)\n    self.assertEqual(list(queryset), [TaggedItem.objects.get(content_object=python_bookmark)])\n", "def test_genericforeignkey(self):\n    from .models import TaggedItem\n\n    class GenericForeignKeyAdmin(ModelAdmin):\n        list_filter = ('content_object',)\n\n    modeladmin = GenericForeignKeyAdmin(TaggedItem, site)\n    request = self.request_factory.get('/')\n    request.user = self.alfred\n    changelist = modeladmin.get_changelist_instance(request)\n\n    # Make sure the correct queryset is returned\n    queryset = changelist.get_queryset(request)\n    self.assertEqual(list(queryset), list(TaggedItem.objects.all()))\n\n    # Make sure the correct choice is selected\n    filterspec = changelist.get_filters(request)[0][0]\n    self.assertEqual(filterspec.title, 'content object')\n    choices = list(filterspec.choices(changelist))\n\n    self.assertEqual(choices[0]['display'], 'All')\n    self.assertIs(choices[0]['selected'], True)\n    self.assertEqual(choices[0]['query_string'], '?')\n\n    # TaggedItem has one Bookmark instance.\n    bookmark_tag = TaggedItem.objects.create(content_object=self.python_bookmark, tag='python')\n    request = self.request_factory.get('/', {'content_object__id__exact': bookmark_tag.pk})\n    request.user = self.alfred\n    changelist = modeladmin.get_changelist_instance(request)\n\n    # Make sure the correct queryset is returned\n    queryset = changelist.get_queryset(request)\n    self.assertEqual(list(queryset), [bookmark_tag])\n\n    # Make sure the correct choice is selected\n    filterspec = changelist.get_filters(request)[0][0]\n    self.assertEqual(filterspec.title, 'content object')\n    choices = list(filterspec.choices(changelist))\n\n    self.assertEqual(choices[1]['display'], 'https://www.python.org/')\n    self.assertIs(choices[1]['selected'], True)\n    self.assertEqual(choices[1]['query_string'], '?content_object__id__exact=%d' % bookmark_tag.pk)\n", "def test_genericrelation_reverse_descriptor(self):\n    \"\"\"\n    Ensure GenericRelation reverse descriptor behaves like other reverse\n    descriptors (#20473).\n    \"\"\"\n    class BookmarkTagAdmin(ModelAdmin):\n        list_filter = ('bookmarks',)\n\n    bookmark = Bookmark.objects.create(url='https://www.djangoproject.com/')\n    TaggedItem.objects.create(content_object=bookmark, tag='python')\n    TaggedItem.objects.create(content_object=Bookmark.objects.create(url='https://www.python.org/'), tag='python')\n\n    modeladmin = BookmarkTagAdmin(TaggedItem, site)\n\n    request = self.request_factory.get('/')\n    request.user = self.alfred\n    changelist = modeladmin.get_changelist_instance(request)\n\n    # Make sure that only actual bookmarks are present in bookmarks's list filter\n    filterspec = changelist.get_filters(request)[0][0]\n    expected = [(bookmark.pk, bookmark.url)]\n    self.assertEqual(filterspec.lookup_choices, expected)\n"], "sample_756": ["def test_min_cluster_size_fraction():\n    # Test min_cluster_size as a fraction of the total number of samples\n    clust = OPTICS(min_samples=10, min_cluster_size=0.1).fit(X)\n    cluster_sizes = np.bincount(clust.labels_[clust.labels_ != -1])\n    if cluster_sizes.size:\n        assert min(cluster_sizes) >= int(0.1 * X.shape[0])\n", "def test_extract_dbscan_at_zero_eps():\n    # Test extract_dbscan where eps is set to 0\n    centers = [[1, 1], [-1, -1], [1, -1]]\n    X, labels_true = make_blobs(n_samples=750, centers=centers,\n                                cluster_std=0.4, random_state=0)\n\n    # Compute OPTICS\n    clust = OPTICS(max_eps=5.0 * 0.03, min_samples=10)\n    clust2 = clust.fit(X)\n    assert_warns(RuntimeWarning, clust2.extract_dbscan, 0)\n", "def test_cluster_tree():\n    # Test that the cluster tree is built correctly\n    reachability_plot = [np.inf, 0.9, 0.9, 1.0, 0.89, 0.88, 10, .9, .9, .9, 10, 0.9,\n                         0.9, 0.89, 0.88, 10, .9, .9, .9, .9]\n    ordering = np.r_[0:20]\n    min_cluster_size = 5\n    maxima_ratio = .75\n    rejection_ratio = .7\n    similarity_threshold = .4\n    significant_min = .3\n    min_maxima_ratio = .001\n\n    cluster_boundaries = _find_local_maxima(reachability_plot, 5)\n    root = _TreeNode(ordering, 0, 20, None)\n\n    # Build cluster tree inplace on root node\n    _cluster_tree(root, None, cluster_boundaries, reachability_plot, ordering,\n                  min_cluster_size, maxima_ratio, rejection_ratio,\n                  similarity_threshold, significant_min)\n\n    assert root.split_point == cluster_boundaries[0]\n    assert len(root.children) == 2\n    assert_array_equal(root.children[0].points, np.r_[0:6])\n    assert_array_equal(root.children[1].points, np.r_[11:20])\n\n    # Test that the cluster tree is built correctly with multiple levels\n    reachability_plot = [np.inf, 0.9, 0.9, 1.0, 0.89, 0.88, 10, .9, .9, .9, 10, 0.9,\n                         0.9, 0.89, 0.88, 10, .9, .9, .9, .9, .9, .9, .9, .9, 10]\n    ordering = np.r_[0:25]\n    min_cluster_size = 5\n    maxima_ratio = .75\n    rejection_ratio = .7\n    similarity_threshold = .4\n    significant_min = .3\n    min_maxima_ratio = .001\n\n    cluster_boundaries = _find_local_maxima(reachability_plot, 5)\n    root = _TreeNode(ordering, 0, 25, None)\n\n    # Build cluster tree inplace on root node", "def test_min_cluster_size_zero():\n    # Test that min_cluster_size is correctly rounded to 2 when set to 0\n    clust = OPTICS(min_samples=10, min_cluster_size=0).fit(X)\n    cluster_sizes = np.bincount(clust.labels_[clust.labels_ != -1])\n    if cluster_sizes.size:\n        assert min(cluster_sizes) >= 2\n    # check that min_cluster_size 0 is rounded to 2, not 1\n    assert clust.min_cluster_size == 2\n", "def test_extract_dbscan_min_samples():\n    # Test that min_samples parameter from OPTICS is used by extract_dbscan\n    # Compute OPTICS\n    clust = OPTICS(min_samples=9).fit(X)\n    # Extract DBSCAN\n    core_optics, labels_optics = clust.extract_dbscan(0.3)\n    # Compute DBSCAN with same parameters\n    db = DBSCAN(min_samples=9, eps=0.3).fit(X)\n    # Verify labels and core samples are the same\n    assert_array_equal(labels_optics, db.labels_)\n    assert_array_equal(core_optics, db.core_sample_indices_)\n"], "sample_44": ["def test_function_unit_to_string():\n    lu1 = u.mag(u.Jy)\n    lu2 = u.MagUnit(u.Jy, function_unit=2*u.mag)\n\n    assert lu1.to_string('generic') == 'mag(Jy)'\n    assert lu1.to_string('unscaled') == 'mag(Jy)'\n    assert lu1.to_string('latex') == ('$\\\\mathrm{mag}$$\\\\mathrm{\\\\left( '\n                                    '\\\\mathrm{Jy} \\\\right)}$')\n    assert lu1._repr_latex_() == lu1.to_string('latex')\n\n    assert lu2.to_string('generic') == '2 mag(Jy)'\n    assert lu2.to_string('unscaled') == '2 mag(Jy)'\n    assert lu2.to_string('latex') == ('$2\\\\mathrm{mag}$$\\\\mathrm{\\\\left( '\n                                    '\\\\mathrm{Jy} \\\\right)}$')\n    assert lu2._repr_latex_() == lu2.to_string('latex')\n\n    with pytest.raises(ValueError):\n        lu1.to_string('fits')\n\n    lu3 = u.dex(u.Jy)\n    assert lu3.to_string() == 'dex(Jy)'\n    assert lu3.to_string('latex') == ('$\\\\mathrm{dex}$$\\\\mathrm{\\\\left( '\n                                    '\\\\mathrm{Jy} \\\\right)}$')\n    assert lu3._repr_latex_() == lu3.to_string('latex')\n", "    def setup(self):\n        self.lq = u.Magnitude(np.arange(10.) * u.Jy)\n        self.lq2 = u.Magnitude(np.arange(5.))\n", "    def test_function_view(self):\n        \"\"\"Test the _function_view property.\"\"\"\n        lq = u.Magnitude(np.arange(1., 11.)*u.Jy)\n        fv = lq._function_view\n        assert fv.unit is lq.unit.function_unit\n        fv[3] = -2. * fv.unit\n        assert np.all(lq.value == fv.value)\n", "    def test_comparison_to_non_log_quantities(self):\n        lq = u.Magnitude(np.arange(1., 10.)*u.Jy)\n        q = 1.23 * u.Jy\n\n        assert np.all((lq > q) == np.array([True, True, True, True, True, True, True, True, True]))\n        assert np.all((lq == q) == np.array([False, False, False, False, False, False, False, False, False, False]))\n        assert np.all((lq < q) == np.array([False, False, False, False, False, False, False, False, False]))\n", "    def test_add_equivalency(self):\n        lu = u.mag(u.Jy)\n        lu.add_equivalency(u.Jy/u.Hz, u.Jy/u.Hz)\n        assert lu.is_equivalent(u.Jy/u.Hz)\n        assert lu.to(u.Jy/u.Hz, 1) == 0\n"], "sample_930": ["def test_create_index_with_fixre(app):\n    text = (\".. index:: func() (in module foo)\\n\"\n            \".. index:: func() (in module bar)\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 1\n    assert index[0] == ('F', [('func()', [[(' (in module foo)', '#index-0'),\n                                            (' (in module bar)', '#index-1')], [], None])])\n", "def test_create_index_with_empty_entries(app):\n    text = (\".. index::\\n\"\n            \".. index:: single: \\n\"\n            \".. index:: pair: ; \\n\"\n            \".. index:: triple: ; ; \\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 0\n", "def test_create_index_with_fixre(app):\n    text = (\".. index:: foo (bar)\\n\"\n            \".. index:: foo (baz)\\n\"\n            \".. index:: foo (bar) (in module qux)\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 1\n    assert index[0] == ('F', [('foo', [[], [('bar (in module qux)', [('main', '#index-2')],\n                                               ('baz', [('main', '#index-1')])], None])])\n", "def test_create_index_with_multiple_subentries(app):\n    text = (\".. index:: single: docutils\\n\"\n            \".. index:: single: docutils; subentry1\\n\"\n            \".. index:: single: docutils; subentry2\\n\"\n            \".. index:: pair: Python; interpreter\\n\"\n            \".. index:: pair: Python; :+1:\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 2\n    assert index[0] == ('D', [('docutils', [[('', '#index-0')],\n                                            [('subentry1', [('', '#index-1')]),\n                                             ('subentry2', [('', '#index-2')])], None])])\n    assert index[1] == ('P', [('Python', [[],\n                                             [(':+1:', [('', '#index-4')]),\n                                              ('interpreter', [('', '#index-3')])], None])])\n", "def test_create_index_with_multiple_same_subentry(app):\n    text = (\".. index:: single: docutils\\n\"\n            \".. index:: single: docutils\\n\"\n            \".. index:: pair: docutils; reStructuredText\\n\"\n            \".. index:: pair: docutils; reStructuredText\\n\"\n            \".. index:: triple: docutils; reStructuredText; Python\\n\"\n            \".. index:: triple: docutils; reStructuredText; Python\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 3\n    assert index[0] == ('D', [('docutils', [[('', '#index-0'), ('', '#index-1')], \n                                             [('reStructuredText', [('', '#index-2'), \n                                                                      ('', '#index-4')])], \n                                             None])])\n    assert index[1] == ('P', [('Python', [[], [('docutils reStructuredText', [('', '#index-4')])], None])])\n    assert index[2] == ('R', [('reStructuredText', [[], [('docutils', [('', '#index-2'), \n                                                                      ('', '#index-4')])], None])])\n"], "sample_375": ["    def test_get_related_models_recursive(self):\n        class A(models.Model):\n            pass\n\n        class B(A):\n            pass\n\n        class C(models.Model):\n            a = models.ForeignKey(A, models.CASCADE)\n\n        class D(models.Model):\n            b = models.ForeignKey(B, models.CASCADE)\n\n        class E(models.Model):\n            c = models.ForeignKey(C, models.CASCADE)\n\n        self.assertEqual(\n            get_related_models_recursive(A),\n            {(A._meta.app_label, A._meta.model_name),\n             (B._meta.app_label, B._meta.model_name),\n             (C._meta.app_label, C._meta.model_name),\n             (D._meta.app_label, D._meta.model_name),\n             (E._meta.app_label, E._meta.model_name)},\n        )\n\n        self.assertEqual(\n            get_related_models_recursive(B),\n            {(A._meta.app_label, A._meta.model_name),\n             (B._meta.app_label, B._meta.model_name),\n             (C._meta.app_label, C._meta.model_name),\n             (D._meta.app_label, D._meta.model_name),\n             (E._meta.app_label, E._meta.model_name)},\n        )\n\n        self.assertEqual(\n            get_related_models_recursive(C),\n            {(A._meta.app_label, A._meta.model_name),\n             (B._meta.app_label, B._meta.model_name),\n             (C._meta.app_label, C._meta.model_name),\n             (D._meta.app_label, D._meta.model_name),\n             (E._meta.app_label, E._meta.model_name)},\n        )\n\n        self.assertEqual(\n            get_related_models_recursive(D),\n            {(A._meta.app_label, A._meta.model_name),\n             (B._meta.app_label, B._meta.model_name),\n             (C._meta.app_label, C._meta.model_name),\n             (D._meta.app_label, D._meta.model_name),\n             (E._meta.app_label, E._meta.model_name)},\n        )\n\n        self.assertEqual(\n            get_related_models_recursive(E),\n            {(A._meta.app_label, A._meta.model_name),\n             (B._meta.app_label, B._meta.model_name),\n             (C._meta.app_label, C._meta.model_name),\n             (D._meta.app_label, D._meta.model_name),\n             (E._meta.app_label, E._meta.model_name)},\n        )\n", "def test_rename_field_to_same_name(self):\n    project_state = ProjectState()\n    project_state.add_model(ModelState(\n        app_label='migrations',\n        name='Tag',\n        fields=[\n            ('id', models.AutoField(primary_key=True)),\n            ('name', models.CharField(max_length=100)),\n        ],\n    ))\n    project_state.apps  # Render project state.\n    operation = RenameField(\n        model_name='tag',\n        old_name='name',\n        new_name='name',\n    )\n    operation.state_forwards('migrations', project_state)\n    self.assertEqual(project_state.models['migrations', 'tag'].fields, {\n        'id': models.AutoField(primary_key=True),\n        'name': models.CharField(max_length=100),\n    })\n", "    def test_register_model(self):\n        \"\"\"\n        Test that the StateApps.register_model method correctly registers a model\n        and sets the model's apps attribute.\n        \"\"\"\n        project_state = ProjectState()\n        apps = project_state.apps\n        model_state = ModelState('migrations', 'TestModel', [\n            ('id', models.AutoField(primary_key=True)),\n        ])\n        model = model_state.render(apps)\n        apps.register_model('migrations', model)\n        self.assertEqual(model._meta.apps, apps)\n        self.assertEqual(apps.get_model('migrations', 'TestModel'), model)\n", "    def test_rename_model_referencing_self(self):\n        project_state = ProjectState()\n        project_state.add_model(ModelState(\n            app_label='migrations',\n            name='Tag',\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=100)),\n            ],\n        ))\n        project_state.add_model(ModelState(\n            app_label='migrations',\n            name='Post',\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"title\", models.CharField(max_length=100)),\n                (\"tag\", models.ForeignKey('migrations.Tag', models.CASCADE)),\n            ],\n        ))\n        project_state.apps  # Render project state.\n        old_state = project_state.clone()\n        A = old_state.apps.get_model('migrations', 'Tag')\n        B = old_state.apps.get_model('migrations', 'Post')\n        self.assertIs(B._meta.get_field('tag').related_model, A)\n        project_state.rename_model('migrations', 'Tag', 'Category')\n        new_state = project_state.clone()\n        A = new_state.apps.get_model('migrations', 'Category')\n        B = new_state.apps.get_model('migrations', 'Post')\n        self.assertIs(B._meta.get_field('tag').related_model, A)\n", "def test_remove_concrete_model_from_proxy(self):\n    new_apps = Apps()\n\n    class User(models.Model):\n        class Meta:\n            app_label = 'tests'\n            apps = new_apps\n\n    class UserProxy(User):\n        class Meta:\n            app_label = 'tests'\n            apps = new_apps\n            proxy = True\n\n    class UserProfile(UserProxy):\n        bio = models.TextField()\n\n        class Meta:\n            app_label = 'tests'\n            apps = new_apps\n\n    project_state = ProjectState()\n    project_state.add_model(ModelState.from_model(User))\n    project_state.add_model(ModelState.from_model(UserProxy))\n    project_state.add_model(ModelState.from_model(UserProfile))\n\n    # Removing the User model will trigger a removal of the UserProxy model\n    # due to the proxy model mechanics.\n    project_state.remove_model('tests', 'user')\n    with self.assertRaises(KeyError):\n        project_state.models['tests', 'userproxy']\n    with self.assertRaises(KeyError):\n        project_state.models['tests', 'userprofile']\n\n    # The same should happen if we remove UserProxy.\n    project_state.add_model(ModelState.from_model(User))\n    project_state.add_model(ModelState.from_model(UserProxy))\n    project_state.add_model(ModelState.from_model(UserProfile))\n    project_state.remove_model('tests', 'userproxy')\n    with self.assertRaises(KeyError):\n        project_state.models['tests', 'userproxy']\n    with self.assertRaises(KeyError):\n        project_state.models['tests', 'userprofile']\n\n    # However, removing UserProfile should not remove the User model.\n    project_state.add_model(ModelState.from_model(User))\n    project_state.add_model(ModelState.from_model(UserProxy))\n    project_state.add_model(ModelState.from_model(UserProfile))\n    project_state.remove_model('tests', 'userprofile')\n    self.assertIn(('tests', 'user'), project_state.models)\n"], "sample_980": ["def test_power():\n    # Test __pow__ method with a Symbol\n    a = Symbol('a', integer=True)\n    p = Permutation([0, 2, 1])\n    raises(NotImplementedError, lambda: p**a)\n", "def test_power():\n    p = Permutation([1, 2, 3, 0])\n    assert p**0 == Permutation([0, 1, 2, 3])\n    assert p**1 == p\n    assert p**2 == Permutation([2, 3, 0, 1])\n    assert p**3 == Permutation([3, 0, 1, 2])\n    assert p**4 == Permutation([0, 1, 2, 3])\n    p = Permutation([1, 2, 0, 3])\n    assert p**0 == Permutation([0, 1, 2, 3])\n    assert p**1 == p\n    assert p**2 == Permutation([2, 0, 1, 3])\n    assert p**3 == Permutation([0, 1, 2, 3])\n    p = Permutation([1, 0, 2, 3])\n    assert p**0 == Permutation([0, 1, 2, 3])\n    assert p**1 == p\n    assert p**2 == Permutation([0, 1, 2, 3])\n    p = Permutation([3, 0, 1, 2])\n    assert p**0 == Permutation([0, 1, 2, 3])\n    assert p**1 == p\n    assert p**2 == Permutation([0, 1, 2, 3])\n    assert p**(-1) == Permutation([1, 2, 3, 0])\n    assert p**(-2) == Permutation([0, 1, 2, 3])\n", "def test_printing_cyclic_with_singletons():\n    Permutation.print_cyclic = True\n    p1 = Permutation([0, 1, 2, 3, 4, 5, 6])\n    assert repr(p1) == 'Permutation()'\n    assert str(p1) == '()'\n    p2 = Permutation([1, 2, 0, 3, 4, 5, 6])\n    assert repr(p2) == 'Permutation(3)(0, 1)'\n    assert str(p2) == '(0 1)'\n    p3 = Permutation([0, 2, 1, 3, 4, 5, 6])\n    assert repr(p3) == 'Permutation(3)(1, 2)'\n    assert str(p3) == '(1 2)'\n    p4 = Permutation([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n    assert repr(p4) == 'Permutation()'\n    assert str(p4) == '()'\n    p5 = Permutation([1, 2, 0, 3, 4, 5, 6, 7, 8, 9, 10])\n    assert repr(p5) == 'Permutation(10)(0, 1)'\n    assert str(p5) == '(0 1)(10)'\n    p6 = Permutation([0, 2, 1, 3, 4, 5, 6, 7, 8, 9, 10])\n    assert repr(p6) == 'Permutation(10)(1, 2)'\n    assert str(p6) == '(1 2)(10)'\n    p7 = Permutation([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])\n    assert repr(p7) == 'Permutation()'\n    assert str(p7) == '()'\n    p8 = Permutation([1, 2, 0, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])\n    assert repr(p8) == 'Permutation(12", "def test_edge_cases():\n    # test corner cases for the functions\n    p = Permutation([0, 1, 2, 3])\n    assert p.rank_trotterjohnson() == 0\n    assert p.next_trotterjohnson() == Permutation([0, 1, 3, 2])\n    assert p.unrank_trotterjohnson(4, 0) == p\n    assert p.rank_nonlex() == 23\n    assert p.next_nonlex() is not None\n    assert p.unrank_nonlex(4, 23) == p\n    assert p.inversion_vector() == [0, 0, 0, 0]\n    assert p.get_precedence_distance(p) == 0\n    assert p.get_adjacency_distance(p) == 3\n    assert p.get_positional_distance(p) == 0\n    assert p.get_precedence_matrix().is_zero\n    assert p.get_adjacency_matrix().is_zero\n    raises(ValueError, lambda: p.get_adjacency_distance(Permutation([])))\n    raises(ValueError, lambda: p.get_positional_distance(Permutation([])))\n    raises(ValueError, lambda: p.get_precedence_distance(Permutation([])))\n\n    p = Permutation([1, 0, 2, 3])\n    assert p.rank_trotterjohnson() == 7\n    assert p.next_trotterjohnson() == Permutation([0, 1, 3, 2])\n    assert p.unrank_trotterjohnson(4, 7) == p\n    assert p.rank_nonlex() == 5\n    assert p.next_nonlex() is not None\n    assert p.unrank_nonlex(4, 5) == p\n    assert p.inversion_vector() == [1, 0, 0, 0]\n    assert p.get_precedence_distance(p) == 0\n    assert p.get_adjacency_distance(p) == 3\n    assert p.get_positional_distance(p) == 0\n    assert p.get_precedence_matrix().is_zero\n    assert p.get_adjacency_matrix().is_zero\n\n    p = Permutation([3, 2, 1, 0])\n    assert p.rank_trotterjohnson() == 23\n    assert p.next_trotterjohnson() is None\n    assert p.unrank_trotterjohnson", "def test_get_adjacency_matrix_and_distance():\n    # adjacency matrix and distance testing\n    p = Permutation([4, 1, 5, 0, 2, 3])\n    assert p.get_adjacency_matrix() == [\n        [0, 0, 0, 1, 0, 0],\n        [1, 0, 0, 0, 1, 0],\n        [0, 1, 0, 0, 0, 1],\n        [0, 0, 1, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 1, 0, 0]\n    ]\n    q = Permutation([2, 3, 1, 0, 4, 5])\n    assert p.get_adjacency_distance(q) == 5\n    # test get_adjacency_distance on an identity permutation\n    assert p.get_adjacency_distance(Permutation(6)) == 5\n    # test get_adjacency_distance with identical permutations\n    assert p.get_adjacency_distance(p) == 0\n"], "sample_284": ["    def test_hashed_name_content_none(self):\n        storage.staticfiles_storage = storage.StaticFilesStorage()\n        name = 'test.txt'\n        content = None\n        with self.assertRaises(ValueError):\n            storage.staticfiles_storage.hashed_name(name, content)\n", "    def test_hashed_name_multiple_passes(self):\n        \"\"\"\n        The hashed name of a file is resolved correctly after multiple passes\n        of post-processing.\n        \"\"\"\n        # Create initial static files.\n        file_contents = (\n            ('foo.png', 'foo'),\n            ('bar.css', 'url(\"foo.png\")\\nurl(\"xyz.png\")'),\n            ('xyz.png', 'url(\"foo.png\")'),\n        )\n        for filename, content in file_contents:\n            with open(os.path.join(settings.STATIC_ROOT, filename), 'w') as f:\n                f.write(content)\n\n        storage.staticfiles_storage.max_post_process_passes = 3\n        with self.modify_settings(STATICFILES_DIRS={'append': settings.STATIC_ROOT}):\n            finders.get_finder.cache_clear()\n            err = StringIO()\n            call_command('collectstatic', interactive=False, verbosity=0, stderr=err)\n\n        relpath = self.hashed_file_path('bar.css')\n        with storage.staticfiles_storage.open(relpath) as relfile:\n            content = relfile.read()\n            self.assertIn(b'foo.', content)\n            self.assertIn(b'xyz.', content)\n", "    def setUp(self):\n        self.storage = ManifestFilesMixin()\n", "    def setUp(self):\n        super().setUp()\n        self._temp_dir = temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(temp_dir, 'test'))\n        self.addCleanup(shutil.rmtree, temp_dir)\n        storage.staticfiles_storage.manifest_strict = False\n", "    def test_post_processing_failure(self):\n        \"\"\"\n        post_processing failure raises an exception with a useful message.\n        \"\"\"\n        finders.get_finder.cache_clear()\n        err = StringIO()\n        with self.assertRaises(Exception) as cm:\n            call_command('collectstatic', interactive=False, verbosity=0, stderr=err)\n        self.assertEqual(\"Post-processing 'All' failed!\\n\\n\", err.getvalue())\n        self.assertIsInstance(cm.exception, RuntimeError)\n"], "sample_771": ["def test_power_transformer_check_error():\n    X = np.transpose([[0, 25, 50, 0, 0, 0, 75, 0, 0, 100],\n                      [2, 4, 0, 0, 6, 8, 0, 10, 0, 0],\n                      [0, 0, 2.6, 4.1, 0, 0, 2.3, 0, 9.5, 0.1]])\n    X = sparse.csc_matrix(X)\n    X_neg = np.transpose([[0, 25, 50, 0, 0, 0, 75, 0, 0, 100],\n                          [-2, 4, 0, 0, 6, 8, 0, 10, 0, 0],\n                          [0, 0, 2.6, 4.1, 0, 0, 2.3, 0, 9.5, 0.1]])\n    X_neg = sparse.csc_matrix(X_neg)\n\n    assert_raises_regex(ValueError, \"Invalid value for 'n_quantiles': 0.\",\n                        PowerTransformer(n_quantiles=0).fit, X)\n    assert_raises_regex(ValueError, \"Invalid value for 'subsample': 0.\",\n                        PowerTransformer(subsample=0).fit, X)\n    assert_raises_regex(ValueError, \"The number of quantiles cannot be greater than\"\n                        \" the number of samples used. Got 1000 quantiles\"\n                        \" and 10 samples.\", PowerTransformer(subsample=10).fit, X)\n\n    transformer = PowerTransformer(n_quantiles=10)\n    assert_raises_regex(ValueError, \"PowerTransformer only accepts\"\n                        \" non-negative sparse matrices.\",\n                        transformer.fit, X_neg)\n    transformer.fit(X)\n    assert_raises_regex(ValueError, \"PowerTransformer only accepts \"\n                        \"non-negative sparse matrices.\",\n                        transformer.transform, X_neg)\n\n    X_bad_feat = np.transpose([[0, 25, 50, 0, 0, 0, 75, 0, 0, 100],\n                               [0, 0, 2.6, 4.1, 0, 0, 2.3, 0, 9.5, 0.1]])\n    assert_raises_regex(ValueError, \"X does not", "def test_power_transformer_dtypes():\n    # Check that the PowerTransformer handles dtypes correctly\n    pt = PowerTransformer(method='yeo-johnson', standardize=False, copy=False)\n\n    X = np.abs(X_2d).astype(np.float32)\n    X_trans = pt.fit_transform(X)\n    assert X_trans.dtype == np.float32\n    X_inv_trans = pt.inverse_transform(X_trans)\n    assert X_inv_trans.dtype == np.float32\n\n    X = np.abs(X_2d).astype(np.float64)\n    X_trans = pt.fit_transform(X)\n    assert X_trans.dtype == np.float64\n    X_inv_trans = pt.inverse_transform(X_trans)\n    assert X_inv_trans.dtype == np.float64\n", "def test_power_transformer_edge_cases(method):\n    pt = PowerTransformer(method=method)\n    X = np.abs(X_2d)\n\n    # Test edge cases for lambda\n    pt.lambdas_ = np.array([1e-10, 1e10])\n    X_trans = pt.transform(X[:, :2])\n    assert_array_almost_equal(X, pt.inverse_transform(X_trans))\n\n    pt.lambdas_ = np.array([1e-5, 1e-5])\n    X_trans = pt.transform(X[:, :2])\n    assert_array_almost_equal(X, pt.inverse_transform(X_trans))\n\n    pt.lambdas_ = np.array([1e5, 1e5])\n    X_trans = pt.transform(X[:, :2])\n    assert_array_almost_equal(X, pt.inverse_transform(X_trans))\n", "def test_power_transformer_axis_exception():\n    # Exceptions should be raised for non-integer axis values\n    # and for axes outside the allowed range\n\n    pt = PowerTransformer(method='box-cox')\n\n    X = np.abs(X_2d)\n\n    not_int_message = \"The axis must be an integer\"\n    assert_raise_message(ValueError, not_int_message,\n                         pt.fit, X, axis=2.5)\n\n    not_in_range_message = \"The axis must be either 0 or 1\"\n    assert_raise_message(ValueError, not_in_range_message,\n                         pt.fit, X, axis=-1)\n    assert_raise_message(ValueError, not_in_range_message,\n                         pt.fit, X, axis=2)\n", "def test_quantile_transform_random_state():\n    # Test that the random state is used to subsample the input.\n    # We check that different random states yield different results.\n    n_samples = 1000\n    n_features = 1\n    X = np.random.RandomState(42).randn(n_samples, n_features)\n\n    transformer_1 = QuantileTransformer(n_quantiles=100, random_state=0)\n    transformer_2 = QuantileTransformer(n_quantiles=100, random_state=1)\n    transformer_3 = QuantileTransformer(n_quantiles=100, random_state=0)\n\n    X_trans_1 = transformer_1.fit_transform(X)\n    X_trans_2 = transformer_2.fit_transform(X)\n    X_trans_3 = transformer_3.fit_transform(X)\n\n    assert_array_almost_equal(X_trans_1, X_trans_3)\n    assert X_trans_1 is not X_trans_2\n    assert X_trans_1 is not X_trans_3\n    assert_array_not_almost_equal(X_trans_1, X_trans_2)\n"], "sample_1192": ["def test_Wild_exclude():\n    x, y, z = symbols('x,y,z')\n\n    a = Wild('a')\n    b = Wild('b', exclude=[x])\n\n    assert y.match(a) == {a_: y}\n    assert z.match(a) == {a_: z}\n    assert x.match(b) is None\n    assert y.match(b) == {b_: y}\n    assert z.match(b) == {b_: z}\n", "def test_var():\n    from sympy import var\n    x = var('x')\n    assert x == Symbol('x')\n    x, y = var('x y')\n    assert x == Symbol('x')\n    assert y == Symbol('y')\n\n    x, y, z = var('x,y,z')\n    assert x == Symbol('x')\n    assert y == Symbol('y')\n    assert z == Symbol('z')\n\n    x, y, z = var(('x', 'y', 'z'))\n    assert x == Symbol('x')\n    assert y == Symbol('y')\n    assert z == Symbol('z')\n\n    x, y = var('x:2')\n    assert x == Symbol('x0')\n    assert y == Symbol('x1')\n\n    x, y, z = var('x:3')\n    assert x == Symbol('x0')\n    assert y == Symbol('x1')\n    assert z == Symbol('x2')\n\n    x, y = var(('x', 'y'))\n    assert x == Symbol('x')\n    assert y == Symbol('y')\n\n    x, y = var(('x:2', 'y:2'))\n    assert x == Symbol('x0')\n    assert y == Symbol('y0')\n\n    x, y = var('x,y', real=True)\n    assert x.is_real\n    assert y.is_real\n\n    raises(ValueError, lambda: var(''))\n\n    raises(ValueError, lambda: var(','))\n", "def test_Wild():\n    a = Wild('a')\n    b = Wild('b')\n    c = Wild('c', exclude=[Symbol('x'), Symbol('y')])\n    d = Wild('d', properties=[lambda x: x.is_real])\n    e = Wild('e', exclude=[Symbol('z')], properties=[lambda x: x.is_real])\n\n    x, y, z = Symbol('x'), Symbol('y'), Symbol('z')\n    p = Symbol('p', positive=True)\n    n = Symbol('n', negative=True)\n\n    assert x.match(a) == {a_: x}\n    assert y.match(a) == {a_: y}\n    assert z.match(a) == {a_: z}\n    assert x.match(b) == {b_: x}\n    assert y.match(b) == {b_: y}\n    assert z.match(b) == {b_: z}\n    assert x.match(c) == {c_: x}\n    assert y.match(c) == {c_: y}\n    assert z.match(c) is None\n    assert x.match(d) == {d_: x}\n    assert y.match(d) == {d_: y}\n    assert z.match(d) == {d_: z}\n    assert n.match(d) == {d_: n}\n    assert p.match(d) is None\n    assert x.match(e) == {e_: x}\n    assert y.match(e) == {e_: y}\n    assert z.match(e) is None\n    assert n.match(e) == {e_: n}\n    assert p.match(e) is None\n\n    w = Wild('w')\n    A = WildFunction('A')\n    assert A.match(w) == {w_: A_}\n", "def test_Wild_matches():\n    S = sympify\n    x = Symbol(\"x\")\n    y = Symbol(\"y\")\n    p = Symbol(\"p\", positive=True)\n    k = Symbol(\"k\", integer=True)\n    n = Symbol(\"n\", integer=True, positive=True)\n    a = Symbol(\"a\")\n    b = Symbol(\"b\")\n\n    # Test matches with Symbol instances\n    X = Wild(\"X\")\n    assert x.match(X) == {X: x}\n    assert y.match(X) == {X: y}\n    assert p.match(X) == {X: p}\n    assert k.match(X) == {X: k}\n    assert n.match(X) == {X: n}\n    assert a.match(X) == {X: a}\n    assert b.match(X) == {X: b}\n\n    # Test matches with other instances\n    assert S(1).match(X) is None\n    assert S(pi).match(X) is None\n    assert S(True).match(X) is None\n\n    # Test matches with Wild instances\n    Y = Wild(\"Y\")\n    assert X.match(Y) == {Y: X}\n\n    # Test matches with exclude\n    XExcludeY = Wild(\"X\", exclude=[y])\n    assert x.match(XExcludeY) == {XExcludeY: x}\n    assert y.match(XExcludeY) is None\n    assert p.match(XExcludeY) == {XExcludeY: p}\n    assert k.match(XExcludeY) == {XExcludeY: k}\n    assert n.match(XExcludeY) == {XExcludeY: n}\n    assert a.match(XExcludeY) == {XExcludeY: a}\n    assert b.match(XExcludeY) == {XExcludeY: b}\n\n    # Test matches with properties\n    XPositive = Wild(\"X\", properties=[lambda k: k.is_positive])\n    assert x.match(XPositive) is None\n    assert y.match(XPositive) is None\n    assert p.match(XPositive) == {XPositive: p}\n    assert k.match(XPositive) is None\n    assert n.match(XPositive) == {XPositive: n}\n    assert a.match(XPositive) is None\n    assert b.match(XPositive) is None\n\n    XInteger = Wild(\"X\", properties=[lambda k: k.is_integer])\n    assert x.match(XInteger) is None\n    assert y.match(XInteger) is None\n    assert", "def test_var():\n    # Test that var puts the symbols in the global namespace\n    assert 'x' not in globals()\n    assert 'y' not in globals()\n    var('x, y')\n    assert 'x' in globals()\n    assert 'y' in globals()\n    assert isinstance(x, Symbol)\n    assert isinstance(y, Symbol)\n\n    # Test that var works with multiple symbols\n    var('a, b, c', real=True)\n    assert 'a' in globals()\n    assert 'b' in globals()\n    assert 'c' in globals()\n    assert isinstance(a, Symbol)\n    assert isinstance(b, Symbol)\n    assert isinstance(c, Symbol)\n    assert a.is_real\n    assert b.is_real\n    assert c.is_real\n\n    # Test that var raises an error when given no symbols\n    raises(ValueError, lambda: var(''))\n\n    # Test that var raises an error when given a single empty symbol\n    raises(ValueError, lambda: var(','))\n\n    # Test that var raises an error when given multiple empty symbols\n    raises(ValueError, lambda: var('x,,y,,z'))\n\n    # Test that var raises an error when given a symbol with a space\n    raises(ValueError, lambda: var(' x '))\n\n    # Test that var works with a sequence of strings\n    var(('x', 'y', 'z'))\n    assert 'x' in globals()\n    assert 'y' in globals()\n    assert 'z' in globals()\n    assert isinstance(x, Symbol)\n    assert isinstance(y, Symbol)\n    assert isinstance(z, Symbol)\n\n    # Test that var works with a list of strings\n    var(['a', 'b', 'c'])\n    assert 'a' in globals()\n    assert 'b' in globals()\n    assert 'c' in globals()\n    assert isinstance(a, Symbol)\n    assert isinstance(b, Symbol)\n    assert isinstance(c, Symbol)\n\n    # Test that var works with a set of strings\n    var({'x', 'y', 'z'})\n    assert 'x' in globals()\n    assert 'y' in globals()\n    assert 'z' in globals()\n    assert isinstance(x, Symbol)\n    assert isinstance(y, Symbol)\n    assert isinstance(z, Symbol)\n\n    # Test that var works with a tuple of strings\n    var(('x', 'y', 'z'))\n    assert 'x' in globals()\n    assert 'y' in globals()\n    assert 'z' in globals()\n"], "sample_670": ["def test_evaluate(input: str, matcher: Callable[[str], bool], expected: bool) -> None:\n    scanner = Scanner(input)\n    assert expression(scanner, matcher) is expected\n", "def test_matcher_logic(matcher: Callable[[str], bool], expected: bool) -> None:\n    assert evaluate(\"true\", matcher) is expected\n", "def test_nested_negation(expr: str, ident: str, expected: bool) -> None:\n    matcher = {ident: expected}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_matcher_return_values(matcher: Callable[[str], bool], expr: str, expected: bool) -> None:\n    assert evaluate(expr, matcher) is expected\n", "def test_matcher_function_called(expr: str, ident: str, expected: bool) -> None:\n    called = []\n    matcher = lambda x: (called.append(x) or True)\n    evaluate(expr, matcher)\n    assert ident in called is expected\n"], "sample_82": ["def test_value_from_datadict_with_empty_strings(self):\n    data = {'field_year': '', 'field_month': '', 'field_day': ''}\n    self.assertIsNone(self.widget.value_from_datadict(data, {}, 'field'))\n", "def test_id_for_label(self):\n    widget = SelectDateWidget()\n    self.assertEqual(widget.id_for_label('my_date'), 'my_date_month')\n    self.assertEqual(widget.id_for_label(None), None)\n    self.assertEqual(widget.id_for_label(''), '')\n", "def test_render_with_empty_label_and_placeholder(self):\n    widget = SelectDateWidget(\n        years=('2018', '2019'),\n        empty_label=('', 'Month', 'Day'),\n    )\n    self.check_html(widget, 'my_date', '', html=(\n        \"\"\"\n        <select name=\"my_date_month\" id=\"id_my_date_month\" placeholder=\"Month\">\n            <option selected value=\"\">Month</option>\n            <option value=\"1\">January</option>\n            <option value=\"2\">February</option>\n            <option value=\"3\">March</option>\n            <option value=\"4\">April</option>\n            <option value=\"5\">May</option>\n            <option value=\"6\">June</option>\n            <option value=\"7\">July</option>\n            <option value=\"8\">August</option>\n            <option value=\"9\">September</option>\n            <option value=\"10\">October</option>\n            <option value=\"11\">November</option>\n            <option value=\"12\">December</option>\n        </select>\n        <select name=\"my_date_day\" id=\"id_my_date_day\" placeholder=\"Day\">\n            <option selected value=\"\">Day</option>\n            <option value=\"1\">1</option>\n            <option value=\"2\">2</option>\n            <option value=\"3\">3</option>\n            <option value=\"4\">4</option>\n            <option value=\"5\">5</option>\n            <option value=\"6\">6</option>\n            <option value=\"7\">7</option>\n            <option value=\"8\">8</option>\n            <option value=\"9\">9</option>\n            <option value=\"10\">10</option>\n            <option value=\"11\">11</option>\n            <option value=\"12\">12</option>\n            <option value=\"13\">13</option>\n            <option value=\"14\">14</option>\n            <option value=\"15\">15</option>\n            <option value=\"16\">16</option>\n            <option value=\"17\">17</option>\n            <option value=\"18\">18</option>\n            <option value=\"19\">19</option>\n            <option value=\"20\">20</option>\n            <option value=\"21\">21</option>\n            <option value=\"22\">22</option>\n            <option value=\"23\">23</option>\n            <option value=\"", "def test_value_from_datadict_with_zero(self):\n    data = {'field_year': '0', 'field_month': '12', 'field_day': '1'}\n    self.assertEqual(self.widget.value_from_datadict(data, {}, 'field'), '0-12-1')\n", "def test_format_value_from_date_instance(self):\n    d = date(2010, 4, 15)\n    self.assertEqual(self.widget.format_value(d), {'day': 15, 'month': 4, 'year': 2010})\n"], "sample_627": ["def test_concat_fill_value_with_dict() -> None:\n    datasets = [\n        Dataset({\"a\": (\"x\", [2, 3]), \"b\": (\"x\", [-2, 1]), \"x\": [1, 2]}),\n        Dataset({\"a\": (\"x\", [1, 2]), \"b\": (\"x\", [3, -1]), \"x\": [0, 1]}),\n    ]\n    fill_value = {\"a\": 2, \"b\": 1}\n    expected = Dataset(\n        {\n            \"a\": ((\"t\", \"x\"), [[fill_value[\"a\"], 2, 3], [1, 2, fill_value[\"a\"]]]),\n            \"b\": ((\"t\", \"x\"), [[fill_value[\"b\"], -2, 1], [3, -1, fill_value[\"b\"]]]),\n        },\n        {\"x\": [0, 1, 2]},\n    )\n    actual = concat(datasets, dim=\"t\", fill_value=fill_value)\n    assert_identical(actual, expected)\n", "def test_concat_dataarray_with_coords_overrides() -> None:\n    # test that the coords argument behaves as expected when concatenating dataarrays\n    # with coords that have the same name but different values\n    da1 = DataArray(\n        np.array([1, 2, 3]), dims=\"x\", coords={\"x\": [1, 2, 3], \"y\": 1}\n    )\n    da2 = DataArray(\n        np.array([4, 5, 6]), dims=\"x\", coords={\"x\": [4, 5, 6], \"y\": 2}\n    )\n\n    actual = concat([da1, da2], \"x\", coords=\"minimal\")\n    expected = DataArray(\n        np.array([1, 2, 3, 4, 5, 6]), dims=\"x\", coords={\"x\": [1, 2, 3, 4, 5, 6]}\n    )\n    assert_identical(actual, expected)\n\n    actual = concat([da1, da2], \"x\", coords=\"different\")\n    expected = DataArray(\n        np.array([1, 2, 3, 4, 5, 6]), dims=\"x\", coords={\"x\": [1, 2, 3, 4, 5, 6], \"y\": 1}\n    )\n    assert_identical(actual, expected)\n\n    with pytest.raises(ValueError, match=r\"'y' not present in all datasets\"):\n        concat([da1, da2], dim=\"x\", coords=\"all\")\n\n    actual = concat([da1, da2], \"x\", coords=\"all\", join=\"outer\")\n    expected = DataArray(\n        np.array([1, 2, 3, 4, 5, 6]),\n        dims=\"x\",\n        coords={\"x\": [1, 2, 3, 4, 5, 6], \"y\": [1, 1, 1, 2, 2, 2]},\n    )\n    assert_identical(actual, expected)\n", "def test_concat_with_datetime64_and_timedelta64_coordinates() -> None:\n    # GH 7843\n    # test concat with datetime64[ns] and timedelta64[ns] coordinates\n    time_data1 = np.array([\"2022-01-01\", \"2022-02-01\"], dtype=\"datetime64[ns]\")\n    time_data2 = np.array(\"2022-03-01\", dtype=\"datetime64[ns]\")\n    time_expected = np.array(\n        [\"2022-01-01\", \"2022-02-01\", \"2022-03-01\"], dtype=\"datetime64[ns]\"\n    )\n    time_data3 = np.array(\n        [\"2022-01-01\", \"2022-02-01\"], dtype=\"datetime64[ns]\"\n    )\n    time_data4 = np.array(\"2022-03-01\", dtype=\"datetime64[ns]\")\n    time_data4 += np.timedelta64(1, \"D\")\n\n    timedelta_data1 = np.array(\n        [pd.Timedelta(days=1), pd.Timedelta(days=2)], dtype=\"timedelta64[ns]\"\n    )\n    timedelta_data2 = np.array(pd.Timedelta(days=3), dtype=\"timedelta64[ns]\")\n    timedelta_expected = np.array(\n        [\n            pd.Timedelta(days=1),\n            pd.Timedelta(days=2),\n            pd.Timedelta(days=3),\n        ],\n        dtype=\"timedelta64[ns]\",\n    )\n    timedelta_data3 = np.array(\n        [pd.Timedelta(days=1), pd.Timedelta(days=2)], dtype=\"timedelta64[ns]\"\n    )\n    timedelta_data4 = np.array(pd.Timedelta(days=3), dtype=\"timedelta64[ns]\")\n    timedelta_data4 += pd.Timedelta(days=1)\n\n    time_coord = (\"time\", time_data1)\n    time_coord2 = (\"time\", time_data2)\n    timedelta_coord = (\"time\", timedelta_data1)\n    timedelta_coord2 = (\"time\", timedelta_data2)\n\n    time_coord3 = (\"time\", time_data3)\n    time_coord4 = (\"time\", time_data4)\n    timedelta_coord3 = (\"time\", timedelta_data3)\n    timedelta_coord4 = (\"time\", timedelta_data4)\n\n    ds1 = Dataset(coords={\"time\": time_coord})\n    ds2 = Dataset(coords={\"time\": time_coord2})\n    ds3 = Dataset(coords={\"", "def test_concat_reindex_coordinate_variables() -> None:\n    ds1 = Dataset(coords={\"x\": [0, 1]})\n    ds2 = Dataset(coords={\"x\": [2, 3]})\n\n    # Create a Dataset with coordinate variable x that has an Index\n    ds1._indexes[\"x\"] = PandasIndex([0, 1], \"x\")\n\n    with pytest.raises(ValueError, match=r\"Cannot concatenate along dimension 'x' indexes\"):\n        concat([ds1, ds2], dim=\"x\")\n\n    # Test that a coordinate variable with an Index can be concatenated with one without\n    # by passing a DataArray as the dim argument\n    dim_array = DataArray([0, 1], dims=[\"x\"])\n    expected = Dataset(coords={\"x\": [0, 1, 2, 3]})\n    actual = concat([ds1, ds2], dim=dim_array)\n    assert_identical(expected, actual)\n\n    # Test that a coordinate variable with an Index can be concatenated with another\n    # with an Index by passing a DataArray as the dim argument\n    ds2._indexes[\"x\"] = PandasIndex([2, 3], \"x\")\n    dim_array = DataArray([0, 1], dims=[\"x\"])\n    expected = Dataset(coords={\"x\": [0, 1, 2, 3]})\n    actual = concat([ds1, ds2], dim=dim_array)\n    assert_identical(expected, actual)\n", "def test_concat_data_vars_and_coords_with_overlapping_names() -> None:\n    \"\"\"Test that when data_vars and coords have overlapping names, the correct\n    variables are concatenated and merged correctly.\n\n    Regression test for https://github.com/pydata/xarray/issues/6320.\n    \"\"\"\n\n    # create a dataset with overlapping data_var and coord names\n    ds1 = Dataset(\n        {\n            \"x\": ((\"y\",), [1]),\n            \"y\": ((\"y\",), [2]),\n            \"z\": ((\"y\",), [3]),\n        },\n        coords={\"x\": [0], \"y\": [10]},\n    )\n\n    ds2 = Dataset(\n        {\n            \"x\": ((\"y\",), [4]),\n            \"y\": ((\"y\",), [5]),\n            \"z\": ((\"y\",), [6]),\n        },\n        coords={\"x\": [1], \"y\": [20]},\n    )\n\n    # concat with 'all' data_vars and coords\n    actual = concat([ds1, ds2], \"dim\")\n    expected = Dataset(\n        {\n            \"x\": ((\"dim\", \"y\"), [[1, 4], [2, 5]]),\n            \"y\": ((\"dim\", \"y\"), [[3, 6], np.nan]),\n            \"z\": ((\"dim\",), [0, 1]),\n        },\n        coords={\"y\": [10, 20]},\n    )\n    assert_identical(actual, expected)\n\n    # concat with 'minimal' data_vars and 'all' coords\n    actual = concat([ds1, ds2], \"dim\", data_vars=\"minimal\", coords=\"all\")\n    expected = Dataset(\n        {\n            \"x\": ((\"dim\", \"y\"), [[1, 4], [2, 5]]),\n            \"z\": ((\"dim\", \"y\"), [[3, 6], np.nan]),\n        },\n        coords={\"x\": [0, 1], \"y\": [10, 20]},\n    )\n    assert_identical(actual, expected)\n\n    # concat with 'all' data_vars and 'minimal' coords\n    actual = concat([ds1, ds2], \"dim\", data_vars=\"all\", coords=\"minimal\")\n    expected = Dataset(\n        {\n            \"x\": ((\"dim\", \"y\"), [[1, 4], [2, 5]]),\n            \"y\": ((\"dim\", \"y\"), [[3, 6], np.nan]),\n            \""], "sample_340": ["def test_detect_conflicts(self):\n    \"\"\"\n    Tests detecting conflicts in the migration graph.\n    \"\"\"\n    migration_loader = MigrationLoader(connection)\n    conflicts = migration_loader.detect_conflicts()\n    self.assertEqual(len(conflicts), 1)\n    self.assertIn(\"migrations\", conflicts)\n    self.assertEqual(len(conflicts[\"migrations\"]), 2)\n", "def test_detect_conflicts(self):\n    \"\"\"\n    MigrationLoader should detect conflicts between multiple leaf migrations.\n    \"\"\"\n    loader = MigrationLoader(connection)\n    conflicts = loader.detect_conflicts()\n    self.assertEqual(conflicts, {\"migrations\": [\"0001_initial\", \"0004_extra\"]})\n", "def test_loading_squashed_complex_replacement(self):\n    \"\"\"\n    Tests loading a complex set of squashed migrations with a replacement\n    migration that has itself a replacement migration.\n    \"\"\"\n    loader = MigrationLoader(connection)\n    recorder = MigrationRecorder(connection)\n    self.addCleanup(recorder.flush)\n\n        plan = set(loader.graph.forwards_plan(('app1', '7_auto')))\n        return len(plan - loader.applied_migrations.keys())\n\n    # Empty database: use squashed migration\n    loader.build_graph()\n    self.assertEqual(num_nodes(), 5)\n\n    # Starting at 1 or 2 should use the squashed migration too\n    self.record_applied(recorder, 'app1', '1_auto')\n    loader.build_graph()\n    self.assertEqual(num_nodes(), 4)\n\n    self.record_applied(recorder, 'app1', '2_auto')\n    loader.build_graph()\n    self.assertEqual(num_nodes(), 3)\n\n    # However, starting at 3 to 5 cannot use the squashed migration\n    self.record_applied(recorder, 'app1', '3_auto')\n    loader.build_graph()\n    self.assertEqual(num_nodes(), 4)\n\n    self.record_applied(recorder, 'app1', '4_auto')\n    loader.build_graph()\n    self.assertEqual(num_nodes(), 3)\n\n    # Starting at 5 to 7 we are past the squashed migrations.\n    self.record_applied(recorder, 'app1', '5_auto')\n    loader.build_graph()\n    self.assertEqual(num_nodes(), 2)\n\n    self.record_applied(recorder, 'app1', '6_auto')\n    loader.build_graph()\n    self.assertEqual(num_nodes(), 1)\n\n    self.record_applied(recorder, 'app1', '7_auto')\n    loader.build_graph()\n    self.assertEqual(num_nodes(), 0)\n", "def test_loading_squashed_complex_with_circular_dependencies(self):\n    \"\"\"\n    Tests loading a complex set of squashed migrations with circular dependencies.\n\n    This test case covers a scenario where a squashed migration in one app depends\n    on a squashed migration in another app, which in turn depends on the first app.\n    \"\"\"\n    loader = MigrationLoader(connection)\n    recorder = MigrationRecorder(connection)\n    self.addCleanup(recorder.flush)\n\n    # Load with nothing applied: both migrations squashed.\n    loader.build_graph()\n    plan = set(loader.graph.forwards_plan(('app1', '4_auto')))\n    plan = plan - loader.applied_migrations.keys()\n    expected_plan = {\n        ('app1', '1_auto'),\n        ('app2', '1_squashed_2'),\n        ('app1', '2_squashed_3'),\n        ('app1', '4_auto'),\n    }\n    self.assertEqual(plan, expected_plan)\n\n    # Fake-apply a few from app1: unsquashes migration in app1.\n    self.record_applied(recorder, 'app1', '1_auto')\n    self.record_applied(recorder, 'app1', '2_auto')\n    loader.build_graph()\n    plan = set(loader.graph.forwards_plan(('app1', '4_auto')))\n    plan = plan - loader.applied_migrations.keys()\n    expected_plan = {\n        ('app2', '1_squashed_2'),\n        ('app1', '3_auto'),\n        ('app1', '4_auto'),\n    }\n    self.assertEqual(plan, expected_plan)\n\n    # Fake-apply one from app2: unsquashes migration in app2 too.\n    self.record_applied(recorder, 'app2', '1_auto')\n    loader.build_graph()\n    plan = set(loader.graph.forwards_plan(('app1', '4_auto')))\n    plan = plan - loader.applied_migrations.keys()\n    expected_plan = {\n        ('app2', '2_auto'),\n        ('app1', '3_auto'),\n        ('app", "    def test_loading_replacing_migration(self):\n        \"\"\"\n        Test loading replacing migrations.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        recorder = MigrationRecorder(connection)\n        self.addCleanup(recorder.flush)\n\n        # Empty database: use replacing migration\n        loader.build_graph()\n        plan = set(loader.graph.forwards_plan(('migrations', '0003_second')))\n        plan = plan - loader.applied_migrations.keys()\n        expected_plan = {\n            ('migrations', '0001_initial_replaced'),\n            ('migrations', '0002_second_replaced'),\n            ('migrations', '0003_second'),\n        }\n        self.assertEqual(plan, expected_plan)\n\n        # Fake-apply one migration: use replaced migration\n        self.record_applied(recorder, 'migrations', '0002_first')\n        loader.build_graph()\n        plan = set(loader.graph.forwards_plan(('migrations', '0003_second')))\n        plan = plan - loader.applied_migrations.keys()\n        expected_plan = {\n            ('migrations', '0002_second'),\n            ('migrations', '0003_second'),\n        }\n        self.assertEqual(plan, expected_plan)\n\n        # Fake-apply replaced migration: use replacing migration\n        self.record_applied(recorder, 'migrations', '0002_second_replaced')\n        loader.build_graph()\n        plan = set(loader.graph.forwards_plan(('migrations', '0003_second')))\n        plan = plan - loader.applied_migrations.keys()\n        expected_plan = {\n            ('migrations', '0003_second'),\n        }\n        self.assertEqual(plan, expected_plan)\n\n        # Fake-apply all migrations: use replaced migration\n        self.record_applied(recorder, 'migrations', '0002_second')\n        loader.build_graph()\n        plan = set(loader.graph.forwards_plan(('migrations', '0003_second')))\n        plan = plan - loader.applied_migrations.keys()\n        expected_plan = set()\n        self.assertEqual(plan, expected_plan)\n"], "sample_77": ["def test_avoid_wrapping(self):\n    tests = (\n        (\"Hello world\", \"Hello\\u00a0world\"),\n        (\"Hello  world\", \"Hello\\u00a0\\u00a0world\"),\n        (\"Hello   world\", \"Hello\\u00a0\\u00a0\\u00a0world\"),\n        (\"Hello\\nworld\", \"Hello\\nworld\"),\n        (\"Hello\\tworld\", \"Hello\\tworld\"),\n    )\n    for value, output in tests:\n        with self.subTest(value=value, output=output):\n            self.assertEqual(avoid_wrapping(value), output)\n            self.assertEqual(avoid_wrapping(lazystr(value)), output)\n", "def test_avoid_wrapping(self):\n    items = (\n        (\"Avoid wrapping in the middle of a phrase\", \"Avoid\\xa0wrapping\\xa0in\\xa0the\\xa0middle\\xa0of\\xa0a\\xa0phrase\"),\n        (\"Hello world!\", \"Hello\\xa0world!\"),\n        (\"Multiple  spaces  between words\", \"Multiple\\xa0\\xa0spaces\\xa0\\xa0between\\xa0words\"),\n    )\n    for value, output in items:\n        with self.subTest(value=value, output=output):\n            self.check_output(avoid_wrapping, value, output)\n            self.check_output(avoid_wrapping, lazystr(value), output)\n", "    def test_avoid_wrapping(self):\n        tests = (\n            (\"Hello World\", \"Hello\u00a0World\"),\n            (\"  Hello World  \", \" \u00a0Hello\u00a0World\u00a0\"),\n            (\"Hello\\nWorld\", \"Hello\\nWorld\"),  # Avoid wrapping does not affect newlines\n            (\"Hello\\tWorld\", \"Hello\\tWorld\"),  # Avoid wrapping does not affect tabs\n        )\n        for value, output in tests:\n            with self.subTest(value=value):\n                self.assertEqual(avoid_wrapping(value), output)\n", "    def test_urlize_wrap_and_trailing(self):\n        tests = (\n            (\n                'Hello (http://example.com).',\n                'Hello (<a href=\"http://example.com\">http://example.com</a>).'\n            ),\n            (\n                '(http://example.com)',\n                '(<a href=\"http://example.com\">http://example.com</a>)'\n            ),\n            (\n                '[http://example.com]',\n                '[<a href=\"http://example.com\">http://example.com</a>]'\n            ),\n            (\n                'Hello, http://example.com.',\n                'Hello, <a href=\"http://example.com\">http://example.com</a>.'\n            ),\n            (\n                'Hello http://example.com!',\n                'Hello <a href=\"http://example.com\">http://example.com</a>!'\n            ),\n            (\n                'Hello http://example.com?',\n                'Hello <a href=\"http://example.com\">http://example.com</a>?'\n            ),\n        )\n        for value, output in tests:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value), output)\n", "    def test_urlize_options(self):\n        tests = (\n            # Test nofollow.\n            (\n                'Search for google.com/?q=! and see.',\n                'Search for <a href=\"http://google.com/?q=\" rel=\"nofollow\">google.com/?q=</a>! and see.',\n                True,\n            ),\n            # Test autoescape.\n            (\n                'Search for google.com/?q=! and see <script>alert(\"xss\")</script>',\n                'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>! and see &lt;script&gt;alert(&quot;xss&quot;)&lt;/script&gt;',\n                False,\n                True,\n            ),\n            # Test trim_url_limit.\n            (\n                'Search for verylongdomainnameverylongdomainnameverylongdomainname.com and see.',\n                'Search for <a href=\"http://verylongdomainnameverylongdomainnameverylongdomainname.com\">verylongdomainnameverylongdomainname\u2026</a> and see.',\n                False,\n                False,\n                30,\n            ),\n        )\n        for value, output, nofollow, autoescape, trim_url_limit in tests:\n            with self.subTest(value=value, output=output):\n                self.assertEqual(urlize(value, nofollow=nofollow, autoescape=autoescape, trim_url_limit=trim_url_limit), output)\n"], "sample_1146": ["def test_latex_inequality():\n    assert latex(x > y) == r\"x > y\"\n    assert latex(x < y) == r\"x < y\"\n    assert latex(x >= y) == r\"x \\geq y\"\n    assert latex(x <= y) == r\"x \\leq y\"\n    assert latex(x != y) == r\"x \\neq y\"\n    assert latex(x == y) == r\"x = y\"\n", "def test_latex_mathematical_constants():\n    assert latex(S.E) == r\"E\"\n    assert latex(S.EulerGamma) == r\"E\"\n    assert latex(S.Catalan) == r\"Catalan\"\n    assert latex(S.GoldenRatio) == r\"\\phi\"\n    assert latex(S.TribonacciConstant) == r\"TribonacciConstant\"\n    assert latex(S.Pi) == r\"\\pi\"\n", "def test_latex_manifold():\n    from sympy.diffgeom import Manifold, Patch, CoordSystem\n    from sympy import symbols\n\n    coords = symbols('x:z')\n    M = Manifold('M', 3, coords)\n    P = Patch('P', M)\n    cs = CoordSystem('cs_P', P, coords)\n\n    assert latex(M) == r\"\\text{M}\"\n    assert latex(P) == r\"\\text{P}_{\\text{M}}\"\n    assert latex(cs) == r\"\\text{cs}_{\\text{P}}^{\\text{M}}\"\n", "def test_latex_integrals_indefinite():\n    assert latex(Integral(x**2, x)) == r\"\\int x^{2}\\, dx\"\n    assert latex(Integral(2*x, x)) == r\"2 \\int x\\, dx\"\n    assert latex(Integral(x**2 + 1, x)) == r\"\\int \\left(x^{2} + 1\\right)\\, dx\"\n", "def test_latex_tensor_index():\n    from sympy.tensor.tensor import TensorIndexType, tensor_indices\n\n    nu = TensorIndexType('nu')\n    mu = TensorIndexType('mu')\n    rho = TensorIndexType('rho')\n    sigma = TensorIndexType('sigma')\n    alpha = tensor_indices('alpha', nu)\n    beta = tensor_indices('beta', mu)\n    gamma = tensor_indices('gamma', rho)\n    delta = tensor_indices('delta', sigma)\n\n    # Test for the latex name of the index\n    assert latex(alpha) == r\"^{\\alpha}\"\n\n    # Test for the latex name of the negative index\n    assert latex(-alpha) == r\"_{\\alpha}\"\n\n    # Test for latex of multiple indices\n    assert latex(alpha*beta) == r\"^{\\alpha \\beta}\"\n    assert latex(alpha*beta*gamma) == r\"^{\\alpha \\beta \\gamma}\"\n    assert latex(alpha*beta*gamma*delta) == r\"^{\\alpha \\beta \\gamma \\delta}\"\n    assert latex(alpha*beta*gamma*delta*-alpha) == r\"^{\\alpha \\beta \\gamma \\delta}_{\\alpha}\"\n"], "sample_125": ["    def test_status_code(self):\n        response = HttpResponse()\n        self.assertEqual(response.status_code, 200)\n        response.status_code = 404\n        self.assertEqual(response.status_code, 404)\n", "    def test_init_content(self):\n        response = HttpResponse(b'content')\n        self.assertEqual(response.content, b'content')\n", "    def test_content_can_be_set_to_a_string(self):\n        response = HttpResponse('Hello, World!')\n        self.assertEqual(response.content, b'Hello, World!')\n", "    def test_custom_status_code(self):\n        response = HttpResponse(content='Hello', status=201)\n        self.assertEqual(response.status_code, 201)\n        self.assertEqual(response['Content-Type'], 'text/html; charset=utf-8')\n", "    def test_setdefault(self):\n        response = HttpResponse()\n        response.setdefault('Content-Type', 'text/plain')\n        self.assertEqual(response['Content-Type'], 'text/plain')\n\n        response = HttpResponse()\n        response['Content-Type'] = 'text/html'\n        response.setdefault('Content-Type', 'text/plain')\n        self.assertEqual(response['Content-Type'], 'text/html')\n"], "sample_660": ["def test_record_testsuite_property_multiple_values(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            record_testsuite_property(\"stats\", \"all good\")\n            record_testsuite_property(\"stats\", 10)\n            record_testsuite_property(\"stats\", 20)\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testsuite\")\n    properties_node = node.find_first_by_tag(\"properties\")\n    p1_node = properties_node.find_nth_by_tag(\"property\", 0)\n    p2_node = properties_node.find_nth_by_tag(\"property\", 1)\n    p3_node = properties_node.find_nth_by_tag(\"property\", 2)\n    p1_node.assert_attr(name=\"stats\", value=\"all good\")\n    p2_node.assert_attr(name=\"stats\", value=\"10\")\n    p3_node.assert_attr(name=\"stats\", value=\"20\")\n", "def test_property_with_xml_invalid_chars(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            record_property(\"prop\", \"abc<>&'\")\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testsuite\")\n    tnode = node.find_first_by_tag(\"testcase\")\n    psnode = tnode.find_first_by_tag(\"properties\")\n    pnode = psnode.find_first_by_tag(\"property\")\n    pnode.assert_attr(name=\"prop\", value=\"abc&lt;&amp;&#39;\")\n", "def test_junit_logging_system_out_and_err(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        junit_logging=system-out\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n            import sys\n            sys.stdout.write('hello-stdout')\n            sys.stderr.write('hello-stderr')\n    \"\"\"\n    )\n    _, dom = runandparse(testdir)\n    node = dom.find_first_by_tag(\"testcase\")\n    systemout = node.find_first_by_tag(\"system-out\")\n    assert \"hello-stdout\" in systemout.toxml()\n    assert \"hello-stderr\" in systemout.toxml()\n    assert len(node.find_by_tag(\"system-err\")) == 0\n", "def test_record_testsuite_property_multiline_string(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            record_testsuite_property(\"multiline\", \"\"\" \n                This is a \n                multi line \n                string\n            \"\"\")\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testsuite\")\n    properties_node = node.find_first_by_tag(\"properties\")\n    p1_node = properties_node.find_nth_by_tag(\"property\", 0)\n    p1_node.assert_attr(name=\"multiline\", value='''This is a \n                multi line \n                string''')\n", "def test_duration_reporting_issue1801(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import time\n\n            time.sleep(1)\n            time.sleep(2)\n            assert False\n    \"\"\"\n    )\n    result, dom = runandparse(testdir, \"-o\", \"junit_duration_report=call\")\n    node = dom.find_first_by_tag(\"testsuite\")\n    tnode1 = node.find_nth_by_tag(\"testcase\", 0)\n    tnode2 = node.find_nth_by_tag(\"testcase\", 1)\n    assert float(tnode1[\"time\"]) == 1.0\n    assert float(tnode2[\"time\"]) == 2.0\n"], "sample_799": ["def test_cross_val_score_parallel():\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    clf = MockClassifier()\n    # Smoke test\n    scores = cross_val_score(clf, X, y, cv=5, n_jobs=2)\n    assert_array_almost_equal(scores, clf.score(X, y), 2)\n    scores = cross_val_score(clf, X, y, cv=5, n_jobs=-1)\n    assert_array_almost_equal(scores, clf.score(X, y), 2)\n", "def test_cross_val_score_multimetric():\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    clf = LogisticRegression()\n\n    scoring = ['accuracy', 'f1_macro', 'roc_auc_ovr']\n    scores = cross_val_score(clf, X, y, scoring=scoring, cv=3)\n    assert_array_equal(scores, [1.0, 1.0, 1.0])\n\n    scoring = {'accuracy': 'accuracy', 'f1': 'f1_macro', 'roc_auc': 'roc_auc_ovr'}\n    scores = cross_val_score(clf, X, y, scoring=scoring, cv=3)\n    assert_array_equal(scores, [1.0, 1.0, 1.0])\n", "def test_cross_val_score_with_sample_weights():\n    X, y = make_classification(n_samples=10, n_features=1, random_state=0)\n    clf = LogisticRegression(random_state=0)\n    sample_weights = np.ones(10)\n    sample_weights[5:] *= 2\n    cv = KFold(2)\n    score_weighted = cross_val_score(clf, X, y, cv=cv, fit_params={'sample_weight': sample_weights})\n    score_unweighted = cross_val_score(clf, X, y, cv=cv)\n    assert_array_almost_equal(score_weighted, score_unweighted * 2)\n", "def test_cross_val_score_with_sample_weight():\n    # test that cross_val_score works with sample_weight\n    X, y = make_classification(n_samples=50, n_features=1, n_informative=1,\n                               n_redundant=0, n_classes=2,\n                               n_clusters_per_class=1, random_state=0)\n    est = LogisticRegression()\n    sample_weight = np.abs(np.random.RandomState(42).randn(50))\n    sample_weight /= sample_weight.sum()\n    scores = cross_val_score(est, X, y, cv=5, fit_params={'sample_weight': sample_weight})\n    assert_array_almost_equal(scores, [0.96, 0.98, 0.98, 1., 1.])\n", "def test_cross_val_predict_predict_log_proba_on_unordered_classes():\n    # Test cross_val_predict with predict_log_proba on unordered classes\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    X, y = shuffle(X, y, random_state=0)\n    y = np.array([2, 2, 0, 1, 0, 0, 0, 2, 1, 1, 2, 0, 0, 0, 1, 1, 2, 1, 1, 2, 2, 0, 1, 2, 2, 1, 0, 2, 2, 0, 0, 1, 1, 1, 1, 0, 1, 1, 2, 1, 2, 0, 1, 2, 2, 0, 0, 0, 0, 2, 0, 0, 1, 2, 2, 1, 0, 2, 1, 0, 0, 2, 1, 2, 0, 0, 1, 2, 1, 2, 1, 1, 2, 1, 0, 2, 1, 2, 2, 1, 1, 2, 2, 0, 0, 0, 1, 2, 0, 2, 0, 2, 0, 2, 1, 0, 1, 0, 1, 0, 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 2, 0, 1, 1, 1, 0, 1, 1, 2, 0, 0, 0, 2, 1, 2, 1, 2, 2, 2, 0, 0, 1, 2, 1, 2, 1,"], "sample_765": ["def test_brier_score_loss_string_labels():\n    # Check brier_score_loss function\n    y_true = np.array([\"ham\", \"spam\", \"spam\", \"ham\"])\n    y_pred = np.array([0.1, 0.8, 0.9, 0.3])\n    pos_label = \"spam\"\n    true_score = linalg.norm(label_binarize(y_true, [\"ham\", \"spam\"])[:, 0] - y_pred) ** 2 / len(y_true)\n\n    assert_almost_equal(brier_score_loss(y_true, y_pred, pos_label=pos_label), true_score)\n", "def test_hamming_loss_with_sample_weights():\n    y1 = np.array([[0, 1, 1], [1, 0, 1]])\n    y2 = np.array([[0, 0, 1], [1, 0, 1]])\n\n    sample_weights = np.array([1, 2])\n    assert_equal(hamming_loss(y1, y2, sample_weight=sample_weights), 1 / 6)\n    assert_equal(hamming_loss(y1, y1, sample_weight=sample_weights), 0)\n    assert_equal(hamming_loss(y2, y2, sample_weight=sample_weights), 0)\n    assert_equal(hamming_loss(y2, np.logical_not(y2), sample_weight=sample_weights), 1)\n    assert_equal(hamming_loss(y1, np.logical_not(y1), sample_weight=sample_weights), 1)\n    assert_equal(hamming_loss(y1, np.zeros(y1.shape), sample_weight=sample_weights), 4 / 6)\n    assert_equal(hamming_loss(y2, np.zeros(y1.shape), sample_weight=sample_weights), 0.5)\n", "def test_confusion_matrix_multiclass_multiclass():\n    # Test confusion matrix - multi-class case with non-unique labels\n    y_true, y_pred, _ = make_prediction(binary=False)\n\n    # compute confusion matrix with only first two labels considered\n    cm = confusion_matrix(y_true, y_pred, labels=[0, 2, 0])\n    assert_array_equal(cm, [[19, 4],\n                            [4, 3],\n                            [2, 2]])\n", "def test_multilabel_confusion_matrix_errors_shape():\n    # Test multilabel confusion matrix - errors, test for correct input shape\n    y_true = np.array([[1, 0, 1], [0, 1, 0]])\n    y_pred = np.array([[0, 0, 0], [0, 1, 1]])\n\n    # bad input shape, y_true and y_pred should have the same number of rows\n    assert_raise_message(ValueError, 'y_true and y_pred should have the same '\n                         'number of samples', multilabel_confusion_matrix,\n                         y_true, np.array([0, 0, 0]))\n\n    # bad input shape, y_true and y_pred should have the same number of rows\n    assert_raise_message(ValueError, 'y_true and y_pred should have the same '\n                         'number of samples', multilabel_confusion_matrix,\n                         np.array([0, 0, 0]), y_pred)\n\n    # bad input shape, y_true and y_pred should have the same number of columns\n    assert_raise_message(ValueError, 'y_true and y_pred should have the same '\n                         'number of labels', multilabel_confusion_matrix,\n                         y_true, np.array([[1, 0, 1, 0], [0, 1, 0, 0]]))\n\n    # bad input shape, y_true and y_pred should have the same number of columns\n    assert_raise_message(ValueError, 'y_true and y_pred should have the same '\n                         'number of labels', multilabel_confusion_matrix,\n                         np.array([[1, 0, 1, 0], [0, 1, 0, 0]]), y_pred)\n", "def test_precision_recall_f_score_different_target_types():\n    # Test that precision, recall, f1 score and support work with\n    # different target types\n    y_true = np.array([1, 0, 1, 0, 1])\n    y_pred = np.array([1, 0, 0, 1, 1])\n    y_true_dict = y_true.copy().astype(object)\n    y_pred_dict = y_pred.copy().astype(object)\n\n    # test with int and int\n    p, r, f, s = precision_recall_fscore_support(y_true, y_pred, average=None)\n    assert_array_almost_equal(p, [1., 0.5], 2)\n    assert_array_almost_equal(r, [1., 0.5], 2)\n    assert_array_almost_equal(f, [1., 0.5], 2)\n    assert_array_almost_equal(s, [3, 2], 2)\n\n    # test with int and object\n    p, r, f, s = precision_recall_fscore_support(y_true, y_pred_dict, average=None)\n    assert_array_almost_equal(p, [1., 0.5], 2)\n    assert_array_almost_equal(r, [1., 0.5], 2)\n    assert_array_almost_equal(f, [1., 0.5], 2)\n    assert_array_almost_equal(s, [3, 2], 2)\n\n    # test with object and int\n    p, r, f, s = precision_recall_fscore_support(y_true_dict, y_pred, average=None)\n    assert_array_almost_equal(p, [1., 0.5], 2)\n    assert_array_almost_equal(r, [1., 0.5], 2)\n    assert_array_almost_equal(f, [1., 0.5], 2)\n    assert_array_almost_equal(s, [3, 2], 2)\n\n    # test with object and object\n    p, r, f, s = precision_recall_fscore_support(y_true_dict, y_pred_dict, average=None)\n    assert_array_almost_equal(p, [1., 0.5], 2)\n    assert_array_almost_equal(r, [1., 0.5], 2)\n    assert_array_almost_equal(f, [1., 0.5], 2)\n    assert_array_almost_equal(s, [3, 2], 2)\n"], "sample_820": ["def test_transform_shape_on_different_classifiers():\n    \"\"\"Check transform method of VotingClassifier on different classifiers.\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    clf3 = GaussianNB()\n    clf4 = KNeighborsClassifier()\n    X = np.array([[-1.1, -1.5], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\n    y = np.array([1, 1, 2, 2])\n\n    eclf1 = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n        voting='soft').fit(X, y)\n    eclf2 = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n        voting='soft',\n        flatten_transform=True).fit(X, y)\n    eclf3 = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n        voting='soft',\n        flatten_transform=False).fit(X, y)\n    eclf4 = VotingClassifier(estimators=[\n        ('lr', clf1), ('knn', clf4)],\n        voting='soft',\n        flatten_transform=False).fit(X, y)\n\n    assert_array_equal(eclf1.transform(X).shape, (4, 6))\n    assert_array_equal(eclf2.transform(X).shape, (4, 6))\n    assert_array_equal(eclf3.transform(X).shape, (3, 4, 2))\n    assert_array_almost_equal(eclf1.transform(X),\n                              eclf2.transform(X))\n    assert_array_almost_equal(\n            eclf3.transform(X).swapaxes(0, 1).reshape((4, 6)),\n            eclf2.transform(X)\n    )\n    assert_array_equal(eclf4.transform(X).shape, (2, 4, 2))\n", "def test_get_params(voter, X, y):\n    voter.fit(X, y)\n    params = voter.get_params(deep=False)\n    for name, value in params.items():\n        if hasattr(value, 'get_params'):\n            continue\n        assert isinstance(value, (str, int, float, list, tuple, dict, bool, type(None)))\n    \n    params = voter.get_params(deep=True)\n    for name, value in params.items():\n        if name.endswith('_'):\n            continue\n        assert isinstance(value, (str, int, float, list, tuple, dict, bool, type(None))) or hasattr(value, 'get_params')\n", "def test_weights_property(voter, weights, weights_expected):\n    voter.weights = weights\n    if weights is not None and 'auto' in weights:\n        weights = [1 if w == 'auto' else w for w in weights]\n    if weights is not None and 'uniform' in weights:\n        weights = [1 if w == 'uniform' else w for w in weights]\n    assert_array_equal(voter.weights, weights_expected)\n", "def test_none_estimator_in_estimator_list(drop):\n    # Test None in the middle of the estimator list\n    clf1 = LogisticRegression(random_state=123)\n    eclf = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', drop), ('gnb', GaussianNB())],\n        voting='soft', weights=[1, 1, 1]).fit(X, y)\n    assert eclf.predict(X).shape == y.shape\n\n    eclf = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', RandomForestClassifier(n_estimators=5)),\n        ('gnb', drop)],\n        voting='soft', weights=[1, 1, 1]).fit(X, y)\n    assert eclf.predict(X).shape == y.shape\n\n    eclf = VotingClassifier(estimators=[\n        ('lr', drop), ('rf', RandomForestClassifier(n_estimators=5)),\n        ('gnb', GaussianNB())],\n        voting='soft', weights=[1, 1, 1]).fit(X, y)\n    assert eclf.predict(X).shape == y.shape\n", "def test_predict_shape(estimator):\n    # check shape of predict method\n    eclf = VotingClassifier(estimators=[('est', estimator)])\n    eclf.fit(X, y)\n    assert eclf.predict(X).shape == (X.shape[0],)\n    assert eclf.predict(X[:1]).shape == (1,)\n\n    ereg = VotingRegressor([('est', estimator)])\n    ereg.fit(X_r, y_r)\n    assert ereg.predict(X_r).shape == (X_r.shape[0],)\n    assert ereg.predict(X_r[:1]).shape == (1,)\n\n"], "sample_1074": ["def test_copy():\n    G = SymmetricGroup(3)\n    G_copy = G.copy()\n    assert G == G_copy\n    assert G is not G_copy\n", "def test_polycyclic_group():\n    a = Permutation([0, 1, 2])\n    b = Permutation([2, 1, 0])\n    G = PermutationGroup([a, b])\n    pc_G = G.polycyclic_group()\n    assert pc_G.order() == G.order()\n    assert pc_G.is_polycyclic\n\n    a = Permutation([1, 2, 3, 4, 0])\n    b = Permutation([1, 0, 2, 3, 4])\n    G = PermutationGroup([a, b])\n    raises(ValueError, lambda: G.polycyclic_group())\n", "def test_abelian_invariants():\n    a = Permutation([0, 2, 1, 3])\n    b = Permutation([3, 1, 2, 0])\n    G = PermutationGroup([a, b])\n    assert G.abelian_invariants() == [2, 2]\n    a = Permutation([1, 0, 2, 3])\n    b = Permutation([3, 1, 2, 0])\n    G = PermutationGroup([a, b])\n    assert G.abelian_invariants() == [2, 2]\n", "def test_abelian_invariants_large_group():\n    a = Permutation(1, 2, 3, 4, 5, 6, 7, 8)\n    b = Permutation(0, 1, 2, 3, 4, 5, 6, 7)\n    c = Permutation(0, 4, 8, 2)(1, 3, 6)(5, 7)\n    G = PermutationGroup([a, b, c])\n    assert G.abelian_invariants() == [8, 3, 2]\n", "def test_abelian():\n    # test on SymmetricGroup\n    assert SymmetricGroup(5).is_abelian is False\n\n    # test on AlternatingGroup\n    assert AlternatingGroup(3).is_abelian is False\n    assert AlternatingGroup(4).is_abelian is False\n\n    # test on DihedralGroup\n    assert DihedralGroup(4).is_abelian is True\n    assert DihedralGroup(5).is_abelian is False\n\n    # test on CyclicGroup\n    assert CyclicGroup(5).is_abelian is True\n\n    # test on AbelianGroup\n    assert AbelianGroup(1, 2, 3, 4).is_abelian is True\n\n    # test on trivial group\n    assert PermutationGroup(Permutation(0)).is_abelian is True\n\n    # test on identity group\n    assert PermutationGroup(Permutation(0, 1, 2)).is_abelian is False\n"], "sample_850": ["def test_nystroem_components():\n    # test that the components are correctly sampled\n    rnd = np.random.RandomState(42)\n    X = rnd.uniform(size=(10, 4))\n\n    nystroem = Nystroem(n_components=5, random_state=rnd)\n    nystroem.fit(X)\n\n    assert nystroem.components_.shape == (5, 4)\n    assert np.all(np.isin(nystroem.component_indices_, np.arange(10)))\n\n    # test that the same components are sampled with the same random state\n    nystroem2 = Nystroem(n_components=5, random_state=rnd)\n    nystroem2.fit(X)\n\n    assert_array_equal(nystroem.components_, nystroem2.components_)\n    assert_array_equal(nystroem.component_indices_, nystroem2.component_indices_)\n\n    # test that different components are sampled with a different random state\n    nystroem3 = Nystroem(n_components=5, random_state=np.random.RandomState(43))\n    nystroem3.fit(X)\n\n    assert not np.array_equal(nystroem.components_, nystroem3.components_)\n    assert not np.array_equal(nystroem.component_indices_, nystroem3.component_indices_)\n", "def test_nystroem_stateless():\n    rnd = np.random.RandomState(42)\n    X = rnd.uniform(size=(10, 4))\n    X_trans = Nystroem(n_components=5, random_state=rnd).fit_transform(X)\n    assert not hasattr(Nystroem, 'fit') or not callable(Nystroem.fit)\n", "def test_nystroem_random_state():\n    # Test that Nystroem is reproducible with different random state\n    rnd = np.random.RandomState(42)\n    X = rnd.uniform(size=(10, 4))\n\n    # test with different random states\n    nystroem = Nystroem(n_components=5, random_state=0)\n    X_transformed_1 = nystroem.fit_transform(X)\n\n    nystroem = Nystroem(n_components=5, random_state=0)\n    X_transformed_2 = nystroem.fit_transform(X)\n    assert_array_almost_equal(X_transformed_1, X_transformed_2)\n\n    nystroem = Nystroem(n_components=5, random_state=rnd)\n    X_transformed_3 = nystroem.fit_transform(X)\n    assert_array_almost_equal(X_transformed_1, X_transformed_3)\n\n    # test with different random states (using RandomState object)\n    nystroem = Nystroem(n_components=5, random_state=np.random.RandomState(0))\n    X_transformed_4 = nystroem.fit_transform(X)\n    assert_array_almost_equal(X_transformed_1, X_transformed_4)\n\n    # test that different random states give different results\n    nystroem = Nystroem(n_components=5, random_state=1)\n    X_transformed_5 = nystroem.fit_transform(X)\n    assert np.abs(X_transformed_1 - X_transformed_5).sum() > 1e-5\n", "def test_rbf_sampler_stateful():\n    # test that RBFSampler is not stateful\n    rbf_transform = RBFSampler(gamma=1., n_components=100, random_state=42)\n\n    X_trans1 = rbf_transform.fit_transform(X)\n    X_trans2 = rbf_transform.fit_transform(X)\n\n    assert_array_almost_equal(X_trans1, X_trans2)\n\n    # check if fit_transform does not reuse the components from previous call\n    rbf_transform.random_state = 1\n    X_trans3 = rbf_transform.fit_transform(X)\n    assert_array_not_almost_equal(X_trans1, X_trans3)\n\n", "def test_nystroem_edge_cases():\n    # test that Nystroem handles edge cases\n    rnd = np.random.RandomState(42)\n    X = rnd.uniform(size=(1, 1))\n    \n    # n_components larger than n_samples\n    nystroem = Nystroem(n_components=2)\n    X_transformed = nystroem.fit_transform(X)\n    assert X_transformed.shape == (1, 1)\n\n    # n_components equals n_samples\n    nystroem = Nystroem(n_components=1)\n    X_transformed = nystroem.fit_transform(X)\n    assert X_transformed.shape == (1, 1)\n\n    # n_components smaller than 1\n    with pytest.raises(ValueError, match=\"n_components must be a positive\"):\n        Nystroem(n_components=0)\n\n    # invalid kernel\n    with pytest.raises(ValueError, match=\"Invalid kernel\"):\n        Nystroem(kernel='invalid_kernel')\n\n    # kernel is a callable but does not take exactly two arguments\n        return np.dot(X, X.T)\n    with pytest.raises(ValueError, match=\"The kernel must take exactly two\"):\n        Nystroem(kernel=invalid_kernel)\n\n    # test that the components_ and component_indices_ are correctly set\n    nystroem = Nystroem(n_components=1)\n    nystroem.fit(X)\n    assert_array_equal(nystroem.components_, X)\n    assert_array_equal(nystroem.component_indices_, np.array([0]))\n"], "sample_235": ["def test_multiple_callbacks_with_savepoint(self):\n    callbacks = []\n\n        callbacks.append(1)\n\n        callbacks.append(2)\n\n        callbacks.append(3)\n\n    with transaction.atomic():\n        with transaction.atomic():\n            transaction.on_commit(callback1)\n            transaction.on_commit(callback2)\n        transaction.on_commit(callback3)\n\n    self.assertEqual(callbacks, [1, 2, 3])\n", "    def test_on_commit_with_nested_transactions_and_exception(self):\n        try:\n            with transaction.atomic():\n                with transaction.atomic():\n                    self.do(1)\n                    try:\n                        with transaction.atomic():\n                            raise ForcedError()\n                    except ForcedError:\n                        pass\n                    self.do(2)\n                self.do(3)\n                raise ForcedError()\n        except ForcedError:\n            pass\n\n        self.assertDone([1, 2])\n", "    def test_validate_thread_sharing(self):\n        # Test that validate_thread_sharing raises an error when the connection\n        # is accessed from a different thread\n            connection.validate_thread_sharing()\n\n        import threading\n        thread = threading.Thread(target=access_connection)\n        with self.assertRaises(DatabaseError):\n            thread.start()\n            thread.join()\n\n        # Test that validate_thread_sharing does not raise an error when\n        # allow_thread_sharing is True\n        connection.inc_thread_sharing()\n        try:\n            access_connection()\n        finally:\n            connection.dec_thread_sharing()\n\n        # Test that validate_thread_sharing raises an error when\n        # allow_thread_sharing is False and accessed from the same thread\n        connection.dec_thread_sharing()\n        with self.assertRaises(DatabaseError):\n            access_connection()\n", "def test_run_and_clear_commit_hooks_multiple_times(self):\n    with transaction.atomic():\n        self.do(1)\n    with transaction.atomic():\n        self.do(2)\n\n    connection.run_and_clear_commit_hooks()\n    connection.run_and_clear_commit_hooks()  # should be idempotent\n\n    self.assertDone([1, 2])  # not [1, 1, 2, 2]\n", "def test_run_on_commit_called_multiple_times(self):\n        self.notified.append('commit')\n\n    with transaction.atomic():\n        transaction.on_commit(on_commit)\n        transaction.on_commit(on_commit)\n\n    self.assertEqual(self.notified, ['commit'])\n"], "sample_29": ["def test_write_latex_invalid_cosmology_instance(self, write, tmp_path):\n    \"\"\"Test passing an invalid cosmology instance\"\"\"\n    invalid_cosmology = \"Invalid Cosmology\"\n    fp = tmp_path / \"test_write_latex_invalid_cosmology_instance.tex\"\n    with pytest.raises(TypeError, match=\"cosmology must be a Cosmology instance\"):\n        write(invalid_cosmology, fp, format=\"latex\")\n", "def test_write_latex_parameter_unit_conversion(self, write, tmp_path):\n    \"\"\"Test parameter unit conversion when writing to LaTeX.\"\"\"\n    fp = tmp_path / \"test_write_latex_parameter_unit_conversion.tex\"\n    write(fp, format=\"latex\")\n    tbl = QTable.read(fp)\n    for column_name in tbl.colnames[2:]:\n        column = tbl[column_name]\n        assert column.unit is not None, f\"Unit is not set for column '{column_name}'\"\n", "def test_latex_no_overwrite_exception_message(self, write, tmp_path):\n    \"\"\"Test the overwrite exception message\"\"\"\n    fp = tmp_path / \"test_latex_no_overwrite_exception_message.tex\"\n    write(fp, format=\"latex\", overwrite=True)\n    try:\n        write(fp, format=\"latex\", overwrite=False)\n    except OSError as e:\n        assert \"overwrite=True\" in str(e)\n    else:\n        assert False, \"Expected an OSError to be raised\"\n", "def test_write_latex_with_non_latex_cls(self, write, tmp_path):\n    \"\"\"Test passing a non-Table subclass as cls\"\"\"\n    fp = tmp_path / \"test_write_latex_with_non_latex_cls.tex\"\n    with pytest.raises(TypeError, match=\"'cls' must be a subclass\"):\n        write(fp, format=\"latex\", cls=str)\n", "def test_latex_parameter_unit_conversion(self, write, tmp_path):\n    \"\"\"Test parameter unit conversion in LaTeX output\"\"\"\n    fp = tmp_path / \"test_latex_parameter_unit_conversion.tex\"\n    write(fp, format=\"latex\")\n    tbl = QTable.read(fp)\n    for column_name in tbl.colnames[2:]:\n        if column_name in _FORMAT_TABLE.values():\n            # Get original parameter name\n            original_name = [k for k, v in _FORMAT_TABLE.items() if v == column_name][0]\n            # Get unit of original parameter\n            param = getattr(type(self.cosmo), original_name)\n            # Check if unit is not None or dimensionless\n            if param.unit not in (None, u.one):\n                # Check if column has correct unit\n                assert tbl[column_name].unit == param.unit\n"], "sample_749": ["def test_column_transformer_transformer_weights_with_remainder_transformer():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).T\n\n    # second and third columns are doubled when remainder = DoubleTrans\n    X_res_both = X_array.copy()\n\n    # first column is scaled by 0.5\n    X_res_both[:, 0] *= 0.5\n\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           remainder=DoubleTrans(),\n                           transformer_weights={'trans1': 0.5})\n\n    assert_array_equal(ct.fit_transform(X_array), X_res_both)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert isinstance(ct.transformers_[-1][1], DoubleTrans)\n    assert_array_equal(ct.transformers_[-1][2], [1, 2])\n", "def test_column_transformer_transformer_weights_with_remainder():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).T\n\n    # second and third columns are doubled when remainder = DoubleTrans\n    X_res_both = X_array.copy()\n    X_res_both[:, 1:3] *= 2\n\n    transformer_weights = {'trans1': 0.5, 'remainder': 2}\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           remainder=DoubleTrans(),\n                           transformer_weights=transformer_weights)\n\n    X_trans = ct.fit_transform(X_array)\n    X_res_both[:, 0] *= 0.5\n    assert_array_equal(X_trans, X_res_both)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert isinstance(ct.transformers_[-1][1], DoubleTrans)\n    assert_array_equal(ct.transformers_[-1][2], [1, 2])\n", "def test_column_transformer_parallel_processing():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_res_first1D = np.array([0, 1, 2])\n    X_res_first = X_res_first1D.reshape(-1, 1)\n\n    # test parallel processing with n_jobs=2\n    ct = ColumnTransformer([('trans1', Trans(), [0]), ('trans2', Trans(), [1])],\n                           n_jobs=2)\n    assert_array_equal(ct.fit_transform(X_array), X_array)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_array)\n    assert len(ct.transformers_) == 2\n\n    # test parallel processing with n_jobs=-1\n    ct = ColumnTransformer([('trans1', Trans(), [0]), ('trans2', Trans(), [1])],\n                           n_jobs=-1)\n    assert_array_equal(ct.fit_transform(X_array), X_array)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_array)\n    assert len(ct.transformers_) == 2\n\n    # test parallel processing with n_jobs=1\n    ct = ColumnTransformer([('trans1', Trans(), [0]), ('trans2', Trans(), [1])],\n                           n_jobs=1)\n    assert_array_equal(ct.fit_transform(X_array), X_array)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_array)\n    assert len(ct.transformers_) == 2\n", "def test_column_transformer_dtypes():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_array_with_object_dtype = np.array([[0, 1, 2], [2, 4, 6]],\n                                         dtype=object).T\n\n    scaler = StandardScaler()\n    ct = ColumnTransformer([('trans', scaler, [0])])\n\n    # check that the transformer is applied to the first column of X_array\n    ct.fit(X_array)\n    assert_equal(ct.transformers_[0][1].mean_, 1.)\n\n    # check that the transformer is not applied to the first column of X_array_with_object_dtype\n    ct.fit(X_array_with_object_dtype)\n    assert_raises(AttributeError, getattr, ct.transformers_[0][1], 'mean_')\n", "def test_column_transformer_parallel():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    n_jobs = 3\n\n    ct = ColumnTransformer([('trans1', Trans(), [0]),\n                            ('trans2', Trans(), [1])],\n                           n_jobs=n_jobs)\n    assert_array_equal(ct.fit_transform(X_array), X_array)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_array)\n\n    # check that parallel has been executed\n    assert ct.n_jobs == n_jobs\n\n    # check that there are more than one transformers\n    assert len(ct.transformers_) > 1\n\n    # check that transformers_ is still in the correct order\n    assert_array_equal(ct.transformers_[0][2], [0])\n    assert_array_equal(ct.transformers_[1][2], [1])\n"], "sample_444": ["    def test_hash_key(self):\n        storage = storage.HashedFilesMixin()\n        self.assertEqual(\n            storage.hash_key(\"test/file.txt\"),\n            \"test/file.txt\",\n        )\n        self.assertEqual(\n            storage.hash_key(\"/test/file.txt\"),\n            \"test/file.txt\",\n        )\n        self.assertEqual(\n            storage.hash_key(\"test/file.txt?query\"),\n            \"test/file.txt\",\n        )\n        self.assertEqual(\n            storage.hash_key(\"/test/file.txt?query\"),\n            \"test/file.txt\",\n        )\n        self.assertEqual(\n            storage.hash_key(\"test/file.txt#fragment\"),\n            \"test/file.txt\",\n        )\n        self.assertEqual(\n            storage.hash_key(\"/test/file.txt#fragment\"),\n            \"test/file.txt\",\n        )\n", "    def test_url_converter_context(self):\n        static_files_storage = storage.StaticFilesStorage()\n        converter = static_files_storage.url_converter(\"test.js\", {})\n        matchobj = mock.Mock()\n        matchobj.groupdict.return_value = {\"matched\": \"import ('test.css')\", \"url\": \"test.css\"}\n        self.assertEqual(converter(matchobj), \"import ('test.css')\")\n        matchobj.groupdict.assert_called_once_with()\n", "    def setUp(self):\n        self.manifest_storage = ManifestFilesMixin()\n", "    def setUp(self):\n        storage.staticfiles_storage.hashed_files.clear()  # avoid cache interference\n        super().setUp()\n", "    def test_max_post_process_passes_exceeded(self):\n        \"\"\"\n        Raise an exception when the max post-process passes is exceeded.\n        \"\"\"\n        staticfiles_storage = storage.HashedFilesMixin()\n        staticfiles_storage.max_post_process_passes = 1\n        paths = {\n            \"cached/css/window.css\": (\"storage\", \"path/to/cached/css/window.css\"),\n            \"cached/css/window.png\": (\"storage\", \"path/to/cached/css/window.png\"),\n        }\n        with self.assertRaisesMessage(RuntimeError, \"Max post-process passes exceeded\"):\n            list(staticfiles_storage.post_process(paths))\n"], "sample_106": ["    def tearDown(self):\n        cache.clear()\n", "    def test_patch_vary_headers(self):\n        headers = (\n            # Initial vary, new headers, resulting vary.\n            (None, ('Accept-Encoding',), 'Accept-Encoding'),\n            ('Accept-Encoding', ('accept-encoding',), 'Accept-Encoding'),\n            ('Accept-Encoding', ('ACCEPT-ENCODING',), 'Accept-Encoding'),\n            ('Cookie', ('Accept-Encoding',), 'Cookie, Accept-Encoding'),\n            ('Cookie, Accept-Encoding', ('Accept-Encoding',), 'Cookie, Accept-Encoding'),\n            ('Cookie, Accept-Encoding', ('Accept-Encoding', 'cookie'), 'Cookie, Accept-Encoding'),\n            (None, ('Accept-Encoding', 'COOKIE'), 'Accept-Encoding, COOKIE'),\n            ('Cookie,     Accept-Encoding', ('Accept-Encoding', 'cookie'), 'Cookie, Accept-Encoding'),\n            ('Cookie    ,     Accept-Encoding', ('Accept-Encoding', 'cookie'), 'Cookie, Accept-Encoding'),\n            ('*', ('Accept-Language', 'Cookie'), '*'),\n            ('Accept-Language, Cookie', ('*',), '*'),\n        )\n        for initial_vary, newheaders, resulting_vary in headers:\n            with self.subTest(initial_vary=initial_vary, newheaders=newheaders):\n                response = HttpResponse()\n                if initial_vary is not None:\n                    response['Vary'] = initial_vary\n                patch_vary_headers(response, newheaders, vary_on=['Accept-Encoding'])\n                self.assertEqual(response['Vary'], resulting_vary)\n", "    def test_vary_header_addition(self):\n        response = HttpResponse()\n        patch_vary_headers(response, ['Accept-Encoding'])\n        self.assertEqual(response['Vary'], 'Accept-Encoding')\n", "    def test_patch_cache_control_empty(self):\n        response = HttpResponse()\n        patch_cache_control(response)\n        self.assertEqual(response['Cache-Control'], '')\n", "    def test_patch_response_headers_with_integers(self):\n        response = HttpResponse()\n        patch_response_headers(response, max_age=10)\n        self.assertIn('max-age=10', response['Cache-Control'])\n"], "sample_69": ["    def test_iter_all_python_module_files_calls_iter_modules_and_files(self, mocked_iter_modules_and_files):\n        autoreload.iter_all_python_module_files()\n        self.assertEqual(mocked_iter_modules_and_files.call_count, 1)\n        self.assertEqual(mocked_iter_modules_and_files.call_args[0][0], tuple(sys.modules.values()))\n        self.assertEqual(mocked_iter_modules_and_files.call_args[0][1], frozenset(_error_files))\n", "    def test_echo_is_enabled(self, mocked_termios):\n        mocked_termios.isatty.return_value = True\n        mocked_termios.tcgetattr.return_value = [0, 1, 2, 3 | mocked_termios.ECHO, 4]\n        autoreload.ensure_echo_on()\n        mocked_termios.tcsetattr.assert_not_called()\n", "    def setUp(self):\n        self.reloader = autoreload.StatReloader()\n", "    def test_common_roots_with_nonexisting_directories(self):\n        paths = (\n            Path('/first/second'),\n            Path('/first/second/third'),\n            Path('/first/'),\n            Path('/root/first/'),\n            Path('/nonexistent/'),\n        )\n        results = autoreload.common_roots(paths)\n        self.assertCountEqual(results, [Path('/first/'), Path('/root/first/')])\n\n", "    def test_enable_echo_mode(self, mocked_tcsetattr, mocked_tcgetattr):\n        # Check the echo mode is enabled when it is disabled.\n        mocked_tcgetattr.return_value[3] = 0\n        autoreload.ensure_echo_on()\n        mocked_tcsetattr.assert_called_once_with(mock.ANY, termios.TCSANOW, [1, 2, 3, 4, 5 | termios.ECHO])\n"], "sample_522": ["def test_colorbar_remove_from_figure_twice():\n    fig, ax = plt.subplots()\n    sc = ax.scatter([1, 2], [3, 4])\n    sc.set_array(np.array([5, 6]))\n    cb = fig.colorbar(sc)\n    fig.draw_without_rendering()\n    cb.remove()\n    fig.draw_without_rendering()\n    cb.remove()  # This should not crash\n", "def test_colorbar_spines():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.arange(100).reshape(10, 10))\n    cbar = fig.colorbar(pc)\n    cbar.ax.spines['left'].set_visible(False)\n    cbar.ax.spines['right'].set_visible(False)\n    cbar.ax.spines['top'].set_visible(False)\n    cbar.ax.spines['bottom'].set_visible(False)\n    cbar.outline.set_visible(False)\n", "def test_colorbar_subplotspec():\n    fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n    ax = axs[0]\n    ax.pcolormesh([[1, 2], [3, 4]])\n    # test that if a subplotspec is passed in, it is used\n    fig.colorbar(ax=ax, ax=ax, use_gridspec=True)\n    ax = axs[1]\n    ax.pcolormesh([[1, 2], [3, 4]])\n    # test that the default subplotspec is used\n    fig.colorbar(ax=ax, use_gridspec=True)\n    fig.tight_layout()\n", "def test_vertical_horizontal_colorbar(cbar_location):\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.arange(100).reshape(10, 10))\n    fig.colorbar(pc, location=cbar_location)\n    # After the colorbar is added, the original axis should still be\n    # contained in the figure.\n    assert ax in fig.axes\n    # The colorbar axes should be in the figure's axes.\n    assert fig.colorbar(pc).ax in fig.axes\n", "def test_colorbar_sca_with_subplotspec(use_gridspec):\n    # test fix for #23094\n    fig = plt.figure()\n    gs = fig.add_gridspec(2, 2)\n    ax = fig.add_subplot(gs[0, :])\n    ax2 = fig.add_subplot(gs[1, 0])\n    plt.sca(ax2)\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cbar = fig.colorbar(pc, ax=ax2, use_gridspec=use_gridspec)\n    fig.draw_without_rendering()\n    ax2p = cbar.ax.get_position(original=False)\n    axp = ax2.get_position(original=False)\n    assert axp.x0 <= ax2p.x0\n"], "sample_148": ["def test_get_model_from_relation(self):\n    \"\"\"\n    Test the get_model_from_relation function\n    \"\"\"\n    # Test a ForeignKey\n    field = Article._meta.get_field('site')\n    self.assertEqual(get_model_from_relation(field), Site)\n\n    # Test a ManyToManyField\n    field = Article._meta.get_field('sites')\n    self.assertEqual(get_model_from_relation(field), Site)\n\n    # Test a reverse OneToOneField\n    field = Article._meta.get_field('location')\n    self.assertEqual(get_model_from_relation(field), Location)\n\n    # Test a forward OneToOneField\n    field = Location._meta.get_field('article')\n    self.assertEqual(get_model_from_relation(field), Article)\n\n    # Test a field that is not a relation\n    field = Article._meta.get_field('title')\n    with self.assertRaises(NotRelationField):\n        get_model_from_relation(field)\n", "    def test_get_model_from_relation(self):\n        \"\"\"Test get_model_from_relation function.\"\"\"\n        article = Article.objects.create()\n        site = article.site\n        self.assertEqual(get_model_from_relation(site._meta.get_field('article')), Article)\n\n        with self.assertRaises(NotRelationField):\n            get_model_from_relation(site._meta.get_field('domain'))\n", "    def test_get_fields_from_path(self):\n        \"\"\"\n        Tests for get_fields_from_path\n        \"\"\"\n        # Test with a normal path\n        fields = get_fields_from_path(Article, 'site__domain')\n        self.assertEqual(len(fields), 2)\n        self.assertEqual(fields[0].__class__.__name__, 'ForwardOneToOneDescriptor')\n        self.assertEqual(fields[1].__class__.__name__, 'ForwardField')\n\n        # Test with a path that doesn't exist\n        with self.assertRaises(FieldDoesNotExist):\n            get_fields_from_path(Article, 'nonexistent__field')\n\n        # Test with a path that ends with a non-field\n        fields = get_fields_from_path(Article, 'site__test_from_model')\n        self.assertEqual(len(fields), 1)\n        self.assertEqual(fields[0].__class__.__name__, 'ForwardOneToOneDescriptor')\n", "def test_model_format_dict(self):\n    article = Article(title='Test article')\n    article_queryset = Article.objects.all()\n\n    self.assertEqual(model_format_dict(article), {\n        'verbose_name': 'article',\n        'verbose_name_plural': 'articles',\n    })\n    self.assertEqual(model_format_dict(Article), {\n        'verbose_name': 'article',\n        'verbose_name_plural': 'articles',\n    })\n    self.assertEqual(model_format_dict(article_queryset), {\n        'verbose_name': 'article',\n        'verbose_name_plural': 'articles',\n    })\n", "def test_lookup_needs_distinct(self):\n    \"\"\"\n    Tests for lookup_needs_distinct\n    \"\"\"\n    article = Article.objects.create()\n    site = Site.objects.create()\n    article.site = site\n    article.save()\n\n    self.assertFalse(lookup_needs_distinct(Article._meta, 'site__id'))\n    self.assertFalse(lookup_needs_distinct(Article._meta, 'id'))\n    self.assertFalse(lookup_needs_distinct(Site._meta, 'article__id'))\n    self.assertFalse(lookup_needs_distinct(Count._meta, 'id'))\n\n    self.assertTrue(lookup_needs_distinct(Event._meta, 'guest__name'))\n    self.assertTrue(lookup_needs_distinct(Location._meta, 'event__name'))\n"], "sample_1206": ["def test_issue_19417():\n    assert S.Half._eval_power(-3) == Rational(8, 1)\n", "def test_issue_20778():\n    assert Rational(1)._as_mpf_val(10)._prec == 10\n    assert Rational(1)._as_mpf_op(10) == ((0, 1, 0, 1), 10)\n    assert Rational(1)._eval_power(Integer(2)) == 1\n    assert Rational(1)._eval_power(S.Half) == 1\n    assert Rational(1)._eval_power(S.Pi) == 1\n", "def test_issue_13585():\n    assert S.Infinity - S.Infinity is S.NaN\n    assert S.Infinity - (-S.Infinity) is S.Infinity\n    assert S.Infinity - oo is S.Infinity\n    assert S.Infinity - (-oo) is S.Infinity\n    assert -S.Infinity - S.Infinity is -S.Infinity\n    assert -S.Infinity - (-S.Infinity) is S.NaN\n    assert -S.Infinity - oo is -S.Infinity\n    assert -S.Infinity - (-oo) is S.Infinity\n    assert oo - S.Infinity is S.Infinity\n    assert oo - (-S.Infinity) is S.Infinity\n    assert -oo - S.Infinity is -S.Infinity\n    assert -oo - (-S.Infinity) is S.Infinity\n    assert S.Infinity - S.Infinity.is_real is False\n    assert S.Infinity - (-S.Infinity).is_real is True\n    assert S.Infinity - oo.is_real is True\n    assert S.Infinity - (-oo).is_real is True\n    assert -S.Infinity - S.Infinity.is_real is False\n    assert -S.Infinity - (-S.Infinity).is_real is True\n    assert -S.Infinity - oo.is_real is True\n    assert -S.Infinity - (-oo).is_real is True\n    assert oo - S.Infinity.is_real is True\n    assert oo - (-S.Infinity).is_real is True\n    assert -oo - S.Infinity.is_real is True\n    assert -oo - (-S.Infinity).is_real is True\n", "def test_issue_9305():\n    assert Rational(1, 0) is S.ComplexInfinity\n    assert Rational(-1, 0) is S.ComplexInfinity\n    assert Rational(0, 0) is S.ComplexInfinity\n    assert Rational(1, 0)._hashable_content() == (1, 0)\n    assert Rational(-1, 0)._hashable_content() == (1, 0)\n    assert Rational(0, 0)._hashable_content() == (0, 0)\n    assert hash(Rational(1, 0)) == hash(Rational(-1, 0))\n    assert hash(Rational(0, 0)) == hash(S.ComplexInfinity)\n    assert Rational(1, 0) != S.Infinity\n    assert Rational(-1, 0) != -S.Infinity\n    assert Rational(0, 0) != S.NaN\n    assert Rational(1, 0) != S.NegativeInfinity\n    assert Rational(-1, 0) != S.Infinity\n    assert Rational(0, 0) != S.Infinity\n    assert Rational(0, 0) != S.NegativeInfinity\n", "def test_AlgebraicNumber():\n    \"\"\"\n    Test AlgebraicNumber\n    \"\"\"\n    A = AlgebraicNumber\n    S = Dummy('S')\n    sqrt2 = A(sqrt(2))\n    sqrt3 = A(sqrt(3))\n\n    # Test alias\n    assert A(sqrt2, [1, 2], alias=S).alias == S\n    assert A(sqrt2, [1, 2], alias='S').alias == S\n\n    # Test primitive element\n    assert A(sqrt2, [1, 2]).primitive_element() == sqrt2\n    assert A(sqrt2, [1, 2], alias=S).primitive_element() == sqrt2\n\n    # Test expansion of primitive element\n    expr = sqrt2 + sqrt3\n    assert A(expr, [1, 2], alias=S).primitive_element() == expr\n    assert A(expr, [1, 2], alias=S).primitive_element().evalf() == expr.evalf()\n\n    # Test to_primitive_element\n    assert A(sqrt2, [1, 2]).to_primitive_element() == A(sqrt2, [1, 2])\n    assert A(sqrt2, [1, 2], alias=S).to_primitive_element() == A(sqrt2, [1, 2], alias=S)\n\n    # Test field_element\n    assert A(sqrt2, [1, 2]).field_element([1, 3]) == A(sqrt2, [1, 3])\n    assert A(sqrt2, [1, 2], alias=S).field_element([1, 3]) == A(sqrt2, [1, 3], alias=S)\n\n    # Test coeffs\n    assert A(sqrt2, [1, 2]).coeffs() == [1, 2]\n    assert A(sqrt2, [1, 2], alias=S).coeffs() == [1, 2]\n\n    # Test minpoly_of_element\n    assert A(sqrt2, [1, 2]).minpoly_of_element().as_expr() == -1 + 4*x**2\n    assert A(sqrt2, [1, 2], alias=S).minpoly_of_element().as_expr() == -1 + 4*x**2\n\n    # Test to_root\n    assert A(sqrt2, [1, 2]).to_root() == sqrt2\n    assert A(sqrt2, [1, 2], alias=S).to_root()"], "sample_128": ["    def test_clone(self):\n        index = Index(\n            name='test_clone',\n            fields=['headline'],\n            opclasses=['varchar_pattern_ops'],\n        )\n        cloned_index = index.clone()\n        self.assertEqual(index.deconstruct(), cloned_index.deconstruct())\n", "    def test_deconstruct(self):\n        index = Index(fields=['headline', '-body'], name='test_index', db_tablespace='test_tablespace')\n        path, args, kwargs = index.deconstruct()\n        self.assertEqual(path, 'django.db.models.indexes.Index')\n        self.assertEqual(args, ())\n        self.assertEqual(kwargs, {'fields': ['headline', '-body'], 'name': 'test_index', 'db_tablespace': 'test_tablespace'})\n", "    def test_set_name_with_model(self):\n        \"\"\"\n        Ensure that the set_name_with_model method generates a unique name for the index.\n        \"\"\"\n        index = Index(fields=['headline'])\n        model = Article\n        index.set_name_with_model(model)\n        self.assertLessEqual(len(index.name), index.max_name_length)\n        self.assertIn(model._meta.db_table[:11], index.name)\n        self.assertIn('headline', index.name)\n        self.assertIn(index.suffix, index.name)\n\n        # Check that the name is deterministic\n        index2 = Index(fields=['headline'])\n        index2.set_name_with_model(model)\n        self.assertEqual(index.name, index2.name)\n", "    def test_index_name_generation(self):\n        index = Index(fields=['headline', 'pub_date'])\n        index.set_name_with_model(Article)\n        self.assertLessEqual(len(index.name), index.max_name_length)\n", "    def test_index_init(self):\n        with self.assertRaises(ValueError):\n            Index(fields=(), name='test')\n        with self.assertRaises(ValueError):\n            Index(fields=['field1'], name=None, opclasses=['varchar_pattern_ops'])\n        with self.assertRaises(ValueError):\n            Index(fields=['field1'], name=None, condition=Q(field1='value'))\n        with self.assertRaises(ValueError):\n            Index(fields=['field1'], name='test', opclasses=['varchar_pattern_ops', 'text_pattern_ops'])\n        with self.assertRaises(ValueError):\n            Index(fields=['field1'], name=None, include=['field2'])\n        with self.assertRaises(ValueError):\n            Index(fields=['field1'], name='test', include='field2')\n"], "sample_191": ["    def test_stat_reloader_catches_stopiteration(self):\n            yield\n            self.reloader.stop()\n            return  # Raises StopIteration\n\n        with mock.patch.object(self.reloader, 'tick', side_effect=mocked_tick) as tick:\n            self.reloader.run_loop()\n\n        self.assertEqual(tick.call_count, 1)\n", "    def test_python_m_django_windows(self):\n        main = 'C:\\\\PythonXX\\\\Lib\\\\site-packages\\\\django\\\\__main__.py'\n        argv = [main, 'runserver']\n        mock_call = self.patch_autoreload(argv)\n        with mock.patch('django.__main__.__file__', main):\n            autoreload.restart_with_reloader()\n            self.assertEqual(mock_call.call_count, 1)\n            self.assertEqual(mock_call.call_args[0][0], [self.executable, '-Wall', '-m', 'django'] + argv[1:])\n", "    def test_echo_enabled(self, mocked_termios):\n        mocked_termios.tcgetattr.return_value = [0, 0, 0, 5]\n        autoreload.ensure_echo_on()\n        mocked_termios.tcgetattr.assert_called_once_with(0)\n        mocked_termios.tcsetattr.assert_called_once_with(0, mocked_termios.TCSANOW, [0, 0, 0, 5 | mocked_termios.ECHO])\n", "    def setUp(self):\n        self.termios = autoreload.termios\n        autoreload.termios = mock.MagicMock()\n", "    def test_watcher_ends_on_close(self, mocked_reloader, mocked_client):\n        reloader = autoreload.WatchmanReloader()\n        self.assertEqual(reloader.client._closed, False)\n        reloader.stop()\n        self.assertEqual(reloader.client._closed, True)\n"], "sample_591": ["    def test_update(self):\n        data = create_test_data()\n        ds1 = data[[\"var1\"]]\n        ds2 = data[[\"var3\"]]\n        expected = data[[\"var1\", \"var3\"]]\n        actual = ds1.update(ds2)\n        assert expected.identical(actual)\n\n        actual = data.update(data)\n        assert data.identical(actual)\n        actual = data.reset_coords(drop=True).update(data)\n        assert data.identical(actual)\n        actual = data.update(data.reset_coords(drop=True))\n        assert data.identical(actual)\n\n        with pytest.raises(ValueError):\n            ds1.update(ds2.rename({\"var3\": \"var1\"}))\n        with raises_regex(ValueError, \"should be coordinates or not\"):\n            data.reset_coords().update(data)\n        with raises_regex(ValueError, \"should be coordinates or not\"):\n            data.update(data.reset_coords())\n", "def test_merge_virtual(self):\n    ds1 = xr.Dataset({\"x\": [1, 2]})\n    ds2 = xr.Dataset({\"y\": [2, 3]})\n    expected = xr.Dataset({\"x\": (\"y\", [1, 2]), \"y\": [2, 3]})\n    assert expected.identical(ds1.merge(ds2, compat=\"no_conflicts\", join=\"override\"))\n", "def test_merge_no_conflicts_with_fill_value(self):\n    ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n    ds2 = xr.Dataset({\"a\": (\"x\", [2, 3]), \"x\": [1, 2]})\n\n    fill_value = 9\n    expected = xr.Dataset({\"a\": (\"x\", [1, 2, 3]), \"x\": [0, 1, 2]})\n    assert expected.identical(ds1.merge(ds2, compat=\"no_conflicts\", fill_value=fill_value))\n\n    expected = xr.Dataset({\"a\": (\"x\", [1, 2, 9]), \"x\": [0, 1, 2]})\n    assert expected.identical(ds1.merge(ds2, compat=\"no_conflicts\", join=\"left\", fill_value=fill_value))\n\n    expected = xr.Dataset({\"a\": (\"x\", [9, 2, 3]), \"x\": [0, 1, 2]})\n    assert expected.identical(ds1.merge(ds2, compat=\"no_conflicts\", join=\"right\", fill_value=fill_value))\n\n    expected = xr.Dataset({\"a\": (\"x\", [9]), \"x\": [0]})\n    assert expected.identical(ds1.merge(ds2, compat=\"no_conflicts\", join=\"inner\", fill_value=fill_value))\n", "    def test_dataset_merge_method(self):\n        # Test that merge method uses dataset_merge_method\n        ds = xr.Dataset({\"x\": 1})\n        other = xr.Dataset({\"x\": 2})\n        with pytest.raises(xr.MergeError):\n            ds.merge(other)\n\n        actual = xr.merge([ds, other])\n        assert not actual.identical(ds.merge(other))\n", "def test_dataset_repr(self):\n    # repr\n    ds = xr.Dataset({\"foo\": 0}, {\"bar\": 1})\n    assert \"Dataset\" in repr(ds)\n    # assert \"foo\" in repr(ds)\n    assert \"dims:\" in repr(ds)\n    assert \"coords:\" in repr(ds)\n    assert \"data_vars:\" in repr(ds)\n\n    # info\n    ds.info()\n"], "sample_170": ["    def test_technical_500_response(self):\n        request = RequestFactory().get('/test_view/')\n        try:\n            raise ValueError(\"Can't find my keys\")\n        except ValueError:\n            exc_type, exc_value, tb = sys.exc_info()\n        response = technical_500_response(request, exc_type, exc_value, tb)\n        self.assertEqual(response.status_code, 500)\n        self.assertEqual(response.content_type, 'text/html')\n", "    def test_request_with_items_key(self):\n        \"\"\"\n        An exception report can be generated for requests with 'items' in\n        request GET, POST, FILES, or COOKIES QueryDicts.\n        \"\"\"\n        value = '<td>items</td><td class=\"code\"><pre>&#x27;Oops&#x27;</pre></td>'\n        # GET\n        request = RequestFactory().get('/test_view/?items=Oops')\n        reporter = ExceptionReporter(request, None, None, None)\n        html = technical_500_response(request, None, \"I'm a little teapot\", None)\n        self.assertInHTML(value, html)\n        # POST\n        request = RequestFactory().post('/test_view/', data={'items': 'Oops'})\n        reporter = ExceptionReporter(request, None, None, None)\n        html = technical_500_response(request, None, \"I'm a little teapot\", None)\n        self.assertInHTML(value, html)\n        # FILES\n        fp = StringIO('filecontent')\n        request = RequestFactory().post('/test_view/', data={'name': 'filename', 'items': fp})\n        reporter = ExceptionReporter(request, None, None, None)\n        html = technical_500_response(request, None, \"I'm a little teapot\", None)\n        self.assertInHTML(\n            '<td>items</td><td class=\"code\"><pre>&lt;InMemoryUploadedFile: '\n            'items (application/octet-stream)&gt;</pre></td>',\n            html\n        )\n        # COOKIES\n        rf = RequestFactory()\n        rf.cookies['items'] = 'Oops'\n        request = rf.get('/test_view/')\n        reporter = ExceptionReporter(request, None, None, None)\n        html = technical_500_response(request, None, \"I'm a little teapot\", None)\n        self.assertInHTML('<td>items</td><td class=\"code\"><pre>&#x27;Oops&#x27;</pre></td>', html)\n", "    def test_cleansing_non_string_values(self):\n        reporter_filter = SafeExceptionReporterFilter()\n        self.assertEqual(reporter_filter.cleanse_setting('SECRET_KEY', 123), 123)\n        self.assertEqual(reporter_filter.cleanse_setting('SECRET_KEY', None), None)\n        self.assertEqual(reporter_filter.cleanse_setting('SECRET_KEY', True), True)\n", "    def test_traceback_frames_without_source_two_frames(self):\n        \"\"\"\n        Check that the ExceptionReporter doesn't crash when there's no source\n        for two frames in the traceback.\n        \"\"\"\n                raise RuntimeError('inner')\n            inner()\n        try:\n            test_func()\n        except Exception:\n            exc_type, exc_value, tb = sys.exc_info()\n        request = self.rf.get('/test_view/')\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n        frames = reporter.get_traceback_frames()\n        self.assertEqual(len(frames), 2)\n        for frame in frames:\n            self.assertEqual(frame['context_line'], '<source code not available>')\n", "    def test_sensitive_variables_function(self):\n            return arg1, arg2, kwargs\n        reporter_filter = SafeExceptionReporterFilter()\n        variables = reporter_filter.get_traceback_frame_variables(None, inspect.currentframe().f_back)\n        self.assertEqual(variables, [('arg1', 'arg1'), ('arg2', 'arg2'), ('kwargs', 'kwargs'), ('self', 'self')])\n"], "sample_801": ["def test_clone():\n    # Test cloning of estimators\n    lr = LogisticRegression()\n    lr_clone = clone(lr)\n\n    # Check that cloning works correctly\n    assert lr.__class__ == lr_clone.__class__\n    assert lr.get_params() == lr_clone.get_params()\n\n    # Check that cloning works with nested objects\n    pipeline = make_pipeline(StandardScaler(), LogisticRegression())\n    pipeline_clone = clone(pipeline)\n    assert pipeline.__class__ == pipeline_clone.__class__\n    assert pipeline.get_params() == pipeline_clone.get_params()\n\n    # Check that cloning fails with non-estimators\n    try:\n        clone(\"not an estimator\")\n        assert False, \"Expected TypeError\"\n    except TypeError:\n        pass\n", "def test_clone():\n    # Test the clone function\n    lr = LogisticRegression(C=99)\n    cloned_lr = clone(lr)\n    assert lr.get_params() == cloned_lr.get_params()\n\n    # Test cloning a list of estimators\n    estimators = [LogisticRegression(C=99), StandardScaler()]\n    cloned_estimators = clone(estimators)\n    assert len(estimators) == len(cloned_estimators)\n    for estimator, cloned_estimator in zip(estimators, cloned_estimators):\n        assert estimator.get_params() == cloned_estimator.get_params()\n\n    # Test cloning a non-estimator object\n    obj = [1, 2, 3]\n    cloned_obj = clone(obj, safe=False)\n    assert obj == cloned_obj\n\n    # Test cloning a non-estimator object with safe=True\n    try:\n        clone(obj, safe=True)\n        assert False, \"Expected a TypeError to be raised\"\n    except TypeError:\n        pass\n\n    # Test cloning a pipeline\n    pipeline = make_pipeline(LogisticRegression(C=99), StandardScaler())\n    cloned_pipeline = clone(pipeline)\n    assert pipeline.get_params() == cloned_pipeline.get_params()\n\n    # Test cloning a grid search\n    gs = GridSearchCV(LogisticRegression(), {'C': [1, 10]})\n    cloned_gs = clone(gs)\n    assert gs.get_params() == cloned_gs.get_params()\n", "def test_clone():\n    # Test cloning of estimators with various parameters\n    lr = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                            intercept_scaling=1, l1_ratio=None, max_iter=100,\n                            multi_class='warn', n_jobs=None, penalty='l2',\n                            random_state=None, solver='warn', tol=0.0001, verbose=0,\n                            warm_start=False)\n    lr_cloned = clone(lr)\n    assert lr_cloned.get_params() == lr.get_params()\n\n    # Test cloning of estimators with array parameters\n    lr_array = LogisticRegressionCV(Cs=np.array([0.1, 1]))\n    lr_array_cloned = clone(lr_array)\n    assert lr_array_cloned.get_params() == lr_array.get_params()\n\n    # Test cloning of estimators with non-estimator parameters\n    pipeline = Pipeline([('reduce_dim', PCA()), ('classify', SVC())])\n    pipeline_cloned = clone(pipeline)\n    assert pipeline_cloned.get_params() == pipeline.get_params()\n\n    # Test cloning of estimators with nested estimators\n    nested_pipeline = Pipeline([('reduce_dim', PCA()), ('classify', make_pipeline(LogisticRegression(), SelectKBest(chi2)))])\n    nested_pipeline_cloned = clone(nested_pipeline)\n    assert nested_pipeline_cloned.get_params() == nested_pipeline.get_params()\n\n    # Test cloning of estimators with non-estimator objects\n    gs = GridSearchCV(SVC(), param_grid={'C': [1, 10, 100, 1000]})\n    gs_cloned = clone(gs)\n    assert gs_cloned.get_params() == gs.get_params()\n\n    # Test cloning of lists of estimators\n    estimators = [LogisticRegression(), SVC(), PCA()]\n    estimators_cloned = clone(estimators)\n    assert [estimator.get_params() for estimator in estimators_cloned] == [estimator.get_params() for estimator in estimators]\n", "def test_clone():\n    # Test cloning of estimators\n    lr = LogisticRegression()\n    cloned_lr = clone(lr)\n    assert lr != cloned_lr\n    assert lr.__dict__ == cloned_lr.__dict__\n\n    # Test cloning of estimators with nested objects\n    pipeline = make_pipeline(StandardScaler(), LogisticRegression(C=999))\n    cloned_pipeline = clone(pipeline)\n    assert pipeline != cloned_pipeline\n    assert pipeline.__dict__ == cloned_pipeline.__dict__\n    assert pipeline.steps[1][1] != cloned_pipeline.steps[1][1]\n    assert pipeline.steps[1][1].__dict__ == cloned_pipeline.steps[1][1].__dict__\n\n    # Test cloning of estimators with lists\n    estimators = [LogisticRegression(), LogisticRegression(C=999)]\n    cloned_estimators = clone(estimators)\n    assert estimators != cloned_estimators\n    assert estimators[0] != cloned_estimators[0]\n    assert estimators[0].__dict__ == cloned_estimators[0].__dict__\n    assert estimators[1] != cloned_estimators[1]\n    assert estimators[1].__dict__ == cloned_estimators[1].__dict__\n\n    # Test cloning of estimators with non-estimator objects\n    non_estimator = [1, 2, 3]\n    cloned_non_estimator = clone(non_estimator, safe=False)\n    assert non_estimator == cloned_non_estimator\n\n    # Test cloning of estimators with non-estimator objects and safe=True\n    non_estimator = [1, 2, 3]\n    try:\n        clone(non_estimator, safe=True)\n        assert False, \"Expected a TypeError\"\n    except TypeError:\n        pass\n\n    # Test cloning of estimators with TypeError\n    class NonEstimator:\n            self.param = 1\n\n    non_estimator = NonEstimator()\n    try:\n        clone(non_estimator)\n        assert False, \"Expected a TypeError\"\n    except TypeError:\n        pass\n\n    # Test cloning of estimators with safe=False\n    non_estimator = NonEstimator()\n    cloned_non_estimator = clone(non_estimator, safe=False)\n    assert non_estimator != cloned_non_estimator\n    assert non_estimator.__dict__ == cloned_non_estimator.__dict__\n", "def test_get_params():\n    # Test get_params and set_params methods\n    estimator = LogisticRegression(C=10, max_iter=500)\n    params = estimator.get_params(deep=False)\n    assert params['C'] == 10\n    assert params['max_iter'] == 500\n    estimator.set_params(C=20, max_iter=1000)\n    params = estimator.get_params(deep=False)\n    assert params['C'] == 20\n    assert params['max_iter'] == 1000\n\n    # Test get_params with deep=True\n    nested_estimator = Pipeline([('scaler', StandardScaler()), ('logistic', LogisticRegression())])\n    params = nested_estimator.get_params(deep=True)\n    assert params['logistic__C'] == 1.0\n    assert params['scaler__with_mean'] == True\n\n    # Test set_params with deep=True\n    nested_estimator.set_params(logistic__C=10, scaler__with_mean=False)\n    params = nested_estimator.get_params(deep=True)\n    assert params['logistic__C'] == 10\n    assert params['scaler__with_mean'] == False\n\n    # Test that invalid parameters raise an error\n    try:\n        estimator.set_params(foo='bar')\n        assert False, \"Expected ValueError\"\n    except ValueError:\n        pass\n\n    # Test that valid parameters are accepted even if they are not present in the\n    # estimator's __init__ method\n    estimator.set_params(warm_start=True)\n    params = estimator.get_params(deep=False)\n    assert params['warm_start'] == True\n"], "sample_1098": ["def test_hyper_representatives():\n    from sympy.functions.special.hyper import (HyperRep_power1, HyperRep_power2,\n        HyperRep_log1, HyperRep_asin1, HyperRep_asin2, HyperRep_sqrts1,\n        HyperRep_sqrts2, HyperRep_log2, HyperRep_cosasin, HyperRep_sinasin)\n    z = symbols('z')\n    a = symbols('a', positive=True)\n    h = symbols('h', nonnegative=True, integer=True)\n\n    assert HyperRep_power1(a, z).rewrite('nonrepsmall') == (1 - z)**a\n    assert HyperRep_power1(a, z).rewrite('nonrep') == \\\n        Piecewise(((1 - z)**a, abs(z) < 1),\n                  ((1 + z)**a*exp((2*h - 1)*pi*I*a), abs(z) > 1))\n\n    assert HyperRep_power2(a, z).rewrite('nonrepsmall') == \\\n        2**(2*a - 1)*(1 + sqrt(1 - z))**(1 - 2*a)\n    assert HyperRep_power2(a, z).rewrite('nonrep') == \\\n        Piecewise((2**(2*a - 1)*(1 + sqrt(1 - z))**(1 - 2*a),\n                  abs(z) < 1),\n                  (2**(2*a - 1)*(sqrt(z - 1) + (-1)**h)**(1 - 2*a)*\n                   exp(-2*pi*I*a*h), abs(z) > 1))\n\n    assert HyperRep_log1(z).rewrite('nonrepsmall') == log(1 - z)\n    assert HyperRep_log1(z).rewrite('nonrep') == \\\n        Piecewise((log(z - 1) + (2*h - 1)*pi*I, abs(z) > 1),\n                  (log(1 - z), abs(z) < 1))\n\n    assert HyperRep_asin1(z).rewrite('nonrepsmall') == asin(sqrt(z))/sqrt(z)\n    assert HyperRep_asin1(z).rewrite('nonrep') == \\\n        Piecewise((asin(sqrt(z))/sqrt(z), abs(z) < 1),\n                  ((-1)**h*(S.Half - h)*pi/sqrt(z) +\n                   I*acosh(sqrt(z))/sqrt(z), abs(z) > ", "def test_hyper_representatives():\n    from sympy import symbols, I, exp_polar, sqrt, pi, oo\n    from sympy.functions.special.hyper import (HyperRep_power1, HyperRep_power2,\n        HyperRep_log1, HyperRep_asin1, HyperRep_asin2, HyperRep_sqrts1,\n        HyperRep_sqrts2, HyperRep_log2, HyperRep_cosasin, HyperRep_sinasin)\n    z = symbols('z')\n    reps = [HyperRep_power1, HyperRep_power2, HyperRep_log1, HyperRep_asin1,\n            HyperRep_asin2, HyperRep_sqrts1, HyperRep_sqrts2, HyperRep_log2,\n            HyperRep_cosasin, HyperRep_sinasin]\n\n    for rep in reps:\n        assert rep(z, z).rewrite('nonrep') == rep(z, z).rewrite('nonrepsmall')\n\n        for x in [z, -z, sqrt(z), -sqrt(z), z**2, -z**2]:\n            for n in range(-10, 11):\n                expr1 = rep(z, x).subs(z, exp_polar(2*pi*I*n)*z)\n                expr2 = rep(z, x).subs(z, exp_polar(2*pi*I*(n + 1))*z)\n                assert expr1.rewrite('nonrep') == expr2.rewrite('nonrep')\n\n        for x in [z, -z, sqrt(z), -sqrt(z), z**2, -z**2]:\n            for n in range(-10, 11):\n                expr1 = rep(z, x).subs(z, exp_polar(2*pi*I*n + pi*I)*z)\n                expr2 = rep(z, x).subs(z, exp_polar(2*pi*I*(n + 1) + pi*I)*z)\n                assert expr1.rewrite('nonrep') == expr2.rewrite('nonrep')\n", "def test_hyper_convergence():\n    # issue 21164: check convergence statement\n    assert hyper([], [S(3)/2], -z**2/4).convergence_statement\n    assert not hyper([S(1)/2], [S(3)/2], -z**2/4).convergence_statement\n    assert not hyper([S(1)/2], [S(1)/2], -z**2/4).convergence_statement\n    assert not hyper([S(1)/2, S(1)/2], [S(3)/2], -z**2/4).convergence_statement\n    assert hyper([], [], z**2/4).convergence_statement\n    assert hyper([], [S(3)/2], z**2/4).convergence_statement\n    assert hyper([S(1)/2], [S(3)/2], z**2/4).convergence_statement\n    assert hyper([S(1)/2, S(1)/2], [S(3)/2], z**2/4).convergence_statement\n", "def test_meijerg_hyper():\n    from sympy.abc import a, b, c, z\n    assert meijerg([[a], []], [[c], [b]], z).doit() == hyper([a, 1 - c], [b], z)\n    assert meijerg([[], [a]], [[b], [c]], z).doit() == hyper([a, 1 - b], [c], z)\n    assert meijerg([[a], [b]], [[c], [d]], z).doit() == hyper([a, 1 - d], [1 - c], z)\n    assert meijerg([[a, b], [c, d]], [[e], [f]], z).doit() == \\\n        hyper([a, 1 - f], [1 - e], z)*hyper([b, 1 - f], [c], z)/hyper([1 - e], [d], z)\n    assert meijerg([[a, b], [c]], [[d], [e]], z).doit() == \\\n        hyper([a, 1 - e], [1 - d], z)*hyper([b, 1 - e], [c], z)/hyper([1 - d], [], z)\n    assert meijerg([[a, b], []], [[c, d], [e]], z).doit() == \\\n        hyper([a, 1 - e], [1 - c], z)*hyper([b, 1 - e], [1 - d], z)/hyper([1 - c], [1 - d], z)\n    assert meijerg([[a], [b, c]], [[d], [e]], z).doit() == \\\n        hyper([a, 1 - e], [1 - d], z)*hyper([b, 1 - e], [1 - d], z)/hyper([1 - d], [c], z)\n    assert meijerg([[a], [b]], [[c, d], []], z).doit() == \\\n        hyper([a, 1], [1 - c], z)*hyper([b, 1], [1 - d], z)/hyper([1], [], z)\n    assert meijerg([[a, b], [c, d]], [[e, f], []], z).doit() == \\\n        hyper([a, 1", "def test_meijerg_simplify():\n    from sympy import sqrt, sin, cos, tan, hyper, asin, symbols\n    from sympy.abc import z\n    a, b, c, d = symbols('a b c d')\n    assert meijerg((1/2,), [], [0,], [], z**2)._eval_simplify() == sqrt(z)*sin(z)/sqrt(pi)\n    assert meijerg((1/2,), [], [0, -1/2], [], z**2)._eval_simplify() == cos(z)/sqrt(pi)\n    assert meijerg((1/2,), [], [0, 1/2], [], z**2)._eval_simplify() == sin(z)/sqrt(pi)\n    assert meijerg((3/2,), [], [1,], [], z**2)._eval_simplify() == -z*sqrt(z)*sin(z)/(2*sqrt(pi))\n    assert meijerg((3/2,), [], [1, 1/2], [], z**2)._eval_simplify() == -z*sqrt(z)*cos(z)/(2*sqrt(pi))\n    assert meijerg((3/2,), [], [1, -1/2], [], z**2)._eval_simplify() == -z*sqrt(z)*sin(z)/(2*sqrt(pi))\n    assert meijerg((3/2,), [], [0, 3/2], [], z**2)._eval_simplify() == z**2*tan(z)/pi\n    assert meijerg((a, a - 1/2), (2*a,), [], z)._eval_simplify() == 2**(2*a - 1)*(1 + sqrt(1 - z))**(1 - 2*a)\n    assert meijerg([1, 1], [], [1], [], -z)._eval_simplify() == hyper((1, 1), (2,), z)\n    assert meijerg([1, 1], [], [], [1], z)._eval_simplify() == hyper((1, 1), (1,), z)\n    assert meijerg([1, 1], [], [1, 1/2], [], z)._eval_simplify() == z*hyper((1, 1), (2, 3/2), z)\n    assert meijerg([1, 1], [], [1, 3/2],"], "sample_428": ["def test_invalid_input(self):\n    with self.assertRaises(TypeError):\n        nformat(object(), \".\")\n\n    with self.assertRaises(ValueError):\n        nformat(1234, \"\", decimal_pos=2)\n\n    with self.assertRaises(ValueError):\n        nformat(1234, \".\", decimal_pos=-1)\n\n    with self.assertRaises(ValueError):\n        nformat(1234, \".\", grouping=-1, thousand_sep=\",\")\n\n    with self.assertRaises(ValueError):\n        nformat(1234, \".\", grouping=(3, -1), thousand_sep=\",\")\n", "def test_use_l10n_setting(self):\n    with self.settings(USE_L10N=True, USE_THOUSAND_SEPARATOR=True):\n        self.assertEqual(nformat(1234, \".\", grouping=3, thousand_sep=\",\"), \"1,234\")\n        self.assertEqual(nformat(1234, \".\", grouping=3, thousand_sep=\",\", use_l10n=False), \"1234\")\n\n    with self.settings(USE_L10N=False, USE_THOUSAND_SEPARATOR=True):\n        self.assertEqual(nformat(1234, \".\", grouping=3, thousand_sep=\",\"), \"1234\")\n        self.assertEqual(nformat(1234, \".\", grouping=3, thousand_sep=\",\", use_l10n=True), \"1,234\")\n\n    with self.settings(USE_L10N=True, USE_THOUSAND_SEPARATOR=False):\n        self.assertEqual(nformat(1234, \".\", grouping=3, thousand_sep=\",\"), \"1234\")\n        self.assertEqual(nformat(1234, \".\", grouping=3, thousand_sep=\",\", force_grouping=True), \"1,234\")\n", "    def test_edge_cases(self):\n        self.assertEqual(nformat(float('inf'), \".\"), \"inf\")\n        self.assertEqual(nformat(float('-inf'), \".\"), \"-inf\")\n        self.assertEqual(nformat(float('nan'), \".\"), \"nan\")\n        self.assertEqual(nformat(Decimal('inf'), \".\"), \"inf\")\n        self.assertEqual(nformat(Decimal('-inf'), \".\"), \"-inf\")\n        self.assertEqual(nformat(Decimal('nan'), \".\"), \"nan\")\n        self.assertEqual(nformat(\"inf\", \".\"), \"inf\")\n        self.assertEqual(nformat(\"-inf\", \".\"), \"-inf\")\n        self.assertEqual(nformat(\"nan\", \".\"), \"nan\")\n        self.assertEqual(nformat(\"1234.56789\", \".\", decimal_pos=10), \"1234.5678900000\")\n        self.assertEqual(nformat(1234.56789, \".\", decimal_pos=10), \"1234.5678900000\")\n        self.assertEqual(nformat(Decimal(\"1234.56789\"), \".\", decimal_pos=10), \"1234.5678900000\")\n", "def test_invalid_input(self):\n    with self.assertRaises(TypeError):\n        nformat(object(), \".\")\n\n    with self.assertRaises(TypeError):\n        nformat(None, \".\")\n\n    with self.assertRaises(TypeError):\n        nformat(\"\", \".\", grouping=\"abc\")\n\n    with self.assertRaises(ValueError):\n        nformat(1234, \".\", decimal_pos=-1)\n\n    with self.assertRaises(ValueError):\n        nformat(1234, \".\", grouping=-1)\n\n    with self.assertRaises(ValueError):\n        nformat(1234, \".\", thousand_sep=\"\")\n\n    with self.assertRaises(ValueError):\n        nformat(1234, \"\", decimal_pos=2)\n", "def test_edge_cases(self):\n    # Test case where decimal_pos is greater than the number of decimal places\n    self.assertEqual(nformat(123.45, \".\", decimal_pos=5), \"123.45000\")\n    self.assertEqual(nformat(Decimal(\"123.45\"), \".\", decimal_pos=5), \"123.45000\")\n    # Test case where decimal_pos is less than the number of decimal places\n    self.assertEqual(nformat(123.456, \".\", decimal_pos=2), \"123.46\")\n    self.assertEqual(nformat(Decimal(\"123.456\"), \".\", decimal_pos=2), \"123.46\")\n    # Test case where decimal_pos is 0 and number has decimal places\n    self.assertEqual(nformat(123.45, \".\", decimal_pos=0), \"123\")\n    self.assertEqual(nformat(Decimal(\"123.45\"), \".\", decimal_pos=0), \"123\")\n    # Test case where number is an integer and decimal_pos is 0\n    self.assertEqual(nformat(123, \".\", decimal_pos=0), \"123\")\n    self.assertEqual(nformat(Decimal(\"123\"), \".\", decimal_pos=0), \"123\")\n    # Test case where number is a float with no fractional part\n    self.assertEqual(nformat(123.0, \".\", decimal_pos=None), \"123.0\")\n    self.assertEqual(nformat(Decimal(\"123.0\"), \".\", decimal_pos=None), \"123.0\")\n    # Test case where number is a float with a very small fractional part\n    self.assertEqual(nformat(123.0 + 1e-100, \".\", decimal_pos=None), \"123.0\")\n    self.assertEqual(nformat(Decimal(\"123.0\") + Decimal(\"1e-100\"), \".\", decimal_pos=None), \"123.0\")\n"], "sample_1209": ["def test_prefix_latex():\n    assert kilo._latex(None) == r'\\text{k}'\n    assert micro._latex(None) == r'\\mu'\n", "def test_prefix_latex():\n    assert kibi._latex(None) == r'\\text{Ki}'\n    assert micro._latex(None) == r'\\mu'\n    assert PREFIXES['m']._latex(None) == r'\\text{m}'\n", "def test_latex_repr():\n    assert kilo._latex(None) == r'\\text{k}'\n    assert micro._latex(None) == r\"\\mu\"\n    assert repr(kilo._latex(None)) == repr(r'\\text{k}')\n    assert repr(micro._latex(None)) == repr(r\"\\mu\")\n", "def test_prefix_latex_repr():\n    m = PREFIXES['m']\n    mu = PREFIXES['mu']\n\n    assert m._latex(None) == r'\\text{m}'\n    assert mu._latex(None) == r\"\\mu\"\n\n    assert str(mu) == \"mu\"\n\n    assert repr(m) == \"Prefix('milli', 'm', -3)\"\n    assert repr(mu) == \"Prefix('micro', 'mu', -6, base=10)\"\n", "def test_prefix_latex_repr():\n    assert kilo._latex(None) == r'\\text{k}'\n    assert micro._latex(None) == r'\\mu'\n"], "sample_661": ["def test_record_testsuite_property_with_invalid_name_type(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            record_testsuite_property(1, \"all good\")\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--junitxml=tests.xml\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\"*TypeError: name parameter needs to be a string, but int given\"]\n    )\n", "def test_record_property_empty_string(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            record_property(\"\", 1)\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    node = dom.find_first_by_tag(\"testsuite\")\n    tnode = node.find_first_by_tag(\"testcase\")\n    psnode = tnode.find_first_by_tag(\"properties\")\n    pnodes = psnode.find_by_tag(\"property\")\n    pnodes[0].assert_attr(name=\"\", value=\"1\")\n", "def test_record_testsuite_property_multiple_times(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            record_testsuite_property(\"stats\", \"all good\")\n            record_testsuite_property(\"stats\", 10)\n            record_testsuite_property(\"stats\", \"20\")\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testsuite\")\n    properties_node = node.find_first_by_tag(\"properties\")\n    p_nodes = properties_node.find_by_tag(\"property\")\n    assert len(p_nodes) == 3\n    p_nodes[0].assert_attr(name=\"stats\", value=\"all good\")\n    p_nodes[1].assert_attr(name=\"stats\", value=\"10\")\n    p_nodes[2].assert_attr(name=\"stats\", value=\"20\")\n", "def test_record_testsuite_property_overwrite(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            record_testsuite_property(\"stats\", \"first\")\n            record_testsuite_property(\"stats\", \"second\")\n\n            record_testsuite_property(\"stats\", \"first\")\n            record_testsuite_property(\"stats\", \"second\")\n            record_testsuite_property(\"stats\", \"third\")\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testsuite\")\n    properties_node = node.find_first_by_tag(\"properties\")\n    p1_node = properties_node.find_nth_by_tag(\"property\", 0)\n    p2_node = properties_node.find_nth_by_tag(\"property\", 1)\n    p3_node = properties_node.find_nth_by_tag(\"property\", 2)\n    p1_node.assert_attr(name=\"stats\", value=\"first\")\n    p2_node.assert_attr(name=\"stats\", value=\"second\")\n    p3_node.assert_attr(name=\"stats\", value=\"third\")\n", "def test_classname_with_junit_prefix_and_double_colon(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        class MyClass:\n                pass\n    \"\"\"\n    )\n    result, dom = runandparse(testdir, \"--junitprefix=xyz\")\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testsuite\")\n    node.assert_attr(tests=1)\n    tnode = node.find_first_by_tag(\"testcase\")\n    tnode.assert_attr(\n        classname=\"xyz.test_classname_with_junit_prefix_and_double_colon.MyClass\", name=\"test_method\"\n    )\n"], "sample_862": ["def test_vectorizer_pickling_after_partial_fit():\n    # Test that a vectorizer can be pickled after a partial_fit\n    vect = HashingVectorizer()\n    vect.partial_fit(JUNK_FOOD_DOCS)\n    pickled_vect = pickle.dumps(vect)\n    unpickled_vect = pickle.loads(pickled_vect)\n    assert_array_equal(vect.transform(JUNK_FOOD_DOCS).toarray(),\n                       unpickled_vect.transform(JUNK_FOOD_DOCS).toarray())\n", "def test_vectorizer_warning_on_invalid_input():\n    # test for warning with invalid input\n    vect = CountVectorizer()\n    with pytest.warns(Warning):\n        vect.fit_transform([1, 2, 3])\n    with pytest.warns(Warning):\n        vect.fit(1)\n    with pytest.warns(Warning):\n        vect.transform([1, 2, 3])\n", "def test_hashing_vectorizer_dtypes():\n    data = [\"This is a test\", \"This is another test\"]\n\n        X = vectorizer.fit_transform(data)\n        assert X.dtype == vectorizer.dtype\n\n    # float64 is the default dtype\n    vectorizer = HashingVectorizer()\n    _check_dtypes(vectorizer)\n\n    for dtype in [np.float32, np.float64]:\n        vectorizer = HashingVectorizer(dtype=dtype)\n        _check_dtypes(vectorizer)\n\n    for dtype in [np.int8, np.int16, np.int32, np.int64]:\n        vectorizer = HashingVectorizer(dtype=dtype)\n        with pytest.raises(ValueError):\n            _check_dtypes(vectorizer)\n", "def test_stop_words_with_analyzer():\n    # Test that stop words are ignored when using a custom analyzer\n    vect = CountVectorizer(analyzer=lambda x: x.split(), stop_words=[\"you've\"])\n    X = vect.fit_transform([\"hello you've world\"])\n    assert \"you've\" not in vect.vocabulary_\n\n    # Test that stop words are not ignored when using a custom analyzer\n    # that returns a single token\n    vect = CountVectorizer(analyzer=lambda x: [\"custom_token\"],\n                           stop_words=[\"you've\"])\n    X = vect.fit_transform([\"hello you've world\"])\n    assert \"custom_token\" in vect.vocabulary_\n", "def test_countvectorizer_fit_transform_empty_docs():\n    # Regression test for a bug reported on the mailing list\n    # https://mail.python.org/pipermail/scikit-learn/2015-May/000889.html\n    cv = CountVectorizer()\n    docs = [\"\", \"\", \"\"]\n    X = cv.fit_transform(docs)\n    assert X.shape == (3, 0)\n"], "sample_33": ["def test_strip_accents():\n    assert misc.strip_accents('\u00e5ngstr\u00f6m') == 'angstrom'\n    assert misc.strip_accents('Ren\u00e9') == 'Rene'\n    assert misc.strip_accents('caf\u00e9') == 'cafe'\n", "def test_did_you_mean():\n    candidates = ['apple', 'banana', 'cherry']\n    assert misc.did_you_mean('appel', candidates) == \"Did you mean apple?\"\n    assert misc.did_you_mean('appel', candidates, n=2) == \"Did you mean apple or banana?\"\n    assert misc.did_you_mean('appel', candidates, cutoff=0.7) == ''\n    assert misc.did_you_mean('appel', candidates, fix=lambda s: [s, s.upper()]) == \"Did you mean apple or apple?\"\n    assert misc.did_you_mean('grape', candidates) == ''\n", "def test_walk_skip_hidden_edge_cases():\n    # Test edge cases for walk_skip_hidden\n    for root, dirs, files in misc.walk_skip_hidden('.'):\n        assert len(files) == 0\n        break  # Only need to check the current dir\n\n    # Test that we don't skip non-hidden directories\n    for root, dirs, files in misc.walk_skip_hidden('.'):\n        assert 'tests' in dirs\n\n    # Test that we do skip hidden directories\n    for root, dirs, files in misc.walk_skip_hidden('.'):\n        assert '.tox' not in dirs\n", "def test_ordered_descriptor():\n    class ExampleDecorator(misc.OrderedDescriptor):\n        _class_attribute_ = '_examples_'\n\n    class TestClass(metaclass=misc.OrderedDescriptorContainer):\n        a = ExampleDecorator()\n        b = ExampleDecorator()\n\n    assert hasattr(TestClass, '_examples_')\n    assert isinstance(TestClass._examples_, dict)\n    assert len(TestClass._examples_) == 2\n    assert TestClass._examples_['a'].__class__ is ExampleDecorator\n    assert TestClass._examples_['b'].__class__ is ExampleDecorator\n", "def test_shapedlikendarray():\n    class Example(misc.ShapedLikeNDArray):\n            self.data = np.array(data)\n\n        @property\n            return self.data.shape\n\n            new_data = getattr(self.data, method)(*args, **kwargs)\n            return type(self)(new_data)\n\n    ex = Example(np.arange(6).reshape(2, 3))\n\n    assert ex.shape == (2, 3)\n    assert ex.ndim == 2\n    assert ex.size == 6\n    assert not ex.isscalar\n    assert len(ex) == 2\n\n    assert ex[0].shape == (3,)\n    assert ex[0, 1] == 1\n\n    for i, row in enumerate(ex):\n        assert row.shape == (3,)\n        for j, val in enumerate(row):\n            assert val == i * 3 + j\n\n    ex2 = ex.copy()\n    assert np.array_equal(ex2.data, ex.data)\n    assert not np.may_share_memory(ex2.data, ex.data)\n\n    ex3 = ex.reshape((3, 2))\n    assert ex3.shape == (3, 2)\n\n    ex4 = ex.ravel()\n    assert ex4.shape == (6,)\n\n    ex5 = ex.flatten()\n    assert ex5.shape == (6,)\n\n    ex6 = ex.transpose()\n    assert ex6.shape == (3, 2)\n\n    ex7 = ex.T\n    assert ex7.shape == (3, 2)\n\n    ex8 = ex.swapaxes(0, 1)\n    assert ex8.shape == (3, 2)\n\n    ex9 = ex.diagonal()\n    assert ex9.shape == (3,)\n\n    ex10 = ex.squeeze()\n    assert ex10.shape == (2, 3)\n\n    ex11 = ex.take([0, 1], axis=1)\n    assert ex11.shape == (2, 2)\n"], "sample_489": ["def test_update_conflicts_update_fields_db_column_with_target(self):\n    FieldsWithDbColumns.objects.bulk_create(\n        [\n            FieldsWithDbColumns(rank=1, name=\"a\"),\n            FieldsWithDbColumns(rank=2, name=\"b\"),\n        ]\n    )\n    self.assertEqual(FieldsWithDbColumns.objects.count(), 2)\n\n    conflicting_objects = [\n        FieldsWithDbColumns(rank=1, name=\"c\"),\n        FieldsWithDbColumns(rank=2, name=\"d\"),\n    ]\n    results = FieldsWithDbColumns.objects.bulk_create(\n        conflicting_objects,\n        update_conflicts=True,\n        unique_fields=[\"rank\"],\n        update_fields=[\"name_with_db_column\"],\n    )\n    self.assertEqual(len(results), len(conflicting_objects))\n    if connection.features.can_return_rows_from_bulk_insert:\n        for instance in results:\n            self.assertIsNotNone(instance.pk)\n    self.assertEqual(FieldsWithDbColumns.objects.count(), 2)\n    self.assertCountEqual(\n        FieldsWithDbColumns.objects.values(\"rank\", \"name\"),\n        [\n            {\"rank\": 1, \"name\": \"c\"},\n            {\"rank\": 2, \"name\": \"d\"},\n        ],\n    )\n", "    def test_update_conflicts_update_fields_as_model_fields(self):\n        UpsertConflict.objects.bulk_create(\n            [\n                UpsertConflict(number=1, rank=1, name=\"John\"),\n                UpsertConflict(number=2, rank=2, name=\"Mary\"),\n                UpsertConflict(number=3, rank=3, name=\"Hannah\"),\n            ]\n        )\n        self.assertEqual(UpsertConflict.objects.count(), 3)\n\n        conflicting_objects = [\n            UpsertConflict(number=1, rank=4, name=\"Steve\"),\n            UpsertConflict(number=2, rank=2, name=\"Olivia\"),\n            UpsertConflict(number=3, rank=1, name=\"Hannah\"),\n        ]\n        results = UpsertConflict.objects.bulk_create(\n            conflicting_objects,\n            update_conflicts=True,\n            update_fields=[\"number\", \"rank\"],\n            unique_fields=[\"number\"],\n        )\n        self.assertEqual(len(results), len(conflicting_objects))\n        if connection.features.can_return_rows_from_bulk_insert:\n            for instance in results:\n                self.assertIsNotNone(instance.pk)\n        self.assertEqual(UpsertConflict.objects.count(), 3)\n        self.assertCountEqual(\n            UpsertConflict.objects.values(\"number\", \"rank\", \"name\"),\n            [\n                {\"number\": 1, \"rank\": 4, \"name\": \"Steve\"},\n                {\"number\": 2, \"rank\": 2, \"name\": \"Olivia\"},\n                {\"number\": 3, \"rank\": 1, \"name\": \"Hannah\"},\n            ],\n        )\n\n        results = UpsertConflict.objects.bulk_create(\n            conflicting_objects + [UpsertConflict(number=4, rank=4, name=\"Mark\")],\n            update_conflicts=True,\n            update_fields=[\"number\", \"rank\"],\n            unique_fields=[\"number\"],\n        )\n        self.assertEqual(len(results), 4)\n        if connection.features.can_return_rows_from_bulk_insert:\n            for instance in results:\n                self.assertIsNotNone(instance.pk)\n        self.assertEqual(UpsertConflict.objects.count(), 4)\n        self.assertCountEqual(\n            UpsertConflict.objects.values(\"number\", \"rank\", \"name\"),\n            [\n                {\"number\": 1, \"rank\": 4, \"name\": \"Steve\"},\n                {\"number\": 2, \"rank\": 2, \"name\": \"Olivia\"},\n                {\"number\": 3, \"rank\": 1, \"name\": \"Hannah\"},\n                {\"number\": 4, \"rank\":", "def test_bulk_create_large_batch_mixed_ordered_insertion(self):\n    \"\"\"\n    Test inserting a large batch with objects having primary key set\n    mixed together with objects without PK set in a specific order.\n    \"\"\"\n    existing = TwoFields.objects.create(id=500, f1=500, f2=500)\n    TwoFields.objects.bulk_create(\n        [\n            TwoFields(id=100, f1=100, f2=100),\n            TwoFields(id=None, f1=200, f2=200),\n            existing,\n            TwoFields(id=300, f1=300, f2=300),\n            TwoFields(id=None, f1=400, f2=400),\n        ]\n    )\n    self.assertEqual(TwoFields.objects.count(), 6)\n    self.assertEqual(TwoFields.objects.filter(id__in=[100, 300, 500]).count(), 3)\n    self.assertEqual(TwoFields.objects.exclude(id__in=[100, 300, 500]).count(), 3)\n", "def test_bulk_create_large_batch_efficiency_for_concrete_inheritance(self):\n    # Test that bulk_create() is efficient for a model with concrete\n    # inheritance.\n    objs = [\n        Pizzeria(name=\"Pizzeria %s\" % i, specialty=\"Specialty %s\" % i)\n        for i in range(0, 1000)\n    ]\n    with self.assertNumQueries(2):\n        Pizzeria.objects.bulk_create(objs)\n", "def test_bulk_create_expression_fields(self):\n    # Check that a bulk_create with an expression field doesn't attempt to assign the expression to the field.\n    obj1 = UpsertConflict(number=1, name=\"John\", rank=1)\n    obj2 = UpsertConflict(number=2, name=\"Mary\", rank=2)\n    # When using ignore_conflicts=True, any conflicts are silently ignored.\n    # These objects will be created in the database.\n    UpsertConflict.objects.bulk_create([obj1, obj2], ignore_conflicts=True)\n    obj3 = UpsertConflict(number=1, name=\"Olivia\", rank=3)\n    # Replace the rank attribute with a Value object that will be evaluated in the database.\n    obj3.rank = Value(obj3.rank)\n    # The actual object has the original value (3), but the database will see the evaluated value (4).\n    self.assertEqual(obj3.rank, 3)\n    # If we attempt to bulk create obj3, the database will see a conflict with obj1 and silently ignore it.\n    # As a result, obj3 will not be created in the database, and will still have the original value (3) for its rank.\n    UpsertConflict.objects.bulk_create([obj3], ignore_conflicts=True)\n    self.assertEqual(obj3.rank, 3)\n    self.assertEqual(UpsertConflict.objects.filter(number=1).first().rank, 1)\n    # But if we try to bulk create obj3 again without ignore_conflicts=True, an IntegrityError will be raised.\n    with self.assertRaises(IntegrityError):\n        UpsertConflict.objects.bulk_create([obj3])\n    # Now we create obj4, which has a rank that will conflict with obj2 when bulk created.\n    obj4 = UpsertConflict(number=2, name=\"Steve\", rank=4)\n    obj4.rank = Value(obj4.rank)\n    # Again, the actual object has the original value (4), but the database will see the evaluated value (5).\n    self.assertEqual(obj4.rank, 4)\n    # If we bulk create obj4 with ignore_conflicts=True, it will not be created in the database.\n    # As a result, it will still have the original value (4) for its rank.\n    UpsertConflict.objects.bulk_create([obj4], ignore_conflicts=True)\n    self.assertEqual(obj4.rank, 4)\n    self.assertEqual(UpsertConflict.objects.filter(number=2).first()."], "sample_98": ["    def test_broken_pipe_error(self):\n        conn = HTTPConnection(LiveServerViews.server_thread.host, LiveServerViews.server_thread.port, timeout=1)\n        try:\n            conn.connect()\n            conn.sock.settimeout(0.1)\n            conn.request('GET', '/example_view/')\n            conn.sock.close()\n            with self.assertRaises(HTTPError) as err:\n                conn.getresponse()\n            self.assertEqual(err.exception.code, 500)\n        finally:\n            conn.close()\n", "    def test_broken_pipe_logging(self):\n        \"\"\"Test that broken pipe errors are logged correctly.\"\"\"\n        conn = HTTPConnection(LiveServerViews.server_thread.host, LiveServerViews.server_thread.port)\n        try:\n            conn.request('GET', '/example_view/', headers={'Connection': 'keep-alive'})\n            response = conn.getresponse()\n            response.read(1)\n            conn.close()\n            # The connection is closed, so the next read will raise a BrokenPipeError\n            with self.assertRaises(BrokenPipeError):\n                response.read()\n        except Exception as e:\n            self.fail(f\"Expected BrokenPipeError, but got {e.__class__.__name__}\")\n        finally:\n            conn.close()\n", "    def test_server_handler_with_limited_stream(self):\n        \"\"\"\n        Test that ServerHandler uses a LimitedStream and that it correctly\n        ignores unread request data at the end of the request.\n        \"\"\"\n        conn = HTTPConnection(LiveServerViews.server_thread.host, LiveServerViews.server_thread.port)\n        try:\n            conn.request('POST', '/example_view/', b'{}extra data', headers={\"Content-Length\": 2})\n            response = conn.getresponse()\n            self.assertEqual(response.status, 200)\n            self.assertEqual(response.read(), b'example view')\n        finally:\n            conn.close()\n", "    def test_server_handler(self):\n        \"\"\"\n        Test the custom ServerHandler implementation.\n        \"\"\"\n        conn = HTTPConnection(LiveServerServerHandler.server_thread.host, LiveServerServerHandler.server_thread.port)\n        try:\n            # Test that the LimitedStream is used\n            conn.request('POST', '/example_view/', b'{\"key\": \"value\"}', headers={\"Content-Length\": \"16\"})\n            response = conn.getresponse()\n            self.assertEqual(response.status, 200)\n            self.assertEqual(response.read(), b'example view')\n\n            # Test that the connection is closed if the content length is unknown\n            conn.request('POST', '/example_view/', b'{\"key\": \"value\"}')\n            response = conn.getresponse()\n            self.assertEqual(response.status, 200)\n            self.assertEqual(response.read(), b'example view')\n            self.assertEqual(response.getheader('Connection'), 'close')\n        finally:\n            conn.close()\n", "    def test_wsgi_server_address_family(self):\n        \"\"\"\n        The WSGIServer should use the IPv6 address family when the ipv6 flag is set.\n        \"\"\"\n        server_thread = LiveServerThread()\n        server_thread.host = '::1'\n        server_thread.port = 0\n        server_thread.is_ready = lambda: True\n        server_thread.server_cls = lambda *args, **kwargs: WSGIServer(*args, ipv6=True, **kwargs)\n        server_thread.start()\n        self.addCleanup(server_thread.terminate)\n        self.assertEqual(server_thread.server.server_address[0], '::1')\n"], "sample_17": ["    def test_matmul(self):\n        q1 = np.arange(6.0).reshape(2, 3) * u.m\n        q2 = np.arange(3.0).reshape(3, 1) * u.s\n        out = np.matmul(q1, q2)\n        expected = np.matmul(q1.value, q2.value) * u.m * u.s\n        assert np.all(out == expected)\n\n        out2 = 0 * out\n        result = np.matmul(q1, q2, out=out2)\n        assert result is out2\n        assert np.all(out2 == out)\n\n        with pytest.raises(TypeError):\n            np.matmul(q1, q2, out=object())\n", "    def test_diagonal(self):\n        q = np.arange(9.0).reshape(3, 3) * u.m\n        out = np.diagonal(q)\n        expected = np.diagonal(q.value) * q.unit\n        assert np.all(out == expected)\n        out2 = np.diagonal(q, 1)\n        expected2 = np.diagonal(q.value, 1) * q.unit\n        assert np.all(out2 == expected2)\n", "    def setup_method(self):\n        self.q = np.array([[1, 2], [3, 4]]) * u.m\n", "    def test_drop_fields(self):\n        q = self.q_pv\n        qd = rfn.drop_fields(q, \"p\")\n        assert_array_equal(qd[\"v\"], q[\"v\"])\n        assert qd.unit == (q.unit[\"v\"],)\n", "    def test_repack_fields(self):\n        x = np.array([1, 2, 3]) * u.m\n        y = np.array([4, 5, 6]) * u.cm\n        z = np.array([7, 8, 9]) * u.km\n        a = rfn.repack_fields((x, y, z))\n        assert len(a.dtype) == 3\n        assert a.dtype.names == ('f0', 'f1', 'f2')\n        assert a.unit == (u.m, u.cm, u.km)\n        assert a.dtype['f0'].unit == u.m\n        assert a.dtype['f1'].unit == u.cm\n        assert a.dtype['f2'].unit == u.km\n"], "sample_307": ["def test_escaped_format_chars(self):\n    my_birthday = datetime(1979, 7, 8, 22, 00)\n    self.assertEqual(dateformat.format(my_birthday, r'\\Y'), 'Y')\n    self.assertEqual(dateformat.format(my_birthday, r'\\a'), 'a')\n    self.assertEqual(dateformat.format(my_birthday, r'\\\\Y'), '1979')\n    self.assertEqual(dateformat.format(my_birthday, r'\\\\a'), 'p.m.')\n", "    def test_timezone_offset(self):\n        tz = get_fixed_timezone(-210)\n        aware_dt = datetime(2009, 5, 16, 5, 30, 30, tzinfo=tz)\n        self.assertEqual(dateformat.format(aware_dt, 'O'), '-0330')\n        self.assertEqual(dateformat.format(aware_dt, 'Z'), '-21000')\n", "    def test_e_format_with_different_timezones(self):\n        if TZ_SUPPORT:\n            dt = datetime(2009, 5, 16, 5, 30, 30)\n            tz = get_fixed_timezone(-210)\n            aware_dt = make_aware(dt, tz)\n            self.assertEqual(dateformat.format(aware_dt, 'e'), '-0330')\n\n            tz = get_fixed_timezone(210)\n            aware_dt = make_aware(dt, tz)\n            self.assertEqual(dateformat.format(aware_dt, 'e'), '+0330')\n\n            tz = get_fixed_timezone(0)\n            aware_dt = make_aware(dt, tz)\n            self.assertEqual(dateformat.format(aware_dt, 'e'), '+0000')\n", "def test_escaping_format_characters(self):\n    my_birthday = datetime(1979, 7, 8, 22, 00)\n\n    self.assertEqual(dateformat.format(my_birthday, r'\\d'), 'd')\n    self.assertEqual(dateformat.format(my_birthday, r'\\a'), 'a')\n    self.assertEqual(dateformat.format(my_birthday, r'\\Y'), 'Y')\n    self.assertEqual(dateformat.format(my_birthday, r'\\j'), 'j')\n\n    # Test that escaped and non-escaped characters are treated differently\n    self.assertEqual(dateformat.format(my_birthday, r'\\jS \\o\\f F'), '8S of July')\n    self.assertEqual(dateformat.format(my_birthday, r'jS \\o\\f F'), '8th of July')\n", "def test_format_edge_cases(self):\n    # Test edge cases for various format specifiers\n\n    # Test 'a' and 'A' at midnight and noon\n    self.assertEqual(dateformat.format(datetime(2000, 1, 1, 0, 0), 'a'), 'a.m.')\n    self.assertEqual(dateformat.format(datetime(2000, 1, 1, 12, 0), 'a'), 'p.m.')\n    self.assertEqual(dateformat.format(datetime(2000, 1, 1, 0, 0), 'A'), 'AM')\n    self.assertEqual(dateformat.format(datetime(2000, 1, 1, 12, 0), 'A'), 'PM')\n\n    # Test 'P' at midnight and noon\n    self.assertEqual(dateformat.format(datetime(2000, 1, 1, 0, 0), 'P'), 'midnight')\n    self.assertEqual(dateformat.format(datetime(2000, 1, 1, 12, 0), 'P'), 'noon')\n\n    # Test 'f' with zero minutes\n    self.assertEqual(dateformat.format(datetime(2000, 1, 1, 10, 0), 'f'), '10')\n\n    # Test 'S' with day of month in [11, 12, 13]\n    self.assertEqual(dateformat.format(datetime(2000, 1, 11), 'S'), 'th')\n    self.assertEqual(dateformat.format(datetime(2000, 1, 12), 'S'), 'th')\n    self.assertEqual(dateformat.format(datetime(2000, 1, 13), 'S'), 'th')\n\n    # Test 'w' and 'W' with different days of the week\n    self.assertEqual(dateformat.format(datetime(2000, 1, 1), 'w'), '6')\n    self.assertEqual(dateformat.format(datetime(2000, 1, 2), 'w'), '0')\n    self.assertEqual(dateformat.format(datetime(2000, 1, 1), 'W'), '1')\n    self.assertEqual(dateformat.format(datetime(2000, 1, 2), 'W'), '1')\n"], "sample_292": ["def test_process_view_token_too_short(self):\n    \"\"\"\n    If the token is shorter than expected, it is ignored and a new token is\n    created.\n    \"\"\"\n    req = self._get_GET_no_csrf_cookie_request()\n    req.COOKIES[settings.CSRF_COOKIE_NAME] = 'x' * (CSRF_TOKEN_LENGTH - 1)\n    mw = CsrfViewMiddleware(token_view)\n    mw.process_view(req, token_view, (), {})\n    resp = mw(req)\n    csrf_cookie = resp.cookies.get(settings.CSRF_COOKIE_NAME, False)\n    self.assertEqual(len(csrf_cookie.value), CSRF_TOKEN_LENGTH)\n", "def test_rotate_token(self):\n    \"\"\"\n    Test that the CSRF token is rotated on login.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    mw = CsrfViewMiddleware(post_form_view)\n    mw.process_request(req)\n    old_token = req.META['CSRF_COOKIE']\n    rotate_token(req)\n    mw.process_view(req, post_form_view, (), {})\n    mw(req)\n    new_token = req.META['CSRF_COOKIE']\n    self.assertNotEqual(old_token, new_token)\n    self.assertTrue(equivalent_tokens(old_token, new_token))\n", "def test_rotate_token(self):\n    \"\"\"\n    Test that the CSRF token is rotated on login.\n    \"\"\"\n    req = self._get_GET_csrf_cookie_request()\n    mw = CsrfViewMiddleware(post_form_view)\n    mw.process_request(req)\n    mw.process_view(req, post_form_view, (), {})\n    orig_token = req.META['CSRF_COOKIE']\n    mw = CsrfViewMiddleware(post_form_view)\n    mw.process_request(req)\n    mw.process_view(req, rotate_token, (), {})\n    mw(req)\n    new_token = req.META['CSRF_COOKIE']\n    self.assertNotEqual(orig_token, new_token)\n", "def test_origin_verified_ignores_port(self):\n    \"\"\"\n    CsrfViewMiddleware._origin_verified() ignores the port in the Origin\n    header.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req._is_secure_override = True\n    req.META['HTTP_HOST'] = 'www.example.com'\n    req.META['HTTP_ORIGIN'] = 'https://www.example.com:8080'\n    mw = CsrfViewMiddleware(post_form_view)\n    self.assertIs(mw._origin_verified(req), True)\n", "def test_custom_csrf_header_name(self):\n    \"\"\"\n    Test that a custom CSRF header name is properly checked.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req.META['HTTP_HOST'] = 'www.example.com'\n    req.META['HTTP_ORIGIN'] = 'https://dashboard.example.com'\n    req.META['HTTP_X_CSRFTOKEN_CUSTOMIZED'] = self._csrf_id\n    mw = CsrfViewMiddleware(post_form_view)\n    mw.process_request(req)\n    response = mw.process_view(req, post_form_view, (), {})\n    self.assertIsNone(response)\n"], "sample_517": ["def test_transform_rotates_text_updates():\n    ax = plt.gca()\n    transform = mtransforms.Affine2D().rotate_deg(30)\n    text = ax.text(0, 0, 'test', transform=transform,\n                   transform_rotates_text=True)\n    result = text.get_rotation()\n    assert_almost_equal(result, 30)\n\n    text.set_transform_rotates_text(False)\n    result = text.get_rotation()\n    assert_almost_equal(result, 0)\n", "def test_rotation_mode(fig_test, rotation_mode, expected_rotation):\n    ax = fig_test.add_subplot()\n    transform = mtransforms.Affine2D().rotate_deg(30)\n    text = ax.text(0, 0, 'test', transform=transform,\n                   transform_rotates_text=True, rotation_mode=rotation_mode)\n    assert_almost_equal(text.get_rotation(), expected_rotation)\n", "def test_bbox_rotation():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, \"bbox rotation\",\n                   ha='center', va='center', rotation=30)\n    text.set_bbox(dict(facecolor='red', edgecolor='black'))\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_aspect('equal')\n", "def test_bbox_fontsize():\n    fig, ax = plt.subplots()\n\n    # test a regular bbox\n    ann = ax.annotate('test', xy=(0, 0), xytext=(0, 0), textcoords='figure pixels')\n    fig.canvas.draw()\n\n    init_pos = ann.get_window_extent(fig.canvas.renderer)\n    ann.set_bbox(dict(facecolor='red', alpha=0.5))\n    fig.canvas.draw()\n    post_pos = ann.get_window_extent(fig.canvas.renderer)\n\n    assert_almost_equal(init_pos.intervalx, post_pos.intervalx)\n    assert_almost_equal(init_pos.intervaly, post_pos.intervaly)\n\n    # test a bbox with a fontsize\n    ann = ax.annotate('test', xy=(0, 0), xytext=(0, 0), textcoords='figure pixels')\n    ann.set_fontsize(30)\n    fig.canvas.draw()\n\n    init_pos = ann.get_window_extent(fig.canvas.renderer)\n    ann.set_bbox(dict(facecolor='red', alpha=0.5))\n    fig.canvas.draw()\n    post_pos = ann.get_window_extent(fig.canvas.renderer)\n\n    assert_almost_equal(init_pos.intervalx, post_pos.intervalx)\n    assert_almost_equal(init_pos.intervaly, post_pos.intervaly)\n", "def test_offset_from():\n    fig, ax = plt.subplots()\n\n    artist = ax.text(0.5, 0.5, \"Test\")\n    fig.canvas.draw()\n    renderer = fig.canvas.get_renderer()\n    of = OffsetFrom(artist, (0.5, 0.5))\n\n    assert isinstance(of(renderer), mtransforms.Transform)\n\n    with pytest.raises(ValueError, match=\"Unknown type\"):\n        OffsetFrom(\"invalid type\", (0, 0))\n\n    of = OffsetFrom((0, 0), (0.5, 0.5))\n    with pytest.raises(ValueError, match=\"Unknown return type\"):\n        of(renderer)\n\n    of = OffsetFrom(ax, (0.5, 0.5), unit=\"invalid\")\n    with pytest.raises(ValueError, match=\"Unknown unit\"):\n        of(renderer)\n"], "sample_143": ["def test_camel_case_to_spaces(self):\n    self.assertEqual(text.camel_case_to_spaces('helloWorld'), 'hello world')\n    self.assertEqual(text.camel_case_to_spaces('helloJSONWorld'), 'hello json world')\n    self.assertEqual(text.camel_case_to_spaces('HelloWorld'), 'hello world')\n    self.assertEqual(text.camel_case_to_spaces('hello'), 'hello')\n    self.assertEqual(text.camel_case_to_spaces(''), '')\n    self.assertEqual(text.camel_case_to_spaces(lazystr('helloWorld')), 'hello world')\n", "def test_camel_case_to_spaces(self):\n    self.assertEqual(text.camel_case_to_spaces('HelloWorld'), 'hello world')\n    self.assertEqual(text.camel_case_to_spaces('Hello'), 'hello')\n    self.assertEqual(text.camel_case_to_spaces('hello'), 'hello')\n    self.assertEqual(text.camel_case_to_spaces('helloWorld'), 'hello world')\n    self.assertEqual(text.camel_case_to_spaces(''), '')\n    self.assertEqual(text.camel_case_to_spaces('helloWorldFoo'), 'hello world foo')\n    self.assertEqual(text.camel_case_to_spaces(lazystr('HelloWorld')), 'hello world')\n", "    def test_camel_case_to_spaces(self):\n        items = [\n            ('CamelCase', 'camel case'),\n            ('camelCase', 'camel case'),\n            ('Camel', 'camel'),\n            ('multipleCamelCaseStrings', 'multiple camel case strings'),\n            ('CamelCaseStringWithNumbers123', 'camel case string with numbers123'),\n            ('', ''),\n        ]\n        for value, output in items:\n            self.assertEqual(text.camel_case_to_spaces(value), output)\n            self.assertEqual(text.camel_case_to_spaces(lazystr(value)), output)\n", "def test_camel_case_to_spaces(self):\n    items = [\n        ('helloWorld', 'hello world'),\n        ('hello_World', 'hello world'),\n        ('hello-world', 'hello-world'),  # Test non-alphanumeric character\n        ('helloWorldFooBar', 'hello world foo bar'),\n        ('hello', 'hello'),  # Test single word\n        ('helloWorld123', 'hello world123'),  # Test numbers\n        ('HelloWorld', 'hello world'),  # Test capital first letter\n    ]\n    for value, output in items:\n        self.assertEqual(text.camel_case_to_spaces(value), output)\n        self.assertEqual(text.camel_case_to_spaces(lazystr(value)), output)\n", "def test_camel_case_to_spaces(self):\n    test_data = [\n        ('helloWorld', 'hello world'),\n        ('hello-world', 'hello-world'),  # Should not be split\n        ('helloWorldFooBar', 'hello world foo bar'),\n        ('hello_World', 'hello_world'),  # Should not be split\n        ('hello_WorldFooBar', 'hello_world foo bar'),  # Should not be split\n        ('_helloWorld', '_hello world'),  # Should not strip leading/trailing underscores\n    ]\n    for input_str, expected_output in test_data:\n        self.assertEqual(text.camel_case_to_spaces(input_str), expected_output)\n"], "sample_24": ["    def setup_class(self):\n        self.a = np.array([1 + 2j, 3 + 4j, 5 + 6j])\n        self.mask_a = np.array([True, False, True])\n        self.ma = Masked(self.a, mask=self.mask_a)\n", "    def setup_class(self):\n        self.a = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])\n        self.mask_a = np.array([False, True, False, True, False, False])\n        self.ma = Masked(self.a, mask=self.mask_a)\n", "    def setup_class(self):\n        self.a = np.arange(9).reshape(3, 3)\n        self.mask_a = np.zeros(self.a.shape, dtype=bool)\n        self.ma = Masked(self.a, mask=self.mask_a)\n        self.b = np.arange(3)\n        self.mask_b = np.array([True, False, False])\n        self.mb = Masked(self.b, mask=self.mask_b)\n", "    def setup_class(self):\n        self.a = np.arange(10, dtype=[(\"a\", \"i4\"), (\"b\", \"f4\")])\n        self.mask_a = np.array([True, False, False, True, False, False, False, False, False, False])\n        self.ma = Masked(self.a, mask=self.mask_a)\n", "    def test_inv(self):\n        ma = Masked(np.array([[1, 2], [3, 4]]), mask=[[False, True], [False, False]])\n        with pytest.raises(LinearAlgebraError):\n            np.linalg.inv(ma)\n"], "sample_868": ["def test_empty_input(metric_name):\n    metric = SUPERVISED_METRICS[metric_name]\n    with pytest.raises(ValueError, match='n_samples must be'):\n        metric([], [])\n", "def test_empty_input(metric_name):\n    metric = SUPERVISED_METRICS[metric_name]\n    with pytest.raises(ValueError, match='input must be 1D'):\n        metric([], [])\n", "def test_average_methods(metric_name, y1, y2, average_method):\n    metric = SUPERVISED_METRICS[metric_name]\n    score = metric(y1, y2, average_method=average_method)\n    assert score >= 0.0\n\n", "def test_average_methods(metric_name, average_method):\n    metric = SUPERVISED_METRICS[metric_name]\n    y_true = np.array([0, 0, 0, 0, 1, 1, 1, 1])\n    y_pred = np.array([0, 1, 2, 3, 4, 5, 6, 7])\n    # Ensure the average_method is valid\n    if metric_name == \"adjusted_mutual_info_score\":\n        score = metric(y_true, y_pred, average_method=average_method)\n    elif metric_name == \"normalized_mutual_info_score\":\n        score = metric(y_true, y_pred, average_method=average_method)\n    # Ensure the score is not nan\n    assert not np.isnan(score)\n", "def test_empty_input(metric_name):\n    metric = SUPERVISED_METRICS[metric_name]\n    with pytest.raises(ValueError, match=\"labels_true must be 1D\"):\n        metric([], [0, 1])\n    with pytest.raises(ValueError, match=\"labels_pred must be 1D\"):\n        metric([0, 1], [])\n    with pytest.raises(ValueError, match=\"Input arrays should have the same \"\n                         \"number of samples/rows\"):\n        metric([0, 1], [0, 1, 2])\n"], "sample_476": ["    def test_save_new_file(self):\n        \"\"\"\n        Test saving a new file to an ImageField.\n        \"\"\"\n        p = self.PersonModel(name=\"Joe\")\n        p.mugshot.save(\"mug\", self.file1)\n        self.check_dimensions(p, 4, 8)\n        self.assertEqual(p.mugshot.name, \"mug\")\n", "    def test_check_primary_key(self):\n        \"\"\"\n        primary_key is not a valid argument for an ImageField.\n        \"\"\"\n        with self.assertRaisesMessage(\n            checks.Error,\n            \"'primary_key' is not a valid argument for a ImageField.\",\n        ):\n            ImageField(primary_key=True)\n", "    def test_storage(self):\n        \"\"\"\n        Test that different storage backends can be used with ImageField.\n        \"\"\"\n        p = self.PersonModel(name=\"Joe\")\n        p.mugshot.save(\"mug\", self.file1)\n\n        # Test default storage\n        self.assertEqual(p.mugshot.storage, default_storage)\n\n        # Test custom storage\n        from django.core.files.storage import DefaultStorage\n        storage = DefaultStorage()\n        p.mugshot.storage = storage\n        p.mugshot.save(\"mug\", self.file1)\n        self.assertEqual(p.mugshot.storage, storage)\n", "    def test_upload_to_absolute_path(self):\n        \"\"\"\n        Test that absolute upload_to path is not allowed.\n        \"\"\"\n        with self.assertRaisesMessage(\n            checks.Error,\n            \"%s's 'upload_to' argument must be a relative path, not an absolute path.\" % ImageField.__name__,\n        ):\n            ImageField(upload_to=\"/path/to/upload\")\n", "    def test_save_form_data_empty_file(self):\n        \"\"\"\n        Tests that saving a File instance with an empty file.\n        \"\"\"\n        p = self.PersonModel(name=\"Joe\")\n        file = self.File(open(os.path.join(os.path.dirname(__file__), \"empty.txt\"), \"rb\"), name=\"empty.txt\")\n        self.PersonModel.save_form_data(p, file)\n        self.assertEqual(p.mugshot.name, \"empty.txt\")\n"], "sample_1085": ["compilation error", "def test_issue_15520():\n    assert same_and_same_prec(Float('0.12345678901234567890', 20),\n                              Float('1.2345678901234567890E-1', 20))\n", "def test_Infinity_mod_inverse():\n    assert mod_inverse(1, oo) == 0\n    assert mod_inverse(oo, 5) is None\n    assert mod_inverse(1, -oo) == 0\n    assert mod_inverse(-oo, 5) is None\n    assert mod_inverse(1, S.ComplexInfinity) is None\n    assert mod_inverse(S.ComplexInfinity, 5) is None\n", "def test_Number_new_unexpected_input():\n    # Test that Number does not return unexpected results when passed unexpected input\n    raises(TypeError, lambda: Number(Symbol('x')))\n    raises(TypeError, lambda: Number(1 + I))\n    raises(TypeError, lambda: Number(S.true))\n    raises(TypeError, lambda: Number(S.false))\n", "def test_comp2():\n    from sympy.core import comp as comp_\n    assert comp_(S.Half, 0.49999999999999994)  # near exact value\n    assert comp_(S.Half, 0.5)  # exact value\n    assert comp_(S.Half, 0.5000000000000001)  # near exact value\n    assert comp_(S.Half, 0.5000000000000003) is False  # above exact value\n    assert comp_(S.Half, 0.4999999999999998) is False  # below exact value\n    assert comp_(S.Half, 0.4999999999999998, 1e-15) is False  # below exact value\n    assert comp_(S.Half, 0.49999999999999994, 1e-15)  # near exact value\n    assert comp_(S.Half, 0.5000000000000001, 1e-15)  # near exact value\n    assert comp_(S.Half, 0.5000000000000003, 1e-15) is False  # above exact value\n    assert comp_(S.Half, 0.4999999999999998, 1e-17) is False  # below exact value\n    assert comp_(S.Half, 0.49999999999999994, 1e-17) is False  # below exact value\n    assert comp_(S.Half, 0.5000000000000001, 1e-17) is False  # above exact value\n    assert comp_(S.Half, 0.5000000000000003, 1e-17) is False  # above exact value\n"], "sample_773": ["def test_logistic_regression_check_solver_penalty():\n    # Test that the solver and penalty are properly checked\n\n    for solver in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']:\n        for penalty in ['l1', 'l2', 'elasticnet', 'none']:\n            if solver not in ['liblinear', 'saga'] and penalty not in ['l2', 'none']:\n                with pytest.raises(ValueError):\n                    LogisticRegression(solver=solver, penalty=penalty)\n            else:\n                LogisticRegression(solver=solver, penalty=penalty)\n", "def test_elastic_net_vs_l1_l2_best_C():\n    # Make sure that elasticnet with grid search on l1_ratio and C gives same or\n    # better results than just l1 or just l2.\n\n    X, y = make_classification(500, random_state=0)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n    param_grid = {'l1_ratio': np.linspace(0, 1, 5), 'C': np.logspace(-4, 4, 5)}\n\n    enet_clf = LogisticRegression(penalty='elasticnet', solver='saga',\n                                  random_state=0)\n    gs = GridSearchCV(enet_clf, param_grid, cv=5, iid=False, refit=True)\n\n    l1_clf = LogisticRegression(penalty='l1', solver='saga', random_state=0)\n    l2_clf = LogisticRegression(penalty='l2', solver='saga', random_state=0)\n    gs_l1 = GridSearchCV(l1_clf, param_grid={'C': np.logspace(-4, 4, 5)},\n                         cv=5, iid=False, refit=True)\n    gs_l2 = GridSearchCV(l2_clf, param_grid={'C': np.logspace(-4, 4, 5)},\n                         cv=5, iid=False, refit=True)\n\n    for clf in (gs, gs_l1, gs_l2):\n        clf.fit(X_train, y_train)\n\n    assert gs.score(X_test, y_test) >= gs_l1.score(X_test, y_test)\n    assert gs.score(X_test, y_test) >= gs_l2.score(X_test, y_test)\n", "def test_multi_class_ohe_no_refit():\n    # Test LogisticRegressionCV attribute shapes when refit is False and\n    # multi_class is 'ovr'\n    n_classes = 3\n    n_features = 20\n    X, y = make_classification(n_samples=200, n_classes=n_classes,\n                               n_informative=n_classes, n_features=n_features,\n                               random_state=0)\n\n    Cs = np.logspace(-4, 4, 3)\n\n    lrcv = LogisticRegressionCV(penalty='l2', Cs=Cs, solver='saga', cv=5,\n                                l1_ratios=[0], random_state=0,\n                                multi_class='ovr', refit=False)\n    lrcv.fit(X, y)\n    assert lrcv.C_.shape == (n_classes,)\n    assert lrcv.l1_ratio_.shape == (n_classes,)\n    assert lrcv.coef_.shape == (n_classes, n_features)\n", "def test_logistic_regression_path_deprecation():\n    # Make sure logistic_regression_path is deprecated\n    X, y = make_classification(n_samples=10, n_features=5, random_state=0)\n    X_sp = sparse.csr_matrix(X)\n\n    for X in (X, X_sp):\n        msg = \"logistic_regression_path was deprecated in version 0.21 and \"\n        \"will be removed in version 0.23.0\"\n        assert_warns_message(DeprecationWarning, msg, logistic_regression_path,\n                             X, y, Cs=10, fit_intercept=True, max_iter=100,\n                             tol=1e-4, verbose=0, solver='lbfgs', coef=None,\n                             class_weight=None, dual=False, penalty='l2',\n                             intercept_scaling=1., multi_class='warn',\n                             random_state=None, check_input=True,\n                             max_squared_sum=None, sample_weight=None,\n                             l1_ratio=None)\n", "def test_class_weight_auto():\n    # Test the auto-class_weight in case the class distribution is\n    # extremely unbalanced\n    y = np.array([1] * 99 + [0])\n    X = np.arange(100).reshape(100, 1)\n\n    # Test auto-class-weighting with logistic regression\n    clf = LogisticRegression(random_state=0, max_iter=1000,\n                             class_weight='balanced')\n    clf.fit(X, y)\n    assert_greater(clf.score(X, y), 0.9)\n\n    # Test auto-class-weighting with LogisticRegressionCV\n    cv = StratifiedKFold(5, shuffle=True, random_state=0)\n    clf = LogisticRegressionCV(Cs=[1], cv=cv, random_state=0,\n                               class_weight='balanced', solver='lbfgs')\n    clf.fit(X, y)\n    assert_greater(clf.score(X, y), 0.9)\n\n    # Test auto-class-weighting with LogisticRegressionCV when penalty='l1'\n    clf = LogisticRegressionCV(Cs=[1], cv=cv, random_state=0,\n                               class_weight='balanced', solver='liblinear',\n                               penalty='l1')\n    clf.fit(X, y)\n    assert_greater(clf.score(X, y), 0.9)\n\n    # Test that class_weight is multiplied with sample_weight\n    sample_weight = np.ones(len(y))\n    sample_weight[y == 0] = 0.1\n    clf = LogisticRegression(random_state=0, max_iter=1000,\n                             class_weight='balanced')\n    clf.fit(X, y, sample_weight=sample_weight)\n    assert_greater(clf.score(X, y), 0.9)\n\n    # Test auto-class-weighting with sample weights\n    clf = LogisticRegression(random_state=0, max_iter=1000,\n                             class_weight='balanced')\n    clf.fit(X, y, sample_weight=sample_weight)\n    assert_greater(clf.score(X, y), 0.9)\n"], "sample_1202": ["def test_sort_key():\n    assert Integer(1).sort_key() == ((1, 0, 'Number'), (0,), (), Integer(1))\n    assert Rational(1, 2).sort_key() == ((1, 0, 'Number'), (0,), (), Rational(1, 2))\n    assert Float(1.1).sort_key() == ((1, 0, 'Number'), (0,), (), Float(1.1))\n    assert S.Infinity.sort_key() == ((1, 0, 'Number'), (0,), (), S.Infinity)\n    assert S.NegativeInfinity.sort_key() == ((1, 0, 'Number'), (0,), (), S.NegativeInfinity)\n    assert S.NegativeOne.sort_key() == ((1, 0, 'Number'), (0,), (), S.NegativeOne)\n    assert S.One.sort_key() == ((1, 0, 'Number'), (0,), (), S.One)\n    assert S.Half.sort_key() == ((1, 0, 'Number'), (0,), (), S.Half)\n    assert S.Zero.sort_key() == ((1, 0, 'Number'), (0,), (), S.Zero)\n    assert S.NaN.sort_key() == ((1, 0, 'Number'), (0,), (), S.NaN)\n    assert S.ComplexInfinity.sort_key() == ((1, 0, 'Number'), (0,), (), S.ComplexInfinity)\n", "def test_issue_19332():\n    x = Symbol('x', real=True, positive=True)\n    assert (oo + x).is_extended_real\n    assert (x + oo).is_extended_real\n    assert (-oo + x).is_extended_real\n    assert (x - oo).is_extended_real\n    assert (oo - x).is_extended_real\n    assert (-oo - x).is_extended_real\n\n    x = Symbol('x', real=True, negative=True)\n    assert (oo + x).is_extended_real\n    assert (x + oo).is_extended_real\n    assert (-oo + x).is_extended_real\n    assert (x - oo).is_extended_real\n    assert (oo - x).is_extended_real\n    assert (-oo - x).is_extended_real\n\n    x = Symbol('x', real=True)\n    assert (oo + x).is_extended_real\n    assert (x + oo).is_extended_real\n    assert (-oo + x).is_extended_real\n    assert (x - oo).is_extended_real\n    assert (oo - x).is_extended_real\n    assert (-oo - x).is_extended_real\n", "def test_issue_18863():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    assert (0 < x*y).simplify() == (0 < x) | (0 < y)\n    assert (0 > x*y).simplify() == (0 > x) | (0 > y)\n    assert (x*y < 0).simplify() == ((0 > x) & (0 < y)) | ((0 < x) & (0 > y))\n    assert (x*y > 0).simplify() == ((0 < x) & (0 < y)) | ((0 > x) & (0 > y))\n    assert (x*y <= 0).simplify() == (x <= 0) | (y <= 0)\n    assert (x*y >= 0).simplify() == (x >= 0) | (y >= 0)\n    assert (0 <= x*y).simplify() == ((0 <= x) & (0 <= y)) | ((0 >= x) & (0 >= y))\n    assert (0 >= x*y).simplify() == ((0 >= x) & (0 <= y)) | ((0 <= x) & (0 >= y))\n", "def test_issue_7255():\n    assert (2**(oo/3)).is_Add is False\n    assert (2**(oo/3)).is_Pow is True\n    assert (2**(oo/3)).is_number is False\n    assert (2**(oo/3)).is_finite is False\n    assert (2**(oo/3)).is_Mul is False\n    assert (2**(oo/3)).is_Rational is False\n", "def test_issue_11707():\n    # issue 11707\n    # the exactness of Float/Rational comparisons are checked in test_Float_eq,\n    # but here we test a specific thing: that Rational(x) != Float(y) when\n    # Rational(x) != Rational(Float(y))\n    assert Rational(1, 100) != Float(0.01)\n    assert Rational(1, 100) != Float('0.01', 20)\n    assert Rational(1, 100) != Float(.01)\n    assert Rational(1, 100) != Float(.0100000000000000001)\n    assert Rational(1, 100) != Float('0.0100000000000000001')\n    assert Rational(1, 100) != Float('0.01', 22)\n    assert Rational(1, 100) != Float(Rational(1, 100), 22)\n"], "sample_734": ["def test_contingency_matrix_edge_cases():\n    # Test edge cases for contingency_matrix\n    assert_array_almost_equal(contingency_matrix([0], [0]).ravel(), [1])\n    assert_array_almost_equal(contingency_matrix([0, 0], [0, 0]).ravel(), [2])\n    assert_array_almost_equal(contingency_matrix([0, 1], [0, 0]).ravel(), [2, 0])\n    assert_array_almost_equal(contingency_matrix([0, 0], [0, 1]).ravel(), [1, 1])\n    assert_array_almost_equal(contingency_matrix([0], [0, 1]).ravel(), [1, 0])\n    assert_array_almost_equal(contingency_matrix([0, 1], [0]).ravel(), [1, 0])\n    assert_array_almost_equal(contingency_matrix([], []).ravel(), [])\n    assert_array_almost_equal(contingency_matrix([], [0, 1]).ravel(), [])\n    assert_array_almost_equal(contingency_matrix([0, 1], []).ravel(), [])\n", "def test_homogeneity_completeness_v_measure_edge_cases():\n    # Test edge cases where the number of samples is 1 or 0\n    h, c, v = homogeneity_completeness_v_measure([0], [0])\n    assert_almost_equal(h, 1.0, 2)\n    assert_almost_equal(c, 1.0, 2)\n    assert_almost_equal(v, 1.0, 2)\n\n    h, c, v = homogeneity_completeness_v_measure([], [])\n    assert_almost_equal(h, 1.0, 2)\n    assert_almost_equal(c, 1.0, 2)\n    assert_almost_equal(v, 1.0, 2)\n\n    h, c, v = homogeneity_completeness_v_measure([0, 1, 2], [0, 1, 2])\n    assert_almost_equal(h, 1.0, 2)\n    assert_almost_equal(c, 1.0, 2)\n    assert_almost_equal(v, 1.0, 2)\n\n    # Test edge cases where the number of classes or clusters is 1\n    h, c, v = homogeneity_completeness_v_measure([0, 0, 0], [0, 1, 2])\n    assert_almost_equal(h, 1.0, 2)\n    assert_almost_equal(c, 0.0, 2)\n    assert_almost_equal(v, 0.0, 2)\n\n    h, c, v = homogeneity_completeness_v_measure([0, 0, 0], [0, 0, 0])\n    assert_almost_equal(h, 1.0, 2)\n    assert_almost_equal(c, 1.0, 2)\n    assert_almost_equal(v, 1.0, 2)\n\n    h, c, v = homogeneity_completeness_v_measure([0, 1, 2], [0, 0, 0])\n    assert_almost_equal(h, 0.0, 2)\n    assert_almost_equal(c, 1.0, 2)\n    assert_almost_equal(v, 0.0, 2)\n", "def test_contingency_matrix_output():\n    labels_a = np.array([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3])\n    labels_b = np.array([1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 3, 1, 3, 3, 3, 2, 2])\n    C = contingency_matrix(labels_a, labels_b)\n    assert_equal(C.shape, (3, 3))\n    C_sparse = contingency_matrix(labels_a, labels_b, sparse=True)\n    assert_equal(C_sparse.shape, (3, 3))\n    assert_array_almost_equal(C_sparse.toarray(), C)\n    C_eps = contingency_matrix(labels_a, labels_b, eps=0.1)\n    assert_array_almost_equal(C_eps, C + 0.1)\n    assert_raises(ValueError, contingency_matrix, labels_a, labels_b, eps=0.1, sparse=True)\n", "def test_contingency_matrix_edge_cases():\n    # Test edge cases for contingency_matrix\n    labels_a = np.array([1, 2, 3, 4, 5])\n    labels_b = np.array([1, 1, 2, 2, 2])\n\n    # Test with a single class in labels_a\n    C = contingency_matrix(labels_a, labels_b)\n    assert_array_almost_equal(C, np.array([[1, 0], [0, 1], [0, 1], [0, 1], [0, 1]]))\n\n    # Test with a single class in labels_b\n    C = contingency_matrix(labels_a, labels_b)\n    assert_array_almost_equal(C, np.array([[1, 0], [0, 1], [0, 1], [0, 1], [0, 1]]))\n\n    # Test with a single sample\n    C = contingency_matrix([1], [1])\n    assert_array_almost_equal(C, np.array([[1]]))\n\n    # Test with a single sample and eps\n    C = contingency_matrix([1], [1], eps=1e-10)\n    assert_array_almost_equal(C, np.array([[1 + 1e-10]]))\n", "def test_empty_input_to_entropy_function():\n    # Test that entropy function returns 1.0 for an empty input\n    assert_almost_equal(entropy(np.array([])), 1.0)\n    assert_almost_equal(entropy(np.array([], dtype=int)), 1.0)\n    assert_almost_equal(entropy(np.array([], dtype=float)), 1.0)\n"], "sample_485": ["    def test_urlize_trailing_punctuation(self):\n        tests = (\n            (\n                \"Search for google.com/?q=! and see.\",\n                'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>! and see.',\n            ),\n            (\n                \"Search for google.com/?q=1&lt! and see.\",\n                'Search for <a href=\"http://google.com/?q=1%3C\">google.com/?q=1&lt'\n                \"</a>! and see.\",\n            ),\n            (\n                \"Search for google.com/?q=!. and see.\",\n                'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>!. and see.',\n            ),\n            (\n                \"Search for google.com/?q=! and see,\",\n                'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>! and see,',\n            ),\n            (\n                \"Search for google.com/?q=! and see.\",\n                'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>! and see.',\n            ),\n        )\n        for value, output in tests:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value), output)\n", "def test_avoid_wrapping(self):\n    tests = (\n        (\"This is a test\", \"This\\xa0is\\xa0a\\xa0test\"),\n        (\"Hello, world!\", \"Hello,\\xa0world!\"),\n        (\" foo bar \", \"\\xa0foo\\xa0bar\\xa0\"),\n        (\"No spaces\", \"No spaces\"),\n    )\n    for input_str, expected_output in tests:\n        with self.subTest(input_str=input_str, expected_output=expected_output):\n            self.assertEqual(avoid_wrapping(input_str), expected_output)\n", "def test_avoid_wrapping(self):\n    self.assertEqual(avoid_wrapping(\"Test string with spaces\"), \"Test\\xa0string\\xa0with\\xa0spaces\")\n    self.assertEqual(avoid_wrapping(\"Test string without spaces\"), \"Test\\xa0string\\xa0without\\xa0spaces\")\n    self.assertEqual(avoid_wrapping(\"\"), \"\")\n    self.assertEqual(avoid_wrapping(\"   \"), \"\\xa0\\xa0\\xa0\")\n\n    self.assertEqual(avoid_wrapping(lazystr(\"Test string with spaces\")), \"Test\\xa0string\\xa0with\\xa0spaces\")\n    self.assertEqual(avoid_wrapping(lazystr(\"Test string without spaces\")), \"Test\\xa0string\\xa0without\\xa0spaces\")\n    self.assertEqual(avoid_wrapping(lazystr(\"   \")), \"\\xa0\\xa0\\xa0\")\n    self.assertEqual(avoid_wrapping(lazystr(\"\")), \"\")\n\n    self.assertEqual(avoid_wrapping(mark_safe(\"Test string with spaces\")), \"Test\\xa0string\\xa0with\\xa0spaces\")\n    self.assertEqual(avoid_wrapping(mark_safe(\"Test string without spaces\")), \"Test\\xa0string\\xa0without\\xa0spaces\")\n    self.assertEqual(avoid_wrapping(mark_safe(\"   \")), \"\\xa0\\xa0\\xa0\")\n    self.assertEqual(avoid_wrapping(mark_safe(\"\")), \"\")\n\n    self.assertEqual(avoid_wrapping(lazystr(mark_safe(\"Test string with spaces\"))), \"Test\\xa0string\\xa0with\\xa0spaces\")\n    self.assertEqual(avoid_wrapping(lazystr(mark_safe(\"Test string without spaces\"))), \"Test\\xa0string\\xa0without\\xa0spaces\")\n    self.assertEqual(avoid_wrapping(lazystr(mark_safe(\"   \"))), \"\\xa0\\xa0\\xa0\")\n    self.assertEqual(avoid_wrapping(lazystr(mark_safe(\"\"))), \"\")\n", "def test_avoid_wrapping(self):\n    tests = (\n        (\"hello world\", \"hello\\xa0world\"),\n        (\"   \", \"\\xa0\\xa0\\xa0\"),\n        (\"hello  world\", \"hello\\xa0\\xa0world\"),\n        (\"hello   world\", \"hello\\xa0\\xa0\\xa0world\"),\n        (\"  hello world  \", \"\\xa0\\xa0hello\\xa0world\\xa0\\xa0\"),\n        (\"multiple   spaces\", \"multiple\\xa0\\xa0\\xa0spaces\"),\n    )\n    for value, output in tests:\n        with self.subTest(value=value, output=output):\n            self.assertEqual(avoid_wrapping(value), output)\n", "def test_avoid_wrapping(self):\n    tests = (\n        (\" foo \", \"\u00a0foo\u00a0\"),\n        (\"foo bar\", \"foo\\xa0bar\"),\n        (\"multiple   spaces\", \"multiple\\xa0\\xa0\\xa0spaces\"),\n        (\"trailing space \", \"trailing\\xa0space\\xa0\"),\n    )\n    for value, output in tests:\n        with self.subTest(value=value, output=output):\n            self.assertEqual(avoid_wrapping(value), output)\n"], "sample_956": ["def test_inventory_exists(tempdir, app, status, warning):\n    inv_file = tempdir / 'inventory'\n    inv_file.write_bytes(inventory_v2)\n    set_config(app, {\n        'inv_name': ('https://docs.python.org/', inv_file),\n    })\n\n    # load the inventory and check if it's done correctly\n    normalize_intersphinx_mapping(app, app.config)\n    load_mappings(app)\n\n    assert inventory_exists(app.env, 'inv_name')\n    assert not inventory_exists(app.env, 'nonexistent_inv_name')\n", "def test_resolve_reference_detect_inventory(tempdir, app, status, warning):\n    inv_file = tempdir / 'inventory'\n    inv_file.write_bytes(inventory_v2)\n    set_config(app, {\n        'https://docs.python.org/': inv_file,\n    })\n\n    # load the inventory and check if it's done correctly\n    normalize_intersphinx_mapping(app, app.config)\n    load_mappings(app)\n\n    # test split target into 'inv_name:target'\n    node, contnode = fake_node('py', 'func', 'module1.func', 'module1.func')\n    node['reftarget'] = 'https://docs.python.org:module1.func'\n    rn = resolve_reference_detect_inventory(app.env, node, contnode)\n    assert rn is not None\n    assert rn['refuri'] == 'https://docs.python.org/sub/foo.html#module1.func'\n    assert rn['reftitle'] == '(in foo v2.0)'\n    assert rn[0].astext() == 'module1.func'\n\n    # test split target into 'inv_name:target' and inv_name is not found\n    node, contnode = fake_node('py', 'func', 'module1.func', 'module1.func')\n    node['reftarget'] = 'not_found:module1.func'\n    rn = resolve_reference_detect_inventory(app.env, node, contnode)\n    assert rn is None\n", "def test_inventory_exists(tempdir, app, status, warning):\n    inv_file = tempdir / 'inventory'\n    inv_file.write_bytes(inventory_v2)\n    set_config(app, {\n        'https://docs.python.org/': inv_file,\n    })\n\n    # load the inventory and check if it's done correctly\n    normalize_intersphinx_mapping(app, app.config)\n    load_mappings(app)\n\n    # existing inventory name\n    assert inventory_exists(app.env, 'https://docs.python.org/')\n\n    # non existing inventory name\n    assert not inventory_exists(app.env, 'https://other-docs.org/')\n\n    # existing inventory name with custom name\n    app.config.intersphinx_mapping = {\n        'my_inv': ('https://docs.python.org/', inv_file),\n    }\n    normalize_intersphinx_mapping(app, app.config)\n    load_mappings(app)\n    assert inventory_exists(app.env, 'my_inv')\n", "def test_resolve_reference_detect_inventory(tempdir, app, status, warning):\n    inv_file = tempdir / 'inventory'\n    inv_file.write_bytes(inventory_v2)\n    set_config(app, {\n        'https://docs.python.org/': inv_file,\n        'inv': ('https://docs.python.org/', inv_file),\n    })\n\n    # load the inventory and check if it's done correctly\n    normalize_intersphinx_mapping(app, app.config)\n    load_mappings(app)\n\n    # test without prefix\n    node, contnode = fake_node('py', 'func', 'module1.func', 'func()')\n    rn = resolve_reference_detect_inventory(app.env, node, contnode)\n    assert rn['refuri'] == 'https://docs.python.org/sub/foo.html#module1.func'\n\n    # test with prefix\n    node, contnode = fake_node('py', 'func', 'inv:module1.func', 'func()')\n    rn = resolve_reference_detect_inventory(app.env, node, contnode)\n    assert rn['refuri'] == 'https://docs.python.org/sub/foo.html#module1.func'\n\n    # test with prefix but not in inventory\n    node, contnode = fake_node('py', 'func', 'nonexist:module1.func', 'func()')\n    rn = resolve_reference_detect_inventory(app.env, node, contnode)\n    assert rn is None\n\n    # test with prefix but target not found\n    node, contnode = fake_node('py', 'func', 'inv:nonexist.func', 'func()')\n    rn = resolve_reference_detect_inventory(app.env, node, contnode)\n    assert rn is None\n\n    # test with prefix but refexplicit\n    node, contnode = fake_node('py', 'func', 'inv:module1.func', 'func()', refexplicit=True)\n    rn = resolve_reference_detect_inventory(app.env, node, contnode)\n    assert rn['refuri'] == 'https://docs.python.org/sub/foo.html#module1.func'\n    assert rn[0].astext() == 'inv:module1.func'\n", "def test_resolve_reference_detect_inventory_split(tempdir, app, status, warning):\n    inv_file = tempdir / 'inventory'\n    inv_file.write_bytes(inventory_v2)\n    set_config(app, {\n        'https://docs.python.org/': inv_file,\n    })\n\n    # load the inventory and check if it's done correctly\n    normalize_intersphinx_mapping(app, app.config)\n    load_mappings(app)\n\n    # test that a target that is split into 'inv_name:newtarget' is resolved correctly\n    node, contnode = fake_node('py', 'func', 'inv:module1.func', 'module1.func')\n    rn = missing_reference(app, app.env, node, contnode)\n    assert isinstance(rn, nodes.reference)\n    assert rn['refuri'] == 'https://docs.python.org/sub/foo.html#module1.func'\n    assert rn['reftitle'] == '(in foo v2.0)'\n    assert rn[0].astext() == 'module1.func'\n\n    # test that a target that is split into 'inv_name:newtarget' is not resolved when\n    # the inv_name is not found in the inventory\n    node, contnode = fake_node('py', 'func', 'inv_not_found:module1.func', 'module1.func')\n    rn = missing_reference(app, app.env, node, contnode)\n    assert rn is None\n\n    # test that a target that is split into 'inv_name:newtarget' is not resolved when\n    # the newtarget is not found in the inventory\n    node, contnode = fake_node('py', 'func', 'inv:func_not_found', 'func_not_found')\n    rn = missing_reference(app, app.env, node, contnode)\n    assert rn is None\n"], "sample_554": ["def test_text_transform_rotates_text():\n    fig = plt.figure()\n    ax = plt.gca()\n    transform = mtransforms.Affine2D().rotate_deg(30)\n    text = ax.text(0, 0, 'test', transform=transform)\n    assert text.get_transform_rotates_text() is False\n    assert text.get_rotation() == 0\n\n    text.set_transform_rotates_text(True)\n    assert text.get_transform_rotates_text() is True\n    assert text.get_rotation() == 30\n", "def test_get_linespacing():\n    text1 = Text(0.5, 0.5, 'test', linespacing=1.2)\n    text2 = Text(0.5, 0.5, 'test', linespacing=1.5)\n\n    assert text1._linespacing == 1.2\n    assert text2._linespacing == 1.5\n\n    text1.set_linespacing(2.0)\n    text2.set_linespacing(2.5)\n\n    assert text1._linespacing == 2.0\n    assert text2._linespacing == 2.5\n", "def test_wrap_multiple_times():\n    fig, ax = plt.subplots()\n    s = 'This is a very long text that should be wrapped multiple times and '\n    s += 'then again and again because it is just too long.'\n    text = fig.text(0.5, 0.5, s, wrap=True)\n    fig.canvas.draw()\n    assert len(text._get_wrapped_text().split('\\n')) > 4\n", "def test_transform_rotates_text_mode():\n    # Test that transform_rotates_text affects the text position when rotation_mode\n    # is 'anchor'.\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n\n    t0 = ax.text(0.5, 0.5, 'test', rotation=45, rotation_mode='anchor',\n                  transform_rotates_text=True)\n    t1 = ax.text(0.5, 0.5, 'test', rotation=45, rotation_mode='anchor',\n                  transform_rotates_text=False)\n\n    fig.canvas.draw()\n\n    assert t0.get_window_extent(fig.canvas.renderer).min != \\\n        t1.get_window_extent(fig.canvas.renderer).min\n", "def test_annotation_transform_rotates_text():\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_aspect('equal')\n\n    text = ax.text(0.5, 0.5, 'horizontal', transform=ax.transAxes, rotation=45,\n                   ha='center', va='center')\n    ax.add_artist(text)\n\n    ax2 = ax.secondary_xaxis('top')\n    ax2.set_xlim(-1, 2)\n    text = ax2.text(0.5, 0.5, 'transform_rotates_text=True',\n                    transform=ax2.transAxes, rotation=45, ha='center',\n                    va='center', transform_rotates_text=True)\n    ax2.add_artist(text)\n\n    ax3 = ax.secondary_xaxis('top')\n    ax3.set_xlim(-1, 2)\n    text = ax3.text(0.5, 0.5, 'transform_rotates_text=False',\n                    transform=ax3.transAxes, rotation=45, ha='center',\n                    va='center', transform_rotates_text=False)\n    ax3.add_artist(text)\n"], "sample_169": ["    def test_xml_serialization(self):\n        test_xml_data = (\n            '<django-objects version=\"1.0\">'\n            '<object model=\"model_fields.jsonmodel\">'\n            '<field name=\"value\" type=\"JSONField\">%s'\n            '</field></object></django-objects>'\n        )\n        test_values = (\n            # (Python value, serialized value),\n            ({'a': 'b', 'c': None}, '{\"a\": \"b\", \"c\": null}'),\n            ('abc', '\"abc\"'),\n            ('{\"a\": \"a\"}', '\"{\\\\\"a\\\\\": \\\\\"a\\\\\"}\"'),\n        )\n        for value, serialized in test_values:\n            with self.subTest(value=value):\n                instance = JSONModel(value=value)\n                data = serializers.serialize('xml', [instance], fields=['value'])\n                self.assertXMLEqual(data, test_xml_data % serialized)\n                new_instance = list(serializers.deserialize('xml', data))[0].object\n                self.assertEqual(new_instance.value, instance.value)\n", "    def test_invalid_xml(self):\n        invalid_xml = '<django-objects version=\"1.0\"><object model=\"model_fields.jsonmodel\"><field name=\"value\" type=\"JSONField\"> invalid xml </field></object></django-objects>'\n        with self.assertRaises(serializers.base.DeserializationError):\n            list(serializers.deserialize('xml', invalid_xml))\n", "    def test_serialization(self):\n        obj = JSONModel.objects.create(value={'a': 'b', 'c': None})\n        xml_data = serializers.serialize('xml', [obj])\n        self.assertXMLEqual(\n            xml_data,\n            '<django-objects version=\"1.0\">'\n            '<object model=\"model_fields.jsonmodel\">'\n            '<field name=\"value\" type=\"JSONField\">{\\'a\\': \\'b\\', \\'c\\': null}</field>'\n            '</object></django-objects>',\n        )\n        deserialized_obj = list(serializers.deserialize('xml', xml_data))[0].object\n        self.assertEqual(deserialized_obj.value, obj.value)\n", "    def test_xml_serialization(self):\n        value = {'a': 'b', 'c': None}\n        instance = JSONModel(value=value)\n        xml_data = serializers.serialize('xml', [instance])\n        self.assertXMLElementEqual(xml_data, '<django-objects version=\"1.0\"><object model=\"model_fields.jsonmodel\" pk=\"None\"><field name=\"value\" type=\"JSONField\">{&quot;a&quot;: &quot;b&quot;, &quot;c&quot;: null}</field></object></django-objects>')\n", "    def test_serialization(self):\n        test_data = NullableJSONModel.objects.create(value={'name': 'John', 'age': 20})\n        data = serializers.serialize('xml', [test_data])\n        expected_xml = (\n            '<django-objects version=\"1.0\">'\n            '<object model=\"model_fields.nullablejsonmodel\" pk=\"{}\">'\n            '<field name=\"value\" type=\"JSONField\">{{}}</field>'\n            '</object></django-objects>'\n        ).format(test_data.pk)\n        self.assertXMLEqual(data, expected_xml)\n"], "sample_1023": ["def test_sieve_edge_cases():\n    # test edge cases for sieve\n    s = Sieve()\n    assert s._list == _aset(2, 3, 5, 7, 11, 13)\n    assert s._tlist == _aset(0, 1, 1, 2, 2, 4)\n    assert s._mlist == _aset(0, 1, -1, -1, 0, -1)\n    s._reset(prime=True)\n    assert s._list == _aset(2, 3, 5, 7, 11, 13)\n    assert s._tlist == _aset(0, 1, 1, 2, 2, 4)\n    assert s._mlist == _aset(0, 1, -1, -1, 0, -1)\n    s._reset(totient=True)\n    assert s._list == _aset(2, 3, 5, 7, 11, 13)\n    assert s._tlist == _aset(0, 1, 1, 2, 2, 4)\n    assert s._mlist == _aset(0, 1, -1, -1, 0, -1)\n    s._reset(mobius=True)\n    assert s._list == _aset(2, 3, 5, 7, 11, 13)\n    assert s._tlist == _aset(0, 1, 1, 2, 2, 4)\n    assert s._mlist == _aset(0, 1, -1, -1, 0, -1)\n\n    s._reset(prime=True, totient=True, mobius=True)\n    assert s._list == _aset(2, 3, 5, 7, 11, 13)\n    assert s._tlist == _aset(0, 1, 1, 2, 2, 4)\n    assert s._mlist == _aset(0, 1, -1, -1, 0, -1)\n", "def test_cycle_length():\n    # test cycle_length on an empty function\n        return x\n    assert next(cycle_length(func, 1)) == (1, 0)\n    assert list(cycle_length(func, 1, values=True)) == [1]\n\n    # test cycle_length with a limit\n        return x + 1\n    assert next(cycle_length(func, 1, nmax=5)) == (5, None)\n    assert list(cycle_length(func, 1, nmax=5, values=True)) == [2, 3, 4, 5, 6]\n", "def test_cycle_length_edge_cases():\n    func = lambda i: i\n    assert next(cycle_length(func, 0, values=False)) == (1, 0)\n    assert list(cycle_length(func, 0, values=True)) == [0]\n\n    func = lambda i: i\n    assert next(cycle_length(func, 1, values=False)) == (1, 0)\n    assert list(cycle_length(func, 1, values=True)) == [1]\n\n    func = lambda i: i + 1\n    assert next(cycle_length(func, 0, values=False)) == (1, None)\n    assert list(cycle_length(func, 0, values=True, nmax=10)) == list(range(10))\n\n    func = lambda i: (i + 1) % 2\n    assert next(cycle_length(func, 0, values=False)) == (2, 0)\n    assert list(cycle_length(func, 0, values=True)) == [1, 0, 1, 0]\n\n    raises(ValueError, lambda: cycle_length(func, -1))\n    raises(ValueError, lambda: cycle_length(func, 0, nmax=-1))\n", "def test_cycle_length_values():\n        if i == 1:\n            return 2\n        elif i == 2:\n            return 1\n        else:\n            return i + 1\n\n    assert list(cycle_length(func, 1, values=True)) == [2, 1, 3, 2, 1]\n    assert next(cycle_length(func, 1)) == (3, 0)\n    assert next(cycle_length(func, 2)) == (3, 0)\n\n        if i == 1:\n            return 3\n        elif i == 2:\n            return 3\n        else:\n            return i + 1\n\n    assert list(cycle_length(func, 1, values=True)) == [3, 4, 3, 4]\n    assert next(cycle_length(func, 1)) == (2, 1)\n\n        return i + 1\n\n    assert list(cycle_length(func, 1, values=True)) == [2, 3, 4, 5, 6]\n    assert next(cycle_length(func, 1, nmax=5)) == (5, None)\n\n        return i\n\n    assert list(cycle_length(func, 1, values=True)) == [1]\n    assert next(cycle_length(func, 1)) == (1, 0)\n", "def test_cycle_length():\n        return x + 1\n    assert next(cycle_length(func, 0)) == (1, 0)\n    assert next(cycle_length(func, 5)) == (1, 0)\n\n        return x - 1\n    assert next(cycle_length(func, 5)) == (1, 0)\n    assert next(cycle_length(func, 0)) == (1, 0)\n\n        return x * x\n    assert next(cycle_length(func, 1)) == (2, 1)\n    assert list(cycle_length(func, 1, values=True)) == [1, 1]\n\n        return x * x\n    assert next(cycle_length(func, 2)) == (2, 1)\n    assert list(cycle_length(func, 2, values=True)) == [2, 4]\n\n        if x % 2 == 0:\n            return x // 2\n        else:\n            return 3 * x + 1\n    assert next(cycle_length(func, 6)) == (8, 1)\n    assert list(cycle_length(func, 6, values=True)) == [6, 3, 10, 5, 16, 8, 4, 2, 1]\n\n        return x + 10\n    assert next(cycle_length(func, 0, nmax=10)) == (10, None)\n    assert list(cycle_length(func, 0, nmax=10, values=True)) == [0, 10, 20, 30, 40, 50, 60, 70, 80, 90]\n"], "sample_1122": ["def test_unbranched_argument():\n    x = Symbol('x')\n    z = Symbol('z', polar=True)\n    f = Function('f')\n\n    assert unbranched_argument(1) == 0\n    assert unbranched_argument(-1) == pi\n    assert unbranched_argument(I) == pi/2\n    assert unbranched_argument(-I) == -pi/2\n    assert unbranched_argument(1 + I) == pi/4\n    assert unbranched_argument(-1 + I) == pi*Rational(3, 4)\n    assert unbranched_argument(1 - I) == -pi/4\n    assert unbranched_argument(exp_polar(4*pi*I)) == 4*pi\n    assert unbranched_argument(exp_polar(-7*pi*I)) == -7*pi\n    assert unbranched_argument(exp_polar(5 - 3*pi*I/4)) == pi*Rational(-3, 4)\n    assert unbranched_argument(f(0) + I*f(1)) == arg(f(0) + I*f(1))\n\n    assert unbranched_argument(x) == arg(x)\n    assert unbranched_argument(z) == arg(z)\n    assert unbranched_argument(f(x)) == arg(f(x))\n    assert unbranched_argument(f(z)) == arg(f(z))\n", "def test_polar_lift():\n    from sympy import I, pi, exp, exp_polar, sin, cos\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    z = Symbol('z', positive=True)\n\n    assert polar_lift(1).func == exp_polar\n    assert polar_lift(1).args[0] == 0\n\n    assert polar_lift(I).func == exp_polar\n    assert polar_lift(I).args[0] == -I*pi/2\n\n    assert polar_lift(-1).func == exp_polar\n    assert polar_lift(-1).args[0] == I*pi\n\n    assert polar_lift(-I).func == exp_polar\n    assert polar_lift(-I).args[0] == I*pi/2\n\n    assert polar_lift(0).func == exp_polar\n    assert polar_lift(0).args[0] == 0\n\n    assert polar_lift(x).func == polar_lift\n    assert polar_lift(x).args[0] == x\n\n    assert polar_lift(y).func == polar_lift\n    assert polar_lift(y).args[0] == y\n\n    assert polar_lift(z).func == exp_polar\n    assert polar_lift(z).args[0] == 0\n\n    assert polar_lift(exp_polar(2*pi*I)).func == exp_polar\n    assert polar_lift(exp_polar(2*pi*I)).args[0] == 2*pi*I\n\n    assert polar_lift(sin(x)).func == polar_lift\n    assert polar_lift(sin(x)).args[0] == sin(x)\n\n    assert polar_lift(cos(x)).func == polar_lift\n    assert polar_lift(cos(x)).args[0] == cos(x)\n", "def test_periodic_argument_branch():\n    from sympy import periodic_argument, pi, exp_polar, oo\n    from sympy.abc import x\n    assert periodic_argument(exp_polar(3*pi*I), pi) == -pi\n    assert periodic_argument(exp_polar(-5*pi*I), pi) == -pi\n    assert periodic_argument(exp_polar(3*pi*I), -pi) == pi\n    assert periodic_argument(exp_polar(-5*pi*I), -pi) == pi\n    assert periodic_argument(exp_polar(7*pi*I), 3*pi) == pi\n    assert periodic_argument(exp_polar(3*pi*I), -3*pi) == -pi\n    assert periodic_argument(exp_polar(oo*I), oo) == periodic_argument(x, oo)\n    assert periodic_argument(exp_polar(-oo*I), oo) == periodic_argument(x, oo)\n    assert periodic_argument(exp_polar(oo*pi*I), oo) == oo\n    assert periodic_argument(exp_polar(-oo*pi*I), oo) == -oo\n", "def test_conjugate_properties():\n    z = Symbol('z')\n    assert conjugate(z).is_real is None\n    assert conjugate(z).is_finite is None\n    assert conjugate(z).is_finite is conjugate(z).is_extended_real\n    assert conjugate(z).is_nonzero is conjugate(z).is_zero is None\n\n    z = Symbol('z', complex=True, zero=False)\n    assert conjugate(z).is_real is True\n    assert conjugate(z).is_finite is True\n    assert conjugate(z).is_zero is False\n\n    z = Symbol('z', complex=True, finite=False)\n    assert conjugate(z).is_finite is conjugate(z).is_extended_finite is False\n    assert conjugate(z).is_infinite is conjugate(z).is_zero is None\n\n    z = Symbol('z', real=True)\n    assert conjugate(z).is_real is True\n    assert conjugate(z).is_finite is True\n    assert conjugate(z).is_finite is conjugate(z).is_extended_real\n    assert conjugate(z).is_zero is None\n\n    z = Symbol('z', real=True, zero=False)\n    assert conjugate(z).is_finite is True\n    assert conjugate(z).is_zero is False\n\n    z = Symbol('z', real=True, finite=False)\n    assert conjugate(z).is_finite is conjugate(z).is_extended_finite is False\n    assert conjugate(z).is_infinite is conjugate(z).is_zero is None\n\n    z = Symbol('z', real=True, nonnegative=True)\n    assert conjugate(z).is_nonnegative is True\n\n    z = Symbol('z', real=True, nonpositive=True)\n    assert conjugate(z).is_nonpositive is True\n\n    z = Symbol('z', real=True, positive=True)\n    assert conjugate(z).is_positive is True\n\n    z = Symbol('z', real=True, negative=True)\n    assert conjugate(z).is_negative is True\n", "def test_polar_lift():\n    assert polar_lift(1) == 1\n    assert polar_lift(-1) == exp_polar(I*pi)\n    assert polar_lift(I) == exp_polar(-I*pi/2)\n    assert polar_lift(1 + I) == polar_lift(1 + I)\n    assert polar_lift(2 + I) == 2*exp_polar(0) + exp_polar(I*atan2(1, 2))\n\n    a = Symbol('a', algebraic=True)\n    t = Symbol('t', transcendental=True)\n    x = Symbol('x')\n    assert polar_lift(a).is_algebraic is None\n    assert polar_lift(x).is_algebraic is None\n    assert polar_lift(t).is_algebraic is False\n\n    p = Symbol('p', positive=True)\n    assert polar_lift(p).is_polar\n    assert polar_lift(p).is_comparable is False\n    assert polar_lift(p).doit() == polar_lift(p)\n    assert polar_lift(p).evalf() == p\n\n    n = Symbol('n', negative=True)\n    assert polar_lift(n).is_polar\n    assert polar_lift(n).is_comparable is False\n    assert polar_lift(n).doit() == polar_lift(n)\n    assert polar_lift(n).evalf() == abs(n)\n    assert polar_lift(n).is_nonnegative\n    assert polar_lift(n).is_nonpositive is None\n\n    assert polar_lift(S.NaN).is_polar is False\n    assert polar_lift(S.NaN).is_comparable is False\n    assert polar_lift(S.NaN).doit() == polar_lift(S.NaN)\n    assert polar_lift(S.NaN).evalf() == S.NaN\n\n    z = Symbol('z', complex=True)\n    assert polar_lift(z*I).is_polar\n    assert polar_lift(z*I).is_comparable is False\n    assert polar_lift(z*I).doit() == polar_lift(z*I)\n    assert polar_lift(z*I).evalf() == z*I\n"], "sample_383": ["    def setUpTestData(cls):\n        cls.t1 = Tag.objects.create(name=\"t1\")\n        cls.t2 = Tag.objects.create(name=\"t2\", parent=cls.t1)\n        cls.t3 = Tag.objects.create(name=\"t3\", parent=cls.t1)\n        cls.t4 = Tag.objects.create(name=\"t4\", parent=cls.t3)\n        cls.t5 = Tag.objects.create(name=\"t5\", parent=cls.t3)\n        cls.i1 = Item.objects.create(name=\"one\")\n        cls.i1.tags.set([cls.t1, cls.t2])\n        cls.i2 = Item.objects.create(name=\"two\")\n        cls.i2.tags.set([cls.t1, cls.t3])\n        cls.i3 = Item.objects.create(name=\"three\")\n        cls.i4 = Item.objects.create(name=\"four\")\n        cls.i4.tags.set([cls.t4])\n", "    def test_ticket_24352(self):\n        e1 = ExtraInfo.objects.create()\n        self.assertEqual(\n            Author.objects.filter(extra__in=[e1]).values(\"id\").order_by(\"id\"),\n            [{\"id\": 1}],\n        )\n", "    def test_ticket_30561(self):\n        a1 = Author.objects.create(name=\"a1\", num=1001)\n        a2 = Author.objects.create(name=\"a2\", num=2002)\n        a3 = Author.objects.create(name=\"a3\", num=3003)\n        Item.objects.create(name=\"one\", creator=a1)\n        Item.objects.create(name=\"two\", creator=a2)\n        Item.objects.create(name=\"three\", creator=a3)\n        qs = Author.objects.filter(\n            item__in=Item.objects.filter(creator__in=[a1, a2])\n        )\n        self.assertSequenceEqual(qs, [a1, a2])\n", "    def setUpTestData(cls):\n        cls.t1 = Tag.objects.create(name=\"t1\")\n        cls.t2 = Tag.objects.create(name=\"t2\", parent=cls.t1)\n\n        cls.n1 = Note.objects.create(note=\"n1\", misc=\"foo\", id=1)\n        cls.ann1 = Annotation.objects.create(name=\"a1\", tag=cls.t1)\n        cls.ann1.notes.add(cls.n1)\n        ann2 = Annotation.objects.create(name=\"a2\", tag=cls.t2)\n        ann2.notes.add(cls.n1)\n", "    def test_ticket_26957(self):\n        \"\"\"\n        When we call select_related() on a model instance that is already\n        deleted, we shouldn't try to access the database.\n        \"\"\"\n        p = Ticket21203Parent.objects.create(parent_bool=True)\n        c = Ticket21203Child.objects.create(parent=p)\n        c.delete()\n        with self.assertNumQueries(0):\n            c.select_related(\"parent\")\n"], "sample_514": ["def test_colorbar_set_scale():\n    # Test the set_scale method and the scale attribute of the colorbar.\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc)\n    cb.set_scale('log')\n    assert cb.ax.get_yscale() == 'log'\n    assert cb.__scale == 'log'\n    cb.set_scale('linear')\n    assert cb.ax.get_yscale() == 'linear'\n    assert cb.__scale == 'linear'\n    cb.set_scale('symlog')\n    assert cb.ax.get_yscale() == 'symlog'\n    assert cb.__scale == 'symlog'\n    cb.set_scale('logit')\n    assert cb.ax.get_yscale() == 'logit'\n    assert cb.__scale == 'logit'\n", "def test_cmap_with_nonuniform_lut():\n    fig, ax = plt.subplots()\n    cmap = plt.get_cmap('viridis', lut=50)\n    x = np.arange(1, 11)\n    y = np.arange(1, 11)\n    X, Y = np.meshgrid(x, y)\n    Z = np.sin(np.sqrt(X**2 + Y**2))\n    im = ax.pcolormesh(X, Y, Z, cmap=cmap)\n    fig.colorbar(im)\n", "def test_colorbar_discrete_cmap():\n    # issue #18610\n    data = np.arange(12).reshape(3, 4)\n    fig, ax = plt.subplots()\n    cmap = plt.get_cmap(\"viridis\", lut=5)\n    pc = ax.imshow(data, cmap=cmap, interpolation='nearest')\n    fig.colorbar(pc)\n    fig.canvas.draw()\n    ticks = [0.5, 1.5, 2.5, 3.5, 4.5]\n    assert np.allclose(pc.norm.boundaries, ticks)\n    np.testing.assert_allclose(ax.get_yticks(), ticks)\n", "def test_colorbar_renorm_no_norm():\n    # Issue #4344 - Test that when we call update_normal and the norm\n    # is None, we can still recover without raising an error\n    fig, ax = plt.subplots()\n    sm = cm.ScalarMappable(cmap='viridis')\n    cb = fig.colorbar(sm)\n    sm.set_norm(None)\n    sm.callbacks.disconnect(sm.colorbar_cid)\n    cb.update_normal(sm)\n    assert cb.norm is None\n", "def test_colorbar_different_locations_of_two_cbars():\n    # Test for issue #23536\n    # This is a regression test to ensure that\n    # two colorbars with same extent have different locations of\n    # the tick locations when 'min' is specified.\n    fig, axs = plt.subplots(1, 2)\n    x = np.linspace(-10, 10, 100)\n    y = np.linspace(-10, 10, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = np.sin(np.sqrt(X**2 + Y**2))\n    pc1 = axs[0].pcolormesh(x, y, Z, cmap='viridis', vmin=-1, vmax=1)\n    pc2 = axs[1].pcolormesh(x, y, Z, cmap='viridis', vmin=-1, vmax=1)\n    cbar1 = fig.colorbar(pc1, ax=axs[0], extend='min')\n    cbar2 = fig.colorbar(pc2, ax=axs[1], extend='min')\n    assert (cbar1.get_ticks() == cbar2.get_ticks()).all()\n"], "sample_222": ["    def test_lock_unlock_exclusive(self):\n        file_path = Path(__file__).parent / 'test_lock.txt'\n        with open(file_path, 'w') as f1, open(file_path, 'w') as f2:\n            self.assertIs(locks.lock(f1, locks.LOCK_EX), True)\n            self.assertIs(locks.lock(f2, locks.LOCK_EX), False)\n            self.assertIs(locks.unlock(f1), True)\n            self.assertIs(locks.lock(f2, locks.LOCK_EX), True)\n", "    def test_lock_exclusive_twice(self):\n        file_path = Path(__file__).parent / 'test.png'\n        with open(file_path, 'r') as f:\n            self.assertIs(locks.lock(f, locks.LOCK_EX), True)\n            self.assertIs(locks.lock(f, locks.LOCK_EX), False)\n", "    def test_lock_unlock(self):\n        with tempfile.NamedTemporaryFile() as tmp:\n            file = File(tmp)\n            self.assertIs(locks.lock(file, locks.LOCK_EX), True)\n            self.assertIs(locks.unlock(file), True)\n", "    def test_lock_ex_unlock(self):\n        with tempfile.NamedTemporaryFile() as temp:\n            self.assertIs(locks.lock(temp, locks.LOCK_EX), True)\n            self.assertIs(locks.unlock(temp), True)\n", "    def test_locking_shared_lock(self):\n        with tempfile.NamedTemporaryFile() as f1, tempfile.NamedTemporaryFile() as f2:\n            self.assertTrue(locks.lock(f1, locks.LOCK_SH))\n            self.assertTrue(locks.lock(f2, locks.LOCK_SH))\n            self.assertTrue(locks.unlock(f1))\n            self.assertTrue(locks.unlock(f2))\n"], "sample_1167": ["def test_latex_TensorIndex():\n    from sympy.tensor.tensor import TensorIndexType\n    L = TensorIndexType(\"L\")\n    i = L(\"i\")\n    assert latex(i) == r\"{}^{i}\"\n    assert latex(-i) == r\"{}_{i}\"\n", "def test_latex_piecewise_function():\n    from sympy import piecewise\n    x = symbols('x')\n    f = piecewise((x**2, x < 0), (x**3, x >= 0))\n    assert latex(f) == r\"\\begin{cases} x^{2} & \\text{for}\\: x < 0 \\\\x^{3} & \\text{otherwise} \\end{cases}\"\n", "def test_latex_gohtic_re_im():\n    assert latex(im(x), gothic_re_im=True) == r'\\Im{\\left(x\\right)}'\n    assert latex(im(x), gothic_re_im=False) == r'\\operatorname{im}{\\left(x\\right)}'\n    assert latex(re(x), gothic_re_im=True) == r'\\Re{\\left(x\\right)}'\n    assert latex(re(x), gothic_re_im=False) == r'\\operatorname{re}{\\left(x\\right)}'\n    assert latex(im(x + y), gothic_re_im=True) == r'\\Im{\\left(x + y\\right)}'\n    assert latex(im(x + y), gothic_re_im=False) == r'\\operatorname{im}{\\left(x + y\\right)}'\n    assert latex(re(x + y), gothic_re_im=True) == r'\\Re{\\left(x + y\\right)}'\n    assert latex(re(x + y), gothic_re_im=False) == r'\\operatorname{re}{\\left(x + y\\right)}'\n    assert latex(im(x * y), gothic_re_im=True) == r'\\Im{\\left(x y\\right)}'\n    assert latex(im(x * y), gothic_re_im=False) == r'\\operatorname{im}{\\left(x y\\right)}'\n    assert latex(re(x * y), gothic_re_im=True) == r'\\Re{\\left(x y\\right)}'\n    assert latex(re(x * y), gothic_re_im=False) == r'\\operatorname{re}{\\left(x y\\right)}'\n    assert latex(im(x - y), gothic_re_im=True) == r'\\Im{\\left(- y + x\\right)}'\n    assert latex(im(x - y), gothic_re_im=False) == r'\\operatorname{im}{\\left(- y + x\\right)}'\n    assert latex(re(x - y), gothic_re_im=True) == r'\\Re{\\left(- y + x\\right)}'\n    assert latex(re(x - y), gothic_re_im=False) == r'\\operatorname{re}{\\left(- y + x\\right)}'\n    assert latex(im(x / y), gothic_re_im=True) == r'\\Im{\\left(\\frac{x}{y}\\right)}'\n    assert latex(im(x / y), gothic_re_im=False) == r'\\operatorname{im}{", "def test_latex_error_message():\n    from sympy import ImmutableDenseNDimArray\n\n        M = ImmutableDenseNDimArray([1, 2, 3])\n        latex(M)\n\n    raises(NotImplementedError, f)\n", "def test_latex_vectors():\n    from sympy import symbols, Matrix\n    n = symbols('n', integer=True)\n    A = MatrixSymbol('A', n, 1)\n    B = MatrixSymbol('B', n, 1)\n    assert latex(A) == r\"A\"\n    assert latex(B) == r\"B\"\n    assert latex(A + B) == r\"A + B\"\n    assert latex(A - B) == r\"A - B\"\n    assert latex(2 * A) == r\"2 A\"\n    assert latex(-2 * A) == r\"- 2 A\"\n    assert latex(A / 2) == r\"\\frac{A}{2}\"\n    assert latex(A.T) == r\"A^{T}\"\n    assert latex(A + 2 * B) == r\"A + 2 B\"\n    assert latex(2 * A - B) == r\"2 A - B\"\n    assert latex(A - 2 * B) == r\"A - 2 B\"\n    assert latex(-A - 2 * B) == r\"- A - 2 B\"\n    assert latex(-A + 2 * B) == r\"- A + 2 B\"\n"], "sample_782": ["def test_column_transformer_sparse_remainder_transformer_fit():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).T\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           remainder=SparseMatrixTrans(),\n                           sparse_threshold=0.8)\n\n    # Call fit before transform\n    ct.fit(X_array)\n    X_trans = ct.transform(X_array)\n    assert sparse.issparse(X_trans)\n    # SparseMatrixTrans creates 3 features for each column. There is\n    # one column in ``transformers``, thus:\n    assert X_trans.shape == (3, 3 + 1)\n\n    exp_array = np.hstack(\n        (X_array[:, 0].reshape(-1, 1), np.eye(3)))\n    assert_array_equal(X_trans.toarray(), exp_array)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert isinstance(ct.transformers_[-1][1], SparseMatrixTrans)\n    assert_array_equal(ct.transformers_[-1][2], [1, 2])\n", "def test_column_transformer_get_feature_names_remainder_transformer():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).T\n\n    X = np.array([[{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}],\n                  [{'c': 5}, {'c': 6}, {'c': 7}]], dtype=object).T\n\n    ct = ColumnTransformer([('col' + str(i), DictVectorizer(), i) for i in range(2)],\n                           remainder=StandardScaler())\n    ct.fit(X)\n    with pytest.raises(NotImplementedError,\n                       match=\"get_feature_names is not yet supported\"):\n        ct.get_feature_names()\n", "def test_column_transformer_named_transformers_attribute():\n    X_array = np.array([[0., 1., 2.], [2., 4., 6.]]).T\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0]),\n                            ('trans2', StandardScaler(with_std=False), [1])])\n\n    # test attribute is not available before fit\n    assert not hasattr(ct, 'named_transformers_')\n\n    ct.fit(X_array)\n    assert hasattr(ct, 'named_transformers_')\n\n    # test attribute is read-only\n    with pytest.raises(AttributeError):\n        ct.named_transformers_ = 'invalid'\n", "def test_column_transformer_transformer_weights():\n    X_array = np.array([[0., 1., 2.], [2., 4., 6.]]).T\n    ct1 = ColumnTransformer([('trans1', Trans(), [0, 1])],\n                            transformer_weights={'trans1': 2})\n    ct2 = ColumnTransformer([('trans1', DoubleTrans(), [0, 1])])\n    assert_array_equal(ct1.fit_transform(X_array),\n                       ct2.fit_transform(X_array))\n    assert_array_equal(ct1.fit(X_array).transform(X_array),\n                       ct2.fit(X_array).transform(X_array))\n\n    # invalid keys\n    X_array = np.array([[0., 1., 2.], [2., 4., 6.]]).T\n    ct1 = ColumnTransformer([('trans1', Trans(), [0, 1])],\n                            transformer_weights={'trans': 2})\n    assert_raise_message(ValueError, \"Transformer trans is not a valid key\",\n                         ct1.fit_transform, X_array)\n    assert_raise_message(ValueError, \"Transformer trans is not a valid key\",\n                         ct1.fit, X_array)\n    assert_raise_message(ValueError, \"Transformer trans is not a valid key\",\n                         ct1.transform, X_array)\n\n    # invalid transformer weights\n    X_array = np.array([[0., 1., 2.], [2., 4., 6.]]).T\n    ct1 = ColumnTransformer([('trans1', Trans(), [0, 1])],\n                            transformer_weights=[2])\n    assert_raise_message(TypeError, \"Invalid transformer_weights\",\n                         ct1.fit_transform, X_array)\n    assert_raise_message(TypeError, \"Invalid transformer_weights\",\n                         ct1.fit, X_array)\n    assert_raise_message(TypeError, \"Invalid transformer_weights\",\n                         ct1.transform, X_array)\n", "def test_column_transformer_warnings():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0]),\n                            ('trans2', StandardScaler(), [1])])\n\n    with pytest.warns(DataConversionWarning):\n        assert_array_equal(ct.fit_transform(X_array), ct.fit(X_array).transform(X_array))\n\n    # test warning when using deprecated 'remainder' transformer in 'make_column_transformer'\n    with pytest.warns(DeprecationWarning,\n                      match='`make_column_transformer` now expects'):\n        make_column_transformer(('drop', 'first'))\n"], "sample_847": ["def test_sparse_input_dtype_enetcv():\n    X, y, _, _ = build_dataset(n_features=10)\n    clf = ElasticNetCV(n_alphas=5)\n    clf.fit(sparse.csr_matrix(X), y)\n    clf1 = ElasticNetCV(n_alphas=5)\n    clf1.fit(sparse.csr_matrix(X, dtype=np.float32), y)\n    assert_almost_equal(clf.alpha_, clf1.alpha_, decimal=6)\n    assert_almost_equal(clf.coef_, clf1.coef_, decimal=6)\n", "def test_sparse_input_dtype_enet():\n    X, y, _, _ = build_dataset(n_features=10)\n    clf = ElasticNet(n_jobs=-1, max_iter=1000)\n    clf.fit(sparse.csr_matrix(X), y)\n    clf1 = ElasticNet(n_jobs=-1, max_iter=1000)\n    clf1.fit(sparse.csr_matrix(X, dtype=np.float32), y)\n    assert_almost_equal(clf.alpha_, clf1.alpha_, decimal=6)\n    assert_almost_equal(clf.coef_, clf1.coef_, decimal=6)\n", "def test_enet_path_2():\n    # We use a large number of samples and of informative features so that\n    # the l1_ratio selected is more toward ridge than lasso\n    X, y, X_test, y_test = build_dataset(n_samples=200, n_features=100,\n                                         n_informative_features=100)\n    max_iter = 150\n\n    # Here we have a small number of iterations, and thus the\n    # ElasticNet might not converge. This is to speed up tests\n    clf = ElasticNetCV(n_alphas=50, eps=1e-3, max_iter=max_iter,\n                       l1_ratio=0.5, tol=1e-3)\n    ignore_warnings(clf.fit)(X, y)  # new params\n    assert_almost_equal(0.5, clf.l1_ratio)\n    assert 50 == clf.n_alphas\n    assert 50 == len(clf.alphas_)\n\n    # Multi-output/target case\n    X, y, X_test, y_test = build_dataset(n_features=10, n_targets=3)\n    clf = MultiTaskElasticNetCV(n_alphas=5, eps=2e-3, l1_ratio=[0.5, 0.7],\n                                cv=3, max_iter=max_iter)\n    ignore_warnings(clf.fit)(X, y)\n    # We are in well-conditioned settings with low noise: we should\n    # have a good test-set performance\n    assert clf.score(X_test, y_test) > 0.99\n    assert clf.coef_.shape == (3, 10)\n\n    # Mono-output should have same cross-validated alpha_ and l1_ratio_\n    # in both cases.\n    X, y, _, _ = build_dataset(n_features=10)\n    clf1 = ElasticNetCV(n_alphas=5, eps=2e-3, l1_ratio=[0.5, 0.7])\n    clf1.fit(X, y)\n    clf2 = MultiTaskElasticNetCV(n_alphas=5, eps=2e-3, l1_ratio=[0.5, 0.7])\n    clf2.fit(X, y[:, np.newaxis])\n    assert_almost_equal(clf1.l1_ratio_, clf2.l1_ratio_)\n    assert_almost_equal(clf1.alpha_, clf2.alpha_)\n", "def test_enet_path_multitask_copy_X():\n    X, y, _, _ = build_dataset(n_samples=20, n_features=10, n_targets=2)\n    X = X.copy(order='F')\n\n    original_X = X.copy()\n    enet = MultiTaskElasticNet(copy_X=True)\n    enet.fit(X, y)\n\n    assert_array_equal(original_X, X)\n\n    enet = MultiTaskElasticNet(copy_X=False)\n    enet.fit(X, y)\n\n    assert np.any(np.not_equal(original_X, X))\n", "def test_lasso_path_return_models():\n    # Test that lasso_path with return_models=True is giving the same result\n    # as lasso_path with return_models=False\n\n    # Some toy data\n    X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n    y = np.array([1, 2, 3.1])\n    alphas = [5., 1., .5]\n\n    # Use lasso_path with return_models=True to compute the same path\n    alphas_lasso, models_lasso, _ = lasso_path(X, y, alphas=alphas,\n                                                return_models=True)\n    # Convert alphas to have the same shape as alphas_lasso\n    alphas = np.array(alphas)[:, np.newaxis]\n    assert_array_almost_equal(alphas, alphas_lasso)\n\n    # Convert models_lasso to an array of coefficients\n    coefs_lasso = np.array([model.coef_ for model in models_lasso])\n    # Use lasso_path with return_models=False to compute the same path\n    alphas_lasso2, coefs_lasso2, _ = lasso_path(X, y, alphas=alphas,\n                                                 return_models=False)\n    # Convert alphas to have the same shape as alphas_lasso\n    alphas = np.array(alphas)[:, np.newaxis]\n    assert_array_almost_equal(alphas, alphas_lasso2)\n    # Compare coefs_lasso and coefs_lasso2\n    assert_array_almost_equal(coefs_lasso, coefs_lasso2)\n"], "sample_651": ["def test_re_emit_unmatched_warning_within_unmatched_warning() -> None:\n    with pytest.warns(UserWarning):\n        with pytest.warns(UserWarning, match=\"v1 warning\"):\n            warnings.warn(\"v1 warning\", UserWarning)\n            with pytest.warns(UserWarning, match=\"v2 warning\"):\n                warnings.warn(\"v2 warning\", UserWarning)\n                warnings.warn(\"v3 warning\", UserWarning)\n", "    def test_clear(self, recwarn: WarningsRecorder) -> None:\n        \"\"\"Test that the clear method of WarningsRecorder works correctly.\"\"\"\n        warnings.warn(\"hello\")\n        assert len(recwarn.list) == 1\n        recwarn.clear()\n        assert len(recwarn.list) == 0\n        warnings.warn(\"hello\")\n        assert len(recwarn.list) == 1\n        recwarn.clear()\n        assert len(recwarn.list) == 0\n", "def test_warns_re_emit_invalid_match(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        import warnings\n\n            with pytest.warns(UserWarning, match=\"abc\"):\n                with pytest.warns(UserWarning, match=\"abc\"):\n                    warnings.warn(\"abc\", UserWarning)\n                    warnings.warn(\"def\", UserWarning)\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*1 failed, 0 passed in*\"])\n    result.stdout.fnmatch_lines(\n        [\"*DID NOT WARN. No warnings of type UserWarning matching the regex were emitted.*\"]\n    )\n", "def test_warns_exit_stacklevel(recwarn: WarningsRecorder) -> None:\n    \"\"\"Test that WarningsChecker's exit function correctly sets __tracebackhide__.\"\"\"\n    with pytest.warns(RuntimeWarning, match=r\"runtime\") as record:\n        warnings.warn(\"runtime\", RuntimeWarning, stacklevel=2)\n    assert record.__tracebackhide__\n", "    def test_warningsRecorder_reentrant(self, recwarn: WarningsRecorder) -> None:\n        with recwarn:\n            warnings.warn(\"first warning\")\n            with recwarn:\n                warnings.warn(\"second warning\")\n                assert len(recwarn) == 2\n                recwarn.pop()\n                assert len(recwarn) == 1\n        assert len(recwarn) == 2\n"], "sample_1102": ["def test_factor_irreducible_polynomial():\n    f = x**3 + 3*x**2 + 4*x + 3\n    assert Poly(f).factor_list() == (3, [(Poly(f, x), 1)])\n    assert Poly(f, domain='QQ').factor_list() == (3, [(Poly(f, x), 1)])\n    assert Poly(f, domain='ZZ').factor_list() == (3, [(Poly(f, x), 1)])\n", "def test_issue_18613():\n    assert Poly(x/2 + 1, x).as_poly() is None\n    assert Poly(x**2 + x, x).as_poly() == Poly(x**2 + x, x)\n    assert Poly(sqrt(x*y), x).as_poly() == Poly(sqrt(x*y), x, sqrt(x*y))\n    assert Poly(sin(x), x).as_poly() == Poly(sin(x), x, sin(x))\n    assert Poly(2*x + 3*y, x, y).as_poly() == Poly(2*x + 3*y, x, y)\n    assert Poly(sin(x), x, domain='EX').as_poly() == Poly(sin(x), x, sin(x))\n", "def test_factor_list_univariate_polynomial():\n    assert factor_list(x**4 + 3*x**3 - x**2 - 3*x - 2, x, extension=True) == \\\n        (1, [(Poly(x + 1, x, extension=True), 1), (Poly(x - 1, x, extension=True), 1),\n         (Poly(x**2 + 3*x + 2, x, extension=True), 1)])\n\n    assert factor_list(x**4 + 3*x**3 - x**2 - 3*x - 2, x, extension=[I]) == \\\n        (1, [(Poly(x + 1, x, extension=[I]), 1), (Poly(x - 1, x, extension=[I]), 1),\n         (Poly(x**2 + 3*x + 2, x, extension=[I]), 1)])\n\n    assert factor_list(x**4 + 3*x**3 - x**2 - 3*x - 2, x, extension=[sqrt(2)]) == \\\n        (1, [(Poly(x + 1, x, extension=[sqrt(2)]), 1), (Poly(x - 1, x, extension=[sqrt(2)]), 1),\n         (Poly(x**2 + 3*x + 2, x, extension=[sqrt(2)]), 1)])\n\n    assert factor_list(x**4 + 3*x**3 - x**2 - 3*x - 2, x, extension=[sqrt(2), sqrt(3)]) == \\\n        (1, [(Poly(x + 1, x, extension=[sqrt(2), sqrt(3)]), 1), (Poly(x - 1, x, extension=[sqrt(2), sqrt(3)]), 1),\n         (Poly(x**2 + 3*x + 2, x, extension=[sqrt(2), sqrt(3)]), 1)])\n", "def test_issue_14897():\n    assert poly(sqrt(5)*sqrt(2) + x) == Poly(sqrt(5)*sqrt(2) + x, x, domain='EX')\n    assert Poly(sqrt(2) + sqrt(5)*x, x, domain='EX').is_monogenic is True\n    assert Poly(sqrt(2)*x + sqrt(5)*x, x, domain='EX').is_monogenic is True\n    assert Poly(sqrt(2) + sqrt(5)*x + x, x, domain='EX').is_monogenic is True\n    assert Poly(sqrt(2)*x + sqrt(5)*x + x, x, domain='EX').is_monogenic is False\n", "def test_factor_extension():\n    # issue 13224\n    f = x**4 + 3*x**2 + x + 1\n    assert factor(f, extension=sqrt(2)) == (x**2 + sqrt(2)*x + 1)*(x**2 - sqrt(2)*x + 1)\n    assert factor(f, extension=2**Rational(1, 4)) == (x**2 + x + 1)*(x**2 + x + 1)\n    assert factor(f, extension=2) == (x**2 + 2*x + 1)*(x**2 - 2*x + 1)\n    assert factor(f, extension=1) == x**4 + 3*x**2 + x + 1\n    assert factor(f, extension=True) == x**4 + 3*x**2 + x + 1\n    assert factor(f, extension=1.0) == x**4 + 3*x**2 + x + 1\n    assert factor(f, extension=None) == x**4 + 3*x**2 + x + 1\n    assert factor(f, extension=False) == x**4 + 3*x**2 + x + 1\n"], "sample_495": ["def test_empty_page_range_iterator(self):\n    \"\"\"\n    Paginator.page_range should be an iterator even if it's empty.\n    \"\"\"\n    paginator = Paginator([], 2)\n    page_range = paginator.page_range\n    self.assertIsInstance(page_range, type(range(0)))\n    with self.assertRaises(StopIteration):\n        next(page_range)\n", "def test_page_iteration(self):\n    \"\"\"\n    A paginator page is iterable.\n    \"\"\"\n    eleven = 'abcdefghijk'\n    paginator = Paginator(eleven, per_page=5, orphans=1)\n    page2 = paginator.page(2)\n    page_iterator = iter(page2)\n    self.assertEqual(next(page_iterator), 'f')\n    self.assertEqual(next(page_iterator), 'g')\n    self.assertEqual(next(page_iterator), 'h')\n    self.assertEqual(next(page_iterator), 'i')\n    self.assertEqual(next(page_iterator), 'j')\n    self.assertEqual(next(page_iterator), 'k')\n    with self.assertRaises(StopIteration):\n        next(page_iterator)\n", "    def test_page_sequence_out_of_range(self):\n        \"\"\"\n        Paginator page sequence raises IndexError when index is out of range.\n        \"\"\"\n        eleven = 'abcdefghijk'\n        paginator = Paginator(eleven, per_page=5, orphans=1)\n        page1 = paginator.page(1)\n        with self.assertRaises(IndexError):\n            page1[len(page1)]\n        with self.assertRaises(IndexError):\n            page1[-(len(page1) + 1)]\n        with self.assertRaises(IndexError):\n            page1[len(page1):]\n        with self.assertRaises(IndexError):\n            page1[-(len(page1) + 1):]\n", "def test_page_range_with_zero_orphan(self):\n    \"\"\"Paginator page_range with zero orphan.\"\"\"\n    paginator = Paginator([1, 2, 3, 4], 3, orphans=0)\n    self.assertEqual(list(paginator.page_range), [1, 2])\n    self.assertEqual(paginator.num_pages, 2)\n    self.assertEqual(paginator.count, 4)\n", "def test_page_has_previous_next_pages_orphan(self):\n    \"\"\"\n    Test that a page has correct has_previous and has_next behavior\n    with orphan pages\n    \"\"\"\n    eleven = 'abcdefghijk'\n    paginator = Paginator(eleven, per_page=5, orphans=1)\n    page1 = paginator.page(1)\n    page2 = paginator.page(2)\n\n    self.assertFalse(page1.has_previous())\n    self.assertTrue(page1.has_next())\n    self.assertTrue(page1.has_other_pages())\n\n    self.assertTrue(page2.has_previous())\n    self.assertFalse(page2.has_next())\n    self.assertTrue(page2.has_other_pages())\n\n    # Test with 0 orphans, should behave the same\n    paginator = Paginator(eleven, per_page=5, orphans=0)\n    page1 = paginator.page(1)\n    page2 = paginator.page(2)\n\n    self.assertFalse(page1.has_previous())\n    self.assertTrue(page1.has_next())\n    self.assertTrue(page1.has_other_pages())\n\n    self.assertTrue(page2.has_previous())\n    self.assertFalse(page2.has_next())\n    self.assertTrue(page2.has_other_pages())\n"], "sample_553": ["def test_adjusted_figsize():\n    dpi = 100\n    n = 2\n    w, h = adjusted_figsize(10, 20, dpi, n)\n    assert int(w * dpi) % n == 0\n    assert int(h * dpi) % n == 0\n", "def test_animation_repr_html_embed_limit(tmpdir, anim):\n    if platform.python_implementation() == 'PyPy':\n        np.testing.break_cycles()\n    with mpl.rc_context({\"animation.embed_limit\": 1e-6}):  # ~1 byte.\n        with pytest.warns(UserWarning, match=r'Animation size has reached'):\n            anim._repr_html_()\n", "def test_save_animation_with_extra_anim(anim):\n    fig = anim._fig\n    ax1 = fig.add_subplot(1, 2, 1)\n    ax2 = fig.add_subplot(1, 2, 2)\n\n    line1, = ax1.plot([], [])\n    line2, = ax2.plot([], [])\n\n    ax1.set_xlim(0, 10)\n    ax1.set_ylim(-1, 1)\n    ax2.set_xlim(0, 10)\n    ax2.set_ylim(-1, 1)\n\n        line1.set_data([], [])\n        line2.set_data([], [])\n        return line1, line2,\n\n        x = np.linspace(0, 10, 100)\n        y1 = np.sin(x + i)\n        y2 = np.cos(x + i)\n        line1.set_data(x, y1)\n        line2.set_data(x, y2)\n        return line1, line2,\n\n    anim2 = animation.FuncAnimation(fig, animate, init_func=init, frames=5)\n\n    anim.save(\"test.mp4\", writer='ffmpeg', fps=30, bitrate=500, dpi=100,\n              extra_anim=[anim2])\n", "def test_save_animation_progress_callback(tmpdir, anim):\n    # Test saving an animation with a progress callback.\n\n    progress_callback_was_called = False\n\n        nonlocal progress_callback_was_called\n        progress_callback_was_called = True\n        assert current_frame == 0\n        assert total_frames == anim._save_count\n\n    anim.save('unused.null', writer=NullMovieWriter(),\n              progress_callback=progress_callback)\n\n    assert progress_callback_was_called\n", "def test_html5_video_embed_limit_with_tight_layout(anim, caplog):\n    caplog.set_level(\"WARNING\")\n    with plt.rc_context({\"animation.embed_limit\": 1e-6}):  # ~1 byte.\n        with plt.tight_layout(pad=0):\n            anim.to_html5_video()\n    assert len(caplog.records) == 1\n    record, = caplog.records\n    assert (record.name == \"matplotlib.animation\"\n            and record.levelname == \"WARNING\")\n"], "sample_618": ["compilation error", "def test_cross_incompatible_sizes() -> None:\n    a = xr.DataArray(\n        np.array([[1, 2, 3], [4, 5, 6]]),\n        dims=(\"time\", \"cartesian\"),\n        coords=dict(\n            time=([\"time\"], [0, 1]),\n            cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"]),\n        ),\n    )\n    b = xr.DataArray(\n        np.array([4, 5]),\n        dims=[\"cartesian\"],\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\"])),\n    )\n    with pytest.raises(ValueError, match=r\"dimensions without coordinates must have a length of 2 or 3\"):\n        xr.cross(a, b, dim=\"cartesian\")\n", "def test_polyval_coords(x, coeffs, expected):\n    actual = xr.polyval(x, coeffs)\n    xr.testing.assert_allclose(actual, expected)\n    assert actual.coords[\"x\"].attrs == expected.coords[\"x\"].attrs\n", "def test_cross_dataset(a, b, ae, be, dim, axis) -> None:\n    # Test cross on Datasets\n    ds_a = a.to_dataset(dim=\"cartesian\")\n    ds_b = b.to_dataset(dim=\"cartesian\")\n    expected = xr.cross(a, b, dim=dim).to_dataset(dim=\"cartesian\")\n\n    actual = xr.cross(ds_a, ds_b, dim=dim)\n    xr.testing.assert_identical(actual, expected)\n", "compilation error"], "sample_241": ["    def test_init(self):\n        final_field = 'field'\n        targets = ['target']\n        opts = 'opts'\n        joins = ['join']\n        path = 'path'\n        transform_function = 'function'\n\n        info = JoinInfo(final_field, targets, opts, joins, path, transform_function)\n\n        self.assertEqual(info.final_field, final_field)\n        self.assertEqual(info.targets, targets)\n        self.assertEqual(info.opts, opts)\n        self.assertEqual(info.joins, joins)\n        self.assertEqual(info.path, path)\n        self.assertEqual(info.transform_function, transform_function)\n", "    def test_add_to_dict(self):\n        data = {}\n        add_to_dict(data, 'a', 1)\n        add_to_dict(data, 'a', 2)\n        add_to_dict(data, 'b', 3)\n        self.assertEqual(data, {'a': {1, 2}, 'b': {3}})\n", "    def test_resolve_ref(self):\n        qs = Company.objects.annotate(\n            col=ExpressionWrapper(F('num_employees') + F('num_chairs'), output_field=IntegerField()),\n        ).values('col')\n        self.assertEqual(str(qs.query).count('num_employees'), 2)\n", "    def test_resolve_ref_recursive_alias(self):\n        # When an F expression contains a reference to another F expression,\n        # its alias is resolved correctly.\n        self.max.point_of_contact = self.max\n        self.max.save()\n        qs = Employee.objects.annotate(\n            pk_alias=F('pk'),\n            ref_alias=F('pk_alias'),\n        )\n        self.assertEqual(qs.get().ref_alias, self.max.pk)\n", "    def test_clone(self):\n        qs = Company.objects.all()\n        cloned_qs = qs.clone()\n        self.assertEqual(qs.__class__, cloned_qs.__class__)\n        self.assertEqual(qs.model, cloned_qs.model)\n        self.assertEqual(qs.where, cloned_qs.where)\n        self.assertEqual(qs.select, cloned_qs.select)\n        self.assertEqual(qs.low_mark, cloned_qs.low_mark)\n        self.assertEqual(qs.high_mark, cloned_qs.high_mark)\n        self.assertEqual(qs.select_related, cloned_qs.select_related)\n        self.assertEqual(qs.extra, cloned_qs.extra)\n"], "sample_1110": ["def test_AbstractPythonCodePrinter__format_code():\n    prntr = AbstractPythonCodePrinter()\n    lines = ['def foo():', '    pass']\n    formatted_code = prntr._format_code(lines)\n    assert formatted_code == 'def foo():\\n    pass'\n\n    lines = ['def foo():', '    pass\\n  # comment']\n    formatted_code = prntr._format_code(lines)\n    assert formatted_code == 'def foo():\\n    pass  # comment'\n", "def test_parenthesize():\n    prntr = PythonCodePrinter()\n\n    # Test simple cases\n    assert prntr.parenthesize('x + y') == '(x + y)'\n    assert prntr.parenthesize('(x + y)') == '(x + y)'\n    assert prntr.parenthesize('x') == 'x'\n\n    # Test with precedence\n    assert prntr.parenthesize('x + y', precedence('+')) == '(x + y)'\n    assert prntr.parenthesize('x', precedence('+')) == 'x'\n\n    # Test with strict=False\n    assert prntr.parenthesize('x + y', precedence('+'), strict=False) == 'x + y'\n    assert prntr.parenthesize('(x + y)', precedence('+'), strict=False) == '(x + y)'\n", "def test_AbstractPythonCodePrinter__print_FunctionDefinition():\n    prntr = AbstractPythonCodePrinter()\n\n    from sympy import symbols\n    x, y = symbols('x y')\n\n    # Test single line body\n    fd = Assignment(x, y)\n    assert prntr._print_FunctionDefinition(fd) == 'def x(y):\\n    y'\n\n    # Test multi line body\n    fd = Assignment(x, y)\n    fd.body = [Assignment(x, y), Assignment(y, x)]\n    assert prntr._print_FunctionDefinition(fd) == 'def x(y):\\n    y\\n    x'\n\n    # Test empty body\n    fd = Assignment(x, y)\n    fd.body = []\n    assert prntr._print_FunctionDefinition(fd) == 'def x(y):\\n    None'\n", "def test_AbstractPythonCodePrinter():\n    prntr = AbstractPythonCodePrinter()\n    assert prntr._format_code(['a']) == ['a']\n    assert prntr._get_statement('a') == 'a'\n    assert prntr._get_comment('a') == '  # a'\n    assert prntr._indent_codestring('a') == '    a'\n\n    assert prntr._declare_number_const('x', '2') == 'x = 2'\n    assert prntr._module_format('numpy') == 'numpy'\n    assert prntr._expand_fold_binary_op('numpy.add', [1, 2, 3]) == \"numpy.add(numpy.add(1, 2), 3)\"\n    assert prntr._expand_reduce_binary_op('numpy.add', [1, 2, 3, 4]) == \"numpy.add(numpy.add(1, 2), numpy.add(3, 4))\"\n\n    assert prntr._print_Infinity(S.Infinity) == 'float(\\'inf\\')'\n    assert prntr._print_NegativeInfinity(S.NegativeInfinity) == 'float(\\'-inf\\')'\n    assert prntr._print_NaN(S.NaN) == 'float(\\'nan\\')'\n    assert prntr._print_ComplexInfinity(S.ComplexInfinity) == 'float(\\'nan\\')'\n    assert prntr._print_Mod(Mod(x, 2)) == 'x % 2'\n    assert prntr._print_Piecewise((1, Eq(x, 0)), (2, x>6)) == '((1) if (x == 0) else (2) if (x > 6) else None)'\n    assert prntr._print_Piecewise((2, Le(x, 0)), (3, Gt(x, 0)), evaluate=False) == '((2) if (x <= 0) else (3) if (x > 0) else None)'\n    assert prntr._print_Relational(Eq(x, y)) == '(x == y)'\n    assert prntr._print_ITE(Piecewise((1, Eq(x, 0)), (2, x>6))) == '((1) if (x == 0) else (2) if (x > 6) else None)'\n    assert prntr._print_Sum(x, (x, ", "def test_AbstractPythonCodePrinter():\n    prntr = AbstractPythonCodePrinter()\n    \n    assert prntr._print_Infinity(S.Infinity) == 'float(\\'inf\\')'\n    assert prntr._print_NegativeInfinity(S.NegativeInfinity) == 'float(\\'-inf\\')'\n    assert prntr._print_ComplexInfinity(S.ComplexInfinity) == 'float(\\'nan\\')'\n    assert prntr._print_NaN(S.NaN) == 'float(\\'nan\\')'\n    assert prntr._print_KroneckerDelta(KroneckerDelta(x, y)) == '(1 if x == y else 0)'\n    assert prntr._print_ITE(Piecewise((x, y > 0), (z, True))) == '((x) if (y > 0) else (z) if True else None)'\n    assert prntr._print_Sum(x**y, (x, 0, 5)) == '(builtins.sum((x**y for x in range(0, 5+1)))'\n    assert prntr._print_ImaginaryUnit(S.ImaginaryUnit) == '1j'\n    assert prntr._print_FunctionDefinition(Assignment(x, x + y)) == 'def x(x, y):\\n    x = x + y'\n    assert prntr._print_Declaration(Assignment(x, x + y)) == 'x = x + y'\n    assert prntr._print_Return(Assignment(x, x + y)) == 'return x + y'\n    assert prntr._print_Print(Assignment(x, x + y)) == 'print(x + y)'\n    assert prntr._print_CodegenArrayTensorProduct(Assignment(x, x + y)) == 'numpy.einsum(\"ij,ij->ij\", x + y, \"[0, 0]\")'\n    assert prntr._print_CodegenArrayContraction(Assignment(x, x + y)) == 'numpy.einsum(\"ij,jk->ik\", x + y)'\n    assert prntr._print_CodegenArrayDiagonal(Assignment(x, x + y)) == 'numpy.diagonal(x + y, 0, axis1=0, axis2=1)'\n    assert prntr._print_CodegenArrayPermuteDims(Assignment(x, x + y)) == 'numpy.transpose(x"], "sample_1194": ["def test_julia_Idx_printing():\n    # test cases for issue #11821\n    A = MatrixSymbol(\"A\", 1, 3)\n    i = Symbol('i')\n    assert(julia_code(A[0, i]) == \"A[1,i + 1]\")\n    assert(julia_code(A[i, 0]) == \"A[i + 1,1]\")\n    assert(julia_code(A[i, i]) == \"A[i + 1,i + 1]\")\n", "def test_Indexed():\n    from sympy.tensor import IndexedBase, Idx\n    i = Idx('i', 10)\n    j = Idx('j', 10)\n    A = IndexedBase('A', (10, 10))\n    assert julia_code(A[i, j]) == \"A[i, j]\"\n    assert julia_code(A[i, j] + 3) == \"A[i, j] + 3\"\n    assert julia_code(3*A[i, j]) == \"3 * A[i, j]\"\n", "def test_MatMul():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    C = MatrixSymbol('C', 3, 3)\n    D = MatrixSymbol('D', 3, 3)\n    assert julia_code(A*B) == \"A * B\"\n    assert julia_code(3*A*B) == \"3 * A * B\"\n    assert julia_code(A*(B + 3*Identity(3))) == \"A * (3 * eye(3) + B)\"\n    assert julia_code(A*(B + C)) == \"A * (B + C)\"\n    assert julia_code(A*(B*C)) == \"A * B * C\"\n    assert julia_code((A*B)*C) == \"A * B * C\"\n    assert julia_code(A*(B + C)*D) == \"A * (B + C) * D\"\n    assert julia_code((A*B)*(C + D)) == \"A * B * (C + D)\"\n", "def test_julia_Pow_on_constants():\n    # Make sure we use '^' with two number constants, but '.*' for any with a Symbol\n    assert julia_code(Rational(2, 3)**2) == \"(2 // 3) ^ 2\"\n    assert julia_code(3**Rational(2, 3)) == \"3 ^ (2 // 3)\"\n    assert julia_code(x**Rational(2, 3)) == \"x .^ (2 // 3)\"\n    assert julia_code(Rational(2, 3)**x) == \"(2 // 3) .^ x\"\n    assert julia_code(Rational(2, 3)**(x**2)) == \"(2 // 3) .^ (x .^ 2)\"\n    assert julia_code(Rational(2, 3)**Rational(3, 4)) == \"(2 // 3) ^ (3 // 4)\"\n    assert julia_code(Rational(2, 3)**(Rational(3, 4)**2)) == \"(2 // 3) ^ ((3 // 4) ^ 2)\"\n    assert julia_code(Rational(2, 3)**(Rational(3, 4)**Rational(5, 6))) == \"(2 // 3) ^ ((3 // 4) ^ (5 // 6))\"\n    assert julia_code((Rational(2, 3)**Rational(3, 4))**Rational(5, 6)) == \"((2 // 3) ^ (3 // 4)) ^ (5 // 6)\"\n", "def test_julia_matrix_hadamard_product():\n    # Check for HadamardProduct in matrix and matrix_element\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    C = MatrixSymbol('C', 3, 3)\n    assert julia_code(A*B) == \"A * B\"\n    assert julia_code(A*B*C) == \"A * B * C\"\n    assert julia_code(A*B + C) == \"(A * B) + C\"\n    assert julia_code(A*(B + C)) == \"A * (B + C)\"\n    assert julia_code(A*B*C**2) == \"A * B * (C ^ 2)\"\n    assert julia_code(A*B*C + A*C) == \"(A * B * C) + A * C\"\n    assert julia_code((A*B)*C) == \"(A * B) * C\"\n    assert julia_code(A*(B*C)) == \"A * (B * C)\"\n    assert julia_code(A**2*B*C) == \"(A ^ 2) * B * C\"\n    assert julia_code(A*B**2*C) == \"A * (B ^ 2) * C\"\n    assert julia_code(A*B*C**2) == \"A * B * (C ^ 2)\"\n    assert julia_code((A+B)*C) == \"(A + B) * C\"\n    assert julia_code(A*(B+C)) == \"A * (B + C)\"\n    assert julia_code(A*B + B*C) == \"(A * B) + B * C\"\n    assert julia_code(A*B + A*C) == \"(A * B) + A * C\"\n    assert julia_code(A*B + B) == \"(A * B) + B\"\n    assert julia_code(A*B + C) == \"(A * B) + C\"\n    assert julia_code(A + B*C) == \"A + B * C\"\n    assert julia_code(A + B + C) == \"A + B + C\"\n    assert julia_code(A*B + B*C + C) == \"(A * B) + B * C + C\"\n    assert julia_code(A*B + A*C + C) == \"(A * B) + A * C + C\"\n    assert julia_code(A"], "sample_479": ["def test_create_alter_table_comment(self):\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                \"Foo\", [(\"name\", models.CharField(max_length=255))]\n            ),\n            migrations.AlterModelTableComment(\"Foo\", \"Table comment\"),\n            migrations.AlterModelTableComment(\"Foo\", \"New table comment\"),\n        ],\n        [\n            migrations.CreateModel(\n                \"Foo\", [(\"name\", models.CharField(max_length=255))]\n            ),\n            migrations.AlterModelTableComment(\"Foo\", \"New table comment\"),\n        ],\n    )\n", "def test_create_model_remove_model_options(self):\n    \"\"\"\n    RemoveModelOptions should optimize into CreateModel.\n    \"\"\"\n    managers = [(\"objects\", EmptyManager())]\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[(\"name\", models.CharField(max_length=255))],\n                options={\"verbose_name\": \"Foo\"},\n                bases=(UnicodeModel,),\n                managers=managers,\n            ),\n            migrations.RemoveModelOptions(\"Foo\", options={\"verbose_name\": \"Foo\"}),\n        ],\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[(\"name\", models.CharField(max_length=255))],\n                bases=(UnicodeModel,),\n                managers=managers,\n            ),\n        ],\n    )\n", "def test_alter_model_table_comment(self):\n    \"\"\"\n    AlterModelTableComment should optimize into CreateModel.\n    \"\"\"\n    managers = [(\"objects\", EmptyManager())]\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[(\"name\", models.CharField(max_length=255))],\n                options={\"verbose_name\": \"Foo\"},\n                bases=(UnicodeModel,),\n                managers=managers,\n            ),\n            migrations.AlterModelTableComment(\"Foo\", \"This is a comment\"),\n        ],\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[(\"name\", models.CharField(max_length=255))],\n                options={\"verbose_name\": \"Foo\", \"db_table_comment\": \"This is a comment\"},\n                bases=(UnicodeModel,),\n                managers=managers,\n            ),\n        ],\n    )\n", "def test_create_model_remove_index(self):\n    \"\"\"\n    RemoveIndex should optimize into CreateModel.\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[\n                    (\"name\", models.CharField(max_length=255)),\n                ],\n                options={\"indexes\": [models.Index(fields=[\"name\"])]},\n            ),\n            migrations.RemoveIndex(\"Foo\", \"Foo_name_3f58a3_idx\"),\n        ],\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[\n                    (\"name\", models.CharField(max_length=255)),\n                ],\n            ),\n        ],\n    )\n", "def test_remove_constraint_add_constraint(self):\n    \"\"\"\n    RemoveConstraint should cancel AddConstraint\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.AddConstraint(\n                \"Foo\", models.CheckConstraint(check=models.Q(age__gt=18), name=\"age_check\")\n            ),\n            migrations.RemoveConstraint(\"Foo\", \"age_check\"),\n        ],\n        [],\n    )\n\n    self.assertOptimizesTo(\n        [\n            migrations.RemoveConstraint(\"Foo\", \"age_check\"),\n            migrations.AddConstraint(\n                \"Foo\", models.CheckConstraint(check=models.Q(age__gt=18), name=\"age_check\")\n            ),\n        ],\n        [\n            migrations.AddConstraint(\n                \"Foo\", models.CheckConstraint(check=models.Q(age__gt=18), name=\"age_check\")\n            ),\n        ],\n    )\n"], "sample_617": ["def test_cross_1d_and_3d() -> None:\n    # test 1d and 3d array cross product\n    a = xr.DataArray(\n        np.array([1, 2, 3]),\n        dims=[\"cartesian\"],\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])),\n    )\n    b = xr.DataArray(\n        np.arange(12).reshape(4, 3),\n        dims=(\"time\", \"cartesian\"),\n        coords=dict(\n            time=([\"time\"], [1, 2, 3, 4]),\n            cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"]),\n        ),\n    )\n    expected = xr.DataArray(\n        np.cross(np.array([1, 2, 3]), np.arange(12).reshape(4, 3), axis=1),\n        dims=(\"time\", \"cartesian\"),\n        coords=dict(\n            time=([\"time\"], [1, 2, 3, 4]),\n            cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"]),\n        ),\n    )\n    actual = xr.cross(a, b, dim=\"cartesian\")\n    assert_identical(expected, actual)\n", "def test_polyval_with_multidimensional_variables():\n    x = xr.DataArray(\n        np.array([1, 2, 3, 4]),\n        dims=[\"x\", \"y\"],\n        coords={\"x\": [0, 0], \"y\": [0, 1]},\n    )\n    coeffs = xr.DataArray(\n        np.array([[0, 1], [2, 3]]),\n        dims=[\"degree\", \"z\"],\n        coords={\"degree\": [0, 1], \"z\": [0, 1]},\n    )\n    expected = xr.DataArray(\n        np.array([[2, 5], [8, 11]]),\n        dims=[\"x\", \"y\"],\n        coords={\"x\": [0, 0], \"y\": [0, 1]},\n    )\n    actual = xr.polyval(x, coeffs, degree_dim=\"degree\")\n    xr.testing.assert_allclose(actual, expected)\n", "def test_cross_size_mismatch(dim, coords, a_shape, b_shape, expected_shape):\n    a = xr.DataArray(np.arange(np.prod(a_shape)).reshape(a_shape), dims=coords)\n    b = xr.DataArray(np.arange(np.prod(b_shape)).reshape(b_shape), dims=coords)\n    actual = xr.cross(a, b, dim=dim)\n    assert actual.shape == expected_shape\n", "def test_cross_dimsize_mismatch(use_dask) -> None:\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"test for dask.\")\n\n    # Test with mismatched cartesian sizes\n    a = xr.DataArray(\n        [[1, 2, 3], [2, 3, 4]],\n        dims=(\"time\", \"cartesian\"),\n        coords=dict(\n            time=([\"time\"], [0, 1]),\n            cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"]),\n        ),\n    )\n    b = xr.DataArray(\n        [[1, 2, 3], [2, 3, 4], [3, 4, 5]],\n        dims=(\"time\", \"cartesian\"),\n        coords=dict(\n            time=([\"time\"], [0, 1, 2]),\n            cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"]),\n        ),\n    )\n\n    if use_dask:\n        a = a.chunk()\n        b = b.chunk()\n\n    with pytest.raises(ValueError, match=r\"The size of 'cartesian' on a must be\"):\n        xr.cross(a, b, dim=\"cartesian\")\n", "def test_cross_errors() -> None:\n    dim = \"cartesian\"\n    a = xr.DataArray(\n        [1, 2],\n        dims=[dim],\n        coords=[(\"cartesian\", [\"x\", \"z\"])],\n    )\n    b = xr.DataArray(\n        [4, 5, 6],\n        dims=[dim],\n        coords=[(\"cartesian\", [\"x\", \"y\", \"z\"])],\n    )\n\n    # Size is not 1, 2, or 3\n    with pytest.raises(ValueError, match=\"must be 1, 2, or 3\"):\n        xr.cross(\n            xr.DataArray(\n                [1, 2, 3, 4],\n                dims=[dim],\n                coords=[(\"cartesian\", [\"x\", \"y\", \"z\", \"t\"])],\n            ),\n            b,\n            dim=dim,\n        )\n\n    # Both arrays have different sizes\n    with pytest.raises(\n        ValueError,\n        match=\"arrays have different sizes, but don't have coords to fill zeros\",\n    ):\n        xr.cross(\n            xr.Variable([1, 2], dims=[dim]),\n            xr.Variable([4, 5, 6], dims=[dim]),\n            dim=dim,\n        )\n\n    # Both arrays have same size of 2, but different coordinates\n    with pytest.raises(\n        ValueError,\n        match=\"arrays have different sizes, but don't have coords to fill zeros\",\n    ):\n        xr.cross(\n            xr.DataArray(\n                [1, 2],\n                dims=[dim],\n                coords=[(\"cartesian\", [\"x\", \"y\"])],\n            ),\n            xr.DataArray(\n                [4, 5],\n                dims=[dim],\n                coords=[(\"cartesian\", [\"y\", \"z\"])],\n            ),\n            dim=dim,\n        )\n"], "sample_810": ["def test_pipeline_memory_clone():\n    # Test that the transformers are cloned when memory is not None\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n    try:\n        if LooseVersion(joblib_version) < LooseVersion('0.12'):\n            # Deal with change of API in joblib\n            memory = Memory(cachedir=cachedir, verbose=10)\n        else:\n            memory = Memory(location=cachedir, verbose=10)\n        transf = DummyTransf()\n        pipe = Pipeline([('transf', transf), ('svc', SVC(gamma='scale'))],\n                        memory=memory)\n        pipe.fit(X, y)\n        # Check that the transformer in the pipeline has been cloned\n        assert id(pipe.named_steps['transf']) != id(transf)\n    finally:\n        shutil.rmtree(cachedir)\n", "def test_pipeline_inverse_transform():\n    # test inverse transform when last estimator is a transformer\n    iris = load_iris()\n    X = iris.data\n    pca = PCA(n_components=2, svd_solver='full')\n    pipeline = Pipeline([('pca', pca)])\n\n    # test inverse_transform\n    X_trans = pipeline.fit_transform(X)\n    X_inv_trans = pipeline.inverse_transform(X_trans)\n    assert_array_almost_equal(X_inv_trans, pca.inverse_transform(X_trans))\n\n    # test inverse_transform with passthrough\n    pipeline = Pipeline([('pca', pca), ('passthrough', 'passthrough')])\n    X_trans = pipeline.fit_transform(X)\n    X_inv_trans = pipeline.inverse_transform(X_trans)\n    assert_array_almost_equal(X_inv_trans, pca.inverse_transform(X_trans))\n\n    # test inverse_transform with multiple transformers\n    pca2 = PCA(n_components=1, svd_solver='full')\n    pipeline = Pipeline([('pca', pca), ('pca2', pca2)])\n\n    # test inverse_transform\n    X_trans = pipeline.fit_transform(X)\n    X_inv_trans = pipeline.inverse_transform(X_trans)\n    assert_array_almost_equal(X_inv_trans, pca.inverse_transform(pca2.inverse_transform(X_trans)))\n", "def test_pipeline_with_cloneable_estimators():\n    # Test that a pipeline with cloneable estimators clones them correctly\n\n    # Define a class that supports clone\n    class CloneableEstimator(BaseEstimator):\n            self.value = value\n\n            return self\n\n            return X\n\n    # Create a pipeline with cloneable estimators\n    pipeline = Pipeline([\n        ('est1', CloneableEstimator(1)),\n        ('est2', CloneableEstimator(2))\n    ])\n\n    # Clone the pipeline\n    pipeline_clone = clone(pipeline)\n\n    # Check that the estimators are cloned correctly\n    assert pipeline.named_steps['est1'].value == 1\n    assert pipeline_clone.named_steps['est1'].value == 1\n    assert pipeline.named_steps['est1'] is not pipeline_clone.named_steps['est1']\n\n    assert pipeline.named_steps['est2'].value == 2\n    assert pipeline_clone.named_steps['est2'].value == 2\n    assert pipeline.named_steps['est2'] is not pipeline_clone.named_steps['est2']\n", "def test_pipeline_transformer_weights():\n    # Test transformer weights in Pipeline\n    X = np.array([[1, 2], [2, 3]])\n    y = np.array([1, 2])\n    pca = PCA(n_components=1, svd_solver='full')\n    mult = Mult(mult=2)\n    pipeline = Pipeline([('pca', pca), ('mult', mult)])\n    pipeline.fit(X, y)\n    X_trans = pipeline.transform(X)\n\n    pca.fit(X, y)\n    mult.fit(pca.transform(X), y)\n    expected_trans = mult.transform(pca.transform(X))\n\n    assert_array_almost_equal(X_trans, expected_trans)\n\n    pca_2 = PCA(n_components=1, svd_solver='full')\n    pca_2.fit(X, y)\n    mult_2 = Mult(mult=2)\n    mult_2.fit(pca_2.transform(X), y)\n    pipeline_2 = Pipeline([('pca', pca_2), ('mult', mult_2)],\n                          transformer_weights={'pca': 10})\n    pipeline_2.fit(X, y)\n    X_trans_2 = pipeline_2.transform(X)\n    expected_trans_2 = mult_2.transform(10 * pca_2.transform(X))\n\n    assert_array_almost_equal(X_trans_2, expected_trans_2)\n", "def test_pipeline_caching_different_memory_locations():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n\n    clf = SVC(gamma='scale', probability=True, random_state=0)\n    transf = DummyTransf()\n\n    # Create a pipeline with memory location 1\n    cachedir1 = mkdtemp()\n    if LooseVersion(joblib_version) < LooseVersion('0.12'):\n        # Deal with change of API in joblib\n        memory1 = Memory(cachedir=cachedir1, verbose=10)\n    else:\n        memory1 = Memory(location=cachedir1, verbose=10)\n    pipe1 = Pipeline([('transf', transf), ('svc', clf)], memory=memory1)\n\n    # Create a pipeline with memory location 2\n    cachedir2 = mkdtemp()\n    if LooseVersion(joblib_version) < LooseVersion('0.12'):\n        # Deal with change of API in joblib\n        memory2 = Memory(cachedir=cachedir2, verbose=10)\n    else:\n        memory2 = Memory(location=cachedir2, verbose=10)\n    pipe2 = Pipeline([('transf', transf), ('svc', clf)], memory=memory2)\n\n    # Fit the pipeline with memory location 1\n    pipe1.fit(X, y)\n    # Get the time stamp of the transformer in the cached pipeline\n    ts1 = pipe1.named_steps['transf'].timestamp_\n\n    # Fit the pipeline with memory location 2\n    pipe2.fit(X, y)\n    # Get the time stamp of the transformer in the cached pipeline\n    ts2 = pipe2.named_steps['transf'].timestamp_\n\n    # Check that the time stamps are different\n    assert ts1 != ts2\n\n    # Check that the transformers are different\n    assert pipe1.named_steps['transf'] != pipe2.named_steps['transf']\n\n    # Check that the pipeline with memory location 1 yields the same result as\n    # the pipeline with memory location 2\n    assert_array_equal(pipe1.predict(X), pipe2.predict(X))\n\n    # Clean up\n    shutil.rmtree(cachedir1)\n    shutil.rmtree(cachedir2)\n"], "sample_408": ["def test_alter_unique_together_fk_to_m2m_with_through(self):\n    changes = self.get_changes(\n        [\n            self.author_name,\n            ModelState(\n                \"otherapp\",\n                \"Book\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n                    (\"title\", models.CharField(max_length=200)),\n                ],\n                options={\"unique_together\": {(\"author\",)}},\n            ),\n        ],\n        [\n            self.author_name,\n            ModelState(\n                \"otherapp\",\n                \"Book\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\n                        \"authors\",\n                        models.ManyToManyField(\n                            \"testapp.Author\",\n                            through=\"otherapp.Attribution\",\n                        ),\n                    ),\n                    (\"title\", models.CharField(max_length=200)),\n                ],\n            ),\n        ],\n    )\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(\n        changes,\n        \"otherapp\",\n        0,\n        [\"AlterUniqueTogether\", \"RemoveField\", \"AddField\"],\n    )\n    self.assertOperationAttributes(\n        changes, \"otherapp\", 0, 0, name=\"book\", unique_together=set()\n    )\n    self.assertOperationAttributes(\n        changes, \"otherapp\", 0, 1, model_name=\"book\", name=\"author\"\n    )\n    self.assertOperationAttributes(\n        changes, \"otherapp\", 0, 2, model_name=\"book\", name=\"authors\"\n    )\n", "def test_nested_deconstructible_objects_in_list(self):\n    \"\"\"\n    Nested deconstruction is applied recursively to the args/kwargs of\n    deconstructed objects within a list.\n    \"\"\"\n    author_nested_deconstructible_list = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\n                \"name\",\n                models.CharField(\n                    max_length=200,\n                    default=[\n                        DeconstructibleObject(\n                            DeconstructibleObject(1),\n                            (\n                                DeconstructibleObject(\"t1\"),\n                                DeconstructibleObject(\"t2\"),\n                            ),\n                            a=DeconstructibleObject(\"A\"),\n                            b=DeconstructibleObject(B=DeconstructibleObject(\"c\")),\n                        ),\n                        DeconstructibleObject(2),\n                    ],\n                ),\n            ),\n        ],\n    )\n    author_nested_deconstructible_list_changed = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\n                \"name\",\n                models.CharField(\n                    max_length=200,\n                    default=[\n                        DeconstructibleObject(\n                            DeconstructibleObject(1),\n                            (\n                                DeconstructibleObject(\"t1\"),\n                                DeconstructibleObject(\"t2\"),\n                            ),\n                            a=DeconstructibleObject(\"A\"),\n                            b=DeconstructibleObject(B=DeconstructibleObject(\"c\")),\n                        ),\n                        DeconstructibleObject(3),\n                    ],\n                ),\n            ),\n        ],\n    )\n    changes = self.get_changes(\n        [author_nested_deconstructible_list],\n        [author_nested_deconstructible_list_changed],\n    )\n    self.assertEqual(len(changes), 1)\n", "def test_add_index_with_options(self):\n    \"\"\"\n    Tests the creation of an index with options.\n    \"\"\"\n    book_index_options = ModelState(\n        \"otherapp\",\n        \"Book\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n            (\"title\", models.CharField(max_length=200)),\n        ],\n        {\n            \"indexes\": [\n                models.Index(\n                    fields=[\"author\", \"title\"],\n                    name=\"book_title_author_idx\",\n                    opclasses=[\"varchar_pattern_ops\"],\n                )\n            ]\n        },\n    )\n    changes = self.get_changes(\n        [self.author_empty, self.book],\n        [self.author_empty, book_index_options],\n    )\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"AddIndex\"])\n    added_index = models.Index(\n        fields=[\"author\", \"title\"],\n        name=\"book_title_author_idx\",\n        opclasses=[\"varchar_pattern_ops\"],\n    )\n    self.assertOperationAttributes(\n        changes, \"otherapp\", 0, 0, model_name=\"book\", index=added_index\n    )\n", "def test_alter_model_table_name_with_reserved_keywords(self):\n    \"\"\"Tests detection for changing db_table in model's options' with reserved keywords.\"\"\"\n    keyword_table_name = \"select\"\n    author_with_reserved_db_table_options = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n        ],\n        {\"db_table\": keyword_table_name},\n    )\n    changes = self.get_changes(\n        [self.author_empty], [author_with_reserved_db_table_options]\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelTable\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, name=\"author\", table=keyword_table_name\n    )\n", "def test_add_field_with_deconstructible_default(self):\n    \"\"\"\n    Adding a field with a deconstructible default should work.\n    \"\"\"\n        return \"Ada Lovelace\"\n\n    changes = self.get_changes(\n        [self.author_empty],\n        [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=200, default=some_default())),\n                ],\n            ),\n        ],\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AddField\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"name\")\n\n    # The default should be serialized.\n    self.assertIsInstance(\n        changes[\"testapp\"][0].operations[0].field.deconstruct()[2].get(\"default\"),\n        str,\n    )\n"], "sample_314": ["    def test_to_python(self):\n        field = UsernameField()\n        self.assertEqual(field.to_python('test'), 'test')\n", "    def test_normalize_username(self):\n        # The normalization happens in UsernameField.to_python().\n        ohm_username = 'test\u2126'  # U+2126 OHM SIGN\n        data = {\n            'username': ohm_username,\n        }\n        form = UserCreationForm(data)\n        self.assertTrue(form.is_valid())\n        self.assertNotEqual(form.cleaned_data['username'], ohm_username)\n        self.assertEqual(form.cleaned_data['username'], 'test\u03a9')  # U+03A9 GREEK CAPITAL LETTER OMEGA\n", "    def test_invalid_password_format(self):\n        # Test that a ValueError is raised when the password format is invalid.\n        widget = ReadOnlyPasswordHashWidget()\n        value = 'invalid$format'\n        with self.assertRaises(ValueError):\n            widget.get_context('name', value, {})\n", "    def test_render_empty_value(self):\n        widget = ReadOnlyPasswordHashWidget()\n        html = widget.render('password', '', attrs={'id': 'id_password'})\n        self.assertIn(_(\"No password set.\"), html)\n", "    def test_user_model_without_is_active_field(self):\n        user = CustomUserWithoutIsActiveField.objects.create_user('testuser', 'test@example.com')\n        data = {'email': user.email}\n        form = PasswordResetForm(data)\n        self.assertTrue(form.is_valid())\n        form.save()\n        self.assertEqual(len(mail.outbox), 1)\n"], "sample_178": ["def test_formset_management_form_prefix_with_auto_id(self):\n    \"\"\"The management form has the correct prefix with auto_id.\"\"\"\n    formset = FavoriteDrinksFormSet(auto_id='some_id_%s')\n    self.assertEqual(formset.management_form.prefix, 'some_id_form')\n    data = {\n        'form-TOTAL_FORMS': '2',\n        'form-INITIAL_FORMS': '0',\n        'form-MIN_NUM_FORMS': '0',\n        'form-MAX_NUM_FORMS': '0',\n    }\n    formset = FavoriteDrinksFormSet(data=data, auto_id='some_id_%s')\n    self.assertEqual(formset.management_form.prefix, 'some_id_form')\n    formset = FavoriteDrinksFormSet(initial={}, auto_id='some_id_%s')\n    self.assertEqual(formset.management_form.prefix, 'some_id_form')\n", "    def test_management_form_widgets(self):\n        data = {\n            'choices-TOTAL_FORMS': '1',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-MAX_NUM_FORMS': '0',\n        }\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        management_form = formset.management_form\n        self.assertIsInstance(management_form.fields[TOTAL_FORM_COUNT].widget, HiddenInput)\n        self.assertIsInstance(management_form.fields[INITIAL_FORM_COUNT].widget, HiddenInput)\n        self.assertIsInstance(management_form.fields[MIN_NUM_FORM_COUNT].widget, HiddenInput)\n        self.assertIsInstance(management_form.fields[MAX_NUM_FORM_COUNT].widget, HiddenInput)\n        self.assertEqual(management_form.fields[TOTAL_FORM_COUNT].initial, 1)\n        self.assertEqual(management_form.fields[INITIAL_FORM_COUNT].initial, 0)\n        self.assertEqual(management_form.fields[MIN_NUM_FORM_COUNT].initial, 0)\n        self.assertEqual(management_form.fields[MAX_NUM_FORM_COUNT].initial, 1000)\n", "def test_formset_total_form_count(self):\n    \"\"\"Test total_form_count with validate_max=True\"\"\"\n    ChoiceFormSet = formset_factory(Choice, max_num=1, validate_max=True)\n    data = {\n        'choices-TOTAL_FORMS': '2',  \n        'choices-INITIAL_FORMS': '0',  \n        'choices-MIN_NUM_FORMS': '0',  \n        'choices-MAX_NUM_FORMS': '0',  \n        'choices-0-choice': 'Zero',\n        'choices-0-votes': '0',\n        'choices-1-choice': 'One',\n        'choices-1-votes': '1',\n    }\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertEqual(formset.total_form_count(), 2)\n", "def test_formset_add_fields(self):\n    class MyFormSet(BaseFormSet):\n            form.fields['my_field'] = IntegerField()\n\n    class MyForm(Form):\n        field = CharField()\n\n    MyFormSet = formset_factory(MyForm, formset=MyFormSet)\n    formset = MyFormSet()\n    for i, form in enumerate(formset.forms):\n        self.assertIn('my_field', form.fields)\n        self.assertNotIn('my_field', form.initial)\n        if i > 0:\n            self.assertEqual(form.initial, {})\n        else:\n            self.assertEqual(form.initial, {'field': ''})\n", "    def test_formset_management_form_absolute_max(self):\n        \"\"\"ManagementForm's absolute_max is the same as the formset's.\"\"\"\n        LimitedFavoriteDrinkFormSet = formset_factory(FavoriteDrinkForm, max_num=5)\n        formset = LimitedFavoriteDrinkFormSet()\n        self.assertEqual(formset.management_form.absolute_max, formset.absolute_max)\n        formset = LimitedFavoriteDrinkFormSet(initial=[{'name': 'Gin and Tonic'}])\n        self.assertEqual(formset.management_form.absolute_max, formset.absolute_max)\n"], "sample_491": ["def test_boundfield_custom_renderer(self):\n    class CustomRenderer(DjangoTemplates):\n        field_template_name = \"forms_tests/custom_field.html\"\n\n    class CustomFrameworkForm(FrameworkForm):\n        renderer = CustomRenderer()\n\n    f = CustomFrameworkForm(auto_id=\"id_%s\")\n    self.assertHTMLEqual(\n        str(f[\"language\"]),\n        \"\"\"\n        <div id=\"id_language\">\n        <label for=\"id_language_0\">\n        <p>Custom Field<p>\n        <input type=\"radio\" name=\"language\" value=\"P\" required id=\"id_language_0\">\n        Python</label></div>\n        <div><label for=\"id_language_1\">\n        <p>Custom Field<p>\n        <input type=\"radio\" name=\"language\" value=\"J\" required id=\"id_language_1\">\n        Java</label></div>\n        </div>\n        \"\"\",\n    )\n", "    def test_boundfield_get_context(self):\n        class MyForm(Form):\n            name = CharField()\n\n        f = MyForm()\n        bf = f[\"name\"]\n        self.assertEqual(\n            bf.get_context(),\n            {\n                \"field\": bf,\n                \"widget\": bf.widget,\n                \"label\": bf.label,\n                \"name\": bf.name,\n                \"value\": bf.value(),\n                \"help_text\": bf.help_text,\n                \"errors\": bf.errors,\n            },\n        )\n", "def test_fields_html_id_for_label(self):\n    class CustomIdForLabelTextInput(TextInput):\n            return \"custom_\" + id\n\n    class CustomIdForLabelCheckboxSelectMultiple(CheckboxSelectMultiple):\n            return \"custom_\" + id\n\n    class CustomIdForLabelRadioSelect(RadioSelect):\n            return \"custom_\" + id\n\n    class SomeForm(Form):\n        custom = CharField(widget=CustomIdForLabelTextInput)\n        custom_checkbox = MultipleChoiceField(\n            choices=[(\"a\", \"A\"), (\"b\", \"B\")],\n            widget=CustomIdForLabelCheckboxSelectMultiple,\n        )\n        custom_radio = ChoiceField(\n            choices=[(\"a\", \"A\"), (\"b\", \"B\")], widget=CustomIdForLabelRadioSelect\n        )\n\n    form = SomeForm(auto_id=\"id_%s\")\n    self.assertEqual(form[\"custom\"].id_for_label, \"custom_id_custom\")\n    self.assertEqual(form[\"custom_checkbox\"].id_for_label, \"custom_id_custom_checkbox\")\n    self.assertEqual(form[\"custom_radio\"].id_for_label, \"custom_id_custom_radio\")\n", "def test_boundfield_css_classes_context(self):\n    \"\"\"\n    BoundField's css_classes method should be able to accept a context\n    when using a template renderer\n    \"\"\"\n\n    class MyForm(Form):\n        field1 = CharField()\n        field2 = CharField()\n\n            return \"extra_css\"\n\n    form = MyForm()\n    field1 = form[\"field1\"]\n    field2 = form[\"field2\"]\n\n    css_classes = field1.css_classes(context={\"extra_css\": \"extra_css\"})\n    self.assertEqual(css_classes, \"extra_css\")\n\n    css_classes = field2.css_classes(context={\"extra_css\": \"extra_css\"})\n    self.assertEqual(css_classes, \"extra_css\")\n", "def test_widget_build_attrs(self):\n    class CustomWidget(TextInput):\n            if extra_attrs is None:\n                extra_attrs = {}\n            return {**(base_attrs or {}), **extra_attrs}\n\n    class CustomBoundWidget(BoundWidget):\n            return self.parent_widget._render(\n                self.template_name, {\"widget\": self.data}, self.renderer\n            )\n\n    class PersonNew(Form):\n        first_name = CharField(widget=CustomWidget)\n        birthday = DateField()\n\n    p = PersonNew()\n    p.renderer = get_default_renderer()\n    p.rendered_fields = []\n\n        p.rendered_fields.append(field)\n        return field.as_widget(**kwargs)\n\n    p.renderer.render = render_field\n\n    p.as_div()\n    rendered_fields = p.rendered_fields\n    self.assertEqual(len(rendered_fields), 2)\n    self.assertEqual(\n        rendered_fields[0].field.widget.build_attrs(\n            {\"id\": \"id_first_name\"}, {\"class\": \"wide\"}\n        ),\n        {\"id\": \"id_first_name\", \"class\": \"wide\"},\n    )\n    self.assertEqual(\n        rendered_fields[1].field.widget.build_attrs(\n            {}, {\"class\": \"wide\", \"disabled\": True}\n        ),\n        {\"class\": \"wide\", \"disabled\": True},\n    )\n\n    p = PersonNew(auto_id=False)\n    p.renderer = get_default_renderer()\n    p.rendered_fields = []\n\n    rendered_fields = p.rendered_fields\n    p.as_div()\n    self.assertEqual(len(rendered_fields), 2)\n    self.assertEqual(\n        rendered_fields[0].field.widget.build_attrs({}, {\"class\": \"wide\"}),\n        {\"class\": \"wide\"},\n    )\n    self.assertEqual(\n        rendered_fields[1].field.widget.build_attrs({}, {\"class\": \"wide\", \"disabled\": True}),\n        {\"class\": \"wide\", \"disabled\": True},\n    )\n\n    p = PersonNew(auto_id=\"id_%s\")\n    p.renderer = get_default_renderer()\n    p.rendered_fields = []\n\n    rendered_fields = p.rendered_fields\n    p.as_div()\n    self.assertEqual(len(rendered_fields), 2)\n    self.assertEqual(\n        rendered_fields[0].field.widget.build_attrs({\"id\": \"id_first_name\"}, {\"class\": \"wide\"}),\n        {\"id\": \"id_first_name\", \"class\": \"wide\"},\n    )\n    self"], "sample_1022": ["def test_repeated_decimals():\n    cases = {\n        '0.[1]': '19/90',\n        '0.[12]': '4/33',\n        '0.[123]': '41/333',\n        '0.2[1]': '19/90 + 2/10',\n        '0.2[12]': '4/33 + 2/10',\n        '0.2[123]': '41/333 + 2/10',\n        '.2[1]': '19/90 + 2/10',\n        '.2[12]': '4/33 + 2/10',\n        '.2[123]': '41/333 + 2/10',\n        '0.2[12] + 3[1]': '4/33 + 2/10 + 3 + 1',\n        '0.2[12] + 3.[1]': '4/33 + 2/10 + 3 + 19/90',\n        '0.2[12] + 3.[12]': '4/33 + 2/10 + 3 + 4/33',\n        '0.2[12] + 3.[123]': '4/33 + 2/10 + 3 + 41/333',\n        '0.2[12] + 3[123]': '4/33 + 2/10 + 3 + 41/333',\n        '0.2[12] + 3.1[123]': '4/33 + 2/10 + 31/10 + 41/333',\n        '0.2[12] + 3.12[123]': '4/33 + 2/10 + 312/100 + 41/333',\n        '0.2[12] + 3.123[123]': '4/33 + 2/10 + 3123/1000 + 41/333',\n        '.2[12] + 3.[12]': '4/33 + 2/10 + 3 + 4/33',\n        '.2[12] + 3.[123]': '4/33 + 2/10 + 3 + 41/333',\n        '.2[12] + 3[123]': '4/", "def test_repeated_decimals():\n    transformations = standard_transformations + (convert_xor, auto_number)\n    cases = {\n        '1.[1]': '19/90',\n        '0.[1]': '1/9',\n        '0.2[1]': '19/90',\n        '1.2[3]': '121/99',\n        '0.9[9]': '1',\n        '0.3[33]': '10/33',\n    }\n    for case in cases:\n        implicit = parse_expr(case, transformations=transformations)\n        normal = parse_expr(cases[case], transformations=transformations)\n        assert(implicit == normal)\n\n    error_cases = [\n        '0.2[0]',\n        '0.[01]',\n        '0.2[1]2',\n        '0.2[12]',\n        '0.[123]',\n    ]\n    for case in error_cases:\n        raises(SyntaxError,\n               lambda: parse_expr(case, transformations=transformations))\n", "def test_repeated_decimals():\n    cases = {\n        '.5[1]': '19/90',\n        '0.5[1]': '19/90',\n        '.123[456]': '41/330',\n        '0.123[456]': '41/330',\n        '.2[1]': '19/90',\n        '0.2[1]': '19/90',\n        '.2[12]': '223/999',\n        '0.2[12]': '223/999',\n        '0.2[0]': '0',\n        '0.000123[456]': '41/330000',\n        '0.000123[0456]': '41/330000',\n    }\n    transformations = standard_transformations + (convert_xor, repeated_decimals)\n    for case, expected in cases.items():\n        assert(parse_expr(case, transformations=transformations) ==\n               parse_expr(expected, transformations=standard_transformations))\n", "def test_rationalize():\n    cases = {\n        '0.5': 'Rational(1, 2)',\n        '1.5': 'Rational(3, 2)',\n        '0.1234567890123456789': 'Rational(4444444444444444445, 36028797018963968)',\n        '0.9999999999999999': 'Rational(9999999999999999, 10000000000000000)',\n        '1.0': 'Rational(1)',\n        '123.456': 'Rational(24603, 200)',\n        '1e-10': 'Rational(1, 10000000000)',\n        '1.23e+2': 'Rational(123, 1)',\n        '1e10': 'Rational(10000000000)',\n        '1.23e-10': 'Rational(123, 10000000000)',\n        '-1.0': 'Rational(-1)',\n        '-0.5': 'Rational(-1, 2)',\n        '-0.1234567890123456789': 'Rational(-4444444444444444445, 36028797018963968)',\n    }\n    transformations = standard_transformations + (convert_xor, auto_number, rationalize)\n    for case, expected in cases.items():\n        assert(parse_expr(case, transformations=transformations) == parse_expr(expected))\n\n    transformations = standard_transformations + (convert_xor, rationalize, auto_number)\n    for case, expected in cases.items():\n        assert(parse_expr(case, transformations=transformations) == parse_expr(expected))\n\n    transformations = standard_transformations + (rationalize, convert_xor, auto_number)\n    for case, expected in cases.items():\n        assert(parse_expr(case, transformations=transformations) == parse_expr(expected))\n", "def test_repeated_decimals():\n    transformations = standard_transformations + (convert_xor,)\n    cases = {\n        '0.[1]': '19/90',\n        '.1[1]': '11/99',\n        '0.2[1]': '211/990',\n        '0.23[4]': '230011/99900',\n        '0.[12]': '4/33',\n        '0.34[56]': '34125656/999900',\n        '3.4[56]': '341256/99900 + 3',\n        '0.2[12]': '223/990',\n    }\n    for case in cases:\n        implicit = parse_expr(case, transformations=transformations)\n        normal = parse_expr(cases[case], transformations=transformations)\n        assert(implicit == normal)\n\n    invalid_cases = ['.[1]', '.1[1]2', '0.1[12]', '0.1[12]2']\n    for case in invalid_cases:\n        raises(SyntaxError, lambda: parse_expr(case, transformations=transformations))\n"], "sample_623": ["    def test_open_dataset_with_cache_and_chunks(self):\n        \"\"\"Test that opening a dataset with both cache and chunks raises an error.\"\"\"\n        with pytest.raises(ValueError):\n            xr.open_dataset(\"example.nc\", cache=True, chunks=10)\n", "def test_chunks_with_pref_chunks_as_none() -> None:\n    \"\"\"Test that `chunks` argument with `None` as value respects the preferred chunks.\"\"\"\n    dataset = xr.Dataset(\n        {\n            \"data\": xr.Variable(\n                (\"x\", \"y\"),\n                np.empty((4, 5)),\n                encoding={\"preferred_chunks\": {\"x\": 2, \"y\": None}},\n            )\n        }\n    )\n    chunks = {\"x\": None, \"y\": 3}\n    result = xr.open_dataset(\n        dataset, engine=PassThroughBackendEntrypoint, chunks=chunks\n    )\n    assert result[\"data\"].chunks == (2, 3)\n", "def test_open_dataset_encoding() -> None:\n    # Create a dataset with various data types\n    dataset = xr.Dataset(\n        {\n            \"int_var\": ((\"x\",), np.arange(5, dtype=np.int32)),\n            \"float_var\": ((\"x\",), np.arange(5, dtype=np.float32)),\n            \"bool_var\": ((\"x\",), np.array([True, False, True, False, True], dtype=np.bool_)),\n            \"str_var\": ((\"x\",), np.array([\"a\", \"b\", \"c\", \"d\", \"e\"], dtype=np.object)),\n            \"datetime_var\": ((\"x\",), np.array([\"2022-01-01\", \"2022-01-02\"], dtype=np.datetime64)),\n        },\n        coords={\"x\": ((\"x\",), np.arange(5))},\n    )\n\n    # Add attributes with different data types\n    dataset.attrs[\"int_attr\"] = 1\n    dataset.attrs[\"float_attr\"] = 2.5\n    dataset.attrs[\"bool_attr\"] = True\n    dataset.attrs[\"str_attr\"] = \"hello\"\n    dataset.attrs[\"list_attr\"] = [1, 2, 3]\n\n    # Save the dataset to a netCDF file\n    with tmp_file(\".nc\") as tmp:\n        dataset.to_netcdf(tmp)\n\n    # Open the dataset with the default engine\n    with xr.open_dataset(tmp) as ds:\n        # Check that the data types are correct\n        assert ds[\"int_var\"].dtype == np.int32\n        assert ds[\"float_var\"].dtype == np.float32\n        assert ds[\"bool_var\"].dtype == np.bool_\n        assert ds[\"str_var\"].dtype == np.object\n        assert ds[\"datetime_var\"].dtype == np.datetime64\n\n        # Check that the attributes are correct\n        assert ds.attrs[\"int_attr\"] == 1\n        assert ds.attrs[\"float_attr\"] == 2.5\n        assert ds.attrs[\"bool_attr\"] is True\n        assert ds.attrs[\"str_attr\"] == \"hello\"\n        assert ds.attrs[\"list_attr\"] == [1, 2, 3]\n", "def test_chunk_ds():\n    \"\"\"Test _chunk_ds helper function.\"\"\"\n    import dask.array as da\n    from xarray.backends.common import _get_chunk\n\n    # Create a fake backend dataset with variable attributes\n    backend_ds = xr.Dataset(\n        {\n            \"var1\": xr.Variable((\"x\", \"y\"), np.random.rand(10, 10)),\n            \"var2\": xr.Variable((\"x\", \"y\"), np.random.rand(10, 10)),\n        },\n        attrs={\"foo\": \"bar\"},\n    )\n    backend_ds[\"var1\"].encoding[\"chunks\"] = (5, 5)\n    backend_ds[\"var2\"].encoding[\"chunks\"] = (3, 3)\n\n    # Create expected output\n    expected_ds = xr.Dataset(\n        {\n            \"var1\": xr.Variable(\n                (\"x\", \"y\"),\n                da.from_array(np.random.rand(10, 10), chunks=(5, 5)),\n                dims=(\"x\", \"y\"),\n            ),\n            \"var2\": xr.Variable(\n                (\"x\", \"y\"),\n                da.from_array(np.random.rand(10, 10), chunks=(3, 3)),\n                dims=(\"x\", \"y\"),\n            ),\n        },\n        attrs={\"foo\": \"bar\"},\n    )\n\n    # Test _chunk_ds function\n    chunks = {\"x\": 5, \"y\": 3}\n    actual_ds = _chunk_ds(\n        backend_ds, \"fake_filename\", \"netcdf4\", chunks, False, False\n    )\n\n    assert_identical(actual_ds, expected_ds)\n\n    # Test _get_chunk function\n    assert _get_chunk(backend_ds[\"var1\"], chunks) == (5, 5)\n    assert _get_chunk(backend_ds[\"var2\"], chunks) == (3, 3)\n\n    # Test _get_chunk function with integer chunks\n    chunks = 4\n    assert _get_chunk(backend_ds[\"var1\"], chunks) == (4, 4)\n    assert _get_chunk(backend_ds[\"var2\"], chunks) == (4, 4)\n\n    # Test _get_chunk function with None chunks\n    chunks = None\n    assert _get_chunk(backend_ds[\"var1\"], chunks) == (5, 5)\n    assert _get_chunk(backend_ds[\"var2\"], chunks) == (3, 3)\n", "def test_to_zarr_append_dim():\n    # GH 7855\n    # Check that append_dim argument works correctly\n\n    time = xr.DataArray(np.arange(3), dims=[\"time\"])\n    ds = xr.Dataset({\"a\": time * 2})\n\n    # Create a store with one variable\n    store = xr.backends.ZarrStore(\"test.zarr\", mode=\"w\")\n    ds.to_zarr(store)\n\n    # Append to the variable along the time dimension\n    ds2 = xr.Dataset({\"a\": time * 4})\n    ds2.to_zarr(store, append_dim=\"time\")\n\n    # Load the appended data\n    loaded = xr.open_zarr(store)\n\n    # Check that the data was appended correctly\n    expected_time = xr.DataArray(np.arange(6), dims=[\"time\"])\n    expected = xr.Dataset({\"a\": expected_time * 2})\n    expected[:3] = ds2\n\n    assert_identical(loaded, expected)\n"], "sample_168": ["def test_noinput_with_dependent_objects(self):\n    \"\"\"\n    non-interactive mode (--noinput) deletes stale content types and warns of\n    dependent objects.\n    \"\"\"\n    post = Post.objects.create(title='post', content_type=self.content_type)\n    # A related object is needed to show that a custom collector with\n    # can_fast_delete=False is needed.\n    ModelWithNullFKToSite.objects.create(post=post)\n    with captured_stdout() as stdout:\n        call_command('remove_stale_contenttypes', noinput=True, verbosity=2)\n    self.assertEqual(Post.objects.count(), 0)\n    output = stdout.getvalue()\n    self.assertIn('- Content type for contenttypes_tests.Fake', output)\n    self.assertIn('- 1 contenttypes_tests.Post object(s)', output)\n    self.assertIn('- 1 contenttypes_tests.ModelWithNullFKToSite', output)\n    self.assertIn('Deleting stale content type', output)\n    self.assertEqual(ContentType.objects.count(), self.before_count)\n", "def test_noinput_argument(self):\n    \"\"\"The --noinput argument works as an alias for --no-input.\"\"\"\n    with mock.patch('builtins.input', return_value='yes'), captured_stdout() as stdout:\n        call_command('remove_stale_contenttypes', noinput=True, verbosity=2)\n    self.assertIn('Deleting stale content type', stdout.getvalue())\n    self.assertEqual(ContentType.objects.count(), self.before_count)\n", "def test_database_argument(self):\n    \"\"\"\n    The database argument works as expected.\n    \"\"\"\n    ContentType.objects.using('default').create(app_label='contenttypes_tests', model='Fake')\n    ContentType.objects.using('other').create(app_label='contenttypes_tests', model='Fake')\n\n    with mock.patch('builtins.input', return_value='yes'), captured_stdout() as stdout:\n        call_command('remove_stale_contenttypes', database='default', verbosity=2)\n    self.assertIn(\"Deleting stale content type\", stdout.getvalue())\n    self.assertEqual(ContentType.objects.using('default').count(), self.before_count)\n    self.assertEqual(ContentType.objects.using('other').count(), 1)\n\n    with mock.patch('builtins.input', return_value='yes'), captured_stdout() as stdout:\n        call_command('remove_stale_contenttypes', database='other', verbosity=2)\n    self.assertIn(\"Deleting stale content type\", stdout.getvalue())\n    self.assertEqual(ContentType.objects.using('default').count(), self.before_count)\n    self.assertEqual(ContentType.objects.using('other').count(), self.before_count)\n", "def test_contenttype_model_class_is_none(self):\n    \"\"\"\n    A ContentType is considered stale when model_class() returns None.\n    \"\"\"\n    content_type = ContentType.objects.create(app_label='contenttypes_tests', model='NonExistentModel')\n    self.assertIsNone(content_type.model_class())\n    with mock.patch('builtins.input', return_value='yes'):\n        with captured_stdout() as stdout:\n            call_command('remove_stale_contenttypes', verbosity=2)\n    self.assertIn(\"Deleting stale content type\", stdout.getvalue())\n    self.assertEqual(ContentType.objects.count(), self.before_count)\n", "def test_interactive_false_without_changes(self):\n    \"\"\"non-interactive mode does not delete stale content types if none exist.\"\"\"\n    ContentType.objects.filter(app_label='contenttypes_tests', model='Fake').delete()\n    with captured_stdout() as stdout:\n        call_command('remove_stale_contenttypes', interactive=False, verbosity=2)\n    self.assertNotIn('Deleting stale content type', stdout.getvalue())\n    self.assertEqual(ContentType.objects.count(), self.before_count - 1)\n"], "sample_851": ["def test_mean_poisson_deviance():\n    y_true = np.array([2, 0, 1, 4])\n    y_pred = np.array([0.5, 0.5, 2., 2.])\n    assert_almost_equal(mean_poisson_deviance(y_true, y_pred),\n                        mean_tweedie_deviance(y_true, y_pred, power=1))\n    # Poisson deviance needs non-negative y_true and strictly positive y_pred\n    with pytest.raises(ValueError,\n                       match=\"can only be used on non-negative y_true and \"\n                             \"strictly positive y_pred\"):\n        mean_poisson_deviance([-1., 0, 1, 4], [0.5, 0.5, 2., 2.])\n    with pytest.raises(ValueError,\n                       match=\"can only be used on non-negative y_true and \"\n                             \"strictly positive y_pred\"):\n        mean_poisson_deviance([2., 0, 1, 4], [0., 0.5, 2., 2.])\n", "def test_mean_tweedie_deviance_edge_cases():\n    # Test edge cases for mean_tweedie_deviance\n    y_true = np.array([1e-10, 1, 1e10])\n    y_pred = np.array([1e-10, 1, 1e10])\n\n    # Test p = 0\n    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, power=0), 0)\n\n    # Test p = 1\n    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, power=1), 0)\n\n    # Test p = 2\n    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, power=2), 0)\n\n    # Test p = 3\n    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, power=3), 0)\n\n    # Test y_true and y_pred are very close to zero\n    y_true = np.array([1e-15, 1e-10, 1e-5])\n    y_pred = np.array([1e-15, 1e-10, 1e-5])\n    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, power=1.5), 0)\n\n    # Test y_true and y_pred are very large\n    y_true = np.array([1e15, 1e10, 1e5])\n    y_pred = np.array([1e15, 1e10, 1e5])\n    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, power=1.5), 0)\n", "def test_regression_metrics_y_true_constant():\n    y_true = np.ones(50)\n    y_pred = np.arange(50)\n    assert_almost_equal(mean_squared_error(y_true, y_pred), 322.5)\n    assert_almost_equal(mean_squared_log_error(y_true, y_pred),\n                        mean_squared_error(np.log(1 + y_true),\n                                           np.log(1 + y_pred)))\n    assert_almost_equal(mean_absolute_error(y_true, y_pred), 24.5)\n    assert_almost_equal(median_absolute_error(y_true, y_pred), 24.5)\n    assert_almost_equal(max_error(y_true, y_pred), 49.)\n    assert_almost_equal(explained_variance_score(y_true, y_pred), 0.)\n    assert_almost_equal(r2_score(y_true, y_pred), -48.5)\n    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, power=0),\n                        mean_squared_error(y_true, y_pred))\n", "def test_mean_gamma_deviance():\n    y_true = np.array([2, 0.5, 1, 4])\n    y_pred = np.array([0.5, 0.5, 2., 2.])\n\n    # Test for the case when y_true and y_pred are both positive\n    result = mean_gamma_deviance(y_true, y_pred)\n    assert_almost_equal(result, 1.0568, decimal=4)\n\n    # Test for the case when y_true is zero\n    y_true = np.array([0, 0.5, 1, 4])\n    with pytest.raises(ValueError, match=\"can only be used on strictly positive y_true and y_pred.\"):\n        mean_gamma_deviance(y_true, y_pred)\n\n    # Test for the case when y_pred is zero\n    y_true = np.array([2, 0.5, 1, 4])\n    y_pred = np.array([0, 0.5, 2., 2.])\n    with pytest.raises(ValueError, match=\"can only be used on strictly positive y_true and y_pred.\"):\n        mean_gamma_deviance(y_true, y_pred)\n\n", "def test_mean_tweedie_deviance_edge_cases(y_true, y_pred, power, expected_error):\n    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, power=power),\n                        expected_error)\n"], "sample_400": ["def test_renamed_custom_fk_with_hardcoded_to(self):\n    class HardcodedForeignKey(models.ForeignKey):\n            kwargs[\"to\"] = \"testapp.Author\"\n            super().__init__(*args, **kwargs)\n\n            name, path, args, kwargs = super().deconstruct()\n            del kwargs[\"to\"]\n            return name, path, args, kwargs\n\n    book_hardcoded_fk_to = ModelState(\n        \"testapp\",\n        \"Book\",\n        [\n            (\"author\", HardcodedForeignKey(on_delete=models.CASCADE)),\n        ],\n    )\n    book_renamed_hardcoded_fk_to = ModelState(\n        \"testapp\",\n        \"Book\",\n        [\n            (\"writer\", HardcodedForeignKey(on_delete=models.CASCADE)),\n        ],\n    )\n    changes = self.get_changes(\n        [self.author_empty, book_hardcoded_fk_to],\n        [self.author_empty, book_renamed_hardcoded_fk_to],\n        MigrationQuestioner({\"ask_rename\": True}),\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameField\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, old_name=\"author\", new_name=\"writer\"\n    )\n", "def test_alter_db_table_with_renamed_model_and_options(self):\n    \"\"\"\n    Tests when model, db_table changes, and options change, autodetector must create\n    three operations.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_with_db_table_options],\n        [self.author_renamed_with_new_db_table_options],\n        MigrationQuestioner({\"ask_rename_model\": True}),\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(\n        changes,\n        \"testapp\",\n        0,\n        [\"RenameModel\", \"AlterModelTable\", \"AlterModelOptions\"],\n    )\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, old_name=\"Author\", new_name=\"NewAuthor\"\n    )\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 1, name=\"newauthor\", table=\"author_three\"\n    )\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 2, name=\"newauthor\", options={\"managed\": True}\n    )\n", "def test_renamed_foreignkey_in_index_together(self):\n    before = [\n        ModelState(\n            \"app\",\n            \"Foo\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"bar\", models.IntegerField()),\n            ],\n            options={\"index_together\": {(\"bar\",)}}),\n        ModelState(\n            \"app\",\n            \"Bar\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"foo\", models.ForeignKey(\"app.Foo\", models.CASCADE)),\n            ],\n        ),\n    ]\n    after = [\n        ModelState(\n            \"app\",\n            \"Foo\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"baz\", models.IntegerField()),\n            ],\n            options={\"index_together\": {(\"baz\",)}}),\n        ModelState(\n            \"app\",\n            \"Bar\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"foo\", models.ForeignKey(\"app.Foo\", models.CASCADE)),\n            ],\n        ),\n    ]\n    changes = self.get_changes(before, after, MigrationQuestioner({\"ask_rename\": True}))\n    self.assertNumberMigrations(changes, \"app\", 1)\n    self.assertOperationTypes(changes, \"app\", 0, [\"RenameField\", \"AlterIndexTogether\"])\n    self.assertOperationAttributes(changes, \"app\", 0, 0, model_name=\"foo\", old_name=\"bar\", new_name=\"baz\")\n    self.assertOperationAttributes(changes, \"app\", 0, 1, name=\"foo\", index_together={(\"baz\",)})\n", "def test_alter_field_to_many_to_many_with_through_model(self):\n    \"\"\"\n    #25788 - Altering a field to a ManyToManyField with a through model\n    does not create an additional migration for creating the through model.\n    \"\"\"\n    author_with_foreign_key = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\n                \"publisher\",\n                models.ForeignKey(\"testapp.Publisher\", models.CASCADE),\n            ),\n        ],\n    )\n    author_with_many_to_many = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\n                \"publishers\",\n                models.ManyToManyField(\"testapp.Publisher\", through=\"testapp.Contract\"),\n            ),\n        ],\n    )\n    changes = self.get_changes(\n        [author_with_foreign_key, self.publisher],\n        [author_with_many_to_many, self.publisher, self.contract],\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(\n        changes,\n        \"testapp\",\n        0,\n        [\"RemoveField\", \"CreateModel\", \"AddField\"],\n    )\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, name=\"publisher\", model_name=\"author\"\n    )\n    self.assertOperationAttributes(changes, \"testapp\", 0, 1, name=\"Contract\")\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 2, name=\"publishers\", model_name=\"author\"\n    )\n", "def test_alter_swappable_foreign_key_with_default(self):\n    \"\"\"\n    #23692 - Altering a ForeignKey's to_field from a swappable model to a\n    different swappable model correctly includes the swappable model name\n    in the dependency list.\n    \"\"\"\n    before = [\n        ModelState(\n            \"app\",\n            \"Model\",\n            [\n                (\n                    \"other\",\n                    models.ForeignKey(\n                        \"auth.User\",\n                        models.CASCADE,\n                        default=models.SET(settings.AUTH_USER_MODEL),\n                    ),\n                ),\n            ],\n        ),\n    ]\n    after = [\n        ModelState(\n            \"app\",\n            \"Model\",\n            [\n                (\n                    \"other\",\n                    models.ForeignKey(\n                        \"thirdapp.CustomUser\",\n                        models.CASCADE,\n                        default=models.SET(settings.AUTH_USER_MODEL),\n                    ),\n                ),\n            ],\n        ),\n    ]\n    with override_settings(AUTH_USER_MODEL=\"thirdapp.CustomUser\"):\n        with isolate_lru_cache(apps.get_swappable_settings_name):\n            changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, \"app\", 1)\n    self.assertOperationTypes(changes, \"app\", 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, \"app\", 0, 0, name=\"other\")\n    self.assertMigrationDependencies(\n        changes, \"app\", 0, [(\"__setting__\", \"AUTH_USER_MODEL\")]\n    )\n"], "sample_614": ["def test_limit_lines() -> None:\n    cases = [\n        (\"a\\nb\\nc\", 3, \"a\\nb\\nc\"),\n        (\"a\\nb\\nc\", 2, \"a\\n...\\nc\"),\n        (\"a\\nb\\nc\", 1, \"...\"),\n        (\"a\" * 80 + \"\\n\" + \"b\" * 80, 2, \"a\" * 80 + \"\\n...\"),\n    ]\n\n    for string, limit, expected in cases:\n        actual = formatting.limit_lines(string, limit=limit)\n        assert actual == expected\n", "def test_format_item_with_bytes() -> None:\n    cases = [\n        (b\"foo\", \"b'foo'\"),\n        (b\"\\xc3\\xbc\", \"b'\\\\xc3\\\\xbc'\"),\n        (b\"hello world\", \"b'hello world'\"),\n    ]\n    for item, expected in cases:\n        actual = formatting.format_item(item, quote_strings=True)\n        assert expected == actual\n\n        actual = formatting.format_item(item, quote_strings=False)\n        assert item.decode(\"utf-8\") == actual\n", "def test_wrap_indent():\n    text = \"Hello\\nWorld\"\n    expected = \"  Hello\\n  World\"\n    assert formatting.wrap_indent(text, start=\"  \") == expected\n\n    text = \"Hello\"\n    expected = \"  Hello\"\n    assert formatting.wrap_indent(text, start=\"  \") == expected\n\n    text = \"Hello\\nWorld\\n!\"\n    expected = \"  Hello\\n  World\\n  !\"\n    assert formatting.wrap_indent(text, start=\"  \") == expected\n\n    text = \"Hello\\nWorld\\n!\"\n    expected = \"abc\\n  Hello\\n  World\\n  !\"\n    assert formatting.wrap_indent(text, start=\"abc\") == expected\n\n    text = \"Hello\"\n    expected = \"abcHello\"\n    assert formatting.wrap_indent(text, start=\"abc\", length=0) == expected\n", "def test_format_array_flat_array_with_uneven_string_lengths() -> None:\n    # Test arrays with string elements of different lengths\n    array = np.array([\"a\", \"bb\", \"ccc\", \"dddd\", \"eeeee\"])\n    actual = formatting.format_array_flat(array, 20)\n    expected = \"a bb ccc ... eeeee\"\n    assert actual == expected\n\n    array = np.array([\"a\", \"bb\", \"ccc\", \"dddd\", \"eeeee\", \"ffffff\"])\n    actual = formatting.format_array_flat(array, 20)\n    expected = \"a bb ccc ... ffffff\"\n    assert actual == expected\n\n    array = np.array([\"a\", \"bb\", \"ccc\", \"dddd\", \"eeeee\", \"ffffff\", \"ggggggg\"])\n    actual = formatting.format_array_flat(array, 20)\n    expected = \"a bb ccc ... ggggggg\"\n    assert actual == expected\n\n    array = np.array([\"a\", \"bb\", \"ccc\", \"dddd\", \"eeeee\", \"ffffff\", \"ggggggg\", \"hhhhhhh\"])\n    actual = formatting.format_array_flat(array, 20)\n    expected = \"a bb ccc ... hhhhhhh\"\n    assert actual == expected\n\n    array = np.array([\"a\", \"bb\", \"ccc\", \"dddd\", \"eeeee\", \"ffffff\", \"ggggggg\", \"hhhhhhh\", \"iiiiiiii\"])\n    actual = formatting.format_array_flat(array, 20)\n    expected = \"a bb ccc ... iiiiiiii\"\n    assert actual == expected\n", "def test_dim_summary_limited() -> None:\n    obj = xr.DataArray(\n        np.arange(100),\n        dims=[\"dim1\", \"dim2\", \"dim3\", \"dim4\", \"dim5\", \"dim6\"],\n        shape=(1, 3, 3, 3, 3, 3),\n    )\n\n    col_width = 20\n    max_rows = 3\n\n    actual = formatting.dim_summary_limited(obj, col_width, max_rows)\n    expected = \"dim1: 1, dim2: 3, dim3: 3, dim4: 3, ...\\ndim6: 3\"\n    expected = \" \".join([\"    \" + s for s in expected.split(\", \")[:2]]) + \", ...\\n\" + \"    \" + \", \".join(\n        [\"dim\" + s for s in expected.split(\", \")[-2:]]\n    )\n\n    assert actual == expected\n"], "sample_100": ["    def test_trigger_reload_exits(self, mocked_exit):\n        autoreload.trigger_reload('test_file.py')\n        self.assertEqual(mocked_exit.call_count, 1)\n        self.assertEqual(mocked_exit.call_args[0][0], 3)\n", "    def test_file_added_in_thread(self):\n        \"\"\"\n        When a file is added in a thread, it's returned by iter_all_python_module_files().\n        \"\"\"\n        filename = self.temporary_file('test_added_in_thread.py')\n        filename.touch()\n\n        thread = threading.Thread(target=lambda: self.import_and_cleanup('test_added_in_thread'))\n        thread.start()\n        thread.join()\n\n        self.assertFileFound(filename.absolute())\n", "    def test_tick_sleep_time(self):\n        with mock.patch('time.sleep') as mocked_sleep:\n            with mock.patch.object(self.reloader, 'snapshot_files', return_value=[(self.existing_file, 1)]):\n                ticker = self.reloader.tick()\n                next(ticker)\n                next(ticker)\n                self.assertEqual(mocked_sleep.call_args[0][0], self.reloader.SLEEP_TIME)\n", "    def test_empty_paths(self):\n        with self.assertRaises(ValueError):\n            autoreload.common_roots(())\n", "    def test_files_removed(self):\n        \"\"\"\n        When a file is removed, it's no longer returned by iter_all_python_module_files().\n        \"\"\"\n        filename = self.temporary_file('test_removed_module.py')\n        filename.touch()\n\n        with extend_sys_path(str(filename.parent)):\n            self.import_and_cleanup('test_removed_module')\n\n        filename.unlink()\n        self.assertFileNotFound(filename.absolute())\n"], "sample_767": ["def test_column_transformer_sparse_remainder_transformer_weights():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).T\n\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           remainder=SparseMatrixTrans(),\n                           transformer_weights={'trans1': 2},\n                           sparse_threshold=0.8)\n\n    X_trans = ct.fit_transform(X_array)\n    assert sparse.issparse(X_trans)\n    # SparseMatrixTrans creates 3 features for each column. There is\n    # one column in ``transformers``, thus:\n    assert X_trans.shape == (3, 3 + 1)\n\n    exp_array = np.hstack(\n        (2 * X_array[:, 0].reshape(-1, 1), np.eye(3)))\n    assert_array_equal(X_trans.toarray(), exp_array)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert isinstance(ct.transformers_[-1][1], SparseMatrixTrans)\n    assert_array_equal(ct.transformers_[-1][2], [1, 2])\n", "def test_column_transformer_is_fitted():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    ct = ColumnTransformer([('trans1', Trans(), [0])])\n\n    assert not hasattr(ct, 'transformers_')\n\n    with pytest.raises(NotFittedError):\n        ct.transform(X_array)\n\n    ct.fit(X_array)\n    assert hasattr(ct, 'transformers_')\n    assert_array_equal(ct.transform(X_array), ct.fit_transform(X_array))\n", "def test_column_transformer_remainder_with_no_columns():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).T\n\n    ct = ColumnTransformer([], remainder=DoubleTrans())\n\n    X_trans = ct.fit_transform(X_array)\n    assert_array_equal(X_trans, 2 * X_array)\n    assert len(ct.transformers_) == 1\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert isinstance(ct.transformers_[-1][1], DoubleTrans)\n    assert_array_equal(ct.transformers_[-1][2], [0, 1, 2])\n", "def test_column_transformer_validate_output_for_sparse_input():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    sparse_X = sparse.csr_matrix(X_array)\n\n    ct = ColumnTransformer([('trans1', TransNo2D(), [0]),\n                            ('trans2', Trans(), 1)],\n                           sparse_threshold=0.8)\n    with pytest.raises(ValueError,\n                       match=\"the 'trans1' transformer should be 2D\"):\n        ct.fit(sparse_X)\n\n    ct = ColumnTransformer([('trans1', Trans(), [0]),\n                            ('trans2', TransNo2D(), 1)],\n                           sparse_threshold=0.8)\n    with pytest.raises(ValueError,\n                       match=\"the 'trans2' transformer should be 2D\"):\n        ct.fit(sparse_X)\n", "def test_column_transformer_transformer_weights():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           transformer_weights={'trans1': 2})\n    X_res_first = np.array([[0., 1., 2.]]).T * 2\n    assert_array_equal(ct.fit_transform(X_array), X_res_first)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_first)\n    assert len(ct.transformers_) == 1\n    assert ct.transformers_[0][1] == 'trans1'\n\n    # ignored if transformer is 'passthrough'\n    ct = ColumnTransformer([('trans1', 'passthrough', [0])],\n                           transformer_weights={'trans1': 2})\n    X_res_first = np.array([[0., 1., 2.]]).T\n    assert_array_equal(ct.fit_transform(X_array), X_res_first)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_first)\n    assert len(ct.transformers_) == 1\n    assert ct.transformers_[0][1] == 'passthrough'\n\n    # ignored if transformer is 'drop'\n    ct = ColumnTransformer([('trans1', 'drop', [0])],\n                           transformer_weights={'trans1': 2})\n    X_res_first = np.array([[0., 1., 2.]]).T\n    assert_array_equal(ct.fit_transform(X_array), X_res_first)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_first)\n    assert len(ct.transformers_) == 1\n    assert ct.transformers_[0][1] == 'drop'\n\n    # missing weight for a transformer\n    ct = ColumnTransformer([('trans1', Trans(), [0]),\n                            ('trans2', Trans(), [1])],\n                           transformer_weights={'trans1': 2})\n    X_res_both = np.array([[0, 1, 2], [2, 4, 6]]).T\n    assert_array_equal(ct.fit_transform(X_array), X_res_both)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[0][1] == 'trans1'\n    assert ct.transformers_[1][1] == 'trans2'\n\n"], "sample_895": ["def test_column_transformer_set_output_after_fitting_pandas_out():\n    \"\"\"Check column transformer behavior with set_output after fitting.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame(\n        {\n            \"pet\": pd.Series([\"dog\", \"cat\", \"snake\"], dtype=\"category\"),\n            \"age\": [1.4, 2.1, 4.4],\n            \"height\": [20, 40, 10],\n        }\n    )\n    ct = ColumnTransformer(\n        [\n            (\n                \"color_encode\",\n                OneHotEncoder(sparse_output=False, dtype=\"int16\"),\n                [\"pet\"],\n            ),\n            (\"age\", StandardScaler(), [\"age\"]),\n        ]\n    )\n    ct.fit(df)\n\n    # fit without calling set_output\n    X_trans = ct.transform(df)\n    assert isinstance(X_trans, np.ndarray)\n    assert X_trans.dtype == \"float64\"\n\n    ct.set_output(transform=\"pandas\")\n    X_trans_df = ct.transform(df)\n    expected_dtypes = {\n        \"pet_cat\": \"int16\",\n        \"pet_dog\": \"int16\",\n        \"pet_snake\": \"int16\",\n        \"height\": \"int64\",\n        \"age\": \"float64\",\n    }\n    for col, dtype in X_trans_df.dtypes.items():\n        assert dtype == expected_dtypes[col]\n", "def test_column_transformer_feature_names_out_with_remainder_transformer():\n    \"\"\"Check that feature_names_out is correct when remainder is a transformer\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    df = pd.DataFrame(\n        {\"col1\": [\"a\", \"a\", \"b\"], \"col2\": [\"z\", \"z\", \"z\"], \"col3\": [1, 2, 3]}\n    )\n\n    remainder = StandardScaler()\n    ct = ColumnTransformer(\n        [(\"ohe\", OneHotEncoder(), [\"col1\", \"col2\"])], remainder=remainder\n    )\n    ct.fit(df)\n\n    names = ct.get_feature_names_out()\n    assert isinstance(names, np.ndarray)\n    assert names.dtype == object\n    assert_array_equal(names, [\"ohe__col1_a\", \"ohe__col1_b\", \"ohe__col2_z\", \"col3\"])\n", "def test_column_transformer_feature_names_out_transformers_names():\n    \"\"\"Check that the transformers names are correctly prefixed in feature_names_out.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame([[1, 2, 3, 4]], columns=[\"a\", \"b\", \"c\", \"d\"])\n    ct = ColumnTransformer(\n        [\n            (\"transformer_1\", TransWithNames(), [\"a\", \"b\"]),\n            (\"transformer_2\", TransWithNames(), [\"c\", \"d\"]),\n        ],\n        remainder=\"passthrough\",\n    )\n    ct.fit(df)\n\n    feature_names_out = ct.get_feature_names_out()\n    assert_array_equal(\n        feature_names_out,\n        [\"transformer_1__a\", \"transformer_1__b\", \"transformer_2__c\", \"transformer_2__d\"],\n    )\n", "def test_column_transformer_transformer_weights_to_remainder():\n    X_array = np.array([[0, 1, 2], [2, 4, 6], [8, 6, 4]]).T\n    X_res_both = 2 * X_array.copy()[:, 1:3]\n\n    # columns are doubled when remainder = DoubleTrans\n    ct = ColumnTransformer(\n        [(\"trans1\", \"drop\", [0])], remainder=DoubleTrans(), transformer_weights={\"remainder\": 10}\n    )\n    assert_array_equal(ct.fit_transform(X_array), 10 * X_res_both)\n    assert_array_equal(ct.fit(X_array).transform(X_array), 10 * X_res_both)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == \"remainder\"\n    assert isinstance(ct.transformers_[-1][1], DoubleTrans)\n    assert_array_equal(ct.transformers_[-1][2], [1, 2])\n\n    # check that other transformers are not affected\n    X_res_both = X_array.copy()\n    ct = ColumnTransformer(\n        [(\"trans1\", Trans(), [0, 1]), (\"trans2\", \"drop\", [2])],\n        remainder=DoubleTrans(),\n        transformer_weights={\"remainder\": 10},\n    )\n    assert_array_equal(ct.fit_transform(X_array), X_res_both)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both)\n    assert len(ct.transformers_) == 3\n    assert ct.transformers_[-1][0] == \"remainder\"\n    assert isinstance(ct.transformers_[-1][1], DoubleTrans)\n    assert_array_equal(ct.transformers_[-1][2], [2])\n", "def test_column_transformer_pickle():\n    # Test pickling and unpickling of ColumnTransformer\n    # including the transformer weights\n    pd = pytest.importorskip(\"pandas\")\n\n    X_df = pd.DataFrame({\"a\": [1, 2], \"b\": [2, 3]})\n    trans = ColumnTransformer(\n        [(\"a\", StandardScaler(), [\"a\"]), (\"b\", StandardScaler(), [\"b\"])],\n        transformer_weights={\"a\": 0.1, \"b\": 0.2},\n    )\n\n    trans.fit(X_df)\n    trans_picked = pickle.loads(pickle.dumps(trans))\n\n    assert_array_equal(trans.transform(X_df), trans_picked.transform(X_df))\n    assert trans.get_params() == trans_picked.get_params()\n"], "sample_450": ["def test_get_admin_log_tag(self):\n    \"\"\"\n    Test the get_admin_log template tag.\n    \"\"\"\n    template_str = \"{% load admin_utils %}{% get_admin_log 10 as admin_log %}\"\n    template = template.Template(template_str)\n    context = template.Context({\"log_entries\": LogEntry.objects.all()})\n    rendered = template.render(context)\n    self.assertEqual(rendered, \"\")\n\n    # Test with 'for_user' argument\n    template_str = \"{% load admin_utils %}{% get_admin_log 10 as admin_log for_user user %}\"\n    template = template.Template(template_str)\n    context = template.Context({\"log_entries\": LogEntry.objects.all(), \"user\": self.user})\n    rendered = template.render(context)\n    self.assertEqual(rendered, \"\")\n\n    # Test with invalid 'limit' argument\n    template_str = \"{% load admin_utils %}{% get_admin_log foo as admin_log %}\"\n    template = template.Template(template_str)\n    context = template.Context({\"log_entries\": LogEntry.objects.all()})\n    with self.assertRaises(template.TemplateSyntaxError):\n        template.render(context)\n\n    # Test with invalid 'for_user' argument\n    template_str = \"{% load admin_utils %}{% get_admin_log 10 as admin_log for_user foo %}\"\n    template = template.Template(template_str)\n    context = template.Context({\"log_entries\": LogEntry.objects.all()})\n    with self.assertRaises(template.TemplateSyntaxError):\n        template.render(context)\n", "def test_get_admin_log_tag(self):\n    \"\"\"\n    Test the get_admin_log template tag.\n    \"\"\"\n    template = \"{% load admin_utils %}{% get_admin_log 10 as admin_log for_user super %}\"\n    rendered_template = template.render(context={\"log_entries\": LogEntry.objects.all()})\n    self.assertIn(\"admin_log\", rendered_template.context)\n    self.assertEqual(len(rendered_template.context[\"admin_log\"]), 10)\n", "def test_get_admin_log_tag(self):\n    \"\"\"\n    Test the get_admin_log template tag.\n    \"\"\"\n    template = template.Template(\"{% load admin_utils %}{% get_admin_log 10 as admin_log %}\")\n    context = {\"log_entries\": LogEntry.objects.all()}\n    rendered = template.render(context)\n    self.assertEqual(rendered, \"\")\n\n    context = {\"log_entries\": LogEntry.objects.all(), \"admin_log\": []}\n    rendered = template.render(context)\n    self.assertEqual(len(context[\"admin_log\"]), 10)\n\n    template = template.Template(\n        \"{% load admin_utils %}{% get_admin_log 10 as admin_log for_user user %}\"\n    )\n    context = {\"log_entries\": LogEntry.objects.all(), \"user\": self.user}\n    rendered = template.render(context)\n    self.assertEqual(rendered, \"\")\n\n    context = {\"log_entries\": LogEntry.objects.all(), \"admin_log\": []}\n    rendered = template.render(context)\n    self.assertEqual(len(context[\"admin_log\"]), 0)\n\n    template = template.Template(\n        \"{% load admin_utils %}{% get_admin_log 10 as admin_log for_user 999 %}\"\n    )\n    context = {\"log_entries\": LogEntry.objects.all()}\n    rendered = template.render(context)\n    self.assertEqual(len(context[\"admin_log\"]), 0)\n\n    template = template.Template(\n        \"{% load admin_utils %}{% get_admin_log 10 as admin_log for_user %}\"\n    )\n    context = {\"log_entries\": LogEntry.objects.all()}\n    with self.assertRaises(template.TemplateSyntaxError):\n        template.render(context)\n", "def test_get_admin_log_template_tag(self):\n    \"\"\"\n    The get_admin_log template tag should correctly render the admin log.\n    \"\"\"\n    template_str = \"{% load admin_tags %}{% get_admin_log 10 as admin_log %}{% for entry in admin_log %}{{ entry.object_repr }}{% endfor %}\"\n    template = template.Template(template_str)\n    context = {\n        \"log_entries\": LogEntry.objects.all(),\n    }\n    rendered_template = template.render(context)\n    self.assertContains(rendered_template, \"Title\")\n    self.assertContains(rendered_template, \"Changed something\")\n\n    # Test with a user filter\n    template_str = \"{% load admin_tags %}{% get_admin_log 10 as admin_log for_user super %}{% for entry in admin_log %}{{ entry.object_repr }}{% endfor %}\"\n    template = template.Template(template_str)\n    context = {\n        \"log_entries\": LogEntry.objects.all(),\n        \"super\": self.user,\n    }\n    rendered_template = template.render(context)\n    self.assertContains(rendered_template, \"Title\")\n    self.assertContains(rendered_template, \"Changed something\")\n\n    # Test with a non-existent user filter\n    template_str = \"{% load admin_tags %}{% get_admin_log 10 as admin_log for_user non_existent_user %}{% for entry in admin_log %}{{ entry.object_repr }}{% endfor %}\"\n    template = template.Template(template_str)\n    context = {\n        \"log_entries\": LogEntry.objects.all(),\n    }\n    rendered_template = template.render(context)\n    self.assertNotContains(rendered_template, \"Title\")\n    self.assertNotContains(rendered_template, \"Changed something\")\n\n    # Test with an invalid user filter\n    template_str = \"{% load admin_tags %}{% get_admin_log 10 as admin_log for_user abc %}{% for entry in admin_log %}{{ entry.object_repr }}{% endfor %}\"\n    template = template.Template(template_str)\n    context = {\n        \"log_entries\": LogEntry.objects.all(),\n    }\n    with self.assertRaises(template.TemplateSyntaxError):\n        template.render(context)\n\n    # Test with an invalid limit\n    template_str = \"{% load admin_tags %}{% get_admin_log abc as admin_log %}{% for entry in admin_log %}{{ entry.object_repr }}{% endfor %}\"\n    template = template.Template(template_str)\n    context = {\n        \"log_entries\": LogEntry.objects.all(),\n    }\n    with", "def test_get_admin_log_tag(self):\n    \"\"\"\n    Test the get_admin_log template tag.\n    \"\"\"\n    template = \"\"\"\n    {% load admin_utils %}\n    {% get_admin_log 10 as admin_log %}\n    {{ admin_log|length }}\n    \"\"\"\n\n    rendered_template = template.render(RequestContext(self.client.get(reverse(\"admin:index\")).request))\n    self.assertEqual(rendered_template.strip(), \"1\")\n\n    template = \"\"\"\n    {% load admin_utils %}\n    {% get_admin_log 10 as admin_log for_user user %}\n    {{ admin_log|length }}\n    \"\"\"\n\n    rendered_template = template.render(RequestContext(self.client.get(reverse(\"admin:index\")).request, {\"user\": self.user}))\n    self.assertEqual(rendered_template.strip(), \"1\")\n\n    template = \"\"\"\n    {% load admin_utils %}\n    {% get_admin_log 10 as admin_log for_user 23 %}\n    {{ admin_log|length }}\n    \"\"\"\n\n    rendered_template = template.render(RequestContext(self.client.get(reverse(\"admin:index\")).request))\n    self.assertEqual(rendered_template.strip(), \"1\")\n\n    template = \"\"\"\n    {% load admin_utils %}\n    {% get_admin_log 10 as admin_log %}\n    {% for entry in admin_log %}\n        {{ entry }}\n    {% endfor %}\n    \"\"\"\n\n    rendered_template = template.render(RequestContext(self.client.get(reverse(\"admin:index\")).request))\n    self.assertIn(str(LogEntry.objects.first()), rendered_template)\n\n    # Test invalid usage\n    template = \"\"\"\n    {% load admin_utils %}\n    {% get_admin_log %}\n    \"\"\"\n\n    with self.assertRaises(template.TemplateSyntaxError):\n        template.render(RequestContext(self.client.get(reverse(\"admin:index\")).request))\n\n    template = \"\"\"\n    {% load admin_utils %}\n    {% get_admin_log 10 %}\n    \"\"\"\n\n    with self.assertRaises(template.TemplateSyntaxError):\n        template.render(RequestContext(self.client.get(reverse(\"admin:index\")).request))\n\n    template = \"\"\"\n    {% load admin_utils %}\n    {% get_admin_log 10 as %}\n    \"\"\"\n\n    with self.assertRaises(template.TemplateSyntaxError):\n        template.render(RequestContext(self.client.get(reverse(\"admin:index\")).request))\n\n    template = \"\"\"\n    {% load admin_utils %}\n    {% get_admin_log 10 as admin_log for %}\n    \"\"\"\n\n    with self.assertRaises(template.TemplateSyntaxError):\n        template.render(RequestContext(self.client.get(reverse(\"admin:index\")).request))\n\n    template = \"\"\"\n    {% load admin_utils %}\n    {% get_admin_log 10 as admin_log for_user %}\n    \"\"\"\n\n    with self.assertRaises(template.TemplateSyntaxError):\n        template.render(RequestContext(self.client.get(reverse"], "sample_205": ["def test_update_error_dict(self):\n    exception = ValidationError({'field1': ['error1']})\n    error_dict = {'field2': ['error2']}\n    exception.update_error_dict(error_dict)\n    self.assertEqual(error_dict, {'field1': ['error1'], 'field2': ['error2']})\n\n    exception = ValidationError('error')\n    error_dict = {'field1': ['error1']}\n    exception.update_error_dict(error_dict)\n    self.assertEqual(error_dict, {'field1': ['error1'], NON_FIELD_ERRORS: ['error']})\n\n    exception = ValidationError(['error1', 'error2'])\n    error_dict = {'field1': ['error3']}\n    exception.update_error_dict(error_dict)\n    self.assertEqual(error_dict, {'field1': ['error3'], NON_FIELD_ERRORS: ['error1', 'error2']})\n", "def test_update_error_dict(self):\n    error_dict1 = {'field1': 'error1'}\n    error_dict2 = {'field2': 'error2'}\n    error1 = ValidationError(error_dict1)\n    error2 = ValidationError(error_dict2)\n    updated_error_dict = error1.update_error_dict({})\n    self.assertEqual(updated_error_dict, error_dict1)\n    updated_error_dict = error2.update_error_dict(updated_error_dict)\n    self.assertEqual(updated_error_dict, {**error_dict1, **error_dict2})\n\n    error_dict3 = {'field1': 'error3'}\n    error3 = ValidationError(error_dict3)\n    updated_error_dict = error3.update_error_dict(updated_error_dict)\n    self.assertEqual(updated_error_dict, {**error_dict1, 'field2': 'error2', 'field1': ['error1', 'error3']})\n\n    error_dict4 = {'__all__': 'error4'}\n    error4 = ValidationError(error_dict4)\n    updated_error_dict = error4.update_error_dict(updated_error_dict)\n    self.assertEqual(updated_error_dict, {**error_dict1, 'field2': 'error2', 'field1': ['error1', 'error3'], '__all__': ['error4']})\n\n    error5 = ValidationError('error5')\n    updated_error_dict = error5.update_error_dict(updated_error_dict)\n    self.assertEqual(updated_error_dict, {**error_dict1, 'field2': 'error2', 'field1': ['error1', 'error3'], '__all__': ['error4', 'error5']})\n\n    error6 = ValidationError([error1, error5])\n    updated_error_dict = error6.update_error_dict(updated_error_dict)\n    self.assertEqual(updated_error_dict, {**error_dict1, 'field2': 'error2', 'field1': ['error1', 'error3'], '__all__': ['error4', 'error5', 'error1', 'error5']})\n", "    def test_iter(self):\n        error1 = ValidationError('message')\n        self.assertEqual(list(error1), ['message'])\n\n        error2 = ValidationError({\n            'field1': 'message1',\n            'field2': 'message2',\n        })\n        self.assertEqual(list(error2), [('field1', ['message1']), ('field2', ['message2'])])\n\n        error3 = ValidationError([\n            ValidationError({'field1': 'field error', 'field2': 'other'}),\n            'message',\n        ])\n        self.assertEqual(list(error3), [\n            ('field1', ['field error']),\n            ('field2', ['other']),\n            ('__all__', ['message']),\n        ])\n\n        error4 = ValidationError({\n            'field1': [\n                ValidationError({'field1': 'field error', 'field2': 'other'}),\n                'message',\n            ],\n        })\n        self.assertEqual(list(error4), [\n            ('field1', [\n                ('field1', ['field error']),\n                ('field2', ['other']),\n                ('__all__', ['message']),\n            ]),\n        ])\n", "def test_repr(self):\n    error = ValidationError('message')\n    self.assertEqual(repr(error), \"ValidationError('message')\")\n\n    error = ValidationError('message', code='my_code')\n    self.assertEqual(repr(error), \"ValidationError('message', code='my_code')\")\n\n    error = ValidationError('error %(parm1)s %(parm2)s', params={'parm1': 'val1', 'parm2': 'val2'})\n    self.assertEqual(repr(error), \"ValidationError('error %(parm1)s %(parm2)s', params={'parm1': 'val1', 'parm2': 'val2'})\")\n\n    error_dict = {'field1': 'message', 'field2': 'other'}\n    error = ValidationError(error_dict)\n    self.assertEqual(repr(error), \"ValidationError({'field1': 'message', 'field2': 'other'})\")\n", "def test_update_error_dict(self):\n    error_dict = {}\n    exception = ValidationError({'field1': 'message'})\n    self.assertEqual(exception.update_error_dict(error_dict), {'field1': ['message']})\n    error_dict = {'field1': ['message'], 'field2': ['error']}\n    exception = ValidationError({'field1': ['error1', 'error2'], 'field3': 'error'})\n    self.assertEqual(exception.update_error_dict(error_dict), {\n        'field1': ['message', 'error1', 'error2'],\n        'field2': ['error'],\n        'field3': ['error'],\n    })\n    exception = ValidationError('message')\n    error_dict = {}\n    self.assertEqual(exception.update_error_dict(error_dict), {NON_FIELD_ERRORS: ['message']})\n    exception = ValidationError('message')\n    error_dict = {'field1': ['message']}\n    self.assertEqual(exception.update_error_dict(error_dict), {\n        'field1': ['message'],\n        NON_FIELD_ERRORS: ['message'],\n    })\n"], "sample_379": ["def test_mark_safe_safe_data(self):\n    \"\"\"\n    Test that mark_safe() returns the original object if it's already a SafeData.\n    \"\"\"\n    s = SafeString('a&b')\n    self.assertIs(mark_safe(s), s)\n", "def test_mark_safe_with_bytes(self):\n    \"\"\"\n    Calling mark_safe with a bytes instance should raise a TypeError.\n    \"\"\"\n    with self.assertRaisesMessage(TypeError, \"Cannot mark a bytes object as safe\"):\n        mark_safe(b'a&b')\n", "def test_mark_safe_repeated_calls(self):\n    \"\"\"\n    Repeated calls to mark_safe on the same string do not cause any issues.\n    \"\"\"\n    s = 'a&b'\n    self.assertIs(mark_safe(mark_safe(s)), mark_safe(s))\n\n    self.assertRenderEqual('{{ s }}', 'a&b', s=mark_safe(mark_safe(s)))\n    self.assertRenderEqual('{{ s|force_escape }}', 'a&amp;b', s=mark_safe(mark_safe(s)))\n", "def test_mark_safe_already_safe(self):\n    \"\"\"\n    mark_safe on a SafeString returns the original SafeString instance.\n    \"\"\"\n    s = mark_safe('a&b')\n    self.assertIs(mark_safe(s), s)\n", "    def test_mark_safe_with_safe_string(self):\n        s = mark_safe(mark_safe('a&b'))\n        self.assertIsInstance(s, SafeString)\n        self.assertRenderEqual('{{ s }}', 'a&b', s=s)\n        self.assertRenderEqual('{{ s|force_escape }}', 'a&amp;b', s=s)\n"], "sample_747": ["def test_QuantileTransformer_not_fitted():\n    qt = QuantileTransformer()\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    assert_raises(NotFittedError, qt.transform, X)\n", "def test_power_transformer_n_features():\n    pt = PowerTransformer(method='box-cox', standardize=False)\n    X = np.abs(X_2d)[:, 0:1]\n\n    # Test that the number of features is correctly stored in the object\n    pt.fit(X)\n    assert_equal(pt.n_features_, 1)\n\n    X = np.abs(X_2d)\n    pt.fit(X)\n    assert_equal(pt.n_features_, X.shape[1])\n", "def test_quantile_transform_preserve_type():\n    X = np.array([[0., 1.], [1., 2.]], dtype=np.float32)\n    transformer = QuantileTransformer(n_quantiles=10, random_state=0)\n    X_trans = transformer.fit_transform(X)\n    assert X_trans.dtype == X.dtype\n\n    transformer = QuantileTransformer(n_quantiles=10, random_state=0)\n    X_trans = transformer.fit_transform(X.astype(np.float64))\n    assert X_trans.dtype == X.dtype\n\n    transformer = QuantileTransformer(n_quantiles=10, random_state=0)\n    X_trans = transformer.fit_transform(X.astype(np.int64))\n    assert X_trans.dtype == X.dtype\n", "def test_power_transformer_large_values():\n    pt = PowerTransformer(method='box-cox')\n    X = np.abs(np.random.RandomState(0).randn(100, 10)) * 1e100\n\n    # The transformation should not run into numerical issues\n    pt.fit(X)\n\n    # Check that the inverse transformation can be applied\n    X_trans = pt.transform(X)\n    assert_array_almost_equal(pt.inverse_transform(X_trans), X)\n\n    # Test function interface\n    X_trans_func = power_transform(X, method='box-cox')\n    assert_array_almost_equal(X_trans_func, X_trans)\n", "def test_power_transformer_bad_input():\n    # Test that PowerTransformer raises errors for invalid input\n    pt = PowerTransformer(method='box-cox')\n\n    # Test that an error is raised for non-array input\n    assert_raise_message(TypeError, 'Expected 2D array', pt.fit, 'invalid')\n\n    # Test that an error is raised for non-positive input values\n    X = np.array([[0, 1], [2, 3]])\n    assert_raise_message(ValueError, 'strictly positive', pt.fit, X)\n\n    # Test that an error is raised for array-like input\n    X = [[0, 1], [2, 3]]\n    assert_raise_message(TypeError, 'Expected 2D array', pt.fit, X)\n"], "sample_0": ["def test_conversion_between_supported_types(UncertClass1, UncertClass2):\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = UncertClass1(uncert)\n    final_uncert = start_uncert.represent_as(UncertClass2)\n    assert isinstance(final_uncert, UncertClass2)\n    assert start_uncert.quantity.unit.is_equivalent(final_uncert.quantity.unit)\n", "def test_conversion_between_uncertainty_types_supported(UncertClass, otherUncertClass):\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = UncertClass(uncert)\n    final_uncert = start_uncert.represent_as(otherUncertClass)\n    assert isinstance(final_uncert, otherUncertClass)\n    assert final_uncert.array is not start_uncert.array\n    assert_array_equal(final_uncert.array, final_uncert.array)\n    assert final_uncert.unit == final_uncert.unit\n", "def test_nduncertainty_repr():\n    # Test that NDUncertainty's __repr__ works\n    uncertainty = StdDevUncertainty([1, 2, 3])\n    assert repr(uncertainty).startswith('StdDevUncertainty([')\n\n    # Test that UnknownUncertainty's __repr__ works\n    uncertainty = UnknownUncertainty([1, 2, 3])\n    assert repr(uncertainty).startswith('UnknownUncertainty([')\n\n    # Test that VarianceUncertainty's __repr__ works\n    uncertainty = VarianceUncertainty([1, 2, 3])\n    assert repr(uncertainty).startswith('VarianceUncertainty([')\n\n    # Test that InverseVariance's __repr__ works\n    uncertainty = InverseVariance([1, 2, 3])\n    assert repr(uncertainty).startswith('InverseVariance([')\n\n    # Test that FakeUncertainty's __repr__ works\n    uncertainty = FakeUncertainty([1, 2, 3])\n    assert repr(uncertainty).startswith('FakeUncertainty([')\n", "def test_conversion_between_supported_types(UncertClass, other_uncert_class):\n    if UncertClass is other_uncert_class:\n        pytest.skip('Conversion between same type is covered elsewhere')\n\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = UncertClass(uncert)\n    final_uncert = start_uncert.represent_as(other_uncert_class)\n    assert isinstance(final_uncert, other_uncert_class)\n", "def test_getitem(UncertClass):\n    data = np.arange(10)\n    uncert = UncertClass(data)\n    sliced_data = data[::2]\n    sliced_uncert = uncert[::2]\n    assert_array_equal(sliced_uncert.array, sliced_data)\n    assert sliced_uncert.unit == uncert.unit\n\n    # Check that slicing returns a reference\n    sliced_uncert.array[0] = 10\n    assert sliced_uncert.array[0] == 10\n    assert sliced_uncert.array is not uncert.array\n    assert sliced_uncert.array[1:] is not sliced_data[1:]\n\n    # Test slicing with negative step\n    sliced_uncert = uncert[::-2]\n    assert_array_equal(sliced_uncert.array, sliced_data[::-1])\n    assert sliced_uncert.unit == uncert.unit\n\n    # Test slicing with negative start\n    sliced_uncert = uncert[-5::-2]\n    assert_array_equal(sliced_uncert.array, sliced_data[-5::-2])\n    assert sliced_uncert.unit == uncert.unit\n\n    # Test slicing with negative end\n    sliced_uncert = uncert[:-2:2]\n    assert_array_equal(sliced_uncert.array, sliced_data[:-2:2])\n    assert sliced_uncert.unit == uncert.unit\n\n    # Test slicing with negative start and end\n    sliced_uncert = uncert[-5:-2:2]\n    assert_array_equal(sliced_uncert.array, sliced_data[-5:-2:2])\n    assert sliced_uncert.unit == uncert.unit\n"], "sample_1041": ["def test_matrix_element_subs():\n    i, j = symbols('i j')\n    M = MatrixSymbol('M', 3, 3)\n    ME = M[i, j]\n    assert ME.subs({i: 0, j: 1}) == M[0, 1]\n    assert ME.subs(M, Matrix([[1, 2], [3, 4], [5, 6]])) == Matrix([[1, 2], [3, 4], [5, 6]])[i, j]\n    assert ME.subs({M: Matrix([[1, 2], [3, 4], [5, 6]]), j: 0}) == Matrix([[1, 2], [3, 4], [5, 6]])[i, 0]\n", "def test_MatrixExpr_from_index_summation():\n    i, j, k, l, N = symbols('i j k l N')\n    A = MatrixSymbol('A', N, N)\n    B = MatrixSymbol('B', N, N)\n    C = MatrixSymbol('C', N, N)\n    D = MatrixSymbol('D', N, N)\n    E = MatrixSymbol('E', N, N)\n    expr1 = Sum(A[i, j]*B[j, k], (j, 0, N-1))\n    assert MatrixExpr.from_index_summation(expr1) == A*B\n    expr2 = Sum(A[i, j]*B[k, j], (j, 0, N-1))\n    assert MatrixExpr.from_index_summation(expr2) == A.T*B\n    expr3 = Sum(A[i, i], (i, 0, N-1))\n    assert MatrixExpr.from_index_summation(expr3) == A.trace()\n    expr4 = Sum(A[i, j]*B[k, j]*A[l, k], (j, 0, N-1), (k, 0, N-1))\n    assert MatrixExpr.from_index_summation(expr4) == A*B.T*A.T\n    expr5 = Sum(A[i, j]*B[j, k]*C[k, l], (j, 0, N-1), (k, 0, N-1))\n    assert MatrixExpr.from_index_summation(expr5) == A*B*C\n    expr6 = Sum(KroneckerDelta(i, j)*KroneckerDelta(k, l), (j, 0, N-1), (k, 0, N-1))\n    assert MatrixExpr.from_index_summation(expr6) == A*B\n    expr7 = Sum(A[i, j]*B[j, k]*C[k, l]*D[l, i], (j, 0, N-1), (k, 0, N-1), (l, 0, N-1))\n    assert MatrixExpr.from_index_summation(expr7) == A*B*C*D\n", "def test_matrixelement_subs():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    i, j = symbols('i, j')\n    assert A[i, j].subs(A, B) == B[i, j]\n    assert A[i, j].subs({i: 0, j: 1}) == A[0, 1]\n    assert A[i, j].subs(j, 0) == A[i, 0]\n    assert A[i, j].subs(i, j) == A[j, j]\n    assert A[i, i].subs(i, 0) == A[0, 0]\n    assert A[i, i].subs(i, j) == A[j, j]\n    assert A[1, 1].subs(A, B) == B[1, 1]\n", "def test_MatrixExpr_from_index_summation():\n    from sympy import MatrixSymbol, MatrixExpr, Sum, Dummy, Eq\n\n    N = symbols('N')\n    i, j, k, l = symbols('i j k l')\n    A = MatrixSymbol('A', N, N)\n    B = MatrixSymbol('B', N, N)\n    C = MatrixSymbol('C', N, N)\n\n    expr = Sum(A[i, j]*B[j, k], (j, 0, N-1))\n    assert Eq(MatrixExpr.from_index_summation(expr), A*B)\n\n    expr = Sum(A[i, j]*B[k, j], (j, 0, N-1))\n    assert Eq(MatrixExpr.from_index_summation(expr), A.T*B)\n\n    expr = Sum(A[i, i], (i, 0, N-1))\n    assert Eq(MatrixExpr.from_index_summation(expr), A.trace())\n\n    expr = Sum(A[i, j]*B[k, j]*A[l, k], (j, 0, N-1), (k, 0, N-1))\n    assert Eq(MatrixExpr.from_index_summation(expr), A*B.T*A.T)\n\n    expr = Sum(A[j, i]*B[j, k], (j, 0, N-1))\n    assert Eq(MatrixExpr.from_index_summation(expr, first_index=i), A.T*B)\n\n    expr = Sum(A[i, j]*B[l, k]*C[j, k], (j, 0, N-1), (k, 0, N-1))\n    assert Eq(MatrixExpr.from_index_summation(expr), A*B*C.T)\n\n    d = Dummy()\n    expr = Sum(A[i, j]*B[j, d], (j, 0, N-1))\n    assert Eq(MatrixExpr.from_index_summation(expr), A*B)\n", "def test_matrixelement_from_index_summation():\n    i, j, k, N = symbols('i j k N')\n\n    A = MatrixSymbol('A', N, N)\n    B = MatrixSymbol('B', N, N)\n    C = MatrixSymbol('C', N, N)\n    D = MatrixSymbol('D', N, N)\n\n    expr1 = Sum(A[i, j]*B[j, k], (j, 0, N-1))\n    expr2 = Sum(B[j, k]*A[j, i], (j, 0, N-1))\n    expr3 = Sum(A[i, j]*B[k, j], (j, 0, N-1))\n    expr4 = Sum(A[i, i], (i, 0, N-1))\n    expr5 = Sum(KroneckerDelta(i, j), (i, 0, N-1))\n    expr6 = Sum(A[j, i]*A[j, k], (j, 0, N-1))\n    expr7 = Sum(A[j, i]*C[j, j]*D[j, k], (j, 0, N-1))\n\n    assert MatrixExpr.from_index_summation(expr1) == A*B\n    assert MatrixExpr.from_index_summation(expr2) == B.T*A.T\n    assert MatrixExpr.from_index_summation(expr3) == A*B.T\n    assert MatrixExpr.from_index_summation(expr4) == trace(A)\n    assert MatrixExpr.from_index_summation(expr5) == N\n    assert MatrixExpr.from_index_summation(expr6) == C.T*A*A.T\n    assert MatrixExpr.from_index_summation(expr7) == C*D.T*A.T\n"], "sample_134": ["def test_serialize_type_serializer(self):\n    string, imports = MigrationWriter.serialize(type)\n    self.assertEqual(string, 'type')\n    self.assertEqual(imports, set())\n\n    class TestTypeSerializer(BaseSerializer):\n            return repr(self.value), {}\n\n    MigrationWriter.register_serializer(type, TestTypeSerializer)\n    string, imports = MigrationWriter.serialize(type)\n    self.assertEqual(string, \"type\")\n    self.assertEqual(imports, set())\n    MigrationWriter.unregister_serializer(type)\n", "def test_serialize_function_type(self):\n        return 'Hello, World!'\n\n    self.assertSerializedResultEqual(\n        my_function,\n        ('migrations.test_writer.my_function', {'import migrations.test_writer'})\n    )\n\n        return arg1, arg2, arg3\n\n    self.assertSerializedResultEqual(\n        my_function_with_default_value,\n        ('migrations.test_writer.my_function_with_default_value', {'import migrations.test_writer'})\n    )\n\n    class MyClass:\n            return 'Hello, World!'\n\n    obj = MyClass()\n    self.assertSerializedResultEqual(\n        obj.my_method,\n        ('migrations.test_writer.MyClass().my_method', {'import migrations.test_writer'})\n    )\n", "def test_serialize_function_type(self):\n    \"\"\"\n    Tests serialization of function types.\n    \"\"\"\n        pass\n\n    self.assertSerializedResultEqual(\n        my_function,\n        (\"migrations.test_writer.my_function\", {'import migrations.test_writer'})\n    )\n\n    class MyClass:\n            pass\n\n    self.assertSerializedResultEqual(\n        MyClass.my_method,\n        (\"migrations.test_writer.MyClass.my_method\", {'import migrations.test_writer'})\n    )\n\n    with self.assertRaisesMessage(ValueError, 'Could not find function my_function in migrations.test_writer'):\n        self.serialize_round_trip(lambda: my_function())\n\n    with self.assertRaisesMessage(ValueError, 'Could not find function my_method in migrations.test_writer'):\n        self.serialize_round_trip(MyClass.my_method)\n\n        yield 1\n        yield 2\n\n    self.assertSerializedResultEqual(\n        my_generator,\n        (\"migrations.test_writer.my_generator\", {'import migrations.test_writer'})\n    )\n\n    async def my_coroutine():\n        pass\n\n    self.assertSerializedResultEqual(\n        my_coroutine,\n        (\"migrations.test_writer.my_coroutine\", {'import migrations.test_writer'})\n    )\n", "    def test_serialize_lazy_import(self):\n        \"\"\"\n        Test serialization of lazy imports (lazy_imports=True in MigrationWriter).\n        \"\"\"\n        migration = type(\"Migration\", (migrations.Migration,), {\n            \"operations\": [\n                migrations.AddField(\"mymodel\", \"myfield\", models.DateTimeField(\n                    default=datetime.datetime(2012, 1, 1, 1, 1, tzinfo=utc),\n                )),\n            ]\n        })\n        writer = MigrationWriter(migration, include_header=False, lazy_imports=True)\n        output = writer.as_string()\n        self.assertIn(\n            \"from __future__ import absolute_import\\n\"\n            \"from django.db import migrations, models\\n\"\n            \"from django.utils.timezone import utc\\n\"\n            \"from django.db.migrations import operations\\n\"\n            \"from django.db.migrations import RunPython\\n\"\n            \"from django.db.migrations import RunSQL\\n\",\n            output\n        )\n        self.assertIn(\n            \"operations = [\\n\"\n            \"    migrations.AddField(\\n\"\n            \"        model_name='mymodel',\\n\"\n            \"        name='myfield',\\n\"\n            \"        field=models.DateTimeField(default=datetime.datetime(2012, 1, 1, 1, 1, tzinfo=utc)),\\n\"\n            \"    ),\\n\"\n            \"]\\n\",\n            output\n        )\n", "def test_serialize_uuid_from_bytes(self):\n    \"\"\"Tests UUID serialization from bytes.\"\"\"\n    # Create a UUID from bytes.\n    uuid_from_bytes = uuid.UUID(bytes=b'\\x12\\x34\\x56\\x78\\x90\\xab\\xcd\\xef\\x01\\x23\\x45\\x67\\x89\\xab\\xcd\\xef')\n\n    # Serialize and deserialize the UUID.\n    serialized_uuid, imports = MigrationWriter.serialize(uuid_from_bytes)\n\n    # Check the serialized string.\n    expected_serialized_uuid = \"uuid.UUID('12345678-90ab-cdef-0123-456789abcdef')\"\n    self.assertEqual(serialized_uuid, expected_serialized_uuid)\n\n    # Check the imports.\n    self.assertEqual(imports, {'import uuid'})\n\n    # Deserialize the UUID and compare it to the original UUID.\n    self.assertEqual(self.safe_exec(\"%s\\ntest_uuid_result = %s\" % (\"\\n\".join(imports), serialized_uuid), uuid_from_bytes)['test_uuid_result'], uuid_from_bytes)\n"], "sample_457": ["    def test_deferrable_eq(self):\n        self.assertEqual(Deferrable.DEFERRED, Deferrable.DEFERRED)\n        self.assertEqual(Deferrable.IMMEDIATE, Deferrable.IMMEDIATE)\n        self.assertNotEqual(Deferrable.DEFERRED, Deferrable.IMMEDIATE)\n        self.assertNotEqual(Deferrable.IMMEDIATE, Deferrable.DEFERRED)\n        self.assertNotEqual(Deferrable.DEFERRED, \"DEFERRED\")\n        self.assertNotEqual(Deferrable.IMMEDIATE, \"IMMEDIATE\")\n", "def test_validate_ordered_expressions(self):\n    constraint = models.UniqueConstraint(\n        Lower(\"name\").desc(), F(\"color\").desc(nulls_last=True), name=\"name_color_uniq\"\n    )\n    msg = \"Constraint \u201cname_color_uniq\u201d is violated.\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        constraint.validate(\n            UniqueConstraintProduct,\n            UniqueConstraintProduct(name=self.p1.name.upper(), color=self.p1.color),\n        )\n    constraint.validate(\n        UniqueConstraintProduct,\n        UniqueConstraintProduct(name=\"another-name\", color=\"another-color\"),\n    )\n    # Existing instances have their existing row excluded.\n    constraint.validate(UniqueConstraintProduct, self.p1)\n    # Unique field is excluded.\n    constraint.validate(\n        UniqueConstraintProduct,\n        UniqueConstraintProduct(name=self.p1.name.upper(), color=self.p1.color),\n        exclude={\"name\"},\n    )\n", "    def test_base_constraint_clone(self):\n        constraint = BaseConstraint(name=\"base_name\")\n        cloned_constraint = constraint.clone()\n        self.assertEqual(constraint.name, cloned_constraint.name)\n        self.assertEqual(constraint.violation_error_message, cloned_constraint.violation_error_message)\n        self.assertIsNot(constraint, cloned_constraint)\n", "def test_unique_constraint_create_sql(self):\n    fields = [\"foo\", \"bar\"]\n    name = \"unique_fields\"\n    constraint = models.UniqueConstraint(fields=fields, name=name)\n    create_sql = constraint.create_sql(Product, self connections[DEFAULT_DB_ALIAS])\n    self.assertIsInstance(create_sql, str)\n    self.assertIn(\"ALTER TABLE\", create_sql)\n    self.assertIn(name, create_sql)\n    self.assertIn(\"foo\", create_sql)\n    self.assertIn(\"bar\", create_sql)\n\n    constraint = models.UniqueConstraint(Lower(\"name\"), name=\"name_lower_uniq\")\n    create_sql = constraint.create_sql(Product, self.connections[DEFAULT_DB_ALIAS])\n    self.assertIsInstance(create_sql, str)\n    self.assertIn(\"ALTER TABLE\", create_sql)\n    self.assertIn(\"name_lower_uniq\", create_sql)\n    self.assertIn(\"LOWER\", create_sql)\n    self.assertIn(\"name\", create_sql)\n", "    def test_create_covering_index(self):\n        # Ensure that a covering index is created on the included fields.\n        UniqueConstraintInclude.objects.create(name=\"p1\", color=\"red\")\n        index_name = \"constraints_uniqueconstraintincl_name_color_covering\"\n        with connection.cursor() as cursor:\n            indexes = connection.introspection.get_indexes(cursor, UniqueConstraintInclude._meta.db_table)\n            for index in indexes:\n                if index[\"name\"] == index_name:\n                    self.assertEqual(index[\"columns\"], [\"name\"])\n                    self.assertEqual(index[\"covering\"], [\"color\"])\n                    break\n            else:\n                self.fail(f\"Index {index_name} not found.\")\n"], "sample_190": ["def test_lookup_int_as_float(self):\n    # Float value can be queried using integer\n    self.assertQuerysetEqual(\n        Article.objects.filter(id__iexact=float(self.a1.id)),\n        ['<Article: Article 1>']\n    )\n", "def test_year_lookups(self):\n    # Test year lookups on both date and datetime fields.\n    self.assertEqual(Article.objects.filter(pub_date__year=2005).count(), 5)\n    self.assertEqual(Article.objects.filter(pub_date__year__gt=2005).count(), 2)\n    self.assertEqual(Article.objects.filter(pub_date__year__lt=2005).count(), 0)\n    self.assertEqual(Article.objects.filter(pub_date__year__gte=2005).count(), 7)\n    self.assertEqual(Article.objects.filter(pub_date__year__lte=2005).count(), 5)\n    self.assertEqual(Article.objects.filter(pub_date__year__range=(2004, 2006)).count(), 7)\n", "def test_exact_sliced_queryset_non_queryset_rhs(self):\n    self.assertCountEqual(\n        Article.objects.filter(author=Author.objects.all()[:1]),\n        [self.a1, self.a2, self.a3, self.a4]\n    )\n\n    with self.assertRaises(ValueError):\n        Article.objects.filter(author=self.a1)\n", "def test_uuid_text_mixin(self):\n    # Test UUIDTextMixin on PostgreSQL\n    if connection.vendor != 'postgresql':\n        self.skipTest('Only PostgreSQL supports native UUID fields.')\n\n    class UUIDModel(models.Model):\n        uuid = models.UUIDField()\n\n        class Meta:\n            app_label = 'lookup'\n\n    UUIDModel.objects.create(uuid='01234567-89ab-cdef-0123-456789abcdef')\n\n    # With native UUID field\n    self.assertSequenceEqual(\n        UUIDModel.objects.filter(uuid__startswith='01234567').values_list('uuid', flat=True),\n        ['01234567-89ab-cdef-0123-456789abcdef']\n    )\n\n    # Without native UUID field\n    UUIDModel.objects.create(uuid='01234567-89ab-cdef-0123-456789abcde')\n    self.assertSequenceEqual(\n        UUIDModel.objects.filter(uuid__startswith='01234567-89ab-cdef-0123-456789abcde').values_list('uuid', flat=True),\n        ['01234567-89ab-cdef-0123-456789abcde']\n    )\n\n    # With contains and icontains\n    self.assertSequenceEqual(\n        UUIDModel.objects.filter(uuid__contains='89ab').values_list('uuid', flat=True),\n        ['01234567-89ab-cdef-0123-456789abcdef', '01234567-89ab-cdef-0123-456789abcde']\n    )\n    self.assertSequenceEqual(\n        UUIDModel.objects.filter(uuid__icontains='89AB').values_list('uuid', flat=True),\n        ['01234567-89ab-cdef-0123-456789abcdef', '01234567-89ab-cdef-0123-456789abcde']\n    )\n\n    # With startswith and istartswith\n    self.assertSequenceEqual(\n        UUIDModel.objects.filter(uuid__startswith='01234567').values_list('uuid', flat=True),\n        ['01234567-89ab-cdef-0123-456789abcdef', '01234567-89ab-cdef-0123-456789abcde']\n    )\n    self.assertSequenceEqual(\n        UUIDModel.objects.filter(uuid__istartswith='01234567').values_list('uuid', flat=True),\n        ['01234567-89ab-cdef-0123-456789abcdef', '01234567-89ab-cdef-0123-456789abcde']\n   ", "def test_postgres_operator_lookup(self):\n    # Ensure that PostgreSQL-specific lookups can be used\n    from django.db.models import Q\n\n    # Create a test object\n    a = Author.objects.create(name='Test Author', alias='test')\n\n    # Test the PostgresOperatorLookup\n    with self.assertNumQueries(1):\n        self.assertEqual(Author.objects.filter(Q(name__contains='Test')), [a])\n\n    # Test a non-existent lookup\n    msg = \"Unsupported lookup 'startswith' for CharField or join on the field not permitted.\"\n    with self.assertRaisesMessage(FieldError, msg):\n        Author.objects.filter(Q(name__startswith='Test'))\n"], "sample_785": ["def test_time_series_split_with_high_n_splits():\n    X = np.zeros(5)\n    assert_raises(ValueError, next, TimeSeriesSplit(n_splits=6).split(X))\n", "def test_build_repr_repeated_kfold():\n    # Check that the repr works for RepeatedKFold and RepeatedStratifiedKFold\n    rkf = RepeatedKFold(n_splits=5, n_repeats=3, random_state=42)\n    rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n\n    assert_equal(repr(rkf),\n                 (\"RepeatedKFold(n_splits=5, n_repeats=3, \"\n                  \"random_state=42)\"))\n    assert_equal(repr(rskf),\n                 (\"RepeatedStratifiedKFold(n_splits=5, n_repeats=3, \"\n                  \"random_state=42)\"))\n", "def test_leave_p_groups_out_max_groups():\n    # Ensure LeavePGroupsOut does not use more than `n_groups` unique groups\n    groups = np.array([0, 1, 2, 3, 4])\n    X = np.ones(len(groups))\n    lpo = LeavePGroupsOut(n_groups=2)\n\n    unique_groups_used = set()\n    for train, test in lpo.split(X, y=None, groups=groups):\n        test_groups = groups[test]\n        unique_test_groups = np.unique(test_groups)\n        assert len(unique_test_groups) <= lpo.n_groups\n        unique_groups_used.update(unique_test_groups)\n\n    # Ensure all unique groups are used\n    assert len(unique_groups_used) == len(np.unique(groups))\n", "def test_stratified_kfold_extreme_class_weights():\n    # Test that StratifiedKFold does not crash on unbalanced data.\n    # See https://github.com/scikit-learn/scikit-learn/issues/17589\n    rng = np.random.RandomState(42)\n    X = np.arange(1000).reshape(-1, 1)\n    y = rng.randint(2, size=1000)\n    y[:10] = 1\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=rng)\n    for train, test in cv.split(X, y):\n        pass\n", "def test_stratified_shuffle_split_non_integer_classes():\n    X = np.ones(10)\n    y = np.array([0, 1, 1, 1, 2, 2, 2, 3, 3, 3])\n    sss = StratifiedShuffleSplit(test_size=0.5, random_state=42)\n    train, test = next(sss.split(X, y))\n\n    # Check that classes in the training set are non-integer\n    train_classes = y[train].tolist()\n    assert '0.0' in train_classes\n    assert '1.0' in train_classes\n    assert '2.0' in train_classes\n\n    # Check that classes in the test set are non-integer\n    test_classes = y[test].tolist()\n    assert '0.0' in test_classes\n    assert '1.0' in test_classes\n    assert '2.0' in test_classes\n\n    # Check that classes in the training set are balanced\n    assert train_classes.count('0.0') == train_classes.count('1.0')\n    assert train_classes.count('1.0') == train_classes.count('2.0')\n\n    # Check that classes in the test set are balanced\n    assert test_classes.count('0.0') == test_classes.count('1.0')\n    assert test_classes.count('1.0') == test_classes.count('2.0')\n"], "sample_208": ["def test_alter_field_to_fk(self):\n    \"\"\"\n    #25197 - AlterField from CharField to ForeignKey.\n    \"\"\"\n    before = [\n        ModelState(\"app\", \"Foo\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"field\", models.CharField(max_length=255)),\n        ]),\n        ModelState(\"app\", \"Bar\", [\n            (\"id\", models.AutoField(primary_key=True)),\n        ]),\n    ]\n    after = [\n        ModelState(\"app\", \"Foo\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"field\", models.ForeignKey(\"app.Bar\", models.CASCADE)),\n        ]),\n        ModelState(\"app\", \"Bar\", [\n            (\"id\", models.AutoField(primary_key=True)),\n        ]),\n    ]\n    changes = self.get_changes(before, after)\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['AlterField'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, model_name='foo', name='field')\n    fk_field = changes['app'][0].operations[0].field\n    to_model = '%s.%s' % (\n        fk_field.remote_field.model._meta.app_label,\n        fk_field.remote_field.model._meta.object_name,\n    )\n    self.assertEqual(to_model, 'app.Bar')\n", "def test_alter_unique_together_indexes(self):\n    \"\"\"\n    Altering unique_together and indexes to add or remove the same field\n    at the same time works correctly.\n    \"\"\"\n    before = ModelState(\"app\", \"model\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"field1\", models.IntegerField()),\n        (\"field2\", models.IntegerField()),\n    ], {\n        \"unique_together\": {(\"field1\",)},\n        \"indexes\": [models.Index(fields=[\"field1\"])],\n    })\n    after = ModelState(\"app\", \"model\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"field1\", models.IntegerField()),\n        (\"field2\", models.IntegerField()),\n    ], {\n        \"unique_together\": {(\"field1\", \"field2\")},\n        \"indexes\": [models.Index(fields=[\"field1\", \"field2\"])],\n    })\n    changes = self.get_changes([before], [after])\n    # Right number of migrations?\n    self.assertEqual(len(changes), 1)\n    # Right number of actions?\n    migration = changes['app'][0]\n    self.assertEqual(len(migration.operations), 2)\n    # Right actions order?\n    self.assertOperationTypes(changes, 'app', 0, ['AlterUniqueTogether', 'AddIndex'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name=\"model\", unique_together={(\"field1\", \"field2\")})\n    self.assertOperationAttributes(changes, 'app', 0, 1, model_name='model', index=models.Index(fields=[\"field1\", \"field2\"]))\n", "def test_create_model_with_custom_primary_key(self):\n    \"\"\"\n    Test that the primary key field is created when the model has a custom primary key.\n    \"\"\"\n    changes = self.get_changes([], [self.aardvark_pk_fk_author])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"CreateModel\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"Aardvark\")\n    # Right fields?\n    self.assertEqual(len(changes['testapp'][0].operations[0].fields), 1)\n    self.assertEqual(changes['testapp'][0].operations[0].fields[0][0], \"pk_field\")\n    self.assertIsInstance(changes['testapp'][0].operations[0].fields[0][1], models.IntegerField)\n    self.assertTrue(changes['testapp'][0].operations[0].fields[0][1].primary_key)\n", "def test_rename_model_with_managed_change(self):\n    \"\"\"\n    #25787 - Renaming a model and changing its \"managed\" option in the same\n    change results in the correct migration.\n    \"\"\"\n    before = [self.author_unmanaged]\n    after = [self.author_renamed_with_db_table_options]\n    changes = self.get_changes(before, after, MigrationQuestioner({\"ask_rename_model\": True}))\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"RenameModel\", \"AlterModelOptions\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name=\"Author\", new_name=\"NewAuthor\")\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, name=\"newauthor\", options={})\n", "def test_rename_foreign_key_field_with_swappable_setting(self):\n    # FKs to swappable models are handled in #23322\n    with isolate_lru_cache(apps.get_swappable_settings_name):\n        before = [\n            ModelState('app', 'A', [\n                ('id', models.AutoField(primary_key=True)),\n                ('b', models.ForeignKey(settings.AUTH_USER_MODEL, models.CASCADE)),\n            ])\n        ]\n        after = [\n            ModelState('app', 'A', [\n                ('id', models.AutoField(primary_key=True)),\n                ('renamed_b', models.ForeignKey(settings.AUTH_USER_MODEL, models.CASCADE)),\n            ])\n        ]\n        changes = self.get_changes(before, after, MigrationQuestioner({\"ask_rename\": True}))\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, [\"RenameField\"])\n        self.assertOperationAttributes(changes, 'app', 0, 0, model_name='a', old_name='b', new_name='renamed_b')\n"], "sample_311": ["    def test_each_context(self):\n        admin_site = AdminSite()\n        context = admin_site.each_context(self.client.request)\n        self.assertIn('has_permission', context)\n        self.assertIn('site_title', context)\n        self.assertIn('site_header', context)\n        self.assertIn('site_url', context)\n        self.assertIn('available_apps', context)\n        self.assertIn('is_popup', context)\n        self.assertIn('is_nav_sidebar_enabled', context)\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.s1 = Section.objects.create(name='Test section')\n        cls.a1 = Article.objects.create(\n            content='<p>Middle content</p>', date=datetime.datetime(2008, 3, 18, 11, 54, 58), section=cls.s1\n        )\n        cls.a2 = Article.objects.create(\n            content='<p>Oldest content</p>', date=datetime.datetime(2000, 3, 18, 11, 54, 58), section=cls.s1\n        )\n        cls.a3 = Article.objects.create(\n            content='<p>Newest content</p>', date=datetime.datetime(2009, 3, 18, 11, 54, 58), section=cls.s1\n        )\n        cls.p1 = PrePopulatedPost.objects.create(title='A Long Title', published=True, slug='a-long-title')\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.user = User.objects.create_user(username='user', password='secret', is_staff=True)\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.s1 = Section.objects.create(name='Test section')\n        cls.a1 = Article.objects.create(\n            content='<p>Middle content</p>', date=datetime.datetime(2008, 3, 18, 11, 54, 58), section=cls.s1\n        )\n        cls.a2 = Article.objects.create(\n            content='<p>Oldest content</p>', date=datetime.datetime(2000, 3, 18, 11, 54, 58), section=cls.s1\n        )\n        cls.a3 = Article.objects.create(\n            content='<p>Newest content</p>', date=datetime.datetime(2009, 3, 18, 11, 54, 58), section=cls.s1\n        )\n        cls.p1 = PrePopulatedPost.objects.create(title='A Long Title', published=True, slug='a-long-title')\n", "    def test_app_index_url_reverses_correctly(self):\n        self.assertEqual(reverse('admin:app_list', args=('auth',)), '/test_admin/admin/auth/')\n"], "sample_1132": ["def test_generate_oriented_forest_edge_cases():\n    assert list(generate_oriented_forest(0)) == []\n    assert list(generate_oriented_forest(1)) == [[0]]\n", "def test_signed_permutations():\n    assert len(list(signed_permutations([1, 2]))) == 8\n    assert len(list(signed_permutations([1, 2, 3]))) == 48\n    assert list(signed_permutations([])) == [()]\n    assert list(signed_permutations([0])) == [(0,)]\n    assert len(list(signed_permutations([1, 1, 2]))) == 24\n    assert list(signed_permutations([0, 0])) == [(0, 0), (0, 0)]\n", "def test_least_rotation():\n    # Check if the least rotation correctly finds the lexicographically minimal\n    # string/list/tuple, etc.\n    assert least_rotation([3, 1, 5, 1, 2]) == 3\n    assert rotate_left([3, 1, 5, 1, 2], 3) == [1, 2, 3, 1, 5]\n    assert least_rotation([1, 2, 3, 4]) == 0\n    assert least_rotation(['c', 'b', 'a']) == 2\n    assert least_rotation((1, 1, 1, 1)) == 0\n", "def test_permute_signs():\n    assert list(permute_signs((0, 1, 2))) == [(0, 1, 2), (0, -1, 2), (0, 1, -2), (0, -1, -2)]\n    assert list(permute_signs((1, 2, 3))) == [(1, 2, 3), (-1, 2, 3), (1, -2, 3),\n                                             (1, 2, -3), (-1, -2, 3), (-1, 2, -3),\n                                             (-1, -2, -3), (1, -2, -3), (1, -2, 3),\n                                             (-1, 2, -3), (-1, 2, 3), (1, 2, -3),\n                                             (1, -2, -3), (-1, -2, -3), (1, 2, 3),\n                                             (-1, -2, 3), (1, 2, -3), (-1, -2, -3)]\n    assert list(permute_signs([0, 1, 2])) == [(0, 1, 2), (0, -1, 2), (0, 1, -2), (0, -1, -2)]\n    assert list(permute_signs((1, 0, 0))) == [(1, 0, 0), (-1, 0, 0)]\n    assert list(permute_signs((0, 0, 0))) == [(0, 0, 0)]\n    assert list(permute_signs(())) == [()]\n    assert list(permute_signs((1, 2, 3, 4, 5))) == [\n        (1, 2, 3, 4, 5), (-1, 2, 3, 4, 5), (1, -2, 3, 4, 5), (1, 2, -3, 4, 5),\n        (1, 2, 3, -4, 5), (1, 2, 3, 4, -5), (-1, -2, 3, 4, 5), (-", "def test_minlex():\n    assert minlex([3, 2, 1], small=1) == (1, 2, 3)\n    assert minlex((3, 2, 1), small=1) == (1, 2, 3)\n    assert minlex([3, 2, 1], small=0) == (3, 2, 1)\n    assert minlex([3, 2, 1], small=4) == (1, 2, 3)\n    assert minlex([1, 2, 3]) == (1, 2, 3)\n    assert minlex([3, 2, 1], directed=False) == (1, 2, 3)\n    assert minlex((3, 2, 1), directed=False) == (1, 2, 3)\n    assert minlex([1, 2, 3], directed=False) == (1, 2, 3)\n    assert minlex((1, 2, 3), directed=False) == (1, 2, 3)\n    assert minlex('cbab') == 'abbc'\n    assert minlex('cbab', directed=False) == 'abbc'\n    raises(ValueError, lambda: minlex(1))\n    raises(ValueError, lambda: minlex(1, directed=False))\n    raises(ValueError, lambda: minlex(1, small=2))\n    raises(ValueError, lambda: minlex(1, directed=False, small=2))\n"], "sample_612": ["def test_groupby_list_input() -> None:\n    array = xr.DataArray(\n        [1, 2, 3, 4, 5, 6],\n        [(\"x\", [1, 1, 1, 2, 2, 2])],\n        name=\"foo\",\n    )\n    expected = xr.DataArray(\n        [2, 5], coords={\"x\": [1, 2]}, dims=\"x\", name=\"foo\"\n    )\n    actual = array.groupby(array.coords[\"x\"].values).mean()\n    assert_identical(expected, actual)\n\n    expected = xr.Dataset({\"foo\": (\"x\", [2, 5])}, coords={\"x\": [1, 2]})\n    actual = array.to_dataset().groupby(array.coords[\"x\"].values).mean()\n    assert_identical(expected, actual)\n", "def test_groupby_map_with_func_kwargs() -> None:\n        return x + foo\n\n    array = xr.DataArray([1, 1, 1], [(\"x\", [1, 2, 3])])\n    expected = xr.DataArray([2, 3, 4], [(\"x\", [1, 2, 3])])\n    actual = array.groupby(\"x\").map(func, foo=1)\n    assert_identical(expected, actual)\n\n    actual = array.groupby(\"x\").map(func, bar=\"qux\")\n    expected = xr.DataArray([1, 2, 3], [(\"x\", [1, 2, 3])])\n    assert_identical(expected, actual)\n\n    with pytest.raises(TypeError, match=r\"func got an unexpected keyword argument\"):\n        array.groupby(\"x\").map(func, baz=1)\n", "def test_da_groupby_quantile_interpolation_in_groupby():\n    # GH: 6378\n    array = DataArray(\n        data=[1, 2, 3, 4], coords={\"x\": [1, 1, 2, 2]}, dims=\"x\"\n    )\n\n    actual = array.groupby(\"x\").map(lambda x: x.quantile(0.5, method=\"linear\"))\n    expected = array.groupby(\"x\").quantile(0.5, method=\"linear\")\n\n    assert_identical(actual, expected)\n", "def test_groupby_overwrite_dims_with_ensure_1d() -> None:\n    array = DataArray(\n        np.arange(6).reshape(2, 3),\n        dims=(\"a\", \"b\"),\n        coords={\"a\": (\"a\", [1, 2]), \"b\": (\"b\", [3, 3, 4])},\n    )\n    array[\"b\"] = np.array([3, 3, 4])\n\n    expected = DataArray(\n        np.arange(6).reshape(2, 3),\n        dims=(\"a\", \"b\"),\n        coords={\"a\": (\"a\", [1, 2]), \"b\": (\"b\", [3, 3, 4])},\n    )\n\n    actual = array.groupby(\"b\").map(lambda x: x)\n\n    assert_identical(expected, actual)\n", "def test_groupby_restore_coord_dims_edge_cases(shortcut):\n    # Ensure restore_coord_dims works with scalar coordinates and multiple group dimensions\n    array = xr.DataArray(\n        np.random.randn(2, 3, 4, 5),\n        dims=[\"x\", \"y\", \"z\", \"t\"],\n        coords={\n            \"a\": (\"x\", [1, 2]),\n            \"b\": (\"y\", [3, 4, 5]),\n            \"c\": (\"x\", [6, 7]),\n        },\n    )\n    array[\"t\"] = 0\n\n    result = array.groupby([\"a\", \"b\"]).map(\n        lambda x: x.sum(dim=[\"x\", \"y\", \"z\"]), shortcut=shortcut, restore_coord_dims=True\n    )\n\n    expected_dims = [\"a\", \"b\", \"t\"]\n    expected_coords = {\"c\": (\"a\", [6, 7])}\n\n    assert result.dims == expected_dims\n    assert result.coords.keys() == expected_coords.keys()\n    assert_allclose(result.coords[\"c\"].values, expected_coords[\"c\"][1])\n\n    # Ensure restore_coord_dims works with non-coordinate DataArray variables\n    array = xr.DataArray(\n        np.random.randn(2, 3, 4, 5),\n        dims=[\"x\", \"y\", \"z\", \"t\"],\n        coords={\"a\": (\"x\", [1, 2])},\n    )\n    array[\"b\"] = ((\"y\", \"z\"), np.random.randn(3, 4))\n\n    result = array.groupby(\"a\").map(\n        lambda x: x.sum(dim=[\"x\", \"y\", \"z\"]), shortcut=shortcut, restore_coord_dims=True\n    )\n\n    expected_dims = [\"a\", \"t\"]\n    expected_coords = {\"b\": ((\"a\", \"y\", \"z\"), np.random.randn(2, 3, 4))}\n\n    assert result.dims == expected_dims\n    assert result.coords.keys() == expected_coords.keys()\n    assert_allclose(result.coords[\"b\"].values, expected_coords[\"b\"][1])\n"], "sample_995": ["def test_Float_as_coeff_Mul():\n    # issue 8855\n    assert Float(1.2, 3).as_coeff_Mul()[0].evalf() == 1.2\n", "def test_rational_bug_15740():\n    assert (1 + 1/Rational(1, 2))**3 == 27/8\n", "def test_issue_11737():\n    x = Symbol('x')\n    a = Rational(1, 2)\n    assert a.is_comparable == True\n    assert a.is_comparable is not False\n    assert a.is_finite is True\n    assert a.is_finite is not False\n    assert a.is_number is True\n    assert a.is_number is not False\n    assert a.is_integer is False\n    assert a.is_integer is not True\n    assert a.is_rational is True\n    assert a.is_rational is not False\n    assert a.is_real is True\n    assert a.is_real is not False\n\n    assert x.is_comparable == None\n    assert x.is_comparable is not True\n    assert x.is_comparable is not False\n    assert x.is_finite == None\n    assert x.is_finite is not True\n    assert x.is_finite is not False\n    assert x.is_number == None\n    assert x.is_number is not True\n    assert x.is_number is not False\n    assert x.is_integer == None\n    assert x.is_integer is not True\n    assert x.is_integer is not False\n    assert x.is_rational == None\n    assert x.is_rational is not True\n    assert x.is_rational is not False\n    assert x.is_real == None\n    assert x.is_real is not True\n    assert x.is_real is not False\n", "def test_literal_float():\n    assert Rational('1e2').limit_denominator() == 100\n    assert Rational('1e-2').limit_denominator() == Rational(1,100)\n    assert Rational('1e2.3').limit_denominator() == 1230\n    assert Rational('1e-2.3').limit_denominator() == Rational(1, 1030)\n    assert Rational('1e+2').limit_denominator() == 100\n    assert Rational('1e-2.3e+3').limit_denominator() == 123000\n    assert Rational('1e-2.3e-3').limit_denominator() == Rational(1, 1030000)\n", "def test_issue_6332():\n    from sympy import symbols\n    x = symbols('x')\n    assert ((-1)**x).as_base_exp() == (-1, x)\n    assert ((-1)**(-x)).as_base_exp() == (-1, -x)\n    assert ((-1)**(x + 1)).as_base_exp() == (-1, x + 1)\n    assert ((-1)**(-x - 1)).as_base_exp() == (-1, -x - 1)\n    assert ((-1)**(x/2)).as_base_exp() == ((-1)**(1/2), x)\n    assert ((-1)**(-x/2)).as_base_exp() == ((-1)**(1/2), -x)\n    assert ((-1)**(x/3)).as_base_exp() == ((-1)**(1/3), x)\n    assert ((-1)**(-x/3)).as_base_exp() == ((-1)**(1/3), -x)\n    assert ((-1)**(x**2)).as_base_exp() == (-1, x**2)\n    assert ((-1)**(-x**2)).as_base_exp() == (-1, -x**2)\n    assert ((-1)**(x**3)).as_base_exp() == (-1, x**3)\n    assert ((-1)**(-x**3)).as_base_exp() == (-1, -x**3)\n    assert ((-1)**(x/3 + 1)).as_base_exp() == ((-1)**(1/3), x + 3)\n    assert ((-1)**(-x/3 - 1)).as_base_exp() == ((-1)**(1/3), -x - 3)\n"], "sample_803": ["def test_roc_auc_score_pos_label_errors():\n    # Raise an error when pos_label is not in binary y_true\n    y_true = np.array([0, 1])\n    y_pred = np.array([0.5, 0.5])\n    error_message = (\"pos_label=2 is invalid. Set it to a label in y_true.\")\n    assert_raise_message(ValueError, error_message, roc_auc_score,\n                         y_true, y_pred, pos_label=2)\n    # Raise an error for multilabel-indicator y_true with\n    # pos_label other than 1\n    y_true = np.array([[1, 0], [0, 1], [0, 1], [1, 0]])\n    y_pred = np.array([[0.9, 0.1], [0.1, 0.9], [0.8, 0.2], [0.2, 0.8]])\n    error_message = (\"Parameter pos_label is fixed to 1 for multilabel\"\n                     \"-indicator y_true. Do not set pos_label or set \"\n                     \"pos_label to 1.\")\n    assert_raise_message(ValueError, error_message, roc_auc_score,\n                         y_true, y_pred, pos_label=0)\n", "def test_roc_auc_score_tie_handling():\n    # Test tie handling in roc_auc_score\n    y_true = np.array([0, 1, 1, 0])\n    y_score = np.array([0.5, 0.5, 0.5, 0.5])\n    assert_almost_equal(roc_auc_score(y_true, y_score), 0.5)\n\n    y_true = np.array([0, 1, 1, 0])\n    y_score = np.array([0.5, 0.5, 0.500001, 0.5])\n    assert_almost_equal(roc_auc_score(y_true, y_score), 0.5)\n\n    y_true = np.array([0, 1, 1, 0])\n    y_score = np.array([0.5, 0.500001, 0.5, 0.5])\n    assert_almost_equal(roc_auc_score(y_true, y_score), 0.5)\n\n    y_true = np.array([0, 1, 1, 0])\n    y_score = np.array([0.5, 0.5, 0.5, 0.500001])\n    assert_almost_equal(roc_auc_score(y_true, y_score), 0.5)\n", "def test_roc_auc_score_multilabel_multioutput():\n    # Test roc_auc_score for multilabel and multioutput problems\n    y_true = np.array([[1, 0, 1], [0, 1, 1]])\n    y_pred = np.array([[0.8, 0.4, 0.9], [0.1, 0.8, 0.9]])\n\n    # Test 'macro' averaging\n    assert_almost_equal(roc_auc_score(y_true, y_pred, average='macro'), 0.75)\n\n    # Test 'weighted' averaging\n    assert_almost_equal(roc_auc_score(y_true, y_pred, average='weighted'), 0.75)\n\n    # Test 'samples' averaging\n    assert_almost_equal(roc_auc_score(y_true, y_pred, average='samples'), 0.75)\n\n    # Test 'micro' averaging\n    assert_almost_equal(roc_auc_score(y_true, y_pred, average='micro'), 0.75)\n\n    # Test no averaging\n    assert_array_almost_equal(roc_auc_score(y_true, y_pred, average=None),\n                              np.array([1., 0.5, 1.]))\n", "def test_roc_auc_score_multiclass_one_vs_rest():\n    # Test ROC AUC for multiclass classification with one-vs-rest\n    y_true = np.array([0, 1, 2] * 20)\n    y_pred = np.random.RandomState(42).rand(60, 3)\n    # Test with sample weights\n    sample_weight = np.random.RandomState(42).rand(60)\n    assert_almost_equal(\n        roc_auc_score(y_true, y_pred, average=\"macro\", sample_weight=sample_weight),\n        0.5)\n    assert_almost_equal(\n        roc_auc_score(y_true, y_pred, average=\"weighted\", sample_weight=sample_weight),\n        0.5)\n    assert_almost_equal(\n        roc_auc_score(y_true, y_pred, average=\"samples\", sample_weight=sample_weight),\n        0.5)\n    assert_almost_equal(\n        roc_auc_score(y_true, y_pred, sample_weight=sample_weight),\n        0.5)\n    # Test without sample weights\n    assert_almost_equal(roc_auc_score(y_true, y_pred, average=\"macro\"), 0.5)\n    assert_almost_equal(roc_auc_score(y_true, y_pred, average=\"weighted\"), 0.5)\n    assert_almost_equal(roc_auc_score(y_true, y_pred, average=\"samples\"), 0.5)\n    assert_almost_equal(roc_auc_score(y_true, y_pred), 0.5)\n", "def test_roc_auc_score_max_fpr():\n    # Test that the max_fpr parameter in roc_auc_score is correctly\n    # applied for the 'macro' and 'weighted' average methods\n    y_true = np.array([0, 0, 1, 1])\n    y_pred = np.array([0.1, 0.4, 0.35, 0.8])\n\n    # Make a multi-label version of y_true and y_pred\n    y_true_multi = np.array([[0, 1], [1, 0], [0, 1], [1, 0]])\n    y_pred_multi = np.array([[0.1, 0.9], [0.9, 0.1], [0.2, 0.8], [0.8, 0.2]])\n\n    # For binary classification, the max_fpr parameter has no effect\n    assert_almost_equal(roc_auc_score(y_true, y_pred, max_fpr=0.5),\n                        roc_auc_score(y_true, y_pred))\n\n    # Test that the 'macro' average method is correctly averaged\n    # for different max_fpr values\n    for max_fpr in [0.3, 0.5, 0.7, 1]:\n        auc_macro = roc_auc_score(y_true_multi, y_pred_multi, average='macro',\n                                 max_fpr=max_fpr)\n        auc_weighted = roc_auc_score(y_true_multi, y_pred_multi,\n                                    average='weighted', max_fpr=max_fpr)\n        auc_samples = roc_auc_score(y_true_multi, y_pred_multi,\n                                   average='samples', max_fpr=max_fpr)\n\n        assert_almost_equal(auc_macro, auc_samples)\n        assert_almost_equal(auc_macro, auc_weighted)\n\n    # Test that the 'weighted' average method is correctly weighted\n    # for different max_fpr values\n    for max_fpr in [0.3, 0.5, 0.7, 1]:\n        auc_macro = roc_auc_score(y_true_multi, y_pred_multi, average='macro',\n                                 max_fpr=max_fpr)\n        auc_weighted = roc_auc_score(y_true_multi, y_pred_multi,\n                                    average='weighted', max_fpr=max_fpr)\n\n        # The 'weighted' average method should weight the AUCs by support\n        assert_not_equal(auc_macro, auc_weighted)\n"], "sample_579": ["def test_clustermap_heatmap_annotation_with_mask(self):\n\n    df = pd.DataFrame(data={'a': [1, 1, 1],\n                            'b': [2, np.nan, 2],\n                            'c': [3, 3, np.nan]})\n    mask = np.isnan(df.values)\n    df_masked = np.ma.masked_where(mask, df)\n    g = mat.clustermap(df, annot=True, fmt='.1f', mask=mask)\n    assert len(df_masked.compressed()) == len(g.ax_heatmap.texts)\n    for val, text in zip(df_masked.compressed(), g.ax_heatmap.texts):\n        assert f\"{val:.1f}\" == text.get_text()\n", "def test_custom_cmap_with_nans(self):\n\n    kws = self.default_kws.copy()\n    cmap = copy.copy(get_colormap(\"BrBG\"))\n    cmap.set_bad(\"red\")\n    kws[\"cmap\"] = cmap\n    hm = mat._HeatMapper(self.df_norm, **kws)\n\n    npt.assert_array_equal(\n        cmap(np.ma.masked_invalid([np.nan])),\n        hm.cmap(np.ma.masked_invalid([np.nan])))\n\n    kws[\"center\"] = 0.5\n    hm = mat._HeatMapper(self.df_norm, **kws)\n    npt.assert_array_equal(\n        cmap(np.ma.masked_invalid([np.nan])),\n        hm.cmap(np.ma.masked_invalid([np.nan])))\n\n    kws = self.default_kws.copy()\n    cmap = copy.copy(get_colormap(\"BrBG\"))\n    cmap.set_under(\"red\")\n    kws[\"cmap\"] = cmap\n    hm = mat._HeatMapper(self.df_norm, **kws)\n    npt.assert_array_equal(cmap(-np.inf), hm.cmap(-np.inf))\n\n    kws[\"center\"] = .5\n    hm = mat._HeatMapper(self.df_norm, **kws)\n    npt.assert_array_equal(cmap(-np.inf), hm.cmap(-np.inf))\n\n    kws = self.default_kws.copy()\n    cmap = copy.copy(get_colormap(\"BrBG\"))\n    cmap.set_over(\"red\")\n    kws[\"cmap\"] = cmap\n    hm = mat._HeatMapper(self.df_norm, **kws)\n    npt.assert_array_equal(cmap(np.inf), hm.cmap(np.inf))\n\n    kws[\"center\"] = .5\n    hm = mat._HeatMapper(self.df_norm, **kws)\n    npt.assert_array_equal(cmap(np.inf), hm.cmap(np.inf))\n", "def test_mask_boolean_array_input(self):\n    kws = self.default_kws.copy()\n\n    mask = np.random.choice(a=[False, True], size=self.df_norm.shape)\n    kws[\"mask\"] = mask\n\n    g = mat.clustermap(self.df_norm, **kws)\n    npt.assert_array_equal(g.mask.index, g.data2d.index)\n    npt.assert_array_equal(g.mask.columns, g.data2d.columns)\n    npt.assert_array_equal(g.mask, mask)\n", "def test_cbar_pos_out_of_bounds(self):\n\n    kws = self.default_kws.copy()\n    kws[\"cbar_pos\"] = (.2, .1, .4, 1.1)\n\n    with pytest.warns(UserWarning):\n        g = mat.clustermap(self.df_norm, **kws)\n    pos = g.ax_cbar.get_position()\n    assert pos.height < 1\n\n    kws[\"cbar_pos\"] = (.2, .1, 1.1, .3)\n    with pytest.warns(UserWarning):\n        g = mat.clustermap(self.df_norm, **kws)\n    pos = g.ax_cbar.get_position()\n    assert pos.width < 1\n\n    kws[\"cbar_pos\"] = (-.1, .1, .4, .3)\n    with pytest.warns(UserWarning):\n        g = mat.clustermap(self.df_norm, **kws)\n    pos = g.ax_cbar.get_position()\n    assert pos.x0 > 0\n\n    kws[\"cbar_pos\"] = (.2, -.1, .4, .3)\n    with pytest.warns(UserWarning):\n        g = mat.clustermap(self.df_norm, **kws)\n    pos = g.ax_cbar.get_position()\n    assert pos.y0 > 0\n", "    def test_z_score_with_robust_flag(self):\n        df = self.df_norm.copy()\n        df = (df - df.mean()) / df.std()\n        kws = dict(pivot_kws=None, z_score=None, standard_scale=None,\n                   figsize=(10, 10), row_colors=None, col_colors=None,\n                   dendrogram_ratio=.2, colors_ratio=.03,\n                   cbar_pos=(0, .8, .05, .2))\n\n        kws['z_score'] = True\n\n        cg = mat.ClusterGrid(self.df_norm, **kws)\n        pdt.assert_frame_equal(cg.data2d, df)\n"], "sample_328": ["    def setUpTestData(cls):\n        cls.notes = [\n            Note.objects.create(note=str(i), misc=str(i))\n            for i in range(10)\n        ]\n", "    def setUpTestData(cls):\n        cls.note1 = Note.objects.create(note='Test', misc='Misc')\n", "    def test_exists(self):\n        Note.objects.create(note='test-note', misc='test')\n        self.assertTrue(Note.objects.exists())\n        self.assertFalse(Note.objects.filter(note='non-existent').exists())\n", "    def test_union(self):\n        article1 = Article.objects.create(name='article1')\n        article2 = Article.objects.create(name='article2')\n        article3 = Article.objects.create(name='article3')\n\n        qs1 = Article.objects.filter(name='article1')\n        qs2 = Article.objects.filter(name='article2')\n        qs3 = Article.objects.filter(name='article3')\n\n        qs = qs1.union(qs2, qs3)\n\n        self.assertQuerysetEqual(qs, [article1, article2, article3], transform=lambda x: x)\n", "    def test_values_distinct(self):\n        Note.objects.create(note='test1', misc='misc1')\n        Note.objects.create(note='test2', misc='misc2')\n        Note.objects.create(note='test1', misc='misc2')\n        qs = Note.objects.values('note').distinct().order_by('note')\n        self.assertEqual([d['note'] for d in qs], ['test1', 'test2'])\n"], "sample_905": ["def test_signature_from_str_positional_only_args_with_type_annotation():\n    sig = inspect.signature_from_str('(a: int, b=0, /, c: str)')\n    assert list(sig.parameters.keys()) == ['a', 'b', 'c']\n    assert sig.parameters['a'].kind == Parameter.POSITIONAL_ONLY\n    assert sig.parameters['a'].default == Parameter.empty\n    assert sig.parameters['a'].annotation == 'int'\n    assert sig.parameters['b'].kind == Parameter.POSITIONAL_ONLY\n    assert sig.parameters['b'].default == '0'\n    assert sig.parameters['b'].annotation == Parameter.empty\n    assert sig.parameters['c'].kind == Parameter.POSITIONAL_OR_KEYWORD\n    assert sig.parameters['c'].default == Parameter.empty\n    assert sig.parameters['c'].annotation == 'str'\n", "def test_is_singledispatch_function():\n    import functools\n\n    @functools.singledispatch\n        pass\n\n    assert inspect.is_singledispatch_function(foo) is True\n\n        pass\n\n    assert inspect.is_singledispatch_function(bar) is False\n\n    class Baz:\n        @functools.singledispatchmethod\n            pass\n\n    assert inspect.is_singledispatch_function(Baz.baz) is False\n", "def test_isasyncgenfunction():\n    async def async_gen_func():\n        yield\n\n    assert inspect.isasyncgenfunction(async_gen_func) is True\n\n    async def async_gen_func_with_params(param1, param2):\n        yield param1, param2\n\n    assert inspect.isasyncgenfunction(async_gen_func_with_params) is True\n\n        yield\n\n    assert inspect.isasyncgenfunction(sync_gen_func) is False\n\n    class AsyncGenClass:\n        async def __call__(self):\n            yield\n\n    assert inspect.isasyncgenfunction(AsyncGenClass()) is True\n\n    class SyncGenClass:\n            yield\n\n    assert inspect.isasyncgenfunction(SyncGenClass()) is False\n", "def test_is_new_type():\n    if sys.version_info >= (3, 10):\n        NT = typing.NewType('NT', int)\n        assert inspect.isNewType(NT) is True\n        assert inspect.isNewType(int) is False\n    else:\n        NT = typing.NewType('NT', int)\n        assert inspect.isNewType(NT) is True\n        assert inspect.isNewType(int) is False\n\n    with pytest.raises(TypeError):\n        inspect.isNewType(\"string\")\n\n    class CustomType:\n        pass\n\n    assert inspect.isNewType(CustomType) is False\n", "def test_isabstractmethod():\n    class AbstractBase:\n        @abstractmethod\n            pass\n\n    class ConcreteBase:\n            pass\n\n    class InheritedConcreteBase(ConcreteBase):\n        pass\n\n    abstract_base = AbstractBase()\n    concrete_base = ConcreteBase()\n    inherited_concrete_base = InheritedConcreteBase()\n\n    assert inspect.isabstractmethod(AbstractBase.abstract_method) is True\n    assert inspect.isabstractmethod(abstract_base.abstract_method) is False\n    assert inspect.isabstractmethod(ConcreteBase.concrete_method) is False\n    assert inspect.isabstractmethod(concrete_base.concrete_method) is False\n    assert inspect.isabstractmethod(InheritedConcreteBase.concrete_method) is False\n    assert inspect.isabstractmethod(inherited_concrete_base.concrete_method) is False\n"], "sample_56": ["    def test_ordering_random_marker_with_fields(self):\n        class SongAdmin(admin.ModelAdmin):\n            ordering = ['?', 'title']\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'ordering' has the random ordering marker '?', \"\n                \"but contains other fields as well.\",\n                obj=SongAdmin,\n                id='admin.E032',\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "def test_autocomplete_fields_non_foreign_key_or_many_to_many(self):\n    class SongAdmin(admin.ModelAdmin):\n        autocomplete_fields = ('title',)\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'autocomplete_fields[0]' refers to 'title', which is not a foreign key or a many-to-many field.\",\n            obj=SongAdmin,\n            id='admin.E038',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "    def test_list_display_links(self):\n        class SongAdmin(admin.ModelAdmin):\n            list_display = [\"title\"]\n            list_display_links = [\"title\"]\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        self.assertEqual(errors, [])\n\n        class SongAdmin(admin.ModelAdmin):\n            list_display = [\"title\"]\n            list_display_links = [\"title\", \"original_release\"]\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'list_display_links[1]' refers to 'original_release', \"\n                \"which is not defined in 'list_display'.\",\n                obj=SongAdmin,\n                id='admin.E111',\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "def test_check_ordering(self):\n    class SongAdmin(admin.ModelAdmin):\n        ordering = 'nonexistent'\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'ordering[0]' refers to 'nonexistent', which \"\n            \"is not an attribute of 'admin_checks.Song'.\",\n            obj=SongAdmin,\n            id='admin.E033',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_check_raw_id_fields_item_not_field(self):\n    \"\"\"\n    Check an item of `raw_id_fields`, i.e. check that field named\n    `field_name` exists in model `model` and is a ForeignKey or a\n    ManyToManyField. Check for field that is not a model field but\n    exists on the model admin.\n    \"\"\"\n    class RawIdNonFieldAdmin(admin.ModelAdmin):\n        raw_id_fields = ('non_field',)\n\n        return \"Something\"\n\n    RawIdNonFieldAdmin.non_field = non_field\n\n    errors = RawIdNonFieldAdmin(Album, AdminSite()).check()\n    expected = []\n    self.assertEqual(errors, expected)\n"], "sample_838": ["def test_column_transformer_invalid_columns_transform(col):\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    # general invalid\n    ct = ColumnTransformer([('trans', Trans(), col)],\n                           remainder='passthrough')\n    assert_raise_message(ValueError, \"No valid specification\",\n                         ct.transform, X_array)\n", "def test_column_transformer_empty_features():\n    X_array = np.array([[]]).T\n    X_res = np.array([[]]).T\n    ct = ColumnTransformer([('trans1', Trans(), [0])], remainder='drop')\n    assert_array_equal(ct.fit_transform(X_array), X_res)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert ct.transformers_[-1][1] == 'drop'\n    assert_array_equal(ct.transformers_[-1][2], [])\n", "def test_column_transformer_check_is_fitted():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    ct = ColumnTransformer([('trans', Trans(), [0])])\n\n    assert not hasattr(ct, 'transformers_')\n\n    assert_raise_message(NotFittedError, \"This ColumnTransformer instance\",\n                         ct.transform, X_array)\n\n    ct.fit(X_array)\n\n    assert hasattr(ct, 'transformers_')\n\n    # check that check_is_fitted is called on get_feature_names\n    ct = ColumnTransformer([('trans', Trans(), [0])])\n    assert_raise_message(NotFittedError, \"This ColumnTransformer instance\",\n                         ct.get_feature_names)\n", "def test_column_transformer_transformer_weights_update():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    transformer_weights = {'trans1': 1, 'trans2': 1}\n    transformer_weights_update = {'trans1': 2, 'trans2': 3}\n\n    ct = ColumnTransformer([('trans1', Trans(), [0]),\n                            ('trans2', Trans(), [1])],\n                           transformer_weights=transformer_weights)\n    ct.set_params(transformer_weights=transformer_weights_update)\n    assert ct.get_params()['transformer_weights'] == transformer_weights_update\n\n    # test updating a single transformer weight\n    ct.set_params(**{'trans1': 4})\n    assert ct.get_params()['transformer_weights']['trans1'] == 4\n    assert ct.get_params()['transformer_weights']['trans2'] == 3\n\n    # test setting transformer weights to None\n    ct.set_params(transformer_weights=None)\n    assert ct.get_params()['transformer_weights'] is None\n\n    # test setting transformer weights to empty dict\n    ct.set_params(transformer_weights={})\n    assert ct.get_params()['transformer_weights'] == {}\n\n    # test raising error when setting a non-existent transformer weight\n    with pytest.raises(KeyError):\n        ct.set_params(**{'trans3': 5})\n", "def test_column_transformer_validation():\n    # Test that transformers can raise errors and that they are properly\n    # caught and reported with the transformer name.\n    X = np.array([[0, 1], [2, 4]])\n\n    class TransRaise(BaseEstimator):\n            raise ValueError(\"Error in fit\")\n\n            return X\n\n    ct = ColumnTransformer([('trans1', TransRaise(), 0)],\n                           remainder='passthrough')\n    assert_raise_message(ValueError, \"Error in fit\", ct.fit, X)\n\n    class TransRaise2(BaseEstimator):\n            return self\n\n            raise ValueError(\"Error in transform\")\n\n    ct = ColumnTransformer([('trans1', TransRaise2(), 0)],\n                           remainder='passthrough')\n    assert_raise_message(ValueError, \"Error in transform\", ct.fit_transform, X)\n\n    # Check that errors in the transformers are not swallowed\n    class TransRaise3(BaseEstimator):\n            raise ValueError(\"Error in fit\")\n\n            raise ValueError(\"Error in transform\")\n\n    ct = ColumnTransformer([('trans1', TransRaise3(), 0)],\n                           remainder='passthrough')\n    assert_raise_message(ValueError, \"Error in fit\", ct.fit, X)\n"], "sample_886": ["def test__wrap_method_output():\n    \"\"\"Check that _wrap_method_output works as expected.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    class Estimator(_SetOutputMixin):\n            return X\n\n            return np.asarray([f\"X{i}\" for i in range(3)], dtype=object)\n\n    est = Estimator().set_output(transform=\"pandas\")\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n    X_trans = est.transform(X)\n    assert isinstance(X_trans, pd.DataFrame)\n\n    class EstimatorReturnTuple(_SetOutputMixin):\n            return (X, X)\n\n            return np.asarray([f\"X{i}\" for i in range(3)], dtype=object)\n\n    est = EstimatorReturnTuple().set_output(transform=\"pandas\")\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n    X_trans = est.transform(X)\n    assert isinstance(X_trans[0], pd.DataFrame)\n    assert not isinstance(X_trans[1], pd.DataFrame)\n", "def test__wrap_data_with_container_sparse_input():\n    \"\"\"Check _wrap_data_with_container raises an error for sparse input.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    class EstimatorWithSparseOutput(_SetOutputMixin):\n            return csr_matrix(X)\n\n            return np.asarray([f\"X{i}\" for i in range(3)], dtype=object)\n\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n    est = EstimatorWithSparseOutput().set_output(transform=\"pandas\")\n\n    match = \"Pandas output does not support sparse data.\"\n    with pytest.raises(ValueError, match=match):\n        est.transform(X)\n", "def test__wrap_data_with_container():\n    \"\"\"Check _wrap_data_with_container works as expected.\"\"\"\n\n    pd = pytest.importorskip(\"pandas\")\n\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n    est = EstimatorWithSetOutput().fit(X)\n\n    # If output_config is \"default\", then data_to_wrap is not changed\n    config = _get_output_config(\"transform\", est)\n    wrapped_data = _wrap_data_with_container(\"transform\", X, X, est)\n    assert_array_equal(wrapped_data, X)\n    assert not isinstance(wrapped_data, pd.DataFrame)\n\n    # If output_config is \"pandas\", then data_to_wrap is wrapped in a DataFrame\n    est.set_output(transform=\"pandas\")\n    config = _get_output_config(\"transform\", est)\n    wrapped_data = _wrap_data_with_container(\"transform\", X, X, est)\n    assert isinstance(wrapped_data, pd.DataFrame)\n    assert_array_equal(wrapped_data.columns, est.get_feature_names_out())\n\n    # If _auto_wrap_is_configured is False, then data_to_wrap is not changed\n    est = EstimatorNoSetOutputWithTransformNoFeatureNamesOut()\n    wrapped_data = _wrap_data_with_container(\"transform\", X, X, est)\n    assert_array_equal(wrapped_data, X)\n    assert not isinstance(wrapped_data, pd.DataFrame)\n\n    # If data_to_wrap is a tuple, then only the first element is wrapped\n    X_tuple = (X, X)\n    est.set_output(transform=\"pandas\")\n    wrapped_data = _wrap_data_with_container(\"transform\", X_tuple, X_tuple, est)\n    assert isinstance(wrapped_data[0], pd.DataFrame)\n    assert_array_equal(wrapped_data[0].columns, est.get_feature_names_out())\n    assert_array_equal(wrapped_data[1], X)\n", "def test_wrap_method_output():\n    \"\"\"Check that _wrap_method_output correctly wraps an estimator's methods.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    class EstimatorWithWrappedMethod(_SetOutputMixin):\n            return X\n\n            return np.asarray([f\"X{i}\" for i in range(X.shape[1])], dtype=object)\n\n    X = np.asarray([[1, 2, 3], [4, 5, 6]])\n    est = EstimatorWithWrappedMethod()\n\n    # Check that the method is not initially wrapped\n    assert not hasattr(est.transform, \"_sklearn_output_config\")\n\n    # Wrap the method\n    wrapped_transform = _wrap_method_output(est.transform, \"transform\")\n    est.transform = wrapped_transform\n\n    # Check that the method is now wrapped\n    assert hasattr(est.transform, \"_sklearn_output_config\")\n\n    # Check that the wrapped method returns a pandas DataFrame\n    est.set_output(transform=\"pandas\")\n    result = est.transform(X)\n    assert isinstance(result, pd.DataFrame)\n\n    # Check that the original method is called correctly\n    original_transform = est.__class__.__dict__[\"transform\"]\n    assert_array_equal(original_transform(est, X), X)\n\n    # Check that the wrapped method handles tuple outputs correctly\n    class EstimatorWithWrappedMethodTupleOutput(_SetOutputMixin):\n            return (X, X)\n\n            return np.asarray([f\"X{i}\" for i in range(X.shape[1])], dtype=object)\n\n    est_tuple = EstimatorWithWrappedMethodTupleOutput()\n    wrapped_transform_tuple = _wrap_method_output(est_tuple.transform, \"transform\")\n    est_tuple.transform = wrapped_transform_tuple\n    est_tuple.set_output(transform=\"pandas\")\n    result_tuple = est_tuple.transform(X)\n    assert isinstance(result_tuple[0], pd.DataFrame)\n    assert_array_equal(result_tuple[1], X)\n", "def test__wrap_data_with_container():\n    \"\"\"Check that _wrap_data_with_container returns data with correct container.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n    est = EstimatorWithSetOutput().fit(X)\n\n    # Without transform method, return data unchanged\n    class EstimatorNoTransform(_SetOutputMixin):\n        pass\n    est_no_transform = EstimatorNoTransform()\n    X_wrapped = _wrap_data_with_container(\"transform\", X, X, est_no_transform)\n    assert X_wrapped is X\n\n    # Without get_feature_names_out, return data unchanged\n    class EstimatorNoGetFeatureNames(_SetOutputMixin):\n            return X\n    est_no_get_feature_names = EstimatorNoGetFeatureNames()\n    X_wrapped = _wrap_data_with_container(\"transform\", X, X, est_no_get_feature_names)\n    assert X_wrapped is X\n\n    # Without transform_output config, use global config\n    global_config = get_config()[\"transform_output\"]\n    X_wrapped = _wrap_data_with_container(\"transform\", X, X, est)\n    if global_config == \"pandas\":\n        assert isinstance(X_wrapped, pd.DataFrame)\n    else:\n        assert X_wrapped is X\n\n    # With transform_output config, use local config\n    est.set_output(transform=\"pandas\")\n    X_wrapped = _wrap_data_with_container(\"transform\", X, X, est)\n    assert isinstance(X_wrapped, pd.DataFrame)\n\n    # With sparse input, raise ValueError\n    X_csr = csr_matrix(X)\n    match = \"Pandas output does not support sparse data\"\n    with pytest.raises(ValueError, match=match):\n        _wrap_data_with_container(\"transform\", X_csr, X, est)\n\n    # With tuple input, only wrap the first output\n    X_tuple = (X, np.asarray([1, 2]))\n    X_wrapped = _wrap_data_with_container(\"transform\", X_tuple, X, est)\n    assert isinstance(X_wrapped[0], pd.DataFrame)\n    assert X_wrapped[1] is X_tuple[1]\n"], "sample_464": ["def test_non_ascii_filename(self):\n    response = FileResponse(\n        ContentFile(b\"binary content\", name=\"test_file_\u00e9.txt\"),\n        as_attachment=True,\n    )\n    self.assertEqual(\n        response.headers[\"Content-Disposition\"],\n        'attachment; filename=\"test_file_%C3%A9.txt\"',\n    )\n", "def test_delete_cookie(self):\n    response = HttpResponse()\n    response.set_cookie('test_cookie', 'test_value')\n    response.delete_cookie('test_cookie')\n    self.assertFalse(response.cookies.get('test_cookie'))\n    response.delete_cookie('test_cookie', path='/test_path')\n    self.assertFalse(response.cookies.get('test_cookie', path='/test_path'))\n    response.delete_cookie('test_cookie', domain='test_domain')\n    self.assertFalse(response.cookies.get('test_cookie', domain='test_domain'))\n    response.delete_cookie('test_cookie', samesite='Lax')\n    self.assertFalse(response.cookies.get('test_cookie', samesite='Lax'))\n", "def test_set_signed_cookie(self):\n    response = HttpResponse()\n    response.set_signed_cookie(\"key\", \"value\")\n    self.assertIn(\"Set-Cookie\", response[\"Set-Cookie\"])\n    self.assertIn(\"value\", response[\"Set-Cookie\"])\n", "def test_async_iterable(self):\n    async def async_generator():\n        yield b\"binary \"\n        yield b\"content\"\n\n    response = FileResponse(async_generator())\n    with self.assertRaises(TypeError):\n        list(response)\n\n    async def test_async_iterable_list():\n        response = FileResponse(async_generator())\n        result = []\n        async for chunk in response:\n            result.append(chunk)\n        self.assertEqual(result, [b\"binary \", b\"content\"])\n\n    async_to_sync(test_async_iterable_list)()\n", "def test_headers(self):\n    response = FileResponse(io.BytesIO(b\"binary content\"))\n    self.assertEqual(response.headers[\"Content-Type\"], \"application/octet-stream\")\n    response = FileResponse(io.BytesIO(b\"binary content\"), content_type=\"video/webm\")\n    self.assertEqual(response.headers[\"Content-Type\"], \"video/webm\")\n"], "sample_882": ["def test_mlp_hidden_layer_sizes(MLPEstimator):\n    \"\"\"Check that MLP can handle hidden_layer_sizes with more than 2 layers.\"\"\"\n    X = X_digits_binary[:100]\n    y = y_digits_binary[:100]\n\n    mlp = MLPEstimator(hidden_layer_sizes=(50, 20, 10), random_state=0)\n    mlp.fit(X, y)\n\n    assert len(mlp.coefs_) == 3\n    assert len(mlp.intercepts_) == 3\n    assert mlp.n_layers_ == 4\n\n    mlp = MLPEstimator(hidden_layer_sizes=[50, 20, 10], random_state=0)\n    mlp.fit(X, y)\n\n    assert len(mlp.coefs_) == 3\n    assert len(mlp.intercepts_) == 3\n    assert mlp.n_layers_ == 4\n\n    with pytest.raises(ValueError):\n        MLPEstimator(hidden_layer_sizes=0)\n\n    with pytest.raises(ValueError):\n        MLPEstimator(hidden_layer_sizes=[50, 0, 10])\n", "def test_mlp_n_iter_no_change_check(MLPEstimator):\n    # Test that n_iter_no_change is checked when n_iter_no_change is an integer\n    # and a float\n    n_iter_no_change = 5\n    model = MLPEstimator(\n        max_iter=10, random_state=0, early_stopping=False, n_iter_no_change=n_iter_no_change\n    )\n    with pytest.raises(ValueError, match=\"n_iter_no_change must be an integer or np.inf\"):\n        model.set_params(n_iter_no_change=5.0)\n\n    model.set_params(n_iter_no_change=np.inf)\n    with pytest.warns(ConvergenceWarning):\n        model.fit(X_iris, y_iris)\n    assert model.n_iter_ == model.max_iter\n\n    model.set_params(n_iter_no_change=5, max_iter=10)\n    with pytest.warns(ConvergenceWarning):\n        model.fit(X_iris, y_iris)\n    assert model.n_iter_ < model.max_iter\n", "def test_mlp_fit_input_validation(Estimator):\n    \"\"\"Check input validation in the fit method.\"\"\"\n    X = X_digits\n    y = y_digits\n    estimator = Estimator(hidden_layer_sizes=(5, 3), random_state=1)\n\n    # Test invalid input types\n    with pytest.raises(ValueError, match=\"y should be an array-like\"):\n        estimator.fit(X, \"y\")\n\n    with pytest.raises(ValueError, match=\"y should be an array-like\"):\n        estimator.fit(\"X\", y)\n\n    # Test invalid input lengths\n    with pytest.raises(ValueError, match=\"X and y have different lengths\"):\n        estimator.fit(X[:10], y)\n\n    with pytest.raises(ValueError, match=\"X and y have different lengths\"):\n        estimator.fit(X, y[:10])\n\n    # Test invalid input shapes\n    X_invalid_shape = X.reshape(-1, 1, 8, 8)\n    with pytest.raises(ValueError, match=\"X has an invalid shape\"):\n        estimator.fit(X_invalid_shape, y)\n\n    # Test invalid input types for sparse matrices\n    X_sparse = csr_matrix(X)\n    X_sparse_invalid_type = X_sparse.astype(np.int32)\n    with pytest.raises(ValueError, match=\"X has an invalid dtype\"):\n        estimator.fit(X_sparse_invalid_type, y)\n", "def test_mlp_intercepts_shrinkage(MLPEstimator):\n    \"\"\"Test that MLP intercepts are shrunk by the L2 penalty.\n\n    Non-regression test for:\n    https://github.com/scikit-learn/scikit-learn/issues/24697\n    \"\"\"\n    rng = np.random.RandomState(0)\n\n    X = rng.randn(100, 10)\n    if MLPEstimator == MLPClassifier:\n        y = rng.randint(0, 2, size=100)\n    else:\n        y = rng.randn(100)\n\n    # Compare L2 penalty to no penalty\n    mlp_with_penalty = MLPEstimator(\n        hidden_layer_sizes=(10,), alpha=1e-2, solver=\"lbfgs\", random_state=rng\n    )\n    mlp_without_penalty = MLPEstimator(\n        hidden_layer_sizes=(10,), alpha=0, solver=\"lbfgs\", random_state=rng\n    )\n\n    mlp_with_penalty.fit(X, y)\n    mlp_without_penalty.fit(X, y)\n\n    # Check that L2 penalty reduces the magnitude of intercepts\n    for intercepts_with_penalty, intercepts_without_penalty in zip(\n        mlp_with_penalty.intercepts_, mlp_without_penalty.intercepts_\n    ):\n        assert np.all(np.abs(intercepts_with_penalty) < np.abs(intercepts_without_penalty))\n", "def test_mlp_label_binarizer_incompatibility(MLPEstimator):\n    # Test that an error is raised when a LabelBinarizer\n    # with an incompatible y_type_ is used in a MLP.\n\n    # Multi-output regression\n    X, y = make_regression(n_samples=50, n_features=5, n_targets=3, random_state=0)\n    mlp = MLPEstimator()\n    with pytest.raises(ValueError):\n        mlp.fit(X, y)\n\n    # Multi-label classification\n    X, y = make_multilabel_classification(\n        n_samples=50, random_state=0, return_indicator=True\n    )\n    mlp = MLPEstimator()\n    mlp.fit(X, y)\n\n    y_binary = np.array([0] * 25 + [1] * 25)\n    mlp = MLPEstimator()\n    with pytest.raises(ValueError):\n        mlp.fit(X, y_binary)\n\n    y_multiclass = np.array([0] * 25 + [1] * 15 + [2] * 10)\n    mlp = MLPEstimator()\n    mlp.fit(X, y_multiclass)\n    with pytest.raises(ValueError):\n        mlp.fit(X, y)\n"], "sample_876": ["def test_mlp_warm_start_with_partial_fit(MLPEstimator):\n    \"\"\"Check that warm_start works with partial_fit.\"\"\"\n    mlp = MLPEstimator(warm_start=True)\n    mlp.partial_fit(X_iris[:10], y_iris[:10])\n    n_iter = mlp.n_iter_\n    mlp.partial_fit(X_iris[10:20], y_iris[10:20])\n    assert mlp.n_iter_ > n_iter\n", "def test_mlp_predict_proba_on_sparse_input(MLPEstimator):\n    \"\"\"Check that predict_proba works as expected for sparse input.\"\"\"\n    X, y = X_digits_binary[:10], y_digits_binary[:10]\n    mlp = MLPEstimator(hidden_layer_sizes=5, random_state=1)\n    mlp.fit(X, y)\n    X_sparse = csr_matrix(X)\n    y_proba = mlp.predict_proba(X_sparse)\n    assert y_proba.shape == (X_sparse.shape[0], np.unique(y).size)\n", "def test_mlp_output_shape(MLPEstimator):\n    # Test that the shape of the output matches the shape of y\n    X = X_digits[:100]\n    y = y_digits[:100]\n\n    mlp = MLPEstimator(hidden_layer_sizes=10, random_state=1)\n    mlp.fit(X, y)\n\n    # Make sure output shape matches y.shape\n    assert mlp.predict(X).shape == y.shape\n\n    # Test multi-output regression\n    X, y = make_regression(n_samples=100, n_features=5, n_targets=3)\n    mlp = MLPRegressor(hidden_layer_sizes=10, random_state=1)\n    mlp.fit(X, y)\n    assert mlp.predict(X).shape == y.shape\n", "def test_mlp_regressor_intercepts():\n    # Test that the MLP regressor can handle intercepts when y is a vector.\n    X = np.array([[1, 2, 3]])\n    y = np.array([1, 2, 3])\n    mlp = MLPRegressor(hidden_layer_sizes=(1,))\n    mlp.fit(X, y)\n    assert mlp.intercepts_[0].shape == (1,)\n    assert mlp.intercepts_[1].shape == (3,)\n", "def test_mlp_input_validation():\n    # Test input validation\n    X, y = X_digits_binary[:10], y_digits_binary[:10]\n    mlp = MLPClassifier(hidden_layer_sizes=5, random_state=1)\n\n    # Test invalid input for X\n    with pytest.raises(ValueError, match=\"0.0\"):\n        mlp.fit(X.astype(np.int8), y)\n\n    # Test invalid input for y\n    with pytest.raises(ValueError, match=\"y should be an array-like of shape\"):\n        mlp.fit(X, y.astype(np.float64))\n\n    # Test non-finite input\n    X[0, 0] = np.nan\n    with pytest.raises(ValueError, match=\"Input contains NaN, infinity or a value too large\"):\n        mlp.fit(X, y)\n\n    # Test inconsistent number of features\n    X[0, :] = np.nan\n    X = np.vstack((X, np.ones(2)))\n    with pytest.raises(ValueError, match=\"Input contains NaN, infinity or a value too large\"):\n        mlp.fit(X, y)\n\n    # Test sparse input\n    from scipy.sparse import csr_matrix\n    X_sparse = csr_matrix(X)\n    mlp.fit(X_sparse, y)\n\n    # Test input with feature names\n    import pandas as pd\n    X_df = pd.DataFrame(X, columns=['feature1', 'feature2'])\n    mlp.fit(X_df, y)\n\n    # Test input with non-string feature names\n    X_df.columns = [1, 2]\n    with pytest.raises(ValueError, match=\"feature_names_in_ does not match\"):\n        mlp.fit(X_df, y)\n"], "sample_424": ["def test_alter_field_reloads_state_on_fk_with_to_field_target_type_change_in_reverse(self):\n    app_label = \"alter_alter_field_reloads_state_on_fk_with_to_field_target_type_change_in_reverse\"\n    project_state = self.apply_operations(\n        app_label,\n        ProjectState(),\n        operations=[\n            migrations.CreateModel(\n                \"Rider\",\n                fields=[\n                    (\"id\", models.CharField(primary_key=True, max_length=100)),\n                    (\"code\", models.CharField(unique=True, max_length=100)),\n                ],\n            ),\n            migrations.CreateModel(\n                \"Pony\",\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\n                        \"rider\",\n                        models.ForeignKey(\n                            \"%s.Rider\" % app_label, models.CASCADE, to_field=\"code\"\n                        ),\n                    ),\n                ],\n            ),\n        ],\n    )\n    operation = migrations.AlterField(\n        \"Rider\",\n        \"code\",\n        models.IntegerField(unique=True),\n    )\n    self.unapply_operations(app_label, project_state, operations=[operation])\n    id_type, id_null = [\n        (c.type_code, c.null_ok)\n        for c in self.get_table_description(\"%s_rider\" % app_label)\n        if c.name == \"code\"\n    ][0]\n    fk_type, fk_null = [\n        (c.type_code, c.null_ok)\n        for c in self.get_table_description(\"%s_pony\" % app_label)\n        if c.name == \"rider_id\"\n    ][0]\n    self.assertEqual(id_type, fk_type)\n    self.assertEqual(id_null, fk_null)\n", "def test_add_constraint_string_quoting(self):\n    app_label = \"test_add_constraint_string_quoting\"\n    constraint_name = \"test_add_constraint_string_quoting\"\n    project_state = self.set_up_test_model(app_label)\n    check = ~models.Q(name__startswith=models.F(\"surname\"))\n    constraint = models.CheckConstraint(check=check, name=constraint_name)\n    operation = migrations.AddConstraint(\"Pony\", constraint)\n    to_state = project_state.clone()\n    operation.state_forwards(app_label, to_state)\n    with connection.schema_editor() as editor:\n        operation.database_forwards(app_label, editor, project_state, to_state)\n    Pony = to_state.apps.get_model(app_label, \"Pony\")\n    with self.assertRaises(IntegrityError), transaction.atomic():\n        Pony.objects.create(name=\"Albert\", surname=\"Alberto\")\n    Pony.objects.create(name=\"Hemingway\", surname=\"Hemingway\")\n", "def test_rename_model_partial_unique_constraint(self):\n    \"\"\"\n    RenameModel should preserve partial unique constraints.\n    \"\"\"\n    app_label = \"test_rnmpuc\"\n    project_state = self.apply_operations(\n        app_label,\n        ProjectState(),\n        operations=[\n            migrations.CreateModel(\n                \"Pony\",\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"pink\", models.IntegerField(default=3)),\n                    (\"weight\", models.FloatField()),\n                ],\n                options={\n                    \"constraints\": [\n                        models.UniqueConstraint(\n                            fields=[\"pink\"],\n                            condition=models.Q(weight__gt=5),\n                            name=\"test_constraint_pony_pink_for_weight_gt_5_uniq\",\n                        ),\n                    ]\n                },\n            )\n        ],\n    )\n    Pony = project_state.apps.get_model(app_label, \"Pony\")\n    Pony.objects.create(pink=1, weight=4.0)\n    Pony.objects.create(pink=1, weight=4.0)\n    Pony.objects.create(pink=1, weight=6.0)\n    if connection.features.supports_partial_indexes:\n        with self.assertRaises(IntegrityError):\n            Pony.objects.create(pink=1, weight=7.0)\n    else:\n        Pony.objects.create(pink=1, weight=7.0)\n\n    project_state = self.apply_operations(\n        app_label,\n        project_state,\n        operations=[\n            migrations.RenameModel(\"Pony\", \"LittleHorse\"),\n        ],\n        atomic=connection.features.supports_atomic_references_rename,\n    )\n    LittleHorse = project_state.apps.get_model(app_label, \"LittleHorse\")\n    LittleHorse.objects.create(pink=1, weight=4.0)\n    LittleHorse.objects.create(pink=1, weight=4.0)\n    LittleHorse.objects.create(pink=1, weight=6.0)\n    if connection.features.supports_partial_indexes:\n        with self.assertRaises(IntegrityError):\n            LittleHorse.objects.create(pink=1, weight=7.0)\n    else:\n        LittleHorse.objects.create(pink=1, weight=7.0)\n", "def test_rename_model_multiple_fk(self):\n    \"\"\"\n    Test RenameModel operation on a model which has multiple foreign keys.\n    \"\"\"\n    app_label = \"test_rename_model_multiple_fk\"\n    project_state = self.apply_operations(\n        app_label,\n        ProjectState(),\n        operations=[\n            migrations.CreateModel(\n                \"Pony\",\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                ],\n            ),\n            migrations.CreateModel(\n                \"Rider\",\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"pony\", models.ForeignKey(\"Pony\", models.CASCADE)),\n                    (\"friend\", models.ForeignKey(\"Pony\", models.CASCADE)),\n                ],\n            ),\n        ],\n    )\n    Pony = project_state.apps.get_model(app_label, \"Pony\")\n    Rider = project_state.apps.get_model(app_label, \"Rider\")\n    pony1 = Pony.objects.create()\n    pony2 = Pony.objects.create()\n    rider = Rider.objects.create(pony=pony1, friend=pony2)\n\n    project_state = self.apply_operations(\n        app_label,\n        project_state,\n        operations=[\n            migrations.RenameModel(\"Pony\", \"Horse\"),\n        ],\n        atomic=connection.features.supports_atomic_references_rename,\n    )\n    Horse = project_state.apps.get_model(app_label, \"Horse\")\n    Rider = project_state.apps.get_model(app_label, \"Rider\")\n    self.assertEqual(rider.pony_id, pony1.pk)\n    self.assertEqual(rider.friend_id, pony2.pk)\n    self.assertEqual(Rider.objects.count(), 1)\n    self.assertEqual(Horse.objects.count(), 2)\n", "def test_rename_field_case_insensitive_references(self):\n    \"\"\"\n    RenameField should correctly rename the field on the referencing side even if\n    the model name is not case matched.\n    \"\"\"\n    app_label = \"test_rename_field_case_insensitive_references\"\n    project_state = self.set_up_test_model(app_label, related_model=True)\n\n    new_state = project_state.clone()\n    operation = migrations.RenameField(\"Pony\", \"pink\", \"blue\")\n    operation.state_forwards(app_label, new_state)\n\n    Pony = new_state.apps.get_model(app_label, \"Pony\")\n    Rider = new_state.apps.get_model(app_label, \"Rider\")\n    self.assertEqual(\n        Rider._meta.get_field(\"pony\").remote_field.model,\n        \"Pony\",\n    )\n    self.assertEqual(\n        Rider._meta.get_field(\"pony\").remote_field.model._meta.model_name,\n        \"pony\",\n    )\n    with connection.schema_editor() as editor:\n        operation.database_forwards(app_label, editor, project_state, new_state)\n\n    Pony = new_state.apps.get_model(app_label, \"Pony\")\n    Rider = new_state.apps.get_model(app_label, \"Rider\")\n    rider = Rider.objects.create(pony=Pony.objects.create(blue=1))\n    self.assertEqual(rider.pony_id, rider.pony.pk)\n"], "sample_246": ["    def test_no_obsoletes_enabled(self):\n        \"\"\"Behavior is correct if --no-obsolete switch is specified.\"\"\"\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0, no_obsolete=True)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertNotIn('#~ msgid', po_contents)\n            self.assertNotIn('#~ msgstr', po_contents)\n", "    def setUp(self):\n        super().setUp()\n        self.symlinked_dir = os.path.join(self.test_dir, 'symlinked_templates')\n        os.symlink(os.path.join(self.test_dir, 'templates'), self.symlinked_dir)\n        self.symlinked_template = os.path.join(self.symlinked_dir, 'test.html')\n", "    def test_locale_paths_without_domain(self):\n        \"\"\"\n        * translations for an app containing a locale folder are stored in that folder\n        * translations outside of that app are in LOCALE_PATHS[0]\n        * when the domain is 'djangojs', files in LOCALE_PATHS are ignored\n        \"\"\"\n        with override_settings(LOCALE_PATHS=[os.path.join(self.test_dir, 'locale_path_without_domain', 'de', 'LC_MESSAGES')]):\n            management.call_command('makemessages', domain='django', locale=[LOCALE], verbosity=0)\n            locale_file = os.path.join(self.test_dir, 'locale_path_without_domain', 'de', 'LC_MESSAGES', 'django.po')\n            self.assertTrue(os.path.exists(locale_file))\n            with open(locale_file) as fp:\n                po_contents = fp.read()\n                self.assertMsgId('This app has no locale directory', po_contents)\n                self.assertMsgId('This is a project-level string', po_contents)\n            management.call_command('makemessages', domain='djangojs', locale=[LOCALE], verbosity=0)\n            self.assertTrue(os.path.exists(self.PO_FILE))\n            with open(self.PO_FILE) as fp:\n                po_contents = fp.read()\n                self.assertMsgId('This literal should be included.', po_contents)\n                self.assertMsgId('gettext_noop should, too.', po_contents)\n                self.assertMsgId('This one as well.', po_contents)\n            self.assertFalse(os.path.exists(locale_file))\n", "    def test_cleanup_no_file(self):\n        build_file = self.build_file_class(\n            mock.Mock(),\n            'django',\n            self.translatable_file_class('.', 'example.txt', '.'),\n        )\n        build_file.cleanup()\n        # No exception is raised when trying to remove a non-existent file.\n", "    def test_process_locale_dir_with_non_existing_pofile(self):\n        \"\"\"\n        process_locale_dir does not raise an error when no POT file exists.\n        \"\"\"\n        locale_dir = os.path.join(self.test_dir, 'locale')\n        os.makedirs(os.path.join(locale_dir, 'de', 'LC_MESSAGES'))\n        files = [\n            self.translatable_file_class(self.test_dir, 'test.html', locale_dir),\n        ]\n        command = MakeMessagesCommand()\n        command.locale_paths = [locale_dir]\n        command.domain = 'django'\n        command.extensions = ['html']\n        command.process_locale_dir(locale_dir, files)\n        self.assertTrue(os.path.exists(os.path.join(locale_dir, 'de', 'LC_MESSAGES', 'django.po')))\n"], "sample_676": ["def test_rewrite_with_erase_longer_line(testdir, monkeypatch):\n    config = testdir.parseconfig()\n    f = py.io.TextIO()\n    monkeypatch.setattr(f, \"isatty\", lambda *args: True)\n    tr = TerminalReporter(config, f)\n    tr._tw.fullwidth = 10\n    tr.write(\"hello\")\n    tr.rewrite(\"hey there\", erase=True)\n    assert f.getvalue() == \"hello\" + \"\\r\" + \"hey there\" + (6 * \" \")\n", "    def test_verbosity_level(self, testdir):\n        p1 = testdir.makepyfile(\n            \"\"\"\n                pass\n            class TestClass(object):\n                    pass\n        \"\"\"\n        )\n        result = testdir.runpytest(p1, \"-v\", \"-v\")\n        result.stdout.fnmatch_lines(\n            [\n                \"*test_passes.py::test_passes PASSED*\",\n                \"*TestClass::test_method PASSED*\",\n            ]\n        )\n        assert result.ret == 0\n\n        result = testdir.runpytest(p1, \"-v\", \"-v\", \"-v\")\n        result.stdout.fnmatch_lines(\n            [\n                \"*test_passes.py::test_passes PASSED*\",\n                \"*TestClass::test_method PASSED*\",\n                \"*2 passed in*\",\n            ]\n        )\n        assert result.ret == 0\n\n        result = testdir.runpytest(p1, \"-v\", \"-v\", \"-v\", \"-v\")\n        result.stdout.fnmatch_lines(\n            [\n                \"*test_passes.py::test_passes PASSED*\",\n                \"*TestClass::test_method PASSED*\",\n                \"*2 passed in*\",\n            ]\n        )\n        assert result.ret == 0\n", "def test_summary_warnings_interleaved_with_other_summaries(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import warnings\n            warnings.warn(UserWarning('warning_from_test'))\n            assert 0\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-ra\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*= warnings summary =*\",\n            \"*warning_from_test*\",\n            \"*= short test summary info =*\",\n            \"*FAILED test_summary_warnings_interleaved_with_other_summaries.py::test_warning - assert 0*\",\n            \"*= warnings summary (final) =*\",\n            \"*= 1 failed, 1 passed, 1 warnings in *\",\n        ]\n    )\n    assert \"None\" not in result.stdout.str()\n    stdout = result.stdout.str()\n    assert stdout.count(\"warning_from_test\") == 1\n    assert stdout.count(\"=== warnings summary \") == 2\n", "def test_short_test_summary_info_has_correct_output_for_xfailed_tests(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.xfail(reason=\"some reason\")\n            assert 0\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-rx\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_short_test_summary_info_has_correct_output_for_xfailed_tests.py::test_xfail XFAIL*\",\n            \"*some reason*\",\n        ]\n    )\n    assert result.ret == 0\n", "def test_fulltrace_reporting(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            assert 0\n            import pytest\n            pytest.skip('skip me please!')\n            raise KeyboardInterrupt   # simulating the user\n                b()\n                c()\n                assert False\n            a()\n    \"\"\"\n    )\n\n    result = testdir.runpytest(*[\"--fulltrace\"])\n    result.stdout.fnmatch_lines(\n        [\n            \"    def test_foobar():\",\n            \">       assert 0\",\n            \"E       assert 0\",\n            \"*_fulltrace_reporting.py:6: AssertionError*\",\n            \"*_fulltrace_reporting.py:10: KeyboardInterrupt*\",\n        ]\n    )\n    assert \"_fulltrace_reporting.py:17: AssertionError\" in result.stdout.str()\n    assert \"def a()\" in result.stdout.str()\n    assert \"def b()\" in result.stdout.str()\n    assert \"def c()\" in result.stdout.str()\n    assert result.stdout.str().count(\"_fulltrace_reporting.py:21: AssertionError\") == 1\n"], "sample_1175": ["def test_pretty_add_sub():\n    expr = x + y - x\n    ascii_str = \\", "def test_DiagramGrid():\n    from sympy.categories import DiagramGrid\n\n    A = Object(\"A\")\n    B = Object(\"B\")\n    C = Object(\"C\")\n\n    d = Diagram({\n        NamedMorphism(A, B, \"f\"): A,\n        NamedMorphism(B, C, \"g\"): B,\n        NamedMorphism(A, C, \"h\"): C\n    })\n\n    grid = DiagramGrid(d)\n\n    assert pretty(grid) == \"A  B\\n      \\nA    \"\n    assert upretty(grid) == \"A\u2081  A\u2082\\n      \\nA\u2083   \"\n    assert pretty(grid) == \"A  B\\n      \\nC    \"\n    assert upretty(grid) == \"A\u2081  A\u2082\\n      \\nC\u2083   \"\n    assert pretty(grid) == \"A  B\\n      \\nC    \"\n    assert upretty(grid) == \"A\u2081  A\u2082\\n      \\nC\u2083   \"\n    assert pretty(grid) == \"A  B\\n      \\nA    \"\n    assert upretty(grid) == \"A\u2081  A\u2082\\n      \\nA\u2083   \"\n    assert pretty(grid) == \"A  B\\n      \\nA    \"\n    assert upretty(grid) == \"A\u2081  A\u2082\\n      \\nA\u2083   \"\n    assert pretty(grid) == \"A  B\\n      \\nC    \"\n    assert upretty(grid) == \"A\u2081  A\u2082\\n      \\nC\u2083   \"\n    assert pretty(grid) == \"A  B\\n      \\nC    \"\n    assert upretty(grid) == \"A\u2081  A\u2082\\n      \\nC\u2083   \"\n", "def test_pretty_pretty_form():\n    expr = 1/(1 + 1/(1 + 1/(1/x + 1) + 1))\n    ucode_str = \\", "def test_issue_18055():\n    from sympy.physics.control.lti import MIMOParallel\n    tf1 = TransferFunction(x + y, x - 2*y, y)\n    tf2 = TransferFunction(x - y, x + y, y)\n    tf3 = TransferFunction(x**2 + y, y - x, y)\n    tf4 = TransferFunction(x**3 + y**2, x**2 + x, y)\n\n    tfm1 = TransferFunctionMatrix([[tf1, tf2], [tf3, -tf4], [-tf2, -tf1]])\n    tfm2 = TransferFunctionMatrix([[-tf2, -tf1], [tf4, -tf3], [tf2, tf1]])\n    tfm3 = TransferFunctionMatrix([[tf1, tf1], [tf3, tf3], [tf2, tf2]])\n    tfm4 = TransferFunctionMatrix([[-tf1, -tf2], [-tf3, -tf4]])\n\n    expected1 = \\", "def test_pretty_powissue_20889():\n    # test case for issue 20889\n    a = Symbol(\"a\")\n    b = Symbol(\"b\")\n    e = Pow(b, 3)\n    assert pretty(e**a) == \\"], "sample_796": ["def test_huber_epsilon_validation():\n    # Test that HuberRegressor raises an error for invalid epsilon values\n    with pytest.raises(ValueError):\n        HuberRegressor(epsilon=0.5)\n", "def test_huber_invalid_epsilon():\n    # Test that HuberRegressor raises an error when epsilon is invalid\n    X, y = make_regression_with_outliers()\n    with pytest.raises(ValueError):\n        HuberRegressor(epsilon=0.9).fit(X, y)\n\n    with pytest.raises(ValueError):\n        HuberRegressor(epsilon=-1.0).fit(X, y)\n\n    with pytest.raises(TypeError):\n        HuberRegressor(epsilon=\"1.0\").fit(X, y)\n", "def test_huber_epsilon_validation():\n    # Test that HuberRegressor raises a ValueError for epsilon < 1.0\n\n    X, y = make_regression_with_outliers()\n    huber = HuberRegressor(fit_intercept=True)\n\n    # epsilon should be greater than or equal to 1.0\n    with pytest.raises(ValueError):\n        huber.epsilon = 0.5\n        huber.fit(X, y)\n\n    # epsilon should be greater than or equal to 1.0\n    with pytest.raises(ValueError):\n        huber.epsilon = -1.0\n        huber.fit(X, y)\n\n    # epsilon should be greater than or equal to 1.0\n    with pytest.raises(ValueError):\n        HuberRegressor(epsilon=0.5).fit(X, y)\n\n    # epsilon should be greater than or equal to 1.0\n    with pytest.raises(ValueError):\n        HuberRegressor(epsilon=-1.0).fit(X, y)\n", "def test_huber_with_zeros():\n    # Test HuberRegressor with all zeros data\n    X = np.zeros((100, 5))\n    y = np.zeros(100)\n\n    huber = HuberRegressor(fit_intercept=True)\n    huber.fit(X, y)\n\n    assert_array_almost_equal(huber.coef_, np.zeros(5))\n    assert_almost_equal(huber.intercept_, 0.0)\n    assert_array_equal(huber.outliers_, np.zeros(100, dtype=bool))\n\n    # Test HuberRegressor with sample weights\n    sample_weight = np.ones(100)\n    sample_weight[0] = 0\n    huber.fit(X, y, sample_weight=sample_weight)\n\n    assert_array_almost_equal(huber.coef_, np.zeros(5))\n    assert_almost_equal(huber.intercept_, 0.0)\n    assert_array_equal(huber.outliers_, np.zeros(100, dtype=bool))\n\n    # Test HuberRegressor with sparse data\n    X_sparse = sparse.csr_matrix(X)\n    huber.fit(X_sparse, y)\n\n    assert_array_almost_equal(huber.coef_, np.zeros(5))\n    assert_almost_equal(huber.intercept_, 0.0)\n    assert_array_equal(huber.outliers_, np.zeros(100, dtype=bool))\n", "def test_huber_input_validation():\n    # Test that the input validation raises errors when necessary\n    X, y = make_regression_with_outliers()\n    huber = HuberRegressor(fit_intercept=True)\n\n    # Test that epsilon < 1.0 raises an error\n    with pytest.raises(ValueError):\n        HuberRegressor(epsilon=0.5).fit(X, y)\n\n    # Test that X or y is not array-like raises an error\n    with pytest.raises(TypeError):\n        huber.fit(\"not array-like\", y)\n\n    with pytest.raises(TypeError):\n        huber.fit(X, \"not array-like\")\n\n    # Test that sample_weight is not array-like raises an error\n    with pytest.raises(TypeError):\n        huber.fit(X, y, sample_weight=\"not array-like\")\n\n    # Test that sample_weight is not 1D raises an error\n    with pytest.raises(ValueError):\n        huber.fit(X, y, sample_weight=np.array([[1, 2]]))\n\n    # Test that sample_weight is not of same length as y raises an error\n    with pytest.raises(ValueError):\n        huber.fit(X, y, sample_weight=np.array([1, 2]))\n"], "sample_662": ["def test_collectreport_with_result(self, testdir):\n    \"\"\"This test came originally from test_remote.py in xdist (ca03269).\"\"\"\n    reprec = testdir.inline_runsource(\n        \"\"\"\n    \"\"\"\n    )\n    reports = reprec.getreports(\"pytest_collectreport\")\n    for rep in reports:\n        d = rep._to_json()\n        newrep = CollectReport._from_json(d)\n        assert newrep.passed == rep.passed\n        assert newrep.failed == rep.failed\n        assert newrep.skipped == rep.skipped\n        assert newrep.result == rep.result\n", "def test_base_report_longreprtext_property(testdir):\n    \"\"\"\n    Test that the longreprtext property of BaseReport correctly handles\n    different types of longrepr objects.\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n    \"\"\"\n    )\n    reprec = testdir.inline_run()\n    reports = reprec.getreports(\"pytest_runtest_logreport\")\n    assert len(reports) == 3\n    test_a_call = reports[1]\n    assert test_a_call.when == \"call\"\n    assert test_a_call.outcome == \"failed\"\n\n    # Test with longrepr being an instance of ExceptionInfo\n    assert isinstance(test_a_call.longrepr, ExceptionInfo)\n    assert test_a_call.longreprtext\n\n    # Test with longrepr being a string\n    test_a_call.longrepr = \"Some error message\"\n    assert test_a_call.longreprtext == \"Some error message\"\n\n    # Test with longrepr being None\n    test_a_call.longrepr = None\n    assert test_a_call.longreprtext == \"\"\n", "def test_caplog_capstdout_capstderr(self, testdir):\n    \"\"\"Test that captured logs and stdout/stderr are properly serialized/deserialized.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import sys\n\n            logging.warning(\"warning\")\n            logging.error(\"error\")\n            print(\"stdout\")\n            print(\"stderr\", file=sys.stderr)\n            assert False\n    \"\"\"\n    )\n    reprec = testdir.inline_run()\n    reports = reprec.getreports(\"pytest_runtest_logreport\")\n    assert len(reports) == 3\n    rep = reports[1]\n    data = rep._to_json()\n    new_rep = TestReport._from_json(data)\n    assert new_rep.caplog == rep.caplog\n    assert new_rep.capstdout == rep.capstdout\n    assert new_rep.capstderr == rep.capstderr\n", "def test_report_sections(self, testdir):\n    \"\"\"\n    Test that report sections are correctly serialized and deserialized.\n    \"\"\"\n    reprec = testdir.inline_runsource(\n        \"\"\"\n            assert False\n    \"\"\"\n    )\n    reports = reprec.getreports(\"pytest_runtest_logreport\")\n    assert len(reports) == 3\n    report = reports[1]\n    report.sections.append((\"Section 1\", \"content\", \"=\"))\n    report.sections.append((\"Section 2\", \"content\", \"-\"))\n    data = report._to_json()\n    new_report = TestReport._from_json(data)\n    assert new_report.sections == report.sections\n", "    def test_report_with_extra_attributes(self, testdir):\n        reprec = testdir.inline_runsource(\n            \"\"\"\n        \"\"\"\n        )\n        reports = reprec.getreports(\"pytest_runtest_logreport\")\n        assert len(reports) == 3\n        report = reports[1]\n        report.extra_attribute = \"value\"\n        data = report._to_json()\n        new_report = TestReport._from_json(data)\n        assert hasattr(new_report, \"extra_attribute\")\n        assert new_report.extra_attribute == \"value\"\n"], "sample_442": ["    def test_dumps(self):\n        serializer = signing.JSONSerializer()\n        examples = [\n            {\"key\": \"value\"},\n            [1, 2, 3],\n            \"a string\",\n        ]\n        for obj in examples:\n            dumped = serializer.dumps(obj)\n            self.assertIsInstance(dumped, bytes)\n            self.assertEqual(obj, serializer.loads(dumped))\n", "    def test_dumps(self):\n        \"dumps should encode the object as a json string\"\n        serializer = signing.JSONSerializer()\n        obj = {\"foo\": \"bar\", \"baz\": 1}\n        expected = b'{\"foo\":\"bar\",\"baz\":1}'\n        self.assertEqual(serializer.dumps(obj), expected)\n", "    def test_compression_saves_space(self):\n        signer = signing.Signer(key=\"predictable-secret\")\n        data = [1] * 1000\n        compressed = signer.sign_object(data, compress=True)\n        uncompressed = signer.sign_object(data)\n        self.assertLess(len(compressed), len(uncompressed))\n        self.assertEqual(signer.unsign_object(compressed), data)\n        self.assertEqual(signer.unsign_object(uncompressed), data)\n", "    def test_custom_salt(self):\n        signer = signing.Signer(key=\"predictable-secret\", salt=\"custom-salt\")\n        self.assertEqual(\n            signer.signature(\"hello\"),\n            signing.base64_hmac(\n                \"custom-salt\" + \"signer\",\n                \"hello\",\n                \"predictable-secret\",\n                algorithm=signing.Signer.algorithm,\n            ),\n        )\n", "    def test_serializer(self):\n        \"Serializer should be able to serialize and deserialize objects\"\n        serializer = signing.JSONSerializer()\n        obj = {\"a\": \"dictionary\", \"b\": 1, \"c\": 1.23}\n        self.assertEqual(obj, serializer.loads(serializer.dumps(obj)))\n"], "sample_369": ["def test_add_custom_field_with_to_field(self):\n    class HardcodedForeignKey(models.ForeignKey):\n            kwargs['to'] = 'testapp.Author'\n            kwargs['to_field'] = 'custom_id'\n            super().__init__(*args, **kwargs)\n\n            name, path, args, kwargs = super().deconstruct()\n            del kwargs['to']\n            del kwargs['to_field']\n            return name, path, args, kwargs\n\n    author_custom_id = ModelState('testapp', 'Author', [\n        ('custom_id', models.AutoField(primary_key=True)),\n    ])\n    book_hardcoded_fk_to = ModelState('testapp', 'Book', [\n        ('author', HardcodedForeignKey(on_delete=models.CASCADE)),\n    ])\n    changes = self.get_changes(\n        [author_custom_id],\n        [author_custom_id, book_hardcoded_fk_to],\n    )\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['CreateModel'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='Book')\n", "def test_alter_index_name(self):\n    \"\"\"\n    Tests changing the name of an index.\n    \"\"\"\n    before = ModelState('testapp', 'Book', [\n        ('id', models.AutoField(primary_key=True)),\n        ('author', models.ForeignKey('testapp.Author', models.CASCADE)),\n        ('title', models.CharField(max_length=200)),\n    ], {\n        \"indexes\": [models.Index(fields=['author', 'title'], name=\"book_title_author_idx\")],\n    })\n    after = ModelState('testapp', 'Book', [\n        ('id', models.AutoField(primary_key=True)),\n        ('author', models.ForeignKey('testapp.Author', models.CASCADE)),\n        ('title', models.CharField(max_length=200)),\n    ], {\n        \"indexes\": [models.Index(fields=['author', 'title'], name=\"book_title_author_idx_new\")],\n    })\n    changes = self.get_changes([before], [after])\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['RemoveIndex', 'AddIndex'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, model_name='book', name='book_title_author_idx')\n    added_index = models.Index(fields=['author', 'title'], name='book_title_author_idx_new')\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, model_name='book', index=added_index)\n", "def test_reduce_operations(self):\n    # AddField to CreateModel\n    create_model = CreateModel(\"TestModel\", [(\"id\", models.AutoField(primary_key=True))])\n    add_field = AddField(\"TestModel\", \"test_field\", models.IntegerField())\n    reduced = create_model.reduce(add_field, \"test_app\")\n    self.assertEqual(len(reduced), 1)\n    self.assertIsInstance(reduced[0], CreateModel)\n    self.assertEqual(reduced[0].fields, [(\"id\", models.AutoField(primary_key=True)), (\"test_field\", models.IntegerField())])\n\n    # AlterField to CreateModel\n    alter_field = AlterField(\"TestModel\", \"test_field\", models.IntegerField())\n    reduced = create_model.reduce(alter_field, \"test_app\")\n    self.assertEqual(len(reduced), 1)\n    self.assertIsInstance(reduced[0], CreateModel)\n    self.assertEqual(reduced[0].fields, [(\"id\", models.AutoField(primary_key=True)), (\"test_field\", models.IntegerField())])\n\n    # RenameField to CreateModel\n    rename_field = RenameField(\"TestModel\", \"test_field\", \"renamed_field\")\n    reduced = create_model.reduce(rename_field, \"test_app\")\n    self.assertEqual(len(reduced), 1)\n    self.assertIsInstance(reduced[0], CreateModel)\n    self.assertEqual(reduced[0].fields, [(\"id\", models.AutoField(primary_key=True)), (\"renamed_field\", models.IntegerField())])\n\n    # RemoveField to CreateModel\n    remove_field = RemoveField(\"TestModel\", \"test_field\")\n    reduced = create_model.reduce(remove_field, \"test_app\")\n    self.assertEqual(len(reduced), 1)\n    self.assertIsInstance(reduced[0], CreateModel)\n    self.assertEqual(reduced[0].fields, [(\"id\", models.AutoField(primary_key=True))])\n\n    # AlterTogetherOptionOperation to CreateModel\n    alter_unique_together = AlterUniqueTogether(\"TestModel\", {(\"test_field\",)})\n    reduced = create_model.reduce(alter_unique_together, \"test_app\")\n    self.assertEqual(len(reduced), 1)\n    self.assertIsInstance(reduced[0], CreateModel)\n    self.assertEqual(reduced[0].options, {\"unique_together\": {(\"test_field\",)}})\n\n    # AlterOrderWithRespectTo to CreateModel\n    alter_order_with_respect_to = AlterOrderWithRespectTo(\"TestModel\", \"test_field\")\n    reduced = create_model.reduce(alter_order_with_respect_to, \"test_app\")\n    self.assertEqual(len(re", "def test_add_field_with_nested_deconstructible_default(self):\n    \"\"\"\n    Adding a field with a default that deconstructs into a nested object\n    should work.\n    \"\"\"\n    tests = [\n        {\n            'field_name': 'name',\n            'default_value': DeconstructibleObject(),\n            'expected_field_name': 'name',\n            'expected_default_value': DeconstructibleObject(),\n        },\n        {\n            'field_name': 'name',\n            'default_value': [DeconstructibleObject(), 123],\n            'expected_field_name': 'name',\n            'expected_default_value': [DeconstructibleObject(), 123],\n        },\n        {\n            'field_name': 'name',\n            'default_value': (DeconstructibleObject(), 123),\n            'expected_field_name': 'name',\n            'expected_default_value': (DeconstructibleObject(), 123),\n        },\n        {\n            'field_name': 'name',\n            'default_value': {\n                'item': DeconstructibleObject(),\n                'otheritem': 123,\n            },\n            'expected_field_name': 'name',\n            'expected_default_value': {\n                'item': DeconstructibleObject(),\n                'otheritem': 123,\n            },\n        },\n        {\n            'field_name': 'name',\n            'default_value': DeconstructibleObject(\n                DeconstructibleObject(1),\n                (DeconstructibleObject('t1'), DeconstructibleObject('t2'),),\n                a=DeconstructibleObject('A'),\n                b=DeconstructibleObject(B=DeconstructibleObject('c')),\n            ),\n            'expected_field_name': 'name',\n            'expected_default_value': DeconstructibleObject(\n                DeconstructibleObject(1),\n                (DeconstructibleObject('t1'), DeconstructibleObject('t2'),),\n                a=DeconstructibleObject('A'),\n                b=DeconstructibleObject(B=DeconstructibleObject('c')),\n            ),\n        },\n    ]\n    for test in tests:\n        author = ModelState(\n            'testapp',\n            'Author',\n            [\n                ('id', models.AutoField(primary_key=True)),\n                (test['field_name'], models.CharField(max_length=200, default=test['default_value'])),\n            ],\n        )\n        changes = self.get_changes([], [author])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp',", "def test_foreign_key_pointer_field_order(self):\n    \"\"\"\n    #24637 - The autodetector correctly handles ordering for ForeignKeys when\n    pointing to a model with a custom primary key.\n    \"\"\"\n    changes = self.get_changes([], [\n        self.author_empty,\n        self.book,\n        ModelState('otherapp', 'Edition', [\n            ('book', models.ForeignKey('otherapp.Book', models.CASCADE)),\n            ('id', models.AutoField(primary_key=True)),\n            ('custom_field', models.IntegerField(unique=True)),\n            ('custom_pk', models.ForeignKey('otherapp.Book', models.CASCADE, primary_key=True)),\n        ]),\n    ])\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, ['CreateModel', 'AddField', 'CreateModel'])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='Book')\n    self.assertOperationAttributes(changes, 'otherapp', 0, 1, model_name='book', name='author')\n    self.assertOperationAttributes(changes, 'otherapp', 0, 2, name='Edition')\n"], "sample_1124": ["def test_FracField_to_domain_to_ring():\n    F, x, y = field(\"x,y\", ZZ)\n\n    D = F.to_domain()\n    assert D == F.ring.to_field()\n\n    R = D.to_ring()\n    assert R == F.ring\n", "def test_FracField_parent():\n    F, x,y = field(\"x,y\", ZZ)\n\n    f = x/y\n    assert f.parent() == F\n\n    Fuv, u,v = field(\"u,v\", ZZ)\n    Fxyzt, x,y,z,t = field(\"x,y,z,t\", Fuv)\n\n    f = (u*v + x)/(y + u*v)\n    assert f.parent() == Fxyzt\n\n    Ruv, u,v = ring(\"u,v\", ZZ)\n    Fxyzt, x,y,z,t = field(\"x,y,z,t\", Ruv)\n\n    f = (u*v + x)/(y + u*v)\n    assert f.parent() == Fxyzt\n", "def test_FracElement___rpow__():\n    F, x,y = field(\"x,y\", QQ)\n\n    f = 1/x\n\n    assert 2**f == 2**(1/x)\n\n    F, x,y = field(\"x,y\", ZZ)\n    assert 2**f == 2**(1/x)\n\n    Fuv, u,v = field(\"u,v\", ZZ)\n    Fxyzt, x,y,z,t = field(\"x,y,z,t\", Fuv)\n\n    f = (u*v + x)/(y + u*v)\n    assert (u*v)**f == (u*v)**((u*v + x)/(y + u*v))\n\n    Ruv, u,v = ring(\"u,v\", ZZ)\n    Fxyzt, x,y,z,t = field(\"x,y,z,t\", Ruv)\n\n    f = (u*v + x)/(y + u*v)\n    assert (u*v)**f == (u*v)**((u*v + x)/(y + u*v))\n", "def test_FracElement_to_poly():\n    F, x,y,z = field(\"x,y,z\", ZZ)\n    f = (3*x**2*y - x*y*z)/(7*z**3 + 1)\n\n    assert f.to_poly() == f.numer\n    assert f.to_poly() != f.denom\n\n    raises(ValueError, lambda: (1/(x + 1)).to_poly())\n", "def test_FracElement___truediv___inverse():\n    F, x, y = field(\"x,y\", QQ)\n\n    f, g = 1/x, 1/y\n\n    assert 1/f == x\n    assert 1/g == y\n\n    assert f / (1/f) == F.one\n    assert g / (1/g) == F.one\n\n    raises(ZeroDivisionError, lambda: 1/(F.zero))\n    raises(ZeroDivisionError, lambda: 1/f)\n\n    Fuv, u,v = field(\"u,v\", ZZ)\n    Fxyzt, x,y,z,t = field(\"x,y,z,t\", Fuv)\n\n    f = (u*v)/(x*y)\n    assert dict(f.numer) == {(0, 0, 0, 0): u*v}\n    assert dict(f.denom) == {(1, 1, 0, 0): 1}\n\n    g = 1/f\n    assert dict(g.numer) == {(1, 1, 0, 0): 1}\n    assert dict(g.denom) == {(0, 0, 0, 0): u*v}\n\n    assert f * g == F.one\n    assert g * f == F.one\n"], "sample_594": ["def test_format_array_flat_with_large_string(self):\n    array = np.array([\"a\" * 1000])\n    actual = formatting.format_array_flat(array, 10)\n    expected = \"'a'...\" \n    assert expected == actual\n", "def test_summarize_variable(self):\n    var = xr.DataArray([1, 2, 3], dims=\"x\", name=\"foo\")\n    actual = formatting.summarize_variable(\"foo\", var, col_width=10)\n    expected = \"    * foo     (x) int64 1 2 3\"\n    assert actual == expected\n\n    var.attrs[\"units\"] = \"m\"\n    actual = formatting.summarize_variable(\"foo\", var, col_width=10)\n    expected = \"    * foo     (x) int64 1 2 3\"\n    assert actual == expected\n\n    var = xr.DataArray([1, 2, 3], dims=\"x\", name=\"foo\", coords={\"x\": [1, 2, 3]})\n    actual = formatting.summarize_variable(\"foo\", var, col_width=10)\n    expected = \"    * foo     (x) int64 1 2 3\"\n    assert actual == expected\n", "def test_limit_lines(self):\n    short_string = \"This is a short string\"\n    long_string = \"This is a long string that should be limited\\n\" * 100\n    assert formatting.limit_lines(short_string) == short_string\n    assert len(formatting.limit_lines(long_string).splitlines()) == 20\n", "def test_wrap_indent(self):\n    text = \"This is a long text that needs to be wrapped.\"\n    start = \"Start \"\n    expected = \"Start This is a long text that needs to be wrapped.\"\n    actual = formatting.wrap_indent(text, start)\n    assert actual == expected\n\n    text = \"\"\"This is a long text that needs to be wrapped.", "def test_diff_coords_repr(self):\n    coords_a = xr.DataArray(\n        np.array([1, 2, 3], dtype=\"int64\"),\n        dims=\"x\",\n        coords={\n            \"x\": np.array([\"a\", \"b\", \"c\"], dtype=\"U1\"),\n            \"y\": (\"x\", np.array([1, 2, 3], dtype=\"int64\")),\n        },\n    )\n\n    coords_b = xr.DataArray(\n        np.array([4, 5, 6], dtype=\"int64\"),\n        dims=\"x\",\n        coords={\n            \"x\": np.array([\"a\", \"c\", \"d\"], dtype=\"U1\"),\n            \"y\": (\"x\", np.array([1, 2, 3], dtype=\"int64\")),\n            \"z\": (\"x\", np.array([7, 8, 9], dtype=\"int64\")),\n        },\n    )\n\n    expected = dedent(\n        \"\"\"\\\n        Differing coordinates:\n        L * x        (x) %sU1 'a' 'b' 'c'\n        R * x        (x) %sU1 'a' 'c' 'd'\n        Coordinates only on the left object:\n          * y        (x) int64 1 2 3\n        Coordinates only on the right object:\n            z        (x) int64 7 8 9\"\"\"\n        % (\"<\" if sys.byteorder == \"little\" else \">\", \"<\" if sys.byteorder == \"little\" else \">\")\n    )\n\n    actual = formatting.diff_coords_repr(coords_a.coords, coords_b.coords, \"identical\")\n    assert actual == expected\n\n    # test MultiIndex\n    coords_a = xr.DataArray(\n        np.array([1, 2, 3], dtype=\"int64\"),\n        dims=\"x\",\n        coords={\n            \"x\": xr.IndexVariable(\n                (\"x\",), pd.MultiIndex.from_product([[\"a\", \"b\"], [\"c\", \"d\"]])\n            ),\n            \"y\": (\"x\", np.array([1, 2, 3], dtype=\"int64\")),\n        },\n    )\n\n    coords_b = xr.DataArray(\n        np.array([4, 5, 6], dtype=\"int64\"),\n        dims=\"x\",\n        coords={\n            \"x\": xr.IndexVariable(\n                (\"x\",), pd.MultiIndex.from_product([[\"a\", \"b\"], [\"e"], "sample_258": ["def test_receiver_sender(self):\n    @receiver(a_signal, sender=self)\n        self.state = val\n    @receiver(a_signal, sender=object())\n        self.state = 'not this'\n    self.state = False\n    a_signal.send(sender=self, val=True)\n    self.assertTrue(self.state)\n    a_signal.send(sender=object(), val='not this')\n    self.assertNotEqual(self.state, 'not this')\n", "def test_receiver_sender(self):\n    @receiver(a_signal, sender=self)\n        self.state.append(val)\n    @receiver(a_signal)\n        self.state.append(val * 2)\n    self.state = []\n    a_signal.send(sender=self, val='a')\n    a_signal.send(sender='other', val='b')\n    self.assertIn('a', self.state)\n    self.assertIn('aa', self.state)\n    self.assertNotIn('b', self.state)\n    self.assertIn('bb', self.state)\n", "    def test_receiver_multiple_signals(self):\n        @receiver(a_signal)\n            self.state.append('a')\n        @receiver(b_signal)\n            self.state.append('b')\n        self.state = []\n        a_signal.send(sender=self, val='test')\n        b_signal.send(sender=self, val='test')\n        self.assertEqual(self.state, ['a', 'b'])\n        self.state = []\n        a_signal.send_robust(sender=self, val='test')\n        b_signal.send_robust(sender=self, val='test')\n        self.assertEqual(self.state, ['a', 'b'])\n", "def test_receiver_decorator_with_kwargs(self):\n    @receiver(a_signal, sender=self)\n        self.state = val\n    self.state = False\n    a_signal.send(sender=self, val=True)\n    self.assertTrue(self.state)\n\n    @receiver(b_signal, sender=self, dispatch_uid='unique_uid')\n        self.state = val\n    self.state = False\n    b_signal.send(sender=self, val=True)\n    self.assertTrue(self.state)\n\n    with self.assertRaisesMessage(ValueError, 'Signal receivers must accept keyword arguments (**kwargs).'):\n        @receiver(c_signal)\n            pass\n\n    @receiver(d_signal, sender=self)\n        self.state = val\n    self.state = False\n    d_signal.send(sender=self, val=True)\n    self.assertTrue(self.state)\n", "    def test_connect_weak_method(self):\n        class Receiver:\n                self.val = None\n\n                self.val = val\n\n        receiver = Receiver()\n        a_signal.connect(receiver.handle_signal, weak=True)\n        a_signal.send(sender=self, val='test')\n        self.assertEqual(receiver.val, 'test')\n        del receiver\n        garbage_collect()\n        self.assertTestIsClean(a_signal)\n"], "sample_1083": ["def test_hyperbolic_functions():\n    x = Symbol('x')\n    y = Symbol('y', real=True)\n    assert sinh(x).diff(y) == cosh(x)\n    assert cosh(x).diff(y) == sinh(x)\n    assert tanh(x).diff(y) == 1 - tanh(x)**2\n    assert coth(x).diff(y) == -1/sinh(x)**2\n    assert csch(x).diff(y) == -coth(x)*csch(x)\n    assert sech(x).diff(y) == -tanh(x)*sech(x)\n    assert asinh(x).diff(y) == 1/sqrt(x**2 + 1)\n    assert acosh(x).diff(y) == 1/sqrt(x**2 - 1)\n    assert atanh(x).diff(y) == 1/(1 - x**2)\n    assert acoth(x).diff(y) == 1/(1 - x**2)\n    assert asech(x).diff(y) == -1/(x*sqrt(1 - x**2))\n    assert acsch(x).diff(y) == -1/(x**2*sqrt(1 + 1/x**2))\n", "def test_hyperbolic_extension():\n    x = Symbol('x', real=True, finite=True)\n    y = Symbol('y', real=True, finite=True)\n\n    assert sinh(x).is_extended_real is True\n    assert cosh(x).is_extended_real is True\n    assert tanh(x).is_extended_real is True\n    assert coth(x).is_extended_real is True\n    assert csch(x).is_extended_real is True\n    assert sech(x).is_extended_real is True\n    assert asinh(x).is_extended_real is True\n    assert acosh(x).is_extended_real is True\n    assert atanh(x).is_extended_real is True\n    assert acoth(x).is_extended_real is True\n    assert asech(x).is_extended_real is True\n    assert acsch(x).is_extended_real is True\n\n    assert sinh(x + y*I).is_extended_real is None\n    assert cosh(x + y*I).is_extended_real is None\n    assert tanh(x + y*I).is_extended_real is None\n    assert coth(x + y*I).is_extended_real is None\n    assert csch(x + y*I).is_extended_real is None\n    assert sech(x + y*I).is_extended_real is None\n    assert asinh(x + y*I).is_extended_real is None\n    assert acosh(x + y*I).is_extended_real is None\n    assert atanh(x + y*I).is_extended_real is None\n    assert acoth(x + y*I).is_extended_real is None\n    assert asech(x + y*I).is_extended_real is None\n    assert acsch(x + y*I).is_extended_real is None\n", "def test_hyperbolic_rewrite():\n    x, y = symbols('x,y')\n\n    assert sinh(x).rewrite(sinh) == sinh(x)\n    assert cosh(x).rewrite(cosh) == cosh(x)\n    assert tanh(x).rewrite(tanh) == tanh(x)\n    assert coth(x).rewrite(coth) == coth(x)\n    assert csch(x).rewrite(csch) == csch(x)\n    assert sech(x).rewrite(sech) == sech(x)\n    assert asinh(x).rewrite(asinh) == asinh(x)\n    assert acosh(x).rewrite(acosh) == acosh(x)\n    assert atanh(x).rewrite(atanh) == atanh(x)\n    assert acoth(x).rewrite(acoth) == acoth(x)\n    assert asech(x).rewrite(asech) == asech(x)\n    assert acsch(x).rewrite(acsch) == acsch(x)\n\n    assert sinh(x).rewrite('sinh') == sinh(x)\n    assert cosh(x).rewrite('cosh') == cosh(x)\n    assert tanh(x).rewrite('tanh') == tanh(x)\n    assert coth(x).rewrite('coth') == coth(x)\n    assert csch(x).rewrite('csch') == csch(x)\n    assert sech(x).rewrite('sech') == sech(x)\n    assert asinh(x).rewrite('asinh') == asinh(x)\n    assert acosh(x).rewrite('acosh') == acosh(x)\n    assert atanh(x).rewrite('atanh') == atanh(x)\n    assert acoth(x).rewrite('acoth') == acoth(x)\n    assert asech(x).rewrite('asech') == asech(x)\n    assert acsch(x).rewrite('acsch') == acsch(x)\n\n    assert sinh(x).rewrite('Sin') == sinh(x)\n    assert cosh(x).rewrite('Cos') == cosh(x)\n    assert tanh(x).rewrite('Tan') == tanh(x)\n    assert coth(x).rewrite('Cot') == coth(x)\n    assert csch(x).rewrite('Csch') == csch(x)\n    assert sech(x).rewrite('Sech') == sech(x)\n    assert asinh(x).rewrite('ASin') == asinh(x)\n    assert acosh(x).", "def test_hyperbolic_functions_with_high_degree():\n    x = Symbol('x')\n    for func in [sinh, cosh, tanh, coth, sech, csch]:\n        for degree in range(10, 50):\n            eq = func(x)**degree\n            assert eq.is_real is None\n            assert eq.is_extended_real is None\n", "def test_hyperbolic_edge_cases():\n    x = Symbol('x')\n\n    # Check that hyperbolic functions behave correctly at extreme values\n    assert sinh(S.Infinity) == S.Infinity\n    assert sinh(-S.Infinity) == -S.Infinity\n    assert sinh(S.ComplexInfinity) == S.ComplexInfinity\n\n    assert cosh(S.Infinity) == S.Infinity\n    assert cosh(-S.Infinity) == S.Infinity\n    assert cosh(S.ComplexInfinity) == S.ComplexInfinity\n\n    assert tanh(S.Infinity) == S.One\n    assert tanh(-S.Infinity) == S.NegativeOne\n    assert tanh(S.ComplexInfinity) == S.ComplexInfinity\n\n    assert coth(S.Infinity) == S.One\n    assert coth(-S.Infinity) == S.NegativeOne\n    assert coth(S.ComplexInfinity) == S.ComplexInfinity\n\n    assert csch(S.Infinity) == S.Zero\n    assert csch(-S.Infinity) == S.Zero\n    assert csch(S.ComplexInfinity) == S.ComplexInfinity\n\n    assert sech(S.Infinity) == S.Zero\n    assert sech(-S.Infinity) == S.Zero\n    assert sech(S.ComplexInfinity) == S.ComplexInfinity\n\n    # Check that inverse hyperbolic functions behave correctly at extreme values\n    assert asinh(S.Infinity) == S.Infinity\n    assert asinh(-S.Infinity) == -S.Infinity\n    assert asinh(S.ComplexInfinity) == S.ComplexInfinity\n\n    assert acosh(S.Infinity) == S.Infinity\n    assert acosh(-S.Infinity) == S.Infinity\n    assert acosh(S.ComplexInfinity) == S.ComplexInfinity\n\n    assert atanh(S.Infinity) == S.ComplexInfinity\n    assert atanh(-S.Infinity) == S.ComplexInfinity\n    assert atanh(S.ComplexInfinity) == S.ComplexInfinity\n\n    assert acoth(S.Infinity) == S.Zero\n    assert acoth(-S.Infinity) == S.Zero\n    assert acoth(S.ComplexInfinity) == S.ComplexInfinity\n\n    assert asech(S.Infinity) == S.ComplexInfinity\n    assert asech(-S.Infinity) == S.ComplexInfinity\n    assert asech(S.ComplexInfinity) == S.ComplexInfinity\n\n"], "sample_310": ["    def test_view_index_with_empty_namespace(self):\n        url_resolver = get_resolver(get_urlconf())\n        view_functions = extract_views_from_urlpatterns(url_resolver.url_patterns)\n        views = []\n        for (func, regex, namespace, name) in view_functions:\n            views.append({\n                'full_name': get_view_name(func),\n                'url': simplify_regex(regex),\n                'url_name': ':'.join((namespace or []) + (name and [name] or [])),\n                'namespace': ':'.join(namespace or []),\n                'name': name,\n            })\n        for view in views:\n            if view['namespace'] == '':\n                self.assertEqual(view['url_name'], view['name'])\n", "    def test_get_view_func(self):\n        view_func = ViewDetailView._get_view_func('django.contrib.admindocs.views.BaseAdminDocsView')\n        self.assertIsNotNone(view_func)\n", "    def test_empty_url_patterns(self):\n        self.assertEqual(views.extract_views_from_urlpatterns([]), [])\n", "    def test_extract_views_from_urlpatterns(self):\n        class View:\n            pass\n        urlpatterns = [\n            path('path1/', View),\n            path('path2/<int:pk>/', View),\n            path('path3/<str:slug>/', View),\n        ]\n        views = extract_views_from_urlpatterns(urlpatterns)\n        self.assertEqual(len(views), 3)\n        for view, path, namespace, name in views:\n            self.assertEqual(view, View)\n            self.assertIn(path, ['/path1/', '/path2/<int:pk>/', '/path3/<str:slug>/'])\n", "    def test_extract_views_from_urlpatterns(self):\n        from django.urls import path, re_path\n        from django.views import View\n\n        class MyView(View):\n            pass\n\n        urlpatterns = [\n            path('path1/', MyView.as_view()),\n            re_path(r'^re_path1/$', MyView.as_view()),\n            path('path2/<int:pk>/', MyView.as_view()),\n        ]\n\n        extracted_views = extract_views_from_urlpatterns(urlpatterns)\n        self.assertEqual(len(extracted_views), 3)\n        for view in extracted_views:\n            self.assertIsInstance(view[0], callable)\n            self.assertIsInstance(view[1], str)\n            self.assertIsNone(view[2])\n            self.assertIsNone(view[3])\n"], "sample_560": ["def test_legend_get_window_extent():\n    fig, ax = plt.subplots()\n    ax.plot(range(10))\n    leg = ax.legend()\n    fig.canvas.draw()\n    renderer = fig.canvas.get_renderer()\n    assert isinstance(leg.get_window_extent(renderer), mtransforms.Bbox)\n    assert isinstance(leg.get_window_extent(), mtransforms.Bbox)\n", "def test_legend_draggable_outside():\n    fig, axs = plt.subplots(1, 2, figsize=(8, 4), constrained_layout=True)\n    axs[0].plot([1, 2, 3], label='line')\n    leg = fig.legend(loc='outside right', draggable=True)\n    axs[1].plot([3, 2, 1], label='line')\n    fig.canvas.draw()\n    assert fig.get_tightbbox().p1y < leg.get_window_extent().p0y < leg.get_window_extent().p1y\n", "def test_legend_update():\n    # test that legend updates properly\n    fig, ax = plt.subplots()\n    ax.plot(np.arange(10), label='old label')\n    leg = ax.legend()\n    ax.plot(np.arange(10), label='new label')\n    ax.legend()\n    assert len(leg.get_texts()) == 1\n    assert leg.get_texts()[0].get_text() == 'old label'\n    leg.update_default_handler_map({Line2D: mlegend.HandlerLine2D(numpoints=2)})\n    assert len(leg.legend_handles) == 2\n    assert leg.legend_handles[0].get_numpoints() == 2\n", "def test_legend_update():\n    # test that updating the legend changes the legend itself\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='test')\n    leg = ax.legend()\n    leg.set_title('new title')\n    assert leg.get_title().get_text() == 'new title'\n\n    leg.set_frame_on(False)\n    assert not leg.get_frame_on()\n\n    leg.set_frame_on(True)\n    assert leg.get_frame_on()\n\n    leg.set_bbox_to_anchor((1.1, 0.5))\n    assert leg.get_bbox_to_anchor() == (1.1, 0.5)\n", "def test_legend_custom_draggable(fig_test, fig_ref):\n    fig_test, ax_test = plt.subplots()\n    fig_ref, ax_ref = plt.subplots()\n\n    x = np.linspace(0, 1, 100)\n    y1 = x\n    y2 = 2*x\n\n    ax_test.plot(x, y1, label='Line 1')\n    ax_test.plot(x, y2, label='Line 2')\n\n    ax_ref.plot(x, y1, label='Line 1')\n    ax_ref.plot(x, y2, label='Line 2')\n\n    leg_test = ax_test.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n                              title='Legend', fancybox=True, shadow=True,\n                              frameon=True, framealpha=0.5,\n                              draggable=True, mode='expand')\n\n    leg_ref = ax_ref.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n                            title='Legend', fancybox=True, shadow=True,\n                            frameon=True, framealpha=0.5,\n                            draggable=False, mode='expand')\n\n    assert leg_test.get_draggable() is True\n    assert leg_ref.get_draggable() is False\n"], "sample_215": ["    def test_cleanses_exception_value(self):\n        request = self.rf.get('/test_view/')\n        exc_type = ValueError\n        exc_value = ValueError(mark_safe('<p>Exception value</p>'))\n        tb = None\n        response = technical_500_response(request, exc_type, exc_value, tb)\n        self.assertInHTML('<pre class=\"exception_value\">&lt;p&gt;Exception value&lt;/p&gt;</pre>', response.content)\n", "    def test_500_response(self):\n        with self.settings(DEBUG=True):\n            try:\n                request = self.rf.get('/test_view/')\n                raise RuntimeError(\"Can't find my keys\")\n            except RuntimeError:\n                exc_type, exc_value, tb = sys.exc_info()\n                response = technical_500_response(request, exc_type, exc_value, tb)\n            self.assertEqual(response.status_code, 500)\n            self.assertContains(response, 'RuntimeError at /test_view/', status_code=500)\n            self.assertContains(response, \"Can't find my keys\", status_code=500)\n            self.assertContains(response, 'Request Method:', status_code=500)\n            self.assertContains(response, 'Request URL:', status_code=500)\n            self.assertContains(response, 'Exception Type:', status_code=500)\n            self.assertContains(response, 'Exception Value:', status_code=500)\n            self.assertContains(response, 'Traceback ', status_code=500)\n            self.assertContains(response, 'Request information:', status_code=500)\n", "    def test_get_safe_settings(self):\n        settings_dict = {\n            'SOME_SETTING': 'some_value',\n            'API_KEY': 'api_key_value',\n            'SECRET_PASSWORD': 'secret_password_value',\n            'NORMAL_SETTING': 'normal_value'\n        }\n\n        with self.settings(**settings_dict):\n            reporter_filter = SafeExceptionReporterFilter()\n            safe_settings = reporter_filter.get_safe_settings()\n\n            self.assertEqual(safe_settings['SOME_SETTING'], 'some_value')\n            self.assertEqual(safe_settings['API_KEY'], reporter_filter.cleansed_substitute)\n            self.assertEqual(safe_settings['SECRET_PASSWORD'], reporter_filter.cleansed_substitute)\n            self.assertEqual(safe_settings['NORMAL_SETTING'], 'normal_value')\n", "    def test_cleanse_special_types_callable(self):\n        reporter_filter = SafeExceptionReporterFilter()\n            return 'callable'\n        self.assertEqual(reporter_filter.cleanse_special_types(None, func), CallableSettingWrapper(func))\n", "    def test_technical_404(self):\n        urlconf = \"django.urls\"\n        request_path = \"/non_existent_url\"\n        exception = Http404()\n        exception.args = ({\"path\": request_path},)\n        tried = (\n            (r\"^$\"),\n            (r\"^non_existent_url/$\"),\n        )\n        template = DEBUG_ENGINE.from_string(\n            \"\"\"\n                {% if urlconf %}\n                    <p>URLconf: {{ urlconf }}</p>\n                {% endif %}\n                <p>URLconf: {{ root_urlconf }}</p>\n                <p>Request path: {{ request_path }}</p>\n                <p>URL patterns tried:\n                    <ul>\n                    {% for pattern in urlpatterns %}\n                        <li>\n                            <code>{{ pattern }}</code>\n                            {% if pattern.name %}\n                                [name='{{ pattern.name }}']\n                            {% endif %}\n                        </li>\n                    {% endfor %}\n                    </ul>\n                </p>\n                <p>Exception: {{ reason }}</p>\n                <p>Raising view: {{ raising_view_name }}</p>\n                <p>Settings: {{ settings }}</p>\n            \"\"\"\n        )\n        c = Context({\n            'urlconf': urlconf,\n            'root_urlconf': urlconf,\n            'request_path': request_path,\n            'urlpatterns': tried,\n            'reason': str(exception),\n            'raising_view_name': None,\n            'settings': {},\n        })\n        response = technical_404_response(None, exception)\n        self.assertHTMLEqual(response.content, template.render(c))\n"], "sample_38": ["def test_change_crpix():\n    \"\"\"\n    Issue #7450\n    \"\"\"\n    header = get_pkg_data_contents(\n        'data/irac_sip.hdr', encoding='binary')\n    w = wcs.WCS(header)\n    crpix1 = w.wcs.crpix\n    w.wcs.crpix = crpix1 + 1\n    assert_array_almost_equal(w.wcs.crpix, crpix1 + 1)\n", "def test_inconsistent_unit():\n    \"\"\"\n    Test issue #8492\n    \"\"\"\n    header = fits.Header([\n        ('CTYPE1', 'RA---TAN'),\n        ('CTYPE2', 'DEC--TAN'),\n        ('CUNIT1', 'deg'),\n        ('CUNIT2', 'deg'),\n        ('CRVAL1', 1.0),\n        ('CRVAL2', 1.0),\n        ('CRPIX1', 1.0),\n        ('CRPIX2', 1.0),\n        ('CDELT1', 1.0),\n        ('CDELT2', 1.0),\n        ('PV1_1', 1.0),\n        ('PV1_2', 0.0),\n        ('PV2_1', 0.0),\n        ('PV2_2', 1.0),\n        ('PV2_3', 1.0),\n    ])\n    w = wcs.WCS(header)\n    assert_array_almost_equal(w.wcs.cunit, ['deg', 'deg'])\n", "def test_sip_foc2pix():\n    \"\"\"\n    Issue #8309\n    \"\"\"\n    w = wcs.WCS(naxis=2)\n    w.wcs.crpix = [2048, 1024]\n    w.sip = wcs.Sip(np.array([[0]]), np.array([[0]]),\n                    np.array([[0]]), np.array([[0]]),\n                    [2048, 1024])\n    foc = [0, 0]\n    pix = w.sip_foc2pix(foc[0], foc[1], 0)\n    assert np.allclose(pix, foc)\n\n    # Test with numpy arrays\n    foc = np.array([0, 0])\n    pix = w.sip_foc2pix(foc, 0)\n    assert np.allclose(pix, foc)\n\n    # Test with list\n    foc = [0, 0]\n    pix = w.sip_foc2pix(foc, 0)\n    assert np.allclose(pix, foc)\n", "def test_sip_roundtrip():\n    \"\"\"\n    Test that creating a WCS object, writing it to a FITS header, and then\n    reading the FITS header back in creates the same WCS object\n    \"\"\"\n    w = wcs.WCS(naxis=2)\n\n    # Create a SIP object with a non-trivial polynomial\n    a = np.zeros((3, 3))\n    a[0, 0] = 1.0\n    a[1, 1] = 1.0\n    a[1, 0] = 0.5\n    a[2, 1] = 0.2\n\n    w.sip = wcs.Sip(a, None, None, None, (1.0, 1.0))\n\n    # Write the WCS to a FITS header\n    header = w.to_header()\n\n    # Read the FITS header back into a new WCS object\n    w_new = wcs.WCS(header)\n\n    # Check that the two WCS objects are the same\n    assert np.allclose(w.wcs.crval, w_new.wcs.crval)\n    assert np.allclose(w.wcs.crpix, w_new.wcs.crpix)\n    assert np.allclose(w.wcs.cdelt, w_new.wcs.cdelt)\n    assert np.allclose(w.wcs.ctype, w_new.wcs.ctype)\n\n    assert w.sip.a_order == w_new.sip.a_order\n    assert w.sip.b_order == w_new.sip.b_order\n    assert w.sip.ap_order == w_new.sip.ap_order\n    assert w.sip.bp_order == w_new.sip.bp_order\n\n    assert np.allclose(w.sip.crpix, w_new.sip.crpix)\n\n    assert np.allclose(w.sip.a, w_new.sip.a)\n    assert np.allclose(w.sip.b, w_new.sip.b)\n    assert np.allclose(w.sip.ap, w_new.sip.ap)\n    assert np.allclose(w.sip.bp, w_new.sip.bp)\n", "def test_sip_with_single_order():\n    \"\"\"\n    Test SIP distortion with a single order.\n\n    This test was added because a single order SIP was not being tested.\n    \"\"\"\n    header = fits.Header()\n    header['CTYPE1'] = 'RA---SIP'\n    header['CTYPE2'] = 'DEC--SIP'\n    header['CRPIX1'] = 2048\n    header['CRPIX2'] = 1026\n    header['CRVAL1'] = 3.185790700000e+02\n    header['CRVAL2'] = 4.388538000000e+01\n    header['CDELT1'] = -2.386e-05\n    header['CDELT2'] = 2.386e-05\n    header['A_ORDER'] = 1\n    header['A_0_0'] = 0\n    header['A_0_1'] = 0\n    header['A_1_0'] = 6.35e-08\n    header['A_1_1'] = -1.33e-08\n    header['B_ORDER'] = 1\n    header['B_0_0'] = 0\n    header['B_0_1'] = 0\n    header['B_1_0'] = -1.33e-08\n    header['B_1_1'] = 6.35e-08\n    w = wcs.WCS(header)\n    x, y = w.wcs_pix2world(2000, 1024, 0)\n    assert_allclose(x, 3.185791134e+02, atol=1e-6)\n    assert_allclose(y, 4.388537718e+01, atol=1e-6)\n"], "sample_144": ["def test_model_inheritance_delete(self):\n    # Test that deleting a child model deletes the parent models.\n    italian_restaurant = ItalianRestaurant.objects.create(\n        name=\"Guido's House of Pasta\",\n        address='944 W. Fullerton',\n        serves_hot_dogs=True,\n        serves_pizza=False,\n        serves_gnocchi=True,\n    )\n    italian_restaurant.delete()\n    with self.assertRaises(Place.DoesNotExist):\n        Place.objects.get(pk=italian_restaurant.pk)\n    with self.assertRaises(Restaurant.DoesNotExist):\n        Restaurant.objects.get(pk=italian_restaurant.pk)\n    with self.assertRaises(ItalianRestaurant.DoesNotExist):\n        ItalianRestaurant.objects.get(pk=italian_restaurant.pk)\n\n    # Test that deleting a parent model deletes the child models.\n    italian_restaurant = ItalianRestaurant.objects.create(\n        name=\"Guido's House of Pasta\",\n        address='944 W. Fullerton',\n        serves_hot_dogs=True,\n        serves_pizza=False,\n        serves_gnocchi=True,\n    )\n    restaurant = italian_restaurant.restaurant_ptr\n    restaurant.delete()\n    with self.assertRaises(Place.DoesNotExist):\n        Place.objects.get(pk=italian_restaurant.pk)\n    with self.assertRaises(Restaurant.DoesNotExist):\n        Restaurant.objects.get(pk=italian_restaurant.pk)\n    with self.assertRaises(ItalianRestaurant.DoesNotExist):\n        ItalianRestaurant.objects.get(pk=italian_restaurant.pk)\n\n    # Test that deleting a grandparent model deletes the child and grandchild models.\n    italian_restaurant = ItalianRestaurant.objects.create(\n        name=\"Guido's House of Pasta\",\n        address='944 W. Fullerton',\n        serves_hot_dogs=True,\n        serves_pizza=False,\n        serves_gnocchi=True,\n    )\n    place = italian_restaurant.place_ptr\n    place.delete()\n    with self.assertRaises(Place.DoesNotExist):\n        Place.objects.get(pk=italian_restaurant.pk)\n    with self.assertRaises(Restaurant.DoesNotExist):\n        Restaurant.objects.get(pk=italian_restaurant.pk)\n    with self.assertRaises(ItalianRestaurant.DoesNotExist):\n        ItalianRestaurant.objects.get(pk=italian_restaurant.pk)\n", "def test_abstract_model_with_unique_together(self):\n    \"\"\"\n    Regression test for #26061\n    Make sure that abstract models with unique_together are properly handled.\n    \"\"\"\n    # Create a Profile\n    p = Profile.objects.create(username='john')\n    # Try to create another Profile with the same username\n    with self.assertRaises(IntegrityError):\n        Profile.objects.create(username='john')\n", "    def test_save_base_with_deferred_fields(self):\n        \"\"\"\n        Regression test for saving a model with deferred fields.\n        \"\"\"\n        # Create a new instance with some deferred fields.\n        italian_restaurant = ItalianRestaurant.objects.create(\n            name=\"Guido's House of Pasta\",\n            address='944 W. Fullerton',\n            serves_hot_dogs=True,\n            serves_pizza=False,\n        )\n\n        # Retrieve the instance with some fields deferred.\n        italian_restaurant_deferred = ItalianRestaurant.objects.defer('serves_gnocchi').get(pk=italian_restaurant.pk)\n\n        # Update the instance and save it with the deferred fields.\n        italian_restaurant_deferred.serves_hot_dogs = False\n        italian_restaurant_deferred.save_base()\n\n        # The deferred fields should not be updated in the database.\n        italian_restaurant_retrieved = ItalianRestaurant.objects.get(pk=italian_restaurant.pk)\n        self.assertEqual(italian_restaurant_retrieved.serves_gnocchi, italian_restaurant.serves_gnocchi)\n\n        # But the non-deferred fields should be updated.\n        self.assertEqual(italian_restaurant_retrieved.serves_hot_dogs, False)\n", "def test_model_validate_unique_check(self):\n    \"\"\"\n    Regression test for #9999 (placeholder)\n    Make sure validate_unique() works correctly with inherited fields.\n    \"\"\"\n    # Create a User instance.\n    User.objects.create(username=\"test_user\")\n\n    # Create a Profile instance with the same username as the User instance.\n    with self.assertRaises(ValidationError):\n        Profile.objects.create(username=\"test_user\")\n\n    # Create a Profile instance with a different username.\n    Profile.objects.create(username=\"different_user\")\n\n    # Create a User instance with the same username as the Profile instance.\n    with self.assertRaises(ValidationError):\n        User.objects.create(username=\"different_user\")\n", "def test_inheritance_refresh_from_db(self):\n    # Regression test for #29963: refresh_from_db() on a model instance\n    # with multi-table inheritance should work correctly.\n    restaurant = Restaurant.objects.create(\n        name=\"Guido's House of Pasta\",\n        address='944 W. Fullerton',\n        serves_hot_dogs=True,\n        serves_pizza=False,\n    )\n    italian_restaurant = ItalianRestaurant.objects.create(\n        restaurant_ptr=restaurant,\n        serves_gnocchi=True,\n    )\n    italian_restaurant.refresh_from_db()\n    self.assertEqual(italian_restaurant.name, \"Guido's House of Pasta\")\n    self.assertEqual(italian_restaurant.address, '944 W. Fullerton')\n    self.assertEqual(italian_restaurant.serves_hot_dogs, True)\n    self.assertEqual(italian_restaurant.serves_pizza, False)\n    self.assertEqual(italian_restaurant.serves_gnocchi, True)\n\n    # Test that refresh_from_db() doesn't reset related objects.\n    supplier = Supplier.objects.create(restaurant=restaurant)\n    italian_restaurant.refresh_from_db()\n    self.assertEqual(italian_restaurant.restaurant.supplier_set.all()[0], supplier)\n\n    # Test that refresh_from_db() doesn't reset attributes that aren't\n    # database fields.\n    italian_restaurant._custom_attribute = 'foo'\n    italian_restaurant.refresh_from_db()\n    self.assertEqual(italian_restaurant._custom_attribute, 'foo')\n"], "sample_920": ["    def test_napoleon_custom_sections(self):\n        config = Config()\n        self.assertIsNone(config.napoleon_custom_sections)\n\n        config = Config(napoleon_custom_sections=['see_also'])\n        self.assertEqual(config.napoleon_custom_sections, ['see_also'])\n\n        config = Config(napoleon_custom_sections=[('see_also', 'hint')])\n        self.assertEqual(config.napoleon_custom_sections, [('see_also', 'hint')])\n\n        config = Config(napoleon_custom_sections=['see_also', ('see_also', 'hint')])\n        self.assertEqual(config.napoleon_custom_sections, ['see_also', ('see_also', 'hint')])\n", "    def test_parse_attribute_docstring(self):\n        class ClassWithAttribute:\n            attr = 42\n            \"\"\"Attribute description\"\"\"\n\n        docstring = ClassWithAttribute.attr.__doc__\n        config = Config()\n        actual = str(GoogleDocstring(docstring, config, app=None, what='attribute', name='attr', obj=ClassWithAttribute.attr))\n        expected = \"\"\"Attribute description\n", "    def test_description_after_empty_line(self):\n        docstring = \"\"\"", "    def test_custom_sections(self):\n        docstrings = ((\"\"\"\\", "    def test_long_lines(self):\n        config = Config(\n            napoleon_use_param=True,\n            napoleon_use_rtype=True,\n            napoleon_use_keyword=False\n        )\n        for docstring, expected in self.docstrings:\n            actual = str(GoogleDocstring(dedent(docstring), config))\n            expected = dedent(expected)\n            self.assertEqual(expected, actual)\n"], "sample_538": ["def test_bbox_padded():\n    b1 = mtransforms.Bbox([[0, 0], [1, 1]])\n    b2 = b1.padded(0.1)\n    assert_bbox_eq(b2, mtransforms.Bbox.from_extents(-0.1, -0.1, 1.1, 1.1))\n    b3 = b1.padded(0.1, 0.2)\n    assert_bbox_eq(b3, mtransforms.Bbox.from_extents(-0.1, -0.2, 1.1, 1.2))\n", "def test_transformed_bbox():\n    bbox = mtransforms.Bbox.from_bounds(0, 0, 10, 10)\n    transform = mtransforms.Affine2D().rotate(np.pi / 2)\n\n    transformed_bbox = mtransforms.TransformedBbox(bbox, transform)\n\n    assert_array_almost_equal(transformed_bbox.get_points(), bbox.get_points())\n\n    transform.rotate(np.pi / 2)\n\n    assert_array_almost_equal(transformed_bbox.get_points(),\n                             [[-10, 0], [0, 10]])\n", "def test_transformed_bbox():\n    # Test that transforming a Bbox correctly updates its points.\n    bbox = mtransforms.Bbox([[0, 0], [2, 2]])\n    trans = mtransforms.Affine2D().scale(2, 3)\n    t_bbox = mtransforms.TransformedBbox(bbox, trans)\n    assert_array_equal(t_bbox.get_points(), [[0, 0], [4, 6]])\n\n    # Test that changing the child Bbox triggers an update.\n    bbox.set_points([[1, 1], [3, 3]])\n    assert_array_equal(t_bbox.get_points(), [[2, 3], [6, 9]])\n\n    # Test that changing the transform triggers an update.\n    trans.scale(2, 2)\n    assert_array_equal(t_bbox.get_points(), [[4, 6], [12, 18]])\n", "def test_affine2d_rotated_path():\n    \"\"\"Test that an affine transform rotates the path correctly.\"\"\"\n    t = mtransforms.Affine2D().rotate(np.pi/4)\n    p = mpatches.Wedge([0, 0], 1, 0, 90)\n    pp = t.transform_path(p.get_path())\n    expected = [[-0.5, -0.5], [0.5, 0.5], [0.5, -0.5]]\n    assert_array_almost_equal(pp.vertices, expected)\n", "def test_affine_delta_transform():\n    # Check that AffineDeltaTransform does not pass through offset components\n    # of the transform matrix\n    transform = mtransforms.Affine2D().translate(1, 2)\n    delta_transform = mtransforms.AffineDeltaTransform(transform)\n    expected_matrix = np.array([[1, 0, 0],\n                                [0, 1, 0],\n                                [0, 0, 1]])\n    assert_array_equal(delta_transform.get_matrix(), expected_matrix)\n\n    # Check that AffineDeltaTransform correctly transforms points\n    points = np.array([[0, 0], [1, 1]])\n    expected_transformed_points = np.array([[0, 0], [1, 1]])\n    assert_array_equal(delta_transform.transform(points), expected_transformed_points)\n\n    # Check that AffineDeltaTransform does not pass through offset components\n    # of the transform matrix when transforming points\n    points = np.array([[0, 0], [1, 1]])\n    expected_transformed_points = np.array([[0, 0], [1, 1]])\n    assert_array_equal(delta_transform.transform(points), expected_transformed_points)\n\n    # Check that AffineDeltaTransform does not affect the result of the\n    # underlying transform when transforming points\n    points = np.array([[0, 0], [1, 1]])\n    expected_transformed_points = np.array([[1, 2], [2, 3]])\n    assert_array_equal(transform.transform(points), expected_transformed_points)\n"], "sample_680": ["def test_xfail_imperative_with_run_keyword(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n            pytest.xfail(run=True, reason=\"hello\")\n    \"\"\"\n    )\n    result = testdir.runpytest(p)\n    result.stdout.fnmatch_lines([\"*1 xfailed*\"])\n    result = testdir.runpytest(p, \"-rx\")\n    result.stdout.fnmatch_lines([\"*XFAIL*test_this*\", \"*reason:*hello*\"])\n    result = testdir.runpytest(p, \"--runxfail\")\n    result.stdout.fnmatch_lines([\"*1 pass*\"])\n", "def test_xfail_with_invalid_run_value(testdir):\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(run=\"hello\", reason=\"nope\")\n            assert 1\n    \"\"\"\n    )\n    with pytest.raises(pytest.fail.Exception) as excinfo:\n        evaluate_xfail_marks(item)\n    assert excinfo.value.msg is not None\n    assert (\n        \"\"\"Error evaluating 'xfail': 'run' parameter must be a boolean (got 'hello')\"\"\"\n        in excinfo.value.msg\n    )\n", "def test_xfail_with_multiple_conditions_and_raises(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(condition=False, reason='unsupported feature', raises=TypeError)\n            raise ValueError(\"Invalid value\")\n    \"\"\"\n    )\n    result = testdir.runpytest(p, \"-rxX\")\n    result.stdout.fnmatch_lines([\"*1 failed*\"])\n    assert result.ret == 1\n", "def test_xfail_strict_not_run(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.xfail(strict=True, reason='hello')\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*1 failed*\",\n            \"XPASS(strict) hello\",\n        ]\n    )\n    assert result.ret == 1\n", "def test_xfail_in_setup_with_strict(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            pytest.xfail(\"hello\", strict=True)\n\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest(p)\n    result.stdout.fnmatch_lines(\n        [\n            \"*def test_func*\",\n            \"*strictly xfailed*hello*\",\n        ]\n    )\n    assert result.ret == 0\n"], "sample_399": ["def test_aggregation_subquery_annotation_exists_with_default(self):\n    \"\"\"\n    Subquery annotations with a default are excluded from the GROUP BY if\n    they are not explicitly grouped against.\n    \"\"\"\n    latest_book_pubdate_qs = (\n        Book.objects.filter(publisher=OuterRef(\"pk\"))\n        .order_by(\"-pubdate\")\n        .values(\"pubdate\")[:1]\n    )\n    publisher_qs = Publisher.objects.annotate(\n        latest_book_pubdate=Subquery(latest_book_pubdate_qs, default=timezone.now()),\n    ).annotate(count=Count(\"book\"))\n    with self.assertNumQueries(1) as ctx:\n        list(publisher_qs)\n    self.assertEqual(ctx[0][\"sql\"].count(\"SELECT\"), 2)\n    # The GROUP BY should not be by alias either.\n    self.assertEqual(ctx[0][\"sql\"].lower().count(\"latest_book_pubdate\"), 1)\n", "def test_aggregation_outerref_subquery(self):\n    subquery = Book.objects.filter(publisher=OuterRef(\"pk\")).values(\"price\")\n    publisher_qs = Publisher.objects.annotate(price=Subquery(subquery))\n    with self.assertNumQueries(1):\n        list(publisher_qs)\n    # The GROUP BY should not be by alias either.\n    self.assertEqual(\n        connection.queries[-1][\"sql\"].lower().count(\"price\"), 1\n    )\n", "def test_distinct_with_aggregation(self):\n    \"\"\"Regression test for #32466: DISTINCT ON with aggregations.\"\"\"\n    result = (\n        Book.objects.values(\"rating\")\n        .annotate(num_authors=Count(\"authors__id\"))\n        .order_by(\"rating\", \"num_authors\")\n        .distinct(\"rating\")\n        .values(\"rating\", \"num_authors\")\n    )\n    self.assertEqual(\n        list(result),\n        [\n            {\"rating\": 3.0, \"num_authors\": 1},\n            {\"rating\": 4.0, \"num_authors\": 3},\n            {\"rating\": 4.5, \"num_authors\": 2},\n            {\"rating\": 5.0, \"num_authors\": 1},\n        ],\n    )\n", "def test_aggregation_with_distinct_on_aggregate_expression(self):\n    class MyMax(Max):\n            self.template = \"MAX(%(expressions)s)\"\n            super().__init__(*expressions, **extra)\n            self.distinct = True\n\n    with self.assertRaisesMessage(\n        TypeError, \"Cannot set distinct to True on a MyMax expression.\"\n    ):\n        Book.objects.aggregate(value=MyMax(\"price\"))\n", "def test_aggregate_default_compatibility(self):\n    # Counting distinct non-nullable fields returns an integer.\n    result = Author.objects.aggregate(\n        value=Count(\"age\", distinct=True, default=\"unknown\")\n    )\n    self.assertEqual(result[\"value\"], 9)\n\n    # Counting non-distinct non-nullable fields returns an integer.\n    result = Author.objects.aggregate(\n        value=Count(\"age\", default=\"unknown\"),\n    )\n    self.assertEqual(result[\"value\"], 9)\n\n    # Summing non-nullable integer fields returns an integer.\n    result = Author.objects.aggregate(\n        value=Sum(\"age\", default=\"unknown\"),\n    )\n    self.assertEqual(result[\"value\"], 337)\n\n    # Summing non-nullable decimal fields returns a Decimal.\n    result = Book.objects.aggregate(\n        value=Sum(\"price\", default=\"unknown\"),\n    )\n    self.assertIsInstance(result[\"value\"], Decimal)\n\n    # Summing distinct non-nullable integer fields returns an integer.\n    result = Author.objects.aggregate(\n        value=Sum(\"age\", distinct=True, default=\"unknown\"),\n    )\n    self.assertEqual(result[\"value\"], 337)\n\n    # Summing distinct non-nullable decimal fields returns a Decimal.\n    result = Book.objects.aggregate(\n        value=Sum(\"price\", distinct=True, default=\"unknown\"),\n    )\n    self.assertIsInstance(result[\"value\"], Decimal)\n\n    # Avg of non-nullable integer fields returns a float.\n    result = Author.objects.aggregate(\n        value=Avg(\"age\", default=\"unknown\"),\n    )\n    self.assertIsInstance(result[\"value\"], float)\n\n    # Avg of non-nullable decimal fields returns a Decimal.\n    result = Book.objects.aggregate(\n        value=Avg(\"price\", default=\"unknown\"),\n    )\n    self.assertIsInstance(result[\"value\"], Decimal)\n\n    # Avg of distinct non-nullable integer fields returns a float.\n    result = Author.objects.aggregate(\n        value=Avg(\"age\", distinct=True, default=\"unknown\"),\n    )\n    self.assertIsInstance(result[\"value\"], float)\n\n    # Avg of distinct non-nullable decimal fields returns a Decimal.\n    result = Book.objects.aggregate(\n        value=Avg(\"price\", distinct=True, default=\"unknown\"),\n    )\n    self.assertIsInstance(result[\"value\"], Decimal)\n\n    # Min of non-nullable integer fields returns an integer.\n    result = Author.objects.aggregate(\n        value=Min(\"age\", default=\"unknown\"),\n    )\n    self.assertEqual(result[\"value\"], 25)\n\n    # Min of non-nullable decimal fields returns a Decimal"], "sample_354": ["    def test_not_running_in_tty(self):\n        \"\"\"\n        If the command is not called from a TTY, it should be skipped and a\n        message should be displayed (#7423).\n        \"\"\"\n        new_io = StringIO()\n        class FakeStdin:\n            \"\"\"A fake stdin object that has isatty() return False.\"\"\"\n                return False\n\n        call_command(\n            \"createsuperuser\",\n            stdin=FakeStdin(),\n            stdout=new_io,\n            interactive=True,\n        )\n\n        self.assertEqual(User._default_manager.count(), 0)\n        self.assertIn(\"Superuser creation skipped\", new_io.getvalue())\n", "    def test_fields_with_fk_interactive_invalid_foreign_key(self):\n        new_io = StringIO()\n        email = Email.objects.create(email='mymail@gmail.com')\n        group = Group.objects.create(name='mygroup')\n        entered_values = [group.pk + 1, group.pk]\n\n            return str(entered_values.pop())\n\n        @mock_inputs({\n            'password': 'nopasswd',\n            'Username (Email.id): ': email.pk,\n            'Email (Email.email): ': email.email,\n            'Group (Group.id): ': return_values,\n        })\n            with self.assertRaisesMessage(CommandError, f\"group instance with id {group.pk + 1} does not exist.\"):\n                call_command(\n                    'createsuperuser',\n                    interactive=True,\n                    stdin=MockTTY(),\n                    stdout=new_io,\n                    stderr=new_io,\n                )\n\n            self.assertEqual(\n                new_io.getvalue().strip(),\n                \"Superuser created successfully.\"\n            )\n\n        test(self)\n", "    def test_required_field_error_message(self):\n        \"\"\"\n        Error messages for required fields should include the field's verbose name.\n        \"\"\"\n        with self.assertRaisesMessage(CommandError, \"Error: First name cannot be blank.\") as cm:\n            call_command('createsuperuser', interactive=False, stdout=self.stdout)\n\n        self.assertIn(\n            'You must use --first_name with --noinput.',\n            str(cm.exception),\n            \"The error message should include the field's verbose name.\"\n        )\n", "    def test_get_input_data_with_foreign_key(self):\n        \"\"\"\n        Ensure get_input_data handles foreign key fields properly.\n        \"\"\"\n        new_io = StringIO()\n        group = Group.objects.create(name='mygroup')\n        email = Email.objects.create(email='mymail@gmail.com')\n        field = CustomUserWithFK._meta.get_field('group')\n\n        # Test valid input\n        raw_value = str(group.pk)\n        val = CustomUserWithFK.Command().get_input_data(field, 'Group (Group.id): ', default=None)\n        self.assertEqual(val, group.pk)\n\n        # Test invalid input\n        raw_value = 'nonexistent'\n        val = CustomUserWithFK.Command().get_input_data(field, 'Group (Group.id): ', default=None)\n        self.assertIsNone(val)\n\n        # Test blank input with default\n        raw_value = ''\n        default_group = Group.objects.create(name='mydefaultgroup')\n        val = CustomUserWithFK.Command().get_input_data(field, 'Group (Group.id): ', default=default_group.pk)\n        self.assertEqual(val, default_group.pk)\n", "    def test_error_handling_during_create_superuser(self):\n        # Simulate an error during user creation.\n        old_create_superuser = CustomUser.objects.create_superuser\n        CustomUser.objects.create_superuser = mock.Mock(side_effect=Exception('Test exception'))\n\n        new_io = StringIO()\n        with self.assertRaisesMessage(CommandError, 'Error: Test exception'):\n            call_command(\n                'createsuperuser',\n                interactive=False,\n                username='testuser',\n                email='test@example.com',\n                date_of_birth='1970-01-01',\n                first_name='Test',\n                stdout=new_io,\n                stderr=new_io,\n            )\n\n        CustomUser.objects.create_superuser = old_create_superuser\n        self.assertEqual(CustomUser._default_manager.count(), 0)\n"], "sample_1196": ["def test_eval():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert Contains(x, S.EmptySet).eval(x, S.EmptySet) is S.false\n    assert Contains(x, S.UniversalSet).eval(x, S.UniversalSet) is S.true\n    assert Contains(y, FiniteSet(x)).eval(y, FiniteSet(x)) == Contains(y, FiniteSet(x))\n", "def test_eval():\n    x = Symbol('x')\n    assert Contains(x, S.Integers).eval(x, S.Integers) is None\n    assert Contains(2, S.Integers).eval(2, S.Integers) == S.true\n    assert Contains(-2, S.Naturals).eval(-2, S.Naturals) == S.false\n    assert Contains(2, FiniteSet(1, 2)).eval(2, FiniteSet(1, 2)) == S.true\n    assert Contains(3, FiniteSet(1, 2)).eval(3, FiniteSet(1, 2)) == S.false\n", "def test_ne():\n    x = Symbol('x')\n    assert Contains(x, S.EmptySet) is S.false\n    assert Contains(x, S.UniversalSet) is S.true\n    assert Contains(x, FiniteSet(x)) is S.true\n    assert Contains(x, FiniteSet(x, Eq(x, 1))) == Contains(x, FiniteSet(x, Eq(x, 1)))\n", "def test_ne_class():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert Contains(x, S.Integers).func != Ne\n    assert Contains(x, S.Integers).func == Contains\n    assert Contains(x, S.Integers).args == (x, S.Integers)\n    assert Contains(x, S.Integers).lhs == x\n    assert Contains(x, S.Integers).rhs == S.Integers\n", "def test_ne():\n    x = Symbol('x')\n    assert Contains(x, S.EmptySet) == S.false\n    assert Contains(x, FiniteSet(x)) == S.true\n    assert Contains(x, FiniteSet(1)) == S.false\n    assert Contains(x, Interval(0, 1, left_open=True, right_open=True)) != S.true\n    assert Contains(x, S.UniversalSet) == S.true\n"], "sample_1127": ["def test_polycyclic_group():\n    a = Permutation([2, 0, 1])\n    b = Permutation([2, 1, 0])\n    G = PermutationGroup([a, b])\n    assert G.polycyclic_group().order() == G.order()\n    G = AlternatingGroup(5)\n    assert G.polycyclic_group().order() == G.order()\n    G = SymmetricGroup(5)\n    assert G.polycyclic_group().order() == G.order()\n", "def test_polycyclic_group():\n    D = DihedralGroup(10)\n    assert D.polycyclic_group().order() == D.order()\n\n    A = AbelianGroup(2, 2, 5)\n    assert A.polycyclic_group().order() == A.order()\n\n    S = SymmetricGroup(3)\n    assert S.polycyclic_group().order() == S.order()\n", "def test_elementary_to_is_cyclic():\n    a = Permutation([0, 2, 1])\n    G = PermutationGroup([a])\n    assert G.is_elementary(2) == True\n    assert G.is_cyclic == True\n\n    a = Permutation([0, 2, 1])\n    b = Permutation([3, 1, 2, 0])\n    G = PermutationGroup([a, b])\n    assert G.is_elementary(2) == True\n    assert G.is_cyclic == True\n\n    G = SymmetricGroup(4).sylow_subgroup(2)\n    assert G.is_elementary(2) == False\n    assert G.is_cyclic == False\n    H = AlternatingGroup(4).sylow_subgroup(2)\n    assert H.is_elementary(2) == True\n    assert H.is_cyclic == True\n", "def test_transitivity_degree():\n    S = SymmetricGroup(6)\n    assert S.transitivity_degree == 6\n    S = AlternatingGroup(4)\n    assert S.transitivity_degree == 3\n    S = PermutationGroup(Permutation(1, 3), Permutation(1, 2))\n    assert S.transitivity_degree == 3\n    S = PermutationGroup(Permutation(1, 2), Permutation(3, 4))\n    assert S.transitivity_degree == 2\n", "def test_transversal_slp():\n    S = SymmetricGroup(4)\n    base = S.schreier_sims_incremental(base=[0])[0]\n    strong_gens = S.strong_gens\n    basic_transversals, transversal_slps = _orbits_transversals_from_bsgs(\n        base, _distribute_gens_by_base(base, strong_gens), slp=True)\n    transversals = basic_transversals\n    base = [0, 1]\n    for i, orb in enumerate(transversals):\n        for point, trans in orb.items():\n            slp = transversal_slps[i][point]\n            w = S.identity\n            for s in slp:\n                w = strong_gens[s] * w\n            assert w == trans\n"], "sample_426": ["def test_timeuntil_depth_invalid(self):\n    msg = \"depth must be greater than 0.\"\n    with self.assertRaisesMessage(ValueError, msg):\n        timeuntil(self.t, self.t, depth=0)\n", "    def test_timeuntil_with_depth(self):\n        \"\"\"Test timeuntil with depth parameter.\"\"\"\n        t = (\n            self.t\n            + self.oneyear\n            + self.onemonth\n            + self.oneweek\n            + self.oneday\n            + self.onehour\n        )\n        tests = [\n            (t, 1, \"1\\xa0year\"),\n            (t, 2, \"1\\xa0year, 1\\xa0month\"),\n            (t, 3, \"1\\xa0year, 1\\xa0month, 1\\xa0week\"),\n            (t, 4, \"1\\xa0year, 1\\xa0month, 1\\xa0week, 1\\xa0day\"),\n            (t, 5, \"1\\xa0year, 1\\xa0month, 1\\xa0week, 1\\xa0day, 1\\xa0hour\"),\n            (t, 6, \"1\\xa0year, 1\\xa0month, 1\\xa0week, 1\\xa0day, 1\\xa0hour\"),\n            (self.t + self.onehour, 5, \"1\\xa0hour\"),\n            (self.t + (4 * self.oneminute), 3, \"4\\xa0minutes\"),\n            (self.t + self.onehour + self.oneminute, 1, \"1\\xa0hour\"),\n            (self.t + self.oneday + self.onehour, 1, \"1\\xa0day\"),\n            (self.t + self.oneweek + self.oneday, 1, \"1\\xa0week\"),\n            (self.t + self.onemonth + self.oneweek, 1, \"1\\xa0month\"),\n            (self.t + self.oneyear + self.onemonth, 1, \"1\\xa0year\"),\n            (self.t + self.oneyear + self.oneweek + self.oneday, 3, \"1\\xa0year\"),\n        ]\n        for value, depth, expected in tests:\n            with self.subTest():\n                self.assertEqual(timeuntil(value, self.t, depth=depth), expected)\n", "    def test_time_strings_parameter(self):\n        \"\"\"Test the time_strings parameter.\"\"\"\n        custom_time_strings = {\n            \"year\": npgettext_lazy(\"%(num)d year\", \"%(num)d years\", \"num\"),\n            \"month\": npgettext_lazy(\"%(num)d month\", \"%(num)d months\", \"num\"),\n            \"week\": npgettext_lazy(\"%(num)d week\", \"%(num)d weeks\", \"num\"),\n            \"day\": npgettext_lazy(\"%(num)d day\", \"%(num)d days\", \"num\"),\n            \"hour\": npgettext_lazy(\"%(num)d hour\", \"%(num)d hours\", \"num\"),\n            \"minute\": npgettext_lazy(\"%(num)d minute\", \"%(num)d minutes\", \"num\"),\n        }\n        self.assertEqual(\n            timesince(self.t, self.t + self.oneminute, time_strings=custom_time_strings),\n            \"1\\xa0minute\",\n        )\n        self.assertEqual(\n            timesince(self.t, self.t + self.onehour, time_strings=custom_time_strings),\n            \"1\\xa0hour\",\n        )\n        self.assertEqual(\n            timesince(self.t, self.t + self.oneday, time_strings=custom_time_strings),\n            \"1\\xa0day\",\n        )\n        self.assertEqual(\n            timesince(self.t, self.t + self.oneweek, time_strings=custom_time_strings),\n            \"1\\xa0week\",\n        )\n        self.assertEqual(\n            timesince(self.t, self.t + self.onemonth, time_strings=custom_time_strings),\n            \"1\\xa0month\",\n        )\n        self.assertEqual(\n            timesince(self.t, self.t + self.oneyear, time_strings=custom_time_strings),\n            \"1\\xa0year\",\n        )\n", "def test_time_strings_override(self):\n    custom_time_strings = {\n        \"year\": \"anio\",\n        \"month\": \"mes\",\n        \"week\": \"semana\",\n        \"day\": \"d\u00eda\",\n        \"hour\": \"hora\",\n        \"minute\": \"minuto\",\n    }\n    self.assertEqual(\n        timesince(self.t, self.t + self.onemonth, time_strings=custom_time_strings),\n        \"1\\xa0mes\",\n    )\n    self.assertEqual(\n        timesince(\n            self.t,\n            self.t + self.onemonth + self.oneweek,\n            time_strings=custom_time_strings,\n            depth=2,\n        ),\n        \"1\\xa0mes, 1\\xa0semana\",\n    )\n    self.assertEqual(\n        timesince(\n            self.t,\n            self.t + self.onemonth + self.oneweek,\n            time_strings=custom_time_strings,\n            depth=1,\n        ),\n        \"1\\xa0mes\",\n    )\n", "def test_pivot_time_calculation(self):\n    \"\"\"\n    Test pivot time calculation with different months and years.\n    \"\"\"\n    t = datetime.datetime(2022, 1, 1)\n    tests = [\n        (datetime.datetime(2022, 1, 31), 1, 0, 31),\n        (datetime.datetime(2022, 2, 1), 1, 1, 1),\n        (datetime.datetime(2022, 2, 28), 1, 1, 28),\n        (datetime.datetime(2022, 3, 1), 1, 2, 1),\n        (datetime.datetime(2022, 12, 31), 1, 11, 31),\n        (datetime.datetime(2023, 1, 1), 2, 0, 1),\n        (datetime.datetime(2023, 2, 1), 2, 1, 1),\n    ]\n    for value, years, months, day in tests:\n        pivot_year = t.year + years\n        pivot_month = t.month + months\n        if pivot_month > 12:\n            pivot_month -= 12\n            pivot_year += 1\n        pivot = datetime.datetime(pivot_year, pivot_month, min(MONTHS_DAYS[pivot_month - 1], t.day), t.hour, t.minute, t.second)\n        self.assertEqual(pivot.year, value.year)\n        self.assertEqual(pivot.month, value.month)\n        self.assertEqual(pivot.day, day)\n"], "sample_287": ["def test_list_editable_overlaps_with_list_display_links(self):\n    class SongAdmin(admin.ModelAdmin):\n        list_editable = [\"title\"]\n        list_display_links = [\"title\"]\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'list_editable[0]' cannot be in both 'list_editable' and 'list_display_links'.\",\n            obj=SongAdmin,\n            id='admin.E123',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "    def test_check_list_editable_links_conflict(self):\n        class SongAdmin(admin.ModelAdmin):\n            list_display = ['title', 'album']\n            list_editable = ['title']\n            list_display_links = ['title']\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'list_editable[0]' cannot be in both 'list_editable' and 'list_display_links'.\",\n                obj=SongAdmin,\n                id='admin.E123',\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_check_autocomplete_fields(self):\n        class SongAdmin(admin.ModelAdmin):\n            autocomplete_fields = ['album']\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        self.assertEqual(errors, [])\n", "def test_ordering_random(self):\n    class SongAdmin(admin.ModelAdmin):\n        ordering = ['?', 'title']\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'ordering' has the random ordering marker '?', \"\n            \"but contains other fields as well.\",\n            hint='Either remove the \"?\", or remove the other fields.',\n            obj=SongAdmin,\n            id='admin.E032',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_list_filter_works_with_m2m_fields_through_reverse_relationship(self):\n    \"\"\"\n    Ensure list_filter can access ManyToManyField through reverse relationships.\n    \"\"\"\n    class BookAdmin(admin.ModelAdmin):\n        list_filter = ['album__title']\n\n    errors = BookAdmin(Book, AdminSite()).check()\n    self.assertEqual(errors, [])\n"], "sample_1091": ["def test_issue_18189():\n    from sympy.sets.conditionset import ConditionSet\n    result1 = Gt(x*cos(x) - 3*sin(x), 0)\n    assert result1.as_set() == ConditionSet(x, Gt(x*cos(x) - 3*sin(x), 0), Reals)\n\n    result2 = Lt(x**2 + sqrt(x*2) + sin(x), 0)\n    assert result2.as_set() == ConditionSet(x, Lt(x**2 + sqrt(2)*sqrt(x) + sin(x), 0), Reals)\n", "def test_relational_properties():\n    e = Eq(x, y)\n    assert e.reversedsign.reversed == e\n    assert e.reversedsign == Eq(-x, -y)\n    assert e.reversed.reversedsign == e.reversedsign\n    assert e.reversed.reversedsign.reversed == e.reversedsign\n\n    e = Ne(x, y)\n    assert e.reversedsign.reversed == e\n    assert e.reversedsign == Ne(-x, -y)\n    assert e.reversed.reversedsign == e.reversedsign\n    assert e.reversed.reversedsign.reversed == e.reversedsign\n\n    e = Gt(x, y)\n    assert e.reversedsign.reversed == e\n    assert e.reversedsign == Lt(-x, -y)\n    assert e.reversed.reversedsign == e.reversedsign\n    assert e.reversed.reversedsign.reversed == e.reversedsign\n\n    e = Lt(x, y)\n    assert e.reversedsign.reversed == e\n    assert e.reversedsign == Gt(-x, -y)\n    assert e.reversed.reversedsign == e.reversedsign\n    assert e.reversed.reversedsign.reversed == e.reversedsign\n\n    e = Ge(x, y)\n    assert e.reversedsign.reversed == e\n    assert e.reversedsign == Le(-x, -y)\n    assert e.reversed.reversedsign == e.reversedsign\n    assert e.reversed.reversedsign.reversed == e.reversedsign\n\n    e = Le(x, y)\n    assert e.reversedsign.reversed == e\n    assert e.reversedsign == Ge(-x, -y)\n    assert e.reversed.reversedsign == e.reversedsign\n    assert e.reversed.reversedsign.reversed == e.reversedsign\n", "def test_relational_different_domain():\n    # Test relational between numbers of different domain\n    assert rel_check(Float(1, 10), Rational(1, 10))\n    assert rel_check(Rational(1, 10), Float(1, 10))\n", "def test_relational_simplify_with_nested_relational():\n    # Test that relational simplify() does not go into infinite recursion with nested relationals.\n    assert Ge(Ge(x, y), z).simplify() == Ge(x, z)\n    assert Ge(Ge(x, Ge(y, z)), w).simplify() == Ge(x, w)\n    assert Le(Le(x, y), z).simplify() == Le(x, z)\n    assert Le(Le(x, Le(y, z)), w).simplify() == Le(x, w)\n", "def test_relational_canonical():\n    # Test that the `canonical` method of `Relational` is idempotent.\n    for f in [Eq, Ne, Ge, Gt, Le, Lt]:\n        rel = f(x, x**2 + 2*x + 1, evaluate=False)\n        assert rel.canonical.canonical == rel.canonical\n        rel = f(x + 1, x, evaluate=False)\n        assert rel.canonical.canonical == rel.canonical\n        rel = f(x, x + 1, evaluate=False)\n        assert rel.canonical.canonical == rel.canonical\n        rel = f(x**2 + 2*x + 1, x, evaluate=False)\n        assert rel.canonical.canonical == rel.canonical\n"], "sample_656": ["def test_resume_global_capture_after_keyboardinterrupt(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n            raise KeyboardInterrupt()\n            print(\"hello\")\n        \"\"\"\n    )\n    result = testdir.runpytest_subprocess(p)\n    result.stdout.fnmatch_lines(\n        [\n            \"*KeyboardInterrupt*\",\n        ]\n    )\n    assert result.ret == 2\n", "def test_suspend_resume_capturing_during_keyboardinterrupt(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\\\n        import os\n            import os\n            os.write(1, b'hello\\\\n')\n            raise KeyboardInterrupt()\n            os.write(1, b'world\\\\n')\n    \"\"\"\n    )\n    result = testdir.runpytest_subprocess(p)\n    result.stdout.fnmatch_lines(\n        [\n            \"*KeyboardInterrupt*\",\n            \"*Captured stdout*\",\n            \"*hello*\",\n        ]\n    )\n    assert result.ret == 2\n", "def test_capture_fixture_works_with_sys_stdout_encoding_when_capture_disabled(testdir, fixture):\n    testdir.makepyfile(\n        \"\"\"\\\n            {}.disabled()\n            print(\"hello\".encode(sys.stdout.encoding, \"replace\"))\n            {}.disabled()\n            assert {}.readouterr() == ('', '')\n        \"\"\".format(\n            fixture, fixture, fixture, fixture\n        )\n    )\n    reprec = testdir.inline_run()\n    reprec.assertoutcome(passed=1)\n", "def test_keyboardinterrupt_while_capturing(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import os\n        import time\n\n            os.write(1, b'hello\\\\n')\n            time.sleep(1)  # introduce a small delay to increase the likelihood of the KeyboardInterrupt happening while capturing\n            raise KeyboardInterrupt()\n    \"\"\"\n    )\n    result = testdir.runpytest_subprocess(p)\n    result.stdout.fnmatch_lines([\"*KeyboardInterrupt*\", \"*Captured stdout*\"])\n    assert result.ret == 2\n", "def test_capturing_dontreadfrominput_contextmanager_issue3248(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n            with capsys.disabled():\n                import pytest\n                pytest.skip(\"test skipped\")\n    \"\"\"\n    )\n    result = testdir.runpytest_subprocess()\n    assert result.ret == 0\n"], "sample_663": ["def test_ignore_collect_virtualenv_norecursedirs_priority(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        norecursedirs = venv\n        \"\"\"\n    )\n    venv = testdir.mkdir(\"venv\")\n    venv.ensure(\"__init__.py\")\n    testfile = venv.ensure(\"test_invenv.py\")\n    testfile.write(\"def test_hello(): pass\")\n\n    result = testdir.runpytest(\"--collect-in-virtualenv\")\n    result.stdout.fnmatch_lines([\"*collected 0 items*\"])\n    # ...unless the virtualenv is explicitly given on the CLI\n    result = testdir.runpytest(\"--collect-in-virtualenv\", \"venv\")\n    assert \"test_invenv\" in result.stdout.str()\n", "def test_session_exit_status(testdir):\n    \"\"\"Verify that session exit status is correctly set\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n            assert 0\n            assert 0\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--maxfail=1\")\n    assert result.ret == ExitCode.TESTS_FAILED\n    assert \"interrupted\" not in result.stdout.str()\n    assert \"1 failed\" in result.stdout.str()\n\n    result = testdir.runpytest(\"--maxfail=2\")\n    assert result.ret == ExitCode.TESTS_FAILED\n    assert \"interrupted\" not in result.stdout.str()\n    assert \"2 failed\" in result.stdout.str()\n\n    result = testdir.runpytest(\"--maxfail=3\")\n    assert result.ret == ExitCode.TESTS_FAILED\n    assert \"interrupted\" not in result.stdout.str()\n    assert \"2 failed\" in result.stdout.str()\n\n    testdir.makepyfile(\n        \"\"\"\n            raise KeyboardInterrupt\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    assert result.ret == ExitCode.INTERRUPTED\n    assert \"interrupted\" in result.stdout.str()\n", "def test_collect_only_initialpaths(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            pass\n            pass\n    \"\"\"\n    )\n    subdir = testdir.mkdir(\"sub\")\n    p = subdir.join(\"test_file.py\")\n    p.write(\n        textwrap.dedent(\n            \"\"\"\n            pass\n    \"\"\"\n        )\n    )\n    result = testdir.runpytest(\"-v\", \"--collect-only\", \".\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_1*\",\n            \"*test_2*\",\n            \"*sub/test_file.py::test_3*\",\n        ]\n    )\n\n    result = testdir.runpytest(\"-v\", \"--collect-only\", \".\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_1*\",\n            \"*test_2*\",\n            \"*sub/test_file.py::test_3*\",\n        ]\n    )\n\n    result = testdir.runpytest(\"-v\", \"--collect-only\", \".\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_1*\",\n            \"*test_2*\",\n            \"*sub/test_file.py::test_3*\",\n        ]\n    )\n\n    result = testdir.runpytest(\"-v\", \"--collect-only\", \".\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_1*\",\n            \"*test_2*\",\n            \"*sub/test_file.py::test_3*\",\n        ]\n    )\n\n    result = testdir.runpytest(\"-v\", \"--collect-only\", \".\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_1*\",\n            \"*test_2*\",\n            \"*sub/test_file.py::test_3*\",\n        ]\n    )\n\n    result = testdir.runpytest(\"-v\", \"--collect-only\", \".\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_1*\",\n            \"*test_2*\",\n            \"*sub/test_file.py::test_3*\",\n        ]\n    )\n", "def test_collect_rootdir_with_pyargs(testdir, monkeypatch):\n    \"\"\"Test that the rootdir is correctly identified when the --pyargs option is used\"\"\"\n    rootdir = testdir.mkdir(\"root\")\n    sub = rootdir.mkdir(\"sub\")\n    sub.ensure(\"__init__.py\")\n    sub.ensure(\"test_file.py\").write(\"def test_file(): pass\")\n\n    # Check that the rootdir is correctly identified when using --pyargs\n    # and running pytest from the rootdir\n    rootdir.chdir()\n    result = testdir.runpytest(\"--pyargs\", \"sub\")\n    result.stdout.fnmatch_lines([\"sub/test_file.py::test_file PASSED*\", \"*1 passed in*\"])\n\n    # Check that the rootdir is correctly identified when using --pyargs\n    # and running pytest from a subdirectory\n    sub.chdir()\n    result = testdir.runpytest(\"--pyargs\", \"..\")\n    result.stdout.fnmatch_lines([\"sub/test_file.py::test_file PASSED*\", \"*1 passed in*\"])\n\n    # Check that the rootdir is correctly identified when using --pyargs\n    # and running pytest with a specific test file\n    sub.chdir()\n    result = testdir.runpytest(\"--pyargs\", \"test_file.py\")\n    result.stdout.fnmatch_lines([\"test_file.py::test_file PASSED*\", \"*1 passed in*\"])\n\n    # Check that the rootdir is correctly identified when using --pyargs\n    # and running pytest with a specific test file and the --rootdir option\n    sub.chdir()\n    result = testdir.runpytest(\"--pyargs\", \"--rootdir=..\", \"test_file.py\")\n    result.stdout.fnmatch_lines([\"sub/test_file.py::test_file PASSED*\", \"*1 passed in*\"])\n", "    def test_collection_modification_with_session_nodeid(self, testdir):\n        testdir.makeconftest(\n            \"\"\"\n            import pytest\n                session.shouldstop = \"foo\"\n                session.shouldfail = \"bar\"\n                assert len(items) == 1\n                assert items[0].nodeid == 'test_session_collection_modification_with_session_nodeid.py::test_func'\n        \"\"\"\n        )\n        testdir.makepyfile(\n            \"\"\"\n                assert True\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines([\"*1 passed in*\"])\n        assert result.ret == ExitCode.OK\n"], "sample_896": ["def test_non_negative_factorization_beta_loss():\n    # Test that an error is raised when beta_loss <= 0 and X contains zeros.\n    # Test that the output has not NaN values when the input contains zeros.\n    n_samples = 6\n    n_features = 5\n    n_components = 3\n\n    rng = np.random.mtrand.RandomState(42)\n    X = rng.randn(n_samples, n_features)\n    np.clip(X, 0, None, out=X)\n    X_csr = sp.csr_matrix(X)\n\n        W, H, _ = non_negative_factorization(\n            X,\n            init=\"random\",\n            n_components=n_components,\n            solver=\"mu\",\n            beta_loss=beta_loss,\n            random_state=0,\n            max_iter=1000,\n        )\n        assert not np.any(np.isnan(W))\n        assert not np.any(np.isnan(H))\n\n    msg = \"When beta_loss <= 0 and X contains zeros, the solver may diverge.\"\n    for beta_loss in [-0.6, 0.0]:\n        with pytest.raises(ValueError, match=msg):\n            _assert_nmf_no_nan(X, beta_loss)\n        _assert_nmf_no_nan(X + 1e-323, beta_loss)\n\n    for beta_loss in (0.2, 1.0, 1.2, 2.0, 2.5):\n        _assert_nmf_no_nan(X, beta_loss)\n        _assert_nmf_no_nan(X_csr, beta_loss)\n", "def test_nmf_shuffle_parameter(Estimator, solver):\n    # Test that the shuffle parameter has an effect on the results\n    rng = np.random.mtrand.RandomState(42)\n    A = np.abs(rng.randn(100, 50))\n    model1 = Estimator(random_state=0, shuffle=False, **solver)\n    model2 = Estimator(random_state=0, shuffle=True, **solver)\n    W1 = model1.fit_transform(A)\n    W2 = model2.fit_transform(A)\n    assert not np.allclose(W1, W2)\n", "def test_nmf_fit_transform_with_custom_init_zero_value(Estimator):\n    # Test that NMF and MiniBatchNMF can handle a zero value in custom initialization\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((6, 5))\n    W_init = rng.random_sample((6, 2))\n    W_init[0, 0] = 0  # Add a zero value to W_init\n    H_init = rng.random_sample((2, 5))\n    est = Estimator(\n        n_components=2,\n        init=\"custom\",\n        random_state=0,\n        tol=1e-6,\n    )\n    est.fit_transform(X, W=W_init, H=H_init)\n", "def test_beta_divergence_at_zero():\n    # Test _beta_divergence at beta=0 with X having zeros\n    # This is a regression test for an underflow issue\n    n_samples = 6\n    n_features = 5\n    n_components = 3\n\n    rng = np.random.mtrand.RandomState(42)\n    X = rng.randn(n_samples, n_features)\n    X[X < 0.5] = 0\n    X = np.abs(X)\n    W = np.abs(rng.randn(n_samples, n_components))\n    H = np.abs(rng.randn(n_components, n_features))\n\n    # Initialize with a non zero dot product\n    W[0, 0] = 1\n    H[0, 0] = 1\n\n    ref = nmf._beta_divergence(X, W, H, beta=0)\n    assert np.isfinite(ref)\n", "def test_nmf_random_state_reset():\n    # Test that setting `random_state` to None resets the random number generator\n    rng = np.random.RandomState(42)\n    X = np.abs(rng.randn(20, 15))\n    nmf = NMF(random_state=42, tol=1e-2)\n    nmf.fit(X)\n\n    # check that the components learned are different from the ones learned\n    # when random_state is not set to None\n    H1 = nmf.components_\n\n    # reset random number generator\n    nmf.random_state = None\n    nmf.fit(X)\n\n    H2 = nmf.components_\n    assert not np.allclose(H1, H2)\n\n    # set back the random number generator\n    nmf.random_state = 42\n    nmf.fit(X)\n\n    # check that the components are the same as before\n    H3 = nmf.components_\n    assert_allclose(H1, H3)\n"], "sample_1018": ["def test_fcode_Do():\n    x, y, z = symbols('x y z')\n    do = Assignment(x, Do(For(y, Range(0, 10), [Assignment(z, y)]) + 2)\n    expected = (\n        \"      do y = 0, 10\\n\"\n        \"         z = y\\n\"\n        \"      end do\\n\"\n        \"      x = do + 2\"\n    )\n    assert fcode(do) == expected\n", "def test_fcode_Infinity():\n    x = symbols('x')\n    assert fcode(S.Infinity) == '      (huge(0d0) + 1)'\n    assert fcode(S.NegativeInfinity) == '      -(huge(0d0) + 1)'\n    assert fcode(2*S.Infinity) == '      2*(huge(0d0) + 1)'\n    assert fcode(3 + S.Infinity) == '      3 + huge(0d0) + 1'\n    assert fcode(S.Infinity + S.Infinity) == '      (huge(0d0) + 1) + huge(0d0) + 1'\n    assert fcode(S.Infinity*S.Infinity) == '      (huge(0d0) + 1)**2'\n    assert fcode(3*S.Infinity + x) == '      3*(huge(0d0) + 1) + x'\n    assert fcode(3 + 2*S.Infinity + x) == '      3 + 2*(huge(0d0) + 1) + x'\n    assert fcode(x/S.Infinity) == '      x/(huge(0d0) + 1)'\n    assert fcode(S.Infinity**3) == '      (huge(0d0) + 1)**3'\n    assert fcode(S.Infinity**-3) == '      1/((huge(0d0) + 1)**3)'\n", "def test_fcode_Subroutine():\n    x = symbols('x')\n    y = symbols('y')\n    f = implemented_function('f', Lambda(x, x**2))\n    sub = f(y)\n    printer = FCodePrinter({'source_format': 'free'})\n    res = printer.doprint(sub, 'z')\n    expected = (\n        '      z = y**2\\n'\n    )\n    assert res == expected\n\n    printer = FCodePrinter()\n    res = printer.doprint(sub, 'z')\n    expected = (\n        '      z = y**2\\n'\n    )\n    assert res == expected\n\n    sub = implemented_function('f', Lambda(x, x**2), 'elemental')\n    printer = FCodePrinter({'source_format': 'free'})\n    res = printer.doprint(sub, 'z')\n    expected = (\n        '      z = y**2\\n'\n    )\n    assert res == expected\n\n    sub = implemented_function('f', Lambda(x, x**2), 'pure')\n    printer = FCodePrinter({'source_format': 'free'})\n    res = printer.doprint(sub, 'z')\n    expected = (\n        '      z = y**2\\n'\n    )\n    assert res == expected\n\n    sub = implemented_function('f', Lambda(x, x**2), 'pure elemental')\n    printer = FCodePrinter({'source_format': 'free'})\n    res = printer.doprint(sub, 'z')\n    expected = (\n        '      z = y**2\\n'\n    )\n    assert res == expected\n", "def test_fcode_Program():\n    x = symbols('x')\n    prog = FunctionPrototype('my_program', real, (real,))\n    assert fcode(prog) == (\n        \"interface\\n\"\n        \"function my_program(x)\\n\"\n        \"      real*8 :: my_program\\n\"\n        \"      real*8, intent(in) :: x\\n\"\n        \"end function\\n\"\n        \"end interface\"\n    )\n", "def test_fcode_Subroutine():\n    x, y = symbols('x y')\n\n        assert fcode(expr, standard=95, source_format='free', **kwargs) == ref\n\n    subr = FunctionDefinition(\n        ProcedureDefinition(\n            Subroutine(Variable(deduced(x), \"mysub\"), [Variable(deduced(y), \"x\")]),\n            CodeBlock([Assignment(Variable(deduced(x), \"x\"), 42)])\n        )\n    )\n    check(subr, \"subroutine mysub(x)\\n   x = 42\\nend subroutine\\n\")\n\n    subr = FunctionDefinition(\n        ProcedureDefinition(\n            Subroutine(Variable(deduced(x), \"mysub\"), [Variable(deduced(y), \"x\")]),\n            CodeBlock([Assignment(Variable(deduced(x), \"x\"), 42)]),\n            attrs=[elemental, pure]\n        )\n    )\n    check(subr, \"pure elemental subroutine mysub(x)\\n   x = 42\\nend subroutine\\n\")\n\n    subr = FunctionDefinition(\n        ProcedureDefinition(\n            Subroutine(Variable(deduced(x), \"mysub\"), [Variable(deduced(y), \"x\")]),\n            CodeBlock([Assignment(Variable(deduced(x), \"x\"), 42)]),\n            attrs=[elemental]\n        )\n    )\n    check(subr, \"elemental subroutine mysub(x)\\n   x = 42\\nend subroutine\\n\")\n\n    subr = FunctionDefinition(\n        ProcedureDefinition(\n            Subroutine(Variable(deduced(x), \"mysub\"), [Variable(deduced(y), \"x\")]),\n            CodeBlock([Assignment(Variable(deduced(x), \"x\"), 42)]),\n            attrs=[pure]\n        )\n    )\n    check(subr, \"pure subroutine mysub(x)\\n   x = 42\\nend subroutine\\n\")\n\n    subr = FunctionDefinition(\n        ProcedureDefinition(\n            Subroutine(Variable(deduced(x), \"mysub\"), [Variable(deduced(y), \"x\")]),\n            CodeBlock([Assignment(Variable(deduced(x), \"x\"), 42)])\n        )\n    )\n    check(subr, \"subroutine mysub(x)\\n   x = 42\\nend subroutine\\n\", type_aliases={real: float32})\n\n    subr = FunctionDefinition(\n        ProcedureDefinition(\n            FunctionDefinition(\n                ProcedureDefinition(\n                    Subroutine(Variable"], "sample_323": ["def test_migration_plan_with_clean_start(self):\n    \"\"\"\n    Test the migration plan with a clean start.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    plan = executor.migration_plan([(\"migrations\", \"0002_second\")], clean_start=True)\n    self.assertEqual(\n        plan,\n        [\n            (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n            (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False),\n        ],\n    )\n    # Fake-apply all migrations\n    executor.migrate([(\"migrations\", \"0002_second\")], fake=True)\n    # Rebuild the graph to reflect the new DB state\n    executor.loader.build_graph()\n    # Now plan a second time and make sure it's empty\n    plan = executor.migration_plan([(\"migrations\", \"0002_second\")], clean_start=True)\n    self.assertEqual(plan, [])\n    # The resulting state should include applied migrations.\n    state = executor.migrate([(\"migrations\", \"0002_second\")])\n    self.assertIn(('migrations', 'book'), state.models)\n    self.assertIn(('migrations', 'author'), state.models)\n    # Erase all the fake records\n    executor.recorder.record_unapplied(\"migrations\", \"0002_second\")\n    executor.recorder.record_unapplied(\"migrations\", \"0001_initial\")\n\n", "    def test_detect_soft_applied_with_proxy_model(self):\n        \"\"\"\n        Tests detection of initial migrations already having been applied, with\n        a proxy model.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        # Were the tables there before?\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_tribble\")\n        # Run it normally\n        self.assertEqual(\n            executor.migration_plan([(\"migrations\", \"0001_initial\")]),\n            [\n                (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n            ],\n        )\n        executor.migrate([(\"migrations\", \"0001_initial\")])\n        # Are the tables there now?\n        self.assertTableExists(\"migrations_author\")\n        self.assertTableExists(\"migrations_tribble\")\n        # Rebuild the graph to reflect the new DB state\n        executor.loader.build_graph()\n        # Fake-reverse that\n        executor.migrate([(\"migrations\", None)], fake=True)\n        # Are the tables still there?\n        self.assertTableExists(\"migrations_author\")\n        self.assertTableExists(\"migrations_tribble\")\n        # Make sure that was faked\n        # Finally, migrate forwards; this should fake-apply our initial migration\n        executor.loader.build_graph()\n        self.assertEqual(\n            executor.migration_plan([(\"migrations\", \"0001_initial\")]),\n            [\n                (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n            ],\n        )\n        # Applying the migration should raise a database level error\n        # because we haven't given the --fake-initial option\n        with self.assertRaises(DatabaseError):\n            executor.migrate([(\"migrations\", \"0001_initial\")])\n        # Reset the faked state\n        # Allow faking of initial CreateModel operations\n        executor.migrate([(\"migrations\", \"0001_initial\")], fake_initial=True)\n        # And migrate back to clean up the database\n        executor.loader.build_graph()\n        executor.migrate([(\"migrations\", None)])\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_tribble\")\n", "    def test_migration_plan_with_none_target(self):\n        \"\"\"\n        Tests migration plan generation when target is (app, None).\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        # Were the tables there before?\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_tribble\")\n        # Run it normally\n        self.assertEqual(\n            executor.migration_plan([(\"migrations\", None)]),\n            [],\n        )\n        # Are the tables there now?\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_tribble\")\n        # Apply the migration\n        executor.migrate([(\"migrations\", \"0001_initial\")])\n        # Are the tables there now?\n        self.assertTableExists(\"migrations_author\")\n        self.assertTableExists(\"migrations_tribble\")\n        # Rebuild the graph to reflect the new DB state\n        executor.loader.build_graph()\n        # Now try to plan to None again\n        plan = executor.migration_plan([(\"migrations\", None)])\n        self.assertEqual(\n            plan,\n            [\n                (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], True),\n            ],\n        )\n        executor.migrate([(\"migrations\", None)])\n        # Are the tables gone?\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_tribble\")\n", "def test_migrate_applies_migrations_in_atomic_blocks(self):\n    \"\"\"\n    Migrate applies migrations in atomic blocks.\n    \"\"\"\n    recorder = MigrationRecorder(connection)\n    executor = MigrationExecutor(connection)\n    # Fake-apply all migrations\n    executor.migrate([(\"migrations\", \"0002_second\")], fake=True)\n    # Rebuild the graph to reflect the new DB state\n    executor.loader.build_graph()\n    migration = migrations.Migration('0001_initial', 'migrations')\n    migration.atomic = False\n    migration.operations = [\n        migrations.CreateModel(\n            name='TestModel',\n            fields=[\n                ('id', models.AutoField(primary_key=True)),\n            ],\n            options={\n            },\n        ),\n    ]\n    executor.apply_migration(ProjectState(), migration, fake=False)\n    recorder.record_applied(\"migrations\", \"0001_initial\")\n    migration = migrations.Migration('0002_second', 'migrations')\n    migration.atomic = False\n    migration.dependencies = [('migrations', '0001_initial')]\n    migration.operations = [\n        migrations.CreateModel(\n            name='TestModel2',\n            fields=[\n                ('id', models.AutoField(primary_key=True)),\n            ],\n            options={\n            },\n        ),\n    ]\n    executor.apply_migration(ProjectState(), migration, fake=False)\n    recorder.record_applied(\"migrations\", \"0002_second\")\n    self.assertTableExists('migrations_testmodel')\n    self.assertTableExists('migrations_testmodel2')\n    executor.loader.build_graph()\n    executor.migrate([(\"migrations\", None)])\n    self.assertTableNotExists('migrations_testmodel')\n    self.assertTableNotExists('migrations_testmodel2')\n", "def test_detect_soft_applied_empty_database(self):\n    \"\"\"\n    Tests detection of initial migrations already having been applied in a case\n    where the database is empty.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    # Were the tables there before?\n    self.assertTableNotExists(\"migrations_author\")\n    self.assertTableNotExists(\"migrations_book\")\n    # Run it normally\n    self.assertEqual(\n        executor.migration_plan([(\"migrations\", \"0001_initial\")]),\n        [\n            (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n        ],\n    )\n    # Before running the migration, delete the migration record from the\n    # database to simulate an empty database.\n    executor.recorder.delete_applied(\"migrations\", \"0001_initial\")\n    executor.migrate([(\"migrations\", \"0001_initial\")], fake=True)\n    # Are the tables there now?\n    self.assertTableNotExists(\"migrations_author\")\n    self.assertTableNotExists(\"migrations_book\")\n    # Make sure the soft-application detection works\n    migration = executor.loader.get_migration(\"migrations\", \"0001_initial\")\n    self.assertIs(executor.detect_soft_applied(None, migration)[0], False)\n    # Rebuild the graph to reflect the new DB state\n    executor.loader.build_graph()\n    # Make sure the migration is not applied when it's actually not applied\n    executor.migrate([(\"migrations\", \"0001_initial\")])\n    # Are the tables there now?\n    self.assertTableExists(\"migrations_author\")\n    self.assertTableExists(\"migrations_book\")\n    # Rebuild the graph to reflect the new DB state\n    executor.loader.build_graph()\n    # Make sure the migration is detected as applied\n    migration = executor.loader.get_migration(\"migrations\", \"0001_initial\")\n    self.assertIs(executor.detect_soft_applied(None, migration)[0], True)\n    # And migrate back to clean up the database\n    executor.migrate([(\"migrations\", None)])\n    self.assertTableNotExists(\"migrations_author\")\n    self.assertTableNotExists(\"migrations_book\")\n"], "sample_214": ["    def test_validate_with_invalid_encoder(self):\n        field = models.JSONField(encoder=lambda x: x)\n        msg = 'The encoder parameter must be a callable object.'\n        with self.assertRaisesMessage(ValueError, msg):\n            field.encoder = 'encoder'\n", "    def setUpTestData(cls):\n        cls.objs = [\n            NullableJSONModel.objects.create(value={'a': 'b', 'c': 14}),\n            NullableJSONModel.objects.create(value={'k': True, 'l': False}),\n            NullableJSONModel.objects.create(value=[{'m': 'n'}, {'o': 'p'}]),\n            NullableJSONModel.objects.create(value={'q': [1, 2, 3]}),\n        ]\n", "    def test_get_transform(self):\n        field = models.JSONField()\n        self.assertIsInstance(field.get_transform('key'), KeyTransformFactory)\n", "    def test_invalid_json_field_value(self):\n        invalid_json_value = '{\"key\": \"value\"}'  # Incorrectly formatted JSON string\n        with self.assertRaises(DataError):\n            JSONModel.objects.create(value=invalid_json_value)\n", "    def test_encoder_decoder_validation(self):\n        field = models.JSONField(encoder=DjangoJSONEncoder, decoder=CustomJSONDecoder)\n        with self.assertRaisesMessage(ValueError, 'The decoder parameter must be a callable object.'):\n            field = models.JSONField(encoder=DjangoJSONEncoder, decoder=CustomJSONDecoder())\n"], "sample_516": ["def test_get_link_annotation():\n    gc = GraphicsContextPdf(None)\n    quadpoints, rect = _get_coordinates_of_block(10, 10, 100, 50)\n    link_annotation = _get_link_annotation(gc, 10, 10, 100, 50)\n    assert link_annotation['Rect'] == rect\n    assert link_annotation['QuadPoints'] is None\n\n    quadpoints, rect = _get_coordinates_of_block(10, 10, 100, 50, 45)\n    link_annotation = _get_link_annotation(gc, 10, 10, 100, 50, 45)\n    assert link_annotation['Rect'] == rect\n    assert link_annotation['QuadPoints'] == quadpoints\n", "def test_multipage_note_attachment():\n    pikepdf = pytest.importorskip('pikepdf')\n\n    note = 'This is a note.'\n\n    fig, ax = plt.subplots()\n    ax.plot(range(5))\n\n    with PdfPages(io.BytesIO()) as pdf:\n        pdf.attach_note(note)\n        pdf.savefig(fig)\n\n    with pikepdf.Pdf.open(pdf._file.fh) as pdf:\n        annots = pdf.pages[0].Annots\n\n        # Iteration over Annots must occur within the context manager,\n        # otherwise it may fail depending on the pdf structure.\n        annot = next(\n            (a for a in annots if a.Contents == note),\n            None)\n        assert annot is not None\n", "def test_softmask():\n    fig, ax = plt.subplots()\n    x, y = np.ogrid[-2:2:.1, -2:2:.1]\n    dd = np.exp(-(x**2 + y**2))\n    dd[dd < .5] = 0\n    dd = np.ma.masked_equal(dd, 0)\n    ax.imshow(dd, interpolation='none', cmap='gray_r', alpha=0.5)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    with io.BytesIO() as fd:\n        fig.savefig(fd, format=\"pdf\")\n", "def test_character_tracker():\n    tracker = _backend_pdf_ps.CharacterTracker()\n    font = mpl.font_manager.FontProperties(family=['serif']).get_font()\n    tracker.track(font, 'hello')\n    tracker.track(font, 'hello')\n    tracker.track(font, 'world')\n    assert len(tracker.used[font.fname]) == 7\n", "def test_embed_empty_font():\n    # test embedding of empty font\n    with NamedTemporaryFile(suffix='.ttf', delete=False) as tmp:\n        fontfile = tmp.name\n    with pytest.raises(FileNotFoundError):\n        get_glyphs_subset(fontfile, 'Test String')\n    with pytest.raises(FileNotFoundError):\n        FT2Font(fontfile)\n    os.remove(fontfile)\n"], "sample_742": ["def test_logistic_regression_n_iter():\n    # Test that the number of iterations is correctly reported for\n    # LogisticRegression and LogisticRegressionCV\n\n    # Binary case\n    X, y = make_classification(n_samples=50, n_features=20, random_state=0)\n    lr = LogisticRegression(solver='liblinear')\n    lr.fit(X, y)\n    assert_equal(lr.n_iter_.shape, (1,))\n\n    lrcv = LogisticRegressionCV(solver='liblinear')\n    lrcv.fit(X, y)\n    assert_equal(lrcv.n_iter_.shape, (1, 3, 10))\n\n    # Multiclass case\n    X, y = make_classification(n_samples=50, n_features=20, n_classes=3,\n                               random_state=0)\n    lr = LogisticRegression(solver='liblinear')\n    lr.fit(X, y)\n    assert_equal(lr.n_iter_.shape, (1,))\n\n    lrcv = LogisticRegressionCV(solver='liblinear')\n    lrcv.fit(X, y)\n    assert_equal(lrcv.n_iter_.shape, (1, 3, 10))\n", "def test_logistic_regression_predict_proba_nonfinite_values():\n    # Test that predict_proba handles non-finite values in the decision\n    # function output correctly.\n    X, y = make_classification(n_samples=10, n_features=5, random_state=0)\n    clf = LogisticRegression(multi_class='multinomial').fit(X, y)\n\n    # Create a copy of X with non-finite values\n    X_nonfinite = X.copy()\n    X_nonfinite[0, 0] = np.inf\n\n    # Check that predict_proba does not raise an error\n    clf.predict_proba(X_nonfinite)\n\n    # Check that the probabilities are still normalized\n    assert_array_almost_equal(clf.predict_proba(X_nonfinite).sum(axis=1),\n                              np.ones(10))\n", "def test_intercept_scaling_liblinear():\n    # Test that the intercept is penalized with the scaling\n    # See Issue: https://github.com/scikit-learn/scikit-learn/issues/12924\n    X, y = make_classification(n_samples=100, n_features=20, random_state=0)\n\n    # Small intercept scaling, so intercept should be large.\n    lr1 = LogisticRegression(penalty='l2', solver='liblinear', C=1.,\n                             intercept_scaling=1e-3)\n    lr1.fit(X, y)\n    assert_greater(np.abs(lr1.intercept_), 1.)\n\n    # Large intercept scaling, so intercept should be small.\n    lr2 = LogisticRegression(penalty='l2', solver='liblinear', C=1.,\n                             intercept_scaling=1e3)\n    lr2.fit(X, y)\n    assert_less(np.abs(lr2.intercept_), 1.)\n", "def test_logistic_regression_stratification():\n    # Test that stratification is handled correctly in the cross-validation\n    # procedure for LogisticRegressionCV.\n\n    n_samples = 100\n    n_features = 5\n    X, y = make_classification(n_samples=n_samples, n_features=n_features,\n                               n_informative=3, n_redundant=0, random_state=0)\n\n    # case where all classes have the same number of samples\n    y = np.array([i % 3 for i in range(n_samples)])\n    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n    clf = LogisticRegressionCV(solver='liblinear', cv=cv).fit(X, y)\n    assert_array_almost_equal(clf.coef_.shape, (3, n_features))\n\n    # case where one class has a different number of samples\n    y = np.array([0] * 40 + [1] * 30 + [2] * 30)\n    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n    clf = LogisticRegressionCV(solver='liblinear', cv=cv).fit(X, y)\n    assert_array_almost_equal(clf.coef_.shape, (3, n_features))\n", "def test_logistic_regressioncv_multinomial_binary():\n    # Test LogisticRegressionCV for multinomial case with binary problem\n    # This test ensures that there is no ambiguity in how we interpret the\n    # \"best\" C value in the case of binary problems.\n    X, y = make_classification(n_samples=10, n_features=5, n_informative=3,\n                               n_redundant=0, random_state=0)\n    # Ensure binary problem by converting all y values to class 0.\n    y[y == 1] = 0\n    y = np.concatenate([y, [1]])\n    X = np.vstack([X, [1] * 5])\n    solver = 'lbfgs'\n\n    # OvR case\n    clf = LogisticRegressionCV(Cs=[1], fit_intercept=False, cv=2,\n                               solver=solver, multi_class='ovr')\n    clf.fit(X, y)\n    assert_array_equal(clf.coef_.shape, (1, 5))\n\n    # multinomial case\n    clf = LogisticRegressionCV(Cs=[1], fit_intercept=False, cv=2,\n                               solver=solver, multi_class='multinomial')\n    clf.fit(X, y)\n    assert_array_equal(clf.coef_.shape, (1, 5))\n"], "sample_481": ["    def test_add01(self):\n        output = self.engine.render_to_string(\"add01\", {\"a\": 1, \"b\": 2})\n        self.assertEqual(output, \"3\")\n", "    def test_truncate01(self):\n        output = self.engine.render_to_string(\"truncate01\", {\"a\": \"Hello, World!\"})\n        self.assertEqual(output, \"Hel...\")\n", "    def test_add01(self):\n        output = self.engine.render_to_string(\"add01\", {\"a\": 5, \"b\": 3})\n        self.assertEqual(output, \"8\")\n", "    def test_truncatechars01(self):\n        output = self.engine.render_to_string(\"truncatechars01\", {\"a\": \"Hello World!\"})\n        self.assertEqual(output, \"Hello W...\")\n", "    def test_pluralize_default(self):\n        self.assertEqual(pluralize(0), \"s\")\n        self.assertEqual(pluralize(1), \"\")\n        self.assertEqual(pluralize(2), \"s\")\n"], "sample_126": ["def test_add_blank_foreignkey(self):\n    \"\"\"\n    #23405 - Adding a NOT NULL and blank `ForeignKey` without default should not prompt for a default.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_empty, self.publisher],\n        [self.author_empty, self.publisher_with_author],\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"author\")\n", "def test_alter_unique_together_in_different_position(self):\n    \"\"\"\n    Alter unique_together also triggers on ordering changes.\n    \"\"\"\n    book_foo_together_ordered = ModelState(\"otherapp\", \"Book\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n        (\"title\", models.CharField(max_length=200)),\n    ], {\n        \"unique_together\": {(\"title\", \"author\")},\n    })\n    changes = self.get_changes(\n        [self.author_empty, self.book_foo_together], [self.author_empty, book_foo_together_ordered]\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"AlterUniqueTogether\"])\n    self.assertOperationAttributes(changes, \"otherapp\", 0, 0, name=\"book\", unique_together={(\"title\", \"author\")})\n", "def test_ordering_of_swappable_dependencies(self):\n    \"\"\"\n    When a model depends on a swappable model, its CreateModel operation\n    should come after the CreateModel operation of the swappable model.\n    \"\"\"\n    with isolate_lru_cache(apps.get_swappable_settings_name):\n        # The swappable model\n        User = ModelState(\"a\", \"User\", [\n            (\"id\", models.AutoField(primary_key=True)),\n        ], bases=(AbstractBaseUser,))\n        \n        # A model that depends on the swappable model\n        Author = ModelState(\"b\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"user\", models.ForeignKey(settings.AUTH_USER_MODEL, models.CASCADE)),\n        ])\n        \n        changes = self.get_changes([], [User, Author])\n        self.assertNumberMigrations(changes, 'a', 1)\n        self.assertNumberMigrations(changes, 'b', 1)\n        self.assertMigrationDependencies(changes, 'b', 0, [('a', 'auto_1')])\n", "def test_foreign_key_to_swappable_renamed(self):\n    \"\"\"\n    Renaming the swappable model a ForeignKey points to should result in the\n    ForeignKey being altered to point to the new model.\n    \"\"\"\n    before = [self.author_with_custom_user]\n    after = [self.author_with_custom_user_renamed]\n    changes = self.get_changes(before, after, MigrationQuestioner({'ask_rename': True}))\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"RenameModel\", \"AlterField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name=\"CustomUser\", new_name=\"RenamedCustomUser\")\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, name=\"user\")\n", "def test_deconstructible_function(self):\n    \"\"\"Nested deconstruction handles nested functions.\"\"\"\n    # Define the nested functions.\n        return x\n\n        return func(x)\n\n    # Define the model states with the nested functions.\n    before = ModelState(\n        \"testapp\", \"model\", [\n            (\"id\", models.AutoField(primary_key=True)),\n        ], {\n            \"default\": outer_func(1, 2, func=nested_func),\n        }\n    )\n    after = ModelState(\n        \"testapp\", \"model\", [\n            (\"id\", models.AutoField(primary_key=True)),\n        ], {\n            \"default\": outer_func(3, 4, func=nested_func),\n        }\n    )\n    changes = self.get_changes([before], [after])\n    # Right number/type of migrations?\n    self.assertEqual(len(changes), 1)\n    # Right number of actions?\n    self.assertEqual(len(changes['testapp'][0].operations), 1)\n    # Right actions order?\n    self.assertOperationTypes(changes, 'testapp', 0, ['AlterField'])\n"], "sample_417": ["    def test_slugify01(self):\n        output = self.engine.render_to_string(\"slugify01\", {\"a\": \"Hello World!\"})\n        self.assertEqual(output, \"hello-world\")\n", "    def test_round_half_up(self):\n        self.assertEqual(floatformat(1.2345, 2), \"1.23\")\n        self.assertEqual(floatformat(1.235, 2), \"1.24\")\n        self.assertEqual(floatformat(-1.2345, 2), \"-1.23\")\n        self.assertEqual(floatformat(-1.235, 2), \"-1.24\")\n        self.assertEqual(floatformat(Decimal(\"1.2345\"), 2), \"1.23\")\n        self.assertEqual(floatformat(Decimal(\"1.235\"), 2), \"1.24\")\n        self.assertEqual(floatformat(Decimal(\"-1.2345\"), 2), \"-1.23\")\n        self.assertEqual(floatformat(Decimal(\"-1.235\"), 2), \"-1.24\")\n", "    def test_add(self):\n        self.assertEqual(add(1, 2), 3)\n        self.assertEqual(add(\"1\", 2), 3)\n        self.assertEqual(add(1, \"2\"), 3)\n        self.assertEqual(add(\"a\", 2), \"\")\n        self.assertEqual(add(1, \"a\"), \"\")\n        self.assertEqual(add(\"a\", \"b\"), \"\")\n", "    def test_addslashes(self):\n        output = self.engine.render_to_string(\"stringfilter01\", {\"value\": \"He'd say, 'Hi, Joe!'\"})\n        self.assertEqual(output, \"He\\\\'d say, \\\\'Hi, Joe!\\\\'\")\n", "    def test_floatformat_international(self):\n        # Test formatting with comma as decimal separator.\n        with translation.override(\"nl\", deactivate=True):\n            output = self.engine.render_to_string(\n                \"floatformat_international\", {\"value\": 1234.56789}\n            )\n            self.assertEqual(output, \"1.234,568\")\n\n        # Test formatting with dot as decimal separator and comma as thousand separator.\n        with translation.override(\"en\", deactivate=True):\n            output = self.engine.render_to_string(\n                \"floatformat_international\", {\"value\": 1234.56789}\n            )\n            self.assertEqual(output, \"1,234.568\")\n\n        # Test formatting with custom decimal and thousand separators.\n        with translation.override(\"es\", deactivate=True):\n            with self.settings(DECIMAL_SEPARATOR=\"\u00b7\", THOUSAND_SEPARATOR=\"\u00b7\"):\n                output = self.engine.render_to_string(\n                    \"floatformat_international\", {\"value\": 1234.56789}\n                )\n                self.assertEqual(output, \"1\u00b7234\u00b7568\")\n\n        # Test formatting with a format specification that includes the 'g' suffix.\n        with translation.override(\"en\", deactivate=True):\n            output = self.engine.render_to_string(\n                \"floatformat_international\", {\"value\": 1234.56789}, {\"format\": \"3g\"}\n            )\n            self.assertEqual(output, \"1,234.568\")\n\n        # Test formatting with a format specification that includes the 'u' suffix.\n        with translation.override(\"nl\", deactivate=True):\n            output = self.engine.render_to_string(\n                \"floatformat_international\", {\"value\": 1234.56789}, {\"format\": \"3u\"}\n            )\n            self.assertEqual(output, \"1234.568\")\n"], "sample_943": ["def test_private_module(make_app, apidoc):\n    outdir = apidoc.outdir\n    assert (outdir / 'conf.py').isfile()\n    assert (outdir / '_private_module.rst').isfile()\n\n    with open(outdir / '_private_module.rst') as f:\n        rst = f.read()\n        assert \".. automodule:: _private_module\\n\" in rst\n        assert \"   :members:\\n\" in rst\n        assert \"   :undoc-members:\\n\" in rst\n        assert \"   :private-members:\\n\" in rst\n        assert \"   :show-inheritance:\\n\" in rst\n", "def test_is_packagedir(tempdir):\n    outdir = path(tempdir)\n    (outdir / 'testpkg').makedirs()\n    (outdir / 'testpkg' / '__init__.py').write_text('')\n    (outdir / 'testpkg' / 'example.py').write_text('')\n\n    assert is_packagedir(outdir / 'testpkg')\n    assert is_packagedir(None, ['__init__.py', 'example.py'])\n    assert not is_packagedir(outdir / 'nonexistent')\n    assert not is_packagedir(None, ['example.py'])\n", "def test_is_skipped_module(tempdir):\n    (tempdir / 'hello.py').write_text('')\n    (tempdir / '_world.py').write_text('')\n    (tempdir / '__init__.py').write_text('')\n\n    # without --private option\n    assert is_skipped_module(tempdir / '_world.py', {'includeprivate': False}, []) == True\n    assert is_skipped_module(tempdir / 'hello.py', {'includeprivate': False}, []) == False\n    assert is_skipped_module(tempdir / '__init__.py', {'includeprivate': False}, []) == False\n\n    # with --private option\n    assert is_skipped_module(tempdir / '_world.py', {'includeprivate': True}, []) == False\n    assert is_skipped_module(tempdir / 'hello.py', {'includeprivate': True}, []) == False\n    assert is_skipped_module(tempdir / '__init__.py', {'includeprivate': True}, []) == False\n", "def test_walk_with_followlinks(tempdir):\n    outdir = path(tempdir)\n    (outdir / 'symlink').makedirs()\n    (outdir / 'symlink' / 'target').makedirs()\n    (outdir / 'symlink' / 'target' / '__init__.py').write_text('')\n    (outdir / 'symlink' / 'target' / 'module.py').write_text('')\n    (outdir / 'symlink' / 'module.py').symlink_to(outdir / 'symlink' / 'target' / 'module.py')\n    apidoc_main(['-o', tempdir, '--follow-links', tempdir])\n\n    assert (outdir / 'symlink.rst').exists()\n    assert (outdir / 'symlink.module.rst').exists()\n\n    content = (outdir / 'symlink.rst').read_text()\n    assert 'symlink namespace' in content\n    assert 'symlink.module module' in content\n\n    content = (outdir / 'symlink.module.rst').read_text()\n    assert 'symlink.module module' in content\n", "def test_implicit_namespaces_without_init(apidoc):\n    \"\"\"Test implicit namespaces with a package that doesn't have __init__.py\"\"\"\n    outdir = apidoc.outdir\n    assert (outdir / 'conf.py').isfile()\n    assert (outdir / 'mypkg.rst').isfile()\n    assert (outdir / 'mypkg.subpkg.rst').isfile()\n    assert (outdir / 'mypkg.subpkg.module.rst').isfile()\n    assert (outdir / 'mypkg.module.rst').isfile()\n\n    with open(outdir / 'mypkg.rst') as f:\n        rst = f.read()\n        assert \"automodule:: mypkg.module\\n\" in rst\n\n    with open(outdir / 'mypkg.subpkg.rst') as f:\n        rst = f.read()\n        assert \"automodule:: mypkg.subpkg.module\\n\" in rst\n\n    app = make_app('text', srcdir=outdir)\n    app.build()\n    print(app._status.getvalue())\n    print(app._warning.getvalue())\n\n    builddir = outdir / '_build' / 'text'\n    assert (builddir / 'mypkg.txt').isfile()\n    assert (builddir / 'mypkg.subpkg.txt').isfile()\n    assert (builddir / 'mypkg.subpkg.module.txt').isfile()\n    assert (builddir / 'mypkg.module.txt').isfile()\n"], "sample_854": ["def test_gamma_auto_scale():\n    # Test that gamma is set correctly for 'auto' and 'scale' when fit is called\n    X, y = [[0.], [1.]], [0, 1]\n\n    clf = svm.SVC(kernel='linear', gamma='auto')\n    clf.fit(X, y)\n    assert clf._gamma == 1.0\n\n    clf = svm.SVC(kernel='linear', gamma='scale')\n    clf.fit(X, y)\n    assert clf._gamma == 4.0\n\n    clf = svm.SVC(kernel='rbf', gamma='auto')\n    clf.fit(X, y)\n    assert clf._gamma == 1.0\n\n    clf = svm.SVC(kernel='rbf', gamma='scale')\n    clf.fit(X, y)\n    assert clf._gamma == 4.0\n", "def test_base_libsvm___init__():\n    # Test that BaseLibSVM __init__ raises an error when gamma is 0.\n    with pytest.raises(ValueError, match=\"The gamma value of 0.0 is invalid.\"):\n        BaseLibSVM(kernel='rbf', degree=3, gamma=0, coef0=0.0,\n                   tol=0.001, C=1.0, nu=0.0, epsilon=0.1,\n                   shrinking=True, probability=False, cache_size=200,\n                   class_weight=None, verbose=False, max_iter=-1,\n                   random_state=0)\n", "def test_svm_sparse_input(SVCClass):\n    X_sparse = sparse.csr_matrix(X)\n    X_dense = np.array(X)\n\n    clf_sparse = SVCClass().fit(X_sparse, Y)\n    clf_dense = SVCClass().fit(X_dense, Y)\n\n    assert_array_equal(clf_sparse.predict(X_sparse), clf_dense.predict(X_dense))\n    assert_array_almost_equal(clf_sparse.support_, clf_dense.support_)\n    assert_array_almost_equal(clf_sparse.dual_coef_, clf_dense.dual_coef_)\n    assert_array_almost_equal(clf_sparse.intercept_, clf_dense.intercept_)\n", "def test_base_libsvm_input_validation():\n    # Test input validation for BaseLibSVM\n\n    # Test error when gamma is zero\n    clf = svm.SVC(gamma=0)\n    with pytest.raises(ValueError, match=\"The gamma value of 0.0 is invalid\"):\n        clf.fit(X, Y)\n\n    # Test error when C is negative\n    clf = svm.SVC(C=-1)\n    with pytest.raises(ValueError, match=\"C should be a positive number\"):\n        clf.fit(X, Y)\n\n    # Test error when nu is negative or greater than 1\n    clf = svm.NuSVC(nu=-1)\n    with pytest.raises(ValueError, match=\"nu should be between 0 and 1\"):\n        clf.fit(X, Y)\n\n    clf = svm.NuSVC(nu=1.1)\n    with pytest.raises(ValueError, match=\"nu should be between 0 and 1\"):\n        clf.fit(X, Y)\n\n    # Test error when kernel is not supported\n    clf = svm.SVC(kernel='foo')\n    with pytest.raises(ValueError, match=\"kernel should be\"):\n        clf.fit(X, Y)\n\n    # Test error when degree is negative\n    clf = svm.SVC(kernel='poly', degree=-1)\n    with pytest.raises(ValueError, match=\"degree should be\"):\n        clf.fit(X, Y)\n\n    # Test error when coef0 is negative\n    clf = svm.SVC(kernel='poly', coef0=-1)\n    with pytest.raises(ValueError, match=\"coef0 should be\"):\n        clf.fit(X, Y)\n", "def test_precomputed_sparse_kernel():\n    # Test precomputed sparse kernel.\n    # Regression test for #10144\n    iris = datasets.load_iris()\n    X, y = iris.data, iris.target\n    K = np.dot(X, X.T)\n    K[K != 0] = 1  # simulate sparse kernel\n    K = sparse.csr_matrix(K)\n    clf = svm.SVC(kernel='precomputed')\n    clf.fit(K, y)\n    assert_array_almost_equal(clf.dual_coef_, clf.dual_coef_)\n"], "sample_840": ["def test_pls_fit_transform():\n    # Test that fit and transform are equivalent to fit_transform\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n\n    pls = pls_.PLSCanonical()\n    pls.fit(X, Y)\n    X_score, Y_score = pls.transform(X, Y)\n\n    pls.fit_transform(X, Y)\n    assert_array_almost_equal(pls.x_scores_, X_score)\n    assert_array_almost_equal(pls.y_scores_, Y_score)\n", "def test_pls_transform_only_x():\n    # Test transform method when only X is provided\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n    pls = pls_.PLSCanonical(n_components=2)\n    pls.fit(X, Y)\n    X_transformed = pls.transform(X)\n    assert X_transformed.shape == (X.shape[0], 2)\n    assert_array_almost_equal(pls.x_scores_, X_transformed)\n", "def test_pls_deflation_mode():\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n\n    pls_reg = pls_.PLSRegression(n_components=2)\n    pls_reg.fit(X, Y)\n\n    pls_can = pls_.PLSCanonical(n_components=2)\n    pls_can.fit(X, Y)\n\n    assert pls_reg.deflation_mode != pls_can.deflation_mode\n    assert_array_almost_equal(pls_reg.x_scores_, pls_can.x_scores_)\n    assert_array_almost_equal(pls_reg.y_scores_, pls_can.y_scores_)\n    assert pls_reg.x_rotations_.shape == pls_can.x_rotations_.shape\n    assert pls_reg.y_rotations_.shape == pls_can.y_rotations_.shape\n", "def test_pls_regression_svd():\n    # Test that PLSSVD with one component gives the same results as PLSRegression\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n    pls_svd = pls_.PLSSVD(n_components=1)\n    pls_svd.fit(X, Y)\n    pls_2 = pls_.PLSRegression(n_components=1)\n    pls_2.fit(X, Y)\n    assert_array_almost_equal(pls_svd.x_scores_, pls_2.x_scores_)\n    assert_array_almost_equal(pls_svd.y_scores_, pls_2.y_scores_)\n    assert_array_almost_equal(pls_svd.x_weights_, pls_2.x_weights_)\n    assert_array_almost_equal(pls_svd.y_weights_, pls_2.y_weights_)\n\n", "def test_pls_copy():\n    # Test that the copy parameter is correctly handled\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n\n    for clf in [pls_.PLSCanonical, pls_.PLSRegression, pls_.PLSSVD]:\n        for copy in [True, False]:\n            X_copy = X.copy()\n            Y_copy = Y.copy()\n\n            # Fit with copy\n            clf_copy = clf(n_components=2, copy=copy)\n            clf_copy.fit(X_copy, Y_copy)\n\n            # Check that the original data is not modified\n            assert_array_equal(X, d.data)\n            assert_array_equal(Y, d.target)\n\n            # Fit without copy\n            clf_nocopy = clf(n_components=2, copy=False)\n            clf_nocopy.fit(X, Y)\n\n            # Check that the original data is modified\n            assert_array_not_equal(X, d.data)\n            assert_array_not_equal(Y, d.target)\n\n            # Reset the data\n            X = d.data\n            Y = d.target\n"], "sample_652": ["def test_pycollect_makemodule_on_package(tmpdir):\n    package = tmpdir.mkdir(\"package\")\n    package.join(\"__init__.py\").write(\n        textwrap.dedent(\n            \"\"\"\\\n            import pytest\n\n            @pytest.fixture\n                return \"package\"\n        \"\"\"\n        )\n    )\n    package.join(\"subpackage\").mkdir(\"__init__.py\").write(\n        textwrap.dedent(\n            \"\"\"\\\n            import pytest\n\n            @pytest.fixture\n                return \"subpackage\"\n        \"\"\"\n        )\n    )\n    package.join(\"test_module.py\").write(\n        textwrap.dedent(\n            \"\"\"\\\n                assert package_fixture == \"package\"\n                assert subpackage_fixture == \"subpackage\"\n        \"\"\"\n        )\n    )\n    reprec = tmpdir.runpytest()\n    reprec.assertoutcome(passed=1)\n", "    def test_id_generation_for_test_with_parametrize_and_fixture(self, testdir):\n        testdir.makeconftest(\n            \"\"\"\n            import pytest\n            @pytest.fixture(params=[1, 2, 3])\n                return request.param\n        \"\"\"\n        )\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.parametrize(\"x\", [4, 5, 6])\n                assert arg in [1, 2, 3]\n                assert x in [4, 5, 6]\n        \"\"\"\n        )\n        result = testdir.runpytest(\"-v\")\n        result.stdout.fnmatch_lines(\n            \"\"\"\n            test_foo.py::test_foo[1-4] PASSED\n            test_foo.py::test_foo[2-4] PASSED\n            test_foo.py::test_foo[3-4] PASSED\n            test_foo.py::test_foo[1-5] PASSED\n            test_foo.py::test_foo[2-5] PASSED\n            test_foo.py::test_foo[3-5] PASSED\n            test_foo.py::test_foo[1-6] PASSED\n            test_foo.py::test_foo[2-6] PASSED\n            test_foo.py::test_foo[3-6] PASSED\n        \"\"\"\n        )\n", "    def test_func_closure_module_auto_parametrize(self, testdir):\n        \"\"\"Ensure parametrize fixtures are ordered by their scopes (#2405)\"\"\"\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.fixture(scope='module')\n\n            @pytest.fixture(scope='function')\n\n            @pytest.mark.parametrize('p1', ['a'], scope='module')\n            @pytest.mark.parametrize('p2', ['b'], scope='function')\n                pass\n        \"\"\"\n        )\n        items, _ = testdir.inline_genitems()\n        request = FixtureRequest(items[0])\n        assert request.fixturenames == \"m1 p1 f1 p2\".split()\n", "    def test_parametrize_inside_generate_tests(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n                metafunc.parametrize(\"arg1\", [1, 2])\n                metafunc.parametrize(\"arg2\", [3, 4])\n\n                pass\n        \"\"\"\n        )\n        reprec = testdir.inline_run()\n        reprec.assertoutcome(passed=4)\n", "    def test_parametrize_twice_different_scopes(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.fixture(scope=\"session\", params=[\"s1\", \"s2\"])\n                return request.param\n\n            @pytest.fixture(scope=\"module\", params=[\"m1\", \"m2\"])\n                return request.param\n\n                assert m in ['m1', 'm2']\n                assert s in ['s1', 's2']\n        \"\"\"\n        )\n        reprec = testdir.inline_run(\"-v\")\n        reprec.assertoutcome(passed=4)\n"], "sample_225": ["    def setUpTestData(cls):\n        cls.u1 = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n", "    def setUp(self):\n        self.site = admin.AdminSite()\n", "    def setUp(self):\n        self.site = admin.AdminSite()\n        self.factory = RequestFactory()\n", "    def setUp(self):\n        self.site = admin.AdminSite()\n        self.user = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        self.request_factory = RequestFactory()\n", "    def setUp(self):\n        self.site = admin.AdminSite()\n"], "sample_1139": ["def test_Rationals_iterator():\n    r = S.Rationals\n    it = iter(r)\n    assert next(it) == 0\n    assert next(it) == 1\n    assert next(it) == -1\n    assert next(it) == S.Half\n    assert next(it) == 2\n    assert next(it) == Rational(-1, 2)\n    assert next(it) == -2\n    assert next(it) == Rational(1, 3)\n    assert next(it) == 3\n    assert next(it) == Rational(-1, 3)\n    assert next(it) == -3\n    assert next(it) == Rational(2, 3)\n", "def test_ComplexRegion_from_real_unevaluated():\n    x = Symbol('x')\n    c1 = ComplexRegion(x * Interval(0, 1), polar=False)\n    raises(ValueError, lambda: c1.from_real(c1))\n    raises(ValueError, lambda: c1.from_real(x))\n    assert c1.from_real(Interval(-1, 1)) == ComplexRegion(Interval(-1, 1) * FiniteSet(0), False)\n", "def test_ComplexRegion_boundary():\n    # Polar form\n    c1 = ComplexRegion(Interval(0, 1)*Interval(0, 2*S.Pi), polar=True)\n    c2 = ComplexRegion(Interval(0, 1)*Interval(0, S.Pi), polar=True)\n    c3 = ComplexRegion(Interval(0, oo)*Interval(0, S.Pi), polar=True)\n    c4 = ComplexRegion(Interval(0, oo)*Interval(S.Pi, 2*S.Pi), polar=True)\n\n    assert c1.boundary == c1.sets\n    assert c2.boundary == c2.sets\n    assert c3.boundary == c3.sets\n    assert c4.boundary == c4.sets\n\n    # Rectangular form\n    c5 = ComplexRegion(Interval(2, 5)*Interval(6, 9))\n    c6 = ComplexRegion(Interval(4, 6)*Interval(10, 12))\n    c7 = ComplexRegion(Interval(0, 10)*Interval(-10, 0))\n    c8 = ComplexRegion(Interval(12, 16)*Interval(14, 20))\n\n    assert c5.boundary == c5.sets\n    assert c6.boundary == c6.sets\n    assert c7.boundary == c7.sets\n    assert c8.boundary == c8.sets\n\n    # Test complex region is subset of its closure\n    c9 = ComplexRegion(Interval(1, 2)*Interval(2, 3))\n    c10 = ComplexRegion(Interval(-1, 1)*Interval(-1, 1))\n    assert c9.is_subset(c9.closure) == True\n    assert c10.is_subset(c10.closure) == True\n\n    # Test complex region is subset of its closure\n    c11 = ComplexRegion(Interval(0, 1)*Interval(0, 2*S.Pi), polar=True)\n    c12 = ComplexRegion(Interval(0, 1)*Interval(0, S.Pi), polar=True)\n    assert c11.is_subset(c11.closure) == True\n    assert c12.is_subset(c12.closure) == True\n", "def test_ComplexRegion_iter():\n    raises(TypeError, lambda: iter(ComplexRegion(Interval(0, 1)*Interval(0, 2*S.Pi), polar=True)))\n    r = Interval(0, 1)\n    theta = Interval(0, 2*pi)\n    c = ComplexRegion(r*theta, polar=True)\n    assert next(iter(c)) == 0\n    assert next(iter(c)) == S.ImaginaryUnit\n    assert next(iter(c)) == 2*S.ImaginaryUnit\n    raises(StopIteration, lambda: next(iter(c)))\n    c = ComplexRegion(Interval(2, 3)*Interval(4, 5))\n    assert next(iter(c)) == 2 + 4*I\n    assert next(iter(c)) == 2 + 5*I\n    assert next(iter(c)) == 3 + 4*I\n    assert next(iter(c)) == 3 + 5*I\n    raises(StopIteration, lambda: next(iter(c)))\n", "def test_ComplexRegion_imageset_intersection():\n    # Polar form\n    unit_disk = ComplexRegion(Interval(0, 1)*Interval(0, 2*S.Pi), polar=True)\n    upper_half_unit_disk = ComplexRegion(Interval(0, 1)*Interval(0, S.Pi), polar=True)\n\n    # Rectangular form\n    unit_square = ComplexRegion(Interval(-1, 1)*Interval(-1, 1))\n    upper_half_unit_square = ComplexRegion(Interval(-1, 1)*Interval(0, 1))\n\n    S1 = ImageSet(Lambda((x, y), (x + y, x*y)), S.Naturals*S.Naturals)\n    S2 = ImageSet(Lambda((x, y), (x - y, x/y)), S.Integers*S.Integers)\n\n    # Check that intersections of the sets with ComplexRegion\n    assert upper_half_unit_disk.intersect(S1) == Intersection(upper_half_unit_disk, S1, evaluate=False)\n    assert upper_half_unit_square.intersect(S2) == Intersection(upper_half_unit_square, S2, evaluate=False)\n\n    # Check that intersection of ComplexRegion with S.EmptySet\n    assert unit_disk.intersect(S.EmptySet) == S.EmptySet\n    assert unit_square.intersect(S.EmptySet) == S.EmptySet\n\n    # Check that intersection of ComplexRegion with S.UniversalSet\n    assert unit_disk.intersect(S.UniversalSet) == unit_disk\n    assert unit_square.intersect(S.UniversalSet) == unit_square\n"], "sample_189": ["    def test_max_entries(self):\n        cache.set('a', 1)\n        cache.set('b', 2)\n        self.assertIsNone(cache.get('a'))\n        self.assertEqual(cache.get('b'), 2)\n", "    def test_set_many_fails_silently_if_not_implemented(self):\n        cache.set_many = lambda x, timeout=None, version=None: []\n        cache.set_many({'key1': 'value1', 'key2': 'value2'})\n        self.assertIsNone(cache.get('key1'))\n        self.assertIsNone(cache.get('key2'))\n", "    def test_culling(self):\n        cache.set('key1', 'value1')\n        cache.set('key2', 'value2')\n        cache.set('key3', 'value3')\n        self.assertIsNone(cache.get('key1'))  # key1 should be evicted by key3\n", "    def setUp(self):\n        super().setUp()\n        self.cache = caches['default']\n", "def test_make_key_with_version(self):\n    \"\"\"\n    Test that the key is correctly generated with version.\n    \"\"\"\n    cache.set('answer1', 42, version=1)\n    cache.set('answer1', 43, version=2)\n\n    key1 = cache.make_key('answer1', version=1)\n    key2 = cache.make_key('answer1', version=2)\n\n    self.assertNotEqual(key1, key2)\n    self.assertIn('1:', key1)\n    self.assertIn('2:', key2)\n"], "sample_552": ["def test_figure_subplotspec():\n    fig = plt.figure()\n    ax1 = fig.add_subplot(2, 2, 1)\n    ax2 = fig.add_subplot(2, 2, 3)\n    ax3 = fig.add_subplot(2, 2, (1, 3))\n\n    assert ax1.get_subplotspec().rowspan == range(0, 1)\n    assert ax1.get_subplotspec().colspan == range(0, 1)\n    assert ax2.get_subplotspec().rowspan == range(1, 2)\n    assert ax2.get_subplotspec().colspan == range(1, 2)\n    assert ax3.get_subplotspec().rowspan == range(0, 2)\n    assert ax3.get_subplotspec().colspan == range(0, 2)\n", "def test_figure_legend_title():\n    fig, axs = plt.subplots(2)\n    axs[0].plot([0, 1], [1, 0], label='x', color='g')\n    axs[0].plot([0, 1], [0, 1], label='y', color='r')\n    axs[0].plot([0, 1], [0.5, 0.5], label='y', color='k')\n\n    axs[1].plot([0, 1], [1, 0], label='_y', color='r')\n    axs[1].plot([0, 1], [0, 1], label='z', color='b')\n    l = fig.legend(title='Legend title')\n    l.set_title('Legend title')\n", "def test_subfigure_mosaic(fig_test, fig_ref):\n    fig_test.subplot_mosaic([['A', 'B'], ['C', 'D']], width_ratios=[1, 2])\n    fig_test.suptitle('Test subfigure mosaic')\n    fig_ref.add_gridspec(2, 2, width_ratios=[1, 2])\n    fig_ref.add_subplot(1, 2, 1).set_title('A')\n    fig_ref.add_subplot(1, 2, 2).set_title('B')\n    fig_ref.add_subplot(2, 1, 1).set_title('C')\n    fig_ref.add_subplot(2, 1, 2).set_title('D')\n    fig_ref.suptitle('Test subfigure mosaic')\n", "def test_figure_removeaxes_order():\n    fig = plt.figure()\n    ax1 = fig.add_subplot(211)\n    ax2 = fig.add_subplot(212)\n    ax1.set_title('Title1')\n    ax2.set_title('Title2')\n\n    fig.delaxes(ax2)\n\n    assert fig.axes == [ax1]\n\n    ax3 = fig.add_subplot(212)\n    assert fig.axes == [ax1, ax3]\n\n    fig.delaxes(ax1)\n\n    assert fig.axes == [ax3]\n\n    ax4 = fig.add_subplot(211)\n    assert fig.axes == [ax3, ax4]\n", "def test_add_artist_transform():\n    fig = plt.figure()\n    ax = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n    l1 = plt.Line2D([.2, .7], [.7, .7], gid='l1', transform=ax.transAxes)\n    l2 = plt.Line2D([.2, .7], [.8, .8], gid='l2', transform=ax.transAxes)\n    r1 = plt.Circle((.5, .5), .2, transform=ax.transAxes, gid='C1')\n    r2 = plt.Circle((.5, .5), .05, transform=ax.transAxes, gid='C2')\n    r3 = plt.Circle((.5, .5), .55, transform=ax.transAxes, facecolor='crimson', gid='C3')\n    for a in [l1, l2, r1, r2, r3]:\n        fig.add_artist(a)\n    fig.add_artist(l2)\n    fig.draw_without_rendering()\n    l2.remove()\n    ax2 = fig.add_axes([0.2, 0.2, 0.8, 0.8])\n    l1 = plt.Line2D([.2, .7], [.7, .7], gid='l1', transform=ax2.transAxes)\n    r1 = plt.Circle((.5, .5), .2, transform=ax2.transAxes, gid='C1')\n    r2 = plt.Circle((.5, .5), .05, transform=ax2.transAxes, gid='C2')\n    r3 = plt.Circle((.5, .5), .55, transform=ax2.transAxes, facecolor='crimson', gid='C3')\n    for a in [l1, r1, r2, r3]:\n        ax2.add_artist(a)\n    fig.draw_without_rendering()\n"], "sample_139": ["def test_response_post_save_change(self):\n    \"\"\"\n    Test the response_post_save_change method in the ModelAdmin class.\n    \"\"\"\n    parent = Parent.objects.create(name='parent')\n    child = Child.objects.create(name='child', parent=parent)\n    model_admin = ChildAdmin(Child, custom_site)\n    request = self._mocked_authenticated_request('/child/', self.superuser)\n    response = model_admin.response_post_save_change(request, child)\n    self.assertEqual(response.status_code, 302)\n    self.assertEqual(response.url, '/admin/admin_changelist/child/')\n", "def test_list_display_callable(self):\n    \"\"\"\n    Callable methods in list_display should be called and their results displayed.\n    \"\"\"\n    parent = Parent.objects.create(name='parent')\n    child = Child.objects.create(parent=parent, name='child')\n\n    class CallableModelAdmin(admin.ModelAdmin):\n        list_display = ('name', 'is_active')\n\n            return True\n\n    m = CallableModelAdmin(Child, custom_site)\n    request = self.factory.get('/child/')\n    request.user = self.superuser\n    cl = m.get_changelist_instance(request)\n    cl.formset = None\n    template = Template('{% load admin_list %}{% spaceless %}{% result_list cl %}{% endspaceless %}')\n    context = Context({'cl': cl, 'opts': Child._meta})\n    table_output = template.render(context)\n    link = reverse('admin:admin_changelist_child_change', args=(child.id,))\n    row_html = build_tbody_html(child.id, link, '<td class=\"field-is_active\">True</td>')\n    self.assertNotEqual(table_output.find(row_html), -1, 'Failed to find expected row element: %s' % table_output)\n", "def test_response_action_handle_custom_permission(self):\n    class DeleteModelAdmin(ModelAdmin):\n            return False\n\n    class DeleteSelectedAction(admin.actions.delete_selected):\n        allowed_permissions = ['delete']\n\n    model_admin = DeleteModelAdmin(Band, custom_site)\n    model_admin.actions = [DeleteSelectedAction]\n    request = self.factory.post('/band/', data={'action': 'delete_selected', 'select_across': '0', 'index': '0'})\n    request.user = self.superuser\n    request._messages = CookieStorage(request)\n    cl = model_admin.get_changelist_instance(request)\n    response = model_admin.response_action(request, cl.get_queryset(request))\n    self.assertIsInstance(response, HttpResponseRedirect)\n    messages = [m.message for m in request._messages]\n    self.assertEqual(1, len(messages))\n    self.assertEqual(\"Items must be selected in order to perform \"\n                     \"actions on them. No items have been changed.\", messages[0])\n", "def test_custom_paginator_class_with_invalid_attribute(self):\n    class CustomPaginator(Paginator):\n        invalid_attribute = None\n\n    class CustomPaginationAdmin(ModelAdmin):\n        paginator = CustomPaginator\n        list_per_page = 5\n\n    m = CustomPaginationAdmin(Child, custom_site)\n    request = self.factory.get('/child/')\n    request.user = self.superuser\n    cl = m.get_changelist_instance(request)\n    cl.get_results(request)\n    self.assertIsInstance(cl.paginator, CustomPaginator)\n    self.assertEqual(cl.paginator.per_page, m.list_per_page)\n\n    # Test with a non-integer 'per_page' value.\n    m.list_per_page = 'string'\n    with self.assertRaises(ValueError):\n        cl.get_results(request)\n\n    # Test with a 'per_page' value that isn't a positive integer.\n    m.list_per_page = -10\n    with self.assertRaises(ValueError):\n        cl.get_results(request)\n", "def test_action_response_cancellation(self):\n    \"\"\"\n    Regression test for #16286: Actions shouldn't raise an exception if\n    response_action returns None.\n    \"\"\"\n    m = BandAdmin(Band, custom_site)\n\n    class NullAction:\n            return None\n\n    m.actions = [NullAction]\n\n    request = self.factory.get('/band/')\n    request.user = self.superuser\n\n    actions = m.get_actions(request)\n    func = actions[m.get_action_choices(request)[0][0]][0]\n\n    # No exception should be raised\n    func(m, request, Band.objects.all())\n"], "sample_861": ["def test_grid_search_multioutput():\n    # Test search with multioutput estimators\n    X = np.random.rand(30, 3)\n    y = np.random.rand(30, 2)\n    est_parameters = {\"max_depth\": [1, 2, 3, 4]}\n    cv = KFold(random_state=0)\n\n    estimators = [DecisionTreeRegressor(random_state=0),\n                  DecisionTreeClassifier(random_state=0)]\n\n    for est in estimators:\n        grid_search = GridSearchCV(est, est_parameters, cv=cv)\n        grid_search.fit(X, y)\n        res_params = grid_search.cv_results_['params']\n        for cand_i in range(len(res_params)):\n            est.set_params(**res_params[cand_i])\n\n            for i, (train, test) in enumerate(cv.split(X, y)):\n                est.fit(X[train], y[train])\n                correct_score = est.score(X[test], y[test])\n                assert_almost_equal(\n                    correct_score,\n                    grid_search.cv_results_['split%d_test_score' % i][cand_i])\n", "def test_grid_search_sparse_output():\n    # test that grid search works with sparse matrices as output\n    X, y = make_classification(n_samples=200, n_features=100, random_state=0)\n\n    clf = DecisionTreeClassifier(random_state=0)\n    grid_search = GridSearchCV(clf, {'max_depth': [1, 2, 3]}, cv=3)\n    grid_search.fit(X, y)\n\n    y_pred = grid_search.predict(X)\n    assert sp.issparse(y_pred)\n    assert y_pred.shape == (X.shape[0],)\n    assert y_pred.format == \"csr\"\n\n    y_pred_proba = grid_search.predict_proba(X)\n    assert sp.issparse(y_pred_proba)\n    assert y_pred_proba.shape == (X.shape[0], 2)\n    assert y_pred_proba.format == \"csr\"\n\n    y_pred_log_proba = grid_search.predict_log_proba(X)\n    assert sp.issparse(y_pred_log_proba)\n    assert y_pred_log_proba.shape == (X.shape[0], 2)\n    assert y_pred_log_proba.format == \"csr\"\n", "def test_grid_search_param_dtypes():\n    # Test GridSearchCV with different parameter dtypes\n    X = np.arange(6).reshape(6, -1)\n    y = [0, 0, 0, 1, 1, 1]\n\n    # Test with integer param values\n    param_grid = {'C': [1, 2, 3]}\n    grid_search = GridSearchCV(LinearSVC(random_state=0), param_grid, cv=3)\n    grid_search.fit(X, y)\n    assert grid_search.best_params_['C'] == 1\n\n    # Test with float param values\n    param_grid = {'C': [1.0, 2.0, 3.0]}\n    grid_search = GridSearchCV(LinearSVC(random_state=0), param_grid, cv=3)\n    grid_search.fit(X, y)\n    assert grid_search.best_params_['C'] == 1.0\n\n    # Test with string param values\n    param_grid = {'kernel': ['linear', 'rbf']}\n    grid_search = GridSearchCV(SVC(random_state=0), param_grid, cv=3)\n    grid_search.fit(X, y)\n    assert grid_search.best_params_['kernel'] == 'linear'\n\n    # Test with object param values\n    param_grid = {'estimator__C': [1, 2, 3]}\n    pipeline = Pipeline([('estimator', LinearSVC(random_state=0))])\n    grid_search = GridSearchCV(pipeline, param_grid, cv=3)\n    grid_search.fit(X, y)\n    assert grid_search.best_params_['estimator__C'] == 1\n", "def test_grid_search_with_pipeline_and_sample_weight():\n    # Test that GridSearchCV works with a pipeline and sample weights\n    X, y = make_classification(n_samples=20, random_state=0)\n    w = np.random.rand(20)\n    pipe = Pipeline([('clf', SVC())])\n    param_grid = {'clf__C': [1, 10]}\n    grid_search = GridSearchCV(pipe, param_grid, cv=2)\n\n    grid_search.fit(X, y, sample_weight=w)\n\n    # Test that the best estimator has the correct parameters\n    assert grid_search.best_estimator_.get_params() == pipe.set_params(\n        **grid_search.best_params_).get_params()\n\n    # Test that the best estimator can make predictions\n    y_pred = grid_search.predict(X)\n    assert_array_equal(y_pred, grid_search.best_estimator_.predict(X))\n", "def test_search_empty_cv_results_without_y(search_cv, specialized_params):\n    # Test search without y\n    # noise-free simple 2d-data\n    X = np.random.rand(10, 2)\n\n    # once with iid=True (default)\n    common_params = {'estimator': KMeans(), 'cv': 3,\n                     'return_train_score': True}\n    search = search_cv(**common_params, **specialized_params)\n    search.fit(X)\n\n    cv_results = search.cv_results_\n    # Check if the test and train scores are the same\n    assert_array_almost_equal(cv_results['mean_test_score'],\n                              cv_results['mean_train_score'])\n    assert_array_almost_equal(cv_results['std_test_score'],\n                              cv_results['std_train_score'])\n    # Check if all the scores are the same for all candidates\n    # as the data is the same and is clustered\n    assert_array_almost_equal(cv_results['mean_test_score'][0],\n                              cv_results['mean_test_score'])\n    assert_array_almost_equal(cv_results['std_test_score'][0],\n                              cv_results['std_test_score'])\n"], "sample_1168": ["def test_has_variety():\n    assert has_variety((1, 2, 1)) is True\n    assert has_variety((1, 1, 1)) is False\n", "def test_permute_signs():\n    assert len(list(permute_signs((1, 2, 3)))) == 8\n    assert list(permute_signs((1, 2, 3))) == [\n        (1, 2, 3), (-1, 2, 3), (1, -2, 3), (-1, -2, 3),\n        (1, 2, -3), (-1, 2, -3), (1, -2, -3), (-1, -2, -3)]\n    assert len(list(permute_signs((1, 2, 3, 0)))) == 8\n    assert len(list(permute_signs((1, 0, 0, 0)))) == 2\n    assert len(list(permute_signs((0, 0, 0, 0)))) == 1\n    assert len(list(permute_signs(()))) == 1\n", "def test_least_rotation():\n    assert least_rotation([3, 1, 5, 1, 2]) == 3\n    assert least_rotation([3, 1, 5, 1, 2], key=lambda x: x**2) == 3\n    assert least_rotation([1, 3, 3, 3, 1]) == 0\n    assert least_rotation([3, 3, 3, 3, 3]) == 0\n    assert least_rotation([1, 3, 3, 3, 1], key=lambda x: x**2) == 0\n    assert least_rotation([3, 3, 3, 3, 3], key=lambda x: x**2) == 0\n", "def test_permute_signs():\n    assert list(permute_signs((0, 1, 2))) == [(0, 1, 2), (0, -1, 2), (0, 1, -2), (0, -1, -2)]\n    assert list(permute_signs([1, 2])) == [(1, 2), (-1, 2), (1, -2), (-1, -2)]\n    assert list(permute_signs((0, 0, 0))) == [(0, 0, 0)]\n    assert list(permute_signs((0, 1, 0))) == [(0, 1, 0), (0, -1, 0)]\n    assert list(permute_signs((0, 1, 2), 1)) == [(0, 1, 2), (0, -1, 2), (0, 1, -2), (0, -1, -2)]\n    raises(TypeError, lambda: list(permute_signs({1, 2, 3})))\n", "def test_least_rotation():\n    # basic functionality\n    assert least_rotation([3, 1, 5, 1, 2]) == 3\n    assert least_rotation([1, 2, 3, 4, 5]) == 0\n    # fixed-length strings\n    assert least_rotation('abcde') == 0\n    assert least_rotation('edcba') == 4\n    # lists of lists\n    assert least_rotation([[3, 1], [1, 2], [1, 5], [2]]) == 3\n    assert least_rotation([[1], [2], [3], [4]]) == 0\n    # non-string, non-list sequences\n    assert least_rotation((3, 1, 5, 1, 2)) == 3\n    assert least_rotation(range(5)) == 0\n    # non-sequence inputs\n    raises(TypeError, lambda: least_rotation(1))\n    # unsortable inputs\n    raises(TypeError, lambda: least_rotation([1, 2, 3, 'a']))\n    # non-callable key\n    raises(TypeError, lambda: least_rotation([1, 2, 3], key=1))\n"], "sample_770": ["def test_silhouette_samples_with_sparse_matrix():\n    # Test silhouette_samples with sparse matrix\n    dataset = datasets.load_iris()\n    X = dataset.data\n    y = dataset.target\n    X_sparse = csr_matrix(X)\n    \n    silhouette_dense = silhouette_samples(X, y)\n    silhouette_sparse = silhouette_samples(X_sparse, y)\n    \n    assert_array_equal(silhouette_dense, silhouette_sparse)\n", "def test_check_number_of_labels():\n    with pytest.raises(ValueError):\n        check_number_of_labels(1, 10)\n    with pytest.raises(ValueError):\n        check_number_of_labels(10, 10)\n    check_number_of_labels(2, 10)  # No exception expected\n", "def test_silhouette_samples_edge_cases():\n    # Test edge cases for silhouette_samples\n    # Test case with single sample\n    X = [[0, 0]]\n    labels = [0]\n    with pytest.raises(ValueError):\n        silhouette_samples(X, labels)\n\n    # Test case with two samples in same cluster\n    X = [[0, 0], [0, 0]]\n    labels = [0, 0]\n    result = silhouette_samples(X, labels)\n    assert_array_equal(result, [1, 1])\n\n    # Test case with two samples in different clusters\n    X = [[0, 0], [1, 1]]\n    labels = [0, 1]\n    result = silhouette_samples(X, labels)\n    assert_array_equal(result, [1, 1])\n\n    # Test case with multiple clusters and samples\n    X = [[0, 0], [0, 1], [1, 0], [1, 1], [2, 2]]\n    labels = [0, 0, 0, 1, 2]\n    result = silhouette_samples(X, labels)\n    assert not np.isnan(result).any()\n\n    # Test case with identical samples in different clusters\n    X = [[0, 0], [0, 0], [1, 1], [1, 1]]\n    labels = [0, 1, 0, 1]\n    result = silhouette_samples(X, labels)\n    assert_array_equal(result, [1, 1, 1, 1])\n", "def test_silhouette_samples_value():\n    # Test the silhouette_samples function returns the expected values\n    X = np.array([[1, 2], [1, 4], [1, 0],\n                  [10, 2], [10, 4], [10, 0]])\n    labels = np.array([0, 0, 0, 1, 1, 1])\n    expected = np.array([0.375, 0.375, 0.375, -0.375, -0.375, -0.375])\n    result = silhouette_samples(X, labels)\n    assert_array_equal(result, expected)\n\n", "def test_davies_bouldin_score_with_zeros():\n    \"\"\"Ensure Davies-Bouldin score calculation does not fail with zero\n    cluster dispersion.\"\"\"\n    X = np.array([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4]])\n    labels = [0, 0, 1, 1, 1]\n    # The intra-cluster dispersion for the first cluster is zero, but\n    # the score should still be computable.\n    pytest.approx(davies_bouldin_score(X, labels), 0.0)\n\n    # Similar case, but with the cluster of size one first\n    labels = [0, 1, 1, 1, 1]\n    pytest.approx(davies_bouldin_score(X, labels), 1.0)\n"], "sample_392": ["    def test_from_db_value_json_object(self):\n        value = '{\"key\": \"value\"}'\n        self.assertEqual(\n            JSONField().from_db_value(value, None, connection=connection),\n            {\"key\": \"value\"},\n        )\n", "def test_key_transform_numeric_lookup(self):\n    values = [\n        {\"ord\": 93},\n        {\"ord\": 22.1},\n        {\"ord\": -1},\n        {\"ord\": 21.931902},\n        {\"ord\": -100291029},\n    ]\n    for field_name in [\"value\", \"value_custom\"]:\n        with self.subTest(field=field_name):\n            for i, value in enumerate(values):\n                NullableJSONModel.objects.create(**{field_name: value})\n            query = NullableJSONModel.objects.filter(\n                **{\"%s__ord__gt\" % field_name: values[0][\"ord\"]}\n            )\n            expected = [self.objs[i + 5] for i in range(len(values) - 1)]\n            self.assertSequenceEqual(query, expected)\n", "    def test_key_transform_with_unsupported_database(self):\n        class UnsupportedDatabaseBackend:\n            features = mock.Mock(supports_json_field=True)\n\n        connection.features = UnsupportedDatabaseBackend()\n\n        key_transform = KeyTransform(\"test\", \"value\")\n\n        msg = \"Lookup type key is not supported on this database backend.\"\n        with self.assertRaisesMessage(NotSupportedError, msg):\n            key_transform.as_sql()\n", "    def test_key_transform_int_key(self):\n        obj = NullableJSONModel.objects.create(value=[{\"a\": 1}, {\"b\": 2}])\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__1__b=2),\n            [obj],\n        )\n", "    def test_database_vendor_check(self):\n        with self.settings(DATABASES={\n            'default': {\n                'ENGINE': 'django.db.backends.sqlite3',\n                'TEST': {\n                    'NAME': ':memory:',\n                },\n            },\n            'other': {\n                'ENGINE': 'django.db.backends.postgresql',\n                'NAME': 'test',\n                'USER': 'test',\n                'PASSWORD': 'test',\n            },\n        }):\n            # Create a model with a JSONField.\n            class MyModel(models.Model):\n                json = models.JSONField()\n\n            # Create a second model with a JSONField and required vendor.\n            class MyRequiredModel(models.Model):\n                json = models.JSONField()\n                class Meta:\n                    required_db_vendor = 'postgresql'\n\n            # Test the check with the default database (sqlite).\n            errors = MyModel.check(databases=['default'])\n            self.assertEqual(len(errors), 1)\n            self.assertEqual(errors[0].msg, \"%s does not support JSONFields.\" % \"sqlite\")\n\n            # Test the check with the other database (postgresql).\n            errors = MyModel.check(databases=['other'])\n            self.assertEqual(len(errors), 0)\n\n            # Test the check with the default database (sqlite) and required vendor.\n            errors = MyRequiredModel.check(databases=['default'])\n            self.assertEqual(len(errors), 1)\n            self.assertEqual(errors[0].msg, \"%s does not support JSONFields.\" % \"sqlite\")\n\n            # Test the check with the other database (postgresql) and required vendor.\n            errors = MyRequiredModel.check(databases=['other'])\n            self.assertEqual(len(errors), 0)\n"], "sample_910": ["def test_LogCollector(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    collector = logging.LogCollector()\n\n    logger.warning('message1')\n    logger.info('message2')\n    logger.critical('message3')\n\n    with collector.collect():\n        logger.warning('message4')\n        logger.info('message5')\n        logger.critical('message6')\n\n    assert len(collector.logs) == 3\n    assert collector.logs[0].getMessage() == 'WARNING: message4'\n    assert collector.logs[1].getMessage() == 'message5'\n    assert collector.logs[2].getMessage() == 'CRITICAL: message6'\n", "def test_log_collector(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    collector = logging.LogCollector()\n\n    with collector.collect():\n        logger.info('message1')\n        logger.warning('message2')\n        logger.debug('message3')\n\n    assert len(collector.logs) == 3\n\n    for i, log in enumerate(collector.logs):\n        if i == 0:\n            assert log.levelname == 'INFO'\n            assert 'message1' in log.getMessage()\n        elif i == 1:\n            assert log.levelname == 'WARNING'\n            assert 'message2' in log.getMessage()\n        elif i == 2:\n            assert log.levelname == 'DEBUG'\n            assert 'message3' in log.getMessage()\n", "def test_pending_logging(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    logger.warning('message1')\n    with logging.pending_logging() as memhandler:\n        # not logged yet (bufferred) in here\n        logger.warning('message2')\n        logger.warning('message3')\n        assert 'WARNING: message1' in warning.getvalue()\n        assert 'WARNING: message2' not in warning.getvalue()\n        assert 'WARNING: message3' not in warning.getvalue()\n\n        # clear pending logs\n        memhandler.clear()\n        assert memhandler.flushTo(logging.getLogger(NAMESPACE)) is None\n\n    # actually logged as ordered\n    assert 'WARNING: message1' in warning.getvalue()\n    assert 'WARNING: message2' not in warning.getvalue()\n    assert 'WARNING: message3' not in warning.getvalue()\n", "def test_log_collector(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    collector = logging.LogCollector()\n\n    with collector.collect():\n        logger.info('message1')\n        logger.warning('message2', location='index')\n        logger.error('message3', type='test', subtype='crash')\n\n    logs = collector.logs\n\n    assert len(logs) == 3\n\n    assert logs[0].levelname == 'INFO'\n    assert logs[0].msg == 'message1'\n    assert logs[0].location is None\n\n    assert logs[1].levelname == 'WARNING'\n    assert logs[1].msg == 'message2'\n    assert logs[1].location == 'index.txt:'\n\n    assert logs[2].levelname == 'ERROR'\n    assert logs[2].msg == 'message3'\n    assert logs[2].type == 'test'\n    assert logs[2].subtype == 'crash'\n", "def test_convert_serializable_records():\n    records = [\n        logging.LogRecord('sphinx', logging.INFO, 'message1', 1, 'message1 %s', ('arg1',), None),\n        logging.LogRecord('sphinx', logging.WARNING, 'message2', 2, 'message2 %s', ('arg2',), None),\n        logging.LogRecord('sphinx', logging.ERROR, 'message3', 3, 'message3 %s', ('arg3',), None),\n    ]\n\n    logging.convert_serializable(records)\n\n    for record in records:\n        assert record.msg == record.getMessage()\n        assert record.args == ()\n        assert not hasattr(record, 'location')\n"], "sample_173": ["def test_adapt_decimalfield_value(self):\n    value = decimal.Decimal('3.14')\n    max_digits = 10\n    decimal_places = 2\n    self.assertEqual(\n        self.ops.adapt_decimalfield_value(value, max_digits, decimal_places),\n        utils.format_number(value, max_digits, decimal_places)\n    )\n", "    def test_window_frame_rows_start_end(self):\n        start = -5\n        end = 10\n        start_sql, end_sql = self.ops.window_frame_rows_start_end(start, end)\n        self.assertEqual(start_sql, '%d %s' % (abs(start), self.ops.PRECEDING))\n        self.assertEqual(end_sql, '%d %s' % (end, self.ops.FOLLOWING))\n", "    def test_year_lookup_bounds_for_date_field(self):\n        year = 2000\n        bounds = self.ops.year_lookup_bounds_for_date_field(year)\n        self.assertEqual(len(bounds), 2)\n        self.assertIsInstance(bounds[0], str)\n        self.assertIsInstance(bounds[1], str)\n", "    def test_adapt_decimalfield_value(self):\n        value = decimal.Decimal('123.456')\n        max_digits = 5\n        decimal_places = 2\n        self.assertEqual(\n            self.ops.adapt_decimalfield_value(value, max_digits, decimal_places),\n            self.ops.adapt_decimalfield_value(value)\n        )\n", "def test_validate_autopk_value(self):\n    self.assertEqual(self.ops.validate_autopk_value(5), 5)\n\n        self.assertEqual(self.ops.force_no_ordering(), [])\n"], "sample_903": ["def test_tsne_with_different_initializations():\n    \"\"\"Test TSNE with different initialization methods\"\"\"\n    random_state = check_random_state(0)\n    n_components = 2\n    X = random_state.randn(50, n_components).astype(np.float32)\n    \n    # Test PCA initialization\n    tsne_pca = TSNE(n_components=n_components, init='pca', random_state=0)\n    X_embedded_pca = tsne_pca.fit_transform(X)\n    assert np.all(np.isfinite(X_embedded_pca))\n    \n    # Test random initialization\n    tsne_random = TSNE(n_components=n_components, init='random', random_state=0)\n    X_embedded_random = tsne_random.fit_transform(X)\n    assert np.all(np.isfinite(X_embedded_random))\n    \n    # Test custom initialization\n    custom_init = np.random.rand(50, n_components).astype(np.float32)\n    tsne_custom = TSNE(n_components=n_components, init=custom_init, random_state=0)\n    X_embedded_custom = tsne_custom.fit_transform(X)\n    assert np.all(np.isfinite(X_embedded_custom))\n", "def test_tsne_with_early_exaggeration_and_different_metrics():\n    \"\"\"Make sure that TSNE with early exaggeration works for different distance metrics\"\"\"\n    random_state = check_random_state(0)\n    n_components_original = 3\n    n_components_embedding = 2\n    X = random_state.randn(50, n_components_original).astype(np.float32)\n    metrics = ['manhattan', 'cosine']\n    for metric in metrics:\n        tsne = TSNE(metric=metric, n_components=n_components_embedding,\n                    random_state=0, early_exaggeration=10.0)\n        X_embedded = tsne.fit_transform(X)\n        assert np.all(np.isfinite(X_embedded))\n", "def test_tsne_with_mismatched_metric_and_input():\n    # Ensure that TSNE checks for mismatched metric and input\n    random_state = check_random_state(0)\n    X = random_state.randn(50, 2)\n    D = pairwise_distances(X)\n    tsne = TSNE(metric=\"cosine\")\n    assert_raises_regexp(ValueError, \"Unknown metric cosine.*\",\n                         tsne.fit_transform, D)\n    tsne = TSNE(metric=\"euclidean\")\n    assert_warns(DeprecationWarning, tsne.fit_transform, D)\n    tsne = TSNE(metric=\"precomputed\")\n    assert_warns(DeprecationWarning, tsne.fit_transform, X)\n", "def test_tsne_with_sparse_input_and_precomputed_metric():\n    # Test t-SNE with sparse input and precomputed distance metric\n    random_state = check_random_state(0)\n    n_samples = 100\n    n_features = 10\n    X = random_state.randn(n_samples, n_features).astype(np.float32)\n    X_csr = sp.csr_matrix(X)\n\n    # Test that using a precomputed distance metric with sparse input\n    # results in the same embedding as using the exact method with sparse input\n    tsne_exact = TSNE(n_components=2, method='exact', random_state=0,\n                       metric='euclidean')\n    tsne_precomputed = TSNE(n_components=2, method='exact', random_state=0,\n                             metric='precomputed')\n\n    X_embedded_exact = tsne_exact.fit_transform(X_csr)\n    X_embedded_precomputed = tsne_precomputed.fit_transform(pairwise_distances(X_csr))\n\n    assert_array_almost_equal(X_embedded_exact, X_embedded_precomputed, decimal=5)\n", "def test_catch_edge_case_barnes_hut_tsne_gradient():\n    # Test the Barnes-Hut t-SNE gradient for an edge case where n_components=1\n    # and n_samples=2.\n    random_state = check_random_state(0)\n    n_samples = 2\n    n_features = 5\n    n_components = 1\n    degrees_of_freedom = float(n_components - 1.0)\n\n    distances = random_state.randn(n_samples, n_features)\n    distances = distances.astype(np.float32)\n    distances = abs(distances.dot(distances.T))\n    np.fill_diagonal(distances, 0.0)\n    params = random_state.randn(n_samples, n_components)\n    P = _joint_probabilities(distances, perplexity=1, verbose=0)\n    grad_bh = _kl_divergence_bh(params, P, degrees_of_freedom, n_samples,\n                                n_components)[1]\n    assert np.all(np.isfinite(grad_bh))\n"], "sample_510": ["def test_subplot_remove_and_reuse():\n    fig = plt.figure()\n    ax1 = fig.add_subplot(121)\n    ax2 = fig.add_subplot(122)\n    ax1.remove()\n    ax3 = fig.add_subplot(121)\n    assert ax3 is not ax1\n    assert ax1 not in fig.axes\n    assert ax3 in fig.axes\n", "def test_twiny_twinx():\n    fig, ax = plt.subplots()\n    ax1 = ax.twinx()\n    assert ax1 is not ax\n    ax2 = ax.twinx()\n    assert ax1 is not ax2\n    assert ax1.get_shared_y_axes().get_siblings(ax1) == {ax, ax2}\n    ax3 = ax.twiny()\n    assert ax3 is not ax\n    assert ax3 is not ax1\n    assert ax3 is not ax2\n    assert ax3.get_shared_x_axes().get_siblings(ax3) == {ax, ax1, ax2}\n", "def test_xkcd_context():\n    with plt.xkcd():\n        fig = plt.figure()\n        assert fig.get_label() == \"\"\n        ax = fig.add_subplot(111)\n        assert ax.get_label() == \"\"\n    fig1 = plt.figure()\n    assert fig1.get_label() != \"\"\n", "def test_subplots_2d_sharex_sharey():\n    # Test sharing x and y axes when creating 2D subplots.\n    fig, axs = plt.subplots(2, 2, sharex='all', sharey='all')\n    assert axs[0, 0].get_shared_x_axes().get_siblings(axs[0, 0]) == {\n        axs[0, 0], axs[0, 1], axs[1, 0], axs[1, 1]}\n    assert axs[0, 0].get_shared_y_axes().get_siblings(axs[0, 0]) == {\n        axs[0, 0], axs[0, 1], axs[1, 0], axs[1, 1]}\n", "def test_subplots():\n    # Create subplots with shared axes.\n    fig, axs = plt.subplots(2, 2, sharex='col', sharey='row')\n    for ax in axs.flat:\n        ax.plot([1, 2, 3])\n\n    # Check that the x ticks are only on the bottom row.\n    assert axs[0, 0].tick_params(axis='x', labelbottom=False)\n    assert axs[0, 1].tick_params(axis='x', labelbottom=False)\n    assert axs[1, 0].tick_params(axis='x', labelbottom=True)\n    assert axs[1, 1].tick_params(axis='x', labelbottom=True)\n\n    # Check that the y ticks are only on the first column.\n    assert axs[0, 0].tick_params(axis='y', labelleft=True)\n    assert axs[1, 0].tick_params(axis='y', labelleft=True)\n    assert axs[0, 1].tick_params(axis='y', labelleft=False)\n    assert axs[1, 1].tick_params(axis='y', labelleft=False)\n\n    # Check that changing the ticks on one subplot changes all subplots.\n    axs[0, 0].set_xticks([1, 2, 3])\n    assert axs[0, 1].get_xticks() == [1, 2, 3]\n    axs[0, 0].set_yticks([1, 2, 3])\n    assert axs[1, 0].get_yticks() == [1, 2, 3]\n\n    # Check that sharex and sharey work independently.\n    fig, axs = plt.subplots(2, 2, sharex=True)\n    axs[0, 0].set_xticks([1, 2, 3])\n    assert axs[0, 1].get_xticks() == [1, 2, 3]\n    axs[0, 0].set_yticks([1, 2, 3])\n    assert axs[1, 0].get_yticks() != [1, 2, 3]\n\n    fig, axs = plt.subplots(2, 2, sharey=True)\n    axs[0, 0].set_yticks([1, 2, 3])\n    assert axs[1, 0].get_yticks() == [1, 2,"], "sample_165": ["    def test_model_to_dict(self):\n        # Create an instance of ChoiceModel\n        instance = ChoiceModel.objects.create(name='a')\n        \n        # Test that model_to_dict returns a dictionary with the correct values\n        data = model_to_dict(instance)\n        self.assertEqual(data['name'], 'a')\n        \n        # Test that model_to_dict returns a dictionary with the correct values when fields are specified\n        data = model_to_dict(instance, fields=['name'])\n        self.assertEqual(data['name'], 'a')\n        \n        # Test that model_to_dict returns a dictionary with the correct values when exclude is specified\n        data = model_to_dict(instance, exclude=['name'])\n        self.assertEqual(len(data), 0)\n", "    def test_base_model_form(self):\n        from django.db import models\n        class TestModel(models.Model):\n            name = models.CharField(max_length=10)\n\n        class TestForm(BaseModelForm):\n            class Meta:\n                model = TestModel\n                fields = ['name']\n\n        form = TestForm(data={'name': 'test'})\n        self.assertTrue(form.is_valid())\n        instance = form.save(commit=False)\n        self.assertEqual(instance.name, 'test')\n", "    def test_model_form_fields(self):\n        from ..models import Book\n\n        class BookForm(ModelForm):\n            class Meta:\n                model = Book\n                fields = ['author', 'title']\n\n        form = BookForm({'author': 'John Doe', 'title': 'Example Book'})\n        self.assertTrue(form.is_valid())\n", "    def test_model_form(self):\n        class TestModelForm(ModelForm):\n            class Meta:\n                model = ChoiceModel\n                fields = ('name',)\n\n        f = TestModelForm(data={'name': ''})\n        self.assertFormErrors(['This field cannot be blank.'], f.is_valid)\n", "    def test_model_form_fields_for_model(self):\n        from django.db import models\n        from ..models import ChoiceModel\n\n        class TestModel(models.Model):\n            name = models.CharField(max_length=10)\n            email = models.EmailField()\n\n        # Test that a ModelForm can be created with fields=None and exclude=None\n        class TestForm(ModelForm):\n            class Meta:\n                model = TestModel\n\n        # Test that fields and exclude are validated\n        with self.assertRaises(ImproperlyConfigured):\n            class TestForm(ModelForm):\n                class Meta:\n                    model = TestModel\n                    fields = 'name'\n                    exclude = 'name'\n\n        # Test that fields_for_model returns all fields\n        fields = fields_for_model(TestModel)\n        self.assertEqual(len(fields), 2)\n        self.assertEqual(fields['name'].__class__.__name__, 'CharField')\n        self.assertEqual(fields['email'].__class__.__name__, 'EmailField')\n\n        # Test that fields_for_model returns only specified fields\n        fields = fields_for_model(TestModel, fields=['name'])\n        self.assertEqual(len(fields), 1)\n        self.assertEqual(fields['name'].__class__.__name__, 'CharField')\n\n        # Test that fields_for_model excludes specified fields\n        fields = fields_for_model(TestModel, exclude=['email'])\n        self.assertEqual(len(fields), 1)\n        self.assertEqual(fields['name'].__class__.__name__, 'CharField')\n\n        # Test that fields_for_model raises a FieldError if a field is not found\n        with self.assertRaises(FieldError):\n            fields_for_model(TestModel, fields=['nonexistent'])\n\n        # Test that ModelForm can handle a model with a ForeignKey to another model\n        class AnotherModel(models.Model):\n            name = models.CharField(max_length=10)\n\n        class TestModel(models.Model):\n            name = models.CharField(max_length=10)\n            another = models.ForeignKey(AnotherModel, on_delete=models.CASCADE)\n\n        class TestForm(ModelForm):\n            class Meta:\n                model = TestModel\n\n        # Test that fields_for_model raises a FieldError if a field is non-editable\n        class TestModel(models.Model):\n            name = models.CharField(max_length=10, editable=False)\n\n        with self.assertRaises(FieldError):\n            fields_for_model(TestModel, fields=['name'])\n\n        # Test that fields_for_model ignores non-editable fields by default\n        fields = fields_for_model(TestModel)\n        self.assertEqual(len(fields), 0)\n\n       "], "sample_1008": ["def test_orient_respects_parent_class():\n    class MyReferenceFrame(ReferenceFrame):\n        pass\n    B = MyReferenceFrame('B')\n    N = ReferenceFrame('N')\n    B.orient(N, 'Axis', [0, N.x])\n    assert isinstance(B, MyReferenceFrame)\n", "def test_orient():\n    N = ReferenceFrame('N')\n    A = ReferenceFrame('A')\n    \n    A.orient(N, 'Body', [0, 0, 0], '123')\n    assert N.ang_vel_in(A) == 0\n    assert A.ang_vel_in(N) == 0\n\n    A.orient(N, 'Body', [0, 0, 0], '231')\n    assert N.ang_vel_in(A) == 0\n    assert A.ang_vel_in(N) == 0\n\n    A.orient(N, 'Body', [0, 0, 0], '312')\n    assert N.ang_vel_in(A) == 0\n    assert A.ang_vel_in(N) == 0\n\n    A.orient(N, 'Body', [0, 0, 0], '321')\n    assert N.ang_vel_in(A) == 0\n    assert A.ang_vel_in(N) == 0\n\n    A.orient(N, 'Space', [0, 0, 0], '123')\n    assert N.ang_vel_in(A) == 0\n    assert A.ang_vel_in(N) == 0\n\n    A.orient(N, 'Space', [0, 0, 0], '231')\n    assert N.ang_vel_in(A) == 0\n    assert A.ang_vel_in(N) == 0\n\n    A.orient(N, 'Space', [0, 0, 0], '312')\n    assert N.ang_vel_in(A) == 0\n    assert A.ang_vel_in(N) == 0\n\n    A.orient(N, 'Space', [0, 0, 0], '321')\n    assert N.ang_vel_in(A) == 0\n    assert A.ang_vel_in(N) == 0\n\n    A.orient(N, 'Quaternion', [1, 0, 0, 0])\n    assert N.ang_vel_in(A) == 0\n    assert A.ang_vel_in(N) == 0\n\n    A.orient(N, 'Axis', [0, N.x])\n    assert N.ang_vel_in(A) == 0\n    assert A.ang_vel_in(N) == 0\n\n    A.orient(N, 'DCM', Matrix(eye(3)))\n    assert N.ang_vel_in(A", "def test_variable_map_with_diff_frame_and_simp():\n    q = dynamicsymbols('q')\n    N = ReferenceFrame('N')\n    B = N.orientnew('B', 'Axis', [q, N.z])\n    Vector.simp = False\n    assert B.variable_map(N) == {B[0]: A[0]*cos(q) - A[1]*sin(q),\n                                 B[1]: A[0]*sin(q) + A[1]*cos(q), B[2]: A[2]}\n    Vector.simp = True\n    assert B.variable_map(N) == {B[0]: A[0]*cos(q) - A[1]*sin(q),\n                                 B[1]: A[0]*sin(q) + A[1]*cos(q), B[2]: A[2]}\n", "def test_orient_exception_handling():\n    N = ReferenceFrame('N')\n    A = ReferenceFrame('A')\n\n    # Test invalid rotation types\n    with raises(ValueError):\n        A.orient(N, 'Invalid', [1, 2, 3], '123')\n    with raises(ValueError):\n        A.orient(N, 'BODY', [1, 2], '123')\n    with raises(ValueError):\n        A.orient(N, 'AXIS', [1, 2, 3, 4])\n    with raises(ValueError):\n        A.orient(N, 'QUATERNION', [1, 2, 3])\n    with raises(ValueError):\n        A.orient(N, 'DCM', 1)\n\n    # Test invalid rotation orders\n    with raises(ValueError):\n        A.orient(N, 'BODY', [1, 2, 3], 'invalid')\n    with raises(ValueError):\n        A.orient(N, 'SPACE', [1, 2, 3], 'invalid')\n\n    # Test invalid amounts\n    with raises(TypeError):\n        A.orient(N, 'BODY', ['a', 2, 3], '123')\n    with raises(TypeError):\n        A.orient(N, 'AXIS', [1, 'a'])\n    with raises(TypeError):\n        A.orient(N, 'QUATERNION', [1, 2, 3, 'a'])\n    with raises(TypeError):\n        A.orient(N, 'DCM', 'a')\n\n    # Test invalid parent frame\n    with raises(VectorTypeError):\n        A.orient(1, 'BODY', [1, 2, 3], '123')\n", "def test_orient():\n    q1, q2, q3 = dynamicsymbols('q1 q2 q3')\n    N = ReferenceFrame('N')\n    A = ReferenceFrame('A')\n    B = ReferenceFrame('B')\n\n    # Body orientation\n    A.orient(N, 'Body', [q1, q2, q3], '123')\n    assert A.ang_vel_in(N) == ((sin(q2)*sin(q3)*q1.diff() + cos(q3)*q2.diff())*A.x +\n        (sin(q2)*cos(q3)*q1.diff() - sin(q3)*q2.diff())*A.y + (cos(q2)*q1.diff() + q3.diff())*A.z)\n\n    # Space orientation\n    B.orient(N, 'Space', [q1, q2, q3], '123')\n    assert B.ang_vel_in(N) == ((cos(q2)*q3.diff() + sin(q2)*sin(q3)*q2.diff())*B.x +\n        (-sin(q2)*q3.diff() + cos(q2)*sin(q3)*q2.diff())*B.y + (cos(q3)*q2.diff() + q1.diff())*B.z)\n\n    # Quaternion orientation\n    A.orient(N, 'Quaternion', (cos(q1/2), sin(q1/2)*sin(q2), sin(q1/2)*sin(q3), sin(q1/2)*cos(q2)))\n    assert A.ang_vel_in(N) == 2 * (sin(q2)*cos(q3)*q1.diff()*A.x + (-sin(q2)*sin(q3)*q1.diff() + 2*q2.diff())*A.y + (-cos(q2)*sin(q3)*q1.diff() + 2*q3.diff())*A.z)\n\n    # Axis orientation\n    B.orient(N, 'Axis', [q1, N.x + N.y])\n    assert B.ang_vel_in(N) == q1.diff() * (N.x + N.y).normalize()\n"], "sample_1183": ["def test_FracField_from_expr():\n    F, x,y = field(\"x,y\", ZZ)\n    f = (x**2 + y**2)/(x + 1)\n    g = (x**2 + y**2)/4\n    h =  x**2 + y**2\n\n    assert F.from_expr(f) == (x**2 + y**2)/(x + 1)\n    assert F.from_expr(g) == (x**2 + y**2)/4\n    assert F.from_expr(h) == x**2 + y**2\n\n    raises(ValueError, lambda: F.from_expr(sin(1)))\n\n    F, x,y = field(\"x,y\", QQ)\n    f = (x**2 + y**2)/(x + 1)\n    g = (x**2 + y**2)/4\n    h =  x**2 + y**2\n\n    assert F.from_expr(f) == (x**2 + y**2)/(x + 1)\n    assert F.from_expr(g) == (x**2 + y**2)/4\n    assert F.from_expr(h) == x**2 + y**2\n\n    raises(ValueError, lambda: F.from_expr(sin(1)))\n\n    F, x,y = field(\"x,y\", RR)\n    f = (x**2 + y**2)/(x + 1)\n    g = (x**2 + y**2)/4\n    h =  x**2 + y**2\n\n    assert F.from_expr(f) == (x**2 + y**2)/(x + 1)\n    assert F.from_expr(g) == (x**2 + y**2)/4\n    assert F.from_expr(h) == x**2 + y**2\n\n    assert F.from_expr(sin(1)) == sin(1)\n", "def test_FracField_fields():\n    K = field(\"x\", ZZ)\n    assert K.symbols == (x,)\n    assert K.ngens == 1\n    assert K.domain == ZZ\n    assert K.order == \"lex\"\n\n    K = field(\"x, y\", ZZ)\n    assert K.symbols == (x, y)\n    assert K.ngens == 2\n    assert K.domain == ZZ\n    assert K.order == \"lex\"\n\n    K = field(\"x\", ZZ, \"grlex\")\n    assert K.symbols == (x,)\n    assert K.ngens == 1\n    assert K.domain == ZZ\n    assert K.order == \"grlex\"\n\n    K = field(\"x\", QQ)\n    assert K.symbols == (x,)\n    assert K.ngens == 1\n    assert K.domain == QQ\n    assert K.order == \"lex\"\n\n    K = field(\"x\", RR)\n    assert K.symbols == (x,)\n    assert K.ngens == 1\n    assert K.domain == RR\n    assert K.order == \"lex\"\n\n    K = field(\"x\", CC)\n    assert K.symbols == (x,)\n    assert K.ngens == 1\n    assert K.domain == CC\n    assert K.order == \"lex\"\n\n    K = field(\"x\", ZZ[x])\n    assert K.symbols == (x,)\n    assert K.ngens == 1\n    assert K.domain == ZZ[x]\n    assert K.order == \"lex\"\n\n    K = field(\"x\", QQ[x])\n    assert K.symbols == (x,)\n    assert K.ngens == 1\n    assert K.domain == QQ[x]\n    assert K.order == \"lex\"\n\n    K = field(\"x\", ZZ[x, y])\n    assert K.symbols == (x,)\n    assert K.ngens == 1\n    assert K.domain == ZZ[x, y]\n    assert K.order == \"lex\"\n\n    K = field(\"x\", QQ[x, y])\n    assert K.symbols == (x,)\n    assert K.ngens == 1\n    assert K.domain == QQ[x, y]\n    assert K.order == \"lex\"\n\n    K = field(\"x\", ZZ.frac_field(y))\n    assert K.symbols == (x,)\n    assert K.ngens == 1\n    assert K.domain == ZZ.frac_field(y)\n    assert K.order == \"lex\"\n\n    K = field(\"x\",", "def test_FracField_operations():\n    F, x, y = field(\"x, y\", ZZ)\n    assert F(1) + F(2) == F(3)\n    assert F(1) - F(2) == F(-1)\n    assert F(1) * F(2) == F(2)\n    assert F(1) / F(2) == F(Rational(1, 2))\n    assert F(x) + F(y) == F(x + y)\n    assert F(x) - F(y) == F(x - y)\n    assert F(x) * F(y) == F(x*y)\n    assert F(x) / F(y) == F(x/y)\n    assert F(x) ** 2 == F(x**2)\n    assert F(x) ** -2 == F(1/x**2)\n    assert F(x) ** Rational(1, 2) == F(sqrt(x))\n    raises(NotImplementedError, lambda: F(x).compose(y, 2))\n", "def test_FracElement_from_expr():\n    _, x, y, z = field(\"x,y,z\", ZZ)\n\n    assert _.from_expr(x**2 + 1) == (x**2 + 1)/(1)\n    assert _.from_expr(1/(x + 1)) == (1)/(x + 1)\n    assert _.from_expr(1/(x**2 + y**2)) == (1)/(x**2 + y**2)\n    assert _.from_expr((x**2 + 1)/(y + 1)) == (x**2 + 1)/(y + 1)\n    assert _.from_expr((x + 1)/(x + 1)) == (1)/(1)\n    assert _.from_expr((x + 1)**2/(x + 1)) == (x + 1)/(1)\n    assert _.from_expr((x + 1)**2/((x + 1)*(y + 1))) == (x + 1)/(y + 1)\n    assert _.from_expr(exp(1/x + log(x)/3)) == exp(1/x + log(x)/3)/(1)\n    assert _.from_expr(sin(x)) == sin(x)/(1)\n\n    raises(ValueError, lambda: _.from_expr(x**2 + y**2))\n    raises(ValueError, lambda: _.from_expr(oo))\n    raises(ValueError, lambda: _.from_expr(sqrt(x)))\n    raises(ValueError, lambda: _.from_expr(sin(x)))\n    raises(ValueError, lambda: _.from_expr(exp(x)))\n", "def test_FracField():\n    F, x, y = field(\"x,y\", ZZ)\n\n    assert F.to_ring() == ZZ[x, y]\n    assert F.to_domain() == F\n\n    a = F.new(x + 1, x + 2)\n    assert a.numer == x + 1\n    assert a.denom == x + 2\n\n    assert a.parent() == F\n\n    assert hash(a) == hash((F, x + 1, x + 2))\n\n    assert F.field_new(x + 1) == a\n    assert F.field_new((x + 1, x + 2)) == a\n    assert F.field_new(Poly(x + 1, x, ZZ) / Poly(x + 2, x, ZZ)) == a\n\n    assert F.from_expr(x + 1) == a\n    assert F.from_expr((x + 1) / (x + 2)) == a\n\n    assert F(1).to_poly() == 1\n    assert F(x).to_poly() == x\n    assert F(1/x).to_poly() == 1/x\n\n    assert a.diff(x) == (x + 2) / ((x + 2)**2)\n    assert a.diff(y) == 0\n\n    assert a(1, 2) == 3 / 5\n    assert a([x, y], [1, 2]) == 3 / 5\n\n    assert a.subs({x: 1, y: 2}) == 3 / 5\n    assert a.subs([(x, 1), (y, 2)]) == 3 / 5\n\n    assert a.compose(x, 1) == 1 / 3\n    raises(NotImplementedError, lambda: a.compose(x, y))\n"], "sample_533": ["def test_contour_labeler_event_handler():\n    x, y = np.meshgrid(np.arange(0, 10), np.arange(0, 10))\n    z = np.max(np.dstack([abs(x), abs(y)]), 2)\n    fig, ax = plt.subplots()\n    cs = ax.contour(x, y, z)\n\n    # Test escape key press\n    event = mpl.backend_bases.LocationEvent('key_press_event', fig.canvas,\n                                           x=0, y=0, key='escape')\n    _contour_labeler_event_handler(cs, inline=True, inline_spacing=5, event=event)\n\n    # Test enter key press\n    event = mpl.backend_bases.LocationEvent('key_press_event', fig.canvas,\n                                           x=0, y=0, key='enter')\n    _contour_labeler_event_handler(cs, inline=True, inline_spacing=5, event=event)\n\n    # Test middle mouse button click\n    event = mpl.backend_bases.LocationEvent('button_press_event', fig.canvas,\n                                           x=0, y=0, button=MouseButton.MIDDLE)\n    _contour_labeler_event_handler(cs, inline=True, inline_spacing=5, event=event)\n\n    # Test right mouse button click\n    event = mpl.backend_bases.LocationEvent('button_press_event', fig.canvas,\n                                           x=0, y=0, button=MouseButton.RIGHT)\n    _contour_labeler_event_handler(cs, inline=True, inline_spacing=5, event=event)\n\n    # Test left mouse button click\n    event = mpl.backend_bases.LocationEvent('button_press_event', fig.canvas,\n                                           x=0, y=0, button=MouseButton.LEFT)\n    _contour_labeler_event_handler(cs, inline=True, inline_spacing=5, event=event)\n", "def test_contourf_legend_elements_levels_beyond_powerlimits():\n    from matplotlib.patches import Rectangle\n    x = np.arange(1, 10)\n    y = x.reshape(-1, 1)\n    h = x * y\n\n    cs = plt.contourf(h, levels=[1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n                      colors=['#FFFF00', '#FF00FF', '#00FFFF', '#00FF00', '#0000FF', '#FF0000', '#800000', '#008000', '#000080', '#800080'],\n                      extend='both')\n    cs.cmap.set_over('red')\n    cs.cmap.set_under('blue')\n    cs.changed()\n    artists, labels = cs.legend_elements()\n    assert labels == ['$x \\\\leq -1e+250s$',\n                      '$1.00e-05 < x \\\\leq 0.01$',\n                      '$0.01 < x \\\\leq 0.10$',\n                      '$0.10 < x \\\\leq 1.00$',\n                      '$1.00 < x \\\\leq 10.00$',\n                      '$10.00 < x \\\\leq 100.00$',\n                      '$100.00 < x \\\\leq 1000.00$',\n                      '$1000.00 < x \\\\leq 10000.00$',\n                      '$x > 1e+250s$']\n    expected_colors = ('blue', '#FFFF00', '#FF00FF', '#00FFFF', '#00FF00', '#0000FF', '#FF0000', '#800000', '#008000', 'red')\n    assert all(isinstance(a, Rectangle) for a in artists)\n    assert all(same_color(a.get_facecolor(), c)\n               for a, c in zip(artists, expected_colors))\n", "def test_contour_labeler_event_handler():\n    x = np.arange(9)\n    z = np.random.random((9, 9))\n\n    fig, ax = plt.subplots()\n    cs = ax.contour(x, x, z)\n\n    # Test mouse press events\n    event = mpl.backend_bases.LocationEvent(name=\"button_press_event\",\n                                          x=0.5, y=0.5, inaxes=ax,\n                                          button=MouseButton.LEFT)\n    _contour_labeler_event_handler(cs, True, 5, event)\n    assert len(cs.labelTexts) == 1\n\n    # Test middle button press event\n    event = mpl.backend_bases.LocationEvent(name=\"button_press_event\",\n                                          x=0.5, y=0.5, inaxes=ax,\n                                          button=MouseButton.MIDDLE)\n    _contour_labeler_event_handler(cs, True, 5, event)\n    assert len(cs.labelTexts) == 1\n\n    # Test right button press event\n    event = mpl.backend_bases.LocationEvent(name=\"button_press_event\",\n                                          x=0.5, y=0.5, inaxes=ax,\n                                          button=MouseButton.RIGHT)\n    _contour_labeler_event_handler(cs, True, 5, event)\n    assert len(cs.labelTexts) == 0\n\n    # Test key press events\n    event = mpl.backend_bases.KeyEvent(name=\"key_press_event\", key=\"enter\")\n    _contour_labeler_event_handler(cs, True, 5, event)\n    assert len(cs.labelTexts) == 0\n\n    event = mpl.backend_bases.KeyEvent(name=\"key_press_event\", key=\"a\")\n    _contour_labeler_event_handler(cs, True, 5, event)\n    assert len(cs.labelTexts) == 1\n\n    # Test that KeyError is not raised when key is None.\n    event = mpl.backend_bases.KeyEvent(name=\"key_press_event\", key=None)\n    _contour_labeler_event_handler(cs, True, 5, event)\n    assert len(cs.labelTexts) == 1\n", "def test_contour_clabel_width():\n    x, y = np.meshgrid(np.arange(0, 10), np.arange(0, 10))\n    z = np.max(np.dstack([abs(x), abs(y)]), 2)\n\n    fig, ax = plt.subplots()\n    cs = ax.contour(x, y, z, levels=[1, 2, 3])\n    labels = ax.clabel(cs, inline_spacing=1)\n    # check that inline spacing works with negative contour values\n    cs = ax.contour(x, y, -z, levels=[1, 2, 3])\n    labels = ax.clabel(cs, inline_spacing=1)\n\n    fig, ax = plt.subplots()\n    cs = ax.contour(x, y, z, levels=[1, 2, 3])\n    labels = ax.clabel(cs, inline_spacing=5)\n    # check that inline spacing works with negative contour values\n    cs = ax.contour(x, y, -z, levels=[1, 2, 3])\n    labels = ax.clabel(cs, inline_spacing=5)\n", "def test_contour_labeler_event_handler():\n    # Test for manual label placement, in particular event handler behavior.\n    x, y = np.meshgrid(np.arange(0, 10), np.arange(0, 10))\n    z = np.max(np.dstack([abs(x), abs(y)]), 2)\n    fig, ax = plt.subplots()\n    cs = ax.contour(x, y, z)\n    pts = np.array([(1.0, 3.0), (1.0, 4.4), (1.0, 6.0)])\n    cs.clabel(manual=pts)\n    pts = np.array([(2.0, 3.0), (2.0, 4.4), (2.0, 6.0)])\n    cs.clabel(manual=pts, fontsize='small', colors=('r', 'g'))\n    assert len(ax.texts) == 6\n    cs.pop_label(-1)\n    assert len(ax.texts) == 5\n    cs.pop_label()\n    assert len(ax.texts) == 4\n"], "sample_655": ["def test_capturemanager_is_capturing(testdir):\n    testdir.makepyfile(\n        \"\"\"\\\n            assert capsys._capture_fixture._capture.is_capturing() == \"fixture capsys\"\n            capsys.disabled()\n            assert capsys._capture_fixture._capture.is_capturing() == False\n            capsys.enabled()\n            assert capsys._capture_fixture._capture.is_capturing() == \"fixture capsys\"\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n", "def test_encode_file_buffer_readline(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import sys\n            import pytest\n            ef = pytest._code.EncodedFile(None, None)\n            pytest.raises(NotImplementedError, ef.buffer.readline)\n    \"\"\"\n    )\n    result = testdir.runpytest(str(p))\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n", "def test_capture_nested_context_managers(testdir, method):\n    testdir.makepyfile(\n        \"\"\"\\\n            import sys\n            with {}:\n                print('outer')\n                with {}:\n                    print('inner')\n            assert {}.readouterr() == ('outer\\\\ninner\\\\n', '')\n        \"\"\".format(\n            \"capfd\" if method == \"fd\" else \"capsys\",\n            \"capfd\" if method == \"fd\" else \"capsys\",\n            \"capfd\" if method == \"fd\" else \"capsys\",\n            \"capfd\" if method == \"fd\" else \"capsys\",\n        )\n    )\n    result = testdir.runpytest_subprocess(\"--capture={}\".format(method))\n    result.assert_outcomes(passed=1)\n", "def test_capturing_binary_mode_sys_and_fd(testdir):\n    if not hasattr(os, \"dup\"):\n        pytest.skip(\"need os.dup\")\n    if sys.version_info < (3,):\n        pytest.skip(\"python2 has no buffer\")\n\n    testdir.makepyfile(\n        \"\"\"\\\n        import sys\n            sys.stdout.buffer.write(b'\\\\xfe\\\\x98\\\\x20')\n            sys.stderr.buffer.write(b'\\\\xfe\\\\x98\\\\x20')\n            import os\n            os.write(1, b'\\\\xfe\\\\x98\\\\x20')\n            os.write(2, b'\\\\xfe\\\\x98\\\\x20')\n    \"\"\"\n    )\n    result = testdir.runpytest_subprocess(\"--capture=sys\")\n    result.assert_outcomes(passed=2)\n\n    result = testdir.runpytest_subprocess(\"--capture=fd\")\n    result.assert_outcomes(passed=2)\n", "def test_capture_fixture_suspension(testdir):\n    \"\"\"Ensure the capture fixture is properly suspended and resumed when used by another fixture (#3033)\"\"\"\n    testdir.makepyfile(\n        \"\"\"\\\n            import pytest\n            import sys\n\n            @pytest.fixture\n                print(\"setup myfixture\")\n                yield\n                out, err = capsys.readouterr()\n                assert out == \"setup myfixture\\\\n\"\n\n                print(\"in test\")\n                out, err = myfixture.readouterr()\n                assert out == \"in test\\\\n\"\n        \"\"\"\n    )\n    result = testdir.runpytest_subprocess()\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n    assert \"setup myfixture\" not in result.stdout.str()\n    assert \"in test\" not in result.stdout.str()\n"], "sample_1185": ["def test_compogen_edge_cases():\n    assert compogen([x], x) == x\n    assert compogen([1], x) == 1\n    assert compogen([sin(x), 1], x) == sin(x)\n    assert compogen([1, sin(x)], x) == sin(x)\n    assert compogen([x, x], x) == x\n    assert compogen([x**2, x**2], x) == x**4\n    raises(TypeError, lambda: compogen([x < 5], x))\n    raises(TypeError, lambda: compogen([x, x < 5], x))\n", "def test_compogen_empty_list():\n    raises(TypeError, lambda: compogen([], x))\n", "def test_decompogen_edge_cases():\n    # test with a constant expression\n    assert decompogen(5, x) == [5]\n    \n    # test with a single variable\n    assert decompogen(x, x) == [x]\n    \n    # test with an expression containing multiple variables\n    assert decompogen(x**2 + y**2, x) == [x**2 + y**2]\n    \n    # test with an expression containing a single variable multiple times\n    assert decompogen(x**2 + x + 1, x) == [x**2 + x + 1, x]\n    \n    # test with an expression containing a power of a single variable\n    assert decompogen(x**3, x) == [x**3]\n    \n    # test with an expression containing a square root of a single variable\n    assert decompogen(sqrt(x), x) == [sqrt(x)]\n    \n    # test with an expression containing an exponential of a single variable\n    assert decompogen(exp(x), x) == [exp(x)]\n    \n    # test with an expression containing a trigonometric function of a single variable\n    assert decompogen(sin(x), x) == [sin(x)]\n    \n    # test with an expression containing an absolute value of a single variable\n    assert decompogen(Abs(x), x) == [Abs(x)]\n", "def test_decompogen_corner_cases():\n    assert decompogen(exp(x), x) == [exp(x)]\n    assert decompogen(exp(x**2), x) == [exp(x), x**2]\n    assert decompogen(sin(x)**2 + cos(x)**2, x) == [sin(x)**2 + cos(x)**2]\n    assert decompogen(sin(x)**2 + cos(x), x) == [x**2 + x, sin(x)]\n    assert decompogen(x + 1, x) == [x + 1]\n    assert decompogen(x - 1, x) == [x - 1]\n    assert decompogen(1/x, x) == [1/x]\n    assert decompogen(-x, x) == [-x]\n    assert decompogen(2*x, x) == [2*x]\n    assert decompogen(2 + 3*x, x) == [2 + 3*x]\n    assert decompogen(2*x + 3, x) == [2*x + 3]\n", "def test_decompogen_and_compogen():\n    f = x**3 + 2*x**2 - x + 1\n    g = x**2 - 3*x + 2\n    decomposed_f = decompogen(f, x)\n    decomposed_g = decompogen(g, x)\n    recomposed_f = compogen(decomposed_f, x)\n    recomposed_g = compogen(decomposed_g, x)\n    assert recomposed_f.equals(f)\n    assert recomposed_g.equals(g)\n"], "sample_381": ["def test_alter_unique_constraint(self):\n    \"\"\"Changing a unique constraint should make a change.\"\"\"\n    before_state = ModelState('testapp', 'Author', [\n        ('id', models.AutoField(primary_key=True)),\n        ('name', models.CharField(max_length=200)),\n    ], {\n        'constraints': [\n            models.UniqueConstraint(fields=['name'], name='unique_name'),\n        ]\n    })\n    after_state = ModelState('testapp', 'Author', [\n        ('id', models.AutoField(primary_key=True)),\n        ('name', models.CharField(max_length=200)),\n    ], {\n        'constraints': [\n            models.UniqueConstraint(fields=['name'], name='unique_author_name'),\n        ]\n    })\n    changes = self.get_changes([before_state], [after_state])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['RemoveConstraint', 'AddConstraint'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, model_name='author', name='unique_name')\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, model_name='author', constraint=models.UniqueConstraint(fields=['name'], name='unique_author_name'))\n\n    # Test when the constraint name is None\n    before_state = ModelState('testapp', 'Author', [\n        ('id', models.AutoField(primary_key=True)),\n        ('name', models.CharField(max_length=200)),\n    ], {\n        'constraints': [\n            models.UniqueConstraint(fields=['name'], name=None),\n        ]\n    })\n    after_state = ModelState('testapp', 'Author', [\n        ('id', models.AutoField(primary_key=True)),\n        ('name', models.CharField(max_length=200)),\n    ], {\n        'constraints': [\n            models.UniqueConstraint(fields=['name'], name='unique_name'),\n        ]\n    })\n    changes = self.get_changes([before_state], [after_state])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['RemoveConstraint', 'AddConstraint'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, model_name='author')\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, model_name='author', constraint=models.UniqueConstraint(fields=['name'], name='unique_name'))\n\n    # Test when the constraint name", "def test_adding_model_with_unique_together_ordering(self):\n    \"\"\"Tests adding a new model with unique_together fields in a specific order.\"\"\"\n    model_state = ModelState('testapp', 'Model', [\n        ('id', models.AutoField(primary_key=True)),\n        ('field1', models.CharField(max_length=200)),\n        ('field2', models.CharField(max_length=200)),\n    ], options={'unique_together': {('field1', 'field2')}})\n    changes = self.get_changes([], [model_state])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"CreateModel\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='Model', options={'unique_together': {('field1', 'field2')}})\n\n    # Create a similar model, but with the unique_together fields in a different order.\n    model_state_different_order = ModelState('testapp', 'Model', [\n        ('id', models.AutoField(primary_key=True)),\n        ('field2', models.CharField(max_length=200)),\n        ('field1', models.CharField(max_length=200)),\n    ], options={'unique_together': {('field2', 'field1')}})\n    changes = self.get_changes([model_state], [model_state_different_order])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterUniqueTogether\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='model', unique_together={('field2', 'field1')})\n", "def test_alter_unique_together_and_indexes_with_through_model(self):\n    \"\"\"\n    Alter unique_together and indexes while considering a through model.\n    \"\"\"\n    changes = self.get_changes(\n        [self.contract, self.publisher, self.author_with_m2m_through],\n        [self.contract, self.publisher, self.author_with_m2m_through],\n        MigrationQuestioner({\"ask_rename\": True})\n    )\n    # Right number/type of migrations?\n    self.assertEqual(len(changes), 0)\n\n    changes = self.get_changes(\n        [self.contract, self.publisher, self.author_with_m2m_through],\n        [\n            self.contract_renamed, self.publisher, self.author_with_renamed_m2m_through,\n        ],\n        MigrationQuestioner({\"ask_rename\": True})\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"RenameModel\", \"AlterField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name=\"Contract\", new_name=\"Deal\")\n\n    changes = self.get_changes(\n        [\n            self.contract,\n            self.publisher,\n            self.author_with_m2m_through,\n            self.publisher_with_author,\n        ],\n        [\n            self.contract_renamed,\n            self.publisher,\n            self.author_with_renamed_m2m_through,\n            self.publisher_with_aardvark_author,\n        ],\n        MigrationQuestioner({\"ask_rename\": True})\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\n        \"RenameModel\", \"AlterField\", \"AlterField\", \"RenameField\",\n    ])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name=\"Contract\", new_name=\"Deal\")\n\n    changes = self.get_changes(\n        [self.contract, self.publisher, self.author_with_m2m_through],\n        [self.contract_renamed, self.publisher, self.author_with_renamed_m2m_through],\n        MigrationQuestioner({\"ask_rename\": True})\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"RenameModel\", \"AlterField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, ", "def test_alter_field_deconstructible_object(self):\n    \"\"\"\n    Deep-deconstructible objects inside fields are correctly identified and\n    deconstructed.\n    \"\"\"\n    class ModelDeconstructibleObject:\n            self.args = args\n            self.kwargs = kwargs\n\n            return (\n                self.__module__ + '.' + self.__class__.__name__,\n                self.args,\n                self.kwargs\n            )\n\n    before_state = ModelState(\n        \"testapp\",\n        \"Model\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"field\", models.CharField(max_length=200, default=ModelDeconstructibleObject())),\n        ],\n    )\n    after_state = ModelState(\n        \"testapp\",\n        \"Model\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"field\", models.CharField(max_length=200, default=ModelDeconstructibleObject())),\n        ],\n    )\n    changes = self.get_changes([before_state], [after_state])\n    # No changes\n    self.assertEqual(changes, {})\n\n    after_state = ModelState(\n        \"testapp\",\n        \"Model\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"field\", models.CharField(max_length=200, default=ModelDeconstructibleObject())),\n        ],\n    )\n    after_state.fields[1].default.args = ('new_arg',)\n    changes = self.get_changes([before_state], [after_state])\n    self.assertEqual(len(changes), 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AlterField'])\n\n    after_state = ModelState(\n        \"testapp\",\n        \"Model\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"field\", models.CharField(max_length=200, default=ModelDeconstructibleObject())),\n        ],\n    )\n    after_state.fields[1].default.kwargs = {'new_kwarg': 'new_value'}\n    changes = self.get_changes([before_state], [after_state])\n    self.assertEqual(len(changes), 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AlterField'])\n", "def test_unique_together_with_check_constraint(self):\n    \"\"\"\n    #24495 - If a model has a check constraint and a unique_together constraint\n    that references a field used in the check constraint, the autodetector\n    must correctly order the operations to remove them.\n    \"\"\"\n    model_state = ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=200)),\n        (\"active\", models.BooleanField()),\n    ], {\n        \"constraints\": [\n            models.CheckConstraint(\n                check=models.Q(active=True) & models.Q(name__startswith='A'),\n                name='active_and_startswith_A',\n            ),\n            models.UniqueConstraint(\n                fields=['name'],\n                name='unique_name',\n            ),\n        ],\n    })\n    changes = self.get_changes([self.author_empty], [model_state])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['CreateModel', 'AddConstraint', 'AddConstraint'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, model_name='author', constraint=model_state.constraints[0])\n    self.assertOperationAttributes(changes, 'testapp', 0, 2, model_name='author', constraint=model_state.constraints[1])\n\n    # Now, test the removal in the reverse order to ensure that the constraint\n    # removal is done in the correct order.\n    changes = self.get_changes([model_state], [self.author_empty])\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['RemoveConstraint', 'RemoveConstraint', 'DeleteModel'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, model_name='author', name='unique_name')\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, model_name='author', name='active_and_startswith_A')\n"], "sample_460": ["    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n", "def test_empty_value_display(self):\n    user = User.objects.get(username=\"super\")\n    response = self.client.get(reverse(\"admin:admin_views_user_changelist\"))\n    self.assertContains(\n        response,\n        '<td class=\"field-is_staff\">%s</td>' % self.site.empty_value_display,\n        html=True,\n    )\n    self.assertContains(\n        response,\n        '<td class=\"field-is_superuser\">%s</td>' % self.site.empty_value_display,\n        html=True,\n    )\n", "def test_check_dependencies_in_admin_site(self):\n    class BadAdminSite(AdminSite):\n        # No site_header and no index_title\n\n            return []\n\n    site = BadAdminSite()\n    self.assertEqual(\n        site.check(),\n        [\n            Error(\n                \"The value of 'site_header' must be a string.\",\n                obj=site,\n                id=\"admin.E001\",\n            ),\n            Error(\n                \"The value of 'index_title' must be a string.\",\n                obj=site,\n                id=\"admin.E002\",\n            ),\n        ],\n    )\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n"], "sample_601": ["def test_cftime_strftime_accessor(cftime_rounding_dataarray, use_dask):\n    import dask.array as da\n\n    date_format = \"%Y-%m-%d\"\n    expected = xr.DataArray(\n        [\n            [\"0001-01-01\", \"0001-01-01\"],\n            [\"0001-01-01\", \"0001-01-02\"],\n        ],\n        name=\"strftime\",\n    )\n\n    if use_dask:\n        chunks = {\"dim_0\": 1}\n        # Currently a compute is done to inspect a single value of the array\n        # if it is of object dtype to check if it is a cftime.datetime (if not\n        # we raise an error when using the dt accessor).\n        with raise_if_dask_computes(max_computes=1):\n            result = cftime_rounding_dataarray.chunk(chunks).dt.strftime(date_format)\n        expected = expected.chunk(chunks)\n        assert isinstance(result.data, da.Array)\n        assert result.chunks == expected.chunks\n    else:\n        result = cftime_rounding_dataarray.dt.strftime(date_format)\n\n    assert_identical(result, expected)\n", "def test_cftime_strftime_accessor(cftime_rounding_dataarray, cftime_date_type, use_dask):\n    import dask.array as da\n\n    date_format = \"%Y-%m-%d %H:%M:%S\"\n    expected = xr.DataArray(\n        [\n            [\n                f\"0001-01-01 01:00:00\",\n                f\"0001-01-01 15:00:00\",\n            ],\n            [\n                f\"0001-01-01 23:00:00\",\n                f\"0001-01-02 01:00:00\",\n            ],\n        ],\n        name=\"strftime\",\n    )\n\n    if use_dask:\n        chunks = {\"dim_0\": 1}\n        # Currently a compute is done to inspect a single value of the array\n        # if it is of object dtype to check if it is a cftime.datetime (if not\n        # we raise an error when using the dt accessor).\n        with raise_if_dask_computes(max_computes=1):\n            result = cftime_rounding_dataarray.chunk(chunks).dt.strftime(date_format)\n        expected = expected.chunk(chunks)\n        assert isinstance(result.data, da.Array)\n        assert result.chunks == expected.chunks\n    else:\n        result = cftime_rounding_dataarray.dt.strftime(date_format)\n\n    assert_equal(result, expected)\n", "def test_cftime_strftime_accessor(cftime_rounding_dataarray, cftime_date_type, use_dask):\n    import dask.array as da\n\n    date_format = \"%Y%m%d%H\"\n    expected = xr.DataArray(\n        [cftime_date_type(1, 1, 1, 1).strftime(date_format), cftime_date_type(1, 1, 1, 15).strftime(date_format)],\n        dims=[\"dim_0\"],\n    )\n\n    if use_dask:\n        chunks = {\"dim_0\": 1}\n        # Currently a compute is done to inspect a single value of the array\n        # if it is of object dtype to check if it is a cftime.datetime (if not\n        # we raise an error when using the dt accessor).\n        with raise_if_dask_computes(max_computes=1):\n            result = cftime_rounding_dataarray.chunk(chunks).dt.strftime(date_format)\n        expected = expected.chunk(chunks)\n        assert isinstance(result.data, da.Array)\n        assert result.chunks == expected.chunks\n    else:\n        result = cftime_rounding_dataarray.dt.strftime(date_format)\n\n    assert_equal(result, expected)\n", "def test_dask_field_access_with_object_dtype(data, field):\n    import dask.array as da\n\n    if field == \"dayofyear\" or field == \"dayofweek\":\n        pytest.importorskip(\"cftime\", minversion=\"1.0.2.1\")\n    times = data.time.values\n    times = np.asarray(times, dtype=object)\n    times_da = xr.DataArray(times, dims=[\"time\"])\n    times_da = times_da.chunk({\"time\": 50})\n    result = getattr(times_da.dt, field)\n    expected = getattr(xr.coding.cftimeindex.CFTimeIndex(times_da.values), field)\n    expected_da = xr.DataArray(expected, dims=[\"time\"])\n\n    assert isinstance(result.data, da.Array)\n    assert result.chunks == times_da.chunks\n    assert_equal(result.compute(), expected_da)\n", "def test_cftime_strftime_access_various_types(times):\n    date_format = \"%Y-%m-%d %H:%M:%S\"\n    result = times.dt.strftime(date_format)\n\n    # Check the result is an object dtype DataArray\n    assert result.dtype == object\n\n    # Check the result is a string for each value\n    assert np.all([isinstance(value, str) for value in result.values])\n\n    # Check the result is the expected string format\n    expected = xr.DataArray(\n        [time.strftime(date_format) for time in times.values], name=\"stftime\", dims=[\"time\"]\n    )\n    assert_equal(result, expected)\n"], "sample_722": ["def test_k_means_invalid_algorithm():\n    km = KMeans(algorithm=\"invalid\")\n    assert_raises(ValueError, km.fit, X)\n", "def test_kmeans_invalid_algorithm():\n    # check that a warning is raised if the algorithm is not supported\n    km = KMeans(algorithm=\"wrong\")\n    assert_raises(ValueError, km.fit, X)\n", "def test_kmeans_elkan():\n    X = np.random.rand(100, 5)\n    n_clusters = 5\n    centers = np.random.rand(n_clusters, X.shape[1])\n    km = KMeans(n_clusters=n_clusters, algorithm='elkan')\n    km.cluster_centers_ = centers\n    km.fit(X)\n\n    X = sp.csr_matrix(X)\n    km = KMeans(n_clusters=n_clusters, algorithm='elkan')\n    km.cluster_centers_ = centers\n    assert_raises(TypeError, km.fit, X)\n", "def test_mb_kmeans_random_reassignment():\n    # Check that random reassignment in mini batch k-means does not result in\n    # all centers being reassigned\n    mb_k_means = MiniBatchKMeans(n_clusters=n_clusters, batch_size=100,\n                                 random_state=42, reassignment_ratio=1.0)\n    mb_k_means.fit(X)\n    assert_less(mb_k_means.cluster_centers_.any(axis=1).sum(), n_clusters)\n", "def test_k_means_max_iter():\n    # test that setting max_iter to 0 will prevent convergence\n    km = KMeans(n_clusters=n_clusters, max_iter=0, random_state=42).fit(X)\n    assert_equal(km.n_iter_, 0)\n\n    # test that setting max_iter to 1 will lead to a single iteration\n    km = KMeans(n_clusters=n_clusters, max_iter=1, random_state=42).fit(X)\n    assert_equal(km.n_iter_, 1)\n\n    # test that a warning is raised when max_iter is set to a non-positive value\n    assert_warns_message(ValueError, 'Number of iterations should be a positive',\n                        KMeans, n_clusters=n_clusters, max_iter=0)\n\n    # test that setting max_iter to None raises a ValueError\n    assert_raises_regex(ValueError, \"max_iter should be a positive integer or\"\n                         \" None, got max_iter=([0-9]+|None)\", KMeans,\n                         n_clusters=n_clusters, max_iter=0)\n"], "sample_853": ["def test_transform_target_regressor_default_transformer():\n    X, y = friedman\n    regr = TransformedTargetRegressor(regressor=LinearRegression())\n    y_pred = regr.fit(X, y).predict(X)\n    # check the transformer output\n    y_tran = regr.transformer_.transform(y.reshape(-1, 1)).squeeze()\n    assert_allclose(y, y_tran)\n    assert_allclose(y, regr.transformer_.inverse_transform(\n        y_tran.reshape(-1, 1)).squeeze())\n    assert y.shape == y_pred.shape\n    assert_allclose(y_pred, regr.regressor_.predict(X))\n    # check the regressor output\n    lr = LinearRegression().fit(X, y)\n    assert_allclose(regr.regressor_.coef_.ravel(), lr.coef_.ravel())\n", "def test_transform_target_regressor_fit_params_default_regressor():\n    X, y = friedman\n    regr = TransformedTargetRegressor(transformer=DummyTransformer())\n    regr.fit(X, y, fit_intercept=False)\n    assert regr.regressor_.fit_intercept == False\n", "def test_transform_target_regressor_empty_X():\n    X = np.array([]).reshape(-1, 1)\n    y = np.array([1.0])\n    regr = TransformedTargetRegressor(regressor=LinearRegression(),\n                                      func=np.log, inverse_func=np.exp)\n    with pytest.raises(ValueError,\n                       match=\"X.shape[0] = 0, \"\n                       \"X.shape[1]  should be > 0 but is 0\"):\n        regr.fit(X, y)\n", "def test_transform_target_regressor_no_fit_params():\n    X, y = friedman\n    # provide a transformer and functions at the same time\n    regr = TransformedTargetRegressor(regressor=LinearRegression(),\n                                      transformer=StandardScaler())\n    regr.fit(X, y)\n    regr.predict(X)\n    assert regr.transformer_.fit_counter == 1\n\n    regr = TransformedTargetRegressor(regressor=LinearRegression(),\n                                      func=np.log, inverse_func=np.exp)\n    regr.fit(X, y)\n    regr.predict(X)\n    assert regr.transformer_.fit_counter == 1\n", "def test_transform_target_regressor_dimensionality():\n    X, y = friedman\n    y_1d = y.reshape(-1, 1)\n    y_2d = y_1d\n\n    # check output shape when fitting with 1d array and predict with 1d and 2d array\n    tt = TransformedTargetRegressor(transformer=DummyTransformer())\n    tt.fit(X, y_1d)\n    assert tt.predict(X).shape == y_1d.shape\n    assert tt.predict(X.T).shape == y_1d.shape\n\n    # check output shape when fitting with 2d array and predict with 1d and 2d array\n    tt.fit(X, y_2d)\n    assert tt.predict(X).shape == y_2d.shape\n    assert tt.predict(X.T).shape == y_2d.shape\n\n    # check output shape when fitting with 1d array and predict with 1d and 2d array using functions\n    tt = TransformedTargetRegressor(func=np.log, inverse_func=np.exp)\n    tt.fit(X, y_1d)\n    assert tt.predict(X).shape == y_1d.shape\n    assert tt.predict(X.T).shape == y_1d.shape\n\n    # check output shape when fitting with 2d array and predict with 1d and 2d array using functions\n    tt.fit(X, y_2d)\n    assert tt.predict(X).shape == y_2d.shape\n    assert tt.predict(X.T).shape == y_2d.shape\n"], "sample_740": ["def test_check_X_y_force_all_finite_valid():\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([1, 2])\n    X[0, 0] = np.inf\n    X_checked, y_checked = check_X_y(X, y, force_all_finite=False)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, y)\n\n    y[0] = np.inf\n    X_checked, y_checked = check_X_y(X, y, force_all_finite=False)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, y)\n\n    X[0, 0] = np.nan\n    X_checked, y_checked = check_X_y(X, y, force_all_finite='allow-nan')\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, y)\n\n    y[0] = np.inf\n    with pytest.raises(ValueError, match=\"Input contains infinity\"):\n        check_X_y(X, y, force_all_finite='allow-nan')\n", "def test_check_X_y_multioutput_y_dtype_object_conversion():\n    # test that data-frame like objects with dtype object in y\n    # get converted\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([[1, 2], [3, 4], [5, 6]], dtype=object)\n    y_df = MockDataFrame(y)\n    X_checked, y_checked = check_X_y(X, y_df, multi_output=True)\n    assert_equal(y_checked.dtype.kind, \"f\")\n", "def test_check_X_y_force_all_finite():\n    # Test that force_all_finite works for both X and y\n    X = np.array([[1, 2], [np.inf, 4]])\n    y = np.array([0, np.nan])\n    assert_raises_regex(ValueError, \"Input contains infinity\", check_X_y, X, y)\n\n    X = np.array([[1, 2], [np.nan, 4]])\n    y = np.array([0, np.inf])\n    assert_raises_regex(ValueError, \"Input contains infinity\", check_X_y, X, y)\n\n    X = np.array([[1, 2], [np.inf, 4]])\n    y = np.array([0, np.inf])\n    assert_raises_regex(ValueError, \"Input contains infinity\", check_X_y, X, y)\n\n    X = np.array([[1, 2], [np.nan, 4]])\n    y = np.array([0, np.nan])\n    assert_raises_regex(ValueError, \"Input contains NaN, infinity\",\n                        check_X_y, X, y)\n\n    X = np.array([[1, 2], [np.inf, 4]])\n    y = np.array([0, np.nan])\n    assert_raises_regex(ValueError, \"Input contains infinity\", check_X_y, X, y,\n                        force_all_finite='allow-nan')\n", "def test_check_X_y_force_all_finite_valid():\n    # Test that check_X_y behaves correctly when force_all_finite is valid\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([1, 2])\n\n    X_checked, y_checked = check_X_y(X, y, force_all_finite=True)\n    assert_array_equal(X, X_checked)\n    assert_array_equal(y, y_checked)\n\n    X = sp.csr_matrix(X)\n    y = np.array([1, np.nan])\n\n    X_checked, y_checked = check_X_y(X, y, force_all_finite='allow-nan')\n    assert_array_equal(X.toarray(), X_checked.toarray())\n    assert_array_equal(y, y_checked)\n\n", "def test_check_X_y():\n    # test function for check_X_y\n    X = np.ones((3, 10))\n    y = np.ones((3,))\n    X_checked, y_checked = check_X_y(X, y)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, y)\n\n    # multioutput y\n    y = np.ones((3, 3))\n    X_checked, y_checked = check_X_y(X, y, multi_output=True)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, y)\n\n    # check y dtype\n    y = y.astype(object)\n    X_checked, y_checked = check_X_y(X, y, y_numeric=True)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, y.astype(np.float64))\n\n    # test y with NaN and Inf\n    y = np.ones((3, 3))\n    y[0, 0] = np.nan\n    y[1, 0] = np.inf\n    assert_raises(ValueError, check_X_y, X, y)\n\n    # test y with string\n    y = np.array(['a', 'b', 'c'])\n    assert_raises(ValueError, check_X_y, X, y)\n\n    # test y with sparse\n    y = sp.csr_matrix(np.ones((3, 3)))\n    X_checked, y_checked = check_X_y(X, y)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked.toarray(), y.toarray())\n\n    # test consistent length\n    assert_raises(ValueError, check_X_y, X, np.ones((4,)))\n\n    # test X with non numeric dtype\n    X = np.ones((3, 10), dtype=object)\n    X_checked, y_checked = check_X_y(X, y)\n    assert_array_equal(X_checked, X.astype(np.float64))\n    assert_array_equal(y_checked, y)\n"], "sample_977": ["def test_custom_functions():\n    assert mcode(exp(x), user_functions={'exp': 'MyExp'}) == \"MyExp[x]\"\n    assert mcode(sin(x) + cos(x), user_functions={'sin': 'MySin', 'cos': 'MyCos'}) == \"MySin[x] + MyCos[x]\"\n", "def test_user_functions():\n    settings = {\n        'user_functions': {\n            'f': 'myf',\n            'g': 'myg'\n        }\n    }\n    assert mcode(f(x, y, z), **settings) == \"myf[x, y, z]\"\n    assert mcode(g(x, y, z), **settings) == \"myg[x, y, z]\"\n", "def test_user_functions():\n    assert mcode(sin(x), user_functions={'sin': 'my_sin'}) == \"my_sin[x]\"\n    assert mcode(sin(x) + cos(x), user_functions={'sin': 'my_sin', 'cos': 'my_cos'}) == \"my_sin[x] + my_cos[x]\"\n    assert mcode(sin(x) + cos(x), user_functions={'sin': ['lambda x: x.is_real', 'my_sin']}) == \"my_sin[x]\"\n    assert mcode(sin(x) + cos(x), user_functions={'sin': [('lambda x: x.is_real', 'my_sin'), ('lambda x: True', 'default_sin')]}) == \"my_sin[x]\"\n", "def test_user_functions():\n    assert mcode(sin(x), user_functions={'sin': 'MySin'}) == \"MySin[x]\"\n    assert mcode(cos(x), user_functions={'cos': ['MyCos', 'MyCos2']}) == \"MyCos[x]\"\n    assert mcode(sin(x), user_functions={'sin': [(lambda x: x.is_positive, 'MySin'), (lambda x: True, 'DefaultSin')]}) == \"MySin[x]\"\n    assert mcode(sin(-x), user_functions={'sin': [(lambda x: x.is_positive, 'MySin'), (lambda x: True, 'DefaultSin')]}) == \"DefaultSin[x]\"\n", "def test_UserDefinedFunctions():\n    # Test user defined function with a lambda condition\n        return x.is_real\n    user_funcs = {'f': [(user_func_cond, \"MyF\")]}\n    assert mcode(f(x), user_functions=user_funcs) == \"MyF[x]\"\n    assert mcode(f(1 + 2j), user_functions=user_funcs) == \"f[1 + 2*I]\"\n\n    # Test user defined function without a lambda condition\n    user_funcs = {'g': \"MyG\"}\n    assert mcode(f(x), user_functions=user_funcs) == \"f[x]\"\n    assert mcode(g(x), user_functions=user_funcs) == \"MyG[x]\"\n\n    # Test user defined function with multiple conditions\n        return x.is_real\n        return x.is_integer\n    user_funcs = {'h': [(user_func_cond1, \"MyH1\"), (user_func_cond2, \"MyH2\")]}\n    assert mcode(h(x), user_functions=user_funcs) == \"MyH1[x]\"\n    assert mcode(h(1), user_functions=user_funcs) == \"MyH2[1]\"\n"], "sample_26": ["def test_update_header_with_int8(tmp_path):\n    \"\"\"Regression test for https://github.com/astropy/astropy/issues/12314\"\"\"\n\n    img = np.arange(-50, 50, dtype=np.int8).reshape(10, 10)\n    hdu = fits.PrimaryHDU(img)\n    hdu.writeto(tmp_path / \"int8.fits\")\n\n    with fits.open(tmp_path / \"int8.fits\", mode=\"update\") as hdul:\n        assert hdul[0].header[\"BITPIX\"] == 8\n        assert hdul[0].header[\"BZERO\"] == -128\n        assert hdul[0].header[\"BSCALE\"] == 1.0\n        assert_equal(hdul[0].data, img)\n        assert hdul[0].data.dtype == img.dtype\n\n        # Update the header\n        hdul[0].header[\"BITPIX\"] = 32\n        hdul[0].header[\"BZERO\"] = 0\n        hdul[0].header[\"BSCALE\"] = 1.0\n\n    with fits.open(tmp_path / \"int8.fits\") as hdul:\n        assert hdul[0].header[\"BITPIX\"] == 32\n        assert \"BZERO\" not in hdul[0].header\n        assert \"BSCALE\" not in hdul[0].header\n        assert_equal(hdul[0].data, img.astype(np.float32))\n        assert hdul[0].data.dtype == np.float32\n", "def test_scale_with_invalid_bzero_bscale(tmp_path):\n    \"\"\"\n    Regression test for https://github.com/astropy/astropy/issues/6399\n\n    Tests that passing invalid BZERO/BSCALE values raises an error.\n    \"\"\"\n\n    hdu2 = fits.ImageHDU(np.random.rand(100, 100))\n    with pytest.raises(ValueError, match=r\"BSCALE must be a float\"):\n        hdu2.scale(type=\"uint8\", bscale=\"a\", bzero=0)\n    with pytest.raises(ValueError, match=r\"BZERO must be a number\"):\n        hdu2.scale(type=\"uint8\", bscale=1, bzero=\"b\")\n    with pytest.raises(ValueError, match=r\"BSCALE and BZERO must be finite\"):\n        hdu2.scale(type=\"uint8\", bscale=float(\"inf\"), bzero=0)\n    with pytest.raises(ValueError, match=r\"BSCALE and BZERO must be finite\"):\n        hdu2.scale(type=\"uint8\", bscale=1, bzero=float(\"inf\"))\n", "def test_compressed_imagehdus_with_bzero_bscale(tmp_path):\n    # Regression test for https://github.com/astropy/astropy/issues/11638\n    # Test that BZERO/BSCALE values are applied when reading a CompImageHDU\n\n    data = np.arange(100, 200, dtype=np.int16)\n    hdu = fits.CompImageHDU(data)\n    hdu.header[\"BZERO\"] = 100\n    hdu.header[\"BSCALE\"] = 2\n    hdu.writeto(tmp_path / \"test.fits\")\n\n    with fits.open(tmp_path / \"test.fits\") as hdul:\n        assert np.all(hdul[1].data == (data + 100) / 2)\n", "def test_comp_image_header_update_with_reserved_keywords(tmp_path):\n    \"\"\"\n    Regression test for https://github.com/astropy/astropy/issues/12345\n\n    Test that setting restricted keywords on CompImageHDU image headers fails.\n    \"\"\"\n\n        with pytest.warns(UserWarning) as w:\n            hdr[keyword] = value\n        assert len(w) == 1\n        assert str(w[0].message).startswith(f\"Keyword {keyword!r} is reserved\")\n        assert keyword not in hdr\n\n    data = np.arange(100, dtype=np.int32)\n    hdu = fits.CompImageHDU(data=data)\n\n    test_set_keyword(hdu.header, \"NAXIS\", 5)\n    test_set_keyword(hdu.header, \"BITPIX\", 16)\n    test_set_keyword(hdu.header, \"NAXIS1\", 10)\n    test_set_keyword(hdu.header, \"NAXIS2\", 10)\n    test_set_keyword(hdu.header, \"PCOUNT\", 0)\n    test_set_keyword(hdu.header, \"GCOUNT\", 1)\n    test_set_keyword(hdu.header, \"XTENSION\", 'IMAGE   ')\n\n    hdu.writeto(tmp_path / \"test.fits\")\n\n    with fits.open(tmp_path / \"test.fits\") as hdul:\n        assert hdul[1].header[\"NAXIS\"] == 1\n        assert hdul[1].header[\"BITPIX\"] == 32\n        assert \"NAXIS1\" not in hdul[1].header\n        assert \"NAXIS2\" not in hdul[1].header\n        assert hdul[1].header[\"PCOUNT\"] == 0\n        assert hdul[1].header[\"GCOUNT\"] == 1\n        assert hdul[1].header[\"XTENSION\"] == 'IMAGE   '\n", "def test_scale_back_with_bzero_and_bscale(tmp_path):\n    \"\"\"\n    Regression test for https://github.com/astropy/astropy/issues/5847\n\n    This test ensures that when an HDU contains both BZERO and BSCALE,\n    and the scale_back option is True, that the original BZERO and BSCALE\n    values are used to rescale the data when it is written out.\n    \"\"\"\n\n    # Make the sample file\n    arr = np.arange(5, dtype=np.int32)\n    hdu = fits.PrimaryHDU(data=arr)\n    hdu.scale(\"int16\", bscale=1.23, bzero=45)\n\n    # Creating data that uses BZERO/BSCALE is currently a bit kludgy--a separate issue\n    # TODO: Rewrite this test when scaling with blank support is better supported\n\n    # Let's just add a value to the data that should be converted to NaN\n    # when it is read back in:\n    filename = tmp_path / \"test.fits\"\n    hdu.data[0] = 9999\n    hdu.header[\"BLANK\"] = 9999\n    hdu.writeto(filename)\n\n    with fits.open(filename) as hdul:\n        data = hdul[0].data\n        assert np.isnan(data[0])\n        with pytest.warns(\n            fits.verify.VerifyWarning, match=r\"Invalid 'BLANK' keyword in header\"\n        ):\n            hdul.writeto(tmp_path / \"test2.fits\")\n\n    # Now reopen the newly written file.  It should not have a 'BLANK'\n    # keyword\n    with fits.open(tmp_path / \"test2.fits\") as hdul2:\n        assert \"BLANK\" not in hdul2[0].header\n        data = hdul2[0].data\n        assert np.isnan(data[0])\n\n    # Finally, test that scale_back keeps the BZEROs correctly\n    with fits.open(filename, scale_back=True, mode=\"update\") as hdul3:\n        data = hdul3[0].data\n        # This emits warning that pytest cannot catch properly, so we\n        # catch it with pytest.mark.filterwarnings above.\n        assert np.isnan(data[0])\n\n    with fits.open(filename, do_not_scale_image_data=True) as hdul4:\n        assert hdul4[0].header[\"BLANK\"] == 9999\n        assert hdul4[0]."], "sample_9": ["def test_read_html_table_with_css():\n    \"\"\"\n    Test reading an HTML table with CSS styling\n    \"\"\"\n    table_in = ['<table style=\"border-collapse: collapse;\">',\n                '<tr><th style=\"border: 1px solid black;\">A</th></tr>',\n                '<tr><td style=\"border: 1px solid black;\">1</td></tr>',\n                '</table>']\n    dat = Table.read(table_in, format='ascii.html')\n    assert dat.colnames == ['A']\n    assert len(dat) == 1\n    assert np.all(dat['A'] == ['1'])\n\n    # Test with custom CSS\n    dat = Table.read(table_in, format='ascii.html',\n                     htmldict={'css': 'table {border-collapse: collapse;}'})\n    assert dat.colnames == ['A']\n    assert len(dat) == 1\n    assert np.all(dat['A'] == ['1'])\n", "def test_html_writer_css():\n    \"\"\"\n    Test that the HTML writer correctly includes CSS styles\n    when writing a table.\n    \"\"\"\n\n    col1 = [1, 2, 3]\n    col2 = [(1.0, 1.0), (2.0, 2.0), (3.0, 3.0)]\n    col3 = [('a', 'a', 'a'), ('b', 'b', 'b'), ('c', 'c', 'c')]\n    table = Table([col1, col2, col3], names=('C1', 'C2', 'C3'))\n\n    css = 'body { background-color: #f2f2f2; }'\n    htmldict = {'css': css}\n    expected = \"\"\"\\", "def test_html_writer_multiple_tables():\n    \"\"\"\n    Test to make sure the HTML writer correctly handles writing\n    multiple tables to the same output file.\n    \"\"\"\n\n    t1 = Table([1, 2, 3], names=('C1',))\n    t2 = Table([4, 5, 6], names=('C2',))\n\n    out = StringIO()\n    t1.write(out, format='html')\n    t2.write(out, format='html')\n\n    output = out.getvalue()\n    assert '<table>' in output\n    assert '</table>' in output\n    assert output.count('<table>') == 2\n    assert output.count('</table>') == 2\n    assert output.count('<tr><td>1</td></tr>') == 1\n    assert output.count('<tr><td>2</td></tr>') == 1\n    assert output.count('<tr><td>3</td></tr>') == 1\n    assert output.count('<tr><td>4</td></tr>') == 1\n    assert output.count('<tr><td>5</td></tr>') == 1\n    assert output.count('<tr><td>6</td></tr>') == 1\n", "def test_html_writer_css_and_js():\n    \"\"\"\n    Test to make sure that the HTML writer includes CSS and JS files\n    when the appropriate parameters are given in htmldict.\n    \"\"\"\n\n    col1 = [1, 2, 3]\n    table = Table([col1], names=('C1',))\n    expected = \"\"\"\\", "def test_write_table_html_css_cssfiles():\n    \"\"\"\n    Test that passing custom css and css files to htmldict\n    should include them in the output.\n    \"\"\"\n    table = Table([1, 2, 3], names=['a'])\n\n    htmldict = {'css': 'table { border-collapse: collapse; }',\n                'cssfiles': ['https://example.com/style.css'],\n                'table_class': 'my_table'}\n\n    out = html.HTML(htmldict=htmldict).write(table)[0].strip()\n\n    assert '<style>table { border-collapse: collapse; }</style>' in out\n    assert '<link href=\"https://example.com/style.css\" rel=\"stylesheet\" type=\"text/css\"/>' in out\n    assert '<table class=\"my_table\"' in out\n"], "sample_116": ["    def test_empty_fragment_name(self):\n        key = make_template_fragment_key('')\n        self.assertEqual(key, 'template.cache..d41d8cd98f00b204e9800998ecf8427e')\n", "    def test_collision(self):\n        # Ensure that two different inputs produce different keys.\n        key1 = make_template_fragment_key('collision', vary_on=['a'])\n        key2 = make_template_fragment_key('collision', vary_on=['b'])\n        self.assertNotEqual(key1, key2)\n", "    def test_vary_on_none(self):\n        with self.assertRaises(TypeError):\n            make_template_fragment_key('foo', vary_on=None)\n", "    def test_with_none_vary_on(self):\n        key = make_template_fragment_key('foo', None)\n        self.assertEqual(key, 'template.cache.foo.d41d8cd98f00b204e9800998ecf8427e')\n", "    def test_vary_on_with_none(self):\n        key = make_template_fragment_key('foo', [None])\n        self.assertEqual(key, 'template.cache.foo.c4ca4238a0b923820dcc509a6f75849b')\n"], "sample_439": ["def test_order_fields(self):\n    class TestForm(Form):\n        field1 = CharField()\n        field2 = CharField()\n        field3 = CharField()\n        field4 = CharField()\n        field5 = CharField()\n        field6 = CharField()\n        field7 = CharField()\n        field8 = CharField()\n        field9 = CharField()\n        field10 = CharField()\n        field11 = CharField()\n        field12 = CharField()\n        field13 = CharField()\n        field14 = CharField()\n\n    p = TestForm(auto_id=False)\n    self.assertHTMLEqual(\n        p.as_table(),\n        \"\".join(\n            f\"<tr><th>Field{i}:</th><td>\"\n            f'<input type=\"text\" name=\"field{i}\" required></td></tr>'\n            for i in range(1, 15)\n        ),\n    )\n\n    p = TestForm(field_order=[\"field2\", \"field1\", \"field3\", \"field5\"], auto_id=False)\n    self.assertHTMLEqual(\n        p.as_table(),\n        \"<tr><th>Field2:</th><td>\"\n        '<input type=\"text\" name=\"field2\" required></td></tr>'\n        \"<tr><th>Field1:</th><td>\"\n        '<input type=\"text\" name=\"field1\" required></td></tr>'\n        \"<tr><th>Field3:</th><td>\"\n        '<input type=\"text\" name=\"field3\" required></td></tr>'\n        \"<tr><th>Field5:</th><td>\"\n        '<input type=\"text\" name=\"field5\" required></td></tr>'\n        \"<tr><th>Field4:</th><td>\"\n        '<input type=\"text\" name=\"field4\" required></td></tr>'\n        \"<tr><th>Field6:</th><td>\"\n        '<input type=\"text\" name=\"field6\" required></td></tr>'\n        \"<tr><th>Field7:</th><td>\"\n        '<input type=\"text\" name=\"field7\" required></td></tr>'\n        \"<tr><th>Field8:</th><td>\"\n        '<input type=\"text\" name=\"field8\" required></td></tr>'\n        \"<tr><th>Field9:</th><td>\"\n        '<input type=\"text\" name=\"field9\" required></td></tr>'\n        \"<tr><th>Field", "def test_invalid_media(self):\n    class InvalidMediaForm(Form):\n        field = CharField()\n\n        @property\n            return \"Invalid media\"\n\n    f = InvalidMediaForm()\n    with self.assertRaisesMessage(TypeError, \"must be an instance of django.forms.Media\"):\n        f.media\n", "def test_set_initial_on_dynamic_fields(self):\n    class DynamicForm(Form):\n            super().__init__(*args, **kwargs)\n            self.fields[\"dynamic_field\"] = CharField(initial=\"dynamic\")\n\n    form = DynamicForm()\n    self.assertEqual(form[\"dynamic_field\"].initial, \"dynamic\")\n    self.assertEqual(form[\"dynamic_field\"].value(), \"dynamic\")\n\n    form = DynamicForm(initial={\"dynamic_field\": \"new_initial\"})\n    self.assertEqual(form[\"dynamic_field\"].initial, \"new_initial\")\n    self.assertEqual(form[\"dynamic_field\"].value(), \"new_initial\")\n\n    form = DynamicForm(data={\"dynamic_field\": \"data\"})\n    self.assertEqual(form[\"dynamic_field\"].initial, \"dynamic\")\n    self.assertEqual(form[\"dynamic_field\"].value(), \"data\")\n", "def test_dynamic_construction_metaclass(self):\n    # It's possible to construct a Form dynamically by adding to the self.fields\n    # dictionary in __init__(). Don't forget to call Form.__init__() within the\n    # subclass' __init__(). This should work with the metaclass as well.\n\n    class Person(metaclass=DeclarativeFieldsMetaclass, Base=Form):\n        first_name = CharField()\n        last_name = CharField()\n\n            super().__init__(*args, **kwargs)\n            self.fields[\"birthday\"] = DateField()\n\n    p = Person(auto_id=False)\n    self.assertHTMLEqual(\n        p.as_table(),\n        \"\"\"\n        <tr><th>First name:</th><td>\n        <input type=\"text\" name=\"first_name\" required></td></tr>\n        <tr><th>Last name:</th><td>\n        <input type=\"text\" name=\"last_name\" required></td></tr>\n        <tr><th>Birthday:</th><td>\n        <input type=\"text\" name=\"birthday\" required></td></tr>\n        \"\"\",\n    )\n\n    # Instances of a dynamic Form do not persist fields from one Form instance to\n    # the next.\n    class MyForm(metaclass=DeclarativeFieldsMetaclass, Base=Form):\n            Form.__init__(self, data, auto_id=auto_id)\n\n            for field in field_list:\n                self.fields[field[0]] = field[1]\n\n    field_list = [(\"field1\", CharField()), (\"field2\", CharField())]\n    my_form = MyForm(field_list=field_list)\n    self.assertHTMLEqual(\n        my_form.as_table(),\n        \"\"\"\n        <tr><th>Field1:</th><td><input type=\"text\" name=\"field1\" required></td></tr>\n        <tr><th>Field2:</th><td><input type=\"text\" name=\"field2\" required></td></tr>\n        \"\"\",\n    )\n    field_list = [(\"field3\", CharField()), (\"field4\", CharField())]\n    my_form = MyForm(field_list=field_list)\n    self.assertHTMLEqual(\n        my_form.as_table(),\n        \"\"\"\n        <tr><th>Field3:</th><td><input type", "def test_forms_with_disabled_fields_hidden_initial(self):\n    class PersonForm(Form):\n        name = CharField(show_hidden_initial=True)\n\n    # Without form data.\n    form = PersonForm()\n    self.assertEqual(form.fields[\"name\"].show_hidden_initial, True)\n    self.assertHTMLEqual(\n        form.as_table(),\n        '<tr><th><label for=\"id_name\">Name:</label></th><td>'\n        '<input id=\"id_name\" type=\"text\" name=\"name\" required>'\n        '<input type=\"hidden\" name=\"initial-name\" id=\"initial-id_name\">'\n        \"</td></tr>\",\n    )\n    self.assertHTMLEqual(\n        form.as_ul(),\n        '<li><label for=\"id_name\">Name:</label>'\n        '<input type=\"text\" name=\"name\" required id=\"id_name\">'\n        '<input type=\"hidden\" name=\"initial-name\" id=\"initial-id_name\"></li>',\n    )\n    self.assertHTMLEqual(\n        form.as_p(),\n        '<p><label for=\"id_name\">Name:</label>'\n        '<input type=\"text\" name=\"name\" required id=\"id_name\">'\n        '<input type=\"hidden\" name=\"initial-name\" id=\"initial-id_name\"></p>',\n    )\n    self.assertHTMLEqual(\n        form.as_div(),\n        '<div><label for=\"id_name\">Name:</label>'\n        '<input type=\"text\" name=\"name\" required id=\"id_name\">'\n        '<input type=\"hidden\" name=\"initial-name\" id=\"initial-id_name\"></div>',\n    )\n\n    # With form data.\n    form = PersonForm({\"name\": \"John Doe\"})\n    self.assertHTMLEqual(\n        form.as_table(),\n        '<tr><th><label for=\"id_name\">Name:</label></th><td>'\n        '<input type=\"text\" name=\"name\" value=\"John Doe\" id=\"id_name\" required>'\n        '<input type=\"hidden\" name=\"initial-name\" id=\"initial-id_name\"></td></tr>',\n    )\n    self.assertHTMLEqual(\n        form.as_ul(),\n        '<li><label for=\"id_name\">Name:</label>'\n        '<input type=\"text\" name=\"name\" value=\"John Doe\" id=\"id_name\" required>'\n        '<input type=\"hidden\" name=\"initial-name\" id=\"initial-id_name\"></li>',\n    )\n    self.assertHTMLEqual(\n        form.as_p(),\n        '<p><label for"], "sample_376": ["def test_empty_message_storage(self):\n    \"\"\"\n    Test that an empty message storage doesn't raise any errors.\n    \"\"\"\n    storage = self.storage_class(self.get_request())\n    self.assertEqual(list(storage), [])\n    response = self.get_response()\n    self.assertEqual(storage.update(response), [])\n    self.assertEqual(stored_cookie_messages_count(storage, response), 0)\n", "def test_store_messages_larger_than_max_cookie_size(self):\n    \"\"\"\n    Test that messages are stored correctly even if they exceed the max cookie size.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # Create messages that exceed the max cookie size\n    large_message = 'a' * 3000\n    storage.add(constants.INFO, large_message)\n    storage.add(constants.INFO, 'small message')\n\n    # Store the messages\n    unstored_messages = storage.update(response)\n\n    # Check that the small message was stored in the cookie\n    stored_messages = storage._decode(response.cookies['messages'].value)\n    self.assertEqual(len(stored_messages), 1)\n    self.assertEqual(stored_messages[0].message, 'small message')\n\n    # Check that the large message was not stored in the cookie and is in the unstored messages\n    self.assertEqual(len(unstored_messages), 1)\n    self.assertEqual(unstored_messages[0].message, large_message)\n", "def test_process_messages(self):\n    \"\"\"\n    Test the MessageDecoder's process_messages method.\n    \"\"\"\n    encoder = MessageEncoder()\n    decoder = MessageDecoder()\n\n    # Test list containing a Message instance\n    messages = [\n        Message(constants.INFO, 'Test message'),\n        'Not a message',\n    ]\n    encoded_messages = encoder.encode(messages)\n    decoded_messages = json.loads(encoded_messages, cls=decoder)\n    self.assertIsInstance(decoded_messages[0], Message)\n    self.assertEqual(decoded_messages[0].message, 'Test message')\n    self.assertEqual(decoded_messages[1], 'Not a message')\n\n    # Test dictionary containing a Message instance\n    messages = {\n        'message': Message(constants.INFO, 'Test message'),\n        'not a message': 'Test string',\n    }\n    encoded_messages = encoder.encode(messages)\n    decoded_messages = json.loads(encoded_messages, cls=decoder)\n    self.assertIsInstance(decoded_messages['message'], Message)\n    self.assertEqual(decoded_messages['message'].message, 'Test message')\n    self.assertEqual(decoded_messages['not a message'], 'Test string')\n\n    # Test nested structure containing a Message instance\n    messages = {\n        'message': {\n            'nested_message': Message(constants.INFO, 'Test message'),\n            'not a message': 'Test string',\n        },\n    }\n    encoded_messages = encoder.encode(messages)\n    decoded_messages = json.loads(encoded_messages, cls=decoder)\n    self.assertIsInstance(decoded_messages['message']['nested_message'], Message)\n    self.assertEqual(decoded_messages['message']['nested_message'].message, 'Test message')\n    self.assertEqual(decoded_messages['message']['not a message'], 'Test string')\n\n    # Test list of lists containing a Message instance\n    messages = [\n        [\n            Message(constants.INFO, 'Test message'),\n            'Not a message',\n        ],\n        'Not a list',\n    ]\n    encoded_messages = encoder.encode(messages)\n    decoded_messages = json.loads(encoded_messages, cls=decoder)\n    self.assertIsInstance(decoded_messages[0][0], Message)\n    self.assertEqual(decoded_messages[0][0].message, 'Test message')\n    self.assertEqual(decoded_messages[0][1], 'Not a message')\n    self.assertEqual(decoded_messages[1], 'Not a list')\n", "def test_store_unstorable_messages(self):\n    \"\"\"\n    If the data exceeds what is allowed in a cookie, the not_finished\n    sentinel value is appended to the encoded data and returned\n    by the ``update`` method.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # When storing as a cookie, the cookie has constant overhead of approx\n    # 54 chars, and each message has a constant overhead of about 37 chars\n    # and a variable overhead of zero in the best case. We aim for a message\n    # size which will fit 4 messages into the cookie, but not 5.\n    msg_size = int((CookieStorage.max_cookie_size - 54) / 4.5 - 37)\n    # Generate the same (tested) content every time that does not get run\n    # through zlib compression.\n    random.seed(42)\n    for i in range(5):\n        msg = get_random_string(msg_size)\n        storage.add(constants.INFO, msg)\n    unstored_messages = storage.update(response)\n\n    cookie_storing = self.stored_messages_count(storage, response)\n    self.assertEqual(cookie_storing, 4)\n\n    self.assertEqual(len(unstored_messages), 1)\n    self.assertEqual(unstored_messages[-1] == CookieStorage.not_finished, False)\n\n    # The not_finished sentinel value is appended to the encoded data\n    cookie = response.cookies.get(CookieStorage.cookie_name)\n    data = storage._decode(cookie.value)\n    self.assertEqual(data[-1], CookieStorage.not_finished)\n", "def test_cookie_update(self):\n    \"\"\"\n    Test the CookieStorage.update() method when data is partially stored.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # Generate a large number of messages.\n    num_messages = 20\n    for i in range(num_messages):\n        storage.add(constants.INFO, f'Message {i}')\n\n    # Update the response and check the number of stored messages.\n    unstored_messages = storage.update(response)\n    cookie_storing = self.stored_messages_count(storage, response)\n    self.assertLess(cookie_storing, num_messages)\n\n    # Check the number of unstored messages.\n    self.assertEqual(len(unstored_messages), num_messages - cookie_storing)\n\n    # Check that the not_finished sentinel value is added to the cookie.\n    cookie = response.cookies.get(storage.cookie_name)\n    data = storage._decode(cookie.value)\n    self.assertEqual(data[-1], CookieStorage.not_finished)\n"], "sample_203": ["    def test_prohibit_null_characters_validator(self):\n        validator = validators.ProhibitNullCharactersValidator()\n        self.assertRaises(ValidationError, validator, 'a\\0b')\n        try:\n            validator('a\\0b')\n        except ValidationError as e:\n            self.assertEqual(e.message, _('Null characters are not allowed.'))\n            self.assertEqual(e.code, 'null_characters_not_allowed')\n            self.assertEqual(e.params, {'value': 'a\\0b'})\n", "    def test_validator_equivalence(self):\n        # Test equivalence of validators\n        cases = [\n            (validators.RegexValidator(regex='^a+$'), validators.RegexValidator(regex='^a+$')),\n            (validators.URLValidator(), validators.URLValidator()),\n            (validators.EmailValidator(), validators.EmailValidator()),\n            (validators.MinValueValidator(5), validators.MinValueValidator(5)),\n            (validators.MaxValueValidator(5), validators.MaxValueValidator(5)),\n            (validators.MinLengthValidator(5), validators.MinLengthValidator(5)),\n            (validators.MaxLengthValidator(5), validators.MaxLengthValidator(5)),\n            (validators.FileExtensionValidator(allowed_extensions=['txt']), validators.FileExtensionValidator(allowed_extensions=['txt'])),\n            (validators.DecimalValidator(max_digits=5, decimal_places=2), validators.DecimalValidator(max_digits=5, decimal_places=2)),\n            (validators.ProhibitNullCharactersValidator(), validators.ProhibitNullCharactersValidator()),\n        ]\n\n        for validator1, validator2 in cases:\n            self.assertEqual(validator1, validator2)\n", "    def test_email_validator(self):\n        cases = [\n            ('test@example.com', True),\n            ('test@example', False),\n            ('@example.com', False),\n            ('test@', False),\n            ('test.example.com', False),\n            ('test@example.co.uk', True),\n            ('test@example-one.com', True),\n            ('test+tag@example.com', True),\n            ('\"test\"@example.com', True),\n            ('test.\"@example.com', True),\n            ('test\"@example.com', True),\n            ('test\"@example.co.uk', True),\n            ('\"test.test\"@example.com', True),\n            ('\"test.test\"@example.co.uk', True),\n        ]\n        for email, expected in cases:\n            with self.subTest(email=email):\n                try:\n                    validators.validate_email(email)\n                except ValidationError:\n                    self.assertFalse(expected)\n                else:\n                    self.assertTrue(expected)\n", "    def test_regex_validator_flags(self):\n        class MyForm(forms.Form):\n            field = forms.CharField(\n                validators=[validators.RegexValidator(regex=r'^[a-zA-Z]*$', flags=re.IGNORECASE)],\n            )\n\n        form = MyForm({'field': 'IgnORE Case strIng'})\n        self.assertIs(form.is_valid(), True)\n", "    def test_email_validator(self):\n        validator = EmailValidator()\n\n        # Test valid email addresses\n        valid_emails = [\n            'test@example.com',\n            'test@example.co.uk',\n            'test+tag@example.com',\n            'test.tag@example.com',\n        ]\n        for email in valid_emails:\n            with self.subTest(email=email):\n                try:\n                    validator(email)\n                except ValidationError:\n                    self.fail(f\"Valid email '{email}' failed validation.\")\n\n        # Test invalid email addresses\n        invalid_emails = [\n            'test@example',\n            '@example.com',\n            'test@',\n            'test@example.com.au@',\n            'test@example.com.',\n            'test@example.',\n            'test..@example.com',\n            'test@example',\n        ]\n        for email in invalid_emails:\n            with self.subTest(email=email):\n                with self.assertRaises(ValidationError):\n                    validator(email)\n\n        # Test email addresses with non-ASCII characters\n        non_ascii_emails = [\n            'test@\u00e9xample.com',\n            'test@ex\u00e4mple.com',\n        ]\n        for email in non_ascii_emails:\n            with self.subTest(email=email):\n                try:\n                    validator(email)\n                except ValidationError:\n                    self.fail(f\"Valid email '{email}' failed validation.\")\n\n        # Test email addresses with domain allowlist\n        validator = EmailValidator(domain_allowlist=['example.com'])\n        with self.assertRaises(ValidationError):\n            validator('test@otherdomain.com')\n\n        # Test email addresses with invalid domain\n        with self.assertRaises(ValidationError):\n            validator('test@invalid_domain')\n"], "sample_653": ["def test_set_log_path_different_directory(testdir):\n    report_dir_base = testdir.tmpdir.join(\"report_dir\").strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file_level = DEBUG\n        log_cli=true\n        \"\"\"\n    )\n    testdir.makeconftest(\n        \"\"\"\n            import os\n            import pytest\n            @pytest.hookimpl(hookwrapper=True, tryfirst=True)\n                config = item.config\n                logging_plugin = config.pluginmanager.get_plugin(\"logging-plugin\")\n                report_file = os.path.join({}, item._request.node.name)\n                logging_plugin.set_log_path(report_file)\n                yield\n        \"\"\".format(\n            repr(report_dir_base)\n        )\n    )\n    testdir.makepyfile(\n        \"\"\"\n            import logging\n            logger = logging.getLogger(\"testcase-logger\")\n                logger.info(\"message from test 1\")\n                assert True\n\n                logger.debug(\"message from test 2\")\n                assert True\n        \"\"\"\n    )\n    testdir.runpytest()\n    assert os.path.isdir(report_dir_base)\n    with open(os.path.join(report_dir_base, \"test_first\"), \"r\") as rfh:\n        content = rfh.read()\n        assert \"message from test 1\" in content\n\n    with open(os.path.join(report_dir_base, \"test_second\"), \"r\") as rfh:\n        content = rfh.read()\n        assert \"message from test 2\" in content\n", "def test_log_set_path_absolute_file(testdir):\n    report_dir_base = testdir.tmpdir.strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file_level = DEBUG\n        log_cli=true\n        \"\"\"\n    )\n    testdir.makeconftest(\n        \"\"\"\n            import os\n            import pytest\n            @pytest.hookimpl(hookwrapper=True, tryfirst=True)\n                config = item.config\n                logging_plugin = config.pluginmanager.get_plugin(\"logging-plugin\")\n                report_file = os.path.join({}, item._request.node.name)\n                logging_plugin.set_log_path(report_file)\n                yield\n        \"\"\".format(\n            repr(report_dir_base)\n        )\n    )\n    testdir.makepyfile(\n        \"\"\"\n            import logging\n            logger = logging.getLogger(\"testcase-logger\")\n                logger.info(\"message from test 1\")\n                assert True\n\n                logger.debug(\"message from test 2\")\n                assert True\n        \"\"\"\n    )\n    testdir.runpytest()\n    with open(os.path.join(report_dir_base, \"test_first\"), \"r\") as rfh:\n        content = rfh.read()\n        assert \"message from test 1\" in content\n\n    with open(os.path.join(report_dir_base, \"test_second\"), \"r\") as rfh:\n        content = rfh.read()\n        assert \"message from test 2\" in content\n\n    # Test with absolute file path\n    testdir.makeconftest(\n        \"\"\"\n            import os\n            import pytest\n            @pytest.hookimpl(hookwrapper=True, tryfirst=True)\n                config = item.config\n                logging_plugin = config.pluginmanager.get_plugin(\"logging-plugin\")\n                report_file = \"{}\".format(os.path.join(report_dir_base, item._request.node.name))\n                logging_plugin.set_log_path(report_file)\n                yield\n        \"\"\".format(\n            repr(report_dir_base)\n        )\n    )\n    testdir.runpytest()\n    with open(os.path.join(report_dir_base, \"test_first\"), \"r\") as rfh:\n        content = rfh.read()\n        assert \"message from test 1\" in content\n\n    with open(os.path.join(report_dir_base, \"test_second\"), \"r\") as rfh:\n        content = rfh.read()\n        assert \"message from test 2\" in content\n", "def test_log_set_path_before_configuration(testdir):\n    report_dir_base = testdir.tmpdir.strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file_level = DEBUG\n        \"\"\"\n    )\n    testdir.makeconftest(\n        \"\"\"\n            import os\n            import pytest\n            @pytest.hookimpl(hookwrapper=True, tryfirst=True)\n                logging_plugin = config.pluginmanager.get_plugin(\"logging-plugin\")\n                report_file = os.path.join({}, 'test.log')\n                logging_plugin.set_log_path(report_file)\n                yield\n        \"\"\".format(\n            repr(report_dir_base)\n        )\n    )\n    testdir.makepyfile(\n        \"\"\"\n            import logging\n            logger = logging.getLogger(\"testcase-logger\")\n                logger.info(\"message from test 1\")\n                assert True\n\n                logger.debug(\"message from test 2\")\n                assert True\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*- Captured log call -*\", \"*message from test 1*\"])\n    with open(os.path.join(report_dir_base, \"test.log\"), \"r\") as rfh:\n        content = rfh.read()\n        assert \"message from test 1\" in content\n        assert \"message from test 2\" in content\n", "def test_log_capture_handler_emit(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n            import pytest\n            logger = logging.getLogger(__name__)\n            logger.setLevel(logging.DEBUG)\n            handler = pytest.LogCaptureHandler()\n            logger.addHandler(handler)\n            logger.debug('debug message')\n            logger.info('info message')\n            logger.warning('warning message')\n            logger.error('error message')\n            logger.critical('critical message')\n            logger.removeHandler(handler)\n            return handler.records\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*1 passed in *\"])\n    records = result.parseoutcomes().passed[0].user_properties[0][1]\n    assert len(records) == 5\n    assert records[0].levelname == 'DEBUG'\n    assert records[1].levelname == 'INFO'\n    assert records[2].levelname == 'WARNING'\n    assert records[3].levelname == 'ERROR'\n    assert records[4].levelname == 'CRITICAL'\n", "def test_log_set_path_directory(testdir):\n    report_dir_base = testdir.tmpdir.strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file_level = DEBUG\n        log_cli=true\n        \"\"\"\n    )\n    testdir.makeconftest(\n        \"\"\"\n            import os\n            import pytest\n            @pytest.hookimpl(hookwrapper=True, tryfirst=True)\n                config = item.config\n                logging_plugin = config.pluginmanager.get_plugin(\"logging-plugin\")\n                report_file = os.path.join({}, item._request.node.name)\n                logging_plugin.set_log_path(report_file)\n                yield\n        \"\"\".format(\n            repr(report_dir_base)\n        )\n    )\n    testdir.makepyfile(\n        \"\"\"\n            import logging\n            logger = logging.getLogger(\"testcase-logger\")\n                logger.info(\"message from test 1\")\n                assert True\n\n                logger.debug(\"message from test 2\")\n                assert True\n        \"\"\"\n    )\n\n    # Check that logging to a directory works\n    testdir.runpytest()\n\n    with open(os.path.join(report_dir_base, \"test_first\"), \"r\") as rfh:\n        content = rfh.read()\n        assert \"message from test 1\" in content\n\n    with open(os.path.join(report_dir_base, \"test_second\"), \"r\") as rfh:\n        content = rfh.read()\n        assert \"message from test 2\" in content\n\n    # Check that logging to a non-existent directory raises an error\n    testdir.makeconftest(\n        \"\"\"\n            import os\n            import pytest\n            @pytest.hookimpl(hookwrapper=True, tryfirst=True)\n                config = item.config\n                logging_plugin = config.pluginmanager.get_plugin(\"logging-plugin\")\n                report_file = os.path.join('non_existent_dir', item._request.node.name)\n                logging_plugin.set_log_path(report_file)\n                yield\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    assert \"No such file or directory\" in result.stderr.str()\n    assert result.ret != 0\n"], "sample_43": ["def test_fit_empty_data():\n    with pytest.raises(ValueError):\n        bayesian_blocks(np.array([]), fitness='events')\n\n    with pytest.raises(ValueError):\n        bayesian_blocks(np.array([]), fitness='regular_events', dt=1)\n\n    with pytest.raises(ValueError):\n        bayesian_blocks(np.array([]), fitness='measures', x=np.array([]), sigma=np.array([]))\n", "def test_point_measures_fitness_functions():\n    \"\"\"Test results for point measures fitness function\"\"\"\n    rng = np.random.RandomState(42)\n\n    # Homoscedastic\n    t = np.linspace(0, 1, 11)\n    x = np.exp(-0.5 * (t - 0.5) ** 2 / 0.01 ** 2)\n    sigma = 0.05\n    x = x + sigma * rng.randn(len(x))\n\n    # String fitness\n    edges1 = bayesian_blocks(t, x, sigma, fitness='measures')\n    assert_allclose(edges1, [0, 0.45, 0.55, 1])\n\n    # Class name fitness\n    edges2 = bayesian_blocks(t, x, sigma, fitness=PointMeasures)\n    assert_allclose(edges1, edges2)\n\n    # Class instance fitness\n    edges3 = bayesian_blocks(t, x, sigma, fitness=PointMeasures(p0=0.05))\n    assert_allclose(edges1, edges3)\n\n    # Heteroscedastic\n    sigma = 0.02 + 0.02 * rng.rand(len(x))\n    x = x + sigma * rng.randn(len(x))\n\n    # String fitness\n    edges1 = bayesian_blocks(t, x, sigma, fitness='measures')\n    assert_allclose(edges1, [0, 0.45, 0.55, 1])\n\n    # Class name fitness\n    edges2 = bayesian_blocks(t, x, sigma, fitness=PointMeasures)\n    assert_allclose(edges1, edges2)\n\n    # Class instance fitness\n    edges3 = bayesian_blocks(t, x, sigma, fitness=PointMeasures(p0=0.05))\n    assert_allclose(edges1, edges3)\n", "def test_computing_ncp_prior():\n    \"\"\"Test that ncp_prior is computed correctly when gamma or p0 is specified\"\"\"\n    fitness = FitnessFunc(p0=0.01)\n    N = 10\n    assert_allclose(fitness.compute_ncp_prior(N), fitness.p0_prior(N))\n\n    fitness = FitnessFunc(gamma=0.1)\n    assert_allclose(fitness.compute_ncp_prior(N), -np.log(fitness.gamma))\n\n    with pytest.raises(ValueError):\n        fitness = FitnessFunc()\n        fitness.compute_ncp_prior(N)\n", "def test_events_fitness_heterogeneous_data():\n    \"\"\"Test Bayesian blocks with heterogeneous event data\"\"\"\n    rng = np.random.RandomState(42)\n    t1 = rng.rand(100)\n    t2 = rng.rand(100) + 1\n    t = np.concatenate([t1, t2])\n    edges = bayesian_blocks(t, fitness='events')\n    assert_allclose(edges, [0.003, 0.499, 1.001, 1.996])\n\n    # Optional arguments are passed (p0)\n    p0_sel = 0.05\n    edges = bayesian_blocks(t, fitness='events', p0=p0_sel)\n    assert_allclose(edges, [0.003, 0.499, 1.001, 1.996])\n\n    # Optional arguments are passed (ncp_prior)\n    ncp_prior_sel = 4 - np.log(73.53 * p0_sel * (len(t) ** -0.478))\n    edges = bayesian_blocks(t, fitness='events',\n                            ncp_prior=ncp_prior_sel)\n    assert_allclose(edges, [0.003, 0.499, 1.001, 1.996])\n\n    # Optional arguments are passed (gamma)\n    gamma_sel = np.exp(-ncp_prior_sel)\n    edges = bayesian_blocks(t, fitness='events',\n                            gamma=gamma_sel)\n    assert_allclose(edges, [0.003, 0.499, 1.001, 1.996])\n", "def test_errors_on_fitness_class():\n    rng = np.random.RandomState(0)\n    t = rng.rand(100)\n\n    # test that a custom fitness function can't be used without a fitness method\n    class FitnessClass(FitnessFunc):\n            super().__init__()\n\n    with pytest.raises(NotImplementedError):\n        bayesian_blocks(t, fitness=FitnessClass)\n\n    # test that a custom fitness function can't be used without a validate_input method\n    class FitnessClass(FitnessFunc):\n            super().__init__()\n\n            pass\n\n    with pytest.raises(NotImplementedError):\n        bayesian_blocks(t, fitness=FitnessClass)\n"], "sample_91": ["    def test_permission_denied_with_exception(self):\n        \"\"\"\n        The permission_denied view should include the exception message in the template context.\n        \"\"\"\n        request = self.request_factory.get('/')\n        exception = Exception('Test exception')\n        response = permission_denied(request, exception)\n        self.assertContains(response, b'Test exception', status_code=403)\n", "def test_exception_message_in_404(self):\n    \"\"\"\n    The exception message is available in the 404 page context\n    \"\"\"\n    request = self.request_factory.get('/')\n    exception = Http404('Testing technical 404.')\n    response = page_not_found(request, exception)\n    self.assertContains(response, b'Testing technical 404.', status_code=404)\n    self.assertContains(response, b'path: /', status_code=404)\n", "def test_permission_denied_with_exception(self):\n    \"\"\"\n    The permission_denied view renders the 403 template with the exception\n    context variable\n    \"\"\"\n    request = self.request_factory.get('/')\n    exception = Exception('Testing permission denied')\n    response = permission_denied(request, exception)\n    self.assertContains(response, b'403 Forbidden', status_code=403)\n    self.assertContains(response, str(exception).encode('utf-8'), status_code=403)\n", "    def test_page_not_found_exception_details(self):\n        \"The page_not_found view includes details from the exception\"\n        request = self.request_factory.get('/nonexistent_url/')\n        exception = Http404(\"Testing technical 404.\")\n        response = page_not_found(request, exception)\n        self.assertContains(response, b'Testing technical 404.', status_code=404)\n        exception = Http404({\"message\": \"Testing technical 404.\"})\n        response = page_not_found(request, exception)\n        self.assertContains(response, b'Http404', status_code=404)\n", "def test_permission_denied_view(self):\n    \"\"\"\n    Test the permission_denied view.\n    \"\"\"\n    request = self.request_factory.get('/')\n    response = permission_denied(request, Exception('Forbidden'))\n    self.assertEqual(response.status_code, 403)\n    self.assertContains(response, b'<h1>403 Forbidden</h1>')\n    self.assertContains(response, b'403 Forbidden', status_code=403)\n    self.assertContains(response, b'<!doctype html>', response.content)\n    self.assertContains(response, b'<html lang=\"en\">', response.content)\n    self.assertContains(response, b'<head>', response.content)\n    self.assertContains(response, b'<title>403 Forbidden</title>', response.content)\n    self.assertContains(response, b'<body>', response.content)\n\n    # Test that the exception message is passed to the template\n    self.assertContains(response, b'Forbidden', status_code=403)\n"], "sample_174": ["    def test_validate_autopk_value(self):\n        self.assertEqual(self.ops.validate_autopk_value(42), 42)\n", "def test_check_expression_support(self):\n    self.ops.check_expression_support('supported_expression')\n", "    def test_year_lookup_bounds_for_date_field(self):\n        value = 2024\n        first, second = self.ops.year_lookup_bounds_for_date_field(value)\n        self.assertIsInstance(first, str)\n        self.assertIsInstance(second, str)\n        self.assertEqual(first, '2024-01-01')\n        self.assertEqual(second, '2024-12-31')\n", "def test_adapt_decimalfield_value(self):\n    value = decimal.Decimal('3.14')\n    self.assertEqual(\n        self.ops.adapt_decimalfield_value(value),\n        utils.format_number(value)\n    )\n", "def test_limit_offset_sql(self):\n    self.assertEqual(self.ops.limit_offset_sql(5, 10), 'LIMIT 5 OFFSET 5')\n    self.assertEqual(self.ops.limit_offset_sql(None, 10), 'LIMIT 10 OFFSET 0')\n    self.assertEqual(self.ops.limit_offset_sql(5, None), 'OFFSET 5')\n"], "sample_991": ["def test_infinite_product_convergence():\n    n = Symbol('n', integer=True, positive=True)\n    assert Product(n**2, (n, 1, oo)).is_convergent() is S.false\n    assert Product(1/n**2, (n, 1, oo)).is_convergent() is S.false\n    assert Product(1/n, (n, 1, oo)).is_convergent() is S.false\n    assert Product(1 + 1/n, (n, 1, oo)).is_convergent() is S.false\n    assert Product(1 + 1/n**2, (n, 1, oo)).is_convergent() is S.true\n    assert Product(1 - 1/n**2, (n, 1, oo)).is_convergent() is S.true\n    assert Product(n**2/n**(n+1), (n, 1, oo)).is_convergent() is S.false\n    assert Product(n**n/n**(n+1), (n, 1, oo)).is_convergent() is S.false\n", "def test_product_doit_edge_cases():\n    assert Product(0, (k, 1, n)).doit() == 0\n    assert Product(1, (k, 1, n)).doit() == 1\n    assert Product(x, (k, 1, 1)).doit() == x\n    assert Product(x**2, (k, 1, 1)).doit() == x**2\n    assert Product(x**k, (k, 1, 1)).doit() == x\n    assert Product(x**2, (k, 1, 2)).doit() == x**2 * x**2\n    assert Product(x**k, (k, 1, 2)).doit() == x * x**2\n", "def test_doit_with_deep():\n    # Test doit() method with deep=True and deep=False\n    n = Symbol('n')\n    p = Product(n**2, (n, 1, 5))\n    assert p.doit(deep=True) == p.doit()\n    assert p.doit(deep=False) == p.doit(deep=True)\n    assert p.doit(deep=True).doit(deep=False) == p.doit(deep=True)\n    assert p.doit(deep=False).doit(deep=True) == p.doit(deep=True)\n", "def test_infinite_product_divergent():\n    # Test that infinite products with a divergent sum of logs are divergent\n    from sympy import pi\n    assert Product(1 + 1/n, (n, 1, oo)).is_convergent() is S.false\n    assert Product(1 + 1/n**2, (n, 1, oo)).is_convergent() is S.true\n    assert Product(1 + 1/(n*pi), (n, 1, oo)).is_convergent() is S.false\n    assert Product(1 + 1/(n**2*pi), (n, 1, oo)).is_convergent() is S.true\n", "def test_issue_inverse_product_convergence():\n    from sympy import symbols, Product, log, exp, oo\n    n = symbols('n', integer=True)\n    # Test that the product convergence is properly handled when\n    # there is an inverse term.\n    p = Product(1 + 1/n, (n, 1, oo))\n    assert p.is_convergent() is S.false\n\n    p = Product(1 + 1/n**2, (n, 1, oo))\n    assert p.is_convergent() is S.true\n\n    p = Product(exp(1/n), (n, 1, oo))\n    assert p.is_convergent() is S.true\n\n    p = Product(log(n)/n, (n, 1, oo))\n    assert p.is_convergent() is S.false\n"], "sample_982": ["def test_factorrat_divisors():\n    from sympy import Rational\n    assert divisors(Rational(4, 5)) == [1, 2, 2, 4, 1/5, 2/5, 4/5]\n    assert udivisors(Rational(4, 5)) == [1, 4, 1/5]\n", "def test_core_edge_cases():\n    assert core(0, 3) == 0\n    assert core(1, 3) == 1\n    assert core(2, 3) == 2\n    assert core(8, 3) == 1\n    assert core(27, 3) == 1\n    assert core(7, 3) == 7\n    assert core(-35**13, 10) == -42875\n", "def test_repeated_prime_factors():\n    assert factorint(2**63) == {2: 63}\n    assert factorint(3**62) == {3: 62}\n    assert factorint(2**30 * 3**30) == {2: 30, 3: 30}\n    assert factorint(2**50 * 3**40 * 5**30) == {2: 50, 3: 40, 5: 30}\n", "def test_udivisor_count_issue_1():\n    # Test edge case reported in #18443\n    assert udivisor_count(2**32) == 2\n", "def test_edge_cases():\n    assert smoothness(2**33) == (2, 2**33)\n    assert smoothness(2**33+1) == (2**33+1, 2**33+1)\n    assert smoothness_p(2**33) == (0, [(2, (33, 2, 2**33))])\n    assert smoothness_p(2**33+1, visual=1) == 'p**i=2**33+1**1 has p-1 B=2, B-pow=2**33'\n    assert multiplicity(2, 2**33) == 33\n    assert perfect_power(2**33) == (2, 33)\n    assert trailing(2**33) == 33\n    assert factorint(2**33, limit=2**32) == {2**33: 1}\n    assert factorint(2**33+1, limit=2**32) == {2**33+1: 1}\n    assert factorrat(Rational(2**33), limit=2**32) == {2: 33}\n    assert factorrat(Rational(2**33+1), limit=2**32) == {2**33+1: 1}\n    assert primefactors(2**33) == [2]\n    assert primefactors(2**33+1) == [2**33+1]\n    assert divisors(2**33) == [1, 2, 2**2, 2**3, 2**4, 2**5, 2**6, 2**7, 2**8, 2**9, 2**10, 2**11, 2**12, 2**13, 2**14, 2**15, 2**16, 2**17, 2**18, 2**19, 2**20, 2**21, 2**22, 2**23, 2**24, 2**25, 2**26, 2**27, 2**28, 2**29, 2**30, 2**31, 2**32, 2**33]\n    assert udivisors(2**33) == [1, 2**33]\n    assert udivisor_count("], "sample_938": ["def test_man_show_urls(app, status, warning):\n    app.build()\n    content = (app.outdir / 'python.1').read_text()\n    assert 'https://' in content\n    assert 'http://' in content\n", "def test_no_man_pages_config(app, status, warning):\n    app.builder.build_all()\n    assert (app.outdir / 'sphinxtests.1').exists() is False\n", "def test_get_target_uri(app, status, warning):\n    app.build()\n    builder = app.builder\n    assert builder.get_target_uri('index') == ''\n    with pytest.raises(builder.env.NoUri):\n        builder.get_target_uri('index', 'token')\n", "def test_no_man_pages_config(app, status, warning):\n    app.config.man_pages = None\n    app.builder.init()\n    assert app.builder.init.called\n    assert not app.builder.write.called\n", "def test_man_pages_with_no_authors(app, status, warning):\n    app.config.man_pages = [('index', 'python', 'Python Documentation', '', 1)]\n    app.build()\n    content = (app.outdir / 'python.1').read_text()\n    assert '.TH \"python\" \"1\" \"\" \"Python Documentation\"' in content\n"], "sample_59": ["def test_model_state_fields_cache_descriptor(self):\n    model = Worker()\n    self.assertIsInstance(model.fields_cache, dict)\n    model.fields_cache['test'] = 'value'\n    self.assertEqual(model.fields_cache['test'], 'value')\n", "    def test_check_constraints(self):\n        # Test that model constraints are checked\n        class ModelWithConstraint(models.Model):\n            x = models.IntegerField()\n            y = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(\n                        check=models.Q(x__gt=models.F('y')),\n                        name='check_x_gt_y'\n                    )\n                ]\n\n        with self.assertRaisesMessage(ValidationError, 'x must be greater than y'):\n            ModelWithConstraint(x=1, y=2).full_clean()\n\n        # Test that a model without constraints does not raise an error\n        class ModelWithoutConstraint(models.Model):\n            x = models.IntegerField()\n            y = models.IntegerField()\n\n        ModelWithoutConstraint(x=1, y=2).full_clean()\n\n        # Test that a model with a constraint that is not a CheckConstraint\n        # does not raise a validation error\n        class ModelWithUniqueConstraint(models.Model):\n            x = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['x'],\n                        name='unique_x'\n                    )\n                ]\n\n        ModelWithUniqueConstraint(x=1).full_clean()\n", "    def test_full_clean_with_non_field_error(self):\n        \"\"\"\n        Regression test for #28566: full_clean raises a ValidationError with\n        non-field error if a model's clean method raises a ValidationError.\n        \"\"\"\n        class TestModel(models.Model):\n                raise ValidationError(_('Non-field error.'), code='non_field_error')\n\n        test_model = TestModel()\n        with self.assertRaisesMessage(ValidationError, 'Non-field error.'):\n            test_model.full_clean()\n", "    def test_modelbase_metaclass(self):\n        \"\"\"\n        Test the ModelBase metaclass.\n        \"\"\"\n        # Check that the metaclass is correctly set.\n        self.assertEqual(Model._meta.__class__, ModelBase)\n\n        # Check that the metaclass is correctly used when creating a new model.\n        class NewModel(metaclass=ModelBase):\n            pass\n\n        # Check that the metaclass is correctly used when creating a new model with a custom metaclass.\n        class CustomMetaclass(type):\n                return super().__new__(cls, name, bases, attrs)\n\n        class NewModelWithCustomMetaclass(metaclass=CustomMetaclass):\n            pass\n\n        # Check that the ModelBase metaclass is correctly used when creating a new model with a custom metaclass.\n        class NewModelWithModelBaseAndCustomMetaclass(metaclass=ModelBase, metaclass=CustomMetaclass):\n            pass\n\n        # Check that the metaclass is correctly used when creating a new model with multiple inheritance.\n        class ParentModel(metaclass=ModelBase):\n            pass\n\n        class ChildModel(ParentModel):\n            pass\n", "    def test_modelbase_instantiation(self):\n        # ModelBase shouldn't accept a Meta class with a non-string db_table.\n        with self.assertRaisesMessage(TypeError, \"db_table must be a string\"):\n            class Model(metaclass=ModelBase):\n                class Meta:\n                    db_table = 123\n"], "sample_160": ["def test_format_edge_cases(self):\n    self.assertEqual(nformat(float('inf'), '.'), 'inf')\n    self.assertEqual(nformat(float('-inf'), '.'), '-inf')\n    self.assertEqual(nformat(float('nan'), '.'), 'nan')\n    self.assertEqual(nformat(Decimal('inf'), '.'), 'inf')\n    self.assertEqual(nformat(Decimal('-inf'), '.'), '-inf')\n    self.assertEqual(nformat(Decimal('NaN'), '.'), 'nan')\n    self.assertEqual(nformat('inf', '.'), 'inf')\n    self.assertEqual(nformat('-inf', '.'), '-inf')\n    self.assertEqual(nformat('nan', '.'), 'nan')\n", "def test_force_grouping_with_use_l10n(self):\n    # Test that `force_grouping` takes precedence over `USE_L10N`\n    with self.settings(USE_THOUSAND_SEPARATOR=True, USE_L10N=False):\n        self.assertEqual(nformat(1234, '.', grouping=3, thousand_sep=',', use_l10n=True, force_grouping=False), '1234')\n        self.assertEqual(nformat(1234, '.', grouping=3, thousand_sep=',', use_l10n=True, force_grouping=True), '1,234')\n\n    # Test that `force_grouping` takes precedence over `USE_L10N` when USE_L10N is True\n    with self.settings(USE_THOUSAND_SEPARATOR=True, USE_L10N=True):\n        self.assertEqual(nformat(1234, '.', grouping=3, thousand_sep=',', use_l10n=False, force_grouping=False), '1,234')\n        self.assertEqual(nformat(1234, '.', grouping=3, thousand_sep=',', use_l10n=False, force_grouping=True), '1,234')\n\n    # Test that `force_grouping` takes precedence over `USE_L10N` when USE_THOUSAND_SEPARATOR is False\n    with self.settings(USE_THOUSAND_SEPARATOR=False, USE_L10N=True):\n        self.assertEqual(nformat(1234, '.', grouping=3, thousand_sep=',', use_l10n=True, force_grouping=False), '1234')\n        self.assertEqual(nformat(1234, '.', grouping=3, thousand_sep=',', use_l10n=True, force_grouping=True), '1,234')\n", "def test_force_grouping_and_decimal_pos(self):\n    self.assertEqual(nformat(1234, '.', decimal_pos=2, grouping=3, thousand_sep=',', force_grouping=True), '1,234.00')\n    self.assertEqual(nformat(1234, '.', decimal_pos=2, grouping=3, thousand_sep='.', force_grouping=True), '1.234.00')\n    self.assertEqual(nformat(1234, '.', decimal_pos=None, grouping=3, thousand_sep='.', force_grouping=True), '1.234')\n    self.assertEqual(nformat(1234, '.', decimal_pos=0, grouping=3, thousand_sep='.', force_grouping=True), '1.234')\n", "def test_format_edge_cases(self):\n    # Test that a negative number with a large exponent is formatted correctly.\n    self.assertEqual(nformat(Decimal('-1e-1000'), '.'), '0')\n    \n    # Test that a very large Decimal with a large exponent is formatted correctly.\n    self.assertEqual(nformat(Decimal('1e1000'), '.'), '1e+1000')\n    \n    # Test that a very small Decimal with a large exponent is formatted correctly.\n    self.assertEqual(nformat(Decimal('1e-1000'), '.'), '0')\n    \n    # Test that a Decimal with a large exponent and decimal positions is formatted correctly.\n    self.assertEqual(nformat(Decimal('1e1000'), '.', decimal_pos=2), '1.00e+1000')\n    \n    # Test that a Decimal with a very large exponent and decimal positions is formatted correctly.\n    self.assertEqual(nformat(Decimal('1e2000'), '.', decimal_pos=2), '1.00e+2000')\n    \n    # Test that a Decimal with a very small exponent and decimal positions is formatted correctly.\n    self.assertEqual(nformat(Decimal('1e-2000'), '.', decimal_pos=2), '0.00')\n    \n    # Test that a large negative number with decimal positions is formatted correctly.\n    self.assertEqual(nformat(-123456789012345678901234567890, '.', decimal_pos=2), '-123456789012345678901234567890.00')\n    \n    # Test that a large negative number with thousand separator and decimal positions is formatted correctly.\n    self.assertEqual(nformat(-123456789012345678901234567890, '.', thousand_sep=',', grouping=3, decimal_pos=2, force_grouping=True), '-123,456,789,012,345,678,901,234,567,890.00')\n", "def test_format_exponential(self):\n    # Test numbers that require exponential notation due to high memory usage\n    tests = [\n        ('1e200', None, '1e+200'),\n        ('9e200', None, '9e+200'),\n        ('1e201', None, '1e+201'),\n        ('9e201', None, '9e+201'),\n        ('1' + ('0' * 200), None, '1e+200'),\n        ('9' + ('9' * 200), None, '9e+200'),\n    ]\n    for value, decimal_pos, expected_value in tests:\n        with self.subTest(value=value):\n            self.assertEqual(nformat(Decimal(value), '.'), expected_value)\n"], "sample_798": ["def test_ridge_regression_inconsistent_target_shape():\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([1, 2, 3])\n\n    with pytest.raises(ValueError):\n        ridge_regression(X, y, alpha=1.0, solver='cholesky')\n", "def test_ridge_regression_alpha_array_length(solver):\n    # Test ridge regression with alpha array length not equal to number of targets\n\n    rng = np.random.RandomState(42)\n    n_samples, n_features, n_targets = 20, 10, 5\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples, n_targets)\n\n    # Number of alphas does not match the number of targets\n    alpha = np.arange(n_targets - 1)\n\n    ridge = Ridge(alpha=alpha, solver=solver)\n    assert_raises(ValueError, ridge.fit, X, y)\n\n    # Number of alphas matches the number of targets\n    alpha = np.arange(n_targets)\n    ridge = Ridge(alpha=alpha, solver=solver)\n    ridge.fit(X, y)\n    assert_equal(ridge.coef_.shape, (n_targets, n_features))\n", "def test_ridgecv_solvers():\n    # Test RidgeCV with all solvers.\n    X, y = datasets.load_diabetes(return_X_y=True)\n    X = np.c_[X, np.ones(X.shape[0])]\n    for solver in ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']:\n        ridgecv = RidgeCV(solver=solver, alphas=(1.0,))\n        ridgecv.fit(X, y)\n", "def test_ridge_regression_check_alpha():\n    \"\"\"check if ridge_regression function can handle different types of alpha\"\"\"\n\n    rng = check_random_state(42)\n    X = rng.rand(1000, 3)\n    true_coefs = [1, 2, 0.1]\n    y = np.dot(X, true_coefs)\n\n    # test list of alpha values\n    alpha_list = [1e-3, 1e-2, 1e-1]\n    out_list = ridge_regression(X, y, alpha=alpha_list)\n    assert len(out_list.shape) == 2\n\n    # test single alpha value\n    alpha_single = 1e-2\n    out_single = ridge_regression(X, y, alpha=alpha_single)\n    assert len(out_single.shape) == 1\n\n    # test numpy array of alpha values\n    alpha_array = np.array([1e-3, 1e-2, 1e-1])\n    out_array = ridge_regression(X, y, alpha=alpha_array)\n    assert len(out_array.shape) == 2\n\n    # test numpy array of a single alpha value\n    alpha_array_single = np.array([1e-2])\n    out_array_single = ridge_regression(X, y, alpha=alpha_array_single)\n    assert len(out_array_single.shape) == 1\n\n    # test scalar alpha value\n    alpha_scalar = 1e-2\n    out_scalar = ridge_regression(X, y, alpha=alpha_scalar)\n    assert len(out_scalar.shape) == 1\n", "def test_ridge_regression_check_shapes(X_type, y_type, alpha_type):\n    \"\"\"Check the output shapes of the ridge regression function\"\"\"\n\n    n_samples, n_features = 100, 10\n    X = np.random.rand(n_samples, n_features)\n    if X_type == 'sparse':\n        X = sp.csr_matrix(X)\n\n    if y_type == '1d':\n        y = np.random.rand(n_samples)\n    elif y_type == '2d':\n        y = np.random.rand(n_samples, 5)\n\n    if alpha_type == 'scalar':\n        alpha = 1.0\n    elif alpha_type == 'array':\n        if y_type == '1d':\n            alpha = np.random.rand(1)\n        elif y_type == '2d':\n            alpha = np.random.rand(y.shape[1])\n\n    if y_type == '1d':\n        coef = ridge_regression(X, y, alpha, solver='cholesky')\n        assert coef.shape == (n_features, )\n    elif y_type == '2d':\n        coef = ridge_regression(X, y, alpha, solver='cholesky')\n        assert coef.shape == (y.shape[1], n_features)\n"], "sample_23": ["def test_latitude_edge_cases():\n    \"\"\"\n    Test Latitude creation with edge cases.\n    \"\"\"\n    # Test the edge case where the value is exactly pi/2\n    lat = Latitude(np.pi / 2, u.rad)\n    assert lat.value == np.pi / 2\n    assert lat.unit == u.rad\n\n    # Test the edge case where the value is slightly less than pi/2\n    lat = Latitude(0.49999 * np.pi, u.rad)\n    assert lat.value == 0.49999 * np.pi\n    assert lat.unit == u.rad\n\n    # Test the edge case where the value is exactly -pi/2\n    lat = Latitude(-np.pi / 2, u.rad)\n    assert lat.value == -np.pi / 2\n    assert lat.unit == u.rad\n\n    # Test the edge case where the value is slightly greater than -pi/2\n    lat = Latitude(-0.49999 * np.pi, u.rad)\n    assert lat.value == -0.49999 * np.pi\n    assert lat.unit == u.rad\n", "def test_longitude_with_NaN_wrap_at():\n    # Check wrapping a Longitude object containing NaN values does not raise an error\n    lon = Longitude([0, np.nan, 1] * u.deg)\n    lon.wrap_at(180 * u.deg)\n    assert np.all(np.isnan(lon.value))\n", "def test_longitude_wrap_at_in_place(value, expected_value, wrap_angle, expected_wrap_angle):\n    \"\"\"\n    Test that Longitude values can be wrapped at different angles.\n    \"\"\"\n    longitude = Longitude(value, u.rad, wrap_angle=wrap_angle)\n    assert longitude.value == expected_value\n    assert longitude.wrap_angle.value == expected_wrap_angle\n    assert longitude.wrap_angle.unit == u.radian\n    assert longitude.dtype == np.float64\n", "def test_longitude_with_dms_tuple():\n    \"\"\"\n    Test creating a Longitude object with a (d, m, s) tuple.\n\n    Regression test for issue #13162\n    \"\"\"\n\n    with pytest.warns(AstropyDeprecationWarning, match=\"dms_to_degrees\"):\n        Longitude((1, 30, 0), unit=u.deg)\n    Longitude((-1, -30, 0), unit=u.deg)  # negative degrees\n    Longitude((1, 30, 0), unit=u.deg)  # positive degrees\n    Longitude((-1, 30, 0), unit=u.deg)  # negative degrees\n", "def test_angle_copy():\n    \"\"\"\n    Test the copy method\n    \"\"\"\n    a1 = Angle([1, 2, 3] * u.deg)\n    a2 = a1.copy()\n    assert a1 is not a2\n    assert np.all(a1.value == a2.value)\n    assert a1.unit is a2.unit\n    assert a1.dtype is a2.dtype\n\n    a3 = Longitude(a1)\n    assert isinstance(a3, Longitude)\n    a4 = a3.copy()\n    assert a4 is not a3\n    assert np.all(a3.value == a4.value)\n    assert a3.unit is a4.unit\n    assert a3.dtype is a4.dtype\n\n    a5 = Latitude(a1)\n    assert isinstance(a5, Latitude)\n    a6 = a5.copy()\n    assert a6 is not a5\n    assert np.all(a5.value == a6.value)\n    assert a5.unit is a6.unit\n    assert a5.dtype is a6.dtype\n\n    a7 = a1.copy(dtype=np.int32)\n    assert a7.dtype == np.int32\n    assert np.all(a1.value == a7.value)\n    assert a1.unit is a7.unit\n"], "sample_277": ["def test_invert(self):\n    q = Q(price__gt=F('discounted_price'))\n    path, args, kwargs = (~q).deconstruct()\n    self.assertEqual(args, ())\n    self.assertEqual(kwargs, {\n        'price__gt': F('discounted_price'),\n        '_negated': True,\n    })\n\n    self.assertEqual(~Q(), Q())\n\n    with self.assertRaisesMessage(TypeError, str(object())):\n        ~object()\n", "def test_combine_or_with_negated(self):\n    q1 = Q(price__gt=F('discounted_price'))\n    q2 = ~Q(price=F('discounted_price'))\n    q = q1 | q2\n    path, args, kwargs = q.deconstruct()\n    self.assertEqual(args, (\n        ('price__gt', F('discounted_price')),\n        ('price', F('discounted_price')),\n        ('_negated', True),\n    ))\n    self.assertEqual(kwargs, {'_connector': 'OR'})\n", "def test_resolve_expression(self):\n    q = Q(price__gt=F('discounted_price'))\n    query = Mock()\n    query._add_q = Mock(return_value=([], []))\n    query.promote_joins = Mock()\n    clause = q.resolve_expression(query)\n    query._add_q.assert_called_once()\n    query.promote_joins.assert_called_once()\n", "def test_negation(self):\n    q1 = Q(price__gt=F('discounted_price'))\n    q2 = ~Q(price=F('discounted_price'))\n    q = q1 & q2\n    path, args, kwargs = q.deconstruct()\n    self.assertEqual(args, (\n        ('price__gt', F('discounted_price')),\n        ('price', F('discounted_price')),\n    ))\n    self.assertEqual(kwargs, {'_negated': True})\n", "    def test_combine_not_empty_q(self):\n        q = Q(x=1)\n        self.assertEqual(~Q(), q)\n"], "sample_223": ["    def setUpTestData(cls):\n        # Create a few Orders.\n        cls.o1 = Order.objects.create(pk=1)\n        cls.o2 = Order.objects.create(pk=2)\n        cls.o3 = Order.objects.create(pk=3)\n\n        # Create some OrderItems for the first order with homogeneous\n        # status_id values\n        cls.oi1 = OrderItem.objects.create(order=cls.o1, status=1)\n        cls.oi2 = OrderItem.objects.create(order=cls.o1, status=1)\n        cls.oi3 = OrderItem.objects.create(order=cls.o1, status=1)\n\n        # Create some OrderItems for the second order with heterogeneous\n        # status_id values\n        cls.oi4 = OrderItem.objects.create(order=cls.o2, status=1)\n        cls.oi5 = OrderItem.objects.create(order=cls.o2, status=2)\n        cls.oi6 = OrderItem.objects.create(order=cls.o2, status=3)\n\n        # Create some OrderItems for the second order with heterogeneous\n        # status_id values\n        cls.oi7 = OrderItem.objects.create(order=cls.o3, status=2)\n        cls.oi8 = OrderItem.objects.create(order=cls.o3, status=3)\n        cls.oi9 = OrderItem.objects.create(order=cls.o3, status=4)\n", "    def setUpTestData(cls):\n        cls.n1 = Note.objects.create(note='n1', misc='foo', id=1)\n        cls.n2 = Note.objects.create(note='n2', misc='bar', id=2)\n        cls.e1 = ExtraInfo.objects.create(info='e1', note=cls.n1)\n        cls.e2 = ExtraInfo.objects.create(info='e2', note=cls.n2)\n        cls.a1 = Author.objects.create(name='a1', num=1001, extra=cls.e1)\n        cls.a2 = Author.objects.create(name='a2', num=2002, extra=cls.e1)\n", "    def test_ticket24525(self):\n        \"\"\"\n        Regression test for #24525: annotations on models with multiple inheritance\n        \"\"\"\n        leafa = LeafA.objects.create(data='first')\n        leafb = LeafB.objects.create(data='second')\n        Join.objects.create(a=leafa, b=leafb)\n        qs = Join.objects.annotate(leafa_data=F('a__data'), leafb_data=F('b__data'))\n        obj = qs.first()\n        self.assertEqual(obj.leafa_data, 'first')\n        self.assertEqual(obj.leafb_data, 'second')\n", "    def test_ticket_29963(self):\n        # Test filtering on a subquery that involves both model inheritance\n        # and a Q object.\n        s1 = School.objects.create()\n        st1 = Student.objects.create(school=s1)\n        s2 = School.objects.create()\n        st2 = Student.objects.create(school=s2)\n        s3 = School.objects.create()\n        Teacher.objects.create(school=s3)\n        self.assertQuerysetEqual(\n            School.objects.filter(\n                Q(school__isnull=True) |\n                Q(student__isnull=True)\n            ).order_by('pk'),\n            [s2, s1, s3]\n        )\n", "    def test_ticket_25558(self):\n        i1 = Item.objects.create(name='item1', created=datetime.datetime.now(), creator=self.a2, note=self.n2)\n        i2 = Item.objects.create(name='item2', created=datetime.datetime.now(), creator=self.a2, note=self.n2)\n        e1 = ExtraInfo.objects.create(info='e1', note=self.n2)\n        e2 = ExtraInfo.objects.create(info='e2', note=self.n2)\n        self.a2.extra = e1\n        self.a2.save()\n        self.a4.extra = e2\n        self.a4.save()\n        self.a1.item_set.add(i1)\n        self.a2.item_set.add(i2)\n        self.a2.item_set.add(i1)\n        self.a4.item_set.add(i2)\n        # Ensure the filtering works when the subquery is on the left side\n        self.assertQuerysetEqual(\n            Item.objects.filter(\n                creator__in=Author.objects.filter(extra__note=self.n2).values_list('item__creator', flat=True)\n            ).order_by('name'),\n            ['<Item: item1>', '<Item: item2>']\n        )\n        # And when the subquery is on the right side\n        self.assertQuerysetEqual(\n            Item.objects.filter(\n                creator__in=Author.objects.values_list('item__creator', flat=True).filter(extra__note=self.n2)\n            ).order_by('name'),\n            ['<Item: item1>', '<Item: item2>']\n        )\n"], "sample_30": ["def test_resource_structure_2():\n    # Based on issue #1223, as reported by @astro-friedel and @RayPlante\n    from astropy.io.votable import tree as vot\n\n    vtf = vot.VOTableFile()\n\n    r1 = vot.Resource()\n    vtf.resources.append(r1)\n    t1 = vot.Table(vtf)\n    t1.name = \"t1\"\n    t2 = vot.Table(vtf)\n    t2.name = \"t2\"\n    r1.tables.append(t1)\n    r1.tables.append(t2)\n\n    r2 = vot.Resource()\n    vtf.resources.append(r2)\n    t3 = vot.Table(vtf)\n    t3.name = \"t3\"\n    t4 = vot.Table(vtf)\n    t4.name = \"t4\"\n    r2.tables.append(t3)\n    r2.tables.append(t4)\n\n    r3 = vot.Resource()\n    vtf.resources.append(r3)\n    t5 = vot.Table(vtf)\n    t5.name = \"t5\"\n    t6 = vot.Table(vtf)\n    t6.name = \"t6\"\n    r3.tables.append(t5)\n    r3.tables.append(t6)\n\n    r3r1 = vot.Resource()\n    r3.resources.append(r3r1)\n    t7 = vot.Table(vtf)\n    t7.name = \"t7\"\n    t8 = vot.Table(vtf)\n    t8.name = \"t8\"\n    r3r1.tables.append(t7)\n    r3r1.tables.append(t8)\n\n    buff = io.BytesIO()\n    vtf.to_xml(buff)\n\n    buff.seek(0)\n    vtf2 = parse(buff)\n\n    assert len(vtf2.resources) == 3\n\n    for r in range(len(vtf2.resources)):\n        res = vtf2.resources[r]\n        if r == 2:\n            assert len(res.tables) == 2\n            assert len(res.resources) == 1\n            assert len(res.resources[0].tables) == 2\n        else:\n            assert len(res.tables) == 2\n            assert len(res.resources) == 0\n", "def test_get_field_by_id_or_name():\n    votable = parse(get_pkg_data_filename(\"data/regression.xml\"))\n    field = votable.get_field_by_id_or_name(\"string_test\")\n    assert field.ID == \"string_test\"\n    assert field.datatype == \"char\"\n\n    field = votable.get_field_by_id_or_name(\"string test\")\n    assert field.ID == \"string_test\"\n    assert field.datatype == \"char\"\n\n    # Names with spaces are not unique\n    with pytest.raises(KeyError):\n        votable.get_field_by_id_or_name(\"unicode test\")\n", "def test_arraysize_mismatch():\n    votable = parse(get_pkg_data_filename(\"data/array_size_mismatch.xml\"))\n    table = votable.get_first_table()\n    assert table.fields[1].arraysize == \"2x2\"\n    assert table.fields[2].arraysize == \"2x2\"\n    assert table.fields[3].arraysize == \"2x2\"\n    assert_array_equal(table.array[\"matrix\"], [[[1, 0], [0, 1]], [[1, 0], [0, 1]]])\n    assert_array_equal(table.array[\"matrix2\"], [[[1, 0], [0, 1]], [[1, 0], [0, 1]]])\n    assert_array_equal(table.array[\"matrix3\"], [[[1, 0], [0, 1]], [[1, 0], [0, 1]]])\n", "def test_table_to_xml_roundtrip():\n    from astropy.table import Table\n    from astropy.io.votable.tree import VOTableFile\n\n    tab = Table(\n        {\n            \"ra\": [1.0, 2.0],\n            \"dec\": [1.1, 2.1],\n            \"flux\": [1.0, 2.0],\n            \"flux_err\": [0.1, 0.2],\n        }\n    )\n\n    vot = VOTableFile.from_table(tab)\n    vot.set_all_tables_format(\"tabledata\")\n\n    bio = io.BytesIO()\n    vot.to_xml(bio)\n    bio.seek(0)\n    vot2 = parse(bio)\n\n    tab2 = vot2.get_first_table().to_table()\n\n    assert tab.meta == tab2.meta\n    assert tab.colnames == tab2.colnames\n    assert np.all(tab[\"ra\"] == tab2[\"ra\"])\n    assert np.all(tab[\"dec\"] == tab2[\"dec\"])\n    assert np.all(tab[\"flux\"] == tab2[\"flux\"])\n    assert np.all(tab[\"flux_err\"] == tab2[\"flux_err\"])\n", "def test_group_roundtrip():\n    from astropy.table import Table\n    from astropy.io.votable.tree import Field, Group, Param, Resource, Table as VOTable, VOTableFile\n\n    t = Table(\n        {\n            \"a\": [1, 2, 3],\n            \"b\": [4, 5, 6],\n            \"c\": [7, 8, 9],\n            \"d\": [10, 11, 12],\n        }\n    )\n\n    votable_file = VOTableFile()\n    resource = Resource()\n    votable_file.resources.append(resource)\n    votable = VOTable(votable_file)\n    resource.tables.append(votable)\n\n    field_a = Field(votable_file, name=\"a\", datatype=\"int\")\n    field_b = Field(votable_file, name=\"b\", datatype=\"int\")\n    field_c = Field(votable_file, name=\"c\", datatype=\"int\")\n    field_d = Field(votable_file, name=\"d\", datatype=\"int\")\n\n    group1 = Group(resource, name=\"group1\")\n    group1.entries.extend([field_a, field_b])\n\n    group2 = Group(resource, name=\"group2\")\n    group2.entries.extend([field_c, field_d])\n\n    votable.fields.extend([group1, group2])\n\n    # Add a parameter to test that it's not included in the flattened list\n    param = Param(votable_file, name=\"param\", datatype=\"int\")\n    votable.params.append(param)\n\n    bio = io.BytesIO()\n    votable_file.to_xml(bio)\n    bio.seek(0)\n    votable_file2 = parse(bio)\n\n    # Check that the groups were written out to the file correctly\n    assert len(votable_file2.resources[0].groups) == 2\n\n    # Check that the fields in the groups are the same as the original fields\n    assert len(votable_file2.resources[0].tables[0].fields) == 2\n    assert len(votable_file2.resources[0].tables[0].fields[0].entries) == 2\n    assert len(votable_file2.resources[0].tables[0].fields[1].entries) == 2\n\n    # Check that the parameter was not included in the flattened list\n    assert len(votable_file2.resources[0].tables[0].params)"], "sample_544": ["def test_interpolation_stage(fig_test, fig_ref):\n    x = np.linspace(0, 1)\n    xx, yy = np.meshgrid(x, x)\n\n    zz = np.exp(- 3 * ((xx - 0.5) ** 2) + (yy - 0.7 ** 2))\n    cmap = mpl.colormaps['viridis']\n    ax = fig_test.add_subplot()\n    ax.imshow(zz, interpolation='antialiased', interpolation_stage='data',\n              cmap=cmap)\n\n    ax = fig_ref.add_subplot()\n    ax.imshow(zz, interpolation='antialiased', interpolation_stage='rgba',\n              cmap=cmap)\n", "def test_interpolation_stage(fig_test, fig_ref):\n    # Test that changing the interpolation_stage changes the output.\n    ax_test = fig_test.subplots()\n    ax_ref = fig_ref.subplots()\n\n    np.random.seed(19680801)\n    A = np.random.rand(50, 50)\n\n    im = ax_test.imshow(A, interpolation='antialiased', interpolation_stage='data')\n    im = ax_test.imshow(A, interpolation='antialiased', interpolation_stage='rgba')\n    ax_test.set_xlim(0, 10)\n    ax_test.set_ylim(0, 10)\n\n    im = ax_ref.imshow(A, interpolation='hanning')\n    im = ax_ref.imshow(A, interpolation='nearest')\n    ax_ref.set_xlim(0, 10)\n    ax_ref.set_ylim(0, 10)\n", "def test_bboimage():\n    fig, ax = plt.subplots()\n    image = np.arange(100).reshape((10, 10))\n    bbox_im = BboxImage(ax.bbox)\n    bbox_im.set_data(image)\n    ax.add_artist(bbox_im)\n    with pytest.raises(ValueError, match=\"Unknown type of bbox\"):\n        bbox_im.get_window_extent()\n    bbox_im = BboxImage(TransformedBbox(Bbox([[0, 0], [1, 1]]), ax.transData))\n    bbox_im.set_data(image)\n    ax.add_artist(bbox_im)\n    assert isinstance(bbox_im.get_window_extent(), BboxBase)\n    bbox_im = BboxImage(lambda r: TransformedBbox(Bbox([[0, 0], [1, 1]]), r))\n    bbox_im.set_data(image)\n    ax.add_artist(bbox_im)\n    assert isinstance(bbox_im.get_window_extent(), BboxBase)\n", "def test_image_clip_path_transformation(fig_test, fig_ref):\n    \"\"\"Check that clipped images are transformed correctly.\"\"\"\n    # Create a test image and a clip path, and then apply a transformation\n    # to the axes and check that the clip path is transformed correctly.\n    im = np.arange(100).reshape((10, 10))\n\n    fig_test, ax_test = fig_test.subplots()\n    fig_ref, ax_ref = fig_ref.subplots()\n\n    # Create a clip path in the form of a circle\n    circle = patches.Circle((5, 5), 3, transform=ax_test.transData)\n\n    # Apply a transformation to the axes\n    ax_test.set_xlim([0, 10])\n    ax_test.set_ylim([0, 10])\n\n    # Create an image object and apply the clip path\n    im_test = ax_test.imshow(im, clip_path=circle, clip_on=True)\n\n    # Create an image object with the transformed clip path\n    circle_ref = patches.Circle((5, 5), 3, transform=ax_ref.transData)\n    im_ref = ax_ref.imshow(im, clip_path=circle_ref, clip_on=True)\n\n    # Apply the same transformation to the reference axes\n    ax_ref.set_xlim([0, 10])\n    ax_ref.set_ylim([0, 10])\n", "def test_non_uniform_image_edges(fig_test, fig_ref):\n    \"\"\"Test that NonUniformImage does not \"leak\" beyond its edges.\"\"\"\n    np.random.seed(19680801)\n    dpi = 100\n    x = np.linspace(0, 1, 1000)\n    y = np.linspace(0, 1, 1000)\n    a = np.random.rand(1000, 1000)\n\n    for fig in [fig_test, fig_ref]:\n        ax = fig.add_axes([0, 0, 1, 1], aspect='auto', frameon=False,\n                          xticks=[], yticks=[])\n\n    ax_test = fig_test.axes[0]\n    ax_test.imshow(a, extent=(0, 1, 0, 1), interpolation='nearest')\n\n    ax_ref = fig_ref.axes[0]\n    im = ax_ref.imshow(a, extent=(0, 1, 0, 1), interpolation='nearest')\n    im.set_clip_on(False)\n    ax_ref.set_xlim(-0.5, 1.5)\n    ax_ref.set_ylim(-0.5, 1.5)\n\n    # Make the image invisible in ax_test, but visible in ax_ref if we pan\n    # ax_test up.\n    im.set_clip_box(ax_test.bbox)\n\n    # The black background color does not matter here as the only point is that\n    # the NonUniformImage does not \"leak\" out of the edges.\n    ax_ref.set_facecolor('black')\n\n    buff_test = io.BytesIO()\n    fig_test.savefig(buff_test, facecolor=\"black\", format=\"rgba\")\n    buff_ref = io.BytesIO()\n    fig_ref.savefig(buff_ref, facecolor=\"black\", format=\"rgba\")\n\n    buff_test.seek(0)\n    arr_test = plt.imread(buff_test)\n\n    buff_ref.seek(0)\n    arr_ref = plt.imread(buff_ref)\n\n    assert_array_equal(arr_test[:, :, :3], arr_ref[:, :, :3])\n"], "sample_986": ["def test_evalf_abs():\n    assert NS(abs(pi*I), 10) == '3.141592654*I'\n    assert NS(abs(pi), 10) == '3.141592654'\n    assert NS(abs(-pi), 10) == '3.141592654'\n    assert NS(abs(pi + I), 10) == '3.31662479'\n    assert NS(abs(-pi - I), 10) == '3.31662479'\n", "def test_evalf_evalf_table():\n    # Test to ensure evalf_table is generated only once\n    global evalf_table\n    old_evalf_table = evalf_table\n    evalf_table = None\n    _create_evalf_table()\n    assert evalf_table == old_evalf_table\n    evalf_table = old_evalf_table\n", "def test_evalf_hyperbolic_functions():\n    assert NS('sinh(1)', 15) == '1.17520119364380'\n    assert NS('cosh(1)', 15) == '1.54308063481524'\n    assert NS('tanh(1)', 15) == '0.76159415595575'\n    assert NS('acosh(1)', 15) == '0.00000000000000'\n    assert NS('asinh(1)', 15) == '0.88137804997374'\n    assert NS('atanh(1/2)', 15) == '0.54930614433405'\n    assert NS('sinh(10**-6)', 15) == '1.00000000000000e-6'\n    assert NS('cosh(10**-6)', 15) == '1.00000000000000'\n    assert NS('tanh(10**-6)', 15) == '1.00000000000000e-6'\n    assert NS('acosh(1+10**-6)', 15) == '1.00000000000000e-6'\n    assert NS('asinh(10**-6)', 15) == '9.99999999999833e-7'\n    assert NS('atanh(1/2-10**-6)', 15) == '0.54930614433404'\n", "def test_evalf_product_with_powers():\n    # issue 11693\n    assert NS(Product(n**n, (n, 1, 3)), 5) == '9.3647e+5'\n", "def test_evalf_abs():\n    # Check that Abs(x) correctly handles complex and real cases\n    assert NS(Abs(3+4*I), 15) == '5.00000000000000'\n    assert NS(Abs(-3-4*I), 15) == '5.00000000000000'\n    assert NS(Abs(3+0*I), 15) == '3.00000000000000'\n    assert NS(Abs(0+3*I), 15) == '3.00000000000000'\n    assert NS(Abs(0+0*I), 15) == '0.000000000000000'\n    # Check that Abs(x) also works with Float\n    assert NS(Abs(3.5+4.2*I), 15) == '5.51097386587665'\n    assert NS(Abs(-3.5-4.2*I), 15) == '5.51097386587665'\n    # Check that Abs(x) works with high precision\n    assert NS(Abs(3+4*I), 100) == '5.00000000000000000000000000000000000000000000000000'\n    assert NS(Abs(-3-4*I), 100) == '5.00000000000000000000000000000000000000000000000000'\n"], "sample_718": ["def test_check_estimators_partial_fit_n_features():\n    # Test that check_estimator raises an error when partial_fit with\n    # changing n_features is run.\n    class EstimatorWithPartialFit(BaseEstimator):\n            self.n_features_ = X.shape[1]\n            return self\n\n            return self\n\n    est = EstimatorWithPartialFit()\n    msg = (\"The estimator EstimatorWithPartialFit does not raise an error when \"\n           \"the number of features changes between calls to partial_fit.\")\n    assert_raises_regex(AssertionError, msg, check_estimator, est)\n", "def test_check_estimator_sparse_data():\n    # Test that check_estimator checks for sparse data\n    class SparseErrorEstimator(BaseEstimator):\n            X, y = check_X_y(X, y)\n            if sp.issparse(X):\n                raise ValueError(\"Sparse data not supported\")\n            return self\n\n            X = check_array(X)\n            return np.ones(X.shape[0])\n\n    name = SparseErrorEstimator.__name__\n    msg = \"Estimator {} doesn't seem to fail gracefully on sparse data\".format(\n        name)\n    assert_raises_regex(AssertionError, msg, check_estimator, SparseErrorEstimator)\n\n    class SparseNotSupportedEstimator(BaseEstimator):\n            X, y = check_X_y(X, y, accept_sparse='csr')\n            return self\n\n            X = check_array(X, accept_sparse='csr')\n            return np.ones(X.shape[0])\n\n    check_estimator(SparseNotSupportedEstimator)\n", "def test_check_estimator_data_type():\n    # test that check_estimator checks for the correct handling of data types\n    class WrongDataType(BaseEstimator):\n            X, y = check_X_y(X, y)\n            if X.dtype == np.float32:\n                raise ValueError(\"Does not accept float32\")\n            return self\n\n            return np.ones(X.shape[0])\n\n    msg = \"Unknown label type\"\n    assert_raises_regex(AssertionError, msg, check_estimator, WrongDataType)\n\n    class WrongDataTypePredict(BaseEstimator):\n            X, y = check_X_y(X, y)\n            return self\n\n            if X.dtype == np.float32:\n                raise ValueError(\"Does not accept float32\")\n            return np.ones(X.shape[0])\n\n    msg = \"Estimator doesn't check for unknown data type in predict\"\n    assert_raises_regex(AssertionError, msg, check_estimator, WrongDataTypePredict)\n", "def test_check_estimator_sparse_data():\n    # check that check_estimator() checks sparse data\n    from sklearn.datasets import load_iris\n    iris = load_iris()\n\n    # dense\n    check_estimator(LinearRegression())\n\n    # sparse\n    X_sparse = sp.csr_matrix(iris.data)\n    y_sparse = iris.target\n    set_checking_parameters(LinearRegression())\n    set_random_state(LinearRegression())\n    LinearRegression().fit(X_sparse, y_sparse)\n\n    # should fail\n    class NonConformantEstimatorSparseData(object):\n            pass\n\n    assert_raises_regex(AssertionError,\n                        \"Estimator NonConformantEstimatorSparseData doesn't\"\n                        \" seem to fail gracefully on sparse data\",\n                        check_estimator, NonConformantEstimatorSparseData())\n", "def test_check_sparsify_coefficients():\n    # Test that check_sparsify_coefficients fails on a classifier without a\n    # sparsify method\n\n    class ClassifierWithNoSparsify(BaseEstimator):\n            self.coef_ = np.random.rand(X.shape[1])\n            return self\n\n            return np.ones(X.shape[0])\n\n    msg = (\"ClassifierWithNoSparsify should have a sparsify method that \"\n           \"makes the coef_ attribute sparse\")\n    assert_raises_regex(AssertionError, msg, check_estimator, ClassifierWithNoSparsify)\n\n"], "sample_869": ["def test_jaccard_similarity_score_deprecation_and_removal():\n    # Test that the function is actually deprecated and will be removed\n    # Dense label indicator matrix format\n    y1 = np.array([[0, 1, 1], [1, 0, 1]])\n    y2 = np.array([[0, 0, 1], [1, 0, 1]])\n\n    # size(y1 \\inter y2) = [1, 2]\n    # size(y1 \\union y2) = [2, 2]\n\n    with pytest.warns(FutureWarning):\n        jaccard_similarity_score(y1, y2)\n    \n    # Remove the function\n    del sklearn.metrics.jaccard_similarity_score\n    \n    with pytest.raises(AttributeError):\n        sklearn.metrics.jaccard_similarity_score\n", "def test_multilabel_jaccard_score_multiclass_data():\n    y_true = np.array([[1, 0, 1], [1, 1, 1]])\n    y_pred = np.array([[0, 1, 0], [0, 1, 1]])\n    y_true_mc = [0, 2]\n    y_pred_mc = [1, 2]\n\n    lb = LabelBinarizer()\n    y_true_mc_bin = lb.fit_transform(y_true_mc)\n    y_pred_mc_bin = lb.fit_transform(y_pred_mc)\n\n    assert_almost_equal(jaccard_score(y_true_mc, y_pred_mc,\n                                      average='samples'),\n                        jaccard_score(y_true, y_pred,\n                                       average='samples'))\n    assert_almost_equal(jaccard_score(y_true_mc, y_pred_mc,\n                                      average='macro'),\n                        jaccard_score(y_true_mc_bin, y_pred_mc_bin,\n                                       average='macro'))\n", "def test_multilabel_jaccard_score_with_empty_labels():\n    # Empty labels\n    y_true = np.array([[0, 1], [1, 0]])\n    y_pred = np.array([[0, 1], [0, 1]])\n    msg = (\"Jaccard is ill-defined and being set to 0.0 in samples with \"\n           \"no true or predicted labels.\")\n    with pytest.raises(AssertionError):\n        assert (jaccard_score(y_true, y_pred, average=\"samples\",\n                              labels=[3, 4]) ==\n                assert_warns_message(UndefinedMetricWarning, msg,\n                                     jaccard_score,\n                                     y_true, y_pred, labels=[3, 4],\n                                     average=\"samples\"))\n", "def test_multilabel_jaccard_score_multiclass_labels(recwarn):\n    y_true = np.array([[0, 0], [0, 1]])\n    y_pred = np.array([[0, 1], [1, 1]])\n\n    y_true_mc = np.array([0, 1])\n    y_pred_mc = np.array([1, 2])\n\n    labels = ['a', 'b', 'c']\n    lb = LabelBinarizer()\n    lb.fit(labels)\n\n    y_true_mc_lb = lb.transform(y_true_mc)\n    y_pred_mc_lb = lb.transform(y_pred_mc)\n\n    assert_array_almost_equal(jaccard_score(y_true, y_pred), jaccard_score(y_true_mc, y_pred_mc, labels=labels))\n\n    assert_array_almost_equal(jaccard_score(y_true, y_pred, average='macro'), jaccard_score(y_true_mc, y_pred_mc, labels=labels, average='macro'))\n    assert_array_almost_equal(jaccard_score(y_true, y_pred, average='weighted'), jaccard_score(y_true_mc, y_pred_mc, labels=labels, average='weighted'))\n    assert_array_almost_equal(jaccard_score(y_true, y_pred, average='micro'), jaccard_score(y_true_mc, y_pred_mc, labels=labels, average='micro'))\n    assert_array_almost_equal(jaccard_score(y_true, y_pred, average='samples'), jaccard_score(y_true_mc_lb, y_pred_mc_lb, labels=labels, average='samples'))\n\n    assert_array_almost_equal(jaccard_score(y_true, y_pred, labels=['b'], average='macro'), jaccard_score(y_true_mc, y_pred_mc, labels=['b'], average='macro'))\n    assert_array_almost_equal(jaccard_score(y_true, y_pred, labels=['b'], average='weighted'), jaccard_score(y_true_mc, y_pred_mc, labels=['b'], average='weighted'))\n    assert_array_almost_equal(jaccard_score(y_true, y_pred, labels=['b'], average='micro'), jaccard_score(y_true_mc, y_pred_mc, labels=['b'], average='micro'))\n    assert_array_almost_equal(jaccard_score(y_true, y_pred, labels=['b'], average='samples'), jaccard_score(y_true_mc_lb, y_pred_mc_lb, labels=['b'], average='samples'))\n\n    assert not list(recwarn)\n", "def test_multilabel_confusion_matrix_normalized():\n    # Test multilabel confusion matrix - normalized\n    y_true = np.array([[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 1, 1]])\n    y_pred = np.array([[0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 0, 0]])\n\n    # test support for sample_weight with sample_wise\n    cm = multilabel_confusion_matrix(y_true, y_pred, sample_weight=[2, 3, 4])\n    assert_array_almost_equal(cm, [[[0, 0, 0, 3], [2, 0, 0, 0]],\n                                   [[2, 0, 0, 0], [0, 0, 0, 3]],\n                                   [[0, 4, 0, 0], [0, 0, 4, 0]],\n                                   [[0, 0, 0, 0], [0, 0, 0, 12]]])\n\n    # test support for sample_weight with sample_wise\n    cm = multilabel_confusion_matrix(y_true, y_pred, sample_weight=[2, 3, 4],\n                                     samplewise=True)\n    assert_array_almost_equal(cm, [[[0, 0, 0, 3], [2, 0, 0, 0]],\n                                   [[2, 0, 0, 0], [0, 0, 0, 3]],\n                                   [[0, 8, 0, 0], [0, 0, 12, 0]]])\n\n    # test support for labels with sample_wise\n    cm = multilabel_confusion_matrix(y_true, y_pred, labels=[0, 3],\n                                     sample_weight=[2, 3, 4], samplewise=True)\n    assert_array_almost_equal(cm, [[[0, 0], [2, 0]],\n                                   [[0, 0], [0, 3]],\n                                   [[0, 8], [0, 12]]])\n\n    # test support for sample_weight with sample_wise\n    cm = multilabel_confusion_matrix(y_true, y_pred, labels=[0, 3],\n                                     sample_weight=[2"], "sample_76": ["def test_language_settings_consistent_with_default(self):\n    with self.settings(LANGUAGE_CODE='en-us', LANGUAGES=[('en', 'English')]):\n        self.assertEqual(check_language_settings_consistent(None), [])\n", "def test_inconsistent_language_settings_with_empty_languages(self):\n    msg = (\n        'You have provided a value for the LANGUAGE_CODE setting that is '\n        'not in the LANGUAGES setting.'\n    )\n    with self.settings(LANGUAGE_CODE='fr', LANGUAGES=[]):\n        self.assertEqual(check_language_settings_consistent(None), [\n            Error(msg, id='translation.E004'),\n        ])\n", "def test_language_settings_consistent(self):\n    with self.settings(LANGUAGE_CODE='en', LANGUAGES=[('en', 'English')]):\n        self.assertEqual(check_language_settings_consistent(None), [])\n\n    with self.settings(LANGUAGE_CODE='en-us', LANGUAGES=[('en-us', 'English-US')]):\n        self.assertEqual(check_language_settings_consistent(None), [])\n\n    with self.settings(LANGUAGE_CODE='fr-CA', LANGUAGES=[('fr-CA', 'French-Canada')]):\n        self.assertEqual(check_language_settings_consistent(None), [])\n\n    with self.settings(LANGUAGE_CODE='es-419', LANGUAGES=[('es-419', 'Spanish-419')]):\n        self.assertEqual(check_language_settings_consistent(None), [])\n\n    with self.settings(LANGUAGE_CODE='zh-Hans', LANGUAGES=[('zh-Hans', 'Chinese-Hans')]):\n        self.assertEqual(check_language_settings_consistent(None), [])\n\n    with self.settings(LANGUAGE_CODE='ca-ES-valencia', LANGUAGES=[('ca-ES-valencia', 'Catalan-ES-valencia')]):\n        self.assertEqual(check_language_settings_consistent(None), [])\n\n    with self.settings(LANGUAGE_CODE='en', LANGUAGES=[]):\n        self.assertEqual(check_language_settings_consistent(None), [\n            Error('You have provided a value for the LANGUAGE_CODE setting that is not in the LANGUAGES setting.', id='translation.E004'),\n        ])\n\n    with self.settings(LANGUAGE_CODE=None, LANGUAGES=[('en', 'English')]):\n        self.assertEqual(check_language_settings_consistent(None), [\n            Error('You have provided a value for the LANGUAGE_CODE setting that is not in the LANGUAGES setting.', id='translation.E004'),\n        ])\n", "    def test_consistent_language_settings(self):\n        with self.settings(LANGUAGE_CODE='fr', LANGUAGES=[('fr', 'French')]):\n            self.assertEqual(check_language_settings_consistent(None), [])\n", "def test_language_settings_consistent(self):\n    with self.settings(LANGUAGE_CODE='en', LANGUAGES=[('en', 'English')]):\n        self.assertEqual(check_language_settings_consistent(None), [])\n"], "sample_1181": ["def test_permutedims():\n    if not np:\n        skip(\"NumPy not installed\")\n\n    A = np.array([[1, 2], [3, 4]])\n    B = np.array([[5, 6], [7, 8]])\n\n    expr = PermuteDims(ArrayTensorProduct(A, B), [1, 2, 3, 0])\n    f = lambdify((A, B), expr, 'numpy')\n    assert np.array_equal(f(A, B), np.transpose(np.einsum(A, [0, 1], B, [2, 3]), (1, 2, 3, 0)))\n\n    expr = PermuteDims(A, [1, 0])\n    f = lambdify((A,), expr, 'numpy')\n    assert np.array_equal(f(A), A.T)\n", "def test_identity_matrix():\n    if not np:\n        skip(\"NumPy not installed\")\n\n    printer = NumPyPrinter()\n    mat = Identity(3)\n    expected = 'numpy.eye(3)'\n    assert printer.doprint(mat) == expected\n", "def test_numpy_And():\n    if not np:\n        skip(\"NumPy not installed\")\n\n    e = (a > 1) & (b < 2)\n\n    f = lambdify((a, b), e, 'numpy')\n    a_ = np.array([1, 2, 3])\n    b_ = np.array([1, 2, 3])\n    assert np.array_equal(f(a_, b_), [False, True, True])\n\n    e = (a > 1) & (b < 2) & (c > 3)\n\n    f = lambdify((a, b, c), e, 'numpy')\n    a_ = np.array([1, 2, 3])\n    b_ = np.array([1, 2, 3])\n    c_ = np.array([2, 3, 4])\n    assert np.array_equal(f(a_, b_, c_), [False, True, True])\n", "def test_parity():\n    if not np:\n        skip(\"NumPy not installed\")\n\n    printer = NumPyPrinter()\n    expr = (1 + (-1)**x)\n    expected = \"numpy.select([numpy.mod(x, 2) == 0], [2], default=0)\"\n    assert printer.doprint(expr) == expected\n\n    expr = (1 - (-1)**x)\n    expected = \"numpy.select([numpy.mod(x, 2) != 0], [2], default=0)\"\n    assert printer.doprint(expr) == expected\n", "def test_adjoint_matrix():\n    if not np:\n        skip(\"NumPy not installed\")\n\n    from sympy import Matrix\n    M = Matrix([[1, 2], [3, 4]])\n\n    f = lambdify([], M.adjugate(), 'numpy')\n    assert np.array_equal(f(), np.array([[1-2j, -2-4j], [-3+4j, 1-2j]]))\n\n    f = lambdify([], M.adjugate().conjugate(), 'numpy')\n    assert np.array_equal(f(), np.array([[1+2j, 2-4j], [3+4j, 1+2j]]))\n\n    M = Matrix([[1+2j, 2-3j], [3+4j, 5-6j]])\n    f = lambdify([], M.adjugate(), 'numpy')\n    assert np.array_equal(f(), np.array([[1-2j, -2-3j], [-3-4j, 5+6j]]))\n"], "sample_624": ["def test_limit_lines() -> None:\n    s = \"a\\nb\\nc\\nd\\ne\"\n    assert formatting.limit_lines(s) == s\n    assert formatting.limit_lines(s, limit=2) == \"a\\n...\\nd\\ne\"\n    assert formatting.limit_lines(s, limit=1) == \"...\\nd\\ne\"\n    assert formatting.limit_lines(s, limit=0) == \"...\\nd\\ne\"\n", "def test_calc_max_rows_first_and_last() -> None:\n    max_rows = 100\n    assert formatting.calc_max_rows_first(max_rows) == 50 + (max_rows % 2)\n    assert formatting.calc_max_rows_last(max_rows) == max_rows // 2\n    max_rows = 99\n    assert formatting.calc_max_rows_first(max_rows) == 50\n    assert formatting.calc_max_rows_last(max_rows) == 49\n    max_rows = 1\n    assert formatting.calc_max_rows_first(max_rows) == 1\n    assert formatting.calc_max_rows_last(max_rows) == 0\n    max_rows = 2\n    assert formatting.calc_max_rows_first(max_rows) == 2\n    assert formatting.calc_max_rows_last(max_rows) == 0\n    max_rows = 3\n    assert formatting.calc_max_rows_first(max_rows) == 2\n    assert formatting.calc_max_rows_last(max_rows) == 1\n", "def test_unindexed_dims_repr() -> None:\n    da = xr.DataArray(np.arange(4), dims=[\"x\", \"y\"])\n    da.coords[\"x\"] = (\"x\", np.arange(4), {\"units\": \"m\"})\n\n    expected = \"Dimensions without coordinates: y\"\n    actual = formatting.unindexed_dims_repr(da.dims, da.coords)\n    assert actual == expected\n\n    expected = None\n    actual = formatting.unindexed_dims_repr(da.dims, da.coords, max_rows=0)\n    assert actual == expected\n", "def test_limit_lines() -> None:\n    text = \"This is a very long string\\nthat spans multiple lines.\\nIt will be truncated.\"\n    assert xr.core.formatting.limit_lines(text, limit=2) == \"This is a very long string\\n...\"\n\n    text = \"This is a very long string\\nthat spans multiple lines.\\nIt will be truncated.\\nAnd even more lines.\"\n    assert xr.core.formatting.limit_lines(text, limit=2) == \"This is a very long string\\n...\"\n    \n    text = \"This is a very long string\\nthat spans multiple lines.\\nIt will be truncated.\\nAnd even more lines.\\nAnd even more lines.\"\n    assert xr.core.formatting.limit_lines(text, limit=3) == \"This is a very long string\\nthat spans multiple lines.\\n...\"\n", "def test_calc_max_rows_first_and_last() -> None:\n    cases = [\n        (10, 6, 4),\n        (11, 6, 5),\n        (12, 7, 5),\n        (13, 7, 6),\n        (50, 26, 24),\n        (51, 26, 25),\n    ]\n    for max_rows, first_expected, last_expected in cases:\n        first = formatting.calc_max_rows_first(max_rows)\n        last = formatting.calc_max_rows_last(max_rows)\n        assert (first, last) == (first_expected, last_expected)\n\n    # Test that max_rows=1 case is properly handled\n    assert formatting.calc_max_rows_first(1) == 1\n    assert formatting.calc_max_rows_last(1) == 0\n"], "sample_725": ["def test_check_X_y_force_all_finite_valid():\n    X = np.arange(4).reshape(2, 2).astype(np.float)\n    X[0, 0] = np.nan\n    y = np.arange(2)\n    X_checked, y_checked = check_X_y(X, y, force_all_finite='allow-nan')\n    assert_allclose_dense_sparse(X_checked, X)\n    assert_array_equal(y_checked, y)\n\n    X = np.arange(4).reshape(2, 2).astype(np.float)\n    y = np.arange(2)\n    y[0] = np.inf\n    X_checked, y_checked = check_X_y(X, y, force_all_finite=True)\n    assert_allclose_dense_sparse(X_checked, X)\n    assert_array_equal(y_checked, y)\n", "def test_check_X_y_force_all_finite():\n    # Check that check_X_y raises a ValueError when X or y contains NaN/inf\n    # with force_all_finite=True\n    X = np.array([[1, 2], [3, np.nan]])\n    y = np.array([0, 1])\n    assert_raises_regex(ValueError, 'Input contains NaN, infinity',\n                        check_X_y, X, y, force_all_finite=True)\n\n    # Check that check_X_y does not raise an error when X contains NaN with\n    # force_all_finite='allow-nan'\n    X = np.array([[1, 2], [3, np.nan]])\n    y = np.array([0, 1])\n    X_checked, y_checked = check_X_y(X, y, force_all_finite='allow-nan')\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, y)\n\n    # Check that check_X_y raises a ValueError when X contains inf with\n    # force_all_finite='allow-nan'\n    X = np.array([[1, 2], [3, np.inf]])\n    y = np.array([0, 1])\n    assert_raises_regex(ValueError, 'Input contains infinity',\n                        check_X_y, X, y, force_all_finite='allow-nan')\n\n    # Check that check_X_y raises a ValueError when y contains NaN or inf\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, np.nan])\n    assert_raises_regex(ValueError, 'Input contains NaN, infinity',\n                        check_X_y, X, y, force_all_finite=True)\n    y = np.array([0, np.inf])\n    assert_raises_regex(ValueError, 'Input contains NaN, infinity',\n                        check_X_y, X, y, force_all_finite=True)\n", "def test_check_array_min_samples_and_features_on_nd_array():\n    # Test that check_array does not incorrectly check for the minimum number\n    # of features on nd-arrays when allow_nd=True\n    X = np.ones((10, 10, 10))\n    y = np.ones(10)\n    msg = \"10 sample(s) (shape=(10, 10, 10)) while a minimum of 20 is required\"\n    assert_raise_message(ValueError, msg, check_X_y, X, y,\n                         ensure_min_samples=20, allow_nd=True)\n\n    X = np.ones((10, 10, 0))\n    y = np.ones(10)\n    X_checked, y_checked = check_X_y(X, y, allow_nd=True)\n    assert_array_equal(X, X_checked)\n    assert_array_equal(y, y_checked)\n", "def test_check_X_y_valid():\n    # test that check_X_y raises a ValueError for invalid input\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2, 3, 4])\n\n    with pytest.raises(ValueError):\n        check_X_y(X, y)\n\n    # test that check_X_y returns correct results for valid input\n    y = np.array([1, 2, 3])\n    X_checked, y_checked = check_X_y(X, y)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, y)\n\n    # test that check_X_y works with sparse matrices\n    X_sparse = sp.csr_matrix(X)\n    X_checked, y_checked = check_X_y(X_sparse, y)\n    assert_array_equal(X_checked.toarray(), X)\n    assert_array_equal(y_checked, y)\n\n    # test that check_X_y works with lists\n    X_list = X.tolist()\n    X_checked, y_checked = check_X_y(X_list, y)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, y)\n\n    # test that check_X_y works with pandas DataFrame\n    try:\n        import pandas as pd\n        X_df = pd.DataFrame(X)\n        X_checked, y_checked = check_X_y(X_df, y)\n        assert_array_equal(X_checked, X)\n        assert_array_equal(y_checked, y)\n    except ImportError:\n        raise SkipTest(\"Pandas not found\")\n\n    # test that check_X_y works with non-numpy arrays\n    X_object = X.astype(object)\n    X_checked, y_checked = check_X_y(X_object, y)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, y)\n\n    # test that check_X_y works with dtype 'object'\n    X_object = X.astype(object)\n    X_checked, y_checked = check_X_y(X_object, y, dtype='object')\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, y)\n", "def test_check_X_y():\n    # Test function for check_X_y\n    X = np.ones((10, 5))\n    y = np.ones(10)\n\n    X_checked, y_checked = check_X_y(X, y)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, y)\n\n    y = np.ones((10, 1))\n    X_checked, y_checked = check_X_y(X, y)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, y)\n\n    y = np.ones((10, 2))\n    assert_raises_regex(\n        ValueError, \"bad input shape \\(10, 2\\)\", check_X_y, X, y)\n\n    y = np.ones(5)\n    assert_raises_regex(\n        ValueError, \"Found input variables with inconsistent numbers of \"\n                     \"samples: \\[10, 5\\]\", check_X_y, X, y)\n\n    y = \"not numeric\"\n    X = np.array([1, 2, 3])\n    assert_raises_regex(\n        ValueError, \"could not convert string to float: 'not numeric'\",\n        check_X_y, X, y)\n\n    X = sp.csr_matrix(np.ones((10, 5)))\n    y = np.ones(10)\n\n    X_checked, y_checked = check_X_y(X, y)\n    assert_array_equal(X_checked.toarray(), X.toarray())\n    assert_array_equal(y_checked, y)\n\n    y = np.ones(5)\n    assert_raises_regex(\n        ValueError, \"Found input variables with inconsistent numbers of \"\n                     \"samples: \\[10, 5\\]\", check_X_y, X, y)\n\n    # Test check_X_y with allow_nd=True\n    X = np.ones((10, 5, 3))\n    y = np.ones(10)\n    assert_raises_regex(\n        ValueError, \"Found array with dim 3. Estimator expected <= 2.\",\n        check_X_y, X, y)\n    X_checked, y_checked = check_X_y(X, y, allow_nd=True)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, y)\n\n    # Test check_X_y with allow_nd=False and estimator\n    X = np.ones((10, 5, 3))\n    y = np.ones(10)\n    assert_raises_regex(\n        ValueError, \"Found array with dim 3. Estimator expected <= 2.\",\n        check_X_y"], "sample_1053": ["def test_Rational_as_numer_denom():\n    r = Rational(3, 4)\n    assert r.as_numer_denom() == (Integer(3), Integer(4))\n    r = Rational(1, 2)\n    assert r.as_numer_denom() == (Integer(1), Integer(2))\n    r = Rational(0)\n    assert r.as_numer_denom() == (Integer(0), Integer(1))\n", "def test_igcd2_big():\n    a, b = int(fibonacci(1000001)), int(fibonacci(1000000))\n    assert igcd2(a, b) == 1\n", "def test_issue_14162():\n    from mpmath import mpf\n    assert comp(mpf(0.1, 20), Float(0.1, 20), 0.00000000001)\n    assert not comp(mpf(0.1, 20), Float(0.1, 20), 0.000000000001)\n", "def test_issue_13470():\n    # test that Float can handle being instantiated with a pickled mpf tuple\n    # that ends in 'L'\n    import pickle\n    mpf = Float((0, long(0x13333333333333), -52, 53))\n    s = pickle.dumps(mpf)\n    assert pickle.loads(s) == mpf\n", "def test_Float_numeric_properties():\n    # test when the precision of the Float is known\n    assert S.Half.is_positive\n    assert S.Half.is_nonnegative\n    assert S.Half.is_finite\n    assert not S.Half.is_nonpositive\n    assert not S.Half.is_negative\n    assert not S.Half.is_infinite\n    assert not S.Half.is_zero\n\n    assert S.NegativeOne.is_negative\n    assert S.NegativeOne.is_nonpositive\n    assert S.NegativeOne.is_finite\n    assert not S.NegativeOne.is_positive\n    assert not S.NegativeOne.is_nonnegative\n    assert not S.NegativeOne.is_infinite\n    assert not S.NegativeOne.is_zero\n\n    assert S.Zero.is_zero\n    assert S.Zero.is_finite\n    assert not S.Zero.is_nonzero\n    assert not S.Zero.is_nonpositive\n    assert not S.Zero.is_negative\n    assert not S.Zero.is_positive\n    assert not S.Zero.is_nonnegative\n    assert not S.Zero.is_infinite\n\n    # test when the precision of the Float is not known\n    assert not S.Pi.is_positive\n    assert not S.Pi.is_negative\n    assert not S.Pi.is_nonpositive\n    assert not S.Pi.is_nonnegative\n    assert S.Pi.is_finite\n    assert not S.Pi.is_infinite\n    assert not S.Pi.is_zero\n\n    # test that the number symbols match with Floats\n    assert not pi.is_positive\n    assert not pi.is_negative\n    assert not pi.is_nonpositive\n    assert not pi.is_nonnegative\n    assert pi.is_finite\n    assert not pi.is_infinite\n    assert not pi.is_zero\n\n    # test that the number symbols don't match with exact numbers\n    assert S.Pi.is_positive\n    assert not S.Pi.is_nonpositive\n    assert not S.Pi.is_negative\n    assert S.Pi.is_nonnegative\n    assert S.Pi.is_finite\n    assert not S.Pi.is_infinite\n    assert not S.Pi.is_zero\n"], "sample_219": ["    def test_row_range(self):\n        wf = RowRange(1, 3)\n        self.assertEqual(wf.frame_type, 'ROWS')\n        self.assertEqual(wf.window_frame_start_end(None, 1, 3), ('1 %s' % 'PRECEDING', '3 %s' % 'FOLLOWING'))\n", "    def setUpTestData(cls):\n        cls.n1 = Number.objects.create(integer=1, float=1.0)\n        cls.n2 = Number.objects.create(integer=2, float=2.0)\n        cls.n3 = Number.objects.create(integer=3, float=3.0)\n", "    def test_nested_outerref_with_aggregate(self):\n        inner = Company.objects.annotate(\n            ceo_salary=Subquery(\n                Employee.objects.filter(\n                    id=OuterRef(OuterRef('ceo__pk')),\n                ).values('salary')[:1],\n                output_field=IntegerField(),\n            ),\n        ).values('ceo_salary')\n        qs = Employee.objects.annotate(\n            salary_subquery=Subquery(inner, output_field=IntegerField()),\n        ).filter(salary_subquery__gt=10)\n        self.assertSequenceEqual(qs, [self.max])\n", "    def test_window_in_subquery(self):\n        Company.objects.annotate(\n            max_num_employees=Window(Max('num_employees')),\n            max_num_employees_2=Window(Max('num_employees')),\n        ).values(\n            'max_num_employees', 'max_num_employees_2', 'num_employees'\n        ).order_by('max_num_employees', 'max_num_employees_2')\n", "    def setUp(self):\n        self.time1 = Time.objects.create(time=datetime.time(12, 0, 0))\n        self.time2 = Time.objects.create(time=datetime.time(13, 0, 0))\n        self.time3 = Time.objects.create(time=datetime.time(14, 0, 0))\n"], "sample_939": ["def test_unparse_arguments():\n    source = \"\"\"", "def test_unparse_arguments(source, expected):\n    tree = ast.parse(source)\n    assert ast.unparse(tree.body[0].args, source) == expected\n", "def test_unparse_Arguments():\n    source = \"def func(a, b, c=1, *, d=2, e, **kwargs): pass\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args, source) == \"a, b, c=1, *, d=2, e, **kwargs\"\n", "def test_unparse_arguments(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0], source) == expected\n\n", "def test_unparse_stmts(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0], source) == expected\n"], "sample_702": ["def test_pytester_makeconftest(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        [pytest]\n        addopts = --strict\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == 0\n", "def test_pytester_run_result_repr_with_unknown_exit_code(pytester: Pytester) -> None:\n    outlines = [\"some\", \"normal\", \"output\"]\n    errlines = [\"some\", \"nasty\", \"errors\", \"happened\"]\n\n    # unknown exit code\n    r = pytester_mod.RunResult(99, outlines, errlines, duration=0.5)\n    assert repr(r).startswith(\"<RunResult ret=99 len(stdout.lines)=3 len(stderr.lines)=4 duration=\")\n", "def test_pytester_get_public_names() -> None:\n    values = [\"_private_name\", \"public_name\", \"__another_private_name\"]\n    result = get_public_names(values)\n    assert result == [\"public_name\"]\n", "def test_pytester_makedirs(pytester: Pytester) -> None:\n    pytester.mkpydir(\"testpackage\")\n    result = pytester.runpytest(str(pytester.path / \"testpackage\"))\n    assert result.ret == ExitCode.OK\n", "def test_pytester_copy_example(pytester: Pytester) -> None:\n    pytester_copy_example_dir = pytester.path / \"pytester_example_dir\"\n    pytester_copy_example_dir.mkdir()\n\n    test_example_file = pytester_copy_example_dir / \"test_example.py\"\n    test_example_file.write_text(\"print('Example file contents')\")\n\n    test_example_dir = pytester_copy_example_dir / \"test_example\"\n    test_example_dir.mkdir()\n\n    test_example_file_in_dir = test_example_dir / \"test_example.py\"\n    test_example_file_in_dir.write_text(\"print('Example file contents in dir')\")\n\n    pytester_copy_example_dir.mkdir(parents=True, exist_ok=True)\n    pytester._request.config.stash.addoption(\n        \"--pytester-example-dir\", default=pytester_copy_example_dir\n    )\n\n    copied_file = pytester.copy_example()\n    assert copied_file.read_text() == \"print('Example file contents')\"\n\n    copied_dir = pytester.copy_example(\"test_example\")\n    assert copied_dir.exists()\n    assert (copied_dir / \"test_example.py\").exists()\n    assert (copied_dir / \"test_example.py\").read_text() == \"print('Example file contents in dir')\"\n"], "sample_185": ["def test_get_format_with_setting_overridden(self):\n    with self.settings(FORMAT_MODULE_PATH='i18n.other.locale'):\n        with translation.override('fr', deactivate=True):\n            with self.settings(DECIMAL_SEPARATOR='|'):\n                self.assertEqual(get_format('DECIMAL_SEPARATOR'), '|')\n", "    def test_get_format_modules_with_format_module_path(self):\n        with translation.override('de'):\n            with self.settings(FORMAT_MODULE_PATH='tests.i18n.sampleproject.locale'):\n                modules = get_format_modules()\n                self.assertEqual(len(modules), 1)\n                self.assertEqual(modules[0].__name__, 'tests.i18n.sampleproject.locale.de.formats')\n", "def test_get_format_lazy(self):\n    \"\"\"\n    Tests that get_format_lazy is lazy and uses the current language.\n    \"\"\"\n    lang1 = 'en'\n    lang2 = 'de'\n    with translation.override(lang1):\n        fmt_lazy = get_format_lazy('DATE_FORMAT')\n        fmt_eager = get_format('DATE_FORMAT')\n        self.assertEqual(fmt_lazy, fmt_eager)\n    with translation.override(lang2):\n        fmt_lazy = get_format_lazy('DATE_FORMAT')\n        fmt_eager = get_format('DATE_FORMAT')\n        self.assertEqual(fmt_lazy, fmt_eager)\n        self.assertNotEqual(fmt_lazy, get_format('DATE_FORMAT', lang=lang1))\n", "    def test_get_format_lazy(self):\n        \"\"\"\n        Test the lazy get_format function to ensure it returns the format\n        for the current language without caching the result.\n        \"\"\"\n        with translation.override('en'):\n            self.assertEqual(get_format_lazy('DATE_FORMAT'), 'N j, Y')\n        with translation.override('fr'):\n            self.assertEqual(get_format_lazy('DATE_FORMAT'), 'j N Y')\n", "def test_localize_input_invalid_values(self):\n    \"\"\"\n    Tests that localize_input function raises ValueError for invalid input values\n    \"\"\"\n    invalid_values = [\n        (datetime.datetime.now(), ' invalid datetime format'),\n        (datetime.date.today(), ' invalid date format'),\n        (datetime.time(), ' invalid time format'),\n        ('hello', ' invalid datetime object')\n    ]\n\n    for value, error_message in invalid_values:\n        with self.assertRaisesMessage(ValueError, error_message):\n            localize_input(value)\n"], "sample_166": ["def test_get_random_string_length(self):\n    length = 20\n    random_string = get_random_string(length)\n    self.assertEqual(len(random_string), length)\n", "    def test_get_random_string_with_custom_length(self):\n        length = 20\n        random_string = get_random_string(length)\n        self.assertEqual(len(random_string), length)\n", "    def test_length(self):\n        for length in range(1, 100):\n            self.assertEqual(len(get_random_string(length)), length)\n", "    def test_get_random_string_length(self):\n        length = 20\n        random_string = get_random_string(length)\n        self.assertEqual(len(random_string), length)\n", "def test_salted_hmac_with_invalid_secret_type(self):\n    with self.assertRaises(TypeError):\n        salted_hmac('salt', 'value', secret=123)\n"], "sample_613": ["def test_da_groupby_map_func_kwargs() -> None:\n        return arg1 + arg2 + kwarg1\n\n    array = xr.DataArray([1, 1, 1], [(\"x\", [1, 2, 3])])\n    expected = xr.DataArray([3, 3, 3], [(\"x\", [1, 2, 3])])\n    actual = array.groupby(\"x\").map(func, args=(1,), kwarg1=1)\n    assert_identical(expected, actual)\n", "def test_groupby_quantile_interpolation_invalid_method() -> None:\n    array = xr.DataArray(data=[1, 2, 3, 4], coords={\"x\": [1, 1, 2, 2]}, dims=\"x\")\n\n    with pytest.raises(ValueError, match=r\"Invalid quantile interpolation method\"):\n        array.groupby(\"x\").quantile(0.5, method=\"unknown_method\")\n", "def test_groupby_dataset_apply_func_with_kwargs() -> None:\n        return arg1.mean(\"time\") + kwargs.get(\"arg2\", 0.0) + kwargs.get(\"arg3\", 0.0)\n\n    times = pd.date_range(\"2000\", freq=\"D\", periods=3)\n    ds = xr.Dataset({\"foo\": (\"time\", [1.0, 1.0, 1.0]), \"time\": times})\n    expected = xr.Dataset({\"foo\": (\"time\", [3.0, 3.0, 3.0]), \"time\": times})\n    actual = ds.resample(time=\"D\").map(func, arg2=1.0, arg3=1.0)\n    assert_identical(expected, actual)\n", "def test_groupby_first_and_last_with_skipna(obj, dim) -> None:\n    expected = obj.groupby(dim).first(skipna=False)\n    actual = obj.groupby(dim).reduce(np.nanfirst)\n    assert_identical(expected, actual)\n\n    expected = obj.groupby(dim).last(skipna=False)\n    actual = obj.groupby(dim).reduce(np.nanlast)\n    assert_identical(expected, actual)\n", "def test_groupby_map_stacked_array() -> None:\n    array = xr.DataArray(\n        np.arange(16).reshape(2, 2, 2, 2),\n        dims=(\"a\", \"b\", \"c\", \"d\"),\n        coords={\n            \"e\": (\"c\", [1, 2]),\n            \"f\": (\"d\", [3, 4]),\n        },\n    )\n    stacked = array.stack(z=(\"a\", \"b\"))\n    grouped = stacked.groupby(\"e\")\n    actual = grouped.map(lambda x: x.sum(\"z\"))\n    expected = grouped.sum(\"z\")\n    assert_identical(actual, expected)\n"], "sample_974": ["def test_ccode_MatrixElement_add():\n    A = MatrixSymbol('A', 3, 1)\n    B = MatrixSymbol('B', 3, 1)\n    C = MatrixSymbol('C', 3, 1)\n    assert ccode(A[1] + B[1] + C[2]) == \"A[1] + B[1] + C[2]\"\n    assert ccode(A[1] + B[1], assign_to=C[2]) == \"C[2] = A[1] + B[1];\"\n", "def test_ccode_MultiAssign():\n    x, y, z = symbols('x y z')\n    a = Assignment(x, y + z)\n    b = Assignment(y, x + z)\n    expr = [a, b]\n    assert ccode(expr, assign_to=[x, y]) == (\n            \"x = y + z;\\n\"\n            \"y = x + z;\")\n", "def test_ccode_MultiForLoops():\n    from sympy.tensor import IndexedBase, Idx\n    from sympy import symbols\n    n, m, o, p = symbols('n m o p', integer=True)\n    a = IndexedBase('a')\n    b = IndexedBase('b')\n    c = IndexedBase('c')\n    y = IndexedBase('y')\n    i = Idx('i', m)\n    j = Idx('j', n)\n    k = Idx('k', o)\n    l = Idx('l', p)\n\n    s1 = (\n        'for (int i=0; i<m; i++){\\n'\n        '   for (int j=0; j<n; j++){\\n'\n        '      y[i*n+j] = a[i]*b[j];\\n'\n        '   }\\n'\n        '}'\n    )\n    c = ccode(a[i]*b[j], assign_to=y[i, j])\n    assert c == s1\n\n    s2 = (\n        'for (int i=0; i<m; i++){\\n'\n        '   for (int j=0; j<n; j++){\\n'\n        '      for (int k=0; k<o; k++){\\n'\n        '         y[i*n*o + j*o + k] = a[i]*b[j]*c[k];\\n'\n        '      }\\n'\n        '   }\\n'\n        '}'\n    )\n    c = ccode(a[i]*b[j]*c[k], assign_to=y[i, j, k])\n    assert c == s2\n\n    s3 = (\n        'for (int i=0; i<m; i++){\\n'\n        '   for (int j=0; j<n; j++){\\n'\n        '      for (int k=0; k<o; k++){\\n'\n        '         for (int l=0; l<p; l++){\\n'\n        '            y[i*n*o*p + j*o*p + k*p + l] = a[i]*b[j]*c[k]*b[l];\\n'\n        '         }\\n'\n        '      }\\n'\n        '   }\\n'\n        '}'\n    )\n    c = ccode(a[i]*b[j]*c[k]*b[l], assign_to=y[i, j, k", "def test_ccode_Matrix_printing_multi_line():\n    # Test returning a Matrix\n    mat = Matrix([x*y + sin(z), Piecewise((2 + x, y>0), (y, True)), sin(z) + sin(z)])\n    A = MatrixSymbol('A', 3, 1)\n    expected = (\n        \"A[0] = x*y + sin(z);\\n\"\n        \"if (y > 0) {\\n\"\n        \"   A[1] = x + 2;\\n\"\n        \"}\\n\"\n        \"else {\\n\"\n        \"   A[1] = y;\\n\"\n        \"}\\n\"\n        \"A[2] = sin(z) + sin(z);\")\n    assert ccode(mat, A) == expected\n    # Test using MatrixElements in expressions\n    expr = Piecewise((2*A[2, 0], x > 0), (A[2, 0], True)) + sin(A[1, 0]) + A[0, 0]\n    assert ccode(expr) == (\n        \"((x > 0) ? (\\n\"\n        \"   2*A[2]\\n\"\n        \")\\n\"\n        \": (\\n\"\n        \"   A[2]\\n\"\n        \")) + sin(A[1]) + A[0]\")\n    # Test using MatrixElements in a Matrix\n    q = MatrixSymbol('q', 5, 1)\n    M = MatrixSymbol('M', 3, 3)\n    m = Matrix([[sin(q[1,0]), 0, cos(q[2,0])],\n        [q[1,0] + q[2,0], q[3, 0], 5],\n        [2*q[4, 0]/q[1,0], sqrt(q[0,0]) + 4, 0]])\n    expected = (\n        \"M[0] = sin(q[1]);\\n\"\n        \"M[1] = 0;\\n\"\n        \"M[2] = cos(q[2]);\\n\"\n        \"M[3] = q[1] + q[2];\\n\"\n        \"M[4] = q[3];\\n\"\n        \"M[5] = 5;\\n\"\n        \"M[6] = 2*q[4]/q[1];\\n\"\n       ", "def test_ccode_MatrixSymbol():\n    # Test that MatrixSymbols can be used as symbols in an expression\n    M, A = MatrixSymbol('M', 3, 3), MatrixSymbol('A', 3, 3)\n    expr = M[1, 2] + A[2, 1]\n    assert ccode(expr) == \"M[5] + A[5]\"\n    assert ccode(expr, assign_to=M[1, 2]) == \"M[5] = M[5] + A[5];\"\n"], "sample_743": ["def test_kneighbors_graph_sparse_distances():\n    # Test kneighbors_graph with sparse input and distance mode\n    X = np.array([[0, 1], [1.01, 1.], [2, 0]])\n    Xcsr = csr_matrix(X)\n\n    for n_neighbors in [1, 2, 3]:\n        A = neighbors.kneighbors_graph(Xcsr,\n                                       n_neighbors,\n                                       mode='distance').toarray()\n        assert_array_almost_equal(\n            A,\n            neighbors.kneighbors_graph(X,\n                                       n_neighbors,\n                                       mode='distance').toarray())\n", "def test_radius_neighbors_regressor_on_zero_radius():\n    # Test radius-based regression on zero radius\n    X = np.array([[1.0, 1.0], [1.0, 1.0], [2.0, 2.0], [2.5, 2.5]])\n    y = np.array([1.0, 1.5, 2.0, 0.0])\n    radius = 0.0\n    z = np.array([[1.1, 1.1], [2.0, 2.0]])\n\n    rnn_correct_labels = np.array([np.nan, 2.0])\n\n    for algorithm in ALGORITHMS:\n        # we don't test for weights=_weight_func since user will be expected\n        # to handle zero distances themselves in the function.\n        for weights in ['uniform', 'distance']:\n            rnn = neighbors.RadiusNeighborsRegressor(radius=radius,\n                                                     weights=weights,\n                                                     algorithm=algorithm)\n            rnn.fit(X, y)\n            pred = rnn.predict(z)\n            assert_array_almost_equal(pred, rnn_correct_labels)\n", "def test_unsupervised_neighbors_threading():\n    # Test unsupervised neighbors methods with threading\n    X = rng.rand(10, 5)\n\n    test = rng.rand(2, 5)\n\n    for algorithm in ALGORITHMS:\n        neigh = neighbors.NearestNeighbors(n_neighbors=5,\n                                           algorithm=algorithm, n_jobs=-1)\n        neigh.fit(X)\n\n        neigh.kneighbors(test)\n        neigh.radius_neighbors(test)\n        neigh.kneighbors_graph(test)\n        neigh.radius_neighbors_graph(test)\n", "def test_invalid_algorithm():\n    # Test that error is raised when an invalid algorithm is provided\n    with assert_raises(ValueError):\n        neighbors.NearestNeighbors(algorithm=\"invalid\")\n", "def test_neighbors_base_class():\n    # Test the base class for neighbors, which shouldn't actually do anything\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2, 3])\n    nbrs = neighbors.NeighborsBase(n_neighbors=3, algorithm='brute')\n\n    assert_raises(NotFittedError, nbrs.kneighbors)\n    assert_raises(NotFittedError, nbrs.radius_neighbors)\n\n    nbrs.fit(X, y)\n\n    assert_raises(NotImplementedError, nbrs.predict, X)\n"], "sample_751": ["def test_base_forest_oob_score():\n    # Test out-of-bag score for base forest.\n    X, y = datasets.make_classification(n_samples=200, n_features=5,\n                                        n_informative=3, n_redundant=0,\n                                        n_repeated=0, shuffle=False,\n                                        random_state=1)\n    base_forest = BaseForest(base_estimator=DecisionTreeClassifier(),\n                             n_estimators=10, oob_score=True, random_state=0)\n    base_forest.fit(X, y)\n    assert hasattr(base_forest, 'oob_score_')\n    assert hasattr(base_forest, 'oob_decision_function_')\n\n    # Test regression\n    X, y = datasets.make_regression(n_samples=200, n_features=5,\n                                    n_informative=3, n_targets=1,\n                                    random_state=1)\n    base_forest = BaseForest(base_estimator=DecisionTreeRegressor(),\n                             n_estimators=10, oob_score=True, random_state=0)\n    base_forest.fit(X, y)\n    assert hasattr(base_forest, 'oob_score_')\n    assert hasattr(base_forest, 'oob_prediction_')\n", "def test_forest_base_classes():\n    # Check base classes for forest of trees-based classifiers and regressors.\n\n    class CustomForestClassifier(ForestClassifier):\n            pass\n\n    class CustomForestRegressor(ForestRegressor):\n            pass\n\n    # ForestClassifier\n    clf = CustomForestClassifier(base_estimator=DecisionTreeClassifier())\n    assert_raises(NotImplementedError, clf.fit, X, y_class)\n\n    # ForestRegressor\n    reg = CustomForestRegressor(base_estimator=DecisionTreeRegressor())\n    assert_raises(NotImplementedError, reg.fit, X, y_regr)\n", "def test_forest_classifier():\n    # Test ForestClassifier\n    X, y = datasets.make_classification(n_samples=100, n_features=10, n_informative=3, n_redundant=0, n_repeated=0, shuffle=False, random_state=1)\n    clf = ForestClassifier()\n    clf.fit(X, y)\n    assert_array_equal(clf.predict(X), y)\n\n    # Check multi-output\n    X, y = datasets.make_multilabel_classification(n_samples=100, n_features=10, n_informative=3, n_labels=3, shuffle=False, random_state=1)\n    clf = ForestClassifier()\n    clf.fit(X, y)\n    assert_array_equal(clf.predict(X), y)\n\n    # Check sample weight\n    sample_weight = np.random.rand(len(y))\n    clf = ForestClassifier()\n    clf.fit(X, y, sample_weight=sample_weight)\n    assert_array_equal(clf.predict(X), y)\n", "def test_random_trees_embedding():\n    # Test random trees embedding.\n\n    # Check that the transform shape is as expected.\n    rte = RandomTreesEmbedding(n_estimators=5, random_state=0)\n    X_transformed = rte.fit_transform(X)\n    assert_equal(X_transformed.shape, (len(X), 5))\n\n    # Test sparse and dense output.\n    for sparse_output in [True, False]:\n        rte = RandomTreesEmbedding(n_estimators=5, random_state=0,\n                                   sparse_output=sparse_output)\n        X_transformed = rte.fit_transform(X)\n        assert_equal(X_transformed.shape, (len(X), 5))\n        if sparse_output:\n            assert_true(hasattr(X_transformed, 'toarray'))\n        else:\n            assert_false(hasattr(X_transformed, 'toarray'))\n", "def test_random_forest_class_weight():\n    # Test that class_weight parameter correctly balances the classes\n    # in the forest.\n    X, y = datasets.make_classification(n_samples=1000,\n                                        n_features=4,\n                                        n_informative=2,\n                                        n_redundant=0,\n                                        n_repeated=0,\n                                        n_classes=3,\n                                        n_clusters_per_class=1,\n                                        class_sep=1.5,\n                                        weights=[0.2, 0.5, 0.3],\n                                        shuffle=False,\n                                        random_state=42)\n\n    rf = RandomForestClassifier(random_state=42, n_estimators=10,\n                                class_weight='balanced')\n    rf.fit(X, y)\n\n    # The class weight should be inversely proportional to the class frequency\n    assert_array_almost_equal(rf.classes_, [0, 1, 2])\n    assert_array_almost_equal(rf.class_weight_, [1.5, 1, 1.66666667])\n\n    # Test that sample_weight is multiplied with class_weight\n    sample_weight = np.random.RandomState(42).rand(len(X))\n    rf.fit(X, y, sample_weight)\n    assert_array_almost_equal(rf.class_weight_, [1.5, 1, 1.66666667])\n\n    # Test that class_weight=None returns equal weights\n    rf = RandomForestClassifier(random_state=42, n_estimators=10,\n                                class_weight=None)\n    rf.fit(X, y)\n    assert_array_almost_equal(rf.class_weight_, [1, 1, 1])\n\n    # Test that class_weight=auto correctly handles multi-output\n    X, y = datasets.make_multilabel_classification(n_samples=10,\n                                                   n_features=20,\n                                                   n_classes=3,\n                                                   n_labels=3,\n                                                   allow_unlabeled=True,\n                                                   random_state=0)\n    rf = RandomForestClassifier(random_state=42, n_estimators=10,\n                                class_weight='balanced')\n    rf.fit(X, y)\n    assert_array_almost_equal(rf.class_weight_, [1, 1, 1])\n\n    # Test that class_weight=auto raises an error for multi-output with\n    # different number of classes\n    X, y = datasets.make_multilabel_classification(n_samples=10,\n                                                   n_features=20,\n                                                   n_classes=[2, 3, 4],\n                                                   n_labels=3,\n                                                   allow_unlabeled=True,\n                                                   random_state=0)\n    rf"], "sample_885": ["def test_interval_invalid_type():\n    \"\"\"Check that an informative error is raised when an unknown type is passed\"\"\"\n    with pytest.raises(ValueError, match=\"type must be either numbers.Integral, numbers.Real or 'real_not_int'.\"):\n        Interval(\"unknown\", 0, 1, closed=\"both\")\n", "def test_validate_params_invalid_parameter_constraints():\n    \"\"\"Check that validate_params raises an informative error when the\n    parameter_constraints dict is not valid.\n    \"\"\"\n    with pytest.raises(ValueError, match=\"Unknown constraint type:\"):\n        @validate_params({\"param\": [\"invalid_constraint\"]})\n            pass\n", "def test_generate_invalid_param_val_multiple_constraints(constraints):\n    \"\"\"Check that the value generated does not satisfy any of the constraints\"\"\"\n    bad_value = generate_invalid_param_val(constraints[0], constraints=constraints)\n    for constraint in constraints:\n        assert not constraint.is_satisfied_by(bad_value)\n", "def test_validate_params_warning_on_redefined_constraints():\n    \"\"\"Check that a warning is raised when a parameter constraint is redefined.\"\"\"\n    with pytest.warns(UserWarning, match=\"Parameter constraint for 'a' was redefined\"):\n        @validate_params({\"a\": [Real]})\n        @validate_params({\"a\": [Integral]})\n            pass\n\n    # no warning when constraints are the same\n    with warnings.catch_warnings(record=True) as w:\n        @validate_params({\"a\": [Real]})\n        @validate_params({\"a\": [Real]})\n            pass\n        assert len(w) == 0\n\n    # no warning when no constraint is redefined\n    with warnings.catch_warnings(record=True) as w:\n        @validate_params({\"a\": [Real]})\n        @validate_params({\"b\": [Integral]})\n            pass\n        assert len(w) == 0\n", "def test_validate_params_invalid_function_signature():\n    \"\"\"Check that validate_params raises an informative error for an invalid function signature\"\"\"\n    with pytest.raises(ValueError, match=\"signature function must be a callable\"):\n        validate_params({\"param\": [int]})(lambda: 42)\n\n        pass\n\n    invalid_signature.__signature__ = \"not a signature\"\n\n    with pytest.raises(ValueError, match=\"signature function must be a callable\"):\n        validate_params({\"param\": [int]})(invalid_signature)\n"], "sample_167": ["def test_naturaltime_with_non_integer_delta(self):\n    class MockDateTime(datetime.datetime):\n        @classmethod\n            return now\n\n    orig_humanize_datetime, humanize.datetime = humanize.datetime, MockDateTime\n    try:\n        test_list = [\n            now - datetime.timedelta(seconds=0.5),\n            now - datetime.timedelta(seconds=1.5),\n            now + datetime.timedelta(seconds=0.5),\n            now + datetime.timedelta(seconds=1.5),\n        ]\n        result_list = [\n            'now',\n            'a second ago',\n            'now',\n            'a second from now',\n        ]\n        with translation.override('en'):\n            self.humanize_tester(test_list, result_list, 'naturaltime')\n    finally:\n        humanize.datetime = orig_humanize_datetime\n", "def test_intword_i18n_with_floating_point_numbers(self):\n    # Positive floating point numbers.\n    test_list_positive = (\n        '100.1', '1000000.1', '1200000.1', '1290000.1', '1000000000.1', '2000000000.1',\n        '6000000000000.1', '1300000000000000.1', '3500000000000000000000.1',\n        '8100000000000000000000000000000000.1', ('1' + '0' * 100 + '.1'),\n        ('1' + '0' * 104 + '.1'),\n    )\n    result_list_positive = (\n        '100.1', '1.0 million', '1.2 million', '1.3 million', '1.0 billion',\n        '2.0 billion', '6.0 trillion', '1.3 quadrillion', '3.5 sextillion',\n        '8.1 decillion', '1.0 googol', ('1' + '0' * 104 + '.1'),\n    )\n    # Negative floating point numbers.\n    test_list_negative = ('-' + test for test in test_list_positive)\n    result_list_negative = ('-' + result for result in result_list_positive)\n    with self.settings(USE_L10N=True):\n        with translation.override('de'):\n            self.humanize_tester(\n                (*test_list_positive, *test_list_negative, None),\n                (*result_list_positive, *result_list_negative, None),\n                'intword',\n            )\n", "def test_naturaltime_with_timezones(self):\n    \"\"\"Verify that naturaltime handles timezones correctly.\"\"\"\n    test_list = [\n        # Test with a datetime in a timezone that is ahead of UTC\n        datetime.datetime(2022, 1, 1, 12, 0, tzinfo=datetime.timezone.utc),\n        # Test with a datetime in a timezone that is behind UTC\n        datetime.datetime(2022, 1, 1, 12, 0, tzinfo=datetime.timezone(datetime.timedelta(hours=-5))),\n    ]\n    result_list = [\n        '12 hours ago',\n        '17 hours ago',\n    ]\n\n    orig_humanize_datetime, humanize.datetime = humanize.datetime, MockDateTime\n    try:\n        with translation.override('en'), self.settings(USE_TZ=True):\n            self.humanize_tester(test_list, result_list, 'naturaltime')\n    finally:\n        humanize.datetime = orig_humanize_datetime\n", "def test_naturaltime_with_timezone_edge_case(self):\n    \"\"\"\n    Regression test for a timezone edge case in naturaltime.\n    \"\"\"\n    class NaiveTZ(datetime.tzinfo):\n            return datetime.timedelta(0)\n\n    test_list = [\n        datetime.datetime(2022, 1, 1, 12, 0, tzinfo=NaiveTZ()),\n        datetime.datetime(2022, 1, 1, 12, 0, tzinfo=datetime.timezone.utc),\n        datetime.datetime(2022, 1, 1, 12, 0, tzinfo=datetime.timezone(datetime.timedelta(hours=1))),\n        datetime.datetime(2022, 1, 1, 12, 0, tzinfo=datetime.timezone(datetime.timedelta(hours=-1))),\n    ]\n\n    result_list = ['now', 'now', 'now', 'now']\n\n    orig_humanize_datetime, humanize.datetime = humanize.datetime, MockDateTime\n    try:\n        with translation.override('en'):\n            self.humanize_tester(test_list, result_list, 'naturaltime')\n    finally:\n        humanize.datetime = orig_humanize_datetime\n", "def test_naturaltime_with_timezones(self):\n    \"\"\"\n    Test the naturaltime filter with different timezones.\n    \"\"\"\n    # Create a datetime object in UTC\n    utc_datetime = datetime.datetime(2022, 1, 1, 12, 0, 0, tzinfo=utc)\n    # Create a datetime object in a timezone with an offset\n    offset_datetime = utc_datetime.astimezone(get_fixed_timezone(360))\n\n    test_list = [\n        utc_datetime,\n        offset_datetime,\n        offset_datetime + datetime.timedelta(days=1),\n        offset_datetime + datetime.timedelta(days=30),\n        offset_datetime + datetime.timedelta(days=60),\n        offset_datetime + datetime.timedelta(days=500),\n    ]\n\n    result_list = [\n        'now',\n        'now',\n        '1\\xa0day from now',\n        '1\\xa0month from now',\n        '2\\xa0months from now',\n        '1\\xa0year, 4\\xa0months from now',\n    ]\n\n    with translation.override('en'):\n        self.humanize_tester(test_list, result_list, 'naturaltime')\n\n    # Repeat the test with a different timezone\n    offset_datetime = utc_datetime.astimezone(get_fixed_timezone(-360))\n    result_list = [\n        'now',\n        'now',\n        '1\\xa0day from now',\n        '1\\xa0month from now',\n        '2\\xa0months from now',\n        '1\\xa0year, 4\\xa0months from now',\n    ]\n    with translation.override('en'):\n        self.humanize_tester(test_list, result_list, 'naturaltime')\n"], "sample_35": ["def test_isinstancemethod():\n    class TestClass:\n            pass\n\n            pass\n\n        @classmethod\n            pass\n\n        @staticmethod\n            pass\n\n    obj = TestClass()\n\n    assert isinstancemethod(TestClass, TestClass.instance_method)\n    assert not isinstancemethod(TestClass, TestClass.class_method)\n    assert not isinstancemethod(TestClass, TestClass.static_method)\n    assert not isinstancemethod(TestClass, TestClass.__init__)\n\n    with pytest.raises(AttributeError):\n        isinstancemethod(TestClass, \"non_existent_method\")\n\n    with pytest.raises(TypeError):\n        isinstancemethod(\"not a class\", TestClass.instance_method)\n", "def test_isinstancemethod():\n    class MyClass:\n            pass\n\n        @classmethod\n            pass\n\n        @staticmethod\n            pass\n\n    obj = MyClass()\n\n    assert isinstancemethod(MyClass, MyClass.an_instancemethod)\n    assert isinstancemethod(MyClass, obj.an_instancemethod)\n    assert not isinstancemethod(MyClass, MyClass.a_classmethod)\n    assert not isinstancemethod(MyClass, MyClass.a_staticmethod)\n\n    class MetaClass(type):\n            pass\n\n    class MySubClass(metaclass=MetaClass):\n            pass\n\n    assert isinstancemethod(MySubClass, MySubClass.an_instancemethod)\n    assert not isinstancemethod(MySubClass, MySubClass.a_classmethod)\n\n    with pytest.raises(AttributeError):\n        isinstancemethod(MyClass, object())\n", "def test_resolve_name():\n    # Test resolving a simple module\n    import math\n    assert introspection.resolve_name('math') is math\n\n    # Test resolving an object within a module\n    assert introspection.resolve_name('math', 'sin') is math.sin\n\n    # Test resolving a module that does not exist\n    with pytest.raises(ImportError):\n        introspection.resolve_name('non_existent_module')\n\n    # Test resolving an object that does not exist within a module\n    with pytest.raises(ImportError):\n        introspection.resolve_name('math', 'non_existent_function')\n\n    # Test resolving a module with multiple parts\n    import astropy.utils.introspection\n    assert introspection.resolve_name('astropy', 'utils', 'introspection') is astropy.utils.introspection\n\n    # Test resolving an object within a module with multiple parts\n    assert introspection.resolve_name('astropy', 'utils', 'introspection', 'resolve_name') is astropy.utils.introspection.resolve_name\n", "def test_resolve_name():\n    \"\"\"\n    Tests that the `resolve_name` function works correctly.\n\n    This includes testing that it can resolve both modules and objects within\n    modules, as well as testing that it can handle multiple arguments to the\n    name parameter.\n    \"\"\"\n    import astropy.utils.introspection as iu\n    assert iu.resolve_name('astropy.utils.introspection.resolve_name') == iu.resolve_name\n    assert iu.resolve_name('astropy', 'utils', 'introspection', 'resolve_name') == iu.resolve_name\n    assert iu.resolve_name('astropy.utils.introspection') == iu\n\n    with pytest.raises(ImportError):\n        iu.resolve_name('nonexistentmodule.nonexistentobject')\n", "def test_isinstancemethod():\n    class TestClass:\n            pass\n\n            pass\n\n        @staticmethod\n            pass\n\n        @classmethod\n            pass\n\n    instance = TestClass()\n\n    assert isinstancemethod(TestClass, instance.instance_method.__func__)\n    assert not isinstancemethod(TestClass, TestClass.static_method)\n    assert not isinstancemethod(TestClass, TestClass.class_method)\n    assert not isinstancemethod(TestClass, 42)  # not a method\n\n    # Test that the function doesn't crash when the object is not a member of the class\n    assert not isinstancemethod(TestClass, TestClass.static_method.__func__)\n"], "sample_583": ["def test_explicit_indexing_adapter():\n    array = np.arange(10)\n    indexing_method = lambda key: array[key]\n\n    # test with basic indexing\n    key = indexing.BasicIndexer((3,))\n    result = indexing.explicit_indexing_adapter(\n        key, array.shape, indexing.IndexingSupport.BASIC, indexing_method)\n    assert_array_equal(result, array[key.tuple])\n\n    # test with outer indexing\n    key = indexing.OuterIndexer((np.array([0, 3]),))\n    result = indexing.explicit_indexing_adapter(\n        key, array.shape, indexing.IndexingSupport.OUTER, indexing_method)\n    assert_array_equal(result, array[key.tuple])\n\n    # test with vectorized indexing\n    key = indexing.VectorizedIndexer((np.array([0, 3]),))\n    result = indexing.explicit_indexing_adapter(\n        key, array.shape, indexing.IndexingSupport.VECTORIZED, indexing_method)\n    assert_array_equal(result, array[key.tuple])\n\n    # test with invalid indexing support\n    with raises_regex(ValueError, 'unexpected key type'):\n        indexing.explicit_indexing_adapter(\n            key, array.shape, indexing.IndexingSupport.OUTER, indexing_method)\n", "def test_explicit_indexing_adapter():\n    x = np.arange(10)\n    wrapped = indexing.ImplicitToExplicitIndexingAdapter(x)\n    expected = x[2:5]\n\n        return x[key]\n\n    # Test vectorized indexing\n    actual = indexing.explicit_indexing_adapter(\n        indexing.VectorizedIndexer((np.array([2, 3, 4]),)), x.shape,\n        indexing.IndexingSupport.VECTORIZED, raw_indexing_method)\n    np.testing.assert_array_equal(expected, actual)\n\n    # Test outer indexing\n    actual = indexing.explicit_indexing_adapter(\n        indexing.OuterIndexer((np.array([2, 3, 4]),)), x.shape,\n        indexing.IndexingSupport.OUTER, raw_indexing_method)\n    np.testing.assert_array_equal(expected, actual)\n\n    # Test basic indexing\n    actual = indexing.explicit_indexing_adapter(\n        indexing.BasicIndexer((slice(2, 5),)), x.shape,\n        indexing.IndexingSupport.BASIC, raw_indexing_method)\n    np.testing.assert_array_equal(expected, actual)\n", "def test_explicit_indexing_adapter():\n    shape = (10, 5, 8)\n    data = np.random.randn(*shape)\n    key = indexing.OuterIndexer((1, slice(2, 4), np.array([2, 3, 4])))\n    actual = indexing.explicit_indexing_adapter(\n        key, shape, indexing.IndexingSupport.BASIC, data.__getitem__)\n    np.testing.assert_array_equal(actual, data[1, 2:4, [2, 3, 4]])\n\n    key = indexing.OuterIndexer((1, slice(2, 4), np.array([2, 3, 4])))\n    actual = indexing.explicit_indexing_adapter(\n        key, shape, indexing.IndexingSupport.OUTER, data.__getitem__)\n    np.testing.assert_array_equal(actual, data[1, 2:4, [2, 3, 4]])\n\n    key = indexing.OuterIndexer((1, slice(2, 4), np.array([2, 3, 4])))\n    actual = indexing.explicit_indexing_adapter(\n        key, shape, indexing.IndexingSupport.VECTORIZED, data.__getitem__)\n    np.testing.assert_array_equal(actual, data[1, 2:4, [2, 3, 4]])\n\n    key = indexing.OuterIndexer((1, np.array([2, 3, 4]), np.array([2, 3, 4])))\n    actual = indexing.explicit_indexing_adapter(\n        key, shape, indexing.IndexingSupport.BASIC, data.__getitem__)\n    np.testing.assert_array_equal(actual, data[1, [2, 3, 4], [2, 3, 4]])\n\n    key = indexing.OuterIndexer((1, np.array([2, 3, 4]), np.array([2, 3, 4])))\n    actual = indexing.explicit_indexing_adapter(\n        key, shape, indexing.IndexingSupport.OUTER, data.__getitem__)\n    np.testing.assert_array_equal(actual, data[1, [2, 3, 4], [2, 3, 4]])\n\n    key = indexing.OuterIndexer((1, np.array([2, 3, 4]), np.array([2, 3, 4])))\n    actual = indexing.explicit_indexing_adapter(\n        key, shape, indexing.IndexingSupport.VECTORIZED,", "def test_explicit_indexing_adapter():\n    array = np.arange(10)\n    np_indexing_adapter = indexing.NumpyIndexingAdapter(array)\n    np_indexing_method = np_indexing_adapter.__getitem__\n\n    # test vectorized indexing\n    indexer = indexing.VectorizedIndexer((np.array([0, 1, 2]),))\n    result = indexing.explicit_indexing_adapter(\n        indexer, array.shape, indexing.IndexingSupport.VECTORIZED,\n        np_indexing_method)\n    assert_array_equal(result, array[[0, 1, 2]])\n\n    # test vectorized indexing with backend that only supports basic indexing\n    result = indexing.explicit_indexing_adapter(\n        indexer, array.shape, indexing.IndexingSupport.BASIC,\n        np_indexing_method)\n    assert_array_equal(result, array[[0, 1, 2]])\n\n    # test vectorized indexing with backend that supports outer indexing\n    result = indexing.explicit_indexing_adapter(\n        indexer, array.shape, indexing.IndexingSupport.OUTER,\n        np_indexing_method)\n    assert_array_equal(result, array[[0, 1, 2]])\n\n    # test vectorized indexing with backend that supports outer indexing\n    result = indexing.explicit_indexing_adapter(\n        indexer, array.shape, indexing.IndexingSupport.OUTER_1VECTOR,\n        np_indexing_method)\n    assert_array_equal(result, array[[0, 1, 2]])\n\n    # test outer indexing\n    indexer = indexing.OuterIndexer((np.array([0, 1, 2]),))\n    result = indexing.explicit_indexing_adapter(\n        indexer, array.shape, indexing.IndexingSupport.OUTER,\n        np_indexing_method)\n    assert_array_equal(result, array[[0, 1, 2]])\n\n    # test basic indexing\n    indexer = indexing.BasicIndexer((2,))\n    result = indexing.explicit_indexing_adapter(\n        indexer, array.shape, indexing.IndexingSupport.BASIC,\n        np_indexing_method)\n    assert_array_equal(result, array[2])\n", "def test_posify_mask_indexer(indexer):\n    posified = indexing.posify_mask_indexer(indexer)\n    assert_array_equal(posified.tuple,\n                       (indexing._posify_mask_subindexer(k) if isinstance(k, np.ndarray) else k\n                        for k in indexer.tuple))\n"], "sample_925": ["def test_MockFinder():\n    modnames = ['sphinx.unknown', 'sphinx.unknown2']\n    finder = MockFinder(modnames)\n    assert finder.modnames == modnames\n    assert finder.mocked_modules == []\n\n    spec = finder.find_spec('sphinx.unknown', None)\n    assert spec.name == 'sphinx.unknown'\n    assert spec.loader == finder.loader\n    assert finder.mocked_modules == ['sphinx.unknown']\n\n    spec = finder.find_spec('sphinx.unknown.submodule', None)\n    assert spec.name == 'sphinx.unknown.submodule'\n    assert spec.loader == finder.loader\n    assert finder.mocked_modules == ['sphinx.unknown', 'sphinx.unknown.submodule']\n\n    spec = finder.find_spec('sphinx.known', None)\n    assert spec is None\n\n    finder.invalidate_caches()\n    assert 'sphinx.unknown' not in sys.modules\n    assert 'sphinx.unknown.submodule' not in sys.modules\n", "def test_MockFinder():\n    finder = MockFinder(['sphinx.unknown'])\n    assert finder.find_spec('sphinx.unknown', None) is not None\n    assert finder.find_spec('sphinx.unknown.submodule', None) is not None\n    assert finder.find_spec('sphinx.known', None) is None\n\n    with mock(['sphinx.unknown']):\n        assert 'sphinx.unknown' in sys.modules\n        assert finder.mocked_modules == ['sphinx.unknown']\n        finder.invalidate_caches()\n        assert 'sphinx.unknown' not in sys.modules\n", "def test_MockObject_repr():\n    mock = _MockObject()\n    assert repr(mock) == '_MockObject'\n", "def test_MockFinder():\n    modnames = ['sphinx.unknown', 'sphinx.another']\n    finder = MockFinder(modnames)\n    assert finder.modnames == modnames\n    assert finder.mocked_modules == []\n\n    spec = finder.find_spec('sphinx.unknown', None)\n    assert spec is not None\n    assert spec.name == 'sphinx.unknown'\n    assert finder.mocked_modules == ['sphinx.unknown']\n\n    spec = finder.find_spec('sphinx.unknown.submodule', None)\n    assert spec is not None\n    assert spec.name == 'sphinx.unknown.submodule'\n    assert finder.mocked_modules == ['sphinx.unknown', 'sphinx.unknown.submodule']\n\n    spec = finder.find_spec('sphinx.known', None)\n    assert spec is None\n\n    finder.invalidate_caches()\n    assert sys.modules.get('sphinx.unknown') is None\n    assert sys.modules.get('sphinx.unknown.submodule') is None\n", "def test_MockObject___new__with_superclass():\n    class TestClass(_MockObject):\n        pass\n\n    obj = TestClass('test', (object,), {})\n    assert isinstance(obj, TestClass)\n    assert isinstance(obj, _MockObject)\n    assert obj.__qualname__ == 'TestClass'\n\n    class TestClassWithDisplayname(_MockObject):\n        __display_name__ = 'TestClassName'\n\n    obj = TestClassWithDisplayname('test', (object,), {})\n    assert obj.__display_name__ == 'TestClassName'\n\n"], "sample_746": ["def test_log_loss_multiclass():\n    # multiclass case; adapted from http://bit.ly/RJJHWA\n    y_true = [1, 0, 2]\n    y_pred = [[0.2, 0.7, 0.1], [0.6, 0.2, 0.2], [0.6, 0.1, 0.3]]\n    loss = log_loss(y_true, y_pred, normalize=True)\n    assert_almost_equal(loss, 0.6904911)\n\n    # check that we got all the shapes and axes right\n    # by doubling the length of y_true and y_pred\n    y_true *= 2\n    y_pred *= 2\n    loss = log_loss(y_true, y_pred, normalize=False)\n    assert_almost_equal(loss, 0.6904911 * 6, decimal=6)\n\n    # check eps and handling of absolute zero and one probabilities\n    y_pred = np.asarray(y_pred) > .5\n    loss = log_loss(y_true, y_pred, normalize=True, eps=.1)\n    assert_almost_equal(loss, log_loss(y_true, np.clip(y_pred, .1, .9)))\n\n    # raise error if number of classes are not equal.\n    y_true = [1, 0, 2]\n    y_pred = [[0.2, 0.7], [0.6, 0.5], [0.4, 0.1]]\n    assert_raises(ValueError, log_loss, y_true, y_pred)\n\n    # case when y_true is a string array object\n    y_true = [\"ham\", \"spam\", \"spam\", \"ham\"]\n    y_pred = [[0.2, 0.7], [0.6, 0.5], [0.4, 0.1], [0.7, 0.2]]\n    loss = log_loss(y_true, y_pred)\n    assert_almost_equal(loss, 1.0383217, decimal=6)\n\n    # test labels option\n\n    y_true = [2, 2]\n    y_pred = [[0.2, 0.7], [0.6, 0.5]]\n    y_score = np.array([[0.1, 0.9], [0.1, 0.9]])\n    error_str = ('y_true contains only one label (2). Please provide '\n", "def test_jaccard_similarity_score_multilabel():\n    # Test Jaccard similarity score for multilabel classification task\n    y_true = np.array([[0, 1, 1], [1, 0, 1]])\n    y_pred = np.array([[0, 0, 1], [1, 0, 1]])\n\n    # size(y1 \\inter y2) = [1, 2]\n    # size(y1 \\union y2) = [2, 2]\n\n    assert_equal(jaccard_similarity_score(y_true, y_true), 1)\n    assert_equal(jaccard_similarity_score(y_true, y_pred), 0.75)\n    assert_equal(jaccard_similarity_score(y_pred, y_true), 0.75)\n    assert_equal(jaccard_similarity_score(y_pred, y_pred), 1)\n    assert_equal(jaccard_similarity_score(y_pred, np.logical_not(y_pred)), 0)\n    assert_equal(jaccard_similarity_score(y_true, np.logical_not(y_true)), 0)\n    assert_equal(jaccard_similarity_score(y_true, np.zeros(y_true.shape)), 0)\n    assert_equal(jaccard_similarity_score(y_pred, np.zeros(y_true.shape)), 0)\n", "def test_log_loss_warning():\n    y_true = np.array([0, 1])\n    y_pred = np.array([[.1, .9], [.2, .8]])\n    y_pred2 = np.array([[.1, .8], [.2, .9]])\n    assert_warns_message(UserWarning,\n                        \"y_true contains only one label (0). Please provide \"\n                        \"the true labels explicitly through the labels argument.\",\n                        log_loss, y_true, y_pred)\n    assert_no_warnings(log_loss, y_true, y_pred2)\n", "def test_hinge_loss_single_class():\n    # Test that hinge loss will fail for single class\n    y_true = np.array([1, 1, 1, 1])\n    pred_decision = np.array([0.5, 0.2, 0.7, 0.8])\n    assert_raise_message(ValueError,\n                        \"Found input variables with inconsistent numbers \"\n                        \"of samples: [4, 1]\",\n                        hinge_loss, y_true, pred_decision)\n", "def test_brier_score_loss_multiclass():\n    # Test brier_score_loss function for multiclass problem\n    y_true = [0, 1, 1, 0, 1, 1]\n    y_pred = [[0.1, 0.8, 0.1], [0.6, 0.5, 0.1], [0.4, 0.1, 0.5], [0.7, 0.2, 0.1], [0.05, 0.95, 0], [0.3, 0.7, 0]]\n    true_score = np.mean(np.sum((y_true - y_pred) ** 2, axis=1))\n\n    assert_almost_equal(brier_score_loss(y_true, y_true), 0.0)\n    assert_almost_equal(brier_score_loss(y_true, y_pred), true_score)\n\n    y_pred_bad_shape = [0.1, 0.8, 0.1, 0.6, 0.5, 0.1, 0.4, 0.1, 0.5, 0.7, 0.2, 0.1, 0.05, 0.95, 0, 0.3, 0.7, 0]\n    assert_raises(ValueError, brier_score_loss, y_true, y_pred_bad_shape)\n"], "sample_1014": ["def test_free_symbols():\n    from sympy.abc import x, y, z\n    md = ImmutableDenseNDimArray([x, y, z])\n    assert md.free_symbols == {x, y, z}\n\n    md = ImmutableDenseNDimArray([[x, y], [z, x*y]])\n    assert md.free_symbols == {x, y, z}\n\n    sd = ImmutableSparseNDimArray([x, y, z])\n    assert sd.free_symbols == {x, y, z}\n\n    sd = ImmutableSparseNDimArray([[x, y], [z, x*y]])\n    assert sd.free_symbols == {x, y, z}\n\n    md = ImmutableDenseNDimArray(range(10))\n    assert md.free_symbols == set()\n\n    sd = ImmutableSparseNDimArray(range(10))\n    assert sd.free_symbols == set()\n", "def test_mutable_ndim_array():\n    from sympy.abc import x, y\n\n    arr = MutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n    assert arr.shape == (2, 2)\n    assert arr.rank() == 2\n\n    arr[0, 0] = x\n    assert arr[0, 0] == x\n    assert arr.tomatrix() == Matrix([[x, 2], [3, 4]])\n\n    arr[1, 1] = y\n    assert arr[1, 1] == y\n    assert arr.tomatrix() == Matrix([[x, 2], [3, y]])\n\n    arr1 = arr.as_immutable()\n    assert isinstance(arr1, ImmutableDenseNDimArray)\n    assert arr1.tomatrix() == Matrix([[x, 2], [3, y]])\n\n    arr2 = arr1.as_mutable()\n    assert isinstance(arr2, MutableDenseNDimArray)\n    assert arr2.tomatrix() == Matrix([[x, 2], [3, y]])\n\n    arr3 = arr1 + arr2\n    assert arr3.tomatrix() == Matrix([[2*x, 4], [6, 2*y]])\n\n    arr4 = arr3 / 2\n    assert arr4.tomatrix() == Matrix([[x, 2], [3, y]])\n\n    arr5 = -arr4\n    assert arr5.tomatrix() == Matrix([[-x, -2], [-3, -y]])\n", "def test_mutable_ndim_array():\n    # Test MutableDenseNDimArray\n    m = MutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n    assert len(m) == 4\n    assert m.shape == (2, 2)\n    assert m.rank() == 2\n    assert m.tolist() == [[1, 2], [3, 4]]\n\n    # Test setitem\n    m[0, 0] = 10\n    assert m.tolist() == [[10, 2], [3, 4]]\n\n    # Test as_immutable\n    im = m.as_immutable()\n    assert isinstance(im, ImmutableDenseNDimArray)\n    assert im.tolist() == [[10, 2], [3, 4]]\n\n    # Test as_mutable\n    mm = im.as_mutable()\n    assert isinstance(mm, MutableDenseNDimArray)\n    assert mm.tolist() == [[10, 2], [3, 4]]\n\n    # Test zeros\n    z = MutableDenseNDimArray.zeros(2, 2)\n    assert z.tolist() == [[0, 0], [0, 0]]\n\n    # Test tomatrix\n    matrix = m.tomatrix()\n    assert isinstance(matrix, Matrix)\n    assert matrix.tolist() == [[10, 2], [3, 4]]\n\n    # Test reshape\n    m = MutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n    m = m.reshape(4)\n    assert m.tolist() == [1, 2, 3, 4]\n\n    # Test __iter__\n    i = 0\n    for elem in m:\n        assert elem == i + 1\n        i += 1\n", "def test_getitem_edge_cases():\n    a = ImmutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n    raises(ValueError, lambda: a[1, 2, 0])\n    raises(ValueError, lambda: a[1, 2])\n    raises(ValueError, lambda: a[3])\n    raises(ValueError, lambda: a[1, 1, 1, 1, 1])\n    raises(ValueError, lambda: a[1, 1, 1, 1])\n\n    a = ImmutableDenseNDimArray([1, 2, 3, 4, 5, 6], (2, 3))\n    raises(ValueError, lambda: a[0, 3, 0])\n    raises(ValueError, lambda: a[0, 3])\n    raises(ValueError, lambda: a[3, 0, 0])\n    raises(ValueError, lambda: a[3, 0])\n    raises(ValueError, lambda: a[3])\n\n    raises(ValueError, lambda: ImmutableDenseNDimArray([])[()])\n", "def test_mutable_ndim_array():\n    marr = MutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n    assert marr.shape == (2, 2)\n    assert marr.rank() == 2\n    assert marr[0, 0] == 1\n    assert marr[1, 1] == 4\n\n    # Test item assignment\n    marr[0, 0] = 5\n    assert marr[0, 0] == 5\n\n    # Test reshape\n    marr_reshaped = marr.reshape(4)\n    assert marr_reshaped.shape == (4,)\n    assert marr_reshaped.rank() == 1\n\n    # Test as_immutable\n    imarr = marr.as_immutable()\n    assert imarr.shape == (2, 2)\n    assert imarr.rank() == 2\n    assert imarr[0, 0] == 5\n    assert imarr[1, 1] == 4\n\n    # Test zeros\n    marr_zeros = MutableDenseNDimArray.zeros(2, 2)\n    assert marr_zeros.shape == (2, 2)\n    assert marr_zeros.rank() == 2\n    assert marr_zeros[0, 0] == 0\n    assert marr_zeros[1, 1] == 0\n\n    # Test __setitem__ with symbolic index\n    x, y = symbols('x y')\n    marr[x, y] = 5\n    assert isinstance(marr[x, y], Indexed)\n    assert marr[x, y].subs({x: 0, y: 0}) == 5\n\n    # Test free_symbols\n    assert marr.free_symbols == set()\n\n    # Test __iter__\n    for i, elem in enumerate(marr):\n        assert elem == marr[i]\n\n    # Test __getitem__ with slice\n    assert marr[:, 0] == MutableDenseNDimArray([5, 4], (2,))\n\n    # Test tomatrix\n    assert marr.tomatrix() == Matrix([[5, 2], [3, 4]])\n"], "sample_389": ["    def setUp(self):\n        self.request = HttpRequest()\n", "    def test_build_absolute_uri(self):\n        request = HttpRequest()\n        request.META['SERVER_NAME'] = 'example.com'\n        request.path = '/path/'\n        self.assertEqual(request.build_absolute_uri(), 'http://example.com/path/')\n", "    def test_get_full_path(self):\n        req = HttpRequest()\n        req.path = \"/path/to/page\"\n        req.path_info = \"/path/to/page/info\"\n        self.assertEqual(req.get_full_path(), \"/path/to/page\")\n        self.assertEqual(req.get_full_path(force_append_slash=True), \"/path/to/page/\")\n", "    def test_get_host_with_x_forwarded_host(self):\n        request = HttpRequest()\n        request.META = {\"HTTP_X_FORWARDED_HOST\": \"example.com:8080\"}\n        request.settings = SimpleNamespace(USE_X_FORWARDED_HOST=True, ALLOWED_HOSTS=[])\n        self.assertEqual(request.get_host(), \"example.com:8080\")\n", "    def test_get_full_path(self):\n        req = HttpRequest()\n        req.path = '/path/to/resource'\n        req.path_info = '/path/to/resource/pathinfo'\n        self.assertEqual(req.get_full_path(), '/path/to/resource')\n        self.assertEqual(req.get_full_path(force_append_slash=True), '/path/to/resource/')\n"], "sample_658": ["def test_reportinfo_with_embedded_newlines(self, testdir):\n    \"\"\"Test case to make sure that DoctestItem.reportinfo() returns lineno\n    even if the docstring contains embedded newlines.\n    \"\"\"\n    p = testdir.makepyfile(\n        test_reportinfo_embedded_newlines=\"\"\"\n            '''\n                \\n\\n\n                >>> foo('a')\n                'b'\n            '''\n            return 'c'\n    \"\"\"\n    )\n    items, reprec = testdir.inline_genitems(p, \"--doctest-modules\")\n    reportinfo = items[0].reportinfo()\n    assert reportinfo[1] == 3\n", "    def test_only_first_failure(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n                '''\n                >>> foo()\n                   a  b\n                0  1  4\n                1  2  4\n                2  3  6\n                '''\n                print('   a  b\\\\n'\n                      '0  1  4\\\\n'\n                      '1  2  5\\\\n'\n                      '2  3  6')\n                '''\n                >>> bar()\n                   a  b\n                0  1  4\n                1  2  5\n                2  3  6\n                '''\n                print('   a  b\\\\n'\n                      '0  1  4\\\\n'\n                      '1  2  5\\\\n'\n                      '2  3  6\\\\n'\n                      '3  4  8')\n            \"\"\"\n        )\n        result = testdir.runpytest(\"--doctest-modules\", \"--doctest-report=only_first_failure\")\n        result.stdout.fnmatch_lines(\n            [\n                \"    Expected:\",\n                \"       a  b\",\n                \"    0  1  4\",\n                \"    1  2  4\",\n                \"    2  3  6\",\n                \"    Got:\",\n                \"       a  b\",\n                \"    0  1  4\",\n                \"    1  2  5\",\n                \"    2  3  6\",\n            ]\n        )\n", "def test_setup_fixtures_for_doctestitem(self, testdir):\n    \"\"\"\n    Test that DoctestItem._setup_fixtures sets up fixtures correctly.\n    \"\"\"\n    p = testdir.makepyfile(\n        \"\"\"\n            '''\n            >>> getfixture('tmpdir')\n            '''\n        \"\"\"\n    )\n    items, reprec = testdir.inline_genitems(p, \"--doctest-modules\")\n    assert len(items) == 1\n    assert isinstance(items[0], DoctestItem)\n    assert isinstance(items[0].parent, DoctestModule)\n    assert items[0].fixture_request\n    assert items[0].fixture_request._fixtureinfo\n", "def test_doctest_textfile_encoding_issue_3244(testdir):\n    \"\"\"Test case for issue 3244: File encoding for doctest files.\n    \"\"\"\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        doctest_encoding = latin1\n    \"\"\"\n    )\n    testdir.maketxtfile(\n        test_doc=\"\"\"\n        >>> \"Hello\"\n        'Hello'\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"* 1 passed *\"])\n", "    def test_doctest_with_setup_teardown_in_module(self, testdir):\n        \"\"\"\n        Test that setup/teardown in a doctest is properly executed.\n        \"\"\"\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n                module.testvar = 1\n\n                assert module.testvar == 1\n\n                '''\n                >>> testvar = 2\n                >>> testvar\n                2\n                '''\n                pass\n            \"\"\"\n        )\n        result = testdir.runpytest(\"--doctest-modules\")\n        result.stdout.fnmatch_lines([\"*1 passed*\"])\n"], "sample_183": ["    def test_expression_combination(self):\n        self.assertQuerysetEqual(\n            CaseTestModel.objects.annotate(\n                combined=Case(\n                    When(integer=1, then=F('integer') + 1),\n                    When(integer=2, then=F('integer') * 2),\n                    default=F('integer') - 1,\n                    output_field=IntegerField(),\n                )\n            ).order_by('pk'),\n            [(1, 2), (2, 4), (3, 2), (2, 4), (3, 2), (3, 2), (4, 3)],\n            transform=attrgetter('integer', 'combined')\n        )\n", "    def test_as_sql(self):\n        with self.assertRaisesMessage(ValueError, 'This queryset contains a reference to an outer query and may only be used in a subquery.'):\n            ResolvedOuterRef('alias').as_sql(None, None)\n", "def test_window_frame_start_end(self):\n    # Test that start and end are properly formatted.\n    connection = connection = self.connections['default']\n    frame = RowRange(start=1, end=2)\n    start, end = frame.window_frame_start_end(connection, 1, 2)\n    self.assertEqual(start, '1 ROW')\n    self.assertEqual(end, '2 ROW')\n    frame = RowRange(start=-1, end=-2)\n    start, end = frame.window_frame_start_end(connection, -1, -2)\n    self.assertEqual(start, '1 ROW')\n    self.assertEqual(end, '2 ROW')\n    frame = ValueRange(start=1, end=2)\n    start, end = frame.window_frame_start_end(connection, 1, 2)\n    self.assertEqual(start, '1')\n    self.assertEqual(end, '2')\n    frame = ValueRange(start=-1, end=-2)\n    start, end = frame.window_frame_start_end(connection, -1, -2)\n    self.assertEqual(start, '1')\n    self.assertEqual(end, '2')\n", "    def setUpTestData(cls):\n        cls.o = CaseTestModel.objects.create(integer=1, integer2=1, string='1')\n", "    def test_when_conditional_str(self):\n        when = When(integer=1, then='one')\n        self.assertEqual(str(when), \"When %r THEN %r\" % (when.condition, when.result))\n"], "sample_999": ["def test_issue_17462():\n    from sympy import Heaviside\n    assert latex(Heaviside(x)) == r\"\\theta\\left(x\\right)\"\n    assert latex(Heaviside(-x)) == r\"\\theta\\left(- x\\right)\"\n", "def test_latex_issue_13129():\n    from sympy.stats import Exponential, Uniform\n    X = Exponential('x', 1)\n    Y = Uniform('y', 0, 1)\n    assert latex(X) == r\"x \\sim \\operatorname{Exponential}{\\left (1 \\right )}\"\n    assert latex(Y) == r\"y \\sim \\operatorname{Uniform}{\\left (0, 1 \\right )}\"\n", "def test_latex_MatrixHomomorphism():\n    from sympy.categories import Matrix\n    F = Matrix([[1, 2], [3, 4]])\n    assert latex(F) == r\"\\left[\\begin{matrix}1 & 2\\\\3 & 4\\end{matrix}\\right]\"\n    assert latex(Matrix(F, 2, 3)) == r\"\\left[\\begin{matrix}1 & 2\\\\3 & 4\\end{matrix}\\right]\"\n    assert latex(Matrix(F, 3, 2)) == r\"\\left[\\begin{matrix}1 & 2\\\\3 & 4\\\\0 & 0\\end{matrix}\\right]\"\n", "def test_latex_Dict_with_non_symbol_keys():\n    latex_dict = latex({x: y})\n    assert latex_dict == r'\\left \\{ x : y\\right \\}'\n    dict_str = r'\\left \\{ \\left(x, \\quad y\\right) : z, \\quad \\left[1, 2\\right] : 1 \\right \\}'\n    assert latex({(x, y): z, [1, 2]: 1}) == dict_str\n", "def test_latexissue_3875():\n    expr = (-1)**(x**2)\n    # This should not call _do_exponent method\n    assert \"(-1)^\" not in latex(expr)\n"], "sample_833": ["def test_logistic_regression_solvers_with_imbalanced_data():\n    # Test that logistic regression solvers can handle imbalanced data\n    rng = np.random.RandomState(0)\n    X, y = make_classification(n_samples=100, n_features=10, weights=[0.95, 0.05], random_state=0)\n\n    for solver in ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']:\n        clf = LogisticRegression(solver=solver, random_state=0)\n        clf.fit(X, y)\n        assert_array_equal(clf.classes_, [0, 1])\n        assert_array_almost_equal(clf.predict_proba(X).sum(axis=1), np.ones(X.shape[0]))\n        assert_greater(np.mean(clf.predict(X) == y), 0.9)\n", "def test_logistic_regression_X_y_order(solver):\n    # Test that LogisticRegression still produces the same results if X and y\n    # are passed in different orders.\n    X, y = iris.data, iris.target\n\n    clf1 = LogisticRegression(solver=solver, multi_class='ovr',\n                              random_state=42).fit(X, y)\n    clf2 = LogisticRegression(solver=solver, multi_class='ovr',\n                              random_state=42).fit(y, X)\n    assert_array_almost_equal(clf1.coef_, clf2.coef_)\n    assert_array_almost_equal(clf1.intercept_, clf2.intercept_)\n", "def test_logistic_regression_l1_regularization_binary():\n    # Test that LogisticRegression with L1 penalty works for binary classification\n\n    rng = np.random.RandomState(0)\n    X, y = make_classification(n_samples=200, n_features=10, n_informative=5,\n                               n_redundant=0, n_repeated=0, n_classes=2,\n                               random_state=0)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n    lr_l1 = LogisticRegression(penalty='l1', solver='liblinear', tol=1e-10,\n                               random_state=0, max_iter=1000)\n    lr_l1.fit(X_train, y_train)\n\n    lr_saga = LogisticRegression(penalty='l1', solver='saga', tol=1e-10,\n                                 random_state=0, max_iter=1000)\n    lr_saga.fit(X_train, y_train)\n\n    lr_liblinear_l2 = LogisticRegression(penalty='l2', solver='liblinear',\n                                         tol=1e-10, random_state=0,\n                                         max_iter=1000)\n    lr_liblinear_l2.fit(X_train, y_train)\n\n    # L1 regularization should set some coefficients to zero\n    assert np.any(lr_l1.coef_.ravel() == 0)\n    assert np.any(lr_saga.coef_.ravel() == 0)\n\n    # L2 regularization should not set coefficients to zero\n    assert np.all(lr_liblinear_l2.coef_.ravel() != 0)\n\n    # Check that L1 and L2 regularization give different coefficients\n    assert not np.allclose(lr_l1.coef_, lr_liblinear_l2.coef_)\n    assert not np.allclose(lr_saga.coef_, lr_liblinear_l2.coef_)\n", "def test_logistic_regression_multiclass_penalty():\n    # Test that LogisticRegression correctly applies different penalties\n    # to different classes when multi_class='multinomial'\n\n    # Generate a multiclass dataset\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5,\n                               n_classes=4, random_state=42)\n\n    # Set up the model with different penalties for each class\n    penalties = ['l1', 'l2', 'elasticnet', 'none']\n    model = LogisticRegression(penalty='none', solver='saga', multi_class='multinomial',\n                               random_state=42, max_iter=1000, fit_intercept=False)\n\n    # Check that the correct penalty is applied to each class\n    for i, penalty in enumerate(penalties):\n        model.penalty = penalty\n        model.fit(X, y)\n        coef = model.coef_[i]\n\n        # Check that the coefficients are not all zero for l1 and elasticnet\n        # penalties\n        if penalty in ['l1', 'elasticnet']:\n            assert coef.any()\n", "def test_LogisticRegression_elasticnet_l1_ratio_attribute():\n    # make sure that LogisticRegression stores the attribute l1_ratio when\n    # penalty='elasticnet' and that it is used by GridSearchCV\n    X, y = make_classification(random_state=0)\n    param_grid = {'l1_ratio': [.2, .3, .4]}\n    grid_search = GridSearchCV(LogisticRegression(penalty='elasticnet', solver='saga', random_state=0),\n                               param_grid, cv=2)\n    grid_search.fit(X, y)\n    assert hasattr(grid_search.best_estimator_, 'l1_ratio')\n    assert grid_search.best_estimator_.l1_ratio in param_grid['l1_ratio']\n\n    # test that we raise an informative error when l1_ratio is passed to the\n    # LogisticRegression constructor with penalty != 'elasticnet'\n    msg = 'l1_ratio parameter is only used when penalty is \\'elasticnet\\'. ' \\\n          'Got (penalty=l2)'\n    assert_raise_message(UserWarning, msg,\n                         LogisticRegression(penalty='l2', l1_ratio=.5,\n                                            solver='saga', random_state=0).fit,\n                         X, y)\n"], "sample_231": ["    def test_technical_500_response(self):\n        request = self.rf.get('/test_view/')\n        request.user = User()\n        exc_type, exc_value, tb = sys.exc_info()\n        response = technical_500_response(request, exc_type, exc_value, tb)\n        self.assertEqual(response.status_code, 500)\n        self.assertContains(response, '<h1>Report at /test_view/</h1>', status_code=500)\n        self.assertContains(response, '<pre class=\"exception_value\">No exception message supplied</pre>', status_code=500)\n        self.assertContains(response, '<th>Request Method:</th>', status_code=500)\n        self.assertContains(response, '<th>Request URL:</th>', status_code=500)\n        self.assertNotIn('<th>Exception Type:</th>', response.content)\n        self.assertNotIn('<th>Exception Value:</th>', response.content)\n        self.assertNotIn('<h2>Traceback ', response.content)\n        self.assertContains(response, '<h2>Request information</h2>', status_code=500)\n        self.assertNotIn('<p>Request data not supplied</p>', response.content)\n", "    def test_get_traceback_frames_with_none_values(self):\n        try:\n                try:\n                    raise RuntimeError('outer')\n                except RuntimeError:\n                    raise ValueError('inner')\n            test_func()\n        except Exception:\n            exc_type, exc_value, tb = sys.exc_info()\n        request = self.rf.get('/test_view/')\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n        frames = reporter.get_traceback_frames()\n        self.assertEqual(len(frames), 2)\n        self.assertIsNone(frames[0]['exc_cause'])\n        self.assertIsNone(frames[0]['exc_cause_explicit'])\n        self.assertIsNone(frames[0]['vars'])\n", "    def test_technical_404_response(self):\n        \"A technical 404 response can be generated\"\n        request = self.rf.get('/test_view/')\n        exception = Http404({'path': '/test_view/', 'tried': []})\n        response = technical_404_response(request, exception)\n        self.assertEqual(response.status_code, 404)\n        self.assertContains(response, \"Page not found (404)\", status_code=404)\n", "    def test_response_is_html_by_default(self):\n        exc_type, exc_value, tb = (RuntimeError, RuntimeError('Whoops'), None)\n        response = technical_500_response(RequestFactory().get('/'), *exc_type, *exc_value, *tb)\n        self.assertEqual(response['Content-Type'], 'text/html')\n", "    def test_sensitive_variables_all(self):\n        \"\"\"\n        sensitive_variables('*') should hide all variables.\n        \"\"\"\n        reporter_filter = SafeExceptionReporterFilter()\n        try:\n                sauce = 'worcestershire'\n                return password\n\n            @sensitive_variables('*')\n                sauce = 'worcestershire'\n                return password\n\n            raise Exception(sensitive_test_func('secret'))\n        except Exception:\n            exc_type, exc_value, tb = sys.exc_info()\n        reporter = ExceptionReporter(None, exc_type, exc_value, tb)\n        text = reporter.get_traceback_text()\n        self.assertIn(\"password = '*************'\", text)\n        self.assertIn(\"sauce = '*************'\", text)\n"], "sample_1010": ["def test_latex_Determinant():\n    from sympy import Matrix\n    from sympy.abc import x, y, z\n\n    # Create a 3x3 matrix\n    A = Matrix([[x, y, z], [y, z, x], [z, x, y]])\n\n    assert latex(A.det()) == r\"\\det\\left(\\left[\\begin{matrix}x & y & z\\\\y & z & x\\\\z & x & y\\end{matrix}\\right]\\right)\"\n", "def test_issue_14912():\n    expr = log(x, 10) + log(x, 10)\n    assert latex(expr) == r\"2 \\log{\\left (x \\right )}\"\n    assert latex(expr, ln_notation=True) == r\"2 \\ln{\\left (x \\right )}\"\n", "def test_latex_directories():\n    from sympy.geometry import Point\n    p1 = Point(0, 0)\n    p2 = Point(1, 1)\n    d1 = p2 - p1\n    d2 = p1 - p2\n    assert latex(d1) == r\"\\left[\\begin{smallmatrix}1\\\\1\\end{smallmatrix}\\right]\"\n    assert latex(d2) == r\"\\left[\\begin{smallmatrix}-1\\\\-1\\end{smallmatrix}\\right]\"\n", "def test_printing_functions_with_latex_variable():\n    alpha = Symbol('alpha')\n    alpha_latex = r'\\tilde{\\alpha}'\n    assert latex(alpha) == r\"\\alpha\"\n    assert latex(alpha, symbol_names={alpha: alpha_latex}) == alpha_latex\n", "def test_latex_Greek_with_modifiers():\n    alpha = symbols('alpha')\n    Alpha = symbols('Alpha')\n    beta = symbols('beta')\n    Beta = symbols('Beta')\n\n    # check we can still do the standard way\n    assert latex(alpha) == r'\\alpha'\n    assert latex(beta) == r'\\beta'\n    assert latex(Alpha) == 'A'\n    assert latex(Beta) == 'B'\n\n    # check the new way\n    alpha_tilde = symbols(r'\\alpha~')\n    beta_tilde = symbols(r'\\beta~')\n    assert latex(alpha_tilde) == r'\\tilde{\\alpha}'\n    assert latex(beta_tilde) == r'\\tilde{\\beta}'\n    alpha_vec = symbols(r'\\alpha\\vec')\n    beta_vec = symbols(r'\\beta\\vec')\n    assert latex(alpha_vec) == r'\\vec{\\alpha}'\n    assert latex(beta_vec) == r'\\vec{\\beta}'\n    alpha_h = symbols(r'\\alpha^h')\n    beta_h = symbols(r'\\beta^h')\n    assert latex(alpha_h) == r'\\hat{\\alpha}'\n    assert latex(beta_h) == r'\\hat{\\beta}'\n    alpha_d = symbols(r'\\alpha\\dagger')\n    beta_d = symbols(r'\\beta\\dagger')\n    assert latex(alpha_d) == r'\\alpha^{\\dagger}'\n    assert latex(beta_d) == r'\\beta^{\\dagger}'\n    alpha_prime = symbols(r'\\alpha\\prime')\n    beta_prime = symbols(r'\\beta\\prime')\n    assert latex(alpha_prime) == r'\\alpha^{\\prime}'\n    assert latex(beta_prime) == r'\\beta^{\\prime}'\n    alpha_dot = symbols(r'\\alpha\\dot')\n    beta_dot = symbols(r'\\beta\\dot')\n    assert latex(alpha_dot) == r'\\dot{\\alpha}'\n    assert latex(beta_dot) == r'\\dot{\\beta}'\n    alpha_ddot = symbols(r'\\alpha\\ddot')\n    beta_ddot = symbols(r'\\beta\\ddot')\n    assert latex(alpha_ddot) == r'\\ddot{\\alpha}'\n    assert latex(beta_ddot) == r'\\ddot{\\beta}'\n    alpha_dddot = symbols(r'\\alpha\\dddot')\n    beta_dddot = symbols(r'\\beta\\dddot')\n    assert latex(alpha_dddot) == r'\\dddot{\\alpha}'\n    assert latex(beta_dddot) == r'\\dddot{\\beta}'\n    alpha_dd"], "sample_251": ["def test_annotation_filter_with_subquery_exists(self):\n    long_books_qs = Book.objects.filter(pages__gt=400).values('publisher').annotate(count=Count('pk')).values('count')\n    publisher_books_qs = Publisher.objects.annotate(\n        total_books=Count('book'),\n    ).filter(\n        total_books=Exists(Subquery(long_books_qs, output_field=IntegerField())),\n    ).values('name')\n    self.assertCountEqual(publisher_books_qs, [{'name': 'Sams'}, {'name': 'Morgan Kaufmann'}])\n", "    def test_annotation_with_collected_annotation(self):\n        books = Book.objects.annotate(avg_rating=Avg('rating')).annotate(\n            is_highly_rated=Case(\n                When(avg_rating__gt=4.5, then=Value(True)),\n                default=Value(False),\n                output_field=BooleanField(),\n            )\n        )\n        for book in books:\n            self.assertIsInstance(book.is_highly_rated, bool)\n", "def test_subquery_annotation_with_boolean_expression(self):\n    subquery_books_qs = Book.objects.filter(\n        publisher=OuterRef('pk'),\n    ).values('publisher').annotate(\n        has_pages_gt_400=Case(\n            When(pages__gt=400, then=Value(True)),\n            default=Value(False),\n            output_field=BooleanField(),\n        )\n    ).values('has_pages_gt_400')\n\n    publisher_books_qs = Publisher.objects.annotate(\n        has_pages_gt_400=Subquery(subquery_books_qs, output_field=BooleanField()),\n    ).filter(\n        has_pages_gt_400=True,\n    ).values('name')\n\n    self.assertCountEqual(publisher_books_qs, [{'name': 'Sams'}, {'name': 'Morgan Kaufmann'}])\n", "    def test_case_expression_annotation_with_aggregation(self):\n        books = Book.objects.annotate(\n            rating_bucket=Case(\n                When(rating__gte=5, then=Value('Excellent')),\n                When(rating__gte=3, then=Value('Good')),\n                When(rating__gte=1, then=Value('Okay')),\n                default=Value('Bad'),\n                output_field=CharField()\n            ),\n            rating_count=Count('rating'),\n        ).values('rating_bucket', 'rating_count')\n        self.assertEqual(\n            set(book['rating_bucket'] for book in books),\n            {'Excellent', 'Good'}\n        )\n        self.assertEqual(len(books), Book.objects.count())\n", "def test_window_expression_in_subquery(self):\n    # Create a subquery with a Window expression.\n    subquery = Book.objects.annotate(\n        total_pages=Window(\n            expression=Sum('pages'),\n            partition_by=F('publisher'),\n            order_by=F('pages').desc(),\n        )\n    ).values('publisher')\n\n    # Use the subquery as a filter.\n    publishers = Publisher.objects.filter(book__publisher=OuterRef('pk'), book__in=subquery)\n    self.assertQuerysetEqual(\n        publishers,\n        ['Apress', 'Morgan Kaufmann'],\n        lambda p: p.name\n    )\n"], "sample_249": ["    def test_clone_test_db(self, mocked_clone_test_db):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        creation.clone_test_db(suffix='suffix', verbosity=1, autoclobber=False, keepdb=False)\n        mocked_clone_test_db.assert_called_once_with('suffix', verbosity=1, keepdb=False)\n", "    def test_deserialize_db_from_string_non_existent_model(self):\n        # deserialize_db_from_string() raises a DeserializationError when a model\n        # is deserialized that doesn't exist.\n        data = \"\"\"\n        [\n            {\n                \"model\": \"backends.nonexistentmodel\",\n                \"pk\": 1,\n                \"fields\": {}\n            }\n        ]\n        \"\"\"\n        with self.assertRaises(serializers.base.DeserializationError):\n            connection.creation.deserialize_db_from_string(data)\n", "    def test_constraint_checks(self):\n        # deserialize_db_from_string() handles constraint checks.\n        data = \"\"\"\n        [\n            {\n                \"model\": \"backends.object\",\n                \"pk\": 1,\n                \"fields\": {\"obj_ref\": 1, \"related_objects\": []}\n            },\n            {\n                \"model\": \"backends.objectreference\",\n                \"pk\": 1,\n                \"fields\": {\"obj\": 2}\n            }\n        ]\n        \"\"\"\n        with self.assertRaises(IntegrityError):\n            connection.creation.deserialize_db_from_string(data)\n", "    def test_create_test_db(self):\n        # Create a test database and test its creation.\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        test_db_name = creation._get_test_db_name()\n        try:\n            creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n            self.assertEqual(test_connection.settings_dict['NAME'], test_db_name)\n            self.assertEqual(connections[DEFAULT_DB_ALIAS].settings_dict['NAME'], test_db_name)\n        finally:\n            creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def test_destroy_test_db(self):\n        # Test destroying a test database with a custom test db name.\n        test_name = 'test_hodor'\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST'] = {'NAME': test_name}\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            creation.destroy_test_db(old_database_name, verbosity=0)\n            # Test that the test database is destroyed.\n            with self.assertRaises(Exception):\n                connection.cursor().execute(\"SELECT * FROM django_migrations\")\n        finally:\n            test_connection.settings_dict['NAME'] = old_database_name\n"], "sample_494": ["def test_serialize_settings_reference(self):\n    self.assertSerializedResultEqual(\n        SettingsReference(settings.AUTH_USER_MODEL, \"AUTH_USER_MODEL\"),\n        (\"settings.AUTH_USER_MODEL\", {\"from django.conf import settings\"}),\n    )\n    self.assertSerializedResultEqual(\n        SettingsReference(\"someapp.model\", \"AUTH_USER_MODEL\"),\n        (\"settings.AUTH_USER_MODEL\", {\"from django.conf import settings\"}),\n    )\n", "def test_serialize_function_type_with_lambdas(self):\n    with self.assertRaisesMessage(ValueError, \"Cannot serialize function: lambda\"):\n        self.assertSerializedEqual(lambda x: x**2)\n    with self.assertRaisesMessage(ValueError, \"Cannot serialize function: lambda\"):\n        self.assertSerializedEqual(lambda: \"Hello World\")\n    with self.assertRaisesMessage(ValueError, \"Cannot serialize function: lambda\"):\n        self.assertSerializedEqual(lambda x, y: x + y)\n", "    def test_serialize_function_type_from_module(self):\n        \"\"\"\n        A function type from a module can be serialized.\n        \"\"\"\n        module = types.ModuleType(\"my_module\", \"This is my module\")\n        module.my_function = lambda x: x * x\n        self.assertSerializedResultEqual(\n            module.my_function,\n            (\n                \"my_module.my_function\",\n                {\"import my_module\"},\n            ),\n        )\n", "def test_serialize_settingsreference(self):\n    setting_name = \"TEST_SETTING\"\n    setting_value = \"test_value\"\n    with self.settings(**{setting_name: setting_value}):\n        settings_reference = SettingsReference(setting_name, \"setting_name\")\n        self.assertSerializedResultEqual(\n            settings_reference,\n            (f\"settings.{setting_name}\", {\"from django.conf import settings\"}),\n        )\n        self.assertEqual(\n            self.serialize_round_trip(settings_reference), setting_value\n        )\n\n    # Test SettingsReference that doesn't exist in settings\n    settings_reference = SettingsReference(setting_name, \"setting_name\")\n    self.assertSerializedResultEqual(\n        settings_reference,\n        (f\"settings.{setting_name}\", {\"from django.conf import settings\"}),\n    )\n    with self.assertRaisesMessage(\n        AttributeError, f\"Invalid setting_name 'setting_name'\"\n    ):\n        self.serialize_round_trip(settings_reference)\n", "def test_serialize_pathlib_windows(self):\n    # Concrete path objects work on supported platforms.\n    if sys.platform == \"win32\":\n        self.assertSerializedEqual(pathlib.WindowsPath(\"C:\\\\\\\\Windows\"))\n        self.assertSerializedResultEqual(\n            pathlib.WindowsPath(\"C:\\\\\\\\Windows\"),\n            (\"pathlib.WindowsPath('C:/Windows')\", {\"import pathlib\"}),\n        )\n        self.assertSerializedEqual(pathlib.WindowsPath(\"C:\\\\Windows\\\\path\"))\n        self.assertSerializedResultEqual(\n            pathlib.WindowsPath(\"C:\\\\Windows\\\\path\"),\n            (\"pathlib.WindowsPath('C:/Windows/path')\", {\"import pathlib\"}),\n        )\n        self.assertSerializedEqual(pathlib.WindowsPath(\"C:Windows\"))\n        self.assertSerializedResultEqual(\n            pathlib.WindowsPath(\"C:Windows\"),\n            (\"pathlib.WindowsPath('C:Windows')\", {\"import pathlib\"}),\n        )\n        self.assertSerializedEqual(pathlib.WindowsPath(\"C:Windows\\\\path\"))\n        self.assertSerializedResultEqual(\n            pathlib.WindowsPath(\"C:Windows\\\\path\"),\n            (\"pathlib.WindowsPath('C:/Windows/path')\", {\"import pathlib\"}),\n        )\n        self.assertSerializedEqual(pathlib.WindowsPath(\"C:\\\\\\\\Windows\\\\\\\\path\"))\n        self.assertSerializedResultEqual(\n            pathlib.WindowsPath(\"C:\\\\\\\\Windows\\\\\\\\path\"),\n            (\"pathlib.WindowsPath('C:/Windows/path')\", {\"import pathlib\"}),\n        )\n        self.assertSerializedEqual(pathlib.WindowsPath(\"C:\\\\\\\\Windows\\\\\\\\\\\\path\"))\n        self.assertSerializedResultEqual(\n            pathlib.WindowsPath(\"C:\\\\\\\\Windows\\\\\\\\\\\\path\"),\n            (\"pathlib.WindowsPath('C:/Windows/path')\", {\"import pathlib\"}),\n        )\n    else:\n        self.assertSerializedEqual(pathlib.PosixPath(\"/path/to/file\"))\n        self.assertSerializedResultEqual(\n            pathlib.PosixPath(\"/path/to/file\"),\n            (\"pathlib.PurePosixPath('/path/to/file')\", {\"import pathlib\"}),\n        )\n"], "sample_78": ["def test_handle_default_options(self):\n    \"\"\"\n    Test that handle_default_options correctly sets environment variables.\n    \"\"\"\n    options = mock.Mock()\n    options.settings = 'test_settings'\n    options.pythonpath = '/test/path'\n\n    handle_default_options(options)\n\n    self.assertEqual(os.environ['DJANGO_SETTINGS_MODULE'], 'test_settings')\n    self.assertIn('/test/path', sys.path)\n\n    del os.environ['DJANGO_SETTINGS_MODULE']\n    sys.path.remove('/test/path')\n", "def test_handle_default_options(self):\n    options = mock.Mock()\n    options.settings = 'my_project.settings'\n    options.pythonpath = '/path/to/python'\n\n    with self.settings(DJANGO_SETTINGS_MODULE=None):\n        with mock.patch('sys.path.insert') as mock_insert:\n            handle_default_options(options)\n            self.assertEqual(os.environ['DJANGO_SETTINGS_MODULE'], options.settings)\n            mock_insert.assert_called_once_with(0, options.pythonpath)\n", "def test_base_command_init(self):\n    \"\"\"Test the initialization of BaseCommand with different options.\"\"\"\n    cmd = BaseCommand(stdout=StringIO(), stderr=StringIO(), no_color=True, force_color=False)\n    self.assertIsInstance(cmd.stdout, OutputWrapper)\n    self.assertIsInstance(cmd.stderr, OutputWrapper)\n    self.assertEqual(cmd.style, no_style())\n\n    with self.assertRaises(CommandError):\n        BaseCommand(stdout=StringIO(), stderr=StringIO(), no_color=True, force_color=True)\n", "    def test_base_command_execute_force_color(self):\n        \"\"\" Test that BaseCommand.execute() handles the force_color option correctly \"\"\"\n        out = StringIO()\n        management.call_command('dance', stdout=out, force_color=True, no_color=False)\n        self.assertEqual(self.style == color_style(force_color=True), True)\n", "    def test_command_error_raised_from_handle(self):\n        \"\"\"\n        Exception raised in a command's handle method should raise CommandError\n        with call_command.\n        \"\"\"\n        with self.assertRaises(CommandError):\n            management.call_command('dance', example=\"handle\")\n"], "sample_463": ["def test_alter_model_table_comment_with_quote(self):\n    author_with_table_comment = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n        ],\n        {\"db_table_comment\": 'Table \"comment\"'},\n    )\n    changes = self.get_changes(\n        [self.author_empty],\n        [author_with_table_comment],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelTableComment\"])\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        name=\"author\",\n        table_comment='Table \"comment\"',\n    )\n", "def test_alter_field_with_renamed_model_and_field(self):\n    \"\"\"\n    Tests autodetection of altered fields when the model and field are both\n    renamed.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_name],\n        [\n            ModelState(\n                \"testapp\",\n                \"Writer\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"names\", models.CharField(max_length=400)),\n                ],\n            ),\n        ],\n        MigrationQuestioner({\"ask_rename\": True, \"ask_rename_model\": True}),\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameModel\", \"RenameField\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, old_name=\"Author\", new_name=\"Writer\"\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        1,\n        old_name=\"name\",\n        new_name=\"names\",\n    )\n", "def test_remove_model_with_indexes_and_constraints(self):\n    \"\"\"\n    Removing a model with indexes and constraints should generate a migration\n    that removes the indexes and constraints first, then removes the model.\n    \"\"\"\n    author_with_indexes_and_constraints = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ],\n        {\n            \"indexes\": [\n                models.Index(fields=[\"name\"], name=\"author_name_idx\"),\n            ],\n            \"constraints\": [\n                models.CheckConstraint(\n                    check=models.Q(name__contains=\"Bob\"), name=\"name_contains_bob\"\n                )\n            ],\n        },\n    )\n    changes = self.get_changes(\n        [author_with_indexes_and_constraints], [self.author_empty]\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(\n        changes, \"testapp\", 0, [\"RemoveIndex\", \"RemoveConstraint\", \"DeleteModel\"]\n    )\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, model_name=\"author\", name=\"author_name_idx\"\n    )\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 1, model_name=\"author\", name=\"name_contains_bob\"\n    )\n    self.assertOperationAttributes(changes, \"testapp\", 0, 2, name=\"Author\")\n", "def test_alter_field_with_deconstructed_default(self):\n    model_state = ModelState(\n        \"testapp\",\n        \"Model\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\n                \"name\",\n                models.CharField(max_length=200, default=DeconstructibleObject()),\n            ),\n        ],\n    )\n    deconstructed_default = DeconstructibleObject()\n    changes = self.get_changes([], [model_state])\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"CreateModel\"])\n    self.assertOperationFieldAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        default=deconstructed_default,\n    )\n", "def test_alter_field_to_not_null_with_non_defaultable_field(self):\n    \"\"\"#23609 - Tests autodetection of nullable to non-nullable alterations.\"\"\"\n    changes = self.get_changes([self.author_name_null], [self.author_name])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, name=\"name\", preserve_default=True\n    )\n    self.assertOperationFieldAttributes(\n        changes, \"testapp\", 0, 0, default=models.NOT_PROVIDED\n    )\n\n    # Test the same scenario but with a non-defaultable field\n    changes = self.get_changes(\n        [ModelState(\"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True))])],\n        [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=200, null=False)),\n                ],\n            )\n        ],\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, name=\"name\", preserve_default=True\n    )\n    self.assertOperationFieldAttributes(\n        changes, \"testapp\", 0, 0, default=models.NOT_PROVIDED\n    )\n"], "sample_487": ["    def test_primary_key(self):\n        class TestModelAdmin(ModelAdmin):\n            list_editable = (\"id\",)\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'list_editable[0]' refers to 'id', which is not editable \"\n            \"through the admin.\",\n            \"admin.E125\",\n        )\n", "    def test_invalid_related_field(self):\n        class TestModelAdmin(ModelAdmin):\n            ordering = (\"band__genre\",)\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            Song,\n            \"The value of 'ordering[0]' refers to 'band__genre', which is not a \"\n            \"field of 'modeladmin.Song'.\",\n            \"admin.E033\",\n        )\n", "    def test_autocomplete_fields_many_to_many(self):\n        class Admin(ModelAdmin):\n            autocomplete_fields = (\"many_to_many\",)\n\n        self.assertIsInvalid(\n            Admin,\n            ValidationTestModel,\n            msg=(\n                \"The value of 'autocomplete_fields[0]' cannot include the \"\n                \"ManyToManyField 'many_to_many', because that field manually \"\n                \"specifies a relationship model.\"\n            ),\n            id=\"admin.E013\",\n            invalid_obj=Admin,\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_not_boolean_or_callable(self):\n        class TestModelAdmin(ModelAdmin):\n            view_on_site = \"not boolean or callable\"\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'view_on_site' must be a callable or a boolean value.\",\n            \"admin.E025\",\n        )\n"], "sample_149": ["    def test_builtin_permission_name_length(self):\n        class Checked(models.Model):\n            class Meta:\n                verbose_name = 'x' * 250\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The verbose_name of model 'auth_tests.Checked' must be at most 245 \"\n                \"characters for its builtin permission names to be at most 255 characters.\",\n                obj=Checked,\n                id='auth.E007',\n            ),\n        ])\n\n        class Checked(models.Model):\n            class Meta:\n                verbose_name = 'x' * 245\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [])\n\n        class Checked(models.Model):\n            class Meta:\n                verbose_name = 'x' * 244\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [])\n\n        class Checked(models.Model):\n            class Meta:\n                verbose_name = 'x' * 243\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [])\n", "    def test_model_permission_with_non_string_name(self):\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    (123, 'Some permission'),\n                ]\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The permission codenamed '123' of model 'auth_tests.Checked' \"\n                \"is longer than 100 characters.\",\n                obj=Checked,\n                id='auth.E012',\n            ),\n        ])\n", "    def test_builtin_permission_name_length_with_custom_permissions(self):\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('change_checked', 'Can edit permission (builtin permission)')\n                ]\n                default_permissions = ('change_checked',)\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [])\n", "    def test_builtin_permission_name_length_after_verbose_name_truncation(self):\n        class Checked(models.Model):\n            class Meta:\n                verbose_name = 'some long verbose name' * 3\n\n        Permission = apps.get_model('auth', 'Permission')\n        permission_name_max_length = Permission._meta.get_field('name').max_length\n        max_builtin_permission_name_length = len(\"can_add_%s\" % 'Checked'.lower())\n        verbose_name_max_length = permission_name_max_length - (max_builtin_permission_name_length - len('Checked'))\n        class Checked(models.Model):\n            class Meta:\n                verbose_name = 'a' * (verbose_name_max_length + 1)\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The verbose_name of model 'auth_tests.Checked' must be at most %d \"\n                \"characters for its builtin permission names to be at most %d characters.\" % (\n                    verbose_name_max_length, permission_name_max_length\n                ),\n                obj=Checked,\n                id='auth.E007',\n            ),\n        ])\n", "    def test_builtin_permission_name_length(self):\n        class Checked(models.Model):\n            class Meta:\n                verbose_name = 'a' * 253\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [])\n"], "sample_909": ["    def test_warns_section(self):\n        docstrings = [\n            (\"\"\"\\", "    def test_long_description(self):\n        docstring = \"\"\"", "    def test_custom_section_aliases(self):\n\n        docstrings = ((\"\"\"\\", "    def test_napoleon_custom_sections(self):\n        testConfig = Config(napoleon_custom_sections=['My Custom Section'])\n        docstring = \"\"\"My Custom Section:\n", "    def test_extremely_long_list_of_definitions(self):\n        docstring = \"\"\"One line summary.\n"], "sample_558": ["def test_grid_set_axes_pad():\n    fig = plt.figure()\n    grid = Grid(fig, 111, (2, 2))\n    grid.set_axes_pad((0.1, 0.2))\n    assert grid.get_axes_pad() == (0.1, 0.2)\n", "def test_image_grid_each_right_label_mode_L():\n    imdata = np.arange(100).reshape((10, 10))\n\n    fig = plt.figure(1, (3, 3))\n    grid = ImageGrid(fig, (1, 1, 1), nrows_ncols=(3, 2), axes_pad=(0.5, 0.3),\n                     cbar_mode=\"each\", cbar_location=\"right\", cbar_size=\"15%\",\n                     label_mode=\"L\")\n    # 3-tuple rect => SubplotDivider\n    assert isinstance(grid.get_divider(), SubplotDivider)\n    assert grid.get_axes_pad() == (0.5, 0.3)\n    assert grid.get_aspect()  # True by default for ImageGrid\n    for ax, cax in zip(grid, grid.cbar_axes):\n        im = ax.imshow(imdata, interpolation='none')\n        cax.colorbar(im)\n", "def test_imagegrid_cbar_mode_each(fig_test, fig_ref):\n    imdata = np.arange(100).reshape((10, 10))\n\n    grid = ImageGrid(fig_test, 111, nrows_ncols=(2, 2),\n                     cbar_mode=\"each\", cbar_location=\"right\", cbar_size=\"15%\",\n                     label_mode=\"all\")\n    for ax, cax in zip(grid, grid.cbar_axes):\n        im = ax.imshow(imdata, interpolation='none')\n        cax.colorbar(im)\n\n    grid = ImageGrid(fig_ref, 111, nrows_ncols=(2, 2),\n                     cbar_mode=\"each\", cbar_location=\"right\", cbar_size=\"15%\",\n                     label_mode=\"all\")\n    for ax, cax in zip(grid, grid.cbar_axes):\n        im = ax.imshow(imdata, interpolation='none')\n        cax.colorbar(im)\n", "def test_grid_share_all():\n    fig = plt.figure()\n    grid = Grid(fig, 111, (2, 2), share_all=True)\n    for ax in grid.axes_row:\n        assert ax[0].get_shared_x_axes().get_siblings(ax[0]) == {ax[1]}\n        assert ax[0].get_shared_y_axes().get_siblings(ax[0]) == {ax[1]}\n", "def test_image_grid_single_right_label_mode_L():\n    imdata = np.arange(100).reshape((10, 10))\n\n    fig = plt.figure(1, (3, 3))\n    grid = ImageGrid(fig, (1, 1, 1), nrows_ncols=(2, 2), axes_pad=(0.2, 0.15),\n                     cbar_mode=\"single\", cbar_location=\"right\", cbar_size=\"10%\",\n                     label_mode=\"L\")\n    for i in range(4):\n        im = grid[i].imshow(imdata, interpolation='none')\n    grid.cbar_axes[0].colorbar(im)\n    grid[0].set_title(\"Test\")\n    grid[1].set_xlabel(\"X\")\n    grid[2].set_ylabel(\"Y\")\n    grid.cbar_axes[0].set_ylabel(\"Colorbar\")\n    assert grid.cbar_axes[0].get_ylabel() == \"Colorbar\"\n    assert grid.cbar_axes[0].get_title() == \"\"\n"], "sample_255": ["    def test_cleanup_headers(self):\n        # Test that the 'Connection' header is set to 'close' if the\n        # content length is unknown.\n        handler = ServerHandler(\n            BytesIO(), BytesIO(), BytesIO(), {'REQUEST_METHOD': 'GET'}\n        )\n        handler.cleanup_headers()\n        self.assertEqual(handler.headers['Connection'], 'close')\n\n        # Test that the 'Connection' header is set to 'close' if the\n        # server is not a threading server.\n        handler = ServerHandler(\n            BytesIO(), BytesIO(), BytesIO(), {'REQUEST_METHOD': 'GET', 'CONTENT_LENGTH': '10'}\n        )\n        handler.request_handler.server = WSGIServer(('localhost', 0), WSGIRequestHandler)\n        handler.cleanup_headers()\n        self.assertEqual(handler.headers['Connection'], 'close')\n\n        # Test that the 'Connection' header is not changed if it's\n        # already set to 'close' or if the server is a threading server.\n        handler = ServerHandler(\n            BytesIO(), BytesIO(), BytesIO(), {'REQUEST_METHOD': 'GET', 'CONTENT_LENGTH': '10', 'HTTP_CONNECTION': 'close'}\n        )\n        handler.request_handler.server = WSGIServer(('localhost', 0), WSGIRequestHandler)\n        handler.cleanup_headers()\n        self.assertEqual(handler.headers['Connection'], 'close')\n\n        handler = ServerHandler(\n            BytesIO(), BytesIO(), BytesIO(), {'REQUEST_METHOD': 'GET', 'CONTENT_LENGTH': '10'}\n        )\n        handler.request_handler.server = ThreadedWSGIServer(('localhost', 0), WSGIRequestHandler)\n        handler.cleanup_headers()\n        self.assertNotIn('Connection', handler.headers)\n", "    def test_cleanup_headers(self):\n        request = self.request_factory.get('/')\n        handler = ServerHandler(None, None, None, request.environ)\n        handler.headers = {'Content-Length': '10'}\n        handler.cleanup_headers()\n        self.assertEqual(handler.headers['Connection'], 'keep-alive')\n", "    def test_cleanup_headers(self):\n        handler = ServerHandler(\n            BytesIO(), BytesIO(), BytesIO(), {'HTTP_HOST': 'example.com'}\n        )\n        handler.headers = {'Content-Length': '123'}\n        handler.cleanup_headers()\n        self.assertNotIn('Connection', handler.headers)\n", "    def test_daemon_threads(self):\n        server = ThreadedWSGIServer(('localhost', 0), WSGIRequestHandler)\n        self.assertTrue(server.daemon_threads)\n        self.addCleanup(server.server_close)\n", "    def test_cleanup_headers(self):\n            start_response('200 OK', [])\n            return [b'Test response body']\n\n        request = Stub()\n        server = Stub(get_app=lambda: test_app)\n        handler = ServerHandler(None, None, None, {}, server=server)\n\n        # Test when content length is known\n        handler.headers = {'Content-Length': '10'}\n        handler.cleanup_headers()\n        self.assertEqual(handler.headers.get('Connection'), 'keep-alive')\n\n        # Test when content length is unknown\n        del handler.headers['Content-Length']\n        handler.cleanup_headers()\n        self.assertEqual(handler.headers.get('Connection'), 'close')\n\n        # Test when server is not a ThreadingMixIn\n        server = Stub(get_app=lambda: test_app, ThreadingMixIn=False)\n        handler = ServerHandler(None, None, None, {}, server=server)\n        handler.headers = {'Content-Length': '10'}\n        handler.cleanup_headers()\n        self.assertEqual(handler.headers.get('Connection'), 'close')\n"], "sample_574": ["    def test_setup(self, x):\n\n        s = ContinuousBase()._setup(x, Coordinate())\n        assert_series_equal(s(x), x)\n", "    def test_set_view_interval(self):\n\n        pa = PseudoAxis(None)\n        pa.set_view_interval(1, 10)\n        assert pa.get_view_interval() == (1, 10)\n", "    def test_tick_default_on_datetime64_data(self, t):\n\n        s = Temporal()._setup(t, Coordinate())\n        a = PseudoAxis(s._matplotlib_scale)\n        a.set_view_interval(t.min(), t.max())\n        ticks = a.major.locator()\n        assert np.allclose(mpl.dates.date2num(t), ticks)\n", "    def test_sqrt_transform(self):\n        s = Continuous(trans=\"sqrt\")._setup(pd.Series([1, 4, 9]), Coordinate())\n        assert_array_equal(s(pd.Series([1, 4, 9])), np.array([1, 2, 3]))\n", "    def test_scale_priority(self):\n\n        assert Nominal._priority > Continuous._priority\n        assert Continuous._priority > Temporal._priority\n"], "sample_609": ["def test_polyval_dim_not_found():\n    da = xr.DataArray(\n        np.stack((1.0 + np.arange(10) + 2.0 * np.arange(10) ** 2, 1.0 + 2.0 * np.arange(10) + 3.0 * np.arange(10) ** 2)),\n        dims=(\"d\", \"x\"),\n        coords={\"x\": np.arange(10), \"d\": [0, 1]},\n    )\n    coeffs = xr.DataArray(\n        [[2, 1, 1], [3, 2, 1]],\n        dims=(\"d\", \"degree\"),\n        coords={\"d\": [0, 1], \"degree\": [2, 1, 0]},\n    )\n    with pytest.raises(ValueError, match=r\"degree_dim 'z' not found\"):\n        xr.polyval(da.x, coeffs, degree_dim=\"z\")\n", "def test_polyval_datetime64_coord() -> None:\n    # GH: 5477\n    xcoord = xr.DataArray(\n        pd.date_range(\"2000-01-01\", freq=\"D\", periods=10), dims=(\"x\",), name=\"x\"\n    )\n    x = xr.core.missing.get_clean_interp_index(xcoord, \"x\")\n\n    da = xr.DataArray(\n        np.stack((1.0 + x + 2.0 * x ** 2, 1.0 + 2.0 * x + 3.0 * x ** 2)),\n        dims=(\"d\", \"x\"),\n        coords={\"x\": xcoord, \"d\": [0, 1]},\n    )\n    coeffs = xr.DataArray(\n        [[2, 1, 1], [3, 2, 1]],\n        dims=(\"d\", \"degree\"),\n        coords={\"d\": [0, 1], \"degree\": [2, 1, 0]},\n    )\n\n    da_pv = xr.polyval(da.x, coeffs)\n\n    xr.testing.assert_allclose(da, da_pv.T)\n", "def test_broadcast_compat_data_with_newaxis():\n    data = np.arange(5)\n    var = xr.Variable(\"x\", data)\n\n    assert_identical(data, broadcast_compat_data(var, (\"x\",), ()))\n    assert_identical(data, broadcast_compat_data(var, (), (\"x\",)))\n    assert_identical(data[:], broadcast_compat_data(var, (\"w\",), (\"x\",)))\n    assert_identical(data[:, None], broadcast_compat_data(var, (\"w\", \"x\", \"y\"), ()))\n    assert_identical(data, broadcast_compat_data(var, (\"w\",), (\"x\",), new_dims=(True,)))\n    assert_identical(data[:, None], broadcast_compat_data(var, (\"w\", \"x\", \"y\"), (), new_dims=(True,)))\n", "def test_covcorr_with_broadcasting() -> None:\n    da_a = xr.DataArray(\n        np.array([[1, 2, 3], [0.1, 0.2, 0.3], [3.2, 0.6, 1.8]]),\n        dims=(\"space\", \"time\"),\n        coords=[\n            (\"space\", [\"IA\", \"IL\", \"IN\"]),\n            (\"time\", pd.date_range(\"2000-01-01\", freq=\"1D\", periods=3)),\n        ],\n    )\n\n    da_b = xr.DataArray(\n        np.array([[0.2, 0.4, 0.6], [15, 10, 5], [3.2, 0.6, 1.8]]),\n        dims=(\"space\", \"time\"),\n        coords=[\n            (\"space\", [\"IA\", \"IL\", \"IN\"]),\n            (\"time\", pd.date_range(\"2000-01-01\", freq=\"1D\", periods=3)),\n        ],\n    )\n\n    da_c = xr.DataArray(\n        np.array([0, 1, 2]),\n        dims=(\"time\",),\n        coords=[\n            (\"time\", pd.date_range(\"2000-01-01\", freq=\"1D\", periods=3)),\n        ],\n    )\n\n    da_d = xr.DataArray(\n        np.array([[0.2, 0.4, 0.6], [15, 10, 5], [3.2, 0.6, 1.8]]),\n        dims=(\"space\", \"time\"),\n        coords=[\n            (\"space\", [\"IA\", \"IL\", \"IN\"]),\n            (\"time\", pd.date_range(\"2000-01-01\", freq=\"1D\", periods=3)),\n        ],\n    )\n\n    # broadcasting along non-broadcasting dim\n    expected = xr.DataArray(\n        np.array([0.2, -0.5, 1.69333333]),\n        dims=(\"space\",),\n        coords=[\n            (\"space\", [\"IA\", \"IL\", \"IN\"]),\n        ],\n    )\n    actual = xr.cov(da_a, da_b)\n    assert_identical(expected, actual)\n\n    # broadcasting along broadcasting dim\n    expected = xr.DataArray(\n        np.array([-3.53055556]),\n        dims=(),\n    )\n    actual = xr.cov(da_a, da_c)\n    assert_identical(expected, actual", "def test_cov_corr_all_dims() -> None:\n    da_a = xr.DataArray(np.array([[1, 2, 3], [0.1, 0.2, 0.3]]), dims=(\"space\", \"time\"))\n    da_b = xr.DataArray(np.array([[0.2, 0.4, 0.6], [15, 10, 5]]), dims=(\"space\", \"time\"))\n\n    with pytest.raises(KeyError):\n        xr.cov(da_a, da_b, dim=\"x\")\n\n    with pytest.raises(KeyError):\n        xr.corr(da_a, da_b, dim=\"x\")\n\n    # testing with dim=None\n    expected = xr.DataArray(-3.53055556)\n    actual = xr.cov(da_a, da_b)\n    assert_identical(expected, actual)\n\n    expected = xr.DataArray(-0.57087777)\n    actual = xr.corr(da_a, da_b)\n    assert_identical(expected, actual)\n\n    # test with dim=\"time\"\n    expected = xr.DataArray([ 0.2, -0.5,  1.69333333])\n    actual = xr.cov(da_a, da_b, dim=\"time\")\n    assert_identical(expected, actual)\n\n    expected = xr.DataArray([1., -1.,  1.])\n    actual = xr.corr(da_a, da_b, dim=\"time\")\n    assert_identical(expected, actual)\n\n    # test with dim=\"space\"\n    with pytest.raises(KeyError):\n        xr.cov(da_a, da_b, dim=\"space\")\n\n    with pytest.raises(KeyError):\n        xr.corr(da_a, da_b, dim=\"space\")\n\n    # test with dim=[\"space\", \"time\"]\n    with pytest.raises(KeyError):\n        xr.cov(da_a, da_b, dim=[\"space\", \"time\"])\n\n    with pytest.raises(KeyError):\n        xr.corr(da_a, da_b, dim=[\"space\", \"time\"])\n\n    # test with ddof=1 and dim=None\n    expected = xr.DataArray(-3.53055556)\n    actual = xr.cov(da_a, da_b, dim=None, ddof=1)\n    assert_identical(expected, actual)\n\n    # test with ddof=1 and dim=\"time\"\n    expected = xr.DataArray([ 0.2, -0.5,  1.69333333])\n    actual = xr"], "sample_1104": ["def test_Pow_negative_rational():\n    assert str(x**-Rational(1, 2)) == \"x**(-1/2)\"\n    assert str(x**-Rational(-1, 2)) == \"x**(1/2)\"\n    assert str(x**Rational(-1, 2)) == \"x**(-1/2)\"\n    assert str(x**Rational(1, -2)) == \"x**(-1/2)\"\n    assert str(x**-Rational(2, 2)) == \"x**(-1)\"\n    assert str(x**-Rational(-2, 2)) == \"x**(1)\"\n    assert str(x**Rational(-2, 2)) == \"x**(-1)\"\n    assert str(x**Rational(2, -2)) == \"x**(-1)\"\n", "def test_Mul_explicit_zero_coefficient():\n    assert sstr(0*x) == \"0\"\n    assert sstr(0*x*y) == \"0\"\n    assert sstr(0*Float(0.5)*x) == \"0\"\n    assert sstr(0*Float(0.5)*x*y) == \"0\"\n    assert sstr(Rational(0)*x) == \"0\"\n    assert sstr(Rational(0)*x*y) == \"0\"\n    assert sstr(Rational(0)*Float(0.5)*x) == \"0\"\n    assert sstr(Rational(0)*Float(0.5)*x*y) == \"0\"\n", "def test_Pow_str():\n    assert str(Pow(x, 3, evaluate=False)) == \"x**3\"\n    assert str(Pow(x, -3, evaluate=False)) == \"x**(-3)\"\n    assert str(Pow(x, Rational(3, 4), evaluate=False)) == \"x**(3/4)\"\n    assert str(Pow(x, -Rational(3, 4), evaluate=False)) == \"x**(-3/4)\"\n\n    assert str(Pow(x, 3, evaluate=True)) == \"x**3\"\n    assert str(Pow(x, -3, evaluate=True)) == \"x**(-3)\"\n    assert str(Pow(x, Rational(3, 4), evaluate=True)) == \"x**(3/4)\"\n    assert str(Pow(x, -Rational(3, 4), evaluate=True)) == \"x**(-3/4)\"\n\n    assert str(Pow(x, y, evaluate=False)) == \"(x**y)\"\n    assert str(Pow(x, -y, evaluate=False)) == \"(x**(-y))\"\n    assert str(Pow(x, x, evaluate=False)) == \"(x**x)\"\n    assert str(Pow(x, -x, evaluate=False)) == \"(x**(-x))\"\n\n    assert str(Pow(2, 3, evaluate=False)) == \"2**3\"\n    assert str(Pow(2, -3, evaluate=False)) == \"2**(-3)\"\n    assert str(Pow(2, Rational(3, 4), evaluate=False)) == \"2**(3/4)\"\n    assert str(Pow(2, -Rational(3, 4), evaluate=False)) == \"2**(-3/4)\"\n\n    assert str(Pow(2, 3, evaluate=True)) == \"8\"\n    assert str(Pow(2, -3, evaluate=True)) == \"1/8\"\n    assert str(Pow(2, Rational(3, 4), evaluate=True)) == \"sqrt(8)\"\n    assert str(Pow(2, -Rational(3, 4), evaluate=True)) == \"1/sqrt(8)\"\n\n    assert str(Pow(-2, 3, evaluate=False)) == \"(-2)**3\"\n    assert str(Pow(-2, -3, evaluate=False)) == \"(-2)**(-3)\"\n    assert str(Pow(-2, Rational(3, ", "def test_Pow():\n    # Test Pow printing for symbolic bases and rational exponents.\n    x, y = symbols('x y')\n    assert str(x**Rational(2, 3)) == \"x**(2/3)\"\n    assert str(x**Rational(3, 4)) == \"x**(3/4)\"\n    assert str(x**Rational(4, 3)) == \"x**(4/3)\"\n    assert str(y**Rational(1, 2)) == \"sqrt(y)\"\n    assert str(y**Rational(3, 2)) == \"y*sqrt(y)\"\n    assert str(y**Rational(2, 2)) == \"y\"\n    assert str(y**Rational(4, 2)) == \"y**2\"\n    assert str(y**Rational(5, 2)) == \"y**2*sqrt(y)\"\n    assert str(y**Rational(-1, 2)) == \"1/sqrt(y)\"\n    assert str(y**Rational(-2, 2)) == \"1/y\"\n    assert str(y**Rational(-3, 2)) == \"1/(y*sqrt(y))\"\n    assert str(y**Rational(-4, 2)) == \"1/y**2\"\n    assert str(y**Rational(-5, 2)) == \"1/(y**2*sqrt(y))\"\n", "def test_UniversalSet():\n    assert str(S.UniversalSet) == 'UniversalSet'\n"], "sample_841": ["def test_ridge_regression_dtype_stability_with_multiple_targets(solver, seed):\n    random_state = np.random.RandomState(seed)\n    n_samples, n_features, n_targets = 6, 5, 3\n    X = random_state.randn(n_samples, n_features)\n    coef = random_state.randn(n_targets, n_features)\n    y = np.dot(X, coef.T) + 0.01 * random_state.randn(n_samples, n_targets)\n    alpha = 1.0\n    results = dict()\n    atol = 1e-3 if solver == \"sparse_cg\" else 1e-5\n    for current_dtype in (np.float32, np.float64):\n        results[current_dtype] = ridge_regression(X.astype(current_dtype),\n                                                  y.astype(current_dtype),\n                                                  alpha=alpha,\n                                                  solver=solver,\n                                                  random_state=random_state,\n                                                  sample_weight=None,\n                                                  max_iter=500,\n                                                  tol=1e-10,\n                                                  return_n_iter=False,\n                                                  return_intercept=False)\n\n    assert results[np.float32].dtype == np.float32\n    assert results[np.float64].dtype == np.float64\n    assert_allclose(results[np.float32], results[np.float64], atol=atol)\n", "def test_ridgecv_sample_weight_multitarget():\n    # Test RidgeCV with sample_weight and multiple targets.\n\n    rng = np.random.RandomState(42)\n\n    n_samples = 100\n    n_features = 10\n    n_targets = 3\n\n    X = rng.randn(n_samples, n_features)\n    Y = rng.randn(n_samples, n_targets)\n    sample_weight = np.ones(n_samples)\n\n    cv = KFold(5)\n    ridgecv = RidgeCV(cv=cv)\n    ridgecv.fit(X, Y, sample_weight=sample_weight)\n\n    parameters = {'alpha': ridgecv.alphas}\n    gs = GridSearchCV(Ridge(), parameters, cv=cv)\n    gs.fit(X, Y, sample_weight=sample_weight)\n\n    assert ridgecv.alpha_ == gs.best_estimator_.alpha\n    assert_array_almost_equal(ridgecv.coef_, gs.best_estimator_.coef_)\n", "def test_ridge_regression_n_iter(solver, max_iter):\n    X, y = make_regression(n_samples=100, n_features=10, random_state=0)\n    if solver == 'sparse_cg':\n        X_sparse = sp.csr_matrix(X)\n        out = ridge_regression(X_sparse, y, alpha=1.0, solver=solver,\n                               max_iter=max_iter, return_n_iter=True)\n        assert len(out) == 2\n        coef, n_iter = out\n        assert n_iter is not None\n    else:\n        out = ridge_regression(X, y, alpha=1.0, solver=solver,\n                               max_iter=max_iter, return_n_iter=True)\n        assert len(out) == 2\n        coef, n_iter = out\n        if max_iter is None:\n            assert n_iter is None\n        else:\n            assert n_iter is not None\n", "def test_ridge_regression_individual_penalties_with_sparse_X():\n    \"\"\"Tests the ridge object using individual penalties with sparse X\"\"\"\n    rng = np.random.RandomState(42)\n\n    n_samples, n_features, n_targets = 20, 10, 5\n    X = sp.csr_matrix(rng.randn(n_samples, n_features))\n    y = rng.randn(n_samples, n_targets)\n\n    penalties = np.arange(n_targets)\n\n    coefs = Ridge(solver='sparse_cg', tol=1e-10).fit(X, y).coef_\n    for i, target in enumerate(y.T):\n        target_ridge = Ridge(solver='sparse_cg', alpha=penalties[i],\n                             tol=1e-10).fit(X, target).coef_\n        assert_array_almost_equal(target_ridge, coefs[i])\n", "def test_ridge_regression_max_iter(solver, max_iter, return_intercept,\n                                   return_n_iter):\n    \"\"\"check if all combinations of arguments give valid estimations\"\"\"\n    rng = check_random_state(42)\n    X = rng.rand(1000, 3)\n    true_coefs = [1, 2, 0.1]\n    y = np.dot(X, true_coefs)\n    true_intercept = 0.\n    if return_intercept:\n        true_intercept = 10000.\n    y += true_intercept\n    alpha, atol, tol = 1e-3, 1e-4, 1e-6\n\n    out = ridge_regression(X, y, alpha=alpha, solver=solver,\n                           max_iter=max_iter, tol=tol,\n                           return_intercept=return_intercept,\n                           return_n_iter=return_n_iter)\n\n    if solver in ['sag', 'saga']:\n        if return_intercept and max_iter is None:\n            assert_warns(ConvergenceWarning,\n                         ridge_regression, X, y, alpha=alpha, solver=solver,\n                         max_iter=max_iter, tol=tol,\n                         return_intercept=return_intercept,\n                         return_n_iter=return_n_iter)\n\n    if return_intercept and return_n_iter:\n        coef, intercept, n_iter = out\n        assert_allclose(coef, true_coefs, rtol=0, atol=atol)\n        assert_allclose(intercept, true_intercept, rtol=0, atol=atol)\n        assert n_iter is not None\n    elif return_n_iter:\n        coef, n_iter = out\n        assert_allclose(coef, true_coefs, rtol=0, atol=atol)\n        if solver in ['lsqr', 'sag', 'saga']:\n            assert n_iter is not None\n        else:\n            assert n_iter is None\n    elif return_intercept:\n        coef, intercept = out\n        assert_allclose(coef, true_coefs, rtol=0, atol=atol)\n        assert_allclose(intercept, true_intercept"], "sample_346": ["    def test_sync_only_middleware(self):\n        @sync_only_middleware\n            pass\n        self.assertTrue(middleware_factory.sync_capable)\n        self.assertFalse(middleware_factory.async_capable)\n", "    def test_decorator_from_middleware_with_args(self):\n        \"\"\"\n        Test decorator_from_middleware_with_args\n        \"\"\"\n        middleware = decorator_from_middleware_with_args(XFrameOptionsMiddleware)\n        @middleware\n            return HttpResponse()\n        response = view(HttpRequest())\n        self.assertEqual(response.headers['X-Frame-Options'], 'DENY')\n", "    def test_sync_only_middleware(self):\n        @sync_only_middleware\n            pass\n        self.assertTrue(middleware.sync_capable)\n        self.assertFalse(middleware.async_capable)\n", "    def test_make_middleware_decorator(self):\n        class DummyMiddleware:\n                self.get_response = get_response\n                self.args = args\n                self.kwargs = kwargs\n\n                return self.get_response(request)\n\n        make_decorator = make_middleware_decorator(DummyMiddleware)\n        self.assertEqual(make_decorator.__name__, 'make_decorator')\n\n        decorated_view = make_decorator(lambda request: HttpResponse())('arg1', kwarg1='kwarg1')\n        response = decorated_view(HttpRequest())\n        self.assertIsInstance(response, HttpResponse)\n        self.assertEqual(decorated_view.args, ('arg1',))\n        self.assertEqual(decorated_view.kwargs, {'kwarg1': 'kwarg1'})\n", "    def test_decorating_class_with_non_callable(self):\n        \"\"\"Test that decorating a class with a non-callable raises an error.\"\"\"\n        msg = \"'my_decorator' is not a callable.\"\n        with self.assertRaisesMessage(TypeError, msg):\n            @method_decorator('my_decorator')\n            class Test:\n                pass\n"], "sample_79": ["    def check_values(self, value, arg, expected):\n        output = self.engine.render_to_string('t', {'value': value, 'arg': arg})\n        self.assertEqual(output, expected)\n", "    def test_truncatechars(self):\n        with self.subTest(value='Hello, World!', length=5):\n            output = self.engine.render_to_string('t', {'value': 'Hello, World!', 'length': 5})\n            self.assertEqual(output, 'Hello\u2026')\n        with self.subTest(value='Hello, World!', length='5'):\n            output = self.engine.render_to_string('t', {'value': 'Hello, World!', 'length': '5'})\n            self.assertEqual(output, 'Hello\u2026')\n        with self.subTest(value='Hello, World!', length='abc'):\n            output = self.engine.render_to_string('t', {'value': 'Hello, World!', 'length': 'abc'})\n            self.assertEqual(output, 'Hello, World!')\n", "    def setUp(self):\n        from datetime import datetime, timedelta\n        self.now = datetime.now()\n", "    def test_addslashes(self):\n        self.assertEqual(addslashes('abc\"def'), 'abc\\\\\"def')\n", "    def test_truncatechars(self):\n        self.engine.render_to_string('t', {'value': 'Hello world'})\n        with self.subTest(truncation=5):\n            output = self.engine.render_to_string('t', {'value': 'Hello world'})\n            self.assertEqual(output, 'Hell\u2026')\n        with self.subTest(truncation=8):\n            output = self.engine.render_to_string('t', {'value': 'Hello world'})\n            self.assertEqual(output, 'Hello w\u2026')\n        with self.subTest(truncation=15):\n            output = self.engine.render_to_string('t', {'value': 'Hello world'})\n            self.assertEqual(output, 'Hello world')\n"], "sample_667": ["def test_getbasetemp_absolute_path(tmp_path):\n    from _pytest.tmpdir import TempPathFactory\n\n    config = FakeConfig(tmp_path)\n    t = TempPathFactory.from_config(config)\n    abs_basetemp = t.getbasetemp()\n    assert abs_basetemp == abs_basetemp.resolve()\n    assert abs_basetemp.is_absolute()\n", "def test_tmp_path_factory_mktemp_with_absolute_path(testdir):\n    \"\"\"Test that TempPathFactory.mktemp raises a ValueError when given an absolute path.\"\"\"\n    from _pytest.tmpdir import TempPathFactory\n\n    config = FakeConfig(testdir.tmpdir)\n    factory = TempPathFactory.from_config(config)\n    with pytest.raises(ValueError):\n        factory.mktemp(\"/absolute/path\", numbered=False)\n", "def test_getbasetemp_with_custom_basetemp(testdir, monkeypatch):\n    mytemp = testdir.tmpdir.mkdir(\"xyz\")\n    p = testdir.makepyfile(\n        \"\"\"\n            tmp_path_factory.getbasetemp()\n    \"\"\"\n    )\n    monkeypatch.setenv(\"PYTEST_DEBUG_TEMPROOT\", str(mytemp))\n    testdir.runpytest(p, \"--basetemp=%s\" % mytemp)\n    mytemp.check()\n    assert mytemp.join(\"pytest-of-unknown\").check()\n", "    def test_getbasetemp_without_config(self, tmp_path):\n        from _pytest.tmpdir import TempPathFactory\n\n        t = TempPathFactory(None, None)\n        assert t.getbasetemp().is_dir()\n", "def test_ensure_reset_dir_with_read_only_directory(tmp_path):\n    \"\"\"Ensure ensure_reset_dir can handle read-only directories (#5524)\"\"\"\n    from _pytest.pathlib import ensure_reset_dir\n\n    adir = tmp_path / \"dir\"\n    adir.mkdir()\n\n    (adir / \"foo.txt\").touch()\n\n    ensure_reset_dir(adir)\n\n    assert not adir.is_dir()\n\n    # test with read-only directory\n    adir.mkdir()\n    self.chmod_r(adir)\n\n    ensure_reset_dir(adir)\n\n    assert not adir.is_dir()\n"], "sample_283": ["def test_default_database(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({}),\n        (['psql', 'postgres'], {}),\n    )\n", "def test_default_dbname(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({\n            'USER': 'someuser',\n            'PASSWORD': 'somepassword',\n            'HOST': 'somehost',\n            'PORT': '444',\n        }), (\n            ['psql', '-U', 'someuser', '-h', 'somehost', '-p', '444', 'postgres'],\n            {'PGPASSWORD': 'somepassword'},\n        )\n    )\n", "def test_default_database_name(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({\n            'USER': 'someuser',\n            'PASSWORD': 'somepassword',\n            'HOST': 'somehost',\n            'PORT': '444',\n        }), (\n            ['psql', '-U', 'someuser', '-h', 'somehost', '-p', '444', 'postgres'],\n            {'PGPASSWORD': 'somepassword'},\n        )\n    )\n", "def test_default_dbname(self):\n    \"\"\"Test that the default database name 'postgres' is used when not specified.\"\"\"\n    self.assertEqual(\n        self.settings_to_cmd_args_env({\n            'USER': 'someuser',\n            'PASSWORD': 'somepassword',\n            'HOST': 'somehost',\n            'PORT': '444',\n        }), (\n            ['psql', '-U', 'someuser', '-h', 'somehost', '-p', '444', 'postgres'],\n            {'PGPASSWORD': 'somepassword'},\n        )\n    )\n", "def test_default_database(self):\n    \"\"\"Test default database name when not provided.\"\"\"\n    self.assertEqual(\n        self.settings_to_cmd_args_env({}),\n        (['psql', 'postgres'], {}),\n    )\n"], "sample_630": ["def test_is_exception(node, expected):\n    \"\"\"is_exception\"\"\"\n    node = astroid.extract_node(node)\n    got = is_exception(node)\n    assert got == (expected == \"class\"), f\"got {got} instead of {expected} for value {node}\"\n\n", "def test_write_diagram(tmpdir, config, output_format):\n    \"\"\"Test that write_diagram function generates files correctly\"\"\"\n    project = get_project(os.path.join(os.path.dirname(__file__), \"data\"))\n    linker = Linker(project)\n    handler = DiadefsHandler(config)\n    dd = DefaultDiadefGenerator(linker, handler).visit(project)\n    for diagram in dd:\n        diagram.extract_relationships()\n    writer = DotWriter(config) if output_format == \"dot\" else VCGWriter(config)\n    config.output_directory = str(tmpdir)\n    writer.write(dd)\n    for diagram in dd:\n        basename = diagram.title.strip().replace(\" \", \"_\")\n        file_name = f\"{basename}.{output_format}\"\n        assert os.path.exists(os.path.join(config.output_directory, file_name))\n", "def test_dot_writer_write_packages():\n    config = Config()\n    writer = DotWriter(config)\n    diagram = next(d for d in writer.write([{\"TYPE\": \"package\", \"modules\": [\"module1\", \"module2\"]}]))\n\n    expected = [\n        'digraph G {',\n        '    graph [rankdir=BT];',\n        '    node0 [label=\"module1\", shape=box];',\n        '    node1 [label=\"module2\", shape=box];',\n        '    node0 -> node1 [arrowtail=none, arrowhead=open];',\n        '}',\n    ]\n    with patch(\"pylint.pyreverse.writer.DotWriter.get_title\") as mock_get_title:\n        mock_get_title.return_value = \"module1\"\n        with patch(\"pylint.pyreverse.writer.DotWriter.write_packages\") as mock_write_packages:\n            writer.write([{\"TYPE\": \"package\", \"modules\": [\"module1\", \"module2\"]}])\n            assert mock_write_packages.called\n\n    with open(\"packages_No_Name.dot\", \"r\") as f:\n        lines = [line.strip() for line in f.readlines()]\n        assert lines == expected\n    os.remove(\"packages_No_Name.dot\")\n", "def test_diagram_writer_exceptions():\n    \"\"\"Test DiagramWriter abstract methods\"\"\"\n    with pytest.raises(NotImplementedError):\n        writer = DiagramWriter(None, None)\n        writer.set_printer(\"file\", \"basename\")\n\n    with pytest.raises(NotImplementedError):\n        writer.get_title(None)\n\n    with pytest.raises(NotImplementedError):\n        writer.get_values(None)\n\n    with pytest.raises(NotImplementedError):\n        writer.close_graph()\n\n", "def test_DotWriter_write_classes():\n    \"\"\"Test DotWriter write_classes method\"\"\"\n    CONFIG = Config()\n    writer = DotWriter(CONFIG)\n    diagram = DiadefGenerator(MockLinker()).visit(MockProject())\n    writer.write([diagram])\n\n    # Check if the output file exists\n    assert os.path.exists(\"classes_No_Name.dot\")\n\n    # Check the contents of the output file\n    with open(\"classes_No_Name.dot\", \"r\") as file:\n        contents = file.read()\n        assert \"digraph\" in contents\n        assert \"label=\" in contents\n        assert \"shape=\" in contents\n\n    # Remove the output file\n    os.remove(\"classes_No_Name.dot\")\n\n"], "sample_1054": ["def test_ComplexRegion_boundary():\n    unit_disk = ComplexRegion(Interval(0, 1)*Interval(0, 2*S.Pi), polar=True)\n    upper_half_unit_disk = ComplexRegion(Interval(0, 1)*Interval(0, S.Pi), polar=True)\n    upper_half_disk = ComplexRegion(Interval(0, oo)*Interval(0, S.Pi), polar=True)\n    lower_half_disk = ComplexRegion(Interval(0, oo)*Interval(S.Pi, 2*S.Pi), polar=True)\n    right_half_disk = ComplexRegion(Interval(0, oo)*Interval(-S.Pi/2, S.Pi/2), polar=True)\n    first_quad_disk = ComplexRegion(Interval(0, oo)*Interval(0, S.Pi/2), polar=True)\n\n    unit_square = ComplexRegion(Interval(-1, 1)*Interval(-1, 1))\n    upper_half_unit_square = ComplexRegion(Interval(-1, 1)*Interval(0, 1))\n    upper_half_plane = ComplexRegion(Interval(-oo, oo)*Interval(0, oo))\n    lower_half_plane = ComplexRegion(Interval(-oo, oo)*Interval(-oo, 0))\n    right_half_plane = ComplexRegion(Interval(0, oo)*Interval(-oo, oo))\n    first_quad_plane = ComplexRegion(Interval(0, oo)*Interval(0, oo))\n\n    assert unit_disk.boundary == ComplexRegion(Interval(1, 1)*Interval(0, 2*S.Pi), polar=True)\n    assert upper_half_unit_disk.boundary == ComplexRegion(Interval(0, 1)*FiniteSet(0, S.Pi), polar=True)\n    assert upper_half_disk.boundary == ComplexRegion(Interval(0, oo)*FiniteSet(0, S.Pi), polar=True)\n    assert lower_half_disk.boundary == ComplexRegion(Interval(0, oo)*FiniteSet(S.Pi, 2*S.Pi), polar=True)\n    assert right_half_disk.boundary == ComplexRegion(Interval(0, oo)*FiniteSet(-S.Pi/2, S.Pi/2), polar=True)\n    assert first_quad_disk.boundary == ComplexRegion(Interval(0, oo)*FiniteSet(0, S.Pi/2), polar=True)\n\n    assert unit_square.boundary == ComplexRegion(Interval(-1, 1)*FiniteSet(-1, 1", "def test_Range_iterability():\n    r = Range(0, S.Infinity, 2)\n    assert r.is_iterable\n    it = iter(r)\n    assert next(it) == 0\n    assert next(it) == 2\n\n    r = Range(S.NegativeInfinity, 0, 2)\n    assert r.is_iterable\n    it = iter(r.reversed)\n    assert next(it) == -2\n    assert next(it) == -4\n\n    r = Range(0, S.Infinity, -2)\n    assert not r.is_iterable\n\n    r = Range(S.NegativeInfinity, 0, -2)\n    assert not r.is_iterable\n", "def test_ComplexRegion_bounding_box():\n    # issue 12827\n    unit_disk = ComplexRegion(Interval(0, 1)*Interval(0, 2*S.Pi), polar=True)\n    assert unit_disk.bounding_box == ComplexRegion(Interval(-1, 1)*Interval(-1, 1))\n", "def test_ComplexRegion_complement():\n    # Rectangular form\n    c1 = ComplexRegion(Interval(-2, 2)*Interval(-2, 2))\n    assert c1.complement() == ComplexRegion(Union(Interval(-oo, -2)*Interval(-oo, oo),\n                                        Interval(-2, 2)*Interval(-oo, -2),\n                                        Interval(-2, 2)*Interval(2, oo),\n                                        Interval(2, oo)*Interval(-oo, oo)))\n\n    # Polar form\n    c2 = ComplexRegion(Interval(1, 2)*Interval(0, 2*S.Pi), polar=True)\n    assert c2.complement() == ComplexRegion(Union(Interval(0, 1)*Interval(0, 2*S.Pi),\n                                        Interval(1, 2)*Interval(-S.Pi, 0),\n                                        Interval(1, 2)*Interval(2*S.Pi, 3*S.Pi),\n                                        Interval(2, oo)*Interval(0, 2*S.Pi)), polar=True)\n", "def test_ComplexRegion_ImageSet_intersection():\n    from sympy.abc import n, m\n    theta = Interval(0, 2*S.Pi)\n    r = Interval(1, 3)\n    z = Symbol('z')\n    L = Lambda(n, n)\n    c1 = ComplexRegion(r*theta, polar=True)\n    assert c1.intersect(imageset(L, z, S.Integers)) == S.EmptySet\n    assert imageset(L, z, S.Integers).intersect(c1) == S.EmptySet\n    L = Lambda(n, n + I)\n    assert c1.intersect(imageset(L, z, S.Integers)) == S.EmptySet\n    assert imageset(L, z, S.Integers).intersect(c1) == S.EmptySet\n    L = Lambda(n, n + 1 + I)\n    assert c1.intersect(imageset(L, z, S.Integers)) == S.EmptySet\n    assert imageset(L, z, S.Integers).intersect(c1) == S.EmptySet\n    L = Lambda(n, 2 + n*I)\n    assert c1.intersect(imageset(L, z, S.Integers)) == FiniteSet(2 + I)\n    assert imageset(L, z, S.Integers).intersect(c1) == FiniteSet(2 + I)\n    L = Lambda(n, 2 + n*I + 2)\n    assert c1.intersect(imageset(L, z, S.Integers)) == FiniteSet(2 + 2*I)\n    assert imageset(L, z, S.Integers).intersect(c1) == FiniteSet(2 + 2*I)\n    c2 = ComplexRegion(Union(r*theta, theta*r), polar=True)\n    assert c1.intersect(c2) == c1\n    assert c2.intersect(c1) == c1\n"], "sample_1191": ["def test_hermite_normal_edge_cases():\n    # Test 0x0 matrix\n    m = DomainMatrix([], (0, 0), ZZ)\n    hnf = DomainMatrix([], (0, 0), ZZ)\n    assert hermite_normal_form(m) == hnf\n\n    # Test 1x1 matrix\n    m = DomainMatrix([[2]], (1, 1), ZZ)\n    hnf = DomainMatrix([[2]], (1, 1), ZZ)\n    assert hermite_normal_form(m) == hnf\n\n    # Test 1x1 matrix with D\n    m = DomainMatrix([[2]], (1, 1), ZZ)\n    hnf = DomainMatrix([[2]], (1, 1), ZZ)\n    assert hermite_normal_form(m, D=ZZ(2)) == hnf\n\n    # Test 1x1 matrix with D and check_rank\n    m = DomainMatrix([[2]], (1, 1), ZZ)\n    hnf = DomainMatrix([[2]], (1, 1), ZZ)\n    assert hermite_normal_form(m, D=ZZ(2), check_rank=True) == hnf\n\n    # Test matrix with zero row\n    m = DomainMatrix([[1, 2], [0, 0]], ZZ)\n    hnf = DomainMatrix([[1, 0], [0, 0]], ZZ)\n    assert hermite_normal_form(m) == hnf\n\n    # Test matrix with zero column\n    m = DomainMatrix([[1, 0], [2, 0]], ZZ)\n    hnf = DomainMatrix([[1, 0], [0, 0]], ZZ)\n    assert hermite_normal_form(m) == hnf\n", "def test_invariant_factors():\n    m = DM([[ZZ(1), ZZ(0), ZZ(0), ZZ(0)],\n            [ZZ(0), ZZ(3), ZZ(0), ZZ(0)],\n            [ZZ(0), ZZ(0), ZZ(2), ZZ(0)],\n            [ZZ(0), ZZ(0), ZZ(0), ZZ(6)]], ZZ)\n    assert invariant_factors(m) == (1, 2, 3, 6)\n\n    m = DM([[ZZ(1), ZZ(0), ZZ(0)],\n            [ZZ(0), ZZ(3), ZZ(0)],\n            [ZZ(0), ZZ(0), ZZ(2)]], ZZ)\n    assert invariant_factors(m) == (1, 2, 3)\n\n    m = DM([[ZZ(2), ZZ(0), ZZ(0)],\n            [ZZ(0), ZZ(4), ZZ(0)],\n            [ZZ(0), ZZ(0), ZZ(8)]], ZZ)\n    assert invariant_factors(m) == (2, 4, 8)\n\n    m = DM([[ZZ(1), ZZ(0), ZZ(0)],\n            [ZZ(0), ZZ(1), ZZ(0)],\n            [ZZ(0), ZZ(0), ZZ(1)]], ZZ)\n    assert invariant_factors(m) == (1, 1, 1)\n\n    m = DM([[ZZ(0), ZZ(0), ZZ(0)],\n            [ZZ(0), ZZ(0), ZZ(0)],\n            [ZZ(0), ZZ(0), ZZ(0)]], ZZ)\n    assert invariant_factors(m) == ()\n\n    m = DM([[ZZ(1), ZZ(0)],\n            [ZZ(0), ZZ(1)],\n            [ZZ(0), ZZ(0)],\n            [ZZ(0), ZZ(0)]], ZZ)\n    assert invariant_factors(m) == (1, 1)\n", "def test_clear_column():\n    from sympy.polys.matrices.normalforms import clear_column\n\n    m = [[2, 0, 0], [4, 0, 0], [6, 0, 0]]\n    assert clear_column(m) == [[2, 0, 0], [0, 0, 0], [0, 0, 0]]\n\n    m = [[0, 2, 4], [0, 4, 8], [0, 6, 12]]\n    assert clear_column(m) == [[0, 2, 4], [0, 0, 0], [0, 0, 0]]\n\n    m = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n    assert clear_column(m) == [[1, 0, 0], [0, -1, -2], [0, -6, -6]]\n\n    m = [[0, 0, 1], [0, 0, 2], [0, 0, 3]]\n    assert clear_column(m) == [[0, 0, 1], [0, 0, 0], [0, 0, 0]]\n\n    m = [[0, 0, 0], [0, 0, 0], [0, 0, 0]]\n    assert clear_column(m) == [[0, 0, 0], [0, 0, 0], [0, 0, 0]]\n", "def test_clear_column():\n    m = [[1, 2, 3, 4], [2, 4, 6, 8], [3, 6, 9, 12]]\n    domain = ZZ\n    clear_column(m)\n    assert m[1][0] == 0\n    assert m[2][0] == 0\n    assert m[0][0] == 1\n\n    m = [[2, 4, 6, 8], [1, 2, 3, 4], [3, 6, 9, 12]]\n    clear_column(m)\n    assert m[0][0] == 0\n    assert m[2][0] == 0\n    assert m[1][0] == 1\n\n    m = [[0, 0, 0, 0], [0, 1, 2, 3], [0, 4, 6, 8]]\n    clear_column(m)\n    assert m[0][0] == 0\n    assert m[2][0] == 0\n    assert m[1][0] == 1\n\n    m = [[0, 1, 2, 3], [0, 4, 6, 8], [0, 7, 9, 12]]\n    clear_column(m)\n    assert m[0][0] == 0\n    assert m[2][0] == 0\n    assert m[1][0] == 1\n", "def test_invariant_factors():\n    m = DM([[2, 7, 17, 29, 41], [3, 11, 19, 31, 43], [5, 13, 23, 37, 47]], ZZ)\n    assert invariant_factors(m) == (1, 2, 1)\n\n    m = DM([[1, 0, 0], [0, 1, 0], [0, 0, 1]], ZZ)\n    assert invariant_factors(m) == (1, 1, 1)\n\n    m = DM([[2, 0, 0], [0, 2, 0], [0, 0, 2]], ZZ)\n    assert invariant_factors(m) == (2, 2, 2)\n\n    m = DM([[2, 7, 17, 29, 41], [3, 11, 19, 31, 43], [5, 13, 23, 37, 47]], ZZ)\n    assert invariant_factors(m) == (1, 2, 1)\n\n    m = DM([[2, 0, 0, 0], [0, 3, 0, 0], [0, 0, 0, 0], [0, 0, 0, 5]], ZZ)\n    assert invariant_factors(m) == (1, 6, 0, 5)\n\n    m = DM([[1, 0], [0, 0], [0, 1]], ZZ)\n    assert invariant_factors(m) == (1, 1)\n\n    m = DM([[0, 1], [1, 0], [0, 0]], ZZ)\n    assert invariant_factors(m) == (1, 1)\n"], "sample_198": ["    def setUpTestData(cls):\n        cls.example_inc = Company.objects.create(\n            name=\"Example Inc.\", num_employees=2300, num_chairs=5,\n            ceo=Employee.objects.create(firstname=\"Joe\", lastname=\"Smith\", salary=10)\n        )\n        cls.foobar_ltd = Company.objects.create(\n            name=\"Foobar Ltd.\", num_employees=3, num_chairs=4, based_in_eu=True,\n            ceo=Employee.objects.create(firstname=\"Frank\", lastname=\"Meyer\", salary=20)\n        )\n        cls.max = Employee.objects.create(firstname='Max', lastname='Mustermann', salary=30)\n        cls.gmbh = Company.objects.create(name='Test GmbH', num_employees=32, num_chairs=1, ceo=cls.max)\n", "    def test_row_range_frame(self):\n        RowRange(start=5, end=10)\n", "    def setUpTestData(cls):\n        cls.exp1 = Experiment.objects.create(\n            name='Experiment 1',\n            assigned=datetime.date(2015, 6, 25),\n            start=datetime.datetime(2015, 6, 25, 12, 15, 30, 747000),\n            end=datetime.datetime(2015, 6, 27, 12, 15, 30, 747000),\n            completed=datetime.date(2015, 6, 27),\n            estimated_time=datetime.timedelta(days=2),\n        )\n        cls.exp2 = Experiment.objects.create(\n            name='Experiment 2',\n            assigned=datetime.date(2015, 6, 25),\n            start=datetime.datetime(2015, 6, 25, 12, 15, 30, 747000),\n            end=datetime.datetime(2015, 6, 28, 12, 15, 30, 747000),\n            completed=datetime.date(2015, 6, 28),\n            estimated_time=datetime.timedelta(days=3),\n        )\n        cls.exp3 = Experiment.objects.create(\n            name='Experiment 3',\n            assigned=datetime.date(2015, 6, 25),\n            start=datetime.datetime(2015, 6, 25, 12, 15, 30, 747000),\n            end=datetime.datetime(2015, 6, 29, 12, 15, 30, 747000),\n            completed=datetime.date(2015, 6, 29),\n            estimated_time=datetime.timedelta(days=4),\n        )\n", "    def test_empty_expressionlist(self):\n        msg = 'ExpressionList requires at least one expression'\n        with self.assertRaisesMessage(ValueError, msg):\n            ExpressionList()\n", "    def setUpTestData(cls):\n        # Create some data for testing\n        cls.max = Employee.objects.create(firstname='Max', lastname='Mustermann', salary=30)\n        cls.foobar_ltd = Company.objects.create(name='Foobar Ltd.', num_employees=3, num_chairs=4, based_in_eu=True, ceo=cls.max)\n        cls.example_inc = Company.objects.create(name='Example Inc.', num_employees=2300, num_chairs=5, ceo=cls.max)\n"], "sample_48": ["def test_std_dev_aggregation(self):\n    vals = Book.objects.aggregate(std_dev=StdDev('price'))\n    self.assertIsInstance(vals['std_dev'], float)\n\n    vals = Book.objects.aggregate(std_dev=StdDev('price', sample=True))\n    self.assertIsInstance(vals['std_dev'], float)\n\n    vals = Book.objects.aggregate(std_dev=StdDev('rating'))\n    self.assertIsInstance(vals['std_dev'], float)\n\n    vals = Book.objects.aggregate(std_dev=StdDev('rating', sample=True))\n    self.assertIsInstance(vals['std_dev'], float)\n\n    vals = Book.objects.aggregate(std_dev=StdDev('price', sample=False, output_field=FloatField()))\n    self.assertIsInstance(vals['std_dev'], float)\n\n    vals = Book.objects.aggregate(std_dev=StdDev('rating', sample=False, output_field=FloatField()))\n    self.assertIsInstance(vals['std_dev'], float)\n\n    with self.assertRaisesMessage(TypeError, 'Complex aggregates require an alias'):\n        Book.objects.aggregate(StdDev('price', sample=True))\n\n    with self.assertRaisesMessage(FieldError, 'Cannot resolve expression type, unknown output_field'):\n        Book.objects.aggregate(std_dev=StdDev('price', sample=False))\n", "def test_aggregate_expression_on_aggregate(self):\n    \"\"\"Test that an aggregate expression can be used on another aggregate.\"\"\"\n    qs = Book.objects.annotate(\n        avg_rating=Avg('rating')\n    ).annotate(\n        double_avg_rating=Avg('avg_rating')\n    )\n    self.assertEqual(qs.first().double_avg_rating, Approximate(4.08, places=2))\n\n    qs = Book.objects.annotate(\n        avg_rating=Avg('rating')\n    ).annotate(\n        avg_rating_plus_one=Avg('avg_rating') + 1\n    )\n    self.assertEqual(qs.first().avg_rating_plus_one, Approximate(5.08, places=2))\n\n    qs = Book.objects.annotate(\n        avg_rating=Avg('rating')\n    ).annotate(\n        double_avg_rating=Avg('avg_rating') * 2\n    )\n    self.assertEqual(qs.first().double_avg_rating, Approximate(8.16, places=2))\n", "def test_stddev_sample_and_population(self):\n    authors = Author.objects.annotate(stddev_sample=StdDev('age', sample=True)).order_by('name')\n    authors2 = Author.objects.annotate(stddev_population=StdDev('age', sample=False)).order_by('name')\n    for qs in (authors, authors2):\n        self.assertQuerysetEqual(\n            qs, [\n                ('Adrian Holovaty', Approximate(9.165, places=1)),\n                ('Brad Dayley', None),\n                ('Jacob Kaplan-Moss', Approximate(3.5, places=1)),\n                ('James Bennett', Approximate(5.0, places=1)),\n                ('Jeffrey Forcier', Approximate(5.0, places=1)),\n                ('Paul Bissex', Approximate(5.0, places=1)),\n                ('Peter Norvig', Approximate(5.657, places=1)),\n                ('Stuart Russell', Approximate(5.657, places=1)),\n                ('Wesley J. Chun', Approximate(11.58, places=1)),\n            ],\n            lambda a: (a.name, a.stddev_sample if hasattr(a, 'stddev_sample') else a.stddev_population)\n        )\n", "def test_aggregate_over_annotate_with_function(self):\n    \"\"\"\n    Regression test for #30738. Aggregating over an annotation that uses a\n    database function expression should work correctly.\n    \"\"\"\n    authors = Author.objects.annotate(\n        upper_name=Func('name', function='UPPER')\n    ).aggregate(Max('upper_name'))\n\n    self.assertEqual(authors['upper_name__max'], 'WESLEY J. CHUN')\n", "    def test_stddev_population_and_sample(self):\n        # The population and sample std dev tests need to be separate\n        # since the same query is generated, but the result is different\n        # due to the different formulas used.\n        vals = Book.objects.aggregate(stddev=StdDev('rating'))\n        self.assertIsInstance(vals['stddev'], float)\n        self.assertGreater(vals['stddev'], 0.5)\n        self.assertLess(vals['stddev'], 0.8)\n\n        vals = Book.objects.aggregate(stddev=StdDev('rating', sample=False))\n        self.assertIsInstance(vals['stddev'], float)\n        self.assertGreater(vals['stddev'], 0.5)\n        self.assertLess(vals['stddev'], 0.8)\n\n        vals = Book.objects.aggregate(stddev=StdDev('rating', sample=True))\n        self.assertIsInstance(vals['stddev'], float)\n        self.assertGreater(vals['stddev'], 0.5)\n        self.assertLess(vals['stddev'], 0.8)\n"], "sample_278": ["    def test_path_info_creation(self):\n        path_info = PathInfo(\n            from_opts=self.example_inc._meta,\n            to_opts=self.example_inc.ceo._meta,\n            target_fields=[self.example_inc.ceo._meta.get_field('firstname')],\n            join_field=self.example_inc._meta.get_field('ceo'),\n            m2m=False,\n            direct=True,\n            filtered_relation=None,\n        )\n        self.assertEqual(path_info.from_opts, self.example_inc._meta)\n        self.assertEqual(path_info.to_opts, self.example_inc.ceo._meta)\n        self.assertEqual(path_info.target_fields, [self.example_inc.ceo._meta.get_field('firstname')])\n        self.assertEqual(path_info.join_field, self.example_inc._meta.get_field('ceo'))\n        self.assertFalse(path_info.m2m)\n        self.assertTrue(path_info.direct)\n        self.assertIsNone(path_info.filtered_relation)\n", "    def test_and(self):\n        q1 = Q(firstname='John')\n        q2 = Q(lastname='Doe')\n        q = q1 & q2\n        self.assertIsInstance(q, Q)\n        self.assertEqual(q.connector, Q.AND)\n        self.assertEqual(q.children, (q1, q2))\n", "    def test_empty_q_object(self):\n        q = Q()\n        self.assertEqual(len(q.children), 0)\n        self.assertEqual(q.connector, Q.AND)\n        self.assertFalse(q.negated)\n", "    def test_q_annotation(self):\n        qs = Company.objects.annotate(is_large=Q(num_employees__gt=100))\n        self.assertSequenceEqual(\n            qs.values_list('is_large', flat=True),\n            [True, False, False]\n        )\n", "    def test_q_negation(self):\n        companies = Company.objects.all()\n        self.assertEqual(\n            ~Q(num_employees__gt=3),\n            Q(conditional=False, children=[(~Q(num_employees__gt=3),)])\n        )\n        self.assertEqual(\n            ~(~Q(num_employees__gt=3)),\n            Q(conditional=True, children=[(~Q(num_employees__gt=3),)])\n        )\n        self.assertEqual(\n            ~Q(num_employees__gt=3) & Q(based_in_eu=True),\n            Q(conditional=True, children=[(~Q(num_employees__gt=3),), Q(based_in_eu=True,)])\n        )\n        self.assertEqual(\n            ~(Q(num_employees__gt=3) & Q(based_in_eu=True)),\n            Q(conditional=False, children=[Q(num_employees__gt=3,), Q(based_in_eu=True,)])\n        )\n        self.assertEqual(\n            ~Q(num_employees__gt=3) | Q(based_in_eu=True),\n            Q(conditional=True, children=[(~Q(num_employees__gt=3),), Q(based_in_eu=True,)])\n        )\n        self.assertEqual(\n            ~(Q(num_employees__gt=3) | Q(based_in_eu=True)),\n            Q(conditional=False, children=[Q(num_employees__gt=3,), Q(based_in_eu=True,)])\n        )\n"], "sample_902": ["def test_pipeline_nested_feature_union():\n    # Test that a pipeline with a nested feature union works as expected.\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    pca = PCA(n_components=2, svd_solver='randomized', random_state=0)\n    select = SelectKBest(k=1)\n    fu = FeatureUnion([(\"pca\", pca), (\"select\", select)])\n    pipe = Pipeline([('fu', fu), ('svc', SVC(random_state=0))])\n    pipe.fit(X, y)\n    pipe.predict(X)\n    pipe.predict_proba(X)\n    pipe.score(X, y)\n", "def test_feature_union_n_jobs_0():\n    # test that FeatureUnion works when n_jobs=0\n    X = JUNK_FOOD_DOCS\n\n    fs = FeatureUnion([\n        (\"words\", CountVectorizer(analyzer='word')),\n        (\"chars\", CountVectorizer(analyzer='char')),\n    ])\n\n    fs_parallel = FeatureUnion([\n        (\"words\", CountVectorizer(analyzer='word')),\n        (\"chars\", CountVectorizer(analyzer='char')),\n    ], n_jobs=0)\n\n    fs.fit(X)\n    X_transformed = fs.transform(X)\n    assert_equal(X_transformed.shape[0], len(X))\n\n    fs_parallel.fit(X)\n    X_transformed_parallel = fs_parallel.transform(X)\n    assert_equal(X_transformed.shape, X_transformed_parallel.shape)\n    assert_array_equal(\n        X_transformed.toarray(),\n        X_transformed_parallel.toarray()\n    )\n\n    # fit_transform should behave the same\n    X_transformed_parallel2 = fs_parallel.fit_transform(X)\n    assert_array_equal(\n        X_transformed.toarray(),\n        X_transformed_parallel2.toarray()\n    )\n\n    # transformers should stay fit after fit_transform\n    X_transformed_parallel2 = fs_parallel.transform(X)\n    assert_array_equal(\n        X_transformed.toarray(),\n        X_transformed_parallel2.toarray()\n    )\n", "def test_feature_union_with_empty_input():\n    # Test that FeatureUnion can handle empty input\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    pca = PCA(n_components=2, svd_solver='randomized', random_state=0)\n    select = SelectKBest(k=1)\n    fs = FeatureUnion([(\"pca\", pca), (\"select\", select)])\n    fs.fit(X[:0], y[:0])\n    assert_equal(fs.transform(X).shape, (X.shape[0], 3))\n", "def test_pipeline_with_feature_union():\n    # Test a pipeline with FeatureUnion\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    pca = PCA(n_components=2, svd_solver='randomized', random_state=0)\n    select = SelectKBest(k=1)\n    fs = FeatureUnion([(\"pca\", pca), (\"select\", select)])\n    pipe = Pipeline([('feat_union', fs), ('svc', SVC(random_state=0))])\n    pipe.fit(X, y)\n    pipe.predict(X)\n    pipe.score(X, y)\n    assert_equal(pipe.named_steps['feat_union'].transformer_weights, None)\n    assert_array_almost_equal(pipe.named_steps['feat_union'].transform(X),\n                              np.hstack((pca.fit_transform(X),\n                                         select.fit_transform(X, y))))\n", "def test_pipeline_fit_transform_error_propagation():\n    # Test that pipeline propagates errors raised by fit_transform.\n    pipe = Pipeline([('mock', Transf()), ('clf', FitParamT())])\n    pipe.fit(X=None, y=None, clf__should_succeed=False)\n    # classifier should return False\n    assert_false(pipe.predict(None))\n    # test that an error raised by fit_transform is propagated\n    assert_raise_message(\n        TypeError,\n        \"fit_transform() got an unexpected keyword argument 'bad'\",\n        pipe.fit_transform, None, None, clf__bad=True\n    )\n"], "sample_528": ["def test_use_multiple_styles():\n    mpl.rcParams[PARAM] = 'gray'\n    other_param = 'text.usetex'\n    other_value = True\n    with temp_style('test1', DUMMY_SETTINGS):\n        with temp_style('test2', {other_param: other_value}):\n            with style.context(['test1', 'test2']):\n                assert mpl.rcParams[PARAM] == VALUE\n                assert mpl.rcParams[other_param] == other_value\n    assert mpl.rcParams[PARAM] == 'gray'\n    assert mpl.rcParams[other_param] != other_value\n", "def test_use_invalid_style_name():\n    with pytest.raises(IOError) as excinfo:\n        style.use('non-existent-style')\n    assert \"not found in the style library and input is not a valid URL or path\" in str(excinfo.value)\n", "def test_remove_blacklisted_style_params():\n    style = {'backend': 'agg', 'image.cmap': 'pink'}\n    result = style.core._remove_blacklisted_style_params(style, warn=False)\n    assert result == {'image.cmap': 'pink'}\n    style = {'interactive': True, 'image.cmap': 'pink'}\n    result = style.core._remove_blacklisted_style_params(style, warn=False)\n    assert result == {'image.cmap': 'pink'}\n", "def test_remove_blacklisted_style_params():\n    original_dict = {'image.cmap': 'pink', 'interactive': True}\n    result_dict = style.core._remove_blacklisted_style_params(original_dict)\n    assert 'image.cmap' in result_dict\n    assert 'interactive' not in result_dict\n\n    result_dict = style.core._remove_blacklisted_style_params(original_dict, warn=False)\n    assert 'image.cmap' in result_dict\n    assert 'interactive' not in result_dict\n", "def test_update_user_library(tmpdir):\n    # Create a temporary directory with a custom style file.\n    temp_file = 'custom.%s' % STYLE_EXTENSION\n    path = Path(tmpdir, temp_file)\n    path.write_text('lines.linewidth: 10', encoding='utf-8')\n\n    # Add the temporary directory to the style path.\n    USER_LIBRARY_PATHS.append(tmpdir)\n    original_library = style.library.copy()\n    style.reload_library()\n\n    # Check if the custom style has been added to the library.\n    assert 'custom' in style.library\n\n    # Check if the custom style is correctly applied.\n    with style.context('custom'):\n        assert mpl.rcParams['lines.linewidth'] == 10\n\n    # Check if the original library has been updated.\n    assert style.library != original_library\n\n    # Remove the temporary directory from the style path.\n    USER_LIBRARY_PATHS.remove(tmpdir)\n    style.reload_library()\n    assert 'custom' not in style.library\n"], "sample_945": ["def test_parse_annotation_unsupported_syntax(app):\n    with pytest.raises(SyntaxError):\n        _parse_annotation(\"Unsupported[Syntax]\", app.env)\n", "def test_get_full_qualified_name_with_pending_xref(app, status, warning):\n    app.builder.build_all()\n\n    doctree = app.env.get_doctree('module')\n    refnode = list(doctree.traverse(pending_xref))[0]\n    domain = app.env.get_domain('py')\n\n    assert domain.get_full_qualified_name(refnode) == 'module_a.submodule.ModTopLevel'\n", "def test_python_domain_merge_domaindata(app):\n    text1 = (\".. py:function:: foo\\n\"\n             \".. py:class:: bar\\n\")\n    text2 = (\".. py:function:: baz\\n\"\n             \".. py:class:: qux\\n\")\n    restructuredtext.parse(app, text1)\n    restructuredtext.parse(app, text2)\n    domain = app.env.get_domain('py')\n    domaindata = domain.get_domaindata()\n    domain.clear_doc('index')\n    domain.merge_domaindata(['index'], domaindata)\n    assert domain.objects == {'foo': ('index', 'foo', 'function', False),\n                             'bar': ('index', 'bar', 'class', False)}\n    assert domain.modules == {}\n", "def test_type_to_xref(app):\n    domain = PythonDomain(Mock(), Mock())\n    text = \"List[str]\"\n    result = domain.type_to_xref(text)\n    assert_node(result, (pending_xref, ([pending_xref_condition, \"List\"],\n                                        [pending_xref_condition, \"List[str]\"])))\n\n    result = domain.type_to_xref(text, env=app.env)\n    assert_node(result, (pending_xref, ([pending_xref_condition, \"List\"],\n                                        [pending_xref_condition, \"List[str]\"])))\n\n    text = \"None\"\n    result = domain.type_to_xref(text, env=app.env)\n    assert_node(result, (pending_xref, ([pending_xref_condition, \"None\"])))\n    assert_node(result[0], pending_xref, refdomain=\"py\", reftype=\"obj\", reftarget=\"None\")\n", "def test_resolve_xref_for_nested_properties(app):\n    text = (\".. py:class:: Foo\\n\"\n            \"\\n\"\n            \"   .. py:property:: prop\\n\"\n            \"      :type: Optional[str]\\n\"\n            \"\\n\"\n            \"   .. py:property:: nested\\n\"\n            \"\\n\"\n            \"      .. py:property:: prop\\n\"\n            \"         :type: int\\n\")\n    app.env.config.python_use_unqualified_type_names = True\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Foo\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc,\n                                                  addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'prop (Foo property)', 'Foo.prop', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_annotation, \"property \"],\n                                                     [desc_name, \"prop\"],\n                                                     [desc_annotation, \": str\"])],\n                                   [desc_content, ()]))\n    assert_node(doctree[1][1][2], addnodes.index,\n                entries=[('single', 'prop (Foo.nested property)', 'Foo.nested.prop', '', None)])\n    assert_node(doctree[1][1][3], ([desc_signature, ([desc_annotation, \"property \"],\n                                                     [desc_name, \"prop\"],\n                                                     [desc_annotation, \": int\"])],\n                                   [desc_content, ()]))\n"], "sample_645": ["def test_log_format_multiline(caplog):\n    caplog.set_level(logging.INFO)\n    logger.info(\"boo %s\\nbaz %s\", \"arg1\", \"arg2\")\n    assert len(caplog.records) == 1\n    assert \"boo arg1\\nbaz arg2\" in caplog.text\n", "def test_log_capture_handler_reset(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n            logger = logging.getLogger('catchlog')\n            logger.error(\"ERROR message will be shown\")\n            caplog.handler.reset()\n            logger.error(\"ERROR message won't be shown\")\n            assert 'ERROR message will be shown' in caplog.text\n            assert 'ERROR message won\\'t be shown' not in caplog.text\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == 0\n", "def test_log_format_string_with_percent_styles(caplog):\n    caplog.set_level(logging.INFO)\n    logger.info(\"boo %s\", \"arg\")\n\n    assert \"boo arg\" == caplog.text.strip()\n    assert len(caplog.record_tuples) == 1\n    assert len(caplog.messages) == 1\n    assert caplog.records[0].msg == \"boo %s\"\n", "def test_caplog_captures_multiline_messages(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n            logger = logging.getLogger('catchlog')\n            logger.info('line 1\\\\nline 2\\\\nline 3')\n            logger.debug('line 1\\\\nline 2\\\\nline 3')\n\n            caplog.set_level(logging.INFO)\n\n            logger.info('line 1\\\\nline 2\\\\nline 3')\n            logger.debug('line 1\\\\nline 2\\\\nline 3')\n\n            assert len(caplog.messages) == 1\n            assert caplog.messages[0] == 'line 1\\nline 2\\nline 3'\n            assert len(caplog.records) == 1\n            assert caplog.records[0].levelname == 'INFO'\n            assert caplog.records[0].msg == 'line 1\\\\nline 2\\\\nline 3'\n    \"\"\"\n    )\n\n    result = pytester.runpytest()\n    assert result.ret == 0\n", "def test_caplog_handler_isnt_overwritten_by_manual_logging_config(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n        import pytest\n\n            logger = logging.getLogger('catchlog')\n            logger.setLevel(logging.DEBUG)\n\n            logger.warning(\"WARNING message will be shown\")\n\n            assert 'WARNING' in caplog.text\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n"], "sample_63": ["    def test_dirs_defaults_to_empty_list(self):\n        engine = Engine()\n        self.assertEqual(engine.dirs, [])\n", "    def test_engine_init(self):\n        engine = Engine(\n            dirs=[TEMPLATE_DIR],\n            app_dirs=True,\n            context_processors=['path.to.processor'],\n            debug=True,\n            loaders=['django.template.loaders.filesystem.Loader'],\n            string_if_invalid='INVALID',\n            file_charset='utf-8',\n            libraries={'lib': 'path.to.lib'},\n            builtins=['django.template.defaulttags'],\n            autoescape=False,\n        )\n        self.assertEqual(engine.dirs, [TEMPLATE_DIR])\n        self.assertTrue(engine.app_dirs)\n        self.assertEqual(engine.context_processors, ['path.to.processor'])\n        self.assertTrue(engine.debug)\n        self.assertEqual(engine.loaders, ['django.template.loaders.filesystem.Loader'])\n        self.assertEqual(engine.string_if_invalid, 'INVALID')\n        self.assertEqual(engine.file_charset, 'utf-8')\n        self.assertEqual(engine.libraries, {'lib': 'path.to.lib'})\n        self.assertEqual(engine.builtins, ['django.template.defaulttags', 'django.template.defaultfilters', 'django.template.loader_tags'])\n        self.assertFalse(engine.autoescape)\n", "    def test_dirs_none(self):\n        engine = Engine(dirs=None)\n        self.assertEqual(engine.dirs, [])\n", "    def test_init_with_invalid_loaders(self):\n        with self.assertRaisesMessage(ImproperlyConfigured, \"app_dirs must not be set when loaders is defined.\"):\n            Engine(loaders=['django.template.loaders.filesystem.Loader'], app_dirs=True)\n", "    def test_init_dirs_is_none(self):\n        engine = Engine()\n        self.assertEqual(engine.dirs, [])\n"], "sample_739": ["def test_label_binarize_multiclass_sparse_input():\n    # Test that label_binarize works correctly with sparse multiclass input\n    y = csr_matrix([[0, 1, 0], [0, 0, 1], [1, 0, 0]])\n    classes = [0, 1, 2]\n    pos_label = 1\n    neg_label = 0\n    expected = np.array([[0, 1, 0], [0, 0, 1], [1, 0, 0]])\n    \n    for sparse_output in [True, False]:\n        binarized = label_binarize(y, classes, neg_label=neg_label,\n                                   pos_label=pos_label,\n                                   sparse_output=sparse_output)\n        assert_array_equal(toarray(binarized), expected)\n        assert_equal(issparse(binarized), sparse_output)\n\n    # Check inverse\n    y_type = type_of_target(y)\n    inversed = _inverse_binarize_multiclass(binarized, classes=classes)\n    assert_array_equal(toarray(inversed), toarray(y))\n\n    # Check label binarizer\n    lb = LabelBinarizer(neg_label=neg_label, pos_label=pos_label,\n                        sparse_output=sparse_output)\n    binarized = lb.fit_transform(y)\n    assert_array_equal(toarray(binarized), expected)\n    assert_equal(issparse(binarized), sparse_output)\n    inverse_output = lb.inverse_transform(binarized)\n    assert_array_equal(toarray(inverse_output), toarray(y))\n    assert_equal(issparse(inverse_output), issparse(y))\n", "def test_multilabel_binarizer_single_label():\n    # Ensure a single label set is not interpreted as a multilabel sequence\n    inp = [[1]]\n    indicator_mat = np.array([[1]])\n    mlb = MultiLabelBinarizer()\n    assert_array_equal(mlb.fit_transform(inp), indicator_mat)\n    assert_array_equal(mlb.inverse_transform(indicator_mat), inp)\n\n    # Ensure a single label set with multiple elements is not interpreted as\n    # a multilabel sequence\n    inp = [[1, 2]]\n    indicator_mat = np.array([[1, 1]])\n    mlb = MultiLabelBinarizer()\n    assert_array_equal(mlb.fit_transform(inp), indicator_mat)\n    assert_array_equal(mlb.inverse_transform(indicator_mat), inp)\n", "def test_label_binarize_multilabel_binarizer():\n    mlb = MultiLabelBinarizer()\n    y = [(1, 2), (3,), (1, 2)]\n    y_classes = [1, 2, 3]\n    mlb.fit(y)\n    y_ind = mlb.transform(y)\n    lb = LabelBinarizer()\n    y_bin = lb.fit_transform(y_ind)\n    assert_array_equal(y_ind, y_bin.toarray())\n\n    y_inv = lb.inverse_transform(y_bin)\n    y_inv = mlb.inverse_transform(y_inv)\n    assert_array_equal(y, y_inv)\n", "def test_label_binarizer_multiclass_sparse_inverse_transform():\n    # Check that inverse_transform with sparse matrix works correctly for\n    # multiclass case.\n    lb = LabelBinarizer()\n    y = [1, 2, 0, 1]\n    y_sparse = csr_matrix(lb.fit_transform(y))\n    assert_array_equal(lb.inverse_transform(y_sparse), y)\n\n    # Check with different label order\n    y = [1, 2, 0, 1]\n    lb = LabelBinarizer()\n    lb.fit(y)\n    y_sparse = csr_matrix(lb.transform(y))\n    assert_array_equal(lb.inverse_transform(y_sparse), y)\n\n    # Check with different label order and one-class\n    y = [1, 1, 1, 1]\n    lb = LabelBinarizer()\n    lb.fit(y)\n    y_sparse = csr_matrix(lb.transform(y))\n    assert_array_equal(lb.inverse_transform(y_sparse), y)\n", "def test_multilabel_binarizer_custom_order():\n    inp = [(2, 3), (1,), (1, 2)]\n    mlb = MultiLabelBinarizer(classes=[2, 3, 1])\n    assert_array_equal(mlb.fit_transform(inp), np.array([[1, 1, 0],\n                                                         [0, 0, 1],\n                                                         [0, 1, 1]]))\n    assert_array_equal(mlb.inverse_transform(np.array([[1, 1, 0],\n                                                       [0, 0, 1],\n                                                       [0, 1, 1]])), inp)\n\n"], "sample_413": ["    def setUpClass(cls):\n        super().setUpClass()\n        cls.error_same_tags = Error(\n            E003.msg.format(\n                \"'same_tags'\",\n                \"'check_framework.template_test_apps.same_tags_app_1.\"\n                \"templatetags.same_tags', \"\n                \"'check_framework.template_test_apps.same_tags_app_2.\"\n                \"templatetags.same_tags'\",\n            ),\n            id=E003.id,\n        )\n", "    def setUpClass(cls):\n        super().setUpClass()\n        cls.error_same_tags = Error(\n            E003.msg.format(\n                \"'same_tags'\",\n                \"'check_framework.template_test_apps.same_tags_app_1.\"\n                \"templatetags.same_tags', \"\n                \"'check_framework.template_test_apps.same_tags_app_2.\"\n                \"templatetags.same_tags', \"\n                \"'check_framework.template_test_apps.same_tags_app_3.\"\n                \"templatetags.same_tags'\",\n            ),\n            id=E003.id,\n        )\n", "    def setUpClass(cls):\n        super().setUpClass()\n        cls.error_same_tags = Error(\n            E003.msg.format(\n                \"'same_tags'\",\n                \"'check_framework.template_test_apps.same_tags_app_1.\"\n                \"templatetags.same_tags', \"\n                \"'check_framework.template_test_apps.same_tags_app_2.\"\n                \"templatetags.same_tags', \"\n                \"'check_framework.template_test_apps.same_tags_app_3.\"\n                \"templatetags.same_tags'\",\n            ),\n            id=E003.id,\n        )\n", "    def setUpClass(cls):\n        super().setUpClass()\n        cls.error_same_tags = Error(\n            E003.msg.format(\n                \"'same_tags'\",\n                \"'check_framework.template_test_apps.same_tags_app_1.\"\n                \"templatetags.same_tags', \"\n                \"'check_framework.template_test_apps.same_tags_app_2.\"\n                \"templatetags.same_tags', \"\n                \"'check_framework.template_test_apps.same_tags_app_3.\"\n                \"templatetags.same_tags'\",\n            ),\n            id=E003.id,\n        )\n", "    def setUpClass(cls):\n        super().setUpClass()\n        cls.error_same_tags = Error(\n            E003.msg.format(\n                \"'same_tags'\",\n                \"'check_framework.template_test_apps.same_tags_app_1.\"\n                \"templatetags.same_tags', \"\n                \"'check_framework.template_test_apps.same_tags_app_2.\"\n                \"templatetags.same_tags'\",\n            ),\n            id=E003.id,\n        )\n"], "sample_299": ["    def test_multiple_absolute_paths(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': pathlib.Path.cwd() / 'cache',\n            },\n            'other': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': pathlib.Path.cwd() / 'other_cache',\n            },\n        }):\n            self.assertEqual(check_file_based_cache_is_absolute(None), [])\n", "    def test_none_location(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': None,\n            },\n        }):\n            self.assertEqual(check_file_based_cache_is_absolute(None), [\n                Warning(\n                    \"Your 'default' cache LOCATION path is relative. Use an \"\n                    \"absolute path instead.\",\n                    id='caches.W003',\n                ),\n            ])\n", "    def test_mixed_backends(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.locmem.LocMemCache',\n            },\n            'filebased': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': pathlib.Path.cwd() / 'cache',\n            },\n            'db': {\n                'BACKEND': 'django.core.cache.backends.db.DatabaseCache',\n                'LOCATION': 'my_cache_table',\n            },\n            'relative_filebased': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': 'relative_cache',\n            },\n        }):\n            self.assertEqual(check_file_based_cache_is_absolute(None), [\n                Warning(\n                    \"Your 'relative_filebased' cache LOCATION path is relative. Use an absolute path instead.\",\n                    id='caches.W003',\n                ),\n            ])\n", "    def test_mixed_paths(self):\n        caches = {\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': pathlib.Path.cwd() / 'cache',\n            },\n            'other': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': 'relative_path',\n            },\n            'memcached': {\n                'BACKEND': 'django.core.cache.backends.locmem.LocMemCache',\n            },\n        }\n        with self.settings(CACHES=caches):\n            self.assertEqual(check_file_based_cache_is_absolute(None), [\n                Warning(\n                    \"Your 'other' cache LOCATION path is relative. Use an absolute path instead.\",\n                    id='caches.W003',\n                ),\n            ])\n", "    def test_multiple_cache_configurations(self):\n        cache_configs = {\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': pathlib.Path.cwd() / 'cache',\n            },\n            'other': {\n                'BACKEND': 'django.core.cache.backends.locmem.LocMemCache',\n            },\n            'third': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': pathlib.Path.cwd() / 'other',\n            },\n        }\n        with self.settings(CACHES=cache_configs):\n            self.assertEqual(check_file_based_cache_is_absolute(None), [])\n"], "sample_503": ["def test_marker_edge_color_auto(fig_test, fig_ref):\n    fig_test.add_subplot().plot([0, 1], 'bo', markeredgecolor='auto')\n    fig_ref.add_subplot().plot([0, 1], 'bo', markeredgecolor='b')\n", "def test_line2d_update_from():\n    \"\"\"Test the update_from method of Line2D.\"\"\"\n    line1 = mlines.Line2D([0, 1], [0, 1], linestyle='--', color='r', marker='o')\n    line2 = mlines.Line2D([0, 1], [0, 1], linestyle='-', color='b', marker='s')\n\n    line2.update_from(line1)\n\n    assert line2.get_linestyle() == line1.get_linestyle()\n    assert line2.get_color() == line1.get_color()\n    assert line2.get_marker() == line1.get_marker()\n    assert line2.get_markerfacecolor() == line1.get_markerfacecolor()\n    assert line2.get_markeredgecolor() == line1.get_markeredgecolor()\n    assert line2.get_markeredgewidth() == line1.get_markeredgewidth()\n    assert line2.get_linewidth() == line1.get_linewidth()\n", "def test_line_transform(fig_test, fig_ref):\n    t = np.arange(0, 2 * np.pi, 0.1)\n    fig_test.add_subplot().plot(t, np.sin(t), transform=matplotlib.transforms.Affine2D().rotate_deg(45) + fig_test.axes[0].transData)\n    fig_ref.add_subplot().plot(t, np.sin(t), transform=matplotlib.transforms.Affine2D().rotate_deg(45) + fig_ref.axes[0].transData)\n", "def test_subslice_invalidated_on_transform_change():\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 1, 2000)\n    y = np.sin(x)\n    line, = ax.plot(x, y)\n    assert line._subslice\n\n    # Trigger a recache with a new transform\n    line.set_transform(mtransforms.Affine2D().rotate_deg(45) + ax.transData)\n    assert not line._subslice\n", "def test_line2d_with_none_data():\n    \"\"\"\n    Test that a Line2D can be created with None as x and y data.\n    \"\"\"\n    line = mlines.Line2D(None, None)\n    assert line.get_xdata() is None\n    assert line.get_ydata() is None\n    assert_array_equal(line.get_data(), (None, None))\n    assert line.get_path().vertices.size == 0\n\n    # Test that setting data works\n    line.set_data([1, 2, 3], [4, 5, 6])\n    assert_array_equal(line.get_data(), ([1, 2, 3], [4, 5, 6]))\n    assert line.get_path().vertices.shape == (3, 2)\n\n    # Test that setting None as data again works\n    line.set_data(None, None)\n    assert line.get_xdata() is None\n    assert line.get_ydata() is None\n    assert_array_equal(line.get_data(), (None, None))\n    assert line.get_path().vertices.size == 0\n"], "sample_787": ["def test_multilabel_confusion_matrix_multiclass_with_sample_weight():\n    # Test multilabel confusion matrix - multi-class case with sample weights\n    y_true, y_pred, _ = make_prediction(binary=False)\n\n        sample_weight = np.ones(len(y_true))\n        cm = multilabel_confusion_matrix(y_true, y_pred, sample_weight=sample_weight)\n        assert_array_equal(cm, [[[47, 4], [5, 19]],\n                                [[38, 6], [28, 3]],\n                                [[30, 25], [2, 18]]])\n\n    test(y_true, y_pred)\n    test(list(str(y) for y in y_true),\n         list(str(y) for y in y_pred))\n", "def test_precision_recall_f1_score_multilabel_3():\n    # Test precision_recall_f1_score on a crafted multilabel example 3\n    # Third crafted example\n    y_true = np.array([[1, 1, 1, 0], [0, 1, 1, 0], [0, 0, 0, 0]])\n    y_pred = np.array([[0, 1, 1, 0], [0, 1, 1, 0], [1, 0, 0, 0]])\n\n    # tp = [ 0.,  2.,  2.,  0.]\n    # fp = [ 1.,  0.,  0.,  0.]\n    # fn = [ 1.,  0.,  0.,  0.]\n\n    p, r, f, s = precision_recall_fscore_support(y_true, y_pred,\n                                                 average=None)\n    assert_array_almost_equal(p, [0.0, 1.0, 1.0, 0.0], 2)\n    assert_array_almost_equal(r, [0.0, 1.0, 1.0, 0.0], 2)\n    assert_array_almost_equal(f, [0.0, 1.0, 1.0, 0.0], 2)\n    assert_array_almost_equal(s, [1, 2, 2, 0], 2)\n\n    # Check macro\n    p, r, f, s = precision_recall_fscore_support(y_true, y_pred,\n                                                 average=\"macro\")\n    assert_almost_equal(p, 2 / 3)\n    assert_almost_equal(r, 2 / 3)\n    assert_almost_equal(f, 2 / 3)\n    assert_equal(s, None)\n\n    # Check micro\n    p, r, f, s = precision_recall_fscore_support(y_true, y_pred,\n                                                 average=\"micro\")\n    assert_almost_equal(p, 2 / 3)\n    assert_almost_equal(r, 2 / 3)\n    assert_almost_equal(f, 2 / 3)\n    assert_equal(s, None)\n\n    # Check weighted\n    p, r, f, s = precision_recall_fscore_support(y_true, y_pred,\n                                                 average=\"weighted\")\n    assert_almost_equal(p, 2 / 3)\n    assert", "def test_check_targets_y_type_multilabel():\n    y_true = np.array([[1, 0], [0, 1]])\n    y_pred = np.array([[1, 1], [1, 1]])\n\n    # Test that y_type is correctly determined as multilabel\n    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n    assert y_type == 'multilabel-indicator'\n\n    # Test that y_type is correctly determined as multilabel even if\n    # y_pred is a dense array\n    y_pred = np.array([[1, 1], [1, 1]], dtype=int)\n    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n    assert y_type == 'multilabel-indicator'\n", "def test_multilabel_confusion_matrix_multilabel_input_shapes():\n    y_true = np.array([[0, 1, 1], [1, 0, 1], [1, 1, 0], [1, 1, 1]])\n    y_pred = np.array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [0, 0, 1]])\n\n    # Test input shape (n_samples, n_labels)\n    y_true_2d = y_true\n    y_pred_2d = y_pred\n    cm = multilabel_confusion_matrix(y_true_2d, y_pred_2d)\n    assert cm.shape == (y_true.shape[1], 2, 2)\n\n    # Test input shape (n_samples, n_labels, 1)\n    y_true_3d = y_true[:, :, np.newaxis]\n    y_pred_3d = y_pred[:, :, np.newaxis]\n    cm = multilabel_confusion_matrix(y_true_3d, y_pred_3d)\n    assert cm.shape == (y_true.shape[1], 2, 2)\n\n    # Test input shape (1, n_samples, n_labels)\n    y_true_3d = y_true[np.newaxis, :, :]\n    y_pred_3d = y_pred[np.newaxis, :, :]\n    cm = multilabel_confusion_matrix(y_true_3d, y_pred_3d)\n    assert cm.shape == (y_true.shape[1], 2, 2)\n\n    # Test input shape (n_samples, 1, n_labels)\n    y_true_3d = y_true[:, np.newaxis, :]\n    y_pred_3d = y_pred[:, np.newaxis, :]\n    cm = multilabel_confusion_matrix(y_true_3d, y_pred_3d)\n    assert cm.shape == (y_true.shape[1], 2, 2)\n", "def test_confusion_matrix_sparse_labels():\n    # Test confusion matrix - binary classification case\n    y_true = np.array([0, 0, 1, 1])\n    y_pred = np.array([0, 1, 1, 1])\n    cm = confusion_matrix(y_true, y_pred)\n    assert_array_equal(cm, [[1, 1], [0, 2]])\n\n    # test with sparse labels\n    y_true_sparse = csr_matrix(y_true[:, np.newaxis])\n    y_pred_sparse = csr_matrix(y_pred[:, np.newaxis])\n    cm_sparse = confusion_matrix(y_true_sparse, y_pred_sparse)\n    assert_array_equal(cm_sparse, cm)\n\n    # test with different types of sparse labels\n    y_true_csr = csr_matrix(y_true)\n    y_true_csc = csc_matrix(y_true)\n    y_true_lil = y_true.tolil()\n    y_true_dok = y_true.todok()\n    y_pred_csr = csr_matrix(y_pred)\n    y_pred_csc = csc_matrix(y_pred)\n    y_pred_lil = y_pred.tolil()\n    y_pred_dok = y_pred.todok()\n    cms = [\n        confusion_matrix(y_true_csr, y_pred_csr),\n        confusion_matrix(y_true_csc, y_pred_csc),\n        confusion_matrix(y_true_lil, y_pred_lil),\n        confusion_matrix(y_true_dok, y_pred_dok),\n        confusion_matrix(y_true_csr, y_pred_csc),\n        confusion_matrix(y_true_csc, y_pred_csr),\n        confusion_matrix(y_true_csr, y_pred_lil),\n        confusion_matrix(y_true_csr, y_pred_dok),\n        confusion_matrix(y_true_lil, y_pred_csr),\n        confusion_matrix(y_true_lil, y_pred_csc),\n        confusion_matrix(y_true_lil, y_pred_lil),\n        confusion_matrix(y_true_lil, y_pred_dok),\n        confusion_matrix(y_true_dok, y_pred_csr),\n        confusion_matrix(y_true_dok, y_pred_csc),\n        confusion_matrix(y_true_dok, y_pred_lil),\n        confusion_matrix(y_true_dok, y_pred_dok),\n    ]\n    for cm_sparse in cms:\n        assert_array_equal(cm_sparse, cm)\n"], "sample_304": ["    def test_ip_address_validators(self):\n        self.assertEqual(ip_address_validators('both', False), ([validate_ipv46_address], _('Enter a valid IPv4 or IPv6 address.')))\n        self.assertEqual(ip_address_validators('ipv4', False), ([validate_ipv4_address], _('Enter a valid IPv4 address.')))\n        self.assertEqual(ip_address_validators('ipv6', False), ([validate_ipv6_address], _('Enter a valid IPv6 address.')))\n        with self.assertRaises(ValueError):\n            ip_address_validators('invalid', False)\n        with self.assertRaises(ValueError):\n            ip_address_validators('both', True)\n", "    def test_flags(self):\n        pattern = 'a'\n        self.assertEqual(\n            RegexValidator(pattern, flags=re.IGNORECASE),\n            RegexValidator(pattern, flags=re.IGNORECASE),\n        )\n        self.assertNotEqual(\n            RegexValidator(pattern, flags=re.IGNORECASE),\n            RegexValidator(pattern),\n        )\n        self.assertNotEqual(\n            RegexValidator(pattern),\n            RegexValidator(pattern, flags=re.IGNORECASE),\n        )\n", "    def test_regex_validator_inverse_match(self):\n        v = RegexValidator(regex='^a', inverse_match=True)\n        with self.assertRaises(ValidationError):\n            v('a')\n        self.assertIsNone(v('b'))\n", "    def test_compare_method(self):\n        validator = BaseValidator(10)\n        self.assertTrue(validator.compare(10, 10))  # Test equal values\n        self.assertFalse(validator.compare(5, 10))  # Test less than\n        self.assertFalse(validator.compare(15, 10))  # Test greater than\n", "    def test_domain_length_validation(self):\n        validator = URLValidator(schemes=['http'])\n        with self.assertRaisesMessage(ValidationError, 'Enter a valid URL.'):\n            validator('http://a.' + 'a' * 255 + '.com/')\n"], "sample_412": ["def test_avoid_wrapping(self):\n    self.assertEqual(avoid_wrapping(\"Hello World\"), \"Hello\\u00A0World\")\n    self.assertEqual(avoid_wrapping(\"Hello   World\"), \"Hello\\u00A0\\u00A0\\u00A0World\")\n    self.assertEqual(avoid_wrapping(\"   Hello   World   \"), \"\\u00A0\\u00A0\\u00A0Hello\\u00A0\\u00A0\\u00A0World\\u00A0\\u00A0\\u00A0\")\n    self.assertEqual(avoid_wrapping(\"Hello\\u00A0World\"), \"Hello\\u00A0World\")\n    self.assertEqual(avoid_wrapping(\"Hello\\tWorld\"), \"Hello\\tWorld\")\n", "def test_urlize_edge_cases(self):\n    tests = (\n        # nofollow=True\n        (\n            \"Search for google.com/?q=! and see.\",\n            'Search for <a href=\"http://google.com/?q=\" rel=\"nofollow\">google.com/?q=</a>! and see.',\n        ),\n        # autoescape=True\n        (\n            \"<script>alert('XSS')</script> Search for google.com/?q=! and see.\",\n            '&lt;script&gt;alert(&#x27;XSS&#x27;)&lt;/script&gt; Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>! and see.',\n        ),\n        # trim_url_limit\n        (\n            \"Search for https://www.example.com/this-is-a-really-long-url-that-needs-to-be-truncated/ and see.\",\n            'Search for <a href=\"https://www.example.com/this-is-a-really-long-url-that-needs-to-be-truncated/\">https://www.example.com/this-is-a-really-long-url-that-needs-to-be-truncated/\u2026</a> and see.',\n        ),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value, trim_url_limit=50, nofollow=True, autoescape=True), output)\n", "def test_urlizer(self):\n    urlizer = Urlizer()\n    tests = (\n        (\n            \"Search for google.com/?q=! and see.\",\n            'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>! and '\n            \"see.\",\n        ),\n        (\n            \"Search for google.com/?q=1&lt! and see.\",\n            'Search for <a href=\"http://google.com/?q=1%3C\">google.com/?q=1&lt'\n            \"</a>! and see.\",\n        ),\n        (\n            \"foo@example.com\",\n            '<a href=\"mailto:foo@example.com\">foo@example.com</a>',\n        ),\n        (\n            \"Search for (http://example.com/) and see.\",\n            'Search for (<a href=\"http://example.com/\">http://example.com/</a>) '\n            \"and see.\",\n        ),\n        (\n            \"Search for [http://example.com/] and see.\",\n            'Search for [<a href=\"http://example.com/\">http://example.com/</a>] '\n            \"and see.\",\n        ),\n        (\n            \"Search for www.example.com and see.\",\n            'Search for <a href=\"http://www.example.com\">www.example.com</a> and '\n            \"see.\",\n        ),\n        (\n            \"Search for www.example.com? and see.\",\n            'Search for <a href=\"http://www.example.com\">www.example.com</a>? and '\n            \"see.\",\n        ),\n        (\n            \"Search for www.example.com, and see.\",\n            'Search for <a href=\"http://www.example.com\">www.example.com</a>, and '\n            \"see.\",\n        ),\n        (\n            \"Search for www.example.com! and see.\",\n            'Search for <a href=\"http://www.example.com\">www.example.com</a>! and '\n            \"see.\",\n        ),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlizer(value), output)\n\n    tests = (\n        (\"(http://example.com/)\", '(<a href=\"http://example.com/\">http://example.com/</a>)'),\n        (\"[http://example.com/]\", '[<a href=\"http://example.com/\">http://example.com/</a>]'),\n        (\"(www.example.com)\", '(<a href=\"http://www.example.com\">www.example.com</a>)", "def test_urlizer(self):\n    urlizer = Urlizer()\n    tests = (\n        (\n            \"Visit https://www.google.com for more information.\",\n            'Visit <a href=\"https://www.google.com\">https://www.google.com</a> for more information.',\n        ),\n        (\n            \"Visit www.google.com for more information.\",\n            'Visit <a href=\"http://www.google.com\">www.google.com</a> for more information.',\n        ),\n        (\n            \"Visit google.com for more information.\",\n            'Visit <a href=\"http://google.com\">google.com</a> for more information.',\n        ),\n        (\n            \"Visit google.com/?q=python for more information.\",\n            'Visit <a href=\"http://google.com/?q=python\">google.com/?q=python</a> for more information.',\n        ),\n        (\n            \"Visit google.com/?q=python&foo=bar for more information.\",\n            'Visit <a href=\"http://google.com/?q=python&amp;foo=bar\">google.com/?q=python&amp;foo=bar</a> for more information.',\n        ),\n        (\n            \"Visit google.com for more information (foo@example.com).\",\n            'Visit <a href=\"http://google.com\">google.com</a> for more information (<a href=\"mailto:foo@example.com\">foo@example.com</a>).',\n        ),\n        (\n            \"Visit (foo@example.com) google.com for more information.\",\n            '(<a href=\"mailto:foo@example.com\">foo@example.com</a>) <a href=\"http://google.com\">google.com</a> for more information.',\n        ),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlizer(value), output)\n\n    # Test nofollow\n    self.assertEqual(\n        urlizer(\"Visit https://www.google.com for more information.\", nofollow=True),\n        'Visit <a href=\"https://www.google.com\" rel=\"nofollow\">https://www.google.com</a> for more information.',\n    )\n\n    # Test autoescape\n    self.assertEqual(\n        urlizer(\"Visit <script>alert('XSS')</script> https://www.google.com for more information.\", autoescape=True),\n        'Visit <a href=\"https://www.google.com\">https://www.google.com</a> for more information.',\n    )\n\n    # Test trim_url_limit\n    self.assertEqual(\n        urlizer(\"", "    def test_urlize_with_trailing_punctuation(self):\n        tests = (\n            (\"Search for google.com. and see.\", 'Search for <a href=\"http://google.com\">google.com</a>. and see.'),\n            (\"Search for google.com, and see.\", 'Search for <a href=\"http://google.com\">google.com</a>, and see.'),\n            (\"Search for google.com! and see.\", 'Search for <a href=\"http://google.com\">google.com</a>! and see.'),\n            (\"Search for google.com? and see.\", 'Search for <a href=\"http://google.com\">google.com</a>? and see.'),\n        )\n        for value, output in tests:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value), output)\n"], "sample_348": ["    def test_model_form_save_with_unbound_form(self):\n        class TestModelForm(forms.ModelForm):\n            class Meta:\n                model = ValidationTestModel\n                fields = ('name',)\n\n        form = TestModelForm()\n        with self.assertRaises(ValueError):\n            form.save()\n", "    def test_model_form_custom_formfield_callback(self):\n        class BandForm(forms.ModelForm):\n            class Meta:\n                model = Band\n                fields = ('name', 'bio', 'sign_date')\n\n        class BandModelForm(forms.ModelForm):\n            class Meta:\n                model = Band\n                fields = ('name', 'bio', 'sign_date')\n\n            if field.name == 'bio':\n                kwargs['widget'] = forms.Textarea()\n            return field.formfield(**kwargs)\n\n        form = modelform_factory(Band, form=BandModelForm, formfield_callback=custom_formfield_callback)\n        self.assertIsInstance(form.base_fields['bio'].widget, forms.Textarea)\n        self.assertIsInstance(form.base_fields['name'].widget, forms.TextInput)\n        self.assertIsInstance(form.base_fields['sign_date'].widget, forms.DateTimeInput)\n", "    def test_model_form_mixin_not_properly_configured(self):\n        class TestModelForm(ModelForm):\n            class Meta:\n                model = ValidationTestModel\n                fields = ('name',)\n\n        class TestModelForm2(TestModelForm):\n            class Meta(TestModelForm.Meta):\n                pass\n\n        self.assertIsValid(TestModelForm, ValidationTestModel)\n        self.assertIsInvalid(\n            TestModelForm2, ValidationTestModel,\n            \"Creating a ModelForm without either the 'fields' attribute or the \"\n            \"'exclude' attribute is prohibited; form TestModelForm2 needs updating.\",\n            'admin.E005',\n        )\n", "    def test_save_m2m_called_after_save(self):\n        class MyModel(Model):\n            field = Field()\n\n        class MyForm(ModelForm):\n            class Meta:\n                model = MyModel\n                fields = ['field']\n\n        instance = MyModel()\n        form = MyForm({'field': 'value'}, instance=instance)\n        self.assertTrue(form.is_valid())\n        saved_instance = form.save(commit=False)\n        self.assertFalse(hasattr(form, 'save_m2m'))\n        saved_instance.save()\n        self.assertTrue(hasattr(form, 'save_m2m'))\n        form.save_m2m()\n", "    def test_model_form_metaclass_new(self):\n        class TestForm(ModelForm):\n            class Meta:\n                model = ValidationTestModel\n                fields = ('name',)\n\n        self.assertIsInstance(TestForm, ModelFormMetaclass)\n\n        class TestFormNoModel(ModelForm):\n            pass\n\n        with self.assertRaises(ImproperlyConfigured):\n            class TestFormNoFieldsNorExclude(ModelForm):\n                class Meta:\n                    model = ValidationTestModel\n"], "sample_649": ["def test_log_disabling_works_with_log_file(testdir):\n    testdir.makepyfile(\n        \"\"\"\n    import logging\n    disabled_log = logging.getLogger('disabled')\n    test_log = logging.getLogger('test')\n\n        test_log.info(\"Visible text!\")\n        disabled_log.warning(\"This string will be suppressed.\")\n    \"\"\"\n    )\n    result = testdir.runpytest(\n        \"--log-file=log.txt\",\n        \"--log-disable=disabled\",\n    )\n    assert result.ret == ExitCode.OK\n    assert not result.stderr.lines\n    with open(\"log.txt\") as f:\n        log_text = f.read()\n        assert \"Visible text!\" in log_text\n        assert \"This string will be suppressed.\" not in log_text\n", "def test_log_at_level_restores_original_logger_levels(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n        import pytest\n\n            logger = logging.getLogger(__name__)\n            original_level = logger.level\n            with caplog.at_level(logging.INFO):\n                assert logger.level == logging.INFO\n            assert logger.level == original_level\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_log_cli_with_nested_loggers(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        parent_logger = logging.getLogger(\"parent\")\n        child_logger = parent_logger.getChild(\"child\")\n\n            parent_logger.info(\"Visible parent text!\")\n            child_logger.info(\"Visible child text!\")\n        \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_cli=true\n    \"\"\"\n    )\n\n    result = pytester.runpytest(\"-s\")\n\n    # fnmatch_lines does an assertion internally\n    result.stdout.fnmatch_lines(\n        [\n            \"*INFO     parent:test_log_cli_with_nested_loggers.py:5 Visible parent text!*\",\n            \"*INFO     parent.child:test_log_cli_with_nested_loggers.py:6 Visible child text!*\",\n        ]\n    )\n\n    # make sure that we get a '0' exit code for the testsuite\n    assert result.ret == 0\n", "def test_log_cli_level_set_by_fixture(pytester: Pytester) -> None:\n    \"\"\"Check that live logs level is respected when set by a fixture.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n        @pytest.fixture\n            request.config._logging_plugin.log_cli_level = logging.INFO\n\n            logging.getLogger('catchlog').debug(\"This log message won't be shown\")\n            logging.getLogger('catchlog').info(\"This log message will be shown\")\n            print('PASSED')\n    \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_cli=true\n    \"\"\"\n    )\n\n    result = pytester.runpytest(\"-s\")\n\n    # fnmatch_lines does an assertion internally\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_log_cli_level_set_by_fixture.py* This log message will be shown\",\n            \"PASSED\",  # 'PASSED' on its own line because the log message prints a new line\n        ]\n    )\n    result.stdout.no_fnmatch_line(\"*This log message won't be shown*\")\n\n    # make sure that we get a '0' exit code for the testsuite\n    assert result.ret == 0\n", "def test_log_capture_fixture_resets_levels(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import pytest\n\n        logger = logging.getLogger(__name__)\n\n            logger.setLevel(logging.DEBUG)\n            assert logger.getEffectiveLevel() == logging.DEBUG\n            caplog.clear()\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-s\")\n    assert result.ret == 0\n    # check that the logger level was restored to its default\n    assert logging.getLogger(testdir.basename).getEffectiveLevel() == logging.WARNING\n"], "sample_411": ["def test_base_command_init_invalid_requires_system_checks(self):\n    class Command(BaseCommand):\n        requires_system_checks = \"x\"\n\n    with self.assertRaisesMessage(TypeError, \"requires_system_checks must be a list or tuple.\"):\n        Command()\n", "    def test_output_transaction_with_color(self):\n        output = management.call_command(\n            \"transaction\", stdout=StringIO(), no_color=False\n        )\n        self.assertTrue(\n            output.strip().startswith(connection.ops.start_transaction_sql())\n        )\n        self.assertTrue(output.strip().endswith(connection.ops.end_transaction_sql()))\n", "    def test_command_error_in_handle(self):\n        with self.assertRaisesMessage(CommandError, \"I'm a command error\"):\n            management.call_command(\"command_error\")\n", "def test_command_requiring_migrations_checks(self):\n    dance.Command.requires_migrations_checks = True\n    try:\n        with mock.patch.object(BaseCommand, \"check_migrations\") as check_migrations:\n            management.call_command(\"dance\", verbosity=0)\n            self.assertTrue(check_migrations.called)\n            check_migrations.reset_mock()\n            management.call_command(\"dance\", skip_checks=True, verbosity=0)\n            self.assertFalse(check_migrations.called)\n    finally:\n        dance.Command.requires_migrations_checks = False\n", "def test_base_command_init_sets_default_style(self):\n    cmd = BaseCommand(stdout=StringIO(), stderr=StringIO())\n    self.assertIsInstance(cmd.style, color_style)\n"], "sample_316": ["    def test_image_file_width_height(self):\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image_file = images.ImageFile(fh)\n            image_pil = Image.open(fh)\n            self.assertEqual(image_file.width, image_pil.size[0])\n            self.assertEqual(image_file.height, image_pil.size[1])\n", "    def test_image_file_dimensions(self):\n        \"\"\"\n        Test that ImageFile's width and height properties return the correct values.\n        \"\"\"\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image_file = images.ImageFile(fh)\n            self.assertIsNotNone(image_file.width)\n            self.assertIsNotNone(image_file.height)\n            self.assertIsInstance(image_file.width, int)\n            self.assertIsInstance(image_file.height, int)\n", "    def test_get_image_dimensions_with_image_file(self):\n        \"\"\"\n        get_image_dimensions() should return the correct dimensions for an ImageFile.\n        \"\"\"\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image_file = ImageFile(fh)\n            size = image_file._get_image_dimensions()\n            image_pil = Image.open(fh)\n            self.assertEqual(image_pil.size, size)\n", "    def test_image_file_dimensions(self):\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image_file = ImageFile(fh)\n            self.assertEqual(image_file.width, 540)\n            self.assertEqual(image_file.height, 405)\n            # Test that the dimensions are cached.\n            image_file.open()\n            self.assertEqual(image_file.width, 540)\n            self.assertEqual(image_file.height, 405)\n", "    def test_get_image_dimensions_with_invalid_image(self):\n        \"\"\"\n        Test that get_image_dimensions() raises the correct error when given an invalid image.\n        \"\"\"\n        invalid_image_data = b'Invalid image data'\n        with self.assertRaises(struct.error):\n            images.get_image_dimensions(BytesIO(invalid_image_data))\n"], "sample_336": ["    def test_resolver_pickling(self):\n        resolver = get_resolver('urlpatterns_reverse.urls')\n        msg = 'Cannot pickle URLResolver.'\n        with self.assertRaisesMessage(pickle.PicklingError, msg):\n            pickle.dumps(resolver)\n", "    def test_locale_regex_descriptor(self):\n        # Test that LocaleRegexDescriptor returns a compiled regular expression\n        # based on the active language.\n        instance = RegexPattern(r'^/test/(?P<param>\\w+)/$')\n        self.assertIsInstance(instance.regex, re.Pattern)\n", "    def test_regex_pattern(self):\n        regex_pattern = RegexPattern(r'^test/(?P<arg1>\\d+)/$', name='test')\n        self.assertEqual(regex_pattern.regex.pattern, r'^test/(?P<arg1>\\d+)/$')\n        self.assertEqual(regex_pattern.name, 'test')\n", "    def test_match(self):\n        pattern = LocalePrefixPattern()\n        match = pattern.match('/en/')\n        self.assertEqual(match, ('', (), {}))\n", "    def test_locale_prefix(self):\n        self.assertEqual(reverse('mixed-args'), '/en/mixed_args/42/37/')\n        self.assertEqual(reverse('mixed-args', kwargs={'arg2': '37'}), '/en/mixed_args/42/37/')\n        self.assertEqual(reverse('mixed-args', args=['42']), '/en/mixed_args/42/37/')\n        self.assertEqual(reverse('mixed-args', args=['42'], kwargs={'arg2': '37'}), '/en/mixed_args/42/37/')\n"], "sample_500": ["def test_colorbar_update_ticks():\n    fig, ax = plt.subplots()\n    x = np.arange(-3.0, 4.001)\n    y = np.arange(-4.0, 3.001)\n    X, Y = np.meshgrid(x, y)\n    Z = X * Y\n    Z = Z[:-1, :-1]\n    pcm = ax.pcolormesh(X, Y, Z)\n    cbar = fig.colorbar(pcm, ax=ax, extend='both', orientation='vertical')\n    ticks = cbar.get_ticks()\n    cbar.update_ticks()\n    np.testing.assert_allclose(ticks, cbar.get_ticks())\n", "def test_colorbar_remove_callbacks():\n    # test fix for #20358\n    fig, ax = plt.subplots()\n    sm = cm.ScalarMappable(norm=mcolors.Normalize(), cmap='viridis')\n    cb = fig.colorbar(sm)\n    assert len(sm.callbacksSM.callbacks) == 1\n    cb.remove()\n    assert len(sm.callbacksSM.callbacks) == 0\n", "def test_colorbar_with_transformed_cmap():\n    fig, ax = plt.subplots()\n    x, y = np.mgrid[:3, :3]\n    pc = ax.pcolormesh(x, y, x+y, cmap='viridis', transform=ax.transData)\n    cb = fig.colorbar(pc, ax=ax)\n    assert cb.ax.transData is ax.transData\n", "def test_colorbar_edgecolors():\n    # test the edgecolors kwarg\n    fig, ax = plt.subplots()\n    x = np.arange(1, 5).reshape(2, 2)\n    pc = ax.pcolormesh(x)\n    cbar = fig.colorbar(pc, ax=ax, edgecolors='red')\n    fig.draw_no_output()\n    np.testing.assert_equal(cbar.dividers.get_edgecolors(), [[1., 0., 0., 1.]])\n    cbar = fig.colorbar(pc, ax=ax, edgecolors=['red', 'green', 'blue'])\n    np.testing.assert_equal(cbar.dividers.get_edgecolors(),\n                            [[1., 0., 0., 1.],\n                             [0., 1., 0., 1.],\n                             [0., 0., 1., 1.]])\n    cbar = fig.colorbar(pc, ax=ax, edgecolors='face')\n    np.testing.assert_array_equal(cbar.dividers.get_edgecolors(),\n                                 cbar.solids.get_facecolors())\n", "def test_colorbar_set_ticksLocator():\n    \"\"\"\n    Test that the locator of the colorbar can be set using a\n    `matplotlib.ticker.Locator` instance\n    \"\"\"\n    fig, ax = plt.subplots()\n    x = np.arange(-3.0, 4.001)\n    y = np.arange(-4.0, 3.001)\n    X, Y = np.meshgrid(x, y)\n    Z = X * Y\n    Z = Z[:-1, :-1]\n    pcm = ax.pcolormesh(X, Y, Z)\n    cbar = fig.colorbar(pcm, ax=ax, extend='both', orientation='vertical')\n    locator = ticker.LogLocator(base=10.0, subs=(0.1, 1, 10))\n    cbar.set_ticks(locator=locator)\n    fig.canvas.draw()\n    ticks = cbar.ax.yaxis.get_ticklocs()\n    expected = np.array([1, 10, 100])\n    np.testing.assert_allclose(ticks, expected)\n"], "sample_757": ["def test_ordinal_encoder_unsorted_categories():\n    X = np.array([['a', 'b']], dtype=object).T\n\n    enc = OrdinalEncoder(categories=[['b', 'a', 'c']])\n    exp = np.array([[1.], [0.]])\n    assert_array_equal(enc.fit_transform(X), exp)\n    assert_array_equal(enc.fit(X).transform(X), exp)\n    assert enc.categories_[0].tolist() == ['b', 'a', 'c']\n    assert np.issubdtype(enc.categories_[0].dtype, np.object_)\n\n    # unsorted passed categories still raise for numerical values\n    X = np.array([[1, 2]]).T\n    enc = OrdinalEncoder(categories=[[2, 1, 3]])\n    msg = 'Unsorted categories are not supported'\n    with pytest.raises(ValueError, match=msg):\n        enc.fit_transform(X)\n", "def test_ordinal_encoder_inverse_transform_dtype(X, cats):\n    enc = OrdinalEncoder(categories=cats)\n    X_tr = enc.fit_transform(X)\n    exp = X.astype(object)\n    assert_array_equal(enc.inverse_transform(X_tr), exp)\n    assert enc.inverse_transform(X_tr).dtype == object\n", "def test_ordinal_encoder_handle_unknown():\n    X = np.array([['a', 'b'], ['b', 'c'], ['a', 'b']], dtype=object).T\n    X2 = np.array([['a', 'd']], dtype=object).T\n\n    # Test that one hot encoder raises error for unknown features\n    # present during transform.\n    oe = OrdinalEncoder(handle_unknown='error')\n    oe.fit(X)\n    assert_raises(ValueError, oe.transform, X2)\n\n    # Test the ignore option, ignores unknown features (giving all 0's)\n    oe = OrdinalEncoder(handle_unknown='ignore')\n    oe.fit(X)\n    X2_passed = X2.copy()\n    assert_array_equal(\n        oe.transform(X2_passed),\n        np.array([np.nan, 0.]))\n    # ensure transformed data was not modified in place\n    assert_array_equal(X2, X2_passed)\n\n    # Raise error if handle_unknown is neither ignore or error.\n    oe = OrdinalEncoder(handle_unknown='42')\n    assert_raises(ValueError, oe.fit, X)\n", "def test_ordinal_encoder_handle_unknown():\n    X = np.array([['a', 'b'], ['b', 'c'], ['c', 'a']])\n\n    # Test that ordinal encoder raises error for unknown features\n    # present during transform.\n    oe = OrdinalEncoder(handle_unknown='error')\n    assert_raises(ValueError, oe.fit_transform, X)\n\n    # Test the ignore option, ignores unknown features (giving all 0's)\n    oe = OrdinalEncoder(handle_unknown='ignore')\n    oe.fit(X[:1])\n    X2 = X\n    X2[1, 0] = 'd'\n    assert_array_equal(\n        oe.transform(X2).astype('object'),\n        np.array([[0., 0.],\n                  [None, 0.]]))\n\n    # Raise error if handle_unknown is neither ignore or error.\n    oe = OrdinalEncoder(handle_unknown='42')\n    assert_raises(ValueError, oe.fit, X)\n", "def test_one_hot_encoder_fit_transform_pandas_dataframe():\n    pd = pytest.importorskip('pandas')\n\n    X_df = pd.DataFrame({'A': ['a', 'b', 'a', 'b'], 'B': [1, 2, 1, 2]})\n\n    ohe = OneHotEncoder(sparse=False)\n    Xtr = ohe.fit_transform(X_df)\n    expected = np.array([[0., 1., 1., 0.],\n                         [1., 0., 0., 1.],\n                         [0., 1., 1., 0.],\n                         [1., 0., 0., 1.]])\n    assert_array_equal(Xtr, expected)\n\n    ohe = OneHotEncoder(sparse=True)\n    Xtr = ohe.fit_transform(X_df)\n    assert_array_equal(toarray(Xtr), expected)\n\n    ohe = OneHotEncoder(dtype=int, sparse=False)\n    Xtr = ohe.fit_transform(X_df)\n    expected = expected.astype(int)\n    assert_array_equal(Xtr, expected)\n\n    ohe = OneHotEncoder(dtype=int, sparse=True)\n    Xtr = ohe.fit_transform(X_df)\n    assert_array_equal(toarray(Xtr), expected)\n"], "sample_814": ["def test_gradient_boosting_subsample():\n    # Test that subsampling is actually working\n    X, y = make_classification(n_samples=100, random_state=0)\n\n    gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=1,\n                                     max_depth=1, random_state=42,\n                                     subsample=0.5)\n    gbc.fit(X, y)\n\n    assert gbc.subsample < 1.0\n\n    for tree in gbc.estimators_.flat:\n        assert tree.n_samples < X.shape[0]\n", "def test_gradient_boosting_init_encoder():\n    # Check if GradientBoosting can handle an encoder as an init estimator.\n\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([0, 1, 1])\n\n    encoder = OneHotEncoder()\n    encoder.fit(y[:, np.newaxis])\n\n    gb = GradientBoostingClassifier(init=encoder)\n    gb.fit(X, y)\n    assert gb.score(X, y) == 1.0\n", "def test_gradient_boosting_quantile_loss():\n    # Test if quantile loss function has correct behavior\n    # when alpha is changed\n\n    X = [[1, 2], [3, 4], [5, 6]]\n    y = [1, 2, 3]\n    quantiles = [0.25, 0.5, 0.75]\n    expected_losses = [0.20833333333333334, 0.3333333333333333, 0.4583333333333333]\n\n    for alpha, expected_loss in zip(quantiles, expected_losses):\n        gb = GradientBoostingRegressor(loss='quantile', alpha=alpha,\n                                      n_estimators=1, random_state=42)\n        gb.fit(X, y)\n        loss = gb.loss_(y, gb.predict(X))\n        assert_almost_equal(loss, expected_loss)\n", "def test_gradient_boosting_oob_improvement_by_fit(Cls):\n    # Test if OOB improvement of fit is the same as the sum of OOB improvements\n    # when using warm_start and fit multiple times.\n    X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)\n    est = Cls(n_estimators=100, max_depth=1, subsample=0.5,\n              random_state=1)\n    est.fit(X, y)\n    oob_impr = est.oob_improvement_\n\n    est_ws = Cls(n_estimators=20, max_depth=1, subsample=0.5,\n                 random_state=1, warm_start=True)\n    est_ws.fit(X, y)\n    oob_impr_1 = est_ws.oob_improvement_\n    est_ws.set_params(n_estimators=40)\n    est_ws.fit(X, y)\n    oob_impr_2 = est_ws.oob_improvement_[20:]\n    est_ws.set_params(n_estimators=60)\n    est_ws.fit(X, y)\n    oob_impr_3 = est_ws.oob_improvement_[40:]\n\n    oob_impr_ws = np.concatenate((oob_impr_1, oob_impr_2, oob_impr_3))\n    assert_array_almost_equal(oob_impr, oob_impr_ws)\n", "def test_gradient_boosting_estimators_input_check():\n    # Test if estimators can be fit on unstructured data\n\n    # Test if estimator fit with 1D array raises ValueError\n    for estimator in GRADIENT_BOOSTING_ESTIMATORS:\n        msg = \"Reshape your data either using array.reshape(-1, 1) if your \"\n        msg += \"data has a single feature or array.reshape(1, -1) if it \"\n        msg += \"contains a single sample.\"\n        assert_raise_message(ValueError, msg, estimator().fit, np.array([1, 2]))\n\n    # Test if estimator fit with array with shape (n_samples, 1) is possible\n    X = np.array([1, 2, 3])[:, np.newaxis]\n    for estimator in GRADIENT_BOOSTING_ESTIMATORS:\n        estimator().fit(X, np.array([1, 2, 3]))\n\n    # Test if estimator fit with array with shape (1, n_samples) raises ValueError\n    assert_raise_message(ValueError, msg, estimator().fit, np.array([[1, 2, 3]]))\n\n    # Test if estimator fit with sparse matrix is possible\n    X_sparse = csr_matrix(X)\n    for estimator in GRADIENT_BOOSTING_ESTIMATORS:\n        estimator().fit(X_sparse, np.array([1, 2, 3]))\n"], "sample_361": ["def test_avoid_wrapping(self):\n    self.assertEqual(avoid_wrapping(\"hello world\"), \"hello\\xa0world\")\n    self.assertEqual(avoid_wrapping(\"multiple   spaces\"), \"multiple\\xa0\\xa0\\xa0spaces\")\n    self.assertEqual(avoid_wrapping(\"already\\xa0has\\xa0non-breaking\\xa0spaces\"), \"already\\xa0has\\xa0non-breaking\\xa0spaces\")\n", "def test_urlize_with_email(self):\n    tests = (\n        ('foo bar baz@example.com', 'foo bar <a href=\"mailto:baz@example.com\">baz@example.com</a>'),\n        ('foo@example.com bar', '<a href=\"mailto:foo@example.com\">foo@example.com</a> bar'),\n        ('foo@example.com bar@example.com', '<a href=\"mailto:foo@example.com\">foo@example.com</a> <a href=\"mailto:bar@example.com\">bar@example.com</a>'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n", "def test_urlize_trailing_punctuation(self):\n    tests = (\n        (\n            'Search for google.com/?q=!.',\n            'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>.',\n        ),\n        (\n            'Search for google.com/?q=1&lt!,',\n            'Search for <a href=\"http://google.com/?q=1%3C\">google.com/?q=1&lt</a>,',\n        ),\n        (\n            'Search for google.com/?q=!?',\n            'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>?',\n        ),\n        (\n            'Search for google.com/?q=1&lt!',\n            'Search for <a href=\"http://google.com/?q=1%3C\">google.com/?q=1&lt</a>!',\n        ),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n", "def test_urlize_edge_cases(self):\n    tests = (\n        ('<http://www.example.com>', '<a href=\"http://www.example.com\">http://www.example.com</a>'),\n        ('(http://www.example.com)', '<a href=\"http://www.example.com\">http://www.example.com</a>'),\n        ('[http://www.example.com]', '<a href=\"http://www.example.com\">http://www.example.com</a>'),\n        ('\"http://www.example.com\"', '<a href=\"http://www.example.com\">http://www.example.com</a>'),\n        (\"'http://www.example.com'\", '<a href=\"http://www.example.com\">http://www.example.com</a>'),\n        ('http://www.example.com.', 'http://www.example.com.'),\n        ('http://www.example.com,', 'http://www.example.com,'),\n        ('http://www.example.com;', 'http://www.example.com;'),\n        ('http://www.example.com:', 'http://www.example.com:'),\n        ('http://www.example.com!', 'http://www.example.com!'),\n        ('http://www.example.com?', 'http://www.example.com?'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n\n    tests = (\n        ('http://www.example.com', '<a href=\"http://www.example.com\">http://www.example.com</a>'),\n        ('http://example.com', '<a href=\"http://example.com\">http://example.com</a>'),\n        ('http://example.co.uk', '<a href=\"http://example.co.uk\">http://example.co.uk</a>'),\n        ('http://example.co.uk.', 'http://example.co.uk.'),\n        ('http://example.co.uk,', 'http://example.co.uk,'),\n        ('http://example.co.uk;', 'http://example.co.uk;'),\n        ('http://example.co.uk:', 'http://example.co.uk:'),\n        ('http://example.co.uk!', 'http://example.co.uk!'),\n        ('http://example.co.uk?', 'http://example.co.uk?'),\n        ('http://example.com:8080', '<a href=\"http://example.com:8080\">http://example.com:8080</a>'),\n        ('http://example.com:8080/path', '<a href=\"http://example.com:8080/path\">http", "def test_urlizer(self):\n    urlizer = Urlizer()\n    tests = (\n        ('http://example.com/', '<a href=\"http://example.com/\">http://example.com/</a>'),\n        ('www.example.com/', '<a href=\"http://www.example.com/\">www.example.com/</a>'),\n        ('example.com', '<a href=\"http://example.com\">example.com</a>'),\n        ('example.com.', '<a href=\"http://example.com\">example.com.</a>'),\n        ('example.com, and some text.', '<a href=\"http://example.com\">example.com</a>, and some text.'),\n        ('Some text, (example.com)', 'Some text, (<a href=\"http://example.com\">example.com</a>)'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlizer(value), output)\n\n    # Test with nofollow\n    tests = (\n        ('http://example.com/', '<a href=\"http://example.com/\" rel=\"nofollow\">http://example.com/</a>'),\n        ('www.example.com/', '<a href=\"http://www.example.com/\" rel=\"nofollow\">www.example.com/</a>'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlizer(value, nofollow=True), output)\n\n    # Test with autoescape\n    tests = (\n        ('http://example.com/<script>alert(\"XSS\")</script>',\n         '<a href=\"http://example.com/\" rel=\"nofollow\">&lt;script&gt;alert(&quot;XSS&quot;)&lt;/script&gt;</a>'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlizer(value, nofollow=True, autoescape=True), output)\n"], "sample_877": ["def test_isotonic_regression_oob_clip_extreme():\n    # Set y and x\n    y = np.array([3, 7, 5, 9, 8, 7, 10])\n    x = np.arange(len(y))\n\n    # Create model and fit\n    ir = IsotonicRegression(increasing=\"auto\", out_of_bounds=\"clip\")\n    ir.fit(x, y)\n\n    # Predict from  training and test x and check that min/max match.\n    y1 = ir.predict([min(x) - float('inf'), max(x) + float('inf')])\n    y2 = ir.predict(x)\n    assert max(y1) == max(y2)\n    assert min(y1) == min(y2)\n", "def test_isotonic_regression_out_of_bounds_equal_min_max():\n    # Set y and x\n    y = np.array([1, 1, 1, 1, 1, 1, 1])\n    x = np.arange(len(y))\n\n    # Create model and fit\n    ir = IsotonicRegression(increasing=\"auto\", out_of_bounds=\"raise\")\n    ir.fit(x, y)\n\n    # Check that an exception is thrown\n    msg = \"in x_new is below the interpolation range\"\n    with pytest.raises(ValueError, match=msg):\n        ir.predict([min(x) - 10])\n\n    # Create model and fit\n    ir = IsotonicRegression(increasing=\"auto\", out_of_bounds=\"clip\")\n    ir.fit(x, y)\n\n    # Predict from  training and test x and check that min/max match.\n    y1 = ir.predict([min(x) - 10, max(x) + 10])\n    y2 = ir.predict(x)\n    assert max(y1) == max(y2)\n    assert min(y1) == min(y2)\n\n    # Create model and fit\n    ir = IsotonicRegression(increasing=\"auto\", out_of_bounds=\"nan\")\n    ir.fit(x, y)\n\n    # Predict from  training and test x and check that we have two NaNs.\n    y1 = ir.predict([min(x) - 10, max(x) + 10])\n    assert sum(np.isnan(y1)) == 2\n", "def test_isotonic_regression_input_dtype_preservation():\n    \"\"\"Check that input data type is preserved.\"\"\"\n    X = np.arange(10, dtype=np.float32)\n    y = np.arange(10, dtype=np.float64)\n    iso_reg = IsotonicRegression().fit(X, y)\n    assert iso_reg.X_thresholds_.dtype == X.dtype\n    assert iso_reg.y_thresholds_.dtype == y.dtype\n    assert iso_reg.predict(X).dtype == X.dtype\n    assert iso_reg.transform(X).dtype == X.dtype\n", "def test_isotonic_regression_edge_cases():\n    \"\"\"Test IsotonicRegression with edge cases.\"\"\"\n    # Test with single data point\n    x = [1]\n    y = [2]\n    ir = IsotonicRegression()\n    ir.fit(x, y)\n    assert_array_equal(ir.transform(x), y)\n\n    # Test with two data points and decreasing order\n    x = [1, 2]\n    y = [3, 2]\n    ir = IsotonicRegression(increasing=False)\n    ir.fit(x, y)\n    assert_array_equal(ir.transform(x), y)\n\n    # Test with two data points and increasing order\n    x = [1, 2]\n    y = [2, 3]\n    ir = IsotonicRegression()\n    ir.fit(x, y)\n    assert_array_equal(ir.transform(x), y)\n\n    # Test with all y values equal\n    x = [1, 2, 3]\n    y = [2, 2, 2]\n    ir = IsotonicRegression()\n    ir.fit(x, y)\n    assert_array_equal(ir.transform(x), y)\n", "def test_isotonic_regression_predict_with_repeated_values():\n    # Test that isotonic regression handles repeated values in the\n    # prediction data correctly\n    X_train = np.array([1, 2, 3, 4, 5])\n    y_train = np.array([1, 2, 3, 4, 5])\n    X_test = np.array([1, 2, 2, 3, 3, 3, 4, 5])\n\n    ir = IsotonicRegression()\n    ir.fit(X_train, y_train)\n    y_pred = ir.predict(X_test)\n\n    # Check that the predictions for repeated values are equal\n    assert_array_equal(y_pred[1], y_pred[2])\n    assert_array_equal(y_pred[2], y_pred[3])\n    assert_array_equal(y_pred[4], y_pred[5])\n    assert_array_equal(y_pred[5], y_pred[6])\n"], "sample_830": ["def test_get_blas_info():\n    try:\n        blas_info = _get_blas_info()\n        assert 'macros' in blas_info\n        assert 'lib_dirs' in blas_info\n        assert 'cblas_libs' in blas_info\n    except ImportError:\n        # if scikit-learn is not available, skip this test\n        pass\n", "def test_get_blas_info():\n    try:\n        blas_info = _get_blas_info()\n        assert 'macros' in blas_info\n        assert 'lib_dirs' in blas_info\n        assert 'cblas_libs' in blas_info\n    except ImportError:\n        # If scikit-learn is not installed, skip this test\n        pass\n\n", "def test_get_blas_info():\n    try:\n        from .._build_utils import get_blas_info\n        blas_info = _get_blas_info()\n        assert 'macros' in blas_info\n        assert 'lib_dirs' in blas_info\n        assert 'cblas_libs' in blas_info\n    except ImportError:\n        pass  # _get_blas_info is expected to fail without sklearn\n\n", "def test_get_blas_info():\n    try:\n        blas_info = _get_blas_info()\n        assert 'macros' in blas_info\n        assert 'lib_dirs' in blas_info\n        assert 'cblas_libs' in blas_info\n    except ImportError:\n        # If sklearn._build_utils is not available, _get_blas_info will fail\n        pass\n\n", "def test_get_blas_info():\n    try:\n        blas_info = _get_blas_info()\n    except ImportError:\n        blas_info = {}\n    assert isinstance(blas_info, dict)\n    assert any(key in blas_info for key in ('macros', 'lib_dirs', 'cblas_libs'))\n"], "sample_380": ["def test_aggregate_with_default_and_empty_result_set_value(self):\n    for Aggregate in [Avg, Max, Min, Sum]:\n        with self.subTest(Aggregate):\n            result = Author.objects.filter(age__gt=100).aggregate(\n                value=Aggregate('age', default=21, empty_result_set_value=None),\n            )\n            self.assertIsNone(result['value'])\n\n    result = Author.objects.filter(age__gt=100).aggregate(\n        value=Count('age', default=21),\n    )\n    self.assertEqual(result['value'], 0)\n", "def test_aggregate_with_distinct_and_default(self):\n    vals = Book.objects.aggregate(ratings=Avg('rating', distinct=True, default=0))\n    self.assertEqual(vals['ratings'], 4.08)\n\n    vals = Book.objects.aggregate(ratings=Avg('rating', distinct=True, default=5))\n    self.assertEqual(vals['ratings'], 4.08)\n\n    vals = Book.objects.filter(id__gt=10000).aggregate(ratings=Avg('rating', distinct=True, default=0))\n    self.assertEqual(vals['ratings'], 0)\n\n    vals = Book.objects.filter(id__gt=10000).aggregate(ratings=Avg('rating', distinct=True, default=5))\n    self.assertEqual(vals['ratings'], 5)\n", "def test_aggregate_default_with_distinct(self):\n    # Test default values with distinct=True\n    result = Book.objects.filter(rating__gt=5).aggregate(value=Sum('price', distinct=True, default=Decimal('0.00')))\n    self.assertEqual(result['value'], Decimal('0.00'))\n\n    # Test default values with distinct=True and filter\n    result = Book.objects.filter(rating__gt=5).aggregate(value=Sum('price', distinct=True, filter=Q(rating__gt=4), default=Decimal('0.00')))\n    self.assertEqual(result['value'], Decimal('0.00'))\n\n    # Test default values with distinct=True and default expression\n    result = Book.objects.filter(rating__gt=5).aggregate(value=Sum('price', distinct=True, default=Value(Decimal('0.00'))))\n    self.assertEqual(result['value'], Decimal('0.00'))\n", "    def test_aggregate_with_default_on_empty_query_set(self):\n        \"\"\"\n        Test that aggregate default value is used when the QuerySet is empty.\n        \"\"\"\n        # Aggregate with default value\n        result = Author.objects.none().aggregate(avg_age=Avg('age', default=0))\n        self.assertEqual(result['avg_age'], 0)\n\n        # Aggregate with default expression\n        result = Author.objects.none().aggregate(avg_age=Avg('age', default=F('age')))\n        self.assertEqual(result['avg_age'], 0)\n\n        # Aggregate with default Value expression\n        result = Author.objects.none().aggregate(avg_age=Avg('age', default=Value(0)))\n        self.assertEqual(result['avg_age'], 0)\n\n        # Aggregate with default None\n        result = Author.objects.none().aggregate(avg_age=Avg('age', default=None))\n        self.assertIsNone(result['avg_age'])\n\n        # Aggregate without default\n        result = Author.objects.none().aggregate(avg_age=Avg('age'))\n        self.assertIsNone(result['avg_age'])\n", "    def test_aggregate_stddev_sample(self):\n        \"\"\"\n        Test the sample parameter of the StdDev aggregation function\n        \"\"\"\n        # Without sample\n        vals = Author.objects.aggregate(StdDev(\"age\"))\n        self.assertAlmostEqual(vals[\"age__stddev\"], Approximate(9.51, places=1))\n\n        # With sample\n        vals = Author.objects.aggregate(StdDev(\"age\", sample=True))\n        self.assertAlmostEqual(vals[\"age__stddev\"], Approximate(9.75, places=1))\n"], "sample_110": ["    def test_expression_pickleability(self):\n        # Test that expressions are picklable.\n        expression = Expression()\n        self.assertEqual(expression, pickle.loads(pickle.dumps(expression)))\n", "    def test_base_expression(self):\n        be = BaseExpression()\n        self.assertIsNone(be.output_field)\n", "    def test_f_expression_copy(self):\n        f_expr = F('foo')\n        copied_f_expr = f_expr.copy()\n        self.assertIsNot(f_expr, copied_f_expr)\n        self.assertEqual(f_expr.name, copied_f_expr.name)\n", "    def test_base_expression_copy(self):\n        expr = BaseExpression()\n        copy = expr.copy()\n        self.assertNotEqual(id(expr), id(copy))\n", "    def test_window_functions(self):\n        # Create some data\n        for i in range(1, 11):\n            Happening.objects.create(name=f'Event {i}', number1=i)\n\n        # Test basic window function usage\n        qs = Happening.objects.annotate(rank=Window(Rank(), order_by='number1'))\n        self.assertEqual([h.rank for h in qs], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n        # Test window function with partition by\n        qs = Happening.objects.annotate(rank=Window(Rank(), partition_by='name', order_by='number1'))\n        self.assertEqual([h.rank for h in qs], [1] * 10)\n\n        # Test window function with frame\n        qs = Happening.objects.annotate(rank=Window(Rank(), order_by='number1', frame=RowRange(start=-2, end=0)))\n        self.assertEqual([h.rank for h in qs], [1, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n        # Test window function with output field\n        qs = Happening.objects.annotate(rank=Window(Rank(), order_by='number1', output_field=models.IntegerField()))\n        self.assertEqual([h.rank for h in qs], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n"], "sample_1121": ["def test_Mul_is_complex():\n    r = Symbol('r', real=True)\n    p = Symbol('p', positive=True)\n    i1 = Symbol('i1', imaginary=True)\n    i2 = Symbol('i2', imaginary=True)\n    x = Symbol('x')\n    z = Symbol('z', complex=True)\n\n    assert I.is_complex is True\n    assert (-I).is_complex is True\n    assert (3*I).is_complex is True\n    assert (I*I).is_complex is True\n\n    e = (p + p*I)\n    j = Symbol('j', integer=True, zero=False)\n    assert (e**j).is_complex is None\n    assert (e**(2*j)).is_complex is None\n    assert (e**j).is_imaginary is None\n    assert (e**(2*j)).is_imaginary is None\n\n    assert (e**-1).is_imaginary is False\n    assert (e**2).is_imaginary\n    assert (e**3).is_imaginary is False\n    assert (e**4).is_imaginary is False\n    assert (e**5).is_imaginary is False\n    assert (e**-1).is_real is False\n    assert (e**2).is_real is False\n    assert (e**3).is_real is False\n    assert (e**4).is_real is True\n    assert (e**5).is_real is False\n    assert (e**3).is_complex\n\n    assert (r*i1).is_complex is None\n    assert (r*i1).is_real is None\n\n    assert (x*i1).is_complex is None\n    assert (x*i1).is_real is None\n\n    assert (i1*i2).is_complex is True\n    assert (i1*i2).is_real is False\n\n    assert (r*i1*i2).is_complex is True\n    assert (r*i1*i2).is_real is False\n\n    assert (p*z).is_complex is True\n    assert (p*z).is_real is False\n\n    assert (r*z).is_complex is True\n    assert (r*z).is_real is None\n\n    assert (x*z).is_complex is None\n    assert (x*z).is_real is None\n\n    assert (i1*z).is_complex is True\n    assert (i1*z).is_real is False\n\n", "def test_Mul_as_ordered_factors():\n    # Test that the as_ordered_factors method returns a list of factors\n    # in a canonical order.\n\n    from sympy import sin, cos\n    from sympy.abc import x, y\n\n    assert (2*x*y*sin(x)*cos(x)).as_ordered_factors() == [2, x, y, sin(x), cos(x)]\n    assert (sin(x)*cos(x)*2*x*y).as_ordered_factors() == [2, x, y, sin(x), cos(x)]\n    assert (x*sin(x)*2*cos(x)*y).as_ordered_factors() == [2, x, y, sin(x), cos(x)]\n    assert (y*sin(x)*cos(x)*2*x).as_ordered_factors() == [2, x, y, sin(x), cos(x)]\n", "def test_Mul_flatten_with_polar_lift():\n    from sympy import polar_lift\n    p = Symbol('p', polar=True)\n    x = Symbol('x')\n    assert Mul(polar_lift(2), p, x, evaluate=False).func is Mul\n    assert Mul(polar_lift(2), p, evaluate=False).func is Mul\n    assert (polar_lift(2)*p*x).func is Mul\n    assert (polar_lift(2)*p).func is Mul\n    assert Mul(-1, p, evaluate=False).func is Mul\n    assert Mul(-1, p, x, evaluate=False).func is Mul\n    assert (polar_lift(-2)*p*x).func is Mul\n    assert (polar_lift(-2)*p).func is Mul\n", "def test_Mul_split_reorder():\n    # Testing reordering of factors when splitting the expression into\n    # commutative and non-commutative parts.\n    A, B, C = symbols('A B C', commutative=False)\n    a, b, c = symbols('a b c', commutative=True)\n    assert Mul(A, B, a, C, b, c, evaluate=False).args_cnc() == \\\n        ([a, b, c], [A, B, C])\n    assert Mul(A, a, B, b, C, c, evaluate=False).args_cnc() == \\\n        ([a, b, c], [A, B, C])\n    assert Mul(A, a, b, B, c, C, evaluate=False).args_cnc() == \\\n        ([a, b, c], [A, B, C])\n    assert Mul(A, a, b, c, B, C, evaluate=False).args_cnc() == \\\n        ([a, b, c], [A, B, C])\n    assert Mul(a, A, b, B, c, C, evaluate=False).args_cnc() == \\\n        ([a, b, c], [A, B, C])\n    assert Mul(a, b, A, c, B, C, evaluate=False).args_cnc() == \\\n        ([a, b, c], [A, B, C])\n    assert Mul(a, b, c, A, B, C, evaluate=False).args_cnc() == \\\n        ([a, b, c], [A, B, C])\n", "def test_Mul_args_cnc_split():\n    A = Symbol(\"A\", commutative=False)\n    B = Symbol(\"B\", commutative=False)\n    C = Symbol(\"C\", commutative=False)\n    D = Symbol(\"D\")\n    e = A*B*C*D\n    assert e.args_cnc(split_1=True) == ([D], [A, B*C])\n    e = A*B*D*C\n    assert e.args_cnc(split_1=True) == ([D], [A, B, C])\n    e = A*D*C*B\n    assert e.args_cnc(split_1=True) == ([D, C], [A, B])\n    e = D*C*A*B\n    assert e.args_cnc(split_1=True) == ([D, C], [A, B])\n    e = D*B*C*A\n    assert e.args_cnc(split_1=True) == ([D], [B, C, A])\n    e = D*B*A*C\n    assert e.args_cnc(split_1=True) == ([D], [B, A, C])\n    e = D*A*C*B\n    assert e.args_cnc(split_1=True) == ([D], [A, C, B])\n    e = D*A*B*C\n    assert e.args_cnc(split_1=True) == ([D], [A, B, C])\n    e = A*D\n    assert e.args_cnc(split_1=True) == ([D], [A])\n    e = D*A\n    assert e.args_cnc(split_1=True) == ([D], [A])\n    e = A*B\n    assert e.args_cnc(split_1=True) == ([], [A, B])\n    e = B*A\n    assert e.args_cnc(split_1=True) == ([], [B, A])\n"], "sample_112": ["    def test_prepopulated_fields_js(self):\n        \"\"\"\n        prepopulated_fields_js template tag should render prepopulated fields.\n        \"\"\"\n        article = Article.objects.all()[0]\n        request = self.request_factory.get(reverse('admin:admin_views_article_change', args=[article.pk]))\n        request.user = self.superuser\n        admin = ArticleAdmin(Article, site)\n        response = admin.change_view(request, str(article.pk))\n        template_context = prepopulated_fields_js(response.context_data)\n        self.assertIn('prepopulated_fields', template_context)\n        self.assertIn('prepopulated_fields_json', template_context)\n        self.assertIsInstance(template_context['prepopulated_fields'], list)\n        self.assertIsInstance(template_context['prepopulated_fields_json'], str)\n", "def test_prepopulated_fields_js(self):\n    \"\"\"\n    prepopulated_fields_js template tag should update the context.\n    \"\"\"\n    request = self.request_factory.get(reverse('admin:auth_user_change', args=[self.superuser.pk]))\n    request.user = self.superuser\n    admin = UserAdmin(User, site)\n    extra_context = {'adminform': {'prepopulated_fields': []}}\n    response = admin.change_view(request, str(self.superuser.pk), extra_context=extra_context)\n    template_context = prepopulated_fields_js(response.context_data)\n    self.assertIn('prepopulated_fields', template_context)\n    self.assertIn('prepopulated_fields_json', template_context)\n\n    # Test with prepopulated fields\n    extra_context = {'adminform': {\n        'prepopulated_fields': [{'field': 'username', 'dependencies': ['first_name', 'last_name']}]\n    }}\n    response = admin.change_view(request, str(self.superuser.pk), extra_context=extra_context)\n    template_context = prepopulated_fields_js(response.context_data)\n    self.assertEqual(len(template_context['prepopulated_fields']), 1)\n    self.assertEqual(len(template_context['prepopulated_fields_json']), 1)\n\n    # Test with inline admin formsets\n    extra_context = {\n        'inline_admin_formsets': [{'original': None, 'prepopulated_fields': [{'field': 'username', 'dependencies': ['first_name', 'last_name']}]}\n    }\n    response = admin.change_view(request, str(self.superuser.pk), extra_context=extra_context)\n    template_context = prepopulated_fields_js(response.context_data)\n    self.assertEqual(len(template_context['prepopulated_fields']), 1)\n    self.assertEqual(len(template_context['prepopulated_fields_json']), 1)\n", "def test_prepopulated_fields_js(self):\n    \"\"\"\n    Test prepopulated_fields_js template tag.\n    \"\"\"\n    request = self.request_factory.get(reverse('admin:admin_views_article_change', args=[Article.objects.all()[0].pk]))\n    request.user = self.superuser\n    admin = ArticleAdmin(Article, site)\n    response = admin.change_view(request, str(Article.objects.all()[0].pk))\n    template_context = prepopulated_fields_js(response.context_data)\n    self.assertIn('prepopulated_fields', template_context)\n    self.assertIn('prepopulated_fields_json', template_context)\n    self.assertIsInstance(template_context['prepopulated_fields'], list)\n    self.assertIsInstance(template_context['prepopulated_fields_json'], str)\n\n    # Test that the JSON data is correctly formatted\n    prepopulated_fields_json = json.loads(template_context['prepopulated_fields_json'])\n    self.assertIsInstance(prepopulated_fields_json, list)\n    for field in prepopulated_fields_json:\n        self.assertIn('id', field)\n        self.assertIn('name', field)\n        self.assertIn('dependency_ids', field)\n        self.assertIn('dependency_list', field)\n        self.assertIn('maxLength', field)\n        self.assertIn('allowUnicode', field)\n", "    def test_prepopulated_fields_js_tag(self):\n        \"\"\"\n        prepopulated_fields_js template tag should update context correctly.\n        \"\"\"\n        request = self.request_factory.get(reverse('admin:auth_user_change', args=[self.superuser.pk]))\n        request.user = self.superuser\n        admin = UserAdmin(User, site)\n        response = admin.change_view(request, str(self.superuser.pk))\n        template_context = prepopulated_fields_js(response.context_data)\n        self.assertIn('prepopulated_fields', template_context)\n        self.assertIn('prepopulated_fields_json', template_context)\n", "def test_cell_count_filter(self):\n    \"\"\"\n    Test the cell_count filter.\n    \"\"\"\n    article_admin = ArticleAdmin(Article, site)\n    request = self.request_factory.get(reverse('admin:admin_views_article_change', args=[Article.objects.all()[0].pk]))\n    request.user = self.superuser\n    response = article_admin.change_view(request, str(Article.objects.all()[0].pk))\n    inline_admin_form = response.context_data['inline_admin_formsets'][0]\n    self.assertEqual(cell_count(inline_admin_form), 5)\n"], "sample_1112": ["def test_is_palindromic_negative_numbers():\n    assert is_palindromic(-121)\n    assert is_palindromic(-22)\n    assert is_palindromic(-1)\n    assert not is_palindromic(-123)\n    assert not is_palindromic(-1001, 2)\n    assert is_palindromic(-111, 2)\n", "def test_digits_error_handling():\n    raises(ValueError, lambda: digits(123, 1))\n    raises(ValueError, lambda: digits(123, 10, 0))\n    raises(ValueError, lambda: digits(123, 10, -1))\n    raises(ValueError, lambda: digits(-123, 10, 3))\n    raises(ValueError, lambda: digits(123, 2, 8))\n    raises(ValueError, lambda: digits(123, -10))\n    raises(ValueError, lambda: digits(123, 10, 8))\n", "def test_edge_cases():\n    # test with b equal to 1\n    raises(ValueError, lambda: digits(10, 1))\n    raises(ValueError, lambda: count_digits(10, 1))\n    raises(ValueError, lambda: is_palindromic(10, 1))\n\n    # test with large values of n and b\n    assert len(digits(10**100, 10)) == 101\n    assert count_digits(10**100, 10) == {0: 100, 1: 1}\n    assert is_palindromic(10**100, 10) is False\n\n    # test with negative values of n and b\n    raises(ValueError, lambda: digits(10, -2))\n    raises(ValueError, lambda: count_digits(10, -2))\n    raises(ValueError, lambda: is_palindromic(10, -2))\n", "def test_is_palindromic_edge_cases():\n    # Test edge case with single digit numbers\n    for i in range(10):\n        assert is_palindromic(i)\n    \n    # Test edge case with negative single digit numbers\n    for i in range(-10, 0):\n        assert is_palindromic(i)\n    \n    # Test edge case with zero\n    assert is_palindromic(0)\n", "def test_is_palindromic_negative():\n    assert is_palindromic(-121)\n    assert is_palindromic(-1331)\n"], "sample_268": ["    def test_echo_is_not_enabled_when_stdin_is_not_tty(self, mocked_isatty):\n        autoreload.ensure_echo_on()\n        self.assertFalse(mocked_isatty.called)\n", "    def test_echo_mode_is_enabled(self, mocked_termios):\n        mocked_termios.tcgetattr.return_value = [0, 1, 2, 3, 4, 5, 6, 0]\n        with mock.patch('sys.stdin.isatty', return_value=True):\n            autoreload.ensure_echo_on()\n        mocked_termios.tcsetattr.assert_called_once_with(sys.stdin, mocked_termios.TCSANOW, [0, 1, 2, 3, 4, 5, 6, mocked_termios.ECHO])\n", "    def test_no_watchman(self, mocked_pywatchman):\n        mocked_pywatchman.__bool__.return_value = False\n        with self.assertRaisesMessage(autoreload.WatchmanUnavailable, 'pywatchman not installed.'):\n            autoreload.WatchmanReloader.check_availability()\n", "    def setUp(self):\n        self.roots = [Path('/root1'), Path('/root1/sub1'), Path('/root2')]\n        self.client = mock.MagicMock()\n        self.client.query.return_value = {'roots': self.roots}\n        self.reloader = autoreload.WatchmanReloader()\n        self.reloader.client = self.client\n", "    def test_only_root(self):\n        paths = (\n            Path('/root'),\n            Path('/root'),\n            Path('/root'),\n        )\n        results = autoreload.common_roots(paths)\n        self.assertCountEqual(results, [Path('/root')])\n"], "sample_239": ["    def test_absolute_max_zero(self):\n        data = {\n            'form-TOTAL_FORMS': '0',\n            'form-INITIAL_FORMS': '0',\n        }\n        AbsoluteMaxFavoriteDrinksFormSet = formset_factory(\n            FavoriteDrinkForm,\n            absolute_max=0,\n        )\n        formset = AbsoluteMaxFavoriteDrinksFormSet(data=data)\n        self.assertIs(formset.is_valid(), False)\n        self.assertEqual(\n            formset.non_form_errors(),\n            ['Please submit at most 0 forms.'],\n        )\n        self.assertEqual(len(formset.forms), 0)\n", "def test_validate_max_with_file_field(self):\n    \"\"\"FileField with validate_max.\"\"\"\n    class FileForm(Form):\n        file = FileField()\n\n    data = {\n        'form-TOTAL_FORMS': '2',  # the number of forms rendered\n        'form-INITIAL_FORMS': '0',  # the number of forms with initial data\n        'form-MIN_NUM_FORMS': '0',  # min number of forms\n        'form-MAX_NUM_FORMS': '1',  # max number of forms\n        'form-0-file': 'file1',\n        'form-1-file': 'file2',\n    }\n    FileFormSet = formset_factory(FileForm, max_num=1, validate_max=True)\n    formset = FileFormSet(data, prefix='form')\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.non_form_errors(), ['Please submit at most 1 form.'])\n", "    def test_formset_with_deletion_and_can_delete_extra_false_with_initial_data(self):\n        \"\"\"\n        Test deletion behavior when can_delete_extra is False and there is initial data.\n        \"\"\"\n        ChoiceFormSet = formset_factory(Choice, can_delete=True, can_delete_extra=False)\n        initial = [{'choice': 'Calexico', 'votes': 100}, {'choice': 'Fergie', 'votes': 900}]\n        formset = ChoiceFormSet(initial=initial, auto_id=False, prefix='choices', extra=2)\n        self.assertHTMLEqual(\n            '\\n'.join(form.as_ul() for form in formset.forms),\n            \"\"\"<li>Choice: <input type=\"text\" name=\"choices-0-choice\" value=\"Calexico\"></li>", "def test_management_form_total_forms_greater_than_initial_forms(self):\n    \"\"\"management_form's TOTAL_FORMS should be greater than INITIAL_FORMS.\"\"\"\n    ChoiceFormSet = formset_factory(Choice, min_num=1, extra=1)\n    data = {\n        'choices-TOTAL_FORMS': '1',  # the number of forms rendered\n        'choices-INITIAL_FORMS': '2',  # the number of forms with initial data\n    }\n    with self.assertRaises(ValidationError):\n        ChoiceFormSet(data, auto_id=False, prefix='choices')\n", "    def test_all_valid_errors_merged(self):\n        \"\"\"all_valid() should merge errors across all formsets.\"\"\"\n        data1 = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data1, auto_id=False, prefix='choices')\n        data2 = {\n            'drinks-TOTAL_FORMS': '2',\n            'drinks-INITIAL_FORMS': '0',\n            'drinks-MIN_NUM_FORMS': '0',\n            'drinks-0-name': 'Coke',\n            'drinks-1-name': '',\n        }\n        FavoriteDrinksFormSet = formset_factory(FavoriteDrinkForm)\n        formset2 = FavoriteDrinksFormSet(data2, prefix='drinks')\n        self.assertFalse(all_valid((formset1, formset2)))\n        expected_errors1 = [{'votes': ['This field is required.']}, {}]\n        expected_errors2 = [{}, {'name': ['This field is required.']}]\n        self.assertEqual(formset1._errors, expected_errors1)\n        self.assertEqual(formset2._errors, expected_errors2)\n"], "sample_728": ["def test_make_gaussian_quantiles():\n    X, y = make_gaussian_quantiles(mean=[0.0, 0.0], cov=1., n_samples=100,\n                                   n_features=2, n_classes=3, random_state=0)\n\n    assert_equal(X.shape, (100, 2), \"X shape mismatch\")\n    assert_equal(y.shape, (100,), \"y shape mismatch\")\n    assert_equal(np.unique(y).shape, (3,), \"Unexpected number of classes\")\n", "def test_make_gaussian_quantiles():\n    mean = [0.0, 0.0]\n    cov = 1.0\n    X, y = make_gaussian_quantiles(mean=mean, cov=cov, n_samples=100,\n                                   n_features=2, n_classes=3,\n                                   random_state=0)\n\n    assert_equal(X.shape, (100, 2), \"X shape mismatch\")\n    assert_equal(y.shape, (100,), \"y shape mismatch\")\n    assert_equal(np.unique(y).shape, (3,), \"Unexpected number of classes\")\n", "def test_make_gaussian_quantiles():\n    X, y = make_gaussian_quantiles(mean=[0, 0], cov=1., n_samples=100,\n                                   n_features=2, n_classes=3, random_state=0)\n\n    assert_equal(X.shape, (100, 2), \"X shape mismatch\")\n    assert_equal(y.shape, (100,), \"y shape mismatch\")\n    assert_true(np.all(y >= 0))\n    assert_true(np.all(y < 3))\n    assert_equal(len(np.unique(y)), 3)\n", "def test_make_gaussian_quantiles():\n    X, y = make_gaussian_quantiles(mean=[1, 1], cov=1.5, n_samples=200,\n                                   n_features=2, n_classes=4,\n                                   shuffle=False, random_state=0)\n\n    assert_equal(X.shape, (200, 2), \"X shape mismatch\")\n    assert_equal(y.shape, (200,), \"y shape mismatch\")\n    assert_equal(np.unique(y).shape, (4,), \"Unexpected number of classes\")\n    assert_true(np.all(X[y == 0].mean(axis=0) < X[y == 1].mean(axis=0)))\n    assert_true(np.all(X[y == 1].mean(axis=0) < X[y == 2].mean(axis=0)))\n    assert_true(np.all(X[y == 2].mean(axis=0) < X[y == 3].mean(axis=0)))\n", "def test_make_gaussian_quantiles():\n    X, y = make_gaussian_quantiles(n_samples=100, n_features=3, n_classes=4,\n                                    random_state=0)\n    assert_equal(X.shape, (100, 3), \"X shape mismatch\")\n    assert_equal(y.shape, (100,), \"y shape mismatch\")\n    assert_equal(len(np.unique(y)), 4, \"Unexpected number of classes\")\n\n    X, y = make_gaussian_quantiles(n_samples=100, n_features=3, n_classes=4,\n                                    mean=[1, 2, 3], cov=0.5,\n                                    random_state=0)\n    assert_equal(X.shape, (100, 3), \"X shape mismatch\")\n    assert_equal(y.shape, (100,), \"y shape mismatch\")\n    assert_equal(len(np.unique(y)), 4, \"Unexpected number of classes\")\n\n    X, y = make_gaussian_quantiles(n_samples=100, n_features=3, n_classes=4,\n                                    shuffle=False, random_state=0)\n    assert_equal(X.shape, (100, 3), \"X shape mismatch\")\n    assert_equal(y.shape, (100,), \"y shape mismatch\")\n    assert_equal(len(np.unique(y)), 4, \"Unexpected number of classes\")\n\n    assert_raise_message(ValueError, \"n_samples must be at least n_classes\",\n                         make_gaussian_quantiles, n_samples=3, n_classes=4,\n                         random_state=0)\n"], "sample_900": ["def test_hidden_layer_sizes_list():\n    # Test that hidden_layer_sizes can be a list with different sizes.\n    X = X_digits_binary[:50]\n    y = y_digits_binary[:50]\n    hidden_layer_sizes = [5, 3, 2]\n    clf = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes)\n    with ignore_warnings(category=ConvergenceWarning):\n        clf.fit(X, y)\n    assert clf.score(X, y) > 0.95\n", "def test_MLPClassifier_with_sparse_input():\n    # Test that the MLPClassifier works with sparse input data.\n    X = X_digits_binary[:50]\n    y = y_digits_binary[:50]\n    X_sparse = csr_matrix(X)\n    \n    # Test with different solver\n    for solver in ['lbfgs', 'sgd', 'adam']:\n        clf = MLPClassifier(solver=solver, hidden_layer_sizes=5, random_state=1)\n        with ignore_warnings(category=ConvergenceWarning):\n            clf.fit(X, y)\n        pred1 = clf.predict(X)\n        clf.fit(X_sparse, y)\n        pred2 = clf.predict(X_sparse)\n        assert_array_equal(pred1, pred2)\n        pred1 = clf.predict(X)\n        pred2 = clf.predict(X_sparse)\n        assert_array_equal(pred1, pred2)\n", "def test_mlp_batch_size_auto():\n    # Test batch size 'auto' uses a min of 200 or n_samples.\n    X = X_digits_binary\n    y = y_digits_binary\n\n    # batch size should be min(200, n_samples)\n    clf = MLPClassifier(solver='sgd', batch_size='auto')\n    clf.fit(X, y)\n    assert clf.batch_size == min(200, X.shape[0])\n\n    # batch size should be 1 when n_samples is 1\n    clf = MLPClassifier(solver='sgd', batch_size='auto')\n    clf.fit(X[:1], y[:1])\n    assert clf.batch_size == 1\n", "def test_mlp_classifier_get_params():\n    # Test that get_params returns the correct parameters\n    mlp = MLPClassifier(hidden_layer_sizes=(100,), solver='adam', \n                        activation='relu', alpha=0.0001, batch_size=200,\n                        learning_rate='constant', learning_rate_init=0.001, \n                        power_t=0.5, max_iter=200, shuffle=True, random_state=1, \n                        tol=1e-4, verbose=False, warm_start=False, momentum=0.9, \n                        nesterovs_momentum=True, early_stopping=False, \n                        validation_fraction=0.1, beta_1=0.9, beta_2=0.999, \n                        epsilon=1e-8, n_iter_no_change=10, max_fun=15000)\n\n    params = mlp.get_params()\n    assert isinstance(params, dict)\n    assert 'hidden_layer_sizes' in params\n    assert 'solver' in params\n    assert 'activation' in params\n    assert 'alpha' in params\n    assert 'batch_size' in params\n    assert 'learning_rate' in params\n    assert 'learning_rate_init' in params\n    assert 'power_t' in params\n    assert 'max_iter' in params\n    assert 'shuffle' in params\n    assert 'random_state' in params\n    assert 'tol' in params\n    assert 'verbose' in params\n    assert 'warm_start' in params\n    assert 'momentum' in params\n    assert 'nesterovs_momentum' in params\n    assert 'early_stopping' in params\n    assert 'validation_fraction' in params\n    assert 'beta_1' in params\n    assert 'beta_2' in params\n    assert 'epsilon' in params\n    assert 'n_iter_no_change' in params\n    assert 'max_fun' in params\n\n    # Test that set_params changes the parameters correctly\n    mlp.set_params(hidden_layer_sizes=(50,), solver='sgd', \n                   activation='logistic', alpha=0.01, batch_size=100,\n                   learning_rate='invscaling', learning_rate_init=0.01, \n                   power_t=0.75, max_iter=300, shuffle=False, random_state=2, \n                   tol=1e-5, verbose=True, warm_start=True, momentum=0.8, \n                   nesterovs", "def test_check_input():\n    # Test input validation\n    # classification\n    X = np.random.rand(5, 3)\n    y = np.random.randint(0, 2, size=5)\n\n    # test invalid y types\n    assert_raises(ValueError, MLPClassifier().fit, X, y[:, np.newaxis])\n    assert_raises(ValueError, MLPClassifier().fit, X, np.random.rand(5, 1))\n    assert_raises(ValueError, MLPClassifier().fit, X, y + 2)\n\n    # regression\n    X = np.random.rand(5, 3)\n    y = np.random.rand(5)\n\n    # test invalid y types\n    assert_raises(ValueError, MLPRegressor().fit, X, y[:, np.newaxis])\n    assert_raises(ValueError, MLPRegressor().fit, X, np.random.randint(0, 2, size=5))\n    assert_raises(ValueError, MLPRegressor().fit, X, np.random.rand(5, 2))\n"], "sample_548": ["def test_colorbar_extension_inverted_axis_fill(orientation, extend, expected):\n    \"\"\"Test extension color with an inverted axis and filled\"\"\"\n    data = np.arange(12).reshape(3, 4)\n    fig, ax = plt.subplots()\n    cmap = mpl.colormaps[\"viridis\"].with_extremes(under=(0, 0, 0, 1),\n                                                  over=(1, 1, 1, 1))\n    im = ax.imshow(data, cmap=cmap)\n    cbar = fig.colorbar(im, orientation=orientation, extend=extend,\n                        filled=True)\n    if orientation == \"horizontal\":\n        cbar.ax.invert_xaxis()\n    else:\n        cbar.ax.invert_yaxis()\n    assert cbar._extend_patches[0].get_facecolor() == expected\n    if extend == \"both\":\n        assert len(cbar._extend_patches) == 2\n        assert cbar._extend_patches[1].get_facecolor() == (0, 0, 0, 1)\n    else:\n        assert len(cbar._extend_patches) == 1\n", "def test_location_overrides_orientation(location, orientation, ticklocation):\n    fig, ax = plt.subplots()\n    im = ax.imshow([[0, 1], [2, 3]])\n    cb = fig.colorbar(im, location=location, orientation=orientation)\n    assert cb.ticklocation == ticklocation\n", "def test_colorbar_discrete_extend():\n    fig, axs = plt.subplots(3, 2, figsize=(8, 6))\n\n    # Using extend='max' and 'min'\n    ax = axs[0, 0]\n    cs = ax.pcolormesh(np.arange(16).reshape(4, 4), cmap='viridis', vmin=0, vmax=15)\n    fig.colorbar(cs, ax=ax, extend='max', orientation='horizontal')\n\n    ax = axs[0, 1]\n    cs = ax.pcolormesh(np.arange(16).reshape(4, 4), cmap='viridis', vmin=0, vmax=15)\n    fig.colorbar(cs, ax=ax, extend='min', orientation='horizontal')\n\n    # Using extend='both' and custom boundaries\n    ax = axs[1, 0]\n    cs = ax.pcolormesh(np.arange(16).reshape(4, 4), cmap='viridis', vmin=0, vmax=15)\n    fig.colorbar(cs, ax=ax, extend='both', boundaries=[-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16],\n                 orientation='horizontal')\n\n    ax = axs[1, 1]\n    cs = ax.pcolormesh(np.arange(16).reshape(4, 4), cmap='viridis', vmin=0, vmax=15)\n    fig.colorbar(cs, ax=ax, extend='both', boundaries=[-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16],\n                 orientation='vertical')\n\n    # Using extend='neither' and custom boundaries\n    ax = axs[2, 0]\n    cs = ax.pcolormesh(np.arange(16).reshape(4, 4), cmap='viridis', vmin=0, vmax=15)\n    fig.colorbar(cs, ax=ax, extend='neither', boundaries=[0, 1, 2, 3, 4, 5, 6", "def test_colorbar_extendfrac(extend, extendfrac):\n    \"\"\"\n    Test the effect of extendfrac on a colorbar with extend set.\n    \"\"\"\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 1, 10)\n    y = np.linspace(0, 1, 10)\n    X, Y = np.meshgrid(x, y)\n    Z = X * Y\n    pcm = ax.pcolormesh(X, Y, Z, cmap='RdBu_r', extend=extend,\n                        extendfrac=extendfrac)\n    fig.colorbar(pcm, ax=ax, extend=extend, extendfrac=extendfrac)\n    assert fig.canvas.get_renderer() is not None\n", "def test_colorbar_shrinking():\n    fig, axs = plt.subplots(2, 2, figsize=(8, 8))\n    fig.subplots_adjust(left=0.05, bottom=0.05, right=0.95, top=0.95)\n    data = np.linspace(0, 10, 100).reshape(10, 10)\n    for ax in axs.flat:\n        ax.pcolormesh(data)\n        cbar = fig.colorbar(ax=ax, ax=ax)\n        cbar.ax.set_position([0.6, 0.1, 0.02, 0.7])\n        cbar.ax.set_box_aspect(20)\n        cbar.ax.set_aspect('auto')\n        cbar.shrink = 0.2\n        cbar.update_normal()\n"], "sample_1177": ["def test_unbranched_argument():\n    x = Symbol('x')\n    p = Symbol('p', positive=True)\n    assert unbranched_argument(exp_polar(15*I*pi)) == 15*pi\n    assert unbranched_argument(exp_polar(7*I*pi)) == 7*pi\n    assert unbranched_argument(2 + I) == periodic_argument(2 + I, oo)\n    assert unbranched_argument(1 + x) == periodic_argument(1 + x, oo)\n    assert N_equals(unbranched_argument((1 + I)**2), pi/2)\n    assert N_equals(unbranched_argument((1 - I)**2), -pi/2)\n    assert unbranched_argument(principal_branch(x, pi)) == periodic_argument(x, pi)\n    assert unbranched_argument(polar_lift(2 + I)) == unbranched_argument(2 + I)\n    assert periodic_argument(polar_lift(2 + I), 2*pi) == periodic_argument(2 + I, 2*pi)\n    assert periodic_argument(polar_lift(2 + I), 3*pi) == periodic_argument(2 + I, 3*pi)\n    assert periodic_argument(polar_lift(2 + I), pi) == periodic_argument(polar_lift(2 + I), pi)\n    assert unbranched_argument(polar_lift(1 + I)) == pi/4\n    assert periodic_argument(2*p, p) == periodic_argument(p, p)\n    assert periodic_argument(pi*p, p) == periodic_argument(p, p)\n    assert Abs(polar_lift(1 + I)) == Abs(1 + I)\n", "def test_issue_22190():\n    x = Symbol('x')\n    assert unpolarify(2*principal_branch(x, 2*pi)) == 2*x\n    assert unpolarify(3*principal_branch(x, pi)) == 3*principal_branch(x, pi)\n    assert unpolarify(principal_branch(x, 2*pi)) == x\n", "def test_re_conjugate():\n    x = Symbol('x', real=True)\n    y = Symbol('y')\n    assert re(conjugate(x)).doit() == x\n    assert re(conjugate(y)).doit() == re(y)\n    assert re(conjugate(x + y)).doit() == re(x + y)\n    assert re(conjugate(x - y)).doit() == re(x - y)\n    assert re(conjugate(x * y)).doit() == re(x * conjugate(y))\n    assert re(conjugate(x / y)).doit() == re(x / conjugate(y))\n    assert re(conjugate(-x)).doit() == re(-x)\n", "def test_issue_19078():\n    from sympy import symbols, Function\n    x, y = symbols('x y')\n    f = Function('f')\n    assert sign(f(x)).diff(x).subs(f(x), x) == 2*Derivative(x, x)*DiracDelta(x)\n    assert sign(f(x)).diff(x).subs(f(x), y) == 2*Derivative(y, x)*DiracDelta(y)\n    assert sign(x).diff(x) == 2*Derivative(x, x)*DiracDelta(x)\n    assert sign(x).diff(y) == 0\n    assert sign(x).diff(f(x)) == 0\n", "def test_polar_lift():\n    x = Symbol('x')\n    a, b = symbols('a b', real=True)\n    p = Symbol('p', positive=True)\n\n    assert polar_lift(4) == 4*exp_polar(0)\n    assert polar_lift(-4) == 4*exp_polar(I*pi)\n    assert polar_lift(-I) == exp_polar(-I*pi/2)\n    assert polar_lift(I + 2) == polar_lift(2 + I)\n\n    assert polar_lift(4*x) == 4*polar_lift(x)\n    assert polar_lift(4*p) == 4*p\n\n    assert polar_lift(a + b*I) == polar_lift(a + b*I)\n    assert polar_lift(a + b*p) == a*polar_lift(b) + a\n\n    assert polar_lift(-a + b*I) == b*exp_polar(I*pi/2) + a*exp_polar(I*pi)\n    assert polar_lift(-a - b*I) == b*exp_polar(-I*pi/2) + a*exp_polar(I*pi)\n    assert polar_lift(a - b*I) == b*exp_polar(-I*pi/2) + a\n    assert polar_lift(a + b*I) == b*exp_polar(I*pi/2) + a\n\n    assert polar_lift(a + b) == a + b\n\n    assert polar_lift(3 + 2*I).as_real_imag() == (3, 2)\n    assert polar_lift(-3 - 2*I).as_real_imag() == (-3, -2)\n    assert polar_lift(-3 + 2*I).as_real_imag() == (-3, 2)\n    assert polar_lift(3 - 2*I).as_real_imag() == (3, -2)\n    assert polar_lift(0 + 0*I).as_real_imag() == (0, 0)\n\n    assert polar_lift(polar_lift(3 + 2*I)).as_real_imag() == (3, 2)\n    assert polar_lift(polar_lift(-3 - 2*I)).as_real_imag() == (-3, -2)\n    assert polar_lift(polar_lift(-3 + 2*I)).as_real_imag() == (-3, 2)\n    assert polar_lift(polar_lift(3 - 2*I)).as_real_imag() == (3, -2)\n    assert polar_lift(polar_lift(0"], "sample_878": ["def test_column_transformer_feature_names_out_with_repeated_transformer_names():\n    # Check that feature names are correct when transformer names are repeated\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame([[1, 2, 3, 4]], columns=[\"a\", \"b\", \"c\", \"d\"])\n    ct = ColumnTransformer(\n        [\n            (\"trans1\", TransWithNames(), [\"a\", \"b\"]),\n            (\"trans1\", TransWithNames(), [\"c\", \"d\"]),\n        ]\n    )\n    ct.fit(df)\n\n    names = ct.get_feature_names_out()\n    assert_array_equal(names, [\"trans1__a\", \"trans1__b\", \"trans1__c\", \"trans1__d\"])\n", "def test_column_transformer_set_output_pandas_with_mixed_dtypes():\n    \"\"\"Check that set_output works with pandas and mixed dtypes\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame(\n        {\n            \"A\": pd.Series([1, 2, 3], dtype=\"Int32\"),\n            \"B\": pd.Series([0.0, 1.0, 2.0], dtype=\"float64\"),\n            \"C\": pd.Series([\"a\", \"b\", \"c\"], dtype=\"object\"),\n        }\n    )\n    ct = ColumnTransformer([(\"num\", StandardScaler(), [\"A\", \"B\"]), (\"cat\", \"drop\", [\"C\"])])\n    ct.set_output(transform=\"pandas\")\n    X_trans_df = ct.fit_transform(df)\n\n    assert isinstance(X_trans_df, pd.DataFrame)\n    expected_dtypes = {\n        \"num__A\": \"float64\",\n        \"num__B\": \"float64\",\n    }\n    for col, dtype in X_trans_df.dtypes.items():\n        assert dtype == expected_dtypes[col]\n", "def test_column_transformer_sparsity():\n    # Test that the transformer outputs a sparse matrix if the sum of the\n    # sparsity of the individual transformers is less than sparse_threshold.\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    ohe = OneHotEncoder(sparse_output=True)\n    scaler = StandardScaler()\n    ct = ColumnTransformer(\n        [(\"ohe\", ohe, [0]), (\"scaler\", scaler, [1])], sparse_threshold=0.6\n    )\n    X_trans = ct.fit_transform(X_array)\n    assert sparse.issparse(X_trans)\n\n    # Test that the transformer outputs a dense matrix if the sum of the\n    # sparsity of the individual transformers is greater than or equal to\n    # sparse_threshold.\n    ohe = OneHotEncoder(sparse_output=True)\n    scaler = StandardScaler()\n    ct = ColumnTransformer(\n        [(\"ohe\", ohe, [0]), (\"scaler\", scaler, [1])], sparse_threshold=0.3\n    )\n    X_trans = ct.fit_transform(X_array)\n    assert not sparse.issparse(X_trans)\n\n    # Test that the transformer outputs a sparse matrix if all individual\n    # transformers output sparse matrices, regardless of sparse_threshold.\n    ohe = OneHotEncoder(sparse_output=True)\n    ct = ColumnTransformer([(\"ohe\", ohe, [0]), (\"ohe\", ohe, [1])], sparse_threshold=0)\n    X_trans = ct.fit_transform(X_array)\n    assert sparse.issparse(X_trans)\n\n    # Test that the transformer outputs a dense matrix if all individual\n    # transformers output dense matrices, regardless of sparse_threshold.\n    scaler = StandardScaler()\n    ct = ColumnTransformer(\n        [(\"scaler\", scaler, [0]), (\"scaler\", scaler, [1])], sparse_threshold=1\n    )\n    X_trans = ct.fit_transform(X_array)\n    assert not sparse.issparse(X_trans)\n", "def test_column_transformer_parallel_transform_with_pandas():\n    # Non-regression test for gh-24943\n    pd = pytest.importorskip(\"pandas\")\n    X_df = pd.DataFrame({\"feat1\": [1.0, 2.0, 3.0], \"feat2\": [2.0, 3.0, 4.0]})\n    ct = ColumnTransformer([(\"trans\", Trans(), [\"feat1\"])]).set_output(transform=\"pandas\")\n    X_trans = ct.fit_transform(X_df)\n\n    # Make sure the pandas output works in parallel\n    ct.set_params(n_jobs=2)\n    X_trans_parallel = ct.fit_transform(X_df)\n    assert_array_equal(X_trans.columns, X_trans_parallel.columns)\n    assert_array_equal(X_trans.values, X_trans_parallel.values)\n", "def test_column_transformer_remainder_dataframe_reindexing():\n    # Test that the column transformer handles dataframe reindexing in the remainder\n    pd = pytest.importorskip(\"pandas\")\n\n    X_df = pd.DataFrame(np.arange(6).reshape(3, 2), columns=[\"first\", \"second\"])\n    X_df_trans = X_df.copy()\n    X_df_trans.columns = X_df_trans.columns[::-1]\n\n    tf = ColumnTransformer([(\"bycol\", Trans(), \"first\")], remainder=\"passthrough\")\n    tf.fit(X_df)\n\n    # ColumnTransformer does not reorder columns.\n    assert_array_equal(tf.transform(X_df_trans).columns, X_df_trans.columns)\n"], "sample_562": ["def test_line2d_set_transform(fig_test, fig_ref):\n    \"\"\"Test that setting a new transform updates the plot correctly.\"\"\"\n    fig_test.add_subplot().plot([0, 1], [0, 1], transform=mtransforms.IdentityTransform())\n    fig_ref.add_subplot().plot([0, 1], [0, 1])\n", "def test_markevery_fancy_indexing():\n    fig, ax = plt.subplots()\n    np.random.seed(0)\n    x = np.linspace(0, 1, 100)\n    y = np.random.rand(len(x))\n    ax.plot(x, y, markevery=[True]*10+[False]*20+[True]*10+[False]*60)\n\n    fig.canvas.draw()\n    assert_array_equal(ax.lines[0].get_markevery(),\n                       [True]*10+[False]*20+[True]*10+[False]*60)\n", "def test_markerfacecolor():\n    line, = plt.plot([1, 2, 3], marker='o', markerfacecolor='red')\n    assert line.get_markerfacecolor() == 'red'\n\n    # test markerfacecolor with facecolor and edgecolor\n    line, = plt.plot([1, 2, 3], marker='o', markerfacecolor='red',\n                     markeredgecolor='blue')\n    assert line.get_markerfacecolor() == 'red'\n    assert line.get_markeredgecolor() == 'blue'\n\n    # test markerfacecoloralt with facecolor and edgecolor\n    line, = plt.plot([1, 2, 3], marker='o', markerfacecolor='red',\n                     markerfacecoloralt='green',\n                     markeredgecolor='blue')\n    assert line.get_markerfacecolor() == 'red'\n    assert line.get_markerfacecoloralt() == 'green'\n    assert line.get_markeredgecolor() == 'blue'\n", "def test_markevery_fancy_indexing():\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 1, 10)\n    y = np.sin(x)\n    ax.plot(x, y, 'o-', markevery=[True, False, True, False, True, False, False, False, True, False])\n    ax.plot(x, y, 'o-', markevery=[0, 1, 2, 4, 5, 7, 9])\n\n    with pytest.raises(ValueError):\n        ax.plot(x, y, 'o-', markevery=[True, False, 1, 2, 4, 5, 7, 9])\n    with pytest.raises(ValueError):\n        ax.plot(x, y, 'o-', markevery=[10, 11, 12, 14, 15, 17, 19])\n", "def test_markevery_subsample():\n    x = np.linspace(0, 1, 100)\n    y = np.sin(x)\n\n    fig, axs = plt.subplots(2, 2)\n\n    axs[0, 0].plot(x, y, \"o\", markevery=10)\n    axs[0, 1].plot(x, y, \"o\", markevery=(10, 20))\n    axs[1, 0].plot(x, y, \"o\", markevery=[0, 20, 50, 80])\n    axs[1, 1].plot(x, y, \"o\", markevery=[True, False] * 50)\n\n        line, = ax.lines\n        return np.flatnonzero(line.get_markevery())\n\n    expected_markevery = {\n        axs[0, 0]: np.arange(10, 100, 10),\n        axs[0, 1]: np.arange(10, 100, 20),\n        axs[1, 0]: np.array([0, 20, 50, 80]),\n        axs[1, 1]: np.arange(0, 100, 2),\n    }\n\n    for ax, expected in expected_markevery.items():\n        np.testing.assert_array_equal(get_actual_markevery(ax), expected)\n"], "sample_288": ["    def test_register_lookup(self):\n        @models.JSONField.register_lookup\n        class MyTransform(Transform):\n            lookup_name = 'my_transform'\n        self.assertIn('my_transform', models.JSONField._lookup_classes)\n        self.assertIsInstance(models.JSONField.get_lookup('my_transform'), MyTransform)\n        models.JSONField._unregister_lookup(MyTransform)\n        models.JSONField._clear_cached_lookups()\n        self.assertNotIn('my_transform', models.JSONField._lookup_classes)\n        self.assertIsInstance(models.JSONField.get_lookup('my_transform'), KeyTransformFactory)\n", "    def test_key_transform_as_text_lookup_mixin(self):\n        transform = KeyTransform('test')\n        self.assertIsInstance(\n            KeyTransformTextLookupMixin(transform),\n            KeyTransformTextLookupMixin,\n        )\n        self.assertEqual(\n            KeyTransformTextLookupMixin(transform).lhs,\n            transform,\n        )\n", "    def test_key_transform_factory(self):\n        field = models.JSONField()\n        transform = field.get_transform('test')\n        self.assertIsInstance(transform, KeyTransformFactory)\n        self.assertEqual(transform.key_name, 'test')\n", "    def test_formfield_custom_encoder(self):\n        class CustomJSONEncoder(json.JSONEncoder):\n                if isinstance(obj, int):\n                    return str(obj)\n                return json.JSONEncoder.default(self, obj)\n\n        model_field = models.JSONField(encoder=CustomJSONEncoder)\n        form_field = model_field.formfield()\n        self.assertIsInstance(form_field, forms.JSONField)\n", "    def test_key_transform_expression_chaining(self):\n        qs = NullableJSONModel.objects.annotate(\n            chain=KeyTransform('d', KeyTransform('0', KeyTransform('d', 'value'))),\n            expr=Cast('chain', models.JSONField()),\n        ).filter(chain=F('expr__0'))\n        self.assertSequenceEqual(qs, [self.objs[4]])\n"], "sample_137": ["def test_replace_named_groups(self):\n    pattern = '^(?P<a>\\w+)/b/(?P<c>\\w+)/$'\n    expected_output = '^<a>/b/<c>/$'\n    self.assertEqual(replace_named_groups(pattern), expected_output)\n\n    pattern = '^(?P<a>\\w+)/b/(?P<c>(x|y)\\w+)/$'\n    expected_output = '^<a>/b/<c>/$'\n    self.assertEqual(replace_named_groups(pattern), expected_output)\n\n    pattern = '^(?P<a>\\w+)/b/(?P<c>\\w+)/'\n    expected_output = '^<a>/b/<c>'\n    self.assertEqual(replace_named_groups(pattern), expected_output)\n\n    pattern = '^(?P<a>\\w+)/b/(?P<c>(x|y)\\w+)'\n    expected_output = '^<a>/b/<c>'\n    self.assertEqual(replace_named_groups(pattern), expected_output)\n", "def test_replace_named_groups(self):\n    patterns = [\n        r'^(?P<a>\\w+)/b/(\\w+)$',\n        r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$',\n        r'^(?P<a>\\w+)/b/(\\w+)',\n        r'^(?P<a>\\w+)/b/(?P<c>\\w+)',\n        r'^(?P<a>\\w+)/b/(?P<c>\\w+)/d(?P<e>\\w+)',\n    ]\n    expected_results = [\n        r'^<a>/b/(\\w+)$',\n        r'^<a>/b/<c>/$',\n        r'^<a>/b/(\\w+)',\n        r'^<a>/b/<c>',\n        r'^<a>/b/<c>/d<e>',\n    ]\n    for pattern, expected_result in zip(patterns, expected_results):\n        self.assertEqual(replace_named_groups(pattern), expected_result)\n", "def test_replace_named_groups(self):\n    self.assertEqual(\n        replace_named_groups('(?P<a>b)(?P<c>d)'),\n        '<a><c>'\n    )\n    self.assertEqual(\n        replace_named_groups('(?P<a>b)(d)'),\n        '<a><var>'\n    )\n    self.assertEqual(\n        replace_named_groups('(?P<a>(b|c))d'),\n        '<a>d'\n    )\n    self.assertEqual(\n        replace_named_groups('(?P<a>(b|c))(d)'),\n        '<a><var>'\n    )\n    self.assertEqual(\n        replace_named_groups('(?P<a>(b|c))(?P<d>e)'),\n        '<a><d>'\n    )\n    self.assertEqual(\n        replace_named_groups('(?P<a>(b|c))(?P<d>(e|f))'),\n        '<a><d>'\n    )\n", "def test_replace_named_groups(self):\n    self.assertEqual(replace_named_groups(r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$'), r'^<a>/b/<c>/$')\n    self.assertEqual(replace_named_groups(r'^(?P<a>\\w+)/b/(\\w+)'), r'^<a>/b/(\\w+)')\n    self.assertEqual(replace_named_groups(r'^(?P<a>\\w+)/b/(?P<c>\\w+)'), r'^<a>/b/<c>')\n    self.assertEqual(replace_named_groups(r'^(?P<a>\\w+)/b/(\\w+)/(\\w+)'), r'^<a>/b/<var>/<var>')\n", "def test_replace_named_groups(self):\n    patterns = [\n        ('^(?P<a>\\w+)/b/(?P<c>\\w+)$', '^<a>/b/<c>$'),\n        ('(?P<a>\\w+)/b/(?P<c>\\w+)/$', '<a>/b/<c>/'),\n        ('(?P<a>\\w+)/b/(\\w+)', '<a>/b/(\\\\w+)'),\n        ('(?P<a>\\w+)/b/(?P<c>\\w+)', '<a>/b/<c>'),\n    ]\n    for original, expected in patterns:\n        self.assertEqual(replace_named_groups(original), expected)\n"], "sample_922": ["def test_python_domain_clear_doc(app):\n    text = (\".. py:module:: test_module\\n\"\n            \".. py:function:: test_function\\n\"\n            \".. py:class:: test_class\\n\"\n            \"   .. py:method:: test_method\\n\")\n\n    restructuredtext.parse(app, text)\n    domain = app.env.get_domain('py')\n\n    assert len(domain.objects) == 3\n    assert len(domain.modules) == 1\n\n    domain.clear_doc(app.env.docnames[0])\n\n    assert len(domain.objects) == 0\n    assert len(domain.modules) == 0\n", "def test_pydecorator_function_signature(app, status, warning):\n    text = (\".. py:decorator:: deco1\\n\"\n            \".. py:decorator:: deco2\\n\"\n            \"   :async:\\n\"\n            \".. py:decorator:: deco3\\n\"\n            \"   :classmethod:\\n\"\n            \".. py:decorator:: deco4\\n\"\n            \"   :final:\\n\"\n            \".. py:decorator:: deco5\\n\"\n            \"   :abstractmethod:\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"@\"],\n                                                    [desc_name, \"deco1\"])],\n                                  [desc_content, ()])],\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"@\"],\n                                                    [desc_annotation, \"async \"],\n                                                    [desc_name, \"deco2\"])],\n                                  [desc_content, ()])],\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"@\"],\n                                                    [desc_annotation, \"classmethod \"],\n                                                    [desc_name, \"deco3\"])],\n                                  [desc_content, ()])],\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"@\"],\n                                                    [desc_annotation, \"final \"],\n                                                    [desc_name, \"deco4\"])],\n                                  [desc_content, ()])],\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"@\"],\n                                                    [desc_annotation, \"abstract \"],\n                                                    [desc_name, \"deco5\"])],\n                                  [desc_content, ()])]))\n\n    assert 'deco1' in domain.objects\n    assert domain.objects['deco1'] == ('index', 'deco1', 'function')\n\n    assert 'deco2' in domain.objects\n    assert domain.objects['deco2'] == ('index', 'deco2', 'function')\n\n    assert 'deco3' in domain.objects\n    assert domain.objects['deco3'] == ('index', 'deco3', 'function')\n\n    assert 'deco4' in domain.objects\n    assert domain.objects['deco4'] == ('index', 'deco4', 'function')\n\n    assert '", "def test_pyvariable_options(app, status, warning):\n    app.builder.build_all()\n\n    text = (\".. py:variable:: attr\\n\"\n            \"   :type: str\\n\"\n            \"   :value: hello\\n\")\n    doctree = app.env.get_doctree('roles')\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"attr\"],\n                                                    [desc_annotation, (\": \",\n                                                                       [pending_xref, \"str\"])],\n                                                    [desc_annotation, \" = hello\"])],\n                                  [desc_content, ()])]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"data\",\n                domain=\"py\", objtype=\"data\", noindex=False)\n\n    assert 'attr' in app.env.domains['py'].objects\n    assert app.env.domains['py'].objects['attr'] == ('roles', 'attr', 'data')\n\n    text = (\".. py:variable:: attr\\n\"\n            \"   :type: Optional[str]\\n\"\n            \"   :value: None\\n\")\n    doctree = app.env.get_doctree('roles')\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"attr\"],\n                                                    [desc_annotation, (\": \",\n                                                                       [pending_xref, \"Optional\"],\n                                                                       [desc_sig_punctuation, \"[\"],\n                                                                       [pending_xref, \"str\"],\n                                                                       [desc_sig_punctuation, \"]\"])],\n                                                    [desc_annotation, \" = None\"])],\n                                  [desc_content, ()])]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"data\",\n                domain=\"py\", objtype=\"data\", noindex=False)\n\n    assert 'attr' in app.env.domains['py'].objects\n    assert app.env.domains['py'].objects['attr'] == ('roles', 'attr', 'data')\n", "def test_domain_py_module_directive(app, status, warning):\n    \"\"\"Domain objects created with the module directive\"\"\"\n    app.builder.build_all()\n\n    content = (app.outdir / 'module.html').read_text()\n    assert ('<dt><span class=\"sig-prename descclassname\"><span class=\"pre\">module_a.</span></span>' in content)\n    assert ('<dt><span class=\"sig-prename descclassname\"><span class=\"pre\">module_b.</span></span>' in content)\n\n    doctree = app.env.get_doctree('module')\n    assert_node(doctree, (addnodes.index,\n                          nodes.target,\n                          addnodes.index,\n                          nodes.target,\n                          addnodes.index,\n                          nodes.target,\n                          addnodes.index,\n                          nodes.target,\n                          addnodes.index,\n                          nodes.target,\n                          addnodes.index,\n                          nodes.target,\n                          addnodes.index,\n                          nodes.target,\n                          addnodes.index,\n                          nodes.target,\n                          addnodes.index,\n                          nodes.target,\n                          addnodes.index))\n\n    assert_node(doctree[0], addnodes.index,\n                entries=[('pair', 'module; module_a', 'module-module_a', '', None)])\n    assert_node(doctree[2], addnodes.index,\n                entries=[('pair', 'module; module_a.submodule', 'module-module_a.submodule', '', None)])\n    assert_node(doctree[4], addnodes.index,\n                entries=[('pair', 'module; module_b', 'module-module_b', '', None)])\n    assert_node(doctree[6], addnodes.index,\n                entries=[('pair', 'module; module_b.submodule', 'module-module_b.submodule', '', None)])\n    assert_node(doctree[8], addnodes.index,\n                entries=[('pair', 'module; module_b.submodule.submodule', 'module-module_b.submodule.submodule', '', None)])\n    assert_node(doctree[10], addnodes.index,\n                entries=[('pair', 'module; module_b.submodule.submodule.submodule', 'module-module_b.submodule.submodule.submodule', '', None)])\n    assert_node(doctree[12], addnodes.index,\n                entries=[('pair', 'module; module_b.submodule.submodule.submodule.submodule', 'module-module_b.submodule.submodule.submodule.submodule', '', None)])\n    assert_node(doct", "def test_pydomain_find_obj_with_searchmode(app, status, warning):\n    app.builder.build_all()\n    domain = app.env.get_domain('py')\n    modname = 'module'\n    clsname = 'Class'\n    obj_name = 'obj'\n\n    # test searchmode 0\n    matches = domain.find_obj(app.env, modname, clsname, obj_name)\n    assert matches == [(f\"{modname}.{clsname}.{obj_name}\", ('index', f\"{modname}.{clsname}.{obj_name}\", 'method'))]\n\n    # test searchmode 1\n    matches = domain.find_obj(app.env, modname, clsname, obj_name, searchmode=1)\n    assert matches == [(f\"{modname}.{clsname}.{obj_name}\", ('index', f\"{modname}.{clsname}.{obj_name}\", 'method'))]\n\n    # test searchmode 1 with obj_type\n    matches = domain.find_obj(app.env, modname, clsname, obj_name, 'method', searchmode=1)\n    assert matches == [(f\"{modname}.{clsname}.{obj_name}\", ('index', f\"{modname}.{clsname}.{obj_name}\", 'method'))]\n\n    # test searchmode 1 with obj_type that does not match\n    matches = domain.find_obj(app.env, modname, clsname, obj_name, 'function', searchmode=1)\n    assert matches == []\n"], "sample_520": ["def test_text3d_rotation():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n\n    zdirs = (None, 'x', 'y', 'z', (1, 1, 0), (1, 1, 1))\n    xs = (2, 6, 4, 9, 7, 2)\n    ys = (6, 4, 8, 7, 2, 2)\n    zs = (4, 2, 5, 6, 1, 7)\n\n    for zdir, x, y, z in zip(zdirs, xs, ys, zs):\n        label = '(%d, %d, %d), dir=%s' % (x, y, z, zdir)\n        ax.text(x, y, z, label, zdir, rotation=30)\n\n    ax.text(1, 1, 1, \"red\", color='red', rotation=45)\n    ax.text2D(0.05, 0.95, \"2D Text\", transform=ax.transAxes)\n    ax.set_xlim3d(0, 10)\n    ax.set_ylim3d(0, 10)\n    ax.set_zlim3d(0, 10)\n    ax.set_xlabel('X axis')\n    ax.set_ylabel('Y axis')\n    ax.set_zlabel('Z axis')\n", "def test_line3d_collection_set_get_data_3d():\n    xs = [[0, 1], [2, 3]]\n    ys = [[1, 2], [3, 4]]\n    zs = [[0, 0], [1, 1]]\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    lines = art3d.Line3DCollection(list(zip(xs, ys, zs)))\n    ax.add_collection3d(lines)\n    np.testing.assert_array_equal(xs, [line[:2] for line in lines.get_data_3d()])\n    np.testing.assert_array_equal(ys, [line[1] for line in lines.get_data_3d()])\n    np.testing.assert_array_equal(zs, [line[2] for line in lines.get_data_3d()])\n    xs2 = [[4, 5], [6, 7]]\n    ys2 = [[8, 9], [10, 11]]\n    zs2 = [[12, 13], [14, 15]]\n    lines.set_segments(list(zip(xs2, ys2, zs2)))\n    np.testing.assert_array_equal(xs2, [line[:2] for line in lines.get_data_3d()])\n    np.testing.assert_array_equal(ys2, [line[1] for line in lines.get_data_3d()])\n    np.testing.assert_array_equal(zs2, [line[2] for line in lines.get_data_3d()])\n", "def test_text3d_alignment():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n\n    ax.text(0.5, 0.5, 0.5, \"center\", ha='center', zdir='z')\n    ax.text(0.5, 0.5, 0.5, \"left\", ha='left', zdir='z')\n    ax.text(0.5, 0.5, 0.5, \"right\", ha='right', zdir='z')\n\n    ax.text(0.5, 0.5, 0.5, \"center\", va='center', zdir='z')\n    ax.text(0.5, 0.5, 0.5, \"top\", va='top', zdir='z')\n    ax.text(0.5, 0.5, 0.5, \"bottom\", va='bottom', zdir='z')\n\n    ax.text(0.5, 0.5, 0.5, \"center\", ha='center', va='center', zdir='z')\n    ax.text(0.5, 0.5, 0.5, \"top_left\", ha='left', va='top', zdir='z')\n    ax.text(0.5, 0.5, 0.5, \"top_right\", ha='right', va='top', zdir='z')\n    ax.text(0.5, 0.5, 0.5, \"bottom_left\", ha='left', va='bottom', zdir='z')\n    ax.text(0.5, 0.5, 0.5, \"bottom_right\", ha='right', va='bottom', zdir='z')\n", "def test_text3d_properties():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    t = art3d.Text3D(0.5, 0.5, 0.5, r'Foo bar $\\int$', zdir='z')\n    t.set_alpha(0.5)\n    t.set_fontsize(15)\n    t.set_fontweight('bold')\n    t.set_color('red')\n    ax.add_artist(t)\n    ax.set_xlim3d(0, 1)\n    ax.set_ylim3d(0, 1)\n    ax.set_zlim3d(0, 1)\n", "def test_line_collection_2d_to_3d():\n    # Test that converting a 2D LineCollection to 3D works\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n\n    # Create a 2D LineCollection\n    xs = [[0, 1], [2, 3]]\n    ys = [[0, 1], [2, 3]]\n    lc = LineCollection(list(zip(xs, ys)))\n\n    # Convert to 3D and add to axes\n    art3d.line_collection_2d_to_3d(lc, zs=0, zdir='z')\n    ax.add_collection3d(lc)\n\n    # Check that the 3D collection has the correct attributes\n    assert lc.get_segments() == [np.column_stack(x) for x in zip(xs, ys)]\n    assert lc.get_offsets() is None\n\n    # Check that the 2D collection has been modified\n    assert lc.__class__ == art3d.Line3DCollection\n    assert lc._segments3d == [np.column_stack(x) for x in zip(xs, ys, [[0, 0], [0, 0]])]\n"], "sample_997": ["def test_lambda_notation():\n    inputs = {\n        'lambda x: 2*x': Function('Lambda')(Symbol('x'), 2*Symbol('x')),\n        'lambda x: 2*x, lambda y: 3*y': [Function('Lambda')(Symbol('x'), 2*Symbol('x')),\n                                         Function('Lambda')(Symbol('y'), 3*Symbol('y'))],\n    }\n    for text, result in inputs.items():\n        assert parse_expr(text) == result\n", "def test_function_exponentiation():\n    transformations = standard_transformations + (function_exponentiation, implicit_multiplication)\n    x = Symbol('x')\n    assert parse_expr('sin**2(x)', transformations=transformations) == sin(x)**2\n    assert parse_expr('sin**-1(x)', transformations=transformations) == 1/sin(x)\n    assert parse_expr('sin**(x+1)(x)', transformations=transformations) == sin(x)**(x+1)\n", "def test_implicit_application():\n    transformations = standard_transformations + (implicit_application, )\n    x = Symbol('x')\n    y = Symbol('y')\n    f = Function('f')\n\n    assert parse_expr(\"f x + 1\", transformations=transformations) == f(x) + 1\n    assert parse_expr(\"f(x+1) + f x + 2\", transformations=transformations) == f(x + 1) + f(x) + 2\n    assert parse_expr(\"f x + 1 + f(x+2)\", transformations=transformations) == f(x) + 1 + f(x + 2)\n    assert parse_expr(\"(f x + 1)(x+2)\", transformations=transformations) == (f(x) + 1)*(x + 2)\n", "def test_function_exponentiation():\n    transformations = standard_transformations + \\\n                      (function_exponentiation,)\n    x = Symbol('x')\n    assert parse_expr(\"sin^2(x)\", transformations=transformations) == sin(x)**2\n    assert parse_expr(\"cos**3(x)\", transformations=transformations) == cos(x)**3\n    assert parse_expr(\"sin**-2(x)\", transformations=transformations) == sin(x)**-2\n    assert parse_expr(\"sin^(-2)(x)\", transformations=transformations) == sin(x)**-2\n    assert parse_expr(\"sin**(2/3)(x)\", transformations=transformations) == sin(x)**(2/3)\n    raises(TokenError, lambda: parse_expr(\"sin**(2x)\", transformations=transformations))\n    raises(TokenError, lambda: parse_expr(\"sin**(2x + 1)\", transformations=transformations))\n", "def test_lambda_notation():\n    transformations = standard_transformations + (lambda_notation,)\n\n    x = Symbol('x')\n    y = Symbol('y')\n\n    assert parse_expr('lambda x:x', transformations=transformations) == Function('Lambda')((x,), x)\n    assert parse_expr('lambda x,y:x+y', transformations=transformations) == Function('Lambda')((x, y), x + y)\n    assert parse_expr('(lambda x:x)(2)', transformations=transformations) == 2\n\n    # Make sure that the lambda transformation does not interfere with the rest of the parser.\n    assert parse_expr('lambda', transformations=transformations) == 'lambda'\n\n    raises(TokenError, lambda: parse_expr('lambda x:', transformations=transformations))\n    raises(TokenError, lambda: parse_expr('lambda x,y', transformations=transformations))\n"], "sample_217": ["def test_multiwidget_media_deduplication(self):\n    class MyWidget1(TextInput):\n        class Media:\n            css = {'all': ('/path/to/css1', '/path/to/css2')}\n\n    class MyWidget2(TextInput):\n        class Media:\n            css = {'all': ('/path/to/css1', '/path/to/css3')}\n\n    class MyWidget3(TextInput):\n        class Media:\n            css = {'all': ('/path/to/css2', '/path/to/css3')}\n\n    class MyMultiWidget(MultiWidget):\n            widgets = [MyWidget1, MyWidget2, MyWidget3]\n            super().__init__(widgets, attrs)\n\n    mymulti = MyMultiWidget()\n    self.assertEqual(\n        str(mymulti.media),\n        \"\"\"<link href=\"/path/to/css1\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">", "    def test_multiwidget_media_deduplication(self):\n        class MyWidget1(TextInput):\n            class Media:\n                css = {\n                    'all': ('path/to/css1', '/path/to/css2')\n                }\n                js = ('/path/to/js1', 'http://media.other.com/path/to/js2', 'https://secure.other.com/path/to/js3')\n\n        class MyWidget2(TextInput):\n            class Media:\n                css = {\n                    'all': ('/path/to/css2', '/path/to/css3')\n                }\n                js = ('/path/to/js1', '/path/to/js4')\n\n        class MyWidget3(TextInput):\n            class Media:\n                css = {\n                    'all': ('path/to/css1', '/path/to/css3')\n                }\n                js = ('/path/to/js1', '/path/to/js4')\n\n        class MyMultiWidget(MultiWidget):\n                widgets = [MyWidget1, MyWidget2, MyWidget3]\n                super().__init__(widgets, attrs)\n\n        mymulti = MyMultiWidget()\n        self.assertEqual(\n            str(mymulti.media),\n            \"\"\"<link href=\"http://media.example.com/static/path/to/css1\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">", "def test_select_date_widget(self):\n    ###############################################################\n    # Test SelectDateWidget\n    ###############################################################\n    class MyForm(Form):\n        date_field = CharField(widget=SelectDateWidget)\n\n    form = MyForm()\n    self.assertEqual(\n        form.media,\n        Media(css={}, js=[])\n    )\n\n    class MyWidget(SelectDateWidget):\n        class Media:\n            js = ('/path/to/js1', '/path/to/js2')\n\n    class MyForm(Form):\n        date_field = CharField(widget=MyWidget)\n\n    form = MyForm()\n    self.assertEqual(\n        str(form.media),\n        \"\"\"<script src=\"/path/to/js1\"></script>", "def test_multiwidget_media_deduplication(self):\n    \"\"\"\n    MultiWidgets should remove duplicate media definitions from their component widgets.\n    \"\"\"\n    class MyWidget1(TextInput):\n        class Media:\n            css = {\n                'all': ('/path/to/css1', '/path/to/css2')\n            }\n            js = ('/path/to/js1', '/path/to/js2')\n\n    class MyWidget2(TextInput):\n        class Media:\n            css = {\n                'all': ('/path/to/css1', '/path/to/css2')\n            }\n            js = ('/path/to/js2', '/path/to/js3')\n\n    class MyWidget3(TextInput):\n        class Media:\n            css = {\n                'all': ('/path/to/css3', '/path/to/css4')\n            }\n            js = ('/path/to/js3', '/path/to/js4')\n\n    class MyMultiWidget(MultiWidget):\n            widgets = [MyWidget1, MyWidget2, MyWidget3]\n            super().__init__(widgets, attrs)\n\n    mymulti = MyMultiWidget()\n    self.assertEqual(\n        str(mymulti.media),\n        \"\"\"<link href=\"/path/to/css1\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">", "def test_split_datetime_widget(self):\n    ###############################################################\n    # SplitDateTimeWidget\n    ###############################################################\n    class MyForm(Form):\n        date_time = SplitDateTimeWidget()\n\n    f = MyForm()\n    self.assertEqual(f['date_time'].decompress(datetime.datetime(2022, 1, 1, 12, 0, 0)), [datetime.date(2022, 1, 1), datetime.time(12, 0, 0)])\n    self.assertEqual(f['date_time'].decompress(None), [None, None])\n    self.assertEqual(f['date_time'].format_value(datetime.datetime(2022, 1, 1, 12, 0, 0)), None)\n    self.assertEqual(f['date_time'].needs_multipart_form, False)\n    self.assertEqual(f['date_time'].supports_microseconds, False)\n\n    # Test invalid input values\n    self.assertEqual(f['date_time'].value_from_datadict({'date_time_0': 'invalid', 'date_time_1': 'invalid'}, {}, 'date_time'), [None, None])\n    self.assertEqual(f['date_time'].value_from_datadict({'date_time_0': '2022-01-01', 'date_time_1': 'invalid'}, {}, 'date_time'), [datetime.date(2022, 1, 1), None])\n    self.assertEqual(f['date_time'].value_from_datadict({'date_time_0': 'invalid', 'date_time_1': '12:00:00'}, {}, 'date_time'), [None, datetime.time(12, 0, 0)])\n\n    # Test HTML rendering\n    context = f['date_time'].get_context('date_time', datetime.datetime(2022, 1, 1, 12, 0, 0))\n    self.assertEqual(context['widget']['subwidgets'][0]['value'], '2022-01-01')\n    self.assertEqual(context['widget']['subwidgets'][1]['value'], '12:00:00')\n\n    # Test value_omitted_from_data method\n    data = {'date_time_0': '2022-01-01', 'date_time_1': '12:00:00'}\n    files = {}\n    self.assertFalse(f['date_time'].value_omitted_from_data(data, files, 'date_time'))\n    data = {}\n    self.assertTrue(f['date_time'].value_omitted_from_data(data, files, 'date_time"], "sample_398": ["    def test_login_view_with_custom_success_url_allowed_hosts(self):\n        response = self.client.post(\n            \"/login/custom_success_url_allowed_hosts/\",\n            {\n                \"username\": \"testclient\",\n                \"password\": \"password\",\n                \"next\": \"https://testserver/home\",\n            },\n        )\n        self.assertIn(SESSION_KEY, self.client.session)\n        self.assertRedirects(\n            response, \"https://testserver/home\", fetch_redirect_response=False\n        )\n", "    def test_redirect_authenticated_user_with_custom_redirect(self):\n        self.login()\n        url = \"/login/redirect_authenticated_user_custom_redirect/\"\n        response = self.client.get(url)\n        self.assertRedirects(response, \"/custom/\", fetch_redirect_response=False)\n", "    def test_success_url_allowed_hosts_setting(self):\n        self.login()\n        response = self.client.post(\"/login/allowed_hosts/\")\n        self.assertRedirects(\n            response, \"/accounts/profile/\", fetch_redirect_response=False\n        )\n        self.assertIn(SESSION_KEY, self.client.session)\n", "    def test_context_data(self):\n        user = User.objects.get(email=\"staffmember@example.com\")\n        token = default_token_generator.make_token(user)\n        path = reverse(\"password_reset_confirm\", args=(urlsafe_base64_encode(str(user.pk).encode()), token))\n        response = self.client.get(path)\n        self.assertEqual(response.status_code, 200)\n        self.assertIn(\"validlink\", response.context)\n        self.assertIn(\"form\", response.context)\n        self.assertEqual(response.context[\"validlink\"], True)\n        self.assertIsInstance(response.context[\"form\"], SetPasswordForm)\n", "    def test_login_view_csrf_protect(self):\n        response = self.client.get(reverse(\"login_csrf_protect\"))\n        self.assertEqual(response.status_code, 403)\n        self.assertContains(response, \"Forbidden (CSRF cookie not set.)\")\n"], "sample_1129": ["def test_DiagonalMatrix():\n    from sympy import MatrixSymbol, DiagonalMatrix\n\n    expr = DiagonalMatrix(MatrixSymbol(\"D\", 3, 4))\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == \"numpy.multiply(D, numpy.eye(3, 4))\"\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == \"numpy.multiply(D, numpy.eye(3, 4))\"\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == \"numpy.multiply(D, numpy.eye(3, 4))\"\n\n", "def test_AbstractPythonCodePrinter_einsum_string():\n    from sympy.tensor import IndexedBase, Indexed\n\n    A = IndexedBase('A')\n    i, j, k, l = symbols('i j k l')\n    expr = Indexed(A, i, j) + Indexed(A, k, l)\n\n    prntr = AbstractPythonCodePrinter()\n    result = prntr._get_einsum_string([(2,2), (2,2)], [(0, 2), (1, 3)])\n    expected = ('ij,kl,i,j', ['i', 'j'], ['i', 'j'])\n    assert result == expected\n", "def test_AbstractPythonCodePrinter_einsum():\n    prntr = AbstractPythonCodePrinter()\n\n    from sympy.tensor import IndexedBase\n    A, B, C = map(IndexedBase, 'ABC')\n    i, j, k = symbols('i j k')\n\n    # simple contraction\n    expr = A[i, j]*B[j, k]\n    assert prntr.doprint(expr) == 'numpy.einsum(\"ij,jk->ik\", A, B)'\n\n    # contraction with multiple sums\n    expr = A[i, j]*B[j, k]*C[k, i]\n    assert prntr.doprint(expr) == 'numpy.einsum(\"ijk,ki->\", A, C, B)'\n\n    # contraction with multiple sums and free indices\n    expr = A[i, j]*B[j, k]*C[k, i]\n    assert prntr.doprint(expr) == 'numpy.einsum(\"ijk,ki->\", A, C, B)'\n", "def test_AbstractPythonCodePrinter_fully_qualified_modules():\n    prntr = PythonCodePrinter({'fully_qualified_modules': True})\n    assert prntr.doprint(pi) == 'math.pi'\n\n    prntr = PythonCodePrinter({'fully_qualified_modules': False})\n    assert prntr.doprint(pi) == 'pi'\n", "def test_AbstractPythonCodePrinter():\n    p = AbstractPythonCodePrinter()\n    raises(NotImplementedError, lambda: p.doprint(x))\n    assert p._print_Infinity(S.Infinity) == \"float('inf')\"\n    assert p._print_NegativeInfinity(S.NegativeInfinity) == \"float('-inf')\"\n    assert p._print_NaN(S.NaN) == \"float('nan')\"\n    assert p._print_ComplexInfinity(S.ComplexInfinity) == \"float('nan')\"\n    assert p._print_Piecewise(Piecewise((x, Eq(y, 0)), (y, True))) == \\\n        '((x) if (y == 0) else (y) if (True) else None)'\n    assert p._print_Sum(Sum(x, (x, 0, 1))) == '(builtins.sum((x for x in range(0, 1+1)))'\n    assert p._print_ImaginaryUnit(S.ImaginaryUnit) == '1j'\n    assert p._print_KroneckerDelta(KroneckerDelta(x, y)) == '(1 if x == y else 0)'\n    assert p._print_MatrixBase(Matrix([[1, 2], [3, 4]])) == 'matrix([[1, 2], [3, 4]])'\n    assert p._print_MatrixBase(SparseMatrix(2, 2, {0: 1})) == 'matrix([[1, 0], [0, 0]])'\n    assert p._print_FunctionDefinition(FunctionDefinition(S('f'), [S('x'), S('y')], [S('x') + S('y')])) == \\\n        'def f(x, y):\\n    x + y'\n    assert p._print_While(While(x < 5, [x + 1])) == 'while x < 5:\\n    x + 1'\n    assert p._print_Declaration(Declaration(S('x'), 5)) == 'x = 5'\n    assert p._print_Return(Return(x)) == 'return x'\n    assert p._print_Print(Print([S('Hello, World!')])) == \"print('Hello, World!')\"\n    assert p._print_Print(Print([S('Hello, World!')], S('format_string'), S('file'))) == \\\n        \"print('Hello, World! % (file)', file=file)\"\n    assert p._print_Stream"], "sample_735": ["def test_gaussian_mixture_predict_proba_sum():\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng)\n    for covar_type in COVARIANCE_TYPE:\n        X = rand_data.X[covar_type]\n        g = GaussianMixture(n_components=rand_data.n_components,\n                            random_state=rng, weights_init=rand_data.weights,\n                            means_init=rand_data.means,\n                            precisions_init=rand_data.precisions[covar_type],\n                            covariance_type=covar_type)\n        g.fit(X)\n        assert_array_almost_equal(g.predict_proba(X).sum(axis=1), np.ones(len(X)))\n", "def test_predict_proba():\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng)\n    n_components = rand_data.n_components\n\n    for covar_type in COVARIANCE_TYPE:\n        X = rand_data.X[covar_type]\n        gmm = GaussianMixture(n_components=n_components, random_state=rng,\n                              weights_init=rand_data.weights,\n                              means_init=rand_data.means,\n                              precisions_init=rand_data.precisions[covar_type],\n                              covariance_type=covar_type)\n\n        # Check a warning message arrive if we don't do fit\n        assert_raise_message(NotFittedError,\n                             \"This GaussianMixture instance is not fitted \"\n                             \"yet. Call 'fit' with appropriate arguments \"\n                             \"before using this method.\", gmm.predict_proba, X)\n\n        gmm.fit(X)\n        y_pred = gmm.predict_proba(X)\n        assert_equal(y_pred.shape, (rand_data.n_samples, n_components))\n        assert_almost_equal(y_pred.sum(axis=1), np.ones(rand_data.n_samples))\n", "def test_gaussian_mixture_predict_proba_on_new_data():\n    # test whether the responsibilities are still normalized\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng, scale=5)\n    n_samples = rand_data.n_samples\n    n_features = rand_data.n_features\n    n_components = rand_data.n_components\n\n    X = rng.rand(n_samples, n_features)\n    for covar_type in COVARIANCE_TYPE:\n        weights = rand_data.weights\n        means = rand_data.means\n        precisions = rand_data.precisions[covar_type]\n        g = GaussianMixture(n_components=n_components, random_state=rng,\n                            weights_init=weights, means_init=means,\n                            precisions_init=precisions,\n                            covariance_type=covar_type)\n        g.fit(X)\n\n        # test the responsibilities on a new sample\n        X_new = rng.rand(10, n_features)\n        resp = g.predict_proba(X_new)\n        assert_array_almost_equal(resp.sum(axis=1), np.ones(10))\n", "def test_gaussian_mixture_n_features():\n    # Test that the right number of features is used\n    rng = np.random.RandomState(0)\n    n_samples, n_features, n_components = 500, 2, 2\n    X = rng.rand(n_samples, n_features)\n\n    g = GaussianMixture(n_components=n_components, random_state=rng).fit(X)\n    assert_equal(g.means_.shape[1], n_features)\n    assert_equal(g.covariances_.shape[-1], n_features)\n\n    # Test that an error is raised when the number of features is not equal to\n    # the number of features used to fit the model\n    X_bad_features = rng.rand(n_samples, n_features + 1)\n    assert_raise_message(ValueError, \"Expected the input data X have %d \"\n                         \"features, but got %d features\" % (n_features,\n                                                            X_bad_features.shape[1]),\n                         g.predict_proba, X_bad_features)\n", "def test_m_step():\n    # Test that the M-step updates are computed correctly.\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng)\n    n_components = rand_data.n_components\n    n_features = rand_data.n_features\n\n    for covar_type in COVARIANCE_TYPE:\n        X = rand_data.X[covar_type]\n        gmm = GaussianMixture(n_components=n_components,\n                              covariance_type=covar_type, random_state=rng)\n\n        log_resp = np.log(np.random.dirichlet(np.ones(n_components),\n                                               size=len(X)))\n\n        # Test the weights update\n        expected_weights = np.exp(log_resp).mean(axis=0)\n        gmm.weights_ = expected_weights\n\n        # Test the means update\n        expected_means = (np.exp(log_resp)[:, :, np.newaxis] * X).sum(axis=0) / np.exp(log_resp).sum(axis=0)[:, np.newaxis]\n        gmm.means_ = expected_means\n\n        # Test the covariances update\n        if covar_type == 'full':\n            expected_covariances = np.array([np.cov(X - gmm.means_[k], rowvar=False, aweights=np.exp(log_resp[:, k]))\n                                            for k in range(n_components)])\n        elif covar_type == 'tied':\n            expected_covariances = np.cov(X - gmm.means_, rowvar=False, aweights=np.exp(log_resp).sum(axis=1))\n        elif covar_type == 'diag':\n            expected_covariances = np.array([np.diag(np.var(X - gmm.means_[k], axis=0, ddof=0, aweights=np.exp(log_resp[:, k])))\n                                            for k in range(n_components)])\n        elif covar_type == 'spherical':\n            expected_covariances = np.array([np.var(X - gmm.means_[k], axis=0, ddof=0, aweights=np.exp(log_resp[:, k])).mean()\n                                            for k in range(n_components)])\n\n        gmm._m_step(X, log_resp)\n        assert_array_almost_equal(gmm.weights_, expected_weights)\n        assert_array_almost_equal(gmm.means_, expected_means)\n        assert_array_almost_equal(gmm.covariances_, expected_covariances)\n"], "sample_1186": ["def test_array_kind():\n    from sympy.tensor.array import ArrayKind\n    from sympy.core.kind import NumberKind, UndefinedKind\n\n    # Test default element kind\n    array_kind = ArrayKind()\n    assert array_kind.element_kind == NumberKind\n\n    # Test specified element kind\n    array_kind = ArrayKind(UndefinedKind)\n    assert array_kind.element_kind == UndefinedKind\n\n    # Test repr\n    assert repr(array_kind) == \"ArrayKind(UndefinedKind)\"\n\n    # Test union of kinds\n    kinds = [ArrayKind(NumberKind), ArrayKind(NumberKind)]\n    union_kind = ArrayKind._union(kinds)\n    assert union_kind.element_kind == NumberKind\n\n    kinds = [ArrayKind(NumberKind), ArrayKind(UndefinedKind)]\n    union_kind = ArrayKind._union(kinds)\n    assert union_kind.element_kind == UndefinedKind\n", "def test_applyfunc():\n    for ArrayType in array_types:\n        A = ArrayType([[1, 2], [3, 4]])\n        assert A.applyfunc(lambda x: x**2) == ArrayType([[1, 4], [9, 16]])\n", "def test_ndim_array_rank_and_shape():\n    for ArrayType in array_types:\n        # Test rank and shape of 1-dimensional array\n        array_1d = ArrayType([1, 2, 3])\n        assert array_1d.rank() == 1\n        assert array_1d.shape == (3,)\n\n        # Test rank and shape of 2-dimensional array\n        array_2d = ArrayType([[1, 2, 3], [4, 5, 6]])\n        assert array_2d.rank() == 2\n        assert array_2d.shape == (2, 3)\n\n        # Test rank and shape of 3-dimensional array\n        array_3d = ArrayType([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n        assert array_3d.rank() == 3\n        assert array_3d.shape == (2, 2, 3)\n", "def test_array_kind():\n    from sympy.tensor.array import ArrayKind\n    from sympy.core.kind import NumberKind\n    from sympy.core.singleton import S\n\n    assert ArrayKind(NumberKind) == ArrayKind(NumberKind)\n    assert ArrayKind(NumberKind).element_kind == NumberKind\n    assert ArrayKind(NumberKind) != ArrayKind(S.BooleanKind)\n\n    assert ArrayKind._union([ArrayKind(NumberKind), ArrayKind(NumberKind)]) == ArrayKind(NumberKind)\n    assert ArrayKind._union([ArrayKind(NumberKind), ArrayKind(S.BooleanKind)]) == ArrayKind(S.UndefinedKind)\n", "def test_immutable_ndim_array_operations():\n    for ArrayType in array_types:\n        A = ArrayType([[1, 2], [3, 4]])\n        B = ArrayType([[5, 6], [7, 8]])\n\n        assert (A + B).tolist() == [[6, 8], [10, 12]]\n        assert (A - B).tolist() == [[-4, -4], [-4, -4]]\n        assert (A * 2).tolist() == [[2, 4], [6, 8]]\n        assert (2 * A).tolist() == [[2, 4], [6, 8]]\n        assert (A / 2).tolist() == [[1/2, 1], [3/2, 2]]\n        assert (-A).tolist() == [[-1, -2], [-3, -4]]\n\n        raises(ValueError, lambda: A + [1, 2])\n        raises(ValueError, lambda: A * [1, 2])\n        raises(ValueError, lambda: A / [1, 2])\n        raises(ValueError, lambda: A - [1, 2])\n        raises(ValueError, lambda: A / 0)\n        raises(ValueError, lambda: 0 / A)\n"], "sample_95": ["    def test_patch_vary_headers(self):\n        response = HttpResponse()\n        patch_vary_headers(response, ['Accept-Encoding', 'User-Agent'])\n        self.assertEqual(response['Vary'], 'Accept-Encoding, User-Agent')\n", "    def test_patch_cache_control(self):\n        response = HttpResponse()\n        patch_cache_control(response, public=True, max_age=3600)\n        self.assertEqual(response['Cache-Control'], 'public, max-age=3600')\n", "    def test_patch_cache_control(self):\n        response = HttpResponse()\n        patch_cache_control(response, max_age=3600, public=True)\n        self.assertEqual(response['Cache-Control'], 'max-age=3600, public')\n\n        patch_cache_control(response, private=True)\n        self.assertEqual(response['Cache-Control'], 'max-age=3600, private')\n", "    def test_patch_cache_control_adds_headers(self):\n        response = HttpResponse()\n        patch_cache_control(response, max_age=3600, public=True)\n        self.assertEqual(response['Cache-Control'], 'max-age=3600,public')\n", "    def test_patch_cache_control(self):\n        response = HttpResponse()\n        patch_cache_control(response, max_age=3600, public=True)\n        self.assertEqual(response['Cache-Control'], 'max-age=3600, public')\n"], "sample_844": ["def test_predecessor_correction():\n    rng = np.random.RandomState(0)\n    n_points_per_cluster = 100\n    C1 = [0, 0] + 2 * rng.randn(n_points_per_cluster, 2)\n    C2 = [0, 0] + 50 * rng.randn(n_points_per_cluster, 2)\n    X = np.vstack((C1, C2))\n    X = shuffle(X, random_state=0)\n\n    clusters1 = OPTICS(min_samples=20, xi=.1, predecessor_correction=True).fit(X).cluster_hierarchy_\n    clusters2 = OPTICS(min_samples=20, xi=.1, predecessor_correction=False).fit(X).cluster_hierarchy_\n\n    assert not np.array_equal(clusters1, clusters2)\n", "def test_n_jobs_parameter(n_jobs, error_type, error_msg):\n    clust = OPTICS(n_jobs=n_jobs)\n    with pytest.raises(error_type, match=error_msg):\n        clust.fit(X)\n", "def test_n_jobs_parameter():\n    # Test parallel execution\n    X = np.random.rand(1000, 5)\n    for n_jobs in [1, -1]:\n        clust1 = OPTICS(min_samples=10, n_jobs=1).fit(X)\n        clust2 = OPTICS(min_samples=10, n_jobs=n_jobs).fit(X)\n\n        assert_array_equal(clust1.labels_, clust2.labels_)\n        assert_array_equal(clust1.reachability_, clust2.reachability_)\n        assert_array_equal(clust1.core_distances_, clust2.core_distances_)\n        assert_array_equal(clust1.ordering_, clust2.ordering_)\n        assert_array_equal(clust1.predecessor_, clust2.predecessor_)\n", "def test_min_samples_and_min_cluster_size():\n    # test that min_samples and min_cluster_size don't exceed the total number of samples\n    C1 = [[0, 0], [0, 0.1], [0, -.1], [0.1, 0]]\n    C2 = [[10, 10], [10, 9], [10, 11]]\n    C3 = [[100, 100], [100, 96], [100, 106]]\n    X = np.vstack((C1, C2, C3))\n\n    clust = OPTICS(min_samples=11,\n                   max_eps=7, cluster_method='xi',\n                   xi=0.04)\n    with pytest.raises(ValueError, match=\"min_samples must be no greater than\"):\n        clust.fit(X)\n\n    clust = OPTICS(min_samples=3,\n                   min_cluster_size=11,\n                   max_eps=7, cluster_method='xi',\n                   xi=0.04)\n    with pytest.raises(ValueError, match=\"min_cluster_size must be no greater than\"):\n        clust.fit(X)\n\n    clust = OPTICS(min_samples=3,\n                   min_cluster_size=3,\n                   max_eps=7, cluster_method='xi',\n                   xi=0.04)\n    clust.fit(X)\n", "def test_fraction_min_samples(min_samples):\n    clust = OPTICS(min_samples=min_samples)\n    assert clust.min_samples == int(min_samples * X.shape[0])\n\n    # check that min_samples is rounded up to the nearest integer\n    clust = OPTICS(min_samples=0.01)\n    assert clust.min_samples == 2\n"], "sample_158": ["    def test_foreign_object_to_unique_field(self):\n        class Person(models.Model):\n            unique_id = models.IntegerField(unique=True)\n\n        class MMembership(models.Model):\n            person_unique_id = models.IntegerField()\n            person = models.ForeignObject(\n                Person,\n                on_delete=models.SET_NULL,\n                from_fields=['person_unique_id'],\n                to_fields=['unique_id'],\n            )\n\n        field = MMembership._meta.get_field('person')\n        self.assertEqual(field.check(from_model=MMembership), [])\n", "    def test_foreign_object_to_unique_fields(self):\n        class Person(models.Model):\n            country_id = models.IntegerField(unique=True)\n            city_id = models.IntegerField(unique=True)\n\n        class MMembership(models.Model):\n            person_country_id = models.IntegerField()\n            person_city_id = models.IntegerField()\n            person = models.ForeignObject(\n                Person,\n                on_delete=models.CASCADE,\n                from_fields=['person_country_id', 'person_city_id'],\n                to_fields=['country_id', 'city_id'],\n            )\n\n        field = MMembership._meta.get_field('person')\n        self.assertEqual(field.check(from_model=MMembership), [])\n", "    def test_recursive_relationship_constant(self):\n        self.assertEqual(resolve_relation(models.Model, RECURSIVE_RELATIONSHIP_CONSTANT), models.Model)\n", "    def test_clashing_m2m_through_model(self):\n        class Group(models.Model):\n            pass\n\n        class GroupMember(models.Model):\n            group = models.ForeignKey(Group, models.CASCADE)\n            person = models.ForeignKey('Person', models.CASCADE)\n\n        class Person(models.Model):\n            groups = models.ManyToManyField(Group, through='GroupMember')\n\n        with self.assertRaisesMessage(ValueError, \"The field's intermediary table 'invalid_models_tests_groupmember' clashes with the table name of 'GroupMember'.\"):\n            GroupMember.check()\n", "    def test_unique_constraint_with_condition(self):\n        class Model(models.Model):\n            other = models.ForeignKey('self', models.CASCADE)\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(fields=['pk'], condition=models.Q(other__isnull=True), name='unique_constraint'),\n                ]\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"Field defines a relation with model 'Model', but it has a conditional unique constraint \"\n                \"that is not met when the related field is not null.\",\n                hint=\"Add a constraint that covers all possible values of the related field.\",\n                obj=Model._meta.get_field('other'),\n                id='fields.E323',\n            ),\n        ])\n"], "sample_961": ["def test_python_domain_get_full_qualified_name():\n    env = Mock(domaindata={})\n    domain = PythonDomain(env)\n\n    # non-python references\n    node = nodes.reference()\n    assert domain.get_full_qualified_name(node) is None\n\n    # simple reference\n    node = nodes.reference(reftarget='func')\n    assert domain.get_full_qualified_name(node) == 'func'\n\n    # with py:module context\n    kwargs = {'py:module': 'module1'}\n    node = nodes.reference(reftarget='func', **kwargs)\n    assert domain.get_full_qualified_name(node) == 'module1.func'\n\n    # with py:class context\n    kwargs = {'py:class': 'Class'}\n    node = nodes.reference(reftarget='func', **kwargs)\n    assert domain.get_full_qualified_name(node) == 'Class.func'\n\n    # with both py:module and py:class context\n    kwargs = {'py:module': 'module1', 'py:class': 'Class'}\n    node = nodes.reference(reftarget='func', **kwargs)\n    assert domain.get_full_qualified_name(node) == 'module1.Class.func'\n", "def test_pyattribute_with_union_type_operator(app, status, warning):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :type: int | str\\n\"\n            \"      :value: ''\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'attr (Class attribute)', 'Class.attr', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"attr\"],\n                                                     [desc_annotation, (\": \",\n                                                                        [pending_xref, \"int\"],\n                                                                        \" \",\n                                                                        [desc_sig_punctuation, \"|\"],\n                                                                        \" \",\n                                                                        [pending_xref, \"str\"])],\n                                                     [desc_annotation, \" = ''\"])],\n                                   [desc_content, ()]))\n    assert_node(doctree[1][1][1][0][1][1], pending_xref, **{\"py:class\": \"Class\"})\n    assert_node(doctree[1][1][1][0][1][3], pending_xref, **{\"py:class\": \"Class\"})\n    assert 'Class.attr' in domain.objects\n    assert domain.objects['Class.attr'] == ('index', 'Class.attr', 'attribute', False)\n", "def test_type_to_xref(app):\n    text = \"int\"\n    result = PythonDomain.type_to_xref(text)\n    assert_node(result, pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=text)\n\n    text = \"None\"\n    result = PythonDomain.type_to_xref(text)\n    assert_node(result, pending_xref, refdomain=\"py\", reftype=\"obj\", reftarget=text)\n", "def test_parse_annotation_literal(app):\n    doctree = _parse_annotation(\"Literal['a', 'b']\", app.env)\n    assert_node(doctree, ([pending_xref, \"Literal\"],\n                          [desc_sig_punctuation, \"[\"],\n                          \"'a'\",\n                          [desc_sig_punctuation, \", \"],\n                          \"'b'\",\n                          [desc_sig_punctuation, \"]\"]))\n\n    doctree = _parse_annotation(\"Literal[1, 2, True]\", app.env)\n    assert_node(doctree, ([pending_xref, \"Literal\"],\n                          [desc_sig_punctuation, \"[\"],\n                          \"1\",\n                          [desc_sig_punctuation, \", \"],\n                          \"2\",\n                          [desc_sig_punctuation, \", \"],\n                          \"True\",\n                          [desc_sig_punctuation, \"]\"]))\n\n    doctree = _parse_annotation(\"typing.Literal[True, False, None]\", app.env)\n    assert_node(doctree, ([pending_xref, \"typing.Literal\"],\n                          [desc_sig_punctuation, \"[\"],\n                          \"True\",\n                          [desc_sig_punctuation, \", \"],\n                          \"False\",\n                          [desc_sig_punctuation, \", \"],\n                          \"None\",\n                          [desc_sig_punctuation, \"]\"]))\n", "def test_type_to_xref():\n    env = Mock()\n    assert_node(type_to_xref('int', env),\n                pending_xref, refdomain='py', reftype='class', reftarget='int',\n                pending_xref_condition, nodes.Text, 'int')\n    assert_node(type_to_xref('module.int', env),\n                pending_xref, refdomain='py', reftype='class', reftarget='module.int',\n                pending_xref_condition, nodes.Text, 'int')\n                pending_xref_condition, nodes.Text, 'module.int')\n    assert_node(type_to_xref('None', env),\n                pending_xref, refdomain='py', reftype='obj', reftarget='None',\n                pending_xref_condition, nodes.Text, 'None')\n"], "sample_1016": ["def test_octave_user_function():\n    f = Function('f')\n    g = Function('g')\n    custom_functions = {\n        \"f\": \"existing_octave_fcn\",\n        \"g\": [(lambda x: x.is_Matrix, \"my_mat_fcn\"),\n              (lambda x: not x.is_Matrix, \"my_fcn\")]\n    }\n    expr = f(x) + g(x) + g(Matrix([[x]]))\n    expected = \"existing_octave_fcn(x) + my_fcn(x) + my_mat_fcn([x])\"\n    assert mcode(expr, user_functions=custom_functions) == expected\n", "def test_octave_codeprinter_settings():\n    printer = OctaveCodePrinter({\"inline\": False})\n    assert printer._settings[\"inline\"] is False\n    assert printer._settings[\"contract\"] is True\n\n    printer = OctaveCodePrinter({\"contract\": False})\n    assert printer._settings[\"contract\"] is False\n    assert printer._settings[\"inline\"] is True\n\n    printer = OctaveCodePrinter({\"inline\": False, \"contract\": False})\n    assert printer._settings[\"contract\"] is False\n    assert printer._settings[\"inline\"] is False\n\n    raises(ValueError, lambda: OctaveCodePrinter({\"order\": \"invalid\"}))\n", "def test_MatMul():\n    from sympy import MatrixSymbol, Matrix\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    assert mcode(A*B) == \"A*B\"\n    assert mcode(2*A*B) == \"2*A*B\"\n    assert mcode(A*B*2) == \"A*B*2\"\n    assert mcode(A*(B + 3*Matrix(eye(3)))) == \"A*(3*eye(3) + B)\"\n    assert mcode(A + B) == \"A + B\"\n    assert mcode(A - B) == \"A - B\"\n", "def test_octave_matrix_slice():\n    A = MatrixSymbol('A', 3, 3)\n    assert mcode(A[1:3, 1:2]) == \"A(2:3, 2)\"\n    assert mcode(A[1:3, :]) == \"A(2:3, :)\"\n    assert mcode(A[:, 1:2]) == \"A(:, 2)\"\n    assert mcode(A[1:3, 1:2]) == \"A(2:3, 2)\"\n    assert mcode(A[:2, 1]) == \"A(1:2, 2)\"\n    assert mcode(A[:2, :]) == \"A(1:2, :)\"\n    assert mcode(A[1, :]) == \"A(2, :)\"\n    assert mcode(A[1:2, 1:2]) == \"A(2, 2)\"\n    assert mcode(A[1:2, :]) == \"A(2, :)\"\n    assert mcode(A[1:2, 0]) == \"A(2, 1)\"\n    assert mcode(A[1:2, 1]) == \"A(2, 2)\"\n    assert mcode(A[:, 1]) == \"A(:, 2)\"\n    assert mcode(A[:, 0]) == \"A(:, 1)\"\n    assert mcode(A[0:2, 1:3]) == \"A(1:2, 2:3)\"\n    assert mcode(A[0:2, :]) == \"A(1:2, :)\"\n    assert mcode(A[1, 1:3]) == \"A(2, 2:3)\"\n    assert mcode(A[:, 1:2]) == \"A(:, 2)\"\n    assert mcode(A[2, 1]) == \"A(3, 2)\"\n    assert mcode(A[1:3, 1:2]) == \"A(2:3, 2)\"\n    assert mcode(A[:2, 1]) == \"A(1:2, 2)\"\n    assert mcode(A[:2, :]) == \"A(1:2, :)\"\n    assert mcode(A[1, :]) == \"A(2, :)\"\n    assert mcode(A[1:2, 1:2]) == \"A", "def test_MatrixSlice_printing():\n    # test cases for issue #11821\n    A = MatrixSymbol(\"A\", 3, 3)\n    B = MatrixSymbol(\"B\", 3, 3)\n    C = MatrixSymbol(\"C\", 3, 3)\n    I = S('I')\n\n    assert mcode(A[:2, 1:]) == \"A(1:2, 2:3)\"\n    assert mcode(A[1:3, :]) == \"A(2:3, 1:3)\"\n    assert mcode(A[1:, 1:]) == \"A(2:3, 2:3)\"\n    assert mcode(A[1:, :2]) == \"A(2:3, 1:2)\"\n    assert mcode(A[:2, :2]) == \"A(1:2, 1:2)\"\n    assert mcode(A[:, :]) == \"A(:, :)\"\n    assert mcode(A[:, 1:]) == \"A(:, 2:3)\"\n    assert mcode(A[1:, 0]) == \"A(2:3, 1)\"\n    assert mcode(A[0, 1:]) == \"A(1, 2:3)\"\n    assert mcode(A[0:3, 0:3]) == \"A(1:3, 1:3)\"\n    assert mcode(A[:, 2]) == \"A(:, 3)\"\n    assert mcode(A[0, 0]) == \"A(1, 1)\"\n    assert mcode(A[0, 1:3]) == \"A(1, 2:3)\"\n    assert mcode(A[0:1, 0:3]) == \"A(1, 1:3)\"\n    assert mcode(A[0, :]) == \"A(1, :)\"\n    assert mcode(A[:, 0]) == \"A(:, 1)\"\n    assert mcode(A[:, :3]) == \"A(:, 1:3)\"\n    assert mcode(A[1:3, :]) == \"A(2:3, :)\"\n    assert mcode(A[1:3, 1:3]) == \"A(2:3, 2:3)\"\n    assert mcode(A[:, 1]) == \"A(:, 2)\"\n    assert mcode(A[:, 1:2]) == \"A"], "sample_92": ["    def setUpTestData(cls):\n        cls.user = User.objects.create_user('test', 'test@example.com', 'test')\n        cls.user.is_active = False\n        cls.user.save()\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user(\n            username=cls.remote_user,\n            email='test@example.com',\n            is_active=False,\n        )\n", "    def setUpTestData(cls):\n        cls.user1 = User.objects.create_user('test', 'test@example.com', 'test')\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user('test', 'test@example.com', 'test')\n", "    def setUp(self):\n        self.patched_settings = modify_settings(\n            AUTHENTICATION_BACKENDS={'append': self.backend},\n        )\n        self.patched_settings.enable()\n"], "sample_777": ["def test_gradient_boosting_feature_importances():\n    X, y = make_classification(n_samples=100, n_features=5, n_informative=3,\n                               random_state=42)\n    gbc = GradientBoostingClassifier(n_estimators=100, random_state=42)\n    gbc.fit(X, y)\n    fi = gbc.feature_importances_\n    assert len(fi) == 5\n    assert np.sum(fi) == 1.0\n    assert np.all(fi >= 0)\n\n    gbr = GradientBoostingRegressor(n_estimators=100, random_state=42)\n    gbr.fit(X, y)\n    fi = gbr.feature_importances_\n    assert len(fi) == 5\n    assert np.sum(fi) == 1.0\n    assert np.all(fi >= 0)\n", "def test_gradient_boosting_init_custom(estimator, init):\n    # Check that GradientBoostingRegressor works when init is a custom\n    # estimator.\n    X, y = datasets.make_hastie_10_2()\n    estimator(loss='ls' if estimator == GradientBoostingRegressor else 'deviance',\n              init=init).fit(X, y)\n", "def test_gradient_boosting_init_loss_check():\n    # Test that GradientBoostingRegressor checks the init estimator matches the loss\n    # Check that an error is raised if the init estimator does not match the loss\n    X, y = datasets.make_regression(n_samples=100, random_state=0)\n\n    gbc = GradientBoostingClassifier(loss='deviance', init='zero')\n    with pytest.raises(ValueError,\n                       match=\"The init parameter must be an estimator or \"\n                             \"'zero'\"):\n        gbc.fit(X, y)\n\n    gbr = GradientBoostingRegressor(loss='ls', init='zero')\n    with pytest.raises(ValueError,\n                       match=\"The init parameter must be an estimator or \"\n                             \"'zero'\"):\n        gbr.fit(X, y)\n", "def test_gradient_boosting_persistence(EstimatorClass):\n    # Check that the Gradient Boosting estimators can be persisted and still\n    # behave the same.\n    import pickle\n\n    X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)\n\n    est = EstimatorClass(n_estimators=10, max_depth=1, random_state=1,\n                         subsample=0.5)\n    est.fit(X, y)\n    y_pred_before = est.predict(X)\n\n    serialized_est = pickle.dumps(est)\n    deserialized_est = pickle.loads(serialized_est)\n\n    assert_array_almost_equal(deserialized_est.predict(X), y_pred_before)\n\n", "def test_gradient_boosting_regression_loss_check():\n    # Test if error is raised for unsupported loss function in regression.\n    X, y = make_regression(n_samples=100, random_state=42)\n    est = GradientBoostingRegressor(n_estimators=50, learning_rate=0.1,\n                                    max_depth=3, random_state=42)\n\n    # Try to fit with an unsupported loss function\n    est.set_params(loss='exponential')\n    with pytest.raises(ValueError):\n        est.fit(X, y)\n"], "sample_646": ["def test_unittest_subtest(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n                for i in range(5):\n                    with self.subTest(i=i):\n                        assert i < 3\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    passed, skipped, failed = reprec.countoutcomes()\n    assert passed == 3\n    assert failed == 2\n", "def test_addCleanup_before_setupclass(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n            values = []\n\n            @classmethod\n                    cls.values.append(1)\n                cls.addClassCleanup(cleanup)\n                assert False\n\n                pass\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    reprec.assertoutcome(failed=1)\n    assert MyTestCase.values == [1]\n", "def test_unittest_addCleanup_with_exception_in_setup(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n                    raise Exception(\"Cleanup failed\")\n                self.addCleanup(cleanup)\n                assert False\n\n                pass\n    \"\"\"\n    )\n    result = pytester.runpytest(\"-v\", testpath)\n    result.stdout.fnmatch_lines(\n        [\n            \"*Cleanup failed*\",\n            \"*1 failed*\",\n        ]\n    )\n", "def test_incompatible_exception_info_with_incompatible_repr(pytester: Pytester) -> None:\n    import _pytest.unittest\n\n    (item,) = pytester.getitems(\n        \"\"\"\n        from unittest import TestCase\n        class MyTestCase(TestCase):\n                pass\n    \"\"\"\n    )\n    assert isinstance(item, _pytest.unittest.TestCaseFunction)\n    excinfo = _pytest._code.ExceptionInfo()\n    excinfo.value = type(\"MyException\", (BaseException,), {\"__repr__\": lambda self: None})\n    item.addError(None, (None, excinfo.value, None))  # type: ignore[arg-type]\n    excinfo = item._excinfo\n    assert excinfo is not None\n    assert \"ERROR: Unknown Incompatible\" in str(excinfo.pop(0).getrepr())\n", "def test_unittest_runtest_mixin(pytester: Pytester) -> None:\n    \"\"\"Test that the unittest.TestCase.runTest mixin works correctly.\"\"\"\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class Mixin:\n                pass\n\n        class MyTestCase(unittest.TestCase, Mixin):\n                pass\n\n                super().runTest()\n                assert False\n        \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    passed, skipped, failed = reprec.countoutcomes()\n    assert passed == 1\n    assert failed == 1\n    assert reprec.ret == 1\n"], "sample_471": ["    def test_floatfield_1(self):\n        f = FloatField()\n        self.assertWidgetRendersTo(\n            f, '<input type=\"number\" name=\"f\" id=\"id_f\" required>'\n        )\n        with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n            f.clean(\"\")\n        with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n            f.clean(None)\n        self.assertEqual(1.0, f.clean(\"1\"))\n        self.assertIsInstance(f.clean(\"1\"), float)\n        self.assertEqual(23.0, f.clean(\"23\"))\n        with self.assertRaisesMessage(ValidationError, \"'Enter a number.'\"):\n            f.clean(\"a\")\n        self.assertEqual(42.0, f.clean(42))\n        self.assertEqual(3.14, f.clean(\"3.14\"))\n        self.assertEqual(1.0, f.clean(\"1 \"))\n        self.assertEqual(1.0, f.clean(\" 1\"))\n        self.assertEqual(1.0, f.clean(\" 1 \"))\n        with self.assertRaisesMessage(ValidationError, \"'Enter a number.'\"):\n            f.clean(\"1a\")\n        self.assertIsNone(f.max_value)\n        self.assertIsNone(f.min_value)\n", "def test_floatfield_1(self):\n    f = FloatField()\n    self.assertWidgetRendersTo(\n        f, '<input id=\"id_f\" name=\"f\" step=\"any\" type=\"number\" required>'\n    )\n    with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n        f.clean(\"\")\n    with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n        f.clean(None)\n    self.assertEqual(1.0, f.clean(\"1\"))\n    self.assertIsInstance(f.clean(\"1\"), float)\n    self.assertEqual(23.5, f.clean(\"23.5\"))\n    with self.assertRaisesMessage(ValidationError, \"'Enter a number.'\"):\n        f.clean(\"a\")\n    self.assertEqual(1.0, f.clean(1.0))\n    self.assertEqual(3.14, f.clean(3.14))\n    self.assertEqual(1.0, f.clean(\"1 \"))\n    self.assertEqual(1.0, f.clean(\" 1\"))\n    self.assertEqual(1.0, f.clean(\" 1 \"))\n    with self.assertRaisesMessage(ValidationError, \"'Enter a number.'\"):\n        f.clean(\"1a\")\n    self.assertIsNone(f.max_value)\n    self.assertIsNone(f.min_value)\n", "def test_floatfield(self):\n    f = FloatField()\n    self.assertWidgetRendersTo(\n        f, '<input type=\"number\" name=\"f\" id=\"id_f\" step=\"any\" required>'\n    )\n    with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n        f.clean(\"\")\n    with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n        f.clean(None)\n    self.assertEqual(1.0, f.clean(\"1\"))\n    self.assertIsInstance(f.clean(\"1\"), float)\n    self.assertEqual(23.5, f.clean(\"23.5\"))\n    with self.assertRaisesMessage(ValidationError, \"'Enter a number.'\"):\n        f.clean(\"a\")\n    self.assertEqual(42.0, f.clean(42))\n    self.assertEqual(3.14, f.clean(\"3.14\"))\n    self.assertEqual(1.0, f.clean(\"1 \"))\n    self.assertEqual(1.0, f.clean(\" 1\"))\n    self.assertEqual(1.0, f.clean(\" 1 \"))\n    with self.assertRaisesMessage(ValidationError, \"'Enter a number.'\"):\n        f.clean(\"1a\")\n    self.assertIsNone(f.max_value)\n    self.assertIsNone(f.min_value)\n", "    def test_floatfield_1(self):\n        f = FloatField()\n        with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n            f.clean(\"\")\n        with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n            f.clean(None)\n        self.assertEqual(1.0, f.clean(\"1\"))\n        self.assertIsInstance(f.clean(\"1\"), float)\n        self.assertEqual(23.0, f.clean(\"23\"))\n        self.assertEqual(23.5, f.clean(\"23.5\"))\n        with self.assertRaisesMessage(ValidationError, \"'Enter a number.'\"):\n            f.clean(\"a\")\n        self.assertEqual(1.0, f.clean(1))\n        self.assertEqual(1.0, f.clean(1.0))\n        self.assertEqual(1.0, f.clean(\"1.0\"))\n        self.assertEqual(1.0, f.clean(\" 1.0 \"))\n", "    def test_floatfield(self):\n        f = FloatField()\n        self.assertWidgetRendersTo(\n            f, '<input step=\"any\" type=\"number\" name=\"f\" id=\"id_f\" required>'\n        )\n        with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n            f.clean(\"\")\n        with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n            f.clean(None)\n        self.assertEqual(1.0, f.clean(\"1\"))\n        self.assertIsInstance(f.clean(\"1\"), float)\n        self.assertEqual(23.5, f.clean(\"23.5\"))\n        with self.assertRaisesMessage(ValidationError, \"'Enter a number.'\"):\n            f.clean(\"a\")\n        self.assertEqual(1.0, f.clean(\"1.0\"))\n        self.assertEqual(3.14, f.clean(3.14))\n        with self.assertRaisesMessage(ValidationError, \"'Enter a number.'\"):\n            f.clean(\"1a\")\n        self.assertIsNone(f.max_value)\n        self.assertIsNone(f.min_value)\n        self.assertIsNone(f.step_size)\n"], "sample_608": ["def test_unindexed_dims_repr() -> None:\n    ds = xr.Dataset(\n        coords={\"x\": [1, 2, 3]},\n        data_vars={\"var1\": ((\"x\", \"y\"), np.random.randn(3, 4))},\n    )\n\n    result = formatting.unindexed_dims_repr(ds.dims, ds.coords)\n    expected = \"Dimensions without coordinates: y\"\n    assert result == expected\n\n    result = formatting.unindexed_dims_repr(ds.dims, ds.coords, max_rows=1)\n    assert result == expected\n\n    ds = xr.Dataset(coords={\"x\": [1, 2, 3], \"y\": [1, 2, 3, 4]})\n\n    result = formatting.unindexed_dims_repr(ds.dims, ds.coords)\n    expected = \"Dimensions without coordinates: None\"\n    assert result == expected\n\n    result = formatting.unindexed_dims_repr(ds.dims, ds.coords, max_rows=1)\n    assert result == expected\n", "def test_inline_variable_array_repr_dask_array() -> None:\n    da = xr.DataArray(np.arange(100), dims=\"test\", chunks=10)\n    var = da.variable\n\n    max_width = 10\n    actual = formatting.inline_variable_array_repr(var, max_width=10)\n    assert \"dask.array<chunksize=(10,)\" in actual\n\n    max_width = 80\n    actual = formatting.inline_variable_array_repr(var, max_width=80)\n    assert \"dask.array<chunksize=(10,)>,\" in actual\n    assert \"meta=int64\" in actual\n", "def test_short_data_repr_on_large_variable() -> None:\n    var = xr.Variable(\"x\", np.random.randn(1000, 1000))\n\n    with xr.set_options(display_expand_data=True):\n        actual = formatting.short_data_repr(var)\n        assert actual.startswith(\"[1000000 values with dtype=float64]\")\n\n    with xr.set_options(display_expand_data=False):\n        actual = formatting.short_data_repr(var)\n        assert len(actual.split(\"\\n\")) < 30\n", "def test_summarize_variable_with_quote_strings_false() -> None:\n    da = xr.DataArray(\n        [\"foo\", \"bar\", \"baz\"],\n        dims=\"x\",\n        coords={\"x\": [1, 2, 3]},\n    )\n    col_width = 80\n    actual = formatting.summarize_variable(\"foo\", da, col_width, quote_strings=False)\n    expected = \"  foo (x: 3) object 'f...' \"\n    assert actual.startswith(expected)\n\n    actual = formatting.summarize_variable(\"foo\", da, col_width)\n    expected = \"  foo (x: 3) object '...' \"\n    assert actual.startswith(expected)\n", "def test_format_array_flat_empty_array() -> None:\n    actual = formatting.format_array_flat(np.arange(0), 5)\n    expected = \"\"\n    assert expected == actual\n"], "sample_1111": ["def test_multiple_symbols():\n    x, y = Symbol('x'), Symbol('y')\n    with raises(ValueError):\n        list(textplot_str(x + y, 0, 1))\n\n    z = Symbol('z')\n    with raises(ValueError):\n        list(textplot_str(z**2, 0, 1))\n", "def test_zero_range():\n    x = Symbol('x')\n    lines = [\n        '      0 |_______________________________________________________',\n        '         0                          0                          0'\n    ]\n    assert lines == list(textplot_str(x, 0, 0))\n\n    lines = [\n        '      0 |_______________________________________________________',\n        '         0                          0                          0'\n    ]\n    assert lines == list(textplot_str(x, 0, 0, W=100, H=20))\n\n    lines = [\n        '      0 |_______________________________________________________',\n        '         0                          0                          0'\n    ]\n    assert lines == list(textplot_str(0, 0, 0))\n", "def test_exceptions():\n    x = Symbol('x')\n    lines = [\n        '      1 | .                                                     ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '      0 |-------------------------------------------------------',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '      0 |_______________________________________________________',\n        '         0                          0.5                        1'\n    ]\n    assert list(textplot_str(1/(x-1), 0, 1)) == lines\n\n    lines = [\n        '      1 |                                                     ..',\n        '        |                                                   ...  ',\n        '        |                                                ...     ',\n        '        |                                              ...       ',\n        '        |                                           ...          ',\n        '        |                                        ...             ',\n        '        |                                     ...               ',\n        '        |                                  ...                  ',\n        '        |                               ...                     ',\n        '        |                            ...                       ',\n        '      0 |--------------------------...--------------------------',\n        '        |                       ...                             ',\n        '        |                     ...                                ',\n        '        |                  ...                                  ',\n        '        |               ...                                     ',\n        '        |             ...                                       ',\n        '        |          ...                                          ',\n        '        |       ...                                             ',\n        '        |     ...                                               ',\n        '        |  ...                                                  ',\n        '     -1 |_______________________________________________________',\n        '         -1                         0                          1'\n    ]\n    assert list(textplot_str(x**-2, -1, 1)) == lines\n", "def test_rescale_edge_cases():\n    x = Symbol('x')\n    lines = [\n        '    100 |_______________________________________________________',\n        '         0                          50                         100'\n    ]\n    assert list(textplot_str(x**2, 0, 100, H=1)) == lines\n\n    lines = [\n        '    100 |_______________________________________________________',\n        '         0                          50                         100'\n    ]\n    assert list(textplot_str(x**2, 0, 100, H=1, W=1)) == lines\n\n    lines = [\n        '      1 |_______________________________________________________',\n        '         -1                         0                          1'\n    ]\n    assert list(textplot_str(x, -1, 1, W=1, H=1)) == lines\n\n    lines = [\n        '      1 |_______________________________________________________',\n        '         0'\n    ]\n    assert list(textplot_str(x, 0, 0, W=1, H=1)) == lines\n\n    lines = [\n        '      1 |_______________________________________________________',\n        '         -1'\n    ]\n    assert list(textplot_str(x, 0, -1, W=1, H=1)) == lines\n", "def test_multiple_roots():\n    x = Symbol('x')\n    lines = [\n        '      2 |                      .                               ',\n        '        |                       .                             ',\n        '        |                        .                            ',\n        '        |                         .                           ',\n        '        |                          .                          ',\n        '        |                          .                          ',\n        '        |                           .                         ',\n        '        |                            .                        ',\n        '        |                             .                       ',\n        '        |                              .                      ',\n        '      1 |-----------------------------.------------------------',\n        '        |                             .                      ',\n        '        |                            .                       ',\n        '        |                           .                        ',\n        '        |                          .                         ',\n        '        |                         .                          ',\n        '        |                        .                           ',\n        '        |                       .                            ',\n        '        |                      .                             ',\n        '        |                     .                              ',\n        '      0 |_______________________________________________________',\n        '         -1                        0                          1'\n    ]\n    assert lines == list(textplot_str(x**2, -1, 1))\n\n    lines = [\n        '      2 |                   .                                ',\n        '        |                   .                               ',\n        '        |                    .                              ',\n        '        |                    .                              ',\n        '        |                     .                             ',\n        '        |                     .                             ',\n        '        |                      .                            ',\n        '        |                      .                            ',\n        '        |                       .                           ',\n        '        |                       .                           ',\n        '      1 |----------------------.-----------------------------',\n        '        |                       .                           ',\n        '        |                       .                           ',\n        '        |                      .                            ',\n        '        |                      .                            ',\n        '        |                     .                             ',\n        '        |                     .                             ',\n        '        |                    .                              ',\n        '        |                    .                              ',\n        '      0 |_______________________________________________________',\n        '         -1                        0                          1'\n    ]\n    assert lines == list(textplot_str(x**2, -1, 1, H=19))\n"], "sample_407": ["def test_save_base_with_empty_update_fields(self):\n    article = Article.objects.create(\n        headline=\"Test article\",\n        pub_date=datetime.date(2005, 7, 27),\n        reporter=self.r,\n    )\n    article.save_base(using=\"default\", update_fields=[])\n    self.assertEqual(article.reporter, self.r)\n    self.assertEqual(article.headline, \"Test article\")\n    self.assertEqual(article.pub_date, datetime.date(2005, 7, 27))\n", "def test_model_instantiation_with_deferred_fields(self):\n    # Create a Reporter with a deferred field.\n    reporter = Reporter.objects.defer(\"email\").first()\n    # The deferred field should not be loaded yet.\n    self.assertNotIn(\"email\", reporter.__dict__)\n    # Accessing the field should load it.\n    with self.assertNumQueries(1):\n        self.assertEqual(reporter.email, \"john@example.com\")\n    # The field should now be loaded.\n    self.assertIn(\"email\", reporter.__dict__)\n    # Accessing the field again should not trigger another query.\n    with self.assertNumQueries(0):\n        self.assertEqual(reporter.email, \"john@example.com\")\n", "def test_refresh_from_db(self):\n    # Regression for #23832: refresh_from_db doesn't clear related fields.\n    c = City.objects.create(name=\"Musical City\")\n    d = District.objects.create(name=\"Ladida\", city=c)\n    city = City.objects.get(id=c.id)\n    city.districts.add(d)\n    city.refresh_from_db()\n    self.assertSequenceEqual(city.districts.all(), [d])\n", "def test_save_base(self):\n    parent = Parent.objects.create(name=\"jeff\")\n    child = Child.objects.create(parent=parent)\n    self.assertEqual(child.parent, parent)\n    child.parent.name = \"bill\"\n    child.save_base(raw=True, using=\"default\")\n    child.refresh_from_db()\n    self.assertEqual(child.parent.name, \"bill\")\n    with self.assertRaisesMessage(ValueError, \"save_base() prohibited to prevent data loss due to unsaved related object 'parent'.\"):\n        child.save_base(raw=False, using=\"default\")\n    child.parent.save()\n    child.save_base(raw=False, using=\"default\")\n    child.refresh_from_db()\n    self.assertEqual(child.parent.name, \"bill\")\n", "def test_related_object_with_string_id(self):\n    parent = ParentStringPrimaryKey.objects.create(name=\"Parent\")\n    child = ChildStringPrimaryKeyParent.objects.create(parent=parent)\n    self.assertEqual(child.parent.name, \"Parent\")\n\n    # Assigning a new object results in that object getting cached immediately.\n    parent2 = ParentStringPrimaryKey.objects.create(name=\"Parent 2\")\n    child.parent = parent2\n    self.assertIs(child.parent, parent2)\n\n    # Assigning a new object results in that object getting cached immediately.\n    child.parent_id = parent2.name\n    self.assertIs(child.parent, parent2)\n\n    # Assigning a string pk to a related object will cache that object.\n    parent3 = ParentStringPrimaryKey.objects.create(name=\"Parent 3\")\n    child.parent_id = parent3.name\n    self.assertIs(child.parent, parent3)\n\n    # Assigning a wrong string pk to a related object raises an error.\n    msg = (\n        'Cannot assign \"xyzzy\": \"ChildStringPrimaryKeyParent.parent\" must be a '\n        '\"ParentStringPrimaryKey\" instance.'\n    )\n    with self.assertRaisesMessage(ValueError, msg):\n        child.parent_id = \"xyzzy\"\n\n    # Assigning None to a related object with string pk raises an error.\n    msg = (\n        'Cannot assign None: \"ChildStringPrimaryKeyParent.parent\" cannot be null.'\n    )\n    with self.assertRaisesMessage(ValueError, msg):\n        child.parent_id = None\n"], "sample_699": ["def test_doctest_runner_repr_failure_with_empty_docstring(pytester: Pytester):\n    pytester.makepyfile(\n        test_doctest_runner_repr_failure_with_empty_docstring=\"\"\"\n                \\\"\\\"\\\"\n                >>> 1 + 1\n                3\n                \\\"\\\"\\\"\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--doctest-modules\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*DocTestFailure*\",\n            \"*EXAMPLE LOCATION UNKNOWN, not showing all tests of that example\",\n            \"[?][?][?] >>> 1 + 1\",\n            \"Expected:\",\n            \"    3\",\n            \"Got:\",\n            \"    2\",\n        ]\n    )\n", "    def test_output_difference(self, pytester: Pytester):\n        pytester.makeini(\n            \"\"\"\n            [pytest]\n            doctest_optionflags = ELLIPSIS NORMALIZE_WHITESPACE\n        \"\"\"\n        )\n        p = pytester.maketxtfile(\n            xdoc=\"\"\"\n            >>> a = \"foo\\\\nbar\\\\n\\\\n\\\\t baz\"\n            >>> print(a)\n            'foo\\\\nbar\\\\n\\\\n\\\\t baz'\n        \"\"\"\n        )\n        reprec = pytester.inline_run(p, \"--doctest-glob=x*.txt\")\n        reprec.assertoutcome(passed=1)\n\n        p = pytester.maketxtfile(\n            xdoc=\"\"\"\n            >>> a = \"foo\\\\nbar\\\\n\\\\n\\\\t baz\"\n            >>> print(a)\n            'foo\\\\nbar\\\\n baz'\n        \"\"\"\n        )\n        reprec = pytester.inline_run(p, \"--doctest-glob=x*.txt\")\n        reprec.assertoutcome(failed=1)\n", "def test_doctest_namespace_with_custom_doctest_namespace(pytester: Pytester) -> None:\n    \"\"\"\n    Test that the doctest_namespace fixture can be overridden.\n    \"\"\"\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(autouse=True, scope=\"session\")\n            return {'custom_var': 42}\n    \"\"\"\n    )\n    pytester.maketxtfile(\n        test_doc=\"\"\"\n        >>> custom_var\n        42\n    \"\"\"\n    )\n    reprec = pytester.inline_run()\n    reprec.assertoutcome(passed=1)\n", "def test_doctest_failing_multiline_string(pytester: Pytester):\n    pytester.makepyfile(\n        \"\"\"\n            '''\n            >>> if 0:\n            ...     if 1:\n            ...         pass\n            ...     else:\n            ...         pass\n            '''\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--doctest-modules\")\n    result.stdout.fnmatch_lines([\"* 1 passed *\"])\n", "    def test_doctest_report_choice_none(self, pytester: Pytester):\n        pytester.makepyfile(\n            \"\"\"\n                '''\n                >>> foo()\n                1\n                '''\n                return 2\n            \"\"\"\n        )\n        result = pytester.runpytest(\"--doctest-modules\", \"--doctest-report=none\")\n        result.stdout.fnmatch_lines(\n            [\n                \"* 1 failed *\",\n                \"*Expected:\",\n                \"*    1\",\n                \"*Got:\",\n                \"*    2\",\n            ]\n        )\n"], "sample_585": ["def test_da_groupby_fillna():\n    array = xr.DataArray([1, 2, np.nan, 4, 5, np.nan],\n                         [('x', [1, 1, 1, 2, 2, 2])])\n\n    actual = array.groupby('x').fillna(0)\n    expected = xr.DataArray([1, 2, 0, 4, 5, 0],\n                            [('x', [1, 1, 1, 2, 2, 2])])\n    assert_identical(expected, actual)\n", "def test_da_groupby_fillna():\n\n    array = xr.DataArray([1, np.nan, 3, 4, np.nan, 6],\n                         [('x', [1, 1, 1, 2, 2, 2])])\n\n    expected = xr.DataArray([1., 1., 3, 4, 4., 6],\n                            [('x', [1, 1, 1, 2, 2, 2])])\n    actual = array.groupby('x').fillna(1)\n    assert_identical(expected, actual)\n\n    expected = xr.DataArray([1., 1., 3, 4, 4., 6],\n                            [('x', [1, 1, 1, 2, 2, 2])])\n    actual = array.groupby('x').fillna(array.mean(dim='x'))\n    assert_identical(expected, actual)\n\n    with pytest.raises(ValueError):\n        array.groupby('x').fillna([1, 2, 3])\n", "def test_groupby_da_fillna():\n\n    # Create a test DataArray\n    array = xr.DataArray([1, 2, np.nan, 4, 5, np.nan],\n                         [('x', [1, 1, 1, 2, 2, 2])])\n\n    # Create a test group\n    g = array.groupby('x')\n\n    # Test fillna with a scalar\n    expected = xr.DataArray([1, 2, 0, 4, 5, 0], [('x', [1, 1, 1, 2, 2, 2])])\n    actual = g.fillna(0)\n    assert_identical(expected, actual)\n\n    # Test fillna with an array\n    expected = xr.DataArray([1, 2, 2, 4, 5, 5], [('x', [1, 1, 1, 2, 2, 2])])\n    actual = g.fillna(xr.DataArray([2, 5], [('x', [1, 2])]))\n    assert_identical(expected, actual)\n", "def test_groupby_da_datetime_with_binned_groups():\n    # test groupby with a DataArray of dtype datetime and binned groups\n    # create test data\n    times = pd.date_range('2000-01-01', periods=4)\n    foo = xr.DataArray([1, 2, 3, 4], coords=dict(time=times), dims='time')\n    # create test index\n    binned = pd.cut(times, bins=[pd.Timestamp('2000-01-01'), pd.Timestamp('2000-01-02'), pd.Timestamp('2000-01-03')])\n    ind = xr.DataArray(binned, coords=dict(time=times), dims='time', name='binned_time')\n    g = foo.groupby(ind)\n    actual = g.sum(dim='time')\n    expected = xr.DataArray([3, 7],\n                            coords=dict(binned_time=['(1999-12-31 00:00:00, 2000-01-02 00:00:00]',\n                                                     '(2000-01-02 00:00:00, 2000-01-03 00:00:00)']),\n                            dims='binned_time')\n    assert actual.equals(expected)\n", "def test_groupby_da_bins():\n    # test groupby with bins for GH1878\n    # create test data\n    array = xr.DataArray([1, 2, 3, 4, 5, 6],\n                         [('x', [1, 1, 1, 2, 2, 2])])\n\n    # create bins\n    bins = [1, 2, 4, 6]\n    g = array.groupby_bins('x', bins=bins)\n    expected = xr.DataArray([1.5, 4.5],\n                            [('x_bins', [1, 2])])\n    actual = g.mean()\n    assert_identical(expected, actual)\n\n    # test include_lowest\n    bins = [1, 2, 4, 6]\n    g = array.groupby_bins('x', bins=bins, include_lowest=True)\n    expected = xr.DataArray([1.5, 5],\n                            [('x_bins', [1, 2])])\n    actual = g.mean()\n    assert_identical(expected, actual)\n\n    # test right\n    bins = [1, 2, 4, 6]\n    g = array.groupby_bins('x', bins=bins, right=False)\n    expected = xr.DataArray([1.5, 5],\n                            [('x_bins', [1, 2])])\n    actual = g.mean()\n    assert_identical(expected, actual)\n\n    # test right and include_lowest\n    bins = [1, 2, 4, 6]\n    g = array.groupby_bins('x', bins=bins, right=False, include_lowest=True)\n    expected = xr.DataArray([1.5, 4.5],\n                            [('x_bins', [1, 2])])\n    actual = g.mean()\n    assert_identical(expected, actual)\n\n    # test with labels\n    bins = [1, 2, 4, 6]\n    labels = ['A', 'B']\n    g = array.groupby_bins('x', bins=bins, labels=labels)\n    expected = xr.DataArray([1.5, 4.5],\n                            [('x_bins', labels)])\n    actual = g.mean()\n    assert_identical(expected, actual)\n"], "sample_812": ["def test_n_max_elements_to_show_with_tuple():\n    # Test that ellipsis is added to tuples when n_max_elements_to_show is set\n    n_max_elements_to_show = 5\n    pp = _EstimatorPrettyPrinter(\n        compact=True, indent=1, indent_at_name=True,\n        n_max_elements_to_show=n_max_elements_to_show\n    )\n\n    # No ellipsis\n    vectorizer = CountVectorizer(token_pattern=('a', 'b', 'c', 'd', 'e'))\n    expected = r\"\"\"", "def test_repr_when_n_max_elements_to_show_is_zero():\n    # Test that an ellipsis is used when n_max_elements_to_show is zero\n    n_max_elements_to_show = 0\n    pp = _EstimatorPrettyPrinter(\n        compact=True, indent=1, indent_at_name=True,\n        n_max_elements_to_show=n_max_elements_to_show\n    )\n\n    vocabulary = {i: i for i in range(1)}\n    vectorizer = CountVectorizer(vocabulary=vocabulary)\n\n    expected = r\"\"\"", "def test_changed_params():\n    # Test that _changed_params correctly identifies changed parameters\n\n    # Define a test class with default parameters\n    class TestEstimator(BaseEstimator):\n            self.param1 = param1\n            self.param2 = param2\n\n            return {'param1': self.param1, 'param2': self.param2}\n\n    # Create an instance of the test class with default parameters\n    estimator = TestEstimator()\n\n    # Check that no parameters are reported as changed\n    assert _changed_params(estimator) == {}\n\n    # Create an instance of the test class with a changed parameter\n    estimator = TestEstimator(param1=2)\n\n    # Check that the changed parameter is reported\n    assert _changed_params(estimator) == {'param1': 2}\n\n    # Create an instance of the test class with a changed parameter that is NaN\n    estimator = TestEstimator(param1=float('nan'))\n\n    # Check that the changed parameter is reported\n    assert _changed_params(estimator) == {'param1': float('nan')}\n\n    # Create an instance of the test class with a changed parameter that is a list\n    estimator = TestEstimator(param2=[1, 2, 3])\n\n    # Check that the changed parameter is reported\n    assert _changed_params(estimator) == {'param2': [1, 2, 3]}\n", "def test_edge_cases():\n    # Test edge cases like empty dictionaries, lists and tuples\n    empty_dict = LogisticRegression()\n    empty_list = GridSearchCV(SVC(), param_grid={})\n    empty_tuple = Pipeline(steps=[])\n\n    pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True)\n    \n    expected_empty_dict = \"\"\"", "def test_custom_estimator_repr():\n    class CustomEstimator(BaseEstimator):\n            self.param1 = param1\n            self.param2 = param2\n\n    pp = _EstimatorPrettyPrinter(indent_at_name=False)\n    custom_estimator = CustomEstimator(param1=2)\n    expected = \"\"\"CustomEstimator(param1=2, param2='a')\"\"\"\n    assert pp.pformat(custom_estimator) == expected\n\n    pp = _EstimatorPrettyPrinter(indent_at_name=True)\n    expected = \"\"\"CustomEstimator(param1=2, param2='a')\"\"\"\n    assert pp.pformat(custom_estimator) == expected\n"], "sample_824": ["def test_additive_chi2_kernel():\n    # Check the additive chi-squared kernel computation\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    Y = rng.random_sample((10, 4))\n    K_add = additive_chi2_kernel(X, Y)\n    for i, x in enumerate(X):\n        for j, y in enumerate(Y):\n            chi2 = -np.sum((x - y) ** 2 / (x + y))\n            assert_almost_equal(K_add[i, j], chi2)\n\n    # check diagonal is zeros for data with itself\n    K = additive_chi2_kernel(Y)\n    assert_array_equal(np.diag(K), 0)\n    # check off-diagonal is < 0 but > -2:\n    assert np.all(K < 0)\n    assert np.all(K > -2)\n\n    # check that float32 is preserved\n    X = rng.random_sample((5, 4)).astype(np.float32)\n    Y = rng.random_sample((10, 4)).astype(np.float32)\n    K = additive_chi2_kernel(X, Y)\n    assert_equal(K.dtype, np.float32)\n\n    # check that integer type gets converted,\n    # check that zeros are handled\n    X = rng.random_sample((10, 4)).astype(np.int32)\n    K = additive_chi2_kernel(X, X)\n    assert np.isfinite(K).all()\n    assert_equal(K.dtype, np.float)\n\n    # check that kernel of similar things is greater than dissimilar ones\n    X = [[.3, .7], [1., 0]]\n    Y = [[0, 1], [.9, .1]]\n    K = additive_chi2_kernel(X, Y)\n    assert_greater(K[0, 0], K[0, 1])\n    assert_greater(K[1, 1], K[1, 0])\n\n    # test negative input\n    assert_raises(ValueError, additive_chi2_kernel, [[0, -1]])\n    assert_raises(ValueError, additive_chi2_kernel, [[0, -1]], [[-1, -1]])\n    assert_raises(ValueError, additive_chi2_kernel, [[0, 1]], [[-1, -1]])\n\n    # different n_features in X and Y\n    assert_raises(ValueError, additive_chi2_kernel, [[0, 1]], [[.2, .2, .6]])\n\n    # sparse matrices\n   ", "def test_check_pairwise_arrays_precomputed():\n    # Ensure that check_pairwise_arrays works for precomputed distance matrices.\n    # Check that if XB is None, XB is returned as reference to XA\n    XA = np.arange(16, dtype=np.float32).reshape(4, 4)\n    XA_checked, XB_checked = check_pairwise_arrays(XA, None, precomputed=True)\n    assert XA_checked is XB_checked\n    assert_array_equal(XA, XA_checked)\n\n    # Check that if XB is not None, it is returned equal.\n    XB = np.arange(16, dtype=np.float32).reshape(4, 4)\n    XA_checked, XB_checked = check_pairwise_arrays(XA, XB, precomputed=True)\n    assert_array_equal(XA, XA_checked)\n    assert_array_equal(XB, XB_checked)\n\n    # Check that input is checked to be a square matrix\n    XB = np.arange(20, dtype=np.float32).reshape(4, 5)\n    assert_raises(ValueError, check_pairwise_arrays, XA, XB, precomputed=True)\n\n    # Check that input is checked to be a square matrix\n    XB = np.arange(12, dtype=np.float32).reshape(3, 4)\n    assert_raises(ValueError, check_pairwise_arrays, XA, XB, precomputed=True)\n", "def test_check_pairwise_arrays():\n    # Check that check_pairwise_arrays returns the correct shape and dtype\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    Y = rng.random_sample((3, 4))\n\n    X_check, Y_check = check_pairwise_arrays(X, Y)\n    assert X_check.shape == (5, 4)\n    assert Y_check.shape == (3, 4)\n\n    X_check, Y_check = check_pairwise_arrays(X, Y, dtype=np.float32)\n    assert X_check.dtype == np.float32\n    assert Y_check.dtype == np.float32\n\n    # Check that check_pairwise_arrays returns the correct shape and dtype\n    # for sparse matrices\n    X_sparse = csr_matrix(X)\n    Y_sparse = csr_matrix(Y)\n\n    X_check, Y_check = check_pairwise_arrays(X_sparse, Y_sparse)\n    assert X_check.shape == (5, 4)\n    assert Y_check.shape == (3, 4)\n\n    X_check, Y_check = check_pairwise_arrays(X_sparse, Y_sparse, dtype=np.float32)\n    assert X_check.dtype == np.float32\n    assert Y_check.dtype == np.float32\n", "def test_pairwise_distances_argmin_min_edge_cases(metric, axis):\n    rng = np.random.RandomState(0)\n    X = rng.rand(1, 100)\n    Y = rng.rand(2, 100)\n    indices, values = pairwise_distances_argmin_min(X, Y, metric=metric, axis=axis)\n    assert_array_almost_equal(indices, [0, 0])\n    assert_array_almost_equal(values, [pairwise_distances(X, Y, metric=metric)[0, 0]] * 2)\n\n    X = rng.rand(100, 1)\n    Y = rng.rand(100, 2)\n    indices, values = pairwise_distances_argmin_min(X, Y, metric=metric, axis=axis)\n    assert_array_almost_equal(indices, [0] * 2)\n    assert_array_almost_equal(values, [pairwise_distances(X, Y, metric=metric)[0, 0]] * 2)\n", "def test_pairwise_distances_random_inputs(metric):\n    # Test the pairwise distances computation with random inputs.\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((100, 10))\n    Y = rng.random_sample((100, 10))\n\n    # Check that the distances are symmetric.\n    D = pairwise_distances(X, Y, metric=metric)\n    assert_array_almost_equal(D, D.T)\n\n    # Check that the distances are the same when X and Y are swapped.\n    D = pairwise_distances(Y, X, metric=metric)\n    assert_array_almost_equal(D, D.T)\n"], "sample_114": ["    def test_mti_inheritance_field_removal(self):\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ])\n        Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n        changes = self.get_changes([Animal, Dog], [Animal])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['DeleteModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='Dog')\n", "def test_check_dependencies_across_apps(self):\n    \"\"\"Test check_dependencies functionality when models are across apps.\"\"\"\n    before = [\n        ModelState(\"app1\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ]),\n        ModelState(\"app2\", \"Book\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"author\", models.ForeignKey(\"app1.Author\", models.CASCADE)),\n        ]),\n    ]\n    after = [\n        ModelState(\"app1\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ]),\n        ModelState(\"app2\", \"Book\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"author\", models.ForeignKey(\"app1.Author\", models.CASCADE)),\n            (\"co_author\", models.ForeignKey(\"app1.Author\", models.CASCADE)),\n        ]),\n    ]\n    changes = self.get_changes(before, after)\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'app2', 1)\n    self.assertOperationTypes(changes, 'app2', 0, [\"AddField\"])\n    self.assertOperationAttributes(changes, 'app2', 0, 0, model_name=\"book\", name=\"co_author\")\n    self.assertMigrationDependencies(changes, 'app2', 0, [(\"app1\", \"auto_1\")])\n", "def test_custom_deconstructible_default(self):\n    \"\"\"#24543 - The deep_deconstruct method should handle custom deconstructibles with defaults.\"\"\"\n    class Deconstructible:\n            self.a = a\n\n            return (\n                'tests.test_migrations.Deconstructible',\n                (),\n                {'a': self.a},\n            )\n\n    # Unchanged custom deconstructible.\n    before = ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=200, default=Deconstructible())),\n    ])\n    after = ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=200, default=Deconstructible())),\n    ])\n    changes = self.get_changes([before], [after])\n    self.assertEqual(changes, {})\n\n    # Changed custom deconstructible.\n    after = ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=200, default=Deconstructible(a=2))),\n    ])\n    changes = self.get_changes([before], [after])\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n", "def test_fk_dependency_swappable_model_removal(self):\n    \"\"\"\n    Removal of swappable model that has a dependency to another app's model\n    should work correctly.\n    \"\"\"\n    with isolate_lru_cache(apps.get_swappable_settings_name):\n        before = self.make_project_state([self.author_empty, self.author_proxy_third, self.book_proxy_fk])\n        after = self.make_project_state([self.author_empty, self.book])\n        changes = self.get_changes(before, after)\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'thirdapp', 1)\n    self.assertOperationTypes(changes, 'thirdapp', 0, [\"DeleteModel\"])\n    self.assertOperationAttributes(changes, 'thirdapp', 0, 0, name=\"AuthorProxy\")\n    self.assertMigrationDependencies(changes, 'thirdapp', 0, [('testapp', 'auto_1')])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, model_name=\"book\", name=\"author\")\n    self.assertEqual(changes['otherapp'][0].operations[0].field.deconstruct(), (\n        'author',\n        'django.db.models.ForeignKey',\n        [],\n        {'to': 'testapp.Author', 'on_delete': models.CASCADE},\n    ))\n    self.assertMigrationDependencies(changes, 'otherapp', 0, [('thirdapp', 'auto_1'), ('testapp', 'auto_1')])\n", "def test_alter_unique_together_with_constraint_name(self):\n    \"\"\"\n    #23672 - Altering unique_together removes any existing constraint with\n    the same name, regardless of the fields it constrains.\n    \"\"\"\n    before = ModelState(\"testapp\", \"Model\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"field1\", models.IntegerField()),\n        (\"field2\", models.IntegerField()),\n    ], options={\n        \"constraints\": [\n            models.UniqueConstraint(fields=[\"field1\"], name=\"constraint_name\"),\n            models.UniqueConstraint(fields=[\"field1\", \"field2\"]),\n        ],\n    })\n    after = ModelState(\"testapp\", \"Model\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"field1\", models.IntegerField()),\n        (\"field2\", models.IntegerField()),\n    ], options={\n        \"unique_together\": {(\"field1\", \"field2\")},\n        \"constraints\": [\n            models.UniqueConstraint(fields=[\"field1\"], name=\"constraint_name\"),\n        ],\n    })\n    changes = self.get_changes([before], [after])\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AlterUniqueTogether', 'RemoveConstraint'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"model\", unique_together={(\"field1\", \"field2\")})\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, name=\"constraint_name\", model_name='model')\n"], "sample_776": ["def test_lars_path_residues_readonly_data():\n    # When using automated memory mapping on large input, the\n    # fold data is in read-only mode\n    # This is a non-regression test for:\n    # https://github.com/scikit-learn/scikit-learn/issues/4597\n    splitted_data = train_test_split(X, y, random_state=42)\n    with TempMemmap(splitted_data) as (X_train, X_test, y_train, y_test):\n        # The following should not fail despite copy=False\n        _lars_path_residues(X_train, y_train, X_test, y_test, copy=False,\n                            method='lar', verbose=0, max_iter=500, eps=1e-7,\n                            positive=False)\n", "def test_lars_zero_features(estimator):\n    \"\"\"Test that Lars and LassoLars handle zero features correctly\"\"\"\n    X = np.zeros((10, 5))\n    y = np.random.rand(10)\n    model = getattr(linear_model, estimator)()\n    model.fit(X, y)\n    assert model.coef_.shape[0] == X.shape[1]\n    assert (model.coef_ == 0).all()\n", "def test_lars_path_edge_cases():\n    # Test lars_path with edge cases\n    # test with zero features\n    X = np.zeros((10, 0))\n    y = np.zeros(10)\n    alphas, active, coefs = linear_model.lars_path(X, y)\n    assert_array_almost_equal(coefs, np.zeros((0, 1)))\n\n    # test with zero samples\n    X = np.zeros((0, 10))\n    y = np.zeros(0)\n    alphas, active, coefs = linear_model.lars_path(X, y)\n    assert_array_almost_equal(coefs, np.zeros((10, 1)))\n\n    # test with single feature and single sample\n    X = np.array([[1]])\n    y = np.array([1])\n    alphas, active, coefs = linear_model.lars_path(X, y)\n    assert_array_almost_equal(coefs, np.array([[1]]))\n\n    # test with single feature and multiple samples\n    X = np.array([[1], [1]])\n    y = np.array([1, 1])\n    alphas, active, coefs = linear_model.lars_path(X, y)\n    assert_array_almost_equal(coefs, np.array([[1]]))\n\n    # test with multiple features and single sample\n    X = np.array([[1, 2, 3]])\n    y = np.array([1])\n    alphas, active, coefs = linear_model.lars_path(X, y)\n    assert_array_almost_equal(coefs, np.array([[1 / 14], [2 / 14], [3 / 14]]))\n\n    # test with empty y\n    X = np.array([[1, 2, 3]])\n    y = np.array([])\n    assert_raises(ValueError, linear_model.lars_path, X, y)\n\n    # test with X and y of different lengths\n    X = np.array([[1, 2, 3]])\n    y = np.array([1, 2])\n    assert_raises(ValueError, linear_model.lars_path, X, y)\n", "def test_lars_cross_val_select_best_alpha(estimator):\n    # test that the cross validation chooses the best alpha\n    # from the path by default\n    est = estimator()\n    est.fit(X, y)\n    assert_array_almost_equal(est.alphas_[est.alpha_ == est.alphas_], [est.alpha_])\n", "def test_lars_path_edge_cases():\n    # test edge cases that were not covered yet\n\n    X, y = np.array([[1, 2], [3, 4]]), np.array([5, 6])\n    assert_warns(ConvergenceWarning, linear_model.lars_path, X, y, max_iter=1)\n\n    X, y = np.array([[1, 2, 3]]).T, np.array([4, 5, 6])\n    alphas, active, coefs = linear_model.lars_path(X, y)\n    assert len(active) == 1\n\n    X, y = np.array([[1, 2]]).T, np.array([4, 5])\n    alphas, active, coefs = linear_model.lars_path(X, y)\n    assert len(active) == 1\n\n    X, y = np.array([[1, 2, 3, 4, 5]]).T, np.array([4, 5, 6, 7, 8])\n    alphas, active, coefs = linear_model.lars_path(X, y, Gram='auto')\n    assert len(active) == 1\n\n    X, y = np.array([1, 2, 3, 4, 5]), np.array([4, 5, 6, 7, 8])\n    alphas, active, coefs = linear_model.lars_path(X, y, Gram='auto')\n    assert len(active) == 1\n\n    X, y = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), np.array([11, 12, 13, 14, 15])\n    alphas, active, coefs = linear_model.lars_path(X, y, max_iter=1)\n    assert len(active) == 1\n"], "sample_789": ["def test_n_estimators_termination():\n    # Check early termination when perfect fit is reached.\n\n    # AdaBoost classification\n    for alg in ['SAMME', 'SAMME.R']:\n        clf = AdaBoostClassifier(algorithm=alg, n_estimators=100)\n        clf.fit(X, y_class)\n\n        assert_less(len(clf.estimators_), 100)\n\n    # AdaBoost regression\n    clf = AdaBoostRegressor(n_estimators=100, random_state=0)\n    clf.fit(X, y_regr)\n\n    assert_less(len(clf.estimators_), 100)\n", "def test_base_estimator_importances():\n    # Check importances of base estimator when using SAMME.R algorithm\n    from sklearn.tree import DecisionTreeClassifier\n    X, y = datasets.make_classification(n_samples=2000,\n                                        n_features=10,\n                                        n_informative=3,\n                                        n_redundant=0,\n                                        n_repeated=0,\n                                        shuffle=False,\n                                        random_state=1)\n\n    clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(),\n                             algorithm=\"SAMME.R\", n_estimators=2)\n\n    clf.fit(X, y)\n    importances = clf.feature_importances_\n\n    assert_equal(importances.shape[0], 10)\n    assert_equal((importances[:3, np.newaxis] >= importances[3:]).all(),\n                 True)\n", "def test_adaboost_classifier_base_estimator_class_weights():\n    # Test AdaBoostClassifier with base estimator that has class weights.\n    from sklearn.tree import DecisionTreeClassifier\n    from sklearn.datasets import make_classification\n    from sklearn.model_selection import train_test_split\n\n    # Create a binary classification dataset with class imbalance\n    X, y = make_classification(n_samples=100, n_features=4, weights=[0.2, 0.8],\n                               random_state=42)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n                                                        random_state=42)\n\n    # Create a DecisionTreeClassifier with class weights\n    tree = DecisionTreeClassifier(class_weight='balanced', random_state=42)\n\n    # Create an AdaBoostClassifier with the DecisionTreeClassifier as base\n    # estimator\n    clf = AdaBoostClassifier(base_estimator=tree, random_state=42)\n\n    # Train the model\n    clf.fit(X_train, y_train)\n\n    # Test the model\n    y_pred = clf.predict(X_test)\n\n    # Check that the model is not overfitting to the majority class\n    assert np.mean(y_pred == y_test) > 0.5\n", "def test_feature_importances():\n    # Check feature importances.\n    X, y = datasets.make_classification(n_samples=2000, n_features=10,\n                                        n_informative=3, n_redundant=0,\n                                        n_repeated=0, shuffle=False,\n                                        random_state=1)\n\n    for alg in ['SAMME', 'SAMME.R']:\n        clf = AdaBoostClassifier(algorithm=alg)\n        clf.fit(X, y)\n        importances = clf.feature_importances_\n\n        assert_equal(importances.shape[0], 10)\n        assert_equal((importances[:3] >= importances[3:]).all(), True)\n\n        # Test that feature importances sum to 1\n        assert_array_almost_equal(np.sum(importances), 1.0)\n\n    # Test that a base estimator that doesn't support feature importances\n    # raises an AttributeError when accessing this attribute\n    clf = AdaBoostClassifier(base_estimator=SVC(gamma=\"scale\"),\n                             algorithm=\"SAMME\")\n    clf.fit(X, y)\n    assert_raises(AttributeError, lambda: clf.feature_importances_)\n\n    # Test that feature importances work for a regressor\n    X, y = datasets.make_regression(n_samples=2000, n_features=10,\n                                    n_informative=3, n_targets=1,\n                                    random_state=1)\n\n    clf = AdaBoostRegressor()\n    clf.fit(X, y)\n    importances = clf.feature_importances_\n\n    assert_equal(importances.shape[0], 10)\n    assert_equal((importances[:3] >= importances[3:]).all(), True)\n\n    # Test that feature importances sum to 1\n    assert_array_almost_equal(np.sum(importances), 1.0)\n", "def test_adaboost_regressor_staged_predict_redundant_features():\n    # Check staged predictions of AdaBoostRegressor with redundant features.\n\n    # Create a simple dataset with redundant features.\n    rng = np.random.RandomState(0)\n    X = rng.randn(100, 2)\n    y = X[:, 0] + rng.randn(100)\n    X = np.hstack((X, X))  # Make 4 features with 2 unique\n\n    # Fit the model\n    boost = AdaBoostRegressor(n_estimators=10, random_state=0)\n    boost.fit(X, y)\n\n    # Check staged predictions\n    predictions = [p for p in boost.staged_predict(X)]\n    assert len(predictions) == 10\n    assert_array_almost_equal(boost.predict(X), predictions[-1])\n\n    # Check feature importances\n    importances = boost.feature_importances_\n    assert_array_almost_equal(importances[:2], importances[2:])\n"], "sample_784": ["def test_calibration_prefit_regressor():\n    \"\"\"Test calibration for prefitted regressors\"\"\"\n    n_samples = 100\n    X, y = make_classification(n_samples=n_samples, n_features=6,\n                               random_state=42)\n\n    # Regressor\n    clf = RandomForestRegressor(n_estimators=10, random_state=42)\n    assert_raises(RuntimeError, CalibratedClassifierCV(clf, cv=\"prefit\").fit,\n                  X, y)\n", "def test_calibration_prefit_pipeline():\n    \"\"\"Test calibration for prefitted classifier pipeline\"\"\"\n    n_samples = 50\n    X, y = make_classification(n_samples=3 * n_samples, n_features=6,\n                               random_state=42)\n    sample_weight = np.random.RandomState(seed=42).uniform(size=y.size)\n\n    X -= X.min()  # MultinomialNB only allows positive X\n\n    # split train and test\n    X_train, y_train, sw_train = \\\n        X[:n_samples], y[:n_samples], sample_weight[:n_samples]\n    X_calib, y_calib, sw_calib = \\\n        X[n_samples:2 * n_samples], y[n_samples:2 * n_samples], \\\n        sample_weight[n_samples:2 * n_samples]\n    X_test, y_test = X[2 * n_samples:], y[2 * n_samples:]\n\n    # Naive-Bayes with pipeline\n    clf = Pipeline([\n        ('imputer', SimpleImputer()),\n        ('clf', MultinomialNB())\n    ])\n    clf.fit(X_train, y_train, clf__sample_weight=sw_train)\n    prob_pos_clf = clf.predict_proba(X_test)[:, 1]\n\n    # Naive Bayes with calibration\n    for this_X_calib, this_X_test in [(X_calib, X_test),\n                                      (sparse.csr_matrix(X_calib),\n                                       sparse.csr_matrix(X_test))]:\n        for method in ['isotonic', 'sigmoid']:\n            pc_clf = CalibratedClassifierCV(clf, method=method, cv=\"prefit\")\n\n            for sw in [sw_calib, None]:\n                pc_clf.fit(this_X_calib, y_calib, sample_weight=sw)\n                y_prob = pc_clf.predict_proba(this_X_test)\n                y_pred = pc_clf.predict(this_X_test)\n                prob_pos_pc_clf = y_prob[:, 1]\n                assert_array_equal(y_pred,\n                                   np.array([0, 1])[np.argmax(y_prob, axis=1)])\n\n                assert_greater(brier_score_loss(y_test, prob_pos_clf),\n                               brier_score_loss(y_test, prob_pos_pc_clf))\n", "def test_calibration_curve_edges():\n    \"\"\"Check calibration_curve function edge cases\"\"\"\n    # test with a single sample\n    y_true = np.array([1])\n    y_pred = np.array([0.5])\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=1)\n    assert_equal(len(prob_true), len(prob_pred))\n    assert_equal(len(prob_true), 1)\n    assert_almost_equal(prob_true, [1])\n    assert_almost_equal(prob_pred, [0.5])\n\n    # test with two samples, one in each bin\n    y_true = np.array([0, 1])\n    y_pred = np.array([0.4, 0.6])\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=2)\n    assert_equal(len(prob_true), len(prob_pred))\n    assert_equal(len(prob_true), 2)\n    assert_almost_equal(prob_true, [0, 1])\n    assert_almost_equal(prob_pred, [0.4, 0.6])\n\n    # test with two samples, both in the same bin\n    y_true = np.array([0, 0])\n    y_pred = np.array([0.4, 0.6])\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=2)\n    assert_equal(len(prob_true), len(prob_pred))\n    assert_equal(len(prob_true), 1)\n    assert_almost_equal(prob_true, [0])\n    assert_almost_equal(prob_pred, [0.5])\n", "def test_calibration_curve_multiple_blobs():\n    \"\"\"Check calibration_curve function with multiple blobs\"\"\"\n    # test multi-class setting with classifier that implements\n    # only decision function\n    clf = LinearSVC()\n    X, y_idx = make_blobs(n_samples=100, n_features=2, random_state=42,\n                          centers=3, cluster_std=3.0)\n\n    # Use categorical labels to check that CalibratedClassifierCV supports\n    # them correctly\n    target_names = np.array(['a', 'b', 'c'])\n    y = target_names[y_idx]\n\n    X_train, y_train = X[::2], y[::2]\n    X_test, y_test = X[1::2], y[1::2]\n\n    clf.fit(X_train, y_train)\n    for method in ['isotonic', 'sigmoid']:\n        cal_clf = CalibratedClassifierCV(clf, method=method, cv=2)\n        cal_clf.fit(X_train, y_train)\n        probas = cal_clf.predict_proba(X_test)\n\n        for i in range(len(target_names)):\n            y_true = (y_test == target_names[i]).astype(int)\n            y_pred = probas[:, i]\n            prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=2)\n\n            assert_equal(len(prob_true), len(prob_pred))\n            assert_equal(len(prob_true), 2)\n            assert_greater(prob_true.min(), 0)\n            assert_greater(prob_true.max(), 0)\n            assert_greater(prob_pred.min(), 0)\n            assert_greater(prob_pred.max(), 0)\n", "def test_calibration_class_labels():\n    \"\"\"Test calibration with class labels that are not integers\"\"\"\n    n_samples = 100\n    X, y = make_classification(n_samples=2 * n_samples, n_features=6,\n                               random_state=42, n_informative=2,\n                               n_redundant=0, n_classes=3)\n    X_train, y_train, X_test = X[:n_samples], y[:n_samples], X[n_samples:]\n    y_train_str = np.array(['a', 'b', 'c'])[y_train]\n\n    clf = LinearSVC(random_state=42)\n    cal_clf = CalibratedClassifierCV(clf, method='sigmoid', cv=2)\n\n    # class labels can be strings\n    cal_clf.fit(X_train, y_train_str)\n    probas_str = cal_clf.predict_proba(X_test)\n\n    # class labels can be integers\n    cal_clf.fit(X_train, y_train)\n    probas_int = cal_clf.predict_proba(X_test)\n\n    assert_array_almost_equal(probas_str, probas_int)\n\n    # check that predict returns correct class labels\n    assert_array_equal(cal_clf.predict(X_test),\n                       np.array(['a', 'b', 'c'])[cal_clf.predict(X_test)])\n"], "sample_783": ["def test_missing_indicator_fit_transform(X_fit, X_trans, features, sparse, error_on_new):\n    indicator = MissingIndicator(missing_values=-1, features=features,\n                                 sparse=sparse, error_on_new=error_on_new)\n    mask = indicator.fit_transform(X_fit)\n    assert_array_equal(indicator.transform(X_trans), mask)\n", "def test_imputation_most_frequent_error_mismatched_dtypes(X_fit, X_trans, error_msg):\n    imputer = SimpleImputer(strategy=\"most_frequent\")\n    with pytest.raises(ValueError, match=error_msg):\n        imputer.fit(X_fit).transform(X_trans)\n", "def test_missing_indicator_error_on_new(array_constructor):\n    # Test MissingIndicator with error_on_new=True\n    X_fit = np.array([\n        [1, 1, 0, 5],\n        [1, 2, -1, 3],\n        [1, 1, 3, -1],\n        [1, 2, 3, 7],\n    ])\n    X_trans = np.array([\n        [-1, 1, 0, 5],\n        [1, 2, -1, 3],\n        [1, 1, 3, -1],\n        [1, 2, 3, 7],\n    ])\n\n    X_fit = array_constructor(X_fit)\n    X_trans = array_constructor(X_trans)\n\n    indicator = MissingIndicator(missing_values=-1,\n                                 features='missing-only',\n                                 error_on_new=True)\n    with pytest.raises(ValueError, match=\"features with missing values in transform but have no missing values in fit\"):\n        indicator.fit(X_fit).transform(X_trans)\n", "def test_imputation_strategy_deletion(strategy):\n    # Verify that the transform method delete the features that contain only\n    # missing values\n    X = np.array([\n        [1, np.nan, np.nan, 1, np.nan],\n        [2, 2, np.nan, 2, np.nan],\n        [3, 3, np.nan, 3, np.nan]\n    ])\n\n    X_true = np.array([\n        [1, 1],\n        [2, 2],\n        [3, 3]\n    ])\n\n    imputer = SimpleImputer(strategy=strategy)\n    X_trans = imputer.fit_transform(X)\n    assert_array_almost_equal(X_trans, X_true)\n    assert X_trans.shape[1] == 2\n", "def test_missing_indicator_param(features, sparse, error_on_new, arr_type):\n    # check the parameter of MissingIndicator\n    X = np.array([[1, 1, 1],\n                  [1, 0, 0],\n                  [1, 0, 0]])\n\n    # check when there is no missing values\n    indicator = MissingIndicator(missing_values=0, features=features,\n                                 sparse=sparse, error_on_new=error_on_new)\n    indicator.fit(X)\n    assert_array_equal(indicator.features_, np.arange(X.shape[1]))\n\n    # check when there is missing values\n    X = np.array([[1, 1, 1],\n                  [1, np.nan, np.nan],\n                  [1, np.nan, np.nan]])\n    X = arr_type(X).astype(np.float64)\n    indicator.fit(X)\n    if features == 'missing-only':\n        assert_array_equal(indicator.features_, np.array([1, 2]))\n    else:\n        assert_array_equal(indicator.features_, np.array([0, 1, 2]))\n\n    # check the output shape\n    X_trans = indicator.transform(X)\n    assert X_trans.shape == (X.shape[0], indicator.features_.size)\n"], "sample_136": ["def test_build_absolute_uri_with_fragment(self):\n    request = HttpRequest()\n    request.get_host = lambda: 'www.example.com'\n    request.path = '/path/with:colons'\n    url = '/path/with:colons#fragment'\n    self.assertEqual(\n        request.build_absolute_uri(location=url),\n        'http://www.example.com/path/with:colons#fragment'\n    )\n    url = 'http://example.com/path/with:colons#fragment'\n    self.assertEqual(\n        request.build_absolute_uri(location=url),\n        url\n    )\n    url = '//example.com/path/with:colons#fragment'\n    self.assertEqual(\n        request.build_absolute_uri(location=url),\n        'http://example.com/path/with:colons#fragment'\n    )\n", "    def test_split_domain_port_ipv4(self):\n        domain, port = split_domain_port('127.0.0.1:8080')\n        self.assertEqual(domain, '127.0.0.1')\n        self.assertEqual(port, '8080')\n", "    def test_get_signed_cookie(self):\n        request = HttpRequest()\n        request.COOKIES = {'signed_cookie': 'YmFzZTY0'}\n        self.assertEqual(request.get_signed_cookie('signed_cookie'), 'base64')\n", "    def test_immutable(self):\n        query_dict = QueryDict(mutable=False)\n        with self.assertRaises(AttributeError):\n            query_dict['key'] = 'value'\n", "def test_get_signed_cookie(self):\n    factory = RequestFactory()\n    request = factory.get('/')\n    request.COOKIES = {'sessionid': 'somesessionid'}\n    request._set_content_type_params({'HTTP_COOKIE': 'sessionid=somesessionid; csrftoken=token'})\n    self.assertEqual(request.get_signed_cookie('sessionid', salt='django.contrib.sessions'), None)\n    request.COOKIES = {'sessionid': 'somesessionid:Qk9mNzY4MTU5ZDFiYzU0MGQ3MjI1YzU3NmU2NmQzODQ2MzJjNGE5NjIyNmI1MWIwMjQwMjRjNGU1ZjU3NmU2NmQzODQ2MzJjNGE5NjIyNmI1MWIwMjQwMjRjNGU1ZjU3NmU2NmQzODQ2MzJjNGE5NjIyNmI1MWIwMjQwMjRjNGU1ZjU3NmU2NmQzODQ2MzJjNGE5NjIyNmI1MWIwMjQwMjRjNGU1ZjU3NmU2NmQzODQ2MzJjNGE5NjIyNmI1MWIwMjQwMjRjNGU1ZjU3NmU2NmQzODQ2MzJjNGE5NjIyNmI1MWIwMjQwMjRjNGU1ZjU3NmU2NmQzODQ2MzJjNGE5NjIyNmI1MWIwMjQwMjRjNGU1ZjU3NmU2NmQzODQ2MzJjNGE5NjIyNmI1MWIwMjQwMjRjNGU1ZjU3NmU2NmQzODQ2MzJjNGE5NjIyNmI1MWIwMjQwMjRjNGU1ZjU3NmU2NmQzODQ2MzJjNGE5"], "sample_637": ["    def test_fixme_with_pragma(self) -> None:\n        code = \"\"\"# pylint: disable=W0511\n                a = 1\n                # FIXME message\n                \"\"\"\n        with self.assertNoMessages():\n            self.checker.process_tokens(_tokenize_str(code))\n", "    def test_encoding_issue(self) -> None:\n        code = b\"\"\"# -*- coding: utf-8 -*-\n                a = 1\n                \u00e9\"\"\"\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"syntax-error\", line=3, args=\"Cannot decode using encoding 'utf-8', bad encoding\")\n        ):\n            self.checker.process_module(nodes.Module(name='test', file_encoding='ascii'))\n            self.checker.process_tokens(_tokenize_str(code))\n", "def test_encoding_decode_error(self) -> None:\n    code = b\"# coding: utf-8\\na = 1\\n\\x80\\x80\"  # invalid utf-8\n    with self.assertAddsMessages(\n        MessageTest(msg_id=\"syntax-error\", line=3, args=\"Cannot decode using encoding 'utf-8', bad encoding\")\n    ):\n        self.checker.process_module(code)\n", "def test_notes_rgx_overrides_notes(self) -> None:\n    code = \"\"\"a = 1\n            # FIXME\n            # ??? some note\n            \"\"\"\n    with set_config(notes=[\"XXX\"], notes_rgx=\"FIXME|???\"):\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"fixme\", line=2, args=\"FIXME\", col_offset=17),\n            MessageTest(msg_id=\"fixme\", line=3, args=\"??? some note\", col_offset=17),\n        ):\n            self.checker.process_tokens(_tokenize_str(code))\n", "def test_encoding_check(self) -> None:\n    code = \"\"\"# -*- coding: utf-8 -*-\n                print(\"Hello World\")\n                \"\"\"\n    with open(\"test_encoding.py\", \"w\", encoding=\"utf-8\") as file:\n        file.write(code)\n\n    with self.assertNoMessages():\n        self.checker.process_module(nodes.Module(name=\"test_encoding\", file=\"test_encoding.py\"))\n\n    code = \"\"\"print(\"Hello World\")\n                \"\"\"\n    with open(\"test_encoding.py\", \"w\", encoding=\"utf-8\") as file:\n        file.write(code)\n\n    with self.assertAddsMessages(\n        MessageTest(\n            msg_id=\"syntax-error\",\n            line=1,\n            args=\"Cannot decode using encoding 'ascii', bad encoding\",\n        )\n    ):\n        self.checker.process_module(nodes.Module(name=\"test_encoding\", file=\"test_encoding.py\"))\n\n    code = \"\"\"# -*- coding: utf-16 -*-\n                print(\"Hello World\")\n                \"\"\"\n    with open(\"test_encoding.py\", \"w\", encoding=\"utf-8\") as file:\n        file.write(code)\n\n    with self.assertAddsMessages(\n        MessageTest(\n            msg_id=\"syntax-error\",\n            line=1,\n            args=\"Cannot decode using encoding 'utf-16', bad encoding\",\n        )\n    ):\n        self.checker.process_module(nodes.Module(name=\"test_encoding\", file=\"test_encoding.py\"))\n"], "sample_370": ["    def setUpTestData(cls):\n        cls.author1 = Author.objects.create(name='Author1')\n        cls.author2 = Author.objects.create(name='Author2')\n        cls.book1 = Book.objects.create(title='Book1')\n        cls.book2 = Book.objects.create(title='Book2')\n\n        cls.book1.authors.add(cls.author1)\n        cls.book1.authors.add(cls.author2)\n        cls.book2.authors.add(cls.author1)\n", "    def setUpTestData(cls):\n        cls.book1 = Book.objects.create(title='Poems')\n        cls.book2 = Book.objects.create(title='Jane Eyre')\n        cls.book3 = Book.objects.create(title='Wuthering Heights')\n        cls.book4 = Book.objects.create(title='Sense and Sensibility')\n\n        cls.author1 = Author.objects.create(name='Charlotte', first_book=cls.book1)\n        cls.author2 = Author.objects.create(name='Anne', first_book=cls.book1)\n        cls.author3 = Author.objects.create(name='Emily', first_book=cls.book1)\n        cls.author4 = Author.objects.create(name='Jane', first_book=cls.book4)\n\n        cls.book1.authors.add(cls.author1, cls.author2, cls.author3)\n        cls.book2.authors.add(cls.author1)\n        cls.book3.authors.add(cls.author3)\n        cls.book4.authors.add(cls.author4)\n", "    def setUpTestData(cls):\n        cls.book1 = Book.objects.create(title='Poems')\n        cls.book2 = Book.objects.create(title='Jane Eyre')\n        cls.book3 = Book.objects.create(title='Wuthering Heights')\n        cls.book4 = Book.objects.create(title='Sense and Sensibility')\n\n        cls.author1 = Author.objects.create(name='Charlotte', first_book=cls.book1)\n        cls.author2 = Author.objects.create(name='Anne', first_book=cls.book1)\n        cls.author3 = Author.objects.create(name='Emily', first_book=cls.book1)\n        cls.author4 = Author.objects.create(name='Jane', first_book=cls.book4)\n\n        cls.book1.authors.add(cls.author1, cls.author2, cls.author3)\n        cls.book2.authors.add(cls.author1)\n        cls.book3.authors.add(cls.author3)\n        cls.book4.authors.add(cls.author4)\n\n        cls.reader1 = Reader.objects.create(name='Amy')\n        cls.reader2 = Reader.objects.create(name='Belinda')\n\n        cls.reader1.books_read.add(cls.book1, cls.book4)\n        cls.reader2.books_read.add(cls.book2, cls.book4)\n", "    def setUpTestData(cls):\n        cls.book1 = Book.objects.create(title='Poems')\n        cls.book2 = Book.objects.create(title='Jane Eyre')\n        cls.book3 = Book.objects.create(title='Wuthering Heights')\n        cls.book4 = Book.objects.create(title='Sense and Sensibility')\n\n        cls.author1 = Author.objects.create(name='Charlotte', first_book=cls.book1)\n        cls.author2 = Author.objects.create(name='Anne', first_book=cls.book1)\n        cls.author3 = Author.objects.create(name='Emily', first_book=cls.book1)\n        cls.author4 = Author.objects.create(name='Jane', first_book=cls.book4)\n\n        cls.book1.authors.add(cls.author1, cls.author2, cls.author3)\n        cls.book2.authors.add(cls.author1)\n        cls.book3.authors.add(cls.author3)\n        cls.book4.authors.add(cls.author4)\n", "    def setUpTestData(cls):\n        cls.bio1 = Bio.objects.create()\n        cls.author1 = Author.objects.create(name='Charlotte', bio=cls.bio1)\n"], "sample_260": ["def test_create_model_add_index(self):\n    \"\"\"\n    AddIndex should optimize into CreateModel.\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\"Foo\", [(\"name\", models.CharField(max_length=255))]),\n            migrations.AddIndex(\"Foo\", models.Index(fields=[\"name\"])),\n        ],\n        [\n            migrations.CreateModel(\n                \"Foo\",\n                [(\"name\", models.CharField(max_length=255))],\n                options={\"indexes\": [models.Index(fields=[\"name\"])]},\n            ),\n        ],\n    )\n", "def test_create_model_add_constraint(self):\n    \"\"\"\n    AddConstraint should optimize into CreateModel.\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[(\"name\", models.CharField(max_length=255))],\n            ),\n            migrations.AddConstraint(\n                model_name=\"Foo\",\n                constraint=models.CheckConstraint(check=models.Q(name__startswith='a'), name=\"name_starts_with_a\"),\n            ),\n        ],\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[(\"name\", models.CharField(max_length=255))],\n                options={\"constraints\": [models.CheckConstraint(check=models.Q(name__startswith='a'), name=\"name_starts_with_a\")]},\n            ),\n        ],\n    )\n", "def test_create_model_alter_unique_together(self):\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                'MyModel',\n                fields=[\n                    ('a', models.IntegerField()),\n                    ('b', models.IntegerField()),\n                ],\n                options={\n                    'unique_together': (('a', 'b'),),\n                },\n            ),\n            migrations.AlterUniqueTogether(\n                'MyModel',\n                unique_together={('a', 'c'), ('b', 'd')},\n            ),\n        ],\n        [\n            migrations.CreateModel(\n                'MyModel',\n                fields=[\n                    ('a', models.IntegerField()),\n                    ('b', models.IntegerField()),\n                    ('c', models.IntegerField()),\n                    ('d', models.IntegerField()),\n                ],\n                options={\n                    'unique_together': (('a', 'c'), ('b', 'd')),\n                },\n            ),\n        ],\n    )\n", "def test_add_index_remove_index(self):\n    \"\"\"\n    AddIndex and RemoveIndex should cancel each other.\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.AddIndex(\"Foo\", models.Index(fields=[\"name\"])),\n            migrations.RemoveIndex(\"Foo\", \"foo_name_012345_idx\"),\n        ],\n        [],\n    )\n", "def test_alter_index_remove_index(self):\n    \"\"\"\n    AddIndex and RemoveIndex should cancel each other.\n    \"\"\"\n    index = models.Index(fields=['name'], name='my_index')\n    self.assertOptimizesTo(\n        [\n            migrations.AddIndex(\"Foo\", index),\n            migrations.RemoveIndex(\"Foo\", \"my_index\"),\n        ],\n        [],\n    )\n"], "sample_294": ["def test_rotate_token(self):\n    \"\"\"\n    The rotate_token function changes the CSRF token.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    mw = CsrfViewMiddleware(post_form_view)\n    mw.process_request(req)\n    original_token = req.META[\"CSRF_COOKIE\"]\n    rotate_token(req)\n    mw.process_request(req)\n    new_token = req.META[\"CSRF_COOKIE\"]\n    self.assertNotEqual(original_token, new_token)\n    self.assertTrue(equivalent_tokens(original_token, new_token))\n", "def test_csrf_token_on_500_stays_constant(self):\n    response = self.client.get('/crashing_view/')\n    # The error handler returns status code 599.\n    self.assertEqual(response.status_code, 599)\n    token1 = response.content\n    response = self.client.get('/crashing_view/')\n    self.assertEqual(response.status_code, 599)\n    token2 = response.content\n    self.assertTrue(equivalent_tokens(token1.decode('ascii'), token2.decode('ascii')))\n", "def test_rotate_token(self):\n    \"\"\"\n    Rotate the CSRF token after a POST request\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    mw = CsrfViewMiddleware(post_form_view)\n    mw.process_request(req)\n    old_token = req.META['CSRF_COOKIE']\n    rotate_token(req)\n    mw.process_view(req, post_form_view, (), {})\n    resp = mw(req)\n    csrf_cookie = resp.cookies.get(settings.CSRF_COOKIE_NAME, None)\n    self.assertIsNotNone(csrf_cookie)\n    self.assertNotEqual(csrf_cookie.value, old_token)\n    self.assertEqual(len(csrf_cookie.value), CSRF_TOKEN_LENGTH)\n", "    def test_origin_verified_no_origin_header(self):\n        \"\"\"\n        Test that _origin_verified() returns True when there's no 'HTTP_ORIGIN'\n        header and the request is insecure.\n        \"\"\"\n        req = self._get_POST_request_with_token()\n        req.META = {}\n        mw = CsrfViewMiddleware(post_form_view)\n        self.assertIs(mw._origin_verified(req), True)\n", "    def test_origin_verified_with_missing_trusted_origins(self):\n        \"\"\"\n        CsrfViewMiddleware rejects requests when CSRF_TRUSTED_ORIGINS is missing.\n        \"\"\"\n        with self.settings(CSRF_TRUSTED_ORIGINS=None):\n            req = self._get_POST_request_with_token()\n            req.META['HTTP_HOST'] = 'www.example.com'\n            req.META['HTTP_ORIGIN'] = 'https://www.example.com'\n            mw = CsrfViewMiddleware(post_form_view)\n            with self.assertLogs('django.security.csrf', 'WARNING') as cm:\n                response = mw.process_view(req, post_form_view, (), {})\n            self.assertEqual(response.status_code, 403)\n            msg = REASON_BAD_ORIGIN % req.META['HTTP_ORIGIN']\n            self.assertEqual(cm.records[0].getMessage(), 'Forbidden (%s): ' % msg)\n"], "sample_509": ["def test_concise_formatter_zero_formats():\n    locator = mdates.AutoDateLocator()\n    with pytest.raises(ValueError,\n                       match=\"zero_formats must be a list of 6 format strings\"):\n        mdates.ConciseDateFormatter(locator, zero_formats=['%y', '%m', '%d'])\n", "def test_ConciseDateFormatter_subsecond_rounding():\n    locator = mdates.AutoDateLocator(interval_multiples=True)\n    formatter = mdates.ConciseDateFormatter(locator)\n    year_1996 = 9861.0\n    strings = formatter.format_ticks([\n        year_1996 + 500 / mdates.MUSECONDS_PER_DAY,\n        year_1996 + 900 / mdates.MUSECONDS_PER_DAY,\n        year_1996 + 999 / mdates.MUSECONDS_PER_DAY\n    ])\n    assert strings == ['00.0005', '00.0009', '00.001']\n", "def test_date2num_roundtrip():\n    # Test round trip num2date, date2num, num2date\n    t = np.linspace(-10, 100, 1000)\n    t_date = mdates.num2date(t)\n    t_num = mdates.date2num(t_date)\n    np.testing.assert_allclose(t, t_num)\n    t_date_again = mdates.num2date(t_num)\n    assert len(t_date) == len(t_date_again)\n    for d, d_again in zip(t_date, t_date_again):\n        assert d == d_again\n", "def test_from_ordinalf_np_vectorized():\n    # Test the np.vectorize'd version of _from_ordinalf\n\n    # with timezone set to None (UTC)\n    dates = mdates.num2date([1, 2, 3, 4, 5])\n    d = np.array([mdates.date2num(date) for date in dates])\n    np.testing.assert_array_equal(dates,\n                                  mdates._from_ordinalf_np_vectorized(d))\n\n    # with timezone set to a specific tz\n    tz = dateutil.tz.gettz(\"US/Eastern\")\n    dates = mdates.num2date([1, 2, 3, 4, 5], tz=tz)\n    d = np.array([mdates.date2num(date) for date in dates])\n    np.testing.assert_array_equal(dates,\n                                  mdates._from_ordinalf_np_vectorized(d, tz))\n", "def test_DateFormatter_tz():\n    # test timezone behavior for DateFormatter\n\n    # setup timezone\n    utc = mdates.UTC\n    pacific = dateutil.tz.gettz('US/Pacific')\n    eastern = dateutil.tz.gettz('US/Eastern')\n\n    # setup data\n    data = [datetime.datetime(2022, 9, 1, tzinfo=utc),\n            datetime.datetime(2022, 9, 1, tzinfo=pacific),\n            datetime.datetime(2022, 9, 1, tzinfo=eastern)]\n\n    # setup formatter\n    fmt = mdates.DateFormatter('%m/%d/%y', tz=pacific)\n\n    # test data w/ formatter\n    result = [fmt(x, 0) for x in data]\n    expected = ['09/01/22', '09/01/22', '09/01/22']\n    assert result == expected\n\n    # test data w/ timezone\n    fmt.set_tzinfo(utc)\n    result = [fmt(x, 0) for x in data]\n    expected = ['09/01/22', '08/31/22', '09/01/22']\n    assert result == expected\n"], "sample_821": ["def test_affinity_propagation_affinity_matrix():\n    # Test that affinity matrix is correctly stored\n    af = AffinityPropagation(affinity=\"euclidean\")\n    af.fit(X)\n    assert_array_equal(af.affinity_matrix_, -euclidean_distances(X, squared=True))\n\n    # Test that affinity matrix is correctly stored when precomputed\n    af = AffinityPropagation(affinity=\"precomputed\")\n    S = -euclidean_distances(X, squared=True)\n    af.fit(S)\n    assert_array_equal(af.affinity_matrix_, S)\n", "def test_affinity_propagation_affinity_precomputed_sparse():\n    # Test with precomputed affinity matrix as sparse matrix\n    S = -euclidean_distances(X, squared=True)\n    S_sparse = csr_matrix(S)\n    af = AffinityPropagation(preference=np.median(S) * 10, affinity=\"precomputed\")\n    labels = af.fit(S_sparse).labels_\n    af_2 = AffinityPropagation(preference=np.median(S) * 10)\n    labels_2 = af_2.fit(X).labels_\n    assert_array_equal(labels, labels_2)\n", "def test_affinity_propagation_affinity_matrix_setting():\n    # Test setting affinity matrix attribute\n    af = AffinityPropagation(affinity=\"precomputed\")\n    S = -euclidean_distances(X, squared=True)\n    af.fit(S)\n    assert_array_equal(af.affinity_matrix_, S)\n", "def test_affinity_propagation_damping_value():\n    # Test AffinityPropagation with damping factor out of bounds.\n    assert_raises(ValueError, AffinityPropagation, damping=0.3)\n    assert_raises(ValueError, AffinityPropagation, damping=1.2)\n", "def test_affinity_propagation_affinity_euclidean():\n    # Test that affinity='euclidean' produces the same results as\n    # passing the precomputed affinity matrix\n    af_euclidean = AffinityPropagation(affinity='euclidean')\n    af_precomputed = AffinityPropagation(affinity='precomputed')\n\n    S = -euclidean_distances(X, squared=True)\n    af_euclidean.fit(X)\n    af_precomputed.fit(S)\n\n    assert_array_equal(af_euclidean.cluster_centers_indices_,\n                       af_precomputed.cluster_centers_indices_)\n    assert_array_equal(af_euclidean.labels_, af_precomputed.labels_)\n"], "sample_808": ["def test_iforest_subsampled_features_with_contamination():\n    # It tests non-regression for #5732 which failed at predict with contamination.\n    rng = check_random_state(0)\n    X_train, X_test, y_train, y_test = train_test_split(boston.data[:50],\n                                                        boston.target[:50],\n                                                        random_state=rng)\n    clf = IsolationForest(max_features=0.8, contamination=0.5)\n    clf.fit(X_train, y_train)\n    clf.predict(X_test)\n", "def test_iforest_subsampled_features_max_features(max_features):\n    # It tests non-regression for #5732 which failed at predict.\n    rng = check_random_state(0)\n    X_train, X_test, y_train, y_test = train_test_split(boston.data[:50],\n                                                        boston.target[:50],\n                                                        random_state=rng)\n    clf = IsolationForest(max_features=max_features)\n    clf.fit(X_train, y_train)\n    clf.predict(X_test)\n", "def test_iforest_verbose():\n    \"\"\"Test verbose parameter in IsolationForest\"\"\"\n    X_train = [[1, 1], [1, 2], [2, 1]]\n    clf1 = IsolationForest(verbose=0).fit(X_train)\n    clf2 = IsolationForest(verbose=1).fit(X_train)\n    clf3 = IsolationForest(verbose=2).fit(X_train)\n    assert_array_equal(clf1.estimators_, clf2.estimators_)\n    assert_array_equal(clf1.estimators_, clf3.estimators_)\n", "def test_iforest_fit_twice():\n    \"\"\"Test Isolation Forest fit method can be called multiple times.\"\"\"\n    X_train = [[1, 1], [1, 2], [2, 1]]\n    X_test = [[2., 2.]]\n    clf1 = IsolationForest(contamination=0.1).fit(X_train)\n    y_pred1 = clf1.predict(X_test)\n    clf1.fit(X_train)\n    y_pred2 = clf1.predict(X_test)\n    assert_array_equal(y_pred1, y_pred2)\n", "def test_iforest_parallel_chunks(n_estimators, n_jobs, expected_chunks):\n    X_train = np.random.rand(100, 5)\n    clf = IsolationForest(n_estimators=n_estimators, n_jobs=n_jobs)\n    clf.fit(X_train)\n    assert clf._compute_chunked_score_samples(X_train).shape[0] == expected_chunks\n"], "sample_755": ["def test_silhouette_samples_sparse_labels():\n    # Test silhouette_samples with sparse labels\n    dataset = datasets.load_iris()\n    X = dataset.data\n    y = sp.csr_matrix(dataset.target)\n\n    with pytest.raises(ValueError, match=\"labels must be 1d\"):\n        silhouette_samples(X, y)\n\n    y = y.toarray().ravel()\n    score = silhouette_samples(X, y)\n    assert not np.isnan(score).any()\n", "def test_silhouette_samples_perfectly_separated_clusters():\n    # Test the case where clusters are perfectly separated.\n    X = np.array([[1, 1], [1, 1], [1, 1], [10, 10], [10, 10], [10, 10]])\n    labels = np.array([0, 0, 0, 1, 1, 1])\n    scores = silhouette_samples(X, labels)\n    expected = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n    assert_array_equal(scores, expected)\n", "def test_silhouette_score_with_random_state():\n    # Test that silhouette_score with sample_size returns same results with\n    # the same random_state\n    dataset = datasets.load_iris()\n    X = dataset.data\n    y = dataset.target\n\n    score1 = silhouette_score(X, y, sample_size=int(X.shape[0] / 2),\n                              random_state=0)\n    score2 = silhouette_score(X, y, sample_size=int(X.shape[0] / 2),\n                              random_state=0)\n    assert score1 == score2\n\n    # Test that silhouette_score with sample_size returns different results with\n    # different random_state\n    score3 = silhouette_score(X, y, sample_size=int(X.shape[0] / 2),\n                              random_state=1)\n    assert score1 != score3\n", "def test_silhouette_samples_sparse_input():\n    \"\"\"Test silhouette_samples with sparse input.\"\"\"\n    # Generate a random sparse matrix\n    X_sparse = sp.csr_matrix(np.random.rand(10, 10))\n    X_dense = X_sparse.toarray()\n\n    # Generate some labels\n    labels = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 2])\n\n    # Calculate silhouette samples for both sparse and dense inputs\n    silhouette_sparse = silhouette_samples(X_sparse, labels)\n    silhouette_dense = silhouette_samples(X_dense, labels)\n\n    # Check that the results are the same\n    assert_array_equal(silhouette_sparse, silhouette_dense)\n", "def test_silhouette_samples_with_one_cluster():\n    # Test silhouette_samples when there is only one cluster\n    # with multiple samples.\n    X = np.array([[0.2, 0.1], [0.12, 1.34], [1.11, 1.6]])\n    labels = np.array([0, 0, 0])\n    expected = np.array([1., 1., 1.])\n    pytest.approx(silhouette_samples(X, labels), expected)\n"], "sample_65": ["    def test_js_catalog_view(self):\n        \"\"\"Test JavaScriptCatalog view.\"\"\"\n        response = self.client.get('/jsi18n/')\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response['Content-Type'], 'text/javascript; charset=\"utf-8\"')\n", "    def test_get_plural(self):\n        view = JavaScriptCatalog.as_view()\n        request = RequestFactory().get('/')\n        view(request, packages='django')\n        self.assertEqual(view._num_plurals, 2)\n        self.assertEqual(view.get_plural(), '(n != 1)')\n", "    def test_js_catalog_empty_catalog(self):\n        \"\"\"\n        Ensure that an empty catalog is returned when no translations are found.\n        \"\"\"\n        with override('xx'):\n            response = self.client.get('/jsi18n/')\n            self.assertEqual(response.status_code, 200)\n            self.assertEqual(response['Content-Type'], 'text/javascript; charset=\"utf-8\"')\n            self.assertContains(response, 'django.catalog = {}')\n            self.assertContains(response, 'django.formats = {}')\n            self.assertContains(response, 'django.pluralidx = function(count) { return (count == 1) ? 0 : 1; };')\n", "    def test_jsi18n_catalog_empty(self):\n        view = JavaScriptCatalog.as_view()\n        request = RequestFactory().get('/')\n        response = view(request)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response['Content-Type'], 'text/javascript; charset=\"utf-8\"')\n", "    def test_i18n_with_custom_locale_path(self):\n        \"\"\"Test that JavaScriptCatalog can handle a custom locale path.\"\"\"\n        extended_locale_paths = settings.LOCALE_PATHS + [\n            path.join(\n                path.dirname(path.dirname(path.abspath(__file__))),\n                'custom_locale',\n            ),\n        ]\n        with self.settings(LOCALE_PATHS=extended_locale_paths):\n            with override('en-us'):\n                response = self.client.get('/jsi18n/')\n                self.assertContains(response, 'Custom translation')\n"], "sample_839": ["def test_vectorizer_subclass_with_get_stop_words():\n    class CustomVectorizer(CountVectorizer):\n            return super().get_stop_words()\n\n    cv = CustomVectorizer()\n    cv.set_params(stop_words='english')\n    assert cv.get_stop_words() == ENGLISH_STOP_WORDS\n", "def test_vectorizer_on_empty_string(Estimator):\n    # raw documents\n    data = ['']\n\n    # label junk food as -1, the others as +1\n    target = [0]\n\n    # split the dataset for model development and final evaluation\n    train_data = data\n    target_train = target\n\n    vectorizer = Estimator()\n\n    # find the best parameters for both the feature extraction and the\n    # classifier\n    X = vectorizer.fit_transform(train_data)\n    assert X.shape == (len(train_data), 0)\n    assert vectorizer.get_feature_names() == []\n", "def test_vectorizer_max_features_with_stop_words():\n    test_data = ['aaabc', 'abbde', 'aadee']\n    vect = CountVectorizer(max_df=1.0, stop_words='english')\n    vect.fit(test_data)\n    assert 'a' not in vect.vocabulary_.keys()  \n    assert len(vect.vocabulary_.keys()) == 3    # {bcde} remain\n    assert 'a' in vect.stop_words_\n    assert len(vect.stop_words_) == 1\n\n    vect.max_features = 2\n    vect.fit(test_data)\n    assert len(vect.vocabulary_.keys()) == 2\n    assert len(vect.stop_words_) == 2\n", "def test_vectorizer_input_type():\n    # Test that input type is correctly validated\n    docs = ['This is a sample document', 'Another sample document']\n    for input_type in ['filename', 'file', 'content']:\n        vec = CountVectorizer(input=input_type)\n        vec.fit_transform(docs)\n\n    # Test that an invalid input type raises an error\n    for input_type in ['invalid', 'invalid_type']:\n        vec = CountVectorizer(input=input_type)\n        with pytest.raises(ValueError):\n            vec.fit_transform(docs)\n", "def test_vectorizer_strip_accents_unicode():\n    # test strip_accents with unicode method for character n-grams\n    text = \"caf\u00e9 restaurant\"\n    expected = \"cafe restaurant\"\n    cv = CountVectorizer(analyzer='char', strip_accents='unicode')\n    X = cv.fit_transform([text]).toarray()\n    assert_array_equal(cv.get_feature_names(), [' ', 'a', 'c', 'e',\n                                               'f', 'n', 'r', 's', 't'])\n    assert_array_equal(X, [[1, 1, 1, 1, 1, 1, 1, 1, 1]])\n\n    # test strip_accents with unicode method for word n-grams\n    text = \"caf\u00e9 restaurant\"\n    expected = \"cafe restaurant\"\n    cv = CountVectorizer(analyzer='word', strip_accents='unicode')\n    X = cv.fit_transform([text]).toarray()\n    assert_array_equal(cv.get_feature_names(), ['cafe', 'restaurant'])\n    assert_array_equal(X, [[1, 1]])\n"], "sample_229": ["def test_union_with_values_and_alias(self):\n    ReservedName.objects.create(name='a', order=2)\n    qs1 = ReservedName.objects.annotate(alias=F('order')).values('name', 'alias', 'id')\n    reserved_name = qs1.union(qs1).get()\n    self.assertEqual(reserved_name['name'], 'a')\n    self.assertEqual(reserved_name['alias'], 2)\n    reserved_name = qs1.union(qs1).values_list('name', 'alias', 'id').get()\n    self.assertEqual(reserved_name[:2], ('a', 2))\n", "def test_union_with_aggregate(self):\n    qs1 = Number.objects.filter(num__lte=1).annotate(avg=Avg('num'))\n    qs2 = Number.objects.filter(num__gte=2, num__lte=3).annotate(avg=Avg('num'))\n    self.assertEqual(qs1.union(qs2).aggregate(avg=Avg('avg'))['avg'], 1.5)\n    self.assertEqual(qs1.union(qs2, all=True).aggregate(avg=Avg('avg'))['avg'], 1.0)\n", "def test_get_with_combined_qs_and_values_list(self):\n    ReservedName.objects.create(name='a', order=2)\n    qs = ReservedName.objects.all().values_list('name', 'order', 'id').union(ReservedName.objects.all().values_list('name', 'order', 'id'))\n    reserved_name = qs.get()\n    self.assertEqual(reserved_name[:2], ('a', 2))\n", "def test_union_with_values_on_different_fields(self):\n    ReservedName.objects.create(name='a', order=2)\n    ReservedName.objects.create(name='b', order=4)\n    qs1 = ReservedName.objects.filter(name='a').values('order')\n    qs2 = ReservedName.objects.filter(name='b').values('name')\n    self.assertEqual(list(qs1.union(qs2)), [(2,), ('b',)])\n", "def test_union_with_get_and_slice(self):\n    qs = Number.objects.all()\n    union = qs.union(qs)\n    obj = union.get(num=2)\n    self.assertEqual(obj.num, 2)\n    self.assertEqual(union[:10].count(), 10)\n    self.assertEqual(union[:20].count(), 10)\n    with self.assertRaises(NotSupportedError):\n        union[10:20].get(num=2)\n"], "sample_657": ["def test_store_mark():\n    obj = mock.Mock()\n    mark = Mark(\"some_mark\", (), {})\n    store_mark(obj, mark)\n    assert obj.pytestmark == [mark]\n\n    store_mark(obj, mark)\n    assert obj.pytestmark == [mark, mark]\n", "def test_mark_generator_update(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        markers =\n            a1: this is a webtest marker\n            a2: this is a smoke marker\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.a1\n            assert True\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--strict-markers\")\n    assert result.ret == 0\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        markers =\n            a1: this is a webtest marker\n            a2: this is a smoke marker\n            a3: this is a new marker\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--strict-markers\")\n    assert result.ret == 0\n    result = testdir.runpytest(\"--strict-markers\", \"-m\", \"a3\")\n    assert result.ret == 0\n", "def test_ParameterSet_extract_from(testdir):\n    from _pytest.mark import ParameterSet\n\n    # test with a tuple\n    assert ParameterSet.extract_from((1, 2, 3)) == ParameterSet(values=(1, 2, 3), marks=[], id=None)\n\n    # test with a single value\n    assert ParameterSet.extract_from(1) == ParameterSet(values=(1,), marks=[], id=None)\n\n    # test with a single value and force_tuple=True\n    assert ParameterSet.extract_from(1, force_tuple=True) == ParameterSet(values=(1,), marks=[], id=None)\n\n    # test with a ParameterSet instance\n    param_set = ParameterSet(values=(1, 2, 3), marks=[pytest.mark.skip], id=\"my_id\")\n    assert ParameterSet.extract_from(param_set) == param_set\n\n    # test with a non-iterable value\n    with pytest.raises(TypeError):\n        ParameterSet.extract_from(None)\n", "def test_get_unpacked_marks_with_none_pytestmark(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n            pass\n    \"\"\"\n    )\n    p = testdir.parseconfig()\n    node = Node(\"Test\", config=p, session=mock.Mock(), nodeid=\"Test\")\n    node.pytestmark = None\n    marks = get_unpacked_marks(node)\n    assert len(marks) == 0\n", "def test_normalize_mark_list(testdir):\n    mark1 = pytest.mark.marker1\n    mark2 = pytest.mark.marker2\n    mark_decorator1 = pytest.mark.marker1(\"arg1\", arg2=\"kwarg1\")\n    mark_decorator2 = pytest.mark.marker2(\"arg2\", arg3=\"kwarg2\")\n\n    # Test normalization of mark objects\n    mark_list = [mark1, mark2]\n    expected = [mark1.mark, mark2.mark]\n    assert normalize_mark_list(mark_list) == expected\n\n    # Test normalization of mark decorators\n    mark_list = [mark_decorator1, mark_decorator2]\n    expected = [mark_decorator1.mark, mark_decorator2.mark]\n    assert normalize_mark_list(mark_list) == expected\n\n    # Test normalization of mixed mark objects and decorators\n    mark_list = [mark1, mark_decorator2, mark_decorator1, mark2]\n    expected = [mark1.mark, mark_decorator2.mark, mark_decorator1.mark, mark2.mark]\n    assert normalize_mark_list(mark_list) == expected\n\n    # Test that non-mark objects raise a TypeError\n    mark_list = [mark1, \"non-mark object\"]\n    with pytest.raises(TypeError):\n        normalize_mark_list(mark_list)\n"], "sample_664": ["def test_funcargnames_attribute_deprecated(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            pass\n\n            assert my_fixture.funcargnames == ['my_fixture']\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: The `funcargnames` attribute was an alias for `fixturenames`, \"\n            \"since pytest 2.3 - use the newer attribute instead.*\"\n        ]\n    )\n\n", "def test_deprecation_warnings(testdir, warning, expected_msg):\n    testdir.makepyfile(\n        \"\"\"\n            import warnings\n            with warnings.catch_warnings(record=True) as w:\n                warnings.simplefilter(\"always\", category=warning.category)\n                raise warning\n            assert len(w) == 1\n            assert w[0].category == warning.category\n            assert str(w[0].message) == warning.message\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([f\"*{expected_msg}*\"])\n", "def test_funcargnames_deprecated(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            return 42\n\n            assert my_fixture.funcargnames == ['my_fixture']\n\n            assert my_fixture.fixturenames == my_fixture.funcargnames\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: The `funcargnames` attribute was an alias for `fixturenames`, since pytest 2.3 - use the newer attribute instead.*\"\n        ]\n    )\n\n", "def test_funcargnames_deprecation(testdir):\n    \"\"\"Test that using the funcargnames attribute raises a deprecation warning.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n            class TestClass:\n                    pass\n            test_funcargnames = TestClass.funcargnames\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: The `funcargnames` attribute was an alias for `fixturenames`, since pytest 2.3 - use the newer attribute instead.\",\n        ]\n    )\n\n", "def test_fixtures_positional_arguments_are_deprecated(testdir, fixture_declaration):\n    \"\"\"Check that positional arguments to pytest.fixture() are deprecated.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        {}(\"arg1\", \"arg2\")\n            pass\n    \"\"\".format(\n            fixture_declaration\n        )\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: Passing arguments to pytest.fixture() as positional arguments is deprecated - pass them as a keyword argument instead.\"\n        ]\n    )\n\n"], "sample_211": ["    def test_get_context_data(self):\n        test_view = views.ContextMixin()\n        context = test_view.get_context_data(foo='bar')\n        self.assertEqual(context['view'], test_view)\n        self.assertEqual(context['foo'], 'bar')\n", "    def test_context_mixin(self):\n        view = views.SimpleContextMixinView()\n        context = view.get_context_data(test_name='test_value')\n        self.assertEqual(context['test_name'], 'test_value')\n        self.assertEqual(context['view'], view)\n", "    def test_context_mixin(self):\n        class TestView(ContextMixin, View):\n            extra_context = {'key': 'value'}\n\n        view = TestView()\n        context = view.get_context_data(foo='bar')\n        self.assertEqual(context['foo'], 'bar')\n        self.assertEqual(context['key'], 'value')\n        self.assertEqual(context['view'], view)\n", "    def test_render_to_response_context_is_none(self):\n        \"\"\"\n        Test render_to_response() when context is None.\n        \"\"\"\n        class TestView(TemplateView):\n            template_name = 'template.html'\n\n                return self.render_to_response(None)\n\n        view = TestView.as_view()\n        response = view(self.rf.get('/'))\n        self.assertEqual(response.status_code, 200)\n", "    def test_dispatch_exception_propagation(self):\n        \"\"\"\n        Test that exceptions raised in the dispatch method are propagated.\n        \"\"\"\n        class TestView(View):\n                raise ValueError(\"Test exception\")\n\n        view = TestView.as_view()\n        request = self.rf.get('/')\n        with self.assertRaises(ValueError):\n            view(request)\n"], "sample_1164": ["def test_cg_simp():\n    a = CG(1, 1, 0, 0, 1, 1)\n    b = CG(1, 0, 0, 0, 1, 0)\n    c = CG(1, -1, 0, 0, 1, -1)\n    assert cg_simp(a+b+c) == 3\n\n    a = CG(1, 1, 0, 0, 1, 1)\n    b = CG(1, 0, 0, 0, 1, 0)\n    c = CG(1, -1, 0, 0, 1, -1)\n    d = 2\n    assert cg_simp(a+b+c+d) == 5\n", "def test_cg_simp():\n    a = CG(1, 1, 0, 0, 1, 1)\n    b = CG(1, 0, 0, 0, 1, 0)\n    c = CG(1, -1, 0, 0, 1, -1)\n    assert cg_simp(a + b + c) == 3\n    assert cg_simp((a + b + c)**2) == 9\n    assert cg_simp((a + b + c)**3) == 27\n    d = CG(1, 1, 0, 0, 1, 1) * CG(1, 1, 0, 0, 1, 1)\n    e = CG(1, 0, 0, 0, 1, 0) * CG(1, 0, 0, 0, 1, 0)\n    f = CG(1, -1, 0, 0, 1, -1) * CG(1, -1, 0, 0, 1, -1)\n    assert cg_simp(d + e + f) == 3\n", "def test_cg_simp():\n    a = symbols('a')\n    alpha = symbols('alpha')\n    b = symbols('b')\n    beta = symbols('beta')\n    c = symbols('c')\n    gamma = symbols('gamma')\n    cg1 = CG(a, alpha, b, beta, c, gamma)\n    cg2 = CG(a, alpha, b, beta, c, gamma)\n    assert cg_simp(cg1) == cg1\n    assert cg_simp(cg1 + cg2) == cg1 + cg2\n    assert cg_simp(cg1 + cg1) == 2*cg1\n    assert cg_simp(cg1*cg2) == cg1*cg2\n    assert cg_simp(cg1*cg1) == cg1*cg1\n    assert cg_simp(cg1**2) == cg1**2\n    assert cg_simp(cg1 + cg1**2) == cg1 + cg1**2\n    assert cg_simp(cg1*cg1**2) == cg1*cg1**2\n    assert cg_simp(cg1**2 + cg1**3) == cg1**2 + cg1**3\n    assert cg_simp(cg1*cg1**2 + cg1*cg1**3) == cg1**3 + cg1**4\n\n    # Test simplification of sums\n    assert cg_simp(Sum(CG(a, alpha, b, 0, a, alpha), (alpha, -a, a))) == (2*a + 1)*KroneckerDelta(b, 0)\n    assert cg_simp(Sum((-1)**(a - alpha)*CG(a, alpha, a, -alpha, c, 0), (alpha, -a, a))) == sqrt(2*a + 1)*KroneckerDelta(c, 0)\n    assert cg_simp(Sum(CG(a, alpha, b, beta, c, gamma)*CG(a, alpha, b, beta, c, gamma), (gamma, -c, c), (c, abs(a-b), a+b))) == 1\n\n    # Test simplification of sum with non-CG terms\n    assert cg_simp(Sum(a*CG(a, alpha, b, 0, a, alpha), (alpha, -a, a))) == a*(2*a +", "def test_cg_simp():\n    from sympy.physics.quantum.cg import CG, cg_simp\n    a, b = symbols('a b', positive=True)\n    alpha, beta = symbols('alpha beta')\n    m = symbols('m', integer=True)\n    s = (CG(a, alpha, b, -alpha, 0, 0) + CG(a, -alpha, b, alpha, 0, 0))\n    assert cg_simp(s) == (2*a + 1)\n    s = (CG(a, alpha, b, m, a, m) + CG(a, alpha, b, m, a, m))\n    assert cg_simp(s) == 2*CG(a, alpha, b, m, a, m)\n    s = CG(a, alpha, b, beta, a, alpha + beta)\n    assert cg_simp(s) == s\n    s = (CG(a, alpha, b, beta, a + b, alpha + beta)**2)\n    assert cg_simp(s) == s\n    s = (CG(a, alpha, b, beta, a + b, alpha + beta)**2 + CG(a, alpha, b, beta, a + b, alpha + beta)**2)\n    assert cg_simp(s) == 2*CG(a, alpha, b, beta, a + b, alpha + beta)**2\n    s = (CG(a, alpha, b, beta, a + b, alpha + beta)**2 + CG(a, alpha, b, beta, a + b, alpha + beta)**3)\n    assert cg_simp(s) == (CG(a, alpha, b, beta, a + b, alpha + beta)**2 + CG(a, alpha, b, beta, a + b, alpha + beta)**3)\n", "def test_cg_simp():\n    from sympy.physics.quantum.cg import CG, cg_simp\n    from sympy import symbols, S\n    a, alpha, b = symbols('a alpha b')\n\n    # Test _check_varsh_871_1\n    expr = CG(a, alpha, b, 0, a, alpha)\n    assert str(cg_simp(expr)) == 'CG(a, alpha, b, 0, a, alpha)'\n    assert str(cg_simp(expr + expr)) == '2*CG(a, alpha, b, 0, a, alpha)'\n    assert str(cg_simp(expr + expr.subs(alpha, -alpha))) == '2*CG(a, alpha, b, 0, a, alpha)'\n\n    # Test _check_varsh_871_2\n    expr = (-1)**(a - alpha)*CG(a, alpha, a, -alpha, b, 0)\n    assert str(cg_simp(expr)) == 'CG(a, alpha, a, -alpha, b, 0)'\n    assert str(cg_simp(expr + expr.subs(alpha, -alpha))) == 'sqrt(2*a + 1)*KroneckerDelta(b, 0)'\n\n    # Test _check_varsh_872_4\n    expr = CG(a, alpha, b, b, b, 0)*CG(a, alpha, b, b, b, 0)\n    assert str(cg_simp(expr)) == 'KroneckerDelta(b, b)'\n    expr = CG(a, alpha, b, b, b, 0)*CG(a, alpha, b, b, b, 0).subs(b, b + 1)\n    assert str(cg_simp(expr)) == 'CG(a, alpha, b, b, b + 1, 0)*CG(a, alpha, b, b, b + 1, 0)'\n    expr = CG(a, alpha, b, b, b, 0) + CG(a, alpha, b, b, b, 0).subs(b, b + 1)\n    assert str(cg_simp(expr)) == 'CG(a, alpha, b, b, b, 0) + CG(a, alpha, b, b, b + 1, 0)'\n\n    # Test numerical simplification\n    expr = CG(S(1)/2"], "sample_988": ["def test_issue_13470():\n    assert Integer(long('12345678901234567890')) is S.Integer('12345678901234567890')\n", "def test_issue_13092():\n    from sympy import symbols\n    x = symbols('x', real=True, positive=True)\n    assert Gt(x, 0).func is StrictGreaterThan\n    assert Ge(x, 0).func is GreaterThan\n    assert Lt(x, 0).func is StrictLessThan\n    assert Le(x, 0).func is LessThan\n", "def test_issue_17034():\n    \"\"\"Test issue #17034\"\"\"\n    x = symbols('x')\n    assert Eq(x, oo).free_symbols == {x}\n    assert Eq(x, -oo).free_symbols == {x}\n    assert Ne(x, oo).free_symbols == {x}\n    assert Ne(x, -oo).free_symbols == {x}\n    assert Gt(x, oo).free_symbols == {x}\n    assert Lt(x, -oo).free_symbols == {x}\n    assert Ge(x, oo).free_symbols == {x}\n    assert Le(x, -oo).free_symbols == {x}\n", "def test_issue_10655():\n    # Test that pi, E and I can be compared to Rational.\n    # The comparison should result in True or False.\n    assert (pi > Rational(3)) is S.true\n    assert (E > Rational(2)) is S.true\n    assert (I > Rational(2)) is S.false\n", "def test_sorting():\n    assert Lt(0, 1, evaluate=False).args == (0, 1)\n    assert Lt(1, 0, evaluate=False).args == (0, 1)\n    assert Lt(-1, 1, evaluate=False).args == (-1, 1)\n    assert Lt(-1, -2, evaluate=False).args == (-2, -1)\n    assert Lt(I, 0, evaluate=False).args == (I, 0)\n    assert Lt(I, 1, evaluate=False).args == (I, 1)\n    assert Lt(-I, 0, evaluate=False).args == (-I, 0)\n    assert Lt(-I, I, evaluate=False).args == (-I, I)\n    assert Lt(x, 0, evaluate=False).args == (x, 0)\n    assert Lt(0, x, evaluate=False).args == (x, 0)\n    assert Lt(1, x, evaluate=False).args == (1, x)\n    assert Lt(x, 1, evaluate=False).args == (x, 1)\n    assert Lt(x, -1, evaluate=False).args == (-1, x)\n    assert Lt(-1, x, evaluate=False).args == (-1, x)\n\n    assert Lt(x, y, evaluate=False).args == (x, y)\n    assert Lt(y, x, evaluate=False).args == (x, y)\n    assert Lt(x, y, evaluate=False).reversed.args == (x, y)\n    assert Lt(y, x, evaluate=False).reversed.args == (x, y)\n\n    assert Lt(0, x, evaluate=False).args == (x, 0)\n    assert Lt(x, 0, evaluate=False).args == (x, 0)\n    assert Lt(1, x, evaluate=False).args == (1, x)\n    assert Lt(x, 1, evaluate=False).args == (x, 1)\n    assert Lt(-1, x, evaluate=False).args == (-1, x)\n    assert Lt(x, -1, evaluate=False).args == (-1, x)\n\n    assert Lt(x, y, evaluate=False).args == (x, y)\n    assert Lt(y, x, evaluate=False).args == (x, y)\n    assert Lt(x, y, evaluate=False).reversed.args == (x, y)\n   "], "sample_1169": ["def test_dummy_order_inner_outer_lines_VT1T1T1T1_AT_with_fermi_level():\n    i, j = symbols('i j', below_fermi=True)\n    a, b = symbols('a b', above_fermi=True)\n    k, l = symbols('k l', below_fermi=True, cls=Dummy)\n    c, d = symbols('c d', above_fermi=True, cls=Dummy)\n\n    from sympy.physics.secondquant import FKet\n\n    # Coupled-Cluster T2 terms with V*T1*T1*T1*T1\n    # t^{a}_{k} t^{c}_{i} t^{d}_{l} v^{lk}_{dc}\n    exprs = [\n        atv(k, l, c, d)*att(c, i)*att(d, l)*att(a, k)*FKet([], 4),\n        atv(l, k, c, d)*att(c, i)*att(d, k)*att(a, l)*FKet([], 4),\n        atv(k, l, d, c)*att(d, i)*att(c, l)*att(a, k)*FKet([], 4),\n        atv(l, k, d, c)*att(d, i)*att(c, k)*att(a, l)*FKet([], 4),\n    ]\n    for permut in exprs[1:]:\n        assert substitute_dummies(exprs[0]) == substitute_dummies(permut)\n", "def test_latex_printing():\n    from sympy.physics.secondquant import latex, AnnihilateBoson, CreateBoson, Dagger, AnnihilateFermion, CreateFermion\n    from sympy import symbols, sqrt\n\n    i, j = symbols('i j', below_fermi=True)\n    a, b = symbols('a b', above_fermi=True)\n\n    assert latex(AnnihilateBoson(0)) == 'b_{0}'\n    assert latex(CreateBoson(0)) == '{b^\\\\dagger_{0}}'\n    assert latex(Dagger(AnnihilateBoson(0))) == '{b^\\\\dagger_{0}}'\n    assert latex(Dagger(CreateBoson(0))) == 'b_{0}'\n\n    assert latex(AnnihilateFermion(0)) == 'a_{0}'\n    assert latex(CreateFermion(0)) == '{a^\\\\dagger_{0}}'\n    assert latex(Dagger(AnnihilateFermion(0))) == '{a^\\\\dagger_{0}}'\n    assert latex(Dagger(CreateFermion(0))) == 'a_{0}'\n\n    assert latex(AnnihilateBoson(i)) == 'b_{i}'\n    assert latex(CreateBoson(i)) == '{b^\\\\dagger_{i}}'\n    assert latex(Dagger(AnnihilateBoson(i))) == '{b^\\\\dagger_{i}}'\n    assert latex(Dagger(CreateBoson(i))) == 'b_{i}'\n\n    assert latex(AnnihilateFermion(i)) == 'a_{i}'\n    assert latex(CreateFermion(i)) == '{a^\\\\dagger_{i}}'\n    assert latex(Dagger(AnnihilateFermion(i))) == '{a^\\\\dagger_{i}}'\n    assert latex(Dagger(CreateFermion(i))) == 'a_{i}'\n\n    assert latex(AnnihilateBoson(a)) == 'b_{a}'\n    assert latex(CreateBoson(a)) == '{b^\\\\dagger_{a}}'\n    assert latex(Dagger(AnnihilateBoson(a))) == '{b^\\\\dagger_{a}}'\n    assert latex(Dagger(CreateBoson(a))) == 'b_{a}'\n\n    assert latex(AnnihilateFermion(a))", "def test_wicks_contraction():\n    i, j = symbols('i j', below_fermi=True)\n    a, b = symbols('a b', above_fermi=True)\n    p, q = symbols('p q')\n\n    expr = Fd(p)*Fd(q)*F(a)*F(b)\n    result = wicks(expr)\n    expected = NO(Fd(p)*Fd(q)*F(a)*F(b)) - KroneckerDelta(p, a)*NO(Fd(q)*F(b)) + KroneckerDelta(q, b)*NO(Fd(p)*F(a)) - KroneckerDelta(p, a)*KroneckerDelta(q, b)\n    assert result == expected\n\n    expr = Fd(p)*Fd(q)*F(i)*F(j)\n    result = wicks(expr)\n    expected = NO(Fd(p)*Fd(q)*F(i)*F(j)) - KroneckerDelta(p, i)*NO(Fd(q)*F(j)) + KroneckerDelta(q, j)*NO(Fd(p)*F(i)) - KroneckerDelta(p, i)*KroneckerDelta(q, j)\n    assert result == expected\n\n    expr = Fd(p)*Fd(q)*F(a)*F(i)\n    result = wicks(expr)\n    expected = NO(Fd(p)*Fd(q)*F(a)*F(i)) - KroneckerDelta(p, a)*NO(Fd(q)*F(i)) - KroneckerDelta(q, i)*NO(Fd(p)*F(a))\n    assert result == expected\n\n    expr = Fd(p)*Fd(q)*F(b)*F(j)\n    result = wicks(expr)\n    expected = NO(Fd(p)*Fd(q)*F(b)*F(j)) - KroneckerDelta(q, j)*NO(Fd(p)*F(b)) - KroneckerDelta(p, b)*NO(Fd(q)*F(j))\n    assert result == expected\n", "def test_fermion_state():\n    i, j, k, l = symbols('i j k l', below_fermi=True)\n    a, b, c, d = symbols('a b c d', above_fermi=True)\n    p, q = symbols('p q')\n    fock = FKet([p])\n    assert fock.fermi_level == 0\n    fock = FKet([], fermi_level=3)\n    assert fock.fermi_level == 3\n    assert fock.is_below_fermi(i)\n    assert fock.is_below_fermi(p)\n    assert fock.is_above_fermi(a)\n    assert fock.is_above_fermi(p)\n    assert fock.is_only_below_fermi(i)\n    assert fock.is_only_above_fermi(a)\n    assert not fock.is_only_below_fermi(p)\n    assert not fock.is_only_above_fermi(p)\n    fock = FKet([], fermi_level=2)\n    assert not fock.is_below_fermi(i)\n    assert not fock.is_below_fermi(p)\n    assert not fock.is_above_fermi(a)\n    assert fock.is_above_fermi(p)\n    assert not fock.is_only_below_fermi(i)\n    assert fock.is_only_above_fermi(a)\n    assert not fock.is_only_below_fermi(p)\n    assert fock.is_only_above_fermi(p)\n    fock = FKet([a], fermi_level=2)\n    assert fock.is_below_fermi(i)\n    assert fock.is_below_fermi(p)\n    assert fock.is_above_fermi(a)\n    assert fock.is_above_fermi(p)\n    assert fock.is_only_below_fermi(i)\n    assert fock.is_only_above_fermi(a)\n    assert not fock.is_only_below_fermi(p)\n    assert not fock.is_only_above_fermi(p)\n    fock = FKet([i], fermi_level=2)\n    assert fock.is_below_fermi(i)\n    assert fock.is_below_fermi(p)\n    assert fock.is_above_fermi(a)\n    assert fock.is_above_fermi(p)\n    assert fock.is_only_below_fermi(i)\n    assert fock.is_only_above_fermi(a)\n    assert not fock.is", "def test_AntiSymmetricTensor_representation():\n    i, j, k, l = symbols('i j k l', below_fermi=True)\n    a, b, c, d = symbols('a b c d', above_fermi=True)\n    i_j = symbols('i_j', below_fermi=True)\n    a_b = symbols('a_b', above_fermi=True)\n\n    v = symbols('v', cls=Function)\n    v_latex = r'{v^{ab}_{ij}}'\n\n    # simple 2 index\n    expr = AntiSymmetricTensor('v', (a, b), (i, j))\n    assert latex(expr) == v_latex\n    assert latex(expr) == v_latex\n\n    # simple 1 index\n    expr = AntiSymmetricTensor('v', (a,), (i,))\n    assert latex(expr) == r'{v^{a}_{i}}'\n    expr = AntiSymmetricTensor('v', (a,), (i_j,))\n    assert latex(expr) == r'{v^{a}_{i_j}}'\n\n    # 2+2 index\n    expr = AntiSymmetricTensor('v', (a, b, c, d), (i, j, k, l))\n    assert latex(expr) == r'{v^{abcd}_{ijkl}}'\n    expr = AntiSymmetricTensor('v', (a, b, c, d), (i, j, k_l))\n    assert latex(expr) == r'{v^{abcd}_{ij{k_l}}}'\n\n    # 2+1 index\n    expr = AntiSymmetricTensor('v', (a, b, c), (i, j))\n    assert latex(expr) == r'{v^{abc}_{ij}}'\n    expr = AntiSymmetricTensor('v', (a, b, c), (i, j_k))\n    assert latex(expr) == r'{v^{abc}_{i{j_k}}}'\n\n    # 1+2 index\n    expr = AntiSymmetricTensor('v', (a,), (i, j, k))\n    assert latex(expr) == r'{v^{a}_{ijk}}'\n    expr = AntiSymmetricTensor('v', (a_b,), (i, j, k))\n    assert latex(expr) == r'{v^{{a_b}}_{ijk}}'\n\n    # 1+1 index\n    expr = AntiSymmetricTensor('v', (a"], "sample_496": ["    def test_command_parser_error(self):\n        parser = BaseCommand.CommandParser(None)\n        with self.assertRaises(CommandError):\n            parser.error('error message')\n", "    def test_handle_app_config_not_implemented(self):\n        class MyAppCommand(AppCommand):\n            pass\n\n        command = MyAppCommand()\n        with self.assertRaises(NotImplementedError):\n            command.handle_app_config(None)\n", "    def handle(self, *args, **options):\n        self.check()\n", "    def setUp(self):\n        self.write_settings('settings.py')\n", "    def test_run_from_argv_traps_command_error(self):\n        \"\"\"\n        Test that CommandError is caught by run_from_argv, and the error\n        message is printed to stderr.\n        \"\"\"\n        class TestCommand(BaseCommand):\n                raise CommandError(\"Test error\")\n\n        with self.assertRaises(SystemExit) as e:\n            TestCommand().run_from_argv(['command', 'arg'])\n\n        self.assertEqual(e.exception.code, 1)\n"], "sample_280": ["def test_aggregate_on_empty_queryset_with_distinct(self):\n    \"\"\"\n    An empty QuerySet with a distinct() call should not raise an error when\n    an aggregate is called on it.\n    \"\"\"\n    authors = Author.objects.none().distinct()\n    with self.assertNumQueries(0):\n        self.assertEqual(authors.aggregate(Avg('age')), {})\n", "def test_default_in_expression_annotation(self):\n    # Check that a default value can be used in an annotation with an expression.\n    publisher = Publisher.objects.annotate(\n        average_price_or_zero=Avg('book__price', default=0),\n        rating_or_price=(Avg('book__rating') * 10 + Sum('book__price')) / 2,\n    ).order_by('name')\n    self.assertEqual(\n        list(publisher.values('name', 'average_price_or_zero', 'rating_or_price')),\n        [\n            {'name': 'Apress', 'average_price_or_zero': Decimal('59.69') / 2, 'rating_or_price': (Decimal('59.69') / 2 + 8.0)},\n            {'name': \"Jonno's House of Books\", 'average_price_or_zero': Decimal('0.00'), 'rating_or_price': Decimal('0.00')},\n            {'name': 'Morgan Kaufmann', 'average_price_or_zero': Decimal('75.00'), 'rating_or_price': (Decimal('75.00') + 10.0)},\n            {'name': 'Prentice Hall', 'average_price_or_zero': Decimal('112.49') / 2, 'rating_or_price': (Decimal('112.49') / 2 + 8.0)},\n            {'name': 'Sams', 'average_price_or_zero': Decimal('23.09'), 'rating_or_price': (Decimal('23.09') + 6.0)},\n        ]\n    )\n", "def test_aggregate_window_function(self):\n    # Test that window functions can be used with aggregates\n    books = Book.objects.annotate(\n        mean_rating=Avg('rating') + Window(Avg('rating'), partition_by=F('publisher'))\n    ).order_by('pk')\n    self.assertEqual(\n        [book.mean_rating for book in books],\n        [Approximate(4.083, places=3) for _ in range(6)]\n    )\n\n    # Test that window functions can be used within aggregates\n    books = Book.objects.annotate(\n        max_mean_rating=Max(Window(Avg('rating'), partition_by=F('publisher')) + F('rating'))\n    ).order_by('pk')\n    self.assertEqual(\n        [book.max_mean_rating for book in books],\n        [Approximate(8.583, places=3), Approximate(7.083, places=3)] +\n        [Approximate(8.583, places=3), Approximate(8.583, places=3)] +\n        [Approximate(9.083, places=3), Approximate(9.083, places=3)]\n    )\n\n    # Test that aggregates can be used within window functions\n    books = Book.objects.annotate(\n        mean_max_rating=Window(Avg(Max('rating')), partition_by=F('publisher'))\n    ).order_by('pk')\n    self.assertEqual(\n        [book.mean_max_rating for book in books],\n        [Approximate(4.25, places=2), Approximate(4.25, places=2)] +\n        [Approximate(4.25, places=2), Approximate(4.25, places=2)] +\n        [Approximate(5.0, places=1), Approximate(5.0, places=1)]\n    )\n\n    # Test that aggregates can be used with window functions and other expressions\n    books = Book.objects.annotate(\n        mean_max_price=Window(Avg(Max('price')) + F('price'), partition_by=F('publisher'))\n    ).order_by('pk')\n    self.assertEqual(\n        [book.mean_max_price for book in books],\n        [Approximate(59.84, places=2), Approximate(59.84, places=2)] +\n        [Approximate(59.84, places=2), Approximate(59.84, places=2)] +\n        [Approximate(157.8, places=1), Approximate(157.8, places=1)]\n    )\n", "def test_aggregate_function_resolution(self):\n    \"\"\"\n    Aggregate function resolution is case insensitive and considers the vendor.\n    \"\"\"\n    with self.subTest(function='SUM'):\n        # SUM is supported by all vendors.\n        for vendor in ('postgresql', 'mysql', 'sqlite', 'oracle'):\n            with self.settings(DATABASES={'default': {'ENGINE': f'django.db.backends.{vendor}'}}, USE_TZ=False):\n                connection.vendor = vendor\n                agg = Sum('pages')\n                sql, params = agg.as_sql(connection, connection)\n                self.assertEqual(sql.lower(), f'sum(pages)')\n\n    with self.subTest(function='STDDEV'):\n        # STDDEV is supported by PostgreSQL and MySQL.\n        for vendor in ('postgresql', 'mysql'):\n            with self.settings(DATABASES={'default': {'ENGINE': f'django.db.backends.{vendor}'}}, USE_TZ=False):\n                connection.vendor = vendor\n                agg = StdDev('pages')\n                sql, params = agg.as_sql(connection, connection)\n                self.assertEqual(sql.lower(), 'stddev(pages)')\n\n        # STDDEV is not supported by SQLite and Oracle.\n        for vendor in ('sqlite', 'oracle'):\n            with self.settings(DATABASES={'default': {'ENGINE': f'django.db.backends.{vendor}'}}, USE_TZ=False):\n                connection.vendor = vendor\n                agg = StdDev('pages')\n                sql, params = agg.as_sql(connection, connection)\n                self.assertIn('select', sql.lower())\n\n    with self.subTest(function='STDDEV_POP'):\n        # STDDEV_POP is supported by PostgreSQL, MySQL, and Oracle.\n        for vendor in ('postgresql', 'mysql', 'oracle'):\n            with self.settings(DATABASES={'default': {'ENGINE': f'django.db.backends.{vendor}'}}, USE_TZ=False):\n                connection.vendor = vendor\n                agg = StdDev('pages', sample=False)\n                sql, params = agg.as_sql(connection, connection)\n                self.assertEqual(sql.lower(), 'stddev_pop(pages)')\n\n        # STDDEV_POP is not supported by SQLite.\n        with self.settings(DATABASES={'default': {'ENGINE': 'django.db.backends.sqlite3'}}, USE_TZ=False):\n            connection.vendor = 'sqlite'\n            agg = StdDev('pages', sample=False)\n            sql, params = agg.as_sql(connection, connection)\n            self.assertIn('select', sql.lower())\n\n    with self.subTest(function='VAR_POP'):\n        # VAR_POP is supported by PostgreSQL,", "def test_aggregation_default_functionality(self):\n    # Test the default argument functionality of aggregates\n    # when used in annotate and aggregate\n    publishers = Publisher.objects.annotate(avg_price=Avg('book__price', default=10)).order_by('name')\n    self.assertQuerysetEqual(\n        publishers, [\n            ('Apress', Approximate(Decimal('59.69'), places=2)),\n            (\"Jonno's House of Books\", 10),\n            ('Morgan Kaufmann', Decimal('75.00')),\n            ('Prentice Hall', Decimal('112.49')),\n            ('Sams', Decimal('23.09'))\n        ],\n        lambda p: (p.name, p.avg_price)\n    )\n\n    publishers = Publisher.objects.aggregate(avg_price=Avg('book__price', default=10))\n    self.assertEqual(publishers['avg_price'], Approximate(Decimal('47.45'), places=2))\n\n    # Test the default argument functionality with multiple aggregates\n    publishers = Publisher.objects.annotate(\n        avg_price=Avg('book__price', default=10),\n        sum_pages=Sum('book__pages', default=1000)\n    ).order_by('name')\n    self.assertQuerysetEqual(\n        publishers, [\n            ('Apress', Approximate(Decimal('59.69'), places=2), 1347),\n            (\"Jonno's House of Books\", 10, 1000),\n            ('Morgan Kaufmann', Decimal('75.00'), 1132),\n            ('Prentice Hall', Decimal('112.49'), 2476),\n            ('Sams', Decimal('23.09'), 528)\n        ],\n        lambda p: (p.name, p.avg_price, p.sum_pages)\n    )\n\n    publishers = Publisher.objects.aggregate(\n        avg_price=Avg('book__price', default=10),\n        sum_pages=Sum('book__pages', default=1000)\n    )\n    self.assertEqual(publishers, {\n        'avg_price': Approximate(Decimal('47.45'), places=2),\n        'sum_pages': 6483\n    })\n\n    # Test the default argument functionality with a filter\n    publishers = Publisher.objects.filter(book__price__lt=40).annotate(\n        avg_price=Avg('book__price', default=10),\n        sum_pages=Sum('book__pages', default=1000)\n    ).order_by('name')\n    self.assertQuerysetEqual(\n        publishers, [\n           "], "sample_498": ["def test_framealpha_none_with_shadow():\n    # Test that if framealpha is None and shadow is True, that framealpha\n    # is set to 1.\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label=\"test\")\n    leg = ax.legend(shadow=True, facecolor='w', framealpha=None)\n    assert leg.get_frame().get_alpha() == 1\n", "def test_legend_auto_placement_invalid_input():\n    fig, ax = plt.subplots()\n    x = np.arange(100)\n    ax.plot(x, 50 - x, 'o', label='y=1')\n    ax.plot(x, x - 50, 'o', label='y=-1')\n    with pytest.raises(ValueError):\n        ax.legend(loc=[1, 2, 3])  # Invalid input for loc\n    with pytest.raises(ValueError):\n        ax.legend(loc='foo')  # Invalid string input for loc\n", "def test_legend_bounding_box():\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 10, 100)\n    ax.plot(x, np.sin(x), label='sin')\n    ax.plot(x, np.cos(x), label='cos')\n\n    legend = ax.legend(loc='upper right')\n    assert isinstance(legend.get_bbox_to_anchor(), BboxBase)\n    assert isinstance(legend.get_window_extent(), BboxBase)\n\n    legend.set_bbox_to_anchor([0.5, 0.5])\n    assert isinstance(legend.get_bbox_to_anchor(), BboxBase)\n    assert isinstance(legend.get_window_extent(), BboxBase)\n\n    legend.set_bbox_to_anchor((0.5, 0.5, 1, 1))\n    assert isinstance(legend.get_bbox_to_anchor(), BboxBase)\n    assert isinstance(legend.get_window_extent(), BboxBase)\n", "def test_legend_draggable_update():\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='shabnams')\n    leg = ax.legend(draggable=True)\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n    fig.canvas.draw()\n    loc_before = leg._loc\n    x, y = leg.get_window_extent().extents[:2]\n    x += 1  # arbitrary small offset\n    leg.draggable._update_loc((x, y))\n    fig.canvas.draw()\n    assert loc_before != leg._loc\n    assert np.isclose(leg._loc[0], x/10, atol=0.01)\n    assert np.isclose(leg._loc[1], y/10, atol=0.01)\n", "def test_legend_offsetbox():\n    # Test using AnchoredOffsetbox as a handle\n    fig, ax = plt.subplots()\n    box = AnchoredOffsetbox(\n        loc='upper right', frameon=False,\n        child=TextArea(\"Text\", fontproperties=mpl.font_manager.FontProperties(\n            family='monospace')),\n        pad=0., borderpad=0., bbox_to_anchor=(0, 0, 1, 1),\n        bbox_transform=ax.transAxes)\n    ax.add_artist(box)\n    ax.plot([0, 1], [0, 1], label='line')\n    ax.legend(handles=[box], labels=['box'])\n"], "sample_237": ["    def test_model_without_meta(self):\n        class Checked(models.Model):\n            pass\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The permission named 'add_checked' of model 'auth_tests.Checked' is longer \"\n                \"than 255 characters.\",\n                obj=Checked,\n                id='auth.E008',\n            ),\n            checks.Error(\n                \"The permission named 'change_checked' of model 'auth_tests.Checked' is longer \"\n                \"than 255 characters.\",\n                obj=Checked,\n                id='auth.E008',\n            ),\n            checks.Error(\n                \"The permission named 'delete_checked' of model 'auth_tests.Checked' is longer \"\n                \"than 255 characters.\",\n                obj=Checked,\n                id='auth.E008',\n            ),\n            checks.Error(\n                \"The permission named 'view_checked' of model 'auth_tests.Checked' is longer \"\n                \"than 255 characters.\",\n                obj=Checked,\n                id='auth.E008',\n            ),\n        ])\n", "    def test_builtin_permission_name_length_with_custom_verbose_name(self):\n        class Checked(models.Model):\n            class Meta:\n                verbose_name = 'very long verbose name'\n                permissions = [\n                    ('view_checked', 'Some very long permission name'),\n                ]\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The verbose_name of model 'auth_tests.Checked' must be at most 243 \"\n                \"characters for its builtin permission names to be at most 255 characters.\",\n                obj=Checked,\n                id='auth.E007',\n            ),\n        ])\n", "    def test_builtin_permission_length_under_limit(self):\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('can_view', 'Can view')\n                ]\n                verbose_name = 's'\n        Permission = apps.get_model('auth', 'Permission')\n        permission_name_max_length = Permission._meta.get_field('name').max_length\n        errors = check_models_permissions(app_configs=self.apps.get_app_configs())\n        self.assertEqual(len(errors), 0)\n", "    def test_builtin_permission_length(self):\n        # This test ensures that the check_models_permissions function correctly\n        # handles the maximum length of builtin permission names and codenames.\n        class Checked(models.Model):\n            class Meta:\n                verbose_name = 'Checked'\n                verbose_name_plural = 'Checked'\n\n        Permission = apps.get_model('auth', 'Permission')\n        permission_name_max_length = Permission._meta.get_field('name').max_length\n        permission_codename_max_length = Permission._meta.get_field('codename').max_length\n\n        # Create a model with a very long name\n        model_name = 'x' * permission_codename_max_length\n        model = type(model_name, (models.Model,), {'__module__': self.__module__})\n\n        # Check that the length of the builtin permission codename is within the limit\n        builtin_permissions = dict(_get_builtin_permissions(model._meta))\n        max_builtin_permission_codename_length = max(len(codename) for codename in builtin_permissions.keys())\n        self.assertLessEqual(max_builtin_permission_codename_length, permission_codename_max_length)\n\n        # Check that the length of the builtin permission name is within the limit\n        max_builtin_permission_name_length = max(len(name) for name in builtin_permissions.values())\n        self.assertLessEqual(max_builtin_permission_name_length, permission_name_max_length)\n", "    def test_empty_permissions(self):\n        class Checked(models.Model):\n            class Meta:\n                permissions = []\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [])\n"], "sample_582": ["def test_flaskgroup_no_command_loading_error(runner):\n    from flask.cli import cli\n\n    runner = CliRunner(mix_stderr=False)\n    result = runner.invoke(cli, [\"missing\"], standalone_mode=False)\n    assert result.exit_code == 2\n    assert \"FLASK_APP\" in result.stderr\n    assert \"Usage:\" in result.stderr\n    assert \"Error: Could not locate a Flask application.\" in result.stderr\n", "def test_load_dotenv_cwd_change(monkeypatch):\n    for item in (\"FOO\", \"BAR\", \"EGGS\"):\n        monkeypatch._setitem.append((os.environ, item, notset))\n\n    load_dotenv(test_path / \".flaskenv\")\n    assert Path.cwd() == test_path\n    assert os.environ[\"FOO\"] == \"bar\"\n", "def test_with_appcontext_nested(runner):\n    @click.group()\n        pass\n\n    @cli.command()\n    @with_appcontext\n        click.echo(current_app.name)\n\n    @cli.command()\n        @click.command()\n        @with_appcontext\n            click.echo(current_app.name)\n\n        return test3()\n\n    obj = ScriptInfo(create_app=lambda: Flask(\"testapp\"))\n    result = runner.invoke(cli, [\"test1\"], obj=obj)\n    assert result.exit_code == 0\n    assert result.output == \"testapp\\n\"\n\n    result = runner.invoke(cli, [\"test2\"], obj=obj)\n    assert result.exit_code == 0\n    assert result.output == \"testapp\\n\"\n", "def test_flaskgroup_set_debug_flag_from_env(runner, monkeypatch):\n        app = Flask(\"flaskgroup\")\n        return app\n\n    @click.group(cls=FlaskGroup, create_app=create_app)\n        pass\n\n    @cli.command()\n        click.echo(str(current_app.debug))\n\n    # Set debug from environment variable\n    monkeypatch.setenv(\"FLASK_DEBUG\", \"1\")\n\n    result = runner.invoke(cli, [\"test\"])\n    assert result.exit_code == 0\n    assert result.output == \"True\\n\"\n\n    # Set debug from environment variable to 0\n    monkeypatch.setenv(\"FLASK_DEBUG\", \"0\")\n\n    result = runner.invoke(cli, [\"test\"])\n    assert result.exit_code == 0\n    assert result.output == \"False\\n\"\n", "def test_load_app_with_env_file(runner, monkeypatch):\n    # create a test app\n    app = Flask(__name__)\n\n    # create a test env file\n    env_file = Path(__file__).parent / \"test.env\"\n    env_file.write_text(\"DEBUG=True\")\n\n    # set FLASK_APP\n    monkeypatch.setenv(\"FLASK_APP\", __name__)\n\n    # create a FlaskGroup with load_dotenv\n    cli = FlaskGroup(create_app=lambda: app, load_dotenv=True)\n\n    # run the app with the env file\n    result = runner.invoke(cli, [\"--env-file\", str(env_file)])\n\n    # assert the env file was loaded\n    assert result.exit_code == 0\n    assert app.debug is True\n\n    # clean up\n    env_file.unlink()\n"], "sample_1187": ["def test_hyperplane_parameters():\n    triangle = Polygon((0, 3), (5, 3), (1, 1))\n    assert hyperplane_parameters(triangle) == [((0, 1), 3), ((1, -2), -1), ((-2, -1), -3)]\n\n    cube = [[(0, 0, 0), (0, 0, 5), (0, 5, 0), (0, 5, 5), (5, 0, 0),\\\n             (5, 0, 5), (5, 5, 0), (5, 5, 5)],\\\n            [2, 6, 7, 3], [3, 7, 5, 1], [7, 6, 4, 5], [1, 5, 4, 0],\\\n            [3, 1, 0, 2], [0, 4, 6, 2]]\n    vertices = cube[0]\n    faces = cube[1:]\n    assert hyperplane_parameters(faces, vertices) == [([0, -1, 0], -5), ([0, 0, -1], -5), ([-1, 0, 0], -5),\n                                                    ([0, 1, 0], 0), ([1, 0, 0], 0), ([0, 0, 1], 0)]\n", "def test_hyperplane_parameters():\n    triangle = Polygon(Point(0, 3), Point(5, 3), Point(1, 1))\n    assert hyperplane_parameters(triangle) == [((0, 1), 3), ((1, -2), -1), ((-2, -1), -3)]\n    cube = [[(0, 0, 0), (0, 0, 5), (0, 5, 0), (0, 5, 5), (5, 0, 0),\\\n            (5, 0, 5), (5, 5, 0), (5, 5, 5)],\\\n            [2, 6, 7, 3], [3, 7, 5, 1], [7, 6, 4, 5], [1, 5, 4, 0],\\\n            [3, 1, 0, 2], [0, 4, 6, 2]]\n    vertices = cube[0]\n    faces = cube[1:]\n    assert hyperplane_parameters(faces, vertices) == [([0, -1, 0], -5), ([0, 0, -1], -5), ([-1, 0, 0], -5),\n                                                    ([0, 1, 0], 0), ([1, 0, 0], 0), ([0, 0, 1], 0)]\n", "def test_hyperplane_parameters():\n    cube = [[(0, 0, 0), (0, 0, 5), (0, 5, 0), (0, 5, 5), (5, 0, 0),\\\n             (5, 0, 5), (5, 5, 0), (5, 5, 5)],\\\n            [2, 6, 7, 3], [3, 7, 5, 1], [7, 6, 4, 5], [1, 5, 4, 0],\\\n            [3, 1, 0, 2], [0, 4, 6, 2]]\n    vertices = cube[0]\n    faces = cube[1:]\n    assert hyperplane_parameters(faces, vertices) == \\\n        [([0, -1, 0], -5), ([0, 0, -1], -5), ([-1, 0, 0], -5),\n         ([0, 1, 0], 0), ([1, 0, 0], 0), ([0, 0, 1], 0)]\n\n    triangle = Polygon(Point(0, 3), Point(5, 3), Point(1, 1))\n    assert hyperplane_parameters(triangle) == [((0, 1), 3), ((1, -2), -1), ((-2, -1), -3)]\n", "def test_hyperplane_parameters():\n    # Test for 2D Polytope\n    triangle = Polygon(Point(0, 3), Point(5, 3), Point(1, 1))\n    assert hyperplane_parameters(triangle) == [((-1, 1), 3), ((1, -2), -1),\n                                             ((-2, -1), -3)]\n    # Test for 3D Polytope\n    cube = [[(0, 0, 0), (0, 0, 5), (0, 5, 0), (0, 5, 5), (5, 0, 0),\n             (5, 0, 5), (5, 5, 0), (5, 5, 5)],\n            [2, 6, 7, 3], [3, 7, 5, 1], [7, 6, 4, 5], [1, 5, 4, 0],\n            [3, 1, 0, 2], [0, 4, 6, 2]]\n    vertices = cube[0]\n    faces = cube[1:]\n    assert hyperplane_parameters(faces, vertices) == [([0, -1, 0], -5),\n                                                     ([0, 0, -1], -5),\n                                                     ([-1, 0, 0], -5),\n                                                     ([0, 1, 0], 0),\n                                                     ([1, 0, 0], 0),\n                                                     ([0, 0, 1], 0)]\n", "def test_hyperplane_parameters():\n    # 2D Case\n    triangle = Polygon((0, 0), (1, 0), (1, 1))\n    assert hyperplane_parameters(triangle) == [((-1, 1), 1), ((1, 0), 0),\n                                             ((0, 1), 0)]\n    hexagon = Polygon(Point(0, 0), Point(-sqrt(3) / 2, S.Half),\n                      Point(-sqrt(3) / 2, S(3) / 2), Point(0, 2),\n                      Point(sqrt(3) / 2, S(3) / 2), Point(sqrt(3) / 2, S.Half))\n    assert hyperplane_parameters(hexagon) == [((-Rational(-1, 2), -sqrt(3) / 2), 0),\n                                             ((Rational(-1, 2), sqrt(3) / 2), sqrt(3)),\n                                             ((-1, 0), sqrt(3)), (S.Half, -sqrt(3) / 2),\n                                             (S.Half, sqrt(3) / 2), ((1, 0), 0)]\n    # 3D Case\n    cube = [[(0, 0, 0), (0, 0, 5), (0, 5, 0), (0, 5, 5), (5, 0, 0),\n             (5, 0, 5), (5, 5, 0), (5, 5, 5)],\n            [2, 6, 7, 3], [3, 7, 5, 1], [7, 6, 4, 5], [1, 5, 4, 0],\n            [3, 1, 0, 2], [0, 4, 6, 2]]\n    vertices = cube[0]\n    faces = cube[1:]\n    assert hyperplane_parameters(faces, vertices) == [([0, -1, 0], -5),\n                                                    ([0, 0, -1], -5),\n                                                    ([-1, 0, 0], -5),\n                                                    ([0, 1, 0], 0), ([1, 0, 0], 0),\n                                                    ([0"], "sample_326": ["def test_urlize_with_trailing_punctuation(self):\n    tests = (\n        (\n            'Search for google.com/?q=1! and see.',\n            'Search for <a href=\"http://google.com/?q=1\">google.com/?q=1</a>! and see.'\n        ),\n        (\n            'Search for google.com/?q=1, and see.',\n            'Search for <a href=\"http://google.com/?q=1\">google.com/?q=1</a>, and see.'\n        ),\n        (\n            'Search for google.com/?q=1. and see.',\n            'Search for <a href=\"http://google.com/?q=1\">google.com/?q=1</a>. and see.'\n        ),\n        (\n            'Search for google.com/?q=1: and see.',\n            'Search for <a href=\"http://google.com/?q=1\">google.com/?q=1</a>: and see.'\n        ),\n        (\n            'Search for google.com/?q=1; and see.',\n            'Search for <a href=\"http://google.com/?q=1\">google.com/?q=1</a>; and see.'\n        ),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n", "def test_urlize_with_options(self):\n    tests = (\n        (\n            'Search for google.com/?q=! and see.',\n            'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>! and see.',\n            None, False, False\n        ),\n        (\n            'Search for google.com/?q=1&lt! and see.',\n            'Search for <a href=\"http://google.com/?q=1%3C\">google.com/?q=1&lt</a>! and see.',\n            None, False, False\n        ),\n        (\n            'Search for google.com/?q=! and see.',\n            'Search for <a href=\"http://google.com/?q=\" rel=\"nofollow\">google.com/?q=</a>! and see.',\n            None, True, False\n        ),\n        (\n            'Search for google.com/?q=! and see.',\n            'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>! and see.',\n            10, False, False\n        ),\n        (\n            'Search for google.com/?q=! and see.',\n            'Search for <a href=\"http://google.com/?q=\">google.com/?q=\u2026</a>! and see.',\n            10, False, False\n        ),\n        (\n            'Search for google.com/?q=! and see.',\n            'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>! and see.',\n            None, False, True\n        ),\n        (\n            'Search for google.com/?q=! and see.',\n            'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>! and see.',\n            None, True, True\n        ),\n    )\n    for value, output, trim_url_limit, nofollow, autoescape in tests:\n        with self.subTest(value=value, output=output):\n            self.assertEqual(urlize(value, trim_url_limit, nofollow, autoescape), output)\n", "def test_avoid_wrapping(self):\n    tests = (\n        (\"This is a test string\", \"This\\xa0is\\xa0a\\xa0test\\xa0string\"),\n        (\"Single space\", \"\\xa0\"),\n        (\"No spaces\", \"No\\xa0spaces\"),\n        (\"Multiple   spaces\", \"Multiple\\xa0\\xa0\\xa0spaces\"),\n    )\n    for value, output in tests:\n        with self.subTest(value=value, output=output):\n            self.assertEqual(avoid_wrapping(value), output)\n", "def test_urlize_trailing_punctuation(self):\n    tests = (\n        ('Search for google.com/?q=!', 'Search for <a href=\"http://google.com/?q\">google.com/?q!</a>!'),\n        ('Search for google.com/?q=?', 'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>?'),\n        ('Search for google.com/?q=.', 'Search for <a href=\"http://google.com/?q\">google.com/?q</a>.'),\n        ('Search for google.com/?q=,', 'Search for <a href=\"http://google.com/?q\">google.com/?q</a>,'),\n        ('Search for google.com/?q=;', 'Search for <a href=\"http://google.com/?q\">google.com/?q</a>;'),\n        ('Search for google.com/?q=:)', 'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>):'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n            self.assertEqual(urlize(lazystr(value)), output)\n", "def test_urlize_with_trim_url_limit(self):\n    tests = (\n        (\n            'Search for http://www.google.com/verylongurlhere so see.',\n            'Search for <a href=\"http://www.google.com/verylongurlhere\" rel=\"nofollow\">http://www.google.com/verylongurlhere\u2026</a> so see.',\n        ),\n        (\n            'Search for http://www.google.com/verylongurlhere so see.',\n            'Search for <a href=\"http://www.google.com/verylongurlhere\" rel=\"nofollow\">http://www.google.com/ver\u2026</a> so see.',\n        ),\n        (\n            'Search for http://www.google.com/verylongurlhere so see.',\n            'Search for <a href=\"http://www.google.com/verylongurlhere\" rel=\"nofollow\">http://www.google.com/\u2026</a> so see.',\n        ),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value, trim_url_limit=20), output)\n            self.assertEqual(urlize(value, trim_url_limit=10), output.replace('ver\u2026', '\u2026'))\n            self.assertEqual(urlize(value, trim_url_limit=5), output.replace('http://www.go\u2026', '\u2026'))\n"], "sample_698": ["def test_LogCaptureFixture_at_level() -> None:\n    from _pytest.logging import LogCaptureFixture, logging\n\n    log_level = logging.INFO\n    logger_name = \"test_logger\"\n    record = logging.LogRecord(\n        name=logger_name,\n        level=logging.INFO,\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=\"Test Message\",\n        args=(),\n        exc_info=None,\n    )\n\n    class Item:\n            self._store = {}\n\n    item = Item()\n    caplog = LogCaptureFixture(item)\n\n    with caplog.at_level(log_level, logger_name):\n        logging.getLogger(logger_name).info(record.msg)\n\n    assert len(caplog.records) == 1\n    assert caplog.records[0].name == logger_name\n    assert caplog.records[0].levelno == log_level\n\n    assert logging.getLogger(logger_name).level != log_level\n    assert caplog.handler.level != log_level\n", "def test_LogCaptureFixture() -> None:\n    from _pytest.logging import LogCaptureFixture\n    from _pytest.nodes import Node\n\n    class DummyNode(Node):\n            super().__init__(\"dummy\", \"dummy\")\n\n    fixture = LogCaptureFixture(DummyNode(), _ispytest=True)\n    assert fixture.text == \"\"\n    assert fixture.records == []\n    assert fixture.record_tuples == []\n    assert fixture.messages == []\n\n    fixture.set_level(logging.INFO)\n    assert fixture.handler.level == logging.INFO\n\n    fixture.clear()\n    assert fixture.text == \"\"\n    assert fixture.records == []\n    assert fixture.record_tuples == []\n    assert fixture.messages == []\n\n    with fixture.at_level(logging.DEBUG):\n        logging.debug(\"Test debug message\")\n        logging.info(\"Test info message\")\n        logging.warning(\"Test warning message\")\n        logging.error(\"Test error message\")\n        logging.critical(\"Test critical message\")\n\n    assert len(fixture.records) == 5\n    assert fixture.record_tuples == [\n        (\"root\", logging.DEBUG, \"Test debug message\"),\n        (\"root\", logging.INFO, \"Test info message\"),\n        (\"root\", logging.WARNING, \"Test warning message\"),\n        (\"root\", logging.ERROR, \"Test error message\"),\n        (\"root\", logging.CRITICAL, \"Test critical message\"),\n    ]\n    assert fixture.messages == [\n        \"Test debug message\",\n        \"Test info message\",\n        \"Test warning message\",\n        \"Test error message\",\n        \"Test critical message\",\n    ]\n", "def test_get_log_level_for_setting() -> None:\n    from _pytest.logging import get_log_level_for_setting\n    from _pytest.config import Config\n\n    class DummyConfig(Config):\n            return {\"log_level\": \"INFO\"}.get(name)\n\n            return {\"log_level\": \"DEBUG\"}.get(name)\n\n    config = DummyConfig()\n    assert get_log_level_for_setting(config, \"log_level\") == logging.INFO\n\n    class DummyConfig2(Config):\n            return None\n\n            return {\"log_level\": \"DEBUG\"}.get(name)\n\n    config = DummyConfig2()\n    assert get_log_level_for_setting(config, \"log_level\") == logging.DEBUG\n\n    class DummyConfig3(Config):\n            return {\"log_level\": \"INFO\"}.get(name)\n\n            return None\n\n    config = DummyConfig3()\n    assert get_log_level_for_setting(config, \"log_level\") == logging.INFO\n\n    class DummyConfig4(Config):\n            return {\"log_level\": \"10\"}.get(name)\n\n            return None\n\n    config = DummyConfig4()\n    assert get_log_level_for_setting(config, \"log_level\") == 10\n\n    class DummyConfig5(Config):\n            return {\"log_level\": \"invalid\"}.get(name)\n\n            return None\n\n    config = DummyConfig5()\n    try:\n        get_log_level_for_setting(config, \"log_level\")\n        assert False, \"Expected UsageError\"\n    except Exception as e:\n        assert isinstance(e, UsageError)\n        assert \"'invalid' is not recognized as a logging level name for 'log_level'.\" in str(e)\n", "def test_LogCaptureFixture_at_level() -> None:\n    from _pytest.logging import LogCaptureFixture\n\n    fixture = LogCaptureFixture(None)\n\n    logging.basicConfig(level=logging.INFO)\n\n    logger = logging.getLogger(\"test_logger\")\n    logger.setLevel(logging.INFO)\n\n    with fixture.at_level(logging.WARNING):\n        logger.info(\"This message should not be logged\")\n        logger.warning(\"This message should be logged\")\n        logger.error(\"This message should be logged\")\n\n    assert len(fixture.records) == 2\n\n    with fixture.at_level(logging.ERROR):\n        logger.info(\"This message should not be logged\")\n        logger.warning(\"This message should not be logged\")\n        logger.error(\"This message should be logged\")\n\n    assert len(fixture.records) == 3\n\n    fixture.set_level(logging.INFO)\n    with fixture.at_level(logging.WARNING, \"test_logger\"):\n        logger.info(\"This message should be logged\")\n        logger.warning(\"This message should be logged\")\n        logger.error(\"This message should be logged\")\n\n    assert len(fixture.records) == 6\n", "def test_log_capture_fixture() -> None:\n    from _pytest.logging import caplog, LogCaptureFixture\n\n    # Test that the caplog fixture is correctly initialized\n        logging.info(\"Test log message\")\n        logging.warning(\"Test warning message\")\n\n    test_function(caplog)\n\n    # Test that the log records are correctly captured\n    assert len(caplog.records) == 2\n    assert caplog.record_tuples == [\n        (\"root\", logging.INFO, \"Test log message\"),\n        (\"root\", logging.WARNING, \"Test warning message\"),\n    ]\n\n    # Test that the log text is correctly formatted\n    assert \"INFO     root\" in caplog.text\n    assert \"WARNING  root\" in caplog.text\n\n    # Test that the log messages are correctly retrieved\n    assert caplog.messages == [\"Test log message\", \"Test warning message\"]\n"], "sample_790": ["def test_kernel_pca_centering():\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((10, 5))\n    kpca = KernelPCA(kernel=\"linear\", n_components=2).fit(X)\n    assert_allclose(kpca._centerer.mean_, X.mean(axis=0))\n    assert_array_almost_equal(kpca.transform(X), np.abs(PCA(2).fit(X).transform(X)))\n\n    X_centered = X - X.mean(axis=0)\n    kpca = KernelPCA(kernel=\"linear\", n_components=2).fit(X_centered)\n    assert_array_almost_equal(kpca.transform(X_centered), np.abs(PCA(2).fit(X_centered).transform(X_centered)))\n", "def test_kernel_pca_copy_x():\n    # Test that the input data is copied correctly when `copy_X=True` or `copy_X=False`.\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((10, 10))\n    X_copy = X.copy()\n\n    # Test when `copy_X=True`.\n    kpca = KernelPCA(copy_X=True)\n    kpca.fit(X)\n    X[0, 0] = 1\n    assert_not_equal(kpca.X_fit_, X)\n\n    # Test when `copy_X=False`.\n    kpca = KernelPCA(copy_X=False)\n    kpca.fit(X_copy)\n    X_copy[0, 0] = 1\n    assert_equal(kpca.X_fit_, X_copy)\n", "def test_kernel_pca_edge_cases():\n    # Test edge cases, i.e., when n_components is set to 1 or n_samples.\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((10, 10))\n\n    # Test when n_components is set to 1.\n    kpca = KernelPCA(n_components=1)\n    Xt = kpca.fit_transform(X)\n    assert_equal(Xt.shape, (10, 1))\n\n    # Test when n_components is set to n_samples.\n    kpca = KernelPCA(n_components=X.shape[0])\n    Xt = kpca.fit_transform(X)\n    assert_equal(Xt.shape, (10, 10))\n\n    # Test when n_components is set to a value greater than n_samples.\n    with assert_raises(ValueError):\n        KernelPCA(n_components=X.shape[0] + 1).fit(X)\n", "def test_kernel_pca_invalid_eigen_solver():\n    assert_raises(ValueError, KernelPCA, eigen_solver=\"tototiti\")\n    assert_raises(ValueError, KernelPCA, eigen_solver=\"auto\", n_components=2, kernel=\"precomputed\")\n", "def test_kernel_pca_centering():\n    # Test if the centering works as expected.\n    rng = np.random.RandomState(0)\n    X = rng.rand(5, 4)\n    X_mean = np.mean(X, axis=0)\n\n    # fit KernelPCA\n    kpca = KernelPCA(kernel='linear')\n    K = kpca._get_kernel(X)\n    K_centered = kpca._centerer.fit_transform(K)\n\n    # Check if the sum of rows of centered kernel matrix is close to zero\n    assert_allclose(K_centered.sum(axis=0), np.zeros(5))\n\n    # Check if the sum of columns of centered kernel matrix is close to zero\n    assert_allclose(K_centered.sum(axis=1), np.zeros(5))\n"], "sample_633": ["def test_remove_successives() -> None:\n    all_couples = {\n        (11, 34): ([5, 9], [27, 31]),\n        (23, 79): ([15, 19], [45, 49]),\n        (12, 35): ([6, 10], [28, 32]),\n        (13, 36): ([7, 11], [29, 33]),\n    }\n    similar.remove_successives(all_couples)\n    assert len(all_couples) == 2\n    assert all_couples[(11, 34)] == ([5, 11], [27, 33])\n    assert all_couples[(23, 79)] == ([15, 19], [45, 49])\n", "def test_remove_successives() -> None:\n    \"\"\"Test the remove_successives function\"\"\"\n    all_couples = {\n        (11, 34): ([5, 9], [27, 31]),\n        (12, 35): ([6, 10], [28, 32]),\n        (23, 79): ([15, 19], [45, 49]),\n    }\n\n    similar.remove_successives(all_couples)\n\n    assert all_couples == {(11, 34): ([5, 10], [27, 32]), (23, 79): ([15, 19], [45, 49])}\n", "def test_stripped_lines() -> None:\n    lines = [\n        \"import os\",\n        \"# comment\",\n        \"\",\n        \"def func():\",\n        '    \"\"\"docstring\"\"\"',\n        \"    pass\",\n        \"if True:\",\n        \"    print('hello')\",\n        \"    # another comment\",\n    ]\n\n    expected_stripped = [\n        LineSpecifs(text=\"import os\", line_number=0),\n        LineSpecifs(text=\"def func():\", line_number=3),\n        LineSpecifs(text='    \"\"\"docstring\"\"\"', line_number=4),\n        LineSpecifs(text=\"    pass\", line_number=5),\n        LineSpecifs(text=\"if True:\", line_number=6),\n        LineSpecifs(text=\"    print('hello')\", line_number=7),\n    ]\n\n    actual_stripped = stripped_lines(lines, ignore_comments=True, ignore_docstrings=False, ignore_imports=False, ignore_signatures=False)\n    assert actual_stripped == expected_stripped\n\n    expected_stripped = [\n        LineSpecifs(text=\"import os\", line_number=0),\n        LineSpecifs(text=\"def func():\", line_number=3),\n        LineSpecifs(text=\"    pass\", line_number=5),\n        LineSpecifs(text=\"if True:\", line_number=6),\n        LineSpecifs(text=\"    print('hello')\", line_number=7),\n    ]\n\n    actual_stripped = stripped_lines(lines, ignore_comments=True, ignore_docstrings=True, ignore_imports=False, ignore_signatures=False)\n    assert actual_stripped == expected_stripped\n\n    expected_stripped = [\n        LineSpecifs(text=\"def func():\", line_number=3),\n        LineSpecifs(text='    \"\"\"docstring\"\"\"', line_number=4),\n        LineSpecifs(text=\"    pass\", line_number=5),\n        LineSpecifs(text=\"if True:\", line_number=6),\n        LineSpecifs(text=\"    print('hello')\", line_number=7),\n    ]\n\n    actual_stripped = stripped_lines(lines, ignore_comments=True, ignore_docstrings=False, ignore_imports=True, ignore_signatures=False)\n    assert actual_stripped == expected_stripped\n\n    expected_stripped = [\n        LineSpecifs(text=\"import os\", line_number=0),\n        LineSpecifs(text=\"def func():\", line_number=3),\n        LineSpecifs(text='    \"\"\"docstring\"\"\"', line_number=", "def test_stripped_lines() -> None:\n    lines = [\n        \"# This is a comment line\\n\",\n        \"import os\\n\",\n        \"\\n\",\n        \"def my_function():\\n\",\n        \"    \\\"\\\"\\\"This is a docstring\\\"\\\"\\\"\\n\",\n        \"    x = 5  # This is a comment after code\\n\",\n        \"    y = 10\\n\",\n        \"    z = x + y\\n\",\n    ]\n\n    # Test default behavior\n    stripped_lines_list = stripped_lines(lines, False, False, False, False)\n    expected_stripped_lines_list = [\n        LineSpecifs(text=\"import os\", line_number=1),\n        LineSpecifs(text=\"def my_function():\", line_number=3),\n        LineSpecifs(text='x = 5  # This is a comment after code', line_number=5),\n        LineSpecifs(text=\"y = 10\", line_number=6),\n        LineSpecifs(text=\"z = x + y\", line_number=7),\n    ]\n    assert stripped_lines_list == expected_stripped_lines_list\n\n    # Test ignoring comments\n    stripped_lines_list = stripped_lines(lines, True, False, False, False)\n    expected_stripped_lines_list = [\n        LineSpecifs(text=\"import os\", line_number=1),\n        LineSpecifs(text=\"def my_function():\", line_number=3),\n        LineSpecifs(text=\"x = 5\", line_number=5),\n        LineSpecifs(text=\"y = 10\", line_number=6),\n        LineSpecifs(text=\"z = x + y\", line_number=7),\n    ]\n    assert stripped_lines_list == expected_stripped_lines_list\n\n    # Test ignoring docstrings\n    stripped_lines_list = stripped_lines(lines, False, True, False, False)\n    expected_stripped_lines_list = [\n        LineSpecifs(text=\"import os\", line_number=1),\n        LineSpecifs(text=\"def my_function():\", line_number=3),\n        LineSpecifs(text=\"x = 5  # This is a comment after code\", line_number=5),\n        LineSpecifs(text=\"y = 10\", line_number=6),\n        LineSpecifs(text=\"z = x + y\", line_number=7),\n    ]\n    assert stripped_lines_list == expected_stripped_lines_list\n\n    # Test ignoring imports\n    stripped_lines_list = stripped_lines(lines, False, False", "def test_stripped_lines() -> None:\n    lines = [\n        \"# this is a comment\",\n        \"   def foo():\\n\",\n        '    \"\"\"This is a docstring.\"\"\"\\n',\n        \"    # another comment\\n\",\n        \"    pass\\n\",\n        \"    print('This is not a comment')\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"   # This line is a comment but has leading whitespace\\n\",\n    ]\n    result = stripped_lines(\n        lines, ignore_comments=True, ignore_docstrings=True, ignore_imports=False, ignore_signatures=False\n    )\n    expected = [\n        LineSpecifs(line_number=1, text=\"def foo():\"),\n        LineSpecifs(line_number=4, text=\"pass\"),\n        LineSpecifs(line_number=5, text=\"print('This is not a comment')\"),\n    ]\n    assert result == expected\n"], "sample_1108": ["def test_least_rotation():\n    assert least_rotation([3, 1, 5, 1, 2]) == 3\n    assert least_rotation([0, 1, 2, 3, 4]) == 0\n    assert least_rotation([4, 0, 1, 2, 3]) == 1\n    assert least_rotation([3, 0, 1, 2, 4]) == 2\n    assert least_rotation([2, 0, 1, 3, 4]) == 3\n    assert least_rotation([1, 0, 2, 3, 4]) == 4\n    assert least_rotation([0, 1, 2, 3, 4]) == 0\n", "def test_permute_signs():\n    assert list(permute_signs((0, 1, 2))) == [(0, 1, 2), (0, -1, 2), (0, 1, -2), (0, -1, -2)]\n    assert list(permute_signs((-1, -1, 2))) == [(-1, -1, 2), (-1, 1, 2), (-1, -1, -2), (-1, 1, -2), (1, -1, 2), (1, 1, 2), (1, -1, -2), (1, 1, -2)]\n    assert list(permute_signs([0, 1, -2])) == [(0, 1, -2), (0, -1, -2), (0, 1, 2), (0, -1, 2)]\n    assert list(permute_signs((0, 0, 0))) == [(0, 0, 0)]\n", "def test_permute_signs():\n    assert list(permute_signs((0, 1, 2))) == [\n        (0, 1, 2), (0, -1, 2), (0, 1, -2), (0, -1, -2)]\n    assert list(permute_signs((1, 2))) == [\n        (1, 2), (-1, 2), (1, -2), (-1, -2)]\n    assert list(permute_signs((1,))) == [(1,), (-1,)]\n    assert list(permute_signs(())) == [()]\n    assert list(permute_signs((0, 0))) == [(0, 0)]\n", "def test_interactive_traversal():\n    # Smoke test to check for SyntaxError\n    expr = w + (x + y)*z\n    interactive_traversal(expr)\n", "def test_permute_signs():\n    assert list(permute_signs((0, 1, 2))) == [(0, 1, 2), (0, -1, 2), (0, 1, -2), (0, -1, -2)]\n    assert list(permute_signs([0, 1, 2])) == [[0, 1, 2], [0, -1, 2], [0, 1, -2], [0, -1, -2]]\n    assert list(permute_signs([0, 0, 0])) == [[0, 0, 0]]\n    assert list(permute_signs((0, 1, 2, 3))) == [\n        (0, 1, 2, 3), (0, 1, 2, -3), (0, 1, -2, 3), (0, 1, -2, -3),\n        (0, -1, 2, 3), (0, -1, 2, -3), (0, -1, -2, 3), (0, -1, -2, -3),\n        (0, 1, 2, 3), (0, 1, 2, -3), (0, 1, -2, 3), (0, 1, -2, -3),\n        (0, -1, 2, 3), (0, -1, 2, -3), (0, -1, -2, 3), (0, -1, -2, -3)]\n"], "sample_293": ["    def test_match(self):\n        pattern = RegexPattern(r'^path/(?P<arg1>\\w+)/(?P<arg2>\\d+)/$', name='test-pattern')\n        self.assertEqual(\n            pattern.match('/path/abc/123/'),\n            ('', ('abc', '123'), {'arg1': 'abc', 'arg2': '123'})\n        )\n        self.assertIsNone(pattern.match('/path/abc/123'))  # No trailing slash\n", "    def test_url_pattern_resolve_with_default_args(self):\n        \"\"\"\n        Test that URL patterns with default arguments can be resolved.\n        \"\"\"\n        url_pattern = URLPattern(\n            RegexPattern(r'^default/(?P<arg1>[0-9]+)/$'),\n            views.empty_view,\n            default_args={'arg2': 'default'}\n        )\n        match = url_pattern.resolve('/default/42/')\n        self.assertEqual(match.args, ())\n        self.assertEqual(match.kwargs, {'arg1': '42', 'arg2': 'default'})\n", "    def test_regex_pattern_description(self):\n        regex_pattern = RegexPattern(r'^test/(?P<test_arg>\\w+)/$', name='test-pattern')\n        self.assertEqual(regex_pattern.describe(), \"'test/(?P<test_arg>\\\\w+)/$' [name='test-pattern']\")\n", "    def test_regex_pattern_match(self):\n        pattern = RegexPattern(r'^articles/(?P<year>[0-9]{4})/(?P<month>[0-9]{2})/(?P<slug>[-\\w]+)/$')\n        self.assertEqual(pattern.match('/articles/2022/07/django-urls/'), ('', (), {'year': '2022', 'month': '07', 'slug': 'django-urls'}))\n        self.assertIsNone(pattern.match('/articles/2022/07/django_urls/'))\n", "    def test_str(self):\n        regex = RegexPattern(r'^a$')\n        self.assertEqual(str(regex), '^a$')\n"], "sample_438": ["    def test_modelbase_init(self):\n        class MyModel(metaclass=ModelBase):\n            class Meta:\n                app_label = 'tests'\n\n        with self.assertRaisesMessage(TypeError, \"Abstract models cannot be instantiated.\"):\n            MyModel()\n", "    def test_model_base_methods(self):\n        class Model(models.Model):\n            pass\n\n        self.assertEqual(Model._meta.concrete_model, Model)\n        self.assertIsInstance(Model._base_manager, models.Manager)\n        self.assertIsInstance(Model._default_manager, models.Manager)\n\n        instance = Model()\n        self.assertEqual(instance._get_FIELD_display(None), None)\n        with self.assertRaises(ValueError):\n            instance._get_next_or_previous_by_FIELD(None, is_next=True)\n\n        class AbstractModel(models.Model):\n            class Meta:\n                abstract = True\n\n        self.assertFalse(hasattr(AbstractModel, \"DoesNotExist\"))\n        self.assertFalse(hasattr(AbstractModel, \"MultipleObjectsReturned\"))\n\n        with self.assertRaises(TypeError):\n            AbstractModel()\n", "    def test_model_check(self):\n        class MyModel(models.Model):\n            pass\n\n        errors = MyModel.check()\n        self.assertEqual(len(errors), 0)\n", "    def test_model_base_metaclass(self):\n        class AbstractModel(models.Model):\n            class Meta:\n                abstract = True\n\n        class ConcreteModel(AbstractModel):\n            pass\n\n        class AnotherConcreteModel(AbstractModel):\n            pass\n\n        class ProxyModel(ConcreteModel):\n            class Meta:\n                proxy = True\n\n        self.assertIsInstance(ConcreteModel, ModelBase)\n        self.assertIsInstance(AnotherConcreteModel, ModelBase)\n        self.assertIsInstance(ProxyModel, ModelBase)\n\n        with self.assertRaises(TypeError):\n            AbstractModel()\n\n        with self.assertRaises(TypeError):\n            ProxyModel(concrete_model=ConcreteModel)\n\n        class ModelWithFields(models.Model):\n            field1 = models.CharField(max_length=255)\n            field2 = models.CharField(max_length=255)\n\n            class Meta:\n                unique_together = ('field1', 'field2')\n\n        with self.assertRaisesMessage(\n            FieldError, \"The model has two identical many-to-many relations through the intermediate model\"\n        ):\n            class ModelWithM2M(models.Model):\n                field1 = models.ManyToManyField(ModelWithFields, through='self')\n                field2 = models.ManyToManyField(ModelWithFields, through='self')\n", "    def test_get_field_name_clashes(self):\n        # Create a parent model with a field\n        class Parent(models.Model):\n            field1 = models.CharField(max_length=10)\n\n        # Create a child model with the same field name\n        class Child(Parent):\n            field1 = models.CharField(max_length=20)\n\n        # Check that the child model has a field name clash\n        with self.assertNumQueries(0):\n            errors = Child.check()\n            self.assertEqual(len(errors), 1)\n            self.assertEqual(errors[0].msg, \"'field1' from parent model 'contenttypes_tests.Parent' clashes with the field 'field1' from parent model 'contenttypes_tests.Child'.\")\n"], "sample_478": ["    def test_not_callable(self):\n        class TestModelAdmin(ModelAdmin):\n            view_on_site = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'view_on_site' must be a callable or a boolean value.\",\n            \"admin.E025\",\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_inline_admin_valid(self):\n        class InlineAdmin(admin.TabularInline):\n            model = ValidationTestInlineModel\n            readonly_fields = (\"parent\",)\n\n        class TestModelAdmin(ModelAdmin):\n            inlines = [InlineAdmin]\n\n        self.assertIsValid(TestModelAdmin, ValidationTestModel)\n"], "sample_456": ["def test_formset_get_context(self):\n    \"\"\"The FormSet.get_context() method returns the formset instance.\"\"\"\n    formset = self.make_choiceformset()\n    context = formset.get_context()\n    self.assertIn(\"formset\", context)\n    self.assertEqual(context[\"formset\"], formset)\n", "def test_formset_with_ordering_and_deletion_and_empty_data(self):\n    \"\"\"\n    FormSets with ordering + deletion, with empty data.\n    \"\"\"\n    ChoiceFormSet = formset_factory(Choice, can_order=True, can_delete=True)\n    initial = [\n        {\"choice\": \"Calexico\", \"votes\": 100},\n        {\"choice\": \"Fergie\", \"votes\": 900},\n        {\"choice\": \"The Decemberists\", \"votes\": 500},\n    ]\n    formset = ChoiceFormSet(initial=initial, auto_id=False, prefix=\"choices\")\n    self.assertHTMLEqual(\n        \"\\n\".join(form.as_ul() for form in formset.forms),\n        '<li>Choice: <input type=\"text\" name=\"choices-0-choice\" value=\"Calexico\">'\n        \"</li>\"\n        '<li>Votes: <input type=\"number\" name=\"choices-0-votes\" value=\"100\"></li>'\n        '<li>Order: <input type=\"number\" name=\"choices-0-ORDER\" value=\"1\"></li>'\n        '<li>Delete: <input type=\"checkbox\" name=\"choices-0-DELETE\"></li>'\n        '<li>Choice: <input type=\"text\" name=\"choices-1-choice\" value=\"Fergie\">'\n        \"</li>\"\n        '<li>Votes: <input type=\"number\" name=\"choices-1-votes\" value=\"900\"></li>'\n        '<li>Order: <input type=\"number\" name=\"choices-1-ORDER\" value=\"2\"></li>'\n        '<li>Delete: <input type=\"checkbox\" name=\"choices-1-DELETE\"></li>'\n        '<li>Choice: <input type=\"text\" name=\"choices-2-choice\" '\n        'value=\"The Decemberists\"></li>'\n        '<li>Votes: <input type=\"number\" name=\"choices-2-votes\" value=\"500\"></li>'\n        '<li>Order: <input type=\"number\" name=\"choices-2-ORDER\" value=\"3\"></li>'\n        '<li>Delete: <input type=\"checkbox\" name=\"choices-2-DELETE\"></li>'\n        '<li>Choice: <input type=\"text\" name=\"choices-3-choice\"></li>'\n        '<li>Votes: <input type=\"number\" name=\"choices-3-votes\"></li>'\n        '<li>Order: <input type=\"number\" name=\"choices-", "def test_min_num_with_can_delete(self):\n    \"\"\"\n    If min_num is more than the number of non-deleted forms, a ValidationError is raised.\n    \"\"\"\n    data = {\n        \"choices-TOTAL_FORMS\": \"2\",\n        \"choices-INITIAL_FORMS\": \"2\",\n        \"choices-MIN_NUM_FORMS\": \"2\",\n        \"choices-0-choice\": \"Zero\",\n        \"choices-0-votes\": \"0\",\n        \"choices-0-DELETE\": \"on\",\n        \"choices-1-choice\": \"One\",\n        \"choices-1-votes\": \"1\",\n        \"choices-1-DELETE\": \"\",\n    }\n    ChoiceFormSet = formset_factory(Choice, can_delete=True, min_num=2, validate_min=True)\n    formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(\n        formset.non_form_errors(),\n        [\"Please submit at least 2 forms.\"],\n    )\n", "def test_clean_with_empty_formset(self):\n    \"\"\"\n    A formset's clean() hook is still called even if the formset is empty.\n    \"\"\"\n    class BaseEmptyFormSet(BaseFormSet):\n            self.cleaned_data[\"test\"] = True\n\n    EmptyFormSet = formset_factory(Choice, formset=BaseEmptyFormSet, extra=0)\n    formset = EmptyFormSet(\n        data={\"form-TOTAL_FORMS\": 0, \"form-INITIAL_FORMS\": 0},\n        prefix=\"form\",\n    )\n    self.assertTrue(formset.is_valid())\n    self.assertEqual(formset.non_form_errors(), [])\n    self.assertEqual(formset.cleaned_data, {\"test\": True})\n", "def test_max_num_zero_with_deletion(self):\n    \"\"\"\n    If max_num is 0 then no form is rendered at all, unless initial data is\n    present and deletion is allowed. In that case, the initial forms are\n    rendered, but without deletion fields, and the formset is valid if the\n    initial data is valid.\n    \"\"\"\n    LimitedFavoriteDrinkFormSet = formset_factory(\n        FavoriteDrinkForm, extra=1, max_num=0, can_delete=True\n    )\n    formset = LimitedFavoriteDrinkFormSet()\n    self.assertEqual(formset.forms, [])\n    initial = [{\"name\": \"Fernet and Coke\"}, {\"name\": \"Bloody Mary\"}]\n    formset = LimitedFavoriteDrinkFormSet(initial=initial)\n    self.assertEqual(len(formset.forms), 2)\n    self.assertHTMLEqual(\n        \"\\n\".join(str(form) for form in formset.forms),\n        \"\"\"\n        <div><label for=\"id_form-0-name\">Name:</label>\n        <input id=\"id_form-0-name\" name=\"form-0-name\" type=\"text\"\n            value=\"Fernet and Coke\"></div>\n        <div><label for=\"id_form-1-name\">Name:</label>\n        <input id=\"id_form-1-name\" name=\"form-1-name\" type=\"text\"\n            value=\"Bloody Mary\"></div>\n        \"\"\",\n    )\n    self.assertTrue(formset.is_valid())\n"], "sample_61": ["    def test_ascii_validator_edge_cases(self):\n        v = validators.ASCIIUsernameValidator()\n        with self.subTest(empty_string=''):\n            with self.assertRaises(ValidationError):\n                v('')\n        with self.subTest(leading_trailing_whitespace='   username   '):\n            with self.assertRaises(ValidationError):\n                v('   username   ')\n        with self.subTest(non_ascii_whitespace='\\u200Busername\\u200B'):\n            with self.assertRaises(ValidationError):\n                v('\\u200Busername\\u200B')\n", "    def test_regex(self):\n        # Test ASCIIUsernameValidator\n        valid_usernames = ['joe', 'joe123', 'joe@123', 'joe.123', 'joe+123', 'joe_123']\n        invalid_usernames = [\"o'connell\", '\u00c9ric', 'jean marc', \"\u0623\u062d\u0645\u062f\", 'trailingnewline\\n']\n        v = ASCIIUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n\n        # Test UnicodeUsernameValidator\n        valid_usernames = ['joe', 'joe123', 'joe@123', 'joe.123', 'joe+123', 'joe_123', 'Ren\u00e9', '\u1d2e\u1d35\u1d33\u1d2e\u1d35\u1d3f\u1d30', '\u0623\u062d\u0645\u062f']\n        invalid_usernames = [\"o'connell\", \"\u0639\u0628\u062f \u0627\u0644\", \"zerowidth\\u200Bspace\", \"nonbreaking\\u00A0space\", \"en\\u2013dash\", 'trailingnewline\\u000A']\n        v = UnicodeUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n", "    def test_regex_matches_empty_string(self):\n        v = validators.ASCIIUsernameValidator()\n        with self.assertRaises(ValidationError):\n            v('')\n", "    def test_non_latin_scripts(self):\n        v = validators.UnicodeUsernameValidator()\n        valid_usernames = ['\u4e2d\u6587', '\u65e5\u672c\u8a9e', '\u0930\u093e\u0937\u094d\u091f\u094d\u0930\u0940\u092f', '\u0420\u043e\u0441\u0441\u0438\u044f']\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n", "    def test_unicode_edge_cases(self):\n        v = UnicodeUsernameValidator()\n        self.assertIsNone(v(''))\n        with self.assertRaises(ValidationError):\n            v('a' * 151)\n        self.assertIsNone(v('a' * 150))\n        self.assertIsNone(v('12345'))\n        self.assertIsNone(v('.'))\n"], "sample_792": ["def test_complementnb_scale_invariance():\n    # Scaling the data should not change the prediction results\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    labels = [ComplementNB().fit(f * X, y).predict(f * X)\n              for f in [1E-10, 1, 1E10]]\n    assert_array_equal(labels[0], labels[1])\n    assert_array_equal(labels[1], labels[2])\n", "def test_complementnb_prior_unobserved_targets():\n    # test smoothing of prior for yet unobserved targets\n\n    # Create toy training data\n    X = np.array([[0, 1], [1, 0]])\n    y = np.array([0, 1])\n\n    clf = ComplementNB()\n\n    assert_no_warnings(\n        clf.partial_fit, X, y, classes=[0, 1, 2]\n    )\n\n    assert clf.predict([[0, 1]]) == 0\n    assert clf.predict([[1, 0]]) == 1\n    assert clf.predict([[1, 1]]) == 0\n\n    # add a training example with previously unobserved class\n    assert_no_warnings(\n        clf.partial_fit, [[1, 1]], [2]\n    )\n\n    assert clf.predict([[0, 1]]) == 0\n    assert clf.predict([[1, 0]]) == 1\n    assert clf.predict([[1, 1]]) == 2\n", "def test_complement_nb_partial_fit():\n    # Test Complement NB partial fit\n    clf = ComplementNB()\n    clf.partial_fit(X2[:2], y2[:2], classes=np.unique(y2))\n    clf.partial_fit(X2[2:], y2[2:])\n\n    clf2 = ComplementNB()\n    clf2.fit(X2, y2)\n\n    assert_array_equal(clf.feature_count_, clf2.feature_count_)\n    assert_array_equal(clf.class_count_, clf2.class_count_)\n    assert_array_almost_equal(clf.feature_log_prob_, clf2.feature_log_prob_)\n    assert_array_almost_equal(clf.predict_proba(X2), clf2.predict_proba(X2))\n", "def test_complementnb_partial_fit():\n    # Test Complement NB partial_fit method\n\n    X = np.array([[0, 1], [1, 0], [1, 1]])\n    y = np.array([0, 0, 1])\n\n    clf = ComplementNB()\n    clf.fit(X, y)\n    clf_pf = ComplementNB()\n    clf_pf.partial_fit(X[:2], y[:2], classes=np.unique(y))\n    clf_pf.partial_fit(X[2:], y[2:])\n\n    assert_array_almost_equal(clf.feature_count_, clf_pf.feature_count_)\n    assert_array_almost_equal(clf.class_count_, clf_pf.class_count_)\n    assert_array_almost_equal(clf.feature_all_, clf_pf.feature_all_)\n    assert_array_almost_equal(clf.feature_log_prob_, clf_pf.feature_log_prob_)\n    assert_array_almost_equal(clf.predict(X), clf_pf.predict(X))\n", "def test_bnb_pickle_unfitted():\n    # Test picklability of discrete naive Bayes classifiers that have not been fit\n\n    clf = BernoulliNB()\n    store = BytesIO()\n    pickle.dump(clf, store)\n    loaded_clf = pickle.load(BytesIO(store.getvalue()))\n\n    # Test that pickleing an unfitted classifier does not result in any\n    # attributes being created\n    assert_equal(loaded_clf.__dict__, clf.__dict__)\n"], "sample_220": ["    def test_init(self):\n        response = HttpResponseBase(content_type='text/plain', status=200, reason='OK', charset='utf-8')\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.reason_phrase, 'OK')\n        self.assertEqual(response.charset, 'utf-8')\n        self.assertEqual(response['Content-Type'], 'text/plain; charset=utf-8')\n", "    def test_reason_phrase_setter(self):\n        response = HttpResponse()\n        response.reason_phrase = 'Test Phrase'\n        self.assertEqual(response.reason_phrase, 'Test Phrase')\n", "    def test_initial_status_code(self):\n        response = HttpResponseBase()\n        self.assertEqual(response.status_code, 200)\n", "    def test_custom_charset(self):\n        response = HttpResponse()\n        response.charset = 'iso-8859-1'\n        self.assertEqual(response.charset, 'iso-8859-1')\n", "    def test_status_code(self):\n        response = HttpResponse(status=404)\n        self.assertEqual(response.status_code, 404)\n"], "sample_289": ["    def test_pickle(self):\n        x = ImmutableList([1, 2, 3])\n        self.assertEqual(x, pickle.loads(pickle.dumps(x)))\n", "    def test_dictwrapper_prefix_collision(self):\n            return \"*%s\" % x\n        d = DictWrapper({'xx_a': 'xx_a'}, f, 'xx_')\n        self.assertEqual(d['xx_a'], 'xx_a')\n        self.assertEqual(d['xx_xx_a'], '*xx_a')\n", "    def test_dictwrapper_with_prefix(self):\n            return \"*%s\" % x\n        d = DictWrapper({'xx_a': 'a', 'b': 'b'}, f, 'xx_')\n        self.assertEqual(d['xx_a'], '*a')\n        self.assertEqual(d['b'], 'b')\n", "    def test_prefix_key_collision(self):\n            return \"*%s\" % x\n        d = DictWrapper({'xx_a': 'a'}, f, 'xx_')\n        with self.assertRaises(KeyError):\n            d['a']\n", "    def test_dictwrapper_empty_dict(self):\n            return \"*%s\" % x\n        d = DictWrapper({}, f, 'xx_')\n        self.assertEqual({}, d)\n"], "sample_954": ["def test_visit_centered(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n    assert '.sp\\n.ce\\n' in content\n    assert '.ce 0\\n' in content\n", "def test_nested_inline_nodes(app, status, warning):\n    \"\"\"\n    Test that nested inline nodes are flattened correctly.\n    \"\"\"\n    app.builder.build_all()\n    content = (app.outdir / 'sphinxtests.1').read_text()\n    assert r'\\fBfoo=\\fP\\\\fI1\\fP' in content\n    assert r'\\fB&amp;bar=\\fP\\\\fI2\\fP' in content\n", "def test_inline_markup(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n\n    assert '\\n.I' in content  # italic\n    assert '\\n.B' in content  # bold\n    assert '\\n.R' in content  # roman\n    assert '\\n.SM' in content  # small caps\n    assert '\\n.BI' in content  # bold italic\n    assert '\\n.BR' in content  # bold roman\n    assert '\\n.EM' in content  # emphasis\n", "def test_visit_term_with_strong_node(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'sphinxtests.1').read_text()\n    assert '\\n.B term1\\n' in content\n    assert '\\nterm2 (\\\\fBstronged partially\\\\fP)\\n' in content\n\n    # Test term with strong node\n    app.srcdir.new_document('term_with_strong.rst', '''\n    .. option:: --foo\n       :strong: some text\n       description\n    ''')\n    app.builder.build_all()\n    content = (app.outdir / 'sphinxtests.1').read_text()\n    assert '\\n.B --foo\\n' in content\n    assert 'some text' not in content\n", "def test_manpage_translator(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'sphinxtests.1').read_text()\n    assert '.SH NAME\\nsphinxtests \\\\- Test\\n' in content\n\n    # test term of definition list including nodes.strong and nodes.emphasis\n    assert '\\n.B term3 \\\\(strong and emphasis\\\\)\\n' in content\n\n    # test literal_emphasis and literal_strong nodes\n    assert '\\\\fIemphasis\\\\fP' in content\n    assert '\\\\fBstrong\\\\fP' in content\n\n    # test section title\n    assert '.SH SECTION\\n' in content\n    assert '.SH SUBSECTION\\n' in content\n"], "sample_628": ["def test_ignore_words(self):\n    with self.assertAddsMessages(\n        Message(\n            \"wrong-spelling-in-comment\",\n            line=1,\n            args=(\n                \"coment\",\n                \"# bad coment ignore\",\n                \"      ^^^^^^\",\n                self._get_msg_suggestions(\"coment\"),\n            ),\n        )\n    ):\n        self.checker.process_tokens(_tokenize_str(\"# bad coment ignore\"))\n", "    def test_words_with_trailing_colons(self):\n        stmt = astroid.extract_node(\n            'class ComentAbc(object):\\n   \"\"\"This is a bad: coment\"\"\"\\n   pass'\n        )\n        with self.assertAddsMessages(\n            Message(\n                \"wrong-spelling-in-docstring\",\n                line=2,\n                args=(\n                    \"coment\",\n                    \"This is a bad: coment\",\n                    \"                ^^^^^^\",\n                    self._get_msg_suggestions(\"coment\"),\n                ),\n            )\n        ):\n            self.checker.visit_classdef(stmt)\n", "    def test_private_dict_storage(self):\n        self.checker.config.spelling_store_unknown_words = True\n        self.checker.config.spelling_private_dict_file = \"test_spelling.dict\"\n        self.checker.open()\n        stmt = astroid.extract_node(\n            'def fff():\\n   \"\"\"bad coment\"\"\"\\n   pass'\n        )\n        self.checker.visit_functiondef(stmt)\n        with open(\"test_spelling.dict\", \"r\") as file:\n            lines = file.readlines()\n            assert lines == [\"coment\\n\"]\n        os.remove(\"test_spelling.dict\")\n", "def test_check_spelling_store_unknown_words(self):\n    with self.assertAddsMessages():\n        self.checker.process_tokens(_tokenize_str(\"# unknowndictionaryword\"))\n        self.assertTrue(\"unknowndictionaryword\" in self.checker.unknown_words)\n        self.checker.open()  # Reopen the checker to read the stored word\n        self.checker.process_tokens(_tokenize_str(\"# unknowndictionaryword\"))\n        assert self.linter.release_messages() == []\n", "    def test_spelling_private_dict_file(self, private_dict, tmp_path):\n        private_dict_path = tmp_path / \"private_dict.txt\"\n        private_dict_path.write_text(private_dict)\n        set_config(\n            spelling_private_dict_file=str(private_dict_path),\n            spelling_store_unknown_words=True,\n        )\n        with self.assertAddsMessages(\n            Message(\n                \"wrong-spelling-in-comment\",\n                line=1,\n                args=(\n                    \"coment\",\n                    \"# bad coment\",\n                    \"      ^^^^^^\",\n                    self._get_msg_suggestions(\"coment\"),\n                ),\n            )\n        ):\n            self.checker.process_tokens(_tokenize_str(\"# bad coment\"))\n        self.checker.close()\n        private_dict_lines = private_dict_path.read_text().splitlines()\n        assert private_dict_lines[-1] == \"coment\"\n"], "sample_1079": ["def test_orthogonal_direction():\n    p1 = Point3D(1, 2, 3)\n    p2 = Point3D(4, 5, 6)\n    p3 = Point3D(1, 0, 0)\n    assert Line(p1, p1.orthogonal_direction).is_perpendicular(Line(p1, p2))\n    assert Line(p2, p2.orthogonal_direction).is_perpendicular(Line(p2, p1))\n    assert Line(p3, p3.orthogonal_direction).is_perpendicular(Line(p3, p2))\n", "def test_project():\n    p1 = Point3D(1, 1, 1)\n    p2 = Point3D(0, 0, 1)\n    p3 = Point3D(0, 0, 0)\n    assert Point3D.project(p1, p2) == p2\n    assert Point3D.project(p2, p2) == p2\n    assert Point3D.project(p3, p2) == p2\n    raises(ValueError, lambda: Point3D.project(p1, p3))\n", "def test_project():\n    # test 2D\n    p1 = Point2D(1, 1)\n    p2 = Point2D(0, 2)\n    assert Point.project(p1, p2) == Point2D(0, 1)\n    # test 3D\n    p1 = Point3D(1, 1, 1)\n    p2 = Point3D(0, 2, 3)\n    assert Point.project(p1, p2) == Point3D(-5/14, 10/7, 15/14)\n", "def test_canberra_distance():\n    # test case for zero division error\n    p1 = Point(0, 0)\n    p2 = Point(0, 0)\n    raises(ValueError, lambda: p1.canberra_distance(p2))\n\n    # test case with negative values\n    p1 = Point(-1, -1)\n    p2 = Point(0, 1)\n    assert p1.canberra_distance(p2) == 3\n\n    # test case with non-numeric values\n    x = Symbol('x')\n    y = Symbol('y')\n    p1 = Point(x, y)\n    p2 = Point(x + 1, y)\n    assert p1.canberra_distance(p2) == 1/(x + (x + 1))\n", "def test_point_orthogonal_direction():\n    p1 = Point(1, 2)\n    p2 = Point(0, 1)\n    p3 = Point3D(1, 1, 1)\n    p4 = Point3D(0, 0, 0)\n    p5 = Point3D(0, 1, 0)\n    p6 = Point3D(1, 1, 0)\n    p7 = Point(1, 0)\n    p8 = Point(0, 0)\n    p9 = Point3D(1, 0, 0)\n    assert p7.orthogonal_direction == p9\n    assert p1.orthogonal_direction == Point(2, -1)\n    assert p2.orthogonal_direction == Point(0, 1)\n    assert p3.orthogonal_direction == Point3D(-2, 1, 0)\n    assert p4.orthogonal_direction == Point3D(1, 0, 0)\n    assert p5.orthogonal_direction == Point3D(0, 0, 1)\n    assert p6.orthogonal_direction == Point3D(0, 0, 1)\n    assert p8.orthogonal_direction == Point(1, 0)\n    assert p9.orthogonal_direction == Point3D(0, 1, 0)\n    raises(ValueError, lambda: Point(0, 0).orthogonal_direction)\n"], "sample_410": ["    def test_get_session_auth_hash(self):\n        user = User.objects.create_user(\"user\", \"foo@example.com\")\n        session_auth_hash = user.get_session_auth_hash()\n        self.assertIsInstance(session_auth_hash, str)\n        self.assertEqual(len(session_auth_hash), 64)  # length of a SHA256 hash\n", "    def test_get_session_auth_hash(self):\n        user = User.objects.create_user(username=\"user\", password=\"foo\")\n        session_auth_hash = user.get_session_auth_hash()\n        self.assertIsInstance(session_auth_hash, str)\n        self.assertEqual(len(session_auth_hash), 64)  # 64 hexadecimal digits for SHA-256\n", "    def test_get_session_auth_hash(self):\n        user = User(password=\"password123\")\n        session_hash = user.get_session_auth_hash()\n        self.assertIsInstance(session_hash, str)\n        self.assertEqual(len(session_hash), 64)  # SHA-256 produces 64 character hex string\n", "    def test_get_session_auth_hash(self):\n        user = User.objects.create_user(username=\"user\", password=\"password\")\n        self.assertIsInstance(user.get_session_auth_hash(), str)\n", "    def test_is_authenticated_always_true(self):\n        user = AbstractBaseUser(password=\"foo\")\n        self.assertIs(user.is_authenticated, True)\n"], "sample_1182": ["def test_AbstractPythonCodePrinter():\n    prntr = AbstractPythonCodePrinter()\n\n    assert prntr._format_code([\"hello\", \"world\"]) == \"hello\\nworld\"\n\n    assert prntr._get_statement(\"a = 1\") == \"a = 1\"\n\n    assert prntr._get_comment(\"test comment\") == \"  # test comment\"\n\n    assert prntr._indent_codestring(\"hello\\nworld\") == \"    hello\\n    world\"\n", "def test_AbstractPythonCodePrinter():\n    prntr = AbstractPythonCodePrinter()\n\n    assert prntr.doprint(None) == 'None'\n    assert prntr.doprint(oo) == 'float(\"inf\")'\n    assert prntr.doprint(S.NegativeInfinity) == 'float(\"-inf\")'\n    assert prntr.doprint(Expr()) == 'Expr()'\n", "def test_PythonCodePrinter_functions():\n    prntr = PythonCodePrinter()\n\n    assert prntr.doprint(exp(x)) == 'math.exp(x)'\n    assert prntr.doprint(log(x)) == 'math.log(x)'\n    assert prntr.doprint(log(x, 10)) == 'math.log10(x)'\n    assert prntr.doprint(sin(x)) == 'math.sin(x)'\n    assert prntr.doprint(cos(x)) == 'math.cos(x)'\n    assert prntr.doprint(tan(x)) == 'math.tan(x)'\n    assert prntr.doprint(cosh(x)) == 'math.cosh(x)'\n    assert prntr.doprint(sinh(x)) == 'math.sinh(x)'\n    assert prntr.doprint(tanh(x)) == 'math.tanh(x)'\n    assert prntr.doprint(factorial(x)) == 'math.factorial(x)'\n    assert prntr.doprint(floor(x)) == 'math.floor(x)'\n    assert prntr.doprint(ceiling(x)) == 'math.ceil(x)'\n    assert prntr.doprint(hypot(x, y)) == 'math.hypot(x, y)'\n\n    assert prntr.module_imports == {'math': {'exp', 'log', 'log10', 'sin', 'cos', 'tan',\n                                             'cosh', 'sinh', 'tanh', 'factorial', 'floor', 'ceil', 'hypot'}}\n", "def test_issue_18837():\n    prntr = PythonCodePrinter()\n\n    assert prntr.doprint(loggamma(x)) == \"math.lgamma(x)\"\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(loggamma(x)) == \"math.lgamma(x)\"\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(loggamma(x)) == \"scipy.special.gammaln(x)\"\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(loggamma(x)) == \"mpmath.loggamma(x)\"\n\n    prntr = SymPyPrinter()\n    assert prntr.doprint(loggamma(x)) == \"sympy.loggamma(x)\"\n", "def test_issue_with_reserved_words_in_expr():\n    expr = x + 2\n    prntr = PythonCodePrinter({'reserved_word_suffix': '_'})\n    assert prntr._print_Symbol(Expr(\"if\")) == 'if_'\n    assert prntr.doprint(expr) == 'x + 2'\n    assert prntr.doprint(x + 2) == 'x + 2'\n\n    prntr = PythonCodePrinter({'error_on_reserved': True})\n    raises(ValueError, lambda: prntr._print_Symbol(Expr(\"if\")))\n"], "sample_20": ["def test_decode_mixins_with_empty_serialized_columns(tmp_path):\n    filename = tmp_path / \"test.fits\"\n    t1 = Table([[1.0, 2.0]])\n    t1[\"col0\"].description = \"hello\" * 40\n    t1[\"col0\"].format = \"{:8.4f}\"\n    t1[\"col0\"].meta[\"a\"] = {\"b\": \"c\"}\n    t1.meta[\"__serialized_columns__\"] = {}\n    t1.write(filename, overwrite=True)\n\n    t2 = Table.read(filename)\n    assert t2[\"col0\"].description == \"hello\" * 40\n    assert t2[\"col0\"].format == \"{:8.4f}\"\n    assert t2[\"col0\"].meta[\"a\"] == {\"b\": \"c\"}\n", "def test_is_fits_hdu_object():\n    \"\"\"Test that is_fits correctly identifies FITS HDU objects.\"\"\"\n    hdu = fits.PrimaryHDU()\n    assert fits.connect.is_fits(hdu)\n    assert not fits.connect.is_fits(hdu.data)\n", "def test_read_table_fits_without_header(tmp_path):\n    filename = tmp_path / \"test.fits\"\n    data = np.array([(1, 2.0, 'Hello'), (2, 4.0, 'World')],\n                    dtype=[('a', int), ('b', float), ('c', 'U5')])\n    hdu = fits.BinTableHDU(data)\n    hdu.writeto(filename, overwrite=True)\n\n    with fits.open(filename, mode='update') as hdul:\n        del hdul[1].header\n\n    with pytest.raises(KeyError):\n        Table.read(filename)\n", "def test_remove_keywords(tmp_path):\n    \"\"\"\n    Test that the REMOVE_KEYWORDS are removed from the table meta when reading in a FITS file.\n    \"\"\"\n    filename = tmp_path / \"test_remove_keywords.fits\"\n    hdu = BinTableHDU(self.data)\n    hdu.header[\"XTENSION\"] = \"BINTABLE\"\n    hdu.header[\"BITPIX\"] = 8\n    hdu.header[\"NAXIS\"] = 2\n    hdu.header[\"NAXIS1\"] = 4\n    hdu.header[\"NAXIS2\"] = 10\n    hdu.header[\"PCOUNT\"] = 0\n    hdu.header[\"GCOUNT\"] = 1\n    hdu.header[\"TFIELDS\"] = 3\n    hdu.header[\"THEAP\"] = 0\n    hdu.writeto(filename, overwrite=True)\n\n    t = Table.read(filename)\n    for keyword in REMOVE_KEYWORDS:\n        assert keyword not in t.meta\n", "def test_fits_io_scale_error_on_write(tmp_path):\n    \"\"\"\n    Test that a UnitScaleError is raised when writing to a FITS file if a column has a non-FITS-compliant scale\n    \"\"\"\n    filename = tmp_path / \"test.fits\"\n    t = Table([[1, 2, 3]])\n    t[\"col0\"].unit = \"m\"\n    t[\"col0\"].info.scale = 1.2  # This is not a valid FITS scale\n    with pytest.raises(\n        u.UnitScaleError,\n        match=r\"The column 'col0' could not be stored in FITS format because it has a scale '\\(1\\.2\\)'\"\n        r\" that is not recognized by the FITS standard\\. Either scale the data or change the units\\.\",\n    ):\n        t.write(filename, format=\"fits\")\n"], "sample_267": ["    def test_is_in_memory_db(self):\n        with override_settings(DATABASES={\n            'default': {\n                'ENGINE': 'django.db.backends.sqlite3',\n                'NAME': ':memory:',\n            },\n        }):\n            self.assertTrue(connection.vendor == 'sqlite' and connection.creation.is_in_memory_db(connection.settings_dict['NAME']))\n\n        with override_settings(DATABASES={\n            'default': {\n                'ENGINE': 'django.db.backends.sqlite3',\n                'NAME': 'test.db',\n            },\n        }):\n            self.assertFalse(connection.vendor == 'sqlite' and connection.creation.is_in_memory_db(connection.settings_dict['NAME']))\n\n        with override_settings(DATABASES={\n            'default': {\n                'ENGINE': 'django.db.backends.sqlite3',\n                'NAME': 'file:test.db?mode=memory&cache=shared',\n            },\n        }):\n            self.assertTrue(connection.vendor == 'sqlite' and connection.creation.is_in_memory_db(connection.settings_dict['NAME']))\n\n        with override_settings(DATABASES={\n            'default': {\n                'ENGINE': 'django.db.backends.sqlite3',\n                'NAME': 'file:test.db?mode=ro',\n            },\n        }):\n            self.assertFalse(connection.vendor == 'sqlite' and connection.creation.is_in_memory_db(connection.settings_dict['NAME']))\n", "    def test_get_new_connection(self):\n        with mock.patch.object(Database, 'connect') as mock_connect:\n            connection = connection.__class__(settings_dict=connection.settings_dict)\n            connection.get_new_connection(connection.get_connection_params())\n            mock_connect.assert_called_once()\n", "    def test_isolation_level_is_set(self):\n        \"\"\"Test that the isolation level is set correctly.\"\"\"\n        with connection.cursor() as cursor:\n            cursor.execute('PRAGMA isolation_level')\n            isolation_level = cursor.fetchone()[0]\n        self.assertEqual(isolation_level, 'IMMEDIATE')\n", "    def test_close_with_in_memory_db(self):\n        settings_dict = {\n            'default': {\n                'ENGINE': 'django.db.backends.sqlite3',\n                'NAME': ':memory:',\n            },\n        }\n        connection = ConnectionHandler(settings_dict)['default']\n        connection.ensure_connection()\n        connection.close()\n        self.assertIsNone(connection.connection)\n", "    def test_get_connection_params_in_memory_db(self):\n        # Test that connection parameters for in-memory database is correctly set.\n        settings_dict = {\n            'NAME': ':memory:',\n            'OPTIONS': {\n                'check_same_thread': True,\n            }\n        }\n        wrapper = DatabaseWrapper(settings_dict)\n        connection_params = wrapper.get_connection_params()\n        self.assertEqual(connection_params['check_same_thread'], False)\n"], "sample_58": ["def test_order_fields_invalid_field(self):\n    class TestForm(Form):\n        field1 = CharField()\n        field2 = CharField()\n        field3 = CharField()\n        field_order = ['field4', 'field2', 'field1']\n\n    p = TestForm()\n    self.assertEqual(list(p.fields), ['field2', 'field1', 'field3'])\n", "def test_custom_renderer_instance(self):\n    class CustomRendererInstance:\n            self.is_called = False\n\n            self.is_called = True\n            return template.render(context)\n\n    custom_renderer_instance = CustomRendererInstance()\n    form = Form(renderer=custom_renderer_instance)\n    form.as_table()\n    self.assertTrue(custom_renderer_instance.is_called)\n", "def test_repr_bound_field(self):\n    class MyForm(Form):\n        my_field = CharField()\n        my_choice_field = ChoiceField(choices=[('P', 'Python'), ('J', 'Java')])\n\n    form = MyForm(auto_id=False)\n    self.assertEqual(repr(form['my_field']), '<MyForm bound field my_field>')\n    self.assertEqual(repr(form['my_choice_field']), '<MyForm bound field my_choice_field>')\n", "def test_non_field_errors_unicode(self):\n    class TestForm(Form):\n        pass\n\n    f = TestForm({'field1': 'value'})\n    f.add_error(None, mark_safe('<b>Test</b>'))\n    f.add_error(None, 'Test')\n    self.assertHTMLEqual(f.as_ul(), \"\"\"<li><ul class=\"errorlist nonfield\"><li>&lt;b&gt;Test&lt;/b&gt;</li><li>Test</li></ul></li>\"\"\")\n", "def test_prefix_hashing(self):\n    # Prefixes should be hashed in the rendered HTML to avoid conflicts\n    # with JavaScript libraries that use the same class name.\n    class UserRegistration(Form):\n        username = CharField(max_length=10)\n        password = CharField(widget=PasswordInput)\n        password2 = CharField(widget=PasswordInput)\n\n    prefix = 'my_prefix'\n    form = UserRegistration(prefix=prefix, auto_id=True)\n    self.assertHTMLEqual(\n        form.as_p(),\n        \"\"\"<p>"], "sample_182": ["def test_union_with_values_and_select_for_update(self):\n    ReservedName.objects.create(name='a', order=2)\n    qs1 = ReservedName.objects.select_for_update().values('name', 'order', 'id')\n    qs2 = ReservedName.objects.none()\n    reserved_name = qs1.union(qs2).values('name', 'order', 'id').get()\n    self.assertEqual(reserved_name['name'], 'a')\n    self.assertEqual(reserved_name['order'], 2)\n    reserved_name = qs1.union(qs2).values_list('name', 'order', 'id').get()\n    self.assertEqual(reserved_name[:2], ('a', 2))\n", "    def test_intersection_with_extra_and_values_list(self):\n        ReservedName.objects.create(name='a', order=2)\n        qs1 = ReservedName.objects.extra(\n            select={'extra_order': 'order * 2'},\n        ).values_list('order', 'extra_order')\n        qs2 = ReservedName.objects.all().values_list('order', 'id')\n        self.assertCountEqual(qs1.intersection(qs2), [(2, 2)])\n", "def test_union_on_model_with_deferred_fields(self):\n    class DeferredNumber(Number):\n        class Meta:\n            proxy = True\n\n    DeferredNumber.objects.bulk_create(\n        DeferredNumber(num=i, other_num=10 - i) for i in range(10)\n    )\n\n    qs1 = Number.objects.filter(num__lte=1).defer('num')\n    qs2 = DeferredNumber.objects.filter(num__gte=8).defer('other_num')\n    union = qs1.union(qs2)\n    self.assertEqual(union.model, Number)\n    self.assertEqual(len(list(union)), 10)\n", "    def test_union_with_distinct_and_ordering(self):\n        qs1 = Number.objects.filter(num=1).values('num').annotate(\n            count=Value(0, IntegerField()),\n        )\n        qs2 = Number.objects.filter(num=2).annotate(\n            count=F('num'),\n        ).values('num', 'count')\n        union = qs1.union(qs2, all=True).distinct().order_by('num')\n        self.assertEqual(len(union), 2)\n        self.assertQuerysetEqual(union, [{'num': 1, 'count': 0}, {'num': 2, 'count': 2}], transform=lambda x: dict(x))\n", "    def test_union_with_values_list_and_filter(self):\n        ReservedName.objects.bulk_create([\n            ReservedName(name='rn1', order=7),\n            ReservedName(name='rn2', order=5),\n            ReservedName(name='rn0', order=6),\n            ReservedName(name='rn9', order=-1),\n        ])\n        qs1 = ReservedName.objects.filter(order__gte=6)\n        qs2 = ReservedName.objects.filter(order__lte=5)\n        union_qs = qs1.union(qs2)\n        for qs, expected_result in (\n            (union_qs.values_list('order', flat=True).filter(order=6), [6]),\n            (union_qs.values_list('order', flat=True).filter(order__lt=0), [-1]),\n            (union_qs.values_list('order', flat=True).filter(order__gt=7), []),\n            (union_qs.values_list('order', flat=True).filter(order__in=[6, 5]), [6, 5]),\n            (union_qs.values_list('order', flat=True).filter(order__in=[-1, 6, 5]), [6, 5, -1]),\n        ):\n            with self.subTest(qs=qs):\n                self.assertEqual(list(qs), expected_result)\n"], "sample_202": ["def test_process_messages_recursive(self):\n    \"\"\"\n    The MessageDecoder.process_messages() method handles nested data structures.\n    \"\"\"\n    messages = [\n        {\n            'message': Message(constants.INFO, 'Test message'),\n            'message_list': [\n                Message(constants.INFO, 'message %s') for x in range(5)\n            ] + [{'another-message': Message(constants.ERROR, 'error')},\n                {'nested': [{'message': Message(constants.WARNING, 'warning')}]}],\n        },\n        Message(constants.INFO, 'message %s'),\n    ]\n    encoder = MessageEncoder(separators=(',', ':'))\n    value = encoder.encode(messages)\n    decoded_messages = json.loads(value, cls=MessageDecoder)\n    decoder = MessageDecoder()\n    processed_messages = decoder.process_messages(decoded_messages)\n    self.assertEqual(messages, processed_messages)\n", "def test_hash_collision(self):\n    \"\"\"\n    Test that different messages with the same hash are properly handled.\n    \"\"\"\n    storage = self.get_storage()\n    # Create two messages with the same hash\n    message1 = Message(constants.INFO, 'message1')\n    message2 = Message(constants.INFO, 'message2')\n    # Ensure the messages have the same hash\n    storage._hash(message1)  # Call the hash function to cache the hash value\n    storage._hash(message2)  # Call the hash function to cache the hash value\n    with mock.patch('django.contrib.messages.storage.cookie.salted_hmac') as mock_hash:\n        mock_hash.return_value.hexdigest.return_value = '123456'\n        encoded_data = storage._encode([message1, message2])\n        decoded_messages = storage._decode(encoded_data)\n        self.assertEqual(decoded_messages, [message1, message2])\n", "    def test_not_finished_sentinel(self):\n        \"\"\"\n        The not_finished sentinel value is handled correctly.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Store 2 messages, the first of which will exceed the max cookie size.\n        msg_size = int((CookieStorage.max_cookie_size - 54) / 1.5 - 37)\n        storage.add(constants.INFO, '0' * msg_size)\n        storage.add(constants.INFO, 'small message')\n\n        unstored_messages = storage.update(response)\n        cookie_storing = self.stored_messages_count(storage, response)\n\n        # The not_finished sentinel should have been stored.\n        self.assertEqual(cookie_storing, 1)\n        self.assertEqual(len(unstored_messages), 1)\n        self.assertEqual(unstored_messages[0].message, '0' * msg_size)\n\n        # Fetch the sentinel value and remove it.\n        storage2 = self.get_storage()\n        messages, all_retrieved = storage2._get()\n        self.assertFalse(all_retrieved)\n        self.assertEqual(len(messages), 1)\n        self.assertEqual(messages[0].message, 'small message')\n\n        # The sentinel value is removed.\n        self.assertNotIn(CookieStorage.not_finished, storage2._get()[0])\n", "def test_update_with_empty_messages(self):\n    \"\"\"\n    Test that update() correctly handles empty messages.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n    storage.update(response)\n    self.assertEqual(response.cookies.get(CookieStorage.cookie_name), None)\n", "def test_extra_tags(self):\n    \"\"\"\n    Messages that were set with extra tags in the cookie are decoded correctly.\n    \"\"\"\n    storage = self.get_storage()\n    message = Message(constants.INFO, 'message', extra_tags='tag1 tag2')\n    encoded_data = storage._encode([message])\n    decoded_message = storage._decode(encoded_data)[0]\n    self.assertEqual(decoded_message.message, message.message)\n    self.assertEqual(decoded_message.level, message.level)\n    self.assertEqual(decoded_message.extra_tags, message.extra_tags)\n"], "sample_921": ["def test_isenumattribute():\n    class Color(enum.Enum):\n        RED = 1\n        GREEN = 2\n        BLUE = 3\n\n    assert inspect.isenumattribute(Color.RED) is True\n    assert inspect.isenumattribute(Color) is False\n    assert inspect.isenumattribute(1) is False\n", "def test_isabstractmethod():\n    class AbstractClass:\n        @abstractmethod\n            pass\n\n    class NonAbstractClass:\n            pass\n\n    assert inspect.isabstractmethod(AbstractClass().abstract_method) is True\n    assert inspect.isabstractmethod(NonAbstractClass().non_abstract_method) is False\n", "def test_getdoc_partial():\n        \"\"\"docstring.\"\"\"\n        pass\n\n    func.__doc__ = \"func\"\n    partial_func = functools.partial(func, 1, 2)\n\n    assert inspect.getdoc(partial_func) == \"docstring.\"\n    assert inspect.getdoc(partial_func, allow_inherited=True) == \"docstring.\"\n    assert inspect.getdoc(partial_func, allow_inherited=False) == \"func\"\n\n    partial_func.__doc__ = \"partial_func\"\n\n    assert inspect.getdoc(partial_func) == \"partial_func\"\n    assert inspect.getdoc(partial_func, allow_inherited=True) == \"partial_func\"\n    assert inspect.getdoc(partial_func, allow_inherited=False) == \"partial_func\"\n", "def test_signature_from_str_edge_cases():\n    # test empty parentheses\n    sig = inspect.signature_from_str('()')\n    assert list(sig.parameters.keys()) == []\n    assert sig.return_annotation == Parameter.empty\n\n    # test single positional argument\n    sig = inspect.signature_from_str('(a)')\n    assert list(sig.parameters.keys()) == ['a']\n    assert sig.parameters['a'].kind == Parameter.POSITIONAL_OR_KEYWORD\n    assert sig.parameters['a'].default == Parameter.empty\n    assert sig.parameters['a'].annotation == Parameter.empty\n    assert sig.return_annotation == Parameter.empty\n\n    # test single keyword-only argument\n    sig = inspect.signature_from_str('(*, a)')\n    assert list(sig.parameters.keys()) == ['a']\n    assert sig.parameters['a'].kind == Parameter.KEYWORD_ONLY\n    assert sig.parameters['a'].default == Parameter.empty\n    assert sig.parameters['a'].annotation == Parameter.empty\n    assert sig.return_annotation == Parameter.empty\n\n    # test single positional-only argument\n    sig = inspect.signature_from_str('(a, /)')\n    assert list(sig.parameters.keys()) == ['a']\n    assert sig.parameters['a'].kind == Parameter.POSITIONAL_ONLY\n    assert sig.parameters['a'].default == Parameter.empty\n    assert sig.parameters['a'].annotation == Parameter.empty\n    assert sig.return_annotation == Parameter.empty\n\n    # test invalid signature\n    with pytest.raises(SyntaxError):\n        inspect.signature_from_str('(')\n", "def test_isenumclass():\n    class Color(enum.Enum):\n        RED = 1\n        GREEN = 2\n\n    class NotEnum:\n        pass\n\n    assert inspect.isenumclass(Color) is True\n    assert inspect.isenumclass(NotEnum) is False\n\n"], "sample_596": ["def test_concat_coords_with_different_lengths():\n    ds1 = Dataset({\"x\": [1, 2, 3]})\n    ds2 = Dataset({\"x\": [4, 5]})\n\n    with raises_regex(ValueError, \"are not coordinates\"):\n        concat([ds1, ds2], dim=\"x\", coords=[\"y\"])\n\n    actual = concat([ds1, ds2], dim=\"x\", coords=\"minimal\")\n    expected = Dataset({\"x\": [1, 2, 3, 4, 5]})\n    assert_identical(actual, expected)\n\n    actual = concat([ds1, ds2], dim=\"x\", coords=\"different\")\n    expected = Dataset({\"x\": [1, 2, 3, 4, 5]})\n    assert_identical(actual, expected)\n\n    actual = concat([ds1, ds2], dim=\"x\", coords=\"all\")\n    expected = Dataset({\"x\": [1, 2, 3, 4, 5]})\n    assert_identical(actual, expected)\n", "def test_concat_variable_type_mismatch():\n    arr1 = DataArray([1, 2], dims=\"x\", coords={\"x\": [1, 2]})\n    arr2 = DataArray([3, 4], dims=\"x\", coords={\"x\": [1, 2]}, dtype=float)\n\n    with raises_regex(ValueError, \"conflicting dtypes\"):\n        concat([arr1, arr2], dim=\"x\")\n\n    ds1 = Dataset({\"a\": arr1})\n    ds2 = Dataset({\"a\": arr2})\n\n    with raises_regex(ValueError, \"conflicting dtypes\"):\n        concat([ds1, ds2], dim=\"x\")\n", "def test_concat_no_fill_value_for_non_broadcastable_coords():\n    # GH#4344\n    arr1 = DataArray([1, 2, 3], dims=\"x\", coords={\"x\": [1, 2, 3], \"y\": (\"x\", [4, 5, 6])})\n    arr2 = DataArray([7, 8], dims=\"x\", coords={\"x\": [7, 8], \"y\": (\"x\", [9, 10])})\n    actual = concat([arr1, arr2], dim=\"z\")\n    expected = DataArray(\n        [1, 2, 3, 7, 8],\n        dims=[\"z\", \"x\"],\n        coords={\"x\": [1, 2, 3, 7, 8], \"y\": (\"x\", [4, 5, 6, 9, 10])},\n    )\n    assert_identical(actual, expected)\n", "def test_concat_fill_value_dict_matching_dim_length():\n    # test fill value dict with values matching dim length\n    ds1 = Dataset({\"a\": ((\"x\", \"y\"), [[0]])}, coords={\"x\": [0], \"y\": [0]})\n    ds2 = Dataset({\"a\": ((\"x\", \"y\"), [[0]])}, coords={\"x\": [1], \"y\": [0]})\n\n    fill_value = {\"a\": {\"fill_value\": [1, 2]}}\n    actual = concat([ds1, ds2], dim=\"x\", fill_value=fill_value)\n    expected = Dataset(\n        {\"a\": ((\"x\", \"y\"), [[0, 1], [0, 2]])},\n        {\"x\": [0, 1], \"y\": [0]},\n    )\n    assert_identical(actual, expected)\n", "def test_concat_coords_with_labels():\n    # GH2475\n    ds1 = Dataset({\"a\": ((\"x\", \"y\"), [[0]])}, coords={\"x\": [0], \"y\": [0]})\n    ds2 = Dataset({\"a\": ((\"x\", \"y\"), [[1]])}, coords={\"x\": [0], \"y\": [0]})\n\n    actual = concat([ds1, ds2], dim=\"z\")\n    expected = Dataset({\"a\": ((\"z\", \"x\", \"y\"), [[[[0]]])}, {\"x\": [0], \"y\": [0]})\n    assert_identical(actual, expected)\n\n    actual = concat([ds1, ds2], dim=(\"z\", [0, 1]))\n    expected = Dataset({\"a\": ((\"z\", \"x\", \"y\"), [[[[0]]])}, {\"x\": [0], \"y\": [0], \"z\": [0, 1]})\n    assert_identical(actual, expected)\n\n    actual = concat([ds1, ds2], dim=(\"z\", pd.Index([0, 1])))\n    expected = Dataset({\"a\": ((\"z\", \"x\", \"y\"), [[[[0]]])}, {\"x\": [0], \"y\": [0], \"z\": [0, 1]})\n    assert_identical(actual, expected)\n"], "sample_462": ["def test_choicefield_coerce(self):\n    f = TypedChoiceField(choices=[(1, \"One\"), (2, \"Two\")], coerce=lambda val: val)\n    self.assertEqual(1, f.clean(1))\n    self.assertEqual(1, f.clean(\"1\"))\n    msg = \"'Select a valid choice. 3 is not one of the available choices.'\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        f.clean(\"3\")\n", "def test_typed_choice_field_1(self):\n    f = TypedChoiceField(choices=[(\"1\", \"One\"), (\"2\", \"Two\")], coerce=int)\n    with self.assertRaisesMessage(ValidationError, \"'Select a valid choice. 3 is not one of the available choices.'\"):\n        f.clean(\"3\")\n    self.assertEqual(1, f.clean(1))\n    self.assertEqual(1, f.clean(\"1\"))\n    self.assertEqual(2, f.clean(2))\n    self.assertEqual(2, f.clean(\"2\"))\n", "    def test_choicefield_typedchoicefield(self):\n        class FirstNames(models.TextChoices):\n            JOHN = \"J\", \"John\"\n            PAUL = \"P\", \"Paul\"\n\n            return val.upper()\n\n        f = ChoiceField(choices=FirstNames)\n        f_typed = TypedChoiceField(choices=FirstNames, coerce=coerce)\n\n        self.assertEqual(f.clean(\"J\"), \"J\")\n        self.assertEqual(f_typed.clean(\"J\"), \"J\")\n        self.assertEqual(f_typed.clean(\"j\"), \"J\")\n\n        msg = \"'Select a valid choice. 3 is not one of the available choices.'\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            f.clean(\"3\")\n\n        msg = \"'Select a valid choice. 3 is not one of the available choices.'\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            f_typed.clean(\"3\")\n\n        msg = \"'Select a valid choice. j is not one of the available choices.'\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            f_typed.clean(\"j\")  # coerce isn't applied during validation\n", "def test_choicefield_coercion(self):\n    class IntChoices(models.IntegerChoices):\n        ONE = 1, \"One\"\n        TWO = 2, \"Two\"\n\n    f = ChoiceField(choices=IntChoices)\n    self.assertEqual(1, f.clean(\"1\"))\n    self.assertEqual(2, f.clean(2))\n    self.assertEqual(\"1\", f.clean(\"1\"))\n    msg = \"'Select a valid choice. 3 is not one of the available choices.'\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        f.clean(\"3\")\n    with self.assertRaisesMessage(ValidationError, msg):\n        f.clean(3)\n\n    f = ChoiceField(choices=[(\"1\", \"One\"), (\"2\", \"Two\")])\n    self.assertEqual(\"1\", f.clean(\"1\"))\n    self.assertEqual(\"2\", f.clean(\"2\"))\n    msg = \"'Select a valid choice. 3 is not one of the available choices.'\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        f.clean(\"3\")\n", "def test_choicefield_coerce(self):\n    f = ChoiceField(choices=[(1, \"One\"), (2, \"Two\")], required=True, coerce=lambda x: str(x))\n    with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n        f.clean(\"\")\n    with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n        f.clean(None)\n    self.assertEqual(\"1\", f.clean(1))\n    self.assertEqual(\"1\", f.clean(\"1\"))\n    msg = \"'Select a valid choice. 3 is not one of the available choices.'\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        f.clean(\"3\")\n"], "sample_754": ["def test_spca_max_iter(spca):\n    rng = np.random.RandomState(0)\n    Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)\n    n_components = 3\n    model = spca(n_components=n_components, max_iter=1, random_state=rng)\n    model.fit(Y)\n    assert model.n_iter_ == 1\n    assert model.components_.shape == (n_components, 64)\n\n    model = spca(n_components=n_components, max_iter=10, random_state=rng)\n    model.fit(Y)\n    assert model.n_iter_ <= 10\n    assert model.components_.shape == (n_components, 64)\n", "def test_spca_components_sign(spca):\n    rng = np.random.RandomState(0)\n    Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)\n    estimator = spca(n_components=3, alpha=1, random_state=0,\n                     normalize_components=True)\n    estimator.fit(Y)\n    signs = np.sign(estimator.components_[0])\n    assert_array_almost_equal(np.abs(estimator.components_), estimator.components_ * signs)\n", "def test_spca_invalid_input(spca, norm_comp):\n    rng = np.random.RandomState(0)\n    Y = rng.randn(10, 8)\n    estimator = spca(n_components=-1, normalize_components=norm_comp)\n    with pytest.raises(ValueError, match=\"n_components must be greater than zero\"):\n        estimator.fit(Y)\n\n    estimator = spca(n_components=5, alpha=-1, normalize_components=norm_comp)\n    with pytest.raises(ValueError, match=\"alpha must be a positive float\"):\n        estimator.fit(Y)\n\n    estimator = spca(n_components=5, ridge_alpha=-1, normalize_components=norm_comp)\n    with pytest.raises(ValueError, match=\"ridge_alpha must be a positive float\"):\n        estimator.fit(Y)\n\n    estimator = spca(n_components=5, max_iter=-1, normalize_components=norm_comp)\n    with pytest.raises(ValueError, match=\"max_iter must be a positive integer\"):\n        estimator.fit(Y)\n\n    estimator = spca(n_components=5, tol=-1, normalize_components=norm_comp)\n    with pytest.raises(ValueError, match=\"tol must be a positive float\"):\n        estimator.fit(Y)\n\n    estimator = spca(n_components=5, batch_size=-1, normalize_components=norm_comp)\n    with pytest.raises(ValueError, match=\"batch_size must be a positive integer\"):\n        estimator.fit(Y)\n", "def test_spca_fit_with_invalid_input(spca):\n    rng = np.random.RandomState(0)\n    Y = rng.randn(10, 5)\n    estimator = spca(n_components=8, random_state=rng)\n    with pytest.raises(ValueError):\n        estimator.fit(Y)\n\n", "def test_spca_alpha_values(spca):\n    rng = np.random.RandomState(0)\n    Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)\n    warn_message = \"alpha must be a float\"\n    with pytest.raises(ValueError):\n        spca(alpha=\"string\").fit(Y)\n    with pytest.raises(ValueError):\n        spca(alpha=1).fit(Y)\n    with pytest.raises(ValueError):\n        spca(alpha=1.0, n_components=1).fit(Y)\n    with pytest.raises(ValueError):\n        spca(alpha=1.0, n_components=2).fit(Y)\n    with pytest.raises(ValueError):\n        spca(alpha=1.0, n_components=3).fit(Y)\n    with pytest.raises(ValueError):\n        spca(alpha=1.0, n_components=4).fit(Y)\n    with pytest.raises(ValueError):\n        spca(alpha=1.0, n_components=5).fit(Y)\n"], "sample_981": ["def test_Permutation_properties():\n    p = Permutation([2, 0, 3, 1])\n    assert p.is_Empty is False\n    assert p.is_Singleton is False\n    assert p.is_Identity is False\n    q = Permutation([0, 1, 2, 3])\n    assert q.is_Empty is False\n    assert q.is_Singleton is False\n    assert q.is_Identity is True\n    r = Permutation([0])\n    assert r.is_Empty is False\n    assert r.is_Singleton is True\n    assert r.is_Identity is True\n    s = Permutation()\n    assert s.is_Empty is True\n    assert s.is_Singleton is False\n    assert s.is_Identity is True\n", "def test_Permutation_extreme_cases():\n    # extreme cases\n    p = Permutation([])\n    assert p.size == 0\n    assert p.array_form == []\n    assert p.cyclic_form == []\n    assert p.cardinality == 1\n    assert p.inversions() == 0\n    assert p.parity() == 0\n    assert p.is_Identity\n    assert p.is_Empty\n    assert p.is_Singleton == False\n\n    p = Permutation([0])\n    assert p.size == 1\n    assert p.array_form == [0]\n    assert p.cyclic_form == [[0]]\n    assert p.cardinality == 1\n    assert p.inversions() == 0\n    assert p.parity() == 0\n    assert p.is_Identity\n    assert p.is_Empty == False\n    assert p.is_Singleton\n\n    p = Permutation([0, 1])\n    assert p.size == 2\n    assert p.array_form == [0, 1]\n    assert p.cyclic_form == [[0], [1]]\n    assert p.cardinality == 2\n    assert p.inversions() == 0\n    assert p.parity() == 0\n    assert p.is_Identity\n    assert p.is_Empty == False\n    assert p.is_Singleton == False\n\n    p = Permutation([1, 0])\n    assert p.size == 2\n    assert p.array_form == [1, 0]\n    assert p.cyclic_form == [[0, 1]]\n    assert p.cardinality == 2\n    assert p.inversions() == 1\n    assert p.parity() == 1\n    assert p.is_Identity == False\n    assert p.is_Empty == False\n    assert p.is_Singleton == False\n\n    p = Permutation([1, 2, 0])\n    assert p.size == 3\n    assert p.array_form == [1, 2, 0]\n    assert p.cyclic_form == [[0, 1, 2]]\n    assert p.cardinality == 6\n    assert p.inversions() == 2\n    assert p.parity() == 0\n    assert p.is_Identity == False\n    assert p.is_Empty == False\n    assert p.is_Singleton == False\n\n    p = Permutation([0, 2, 1])\n    assert p.size == ", "def test_cycle_structure():\n    # Check cycle structure computation\n    p = Permutation([3, 2, 0, 1])\n    assert p.cycle_structure == {1: 2, 2: 1}\n    p = Permutation([1, 2, 0])\n    assert p.cycle_structure == {3: 1}\n    p = Permutation([0, 1, 2, 3, 4])\n    assert p.cycle_structure == {1: 5}\n    p = Permutation([4, 2, 0, 3, 1])\n    assert p.cycle_structure == {2: 2, 1: 1}\n    p = Permutation([0, 1, 3, 2])\n    assert p.cycle_structure == {2: 2}\n    p = Permutation([0, 3, 2, 1])\n    assert p.cycle_structure == {4: 1}\n    p = Permutation([0, 4, 1, 3, 2])\n    assert p.cycle_structure == {3: 1, 1: 1}\n", "def test_Cycle_conversions():\n    p = Permutation([0, 3, 1, 2])\n    c = Cycle(p)\n    assert Cycle(list(c)).list() == c.list()\n    c = Cycle(1, 2, 0, 3)\n    p = Permutation(c)\n    assert Permutation(list(c)).array_form == p.array_form\n    assert c.list() == p.list()\n    c = Cycle(1, 2, 3, 0)\n    p = Permutation(c)\n    assert c.list() == p.list()\n    assert Cycle(p).list() == c.list()\n    assert Permutation(c).array_form == p.array_form\n    c = Cycle()\n    assert Permutation(c).array_form == [0]\n    c = Cycle(1)\n    assert Permutation(c).array_form == [0, 1]\n    c = Cycle(2)\n    assert Permutation(c).array_form == [0, 1, 2]\n", "def test_af_pow():\n    # test large power\n    af = [3, 2, 0, 1]\n    assert _af_pow(af, 4) == af\n    af = [3, 2, 1, 0]\n    assert _af_pow(af, 5) == af\n    af = [1, 2, 3, 0]\n    assert _af_pow(af, 6) == af\n    # test power of inverse\n    af = [2, 1, 3, 0]\n    assert _af_pow(af, -1) == [1, 0, 3, 2]\n    af = [0, 2, 1, 3]\n    assert _af_pow(af, -1) == [0, 2, 1, 3]\n    af = [0, 2, 1]\n    assert _af_pow(af, -1) == [2, 0, 1]\n    # test zero and one\n    af = [2, 0, 3, 1]\n    assert _af_pow(af, 0) == [0, 1, 2, 3]\n    assert _af_pow(af, 1) == af\n    af = [1, 3, 0, 2]\n    assert _af_pow(af, 0) == [0, 1, 2, 3]\n    assert _af_pow(af, 1) == af\n    af = [0, 3, 2, 1]\n    assert _af_pow(af, 0) == [0, 1, 2, 3]\n    assert _af_pow(af, 1) == af\n"], "sample_72": ["    def test_serialize_uuid_lazy(self):\n        \"\"\"\n        Test UUID serialization with lazy objects.\n        \"\"\"\n        lazy_uuid = SimpleLazyObject(lambda: uuid.UUID('5c859437-d061-4847-b3f7-e6b78852f8c8'))\n        self.assertEqual(self.serialize_round_trip(lazy_uuid), uuid.UUID('5c859437-d061-4847-b3f7-e6b78852f8c8'))\n", "def test_serialize_custom_operation_with_args(self):\n    class CustomOperation(Operation):\n            self.arg1 = arg1\n            self.arg2 = arg2\n            self.kwarg1 = kwarg1\n\n            return (\n                'CustomOperation',\n                (self.arg1, self.arg2),\n                {'kwarg1': self.kwarg1}\n            )\n\n    migration = type(\"Migration\", (migrations.Migration,), {\n        \"operations\": [\n            CustomOperation(1, 2, kwarg1=3)\n        ],\n        \"dependencies\": []\n    })\n    writer = MigrationWriter(migration)\n    output = writer.as_string()\n    result = self.safe_exec(output)\n    self.assertIn(\"CustomOperation\", result)\n    self.assertEqual(result.CustomOperation.arg1, 1)\n    self.assertEqual(result.CustomOperation.arg2, 2)\n    self.assertEqual(result.CustomOperation.kwarg1, 3)\n", "def test_serialize_settings_reference_with_custom_settings_module(self):\n    with self.settings(MY_SETTING='test_value', CUSTOM_SETTINGS_MODULE='migrations.test_writer.settings'):\n        self.assertSerializedEqual(SettingsReference('MY_SETTING', 'MY_SETTING'))\n", "def test_serialize_settingsreference_with_custom_settings(self):\n    \"\"\"\n    Ticket #30636: Test serialization of SettingsReference with a custom setting.\n    \"\"\"\n    custom_setting = SettingsReference('CUSTOM_SETTING', 'CUSTOM_SETTING')\n    self.assertSerializedResultEqual(\n        custom_setting,\n        (\"settings.CUSTOM_SETTING\", {\"from django.conf import settings\"})\n    )\n    with self.settings(CUSTOM_SETTING='value'):\n        self.serialize_round_trip(custom_setting)\n", "def test_serialize_custom_function_type(self):\n    \"\"\"\n    Test the serialization of a custom function.\n    \"\"\"\n\n    # Create a custom function.\n        pass\n\n    # Serialize the custom function.\n    string, imports = MigrationWriter.serialize(custom_function)\n\n    # Check the serialized string.\n    self.assertEqual(string, \"migrations.test_writer.custom_function\")\n\n    # Check the imports.\n    self.assertEqual(imports, {'import migrations.test_writer'})\n\n    # Check the function is callable.\n    result = self.safe_exec(\"%s\\ntest_function_result = %s\" % (\"\\n\".join(imports), string))['test_function_result']\n    self.assertTrue(callable(result))\n"], "sample_1097": ["def test_BlockMatrix_as_real_imag():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', n, m)\n    X = BlockMatrix([[A, B]])\n\n    real_matrices, im_matrices = X.as_real_imag()\n    assert real_matrices.blocks[0, 0] == re(A)\n    assert real_matrices.blocks[0, 1] == re(B)\n    assert im_matrices.blocks[0, 0] == im(A)\n    assert im_matrices.blocks[0, 1] == im(B)\n", "def test_blockcut_issue():\n    M = ImmutableMatrix(4, 4, range(16))\n    B = blockcut(M, (1, 1, 2), (2, 2))\n    assert ImmutableMatrix(B.blocks[0, 1]) == ImmutableMatrix([[2, 3]])\n", "def test_BlockMatrix_as_real_imag():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', m, m)\n    X = BlockMatrix([[A, B]])\n    a, b = X.as_real_imag()\n    assert a.blocks == b.blocks\n    assert a.blocks[0, 0] == A.as_real_imag()[0]\n    assert a.blocks[0, 1] == B.as_real_imag()[0]\n\n    X = BlockMatrix([[A + B*I]])\n    a, b = X.as_real_imag()\n    assert a.blocks[0, 0] == A\n    assert b.blocks[0, 0] == B\n", "def test_block_collapse_shape():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', m, m)\n    C = MatrixSymbol('C', l, l)\n    M = MatrixSymbol('M', n + m + l, n + m + l)\n\n    X = BlockDiagMatrix(A, B, C)\n    Y = BlockDiagMatrix(A, 2*B, 3*C)\n\n    Z1 = block_collapse(X + X)\n    assert Z1.shape == (n + m + l, n + m + l)\n    assert all(Z1.blocks[i, j].is_ZeroMatrix if i != j else Z1.blocks[i, j] in [2*A, 2*B, 2*C]\n            for i in range(3) for j in range(3))\n\n    Z2 = block_collapse(X * Y)\n    assert Z2.shape == (n + m + l, n + m + l)\n    assert all(Z2.blocks[i, j].is_ZeroMatrix if i != j else Z2.blocks[i, j] in [A*A, 2*B*B, 3*C*C]\n            for i in range(3) for j in range(3))\n\n    Z3 = block_collapse(X + Y)\n    assert Z3.shape == (n + m + l, n + m + l)\n    assert all(Z3.blocks[i, j].is_ZeroMatrix if i != j else Z3.blocks[i, j] in [2*A, 3*B, 4*C]\n            for i in range(3) for j in range(3))\n\n    Z4 = block_collapse(X * X)\n    assert Z4.shape == (n + m + l, n + m + l)\n    assert all(Z4.blocks[i, j].is_ZeroMatrix if i != j else Z4.blocks[i, j] in [A*A, B*B, C*C]\n            for i in range(3) for j in range(3))\n\n    Z5 = block_collapse(X.I * X)\n    assert Z5.shape == (n + m + l, n + m + l)\n    assert isinstance(Z5, Identity)\n\n    Z6 = block_collapse(X * M)\n    assert Z6.shape == (n + m + l, n + m + l)\n\n    Z7 = block_collapse(X + M)\n    assert Z7", "def test_block_collapse_edge_cases():\n    # Test BlockMatrix with a single row or column\n    A = MatrixSymbol('A', n, m)\n    B = BlockMatrix([[A]])\n    assert block_collapse(B) == A\n\n    C = MatrixSymbol('C', n, n)\n    D = MatrixSymbol('D', n, n)\n    E = MatrixSymbol('E', n, n)\n    F = BlockMatrix([[C], [D], [E]])\n    assert block_collapse(F) == BlockMatrix([[C], [D], [E]])\n\n    # Test BlockMatrix with a single element\n    G = MatrixSymbol('G', 1, 1)\n    H = BlockMatrix([[G]])\n    assert block_collapse(H) == G\n\n    # Test BlockMatrix with a single ZeroMatrix\n    Z = ZeroMatrix(1, 1)\n    I = BlockMatrix([[Z]])\n    assert block_collapse(I) == Z\n\n    # Test BlockMatrix with a mix of MatrixSymbols and ZeroMatrices\n    J = BlockMatrix([[A, Z], [Z, B]])\n    assert block_collapse(J) == BlockMatrix([[A, Z], [Z, B]])\n\n    # Test BlockMatrix with a mix of BlockMatrices and MatrixSymbols\n    K = BlockMatrix([[A, B], [B, BlockMatrix([[C, D]])]])\n    assert block_collapse(K) == BlockMatrix([[A, B], [B, BlockMatrix([[C, D]])]])\n\n    # Test BlockMatrix with a mix of BlockMatrices and ZeroMatrices\n    L = BlockMatrix([[Z, BlockMatrix([[C, D]])], [BlockMatrix([[E, F]]), Z]])\n    assert block_collapse(L) == BlockMatrix([[Z, BlockMatrix([[C, D]])], [BlockMatrix([[E, F]]), Z]])\n"], "sample_119": ["def test_multi_join(self):\n    query = Query(Item)\n    where = query.build_where(Q(creator__num__gt=2) | Q(note__note_id__lt=0))\n    self.assertEqual(where.connector, OR)\n\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, GreaterThan)\n    self.assertEqual(lookup.rhs, 2)\n    self.assertEqual(lookup.lhs.target, Author._meta.get_field('num'))\n\n    lookup = where.children[1]\n    self.assertIsInstance(lookup, LessThan)\n    self.assertEqual(lookup.rhs, 0)\n    self.assertEqual(lookup.lhs.target, Item._meta.get_field('note_id'))\n", "    def test_deferred_loading(self):\n        query = Query(Author)\n        query.add_deferred_loading(['num'])\n        loaded_fields = query.get_loaded_field_names()\n        self.assertEqual(len(loaded_fields), 1)\n        self.assertIn(Author._meta.concrete_model, loaded_fields)\n        self.assertEqual(len(loaded_fields[Author._meta.concrete_model]), 1)\n        self.assertIn(Author._meta.get_field('name').attname, loaded_fields[Author._meta.concrete_model])\n\n        query = Query(Author)\n        query.add_immediate_loading(['name'])\n        loaded_fields = query.get_loaded_field_names()\n        self.assertEqual(len(loaded_fields), 1)\n        self.assertIn(Author._meta.concrete_model, loaded_fields)\n        self.assertEqual(len(loaded_fields[Author._meta.concrete_model]), 1)\n        self.assertIn(Author._meta.get_field('name').attname, loaded_fields[Author._meta.concrete_model])\n\n        query = Query(Author)\n        query.add_deferred_loading(['num'])\n        query.add_immediate_loading(['name'])\n        loaded_fields = query.get_loaded_field_names()\n        self.assertEqual(len(loaded_fields), 1)\n        self.assertIn(Author._meta.concrete_model, loaded_fields)\n        self.assertEqual(len(loaded_fields[Author._meta.concrete_model]), 1)\n        self.assertIn(Author._meta.get_field('name').attname, loaded_fields[Author._meta.concrete_model])\n", "    def test_sliced_query(self):\n        query = Query(Author)\n        query.set_limits(10, 20)\n        self.assertTrue(query.is_sliced)\n        query.clear_limits()\n        self.assertFalse(query.is_sliced)\n        query.set_limits(10)\n        self.assertTrue(query.is_sliced)\n        query.clear_limits()\n        query.set_limits(0, 10)\n        self.assertTrue(query.is_sliced)\n        query.clear_limits()\n        query.set_limits(high=10)\n        self.assertTrue(query.is_sliced)\n", "def test_split_exclude(self):\n    query = Query(Item)\n    where = query.build_where(~Q(creator__num__gt=2))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, GreaterThan)\n    self.assertEqual(lookup.rhs, 2)\n    self.assertEqual(lookup.lhs.target, Author._meta.get_field('num'))\n    self.assertEqual(lookup.lhs.target.model, Author)\n    subquery = lookup.lhs.lhs.target\n    self.assertIsInstance(subquery, Query)\n    self.assertEqual(subquery.model, Author)\n    self.assertEqual(subquery.where.children[0].rhs, 2)\n    self.assertEqual(subquery.select[0].target, Author._meta.get_field('id'))\n", "def test_reuse_joins(self):\n    query = Query(Item)\n    query.add_filter(('creator__num__gt', 2))\n    clone = query.clone()\n    clone.add_filter(('creator__name__startswith', 'A'))\n    self.assertEqual(len(query.alias_map), len(clone.alias_map))\n"], "sample_298": ["def test_make_token_with_timestamp(self):\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    timestamp = p0._num_seconds(p0._now())\n    tk1 = p0._make_token_with_timestamp(user, timestamp)\n    self.assertIs(p0.check_token(user, tk1), True)\n    # Test with a different timestamp\n    timestamp += 100\n    tk2 = p0._make_token_with_timestamp(user, timestamp)\n    self.assertIs(p0.check_token(user, tk2), True)\n    # Test with a different user\n    user2 = User.objects.create_user('tokentestuser2', 'test3@example.com', 'testpw')\n    tk3 = p0._make_token_with_timestamp(user2, timestamp)\n    self.assertIs(p0.check_token(user2, tk3), True)\n    self.assertIs(p0.check_token(user, tk3), False)\n", "def test_make_token_with_timestamp(self):\n    \"\"\"\n    Test _make_token_with_timestamp function.\n    \"\"\"\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    timestamp = p0._num_seconds(datetime.now())\n    tk1 = p0._make_token_with_timestamp(user, timestamp)\n    tk2 = p0._make_token_with_timestamp(user, timestamp)\n    self.assertEqual(tk1, tk2)\n\n    # Test that a different user returns a different token\n    user2 = User.objects.create_user('tokentestuser2', 'test3@example.com', 'testpw')\n    tk3 = p0._make_token_with_timestamp(user2, timestamp)\n    self.assertNotEqual(tk1, tk3)\n\n    # Test that a different timestamp returns a different token\n    timestamp2 = timestamp + 1\n    tk4 = p0._make_token_with_timestamp(user, timestamp2)\n    self.assertNotEqual(tk1, tk4)\n", "def test_algorithm_property(self):\n    \"\"\"\n    Algorithm property should be read-only and can be set only once.\n    \"\"\"\n    p0 = PasswordResetTokenGenerator()\n    p0.algorithm = 'sha256'\n    with self.assertRaises(AttributeError):\n        p0.algorithm = 'sha512'\n\n    class CustomPasswordResetTokenGenerator(PasswordResetTokenGenerator):\n        algorithm = 'sha512'\n\n    custom_password_generator = CustomPasswordResetTokenGenerator()\n    self.assertEqual(custom_password_generator.algorithm, 'sha512')\n    with self.assertRaises(AttributeError):\n        custom_password_generator.algorithm = 'sha256'\n", "def test_make_token_with_timestamp(self):\n    \"\"\"The token generated includes a timestamp.\"\"\"\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    now = datetime.now()\n    tk1 = p0._make_token_with_timestamp(user, p0._num_seconds(now))\n    self.assertIs(p0.check_token(user, tk1), True)\n    # Token is invalid if the timestamp is tampered with.\n    tk2 = tk1[:-1] + 'a'\n    self.assertIs(p0.check_token(user, tk2), False)\n    # Token is invalid if the timestamp is from the future.\n    future = now + timedelta(days=1)\n    tk3 = p0._make_token_with_timestamp(user, p0._num_seconds(future))\n    self.assertIs(p0.check_token(user, tk3), False)\n", "def test_token_with_invalid_algorithm(self):\n    \"\"\"Invalid algorithm raises ValueError.\"\"\"\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    with self.assertRaises(ValueError):\n        p0.algorithm = 'invalid_algorithm'\n        p0.make_token(user)\n"], "sample_51": ["    def test_parse_date_trailing_whitespace(self):\n        self.assertIsNone(parse_date('2012-04-23 '))\n", "def test_parse_invalid_iso8601(self):\n    test_values = (\n        ('P4', None),\n        ('P4D5', None),\n        ('P4D5H', None),\n        ('P4DT5', None),\n        ('P4DT5H5', None),\n        ('P4DT5H5M5', None),\n        ('P4DT5H5M5S5', timedelta(days=4, hours=5, minutes=5, seconds=5)),\n        ('P4DT5H5M5.5S', timedelta(days=4, hours=5, minutes=5, seconds=5, microseconds=500000)),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n", "    def test_parse_date_leap_year(self):\n        # Test a date in a leap year\n        self.assertEqual(parse_date('2020-02-29'), date(2020, 2, 29))\n", "    def test_empty_string(self):\n        self.assertIsNone(parse_duration(''))\n", "    def test_zero(self):\n        self.assertEqual(parse_duration('0'), timedelta())\n        self.assertEqual(parse_duration('0 days'), timedelta())\n        self.assertEqual(parse_duration('00:00:00'), timedelta())\n"], "sample_286": ["    def test_model_state(self):\n        a = Article.objects.create(headline='foo', pub_date=datetime.now())\n        self.assertIsNotNone(a._state)\n        self.assertIsNone(a._state.db)\n        self.assertTrue(a._state.adding)\n\n        a.save()\n        self.assertIsNotNone(a._state)\n        self.assertIsNotNone(a._state.db)\n        self.assertFalse(a._state.adding)\n", "    def test_adding(self):\n        a = Article.objects.create(headline='test', pub_date=datetime.now())\n        self.assertTrue(a._state.adding)\n\n        a.save()\n        self.assertFalse(a._state.adding)\n\n        a2 = Article.objects.get(id=a.id)\n        self.assertFalse(a2._state.adding)\n", "    def test_default_manager(self):\n        class MyModel(models.Model):\n            pass\n        with self.assertRaises(ValueError):\n            MyModel.objects.create()\n", "    def test_check_constraint_error(self):\n        class ConstraintModel(models.Model):\n            num = models.IntegerField()\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(\n                        check=models.Q(num__gt=5),\n                        name='num_gt_5',\n                    ),\n                ]\n\n        ConstraintModel.objects.create(num=10)  # This should work\n\n        with self.assertRaisesMessage(DatabaseError, 'check constraint \"num_gt_5\" of model \"basic.ConstraintModel\"'):\n            ConstraintModel.objects.create(num=4)\n", "    def test_save_base_force_insert_with_default_pk(self):\n        # Force insert with default primary key should be possible.\n        class MyModel(models.Model):\n            id = models.AutoField(primary_key=True, default=1)\n\n        with self.assertNumQueries(1):\n            MyModel().save(force_insert=True)\n"], "sample_703": ["def test_invalid_input_types() -> None:\n    with pytest.raises(TypeError):\n        Expression.compile(123)\n\n    with pytest.raises(TypeError):\n        Expression.compile([1, 2, 3])\n\n    with pytest.raises(TypeError):\n        Expression.compile({\"a\": 1})\n\n    with pytest.raises(TypeError):\n        Expression.compile(None)\n\n    expr = Expression.compile(\"true\")\n    with pytest.raises(TypeError):\n        expr.evaluate(123)\n\n    with pytest.raises(TypeError):\n        expr.evaluate([1, 2, 3])\n\n    with pytest.raises(TypeError):\n        expr.evaluate({\"a\": 1})\n\n    with pytest.raises(TypeError):\n        expr.evaluate(None)\n", "def test_invalid_input_type() -> None:\n    with pytest.raises(TypeError):\n        Expression.compile(None)  # type: ignore\n\n    with pytest.raises(TypeError):\n        Expression.compile(123)\n\n    with pytest.raises(TypeError):\n        Expression.compile(object())\n\n    with pytest.raises(TypeError):\n        Expression.compile([])\n\n    with pytest.raises(TypeError):\n        Expression.compile({})\n", "def test_matcher_adapter() -> None:\n    ident = \"foo\"\n    matcher = {ident: True}.__getitem__\n    adapter = Expression.MatcherAdapter(matcher)\n    assert adapter[ident] is True\n    assert adapter[\"nonexistent\"] is False\n    with pytest.raises(NotImplementedError):\n        iter(adapter)\n    with pytest.raises(NotImplementedError):\n        len(adapter)\n", "def testMatcherFunctionWithNone(matcher: Mapping[str, bool], expression: str, expected: bool) -> None:\n        return matcher.get(ident, False)\n\n    assert evaluate(expression, matcher_function) is expected\n", "def test_matcher_adapter(matcher_input: str, matcher_output: bool, expected: bool) -> None:\n        return ident == matcher_input and matcher_output\n\n    adapter = MatcherAdapter(matcher)\n    assert adapter[f\"${matcher_input}\"] is expected\n    with pytest.raises(KeyError):\n        adapter[f\"${matcher_input}foo\"]\n\n    assert len(list(adapter.keys())) == 0\n    assert len(adapter) == 0\n"], "sample_889": ["def test_calibrated_classifier_cv_with_pipeline(data):\n    X, y = data\n    pipeline = make_pipeline(StandardScaler(), LinearSVC(random_state=7))\n    calibrated_clf = CalibratedClassifierCV(pipeline, cv=2)\n    calibrated_clf.fit(X, y)\n    assert calibrated_clf.predict(X).shape[0] == X.shape[0]\n    assert calibrated_clf.predict_proba(X).shape == (X.shape[0], len(calibrated_clf.classes_))\n", "def test_calibrated_classifier_cv_multiple_calls(data, method, ensemble):\n    \"\"\"Check that calling `fit` multiple times with the same input does not change the output.\"\"\"\n    X, y = data\n    clf = LogisticRegression()\n    calibrated_clf = CalibratedClassifierCV(clf, method=method, ensemble=ensemble)\n    calibrated_clf.fit(X, y)\n    y_pred_proba_1 = calibrated_clf.predict_proba(X)\n    calibrated_clf.fit(X, y)\n    y_pred_proba_2 = calibrated_clf.predict_proba(X)\n    assert_allclose(y_pred_proba_1, y_pred_proba_2)\n", "def test_calibrated_classifier_cv_unique_sample_weights_equivalence(method):\n    \"\"\"Check that passing unique sample weights is equivalent to passing a dataset\n    with repeated samples.\"\"\"\n    X, y = load_iris(return_X_y=True)\n    # Scale the data to avoid any convergence issue\n    X = StandardScaler().fit_transform(X)\n    # Only use 2 classes\n    X, y = X[:100], y[:100]\n\n    unique_sample_weights = np.random.RandomState(0).uniform(size=len(y))\n    repeated_X = np.repeat(X, 2, axis=0)\n    repeated_y = np.repeat(y, 2)\n\n    estimator = LogisticRegression()\n    calibrated_clf_unique_weights = CalibratedClassifierCV(\n        estimator, method=method, cv=2\n    )\n    calibrated_clf_repeated_samples = CalibratedClassifierCV(\n        estimator, method=method, cv=2\n    )\n\n    calibrated_clf_unique_weights.fit(X, y, sample_weight=unique_sample_weights)\n    calibrated_clf_repeated_samples.fit(repeated_X, repeated_y)\n\n    # Check that the underlying fitted estimators have the same coefficients\n    for est_unique_weights, est_repeated_samples in zip(\n        calibrated_clf_unique_weights.calibrated_classifiers_,\n        calibrated_clf_repeated_samples.calibrated_classifiers_,\n    ):\n        assert_allclose(\n            est_unique_weights.estimator.coef_,\n            est_repeated_samples.estimator.coef_,\n        )\n\n    # Check that the predictions are the same\n    y_pred_unique_weights = calibrated_clf_unique_weights.predict_proba(X)\n    y_pred_repeated_samples = calibrated_clf_repeated_samples.predict_proba(X)\n\n    assert_allclose(y_pred_unique_weights, y_pred_repeated_samples)\n", "def test_calibrated_classifier_cv_more_bins_than_classes(ensemble, method):\n    # Test that CalibrationCurve raises an error when n_bins > n_classes\n    X, y = make_classification(n_samples=100, n_features=5, n_classes=3, random_state=42)\n    clf = LogisticRegression()\n\n    calibrated_clf = CalibratedClassifierCV(clf, cv=3, method=method, ensemble=ensemble)\n    calibrated_clf.fit(X, y)\n\n    n_bins = 5\n    with pytest.raises(ValueError, match=\"Number of bins is higher than number of classes\"):\n        calibration_curve(y, calibrated_clf.predict_proba(X)[:, 1], n_bins=n_bins)\n\n    n_bins = 3\n    prob_true, prob_pred = calibration_curve(y, calibrated_clf.predict_proba(X)[:, 1], n_bins=n_bins)\n    assert len(prob_true) == len(prob_pred)\n    assert len(prob_true) == n_bins\n", "def test_calibration_multiclass_target_names(method, ensemble):\n    # Check that _CalibratedClassifier handles multiclass target names properly\n    X, y = make_classification(n_samples=50, n_features=4, n_informative=3, n_redundant=0, random_state=42)\n    target_names = [\"class1\", \"class2\", \"class3\"]\n    y = np.random.choice(target_names, size=len(y))\n\n    clf = LinearSVC(random_state=7)\n    cal_clf = CalibratedClassifierCV(clf, method=method, ensemble=ensemble)\n    cal_clf.fit(X, y)\n    probas = cal_clf.predict_proba(X)\n    assert_allclose(np.sum(probas, axis=1), np.ones(len(X)))\n\n    # Check that _CalibratedClassifier handles multiclass target names with\n    # classes that have different lengths\n    X, y = make_classification(n_samples=50, n_features=4, n_informative=3, n_redundant=0, random_state=42)\n    target_names = [\"c1\", \"longer_class_name\", \"c3\"]\n    y = np.random.choice(target_names, size=len(y))\n\n    clf = LinearSVC(random_state=7)\n    cal_clf = CalibratedClassifierCV(clf, method=method, ensemble=ensemble)\n    cal_clf.fit(X, y)\n    probas = cal_clf.predict_proba(X)\n    assert_allclose(np.sum(probas, axis=1), np.ones(len(X)))\n"], "sample_13": ["def test_longitude_wrap_at_edge_cases():\n    lon = Longitude(np.array([0.999999999, 1.0, 1.000000001]) * 360, unit=u.deg)\n    assert np.all(lon.degree == np.array([359.999999999, 0.0, 0.000000001]))\n\n    lon = Longitude(np.array([0.999999999, 1.0, 1.000000001]) * 360, unit=u.deg, wrap_angle='180d')\n    assert np.all(lon.degree == np.array([-179.999999999, -180.0, -179.999999999]))\n\n    lon = Longitude(np.array([0.999999999, 1.0, 1.000000001]) * 360, unit=u.deg, wrap_angle='-180d')\n    assert np.all(lon.degree == np.array([179.999999999, 180.0, 179.999999999]))\n", "def test_longitude_wrap_at_edge_cases():\n    \"\"\"\n    Test wrapping of Longitude around some edge cases\n    \"\"\"\n    lon = Longitude([0.000001, 359.999999, 360.0, 360.000001, -0.000001, -359.999999, -360.0, -360.000001] * u.deg)\n    assert np.allclose(lon.degree, [0.000001, 359.999999, 0.0, 0.000001, -0.000001, -359.999999, 0.0, -0.000001])\n\n    lon = Longitude([0.000001, 359.999999, 360.0, 360.000001, -0.000001, -359.999999, -360.0, -360.000001] * u.deg, wrap_angle='180d')\n    assert np.allclose(lon.degree, [0.000001, -179.999999, 180.0, 0.000001, -0.000001, 179.999999, -180.0, -0.000001])\n", "def test_longitude_wrap_angle_property():\n    \"\"\"\n    Tests the wrap_angle property of Longitude\n    \"\"\"\n\n    lon = Longitude('10d')\n    assert lon.wrap_angle == 360*u.deg\n\n    lon.wrap_angle = 180*u.deg\n    assert lon.wrap_angle == 180*u.deg\n    assert lon.degree == -170*u.deg\n\n    lon.wrap_angle = 360*u.deg\n    assert lon.wrap_angle == 360*u.deg\n    assert lon.degree == 10*u.deg\n\n    # wrap_angle should be an Angle\n    assert isinstance(lon.wrap_angle, Angle)\n\n    # ensure it's not copied\n    wrap_angle = 180*u.deg\n    lon.wrap_angle = wrap_angle\n    assert np.may_share_memory(lon.wrap_angle, wrap_angle)\n", "def test_angle_repr_on_array(cls):\n    \"\"\"\n    Test that the repr method of Longitude and Latitude objects\n    contains the string representation of the values\n    \"\"\"\n    angle = cls([1, 2, 3], unit=u.deg)\n    assert repr(angle).startswith(f'<{cls.__name__} [{angle.to_string()}] deg>')\n", "def test_latitude_limits_on_array():\n    \"\"\"\n    Test that the validation of the Latitude value range works on an array\n    of angles.\n    \"\"\"\n    # test with a valid array\n    lat = Latitude(np.array([-np.pi/2, np.pi/2]), u.rad)\n    assert np.allclose(lat.value, np.array([-np.pi/2, np.pi/2]))\n\n    # test with an array containing an invalid value\n    with pytest.raises(ValueError, match=r\"Latitude angle\\(s\\) must be within.*\"):\n        Latitude(np.array([np.pi/2 + 0.001, np.pi/2]), u.rad)\n\n    # test with an array containing an invalid value, but also NaN\n    with pytest.raises(ValueError, match=r\"Latitude angle\\(s\\) must be within.*\"):\n        Latitude(np.array([np.pi/2 + 0.001, np.pi/2, np.nan]), u.rad)\n\n    # test with an array of NaN values\n    Latitude(np.array([np.nan, np.nan]), u.rad)\n"], "sample_11": ["def test_sliced_low_level_wcs_drop_world_dimensions_all():\n    wcs = WCS_SPECTRAL_CUBE\n\n    with pytest.raises(ValueError, match='Cannot slice WCS: the resulting WCS '\n                                         'should have at least one pixel and '\n                                         'one world dimension'):\n        SlicedLowLevelWCS(wcs, np.s_[0, 0, 0])\n", "def test_pixel_to_world_values_all():\n    wcs = SlicedLowLevelWCS(WCS_SPECTRAL_CUBE, [Ellipsis, 5])\n\n    pixel_arrays = [np.asanyarray([39, 44])]\n    result = wcs._pixel_to_world_values_all(*pixel_arrays)\n\n    assert_allclose(result, (12.4, 20, 25))\n", "def test_dropped_dimensions_3d_spectral_axis(cube_3d_fitswcs):\n    sub = SlicedLowLevelWCS(cube_3d_fitswcs, np.s_[12, 5, 5])\n\n    dwd = sub.dropped_world_dimensions\n    wao_classes = dwd.pop(\"world_axis_object_classes\")\n    wao_components = dwd.pop(\"world_axis_object_components\")\n\n    validate_info_dict(dwd, {\n        \"value\": [4.e+00, -2.e+00],\n        \"world_axis_physical_types\": [\"pos.eq.ra\", \"pos.eq.dec\"],\n        \"world_axis_names\": ['Right Ascension', 'Declination'],\n        \"world_axis_units\": [\"deg\", \"deg\"],\n        \"serialized_classes\": False,\n        })\n\n    assert wao_classes['celestial'][0] is SkyCoord\n    assert wao_classes['celestial'][1] == ()\n    assert isinstance(wao_classes['celestial'][2]['frame'], ICRS)\n    assert wao_classes['celestial'][2]['unit'] is u.deg\n    assert wao_components[0] == ('celestial', 0, 'spherical.lon.degree')\n    assert wao_components[1] == ('celestial', 1, 'spherical.lat.degree')\n\n    sub = SlicedLowLevelWCS(cube_3d_fitswcs, np.s_[12, 12, 5])\n\n    dwd = sub.dropped_world_dimensions\n    wao_classes = dwd.pop(\"world_axis_object_classes\")\n    wao_components = dwd.pop(\"world_axis_object_components\")\n    validate_info_dict(dwd, {\n        \"value\": [5.e+00],\n        \"world_axis_physical_types\": [\"pos.eq.dec\"],\n        \"world_axis_names\": [\"Declination\"],\n        \"world_axis_units\": [\"deg\"],\n        \"serialized_classes\": False,\n        })\n    assert wao_components[0] == ('celestial', 1, 'spherical.lat.degree')\n    assert wao_classes['celestial'][0] is SkyCoord\n    assert wao_classes['celestial'][1] == ()\n    assert isinstance(wao_classes['celestial'][2]['frame'], ICRS)\n    assert wao_classes['celestial'][2]['unit'] is u.deg\n", "def test_dropped_dimensions_serialized_classes():\n    wcs = WCS_SPECTRAL_CUBE\n\n    sub = SlicedLowLevelWCS(wcs, np.s_[:, 0, 0])\n\n    dwd = sub.dropped_world_dimensions\n    wao_classes = dwd.pop(\"world_axis_object_classes\")\n    validate_info_dict(dwd, {\n        \"value\": [12.86995801, 20.49217541],\n        \"world_axis_physical_types\": [\"pos.galactic.lat\", \"pos.galactic.lon\"],\n        \"world_axis_names\": [\"Latitude\", \"Longitude\"],\n        \"world_axis_units\": [\"deg\", \"deg\"],\n        \"serialized_classes\": False,\n        \"world_axis_object_components\": [('celestial', 1, 'spherical.lat.degree'), ('celestial', 0, 'spherical.lon.degree')],\n        })\n\n    assert wao_classes['celestial'][0] is SkyCoord\n    assert wao_classes['celestial'][1] == ()\n    assert isinstance(wao_classes['celestial'][2]['frame'], Galactic)\n    assert wao_classes['celestial'][2]['unit'] is u.deg\n\n    # Try with serialized_classes=True\n    sub = SlicedLowLevelWCS(wcs, np.s_[:, 0, 0], serialized_classes=True)\n    dwd = sub.dropped_world_dimensions\n    wao_classes = dwd.pop(\"world_axis_object_classes\")\n    validate_info_dict(dwd, {\n        \"value\": [12.86995801, 20.49217541],\n        \"world_axis_physical_types\": [\"pos.galactic.lat\", \"pos.galactic.lon\"],\n        \"world_axis_names\": [\"Latitude\", \"Longitude\"],\n        \"world_axis_units\": [\"deg\", \"deg\"],\n        \"serialized_classes\": True,\n        \"world_axis_object_components\": [('celestial', 1, 'spherical.lat.degree'), ('celestial', 0, 'spherical.lon.degree')],\n        })\n    assert wao_classes['celestial'] == \"astropy.coordinates.SkyCoord\"\n    assert wao_classes['spectral'] == \"astropy.units.Quantity\"\n", "def test_dropped_dimensions_sliced_spectral():\n    wcs = WCS_SPECTRAL_CUBE\n    sub = SlicedLowLevelWCS(wcs, np.s_[:, 3:8, :])\n\n    dwd = sub.dropped_world_dimensions\n    wao_classes = dwd.pop(\"world_axis_object_classes\")\n    wao_components = dwd.pop(\"world_axis_object_components\")\n\n    validate_info_dict(dwd, {\n        \"value\": [25],\n        \"world_axis_physical_types\": [\"pos.galactic.lon\"],\n        \"world_axis_names\": [\"Longitude\"],\n        \"world_axis_units\": [\"deg\"],\n        \"serialized_classes\": False,\n        })\n\n    assert wao_classes['celestial'][0] is SkyCoord\n    assert wao_classes['celestial'][1] == ()\n    assert isinstance(wao_classes['celestial'][2]['frame'], Galactic)\n    assert wao_classes['celestial'][2]['unit'] is u.deg\n\n    assert wao_components[0] == ('celestial', 0, 'spherical.lon.degree')\n\n    sub = SlicedLowLevelWCS(wcs, np.s_[:, :, 3:8])\n\n    dwd = sub.dropped_world_dimensions\n    wao_classes = dwd.pop(\"world_axis_object_classes\")\n    wao_components = dwd.pop(\"world_axis_object_components\")\n\n    validate_info_dict(dwd, {\n        \"value\": [15],\n        \"world_axis_physical_types\": [\"pos.galactic.lat\"],\n        \"world_axis_names\": [\"Latitude\"],\n        \"world_axis_units\": [\"deg\"],\n        \"serialized_classes\": False,\n        })\n\n    assert wao_classes['celestial'][0] is SkyCoord\n    assert wao_classes['celestial'][1] == ()\n    assert isinstance(wao_classes['celestial'][2]['frame'], Galactic)\n    assert wao_classes['celestial'][2]['unit'] is u.deg\n\n    assert wao_components[0] == ('celestial', 1, 'spherical.lat.degree')\n"], "sample_355": ["    def test_create_user(self):\n        user = User.objects.create_user('test', 'test@example.com', 'test')\n        self.assertIsNotNone(user)\n        self.assertEqual(user.username, 'test')\n        self.assertEqual(user.email, 'test@example.com')\n        self.assertTrue(user.check_password('test'))\n", "    def setUp(self):\n        self.permission = Permission.objects.create(name='test', codename='test', content_type_id=1)\n", "    def test_unique_together(self):\n        \"\"\"\n        Tests that unique_together constraint for Permission model works.\n        \"\"\"\n        content_type = ContentType.objects.create(app_label='auth', model='permission')\n        permission = Permission.objects.create(name='test', content_type=content_type, codename='test')\n        with self.assertRaises(IntegrityError):\n            Permission.objects.create(name='test2', content_type=content_type, codename='test')\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user('test', 'test@example.com', 'test')\n        cls.group = Group.objects.create(name='test_group')\n        cls.content_type = ContentType.objects.get_for_model(Group)\n        cls.perm = Permission.objects.create(name='test', content_type=cls.content_type, codename='test')\n        cls.user.user_permissions.add(cls.perm)\n        cls.obj = Group.objects.create(name='test_object')\n", "    def test_username_validation(self):\n        user = User(username='test')\n        with self.assertRaises(ValidationError) as cm:\n            user.full_clean()\n        self.assertEqual(cm.exception.messages, {\n            'username': [_(\"This field cannot be blank.\")],\n        })\n"], "sample_87": ["    def test_isatty_false(self, isatty):\n        autoreload.ensure_echo_on()\n        isatty.assert_called_once()\n", "    def test_runserver(self):\n        with mock.patch('sys.argv', ['./manage.py', 'runserver']):\n            args = autoreload.get_child_arguments()\n        self.assertEqual(args, [sys.executable, '-Wignore', './manage.py', 'runserver'])\n", "    def test_termios_not_installed(self, mocked_termios):\n        mocked_termios.__bool__.return_value = False\n        autoreload.ensure_echo_on()\n", "    def test_get_child_arguments_no_sys_argv(self):\n        with mock.patch('sys.argv', []):\n            with self.assertRaisesMessage(IndexError, 'list index out of range'):\n                autoreload.get_child_arguments()\n", "    def test_executable(self, mock_get_child_args):\n        # The executable should be sys.executable by default.\n        mock_get_child_args.return_value = [sys.executable]\n        args = autoreload.get_child_arguments()\n        self.assertEqual(args[0], sys.executable)\n"], "sample_101": ["    def test_get_script_name(self):\n        \"\"\"\n        get_script_name() returns the correct SCRIPT_NAME.\n        \"\"\"\n        environ = {\n            'wsgi.url_scheme': 'http',\n            'SERVER_NAME': 'example.com',\n            'SERVER_PORT': '80',\n            'SCRIPT_NAME': '/myapp',\n            'PATH_INFO': '/path/to/resource',\n        }\n        self.assertEqual(get_script_name(environ), '/myapp')\n", "    def test_wsgi_request_init(self):\n        environ = {\n            'REQUEST_METHOD': 'GET',\n            'CONTENT_LENGTH': '0',\n            'wsgi.input': BytesIO(),\n            'PATH_INFO': '/path/info',\n            'SCRIPT_NAME': '/script/name',\n            'HTTP_COOKIE': 'cookie=value',\n            'QUERY_STRING': 'query=value'\n        }\n        request = WSGIRequest(environ)\n\n        self.assertEqual(request.method, 'GET')\n        self.assertEqual(request.path_info, '/path/info')\n        self.assertEqual(request.path, '/script/name/path/info')\n        self.assertEqual(request.META['PATH_INFO'], '/path/info')\n        self.assertEqual(request.META['SCRIPT_NAME'], '/script/name')\n        self.assertEqual(request.COOKIES, {'cookie': 'value'})\n        self.assertEqual(request.GET, {'query': 'value'})\n", "    def test_wsgi_request_init(self):\n        \"\"\"\n        Test the initialization of a WSGIRequest object.\n        \"\"\"\n        environ = {\n            'REQUEST_METHOD': 'GET',\n            'PATH_INFO': '/path/',\n            'SCRIPT_NAME': '/script/',\n            'CONTENT_LENGTH': '10',\n            'wsgi.input': BytesIO(b'Hello World!'),\n        }\n        request = WSGIRequest(environ)\n\n        self.assertEqual(request.method, 'GET')\n        self.assertEqual(request.path_info, '/path/')\n        self.assertEqual(request.path, '/script/path/')\n        self.assertEqual(request.META['PATH_INFO'], '/path/')\n        self.assertEqual(request.META['SCRIPT_NAME'], '/script/')\n        self.assertEqual(request.META['CONTENT_LENGTH'], '10')\n", "    def setUp(self):\n        self.environ = {\n            'wsgi.url_scheme': 'https',\n            'SCRIPT_NAME': '',\n            'PATH_INFO': '/test/',\n            'QUERY_STRING': 'key=value',\n            'CONTENT_LENGTH': '10',\n            'CONTENT_TYPE': 'application/x-www-form-urlencoded',\n            'HTTP_COOKIE': 'sessionid=123',\n            'HTTP_HOST': 'example.com',\n            'REQUEST_METHOD': 'POST',\n            'wsgi.input': BytesIO(b'key=value'),\n        }\n", "def test_limited_stream(self):\n    \"\"\"\n    Test LimitedStream for reading up to a certain limit.\n    \"\"\"\n    stream = BytesIO(b'Hello, World!')\n    limited_stream = LimitedStream(stream, 7)\n\n    self.assertEqual(limited_stream.read(5), b'Hello')\n    self.assertEqual(limited_stream.read(5), b', Worl')\n    with self.assertRaises(ValueError):\n        limited_stream.read()\n"], "sample_224": ["def test_aggregate_subquery_annotation_values_collision_with_group_by(self):\n    \"\"\"\n    Subquery annotations and external aliases are excluded from the GROUP BY\n    if they are not selected, even if they are part of the group by clause.\n    \"\"\"\n    books_qs = Book.objects.annotate(\n        first_author_the_same_age=Subquery(\n            Author.objects.filter(\n                age=OuterRef('contact__friends__age'),\n            ).order_by('age').values('id')[:1],\n        )\n    ).filter(\n        publisher=self.p1,\n        first_author_the_same_age__isnull=False,\n    ).annotate(\n        min_age=Min('contact__friends__age'),\n    ).values('publisher', 'name').order_by('name')\n    with self.assertNumQueries(1) as ctx:\n        list(books_qs)\n    self.assertEqual(ctx[0]['sql'].count('SELECT'), 2)\n    # The GROUP BY should not be by alias either.\n    self.assertEqual(ctx[0]['sql'].lower().count('first_author_the_same_age'), 1)\n    self.assertEqual(list(books_qs), [\n        {'publisher': self.p1.id, 'name': 'Practical Django Projects'},\n        {'publisher': self.p1.id, 'name': 'The Definitive Guide to Django: Web Development Done Right'},\n    ])\n", "def test_bulk_update(self):\n    # Ensure that bulk_update updates all the objects\n    objects = Book.objects.bulk_create([\n        Book(name='Test1', rating=4.0, contact=self.a1),\n        Book(name='Test2', rating=4.5, contact=self.a1),\n        Book(name='Test3', rating=3.0, contact=self.a1),\n    ])\n    self.assertEqual(\n        Book.objects.filter(contact=self.a1).aggregate(Max('rating'))['rating__max'],\n        4.5\n    )\n    Book.objects.bulk_update(objects, ['rating'])\n    self.assertEqual(\n        Book.objects.filter(contact=self.a1).aggregate(Max('rating'))['rating__max'],\n        4.0\n    )\n", "    def test_get_or_create(self):\n        # Test that get_or_create() works correctly with defaults.\n        Book.objects.filter(isbn='123456789').delete()\n        book, created = Book.objects.get_or_create(\n            isbn='123456789',\n            defaults={'name': 'New Book', 'price': Decimal('10.00'), 'pages': 100, 'rating': 1.0},\n        )\n        self.assertEqual(book.isbn, '123456789')\n        self.assertEqual(book.name, 'New Book')\n        self.assertEqual(book.price, Decimal('10.00'))\n        self.assertEqual(book.pages, 100)\n        self.assertEqual(book.rating, 1.0)\n        self.assertTrue(created)\n\n        # Test that get_or_create() raises an IntegrityError if the object\n        # already exists and the defaults don't match the existing object.\n        with self.assertRaises(IntegrityError):\n            Book.objects.get_or_create(\n                isbn='123456789',\n                defaults={'name': 'New Book 2', 'price': Decimal('10.00'), 'pages': 100, 'rating': 1.0},\n            )\n\n        # Test that get_or_create() works correctly with defaults when the\n        # object already exists.\n        book, created = Book.objects.get_or_create(\n            isbn='123456789',\n            defaults={'name': 'New Book', 'price': Decimal('10.00'), 'pages': 100, 'rating': 1.0},\n        )\n        self.assertEqual(book.isbn, '123456789')\n        self.assertEqual(book.name, 'New Book')\n        self.assertEqual(book.price, Decimal('10.00'))\n        self.assertEqual(book.pages, 100)\n        self.assertEqual(book.rating, 1.0)\n        self.assertFalse(created)\n", "def test_union_exclusion(self):\n    # A simple union\n    qs1 = Book.objects.filter(pk=self.b1.pk).annotate(c1=Value('A'))\n    qs2 = Book.objects.filter(pk=self.b2.pk).annotate(c1=Value('B'))\n    qs = qs1.union(qs2)\n    self.assertEqual(\n        list(qs.order_by('pk')),\n        [\n            {'name': 'The Definitive Guide to Django: Web Development Done Right', 'c1': 'A'},\n            {'name': 'Sams Teach Yourself Django in 24 Hours', 'c1': 'B'},\n        ]\n    )\n    qs = qs2.union(qs1)\n    self.assertEqual(\n        list(qs.order_by('pk')),\n        [\n            {'name': 'The Definitive Guide to Django: Web Development Done Right', 'c1': 'A'},\n            {'name': 'Sams Teach Yourself Django in 24 Hours', 'c1': 'B'},\n        ]\n    )\n\n    # Union with an aggregate annotation\n    qs1 = Book.objects.filter(pk=self.b1.pk).annotate(c1=Value('A'), c2=Sum('pages'))\n    qs2 = Book.objects.filter(pk=self.b2.pk).annotate(c1=Value('B'), c2=Sum('pages'))\n    qs = qs1.union(qs2)\n    self.assertEqual(\n        list(qs.order_by('pk')),\n        [\n            {'name': 'The Definitive Guide to Django: Web Development Done Right', 'c1': 'A', 'c2': 447},\n            {'name': 'Sams Teach Yourself Django in 24 Hours', 'c1': 'B', 'c2': 528},\n        ]\n    )\n    qs = qs2.union(qs1)\n    self.assertEqual(\n        list(qs.order_by('pk')),\n        [\n            {'name': 'The Definitive Guide to Django: Web Development Done Right', 'c1': 'A', 'c2': 447},\n            {'name': 'Sams Teach Yourself Django in 24 Hours', 'c1': 'B', 'c2': 528},\n        ]\n    )\n\n    # Union with a Count annotation\n    qs1 = Book.objects.filter(pk=self.b1.pk).annotate(c1=Value('A'), c2=Count('authors'))\n    qs2 = Book.objects.filter(pk=self.b2.pk).annotate(c1=Value('B'), c2=Count('authors'))\n", "def test_annotations_from_two_models(self):\n    \"\"\"\n    Annotations from two different models are combined correctly in a single\n    query.\n    \"\"\"\n    class MyMax(Max):\n            copy = self.copy()\n            copy.set_source_expressions(copy.get_source_expressions()[0:1])\n            return super(MyMax, copy).as_sql(compiler, connection)\n\n    author_annotation = Author.objects.annotate(max_age=MyMax('age'))\n    book_annotation = Book.objects.annotate(max_pages=MyMax('pages'))\n    books_qs = Book.objects.filter(authors__in=author_annotation).annotate(\n        max_author_age=Subquery(author_annotation.filter(id=OuterRef('authors__id')).values('max_age')),\n        max_pages=Subquery(book_annotation.filter(id=OuterRef('id')).values('max_pages'))\n    )\n    self.assertEqual(books_qs.count(), Book.objects.count())\n\n    max_pages, max_author_age = books_qs.aggregate(max_pages=Max('max_pages'), max_author_age=Max('max_author_age'))\n    self.assertEqual(max_pages, 1132)\n    self.assertEqual(max_author_age, 57)\n"], "sample_733": ["def test_tfidf_vectorizer_smooth_idf():\n    # Non-regression test: TfidfVectorizer used to ignore its \"smooth_idf\" param.\n    v = TfidfVectorizer(smooth_idf=False, use_idf=True, norm=None)\n    assert_false(v.smooth_idf)\n\n    X = v.fit_transform(['hello world', 'hello hello']).toarray()\n    assert_array_almost_equal(X, np.array([[0.70710678, 0.70710678],\n                                          [1., 0.]]))\n\n    X2 = v.transform(['hello world', 'hello hello']).toarray()\n    assert_array_almost_equal(X2, np.array([[0.70710678, 0.70710678],\n                                           [1., 0.]]))\n", "def test_vectorizer_stop_words_max_df():\n    # Test that stop words are ignored when max_df is less than 1.0\n    vectorizer = CountVectorizer(max_df=0.7)\n    X = vectorizer.fit_transform(JUNK_FOOD_DOCS)\n    assert_false('the' in vectorizer.vocabulary_)\n    assert_true('pizza' in vectorizer.vocabulary_)\n    assert_true('beer' in vectorizer.vocabulary_)\n", "def test_vectorizer_get_stop_words():\n    # Test _check_stop_list function with different inputs\n    assert_equal(_check_stop_list(\"english\"), ENGLISH_STOP_WORDS)\n    stop_list = [\"the\", \"a\", \"is\"]\n    assert_equal(_check_stop_list(stop_list), frozenset(stop_list))\n    assert_raise_message(ValueError, \"not a built-in stop list\", _check_stop_list, \"invalid\")\n", "def test_countvectorizer_min_max_df():\n    # raw documents\n    data = JUNK_FOOD_DOCS + NOTJUNK_FOOD_DOCS\n\n    vect = CountVectorizer(min_df=0.8)\n    vect.fit(data)\n    assert_equal(len(vect.vocabulary_), 1)\n    assert_equal(list(vect.vocabulary_.keys())[0], 'the')\n\n    vect = CountVectorizer(min_df=0.8, max_df=0.6)\n    assert_raises(ValueError, vect.fit, data)\n", "def test_vectorizer_default_parameters():\n    # Check the default values of the parameters for the vectorizers\n    for Vectorizer in (HashingVectorizer, CountVectorizer, TfidfVectorizer):\n        vectorizer = Vectorizer()\n        assert_equal(vectorizer.input, 'content')\n        assert_equal(vectorizer.encoding, 'utf-8')\n        assert_equal(vectorizer.decode_error, 'strict')\n        assert_equal(vectorizer.strip_accents, None)\n        assert_equal(vectorizer.lowercase, True)\n        assert_equal(vectorizer.preprocessor, None)\n        assert_equal(vectorizer.tokenizer, None)\n        assert_equal(vectorizer.analyzer, 'word')\n        assert_equal(vectorizer.stop_words, None)\n        assert_equal(vectorizer.token_pattern, r\"(?u)\\b\\w\\w+\\b\")\n        assert_equal(vectorizer.ngram_range, (1, 1))\n        if hasattr(vectorizer, 'max_df'):\n            assert_equal(vectorizer.max_df, 1.0)\n        if hasattr(vectorizer, 'min_df'):\n            assert_equal(vectorizer.min_df, 1)\n        if hasattr(vectorizer, 'max_features'):\n            assert_equal(vectorizer.max_features, None)\n        assert_equal(vectorizer.vocabulary, None)\n        assert_equal(vectorizer.binary, False)\n        assert_equal(vectorizer.dtype, np.int64)\n        if hasattr(vectorizer, 'norm'):\n            assert_equal(vectorizer.norm, 'l2')\n        if hasattr(vectorizer, 'use_idf'):\n            assert_equal(vectorizer.use_idf, True)\n        if hasattr(vectorizer, 'smooth_idf'):\n            assert_equal(vectorizer.smooth_idf, True)\n        if hasattr(vectorizer, 'sublinear_tf'):\n            assert_equal(vectorizer.sublinear_tf, False)\n        if hasattr(vectorizer, 'n_features'):\n            assert_equal(vectorizer.n_features, 2**20)\n        if hasattr(vectorizer, 'alternate_sign'):\n            assert_equal(vectorizer.alternate_sign, True)\n        if hasattr(vectorizer, 'non_negative'):\n            assert_equal(vectorizer.non_negative, False)\n"], "sample_578": ["def test_baseline(self, x, y, color):\n\n    baseline = pd.Series([2, 2, 2, 2, 2], name=\"baseline\")\n    p = Plot(x, y, baseline=baseline).add(Bars()).plot()\n    ax = p._figure.axes[0]\n    paths = ax.collections[0].get_paths()\n    for i, path in enumerate(paths):\n        verts = path.vertices\n        assert verts[0, 1] == pytest.approx(2)\n", "def test_baseline(self, x, y):\n\n    baseline = [2, 2, 2, 2, 2]\n    p = Plot(x, y, baseline=baseline).add(Bars()).plot()\n    ax = p._figure.axes[0]\n    paths = ax.collections[0].get_paths()\n    assert len(paths) == len(x)\n    for i, path in enumerate(paths):\n        verts = path.vertices\n        assert verts[0, 1] == pytest.approx(baseline[i])\n        assert verts[3, 1] == pytest.approx(y[i])\n", "    def test_baseline(self, x, y):\n\n        baseline = [2, 4, 1, 6, 3]\n        p = Plot(x, y).add(Bars(baseline=baseline)).plot()\n        ax = p._figure.axes[0]\n        paths = ax.collections[0].get_paths()\n        assert len(paths) == len(x)\n        for i, path in enumerate(paths):\n            verts = path.vertices\n            assert verts[0, 1] == baseline[i]\n            assert verts[3, 1] == y[i] + baseline[i]\n", "    def test_baseline(self, x, y):\n\n        baseline = [1, 2, 3, 4, 5]\n        p = Plot(x, y, baseline=baseline).add(Bars()).plot()\n        ax = p._figure.axes[0]\n        paths = ax.collections[0].get_paths()\n        for i, path in enumerate(paths):\n            verts = path.vertices\n            assert verts[0, 1] == pytest.approx(baseline[i])\n", "def test_auto_edgewidth_is_independent_of_data_values(self):\n\n    x = np.arange(10)\n    y = np.arange(1000)\n\n    p = Plot(x, y).add(Bars()).plot()\n    lw = p._figure.axes[0].collections[0].get_linewidths()\n\n    y = np.arange(1000) + 10000\n    p = Plot(x, y).add(Bars()).plot()\n    new_lw = p._figure.axes[0].collections[0].get_linewidths()\n\n    assert_array_equal(lw, new_lw)\n"], "sample_571": ["def test_residplot_partial(self):\n\n    x = self.rs.randn(100)\n    y = x + self.rs.randn(100)\n    z = x + self.rs.randn(100)\n\n    ax = lm.residplot(x=x, y=y)\n    resid = y - np.polyval(np.polyfit(x, y, 1), x)\n    x_plot, y_plot = ax.collections[0].get_offsets().T\n    npt.assert_array_equal(x, x_plot)\n    npt.assert_array_almost_equal(resid, y_plot)\n\n    ax = lm.residplot(x=x, y=y, x_partial=z)\n    resid = y - np.polyval(np.polyfit(x, y, 1), x)\n    resid -= np.polyval(np.polyfit(z, resid, 1), z)\n    x_plot, y_plot = ax.collections[0].get_offsets().T\n    npt.assert_array_equal(x, x_plot)\n    npt.assert_array_almost_equal(resid, y_plot)\n\n    ax = lm.residplot(x=x, y=y, y_partial=z)\n    resid = y - np.polyval(np.polyfit(x, y, 1), x)\n    resid -= np.polyval(np.polyfit(z, y, 1), z)\n    x_plot, y_plot = ax.collections[0].get_offsets().T\n    npt.assert_array_equal(x, x_plot)\n    npt.assert_array_almost_equal(resid, y_plot)\n\n    ax = lm.residplot(x=x, y=y, x_partial=z, y_partial=z)\n    resid = y - np.polyval(np.polyfit(x, y, 1), x)\n    resid -= np.polyval(np.polyfit(z, y, 1), z)\n    resid -= np.polyval(np.polyfit(z, resid, 1), z)\n    x_plot, y_plot = ax.collections[0].get_offsets().T\n    npt.assert_array_equal(x, x_plot)\n    npt.assert_array_almost_equal(resid, y_plot)\n", "    def test_lmplot_facet_kws_deprecation(self):\n\n        with pytest.warns(UserWarning):\n            g = lm.lmplot(data=self.df, x=\"x\", y=\"y\", col=\"h\", sharex=True)\n\n        with pytest.warns(UserWarning):\n            g = lm.lmplot(data=self.df, x=\"x\", y=\"y\", col=\"h\", sharey=False)\n\n        with pytest.warns(UserWarning):\n            g = lm.lmplot(data=self.df, x=\"x\", y=\"y\", col=\"h\", legend_out=True)\n\n        g = lm.lmplot(data=self.df, x=\"x\", y=\"y\", col=\"h\",\n                      facet_kws={\"sharex\": True, \"sharey\": False})\n", "def test_regplot_multilevel_bootstrap(self):\n\n    f, ax = plt.subplots()\n    ax = lm.regplot(x=\"x\", y=\"y\", data=self.df, units=\"u\", ci=95, seed=1234)\n    assert len(ax.lines) == 1\n    assert len(ax.collections) == 2\n\n    x, y = ax.collections[0].get_offsets().T\n    npt.assert_array_equal(x, self.df.x)\n    npt.assert_array_equal(y, self.df.y)\n", "def test_lmplot_x_ci(self):\n\n    g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, hue=\"h\", x_ci=68)\n    ax = g.axes[0, 0]\n    assert len(ax.lines) == 2\n    assert len(ax.collections) == 4\n\n    for line in ax.lines:\n        xdata, ydata = line.get_xydata().T\n        npt.assert_array_almost_equal(xdata, np.sort(np.unique(self.df.x)))\n", "    def test_regplot_color(self):\n\n        f, ax = plt.subplots()\n        color = \"#3498db\"\n        ax = lm.regplot(x=\"x\", y=\"y\", data=self.df, color=color)\n        assert ax.collections[0].get_facecolors()[0, :3].tolist() == mpl.colors.hex2color(color)\n\n        f, ax = plt.subplots()\n        color = (0.2, 0.5, 0.7, 0.8)\n        ax = lm.regplot(x=\"x\", y=\"y\", data=self.df, color=color)\n        npt.assert_array_almost_equal(ax.collections[0].get_facecolors()[0, :4], color)\n\n        f, ax = plt.subplots()\n        color = mpl.colors.CSS4_COLORS[\"orange\"]\n        ax = lm.regplot(x=\"x\", y=\"y\", data=self.df, color=color)\n        npt.assert_array_almost_equal(ax.collections[0].get_facecolors()[0, :3],\n                                     mpl.colors.to_rgb(color))\n\n        f, ax = plt.subplots()\n        color = np.array([[0.2, 0.5, 0.7, 0.8]])\n        ax = lm.regplot(x=\"x\", y=\"y\", data=self.df, color=color)\n        npt.assert_array_almost_equal(ax.collections[0].get_facecolors()[0, :4], color[0])\n\n        f, ax = plt.subplots()\n        color = np.array([[0.2, 0.5, 0.7]])\n        ax = lm.regplot(x=\"x\", y=\"y\", data=self.df, color=color)\n        npt.assert_array_almost_equal(ax.collections[0].get_facecolors()[0, :3], color[0])\n\n        f, ax = plt.subplots()\n        color = ['r', 'g', 'b']\n        ax = lm.regplot(x=\"x\", y=\"y\", data=self.df, color=color)\n        npt.assert_array_almost_equal(ax.collections[0].get_facecolors()[0, :3],\n                                     mpl.colors.to_rgb(color[0]))\n"], "sample_1094": ["def test_find():\n    x = symbols('x')\n    expr = sin(x) + cos(x)\n    assert expr.find(sin) == {sin(x)}\n    assert expr.find(cos) == {cos(x)}\n    assert expr.find(x) == {x}\n    assert expr.find(S) == set()\n    assert expr.find(Basic) == {sin(x), cos(x)}\n    assert expr.find(sin, group=True) == {sin(x): 1}\n    assert expr.find(cos, group=True) == {cos(x): 1}\n    assert expr.find(x, group=True) == {x: 2}\n    assert expr.find(S, group=True) == {}\n    assert expr.find(Basic, group=True) == {sin(x): 1, cos(x): 1}\n", "def test_sort_key():\n    x, y = symbols('x y')\n    assert sin(x).sort_key() == (10, (1, (Symbol, ('sin',), {})), S.One.sort_key(), S.One)\n    assert (x + y).sort_key() == (11, (2, (Symbol, ('x',), {}), (Symbol, ('y',), {})), S.One.sort_key(), S.One)\n    assert (x + y).sort_key(order='lex') == (11, (2, (Symbol, ('x',), {}), (Symbol, ('y',), {})), S.One.sort_key(), S.One)\n    assert (x + y).sort_key(order='revlex') == (11, (2, (Symbol, ('y',), {}), (Symbol, ('x',), {})), S.One.sort_key(), S.One)\n    raises(ValueError, lambda: (x + y).sort_key(order='invalid'))\n", "def test_preorder_traversal_with_highly_nested_iterables():\n    expr = (x + y)*(z + (x + y)*(w + (x + y)*(t + (x + y)*(x + y))))\n    expected = [expr, x + y, z, x + y, w, x + y, t, x + y, x + y, x, y, x, y, x, y, x, y, x, y, x, y]\n    assert list(preorder_traversal(expr)) == expected\n", "def test_replace_multiple_wildcards():\n    from sympy import Wild\n    a, b, c, d = Wild('a b c d')\n    x, y, z = symbols('x y z')\n    f = 2*x**2 + 3*y + z\n    g = 3*x + 4*y - z\n    h = 2*x**2 + 3*y - z\n    assert f.replace(a*x**2 + b*y + c*z, d) == f.subs({a: 2, b: 3, c: 1, d: g})\n    assert f.replace(a*x**2 + b*y + c*z, d, exact=False) == f.subs({a: 2, b: 3, c: 1, d: g})\n    assert f.replace(a*x**2 + b*y + c*z, d, exact=True) == f.subs({a: 2, b: 3, c: 1, d: g})\n    assert f.replace(a*x**2 + b*y + c*z, d, exact=False).atoms(Wild) == set()\n    assert f.replace(a*x**2 + b*y + c*z, d, exact=True).atoms(Wild) == set()\n    assert f.replace(a*x**2 + b*y + c*z, d, exact=False) == h\n    assert f.replace(a*x**2 + b*y + c*z, d, exact=True) == h\n", "def test_replace_edge_cases():\n    from sympy import Wild\n    x, y, z = symbols('x y z')\n    e = x + y + z\n    p, q, r = symbols('p q r')\n    a = Wild('a')\n    b = Wild('b')\n\n    raises(ValueError, lambda: e.replace('a', 1))\n\n    # check that the replacement does not hang when replacing\n    # the same thing with itself (issue 10764)\n    assert e.replace(e, e) == e\n\n    # test non-atom replacement\n    assert e.replace(x + y, 1) == z + 1\n\n    # test replacement with _query=True\n    assert e.replace(lambda p: p.is_Mul, lambda p: p**2) == x**2 + y**2 + z**2\n\n    # test replacement with _query=True and multiple args\n    assert e.replace(lambda p: p.is_Mul, lambda p, q: p*q**2) == x*y**2 + y*z**2 + z*x**2\n\n    # test replacement with _query=False\n    assert e.replace(lambda p: p.is_Mul, 1) == x + y + z\n\n    # test replacement with _query=False and multiple args\n    assert e.replace(lambda p, q: p.is_Mul, lambda p, q: p*q**2) == x + y + z\n\n    # test replacement with pattern that matches self\n    assert e.replace(a, a + 1) == (x + 1) + (y + 1) + (z + 1)\n\n    # test replacement with a pattern that matches multiple times\n    assert e.replace(x + y, p + q) == p + q + z\n\n    # test replacement with a pattern that has multiple replacements\n    # for the same symbol (issue 17681)\n    assert e.replace({x + y: p + q, x + z: r + p}, simultaneous=True) == r + p + p + q\n    assert e.replace({x + y: p + q, x + z: r + p}) == p + q + r + p\n\n    # test replacement of a dummy variable\n    f = Function('f')\n    assert f(x + y).replace(x + y, p + q) == f(p + q)\n    assert f(x + y).replace(x"], "sample_1067": ["def test_issue_negatives():\n    x = Symbol('x')\n    a = Wild('a')\n\n    e = -(x**2)\n    assert e.match(-a) == {a: x**2}\n    assert e.match(a) == {a: -x**2}\n", "def test_match_mul_with_powers():\n    x = Symbol('x')\n    p, q = map(Wild, 'pq')\n    e = x**2 * x**3\n    assert e.match(p**q) == {p: x, q: 5}\n    e = x**2 * 2**3\n    assert e.match(p**q) == {p: 8*x**2}\n", "def test_issue_5332():\n    from sympy import S, Mul\n    x = S.Symbol('x')\n    y = S.Symbol('y')\n    a = S.Wild('a', exclude=[x])\n    b = S.Wild('b', exclude=[y])\n\n    e = S.Mul(x, y)\n    assert e.match(S.Mul(a, b)) == {a: x, b: y}\n", "def test_mul_complex_and_imaginary():\n    a, b, c = map(Symbol, 'abc')\n    x, y = map(Wild, 'xy')\n\n    e = (2 + 3*I)\n    assert e.match(x + y*I) == {x: 2, y: 3}\n    e = (2*I + 3)\n    assert e.match(x + y*I) == {x: 3, y: 2}\n    e = (2 + 3*I)*a\n    assert e.match((x + y*I)*a) == {x: 2, y: 3}\n    e = (2*I + 3)*a\n    assert e.match((x + y*I)*a) == {x: 3, y: 2}\n    e = (2 + 3*I)*(a + b)\n    assert e.match((x + y*I)*(a + b)) in [{x: 2, y: 3}, {x: 3, y: 2}]\n    e = (2 + 3*I)*(a + b)\n    assert e.match((x + y*I)*(a + b)) in [{x: 2, y: 3}, {x: 3, y: 2}]\n    e = (2*I + 3)*(a + b)\n    assert e.match((x + y*I)*(a + b)) in [{x: 3, y: 2}, {x: 2, y: 3}]\n", "def test_mul_noncommutative_wildcard():\n    x, y = symbols('x y')\n    A, B, C = symbols('A B C', commutative=False)\n    w = symbols('w', cls=Wild, commutative=False)\n    p = symbols('p', cls=Wild)\n\n    assert (w*w*B*w).matches(A*B*A*B) == {w: A}\n    assert (w*B*w*w).matches(A*B*A*B) is None\n    assert (w*w*w*B).matches(A*B*A*B) is None\n    assert (w*w*w).matches(A*B*A*B) is None\n\n    assert (w*B*w*B*w).matches(A*B*A*B*A*B) in [{w: A}, {w: A*B}]\n    assert (w*w*B*w*B*w).matches(A*B*A*B*A*B) in [{w: A}, {w: A*B}]\n    assert (w*w*w*B*w*B).matches(A*B*A*B*A*B) in [{w: A}, {w: A*B}]\n    assert (w*w*w*w*B).matches(A*B*A*B*A*B) in [{w: A*B}, {w: A*B*A}]\n\n    assert (w**2*B*w).matches(A**2*B*A) == {w: A}\n    assert (w*B*w**2).matches(A*B*A**2) == {w: A}\n    assert (w*w*B*w**2).matches(A*B*A**2) is None\n    assert (w**2*w*B*w).matches(A**2*B*A) is None\n\n    assert (w*w*w*B*w).matches(A*B*A*B) is None\n    assert (w*w*B*w*w).matches(A*B*A*B) is None\n    assert (w*B*w*w*w).matches(A*B*A*B) is None\n    assert (w*w*w*w*B).matches(A*B*A*B) is None\n\n    assert (w*B*p*w*B).matches(A*B*p*B*A*B) == {w: A}\n"], "sample_1123": ["def test_CondSet_properties():\n    x = Symbol('x')\n    c = ConditionSet(x, x > 0, S.Reals)\n    assert c.sym == x\n    assert c.condition == x > 0\n    assert c.base_set == S.Reals\n    assert c.free_symbols == {x}\n", "def test_CondSet_base_set():\n    C = ConditionSet\n    I = S.Integers\n    assert C(x, x < 1).base_set == S.UniversalSet\n    assert C(x, x < 1, I).base_set == I\n    assert C(x, x < 1, FiniteSet(1, 2, 3)).base_set == FiniteSet(1, 2, 3)\n    assert C(x, x < 1, Union(I, FiniteSet(1, 2, 3))).base_set == Union(I, FiniteSet(1, 2, 3))\n", "def test_CondSet_sym():\n    c = ConditionSet(x, x < 1, S.Integers)\n    assert c.sym == x\n    c = ConditionSet(x, x < y, S.Integers)\n    assert c.sym == x\n    c = ConditionSet(y, x < y, S.Integers)\n    assert c.sym == y\n    c = ConditionSet(x, x < y, ConditionSet(y, x < 1, S.Integers))\n    assert c.sym == x\n    c = ConditionSet(y, x < 1, ConditionSet(x, y < 1, S.Integers))\n    assert c.sym != x\n    assert c.sym != y\n    assert c == ConditionSet(c.sym, (c.sym < y) & (x < 1), S.Integers)\n", "def test_CondSet_simplified_FiniteSet_union():\n    assert (ConditionSet(x, And(x < 1, x > -3), FiniteSet(0, 1, 2)) ==\n        Union(FiniteSet(0), ConditionSet(x, And(x < 1, x > -3), FiniteSet(1, 2))))\n    assert (ConditionSet(x, x < 0, Union(FiniteSet(0), FiniteSet(1))) ==\n        EmptySet)\n    assert (ConditionSet(x, x > 0, Union(FiniteSet(0), FiniteSet(1))) ==\n        FiniteSet(1))\n    assert (ConditionSet(x, And(x < 0, x > 0), Union(FiniteSet(0), FiniteSet(1))) ==\n        EmptySet)\n", "def test_CondSet_empty_base_set():\n    assert ConditionSet(x, x < 1, EmptySet) == EmptySet\n    assert ConditionSet(x, x < 1, S.EmptySet) == S.EmptySet\n    assert ConditionSet(x, x < 1, FiniteSet()) == FiniteSet()\n    assert ConditionSet(x, x < 1, Union(EmptySet, EmptySet)) == EmptySet\n"], "sample_1002": ["def test_as_coeff_Mul():\n    assert S(5).as_coeff_Mul() == (1, 5)\n    assert (-S(5)).as_coeff_Mul() == (-1, 5)\n    assert S.Half.as_coeff_Mul() == (1, S.Half)\n    assert (-S.Half).as_coeff_Mul() == (-1, S.Half)\n    assert (3*S.Half).as_coeff_Mul() == (3, S.Half)\n    assert (-3*S.Half).as_coeff_Mul() == (-3, S.Half)\n    assert (S.Half + S(3)).as_coeff_Mul() == (1, S.Half + S(3))\n    assert (S.Half - S(3)).as_coeff_Mul() == (1, S.Half - S(3))\n", "def test_issue_12454():\n    assert Rational('3/2').evalf(10) == Rational('3/2')\n    assert Rational('3/2').evalf(20) == Rational('3/2')\n    assert Rational('3/2').evalf(30) == Rational('3/2')\n    assert Rational('3/2').evalf(40) == Rational('3/2')\n    assert Rational('3/2').evalf(50) == Rational('3/2')\n    assert Rational('3/2').evalf(60) == Rational('3/2')\n    assert Rational('3/2').evalf(70) == Rational('3/2')\n    assert Rational('3/2').evalf(80) == Rational('3/2')\n    assert Rational('3/2').evalf(90) == Rational('3/2')\n    assert Rational('3/2').evalf(100) == Rational('3/2')\n", "def test_issue_17048():\n    assert Integer(0).factors(limit=1) == {1: 1}\n    assert Integer(1).factors(limit=1) == {1: 1}\n    assert Integer(-1).factors(limit=1) == {1: 1}\n    assert Integer(2).factors(limit=1) == {2: 1}\n    assert Integer(-2).factors(limit=1) == {2: 1}\n    assert Integer(3).factors(limit=2) == {3: 1}\n    assert Integer(-3).factors(limit=2) == {3: 1}\n    assert Integer(4).factors(limit=2) == {2: 2}\n    assert Integer(-4).factors(limit=2) == {2: 2}\n    assert Integer(5).factors(limit=3) == {5: 1}\n    assert Integer(-5).factors(limit=3) == {5: 1}\n    assert Integer(6).factors(limit=3) == {2: 1, 3: 1}\n    assert Integer(-6).factors(limit=3) == {2: 1, 3: 1}\n    assert Integer(8).factors(limit=3) == {2: 3}\n    assert Integer(-8).factors(limit=3) == {2: 3}\n    assert Integer(9).factors(limit=4) == {3: 2}\n    assert Integer(-9).factors(limit=4) == {3: 2}\n    assert Integer(10).factors(limit=5) == {2: 1, 5: 1}\n    assert Integer(-10).factors(limit=5) == {2: 1, 5: 1}\n    assert Integer(12).factors(limit=6) == {2: 2, 3: 1}\n    assert Integer(-12).factors(limit=6) == {2: 2, 3: 1}\n    assert Integer(13).factors(limit=7) == {13: 1}\n    assert Integer(-13).factors(limit=7) == {13: 1}\n    assert Integer(14).factors(limit=7) == {2: 1, 7: 1}\n    assert Integer(-", "def test_mpf_norm():\n    from mpmath.libmp import fnan, finf, fninf, fzero\n    from mpmath.libmp.backend import MPZ\n    assert mpf_norm((0, MPZ(0), 0, 0), 10) == fzero\n    assert mpf_norm((0, MPZ(0), -10, 0), 10) == fzero\n    assert mpf_norm((0, MPZ(0), -5, 10), 10) == fzero\n    assert mpf_norm((0, MPZ(0), -5, 5), 10) == fzero\n    assert mpf_norm((0, MPZ(1), 0, 0), 10) == fzero\n    assert mpf_norm((0, MPZ(0), -123, -1), 10) == fnan\n    assert mpf_norm((0, MPZ(0), -456, -2), 10) == finf\n    assert mpf_norm((1, MPZ(0), -789, -3), 10) == fninf\n", "def test_Float_comp():\n    assert comp(Float(0.1, 10), Float(0.1, 5), 10**-5) is False\n    assert comp(Float(0.1, 10), Float(0.1, 5), 10**-6)\n    assert comp(Float(0.1, 10), Float(0.1, 10), 10**-10)\n    assert comp(Float(1.1, 10), Float(1.2, 10), 10**-1)\n    assert comp(Float(0.1, 10), '0.1', 10**-10)\n    assert comp(Float(0.1, 10), '0.1', 10**-1)\n"], "sample_564": ["def test_sharez(fig_test, fig_ref):\n    ax1 = fig_test.add_subplot(121, projection=\"3d\")\n    ax2 = fig_test.add_subplot(122, projection=\"3d\", sharez=ax1)\n\n    ax1.plot([0, 1], [0, 1], [0, 1])\n    ax1.set_zlim(0, 2)\n    ax1.set_zticks([0.5, 1, 1.5])\n\n    ax2.plot([0, 1], [0, 1], [0, 1])\n\n    ax3 = fig_ref.add_subplot(121, projection=\"3d\")\n    ax4 = fig_ref.add_subplot(122, projection=\"3d\")\n    ax3.plot([0, 1], [0, 1], [0, 1])\n    ax3.set_zlim(0, 2)\n    ax3.set_zticks([0.5, 1, 1.5])\n    ax4.plot([0, 1], [0, 1], [0, 1])\n    ax4.set_zlim(0, 2)\n    ax4.set_zticks([0.5, 1, 1.5])\n", "def test_pathpatch_2d_to_3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    path = Path.unit_rectangle()\n    patch = PathPatch(path, facecolor='blue', alpha=0.5)\n    patch = art3d.pathpatch_2d_to_3d(patch, z=0, zdir='z')\n    ax.add_patch(patch)\n    ax.set_xlim(-1, 1)\n    ax.set_ylim(-1, 1)\n    ax.set_zlim(-1, 1)\n", "def test_roll_to_vertical():\n    # Smoke test to see that _roll_to_vertical does not raise\n    # See GH#26351\n    ax = plt.figure().add_subplot(projection='3d')\n    M = np.array([1, 2, 3])\n    ax._roll_to_vertical(M)\n", "def test_voxel_colors_surface_shading(fig_test, fig_ref):\n    x, y, z = np.indices((10, 10, 10))\n    v = (x == y) | (y == z)\n\n    ax_test = fig_test.add_subplot(projection='3d')\n    ax_test.voxels(v, facecolors=np.random.rand(*v.shape, 3))\n\n    ax_ref = fig_ref.add_subplot(projection='3d')\n    ax_ref.voxels(v, facecolors=np.random.rand(*v.shape, 3), shade=False)\n    ax_ref.voxels(v, facecolors=np.random.rand(*v.shape, 3), shade=True)\n", "def test_dont_draw_on_empty_collections(fig_test, fig_ref):\n    \"\"\"Regression test for GH#24673.\"\"\"\n    ax_test = fig_test.add_subplot(projection='3d')\n    ax_test.add_collection3d(art3d.Poly3DCollection([]))\n    ax_test.add_collection3d(art3d.Line3DCollection([]))\n    fig_test.canvas.draw()\n\n    ax_ref = fig_ref.add_subplot(projection='3d')\n    fig_ref.canvas.draw()\n"], "sample_1080": ["def test_refine_edge_cases():\n    assert refine(Abs(x), Q.nonzero(x) & ~Q.positive(x) & ~Q.negative(x)) == Abs(x)\n    assert refine((-1)**x, Q.integer(x) & ~Q.even(x) & ~Q.odd(x)) == (-1)**x\n    assert refine(atan2(y, x), Q.real(y) & Q.real(x) & ~Q.positive(x) & ~Q.negative(x)) == atan2(y, x)\n    assert refine(sign(x), Q.real(x) & ~Q.positive(x) & ~Q.negative(x) & ~Q.zero(x)) == sign(x)\n    assert refine(re(x), Q.complex(x) & ~Q.real(x) & ~Q.imaginary(x)) == re(x)\n    assert refine(im(x), Q.complex(x) & ~Q.real(x) & ~Q.imaginary(x)) == im(x)\n", "def test_refine_handlers_dict():\n    assert refine_abs(Abs(x), Q.real(x)) == Abs(x)\n    assert refine_Pow((-1)**x, Q.real(x)) == (-1)**x\n    assert refine_atan2(atan2(y, x), Q.real(y) & Q.positive(x)) == atan2(y, x)\n    assert refine_Relational(x < 0, ~Q.is_true(x < 0)) is False\n    assert refine_re(re(x), Q.real(x)) == x\n    assert refine_im(im(x), Q.real(x)) == 0\n    assert refine_sign(sign(x), Q.positive(x)) == 1\n", "def test_refine_handler():\n    # Test that refine uses the handlers_dict to refine expressions\n    from sympy.abc import x, y\n    assert refine(Abs(x), Q.real(x)) == x**2\n    assert refine(Pow(x, 2), Q.real(x)) == x**2\n    assert refine(atan2(y, x), Q.real(y) & Q.positive(x)) == atan(y/x)\n    assert refine(re(x), Q.real(x)) == x\n    assert refine(im(x), Q.imaginary(x)) == -I*x\n    assert refine(sign(x), Q.positive(x)) == 1\n", "def test_refine_with_nonstandard_assumptions():\n    assert refine(Abs(x), Q.real(x) & Q.is_nonzero(x)) == Abs(x)\n    assert refine(Abs(x), Q.real(x) & Q.is_zero(x)) == x\n    assert refine(Abs(x), Q.imaginary(x) & Q.is_nonzero(x)) == Abs(x)\n    assert refine(Abs(x), Q.imaginary(x) & Q.is_zero(x)) == x\n\n    assert refine((-1)**x, Q.is_integer(x) & Q.is_even(x)) == 1\n    assert refine((-1)**x, Q.is_integer(x) & Q.is_odd(x)) == -1\n    assert refine((-1)**x, Q.is_integer(x) & Q.is_nonzero(x)) == (-1)**x\n    assert refine((-1)**x, Q.is_integer(x) & Q.is_zero(x)) == 1\n\n    assert refine(sign(x), Q.is_integer(x) & Q.is_nonzero(x)) == sign(x)\n    assert refine(sign(x), Q.is_integer(x) & Q.is_zero(x)) == 0\n", "def test_refine_recursive_call():\n    x = Symbol('x', real=True)\n    y = Symbol('y', positive=True)\n    z = Symbol('z', negative=True)\n    expr = Abs(Abs(x + y) + z)\n    expected_expr = Abs(x + y) - z\n    assert refine(expr) == expected_expr\n\n    expr = Abs(Abs(x + z) + y)\n    expected_expr = Abs(-x + Abs(z)) + y\n    assert refine(expr) == expected_expr\n\n    expr = Abs(Abs(x + z) + Abs(y))\n    expected_expr = Abs(-x + Abs(z)) + y\n    assert refine(expr) == expected_expr\n\n    expr = Abs(Abs(x + y) + Abs(z))\n    expected_expr = Abs(x + y) - z\n    assert refine(expr) == expected_expr\n\n    expr = Abs(Abs(x - y) + Abs(z))\n    expected_expr = Abs(Abs(x - y) - z)\n    assert refine(expr) == expected_expr\n\n    expr = Abs(Abs(x - z) + Abs(y))\n    expected_expr = -x + Abs(z) + y\n    assert refine(expr) == expected_expr\n\n    expr = Abs(Abs(x - z) - Abs(y))\n    expected_expr = Abs(Abs(x - z) - y)\n    assert refine(expr) == expected_expr\n"], "sample_230": ["def test_bounded_field(self):\n    class JSONForm(Form):\n        json_field = JSONField()\n\n    form = JSONForm()\n    bound_field = form['json_field']\n    self.assertIsInstance(bound_field, BoundField)\n", "def test_bound_data(self):\n    field = JSONField()\n    data = '{\"a\": 1}'\n    initial = {\"a\": 2}\n    self.assertEqual(field.bound_data(data, initial), data)\n\n    field = JSONField(disabled=True)\n    data = '{\"a\": 1}'\n    initial = '{\"b\": 2}'\n    self.assertEqual(field.bound_data(data, initial), initial)\n", "def test_invalid_json_input(self):\n    field = JSONField()\n    with self.assertRaisesMessage(ValidationError, 'Enter a valid JSON.'):\n        field.clean('\"foo')\n\n    with self.assertRaisesMessage(ValidationError, 'Enter a valid JSON.'):\n        field.clean('{foo: 1}')\n\n    with self.assertRaisesMessage(ValidationError, 'Enter a valid JSON.'):\n        field.clean('[1, 2, 3,]')\n", "def test_uuid(self):\n    field = JSONField()\n    value = {'uuid': str(uuid.uuid4())}\n    self.assertEqual(field.prepare_value(value), json.dumps(value))\n    self.assertEqual(field.clean(field.prepare_value(value)), value)\n\n    field = JSONField(required=False)\n    self.assertIsNone(field.clean('null'))\n\n    field = JSONField(required=False)\n    self.assertIsNone(field.clean(None))\n", "def test_bound_data(self):\n    class JSONForm(Form):\n        json_field = JSONField(required=False)\n\n    form = JSONForm({'json_field': '{\"a\": 1, \"b\": 2}'}, initial={'json_field': '[\"foo\"]'})\n    field = form.fields['json_field']\n    data = field.bound_data(form['json_field'].data, form.initial['json_field'])\n    self.assertEqual(data, {'a': 1, 'b': 2})\n\n    # Test with disabled field\n    class JSONFormDisabled(Form):\n        json_field = JSONField(required=False, disabled=True)\n\n    form = JSONFormDisabled({'json_field': '{\"a\": 1, \"b\": 2}'}, initial={'json_field': '[\"foo\"]'})\n    field = form.fields['json_field']\n    data = field.bound_data(form['json_field'].data, form.initial['json_field'])\n    self.assertEqual(data, ['foo'])\n"], "sample_806": ["def test_gradient_boosting_with_sample_weight():\n    # Test GradientBoostingRegressor with sample weights\n    X, y = datasets.make_regression(n_samples=100, random_state=42)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n    sample_weight = np.random.RandomState(42).rand(len(X_train))\n\n    gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, \n                                    max_depth=3, random_state=42)\n    gbr.fit(X_train, y_train)\n    y_pred_default = gbr.predict(X_test)\n\n    gbr.fit(X_train, y_train, sample_weight=sample_weight)\n    y_pred_sample_weight = gbr.predict(X_test)\n\n    assert_array_almost_equal(y_pred_default, y_pred_sample_weight)\n", "def test_gradient_boosting_max_features_not_int_or_str():\n    # Check that max_features is either an int, float or str\n\n    X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)\n\n    # None is a valid value for max_features\n    gb = GradientBoostingClassifier(n_estimators=100, max_features=None)\n    gb.fit(X, y)\n\n    # Not a valid value for max_features\n    gb = GradientBoostingClassifier(n_estimators=100, max_features=[1, 2, 3])\n    with pytest.raises(ValueError, match=\"max_features must be int, float or str\"):\n        gb.fit(X, y)\n", "def test_init_check(GBEstimator):\n    # Test if init is checked properly.\n    from sklearn.linear_model import LogisticRegression\n\n    # should fail because the init does not support the required methods\n    if GBEstimator == GradientBoostingRegressor:\n        init = LogisticRegression()\n        assert_raise_message(ValueError,\n                             \"The init parameter must be a valid estimator \"\n                             \"and support both fit and predict\",\n                             GBEstimator(n_estimators=10, init=init).fit,\n                             X, y)\n    else:\n        init = LinearRegression()\n        assert_raise_message(ValueError,\n                             \"The init parameter must be a valid estimator \"\n                             \"and support both fit and predict_proba\",\n                             GBEstimator(n_estimators=10, init=init).fit,\n                             X, y)\n", "def test_gradient_boosting_multiclass_oob_improvement():\n    # Test if out-of-bag improvement is properly computed in multiclass case.\n\n    # Create a multiclass classification dataset\n    X, y = datasets.make_classification(n_samples=100, n_features=5,\n                                        n_informative=3, n_redundant=0,\n                                        n_classes=3, random_state=1)\n\n    gbc = GradientBoostingClassifier(n_estimators=50, max_depth=1,\n                                     subsample=0.5, random_state=1)\n    gbc.fit(X, y)\n\n    # Compute the number of classes\n    n_classes = np.max(y) + 1\n\n    # Check that the shape of oob_improvement_ is correct\n    assert_equal(gbc.oob_improvement_.shape[0], gbc.n_estimators)\n    assert_equal(gbc.oob_improvement_.shape[1], n_classes)\n\n    # Check that the oob improvement is non-negative\n    assert np.all(gbc.oob_improvement_ >= 0)\n", "def test_gradient_boosting_reproducibility(Cls):\n    # Test that GradientBoostingRegressor and GradientBoostingClassifier\n    # are reproducible. See issue #10739\n    X, y = datasets.make_regression(n_samples=10, random_state=42)\n    if Cls == GradientBoostingClassifier:\n        y = np.sign(y)\n\n    # fit with fixed random state\n    est = Cls(n_estimators=10, random_state=42).fit(X, y)\n    y_pred_1 = est.predict(X)\n\n    # fit again with different random state and then set the random state\n    # of the estimator to the previous state\n    est = Cls(n_estimators=10, random_state=21).fit(X, y)\n    est.estimators_ = clone(est.estimators_)\n    est.random_state = 42\n    y_pred_2 = est.predict(X)\n\n    assert_array_almost_equal(y_pred_1, y_pred_2)\n"], "sample_47": ["    def test_technical_404_response(self):\n        \"\"\"\n        Test the technical 404 error response.\n        \"\"\"\n        request = RequestFactory().get('/test_view/')\n        exception = Http404('Test 404 error')\n        response = technical_404_response(request, exception)\n        self.assertEqual(response.status_code, 404)\n        self.assertContains(response, 'Raised by:', status_code=404)\n        self.assertContains(response, 'view_tests.tests.test_views.technical_404_response', status_code=404)\n", "    def test_no_request_with_exception(self):\n        try:\n            raise ValueError(\"Can't find my keys\")\n        except ValueError:\n            exc_type, exc_value, tb = sys.exc_info()\n        response = technical_500_response(None, exc_type, exc_value, tb)\n        self.assertEqual(response.status_code, 500)\n        self.assertContains(response, 'ValueError', status_code=500)\n        self.assertContains(response, \"Can't find my keys\", status_code=500)\n        self.assertNotIn('Request Method:', response.content.decode())\n        self.assertNotIn('Request URL:', response.content.decode())\n        self.assertNotIn('USER:', response.content.decode())\n        self.assertIn('Exception Type:', response.content.decode())\n        self.assertIn('Exception Value:', response.content.decode())\n        self.assertIn('Traceback:', response.content.decode())\n        self.assertIn('Request data not supplied', response.content.decode())\n", "    def test_technical_500_response(self):\n        request = RequestFactory().get('/test_view/')\n        exc_type = ValueError\n        exc_value = ValueError('Can\\'t find my keys')\n        tb = sys.exc_info()[2]\n        response = technical_500_response(request, exc_type, exc_value, tb)\n        self.assertEqual(response.status_code, 500)\n", "    def test_technical_404_response_template(self):\n        # Test technical_404_response with a technical 404 template.\n        request = RequestFactory().get('/technical404/')\n        exc = Resolver404()\n        exc.args = ({'path': '/technical404/'}, [], [])\n        response = technical_404_response(request, exc)\n        self.assertContains(response, \"Raised by:\", status_code=404)\n        self.assertContains(response, \"view_tests.views.technical404\", status_code=404)\n", "    def test_non_sensitive_multivalue_dict(self):\n        request = self.rf.get('/test_view/', HTTP_HOST='example.com')\n        request.user = User()\n        try:\n            raise ValueError(\"Can't find my keys\")\n        except ValueError:\n            exc_type, exc_value, tb = sys.exc_info()\n\n        # Create a non-sensitive MultiValueDict\n        multivalue_dict = MultiValueDict({'key': ['value']})\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n        reporter.filter.get_traceback_frame_variables = lambda request, tb_frame: [\n            ('multivalue_dict', multivalue_dict)\n        ]\n        html = reporter.get_traceback_html()\n        self.assertIn('value', html)\n"], "sample_875": ["def test_precision_recall_fscore_support_empty_labels():\n    y_true = np.array([[0, 1, 1], [1, 0, 1]])\n    y_pred = np.array([[1, 1, 1], [1, 0, 1]])\n    labels = []\n    with pytest.raises(ValueError, match=\"'labels' should contains at least one label.\"):\n        precision_recall_fscore_support(y_true, y_pred, average=None, labels=labels)\n    with pytest.raises(ValueError, match=\"'labels' should contains at least one label.\"):\n        precision_recall_fscore_support(y_true, y_pred, average='macro', labels=labels)\n    with pytest.raises(ValueError, match=\"'labels' should contains at least one label.\"):\n        precision_recall_fscore_support(y_true, y_pred, average='weighted', labels=labels)\n    with pytest.raises(ValueError, match=\"'labels' should contains at least one label.\"):\n        precision_recall_fscore_support(y_true, y_pred, average='samples', labels=labels)\n    with pytest.raises(ValueError, match=\"'labels' should contains at least one label.\"):\n        precision_recall_fscore_support(y_true, y_pred, average='micro', labels=labels)\n", "def test_jaccard_score_weighted():\n    # Test weighted Jaccard score\n    y_true = np.array([0, 1, 2])\n    y_pred = np.array([0, 1, 1])\n    sample_weight = np.array([0.1, 0.3, 0.6])\n\n    y_true_bin = label_binarize(y_true, np.unique(y_true))\n    y_pred_bin = label_binarize(y_pred, np.unique(y_true))\n    MCM = multilabel_confusion_matrix(\n        y_true_bin, y_pred_bin, sample_weight=sample_weight, samplewise=False\n    )\n    numerator = MCM[:, 1, 1]\n    denominator = MCM[:, 1, 1] + MCM[:, 0, 1] + MCM[:, 1, 0]\n\n    jaccard_weighted = np.average(numerator / denominator, weights=sample_weight)\n    assert_almost_equal(\n        jaccard_score(y_true, y_pred, average=\"weighted\", sample_weight=sample_weight),\n        jaccard_weighted,\n    )\n", "def test_multilabel_jaccard_score_samples_average():\n    # Test multilabel jaccard score with sample average\n    y_true = np.array([[1, 0, 1, 0], [0, 1, 1, 1], [0, 0, 0, 1]])\n    y_pred = np.array([[1, 0, 0, 0], [0, 1, 1, 0], [1, 0, 0, 1]])\n    assert_almost_equal(jaccard_score(y_true, y_pred, average=\"samples\"), 0.5555555555555555)\n    assert_almost_equal(jaccard_score(y_true, y_true, average=\"samples\"), 1.0)\n    assert_almost_equal(jaccard_score(y_pred, y_pred, average=\"samples\"), 0.5555555555555555)\n    assert_almost_equal(jaccard_score(y_pred, np.logical_not(y_pred), average=\"samples\"), 0.0)\n    assert_almost_equal(jaccard_score(y_true, np.logical_not(y_true), average=\"samples\"), 0.0)\n    assert_almost_equal(jaccard_score(y_true, np.zeros_like(y_true), average=\"samples\"), 0.0)\n    assert_almost_equal(jaccard_score(y_pred, np.zeros_like(y_pred), average=\"samples\"), 0.0)\n", "def test_jaccard_score_pos_label():\n    # Check that the Jaccard score is correctly calculated when the positive\n    # label is specified.\n\n    y_true = [0, 1, 1, 0, 1, 1]\n    y_pred = [0, 1, 1, 0, 0, 1]\n\n    # pos_label=1 (default)\n    assert jaccard_score(y_true, y_pred, average=\"binary\") == 2/3\n\n    # pos_label=0\n    assert jaccard_score(y_true, y_pred, average=\"binary\", pos_label=0) == 1/2\n\n    y_true = np.array([[0, 1], [1, 1]])\n    y_pred = np.array([[0, 1], [1, 0]])\n\n    # pos_label=1 (default)\n    assert jaccard_score(y_true, y_pred, average=\"binary\") == 0.5\n\n    # pos_label=0\n    assert jaccard_score(y_true, y_pred, average=\"binary\", pos_label=0) == 0.5\n", "def test_brier_score_loss_multiclass_error():\n    # ensure to raise an error for multiclass y_true\n    y_true = np.array([0, 1, 2, 0])\n    y_pred = np.array([0.8, 0.6, 0.4, 0.2])\n\n    # try with pos_label=None (error for binary case)\n    y_true_bin = y_true.copy()\n    y_true_bin[y_true_bin != 0] = 1\n    error_message = (\n        \"pos_label must be specified for multiclass target. \"\n        \"Got pos_label=None. Please provide the true labels explicitly \"\n        \"through the labels argument.\"\n    )\n    with pytest.raises(ValueError, match=error_message):\n        brier_score_loss(y_true_bin, y_pred, pos_label=None)\n\n    # try with a non-binary pos_label\n    y_true_bin = y_true.copy()\n    y_true_bin[y_true_bin != 1] = 0\n    error_message = (\n        \"Only binary classification is supported. The type of the target is binary\"\n    )\n    with pytest.raises(ValueError, match=error_message):\n        brier_score_loss(y_true_bin, y_pred, pos_label=2)\n"], "sample_240": ["def test_hash_value_changes_after_password_reset(self):\n    \"\"\"Hash value changes after password reset.\"\"\"\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk1), True)\n    user.set_password('new_password')\n    user.save()\n    self.assertIs(p0.check_token(user, tk1), False)\n", "def test_token_with_different_last_login(self):\n    \"\"\"Updating the user's last login date/time invalidates the token.\"\"\"\n    user = User.objects.create_user('lastloginuser', 'test2@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk1), True)\n    user.last_login = datetime.now() + timedelta(days=1)\n    user.save()\n    self.assertIs(p0.check_token(user, tk1), False)\n", "def test_token_timestamp_tampering(self):\n    \"\"\"\n    Test that a token remains valid even if the timestamp is tampered with.\n    \"\"\"\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    ts_b36, _ = tk1.split(\"-\")\n    ts = base36_to_int(ts_b36)\n    # Tamper with the timestamp\n    tampered_token = f\"{int_to_base36(ts + 1)}-{tk1.split('-')[1]}\"\n    self.assertIs(p0.check_token(user, tampered_token), False)\n", "def test_make_hash_value(self):\n    \"\"\"\n    _make_hash_value() returns a string containing the user's primary key,\n    password, last login timestamp, and email.\n    \"\"\"\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    token_generator = PasswordResetTokenGenerator()\n    timestamp = token_generator._num_seconds(token_generator._now())\n    expected_hash_value = f\"{user.pk}{user.password}{user.last_login}{timestamp}{user.email}\"\n    self.assertEqual(token_generator._make_hash_value(user, timestamp), expected_hash_value)\n    # Test with no last_login and email\n    user.last_login = None\n    user.email = ''\n    user.save()\n    expected_hash_value = f\"{user.pk}{user.password}{timestamp}{user.email}\"\n    self.assertEqual(token_generator._make_hash_value(user, timestamp), expected_hash_value)\n", "    def test_check_token_with_legacy_and_non_legacy_tokens(self):\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        # RemovedInDjango40Warning: when the deprecation ends, remove the\n        # legacy argument and replace with:\n        #   algorithm=self.algorithm,\n        p0.algorithm = 'sha1'\n        legacy_token = p0._make_token_with_timestamp(user, self._num_seconds(self._now()), legacy=True)\n\n        # Test that the non-legacy token is valid\n        self.assertIs(p0.check_token(user, tk1), True)\n        # Test that the legacy token is valid\n        self.assertIs(p0.check_token(user, legacy_token), True)\n        # Test that the non-legacy token is not equal to the legacy token\n        self.assertNotEqual(tk1, legacy_token)\n"], "sample_115": ["    def test_cleansed_multivaluedict(self):\n        filter = SafeExceptionReporterFilter()\n        request = mock.MagicMock()\n        request.META = {'HTTP_ACCEPT': 'text/html'}\n        multivaluedict = MultiValueDict({'username': ['admin'], 'password': ['secret']})\n        request.sensitive_post_parameters = ['password']\n        cleaned_multivaluedict = filter.get_cleansed_multivaluedict(request, multivaluedict)\n        self.assertEqual(cleaned_multivaluedict['username'], ['admin'])\n        self.assertEqual(cleaned_multivaluedict['password'], ['********************'])\n", "    def test_request_repr_without_meta(self):\n        request = RequestFactory().get('/')\n        request.META = None\n        exc_type, exc_value, tb = sys.exc_info()\n        response = technical_500_response(request, exc_type, exc_value, tb)\n        self.assertEqual(response.status_code, 500)\n", "    def setUp(self):\n        self.rf = RequestFactory()\n", "    def test_technical_404_with_exception(self):\n        request = self.rf.get('/technical404/')\n        exception = Http404(\"Raised by: view_tests.views.technical404\")\n        response = technical_404_response(request, exception)\n        self.assertContains(response, \"Raised by: view_tests.views.technical404\", status_code=404)\n        self.assertContains(response, \"view_tests.views.technical404\", status_code=404)\n", "    def setUp(self):\n        self.factory = RequestFactory()\n        self.request = self.factory.get('/some_path')\n"], "sample_1134": ["def test_latex_determinant():\n    from sympy import MatrixSymbol\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    assert latex(A.det()) == r\"\\det{\\left(A \\right)}\"\n    assert latex(A.det(method=\"lu\")) == r\"\\det{\\left(A \\right)}\"\n    assert latex(A.det(method=\"chol\")) == r\"\\det{\\left(A \\right)}\"\n    assert latex(A.det(method=\"default\")) == r\"\\det{\\left(A \\right)}\"\n    assert latex((A*B).det()) == r\"\\det{\\left(A B \\right)}\"\n", "def test_latex_implied_multiplication():\n    # issue 18894\n    from sympy.abc import x, y\n    assert latex(x*y) == r\"x y\"\n    assert latex(x*y**2) == r\"x y^{2}\"\n    assert latex(x*y*z) == r\"x y z\"\n    assert latex(x*y*z**2) == r\"x y z^{2}\"\n    assert latex(x*y*z**2*t) == r\"x y z^{2} t\"\n", "def test_latex_InfiniteSet():\n    from sympy import Interval, S, symbols\n    from sympy.sets.sets import Union, Intersection, Complement, SymmetricDifference, ProductSet, InfiniteSet\n    from sympy.sets.fancysets import Naturals, Rationals, Reals\n    assert latex(S.UniversalSet) == r\"\\mathbb{U}\"\n    assert latex(S.EmptySet) == r\"\\emptyset\"\n    assert latex(Interval(0,1)) == r\"\\left[0, 1\\right]\"\n    assert latex(Union(Interval(0,1), Interval(1,2))) == r\"\\left[0, 1\\right] \\cup \\left[1, 2\\right]\"\n    assert latex(Intersection(Interval(0,1), Interval(0.5, 1.5))) == r\"\\left[0, 1\\right] \\cap \\left[0.5, 1.5\\right]\"\n    assert latex(Complement(Interval(0, 1), Interval(0.5, 1))) == r\"\\left[0, 1\\right] \\setminus \\left[0.5, 1\\right]\"\n    assert latex(SymmetricDifference(Interval(0, 1), Interval(0.5, 1.5))) == r\"\\left[0, 1\\right] \\triangle \\left[0.5, 1.5\\right]\"\n    assert latex(ProductSet(Interval(0,1), Interval(0,1))) == r\"\\left[0, 1\\right] \\times \\left[0, 1\\right]\"\n    assert latex(Naturals) == r\"\\mathbb{N}\"\n    assert latex(Rationals) == r\"\\mathbb{Q}\"\n    assert latex(Reals) == r\"\\mathbb{R}\"\n    assert latex(InfiniteSet(symbols('x', real=True))) == r\"\\mathbb{R}\"\n    assert latex(InfiniteSet(symbols('x', integer=True))) == r\"\\mathbb{Z}\"\n    assert latex(InfiniteSet(symbols('x', real=True), Eq(symbols('x'), symbols('x')))) == r\"\\mathbb{R}\"\n    assert latex(InfiniteSet(symbols('x', integer=True), Eq(symbols('x'), symbols('x')))) == r\"\\mathbb{Z}\"\n", "def test_latex_modifiers():\n    x, y = symbols('x y')\n\n    assert latex(x**2) == r\"x^{2}\"\n    assert latex(x**2 + y) == r\"x^{2} + y\"\n    assert latex(x*y**2) == r\"x y^{2}\"\n    assert latex(x**2*y**2) == r\"x^{2} y^{2}\"\n\n    assert latex(x**2 + y**2) == r\"x^{2} + y^{2}\"\n    assert latex(x**2*y + y**2) == r\"x^{2} y + y^{2}\"\n    assert latex(x*y**2 + y**2) == r\"x y^{2} + y^{2}\"\n\n    assert latex(x**2*y**2 + y**2) == r\"x^{2} y^{2} + y^{2}\"\n\n    assert latex((x**2 + y**2)**2) == r\"\\left(x^{2} + y^{2}\\right)^{2}\"\n    assert latex((x**2 + y**2)**2 + y**2) == r\"\\left(x^{2} + y^{2}\\right)^{2} + y^{2}\"\n\n    assert latex(x**2 + (y**2 + y)**2) == r\"x^{2} + \\left(y^{2} + y\\right)^{2}\"\n\n    assert latex(x**2 + (y**2 + y)**2 + y) == r\"x^{2} + \\left(y^{2} + y\\right)^{2} + y\"\n\n    assert latex(x**2*y + (y**2 + y)**2 + y) == r\"x^{2} y + \\left(y^{2} + y\\right)^{2} + y\"\n\n    assert latex(x**2 + (y**2 + y)**2 + y**2) == r\"x^{2} + \\left(y^{2} + y\\right)^{2} + y^{2}\"\n\n    assert latex(x**2*y + (y**2 + y)**2 + y**2) == r\"x^{2} y + \\left(y^{2} + y\\right)^{2} + y^{2}\"\n\n    assert latex(x**2 + (y**2 + y)**2 + y**2 + y) == r\"x^{2} + \\left(y^{2} + y", "def test_non_ascii_characters():\n    from sympy import symbols, latex, diff, Eq, Function\n\n    x = symbols('x')\n    y = symbols('y')\n\n    assert latex('\u03c0') == r\"\\pi\"\n    assert latex('\u014c') == r'\\bar{O}'\n    assert latex('\u0100') == r'\\bar{A}'\n\n    f = Function('f')\n\n    assert latex(f(x, y)) == r'f{\\left(x, y \\right)}'\n    assert latex(diff(f(x, y), x)) == r'\\frac{\\partial}{\\partial x} f{\\left(x, y \\right)}'\n\n    g = Function('\u014c')\n\n    assert latex(g(x, y)) == r'\\bar{O}{\\left(x, y \\right)}'\n    assert latex(diff(g(x, y), x)) == r'\\frac{\\partial}{\\partial x} \\bar{O}{\\left(x, y \\right)}'\n\n    eq = Eq(x, y)\n\n    assert latex(eq) == r'x = y'\n"], "sample_172": ["    def test_readonly_fields(self):\n        \"\"\"Readonly fields in inline formsets should have disabled inputs.\"\"\"\n        self.client.force_login(self.superuser)\n        response = self.client.get(reverse('admin:admin_widgets_album_change', args=(self.album.id,)))\n        self.assertContains(\n            response,\n            '<input type=\"text\" name=\"album_set-0-name\" value=\"Hybrid Theory\" '\n            'class=\"vTextField\" id=\"id_album_set-0-name\" readonly>',\n            html=True,\n        )\n", "    def test_get_fields(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            readonly_fields = ('readonly_field',)\n            fields = ('field1', 'field2')\n\n        ma = MyModelAdmin(Member, admin.site)\n        request = self.client.get(reverse('admin:admin_widgets_member_changelist'))\n        self.assertEqual(ma.get_fields(request), ('field1', 'field2', 'readonly_field'))\n", "    def test_model_form_meta_exclusion(self):\n        ma = self.MyModelAdmin(self.MyModel, admin.site)\n        form = ma.get_form(None)\n        self.assertIsInstance(form, self.MyModelForm)\n", "    def setUp(self):\n        super().setUp()\n        for i in range(365):\n            Event.objects.create(\n                name='Event %d' % i,\n                start_date=datetime.now() + timedelta(days=i),\n                start_time=datetime.now().time()\n            )\n", "    def test_inlines(self):\n        self.client.force_login(self.superuser)\n        class SchoolAdmin(admin.ModelAdmin):\n            inlines = [StudentAdmin]\n\n        class StudentAdmin(admin.TabularInline):\n            model = Student\n\n        with self.settings(ROOT_URLCONF='admin_widgets.urls'):\n            ma = SchoolAdmin(School, admin.site)\n            inlines = ma.get_inline_instances(self.client.get(reverse('admin:admin_widgets_school_changelist')), obj=self.school)\n            self.assertEqual(len(inlines), 1)\n"], "sample_123": ["    def test_empty_string(self):\n        self.assertEqual(limited_parse_qsl('', keep_blank_values=False, encoding='utf-8', errors='replace'), [])\n", "    def test_empty_string(self):\n        self.assertEqual(limited_parse_qsl(''), [])\n", "    def test_empty_query_string(self):\n        self.assertEqual(limited_parse_qsl(''), [])\n", "    def test_parse_qs(self):\n        tests = (\n            ('a=1&b=2', [('a', '1'), ('b', '2')]),\n            ('a=1&a=2&a=3', [('a', '1'), ('a', '2'), ('a', '3')]),\n            ('a=1&a=2&a=3', [('a', '1'), ('a', '2'), ('a', '3')], fields_limit=4),\n            ('', []),\n            ('a=', [('a', '')], keep_blank_values=True),\n            ('a=', [], keep_blank_values=False),\n            ('a=1&a=', [('a', '1'), ('a', '')], keep_blank_values=True),\n            ('a=1&a=', [('a', '1')], keep_blank_values=False),\n            ('a=1&a=&b=2', [('a', '1'), ('a', ''), ('b', '2')], keep_blank_values=True),\n            ('a=1&a=&b=2', [('a', '1'), ('b', '2')], keep_blank_values=False),\n            ('%D0%9F%D1%80%D0%B8%D0%B2%D0%B5%D1%82%21=a', [('\u00d0\u0178\u02dc\u0160\u00d0\u00b8\u00d0\u00b2\u00d0\u00b5\u00d1\u201a!', 'a')], encoding='utf-8'),\n            ('%D0%9F%D1%80%D0%B8%D0%B2%D0%B5%D1%82%21=a', [('\u00d0\u0178\u02dc\u0160\u00d0\u00b8\u00d0\u00b2\u00d0\u00b5\u00d1\u201a!', 'a')], encoding='utf-8', errors='replace'),\n        )\n        for qs, expected, **kwargs in tests:\n            with self.subTest(qs=qs):\n                self.assertEqual(limited_parse_qsl(qs, **kwargs), expected)\n", "    def test_empty_string(self):\n        self.assertEqual(limited_parse_qsl(''), [])\n"], "sample_155": ["def test_file_from_disk_with_filename(self):\n    response = FileResponse(open(__file__, 'rb'), filename='test.py')\n    self.assertEqual(response['Content-Length'], str(os.path.getsize(__file__)))\n    self.assertIn(response['Content-Type'], ['text/x-python', 'text/plain'])\n    self.assertEqual(response['Content-Disposition'], 'inline; filename=\"test.py\"')\n    response.close()\n", "def test_file_from_disk_response_with_filename(self):\n    with tempfile.NamedTemporaryFile(suffix='.txt') as tmp:\n        response = FileResponse(tmp, filename='custom_name.txt')\n    self.assertEqual(response['Content-Length'], str(tmp.tell()))\n    self.assertEqual(response['Content-Type'], 'text/plain')\n    self.assertEqual(response['Content-Disposition'], 'inline; filename=\"custom_name.txt\"')\n    response.close()\n", "def test_delete_cookie(self):\n    response = HttpResponse()\n    response.set_cookie('cookie_name', 'cookie_value')\n    self.assertIn('Set-Cookie', response.serialize_headers().decode('utf-8'))\n\n    response.delete_cookie('cookie_name')\n    self.assertIn('Set-Cookie', response.serialize_headers().decode('utf-8'))\n    self.assertEqual(response.cookies['cookie_name']['expires'], 'Thu, 01 Jan 1970 00:00:00 GMT')\n    self.assertEqual(response.cookies['cookie_name']['max-age'], 0)\n\n    response.delete_cookie('cookie_name', path='/path')\n    self.assertEqual(response.cookies['cookie_name']['path'], '/path')\n\n    response.delete_cookie('cookie_name', domain='example.com')\n    self.assertEqual(response.cookies['cookie_name']['domain'], 'example.com')\n\n    response.delete_cookie('cookie_name', samesite='lax')\n    self.assertEqual(response.cookies['cookie_name']['samesite'], 'lax')\n", "def test_custom_filename(self):\n    response = FileResponse(open(__file__, 'rb'), filename='custom_name.py')\n    self.assertEqual(response['Content-Disposition'], 'inline; filename=\"custom_name.py\"')\n", "def test_file_from_disk_with_custom_filename(self):\n    with tempfile.NamedTemporaryFile() as tmp:\n        filename = \"test_file.txt\"\n        response = FileResponse(tmp, filename=filename)\n        self.assertEqual(response['Content-Disposition'], f'inline; filename=\"{filename}\"')\n"], "sample_636": ["def test_similar_checker_process_module(self) -> None:\n    \"\"\"Tests the SimilarChecker's process_module method.\"\"\"\n    checker = SimilarChecker()\n    module = astroid.parse(\"def test():\\n    pass\\ndef test2():\\n    pass\")\n    with pytest.raises(SystemExit) as cm:\n        checker.process_module(module)\n    assert cm.value.code == 0\n", "def test_duplicate_code_raw_strings_duplicate_lines(self) -> None:\n    \"\"\"Tests that similar lines that are duplicated but slightly modified are not reported.\"\"\"\n    path = join(DATA, \"raw_strings_duplicate_lines\")\n    expected_output = \"Similar lines in 2 files\"\n    self._test_output(\n        [path, \"--disable=all\", \"--enable=duplicate-code\"],\n        expected_output=expected_output,\n    )\n", "def test_similar_code_threshold(self) -> None:\n    \"\"\"Test similar lines in 3 similar files with a specific threshold.\"\"\"\n    path = join(DATA, \"similar_code_threshold\")\n    expected_output = \"Similar lines in 2 files\"\n    self._test_output(\n        [path, \"--disable=all\", \"--enable=duplicate-code\", \"--min-similarity-lines=3\"],\n        expected_output=expected_output,\n    )\n", "def test_similar_code_stripped_lines(self) -> None:\n    \"\"\"Test the stripped_lines function.\"\"\"\n    lines = [\n        \"# This is a comment line\",\n        \"import os\",\n        '\"\"\"This is a docstring\"\"\"',\n        \"def foo():\",\n        \"    print('Hello, world!')\",\n        \"    # This is a comment line inside a function\",\n        \"    os.mkdir('new_dir')\",\n        \"    # This is another comment line inside a function\",\n        \"    print('Goodbye, world!')\",\n    ]\n    expected_output = [\n        LineSpecifs(text=\"print('Hello, world!')\", line_number=4),\n        LineSpecifs(text=\"os.mkdir('new_dir')\", line_number=6),\n        LineSpecifs(text=\"print('Goodbye, world!')\", line_number=8),\n    ]\n    output = stripped_lines(lines, ignore_comments=True, ignore_docstrings=True, ignore_imports=True, ignore_signatures=True)\n    self.assertEqual(output, expected_output)\n", "def test_duplicate_code_raw_strings_ignore_imports(self) -> None:\n    \"\"\"Tests the ignore-imports option with duplicate lines in an import block.\"\"\"\n    path = join(DATA, \"raw_strings_ignore_imports\")\n    expected_output = \"Similar lines in 2 files\"\n    self._test_output(\n        [path, \"--disable=all\", \"--enable=duplicate-code\", \"--ignore-imports\"],\n        expected_output=expected_output,\n    )\n"], "sample_912": ["def test_python_domain_filter_meta_fields(app, status, warning):\n    \"\"\"Test filter_meta_fields function.\"\"\"\n    app.builder.build_all()\n\n    doctree = app.env.get_doctree('roles')\n    field_list = list(doctree.traverse(nodes.field_list))\n    assert len(field_list) == 2\n    assert len(field_list[0]) == 1\n    assert len(field_list[1]) == 1\n", "def test_pyfunction_deprecated(app):\n    text = (\".. py:function:: func\\n\"\n            \"   :deprecated:\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"func\"],\n                                                    [desc_parameterlist, ()])],\n                                  [desc_content, ()])]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n\n    assert 'func' in domain.objects\n    assert domain.objects['func'] == ('index', 'func', 'function')\n\n    node = doctree[1][0][0]\n    assert node.astext() == \"func\"\n    assert node.get('reftitle') == \"func (deprecated)\"\n", "def test_domain_py_module_with_platform_and_synopsis(app, status, warning):\n    app.builder.build_all()\n\n    doctree = app.env.get_doctree('module')\n    refnodes = list(doctree.traverse(pending_xref))\n    assert refnodes[0]['reftitle'] == 'module_a.submodule: Unix only, Top Level class.'\n\n    doctree = app.env.get_doctree('module_option')\n    refnodes = list(doctree.traverse(pending_xref))\n    assert refnodes[0]['reftitle'] == 'test.extra: Extra module synopsis.'\n\n    modules = app.env.domains['py'].data['modules']\n    assert modules['module_a.submodule'] == ('module', 'module-module_a.submodule', 'Unix only', 'Top Level class.', False)\n    assert modules['test.extra'] == ('module_option', 'module-test.extra', '', 'Extra module synopsis.', False)\n", "def test_python_domain_merge_domaindata(app):\n    text = (\".. py:module:: module1\\n\"\n            \".. py:module:: module2\\n\")\n    restructuredtext.parse(app, text)\n\n    domain = app.env.get_domain('py')\n    data = domain.data.copy()\n\n    # create new environment with different docname\n    app.env.docname = 'newdoc'\n    restructuredtext.parse(app, text)\n\n    # merge domaindata\n    domain.merge_domaindata([], data)\n\n    # check if data from the first doc has been merged\n    assert domain.modules['module1'][0] == 'index'\n    assert domain.modules['module2'][0] == 'index'\n    assert len(domain.modules) == 2\n\n    # check if data from the second doc has not been overwritten\n    assert len(domain.modules) == 2\n", "def test_domain_py_xref_anchor_parse(app):\n    # test for #85, #87 and #88\n    text = \"\"\""], "sample_347": ["def test_localtime(self):\n    naive = datetime.datetime(2011, 9, 1, 13, 20, 30)\n    with self.assertRaisesMessage(ValueError, 'localtime() cannot be applied to a naive datetime'):\n        timezone.localtime(naive)\n    with self.assertRaisesMessage(ValueError, 'localtime() cannot be applied to a naive datetime'):\n        timezone.localtime(naive, timezone=EAT)\n\n    aware = datetime.datetime(2011, 9, 1, 13, 20, 30, tzinfo=ICT)\n    self.assertEqual(timezone.localtime(aware, timezone=EAT), datetime.datetime(2011, 9, 1, 10, 20, 30, tzinfo=EAT))\n    with timezone.override(EAT):\n        self.assertEqual(timezone.localtime(aware), datetime.datetime(2011, 9, 1, 10, 20, 30, tzinfo=EAT))\n\n    with mock.patch('django.utils.timezone.now', return_value=aware):\n        self.assertEqual(timezone.localtime(timezone=EAT), datetime.datetime(2011, 9, 1, 10, 20, 30, tzinfo=EAT))\n        with timezone.override(EAT):\n            self.assertEqual(timezone.localtime(), datetime.datetime(2011, 9, 1, 10, 20, 30, tzinfo=EAT))\n", "def test_localtime(self):\n    aware = datetime.datetime(2015, 1, 1, 0, 0, 1, tzinfo=ICT)\n    self.assertEqual(timezone.localtime(aware, timezone=EAT), datetime.datetime(2014, 12, 31, 17, 0, 1))\n    with timezone.override(EAT):\n        self.assertEqual(timezone.localtime(aware), datetime.datetime(2014, 12, 31, 17, 0, 1))\n\n    naive = datetime.datetime(2015, 1, 1, 0, 0, 1)\n    with self.assertRaisesMessage(ValueError, 'localtime() cannot be applied to a naive datetime'):\n        timezone.localtime(naive)\n    with self.assertRaisesMessage(ValueError, 'localtime() cannot be applied to a naive datetime'):\n        timezone.localtime(naive, timezone=EAT)\n\n    with mock.patch('django.utils.timezone.now', return_value=aware):\n        self.assertEqual(timezone.localtime(timezone=EAT), datetime.datetime(2014, 12, 31, 17, 0, 1))\n        with timezone.override(EAT):\n            self.assertEqual(timezone.localtime(), datetime.datetime(2014, 12, 31, 17, 0, 1))\n", "def test_localtime(self):\n    aware = datetime.datetime(2011, 9, 1, 13, 20, 30, tzinfo=EAT)\n    naive = datetime.datetime(2011, 9, 1, 13, 20, 30)\n    with self.assertRaisesMessage(ValueError, 'localtime() cannot be applied to a naive datetime'):\n        timezone.localtime(naive)\n    with self.assertRaisesMessage(ValueError, 'localtime() cannot be applied to a naive datetime'):\n        timezone.localtime(naive, timezone=EAT)\n    self.assertEqual(timezone.localtime(aware, timezone=EAT), aware.astimezone(EAT))\n    with timezone.override(EAT):\n        self.assertEqual(timezone.localtime(aware), aware.astimezone(EAT))\n\n    # Test with timezone=None\n    self.assertEqual(timezone.localtime(aware), aware.astimezone(timezone.get_current_timezone()))\n", "def test_template_localtime(self):\n    naive = datetime.datetime(2011, 9, 1, 13, 20, 30)\n    aware = datetime.datetime(2011, 9, 1, 13, 20, 30, tzinfo=EAT)\n\n    # Check that naive datetime is not converted when USE_TZ=False\n    with override_settings(USE_TZ=False):\n        self.assertIs(timezone.template_localtime(naive), naive)\n        self.assertIs(timezone.template_localtime(aware), aware)\n\n    # Check that naive datetime is not converted when USE_TZ=True but use_tz=False\n    with override_settings(USE_TZ=True):\n        self.assertIs(timezone.template_localtime(naive, use_tz=False), naive)\n        self.assertIs(timezone.template_localtime(aware, use_tz=False), aware)\n\n    # Check that naive datetime is not converted when USE_TZ=True and use_tz=None and convert_to_local_time=False\n    with override_settings(USE_TZ=True):\n        self.assertIs(timezone.template_localtime(naive, use_tz=None, convert_to_local_time=False), naive)\n        self.assertIs(timezone.template_localtime(aware, use_tz=None, convert_to_local_time=False), aware)\n\n    # Check that naive datetime is converted when USE_TZ=True and use_tz=True\n    with override_settings(USE_TZ=True), timezone.override(EAT):\n        self.assertEqual(timezone.template_localtime(naive, use_tz=True).hour, 13)\n        self.assertEqual(timezone.template_localtime(aware, use_tz=True).hour, 13)\n\n    # Check that naive datetime is converted when USE_TZ=True and use_tz=None\n    with override_settings(USE_TZ=True), timezone.override(EAT):\n        self.assertEqual(timezone.template_localtime(naive).hour, 13)\n        self.assertEqual(timezone.template_localtime(aware).hour, 13)\n", "def test_localtime_edge_cases(self):\n    aware_datetime = datetime.datetime(2011, 9, 1, 13, 20, 30, tzinfo=EAT)\n    naive_datetime = datetime.datetime(2011, 9, 1, 13, 20, 30)\n\n    with self.assertRaisesMessage(ValueError, 'localtime() cannot be applied to a naive datetime'):\n        timezone.localtime(naive_datetime)\n\n    # Test with different timezones\n    self.assertEqual(\n        timezone.localtime(aware_datetime, timezone=EAT),\n        datetime.datetime(2011, 9, 1, 13, 20, 30, tzinfo=EAT)\n    )\n    self.assertEqual(\n        timezone.localtime(aware_datetime, timezone=ICT),\n        datetime.datetime(2011, 9, 1, 17, 20, 30, tzinfo=ICT)\n    )\n\n    # Test with different UTC offset\n    utc_offset = datetime.timedelta(hours=12)\n    aware_datetime = datetime.datetime(2011, 9, 1, 13, 20, 30, tzinfo=datetime.timezone(utc_offset))\n    self.assertEqual(\n        timezone.localtime(aware_datetime),\n        datetime.datetime(2011, 9, 1, 1, 20, 30, tzinfo=timezone.get_current_timezone())\n    )\n\n    # Test with zero UTC offset\n    aware_datetime = datetime.datetime(2011, 9, 1, 13, 20, 30, tzinfo=datetime.timezone(datetime.timedelta()))\n    self.assertEqual(\n        timezone.localtime(aware_datetime),\n        datetime.datetime(2011, 9, 1, 13, 20, 30, tzinfo=timezone.get_current_timezone())\n    )\n"], "sample_684": ["    def test_eval(self) -> None:\n            x = 5\n            return sys._getframe(0)\n\n        frame = Frame(f())\n        assert frame.eval(\"x\") == 5\n        assert frame.eval(\"x + 1\") == 6\n        assert frame.eval(\"y\", y=7) == 7\n", "def test_FormattedExcinfo_repr_traceback_entry_short() -> None:\n    try:\n        if False:\n            pass\n        else:\n            assert False\n    except AssertionError:\n        exci = ExceptionInfo.from_current()\n        fmt = FormattedExcinfo(style=\"short\")\n        entry = exci.traceback[0]\n        reprentry = fmt.repr_traceback_entry(entry, excinfo=exci)\n        assert reprentry.style == \"short\"\n        assert len(reprentry.lines) == 1\n        assert \"assert False\" in reprentry.lines[0]\n", "    def test_cut(self) -> None:\n        try:\n            if False:\n                pass\n            else:\n                assert False\n        except AssertionError:\n            exci = ExceptionInfo.from_current()\n            tb = exci.traceback\n            assert len(tb) > 0\n            cut_tb = tb.cut()\n            assert len(cut_tb) == len(tb)\n", "def test_traceback_cut() -> None:\n    try:\n        with pytest.raises(AssertionError):\n            assert False\n    except AssertionError:\n        exci = ExceptionInfo.from_current()\n    tb = exci.traceback\n\n    # test cut with path\n    cut_tb = tb.cut(path=tb[0].path)\n    assert len(cut_tb) == len(tb)\n\n    # test cut with lineno\n    cut_tb = tb.cut(lineno=tb[0].lineno)\n    assert len(cut_tb) == len(tb)\n\n    # test cut with firstlineno\n    cut_tb = tb.cut(firstlineno=tb[0].frame.code.firstlineno)\n    assert len(cut_tb) == len(tb)\n\n    # test cut with excludepath\n    cut_tb = tb.cut(excludepath=py.path.local(_pytest.__file__).dirpath())\n    assert len(cut_tb) < len(tb)\n\n    # test cut with everything\n    cut_tb = tb.cut(\n        path=tb[0].path,\n        lineno=tb[0].lineno,\n        firstlineno=tb[0].frame.code.firstlineno,\n        excludepath=py.path.local(_pytest.__file__).dirpath(),\n    )\n    assert len(cut_tb) < len(tb)\n", "    def test_cut(self) -> None:\n                raise NotImplementedError()\n            f2()\n\n        try:\n            f1()\n        except NotImplementedError:\n            excinfo = ExceptionInfo.from_current()\n\n        tb = excinfo.traceback\n        assert len(tb) == 3\n\n        # Cutting by path\n        newtb = tb.cut(path=\"test_code.py\")\n        assert len(newtb) == 2\n\n        # Cutting by line number\n        newtb = tb.cut(lineno=13)\n        assert len(newtb) == 2\n\n        # Cutting by first line number\n        newtb = tb.cut(firstlineno=16)\n        assert len(newtb) == 1\n"], "sample_181": ["def test_filtered_aggregate_ref_annotation_with_filter(self):\n    aggs = Author.objects.annotate(\n        double_age=F('age') * 2,\n    ).filter(\n        double_age__gt=100,\n    ).aggregate(\n        avg_age=Avg('age', filter=Q(double_age__gt=150)),\n    )\n    self.assertEqual(aggs['avg_age'], 100)\n", "def test_filtered_aggregate_ref_annotation_with_conditional_expression(self):\n    aggs = Author.objects.annotate(\n        older_than_50=Case(When(age__gt=50, then=True), default=False),\n    ).aggregate(\n        cnt=Count('pk', filter=Q(older_than_50=True)),\n    )\n    self.assertEqual(aggs['cnt'], 2)\n", "def test_filtered_aggregate_ref_annotation_with_transform(self):\n    aggs = Author.objects.annotate(\n        upper_name=Upper('name'),\n    ).aggregate(\n        cnt=Count('pk', filter=Q(upper_name='TEST')),\n    )\n    self.assertEqual(aggs['cnt'], 1)\n", "def test_filtered_aggregate_annotation_with_disjunction(self):\n    aggs = Author.objects.annotate(\n        friends_with_book=Exists(\n            Book.authors.through.objects.filter(author=OuterRef('pk')),\n        ),\n    ).aggregate(\n        cnt=Count('pk', filter=Q(friends_with_book=True) | Q(friends__book__rating__gt=3)),\n    )\n    self.assertEqual(aggs['cnt'], 3)\n", "def test_join_promotion(self):\n    # Test join promotion for multiple filters with different join types\n    Author.objects.create(name='test4', age=50)\n    b4 = Book.objects.create(\n        isbn='123456789', name='Test Book', pages=200, rating=4.0, price=Decimal('20.00'), contact=self.a1,\n        pubdate=datetime.date(2020, 1, 1),\n    )\n    b4.authors.add(self.a2)\n    b4.authors.add(self.a3)\n\n    # Test that join promotion works for multiple filters with different join types\n    qs = Author.objects.filter(\n        Q(friends__book__publisher=self.p1) & Q(friends__book=self.b1) & ~Q(friends__book=self.b2)\n    )\n    self.assertEqual(qs.count(), 1)\n    self.assertEqual(qs.get().pk, self.a1.pk)\n\n    # Test that join promotion works for multiple filters with different join types and negations\n    qs = Author.objects.filter(\n        ~(Q(friends__book__publisher=self.p1) & Q(friends__book=self.b1) & ~Q(friends__book=self.b2))\n    )\n    self.assertEqual(qs.count(), 2)\n    self.assertNotIn(self.a1, qs)\n\n    # Test that join promotion works for multiple filters with different join types and negations and OR\n    qs = Author.objects.filter(\n        ~(Q(friends__book__publisher=self.p1) & Q(friends__book=self.b1)) | ~Q(friends__book=self.b2)\n    )\n    self.assertEqual(qs.count(), 3)\n"], "sample_884": ["def test_deprecated_property():\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        assert MockClass2().n_features_ == 10\n", "def test_deprecated_property():\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        assert MockClass2().n_features_ == 10\n", "def test_property_deprecation():\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        assert MockClass2().n_features_ == 10\n", "def test_property_deprecation():\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        MockClass2().n_features_\n\n    class MockClass7:\n        @property\n        @deprecated(\"n_features_ is deprecated\")\n            \"\"\"Number of input features.\"\"\"\n            return 10\n\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        MockClass7().n_features_\n\n    class MockClass8:\n        @property\n            \"\"\"Number of input features.\"\"\"\n            return 10\n\n    assert not _is_deprecated(MockClass8.n_features_.fget)\n", "def test_decorated_property():\n    \"\"\"Test that the deprecated decorator works with properties.\"\"\"\n    class MockClass:\n        @deprecated(\"n_features_ is deprecated\")\n        @property\n            \"\"\"Number of input features.\"\"\"\n            return 10\n\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        mock_instance = MockClass()\n        val = mock_instance.n_features_\n    assert val == 10\n\n    # Test that the docstring is correctly updated\n    assert \"n_features_ is deprecated\" in MockClass.n_features_.__doc__\n"], "sample_371": ["    def test_request_accepts_text_html(self):\n        request = self.rf.get('/test_view/')\n        request.accepts = lambda mime: True\n        request.accepts = lambda mime: mime == 'text/html'\n        try:\n            raise ValueError(\"Can't find my keys\")\n        except ValueError:\n            exc_type, exc_value, tb = sys.exc_info()\n        response = technical_500_response(request, exc_type, exc_value, tb)\n        self.assertEqual(response.status_code, 500)\n        self.assertEqual(response['Content-Type'], 'text/html; charset=utf-8')\n        self.assertInHTML('<h1>ValueError at /test_view/</h1>', response.content.decode())\n", "    def test_cleansed_multivaluedict_with_non_str_key(self):\n        \"\"\"\n        A MultivalueDict with non-string keys should not break the\n        get_cleansed_multivaluedict() function.\n        \"\"\"\n        reporter_filter = SafeExceptionReporterFilter()\n        multivaluedict = MultiValueDict({42: ['value']})\n        request = self.rf.get('/')\n        self.assertEqual(\n            reporter_filter.get_cleansed_multivaluedict(request, multivaluedict),\n            multivaluedict,\n        )\n", "    def test_sensitive_variables(self):\n        reporter_filter = SafeExceptionReporterFilter()\n        tb_frame = types.SimpleNamespace(\n            f_locals={\n                'user_name': 'johndoe',\n                'user_password': 'secret',\n                'other_var': 'other_value',\n            },\n        )\n        variables = reporter_filter.get_traceback_frame_variables(None, tb_frame)\n        self.assertEqual(variables, [\n            ('other_var', 'other_value'),\n            ('user_name', 'johndoe'),\n            ('user_password', reporter_filter.cleansed_substitute),\n        ])\n", "    def test_exception_reporter_filter_from_request_with_request_subclass(self):\n        \"\"\"\n        An exception reporter filter can be assigned to the request to bypass the one\n        set in DEFAULT_EXCEPTION_REPORTER_FILTER, and this still works when using\n        a request subclass.\n        \"\"\"\n        class CustomRequest(HttpRequest):\n            pass\n\n        class CustomExceptionReporterFilter(SafeExceptionReporterFilter):\n            cleansed_substitute = 'CUSTOM_SUBSTITUTE'\n\n        request = CustomRequest()\n        request.exception_reporter_filter = CustomExceptionReporterFilter()\n\n        try:\n            raise ValueError(\"Can't find my keys\")\n        except ValueError:\n            exc_type, exc_value, tb = sys.exc_info()\n\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n        html = reporter.get_traceback_html()\n        self.assertIn('CUSTOM_SUBSTITUTE', html)\n\n        text = reporter.get_traceback_text()\n        self.assertIn('CUSTOM_SUBSTITUTE', text)\n", "    def test_get_traceback_data_encoding_error(self):\n        \"\"\"The ExceptionReporter handles UnicodeError when generating traceback data.\"\"\"\n        request = HttpRequest()\n        request.path = '/test_view/'\n        try:\n            'hello'.encode('ascii').decode('non-existent-encoding')\n        except Exception:\n            exc_type, exc_value, tb = sys.exc_info()\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n        with mock.patch('django.views.debug.ExceptionReporter._get_source') as mock_get_source:\n            mock_get_source.side_effect = UnicodeError('encoding', 'message', 1, 2, 'object')\n            data = reporter.get_traceback_data()\n            self.assertIn('Unicode error hint', data)\n"], "sample_577": ["    def test_repr_png_uses_retina_scaling(self):\n\n        p = Plot()\n        data, metadata = p._repr_png_()\n        img = Image.open(io.BytesIO(data))\n        assert img.size[0] == metadata[\"width\"] * 2\n        assert img.size[1] == metadata[\"height\"] * 2\n", "    def test_on_axes_multi(self):\n\n        fig = mpl.figure.Figure()\n        axs = fig.subplots(2)\n        m = MockMark()\n        p = Plot([1], [2]).on(axs).add(m).plot()\n        assert m.passed_axes == axs\n        assert p._figure is fig\n", "    def test_user_supplied_figure(self, long_df):\n\n        f = plt.figure()\n        p = Plot(long_df).add(MockMark()).on(f)\n        with pytest.raises(RuntimeError, match=\"Need to call Plot.plot\"):\n            p.show()\n\n        p = p.plot()\n        assert p._figure is f\n        assert len(p._figure.axes) == 1\n        p.show()\n        assert len(plt.get_fignums()) == 1\n        assert p._figure is plt.gcf()\n", "    def test_alpha(self, long_df):\n\n        m = MockMark()\n        Plot(long_df, x=\"a\", y=\"z\").add(m, alpha=0.5).plot()\n        assert m.passed_scales[\"alpha\"] is not None\n", "    def test_empty(self):\n\n        p = Plot().limit(x=(1, 5)).plot()\n        ax, = p._figure.axes\n        assert ax.get_xlim() == (1, 5)\n"], "sample_85": ["    def test_foreignkey_field(self):\n        r = R.objects.create()\n        a = A.objects.create(auto=r)\n        self.assertEqual(a.auto_id, r.id)\n", "    def test_fk_forward_accessor(self):\n        a = create_a('auto')\n        child = a.child\n        self.assertEqual(a, child.r_ptr)\n", "    def test_related_field_string_representation(self):\n        f = RelatedField(to='self', related_name='rel', related_query_name='rel')\n        self.assertEqual(str(f), 'RelatedField(to=\"self\", related_name=\"rel\", related_query_name=\"rel\")')\n", "    def test_reverse_query_name_clash(self):\n        \"\"\"\n        Test that a related field with a custom related_query_name does not\n        clash with a field on the model.\n        \"\"\"\n        class A(models.Model):\n            pass\n\n        class B(models.Model):\n            a = models.ForeignKey(A, on_delete=models.CASCADE, related_query_name='b')\n\n        class C(models.Model):\n            b = models.IntegerField()\n            a = models.ForeignKey(B, on_delete=models.CASCADE)\n\n        with self.assertRaisesMessage(\n                checks.Error,\n                \"Reverse query name for 'C.a' clashes with field name 'C.b'.\"):\n            models.get_app_configs()\n", "    def test_related_query_name_clashes(self):\n        # Regression for #28634\n        class A(models.Model):\n            pass\n\n        class B(models.Model):\n            a = models.ForeignKey(A, on_delete=models.CASCADE, related_name='b')\n\n        class C(models.Model):\n            a = models.ForeignKey(A, on_delete=models.CASCADE, related_name='b')\n\n        with self.assertRaisesMessage(ValueError, \"Reverse One-To-One descriptors must use 'related_name' attribute\"):\n            apps.get_models()\n"], "sample_135": ["    def test_time_format(self):\n        my_birthday = datetime(1979, 7, 8, 22, 00)\n\n        self.assertEqual(dateformat.time_format(my_birthday, 'f'), '10:00')\n        self.assertEqual(dateformat.time_format(my_birthday, 'g'), '10')\n        self.assertEqual(dateformat.time_format(my_birthday, 'G'), '22')\n        self.assertEqual(dateformat.time_format(my_birthday, 'h'), '10')\n        self.assertEqual(dateformat.time_format(my_birthday, 'H'), '22')\n        self.assertEqual(dateformat.time_format(my_birthday, 'i'), '00')\n        self.assertEqual(dateformat.time_format(my_birthday, 's'), '00')\n        self.assertEqual(dateformat.time_format(my_birthday, 'u'), '000000')\n        self.assertEqual(dateformat.time_format(my_birthday, 'e'), '')\n        self.assertEqual(dateformat.time_format(my_birthday, 'O'), '+0100')\n        self.assertEqual(dateformat.time_format(my_birthday, 'T'), 'CET')\n        self.assertEqual(dateformat.time_format(my_birthday, 'Z'), '3600')\n", "def test_leap_year_transition(self):\n    # Test the transition from a non-leap year to a leap year\n    self.assertEqual(dateformat.format(datetime(2020, 12, 31), 'L'), 'True')\n    self.assertEqual(dateformat.format(datetime(2021, 1, 1), 'L'), 'False')\n    # Test the transition from a leap year to a non-leap year\n    self.assertEqual(dateformat.format(datetime(2020, 1, 1), 'L'), 'True')\n    self.assertEqual(dateformat.format(datetime(2021, 12, 31), 'L'), 'False')\n", "def test_timezone_offset_edge_cases(self):\n    # Test edge cases for timezone offset.\n    dt = datetime(2009, 5, 16, 5, 30, 30)\n    tz = get_fixed_timezone(-1439)  # This is equivalent to '-23:59'\n    aware_dt = dt.replace(tzinfo=tz)\n    self.assertEqual(dateformat.format(aware_dt, 'O'), '-2359')\n    self.assertEqual(dateformat.format(aware_dt, 'Z'), -86340)\n\n    tz = get_fixed_timezone(1439)  # This is equivalent to '+23:59'\n    aware_dt = dt.replace(tzinfo=tz)\n    self.assertEqual(dateformat.format(aware_dt, 'O'), '+2359')\n    self.assertEqual(dateformat.format(aware_dt, 'Z'), 86340)\n", "    def test_e_format_with_different_timezones(self):\n        if TZ_SUPPORT:\n            # Test with a few different timezones\n            timezones = [\n                'America/New_York',\n                'Europe/London',\n                'Australia/Sydney',\n                'Asia/Tokyo',\n            ]\n            for tz_name in timezones:\n                tz = pytz.timezone(tz_name)\n                dt = datetime(2009, 5, 16, 5, 30, 30, tzinfo=tz)\n                self.assertEqual(dateformat.format(dt, 'e'), tz_name)\n", "    def test_time_format_functions(self):\n        my_birthday = datetime(1979, 7, 8, 22, 00)\n\n        self.assertEqual(time_format(my_birthday, 'A'), 'PM')\n        self.assertEqual(time_format(my_birthday, 'e'), '')\n        self.assertEqual(time_format(my_birthday, 'f'), '10:00')\n        self.assertEqual(time_format(my_birthday, 'g'), '10')\n        self.assertEqual(time_format(my_birthday, 'G'), '22')\n        self.assertEqual(time_format(my_birthday, 'h'), '10')\n        self.assertEqual(time_format(my_birthday, 'H'), '22')\n        self.assertEqual(time_format(my_birthday, 'i'), '00')\n        self.assertEqual(time_format(my_birthday, 'P'), '10 p.m.')\n        self.assertEqual(time_format(my_birthday, 's'), '00')\n        self.assertEqual(time_format(my_birthday, 'u'), '000000')\n\n        # Test the function with a time object\n        time_obj = datetime.time(22, 0, 0)\n        self.assertEqual(time_format(time_obj, 'A'), 'PM')\n        self.assertEqual(time_format(time_obj, 'g'), '10')\n        self.assertEqual(time_format(time_obj, 'G'), '22')\n        self.assertEqual(time_format(time_obj, 'h'), '10')\n        self.assertEqual(time_format(time_obj, 'H'), '22')\n        self.assertEqual(time_format(time_obj, 'i'), '00')\n        self.assertEqual(time_format(time_obj, 's'), '00')\n"], "sample_6": ["def test_longitude_wrap_at():\n    lon = Longitude([0, 360, 720] * u.deg)\n    lon.wrap_at(180 * u.deg)\n    npt.assert_array_almost_equal(lon.value, [0, -180, -180])\n\n    lon = Longitude([-720, -360, 0] * u.deg)\n    lon.wrap_at(180 * u.deg)\n    npt.assert_array_almost_equal(lon.value, [-180, -180, 0])\n", "def test_longitude_wrap_angle():\n    lon = Longitude([170, 190, -170], unit=u.deg, wrap_angle=180*u.deg)\n    assert np.all(lon.value >= -180)\n    assert np.all(lon.value < 180)\n\n    lon.wrap_angle = 0*u.deg\n    assert np.all(lon.value >= 0)\n    assert np.all(lon.value < 360)\n\n    lon.wrap_angle = 120*u.deg\n    assert np.all(lon.value >= -240)\n    assert np.all(lon.value < 120)\n\n    with pytest.raises(u.UnitsError):\n        lon.wrap_angle = u.meter\n\n    with pytest.raises(u.UnitsError):\n        Longitude([170, 190, -170], unit=u.deg, wrap_angle=u.meter)\n", "def test_longitude_wrap_at():\n    \"\"\"\n    Test the Longitude wrap_at method.\n    \"\"\"\n    lon = Longitude([179, 180, 181, 359, 360, 361] * u.degree)\n    lon.wrap_at(180 * u.degree, inplace=True)\n    npt.assert_almost_equal(lon.degree, [-1, 0, 1, -1, 0, 1])\n\n    lon2 = Longitude([179, 180, 181, 359, 360, 361] * u.degree)\n    lon2.wrap_at('0d', inplace=True)\n    npt.assert_almost_equal(lon2.degree, [-179, -180, -179, -1, 0, 1])\n\n    lon3 = Longitude([179, 180, 181, 359, 360, 361] * u.degree)\n    lon3.wrap_at(90 * u.degree, inplace=True)\n    npt.assert_almost_equal(lon3.degree, [89, 90, 91, 269, 270, 271])\n", "def test_latitude_longitude():\n    \"\"\"\n    Test Latitude and Longitude classes.\n    \"\"\"\n    lat1 = Latitude(\"45d\")\n    assert lat1.value == 45.0\n    assert lat1.unit == u.degree\n\n    lat2 = Latitude([-45, 45], unit=u.degree)\n    npt.assert_almost_equal(lat2.value, [-45, 45])\n    assert lat2.unit == u.degree\n\n    with pytest.raises(ValueError):\n        Latitude(\"100d\")\n\n    with pytest.raises(TypeError):\n        Latitude(Longitude(45, unit=u.degree))\n\n    long1 = Longitude(\"45d\")\n    assert long1.value == 45.0\n    assert long1.unit == u.degree\n\n    long2 = Longitude([-45, 45], unit=u.degree)\n    npt.assert_almost_equal(long2.value, [-45, 45])\n    assert long2.unit == u.degree\n\n    long3 = Longitude([-45, 45], unit=u.degree, wrap_angle=180*u.degree)\n    npt.assert_almost_equal(long3.value, [315, 45])\n    assert long3.unit == u.degree\n\n    with pytest.raises(TypeError):\n        Longitude(Latitude(45, unit=u.degree))\n\n    lat3 = Latitude(long1)\n    assert lat3.value == 45.0\n    assert lat3.unit == u.degree\n\n    long4 = Longitude(lat2)\n    assert long4.value == [-45, 45]\n    assert long4.unit == u.degree\n", "def test_longitude_wrap_angle():\n    \"\"\"\n    Test the wrap_angle attribute of the Longitude class.\n    \"\"\"\n    lon1 = Longitude([0, 180, 360, 720], unit=u.degree, wrap_angle=180*u.degree)\n    npt.assert_array_almost_equal(lon1.value, [-180, 180, -180, 180])\n\n    lon2 = Longitude([0, 180, 360, 720], unit=u.degree, wrap_angle=0*u.degree)\n    npt.assert_array_almost_equal(lon2.value, [0, 180, 0, 180])\n\n    lon3 = Longitude([0, 180, 360, 720], unit=u.degree)\n    lon3.wrap_angle = 180*u.degree\n    npt.assert_array_almost_equal(lon3.value, [-180, 180, -180, 180])\n\n    lon4 = Longitude([0, 180, 360, 720], unit=u.degree, wrap_angle=360*u.degree)\n    lon4.wrap_angle = 180*u.degree\n    npt.assert_array_almost_equal(lon4.value, [-180, 180, -180, 180])\n\n    lon5 = Longitude([0, 180, 360, 720], unit=u.degree, wrap_angle=0*u.degree)\n    lon5.wrap_angle = 360*u.degree\n    npt.assert_array_almost_equal(lon5.value, [0, 180, 0, 180])\n\n    with pytest.raises(ValueError):\n        Longitude([0, 180, 360, 720], unit=u.degree, wrap_angle=90*u.degree)\n\n    with pytest.raises(TypeError):\n        Longitude([0, 180, 360, 720], unit=u.degree, wrap_angle='invalid')\n"], "sample_615": ["def test_cross_chunks() -> None:\n    data_array_1 = xr.DataArray(\n        [[1, 2], [3, 4]], dims=(\"time\", \"cartesian\"), coords={\"cartesian\": [\"x\", \"y\"]}\n    )\n    data_array_2 = xr.DataArray(\n        [[4, 5, 6], [1, 2, 3]], dims=(\"time\", \"cartesian\"), coords={\"cartesian\": [\"x\", \"y\", \"z\"]}\n    )\n\n    expected_chunks = ((2,), (3,))\n    actual_chunks = xr.cross(data_array_1, data_array_2, dim=\"cartesian\").data.chunks\n    assert actual_chunks == expected_chunks\n", "def test_cov_cross_with_2d_arrays() -> None:\n    a = xr.DataArray(\n        np.array([[1, 2], [3, 4]]),\n        dims=[\"space\", \"cartesian\"],\n        coords={\"space\": [\"IA\", \"IL\"], \"cartesian\": [\"x\", \"y\"]},\n    )\n    b = xr.DataArray(\n        np.array([[4, 5], [6, 7]]),\n        dims=[\"space\", \"cartesian\"],\n        coords={\"space\": [\"IA\", \"IL\"], \"cartesian\": [\"x\", \"y\"]},\n    )\n\n    expected = xr.DataArray(\n        np.array([[-2, 2], [-2, 2]]),\n        dims=[\"space\", \"cartesian\"],\n        coords={\"space\": [\"IA\", \"IL\"], \"cartesian\": [\"x\", \"y\"]},\n    )\n    actual = xr.cross(a, b, dim=\"cartesian\")\n    xr.testing.assert_duckarray_allclose(expected, actual)\n", "def test_cross_fill_value(use_dask: bool) -> None:\n    if use_dask and not has_dask:\n        pytest.skip(\"test for dask.\")\n\n    da_a = xr.DataArray(\n        [1, 2],\n        dims=[\"cartesian\"],\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\"])),\n    )\n    da_b = xr.DataArray(\n        [4, 5, 6],\n        dims=[\"cartesian\"],\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])),\n    )\n\n    if use_dask:\n        da_a = da_a.chunk()\n        da_b = da_b.chunk()\n\n    expected = np.array([-10, 2, 5])\n    actual = xr.cross(da_a, da_b, dim=\"cartesian\")\n    xr.testing.assert_duckarray_allclose(expected, actual)\n\n    # fill_value is used\n    da_b = xr.DataArray(\n        [4, 5, np.nan],\n        dims=[\"cartesian\"],\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])),\n    )\n    expected = np.array([-10, 2, np.nan])\n    actual = xr.cross(da_a, da_b, dim=\"cartesian\")\n    xr.testing.assert_duckarray_allclose(expected, actual)\n\n    # fill_value is used with dask\n    da_b = xr.DataArray(\n        [4, 5, np.nan],\n        dims=[\"cartesian\"],\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])),\n    )\n    expected = np.array([-10, 2, np.nan])\n    actual = xr.cross(da_a, da_b, dim=\"cartesian\")\n    xr.testing.assert_duckarray_allclose(expected, actual)\n", "def test_broadcast_compat_data() -> None:\n    data = np.arange(12).reshape(3, 4)\n    var = xr.Variable([\"x\", \"y\"], data)\n\n    broadcast_dims = [\"x\"]\n    core_dims = [\"y\"]\n    assert_identical(broadcast_compat_data(var, broadcast_dims, core_dims), data)\n\n    broadcast_dims = [\"z\", \"x\"]\n    core_dims = [\"y\"]\n    expected = np.expand_dims(data, 0)\n    assert_identical(broadcast_compat_data(var, broadcast_dims, core_dims), expected)\n\n    broadcast_dims = [\"z\", \"x\"]\n    core_dims = [\"w\", \"y\"]\n    with pytest.raises(ValueError):\n        broadcast_compat_data(var, broadcast_dims, core_dims)\n\n    broadcast_dims = [\"z\"]\n    core_dims = [\"x\", \"y\"]\n    expected = np.expand_dims(data, 0)\n    assert_identical(broadcast_compat_data(var, broadcast_dims, core_dims), expected)\n\n    broadcast_dims = []\n    core_dims = [\"y\"]\n    expected = data.T\n    assert_identical(broadcast_compat_data(var, broadcast_dims, core_dims), expected)\n\n    broadcast_dims = []\n    core_dims = [\"x\", \"y\"]\n    assert_identical(broadcast_compat_data(var, broadcast_dims, core_dims), data)\n\n    broadcast_dims = [\"x\", \"y\"]\n    core_dims = []\n    expected = np.expand_dims(data, (0, 1))\n    assert_identical(broadcast_compat_data(var, broadcast_dims, core_dims), expected)\n\n    broadcast_dims = [\"x\"]\n    core_dims = []\n    expected = np.expand_dims(data, (0, 1))\n    assert_identical(broadcast_compat_data(var, broadcast_dims, core_dims), expected)\n", "def test_cross_incompatible_sizes() -> None:\n    array = xr.DataArray([1, 2, 3])\n    array_1d = xr.DataArray([4, 5])\n    array_4d = xr.DataArray(\n        [[[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]]\n    )\n\n    with pytest.raises(ValueError, match=r\"The size of 'cartesian' on a must be 1, 2, or 3\"):\n        xr.cross(array_1d, array, dim=\"cartesian\")\n    with pytest.raises(ValueError, match=r\"The size of 'cartesian' on b must be 1, 2, or 3\"):\n        xr.cross(array, array_4d, dim=\"cartesian\")\n\n    array_4d = xr.DataArray(\n        [\n            [[1, 2, 3], [4, 5, 6]],\n        ]\n    )\n    with pytest.raises(ValueError, match=r\"dimension without coordinates must have a length of 2 or 3\"):\n        xr.cross(array_1d, array_4d, dim=\"cartesian\")\n"], "sample_989": ["def test_Catalan_NaN():\n    assert Catalan._eval_power(nan) == nan\n", "def test_issue_12820():\n    Float('1.0', prec=15)\n    assert Float('1.0', dps=15)._prec == 15\n    assert Float('1.0', precision=15)._prec == 15\n", "def test_issue_13023():\n    assert Rational(1, 2) == Rational(-(-1, 2))\n    assert Rational(-1, 2) == Rational(-(1, 2))\n    assert Rational(-1, -2) == Rational(-(1, -2))\n    assert Rational(1, -2) == Rational(-(-1, -2))\n", "def test_explicit_precision():\n    Float('1.1', 100)\n    Float(1.1, 100)\n    Float(11/10, 100)\n    Rational(11, 10)\n    Float(Rational(11, 10), 100)\n    Float(Float('1.1', 10), 100)\n", "def test_mod_inverse():\n    assert mod_inverse(3, 11) == 4\n    assert mod_inverse(5, 11) == 9\n    assert mod_inverse(21124921, 521512) == 7713\n    assert mod_inverse(124215421, 5125) == 2981\n    assert mod_inverse(214, 12515) == 1579\n    assert mod_inverse(5823991, 3299) == 1442\n    assert mod_inverse(123, 44) == 39\n    assert mod_inverse(2, 5) == 3\n    assert mod_inverse(-2, 5) == -3\n    x = Symbol('x')\n    assert S(2).invert(x) == S.Half\n    raises(TypeError, lambda: mod_inverse(2, x))\n    raises(ValueError, lambda: mod_inverse(2, S.Half))\n    raises(ValueError, lambda: mod_inverse(2, cos(1)**2 + sin(1)**2))\n\n    # Test for issue 5799\n    assert mod_inverse(5, 1) == 0\n\n    # Test for issue 8044\n    assert mod_inverse(1, 7) == 1\n"], "sample_269": ["    def test_js_catalog_json_response(self):\n        view = JavaScriptCatalog.as_view()\n        request = RequestFactory().get('/')\n        response = view(request)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response['Content-Type'], 'text/javascript; charset=\"utf-8\"')\n", "def test_i18n_language_english_default_multiple_catalogs(self):\n    \"\"\"\n    Check if the JavaScript i18n view returns a complete language catalog\n    if the default language is en-us, the selected language has a\n    translation available and a catalog composed by djangojs domain\n    translations of multiple Python packages is requested with a specific\n    package. See #13388, #3594 and #13514 for more details.\n    \"\"\"\n    base_trans_string = 'il faut traduire cette cha\\\\u00eene de caract\\\\u00e8res de '\n    app0_trans_string = base_trans_string + 'app0'\n    app4_trans_string = base_trans_string + 'app4'\n    with self.settings(LANGUAGE_CODE='en-us'), override('fr'):\n        response = self.client.get('/jsi18n_multi_packages4/')\n        self.assertContains(response, app0_trans_string)\n        self.assertContains(response, app4_trans_string)\n\n        response = self.client.get('/jsi18n/app0/')\n        self.assertContains(response, app0_trans_string)\n        self.assertNotContains(response, app4_trans_string)\n\n        response = self.client.get('/jsi18n/app4/')\n        self.assertNotContains(response, app0_trans_string)\n        self.assertContains(response, app4_trans_string)\n", "    def test_json_catalog(self):\n        with override('de'):\n            response = self.client.get('/jsoni18n/')\n            data = json.loads(response.content.decode())\n            self.assertEqual(response.status_code, 200)\n            self.assertIn('catalog', data)\n            self.assertIn('formats', data)\n            self.assertIn('plural', data)\n", "    def test_jsi18n_plural_form(self):\n        \"\"\"Test that the JavaScript plural form function works correctly.\"\"\"\n        with override('ru'):\n            response = self.client.get('/jsi18n/')\n            self.assertContains(response, 'django.pluralidx = function(n) { return (n % 10 == 1 && n % 100 != 11) ? 0 : ((n % 10 >= 2 && n % 10 <= 4 && (n % 100 < 10 || n % 100 >= 20)) ? 1 : 2); };')\n", "    def test_i18n_domain(self):\n        \"\"\"\n        Test that JavaScriptCatalog can handle a custom domain.\n        \"\"\"\n        view = JavaScriptCatalog.as_view(domain='custom_domain')\n        request = RequestFactory().get('/')\n        response = view(request, packages='view_tests.app6')\n        self.assertEqual(response.status_code, 200)\n        self.assertIn('django.catalog = {};', response.content.decode())\n        self.assertIn('custom_domain', response.content.decode())\n"], "sample_557": ["def test_subfigure_repr():\n    fig = plt.figure(layout='constrained')\n    subfig = fig.subfigures(1, 1)\n    assert repr(subfig) == \"<SubFigure size 640x480 with 0 Axes>\"\n    ax = subfig.add_subplot(111)\n    assert repr(subfig) == \"<SubFigure size 640x480 with 1 Axes>\"\n", "def test_layout_mismatch():\n    fig, ax = plt.subplots(layout='constrained')\n    with pytest.warns(UserWarning, match='The figure layout has changed to tight'):\n        plt.tight_layout()\n    with pytest.raises(RuntimeError, match='Colorbar layout of new layout engine not compatible'):\n        fig.set_layout_engine('constrained')\n", "def test_subfigure_text(fig_test, fig_ref):\n    fig_test.dpi = 100\n    fig_ref.dpi = 100\n\n    ax_test = fig_test.add_subplot()\n    l1 = fig_test.text(0.5, 0.5, 'Hello, world!', gid='text1')\n    l2 = fig_test.text(0.6, 0.6, 'Hello, world!', gid='text2')\n    fig_test.texts.remove(l2)\n    l3 = fig_test.text(0.7, 0.7, 'Hello, world!', gid='text3')\n    l3.remove()\n\n    ax_ref = fig_ref.add_subplot()\n    l1 = fig_ref.text(0.5, 0.5, 'Hello, world!', gid='text1', zorder=21,\n                      transform=fig_ref.transFigure)\n    l3 = fig_ref.text(0.7, 0.7, 'Hello, world!', gid='text3', zorder=20,\n                      transform=fig_ref.transFigure)\n", "def test_figure_suptitle_pick():\n    fig, ax = plt.subplots()\n    assert fig.suptitle('hello', color='r').contains((0.5, 0.98)) == (True, {'zorder': 10})\n", "def test_subfigure_tightbbox_shared_axis():\n    # test that we can get the tightbbox with a subfigure...\n    fig = plt.figure(layout='constrained')\n    sub = fig.subfigures(1, 2, wspace=0.01)\n\n    axs = sub[0].subplots(2, 2)\n    for ax in axs.flat:\n        ax.pcolormesh(np.random.randn(30, 30), vmin=-2.5, vmax=2.5)\n\n    axs = sub[1].subplots(2, 2, sharex=axs[-1, 0], sharey=axs[-1, 0])\n    for ax in axs.flat:\n        ax.pcolormesh(np.random.randn(30, 30), vmin=-2.5, vmax=2.5)\n\n    fig.draw_without_rendering()\n\n    np.testing.assert_allclose(\n            fig.get_tightbbox(fig.canvas.get_renderer()).width,\n            8.0)\n"], "sample_800": ["def test_check_classifiers_regression_target():\n    # check that non-binary classifiers raises an exception\n    # when fed regression targets\n    msg = 'Unknown label type: '\n    assert_raises_regex(ValueError, msg, check_classifiers_regression_target,\n                        'estimator', UntaggedBinaryClassifier())\n    # check that binary classifier raises an exception\n    # when fed regression targets\n    msg = 'Unknown label type: '\n    assert_raises_regex(ValueError, msg, check_classifiers_regression_target,\n                        'estimator', TaggedBinaryClassifier())\n", "def test_check_estimator_sparse_data_64():\n    # non-regression test for estimators supporting sparse data with 64-bit indices\n    from sklearn.decomposition import TruncatedSVD\n    from sklearn.feature_extraction.text import TfidfVectorizer\n\n    # TruncatedSVD\n    est = TruncatedSVD(n_components=1)\n    check_estimator_sparse_data(\"TruncatedSVD\", est)\n\n    # TfidfVectorizer\n    est = TfidfVectorizer()\n    check_estimator_sparse_data(\"TfidfVectorizer\", est)\n", "def test_check_estimator_enforce_tags_y():\n    # check that estimator tags are correctly enforced\n    estimator = RequiresPositiveYRegressor()\n\n    y = np.array([-1, -2, -3])\n    assert_raises_regex(ValueError, \"negative y values not supported!\",\n                        check_estimator, estimator)\n\n    y = np.array([1, 2, 3])\n    assert_warns_message(None, None, check_estimator, estimator)\n\n    y = np.array([0, 1, 2])\n    assert_warns_message(None, None, check_estimator, estimator)\n", "def test_check_estimator_sparse_data():\n    # check that check_estimator_sparse_data is checking for sparse support\n    # correctly\n    class SparseSupportedEstimator(BaseEstimator):\n            return self\n\n    class SparseNotSupportedEstimator(BaseEstimator):\n            if sp.issparse(X):\n                raise ValueError(\"Sparse input is not supported\")\n            return self\n\n    SparseSupportedEstimator._get_tags = lambda self: {\"poor_score\": True,\n                                                      \"binary_only\": True,\n                                                      \"allow_nan\": True,\n                                                      \"no_validation\": True}\n\n    SparseNotSupportedEstimator._get_tags = lambda self: {\"poor_score\": True,\n                                                         \"binary_only\": True,\n                                                         \"allow_nan\": True,\n                                                         \"no_validation\": True}\n\n    check_estimator(SparseSupportedEstimator())\n\n    msg = \"Estimator SparseNotSupportedEstimator doesn't seem to support\"\n    assert_raises_regex(AssertionError, msg, check_estimator,\n                        SparseNotSupportedEstimator())\n", "def test_check_estimators_sparse_data():\n    # check that sparse data works with check_estimator_sparse_data\n    # check that check_estimator_sparse_data skips estimators\n    # that have the tag binary_only or no_validation\n    check_estimator_sparse_data('estimator_name', LogisticRegression())\n    check_estimator_sparse_data('estimator_name', LinearRegression())\n\n    # check that estimators that have the tag binary_only\n    # are skipped\n    assert_raises_regex(SkipTest,\n                        'Can\\'t test estimator estimator_name which requires '\n                        'input of type \\[\\'binary\\'\\]',\n                        check_estimator_sparse_data, 'estimator_name',\n                        UntaggedBinaryClassifier)\n\n    # check that estimators that have the tag no_validation\n    # are skipped\n    class NoValidationEstimator(BaseEstimator):\n            return self\n\n            return {\"no_validation\": True}\n    assert_raises_regex(SkipTest, 'Can\\'t test estimator estimator_name '\n                                  'which has no_validation tag',\n                        check_estimator_sparse_data, 'estimator_name',\n                        NoValidationEstimator())\n"], "sample_262": ["def test_lazy_object(self):\n    class TestClass:\n            self.value = value\n\n            return f'TestClass({self.value})'\n\n    lazy_obj = lazy(lambda: TestClass(123), TestClass)\n    self.assertEqual(str(lazy_obj()), 'TestClass(123)')\n    self.assertEqual(repr(lazy_obj()), 'TestClass(123)')\n", "def test_lazy_object(self):\n    \"\"\"Test that LazyObject works correctly\"\"\"\n    class MyLazyObject(LazyObject):\n            self._wrapped = \"Hello, world!\"\n\n    obj = MyLazyObject()\n    self.assertEqual(obj._wrapped, empty)\n    self.assertEqual(str(obj), \"Hello, world!\")\n    self.assertEqual(obj._wrapped, \"Hello, world!\")\n", "    def test_lazy_object_unpickle(self):\n        \"\"\"\n        Ensure that lazy objects can be pickled and unpickled correctly.\n        \"\"\"\n        obj = lazy(lambda: \"Hello, world!\", str)()\n        pickled_obj = pickle.dumps(obj)\n        unpickled_obj = pickle.loads(pickled_obj)\n        self.assertEqual(str(unpickled_obj), \"Hello, world!\")\n", "def test_lazy_partition(self):\n    \"\"\"Test the partition function.\"\"\"\n    is_even = lambda x: x % 2 == 0\n    numbers = [1, 2, 3, 4, 5, 6]\n    even_numbers, odd_numbers = partition(is_even, numbers)\n    self.assertEqual(even_numbers, [2, 4, 6])\n    self.assertEqual(odd_numbers, [1, 3, 5])\n", "def test_lazy_object_hash(self):\n    \"\"\"\n    Test the hash() function on lazy objects.\n    \"\"\"\n    obj1 = lazy(lambda: \"Hello, World!\", str)\n    obj2 = lazy(lambda: \"Hello, World!\", str)\n    obj3 = lazy(lambda: \"Goodbye, World!\", str)\n    self.assertEqual(hash(obj1()), hash(obj2()))\n    self.assertNotEqual(hash(obj1()), hash(obj3()))\n\n    # Test that the hash of a lazy object is the same as the hash of the\n    # object it wraps, even if the wrapped object is also a lazy object.\n    obj4 = lazy(lambda: lazy(lambda: \"Hello, World!\", str), str)\n    self.assertEqual(hash(obj4()), hash(obj1()))\n\n    # Test that the hash of a lazy object is the same as the hash of the\n    # object it wraps, even if the wrapped object is a lazy object that wraps\n    # another lazy object.\n    obj5 = lazy(lambda: lazy(lambda: lazy(lambda: \"Hello, World!\", str), str), str)\n    self.assertEqual(hash(obj5()), hash(obj1()))\n\n    # Test that the hash of a lazy object is not the same as the hash of a\n    # different lazy object, even if they wrap the same type of object.\n    obj6 = lazy(lambda: 5, int)\n    obj7 = lazy(lambda: 5, int)\n    self.assertNotEqual(hash(obj6()), hash(obj1()))\n"], "sample_1081": ["def test_core_edge_cases():\n    assert core(1, 2) == 1\n    assert core(0, 2) == 0\n    assert core(-1, 2) == -1\n    assert core(-2, 2) == -2\n    raises(ValueError, lambda: core(0, 1))\n    raises(ValueError, lambda: core(1, 1))\n    raises(ValueError, lambda: core(-1, 1))\n    raises(ValueError, lambda: core(-2, 1))\n", "def test_core():\n    # test with a large number\n    large_number = 2**1000 * 3**500 * 257**127 * 383**60\n    assert core(large_number) == 1\n    assert core(large_number, 2) == 1\n    assert core(large_number, 3) == 1\n    assert core(large_number, 4) == 3\n    assert core(large_number, 1000) == 2**1000 * 3**500 * 257**127 * 383**60\n", "def test_factorint_visual_to_dict():\n    assert factorint(factorint(101*102*103, visual=True)) == {101: 1, 102: 1, 103: 1}\n    assert factorint(factorint(101*102*103, visual=True), visual=True) == Mul(Pow(101, 1, evaluate=False), Pow(102, 1, evaluate=False), Pow(103, 1, evaluate=False), evaluate=False)\n    assert factorint(factorint(101*102*103, visual=False), visual=True) == Mul(Pow(101, 1, evaluate=False), Pow(102, 1, evaluate=False), Pow(103, 1, evaluate=False), evaluate=False)\n    assert factorint(factorint(101*102*103, visual=True), visual=False) == {101: 1, 102: 1, 103: 1}\n    assert factorint(factorint(101*102*103, visual=False), visual=False) == {101: 1, 102: 1, 103: 1}\n    assert factorint(factorint(101*102*103, visual=True), multiple=True) == [101, 102, 103]\n    assert factorint(factorint(101*102*103, visual=False), multiple=True) == [101, 102, 103]\n    assert factorint(factorint(101*102*103, visual=True), multiple=True, visual=True) == Mul(Pow(101, 1, evaluate=False), Pow(102, 1, evaluate=False), Pow(103, 1, evaluate=False), evaluate=False)\n    assert factorint(factorint(101*102*103, visual=False), multiple=True, visual=True) == Mul(Pow(101, 1, evaluate=False), Pow(102, 1, evaluate=False), Pow(103, 1, evaluate=False), evaluate=False)\n", "def test_mersenne_prime_exponent_coverage():\n    # Test for edge case where Mersenne prime exponent is 2\n    assert mersenne_prime_exponent(1) == 2\n    \n    # Test for edge case where Mersenne prime exponent is large\n    assert mersenne_prime_exponent(50) == 756839\n    \n    # Test for invalid input\n    raises(ValueError, lambda: mersenne_prime_exponent(52))\n    raises(ValueError, lambda: mersenne_prime_exponent(0))\n    raises(ValueError, lambda: mersenne_prime_exponent(51))\n", "def test_issue_21011():\n    n = 4458505580023317606867\n    assert factorint(n, limit=10**6) == {n: 1}\n    assert factorint(n, limit=10**6, use_pm1=False) == {n: 1}\n    assert factorint(n, limit=10**6, use_rho=False) == {n: 1}\n    assert factorint(n, limit=10**6, use_pm1=False, use_rho=False) == {n: 1}\n    assert factorint(n, limit=10**6, use_pm1=True, use_rho=True) == {n: 1}\n"], "sample_264": ["def test_legacy_decode_fail(self):\n    # RemovedInDjango40Warning: pre-Django 3.1 hashes will be invalid.\n    storage = self.storage_class(self.get_request())\n    messages = ['this', 'that']\n    # Encode/decode a message using the pre-Django 3.1 hash.\n    encoder = MessageEncoder()\n    value = encoder.encode(messages)\n    encoded_messages = '%s$%s' % ('wrong_hash', value)\n    decoded_messages = storage._decode(encoded_messages)\n    self.assertIsNone(decoded_messages)\n", "def test_store_with_not_finished(self):\n    storage = self.get_storage()\n    response = self.get_response()\n    messages = [Message(constants.INFO, 'test') for _ in range(10)]\n    encoded_data = storage._encode(messages + [storage.not_finished])\n    self.assertIn(storage.not_finished, encoded_data)\n    storage._update_cookie(encoded_data, response)\n    messages, all_retrieved = storage._get()\n    self.assertEqual(len(messages), len(messages) - 1)\n    self.assertFalse(all_retrieved)\n", "def test_process_messages(self):\n    \"\"\"\n    Test the process_messages method in MessageDecoder.\n    \"\"\"\n    # Test with a list containing a message\n    messages = [{'__json_message': 1, 0, constants.INFO, 'message'}]\n    decoder = MessageDecoder()\n    decoded = decoder.process_messages(messages)\n    self.assertEqual(decoded, [Message(constants.INFO, 'message', extra_tags='')])\n\n    # Test with a list containing multiple messages\n    messages = [{'__json_message': 1, 0, constants.INFO, 'message1'}, {'__json_message': 1, 0, constants.INFO, 'message2'}]\n    decoded = decoder.process_messages(messages)\n    self.assertEqual(decoded, [Message(constants.INFO, 'message1', extra_tags=''), Message(constants.INFO, 'message2', extra_tags='')])\n\n    # Test with a dictionary containing a message\n    messages = {'message': {'__json_message': 1, 0, constants.INFO, 'message'}}\n    decoded = decoder.process_messages(messages)\n    self.assertEqual(decoded, {'message': Message(constants.INFO, 'message', extra_tags='')})\n\n    # Test with a nested dictionary and list\n    messages = {'message': [{'__json_message': 1, 0, constants.INFO, 'message1'}, {'__json_message': 1, 0, constants.INFO, 'message2'}]}\n    decoded = decoder.process_messages(messages)\n    self.assertEqual(decoded, {'message': [Message(constants.INFO, 'message1', extra_tags=''), Message(constants.INFO, 'message2', extra_tags='')]})\n\n", "def test_process_messages(self):\n    \"\"\"\n    Ensure MessageDecoder.process_messages() correctly processes nested data.\n    \"\"\"\n    # Test a list containing a message and a non-message\n    data = [\n        ['__json_message', 0, constants.INFO, 'test'],\n        'not a message',\n    ]\n    decoder = MessageDecoder()\n    processed = decoder.process_messages(data)\n    self.assertIsInstance(processed[0], Message)\n    self.assertEqual(processed[0].message, 'test')\n    self.assertEqual(processed[1], 'not a message')\n\n    # Test a dictionary containing a message and a non-message\n    data = {\n        'message': ['__json_message', 0, constants.INFO, 'test'],\n        'not a message': 'not a message',\n    }\n    processed = decoder.process_messages(data)\n    self.assertIsInstance(processed['message'], Message)\n    self.assertEqual(processed['message'].message, 'test')\n    self.assertEqual(processed['not a message'], 'not a message')\n\n    # Test a nested structure containing a message and a non-message\n    data = [\n        ['__json_message', 0, constants.INFO, 'test'],\n        {\n            'message': ['__json_message', 0, constants.INFO, 'test'],\n            'not a message': 'not a message',\n        },\n        'not a message',\n    ]\n    processed = decoder.process_messages(data)\n    self.assertIsInstance(processed[0], Message)\n    self.assertEqual(processed[0].message, 'test')\n    self.assertIsInstance(processed[1]['message'], Message)\n    self.assertEqual(processed[1]['message'].message, 'test')\n    self.assertEqual(processed[1]['not a message'], 'not a message')\n    self.assertEqual(processed[2], 'not a message')\n\n    # Test a message with SafeData\n    data = ['__json_message', 1, constants.INFO, mark_safe('test')]\n    processed = decoder.process_messages(data)\n    self.assertIsInstance(processed, Message)\n    self.assertIsInstance(processed.message, SafeData)\n    self.assertEqual(processed.message, mark_safe('test'))\n\n    # Test a message with extra tags\n    data = ['__json_message', 0, constants.INFO, 'test', 'tag']\n    processed = decoder.process_messages(data)\n    self.assertIsInstance(processed, Message)\n    self.assertEqual(processed.message, 'test')\n    self.assertEqual(processed.extra_tags, 'tag')\n", "def test_cookie_storage_not_finished_sentinel(self):\n    \"\"\"\n    Ensure the 'not_finished' sentinel value is correctly removed when\n    decoding and that it is added when the encoded data is too large.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # Test that the 'not_finished' sentinel value is correctly removed.\n    messages = [f'message {i}' for i in range(5)] + [CookieStorage.not_finished]\n    set_cookie_data(storage, messages)\n    self.assertEqual(list(storage), [f'message {i}' for i in range(5)])\n\n    # Test that the 'not_finished' sentinel value is added when the encoded data is too large.\n    messages = [f'message {i}' for i in range(10)]  # Increase the number of messages.\n    storage = self.get_storage()\n    response = self.get_response()\n    storage.add(constants.INFO, messages)\n    storage.update(response)\n    decoded_messages = storage._decode(response.cookies['messages'].value)\n    self.assertEqual(len(decoded_messages), 9)\n    self.assertEqual(decoded_messages[-1], CookieStorage.not_finished)\n"], "sample_1015": ["def test_ccode_PrePostIncDec():\n    assert ccode(PreIncrement(x)) == '++x'\n    assert ccode(PostIncrement(x)) == 'x++'\n    assert ccode(PreDecrement(x)) == '--x'\n    assert ccode(PostDecrement(x)) == 'x--'\n", "def test_ccode_sign_function():\n    expr = sign(sin(x) * cos(x))\n    assert ccode(expr) == \"(((sin(x)*cos(x)) > 0) - ((sin(x)*cos(x)) < 0))\"\n    assert ccode(sign(sin(x) * cos(x)), 'z') == 'z = (((sin(x)*cos(x)) > 0) - ((sin(x)*cos(x)) < 0));'\n", "def test_ccode_MacroPrinter():\n    printer = CCodePrinter()\n    assert printer._print_Type(integer) == 'int'\n    assert printer._get_comment('test') == '// test'\n    assert printer._get_statement('test') == 'test;'\n", "def test_ccode_reserved_word_substitution():\n    x, y = symbols('x, y, auto')\n    assert ccode(x + y) == 'x + y'\n    assert ccode(x + auto) == 'x + auto_'\n\n    x, y = symbols('x, y, while')\n    assert ccode(x + y) == 'x + y'\n    assert ccode(x + while_) == 'x + while_'\n\n    x, y = symbols('x, y, struct')\n    assert ccode(x + y) == 'x + y'\n    assert ccode(x + struct_) == 'x + struct_'\n\n    x, y = symbols('x, y, inline')\n    assert ccode(x + y) == 'x + y'\n    assert ccode(x + inline_) == 'x + inline_'\n\n    x, y = symbols('x, y, restrict')\n    assert ccode(x + y) == 'x + y'\n    assert ccode(x + restrict_) == 'x + restrict_'\n\n    x, y = symbols('x, y, bool')\n    assert ccode(x + y) == 'x + y'\n    assert ccode(x + bool_) == 'x + bool_'\n", "def test_C99CodePrinterFLOAT128():\n    # Test __float128 type support\n    f128 = FloatType('_Float128', float128.nbits, float128.nmant, float128.nexp)\n    p128 = C99CodePrinter(dict(\n        type_aliases={real: f128},\n        type_literal_suffixes={f128: 'Q'},\n        type_func_suffixes={f128: 'f128'},\n        type_math_macro_suffixes={\n            real: 'f128',\n            f128: 'f128'\n        },\n        type_macros={\n            f128: ('__STDC_WANT_IEC_60559_TYPES_EXT__',)\n        }\n    ))\n\n    assert p128.doprint(Sqrt(2)) == 'sqrtf128(2.0Q)'\n    assert p128.doprint(Cbrt(2)) == 'cbrtf128(2.0Q)'\n    assert p128.doprint(hypot(x, 2)) == 'hypotf128(x, 2.0Q)'\n    assert p128.doprint(sin(x + 2)) == 'sinf128(x + 2.0Q)'\n    assert p128.doprint(cos(x + 2)) == 'cosf128(x + 2.0Q)'\n    assert p128.doprint(tan(x + 2)) == 'tanf128(x + 2.0Q)'\n    assert p128.doprint(asin(x + 2)) == 'asinf128(x + 2.0Q)'\n    assert p128.doprint(acos(x + 2)) == 'acosf128(x + 2.0Q)'\n    assert p128.doprint(atan(x + 2)) == 'atanf128(x + 2.0Q)'\n    assert p128.doprint(atan2(x, 2)) == 'atan2f128(x, 2.0Q)'\n    assert p128.doprint(sinh(x + 2)) == 'sinhf128(x + 2.0Q)'\n    assert p128.doprint(cosh(x + 2)) == 'coshf128(x + 2.0Q)'\n    assert p128.doprint(tanh(x + 2)) == 'tanhf128(x + 2.0Q)'\n    assert p128.doprint(asinh(x + 2)) == 'asinhf128(x +"], "sample_524": ["def test_colorbar_visibility():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc)\n    assert cb.ax.get_visible()\n\n    cb.ax.set_visible(False)\n    assert not cb.ax.get_visible()\n\n    cb.ax.set_visible(True)\n    assert cb.ax.get_visible()\n\n    fig.delaxes(cb.ax)\n    assert not cb.ax.get_visible()\n", "def test_colorbar_axes_background_color():\n    fig, ax = plt.subplots()\n    im = ax.imshow(np.random.rand(10, 10))\n    cbar = fig.colorbar(im, ax=ax)\n    assert cbar.ax.get_facecolor() == mpl.rcParams['axes.facecolor']\n    cbar.ax.set_facecolor('red')\n    assert cbar.ax.get_facecolor() == (1.0, 0.0, 0.0, 1.0)\n    plt.draw()\n    assert cbar.ax.get_facecolor() == (1.0, 0.0, 0.0, 1.0)\n", "def test_add_subfigure_twice():\n    fig, ax = plt.subplots()\n    subfig = fig.add_subfigure(ax.get_subplotspec())\n    assert subfig not in fig.subfigs\n    assert subfig not in ax.child_axes\n    subfig = fig.add_subfigure(ax.get_subplotspec(), label='subfig')\n    assert subfig in fig.subfigs\n    assert subfig in ax.child_axes\n    assert subfig.get_label() == 'subfig'\n", "def test_colorbar_update_ticks():\n    # test issue #23893\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc)\n    cb.set_ticks([0, 0.5, 1])\n    assert cb.get_ticks() == [0, 0.5, 1]\n    cb.set_ticks([1, 1.5, 2])\n    assert cb.get_ticks() == [1, 1.5, 2]\n    cb.set_ticks([0.5, 1.5])\n    assert cb.get_ticks() == [0.5, 1.5]\n    cb.set_ticks(np.array([1, 2, 3]))\n    assert cb.get_ticks() == [1, 2, 3]\n    cb.set_ticks(np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]))\n    assert cb.get_ticks() == [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n", "def test_colorbar_picker():\n    fig, ax = plt.subplots()\n    data = np.arange(1200).reshape(30, 40)\n    levels = [0, 200, 400, 600, 800, 1000, 1200]\n\n    plt.contourf(data, levels=levels)\n    cbar = fig.colorbar()\n\n    # The colorbar's picker event should return the correct artist.\n    event = cbar.ax.yaxis._picker(1, 1, cbar.ax)[0]\n    assert event[0] == cbar\n\n    # The colorbar's picker event should return the correct event.\n    event = cbar.ax.yaxis._picker(1, 1, cbar.ax)[1]\n    assert event.x == 1\n    assert event.y == 1\n    assert event.inaxes == cbar.ax\n"], "sample_209": ["    def test_modelstate(self):\n        department = Department.objects.create(id=1, name='department1')\n        worker = Worker.objects.create(name='worker', department=department)\n\n        self.assertEqual(worker._state.db, DEFAULT_DB_ALIAS)\n        self.assertTrue(worker._state.adding)\n\n        worker.save()\n        self.assertFalse(worker._state.adding)\n\n        worker.refresh_from_db()\n        self.assertEqual(worker._state.db, DEFAULT_DB_ALIAS)\n        self.assertFalse(worker._state.adding)\n", "    def test_force_insert_force_update(self):\n        # Test force_insert and force_update together\n        msg = \"Cannot force both insert and updating in model saving.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            Article.objects.create(headline=\"Test Article\", pub_date=datetime.datetime.now()).save(force_insert=True, force_update=True)\n", "    def test_save_force_insert_force_update(self):\n        \"\"\"\n        Regression test for #23388: ensure that save() raises ValueError if\n        both force_insert and force_update are True\n        \"\"\"\n        obj = Article()\n        with self.assertRaisesMessage(ValueError, \"Cannot force both insert and updating in model saving.\"):\n            obj.save(force_insert=True, force_update=True)\n", "def test_modelbase_add_to_class(self):\n    \"\"\"\n    Regression test for #30255: ModelBase.add_to_class() should work with\n    descriptors.\n    \"\"\"\n    class MyDescriptor:\n            self.name = name\n\n            setattr(instance, self.name, value)\n\n    class MyModel(metaclass=ModelBase):\n        pass\n\n    ModelBase.add_to_class(MyModel, 'my_descriptor', MyDescriptor())\n    self.assertEqual(MyModel.my_descriptor.name, 'my_descriptor')\n\n    my_instance = MyModel()\n    my_instance.my_descriptor = 'value'\n    self.assertEqual(my_instance.my_descriptor, 'value')\n", "    def test_modelbase_subclass_exception(self):\n        exception = subclass_exception(\n            'CustomDoesNotExist', \n            (ObjectDoesNotExist,), \n            'tests.test_models', \n            attached_to=Article\n        )\n        self.assertEqual(exception.__name__, 'CustomDoesNotExist')\n        self.assertEqual(exception.__qualname__, 'tests.test_models.Article.CustomDoesNotExist')\n        self.assertEqual(exception.__module__, 'tests.test_models')\n"], "sample_151": ["def test_alter_unique_together_and_indexes(self):\n    \"\"\"\n    Tests that altering unique_together and indexes at the same time works\n    correctly.\n    \"\"\"\n    changes = self.get_changes([self.book_foo_together], [self.book_foo_together_2])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"AlterUniqueTogether\", \"AlterIndexTogether\"])\n    self.assertOperationAttributes(changes, \"otherapp\", 0, 0, name=\"book\", unique_together={(\"title\", \"author\")})\n    self.assertOperationAttributes(changes, \"otherapp\", 0, 1, name=\"book\", index_together={(\"title\", \"author\")})\n", "def test_mti_inheritance_field_addition(self):\n    Animal = ModelState('app', 'Animal', [\n        (\"id\", models.AutoField(primary_key=True)),\n    ])\n    Dog = ModelState('app', 'Dog', [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=200)),\n    ], bases=('app.Animal',))\n    changes = self.get_changes([Animal, Dog], [Animal, Dog.clone(\n        fields=[\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n            (\"breed\", models.CharField(max_length=200)),\n        ]\n    )])\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['AlterField', 'AlterField'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, model_name='dog', name='name')\n    self.assertOperationAttributes(changes, 'app', 0, 1, model_name='dog', name='breed')\n", "def test_inherit_ordering(self):\n    \"\"\"\n    #24542 - Inheritance with a non-abstract parent with an ordering.\n    \"\"\"\n    before = [\n        ModelState('app', 'A', [\n            ('id', models.AutoField(primary_key=True)),\n        ], options={'ordering': ['id']}),\n    ]\n    after = [\n        ModelState('app', 'A', [\n            ('id', models.AutoField(primary_key=True)),\n        ], options={'ordering': ['id']}),\n        ModelState('app', 'B', [], bases=('app.A',)),\n    ]\n    changes = self.get_changes(before, after)\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['CreateModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name=\"B\")\n", "def test_alter_field_with_enum_subclass(self):\n    \"\"\"\n    AlterField from an enum subclass to another enum subclass should work correctly.\n    \"\"\"\n    from django.contrib.postgres.fields import IntegerChoices\n    from django.db import models\n\n    class Color(IntegerChoices):\n        RED = 1, 'Red'\n        GREEN = 2, 'Green'\n\n    class NewColor(IntegerChoices):\n        BLUE = 1, 'Blue'\n        YELLOW = 2, 'Yellow'\n\n    class OldModel(models.Model):\n        field = models.IntegerField(choices=Color.choices)\n\n    class NewModel(models.Model):\n        field = models.IntegerField(choices=NewColor.choices)\n\n    old_state = self.make_project_state([ModelState(\"app\", \"OldModel\", [(\"id\", models.AutoField(primary_key=True)), (\"field\", OldModel._meta.get_field(\"field\"))])])\n    new_state = self.make_project_state([ModelState(\"app\", \"NewModel\", [(\"id\", models.AutoField(primary_key=True)), (\"field\", NewModel._meta.get_field(\"field\"))])])\n    changes = self.get_changes(old_state.models, new_state.models)\n    self.assertNumberMigrations(changes, \"app\", 1)\n    self.assertOperationTypes(changes, \"app\", 0, [\"RenameTable\", \"AlterField\"])\n    self.assertOperationAttributes(changes, \"app\", 0, 0, table=\"newmodel\")\n    self.assertOperationAttributes(changes, \"app\", 0, 1, model_name=\"newmodel\", name=\"field\", preserve_default=True)\n", "def test_add_field_with_base(self):\n    \"\"\"\n    #24628 - The autodetector correctly handles adding fields to models that\n    inherit from a model that has fields with the same name as the ones being\n    added.\n    \"\"\"\n    author_with_name = ModelState('app', 'Author', [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=200)),\n    ])\n    author_with_name_and_book = ModelState('app', 'Author', [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=200)),\n    ])\n    book_with_book = ModelState('app', 'Book', [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"book\", models.CharField(max_length=200)),\n    ])\n    book_with_book_and_author = ModelState('app', 'Book', [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"book\", models.CharField(max_length=200)),\n    ], bases=(\"app.Author\",))\n    changes = self.get_changes(\n        [author_with_name, book_with_book],\n        [author_with_name_and_book, book_with_book_and_author],\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"app\", 1)\n    self.assertOperationTypes(changes, \"app\", 0, [\"CreateModel\"])\n    self.assertOperationAttributes(changes, \"app\", 0, 0, name=\"Book\")\n"], "sample_632": ["def test_ignore_signature_with_docstring():\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--ignore-signatures\", \"--ignore-docstrings\", SIMILAR5, SIMILAR6])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == \"\"\"", "def test_compute_sims():\n    sim = similar.Similar(min_lines=3)\n    lineset1 = similar.LineSet(\"file1\", [\"line1\", \"line2\", \"line3\", \"line4\"])\n    lineset2 = similar.LineSet(\"file2\", [\"line1\", \"line2\", \"line3\", \"line4\"])\n    lineset3 = similar.LineSet(\"file3\", [\"line1\", \"line2\", \"different line\", \"line4\"])\n    sim.linesets = [lineset1, lineset2, lineset3]\n    sims = sim._compute_sims()\n    assert len(sims) == 1\n    assert sims[0] == (3, [(lineset1, 0), (lineset2, 0)])\n", "def test_report_similarities():\n    reporter = Reporter()\n    sect = []\n    stats = {\"nb_duplicated_lines\": 10, \"percent_duplicated_lines\": 20.0}\n    old_stats = {\"nb_duplicated_lines\": 15, \"percent_duplicated_lines\": 25.0}\n    similar.report_similarities(sect, stats, old_stats)\n    expected_lines = [\n        \"\",\n        \"now\",\n        \"previous\",\n        \"difference\",\n        \"\",\n        \"nb_duplicated_lines\",\n        \"10\",\n        \"15\",\n        \"-5\",\n        \"\",\n        \"percent_duplicated_lines\",\n        \"20.00\",\n        \"25.00\",\n        \"-5.00\",\n    ]\n    assert sect[0].rows == [expected_lines]\n", "def test_find_common():\n    sim = similar.Similar(min_lines=3, ignore_comments=True, ignore_docstrings=False)\n    with open(SIMILAR1) as stream1, open(SIMILAR2) as stream2:\n        lineset1 = similar.LineSet(\n            SIMILAR1,\n            stream1.readlines(),\n            ignore_comments=True,\n            ignore_docstrings=False,\n            ignore_imports=False,\n            ignore_signatures=False,\n        )\n        lineset2 = similar.LineSet(\n            SIMILAR2,\n            stream2.readlines(),\n            ignore_comments=True,\n            ignore_docstrings=False,\n            ignore_imports=False,\n            ignore_signatures=False,\n        )\n    sims = list(sim._find_common(lineset1, lineset2))\n    assert len(sims) == 3\n    assert sims[0][0] == 5\n    assert sims[1][0] == 8\n    assert sims[2][0] == 5\n", "def test_similar_files_with_different_encodings():\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([SIMILAR1, SIMILAR2, \"--ignore-imports\"])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == \"\"\""], "sample_872": ["def test_ndcg_score_non_negative_y_true():\n    y_true = np.array([[1, 0, 0], [1, 1, 0]])\n    y_score = np.array([[0.9, 0.1, 0.0], [0.9, 0.1, 0.0]])\n    assert ndcg_score(y_true, y_score) == 1.0\n", "def test_ndcg_score_on_sparse_y_true():\n    y_true = np.random.RandomState(42).randint(0, 2, size=(100, 10))\n    y_true_sparse = csr_matrix(y_true)\n\n    y_score = np.random.RandomState(42).rand(100, 10)\n\n    score_dense = ndcg_score(y_true, y_score)\n    score_sparse = ndcg_score(y_true_sparse, y_score)\n\n    assert_allclose(score_dense, score_sparse)\n", "def test_roc_auc_score_input_validation():\n    # Test that roc_auc_score raises an error when trying to compute AUC\n    # for non-probabilistic scores\n    y_true = np.array([0, 0, 1, 1])\n    y_score = np.array([0, 0.5, 0.5, 1])\n    with pytest.raises(ValueError):\n        roc_auc_score(y_true, y_score, multi_class=\"ovr\")\n", "def test_top_k_accuracy_score_multiclass_k_value_error():\n    y_true = np.array([0, 1, 1, 2])\n    y_score = np.array(\n        [\n            [0.2, 0.1, 0.7],\n            [0.4, 0.3, 0.3],\n            [0.3, 0.4, 0.3],\n            [0.4, 0.5, 0.1],\n        ]\n    )\n\n    # Test that k should be positive and integer\n    with pytest.raises(ValueError, match=r\"k should be a positive integer\"):\n        top_k_accuracy_score(y_true, y_score, k=0)\n\n    with pytest.raises(ValueError, match=r\"k should be a positive integer\"):\n        top_k_accuracy_score(y_true, y_score, k=2.5)\n\n    with pytest.raises(ValueError, match=r\"k should be a positive integer\"):\n        top_k_accuracy_score(y_true, y_score, k=-1)\n", "def test_roc_auc_score_multiclass_one_vs_rest():\n    # Test that roc_auc_score function returns the same result\n    # as roc_auc_score for one-vs-rest\n    y_true = [1, 2, 3, 0]\n    y_score = [[0.5, 0.3, 0.1, 0.1], [0.2, 0.5, 0.2, 0.1], [0.1, 0.2, 0.5, 0.2], [0.2, 0.2, 0.3, 0.3]]\n\n    assert_almost_equal(\n        roc_auc_score(y_true, y_score, multi_class=\"ovr\"),\n        [roc_auc_score([1, 0, 0, 0], y_score[:, 0]),\n         roc_auc_score([0, 1, 0, 0], y_score[:, 1]),\n         roc_auc_score([0, 0, 1, 0], y_score[:, 2]),\n         roc_auc_score([0, 0, 0, 1], y_score[:, 3])],\n    )\n\n    # Test micro-averaged multiclass ROC AUC\n    assert_almost_equal(\n        roc_auc_score(y_true, y_score, average=\"micro\", multi_class=\"ovr\"),\n        roc_auc_score(label_binarize(y_true, classes=[0, 1, 2, 3]), y_score, average=\"micro\"),\n    )\n\n    # Test weighted macro-averaged multiclass ROC AUC\n    assert_almost_equal(\n        roc_auc_score(y_true, y_score, average=\"weighted\", multi_class=\"ovr\"),\n        np.array(\n            [\n                roc_auc_score([1, 0, 0, 0], y_score[:, 0]),\n                roc_auc_score([0, 1, 0, 0], y_score[:, 1]),\n                roc_auc_score([0, 0, 1, 0], y_score[:, 2]),\n                roc_auc_score([0, 0, 0, 1], y_score[:, 3]),\n            ]\n        ).mean(),\n    )\n\n    # Test macro-averaged multiclass ROC AUC\n    assert_almost_equal(\n        roc_auc_score(y_true, y_score, average=\"macro\", multi_class=\"ovr\"),\n        np.array(\n            [\n                roc_auc_score"], "sample_714": ["def test_hamming_loss_multilabel_sparse():\n    # Test that hamming loss is correctly calculated for sparse multilabel\n    # indicator matrix\n    y_true = np.array([[0, 1, 1], [1, 0, 1]])\n    y_pred = np.array([[0, 0, 1], [1, 0, 1]])\n\n    assert_equal(hamming_loss(y_true, y_pred), 1 / 6)\n    assert_equal(hamming_loss(y_true, y_true), 0)\n    assert_equal(hamming_loss(y_pred, y_pred), 0)\n    assert_equal(hamming_loss(y_pred, 1 - y_pred), 1)\n    assert_equal(hamming_loss(y_true, 1 - y_true), 1)\n    assert_equal(hamming_loss(y_true, np.zeros(y_true.shape)), 4 / 6)\n    assert_equal(hamming_loss(y_pred, np.zeros(y_true.shape)), 0.5)\n    assert_equal(hamming_loss(y_true, y_pred, sample_weight=[1, 3]), 1. / 12)\n    assert_equal(hamming_loss(y_true, 1-y_pred, sample_weight=[1, 3]), 11. / 12)\n    assert_equal(hamming_loss(y_true, np.zeros_like(y_true), sample_weight=[1, 3]), 2. / 3)\n", "def test_confusion_matrix_multilabel_and_sample_weights():\n    \"\"\"Test confusion matrix - multi-label case with sample weights\"\"\"\n    y_true = np.array([[0, 1, 1], [1, 0, 1]])\n    y_pred = np.array([[0, 0, 1], [1, 0, 1]])\n    sample_weight = np.array([0.2, 0.8])\n\n    # Compute confusion matrix with sample weights\n    cm = confusion_matrix(y_true, y_pred, sample_weight=sample_weight)\n\n    # Compute confusion matrix for each sample separately and sum them\n    cm_sep = np.zeros((3, 3))\n    for i in range(len(y_true)):\n        cm_sep += sample_weight[i] * np.outer(y_true[i], y_pred[i])\n\n    assert_array_almost_equal(cm, cm_sep)\n", "def test_check_targets_multilabel_binary_input():\n    # Test _check_targets when y_true and y_pred are both binary\n    y_true = [0, 1, 1, 0]\n    y_pred = [1, 1, 1, 0]\n    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n    assert_equal(y_type, 'binary')\n    assert_array_equal(y_true, np.array([0, 1, 1, 0]))\n    assert_array_equal(y_pred, np.array([1, 1, 1, 0]))\n\n    # Test when y_true and y_pred have different types\n    y_true = [0, 1, 1, 0]\n    y_pred = [[1], [1], [1], [0]]\n    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n    assert_equal(y_type, 'multiclass')\n    assert_array_equal(y_true, np.array([0, 1, 1, 0]))\n    assert_array_equal(y_pred, np.array([1, 1, 1, 0]))\n", "def test_multilabel_f1_score():\n    # Test multilabel f1_score against precision_recall_fscore_support\n\n    y_true = np.array([[1, 0, 1],\n                       [0, 1, 0]])\n    y_pred = np.array([[0, 1, 1],\n                       [0, 1, 1]])\n\n    f1_score_multilabel = f1_score(y_true, y_pred, average='samples')\n    precision, recall, fscore, _ = precision_recall_fscore_support(y_true,\n                                                                 y_pred,\n                                                                 average='samples')\n    f1_score_prfs = fscore\n\n    assert_almost_equal(f1_score_multilabel, f1_score_prfs)\n", "def test_fbeta_score_multiclass():\n    # Test Fbeta Score for multiclass classification task\n    y_true, y_pred, _ = make_prediction(binary=False)\n\n    # compute scores with default labels introspection\n    for beta in [1, 2, 0.5]:\n        p, r, f, s = precision_recall_fscore_support(y_true, y_pred,\n                                                     beta=beta, average=None)\n        assert_array_almost_equal(f, [0.81, 0.15, 0.57], 2)\n        assert_array_equal(s, [24, 31, 20])\n\n    # averaging tests\n    fs = fbeta_score(y_true, y_pred, beta=2, average='micro')\n    assert_array_almost_equal(fs, 0.53, 2)\n\n    fs = fbeta_score(y_true, y_pred, beta=2, average='macro')\n    assert_array_almost_equal(fs, 0.51, 2)\n\n    fs = fbeta_score(y_true, y_pred, beta=2, average='weighted')\n    assert_array_almost_equal(fs, 0.47, 2)\n\n    # same prediction but with and explicit label ordering\n    for beta in [1, 2, 0.5]:\n        f, _, _, _ = precision_recall_fscore_support(y_true, y_pred, beta=beta,\n                                                    labels=[0, 2, 1], average=None)\n        assert_array_almost_equal(f, [0.81, 0.57, 0.15], 2)\n\n    # explicit average='samples' should fail for multiclass\n    assert_raises(ValueError, fbeta_score, y_true, y_pred, beta=2,\n                  average='samples')\n"], "sample_433": ["def test_alter_index_together_to_index_together(self):\n    changes = self.get_changes(\n        [AutodetectorTests.author_empty, self.book_index_together],\n        [AutodetectorTests.author_empty, self.book_index_together_2],\n    )\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"AlterIndexTogether\"])\n    self.assertOperationAttributes(\n        changes, \"otherapp\", 0, 0, name=\"book\", index_together={(\"title\", \"author\")}\n    )\n", "def test_suggest_name_with_operation_name_fragment_collision(self):\n    class Migration(migrations.Migration):\n        operations = [\n            migrations.CreateModel(\"Person\", fields=[]),\n            migrations.AddConstraint(\n                \"Person\",\n                models.UniqueConstraint(\n                    fields=[\"name\"], name=\"person\"\n                ),\n            ),\n        ]\n\n    migration = Migration(\"0001_initial\", \"test_app\")\n    suggest_name = migration.suggest_name()\n    self.assertEqual(suggest_name, \"person_and_more\")\n", "def test_alter_proxy_model_options(self):\n    \"\"\"\n    Changing a proxy model's options after it has been created as a non-proxy\n    model should also make a change.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_proxy_notproxy, self.author_empty],\n        [self.author_proxy_options, self.author_empty],\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelOptions\"])\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        name=\"authorproxy\",\n        options={\"verbose_name\": \"Super Author\"},\n    )\n", "def test_rename_model_with_indexes(self):\n    author_empty = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n        ],\n    )\n    author_with_indexes = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n        ],\n        {\n            \"indexes\": [\n                models.Index(fields=[\"id\"], name=\"author_id_idx\"),\n            ],\n        },\n    )\n    new_author = ModelState(\n        \"testapp\",\n        \"NewAuthor\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n        ],\n        {\n            \"indexes\": [\n                models.Index(fields=[\"id\"], name=\"newauthor_id_idx\"),\n            ],\n        },\n    )\n\n    changes = self.get_changes(\n        [author_empty],\n        [new_author],\n        MigrationQuestioner({\"ask_rename_model\": True}),\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameModel\", \"RenameIndex\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, old_name=\"Author\", new_name=\"NewAuthor\"\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        1,\n        model_name=\"newauthor\",\n        new_name=\"newauthor_id_idx\",\n        old_name=\"author_id_idx\",\n    )\n", "def test_mutation_on_operations(self):\n    \"\"\"Mutation on the 'operations' attribute doesn't affect the migration.\"\"\"\n    migration = Migration(\"some_migration\", \"test_app\")\n    migration.operations.append(migrations.CreateModel(\"Person\", fields=[]))\n    new_operations = migration.operations[:]\n    self.assertEqual(new_operations, [migrations.CreateModel(\"Person\", fields=[])])\n    migration.operations[0].fields.append((\"age\", models.IntegerField()))\n    self.assertEqual(\n        new_operations, [migrations.CreateModel(\"Person\", fields=[(\"age\", models.IntegerField())])]\n    )\n"], "sample_414": ["    def test_empty_value_display(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            empty_value_display = \"unknown\"\n\n        ma = MyModelAdmin(None, admin.site)\n        self.assertEqual(ma.get_empty_value_display(), mark_safe(\"unknown\"))\n\n        class MyAdminSite(admin.AdminSite):\n            empty_value_display = \"unknown\"\n\n        ma = MyModelAdmin(None, MyAdminSite())\n        self.assertEqual(ma.get_empty_value_display(), mark_safe(\"unknown\"))\n", "    def test_fieldsets_with_fields(self):\n        class AlbumAdmin(admin.ModelAdmin):\n            fieldsets = [\n                (None, {\"fields\": (\"name\",)}),\n                (\"Details\", {\"fields\": (\"band\", \"release_date\")}),\n            ]\n\n        album_admin = AlbumAdmin(Album, admin.site)\n        fieldsets = album_admin.get_fieldsets(request=None)\n        self.assertEqual(len(fieldsets), 2)\n        self.assertEqual(fieldsets[0][1][\"fields\"], [\"name\"])\n        self.assertEqual(fieldsets[1][1][\"fields\"], [\"band\", \"release_date\"])\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n        self.band = Band.objects.create(name=\"Bogey Blues\")\n        self.event = Event.objects.create(\n            name=\"Blues Fest\",\n            main_band=self.band,\n            start_date=\"2022-01-01\",\n            start_time=\"14:00:00\",\n        )\n        self.url = reverse(\"admin:admin_widgets_event_change\", args=(self.event.id,))\n", "    def __init__(self, model, admin_site):\n        self.model = model\n        self.opts = model._meta\n        self.admin_site = admin_site\n", "    def test_ordering_for_relation_fields(self):\n        from admin_widgets.models import Orderable, ForeignKeyTarget, OrderableAdmin\n\n        admin.site.register(ForeignKeyTarget, OrderableAdmin)\n        target = ForeignKeyTarget.objects.create()\n        orderable = Orderable.objects.create(target=target)\n        admin.site.unregister(ForeignKeyTarget)\n\n        self.client.force_login(self.superuser)\n        response = self.client.get(reverse(\"admin:admin_widgets_orderable_changelist\"))\n        self.assertEqual(response.status_code, 200)\n\n        # Ensure that the admin changelist page uses the related field's\n        # ordering specified by the model admin.\n        self.assertContains(response, '<th class=\"column-target\"')\n"], "sample_561": ["def test_marker_filling():\n    marker = markers.MarkerStyle(marker='o', fillstyle='full')\n    assert marker.get_fillstyle() == 'full'\n    assert marker.is_filled()\n\n    marker = markers.MarkerStyle(marker='o', fillstyle='none')\n    assert marker.get_fillstyle() == 'none'\n    assert not marker.is_filled()\n\n    marker = markers.MarkerStyle(marker='o', fillstyle='left')\n    assert marker.get_fillstyle() == 'left'\n    assert marker.is_filled()\n\n    marker = markers.MarkerStyle(marker='o', fillstyle='right')\n    assert marker.get_fillstyle() == 'right'\n    assert marker.is_filled()\n\n    marker = markers.MarkerStyle(marker='o', fillstyle='top')\n    assert marker.get_fillstyle() == 'top'\n    assert marker.is_filled()\n\n    marker = markers.MarkerStyle(marker='o', fillstyle='bottom')\n    assert marker.get_fillstyle() == 'bottom'\n    assert marker.is_filled()\n", "def test_marker_fillstyle_alt_transform():\n    marker_style = markers.MarkerStyle(marker='o', fillstyle='left')\n    assert marker_style.get_fillstyle() == 'left'\n    assert marker_style.is_filled()\n    assert marker_style.get_alt_transform() is not None\n    assert marker_style.get_alt_transform() != marker_style.get_transform()\n\n    marker_style = markers.MarkerStyle(marker='o', fillstyle='right')\n    assert marker_style.get_fillstyle() == 'right'\n    assert marker_style.is_filled()\n    assert marker_style.get_alt_transform() is not None\n    assert marker_style.get_alt_transform() != marker_style.get_transform()\n\n    marker_style = markers.MarkerStyle(marker='o', fillstyle='top')\n    assert marker_style.get_fillstyle() == 'top'\n    assert marker_style.is_filled()\n    assert marker_style.get_alt_transform() is not None\n    assert marker_style.get_alt_transform() != marker_style.get_transform()\n\n    marker_style = markers.MarkerStyle(marker='o', fillstyle='bottom')\n    assert marker_style.get_fillstyle() == 'bottom'\n    assert marker_style.is_filled()\n    assert marker_style.get_alt_transform() is not None\n    assert marker_style.get_alt_transform() != marker_style.get_transform()\n", "def test_marker_fillstyle_iterate():\n    marker = markers.MarkerStyle(marker='o', fillstyle='none')\n    fillstyles = ['full', 'left', 'right', 'bottom', 'top', 'none']\n    for fillstyle in fillstyles:\n        marker._set_fillstyle(fillstyle)\n        assert marker.get_fillstyle() == fillstyle\n        if fillstyle == 'none':\n            assert not marker.is_filled()\n        else:\n            assert marker.is_filled()\n", "def test_marker_style_copy():\n    marker_style = markers.MarkerStyle(marker='o', fillstyle='none')\n    new_marker_style = markers.MarkerStyle(marker_style)\n\n    assert new_marker_style.get_fillstyle() == 'none'\n    assert new_marker_style.get_marker() == 'o'\n    assert not new_marker_style.is_filled()\n\n    # Check that the new marker style is a copy, not the same object.\n    assert new_marker_style is not marker_style\n    assert new_marker_style._path is not marker_style._path\n    assert new_marker_style._transform is not marker_style._transform\n\n    # Check that modifying the new marker style does not affect the original.\n    new_marker_style._set_fillstyle('full')\n    assert new_marker_style.get_fillstyle() == 'full'\n    assert marker_style.get_fillstyle() == 'none'\n", "def test_marker_clone():\n    marker = markers.MarkerStyle(\"o\")\n    marker_clone = markers.MarkerStyle(marker)\n\n    assert marker_clone.get_marker() == marker.get_marker()\n    assert marker_clone.get_fillstyle() == marker.get_fillstyle()\n    assert marker_clone.get_joinstyle() == marker.get_joinstyle()\n    assert marker_clone.get_capstyle() == marker.get_capstyle()\n    assert marker_clone.get_user_transform() == marker.get_user_transform()\n\n    # Check that changing the clone does not affect the original.\n    marker_clone._set_fillstyle('none')\n    assert marker_clone.get_fillstyle() != marker.get_fillstyle()\n    marker_clone._set_joinstyle('bevel')\n    assert marker_clone.get_joinstyle() != marker.get_joinstyle()\n    marker_clone._set_capstyle('projecting')\n    assert marker_clone.get_capstyle() != marker.get_capstyle()\n    marker_clone._user_transform = Affine2D().translate(1, 1)\n    assert marker_clone.get_user_transform() != marker.get_user_transform()\n"], "sample_374": ["    def test_base_iterable_subclass(self):\n        class ModelIterableSubclass(ModelIterable):\n                yield from super().__iter__()\n\n        class TestModel(models.Model):\n            pass\n\n        qs = TestModel.objects_custom.all()\n        self.assertIsInstance(qs._iterable_class(), ModelIterableSubclass)\n", "    def setUpTestData(cls):\n        cls.teacher = Teacher.objects.create(name='Mr. Smith')\n        cls.dept = Department.objects.create(name='Math', teacher=cls.teacher)\n        cls.dept2 = Department.objects.create(name='Science', teacher=cls.teacher)\n", "    def setUpTestData(cls):\n        cls.book = Book.objects.create(title='Poems')\n        cls.book_with_year = BookWithYear.objects.create(book=cls.book, published_year=2010)\n        cls.article = Article.objects.create(name='Django')\n        cls.comment = Comment.objects.create(comment='awesome', content_object=cls.book_with_year)\n", "    def setUp(self):\n        self.book1 = Book.objects.create(title='Poems')\n        self.book2 = Book.objects.create(title='Jane Eyre')\n        self.book3 = Book.objects.create(title='Wuthering Heights')\n        self.author1 = Author.objects.create(name='Charlotte', first_book=self.book1)\n        self.author2 = Author.objects.create(name='Anne', first_book=self.book1)\n        self.author3 = Author.objects.create(name='Emily', first_book=self.book1)\n        self.author4 = Author.objects.create(name='Jane', first_book=self.book3)\n\n        self.book1.authors.add(self.author1, self.author2, self.author3)\n        self.book2.authors.add(self.author1)\n        self.book3.authors.add(self.author4)\n", "    def test_reverse_one_to_one(self):\n        bio1 = Bio.objects.create(author=self.author1)\n        bio2 = Bio.objects.create(author=self.author2)\n\n        with self.assertNumQueries(2):\n            authors = Author.objects.select_related('bio')\n            bios = [a.bio for a in authors]\n        self.assertEqual(bios, [bio1, bio2, bio2, None])\n"], "sample_1044": ["def test_Pow_as_content_primitive():\n    from sympy import sqrt, Rational\n    assert Pow(Rational(4), Rational(1, 2)).as_content_primitive() == (2, sqrt(1))\n    assert Pow(Rational(-4), Rational(1, 2)).as_content_primitive() == (2, sqrt(-1))\n    assert Pow(Rational(4), Rational(3, 2)).as_content_primitive() == (8, sqrt(1))\n    assert Pow(Rational(-4), Rational(3, 2)).as_content_primitive() == (8, sqrt(-1))\n    assert Pow(8, Rational(1, 3)).as_content_primitive() == (2, Pow(2, Rational(1, 3)))\n    assert Pow(27, Rational(1, 3)).as_content_primitive() == (3, Pow(3, Rational(1, 3)))\n    assert Pow(-8, Rational(1, 3)).as_content_primitive() == (2, Pow(-1, Rational(1, 3))*Pow(2, Rational(1, 3)))\n    assert Pow(-27, Rational(1, 3)).as_content_primitive() == (3, Pow(-1, Rational(1, 3))*Pow(3, Rational(1, 3)))\n", "def test_Pow_as_numer_denom():\n    x = Symbol('x')\n    p = Symbol('p', positive=True)\n    n = Symbol('n', negative=True)\n    r = Rational(3, 2)\n    i = I\n\n    assert Pow(r, x).as_numer_denom() == (Pow(3, x), Pow(2, x))\n    assert Pow(-r, x).as_numer_denom() == (Pow(3, x), Pow(2, x))\n    assert Pow(x, 2).as_numer_denom() == (Pow(x, 2), 1)\n    assert Pow(x, r).as_numer_denom() == (Pow(x, r), 1)\n    assert Pow(p, x).as_numer_denom() == (Pow(p, x), 1)\n    assert Pow(n, x).as_numer_denom() == (Pow(-n, x), Pow(-1, x))\n    assert Pow(x, x).as_numer_denom() == (Pow(x, x), 1)\n    assert Pow(i, x).as_numer_denom() == (Pow(i, x), 1)\n", "def test_Pow_is_polar():\n    from sympy import I, sqrt, exp, symbols\n    a, b = symbols('a b', real=True)\n\n    assert Pow(I, 2, evaluate=False).is_polar is True\n    assert Pow(sqrt(2)*I, 2, evaluate=False).is_polar is True\n    assert Pow(sqrt(2), 2, evaluate=False).is_polar is False\n    assert Pow(exp(I*pi), 2, evaluate=False).is_polar is True\n    assert Pow(a + b*I, 2, evaluate=False).is_polar is None\n    assert Pow(a + b*I, 0, evaluate=False).is_polar is False\n    assert Pow(I, a, evaluate=False).is_polar is None\n", "def test_pow_assumptions():\n    # issue 9668\n    b = Symbol('b', nonnegative=True)\n    e = Symbol('e', real=True)\n    assert (b**e).is_real is True\n    assert (b**e).is_positive is None\n    assert (b**e).is_nonnegative is True\n\n    b = Symbol('b', positive=True)\n    e = Symbol('e', real=True)\n    assert (b**e).is_real is True\n    assert (b**e).is_positive is True\n    assert (b**e).is_nonnegative is True\n\n    b = Symbol('b', nonpositive=True)\n    e = Symbol('e', real=True)\n    assert (b**e).is_real is True\n    assert (b**e).is_positive is None\n    assert (b**e).is_nonpositive is True\n\n    b = Symbol('b', negative=True)\n    e = Symbol('e', real=True)\n    assert (b**e).is_real is None\n    assert (b**e).is_positive is None\n    assert (b**e).is_nonnegative is None\n\n    b = Symbol('b', nonnegative=True)\n    e = Symbol('e', even=True)\n    assert (b**e).is_real is True\n    assert (b**e).is_positive is True\n    assert (b**e).is_nonnegative is True\n\n    b = Symbol('b', nonnegative=True)\n    e = Symbol('e', odd=True)\n    assert (b**e).is_real is True\n    assert (b**e).is_positive is True\n    assert (b**e).is_nonnegative is True\n\n    b = Symbol('b', nonpositive=True)\n    e = Symbol('e', odd=True)\n    assert (b**e).is_real is True\n    assert (b**e).is_negative is True\n    assert (b**e).is_nonpositive is True\n\n    b = Symbol('b', negative=True)\n    e = Symbol('e', even=True)\n    assert (b**e).is_real is True\n    assert (b**e).is_positive is True\n    assert (b**e).is_nonnegative is True\n\n    b = Symbol('b', negative=True)\n    e = Symbol('e', odd=True)\n    assert (b**e).is_real is True\n    assert", "def test_Pow_is_algebraic_transcendental():\n    from sympy import sin, cos\n    x = Symbol('x', transcendental=True)\n    z = Symbol('z', transcendental=False)\n    assert (sin(x)**2).is_algebraic is False\n    assert (sin(x)**2).is_transcendental\n    assert (cos(x)**2).is_algebraic is False\n    assert (cos(x)**2).is_transcendental\n    assert (x**2).is_algebraic is False\n    assert (x**2).is_transcendental\n    assert (z**2).is_transcendental is False\n    assert (sin(x) + 2).is_algebraic is False\n    assert (sin(x) + 2).is_transcendental\n    assert (sin(x) + x).is_algebraic is False\n    assert (sin(x) + x).is_transcendental\n    assert (sin(x) + cos(x)).is_algebraic is False\n    assert (sin(x) + cos(x)).is_transcendental\n    assert (sin(x)*cos(x)).is_algebraic is False\n    assert (sin(x)*cos(x)).is_transcendental\n    assert (sin(x) - sin(x)).is_transcendental is False\n    assert (sin(x) - cos(x)).is_transcendental\n    assert (sin(x)*x).is_algebraic is False\n    assert (sin(x)*x).is_transcendental\n    assert (x*sin(x)).is_algebraic is False\n    assert (x*sin(x)).is_transcendental\n    assert (sin(x) - x).is_algebraic is False\n    assert (sin(x) - x).is_transcendental\n    assert (x - sin(x)).is_algebraic is False\n    assert (x - sin(x)).is_transcendental\n"], "sample_1189": ["def test_tensorflow_fresnel():\n    if not tensorflow:\n        skip(\"tensorflow not installed.\")\n\n    f1 = fresnelc(x)\n    f2 = fresnels(x)\n    F1 = lambdify(x, f1, modules='tensorflow')\n    F2 = lambdify(x, f2, modules='tensorflow')\n\n    with tensorflow.compat.v1.Session() as s:\n        assert abs(F1(1.3).eval(session=s) - fresnelc(1.3).evalf()) <= 1e-10\n        assert abs(F2(1.3).eval(session=s) - fresnels(1.3).evalf()) <= 1e-10\n", "def test_tensorflow_logical_ops_with_tensor():\n    if not tensorflow:\n        skip(\"tensorflow not installed.\")\n    expr = And(x, y, z)\n    func = lambdify([x, y, z], expr, modules=\"tensorflow\")\n\n    with tensorflow.compat.v1.Session() as s:\n        a = tensorflow.constant(True)\n        b = tensorflow.constant(False)\n        c = tensorflow.constant(True)\n        assert func(a, b, c).eval(session=s) == False\n", "def test_tensorflow_keyword_args():\n    if not tensorflow:\n        skip(\"tensorflow not installed.\")\n\n    f = lambdify(x, sin(x), modules=\"tensorflow\")\n    with tensorflow.compat.v1.Session() as s:\n        a = tensorflow.constant(1, dtype=tensorflow.float32)\n        raises(TypeError, lambda: f(a=a))\n", "def test_issue_23541():\n    if not numpy:\n        skip(\"numpy not installed\")\n    f = lambdify((x, y, z), Matrix([[z, y, x], [x**2 + y**2 + z**2, x + y + z]]), modules=\"numpy\")\n    numpy.testing.assert_array_equal(f(1, 2, 3), numpy.array([[3, 2, 1], [14, 6]]))\n", "def test_issue_24135():\n    if not tensorflow:\n        skip(\"Tensorflow not installed\")\n    # Test that tensorflow lambdify function can handle complex inputs\n    f = lambdify(x, x**2, \"tensorflow\")\n    z = tensorflow.complex(1, 2)\n    assert f(z).dtype == tensorflow.complex128\n    # Test that tensorflow lambdify function can handle complex outputs\n    f = lambdify(x, x**2 + 1j*x, \"tensorflow\")\n    z = tensorflow.complex(1, 2)\n    assert f(z).dtype == tensorflow.complex128\n"], "sample_816": ["def test_hashingvectorizer_ngram_range(ngram_range):\n    hv = HashingVectorizer(ngram_range=ngram_range)\n    assert hv.ngram_range == ngram_range\n    assert hv._get_hasher().ngram_range == ngram_range\n", "def test_vectorizer_uppercase_input(Estimator):\n    vect = Estimator(lowercase=False)\n    X = vect.fit_transform([\"This is a test case.\"]).toarray()\n    assert_equal(vect.vocabulary_, {\"This\": 0, \"is\": 1, \"a\": 2, \"test\": 3, \"case\": 4})\n    assert_array_equal(X, [[1, 1, 1, 1, 1]])\n\n    vect = Estimator(lowercase=True)\n    X = vect.fit_transform([\"This is a test case.\"]).toarray()\n    assert_equal(vect.vocabulary_, {\"this\": 0, \"is\": 1, \"a\": 2, \"test\": 3, \"case\": 4})\n    assert_array_equal(X, [[1, 1, 1, 1, 1]])\n", "def test_vectorizer_lowercase_parameter(Estimator, strip_accents):\n    # Test lowercase parameter\n    text = \"This is a test.\\n This is only a test.\"\n    vectorizer = Estimator(lowercase=False, strip_accents=strip_accents)\n    X = vectorizer.fit_transform([text])\n    features = vectorizer.get_feature_names()\n    assert 'this' not in features\n    assert 'This' in features\n\n    vectorizer = Estimator(lowercase=True, strip_accents=strip_accents)\n    X = vectorizer.fit_transform([text])\n    features = vectorizer.get_feature_names()\n    assert 'this' in features\n    assert 'This' not in features\n", "def test_tfidf_vectorizer_fixed_vocabulary():\n    # Test that TfidfVectorizer's vocabulary is not modified by fit_transform\n    # or transform\n    vocab = [\"pizza\", \"burger\", \"celeri\"]\n    vect = TfidfVectorizer(vocabulary=vocab)\n    original_vocab = vect.vocabulary_.copy()\n\n    vect.fit_transform(ALL_FOOD_DOCS)\n    assert vect.fixed_vocabulary_\n    assert vect.vocabulary_ == original_vocab\n\n    vect.transform(ALL_FOOD_DOCS)\n    assert vect.vocabulary_ == original_vocab\n", "def test_vectorizer_analyzer_invalid_input():\n    # Check that VectorizerMixin's _check_stop_words_consistency() method\n    # behaves correctly when encountering invalid input\n\n    class MockVectorizer(VectorizerMixin):\n            self.stop_words = stop_words\n            self.analyzer = analyzer\n            self.preprocessor = preprocessor\n            self.tokenizer = tokenizer\n\n            return self.stop_words\n\n            if self.analyzer == 'callable':\n                return self.analyzer_callable\n            else:\n                super().build_analyzer()\n\n            if isinstance(doc, str):\n                return self.analyzer_string(doc)\n            elif isinstance(doc, list):\n                return self.analyzer_list(doc)\n            else:\n                raise TypeError(f'Invalid input type: {type(doc)}')\n\n            return doc.split()\n\n            return [item.split() for item in doc]\n\n    stop_words = {'a', 'b'}\n    analyzer_invalid_input = MockVectorizer(stop_words, 'callable', None, None)\n    preprocess_invalid_input = MockVectorizer(stop_words, 'word', lambda x: 1, None)\n    tokenize_invalid_input = MockVectorizer(stop_words, 'word', None, lambda x: 1)\n\n    # Test with invalid input for analyzer\n    assert analyzer_invalid_input._check_stop_words_consistency(stop_words,\n                                                               analyzer_invalid_input.build_preprocessor(),\n                                                               analyzer_invalid_input.build_tokenizer()) == 'error'\n    # Test with invalid input for preprocessor\n    assert preprocess_invalid_input._check_stop_words_consistency(stop_words,\n                                                                preprocess_invalid_input.build_preprocessor(),\n                                                                preprocess_invalid_input.build_tokenizer()) == 'error'\n    # Test with invalid input for tokenizer\n    assert tokenize_invalid_input._check_stop_words_consistency(stop_words,\n                                                               tokenize_invalid_input.build_preprocessor(),\n                                                               tokenize_invalid_input.build_tokenizer()) == 'error'\n"], "sample_621": ["    def test_filter_indexes_from_coords(self) -> None:\n        indexes = {\n            \"x\": PandasIndex([1, 2, 3], \"x\"),\n            \"y\": PandasIndex([4, 5, 6], \"y\"),\n            \"z\": PandasMultiIndex(\n                pd.MultiIndex.from_product([[\"a\", \"b\"], [1, 2]], names=(\"one\", \"two\")),\n                \"z\",\n            ),\n        }\n\n        filtered_indexes = filter_indexes_from_coords(indexes, {\"x\", \"one\"})\n\n        assert filtered_indexes.keys() == {\"x\", \"one\"}\n        assert filtered_indexes[\"x\"] is indexes[\"x\"]\n        assert filtered_indexes[\"one\"] is indexes[\"z\"]\n\n        filtered_indexes = filter_indexes_from_coords(indexes, {\"x\"})\n\n        assert filtered_indexes.keys() == {\"x\"}\n        assert filtered_indexes[\"x\"] is indexes[\"x\"]\n\n        filtered_indexes = filter_indexes_from_coords(indexes, set())\n\n        assert filtered_indexes == {}\n", "    def test_copy_indexes_deep(self, indexes) -> None:\n        copied, index_vars = indexes.copy_indexes(deep=True)\n\n        assert copied.keys() == indexes.keys()\n        for new, original in zip(copied.values(), indexes.values()):\n            assert new.equals(original)\n            assert new is not original\n            assert new.index is not original.index\n\n        assert index_vars.keys() == indexes.variables.keys()\n        for new, original in zip(index_vars.values(), indexes.variables.values()):\n            assert_identical(new, original)\n\n        # ensure deep copy by changing original and asserting it does not affect the copy\n        original = list(indexes.values())[0]\n        original.index = original.index.rename(\"new_name\")\n        for new in copied.values():\n            assert new.index.name != \"new_name\"\n", "    def test_copy_indexes(self) -> None:\n        x_idx = PandasIndex(pd.Index([1, 2, 3], name=\"x\"), \"x\")\n        y_idx = PandasIndex(pd.Index([4, 5, 6], name=\"y\"), \"y\")\n        z_pd_midx = pd.MultiIndex.from_product(\n            [[\"a\", \"b\"], [1, 2]], names=[\"one\", \"two\"]\n        )\n        z_midx = PandasMultiIndex(z_pd_midx, \"z\")\n\n        indexes = Indexes(\n            {\n                \"x\": x_idx,\n                \"y\": y_idx,\n                \"z\": z_midx,\n                \"one\": z_midx,\n                \"two\": z_midx,\n            },\n            {k: v.create_variables() for k, v in [(\"x\", x_idx), (\"y\", y_idx)]},\n        )\n\n        copied_indexes, copied_variables = indexes.copy_indexes(deep=False)\n        assert copied_indexes.keys() == indexes.keys()\n        for new, original in zip(copied_indexes.values(), indexes.values()):\n            assert new.equals(original)\n        # check unique index objects preserved\n        assert copied_indexes[\"z\"] is copied_indexes[\"one\"] is copied_indexes[\"two\"]\n\n        assert copied_variables.keys() == indexes.variables.keys()\n        for new, original in zip(copied_variables.values(), indexes.variables.values()):\n            assert_identical(new, original)\n\n        # check indexes are shallow-copied (i.e. underlying pandas indexes not copied)\n        assert copied_indexes[\"x\"].index is indexes[\"x\"].index\n        assert copied_indexes[\"z\"].index is indexes[\"z\"].index\n", "    def test_roll_indexes(self) -> None:\n        # test rolling indexes with integer values\n        indexes = Indexes(\n            {\n                \"x\": PandasIndex([0, 1, 2], \"x\"),\n                \"y\": PandasMultiIndex(\n                    pd.MultiIndex.from_product([[\"a\", \"b\"], [1, 2]]),\n                    \"y\",\n                ),\n            },\n            {\"x\": xr.Variable(\"x\", [0, 1, 2]), \"y\": xr.Variable(\"y\", [0, 1, 2, 3])},\n        )\n        expected_indexes = {\n            \"x\": PandasIndex([2, 0, 1], \"x\"),\n            \"y\": PandasMultiIndex(\n                pd.MultiIndex.from_product([[\"a\", \"b\"], [1, 2]])[[2, 0, 1, 3]],\n                \"y\",\n            ),\n        }\n        new_indexes, _ = roll_indexes(indexes, {\"x\": 1, \"y\": 1})\n        assert new_indexes == expected_indexes\n\n        # test rolling indexes with array-like values\n        indexes = Indexes(\n            {\n                \"x\": PandasIndex([0, 1, 2], \"x\"),\n                \"y\": PandasMultiIndex(\n                    pd.MultiIndex.from_product([[\"a\", \"b\"], [1, 2]]),\n                    \"y\",\n                ),\n            },\n            {\"x\": xr.Variable(\"x\", [0, 1, 2]), \"y\": xr.Variable(\"y\", [0, 1, 2, 3])},\n        )\n        expected_indexes = {\n            \"x\": PandasIndex([2, 0, 1], \"x\"),\n            \"y\": PandasMultiIndex(\n                pd.MultiIndex.from_product([[\"a\", \"b\"], [1, 2]])[[2, 0, 1, 3]],\n                \"y\",\n            ),\n        }\n        new_indexes, _ = roll_indexes(indexes, {\"x\": [1], \"y\": [1]})\n        assert new_indexes == expected_indexes\n\n        # test rolling indexes with array-like values with different sizes\n        indexes = Indexes(\n            {\n                \"x\": PandasIndex([0, 1, 2], \"x\"),\n                \"y\": PandasMultiIndex(\n                    pd.MultiIndex.from_product([[\"a\",", "    def testisel_indexes(self, indexes_and_vars) -> None:\n        indexes, variables = indexes_and_vars\n        x_idx, y_idx, z_midx = indexes\n\n        expected = {\n            \"x\": x_idx.isel({\"x\": slice(1, 3)}),\n            \"y\": y_idx,\n            \"z\": z_midx,\n            \"one\": z_midx,\n            \"two\": z_midx,\n        }\n        new_indexes, _ = isel_indexes(Indexes({k: v for k, v in expected.items()}, variables))\n\n        for k, expected in expected.items():\n            assert new_indexes[k] == expected\n"], "sample_238": ["    def test_aggregation_subquery_annotation_with_filter(self):\n        \"\"\"\n        Subquery annotations can be used with filter() clauses.\n        \"\"\"\n        subquery_qs = Book.objects.filter(\n            publisher=OuterRef('pk'),\n            price=Decimal('29.69'),\n        ).values('rating')\n        publisher_qs = Publisher.objects.annotate(\n            rating=Subquery(subquery_qs),\n        ).filter(\n            rating__gte=4.0,\n        )\n        self.assertEqual(publisher_qs.count(), 2)\n", "def test_function_expressions(self):\n    books = Book.objects.annotate(\n        price_in_dollars=Func(F('price'), function='ROUND', arg=2, output_field=FloatField()),\n        price_in_cents=Func(F('price'), function='ROUND', arg=0, output_field=FloatField())\n    )\n    self.assertEqual(books.first().price_in_dollars, round(self.b1.price, 2))\n    self.assertEqual(books.first().price_in_cents, round(self.b1.price))\n", "def test_mixed_type_function(self):\n    msg = 'Mixed-type aggregation is not supported for Max and Min.'\n    with self.assertRaisesMessage(FieldError, msg):\n        Book.objects.annotate(max_pages_price=Max('pages', 'price'))\n    with self.assertRaisesMessage(FieldError, msg):\n        Book.objects.annotate(min_pages_price=Min('pages', 'price'))\n", "    def test_numeric_output_field_mixin(self):\n        class CustomFunc(NumericOutputFieldMixin, Func):\n            function = 'SOME_FUNC'\n            output_field = FloatField()\n\n        with self.assertRaisesMessage(ValueError, 'output_field must be a subclass of models.Field'):\n            CustomFunc(output_field='not a field')\n\n        class CustomFunc2(NumericOutputFieldMixin, Func):\n            function = 'SOME_FUNC'\n            output_field = FloatField()\n\n                super().__init__(*args, output_field=IntegerField(), **kwargs)\n\n        with self.assertRaisesMessage(ValueError, 'output_field must be a subclass of models.Field'):\n            CustomFunc2()\n\n        class CustomFunc3(NumericOutputFieldMixin, Func):\n            function = 'SOME_FUNC'\n\n        with self.assertRaisesMessage(ValueError, 'output_field must be a subclass of models.Field'):\n            CustomFunc3()\n", "def test_abs_function(self):\n    books = Book.objects.annotate(abs_rating=Abs('rating')).order_by('name')\n    self.assertQuerysetEqual(\n        books, [\n            ('Artificial Intelligence: A Modern Approach', 4.0),\n            ('Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp', 5.0),\n            ('Practical Django Projects', 4.0),\n            ('Python Web Development with Django', 4.0),\n            ('Sams Teach Yourself Django in 24 Hours', 3.0),\n            ('The Definitive Guide to Django: Web Development Done Right', 4.5)\n        ],\n        lambda b: (b.name, b.abs_rating)\n    )\n"], "sample_341": ["    def test_management_form_not_submitted(self):\n        data = {\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(\n            formset.non_form_errors(),\n            [\n                'ManagementForm data is missing or has been tampered with. '\n                'Missing fields: choices-TOTAL_FORMS, choices-INITIAL_FORMS. '\n                'You may need to file a bug report if the issue persists.'\n            ],\n        )\n", "def test_management_form_fields_validation(self):\n    data = {\n        'form-TOTAL_FORMS': 'abc',  # invalid value for TOTAL_FORMS\n        'form-INITIAL_FORMS': 'def',  # invalid value for INITIAL_FORMS\n        'form-MIN_NUM_FORMS': 'ghi',  # invalid value for MIN_NUM_FORMS\n        'form-MAX_NUM_FORMS': 'jkl',  # invalid value for MAX_NUM_FORMS\n    }\n    formset = ArticleFormSet(data)\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(len(formset.non_form_errors()), 4)\n    self.assertIn('Enter a whole number.', str(formset.non_form_errors()))\n", "    def test_base_formset_get_deletion_widget(self):\n        \"\"\"Test BaseFormSet.get_deletion_widget() returns an instance.\"\"\"\n        self.assertIsInstance(\n            BaseFormSet.get_deletion_widget(),\n            CheckboxInput,\n        )\n", "def test_can_delete_extra_is_respected(self):\n    ChoiceFormSet = formset_factory(Choice, can_delete=True, can_delete_extra=False)\n    data = {\n        'choices-TOTAL_FORMS': '3',  # number of forms rendered\n        'choices-INITIAL_FORMS': '2',  # number of forms with initial data\n        'choices-MIN_NUM_FORMS': '0',  # min number of forms\n        'choices-MAX_NUM_FORMS': '0',  # max number of forms\n        'choices-0-choice': 'Zero',\n        'choices-0-votes': '0',\n        'choices-0-DELETE': 'on',\n        'choices-1-choice': 'One',\n        'choices-1-votes': '1',\n        'choices-1-DELETE': '',\n        'choices-2-choice': 'Two',\n        'choices-2-votes': '2',\n        'choices-2-DELETE': 'on',\n    }\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices', initial=[{'choice': 'Zero', 'votes': '0'}, {'choice': 'One', 'votes': '1'}])\n    self.assertTrue(formset.is_valid())\n    self.assertEqual(formset._should_delete_form(formset.forms[0]), True)\n    self.assertEqual(formset._should_delete_form(formset.forms[1]), False)\n    self.assertEqual(formset._should_delete_form(formset.forms[2]), False)\n    self.assertEqual(formset.deleted_forms, [formset.forms[0]])\n", "def test_formset_management_form_empty_cleaned_data(self):\n    \"\"\"The ManagementForm's cleaned_data doesn't include the value of required fields if they weren't filled.\"\"\"\n    formset = self.make_choiceformset([('', '')])\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.management_form.cleaned_data, {})\n"], "sample_1001": ["def test_latex_new_function():\n    new_func = Function('new_func')\n    assert latex(new_func(x)) == r'\\operatorname{new\\_func}{\\left (x \\right )}'\n    assert latex(new_func(x)**2) == r'\\operatorname{new\\_func}^{2}{\\left (x \\right )}'\n", "def test_latex_quotient_ring_elements():\n    from sympy.polys.domains import QQ\n    R = QQ.old_poly_ring(x)/[x**2 + 1]\n\n    assert latex(R.convert(x)) == r\"{x} + {\\left< {x^{2} + 1} \\right>}\"\n    assert latex(R.convert(x + 1)) == r\"{x + 1} + {\\left< {x^{2} + 1} \\right>}\"\n    assert latex(R.convert(x**2 + 1)) == r\"{0} + {\\left< {x^{2} + 1} \\right>}\"\n    assert latex(R.convert(x + x**2 + 1)) == r\"{x - 1} + {\\left< {x^{2} + 1} \\right>}\"\n    assert latex(R.convert(x**2 + x + 1)) == r\"{-x - 1} + {\\left< {x^{2} + 1} \\right>}\"\n", "def test_latex_Prod():\n    assert latex(Prod(x, (x, 1, 3))) == r\"\\prod_{x=1}^{3} x\"\n    assert latex(Prod(x*y, (x, 1, 3))) == r\"\\prod_{x=1}^{3} x y\"\n    assert latex(Prod(x, (x, 1, 3))**2) == r\"\\left(\\prod_{x=1}^{3} x\\right)^{2}\"\n    assert latex(Prod(x + y, (x, 1, 3))) == r\"\\prod_{x=1}^{3} \\left(x + y\\right)\"\n    assert latex(Prod(x, (x, 1, oo))) == r\"\\prod_{x=1}^{\\infty} x\"\n    assert latex(Prod(x, (x, oo, 1, -1))) == r\"\\prod_{x=\\infty}^{1} x\"\n    assert latex(Prod(x, (x, 1, 3, 2))) == r\"\\prod_{x=1, 2}^{3} x\"\n    assert latex(Prod(x, (x, 1, 3, 1))) == r\"\\prod_{x=1}^{3} x\"\n    assert latex(Prod(x, (x, 1, 1))) == r\"x\"\n", "def test_latex_issue_16648():\n    # test latex printing of negative fractional powers\n    from sympy import symbols, latex\n    x, y = symbols('x y')\n    assert latex(x**(-S(1)/2)) == r\"x^{- \\frac{1}{2}}\"\n    assert latex(x**(-S(3)/2)) == r\"x^{- \\frac{3}{2}}\"\n    assert latex(y**(S(1)/2)) == r\"y^{\\frac{1}{2}}\"\n    assert latex(y**(S(3)/2)) == r\"y^{\\frac{3}{2}}\"\n    assert latex(x**(-S(1)/2) + y**(S(1)/2)) == r\"x^{- \\frac{1}{2}} + y^{\\frac{1}{2}}\"\n    assert latex(x**(-S(3)/2) + y**(S(3)/2)) == r\"x^{- \\frac{3}{2}} + y^{\\frac{3}{2}}\"\n", "def test_latex_ndimarray():\n    # Test cases for issue #15268\n    for ArrayType in (ImmutableDenseNDimArray, ImmutableSparseNDimArray, MutableDenseNDimArray, MutableSparseNDimArray):\n        arr = ArrayType([[[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n        assert latex(arr) == r\"\\left[\\begin{array}{cc}\\left[\\begin{matrix}1 & 2 & 3\\\\4 & 5 & 6\\end{matrix}\\right] & \\left[\\begin{matrix}7 & 8 & 9\\\\10 & 11 & 12\\end{matrix}\\right]\\end{array}\\right]\"\n\n        arr = ArrayType([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n        assert latex(arr) == r\"\\left[\\begin{array}{cc}\\left[\\begin{matrix}1 & 2\\\\3 & 4\\end{matrix}\\right] & \\left[\\begin{matrix}5 & 6\\\\7 & 8\\end{matrix}\\right]\\end{array}\\right]\"\n\n        arr = ArrayType([1, 2, 3, 4, 5, 6, 7, 8])\n        assert latex(arr) == r\"\\left[\\begin{matrix}1 & 2 & 3 & 4 & 5 & 6 & 7 & 8\\end{matrix}\\right]\"\n\n        arr = ArrayType([1])\n        assert latex(arr) == r\"1\"\n\n        arr = ArrayType([])\n        assert latex(arr) == \"\"\n"], "sample_1072": ["def test_frac_properties():\n    assert frac(nan).is_finite is None\n    assert frac(nan).is_real is None\n    assert frac(nan).is_imaginary is None\n    assert frac(nan).is_integer is None\n    assert frac(nan).is_zero is None\n    assert frac(nan).is_negative is None\n\n    assert frac(oo).is_finite\n    assert frac(oo).is_real\n    assert frac(oo).is_imaginary is None\n    assert frac(oo).is_integer is None\n    assert frac(oo).is_zero is None\n    assert frac(oo).is_negative is None\n\n    assert frac(-oo).is_finite\n    assert frac(-oo).is_real\n    assert frac(-oo).is_imaginary is None\n    assert frac(-oo).is_integer is None\n    assert frac(-oo).is_zero is None\n    assert frac(-oo).is_negative is None\n\n    assert frac(zoo).is_finite is None\n    assert frac(zoo).is_real is None\n    assert frac(zoo).is_imaginary is None\n    assert frac(zoo).is_integer is None\n    assert frac(zoo).is_zero is None\n    assert frac(zoo).is_negative is None\n\n    r = Symbol('r', real=True)\n    assert frac(r).is_finite\n    assert frac(r).is_real\n    assert frac(r).is_imaginary is None\n    assert frac(r).is_integer is None\n    assert frac(r).is_zero is None\n    assert frac(r).is_negative is False\n\n    assert frac(r + I).is_finite is None\n    assert frac(r + I).is_real is False\n    assert frac(r + I).is_imaginary is None\n    assert frac(r + I).is_integer is None\n    assert frac(r + I).is_zero is None\n    assert frac(r + I).is_negative is None\n", "def test_frac_properties():\n    r = Symbol('r', real=True)\n    assert frac(r).is_imaginary is None\n    assert frac(r).is_negative is False\n    assert frac(r).is_nonnegative is None\n    assert frac(r).is_nonpositive is None\n    assert frac(r).is_positive is None\n\n    p_i = Symbol('p_i', integer=True, positive=True)\n    n_i = Symbol('n_i', integer=True, negative=True)\n    np_i = Symbol('np_i', integer=True, nonpositive=True)\n    nn_i = Symbol('nn_i', integer=True, nonnegative=True)\n\n    assert frac(p_i).is_imaginary is False\n    assert frac(n_i).is_imaginary is False\n    assert frac(np_i).is_imaginary is False\n    assert frac(nn_i).is_imaginary is False\n\n    assert frac(p_i).is_negative is False\n    assert frac(n_i).is_negative is False\n    assert frac(np_i).is_negative is None\n    assert frac(nn_i).is_negative is False\n\n    assert frac(p_i).is_nonnegative is True\n    assert frac(n_i).is_nonnegative is True\n    assert frac(np_i).is_nonnegative is None\n    assert frac(nn_i).is_nonnegative is True\n\n    assert frac(p_i).is_nonpositive is False\n    assert frac(n_i).is_nonpositive is False\n    assert frac(np_i).is_nonpositive is None\n    assert frac(nn_i).is_nonpositive is False\n\n    assert frac(p_i).is_positive is False\n    assert frac(n_i).is_positive is False\n    assert frac(np_i).is_positive is None\n    assert frac(nn_i).is_positive is False\n", "def test_frac_rewriting():\n    assert frac(x).rewrite(floor) == x - floor(x)\n    assert frac(x).rewrite(ceiling) == x + ceiling(-x)\n    assert frac(y).rewrite(floor).subs(y, pi) == frac(pi)\n    assert frac(y).rewrite(floor).subs(y, -E) == frac(-E)\n    assert frac(y).rewrite(ceiling).subs(y, -pi) == frac(-pi)\n    assert frac(y).rewrite(ceiling).subs(y, E) == frac(E)\n\n    assert frac(x).rewrite(floor).diff(x) == 1 - 0\n    assert frac(x).rewrite(ceiling).diff(x) == 1 + 0\n\n    # Test rewriting with non-real arguments\n    assert frac(I*x).rewrite(floor) == I*x - floor(I*x)\n    assert frac(I*x).rewrite(ceiling) == I*x + ceiling(-I*x)\n\n    # Test rewriting with non-real arguments and non-integer coefficients\n    assert frac(2*I*x).rewrite(floor) == 2*I*x - floor(2*I*x)\n    assert frac(2*I*x).rewrite(ceiling) == 2*I*x + ceiling(-2*I*x)\n", "def test_frac_edge_cases():\n    assert frac(nan) == nan\n    assert frac(zoo) == nan\n    assert frac(oo) == AccumBounds(0, 1)\n    assert frac(-oo) == AccumBounds(0, 1)\n\n    assert frac(0) == 0\n    assert frac(1) == 0\n    assert frac(-1) == 0\n\n    r = Symbol('r', real=True)\n    assert frac(r).diff(r) == 1\n    assert frac(r).diff(x) == 0\n    assert frac(r).diff(y) == 0\n\n    assert frac(x).diff(x) == 0\n\n    assert frac(r).is_real\n    assert frac(r).is_finite\n    assert frac(r).is_imaginary is None\n    assert frac(r).is_integer is None\n    assert frac(r).is_zero is None\n\n    assert frac(nan).is_real is None\n    assert frac(nan).is_finite is None\n    assert frac(nan).is_imaginary is None\n    assert frac(nan).is_integer is None\n    assert frac(nan).is_zero is None\n\n    assert frac(zoo).is_real is None\n    assert frac(zoo).is_finite is None\n    assert frac(zoo).is_imaginary is None\n    assert frac(zoo).is_integer is None\n    assert frac(zoo).is_zero is None\n", "def test_frac_edge_cases():\n    # Test fractional part for real arguments at integer values\n    assert frac(0) == 0\n    assert frac(1) == 0\n    assert frac(-1) == 0\n    assert frac(10) == 0\n    assert frac(-10) == 0\n\n    # Test fractional part for real arguments at non-integer values\n    assert frac(0.5) == 0.5\n    assert frac(1.5) == 0.5\n    assert frac(-0.5) == 0.5\n    assert frac(-1.5) == 0.5\n\n    # Test fractional part for complex arguments\n    assert frac(1 + 2*I) == 0 + 0*I\n    assert frac(1.5 + 2.5*I) == 0.5 + 0.5*I\n    assert frac(-1.5 + -2.5*I) == 0.5 + 0.5*I\n\n    # Test fractional part for non-real arguments\n    assert frac(I) == I\n    assert frac(2*I) == 0\n    assert frac(-I) == -I\n    assert frac(-2*I) == 0\n\n    # Test fractional part for symbolic arguments\n    assert frac(x) == frac(x)\n    assert frac(y) == frac(y)\n    assert frac(r) == frac(r)\n\n    # Test fractional part for nested functions\n    assert frac(floor(x)) == 0\n    assert frac(ceiling(x)) == 0\n    assert frac(floor(y)) == 0\n    assert frac(ceiling(y)) == 0\n"], "sample_253": ["def test_setting_timeout_from_environment_variable_fails(self):\n    with self.assertRaises(ValueError):\n        self.RELOADER_CLS()\n", "    def test_common_roots_windows_paths(self):\n        paths = (\n            Path('C:\\\\first\\\\second'),\n            Path('C:\\\\first\\\\second\\\\third'),\n            Path('C:\\\\first\\\\'),\n            Path('D:\\\\root\\\\first\\\\'),\n        )\n        results = autoreload.common_roots(paths)\n        self.assertCountEqual(results, [Path('C:\\\\first\\\\'), Path('D:\\\\root\\\\first\\\\')])\n", "    def test_is_django_module_and_path(self):\n        module = types.ModuleType('django.test_module')\n        module.__name__ = 'django.test_module'\n        self.assertTrue(autoreload.is_django_module(module))\n\n        non_django_module = types.ModuleType('test_module')\n        non_django_module.__name__ = 'test_module'\n        self.assertFalse(autoreload.is_django_module(non_django_module))\n", "    def test_watchman_unavailable(self, mocked_stat_reloader, mocked_watchman_reloader):\n            raise autoreload.WatchmanUnavailable\n\n        mocked_watchman_reloader.side_effect = raise_watchman_unavailable\n        mocked_stat_reloader.return_value = mock.sentinel.STAT_RELOADER\n        autoreload.start_django(mocked_watchman_reloader, lambda: None)\n        self.assertEqual(mocked_stat_reloader.call_count, 1)\n", "    def test_subscriptions_result_in_notification(self, mocked_file_changed):\n        reloader = autoreload.WatchmanReloader()\n        reloader._check_subscription({\n            'subscription': 'files:/test',\n            'files': ['file1', 'file2']\n        })\n        mocked_file_changed.send.assert_called_once_with(sender=reloader, file_path=Path('/test/file1'))\n"], "sample_233": ["def test_make_token_with_timestamp_edge_cases(self):\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n\n    # Test with a timestamp of 0\n    ts = 0\n    tk1 = p0._make_token_with_timestamp(user, ts)\n    self.assertIs(p0.check_token(user, tk1), True)\n\n    # Test with a large timestamp\n    ts = 2**31 - 1  # max 32-bit signed integer\n    tk2 = p0._make_token_with_timestamp(user, ts)\n    self.assertIs(p0.check_token(user, tk2), True)\n\n    # Test with a negative timestamp\n    ts = -1\n    with self.assertRaises(ValueError):\n        p0._make_token_with_timestamp(user, ts)\n", "def test_legacy_token_invalidation(self):\n    # RemovedInDjango40Warning: pre-Django 3.1 tokens will be invalid.\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    p_old_generator = PasswordResetTokenGenerator()\n    p_old_generator.algorithm = 'sha1'\n    p_new_generator = PasswordResetTokenGenerator()\n\n    legacy_token = p_old_generator.make_token(user)\n    user.password = 'newpassword'\n    user.save()\n    self.assertIs(p_old_generator.check_token(user, legacy_token), False)\n    self.assertIs(p_new_generator.check_token(user, legacy_token), False)\n\n    # Check that a new token can be generated and validated after the legacy token was invalidated.\n    new_token = p_new_generator.make_token(user)\n    self.assertIs(p_new_generator.check_token(user, new_token), True)\n", "def test_make_token_with_timestamp(self):\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    timestamp = self._num_seconds(datetime.now())\n    tk1 = p0._make_token_with_timestamp(user, timestamp)\n    self.assertIs(p0.check_token(user, tk1), True)\n", "def test_token_generation_with_custom_user_model(self):\n    \"\"\"\n    Token generation works correctly with custom user models.\n    \"\"\"\n    # Create a custom user model with a custom email field.\n    custom_user = CustomEmailField.objects.create_user(\n        'customuser', 'customuser@example.com', 'testpw'\n    )\n    # Generate a token with the custom user model.\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(custom_user)\n    self.assertIs(p0.check_token(custom_user, tk1), True)\n\n    # Update the custom user model's email field.\n    custom_user.email_custom = 'customuser2@example.com'\n    custom_user.save()\n    # Check that the token is invalidated.\n    self.assertIs(p0.check_token(custom_user, tk1), False)\n", "def test_check_token_with_legacy_token(self):\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    p_old_generator = PasswordResetTokenGenerator()\n    p_old_generator.algorithm = 'sha1'\n    legacy_token = p_old_generator.make_token(user)\n\n    # Remove the leading 6 digits (timestamp) and one character from the hash\n    # to simulate a truncated token\n    truncated_token = legacy_token[6:-1]\n\n    # Check that the legacy token is still valid but the truncated one is not\n    p_new_generator = PasswordResetTokenGenerator()\n    self.assertIs(p_new_generator.check_token(user, legacy_token), True)\n    self.assertIs(p_new_generator.check_token(user, truncated_token), False)\n"], "sample_832": ["def test_bayesian_ridge_with_non_positive_lambda():\n    \"\"\"Check that BayesianRidge raises an error for non-positive lambda\"\"\"\n    X = np.array([[1], [2], [6], [8], [10]])\n    y = np.array([1, 2, 6, 8, 10])\n    clf = BayesianRidge(lambda_1=-1, lambda_2=-1)\n    msg = \"lambda_1 and lambda_2 should be positive.\"\n    assert_raise_message(ValueError, msg, clf.fit, X, y)\n", "def test_bayesian_ridge_normalize():\n    \"\"\"Test BayesianRidge with normalization.\"\"\"\n    X, y = diabetes.data, diabetes.target\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n    clf = BayesianRidge(normalize=True).fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    assert_array_almost_equal(clf.predict(X_test), clf.predict(X_test, return_std=False))\n\n    X_train_scaled = StandardScaler().fit_transform(X_train)\n    X_test_scaled = StandardScaler().fit_transform(X_test)\n    clf_scaled = BayesianRidge(normalize=False).fit(X_train_scaled, y_train)\n    y_pred_scaled = clf_scaled.predict(X_test_scaled)\n    assert_array_almost_equal(y_pred, y_pred_scaled, decimal=3)\n", "def test_bayesian_ridge_alpha_bounds():\n    \"\"\"Test BayesianRidge with alpha_1 and alpha_2 bounds.\"\"\"\n    X = np.array([[1, 1], [3, 4], [5, 7], [4, 1], [2, 6], [3, 10], [3, 2]])\n    y = np.array([1, 2, 3, 2, 0, 4, 5]).T\n\n    alpha_1 = 1.0\n    alpha_2 = 1.0\n    br_model = BayesianRidge(alpha_1=alpha_1, alpha_2=alpha_2).fit(X, y)\n\n    # alpha_ should be greater than alpha_1 and less than alpha_2\n    assert br_model.alpha_ > alpha_1, \"alpha_ should be greater than alpha_1\"\n    assert br_model.alpha_ < alpha_2, \"alpha_ should be less than alpha_2\"\n\n", "def test_bayesian_ridge_intercept_variance():\n    # Test that BayesianRidge correctly computes variance of intercept\n    # when fit_intercept=True.\n    X = np.array([[1], [2], [3], [4]])\n    y = np.array([1, 2, 3, 4])\n    br_model = BayesianRidge(fit_intercept=True).fit(X, y)\n    var_intercept = br_model.sigma_[0, 0]\n    expected_var_intercept = 1. / br_model.alpha_\n    assert_almost_equal(var_intercept, expected_var_intercept, decimal=9)\n", "def test_bayesian_ridge_prediction_on_new_data():\n    # Test BayesianRidge prediction on new unseen data\n    X_train = np.array([[1], [2], [6], [8], [10]])\n    y_train = np.array([1, 2, 6, 8, 10])\n    X_new = np.array([[4], [5]])\n    y_new = np.array([4, 5])\n\n    clf = BayesianRidge().fit(X_train, y_train)\n    y_pred = clf.predict(X_new)\n\n    # Since the model was trained on a linear function, we expect the\n    # predictions to match the actual values\n    assert_array_almost_equal(y_pred, y_new, 2)\n"], "sample_330": ["    def test_typecast_date(self):\n        self.assertEqual(typecast_date('2012-03-04'), datetime.date(2012, 3, 4))\n        self.assertIsNone(typecast_date(''))\n", "    def test_truncate_name(self):\n        identifier = 'very_long_identifier_name_1234567890'\n        truncated_name = truncate_name(identifier)\n        self.assertEqual(len(truncated_name), 30)\n", "    def test_typecast_date(self):\n        self.assertIsNone(typecast_date(None))\n        self.assertEqual(typecast_date('2022-07-29'), datetime.date(2022, 7, 29))\n", "    def test_split_identifier(self):\n        # Test split_identifier with and without namespace\n        self.assertEqual(split_identifier('my_table'), ('', 'my_table'))\n        self.assertEqual(split_identifier('\"my_schema\".\"my_table\"'), ('my_schema', 'my_table'))\n", "    def test_truncate_name(self):\n        \"\"\"\n        Test that truncate_name does not truncate strings less than or equal\n        to the given length.\n        \"\"\"\n        self.assertEqual(truncate_name(\"short_string\"), \"short_string\")\n        self.assertEqual(truncate_name(\"a\" * 32), \"a\" * 32)\n"], "sample_893": ["def test_export_text_non_integer_weights(pyplot):\n    # Test export_text with non-integer weights\n    clf = DecisionTreeClassifier(max_depth=2, random_state=0)\n    clf.fit(X, y, sample_weight=[0.5, 0.5, 0.5, 1, 1, 1])\n\n    expected_report = dedent(\"\"\"\n    |--- feature_1 <= 0.00\n    |   |--- weights: [1.50, 0.00] class: -1\n    |--- feature_1 >  0.00\n    |   |--- weights: [0.00, 3.00] class: 1\n    \"\"\").lstrip()\n    assert export_text(clf, show_weights=True) == expected_report\n", "def test_export_text_with_sample_weights():\n    # Check that export_text handles sample weights correctly\n    clf = DecisionTreeClassifier(max_depth=2, random_state=0)\n    clf.fit(X, y, sample_weight=[1, 1, 1, 1, 1, 1])\n\n    expected_report = dedent(\"\"\"\n    |--- feature_1 <= 0.00\n    |   |--- class: -1\n    |--- feature_1 >  0.00\n    |   |--- class: 1\n    \"\"\").lstrip()\n\n    assert export_text(clf) == expected_report\n\n    expected_report = dedent(\"\"\"\n    |--- feature_1 <= 0.00\n    |   |--- weights: [3.00, 0.00] class: -1\n    |--- feature_1 >  0.00\n    |   |--- weights: [0.00, 3.00] class: 1\n    \"\"\").lstrip()\n    assert export_text(clf, show_weights=True) == expected_report\n\n    clf.fit(X, y2, sample_weight=[1, 1, 1, 1, 1, 1])\n\n    expected_report = dedent(\"\"\"\n    |--- feature_1 <= 0.00\n    |   |--- value: [-1.0, -1.0]\n    |--- feature_1 >  0.00\n    |   |--- value: [1.0, 1.0]\n    \"\"\").lstrip()\n    reg = DecisionTreeRegressor(max_depth=2, random_state=0)\n    reg.fit(X_mo, y_mo, sample_weight=[1, 1, 1, 1, 1, 1])\n    assert export_text(reg, decimals=1) == expected_report\n    assert export_text(reg, decimals=1, show_weights=True) == expected_report\n", "def test_export_text_with_uneven_depth(pyplot):\n    # Check correctness of export_text for tree with uneven depth\n    clf = DecisionTreeClassifier(max_depth=4, random_state=0)\n    clf.fit(X, y)\n\n    expected_report = dedent(\"\"\"\n    |--- feature_1 <= 0.00\n    |   |--- feature_1 <= -1.00\n    |   |   |--- class: -1\n    |   |--- feature_1 >  -1.00\n    |--- feature_1 >  0.00\n    |   |--- class: 1\n    \"\"\").lstrip()\n    assert export_text(clf, max_depth=10) == expected_report\n\n    expected_report = dedent(\"\"\"\n    |--- feature_1 <= 0.00\n    |   |--- feature_1 <= -1.00\n    |   |   |--- truncated branch of depth 1\n    |   |--- feature_1 >  -1.00\n    |--- feature_1 >  0.00\n    |   |--- class: 1\n    \"\"\").lstrip()\n    assert export_text(clf, max_depth=1) == expected_report\n", "def test_export_text_multiclass(pyplot):\n    # Test export_text with multi-class classification\n    from sklearn.datasets import load_iris\n    from sklearn.tree import DecisionTreeClassifier\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    clf = DecisionTreeClassifier(max_depth=2, random_state=0)\n    clf.fit(X, y)\n\n    expected_report = dedent(\"\"\"\n    |--- petal width (cm) <= 0.80\n    |   |--- class: 0\n    |--- petal width (cm) >  0.80\n    |   |--- petal width (cm) <= 1.75\n    |   |   |--- class: 1\n    |   |--- petal width (cm) >  1.75\n    |   |   |--- class: 2\n    \"\"\").lstrip()\n    assert export_text(clf, feature_names=iris.feature_names) == expected_report\n    assert (\n        export_text(clf, feature_names=iris.feature_names, show_weights=True)\n        == expected_report\n    )\n\n    expected_report = dedent(\"\"\"\n    |--- petal width (cm) <= 0.80\n    |   |--- weights: [50.00,  0.00,  0.00] class: 0\n    |--- petal width (cm) >  0.80\n    |   |--- petal width (cm) <= 1.75\n    |   |   |--- weights: [0.00, 47.00,  3.00] class: 1\n    |   |--- petal width (cm) >  1.75\n    |   |   |--- weights: [0.00,  1.00, 49.00] class: 2\n    \"\"\").lstrip()\n    assert export_text(clf, show_weights=True, class_names=True, decimals=2) == expected_report\n", "def test_export_text_errors_on_negative_depth(pyplot):\n    # Test that negative values raise an error\n    clf = DecisionTreeClassifier(max_depth=2, random_state=0)\n    clf.fit(X, y)\n\n    err_msg = \"'max_depth' should be greater or equal to 0.\"\n    with pytest.raises(ValueError, match=err_msg):\n        export_text(clf, max_depth=-1)\n\n    err_msg = \"'max_depth' should be an integer.\"\n    with pytest.raises(ValueError, match=err_msg):\n        export_text(clf, max_depth=1.5)\n"], "sample_171": ["def test_migrate_fake_initial_with_unapplied_dependency(self):\n    \"\"\"\n    Running --fake-initial doesn't apply unapplied dependencies.\n    \"\"\"\n    recorder = MigrationRecorder(connection)\n    recorder.record_applied(\"migrations\", \"0001_initial\")\n    call_command(\"migrate\", \"migrations\", \"0002\", fake_initial=True, verbosity=0)\n    applied_migrations = recorder.applied_migrations()\n    self.assertIn((\"migrations\", \"0001_initial\"), applied_migrations)\n    self.assertIn((\"migrations\", \"0002_second\"), applied_migrations)\n    # Rollback changes\n    call_command(\"migrate\", \"migrations\", \"zero\", verbosity=0)\n", "def test_migrate_fake_initial_empty(self):\n    \"\"\"\n    Fake initial detection is skipped when the initial migration is empty.\n    \"\"\"\n    call_command(\"migrate\", \"migrations\", \"0002\", verbosity=0)\n    call_command(\"migrate\", \"migrations\", \"zero\", fake=True, verbosity=0)\n    out = io.StringIO()\n    with mock.patch('django.core.management.color.supports_color', lambda *args: False):\n        call_command(\"migrate\", \"migrations\", \"0002\", fake_initial=True, stdout=out, verbosity=1)\n    value = out.getvalue().lower()\n    self.assertIn(\"migrations.0001_initial... skipped\", value)\n    self.assertIn(\"migrations.0002_second... faked\", value)\n    # Fake an apply\n    call_command(\"migrate\", \"migrations\", fake=True, verbosity=0)\n    # Unmigrate everything\n    call_command(\"migrate\", \"migrations\", \"zero\", verbosity=0)\n", "    def test_migrate_target_specific_migration_with_plan(self):\n        \"\"\"\n        Tests migrate --plan output when targeting a specific migration.\n        \"\"\"\n        out = io.StringIO()\n        call_command('migrate', 'migrations', '0003', plan=True, stdout=out, no_color=True)\n        self.assertEqual(\n            'Planned operations:\\n'\n            'migrations.0001_initial\\n'\n            '    Create model Salamander\\n'\n            '    Raw Python operation -> Grow salamander tail.\\n'\n            'migrations.0002_second\\n'\n            '    Create model Book\\n'\n            \"    Raw SQL operation -> ['SELECT * FROM migrations_book']\\n\"\n            'migrations.0003_third\\n'\n            '    Create model Author\\n'\n            \"    Raw SQL operation -> ['SELECT * FROM migrations_author']\\n\",\n            out.getvalue()\n        )\n        out = io.StringIO()\n        call_command('migrate', 'migrations', '0002', plan=True, stdout=out, no_color=True)\n        self.assertEqual(\n            'Planned operations:\\n'\n            'migrations.0001_initial\\n'\n            '    Create model Salamander\\n'\n            '    Raw Python operation -> Grow salamander tail.\\n'\n            'migrations.0002_second\\n'\n            '    Create model Book\\n'\n            \"    Raw SQL operation -> ['SELECT * FROM migrations_book']\\n\",\n            out.getvalue()\n        )\n        out = io.StringIO()\n        call_command('migrate', 'migrations', '0003', verbosity=0)\n        out = io.StringIO()\n        call_command('migrate', 'migrations', '0002', plan=True, stdout=out, no_color=True)\n        self.assertEqual(\n            'Planned operations:\\n'\n            'migrations.0003_third\\n'\n            '    Undo Create model Author\\n'\n            \"    Raw SQL operation -> ['SELECT * FROM migrations_book']\\n\",\n            out.getvalue()\n        )\n        call_command('migrate', 'migrations', '0003', verbosity=0)\n        out = io.StringIO()\n        call_command('migrate', 'migrations', '0003', plan=True, stdout=out, no_color=True)\n        self.assertEqual(\n            'Planned operations:\\n'\n            '  No planned migration operations.\\n',\n           ", "def test_squashmigrations_replaced_migration_with_replaced_by(self):\n    \"\"\"\n    squashmigrations squashes replaced migration with 'replaces' attribute.\n    \"\"\"\n    out = io.StringIO()\n    with self.temporary_migration_module(module=\"migrations.test_migrations_squashed\"):\n        call_command(\"squashmigrations\", \"migrations\", \"0002\", interactive=False, verbosity=1, stdout=out)\n\n        squashed_migration_file = os.path.join(self.migrations_dir, \"0001_squashed_0002_second.py\")\n        self.assertTrue(os.path.exists(squashed_migration_file))\n\n        with open(squashed_migration_file, encoding='utf-8') as fp:\n            content = fp.read()\n            self.assertIn(\"replaces = [('0001_initial', '0002_second')]\", content)\n", "def test_migrate_reorder_operations(self):\n    \"\"\"\n    Tests that migrate command can reorder operations.\n    \"\"\"\n    out = io.StringIO()\n    call_command(\"migrate\", \"migrations\", verbosity=0)\n    call_command(\"migrate\", \"migrations\", \"zero\", fake=True, verbosity=0)\n    call_command(\"migrate\", \"migrations\", verbosity=0)\n    self.assertEqual(out.getvalue(), \"\")\n\n    out = io.StringIO()\n    call_command(\"migrate\", \"migrations\", \"0001\", verbosity=1, stdout=out)\n    self.assertIn(\"Applying migrations.0001_initial... OK\", out.getvalue())\n\n    out = io.StringIO()\n    call_command(\"migrate\", \"migrations\", \"0002\", verbosity=1, stdout=out)\n    self.assertIn(\"Applying migrations.0002_second... OK\", out.getvalue())\n\n    out = io.StringIO()\n    call_command(\"migrate\", \"migrations\", \"0003\", verbosity=1, stdout=out)\n    self.assertIn(\"Applying migrations.0003_third... OK\", out.getvalue())\n\n    out = io.StringIO()\n    call_command(\"migrate\", \"migrations\", \"0004\", verbosity=1, stdout=out)\n    self.assertIn(\"Applying migrations.0004_fourth... OK\", out.getvalue())\n\n    out = io.StringIO()\n    call_command(\"migrate\", \"migrations\", \"0005\", verbosity=1, stdout=out)\n    self.assertIn(\"Applying migrations.0005_fifth... OK\", out.getvalue())\n"], "sample_805": ["def test_mean_poisson_deviance():\n    y_true = [2, 0, 1, 4]\n    y_pred = [0.5, 0.5, 2., 2.]\n    assert_almost_equal(mean_poisson_deviance(y_true, y_pred),\n                        mean_tweedie_deviance(y_true, y_pred, p=1))\n\n    y_true = [2, 0, 1, 4]\n    y_pred = [0.5, 0.5, 2., 2.]\n    sample_weight = [0.1, 0.2, 0.3, 0.4]\n    assert_almost_equal(mean_poisson_deviance(y_true, y_pred, sample_weight),\n                        mean_tweedie_deviance(y_true, y_pred, sample_weight, p=1))\n\n    y_true = [2, 0, 1, 4]\n    y_pred = [-0.5, 0.5, 2., 2.]\n    msg = \"Mean Poisson deviance error cannot be used when y_pred contains \"\n    with pytest.raises(ValueError, match=msg):\n        mean_poisson_deviance(y_true, y_pred)\n\n    y_true = [2, -1, 1, 4]\n    y_pred = [0.5, 0.5, 2., 2.]\n    msg = \"Mean Poisson deviance error cannot be used when y_true contains \"\n    with pytest.raises(ValueError, match=msg):\n        mean_poisson_deviance(y_true, y_pred)\n\n", "def test_mean_poisson_deviance():\n    y_true = np.array([2, 0, 1, 4])\n    y_pred = np.array([0.5, 0.5, 2., 2.])\n\n    assert_almost_equal(mean_poisson_deviance(y_true, y_pred),\n                        mean_tweedie_deviance(y_true, y_pred, p=1))\n\n    # Poisson deviance error\n    with pytest.raises(ValueError,\n                       match=\"can only be used on non-negative y_true \"\n                             \"and strictly positive y_pred.\"):\n        mean_poisson_deviance(y_true, np.array([0.5, -0.5, 2., 2.]))\n\n    with pytest.raises(ValueError,\n                       match=\"can only be used on non-negative y_true \"\n                             \"and strictly positive y_pred.\"):\n        mean_poisson_deviance(np.array([2, -1, 1, 4]), y_pred)\n", "def test_regression_metrics_multioutput_shapes(multioutput):\n    y_true = np.arange(50).reshape(10, 5)\n    y_pred = y_true + 1\n    if multioutput == 'raw_values':\n        assert y_true.shape == mean_squared_error(y_true, y_pred, multioutput).shape\n        assert y_true.shape == mean_absolute_error(y_true, y_pred, multioutput).shape\n        assert y_true.shape == r2_score(y_true, y_pred, multioutput).shape\n        assert y_true.shape == explained_variance_score(y_true, y_pred, multioutput).shape\n    else:\n        assert np.isscalar(mean_squared_error(y_true, y_pred, multioutput))\n        assert np.isscalar(mean_absolute_error(y_true, y_pred, multioutput))\n        assert np.isscalar(r2_score(y_true, y_pred, multioutput))\n        assert np.isscalar(explained_variance_score(y_true, y_pred, multioutput))\n", "def test_mean_poisson_deviance():\n    y_true = np.array([2, 0, 1, 4])\n    y_pred = np.array([0.5, 0.5, 2., 2.])\n\n    # test with sample weights\n    sample_weight = np.array([0.1, 0.2, 0.3, 0.4])\n    assert_almost_equal(mean_poisson_deviance(y_true, y_pred),\n                        mean_tweedie_deviance(y_true, y_pred, p=1))\n    assert_almost_equal(mean_poisson_deviance(y_true, y_pred, sample_weight),\n                        mean_tweedie_deviance(y_true, y_pred, sample_weight, p=1))\n\n    # test with y_true and y_pred having different shapes\n    y_true_2d = y_true.reshape(-1, 1)\n    y_pred_2d = y_pred.reshape(-1, 1)\n    assert_almost_equal(mean_poisson_deviance(y_true_2d, y_pred_2d),\n                        mean_tweedie_deviance(y_true_2d, y_pred_2d, p=1))\n\n    # test with invalid input\n    assert_raises_regex(ValueError, \"non-negative y_true and strictly \"\n                        \"positive y_pred.\",\n                        mean_poisson_deviance, [-1, 0, 1, 4], [0.5, 0.5, 2., 2.])\n    assert_raises_regex(ValueError, \"non-negative y_true and strictly \"\n                        \"positive y_pred.\",\n                        mean_poisson_deviance, [2, 0, 1, 4], [0, 0.5, 2., 2.])\n", "def test_mean_tweedie_deviance_sample_weight():\n    y_true = np.array([1., 2., 3.])\n    y_pred = np.array([1.5, 2.5, 3.5])\n\n    # check that sample_weight=None is equivalent to sample_weight=ones\n    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, p=0),\n                        mean_tweedie_deviance(y_true, y_pred, p=0,\n                                              sample_weight=np.ones_like(y_true)))\n\n    # check that it raises an error when sample_weight is not 1d\n    with pytest.raises(ValueError, match=\"sample_weight cannot be broadcast\"):\n        mean_tweedie_deviance(y_true, y_pred, p=0, sample_weight=np.array([[1.]]))\n\n    # check that sample_weight is correctly applied\n    sample_weight = np.array([0., 1., 1.])\n    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, p=0,\n                                              sample_weight=sample_weight),\n                        mean_tweedie_deviance(y_true[1:], y_pred[1:], p=0))\n\n    # check that it raises an error when sample_weight is not the same length\n    with pytest.raises(ValueError, match=\"sample_weight and y_true\"):\n        mean_tweedie_deviance(y_true, y_pred, p=0, sample_weight=np.array([1., 1.]))\n"], "sample_1021": ["def test_quaternion_comparison():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(1, 2, 3, 4)\n    q3 = Quaternion(1, 2, 3, 5)\n\n    assert q1 == q2\n    assert q1 != q3\n    assert q1 != 1\n", "def test_quaternion_mul():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n\n    # Test multiplication of two quaternions\n    assert q1 * q2 == Quaternion(-60, 12, 30, 24)\n\n    # Test multiplication of quaternion and scalar\n    assert q1 * 2 == Quaternion(2, 4, 6, 8)\n    assert 2 * q1 == Quaternion(2, 4, 6, 8)\n\n    # Test multiplication of quaternion and complex number\n    assert q1 * (2 + 3*I) == Quaternion(-10, 11, 38, -5)\n    assert (2 + 3*I) * q1 == Quaternion(-10, 11, 38, -5)\n\n    # Test multiplication of quaternion and non-commutative object\n    x = symbols('x', commutative=False)\n    assert q1 * x == Mul(Quaternion(1, 2, 3, 4), x, evaluate=False)\n    assert x * q1 == Mul(x, Quaternion(1, 2, 3, 4), evaluate=False)\n", "def test_quaternion_edge_cases():\n    q1 = Quaternion(0, 0, 0, 0)\n    q2 = Quaternion(1, 0, 0, 0)\n    q3 = Quaternion(0, 1, 0, 0)\n    q4 = Quaternion(0, 0, 1, 0)\n    q5 = Quaternion(0, 0, 0, 1)\n\n    assert q1.a == 0\n    assert q1.b == 0\n    assert q1.c == 0\n    assert q1.d == 0\n\n    assert q1.norm() == 0\n    assert q2.norm() == 1\n    assert q3.norm() == 1\n    assert q4.norm() == 1\n    assert q5.norm() == 1\n\n    assert q1.normalize() == Quaternion(0, 0, 0, 0)\n\n    raises(ValueError, lambda: q1.inverse())\n    assert q2.inverse() == Quaternion(1, 0, 0, 0)\n    assert q3.inverse() == Quaternion(0, -1, 0, 0)\n    assert q4.inverse() == Quaternion(0, 0, -1, 0)\n    assert q5.inverse() == Quaternion(0, 0, 0, -1)\n\n    assert q2.exp() == Quaternion(E, 0, 0, 0)\n    assert q3.exp() == Quaternion(E*cos(1), E*sin(1), 0, 0)\n    assert q4.exp() == Quaternion(E*cos(1), 0, E*sin(1), 0)\n    assert q5.exp() == Quaternion(E*cos(1), 0, 0, E*sin(1))\n\n    assert q2._ln() == Quaternion(0, 0, 0, 0)\n    assert q3._ln() == Quaternion(log(E), 1, 0, 0)\n    assert q4._ln() == Quaternion(log(E), 0, 1, 0)\n    assert q5._ln() == Quaternion(log(E), 0, 0, 1)\n\n    assert q2.pow_cos_sin(1) == Quaternion(1, 0, 0, 0)\n    assert q3.pow_cos_sin(1) == Quaternion(cos(1), sin(1), 0", "def test_quaternion_to_rotation_matrix_with_non_numeric_values():\n    x = symbols('x')\n    q = Quaternion(cos(x/2), sin(x/2), 0, 0)\n    v = (1, 1, 1)\n\n    m = q.to_rotation_matrix(v)\n    assert m.shape == (4, 4)\n    assert m[3, 0] == m[3, 1] == m[3, 2] == 0\n    assert m[3, 3] == 1\n    assert m[0, 3].has(x)\n    assert m[1, 3].has(x)\n    assert m[2, 3].has(x)\n\n    m = q.to_rotation_matrix()\n    assert m.shape == (3, 3)\n    assert m[0, 0].has(x)\n    assert m[0, 1].has(x)\n    assert m[0, 2] == 0\n    assert m[1, 0].has(x)\n    assert m[1, 1].has(x)\n    assert m[1, 2] == 0\n    assert m[2, 0].has(x)\n    assert m[2, 1].has(x)\n    assert m[2, 2] == 1\n", "def test_quaternion_rotation():\n    x, y, z = symbols('x y z')\n    q1 = Quaternion(cos(x/2), 0, 0, sin(x/2))\n    q2 = Quaternion(cos(y/2), 0, sin(y/2), 0)\n    q3 = Quaternion(cos(z/2), sin(z/2), 0, 0)\n    \n    # Check that the rotation matrix of the identity quaternion is the identity matrix\n    assert q1.to_rotation_matrix() == Matrix([\n        [1, 0, 0],\n        [0, cos(x), -sin(x)],\n        [0, sin(x), cos(x)]\n    ])\n    \n    # Check that the rotation matrix of a quaternion with no rotation is the identity matrix\n    assert q1.to_rotation_matrix(subs={x: 0}) == Matrix([\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1]\n    ])\n    \n    # Check that the rotation matrix of a quaternion with a 180 degree rotation is a rotation matrix\n    assert q1.to_rotation_matrix(subs={x: pi}) == Matrix([\n        [1, 0, 0],\n        [0, -1, 0],\n        [0, 0, -1]\n    ])\n    \n    # Check that the rotation matrix of the product of two quaternions is the product of their rotation matrices\n    q_product = q1 * q2\n    assert q_product.to_rotation_matrix() == q1.to_rotation_matrix() * q2.to_rotation_matrix()\n    \n    # Check that the rotation matrix of the product of three quaternions is the product of their rotation matrices\n    q_product = q1 * q2 * q3\n    assert q_product.to_rotation_matrix() == q1.to_rotation_matrix() * q2.to_rotation_matrix() * q3.to_rotation_matrix()\n"], "sample_130": ["def test_is_nullable(self):\n    query = Query(Item)\n    self.assertFalse(query.is_nullable(Item._meta.get_field('name')))\n    self.assertTrue(query.is_nullable(Item._meta.get_field('modified')))\n", "def test_annotation_select(self):\n    query = Query(Author)\n    query.add_annotation(SimpleCol(Author._meta.get_field('name')), alias='name_alias')\n    query.set_annotation_mask(['name_alias'])\n    where = query.build_where(Q(name_alias__startswith='A'))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, Exact)\n    self.assertEqual(lookup.rhs, 'A')\n    self.assertEqual(lookup.lhs.target, Author._meta.get_field('name'))\n", "def test_add_annotation(self):\n    query = Query(Item)\n    annotation = query.resolve_ref('modified')\n    self.assertIsInstance(annotation, SimpleCol)\n    self.assertEqual(annotation.target, Item._meta.get_field('modified'))\n    annotation = query.resolve_ref('nonexistent')\n    self.assertIsNone(annotation)\n", "    def test_is_nullable(self):\n        query = Query(Item)\n        self.assertTrue(query.is_nullable(Item._meta.get_field('name')))\n        self.assertFalse(query.is_nullable(Item._meta.get_field('id')))\n\n        # Test that empty_strings_allowed=False bypasses the default.\n        field = CharField(null=True, blank=False, empty_strings_allowed=False)\n        self.assertFalse(query.is_nullable(field))\n", "def test_deferred_to_data(self):\n    query = Query(Item)\n    query.deferred_loading = (frozenset(['pk', 'name']), True)\n    query.add_immediate_loading(['id', 'modified'])\n    loaded_field_names = query.get_loaded_field_names()\n    self.assertEqual(loaded_field_names, {Item._meta.concrete_model: {'id', 'modified'}})\n\n    query = Query(Item)\n    query.deferred_loading = (frozenset(['modified']), False)\n    query.deferred_loading = (frozenset(['name']), True)\n    loaded_field_names = query.get_loaded_field_names()\n    self.assertEqual(loaded_field_names, {Item._meta.concrete_model: {'pk', 'id', 'modified'}})\n\n    query = Query(Item)\n    query.deferred_loading = (frozenset(['modified']), True)\n    query.deferred_loading = (frozenset(['modified', 'name']), False)\n    loaded_field_names = query.get_loaded_field_names()\n    self.assertEqual(loaded_field_names, {Item._meta.concrete_model: {'pk', 'id'}})\n"], "sample_282": ["def test_boundfield_rendering(self):\n    form = ComplexFieldForm({\n        'field1_0': 'some text',\n        'field1_1': ['J', 'P'],\n        'field1_2_0': '2007-04-25',\n        'field1_2_1': '06:24:00',\n    })\n    bound_field = form['field1']\n    self.assertHTMLEqual(\n        str(bound_field),\n        \"\"\"\n        <input type=\"text\" name=\"field1_0\" value=\"some text\" id=\"id_field1_0\" required>\n        <select multiple name=\"field1_1\" id=\"id_field1_1\" required>\n        <option value=\"J\" selected>John</option>\n        <option value=\"P\" selected>Paul</option>\n        <option value=\"G\">George</option>\n        <option value=\"R\">Ringo</option>\n        </select>\n        <input type=\"text\" name=\"field1_2_0\" value=\"2007-04-25\" id=\"id_field1_2_0\" required>\n        <input type=\"text\" name=\"field1_2_1\" value=\"06:24:00\" id=\"id_field1_2_1\" required>\n        \"\"\",\n    )\n    self.assertHTMLEqual(\n        bound_field.as_text(),\n        '<input type=\"text\" name=\"field1_0\" value=\"some text\" id=\"id_field1_0\" required>',\n    )\n    self.assertHTMLEqual(\n        bound_field.as_textarea(),\n        '<textarea name=\"field1_0\" cols=\"40\" rows=\"10\" id=\"id_field1_0\" required>some text</textarea>',\n    )\n    self.assertHTMLEqual(\n        bound_field.as_hidden(),\n        '<input type=\"hidden\" name=\"field1_0\" value=\"some text\" id=\"id_field1_0\" required>',\n    )\n", "def test_bound_field_rendering(self):\n    class TestForm(Form):\n        field1 = CharField()\n\n    form = TestForm()\n    bound_field = form['field1']\n    self.assertEqual(str(bound_field), '<input type=\"text\" name=\"field1\" required id=\"id_field1\">')\n\n    # Test as_widget with custom widget and attrs\n    widget = TextInput(attrs={'class': 'custom'})\n    self.assertEqual(\n        bound_field.as_widget(widget, {'placeholder': 'Enter text'}),\n        '<input type=\"text\" name=\"field1\" placeholder=\"Enter text\" class=\"custom\" required id=\"id_field1\">'\n    )\n\n    # Test as_text\n    self.assertEqual(\n        bound_field.as_text({'placeholder': 'Enter text'}),\n        '<input type=\"text\" name=\"field1\" placeholder=\"Enter text\" required id=\"id_field1\">'\n    )\n\n    # Test as_textarea\n    self.assertEqual(\n        bound_field.as_textarea({'placeholder': 'Enter text'}),\n        '<textarea name=\"field1\" placeholder=\"Enter text\" cols=\"40\" rows=\"10\" required id=\"id_field1\"></textarea>'\n    )\n\n    # Test as_hidden\n    self.assertEqual(\n        bound_field.as_hidden(),\n        '<input type=\"hidden\" name=\"field1\" required id=\"id_field1\">'\n    )\n", "    def test_bound_field_label_tag(self):\n        form = ComplexFieldForm()\n        bound_field = form['field1']\n        label = bound_field.label_tag()\n        self.assertHTMLEqual(label, '<label for=\"id_field1_0\">Field1:</label>')\n", "def test_bound_field_label_tag_with_custom_suffix(self):\n    class CustomForm(Form):\n        field = CharField(label_suffix='?')\n    form = CustomForm()\n    bound_field = form['field']\n    label_tag = bound_field.label_tag()\n    self.assertHTMLEqual(label_tag, '<label for=\"id_field\">Field?</label>')\n", "    def test_bound_field_str_representation(self):\n        form = ComplexFieldForm({'field1': 'some text,JP,2007-04-25 06:24:00'})\n        bound_field = form['field1']\n        self.assertEqual(str(bound_field), str(bound_field.as_widget()))\n        self.assertIsInstance(str(bound_field), str)\n"], "sample_849": ["def test_leave_p_groups_out_with_large_p():\n    groups = np.array([1, 2] * 1000)\n    X = np.ones(len(groups))\n    n_groups = 2\n    n_splits = int(comb(n_groups, p=2))\n    lpgo = LeavePGroupsOut(n_groups=2)\n    splits = lpgo.split(X, groups=groups)\n    assert len(list(splits)) == n_splits\n", "def test_get_n_splits_works_with_empty_arrays(CVSplitter):\n    # All the splits should work with empty arrays\n    n_splits = CVSplitter().get_n_splits(X=np.empty(0), y=np.empty(0))\n    assert n_splits == 0\n", "def test_leave_p_groups_out_empty_trainset():\n    # No need to check LeaveOneGroupOut\n    cv = LeavePGroupsOut(n_groups=2)\n    X, y = [[1], [2]], [0, 3]  # 2 samples\n    with pytest.raises(\n            ValueError,\n            match='p=2 must be strictly less than the number of samples=2'):\n        next(cv.split(X, y, groups=[1, 2]))\n\n    # Additional checks\n    groups = np.array([1, 1, 1])\n    msg = (\"The groups parameter contains fewer than (or equal to) \"\n           \"n_groups (2) numbers of unique groups (1). LeavePGroupsOut \"\n           \"expects that at least n_groups + 1 (3) unique groups be present\")\n    assert_raise_message(ValueError, msg, next, cv.split(X, y, groups=groups))\n    groups = np.array([1, 2, np.nan])\n    msg = \"The groups parameter contains fewer than (or equal to) \" \\\n          \"n_groups (2) numbers of unique groups (2). LeavePGroupsOut \" \\\n          \"expects that at least n_groups + 1 (3) unique groups be present\"\n    assert_raise_message(ValueError, msg, next, cv.split(X, y, groups=groups))\n    groups = np.array([1, 2, np.inf])\n    msg = \"The groups parameter contains fewer than (or equal to) \" \\\n          \"n_groups (2) numbers of unique groups (2). LeavePGroupsOut \" \\\n          \"expects that at least n_groups + 1 (3) unique groups be present\"\n    assert_raise_message(ValueError, msg, next, cv.split(X, y, groups=groups))\n", "def test_group_kfold_with_empty_group():\n    # Test case for issue 12355 where groupKFold was not handling\n    # empty groups when groups is an array\n    groups = np.array([1, 1, 1, 2, 3, 3, 3, 0, 0])\n    X = np.ones(len(groups))\n    y = np.ones(len(groups))\n    group_kfold = GroupKFold(n_splits=2)\n    for train, test in group_kfold.split(X, y, groups):\n        assert_array_equal(np.intersect1d(groups[train], groups[test]), [])\n", "def test_leave_one_p_group_out_edge_cases():\n    # Test edge cases of LeaveOneGroupOut and LeavePGroupsOut\n    groups = np.array([0, 0, 1, 1])\n    groups2 = np.array([1, 1, 2, 2, 2])\n\n    X = np.ones(len(groups))\n    y = np.ones(len(groups))\n\n    # LeaveOneGroupOut\n    logo = LeaveOneGroupOut()\n    n_splits = 2\n    expected_splits = [\n        (np.array([2, 3]), np.array([0, 1])),\n        (np.array([0, 1]), np.array([2, 3]))\n    ]\n    for (train, test), (expected_train, expected_test) in zip(\n            logo.split(X, y, groups), expected_splits):\n        assert_array_equal(train, expected_train)\n        assert_array_equal(test, expected_test)\n\n    # LeavePGroupsOut\n    lpgo = LeavePGroupsOut(n_groups=2)\n    n_splits = 1\n    expected_splits = [\n        (np.array([]), np.array([0, 1, 2, 3]))\n    ]\n    for (train, test), (expected_train, expected_test) in zip(\n            lpgo.split(X, y, groups), expected_splits):\n        assert_array_equal(train, expected_train)\n        assert_array_equal(test, expected_test)\n\n    # LeaveOneGroupOut\n    logo = LeaveOneGroupOut()\n    n_splits = 2\n    expected_splits = [\n        (np.array([0, 1, 3, 4]), np.array([2]),\n        (np.array([0, 1, 2, 4]), np.array([3])),\n        (np.array([0, 1, 2, 3]), np.array([4]))\n    ]\n    for (train, test), (expected_train, expected_test) in zip(\n            logo.split(X, y, groups2), expected_splits):\n        assert_array_equal(train, expected_train)\n        assert_array_equal(test, expected_test)\n\n    # LeavePGroupsOut\n    lpgo = LeavePGroupsOut(n_groups=2)\n    n_splits = 3\n    expected_splits = [\n        (np.array([0, 1, 4]), np.array([2, 2, 3])),\n        (np.array([0, 1, 2, 4]), np.array(["], "sample_1017": ["def test_as_set_issue_13655():\n    x = symbols('x', real=True)\n    r1 = And(x >= 1, x <= 1)\n    r2 = And(x > 1, x < 1)\n    assert r1.as_set() == Interval(1, 1)\n    assert r2.as_set() == EmptySet()\n", "def test_as_Boolean_edge_cases():\n    # issue 10926\n    assert as_Boolean(1.0) == true\n    assert as_Boolean(0.0) == false\n    # issue 10855\n    assert as_Boolean(oo) == true\n", "def test_issue_14985():\n    # Test that Nand and Nor evaluate properly\n    assert Nand(A, B, B) == ~B\n    assert Nor(A, B, B) == ~B\n    assert Nand(A, A, B) == ~A\n    assert Nor(A, A, B) == ~A\n    assert Nand(True, True, True) is false\n    assert Nor(True, True, True) is false\n    assert Nand(False, False, False) is true\n    assert Nor(False, False, False) is true\n", "def test_boolalg_over_reals():\n    assert And(S.true, 0 < 1) == S.true\n    assert And(S.false, 0 < 1) == S.false\n    assert Or(S.true, 0 < 1) == S.true\n    assert Or(S.false, 0 < 1) == S.true\n    assert Xor(S.true, 0 < 1) == S.false\n    assert Xor(S.false, 0 < 1) == S.true\n    assert Not(0 < 1) == S.false\n    assert Not(1 < 0) == S.true\n    assert Implies(0 < 1, 1 < 1) == S.true\n    assert Implies(0 < 1, 1 < 0) == S.false\n    assert Implies(1 < 0, 0 < 1) == S.true\n    assert Implies(1 < 0, 1 < 0) == S.true\n    assert Equivalent(0 < 1, 1 < 1) == S.true\n    assert Equivalent(0 < 1, 1 < 0) == S.false\n    assert Equivalent(1 < 0, 0 < 1) == S.false\n    assert Equivalent(1 < 0, 1 < 0) == S.true\n    assert ITE(S.true, 0 < 1, 1 < 0) == S.true\n    assert ITE(S.false, 0 < 1, 1 < 0) == S.false\n    assert ITE(0 < 1, 0 < 1, 1 < 0) == S.true\n    assert ITE(1 < 0, 0 < 1, 1 < 0) == S.false\n    assert SOPform([x, y], [[1, 1]], [[0, 0]]) == And(x, y)\n    assert POSform([x, y], [[1, 1]], [[0, 0]]) == Or(Not(x), Not(y))\n", "def test_simplify_logic():\n    # test that deep=True works\n    x = symbols('x')\n    f = Function('f')\n    assert simplify_logic(A & f(x), 'dnf', deep=True) == And(A, f(x))\n    assert simplify_logic(A & f(x), 'cnf', deep=True) == And(A, f(x))\n\n    # test that deep=False works\n    assert simplify_logic(A & f(x), 'dnf') == And(A, f(x))\n    assert simplify_logic(A & f(x), 'cnf') == And(A, f(x))\n\n    # test that non-Boolean expression raises an error\n    raises(TypeError, lambda: simplify_logic(x**2, 'dnf'))\n    raises(TypeError, lambda: simplify_logic(x**2, 'cnf'))\n\n    # test that an invalid value for form raises an error\n    raises(ValueError, lambda: simplify_logic(A & B, 'xor'))\n"], "sample_117": ["    def test_bounded_data(self):\n        field = ReadOnlyPasswordHashField()\n        data = 'test'\n        initial = 'initial'\n        self.assertEqual(field.bound_data(data, initial), initial)\n", "    def test_bound_data(self):\n        field = ReadOnlyPasswordHashField()\n        self.assertEqual(field.bound_data('initial', 'new'), 'initial')\n", "    def test_render_with_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = '!'\n        self.assertHTMLEqual(\n            widget.render('name', value, {'id': 'id_password'}),\n            '<div id=\"id_password\">'\n            '<strong>No password set.</strong>'\n            '</div>'\n        )\n", "    def test_render_value_contains_unusable_password_prefix(self):\n        # Rendering a password that starts with UNUSABLE_PASSWORD_PREFIX\n        # should contain the message \"No password set.\"\n        widget = ReadOnlyPasswordHashWidget()\n        value = UNUSABLE_PASSWORD_PREFIX + 'unusable'\n        html = widget.render('name', value, {'id': 'id_password'})\n        self.assertIn(_(\"No password set.\"), html)\n", "    def test_save_email_template_overrides(self):\n        (user, username, email) = self.create_dummy_user()\n        data = {'email': email}\n        form = PasswordResetForm(data)\n        self.assertTrue(form.is_valid())\n        form.save(\n            subject_template_name='custom_subject.txt',\n            email_template_name='custom_email.txt',\n            from_email='admin@example.com',\n            domain_override='example.com',\n            html_email_template_name='custom_email.html'\n        )\n        self.assertEqual(len(mail.outbox), 1)\n        self.assertEqual(mail.outbox[0].subject, 'Custom Subject')\n        self.assertEqual(mail.outbox[0].from_email, 'admin@example.com')\n        self.assertEqual(len(mail.outbox[0].alternatives), 1)\n        message = mail.outbox[0].message()\n        self.assertEqual(message.get('subject'), 'Custom Subject')\n        self.assertEqual(len(message.get_payload()), 2)\n        self.assertTrue(message.is_multipart())\n        self.assertEqual(message.get_payload(0).get_content_type(), 'text/plain')\n        self.assertEqual(message.get_payload(1).get_content_type(), 'text/html')\n"], "sample_163": ["    def test_success_url_allowed_hosts(self):\n        response = self.client.post(\n            \"/login/success_url_allowed_hosts/\",\n            {\n                \"username\": \"testclient\",\n                \"password\": \"password\",\n                \"next\": \"https://otherserver/home\",\n            },\n        )\n        self.assertIn(SESSION_KEY, self.client.session)\n        self.assertRedirects(\n            response, \"https://otherserver/home\", fetch_redirect_response=False\n        )\n", "    def test_redirect_authenticated_user_custom_login_url(self):\n        \"\"\"Test that redirect_authenticated_user works with a custom login url.\"\"\"\n        self.login()\n        response = self.client.get(\"/custom_login/redirect_authenticated_user/\")\n        self.assertRedirects(response, \"/custom/\", fetch_redirect_response=False)\n", "    def test_get_success_url_allowed_hosts(self):\n        view = LoginView()\n        view.request = HttpRequest()\n        view.request.META[\"HTTP_HOST\"] = \"example.com\"\n        view.success_url_allowed_hosts = {\"example.com\", \"example.org\"}\n        self.assertEqual(view.get_success_url_allowed_hosts(), {\"example.com\", \"example.org\"})\n", "    def test_logout_redirect_authenticated_user_default(self):\n        \"\"\"If LOGOUT_REDIRECT_URL is not set, it defaults to the login URL.\"\"\"\n        self.login()\n        response = self.client.post(\"/logout/redirect_authenticated_user_default/\")\n        self.assertRedirects(\n            response, \"/accounts/profile/\", fetch_redirect_response=False\n        )\n", "    def test_redirect_authenticated_user_error(self):\n        \"\"\"An error is raised if redirect_authenticated_user is set to True\n        and LOGIN_REDIRECT_URL is the same as the login URL.\"\"\"\n        with self.settings(LOGIN_REDIRECT_URL=\"/login/\"):\n            with self.assertRaisesMessage(\n                ValueError,\n                \"Redirection loop for authenticated user detected. Check that \"\n                \"your LOGIN_REDIRECT_URL doesn't point to a login page.\",\n            ):\n                self.client.get(\"/login/redirect_authenticated_user/\")\n"], "sample_1003": ["def test_Options_init():\n    opt = Options((x, y, z), {'domain': 'ZZ', 'order': 'lex'})\n\n    assert opt.gens == (x, y, z)\n    assert opt.domain == ZZ\n    assert opt.order == lex\n\n    raises(OptionError, lambda: Options((x, y, z), {'invalid': 'option'}))\n\n    raises(GeneratorsError, lambda: Options((x, y, y), {'domain': 'ZZ'}))\n\n    raises(OptionError, lambda: Options((x, y, z), {'domain': 'invalid'}))\n", "def test_Domain_postprocess_with_domain_EX_and_gens():\n    opt = {'gens': (x, y), 'domain': EX}\n    Domain.postprocess(opt)\n    assert opt == {'gens': (x, y), 'domain': EX}\n", "def test_Options_class_init_dependencies_order():\n    Options._init_dependencies_order()\n\n    assert Options.__order__ is not None\n\n    options = Options((x, y, z), {'domain': 'ZZ'})\n\n    assert options.order == lex\n", "def test_Domain_postprocess_extension():\n    opt = {'extension': {sqrt(2), I}, 'domain': ZZ}\n    Domain.postprocess(opt)\n\n    assert opt == {'extension': {sqrt(2), I}, 'domain': ZZ}\n\n    opt = {'extension': {sqrt(2)}, 'domain': EX}\n    Domain.postprocess(opt)\n\n    assert opt == {'extension': {sqrt(2)}, 'domain': EX}\n\n    raises(GeneratorsError, lambda: Domain.postprocess({'gens': (x, y),\n           'extension': {x}, 'domain': ZZ}))\n\n    raises(GeneratorsError, lambda: Domain.postprocess({'gens': (x, y),\n           'extension': {x}, 'domain': EX}))\n", "def test_Options_init():\n    opt = Options((x, y, z), {'domain': 'ZZ'})\n\n    assert opt.gens == (x, y, z)\n    assert opt.domain == ZZ\n\n    raises(OptionError, lambda: Options((x, y), {'gens': (x, y, z), 'domain': 'ZZ'}))\n\n    raises(OptionError, lambda: Options((x, y, z), {'unknown_option': 'value'}))\n\n    raises(OptionError, lambda: Options((x, y, z), {'domain': 'invalid_domain'}))\n\n    raises(GeneratorsError, lambda: Options((x, y, z), {'domain': ZZ[y, z]}))\n\n    raises(GeneratorsError, lambda: Options((x, y, z), {'domain': EX}))\n\n    raises(GeneratorsError, lambda: Options((), {'domain': EX}))\n\n    opt = Options((x, y, z), {'domain': 'ZZ', 'strict': False})\n\n    assert opt.strict is False\n"], "sample_367": ["    def test_cache_page_decorator_timeout(self):\n        @cache_page(60)\n            return HttpResponse()\n\n        self.assertEqual(a_view(HttpRequest()).headers['Cache-Control'], 'max-age=60')\n", "    def test_cache_page_decorator_timeout(self):\n        @cache_page(timeout=60)\n            return HttpResponse()\n\n        response = a_view(HttpRequest())\n        self.assertEqual(response.cache_timeout, 60)\n", "    def test_cache_page_decorator(self):\n            return HttpResponse()\n\n        decorated_view = cache_page(60)(my_view)\n        request = HttpRequest()\n        response = decorated_view(request)\n        self.assertEqual(response.status_code, 200)\n        self.assertIn('Cache-Control', response.headers)\n", "    def test_cache_page_decorator(self):\n            return HttpResponse(\"response\")\n\n        my_view_cached = cache_page(60)(my_view)\n        request = HttpRequest()\n        response = my_view_cached(request)\n        self.assertEqual(response.content, b\"response\")\n", "    def test_cache_page_decorator_cache_control(self):\n            return HttpResponse()\n\n        my_view_cached = cache_page(123)(my_view)\n        response = my_view_cached(HttpRequest())\n        self.assertIn('max-age', response.headers['Cache-Control'])\n        self.assertIn('public', response.headers['Cache-Control'])\n"], "sample_891": ["def test_roc_auc_score_with_sparse_y_pred(global_random_seed):\n    # Test that roc_auc_score works with sparse y_pred\n    # Non-regression test for #23096\n    rng = np.random.RandomState(global_random_seed)\n    y_true = rng.randint(0, 2, size=10)\n    y_pred = rng.rand(10)\n    y_pred_sparse = csr_matrix(y_pred)\n    y_pred_dense = y_pred_sparse.toarray().ravel()\n    assert roc_auc_score(y_true, y_pred_sparse) == pytest.approx(\n        roc_auc_score(y_true, y_pred_dense)\n    )\n", "def test_roc_auc_score_negative_class_weights():\n    # Non-regression test for:\n    # https://github.com/scikit-learn/scikit-learn/issues/22575\n    # Test that roc_auc_score works correctly with negative class weights.\n    rng = np.random.RandomState(42)\n    y_true = rng.randint(0, 2, size=100)\n    y_pred = rng.rand(100)\n    sample_weight = rng.rand(100) * 2 - 1\n    assert roc_auc_score(y_true, y_pred, sample_weight=sample_weight) == pytest.approx(\n        roc_auc_score(y_true, y_pred, sample_weight=np.abs(sample_weight))\n    )\n", "def test_label_ranking_metrics_empty_input(metric):\n    \"\"\"Check that all the label ranking metrics raise an error\n    when the input arrays are empty.\n    \"\"\"\n    with pytest.raises(ValueError, match=\"Expected 2D array, got\"):\n        metric(y_true=[[1, 0]], y_score=[])\n", "def test_top_k_accuracy_score_multiclass(global_random_seed, average, labels):\n    \"\"\"Test top-k accuracy score for multiclass classification.\"\"\"\n    rng = np.random.RandomState(global_random_seed)\n    y_true = rng.randint(0, 2, size=10)\n    y_score = rng.rand(10, 2)\n\n    if labels is not None:\n        y_true = np.array(labels)[y_true]\n\n    top_k_accuracy_score(y_true, y_score, average=average)\n", "def test_auc_decreasing_x(metric, x, y):\n    \"\"\"Test that auc raises a ValueError for decreasing x values.\"\"\"\n    error_message = f\"x is neither increasing nor decreasing : {np.array(x)}\"\n    with pytest.raises(ValueError, match=re.escape(error_message)):\n        metric(x, y)\n"], "sample_94": ["    def test_super_user_creation_with_empty_password(self):\n        \"\"\"Test that superuser creation fails with an empty password.\"\"\"\n        new_io = StringIO()\n        entered_passwords = ['', '']\n\n            return entered_passwords.pop(0)\n\n        @mock_inputs({\n            'password': empty_passwords,\n            'username': 'joe1234567890',\n            'email': '',\n        })\n            with self.assertRaisesMessage(CommandError, \"Error: Blank passwords aren't allowed.\"):\n                call_command(\n                    \"createsuperuser\",\n                    interactive=True,\n                    stdin=MockTTY(),\n                    stdout=new_io,\n                    stderr=new_io,\n                )\n\n        test(self)\n", "    def test_fields_with_fk_environment_variable(self):\n        group = Group.objects.create(name='mygroup')\n        email = Email.objects.create(email='mymail@gmail.com')\n        env_vars = {\n            'DJANGO_SUPERUSER_USERNAME': email.pk,\n            'DJANGO_SUPERUSER_EMAIL': email.email,\n            'DJANGO_SUPERUSER_GROUP': group.pk,\n        }\n        with mock.patch.dict(os.environ, env_vars):\n            new_io = StringIO()\n            call_command(\n                'createsuperuser',\n                interactive=False,\n                stdout=new_io,\n            )\n            command_output = new_io.getvalue().strip()\n            self.assertEqual(command_output, 'Superuser created successfully.')\n            u = CustomUserWithFK._default_manager.get(email=email)\n            self.assertEqual(u.username, email)\n            self.assertEqual(u.group, group)\n", "    def setUp(self):\n        self.required_fields = CustomUser.REQUIRED_FIELDS\n", "    def test_swappable_user_non_ascii(self):\n        \"\"\"\n        Test that createsuperuser handles non-ASCII characters correctly.\n        \"\"\"\n        new_io = StringIO()\n        email = 'joe@somewhere.org'\n        first_name = 'J\\xfalia'\n        date_of_birth = '1976-04-01'\n\n            call_command(\n                'createsuperuser',\n                interactive=True,\n                email=email,\n                first_name=first_name,\n                date_of_birth=date_of_birth,\n                stdout=new_io,\n                stdin=MockTTY(),\n            )\n            command_output = new_io.getvalue().strip()\n            self.assertEqual(command_output, 'Superuser created successfully.')\n            u = CustomUser._default_manager.get(email=email)\n            self.assertEqual(u.first_name, first_name)\n            self.assertEqual(u.date_of_birth, date(1976, 4, 1))\n\n        test(self)\n", "    def setUp(self):\n        self.user = CustomUser.objects.create_user(\n            email='test@example.com',\n            date_of_birth='1976-04-01',\n            first_name='Joe'\n        )\n"], "sample_565": ["def test_inset_position():\n    fig, ax = plt.subplots(figsize=(2, 2))\n    ax_ins = plt.axes([0, 0, 1, 1])\n    ip = InsetPosition(ax, [0.2, 0.25, 0.5, 0.4])\n    ax_ins.set_axes_locator(ip)\n    fig.canvas.draw()\n    assert_array_almost_equal(\n        ax_ins.get_position().extents,\n        [0.2, 0.25, 0.5, 0.4])\n    ip = InsetPosition(ax, [0.2, 0.25, 0.5, 0.4])\n    ax_ins = plt.axes([0, 0, 1, 1])\n    ax_ins.set_axes_locator(ip)\n    fig.canvas.draw()\n    assert_array_almost_equal(\n        ax_ins.get_position().extents,\n        [0.2, 0.25, 0.5, 0.4])\n", "def test_insetposition_loc():\n    fig, ax = plt.subplots(figsize=(2, 2))\n    ax_ins = plt.axes([0, 0, 1, 1])\n    with pytest.raises(ValueError, match=\"Invalid loc\"):\n        ip = InsetPosition(ax, [0.2, 0.25, 0.5, 0.4, 0.6])\n    with pytest.raises(ValueError, match=\"Invalid loc\"):\n        ip = InsetPosition(ax, [0.2, 0.25, 0.5])\n    ip = InsetPosition(ax, [0.2, 0.25, 0.5, 0.4])\n    ax_ins.set_axes_locator(ip)\n", "def test_inset_axes_with_zoom():\n    fig, ax = plt.subplots()\n    inset_ax = inset_axes(ax, width=\"100%\", height=\"100%\",\n                          zoom=0.5, loc='upper right')\n    inset_ax.plot([0.5, 1.5], [0.5, 1.5])\n    mark_inset(ax, inset_ax, loc1=2, loc2=4, fc=\"none\", ec='0.3')\n    fig.canvas.draw()\n    assert_array_almost_equal(inset_ax.get_position().extents,\n                              [0.5, 0.5, 0.5, 0.5])\n", "def test_inset_locator_edge_cases():\n    fig, ax = plt.subplots()\n    \n    # Test inset_axes with invalid bbox_to_anchor\n    with pytest.raises(ValueError):\n        inset_axes(ax, width=1, height=1, bbox_to_anchor=[1, 2, 3])\n    \n    # Test zoomed_inset_axes with invalid bbox_to_anchor\n    with pytest.raises(ValueError):\n        zoomed_inset_axes(ax, zoom=2, bbox_to_anchor=[1, 2, 3])\n    \n    # Test inset_axes with invalid width and height\n    with pytest.raises(ValueError):\n        inset_axes(ax, width='a', height=1)\n    \n    # Test zoomed_inset_axes with invalid zoom\n    with pytest.raises(ValueError):\n        zoomed_inset_axes(ax, zoom='a')\n    \n    # Test mark_inset with invalid loc1 or loc2\n    with pytest.raises(ValueError):\n        mark_inset(ax, ax, loc1=5, loc2=4)\n    \n    # Test mark_inset with invalid axes\n    with pytest.raises(ValueError):\n        mark_inset(ax, fig, loc1=2, loc2=4)\n", "def test_inset_axes_positioning():\n    fig, ax = plt.subplots(figsize=(5, 4))\n    Z = cbook.get_sample_data(\"axes_grid/bivariate_normal.npy\")\n    extent = (-3, 4, -4, 3)\n    ax.imshow(Z, extent=extent, interpolation=\"nearest\", origin=\"lower\")\n\n    # Test positioning with relative units\n    ins1 = inset_axes(ax, width=\"30%\", height=\"40%\", loc=\"lower right\")\n    fig.canvas.draw()\n    assert_array_almost_equal(ins1.get_position().extents,\n                              [0.7, 0.6, 0.3, 0.4])\n\n    # Test positioning with absolute units\n    ins2 = inset_axes(ax, width=1., height=1.2, loc=\"upper left\")\n    fig.canvas.draw()\n    assert_array_almost_equal(ins2.get_position().extents,\n                              [0., 0.8, 1., 1.2])\n\n    # Test positioning with bbox_to_anchor and bbox_transform\n    ins3 = inset_axes(ax, width=\"20%\", height=\"30%\", loc=\"upper left\",\n                      bbox_to_anchor=(0.5, 0.5), bbox_transform=ax.transAxes)\n    fig.canvas.draw()\n    assert_array_almost_equal(ins3.get_position().extents,\n                              [0.4, 0.35, 0.2, 0.3])\n\n    # Test positioning with invalid bbox_to_anchor\n    with pytest.raises(ValueError):\n        inset_axes(ax, width=\"20%\", height=\"30%\", loc=\"upper left\",\n                   bbox_to_anchor=(0.5,))\n\n    # Test positioning with invalid bbox_transform\n    with pytest.raises(ValueError):\n        inset_axes(ax, width=\"20%\", height=\"30%\", loc=\"upper left\",\n                   bbox_transform=\"invalid\")\n"], "sample_502": ["def test_fallback_position_polar():\n    # check that position kwarg works if rect not supplied\n    axref = plt.axes([0.2, 0.2, 0.5, 0.5], projection='polar')\n    axtest = plt.axes(position=[0.2, 0.2, 0.5, 0.5], projection='polar')\n    np.testing.assert_allclose(axtest.bbox.get_points(),\n                               axref.bbox.get_points())\n\n    # check that position kwarg ignored if rect is supplied\n    axref = plt.axes([0.2, 0.2, 0.5, 0.5], projection='polar')\n    axtest = plt.axes([0.2, 0.2, 0.5, 0.5], position=[0.1, 0.1, 0.8, 0.8], projection='polar')\n    np.testing.assert_allclose(axtest.bbox.get_points(),\n                               axref.bbox.get_points())\n", "def test_xkcd_context():\n    # Test xkcd as a context manager\n    with plt.xkcd():\n        fig1 = plt.figure()\n        # ...\n    fig2 = plt.figure()\n    assert fig1.get_frame_on() == False\n    assert fig2.get_frame_on() == True\n", "def test_set_cmap():\n    # Test set_cmap with Colormap instance\n    cmap = mpl.cm.get_cmap('viridis')\n    plt.set_cmap(cmap)\n    assert plt.rcParams['image.cmap'] == 'viridis'\n\n    # Test set_cmap with colormap name\n    plt.set_cmap('plasma')\n    assert plt.rcParams['image.cmap'] == 'plasma'\n\n    # Test set_cmap with invalid input\n    with pytest.raises(TypeError):\n        plt.set_cmap(123)\n", "def test_install_repl_displayhook_interactive():\n    import matplotlib.backends\n    matplotlib.backends.backend = \"agg\"\n    mpl.rcParams['backend'] = 'agg'\n    mpl.is_interactive = lambda: False\n    install_repl_displayhook()\n    assert _INSTALL_FIG_OBSERVER\n    mpl.is_interactive = lambda: True\n    install_repl_displayhook()\n    assert not _INSTALL_FIG_OBSERVER\n", "def test_subplot_reuse_with_kwargs():\n    # Check that the axes spec and kwargs are correctly matched.\n    fig = plt.figure()\n    ax = plt.subplot(1, 2, 1, label='test_subplot_reuse_with_kwargs')\n    assert ax.get_label() == 'test_subplot_reuse_with_kwargs'\n\n    # Matching axes spec and kwargs should reuse the axes.\n    ax1 = plt.subplot(1, 2, 1, label='test_subplot_reuse_with_kwargs')\n    assert ax1 is ax\n\n    # Different label should create a new axes.\n    ax2 = plt.subplot(1, 2, 1, label='test_subplot_reuse_with_kwargs2')\n    assert ax2 is not ax\n\n    # Different projection should create a new axes.\n    ax3 = plt.subplot(1, 2, 1, label='test_subplot_reuse_with_kwargs', projection='polar')\n    assert ax3 is not ax\n    assert ax3 is not ax2\n    assert ax3.name == 'polar'\n\n    # Check that the axes spec and kwargs are correctly matched, even if they\n    # are passed in a different order.\n    ax4 = plt.subplot(label='test_subplot_reuse_with_kwargs', nrows=1, ncols=2, index=1)\n    assert ax4 is ax\n"], "sample_449": ["    def test_database_connections_override(self):\n        \"\"\"ThreadedWSGIServer overrides database connections.\"\"\"\n        request = WSGIRequest(RequestFactory().get(\"/\").environ)\n        server = ThreadedWSGIServer((\"localhost\", 0), WSGIRequestHandler)\n        connections_override = {\n            \"default\": \"overridden connection\",\n            \"other\": \"another connection\",\n        }\n        server.connections_override = connections_override\n        old_connections = connections.all()\n        try:\n            server.process_request_thread(request, (\"192.168.2.0\", 8080))\n            self.assertEqual(\n                connections.all(), connections_override\n            )\n        finally:\n            connections.all().clear()\n            connections.all().update(old_connections)\n", "    def test_close_connections(self):\n        server = ThreadedWSGIServer((\"localhost\", 0), WSGIRequestHandler)\n        server.connections_override = {\"default\": \"test_connection\"}\n        server._close_connections()\n        with self.assertRaises(KeyError):\n            connections[\"default\"]\n", "    def test_database_connections_override(self):\n        \"\"\"ThreadedWSGIServer overrides database connections.\"\"\"\n        request = WSGIRequest(RequestFactory().get(\"/\").environ)\n        client_address = (\"192.168.2.0\", 8080)\n        connections_override = {\n            \"default\": \"Override database connection\",\n            \"other\": \"Another override database connection\",\n        }\n        with self.settings(DATABASES=connections_override):\n            server = ThreadedWSGIServer(\n                (\"localhost\", 0),\n                WSGIRequestHandler,\n                connections_override=connections_override,\n            )\n            try:\n                # Check that the database connections are overridden.\n                with captured_stderr() as err:\n                    with self.assertLogs(\"django.server\", \"INFO\") as cm:\n                        server.process_request_thread(request, client_address)\n                self.assertEqual(err.getvalue(), \"\")\n                self.assertEqual(\n                    connections[\"default\"], connections_override[\"default\"]\n                )\n                self.assertEqual(\n                    connections[\"other\"], connections_override[\"other\"]\n                )\n            finally:\n                server.server_close()\n", "    def test_database_connections_override(self):\n        \"\"\"ThreadedWSGIServer overrides database connections.\"\"\"\n        connections_override = {\"default\": \"override\"}\n        server = ThreadedWSGIServer(\n            (\"localhost\", 0), WSGIRequestHandler, connections_override=connections_override\n        )\n        self.assertEqual(server.connections_override, connections_override)\n", "    def test_connections_override(self):\n        \"\"\"ThreadedWSGIServer overrides database connections.\"\"\"\n        # Mocking objects to simulate a scenario where database connections need\n        # to be overridden.\n        class MockRequest:\n            pass\n\n        class MockConnection:\n            alias = \"alias\"\n\n        class MockServer:\n            connections_override = {MockConnection().alias: MockConnection()}\n\n        class MockHandler:\n            server = MockServer()\n\n        handler = MockHandler()\n        request = MockRequest()\n        client_address = (\"192.168.2.0\", 8080)\n\n        # Check that database connections are overridden before and after\n        # processing the request.\n        with self.subTest(\"before\"):\n            self.assertNotEqual(\n                connections[MockConnection().alias], MockConnection()\n            )\n            handler.process_request_thread(request, client_address)\n            self.assertEqual(\n                connections[MockConnection().alias], MockConnection()\n            )\n\n        with self.subTest(\"after\"):\n            # Reset the connections.\n            connections[MockConnection().alias] = None\n            handler.close_request(request)\n            self.assertIsNone(connections[MockConnection().alias])\n            connections.close_all()\n"], "sample_769": ["def test_jaccard_similarity_score_multilabel():\n    # Test jaccard_similarity_score on a crafted multilabel example\n    # First crafted example\n\n    y_true = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 1]])\n    y_pred = np.array([[0, 1, 0, 0], [0, 1, 0, 0], [1, 0, 1, 0]])\n\n    assert_equal(jaccard_similarity_score(y_true, y_true), 1)\n    assert_equal(jaccard_similarity_score(y_true, y_pred), 0.75)\n\n    assert_equal(jaccard_similarity_score(y_true, np.logical_not(y_true)), 0)\n    assert_equal(jaccard_similarity_score(y_pred, np.logical_not(y_pred)), 0)\n    assert_equal(jaccard_similarity_score(y_true, np.zeros(y_true.shape)), 0)\n    assert_equal(jaccard_similarity_score(y_pred, np.zeros(y_true.shape)), 0)\n    assert_equal(jaccard_similarity_score(y_true, np.ones(y_true.shape)), 0.5)\n    assert_equal(jaccard_similarity_score(y_pred, np.ones(y_true.shape)), 0.5)\n", "def test_f1_score_multilabel_weighted_average():\n    # Test F1-score for multilabel classification task with weighted average\n    y_true, y_pred, _ = make_prediction(binary=False)\n\n    # compute scores with default labels introspection\n    f1_weighted = f1_score(y_true, y_pred, average='weighted')\n    _, _, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n\n    assert_almost_equal(f1_weighted, f1, 2)\n", "def test_balanced_accuracy_score_unseen_no_warning():\n    # Test no warning for balanced_accuracy_score when adjusted=True\n    y_true = np.array([1, 1, 1, 2])\n    y_pred = np.array([1, 2, 2, 2])\n    assert_no_warnings(balanced_accuracy_score, y_true, y_pred, adjusted=True)\n", "def test_balanced_accuracy_score_multiclass():\n    # Test balanced accuracy score on a multiclass problem\n    y_true = np.array([0, 1, 2, 3, 4] * 3)\n    y_pred = np.array([0, 1, 1, 3, 4] * 3)\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred),\n                        0.7833333333333333)\n\n    # Test balanced accuracy score on a multiclass problem with sample weights\n    y_true = np.array([0, 1, 2, 3, 4] * 3)\n    y_pred = np.array([0, 1, 1, 3, 4] * 3)\n    sample_weight = np.array([1, 1, 1, 1, 1] * 3)\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred,\n                                               sample_weight=sample_weight),\n                        0.7833333333333333)\n\n    # Test adjusted balanced accuracy score on a multiclass problem\n    y_true = np.array([0, 1, 2, 3, 4] * 3)\n    y_pred = np.array([0, 1, 1, 3, 4] * 3)\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred,\n                                               adjusted=True),\n                        0.8238095238095237)\n\n    # Test adjusted balanced accuracy score on a multiclass problem with sample weights\n    y_true = np.array([0, 1, 2, 3, 4] * 3)\n    y_pred = np.array([0, 1, 1, 3, 4] * 3)\n    sample_weight = np.array([1, 1, 1, 1, 1] * 3)\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred,\n                                               sample_weight=sample_weight,\n                                               adjusted=True),\n                        0.8238095238095237)\n", "def test_log_loss_multiclass_with_missing_labels():\n    # Test case for log_loss when labels in y_true are not all in y_pred.\n    y_true = np.array([1, 2, 0])\n    y_pred = np.array([[0.2, 0.7, 0.1], [0.6, 0.3, 0.1], [0.9, 0.1, 0.0]])\n    with assert_warns(UserWarning):\n        log_loss(y_true, y_pred)\n\n    assert log_loss(y_true, y_pred) > 0\n"], "sample_620": ["def test_concat_error_on_incompatible_indexes() -> None:\n    ds1 = Dataset(coords={\"x\": (\"x\", [1, 2])})\n    ds2 = Dataset(coords={\"x\": (\"x\", [1, 3])})\n\n    with pytest.raises(\n        ValueError,\n        match=r\"Cannot concatenate along dimension 'x' indexes with different values\",\n    ):\n        concat([ds1, ds2], dim=\"x\", compat=\"identical\")\n\n    with pytest.raises(\n        ValueError,\n        match=r\"Cannot concatenate along dimension 'x' indexes with different values\",\n    ):\n        concat([ds1, ds2], dim=\"x\", compat=\"equals\")\n", "def test_concat_empty_dataset() -> None:\n    # Regression test for GH1317\n    ds1 = Dataset()\n    ds2 = Dataset({\"x\": 1})\n    actual = concat([ds1, ds2], \"x\")\n    expected = Dataset({\"x\": (\"x\", [1])})\n    assert_identical(actual, expected)\n", "def test_concat_new_dim_as_dataarray_with_attrs() -> None:\n    da1 = DataArray([1, 2], dims=\"x\", attrs={\"foo\": \"bar\"})\n    da2 = DataArray([3, 4], dims=\"x\", attrs={\"foo\": \"bar\"})\n\n    dim_da = DataArray([0, 1], dims=\"dim\", attrs={\"baz\": \"qux\"})\n\n    expected = DataArray(\n        [[1, 2], [3, 4]], dims=[\"dim\", \"x\"], coords={\"dim\": [0, 1]}, attrs={\"foo\": \"bar\"}\n    )\n    expected.coords[\"dim\"].attrs = {\"baz\": \"qux\"}\n\n    actual = concat([da1, da2], dim=dim_da)\n\n    assert_identical(expected, actual)\n", "def test_concat_with_index(index_type) -> None:\n    data = Dataset(\n        {\"foo\": (\"x\", [1, 2, 3])}, \n        coords={\"x\": [1, 2, 3]}, \n        attrs={\"bar\": \"baz\"}\n    )\n    index = index_type([4, 5, 6], dims=\"x\")\n    actual = concat([data], index)\n    expected = Dataset(\n        {\"foo\": (\"x\", [1, 2, 3])}, \n        coords={\"x\": [4, 5, 6]}, \n        attrs={\"bar\": \"baz\"}\n    )\n    assert_identical(actual, expected)\n", "def test_concat_variable_length_dims() -> None:\n    \"\"\"Test for issue #7115\"\"\"\n    # This test creates two datasets with variable length dimensions\n    # and concatenates them along a new dimension.\n    ds1 = Dataset({\"x\": (\"x\", [1, 2, 3])})\n    ds2 = Dataset({\"x\": (\"x\", [4, 5])})\n    expected = Dataset({\"x\": ((\"y\", \"x\"), [[1, 2, 3], [4, 5]])})\n    actual = concat([ds1, ds2], \"y\")\n    assert_identical(actual, expected)\n"], "sample_107": ["    def test_technical_500_response_is_ajax(self):\n        request = self.rf.get('/test_view/')\n        request.is_ajax = lambda: True\n        try:\n            raise ValueError(\"Can't find my keys\")\n        except ValueError:\n            exc_type, exc_value, tb = sys.exc_info()\n        response = technical_500_response(request, exc_type, exc_value, tb)\n        self.assertEqual(response.status_code, 500)\n        self.assertEqual(response['Content-Type'], 'text/plain; charset=utf-8')\n", "    def test_technical_500_response(self):\n        try:\n            request = self.rf.get('/test_view/')\n            request.user = User()\n            raise ValueError(\"Can't find my keys\")\n        except ValueError:\n            exc_type, exc_value, tb = sys.exc_info()\n        response = technical_500_response(request, exc_type, exc_value, tb)\n        self.assertEqual(response.status_code, 500)\n        self.assertContains(response, 'ValueError at /test_view/', status_code=500)\n        self.assertContains(response, \"Can't find my keys\", status_code=500)\n", "    def test_get_traceback_frames_with_missing_f_locals(self):\n        try:\n                raise ValueError('Test value')\n            test_func()\n        except Exception:\n            exc_type, exc_value, tb = sys.exc_info()\n        reporter = ExceptionReporter(None, exc_type, exc_value, tb)\n        frame = tb.tb_frame\n        del frame.f_locals\n        frames = reporter.get_traceback_frames()\n        self.assertEqual(len(frames), 2)\n        last_frame = frames[-1]\n        self.assertEqual(last_frame['vars'], [])\n", "    def test_cleanse_setting_with_callable_value(self):\n            pass\n\n        result = cleanse_setting('TEST_SETTING', callable_value)\n        self.assertIsInstance(result, CallableSettingWrapper)\n        self.assertEqual(result._wrapped, callable_value)\n", "    def test_threading_cleanup(self):\n        # Test that ExceptionReporter cleans up the request's exception reporter\n        # filter when it's finished, even when an exception is raised within the\n        # ExceptionReporter's __init__ method.\n        class MockRequest:\n            exception_reporter_filter = None\n\n        request = MockRequest()\n\n            raise Exception('Mock exception')\n\n        with mock.patch('django.views.debug.get_exception_reporter_filter', mock_get_exception_reporter_filter):\n            with self.assertRaises(Exception):\n                ExceptionReporter(request, None, None, None)\n\n        self.assertIsNone(request.exception_reporter_filter)\n"], "sample_105": ["    def test_get_context_data(self):\n        mixin = ContextMixin()\n        context = mixin.get_context_data()\n        self.assertIn('view', context)\n", "    def test_get_context_data(self):\n        view = ContextMixin()\n        context = view.get_context_data(key='value')\n        self.assertEqual(context, {'view': view, 'key': 'value'})\n", "    def test_extra_context(self):\n        class TestView(ContextMixin, View):\n            extra_context = {'key': 'value'}\n\n        view = TestView()\n        context = view.get_context_data()\n        self.assertEqual(context['key'], 'value')\n        self.assertEqual(context['view'], view)\n", "    def test_render_to_response(self):\n        \"\"\"\n        Test the render_to_response method.\n        \"\"\"\n        request = self.rf.get('/')\n        view = TemplateView()\n        view.request = request\n        view.template_name = 'generic_views/about.html'\n        response = view.render_to_response(context={})\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, 'generic_views/about.html')\n", "    def test_get_context_data_extra_context(self):\n        \"\"\"\n        Test that extra_context is used to populate the template context\n        \"\"\"\n        view = TemplateView.as_view(extra_context={'key': 'value'})\n        response = view(self.rf.get('/'))\n        self.assertEqual(response.context['key'], 'value')\n"], "sample_364": ["    def test_include_without_namespace(self):\n        urlconf_module = 'tests.test_urlresolvers.test_urls'\n        include_tuple = include((urlconf_module, 'test_app_name'))\n        self.assertEqual(include_tuple[0], urlconf_module)\n        self.assertEqual(include_tuple[1], 'test_app_name')\n        self.assertEqual(include_tuple[2], 'test_app_name')\n", "    def test_include_with_namespace(self):\n        urlconf_module = 'tests.test_urlresolvers.test_urls'\n        app_name = 'myapp'\n        namespace = 'mynamespace'\n        result = include((urlconf_module, app_name), namespace=namespace)\n        self.assertEqual(result, (None, app_name, namespace))\n", "    def test_include_with_namespace(self):\n        url = '/included_urls/extra/something/'\n        match = resolve(url)\n        self.assertEqual(match.url_name, 'inner-extra')\n        self.assertEqual(match.namespace, 'inner')\n        self.assertEqual(match.kwargs, {'extra': 'something'})\n", "    def test_include_with_namespace(self):\n        with self.settings(ROOT_URLCONF='urlpatterns.include_urls'):\n            url = reverse('namespaced:inner-extra', kwargs={'extra': 'something'})\n            self.assertEqual(url, '/included_urls/namespaced/extra/something/')\n", "    def test_include_with_namespace(self):\n        match = resolve('/included_urls/extra/something/')\n        self.assertEqual(match.url_name, 'inner-extra')\n        self.assertEqual(match.app_name, 'inner')\n        self.assertEqual(match.namespace, ['included_urls', 'inner'])\n"], "sample_573": ["def test_low_unique_x(self, df):\n\n    groupby = GroupBy([\"group\"])\n    df[\"x\"] = np.repeat(df[\"x\"].unique()[:3], 33)\n    res = PolyFit(order=1, gridsize=100)(df, groupby, \"x\", {})\n    \n    assert res.columns.to_list() == [\"x\", \"y\", \"group\"]\n    assert res.empty\n", "    def test_low_order(self, df):\n\n        groupby = GroupBy([\"group\"])\n        df = df.groupby(\"group\").head(3).reset_index(drop=True)\n        res = PolyFit(order=2, gridsize=100)(df, groupby, \"x\", {})\n\n        assert res.columns.to_list() == [\"x\", \"y\", \"group\"]\n\n        ngroups = df[\"group\"].nunique()\n        assert_array_equal(res.index, np.arange(ngroups * 100))\n\n        for _, part in res.groupby(\"group\"):\n            assert part[\"x\"].empty\n            assert part[\"y\"].empty\n", "    def test_low_degree_with_few_unique_points(self, df):\n\n        # Make sure there are few unique points in the \"x\" column\n        df[\"x\"] = np.repeat(df[\"x\"].unique()[:5], 20)\n\n        groupby = GroupBy([\"group\"])\n        res = PolyFit(order=1, gridsize=100)(df, groupby, \"x\", {})\n        \n        assert res.columns.to_list() == [\"x\", \"y\", \"group\"]\n\n        ngroups = df[\"group\"].nunique()\n        assert_array_equal(res.index, np.arange(ngroups * 100))\n\n        for _, part in res.groupby(\"group\"):\n            assert_array_equal(part[\"x\"], part[\"x\"].unique()[0])\n            assert_array_equal(part[\"y\"], part[\"y\"].unique()[0])\n", "def test_insufficient_unique_points(self, df):\n    \"\"\"\n    Test that the function behaves correctly when there are not enough unique points\n    to fit a polynomial of the given order.\n    \"\"\"\n\n    groupby = GroupBy([\"group\"])\n    df[\"x\"] = df[\"x\"].round(0)  # Reduce unique points to make this test pass\n    res = PolyFit(order=3)(df, groupby, \"x\", {})\n    \n    assert res.columns.to_list() == [\"x\", \"y\", \"group\"]\n    assert res[\"x\"].empty\n    assert res[\"y\"].empty\n", "    def test_low_degree_with_few_unique_x_values(self, df):\n\n        # Make x values have fewer unique values than the default order of the polynomial\n        df[\"x\"] = np.repeat(range(2), 50)\n\n        groupby = GroupBy([\"group\"])\n        res = PolyFit()(df, groupby, \"x\", {})\n\n        assert res.columns.to_list() == [\"x\", \"y\", \"group\"]\n\n        # Check that the function returns an empty DataFrame when there are too few unique x values\n        assert res.empty\n"], "sample_1095": ["def test_resize_with_cyclic_form():\n    p = Permutation([[0, 1, 2], [3, 4]])\n    assert p.resize(5) == Permutation([[0, 1, 2], [3, 4]])\n    assert p.resize(6) == Permutation([[0, 1, 2], [3, 4], [5]])\n    raises(ValueError, lambda: p.resize(3))\n", "def test_cycle_structure():\n    assert Permutation(0, 1, 2, 3).cycle_structure == {4: 1}\n    assert Permutation(0, 1, 2, 3, 4, 5).cycle_structure == {6: 1}\n    assert Permutation(0, 1)(2, 3, 4).cycle_structure == {2: 2}\n    assert Permutation(0, 1, 2)(3, 4).cycle_structure == {3: 1, 1: 1}\n    assert Permutation().cycle_structure == {1: 0}\n    assert Permutation([]).cycle_structure == {1: 0}\n    assert Permutation([]).cycle_structure == {1: 0}\n", "def test_printing_cyclic():\n    p1 = Permutation([0, 1, 2])\n    assert repr(p1) == 'Permutation(1, 2)'\n    assert str(p1) == '(1 2)'\n    p2 = Permutation()\n    assert repr(p2) == 'Permutation()'\n    assert str(p2) == '()'\n    p3 = Permutation([1, 2, 0, 3])\n    assert repr(p3) == 'Permutation(3)(0, 1, 2)'\n    p4 = Permutation([0, 1, 2, 3, 4])\n    assert repr(p4) == 'Permutation()'\n    assert str(p4) == '()'\n\n    p5 = Permutation([0, 1, 2, 3])\n    assert repr(p5) == 'Permutation()'\n    assert str(p5) == '()'\n\n    p6 = Permutation([0, 1, 2, 3, 4, 5, 6, 7, 8])\n    assert repr(p6) == 'Permutation()'\n    assert str(p6) == '()'\n\n    p7 = Permutation([2, 5, 1, 6, 3, 0, 4])\n    assert repr(p7) == 'Permutation([[1, 2, 3, 5, 6], [0, 4]])'\n    assert str(p7) == '(1 2 3 5 6)(0 4)'\n\n    p8 = Permutation([[1, 2, 3, 5, 6], [0, 4]])\n    assert repr(p8) == 'Permutation([[1, 2, 3, 5, 6], [0, 4]])'\n    assert str(p8) == '(1 2 3 5 6)(0 4)'\n", "def test_Permutation_power():\n    # Test powers of Permutation instances\n    p = Permutation([0, 2, 1])\n    assert p**2 == Permutation([0, 1, 2])\n    assert p**0 == Permutation([0, 1, 2])\n    assert p**-1 == Permutation([0, 2, 1])\n    assert p**-2 == Permutation([0, 1, 2])\n\n    # Test powers with non-integer values\n    raises(NotImplementedError, lambda: p**x)\n\n    # Test powers with out of range values\n    raises(ValueError, lambda: p**6)\n\n    # Test powers with large values\n    p = Permutation([0, 2, 1, 3])\n    assert p**13 == p\n    assert p**-13 == ~p\n    assert p**14 == Permutation([0, 1, 2, 3])\n", "def test_permutation_xor():\n    p = Permutation(0, 1, 2)\n    assert p^1 == p(1) == 2\n    assert p^0 == p(0) == 1\n    assert p^(2) == p(2) == 0\n\n    i = 1.0\n    raises(NotImplementedError, lambda: p^i)\n    i = 'a'\n    raises(NotImplementedError, lambda: p^i)\n\n    q = Permutation(0, 1, 2)\n    assert p^q == p^(q.array_form)\n\n    p = Permutation(0, 1, 2, 3)\n    q = Permutation(1, 2, 3, 0)\n    r = Permutation(2, 3, 0, 1)\n    assert p^(q*r) == p^(q.array_form+r.array_form)\n    assert p^(q*r) == p^q^r\n"], "sample_887": ["def test_calibration_calibrated_classifiers_pickling(data, method, ensemble):\n    \"\"\"Check that calibrated classifiers are picklable.\"\"\"\n    X, y = data\n    calibrated_clf = CalibratedClassifierCV(method=method, ensemble=ensemble)\n    calibrated_clf.fit(X, y)\n    pickled_clf = pickle.dumps(calibrated_clf)\n    unpickled_clf = pickle.loads(pickled_clf)\n    assert_array_almost_equal(\n        calibrated_clf.predict_proba(X), unpickled_clf.predict_proba(X)\n    )\n", "def test_calibration_with_base_estimator_not_fitted(data, ensemble):\n    # Test that when `cv` is 'prefit', `base_estimator` should be fitted\n    X, y = data\n    clf = LogisticRegression()\n    cal_clf = CalibratedClassifierCV(\n        base_estimator=clf, cv=\"prefit\", ensemble=ensemble\n    )\n    with pytest.raises(NotFittedError):\n        cal_clf.fit(X, y)\n", "def test_calibration_multiclass_return_shape(data, method, ensemble):\n    # Test that the shape of the output of predict_proba is correct\n    n_samples, n_features = 10, 5\n    X, y = make_classification(\n        n_samples=n_samples, n_features=n_features, n_classes=5, random_state=42\n    )\n    clf = LinearSVC(random_state=7)\n\n    cal_clf = CalibratedClassifierCV(clf, method=method, cv=3, ensemble=ensemble)\n    cal_clf.fit(X, y)\n\n    probas = cal_clf.predict_proba(X)\n    assert probas.shape == (n_samples, 5)\n", "def test_calibration_iris_multiclass(pyplot, iris_data):\n    \"\"\"Test calibration with iris multiclass classification\"\"\"\n    X, y = iris_data\n    clf = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", random_state=42)\n    clf.fit(X, y)\n    calib_clf = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=\"prefit\")\n    calib_clf.fit(X, y)\n\n    y_pred = clf.predict(X)\n    calib_pred = calib_clf.predict(X)\n    assert_array_equal(y_pred, calib_pred)\n\n    y_prob = clf.predict_proba(X)\n    calib_prob = calib_clf.predict_proba(X)\n    assert_allclose(y_prob, calib_prob)\n", "def test_calibration_base_estimator_deprecation_usage(data, ensemble, cv):\n    \"\"\"Test that `CalibratedClassifierCV` raises a FutureWarning when `base_estimator` is set.\"\"\"\n    X, y = data\n    base_estimator = LogisticRegression()\n\n    warn_msg = (\n        \"`base_estimator` was renamed to `estimator` in version 1.2 and \"\n        \"will be removed in 1.4.\"\n    )\n    with pytest.warns(FutureWarning, match=warn_msg):\n        CalibratedClassifierCV(base_estimator=base_estimator, ensemble=ensemble, cv=cv)\n\n    warn_msg = \"Both `base_estimator` and `estimator` are set. Only set `estimator` \"\n    with pytest.raises(ValueError, match=warn_msg):\n        CalibratedClassifierCV(\n            base_estimator=base_estimator, estimator=base_estimator, ensemble=ensemble, cv=cv\n        )\n"], "sample_1055": ["def test_encipher_decipher_bg_with_large_input():\n    p, q = 383, 503\n    N = bg_public_key(p, q)\n    message = 123456789012345\n    enc = encipher_bg(message, N)\n    dec = decipher_bg(enc, (p, q))\n    assert dec == message\n", "def test_elgamal_edge_cases():\n    for msg in [-1, 0, 1]:\n        for seed in [1, None]:\n            dk = elgamal_private_key(seed=seed)\n            ek = elgamal_public_key(dk)\n            enc = encipher_elgamal(msg, ek, seed)\n            dec = decipher_elgamal(enc, dk)\n            assert dec == msg\n", "def test_lfsr():\n    F = FF(2)\n    key = [F(1), F(0), F(1)]\n    fill = [F(0), F(1), F(0)]\n    s = lfsr_sequence(key, fill, 8)\n    cp = lfsr_connection_polynomial(s)\n    assert cp == x**3 + x + 1\n\n    F = FF(2)\n    key = [F(1), F(1), F(0)]\n    fill = [F(1), F(0), F(1)]\n    s = lfsr_sequence(key, fill, 8)\n    cp = lfsr_connection_polynomial(s)\n    assert cp == x**3 + x**2 + 1\n\n    F = FF(2)\n    key = [F(1), F(1), F(0), F(1)]\n    fill = [F(1), F(0), F(1), F(0)]\n    s = lfsr_sequence(key, fill, 8)\n    cp = lfsr_connection_polynomial(s)\n    assert cp == x**4 + x + 1\n", "def test_padded_key():\n    assert padded_key('b', 'ab') == 'ba'\n    assert padded_key('ba', 'abc') == 'bac'\n    assert padded_key('ba', 'abcde') == 'bacde'\n    raises(ValueError, lambda: padded_key('ab', 'ace'))\n    raises(ValueError, lambda: padded_key('ab', 'abba'))\n    raises(ValueError, lambda: padded_key('ab', 'ba'))\n    raises(ValueError, lambda: padded_key('ab', 'aab'))\n    raises(ValueError, lambda: padded_key('ab', ''))\n", "def test_encipher_decipher_elgamal():\n    ps = [61, 53, 71, 73, 79, 97, 107, 109,\n          113, 127, 131, 137, 139, 149, 151]\n    gs = [11, 8, 7, 5, 3, 13, 19, 47,\n          43, 101, 7, 3, 11, 13, 6]\n    ds = [10, 8, 33, 70, 121, 18, 50, 108,\n          113, 126, 97, 117, 86, 137, 13]\n    messages = [\n        17, 543, 191, 1354, 51, 1030, 523, 180,\n        982, 1919, 227, 1130, 84, 257, 419,\n    ]\n    for p, g, d in zip(ps, gs, ds):\n        dk = (p, g, d)\n        ek = elgamal_public_key(dk)\n        for msg in messages:\n            enc = encipher_elgamal(msg, ek)\n            dec = decipher_elgamal(enc, dk)\n            assert dec == msg\n"], "sample_425": ["    def test_serialize_timezone(self):\n        with ignore_warnings(category=RemovedInDjango50Warning):\n            from django.utils.timezone import utc\n        self.assertSerializedResultEqual(\n            get_default_timezone(),\n            (\"datetime.timezone.utc\", {\"import datetime\"}),\n        )\n        self.assertSerializedResultEqual(\n            get_fixed_timezone(180),\n            (\"datetime.timezone.utc\", {\"import datetime\"}),\n        )\n        self.assertSerializedResultEqual(\n            datetime.timezone.utc,\n            (\"datetime.timezone.utc\", {\"import datetime\"}),\n        )\n        self.assertSerializedResultEqual(\n            zoneinfo.ZoneInfo(\"Europe/Paris\"),\n            (\"datetime.timezone.utc\", {\"import datetime\"}),\n        )\n        if pytz:\n            self.assertSerializedResultEqual(\n                pytz.timezone(\"Europe/Paris\"),\n                (\"datetime.timezone.utc\", {\"import datetime\"}),\n            )\n", "def test_serialize_lazy_object_non_serializable(self):\n    class NonSerializable:\n            raise TypeError(\"Non serializable\")\n\n    lazy_obj = SimpleLazyObject(lambda: NonSerializable())\n    with self.assertRaisesMessage(\n        TypeError, \"Non serializable\"\n    ):\n        self.serialize_round_trip(lazy_obj)\n", "def test_serialize_pathlib_path(self):\n    \"\"\"\n    Tests serializing pathlib.Path objects directly.\n    \"\"\"\n    if sys.platform == \"win32\":\n        path = pathlib.Path(\"C:\\\\path\\\\to\\\\file.txt\")\n        expected = (\"pathlib.PureWindowsPath('C:/path/to/file.txt')\", {\"import pathlib\"})\n        self.assertSerializedResultEqual(path, expected)\n    else:\n        path = pathlib.Path(\"/path/to/file.txt\")\n        expected = (\"pathlib.PurePosixPath('/path/to/file.txt')\", {\"import pathlib\"})\n        self.assertSerializedResultEqual(path, expected)\n", "def test_serialize_pathlib_like(self):\n    \"\"\"\n    pathlib.PurePath objects are serialized as if they were the concrete path objects.\n    \"\"\"\n    path = pathlib.PureWindowsPath(\"A:\\\\File.txt\")\n    self.assertSerializedResultEqual(path, (\"pathlib.PureWindowsPath('A:/File.txt')\", {\"import pathlib\"}))\n\n    path = pathlib.PurePosixPath(\"/path/file.txt\")\n    self.assertSerializedResultEqual(path, (\"pathlib.PurePosixPath('/path/file.txt')\", {\"import pathlib\"}))\n", "def test_serialize_function_type_with_module_name(self):\n    \"\"\"\n    Test that a function type with a module name can be serialized.\n    \"\"\"\n        pass\n    self.assertSerializedResultEqual(\n        my_function,\n        (\"migrations.test_writer.test_serialize_function_type_with_module_name.<locals>.my_function\", set()),\n    )\n"], "sample_880": ["def test_check_partial_fit_first_call():\n    class MockEstimator:\n            self.classes_ = None\n\n    estimator = MockEstimator()\n\n    # Test that it raises an error when classes is not provided on the first call\n    with pytest.raises(ValueError):\n        _check_partial_fit_first_call(estimator)\n\n    # Test that it sets the classes_ attribute on the first call\n    classes = [0, 1, 2]\n    assert _check_partial_fit_first_call(estimator, classes=classes)\n    assert_array_equal(estimator.classes_, unique_labels(classes))\n\n    # Test that it raises an error when classes is different on a subsequent call\n    with pytest.raises(ValueError):\n        _check_partial_fit_first_call(estimator, classes=[0, 1, 3])\n\n    # Test that it does not raise an error when classes is the same on a subsequent call\n    assert not _check_partial_fit_first_call(estimator, classes=classes)\n", "def test_check_partial_fit_first_call():\n    class DummyEstimator:\n        pass\n\n    estimator = DummyEstimator()\n    assert _check_partial_fit_first_call(estimator, [0, 1]) is True\n    assert hasattr(estimator, \"classes_\")\n    assert_array_equal(estimator.classes_, [0, 1])\n\n    estimator = DummyEstimator()\n    assert _check_partial_fit_first_call(estimator, [0, 1]) is True\n    with pytest.raises(ValueError):\n        _check_partial_fit_first_call(estimator, [2, 3])\n\n    estimator = DummyEstimator()\n    estimator.classes_ = [0, 1]\n    assert _check_partial_fit_first_call(estimator, None) is False\n    assert hasattr(estimator, \"classes_\")\n    assert_array_equal(estimator.classes_, [0, 1])\n\n    estimator = DummyEstimator()\n    estimator.classes_ = [0, 1]\n    with pytest.raises(ValueError):\n        _check_partial_fit_first_call(estimator, [2, 3])\n\n    estimator = DummyEstimator()\n    with pytest.raises(ValueError):\n        _check_partial_fit_first_call(estimator)\n", "def test_check_partial_fit_first_call():\n    # Test check_partial_fit_first_call with None as classes\n    clf = SVC()\n    assert _check_partial_fit_first_call(clf) is False\n\n    # Test check_partial_fit_first_call with classes provided\n    clf = SVC()\n    _check_partial_fit_first_call(clf, [0, 1])\n    assert clf.classes_.tolist() == [0, 1]\n\n    # Test check_partial_fit_first_call with classes provided multiple times\n    clf = SVC()\n    _check_partial_fit_first_call(clf, [0, 1])\n    _check_partial_fit_first_call(clf, [0, 1])\n    assert clf.classes_.tolist() == [0, 1]\n\n    # Test check_partial_fit_first_call with different classes provided\n    clf = SVC()\n    _check_partial_fit_first_call(clf, [0, 1])\n    with pytest.raises(ValueError):\n        _check_partial_fit_first_call(clf, [0, 2])\n\n    # Test check_partial_fit_first_call with classes not provided on the first call\n    clf = SVC()\n    with pytest.raises(ValueError):\n        _check_partial_fit_first_call(clf)\n", "def test_unique_labels_with_all_finite():\n    y = np.array([1, 2, np.nan, 3, 4])\n    with pytest.raises(ValueError):\n        unique_labels(y)\n    \n    y = np.array([1, 2, np.inf, 3, 4])\n    with pytest.raises(ValueError):\n        unique_labels(y)\n\n    y = np.array([1, 2, -np.inf, 3, 4])\n    with pytest.raises(ValueError):\n        unique_labels(y)\n", "def test_check_partial_fit_first_call():\n    from sklearn.base import BaseEstimator\n\n    class TestEstimator(BaseEstimator):\n        pass\n\n    estimator = TestEstimator()\n\n    # Test when classes is not provided on the first call\n    with pytest.raises(ValueError):\n        _check_partial_fit_first_call(estimator)\n\n    # Test when classes is provided on the first call\n    classes = np.array([0, 1])\n    assert _check_partial_fit_first_call(estimator, classes)\n    assert_array_equal(estimator.classes_, classes)\n\n    # Test when classes is not provided on the second call\n    assert not _check_partial_fit_first_call(estimator)\n\n    # Test when classes is provided on the second call\n    classes_2 = np.array([0, 1, 2])\n    with pytest.raises(ValueError):\n        _check_partial_fit_first_call(estimator, classes_2)\n\n    # Test when classes is provided on the second call but is the same\n    assert not _check_partial_fit_first_call(estimator, classes)\n"], "sample_1093": ["def test_PythonCodePrinter_piecewise():\n    p = PythonCodePrinter()\n    assert p.doprint(Piecewise((x**2, Eq(x, 0)), (2*x, x>6))) == '((x**2) if (x == 0) else (2*x) if (x > 6) else None)'\n    assert p.doprint(Piecewise((x**2, Eq(x, 0)), (2*x, x>6), evaluate=False)) == '((x**2) if (x == 0) else (2*x) if (x > 6) else None)'\n    assert p.doprint(Piecewise((x**2, Eq(x, 0)), (2*x, Eq(x, 1)))) == '((x**2) if (x == 0) else (2*x) if (x == 1) else None)'\n    assert p.doprint(Piecewise((x**2, Eq(x, 0)))) == '((x**2) if (x == 0) else None)'\n    assert p.doprint(Piecewise((x**2, Eq(x, 0)), (x, Eq(x, 1)), (2*x, Eq(x, 2)))) == '((x**2) if (x == 0) else (x) if (x == 1) else (2*x) if (x == 2) else None)'\n", "def test_Piecewise_with_No():\n    p = NumPyPrinter()\n    x = symbols('x')\n\n    pw = Piecewise((1, Eq(x, 0)), (x, Eq(x, 1)), (2, Eq(x, 2)), (x, True))\n    assert p.doprint(pw) == \"numpy.select([x == 0, x == 1, x == 2], [1, x, 2], default=x)\"\n", "def test_AbstractPythonCodePrinter_einsum():\n    from sympy import symbols\n\n    a, b, c, d = symbols('a b c d')\n\n    prntr = AbstractPythonCodePrinter()\n    expr = (a, b, c, d)\n\n    contraction_indices = [[0], [1]]\n    contraction_string, letters_free, letters_dum = prntr._get_einsum_string(expr, contraction_indices)\n    assert contraction_string == \"a,b,c,d\"\n    assert letters_free == ['a', 'c']\n    assert letters_dum == ['b', 'd']\n\n    contraction_indices = [[0], [1], [2, 3]]\n    contraction_string, letters_free, letters_dum = prntr._get_einsum_string(expr, contraction_indices)\n    assert contraction_string == \"a,b,c,d\"\n    assert letters_free == ['a']\n    assert letters_dum == ['b', 'c', 'd']\n\n    contraction_indices = [[0, 1], [0, 2], [1, 3]]\n    contraction_string, letters_free, letters_dum = prntr._get_einsum_string(expr, contraction_indices)\n    assert contraction_string == \"a,b,c,d\"\n    assert letters_free == []\n    assert letters_dum == ['a', 'b', 'c', 'd']\n", "def test_AbstractPythonCodePrinter():\n    prntr = AbstractPythonCodePrinter()\n\n    assert prntr._print_Infinity(S.Infinity) == 'float(\"inf\")'\n    assert prntr._print_NegativeInfinity(S.NegativeInfinity) == \"float('-inf')\"\n    assert prntr._print_NaN(S.NaN) == \"float('nan')\"\n\n    assert prntr._get_statement(\"x = 5\") == \"x = 5\"\n    assert prntr._get_comment(\"This is a comment\") == \"  # This is a comment\"\n\n    assert prntr._format_code(\"x = 5\\ny = 3\") == \"x = 5\\ny = 3\"\n\n    assert prntr._indent_codestring(\"x = 5\\ny = 3\") == \"    x = 5\\n    y = 3\"\n\n    assert prntr._declare_number_const(\"x\", \"5\") == \"x = 5\"\n\n    expr = S.Half\n    assert prntr._declare_number_const(\"x\", expr) == \"x = 0.5\"\n\n    assert prntr._print_Mod(Mod(x, 2)) == 'x % 2'\n    assert prntr._print_Piecewise(Piecewise((x, Eq(x, 1)), (y, Eq(y, 2)))) == '((x) if (x == 1) else (y) if (y == 2) else None)'\n", "def test_AbstractPythonCodePrinter():\n    p = AbstractPythonCodePrinter()\n    assert p._indent_codestring(\"a\\nb\") == \"    a\\n    b\"\n    assert p._format_code([\"a\", \"b\"]) == [\"a\", \"b\"]\n\n    # Testing methods for _get_statement and _get_comment\n    assert p._get_statement(\"a\") == \"a\"\n    assert p._get_comment(\"a\") == \"  # a\"\n\n    # Testing _expand_fold_binary_op and _expand_reduce_binary_op\n    assert p._expand_fold_binary_op(\"op\", [1, 2, 3, 4]) == \"op(op(op(1, 2), 3), 4)\"\n    assert p._expand_reduce_binary_op(\"op\", [1, 2, 3, 4]) == \"op(op(1, 2), op(3, 4))\"\n\n    # Testing _print_FunctionDefinition and _print_While\n    f = p._print_FunctionDefinition({\"name\": \"f\", \"body\": [\"a = 1\"], \"parameters\": [{\"symbol\": \"x\"}]})\n    w = p._print_While({\"body\": [\"a = 1\"], \"condition\": \"x > 0\"})\n    assert f == \"def f(x):\\n    a = 1\"\n    assert w == \"while x > 0:\\n    a = 1\"\n\n    # Testing _print_Declaration\n    decl = {\"variable\": {\"symbol\": \"x\", \"value\": 1}}\n    assert p._print_Declaration(decl) == \"x = 1\"\n\n    # Testing _print_Return\n    ret = {\"args\": [1]}\n    assert p._print_Return(ret) == \"return 1\"\n\n    # Testing _print_Print\n    prnt = {\"print_args\": [1, 2], \"format_string\": None, \"file\": None}\n    prnt2 = {\"print_args\": [1, 2], \"format_string\": \"a %s\", \"file\": None}\n    prnt3 = {\"print_args\": [1, 2], \"format_string\": \"a %s\", \"file\": \"f\"}\n    assert p._print_Print(prnt) == \"print(1, 2)\"\n    assert p._print_Print(prnt2) == \"print(a % (1, 2))\"\n    assert p._print_Print(prnt3"], "sample_556": ["def test_add_gridspec():\n    fig = plt.figure()\n    gs = fig.add_gridspec(3, 3)\n    assert gs.figure is fig\n    assert isinstance(gs, gridspec.GridSpec)\n    assert gs.nrows == 3\n    assert gs.ncols == 3\n    assert gs.figure_adaptive is True\n", "def test_add_subfigure():\n    fig = plt.figure(layout='constrained')\n    gs = fig.add_gridspec(1, 2)\n    sfig = fig.add_subfigure(gs[0], label='subfig')\n    assert sfig.get_label() == 'subfig'\n    sfigs = fig.subfigures(1, 2, label='subfigs')\n    assert sfigs[0].get_label() == 'subfigs'\n", "def test_subfigure_subplotspec_position():\n    fig = plt.figure()\n    subfig = fig.add_subfigure([0, 0, 1, 1])\n    gs = subfig.add_gridspec(1, 1)\n    ax = subfig.add_subplot(gs[0, 0])\n    assert subfig._subplotspec == gs[0, 0].get_topmost_subplotspec()\n    assert ax.get_subplotspec() == gs[0, 0]\n", "def test_fgetsizes():\n    fig, ax = plt.subplots()\n    ax.set_position([0.2, 0.2, 0.4, 0.4])\n    np.testing.assert_allclose(fig.get_size_inches(), [8, 6])\n    fig.set_size_inches([10, 8])\n    np.testing.assert_allclose(fig.get_size_inches(), [10, 8])\n", "def test_align_labels_twotuple():\n    # Test the align_labels functionality for axes with a shared axis\n    fig, axs = plt.subplots(2, 2, figsize=(10, 5))\n    axs[0, 0].plot([1, 2, 3])\n    axs[0, 0].set_xlabel('XLabel 1')\n    axs[0, 1].plot([1, 2, 3])\n    axs[0, 1].set_xlabel('XLabel 2')\n    axs[1, 0].plot([1, 2, 3])\n    axs[1, 0].set_xlabel('XLabel 3')\n    axs[1, 1].plot([1, 2, 3])\n    axs[1, 1].set_xlabel('XLabel 4')\n\n    axs[0, 0].sharex(axs[1, 0])\n    axs[0, 1].sharex(axs[1, 1])\n\n    fig.align_labels(axs=[(0, 0), (0, 1), (1, 0), (1, 1)])\n\n    x0 = axs[0, 0].get_xlabel().get_window_extent().x0\n    x1 = axs[0, 1].get_xlabel().get_window_extent().x0\n    y0 = axs[0, 0].get_xlabel().get_window_extent().y0\n    y1 = axs[0, 1].get_xlabel().get_window_extent().y0\n\n    np.testing.assert_allclose(x0, x1)\n    np.testing.assert_allclose(y0, y1)\n"], "sample_276": ["    def test_model_with_many_to_one_backward_relation(self):\n        response = self.client.get(reverse('django-admindocs-models-detail', args=['admin_docs', 'Person']))\n        self.assertContains(response, \"related admin_docs.Company objects\")\n        self.assertContains(response, \"all related admin_docs.Company objects\")\n        self.assertContains(response, \"number of related admin_docs.Company objects\")\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n", "    def test_model_detail_with_foreign_key(self):\n        \"\"\"\n        A model with a foreign key should show the foreign key as a field.\n        \"\"\"\n        response = self.client.get(reverse('django-admindocs-models-detail', args=['admin_docs', 'Person']))\n        fields = response.context_data.get('fields')\n        self.assertIn('company', [field['name'] for field in fields])\n", "    def test_related_object(self):\n        \"\"\"\n        Model with related objects should be correctly displayed.\n        \"\"\"\n        response = self.client.get(reverse('django-admindocs-models-detail', args=['admin_docs', 'Company']))\n        self.assertContains(response, 'related <a class=\"reference external\" href=\"/admindocs/models/admin_docs.Person/\">admin_docs.Person</a> objects')\n", "    def setUp(self):\n        self.old_view_functions = views.extract_views_from_urlpatterns\n        views.extract_views_from_urlpatterns = self.mock_extract_views_from_urlpatterns\n        self.views = []\n"], "sample_1051": ["def test_dotprint_atom():\n    text = dotprint(x+2, atom=lambda x: not isinstance(x, Basic))\n    assert all(e in text for e in dotedges(x+2, atom=lambda x: not isinstance(x, Basic), repeat=False))\n    assert all(n in text for n in [dotnode(expr, repeat=False) for expr in (x, Integer(2), x+2)])\n    assert 'digraph' in text\n\n    text = dotprint(x+x**2, atom=lambda x: not x.args)\n    assert all(e in text for e in dotedges(x+x**2, atom=lambda x: not x.args, repeat=False))\n    assert all(n in text for n in [dotnode(expr, repeat=False) for expr in (x, x**2, x+x**2)])\n    assert 'digraph' in text\n", "def test_styleof_priority():\n    styles = [(Expr,  {'color': 'red'}), \n              (Basic, {'color': 'blue', 'shape': 'ellipse'})]\n    assert styleof(x + 1, styles) == {'color': 'red', 'shape': 'ellipse'}\n", "def test_dotprint_atom():\n    text = dotprint(x+2, atom=lambda x: not x.is_Atom)\n    assert all(e in text for e in dotedges(x+2))\n    assert all(n in text for n in [dotnode(expr) for expr in (x, Integer(2), x+2)])\n    assert 'digraph' in text\n", "def test_dotprint_maxdepth():\n    text = dotprint(x+x**2, maxdepth=1, repeat=False)\n    assert dotnode(x+x**2, repeat=False) in text\n    assert dotnode(x, repeat=False) in text\n    assert dotnode(x**2, repeat=False) not in text\n\n    text = dotprint(x+x**2, maxdepth=1, repeat=True)\n    assert dotnode(x+x**2, repeat=True) in text\n    assert dotnode(x, pos=(0,)) in text\n    assert dotnode(x, pos=(1,)) in text\n    assert dotnode(x**2, repeat=True) not in text\n\n    text = dotprint(x+x**2, maxdepth=2, repeat=False)\n    assert dotnode(x+x**2, repeat=False) in text\n    assert dotnode(x, repeat=False) in text\n    assert dotnode(x**2, repeat=False) in text\n    assert dotnode(x, pos=(1,0), repeat=False) in text\n\n    text = dotprint(x+x**2, maxdepth=2, repeat=True)\n    assert dotnode(x+x**2, repeat=True) in text\n    assert dotnode(x, pos=(0,), repeat=True) in text\n    assert dotnode(x, pos=(1,0), repeat=True) in text\n    assert dotnode(x**2, pos=(), repeat=True) in text\n", "def test_dotprint_atom():\n    text = dotprint(x+2, atom=lambda x: x.is_Atom)\n    assert all(e in text for e in dotedges(x+2, atom=lambda x: x.is_Atom))\n    assert all(n in text for n in [dotnode(expr, repeat=False) for expr in (x, Integer(2))])\n    assert dotnode(x+2, repeat=False) in text\n    assert 'digraph' in text\n"], "sample_1024": ["def test_issue_14106():\n    assert Float('1.001', 10).as_numer_denom()[1] == Integer(1000)\n", "def test_issue_13298():\n    assert same_and_same_prec(Float(E, 3, ''), Float(E, 3))\n    assert same_and_same_prec(Float(E, 3, 3), Float(E, 3, 3))\n    assert same_and_same_prec(Float(E, 3, ''), Float(E, 3, ''))\n    assert same_and_same_prec(Float(E, 3, '3'), Float(E, 3, 3))\n    assert same_and_same_prec(Float(E, 3, '3'), Float(E, 3, '3'))\n    assert same_and_same_prec(Float(E, 3, 3), Float(E, 3, '3'))\n    assert same_and_same_prec(Float(E, 3, ''), Float(E, 3, 3))\n    assert same_and_same_prec(Float(E, 3, 3), Float(E, 3, ''))\n", "def test_AlgebraicNumber():\n    from sympy import sqrt\n\n    assert str(AlgebraicNumber(sqrt(2)).coeffs()) == str([1, 0, -2])\n    assert str(AlgebraicNumber(sqrt(2)).native_coeffs()) == str([MPZ(1), MPZ(0), MPZ(-2)])\n    assert str(AlgebraicNumber(sqrt(2), [1, 0, -2]).coeffs()) == str([1, 0, -2])\n    assert str(AlgebraicNumber(sqrt(2), [1, 0, -2]).native_coeffs()) == str([MPZ(1), MPZ(0), MPZ(-2)])\n    assert AlgebraicNumber(sqrt(2)).root == sqrt(2)\n    assert AlgebraicNumber(sqrt(2)).minpoly == sqrt(2)**2 + 2\n    assert AlgebraicNumber(sqrt(2)).to_algebraic_integer() == AlgebraicNumber(sqrt(2))\n    assert AlgebraicNumber(sqrt(2)).evalf(10) == 1.414213562\n    assert str(AlgebraicNumber(sqrt(2)).as_expr()) == 'sqrt(2)'\n    assert str(AlgebraicNumber(sqrt(2)).as_poly()) == 'Poly(sqrt(2), sqrt(2), domain='\n", "def test_issue_12820():\n    # Test if the `prec` keyword is correctly deprecated in `Float`\n    Float(1, 10)\n    SymPyDeprecationWarning(\n        feature=\"Using 'prec=XX' to denote decimal precision\",\n        useinstead=\"'dps=XX' for decimal precision and 'precision=XX' \"\\\n                  \"for binary precision\",\n        issue=12820,\n        deprecated_since_version=\"1.1\").warn()\n    Float('1.0', precision=10)\n    SymPyDeprecationWarning(\n        feature=\"Using 'prec=XX' to denote decimal precision\",\n        useinstead=\"'dps=XX' for decimal precision and 'precision=XX' \"\\\n                  \"for binary precision\",\n        issue=12820,\n        deprecated_since_version=\"1.1\").warn()\n    SymPyDeprecationWarning(\n        feature=\"Using 'prec=XX' to denote decimal precision\",\n        useinstead=\"'dps=XX' for decimal precision and 'precision=XX' \"\\\n                  \"for binary precision\",\n        issue=12820,\n        deprecated_since_version=\"1.1\").reset()\n    Float(1, 10)  # should not raise warning\n", "def test_seterr():\n    seterr(divide=True)\n    raises(ValueError, lambda: S.Zero/S.Zero)\n    seterr(divide=False)\n    assert S.Zero / S.Zero == S.NaN\n"], "sample_607": ["def test_get_backend():\n    backend = plugins.get_backend(\"dummy\")\n    assert isinstance(backend, DummyBackendEntrypointArgs)\n\n    backend = plugins.get_backend(DummyBackendEntrypointArgs)\n    assert isinstance(backend, DummyBackendEntrypointArgs)\n\n    with pytest.raises(ValueError):\n        plugins.get_backend(\"non-existent-engine\")\n\n    with pytest.raises(TypeError):\n        plugins.get_backend(1)\n", "def test_get_backend():\n    backend = plugins.get_backend(\"netcdf4\")\n    assert isinstance(backend, common.BackendEntrypoint)\n    assert backend.open_dataset_parameters is not None\n\n    class DummyBackend(common.BackendEntrypoint):\n        pass\n\n    backend = plugins.get_backend(DummyBackend)\n    assert backend is DummyBackend\n\n    with pytest.raises(ValueError):\n        plugins.get_backend(\"non-existent-backend\")\n\n    with pytest.raises(TypeError):\n        plugins.get_backend(\"not-a-backend\")\n", "def test_get_backend():\n    backend_name = \"dummy\"\n    backend_cls = DummyBackendEntrypoint1\n\n    # Test with string\n    backend = plugins.get_backend(backend_name)\n    assert isinstance(backend, backend_cls)\n\n    # Test with BackendEntrypoint class\n    backend = plugins.get_backend(backend_cls)\n    assert isinstance(backend, backend_cls)\n\n    # Test with invalid input\n    with pytest.raises(TypeError):\n        plugins.get_backend(\"invalid\")\n\n    with pytest.raises(TypeError):\n        plugins.get_backend(object)\n\n    # Test with non-existent backend\n    with pytest.raises(ValueError):\n        plugins.get_backend(\"non-existent\")\n", "def test_get_backend():\n    backend_1 = DummyBackendEntrypoint1\n    backend_2 = DummyBackendEntrypoint2\n    engines = {\"engine_1\": backend_1(), \"engine_2\": backend_2()}\n    assert isinstance(plugins.get_backend(\"engine_1\"), backend_1)\n    assert plugins.get_backend(\"engine_1\") == engines[\"engine_1\"]\n\n    assert isinstance(plugins.get_backend(backend_1), backend_1)\n\n    with pytest.raises(ValueError):\n        plugins.get_backend(\"non-existent-engine\")\n\n    with pytest.raises(TypeError):\n        plugins.get_backend(\"not-a-backend\")\n", "def test_get_backend():\n    backend_name = \"dummy_backend\"\n    backend_class = DummyBackendEntrypoint1\n\n    with mock.patch(\"xarray.backends.plugins.list_engines\", return_value={backend_name: backend_class()}):\n        backend = plugins.get_backend(backend_name)\n        assert isinstance(backend, backend_class)\n\n    with mock.patch(\"xarray.backends.plugins.list_engines\", return_value={}):\n        with pytest.raises(ValueError, match=f\"unrecognized engine {backend_name}\"):\n            plugins.get_backend(backend_name)\n\n    backend = plugins.get_backend(backend_class)\n    assert backend is backend_class\n\n    with pytest.raises(TypeError, match=\"engine must be a string or a subclass of\"):\n        plugins.get_backend(\"not a backend\")\n"], "sample_764": ["def test_column_transformer_empty_remainder_remainder_transformer():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).T\n\n    ct = ColumnTransformer([('trans1', Trans(), [0, 1, 2])],\n                           remainder=DoubleTrans())\n\n    assert_array_equal(ct.fit_transform(X_array), X_array)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_array)\n    assert len(ct.transformers_) == 1\n    assert ct.transformers_[-1][0] != 'remainder'\n\n    # check the remainder transformer was not fitted\n    assert_false(hasattr(ct.transformers_[-1][1], 'transformer_weights'))\n", "def test_column_transformer_with_multiple_targets():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    y_array = np.array([[0, 1], [2, 4]])\n    y_list = list(y_array)\n\n    # Test with a single transformer\n    ct = ColumnTransformer([('trans', Trans(), [0, 1])])\n    assert_array_equal(ct.fit_transform(X_array, y=y_array), X_array)\n    assert_array_equal(ct.fit(X_array, y=y_array).transform(X_array), X_array)\n    assert_array_equal(ct.fit_transform(X_array, y=y_list), X_array)\n    assert_array_equal(ct.fit(X_array, y=y_list).transform(X_array), X_array)\n\n    # Test with multiple transformers\n    ct = ColumnTransformer([('trans1', Trans(), [0]), ('trans2', Trans(), [1])])\n    assert_array_equal(ct.fit_transform(X_array, y=y_array), X_array)\n    assert_array_equal(ct.fit(X_array, y=y_array).transform(X_array), X_array)\n    assert_array_equal(ct.fit_transform(X_array, y=y_list), X_array)\n    assert_array_equal(ct.fit(X_array, y=y_list).transform(X_array), X_array)\n", "def test_column_transformer_weights_invalid_input():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    ct = ColumnTransformer([('trans1', Trans(), [0]), ('trans2', Trans(), [1])],\n                           transformer_weights='invalid')\n\n    assert_raise_message(TypeError, \"transformer_weights should be a dict\",\n                         ct.fit, X_array)\n\n    ct = ColumnTransformer([('trans1', Trans(), [0]), ('trans2', Trans(), [1])],\n                           transformer_weights=1)\n\n    assert_raise_message(TypeError, \"transformer_weights should be a dict\",\n                         ct.fit, X_array)\n\n    ct = ColumnTransformer([('trans1', Trans(), [0]), ('trans2', Trans(), [1])],\n                           transformer_weights={'trans1': 'a'})\n\n    assert_raise_message(TypeError, \"transformer_weights should be a dict\",\n                         ct.fit, X_array)\n\n    ct = ColumnTransformer([('trans1', Trans(), [0]), ('trans2', Trans(), [1])],\n                           transformer_weights={'trans1': 1, 'trans2': 0})\n\n    assert_raise_message(ValueError, \"Transformer weights should all be positive.\",\n                         ct.fit, X_array)\n\n    ct = ColumnTransformer([('trans1', Trans(), [0]), ('trans2', Trans(), [1])],\n                           transformer_weights={'trans1': 1, 'trans3': 1})\n\n    assert_raise_message(ValueError, \"Transformer trans3 not found in the list \"\n                         \"of transformers: ['trans1', 'trans2'].\",\n                         ct.fit, X_array)\n", "def test_column_transformer_empty_dataframe():\n    pd = pytest.importorskip('pandas')\n    # Create an empty dataframe\n    df_empty = pd.DataFrame(columns=['col1', 'col2'])\n\n    # Test that ColumnTransformer does not raise an error when fitting on an empty dataframe\n    ct = ColumnTransformer([('trans', Trans(), 'col1')])\n    ct.fit(df_empty)\n    assert len(ct.transformers_) == 1\n\n    # Test that ColumnTransformer returns an empty dataframe when transforming on an empty dataframe\n    assert df_empty.equals(ct.transform(df_empty))\n\n    # Test that ColumnTransformer raises an error when trying to get feature names on an empty dataframe\n    assert_raises(NotFittedError, ct.get_feature_names)\n\n    # Test that ColumnTransformer works correctly when fitting and transforming on a non-empty dataframe after fitting on an empty dataframe\n    df_non_empty = pd.DataFrame([[1, 2], [3, 4]], columns=['col1', 'col2'])\n    ct.fit(df_non_empty)\n    assert_array_equal(ct.transform(df_non_empty).values, df_non_empty.values)\n", "def test_column_transformer_transformer_weights():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    ct = ColumnTransformer([('trans1', Trans(), [0]),\n                            ('trans2', Trans(), [1])],\n                           transformer_weights={'trans1': .1, 'trans2': 10})\n    X_res_first = np.array([0, 1, 2]).reshape(-1, 1)\n    X_res_second = np.array([2, 4, 6]).reshape(-1, 1)\n    X_res_both = np.vstack([.1 * X_res_first, 10 * X_res_second]).T\n    assert_array_equal(ct.fit_transform(X_array), X_res_both)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both)\n    assert len(ct.transformers_) == 2\n\n    # all 'passthrough'\n    ct = ColumnTransformer([('trans1', 'passthrough', [0]),\n                            ('trans2', 'passthrough', [1])],\n                           transformer_weights={'trans1': .1, 'trans2': 10})\n    assert_array_equal(ct.fit_transform(X_array), X_res_both)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both)\n    assert len(ct.transformers_) == 2\n\n    # 'drop'\n    ct = ColumnTransformer([('trans1', 'drop', [0]),\n                            ('trans2', Trans(), [1])],\n                           transformer_weights={'trans1': .1, 'trans2': 10})\n    X_res_both = np.vstack([10 * X_res_second]).T\n    assert_array_equal(ct.fit_transform(X_array), X_res_both)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both)\n    assert len(ct.transformers_) == 2\n\n    # wrong transformer weights\n    ct = ColumnTransformer([('trans1', Trans(), [0]),\n                            ('trans2', Trans(), [1])],\n                           transformer_weights={'trans': .1, 'trans2': 10})\n    assert_raise_message(ValueError, \"Transformer trans is not present in \"\n                         \"transformers list\", ct.fit_transform, X_array)\n\n    # check when transformer is None\n    ct = ColumnTransformer([('trans1', None, [0]),\n                            ('trans2', Trans(), [1])],\n                           transformer_weights={'trans1': .1, '"], "sample_306": ["    def test_parse_datetime_timezone_offset(self):\n        valid_inputs = (\n            ('2012-04-23T09:15:00+02', datetime(2012, 4, 23, 9, 15, 0, 0, get_fixed_timezone(120))),\n            ('2012-04-23T09:15:00+02:30', datetime(2012, 4, 23, 9, 15, 0, 0, get_fixed_timezone(150))),\n            ('2012-04-23T09:15:00+02:30:45', datetime(2012, 4, 23, 9, 15, 0, 0, get_fixed_timezone(150))),\n            ('2012-04-23T09:15:00-02', datetime(2012, 4, 23, 9, 15, 0, 0, get_fixed_timezone(-120))),\n            ('2012-04-23T09:15:00-02:30', datetime(2012, 4, 23, 9, 15, 0, 0, get_fixed_timezone(-150))),\n            ('2012-04-23T09:15:00-02:30:45', datetime(2012, 4, 23, 9, 15, 0, 0, get_fixed_timezone(-150))),\n        )\n        for source, expected in valid_inputs:\n            with self.subTest(source=source):\n                self.assertEqual(parse_datetime(source), expected)\n\n        # Invalid inputs\n        self.assertIsNone(parse_datetime('2012-04-23T09:15:00+25'))\n        self.assertIsNone(parse_datetime('2012-04-23T09:15:00+02:60'))\n        self.assertIsNone(parse_datetime('2012-04-23T09:15:00+02:30:60'))\n", "    def test_parse_date_with_spaces(self):\n        self.assertIsNone(parse_date('2012 - 04 - 23'))\n", "    def test_parse_empty_string(self):\n        self.assertIsNone(parse_date(''))\n        self.assertIsNone(parse_time(''))\n        self.assertIsNone(parse_datetime(''))\n        self.assertIsNone(parse_duration(''))\n", "    def test_zero_duration(self):\n        self.assertEqual(parse_duration('0'), timedelta(0))\n        self.assertEqual(parse_duration('0:0:0'), timedelta(0))\n        self.assertEqual(parse_duration('0 days'), timedelta(0))\n        self.assertEqual(parse_duration('PT0S'), timedelta(0))\n", "def test_parse_invalid_duration_formats(self):\n    test_values = (\n        'abc',\n        '12:34',\n        '12:34:',\n        '12::',\n        ':34',\n        ':',\n        '12:34:56:78',\n        '12345',\n        '1 day 2',\n        '1 2:3',\n        '-1 -2:3',\n        '-1:2:3',\n        '1 day -2 3:4',\n        '1 day 2:3:4:5',\n        'PT5',\n    )\n    for source in test_values:\n        with self.subTest(source=source):\n            self.assertIsNone(parse_duration(source))\n"], "sample_829": ["def test_incremental_pca_batch_size_zero():\n    # Test that batch_size cannot be zero.\n    X = np.array([[0, 1, 0], [1, 0, 0]])\n    assert_raises_regex(ValueError,\n                        \"batch_size must be greater than zero\",\n                        IncrementalPCA(n_components=2, batch_size=0).fit, X)\n", "def test_incremental_pca_edge_cases():\n    # Test edge cases for IncrementalPCA\n    rng = np.random.RandomState(1999)\n\n    # Test with a single sample\n    X = rng.rand(1, 5)\n    ipca = IncrementalPCA(n_components=1).fit(X)\n    assert ipca.components_.shape == (1, 5)\n\n    # Test with a single feature\n    X = rng.rand(5, 1)\n    ipca = IncrementalPCA(n_components=1).fit(X)\n    assert ipca.components_.shape == (1, 1)\n\n    # Test with a square matrix\n    X = rng.rand(5, 5)\n    ipca = IncrementalPCA(n_components=5).fit(X)\n    assert ipca.components_.shape == (5, 5)\n\n    # Test with a matrix with more features than samples\n    X = rng.rand(5, 10)\n    ipca = IncrementalPCA(n_components=5).fit(X)\n    assert ipca.components_.shape == (5, 10)\n\n    # Test with a matrix with more samples than features\n    X = rng.rand(10, 5)\n    ipca = IncrementalPCA(n_components=5).fit(X)\n    assert ipca.components_.shape == (5, 5)\n", "def test_incremental_pca_transform_batch_signs():\n    # Test that components_ sign is stable over batch sizes.\n    rng = np.random.RandomState(1999)\n    n_samples = 100\n    n_features = 3\n    X = rng.randn(n_samples, n_features)\n    batch_sizes = np.arange(10, 20)\n    for batch_size in batch_sizes:\n        ipca = IncrementalPCA(n_components=None, batch_size=batch_size).fit(X)\n        X_transformed_batch = ipca.transform(X)\n        ipca_full = IncrementalPCA(n_components=None).fit(X)\n        X_transformed_full = ipca_full.transform(X)\n        assert_almost_equal(np.abs(X_transformed_batch), np.abs(X_transformed_full), decimal=3)\n", "def test_incremental_pca_inverse_whitening():\n    # Test that the projection of data can be inverted with whitening.\n    rng = np.random.RandomState(1999)\n    n, p = 50, 3\n    X = rng.randn(n, p)  # spherical data\n    X[:, 1] *= .00001  # make middle component relatively small\n    X += [5, 4, 3]  # make a large mean\n\n    # same check that we can find the original data from the transformed\n    # signal (since the data is almost of rank n_components)\n    ipca = IncrementalPCA(n_components=2, batch_size=10, whiten=True).fit(X)\n    Y = ipca.transform(X)\n    Y_inverse = ipca.inverse_transform(Y)\n    assert_almost_equal(X, Y_inverse, decimal=3)\n\n    # Test that the inverse transform of a transformed array is correct\n    # even after the PCA object has been used to transform new data\n    X2 = rng.randn(n, p)  # spherical data\n    X2[:, 1] *= .00001  # make middle component relatively small\n    X2 += [5, 4, 3]  # make a large mean\n    ipca.transform(X2)\n    Y_inverse_2 = ipca.inverse_transform(Y)\n    assert_almost_equal(X, Y_inverse_2, decimal=3)\n", "def test_incremental_pca_batch_size():\n    # Test that batch_size is being utilized correctly\n    rng = np.random.RandomState(1999)\n    n_samples = 100\n    n_features = 3\n    X = rng.randn(n_samples, n_features)\n    X2 = rng.randn(n_samples, n_features)\n    X3 = rng.randn(n_samples, n_features)\n\n    # Decreasing batch_size\n    ipca = IncrementalPCA(n_components=2, batch_size=20).fit(X)\n    assert ipca.batch_size_ == 20\n\n    # Increasing batch_size\n    ipca.set_params(batch_size=30)\n    assert ipca.batch_size_ == 30\n\n    # Returning to original setting\n    ipca.set_params(batch_size=20)\n    assert ipca.batch_size_ == 20\n\n    # Setting batch_size to None\n    ipca.set_params(batch_size=None)\n    assert ipca.batch_size_ == 5 * n_features\n\n    # Test that batch_size is not smaller than min_batch_size\n    ipca.set_params(batch_size=1)\n    assert ipca.batch_size_ >= 2\n\n    # Test that batch_size is not larger than n_samples\n    ipca.set_params(batch_size=n_samples + 1)\n    assert ipca.batch_size_ <= n_samples\n"], "sample_216": ["def test_foreign_key_renamed_model(self):\n    \"\"\"Tests autodetection of ForeignKey fields referencing renamed models.\"\"\"\n    changes = self.get_changes(\n        [self.author_with_publisher, self.publisher],\n        [self.author_with_publisher, self.publisher_with_author],\n        MigrationQuestioner({\"ask_rename\": True}),\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"RenameField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name=\"author\", new_name=\"publisher\")\n", "def test_alter_field_to_jsonfield(self):\n    \"\"\"\n    #25501 - Tests autodetection of AlterField to a JSONField.\n    \"\"\"\n    from_state = ModelState(\"testapp\", \"model\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"data\", models.CharField(max_length=200)),\n    ])\n    to_state = ModelState(\"testapp\", \"model\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"data\", models.JSONField(max_length=200)),\n    ])\n    changes = self.get_changes([from_state], [to_state])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"data\")\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, encoder=None, decoder=None)\n", "def test_get_references(self):\n    \"\"\"\n    Tests that get_references() correctly generates (model_state, name, field, reference)\n    tuples referencing provided context.\n    \"\"\"\n    state = ProjectState()\n    state.add_model(self.author_empty.clone())\n    state.add_model(self.book.clone())\n    references = list(get_references(state, (\"testapp\", \"Author\")))\n    self.assertEqual(len(references), 1)\n    self.assertEqual(references[0][0], state.apps[\"testapp\"].models[\"author\"])\n    self.assertEqual(references[0][1], \"author\")\n    self.assertEqual(references[0][2], self.book.fields[\"author\"])\n    self.assertEqual(references[0][3].to, (self.book.remote_fields[\"author\"], None))\n\n    state = ProjectState()\n    state.add_model(self.author_empty.clone())\n    state.add_model(self.book.clone())\n    references = list(get_references(state, (\"testapp\", \"Author\"), (\"author\",)))\n    self.assertEqual(len(references), 1)\n    self.assertEqual(references[0][0], state.apps[\"testapp\"].models[\"author\"])\n    self.assertEqual(references[0][1], \"author\")\n    self.assertEqual(references[0][2], self.book.fields[\"author\"])\n    self.assertEqual(references[0][3].to, (self.book.remote_fields[\"author\"], None))\n\n    state = ProjectState()\n    state.add_model(self.author_empty.clone())\n    state.add_model(self.book.clone())\n    references = list(get_references(state, (\"otherapp\", \"Book\")))\n    self.assertEqual(len(references), 0)\n\n    state = ProjectState()\n    state.add_model(self.author_empty.clone())\n    state.add_model(self.book.clone())\n    references = list(get_references(state, (\"otherapp\", \"Book\"), (\"author\",)))\n    self.assertEqual(len(references), 1)\n    self.assertEqual(references[0][0], state.apps[\"otherapp\"].models[\"book\"])\n    self.assertEqual(references[0][1], \"author\")\n    self.assertEqual(references[0][2], self.book.fields[\"author\"])\n    self.assertEqual(references[0][3].to, (self.book.remote_fields[\"author\"], None))\n", "def test_field_references(self):\n    model_state = ModelState(\"app\", \"Model\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"author\", models.ForeignKey(\"app.Author\", models.CASCADE)),\n        (\"book\", models.ForeignKey(\"app.Book\", models.CASCADE)),\n    ])\n    reference = field_references(\n        (\"app\", \"Model\"),\n        model_state.fields[\"author\"],\n        (\"app\", \"Author\"),\n        reference_field_name=\"name\",\n        reference_field=model_state.fields[\"author\"],\n    )\n    self.assertIsInstance(reference, FieldReference)\n    self.assertEqual(reference.to, (model_state.fields[\"author\"], None))\n    self.assertIsNone(reference.through)\n\n    reference = field_references(\n        (\"app\", \"Model\"),\n        model_state.fields[\"author\"],\n        (\"app\", \"Author\"),\n        reference_field_name=None,\n        reference_field=None,\n    )\n    self.assertIsInstance(reference, FieldReference)\n    self.assertEqual(reference.to, (model_state.fields[\"author\"], None))\n    self.assertIsNone(reference.through)\n\n    reference = field_references(\n        (\"app\", \"Model\"),\n        model_state.fields[\"book\"],\n        (\"app\", \"Author\"),\n        reference_field_name=\"name\",\n        reference_field=model_state.fields[\"author\"],\n    )\n    self.assertFalse(reference)\n\n    reference = field_references(\n        (\"app\", \"Model\"),\n        model_state.fields[\"author\"],\n        (\"app\", \"Author\"),\n        reference_field_name=\"name\",\n        reference_field=model_state.fields[\"book\"],\n    )\n    self.assertFalse(reference)\n\n    reference = field_references(\n        (\"app\", \"Model\"),\n        model_state.fields[\"author\"],\n        (\"app\", \"Author\"),\n        reference_field_name=\"nonexistent\",\n        reference_field=model_state.fields[\"author\"],\n    )\n    self.assertFalse(reference)\n\n    reference = field_references(\n        (\"app\", \"Model\"),\n        model_state.fields[\"author\"],\n        (\"app\", \"Author\"),\n        reference_field_name=\"name\",\n        reference_field=model_state.fields[\"author\"],\n        through_fields=(\"nonexistent\",),\n    )\n    self.assertFalse(reference)\n", "def test_field_references_through_foreign_object(self):\n    \"\"\"\n    Tests the correct identification of references to a model by another model\n    through a ForeignObject.\n    \"\"\"\n    state = ProjectState()\n    state.add_model(ModelState(\"testapp\", \"Foo\", [\n        (\"id\", models.AutoField(primary_key=True)),\n    ]))\n    state.add_model(ModelState(\"testapp\", \"Bar\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"foo\", models.ForeignObject(\"testapp.Foo\", models.CASCADE)),\n    ]))\n\n    field_references_list = list(get_references(state, (\"testapp\", \"foo\"), ()))\n    self.assertEqual(len(field_references_list), 1)\n    model_state, name, field, reference = field_references_list[0]\n    self.assertEqual(model_state.app_label, \"testapp\")\n    self.assertEqual(model_state.name, \"Bar\")\n    self.assertEqual(name, \"foo\")\n    self.assertIsInstance(field, models.ForeignObject)\n    self.assertIsInstance(reference, FieldReference)\n    self.assertIsNotNone(reference.through)\n    self.assertIsNone(reference.to)\n"], "sample_1020": ["def test_user_functions():\n    user_functions = {'f': 'my_f', 'g': 'my_g'}\n    assert mcode(f(x), user_functions=user_functions) == \"my_f[x]\"\n    assert mcode(g(x), user_functions=user_functions) == \"g[x]\"\n    assert mcode(g(x), user_functions={'g': 'my_g'}) == \"my_g[x]\"\n", "def test_user_functions():\n    assert mcode(sin(x), user_functions={'sin': 'MySin'}) == \"MySin[x]\"\n    assert mcode(Max(x, y, z), user_functions={'Max': 'MyMax'}) == \"MyMax[x, y, z]\"\n    assert mcode(exp(x), user_functions={'exp': 'MyExp'}) == \"MyExp[x]\"\n    assert mcode(sin(x) + Max(x, y, z), user_functions={'sin': 'MySin', 'Max': 'MyMax'}) == \"MySin[x] + MyMax[x, y, z]\"\n", "def test_user_functions():\n        return x**2 + 3*x + 2\n\n    assert mcode(user_func(x), user_functions={'user_func': 'my_func'}) == \\\n        \"my_func[x]\"\n    assert mcode(user_func(x), user_functions={'user_func': lambda x: True}) == \\\n        \"user_func[x]\"\n", "def test_Precendence():\n    assert mcode(1 + 2*3) == \"(1 + 2*3)\"\n    assert mcode(1 - 2*3) == \"(1 - 2*3)\"\n    assert mcode(1*2 + 3) == \"2 + 3\"\n    assert mcode(1*2 - 3) == \"2 - 3\"\n    assert mcode(1/2 + 3) == \"(1/2) + 3\"\n    assert mcode(1/2 - 3) == \"(1/2) - 3\"\n    assert mcode(x + y*z) == \"(x + y*z)\"\n    assert mcode(x - y*z) == \"(x - y*z)\"\n    assert mcode(x*y + z) == \"x*y + z\"\n    assert mcode(x*y - z) == \"x*y - z\"\n    assert mcode(x/y + z) == \"(x/y) + z\"\n    assert mcode(x/y - z) == \"(x/y) - z\"\n", "def test_NegativePow():\n    assert mcode(x**(-3)) == \"x^(-3)\"\n    assert mcode(x**(-Rational(2, 3))) == 'x^(-2/3)'\n    assert mcode(x**(-1.5)) == 'x^(-1.5)'\n    assert mcode(x**(-oo)) == \"x^(-Infinity)\"\n    assert mcode(x**(-S.NegativeInfinity)) == \"x^(-Infinity)\"\n    assert mcode((-x)**(3.5)) == \"(-x)^3.5\"\n    assert mcode((-x)**Rational(3, 2)) == \"(-x)^(3/2)\"\n    assert mcode((-x)**oo) == \"(-x)^Infinity\"\n    assert mcode((-x)**S.NegativeInfinity) == \"(-x)^(-Infinity)\"\n"], "sample_89": ["def test_invalid_timeout(self):\n    with self.assertRaises(ValueError):\n        autoreload.WatchmanReloader()\n", "    def test_common_roots_with_empty_path(self):\n        paths = (\n            Path('/first/second'),\n            Path('/first/second/third'),\n            Path('/first/'),\n            Path('/root/first/'),\n            Path(''),  # Empty path\n        )\n        results = autoreload.common_roots(paths)\n        self.assertCountEqual(results, [Path('/')])\n", "    def setUp(self):\n        self.stream = mock.Mock()\n        self.stream.isatty.return_value = True\n        self.termios = mock.Mock()\n        self.termios.tcgetattr.return_value = [0, 0, 0, 0]\n        patches = [\n            mock.patch('sys.stdin', self.stream),\n            mock.patch('django.utils.autoreload.termios', self.termios),\n        ]\n        for p in patches:\n            p.start()\n            self.addCleanup(p.stop)\n", "    def setUp(self):\n        self.reloader = autoreload.StatReloader()\n        self.reloader.watched_files = mock.MagicMock(return_value=[self.reloader.tempdir / 'test.py'])\n", "    def test_tick_calls_receive(self, mocked_client, mocked_request_finished):\n        with mock.patch.object(self.reloader, 'update_watches') as mocked_update_watches:\n            self.reloader.tick()\n            next(self.reloader.tick())\n        self.assertEqual(mocked_client.return_value.receive.call_count, 1)\n        self.assertEqual(mocked_request_finished.connect.call_count, 1)\n        self.assertEqual(mocked_update_watches.call_count, 1)\n"], "sample_508": ["def test_set_label():\n    art = martist.Artist()\n    with pytest.raises(TypeError, match='^.* object has no property'):\n        art.set_label(123)\n    art.set_label('label')\n    assert art.get_label() == 'label'\n    art.set_label(None)\n    assert art.get_label() is None\n", "def test_artist_update_from():\n    class TestArtist1(martist.Artist):\n            super().__init__()\n            self.a = a\n\n            self.a = a\n\n    class TestArtist2(martist.Artist):\n            super().__init__()\n            self.b = b\n\n            self.b = b\n\n    a1 = TestArtist1(1)\n    a2 = TestArtist2(2)\n    a1.update_from(a2)\n    assert a1.a == 2\n    assert not hasattr(a1, 'b')\n", "def test_artist_inspector_pprint_setters_rest():\n    # test the correct format and type of pprint_setters_rest method\n    ai = martist.ArtistInspector(mlines.Line2D)\n    setters = ai.pprint_setters_rest()\n    assert isinstance(setters, list)\n    assert all(isinstance(line, str) for line in setters)\n    assert ':class:`property-table`' in setters[2]\n    assert 'Property' in setters[3]\n    assert 'Description' in setters[3]\n    assert 'linestyle' in setters[5]\n    assert 'linewidth' in setters[6]\n", "def test_get_transformed_clip_path_and_affine():\n    fig, ax = plt.subplots()\n    rect = mpatches.Rectangle((0.1, 0.1), 0.5, 0.5, transform=ax.transAxes)\n\n    path, affine = rect.get_transformed_clip_path_and_affine()\n    assert path is None\n    assert affine is None\n\n    clip_path = mpath.Path([[0, 0], [1, 0], [1, 1], [0, 1], [0, 0]])\n    rect.set_clip_path(clip_path, ax.transAxes)\n\n    path, affine = rect.get_transformed_clip_path_and_affine()\n    assert path is not None\n    assert affine is not None\n    assert np.allclose(path.vertices, clip_path.vertices)\n    assert np.allclose(affine.get_matrix(), ax.transAxes.get_matrix())\n\n    rect.set_clip_path(None)\n\n    path, affine = rect.get_transformed_clip_path_and_affine()\n    assert path is None\n    assert affine is None\n", "def test_set_pick_radius():\n    fig, ax = plt.subplots()\n    line, = ax.plot([0, 1])\n\n        return (event.xdata, event.ydata) == (0.5, 0.5), {}\n\n    line.set_picker(custom_picker)\n    assert line.get_picker() == custom_picker\n\n    line.set_picker(5)\n    assert line.get_picker() == 5\n\n    line.set_picker(None)\n    assert line.get_picker() is None\n\n    line.set_picker(True)\n    assert line.get_picker() is True\n\n    with pytest.raises(TypeError, match='picker must be None,'):\n        line.set_picker('string')\n"], "sample_1046": ["def test_index_structure():\n    L = TensorIndexType(\"L\")\n\n    i, j, k, l = tensor_indices(\"i j k l\", L)\n    A, B = tensorhead(\"A B\", [L]*2, [[1]*2])\n\n    e1 = A(i, j)\n    is1 = get_index_structure(e1)\n    assert is1.free == [(i, 0), (j, 1)]\n    assert is1.dum == []\n    assert is1.index_types == [L, L]\n    assert is1.indices == [i, j]\n\n    e2 = A(i, -i)\n    is2 = get_index_structure(e2)\n    assert is2.free == []\n    assert is2.dum == [(0, 1)]\n    assert is2.index_types == [L, L]\n    assert is2.indices == [i, -i]\n\n    e3 = A(i, j)*B(-i, -j)\n    is3 = get_index_structure(e3)\n    assert is3.free == []\n    assert is3.dum == [(0, 2), (1, 3)]\n    assert is3.index_types == [L, L, L, L]\n    assert is3.indices == [i, j, -i, -j]\n", "def test_valued_tensor_TensorElement():\n    (A, B, AB, BA, C, Lorentz, E, px, py, pz, LorentzD, mu0, mu1, mu2, ndm, n0, n1,\n     n2, NA, NB, NC, minkowski, ba_matrix, ndm_matrix, i0, i1, i2, i3, i4) = _get_valued_base_test_variables()\n\n    # iteration on VTensorHead\n    te = TensorElement(A, {i0: 1})\n    assert list(te) == [E, px, py, pz]\n\n    # iteration on VTensMul\n    assert list(te(i1)) == [E, px, py, pz]\n    assert list(te(i2)) == [E, px, py, pz]\n    assert list(te(i3)) == [E, px, py, pz]\n    assert list(te(i4)) == [E, px, py, pz]\n\n    # iteration on VTensAdd\n    # A(i1) + A(i1)\n    assert list(te + te) == [2*E, 2*px, 2*py, 2*pz]\n    assert list(te - te) == [0, 0, 0, 0]\n    assert list(BA(i1, i2) - 2 * BA(i1, i2)) == [-i for i in list(ba_matrix)]\n", "def test_components_data_destruction():\n    IT = TensorIndexType('IT', dim=3)\n    i0, i1, i2, i3 = tensor_indices('i0:4', IT)\n    A = tensorhead('A', [IT]*2, [[2]])\n    A.data = [[0, 0, 0], [0, 0, 0], [0, 0, 0]]\n    A(i0, -i0)\n    assert A.data == [[0, 0, 0], [0, 0, 0], [0, 0, 0]]\n    A._components_data_full_destroy()\n    assert A.data is None\n    del A\n\n    IT = TensorIndexType('IT', dim=3)\n    i0, i1, i2, i3 = tensor_indices('i0:4', IT)\n    A = tensorhead('A', [IT]*2, [[2]])\n    A.data = [[0, 0, 0], [0, 0, 0], [0, 0, 0]]\n    B = tensorhead('B', [IT], [[1]])\n    B.data = [0, 0, 0]\n    A(i0, -i0) * B(i1)\n    assert A.data == [[0, 0, 0], [0, 0, 0], [0, 0, 0]]\n    assert B.data == [0, 0, 0]\n    A._components_data_full_destroy()\n    assert A.data is None\n    assert B.data is None\n    del A\n    del B\n\n    IT = TensorIndexType('IT', dim=3)\n    i0, i1, i2, i3 = tensor_indices('i0:4', IT)\n    A = tensorhead('A', [IT]*2, [[2]])\n    A.data = [[0, 0, 0], [0, 0, 0], [0, 0, 0]]\n    B = tensorhead('B', [IT]*3, [[1]]*3)\n    B.data = [[[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n              [[0, 0, 0], [0, 0, 0], [0, 0, ", "def test_TensorHead_components_data():\n    L = TensorIndexType(\"L\", dim=2)\n    i, j = tensor_indices(\"i j\", L)\n\n    A = tensorhead(\"A\", [L, L], [[1]*2])\n    A.data = [[1, 2], [2, 1]]\n\n    assert list(A(i, j)) == [1, 2, 2, 1]\n    assert list(A(i, -j)) == [1, -2, -2, 1]\n    assert list(A(-i, j)) == [1, 2, -2, -1]\n    assert list(A(-i, -j)) == [1, -2, -2, 1]\n    raises(ValueError, lambda: A.data[0][0] = 3)\n\n    A = tensorhead(\"A\", [L, L], [[2]])\n    raises(ValueError, lambda: A.data = [[1, 2], [3, 4]])\n", "def test_TensorIndexType_withdummy_fmt():\n    # test that TensorIndexType _dummy_fmt works properly\n    Lorentz = TensorIndexType('Lorentz', dummy_fmt='a')\n    i0, i1 = tensor_indices('i0:2', Lorentz)\n    A, B = tensorhead('A B', [Lorentz]*2, [[1]*2])\n    t = A(i0, -i0)*B(i1, -i1)\n    assert t.canon_bp() == A('a_0', -'a_0')*B('a_1', -'a_1')\n    raises(ValueError, lambda: TensorIndexType('Lorentz', dummy_fmt='a%'))\n    raises(ValueError, lambda: TensorIndexType('Lorentz', dummy_fmt='a%d%d'))\n    raises(ValueError, lambda: TensorIndexType('Lorentz', dummy_fmt='%d'))\n"], "sample_546": ["def test_subfigure_repr():\n    fig = plt.figure()\n    subfig = fig.add_subfigure()\n    assert repr(subfig).startswith(\"<SubFigure\")\n", "def test_figure_text():\n    fig = plt.figure()\n    text = fig.text(0.5, 0.5, \"Hello world\")\n    assert text.get_transform() == fig.transSubfigure\n    text = fig.text(0.5, 0.5, \"Hello world\", transform=\"figure\")\n    assert text.get_transform() == fig.transFigure\n", "def test_constrained_layout_pads():\n    fig, ax = plt.subplots()\n    fig.set_constrained_layout(True)\n    assert fig.get_constrained_layout_pads() == (0.04167, 0.04167, 0.02, 0.02)\n    fig.set_constrained_layout_pads(w_pad=0.1, h_pad=0.1)\n    assert fig.get_constrained_layout_pads() == (0.1, 0.1, 0.02, 0.02)\n    fig.set_constrained_layout_pads(wspace=0.1, hspace=0.1)\n    assert fig.get_constrained_layout_pads() == (0.1, 0.1, 0.1, 0.1)\n    fig.set_constrained_layout_pads(w_pad=0.1, h_pad=0.1, wspace=0.1, hspace=0.1)\n    assert fig.get_constrained_layout_pads() == (0.1, 0.1, 0.1, 0.1)\n    assert fig.get_constrained_layout_pads(relative=True) == (0.1, 0.1, 0.1, 0.1)\n", "def test_figaspect(arg, expected):\n    assert figaspect(arg) == expected\n", "def test_subplot_mosaic():\n    fig = plt.figure()\n    layout = \"\"\"\n        AAE\n        C.E\n    \"\"\"\n    ret = fig.subplot_mosaic(layout, sharex=True, sharey=True)\n    assert len(ret) == 3\n    assert len(fig.axes) == 3\n    assert fig.axes[0].get_shared_x_axes().get_siblings(fig.axes[0]) == {fig.axes[1], fig.axes[2]}\n    assert fig.axes[0].get_shared_y_axes().get_siblings(fig.axes[0]) == {fig.axes[1], fig.axes[2]}\n\n    fig.clear()\n    layout = \"\"\"\n        AB\n        CC\n    \"\"\"\n    ret = fig.subplot_mosaic(layout)\n    assert len(ret) == 3\n    assert len(fig.axes) == 3\n    assert fig.axes[1].get_shared_x_axes().get_siblings(fig.axes[1]) == {fig.axes[2]}\n    assert fig.axes[1].get_shared_y_axes().get_siblings(fig.axes[1]) == set()\n\n    fig.clear()\n    layout = \"\"\"\n        AB;C.D\n    \"\"\"\n    ret = fig.subplot_mosaic(layout)\n    assert len(ret) == 4\n    assert len(fig.axes) == 4\n    assert fig.axes[0].get_shared_x_axes().get_siblings(fig.axes[0]) == set()\n    assert fig.axes[0].get_shared_y_axes().get_siblings(fig.axes[0]) == {fig.axes[1]}\n\n    fig.clear()\n    layout = \"\"\"\n        A;B;C\n        ....\n        D;E;F\n    \"\"\"\n    ret = fig.subplot_mosaic(layout)\n    assert len(ret) == 6\n    assert len(fig.axes) == 6\n    assert fig.axes[0].get_shared_x_axes().get_siblings(fig.axes[0]) == set()\n    assert fig.axes[0].get_shared_y_axes().get_siblings(fig.axes[0]) == set()\n\n    fig.clear()\n    layout = [\n        ['A', 'B', 'C'],\n        ['D', 'E', 'F'],\n        ['G', 'H', 'I']\n    ]\n    ret = fig.subplot_mosaic(layout)\n    assert len(ret) == 9\n    assert len(fig.axes) == 9\n    assert fig.axes[0].get_shared_x_axes().get_siblings(fig.axes[0]) == set()\n   "], "sample_118": ["def test_in_with_ordered_set(self):\n    # in should preserve the order of elements in the lookup value\n    query = Article.objects.filter(slug__in=OrderedSet(['a5', 'a6', 'a4', 'a2', 'a3', 'a7', 'a1'])).values('pk').query\n    self.assertIn(' IN (a5, a6, a4, a2, a3, a7, a1) ', str(query))\n", "def test_pattern_lookups_with_none_rhs(self):\n    Author.objects.create(name='John Smith')\n    tests = (\n        ('startswith', []),\n        ('istartswith', []),\n        ('contains', []),\n        ('icontains', []),\n        ('endswith', []),\n        ('iendswith', []),\n    )\n    for lookup, result in tests:\n        with self.subTest(lookup=lookup):\n            authors = Author.objects.filter(**{'name__%s' % lookup: None})\n            self.assertCountEqual(authors, result)\n", "def test_lookup_date_range_as_tuple(self):\n    # A date range lookup can be performed using a tuple\n    self.assertQuerysetEqual(\n        Article.objects.filter(pub_date__range=('2005-07-27', '2005-07-28')),\n        [\n            '<Article: Article 2>',\n            '<Article: Article 3>',\n            '<Article: Article 7>',\n            '<Article: Article 4>',\n        ]\n    )\n    self.assertQuerysetEqual(\n        Article.objects.filter(pub_date__range=('2005-08-01', '2005-08-01')),\n        [\n            '<Article: Article 6>',\n            '<Article: Article 5>',\n        ]\n    )\n\n    # An invalid date range lookup raises a ValueError\n    msg = \"Invalid range value: ('2005-08-01', '2005-07-01').\"\n    with self.assertRaisesMessage(ValueError, msg):\n        Article.objects.filter(pub_date__range=('2005-08-01', '2005-07-01'))\n", "def test_lookup_with_get_prep_lookup_value_is_iterable(self):\n    values = [self.a1.slug, self.a2.slug]\n    articles = Article.objects.filter(slug__in=values)\n    self.assertQuerysetEqual(articles, [self.a1, self.a2], ordered=False)\n\n    # Test with an empty list\n    articles = Article.objects.filter(slug__in=[])\n    self.assertQuerysetEqual(articles, [])\n\n    # Test with a single value\n    articles = Article.objects.filter(slug__in=[self.a1.slug])\n    self.assertQuerysetEqual(articles, [self.a1])\n\n    # Test with a single value that doesn't exist\n    articles = Article.objects.filter(slug__in=['non-existent-slug'])\n    self.assertQuerysetEqual(articles, [])\n\n    # Test with a list of values that don't exist\n    articles = Article.objects.filter(slug__in=['non-existent-slug1', 'non-existent-slug2'])\n    self.assertQuerysetEqual(articles, [])\n\n    # Test with a list containing a single value that doesn't exist\n    articles = Article.objects.filter(slug__in=[self.a1.slug, 'non-existent-slug'])\n    self.assertQuerysetEqual(articles, [self.a1])\n\n    # Test with a list containing multiple values that don't exist\n    articles = Article.objects.filter(slug__in=['non-existent-slug1', 'non-existent-slug2', self.a1.slug])\n    self.assertQuerysetEqual(articles, [self.a1])\n", "def test_exact_integer_field_float_rhs(self):\n    # An IntegerField lookup will round a float rhs value to the nearest integer.\n    self.assertQuerysetEqual(\n        Article.objects.filter(id__exact=3.7),\n        ['<Article: Article 4>']\n    )\n\n    # However, an IntegerField will not change the value if it's already an integer.\n    self.assertQuerysetEqual(\n        Article.objects.filter(id__exact=4.0),\n        ['<Article: Article 4>']\n    )\n"], "sample_595": ["def test_slice_replace_edge_cases(dtype):\n    da = lambda x: xr.DataArray(x).astype(dtype)\n    values = da([\"short\", \"a bit longer\", \"evenlongerthanthat\", \"\"])\n\n    expected = da([\"short\", \"a bit longer\", \"evenlongerthanthat\", \"\"])\n    result = values.str.slice_replace(None, None)\n    assert_equal(result, expected)\n\n    expected = da([\"short\", \"a bit longer\", \"evenlongerthanthat\", \"\"])\n    result = values.str.slice_replace(None, None, repl=\"\")\n    assert_equal(result, expected)\n\n    expected = da([\"\", \"\", \"\", \"\"])\n    result = values.str.slice_replace(0, None)\n    assert_equal(result, expected)\n\n    expected = da([\"\", \"\", \"\", \"\"])\n    result = values.str.slice_replace(0, None, repl=\"\")\n    assert_equal(result, expected)\n\n    expected = da([\"short\", \"a bit longer\", \"evenlongerthanthat\", \"\"])\n    result = values.str.slice_replace(1000, 1001)\n    assert_equal(result, expected)\n\n    expected = da([\"short\", \"a bit longer\", \"evenlongerthanthat\", \"\"])\n    result = values.str.slice_replace(1000, 1001, repl=\"z\")\n    assert_equal(result, expected)\n", "def test_slice_replace_errors(dtype):\n    da = xr.DataArray([\"short\", \"a bit longer\", \"evenlongerthanthat\", \"\"]).astype(dtype)\n    with pytest.raises(ValueError):\n        da.str.slice_replace(-10, 3, \"z\")\n\n    with pytest.raises(ValueError):\n        da.str.slice_replace(3, 10, \"z\")\n\n    with pytest.raises(ValueError):\n        da.str.slice_replace(3, 3, \"z\")\n\n    with pytest.raises(ValueError):\n        da.str.slice_replace(10, 3, \"z\")\n", "def test_slice_replace_negative_index(dtype):\n    da = lambda x: xr.DataArray(x).astype(dtype)\n    values = da([\"short\", \"a bit longer\", \"evenlongerthanthat\", \"\"])\n\n    expected = da([\"shrt\", \"a it longer\", \"evnlongerthanthat\", \"\"])\n    result = values.str.slice_replace(-3, -2)\n    assert_equal(result, expected)\n\n    expected = da([\"shzrt\", \"a zit longer\", \"evznlongerthanthat\", \"z\"])\n    result = values.str.slice_replace(-3, -2, \"z\")\n    assert_equal(result, expected)\n\n    expected = da([\"shzort\", \"a zbit longer\", \"evzenlongerthanthat\", \"z\"])\n    result = values.str.slice_replace(-3, -4, \"z\")\n    assert_equal(result, expected)\n\n    expected = da([\"shorz\", \"a bit longez\", \"evenlongerthanzat\", \"z\"])\n    result = values.str.slice_replace(-2, None, \"z\")\n    assert_equal(result, expected)\n\n    expected = da([\"zrt\", \"zer\", \"zat\", \"z\"])\n    result = values.str.slice_replace(None, -2, \"z\")\n    assert_equal(result, expected)\n\n    expected = da([\"shortz\", \"a bit znger\", \"evenlozerthanthat\", \"z\"])\n    result = values.str.slice_replace(-7, -5, \"z\")\n    assert_equal(result, expected)\n\n    expected = da([\"zrt\", \"a zit longer\", \"evenlongzerthanthat\", \"z\"])\n    result = values.str.slice_replace(-11, -3, \"z\")\n    assert_equal(result, expected)\n", "def test_encode_decode_numpy_error(dtype):\n    encodeBase = xr.DataArray([\"a\", \"b\", \"a\\x9d\"])\n\n    msg = (\n        \"Can't encode characters: 'charmap' codec can't encode character '\\\\x9d'\"\n    )\n    with pytest.raises(UnicodeEncodeError, match=msg):\n        encodeBase.astype(np.str_).str.encode(\"cp1252\")\n\n    decodeBase = xr.DataArray([b\"a\", b\"b\", b\"a\\x9d\"])\n\n    msg = (\n        \"Can't decode bytes: 'charmap' codec can't decode byte 0x9d in position\"\n    )\n    with pytest.raises(UnicodeDecodeError, match=msg):\n        decodeBase.astype(np.bytes_).str.decode(\"cp1252\")\n", "def test_slice_replace_negatives(dtype):\n    values = xr.DataArray([\"abcdefghij\", \"abcdefghij\", \"abcdefghij\", \"abcdefghij\"]).astype(dtype)\n\n    result = values.str.slice_replace(-5, -3)\n    expected = xr.DataArray([\"abcdef   j\", \"abcdef   j\", \"abcdef   j\", \"abcdef   j\"]).astype(dtype)\n    assert_equal(result, expected)\n\n    result = values.str.slice_replace(-10, -5)\n    expected = xr.DataArray([\"     fghij\", \"     fghij\", \"     fghij\", \"     fghij\"]).astype(dtype)\n    assert_equal(result, expected)\n\n    result = values.str.slice_replace(-5, None)\n    expected = xr.DataArray([\"abcdef\", \"abcdef\", \"abcdef\", \"abcdef\"]).astype(dtype)\n    assert_equal(result, expected)\n\n    result = values.str.slice_replace(None, -3)\n    expected = xr.DataArray([\"      hij\", \"      hij\", \"      hij\", \"      hij\"]).astype(dtype)\n    assert_equal(result, expected)\n\n    result = values.str.slice_replace(-5, -3, \"z\")\n    expected = xr.DataArray([\"abcdefzzj\", \"abcdefzzj\", \"abcdefzzj\", \"abcdefzzj\"]).astype(dtype)\n    assert_equal(result, expected)\n"], "sample_978": ["def test_n_splines_degree_0():\n    d = 0\n    knots = [0, 2, 4, 6, 8, 10]\n    splines = bspline_basis_set(d, knots, x)\n    for i in range(len(splines)):\n        assert splines[i] == Piecewise((1, Interval(knots[i], knots[i + 1]).contains(x)),\n                                       (0, True))\n\n", "def test_bspline_basis_negative_index():\n    d = 1\n    knots = range(5)\n    with raises(ValueError):\n        bspline_basis(d, knots, -1, x)\n", "def test_bspline_basis_edge_cases():\n    d = 3\n    knots = range(5)\n    n = len(knots) - d - 1\n    x = symbols('x')\n    with pytest.raises(ValueError):\n        bspline_basis(d, knots, n + 1, x)\n    with pytest.raises(ValueError):\n        bspline_basis(-1, knots, 0, x)\n    with pytest.raises(ValueError):\n        bspline_basis(d, [1, 2, 2, 3, 3, 3], 0, x)\n", "def test_bspline_basis_set_edge_cases():\n    d = 3\n    knots = [0, 1, 1, 2, 3, 3, 4]\n    splines = bspline_basis_set(d, knots, x)\n    b0 = Piecewise((-x**3/6 + x**2 - 2*x + 4/3, Interval(0, 1).contains(x)),\n                   (-x**3/2 + 3*x**2 - 8*x + 10/3, Interval(1, 2).contains(x)),\n                   (x**3/6 - 2*x**2 + 5*x - 28/3, Interval(2, 3).contains(x)),\n                   (x**3/6 - x**2, Interval(3, 4).contains(x)),\n                   (0, True))\n    assert splines[0] == b0\n    assert len(splines) == 3\n", "def test_bspline_basis():\n    d = 3\n    knots = range(10)\n    splines = bspline_basis_set(d, knots, x)\n    for b in splines:\n        assert b.free_symbols == {x}, b.free_symbols\n\n    assert all(isinstance(s, Piecewise) for s in splines)\n\n    d = 4\n    knots = [0, 1, 1, 2, 2, 3, 4, 5, 5, 6, 7]\n    splines = bspline_basis_set(d, knots, x)\n    for b in splines:\n        assert b.free_symbols == {x}, b.free_symbols\n\n    assert all(isinstance(s, Piecewise) for s in splines)\n"], "sample_247": ["def test_annotation_with_conditional_expression(self):\n    qs = Author.objects.annotate(\n        is_old=Case(\n            When(age__gt=40, then=Value(True)),\n            default=Value(False),\n            output_field=BooleanField()\n        )\n    )\n    self.assertTrue(qs.get(pk=self.a8.pk).is_old)\n    self.assertFalse(qs.get(pk=self.a1.pk).is_old)\n", "def test_cannot_annotate_with_order_by(self):\n    msg = \"Cannot use an aggregate in order_by() without also including it in annotate(): Sum('rating').\"\n    with self.assertRaisesMessage(FieldError, msg):\n        Book.objects.order_by(Sum('rating'))\n", "def test_annotation_with_empty_string(self):\n    Author.objects.create(name='', age=25)\n    authors = Author.objects.annotate(\n        name_lower=Lower('name'),\n    ).distinct('name_lower')\n    self.assertEqual(len(authors), 2)\n    self.assertEqual({p.name for p in authors}, {None, 'James Bennett'})\n", "def test_values_list_with_aliases(self):\n    qs = Book.objects.alias(is_book=Value(1), another_rating=F('rating')).values_list(\n        'is_book', 'another_rating', 'rating'\n    )\n    self.assertEqual(qs[0][0], 1)\n    self.assertEqual(qs[0][1], 4.5)\n    self.assertEqual(qs[0][2], 4.5)\n", "def test_annotation_filtering_with_multi_table_inheritance(self):\n    \"\"\"Filtering an annotation over a multi-table inheritance setup works.\"\"\"\n    qs = DepartmentStore.objects.annotate(\n        books_count=Count('books')\n    ).filter(books_count__gt=2)\n    self.assertEqual(qs.count(), 1)\n    self.assertEqual(qs.first().name, 'Amazon.com')\n"], "sample_210": ["    def test_extra_context(self):\n        mixin = ContextMixin()\n        mixin.extra_context = {'foo': 'bar'}\n        context = mixin.get_context_data()\n        self.assertEqual(context['foo'], 'bar')\n", "    def test_get_context_data(self):\n        view = ContextMixin()\n        context = view.get_context_data()\n        self.assertIn('view', context)\n        self.assertEqual(context['view'], view)\n", "    def test_context_data(self):\n        mixin = ContextMixin()\n        mixin.extra_context = {'foo': 'bar'}\n        context = mixin.get_context_data(test='value')\n        self.assertEqual(context, {'foo': 'bar', 'test': 'value', 'view': mixin})\n", "    def test_extra_context(self):\n        view = ContextMixin()\n        view.extra_context = {'foo': 'bar'}\n        context = view.get_context_data()\n        self.assertEqual(context['foo'], 'bar')\n", "    def test_redirect_encoding(self):\n        \"Redirection URLs are properly URL-encoded\"\n        view = RedirectView.as_view(url='/bar/', query_string=True)\n        response = view(self.rf.get('/foo/?pork=spam&beans=eggs'))\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, '/bar/?pork=spam&beans=eggs')\n"], "sample_244": ["    def test_can_order_with_can_delete(self):\n        \"\"\"\n        FormSets with ordering and deletion.\n        \"\"\"\n        ChoiceFormSet = formset_factory(Choice, can_order=True, can_delete=True)\n        initial = [\n            {'choice': 'Calexico', 'votes': 100},\n            {'choice': 'Fergie', 'votes': 900},\n            {'choice': 'The Decemberists', 'votes': 500},\n        ]\n        formset = ChoiceFormSet(initial=initial, auto_id=False, prefix='choices')\n        self.assertHTMLEqual(\n            '\\n'.join(form.as_ul() for form in formset.forms),\n            \"\"\"<li>Choice: <input type=\"text\" name=\"choices-0-choice\" value=\"Calexico\"></li>", "    def test_formset_with_deletion_can_delete_extra_false(self):\n        \"\"\"\n        can_delete_extra=False prevents deletion checkboxes from appearing\n        on extra forms.\n        \"\"\"\n        ChoiceFormSet = formset_factory(Choice, can_delete=True, can_delete_extra=False, extra=2)\n        initial = [{'choice': 'Calexico', 'votes': 100}]\n        formset = ChoiceFormSet(initial=initial, auto_id=False, prefix='choices')\n        self.assertHTMLEqual(\n            '\\n'.join(form.as_ul() for form in formset.forms),\n            \"\"\"<li>Choice: <input type=\"text\" name=\"choices-0-choice\" value=\"Calexico\"></li>", "def test_management_form_validation(self):\n    \"\"\"Test validation on management form.\"\"\"\n    data = {\n        'choices-TOTAL_FORMS': 'two',  # the number of forms rendered\n        'choices-INITIAL_FORMS': 'one',  # the number of forms with initial data\n        'choices-MIN_NUM_FORMS': '0',  # min number of forms\n        'choices-MAX_NUM_FORMS': '0',  # max number of forms\n        'choices-0-choice': 'Zero',\n        'choices-0-votes': '0',\n    }\n    ChoiceFormSet = formset_factory(Choice)\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(\n        formset.non_form_errors(),\n        [\n            'ManagementForm data is missing or has been tampered with. '\n            'Missing fields: choices-TOTAL_FORMS, choices-INITIAL_FORMS. '\n            'You may need to file a bug report if the issue persists.',\n        ],\n    )\n", "def test_formset_with_absolute_max_less_than_initial(self):\n    \"\"\"Test that absolute_max less than initial forms is an error.\"\"\"\n    AbsoluteMaxFavoriteDrinkFormSet = formset_factory(\n        FavoriteDrinkForm,\n        absolute_max=1,\n    )\n    initial = [{'name': 'Fernet and Coke'}, {'name': 'Bloody Mary'}]\n    formset = AbsoluteMaxFavoriteDrinkFormSet(initial=initial)\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(\n        formset.non_form_errors(),\n        ['Please submit at most 1 form.'],\n    )\n", "def test_can_delete_extra_formset_forms_with_can_order(self):\n    ChoiceFormFormset = formset_factory(form=Choice, can_delete=True, can_order=True, extra=2)\n    formset = ChoiceFormFormset()\n    self.assertEqual(len(formset), 2)\n    self.assertIn('DELETE', formset.forms[0].fields)\n    self.assertIn('DELETE', formset.forms[1].fields)\n\n    formset = ChoiceFormFormset(data={\n        'form-0-choice': 'Zero',\n        'form-0-votes': '0',\n        'form-0-DELETE': 'on',\n        'form-0-ORDER': '2',\n        'form-1-choice': 'One',\n        'form-1-votes': '1',\n        'form-1-DELETE': '',\n        'form-1-ORDER': '1',\n        'form-TOTAL_FORMS': '2',\n        'form-INITIAL_FORMS': '0',\n    })\n    self.assertEqual(formset.total_error_count(), 0)\n    self.assertEqual(formset.ordered_forms, [formset.forms[1]])\n    self.assertEqual(formset.deleted_forms, [formset.forms[0]])\n\n    ChoiceFormFormset = formset_factory(\n        form=Choice,\n        can_delete=True,\n        can_order=True,\n        can_delete_extra=False,\n        extra=2,\n    )\n    formset = ChoiceFormFormset()\n    self.assertEqual(len(formset), 2)\n    self.assertNotIn('DELETE', formset.forms[0].fields)\n    self.assertNotIn('DELETE', formset.forms[1].fields)\n\n    formset = ChoiceFormFormset(initial=[{'choice': 'Zero', 'votes': '1'}])\n    self.assertEqual(len(formset), 3)\n    self.assertIn('DELETE', formset.forms[0].fields)\n    self.assertNotIn('DELETE', formset.forms[1].fields)\n    self.assertNotIn('DELETE', formset.forms[2].fields)\n\n    formset = ChoiceFormFormset(data={\n        'form-0-choice': 'Zero',\n        'form-0-votes': '0',\n        'form-0-DELETE': 'on',\n        'form-0-ORDER': '2',\n        'form-1-choice': 'One',\n        'form-1-votes': '1',\n        'form-1-DELETE': '',\n        'form-1-ORDER': '1',\n        'form-"], "sample_897": ["def test_partial_dependence_display_with_non_constant_sample_weight(pyplot, clf_diabetes, diabetes):\n    \"\"\"Check that the utilization of a non-constant sample weight maintains the\n    standard behavior.\n    \"\"\"\n    sample_weight = np.arange(1, len(diabetes.target) + 1)\n    disp_sw = PartialDependenceDisplay.from_estimator(\n        clf_diabetes,\n        diabetes.data,\n        [0, 1],\n        sample_weight=sample_weight,\n        kind=\"average\",\n        method=\"brute\",\n    )\n\n    disp_avg = PartialDependenceDisplay.from_estimator(\n        clf_diabetes,\n        diabetes.data,\n        [0, 1],\n        kind=\"average\",\n        method=\"brute\",\n    )\n\n    assert not np.array_equal(\n        disp_sw.pd_results[0][\"average\"], disp_avg.pd_results[0][\"average\"]\n    )\n", "def test_partial_dependence_displaysubsampling_with_constant_sample_weight(\n    pyplot,\n    clf_diabetes,\n    diabetes,", "def test_plot_partial_dependence_categorical_interaction(\n    pyplot, clf_diabetes, diabetes, is_categorical, kind, features, err_msg", "def test_partial_dependence_display_centering(pyplot, kind, n_plots, expected_values):\n    \"\"\"Check that we properly center ICE and PD when passing kind as a string.\"\"\"\n    data = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\n    pd_results = [{\"grid_values\": [data], \"average\": np.array([0.2, 0.5, 0.8])}]\n    disp = PartialDependenceDisplay(\n        pd_results=pd_results,\n        features=[0],\n        feature_names=[\"feature\"],\n        target_idx=0,\n        deciles={},\n        kind=kind,\n        subsample=None,\n        random_state=None,\n    )\n\n    disp.plot(centered=True)\n\n    assert disp.lines_.shape == (1, n_plots)\n    for i, line in enumerate(disp.lines_.ravel()):\n        if line is not None:\n            assert_allclose(line.get_data()[1], expected_values[i])\n", "def test_partial_dependence_display_is_categorical(pyplot, clf_diabetes, diabetes):\n    \"\"\"Check that we properly handle categorical features.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    X = pd.DataFrame(\n        {\n            \"col_A\": [1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3],\n            \"col_B\": [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4],\n        }\n    )\n    y = np.array([1.2, 0.5, 0.45, 0.5, 0.45, 1.2, 1.2, 0.5, 0.45, 0.5, 0.45, 1.2]).T\n\n    categorical_features = [\"col_B\"]\n    preprocessor = make_column_transformer((OneHotEncoder(), categorical_features))\n    model = make_pipeline(preprocessor, LinearRegression())\n    model.fit(X, y)\n\n    disp = PartialDependenceDisplay.from_estimator(\n        model,\n        X,\n        features=[\"col_A\", (\"col_A\", \"col_B\")],\n        categorical_features=categorical_features,\n        n_cols=2,\n        kind=\"both\",\n        subsample=None,\n    )\n\n    # check that there are no bars for the continuous feature\n    assert disp.bars_.shape == (1, 2)\n    assert disp.bars_[0, 0] is None\n    assert disp.bars_[0, 1] is None\n\n    # check that there are no heatmap for the 2-way feature interaction with a\n    # continuous feature\n    assert disp.heatmaps_.shape == (1, 2)\n    assert disp.heatmaps_[0, 0] is None\n    assert disp.heatmaps_[0, 1] is None\n"], "sample_763": ["def test_check_X_y_valid_input():\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([1, 2])\n    X_checked, y_checked = check_X_y(X, y)\n    assert_array_equal(X, X_checked)\n    assert_array_equal(y, y_checked)\n\n    X = sp.csr_matrix(X)\n    y = np.array([1, 2])\n    X_checked, y_checked = check_X_y(X, y)\n    assert_array_equal(X.toarray(), X_checked)\n    assert_array_equal(y, y_checked)\n\n    X = np.array([[1, 2], [3, 4]])\n    y = sp.csr_matrix(y[:, np.newaxis])\n    X_checked, y_checked = check_X_y(X, y)\n    assert_array_equal(X, X_checked)\n    assert_array_equal(y.toarray().ravel(), y_checked)\n\n    X = sp.csr_matrix(np.array([[1, 2], [3, 4]]))\n    y = sp.csr_matrix(np.array([1, 2])[:, np.newaxis])\n    X_checked, y_checked = check_X_y(X, y)\n    assert_array_equal(X.toarray(), X_checked)\n    assert_array_equal(y.toarray().ravel(), y_checked)\n", "def test_check_X_y_force_all_finite_valid():\n    # Test that check_X_y correctly handles allow-nan\n    X = np.array([[1, 2], [3, np.nan]])\n    y = np.array([1, 2])\n    X_checked, y_checked = check_X_y(X, y, force_all_finite='allow-nan')\n    assert_allclose_dense_sparse(X_checked, X)\n    assert_array_equal(y_checked, y)\n\n    # Test that check_X_y correctly handles allow-nan with sparse\n    X = sp.csr_matrix(X)\n    X_checked, y_checked = check_X_y(X, y, force_all_finite='allow-nan')\n    assert_allclose_dense_sparse(X_checked, X)\n    assert_array_equal(y_checked, y)\n", "def test_check_X_y_ensure_min_samples_and_features():\n    # Test the case when ensure_min_samples and ensure_min_features are used\n    # together.\n    X = np.ones((1, 1))\n    y = np.ones(1)\n    check_X_y(X, y, ensure_min_samples=1, ensure_min_features=1)\n\n    X = np.ones((2, 1))\n    y = np.ones(2)\n    assert_raise_message(ValueError, \"1 feature(s) while a minimum of 2 is \"\n                         \"required\", check_X_y, X, y, ensure_min_samples=2,\n                         ensure_min_features=2)\n\n    X = np.ones((1, 2))\n    y = np.ones(1)\n    assert_raise_message(ValueError, \"1 sample(s) while a minimum of 2 is \"\n                         \"required\", check_X_y, X, y, ensure_min_samples=2,\n                         ensure_min_features=2)\n", "def test_check_X_y_for_valid_force_all_finite_values():\n    # Test that check_X_y correctly handles valid values for force_all_finite\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n    valid_force_all_finite_values = [True, False, 'allow-nan']\n    for value in valid_force_all_finite_values:\n        try:\n            check_X_y(X, y, force_all_finite=value)\n        except ValueError:\n            assert False, f\"check_X_y failed with force_all_finite={value}\"\n", "def test_check_X_y_with_sample_weights():\n    # check_X_y should allow 2D y with sample weights\n    X = np.ones((10, 10))\n    y = np.ones((10, 1))\n    sample_weight = np.ones((10,))\n    X, y, _ = check_X_y(X, y, multi_output=True, sample_weight=sample_weight)\n\n    # check_X_y should raise error if sample weights are 2D and multi_output=False\n    with pytest.raises(ValueError):\n        check_X_y(X, y, sample_weight=sample_weight)\n"], "sample_864": ["def test_mean_shift_with_seeds():\n    # Test MeanShift algorithm with custom seeds\n    seeds = np.array([[10, 10], [10, -10], [-10, -10], [-10, 10]])\n    ms = MeanShift(bandwidth=1.2, seeds=seeds)\n    labels = ms.fit(X).labels_\n    labels_unique = np.unique(labels)\n    n_clusters_ = len(labels_unique)\n    assert n_clusters_ == 2\n    assert labels_unique[0] == 0\n\n    cluster_centers, labels_mean_shift = mean_shift(X, bandwidth=1.2, seeds=seeds)\n    labels_mean_shift_unique = np.unique(labels_mean_shift)\n    n_clusters_mean_shift = len(labels_mean_shift_unique)\n    assert n_clusters_mean_shift == 2\n    assert labels_mean_shift_unique[0] == 0\n", "def test_mean_shift_seeds():\n    # Test MeanShift with specific seeds\n    ms = MeanShift(seeds=np.array([[0, 0], [5, 5]]))\n    ms.fit(X)\n    assert_array_equal(ms.cluster_centers_, [[0, 0], [5, 5]])\n    assert_array_equal(ms.labels_, [0, 0, 0, 1, 1, 1])\n", "def test_mean_shift_single_seed():\n    # Test _mean_shift_single_seed function\n    bandwidth = 1.0\n    max_iter = 300\n    X = np.array([[1, 1], [2, 1], [1, 0],\n                  [4, 7], [3, 5], [3, 6]])\n    seeds = np.array([1, 1])\n    nbrs = NearestNeighbors(radius=bandwidth, n_jobs=1).fit(X)\n\n    result = _mean_shift_single_seed(seeds, X, nbrs, max_iter)\n    assert_array_almost_equal(result[0], [1.33333333, 0.66666667])\n    assert result[1] == 3\n    assert result[2] <= max_iter\n\n    # Test when no points are within the bandwidth\n    seeds = np.array([10, 10])\n    result = _mean_shift_single_seed(seeds, X, nbrs, max_iter)\n    assert result[1] == 0\n", "def test_mean_shift_single_seed():\n    # Test the _mean_shift_single_seed function\n    X = np.array([[1, 1], [2, 1], [1, 0],\n                  [4, 7], [3, 5], [3, 6]])\n    bandwidth = 2\n    nbrs = NearestNeighbors(radius=bandwidth, n_jobs=1).fit(X)\n    seed = np.array([1, 1])\n    result = _mean_shift_single_seed(seed, X, nbrs, max_iter=100)\n    assert np.allclose(result[0], [1.66666667, 0.66666667], atol=1e-4)\n    assert result[1] == 3\n    assert result[2] < 100\n", "def test_mean_shift_single_seed_edge_cases():\n    # Test _mean_shift_single_seed with edge cases\n    X = np.array([[1, 1], [2, 1], [1, 0],\n                  [4, 7], [3, 5], [3, 6]])\n    bandwidth = 1.0\n    nbrs = NearestNeighbors(radius=bandwidth, n_jobs=1).fit(X)\n    max_iter = 300\n\n    # Test when points_within is empty\n    result = _mean_shift_single_seed([100, 100], X, nbrs, max_iter)\n    assert result == ((100, 100), 0, 0)\n\n    # Test when len(points_within) == 1\n    result = _mean_shift_single_seed([1, 1], X, nbrs, max_iter)\n    assert np.allclose(result[0], (1, 1))\n\n    # Test when max_iter is small\n    result = _mean_shift_single_seed([1, 1], X, nbrs, 1)\n    assert np.allclose(result[0], (1, 1))\n\n    # Test when my_mean is far away from X\n    result = _mean_shift_single_seed([100, 100], X, nbrs, max_iter)\n    assert result == ((100, 100), 0, 0)\n"], "sample_638": ["def test_natively_supported_image_format(mock_writer, capsys):\n    \"\"\"Test that natively supported image formats do not trigger a Graphviz check.\"\"\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        # we have to catch the SystemExit so the test execution does not stop\n        main.Run([\"-o\", \"dot\", TEST_DATA_DIR])\n    # Check that no info message is shown to the user\n    stdout = capsys.readouterr().out\n    assert \"Format dot is not supported natively.\" not in stdout\n    # Check that pyreverse actually made the call to create the diagram and we exit cleanly\n    mock_writer.DiagramWriter().write.assert_called_once()\n    assert wrapped_sysexit.value.code == 0\n", "def test_graphviz_unavailable(mock_writer, capsys):\n    \"\"\"Test that an error is raised if Graphviz is not available.\"\"\"\n    with mock.patch(\"pylint.pyreverse.utils.shutil.which\", return_value=None):\n        with pytest.raises(SystemExit) as wrapped_sysexit:\n            # we have to catch the SystemExit so the test execution does not stop\n            main.Run([\"-o\", \"png\", TEST_DATA_DIR])\n        # Check that the right info message is shown to the user\n        assert \"Graphviz's dot not found, please install graphviz and ensure 'dot' is on your PATH.\" in capsys.readouterr().out\n        # Check that we exited with the expected error code\n        assert wrapped_sysexit.value.code == 32\n", "def test_directly_supported_formats(mock_writer, capsys):\n    \"\"\"Test that no info message is shown when using a directly supported format.\"\"\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        # we have to catch the SystemExit so the test execution does not stop\n        main.Run([\"-o\", \"dot\", TEST_DATA_DIR])\n    # Check that no info message is shown to the user\n    assert \"Format dot is not supported natively\" not in capsys.readouterr().out\n    # Check that pyreverse actually made the call to create the diagram and we exit cleanly\n    mock_writer.DiagramWriter().write.assert_called_once()\n    assert wrapped_sysexit.value.code == 0\n\n", "def test_output_directory(mock_writer, capsys):\n    \"\"\"Test that the output directory is set correctly.\"\"\"\n    main.Run([\"-d\", \"/path/to/output\", TEST_DATA_DIR])\n    assert main.config.output_directory == \"/path/to/output\"\n    mock_writer.DiagramWriter.assert_called_once_with(mock.ANY)\n    mock_writer.DiagramWriter.return_value.write.assert_called_once()\n\n", "def test_invalid_output_format(mock_writer, capsys):\n    \"\"\"Test that an error is raised when an invalid output format is specified.\"\"\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        # we have to catch the SystemExit so the test execution does not stop\n        main.Run([\"-o\", \"invalid_format\", TEST_DATA_DIR])\n    # Check that the right error message is shown to the user\n    assert (\n        \"Invalid output format 'invalid_format'. Supported formats are: dot, vcg, puml, plantuml, mmd, html.\"\n        in capsys.readouterr().out\n    )\n    # Check that pyreverse actually did not make the call to create the diagram\n    mock_writer.DiagramWriter().write.assert_not_called()\n    # Check that we exited with the expected error code\n    assert wrapped_sysexit.value.code == 32\n\n"], "sample_40": ["def test_pixel_scale_quantity():\n    pix = 75*u.pix\n    asec = 30*u.arcsec\n\n    pixscale = 0.4*u.arcsec/u.pix\n    pixscale2 = 2.5*u.pix/u.arcsec\n\n    assert_quantity_allclose(pix.to(u.arcsec, u.pixel_scale(pixscale.value)), asec)\n    assert_quantity_allclose(pix.to(u.arcmin, u.pixel_scale(pixscale.value)), asec)\n\n    assert_quantity_allclose(pix.to(u.arcsec, u.pixel_scale(pixscale2.value)), asec)\n    assert_quantity_allclose(pix.to(u.arcmin, u.pixel_scale(pixscale2.value)), asec)\n\n    assert_quantity_allclose(asec.to(u.pix, u.pixel_scale(pixscale.value)), pix)\n    assert_quantity_allclose(asec.to(u.pix, u.pixel_scale(pixscale2.value)), pix)\n", "def test_temperature_energy_round_trip():\n    for T in [u.K, u.deg_C, u.deg_F]:\n        for E in [u.keV, u.eV]:\n            x = 1000 * T\n            y = x.to(E, u.temperature_energy())\n            assert_quantity_allclose(y.to(T, u.temperature_energy()), x)\n", "def test_spectral_density_uncommon_units():\n    \"\"\"Test spectral density with uncommon units.\"\"\"\n    wave = u.Quantity([4956.8, 4959.55, 4962.3], u.AA)\n    flux_photlam = [9.7654e-3, 1.003896e-2, 9.78473e-3]\n    flux_photnu = [8.00335589e-14, 8.23668949e-14, 8.03700310e-14]\n    flux_flam = [3.9135e-14, 4.0209e-14, 3.9169e-14]\n    flux_fnu = [3.20735792e-25, 3.29903646e-25, 3.21727226e-25]\n\n    # PHOTLAM <--> FNU\n    assert_allclose(\n        u.photon/u.cm**2/u.s/u.AA.to(\n            u.erg/u.cm**2/u.s/u.Hz, flux_photlam, u.spectral_density(wave)),\n        flux_fnu, rtol=1e-6)\n    assert_allclose(\n        u.erg/u.cm**2/u.s/u.Hz.to(\n            u.photon/u.cm**2/u.s/u.AA, flux_fnu, u.spectral_density(wave)),\n        flux_photlam, rtol=1e-6)\n\n    # PHOTNU <--> FLAM\n    assert_allclose(\n        u.photon/u.cm**2/u.s/u.Hz.to(\n            u.erg/u.cm**2/u.s/u.AA, flux_photnu, u.spectral_density(wave)),\n        flux_flam, rtol=1e-6)\n    assert_allclose(\n        u.erg/u.cm**2/u.s/u.AA.to(\n            u.photon/u.cm**2/u.s/u.Hz, flux_flam, u.spectral_density(wave)),\n        flux_photnu, rtol=1e-6)\n", "def test_spectral_density_exceptions():\n    # Invalid wavelength\n    with pytest.raises(ValueError):\n        u.spectral_density(\"invalid\")\n\n    # Wavelength is not a Quantity\n    with pytest.raises(TypeError):\n        u.spectral_density(3500)\n\n    # Wavelength's unit is not a spectral unit\n    with pytest.raises(ValueError):\n        u.spectral_density(3500*u.kg)\n", "def test_pixel_scale_roundtrip():\n    \"\"\"Check that pixel scale conversions are consistent and round-trip.\"\"\"\n    pix = 100*u.pix\n    asec = 30*u.arcsec\n    pixscale = 0.3*u.arcsec/u.pix\n\n    assert_quantity_allclose(pix.to(u.arcsec, u.pixel_scale(pixscale)), asec)\n    assert_quantity_allclose(asec.to(u.pix, u.pixel_scale(pixscale)), pix)\n\n    pixscale = 3.333*u.pix/u.arcsec\n    assert_quantity_allclose(pix.to(u.arcsec, u.pixel_scale(pixscale)), asec)\n    assert_quantity_allclose(asec.to(u.pix, u.pixel_scale(pixscale)), pix)\n\n    # Test that round-tripping multiple times is consistent\n    for _ in range(5):\n        pix = pix.to(u.arcsec, u.pixel_scale(pixscale)).to(u.pix, u.pixel_scale(pixscale))\n    assert_quantity_allclose(pix, pix)\n"], "sample_1128": ["def test_point_partial_velocity_multiple_generalized_speeds():\n    N = ReferenceFrame('N')\n    A = ReferenceFrame('A')\n\n    p = Point('p')\n\n    u1, u2, u3, u4 = dynamicsymbols('u1, u2, u3, u4')\n\n    p.set_vel(N, u1 * A.x + u2 * N.y + u3 * A.z + u4 * N.x)\n\n    assert p.partial_velocity(N, u1, u2, u3, u4) == (A.x, N.y, A.z, N.x)\n", "def test_partial_velocity_for_point_with_multiple_velocities():\n    q1, q2, u1, u2 = dynamicsymbols('q1 q2 u1 u2')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    P = Point('P')\n    P.set_vel(N, u1 * N.x + u2 * N.y)\n    P.set_vel(B, u1 * B.x + u2 * B.y)\n    raises(ValueError, lambda: P.partial_velocity(B, u1))\n    assert P.partial_velocity(N, u1) == N.x\n    assert P.partial_velocity(N, u2) == N.y\n    assert P.partial_velocity(N, u1, u2) == (N.x, N.y)\n", "def test_auto_point_vel_multiple_paths_with_inconsistent_relative_positions():\n    t = dynamicsymbols._t\n    q1, q2 = dynamicsymbols('q1 q2')\n    B = ReferenceFrame('B')\n    P = Point('P')\n    P.set_vel(B, q1 * B.x)\n    P1 = Point('P1')\n    P1.set_pos(P, q2 * B.y)\n    P2 = Point('P2')\n    P2.set_pos(P1, q1 * B.z)\n    P3 = Point('P3')\n    P3.set_pos(P, q1 * B.y + q2 * B.z)\n    P3.set_pos(P2, -q1 * B.z)\n    raises(ValueError, lambda: P3.vel(B)) # P2 and P3 have inconsistent relative positions\n", "def test_partial_velocity_multiple_velocities():\n    t = dynamicsymbols._t\n    q, q1, q2, u1, u2 = dynamicsymbols('q q1 q2 u1 u2')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    P = Point('P')\n    P.set_vel(N, u1 * N.x + u2 * B.y)\n    P.set_vel(B, u2 * B.x + u1 * B.z)\n    assert P.partial_velocity(N, u1) == N.x\n    assert P.partial_velocity(B, u1) == B.z\n    assert P.partial_velocity(N, u2) == B.y\n    assert P.partial_velocity(B, u2) == B.x\n    raises(ValueError, lambda: P.partial_velocity(N, q))  # q is not a generalized speed for P\n", "def test_point_a1pt_theory_self_velocity():\n    q = dynamicsymbols('q')\n    qd = dynamicsymbols('q', 1)\n    N = ReferenceFrame('N')\n    B = N.orientnew('B', 'Axis', [q, N.z])\n    O = Point('O')\n    P = O.locatenew('P', 10 * B.x)\n    O.set_vel(N, 0)\n    P.set_vel(B, qd * B.y)\n    P.a1pt_theory(O, N, B)\n    assert P.acc(N) == 10 * qd**2 * B.x + qdd * B.y + (qd * B.y).diff(N) + (qd**2) * B.y\n"], "sample_599": ["def test_UnsignedIntegerCoder_roundtrip():\n    original = xr.Variable((\"x\",), np.array([0, 255, 256], dtype=np.uint8), encoding={\"_Unsigned\": \"true\"})\n    coder = variables.UnsignedIntegerCoder()\n    roundtripped = coder.decode(coder.encode(original))\n    assert_identical(original, roundtripped)\n\n", "def test_UnsignedIntegerCoder_encode():\n    data = np.arange(10, dtype=np.int16)\n    original = xr.Variable((\"x\",), data, encoding={\"_Unsigned\": \"true\"})\n    coder = variables.UnsignedIntegerCoder()\n    encoded = coder.encode(original)\n    expected = xr.Variable((\"x\",), data, encoding={})\n    assert_identical(encoded, expected)\n\n", "def test_UnsignedIntegerCoder_encode():\n    original = xr.Variable((\"x\",), [1, 2, 3], encoding={\"_Unsigned\": \"true\"})\n    coder = variables.UnsignedIntegerCoder()\n    encoded = coder.encode(original)\n    assert encoded.dtype.kind == \"i\"\n    roundtripped = coder.decode(encoded)\n    assert_identical(original, roundtripped)\n\n", "def test_UnsignedIntegerCoder_roundtrip():\n    original = xr.Variable((\"x\",), [0, 255, 256], {\"_Unsigned\": \"true\"})\n    coder = variables.UnsignedIntegerCoder()\n    roundtripped = coder.decode(coder.encode(original))\n    assert_identical(original, roundtripped)\n    assert roundtripped.dtype == np.uint8\n\n", "def test_UnsignedIntegerCoder_encode():\n    original = xr.Variable((\"x\",), np.array([0, 255, 256], dtype=np.uint8))\n    expected = xr.Variable((\"x\",), np.array([-128, -1, 0], dtype=np.int8))\n    coder = variables.UnsignedIntegerCoder()\n    encoded = coder.encode(original, name=\"_Unsigned\")\n    encoded.attrs[\"_Unsigned\"] = \"true\"\n    assert_identical(encoded, expected)\n\n"], "sample_788": ["def test_inverse_transform_1d(strategy):\n    X = np.array([0, 1, 2, 3])[:, None]\n    kbd = KBinsDiscretizer(n_bins=4, strategy=strategy, encode='ordinal')\n    kbd.fit(X)\n    Xt = kbd.transform(X)\n    Xinv = kbd.inverse_transform(Xt)\n    assert_array_almost_equal(X, Xinv)\n", "def test_inverse_transform_constant_feature(strategy):\n    X = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1]])\n    kbd = KBinsDiscretizer(n_bins=3, strategy=strategy, encode='ordinal')\n    Xt = kbd.fit_transform(X)\n    Xinv = kbd.inverse_transform(Xt)\n    assert_array_almost_equal(Xinv, np.full((4, 3), 1))\n", "def test_inverse_transform_sparse():\n    X = [[-2, 1.5, -4, -1],\n         [-1, 2.5, -3, -0.5],\n         [0, 3.5, -2, 0.5],\n         [1, 4.5, -1, 2]]\n    est = KBinsDiscretizer(n_bins=3, encode='onehot', strategy='uniform')\n    Xt = est.fit_transform(X)\n    Xinv = est.inverse_transform(Xt)\n    Xinv_dense = est.inverse_transform(Xt.toarray())\n    assert_array_almost_equal(Xinv, Xinv_dense)\n", "def test_inverse_transform_zero_bins(encode):\n    X = np.array([[1, 1], [1, 1]])\n    est = KBinsDiscretizer(n_bins=1, strategy='uniform', encode=encode)\n    Xt = est.fit_transform(X)\n    Xinv = est.inverse_transform(Xt)\n    assert_array_almost_equal(X, Xinv)\n", "def test_edge_cases():\n    # Test with empty array\n    X_empty = np.array([]).reshape(-1, 0)\n    est = KBinsDiscretizer(n_bins=3)\n    assert_raises(ValueError, est.fit, X_empty)\n\n    # Test with 1 sample\n    X_one_sample = np.array([[1, 2, 3]])\n    est = KBinsDiscretizer(n_bins=3)\n    est.fit(X_one_sample)\n    assert_array_equal(est.transform(X_one_sample), [[0, 0, 0]])\n\n    # Test with 1 feature\n    X_one_feature = np.array([[1], [2], [3]])\n    est = KBinsDiscretizer(n_bins=3)\n    est.fit(X_one_feature)\n    assert_array_equal(est.transform(X_one_feature), [[0], [1], [2]])\n"], "sample_1000": ["def test_octave_codeprinter_user_functions():\n    f = Function('f')\n    custom_functions = {\n        \"f\": \"existing_octave_fcn\"\n    }\n    assert octave_code(f(x), user_functions=custom_functions) == \"existing_octave_fcn(x)\"\n    custom_functions = {\n        \"f\": [(lambda x: x.is_Matrix, \"my_mat_fcn\"),\n              (lambda x: not x.is_Matrix, \"my_fcn\")]\n    }\n    assert octave_code(f(x), user_functions=custom_functions) == \"my_fcn(x)\"\n    mat = Matrix([[1, x]])\n    assert octave_code(f(mat), user_functions=custom_functions) == \"my_mat_fcn([1 x])\"\n", "def test_octave_custom_functions():\n    # test custom printing of functions\n    f = Function('f')\n    g = Function('g')\n    custom_functions = {\n        \"f\": \"existing_octave_fcn\",\n        \"g\": [(lambda x: x.is_Matrix, \"my_mat_fcn\"),\n              (lambda x: not x.is_Matrix, \"my_fcn\")]\n    }\n    assert mcode(f(x)) == \"existing_octave_fcn(x)\"\n    assert mcode(g(x)) == \"my_fcn(x)\"\n    M = Matrix([[1, x]])\n    assert mcode(f(M)) == \"existing_octave_fcn([1 x])\"\n    assert mcode(g(M)) == \"my_mat_fcn([1 x])\"\n", "def test_octave_custom_function():\n    f = Function('f')\n    custom_functions = {\n        \"f\": [(lambda x: x.is_Matrix, \"my_mat_fcn\"),\n              (lambda x: not x.is_Matrix, \"my_fcn\")]\n    }\n    A = Matrix([[1, x], [y, x*y]])\n    expr = f(x) + f(A)\n    expected = \"my_fcn(x) + my_mat_fcn([1 x; y x.*y])\"\n    assert mcode(expr, user_functions=custom_functions) == expected\n", "def test_octave_indexed():\n    from sympy import IndexedBase, Idx, symbols\n    n = symbols('n', integer=True, positive=True)\n    x = IndexedBase('x', shape=(n,))\n    i = Idx('i', n)\n    A = MatrixSymbol('A', n, n)\n    expr = x[i]**2 + A[i, i]\n    assert octave_code(expr) == \"x(i).^2 + A(i, i)\"\n    assert octave_code(expr, assign_to='y') == \"y = x(i).^2 + A(i, i);\"\n    # check that expr with multiple symbols are handled\n    y = IndexedBase('y', shape=(n,))\n    expr = x[i]**2 + y[i]**2\n    assert octave_code(expr) == \"x(i).^2 + y(i).^2\"\n", "def test_octave_indexed():\n    x, y, z = symbols('x,y,z')\n    from sympy.tensor import IndexedBase, Idx\n    A = IndexedBase('A')\n    i, j = Idx('i'), Idx('j')\n    assert mcode(A[i, j]) == \"A(i, j)\"\n    assert mcode(A[i, j] + x) == \"A(i, j) + x\"\n    assert mcode(A[i, j] + A[i, j]) == \"2*A(i, j)\"\n    assert mcode(3*A[i, j]) == \"3*A(i, j)\"\n    assert mcode(A[i, j]*x) == \"x.*A(i, j)\"\n    assert mcode(A[i, j]*y*z) == \"x.*y.*A(i, j)\"\n    assert mcode(A[i, j]/x) == \"A(i, j)./x\"\n    assert mcode(A[i, j]/(x*y)) == \"A(i, j)./(x.*y)\"\n    assert mcode(3*A[i, j]/(x*y)) == \"3*A(i, j)./(x.*y)\"\n    assert mcode(x + 3*A[i, j]/(y*z)) == \"x + 3*A(i, j)./(y.*z)\"\n    assert mcode(A[i, j]**2) == \"A(i, j).^2\"\n    assert mcode(A[i, j]**(x**2)) == \"A(i, j).^(x.^2)\"\n    assert mcode(sin(A[i, j])) == \"sin(A(i, j))\"\n    assert mcode(uppergamma(x, A[i, j])) == \"gammainc(A(i, j), x, 'upper')\"\n    # assignment\n    assert mcode(A[i, j], assign_to=A[i, j]) == \"A(i, j) = A(i, j)\"\n    assert mcode(A[i, j] + x, assign_to=A[i, j]) == \"A(i, j) = A(i, j) + x\"\n    assert mcode(A[i, j] + A[i, j], assign_to=A[i, j]) == \"A(i, j) = 2*A(i, j)\"\n    assert mcode(3*A[i, j], assign_to=A[i, j]) == \"A(i, j) = "], "sample_589": ["def test_interpolate_na_max_gap_non_uniform_index(da_time):\n    da_time[\"t\"] = pd.to_datetime(\n        [\"2001-01-01\", \"2001-01-02\", \"2001-01-05\", \"2001-01-10\", \"2001-01-15\"]\n    )\n    expected = da_time.copy(\n        data=[np.nan, 1, 2, np.nan, 5, np.nan, np.nan, np.nan, np.nan, np.nan, 10]\n    )\n    actual = da_time.interpolate_na(\"t\", max_gap=\"3D\")\n    assert_equal(actual, expected)\n", "def test_interpolate_na_max_gap_numeric(da_time, max_gap, expected):\n    da_time[\"t\"] = np.arange(11)\n    actual = da_time.interpolate_na(\"t\", max_gap=max_gap)\n    assert_equal(actual, expected)\n", "def test_get_clean_interp_index_errors(da_time):\n    with raises_regex(ValueError, \"Index 't' must be monotonically increasing\"):\n        da_time[\"t\"] = pd.Index([1, 2, 3, 2, 4])\n        get_clean_interp_index(da_time, \"t\")\n\n    with raises_regex(ValueError, \"Index 't' has duplicate values\"):\n        da_time[\"t\"] = pd.Index([1, 2, 3, 2, 4])\n        get_clean_interp_index(da_time, \"t\", use_coordinate=True)\n\n    with raises_regex(TypeError, \"Index 't' must be castable to float64\"):\n        da_time[\"t\"] = pd.MultiIndex.from_product([[\"a\", \"b\"], [1, 2]])\n        get_clean_interp_index(da_time, \"t\")\n", "def test_interpolate_na_multidimensional():\n    shape = (10, 10)\n    da, _ = make_interpolate_example_data(shape, 0.5)\n    actual = da.interpolate_na(dim=[\"time\", \"x\"], method=\"linear\")\n    assert (da.count(dim=[\"time\", \"x\"]) <= actual.count(dim=[\"time\", \"x\"])).all()\n\n    actual = da.interpolate_na(dim=[\"time\", \"x\"], method=\"nearest\")\n    assert (da.count(dim=[\"time\", \"x\"]) <= actual.count(dim=[\"time\", \"x\"])).all()\n\n    with raises_regex(ValueError, \"only 'linear' and 'nearest' methods are supported\"):\n        da.interpolate_na(dim=[\"time\", \"x\"], method=\"slinear\")\n", "def test_interpolate_na_multidim_dataset(ds):\n    ds[\"var1\"] = ds[\"var1\"].expand_dims(\"z\")\n    actual = ds.interpolate_na(dim=\"time\", method=method)\n    assert actual.dims == ds.dims\n    assert actual[\"var1\"].count(\"time\") <= ds[\"var1\"].count(\"time\")\n"], "sample_325": ["def test_boundfield_css_classes_for_multiple_choice_fields(self):\n    class SongForm(Form):\n        name = CharField()\n        composers = MultipleChoiceField(\n            choices=[('J', 'John Lennon'), ('P', 'Paul McCartney')],\n            widget=CheckboxSelectMultiple,\n        )\n\n    f = SongForm({'name': 'Yesterday', 'composers': ['P']})\n    self.assertEqual(f['composers'].css_classes(), '')\n    self.assertEqual(f['composers'].css_classes(extra_classes=''), '')\n    self.assertEqual(f['composers'].css_classes(extra_classes='test'), 'test')\n    self.assertEqual(f['composers'].css_classes(extra_classes='test test'), 'test')\n    self.assertEqual(f['composers'].css_classes(extra_classes='test test test'), 'test')\n", "def test_boundfield_subwidgets_are_cached(self):\n    class TestForm(Form):\n        field = ChoiceField(choices=[('a', 'A'), ('b', 'B')], widget=CheckboxSelectMultiple)\n\n    form = TestForm()\n    boundfield = form['field']\n\n    # Accessing subwidgets once should cache them.\n    subwidgets = list(boundfield.subwidgets)\n    self.assertIsNotNone(subwidgets)\n\n    # Accessing subwidgets again should not recreate them.\n    self.assertIs(subwidgets, list(boundfield.subwidgets))\n", "def test_boundfield_init_order(self):\n    class MyBoundField(BoundField):\n            super().__init__(*args, **kwargs)\n            self.init_order = [name for name in dir(self) if not name.startswith('__')]\n\n    class MyForm(Form):\n        my_field = CharField()\n\n    form = MyForm()\n    bf = MyBoundField(form, form.fields['my_field'], 'my_field')\n    self.assertEqual(bf.init_order, [\n        'form', 'field', 'name', 'html_name', 'html_initial_name', 'html_initial_id',\n        'auto_id', 'label', 'help_text', 'errors', 'subwidgets',\n    ])\n", "def test_hidden_input_with_id(self):\n    # An ID attribute should be added to a hidden input if auto_id=True.\n    class HiddenInputForm(Form):\n        hidden = CharField(widget=HiddenInput)\n\n    f = HiddenInputForm(auto_id=True)\n    self.assertHTMLEqual(f.as_p(), '<p><input id=\"id_hidden\" name=\"hidden\" type=\"hidden\" required></p>')\n", "def test_boundfield_errors_as_json(self):\n    class MyForm(Form):\n        foo = CharField()\n        bar = CharField()\n\n            raise ValidationError('Non-field error.', code='secret', params={'a': 1, 'b': 2})\n\n    form = MyForm({})\n    self.assertIs(form.is_valid(), False)\n\n    errors = form.errors\n    control = {\n        'foo': [{\"message\": \"This field is required.\", \"code\": \"required\"}],\n        'bar': [{\"message\": \"This field is required.\", \"code\": \"required\"}],\n        '__all__': [{\"message\": \"Non-field error.\", \"code\": \"secret\"}],\n    }\n    self.assertEqual(errors.get_json_data(), control)\n    self.assertEqual(json.dumps(errors.get_json_data()), errors.as_json())\n\n    # When the error list has a custom class, it should be preserved\n    form = MyForm({}, error_class=DivErrorList)\n    self.assertIs(form.is_valid(), False)\n    self.assertEqual(form.errors.get_json_data(), control)\n    self.assertEqual(json.dumps(errors.get_json_data()), errors.as_json())\n\n    # When error list has a custom class and code is set, it should be preserved\n    class CustomErrorList(ErrorList):\n            data = super().get_json_data(escape_html)\n            for field_errors in data.values():\n                for error in field_errors:\n                    error['custom_key'] = 'value'\n            return data\n\n    form = MyForm({}, error_class=CustomErrorList)\n    self.assertIs(form.is_valid(), False)\n    control = {\n        'foo': [{\"message\": \"This field is required.\", \"code\": \"required\", \"custom_key\": \"value\"}],\n        'bar': [{\"message\": \"This field is required.\", \"code\": \"required\", \"custom_key\": \"value\"}],\n        '__all__': [{\"message\": \"Non-field error.\", \"code\": \"secret\", \"custom_key\": \"value\"}],\n    }\n    self.assertEqual(form.errors.get_json_data(), control)\n    self.assertEqual(json.dumps(form.errors.get_json_data()), form.errors.as_json())\n"], "sample_146": ["    def test_valid_language_settings_consistent(self):\n        for tag in ['en', 'es']:\n            with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n                self.assertEqual(check_language_settings_consistent(None), [])\n", "def test_language_settings_consistent_with_supported_variant(self):\n    tests = [\n        # language with supported variant.\n        'sr-latn',\n        'zh-hans',\n        'zh-hant',\n    ]\n    for tag in tests:\n        with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n            self.assertEqual(check_language_settings_consistent(None), [])\n\n    tests = [\n        # language without supported variant.\n        'sr',\n    ]\n    for tag in tests:\n        with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n            self.assertEqual(check_language_settings_consistent(None), [\n                Error('You have provided a value for the LANGUAGE_CODE setting that is not in the LANGUAGES setting.', id='translation.E004'),\n            ])\n", "    def test_language_settings_consistent_no_languages(self):\n        msg = (\n            'You have provided a value for the LANGUAGE_CODE setting that is '\n            'not in the LANGUAGES setting.'\n        )\n        with self.settings(LANGUAGES=[]):\n            self.assertEqual(check_language_settings_consistent(None), [\n                Error(msg, id='translation.E004'),\n            ])\n", "def test_valid_script_consistent_language_settings(self):\n    tests = [\n        # language + script.\n        'sr-latn',\n        'zh-hans',\n    ]\n    for tag in tests:\n        with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n            self.assertEqual(check_language_settings_consistent(None), [])\n", "def test_language_settings_consistent_with_same_language_code(self):\n    self.assertEqual(check_language_settings_consistent(None), [])\n"], "sample_543": ["def test_rectangle_selector_rotation_point():\n    tool = widgets.RectangleSelector(ax, onselect=noop,\n                                     interactive=True,\n                                     props=dict(facecolor='b', alpha=0.2))\n    # Draw rectangle\n    click_and_drag(tool, start=(0, 10), end=(100, 120))\n\n    assert tool._selection_artist.rotation_point == 'center'\n    tool._selection_artist.rotation_point = 'upper left'\n    assert tool._selection_artist.rotation_point == 'upper left'\n    tool._selection_artist.rotation_point = 'center'\n    assert tool._selection_artist.rotation_point == 'center'\n\n    with pytest.raises(ValueError):\n        tool._selection_artist.rotation_point = 'unvalid_value'\n", "def test_MultiCursor_disconnect(ax):\n    (ax1, ax3) = plt.figure().subplots(2, sharex=True)\n    ax2 = plt.figure().subplots()\n\n    # useblit=false to avoid having to draw the figure to cache the renderer\n    multi = widgets.MultiCursor(\n        None, (ax1, ax2), useblit=False, horizOn=True, vertOn=True\n    )\n\n    # Mock a motion_notify_event\n    # Can't use `do_event` as that helper requires the widget\n    # to have a single .ax attribute.\n    event = mock_event(ax1, xdata=.5, ydata=.25)\n    multi.onmove(event)\n\n    # Disconnect events\n    multi.disconnect()\n\n    # the lines in the first two ax should still move\n    for l in multi.vlines:\n        assert l.get_xdata() == (.5, .5)\n    for l in multi.hlines:\n        assert l.get_ydata() == (.25, .25)\n    # The relevant lines get turned on after move.\n    assert len([line for line in multi.vlines if line.get_visible()]) == 2\n    assert len([line for line in multi.hlines if line.get_visible()]) == 2\n\n    # After toggling settings, the opposite lines should be visible after move.\n    multi.horizOn = not multi.horizOn\n    multi.vertOn = not multi.vertOn\n    event = mock_event(ax1, xdata=.5, ydata=.25)\n    multi.onmove(event)\n    assert len([line for line in multi.vlines if line.get_visible()]) == 0\n    assert len([line for line in multi.hlines if line.get_visible()]) == 0\n", "def test_lasso_selector_minspan(ax, direction, minspanx, minspany, x1, y1):\n\n    onselect = mock.Mock(spec=noop, return_value=None)\n\n    x0, y0 = (10, 10)\n    if direction == 'horizontal':\n        minspanx, minspany = (x1 - x0, 0)\n    else:\n        minspanx, minspany = (0, y1 - y0)\n\n    tool = widgets.LassoSelector(ax, onselect, direction=direction,\n                                 minspanx=minspanx, minspany=minspany)\n    # Too small to create a selector\n    do_event(tool, 'press', xdata=x0, ydata=y0, button=1)\n    do_event(tool, 'onmove', xdata=x1, ydata=y1, button=1)\n    do_event(tool, 'release', xdata=x1, ydata=y1, button=1)\n\n    onselect.assert_not_called()\n\n    do_event(tool, 'press', xdata=20, ydata=20, button=1)\n    do_event(tool, 'onmove', xdata=30, ydata=30, button=1)\n    do_event(tool, 'release', xdata=30, ydata=30, button=1)\n    onselect.assert_called_once()\n    (verts, ), kwargs = onselect.call_args\n    assert verts == [(20, 20), (30, 30), (30, 30)]\n    assert kwargs == {}\n", "def test_polygon_selector_box_scale(ax):\n    # Create a diamond shape\n    verts = [(20, 0), (0, 20), (20, 40), (40, 20)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[3]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    # Create selector\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=True)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    # In order to trigger the correct callbacks, trigger events on the canvas\n    # instead of the individual tools\n    t = ax.transData\n    canvas = ax.figure.canvas\n\n    # Scale by 50% using the top right corner of the bounding box\n    MouseEvent(\n        \"button_press_event\", canvas, *t.transform((40, 40)), 1)._process()\n    MouseEvent(\n        \"motion_notify_event\", canvas, *t.transform((40, 40)))._process()\n    np.testing.assert_allclose(\n        tool._box.extents, (20, 40, 20, 40))\n\n    # Scale by 50% using the top right corner of the bounding box\n    MouseEvent(\n        \"button_press_event\", canvas, *t.transform((40, 40)), 1)._process()\n    MouseEvent(\n        \"motion_notify_event\", canvas, *t.transform((20, 20)))._process()\n    MouseEvent(\n        \"button_release_event\", canvas, *t.transform((20, 20)), 1)._process()\n    np.testing.assert_allclose(\n        tool.verts, [(10, 0), (0, 10), (10, 20), (20, 10)])\n    np.testing.assert_allclose(\n        tool._box.extents, (0, 20, 10, 20))\n", "def test_rectangle_selector_add_remove_state(ax, state, button):\n    tool = widgets.RectangleSelector(ax, onselect=noop, interactive=True)\n    # Draw rectangle\n    click_and_drag(tool, start=(0, 10), end=(100, 120))\n\n    # Test state is added correctly\n    tool.add_state(state)\n    assert tool._state == {state}\n    tool.add_state(state)\n    assert tool._state == {state}\n\n    # Test state is removed correctly\n    tool.remove_state(state)\n    assert tool._state == set()\n\n    # Test adding state when button is not pressed\n    tool._state = set()\n    do_event(tool, 'press', xdata=100, ydata=100, button=button)\n    tool.add_state(state)\n    assert tool._state == {state}\n    do_event(tool, 'release', xdata=100, ydata=100, button=button)\n    tool.remove_state(state)\n    assert tool._state == set()\n"], "sample_843": ["def test_kernel_theta_bounds(kernel):\n    # Test that theta of kernel is within its bounds.\n    assert_array_less(kernel.bounds[:, 0], kernel.theta)\n    assert_array_less(kernel.theta, kernel.bounds[:, 1])\n", "def test_theta_bounds(kernel):\n    # Test that bounds are correctly set and read.\n\n    # Check that setting bounds works\n    bounds = np.log(kernel.bounds)\n    assert_array_almost_equal(bounds, kernel.bounds)\n\n    # Check that bounds are correctly read.\n    kernel_bounds = np.exp(bounds)\n    assert_array_almost_equal(kernel_bounds, kernel.bounds)\n", "def test_kernel_bounds(kernel):\n    # Check that bounds of kernel are consistent with theta and hyperparameters.\n\n    # Check that bounds are consistent with theta\n    for hyperparameter in kernel.hyperparameters:\n        if hyperparameter.fixed:\n            continue\n        if hyperparameter.n_elements > 1:  # anisotropic kernels\n            for i, value in enumerate(np.exp(kernel.theta[hyperparameter.name])):\n                assert hyperparameter.bounds[0][i] <= value <= hyperparameter.bounds[1][i]\n        else:\n            value = np.exp(kernel.theta[hyperparameter.name])\n            assert hyperparameter.bounds[0] <= value <= hyperparameter.bounds[1]\n\n    # Check that set_params() throws an error for out-of-bounds values\n    params = kernel.get_params()\n    for hyperparameter in kernel.hyperparameters:\n        if hyperparameter.fixed:\n            continue\n        if hyperparameter.n_elements > 1:  # anisotropic kernels\n            value = hyperparameter.bounds[0] - 1.0\n            params[hyperparameter.name] = [value] * hyperparameter.n_elements\n            with pytest.raises(ValueError):\n                kernel.set_params(**params)\n            value = hyperparameter.bounds[1] + 1.0\n            params[hyperparameter.name] = [value] * hyperparameter.n_elements\n            with pytest.raises(ValueError):\n                kernel.set_params(**params)\n        else:\n            value = hyperparameter.bounds[0] - 1.0\n            params[hyperparameter.name] = value\n            with pytest.raises(ValueError):\n                kernel.set_params(**params)\n            value = hyperparameter.bounds[1] + 1.0\n            params[hyperparameter.name] = value\n            with pytest.raises(ValueError):\n                kernel.set_params(**params)\n", "def test_kernel_bounds(kernel):\n    # Test that setting bounds with set_params works as expected.\n\n    # Check that bounds are correctly set.\n    bounds = (1e-5, 1e5)\n    kernel.set_params(**{hyperparameter.name + \"_bounds\": bounds\n                        for hyperparameter in kernel.hyperparameters\n                        if not hyperparameter.fixed})\n    for hyperparameter in kernel.hyperparameters:\n        if hyperparameter.fixed:\n            assert hyperparameter.bounds == \"fixed\"\n        else:\n            assert_array_equal(hyperparameter.bounds, bounds)\n\n    # Check that trying to set invalid bounds raises an exception.\n    with pytest.raises(ValueError):\n        kernel.set_params(**{hyperparameter.name + \"_bounds\": (0, 1)\n                             for hyperparameter in kernel.hyperparameters\n                             if not hyperparameter.fixed})\n\n    # Check that fixed bounds are correctly set.\n    kernel.set_params(**{hyperparameter.name + \"_bounds\": \"fixed\"\n                        for hyperparameter in kernel.hyperparameters\n                        if not hyperparameter.fixed})\n    for hyperparameter in kernel.hyperparameters:\n        assert hyperparameter.bounds == \"fixed\"\n\n    # Check that trying to set invalid fixed bounds raises an exception.\n    with pytest.raises(ValueError):\n        kernel.set_params(**{hyperparameter.name + \"_bounds\": \"invalid\"\n                             for hyperparameter in kernel.hyperparameters\n                             if not hyperparameter.fixed})\n", "def test_kernel_bounds(kernel):\n    # Test that kernel bounds are correctly returned.\n\n    # Test that bounds are returned with correct shape.\n    assert kernel.bounds.shape == (kernel.n_dims, 2)\n\n    # Test that bounds are correctly set by set_params\n    new_bounds = np.log(np.random.RandomState(0).uniform(0.5, 10, kernel.n_dims))\n    kernel.set_params(**{'bounds': np.exp(new_bounds).tolist()})\n    assert_array_almost_equal(kernel.bounds, new_bounds)\n\n    # Test that bounds are correctly set by set_params for anisotropic kernels\n    if hasattr(kernel, 'length_scale'):\n        new_length_scale_bounds = np.log(np.random.RandomState(0).uniform(0.5, 10, len(kernel.length_scale)))\n        kernel.set_params(**{'length_scale_bounds': np.exp(new_length_scale_bounds).tolist()})\n        assert_array_almost_equal(kernel.bounds, new_length_scale_bounds)\n\n    # Test that setting fixed bounds via set_params works.\n    kernel.set_params(**{'bounds': \"fixed\"})\n    assert_array_equal(kernel.bounds, np.array([]))\n\n    # Test that getting bounds via get_params works.\n    params = kernel.get_params()\n    for hyperparameter in kernel.hyperparameters:\n        assert_array_almost_equal(params[hyperparameter.name + '_bounds'], np.exp(hyperparameter.bounds))\n"], "sample_1158": ["def test_sympify_ImmutableDenseNDimArray():\n    a = ImmutableDenseNDimArray([1, 2, 3], shape=(1, 3))\n    assert sympify(a) == a\n    assert sympify(a).shape == (1, 3)\n    assert sympify(a)[0, 0] == 1\n    assert sympify(a)[0, 1] == 2\n    assert sympify(a)[0, 2] == 3\n", "def test_sympify_issue_21536_invalid_input():\n    #test to check evaluate=False in case of invalid input\n    raises(SympifyError, lambda: sympify(\"x+3*x+2+a\", evaluate=False))\n    raises(SympifyError, lambda: sympify(\"2*x+4*x+2+4+a\", evaluate=False))\n    raises(SympifyError, lambda: sympify([\"x+3*x+2+a\", \"2*x+4*x+2+4+a\"], evaluate=False))\n\n    #test to check evaluate=True in case of invalid input\n    raises(SympifyError, lambda: sympify(\"x+3*x+2+a\", evaluate=True))\n    raises(SympifyError, lambda: sympify(\"2*x+4*x+2+4+a\", evaluate=True))\n    raises(SympifyError, lambda: sympify([\"x+3*x+2+a\", \"2*x+4*x+2+4+a\"], evaluate=True))\n\n    #test to check evaluate with no input in case of invalid input\n    raises(SympifyError, lambda: sympify(\"x+3*x+2+a\"))\n    raises(SympifyError, lambda: sympify(\"2*x+4*x+2+4+a\"))\n    raises(SympifyError, lambda: sympify([\"x+3*x+2+a\", \"2*x+4*x+2+4+a\"]))\n", "def test_sympify_eval_fallback():\n    # Test case where sympify falls back to eval\n    class CustomClass:\n            return 'x'\n\n    a = CustomClass()\n    with warns_deprecated_sympy():\n        assert sympify(a) == Symbol('x')\n", "def test_issue_29999():\n    # issue #29999\n    class CustomFloat:\n            return 1.5\n\n            return Float(2.0)\n\n    cf = CustomFloat()\n\n    assert sympify(cf) == 2.0\n    assert _sympify(cf) == 2.0\n    assert sympify(cf, strict=True) == 2.0\n    assert _sympify(cf, strict=True) == 2.0\n", "def test_issue_22000():\n    class TestClass:\n            return 1.0\n\n            return 1\n\n    obj = TestClass()\n    # Check that conversion to Float is done when a class implements both\n    # __int__ and __float__ methods, even if __int__ is implemented.\n    assert sympify(obj) == 1.0\n    assert isinstance(sympify(obj), Float)\n\n    class TestClass2:\n            return 1.0\n\n            return 1\n\n    obj2 = TestClass2()\n    # Check that conversion to Integer is done when a class implements both\n    # __int__ and __float__ methods, but also implements _sympy_ method.\n    assert sympify(obj2) == 1\n    assert isinstance(sympify(obj2), Integer)\n"], "sample_587": ["    def test_merge_compat_override(self):\n        ds1 = xr.Dataset({\"x\": 0})\n        ds2 = xr.Dataset({\"x\": 1})\n        assert ds1.identical(ds1.merge(ds2, compat=\"override\"))\n", "    def test_merge_compat_override(self):\n        ds1 = xr.Dataset({\"x\": 0})\n        ds2 = xr.Dataset({\"x\": 1})\n        assert ds1.identical(ds1.merge(ds2, compat=\"override\"))\n", "    def test_merge_override(self):\n        ds1 = xr.Dataset({\"x\": 0})\n        ds2 = xr.Dataset({\"x\": 1})\n        assert ds1.identical(ds1.merge(ds2, compat=\"override\"))\n\n        ds1 = xr.Dataset({\"x\": (\"y\", [0, 0])})\n        ds2 = xr.Dataset({\"x\": 1})\n        assert ds1.identical(ds1.merge(ds2, compat=\"override\"))\n\n        ds1 = xr.Dataset({\"x\": 0})\n        ds2 = xr.Dataset({\"x\": (\"y\", [1, 1])})\n        assert ds1.identical(ds1.merge(ds2, compat=\"override\"))\n\n        ds1 = xr.Dataset({\"x\": 0, \"y\": 1})\n        ds2 = xr.Dataset({\"x\": 1, \"z\": 2})\n        expected = xr.Dataset({\"x\": 0, \"y\": 1, \"z\": 2})\n        assert expected.identical(ds1.merge(ds2, compat=\"override\"))\n", "    def test_merge_core_variables(self):\n        data = create_test_data()\n        actual = merge.merge_core(\n            [data],\n            compat=\"broadcast_equals\",\n            join=\"outer\",\n            priority_arg=None,\n            explicit_coords=None,\n            indexes=None,\n            fill_value=dtypes.NA,\n        )\n        expected_variables = data.variables\n        expected_coord_names = set(data.coords)\n        expected_dims = dict(data.dims)\n        assert actual[0] == expected_variables\n        assert actual[1] == expected_coord_names\n        assert actual[2] == expected_dims\n", "    def test_merge_variables(self):\n        var1 = xr.Variable(\"x\", [1, 2])\n        var2 = xr.Variable(\"x\", [2, 3])\n        var3 = xr.Variable(\"y\", [4, 5])\n        vars = merge.merge_variables([OrderedDict(x=var1), OrderedDict(x=var2, y=var3)],\n                                    compat=\"no_conflicts\")\n        assert list(vars.keys()) == [\"x\", \"y\"]\n        np.testing.assert_equal(vars[\"x\"].values, [1, 2, 3])\n        np.testing.assert_equal(vars[\"y\"].values, [4, 5])\n\n        var1 = xr.Variable(\"x\", [1, 2], {\"foo\": \"bar\"})\n        var2 = xr.Variable(\"x\", [2, 3])\n        with pytest.raises(xr.MergeError):\n            merge.merge_variables([OrderedDict(x=var1), OrderedDict(x=var2)],\n                                 compat=\"identical\")\n\n        var1 = xr.Variable(\"x\", [1, 2])\n        var2 = xr.Variable(\"x\", [3, 4])\n        vars = merge.merge_variables([OrderedDict(x=var1), OrderedDict(x=var2)],\n                                    compat=\"override\")\n        assert list(vars.keys()) == [\"x\"]\n        np.testing.assert_equal(vars[\"x\"].values, [1, 2])\n"], "sample_970": ["def test_isabstractmethod():\n    class AbstractClass:\n        @abstractmethod\n            pass\n\n    class ConcreteClass(AbstractClass):\n            pass\n\n    assert inspect.isabstractmethod(AbstractClass.abstract_method) is True\n    assert inspect.isabstractmethod(ConcreteClass.abstract_method) is False\n    assert inspect.isabstractmethod(AbstractClass) is False\n    assert inspect.isabstractmethod(ConcreteClass) is False\n    assert inspect.isabstractmethod(1) is False\n    assert inspect.isabstractmethod('') is False\n", "def test_isabstractmethod():\n    from abc import ABC, abstractmethod\n\n    class BaseClass:\n        @abstractmethod\n            pass\n\n    class SubClass(BaseClass):\n            pass\n\n    assert inspect.isabstractmethod(BaseClass.meth) is True\n    assert inspect.isabstractmethod(SubClass.meth) is False\n    assert inspect.isabstractmethod(BaseClass) is False\n    assert inspect.isabstractmethod(BaseClass.__init__) is False\n", "def test_is_new_type():\n    from typing import NewType\n    from sphinx.util.inspect import isNewType\n\n    NT1 = NewType('NT1', int)\n    assert isNewType(NT1) is True\n\n    NT2 = type('NT2', (int,), {})\n    assert isNewType(NT2) is False\n\n    class NT3:\n        __module__ = 'typing'\n        __qualname__ = 'NewType.<locals>.new_type'\n\n    assert isNewType(NT3) is True\n", "def test_isabstractmethod():\n    class AbstractClass:\n        @abstractmethod\n            pass\n\n            pass\n\n    class ConcreteClass(AbstractClass):\n            pass\n\n    assert inspect.isabstractmethod(AbstractClass.abstract_method) is True\n    assert inspect.isabstractmethod(ConcreteClass.abstract_method) is False\n    assert inspect.isabstractmethod(AbstractClass.concrete_method) is False\n    assert inspect.isabstractmethod(ConcreteClass.concrete_method) is False\n\n    # Test abstractmethod from built-in module abc\n    from abc import ABC, abstractmethod\n\n    class AnotherAbstractClass(ABC):\n        @abstractmethod\n            pass\n\n    assert inspect.isabstractmethod(AnotherAbstractClass.another_abstract_method) is True\n", "def test_is_new_type():\n    # Test isNewType() for NewType (py >= 3.10)\n    if sys.version_info >= (3, 10):\n        from typing import NewType\n        MyType = NewType('MyType', int)\n        assert inspect.isNewType(MyType) is True\n        assert inspect.isNewType(int) is False\n\n    # Test isNewType() for older Python versions\n    else:\n        MyType = typing.NewType('MyType', int)\n        assert inspect.isNewType(MyType) is True\n        assert inspect.isNewType(int) is False\n"], "sample_150": ["def test_database_checks_called_with_app_configs(self, mocked_check):\n    app_configs = [mock.Mock()]\n    check_database_backends(app_configs=app_configs)\n    self.assertTrue(mocked_check.called)\n    self.assertEqual(mocked_check.call_count, len(app_configs))\n", "    def test_command_create_parser(self):\n        class TestCommand(BaseCommand):\n            help = 'Test command'\n\n        command = TestCommand()\n        parser = command.create_parser('manage.py', 'test')\n        self.assertIsInstance(parser, CommandParser)\n        self.assertEqual(parser.prog, 'manage.py test')\n        self.assertEqual(parser.description, 'Test command')\n", "    def test_base_command_execute(self):\n        class TestCommand(BaseCommand):\n                return 'Test output'\n\n        command = TestCommand(stdout=mock.Mock(), stderr=mock.Mock())\n        options = {\n            'force_color': False,\n            'no_color': False,\n            'stdout': None,\n            'stderr': None,\n            'skip_checks': False,\n            'traceback': False,\n            'version': None,\n            'verbosity': 1,\n            'settings': None,\n            'pythonpath': None,\n        }\n        output = command.execute(**options)\n        self.assertEqual(output, 'Test output')\n", "    def test_base_command_without_handle_method(self):\n        class TestCommand(BaseCommand):\n            pass\n\n        with self.assertRaises(NotImplementedError):\n            TestCommand().handle()\n", "    def test_handle_default_options(self):\n        options = mock.Mock(settings='test_settings', pythonpath='/test/path')\n        handle_default_options(options)\n        self.assertEqual(os.environ['DJANGO_SETTINGS_MODULE'], 'test_settings')\n        self.assertEqual(sys.path[0], '/test/path')\n"], "sample_972": ["def test_get_type_hints():\n        pass\n\n    assert get_type_hints(foo) == {'a': int, 'b': str, 'return': None}\n\n    class Bar:\n            pass\n\n    assert get_type_hints(Bar) == {'c': int, 'd': str, 'return': None}\n\n    class Baz:\n        __annotations__ = {'e': int, 'f': str}\n\n    assert get_type_hints(Baz) == {'e': int, 'f': str}\n\n    assert get_type_hints(None) == {}\n\n    try:\n        raise TypeError\n    except TypeError as e:\n        assert get_type_hints(e) == {}\n", "def test_get_type_hints():\n        pass\n\n    annotations = get_type_hints(func)\n    assert annotations == {'a': int, 'b': str, 'return': NoneType}\n\n    class MyClass:\n            self.x = x\n            self.y = y\n\n    annotations = get_type_hints(MyClass)\n    assert annotations == {'x': int, 'y': str}\n\n    annotations = get_type_hints(MyClass.__init__)\n    assert annotations == {'x': int, 'y': str, 'return': NoneType}\n\n    annotations = get_type_hints(MyClass.__new__)\n    assert annotations == {}\n\n    annotations = get_type_hints(mock.Mock())\n    assert annotations == {}\n\n    annotations = get_type_hints(None)\n    assert annotations == {}\n\n    annotations = get_type_hints(123)\n    assert annotations == {}\n\n    annotations = get_type_hints('hello')\n    assert annotations == {}\n", "def test_get_type_hints():\n        pass\n\n    class MyClass:\n            pass\n\n    assert get_type_hints(func) == {'a': int, 'b': str, 'return': NoneType}\n    assert get_type_hints(MyClass.method) == {'a': int, 'b': str, 'return': NoneType}\n    assert get_type_hints(MyClass) == {}\n\n    class TestTypeVar(Tuple[T, T]):\n        pass\n\n    assert get_type_hints(TestTypeVar) == {'T': T}\n\n    class TestUnion(Tuple[Union[int, str], Union[int, str]]):\n        pass\n\n    assert get_type_hints(TestUnion) == {(0,): 'typing.Union[int, str]', (1,): 'typing.Union[int, str]'}\n", "def test_get_type_hints():\n        pass\n\n    assert get_type_hints(foo) == {'a': int, 'b': str, 'return': None}\n\n        pass\n\n    assert get_type_hints(bar) == {'a': int, 'b': List[str], 'return': None}\n\n        pass\n\n    assert get_type_hints(baz) == {'a': int, 'b': List[Dict[str, int]], 'return': None}\n\n    class MyClass:\n            pass\n\n    assert get_type_hints(MyClass.__init__) == {'a': int, 'b': str, 'return': None}\n\n    class MyNewType(NewType('MyNewType', int)):\n        pass\n\n    assert get_type_hints(MyNewType.__new__) == {'_name': str, 'bases': Tuple[type, ...], 'dict': Dict[str, Any], 'return': MyNewType}\n\n    T = TypeVar('T')\n    class MyGeneric(Generic[T]):\n            pass\n\n    assert get_type_hints(MyGeneric.__init__) == {'a': T, 'return': None}\n", "def test_get_type_hints():\n        pass\n\n    assert get_type_hints(func) == {'a': int, 'b': str, 'return': None}\n\n    class Class:\n            pass\n\n    assert get_type_hints(Class) == {'a': int, 'b': str, 'return': None}\n\n    assert get_type_hints(BrokenType) == {}\n\n    class GenericClass(Generic[T]):\n            pass\n\n    assert get_type_hints(GenericClass) == {'a': TypeVar('T')}\n\n    class UnionClass:\n            pass\n\n    if sys.version_info >= (3, 7):\n        assert get_type_hints(UnionClass) == {'a': Union[int, str]}\n    else:\n        assert get_type_hints(UnionClass) == {'a': type(None)}\n\n    class OptionalClass:\n            pass\n\n    if sys.version_info >= (3, 7):\n        assert get_type_hints(OptionalClass) == {'a': Optional[int]}\n    else:\n        assert get_type_hints(OptionalClass) == {'a': Union[int, None]}\n\n    class ForwardRefClass:\n            pass\n\n    assert get_type_hints(ForwardRefClass) == {'a': ForwardRef('MyClass1')}\n\n    class NewTypeClass:\n            pass\n\n    if sys.version_info >= (3, 10):\n        assert get_type_hints(NewTypeClass) == {'a': MyInt}\n    else:\n        assert get_type_hints(NewTypeClass) == {}\n\n    class LiteralClass:\n            pass\n\n    if sys.version_info >= (3, 8):\n        assert get_type_hints(LiteralClass) == {'a': Literal[1, '2', \"\\r\"]}\n    else:\n        assert get_type_hints(LiteralClass) == {}\n"], "sample_1105": ["def test_entry():\n    M = MatrixSymbol('M', 2, 2)\n    N = MatrixSymbol('N', 2, 2)\n    expr = MatMul(M, N)\n    assert expr._entry(0, 0) == M[0, 0]*N[0, 0] + M[0, 1]*N[1, 0]\n    assert expr._entry(0, 1) == M[0, 0]*N[0, 1] + M[0, 1]*N[1, 1]\n    assert expr._entry(1, 0) == M[1, 0]*N[0, 0] + M[1, 1]*N[1, 0]\n    assert expr._entry(1, 1) == M[1, 0]*N[0, 1] + M[1, 1]*N[1, 1]\n", "def test_entry():\n    X = MatrixSymbol('X', 2, 3)\n    Y = MatrixSymbol('Y', 3, 4)\n    expr = MatMul(X, Y)\n    result = expr._entry(0, 0)\n    expected = X[0, 0]*Y[0, 0] + X[0, 1]*Y[1, 0] + X[0, 2]*Y[2, 0]\n    assert result == expected\n", "def test_entry():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    expr = MatMul(A, B)\n    assert expr._entry(0, 0) == Sum(A[0, 0]*B[0, 0] + A[0, 1]*B[1, 0], (Dummy(\"i_1\"), 0, 1))\n    assert expr._entry(1, 1) == Sum(A[1, 0]*B[0, 1] + A[1, 1]*B[1, 1], (Dummy(\"i_1\"), 0, 1))\n", "def test_matmul_entry():\n    n, m = symbols('n m', integer=True, positive=True)\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, n)\n    C = MatrixSymbol('C', n, n)\n    assert MatMul(A, B)[0, 0].doit() == Sum(MatMul(*[A[i, j], B[j, 0]]), (j, 0, m-1))\n    assert MatMul(C, Transpose(C))[0, 0].doit() == Sum(MatMul(*[C[i, j], C[j, 0]]), (j, 0, n-1))\n", "def test_matmul_derivative_matrix_lines():\n    n = Symbol('n', positive=True, integer=True)\n    M = MatrixSymbol('M', n, n)\n    dM = M.diff(x)\n    expr = MatMul(M, dM)\n    expected = [MatMul(M.diff(x), Identity(n), Identity(n)),\n                MatMul(Identity(n), Identity(n), dM)]\n    assert expr._eval_derivative_matrix_lines(x) == expected\n"], "sample_916": ["def test_operator_definitions():\n    check('function', 'operator ~', {1: \"inv-operator\", 2: \"cov\"})\n    check('function', 'operator !', {1: \"not-operator\", 2: \"ntv\"})\n    check('function', 'operator +', {1: \"add-operator\", 2: \"plv\"})\n    check('function', 'operator -', {1: \"sub-operator\", 2: \"miv\"})\n    check('function', 'operator *', {1: \"mul-operator\", 2: \"mlv\"})\n    check('function', 'operator /', {1: \"div-operator\", 2: \"dvv\"})\n    check('function', 'operator %', {1: \"mod-operator\", 2: \"rmv\"})\n    check('function', 'operator ^', {1: \"xor-operator\", 2: \"eov\"})\n    check('function', 'operator &', {1: \"and-operator\", 2: \"anv\"})\n    check('function', 'operator |', {1: \"or-operator\", 2: \"orv\"})\n    check('function', 'operator &&', {1: \"sand-operator\", 2: \"aav\"})\n    check('function', 'operator ||', {1: \"sor-operator\", 2: \"oov\"})\n    check('function', 'operator ==', {1: \"eq-operator\", 2: \"eqv\"})\n    check('function', 'operator !=', {1: \"neq-operator\", 2: \"nev\"})\n    check('function', 'operator <', {1: \"lt-operator\", 2: \"ltv\"})\n    check('function', 'operator >', {1: \"gt-operator\", 2: \"gtv\"})\n    check('function', 'operator <=', {1: \"lte-operator\", 2: \"lev\"})\n    check('function', 'operator >=', {1: \"gte-operator\", 2: \"gev\"})\n    check('function', 'operator =', {1: \"assign-operator\", 2: \"aSv\"})\n    check('function', 'operator +=', {1: \"add-assign-operator\", 2: \"pLv\"})\n    check('function', 'operator -=', {1: \"sub-assign-operator\", 2: \"mIv\"})\n    check('function', 'operator *=', {1: \"mul-assign", "def test_concept_definitions_with_template_args():\n    check('concept', 'template<typename T> concept C = true;',\n          {2: 'I0E1C'})\n    check('concept', 'template<typename T> concept C = requires { };',\n          {2: 'I0E1C'})\n    check('concept', 'template<typename T> concept C = requires(T a) { };',\n          {2: 'I0E1C'})\n    check('concept', 'template<typename T> concept C = requires(T a) { { a } -> int; };',\n          {2: 'I0E1C'})\n    check('concept', 'template<typename T> concept C = requires(T a) { { a } -> int; { a } -> double; };',\n          {2: 'I0E1C'})\n", "def test_template_parameters():\n    check('function', 'void f(T)', {1: 'f__T', 2: '1fv'})\n    check('function', 'void f(T, U)', {1: 'f__T.U', 2: '1fv'})\n    check('function', 'void f(T, U, V)', {1: 'f__T.U.V', 2: '1fv'})\n\n    check('function', 'void f(int T)', {1: 'f__i', 2: '1fv'})\n    check('function', 'void f(int T, int U)', {1: 'f__i.i', 2: '1fv'})\n    check('function', 'void f(int T, int U, int V)', {1: 'f__i.i.i', 2: '1fv'})\n\n    check('function', 'void f(int T)', {1: 'f__i', 2: '1fv'})\n    check('function', 'void f(int T, double U)', {1: 'f__id', 2: '1fv'})\n    check('function', 'void f(int T, double U, float V)', {1: 'f__idf', 2: '1fv'})\n\n    check('function', 'void f(T = 0)', {1: 'f__i', 2: '1fv'})\n    check('function', 'void f(T = 0, U = 0)', {1: 'f__i.i', 2: '1fv'})\n    check('function', 'void f(T = 0, U = 0, V = 0)', {1: 'f__i.i.i', 2: '1fv'})\n\n    check('function', 'void f(T = 42)', {1: 'f__i', 2: '1fv'})\n    check('function', 'void f(T = 42, U = 0)', {1: 'f__i.i', 2: '1fv'})\n    check('function', 'void f(T = 42, U = 0, V = 0)', {1: 'f__i.i.i', 2: '1fv'})\n\n    check('function', 'void f(T = 3.14)', {1: 'f__d', 2: '1fv'})\n    check('function', 'void f(T", "def test_template_alias():\n    check('type', 'template <typename T> using A = T', {2: 'I0E1AE'})\n    check('type', 'template <typename T> using A = T::type', {2: 'I0E1AE'})\n    check('type', 'template <typename T> using A = std::vector<T>', {2: 'I0E1ANSt6vectorIT_EE'})\n    check('type', 'template <typename T, typename U> using A = T::type<U>', {2: 'I00E1AI1TE'})\n    check('type', 'template <typename T, typename U> using A = std::vector<T, U>', {2: 'I00E1ANSt6vectorIT_U_EE'})\n    check('type', 'template <typename T> using A = T::type<T>', {2: 'I0E1AE'})\n    check('type', 'template <typename T> using A = std::vector<T, T>', {2: 'I0E1ANSt6vectorIT_S1_EE'})\n    check('type', 'template <typename T, typename U, typename V> using A = T::type<U, V>', {2: 'I00E1AI1TE'})\n    check('type', 'template <typename T, typename U, typename V> using A = std::vector<T, U, V>', {2: 'I00E1ANSt6vectorIT_U2_EE'})\n    check('type', 'template <typename T> using A = T::type<T, T>', {2: 'I0E1AE'})\n    check('type', 'template <typename T> using A = std::vector<T, T, T>', {2: 'I0E1ANSt6vectorIT_S1_S1_EE'})\n    check('type', 'template <typename T> using A = std::vector<std::vector<T> >', {2: 'I0E1ANSt6vectorINS_6vectorIT_EEEEE'})\n", "def test_template_type_aliases():\n    # from #2858\n    check('type', 'template <typename T> using my_template_alias = T;',\n          {2: 'I0E19my_template_alias'})\n    check('type', 'template <typename T> using my_template_alias = T*', {2: 'I0E19my_template_alias'})\n    check('type', 'template <typename T> using my_template_alias = T&', {2: 'I0E19my_template_alias'})\n    check('type', 'template <typename T> using my_template_alias = T&&', {2: 'I0E19my_template_alias'})\n    check('type', 'template <typename T> using my_template_alias = T const', {2: 'I0E19my_template_alias'})\n    check('type', 'template <typename T> using my_template_alias = T volatile', {2: 'I0E19my_template_alias'})\n    check('type', 'template <typename T> using my_template_alias = T const volatile', {2: 'I0E19my_template_alias'})\n    check('type', 'template <typename T> using my_template_alias = T const volatile *', {2: 'I0E19my_template_alias'})\n    check('type', 'template <typename T> using my_template_alias = T const volatile *const', {2: 'I0E19my_template_alias'})\n    check('type', 'template <typename T> using my_template_alias = T const volatile *volatile', {2: 'I0E19my_template_alias'})\n    check('type', 'template <typename T> using my_template_alias = T const volatile *const volatile', {2: 'I0E19my_template_alias'})\n    check('type', 'template <typename T> using my_template_alias = T[5]', {2: 'I0E19my_template_alias'})\n    check('type', 'template <typename T> using my_template_alias = T[5][3]', {2: 'I0E19my_template_alias'})\n    check('type', 'template <typename T> using my_template_alias = T const [5][3]', {2: 'I0E19my_template_alias'})\n    check('type', 'template <typename T> using my_template_alias = T volatile [5][3]', {2: 'I0E19my_template_alias'})\n    check('type', 'template"], "sample_320": ["def test_rename_index_custom_database_table(self):\n    app_label = \"test_rename_index_custom_database_table\"\n    project_state = self.set_up_test_model(app_label, index=True, db_table=\"ponies\")\n    table_name = f\"{app_label}_ponies\"\n    self.assertIndexNameExists(table_name, \"pony_pink_idx\")\n    self.assertIndexNameNotExists(table_name, \"new_pony_test_idx\")\n    operation = migrations.RenameIndex(\n        \"Pony\", new_name=\"new_pony_test_idx\", old_name=\"pony_pink_idx\"\n    )\n    self.assertEqual(\n        operation.describe(),\n        \"Rename index pony_pink_idx on Pony to new_pony_test_idx\",\n    )\n    self.assertEqual(\n        operation.migration_name_fragment,\n        \"rename_pony_pink_idx_new_pony_test_idx\",\n    )\n\n    new_state = project_state.clone()\n    operation.state_forwards(app_label, new_state)\n    # Rename index.\n    expected_queries = 1 if connection.features.can_rename_index else 2\n    with connection.schema_editor() as editor, self.assertNumQueries(\n        expected_queries\n    ):\n        operation.database_forwards(app_label, editor, project_state, new_state)\n    self.assertIndexNameNotExists(table_name, \"pony_pink_idx\")\n    self.assertIndexNameExists(table_name, \"new_pony_test_idx\")\n    # Reversal.\n    with connection.schema_editor() as editor, self.assertNumQueries(\n        expected_queries\n    ):\n        operation.database_backwards(app_label, editor, new_state, project_state)\n    self.assertIndexNameExists(table_name, \"pony_pink_idx\")\n    self.assertIndexNameNotExists(table_name, \"new_pony_test_idx\")\n    # Deconstruction.\n    definition = operation.deconstruct()\n    self.assertEqual(definition[0], \"RenameIndex\")\n    self.assertEqual(definition[1], [])\n    self.assertEqual(\n        definition[2],\n        {\n            \"model_name\": \"Pony\",\n            \"old_name\": \"pony_pink_idx\",\n            \"new_name\": \"new_pony_test_idx\",\n        },\n    )\n", "def test_alter_field_with_func_index_together(self):\n    app_label = \"test_alfuncinto\"\n    index_name = f\"{app_label}_pony_idx\"\n    table_name = f\"{app_label}_pony\"\n    project_state = self.set_up_test_model(\n        app_label,\n        indexes=[models.Index(Abs(\"pink\"), name=index_name)],\n    )\n    operation = migrations.AlterField(\n        \"Pony\",\n        \"pink\",\n        models.IntegerField(null=True),\n    )\n    new_state = project_state.clone()\n    operation.state_forwards(app_label, new_state)\n    with connection.schema_editor() as editor:\n        operation.database_forwards(app_label, editor, project_state, new_state)\n    self.assertIndexNameExists(table_name, index_name)\n    with connection.schema_editor() as editor:\n        operation.database_backwards(app_label, editor, new_state, project_state)\n    self.assertIndexNameExists(table_name, index_name)\n", "    def test_alter_field_preserve_default_type_change(self):\n        \"\"\"\n        Tests the AlterField operation's state alteration\n        when the field type is changed and preserve_default = False.\n        \"\"\"\n        project_state = self.set_up_test_model(\"test_alflpd\")\n        # Test the state alteration\n        operation = migrations.AlterField(\n            \"Pony\",\n            \"weight\",\n            models.IntegerField(null=True),\n        )\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_alflpd\", new_state)\n        self.assertEqual(len(new_state.models[\"test_alflpd\", \"pony\"].fields), 3)\n        field = new_state.models[\"test_alflpd\", \"pony\"].fields[\"weight\"]\n        self.assertEqual(field.default, models.NOT_PROVIDED)\n        # Test the database alteration\n        project_state.apps.get_model(\"test_alflpd\", \"pony\").objects.create(\n            weight=4,\n        )\n        self.assertColumnExists(\"test_alflpd_pony\", \"weight\")\n        self.assertColumnType(\"test_alflpd_pony\", \"weight\", \"float\")\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_alflpd\", editor, project_state, new_state)\n        self.assertColumnExists(\"test_alflpd_pony\", \"weight\")\n        self.assertColumnType(\"test_alflpd_pony\", \"weight\", \"integer\")\n        # And deconstruction\n        definition = operation.deconstruct()\n        self.assertEqual(definition[0], \"AlterField\")\n        self.assertEqual(definition[1], [])\n        self.assertEqual(sorted(definition[2]), [\"field\", \"model_name\", \"name\"])\n", "def test_rename_model_with_multiple_names(self):\n    \"\"\"\n    RenameModel operations should handle models with multiple names.\n    \"\"\"\n    app_label = \"test_rename_multiple_names\"\n    project_state = self.apply_operations(\n        app_label,\n        ProjectState(),\n        operations=[\n            migrations.CreateModel(\n                \"Pony\",\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=20)),\n                ],\n            ),\n            migrations.CreateModel(\n                \"Rider\",\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\n                        \"pony\",\n                        models.ForeignKey(\"test_rename_multiple_names.Pony\", models.CASCADE),\n                    ),\n                ],\n            ),\n            migrations.CreateModel(\n                \"Rider2\",\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\n                        \"pony\",\n                        models.ForeignKey(\"test_rename_multiple_names.Pony\", models.CASCADE),\n                    ),\n                ],\n            ),\n        ],\n    )\n    Pony = project_state.apps.get_model(app_label, \"Pony\")\n    Rider = project_state.apps.get_model(app_label, \"Rider\")\n    Rider2 = project_state.apps.get_model(app_label, \"Rider2\")\n    pony = Pony.objects.create()\n    rider = Rider.objects.create(pony=pony)\n    rider2 = Rider2.objects.create(pony=pony)\n\n    project_state = self.apply_operations(\n        app_label,\n        project_state,\n        operations=[\n            migrations.RenameModel(\"Pony\", \"Pony2\"),\n        ],\n        atomic=connection.features.supports_atomic_references_rename,\n    )\n    Pony = project_state.apps.get_model(app_label, \"Pony2\")\n    Rider = project_state.apps.get_model(app_label, \"Rider\")\n    Rider2 = project_state.apps.get_model(app_label, \"Rider2\")\n    pony = Pony.objects.create()\n    rider = Rider.objects.create(pony=pony)\n    rider2 = Rider2.objects.create(pony=pony)\n    self.assertEqual(Pony.objects.count(), 2)\n    self.assertEqual(Rider.objects.count(), 2)\n    self.assertEqual(Rider2.objects.count(), 2)\n", "def test_alter_unique_together_to_partial_unique_constraint(self):\n    app_label = \"test_alutoptupc\"\n    project_state = self.set_up_test_model(app_label)\n    partial_unique_constraint = models.UniqueConstraint(\n        fields=[\"pink\"],\n        condition=models.Q(weight__gt=5),\n        name=\"test_constraint_pony_pink_for_weight_gt_5_uniq\",\n    )\n    table_name = f\"{app_label}_pony\"\n    unique_constraint_name = f\"{table_name}_pink_key\"\n    unique_together_constraint_name = f\"{table_name}_pink_694f3b9f_uniq\"\n    project_state = self.apply_operations(\n        app_label,\n        project_state,\n        [\n            migrations.AlterUniqueTogether(\n                \"Pony\",\n                [(\"pink\",)],\n            ),\n        ],\n    )\n    self.assertConstraintExists(table_name, unique_constraint_name, value=False)\n    self.assertConstraintNotExists(table_name, unique_together_constraint_name)\n\n    new_state = project_state.clone()\n    operation = migrations.AlterUniqueTogether(\"Pony\", set())\n    operation.state_forwards(app_label, new_state)\n    with connection.schema_editor() as editor:\n        operation.database_forwards(app_label, editor, project_state, new_state)\n    self.assertConstraintExists(table_name, unique_constraint_name, value=False)\n    self.assertConstraintNotExists(table_name, unique_together_constraint_name)\n\n    new_state = project_state.clone()\n    operation = migrations.AlterUniqueTogether(\"Pony\", {(\"pink\",)}, condition=\"weight>5\")\n    operation.state_forwards(app_label, new_state)\n    with connection.schema_editor() as editor:\n        operation.database_forwards(app_label, editor, project_state, new_state)\n    self.assertConstraintExists(unique_constraint_name, value=False)\n    self.assertConstraintNotExists(unique_together_constraint_name)\n"], "sample_1157": ["def test_implicit_multiplication_application_with_auto_number():\n    transformations = standard_transformations + \\\n                      (implicit_multiplication_application, auto_number)\n    x = Symbol('x')\n    y = Symbol('y')\n    assert parse_expr(\"2x y\", transformations=transformations) == 2*x*y\n    assert parse_expr(\"2.5x y\", transformations=transformations) == Float(2.5)*x*y\n    assert parse_expr(\"2.5 x y\", transformations=transformations) == Float(2.5)*x*y\n", "def test_implicit_application():\n    transformations = standard_transformations + (implicit_application,)\n    x = Symbol('x')\n    f = Function('f')\n    assert parse_expr(\"f x\", transformations=transformations) == f(x)\n    assert parse_expr(\"f x y\", transformations=transformations) == f(x, y)\n    assert parse_expr(\"(f x) y\", transformations=transformations) == f(x, y)\n    assert parse_expr(\"f x (y z)\", transformations=transformations) == f(x, y*z)\n    assert parse_expr(\"f (x y) z\", transformations=transformations) == f(x*y, z)\n", "def test_implicit_application_with_multiple_args():\n    transformations = standard_transformations + \\\n                      (implicit_multiplication_application,)\n    x = Symbol('x')\n    y = Symbol('y')\n    f = Function('f')\n    assert parse_expr(\"f x y\", transformations=transformations) == f(x, y)\n    assert parse_expr(\"f x+y\", transformations=transformations) == f(x+y)\n    assert parse_expr(\"f(x y)\", transformations=transformations) == f(x*y)\n    assert parse_expr(\"f(xy)\", transformations=transformations) == f(x*y)\n    assert parse_expr(\"f(2 3)\", transformations=transformations) == f(2, 3)\n    assert parse_expr(\"f(2, 3)\", transformations=transformations) == f(2, 3)\n", "def test_implicit_multiplication_application_with_greek_letters():\n    transformations = standard_transformations + \\\n                      (implicit_multiplication_application, )\n    alpha = Symbol('alpha')\n    beta = Symbol('beta')\n    x = Symbol('x')\n    y = Symbol('y')\n\n    # Test that implicit multiplication works with Greek letters\n    assert parse_expr(\"alpha x\", transformations=transformations) == alpha*x\n    assert parse_expr(\"alpha x y\", transformations=transformations) == alpha*x*y\n    assert parse_expr(\"alpha beta\", transformations=transformations) == alpha*beta\n    assert parse_expr(\"x alpha\", transformations=transformations) == x*alpha\n    assert parse_expr(\"x alpha y\", transformations=transformations) == x*alpha*y\n    assert parse_expr(\"x y alpha\", transformations=transformations) == x*y*alpha\n", "def test_implicit_multiplication_application():\n    transformations = standard_transformations + \\\n                      (implicit_multiplication_application,)\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n    sin = Function('sin')\n    cos = Function('cos')\n\n    # test implicit multiplication\n    assert parse_expr(\"2x\", transformations=transformations) == 2*x\n    assert parse_expr(\"3 y\", transformations=transformations) == 3*y\n    assert parse_expr(\"4(x+1)\", transformations=transformations) == 4*(x+1)\n    assert parse_expr(\"5 sin(x)\", transformations=transformations) == 5*sin(x)\n    assert parse_expr(\"6 cos(x)sin(y)\", transformations=transformations) == 6*cos(x)*sin(y)\n    assert parse_expr(\"7 (x+1)(x-1)\", transformations=transformations) == 7*(x+1)*(x-1)\n\n    # test implicit application\n    assert parse_expr(\"sin x\", transformations=transformations) == sin(x)\n    assert parse_expr(\"cos(x+1)sin(y)\", transformations=transformations) == cos(x+1)*sin(y)\n    assert parse_expr(\"sin(x+y)\", transformations=transformations) == sin(x+y)\n    assert parse_expr(\"sin(x+y)cos z\", transformations=transformations) == sin(x+y)*cos(z)\n"], "sample_947": ["def test_alias_rendering():\n    text = \"\"\".. c:alias:: A.B::C.D\n", "def test_attributes_on_members_and_typedefs():\n    # from issue # 1638\n    check('member', '__attribute__((always_inline)) int f', {1: 'f'})\n    check('type', '__attribute__((always_inline)) typedef int f', {1: 'f'})\n", "def test_symbol_lookup():\n    root = Symbol(None, None, None, None, None)\n    root._assert_invariants()\n\n    # test that adding a symbol works\n    name = ASTNestedName([ASTIdentifier(\"Test\")], rooted=False)\n    s = root._add_symbols(name, declaration=None, docname=None, line=None)\n    assert s\n    assert s.parent == root\n    assert s.ident == name.names[0]\n    assert s.declaration is None\n    assert s.docname is None\n    assert s.line is None\n\n    # test that we can find it back\n    s2 = root._find_first_named_symbol(ident=name.names[0],\n                                       matchSelf=True,\n                                       recurseInAnon=False)\n    assert s2 == s\n\n    # test that we can add a child\n    name2 = ASTNestedName([name.names[0], ASTIdentifier(\"Child\")], rooted=False)\n    s2 = root._add_symbols(name2, declaration=None, docname=None, line=None)\n    assert s2\n    assert s2.parent == s\n    assert s2.ident == name2.names[1]\n    assert s2.declaration is None\n    assert s2.docname is None\n    assert s2.line is None\n\n    # test that we can find it back\n    s3 = root._find_first_named_symbol(ident=name2.names[1],\n                                       matchSelf=True,\n                                       recurseInAnon=False)\n    assert s3 is None\n    s3 = root._find_first_named_symbol(ident=name2.names[1],\n                                       matchSelf=True,\n                                       recurseInAnon=True)\n    assert s3 == s2\n", "def test_incomplete_declaration():\n    # missing type\n    with pytest.raises(DefinitionError):\n        check('member', 'a')\n    # missing name\n    with pytest.raises(DefinitionError):\n        check('member', 'int ')\n    # missing name (trailing space)\n    with pytest.raises(DefinitionError):\n        check('member', 'int ;')\n    # missing name (trailing semicolon)\n    with pytest.raises(DefinitionError):\n        check('member', 'int ;')\n    # missing name (trailing semicolon)\n    with pytest.raises(DefinitionError):\n        check('function', 'void (')\n    # missing name (trailing semicolon)\n    with pytest.raises(DefinitionError):\n        check('function', 'void f(')\n    # missing name (trailing semicolon)\n    with pytest.raises(DefinitionError):\n        check('function', 'void f();')\n    # missing name (trailing semicolon)\n    with pytest.raises(DefinitionError):\n        check('function', 'void f(;)')\n\n    with pytest.raises(DefinitionError):\n        check('macro', 'M(')\n    with pytest.raises(DefinitionError):\n        check('macro', 'M(')\n", "def test_xref():\n    # test xref lookup\n    # just some simple cases for now\n\n    class Config:\n        c_id_attributes = [\"id_attr\"]\n        c_paren_attributes = [\"paren_attr\"]\n    rootSymbol = Symbol(None, None, None, None, None)\n    symbol = rootSymbol.add_declaration(parse('function', 'int f(int i)'), docname=\"TestDoc\", line=42)\n\n    # simple lookup\n    parser = DefinitionParser('f', location=None, config=Config())\n    name = parser.parse_xref_object()\n    parser.assert_end()\n    s = rootSymbol.find_declaration(name, 'function', matchSelf=True, recurseInAnon=True)\n    assert s is not None\n    assert s.declaration.objectType == 'function'\n\n    # lookup in inner scope\n    parser = DefinitionParser('f.i', location=None, config=Config())\n    name = parser.parse_xref_object()\n    parser.assert_end()\n    s = rootSymbol.find_declaration(name, 'functionParam', matchSelf=True, recurseInAnon=True)\n    assert s is not None\n    assert s.declaration.objectType == 'functionParam'\n\n    # lookup in outer scope\n    parser = DefinitionParser('f', location=None, config=Config())\n    name = parser.parse_xref_object()\n    parser.assert_end()\n    s = symbol.find_declaration(name, 'function', matchSelf=True, recurseInAnon=True)\n    assert s is not None\n    assert s.declaration.objectType == 'function'\n\n    # lookup failure\n    parser = DefinitionParser('f.i.j', location=None, config=Config())\n    name = parser.parse_xref_object()\n    parser.assert_end()\n    s = rootSymbol.find_declaration(name, 'functionParam', matchSelf=True, recurseInAnon=True)\n    assert s is None\n    s = symbol.find_declaration(name, 'functionParam', matchSelf=True, recurseInAnon=True)\n    assert s is None\n\n    # lookup in sibling scope\n    symbol2 = rootSymbol.add_declaration(parse('function', 'int g()'), docname=\"TestDoc\", line=42)\n    parser = DefinitionParser('f', location=None, config=Config())\n    name = parser.parse_xref_object()\n    parser.assert_end()\n    s = symbol2.find_declaration(name, 'function', matchSelf=True, recurseInAnon=True)\n    assert s is not None\n    assert s.declaration.objectType == 'function"], "sample_874": ["def test_clone():\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.pipeline import Pipeline\n    from sklearn.preprocessing import StandardScaler\n\n    # Test cloning of a simple estimator\n    estimator = LogisticRegression()\n    estimator_clone = clone(estimator)\n    assert estimator.get_params() == estimator_clone.get_params()\n\n    # Test cloning of a pipeline\n    pipeline = Pipeline([(\"scaler\", StandardScaler()), (\"logreg\", LogisticRegression())])\n    pipeline_clone = clone(pipeline)\n    assert pipeline.get_params() == pipeline_clone.get_params()\n\n    # Test cloning of a list of estimators\n    estimators = [LogisticRegression(), StandardScaler(), Pipeline([(\"scaler\", StandardScaler()), (\"logreg\", LogisticRegression())])]\n    estimators_clone = clone(estimators)\n    for estimator, estimator_clone in zip(estimators, estimators_clone):\n        assert estimator.get_params() == estimator_clone.get_params()\n", "def test_clone():\n    \"\"\"Test that cloning works correctly\"\"\"\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.decomposition import PCA\n\n    # Test cloning a single estimator\n    lr = LogisticRegression()\n    lr_clone = clone(lr)\n    assert lr != lr_clone\n    assert lr.get_params() == lr_clone.get_params()\n\n    # Test cloning a list of estimators\n    estimators = [LogisticRegression(), PCA()]\n    estimators_clone = clone(estimators)\n    assert estimators != estimators_clone\n    for est, est_clone in zip(estimators, estimators_clone):\n        assert est != est_clone\n        assert est.get_params() == est_clone.get_params()\n\n    # Test cloning a set of estimators\n    estimators = {LogisticRegression(), PCA()}\n    estimators_clone = clone(estimators)\n    assert estimators != estimators_clone\n    for est, est_clone in zip(estimators, estimators_clone):\n        assert est != est_clone\n        assert est.get_params() == est_clone.get_params()\n\n    # Test cloning a tuple of estimators\n    estimators = (LogisticRegression(), PCA())\n    estimators_clone = clone(estimators)\n    assert estimators != estimators_clone\n    for est, est_clone in zip(estimators, estimators_clone):\n        assert est != est_clone\n        assert est.get_params() == est_clone.get_params()\n\n    # Test cloning an estimator with a __sklearn_clone__ method\n    class MyEstimator(BaseEstimator):\n            return copy.deepcopy(self)\n\n    my_est = MyEstimator()\n    my_est_clone = clone(my_est)\n    assert my_est != my_est_clone\n    assert my_est.get_params() == my_est_clone.get_params()\n", "def test_clone():\n    # Test cloning a simple estimator\n    est = StepSelector(step=3)\n    est_clone = est.__sklearn_clone__()\n    assert est_clone.step == est.step\n\n    # Test cloning an estimator with a non-estimator parameter\n    est = StepSelector(step=3)\n    param = np.array([1, 2, 3])\n    est.param = param\n    est_clone = est.__sklearn_clone__()\n    assert est_clone.param is not param\n    assert_array_equal(est_clone.param, param)\n\n    # Test cloning an estimator with a sub-estimator\n    from sklearn.pipeline import Pipeline\n    est = Pipeline([(\"selector\", StepSelector(step=3))])\n    est_clone = est.__sklearn_clone__()\n    assert est_clone.steps[0][1].step == est.steps[0][1].step\n\n    # Test cloning an estimator that doesn't implement get_params\n    class DummyEstimator:\n        pass\n    est = DummyEstimator()\n    with pytest.raises(TypeError):\n        est.__sklearn_clone__()\n\n    # Test cloning an estimator that doesn't implement set_params\n    class DummyEstimator:\n            return {}\n    est = DummyEstimator()\n    with pytest.raises(TypeError):\n        est.__sklearn_clone__()\n", "def test_clone():\n    estimator = StepSelector(step=3)\n    clone = clone(estimator)\n\n    # Check if the cloned estimator is of the same class\n    assert isinstance(clone, StepSelector)\n\n    # Check if the cloned estimator has the same parameters\n    assert clone.get_params() == estimator.get_params()\n\n    # Check if the cloned estimator is not the same object\n    assert clone is not estimator\n\n    # Check if the cloned estimator behaves the same as the original\n    X = np.arange(20).reshape(2, 10)\n    Xt = np.arange(0, 20, 3).reshape(2, 4)\n    Xt_actual = clone.fit(X).transform(X)\n    assert_array_equal(Xt, Xt_actual)\n", "def test_clone():\n    \"\"\"Check that cloning an estimator returns a new, independent instance\"\"\"\n    # Create a simple estimator\n    class SimpleEstimator(BaseEstimator):\n            self.foo = foo\n\n            self.bar = X.shape[0]\n            return self\n\n    estimator = SimpleEstimator(\"original\")\n    estimator.fit(X, y)\n\n    # Clone the estimator\n    clone = clone(estimator)\n\n    # Check that the clone is a new instance\n    assert clone is not estimator\n\n    # Check that the clone has the same parameters\n    assert clone.foo == \"original\"\n\n    # Check that the clone has the same fitted attributes\n    assert clone.bar == estimator.bar\n\n    # Check that the clone is independent of the original\n    clone.bar = 42\n    assert clone.bar != estimator.bar\n\n    # Check that cloning a list of estimators works\n    estimators = [SimpleEstimator(\"foo\"), SimpleEstimator(\"bar\")]\n    clones = clone(estimators)\n    assert len(clones) == 2\n    assert clones[0] is not estimators[0]\n    assert clones[1] is not estimators[1]\n    assert clones[0].foo == \"foo\"\n    assert clones[1].foo == \"bar\"\n\n    # Check that cloning a tuple of estimators works\n    estimators = (SimpleEstimator(\"foo\"), SimpleEstimator(\"bar\"))\n    clones = clone(estimators)\n    assert len(clones) == 2\n    assert clones[0] is not estimators[0]\n    assert clones[1] is not estimators[1]\n    assert clones[0].foo == \"foo\"\n    assert clones[1].foo == \"bar\"\n\n    # Check that cloning a set of estimators works\n    estimators = {SimpleEstimator(\"foo\"), SimpleEstimator(\"bar\")}\n    clones = clone(estimators)\n    assert len(clones) == 2\n    assert clones != estimators\n    assert all(isinstance(clone, SimpleEstimator) for clone in clones)\n    assert {clone.foo for clone in clones} == {\"foo\", \"bar\"}\n\n    # Check that cloning an estimator with a custom __sklearn_clone__ method works\n    class CustomCloneEstimator(BaseEstimator):\n            return CustomCloneEstimator(\"cloned\")\n"], "sample_1005": ["def test_latex_ConditionSet():\n    x = Symbol('x')\n    assert latex(ConditionSet(x, x < 2, Interval(0, 3))) == \\\n        r\"\\left\\{x \\mid x \\in \\left[0, 3\\right) \\wedge x < 2 \\right\\}\"\n    assert latex(ConditionSet(x, x < 2, S.Reals)) == \\\n        r\"\\left\\{x \\mid x \\in \\mathbb{R} \\wedge x < 2 \\right\\}\"\n    assert latex(ConditionSet(x, x < 2, S.UniversalSet)) == \\\n        r\"\\left\\{x \\mid x < 2 \\right\\}\"\n", "compilation error", "def test_latex_nesting():\n    expr = ExpBase(exp(x+y), exp(x+y))\n    assert latex(expr) == r'\\left(e^{x + y}\\right)^{e^{x + y}}'\n    expr = ExpBase(exp(x+y), x+y)\n    assert latex(expr) == r'\\left(e^{x + y}\\right)^{x + y}'\n    expr = ExpBase(x+y, exp(x+y))\n    assert latex(expr) == r'\\left(x + y\\right)^{e^{x + y}}'\n    expr = ExpBase(x+y, x+y)\n    assert latex(expr) == r'\\left(x + y\\right)^{x + y}'\n    expr = ExpBase(exp(x+y), exp(x+y), evaluate=False)\n    assert latex(expr) == r'\\left(e^{x + y}\\right)^{e^{x + y}}'\n    expr = ExpBase(exp(x+y), x+y, evaluate=False)\n    assert latex(expr) == r'\\left(e^{x + y}\\right)^{x + y}'\n    expr = ExpBase(x+y, exp(x+y), evaluate=False)\n    assert latex(expr) == r'\\left(x + y\\right)^{e^{x + y}}'\n    expr = ExpBase(x+y, x+y, evaluate=False)\n    assert latex(expr) == r'\\left(x + y\\right)^{x + y}'\n", "def test_deal_with_super_sub():\n    assert latex(Symbol('xprime')) == r\"x'\"\n    assert latex(Symbol('xprime2')) == r\"x'^{2}\"\n    assert latex(Symbol('xsub2sup2')) == r\"x_{2}^{2}\"\n    assert latex(Symbol('xsub2sup')) == r\"x_{2}\"\n    assert latex(Symbol('xsubxsupx')) == r\"x_{x}^{x}\"\n    assert latex(Symbol('xsubxsupxprime')) == r\"x_{x}^{x}'\"\n    assert latex(Symbol('xsubxsupx2')) == r\"x_{x}^{x2}\"\n    assert latex(Symbol('xsubxsupxx')) == r\"x_{x}^{xx}\"\n", "def test_latex_tensor_derivatives():\n    from sympy.tensor import tensorhead, tensor_indices\n    from sympy.diffgeom import Manifold, Patch, CoordSystem\n    from sympy.diffgeom.diffgeom import TensorField, TensorProduct\n    from sympy.diffgeom.rn import R3\n\n    # Define a manifold\n    M = Manifold('M', 2)\n    P = Patch('P', M)\n    cs = CoordSystem('cs', P)\n    x, y = cs.coordinates\n\n    # Define a tensor field\n    v = TensorField(cs, {(0,): [1, 2], (1,): [3, 4]}, name='v')\n    v2 = TensorField(cs, {(0,): [5, 6], (1,): [7, 8]}, name='v2')\n\n    # Define a derivative\n    dv = v.diff(x)\n    dv2 = v.diff(y)\n    dvxy = v.diff(x, y)\n\n    # Test latex printing\n    assert latex(v) == r'v'\n    assert latex(dv) == r'\\frac{\\partial v}{\\partial x}'\n    assert latex(dv2) == r'\\frac{\\partial v}{\\partial y}'\n    assert latex(dvxy) == r'\\frac{\\partial^{2} v}{\\partial x\\partial y}'\n\n    # Test latex printing with TensorProduct\n    tv = TensorProduct(v, v2)\n    assert latex(tv) == r'v^{(2)}'\n    tdv = TensorProduct(dv, v2)\n    assert latex(tdv) == r'v^{(2)} \\frac{\\partial v}{\\partial x}'\n\n    # Test latex printing with contraction\n    tv_cont = TensorProduct(v, v2, (0, 1))\n    assert latex(tv_cont) == r'v^{(2)}_{v}'\n\n    # Test latex printing with tensorhead\n    vth = tensorhead(v, [tensor_indices('i', M)])\n    assert latex(vth) == r'v_{i}'\n\n    # Test latex printing with TensorField of a Riemannian manifold\n    R3 = Manifold('R3', 3)\n    R3_origin = Patch('origin', R3)\n    cartesian = CoordSystem('cartesian', R3_origin, ['x', 'y', 'z'])\n    v_R3 = TensorField"], "sample_1153": ["def test_periodic_argument_properties():\n    from sympy import periodic_argument, symbols\n    x = symbols('x')\n    p = symbols('p', positive=True)\n    assert periodic_argument(x, p).is_real is None\n    assert periodic_argument(x, p).is_extended_real is True\n    assert periodic_argument(x, p).is_rational is None\n    assert periodic_argument(x, p).is_positive is None\n    assert periodic_argument(x, p).is_nonnegative is None\n    assert periodic_argument(x, p).is_extended_positive is None\n    assert periodic_argument(x, p).is_extended_nonnegative is True\n    assert periodic_argument(x, p).is_extended_nonpositive is True\n\n    f = symbols('f', finite=True)\n    assert periodic_argument(f, p).is_real is True\n    assert periodic_argument(f, p).is_extended_real is True\n    assert periodic_argument(f, p).is_rational is None\n    assert periodic_argument(f, p).is_positive is None\n    assert periodic_argument(f, p).is_nonnegative is True\n    assert periodic_argument(f, p).is_extended_positive is None\n    assert periodic_argument(f, p).is_extended_nonnegative is True\n\n    z = symbols('z', complex=True, zero=False)\n    assert periodic_argument(z, p).is_real is True # since complex implies finite\n    assert periodic_argument(z, p).is_extended_real is True\n    assert periodic_argument(z, p).is_rational is None\n    assert periodic_argument(z, p).is_positive is True\n    assert periodic_argument(z, p).is_extended_positive is True\n    assert periodic_argument(z, p).is_zero is False\n\n    q = symbols('q', rational=True)\n    assert periodic_argument(q, p).is_real is True\n    assert periodic_argument(q, p).is_rational is True\n    assert periodic_argument(q, p).is_integer is None\n    assert periodic_argument(q, p).is_positive is None\n    assert periodic_argument(q, p).is_nonnegative is True\n\n    i = symbols('i', integer=True)\n    assert periodic_argument(i, p).is_real is True\n    assert periodic_argument(i, p).is_integer is True\n    assert periodic_argument(i, p).is_positive is None\n    assert periodic_argument(i, p).is_nonnegative is True\n", "def test_polarify_with_lifting():\n    from sympy import symbols, Function, Matrix\n    x = symbols('x')\n    f = Function('f')\n    assert polarify(f(x), lift=True) == (f(polar_lift(x)), {})\n    A = Matrix([[1, 2], [3, 4]])\n    assert polarify(A, lift=True) == (polar_lift(A), {})\n", "def test_periodic_argument_edge_cases():\n    from sympy import periodic_argument, exp_polar, oo, pi, I\n    x = Symbol('x')\n    p = Symbol('p', positive=True)\n\n    assert periodic_argument(exp_polar(0), oo) == 0\n    assert periodic_argument(exp_polar(0), p) == 0\n    assert periodic_argument(exp_polar(pi*I), oo) == pi\n    assert periodic_argument(exp_polar(pi*I), 2*pi) == pi\n    assert periodic_argument(exp_polar(pi*I), 3*pi) == pi\n    assert periodic_argument(exp_polar(-pi*I), oo) == -pi\n    assert periodic_argument(exp_polar(-pi*I), 2*pi) == -pi\n    assert periodic_argument(exp_polar(-pi*I), 3*pi) == 2*pi\n    assert periodic_argument(exp_polar(oo*I), oo) == oo\n    assert periodic_argument(exp_polar(-oo*I), oo) == -oo\n    assert periodic_argument(oo, oo) == oo\n    assert periodic_argument(-oo, oo) == -oo\n\n    assert periodic_argument(x, 0) is None\n    assert periodic_argument(x, -p) is None\n    assert periodic_argument(x, -oo) is None\n    assert periodic_argument(x, oo*I) is None\n    assert periodic_argument(x, I) is None\n    assert periodic_argument(x, -I) is None\n    assert periodic_argument(x, exp_polar(0)) is None\n", "def test_polar_lift():\n    assert polar_lift(0) == 0\n    assert polar_lift(1) == exp_polar(0)\n    assert polar_lift(-1) == exp_polar(I*pi)\n    assert polar_lift(2) == 2*exp_polar(0)\n    assert polar_lift(-2) == 2*exp_polar(I*pi)\n\n    # Test with symbols\n    x = Symbol('x')\n    a = Symbol('a', real=True)\n    p = Symbol('p', positive=True)\n\n    assert polar_lift(x) == x*exp_polar(0)\n    assert polar_lift(a) == a*exp_polar(0)\n    assert polar_lift(p) == p*exp_polar(0)\n\n    # Test with complex numbers\n    z = 1 + 2*I\n    assert polar_lift(z) == Abs(z)*exp_polar(I*arg(z))\n\n    # Test with polar_lift inside polar_lift\n    assert polar_lift(polar_lift(x)) == polar_lift(x)\n\n    # Test with non-comparable symbols\n    b = Symbol('b', real=False)\n    assert polar_lift(b).func == polar_lift\n    assert polar_lift(b).args == (b,)\n\n    # Test with non-comparable expressions\n    expr = x + y\n    assert polar_lift(expr).func == polar_lift\n    assert polar_lift(expr).args == (expr,)\n\n    # Test _eval_evalf\n    assert polar_lift(2)._eval_evalf(10) == 2\n", "def test_issue_17551():\n    # Test Abs with polar_lift and periodic_argument\n    from sympy import symbols, Abs, periodic_argument, polar_lift, exp_polar, pi\n    x, y = symbols('x y', real=True)\n    expr = Abs(periodic_argument(x + y*I, 2*pi))\n    assert expr == Abs(periodic_argument(x + y*I, 2*pi))\n    assert Abs(periodic_argument(polar_lift(x + y*I), 2*pi)) == \\\n        Abs(periodic_argument(x + y*I, 2*pi))\n    assert Abs(periodic_argument(exp_polar(2*pi*I)*polar_lift(x), 2*pi)) == \\\n        Abs(polar_lift(x))\n    assert Abs(periodic_argument(polar_lift(3*pi*I), 2*pi)) == \\\n        Abs(polar_lift(pi*I))\n"], "sample_924": ["def test_template_declarations():\n    check('class', 'template<> class {key}A : B', {2: \"IE1A\"})\n    check('class', 'template<> class {key}A : B, C', {2: \"IE1A\"})\n    check('class', 'template<> class {key}A : private B', {2: \"IE1A\"})\n    check('class', 'template<> class {key}A : public B', {2: \"IE1A\"})\n    check('class', 'template<> struct {key}A : B', {2: \"IE1A\"})\n    check('class', 'template<> struct {key}A : private B', {2: \"IE1A\"})\n    check('class', 'template<> struct {key}A : public B', {2: \"IE1A\"})\n    check('class', 'template<> union {key}A : B', {2: \"IE1A\"})\n    check('class', 'template<> union {key}A : private B', {2: \"IE1A\"})\n    check('class', 'template<> union {key}A : public B', {2: \"IE1A\"})\n    check('function', 'template<> void f()', {2: \"IE1fv\", 4: \"IE1fvv\"})\n    check('function', 'template<> void f() const', {2: \"IE1fv\", 4: \"IE1fvv\"})\n    check('function', 'template<> void f() volatile', {2: \"IE1fv\", 4: \"IE1fvv\"})\n    check('function', 'template<> void f() const volatile', {2: \"IE1fv\", 4: \"IE1fvv\"})\n\n    check('class', 'template<class> {key}A', {2: \"IE1A\"})\n    check('class', 'template<template<typename>> {key}A', {2: \"II0E1A\"})\n    check('class', 'template<int> {key}A', {2: \"I_iE1A\"})\n    check('class', 'template<int, int> {key}A', {2: \"II_iE1A\"})\n    check('class', 'template<int...> {key}A', {2: \"I_DpiE1A\"})\n    check('class', 'template<template<typename>", "def test_template_parameter_lists():\n    check('type', 'template<typename T> {key}A', {2: \"I0E1A\"})\n    check('type', 'template<typename T, typename U> {key}A', {2: \"I00E1A\"})\n    check('type', 'template<typename T, typename = int> {key}A', {2: \"I0E1A\"})\n    check('type', 'template<typename T, int = 0> {key}A', {2: \"I_iE1A\"})\n    check('type', 'template<typename T, int... = 0> {key}A', {2: \"I_DpiE1A\"})\n    check('type', 'template<typename T = int> {key}A', {2: \"I0E1A\"})\n    check('type', 'template<int... T> {key}A', {2: \"I_DpiE1A\"})\n    check('type', 'template<int = 0> {key}A', {2: \"I_iE1A\"})\n    check('type', 'template<int... = 0> {key}A', {2: \"I_DpiE1A\"})\n    check('type', 'template<template<typename> typename T> {key}A', {2: \"II0E0E1A\"})\n    check('type', 'template<template<typename> typename... T> {key}A', {2: \"II0EDpE1A\"})\n    check('type', 'template<template<typename> typename T = int> {key}A', {2: \"II0E0E1A\"})\n    check('type', 'template<template<typename> int... T> {key}A', {2: \"II0DpiE1A\"})\n    check('type', 'template<template<typename> int = 0> {key}A', {2: \"II0E0E1A\"})\n    check('type', 'template<template<typename> int... = 0> {key}A', {2: \"II0DpiE1A\"})\n    check('type', 'template<template<typename...> typename... T> {key}A', {2: \"IIDI0EDpE1A\"})\n    check('type', 'template<template<typename...> typename T> {key}A',", "def test_class_template_specializations():\n    check('class', 'template< int N > struct MyStaticArray{};',\n          {1: 'MyStaticArray', 2: 'I_iE10MyStaticArrayIiE'})\n    check('class', 'template<> struct MyStaticArray<42>{};',\n          {1: 'MyStaticArray<42>', 2: 'I_iE10MyStaticArrayILi42EE'})\n    check('class', 'template<int N> struct MyStaticArray{};',\n          {1: 'MyStaticArray', 2: 'I_iE10MyStaticArrayIiE'})\n    check('class', 'template<int N> struct MyStaticArray{N};',\n          {1: 'MyStaticArray', 2: 'I_iE10MyStaticArrayIiE'})\n    check('class', 'template<int N> struct MyStaticArray{N};',\n          {1: 'MyStaticArray', 2: 'I_iE10MyStaticArrayIiE'},\n          output='template<int N> struct MyStaticArray{N};')\n    check('class', 'template<int N> struct MyStaticArray{N};',\n          {1: 'MyStaticArray', 2: 'I_iE10MyStaticArrayIiE'})\n    check('class', 'template<int N> struct MyStaticArray{N};',\n          {1: 'MyStaticArray', 2: 'I_iE10MyStaticArrayIiE'},\n          output='template<int N> struct MyStaticArray{N};')\n\n    check('class', 'template<> struct MyStaticArray<1>{0};',\n          {1: 'MyStaticArray<1>', 2: 'I_iE10MyStaticArrayILi1EE'})\n    check('class', 'template<> struct MyStaticArray<-1>{0};',\n          {1: 'MyStaticArray<-1>', 2: 'I_iE10MyStaticArrayILi+1EE'})\n    check('class', 'template<> struct MyStaticArray<-1u>{0};',\n          {1: 'MyStaticArray<-1u>', 2: 'I_iE10MyStaticArrayILy1EE'})\n    check('class', 'template<> struct MyStaticArray<0x1>{0};',\n          {1: 'MyStaticArray<1>', 2: 'I_iE10MyStatic", "def test_lookups():\n    # setup\n    class Config:\n        cpp_id_attributes = [\"id_attr\"]\n        cpp_paren_attributes = [\"paren_attr\"]\n    env = Symbol(None, None, None, None, None, None)\n    env.config = Config()\n    env.temp_data = {}\n    env.ref_context = {}\n    rootSymbol = env.temp_data['cpp:parent_symbol'] = Symbol(None, None, None, None, None, None)\n    env.ref_context['cpp:parent_key'] = rootSymbol.get_lookup_key()\n\n    # actual test\n    # TODO: make some tests that could produce errors\n    # find identifier\n    parser = DefinitionParser(\"f\", location=None, config=env.config)\n    ast = parser.parse_nested_name()\n    s = rootSymbol.find_identifier(ast.identOrOp, matchSelf=False, recurseInAnon=True,\n                                   searchInSiblings=False)\n    assert s is None\n\n    # find a template name\n    parser = DefinitionParser(\"f<int>\", location=None, config=env.config)\n    ast = parser.parse_nested_name()\n    s = rootSymbol.find_identifier(ast.identOrOp, matchSelf=False, recurseInAnon=True,\n                                   searchInSiblings=False)\n    assert s is None\n\n    # find an operator\n    parser = DefinitionParser(\"f()\", location=None, config=env.config)\n    ast = parser.parse_nested_name()\n    s = rootSymbol.find_identifier(ast.identOrOp, matchSelf=False, recurseInAnon=True,\n                                   searchInSiblings=False)\n    assert s is None\n\n    # add a symbol and find it\n    symbol = rootSymbol.add_declaration(parse('function', 'void f()'), docname=\"TestDoc\")\n    s = rootSymbol.find_identifier(symbol.declaration.name.identOrOp, matchSelf=False,\n                                   recurseInAnon=True, searchInSiblings=False)\n    assert s == symbol\n    assert s.declaration.name == symbol.declaration.name\n\n    # add a symbol with an operator as name and find it\n    symbol = rootSymbol.add_declaration(parse('function', 'void operator()()'), docname=\"TestDoc\")\n    s = rootSymbol.find_identifier(symbol.declaration.name.identOrOp, matchSelf=False,\n                                   recurseInAnon=True, searchInSiblings=False)\n    assert s == symbol\n    assert s.declaration.name == symbol.declaration.name\n\n    # add two symbols and find the first\n    symbol = rootSymbol.add_declaration(parse", "def test_xref_to_template():\n    # test :cpp:any: and :any: with templates\n    # see https://en.cppreference.com/w/cpp/language/templates\n        assert target.startswith('{key}')\n        target = target[5:]\n        config = sphinx.domains.cpp.Config()\n        parser = DefinitionParser(target, location=None, config=config)\n        try:\n            ast, isShorthand = parser.parse_xref_object()\n            parser.assert_end()\n        except sphinx.domains.cpp.DefinitionError as e:\n            logger.warning(e, location=None)\n            ast = None\n        assert ast is not None\n        if expectRole is None:\n            expectRole = refRole\n        if expectDisplay is None:\n            expectDisplay = expectTarget\n        rootSymbol = sphinx.domains.cpp.Symbol(None, None, None, None, None, None)\n        parentSymbol = rootSymbol\n        if ast.objectType == 'namespace':\n            # since it's a shorthand we can't actually resolve it\n            symbols = None\n            failReason = None\n        else:\n            symbols, failReason = parentSymbol.find_name(ast.name, ast.templatePrefix,\n                                                        refRole, templateShorthand=True,\n                                                        matchSelf=True,\n                                                        recurseInAnon=True,\n                                                        searchInSiblings=False)\n            if symbols is None:\n                symbols = []\n        # check the xref\n        assert symbols is not None\n        declTyp = None\n        if len(symbols) > 0:\n            declTyp = symbols[0].declaration.objectType\n        if xrefRole.startswith('cpp:'):\n            xrefRole = xrefRole[4:]\n        # TODO: handle declaration xrefs correctly, for now just 'any'\n        if refRole != 'any':\n            refRole = 'any'\n        checkTarget = expectTarget\n        if refRole == 'func':\n            checkTarget += '()'\n        if xrefRole == 'any':\n            assert declTyp == 'templateParam'\n        elif xrefRole == 'function':\n            assert declTyp == 'function'\n        elif xrefRole == 'templateParam':\n            assert declTyp == 'templateParam'\n        if xrefRole == 'any':\n            assert symbols == []\n        else:\n            assert len(symbols) > 0\n\n    # template function\n    check('template <typename T> {"], "sample_308": ["    def test_date_format_S(self):\n        tests = [\n            (1, 'st'),\n            (2, 'nd'),\n            (3, 'rd'),\n            (4, 'th'),\n            (11, 'th'),\n            (12, 'th'),\n            (13, 'th'),\n            (21, 'st'),\n            (22, 'nd'),\n            (23, 'rd'),\n            (24, 'th'),\n        ]\n        for day, suffix in tests:\n            with self.subTest(day=day):\n                self.assertEqual(\n                    dateformat.format(datetime(2000, 1, day), 'S'),\n                    suffix,\n                )\n", "    def test_escaped_characters(self):\n        my_birthday = datetime(1979, 7, 8, 22, 00)\n        self.assertEqual(dateformat.format(my_birthday, r'\\a'), 'a')\n        self.assertEqual(dateformat.format(my_birthday, r'\\j'), '8')\n", "def test_S_formatting(self):\n    tests = [\n        (1, 'st'),\n        (2, 'nd'),\n        (3, 'rd'),\n        (4, 'th'),\n        (10, 'th'),\n        (11, 'th'),\n        (12, 'th'),\n        (13, 'th'),\n        (20, 'th'),\n        (21, 'st'),\n        (22, 'nd'),\n        (23, 'rd'),\n        (24, 'th'),\n        (31, 'st'),\n    ]\n    for day, expected_suffix in tests:\n        with self.subTest(day=day):\n            self.assertEqual(\n                dateformat.format(datetime(2000, 1, day), 'S'),\n                expected_suffix,\n            )\n", "def test_e_format(self):\n    # Test that the 'e' format returns the timezone name if it's available.\n    aware_dt = datetime(2022, 1, 1, tzinfo=utc)\n    self.assertEqual(dateformat.format(aware_dt, 'e'), 'UTC')\n\n    # Test that the 'e' format returns an empty string if the timezone name is not available.\n    naive_dt = datetime(2022, 1, 1)\n    self.assertEqual(dateformat.format(naive_dt, 'e'), '')\n\n    # Test that the 'e' format returns an empty string if the timezone is ambiguous.\n    ambiguous_dt = datetime(2022, 11, 6, 1, 30, tzinfo=get_default_timezone())\n    self.assertEqual(dateformat.format(ambiguous_dt, 'e'), '')\n", "def test_dateformat_with_escape_sequences(self):\n    my_birthday = datetime(1979, 7, 8, 22, 00)\n    self.assertEqual(dateformat.format(my_birthday, r'\\d \\i\\s \\a \\t \\e'), '08 00 p.m.')\n    self.assertEqual(dateformat.format(my_birthday, r'\\jS \\o\\f F'), '8th of July')\n    self.assertEqual(dateformat.format(my_birthday, r'\\m \\i\\s \\a \\t \\e'), '07 00 p.m.')\n"], "sample_232": ["    def test_key_transform_factory(self):\n        field = models.JSONField()\n        transform = field.get_transform('nonexistent')\n        self.assertIsInstance(transform, KeyTransformFactory)\n        transform = transform('key')\n        self.assertIsInstance(transform, KeyTransform)\n        self.assertEqual(transform.key_name, 'key')\n", "    def test_key_transform_factory(self):\n        value = {'a': 'b'}\n        obj = NullableJSONModel.objects.create(value=value)\n        qs = NullableJSONModel.objects.filter(value__a='b')\n        self.assertSequenceEqual(qs, [obj])\n", "    def test_key_transform(self):\n        key_transform = KeyTransform('test')\n        self.assertEqual(key_transform.output_field, None)\n", "    def setUpTestData(cls):\n        cls.obj = NullableJSONModel.objects.create(value={'key': 'Value'})\n", "    def test_invalid_json_string(self):\n        field = models.JSONField()\n        with self.assertRaisesMessage(ValidationError, 'Value must be valid JSON.'):\n            field.validate('Invalid JSON string', model_instance=None)\n"], "sample_610": ["def test_cftimeindex_repr_with_name():\n    \"\"\"Test that cftimeindex has name property in repr.\"\"\"\n    index = xr.cftime_range(start=\"2000\", periods=3, name=\"time\")\n    repr_str = index.__repr__()\n    assert f\"Name: time, \" in repr_str\n", "def test_shift_length_one_index(date_type):\n    index = CFTimeIndex([date_type(1, 1, 1)])\n    expected = CFTimeIndex([date_type(1, 1, 2)])\n    result = index.shift(1, \"D\")\n    assert result.equals(expected)\n    assert isinstance(result, CFTimeIndex)\n", "def test_cftimeindex_repr_with_name():\n    \"\"\"Test that cftimeindex.__repr__ is formatted with name.\"\"\"\n    index = xr.cftime_range(\"2000\", periods=3, name=\"my_cftimeindex\")\n    repr_str = index.__repr__()\n    assert \"name='my_cftimeindex'\" in repr_str\n", "def test_cftimeindex_repr_non_standard_calendar():\n    \"\"\"Test that non-standard calendar is included in CFTimeIndex.__repr__.\"\"\"\n    index = xr.cftime_range(\"2000\", periods=5, calendar=\"noleap\")\n    repr_str = index.__repr__()\n    assert \"calendar='noleap'\" in repr_str\n", "def test_cftimeindex_repr_has_freq():\n    index = xr.cftime_range(\"2000-01-01\", periods=3, freq=\"2MS\")\n    result = index.__repr__()\n    assert \"freq='2MS'\" in result\n"], "sample_455": ["def test_validate_constraint_error_code(self):\n    constraint = UniqueConstraintProduct._meta.constraints[0]\n    constraint.violation_error_code = \"custom_code\"\n    non_unique_product = UniqueConstraintProduct(\n        name=self.p1.name, color=self.p1.color\n    )\n    with self.assertRaises(ValidationError) as cm:\n        constraint.validate(UniqueConstraintProduct, non_unique_product)\n    self.assertEqual(cm.exception.code, \"custom_code\")\n", "def test_validate_condition_with_expressions(self):\n    p1 = UniqueConstraintConditionProduct.objects.create(name=\"p1\", color=\"red\")\n    constraint = UniqueConstraintConditionProduct._meta.constraints[0]\n    msg = \"Constraint \u201cname_without_color_uniq\u201d is violated.\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        constraint.validate(\n            UniqueConstraintConditionProduct,\n            UniqueConstraintConditionProduct(name=p1.name.lower(), color=None),\n        )\n    # Values not matching condition are ignored.\n    constraint.validate(\n        UniqueConstraintConditionProduct,\n        UniqueConstraintConditionProduct(name=p1.name, color=\"anything-but-none\"),\n    )\n    # Existing instances have their existing row excluded.\n    constraint.validate(UniqueConstraintConditionProduct, p1)\n    # Unique field is excluded.\n    constraint.validate(\n        UniqueConstraintConditionProduct,\n        UniqueConstraintConditionProduct(name=p1.name.lower(), color=None),\n        exclude={\"name\"},\n    )\n    # Field from a condition is excluded.\n    constraint.validate(\n        UniqueConstraintConditionProduct,\n        UniqueConstraintConditionProduct(name=p1.name.lower(), color=None),\n        exclude={\"color\"},\n    )\n", "def test_validate_condition_and_exclude(self):\n    p1 = UniqueConstraintConditionProduct.objects.create(name=\"p1\", color=\"red\")\n    constraint = UniqueConstraintConditionProduct._meta.constraints[0]\n    non_unique_product = UniqueConstraintConditionProduct(name=p1.name, color=None)\n    msg = \"Constraint \u201cname_without_color_uniq\u201d is violated.\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        constraint.validate(\n            UniqueConstraintConditionProduct,\n            non_unique_product,\n            exclude={\"name\", \"color\"},\n        )\n", "def test_validate_with_deconstruct(self):\n    name = \"unique_fields\"\n    constraint = models.UniqueConstraint(\n        Lower(\"title\"),\n        F(\"author\"),\n        name=name,\n    )\n    constraint_copy = constraint.clone()\n    self.assertEqual(\n        constraint_copy.constraint_sql(None, None),\n        constraint.constraint_sql(None, None),\n    )\n    self.assertEqual(\n        constraint_copy.create_sql(None, None),\n        constraint.create_sql(None, None),\n    )\n    self.assertEqual(\n        constraint_copy.remove_sql(None, None),\n        constraint.remove_sql(None, None),\n    )\n    self.assertEqual(\n        constraint_copy.validate(None, None),\n        constraint.validate(None, None),\n    )\n", "    def test_validate_expression_with_nullable_field(self):\n        constraint = models.UniqueConstraint(Lower(\"name\"), name=\"name_lower_uniq\")\n        # A unique constraint with expressions does not ignore nullable fields.\n        with self.assertRaises(ValidationError):\n            constraint.validate(\n                UniqueConstraintProduct,\n                UniqueConstraintProduct(name=None),\n            )\n        constraint.validate(UniqueConstraintProduct, UniqueConstraintProduct(name=\"\"))\n"], "sample_576": ["    def test_facet_pair_overlap(self, long_df):\n\n        p = Plot(long_df)\n        err = r\"Cannot facet the columns while pairing on `y`\"\n        with pytest.raises(RuntimeError, match=err):\n            p.facet(col=\"a\").pair(y=[\"x\", \"y\"])\n\n        err = r\"Cannot facet the rows while pairing on `x`\"\n        with pytest.raises(RuntimeError, match=err):\n            p.facet(row=\"a\").pair(x=[\"x\", \"y\"])\n", "    def test_constrained_layout(self, long_df):\n        if Version(mpl.__version__) < Version(\"3.3.0\"):\n            pytest.skip(\"constrained_layout requires matplotlib>=3.3\")\n\n        fig = Plot(long_df, x=\"x\", y=\"y\").add(MockMark()).layout(engine=\"constrained\").plot()._figure\n        assert fig.get_constrained_layout()\n\n        fig = Plot(long_df, x=\"x\", y=\"y\").add(MockMark()).layout(engine=\"tight\").plot()._figure\n        assert not fig.get_constrained_layout()\n", "    def test_mark_inference(self):\n\n        class MockMark(Mark):\n                return \"x\"\n\n        p = Plot(x=[1, 2, 3]).add(MockMark())\n        layer, = p._layers\n        assert layer[\"orient\"] == \"x\"\n", "    def test_identity_scale_with_non_numeric_values(self, long_df):\n\n        m = MockMark()\n        Plot(long_df, x=\"x\", y=\"x\").scale(x=\"identity\").add(m).plot()\n\n        assert_vector_equal(m.passed_data[0][\"x\"], long_df[\"x\"].astype(float))\n", "    def test_no_variables(self):\n\n        p = Plot().add(MockMark()).plot()\n        m, = p._layers\n        assert m[\"orient\"] is None\n"], "sample_724": ["def test_imputation_invalid_axis():\n    # Test imputation with an invalid axis\n    X = np.array([[1, 2], [np.nan, 4]])\n\n    imputer = Imputer(strategy=\"mean\", axis=2)\n    assert_raises(ValueError, imputer.fit, X)\n\n    imputer = Imputer(strategy=\"mean\", axis=-1)\n    assert_raises(ValueError, imputer.fit, X)\n\n    imputer = Imputer(strategy=\"mean\", axis=\"invalid\")\n    assert_raises(ValueError, imputer.fit, X)\n", "def test_imputation_all_missing_values():\n    # Test imputation when all values in a column are missing\n    X = np.array([\n        [np.nan, np.nan, 1],\n        [np.nan, np.nan, 2],\n        [np.nan, np.nan, 3]\n    ])\n\n    imputer = Imputer(strategy=\"mean\", axis=0)\n    with assert_raises(ValueError):\n        imputer.fit(X)\n\n    imputer = Imputer(strategy=\"median\", axis=0)\n    with assert_raises(ValueError):\n        imputer.fit(X)\n\n    imputer = Imputer(strategy=\"most_frequent\", axis=0)\n    with assert_raises(ValueError):\n        imputer.fit(X)\n", "def test_imputation_missing_values_as_string():\n    # Test that missing_values can be passed as a string \"NaN\" for np.nan\n    X = np.array([\n        [np.nan, 0, 0, 5],\n        [np.nan, 1, 0, 3],\n        [np.nan, 2, 0, 0],\n        [np.nan, 6, 0, 13],\n    ])\n\n    X_imputed_mean = np.array([\n        [3, 0, 0, 5],\n        [1, 1, 0, 3],\n        [2, 2, 0, 0],\n        [6, 6, 0, 13],\n    ])\n    statistics_mean = [np.nan, 3, np.nan, np.nan, 7]\n\n    _check_statistics(X, X_imputed_mean, \"mean\", statistics_mean, \"NaN\")\n", "def test_imputation_deprecated():\n    # Test deprecation warnings are raised when using Imputer.\n    import warnings\n\n    # Test that Imputer itself raises a deprecation warning\n    with warnings.catch_warnings(record=True) as w:\n        Imputer()\n        assert len(w) == 1\n        assert \"Imputer was deprecated\" in str(w[-1].message)\n\n    # Test that Imputer's fit method raises a deprecation warning\n    with warnings.catch_warnings(record=True) as w:\n        imputer = Imputer()\n        imputer.fit(np.array([[1, 2], [3, 4]]))\n        assert len(w) == 1\n        assert \"Imputer was deprecated\" in str(w[-1].message)\n\n    # Test that Imputer's transform method raises a deprecation warning\n    with warnings.catch_warnings(record=True) as w:\n        imputer = Imputer()\n        imputer.fit(np.array([[1, 2], [3, 4]]))\n        imputer.transform(np.array([[1, 2], [3, 4]]))\n        assert len(w) == 1\n        assert \"Imputer was deprecated\" in str(w[-1].message)\n\n    # Test that Imputer used in a pipeline raises a deprecation warning\n    from sklearn.pipeline import Pipeline\n    with warnings.catch_warnings(record=True) as w:\n        pipeline = Pipeline([('imputer', Imputer())])\n        pipeline.fit(np.array([[1, 2], [3, 4]]))\n        assert len(w) == 1\n        assert \"Imputer was deprecated\" in str(w[-1].message)\n", "def test_imputation_sparse_random():\n    # Test imputation with sparse random matrix\n    import scipy.sparse as sp\n    from sklearn.utils import check_random_state\n\n    random_state = check_random_state(0)\n\n    for axis in [0, 1]:\n        for strategy in [\"mean\", \"median\", \"most_frequent\"]:\n            X = sp.random(100, 100, density=0.1, random_state=random_state)\n            X[X.nonzero()] = np.nan\n\n            imputer = Imputer(strategy=strategy, axis=axis)\n            imputer.fit(X)\n            X_trans = imputer.transform(X)\n\n            X_non_missing = X.toarray()\n            X_non_missing[np.isnan(X_non_missing)] = 0\n\n            if axis == 0:\n                X_true = X_non_missing.copy()\n                if strategy == \"mean\":\n                    for i in range(X.shape[1]):\n                        X_true[:, i][X_non_missing[:, i] == 0] = np.nanmean(X_non_missing[:, i][X_non_missing[:, i] != 0])\n                elif strategy == \"median\":\n                    for i in range(X.shape[1]):\n                        X_true[:, i][X_non_missing[:, i] == 0] = np.nanmedian(X_non_missing[:, i][X_non_missing[:, i] != 0])\n                elif strategy == \"most_frequent\":\n                    for i in range(X.shape[1]):\n                        X_true[:, i][X_non_missing[:, i] == 0] = _most_frequent(X_non_missing[:, i][X_non_missing[:, i] != 0], 0, 0)\n            else:\n                X_true = X_non_missing.transpose().copy()\n                if strategy == \"mean\":\n                    for i in range(X.shape[0]):\n                        X_true[:, i][X_non_missing[:, i] == 0] = np.nanmean(X_non_missing[:, i][X_non_missing[:, i] != 0])\n                elif strategy == \"median\":\n                    for i in range(X.shape[0]):\n                        X_true[:, i][X_non_missing[:, i] == 0] = np.nanmedian(X_non_missing[:, i][X_non_missing[:, i] != 0])\n                elif strategy == \"most_frequent\":\n                    for i in range(X.shape[0]):\n                        X_true[:, i][X_non_missing[:, i] == 0] = _most_frequent(X_non_missing[:, i][X_non_missing"], "sample_242": ["    def test_get_bilateral_transforms(self):\n        transform = Transform(None, None)\n        self.assertEqual(transform.get_bilateral_transforms(), [])\n", "    def test_get_bound_params(self):\n        year_exact_lookup = YearExact(\n            lhs=Value(datetime(2010, 1, 1, 0, 0, 0), output_field=DateTimeField()),\n            rhs=Value(2010, output_field=DateTimeField()),\n        )\n        self.assertEqual(year_exact_lookup.get_bound_params(datetime(2010, 1, 1, 0, 0, 0), datetime(2010, 12, 31, 23, 59, 59)), (datetime(2010, 1, 1, 0, 0, 0), datetime(2010, 12, 31, 23, 59, 59)))\n", "    def test_get_bound_params(self):\n        look_up = YearExact(\n            lhs=Value(datetime(2010, 1, 1, 0, 0, 0), output_field=DateTimeField()),\n            rhs=Value(2010, output_field=DateTimeField()),\n        )\n        start = datetime(2010, 1, 1, 0, 0, 0)\n        finish = datetime(2010, 12, 31, 23, 59, 59)\n        self.assertEqual(look_up.get_bound_params(start, finish), (start, finish))\n", "    def test_process_rhs_with_multiple_values(self):\n        mixin = FieldGetDbPrepValueIterableMixin()\n        mixin.lhs = mock.Mock(output_field=mock.Mock(get_prep_value=lambda x: x))\n        mixin.rhs = [1, 2, 3]\n        compiler = mock.Mock()\n        connection = mock.Mock()\n        rhs_sql, params = mixin.process_rhs(compiler, connection)\n        self.assertEqual(rhs_sql, ('%s', '%s', '%s'))\n        self.assertEqual(params, (1, 2, 3))\n", "    def test_rhs_is_direct_value(self):\n        lookup = PatternLookup(lhs=Value('value'), rhs='rhs')\n        self.assertTrue(lookup.rhs_is_direct_value())\n\n        lookup = PatternLookup(lhs=Value('value'), rhs=Value('rhs'))\n        self.assertFalse(lookup.rhs_is_direct_value())\n"], "sample_842": ["def test_clone(estimator):\n    # Test that clone works correctly on estimators.\n    estimator_cloned = clone(estimator)\n\n    # Check that all parameters are equal.\n    assert estimator.get_params() == estimator_cloned.get_params()\n\n    # Check that the clone is not the same object.\n    assert id(estimator) != id(estimator_cloned)\n\n    # Check that cloning a cloned estimator produces an identical estimator.\n    estimator_cloned_cloned = clone(estimator_cloned)\n    assert estimator_cloned.get_params() == estimator_cloned_cloned.get_params()\n    assert id(estimator_cloned) != id(estimator_cloned_cloned)\n", "def test_clone_on_different_types():\n    # Test clone function with different types of estimators\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.cluster import KMeans\n    from sklearn.decomposition import PCA\n    from sklearn.pipeline import Pipeline\n    from sklearn.ensemble import RandomForestClassifier\n\n    estimators = [\n        LogisticRegression(),\n        KMeans(),\n        PCA(),\n        Pipeline([('pca', PCA()), ('logistic', LogisticRegression())]),\n        RandomForestClassifier()\n    ]\n\n    for estimator in estimators:\n        cloned = clone(estimator)\n        assert id(estimator) != id(cloned)\n        assert type(estimator) == type(cloned)\n        assert estimator.get_params() == cloned.get_params()\n", "def test_clone(estimator):\n    # Test that the clone function works correctly\n    clone_est = clone(estimator)\n    assert id(estimator) != id(clone_est)\n    assert type(estimator) == type(clone_est)\n\n    # Test that the clone function raises an error for non-estimators\n    with pytest.raises(TypeError):\n        clone(\"Not an estimator\")\n\n    # Test that the clone function raises an error for non-estimators with safe=False\n    with pytest.raises(TypeError):\n        clone(\"Not an estimator\", safe=False)\n\n    # Test that the clone function works correctly for estimators with safe=False\n    clone_est_safe_false = clone(estimator, safe=False)\n    assert id(estimator) != id(clone_est_safe_false)\n    assert type(estimator) == type(clone_est_safe_false)\n", "def test_clone():\n    # Test that cloning works correctly for BaseEstimator.\n    estimator = BaseEstimator()\n    estimator_clone = clone(estimator)\n\n    # Check that the cloned estimator has the same attributes as the original.\n    for attr in dir(estimator):\n        if not attr.startswith('__'):\n            assert hasattr(estimator_clone, attr)\n\n    # Check that the cloned estimator is not the same instance as the original.\n    assert id(estimator) != id(estimator_clone)\n\n    # Test that cloning a list of estimators works correctly.\n    estimators = [BaseEstimator() for _ in range(5)]\n    estimators_clone = clone(estimators)\n\n    # Check that the cloned estimators are not the same instance as the original.\n    for i in range(5):\n        assert id(estimators[i]) != id(estimators_clone[i])\n\n    # Test that cloning a tuple of estimators works correctly.\n    estimators_tuple = tuple(BaseEstimator() for _ in range(5))\n    estimators_tuple_clone = clone(estimators_tuple)\n\n    # Check that the cloned estimators are not the same instance as the original.\n    for i in range(5):\n        assert id(estimators_tuple[i]) != id(estimators_tuple_clone[i])\n\n    # Test that cloning a set of estimators works correctly.\n    estimators_set = {BaseEstimator() for _ in range(5)}\n    estimators_set_clone = clone(estimators_set)\n\n    # Check that the cloned estimators are not the same instance as the original.\n    for estimator, estimator_clone in zip(estimators_set, estimators_set_clone):\n        assert id(estimator) != id(estimator_clone)\n", "def test_get_params(estimator):\n    # Test that get_params returns a dictionary with parameter names as keys\n    params = estimator.get_params()\n    assert isinstance(params, dict)\n\n    # Test that all parameters are included\n    param_names = estimator._get_param_names()\n    assert set(param_names).issubset(set(params.keys()))\n\n    # Test that deep=False returns only the estimator's own parameters\n    params_shallow = estimator.get_params(deep=False)\n    assert set(params_shallow.keys()) == set(param_names)\n\n    # Test that get_params with deep=True returns all nested parameters\n    # (only applicable for estimators with nested parameters)\n    if hasattr(estimator, 'estimator'):\n        nested_params = estimator.estimator.get_params()\n        assert nested_params.items() <= params.items()\n"], "sample_1026": ["def test_lambdify_kwargs():\n    f = lambdify(x, x**2, dummify=True)\n    assert f(2) == 4\n    f = lambdify(x, x**2, printer=LambdaPrinter())\n    assert f(2) == 4\n    raises(ValueError, lambda: lambdify(x, x**2, unknown_kwarg=True))\n", "def test_lambdify_issue_16957():\n    x, y = symbols('x y')\n    f = lambdify((x,), [y])\n    raises(TypeError, lambda: f(1))\n", "def test_lambdify_ITE():\n    f = lambdify((x, y, z), ITE(x, y, z))\n    assert f(True, 1, 2) == 1\n    assert f(False, 1, 2) == 2\n    assert f(1, 1, 2) == 1\n    assert f(0, 1, 2) == 2\n    assert f(-1, 1, 2) == 2\n    assert f(x, y, z) == ITE(x, y, z)\n    f = lambdify(x, ITE(x, y, z))\n    assert f(True) == y\n    assert f(False) == z\n    assert f(1) == y\n    assert f(0) == z\n    assert f(-1) == z\n    assert f(x) == ITE(x, y, z)\n", "def test_issue_18551():\n    f = Function('f')\n    raises(ValueError, lambda: implemented_function('f', lambda x: x, strict=True))\n    raises(ValueError, lambda: implemented_function(f, lambda x: x, strict=True))\n    g = implemented_function('f', lambda x: x, strict=False)\n    assert g._imp_(4) == 4\n    h = implemented_function(f, lambda x: x, strict=False)\n    assert h._imp_(4) == 4\n", "def testtensorflow_function_signatures():\n    if not tensorflow:\n        skip(\"tensorflow not installed.\")\n\n    # test function with signature\n        return a + b\n    f = lambdify((x, y), my_func(x, y), modules=\"tensorflow\")\n    assert f(tensorflow.constant(1.0), tensorflow.constant(2.0)).eval(session=tensorflow.Session()) == 3.0\n\n    # test function without signature\n        return a + b\n    my_func.__signature__ = None\n    f = lambdify((x, y), my_func(x, y), modules=\"tensorflow\")\n    raises(AttributeError, lambda: f(tensorflow.constant(1.0), tensorflow.constant(2.0)).eval(session=tensorflow.Session()))\n\n    # test function with signature with kwonlyargs\n        return a + b + c\n    f = lambdify((x, y, z), my_func(x, y, c=z), modules=\"tensorflow\")\n    assert f(tensorflow.constant(1.0), tensorflow.constant(2.0), tensorflow.constant(3.0)).eval(session=tensorflow.Session()) == 6.0\n"], "sample_153": ["def test_database_checks_called_with_multiple_databases(self, mocked_check):\n    check_database_backends(databases=['default'])\n    self.assertEqual(mocked_check.call_count, 1)\n    mocked_check.reset_mock()\n    check_database_backends(databases=['other'])\n    self.assertEqual(mocked_check.call_count, 1)\n    mocked_check.reset_mock()\n    check_database_backends(databases=['default', 'other'])\n    self.assertEqual(mocked_check.call_count, 2)\n", "def test_check_model_name_db_lookup_clashes(self):\n    class Model(models.Model):\n        pass\n\n    with mock.patch('django.db.models.Model._check_model_name_db_lookup_clashes') as check_model_name_db_lookup_clashes:\n        Model._check_model_name_db_lookup_clashes()\n        check_model_name_db_lookup_clashes.assert_called_once()\n\n    class _Model(models.Model):\n        pass\n\n    with mock.patch('django.db.models.Model._check_model_name_db_lookup_clashes') as check_model_name_db_lookup_clashes:\n        _Model._check_model_name_db_lookup_clashes()\n        result = check_model_name_db_lookup_clashes.return_value\n        self.assertEqual(len(result), 1)\n        self.assertEqual(result[0].id, 'models.E023')\n\n    class Model_Model(models.Model):\n        pass\n\n    with mock.patch('django.db.models.Model._check_model_name_db_lookup_clashes') as check_model_name_db_lookup_clashes:\n        Model_Model._check_model_name_db_lookup_clashes()\n        result = check_model_name_db_lookup_clashes.return_value\n        self.assertEqual(len(result), 1)\n        self.assertEqual(result[0].id, 'models.E024')\n", "    def test_model_check_id_field(self):\n        class ModelWithInvalidIdField(Model):\n            id = IntegerField()\n            id_2 = IntegerField(primary_key=True)\n\n        with self.assertRaisesMessage(checks.Error, \"The model cannot have more than one field with 'primary_key=True'.\"):\n            ModelWithInvalidIdField.check()\n", "    def test_model_base_produces_unique_error_messages(self):\n        class TestModel(metaclass=ModelBase):\n            _meta = mock.Mock()\n            _meta.unique_together = (('field1', 'field2'),)\n            _meta.unique_error_message = 'Test error message'\n            _meta.object_name = 'TestModel'\n\n        model = TestModel()\n        error = model.unique_error_message(TestModel, ('field1', 'field2'))\n        self.assertEqual(error.message, 'Test error message')\n        self.assertEqual(error.params['model'], model)\n        self.assertEqual(error.params['model_name'], 'TestModel')\n        self.assertEqual(error.params['unique_check'], ('field1', 'field2'))\n", "def test_modelbase_check_clashes(self):\n    # Test that ModelBase's check() raises an error when two fields clash.\n    class TestModel(metaclass=ModelBase):\n        _meta = mock.Mock()\n        _meta.local_fields = [mock.Mock(name='clashing_field', attname='clashing_field')]\n        _meta.parents = {\n            mock.Mock(_meta=mock.Mock(local_fields=[mock.Mock(name='clashing_field', attname='clashing_field')])),\n        }\n\n    error = ModelBase._check_field_name_clashes(TestModel)\n    self.assertEqual(len(error), 1)\n    self.assertEqual(error[0].id, 'models.E005')\n    self.assertEqual(error[0].msg, \"The field 'clashing_field' from parent model \"\n                                   \"'<class 'unittest.test_modelbase_check_clashes.<locals>.TestModel'>\" \n                                   \"clashes with the field 'clashing_field' from parent model \"\n                                   \"'<class 'unittest.test_modelbase_check_clashes.<locals>.TestModel'>.\")\n\n    # Test that ModelBase's check() raises an error when two fields clash in a multi-table inheritance.\n    class ParentModel(metaclass=ModelBase):\n        _meta = mock.Mock()\n        _meta.local_fields = [mock.Mock(name='clashing_field', attname='clashing_field')]\n\n    class ChildModel(ParentModel, metaclass=ModelBase):\n        _meta = mock.Mock()\n        _meta.local_fields = [mock.Mock(name='clashing_field', attname='clashing_field')]\n\n    error = ModelBase._check_field_name_clashes(ChildModel)\n    self.assertEqual(len(error), 1)\n    self.assertEqual(error[0].id, 'models.E005')\n    self.assertEqual(error[0].msg, \"The field 'clashing_field' from parent model \"\n                                   \"'<class 'unittest.test_modelbase_check_clashes.<locals>.ParentModel'>\" \n                                   \"clashes with the field 'clashing_field' from parent model \"\n                                   \"'<class 'unittest.test_modelbase_check_clashes.<locals>.ChildModel'>.\")\n"], "sample_1056": ["def test_numexprprinter_functions():\n    printer = NumExprPrinter()\n    for func_name, numexpr_name in printer._numexpr_functions.items():\n        func = getattr(Expr, func_name, None)\n        if func is not None:\n            expr = func(x)\n            l = printer.doprint(expr)\n            assert l == f\"evaluate('{numexpr_name}(x)', truediv=True)\"\n        else:\n            assert hasattr(printer, f'_print_{func_name}')\n", "def test_lambda_printer_functions():\n    expr = sin(x)\n    assert LambdaPrinter()._print_Function(expr) == \"sin(x)\"\n    assert LambdaPrinter()._print_Function(sin(x)) == \"sin(x)\"\n    assert LambdaPrinter()._print_Function(cos(x)) == \"cos(x)\"\n    assert LambdaPrinter()._print_Function(tan(x)) == \"tan(x)\"\n    assert LambdaPrinter()._print_Function(asin(x)) == \"asin(x)\"\n    assert LambdaPrinter()._print_Function(acos(x)) == \"acos(x)\"\n    assert LambdaPrinter()._print_Function(atan(x)) == \"atan(x)\"\n    assert LambdaPrinter()._print_Function(atan2(x, y)) == \"atan2(x, y)\"\n    assert LambdaPrinter()._print_Function(sinh(x)) == \"sinh(x)\"\n    assert LambdaPrinter()._print_Function(cosh(x)) == \"cosh(x)\"\n    assert LambdaPrinter()._print_Function(tanh(x)) == \"tanh(x)\"\n    assert LambdaPrinter()._print_Function(asinh(x)) == \"asinh(x)\"\n    assert LambdaPrinter()._print_Function(acosh(x)) == \"acosh(x)\"\n    assert LambdaPrinter()._print_Function(atanh(x)) == \"atanh(x)\"\n    assert LambdaPrinter()._print_Function(ln(x)) == \"log(x)\"\n    assert LambdaPrinter()._print_Function(log(x)) == \"log(x)\"\n    assert LambdaPrinter()._print_Function(exp(x)) == \"exp(x)\"\n    assert LambdaPrinter()._print_Function(sqrt(x)) == \"sqrt(x)\"\n    assert LambdaPrinter()._print_Function(Abs(x)) == \"abs(x)\"\n    assert LambdaPrinter()._print_Function(conjugate(x)) == \"conjugate(x)\"\n    assert LambdaPrinter()._print_Function(im(x)) == \"im(x)\"\n    assert LambdaPrinter()._print_Function(re(x)) == \"re(x)\"\n", "def test_implemented_functions():\n    from sympy import sin\n    from sympy.abc import x\n    from sympy.functions.elementary.trigonometric import TrigonometricFunction\n\n    class MyTrigonometricFunction(TrigonometricFunction):\n        _imp_ = staticmethod(sin)\n\n    expr = MyTrigonometricFunction(x)\n    printer = NumExprPrinter()\n    assert printer._print_Function(expr) == \"sin(x)\"\n", "def test_numexpr_printer():\n    # Test that the NumExprPrinter raises an error for unsupported functions\n    expr = x * y + sin(x)\n    printer = NumExprPrinter()\n    assert printer.doprint(expr) == \"evaluate('(x*y + sin(x))', truediv=True)\"\n\n    expr = x * y + Matrix([[1, 2], [3, 4]])\n    raises(TypeError, lambda: printer.doprint(expr))\n\n    expr = x * y + Sum(x ** i, (i, a, b))\n    raises(TypeError, lambda: printer.doprint(expr))\n", "def test_numexpr_printer():\n    # Test NumExprPrinter\n    expr = sin(x) + cos(y)\n    numexpr_printer = NumExprPrinter()\n    assert numexpr_printer.doprint(expr) == \"evaluate('sin(x) + cos(y)', truediv=True)\"\n    \n    # Test _numexpr_functions\n    for func_name, numexpr_name in numexpr_printer._numexpr_functions.items():\n        expr = getattr(Expr, func_name)(x)\n        assert numexpr_printer._print_Function(expr) == f\"{numexpr_name}(x)\"\n\n    # Test unsupported function\n    expr = Expr(x)\n    raises(TypeError, lambda: numexpr_printer._print_Function(expr))\n\n    # Test _print_ImaginaryUnit\n    expr = 1j\n    assert numexpr_printer._print_ImaginaryUnit(expr) == \"1j\"\n\n    # Test blacklisted methods\n    expr = Matrix([[x, y], [y*x, z**2]])\n    raises(TypeError, lambda: numexpr_printer._print_SparseMatrix(expr))\n\n    # Test _print_seq\n    expr = (x, y, z)\n    assert numexpr_printer._print_seq(expr) == \"x, y, z\"\n"], "sample_1076": ["def test_Infinity():\n    prntr = PythonCodePrinter()\n    assert prntr._print_Infinity(oo) == \"float('inf')\"\n    assert prntr._print_NegativeInfinity(-oo) == \"float('-inf')\"\n    assert prntr._print_ComplexInfinity(zoo) == \"float('nan')\"\n", "def test_AbstractPythonCodePrinter_streams():\n    prntr = PythonCodePrinter()\n    assert prntr._print_Stream('stdout') == 'sys.stdout'\n    assert prntr.module_imports == {'sys': {'stdout'}}\n    assert prntr._print_Stream('stderr') == 'sys.stderr'\n    assert prntr.module_imports == {'sys': {'stdout', 'stderr'}}\n    assert prntr._print_Stream('other') == 'other'\n    assert prntr.module_imports == {'sys': {'stdout', 'stderr'}}\n", "def test_MatrixBase():\n    n = NumPyPrinter()\n    s = SymPyPrinter()\n    m = MpmathPrinter()\n    p = SciPyPrinter()\n    py = PythonCodePrinter()\n\n    # Test dense matrix\n    matrix = [[1, 2], [3, 4]]\n    assert n.doprint(matrix) == 'numpy.array([[1, 2], [3, 4]])'\n    assert s.doprint(matrix) == 'sympy.Matrix([[1, 2], [3, 4]])'\n    assert m.doprint(matrix) == 'mpmath.matrix([[1, 2], [3, 4]])'\n    assert p.doprint(matrix) == 'numpy.array([[1, 2], [3, 4]])'\n    assert py.doprint(matrix) == 'math.matrix([[1, 2], [3, 4]])'\n\n    # Test sparse matrix\n    matrix = SparseMatrix(2, 2, {(0, 0): 1, (1, 1): 2})\n    assert n.doprint(matrix) == 'numpy.array([[1, 0], [0, 2]])'\n    assert s.doprint(matrix) == 'sympy.SparseMatrix([[1, 0], [0, 2]])'\n    assert m.doprint(matrix) == 'mpmath.matrix([[1, 0], [0, 2]])'\n    assert p.doprint(matrix) == 'scipy.sparse.coo_matrix([1, 2], ([0, 1], [0, 1]), shape=(2, 2))'\n    assert py.doprint(matrix) == 'sympy.SparseMatrix([[1, 0], [0, 2]])'\n", "def test_AbstractPythonCodePrinter_einsum():\n    from sympy import symbols\n    from sympy.codegen.array_utils import CodegenArrayTensorProduct, CodegenArrayContraction\n    from sympy.tensor import IndexedBase\n\n    x, y = symbols('x y')\n    p = IndexedBase('p')\n    q = IndexedBase('q')\n\n    # Test basic einsum\n    expr = CodegenArrayTensorProduct([[p[0], 0], [q[1], 1]], [[0], [1]])\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.einsum(p[0], [0], q[1], [1])'\n\n    # Test contraction with different index positions\n    expr = CodegenArrayContraction(\n        CodegenArrayTensorProduct([[p[0], 0], [q[1], 1]], [[0], [1]]),\n        [[0], [1]]\n    )\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.einsum(p[0], [0], q[1], [1])'\n\n    # Test contraction with multiple indices at the same position\n    expr = CodegenArrayContraction(\n        CodegenArrayTensorProduct([[p[0], 0, 0], [q[1], 1, 1]], [[0, 0], [1, 1]]),\n        [[0, 0], [1, 1]]\n    )\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.einsum(p[0], [0, 0], q[1], [1, 1])'\n", "def test_PythonCodePrinter_functions():\n    prntr = PythonCodePrinter()\n\n    assert prntr.doprint(Expr().func) == 'sympy.FunctionClass'\n    assert prntr.doprint(Assignment(x, 2)) == 'x = 2'\n\n    # Test PythonCodePrinter _print_FunctionDefinition\n    from sympy.codegen.ast import FunctionDefinition, Variable\n    code_ast = FunctionDefinition(Variable('foo', 'int', '', None),\n                                 [Variable('x', 'int', '', None)],\n                                 [Assignment(Variable('x', 'int', '', None), 2)])\n    assert prntr._print_FunctionDefinition(code_ast) == 'def foo(x):\\n    x = 2'\n\n    # Test PythonCodePrinter _print_While\n    from sympy.codegen.ast import While\n    code_ast = While(x > 0, [Assignment(x, 2)])\n    assert prntr._print_While(code_ast) == 'while (x > 0):\\n    x = 2'\n\n    # Test PythonCodePrinter _print_Declaration\n    from sympy.codegen.ast import Declaration\n    code_ast = Declaration(Variable('x', 'int', '', 2))\n    assert prntr._print_Declaration(code_ast) == 'x = 2'\n\n    # Test PythonCodePrinter _print_Return\n    from sympy.codegen.ast import Return\n    code_ast = Return(2)\n    assert prntr._print_Return(code_ast) == 'return 2'\n\n    # Test PythonCodePrinter _print_Print\n    from sympy.codegen.ast import Print\n    code_ast = Print('test')\n    assert prntr._print_Print(code_ast) == \"print('test')\"\n\n    # Test PythonCodePrinter _print_Stream\n    from sympy.codegen.ast import stream\n    code_ast = stream.stdout\n    assert prntr._print_Stream(code_ast) == 'sys.stdout'\n\n    # Test PythonCodePrinter _print_NoneToken\n    from sympy.codegen.ast import none\n    code_ast = none\n    assert prntr._print_NoneToken(code_ast) == 'None'\n"], "sample_1057": ["def test_fully_qualified_modules():\n    ast = Print('x y'.split(), \"coordinate: %12.5g %12.5g\")\n    printer = PythonCodePrinter({'standard':'python3', 'fully_qualified_modules': True})\n    printer.doprint(ast)\n    assert render_as_module(ast, standard='python3') == \\\n        '\\nsympy\\n\\nprint(\"coordinate: %12.5g %12.5g\" % (sympy.Symbol(\"x\"), sympy.Symbol(\"y\")))'\n", "def test_module_imports():\n    ast = Print('x y'.split(), \"coordinate: %12.5g %12.5g\")\n    printer = PythonCodePrinter({'standard':'python3', 'fully_qualified_modules':True})\n    printer.doprint(ast)\n    assert render_as_module(ast, standard='python3') == \\\n        '\\n\\nprint(\"coordinate: %12.5g %12.5g\" % (x, y))'\n    \n    ast = Print('x y'.split(), \"coordinate: %12.5g %12.5g\")\n    printer = PythonCodePrinter({'standard':'python3', 'fully_qualified_modules':False})\n    printer.doprint(ast)\n    assert render_as_module(ast, standard='python3') == \\\n        '\\n\\nprint(\"coordinate: %12.5g %12.5g\" % (x, y))'\n", "def test_module_imports():\n    from sympy import sin, cos\n    ast = Print(['sin(x)', 'cos(x)'], \"Trigonometric functions: %12.5g %12.5g\")\n    result = render_as_module(ast, standard='python3')\n    expected = 'import math\\n\\nprint(\"Trigonometric functions: %12.5g %12.5g\" % (math.sin(x), math.cos(x)))'\n    assert result == expected\n\n    result = render_as_module(ast, standard='python2')\n    expected = 'import math\\n\\nprint \"Trigonometric functions: %12.5g %12.5g\" % (math.sin(x), math.cos(x))'\n    assert result == expected\n", "def test_fully_qualified_modules():\n    ast = Print('x y'.split(), \"coordinate: %12.5g %12.5g\")\n    printer = PythonCodePrinter({'standard':'python3', 'fully_qualified_modules': True})\n    printer.doprint(ast)\n    printer.module_imports = {'__future__': ['division']}\n    assert render_as_module(ast, standard='python3') == \\\n        'import __future__\\n\\nfrom __future__ import division\\n\\nprint(\"coordinate: %12.5g %12.5g\" % (x, y))'\n", "def test_rendering_with_imports():\n    from sympy import symbols, sin\n    x, y = symbols('x y')\n    ast = sin(x) + sin(y)\n    python_code = render_as_module(ast, standard='python3')\n    assert 'import sympy' in python_code\n    assert 'from sympy import sin' not in python_code\n    assert 'sin' in python_code\n\n    python_code_fully_qualified = render_as_module(ast, standard='python3_fully_qualified')\n    assert 'import sympy' in python_code_fully_qualified\n    assert 'from sympy import sin' in python_code_fully_qualified\n    assert 'sympy.sin' in python_code_fully_qualified\n"], "sample_196": ["def test_adapt_decimalfield_value(self):\n    value = decimal.Decimal('3.14')\n    max_digits = 5\n    decimal_places = 2\n    self.assertEqual(\n        self.ops.adapt_decimalfield_value(value, max_digits, decimal_places),\n        utils.format_number(value, max_digits, decimal_places)\n    )\n", "def test_force_no_ordering(self):\n    self.assertEqual(self.ops.force_no_ordering(), [])\n", "    def test_last_insert_id(self):\n        # Create a table with an auto-incrementing ID.\n        with self.settings(DATABASES={\n            'default': {\n                'ENGINE': 'django.db.backends.sqlite3',\n                'NAME': ':memory:',\n            }\n        }):\n            with transaction.atomic():\n                from django.db import models\n                class AutoModel(models.Model):\n                    id = models.AutoField(primary_key=True)\n                AutoModel._meta.db_table = 'test_table'\n                with connection.cursor() as cursor:\n                    cursor.execute('''\n                        CREATE TABLE {} (\n                            id INTEGER PRIMARY KEY AUTOINCREMENT\n                        )\n                    '''.format(AutoModel._meta.db_table))\n                    cursor.execute('''\n                        INSERT INTO {} DEFAULT VALUES\n                    '''.format(AutoModel._meta.db_table))\n                    self.assertEqual(\n                        connection.ops.last_insert_id(cursor, AutoModel._meta.db_table, 'id'),\n                        1\n                    )\n", "    def test_adapt_decimalfield_value(self):\n        value = decimal.Decimal('123.456')\n        self.assertEqual(\n            self.ops.adapt_decimalfield_value(value),\n            utils.format_number(value)\n        )\n", "def test_adapt_decimalfield_value(self):\n    decimal_value = decimal.Decimal('123.45')\n    self.assertEqual(\n        self.ops.adapt_unknown_value(decimal_value),\n        self.ops.adapt_decimalfield_value(decimal_value)\n    )\n"], "sample_1106": ["def test_matadd_doit():\n    A = MatrixSymbol('A', 5, 5)\n    B = MatrixSymbol('B', 5, 5)\n    C = MatrixSymbol('C', 5, 5)\n    expr = MatAdd(A, B, C)\n    assert expr.doit().args == (A, B, C)\n    assert expr.doit(deep=False).args == (A, B, C)\n    assert expr.doit(deep=True).args == (A, B, C)\n", "def test_matadd_doit():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', n, m)\n    C = MatrixSymbol('C', n, m)\n    assert MatAdd(A, B).doit().func == MatAdd\n    assert MatAdd(A, B, C).doit().func == MatAdd\n    assert MatAdd(A, 2*B, C).doit().func == MatAdd\n    assert MatAdd(2*A, B, C).doit().func == MatAdd\n", "def test_matadd_doit():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = Matrix([[1, 0], [0, 1]])\n    X = MatAdd(A, B, C)\n    assert X.doit().shape == (2, 2)\n    assert X.doit(deep=False).shape == (2, 2)\n", "def test_matadd():\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n    Z = MatrixSymbol('Z', 2, 2)\n    \n    # Test MatAdd with multiple matrix symbols\n    assert MatAdd(X, Y, Z) == MatAdd(X, Y, Z)\n    \n    # Test MatAdd with a scalar\n    assert MatAdd(2, X) == 2 + X\n    \n    # Test MatAdd with a zero matrix\n    assert MatAdd(X, ZeroMatrix(2, 2)) == X\n    \n    # Test MatAdd with a matrix and its transpose\n    assert MatAdd(X, X.T) == MatAdd(X, Transpose(X))\n    \n    # Test MatAdd with a matrix and its adjoint\n    assert MatAdd(X, X.H) == MatAdd(X, adjoint(X))\n    \n    # Test MatAdd with a matrix and a constant matrix\n    M = Matrix([[1, 2], [3, 4]])\n    assert MatAdd(X, M) == MatAdd(X, M)\n    \n    # Test MatAdd with multiple constant matrices\n    N = Matrix([[5, 6], [7, 8]])\n    assert MatAdd(M, N) == MatAdd(M, N)\n", "def test_matadd_canonicalize():\n    A = MatrixSymbol('A', 2, 2)\n    B = Matrix([[1, 2], [3, 4]])\n    C = Matrix([[1, 1], [1, 1]])\n    D = MatrixSymbol('D', 2, 2)\n    X = MatAdd(A, B, C, D)\n    canonicalized_X = canonicalize(X)\n    assert isinstance(canonicalized_X, MatAdd)\n    assert canonicalized_X.args == (A, D, B + C)\n"], "sample_1088": ["def test_symmetrize_edge_cases():\n    assert symmetrize(x**2 + y**2, x, y, formal=True) == \\\n        (x**2 + 2*x*y + y**2, 0, [(x + y, x + y), (x*y, x*y)])\n\n    assert symmetrize(x + y + z, x, y, z, formal=True) == \\\n        (x + y + z, 0, [(x + y + z, x + y + z)])\n\n    assert symmetrize(x**3 + y**3 + z**3, x, y, z) == \\\n        (x**3 + y**3 + z**3 - 3*x*y*z, 3*x*y*z)\n\n    assert symmetrize(x**3 + y**3 + z**3, x, y, z, formal=True) == \\\n        ((x + y + z)*(x**2 + y**2 + z**2 - x*y - x*z - y*z), 3*x*y*z,\n         [(x + y + z, x + y + z), (x*y + x*z + y*z, x*y + x*z + y*z)])\n\n    assert symmetrize([x**2 + y**2, x + y], x, y, formal=True) == \\\n        [((x + y)**2, 0, [(x + y, x + y)]),\n         (x + y, 0, [(x + y, x + y)])]\n", "def test_symmetrize_formal_symbols():\n    s1 = x + y + z\n    s2 = x*y + x*z + y*z\n    U = [u0, u1, u2] = symbols('u:3')\n    assert symmetrize(x + 1, x, y, z, formal=True, symbols=U) == \\\n        (u0 + 1, -y - z, [(u0, x + y + z), (u1, x*y + x*z + y*z), (u2, x*y*z)])\n    assert symmetrize(x**2, x, y, z, formal=True, symbols=U) == \\\n        (u0**2 - 2*u1, -y**2 - z**2, [(u0, x + y + z), (u1, x*y + x*z + y*z), (u2, x*y*z)])\n    assert symmetrize(x**2 + y**2, x, y, formal=True, symbols=U) == \\\n        (u0**2 - 2*u1, 0, [(u0, x + y), (u1, x*y)])\n", "def test_symmetrize_edge_cases():\n    assert symmetrize(1, x) == (1, 0)\n    assert symmetrize(x, x, x) == (3*x, 0)\n    assert symmetrize(x*y, x, y) == (x*y, 0)\n    assert symmetrize(x**2*y**2, x, y, z) == ((x + y + z)**2 - 2*(x*y + x*z + y*z), 0)\n\n    assert symmetrize([], x, y) == []\n    assert symmetrize([1, x, x**2], x) == [(1, 0), (x, 0), (x**2, 0)]\n    assert symmetrize([x + y, x - y], x, y) == [(x + y, 0), (x + y, -2*y)]\n\n    raises(ValueError, lambda: symmetrize([1, 2, 3], x, y, z, formal=True, symbols=[]))\n    raises(OptionError, lambda: symmetrize(x, x, y, invalid_option=True))\n", "def test_symmetrize_edge_cases():\n    assert symmetrize(x**2 + y**3, x, y, formal=True) == \\\n        (s1**2, -s2**3 + 3*s1*s3 - 3*s4, [(s1, x + y), (s2, x*y), (s3, x**2*y + x*y**2), (s4, x**2*y**2)])\n    assert symmetrize(x**2 + y**3, x, y, z, formal=True) == \\\n        (s1**2, -s2**3 + 3*s1*s3 - 3*s4 + 3*s2*s5 - 6*s6, [(s1, x + y), (s2, x*y), (s3, x**2*y + x*y**2), (s4, x**2*y**2), (s5, x*z + y*z), (s6, x**2*y*z + x*y**2*z)])\n\n    assert symmetrize(x**2 + y**3, x, y, z, formal=True, symbols=[a, b, c, d, e, f]) == \\\n        (a**2, -b**3 + 3*a*c - 3*d + 3*b*e - 6*f, [(a, x + y), (b, x*y), (c, x**2*y + x*y**2), (d, x**2*y**2), (e, x*z + y*z), (f, x**2*y*z + x*y**2*z)])\n\n    assert symmetrize(x**2 + y**3, x, y, z, formal=True, symbols='t') == \\\n        (t0**2, -t1**3 + 3*t0*t2 - 3*t3 + 3*t1*t4 - 6*t5, [(t0, x + y), (t1, x*y), (t2, x**2*y + x*y**2), (t3, x**2*y**2), (t4, x*z + y*z), (t5, x**2*y*z + x*y**2*z)])\n\n    assert symmetrize([x**2 + y**3, x**2 + y**2]) ==", "def test_viete_negative_degrees():\n    r1, r2 = symbols('r1, r2')\n\n    raises(ValueError, lambda: viete(1/a*x**(-2) + b*x**(-1) + c, [r1, r2], x))\n    raises(ValueError, lambda: viete(1/a*x**(1/2) + b*x**(3/4) + c, [r1, r2], x))\n    raises(ValueError, lambda: viete(a*x**2 + b*x + c, [r1, r2, r1], x))\n"], "sample_1068": ["def test_octave_Indexed_printing():\n    from sympy.tensor import IndexedBase, Idx\n    i = Idx('i', 3)\n    j = Idx('j', 3)\n    A = IndexedBase('A', shape=(3, 3))\n    B = IndexedBase('B', shape=(3, 3))\n    expr = A[i, j] + 2*B[i, j]\n    assert mcode(expr) == \"A(i, j) + 2*B(i, j)\"\n", "def test_octave_matrix_with_function():\n    A = Matrix([[sin(x/2), 2, ceiling(x)],\n                [0, 1, exp(1)],\n                [0, log(x), 3]])\n    expected = \"[sin(x/2) 2 ceil(x); 0 1 exp(1); 0 log(x) 3]\"\n    assert mcode(A) == expected\n", "def test_power_simplifications():\n    assert mcode(x**0) == \"1\"\n    assert mcode(x**1) == \"x\"\n    assert mcode(x**-1) == \"1./x\"\n    assert mcode(x**(S(1)/2)) == \"sqrt(x)\"\n    assert mcode(x**(S(1)/3)) == \"x.^(1/3)\"\n    assert mcode(x**(S(2)/3)) == \"x.^(2/3)\"\n    assert mcode(x**-S.Half) == \"1./sqrt(x)\"\n    assert mcode(x**(S(-1)/3)) == \"1./x.^(1/3)\"\n    assert mcode(x**(S(-2)/3)) == \"1./x.^(2/3)\"\n    assert mcode(x**(S(1)/4)) == \"x.^(1/4)\"\n    assert mcode(x**(S(3)/4)) == \"x.^(3/4)\"\n    assert mcode(x**(S(2)/4)) == \"sqrt(x)\"\n    assert mcode(x**(S(-1)/4)) == \"1./x.^(1/4)\"\n    assert mcode(x**(S(-3)/4)) == \"1./x.^(3/4)\"\n    assert mcode(x**(S(-2)/4)) == \"1./sqrt(x)\"\n", "def test_octave_indexed():\n    from sympy import IndexedBase, Idx\n    x = IndexedBase('x')\n    i = Idx('i', 5)\n    e = x[i]\n    assert mcode(e) == \"x(i)\"\n    assert mcode(e, assign_to='y') == \"y = x(i);\"\n    assert mcode(x[i]*y) == \"x(i).*y\"\n    assert mcode(x[i]**2) == \"x(i).^2\"\n    assert mcode(x[i] + y) == \"x(i) + y\"\n    assert mcode(x[i] - y) == \"x(i) - y\"\n    assert mcode(-x[i]) == \"-x(i)\"\n    assert mcode(x[i]/y) == \"x(i)./y\"\n    assert mcode(x[i]*y/z) == \"x(i).*y./z\"\n    assert mcode(x[i]/(y*z)) == \"x(i)./(y.*z)\"\n    assert mcode((x[i]+y)/z) == \"(x(i) + y)./z\"\n    assert mcode((x[i]+y)/(z+x[i])) == \"(x(i) + y)./(x(i) + z)\"\n", "def test_octave_max_min_of_matrices():\n    # Check that Max/Min are element-wise operations when given matrices\n    A = Matrix([[1, 2, 3], [4, 5, 6]])\n    B = Matrix([[7, 8, 9], [10, 11, 12]])\n    C = Matrix([[13, 14, 15], [16, 17, 18]])\n    assert mcode(Max(A, B)) == (\n        \"[7 8 9; 10 11 12]\"\n    )\n    assert mcode(Min(A, B)) == (\n        \"[1 2 3; 4 5 6]\"\n    )\n    assert mcode(Max(A, B, C)) == (\n        \"[13 14 15; 16 17 18]\"\n    )\n    assert mcode(Min(A, B, C)) == (\n        \"[1 2 3; 4 5 6]\"\n    )\n"], "sample_973": ["def test_isabstractmethod():\n    class AbstractClass:\n            pass\n\n    class ConcreteClass(AbstractClass):\n            pass\n\n    class AbstractBaseClass(metaclass=abc.ABCMeta):\n        @abc.abstractmethod\n            pass\n\n    class ConcreteSubclass(AbstractBaseClass):\n            pass\n\n    assert inspect.isabstractmethod(AbstractBaseClass.method) is True\n    assert inspect.isabstractmethod(ConcreteSubclass.method) is False\n    assert inspect.isabstractmethod(AbstractClass.method) is False\n    assert inspect.isabstractmethod(ConcreteClass.method) is False\n", "def test_isabstractmethod():\n    class AbstractClass:\n            raise NotImplementedError\n\n            pass\n\n    abstract_method = AbstractClass.abstract_method\n    concrete_method = AbstractClass.concrete_method\n\n    assert inspect.isabstractmethod(abstract_method) is True\n    assert inspect.isabstractmethod(concrete_method) is False\n\n    class SubClass(AbstractClass):\n            pass\n\n    subclass_abstract_method = SubClass.abstract_method\n    assert inspect.isabstractmethod(subclass_abstract_method) is False\n\n    class AbstractClassWithMetaclass(metaclass=abc.ABCMeta):\n        @abc.abstractmethod\n            pass\n\n            pass\n\n    abstract_method = AbstractClassWithMetaclass.abstract_method\n    concrete_method = AbstractClassWithMetaclass.concrete_method\n\n    assert inspect.isabstractmethod(abstract_method) is True\n    assert inspect.isabstractmethod(concrete_method) is False\n\n    class SubClass(AbstractClassWithMetaclass):\n            pass\n\n    subclass_abstract_method = SubClass.abstract_method\n    assert inspect.isabstractmethod(subclass_abstract_method) is False\n", "def test_isabstractmethod():\n    class AbstractClass:\n            pass\n\n        abstract_method.__isabstractmethod__ = True\n\n    class ConcreteClass:\n            pass\n\n    assert inspect.isabstractmethod(AbstractClass.abstract_method) is True\n    assert inspect.isabstractmethod(ConcreteClass.concrete_method) is False\n    assert inspect.isabstractmethod(ConcreteClass().concrete_method) is False\n    assert inspect.isabstractmethod(AbstractClass().abstract_method) is False\n    assert inspect.isabstractmethod(object()) is False\n    assert inspect.isabstractmethod(lambda x: x) is False\n    assert inspect.isabstractmethod(123) is False\n    assert inspect.isabstractmethod(None) is False\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        pass\n\n    assert inspect.is_singledispatch_function(fun) is True\n\n        pass\n\n    assert inspect.is_singledispatch_function(non_singledispatch_func) is False\n\n", "def test_get_type_hints_handling():\n    from typing import Union, NewType\n    import inspect\n\n    # Test case to improve coverage for the get_type_hints function handling\n    class NewTypeTest:\n            pass\n\n            pass\n\n    # Test for NewType\n    sig = inspect.signature(NewTypeTest.__init__)\n    type_hints = inspect.get_type_hints(NewTypeTest.__init__)\n    assert 'NewTypeTestType' in str(sig.parameters['value'].annotation)\n    assert 'NewType' in str(type_hints['value'])\n\n    # Test for Union\n    sig = inspect.signature(NewTypeTest.method_with_union)\n    type_hints = inspect.get_type_hints(NewTypeTest.method_with_union)\n    assert 'Union[str, int]' in str(sig.parameters['value'].annotation)\n    assert 'Union' in str(type_hints['value'])\n"], "sample_1154": ["def test__linsolve_repeated():\n    # Test repeated equations\n    eqs = [\n        x - 1,\n        x - 1,\n        x - 1,\n    ]\n    assert _linsolve(eqs, [x]) == {x:1}\n\n    # Test repeated equations with different rhs\n    eqs = [\n        x - 1,\n        x - 2,\n        x - 3,\n    ]\n    assert _linsolve(eqs, [x]) is None\n\n    # Test repeated equations with different coefficients\n    eqs = [\n        x - 1,\n        2*x - 2,\n        3*x - 3,\n    ]\n    assert _linsolve(eqs, [x]) == {x:1}\n", "def test__linsolve_underdetermined():\n    # Test that underdetermined system returns free variables\n    assert _linsolve([x - y], [x, y, z]) == {x: y, y: y, z: z}\n    assert _linsolve([x + 2*y - z], [x, y, z]) == {x: z - 2*y, y: y, z: z}\n    assert _linsolve([Eq(x + y, z), Eq(x - y, 2*z)], [x, y, z]) == {x: 3*z/2, y: z/2, z: z}\n\n    # Test that underdetermined system with no free variables\n    # is handled correctly.\n    assert _linsolve([x, y, z], [x, y, z]) == {x: 0, y: 0, z: 0}\n\n    # Test that an empty system of equations returns a solution\n    # with all variables being free variables\n    assert _linsolve([], [x, y, z]) == {x: x, y: y, z: z}\n", "def test__linsolve_edge_cases():\n    # Test with complex coefficients\n    eqs = [\n        Eq(x + 2*y, 1 + I),\n        Eq(2*x - y, 3 - 2*I)\n    ]\n    sol = _linsolve(eqs, [x, y])\n    assert sol == {x: 11/5 - 3/5*I, y: 1/5 + 4/5*I}\n\n    # Test with negative coefficients\n    eqs = [\n        Eq(-x - y, -1),\n        Eq(-x + y, 2)\n    ]\n    sol = _linsolve(eqs, [x, y])\n    assert sol == {x: -3/2, y: 5/2}\n\n    # Test with fraction coefficients\n    eqs = [\n        Eq(1/2*x + 1/3*y, 1/2),\n        Eq(1/3*x - 1/4*y, 1/3)\n    ]\n    sol = _linsolve(eqs, [x, y])\n    assert sol == {x: 13/25, y: 17/25}\n\n    # Test with multiple variables and multiple equations\n    eqs = [\n        Eq(x + y + z, 1),\n        Eq(x - y + z, 2),\n        Eq(x + y - z, 3)\n    ]\n    sol = _linsolve(eqs, [x, y, z])\n    assert sol == {x: 2, y: 0, z: 1}\n\n    # Test with equations having no solution\n    eqs = [\n        Eq(x + y, 1),\n        Eq(x + y, 2)\n    ]\n    sol = _linsolve(eqs, [x, y])\n    assert sol is None\n", "def test__linsolve_symbolic_matrix():\n    # Test a case with a matrix of symbols\n    from sympy import Matrix, symbols\n    A, B, C, D = symbols('A B C D')\n    eqs = [\n        Eq(A*x + B*y, 2),\n        Eq(C*x + D*y, 3)\n    ]\n    sol = {x: -3/D + 3*B/D, y: 2/D - A/D}\n    assert _linsolve(eqs, [x, y]) == sol\n", "def test__linsolve_deprecated_linear():\n    # Test that the old deprecated method is able to correctly handle systems\n    # with a single term.\n    assert _linsolve([Eq(x, 0)], [x]) == {x: 0}\n    assert _linsolve([x - x], [x]) == {x: x}\n    assert _linsolve([Eq(2*x, 2)], [x]) == {x: 1}\n    assert _linsolve([x - 1, Eq(y, 0)], [x, y]) == {x: 1, y: 0}\n    assert _linsolve([Eq(x, 0), Eq(y, 1)], [x, y]) == {x: 0, y: 1}\n"], "sample_1119": ["def test_inverse_matpow():\n    A = MatrixSymbol('A', 3, 3)\n    n = Symbol('n', integer=True)\n    assert Inverse(MatPow(A, n)).doit() == MatPow(Inverse(A), n).doit()\n", "def test_matpow_canonicalization():\n    assert MatPow(Inverse(A), S.Half).doit() == Inverse(MatPow(A, S.Half)).doit()\n", "def test_inverse_non_square_error_message():\n    raises(NonSquareMatrixError, lambda: Inverse(A),\n            r\"Matrix A is not square\")\n    raises(NonSquareMatrixError, lambda: Inverse(A*B),\n            r\"Matrix A*B is not square\")\n    raises(NonSquareMatrixError, lambda: ZeroMatrix(n, m).I,\n            r\"Matrix 0 is not square\")\n    raises(NonInvertibleMatrixError, lambda: ZeroMatrix(n, n).I,\n            r\"Matrix 0 is not invertible\")\n    raises(NonSquareMatrixError, lambda: OneMatrix(n, m).I,\n            r\"Matrix 1 is not square\")\n    raises(NonInvertibleMatrixError, lambda: OneMatrix(2, 2).I,\n            r\"Matrix 1 is not invertible\")\n", "def test_inverse_errors():\n    raises(NonSquareMatrixError, lambda: Inverse(B))\n    raises(NonInvertibleMatrixError, lambda: Inverse(ZeroMatrix(n, n)))\n    raises(NonSquareMatrixError, lambda: Inverse(OneMatrix(n, m)))\n    raises(NonInvertibleMatrixError, lambda: Inverse(OneMatrix(2, 2)))\n    raises(ValueError, lambda: Inverse(ZeroMatrix(0, 0)))\n    raises(ValueError, lambda: Inverse(OneMatrix(0, 0)))\n", "def test_inverse_expanded():\n    assert (3*Identity(n)).I.expand().doit() == Identity(n)/3\n    assert Inverse(C).doit(inv_expand=False) == Inverse(C)\n    assert Inverse(ZeroMatrix(n, n)).doit(inv_expand=False) == Inverse(ZeroMatrix(n, n))\n    assert Inverse(OneMatrix(n, n)).doit(inv_expand=False) == Inverse(OneMatrix(n, n))\n"], "sample_1036": ["def test_eval_power():\n    x = symbols('x')\n    assert Mul(x, x).eval_power(x) == x**2\n    assert Mul(x, x, x).eval_power(x) == x**3\n    assert Mul(x, x, x).eval_power(x**2) == (x**2)**3\n", "def test_as_coeff_Mul():\n    x, y = symbols('x y', commutative=False)\n    assert Mul(x, 2, y).as_coeff_Mul() == (2, Mul(x, y))\n    assert Mul(-1, x, 3, y).as_coeff_Mul() == (S.NegativeOne, Mul(x, 3, y))\n    assert Mul(x, y).as_coeff_Mul() == (S.One, Mul(x, y))\n", "def test_Mul_matches():\n    x, y = symbols('x y')\n    assert (2*x).matches('3*y') == {x: Rational(3, 2)*y}\n    assert (x*y).matches('2*x*z') == {y: 2*z}\n    assert (x*y).matches('x*y') == {}\n    assert (x*y).matches('x*z') == {y: z}\n    assert (x*y).matches('2*x*y') is None\n", "def test_Mul_flatten():\n    x, y, z = symbols('x y z', commutative=False)\n    assert Mul(x, y, x, y).args == (x*y, x*y)\n    assert Mul(x, x, y, y).args == (x**2, y**2)\n    assert Mul(x, y, x, z).args == (x, y, x, z)\n    assert Mul(x, y, x, z, z).args == (x, y, x, z**2)\n    assert Mul(x, y, x**2, z, z).args == (x**3, y, z**2)\n    assert Mul(x, y, x**2, z**2, z).args == (x**3, y, z**3)\n    assert Mul(x, y, x**2, z**2, z, evaluate=False).args == (x, y, x**2, z**2, z)\n    assert Mul(x, y, x**2, z**2, z).args_cnc() == [[x**3, y, z**3], []]\n", "def test_mul_subs():\n    # Test substitution of commutative and non-commutative expressions\n    # inside a Mul\n    n = Symbol('n', commutative=False)\n    m = Symbol('m', commutative=True)\n    assert (n*m).subs(n, 2) == 2*m\n    assert (n*m).subs(m, 3) == n*3\n    assert (n*m*m).subs(m, 2) == n*2**2\n    assert (n*m*n).subs(m, 2) == n*2*n\n    assert (n*m*n**2).subs(m, 2) == n*2*n**2\n    assert (n*m*n**2).subs(n, 2) == m*2**3\n    assert (n*m*n**2*m**2).subs(m, 2) == n*2**3*n\n"], "sample_927": ["def test_class_template_inheritance():\n    check('class', 'template <typename T> class A : B<T>', {2: 'I0E1A'})\n    check('class', 'template <typename T> class A : B<T, C<T>>', {2: 'I0E1A'})\n    check('class', 'template <typename T> class A : B<T, C<T>, D<T>>', {2: 'I0E1A'})\n    check('class', 'template <typename T> class A : B<T>, C<T>', {2: 'I0E1A'})\n    check('class', 'template <typename T> class A : B<T>, C<T>, D<T>', {2: 'I0E1A'})\n    check('class', 'template <typename T> class A : virtual B<T>', {2: 'I0E1A'})\n    check('class', 'template <typename T> class A : virtual B<T, C<T>>', {2: 'I0E1A'})\n    check('class', 'template <typename T> class A : virtual B<T, C<T>, D<T>>', {2: 'I0E1A'})\n    check('class', 'template <typename T> class A : virtual B<T>, C<T>', {2: 'I0E1A'})\n    check('class', 'template <typename T> class A : virtual B<T>, C<T>, D<T>', {2: 'I0E1A'})\n    check('class', 'template <typename T> class A : public B<T>', {2: 'I0E1A'})\n    check('class', 'template <typename T> class A : public B<T, C<T>>', {2: 'I0E1A'})\n    check('class', 'template <typename T> class A : public B<T, C<T>, D<T>>', {2: 'I0E1A'})\n    check('class', 'template <typename T> class A : public B<T>, C<T>', {2: 'I0E1A'})\n    check('class', 'template <typename T> class A : public B<T>, C<T>, D<T>', {2: 'I0E1A'})\n    check('class', 'template <typename T> class A : protected B<T>', {2: 'I0E1A", "def test_attribute():\n    class Config:\n        cpp_id_attributes = [\"id_attr\"]\n        cpp_paren_attributes = [\"paren_attr\"]\n    parser = DefinitionParser(\"[[nodiscard]] int f()\", location=None,\n                              config=Config())\n    ast = parser.parse_declaration(\"function\", \"function\")\n    parser.assert_end()\n    assert ast.declSpecs.leftSpecs.attrs == [ASTAttribute(\"nodiscard\")]\n    parser = DefinitionParser(\"[[nodiscard]] extern int f()\", location=None,\n                              config=Config())\n    ast = parser.parse_declaration(\"function\", \"function\")\n    parser.assert_end()\n    assert ast.declSpecs.leftSpecs.attrs == [ASTAttribute(\"nodiscard\")]\n    parser = DefinitionParser(\"[[nodiscard]] [[maybe_unused]] int f()\", location=None,\n                              config=Config())\n    ast = parser.parse_declaration(\"function\", \"function\")\n    parser.assert_end()\n    assert ast.declSpecs.leftSpecs.attrs == [ASTAttribute(\"nodiscard\"),\n                                            ASTAttribute(\"maybe_unused\")]\n", "def test_function_param_parsing():\n    check('function', 'void f(T t)', {1: 'f__T', 2: '1f1T'})\n    check('function', 'void f(T const t)', {1: 'f__T', 2: '1f1T'})\n    check('function', 'void f(T volatile t)', {1: 'f__T', 2: '1f1T'})\n    check('function', 'void f(T volatile const t)', {1: 'f__T', 2: '1f1T'})\n    check('function', 'void f(T const volatile t)', {1: 'f__T', 2: '1f1T'})\n    check('function', 'void f(T volatile const &t)', {1: 'f__T', 2: '1fR1T'})\n    check('function', 'void f(T volatile const &&t)', {1: 'f__T', 2: '1fO1T'})\n    check('function', 'void f(T volatile const *t)', {1: 'f__T', 2: '1fPC1T'})\n    check('function', 'void f(T volatile const *const t)', {1: 'f__T', 2: '1fPKC1T'})\n    check('function', 'void f(T volatile const *volatile t)', {1: 'f__T', 2: '1fPKV1T'})\n    check('function', 'void f(T volatile const *const volatile t)', {1: 'f__T', 2: '1fPKVC1T'})\n\n    check('function', 'void f(T t[])', {1: 'f__T', 2: '1fA1T'})\n    check('function', 'void f(T const t[])', {1: 'f__T', 2: '1fA1T'})\n    check('function', 'void f(T volatile t[])', {1: 'f__T', 2: '1fA1T'})\n    check('function', 'void f(T volatile const t[])', {1: 'f__T', 2: '1fA1T'})\n    check('function', 'void f(T const volatile t[])', {1: 'f__T', 2: '1fA1T'})\n   ", "def test_template_parameter_lists():\n    check('class', 'template<template<typename> typename> {key}A',\n          {2: 'II0E0E1A'})\n    check('class', 'template<template<typename> class> {key}A',\n          {2: 'II0E0E1A'})\n    check('class', 'template<template<int> typename> {key}A',\n          {2: 'II_i0E1A'})\n    check('class', 'template<template<int> int> {key}A',\n          {2: 'II_i0E1A'})\n    check('class', 'template<template<int> class> {key}A',\n          {2: 'II_i0E1A'})\n    check('class', 'template<template<int> typename...> {key}A',\n          {2: 'II_iDpE1A'})\n    check('class', 'template<template<int> class...> {key}A',\n          {2: 'II_iDpE1A'})\n    check('class', 'template<template<typename> typename...> {key}A',\n          {2: 'II0EDpE1A'})\n    check('class', 'template<template<typename> class...> {key}A',\n          {2: 'II0EDpE1A'})\n    check('class', 'template<typename T1, typename T2, template<typename> typename> {key}A',\n          {2: 'I00EII0E0E1A'})\n    check('class', 'template<typename T1, typename T2, template<int> typename> {key}A',\n          {2: 'I00EII_i0E1A'})\n    check('class', 'template<typename T1, typename T2, template<typename> class> {key}A',\n          {2: 'I00EII0E0E1A'})\n    check('class', 'template<typename T1, typename T2, template<int> class> {key}A',\n          {2: 'I00EII_i0E1A'})\n    check('class', 'template<typename T1, typename T2, template<typename> typename...> {key}A',\n          {2: 'I00EII0EDpE1A'})\n    check('class', 'template<typename T1, typename T2, template<int> typename", "def test_member_declarators():\n    # types of declarators for members\n    check('member', 'int i', {1: 'i__i', 2: '1i'})\n    check('member', 'const int i', {1: 'i__iC', 2: '1i'})\n    check('member', 'int i[10]', {1: 'i__iA', 2: '1i'})\n    check('member', 'int i[10][5]', {1: 'i__iAA', 2: '1i'})\n    check('member', 'int *i', {1: 'i__iP', 2: '1i'})\n    check('member', 'int i[10][5]', {1: 'i__iAA', 2: '1i'})\n    check('member', 'int *i[10]', {1: 'i__iPA', 2: '1i'})\n    check('member', 'int i[10][5]', {1: 'i__iAA', 2: '1i'})\n    check('member', 'const int *i', {1: 'i__iPC', 2: '1i'})\n    check('member', 'int *const i', {1: 'i__iPC', 2: '1i'})\n    check('member', 'volatile int *i', {1: 'i__iPV', 2: '1i'})\n    check('member', 'int *volatile i', {1: 'i__iPV', 2: '1i'})\n    check('member', 'int *const volatile i', {1: 'i__iPVC', 2: '1i'},\n          output='int *volatile const i')\n    check('member', 'const int *const i', {1: 'i__iPCC', 2: '1i'})\n    check('member', 'const int *volatile i', {1: 'i__iPVC', 2: '1i'},\n          output='const int *volatile i')\n    check('member', 'int *const volatile *i', {1: 'i__iPVC', 2: '1i'},\n          output='int *volatile const i')\n    check('member', 'const int *const volatile *i', {1: 'i__iPVC"], "sample_588": ["    def test_combine_by_coords(self):\n        objs = [Dataset({\"x\": [0]}), Dataset({\"x\": [1]})]\n        actual = combine_by_coords(objs)\n        expected = Dataset({\"x\": [0, 1]})\n        assert_identical(expected, actual)\n\n        actual = combine_by_coords([actual])\n        assert_identical(expected, actual)\n\n        objs = [Dataset({\"x\": [0, 1]}), Dataset({\"x\": [2]})]\n        actual = combine_by_coords(objs)\n        expected = Dataset({\"x\": [0, 1, 2]})\n        assert_identical(expected, actual)\n\n        # ensure combine_by_coords handles non-sorted variables\n        objs = [\n            Dataset({\"x\": (\"a\", [0]), \"y\": (\"a\", [0]), \"a\": [0]}),\n            Dataset({\"x\": (\"a\", [1]), \"y\": (\"a\", [1]), \"a\": [1]}),\n        ]\n        actual = combine_by_coords(objs)\n        expected = Dataset({\"x\": (\"a\", [0, 1]), \"y\": (\"a\", [0, 1]), \"a\": [0, 1]})\n        assert_identical(expected, actual)\n\n        objs = [Dataset({\"x\": [0], \"y\": [0]}), Dataset({\"y\": [1], \"x\": [1]})]\n        actual = combine_by_coords(objs)\n        expected = Dataset({\"x\": [0, 1], \"y\": [0, 1]})\n        assert_equal(actual, expected)\n\n        objs = [Dataset({\"x\": 0}), Dataset({\"x\": 1})]\n        with raises_regex(ValueError, \"Could not find any dimension coordinates\"):\n            combine_by_coords(objs)\n\n        objs = [Dataset({\"x\": [0], \"y\": [0]}), Dataset({\"x\": [0]})]\n        with raises_regex(ValueError, \"Every dimension needs a coordinate\"):\n            combine_by_coords(objs)\n", "    def test_empty_input(self):\n        assert_identical(Dataset(), combine_by_coords([]))\n", "    def test_empty_input(self):\n        assert_identical(Dataset(), combine_by_coords([]))\n", "def test_combine_nested_compat(compat):\n    data = create_test_data()\n    data1 = data.copy(deep=True)\n    data2 = data.copy(deep=True)\n    data2.attrs[\"foo\"] = \"bar\"\n\n    if compat == \"identical\":\n        with raises_regex(ValueError, \"attributes are not equal\"):\n            combine_nested([data1, data2], concat_dim=\"dim1\", compat=compat)\n\n    elif compat == \"equals\":\n        with raises_regex(ValueError, \"not equal\"):\n            combine_nested([data1, data2], concat_dim=\"dim1\", compat=compat)\n\n    elif compat == \"override\":\n        actual = combine_nested([data1, data2], concat_dim=\"dim1\", compat=compat)\n        assert actual.identical(data1)\n\n    else:\n        actual = combine_nested([data1, data2], concat_dim=\"dim1\", compat=compat)\n        assert actual.identical(data1)\n", "    def test_merge_datasets_with_unsorted_variables(self):\n        # Issue #3753: combine_by_coords fails when variables are unsorted\n        ds1 = Dataset({\"c\": (\"x\", [3]), \"a\": (\"x\", [1]), \"b\": (\"x\", [2])}, {\"x\": [0]})\n        ds2 = Dataset({\"c\": (\"x\", [6]), \"b\": (\"x\", [5]), \"a\": (\"x\", [4])}, {\"x\": [1]})\n\n        actual = combine_by_coords([ds1, ds2])\n        expected = Dataset({\"a\": (\"x\", [1, 4]), \"b\": (\"x\", [2, 5]), \"c\": (\"x\", [3, 6])}, {\"x\": [0, 1]})\n        assert_identical(expected, actual)\n"], "sample_430": ["def test_alter_db_table_comment_options(self):\n    changes = self.get_changes(\n        [self.author_empty], [self.author_with_db_table_options]\n    )\n    self.author_with_db_table_options.options.update({\"db_table_comment\": \"New table comment\"})\n    changes = self.get_changes(\n        [self.author_with_db_table_options], [self.author_with_db_table_options]\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelTableComment\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, name=\"author\", table_comment=\"New table comment\"\n    )\n", "def test_generate_renamed_fields_twice(self):\n    before = [\n        ModelState(\n            \"app\",\n            \"Foo\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"field\", models.IntegerField(unique=True)),\n            ],\n            options={\"unique_together\": {(\"field\",)}},\n        ),\n    ]\n    after = [\n        ModelState(\n            \"app\",\n            \"Foo\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"renamed_field\", models.IntegerField(unique=True)),\n            ],\n            options={\"unique_together\": {(\"renamed_field\",)}},\n        ),\n    ]\n    changes = self.get_changes(before, after, MigrationQuestioner({\"ask_rename\": True}))\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"app\", 1)\n    self.assertOperationTypes(changes, \"app\", 0, [\"RenameField\"])\n    self.assertOperationAttributes(\n        changes, \"app\", 0, 0, old_name=\"field\", new_name=\"renamed_field\"\n    )\n\n    before = after\n    after = [\n        ModelState(\n            \"app\",\n            \"Foo\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"renamed_field_again\", models.IntegerField(unique=True)),\n            ],\n            options={\"unique_together\": {(\"renamed_field_again\",)}},\n        ),\n    ]\n    changes = self.get_changes(before, after, MigrationQuestioner({\"ask_rename\": True}))\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"app\", 1)\n    self.assertOperationTypes(changes, \"app\", 0, [\"RenameField\"])\n    self.assertOperationAttributes(\n        changes,\n        \"app\",\n        0,\n        0,\n        old_name=\"renamed_field\",\n        new_name=\"renamed_field_again\",\n    )\n", "def test_order_with_respect_to_removed_before_fields(self):\n    \"\"\"Tests that order_with_respect_to is removed before fields that depend on it.\"\"\"\n    author_with_book_order_wrt = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n            (\"book\", models.ForeignKey(\"otherapp.Book\", models.CASCADE)),\n        ],\n        options={\"order_with_respect_to\": \"book\"},\n    )\n    author_with_book = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n            (\"book\", models.ForeignKey(\"otherapp.Book\", models.CASCADE)),\n        ],\n    )\n    changes = self.get_changes(\n        [author_with_book_order_wrt, self.book],\n        [author_with_book, self.book],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(\n        changes,\n        \"testapp\",\n        0,\n        [\"AlterOrderWithRespectTo\"],\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        name=\"author\",\n        order_with_respect_to=None,\n    )\n", "def test_change_db_column_from_model_field(self):\n    \"\"\"\n    Test autodetection of db_column changes.\n    \"\"\"\n    before = [\n        ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"name\", models.CharField(max_length=200, db_column=\"custom_name\")),\n            ],\n        ),\n    ]\n    after = [\n        ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"name\", models.CharField(max_length=200, db_column=\"standard_name\")),\n            ],\n        ),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"name\")\n    self.assertOperationFieldAttributes(\n        changes, \"testapp\", 0, 0, db_column=\"standard_name\"\n    )\n", "    def test_rename_and_alter_field_on_intermediate_state(self):\n        state1 = self.make_project_state([self.author_name])\n        state2 = self.make_project_state(\n            [ModelState(\"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True)), (\"name\", models.CharField(max_length=300))])]\n        )\n        state3 = self.make_project_state(\n            [\n                ModelState(\n                    \"testapp\",\n                    \"RenamedAuthor\",\n                    [\n                        (\"id\", models.AutoField(primary_key=True)),\n                        (\"new_name\", models.CharField(max_length=300)),\n                    ],\n                )\n            ]\n        )\n\n        autodetector = MigrationAutodetector(state1, state2)\n        changes = autodetector._detect_changes()\n        autodetector = MigrationAutodetector(state2, state3, questioner=MigrationQuestioner({\"ask_rename\": True, \"ask_rename_model\": True}))\n        changes.update(autodetector._detect_changes())\n\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameModel\", \"RenameField\"])\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 0, old_name=\"Author\", new_name=\"RenamedAuthor\"\n        )\n        self.assertOperationAttributes(changes, \"testapp\", 0, 1, old_name=\"name\", new_name=\"new_name\")\n"], "sample_959": ["def test_domain_cpp_ast_explicit_instantiation():\n    check('function', 'template void f();', {1: None, 2: 'Iv', 4: 'Ivv'})\n    check('function', 'template<> void f();', {1: None, 2: 'Iv', 4: 'Ivv'})\n    check('function', 'extern template void f();', {1: None, 2: 'Iv', 4: 'Ivv'})\n    check('function', 'extern template<> void f();', {1: None, 2: 'Iv', 4: 'Ivv'})\n    check('function', 'template void f<int>();', {1: None, 2: 'IiEv', 4: 'IiEv'})\n    check('function', 'template<> void f<int>();', {1: None, 2: 'IiEv', 4: 'IiEv'})\n    check('function', 'extern template void f<int>();', {1: None, 2: 'IiEv', 4: 'IiEv'})\n    check('function', 'extern template<> void f<int>();', {1: None, 2: 'IiEv', 4: 'IiEv'})\n    check('function', 'template void f<int, char>();', {1: None, 2: 'IicEv', 4: 'IicEv'})\n    check('function', 'template<> void f<int, char>();', {1: None, 2: 'IicEv', 4: 'IicEv'})\n    check('function', 'extern template void f<int, char>();', {1: None, 2: 'IicEv', 4: 'IicEv'})\n    check('function', 'extern template<> void f<int, char>();', {1: None, 2: 'IicEv', 4: 'IicEv'})\n    check('function', 'template void f(); void f<int>();', {1: None, 2: 'Iv', 4: 'Ivv'})\n    check('function', 'template<> void f(); void f<int>();', {1: None, 2: 'Iv', 4: 'Ivv'})\n    check('function', 'extern template void f(); void f<int>();', {1: None, 2: '", "def test_domain_cpp_ast_template_declarations():\n    check('class', 'template<template<typename> typename T> {key}A', {2: 'II0E0E1A'})\n    check('class', 'template<typename T, template<typename> typename> {key}A', {2: 'I0I0E0E1A'})\n    check('class', 'template<typename T, template<typename> typename...> {key}A', {2: 'I0I0EDpE1A'})\n    check('class', 'template<typename... T, template<typename> typename> {key}A', {2: 'IDpI0E0E1A'})\n    check('class', 'template<typename T, template<typename> typename...> {key}A', {2: 'I0I0EDpE1A'})\n    check('class', 'template<int> {key}A', {2: 'I_iE1A'})\n    check('class', 'template<int T> {key}A', {2: 'I_iE1A'})\n    check('class', 'template<int... T> {key}A', {2: 'I_DpiE1A'})\n    check('class', 'template<int T = 42> {key}A', {2: 'I_iE1A'})\n    check('class', 'template<int = 42> {key}A', {2: 'I_iE1A'})\n    check('class', 'template<typename T, int> {key}A', {2: 'I0E_iE1A'})\n    check('class', 'template<int, typename T> {key}A', {2: 'I_i0E1A'})\n    check('class', 'template<int... T, int> {key}A', {2: 'I_Dpi_iE1A'})\n    check('class', 'template<int, int... T> {key}A', {2: 'I_iDpiE1A'})\n\n    check('function', 'template<template<typename> typename T> void f()', {2: 'II0E0Ev1A'})\n    check('function', 'template<typename T, template<typename> typename> void f()', {2: 'I0I0Ev1A'})\n    check('function', 'template<typename T, template<typename> typename...> void f()', {2: '", "def test_domain_cpp_ast_id_prefix():\n    check('class', '{key}A', {1: 'A', 2: \"1A\"}, key='typedef')\n    check('class', '{key}_CPPv2A', {1: 'A', 2: \"1A\"})\n    check('class', '{key}_CPPv3A', {1: None, 2: \"1A\"})\n    check('class', '{key}_CPPv4A', {1: None, 2: \"1A\"})\n\n    check('class', '{key}A<int>', {1: \"A<int>\", 2: \"IE1AIiE\"}, output='{key}A<int>')\n    check('class', '{key}_CPPv2A<int>', {1: \"A<int>\", 2: \"IE1AIiE\"})\n    check('class', '{key}_CPPv3A<int>', {1: None, 2: \"IE1AIiE\"})\n    check('class', '{key}_CPPv4A<int>', {1: None, 2: \"IE1AIiE\"})\n\n    check('function', '{key}A', {1: \"A\", 2: \"1A\"})\n    check('function', '{key}_CPPv2A', {1: \"A\", 2: \"1A\"})\n    check('function', '{key}_CPPv3A', {1: None, 2: \"1A\"})\n    check('function', '{key}_CPPv4A', {1: None, 2: \"1A\"})\n\n    check('function', '{key}f()', {1: \"f\", 2: \"1fv\"})\n    check('function', '{key}_CPPv2f()', {1: \"f\", 2: \"1fv\"})\n    check('function', '{key}_CPPv3f()', {1: None, 2: \"1fv\"})\n    check('function', '{key}_CPPv4f()', {1: None, 2: \"1fv\"})\n\n    check('function', '{key}f(int)', {1: \"f__i\", 2: \"1fi\"})\n    check('function', '{key}_CPPv2f(int)', {1: \"f__i\", 2: \"1fi\"})\n    check('function', '{key}_CPPv3f(int)', {1:", "def test_domain_cpp_ast_template_introducer_params():\n    check('function',\n          'template<concept Q{int, int...}> void f()',\n          {2: 'I00X7QIiDpiEEv', 4: 'I00X7QIiDpiEEvv'})\n    check('function',\n          'template<concept Q{int, int...}> requires C void f()',\n          {4: 'I00X7QIiDpiEEIQaa1CEv'})\n    check('function',\n          'template<concept Q{int, int...}> void f() requires C',\n          {4: 'I00X7QIiDpiEEIQaa1CEvv'})\n    check('function',\n          'template<concept Q{int, int...}> requires C requires D void f()',\n          {4: 'I00X7QIiDpiEEIQoo1C1DEvv'})\n", "def test_domain_cpp_ast_class_template_declarations():\n    check('class', 'template<typename T> {key}A', {2: \"IE1AI1TE\"})\n    check('class', 'template<template<typename> typename T> {key}A', {2: \"II0E0E1A\"})\n    check('class', 'template<int> {key}A', {2: \"I_iE1A\"})\n    check('class', 'template<int T> {key}A', {2: \"I_iE1A\"})\n    check('class', 'template<int...> {key}A', {2: \"I_DpiE1A\"})\n    check('class', 'template<auto T> {key}A', {2: \"I_AE1A\"})\n    check('class', 'template<auto...> {key}A', {2: \"IDpE1A\"})\n    check('class', 'template<auto T> {key}A<int>', {2: \"I_AE1A\"})\n    check('class', 'template<auto T> {key}A<int, int>', {2: \"I_AE1A\"})\n    check('class', 'template<auto T, auto...> {key}A', {2: \"IDpE1A\"})\n    check('class', 'template<int, auto...> {key}A', {2: \"I0I0ADpE1A\"})\n    check('class', 'template<auto, auto...> {key}A', {2: \"IDpE1A\"})\n    check('class', 'template<auto T> {key}A<int, int, int>', {2: \"I_AE1A\"})\n    check('class', 'template<auto T, auto...> {key}A<int, int>', {2: \"IDpE1A\"})\n    check('class', 'template<auto T, auto U, auto...> {key}A', {2: \"IDpE1A\"})\n    check('class', 'template<auto T, auto U, auto V, auto...> {key}A', {2: \"IDpE1A\"})\n\n    check('class', 'template<auto T, decltype(auto)> {key}A', {2: \"I_AE1A\"})\n    check('class', 'template<auto T, decltype"], "sample_1118": ["def test_matpow():\n    assert MatPow(C, 2).args == (C, 2)\n    assert MatPow(C, 2).shape == (n, n)\n    assert MatPow(A, 2).shape == (n, l)\n    assert MatPow(E, 2).shape == (m, m)\n    assert MatPow(C, 2).base == C\n    assert MatPow(C, 2).exp == 2\n\n    assert MatPow(MatPow(C, 2), 3).doit() == MatPow(C, 6).doit()\n    assert MatPow(C, 0).doit() == Identity(n)\n    assert MatPow(ZeroMatrix(n, n), -1).doit() == ZeroMatrix(n, n)\n    assert MatPow(Identity(n), 2).doit() == Identity(n)\n    assert MatPow(C, 1).doit() == C\n    raises(NonSquareMatrixError, lambda: MatPow(A, 2).doit())\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(n, n), -2).doit())\n\n    assert MatPow(C, 2).transpose().doit() == MatPow(C.T, 2).doit()\n", "def test_matpow_derivative():\n    x = symbols('x')\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert MatPow(A, x).doit()._eval_derivative_matrix_lines(x) == (A**x)._eval_derivative_matrix_lines(x)\n    assert MatPow(A, 3)._eval_derivative_matrix_lines(x) == (A**3)._eval_derivative_matrix_lines(x)\n    assert MatPow(A, -1)._eval_derivative_matrix_lines(x) == (A**-1)._eval_derivative_matrix_lines(x)\n    assert MatPow(A, 0)._eval_derivative_matrix_lines(x) == (A**0)._eval_derivative_matrix_lines(x)\n    assert MatPow(A, 1)._eval_derivative_matrix_lines(x) == (A**1)._eval_derivative_matrix_lines(x)\n    assert MatPow(B, 2)._eval_derivative_matrix_lines(x) == (B**2)._eval_derivative_matrix_lines(x)\n", "def test_matpow_doit():\n    assert MatPow(C, S.Zero).doit() == Identity(n)\n    assert MatPow(C, S.One).doit() == C\n    assert MatPow(C, S.NegativeOne).doit() == Inverse(C).doit()\n\n    assert MatPow(ZeroMatrix(n, n), S.One).doit() == ZeroMatrix(n, n)\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(n, n), S.NegativeOne).doit())\n\n    assert MatPow(Identity(n), S.Two).doit() == Identity(n)\n\n    assert MatPow(PermutationMatrix(n), S.Two).doit().is_PermutationMatrix\n    assert MatPow(PermutationMatrix(n), S.NegativeOne).doit().is_PermutationMatrix\n\n    # Test for issue #18495\n    assert MatPow(C, S.One + S.Zero).doit() == C\n    assert MatPow(C, S.One - S.One).doit() == Identity(n)\n    assert MatPow(C, S.One + S.One).doit() == MatPow(C, 2).doit()\n", "def test_matpow():\n    assert MatPow(A, 0).doit() == Identity(n)\n    assert MatPow(A, 1).doit() == A\n    assert MatPow(ZeroMatrix(n, n), 2).doit() == ZeroMatrix(n, n)\n    assert MatPow(Identity(n), -1).doit() == Identity(n)\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(n, n), -1).doit())\n    assert MatPow(MatPow(A, 2), 3).doit() == A**6\n    assert MatPow(A, S.Half).shape == (n, m)\n    assert MatPow(C, 2).doit().shape == (n, n)\n    assert MatPow(C, -2).doit().shape == (n, n)\n    assert MatPow(E*A, 2).doit().shape == (m, m)\n    assert MatPow(A*E, 2).doit().shape == (n, n)\n\n    # Test derivative\n    x = symbols('x')\n    A_expr = MatPow(A, x)\n    assert A_expr.diff(x).doit().shape == (n, m)\n    raises(NotImplementedError, lambda: MatPow(A, x+1).doit())\n\n    # Test transpose\n    assert MatPow(A, 2).T.doit() == MatPow(A.T, 2).doit()\n", "def test_matpow():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    assert MatPow(A, 0).doit() == Identity(n)\n    assert MatPow(A, 1).doit() == A\n    assert MatPow(A, -1).doit() == Inverse(A).doit()\n    assert MatPow(A, S.NegativeOne).doit() == Inverse(A).doit()\n    assert MatPow(A, -2).doit() == Inverse(A).doit()**2\n    assert MatPow(A, 2).doit() == A**2\n    assert MatPow(A, S.One).doit() == A\n    assert MatPow(A, 0).shape == (n, n)\n    assert MatPow(A, S.NegativeOne).shape == (n, n)\n    assert MatPow(A, 2).shape == (n, n)\n    assert MatPow(A, 0).base == A\n    assert MatPow(A, S.NegativeOne).base == A\n    assert MatPow(A, 2).base == A\n    assert MatPow(A, 0).exp == 0\n    assert MatPow(A, S.NegativeOne).exp == S.NegativeOne\n    assert MatPow(A, 2).exp == 2\n"], "sample_969": ["def test_is_system_TypeVar():\n    T = TypeVar('T')\n    assert is_system_TypeVar(T)\n\n    class MyTypeVar(TypeVar):\n        pass\n\n    assert not is_system_TypeVar(MyTypeVar)\n", "def test_get_type_hints():\n        pass\n\n        return {}\n\n    assert get_type_hints(foo) == {'a': int, 'b': str, 'return': type(None)}\n    assert get_type_hints(bar) == {'return': Dict[str, int]}\n\n    class MyClass:\n            pass\n\n    assert get_type_hints(MyClass) == {'a': int, 'b': str, 'return': type(None)}\n\n    class BrokenClass:\n            pass\n\n    assert get_type_hints(BrokenClass) == {}\n", "def test_get_type_hints():\n        pass\n\n    type_hints = get_type_hints(test_func)\n    assert type_hints == {'a': int, 'b': str, 'c': List[int], 'return': Tuple[int, str]}\n\n    class TestClass:\n            pass\n\n    type_hints = get_type_hints(TestClass.__init__)\n    assert type_hints == {'a': int, 'b': str, 'return': None}\n\n        pass\n\n    type_hints = get_type_hints(test_func_without_type_hints)\n    assert type_hints == {}\n\n    class TestClassWithoutTypeHints:\n            pass\n\n    type_hints = get_type_hints(TestClassWithoutTypeHints.__init__)\n    assert type_hints == {}\n\n    # Test with ForwardRef\n    from typing import ForwardRef  # type: ignore\n    T = ForwardRef('T')\n        pass\n\n    type_hints = get_type_hints(test_func_with_ForwardRef)\n    assert type_hints == {'a': T, 'return': T}\n", "def test_get_type_hints():\n        pass\n\n    assert get_type_hints(func) == {'arg1': int, 'arg2': str, 'return': NoneType}\n\n    class TestClass:\n            pass\n\n    assert get_type_hints(TestClass) == {'arg1': int, 'arg2': str, 'return': NoneType}\n\n        pass\n\n    assert get_type_hints(func_with_defaults) == {'arg1': int, 'arg2': str, 'return': NoneType}\n\n        pass\n\n    assert get_type_hints(func_with_varargs) == {'args': int, 'return': NoneType}\n\n        pass\n\n    assert get_type_hints(func_with_kwargs) == {'kwargs': str, 'return': NoneType}\n\n        pass\n\n    assert get_type_hints(func_with_union) == {'arg': Union[int, str], 'return': NoneType}\n\n        pass\n\n    assert get_type_hints(func_with_optional) == {'arg': Union[int, NoneType], 'return': NoneType}\n\n        pass\n\n    assert get_type_hints(func_with_callable) == {'arg': Callable[[int], str], 'return': NoneType}\n\n        pass\n\n    assert get_type_hints(func_with_list) == {'arg': List[int], 'return': NoneType}\n\n        pass\n\n    assert get_type_hints(func_with_tuple) == {'arg': Tuple[int, str], 'return': NoneType}\n\n        pass\n\n    assert get_type_hints(func_with_dict) == {'arg': Dict[int, str], 'return': NoneType}\n\n        pass", "def test_get_type_hints():\n        pass\n\n    assert get_type_hints(my_function) == {'arg1': int, 'arg2': str, 'return': None}\n\n        pass\n\n    assert get_type_hints(my_function_with_forward_ref) == {'arg1': 'int', 'arg2': 'str', 'return': None}\n\n        pass\n\n    assert get_type_hints(my_function_with_new_type) == {'arg1': MyInt, 'arg2': 'MyInt', 'return': None}\n\n    class MyClass:\n            pass\n\n    assert get_type_hints(MyClass().my_method) == {'arg1': int, 'arg2': str, 'return': None}\n\n    # Test with a broken __annotations__ attribute\n    class BrokenClass:\n        __annotations__ = None\n\n    assert get_type_hints(BrokenClass()) == {}\n"], "sample_1141": ["def test_expr_free_symbols_deprecation():\n    from sympy.utilities.exceptions import SymPyDeprecationWarning\n    from sympy.abc import x\n    e = x + 1\n    with raises(SympifyError, match=\"expr_free_symbols method\"):\n        e.expr_free_symbols\n", "def test_as_coeff_Mul():\n    A = MatrixSymbol('A', 2, 2)\n    expr = 3*A\n    c, terms = expr.as_coeff_Mul()\n    assert c == 3\n    assert len(terms) == 1\n    assert terms[0] == A\n\n    expr = -A\n    c, terms = expr.as_coeff_Mul()\n    assert c == -1\n    assert len(terms) == 1\n    assert terms[0] == A\n\n    expr = A/2\n    c, terms = expr.as_coeff_Mul()\n    assert c == S.Half\n    assert len(terms) == 1\n    assert terms[0] == A\n\n    expr = 2*A/3\n    c, terms = expr.as_coeff_Mul()\n    assert c == Rational(2, 3)\n    assert len(terms) == 1\n    assert terms[0] == A\n\n    expr = A + 2*A\n    c, terms = expr.as_coeff_Mul()\n    assert c == 3\n    assert len(terms) == 1\n    assert terms[0] == A\n\n    expr = 2*A + 3*A\n    c, terms = expr.as_coeff_Mul()\n    assert c == 5\n    assert len(terms) == 1\n    assert terms[0] == A\n\n    expr = A**2\n    c, terms = expr.as_coeff_Mul()\n    assert c == 1\n    assert len(terms) == 1\n    assert terms[0] == A**2\n\n    expr = 2*A**2\n    c, terms = expr.as_coeff_Mul()\n    assert c == 2\n    assert len(terms) == 1\n    assert terms[0] == A**2\n", "def test_expr_as_coeff_Mul():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    x = symbols('x')\n    assert (2*x*A).as_coeff_Mul() == (2*x, A)\n    assert (x*A + 2*x*B).as_coeff_Mul(rational=True) == (2*x, A + B)\n    assert (x**2*A).as_coeff_Mul() == (x**2, A)\n    assert (2*x + A).as_coeff_Mul() == (S.One, 2*x + A)\n    assert (A*x**2 + 2*x + 1).as_coeff_Mul() == (S.One, A*x**2 + 2*x + 1)\n    assert (A + B).as_coeff_Mul() == (S.One, A + B)\n", "def test_expr_free_symbols():\n    from sympy.abc import x, y\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert A.expr_free_symbols == set()\n    assert (A + x).expr_free_symbols == {x}\n    assert (A + x*y).expr_free_symbols == {x, y}\n    assert (A + (x + y)*B).expr_free_symbols == {x, y, B}\n    assert (A*B + x*y*B).expr_free_symbols == {x, y, B}\n    assert (A*B + x*y*A).expr_free_symbols == {x, y, A, B}\n    assert (A*B - x*y*A).expr_free_symbols == {x, y, A, B}\n    assert (A*B + x*y*A).expr_free_symbols == {x, y, A, B}\n    assert (A*B - x*y*A).expr_free_symbols == {x, y, A, B}\n    assert (A*B + x*y*A.T).expr_free_symbols == {x, y, A, B}\n    assert (A*B - x*y*A.T).expr_free_symbols == {x, y, A, B}\n    assert (A*B + x*y*A.I).expr_free_symbols == {x, y, A, B}\n    assert (A*B - x*y*A.I).expr_free_symbols == {x, y, A, B}\n", "def test_random():\n    from sympy import sin, sqrt, pi, I, Rational, Float\n    from sympy.abc import x, y\n    from sympy.abc import z as zz\n    from sympy import S\n\n    # Issue 14553: Non-commutative numbers don't work with random_complex_number\n    assert (x._random() or S.Zero).is_number\n    assert (y._random() or S.Zero).is_number\n    assert (z._random() or S.Zero).is_number\n    assert (zz._random() or S.Zero).is_number\n\n    assert (sin(x)._random() or S.Zero).is_number\n    assert (sqrt(1 + x)._random() or S.Zero).is_number\n\n    assert (S.NaN)._random() is None\n    assert (S.Infinity)._random() is None\n    assert (S.NegativeInfinity)._random() is None\n    assert (S.ComplexInfinity)._random() is None\n    assert (Rational(1, 2))._random() == Rational(1, 2)\n    assert (Float(0.5, 3))._random() == Float(0.5, 3)\n    assert (I)._random() == I\n    assert (I)._random() == I\n\n    # When there are multiple variables, we can't guarantee that the\n    # substitution won't cause the evaluation to fail, e.g.\n    # Float(1/(x-y)).n(subs={x:1, y:1}) fails, so in such cases we\n    # return None rather than raise an exception.\n    assert (1/(x - y))._random() is None\n    assert (1/(x**2 + y**2))._random() is not None\n"], "sample_1174": ["def test_unbranched_argument():\n    assert unbranched_argument(0) == 0\n    assert unbranched_argument(1) == 0\n    assert unbranched_argument(I) == pi/2\n    assert unbranched_argument(-I) == -pi/2\n    assert unbranched_argument(1 + I) == pi/4\n    assert unbranched_argument(-1 + I) == 3*pi/4\n    assert unbranched_argument(1 - I) == -pi/4\n    assert unbranched_argument(exp_polar(4*pi*I)) == 4*pi\n    assert unbranched_argument(exp_polar(-7*pi*I)) == -7*pi\n    assert unbranched_argument(exp_polar(5 - 3*pi*I/4)) == -3*pi/4\n    f = Function('f')\n    assert not unbranched_argument(f(0) + I*f(1)).atoms(re)\n\n    x = Symbol('x')\n    p = Function('p', extended_positive=True)\n    assert unbranched_argument(p(x)) == 0\n    assert unbranched_argument((3 + I)*p(x)) == unbranched_argument(3 + I)\n\n    p = Symbol('p', positive=True)\n    assert unbranched_argument(p) == 0\n\n    n = Symbol('n', negative=True)\n    assert unbranched_argument(n) == pi\n\n    x = Symbol('x')\n    assert conjugate(unbranched_argument(x)) == unbranched_argument(x)\n", "def test_principal_branch_periodicity():\n    from sympy import principal_branch, exp_polar, pi\n    x = Symbol('x')\n    assert principal_branch(exp_polar(pi*I)*x, 2*pi) == \\\n        principal_branch(x, 2*pi)\n    assert principal_branch(exp_polar(2*pi*I)*x, 2*pi) == \\\n        principal_branch(x, 2*pi)\n    assert principal_branch(exp_polar(-pi*I)*x, 2*pi) == \\\n        principal_branch(x, 2*pi)\n    assert principal_branch(exp_polar(-2*pi*I)*x, 2*pi) == \\\n        principal_branch(x, 2*pi)\n    assert principal_branch(exp_polar(pi*I)*x, pi) == \\\n        principal_branch(-x, pi)\n    assert principal_branch(exp_polar(2*pi*I)*x, pi) == \\\n        principal_branch(x, pi)\n    assert principal_branch(exp_polar(-pi*I)*x, pi) == \\\n        principal_branch(-x, pi)\n    assert principal_branch(exp_polar(-2*pi*I)*x, pi) == \\\n        principal_branch(x, pi)\n", "def test_periodic_argument_coverage():\n    from sympy import (periodic_argument, unbranched_argument, oo,\n                       exp_polar, I, pi)\n    x = Symbol('x')\n    p = Symbol('p', positive=True)\n\n    assert periodic_argument(1 + I, oo) == periodic_argument(1 + I, oo)\n    assert periodic_argument(2*p, oo) == periodic_argument(p, oo)\n    assert periodic_argument(-2*p, oo) == periodic_argument(p, oo)\n\n    assert periodic_argument(exp_polar(I*pi), oo) == pi\n    assert periodic_argument(exp_polar(-I*pi), oo) == -pi\n    assert periodic_argument(exp_polar(I*pi), 2*pi) == pi\n    assert periodic_argument(exp_polar(-I*pi), 2*pi) == -pi\n    assert periodic_argument(exp_polar(2*I*pi), 2*pi) == 0\n    assert periodic_argument(exp_polar(-2*I*pi), 2*pi) == 0\n", "def test_polar_lift():\n    x = Symbol('x')\n    a = Symbol('a', real=True)\n    assert polar_lift(2) == 2*exp_polar(0)\n    assert polar_lift(-2) == 2*exp_polar(I*pi)\n    assert polar_lift(2 + I) == polar_lift(2 + I)\n    assert polar_lift(-2 + I) == polar_lift(-2 + I)\n    assert polar_lift(1 + I).doit() == exp_polar(I*pi/4)\n    assert polar_lift(-1 + I).doit() == exp_polar(3*I*pi/4)\n    assert polar_lift(1 - I).doit() == exp_polar(-I*pi/4)\n    assert polar_lift(-1 - I).doit() == exp_polar(-3*I*pi/4)\n    assert polar_lift(2*I) == exp_polar(-I*pi/2)\n    assert polar_lift(-2*I) == exp_polar(I*pi/2)\n    assert polar_lift(a*I) == exp_polar(-I*pi/2)*a\n    assert polar_lift(-a*I) == exp_polar(I*pi/2)*a\n    assert polar_lift(x*I) == polar_lift(x*I)\n    assert polar_lift(-x*I) == polar_lift(-x*I)\n    assert polar_lift(-2 + 2*I).doit() == 2*exp_polar(I*pi/4)\n    assert polar_lift(2 - 2*I).doit() == 2*exp_polar(-I*pi/4)\n    assert polar_lift(-2 - 2*I).doit() == 2*exp_polar(3*I*pi/4)\n    assert polar_lift(2 + 2*I).doit() == 2*exp_polar(-3*I*pi/4)\n    assert polar_lift(4*exp_polar(I*pi/4)).doit() == 4*exp_polar(I*pi/4)\n    assert polar_lift(4*exp_polar(-I*pi/4)).doit() == 4*exp_polar(-I*pi/4)\n    assert polar_lift(4*exp_polar(3*I*pi/4)).doit() == 4*exp_polar(3*I*pi/4)\n    assert polar_lift(4*exp_polar(-3*I*pi/4)).doit() == 4", "def test_periodic_argument_polar_number():\n    from sympy import (periodic_argument, unbranched_argument, oo,\n                       principal_branch, polar_lift, pi, I)\n    p = Symbol('p', positive=True)\n    x = Symbol('x')\n\n    assert periodic_argument(polar_lift(2 + I), oo) == unbranched_argument(polar_lift(2 + I))\n    assert periodic_argument(polar_lift(2 + I), 2*pi) == periodic_argument(polar_lift(2 + I), oo)\n    assert periodic_argument(principal_branch(x, pi), pi) == periodic_argument(x, pi)\n    assert periodic_argument(principal_branch(p, 2*pi), 2*pi) == periodic_argument(p, 2*pi)\n    assert periodic_argument(principal_branch(polar_lift(2 + I), 2*pi), 2*pi) == \\\n        periodic_argument(principal_branch(polar_lift(2 + I), 2*pi), 2*pi)\n    assert periodic_argument(polar_lift(2 + I), pi) == periodic_argument(polar_lift(2 + I), oo)\n    assert periodic_argument(polar_lift(1 + I), pi/2) == periodic_argument(polar_lift(1 + I), oo)\n\n    # Test the case where the polar number is already in the principal branch.\n    # This is to check the case where unbranched_argument is not called.\n    z = principal_branch(x, 2*pi)\n    assert periodic_argument(z, 2*pi) == arg(z)\n\n    # Test the case where the period is pi.\n    # This is to check the case where the number is not in the principal branch.\n    z = principal_branch(x, pi)\n    assert periodic_argument(z, pi) == arg(z)\n"], "sample_133": ["    def test_jsi18n_empty_catalog_with_invalid_language(self):\n        \"\"\"\n        The javascript_catalog returns an empty catalog when the selected language is not valid.\n        \"\"\"\n        with self.settings(LANGUAGE_CODE='es'), override('invalid'):\n            response = self.client.get('/jsi18n/')\n            self.assertEqual(response['Content-Type'], 'text/javascript; charset=\"utf-8\"')\n            # response content must include a line like:\n            # \"django.catalog = {};\"\n            self.assertContains(response, 'django.catalog = {};')\n            self.assertNotContains(response, 'var newcatalog =')\n", "def test_jsi18n_fallback_language_plural_with_no_fallback_translations(self):\n    \"\"\"\n    The fallback to a language with less plural forms maintains the real\n    language's number of plural forms and correct translations.\n    \"\"\"\n    with self.settings(LANGUAGE_CODE='pt'), override('fr'):\n        response = self.client.get('/jsi18n/')\n        self.assertEqual(response.context['plural'], None)\n        self.assertEqual(response.context['catalog'], {})\n\n    with self.settings(LANGUAGE_CODE='fr'), override('pt'):\n        response = self.client.get('/jsi18n/')\n        self.assertEqual(response.context['plural'], '(n != 1)')\n        self.assertEqual(\n            response.context['catalog'],\n            {'month name\\x04May': 'Mai'}\n        )\n", "    def test_jsi18n_domain(self):\n        with override('es'):\n            response = self.client.get('/jsi18n_domain/')\n            self.assertEqual(response['Content-Type'], 'text/javascript; charset=\"utf-8\"')\n            self.assertContains(response, 'var django = globals.django || (globals.django = {});')\n            self.assertContains(response, 'django.catalog = django.catalog || {};')\n", "    def test_jsi18n_view_with_custom_domain(self):\n        with self.settings(LANGUAGE_CODE='fr'), override('en-us'):\n            response = self.client.get('/jsi18n_custom_domain/')\n            self.assertContains(response, 'this app0 string is to be translated')\n", "def test_i18n_unknown_language(self):\n    \"\"\"\n    The JavaScriptCatalog view should not crash when an unknown language code is provided.\n    \"\"\"\n    with override('unknown'):\n        response = self.client.get('/jsi18n/')\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response['Content-Type'], 'text/javascript; charset=\"utf-8\"')\n        self.assertEqual(response.context['catalog'], {})\n        self.assertIn('plural', response.context)\n        self.assertIsNone(response.context['plural'])\n"], "sample_1058": ["def test_AbstractPythonCodePrinter_Declaration():\n    prntr = AbstractPythonCodePrinter()\n    x = symbols('x')\n    expr = x**2\n    decl = Assignment(x, expr)\n    assert prntr.doprint(decl) == 'x = x**2'\n", "compilation error", "def test_PythonCodePrinter_fully_qualified_modules():\n    prntr = PythonCodePrinter({'fully_qualified_modules': False})\n    assert prntr.doprint(x**y) == 'x**y'\n    assert prntr.module_imports == set()\n\n    assert prntr.doprint(pi) == 'pi'\n    assert prntr.module_imports == set()\n\n    assert prntr.doprint(x**Rational(1, 2)) == 'sqrt(x)'\n    assert prntr.module_imports == set()\n\n    assert prntr.doprint(acos(x)) == 'acos(x)'\n    assert prntr.module_imports == set()\n\n    prntr = PythonCodePrinter({'fully_qualified_modules': True})\n    assert prntr.doprint(x**y) == 'x**y'\n    assert prntr.module_imports == set()\n\n    assert prntr.doprint(pi) == 'math.pi'\n    assert prntr.module_imports == {'math': {'pi'}}\n\n    assert prntr.doprint(x**Rational(1, 2)) == 'math.sqrt(x)'\n    assert prntr.module_imports == {'math': {'pi', 'sqrt'}}\n\n    assert prntr.doprint(acos(x)) == 'math.acos(x)'\n    assert prntr.module_imports == {'math': {'pi', 'sqrt', 'acos'}}\n", "def test_AbstractPythonCodePrinter():\n    p = AbstractPythonCodePrinter()\n\n    assert p._print_Infinity(oo) == 'float(\"inf\")'\n    assert p._print_NegativeInfinity(-oo) == 'float(\"-inf\")'\n    assert p._print_ComplexInfinity(zoo) == 'float(\"nan\")'\n    assert p._print_Piecewise((1, Eq(x, 0)), evaluate=False) == '(1) if (x == 0) else None'\n    assert p._print_FunctionDefinition((x + 2, x)) == 'def x(x):\\n    return x + 2'\n    assert p._print_While((x < 10, (x, x+1))) == 'while (x < 10):\\n    x = x + 1'\n    assert p._print_Declaration((x, 2)) == 'x = 2'\n    assert p._print_Return(x) == 'return x'\n    assert p._print_Print([x, y, z]) == 'print(x, y, z)'\n    assert p._print_Stream(\"stdout\") == 'sys.stdout'\n    assert p._print_NoneToken(None) == 'None'\n", "def test_AbstractPythonCodePrinter():\n    p = AbstractPythonCodePrinter()\n    assert p._get_statement(\"statement\") == \"statement\"\n    assert p._get_comment(\"comment\") == \"  # comment\"\n    assert p._format_code([\"line1\", \"line2\"]) == \"line1\\nline2\"\n    assert p._declare_number_const(\"name\", 5) == \"name = 5\"\n    assert p._indent_codestring(\"line1\\nline2\") == \"    line1\\n    line2\"\n    assert p._expand_fold_binary_op(\"operator\", [1, 2, 3]) == \"operator(operator(1, 2), 3)\"\n    assert p._expand_reduce_binary_op(\"operator\", [1, 2, 3, 4]) == \"operator(operator(1, 2), operator(3, 4))\"\n    assert p._print_Infinity(None) == \"float('inf')\"\n    assert p._print_NegativeInfinity(None) == \"float('-inf')\"\n    assert p._print_ComplexInfinity(None) == \"float('nan')\"\n    assert p._print_NaN(None) == \"float('nan')\"\n\n    p = AbstractPythonCodePrinter({'fully_qualified_modules': False})\n    assert p._module_format(\"math.sqrt\", register=False) == \"sqrt\"\n    assert p._module_format(\"math.sqrt\", register=True) == \"sqrt\"\n    assert p._module_format(\"math.sqrt\") == \"sqrt\"\n    assert p._module_format(\"math.sqrt\", register=False) == \"sqrt\"\n    assert p._module_format(\"math.sqrt\") == \"sqrt\"\n    assert \"math\" in p.module_imports\n\n    p = AbstractPythonCodePrinter({'fully_qualified_modules': True})\n    assert p._module_format(\"math.sqrt\", register=False) == \"math.sqrt\"\n    assert p._module_format(\"math.sqrt\", register=True) == \"math.sqrt\"\n    assert p._module_format(\"math.sqrt\") == \"math.sqrt\"\n    assert p._module_format(\"math.sqrt\", register=False) == \"math.sqrt\"\n    assert p._module_format(\"math.sqrt\") == \"math.sqrt\"\n    assert \"math\" in p.module_imports\n\n    p = AbstractPythonCodePrinter({'contract': True})\n    assert p._print_FunctionDefinition(None) == \"def None():\\n    pass\"\n\n    p = AbstractPythonCodePrinter({'contract': False})\n    assert p._print_Function"], "sample_828": ["def test_cosine_similarity_symmetry():\n    # Test that cosine_similarity is symmetric\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    S1 = cosine_similarity(X)\n    S2 = cosine_similarity(X.T)\n    assert_array_almost_equal(S1, S2.T)\n", "def test_check_paired_arrays_error():\n    # Ensures that check_paired_arrays raises a ValueError if the arrays have\n    # different shapes.\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    Y = rng.random_sample((3, 4))\n    assert_raises(ValueError, check_paired_arrays, X, Y)\n", "def test_pairwise_distances_symmetry(dtype):\n    # check that distance matrix is symmetric\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((100, 10)).astype(dtype, copy=False)\n    X[X < 0.8] = 0\n    distances = pairwise_distances(X)\n    assert_array_almost_equal(distances, distances.T)\n", "def test_pairwise_distances_dtype(metric, func, x_array_constr, y_array_constr):\n    # check that pairwise distances gives same result as corresponding function\n    # when X and Y != X are provided\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((10, 10)).astype(np.float32)\n    Y = rng.random_sample((10, 10)).astype(np.float32)\n\n    expected = func(X, Y)\n    assert expected.dtype == np.float32\n\n    X = x_array_constr(X)\n    Y = y_array_constr(Y)\n    distances = pairwise_distances(X, Y, metric=metric)\n\n    assert_allclose(distances, expected)\n    assert distances.dtype == np.float32\n", "def test_haversine_distances_computed_chunkwise():\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 2))\n    X[:, 0] = (X[:, 0] - 0.5) * 2 * np.pi/2\n    X[:, 1] = (X[:, 1] - 0.5) * 2 * np.pi\n\n        dist_lat = y[0] - x[0]\n        dist_lon = y[1] - x[1]\n        a = np.sin(dist_lat / 2) ** 2 + (\n            np.cos(x[0]) * np.cos(y[0]) * np.sin(dist_lon/2) ** 2\n        )\n        c = 2 * np.arcsin(np.sqrt(a))\n        return c\n\n    expected = np.array([[haversine(x, y) for y in X] for x in X])\n\n        return D\n\n    chunks = pairwise_distances_chunked(X, metric=\"haversine\",\n                                        working_memory=1,\n                                        reduce_func=reduce_func)\n    assert isinstance(chunks, GeneratorType)\n    chunks = list(chunks)\n    assert len(chunks) > 1\n    # atol is for diagonal where expected is explicitly zeroed on the diagonal\n    assert_allclose(np.vstack(chunks), expected, atol=1e-10)\n"], "sample_827": ["def test_incr_mean_variance_axis_non_nan():\n    for axis in [0, 1]:\n        rng = np.random.RandomState(0)\n        n_features = 50\n        n_samples = 10\n        data_chunks = [rng.randint(0, 2, size=n_features)\n                       for i in range(n_samples)]\n\n        # default params for incr_mean_variance\n        last_mean = np.zeros(n_features)\n        last_var = np.zeros_like(last_mean)\n        last_n = np.zeros_like(last_mean, dtype=np.int64)\n\n        # Test _incr_mean_and_var with a 1 row input\n        X = np.array(data_chunks[0])\n        X = np.atleast_2d(X)\n        X_lil = sp.lil_matrix(X)\n        X_csr = sp.csr_matrix(X_lil)\n        X_csc = sp.csc_matrix(X_lil)\n\n        X_means, X_vars = mean_variance_axis(X_csr, axis)\n        X_means_incr, X_vars_incr, n_incr = \\\n            incr_mean_variance_axis(X_csr, axis, last_mean, last_var, last_n)\n        assert_array_almost_equal(X_means, X_means_incr)\n        assert_array_almost_equal(X_vars, X_vars_incr)\n        assert_equal(X.shape[axis], n_incr)  # X.shape[axis] picks # samples\n\n        X_means, X_vars = mean_variance_axis(X_csc, axis)\n        assert_array_almost_equal(X_means, X_means_incr)\n        assert_array_almost_equal(X_vars, X_vars_incr)\n        assert_equal(X.shape[axis], n_incr)\n\n        # Test _incremental_mean_and_var with whole data\n        X = np.vstack(data_chunks)\n        X_lil = sp.lil_matrix(X)\n        X_csr = sp.csr_matrix(X_lil)\n        X_csc = sp.csc_matrix(X_lil)\n\n        expected_dtypes = [(np.float32, np.float32),\n                           (np.float64, np.float64),\n                           (np.int32, np.float64),\n                           (np.int64, np.float64)]\n\n        for input_dtype, output_dtype in expected_dtypes:\n            for X_sparse in (X_csr, X_csc):\n                X_sparse = X_sparse.astype(input_dtype)\n                last_mean = last_mean.astype(output_dtype)\n                last_var = last_var.astype(output_dtype)\n                X_means, X_vars = mean_variance_axis(X_sparse, axis", "def test_inplace_swap_row_csc():\n    X = sp.csc_matrix(np.array([[0, 3, 0],\n                                [2, 4, 0],\n                                [0, 0, 0],\n                                [9, 8, 7],\n                                [4, 0, 5]], dtype=np.float64))\n    X_copy = X.copy()\n    inplace_swap_row_csc(X, 0, -1)\n    X_copy[[0, -1]] = X_copy[[-1, 0]]\n    assert_array_equal(X.toarray(), X_copy.toarray())\n\n    X = sp.csc_matrix(np.array([[0, 3, 0],\n                                [2, 4, 0],\n                                [0, 0, 0],\n                                [9, 8, 7],\n                                [4, 0, 5]], dtype=np.float32))\n    X_copy = X.copy()\n    inplace_swap_row_csc(X, 0, -1)\n    X_copy[[0, -1]] = X_copy[[-1, 0]]\n    assert_array_equal(X.toarray(), X_copy.toarray())\n\n    assert_raises(TypeError, inplace_swap_row_csc, X.tocsr(), 0, 1)\n    assert_raises(TypeError, inplace_swap_row_csc, X, [0, 1], 1)\n    assert_raises(TypeError, inplace_swap_row_csc, X, 0, [1, 2])\n", "def test_min_max_axis_input_validation():\n    X = np.array([[0, 3, 0],\n                  [2, -1, 0],\n                  [0, 0, 0],\n                  [9, 8, 7],\n                  [4, 0, 5]], dtype=np.float64)\n    X_csr = sp.csr_matrix(X)\n    X_csc = sp.csc_matrix(X)\n\n    # Test invalid input type\n    assert_raises(TypeError, min_max_axis, X, axis=0)\n    assert_raises(TypeError, min_max_axis, X_csr.tolil(), axis=0)\n    assert_raises(TypeError, min_max_axis, X_csc.tolil(), axis=0)\n\n    # Test invalid axis\n    assert_raises(ValueError, min_max_axis, X_csr, axis=2)\n    assert_raises(ValueError, min_max_axis, X_csc, axis=-3)\n\n    # Test invalid ignore_nan\n    assert_raises(TypeError, min_max_axis, X_csr, axis=0, ignore_nan='True')\n    assert_raises(TypeError, min_max_axis, X_csc, axis=0, ignore_nan=1)\n", "def test_incr_mean_variance_axis_edge_cases():\n    # Test incr_mean_variance_axis with zero-sized inputs\n    X_csr = sp.csr_matrix((5, 4))\n    X_csc = sp.csc_matrix((5, 4))\n    last_mean = np.zeros(4)\n    last_var = np.zeros(4)\n    last_n = np.zeros(4, dtype=np.int64)\n    X_means, X_vars, X_sample_count = incr_mean_variance_axis(\n        X_csr, axis=0, last_mean=last_mean, last_var=last_var, last_n=last_n)\n    assert_array_equal(X_means, np.zeros(4))\n    assert_array_equal(X_vars, np.zeros(4))\n    assert_array_equal(X_sample_count, np.zeros(4))\n\n    # Test incr_mean_variance_axis with zero variance\n    X_csr = sp.csr_matrix(np.ones((5, 4)))\n    X_csc = sp.csc_matrix(np.ones((5, 4)))\n    X_means, X_vars, X_sample_count = incr_mean_variance_axis(\n        X_csr, axis=0, last_mean=last_mean, last_var=last_var, last_n=last_n)\n    assert_array_equal(X_means, np.ones(4))\n    assert_array_equal(X_vars, np.zeros(4))\n    assert_array_equal(X_sample_count, np.ones(4))\n\n    # Test incr_mean_variance_axis with NaNs\n    X_csr = sp.csr_matrix(np.ones((5, 4)))\n    X_csr.data[0] = np.nan\n    X_csc = sp.csc_matrix(X_csr)\n    X_means, X_vars, X_sample_count = incr_mean_variance_axis(\n        X_csr, axis=0, last_mean=last_mean, last_var=last_var, last_n=last_n)\n    assert_array_equal(X_means, np.ones(4))\n    assert_array_equal(X_vars, np.zeros(4))\n    assert_array_equal(X_sample_count, np.ones(4))\n", "def test_csc_median_axis_0_edge_cases():\n    # Test edge cases of csc_median_axis_0.\n\n    # Test that it returns NaN for a matrix with zero rows.\n    X = np.array([[1, 2], [3, 4]])\n    csc = sp.csc_matrix(np.zeros((0, 2)))\n    assert_array_equal(csc_median_axis_0(csc), np.array([np.nan, np.nan]))\n\n    # Test that it returns NaN for a matrix with zero columns.\n    X = np.array([[1, 2], [3, 4]])\n    csc = sp.csc_matrix(np.zeros((2, 0)))\n    assert_array_equal(csc_median_axis_0(csc), np.array([]))\n\n    # Test that it returns NaN for a matrix with a single column and no data.\n    X = np.array([[1, 2], [3, 4]])\n    csc = sp.csc_matrix(np.zeros((2, 1)))\n    assert_array_equal(csc_median_axis_0(csc), np.array([np.nan]))\n\n    # Test that it returns the median of a single column.\n    X = np.array([[1, 2], [3, 4]])\n    csc = sp.csc_matrix(np.array([[1], [3]]))\n    assert_array_equal(csc_median_axis_0(csc), np.array([2]))\n\n    # Test that it returns the median of multiple columns.\n    X = np.array([[1, 2], [3, 4]])\n    csc = sp.csc_matrix(np.array([[1, 3], [3, 1]]))\n    assert_array_equal(csc_median_axis_0(csc), np.array([2, 2]))\n"], "sample_154": ["def test_database_backends_with_kwargs(self):\n    databases = {'default', 'other'}\n    kwargs = {'some_key': 'some_value'}\n    with mock.patch('django.db.backends.base.validation.BaseDatabaseValidation.check') as mocked_check:\n        check_database_backends(databases=databases, **kwargs)\n        self.assertEqual(mocked_check.call_count, len(databases))\n        for call in mocked_check.call_args_list:\n            self.assertEqual(call[1], kwargs)\n", "def test_check_database_backends_passes_kwargs_to_validation_check(self):\n    with mock.patch('django.db.backends.base.validation.BaseDatabaseValidation.check') as mocked_check:\n        kwargs = {'foo': 'bar'}\n        check_database_backends(databases=self.databases, **kwargs)\n        mocked_check.assert_called_once_with(**kwargs)\n", "def test_database_backends_with_invalid_databases(self):\n    with mock.patch('django.db.connections') as mocked_connections:\n        mocked_connections.__getitem__.side_effect = KeyError('invalid_alias')\n        with self.assertRaises(KeyError):\n            check_database_backends(databases={'invalid_alias'})\n\n    with mock.patch('django.db.connections') as mocked_connections:\n        mocked_connections.__getitem__.return_value.validation.check.side_effect = Exception('Mocked exception')\n        with self.assertRaises(Exception):\n            check_database_backends(databases={'default'})\n", "def test_check_database_backends_with_database_error(self, mocked_connections):\n    mocked_connection = mock.Mock()\n    mocked_connection.validation.check.side_effect = Exception('Mocked database error')\n    mocked_connections.__getitem__.return_value = mocked_connection\n    with self.assertRaisesMessage(Exception, 'Mocked database error'):\n        check_database_backends(databases=self.databases)\n", "def test_database_backends_returns_empty_list_if_no_databases_provided(self):\n    result = check_database_backends()\n    self.assertEqual(result, [])\n"], "sample_319": ["def test_alter_unique_together_fk_to_m2m(self):\n    changes = self.get_changes(\n        [self.author_name, self.book_unique_together],\n        [\n            self.author_name,\n            ModelState(\n                \"otherapp\",\n                \"Book\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"author\", models.ManyToManyField(\"testapp.Author\")),\n                    (\"title\", models.CharField(max_length=200)),\n                ],\n            ),\n        ],\n    )\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(\n        changes, \"otherapp\", 0, [\"AlterUniqueTogether\", \"RemoveField\", \"AddField\"]\n    )\n    self.assertOperationAttributes(\n        changes, \"otherapp\", 0, 0, name=\"book\", unique_together=set()\n    )\n    self.assertOperationAttributes(\n        changes, \"otherapp\", 0, 1, model_name=\"book\", name=\"author\"\n    )\n    self.assertOperationAttributes(\n        changes, \"otherapp\", 0, 2, model_name=\"book\", name=\"author\"\n    )\n", "def test_alter_model_order_with_respect_to_db_table_comment(self):\n    changes = self.get_changes(\n        [self.book, self.author_with_book],\n        [\n            self.book,\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=200)),\n                    (\"book\", models.ForeignKey(\"otherapp.Book\", models.CASCADE)),\n                ],\n                options={\n                    \"order_with_respect_to\": \"book\",\n                    \"db_table_comment\": \"Table comment\",\n                },\n            ),\n        ],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 2)\n    self.assertOperationTypes(\n        changes,\n        \"testapp\",\n        0,\n        [\"AlterOrderWithRespectTo\"],\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        name=\"author\",\n        order_with_respect_to=\"book\",\n    )\n    self.assertOperationTypes(\n        changes,\n        \"testapp\",\n        1,\n        [\"AlterModelTableComment\"],\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        1,\n        0,\n        name=\"author\",\n        table_comment=\"Table comment\",\n    )\n", "def test_alter_unique_together_case(self):\n    \"\"\"\n    Test that unique_together updates are case insensitive.\n    \"\"\"\n    author_unique_together_lower = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ],\n        {\"unique_together\": {(\"name\",)}},\n    )\n    author_unique_together_upper = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ],\n        {\"unique_together\": {(\"Name\",)}},\n    )\n    changes = self.get_changes([author_unique_together_lower], [author_unique_together_upper])\n    self.assertEqual(changes, {})\n", "def test_reverse_one_to_one_fk(self):\n    book_author_fk = ModelState(\n        \"otherapp\",\n        \"Book\",\n        [\n            (\"author\", models.OneToOneField(\"testapp.Author\", models.CASCADE)),\n        ],\n    )\n    book_author_fk_null = ModelState(\n        \"otherapp\",\n        \"Book\",\n        [\n            (\"author\", models.OneToOneField(\"testapp.Author\", models.CASCADE, null=True)),\n        ],\n    )\n    author_empty = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n        ],\n    )\n    changes = self.get_changes([book_author_fk, author_empty], [book_author_fk_null, author_empty])\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, \"otherapp\", 0, 0, model_name=\"book\", name=\"author\")\n    self.assertOperationFieldAttributes(changes, \"otherapp\", 0, 0, null=True)\n", "def test_add_model_with_relation_to_renamed_model(self):\n    \"\"\"Adding a new model with a relation to a renamed model works.\"\"\"\n    changes = self.get_changes(\n        [self.author_empty],\n        [\n            ModelState(\n                \"testapp\",\n                \"NewAuthor\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                ],\n            ),\n            ModelState(\n                \"otherapp\",\n                \"Book\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"author\", models.ForeignKey(\"testapp.NewAuthor\", models.CASCADE)),\n                ],\n            ),\n        ],\n        MigrationQuestioner({\"ask_rename_model\": True}),\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"CreateModel\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, name=\"NewAuthor\", options={}\n    )\n    self.assertOperationAttributes(\n        changes, \"otherapp\", 0, 0, name=\"Book\", fields=[\n            ('id', models.AutoField(primary_key=True)),\n            ('author', models.ForeignKey(\n                'testapp.NewAuthor', models.CASCADE, related_name='+'))\n        ])\n"], "sample_415": ["def test_validate_clone(self):\n    constraint = UniqueConstraintProduct._meta.constraints[0]\n    cloned_constraint = constraint.clone()\n    msg = \"Unique constraint product with this Name and Color already exists.\"\n    non_unique_product = UniqueConstraintProduct(\n        name=self.p1.name, color=self.p1.color\n    )\n    with self.assertRaisesMessage(ValidationError, msg):\n        cloned_constraint.validate(UniqueConstraintProduct, non_unique_product)\n", "    def test_abstract_name(self):\n        constraints = get_constraints(UniqueConstraintProduct._meta.db_table)\n        self.assertIn(\"name_color_uniq\", constraints)\n", "    def test_create_sql_with_deferrable(self):\n        constraint = models.UniqueConstraint(\n            fields=[\"name\"],\n            name=\"name_unique\",\n            deferrable=models.Deferrable.DEFERRED,\n        )\n        self.assertIn(\"DEFERRABLE INITIALLY DEFERRED\", constraint.create_sql(Product, mock.MagicMock()))\n", "    def test_validate_expressions_with_condition_on_model_instance(self):\n        constraint = UniqueConstraint(\n            Lower(\"name\"),\n            name=\"name_lower_uniq\",\n            condition=models.Q(color__isnull=True),\n        )\n        obj = UniqueConstraintProduct.objects.create(name=\"p1\", color=None)\n        with self.assertRaises(ValidationError):\n            constraint.validate(UniqueConstraintProduct, obj, exclude={\"color\"})\n", "    def setUpTestData(cls):\n        cls.product = UniqueConstraintProduct.objects.create(name=\"product\", color=\"red\")\n"], "sample_826": ["def test_ordinal_encoder_unsorted_categories():\n    X = np.array([['a', 'b']], dtype=object).T\n\n    enc = OrdinalEncoder(categories=[['b', 'a', 'c']])\n    exp = np.array([[0.], [1.]])\n    assert_array_equal(enc.fit(X).transform(X), exp)\n    assert_array_equal(enc.fit_transform(X), exp)\n    assert enc.categories_[0].tolist() == ['b', 'a', 'c']\n    assert np.issubdtype(enc.categories_[0].dtype, np.object_)\n\n    # unsorted passed categories still raise for numerical values\n    X = np.array([[1, 2]]).T\n    enc = OrdinalEncoder(categories=[[2, 1, 3]])\n    msg = 'Unsorted categories are not supported'\n    with pytest.raises(ValueError, match=msg):\n        enc.fit_transform(X)\n", "def test_ordinal_encoder_unsorted_categories():\n    X = np.array([['a', 'b']], dtype=object).T\n\n    enc = OrdinalEncoder(categories=[['b', 'a', 'c']])\n    exp = np.array([[1.], [0.]])\n    assert_array_equal(enc.fit_transform(X), exp)\n    assert_array_equal(enc.fit(X).transform(X), exp)\n    assert enc.categories_[0].tolist() == ['b', 'a', 'c']\n    assert np.issubdtype(enc.categories_[0].dtype, np.object_)\n\n    # unsorted passed categories still raise for numerical values\n    X = np.array([[1, 2]]).T\n    enc = OrdinalEncoder(categories=[[2, 1, 3]])\n    msg = 'Unsorted categories are not supported'\n    with pytest.raises(ValueError, match=msg):\n        enc.fit_transform(X)\n", "def test_ordinal_encoder_dtypes_pandas():\n    # check dtype (similar to test_categorical_encoder_dtypes for dataframes)\n    pd = pytest.importorskip('pandas')\n\n    enc = OrdinalEncoder()\n    exp = np.array([[0., 1.],\n                    [1., 0.]], dtype='float64')\n\n    X = pd.DataFrame({'A': [1, 2], 'B': [3, 4]}, dtype='int64')\n    enc.fit(X)\n    assert all([enc.categories_[i].dtype == 'int64' for i in range(2)])\n    assert_array_equal(enc.transform(X), exp)\n\n    X = pd.DataFrame({'A': [1, 2], 'B': ['a', 'b'], 'C': [3., 4.]})\n    X_type = [int, object, float]\n    enc.fit(X)\n    assert all([enc.categories_[i].dtype == X_type[i] for i in range(3)])\n    assert_array_equal(enc.transform(X), exp)\n", "def test_ordinal_encoder_inconsistent_category_type():\n    X = [['a', 1], ['b', 2]]\n    enc = OrdinalEncoder(categories=[['a', 'b'], [1, 'c']])\n    with pytest.raises(ValueError, match=\"Unsorted categories are not supported\"):\n        enc.fit_transform(X)\n", "def test_one_hot_encoder_handle_unknown_drop():\n    # test that an error is raised when an unknown category is present\n    # during transform if drop is set.\n    X = np.array([[1, 1, 1], [2, 1, 1]])\n    X2 = np.array([[2, 2, 1]])\n    enc = OneHotEncoder(drop='first', handle_unknown='ignore')\n    assert_warns(FutureWarning, enc.fit, X)\n    assert_raises(ValueError, enc.transform, X2)\n"], "sample_781": ["def test_forest_feature_importances_permutation():\n    X, y = datasets.make_classification(n_samples=15, n_informative=3, random_state=1,\n                                       n_classes=3)\n    perm = np.random.permutation(X.shape[1])\n    X_perm = X[:, perm]\n    clf = RandomForestClassifier(min_samples_leaf=5, random_state=42,\n                                 n_estimators=200).fit(X, y)\n    clf_perm = RandomForestClassifier(min_samples_leaf=5, random_state=42,\n                                      n_estimators=200).fit(X_perm, y)\n    assert_array_almost_equal(clf.feature_importances_[perm], clf_perm.feature_importances_)\n", "def test_feature_importances_on_dense_and_sparse_data(name):\n    # Test that feature importances are the same on dense and sparse data\n    ForestEstimator = FOREST_ESTIMATORS[name]\n    X, y = datasets.make_classification(n_samples=100, n_features=10,\n                                       n_informative=5, random_state=0)\n    X_sparse = csr_matrix(X)\n\n    est_dense = ForestEstimator(random_state=0)\n    est_dense.fit(X, y)\n\n    est_sparse = ForestEstimator(random_state=0)\n    est_sparse.fit(X_sparse, y)\n\n    assert_array_almost_equal(est_dense.feature_importances_,\n                              est_sparse.feature_importances_)\n", "def check_tree_ids(name):\n    # Check that tree ids in estimators_ are not repeated\n    ForestEstimator = FOREST_ESTIMATORS[name]\n    est = ForestEstimator(n_estimators=10, random_state=0)\n    est.fit(X, y)\n\n    tree_ids = [estimator.tree_.node_count for estimator in est.estimators_]\n    assert len(tree_ids) == len(set(tree_ids))\n\n", "def test_feature_importances_sanity(forest):\n    X = np.array([[0, 1], [2, 3], [4, 5], [6, 7]])\n    y = np.array([0, 1, 1, 0])\n\n    clf = forest()\n    clf.fit(X, y)\n\n    feature_importances = clf.feature_importances_\n\n    assert np.all(feature_importances >= 0)\n    assert np.all(feature_importances <= 1)\n    assert np.isclose(np.sum(feature_importances), 1)\n", "def test_feature_importances_permutation_invariance(name):\n    X, y = datasets.make_classification(n_samples=100, n_features=10,\n                                       n_informative=3, random_state=42)\n    est = FOREST_ESTIMATORS[name](random_state=42)\n    est.fit(X, y)\n    importances1 = est.feature_importances_\n    importances2 = est.feature_importances_\n\n    # feature importances are invariant to feature permutation\n    idx = np.arange(X.shape[1])\n    np.random.shuffle(idx)\n    X_permuted = X[:, idx]\n    est.fit(X_permuted, y)\n    importances_permuted = est.feature_importances_\n\n    # check that importances are still the same\n    assert_array_almost_equal(importances1, importances_permuted[idx])\n    assert_array_almost_equal(importances2, importances_permuted[idx])\n"], "sample_195": ["    def setUp(self):\n        self.ops = DatabaseOperations(connection=connection)\n", "    def setUp(self):\n        self.ops = DatabaseOperations(connection=connection)\n", "    def setUp(self):\n        self.ops = DatabaseOperations(connection=connection)\n", "    def setUp(self):\n        self.ops = DatabaseOperations(connection=connection)\n", "    def setUp(self):\n        self.ops = DatabaseOperations(connection=connection)\n"], "sample_1152": ["def test_powsimp_measure_function():\n    x, y = symbols('x y')\n    assert powsimp(exp(x)*exp(y), measure=lambda x: (x.has(x), x.has(y))) == exp(x + y)\n    assert powsimp(exp(x)*exp(y), measure=lambda x: (x.has(y), x.has(x))) == exp(y + x)\n    assert powsimp(x**2*y**3*x**y, measure=lambda x: (x.has(x), x.has(y))) == x*y**(3 + x)\n    assert powsimp(x**2*y**3*x**y, measure=lambda x: (x.has(y), x.has(x))) == x**(2 + y)*y**3\n", "def test_powsimp_multiple_negative_bases():\n    x, y = symbols('x y', negative=True)\n    assert powsimp((-x)**(2*y)) == x**(-2*y)\n    assert powsimp((-x)**(2*y), force=True) == x**(-2*y)\n    assert powsimp((-x)**(2*y)*(-y)**(2*x), force=True) == (x*y)**(2*x + 2*y)\n    assert powsimp((-x)**(2*y)*(-y)**(2*x)) == (-x)**(2*y)*(-y)**(2*x)\n    assert powsimp((-x)**(2*y)*(-y)**(2*x), combine='base', force=True) == \\\n        ((-x)*(-y))**(2*x + 2*y)\n    assert powsimp((-x)**(2*y)*(-y)**(2*x), combine='exp', force=True) == \\\n        x**(-2*y)*y**(-2*x)\n", "def test_powsimp_rational_exponent():\n    from sympy import Rational\n    x, y, z = symbols('x y z')\n    a, b = symbols('a b', real=True)\n    assert powsimp(x**(Rational(2, 3)) * x**(Rational(1, 3))) == x\n    assert powsimp(x**(Rational(2, 3)) * y**(Rational(1, 3))) == x**(2/3)*y**(1/3)\n    assert powsimp(x**(Rational(2, 3)) * x**(Rational(1, 6))) == x**(5/6)\n    assert powsimp(x**(Rational(2, 3)) * x**(Rational(5, 6))) == x**(3/2)\n    assert powsimp(x**(Rational(1, 2)) * y**(Rational(1, 3))) == x**(1/2)*y**(1/3)\n    assert powsimp(x**(Rational(1, 2)) * x**(Rational(1, 3))) == x**(5/6)\n    assert powsimp(x**(Rational(1, 2)) * x**(Rational(2, 3))) == x**(7/6)\n    assert powsimp(x**(Rational(1, 2)) * x**(Rational(3, 4))) == x**(5/4)\n    assert powsimp(x**(Rational(2, 3)) * x**(Rational(3, 4))) == x**(17/12)\n    assert powsimp(x**(Rational(2, 3)) * x**(Rational(1, 4))) == x**(11/12)\n    assert powsimp(x**(Rational(2, 3)) * x**(Rational(5, 12))) == x**(3/2)\n    assert powsimp(x**(Rational(1, 2)) * x**(Rational(1, 4))) == x**(3/4)\n    assert powsimp(x**(Rational(1, 2)) * x**(Rational(3, 4))) == x**(5/4)\n    assert powsimp(x**(Rational(1, 2)) * x**(Rational(7, 12))) == x**(", "def test_powsimp_powdenest_edge_cases():\n    from sympy import symbols, Eq, powsimp, powdenest\n    x, y, z = symbols('x y z')\n    a, b, c = symbols('a b c', real=True)\n\n    # Test powsimp with deep=True and combine='exp'\n    eq = powsimp((x**y)**z * (x**z)**y, deep=True, combine='exp')\n    assert eq == x**(y*z + z*y)\n\n    # Test powdenest with force=True and polar=True\n    eq = powdenest(exp(x*log(2))**y, force=True, polar=True)\n    assert eq == 2**(x*y)\n\n    # Test powsimp with non-commutative symbols\n    A, B = symbols('A B', commutative=False)\n    eq = powsimp(A**x * B**x)\n    assert eq == A**x * B**x\n\n    # Test powdenest with Rational exponent\n    eq = powdenest(x**(a/b)**c)\n    assert eq == (x**(a/b))**c\n\n    # Test powsimp with complex exponent\n    eq = powsimp(x**(a + b*I) * x**(c + d*I))\n    assert eq == x**(a + c + (b + d)*I)\n\n    # Test powdenest with complex base\n    eq = powdenest((x + y*I)**(a + b*I))\n    assert eq == (x + y*I)**(a + b*I)\n", "def test_powsimp_rational_bases():\n    x, y, z = symbols('x y z', positive=True)\n    assert powsimp((x**x)**(2*y)) == x**(2*x*y)\n    assert powsimp((x**(x**2))**(2*y)) == x**(x**2*2*y)\n    assert powsimp(((x**(x**2))**(x**3))**(2*y)) == x**(2*x**5*y)\n    assert powsimp(((x**(x**2))**(x**3))**(x**4)) == x**(x**9*x**4)\n\n    # issue 19565\n    e = ((x**(x*y))**z)**y\n    assert powsimp(e) == e\n    assert powsimp(e, deep=True) == x**(y**2*z)\n    assert powsimp(e, force=True) == x**(y**2*z)\n\n    e = ((x**(x**y))**z)**y\n    assert powsimp(e) == e\n    assert powsimp(e, deep=True) == e\n    assert powsimp(e, force=True) == x**(y**2*z)\n\n    e = ((x**x)**(y**2))**z\n    assert powsimp(e) == e\n    assert powsimp(e, deep=True) == x**(x*z*y**2)\n    assert powsimp(e, force=True) == x**(x*z*y**2)\n"], "sample_934": ["def test_xref_parsing_prefixes():\n        class Config:\n            cpp_id_attributes = [\"id_attr\"]\n            cpp_paren_attributes = [\"paren_attr\"]\n        parser = DefinitionParser(target, location=None,\n                                  config=Config())\n        ast, isShorthand = parser.parse_xref_object()\n        parser.assert_end()\n        assert isShorthand == False\n    check('f')\n    check('f()')\n    check('void f()')\n    check('T f()')\n", "def test_xref_object():\n    parser = DefinitionParser(\"f()\", location=None)\n    try:\n        ast = parser.parse_xref_object()\n    except DefinitionError as e:\n        raise e\n    assert str(ast) == \"f()\"\n\n    parser = DefinitionParser(\"f(int arg)\", location=None)\n    try:\n        ast = parser.parse_xref_object()\n    except DefinitionError as e:\n        raise e\n    assert str(ast) == \"f(arg)\"\n\n    parser = DefinitionParser(\"f(int arg1, int arg2)\", location=None)\n    try:\n        ast = parser.parse_xref_object()\n    except DefinitionError as e:\n        raise e\n    assert str(ast) == \"f(arg1, arg2)\"\n", "def test_xref_object_with_template():\n        parser = DefinitionParser(target, location=None,\n                                  config=None)\n        ast = parser.parse_xref_object()\n        parser.assert_end()\n        assert ast.names[0].identifier == 'A'\n        assert len(ast.names) == 1\n    check('A<T>')\n    check('A<int>')\n    check('A<int, double>')\n    check('A<int, double, char>')\n", "def test_template_args_substitution():\n    check('function',\n          \"template<typename T> \"\n          \"void f(std::vector<T> v, typename std::vector<T>::iterator it)\",\n          {2: \"I0E1fNSt6vectorIT_EE3itE\",\n           3: \"I0E1fNSt6vectorIT_EE8iteratorEE3itE\",\n           4: \"I0E1fNSt6vectorIT_EE8iteratorEE3itE\"})\n    check('function',\n          \"template<typename T> \"\n          \"void f(std::vector<T> v, typename std::vector<T>::iterator::value_type x)\",\n          {2: \"I0E1fNSt6vectorIT_EE3itE11value_typeE\",\n           3: \"I0E1fNSt6vectorIT_EE8iteratorE11value_typeE\",\n           4: \"I0E1fNSt6vectorIT_EE8iteratorE11value_typeE\"})\n", "def test_breath_test_fixtures():\n    # from breathe#293\n    check('class', 'void f(Comma, Comma, Comma)', {1: 'f__Comma.Comma.Comma', 2: '1fN5CommaES1_ES1_E'})\n    check('class', 'void f(Comma<1, 2, 3>, Comma<4, 5, 6>, Comma<7, 8, 9>)',\n          {2: '1fN5CommaIJiLi2EEES1_JiLi2EEES1_JiLi2EEE'})\n    check('class', 'void f(Comma<>, Comma<>, Comma<>)', {2: '1fN5CommaIJEEEES1_EES1_E'})\n    # from breathe#311\n    check('class', 'void f(This, This, This)', {1: 'f__This.This.This', 2: '1fN4ThisES4_ThisES4_ThisE'})\n    check('class', 'void f(This<int, int, int>, This<int, int, int>, This<int, int, int>)',\n          {2: '1fN4ThisIJiLi2EEES4_JiLi2EEES4_JiLi2EEE'})\n    check('class', 'void f(This<>, This<>, This<>)', {2: '1fN4ThisIJEEEES4_EES4_E'})\n    # from breathe#323\n    check('class', 'void f(std::pair<int, int>)', {1: 'f__std::pair:int.int:', 2: '1fNSt4pairIiE'})\n    check('class', 'void f(std::pair<int, int>)', {1: 'f__std::pair:int.int:', 2: '1fNSt4pairIiE'})\n    # from breathe#403\n    check('class', 'void f(A<>, B<>, C<>)', {1: 'f__A.B.C', 2: '1fN1AEE1BEE1CEE'})\n    check('class', 'void f(A<1>, B<2>, C<3>)', {1: 'f__A.B.C', 2: '1fN1AI1EE1"], "sample_132": ["    def test_technical_500_response_request_is_none(self):\n        \"\"\"Test technical_500_response when request is None\"\"\"\n        try:\n            raise ValueError(\"Can't find my keys\")\n        except ValueError:\n            exc_type, exc_value, tb = sys.exc_info()\n        response = technical_500_response(None, exc_type, exc_value, tb)\n        self.assertEqual(response.status_code, 500)\n        self.assertEqual(response['Content-Type'], 'text/plain; charset=utf-8')\n", "    def test_technical_500_response(self):\n        \"\"\"\n        Test the technical_500_response function.\n        \"\"\"\n        request = RequestFactory().get('/test_view/')\n        try:\n            raise ValueError(\"Test exception\")\n        except ValueError:\n            exc_type, exc_value, tb = sys.exc_info()\n        response = technical_500_response(request, exc_type, exc_value, tb)\n        self.assertEqual(response.status_code, 500)\n        self.assertIn('Technical 500', str(response.content))\n", "    def test_technical_404_response(self):\n        \"\"\"\n        The technical_404_response function should return a HttpResponseNotFound with a HTML page.\n        \"\"\"\n        request = RequestFactory().get('/technical404/')\n        exception = Http404('Page not found')\n        response = technical_404_response(request, exception)\n        self.assertIsInstance(response, HttpResponseNotFound)\n        self.assertEqual(response.status_code, 404)\n        self.assertInHTML('<h1>Page not found</h1>', response.content.decode())\n        self.assertContains(response, 'Technical 404 error', status_code=404)\n", "    def test_invalid_accept_header(self):\n        request = self.rf.get('/test_view/', HTTP_ACCEPT='*/*;q=0')\n        exc_type, exc_value, tb = sys.exc_info()\n        response = technical_500_response(request, exc_type, exc_value, tb)\n        self.assertEqual(response.status_code, 500)\n        self.assertEqual(response['Content-Type'], 'text/plain; charset=utf-8')\n", "    def test_sensitive_variables(self):\n        reporter_filter = SafeExceptionReporterFilter()\n        # Non-sensitive variable's value is shown.\n        self.assertEqual(reporter_filter.cleanse_special_types(None, 'non_sensitive'), 'non_sensitive')\n        # Sensitive variable's value is not shown.\n        self.assertEqual(reporter_filter.cleanse_special_types(None, 'sensitive'), reporter_filter.cleansed_substitute)\n"], "sample_731": ["def test_fetch_california_housing_metadata():\n    data = fetch()\n    assert data.DESCR.startswith(\"California housing dataset.\")\n    assert len(data.feature_names) == 8\n    assert data.feature_names == [\"MedInc\", \"HouseAge\", \"AveRooms\", \"AveBedrms\",\n                                 \"Population\", \"AveOccup\", \"Latitude\", \"Longitude\"]\n", "def test_fetch_non_default_data_home(tmpdir):\n    \"\"\"Test that fetch_california_housing can download to a non-default data home.\"\"\"\n    data_home = str(tmpdir.mkdir(\"california_housing_data\"))\n    try:\n        data = fetch_california_housing(data_home=data_home)\n    except IOError:\n        raise SkipTest(\"California housing dataset can not be loaded.\")\n    assert((20640, 8) == data.data.shape)\n    assert((20640, ) == data.target.shape)\n    assert exists(data_home)\n", "def test_feature_names():\n    data = fetch()\n    expected_feature_names = [\"MedInc\", \"HouseAge\", \"AveRooms\", \"AveBedrms\",\n                              \"Population\", \"AveOccup\", \"Latitude\", \"Longitude\"]\n    assert data.feature_names == expected_feature_names\n", "def test_fetch_california_housing_download_if_missing():\n    from sklearn.datasets import fetch_california_housing\n    from sklearn.datasets import get_data_home\n    from os import path\n    import shutil\n\n    # remove the dataset if it already exists\n    data_home = get_data_home()\n    data_dir = path.join(data_home, 'cal_housing')\n    if path.exists(data_dir):\n        shutil.rmtree(data_dir)\n\n    # test that the data is downloaded when download_if_missing is True\n    data = fetch_california_housing(download_if_missing=True)\n    assert((20640, 8) == data.data.shape)\n    assert((20640, ) == data.target.shape)\n\n    # test that the data is loaded from cache when download_if_missing is True\n    data = fetch_california_housing(download_if_missing=True)\n    assert((20640, 8) == data.data.shape)\n    assert((20640, ) == data.target.shape)\n\n    # test that the data raises an error when download_if_missing is False\n    try:\n        shutil.rmtree(data_dir)\n        fetch_california_housing(download_if_missing=False)\n        assert False\n    except IOError:\n        assert True\n", "def test_fetch_with_download():\n    try:\n        data_home = fetch_california_housing(download_if_missing=True).data\n    except Exception:\n        raise SkipTest(\"Failed to download California housing dataset.\")\n    assert exists(data_home)\n\n    # Remove the downloaded data for cleanup\n    import shutil\n    shutil.rmtree(data_home)\n"], "sample_603": ["def test_collapsible_section():\n    formatted = fh.collapsible_section(\"Test Section\", \"inline details\", \"section details\")\n    assert formatted.startswith(\"<input id='section-\")\n    assert \"Test Section\" in formatted\n    assert \"inline details\" in formatted\n    assert \"section details\" in formatted\n\n    formatted = fh.collapsible_section(\n        \"Test Section\", \"inline details\", \"section details\", n_items=5\n    )\n    assert \"(5)\" in formatted\n\n    formatted = fh.collapsible_section(\n        \"Test Section\", \"inline details\", \"section details\", enabled=False\n    )\n    assert \"disabled\" in formatted\n\n    formatted = fh.collapsible_section(\n        \"Test Section\", \"inline details\", \"section details\", collapsed=True\n    )\n    assert \"checked\" in formatted\n", "def test_collapsible_section():\n    name = \"Test Section\"\n    inline_details = \"Inline details\"\n    details = \"Detailed information\"\n    n_items = 10\n    enabled = True\n    collapsed = False\n\n    formatted = fh.collapsible_section(\n        name, inline_details, details, n_items, enabled, collapsed\n    )\n    assert name in formatted\n    assert inline_details in formatted\n    assert details in formatted\n    assert str(n_items) in formatted\n    assert \"class='xr-section-summary-in' type='checkbox'\" in formatted\n    assert \"checked\" not in formatted\n\n    formatted = fh.collapsible_section(\n        name, inline_details, details, n_items, enabled, not collapsed\n    )\n    assert \"checked\" in formatted\n\n    formatted = fh.collapsible_section(\n        name, inline_details, details, n_items, not enabled, collapsed\n    )\n    assert \"disabled\" in formatted\n", "def test_collapsible_section():\n    formatted = fh.collapsible_section(\"Dimensions\", n_items=10, enabled=True)\n    assert \"class='xr-section-summary-in' type='checkbox'\" in formatted\n    assert \"title='Expand/collapse section'\" in formatted\n\n    formatted = fh.collapsible_section(\"Dimensions\", n_items=10, enabled=False)\n    assert \"disabled\" in formatted\n\n    formatted = fh.collapsible_section(\"Dimensions\", n_items=0, enabled=True)\n    assert \"disabled\" in formatted\n\n    formatted = fh.collapsible_section(\"Dimensions\", n_items=None, enabled=True)\n    assert \"class='xr-section-summary-in' type='checkbox'\" in formatted\n", "def test_collapsible_section_with_no_items():\n    name = \"Test Section\"\n    n_items = 0\n    section = fh.collapsible_section(name, n_items=n_items)\n    assert section.count(\"disabled\") == 2\n    assert section.count(\"checked\") == 1\n", "def test_collapsible_section():\n    name = \"test\"\n    inline_details = \"inline\"\n    details = \"details\"\n    n_items = 5\n    enabled = True\n    collapsed = True\n    formatted = fh.collapsible_section(name, inline_details, details, n_items, enabled, collapsed)\n    assert f\"<input id='section-{uuid.UUID('')\" in formatted\n    assert f\"{name}:\" in formatted\n    assert f\"<span>({n_items})</span>\" in formatted\n    assert \"<label for=\" in formatted\n    assert \"<div class='xr-section-inline-details'>\" in formatted\n    assert \"<div class='xr-section-details'>\" in formatted\n\n    # Test collapsible section with disabled option\n    enabled = False\n    formatted = fh.collapsible_section(name, inline_details, details, n_items, enabled, collapsed)\n    assert \"disabled\" in formatted\n\n    # Test collapsible section without items\n    n_items = 0\n    formatted = fh.collapsible_section(name, inline_details, details, n_items, enabled, collapsed)\n    assert \"disabled\" in formatted\n"], "sample_935": ["def test_type_alias():\n    check('type', '{key}T = int', {1: \"T\", 2: \"1T\"}, key='using')\n    check('type', 'using {key}T = int', {1: \"T\", 2: \"1T\"}, key='using')\n    check('type', '{key}T = int* volatile const', {1: \"T\", 2: \"1T\"}, key='using')\n    check('type', '{key}T = int* volatile const*', {1: \"T\", 2: \"1T\"}, key='using')\n    check('type', '{key}T = std::vector<int>', {1: \"T\", 2: \"5T\"}, key='using')\n    check('type', '{key}T = std::vector<std::pair<std::string, int>>', {1: \"T\", 2: \"5T\"},\n          key='using')\n", "def test_trailing_returns():\n    check('function', 'int f() -> void', {1: \"f\", 2: \"1f\"})\n    check('function', 'int f() -> int', {1: \"f\", 2: \"1f\"})\n    check('function', 'void f() -> int', {1: \"f\", 2: \"1f\"})\n    check('function', 'int f() -> double', {1: \"f\", 2: \"1f\"})\n    check('function', 'int f() -> const int', {1: \"f\", 2: \"1f\"})\n    check('function', 'int f() -> volatile int', {1: \"f\", 2: \"1f\"})\n    check('function', 'int f() -> const volatile int', {1: \"f\", 2: \"1f\"})\n    check('function', 'int f() -> volatile const int', {1: \"f\", 2: \"1f\"})\n    check('function', 'int f() -> int *', {1: \"f\", 2: \"1f\"})\n    check('function', 'int f() -> int &', {1: \"f\", 2: \"1f\"})\n    check('function', 'int f() -> int &&', {1: \"f\", 2: \"1f\"})\n    check('function', 'int f() -> int[2]', {1: \"f\", 2: \"1f\"})\n    check('function', 'int f() -> int[2][3]', {1: \"f\", 2: \"1f\"})\n    check('function', 'int f() -> int * *', {1: \"f\", 2: \"1f\"})\n    check('function', 'int f() -> int * const', {1: \"f\", 2: \"1f\"})\n    check('function', 'int f() -> int * volatile', {1: \"f\", 2: \"1f\"})\n    check('function', 'int f() -> int * const volatile', {1: \"f\", 2: \"1f\"})\n    check('function', 'int f() -> int * volatile const', {1: \"f\", 2: \"1f\"})\n    check('function', 'auto f() -> int *', {1: \"f\", 2: \"1f\"})\n", "def test_template_introductions_with_parameter_packs():\n    check('class', 'template<typename... Ts> requires X<Ts...> {key}A',\n          {4: 'IDpEIQaa1A'})\n    check('class', 'template<typename... Ts> requires (X<Ts> && ...) {key}A',\n          {4: 'IDpEIQaa1A'})\n    check('class', 'template<typename... Ts> requires (X<Ts> || ...) {key}A',\n          {4: 'IDpEIQoo1Aoo1XIDpE'})\n    check('class', 'template<typename T, typename... Ts> requires (X<T> && ... && X<Ts> && ...) {key}A',\n          {4: 'I0EIDpEIQaa1X1Taa1A'})\n    check('class', 'template<typename T, typename... Ts> requires (X<T> || ... || X<Ts> || ...) {key}A',\n          {4: 'I0EIDpEIQoo1X1Too1Aoo1XIDpE'})\n", "def test_expression_parsing():\n        ids = 'IE1CIA%s_1aE'\n        check('class', 'template<> {key}C<a[%s]>' % expr, {2: ids % expr.format(), 3: ids % id})\n        class Config:\n            cpp_id_attributes = [\"id_attr\"]\n            cpp_paren_attributes = [\"paren_attr\"]\n        parser = DefinitionParser(expr, location=None,\n                                  config=Config())\n        parser.allowFallbackExpressionParsing = False\n        ast = parser.parse_expression()\n        res = str(ast)\n        if res != expr:\n            print(\"\")\n            print(\"Input:    \", expr)\n            print(\"Result:   \", res)\n            raise DefinitionError(\"\")\n        displayString = ast.get_display_string()\n        if res != displayString:\n            # note: if the expression contains an anon name then this will trigger a falsely\n            print(\"\")\n            print(\"Input:    \", expr)\n            print(\"Result:   \", res)\n            print(\"Display:  \", displayString)\n            raise DefinitionError(\"\")\n\n    exprCheck('a->*b', 'pm1a1b')\n    exprCheck('a->*b->*c', 'pm1a1b1c')\n    exprCheck('a->*b->*c->*d', 'pm1a1b1c1d')\n    exprCheck('a.b->*b', 'dt1a1b1b')\n    exprCheck('a->*b[2]', 'ixpm1a1bL2E')\n    exprCheck('a->*b[2][3]', 'ixixpm1a1bL2EL3E')\n    exprCheck('a->*b[c]', 'ixpm1a1b1c')\n    exprCheck('a->*b[c][3]', 'ixixpm1a1b1cL3E')\n    exprCheck('a->*b[c][3][d]', 'ixixixpm1a1b1cL3ELd')\n    exprCheck('a->*b[c][3][d]', 'ixixixpm1a1b1cL3ELd')\n    exprCheck('a->*b[c]', 'ixpm1a1b1c')\n    exprCheck('a->*b[c][3]', 'ixixpm1a1b1cL3E')\n", "def test_template_introductions():\n    # a template introduction with 1 template parameter\n    # should also work with 2 template parameters\n    # from #3235\n    check('class', 'template<template<auto> class T> {key}A',\n          {2: 'II0E0E1A'})\n    check('class', 'template<template<auto> T> {key}A',\n          {2: 'II0E0E1A'})\n    check('class', 'template<template<auto&> class T> {key}A',\n          {2: 'II0ER0E1A'})\n    check('class', 'template<template<auto&> T> {key}A',\n          {2: 'II0ER0E1A'})\n\n    # template introduction with template template parameters\n    # from #3339\n    check('class', 'template<template<template<auto> class> class T> {key}A',\n          {2: 'II0I0E0E1A'})\n    check('class', 'template<template<template<auto> T> class T> {key}A',\n          {2: 'II0I0E0E1A'})\n\n    # template introductions with a parameter pack\n    # from #3692\n    check('class', 'template<template<auto...> class T> {key}A',\n          {2: 'II0EDpE1A'})\n    check('class', 'template<template<auto&...> class T> {key}A',\n          {2: 'II0ERDpE1A'})\n    check('class', 'template<template<auto*...> class T> {key}A',\n          {2: 'II0EPDpE1A'})\n    check('class', 'template<template<auto* const...> class T> {key}A',\n          {2: 'II0EPKDpE1A'})\n    check('class', 'template<template<auto* volatile...> class T> {key}A',\n          {2: 'II0EPVDpE1A'})\n    check('class', 'template<template<auto* const volatile...> class T> {key}A',\n          {2: 'II0EPVKDpE1A'})\n\n    # template introductions with a parameter pack and default"], "sample_923": ["def test_struct_definitions():\n    check('struct', '{key}A', {1: \"A\", 2: \"1A\"}, output='{key}A')\n    check('struct', 'public {key}A', {1: \"A\", 2: \"1A\"})\n    check('struct', '{key}A final', {1: 'A', 2: '1A'})\n\n    # test bases\n    check('struct', '{key}A', {1: \"A\", 2: \"1A\"})\n    check('struct', '{key}A : B', {1: \"A\", 2: \"1A\"})\n    check('struct', '{key}A : private B', {1: \"A\", 2: \"1A\"})\n    check('struct', '{key}A : public B', {1: \"A\", 2: \"1A\"})\n    check('struct', '{key}A : B, C', {1: \"A\", 2: \"1A\"})\n    check('struct', '{key}A : B, protected C, D', {1: \"A\", 2: \"1A\"})\n    check('struct', 'A : virtual private B', {1: 'A', 2: '1A'}, output='{key}A : private virtual B')\n    check('struct', '{key}A : private virtual B', {1: 'A', 2: '1A'})\n    check('struct', '{key}A : B, virtual C', {1: 'A', 2: '1A'})\n    check('struct', '{key}A : public virtual B', {1: 'A', 2: '1A'})\n    check('struct', '{key}A : B..., C', {1: 'A', 2: '1A'})\n\n    # from #4094\n    check('struct', 'template<class, class = std::void_t<>> {key}has_var', {2: 'I00E7has_var'})\n    check('struct', 'template<class T> {key}has_var<T, std::void_t<decltype(&T::var)>>',\n          {2: 'I0E7has_varI1TNSt6void_tIDTadN1T3varEEEEE'})\n\n    check('struct', 'template<typename ...Ts> {key}T<int (*)(Ts)...>',\n          {", "def test_c_domain():\n    # Test the C domain\n    class Config:\n        c_id_attributes = [\"id_attr\"]\n        c_paren_attributes = [\"paren_attr\"]\n    parser = DefinitionParser(\"int f()\", location=None, config=Config())\n    ast = parser.parse_declaration(\"function\", \"function\")\n    assert str(ast) == \"int f()\"\n", "def test_arrays():\n    check('member', 'A a[2]', {1: 'a__A', 2: '1a'})\n    check('member', 'A a[2][3]', {1: 'a__A', 2: '1a'})\n    check('member', 'A a[2][3][4]', {1: 'a__A', 2: '1a'})\n    check('member', 'A a[2][3][4][5]', {1: 'a__A', 2: '1a'})\n    check('member', 'A a[2][3][4][5][6]', {1: 'a__A', 2: '1a'})\n    check('member', 'A a[2][3][4][5][6][7]', {1: 'a__A', 2: '1a'})\n    check('member', 'A a[2][3][4][5][6][7][8]', {1: 'a__A', 2: '1a'})\n    check('member', 'A a[2][3][4][5][6][7][8][9]', {1: 'a__A', 2: '1a'})\n    check('member', 'A a[2][3][4][5][6][7][8][9][10]', {1: 'a__A', 2: '1a'})\n\n    check('member', 'A a[2] = 42', {1: 'a__A', 2: '1a'})\n    check('member', 'A a[2][3] = 42', {1: 'a__A', 2: '1a'})\n    check('member', 'A a[2][3][4] = 42', {1: 'a__A', 2: '1a'})\n    check('member', 'A a[2][3][4][5] = 42', {1: 'a__A', 2: '1a'})\n    check('member', 'A a[2][3][4][5][6] = 42', {1: 'a__A', 2: '1a'})\n    check('member', 'A a[2][3][4][5][6][7] = 42', {1: '", "def test_template_member_declarations():\n    check('member', 'template<> T::A<int>::a', {2: 'IEN1A1AIXiE1aE'})\n    check('member', 'template T::A<int>::a', {2: 'IEN1A1AIXiE1aE'},\n          output='template<> T::A<int>::a')  # same as above\n    check('member', 'template<> template<> T::A<int>::B<int>::b', {2: 'IEIEN1A1AIXiE1BIiE1bE'})\n    check('member', 'template T::A<int>::B<int>::b', {2: 'IEIEN1A1AIXiE1BIiE1bE'},\n          output='template<> template<> T::A<int>::B<int>::b')  # same as above\n", "def test_no_match_symbols():\n        parser = DefinitionParser(name, location=None, config=None)\n        try:\n            name = parser.parse_xref_object()\n        except DefinitionError as e:\n            logger.warning('Unparseable C cross-reference: %r\\n%s', name, e)\n            return\n        parentKey = LookupKey([])\n        rootSymbol = Symbol(None, None, None, None)\n        s = rootSymbol.direct_lookup(parentKey)\n        assert s is None\n\n    make_test('::')\n    make_test('::a')\n    make_test('a::')\n    make_test('a::b')\n    make_test('::a::b')\n    make_test('::a::b::c')\n    make_test('a::<int>')\n    make_test('::a::<int>')\n    make_test('a::<int>::b')\n    make_test('::a::<int>::b')\n    make_test('a::<int>::b::c')\n    make_test('::a::<int>::b::c')\n"], "sample_302": ["    def setUp(self):\n        self.client = DatabaseClient(connection=connection)\n", "def test_settings_to_cmd_args_env(self):\n    from django.db.backends.postgresql.client import DatabaseClient\n    client = DatabaseClient(connection=connection)\n    settings_dict = {\n        'ENGINE': 'django.db.backends.postgresql',\n        'NAME': 'mydatabase',\n        'USER': 'mydatabaseuser',\n        'PASSWORD': 'mypassword',\n        'HOST': '127.0.0.1',\n        'PORT': '5432',\n    }\n    options = {\n        'service': 'my_service',\n        'sslmode': 'require',\n        'sslrootcert': '/path/to/ssl/root/cert',\n        'sslcert': '/path/to/ssl/cert',\n        'sslkey': '/path/to/ssl/key',\n        'passfile': '/path/to/passfile',\n    }\n    settings_dict['OPTIONS'] = options\n    parameters = ['-c', 'SELECT 1;']\n    expected_args = ['psql', '-U', 'mydatabaseuser', '-h', '127.0.0.1', '-p', '5432', 'mydatabase', '-c', 'SELECT 1;']\n    expected_env = {\n        'PGPASSWORD': 'mypassword',\n        'PGSERVICE': 'my_service',\n        'PGSSLMODE': 'require',\n        'PGSSLROOTCERT': '/path/to/ssl/root/cert',\n        'PGSSLCERT': '/path/to/ssl/cert',\n        'PGSSLKEY': '/path/to/ssl/key',\n        'PGPASSFILE': '/path/to/passfile',\n    }\n    args, env = client.settings_to_cmd_args_env(settings_dict, parameters)\n    self.assertEqual(args, expected_args)\n    self.assertEqual(env, expected_env)\n", "    def setUp(self):\n        self.client = DatabaseClient(connection=connection)\n        self.settings_dict = {\n            'HOST': 'localhost',\n            'PORT': 5432,\n            'NAME': 'mydb',\n            'USER': 'myuser',\n            'PASSWORD': 'mypassword',\n            'OPTIONS': {\n                'passfile': '/path/to/passfile',\n                'service': 'myservice',\n                'sslmode': 'verify-full',\n                'sslrootcert': '/path/to/sslrootcert',\n                'sslcert': '/path/to/sslcert',\n                'sslkey': '/path/to/sslkey',\n            }\n        }\n        self.parameters = ['-c', 'SELECT 1']\n", "    def setUp(self):\n        self.client = DatabaseClient(connection=connection)\n", "    def test_settings_to_cmd_args_env(self):\n        # Test with all options set\n        settings_dict = {\n            'HOST': 'localhost',\n            'PORT': 5432,\n            'NAME': 'mydb',\n            'USER': 'myuser',\n            'PASSWORD': 'mypassword',\n            'OPTIONS': {\n                'passfile': '/path/to/passfile',\n                'service': 'myservice',\n                'sslmode': 'require',\n                'sslrootcert': '/path/to/sslrootcert',\n                'sslcert': '/path/to/sslcert',\n                'sslkey': '/path/to/sslkey',\n            },\n        }\n        parameters = ['--flag1', '--flag2']\n        expected_args = [\n            'psql', '-h', 'localhost', '-p', '5432', '-U', 'myuser', 'mydb',\n            '--flag1', '--flag2',\n        ]\n        expected_env = {\n            'PGPASSWORD': 'mypassword',\n            'PGSERVICE': 'myservice',\n            'PGSSLMODE': 'require',\n            'PGSSLROOTCERT': '/path/to/sslrootcert',\n            'PGSSLCERT': '/path/to/sslcert',\n            'PGSSLKEY': '/path/to/sslkey',\n            'PGPASSFILE': '/path/to/passfile',\n        }\n        client = DatabaseClient()\n        args, env = client.settings_to_cmd_args_env(settings_dict, parameters)\n        self.assertEqual(args, expected_args)\n        self.assertEqual(env, expected_env)\n"], "sample_732": ["def test_random_state():\n    try:\n        dataset1 = fetch_kddcup99(random_state=42, subset='SA', shuffle=True,\n                                  percent10=True, download_if_missing=False)\n        dataset2 = fetch_kddcup99(random_state=42, subset='SA', shuffle=True,\n                                  percent10=True, download_if_missing=False)\n    except IOError:\n        raise SkipTest(\"kddcup99 dataset can not be loaded.\")\n\n    assert_equal(dataset1.data, dataset2.data)\n    assert_equal(dataset1.target, dataset2.target)\n\n    try:\n        dataset3 = fetch_kddcup99(random_state=None, subset='SA', shuffle=True,\n                                  percent10=True, download_if_missing=False)\n    except IOError:\n        raise SkipTest(\"kddcup99 dataset can not be loaded.\")\n\n    assert_not_equal(dataset1.data, dataset3.data)\n    assert_not_equal(dataset1.target, dataset3.target)\n", "def test_fetch_kddcup99_random_state():\n    try:\n        dataset1 = fetch_kddcup99(random_state=42, subset='SA', shuffle=True,\n                                  percent10=True, download_if_missing=False)\n        dataset2 = fetch_kddcup99(random_state=42, subset='SA', shuffle=True,\n                                  percent10=True, download_if_missing=False)\n        dataset3 = fetch_kddcup99(random_state=43, subset='SA', shuffle=True,\n                                  percent10=True, download_if_missing=False)\n    except IOError:\n        raise SkipTest(\"kddcup99 dataset can not be loaded.\")\n\n    assert_equal(dataset1.data.shape, dataset2.data.shape)\n    assert_equal(dataset1.target.shape, dataset2.target.shape)\n    assert_equal(dataset1.data, dataset2.data)\n    assert_equal(dataset1.target, dataset2.target)\n\n    assert not np.array_equal(dataset1.data, dataset3.data)\n    assert not np.array_equal(dataset1.target, dataset3.target)\n", "def test_random_state():\n    try:\n        dataset = fetch_kddcup99(subset='SA', shuffle=True, random_state=42,\n                                 percent10=True, download_if_missing=False)\n    except IOError:\n        raise SkipTest(\"kddcup99 dataset can not be loaded.\")\n\n    try:\n        dataset2 = fetch_kddcup99(subset='SA', shuffle=True, random_state=42,\n                                  percent10=True, download_if_missing=False)\n    except IOError:\n        raise SkipTest(\"kddcup99 dataset can not be loaded.\")\n\n    assert_equal(dataset.data.shape, dataset2.data.shape)\n    assert_equal(dataset.target.shape, dataset2.target.shape)\n    assert_equal(dataset.data, dataset2.data)\n    assert_equal(dataset.target, dataset2.target)\n\n    try:\n        dataset3 = fetch_kddcup99(subset='SA', shuffle=True, random_state=0,\n                                  percent10=True, download_if_missing=False)\n    except IOError:\n        raise SkipTest(\"kddcup99 dataset can not be loaded.\")\n\n    assert_not_equal(dataset.data, dataset3.data)\n    assert_not_equal(dataset.target, dataset3.target)\n", "def test_fetch_kddcup99_download_if_missing():\n    try:\n        data = fetch_kddcup99(download_if_missing=False)\n    except IOError:\n        raise SkipTest(\"kddcup99 dataset can not be loaded.\")\n\n    # Test that the data is not re-downloaded when it already exists\n    data_home = fetch_kddcup99(data_home='./', download_if_missing=False)\n    assert_equal(data_home.data.shape, (494021, 41))\n    assert_equal(data_home.target.shape, (494021,))\n\n    # Test that the data is downloaded when download_if_missing is True\n    import shutil\n    shutil.rmtree('./', ignore_errors=True)\n    fetch_kddcup99(data_home='./', download_if_missing=True)\n    assert_equal(fetch_kddcup99(data_home='./', download_if_missing=False).data.shape, (494021, 41))\n    assert_equal(fetch_kddcup99(data_home='./', download_if_missing=False).target.shape, (494021,))\n", "def test_fetch_brute_kddcup99():\n    try:\n        data = fetch_kddcup99(download_if_missing=False, percent10=False)\n    except IOError:\n        raise SkipTest(\"kddcup99 dataset can not be loaded.\")\n\n    assert_equal(data.data.shape, (4898431, 41))\n    assert_equal(data.target.shape, (4898431,))\n\n    data_percent10 = fetch_kddcup99(download_if_missing=False, percent10=True)\n    assert_equal(data_percent10.data.shape, (494021, 41))\n    assert_equal(data_percent10.target.shape, (494021,))\n\n    # Test _fetch_brute_kddcup99 with custom data_home\n    custom_data_home = os.path.join(os.getcwd(), \"custom_data_home\")\n    os.mkdir(custom_data_home)\n    try:\n        data = fetch_kddcup99(download_if_missing=False, percent10=False, data_home=custom_data_home)\n    except IOError:\n        raise SkipTest(\"kddcup99 dataset can not be loaded.\")\n    finally:\n        os.rmdir(custom_data_home)\n\n    assert_equal(data.data.shape, (4898431, 41))\n    assert_equal(data.target.shape, (4898431,))\n\n    # Test _fetch_brute_kddcup99 with download_if_missing=False when data is not present\n    custom_data_home = os.path.join(os.getcwd(), \"custom_data_home\")\n    os.mkdir(custom_data_home)\n    try:\n        fetch_kddcup99(download_if_missing=False, percent10=False, data_home=custom_data_home)\n    except IOError:\n        pass\n    else:\n        raise AssertionError(\"IOError was not raised when data was not present.\")\n    finally:\n        os.rmdir(custom_data_home)\n"], "sample_575": ["    def x(self):\n        return pd.Series([\"a\", \"c\", \"b\", \"c\"], name=\"x\")\n", "def test_tick_locator_input_check(self, t):\n\n    err = \"Tick locator must be an instance of .*?, not <class 'tuple'>.\"\n    with pytest.raises(TypeError, match=err):\n        Temporal().tick((1, 2))\n", "def test_symlog_transform(self, x):\n\n    s = Continuous(trans=\"symlog\")._setup(x, Coordinate())\n    a = PseudoAxis(s._matplotlib_scale)\n    a.set_view_interval(-1050, 1050)\n    ticks = a.major.locator()\n    assert ticks[0] == -ticks[-1]\n    pos_ticks = np.sort(np.unique(np.abs(ticks)))\n    assert np.allclose(np.diff(np.log10(pos_ticks[1:])), 1)\n    assert pos_ticks[0] == 0\n", "def test_scale_priority(self):\n\n    scales = [\n        Nominal(),\n        Continuous(),\n        Temporal(),\n    ]\n    sorted_scales = sorted(scales, key=lambda x: x._priority)\n    assert sorted_scales == [Temporal(), Continuous(), Nominal()]\n", "    def x(self):\n        return pd.Series([1, 3, 9], name=\"x\", dtype=float)\n"], "sample_926": ["def test_nested_templates():\n    check('class', 'template <template <typename> class T> {key}A', {2: 'II0E0E1A'})\n    check('class', 'template <template <typename> class T> {key}A<T>', {2: 'II0E0E1AT'})\n    check('class', 'template <template <typename> typename T> {key}A', {2: 'II0E0E1A'})\n    check('class', 'template <template <typename> typename T> {key}A<T>', {2: 'II0E0E1AT'})\n    check('class', 'template <template <typename> template <typename> typename T> {key}A', {2: 'III0E0E0E1A'})\n    check('class', 'template <template <typename> template <typename> typename T> {key}A<T>', {2: 'III0E0E0E1AT'})\n    check('class', 'template <template <typename> class T, typename U> {key}A', {2: 'II0E0E1AE'})\n    check('class', 'template <template <typename> typename T, typename U> {key}A', {2: 'II0E0E1AE'})\n    check('class', 'template <typename T, template <typename> typename U> {key}A', {2: 'I0EII0E0E1AE'})\n    check('class', 'template <typename T, template <typename> class U> {key}A', {2: 'I0EII0E0E1AE'})\n", "def test_enum_definitions():\n    # check when the enum is not in a scope\n    check('enum', 'enum class A : unsigned int : public {key}B',\n          {2: \"1B\"},\n          output='{key}B')\n    # check when the enum is in a scope\n    check('enum', 'enum class A : unsigned int : public A::{key}B',\n          {2: \"N1A1BE\"},\n          output='A::{key}B')\n", "def test_attributes_multiple():\n    # multiple attributes\n    check('member', '[[a, b]] int f', {1: 'f__i', 2: '1f'})\n    check('member', '__attribute__((a, b)) int f', {1: 'f__i', 2: '1f'})\n    check('member', '[[a]] __attribute__((b)) int f', {1: 'f__i', 2: '1f'})\n    check('member', '__attribute__((a)) [[b]] int f', {1: 'f__i', 2: '1f'})\n    check('member', '[[a]] [[b]] int f', {1: 'f__i', 2: '1f'})\n    check('member', '__attribute__((a)) __attribute__((b)) int f', {1: 'f__i', 2: '1f'})\n    # multiple attributes in different positions\n    check('function', '[[a]] void f() [[b]]', {1: 'f', 2: '1fv'})\n    check('function', '__attribute__((a)) void f() __attribute__((b))', {1: 'f', 2: '1fv'})\n    # multiple attributes on declarators\n    check('member', 'int *[[a]] volatile i', {1: 'i__iPVC', 2: '1i'},\n          output='int *[[a]] volatile const i')\n    check('member', 'int *[[a]] [[b]] volatile i', {1: 'i__iPVC', 2: '1i'},\n          output='int *[[a]] [[b]] volatile const i')\n    check('member', 'int *volatile [[a]] i', {1: 'i__iPVC', 2: '1i'},\n          output='int *[[a]] volatile const i')\n    check('member', 'int *volatile [[a]] [[b]] i', {1: 'i__iPVC', 2: '1i'},\n          output='int *[[a]] [[b]] volatile const i')\n    check('member', 'int *[[a]] [[b]] volatile [[c]] i', {1: 'i__iPVC', 2: '1i'},\n          output='int *[[a]] [[b]] [[c]] volatile const", "def test_xref_object_parsing():\n        parser = DefinitionParser(target, location=None, config=None)\n        try:\n            name = parser.parse_xref_object()\n        except DefinitionError as e:\n            print(\"\")\n            print(\"Input:    \", target)\n            print(\"Error:    \", e)\n            raise\n        parser.assert_end()\n        # TODO: check name is what we expect?\n        # print(name)\n\n    check('f')\n    check('f()')\n    check('void f()')\n    check('T f()')\n    check('ns::f()')\n    check('::ns::f()')\n    check('ns::T f()')\n    check('::ns::T f()')\n    check('ns::T::f()')\n    check('::ns::T::f()')\n    check('ns::T::f<int>()')\n    check('::ns::T::f<int>()')\n    check('ns::T::f<int, double>()')\n    check('::ns::T::f<int, double>()')\n    check('ns::T::f<int, double, char>()')\n    check('::ns::T::f<int, double, char>()')\n    check('ns::T::f<int, double, char...>()')\n    check('::ns::T::f<int, double, char...>()')\n    check('ns::T::f<>()')\n    check('::ns::T::f<>()')\n    check('ns::T::f<int>()')\n    check('::ns::T::f<int>()')\n    check('ns::T::f<>()')\n    check('::ns::T::f<>()')\n    check('ns::T::f<int>()')\n    check('::ns::T::f<int>()')\n    check('ns::T::f<int>()')\n    check('::ns::T::f<int>()')\n    check('ns::T::f<int>()')\n    check('::ns::T::f<int>()')\n    check('ns::T::f<int>()')\n    check('::ns::T::f<int>()')\n    check('ns::T::f<int>()')\n    check('::ns::T::f<int>()')\n    check('ns::T::f<int>()')\n    check('::ns::T::f<int>()')\n    check('ns::T::f<int>()')\n    check('::ns", "def test_name_prefix():\n    # 'C' should be used as the domain prefix.\n    check(\"member\", \"int ::f\", {1: \"f__i\", 2: \"1f\"},\n          output='int ::f')\n    # 'c' should not be used as the domain prefix.\n    check(\"member\", \"int c::f\", {1: \"c::f__i\", 2: \"1c1f\"},\n          output='int c::f')\n    # Check that the prefix is not changed even when a domain prefix is explicitly given.\n    check(\"member\", \"int C::f\", {1: \"C::f__i\", 2: \"1C1f\"},\n          output='int C::f')\n    check(\"member\", \"int c::f\", {1: \"c::f__i\", 2: \"1c1f\"},\n          output='int c::f')\n"], "sample_279": ["    def test_clone(self):\n        check_constraint = models.CheckConstraint(\n            check=models.Q(price__gt=models.F('discounted_price')),\n            name='price_gt_discounted_price',\n        )\n        cloned_constraint = check_constraint.clone()\n        self.assertEqual(check_constraint, cloned_constraint)\n        self.assertIsNot(check_constraint, cloned_constraint)\n\n        unique_constraint = models.UniqueConstraint(\n            fields=['name', 'color'],\n            name='name_color_uniq',\n        )\n        cloned_constraint = unique_constraint.clone()\n        self.assertEqual(unique_constraint, cloned_constraint)\n        self.assertIsNot(unique_constraint, cloned_constraint)\n", "    def test_empty_fields(self):\n        message = 'At least one field is required to define a unique constraint.'\n        with self.assertRaisesMessage(ValueError, message):\n            models.UniqueConstraint(fields=[], name='empty')\n", "def test_unique_constraint_fields_length_zero(self):\n    with self.assertRaisesMessage(ValueError, 'At least one field is required to define a unique constraint.'):\n        models.UniqueConstraint(fields=[], name='unique')\n", "    def test_check_constraint_clone(self):\n        check = models.Q(price__gt=models.F('discounted_price'))\n        name = 'price_gt_discounted_price'\n        constraint = models.CheckConstraint(check=check, name=name)\n        cloned_constraint = constraint.clone()\n        self.assertEqual(constraint, cloned_constraint)\n        self.assertIsNot(constraint, cloned_constraint)\n        self.assertEqual(constraint.check, cloned_constraint.check)\n        self.assertIsNot(constraint.check, cloned_constraint.check)\n", "def test_unique_constraint_with_non_string_field_names(self):\n    class MyModel(models.Model):\n        field_1 = models.IntegerField()\n        field_2 = models.CharField(max_length=255)\n\n        class Meta:\n            constraints = [\n                models.UniqueConstraint(fields=['field_1', 'field_2'], name='unique_fields')\n            ]\n\n    MyModel.objects.create(field_1=1, field_2='value')\n    with self.assertRaises(IntegrityError):\n        MyModel.objects.create(field_1=1, field_2='value')\n"], "sample_611": ["def test_cftime_range_monthly_offsets(calendar, start, end, freq, expected_len):\n    result = cftime_range(start, end, freq=freq, calendar=calendar)\n    assert len(result) == expected_len\n", "def test_to_offset_annual_quarter_anchor(calendar):\n    expected = QuarterBegin(month=3, n=4)\n    result = to_offset(\"4QS-MAR\")\n    assert result == expected\n", "def test_cftime_range_timestamps(calendar):\n    date_type = get_date_type(calendar)\n    start = date_type(2020, 1, 1, 12, 0, 0)\n    end = date_type(2020, 1, 1, 12, 0, 3)\n    expected = np.array(\n        [date_type(2020, 1, 1, 12, 0, 0), date_type(2020, 1, 1, 12, 0, 1), date_type(2020, 1, 1, 12, 0, 2), date_type(2020, 1, 1, 12, 0, 3)]\n    )\n    result = cftime_range(start, end, periods=4, freq=\"S\", calendar=calendar).values\n    np.testing.assert_array_equal(result, expected)\n", "def test_cftime_range_non_standard_calendars(calendar, start, end, freq, expected):\n    result = cftime_range(start, end, freq=freq, calendar=calendar).values[0]\n    assert result.isoformat() == expected\n", "def test_add_sub_monthly_microsecond(calendar, initial_date, offset, expected_date):\n    date_type = get_date_type(calendar)\n    initial = to_cftime_datetime(initial_date, calendar=calendar)\n    result = initial + to_offset(offset)\n    expected = to_cftime_datetime(expected_date, calendar=calendar)\n    assert result == expected\n"], "sample_1064": ["def test_tensorflow_max_min():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    expr = Max(x, y)\n    assert tensorflow_code(expr) == \"tensorflow.math.maximum(x, y)\"\n    _compare_tensorflow_scalar((x, y), expr)\n\n    expr = Min(x, y)\n    assert tensorflow_code(expr) == \"tensorflow.math.minimum(x, y)\"\n    _compare_tensorflow_scalar((x, y), expr)\n", "def test_tensorflow_power():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    expr = x ** 0.5\n    assert tensorflow_code(expr) == \"tensorflow.math.sqrt(x)\"\n    _compare_tensorflow_scalar((x,), expr, rng=lambda: random.random())\n\n    expr = x ** 2\n    assert tensorflow_code(expr) == \"tensorflow.math.pow(x, 2)\"\n    _compare_tensorflow_scalar((x,), expr, rng=lambda: random.random())\n\n    expr = x ** -2\n    assert tensorflow_code(expr) == \"tensorflow.math.pow(x, -2)\"\n    _compare_tensorflow_scalar((x,), expr, rng=lambda: random.random())\n", "def test_tensorflow_Pow_edge_cases():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    expr = x ** 0\n    assert tensorflow_code(expr) == \"tensorflow.math.pow(x, 0)\"\n    _compare_tensorflow_scalar((x,), expr)\n\n    expr = x ** 1\n    assert tensorflow_code(expr) == \"x\"\n    _compare_tensorflow_scalar((x,), expr)\n\n    expr = x ** 2\n    assert tensorflow_code(expr) == \"tensorflow.math.pow(x, 2)\"\n    _compare_tensorflow_scalar((x,), expr)\n", "def test_tensorflow_LogicalOperations():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    expr = sympy.And(x > 0, y < 0)\n    assert tensorflow_code(expr) == \\\n        \"tensorflow.math.logical_and(tensorflow.math.greater(x, 0), tensorflow.math.less(y, 0))\"\n    _compare_tensorflow_relational((x, y), expr, rng=lambda: random.randint(-10, 10))\n\n    expr = sympy.Or(x > 0, y < 0)\n    assert tensorflow_code(expr) == \\\n        \"tensorflow.math.logical_or(tensorflow.math.greater(x, 0), tensorflow.math.less(y, 0))\"\n    _compare_tensorflow_relational((x, y), expr, rng=lambda: random.randint(-10, 10))\n\n    expr = sympy.Not(x > 0)\n    assert tensorflow_code(expr) == \"tensorflow.math.logical_not(tensorflow.math.greater(x, 0))\"\n    _compare_tensorflow_relational((x,), expr, rng=lambda: random.randint(-10, 10))\n\n    expr = sympy.Max(x, y)\n    assert tensorflow_code(expr) == \"tensorflow.math.maximum(x, y)\"\n    _compare_tensorflow_scalar((x, y), expr)\n\n    expr = sympy.Min(x, y)\n    assert tensorflow_code(expr) == \"tensorflow.math.minimum(x, y)\"\n    _compare_tensorflow_scalar((x, y), expr)\n", "def test_tensorflow_Max_and_Min():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    expr = Max(x, y)\n    assert tensorflow_code(expr) == \"tensorflow.math.maximum(x, y)\"\n    _compare_tensorflow_scalar((x, y), expr)\n\n    expr = Min(x, y)\n    assert tensorflow_code(expr) == \"tensorflow.math.minimum(x, y)\"\n    _compare_tensorflow_scalar((x, y), expr)\n\n    expr = Max(x, y, z)\n    assert tensorflow_code(expr) == \\\n        \"tensorflow.math.maximum(tensorflow.math.maximum(x, y), z)\"\n    _compare_tensorflow_scalar((x, y, z), expr)\n\n    expr = Min(x, y, z)\n    assert tensorflow_code(expr) == \\\n        \"tensorflow.math.minimum(tensorflow.math.minimum(x, y), z)\"\n    _compare_tensorflow_scalar((x, y, z), expr)\n\n    expr = Max(x, y).subs({x: z})\n    assert tensorflow_code(expr) == \"tensorflow.math.maximum(z, y)\"\n    _compare_tensorflow_scalar((y, z), expr)\n\n    expr = Min(x, y).subs({x: z})\n    assert tensorflow_code(expr) == \"tensorflow.math.minimum(z, y)\"\n    _compare_tensorflow_scalar((y, z), expr)\n"], "sample_948": ["def test_template_type_params():\n    check('function', 'template<template<typename> typename T> void f(T)',\n          {2: 'I0E0E1fv', 4: 'I0E0E1fv'})\n    check('function', 'template<template<typename> typename T, typename U> void f(T, U)',\n          {2: 'I0E0E1I0E0E2fv', 4: 'I0E0E1I0E0E2fv'})\n    check('function', 'template<template<typename> typename T, typename U, typename V> void f(T, U, V)',\n          {2: 'I0E0E1I0E0E2I0E0E3fv', 4: 'I0E0E1I0E0E2I0E0E3fv'})\n    check('function', 'template<template<typename> typename T, template<typename> typename U> void f(T, U)',\n          {2: 'I0E0E1I0E0E2fv', 4: 'I0E0E1I0E0E2fv'})\n    check('function', 'template<template<typename> typename T, template<typename> typename U, typename V> void f(T, U, V)',\n          {2: 'I0E0E1I0E0E2I0E0E3fv', 4: 'I0E0E1I0E0E2I0E0E3fv'})\n    check('function', 'template<template<typename> typename T, template<typename> typename U, template<typename> typename V> void f(T, U, V)',\n          {2: 'I0E0E1I0E0E2I0E0E3fv', 4: 'I0E0E1I0E0E2I0E0E3fv'})\n\n    check('function', 'template<template<typename...> typename T> void f(T)',\n          {2: 'I0EDpE1fv', 4: 'I0EDpE1fv'})\n    check('function', 'template<template<typename...> typename T, typename U> void f(T, U)',\n          {2: 'I0EDpE1I0E0E2fv', 4: 'I0EDpE1", "def test_anonymous_structs_and_unions():\n    # Issue 845\n    check('class', 'struct { key }A', {1: 'A', 2: '1A'},\n          output='struct [anonymous] { key }A')\n    check('union', 'union { key }A', {1: 'A', 2: '1A'},\n          output='union [anonymous] { key }A')\n", "def test_template_function_types():\n    check('function', 'template<int A> void f()', {2: \"I_iE1fv\", 4: \"I_iE1fvv\"})\n    check('function', 'template<int A> void f(A)', {2: \"I_iE1fA\", 4: \"I_iE1fvA\"})\n    check('function', 'template<int A> void f(A, A)', {2: \"I_iE1fAA\", 4: \"I_iE1fvAA\"})\n    check('function', 'template<int A> void f(A, A, A)', {2: \"I_iE1fAAA\", 4: \"I_iE1fvAAA\"})\n    check('function', 'template<int A, int B> void f(A, B)', {2: \"I_iiE1fAB\", 4: \"I_iiE1fvAB\"})\n    check('function', 'template<int A, int B> void f(A, A)', {2: \"I_iiE1fAA\", 4: \"I_iiE1fvAA\"})\n    check('function', 'template<int A, int B> void f(A, B, A)', {2: \"I_iiE1fABA\", 4: \"I_iiE1fvABA\"})\n    check('function', 'template<int A, int B> void f(A, A, B)', {2: \"I_iiE1fAAB\", 4: \"I_iiE1fvAAB\"})\n    check('function', 'template<int A, int B> void f(A, B, B)', {2: \"I_iiE1fABB\", 4: \"I_iiE1fvABB\"})\n    check('function', 'template<int A, int B> void f(A, A, A, A)', {2: \"I_iiE1fAAAA\", 4: \"I_iiE1fvAAAA\"})\n    check('function', 'template<int A, int B, int C> void f(A, B, C)',\n          {2: \"I_iiiE1fABC\", 4: \"I_iiiE1fvABC\"})\n    check('function', 'template<int A, int B, int C, int D> void f(A, B, C, D)',\n          {2: \"I_", "def test_template_specialization():\n    check('class', 'template<> {key}A<int>', {2: 'IE1A1IiE'})\n    check('class', 'template<> {key}A<>{key}<int>', {2: 'IE1A1IiE'})\n    check('class', 'template<> {key}A<int> {key}<double>', {2: 'IE1A1IiE1IeE'})\n    check('class', 'template<> {key}A<int, double>', {2: 'IE1A1IiE1IeE'})\n    check('class', 'template<> {key}A<int, ...>', {2: 'IE1A1IiE'})\n    check('class', 'template<> {key}A<...>', {2: 'IE1A1IiE'})\n    check('class', 'template<> {key}A<int, int>', {2: 'IE1A1IiE1IiE'})\n    check('class', 'template<> {key}A<int, int, ...>', {2: 'IE1A1IiE1IiE'})\n    check('class', 'template<> {key}A<int, ...>', {2: 'IE1A1IiE'})\n    check('class', 'template<> {key}A<int, int, int>', {2: 'IE1A1IiE1IiE1IiE'})\n    check('class', 'template<> {key}A<>', {2: 'IE1A'})\n    check('class', 'template<> {key}A<,>', {2: 'IE1A'})\n    check('class', 'template<> {key}A<, int>', {2: 'IE1A1IiE'})\n    check('class', 'template<> {key}A<, ...>', {2: 'IE1A1IiE'})\n    check('class', 'template<> {key}A<, int, int>', {2: 'IE1A1IiE1IiE'})\n    check('class', 'template<> {key}A<, int, ...>', {2: 'IE1A1IiE'})\n    check('class', 'template<> {key}A<int, ,>', {2: 'IE1A'})\n    check('class", "def test_template_parameters_with_default_values():\n    # test default template parameter values are correctly handled\n    check('class', 'template<typename T = void> {key}A', {2: 'I0E1A'})\n    check('class', 'template<typename T = int> {key}A', {2: 'I0E1A'})\n    check('class', 'template<typename T = const char*> {key}A', {2: 'I0E1A'})\n    check('class', 'template<typename T = std::pair<int, float>> {key}A', {2: 'I0E1A'})\n\n    # test template parameter default values that are themselves templates\n    check('class', 'template<typename T = std::vector<int>> {key}A', {2: 'I0E1A'})\n    check('class', 'template<typename T = std::pair<std::vector<int>, std::vector<std::string>>> {key}A',\n          {2: 'I0E1A'})\n\n    # test template parameter default values with multiple template parameters\n    check('class', 'template<typename T = int, typename U = float> {key}A', {2: 'I00E1A'})\n    check('class', 'template<typename T = const char*, typename U = int> {key}A', {2: 'I00E1A'})\n\n    # test template parameter default values that are themselves templates\n    # with multiple template parameters\n    check('class', 'template<typename T = std::pair<std::vector<int>, std::vector<std::string>>, '\n          'typename U = std::vector<std::vector<std::string>>> {key}A', {2: 'I00E1A'})\n\n    # test function template with default value\n    check('function', 'template<typename T = void> void f(T)', {2: 'I0E1fv'})\n    check('function', 'template<typename T = int> void f(T)', {2: 'I0E1fv'})\n    check('function', 'template<typename T = const char*> void f(T)', {2: 'I0E1fv'})\n    check('function', 'template<typename T = std::pair<int, float>> void f(T)', {2: 'I0E1fv'})\n\n    # test template template parameter with default value\n    check('class', 'template<template<typename> typename"], "sample_1069": ["def test_glsl():\n    assert glsl_code(x**2) == 'pow(x, 2.0)'\n    assert glsl_code(sin(x)) == 'sin(x)'\n    assert glsl_code(cos(x)) == 'cos(x)'\n    assert glsl_code(tan(x)) == 'tan(x)'\n    assert glsl_code(acos(x)) == 'acos(x)'\n    assert glsl_code(asin(x)) == 'asin(x)'\n    assert glsl_code(atan(x)) == 'atan(x)'\n    assert glsl_code(atan2(y, x)) == 'atan(y, x)'\n    assert glsl_code(ceiling(x)) == 'ceil(x)'\n    assert glsl_code(floor(x)) == 'floor(x)'\n    assert glsl_code(sign(x)) == 'sign(x)'\n    assert glsl_code(exp(x)) == 'exp(x)'\n    assert glsl_code(log(x)) == 'log(x)'\n    assert glsl_code(x + y) == 'add(x, y)'\n    assert glsl_code(x - y) == 'sub(x, y)'\n    assert glsl_code(x * y) == 'mul(x, y)'\n    assert glsl_code(x ** y) == 'pow(x, y)'\n    assert glsl_code(x + y, use_operators=False) == 'add(x, y)'\n    assert glsl_code(x - y, use_operators=False) == 'sub(x, y)'\n    assert glsl_code(x * y, use_operators=False) == 'mul(x, y)'\n    assert glsl_code(x ** y, use_operators=False) == 'pow(x, y)'\n    assert glsl_code(Piecewise((x, x > 0), (x**2, True))) == 'x > 0 ? x : pow(x, 2.0)'\n", "def test_glsl_code():\n    # Testing GLSL code generation\n    assert glsl_code(x + y) == \"x + y\"\n    assert glsl_code(x * y) == \"x * y\"\n    assert glsl_code(x - y) == \"x - y\"\n    assert glsl_code(x / y) == \"x / y\"\n    assert glsl_code(x ** y) == \"pow(x, y)\"\n    assert glsl_code(sin(x)) == \"sin(x)\"\n    assert glsl_code(cos(x)) == \"cos(x)\"\n    assert glsl_code(tan(x)) == \"tan(x)\"\n\n    # Testing GLSL code generation with assignment\n    assert glsl_code(x + y, assign_to=\"result\") == \"float result = x + y;\"\n    assert glsl_code(x * y, assign_to=\"result\") == \"float result = x * y;\"\n    assert glsl_code(x - y, assign_to=\"result\") == \"float result = x - y;\"\n    assert glsl_code(x / y, assign_to=\"result\") == \"float result = x / y;\"\n    assert glsl_code(x ** y, assign_to=\"result\") == \"float result = pow(x, y);\"\n\n    # Testing GLSL code generation with matrices\n    M = Matrix([[1, 2], [3, 4]])\n    assert glsl_code(M) == \"mat2(1, 2, 3, 4)\"\n    assert glsl_code(M, assign_to=\"matrix\") == \"mat2 matrix = mat2(1, 2, 3, 4);\"\n\n    # Testing GLSL code generation with custom functions\n    custom_functions = {\n        \"sin\": \"my_sin\",\n        \"cos\": \"my_cos\"\n    }\n    assert glsl_code(sin(x), user_functions=custom_functions) == \"my_sin(x)\"\n    assert glsl_code(cos(x), user_functions=custom_functions) == \"my_cos(x)\"\n", "def test_octave_glsl_code():\n    assert glsl_code(sin(x)) == 'sin(x)'\n    assert glsl_code(cos(x)) == 'cos(x)'\n    assert glsl_code(tan(x)) == 'tan(x)'\n    assert glsl_code(acos(x)) == 'acos(x)'\n    assert glsl_code(asin(x)) == 'asin(x)'\n    assert glsl_code(atan(x)) == 'atan(x)'\n    assert glsl_code(exp(x)) == 'exp(x)'\n    assert glsl_code(log(x)) == 'log(x)'\n    assert glsl_code(sqrt(x)) == 'sqrt(x)'\n    assert glsl_code(ceiling(x)) == 'ceil(x)'\n    assert glsl_code(floor(x)) == 'floor(x)'\n    assert glsl_code(sign(x)) == 'sign(x)'\n    assert glsl_code(sin(x)**2 + cos(x)**2) == 'pow(sin(x), 2) + pow(cos(x), 2)'\n    assert glsl_code(sin(x) * cos(x)) == 'sin(x)*cos(x)'\n    assert glsl_code(sin(x) / cos(x)) == 'sin(x)/cos(x)'\n    assert glsl_code(x**2 + 3*x + 2) == 'add(add(pow(x, 2), mul(3, x)), 2)'\n", "def test_GLSLPrinter():\n    assert glsl_code(x**2 + 2*x + 1, assign_to=\"float y\") == \"float y = pow(x, 2.0) + 2*x + 1;\"\n    assert glsl_code(x**2 + 2*x + 1, assign_to=\"float y\", use_operators=False) == \"float y = add(add(pow(x, 2.0), mul(2, x)), 1);\"\n    assert glsl_code(x**2 + 2*x + 1, use_operators=False) == \"add(add(pow(x, 2.0), mul(2, x)), 1)\"\n    assert glsl_code(x**2 + 2*x + 1, glsl_types=False) == \"add(add(pow(x, 2.0), mul(2, x)), 1)\"\n    assert glsl_code(x**2 + 2*x + 1, glsl_types=False, use_operators=True) == \"x**2 + 2*x + 1\"\n    assert glsl_code(x**2 + 2*x + 1, use_operators=False, glsl_types=False) == \"add(add(pow(x, 2.0), mul(2, x)), 1)\"\n    assert glsl_code(x**2 + 2*x + 1, use_operators=True, glsl_types=False) == \"x**2 + 2*x + 1\"\n    assert glsl_code(Matrix([1,2,3]), assign_to=\"vec3 v\") == \"vec3 v = vec3(1, 2, 3);\"\n    assert glsl_code(Matrix([[1,2,3]]), assign_to=\"vec3 v\") == \"vec3 v = vec3(1, 2, 3);\"\n    assert glsl_code(Matrix([[1,2],[3,4]]), assign_to=\"mat2 m\") == \"mat2 m = mat2(1, 2, 3, 4);\"\n    assert glsl_code(Matrix([[1,2],[3,4]]), assign_to=\"mat2 m\", mat_transpose=True) == \"mat2 m = mat2(1, 3, 2, 4);\"\n    assert glsl_code(Matrix([[1,2],[3,4]]), assign_to=\"mat2 m\", mat_nested=True) == \"", "def test_glsl_code_assign_to_Indexed():\n    # assigning to IndexedBase\n    A = MatrixSymbol('A', 3, 3)\n    x = symbols('x')\n    y = symbols('y')\n    i = symbols('i', integer=True)\n    assert glsl_code(x, assign_to=A[i, 1]) == \"A[i][1] = x;\"\n    assert glsl_code(x*y, assign_to=A[i, 1]) == \"A[i][1] = x*y;\"\n\n    # assigning to IndexedBase of a MatrixSymbol with a string\n    A = MatrixSymbol('A', 3, 3)\n    x = symbols('x')\n    y = symbols('y')\n    i = symbols('i', integer=True)\n    assert glsl_code(x, assign_to=\"A[i, 1]\") == \"A[i][1] = x;\"\n    assert glsl_code(x*y, assign_to=\"A[i, 1]\") == \"A[i][1] = x*y;\"\n"], "sample_1125": ["def test_operator_application():\n    op = Operator('A')\n    op2 = Operator('B')\n    id_op = IdentityOperator()\n\n    assert op * id_op == op\n    assert id_op * op == op\n    assert id_op * id_op == id_op\n\n    assert (op * op2) * id_op == op * op2\n    assert id_op * (op * op2) == op * op2\n\n    assert op.inv() * op == id_op\n    assert op * op.inv() == id_op\n", "def test_operator():\n    A = Operator('A')\n    B = Operator('B')\n    I = IdentityOperator()\n\n    assert A * B != B * A\n\n    assert A * I == A\n    assert I * A == A\n\n    assert (A * I).hilbert_space == A.hilbert_space\n    assert (I * A).hilbert_space == A.hilbert_space\n\n    assert A.inv() == A**(-1)\n\n    assert A * A.inv() == I\n\n    assert A._eval_inverse() == A.inv()\n\n    assert (A * B)._eval_inverse() == B.inv() * A.inv()\n", "def test_operator():\n    A = Operator('A')\n    B = Operator('B')\n    I = IdentityOperator()\n    \n    assert A.is_commutative is False\n    assert A.label == ('A', )\n    assert A.hilbert_space == 'H'\n    \n    assert A*B != B*A\n    \n    assert A.inv() == A**(-1)\n    assert A*A.inv() == 1\n    \n    assert I.dimension == oo\n    \n    assert I._eval_power(2) == I\n    assert I._apply_operator(A) == A\n", "def test_operator():\n    A = Operator('A')\n    B = Operator('B')\n\n    assert A.is_commutative is False\n    assert B.is_commutative is False\n    assert A*B != B*A\n\n    C = 2*A*A + I*B\n    assert C == 2*A**2 + I*B\n\n    assert A.inv() == A**(-1)\n    assert A*A.inv() == 1\n\n    I = IdentityOperator()\n    assert A*I == A\n    assert I*A == A\n    assert I.inv() == I\n\n    assert A*I*B == A*B\n    assert I*A*I*B == A*B\n\n    assert I._represent_default_basis() == [[1]]\n    assert I._represent_default_basis({'format': 'numpy'}) == [[1]]\n", "def test_OuterProduct():\n    from sympy.physics.quantum.state import Ket, Bra\n    from sympy.physics.quantum import Dagger\n    k = Ket('k')\n    b = Bra('b')\n    op = OuterProduct(k, b)\n    assert op.ket == k\n    assert op.bra == b\n    assert Dagger(op) == OuterProduct(Dagger(b), Dagger(k))\n"], "sample_723": ["def test_imputation_axis_error():\n    # Test that an error is raised when axis is not 0 or 1.\n    imputer = SimpleImputer()\n    X = np.random.randn(5, 5)\n    assert_raises(ValueError, imputer.fit, X, axis=2)\n", "def test_imputation_most_frequent_tie():\n    # Test imputation using the most-frequent strategy when there's a tie.\n    X = np.array([\n        [-1, -1, -1, 5],\n        [-1, 2, -1, 3],\n        [-1, 2, 3, -1],\n        [-1, 2, 3, 7],\n    ])\n\n    X_true = np.array([\n        [-1, 2, 5],\n        [-1, 2, 3],\n        [-1, 2, 3],\n        [-1, 2, 7],\n    ])\n\n    _check_statistics(X, X_true, \"most_frequent\", [-1, 2, 3, 3], -1)\n", "def test_imputation_edge_cases():\n    # Test imputation with edge cases: empty arrays and single-element arrays.\n\n    # Test empty arrays\n    X_empty = np.array([])\n    imputer = SimpleImputer(strategy=\"mean\")\n    assert_raises(ValueError, imputer.fit, X_empty)\n\n    X_empty_sparse = sparse.csc_matrix((0, 0))\n    imputer = SimpleImputer(strategy=\"mean\")\n    assert_raises(ValueError, imputer.fit, X_empty_sparse)\n\n    # Test single-element arrays\n    X_single = np.array([1])\n    imputer = SimpleImputer(strategy=\"mean\")\n    imputer.fit(X_single)\n    assert_array_equal(imputer.statistics_, [1])\n\n    X_single_sparse = sparse.csc_matrix([1])\n    imputer = SimpleImputer(strategy=\"mean\")\n    imputer.fit(X_single_sparse)\n    assert_array_equal(imputer.statistics_, [1])\n\n    # Test imputation with single-element arrays\n    X_single = np.array([np.nan])\n    imputer = SimpleImputer(strategy=\"mean\")\n    imputer.fit(X_single)\n    assert_array_equal(imputer.statistics_, [np.nan])\n\n    X_single_sparse = sparse.csc_matrix([np.nan])\n    imputer = SimpleImputer(strategy=\"mean\")\n    imputer.fit(X_single_sparse)\n    assert_array_equal(imputer.statistics_, [np.nan])\n\n    # Test imputation with single-element arrays and most_frequent strategy\n    X_single = np.array([-1])\n    imputer = SimpleImputer(strategy=\"most_frequent\")\n    imputer.fit(X_single)\n    assert_array_equal(imputer.statistics_, [-1])\n", "def test_imputation_strategy_edge_cases():\n    # Test imputation with edge case strategies and inputs\n    X = np.array([\n        [np.nan, 1, 2],\n        [0, np.nan, 3],\n        [1, 2, np.nan]\n    ])\n\n    # Test with strategy = \"mean\" and missing_values = np.nan\n    imputer = SimpleImputer(strategy=\"mean\", missing_values=np.nan)\n    X_imputed = imputer.fit_transform(X)\n    assert_array_almost_equal(X_imputed, np.array([\n        [1, 1, 2],\n        [0, 1.5, 3],\n        [1, 2, 2.5]\n    ]))\n\n    # Test with strategy = \"median\" and missing_values = 0\n    imputer = SimpleImputer(strategy=\"median\", missing_values=0)\n    X_imputed = imputer.fit_transform(X)\n    assert_array_almost_equal(X_imputed, np.array([\n        [1, 1, 2],\n        [0, 1.5, 3],\n        [1, 2, 2.5]\n    ]))\n\n    # Test with strategy = \"most_frequent\" and missing_values = np.nan\n    imputer = SimpleImputer(strategy=\"most_frequent\", missing_values=np.nan)\n    X_imputed = imputer.fit_transform(X)\n    assert_array_almost_equal(X_imputed, np.array([\n        [1, 1, 2],\n        [0, 2, 3],\n        [1, 2, 3]\n    ]))\n\n    # Test with strategy = \"mean\" and missing_values = 0, but no missing values\n    imputer = SimpleImputer(strategy=\"mean\", missing_values=0)\n    X_imputed = imputer.fit_transform(X)\n    assert_array_almost_equal(X_imputed, X)\n\n    # Test with strategy = \"median\" and missing_values = np.nan, but no missing values\n    imputer = SimpleImputer(strategy=\"median\", missing_values=np.nan)\n    X_imputed = imputer.fit_transform(X)\n    assert_array_almost_equal(X_imputed, X)\n\n    # Test with strategy = \"most_frequent\" and missing_values = 0, but no missing values\n    imputer = SimpleImputer(strategy=\"most_frequent\", missing_values=0)\n    X_imputed = imputer.fit_transform(X)\n    assert_array_almost", "def test_imputation_most_frequent_ties():\n    # Test imputation using the most-frequent strategy, with ties.\n    X = np.array([\n        [-1, -1, 0, 5],\n        [-1, 2, -1, 3],\n        [-1, 1, 3, 3],\n        [-1, 2, 3, 7],\n    ])\n\n    X_true = np.array([\n        [-1, 0, 5],\n        [-1, 3, 3],\n        [-1, 3, 3],\n        [-1, 3, 7],\n    ])\n\n    _check_statistics(X, X_true, \"most_frequent\", [-1, 3, 3, 3], -1)\n\n    # Test with sparse matrix\n    X_sparse = sparse.csc_matrix(X)\n    X_true_sparse = sparse.csc_matrix(X_true)\n\n    _check_statistics(X_sparse, X_true_sparse, \"most_frequent\", [-1, 3, 3, 3], -1)\n\n"], "sample_1142": ["def test_matrix_element_derivative():\n    A = MatrixSymbol('A', n, m)\n    i, j = symbols('i, j', integer=True)\n    assert A[i, j].diff(A[i, j]) == 1\n    assert A[i, j].diff(A[0, 0]) == KroneckerDelta(0, i, (0, n-1))*KroneckerDelta(0, j, (0, m-1))\n    assert A[i, j].diff(A[i, j+1]) == 0\n    assert A[0, 0].diff(A[0, 0]) == 1\n", "def test_matrix_expr_from_index_summation():\n    i, j, k, l, n = symbols('i j k l n')\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    expr = Sum(A[i, j]*B[j, k], (j, 0, n-1))\n    assert MatrixExpr.from_index_summation(expr) == A*B\n    expr = Sum(A[j, i]*B[j, k], (j, 0, n-1))\n    assert MatrixExpr.from_index_summation(expr) == A.T*B\n    expr = Sum(A[i, i], (i, 0, n-1))\n    assert MatrixExpr.from_index_summation(expr) == Trace(A)\n    expr = Sum(A[i, j]*B[k, j]*A[l, k], (j, 0, n-1), (k, 0, n-1))\n    assert MatrixExpr.from_index_summation(expr) == A*B.T*A.T\n    expr = Sum(A[i, j]*B[i, j], (i, 0, n-1), (j, 0, n-1))\n    assert MatrixExpr.from_index_summation(expr) == Trace(A*B.T)\n    expr = Sum(KroneckerDelta(i, j, (0, n-1)), (i, 0, n-1))\n    assert MatrixExpr.from_index_summation(expr) == Identity(n)\n    expr = Sum(KroneckerDelta(i, j, (0, n-1))*KroneckerDelta(j, k, (0, n-1)), (i, 0, n-1), (j, 0, n-1))\n    assert MatrixExpr.from_index_summation(expr) == Identity(n)\n    expr = Sum(KroneckerDelta(i, j, (0, n-1)), (j, 0, n-1))\n    assert MatrixExpr.from_index_summation(expr) == Sum(KroneckerDelta(i, j, (0, n-1)), (j, 0, n-1))\n", "def test_matrix_symbol_derivative():\n    i = symbols('i')\n    j = symbols('j')\n    k = symbols('k')\n    n = symbols('n')\n    A = MatrixSymbol('A', n, n)\n    assert A.diff(i) == ZeroMatrix(n, n)\n    assert A.diff(j) == ZeroMatrix(n, n)\n    assert A.diff(k) == ZeroMatrix(n, n)\n    assert A.diff(A[i, j]) == ZeroMatrix(n, n)\n    assert A.diff(A[i, k]) == ZeroMatrix(n, n)\n", "def test_matrixexpr_from_index_summation():\n    n, m, l, k, p = symbols('n m l k p', integer=True, positive=True)\n    i, j, r, s = symbols('i j r s', integer=True)\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    C = MatrixSymbol('C', n, n)\n    D = MatrixSymbol('D', n, n)\n\n    expr = Sum(A[i, j]*B[j, k], (j, 0, m-1))\n    assert MatrixExpr.from_index_summation(expr) == A*B\n\n    expr = Sum(A[j, i]*B[j, k], (j, 0, m-1))\n    assert MatrixExpr.from_index_summation(expr) == A.T*B\n\n    expr = Sum(A[i, i], (i, 0, n-1))\n    assert MatrixExpr.from_index_summation(expr) == Trace(A)\n\n    expr = Sum(A[i, j]*B[k, j]*A[l, k], (j, 0, m-1), (k, 0, m-1))\n    assert MatrixExpr.from_index_summation(expr) == A*B.T*A.T\n\n    expr = Sum(A[i, j]*B[j, k]*C[k, s]*D[r, i], (i, 0, n-1), (j, 0, m-1), (k, 0, l-1))\n    assert MatrixExpr.from_index_summation(expr) == A*B*C*D.T\n\n    expr = Sum(KroneckerDelta(i, j, (0, n-1)), (i, 0, n-1))\n    assert MatrixExpr.from_index_summation(expr) == Identity(n)\n\n    expr = Sum(KroneckerDelta(i, j, (0, n-1)), (i, 0, n-1), (j, 0, n-1))\n    assert MatrixExpr.from_index_summation(expr) == n*Identity(n)\n\n    raises(ValueError, lambda: MatrixExpr.from_index_summation(Sum(A[i, j], (i, 1, n))))\n\n    raises(ValueError, lambda: MatrixExpr.from_index_summation(Sum(A[i, j], (i, 0, n))))\n\n    raises(ValueError, lambda: MatrixExpr.from", "def test_matrix_derivative():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    C = MatrixSymbol('C', n, n)\n    D = MatrixSymbol('D', m, m)\n\n    assert A.diff(A) == Identity(n)*Identity(m)\n    assert A.diff(B) == ZeroMatrix(n, m)\n    assert (A*B).diff(B) == A\n    assert (B*A).diff(B) == ZeroMatrix(l, n)\n    assert (A.T).diff(A) == ZeroMatrix(m, n)\n    assert (A*B*C).diff(B) == A*C.T\n    assert (B*C*A).diff(B) == ZeroMatrix(m, l)\n    assert (A**2).diff(A) == 2*A\n    assert (A**2).diff(B) == ZeroMatrix(n, m)\n    assert (A*I).diff(A) == Identity(n)\n    assert (I*A).diff(A) == Identity(n)\n    assert (I*C).diff(C) == Identity(n)\n    assert (C*I).diff(C) == Identity(n)\n    assert (A*B.I).diff(B) == -A*(B**-1)*(B**-1)\n    assert (B.I*A).diff(B) == -(B**-1)*(B**-1)*A\n    assert (A*B*C.I).diff(B) == A*C.I\n    assert (B.I*C*A).diff(B) == -B.I*C*A*B.I\n    assert (B**-1*C).diff(B) == -B.I*(C.T)*B.I\n    assert (C*B**-1).diff(B) == C*B.I\n    assert (B.I*C*B.I).diff(B) == -B.I*(C.T)*B.I*B.I - B.I*B.I*(C.T)*B.I\n    assert (B.I*C.D*B).diff(B) == -B.I*(C.D*B).T*B.I\n"], "sample_309": ["    def test_valid_date(self):\n        date_str = 'Sun, 06 Nov 1994 08:49:37 GMT'\n        self.assertEqual(parse_http_date_safe(date_str), parse_http_date(date_str))\n", "    def test_valid_date(self):\n        self.assertEqual(parse_http_date_safe('Mon, 01 Jan 2007 01:54:21 GMT'), parse_http_date('Mon, 01 Jan 2007 01:54:21 GMT'))\n", "    def test_valid_dates(self):\n        valid_dates = (\n            'Sun, 06 Nov 1994 08:49:37 GMT',\n            'Tuesday, 31-Dec-69 08:49:37 GMT',\n            'Sun Nov  6 08:49:37 1994',\n        )\n        for date in valid_dates:\n            with self.subTest(date=date):\n                self.assertIsNotNone(parse_http_date_safe(date))\n", "    def test_valid_date(self):\n        date_string = 'Sun, 06 Nov 1994 08:49:37 GMT'\n        self.assertIsNotNone(parse_http_date_safe(date_string))\n", "    def test_valid_dates(self):\n        valid_dates = (\n            ('Sun, 06 Nov 1994 08:49:37 GMT', 784345837),\n            ('Sunday, 06-Nov-94 08:49:37 GMT', 784345837),\n            ('Sun Nov  6 08:49:37 1994', 784345837),\n        )\n        for date_str, expected in valid_dates:\n            with self.subTest(date_str=date_str):\n                self.assertEqual(parse_http_date_safe(date_str), expected)\n"], "sample_1038": ["def test_matrix_element_subs():\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n    i, j = symbols(\"i, j\")\n    assert A[i, j].subs(A, B) == B[i, j]\n    assert B[i, j].subs(B, A) == A[i, j]\n\n    x, y = symbols(\"x y\")\n    assert A[i, j].subs({i: x, j: y}) == A[x, y]\n    assert A[i, j].subs({i: 0, j: 1}) == A[0, 1]\n    raises(ValueError, lambda: A[i, j].subs({i: 2, j: 1}))\n    raises(ValueError, lambda: A[i, j].subs({i: 0, j: 2}))\n", "def test_MatrixElement_conjugate():\n    A = MatrixSymbol('A', 3, 3)\n    assert A[1, 2].conjugate() == conjugate(A[1, 2])\n    assert A[1, 2].conjugate().func == conjugate\n    assert A[1, 2].conjugate().args == (A[1, 2],)\n\n    B = MatrixSymbol('B', 3, 3)\n    assert (A[1, 2] + B[1, 2]).conjugate() == conjugate(A[1, 2]) + conjugate(B[1, 2])\n    assert (A[1, 2] - B[1, 2]).conjugate() == conjugate(A[1, 2]) - conjugate(B[1, 2])\n    assert (A[1, 2] * B[1, 2]).conjugate() == conjugate(B[1, 2]) * conjugate(A[1, 2])\n    assert (A[1, 2] / B[1, 2]).conjugate() == conjugate(A[1, 2]) / conjugate(B[1, 2])\n", "def test_indexing_with_unequal_matrix_symbol_dimensions():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 2, 2)\n\n    raises(ValueError, lambda: A[0, 3])\n    raises(ValueError, lambda: A[3, 0])\n    raises(ValueError, lambda: B[0, 2])\n    raises(ValueError, lambda: B[2, 0])\n", "def test_MatMul_postprocessor_with_multiple_scalars():\n    A = MatrixSymbol(\"A\", 2, 2)\n    M = Matrix([[1, 2], [3, 4]])\n    assert Mul(x, x, A, M) == MatMul(x**2, A, M)\n    assert Mul(x, A, M, x) == MatMul(x, A, Mx)\n    assert Mul(x, M, x, A) == MatMul(Mx, x, A)\n    assert Mul(x, x, M, A) == MatMul(x**2, M, A)\n    assert Mul(x, y, A, M) == MatMul(x*y, A, M)\n    assert Mul(x, A, M, y) == MatMul(x, A, M*y)\n    assert Mul(x, M, y, A) == MatMul(M*x*y, A)\n    assert Mul(x, y, M, A) == MatMul(M*x*y, A)\n", "def test_matrix_derivative():\n    X = MatrixSymbol('X', 2, 2)\n    x = symbols('x')\n    assert X.diff(x) == ZeroMatrix(2, 2)\n    i, j = symbols('i, j')\n    assert X[i, j].diff(x) == 0\n    assert X[i, j].diff(X[i, j]) == 1\n    assert X[i, j].diff(X[1-i, 1-j]) == 0\n    A = Matrix([[x, x**2], [x**3, x**4]])\n    assert A.diff(x) == Matrix([[1, 2*x], [3*x**2, 4*x**3]])\n    assert A[0, 0].diff(x) == 1\n    assert A[1, 1].diff(x) == 4*x**3\n    assert A[0, 1].diff(x**2) == 2\n    assert A.diff(x, 2) == Matrix([[0, 2], [6*x, 12*x**2]])\n    assert A.diff(x, x, 2) == Matrix([[0, 2], [6*x, 12*x**2]])\n    assert A.diff(x, 3) == Matrix([[0, 0], [6, 24*x]])\n    assert A.diff(x, 4) == Matrix([[0, 0], [0, 24]])\n    assert A.diff(x, 5) == Matrix([[0, 0], [0, 0]])\n    assert X.diff(X) == Identity(2)\n    Y = MatrixSymbol('Y', 3, 3)\n    assert (X*Y).diff(X) == Y.T\n    assert (Y*X).diff(X) == Y\n    assert (X*X.T).diff(X) == X.T + X.T\n"], "sample_431": ["    def test_repr(self):\n        a = Article(\n            id=None,\n            headline=\"Parrot programs in Python\",\n            pub_date=datetime(2005, 7, 28),\n        )\n        a.save()\n        self.assertEqual(repr(a), \"<Article: Parrot programs in Python (Article object)>\")\n\n        s = SelfRef.objects.create()\n        self.assertEqual(repr(s), \"<SelfRef: SelfRef object>\")\n", "    def test_serializable_value(self):\n        a = Article.objects.create(\n            headline=\"Parrot programs in Python\",\n            pub_date=datetime(2005, 7, 28),\n        )\n        self.assertEqual(\n            a.serializable_value(\"headline\"), \"Parrot programs in Python\"\n        )\n        self.assertEqual(\n            a.serializable_value(\"pub_date\"), datetime(2005, 7, 28, 0, 0)\n        )\n", "    def test_repr_with_default_values(self):\n        article = Article()\n        self.assertEqual(\n            repr(article),\n            \"<Article: Article object (%s)>\" % article.pk,\n        )\n", "    def test_adding_state(self):\n        a = Article.objects.create()\n        self.assertTrue(a._state.adding)\n        a.save()\n        self.assertFalse(a._state.adding)\n        a = Article.objects.get(id=a.id)\n        self.assertFalse(a._state.adding)\n        a.save()\n        self.assertFalse(a._state.adding)\n", "    def test_model_base_subclass_exception(self):\n        name = \"MyError\"\n        bases = (ValueError,)\n        module = \"tests\"\n        attached_to = type(\"MyModel\", (), {})\n\n        subclass_exception = subclass_exception(name, bases, module, attached_to)\n        self.assertEqual(subclass_exception.__name__, name)\n        self.assertEqual(subclass_exception.__qualname__, f\"MyModel.{name}\")\n        self.assertEqual(subclass_exception.__module__, module)\n        self.assertEqual(subclass_exception.__base__, ValueError)\n        self.assertEqual(subclass_exception.__bases__, (ValueError,))\n"], "sample_604": ["def test_summarize_attr_with_quotes():\n    cases = [\n        (\"key\", '\"string\"'),\n        (\"key\", \"'string'\"),\n        (\"key\", \"b'string'\"),\n        (\"key\", \"b'\\\\xc3\\\\xb1'\"),\n    ]\n    for key, value in cases:\n        actual = formatting.summarize_attr(key, value)\n        expected = f\"    {key}: {value}\"\n        assert actual == expected\n", "def test_format_array_flat_datetime64(self):\n    dates = pd.date_range(start='1/1/2022', periods=100, freq='D')\n    actual = formatting.format_array_flat(dates, 20)\n    expected = '1970-01-01 ... 1970-01-03'\n    assert actual == expected\n\n    actual = formatting.format_array_flat(dates, 30)\n    expected = '1970-01-01 1970-01-02 ... 1970-01-04'\n    assert actual == expected\n", "def test_format_timestamp(self):\n    # test formatting of datetime objects\n    dt = datetime(2022, 1, 1, 12, 0, 0)\n    expected = \"2022-01-01T12:00:00\"\n    actual = formatting.format_timestamp(dt)\n    assert actual == expected\n\n    # test formatting of datetime objects with time zone\n    dt = datetime(2022, 1, 1, 12, 0, 0, tzinfo=pd.Timestamp(\"2022-01-01\").tzinfo)\n    expected = \"2022-01-01T12:00:00+00:00\"\n    actual = formatting.format_timestamp(dt)\n    assert actual == expected\n\n    # test formatting of numpy datetime64 objects\n    dt64 = np.datetime64(\"2022-01-01T12:00:00\")\n    expected = \"2022-01-01T12:00:00\"\n    actual = formatting.format_timestamp(dt64)\n    assert actual == expected\n\n    # test formatting of numpy datetime64 objects with time zone\n    dt64 = np.datetime64(\"2022-01-01T12:00:00+00:00\")\n    expected = \"2022-01-01T12:00:00+00:00\"\n    actual = formatting.format_timestamp(dt64)\n    assert actual == expected\n\n    # test formatting of pandas Timestamp objects\n    ts = pd.Timestamp(\"2022-01-01T12:00:00\")\n    expected = \"2022-01-01T12:00:00\"\n    actual = formatting.format_timestamp(ts)\n    assert actual == expected\n\n    # test formatting of pandas Timestamp objects with time zone\n    ts = pd.Timestamp(\"2022-01-01T12:00:00+00:00\")\n    expected = \"2022-01-01T12:00:00+00:00\"\n    actual = formatting.format_timestamp(ts)\n    assert actual == expected\n\n    # test formatting of NaT\n    nat = pd.NaT\n    expected = \"NaT\"\n    actual = formatting.format_timestamp(nat)\n    assert actual == expected\n", "def test_inline_variable_array_repr_dask_array():\n    import dask.array as da\n\n    # Create a dask array\n    x = np.arange(1000)\n    dask_array = da.from_array(x, chunks=(10,))\n\n    # Create a variable from the dask array\n    variable = xr.Variable(\"x\", dask_array)\n\n    max_width = 10\n    actual = formatting.inline_variable_array_repr(variable, max_width=max_width)\n\n    expected = \"dask.array<chunksize=(10,)>\"\n    assert actual == expected\n", "def test_wrap_indent():\n    text = \"This is a long string that will be split across multiple lines\"\n    expected = \"This is a long string that will be\\n\" \"  split across multiple lines\"\n    assert formatting.wrap_indent(text, \"    \") == expected\n\n    text = \"Short string\"\n    expected = \"Short string\"\n    assert formatting.wrap_indent(text) == expected\n\n    text = \"\\nThis is a string with newline at the beginning\"\n    expected = \"This is a string with newline at the beginning\"\n    assert formatting.wrap_indent(text) == expected\n\n    text = \"This is a string with newline at the end\\n\"\n    expected = \"This is a string with newline at the end\"\n    assert formatting.wrap_indent(text) == expected\n\n    text = \"This is a long string that will be split across multiple lines and has a long start string to wrap around\"\n    start = \"This is a long start string to wrap around: \"\n    expected = \"This is a long start string to wrap around: This is a long string that will be\\n\" \\\n               \"  split across multiple lines and has a long start string to wrap around\"\n    assert formatting.wrap_indent(text, start) == expected\n"], "sample_917": ["def test_declarations_in_template_instantiations():\n    check('class', 'template<> C<int>', {2: 'IE1CIiE'})\n    check('function', 'template<> void f<int>()', {2: 'IE1fIiEv', 4: 'IE1fv'})\n    check('member', 'template<> int A<int>::a', {2: 'IEN1AIiE1aE'})\n    check('type', 'template<> using T = A<int>', {2: 'IE1T'})\n", "def test_function_template_definitions():\n    check('function', 'template<typename T> void f(T)', {1: \"f__T\", 2: \"1f1T\"})\n    check('function', 'template<typename T> void f(T, T)', {1: \"f__T.T\", 2: \"1f1T1T\"})\n    check('function', 'template<typename T> void f(T, T, T)', {1: \"f__T.T.T\", 2: \"1f1T1T1T\"})\n    check('function', 'template<typename T, typename U> void f(T, U)', {1: \"f__T.U\", 2: \"1f1T1U\"})\n    check('function', 'template<typename T, typename... Us> void f(T, Us...)', {1: \"f__T.z\", 2: \"1f1TDp1U\"})\n    check('function', 'template<typename T, typename... Us> void f(T, Us...)',\n          {1: \"f__T.z\", 2: \"1f1TDp1U\"}, output='template<typename T, typename... Us> void f(T, Us...)')\n    check('function', 'template<typename... Ts> void f(Ts...)', {1: \"f__.z\", 2: \"1fDp1T\"})\n    check('function', 'template<typename... Ts> void f(Ts...)',\n          {1: \"f__.z\", 2: \"1fDp1T\"}, output='template<typename... Ts> void f(Ts...)')\n    check('function', 'template<typename T, typename U, typename... Vs> void f(T, U, Vs...)',\n          {1: \"f__T.U.z\", 2: \"1f1T1UDp1V\"})\n    check('function', 'template<typename T, typename U, typename... Vs> void f(T, U, Vs...)',\n          {1: \"f__T.U.z\", 2: \"1f1T1UDp1V\"},\n          output='template<typename T, typename U, typename... Vs> void f(T, U, Vs...)')\n", "def test_xref_parsing_of_template_definitions():\n        class Config:\n            cpp_id_attributes = [\"id_attr\"]\n            cpp_paren_attributes = [\"paren_attr\"]\n        parser = DefinitionParser(target, location=None,\n                                  config=Config())\n        ast, isShorthand = parser.parse_xref_object()\n        parser.assert_end()\n    check('f<int>')\n    check('f<int, char>')\n    check('f<int, char, double>')\n    check('f<int, char, double, float>')\n    check('f<int, char, double, float, bool>')\n    check('f<42>')\n    check('f<42, 3.14>')\n    check('f<42, 3.14, \"hello\">')\n    check('f<42, 3.14, \"hello\", nullptr>')\n    check('f<42, 3.14, \"hello\", nullptr, true>')\n    check('f<42, 3.14, \"hello\", nullptr, true, false>')\n    check('f<int, char...>')\n    check('f<int, char..., double>')\n    check('f<int, char..., double, float>')\n    check('f<int, char..., double, float, bool>')\n    check('f<42, 3.14, ...>')\n    check('f<42, 3.14, ..., \"hello\">')\n    check('f<42, 3.14, ..., \"hello\", nullptr>')\n    check('f<42, 3.14, ..., \"hello\", nullptr, true>')\n    check('f<42, 3.14, ..., \"hello\", nullptr, true, false>')\n    check('f<id_0, id_1, ...id_2>')\n    check('f<id_0, id_1, id_2>')\n    check('f<id_0, id_1, id_2, id_3>')\n    check('f<id_0, id_1, id_2, id_3, id_4>')\n    check('f<id_0, id_1, ...id_2>')\n    check('f<id_0, id_1, ...id_2, id_3>')\n    check('f<id_0, id_1, ...id_2, id_3, id_4>')\n", "def test_template_parameter_defaults():\n    check('function',\n          \"template<typename T, typename... Ts = std::tuple<>> void f()\",\n          {2: \"I0DpEXv\", 4: \"I0DpEXv\"},\n          output=\"template<typename T, typename... Ts = std::tuple<>> void f()\")\n    check('function',\n          \"template<typename T = int, typename... Ts> void f()\",\n          {2: \"I0DpEXv\", 4: \"I_1TE1fvv\"},\n          output=\"template<typename T = int, typename... Ts> void f()\")\n    check('function',\n          \"template<typename T = int, typename... Ts = std::tuple<>> void f()\",\n          {2: \"I_1CE1DpEXv\", 4: \"I_1TE1DpEXv\"},\n          output=\"template<typename T = int, typename... Ts = std::tuple<>> void f()\")\n    check('function',\n          \"template<typename... Ts = std::tuple<>, typename T = int> void f()\",\n          {2: \"I00DpE1CE1v\", 4: \"I_1TE1DpEXv\"},\n          output=\"template<typename... Ts = std::tuple<>, typename T = int> void f()\")\n    check('function',\n          \"template<typename... Ts = std::tuple<>, typename T> void f()\",\n          {2: \"I00DpE1v\", 4: \"I_1DpEXv\"},\n          output=\"template<typename... Ts = std::tuple<>, typename T> void f()\")\n", "def test_id_generation_with_nested_template_types():\n    check('class', 'template<typename T> C<T>',\n          {2: 'I0E1C1T'})\n    check('class', 'template<typename T> C<T<T>>',\n          {2: 'I0E1C1T1T'})\n    check('class', 'template<typename T> C<T<T<T>>>',\n          {2: 'I0E1C1T1T1T'})\n    check('class', 'template<typename T> C<T<0>>',\n          {2: 'I0E1C1T0'})\n    check('class', 'template<typename T> C<T<0, 0>>',\n          {2: 'I0E1C1T0_0'})\n    check('class', 'template<typename T> C<T<0, 0, 0>>',\n          {2: 'I0E1C1T0_0_0'})\n    check('class', 'template<typename T> C<T<T<0>>',\n          {2: 'I0E1C1T1T0'})\n    check('class', 'template<typename T> C<T<T<0, 0>>',\n          {2: 'I0E1C1T1T0_0'})\n    check('class', 'template<typename T> C<T<T<0, 0, 0>>',\n          {2: 'I0E1C1T1T0_0_0'})\n    check('class', 'template<typename T> C<T<T<T<0, 0, 0>>>',\n          {2: 'I0E1C1T1T1T0_0_0'})\n    check('class', 'template<typename T> C<T<template<typename U> T<U>>>',\n          {2: 'I0E1C1T1T1U'})\n    check('class', 'template<typename T> C<T<template<typename U> T<U<0>>>',\n          {2: 'I0E1C1T1T1U0'})\n"], "sample_1159": ["def test_issue_failing_assumptions_2():\n    x = Symbol('x', real=True, positive=True)\n    y = Symbol('y', positive=True)\n    assert failing_assumptions(6*x + y, real=True, positive=True) == \\\n    {'positive': None, 'real': None}\n", "def test_check_assumptions_against_object():\n    x = Symbol('x', real=True, positive=True)\n    y = Symbol('y')\n    assert check_assumptions(1, x) is True\n    assert check_assumptions(x, x) is True\n    assert check_assumptions(x, 1) is True\n    assert check_assumptions(x, y) is None\n", "def test_assumptions_function_symbols():\n    # Ensure assumptions are properly handled for function symbols\n    from sympy import Function\n    f = Function('f', commutative=True)\n    assert f(1).is_commutative is True\n    g = Function('g', commutative=False)\n    assert g(1).is_commutative is False\n    h = Function('h')\n    assert h(1).is_commutative is None\n", "def test_check_assumptions_symmetric():\n    x = Symbol('x', positive=True)\n    y = Symbol('y', positive=True)\n    assert check_assumptions(x + 1, y + 1) is True\n    assert check_assumptions(x - 1, y + 1) is False\n    assert check_assumptions(x + 1, -(y + 1)) is False\n    assert check_assumptions(x + I, y) is None\n    assert check_assumptions(x, y + I) is None\n", "def test_issue_18899():\n    # Verify that assumptions for symbols with self-contradictory properties\n    # behave as expected.\n    assert Symbol('x', positive=True, nonpositive=True).is_real is None\n    assert Symbol('x', positive=True, nonpositive=True).is_positive is None\n    assert Symbol('x', positive=False, nonnegative=True).is_real is None\n    assert Symbol('x', positive=False, nonnegative=True).is_nonnegative is None\n    assert Symbol('x', nonnegative=True, negative=True).is_real is None\n    assert Symbol('x', nonnegative=True, negative=True).is_nonnegative is None\n    assert Symbol('x', nonnegative=False, nonpositive=False).is_real is None\n    assert Symbol('x', nonnegative=False, nonpositive=False).is_nonnegative is None\n    assert Symbol('x', real=True, nonreal=True).is_real is None\n    assert Symbol('x', real=True, nonreal=True).is_imaginary is None\n    assert Symbol('x', real=False, imaginary=False).is_real is None\n    assert Symbol('x', real=False, imaginary=False).is_imaginary is None\n    assert Symbol('x', real=False, algebraic=True).is_algebraic is None\n    assert Symbol('x', real=False, algebraic=True).is_transcendental is None\n    assert Symbol('x', algebraic=True, transcendental=True).is_algebraic is None\n    assert Symbol('x', algebraic=True, transcendental=True).is_transcendental is None\n    assert Symbol('x', rational=True, irrational=True).is_rational is None\n    assert Symbol('x', rational=True, irrational=True).is_irrational is None\n    assert Symbol('x', integer=True, noninteger=True).is_integer is None\n    assert Symbol('x', integer=True, noninteger=True).is_noninteger is None\n    assert Symbol('x', odd=True, even=True).is_odd is None\n    assert Symbol('x', odd=True, even=True).is_even is None\n    assert Symbol('x', prime=True, composite=True).is_prime is None\n    assert Symbol('x', prime=True, composite=True).is_composite is None\n    assert Symbol('x', zero=True, nonzero=True).is_zero is None\n    assert Symbol('x', zero=True, nonzero=True).is_nonzero is"], "sample_1173": ["def test_implicit_multiplication_application():\n    transformations = standard_transformations + \\\n                      (implicit_multiplication_application,)\n\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n\n    assert parse_expr(\"10sin**2 x**2 + 3xyz + tan theta\",\n                      transformations=transformations) == 3*x*y*z + 10*sin(x**2)**2 + tan(Symbol('theta'))\n    assert parse_expr(\"10sin^2 x^2 + 3xyz + tan theta\",\n                      transformations=transformations) == 3*x*y*z + 10*sin(x**2)**2 + tan(Symbol('theta'))\n    assert parse_expr(\"10sin**2 x^2 + 3xyz + tan theta\",\n                      transformations=transformations) == 3*x*y*z + 10*sin(x**2)**2 + tan(Symbol('theta'))\n    assert parse_expr(\"10sin^2 x**2 + 3xyz + tan theta\",\n                      transformations=transformations) == 3*x*y*z + 10*sin(x**2)**2 + tan(Symbol('theta'))\n    assert parse_expr(\"10sin x**2 + 3xyz + tan theta\",\n                      transformations=transformations) == 3*x*y*z + 10*sin(x**2) + tan(Symbol('theta'))\n", "def test_issue_lambda_expression():\n    transformations = standard_transformations + (implicit_multiplication, implicit_application)\n    x = Symbol('x')\n    assert parse_expr(\"lambda x: x**2 + 2*x + 1\", transformations=transformations) == Lambda(x, x**2 + 2*x + 1)\n    assert parse_expr(\"(lambda x: x**2 + 2*x + 1)(x)\", transformations=transformations) == x**2 + 2*x + 1\n    assert parse_expr(\"(lambda x: x**2 + 2*x + 1)(2*x)\", transformations=transformations) == (2*x)**2 + 2*(2*x) + 1\n    assert parse_expr(\"lambda x, y: x + y\", transformations=transformations) == Lambda(x, Lambda(y, x + y))\n", "def test_implicit_multiplication_application():\n    transformations = standard_transformations + \\\n                      (implicit_multiplication_application, )\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n    f = Function('f')\n\n    assert parse_expr(\"f x y\", transformations=transformations) == f(x, y)\n    assert parse_expr(\"f x(y + 1)\", transformations=transformations) == f(x*(y + 1))\n    assert parse_expr(\"f(x y)\", transformations=transformations) == f(x*y)\n    assert parse_expr(\"f x y^2\", transformations=transformations) == f(x, y**2)\n    assert parse_expr(\"f x y z\", transformations=transformations) == f(x, y, z)\n    assert parse_expr(\"f x (y + z)\", transformations=transformations) == f(x, y + z)\n    assert parse_expr(\"f (x + y) z\", transformations=transformations) == f(x + y, z)\n    assert parse_expr(\"f (x + y) (z + 1)\", transformations=transformations) == f(x + y, z + 1)\n    assert parse_expr(\"f x^2\", transformations=transformations) == f(x**2)\n    assert parse_expr(\"f x^2 y\", transformations=transformations) == f(x**2, y)\n    assert parse_expr(\"f x y^2 z\", transformations=transformations) == f(x, y**2, z)\n", "def test_lambda_expression():\n    transformations = standard_transformations + (lambda_notation,)\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n    expr1 = parse_expr('lambda x,y: x + y', transformations=transformations)\n    expr2 = parse_expr('(x, y) -> x + y', transformations=transformations)\n    assert expr1 == expr2 == Function(Lambda((x, y), x + y))\n    raises(SyntaxError, lambda: parse_expr('lambda x: x', transformations=transformations))\n    raises(SyntaxError, lambda: parse_expr('lambda x,y:', transformations=transformations))\n    raises(SyntaxError, lambda: parse_expr('lambda : x + y', transformations=transformations))\n    raises(SyntaxError, lambda: parse_expr('lambda x,y: ', transformations=transformations))\n    raises(SyntaxError, lambda: parse_expr('lambda x,y: x + y z', transformations=transformations))\n", "def test_implicit_multiplication_application():\n    transformations = standard_transformations + \\\n                      (implicit_multiplication_application,)\n\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n    f = Function('f')\n\n    assert parse_expr(\"3 x\") == parse_expr(\"3*x\", transformations=transformations)\n    assert parse_expr(\"3 x y\") == parse_expr(\"3*x*y\", transformations=transformations)\n    assert parse_expr(\"3 x y z\") == parse_expr(\"3*x*y*z\", transformations=transformations)\n    assert parse_expr(\"3 x y (z + 1)\") == parse_expr(\"3*x*y*(z + 1)\", transformations=transformations)\n    assert parse_expr(\"3 (x + y) z\") == parse_expr(\"3*(x + y)*z\", transformations=transformations)\n    assert parse_expr(\"3 x (y + z)\") == parse_expr(\"3*x*(y + z)\", transformations=transformations)\n    assert parse_expr(\"f x\") == parse_expr(\"f(x)\", transformations=transformations)\n    assert parse_expr(\"f 3 x\") == parse_expr(\"f(3*x)\", transformations=transformations)\n    assert parse_expr(\"f x y\") == parse_expr(\"f(x*y)\", transformations=transformations)\n    assert parse_expr(\"f x y z\") == parse_expr(\"f(x*y*z)\", transformations=transformations)\n    assert parse_expr(\"f x (y + z)\") == parse_expr(\"f(x*(y + z))\", transformations=transformations)\n    assert parse_expr(\"f (x + y) z\") == parse_expr(\"f(x + y)*z\", transformations=transformations)\n    assert parse_expr(\"sin x y\") == parse_expr(\"sin(x*y)\", transformations=transformations)\n    assert parse_expr(\"sin (x + y) z\") == parse_expr(\"sin(x + y)*z\", transformations=transformations)\n    assert parse_expr(\"sin x (y + z)\") == parse_expr(\"sin(x*(y + z))\", transformations=transformations)\n"], "sample_1034": ["def test_apply_grover_invalid_iterations():\n    nqubits = 2\n    with raises(ValueError):\n        apply_grover(return_one_on_one, nqubits, -1)\n    with raises(ValueError):\n        apply_grover(return_one_on_one, nqubits, 0)\n    with raises(ValueError):\n        apply_grover(return_one_on_one, nqubits, None)\n", "def test_apply_grover_iterations():\n    nqubits = 2\n    oracle = lambda qubits: qubits == IntQubit(2, nqubits=nqubits)\n    assert apply_grover(oracle, nqubits, 1) == qapply(IntQubit(2, nqubits=nqubits))\n\n    nqubits = 4\n    oracle = lambda qubits: qubits == IntQubit(8, nqubits=nqubits)\n    assert apply_grover(oracle, nqubits, 4) == qapply(IntQubit(8, nqubits=nqubits))\n", "def test_apply_grover_edge_cases():\n    nqubits = 0\n    oracle = lambda qubits: True\n    try:\n        apply_grover(oracle, nqubits)\n    except QuantumError as e:\n        assert str(e) == \"Grover's algorithm needs nqubits > 0, received 0 qubits\"\n\n    nqubits = 1\n    oracle = None\n    try:\n        OracleGate(nqubits, oracle)\n    except TypeError as e:\n        assert str(e) == \"Callable expected, got: None\"\n\n    nqubits = 1\n    oracle = return_one_on_one\n    v = OracleGate(nqubits, oracle)\n    assert v.targets == (0,)\n\n    nqubits = 2\n    oracle = return_one_on_two\n    v = OracleGate(nqubits, oracle)\n    assert v.targets == (0, 1)\n", "def test_apply_grover_edge_cases():\n    # Test with no iterations\n    assert apply_grover(return_one_on_one, 1, iterations=0) == superposition_basis(1)\n\n    # Test with no qubits\n    try:\n        apply_grover(return_one_on_one, 0)\n        assert False, \"Expected a QuantumError to be raised\"\n    except QuantumError:\n        pass\n\n    # Test with a non-callable oracle\n    try:\n        apply_grover(\"not a function\", 2)\n        assert False, \"Expected a TypeError to be raised\"\n    except TypeError:\n        pass\n", "def test_apply_grover_iterations():\n    nqubits = 2\n    assert apply_grover(return_one_on_one, nqubits, 1) == IntQubit(1, nqubits=nqubits)\n\n    nqubits = 4\n    assert apply_grover(return_one_on_two, 4, 1) != apply_grover(return_one_on_two, 4)\n\n    # Test with zero iterations\n    nqubits = 2\n    assert apply_grover(return_one_on_one, nqubits, 0) == superposition_basis(nqubits)\n\n    # Test with zero iterations and a larger nqubits\n    nqubits = 4\n    assert apply_grover(return_one_on_two, nqubits, 0) == superposition_basis(nqubits)\n"], "sample_437": ["    def test_set_rollback(self):\n        with transaction.atomic():\n            connection.set_rollback(True)\n            self.assertTrue(connection.get_rollback())\n            connection.set_rollback(False)\n            self.assertFalse(connection.get_rollback())\n", "    def test_timezone_constructor(self):\n        tzname = \"America/New_York\"\n        if settings.USE_DEPRECATED_PYTZ:\n            import pytz\n            tzinfo = timezone_constructor(tzname)\n            self.assertIsInstance(tzinfo, pytz.BaseTzInfo)\n            self.assertEqual(tzinfo.zone, tzname)\n        else:\n            tzinfo = timezone_constructor(tzname)\n            self.assertIsInstance(tzinfo, zoneinfo.ZoneInfo)\n            self.assertEqual(tzinfo.key, tzname)\n", "    def test_on_commit(self):\n            Person.objects.create(first_name=\"first\", last_name=\"last\")\n\n        with self.assertNumQueries(1):\n            with transaction.atomic():\n                connection.on_commit(callback)\n", "    def setUp(self):\n        self.conn = connections[DEFAULT_DB_ALIAS]\n        self.conn.ensure_connection()\n", "    def test_connect_calls_get_connection_params(self):\n        with patch.object(connection, \"get_connection_params\", return_value={}) as mock_get_connection_params:\n            with patch.object(connection, \"get_new_connection\", return_value=None):\n                connection.connect()\n                self.assertTrue(mock_get_connection_params.called)\n"], "sample_1155": ["def test_algebraic_extensions():\n    assert construct_domain([1, sqrt(2), sqrt(3)], extension=True)[0].gens == [sqrt(2) + sqrt(3)]\n    assert construct_domain([1, sqrt(2) - sqrt(3)], extension=True)[0].gens == [sqrt(2) - sqrt(3)]\n    assert construct_domain([1, sqrt(2)*sqrt(3)], extension=True)[0].gens == [sqrt(6)]\n    assert construct_domain([1, sqrt(2)/sqrt(3)], extension=True)[0].gens == [sqrt(6)/3]\n    assert construct_domain([1, sqrt(2) + I*sqrt(3)], extension=True)[0].gens == [sqrt(2) + I*sqrt(3)]\n", "def test_algebraic_domain():\n    assert construct_domain([sqrt(2), sqrt(3)], extension=True) == \\\n        (QQ.algebraic_field(sqrt(2) + sqrt(3)), \n         [QQ.algebraic_field(sqrt(2) + sqrt(3)).convert(sqrt(2)), \n          QQ.algebraic_field(sqrt(2) + sqrt(3)).convert(sqrt(3))])\n\n    assert construct_domain([sqrt(2), sqrt(5)], extension=True) == \\\n        (QQ.algebraic_field(sqrt(2) + sqrt(5)), \n         [QQ.algebraic_field(sqrt(2) + sqrt(5)).convert(sqrt(2)), \n          QQ.algebraic_field(sqrt(2) + sqrt(5)).convert(sqrt(5))])\n\n    assert construct_domain([sqrt(2), sqrt(3), sqrt(6)], extension=True) == \\\n        (QQ.algebraic_field(sqrt(2) + sqrt(3)), \n         [QQ.algebraic_field(sqrt(2) + sqrt(3)).convert(sqrt(2)), \n          QQ.algebraic_field(sqrt(2) + sqrt(3)).convert(sqrt(3)), \n          QQ.algebraic_field(sqrt(2) + sqrt(3)).convert(sqrt(6))])\n\n    assert construct_domain([sqrt(2), sqrt(3), sqrt(7)], extension=True) == \\\n        (QQ.algebraic_field(sqrt(2) + sqrt(3) + sqrt(7)), \n         [QQ.algebraic_field(sqrt(2) + sqrt(3) + sqrt(7)).convert(sqrt(2)), \n          QQ.algebraic_field(sqrt(2) + sqrt(3) + sqrt(7)).convert(sqrt(3)), \n          QQ.algebraic_field(sqrt(2) + sqrt(3) + sqrt(7)).convert(sqrt(7))])\n\n    assert construct_domain([sqrt(2) + 1, sqrt(3)], extension=True) == \\\n        (QQ.algebraic_field(sqrt(2) + sqrt(3)), \n         [QQ.algebraic_field(sqrt(2) + sqrt(3)).convert(sqrt(2) + 1), \n          QQ.algebraic_field(sqrt(2) + sqrt(3)).convert(sqrt(3))])\n\n    assert construct_domain([sqrt(2), sqrt(3) + 1], extension=True) == \\\n        (QQ.algebraic_field(sqrt(2) +", "def test_algebraic_extension():\n    assert construct_domain([sqrt(2) + 1, sqrt(3)], extension=True)[0].is_AlgebraicField\n    assert construct_domain([sqrt(2) - sqrt(3), 1], extension=True)[0].is_AlgebraicField\n    assert construct_domain([sqrt(2) * sqrt(3), sqrt(2) + sqrt(3)], extension=True)[0].is_AlgebraicField\n    assert construct_domain([sqrt(2) / sqrt(3), 1], extension=True)[0].is_AlgebraicField\n", "def test_algebraic_extension_with_floating_points():\n    assert construct_domain([sqrt(2), sqrt(3), 0.5], extension=True) == \\\n        (EX, [EX(sqrt(2)), EX(sqrt(3)), EX(0.5)])\n    assert construct_domain([sqrt(2) + 0.5, sqrt(3)], extension=True) == \\\n        (EX, [EX(sqrt(2) + 0.5), EX(sqrt(3))])\n    assert construct_domain([sqrt(2)*0.5, sqrt(3)], extension=True) == \\\n        (EX, [EX(sqrt(2)*0.5), EX(sqrt(3))])\n", "def test_algebraic_extensions():\n    # Test that algebraic extensions are created correctly\n    # when the coefficients have algebraic numbers\n    dom, _ = construct_domain([sqrt(2) + sqrt(3), 2*sqrt(2) + 3*sqrt(3)],\n        extension=True)\n    assert dom == QQ.algebraic_field(sqrt(2) + sqrt(3))\n\n    # Test that the extension is not created when extension=False\n    dom, _ = construct_domain([sqrt(2) + sqrt(3), 2*sqrt(2) + 3*sqrt(3)],\n        extension=False)\n    assert dom == EX\n\n    # Test that the extension is created correctly when there are multiple\n    # algebraic numbers\n    dom, _ = construct_domain([sqrt(2) + sqrt(3), sqrt(5) + sqrt(7)],\n        extension=True)\n    assert dom == QQ.algebraic_field(sqrt(2) + sqrt(3), sqrt(5) + sqrt(7))\n\n    # Test that the extension is created correctly when the coefficients\n    # have complex algebraic numbers\n    dom, _ = construct_domain([sqrt(2) + I*sqrt(3), 2*sqrt(2) + 3*I*sqrt(3)],\n        extension=True)\n    assert dom == QQ.algebraic_field(sqrt(2) + I*sqrt(3))\n\n    # Test that the extension is not created when there are non-algebraic\n    # coefficients\n    dom, _ = construct_domain([sqrt(2) + sqrt(3), 2*sqrt(2) + 3*sqrt(3), pi],\n        extension=True)\n    assert dom == EX\n"], "sample_1037": ["def test_MatMul_refine():\n    A = MatrixSymbol('A', 2, 2)\n    assert refine_MatMul(A*A.T, Q.orthogonal(A)) == Identity(2)\n    assert refine_MatMul(A*A.conjugate(), Q.unitary(A)) == Identity(2)\n", "def test_MatMul_transpose_and_adjoint():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    assert (A*B).T == B.T*A.T\n    assert (A*B).T.shape == (l, n)\n    assert (A*B).H == B.H*A.H\n    assert (A*B).H.shape == (l, n)\n", "def test_MatMul_validate():\n    A = MatrixSymbol('A', 2, 3)\n    B = MatrixSymbol('B', 3, 2)\n    C = MatrixSymbol('C', 2, 3)\n\n    raises(ShapeError, lambda: validate(B, A))\n    raises(ShapeError, lambda: validate(A, C))\n\n    assert validate(A, B) == None\n", "def test_MatMul_refine():\n    n = symbols('n', integer=True)\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    assert refine_MatMul(A*B, {Q.square(A), Q.square(B)}) == A*B\n    assert refine_MatMul(A*transpose(A), {Q.orthogonal(A)}) == Identity(n)\n    assert refine_MatMul(B*adjoint(B), {Q.unitary(B)}) == Identity(n)\n", "def test_MatMul_doit():\n    A = MatrixSymbol('A', 2, 3)\n    B = MatrixSymbol('B', 3, 2)\n    C = MatrixSymbol('C', 2, 2)\n    D = MatrixSymbol('D', 3, 3)\n    I = Identity(2)\n\n    assert (A*B).doit() == A*B\n    assert (A*C).doit() == A*C\n    assert (B*D).doit() == B*D\n    assert (I*I).doit() == I\n    assert (A*B*I).doit() == A*B\n    assert (I*B*D).doit() == B*D\n    assert (A*I*C).doit() == A*C\n    assert (C*I*B).doit() == C*B\n\n    # Test doit(deep=False)\n    assert (A*B).doit(deep=False) == A*B\n    assert (A*C).doit(deep=False) == A*C\n    assert (B*D).doit(deep=False) == B*D\n    assert (I*I).doit(deep=False) == I\n    assert (A*B*I).doit(deep=False) == A*B*I\n    assert (I*B*D).doit(deep=False) == I*B*D\n    assert (A*I*C).doit(deep=False) == A*I*C\n    assert (C*I*B).doit(deep=False) == C*I*B\n"], "sample_1063": ["def test_issue_16536_fresnel_integrals_mpmath():\n    f1 = fresnelc(x)\n    f2 = fresnels(x)\n    F1 = lambdify(x, f1, modules='mpmath')\n    F2 = lambdify(x, f2, modules='mpmath')\n\n    assert abs(fresnelc(1.3) - F1(mpmath.mpf(1.3))) <= 1e-10\n    assert abs(fresnels(1.3) - F2(mpmath.mpf(1.3))) <= 1e-10\n", "def test_lambdify_issue_17719():\n    from sympy import symbols, lambdify\n    x, y = symbols('x y')\n    f = lambdify((x, y), [x**2, y**2], 'numpy')\n    assert f(2, 3).dtype == 'float64'\n    assert f([2, 3]).dtype == 'int64'\n", "def test_tensorflow_Derivative():\n    if not tensorflow:\n        skip(\"tensorflow not installed.\")\n    x = symbols('x')\n    f = Function('f')\n    f_x = f(x)\n    f_x_prime = f_x.diff(x)\n    f_prime_implementation = lambda x: 3*x**2\n    f_prime = implemented_function(f_x_prime, f_prime_implementation)\n    f_prime_lambdified = lambdify(x, f_prime, 'tensorflow')\n    with tensorflow.compat.v1.Session() as s:\n        assert f_prime_lambdified(tensorflow.constant(1.0)).eval(session=s) == 3.0\n", "def test_issue_17321():\n    if not numexpr:\n        skip(\"numexpr not installed.\")\n    expr = x * y + sin(x)\n    f = lambdify((x, y), expr, modules='numexpr')\n    # Issue was that `sin` was not in numexpr's dictionary, so we test that\n    assert 'sin' in f.__globals__\n", "def test_lambdastr():\n    f = lambdastr((x, y), x + y)\n    assert f(1, 2) == 3\n\n    f = lambdastr([x, y], x + y)\n    assert f(1, 2) == 3\n\n    f = lambdastr({x: x, y: y}, x + y)\n    assert f(1, 2) == 3\n\n    # Test that lambdastr does not attempt to flatten the arguments\n    assert lambdastr([x, y], [x, y]) == 'lambda x,y: ([x, y])'\n\n    # Test that lambdastr can handle complex nested structures\n    f = lambdastr(((x, y), [z]), ((x, y), [z]))\n    assert f((1, 2), [3]) == ((1, 2), [3])\n\n    # Test that lambdastr can handle strings as arguments\n    f = lambdastr(\"x\", x**2)\n    assert f(2) == 4\n\n    # Test that lambdastr can handle a single argument\n    f = lambdastr(x, x**2)\n    assert f(2) == 4\n\n    # Test that lambdastr can handle a single argument that is a string\n    f = lambdastr(\"x\", x**2)\n    assert f(2) == 4\n"], "sample_586": ["    def test_concat_dim_precedence(self):\n        ds1 = Dataset({\"foo\": ((\"x\", \"y\"), np.random.randn(2, 3))}, {\"x\": [0, 1]})\n        ds2 = Dataset({\"foo\": ((\"x\", \"y\"), np.random.randn(2, 3))}, {\"x\": [1, 2]})\n\n        dim = (2 * ds1[\"x\"]).rename(\"x\")\n        expected = Dataset({\"foo\": ((\"x\", \"y\"), np.concatenate((ds1[\"foo\"], ds2[\"foo\"]))), \"x\": dim})\n        actual = concat([ds1, ds2], dim)\n        assert_identical(expected, actual)\n", "    def test_concat_mixed_types(self):\n        ds1 = Dataset({\"a\": (\"x\", [1, 2])})\n        ds2 = Dataset({\"a\": (\"x\", [3, 4])}, {\"x\": [0, 1], \"y\": 2})\n\n        with raises_regex(ValueError, \"global attributes not equal\"):\n            concat([ds1, ds2], dim=\"x\", compat=\"identical\")\n\n        actual = concat([ds1, ds2], dim=\"x\", compat=\"equals\")\n        expected = Dataset({\"a\": (\"x\", [1, 2, 3, 4])}, {\"y\": 2})\n        assert_identical(actual, expected)\n", "def test_concat_positions():\n    ds1 = Dataset({\"a\": ((\"x\",), [1, 2])}, {\"x\": [0, 1]})\n    ds2 = Dataset({\"a\": ((\"x\",), [3, 4])}, {\"x\": [2, 3]})\n    positions = [[0, 1], [2, 3]]\n\n    expected = Dataset({\"a\": ((\"x\",), [1, 2, 3, 4])}, {\"x\": [0, 1, 2, 3]})\n    actual = concat([ds1, ds2], \"x\", positions=positions)\n    assert_identical(expected, actual)\n\n    with raises_regex(ValueError, \"must be a list of integer arrays\"):\n        concat([ds1, ds2], \"x\", positions=\"foo\")\n\n    with raises_regex(ValueError, \"positions and datasets must have the same length\"):\n        concat([ds1, ds2], \"x\", positions=[[0, 1]])\n", "def test_concat_positions_kwarg():\n    data = Dataset({\"a\": (\"x\", [1, 2, 3]), \"x\": [0, 1, 2]})\n    split_data = [data.isel(x=slice(2)), data.isel(x=slice(2, None))]\n\n    expected = Dataset(\n        {\"a\": (\"x\", [3, 1, 2]), \"x\": [2, 0, 1]},\n    )\n    actual = concat(split_data, dim=\"x\", positions=[1, 0])\n    assert_identical(expected, actual)\n\n    with raises_regex(ValueError, \"positions must be an integer array\"):\n        concat(split_data, dim=\"x\", positions=[0.5, 1])\n\n    with raises_regex(ValueError, \"length of positions does not match the number of objects\"):\n        concat(split_data, dim=\"x\", positions=[0])\n\n    with raises_regex(ValueError, \"positions must be an integer array\"):\n        concat(split_data, dim=\"x\", positions=[\"a\", \"b\"])\n", "def test_concat_with_positions():\n    ds1 = Dataset({\"foo\": (\"x\", [1, 2, 3])}, {\"x\": [0, 1, 2]})\n    ds2 = Dataset({\"foo\": (\"x\", [4, 5, 6])}, {\"x\": [0, 1, 2]})\n    positions = [[0, 2], [1, 0, 1]]\n\n    expected = Dataset(\n        {\"foo\": ((\"y\", \"x\"), [[1, 2, 6], [4, 5, 3]])}, {\"x\": [0, 1, 2], \"y\": [0, 1]}\n    )\n    actual = concat([ds1, ds2], \"y\", positions=positions)\n    assert_identical(actual, expected)\n\n    with raises_regex(ValueError, \"length of positions\"):\n        concat([ds1, ds2], \"y\", positions=[0, 2])\n\n    with raises_regex(ValueError, \"length of positions\"):\n        concat([ds1, ds2], \"y\", positions=[[0, 2], [1]])\n"], "sample_780": ["def test_lda_doc_topic_prior():\n    # Test LDA with different doc_topic_prior\n    n_components, X = _build_sparse_mtx()\n    prior_1 = 1. / n_components\n    prior_2 = 0.5\n    lda_1 = LatentDirichletAllocation(n_components=n_components,\n                                      doc_topic_prior=prior_1, random_state=0)\n    lda_2 = LatentDirichletAllocation(n_components=n_components,\n                                      doc_topic_prior=prior_2, random_state=0)\n    lda_1.fit(X)\n    lda_2.fit(X)\n    assert lda_1.components_.shape == lda_2.components_.shape\n    assert not np.allclose(lda_1.components_, lda_2.components_)\n", "def test_lda_doc_topic_prior():\n    # test `doc_topic_prior` with different values\n    n_components, X = _build_sparse_mtx()\n    lda_1 = LatentDirichletAllocation(n_components=n_components,\n                                      doc_topic_prior=0.1, random_state=0)\n    lda_2 = LatentDirichletAllocation(n_components=n_components,\n                                      doc_topic_prior=1, random_state=0)\n    lda_3 = LatentDirichletAllocation(n_components=n_components,\n                                      doc_topic_prior=10, random_state=0)\n\n    lda_1.fit(X)\n    lda_2.fit(X)\n    lda_3.fit(X)\n\n    assert not np.array_equal(lda_1.components_, lda_2.components_)\n    assert not np.array_equal(lda_1.components_, lda_3.components_)\n    assert not np.array_equal(lda_2.components_, lda_3.components_)\n", "def test_lda_sub_sampling():\n    # Test LDA online learning (`fit` method with 'online' learning)\n    # and sub-sampling\n    rng = np.random.RandomState(0)\n    n_components, X = _build_sparse_mtx()\n    lda = LatentDirichletAllocation(n_components=n_components,\n                                    learning_offset=10., evaluate_every=1,\n                                    learning_method='online', random_state=rng)\n    lda.fit(X)\n\n    # Test perplexity with sub-sampling\n    perp_subsampling = lda.perplexity(X, sub_sampling=True)\n    assert_greater(perp_subsampling, 0)\n\n    # Test perplexity without sub-sampling\n    perp_no_subsampling = lda.perplexity(X, sub_sampling=False)\n    assert_greater(perp_no_subsampling, 0)\n", "def test_lda_edge_cases():\n    # Test LDA with edge cases (e.g., single document, single topic)\n    rng = np.random.RandomState(0)\n    lda = LatentDirichletAllocation(max_iter=10, random_state=rng)\n    \n    # Test single document\n    X = np.array([[1, 2, 3]])\n    lda.fit(X)\n    assert lda.components_.shape == (1, 3)\n    \n    # Test single topic\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    lda = LatentDirichletAllocation(n_components=1, max_iter=10, random_state=rng)\n    lda.fit(X)\n    assert lda.components_.shape == (1, 3)\n    \n    # Test single word\n    X = np.array([[1], [2]])\n    lda = LatentDirichletAllocation(max_iter=10, random_state=rng)\n    lda.fit(X)\n    assert lda.components_.shape == (10, 1)\n    \n    # Test empty vocabulary\n    X = np.array([[]])\n    lda = LatentDirichletAllocation(max_iter=10, random_state=rng)\n    with pytest.raises(ValueError):\n        lda.fit(X)\n", "def test_lda_init_latent_vars():\n    # Test _init_latent_vars method\n    n_components = 5\n    lda = LatentDirichletAllocation(n_components=n_components)\n    assert hasattr(lda, 'n_components_') is False\n    assert hasattr(lda, 'n_batch_iter_') is False\n    assert hasattr(lda, 'n_iter_') is False\n    assert hasattr(lda, 'doc_topic_prior_') is False\n    assert hasattr(lda, 'topic_word_prior_') is False\n    assert hasattr(lda, 'components_') is False\n    assert hasattr(lda, 'exp_dirichlet_component_') is False\n    assert lda.components_ is None\n    assert lda.exp_dirichlet_component_ is None\n    \n    # Initialize latent variables\n    lda._init_latent_vars(n_features=10)\n    \n    # Check attributes are set\n    assert hasattr(lda, 'n_components_')\n    assert hasattr(lda, 'n_batch_iter_')\n    assert hasattr(lda, 'n_iter_')\n    assert hasattr(lda, 'doc_topic_prior_')\n    assert hasattr(lda, 'topic_word_prior_')\n    assert hasattr(lda, 'components_')\n    assert hasattr(lda, 'exp_dirichlet_component_')\n    \n    # Check attributes values\n    assert lda.n_components_ == n_components\n    assert lda.n_batch_iter_ == 1\n    assert lda.n_iter_ == 0\n    assert lda.doc_topic_prior_ == 1. / n_components\n    assert lda.topic_word_prior_ == 1. / n_components\n    assert lda.components_.shape == (n_components, 10)\n    assert lda.exp_dirichlet_component_.shape == (n_components, 10)\n"], "sample_1075": ["def test_beta_is_real():\n    x, y = Symbol('x', real=True), Symbol('y', real=True)\n    assert beta(x, y).is_real\n\n    x, y = Symbol('x', real=False), Symbol('y', real=True)\n    assert not beta(x, y).is_real\n\n    x, y = Symbol('x', real=True), Symbol('y', real=False)\n    assert not beta(x, y).is_real\n", "def test_beta_eval_is_real():\n    x, y = Symbol('x', real=True), Symbol('y', real=True)\n    assert beta(x, y)._eval_is_real()\n\n    x, y = Symbol('x', real=False), Symbol('y', real=True)\n    assert not beta(x, y)._eval_is_real()\n\n    x, y = Symbol('x', real=True), Symbol('y', real=False)\n    assert not beta(x, y)._eval_is_real()\n", "def test_beta_eval_is_real():\n    x, y = Symbol('x'), Symbol('y')\n\n    assert beta(x, y)._eval_is_real() is None\n    assert beta(1, 2)._eval_is_real() is True\n    assert beta(1 + 2j, 2)._eval_is_real() is False\n", "def test_beta_edge_cases():\n    x, y = Symbol('x'), Symbol('y')\n    \n    assert beta(x, 1) == 1/x\n    assert beta(1, y) == 1/y\n    assert beta(1, 1) == 1\n    \n    # Check for invalid input\n    raises(ValueError, lambda: beta(x, 0))\n    raises(ValueError, lambda: beta(0, y))\n    raises(ValueError, lambda: beta(0, 0))\n", "def test_beta_edge_cases():\n    assert beta(1, Symbol('y')).evalf() == 1/(Symbol('y'))\n    assert beta(Symbol('x'), 1).evalf() == 1/(Symbol('x'))\n    assert beta(1, 1).evalf() == 1\n    assert beta(Symbol('x'), Symbol('x')).evalf() == 1/Symbol('x')\n    assert beta(0, 1).evalf() == float('inf')\n    assert beta(1, 0).evalf() == float('inf')\n"], "sample_906": ["def test_domain_cpp_ast_template_args_with_default_values():\n    check('type', \"template<typename T = int> {key}A = B\", {2: \"I0E1A\"}, key='using')\n    check('type', \"template<typename T = std::pair<int, int>> {key}A = B\", {2: \"I0E1A\"}, key='using')\n    check('type', \"template<typename T = std::vector<std::pair<int, int>>> {key}A = B\", {2: \"I0E1A\"}, key='using')\n    check('type', \"template<typename T = std::map<std::string, std::vector<std::pair<int, int>>>> {key}A = B\", {2: \"I0E1A\"}, key='using')\n    check('type', \"template<typename T = std::map<std::string, std::vector<std::pair<int, int>>, std::less<std::string>>> {key}A = B\", {2: \"I0E1A\"}, key='using')\n", "def test_domain_cpp_ast_concept_definitions_template():\n    check('concept', 'template<typename T> {key}A<T>::B',\n          {2: 'I0EN1A1B'})\n    check('concept', 'template<typename T, typename U> {key}A<T>::B<U>',\n          {2: 'I0EN1A1BT1U'})\n    check('concept', 'template<typename T> {key}A<T>::template B<int>',\n          {2: 'I0EN1A1BIiE'})\n    check('concept', 'template<typename T, typename U> {key}A<T>::template B<U>',\n          {2: 'I0EN1A1BT1UE'})\n    check('concept', 'template<typename T, typename U> {key}A<T>::template B<U, int>',\n          {2: 'I0EN1A1BT1UEiE'})\n    check('concept', 'template<typename T, typename U> {key}A<T>::template B<int, U>',\n          {2: 'I0EN1A1BIi1UE'})\n    check('concept', 'template<typename T, typename U> {key}A<T>::template B<int, U>',\n          {2: 'I0EN1A1BIi1UE'})\n    check('concept', 'template<typename T, typename U> {key}A<T>::template B<int>',\n          {2: 'I0EN1A1BIiE'})\n", "def test_domain_cpp_ast_c_support():\n    check('type', 'typedef long long int', {1: 'int', 2: '3int'})\n    check('type', 'typedef long long int foo', {1: 'foo', 2: '3foo'})\n\n    check('function', 'void f(const char *)', {1: \"f__cPK\", 2: \"1fPKc\"})\n    check('function', 'void f(const char *const)', {1: \"f__cPKC\", 2: \"1fPKCc\"})\n    check('function', 'void f(const char *volatile)', {1: \"f__cPKV\", 2: \"1fPKVc\"})\n    check('function', 'void f(const char *const volatile)', {1: \"f__cPKVC\", 2: \"1fPKVCc\"})\n\n    check('function', 'void f(const char[])', {1: \"f__cA\", 2: \"1fA_c\"})\n    check('function', 'void f(const char[1])', {1: \"f__cA1\", 2: \"1fA1_c\"})\n    check('function', 'void f(const char[1][2])', {1: \"f__cA1A2\", 2: \"1fA1A2_c\"})\n    check('function', 'void f(const char[1][2][3])', {1: \"f__cA1A2A3\", 2: \"1fA1A2A3_c\"})\n\n    check('function', 'void f(char **)', {1: \"f__cPP\", 2: \"1fPPc\"})\n    check('function', 'void f(char *const *)', {1: \"f__cPPC\", 2: \"1fPPCc\"})\n    check('function', 'void f(char *volatile *)', {1: \"f__cPPV\", 2: \"1fPPVc\"})\n    check('function', 'void f(char *const volatile *)', {1: \"f__cPPVC\", 2: \"1fPPVCc\"})\n\n    check('function', 'void f(char (*)[])', {1: \"f__cPA\", 2: \"1fPAc\"})\n    check('function', 'void f(char (*)", "def test_domain_cpp_alias_object():\n    # Test for issue #3513: check that the alias domain object does not crash\n    text = (\".. cpp:alias:: x\\n\"\n            \"   :maxdepth: 1\\n\"\n            \"   :noroot: True\\n\"\n            \"   :noindex:\\n\"\n            \"\\n\"\n            \"   void f();\\n\"\n            \"\\n\"\n            \"   .. cpp:function:: f()\\n\"\n            \"       :noindex:\\n\")\n\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (desc, desc_content, desc, desc_content))\n    assert_node(doctree[0], desc, domain='c', objtype='alias')\n    assert_node(doctree[0][0], desc_signature, text='void f();')\n    assert_node(doctree[2], desc, domain='c', objtype='function')\n    assert_node(doctree[2][0], desc_signature, text='void f();')\n", "def test_domain_cpp_ast_symbol_lookup():\n              expected):\n        s = root._symbol_lookup(parent, ident, lambda *args: None,\n                                ancestorLookupType=None, matchSelf=matchSelf,\n                                recurseInAnon=recurseInAnon, searchInSiblings=searchInSiblings)\n        if s is None:\n            assert expected is None\n            return\n        assert expected is not None\n        assert len(list(s.symbols)) == len(expected)\n        for i in range(len(expected)):\n            assert s.symbols[i].ident == expected[i]\n\n    root = Symbol(None, None, None, None, None, None, None)\n    # the root has no parent\n    root.parent = None\n    child1 = root.add_name(ASTNestedName([ASTIdentifier(\"child1\")], rooted=False))\n    child2 = root.add_name(ASTNestedName([ASTIdentifier(\"child2\")], rooted=False))\n    assert root.children == [child1, child2]\n    child11 = child1.add_name(ASTNestedName([ASTIdentifier(\"child11\")], rooted=False))\n    child12 = child1.add_name(ASTNestedName([ASTIdentifier(\"child12\")], rooted=False))\n    assert child1.children == [child11, child12]\n    child111 = child11.add_name(ASTNestedName([ASTIdentifier(\"child111\")], rooted=False))\n    assert child11.children == [child111]\n    check(root, root, ASTIdentifier(\"child1\"), False, False, False, [child1])\n    check(root, root, ASTIdentifier(\"child2\"), False, False, False, [child2])\n    check(root, root, ASTIdentifier(\"child3\"), False, False, False, None)\n    check(root, root, ASTIdentifier(\"child1\"), True, False, False, [root, child1])\n    check(root, root, ASTIdentifier(\"child11\"), False, False, False, [child11])\n    check(root, root, ASTIdentifier(\"child111\"), False, False, False, [child111])\n    check(root, root, ASTIdentifier(\"child12\"), False, False, False, [child12])\n    check(root, child1, ASTIdentifier(\"child1\"), True, False, False, [child1])\n    check(root, child1, ASTIdentifier(\"child2\"), False,"], "sample_825": ["def test_pls_algorithm_svd_vs_nipals():\n    # Test that both algorithms (nipals and svd) give the same results\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n\n    pls_nipals = pls_.PLSRegression(n_components=2, algorithm=\"nipals\")\n    pls_svd = pls_.PLSRegression(n_components=2, algorithm=\"svd\")\n\n    pls_nipals.fit(X, Y)\n    pls_svd.fit(X, Y)\n\n    assert_array_almost_equal(pls_nipals.x_scores_, pls_svd.x_scores_)\n    assert_array_almost_equal(pls_nipals.y_scores_, pls_svd.y_scores_)\n    assert_array_almost_equal(pls_nipals.x_weights_, pls_svd.x_weights_)\n    assert_array_almost_equal(pls_nipals.y_weights_, pls_svd.y_weights_)\n    assert_array_almost_equal(pls_nipals.x_loadings_, pls_svd.x_loadings_)\n    assert_array_almost_equal(pls_nipals.y_loadings_, pls_svd.y_loadings_)\n    assert_array_almost_equal(pls_nipals.x_rotations_, pls_svd.x_rotations_)\n    assert_array_almost_equal(pls_nipals.y_rotations_, pls_svd.y_rotations_)\n    assert_array_almost_equal(pls_nipals.coef_, pls_svd.coef_)\n", "def test_pls_n_components_edge_cases():\n    # Test edge cases for the n_components parameter\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n\n    for clf in [pls_.PLSCanonical(), pls_.PLSRegression(), pls_.PLSSVD()]:\n        # Test that n_components=0 raises an error\n        clf.n_components = 0\n        assert_raise_message(ValueError, \"Invalid number of components\",\n                             clf.fit, X, Y)\n\n        # Test that n_components=1 is valid\n        clf.n_components = 1\n        clf.fit(X, Y)\n\n        # Test that n_components=X.shape[1] is valid\n        clf.n_components = X.shape[1]\n        clf.fit(X, Y)\n\n        # Test that n_components=X.shape[1] + 1 raises an error\n        clf.n_components = X.shape[1] + 1\n        assert_raise_message(ValueError, \"Invalid number of components\",\n                             clf.fit, X, Y)\n\n        # Test that n_components=-1 raises an error\n        clf.n_components = -1\n        assert_raise_message(ValueError, \"Invalid number of components\",\n                             clf.fit, X, Y)\n", "def test_pls_y_single_column():\n    # Test PLS on Y with a single column\n    d = load_linnerud()\n    X = d.data\n    Y = d.target[:, 0:1]  # take only the first column of Y\n\n    for clf in [pls_.PLSCanonical(), pls_.PLSRegression(),\n                pls_.PLSSVD()]:\n        clf.fit(X, Y)\n        X_score, Y_score = clf.transform(X, Y)\n        assert_equal(X_score.shape[1], clf.n_components)\n        assert_equal(Y_score.shape[1], clf.n_components)\n", "def test_pls_input_validation():\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n    pls = pls_.PLSRegression()\n    assert_raise_message(ValueError, \"Got algorithm.*when only.*are known\",\n                         pls.fit, X, Y, algorithm=\"unknown\")\n    assert_raise_message(ValueError, \"Invalid number of components\",\n                         pls.fit, X, Y, n_components=4)\n    assert_raise_message(ValueError, \"The deflation mode is unknown\",\n                         pls.fit, X, Y, deflation_mode=\"unknown\")\n", "def test_pls_deflation_mode():\n    # Test that canonical and regression deflation modes work correctly\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n\n    pls_canonical = pls_.PLSCanonical(n_components=X.shape[1])\n    pls_canonical.fit(X, Y)\n    pls_regression = pls_.PLSRegression(n_components=X.shape[1])\n    pls_regression.fit(X, Y)\n\n    # Check that the scores are different for the two deflation modes\n    assert_array_almost_equal(pls_canonical.x_scores_, pls_regression.x_scores_,\n                              err_msg=\"x scores are identical for canonical \"\n                                      \"and regression deflation modes\",\n                              decimal=1)\n\n    assert_array_almost_equal(pls_canonical.y_scores_, pls_regression.y_scores_,\n                              err_msg=\"y scores are identical for canonical \"\n                                      \"and regression deflation modes\",\n                              decimal=1)\n"], "sample_1004": ["def test_conditionset_properties():\n    C = ConditionSet\n    I = S.Integers\n    c = C(x, x > 0, I)\n    assert c.base_set == I\n    assert c.condition == (x > 0)\n    assert c.sym == x\n    c = C((x, y), {x + 1, x + y}, S.Reals)\n    assert c.base_set == S.Reals\n    assert c.condition == FiniteSet(x + 1, x + y)\n    assert c.sym == (x, y)\n", "def test_issue_lambda_symbol():\n    lam = Symbol('lambda')\n    c = ConditionSet(x, x < 1, S.Integers)\n    assert c.subs(x, lam) == ConditionSet(lam, lam < 1, S.Integers)\n\n    lam = Symbol('lambda', positive=True)\n    c = ConditionSet(x, x < 1, S.Integers)\n    raises(ValueError, lambda: c.subs(x, lam))\n", "def test_conditionset_with_non_symbol_dummy():\n    c = ConditionSet(x + y, x + y < 1, S.Reals)\n    assert c == ConditionSet(L, L < 1, S.Reals)\n    raises(ValueError, lambda: ConditionSet(x + 1, x < 1, S.Integers))\n    raises(ValueError, lambda: ConditionSet(x + 1, x < 1, FiniteSet(1, 2, 3)))\n", "def test_conditionset_repr():\n    c = ConditionSet(x, x > 0, S.Reals)\n    assert repr(c) == \"ConditionSet(x, x > 0, S.Reals)\"\n    c = ConditionSet(x, x > 0, Interval(0, 1))\n    assert repr(c) == \"ConditionSet(x, x > 0, Interval(0, 1, True, True))\"\n\n    # Test with a ConditionSet in the base set\n    c = ConditionSet(x, x > 0, ConditionSet(y, y > 0, S.Reals))\n    assert repr(c) == \"ConditionSet(x, x > 0, ConditionSet(y, y > 0, S.Reals))\"\n\n    # Test with a Lambda function in the condition\n    lam = Lambda(x, x > 0)\n    c = ConditionSet(x, lam(x), S.Reals)\n    assert repr(c) == \"ConditionSet(x, Lambda(_Dummy_11, _Dummy_11 > 0)(x), S.Reals)\"\n", "def test_CondSet_edge_cases():\n    # Test the case where the condition is a single symbol\n    assert ConditionSet(x, x, S.Reals) == S.EmptySet\n\n    # Test the case where the base set is a single element\n    assert ConditionSet(x, x > 0, S.UniversalSet).contains(1) == S.true\n    assert ConditionSet(x, x > 0, S.UniversalSet).contains(-1) == S.false\n\n    # Test the case where the condition is a complex expression\n    assert ConditionSet(x, x**2 + y**2 < 1, S.Reals).contains((0, 0)) == S.true\n    assert ConditionSet(x, x**2 + y**2 < 1, S.Reals).contains((1, 1)) == S.false\n\n    # Test the case where the base set is an interval\n    assert ConditionSet(x, x > 0, Interval(0, 1)).contains(0.5) == S.true\n    assert ConditionSet(x, x > 0, Interval(0, 1)).contains(-1) == S.false\n\n    # Test the case where the condition is a piecewise function\n    from sympy import Piecewise\n    pw = Piecewise((x, x > 0), (0, x <= 0))\n    assert ConditionSet(x, pw, S.Reals).contains(1) == S.true\n    assert ConditionSet(x, pw, S.Reals).contains(-1) == S.false\n\n    # Test the case where the base set is a union of intervals\n    assert ConditionSet(x, x > 0, Union(Interval(0, 1), Interval(2, 3))).contains(0.5) == S.true\n    assert ConditionSet(x, x > 0, Union(Interval(0, 1), Interval(2, 3))).contains(2.5) == S.true\n    assert ConditionSet(x, x > 0, Union(Interval(0, 1), Interval(2, 3))).contains(-1) == S.false\n"], "sample_958": ["def test_domain_cpp_ast_xref_parsing_template():\n        class Config:\n            cpp_id_attributes = [\"id_attr\"]\n            cpp_paren_attributes = [\"paren_attr\"]\n        parser = DefinitionParser(target, location=None,\n                                  config=Config())\n        ast, isShorthand = parser.parse_xref_object()\n        parser.assert_end()\n    check('T<int>')\n    check('T<int, double>')\n    check('T<int, double, float>')\n    check('T<template<typename T> class C>')\n    check('T<template<typename T> class C<int>>')\n    check('T<template<typename T> class C<int, double>>')\n    check('T<template<typename T, typename U> class C>')\n    check('T<template<typename T, typename U> class C<int>>')\n    check('T<template<typename T, typename U> class C<int, double>>')\n    check('T<T>')\n    check('T<T, int>')\n    check('T<T, int, double>')\n    check('T<template<typename T> class C<T>>')\n    check('T<template<typename T> class C<T, int>>')\n    check('T<template<typename T> class C<T, int, double>>')\n    check('T<template<typename T, typename U> class C>')\n    check('T<template<typename T, typename U> class C<T>>')\n    check('T<template<typename T, typename U> class C<T, int>>')\n    check('T<template<typename T, typename U> class C<T, int, double>>')\n", "def test_domain_cpp_ast_function_template_specializations():\n    check('function', 'template<> void f<A<int>>()',\n          {1: 'f__A<int>', 2: 'IE1fN1AIiEEE', 4: 'IE1fvN1AIiEEE'})\n    check('function', 'void f<>(A<int> a)',\n          {1: 'f__A<int>', 2: 'IE1fN1AIiEE', 4: 'IE1fvN1AIiEE'})\n    check('function', 'void f<A<int>>(A<int> a)',\n          {1: 'f__A<int>', 2: 'IE1fN1AIiEE', 4: 'IE1fvN1AIiEE'})\n    check('function', 'template<> void f<A<int, char>>()',\n          {1: 'f__A<int, char>', 2: 'IE1fN1AIi1cEEE', 4: 'IE1fvN1AIi1cEEE'})\n    check('function', 'void f<>(A<int, char> a)',\n          {1: 'f__A<int, char>', 2: 'IE1fN1AIi1cEE', 4: 'IE1fvN1AIi1cEE'})\n    check('function', 'void f<A<int, char>>(A<int, char> a)',\n          {1: 'f__A<int, char>', 2: 'IE1fN1AIi1cEE', 4: 'IE1fvN1AIi1cEE'})\n    check('function', 'void f<>(A<int, char, double> a)',\n          {1: 'f__A<int, char, double>', 2: 'IE1fN1AIi1c1dEE', 4: 'IE1fvN1AIi1c1dEE'})\n    check('function', 'void f<A<int, char, double>>(A<int, char, double> a)',\n          {1: 'f__A<int, char, double>', 2: 'IE1fN1AIi1c1dEE', 4: 'IE1fvN1AIi1c1dEE'})\n", "def test_domain_cpp_ast_template_template_parameters():\n    # from #3508\n    check('type', 'template<template<int> class> {key}A', {2: 'II0E0E1A'})\n    check('type', 'template<template<int> typename> {key}A', {2: 'II0E0E1A'})\n    check('type', 'template<template<int> typename...> {key}A', {2: 'II0EDpE1A'})\n    check('type', 'template<template<int> typename... Ts> {key}A', {2: 'II0EDpE1A'})\n    check('type', 'template<template<int> auto> {key}A', {2: 'II0E0E1A'})\n    check('type', 'template<template<int> auto...> {key}A', {2: 'II0EDpE1A'})\n    check('type', 'template<template<int> auto... Ts> {key}A', {2: 'II0EDpE1A'})\n\n    # from #3542\n    check('type', \"template<template<typename> typename... T> {key}A\",\n          {2: \"I0I0EDpE1A\", 3: \"I0I0EDpE1AE\", 4: \"I0I0EDpE1AEE\"})\n\n    # from #3542\n    check('type', \"template<template<typename> auto... T> {key}A\",\n          {2: \"I0I0EDpE1A\", 3: \"I0I0EDpE1AE\", 4: \"I0I0EDpE1AEE\"})\n\n    # from #3542\n    check('type', \"template<template<typename> class... T> {key}A\",\n          {2: \"I0I0EDpE1A\", 3: \"I0I0EDpE1AE\", 4: \"I0I0EDpE1AEE\"})\n\n    # from #3542\n    check('type', \"template<template<typename...> class> {key}A\",\n          {2: \"II0EDpE0E1A\", 3: \"II0EDpE0E1AE\", 4: \"II0", "def test_domain_cpp_parse_expression_fallback():\n        ids = 'IE1CIA%s_1aE'\n        # call .format() on the expr to unescape double curly braces\n        idDict = {2: ids % expr.format(), 3: ids % id}\n        if id4 is not None:\n            idDict[4] = ids % id4\n        check('class', 'template<> {key}C<a[%s]>' % expr, idDict)\n\n        class Config:\n            cpp_id_attributes = [\"id_attr\"]\n            cpp_paren_attributes = [\"paren_attr\"]\n\n        parser = DefinitionParser(expr, location=None,\n                                  config=Config())\n        parser.allowFallbackExpressionParsing = True\n        ast = parser.parse_expression()\n        res = str(ast)\n        if res != expr:\n            print(\"\")\n            print(\"Input:    \", expr)\n            print(\"Result:   \", res)\n            raise DefinitionError(\"\")\n        displayString = ast.get_display_string()\n        if res != displayString:\n            # note: if the expression contains an anon name then this will trigger a falsely\n            print(\"\")\n            print(\"Input:    \", expr)\n            print(\"Result:   \", res)\n            print(\"Display:  \", displayString)\n            raise DefinitionError(\"\")\n\n    # these tests should fail with a DefinitionError\n    exprCheck('5[a]', id='flpl2_5aE')\n    exprCheck('a[5]', id='ix1aL5E')\n    exprCheck('5 + 5[a]', id='plL5Eflpl2_5aE')\n    exprCheck('5 + a[5]', id='plL5Eix1aL5E')\n    exprCheck('5[a] + 5', id='flpl2_5aEplL5E')\n    exprCheck('a[5] + 5', id='ix1aL5EplL5E')\n", "def test_domain_cpp_ast_function_template_parameter_packing():\n    # test for https://github.com/sphinx-doc/sphinx/issues/7714\n    check('function', 'template<template<int ...> typename T> void f(T<>)',\n          {2: 'I0_X1T1TIDp_iEEEvv'})\n    check('function', 'template<template<int ...> typename T> void f(T<1, 2>)',\n          {2: 'I0_X1T1TIDp_iEEv'})\n    check('function', 'template<template<int ...> typename T, typename ... Ts> void f(T<Ts...>)',\n          {2: 'I00_X1T1TIDp_iEEDpEEEvv'})\n    check('function', 'template<template<int ...> typename T, typename ... Ts> void f(T<>)',\n          {2: 'I00_X1T1TIDp_iEEDpEEEvv'})\n    check('function', 'template<template<int ...> typename T, typename ... Ts> void f(T<Ts>)',\n          {2: 'I00_X1T1TIDp_iEEDpEEEvv'})\n    check('function', 'template<template<int ...> typename T, typename ... Ts> void f(T<Ts, Ts...>)',\n          {2: 'I00_X1T1TIDp_iEEDpEEEvv'})\n"], "sample_303": ["def test_runshell_passes_args(self):\n    args = ['arg1', 'arg2']\n    env = {'KEY': 'value'}\n    with mock.patch('subprocess.run') as run:\n        with mock.patch.object(\n            BaseDatabaseClient,\n            'settings_to_cmd_args_env',\n            return_value=(args, env),\n        ):\n            self.client.runshell(None)\n        run.assert_called_once_with(args, env={'KEY': 'value', **os.environ}, check=True)\n", "def test_runshell_passes_args(self):\n    with mock.patch('subprocess.run') as run:\n        with mock.patch.object(\n            BaseDatabaseClient,\n            'settings_to_cmd_args_env',\n            return_value=(['arg1', 'arg2'], None),\n        ):\n            self.client.runshell(None)\n        run.assert_called_once_with(['arg1', 'arg2'], env=None, check=True)\n", "    def test_runshell_use_environment(self):\n        env = {'VARIABLE': 'value'}\n        args = ['command']\n        with mock.patch('subprocess.run') as run:\n            with mock.patch.object(\n                BaseDatabaseClient,\n                'settings_to_cmd_args_env',\n                return_value=(args, env),\n            ):\n                self.client.runshell(None)\n            run.assert_called_once_with(args, env={'VARIABLE': 'value', **os.environ}, check=True)\n", "def test_runshell_executable_not_implemented(self):\n    with mock.patch.object(BaseDatabaseClient, 'executable_name', None):\n        msg = 'subclasses of BaseDatabaseClient must provide an executable_name attribute'\n        with self.assertRaisesMessage(NotImplementedError, msg):\n            self.client.settings_to_cmd_args_env(None, None)\n", "def test_runshell_use_subprocess_run(self):\n    args = ['mock_arg1', 'mock_arg2']\n    env = {'VARIABLE': 'value'}\n    parameters = 'mock_parameters'\n    with mock.patch('subprocess.run') as run:\n        with mock.patch.object(\n            BaseDatabaseClient,\n            'settings_to_cmd_args_env',\n            return_value=(args, env),\n        ):\n            self.client.runshell(parameters)\n        run.assert_called_once_with(args, env={'VARIABLE': 'value', **os.environ}, check=True)\n"], "sample_1126": ["def test_dagger_power():\n    O = Operator('O')\n    assert Dagger(O**2) == Dagger(O)**2\n    assert Dagger(O**3) == Dagger(O)**3\n    assert Dagger(O**0) == IdentityOperator()\n", "def test_dagger_add():\n    O1 = Operator('O1')\n    O2 = Operator('O2')\n    assert Dagger(O1 + O2) == Dagger(O1) + Dagger(O2)\n    assert Dagger(O1 + I*O2) == Dagger(O1) - I*Dagger(O2)\n\n    A = symbols('A', commutative=False)\n    B = symbols('B', commutative=False)\n    assert Dagger(A + B).is_commutative is False\n", "def test_dagger_addition():\n    A = Operator('A')\n    B = Operator('B')\n    assert Dagger(A + B) == Dagger(A) + Dagger(B)\n    assert Dagger(A + 2*B) == Dagger(A) + Dagger(2*B)\n    assert Dagger(2*A + B) == Dagger(2*A) + Dagger(B)\n", "def test_dagger_power():\n    A = Operator('A')\n    assert Dagger(A**2) == Dagger(A)**2\n    assert Dagger(A**3) == Dagger(A)**3\n    assert Dagger(A**(3/2)) == Dagger(A)**(3/2)\n    assert Dagger(A**(-2)) == Dagger(A)**(-2)\n", "def test_dagger_power():\n    A = Operator('A')\n    B = Operator('B')\n    assert Dagger(A**2) == Dagger(A)**2\n    assert Dagger(A**3) == Dagger(A)**3\n    assert Dagger(A+B) == Dagger(A) + Dagger(B)\n    assert Dagger(A*B) == Dagger(B)*Dagger(A)\n    assert Dagger(A**0) == Dagger(IdentityOperator())\n    assert Dagger(A**1) == Dagger(A)\n"], "sample_1117": ["def test_lower_triangular():\n    assert ask(Q.lower_triangular(X + Z.T + Identity(2)), Q.lower_triangular(X) &\n            Q.upper_triangular(Z)) is True\n    assert ask(Q.lower_triangular(X*Z.T), Q.lower_triangular(X) &\n            Q.upper_triangular(Z)) is True\n    assert ask(Q.lower_triangular(Identity(3))) is True\n    assert ask(Q.lower_triangular(ZeroMatrix(3, 3))) is True\n    assert ask(Q.lower_triangular(OneMatrix(1, 1))) is True\n    assert ask(Q.lower_triangular(OneMatrix(3, 3))) is False\n    assert ask(Q.triangular(X), Q.unit_triangular(X))\n    assert ask(Q.lower_triangular(X**3), Q.lower_triangular(X))\n    assert ask(Q.lower_triangular(Y)) is False\n    assert ask(Q.lower_triangular(Y.T*X*Y)) is None\n    assert ask(Q.lower_triangular(Y.T*X*Y), Q.lower_triangular(X)) is True\n    assert ask(Q.lower_triangular(A1x1)) is True\n    assert ask(Q.lower_triangular(A1x1 + B1x1)) is True\n    assert ask(Q.lower_triangular(A1x1 * B1x1)) is True\n    assert ask(Q.lower_triangular(V1.T*V1)) is True\n    assert ask(Q.lower_triangular(V1.T*(V1 + V2))) is True\n    assert ask(Q.lower_triangular(V1.T*(V1 + V2) + A1x1)) is True\n    assert ask(Q.lower_triangular(MatrixSlice(Y, (0, 1), (1, 2)))) is True\n    assert ask(Q.lower_triangular(V1.T*(V1 + V2))) is True\n    assert ask(Q.lower_triangular(X**3), Q.lower_triangular(X))\n    assert ask(Q.lower_triangular(Identity(3)))\n    assert ask(Q.lower_triangular(DiagMatrix(V1)))\n    assert ask(Q.lower_triangular(DiagonalMatrix(X)))\n", "def test_element_wise_operations():\n    X = MatrixSymbol('X', 4, 4)\n    Y = MatrixSymbol('Y', 4, 4)\n    assert ask(Q.real_elements(X + Y), Q.real_elements(X) & Q.real_elements(Y))\n    assert ask(Q.real_elements(X * Y), Q.real_elements(X) & Q.real_elements(Y))\n    assert ask(Q.real_elements(X - Y), Q.real_elements(X) & Q.real_elements(Y))\n    assert ask(Q.real_elements(X.T), Q.real_elements(X))\n    assert ask(Q.real_elements(X.I), Q.real_elements(X) & Q.invertible(X))\n    assert ask(Q.integer_elements(X + Y), Q.integer_elements(X) & Q.integer_elements(Y))\n    assert ask(Q.integer_elements(X * Y), Q.integer_elements(X) & Q.integer_elements(Y))\n    assert ask(Q.integer_elements(X - Y), Q.integer_elements(X) & Q.integer_elements(Y))\n    assert ask(Q.integer_elements(X.T), Q.integer_elements(X))\n    assert ask(Q.integer_elements(X.I), Q.integer_elements(X) & Q.invertible(X))\n", "def test_lower_triangular():\n    assert ask(Q.lower_triangular(X + Z.T + Identity(2)), Q.lower_triangular(X) &\n            Q.upper_triangular(Z)) is True\n    assert ask(Q.lower_triangular(X*Z.T), Q.lower_triangular(X) &\n            Q.upper_triangular(Z)) is True\n    assert ask(Q.lower_triangular(Identity(3))) is True\n    assert ask(Q.lower_triangular(ZeroMatrix(3, 3))) is True\n    assert ask(Q.lower_triangular(OneMatrix(1, 1))) is True\n    assert ask(Q.lower_triangular(OneMatrix(3, 3))) is False\n    assert ask(Q.triangular(X), Q.unit_triangular(X))\n    assert ask(Q.lower_triangular(X**3), Q.lower_triangular(X))\n    assert ask(Q.lower_triangular(X), Q.upper_triangular(X) & Q.diagonal(X))\n    assert ask(Q.lower_triangular(X), Q.upper_triangular(X)) is None\n    assert ask(Q.lower_triangular(X.T), Q.upper_triangular(X))\n", "def test_upper_lower_triangular_square():\n    X = MatrixSymbol('X', 3, 3)\n    assert ask(Q.upper_triangular(X + X.T), Q.upper_triangular(X) & Q.lower_triangular(X.T))\n    assert ask(Q.lower_triangular(X + X.T), Q.upper_triangular(X.T) & Q.lower_triangular(X))\n    assert ask(Q.upper_triangular(X + X.T), Q.lower_triangular(X) & Q.lower_triangular(X.T)) is None\n    assert ask(Q.lower_triangular(X + X.T), Q.upper_triangular(X) & Q.upper_triangular(X.T)) is None\n    assert ask(Q.upper_triangular(X**3), Q.upper_triangular(X))\n    assert ask(Q.lower_triangular(X**3), Q.lower_triangular(X))\n", "def test_elementwise_operations():\n    X = MatrixSymbol('X', 4, 4)\n    Y = MatrixSymbol('Y', 4, 4)\n    assert ask(Q.real_elements(X + Y), Q.real_elements(X) & Q.real_elements(Y))\n    assert ask(Q.real_elements(X * Y), Q.real_elements(X) & Q.real_elements(Y))\n    assert ask(Q.real_elements(X.T), Q.real_elements(X))\n    assert ask(Q.real_elements(X.I), Q.real_elements(X) & Q.invertible(X))\n    assert ask(Q.real_elements(X + X.T), Q.real_elements(X))\n    assert ask(Q.real_elements(X - X.T), Q.real_elements(X))\n    assert ask(Q.real_elements(X * X.T), Q.real_elements(X))\n    assert ask(Q.real_elements(X - X), Q.real_elements(X))\n    assert ask(Q.real_elements(X * X), Q.real_elements(X))\n    assert ask(Q.real_elements(X + Identity(4)), Q.real_elements(X))\n    assert ask(Q.real_elements(X + ZeroMatrix(4, 4)), Q.real_elements(X))\n    assert ask(Q.real_elements(X - Identity(4)), Q.real_elements(X))\n    assert ask(Q.real_elements(X - ZeroMatrix(4, 4)), Q.real_elements(X))\n"], "sample_1035": ["def test_measure_all():\n    from sympy.physics.quantum.qubit import measure_all\n    from sympy.physics.quantum.gate import H\n    from sympy.physics.quantum.qapply import qapply\n\n    qubit = qapply(H(0)*H(1)*IntQubit(0, nqubits=2))\n    result = measure_all(qubit)\n    expected = [(IntQubit(0, nqubits=2), 1/4), (IntQubit(1, nqubits=2), 1/4),\n                (IntQubit(2, nqubits=2), 1/4), (IntQubit(3, nqubits=2), 1/4)]\n    assert result == expected\n\n    qubit = qapply(H(0)*IntQubit(0, nqubits=2))\n    result = measure_all(qubit)\n    expected = [(IntQubit(0, nqubits=2), 1/2), (IntQubit(2, nqubits=2), 1/2)]\n    assert result == expected\n", "def test_measure_all():\n    from sympy.physics.quantum.qubit import measure_all\n    from sympy.physics.quantum.gate import H\n    from sympy.physics.quantum.qapply import qapply\n\n    # Test that measure_all returns the correct results for a simple case\n    q = qapply(H(0)*H(1)*IntQubit(0, nqubits=2))\n    expected = [(IntQubit(0, nqubits=2), 1/4), (IntQubit(1, nqubits=2), 1/4),\n                (IntQubit(2, nqubits=2), 1/4), (IntQubit(3, nqubits=2), 1/4)]\n    assert sorted(measure_all(q)) == expected\n\n    # Test that measure_all returns the correct results for a more complex case\n    q = qapply(H(0)*H(1)*IntQubit(1, nqubits=2))\n    expected = [(IntQubit(0, nqubits=2), 1/4), (IntQubit(1, nqubits=2), 1/4),\n                (IntQubit(2, nqubits=2), 1/4), (IntQubit(3, nqubits=2), 1/4)]\n    assert sorted(measure_all(q)) == expected\n\n", "def test_measure_all_oneshot():\n    nbits = 2\n    basis_states = superposition_basis(nbits)\n    results = [measure_all_oneshot(basis_states, 'sympy') for _ in range(100)]\n    counts = {str(x): results.count(x) for x in set(results)}\n    for x in counts:\n        assert abs(counts[x] / 100 - 0.25) < 0.05\n", "def test_qubit_measurement():\n    from sympy.physics.quantum.qubit import measure_all, measure_partial, measure_all_oneshot, measure_partial_oneshot\n    from sympy.physics.quantum.gate import H, X, Y, Z\n    from sympy.physics.quantum.qapply import qapply\n    from sympy.physics.quantum.qubit import Qubit, IntQubit\n\n    q = qapply(H(0)*H(1)*Qubit('00'))\n    result = measure_all(q)\n    expected = [(Qubit('00'), 1/4), (Qubit('01'), 1/4), (Qubit('10'), 1/4), (Qubit('11'), 1/4)]\n    assert set(result) == set(expected)\n\n    q = qapply(X(0)*H(1)*Qubit('00'))\n    result = measure_partial(q, (0,))\n    expected = [(qapply(Qubit('00')/sqrt(2) + Qubit('10')/sqrt(2)), 1/2), (qapply(Qubit('01')/sqrt(2) + Qubit('11')/sqrt(2)), 1/2)]\n    assert set(result) == set(expected)\n\n    q = qapply(H(0)*H(1)*Qubit('00'))\n    result = measure_all_oneshot(q)\n    expected = [Qubit('00'), Qubit('01'), Qubit('10'), Qubit('11')]\n    assert result in expected\n\n    q = qapply(X(0)*H(1)*Qubit('00'))\n    result = measure_partial_oneshot(q, (0,))\n    expected = [qapply(Qubit('00')/sqrt(2) + Qubit('10')/sqrt(2)), qapply(Qubit('01')/sqrt(2) + Qubit('11')/sqrt(2))]\n    assert result in expected\n\n    q = qapply(H(0)*Qubit('0'))\n    result = measure_all(q)\n    expected = [(Qubit('0'), 1/2), (Qubit('1'), 1/2)]\n    assert set(result) == set(expected)\n\n    q = qapply(X(0)*Qubit('0'))\n    result = measure_all(q)\n    expected = [(Qubit('0'), 0), (Qubit('1'), 1)]\n    assert set(result) ==", "def test_qubit_measure():\n    # Test ensemble measurement of all qubits\n    q = IntQubit(1)/sqrt(2) + IntQubit(3)/sqrt(2)\n    res = q.qubit_to_matrix(q)\n    expected = [(IntQubit(1), 0.5), (IntQubit(3), 0.5)]\n    assert measure_all(res) == expected\n\n    # Test oneshot measurement of all qubits\n    res = q.qubit_to_matrix(q)\n    outcome = measure_all_oneshot(res)\n    assert outcome in [IntQubit(1), IntQubit(3)]\n\n    # Test partial ensemble measurement\n    q = IntQubit(1)/2 + IntQubit(2)/2 + IntQubit(5)/2 + IntQubit(6)/2\n    res = q.qubit_to_matrix(q)\n    expected = [\n        ((IntQubit(1) + IntQubit(5))/sqrt(2), 0.5),\n        ((IntQubit(2) + IntQubit(6))/sqrt(2), 0.5)\n    ]\n    assert measure_partial(res, (1,)) == expected\n\n    # Test oneshot partial measurement\n    outcome = measure_partial_oneshot(res, (1,))\n    assert outcome in [(IntQubit(1) + IntQubit(5))/sqrt(2), (IntQubit(2) + IntQubit(6))/sqrt(2)]\n\n    # Test measurement of a single qubit\n    q = IntQubit(1)/sqrt(2) + IntQubit(2)/sqrt(2)\n    res = q.qubit_to_matrix(q)\n    expected = [(IntQubit(1), 0.5), (IntQubit(2), 0.5)]\n    assert measure_all(res) == expected\n"], "sample_1116": ["def test_inverse_derivative_matrix_lines():\n    x = symbols('x')\n    A = MatrixSymbol('A', 3, 3)\n    expr = Inverse(A)\n    lines = expr._eval_derivative_matrix_lines(x)\n    assert len(lines) == 1\n    assert lines[0].first_pointer == -expr.T\n    assert lines[0].second_pointer == expr\n", "def test_inverse_derivative():\n    x = symbols('x')\n    A = MatrixSymbol('A', 3, 3)\n    assert isinstance((A**(-1))._eval_derivative_matrix_lines(x), list)\n    assert all(isinstance(line, tuple) for line in (A**(-1))._eval_derivative_matrix_lines(x))\n    assert all(len(line) == 2 for line in (A**(-1))._eval_derivative_matrix_lines(x))\n", "def test_derivative_matrix_lines():\n    x = symbols('x')\n    n = symbols('n', positive=True)\n    M = MatrixSymbol('M', n, n)\n    IM = Inverse(M)\n    dM = M._eval_derivative_matrix_lines(x)\n    assert len(dM) == n\n    for i in range(n):\n        assert dM[i].first_pointer == M.row(i)\n        assert dM[i].second_pointer == eye(n).row(i)\n\n    dIM = IM._eval_derivative_matrix_lines(x)\n    assert len(dIM) == n\n    for i in range(n):\n        assert dIM[i].first_pointer == -IM.T * M.row(i)\n        assert dIM[i].second_pointer == IM * eye(n).row(i)\n", "def test_derivative():\n    from sympy import Symbol\n    x = Symbol('x')\n    A = MatrixSymbol('A', 2, 2)\n    assert A.diff(x) == ZeroMatrix(2, 2)\n    assert Inverse(A).diff(x) == -Inverse(A)*A.diff(x)*Inverse(A)\n\n    A = MatrixSymbol('A', 2, 2).as_explicit()\n    assert A.diff(x) == ZeroMatrix(2, 2)\n    assert Inverse(A).diff(x) == -Inverse(A)*A.diff(x)*Inverse(A)\n\n    A = MatrixSymbol('A', 2, 2).as_explicit()\n    A.data[0, 0] = x\n    assert A.diff(x)[0, 0] == 1\n    assert Inverse(A).diff(x)[0, 0] == -1/x**2\n", "def test_inverse_derivative_matrix_lines():\n    from sympy import Symbol\n    x = Symbol('x')\n    A = MatrixSymbol('A', 3, 3)\n    expr = Inverse(A)\n    lines = expr._eval_derivative_matrix_lines(x)\n    assert len(lines) == 1\n    line = lines[0]\n    assert line.first_pointer == -expr.T\n    assert line.second_pointer == expr\n"], "sample_779": ["def test_check_class_weight_classifiers():\n    # check that ill-computed class weights raises an exception\n    est = BadBalancedWeightsClassifier(class_weight='balanced')\n    assert_raises_regex(AssertionError,\n                        \"Classifier estimator_name is not computing\"\n                        \" class_weight=balanced properly.\",\n                        check_class_weight_classifiers,\n                        'estimator_name',\n                        est)\n", "def test_check_dict_unchanged():\n    # check that check_dict_unchanged raises an error\n    # if the estimator changes its internal state\n    # during predict, transform, etc.\n    name = ChangesDict.__name__\n    msg = 'Estimator changes __dict__ during predict'\n    assert_raises_regex(AssertionError, msg, check_dict_unchanged,\n                        name, ChangesDict())\n", "def test_check_fit2d_1sample_memmap():\n    # Check that fitting a 2d array with only one sample either works or\n    # returns an informative message. The error message should either mention\n    # the number of samples or the number of classes.\n    from sklearn.utils.estimator_checks import check_fit2d_1sample\n    from sklearn.utils.estimator_checks import _boston_subset\n    from sklearn.linear_model import LinearRegression\n\n    X, y = _boston_subset()\n    X = X[:1]\n\n    assert_raises_regex(ValueError, \"Reshape your data either using array\"\n                                   r\".reshape\\(-1, 1\\) if your data has a \"\n                                   \"single feature or array.reshape\\(1, -1\\) \"\n                                   \"if it contains a single sample.\",\n                        check_fit2d_1sample, \"estimator\", LinearRegression())\n", "def test_check_estimator_no_validation():\n    # Test that estimators with no validation don't perform validation checks\n    class EstimatorWithoutValidation(BaseEstimator):\n            return self\n\n            return X\n\n            return {'no_validation': True}\n\n    est = EstimatorWithoutValidation()\n    # Check that fit2d_1sample doesn't fail with no validation\n    check_fit2d_1sample(EstimatorWithoutValidation.__name__, est)\n    # Check that fit2d_1feature doesn't fail with no validation\n    check_fit2d_1feature(EstimatorWithoutValidation.__name__, est)\n    # Check that fit1d doesn't fail with no validation\n    check_fit1d(EstimatorWithoutValidation.__name__, est)\n    # Check that complex data doesn't fail with no validation\n    check_complex_data(EstimatorWithoutValidation.__name__, est)\n    # Check that estimators_nan_inf doesn't fail with no validation\n    check_estimators_nan_inf(EstimatorWithoutValidation.__name__, est)\n    # Check that estimators_empty_data_messages doesn't fail with no validation\n    check_estimators_empty_data_messages(EstimatorWithoutValidation.__name__, est)\n", "def test_check_estimator_errors_on_input_validation():\n    # tests that check_estimator raises informative errors\n    # when the estimator fails on input validation.\n\n    class BadInputValidationClassifier(BaseEstimator, ClassifierMixin):\n            raise ValueError(\"input validation failed\")\n\n            pass\n\n    name = BadInputValidationClassifier.__name__\n    msg = (\"The classifier {} does not raise an error when incorrect/\"\n           \"malformed input data for fit is passed. Perhaps use \"\n           \"check_X_y in fit.\".format(name))\n    assert_raises_regex(AssertionError, msg, check_estimator, BadInputValidationClassifier())\n"], "sample_454": ["    def test_eq(self):\n        exclusion1 = models.Q(field__gt=models.F(\"field\"))\n        exclusion2 = models.Q(field__lt=models.F(\"field\"))\n        self.assertEqual(\n            models.ExclusionConstraint(\n                expressions=[(\"field\", \"&&\")],\n                name=\"exclusion\",\n                index_type=\"GIST\",\n            ),\n            models.ExclusionConstraint(\n                expressions=[(\"field\", \"&&\")],\n                name=\"exclusion\",\n                index_type=\"GIST\",\n            ),\n        )\n        self.assertEqual(\n            models.ExclusionConstraint(\n                expressions=[(\"field\", \"&&\")],\n                name=\"exclusion\",\n                index_type=\"GIST\",\n            ),\n            mock.ANY,\n        )\n        self.assertNotEqual(\n            models.ExclusionConstraint(\n                expressions=[(\"field\", \"&&\")],\n                name=\"exclusion\",\n                index_type=\"GIST\",\n            ),\n            models.ExclusionConstraint(\n                expressions=[(\"field\", \"&&\")],\n                name=\"exclusion2\",\n                index_type=\"GIST\",\n            ),\n        )\n        self.assertNotEqual(\n            models.ExclusionConstraint(\n                expressions=[(\"field\", \"&&\")],\n                name=\"exclusion\",\n                index_type=\"GIST\",\n            ),\n            models.ExclusionConstraint(\n                expressions=[(\"field\", \"||\")],\n                name=\"exclusion\",\n                index_type=\"GIST\",\n            ),\n        )\n        self.assertNotEqual(\n            models.ExclusionConstraint(\n                expressions=[(\"field\", \"&&\")],\n                name=\"exclusion\",\n                index_type=\"GIST\",\n            ),\n            models.ExclusionConstraint(\n                expressions=[(\"field\", \"&&\")],\n                name=\"exclusion\",\n                index_type=\"SPGIST\",\n            ),\n        )\n        self.assertNotEqual(\n            models.ExclusionConstraint(\n                expressions=[(\"field\", \"&&\")],\n                name=\"exclusion\",\n                index_type=\"GIST\",\n            ),\n            1,\n        )\n        self.assertNotEqual(\n            models.ExclusionConstraint(\n                expressions=[(\"field\", \"&&\")],\n                name=\"exclusion\",\n                index_type=\"GIST\",\n            ),\n            models.ExclusionConstraint(\n                expressions=[(\"field\", \"&&\")],\n                name=\"exclusion\",\n                index_type=\"GIST\",\n                violation_error_message=\"custom error\",\n            ),\n        )\n        self.assertNotEqual(\n            models.ExclusionConstraint(\n                expressions=[(\"field\", \"&&\")],\n                name=\"exclusion\",\n                index_type", "    def test_init(self):\n        with self.assertRaisesMessage(\n            ValueError, \"Exclusion constraints only support GiST or SP-GiST indexes.\"\n        ):\n            ExclusionConstraint(name=\"constraint\", expressions=[(\"field\", \"&&\")], index_type=\"btree\")\n", "    def test_eq(self):\n        constraint_1 = models.ExclusionConstraint(\n            name=\"exclusion\", expressions=[(\"price\", \"&&\"), (\"discounted_price\", \"&&\")]\n        )\n        constraint_2 = models.ExclusionConstraint(\n            name=\"exclusion\", expressions=[(\"price\", \"&&\"), (\"discounted_price\", \"&&\")]\n        )\n        constraint_3 = models.ExclusionConstraint(\n            name=\"exclusion\", expressions=[(\"price\", \"||\"), (\"discounted_price\", \"||\")]\n        )\n        self.assertEqual(constraint_1, constraint_2)\n        self.assertNotEqual(constraint_1, constraint_3)\n", "    def test_init(self):\n        with self.assertRaisesMessage(\n            ValueError, \"Exclusion constraints only support GiST or SP-GiST indexes.\"\n        ):\n            models.ExclusionConstraint(\n                name=\"exclusion_constraint\",\n                expressions=[(\"field\", \"&&\")],\n                index_type=\"invalid_index\",\n            )\n\n        with self.assertRaisesMessage(\n            ValueError, \"At least one expression is required to define an exclusion constraint.\"\n        ):\n            models.ExclusionConstraint(name=\"exclusion_constraint\", expressions=[])\n\n        with self.assertRaisesMessage(\n            ValueError,\n            \"ExclusionConstraint.condition must be a Q instance.\",\n        ):\n            models.ExclusionConstraint(\n                name=\"exclusion_constraint\",\n                expressions=[(\"field\", \"&&\")],\n                condition=\"invalid\",\n            )\n\n        with self.assertRaisesMessage(\n            ValueError,\n            \"ExclusionConstraint.deferrable must be a Deferrable instance.\",\n        ):\n            models.ExclusionConstraint(\n                name=\"exclusion_constraint\",\n                expressions=[(\"field\", \"&&\")],\n                deferrable=\"invalid\",\n            )\n\n        with self.assertRaisesMessage(\n            ValueError, \"ExclusionConstraint.include must be a list or tuple.\"\n        ):\n            models.ExclusionConstraint(\n                name=\"exclusion_constraint\",\n                expressions=[(\"field\", \"&&\")],\n                include=\"invalid\",\n            )\n", "    def test_eq(self):\n        expressions = [(\"field1\", \"<@\"), (\"field2\", \"&&@\")]\n        self.assertEqual(\n            models.ExclusionConstraint(expressions=expressions, name=\"exclusion\"),\n            models.ExclusionConstraint(expressions=expressions, name=\"exclusion\"),\n        )\n        self.assertEqual(\n            models.ExclusionConstraint(expressions=expressions, name=\"exclusion\"),\n            mock.ANY,\n        )\n        self.assertNotEqual(\n            models.ExclusionConstraint(expressions=expressions, name=\"exclusion\"),\n            models.ExclusionConstraint(expressions=expressions, name=\"exclusion2\"),\n        )\n        self.assertNotEqual(\n            models.ExclusionConstraint(expressions=expressions, name=\"exclusion\"),\n            models.ExclusionConstraint(\n                expressions=[(\"field1\", \"<@\"), (\"field2\", \"||@\")], name=\"exclusion\"\n            ),\n        )\n        self.assertNotEqual(\n            models.ExclusionConstraint(expressions=expressions, name=\"exclusion\"), 1\n        )\n        self.assertNotEqual(\n            models.ExclusionConstraint(expressions=expressions, name=\"exclusion\"),\n            models.ExclusionConstraint(\n                expressions=expressions, name=\"exclusion\", index_type=\"GIN\"\n            ),\n        )\n        self.assertNotEqual(\n            models.ExclusionConstraint(\n                expressions=expressions, name=\"exclusion\", index_type=\"GIST\"\n            ),\n            models.ExclusionConstraint(\n                expressions=expressions, name=\"exclusion\", index_type=\"GIN\"\n            ),\n        )\n        self.assertNotEqual(\n            models.ExclusionConstraint(\n                expressions=expressions, name=\"exclusion\", condition=models.Q()\n            ),\n            models.ExclusionConstraint(\n                expressions=expressions,\n                name=\"exclusion\",\n                condition=models.Q(field1=1),\n            ),\n        )\n        self.assertNotEqual(\n            models.ExclusionConstraint(\n                expressions=expressions, name=\"exclusion\", deferrable=models.Deferrable.DEFERRED\n            ),\n            models.ExclusionConstraint(\n                expressions=expressions,\n                name=\"exclusion\",\n                deferrable=models.Deferrable.IMMEDIATE,\n            ),\n        )\n        self.assertNotEqual(\n            models.ExclusionConstraint(\n                expressions=expressions, name=\"exclusion\", include=[\"field1\"]\n            ),\n            models.ExclusionConstraint(\n                expressions=expressions, name=\"exclusion\", include=[\"field2\"]\n            ),\n        )\n"], "sample_1087": ["def test_f_polys():\n    f0, f1, f2, f3, f4, f5, f6 = f_polys()\n    assert f0.as_expr() == x**2*y*z**2 + 2*x**2*y*z + 3*x**2*y + 2*x**2 + 3*x + 4*y**2*z**2 + 5*y**2*z + 6*y**2 + y*z**2 + 2*y*z + y + 1\n    assert f1.as_expr() == x**3*y*z + x**2*y**2*z**2 + x**2*y**2 + 20*x**2*y*z + 30*x**2*y + x**2*z**2 + 10*x**2*z + x*y**3*z + 30*x*y**2*z + 20*x*y**2 + x*y*z**3 + 10*x*y*z**2 + x*y*z + 610*x*y + 20*x*z**2 + 230*x*z + 300*x + y**2*z**2 + 10*y**2*z + 30*y*z**2 + 320*y*z + 200*y + 600*z + 6000\n    assert f2.as_expr() == x**5*y**3 + x**5*y**2*z + x**5*y*z**2 + x**5*z**3 + x**3*y**2 + x**3*y*z + 90*x**3*y + 90*x**3*z + x**2*y**2*z - 11*x**2*y**2 + x**2*z**3 - 11*x**2*z**2 + y*z - 11*y + 90*z - 990\n    assert f3.as_expr() == x**5*y**2 + x**4*z**4 + x**4 + x**3*y**3*z + x**3*z + x**2*y**4 + x**2*y**3*z**3 + x**2*y*z**5 + x**2*y*z + x*y**2*z**4 + x*y**2 + x*y*z**7 + x*y*z**3 + x*y*z**2 + y**2*z + y*z**4\n    assert f4.as_expr() == -x**9*y**8*z - x", "def test_f_polys():\n    f0, f1, f2, f3, f4, f5, f6 = f_polys()\n\n    assert f0.is_Mul\n    assert f1.is_Add\n    assert f2.is_Add\n    assert f3.is_Add\n    assert f4.is_Add\n    assert f5.is_Add\n    assert f6.is_Add\n\n", "def test_w_polys():\n    polys = w_polys()\n    for poly in polys:\n        assert isinstance(poly, Poly)\n\n    # Check the degree of each polynomial\n    assert polys[0].degree() == (6, 4, 2)\n    assert polys[1].degree() == (8, 3)\n", "def test_wang_polys():\n    f0, f1, f2, f3, f4, f5, f6 = f_polys()\n    w1, w2 = w_polys()\n\n    # test polynomial f0\n    assert f0 == x**2*y*z**2 + 2*x**2*y*z + 3*x**2*y + 2*x**2 + 3*x + 4*y**2*z**2 + 5*y**2*z + 6*y**2 + y*z**2 + 2*y*z + y + 1\n    # test polynomial f1\n    assert f1 == x**3*y*z + x**2*y**2*z**2 + x**2*y**2 + 20*x**2*y*z + 30*x**2*y + x**2*z**2 + 10*x**2*z + x*y**3*z + 30*x*y**2*z + 20*x*y**2 + x*y*z**3 + 10*x*y*z**2 + x*y*z + 610*x*y + 20*x*z**2 + 230*x*z + 300*x + y**2*z**2 + 10*y**2*z + 30*y*z**2 + 320*y*z + 200*y + 600*z + 6000\n    # test polynomial f2\n    assert f2 == x**5*y**3 + x**5*y**2*z + x**5*y*z**2 + x**5*z**3 + x**3*y**2 + x**3*y*z + 90*x**3*y + 90*x**3*z + x**2*y**2*z - 11*x**2*y**2 + x**2*z**3 - 11*x**2*z**2 + y*z - 11*y + 90*z - 990\n    # test polynomial f3\n    assert f3 == x**5*y**2 + x**4*z**4 + x**4 + x**3*y**3*z + x**3*z + x**2*y**4 + x**2*y**3*z**3 + x**2*y*z**5 + x**2*y*z + x*y**2*z**4 + x*y**2 + x*y*z**7 + x*y*z**3 + x*y*z**2 +", "def test_wang_polys():\n    wang_polys = [f.as_expr() for f in w_polys()]\n    assert len(wang_polys) == 2\n\n    assert wang_polys[0].as_poly(x, y, z).degree() == 6\n    assert wang_polys[1].as_poly(x, y).degree() == 8\n\n    wang_polys = [f.as_expr() for f in f_polys()]\n    assert len(wang_polys) == 7\n\n    assert wang_polys[0].as_poly(x, y, z).degree() == 4\n    assert wang_polys[1].as_poly(x, y, z).degree() == 6\n    assert wang_polys[2].as_poly(x, y, z).degree() == 5\n    assert wang_polys[3].as_poly(x, y, z).degree() == 5\n    assert wang_polys[4].as_poly(x, y, z).degree() == 9\n    assert wang_polys[5].as_poly(x, y, z).degree() == 3\n    assert wang_polys[6].as_poly(x, y, z, t).degree() == 4\n"], "sample_243": ["    def test_is_nullable(self):\n        query = Query(Item)\n        self.assertFalse(query.is_nullable(Item._meta.get_field('modified')))\n        self.assertFalse(query.is_nullable(Item._meta.get_field('name')))\n        self.assertTrue(query.is_nullable(Item._meta.get_field('note')))\n", "def test_resolve_lookup_value(self):\n    query = Query(Item)\n    lookup_value = Q(name=['a', 'b'])\n    resolved_value = query.resolve_lookup_value(lookup_value, False, True)\n    self.assertIsInstance(resolved_value, list)\n    self.assertEqual(resolved_value, ['a', 'b'])\n\n    lookup_value = [Q(name='a'), Q(name='b')]\n    resolved_value = query.resolve_lookup_value(lookup_value, False, True)\n    self.assertIsInstance(resolved_value, list)\n    self.assertEqual(resolved_value, [Q(name='a'), Q(name='b')])\n", "def test_add_annotation(self):\n    query = Query(Item)\n    query.add_annotation(Func('name', function='UPPER'), alias='upper_name')\n    self.assertIn('upper_name', query.annotations)\n    self.assertIsInstance(query.annotations['upper_name'], Func)\n    self.assertEqual(query.annotations['upper_name'].function, 'UPPER')\n    self.assertEqual(query.annotations['upper_name'].arg_joiner, ', ')\n    self.assertEqual(query.annotations['upper_name'].args, ['name'])\n\n    query = Query(Item)\n    query.add_annotation(Func('name', function='UPPER', output_field=CharField()), alias='upper_name', is_summary=True)\n    self.assertIn('upper_name', query.annotations)\n    self.assertIsInstance(query.annotations['upper_name'], Func)\n    self.assertEqual(query.annotations['upper_name'].function, 'UPPER')\n    self.assertEqual(query.annotations['upper_name'].arg_joiner, ', ')\n    self.assertEqual(query.annotations['upper_name'].args, ['name'])\n\n    query = Query(Item)\n    query.add_annotation(Func('name', function='UPPER', output_field=CharField()), alias='upper_name', select=False)\n    self.assertNotIn('upper_name', query.annotations)\n", "def test_add_annotation(self):\n    query = Query(Item)\n    query.add_annotation(Func('name', output_field=CharField()), 'name_func')\n    annotation = query.annotations['name_func']\n    self.assertIsInstance(annotation, Func)\n    self.assertEqual(annotation.output_field.__class__, CharField)\n\n    # Check that annotations can be used as filters\n    where = query.build_where(Q(name_func='foo'))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, Exact)\n    self.assertEqual(lookup.rhs, 'foo')\n    self.assertIsInstance(lookup.lhs, Ref)\n    self.assertEqual(lookup.lhs.refs, 'name_func')\n", "    def test_check_query_object_type(self):\n        query = Query(Item)\n        author = Author.objects.create(name='Author1')\n        item = Item.objects.create(name='Item1', creator=author)\n        # Test correct instance\n        query.check_query_object_type(item, Item._meta, Item._meta.get_field('creator'))\n        # Test incorrect instance\n        with self.assertRaisesMessage(ValueError, 'Cannot query \"Author\": Must be \"Item\" instance.'):\n            query.check_query_object_type(author, Item._meta, Item._meta.get_field('creator'))\n        # Test correct queryset\n        query.check_related_objects(Item._meta.get_field('creator'), Item.objects.all(), Item._meta)\n        # Test incorrect queryset\n        with self.assertRaisesMessage(ValueError, 'Cannot use QuerySet for \"Author\": Use a QuerySet for \"Item\".'):\n            query.check_related_objects(Item._meta.get_field('creator'), Author.objects.all(), Item._meta)\n"], "sample_1025": ["def test_AbstractPythonCodePrinter_imports():\n    p = AbstractPythonCodePrinter()\n    assert not p.module_imports\n    assert p.doprint(x**y) == 'x**y'\n    assert not p.module_imports\n    assert p.doprint(pi) == 'math.pi'\n    assert p.module_imports == {'math': {'pi'}}\n    assert p.doprint(acos(x)) == 'math.acos(x)'\n    assert p.module_imports == {'math': {'pi', 'acos'}}\n", "def test_NumPyPrinter_einsum():\n    p = NumPyPrinter()\n    expr = x**y + z**y\n    assert p.doprint(expr) == 'x**y + z**y'\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n    assert p.doprint(A*B) == '(A).dot(B)'\n    assert p.doprint(A*B*C) == '((A).dot(B)).dot(C)'\n", "def test_AbstractPythonCodePrinter_Sum():\n    p = AbstractPythonCodePrinter()\n    x = symbols('x')\n    expr = Sum(x, (x, 0, 10))\n    assert p.doprint(expr) == \"(builtins.sum(x for x in range(0, 10+1)))\"\n", "def test_Rational():\n    p = PythonCodePrinter()\n    assert p.doprint(Rational(1, 2)) == '1/2'\n", "def test_PythonCodePrinter_functions():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(acos(x)) == 'math.acos(x)'\n    assert prntr.doprint(acos(x).diff(x)) == 'math.cos(x)'\n    assert prntr.doprint(And(And(x, y), z)) == '(x and y) and z'\n    assert prntr.doprint(And(x, y, z)) == 'x and y and z'\n    assert prntr.doprint(Or(Or(x, y), z)) == '(x or y) or z'\n    assert prntr.doprint(Or(x, y, z)) == 'x or y or z'\n    assert prntr.doprint(x + y * z) == 'x + y*z'\n    assert prntr.doprint(x + y ** z) == 'x + y**z'\n    assert prntr.doprint(Mod(x, 2)) == 'x % 2'\n    assert prntr.doprint(Mod(x, 2, evaluate=False)) == 'x % 2'\n    assert prntr.doprint(Not(x)) == 'not x'\n    assert prntr.doprint(Not(x*y)) == 'not (x*y)'\n    assert prntr.doprint(And(x, Not(y))) == 'x and not y'\n    assert prntr.doprint(Or(x, Not(y))) == 'x or not y'\n    assert prntr.doprint(Not(And(x, y))) == 'not (x and y)'\n    assert prntr.doprint(Not(Or(x, y))) == 'not (x or y)'\n"], "sample_976": ["def test_var():\n    x = Symbol('x')\n    y = Symbol('y')\n\n    assert var('x') == x\n    assert var('x ') == x\n    assert var(' x ') == x\n    assert var('x,') == (x,)\n    assert var('x, ') == (x,)\n    assert var('x ,') == (x,)\n\n    assert var('x , y') == (x, y)\n\n    assert var('x,y,z') == (x, y, Symbol('z'))\n    assert var('x y z') == (x, y, Symbol('z'))\n\n    assert var('x,y,z,') == (x, y, Symbol('z'))\n    assert var('x y z ') == (x, y, Symbol('z'))\n\n    xyz = Symbol('xyz')\n    abc = Symbol('abc')\n\n    assert var('xyz') == xyz\n    assert var('xyz,') == (xyz,)\n    assert var('xyz,abc') == (xyz, abc)\n\n    assert var(('xyz',)) == (xyz,)\n    assert var(('xyz,',)) == ((xyz,),)\n    assert var(('x,y,z,',)) == ((x, y, Symbol('z')),)\n    assert var(('xyz', 'abc')) == (xyz, abc)\n    assert var(('xyz,abc',)) == ((xyz, abc),)\n    assert var(('xyz,abc', 'x,y,z')) == ((xyz, abc), (x, y, Symbol('z')))\n\n    assert var(('x', 'y', 'z')) == (x, y, Symbol('z'))\n    assert var(['x', 'y', 'z']) == [x, y, Symbol('z')]\n    assert var(set(['x', 'y', 'z'])) == set([x, y, Symbol('z')])\n\n    raises(ValueError, lambda: var(''))\n    raises(ValueError, lambda: var(','))\n    raises(ValueError, lambda: var('x,,y,,z'))\n    raises(ValueError, lambda: var(('x', '', 'y', '', 'z')))\n\n    a, b = var('x,y', real=True)\n    assert a.is_real and b.is_real\n\n    x0 = Symbol('x0')\n    x1 = Symbol('x1')\n    x2 = Symbol('x2')\n\n    y0 = Symbol('y0')\n    y1 = Symbol('y1')\n\n    assert var('", "def test_sort_key():\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n\n    assert x.sort_key() < y.sort_key()\n    assert y.sort_key() < z.sort_key()\n\n    xdummy = Dummy('x')\n    ydummy = Dummy('y')\n    zdummy = Dummy('z')\n\n    assert xdummy.sort_key() < ydummy.sort_key()\n    assert ydummy.sort_key() < zdummy.sort_key()\n\n    xwild = Wild('x')\n    ywild = Wild('y')\n    zwild = Wild('z')\n\n    assert xwild.sort_key() < ywild.sort_key()\n    assert ywild.sort_key() < zwild.sort_key()\n\n    # test that symbols are sorted before Dummy and Wild\n    assert x.sort_key() < xdummy.sort_key()\n    assert x.sort_key() < xwild.sort_key()\n\n    # test that Dummy are sorted before Wild\n    assert xdummy.sort_key() < xwild.sort_key()\n", "def test_Symbol_properties():\n    x = Symbol(\"x\", positive=True, integer=True)\n    y = Symbol(\"y\", negative=True, real=True)\n    z = Symbol(\"z\", odd=True, even=False)\n\n    assert x.is_positive\n    assert x.is_integer\n    assert not x.is_negative\n    assert x.is_real\n\n    assert y.is_negative\n    assert y.is_real\n    assert not y.is_positive\n    assert not y.is_integer\n\n    assert z.is_odd\n    assert not z.is_even\n    assert not z.is_integer\n    assert not z.is_real\n", "def test_Wild_matches():\n    x = Symbol(\"x\")\n    y = Symbol(\"y\")\n    a = Wild(\"a\")\n    b = Wild(\"b\", exclude=[x])\n\n    assert x.match(a) == {a_: x}\n    assert y.match(a) == {a_: y}\n    assert x.match(b) is None\n    assert y.match(b) == {b_: y}\n\n    e = x + y\n    assert e.match(a + b) == {a_: x, b_: y}\n    assert e.match(a + x) is None\n\n    assert x.match(a, {}) == {a_: x}\n    assert x.match(a, {a_: y}) == {a_: x}  # match will not reuse existing keys\n    assert x.match(a, {b_: y}) == {a_: x, b_: y}  # old keys are preserved\n", "def test_Dummy_hashability():\n    d1 = Dummy('x')\n    d2 = Dummy('x')\n\n    # Check that Dummies with same name but different dummy_index are not hashable together\n    assert hash(d1) != hash(d2)\n\n    # Check that Dummy's hash does not change after being used as a key in a dictionary\n    d = {}\n    d[d1] = 'value'\n    assert hash(d1) == hash(d.keys()[0])\n\n    # Check that Dummy's hash is consistent between Python sessions\n    # This is tricky to test as hash values can change between Python sessions\n    # However we can check that the hash is the same when the Dummy is recreated\n    # with the same dummy_index\n    d3 = Dummy('x', dummy_index=d1.dummy_index)\n    assert hash(d1) == hash(d3)\n"], "sample_907": ["def test_domain_cpp_ast_template_concept_constraints():\n    check('function', 'template<typename T> requires A B<T> void f()',\n          {4: 'I0EIQaa1A1B1TEv'})\n    check('function', 'template<typename T> requires A B<T> && C<T> void f()',\n          {4: 'I0EIQaa1A1B1TEaa1C1TEv'})\n    check('function', 'template<typename T> requires A B<T> || C<T> void f()',\n          {4: 'I0EIQoo1A1B1TEoo1C1TEv'})\n    check('function', 'template<typename T> requires A B<T> && C<T> || D<T> void f()',\n          {4: 'I0EIQaa1A1B1TEaa1C1TEoo1D1TEv'})\n    check('function', 'template<typename T> requires A B<T> && C<T> || D<T> and E<T> void f()',\n          {4: 'I0EIQaa1A1B1TEaa1C1TEoo1D1TEaa1E1TEv'})\n    check('function', 'template<typename T> requires A B<T> && C<T> || D<T> or E<T> void f()',\n          {4: 'I0EIQaa1A1B1TEoo1D1TEoo1E1TEv'})\n", "def test_domain_cpp_ast_function_type_in_template():\n    check('function', 'int f<void> (void (*) (int, double))',\n          {1: 'f__voidPFivdE', 2: '1fPFivdE'})\n    check('function', 'int f<int> (void (*) (int, double))',\n          {1: 'f__iPFivdE', 2: '1fPFivdE'})\n    check('function', 'template <int> int f<void> (void (*) (int, double))',\n          {2: 'I_iE1fvPFivdE'})\n    check('function', 'template <int> int f<int> (void (*) (int, double))',\n          {2: 'I_iE1fPFivdE'})\n    check('function', 'template <int> template <void> int f<void> (void (*) (int, double))',\n          {2: 'I_iE1fvPFivdE'})\n    check('function', 'template <int> template <int> int f<void> (void (*) (int, double))',\n          {2: 'I_iE1fvPFivdE'})\n", "def test_domain_cpp_ast_template_instantiations():\n    # TODO: these should be tested with both default and explicit specializations\n    check('function', 'void f(std::vector<int>)', {1: 'f__std::vector:i', 2: '1fNSt7vectorIiEEE'})\n    check('function', 'void f(std::vector<int>) const', {1: 'f__std::vector:iC', 2: '1fNSt7vectorIiEEE'})\n    check('function', 'void f(std::vector<int> const)', {1: 'f__std::vector:Ki', 2: '1fNSt7vectorIiEKE'})\n    check('function', 'void f(std::vector<int> volatile)', {1: 'f__std::vector:iV', 2: '1fNSt7vectorIiEEE'})\n    check('function', 'void f(std::vector<int> volatile const)', {1: 'f__std::vector:iVC', 2: '1fNSt7vectorIiEEE'})\n    check('function', 'void f(std::vector<int> volatile const &)', {1: 'f__std::vector:iVCR', 2: '1fRKVNSt7vectorIiEE'})\n    check('function', 'void f(std::vector<int> volatile const &&)', {1: 'f__std::vector:iVCO', 2: '1fRVNSt7vectorIiEE'})\n    check('function', 'void f(std::vector<int> volatile const *const)', {1: 'f__std::vector:iVCPC', 2: '1fPKVNSt7vectorIiEE'})\n    check('function', 'void f(std::vector<int> volatile const *const &)', {1: 'f__std::vector:iVCRPC', 2: '1fPKRVNSt7vectorIiEE'})\n\n    check('function', 'void f(std::vector<int, float> *)', {1: 'f__std::vector:i.f:PC', 2: '1fPSt7vectorIifE'})\n    check('function', 'void f(std::vector<int, float> volatile const)', {1: 'f__std::vector:i.f:VC', 2: '1fSt7vectorIifE'})\n    check('", "def test_domain_cpp_ast_identifier_name():\n    # test identifier name with various attributes\n    check('member', '{key}[[attr]] T attr_volatile v', {1: \"v__T\", 2: \"1v\"})\n    check('member', '{key}[[attr1, attr2]] T v', {1: \"v__T\", 2: \"1v\"})\n    check('member', '[[attr1]] [[attr2]] {key}T v', {1: \"v__T\", 2: \"1v\"},\n          output='[[attr1]] [[attr2]] {key}T v')\n    check('member', '{key}T [[attr1]] [[attr2]] v', {1: \"v__T\", 2: \"1v\"},\n          output='{key}T [[attr1]] [[attr2]] v')\n    check('member', '[[attr1]] T [[attr2]] {key}v', {1: \"v__T\", 2: \"1v\"},\n          output='[[attr1]] T [[attr2]] {key}v')\n    check('member', '{key}[[attr1]] T [[attr2]] v', {1: \"v__T\", 2: \"1v\"},\n          output='{key}[[attr1]] T [[attr2]] v')\n    check('member', '[[attr1]] [[attr2]] T {key}v', {1: \"v__T\", 2: \"1v\"},\n          output='[[attr1]] [[attr2]] T {key}v')\n\n    # test identifier name with various attributes and ID attribute\n    check('member', '[[]] [[attr]] {key}T attr_volatile v', {1: \"v__T\", 2: \"1v\"})\n    check('member', '[[]] [[attr1, attr2]] {key}T v', {1: \"v__T\", 2: \"1v\"})\n    check('member', '[[attr1]] [[attr2]] {key}[[]] T v', {1: \"v__T\", 2: \"1v\"},\n          output='[[attr1]] [[attr2]] {key}[[]] T v')\n    check('member', '{key}[[]] [[attr1]] [[attr2]] T v', {1: \"v__T\", 2:", "def test_domain_cpp_xref_to_template_function():\n    class Config:\n        cpp_id_attributes = [\"id_attr\"]\n        cpp_paren_attributes = [\"paren_attr\"]\n\n    text = \"f<int>\"\n    parser = DefinitionParser(text, location=None, config=Config())\n    ast, isShorthand = parser.parse_xref_object()\n    parser.assert_end()\n    assert isShorthand is True\n    parentSymbol = Symbol(None, None, None, None, None, None, None)\n    parentSymbol._add_enumerator_to_parent = lambda x: None\n    parentSymbol._add_template_and_function_params = lambda: None\n    rootSymbol = parentSymbol\n    target = 'f<int>'\n    rootSymbol.find_name(ASTNestedName([], [], rooted=False),\n                         [], typ='function',\n                         templateShorthand=True,\n                         matchSelf=True, recurseInAnon=True,\n                         searchInSiblings=False)\n    symbols, _ = rootSymbol.find_name(ASTNestedName([], [], rooted=False),\n                                      [], typ='function',\n                                      templateShorthand=True,\n                                      matchSelf=True, recurseInAnon=True,\n                                      searchInSiblings=False)\n    assert len(symbols) == 0\n    symbols, _ = rootSymbol.find_name(ASTNestedName([], [], rooted=False),\n                                      [], typ='function',\n                                      templateShorthand=False,\n                                      matchSelf=True, recurseInAnon=True,\n                                      searchInSiblings=False)\n    assert len(symbols) == 0\n    # so it should be a shorthand\n    parser = DefinitionParser(target, location=None, config=Config())\n    ast, isShorthand = parser.parse_xref_object()\n    parser.assert_end()\n    assert isShorthand is True\n    # it should not be the case\n    parser = DefinitionParser(target, location=None, config=Config())\n    ast, isShorthand = parser.parse_xref_object()\n    parser.assert_end()\n    assert isShorthand is True\n    # so the ast is correct, now for a non-shorthand\n    parser = DefinitionParser(target, location=None, config=Config())\n    ast, isShorthand = parser.parse_xref_object()\n    parser.assert_end()\n    assert isShorthand is False\n    assert ast.name.get_display_string() == target\n    assert ast.name.num_templates() == 1\n    assert ast.name.templates[0]\n    assert not ast.name.templates[0][0]\n    # should resolve to the arbitrary first symbol\n    symbols,"], "sample_873": ["def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n\n    # Test with default feature names\n    feature_names_out = sel.get_feature_names_out()\n    assert_array_equal(feature_names_t, feature_names_out)\n\n    # Test with custom feature names\n    feature_names_custom = np.array(list(\"abcdefghij\"))\n    feature_names_out_custom = sel.get_feature_names_out(feature_names_custom)\n    assert_array_equal(feature_names_t, feature_names_out_custom)\n\n    # Test with mismatched feature names\n    feature_names_mismatch = np.array(list(\"abcdefghijk\"))\n    with pytest.raises(ValueError):\n        sel.get_feature_names_out(feature_names_mismatch)\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n\n    # Test with default feature names\n    feature_names_out = sel.get_feature_names_out()\n    assert_array_equal(feature_names_t, feature_names_out)\n\n    # Test with custom feature names\n    feature_names_out = sel.get_feature_names_out(feature_names)\n    assert_array_equal(feature_names_t, feature_names_out)\n\n    # Test with feature names that do not match the number of features\n    with pytest.raises(ValueError):\n        sel.get_feature_names_out(feature_names[:-1])\n\n    # Test with None feature names\n    with pytest.raises(ValueError):\n        sel.get_feature_names_out(None)\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n\n    # Test with default feature names\n    assert_array_equal(feature_names_t, sel.get_feature_names_out())\n\n    # Test with custom feature names\n    custom_feature_names = list(\"abcdefghij\")\n    assert_array_equal(feature_names_t, sel.get_feature_names_out(custom_feature_names))\n\n    # Test with feature names that don't match the number of features\n    with pytest.raises(ValueError):\n        sel.get_feature_names_out(list(\"abcdefgh\"))\n\n    # Test with no feature names\n    with pytest.raises(ValueError):\n        sel.get_feature_names_out(None)\n", "def test_get_feature_names_out():\n    \"\"\"Check that feature names are correctly filtered.\"\"\"\n    sel = StepSelector()\n    sel.fit(X, y)\n    assert_array_equal(feature_names_t, sel.get_feature_names_out(input_features=feature_names))\n\n    # Check default feature names\n    assert_array_equal(feature_names_t, sel.get_feature_names_out())\n\n    # Check with incorrect feature names\n    with pytest.raises(ValueError):\n        sel.get_feature_names_out(input_features=[\"wrong_names\"])\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n\n    # Check with default input features\n    feature_names_out_actual = sel.get_feature_names_out()\n    assert_array_equal(feature_names_t, feature_names_out_actual)\n\n    # Check with provided input features\n    feature_names_out_actual = sel.get_feature_names_out(feature_names)\n    assert_array_equal(feature_names_t, feature_names_out_actual)\n\n    # Check with wrong input features\n    with pytest.raises(ValueError):\n        sel.get_feature_names_out([\"x\"] * X.shape[1])\n\n    # Check with None input features\n    with pytest.raises(ValueError):\n        sel.get_feature_names_out(None)\n\n    # Check when feature_names_in_ is not defined\n    sel.fit(X, y)\n    del sel.feature_names_in_\n    feature_names_out_actual = sel.get_feature_names_out()\n    assert_array_equal([f\"x{i}\" for i in range(X.shape[1]) if i % 2 == 0], feature_names_out_actual)\n"]}, "raw_scored_generations": {"sample_0": [false, false, true, false, false], "sample_3": [true, true, false, true, false], "sample_1": [false, false, false, false, false], "sample_2": [false, false, false, false, false], "sample_4": [false, false, false, false, false], "sample_5": [false, false, false, false, false], "sample_6": [false, false, false, false, false], "sample_7": [false, false, false, false, false], "sample_8": [false, false, true, false, false], "sample_9": [false, false, false, false, false], "sample_10": [false, false, true, true, true], "sample_11": [true, false, false, false, false], "sample_12": [true, false, false, false, false], "sample_13": [false, false, false, false, true], "sample_14": [false, true, false, false, false], "sample_15": [true, true, true, true, true], "sample_16": [false, true, false, true, false], "sample_17": [true, true, false, false, false], "sample_18": [false, false, false, false, false], "sample_19": [false, false, false, false, false], "sample_20": [true, false, false, false, false], "sample_21": [true, false, false, false, false], "sample_22": [false, false, false, false, false], "sample_23": [true, false, false, false, false], "sample_24": [false, false, false, false, false], "sample_25": [false, false, false, false, false], "sample_26": [false, false, false, false, false], "sample_27": [true, true, false, true, true], "sample_28": [false, false, false, false, false], "sample_29": [false, false, false, false, false], "sample_30": [true, false, false, true, false], "sample_31": [false, false, false, false, false], "sample_32": [true, false, false, false, true], "sample_33": [true, false, false, true, false], "sample_34": [false, false, false, false, false], "sample_35": [false, false, true, true, false], "sample_36": [false, false, true, false, false], "sample_37": [false, false, false, false, false], "sample_38": [false, false, false, false, false], "sample_39": [true, false, true, false, false], "sample_40": [false, false, false, false, false], "sample_41": [false, false, false, false, false], "sample_42": [false, false, false, false, false], "sample_43": [false, false, false, false, false], "sample_44": [false, false, true, false, false], "sample_45": [false, true, false, true, true], "sample_46": [false, false, false, false, true], "sample_47": [false, false, true, false, false], "sample_48": [true, true, true, true, false], "sample_49": [false, false, true, false, false], "sample_50": [false, true, false, true, false], "sample_51": [true, true, true, false, true], "sample_52": [false, true, false, false, false], "sample_54": [true, true, true, true, true], "sample_53": [true, false, true, true, true], "sample_55": [false, false, false, false, false], "sample_58": [true, false, true, true, false], "sample_56": [false, true, true, true, false], "sample_57": [true, true, true, true, true], "sample_59": [true, false, false, false, false], "sample_60": [false, false, false, false, false], "sample_61": [true, false, true, false, false], "sample_62": [true, false, false, false, false], "sample_63": [true, false, true, true, true], "sample_64": [false, false, false, false, false], "sample_65": [true, true, true, true, true], "sample_67": [true, false, false, false, true], "sample_66": [false, true, false, true, true], "sample_68": [false, false, false, false, false], "sample_69": [false, false, false, false, false], "sample_70": [true, false, true, false, true], "sample_71": [true, true, true, false, true], "sample_72": [true, false, true, true, false], "sample_73": [false, false, false, false, false], "sample_75": [false, false, false, false, false], "sample_74": [false, false, false, false, false], "sample_76": [true, true, true, true, true], "sample_77": [true, false, false, false, false], "sample_78": [true, true, true, false, false], "sample_79": [true, false, false, false, false], "sample_80": [true, true, true, true, true], "sample_82": [true, true, false, true, true], "sample_81": [false, false, false, false, false], "sample_83": [false, true, false, false, false], "sample_85": [false, false, false, false, false], "sample_84": [false, false, false, false, false], "sample_86": [true, false, true, false, false], "sample_88": [true, false, true, true, true], "sample_87": [false, false, false, true, false], "sample_89": [true, false, false, false, false], "sample_90": [false, false, false, false, false], "sample_91": [false, true, true, false, true], "sample_92": [false, false, false, false, false], "sample_93": [true, true, true, false, true], "sample_94": [false, false, false, false, false], "sample_95": [false, false, false, false, false], "sample_98": [false, false, false, false, false], "sample_96": [true, true, true, true, true], "sample_99": [false, true, true, true, false], "sample_97": [true, false, false, false, false], "sample_100": [false, false, true, false, false], "sample_102": [true, true, true, true, true], "sample_101": [false, false, false, false, true], "sample_103": [true, true, true, true, true], "sample_104": [false, true, false, false, false], "sample_107": [false, false, false, false, false], "sample_106": [false, false, true, true, false], "sample_105": [false, false, false, false, false], "sample_108": [false, false, false, false, false], "sample_109": [true, true, true, true, true], "sample_111": [true, true, true, true, false], "sample_110": [false, false, false, false, false], "sample_112": [false, false, true, false, true], "sample_113": [false, false, false, false, true], "sample_114": [true, true, false, true, true], "sample_115": [false, false, false, false, false], "sample_116": [true, true, false, true, false], "sample_117": [true, false, true, false, false], "sample_118": [true, true, true, true, true], "sample_119": [true, false, true, true, true], "sample_120": [true, true, false, true, true], "sample_121": [false, false, true, false, true], "sample_122": [false, false, false, false, false], "sample_123": [false, false, false, false, false], "sample_124": [true, true, true, false, false], "sample_125": [true, true, true, true, false], "sample_126": [true, true, true, true, false], "sample_127": [true, true, true, true, false], "sample_128": [true, false, false, true, true], "sample_129": [false, false, false, false, false], "sample_130": [true, true, true, false, true], "sample_131": [true, false, false, false, true], "sample_132": [false, true, false, false, false], "sample_133": [true, true, true, true, true], "sample_135": [false, true, true, false, false], "sample_134": [false, false, false, false, true], "sample_136": [true, true, false, false, false], "sample_139": [true, false, false, true, false], "sample_137": [true, true, true, true, true], "sample_138": [false, false, false, false, false], "sample_140": [false, false, false, false, false], "sample_141": [false, true, false, true, false], "sample_142": [true, true, true, true, true], "sample_143": [true, true, false, true, true], "sample_144": [true, true, true, true, true], "sample_145": [true, true, true, true, true], "sample_146": [true, true, true, true, true], "sample_147": [true, true, true, true, true], "sample_148": [true, false, false, true, true], "sample_151": [true, true, true, true, true], "sample_149": [false, false, true, false, false], "sample_152": [true, true, false, true, false], "sample_150": [true, false, false, false, false], "sample_153": [true, true, false, false, true], "sample_154": [true, true, true, true, true], "sample_155": [true, true, true, true, true], "sample_156": [true, true, false, false, false], "sample_157": [false, false, true, false, false], "sample_158": [true, true, false, false, false], "sample_159": [false, false, false, false, true], "sample_160": [true, true, true, true, true], "sample_161": [true, false, false, false, true], "sample_162": [true, true, false, false, true], "sample_163": [false, false, false, false, false], "sample_164": [false, false, false, false, false], "sample_165": [false, false, false, false, false], "sample_166": [true, true, true, true, true], "sample_167": [false, false, false, false, false], "sample_168": [true, true, true, true, true], "sample_169": [true, false, false, false, false], "sample_171": [true, true, false, true, true], "sample_170": [false, false, false, false, false], "sample_172": [true, true, true, false, true], "sample_173": [true, false, false, false, false], "sample_174": [false, false, false, true, true], "sample_175": [true, true, false, true, true], "sample_176": [true, false, true, true, true], "sample_177": [true, false, true, true, true], "sample_178": [true, false, true, true, false], "sample_180": [false, false, true, false, false], "sample_179": [false, true, false, false, true], "sample_182": [true, false, true, false, false], "sample_181": [true, true, true, true, true], "sample_183": [false, false, true, false, false], "sample_184": [false, false, false, false, false], "sample_185": [true, false, true, false, true], "sample_186": [false, true, true, true, true], "sample_187": [true, true, true, true, true], "sample_188": [false, false, false, false, false], "sample_189": [false, true, false, false, true], "sample_190": [true, true, true, false, true], "sample_191": [false, false, false, false, false], "sample_192": [true, true, true, false, true], "sample_193": [false, false, true, false, false], "sample_194": [false, true, true, true, false], "sample_195": [false, false, false, false, false], "sample_196": [true, true, true, false, true], "sample_198": [false, false, false, true, false], "sample_197": [true, true, true, true, true], "sample_199": [true, true, true, true, false], "sample_200": [true, true, true, true, true], "sample_201": [true, true, true, true, true], "sample_202": [true, true, false, true, true], "sample_203": [false, true, false, false, false], "sample_204": [true, false, false, true, false], "sample_205": [true, true, false, true, true], "sample_206": [true, true, true, true, true], "sample_207": [false, true, false, false, true], "sample_208": [true, true, true, true, true], "sample_209": [false, true, true, true, false], "sample_210": [false, false, false, false, true], "sample_211": [false, false, false, false, false], "sample_213": [false, false, false, false, true], "sample_212": [true, true, false, true, true], "sample_214": [false, false, true, false, true], "sample_215": [false, false, true, false, false], "sample_216": [true, true, true, true, true], "sample_217": [false, false, false, false, false], "sample_218": [false, false, false, true, true], "sample_219": [false, false, false, false, false], "sample_220": [false, true, false, true, true], "sample_221": [true, false, true, false, true], "sample_222": [false, false, true, true, true], "sample_223": [false, false, true, false, false], "sample_224": [true, true, false, true, false], "sample_225": [false, false, false, false, false], "sample_226": [true, false, false, false, true], "sample_227": [false, true, true, true, true], "sample_228": [true, false, false, false, false], "sample_229": [true, true, true, true, true], "sample_230": [true, true, true, true, true], "sample_231": [false, false, false, false, false], "sample_232": [false, false, false, false, false], "sample_233": [true, true, true, true, true], "sample_234": [true, true, true, true, true], "sample_235": [false, false, false, true, false], "sample_236": [true, false, true, false, true], "sample_237": [false, false, false, false, true], "sample_238": [true, true, true, false, true], "sample_239": [false, true, false, true, false], "sample_240": [true, true, true, true, false], "sample_241": [false, false, false, false, false], "sample_242": [false, false, false, false, false], "sample_243": [false, true, true, true, false], "sample_244": [false, false, true, true, false], "sample_245": [true, true, true, true, true], "sample_246": [true, false, true, false, true], "sample_247": [true, true, true, true, true], "sample_248": [false, true, true, true, false], "sample_249": [false, false, false, true, false], "sample_250": [true, true, true, false, true], "sample_251": [true, false, true, false, true], "sample_252": [true, false, false, true, false], "sample_253": [true, false, true, false, false], "sample_254": [false, false, false, true, false], "sample_256": [true, true, false, true, false], "sample_255": [false, false, false, false, false], "sample_257": [false, false, true, false, false], "sample_258": [false, false, false, false, false], "sample_259": [true, true, true, true, true], "sample_260": [true, true, true, true, true], "sample_261": [true, false, false, false, false], "sample_262": [false, true, false, true, true], "sample_263": [true, true, true, true, true], "sample_264": [true, true, false, true, true], "sample_265": [true, true, true, true, true], "sample_266": [true, true, false, true, true], "sample_267": [false, false, false, false, false], "sample_268": [false, false, false, false, true], "sample_269": [true, true, true, true, true], "sample_270": [true, true, false, false, true], "sample_271": [true, false, false, false, false], "sample_272": [true, false, false, true, false], "sample_273": [false, false, false, false, false], "sample_274": [false, false, false, false, false], "sample_275": [false, false, false, false, true], "sample_276": [false, false, false, false, false], "sample_277": [true, true, true, true, false], "sample_278": [false, false, true, false, false], "sample_279": [true, true, true, false, true], "sample_280": [true, true, true, false, false], "sample_281": [true, true, true, true, true], "sample_282": [true, true, true, true, true], "sample_283": [true, true, true, true, true], "sample_284": [false, true, false, false, false], "sample_285": [true, false, true, true, true], "sample_286": [false, false, false, false, false], "sample_287": [true, false, false, true, true], "sample_288": [false, false, true, false, false], "sample_289": [true, false, false, false, false], "sample_290": [true, true, true, true, true], "sample_291": [false, false, false, false, false], "sample_292": [true, true, true, true, true], "sample_293": [false, false, false, false, true], "sample_294": [true, true, true, false, false], "sample_295": [true, false, false, false, false], "sample_296": [true, true, true, true, true], "sample_297": [false, false, false, true, false], "sample_298": [true, true, true, true, true], "sample_299": [true, false, true, true, true], "sample_300": [true, true, true, true, true], "sample_301": [false, false, false, false, false], "sample_302": [false, true, false, false, false], "sample_303": [true, true, false, true, true], "sample_304": [false, true, true, false, true], "sample_305": [true, false, true, true, false], "sample_306": [false, true, false, true, true], "sample_307": [true, false, true, true, true], "sample_308": [true, false, true, true, true], "sample_309": [false, false, false, false, false], "sample_310": [false, false, true, false, false], "sample_312": [true, true, true, true, true], "sample_311": [false, false, false, false, true], "sample_313": [true, true, true, true, false], "sample_314": [false, false, false, true, false], "sample_315": [true, true, true, false, true], "sample_316": [true, true, false, false, false], "sample_317": [true, true, false, false, true], "sample_318": [false, false, false, false, false], "sample_319": [true, true, true, true, true], "sample_320": [true, true, false, true, true], "sample_321": [false, false, true, true, true], "sample_322": [false, false, true, false, false], "sample_323": [true, false, false, true, true], "sample_324": [false, true, false, true, true], "sample_325": [true, true, true, true, false], "sample_326": [true, true, true, true, true], "sample_327": [true, true, true, true, true], "sample_328": [false, false, true, false, true], "sample_329": [false, false, true, false, false], "sample_330": [false, false, false, false, false], "sample_331": [false, true, false, false, false], "sample_332": [true, true, true, true, false], "sample_333": [true, true, true, true, true], "sample_334": [true, true, true, false, false], "sample_335": [true, true, true, true, true], "sample_336": [false, false, true, false, false], "sample_337": [true, true, true, true, true], "sample_338": [true, true, true, false, false], "sample_339": [false, false, false, false, false], "sample_340": [true, true, false, false, false], "sample_341": [true, true, false, true, true], "sample_342": [true, true, true, true, true], "sample_343": [false, false, false, false, true], "sample_344": [true, false, true, true, false], "sample_345": [false, false, false, false, false], "sample_346": [false, false, false, false, false], "sample_347": [true, true, true, true, true], "sample_348": [false, false, false, false, false], "sample_349": [true, true, false, true, false], "sample_350": [true, true, true, true, true], "sample_351": [false, true, true, false, false], "sample_352": [false, true, false, false, false], "sample_353": [true, false, false, false, false], "sample_354": [false, false, false, false, false], "sample_355": [false, false, false, false, false], "sample_356": [true, true, true, true, false], "sample_357": [true, false, false, true, true], "sample_358": [false, false, false, false, false], "sample_359": [true, false, true, true, true], "sample_360": [false, false, false, true, false], "sample_361": [true, true, true, false, true], "sample_362": [true, true, true, true, true], "sample_363": [true, true, true, false, true], "sample_364": [false, false, false, false, false], "sample_365": [true, false, true, true, true], "sample_366": [false, true, false, true, false], "sample_367": [false, false, false, false, false], "sample_368": [true, true, false, true, true], "sample_369": [false, true, false, false, true], "sample_370": [false, false, false, false, false], "sample_371": [false, false, false, false, false], "sample_372": [false, false, false, false, false], "sample_373": [false, false, false, false, false], "sample_374": [false, false, false, false, false], "sample_375": [false, true, true, true, true], "sample_376": [true, true, true, true, true], "sample_377": [false, false, false, true, false], "sample_378": [true, false, false, false, true], "sample_379": [true, true, true, true, true], "sample_380": [true, true, true, false, false], "sample_381": [true, true, false, false, true], "sample_382": [true, false, true, false, false], "sample_383": [false, false, false, false, false], "sample_384": [false, true, false, false, false], "sample_385": [true, true, true, true, false], "sample_386": [false, false, false, false, true], "sample_387": [true, true, true, true, false], "sample_388": [false, false, false, false, false], "sample_389": [false, false, true, false, true], "sample_390": [true, false, true, true, true], "sample_391": [true, true, true, true, true], "sample_392": [false, true, false, true, false], "sample_393": [true, true, true, true, true], "sample_394": [false, false, false, false, false], "sample_395": [false, false, true, true, false], "sample_396": [false, false, false, false, false], "sample_397": [true, true, true, false, false], "sample_398": [false, false, false, false, false], "sample_399": [true, true, true, true, true], "sample_400": [false, true, true, true, true], "sample_401": [false, true, true, true, false], "sample_402": [false, false, true, true, true], "sample_403": [false, true, true, true, false], "sample_404": [true, true, true, true, true], "sample_405": [false, true, true, true, true], "sample_406": [false, false, false, false, false], "sample_407": [true, true, true, true, true], "sample_408": [true, true, true, true, false], "sample_409": [true, false, true, false, false], "sample_410": [true, true, true, true, false], "sample_411": [true, true, false, true, true], "sample_412": [true, true, false, false, false], "sample_413": [false, false, false, false, false], "sample_414": [true, true, false, false, true], "sample_415": [true, true, false, false, false], "sample_416": [true, true, true, true, true], "sample_417": [false, true, false, false, false], "sample_418": [false, false, false, false, false], "sample_419": [true, true, true, false, true], "sample_420": [true, false, true, false, false], "sample_421": [false, true, false, false, false], "sample_422": [false, false, false, false, false], "sample_423": [true, true, true, true, true], "sample_424": [true, true, true, true, true], "sample_425": [false, true, true, true, false], "sample_426": [true, true, false, true, true], "sample_427": [true, true, true, true, true], "sample_428": [true, true, false, true, true], "sample_429": [true, true, false, true, true], "sample_430": [true, true, true, true, false], "sample_431": [false, true, false, false, false], "sample_432": [true, true, true, true, true], "sample_433": [true, true, true, true, true], "sample_434": [false, false, false, false, false], "sample_435": [true, true, false, false, true], "sample_436": [false, false, false, false, false], "sample_437": [true, false, false, false, false], "sample_438": [false, false, true, false, false], "sample_439": [false, false, true, false, false], "sample_440": [false, true, true, false, true], "sample_441": [false, false, false, false, false], "sample_442": [true, true, true, false, true], "sample_443": [false, true, false, false, false], "sample_444": [false, false, false, false, false], "sample_445": [true, true, true, false, true], "sample_446": [false, false, false, true, false], "sample_447": [true, true, true, true, false], "sample_448": [true, true, true, false, false], "sample_449": [false, false, false, false, false], "sample_450": [true, true, true, false, false], "sample_451": [true, true, false, true, false], "sample_453": [false, true, false, false, true], "sample_452": [false, true, true, true, true], "sample_454": [false, false, false, false, false], "sample_455": [true, true, true, true, false], "sample_456": [true, false, true, true, true], "sample_457": [false, true, true, false, false], "sample_458": [false, true, false, false, true], "sample_459": [false, false, false, false, false], "sample_460": [false, true, false, false, false], "sample_461": [false, false, false, false, false], "sample_462": [true, true, false, true, true], "sample_463": [true, true, true, true, true], "sample_464": [true, true, true, true, true], "sample_465": [false, false, true, false, true], "sample_466": [true, true, true, true, true], "sample_467": [true, true, true, true, true], "sample_469": [true, false, true, true, false], "sample_468": [true, true, true, true, true], "sample_470": [false, false, true, false, false], "sample_471": [false, true, true, false, false], "sample_472": [true, true, false, true, true], "sample_473": [false, false, false, false, false], "sample_474": [true, false, false, false, false], "sample_475": [true, true, true, true, false], "sample_476": [false, false, false, false, false], "sample_477": [false, false, true, false, true], "sample_478": [true, true, true, true, true], "sample_479": [true, true, true, true, true], "sample_480": [true, false, false, true, false], "sample_481": [false, false, false, false, false], "sample_482": [false, false, false, false, false], "sample_483": [true, true, true, false, true], "sample_484": [false, false, false, false, false], "sample_485": [true, true, true, true, true], "sample_486": [true, true, true, false, false], "sample_487": [false, false, false, true, true], "sample_488": [false, false, false, false, false], "sample_489": [true, false, true, true, false], "sample_490": [true, true, true, false, false], "sample_491": [true, false, false, false, false], "sample_492": [true, true, true, true, true], "sample_493": [false, true, true, true, false], "sample_494": [true, true, false, true, true], "sample_495": [true, true, false, true, true], "sample_496": [false, false, false, false, false], "sample_497": [true, true, true, false, true], "sample_498": [true, false, false, false, false], "sample_499": [false, false, false, false, false], "sample_500": [true, true, false, false, false], "sample_501": [true, false, false, true, true], "sample_502": [true, false, false, false, false], "sample_503": [false, true, false, false, false], "sample_504": [false, false, false, false, false], "sample_505": [false, false, false, true, false], "sample_506": [false, false, false, false, false], "sample_507": [true, false, true, true, false], "sample_508": [false, false, false, true, false], "sample_509": [false, false, true, true, false], "sample_510": [true, false, false, false, false], "sample_511": [false, false, false, false, false], "sample_512": [false, true, false, false, false], "sample_513": [true, false, false, false, false], "sample_514": [false, false, false, false, true], "sample_515": [true, false, true, false, false], "sample_516": [false, false, false, false, false], "sample_517": [true, false, false, true, false], "sample_518": [false, false, false, false, false], "sample_519": [false, false, false, false, false], "sample_520": [false, false, false, false, false], "sample_521": [false, false, false, false, false], "sample_522": [false, false, false, false, false], "sample_523": [true, false, false, false, true], "sample_524": [false, false, false, false, false], "sample_525": [false, false, true, true, false], "sample_526": [false, false, false, false, false], "sample_527": [false, false, false, false, false], "sample_528": [true, true, false, false, false], "sample_529": [true, false, false, false, false], "sample_530": [true, false, false, false, false], "sample_531": [false, false, false, false, false], "sample_532": [true, false, false, false, true], "sample_533": [false, false, false, false, true], "sample_534": [false, false, false, false, false], "sample_535": [false, false, false, false, false], "sample_536": [false, false, false, false, false], "sample_537": [false, false, false, false, false], "sample_538": [true, false, false, false, true], "sample_539": [false, false, false, false, false], "sample_540": [false, true, false, false, false], "sample_541": [false, false, false, false, false], "sample_542": [false, true, false, true, false], "sample_543": [false, true, false, false, false], "sample_544": [false, false, false, false, false], "sample_545": [false, true, false, false, false], "sample_546": [false, false, false, false, false], "sample_547": [false, false, false, false, false], "sample_548": [false, false, false, false, false], "sample_549": [true, false, true, false, false], "sample_550": [false, false, false, false, false], "sample_551": [false, false, true, false, false], "sample_552": [false, false, false, true, false], "sample_553": [false, false, false, false, false], "sample_554": [false, true, false, false, false], "sample_555": [false, true, false, false, false], "sample_556": [false, true, false, true, false], "sample_557": [false, false, false, false, false], "sample_558": [true, true, false, false, true], "sample_559": [true, false, false, false, false], "sample_560": [true, false, false, false, false], "sample_561": [true, true, true, false, false], "sample_562": [false, true, true, false, false], "sample_563": [false, false, false, false, false], "sample_564": [false, false, false, false, false], "sample_565": [false, false, false, false, false], "sample_566": [true, false, false, false, false], "sample_567": [false, false, false, false, false], "sample_568": [false, false, false, false, false], "sample_569": [false, false, false, false, false], "sample_570": [true, false, true, false, false], "sample_571": [false, false, false, false, false], "sample_572": [true, false, false, true, false], "sample_573": [false, false, false, false, true], "sample_574": [false, false, false, true, false], "sample_575": [false, false, false, false, false], "sample_576": [false, true, false, false, true], "sample_577": [false, false, false, false, true], "sample_578": [false, false, false, false, false], "sample_579": [false, false, false, false, true], "sample_580": [false, false, false, false, false], "sample_581": [false, false, false, false, false], "sample_582": [false, false, false, false, false], "sample_583": [false, false, false, false, false], "sample_584": [true, false, true, false, true], "sample_585": [true, false, true, false, false], "sample_586": [false, false, false, false, false], "sample_587": [true, true, true, false, false], "sample_588": [true, true, true, false, true], "sample_589": [false, false, false, false, false], "sample_590": [false, false, false, false, true], "sample_591": [false, false, false, false, false], "sample_592": [false, false, false, true, true], "sample_593": [false, false, false, false, false], "sample_594": [false, false, false, false, false], "sample_595": [false, false, false, false, false], "sample_596": [true, false, false, false, false], "sample_597": [false, false, true, false, false], "sample_598": [false, false, false, false, true], "sample_599": [true, false, true, false, false], "sample_600": [false, true, false, false, false], "sample_601": [false, false, false, false, false], "sample_602": [true, false, true, false, false], "sample_603": [false, false, false, false, false], "sample_604": [true, false, false, false, false], "sample_605": [false, false, false, false, false], "sample_606": [false, false, false, true, false], "sample_607": [false, false, false, false, false], "sample_608": [false, false, false, false, true], "sample_609": [false, true, false, false, false], "sample_610": [false, true, false, true, true], "sample_611": [false, true, false, false, false], "sample_612": [false, false, true, true, false], "sample_613": [false, false, false, false, true], "sample_614": [false, false, false, false, false], "sample_615": [false, false, false, false, false], "sample_616": [false, false, false, false, false], "sample_617": [false, false, false, false, false], "sample_618": [false, false, false, false, false], "sample_619": [false, false, false, true, false], "sample_620": [false, false, true, false, false], "sample_621": [false, false, false, false, false], "sample_622": [false, true, true, false, false], "sample_623": [false, false, false, false, false], "sample_624": [false, false, false, false, false], "sample_625": [true, false, false, false, false], "sample_626": [false, false, false, false, false], "sample_627": [true, false, false, false, false], "sample_628": [false, false, false, false, false], "sample_629": [false, false, false, false, false], "sample_630": [false, false, false, false, false], "sample_631": [false, false, false, false, false], "sample_632": [false, false, false, false, false], "sample_633": [false, false, false, false, false], "sample_634": [false, false, false, false, false], "sample_635": [false, false, false, false, false], "sample_636": [false, false, false, false, false], "sample_637": [false, false, false, false, false], "sample_638": [false, false, false, false, false], "sample_639": [false, false, false, false, true], "sample_640": [false, true, false, false, false], "sample_641": [false, false, true, false, true], "sample_642": [false, false, false, false, false], "sample_643": [false, false, false, false, false], "sample_644": [false, false, false, false, false], "sample_645": [true, false, false, false, false], "sample_646": [false, false, false, false, false], "sample_647": [true, true, true, false, false], "sample_648": [false, false, true, false, false], "sample_649": [false, false, false, false, false], "sample_650": [false, false, false, false, false], "sample_651": [false, true, false, false, false], "sample_652": [false, true, true, true, true], "sample_653": [false, false, false, false, false], "sample_654": [false, false, false, false, true], "sample_655": [false, false, false, false, false], "sample_656": [true, false, false, false, false], "sample_657": [false, false, false, false, false], "sample_658": [false, false, false, false, true], "sample_659": [false, false, false, false, false], "sample_660": [false, false, false, false, false], "sample_661": [false, false, false, false, false], "sample_662": [false, false, false, false, false], "sample_663": [true, false, false, false, true], "sample_664": [false, false, false, false, false], "sample_665": [false, false, true, true, true], "sample_666": [true, false, false, false, true], "sample_667": [true, true, false, true, false], "sample_668": [false, false, false, false, false], "sample_669": [false, false, true, false, true], "sample_670": [false, false, false, false, false], "sample_671": [true, false, true, false, false], "sample_672": [false, false, false, false, false], "sample_673": [true, false, false, true, false], "sample_674": [false, false, false, false, false], "sample_675": [false, false, false, false, false], "sample_676": [false, true, false, false, false], "sample_677": [false, false, false, false, false], "sample_678": [false, false, false, false, false], "sample_679": [false, false, false, false, false], "sample_680": [false, false, false, false, false], "sample_681": [false, false, false, false, false], "sample_682": [false, false, false, false, true], "sample_683": [false, false, false, false, false], "sample_684": [false, false, true, false, false], "sample_685": [false, true, false, false, false], "sample_686": [false, false, false, false, false], "sample_687": [false, false, false, false, false], "sample_688": [true, false, true, true, false], "sample_689": [false, false, false, false, false], "sample_690": [false, false, false, false, false], "sample_691": [false, false, false, false, false], "sample_692": [false, false, true, false, true], "sample_693": [false, false, false, false, false], "sample_694": [false, false, false, false, false], "sample_695": [false, false, false, false, false], "sample_696": [false, false, false, false, false], "sample_697": [false, true, false, false, false], "sample_698": [false, false, false, false, false], "sample_699": [false, true, false, false, false], "sample_700": [false, false, false, true, false], "sample_701": [false, false, false, false, false], "sample_702": [false, true, false, false, false], "sample_703": [false, false, false, false, false], "sample_704": [false, false, false, false, false], "sample_705": [false, false, true, false, false], "sample_706": [false, false, false, false, false], "sample_707": [false, false, false, false, false], "sample_708": [false, false, false, false, false], "sample_709": [false, false, false, false, true], "sample_710": [false, false, false, false, false], "sample_711": [true, false, true, false, false], "sample_712": [false, false, false, false, false], "sample_713": [false, false, true, true, false], "sample_714": [true, false, false, true, false], "sample_715": [true, false, false, false, false], "sample_716": [true, true, true, false, false], "sample_717": [true, true, true, true, true], "sample_718": [false, false, false, false, false], "sample_719": [false, false, false, false, false], "sample_720": [true, false, false, false, true], "sample_721": [false, false, false, false, false], "sample_722": [true, true, true, false, false], "sample_723": [false, false, false, false, false], "sample_724": [true, false, false, true, false], "sample_725": [false, true, true, false, false], "sample_726": [false, false, false, false, false], "sample_727": [false, false, false, false, false], "sample_728": [false, false, false, false, false], "sample_729": [false, false, false, false, true], "sample_730": [false, true, false, false, false], "sample_731": [false, false, false, true, false], "sample_732": [true, true, true, true, true], "sample_733": [false, true, false, false, false], "sample_734": [false, true, false, false, true], "sample_735": [true, true, true, true, false], "sample_736": [false, false, false, false, true], "sample_737": [false, true, false, false, false], "sample_738": [false, false, true, false, false], "sample_739": [false, true, false, true, false], "sample_740": [false, false, false, false, false], "sample_741": [false, false, false, false, false], "sample_742": [false, false, false, true, false], "sample_743": [false, true, false, true, false], "sample_744": [false, true, false, false, true], "sample_745": [false, false, false, true, false], "sample_746": [false, true, false, false, false], "sample_747": [true, false, false, false, false], "sample_748": [false, true, false, false, false], "sample_749": [false, false, true, false, true], "sample_750": [false, false, false, false, false], "sample_751": [false, false, false, false, false], "sample_752": [true, false, true, true, true], "sample_753": [false, false, false, false, false], "sample_754": [false, false, false, false, false], "sample_755": [false, true, true, false, false], "sample_756": [true, false, true, false, false], "sample_757": [true, false, false, false, false], "sample_758": [false, false, false, false, true], "sample_759": [false, true, false, true, true], "sample_760": [false, false, false, true, false], "sample_761": [false, false, false, false, false], "sample_762": [false, false, false, false, true], "sample_763": [false, false, false, true, false], "sample_764": [true, true, false, false, false], "sample_765": [true, false, false, false, false], "sample_766": [true, true, false, true, true], "sample_767": [true, true, true, false, false], "sample_768": [false, false, false, false, true], "sample_769": [false, true, true, false, false], "sample_770": [false, false, false, false, false], "sample_771": [false, true, false, false, false], "sample_772": [false, false, false, false, true], "sample_773": [false, true, false, true, false], "sample_774": [false, false, false, false, false], "sample_775": [false, false, false, false, false], "sample_776": [false, false, false, false, false], "sample_777": [false, false, false, false, true], "sample_778": [false, false, true, false, false], "sample_779": [false, false, false, false, false], "sample_780": [true, true, false, false, false], "sample_781": [false, false, true, false, false], "sample_782": [true, false, true, false, false], "sample_783": [false, false, false, false, false], "sample_784": [true, true, false, false, false], "sample_785": [true, false, true, false, false], "sample_786": [false, false, false, false, false], "sample_787": [false, false, true, false, false], "sample_788": [false, false, true, false, false], "sample_789": [false, true, true, false, false], "sample_790": [false, false, false, false, false], "sample_791": [true, false, true, false, false], "sample_792": [false, false, true, true, true], "sample_793": [false, false, false, false, false], "sample_794": [true, false, false, false, false], "sample_795": [false, false, false, true, false], "sample_796": [false, true, true, true, false], "sample_797": [true, true, false, false, false], "sample_798": [true, false, false, false, false], "sample_799": [false, false, false, false, false], "sample_800": [false, false, false, false, false], "sample_801": [false, false, false, false, false], "sample_802": [true, false, true, false, false], "sample_803": [false, false, false, false, false], "sample_804": [false, true, false, false, false], "sample_805": [false, false, false, false, false], "sample_806": [false, false, false, false, false], "sample_807": [false, false, false, false, false], "sample_808": [false, false, false, true, false], "sample_809": [false, false, true, false, false], "sample_810": [true, true, false, false, true], "sample_811": [true, false, false, false, true], "sample_812": [false, false, false, false, false], "sample_813": [false, true, false, true, false], "sample_814": [false, false, false, false, false], "sample_815": [false, true, true, false, false], "sample_816": [false, false, false, false, false], "sample_817": [true, false, true, false, false], "sample_818": [false, true, false, false, false], "sample_819": [true, true, false, false, false], "sample_820": [false, false, false, false, false], "sample_821": [true, false, true, false, true], "sample_822": [false, false, false, false, false], "sample_823": [true, true, false, false, true], "sample_824": [false, false, true, false, false], "sample_825": [false, false, false, false, false], "sample_826": [false, true, false, false, false], "sample_827": [false, false, false, false, false], "sample_828": [false, true, false, false, false], "sample_829": [false, true, true, true, false], "sample_830": [false, false, true, false, false], "sample_831": [false, false, false, false, false], "sample_832": [false, false, false, false, true], "sample_833": [true, false, true, false, false], "sample_834": [true, true, false, true, false], "sample_835": [true, false, true, false, true], "sample_836": [false, false, false, false, false], "sample_837": [false, false, false, false, false], "sample_838": [false, false, true, false, false], "sample_839": [false, false, false, false, false], "sample_840": [true, true, false, false, false], "sample_841": [false, true, false, false, false], "sample_842": [false, false, false, false, false], "sample_843": [false, false, false, false, false], "sample_844": [false, false, false, true, false], "sample_845": [true, false, false, false, false], "sample_846": [true, false, false, false, false], "sample_847": [true, false, true, true, false], "sample_848": [true, false, false, false, false], "sample_849": [false, false, false, true, false], "sample_850": [false, false, false, false, false], "sample_851": [false, true, false, false, false], "sample_852": [false, false, false, false, false], "sample_853": [true, false, false, false, false], "sample_854": [true, false, false, false, false], "sample_855": [true, false, false, false, true], "sample_856": [true, false, false, false, false], "sample_857": [false, false, false, false, false], "sample_858": [true, true, true, true, false], "sample_859": [true, false, false, false, false], "sample_860": [true, true, true, false, false], "sample_861": [false, false, true, false, false], "sample_862": [true, false, false, false, false], "sample_863": [true, false, true, false, false], "sample_864": [false, false, false, false, false], "sample_865": [false, true, true, false, true], "sample_866": [true, true, true, false, false], "sample_867": [true, true, false, false, false], "sample_868": [false, false, false, false, false], "sample_869": [false, false, false, false, false], "sample_870": [true, false, false, false, false], "sample_871": [true, false, true, true, false], "sample_872": [true, false, false, false, false], "sample_873": [false, false, false, false, false], "sample_874": [false, false, false, false, false], "sample_875": [false, false, false, false, false], "sample_876": [false, false, false, false, false], "sample_877": [false, true, false, true, false], "sample_878": [false, true, false, false, false], "sample_879": [true, false, false, false, false], "sample_880": [false, false, false, true, false], "sample_881": [false, true, false, false, false], "sample_882": [false, false, false, false, false], "sample_883": [false, false, false, true, false], "sample_884": [true, true, true, false, false], "sample_885": [true, false, false, false, false], "sample_886": [false, false, false, false, false], "sample_887": [false, false, false, false, false], "sample_888": [false, false, true, false, true], "sample_889": [true, false, false, false, false], "sample_890": [false, false, true, false, true], "sample_891": [false, false, false, false, false], "sample_892": [false, false, false, false, true], "sample_893": [true, false, false, false, false], "sample_894": [false, false, false, false, false], "sample_895": [false, false, true, false, false], "sample_896": [false, false, false, true, false], "sample_897": [true, false, false, false, false], "sample_898": [false, false, false, false, false], "sample_899": [false, false, false, false, false], "sample_900": [true, true, false, false, false], "sample_901": [false, false, false, false, false], "sample_902": [false, false, false, true, false], "sample_903": [true, true, false, false, false], "sample_904": [false, false, false, false, false], "sample_905": [false, false, false, false, false], "sample_906": [false, false, false, false, false], "sample_907": [false, false, false, false, false], "sample_908": [false, false, false, true, false], "sample_909": [false, false, false, false, false], "sample_910": [false, true, false, false, false], "sample_911": [false, false, false, false, false], "sample_912": [false, false, false, false, false], "sample_913": [false, false, false, false, false], "sample_914": [true, false, false, false, false], "sample_915": [true, false, false, false, false], "sample_916": [false, false, false, false, false], "sample_917": [false, false, false, false, false], "sample_918": [false, false, false, false, false], "sample_919": [false, false, false, false, false], "sample_920": [true, false, false, false, false], "sample_921": [false, false, false, true, false], "sample_922": [false, false, false, false, false], "sample_923": [false, false, false, false, false], "sample_924": [false, false, false, false, false], "sample_925": [false, false, true, false, false], "sample_926": [false, false, false, false, false], "sample_927": [false, false, false, false, false], "sample_928": [false, false, false, false, false], "sample_929": [false, false, false, false, false], "sample_930": [false, true, false, false, false], "sample_931": [false, false, false, false, false], "sample_932": [false, false, false, false, false], "sample_933": [false, false, false, false, false], "sample_934": [false, false, false, false, false], "sample_935": [false, false, false, false, false], "sample_936": [true, true, false, true, true], "sample_937": [false, false, true, false, true], "sample_938": [false, true, false, false, false], "sample_939": [false, false, true, false, false], "sample_940": [true, false, false, false, false], "sample_941": [false, false, false, false, false], "sample_942": [false, false, false, false, false], "sample_943": [false, false, false, false, false], "sample_944": [false, false, false, false, false], "sample_945": [false, false, false, false, false], "sample_946": [false, false, false, false, false], "sample_947": [false, false, false, false, false], "sample_948": [false, false, false, false, false], "sample_949": [false, false, false, false, false], "sample_950": [false, false, false, false, false], "sample_951": [false, false, false, false, false], "sample_952": [true, false, false, false, false], "sample_953": [false, false, false, false, false], "sample_954": [false, false, false, false, false], "sample_955": [false, false, false, false, false], "sample_956": [false, false, false, false, false], "sample_957": [false, false, false, true, false], "sample_958": [false, false, false, false, false], "sample_959": [false, false, false, false, false], "sample_960": [false, false, false, false, false], "sample_961": [true, false, false, true, false], "sample_962": [false, false, false, false, false], "sample_963": [false, false, true, false, false], "sample_964": [false, false, false, false, false], "sample_965": [false, false, false, false, false], "sample_966": [true, false, false, false, false], "sample_967": [false, false, false, false, false], "sample_968": [true, false, false, false, false], "sample_969": [false, false, false, false, false], "sample_970": [false, false, true, false, false], "sample_971": [true, true, false, true, false], "sample_972": [false, false, false, false, false], "sample_973": [false, false, false, false, false], "sample_974": [true, false, false, false, false], "sample_975": [false, false, false, false, false], "sample_976": [false, false, false, false, false], "sample_977": [true, false, false, false, false], "sample_978": [true, false, false, false, true], "sample_979": [false, true, false, false, false], "sample_980": [false, false, false, false, false], "sample_981": [true, false, false, false, false], "sample_982": [false, false, true, true, false], "sample_983": [true, false, true, false, false], "sample_984": [false, true, false, false, false], "sample_985": [false, false, false, false, false], "sample_986": [false, false, false, false, false], "sample_987": [false, true, true, false, false], "sample_988": [false, false, true, false, false], "sample_989": [false, false, false, false, false], "sample_990": [false, false, false, false, false], "sample_991": [true, false, true, true, false], "sample_992": [false, false, false, false, false], "sample_993": [false, false, false, false, false], "sample_994": [false, false, false, false, false], "sample_995": [false, false, false, false, false], "sample_996": [false, true, false, false, false], "sample_997": [false, false, false, false, false], "sample_998": [false, false, false, false, false], "sample_999": [true, false, false, false, true], "sample_1000": [true, false, true, false, false], "sample_1001": [false, false, false, false, false], "sample_1002": [false, true, false, false, false], "sample_1003": [true, true, true, false, false], "sample_1004": [true, true, false, false, false], "sample_1005": [false, false, false, false, false], "sample_1006": [false, false, false, false, false], "sample_1007": [false, false, false, false, false], "sample_1008": [true, false, false, false, false], "sample_1009": [false, false, false, false, false], "sample_1010": [false, false, false, true, false], "sample_1011": [true, false, false, false, false], "sample_1012": [false, false, false, false, false], "sample_1013": [true, true, true, true, true], "sample_1014": [true, false, false, false, false], "sample_1015": [false, true, false, false, false], "sample_1016": [false, false, false, false, false], "sample_1017": [true, false, false, false, false], "sample_1018": [false, false, false, false, false], "sample_1019": [false, false, false, false, false], "sample_1020": [false, true, false, false, false], "sample_1021": [true, false, false, false, false], "sample_1022": [false, false, false, false, false], "sample_1023": [false, false, false, false, false], "sample_1024": [false, false, false, false, true], "sample_1025": [false, false, false, true, false], "sample_1026": [false, false, false, false, false], "sample_1027": [false, false, false, false, false], "sample_1028": [false, false, false, false, false], "sample_1029": [false, false, false, false, false], "sample_1030": [false, false, false, false, false], "sample_1031": [true, false, false, false, false], "sample_1032": [false, false, false, false, false], "sample_1033": [false, false, false, false, false], "sample_1034": [false, false, false, false, true], "sample_1035": [false, false, false, false, false], "sample_1036": [false, false, false, false, false], "sample_1037": [false, false, false, false, false], "sample_1038": [false, false, false, false, false], "sample_1039": [false, false, false, false, false], "sample_1040": [true, true, true, false, false], "sample_1041": [true, false, true, false, false], "sample_1042": [false, false, false, false, false], "sample_1043": [false, false, true, false, false], "sample_1044": [false, false, false, false, false], "sample_1045": [false, false, false, false, false], "sample_1046": [false, false, false, false, false], "sample_1047": [false, true, false, false, false], "sample_1048": [false, false, false, false, false], "sample_1049": [false, false, false, false, false], "sample_1050": [true, false, false, true, true], "sample_1051": [false, false, false, false, false], "sample_1052": [false, false, false, false, false], "sample_1053": [true, true, false, true, false], "sample_1054": [false, false, false, false, false], "sample_1055": [false, false, false, false, false], "sample_1056": [true, false, false, false, false], "sample_1057": [false, false, false, false, false], "sample_1058": [false, false, false, false, false], "sample_1059": [false, false, false, false, false], "sample_1060": [false, false, false, false, false], "sample_1061": [false, false, true, false, false], "sample_1062": [false, false, false, false, false], "sample_1063": [true, false, true, true, false], "sample_1064": [true, true, true, true, true], "sample_1065": [false, false, false, false, false], "sample_1066": [false, true, true, false, false], "sample_1067": [true, false, false, true, false], "sample_1068": [true, true, false, false, false], "sample_1069": [false, false, false, false, false], "sample_1070": [false, false, false, true, false], "sample_1071": [true, false, false, false, false], "sample_1072": [false, false, false, false, false], "sample_1073": [false, false, false, false, false], "sample_1074": [true, false, false, false, false], "sample_1075": [true, true, true, false, false], "sample_1076": [true, false, false, false, false], "sample_1077": [false, false, false, false, false], "sample_1078": [false, false, false, false, false], "sample_1079": [false, false, false, false, false], "sample_1080": [false, false, false, false, false], "sample_1081": [false, false, false, false, false], "sample_1082": [true, false, true, false, false], "sample_1083": [false, false, false, true, false], "sample_1084": [false, true, false, false, false], "sample_1085": [false, true, false, true, false], "sample_1086": [false, false, false, false, false], "sample_1087": [false, false, false, false, false], "sample_1088": [false, true, false, false, false], "sample_1089": [false, false, false, false, false], "sample_1090": [false, false, false, false, false], "sample_1091": [true, false, true, false, true], "sample_1092": [false, false, true, true, false], "sample_1093": [true, false, false, false, false], "sample_1094": [false, false, false, false, false], "sample_1095": [false, false, false, false, false], "sample_1096": [false, false, false, false, false], "sample_1097": [false, true, false, true, false], "sample_1098": [false, false, false, false, false], "sample_1099": [false, false, false, false, false], "sample_1100": [false, false, false, false, false], "sample_1101": [false, false, false, false, false], "sample_1102": [false, false, false, false, false], "sample_1103": [false, false, false, false, false], "sample_1104": [false, true, false, false, true], "sample_1105": [true, true, false, false, false], "sample_1106": [false, false, false, false, false], "sample_1107": [false, false, false, false, false], "sample_1108": [false, false, false, false, false], "sample_1109": [false, false, false, false, false], "sample_1110": [false, false, false, false, false], "sample_1111": [false, false, false, false, false], "sample_1112": [false, false, false, false, true], "sample_1113": [false, true, false, false, false], "sample_1114": [false, false, true, false, false], "sample_1115": [true, false, false, false, false], "sample_1116": [false, false, false, false, false], "sample_1117": [false, false, true, false, false], "sample_1118": [false, false, false, false, true], "sample_1119": [false, false, false, false, true], "sample_1120": [false, true, false, false, false], "sample_1121": [false, true, true, false, false], "sample_1122": [false, false, false, false, false], "sample_1123": [false, false, false, true, true], "sample_1124": [false, false, false, false, false], "sample_1125": [false, false, false, false, false], "sample_1126": [false, true, true, false, false], "sample_1127": [false, false, false, false, false], "sample_1128": [true, false, false, false, false], "sample_1129": [false, false, false, true, false], "sample_1130": [true, false, false, false, false], "sample_1131": [false, false, false, false, false], "sample_1132": [false, false, false, false, false], "sample_1133": [false, false, false, false, false], "sample_1134": [false, false, false, false, false], "sample_1135": [false, false, false, false, false], "sample_1136": [false, false, false, false, false], "sample_1137": [true, false, false, false, false], "sample_1138": [false, true, true, false, false], "sample_1139": [true, false, false, false, false], "sample_1140": [false, false, false, false, false], "sample_1141": [false, false, false, false, false], "sample_1142": [true, false, true, false, false], "sample_1143": [true, false, false, false, true], "sample_1144": [true, true, true, false, false], "sample_1145": [true, false, true, true, false], "sample_1146": [false, false, false, false, false], "sample_1147": [false, false, false, false, false], "sample_1148": [false, false, false, false, false], "sample_1149": [false, false, false, false, true], "sample_1150": [false, false, false, false, false], "sample_1151": [false, false, true, false, false], "sample_1152": [false, false, false, false, false], "sample_1153": [false, false, false, false, false], "sample_1154": [true, false, false, false, true], "sample_1155": [false, false, true, true, false], "sample_1156": [true, false, true, false, false], "sample_1157": [false, false, false, false, false], "sample_1158": [true, false, false, false, false], "sample_1159": [false, false, false, false, false], "sample_1160": [false, false, false, false, false], "sample_1161": [true, true, false, false, false], "sample_1162": [false, false, false, false, false], "sample_1163": [false, false, false, false, false], "sample_1164": [false, false, false, false, false], "sample_1165": [false, false, false, false, false], "sample_1166": [false, false, false, false, false], "sample_1167": [false, false, false, false, false], "sample_1168": [false, false, false, false, false], "sample_1169": [true, false, false, false, false], "sample_1170": [true, false, false, false, false], "sample_1171": [false, false, false, false, false], "sample_1172": [false, false, false, false, false], "sample_1173": [false, false, false, false, false], "sample_1174": [false, false, false, false, false], "sample_1175": [false, false, false, false, false], "sample_1176": [false, false, true, false, false], "sample_1177": [false, false, true, false, false], "sample_1178": [false, false, false, false, false], "sample_1179": [true, false, false, false, true], "sample_1180": [true, false, false, true, true], "sample_1181": [true, true, true, true, true], "sample_1182": [false, false, false, false, false], "sample_1183": [false, false, false, false, false], "sample_1184": [false, false, false, false, false], "sample_1185": [false, false, false, false, true], "sample_1186": [false, true, true, false, false], "sample_1187": [true, true, true, false, false], "sample_1188": [true, false, false, false, false], "sample_1189": [true, true, true, true, true], "sample_1190": [false, false, false, false, false], "sample_1191": [false, false, false, false, false], "sample_1192": [false, false, false, false, false], "sample_1193": [false, false, false, true, false], "sample_1194": [true, false, true, false, false], "sample_1195": [false, false, false, false, false], "sample_1196": [false, false, true, false, false], "sample_1197": [false, false, false, false, false], "sample_1198": [false, false, false, false, false], "sample_1199": [false, false, false, false, false], "sample_1200": [false, false, false, false, false], "sample_1201": [false, false, false, false, false], "sample_1202": [false, true, false, false, true], "sample_1203": [false, false, false, false, false], "sample_1204": [false, false, false, false, false], "sample_1205": [true, false, false, false, false], "sample_1206": [false, false, false, false, false], "sample_1207": [false, false, false, false, false], "sample_1208": [true, true, true, true, false], "sample_1209": [false, false, false, false, false]}}