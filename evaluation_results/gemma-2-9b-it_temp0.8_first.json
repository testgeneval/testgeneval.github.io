{"raw_generations": {"sample_427": ["    def test_extra_forms_empty(self):\n        formset = ChoiceFormSet(\n            data={\"TOTAL_FORMS\": \"2\", \"INITIAL_FORMS\": \"1\"},\n            extra=3,\n        )\n        self.assertEqual(len(formset.forms), 3)\n        for i in range(1, 3):\n            self.assertTrue(formset.forms[i].empty_permitted)\n        self.assertEqual(formset.initial_forms, [formset.forms[0]])\n        self.assertEqual(formset.extra_forms, formset.forms[1:])\n  ", "    def test_formset_factory_max_num(self):\n        with self.assertRaises(ValueError):\n            formset_factory(\n                Choice, max_num=100, absolute_max=50\n            )\n", "    def test_formset_factory_with_renderer(self):\n        renderer = mock.Mock(spec=TemplatesSetting)\n        FooFormSet = formset_factory(\n            CustomKwargForm, renderer=renderer, custom_kwarg=\"bar\"\n        )\n        formset = FooFormSet()\n        self.assertEqual(formset.renderer, renderer)\n        self.assertEqual(formset.initial[\"custom_kwarg\"], \"bar\")\n", "    def test_all_valid(self):\n        self.assertTrue(all_valid([]))\n        self.assertTrue(all_valid([ChoiceFormSet(initial=[(\"a\", 1), (\"b\", 2)])]))\n        self.assertFalse(all_valid([ChoiceFormSet(initial=[(\"a\", 1), (\"b\", 2)]), ChoiceFormSet(errors=[ValidationError(\"Invalid form\")])]))\n", "    def test_formset_factory_with_renderer(self):\n        with mock.patch(\"django.forms.renderers.get_default_renderer\") as mock_renderer:\n            mock_renderer.return_value = TemplatesSetting(\n                {\n                    \"formset_template_name\": \"foo\",\n                },\n                \"template_name\",\n                \"formset_template_name\",\n            )\n            mock_formset = formset_factory(\n                Choice, renderer=mock_renderer.return_value\n            )\n\n            self.assertEqual(mock_formset.template_name, \"foo\")\n\n\n"], "sample_900": ["    def test_partial_fit_incremental_predictions(self):\n        X, y = make_regression(n_samples=100, n_features=10, random_state=42)\n        mlp = MLPRegressor(solver='adam', random_state=42)\n        \n        batch_size = 10\n        for i in range(0, len(X), batch_size):\n            mlp.partial_fit(X[i:i + batch_size], y[i:i + batch_size],\n                            classes=np.unique(y))\n        \n        y_pred = mlp.predict(X)\n        assert_almost_equal(y_pred, mlp.predict(mlp.predict(X)), rtol=1e-3)\n", "    def test_partial_fit_multiclass(self):\n        X, y = make_multilabel_classification(n_samples=100,\n                                             n_features=10,\n                                             n_classes=5,\n                                             random_state=42)\n        clf = MLPClassifier(random_state=42, max_iter=1)\n        for i in range(5):\n            with ignore_warnings(category=ConvergenceWarning):\n                clf.partial_fit(X, y, classes=np.unique(y))\n        y_pred = clf.predict(X)\n        assert_array_equal(y_pred.shape, y.shape)\n\n", "    def test_partial_fit_multiclass(self):\n        X, y = make_multilabel_classification(n_samples=100,\n                                             n_features=20,\n                                             n_classes=5,\n                                             random_state=42)\n\n        clf = MLPClassifier(\n            solver='lbfgs',\n            random_state=42,\n            n_iter_no_change=10,\n            warm_start=True\n        )\n        clf.fit(X, y)\n        # Partial fit with the same dataset but shuffled\n        np.random.shuffle(X)\n        np.random.shuffle(y)\n        clf.partial_fit(X, y)\n        assert_array_equal(clf.coefs_, clf._coefs)\n        assert_array_equal(clf.intercepts_, clf._intercepts)\n\n        # Partial fit with a new dataset\n        X_new, y_new = make_multilabel_classification(n_samples=50,\n                                                     n_features=20,\n                                                     n_classes=5,\n                                                     random_state=42)\n        clf.partial_fit(X_new, y_new)\n        assert_array_equal(clf.coefs_, clf._coefs)  # Should update coefs \n        assert_array_equal(clf.intercepts_, clf._intercepts)  # Should update coefs \n\n\n\n", "    def test_partial_fit_incrementally(self):\n        with ignore_warnings(category=ConvergenceWarning):\n            mlp = MLPClassifier(solver='lbfgs', random_state=0, max_iter=5)\n            for X, y in classification_datasets:\n                # Partial fit increments\n                mlp.partial_fit(X[:50], y[:50])\n                mlp.partial_fit(X[50:100], y[50:100])\n                mlp.partial_fit(X[100:150], y[100:150])\n\n                # Check if predictions on combined data match\n                y_pred = mlp.predict(X)\n                assert_array_equal(y_pred, y)\n", "    def test_label_binarizer(self):\n        label_binarizer = LabelBinarizer()\n        y = np.array([0, 1, 2, 0, 2])\n        y_binarized = label_binarizer.fit_transform(y)\n        assert_array_equal(y_binarized,\n                           [[1, 0, 0],\n                            [0, 1, 0],\n                            [0, 0, 1],\n                            [1, 0, 0],\n                            [0, 0, 1]])\n\n\n"], "sample_708": ["    def test_getstatement_no_source(monkeypatch: Any) -> None:\n        # Test case: source is None, should return Source object with empty lines\n        astnode = ast.parse('pass')  # Any valid AST will do\n        with monkeypatch.patch(\"inspect.getsource\", lambda obj: None):\n            start, end = getstatementrange_ast(1, Source(), astnode=astnode)\n            assert start == 0\n            assert end == 1\n", "    def test_strip(\n        monkeypatch: pytest.MonkeyPatch,", "    def test_getstatementrange_empty(monkeypatch):\n        source = Source()\n        with pytest.raises(IndexError):\n            source.getstatementrange(0)\n", "    def test_getstatementrange_ast_empty_source(capsys):\n        source = Source()\n        astnode, start, end = getstatementrange_ast(0, source)\n        assert astnode is not None\n        assert start == 0\n        assert end == 0\n\n", "    def test_getstatementrange_ast(monkeypatch):\n        source = \"\"\"\n            pass\n\n        class Class1:\n                pass\n        \"\"\"\n        source_obj = Source(source)\n        # test simple function\n        start, end = source_obj.getstatementrange_ast(lineno=1, astnode=None)\n        assert start == 0\n        assert end == 3\n        # test simple class\n        start, end = source_obj.getstatementrange_ast(lineno=4, astnode=None)\n        assert start == 4\n        assert end == 7\n"], "sample_1039": ["    def test_printing_golden_ratio():\n        assert mpp._print(GoldenRatio).toprettyxml() == \\\n               '<mi>\u03d5</mi>'\n        assert mp._print(GoldenRatio).toprettyxml() == \\\n               '<mrow><mi>\u03d5</mi></mrow>'\n\n\n", "    def test_matrix_elements():\n        from sympy.matrices import Matrix\n\n        matrix = Matrix([[1, 2], [3, 4]])\n        xml = mpp.doprint(matrix)\n        assert 'mfenced' in xml\n\n        xml = mp.doprint(matrix)\n        assert 'mtable' in xml\n\n", "    def test_print_special_functions(self):\n        for func in [sin, cos, tan, asin, acos, atan, sinh, cosh, tanh, asinh, acosh, atanh]:\n            expr = func(x)\n            s = MathMLContentPrinter()\n            s.doprint(expr)\n            s = MathMLPresentationPrinter()\n            s.doprint(expr)\n\n        expr = Integer(1)\n        s = MathMLContentPrinter()\n        s.doprint(expr)\n        s = MathMLPresentationPrinter()\n        s.doprint(expr)\n\n        expr = Rational(1, 2)\n        s = MathMLContentPrinter()\n        s.doprint(expr)\n        s = MathMLPresentationPrinter()\n        s.doprint(expr)\n\n        expr = Float(1.23)\n        s = MathMLContentPrinter()\n        s.doprint(expr)\n        s = MathMLPresentationPrinter()\n        s.doprint(expr)\n\n        expr = E\n        s = MathMLContentPrinter()\n        s.doprint(expr)\n        s = MathMLPresentationPrinter()\n        s.doprint(expr)\n\n        expr = I\n        s = MathMLContentPrinter()\n        s.doprint(expr)\n        s = MathMLPresentationPrinter()\n        s.doprint(expr)\n\n        expr = oo\n        s = MathMLContentPrinter()\n        s.doprint(expr)\n        s = MathMLPresentationPrinter()\n        s.doprint(expr)\n\n        expr = pi\n        s = MathMLContentPrinter()\n        s.doprint(expr)\n        s = MathMLPresentationPrinter()\n        s.doprint(expr)\n\n        expr = GoldenRatio\n        s = MathMLContentPrinter()\n        s.doprint(expr)\n        s = MathMLPresentationPrinter()\n        s.doprint(expr)\n\n        expr = EulerGamma\n        s = MathMLContentPrinter()\n        s.doprint(expr)\n        s = MathMLPresentationPrinter()\n        s.doprint(expr)\n\n\n\n", "    def test_matrix_symbols():\n        mat = MatrixSymbol('A', 2, 2)\n        print(mp._print(mat))  # Should just print 'A'\n        print(mpp._print(mat))  # Should print a MatrixSymbol in Presentation MathML\n\n        mat = MatrixSymbol('B', 3, 1)\n        print(mp._print(mat))  # Should just print 'B'\n        print(mpp._print(mat))  # Should print a MatrixSymbol in Presentation MathML\n\n        mat = MatrixSymbol('C'[11], 4, 4)\n        print(mp._print(mat))  # Should just print 'C'[11]\n        print(mpp._print(mat))  # Should print a MatrixSymbol in Presentation MathML\n", "    def test_print_matrix_symbol(self):\n        m = MatrixSymbol('A', 2, 2)\n        s = MathMLPresentationPrinter()\n        s._settings['mat_symbol_style'] = 'bold'\n        assert '<b>' in s._print(m)\n        assert '</b>' in s._print(m)\n\n"], "sample_754": ["    def test_sparse_pca_with_sparse_components(norm_comp):\n        n_components = 3\n        n_samples = 100\n        image_size = (10, 10)\n        random_state = 42\n\n        X, U, V = generate_toy_data(n_components, n_samples, image_size, random_state)\n        sparse_pca = SparsePCA(n_components=n_components,\n                               alpha=1.0,\n                               normalize_components=norm_comp)\n        sparse_pca.fit(X)\n\n        # Check components are sparse\n        assert_true(np.sum(sparse_pca.components_.abs().sum(axis=0)) > 0)\n\n        # Check reconstruction error\n        X_recon = sparse_pca.transform(X)\n        assert_allclose(X, X_recon, rtol=1e-3)\n\n", "    def test_sparse_pca_with_different_n_components(norm_comp):\n        n_components = 3\n        n_samples = 100\n        image_size = (8, 8)\n        random_state = 42\n        X, U, V = generate_toy_data(n_components, n_samples, image_size, random_state=random_state)\n\n        sp_pca = SparsePCA(n_components=n_components, normalize_components=norm_comp)\n        sp_pca.fit(X)\n\n        # check the components against the original ones\n        assert_allclose(sp_pca.components_, V.T, rtol=1e-2)\n\n        sp_pca = SparsePCA(n_components=n_components // 2, normalize_components=norm_comp)\n        sp_pca.fit(X)\n        assert_equal(sp_pca.n_components, n_components // 2)\n        assert_allclose(sp_pca.components_.shape, (sp_pca.n_components, X.shape[1]))\n\n\n        sp_pca = SparsePCA(n_components=n_components * 2, normalize_components=norm_comp)\n        sp_pca.fit(X)\n        assert_equal(sp_pca.n_components, n_components * 2)\n        assert_allclose(sp_pca.components_.shape, (sp_pca.n_components, X.shape[1]))\n", "    def test_sparse_pca_with_batch_size_and_shuffle(self, norm_comp):\n        X, U, V = generate_toy_data(n_components=3, n_samples=100, image_size=(10, 10), random_state=0)\n\n        spca = MiniBatchSparsePCA(n_components=3, batch_size=10, shuffle=True, normalize_components=norm_comp)\n        spca.fit(X)\n\n        assert_allclose(spca.components_, V, rtol=1e-3)\n\n\n", "    def test_sparse_pca_transform_ridge_alpha(norm_comp):\n        n_components = 4\n        n_samples = 50\n        n_features = 20\n        random_state = 0\n\n        X, _, _ = generate_toy_data(\n            n_components=n_components, n_samples=n_samples,\n            image_size=(5, 5), random_state=random_state)\n\n        sparse_pca = SparsePCA(n_components=n_components,\n                               normalize_components=norm_comp,\n                               random_state=random_state)\n        sparse_pca.fit(X)\n\n        X_transformed = sparse_pca.transform(X, ridge_alpha=0.1)\n        assert_array_almost_equal(X_transformed.shape, (n_samples, n_components))\n", "    def test_sparse_pca_mini_batch(norm_comp):\n        n_components = 5\n        n_samples = 100\n        image_size = (10, 10)\n        random_state = 42\n\n        X, U_true, V_true = generate_toy_data(\n            n_components, n_samples, image_size, random_state=random_state)\n\n        sparse_pca = MiniBatchSparsePCA(\n            n_components=n_components,\n            alpha=0.1,\n            ridge_alpha=0.01,\n            n_iter=10,\n            batch_size=10,\n            verbose=False,\n            shuffle=True,\n            normalize_components=norm_comp\n        )\n\n        sparse_pca.fit(X)\n\n        components = sparse_pca.components_\n\n        assert_allclose(components, V_true, rtol=1e-3)\n"], "sample_168": ["    def test_stale_content_type_deletion(self):\n        with mock.patch('django.contrib.contenttypes.management.remove_stale_content_types.ContentType.objects.create') as create_mock:\n            create_mock.return_value = None\n            self.content_type.model_class = None\n            call_command('remove_stale_content_types', interactive='no')\n            self.assertEqual(ContentType.objects.count(), self.before_count - 1)\n", "compilation error", "    def test_deleting_stale_content_types_interactive(self):\n        with captured_stdout() as out:\n            call_command(\n                'remove_stale_content_types', interactive=True, verbosity=2,\n            )\n        self.assertEqual(ContentType.objects.count(), self.before_count - 1)\n        self.assertIn(\n            'Deleting stale content type \"contenttypes_tests | Fake\"', out.getvalue(),\n        )\n        self.assertIn(\n            \"Some content types in your database are stale and can be deleted...\", out.getvalue(),\n        )\n        self.assertIn(\"Type 'yes' to continue, or 'no' to cancel:\", out.getvalue())\n", "    def test_remove_stale_content_types(self):\n        # Set up a 'stale' content type\n        self.content_type.model_class = None\n        self.content_type.save()\n\n        # Call the command with the appropriate options\n        with captured_stdout() as stdout:\n            call_command('remove_stale_content_types', verbosity=0, database='default', include_stale_apps=True)\n\n        # Assert that the stale content type was deleted and dependent objects are not affected\n        self.assertEqual(ContentType.objects.count(), self.before_count - 1)\n", "    def test_no_stale_content_types(self):\n        call_command('removestaletypes', verbosity=0, interactive=False, database='default')\n        self.assertEqual(ContentType.objects.count(), self.before_count + 1)\n\n        # Verify no stale types were removed.\n        with captured_stdout() as out:\n            call_command('removestaletypes', verbosity=0, interactive=False, database='default', include_stale_apps=False)\n        self.assertNotIn(\"Stale content types remain.\", out.getvalue())\n"], "sample_92": ["    def test_get_all_permissions(self):\n        backend = SimpleBackend()\n        all_perms = backend.get_all_permissions(self.user)\n        self.assertEqual(all_perms, {'user_perm', 'group_perm'})\n", "    def test_user_permissions(self):\n        backend = SimpleBackend()\n        user = self.user\n        permissions = backend.get_all_permissions(user)\n        self.assertEqual(permissions, {'user_perm', 'group_perm'})\n\n", "    def test_get_all_permissions(self):\n        backend = SimpleBackend()\n        permissions = backend.get_all_permissions(self.user)\n        self.assertEqual(permissions, {'user_perm', 'group_perm', 'can_test'})\n\n\n\n", "    def test_has_perm_with_user_permissions(self):\n        backend = SimpleBackend()\n        self.assertTrue(backend.has_perm(self.user, 'user_perm'))\n        self.assertTrue(backend.has_perm(self.user, 'group_perm'))\n        self.assertFalse(backend.has_perm(self.user, 'nonexistent_perm'))\n", "    def test_has_perm(self):\n        backend = SimpleBackend()\n        self.assertTrue(backend.has_perm(self.user, 'user_perm'))\n        self.assertTrue(backend.has_perm(self.user, 'group_perm'))\n        self.assertFalse(backend.has_perm(self.user, 'unknown_perm'))\n"], "sample_580": ["compilation error", "compilation error", "compilation error", "    def test_categorical_order_numeric():\n        vector = pd.Series([1, 2, 3, 1, 2])\n        assert categorical_order(vector) == [1, 2, 3]\n\n", "compilation error"], "sample_660": ["    def test_simple_report(self, testdir):\n        result, node = runandparse(testdir, \"test_example.py\")\n        assert result.ret == 0\n        assert_attr(node, classname=\"test_example\", name=\"test_something\", passed=\"True\")\n", "    def test_junitxml_double_fail_report(self, testdir):\n        result, dom = runandparse(\n            testdir, \"-vs\", \"--junitxml=junit.xml\", \"test_double_fail.py\"\n        )\n        assert result.ret == 0\n        suite = dom.find_first_by_tag(\"testsuite\")\n        assert_attr(suite, name=\"pytest\", errors=1, failures=1)\n        test = dom.find_first_by_tag(\"testcase\")\n        assert test.find_first_by_tag(\"name\").text == \"test_double_fail\"\n        assert test.find_first_by_tag(\"failure\")\n        assert test.find_first_by_tag(\"error\")\n", "    def test_junitxml_logging(self, testdir):\n        result, xml = runandparse(testdir, \"-v\", \"--junitxml=-\", \"--junit-logging=system-err\")\n        assert result.ret == 0, result.stderr\n        test_results = xml.find_first_by_tag(\"testcase\")\n        assert test_results.tag == \"testcase\"\n        assert \"system-err\" in test_results.children[0].toxml()\n", "    def test_prefix(self, testdir, prefix, expected_output):\n        result = runandparse(\n            testdir,\n            \"--junitxml=-\",\n            \"--junitprefix=%s\" % prefix,\n        )\n        xml = result.root\n        assert xml.tag == expected_output\n", "    def test_double_failure_is_not_counted(self, testdir):\n        result, xml = runandparse(testdir, \"-l\", \"test_double_failures.py\")\n        assert result.ret == 0\n        assert xml.find_first_by_tag(\"testsuite\").getAttribute(\"tests\") == 1\n        assert xml.find_first_by_tag(\"testsuite\").getAttribute(\"failures\") == 1\n        assert xml.find_first_by_tag(\"testsuite\").getAttribute(\"errors\") == 0\n\n\n\n"], "sample_452": ["    def test_alter_together_unique_together_remove(self):\n        with transaction.atomic():\n            self.create_table('my_app.food')\n            Food.objects.create(name='Apple')\n            self.assertEqual(Food.objects.count(), 1)\n\n        state = ProjectState(apps=[])\n        with CaptureQueriesContext(connection) as queries:\n            operations = [\n                AlterTogether(\n                    'my_app.Food',\n                    unique_together=(),\n                ),\n            ]\n            self.apply_operations(state, operations)\n\n        with atomic():\n            Food.objects.create(name='Banana')\n            Food.objects.create(name='Cherry')\n            self.assertEqual(Food.objects.count(), 3)\n\n        # Test backwards\n        with CaptureQueriesContext(connection) as queries:\n            self.apply_operations(state, operations[::-1])\n\n        with atomic():\n            Food.objects.create(name='Apple')\n            self.assertEqual(Food.objects.count(), 4)\n\n\n", "    def test_alter_rename_field(self):\n        with atomic(using=self.connection.alias):\n            self.create_and_run_migrations(\n                [\n                    # Create an app and model with a simple field\n                    migrations.CreateModel(\n                        name=\"UnicodeModel\",\n                        fields=[\n                            migrations.CharField(\n                                max_length=100, name=\"name\"\n                            ),\n                        ],\n                        options={\"db_table\": \"unicode_model\"},\n                    ),\n                    migrations.RenameField(\n                        model_name=\"unicode_model\",\n                        old_name=\"name\",\n                        new_name=\"new_name\",\n                    ),\n                ]\n            )\n\n        # Verify the name is renamed in the database\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT * FROM unicode_model\")\n            result = cursor.fetchone()\n            self.assertIs(result[0], \"unicode_model_name\")\n", "    def test_add_index_rename_index(self):\n        with atomic():\n            self.create_model(\n                \"app\",\n                \"Model\",\n                [],\n                {\"id\": models.IntegerField(primary_key=True)},\n            )\n            # Add an index\n            with self.assertNumQueries(1):\n                self.manager.migrate(\"add_index\")\n            # Rename the index\n            with self.assertNumQueries(1):\n                self.manager.migrate(\"rename_index\")\n\n\n\n", "    def test_rename_index_postgresql(self):\n        with self.assertNumQueries(0):\n            self.create_model(\"rename_index_test\")\n\n        # Rename an existing index with the same name\n\n        with transaction.atomic():\n            with CaptureQueriesContext(connection) as queries:\n                RenameIndex(\n                    \"rename_index_test\", new_name=\"my_new_index\", old_name=\"my_index\"\n                ).database_forwards(\n                    self.app_label,\n                    self.state_forwards,\n                    self.from_state,\n                    self.to_state,\n                )\n        self.assertEqual(len(queries.captured_queries), 0)\n\n", "    def test_rename_index(self):\n        with atomic(using=connection.alias):\n            # Create a model with an index\n            with self.assertRaises(FieldDoesNotExist):\n                self.model.objects.create(name=\"test\")\n            with self.assertRaises(FieldDoesNotExist):\n                self.model.objects.create(name=\"test\")\n\n            # Create the index\n            ModelOperation(\n                name='RenameModel', operation=AddIndex(\n                    \"blog.Post\",\n                    models.Index(fields=[\"name\", \"pub_date\"], name='name_date_idx'),\n                )\n            ).state_forwards(self.state, \"blog\")\n\n            # Update model state\n            with self.assertRaises(FieldDoesNotExist):\n                self.model.objects.create(name=\"test\")\n            with self.assertRaises(FieldDoesNotExist):\n                self.model.objects.create(name=\"test\")\n"], "sample_646": ["def test_skip_marker(pytester: Pytester) -> None:\n    \"\"\"Verify pytest handles `@unittest.skip` markings.\"\"\"\n    # Run test with skip marker\n    result = pytester.runpytest_subprocess(\n        '-s', 'tests/test_unittest.py',\n    )\n\n    # Check for the expected output message\n    stdout, _ = result\n\n    assert \"test_skip_marker - skipped\" in stdout\n\n\n", "compilation error", "    def test_skipped_test(pytester: Pytester, monkeypatch: MonkeyPatch) -> None:\n        monkeypatch.setattr(\n            sys, \"modules\", {\"unittest\": object}\n        )  # Mock unittest module\n\n            self.skip(\"This test is skipped\")\n\n        with pytest.raises(pytest.skip.Exception, match=\"This test is skipped\"):\n            pytester.runpytest(\n                \"-v\",\n                \"--traceback\",\n                \"-k\",\n                \"skipped_test\",\n                \"-o\",\n                f\"collectonly=False\",\n            )\n", "compilation error", "def test_unittest_skip(pytester: Pytester):\n    \"\"\"\n    Test that unittest.skip is translated to pytest.skip.\n    \"\"\"\n\n    pytester.write(\n        \"pytest_unittest_plugin.py\",\n        \"\"\"\n        import unittest\n\n        class TestSkip(unittest.TestCase):\n            @unittest.skip(\"This test is skipped\")\n                pass\n        \"\"\",\n    )\n    result = pytester.runpytest(\"-rs\")\n    assert result.retcode == ExitCode.OK\n    assert (\n        \"This test is skipped\\n\" in result.str()\n    )\n    result, _ = pytester.runpytest(\"--nocollect-in\", \"this_file_is_not_collected\")  \n    assert result.retcode == ExitCode.OK\n"], "sample_343": ["    def test_generic_relation_with_m2m(self):\n        post = Post.objects.create(title='My Post')\n        answer1 = Answer.objects.create(text='Answer 1', post=post)\n        answer2 = Answer.objects.create(text='Answer 2', post=post)\n\n        self.assertEqual(Answer.objects.filter(post=post).count(), 2)\n        self.assertEqual(post.answers.count(), 2)\n        self.assertEqual(set([answer.pk for answer in post.answers]),\n                         set([answer1.pk, answer2.pk]))\n\n        post2 = Post.objects.create(title='Another Post')\n        answer3 = Answer.objects.create(text='Answer 3', post=post2)\n\n        self.assertEqual(Answer.objects.filter(post=post2).count(), 1)\n        self.assertEqual(post2.answers.count(), 1)\n        self.assertEqual(set([answer.pk for answer in post2.answers]),\n                         set([answer3.pk]))\n", "    def test_reverse_generic_many_to_one_with_multiple_objects(self):\n        post = Post.objects.create(title=\"My Post\")\n        answer1 = Answer.objects.create(text=\"Answer 1\", post=post)\n        answer2 = Answer.objects.create(text=\"Answer 2\", post=post)\n        self.assertEqual(post.answers.count(), 2)\n\n        # Test that the reverse relation is properly populated.\n        self.assertEqual(answer1.post.pk, post.pk)\n        self.assertEqual(answer2.post.pk, post.pk)\n\n\n", "    def test_generic_foreign_key_get_cached_value(self):\n        post = Post.objects.create(title='First Post')\n        question = Question.objects.create(content='First Question')\n        answer = Answer.objects.create(question=question, content='First Answer')\n\n        content_type = ContentType.objects.get_for_model(Question)\n        answer_fk = answer.id\n\n        self.assertEqual(Answer.objects.get(pk=answer.pk).content_type, content_type)\n        self.assertEqual(Answer.objects.get(pk=answer.pk).object_id, answer_fk)\n\n\n", "    def test_generic_foreign_key_related_objects(self):\n        Post.objects.create(title='Example Post')\n        answer1 = Answer.objects.create(\n            question=Question.objects.create(title='First Question'),\n            content='Answer for the first question'\n        )\n        answer2 = Answer.objects.create(\n            question=Question.objects.create(title='Second Question'),\n            content='Answer for the second question'\n        )\n\n        post = Post.objects.first()\n        self.assertEqual(post.answers.count(), 0)\n        post.answers.add(answer1, answer2)\n        self.assertEqual(post.answers.count(), 2)\n        self.assertEqual(\n            [a.question.title for a in post.answers.all()],\n            ['First Question', 'Second Question']\n        )\n        post.answers.remove(answer1)\n        self.assertEqual(post.answers.count(), 1)\n        self.assertEqual(\n            [a.question.title for a in post.answers.all()],\n            ['Second Question']\n        )\n", "    def test_generic_foreign_key_related_field(self):\n        question = Question.objects.create(title='Test Question')\n        answer = Answer(question=question, body='Test Answer')\n        answer.save()\n\n        # Check if the related object is accessible through the GenericForeignKey.\n        related_object = question.answers.get()  \n        self.assertEqual(related_object.body, 'Test Answer')\n"], "sample_142": ["    def test_system_checks_passes_correctly_configured_admin(self):\n        # Test that system checks pass when models are correctly configured with fields.\n        with self.assertNumQueries(0):\n            self.assertEqual(checks.run_checks(), [])  \n", "    def test_system_checks(self):\n        site = AdminSite()\n        site.register(Song, MyAdmin)\n\n        errors = checks.run_checks(site)\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'fields.W342')\n        self.assertEqual(errors[0].msg, 'The field \"album\" on model \"Song\" has a ForeignKey constraint with unique=True, which means it is a one-to-one relationship. Consider removing unique=True and using a single field for the relationship.')\n\n\n\n", "    def test_modelformset_factory_can_handle_inline_foreign_key(self):\n        from django.forms.models import modelformset_factory\n\n        class Parent(models.Model):\n            name = models.CharField(max_length=100)\n\n        class Child(models.Model):\n            parent = models.ForeignKey(Parent, on_delete=models.CASCADE)\n            content = models.TextField()\n\n        Form = modelformset_factory(Child, form=forms.ModelForm,\n                                    fk_name='parent', extra=2)\n        formset = Form(data={'parent': [1, 2], 'content': ['a', 'b']})\n        self.assertTrue(formset.is_valid())\n\n\n", "    def test_system_checks_with_inline_formset(self):\n        from admin_checks.models import AlbumInline, Album\n        class AlbumInlineFormSet(admin.TabularInline):\n            model = Album\n            formset = BaseInlineFormSet\n\n        class AlbumAdmin(admin.ModelAdmin):\n            inlines = [AlbumInlineFormSet]\n\n        admin_site = AdminSite()\n        admin_site.register(Song, ValidFields)\n        admin_site.register(Song, MyAdmin)\n        admin_site.register(ValidFormFieldsets)\n        admin_site.register(AlbumAdmin)\n\n        with self.assertNumWarnings(0):\n            checks = checks.run_checks(\n                admin_site,\n                using=self.settings.DATABASES['default'],\n            )\n\n        self.assertEqual(len(checks), 1)\n", "    def test_system_checks_admin(self):\n        site = AdminSite()\n        site.register(Song, MyAdmin)\n        checks = site.check()\n        errors = [check for check in checks\n                  if check.tag == 'admin']\n        self.assertEqual(errors, ['error!'])\n"], "sample_30": ["    def test_complex_table_structure(self):\n        # Test a complex table structure with nesting, to ensure\n        # recursion and element references work properly.\n        votable_text = \"\"\"<?xml version='1.0' encoding='utf-8'?>\n        <!-- Produced with astropy.io.votable version 0.7.0 -->\n        <VOTABLE version=\"1.3\">\n          <RESOURCE>\n            <INFO name=\"description\">Example table with nested structure</INFO>\n            <TABLE ID=\"myTable\">\n              <FIELD name=\"id\" utype=\"INTEGER\">\n                <INFO name=\"description\">Unique identifier</INFO>\n              </FIELD>\n              <FIELD name=\"name\" utype=\"VARCHAR\">\n                <INFO name=\"description\">Table name</INFO>\n              </FIELD>\n              <DATA>\n                <tr><td>1</td><td>Table 1</td></tr>\n                <tr><td>2</td><td>Table 2</td></tr>\n              </DATA>\n              <GROUP ID=\"group1\">\n                <GROUP ID=\"group2\">\n                  <FIELD name=\"value\" utype=\"FLOAT\">\n                    <INFO name=\"description\">Group value</INFO>\n                  </FIELD>\n                </GROUP>\n              </GROUP>\n            </TABLE>\n          </RESOURCE>\n        </VOTABLE>\n        \"\"\"\n\n        votable = parse(io.StringIO(votable_text))\n        \n        # Check if the nested elements are correctly retrieved\n        table = votable.resources[0].tables[0]\n        assert table.fields[0].name == \"id\"\n        assert table.fields[1].name == \"name\"\n        assert table.groups[0].ID == \"group1\"\n        assert table.groups[0].groups[0].ID == \"group2\"\n\n        # Accessing data from nested elements\n        assert table.groups[0].groups[0].fields[0].name == \"value\"\n\n        # Check if the ID references work correctly\n        assert table.groups[0].ID == \"group1\"\n        assert table.groups[0].groups[0].ID == \"group2\"\n\n        # Check if the description from the INFO elements is retrieved\n        assert table.fields[0].attrs[\"description\"] == \"Unique identifier\"\n", "    def test_validate_schema_failure(tmpdir):\n        test_xml = tmpdir.join(\"bad_schema.xml\").strpath\n        with open(test_xml, \"w\") as f:\n            f.write(\"<VOTABLE version='1.3'>\\n\")\n\n        with pytest.raises(VOTableSpecError) as excinfo:\n            validate_schema(test_xml, \"1.3\")\n        assert \"schema validation failed\" in str(excinfo.value)\n\n", "    def test_warn_invalid_version(tmp_path):\n        with (tmp_path / \"test.vot\").open(\"w\") as f:\n            f.write(\n                \"\"\"", "    def test_tabledata_format_override(\n        tmpdir,\n        votable_file,\n        table,", "    def test_get_table_by_index_out_of_bounds(self):\n        votable_file = VOTableFile()\n        with pytest.raises(IndexError):\n            votable_file.get_table_by_index(10)\n"], "sample_44": ["    def test_function_quantity_creation(self, unit_class, unit_instance, value, expected):\n        for unit_cls, unit_inst in zip(unit_class, unit_instance):\n            quantity = unit_cls(value)\n            assert_quantity_allclose(quantity.value, quantity.to(unit_inst).value,\n                                     atol=1e-12)\n\n\n\n", "compilation error", "    def test_unit_creation(self, unit_type, subclass):\n        unit = subclass(unit_type)\n        assert isinstance(unit, unit_type)\n        assert isinstance(unit, u.FunctionUnitBase)\n", "    def test_from_physical(self, unit_type):\n        unit, physical_unit = unit_type\n        if physical_unit == dimensionless_unscaled:\n            return\n        lu = unit\n        physical_lu = lu._default_function_unit\n        # if the unit has a physical equivalent, check if conversion works\n        if physical_lu == physical_unit:\n            assert_allclose(lu.from_physical(physical_unit(1)), 1)\n        else:\n            with pytest.raises(UnitsError):\n                lu.from_physical(physical_unit(1))\n", "    def test_creation_and_equivalencies(self, unit_class, value, expected_value,\n                                      expected_unit, physical_unit):\n        unit = unit_class(physical_unit)\n        lq = unit_class(value, unit=unit)\n        assert_quantity_allclose(lq.value, expected_value)\n        assert lq.unit == expected_unit\n        assert lq.physical_unit == physical_unit\n"], "sample_1103": ["compilation error", "compilation error", "    def test_as_content_primitive_general():\n        from sympy import expand, sqrt, Rational, together\n        test_cases = [\n            (4**(a/2), (4, 4**(a/2))),\n            (sqrt(2) * sqrt(2), (1, 2)),\n            (sqrt(9) * sqrt(2), (3, sqrt(2))),\n            (sqrt(2 + 2*sqrt(2))*sqrt(2 + 2*sqrt(2)), (2, (2 + 2*sqrt(2))**0.5)),\n            ((sqrt(2) + 1)**(1/2), (1, (sqrt(2) + 1)**0.5)),\n            (x**(a/2), (1, x**(a/2))),\n            ((x + y)**a, (1, (x + y)**a)),\n            ((2*x + 2)**a, (2, (x + 1)**a)),\n            (3**(a/2), (1, 3**(a/2))),\n            (sqrt(3 + 3*sqrt(2)), (1, sqrt(3) * sqrt(1 + sqrt(2)))),\n            (sqrt(3 + 3*sqrt(2))*sqrt(3 + 3*sqrt(2)), (3, (1 + sqrt(2))**2)),\n            (sqrt(4 + 4*sqrt(2)), (2, sqrt(1 + sqrt(2)))),\n            ((2*x + 2)**a, (2, (x + 1)**a)),\n        ]\n        for i, (expr, expected) in enumerate(test_cases):\n            r, content = expr.as_content_primitive()\n            if r != expected[0]:\n                print(f\"Test {i}: expected content_primitive = {expected}\")\n                print(f\"  got content_primitive = {r, content}\")\n                assert False\n            if content != expected[1]:\n                print(f\"Test {i}: expected content_primitive = {expected}\")\n                print(f\"  got content_primitive = {r, content}\")\n                assert False\n\n\n\n", "    def test_as_content_primitive():\n        from sympy.polys import poly\n\n        # Test edge cases with 1 and 0\n        assert (1**x).as_content_primitive() == (1, 1**x)\n        assert (0**x).as_content_primitive() == (0, 0**x)\n\n        # Test with radicals\n        assert sqrt(4 + 4*sqrt(2)).as_content_primitive(radical=True) == (2, sqrt(1 + sqrt(2)))\n        assert (sqrt(3 + 3*sqrt(2))).as_content_primitive(radical=True) == (1, sqrt(3)*sqrt(1 + sqrt(2)))\n\n        # Test with non-trivial powers\n        assert ((2*x + 2)**2).as_content_primitive() == (4, (x + 1)**2)\n        assert (4**((1 + y)/2)).as_content_primitive() == (2, 4**(y/2))\n\n        # test (a * b)**c = a**c * b**c\n        a, b, c = symbols('a b c')\n        expr = (a * b) ** c\n        assert expr.as_content_primitive() == (1, (a**c) * (b**c))\n\n        # test with rational exponents and coefficients\n        assert (3**((5 + y)/2)).as_content_primitive() == (9, 3**((y + 1)/2))\n\n        # test with more complex exponents\n        assert ((2 + 2*x)**y).as_content_primitive() == (1, (2*(x + 1))**y)\n\n\n\n        \n", "    def test_as_base_exp():\n        assert Pow(2, 3).as_base_exp() == (2, 3)\n        assert Pow(x, 2).as_base_exp() == (x, 2)\n        assert Pow(x**2, y).as_base_exp() == (x**2, y)\n        assert Pow(x, y*z).as_base_exp() == (x, y*z)\n        assert Pow(2 + x, 3).as_base_exp() == (2 + x, 3)\n        assert Pow(a + b, c).as_base_exp() == (a + b, c)\n"], "sample_601": ["    def test_time_delta_fields(self, field):\n        with raises_regex(\n            AttributeError, r\"'CFTimeIndex' object has no attribute '.*'\"\n        ):\n            self.data.dt.days_in_month\n\n        assert_identical(\n            self.data.dt.__dict__.keys(), DatetimeAccessor.__dict__.keys()\n        )\n", "    def test_isocalendar(self, field):\n        with raises_regex(\n            AttributeError, r\"CFTimeIndex object has no attribute 'isocalendar'\"\n        ):\n            self.data.isel(time=0).dt.isocalendar()\n        result = self.data.dt.isocalendar()\n        assert_identical(result.dims, [\"lon\", \"lat\"])\n        assert_equal(result.coords, self.data.coords)\n\n\n\n", "    def test_datetime_accessor_date_fields(self, field):\n        result = getattr(self.data.dt, field)\n        assert isinstance(result, xr.DataArray)\n        assert_identical(result.sizes, self.data.sizes)\n        assert result.name == f\"{field}\"\n\n\n\n", "    def test_strftime_with_cftime_index(self):\n        with requires_cftime():\n            cftime_data = xr.DataArray(\n                pd.to_datetime(self.times, format=\"%Y-%m-%d\")\n            )\n            result = cftime_data.dt.strftime(\"%Y-%m-%d\")\n            assert_array_equal(result.values, self.times)\n", "    def test_season(self, field, expected_values):\n        result = self.data.dt[field].values\n        assert_array_equal(result, expected_values)\n\n"], "sample_818": ["    def test_affinity_matrix_construction(eigen_solver, assign_labels):\n        n_samples = 10\n        n_features = 2\n        X = np.random.rand(n_samples, n_features)\n        sg = SpectralClustering(n_clusters=2,\n                                n_components=2,\n                                eigen_solver=eigen_solver,\n                                assign_labels=assign_labels)\n\n        # Ensure the affinity matrix is constructed when affinity is not\n        # 'precomputed'\n        sg.fit(X)\n        assert sg.affinity_matrix_ is not None\n\n        # Check when affinity='precomputed'\n        affinity = pairwise_distances(X, metric='euclidean')\n        sg = SpectralClustering(n_clusters=2,\n                                n_components=2,\n                                affinity='precomputed',\n                                assign_labels=assign_labels)\n        sg.fit(affinity)\n        assert sg.affinity_matrix_ is affinity\n", "    def test_affinity_matrix_construction_with_precomputed_affinity(\n        self, eigen_solver, assign_labels", "    def test_spectral_clustering_connectivity(eigen_solver, assign_labels):\n        n_samples = 30\n        n_clusters = 2\n        random_state = check_random_state(0)\n        X, _ = make_blobs(n_samples=n_samples, centers=n_clusters,\n                          cluster_std=0.6, random_state=random_state)\n\n        # Create a graph based on the data\n        graph = img_to_graph(X, n_neighbors=10, mode='knn',\n                             return_coo=True)\n        affinity = sparse.csc_matrix(graph.toarray())\n\n        sc = SpectralClustering(n_clusters=n_clusters,\n                                eigen_solver=eigen_solver,\n                                assign_labels=assign_labels)\n        labels = sc.fit_predict(affinity)\n\n        # Check if the labels are meaningful\n        assert adjusted_rand_score(y_true, labels) >= 0.5\n", "    def test_spectral_clustering_with_custom_affinity(eigen_solver, assign_labels):\n        n_samples = 30\n        n_clusters = 4\n        X, _ = make_blobs(n_samples=n_samples, centers=n_clusters,\n                          cluster_std=0.6, random_state=42)\n\n        # Construct a custom affinity matrix\n        connectivity = img_to_graph(X, mode='knn', n_neighbors=5)\n        affinity_matrix = 0.5 * (connectivity + connectivity.T)\n\n        # Fit spectral clustering with custom affinity\n        sc = SpectralClustering(n_clusters=n_clusters,\n                                eigen_solver=eigen_solver,\n                                affinity='precomputed',\n                                assign_labels=assign_labels)\n        sc.fit(affinity_matrix)\n\n        # Check if the clustering results are consistent\n        expected_labels = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n                                   0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n                                   0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3])\n        assert_array_equal(sc.labels_, expected_labels)\n\n", "    def test_affinity_matrix_from_data(eigen_solver, assign_labels):\n        X = np.random.rand(10, 5)\n        sc = SpectralClustering(n_clusters=3,\n                                eigen_solver=eigen_solver,\n                                assign_labels=assign_labels)\n        sc.fit(X)\n        assert isinstance(sc.affinity_matrix_, (np.ndarray, sparse.csr_matrix))\n\n\n"], "sample_105": ["    def test_simple_post_view(self):\n        view = SimplePostView()\n        request = self.rf.post('/')\n        response = view.dispatch(request)\n        self._assert_simple(response)\n\n", "    def test_redirect_view_with_url(self):\n        redirect_view = RedirectView(url='/about/')\n        request = self.rf.get('/')\n        response = redirect_view.get(request)\n        self.assertRedirects(response, '/about/')\n", "    def test_redirect_view(self):\n        # Test the RedirectView.\n        redirect_view = RedirectView.as_view(url=\"/some/other/url/\")\n        response = redirect_view(self.rf.get('/'))\n        self.assertRedirects(response, '/some/other/url/')\n\n        # Test with a pattern name.\n        redirect_view = RedirectView.as_view(pattern_name=\"some_pattern\")\n        response = redirect_view(self.rf.get('/'))\n        self.assertRedirects(response, reverse(\"some_pattern\"))\n\n        # Test with query string.\n        redirect_view = RedirectView.as_view(url=\"/some/other/url/\", query_string=True)\n        response = redirect_view(self.rf.get('/', params={'query': 'value'}))\n        self.assertRedirects(response, '/some/other/url/?query=value')\n\n        # Test with permanent redirect\n        redirect_view = RedirectView.as_view(url=\"/some/other/url/\", permanent=True)\n        response = redirect_view(self.rf.get('/'))\n        self.assertEqual(response.status_code, 301)\n\n        # Test for missing url and pattern_name\n        with self.assertRaises(ImproperlyConfigured):\n            RedirectView.as_view()\n\n\n\n", "    def test_redirect_view(self):\n        with override_settings(ROOT_URLCONF='django.contrib.admin.urls'):\n            view = RedirectView.as_view(url='http://www.google.com')\n            request = self.rf.get('/')\n            response = view(request)\n            self.assertEqual(response.status_code, 302)\n            self.assertEqual(response['Location'], 'http://www.google.com')\n", "    def test_redirect_view(self):\n        with override_settings(ROOT_URLCONF='test_urls'):\n            view = RedirectView.as_view(url='http://www.example.com/')\n            request = self.rf.get('/')\n            response = view(request)\n            self.assertEqual(response.status_code, 302)\n            self.assertEqual(response['Location'], 'http://www.example.com/')\n"], "sample_1140": ["compilation error", "def test_pretty_printing_matrix_expressions():\n    m = Matrix([[x, y], [z, w]])\n    assert pretty(m) == \"[[x, y], [z, w]]\"\n    assert upretty(m) == \"[[x, y],\\n[z, w]]\"\n\n\n    m2 = MatrixSymbol('M', 2, 2)\n    assert pretty(m2) == \"M\"\n    assert upretty(m2) == \"M\"\n\n    m3 = Matrix([[1, 2], [3, 4]])\n    assert pretty(m3.transpose()) == \"[[1, 3], [2, 4]]\"\n    assert upretty(m3.transpose()) == \"[[1, 3],\\n[2, 4]]\"\n\n    m4 = Matrix([[1, 2], [3, 4]])\n    assert pretty(m4.T) == \"[[1, 3], [2, 4]]\"\n    assert upretty(m4.T) == \"[[1, 3],\\n[2, 4]]\"\n\n\n\n    m5 = Matrix([ [1,2,3], [4,5,6]])\n    assert pretty(m5.extract(0, 1, 1, 2)) == \"[[2, 3]]\"\n    assert upretty(m5.extract(0, 1, 1, 2)) == \"[[2, 3]]\"\n\n    m6 = Matrix([ [1,2,3], [4,5,6]])\n    assert pretty(m6[0, 1:3]) == \"[2, 3]\"\n    assert upretty(m6[0, 1:3]) == \"[2, 3]\" \n\n\n\n    m7 = Matrix([[1, 2, 3], [4, 5, 6]])\n    m8 = m7.col_del(1)\n    assert pretty(m8) == \"[[1, 3], [4, 6]]\"\n    assert upretty(m8) == \"[[1, 3],\\n[4, 6]]\"\n", "        def __init__(self):\n            self.name = 'MyDomain'\n", "compilation error", "compilation error"], "sample_661": ["    def test_junit_no_tests(self, testdir):\n        result, _ = runandparse(testdir, \"-v\")\n        assert result.ret != 0\n        assert len(result.passed) == 0\n        assert \"No tests found\" in result.out\n\n\n", "    def test_global_properties(self, testdir):\n        result, dom = runandparse(testdir, \"test_global_properties.py\")\n\n        assert result.ret == 0\n        assert dom.find_first_by_tag(\"testsuite\").tag == \"testsuite\"\n\n        assert dom.find_first_by_tag(\"testsuite\").get_unique_child.tag == \"testsuite\"\n        assert dom.find_first_by_tag(\"testsuite\").get_unique_child.text == \"pytest\"\n\n        properties = dom.children[0].find_first_by_tag(\"properties\")\n        assert len(properties.children) == 2\n\n        assert properties.children[0].get_unique_child.text == \"value1\"\n        assert properties.children[1].get_unique_child.text == \"value2\"\n\n\n", "    def test_pytest_log_passing_tests(self, testdir):\n        result, node = runandparse(testdir, \"-s\")\n        assert result.ret == 0\n        suite = node.find_first_by_tag(\"testsuite\")\n        assert suite.get_unique_child.get_unique_child.tag == \"testcase\"\n        assert suite.get_unique_child.text == \"\"\n\n\n", "    def test_nested_test_report(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n                assert 1 == 1\n            \"\"\"\n        )\n        result, xml = runandparse(testdir, \"-k\", \"test_nested_test\")\n        assert result.ret == 0\n        suite = xml.get_unique_child\n\n        assert_attr(suite, name=\"TestNestedTest\", classname=\"TestNestedTest\", tests=\"1\")\n        assert_attr(suite.get_unique_child, name=\"test_nested_test\", classname=\"TestNestedTest\")\n", "    def test_double_failure(self, testdir):\n        result, dom = runandparse(testdir, \"-v\", \"--junitxml=junit.xml\")\n        assert result.ret == 0\n        suite = dom.find_first_by_tag(\"testsuite\")\n        assert_attr(suite, name=\"myproject\", tests=\"3\", failures=\"1\", errors=\"1\", skipped=\"0\", time=\"0.000\")\n\n"], "sample_194": ["    def test_unique_constraint_deconstructor(self):\n        fields = ['field1', 'field2']\n        condition = Q(field1__gt=10)\n        deferrable = Deferrable.DEFERRED\n        include = ('field3',)\n        opclasses = ('opclass1', 'opclass2')\n        name = 'unique_constraint'\n        constraint = UniqueConstraint(\n            fields=fields,\n            name=name,\n            condition=condition,\n            deferrable=deferrable,\n            include=include,\n            opclasses=opclasses,\n        )\n        path, args, kwargs = constraint.deconstruct()\n        self.assertEqual(path, 'django.db.models.constraints.UniqueConstraint')\n        self.assertEqual(args, ())\n        self.assertEqual(kwargs, {\n            'fields': fields,\n            'name': name,\n            'condition': condition,\n            'deferrable': deferrable,\n            'include': include,\n            'opclasses': opclasses,\n        })\n\n\n\n", "    def test_unique_constraint_with_condition(self):\n        with atomic():\n            Product.objects.create(name='product A', price=10)\n            with self.assertRaises(IntegrityError):\n                Product.objects.create(name='product A', price=10)\n\n            UniqueConstraintProduct.objects.create(name='product B', price=10)\n            with self.assertRaises(IntegrityError):\n                UniqueConstraintProduct.objects.create(name='product B', price=10)\n\n            with atomic():\n                UniqueConstraintProduct.objects.filter(name='product A').delete()\n                UniqueConstraintProduct.objects.create(name='product A', price=20)\n", "    def test_unique_constraint_deconstruction(self):\n        constraint = UniqueConstraint(\n            fields=['id', 'name'],\n            name='unique_name',\n            deferrable=Deferrable.IMMEDIATE,\n            opclasses=('opclass1', 'opclass2'),\n        )\n        path, args, kwargs = constraint.deconstruct()\n        self.assertEqual(path, 'django.db.models.constraints.UniqueConstraint')\n        self.assertEqual(args, ())\n        self.assertEqual(kwargs, {\n            'fields': ['id', 'name'],\n            'name': 'unique_name',\n            'deferrable': 'immediate',\n            'opclasses': ('opclass1', 'opclass2'),\n        })\n", "    def test_unique_constraint_deconstruct(self):\n        constraint = UniqueConstraint(fields=['name'], name='unique_name')\n        path, args, kwargs = constraint.deconstruct()\n        self.assertEqual(path, 'django.db.models.constraints.UniqueConstraint')\n        self.assertEqual(args, ())\n        self.assertEqual(kwargs, {'fields': ['name'], 'name': 'unique_name'})\n\n        reconstructed_constraint = UniqueConstraint(*args, **kwargs)\n        self.assertEqual(constraint, reconstructed_constraint)\n", "    def test_unique_constraint_with_condition(self):\n        with atomic():\n            Product.objects.create(name='Product A')\n            with self.assertRaises(IntegrityError):\n                UniqueConstraintConditionProduct.objects.create(\n                    name='Product A',\n                )\n        constraints = get_constraints(UniqueConstraintConditionProduct._meta.db_table)\n        self.assertEqual(\n            [c['name'] for c in constraints if c['type'] == 'unique'],\n            ['unique_constraint_condition_product_unique']\n        )\n"], "sample_820": ["    def test_voting_classifier_multilabel(self):\n        X, y = make_multilabel_classification(n_samples=100, n_features=10,\n                                             n_classes=4, random_state=42)\n        clf = VotingClassifier(estimators=[('lr', LogisticRegression(multi_class='multinomial')),\n                                            ('knn', KNeighborsClassifier())],\n                              voting='hard')\n        with pytest.raises(NotImplementedError):\n            clf.fit(X, y)\n\n\n", "    def test_voting_classifier_multilabel(self):\n        X, y = make_multilabel_classification(n_samples=100, n_features=5,\n                                              n_classes=4, random_state=42)\n        clf = VotingClassifier(estimators=[('rf', RandomForestClassifier()),\n                                          ('ovr', OneVsRestClassifier(SVC()))],\n                              voting='hard')\n        with pytest.raises(NotImplementedError):\n            clf.fit(X, y)\n\n\n\n", "    def test_voting_classifier_multi_label_classification_raises(self):\n        X, y = make_multilabel_classification(n_samples=100, n_features=10, n_classes=5, random_state=42)\n        clf = VotingClassifier(estimators=[\n                ('lr', LogisticRegression(random_state=42)),\n                ('rf', RandomForestClassifier(random_state=42))\n            ])\n        with pytest.raises(NotImplementedError,\n                         match=\"Multilabel and multi-output classification is not supported.\"):\n            clf.fit(X, y) \n", "    def test_voting_classifier_multiclass_with_one_vs_rest(self):\n        # Test with multiclass data and OneVsRestClassifier\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=0.3, random_state=42\n        )\n\n        ovr_clf = OneVsRestClassifier(estimator=LogisticRegression())\n        voting_clf = VotingClassifier(\n            [(\"lr\", LogisticRegression()), (\"ovr\", ovr_clf)], voting=\"hard\"\n        )\n        voting_clf.fit(X_train, y_train)\n        y_pred = voting_clf.predict(X_test)\n\n        assert_array_equal(y_pred, ovr_clf.predict(X_test))\n", "    def test_voting_classifier_soft_weights(self):\n        lr = LogisticRegression(random_state=42)\n        rf = RandomForestClassifier(n_estimators=10, random_state=42)\n        gnb = GaussianNB()\n\n        # Define weights\n        weights = [0.5, 0.3, 0.2]\n\n        clf = VotingClassifier(\n            estimators=[('lr', lr), ('rf', rf), ('gnb', gnb)],\n            voting='soft',\n            weights=weights\n        )\n        clf.fit(X, y)\n\n        # Check predictions with weighted average probabilities\n        y_pred = clf.predict(X)\n        sorted_prob = clf.predict_proba(X)\n        for i in range(len(y_pred)):\n            # Assert that the predicted class index corresponds to the\n            # highest weighted probability\n            assert y_pred[i] == np.argmax(\n                np.average(sorted_prob[i], axis=1, weights=weights)\n            )\n"], "sample_1043": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1112": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_422": ["    def test_prefetch_related_complex_with_ordering(self):\n        with self.assertNumQueries(2):\n            qs = Article.objects.prefetch_related(\n                'authors',\n                Prefetch(\n                    'biography',\n                    queryset=Bio.objects.order_by('date_published').select_related('person'),\n                ),\n            ).all()\n\n        self.assertEqual(len(qs), 2)\n        for article in qs:\n            self.assertEqual(len(article.authors), 2)\n            for author in article.authors:\n                self.assertEqual(len(author.biography), 1)\n\n        # Verify ordering on the prefetched `Biography` objects.\n\n\n", "    def test_prefetch_related_circular(self):\n        with ignore_warnings(category=RemovedInDjango50Warning):\n            with self.assertNumQueries(2):\n            # Simulate a circular related object relationship\n                class Article(models.Model):\n                    content = models.TextField()\n                    author = models.ForeignKey(Author, on_delete=models.CASCADE)\n\n                class Author(models.Model):\n                    name = models.CharField(max_length=100)\n                    article = models.OneToOneField(Article, on_delete=models.CASCADE, related_name=\"author_article\")\n\n                article = Article.objects.create(content=\"Test Article\")\n                author = Author.objects.create(name=\"Test Author\")\n                author.article = article\n                author.save()\n\n                articles = Article.objects.prefetch_related(\"author\").get(id=article.id)\n\n                self.assertIsNotNone(articles.author)\n\n\n\n", "    def test_prefetch_related_with_related_manager(self):\n        with CaptureQueriesContext() as query_context:\n            instances = (\n                Book.objects.prefetch_related(\n                    Prefetch(\"authors\", queryset=Author.objects.filter(name=\"Charlotte\"))\n                )\n                .all()\n            )\n\n        self.assertEqual(len(query_context.queries), 2)\n        self.assertWhereContains(query_context.queries[1].sql, \"authors__name='Charlotte'\")\n        for book in instances:\n            self.assertEqual(len(book.authors.all()), 1)\n\n\n", "    def test_prefetch_related_with_nested_object(self):\n        # Test prefetch_related with a nested object via a ManyToMany relationship\n        with CaptureQueriesContext() as queries:\n            prefetch_items = prefetch_related_objects(\n                Book.objects.all(),\n                \"authors\",\n                \"bio__content\",\n            )\n\n        self.assertEqual(len(queries.captured_queries), 1)\n        self.assertWhereContains(queries.captured_queries[0].sql,\n                                \"'book_authors'\")\n\n        for book in prefetch_items:\n            self.assertTrue(book.authors.all())\n            for author in book.authors.all():\n                self.assertTrue(author.bio is not None)\n\n\n\n", "    def test_prefetch_related_with_complex_lookups(self):\n        with CaptureQueriesContext() as queries:\n            qs = Book.objects.prefetch_related(\n                'authors',\n                Prefetch(\n                    'qualification',\n                    queryset=Qualification.objects.filter(name__startswith=\"A\"),\n                ),\n            )\n            qs = qs.filter(title__startswith=\"W\")\n            qs.get()\n\n        expected_count = 2\n        self.assertEqual(len(queries.captured_queries), expected_count)\n        self.assertWhereContains(queries.captured_queries[0], 'title__startswith(\"W\")')\n        self.assertWhereContains(queries.captured_queries[1], 'name__startswith(\"A\")')\n\n\n"], "sample_296": ["    def test_encode_empty_list(self):\n        storage = self.storage_class()\n        messages = []\n        encoded_data = storage._encode(messages)\n        self.assertEqual(encoded_data, b'')\n", "    def test_cookie_encoding_empty_list(self):\n        storage = self.storage_class(request=self.request)\n        set_cookie_data(storage, [])\n        messages = storage._decode(storage.request.COOKIES[storage.cookie_name])\n        self.assertIsNone(messages)\n", "    def test_encode_empty_messages(self):\n        storage = self.storage_class()\n        messages = []\n        encoded_data = storage._encode(messages, encode_empty=True)\n        self.assertEqual(encoded_data, b'')\n", "    def test_encode_empty_message_list(self):\n        storage = self.storage_class()\n        encoded_data = storage._encode([])\n        self.assertEqual(len(encoded_data), 0)\n", "    def test_encode_empty_cookie(self):\n        storage = self.storage_class()\n        messages = []\n        encoded_data = storage._encode(messages, encode_empty=True)\n        self.assertEqual(len(encoded_data), 0)\n        self.assertTrue(storage.signer.unsign_object(encoded_data, serializer=MessageSerializer))\n"], "sample_788": ["def test_inverse_transform():\n    X = np.array(X)\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n    est.fit(X)\n    Xt = est.transform(X)\n    Xinv = est.inverse_transform(Xt)\n    assert_array_almost_equal(Xinv, np.array([[-1.5, 1.5, -3.5, -0.5],\n                                            [-0.5, 2.5, -2.5, -0.5],\n                                            [0.5, 3.5, -1.5, 0.5],\n                                            [0.5, 3.5, -1.5, 1.5]]))\n\n    with warnings.catch_warnings():\n        warnings.simplefilter('ignore')\n        est = KBinsDiscretizer(n_bins=3, encode='onehot', strategy='kmeans')\n        est.fit(X)\n        Xt = est.transform(X)\n        Xinv = est.inverse_transform(Xt)\n    assert_array_almost_equal(Xinv, np.array([[-1.5, 1.5, -3.5, -0.5],\n                                            [-0.5, 2.5, -2.5, -0.5],\n                                            [0.5, 3.5, -1.5, 0.5],\n                                            [0.5, 3.5, -1.5, 1.5]]))\n\n\n\n", "def test_kbinsdiscretizer_inverse_transform(strategy, expected):\n    X = np.array(X)\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy=strategy)\n    est.fit(X)\n    Xt = est.transform(X)\n    Xinv = est.inverse_transform(Xt)\n    assert_array_almost_equal(Xinv, np.array([[ -1.5, 1.5, -3.5, -0.5],\n                                             [ -0.5, 2.5, -2.5, -0.5],\n                                             [ 0.5, 3.5, -1.5, 0.5],\n                                             [ 0.5, 3.5, -1.5, 1.5]]))\n    \n    ", "def test_inverse_transform_ordinal():\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='quantile')\n    est.fit(X)\n    Xt = est.transform(X)\n    Xinv = est.inverse_transform(Xt)\n    expected = [[0.5, 2.5, -2.5, -0.5], [0.5, 2.5, -2.5, -0.5], [0.5, 2.5, -1.5, 0.5], [0.5, 2.5, -1.5, 1.5]]\n    assert_array_almost_equal(Xinv, expected, decimal=2)\n\n", "def test_fit_invalid_n_bins():\n    with assert_raise_message(\"ValueError: KBinsDiscretizer received an invalid number of \"\n                              \"bins at indices 0. Number of bins must be at least 2, \"\n                              \"and must be an int.\"):\n        KBinsDiscretizer(n_bins=[0, 2, 3])\n\n\n\n", "compilation error"], "sample_704": ["def test_path_with_lineno(\n    testdir: Pytester, fspath: str, expected_nodeid: str", "    def test_iterparentnodeids_with_sep(self, pytester: Pytester):\n        from _pytest import nodes\n\n        class MockNode(nodes.Node):\n                super().__init__(name=name)\n                return f\"MockNode('{self.name}')\"\n\n        parent = MockNode(\"parent\")\n        child1 = MockNode(\"child1\")\n        child2 = MockNode(\"child2\")\n        parent.parent = None\n        child1.parent = parent\n        child2.parent = parent\n        iterables = [\n            iterparentnodeids(parent.nodeid),\n            iterparentnodeids(child1.nodeid),\n            iterparentnodeids(child2.nodeid),\n        ]\n        expected_results = [\n            [\"\"],\n            [\"\", \"parent\"],\n            [\"\", \"parent\", \"child1\"],\n            [\"\", \"parent\", \"child2\"],\n        ]\n        for i, iterable in enumerate(iterables):\n            assert list(iterable) == expected_results[i]\n\n        # Check that it doesn't error when handling nodes with :\n        child3 = MockNode(\"child3::child4\")\n        child3.parent = parent\n        for nodeid in [\"child3::child4\", \"child3::child4::child5\"]:\n            assert list(iterparentnodeids(nodeid)) == expected_results[2]\n\n\n\n", "compilation error", "compilation error", "    def test_iterparentnodeids_empty(self, testdir: Pytester) -> None:\n        p = testdir.makepyfile(\n            \"\"\"\n            \"\"\"\n        )\n        result = list(nodes.iterparentnodeids(nodes.Node.from_parent(None, name=\"test_one\")))\n        assert result == [\"\", \"\"]\n\n"], "sample_74": ["    def test_environment_variables(self):\n        dbinfo = {\n            'host': 'localhost',\n            'port': '5432',\n            'database': 'mydatabase',\n            'user': 'myuser',\n            'password': 'mypassword',\n            'sslmode': 'verify-full',\n            'sslrootcert': '/path/to/rootcert',\n            'sslcert': '/path/to/cert',\n            'sslkey': '/path/to/key',\n        }\n        self._run_it(dbinfo)\n        expected_env = {\n            'PGPASSWORD': 'mypassword',\n            'PGSSLMODE': 'verify-full',\n            'PGSSLROOTCERT': '/path/to/rootcert',\n            'PGSSLCERT': '/path/to/cert',\n            'PGSSLKEY': '/path/to/key',\n        }\n        self.assertEqual(self.pg_env, expected_env)\n", "    def test_runshell_db_with_ssl(self):\n        dbinfo = {\n            'database': 'my_database',\n            'user': 'my_user',\n            'password': 'my_password',\n            'host': 'localhost',\n            'port': 5432,\n            'sslmode': 'require',\n            'sslrootcert': '/path/to/rootcert.pem',\n            'sslcert': '/path/to/server.crt',\n            'sslkey': '/path/to/server.key',\n        }\n        _, pg_env = self._run_it(dbinfo)\n        self.assertEqual(pg_env['PGSSLMODE'], 'require')\n        self.assertEqual(pg_env['PGSSLROOTCERT'], '/path/to/rootcert.pem')\n        self.assertEqual(pg_env['PGSSLCERT'], '/path/to/server.crt')\n        self.assertEqual(pg_env['PGSSLKEY'], '/path/to/server.key')\n", "    def test_runshell_db_with_ssl_params(self):\n        dbinfo = {\n            'database': 'mydatabase',\n            'user': 'myuser',\n            'host': 'localhost',\n            'port': '5432',\n            'sslmode': 'require',\n            'sslrootcert': '/path/to/rootcert',\n            'sslcert': '/path/to/cert',\n            'sslkey': '/path/to/key',\n        }\n        args, env = self._run_it(dbinfo)\n        self.assertIn('-h', args)\n        self.assertIn('-p', args)\n        self.assertIn('-U', args)\n        self.assertIn('mydatabase', args)\n        self.assertIn('PGSSLMODE', env)\n        self.assertEqual(env['PGSSLMODE'], 'require')\n        self.assertIn('PGSSLROOTCERT', env)\n        self.assertEqual(env['PGSSLROOTCERT'], '/path/to/rootcert')\n        self.assertIn('PGSSLCERT', env)\n        self.assertEqual(env['PGSSLCERT'], '/path/to/cert')\n        self.assertIn('PGSSLKEY', env)\n        self.assertEqual(env['PGSSLKEY'], '/path/to/key')\n", "compilation error", "    def test_runshell_db_with_ssl_params(self):\n        dbinfo = {\n            'host': 'localhost',\n            'database': 'testdb',\n            'user': 'testuser',\n            'password': 'testpassword',\n            'sslmode': 'verify-full',\n            'sslrootcert': '/path/to/rootcert',\n            'sslcert': '/path/to/cert',\n            'sslkey': '/path/to/key',\n        }\n        args, env = self._run_it(dbinfo)\n        self.assertIn('-U', args)\n        self.assertIn('-h', args)\n        self.assertIn(dbinfo['database'], args)\n        self.assertIn('PGPASSWORD', env)\n        self.assertIn('PGSSLMODE', env)\n        self.assertIn('PGSSLROOTCERT', env)\n        self.assertIn('PGSSLCERT', env)\n        self.assertIn('PGSSLKEY', env)\n"], "sample_714": ["compilation error", "compilation error", "    def test_hamming_loss_multilabel():\n        X, y_true, _ = make_prediction(binary=False)\n        y_pred = np.zeros_like(y_true)\n        for i in range(len(y_true)):\n            y_pred[i] = np.random.randint(0, len(np.unique(y_true)), size=len(y_true[i]))\n        \n        loss = hamming_loss(y_true, y_pred)\n        assert loss != 0\n\n\n\n", "compilation error", "compilation error"], "sample_507": ["    def test_unit_data(self, data, locs):\n        unit_data = cat.UnitData(data)\n        assert unit_data._mapping == dict(zip(data, locs))\n\n\n\n\n", "    def test_unit_data_update(self, data, locs):\n        unit_data = cat.UnitData(data)\n        assert unit_data._mapping == dict(zip(data, locs))\n", "    def test_unit_data(self, data, locs):\n        unit_data = cat.UnitData(data[0])\n        assert unit_data._mapping == dict(zip(data[0], range(len(data[0]))))\n        with pytest.raises(TypeError):\n            cat.UnitData( [1,2,3])\n\n\n\n\n", "    def test_unit_data(self, data, locs):\n        unit_data = cat.UnitData(data)\n        assert len(unit_data._mapping) == len(locs[0])\n        for idx, val in enumerate(data):\n            assert unit_data._mapping[val] == locs[1][idx]\n        \n", "    def test_update_mixed_data(self, data, locs):\n        unit = cat.UnitData(data)\n        assert len(unit._mapping) == len(locs)\n        for i, val in enumerate(data):\n            assert unit._mapping[val] == locs[i]\n        with pytest.warns(MatplotlibDeprecationWarning):\n            # Test that the warning is triggered\n            unit.update(data)\n        assert len(unit._mapping) == len(locs)\n        for i, val in enumerate(data):\n            assert unit._mapping[val] == locs[i]\n\n"], "sample_431": ["    def test_models_with_deferred_fields(self):\n        # Regression test for #13866:\n        # Model instances with deferred fields should be unpicklable.\n        with self.subTest(\"Direct pickle\"):\n            article = Article.objects.create(title=\"Article 1\")\n            unpickled_article = model_unpickle(article)\n            self.assertEqual(unpickled_article.pk, article.pk)\n\n        with self.subTest(\"Tuple pickle\"):\n            unpickled_article = model_unpickle(('django.test.models', 'Article'))\n            self.assertEqual(unpickled_article.__class__.__name__, 'Article')\n\n\n\n", "    def test_model_subclass_unpickling(self):\n        with mock.patch('django.apps.apps') as apps:\n            apps.get_model.return_value = Article\n            article = model_unpickle(Article)\n            self.assertIsInstance(article, Article)\n            \n            # Test with a tuple\n            article_tuple = ('myapp.Article',)\n            model_unpickle(article_tuple)\n            apps.get_model.assert_called_once_with(*article_tuple)\n\n\n", "    def test_model_instance_creation_with_default_values(self):\n        \"\"\"\n        Test that model instances are created with default values set correctly.\n        \"\"\"\n        with self.assertNumQueries(1):\n            article = Article.objects.create(\n                headline=\"Test Article\", publication_date=datetime.now()\n            )\n        self.assertEqual(article.headline, \"Test Article\")\n        self.assertIsInstance(article.publication_date, datetime)\n        self.assertIsNotNone(article.publication_date)\n\n        with self.assertNumQueries(1):\n            child_pk = PrimaryKeyWithDefault.objects.create(\n                name=\"Test Child\", extra_attr=\"Test Extra\"\n            )\n        self.assertEqual(child_pk.name, \"Test Child\")\n        self.assertEqual(child_pk.extra_attr, \"Test Extra\")\n        self.assertTrue(child_pk.is_active)\n\n\n\n        with self.assertNumQueries(1):\n            child_pk_db_default = PrimaryKeyWithDbDefault.objects.create(\n                name=\"Test Child with DB Default\", extra_attr=\"Test Extra\"\n            )\n        self.assertEqual(child_pk_db_default.name, \"Test Child with DB Default\")\n        self.assertEqual(child_pk_db_default.extra_attr, \"Test Extra\")\n        self.assertTrue(child_pk_db_default.is_active)\n\n", "    def test_primary_key_models_with_db_default(self):\n        with self.assertRaises(DatabaseError):\n            PrimaryKeyWithDbDefault.objects.create()\n\n        # Use the default value in the database\n        obj = PrimaryKeyWithDbDefault.objects.create(name='Test')\n        self.assertEqual(obj.id, models.Field.get_default(PrimaryKeyWithDbDefault._meta.pk))\n\n\n", "    def test_field_name_clashes(self):\n        with self.assertRaises(ValueError):\n            class ClashingModel(models.Model):\n                name = models.CharField(max_length=100)\n                name = models.CharField(max_length=100)\n\n        with self.assertRaises(ValueError):\n            class ClashingModel(models.Model):\n                field_name = models.CharField(max_length=100)\n                field_name = models.IntegerField()\n\n\n\n"], "sample_66": ["    def test_multiple_content_types(self):\n        request = self.factory.post(\n            '/my/view/',\n            data={'key1': 'value1', 'key2': 'value2'},\n            content_type='application/x-www-form-urlencoded',\n        )\n        self.assertEqual(request.method, 'POST')\n        self.assertEqual(request.POST['key1'], 'value1')\n        self.assertEqual(request.POST['key2'], 'value2')\n\n", "    def test_body_read_twice(self):\n        request = self.create_request(content_type='text/plain', content=\"hello\")\n        self.assertEqual(request.body, b'hello')\n        with self.assertRaises(RawPostDataException):\n            request.body\n", "    def test_get_full_path_info_with_slash(self):\n        request = HttpRequest()\n        request.path = '/path/to/resource/'\n        self.assertEqual(request.get_full_path_info(force_append_slash=True), '/path/to/resource/')\n", "    def test_upload_multiple_files(self):\n        rf = RequestFactory()\n        request = rf.post(\n            '/upload/',\n            {\n                'file': FakePayload(\n                    'file1', 'application/octet-stream', b'file1 content1', content_type='multipart/form-data'\n                ),\n                'file2': FakePayload(\n                    'file2', 'application/octet-stream', b'file2 content2', content_type='multipart/form-data'\n                )\n            }\n        )\n        request.META = {\n            'CONTENT_TYPE': 'multipart/form-data; boundary=------------------------boundary'\n        }\n        with self.assertRaises(MultiPartParserError):\n            request.parse_file_upload(request.META, request.raw_post_data)\n", "    def test_get_host_with_colon(self):\n        req = HttpRequest()\n        req.META = {\n            'HTTP_HOST': 'example.com:9090',\n        }\n        self.assertEqual(req.get_host(), 'http://example.com:9090')\n"], "sample_1204": ["    def test_coset_properties(self):\n        G = SymmetricPermutationGroup(4)\n        a = Permutation([1, 2, 0, 3])\n        H = PermutationGroup([a])\n        left_coset = Coset(Permutation(2, 3), H, dir=\"-\")\n        right_coset = Coset(Permutation(2, 3), H, dir=\"+\")\n        self.assertEqual(len(left_coset.as_list()), 4)\n        self.assertEqual(len(right_coset.as_list()), 4)\n        self.assertEqual(set(left_coset.as_list()), set(right_coset.as_list()))\n\n", "    def test_coset(self):\n        G = PermutationGroup(list(rubik_cube_generators(cube)))\n        g = G.generators[0]\n        H = G.subgroup(generator=g)\n        cst = Coset(g, H, dir='-')\n        expected = [perm * g for perm in H.elements]\n        assert cst.as_list() == expected \n", "    def test_coset_equality(self):\n      a = Permutation([1, 2, 0, 4, 5, 3])\n      G = PermutationGroup([a])\n      H = G\n      c1 = Coset(a, H, dir='+')\n      c2 = Coset(a, H, dir='+')\n      c3 = Coset(a*a, H, dir='+')\n      assert c1 == c2\n      assert c1 != c3\n", "    def test_coset_equality(self):\n        G = SymmetricPermutationGroup(3)\n        g1 = Permutation([1, 2, 0])\n        g2 = Permutation([0, 1, 2])\n        H = G.subgroup([g1])\n        c1 = Coset(g1, H, dir='+')\n        c2 = Coset(g2, H, dir='+')\n        assert c1 != c2\n        c3 = Coset(g1, H, dir= '-')\n        c4 = Coset(g2, H, dir= '-')\n        assert c3 == c4\n\n\n\n", "    def test_coset_basic(self):\n        G = SymmetricPermutationGroup(4)\n        H = G.subgroup(permutation_list=[Permutation(0, 1), Permutation(2, 3)])\n        g = Permutation(0, 2)\n        left_coset = Coset(g, H, G, dir=\"-\")\n        right_coset = Coset(g, H, G, dir=\"+\")\n        self.assertEqual(left_coset.as_list(), [Permutation(0, 2), Permutation(2, 0), Permutation(1, 3), Permutation(3, 1)])\n        self.assertEqual(right_coset.as_list(), [Permutation(0, 2), Permutation(2, 0), Permutation(1, 3), Permutation(3, 1)])\n"], "sample_961": ["    def test_parse_annotation(self):\n        assert _parse_annotation('int') == 'int'\n        assert _parse_annotation('int, ..., float') == ('int', '...', 'float')\n        assert _parse_annotation('MyType[int, str]') == ('MyType', [('int', None), ('str', None)])\n        assert _parse_annotation('list[int]') == ('list', [('int', None)])\n        assert _parse_annotation('list[int, ...]') == ('list', [('int', None), ('...', None)])\n\n        # typing module\n        assert _parse_annotation('typing.Optional[int]') == ('typing.Optional', [('int', None)])\n\n", "    def test_parse_arglist_empty(self):\n        sig = \"function_name()\"\n        parsed = parse(sig)\n        assert parsed == \"function_name()\"\n", "    def test_parse_annotation(self):\n        assert _parse_annotation('int') == 'int'\n        assert _parse_annotation('int, str') == 'int, str'\n        assert _parse_annotation('int: str') == 'int: str'\n        assert _parse_annotation('int, str: int') == 'int, str: int'\n        assert _parse_annotation('Optional[str]') == 'Optional[str]'\n        assert _parse_annotation('List[int]') == 'List[int]'\n        assert _parse_annotation('Union[int, str]') == 'Union[int, str]'\n        assert _parse_annotation('None') == 'None'\n\n\n\n", "    def test_parse_annotation(self):\n        self.assertEqual(_parse_annotation('str'), ('str',))\n        self.assertEqual(_parse_annotation('NoneType'), ('NoneType',))\n        self.assertEqual(_parse_annotation('list[int]'), ('list', [('int',)]))\n        self.assertEqual(_parse_annotation('dict[str, int]'), ('dict', [('str',), ('int',)]))\n        self.assertEqual(_parse_annotation('Union[str, int]'), ('Union', [('str',), ('int',)]))\n        self.assertEqual(_parse_annotation('Optional[str]'), ('Optional', [('str',)]))\n        self.assertEqual(_parse_annotation('list[None]'), ('list', [('NoneType',)]))\n        self.assertEqual(_parse_annotation('typing.List[str]'), ('typing.List', [('str',)]))\n        self.assertEqual(_parse_annotation('list[typing.List[int]]'), ('list', [('typing.List', [('int',)])]))\n\n", "    def test_parse_annotation(self):\n        self.assertEqual(_parse_annotation('Optional[str]'),\n                         {'type': 'Optional', 'args': ['str']})\n        self.assertEqual(_parse_annotation('List[int]'),\n                         {'type': 'List', 'args': ['int']})\n        self.assertEqual(_parse_annotation('Union[str, int]'),\n                         {'type': 'Union', 'args': ['str', 'int']})\n        self.assertEqual(_parse_annotation('typing.List[int]'),\n                         {'type': 'List', 'args': ['int'], 'module': 'typing'})\n        self.assertEqual(_parse_annotation('typing.Union[str, int]'),\n                         {'type': 'Union', 'args': ['str', 'int'], 'module': 'typing'})"], "sample_689": ["    def test_deprecated_warnings(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.parametrize(\"attribute\", pytest.collect.__all__)  # type: ignore\n                getattr(pytest.collect, attribute)\n                \n                deprecated.FILLFUNCARGS.warning(\"name\")  \n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines(\n            [\n                *re.escape(\"deprecated.FILLFUNCARGS.*use function._request._fillfixtures()\"),\n                \n            ]\n        )\n", "    def test_deprecated_plugin_registration(testdir):\n        testdir.makepyfile(\"\"\"\n            pass\n        \"\"\")\n        with pytest.warns(deprecated.DEPRECATED_EXTERNAL_PLUGINS):\n            result = testdir.runpytest()\n        assert result.ret == 0\n", "    def test_collect_module_deprecation(testdir):\n        deprecation = deprecated.PYTEST_COLLECT_MODULE\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            result = testdir.runpytest(\n                \"-m\", \"deprecated_collect_module\",\n                )\n            assert len(w) == 1, \"Expected one warning\"\n            assert w[0].category is deprecation\n\n\n", "    def test_deprecated_collect_module_warning(testdir):\n        testdir.write_file(\n            \"test_module.py\",\n            \"\"\"\n            import pytest\n\n            @pytest.collect.item\n                pass\n            \"\"\"\n        )\n\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines([\n            \"*deprecated:*pytest.collect.item*\",\n            \"*pytest.collect.my_item*\",\"*pytest.collect.*my_item*\"\n        ])\n", "    def test_deprecated_pytest_collect_module_warning(self, pytester: Pytester):\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.parametrize(\"attribute\", pytest.collect.__all__)  \n                assert attribute == 'name'\n            \"\"\"\n        )\n        result = pytester.runpytest_subprocess()\n        warnings_captured = result.get_warnings(category=deprecated.PYTEST_COLLECT_MODULE)\n        assert len(warnings_captured) == 1\n        assert warnings_captured[0].message == re.sub(\n            r\"\\n\", \"\\n\", \"pytest.collect.name was moved to pytest.name\\nPlease update to the new name.\"\n        )\n"], "sample_1115": ["    def test_riemann_cyclic_replace():\n        from sympy.tensor.tensor import TensorIndexType, tensor_indices, TensorHead, riemann_cyclic_replace, TensorSymmetry\n        Lorentz = TensorIndexType('Lorentz', dummy_name='L')\n        i, j, k, l = tensor_indices('i,j,k,l', Lorentz)\n        R = TensorHead('R', [Lorentz]*4, TensorSymmetry.riemann())\n        t = R(i,j,k,l)*(R(-i,-j,-k,-l) - 2*R(-i,-k,-j,-l))\n        t_cyclic = riemann_cyclic_replace(t)\n        t_cyclic2 = riemann_cyclic_replace(t_cyclic)\n        assert _is_equal(t_cyclic2, S.Zero)\n\n", "    def test_riemann_cyclic_replace():\n        L = TensorIndexType('L')\n        i, j, k, l = tensor_indices('i,j,k,l', L)\n        R = TensorHead('R', [L]*4, TensorSymmetry.riemann())\n        t = R(i,j,k,l)*(R(-i,-j,-k,-l) - 2*R(-i,-k,-j,-l))\n        t_cyclic = riemann_cyclic_replace(t)\n        assert _is_equal(t_cyclic, 0)\n", "    def test_canon_bp_symmetric_indices():\n        from sympy.tensor.tensor import TensorSymmetry\n        L = TensorIndexType('L')\n        A = TensorHead('A', [L, L], TensorSymmetry.fully_symmetric(2))\n        a = A(L(1), L(2))\n        b = canon_bp(a)\n        assert _is_equal(b, a)\n", "    def test_riemann_cyclic_replace():\n        L = TensorIndexType('L')\n        i, j, k, l = tensor_indices('i,j,k,l', L)\n        R = TensorHead('R', [L]*4, TensorSymmetry.riemann())\n        t = R(i,j,k,l)*(R(-i,-j,-k,-l) - 2*R(-i,-k,-j,-l))\n        t_cyclic = riemann_cyclic_replace(t)\n        t_cyclic = t_cyclic.doit()\n        assert t_cyclic == 0\n", "    def test_tensor_head_with_no_symmetry():\n        from sympy.tensor.tensor import TensorHead\n\n        a = TensorHead(\"a\", [TensorIndexType('L',dummy_name='L')]*2, TensorSymmetry.no_symmetry(2))\n        assert a.is_symmetric == False\n        assert a.symmetries == []\n\n\n"], "sample_359": ["    def test_alter_together_option(self):\n        with atomic():\n            unicode_app = self.apps.get_app_config('myapp')\n            unicode_model = unicode_app.get_model('UnicodeModel')\n\n            # Create a state with the initial model\n            state = self.create_state(\n                apps=[unicode_app],\n                initial_models={'myapp': {'UnicodeModel': unicode_model._meta}}\n            )\n\n            # Test a simple 'unique_together' change\n            state.alter_model_options(\n                'myapp',\n                'UnicodeModel',\n                {'unique_together': [('name', 'description')]},\n            )\n            with CaptureQueriesContext(connection) as q_context:\n                self.run_operations(\n                    state,\n                    [\n                        AlterTogetherOptionOperation(\n                            'UnicodeModel',\n                            {'unique_together': [('name', 'description')]},\n                        ),\n                    ],\n                )\n            self.assertEqual(q_context.captured_queries, [])  # No queries in memory\n\n            # Check the model has been updated\n            new_state = self.state_forwards(state)\n            self.assertEqual(\n                new_state.models['myapp']['UnicodeModel'].unique_together,\n                [('name', 'description')],\n            )\n\n            # Test a 'index_together' change\n            state.alter_model_options(\n                'myapp',\n                'UnicodeModel',\n                {'index_together': [('name', 'description')]},\n            )\n            with CaptureQueriesContext(connection) as q_context:\n                self.run_operations(\n                    state,\n                    [\n                        AlterTogetherOptionOperation(\n                            'UnicodeModel',\n                            {'index_together': [('name', 'description')]},\n                        ),\n                    ],\n                )\n            self.assertEqual(q_", "    def test_add_remove_index(self):\n        with atomic(using=self.connection.alias):\n            with self.assertNumQueries(1):\n                # Adding an index\n                self.state.migrate(\n                    ProjectState(\n                        models={\n                            'my_app.UnicodeModel': UnicodeModel.objects.all().order_by('name').values_list('name', flat=True),\n                        },\n                    ),\n                    [\n                        AddIndex(\n                            'my_app.UnicodeModel',\n                            models.Index(\n                                fields=['name'],\n                                name='name_index',\n                            ),\n                        ),\n                    ],\n                )\n            index_exists = self.connection.cursor().execute(\n                \"SELECT EXISTS(SELECT 1 FROM information_schema.statistics WHERE table_name = %s AND index_name = %s)\",\n                (self.get_table_name('my_app', 'unicodemodel'), 'name_index'),\n            )\n            self.assertEqual(index_exists, 1)\n\n            # Removing the index\n            self.state.migrate(\n                ProjectState(\n                    models={\n                        'my_app.UnicodeModel': UnicodeModel.objects.all().order_by('name').values_list('name', flat=True),\n                    },\n                ),\n                [\n                    RemoveIndex(\n                        'my_app.UnicodeModel',\n                        'name_index',\n                    ),\n                ],\n            )\n            index_exists = self.connection.cursor().execute(\n                \"SELECT EXISTS(SELECT 1 FROM information_schema.statistics WHERE table_name = %s AND index_name = %s)\",\n                (self.get_table_name('my_app', 'unicodemodel'), 'name_index'),\n            )\n            self.assertEqual(index_exists, 0)\n\n\n\n", "    def test_create_order_with_respect_to_field_migration(self):\n        \"\"\"\n        Tests that add and remove OrderWithRespectTo operations work correctly\n        and only modify the related field's order if it's added/removed.\n        \"\"\"\n\n        model_name = 'MyModel'\n\n        # Setup\n        self.state.add_model(model_name, models.Model(\n            Meta=models.ModelMeta(ordering=['name'], order_with_respect_to='category'),\n        ))\n        self.state.add_model(\n            'Category',\n            models.Model(name=models.CharField(max_length=100)),\n        )\n        self.state.add_migration(\n            Migration(\n                id='0001_initial',\n                operations=[\n                    CreateModel(\n                        name='Category',\n                        fields=[\n                            ('name', models.CharField(max_length=100)),\n                        ],\n                    ),\n                    CreateModel(\n                        name='MyModel',\n                        fields=[\n                            ('name', models.CharField(max_length=100)),\n                            ('category', models.ForeignKey(to='Category', on_delete=models.CASCADE)),\n                        ],\n                        options={'ordering': ['name'], 'permissions': ['can_view_my_model']},\n                    ),\n                ],\n            ),\n        )\n\n        with atomic():\n            connection.create_model(model_name, tablespace='django_migrations_test')\n            connection.create_model('Category', tablespace='django_migrations_test')\n\n        with transaction.atomic():\n            self.assertEqual(\n                connection.introspection.describe_table(model_name),\n                {'columns': [\n                    ('id', 'INTEGER PRIMARY KEY'),\n                    ('name', 'VARCHAR(100)'),\n                    ('category_id', 'INTEGER'),\n                    ('ordering', 'JSON'),\n                ]}\n            )\n", "    def test_alter_unique_together(self):\n        state = ProjectState()\n        app_name = 'testapp'\n        state.apps.register_app(app_name)\n        with atomic(using=self.connection.alias):\n            models.CreateModel(\n                app_label=app_name,\n                name='MyModel',\n                fields=[\n                    models.CharField(max_length=100),\n                    models.CharField(max_length=100),\n                ],\n                unique_together=(),\n            )\n\n        original_state = state.copy()\n\n        state.alter_model_options(\n            app_name, 'mymodel', {'unique_together': [('field1', 'field2')]},\n        )\n        with self.assertNumQueries(1) :\n            self.execute_migration_forwards(state, app_name)\n        updated_state = state.copy()\n        self.assertEqual(updated_state.models[app_name, 'mymodel'].options['unique_together'], [('field1', 'field2')])\n\n        # Now reverse it\n        state.alter_model_options(\n            app_name, 'mymodel', {'unique_together': []},\n        )\n        with self.assertNumQueries(1):\n            self.execute_migration_backwards(state, app_name)\n        self.assertEqual(state.models[app_name, 'mymodel'].options['unique_together'], ())\n        self.assertEqual(state, original_state)\n\n\n\n", "    def test_add_index_with_expressions(self):\n        from django.db.models import F, Value\n        from django.db.models.functions import Concat\n\n        with atomic(using=self.connection.alias):\n            with transaction.atomic():\n                with self.assertNumQueries(1) as queries:\n                    self.operations.add_index(\n                        'app.UnicodeModel',\n                        name='unicode_concat_index',\n                        fields=('content',),\n                        expressions=[Concat('content', Value('_test'))],\n                    )\n                    self.apps.makemigrations('app')\n                    self.apps.migrate(\n                        []\n                    )\n                    self.assertEqual(len(queries), 1)\n                    self.assertEqual(queries.queries[0].sql,\n                                     \"CREATE INDEX unicode_concat_index ON app_unicodemodel (content || '_test')\")\n\n                with self.assertNumQueries(0) as queries:\n                    self.operations.remove_index('app.UnicodeModel', 'unicode_concat_index')\n                    self.apps.makemigrations('app')\n                    self.apps.migrate(\n                        []\n                    )\n                    self.assertEqual(len(queries), 1)\n\n\n"], "sample_988": ["    def test_infinity_power():\n        raises(TypeError, lambda: oo**oo)\n        raises(TypeError, lambda: oo**-oo)\n        raises(TypeError, lambda: -oo**oo)\n        raises(TypeError, lambda: -oo**-oo)\n\n", "    def test_pow():\n        # Test pow with complex numbers\n        assert (I**2).evalf() == -1.0\n        assert (I**3).evalf() == -I\n        assert (I**float(1.5)).evalf() == 1.7320508075688772j\n\n        # Test integer powers\n        assert x**2 == x*x\n        assert (x**3).__mul__(x).simplify() == x**4\n        assert (x**2)**3 == x**6\n\n        # Test fractional powers\n        assert (x**0.5).as_base_exp() == (x, 0.5)\n        assert (x**-1) == 1/x\n        assert (x**Rational(3, 4)).as_base_exp() == (x, Rational(3, 4))\n", "    def test_complex_infinity_comparison():\n        assert not (zoo == zoo)\n        assert zoo != zoo\n        assert zoo > complex(1e100)\n    ", "    def test_GoldenRatio_limit():\n        from sympy.abc import k\n        from sympy.integrate import integrate\n        # Check the limit as k approaches infinity\n        limit = limit(\n            (integrate(1/(x**GoldenRatio + 1), (x, 0, 1)))/(k+1), k, oo)\n        assert limit == 0\n", "    def test_catalan():\n        assert isinstance(S.Catalan, NumberSymbol)\n        assert S.Catalan > 0\n        assert S.Catalan < 1\n        assert not S.Catalan.is_integer\n\n\n\n"], "sample_1046": ["compilation error", "compilation error", "    def test_TensorElement_free_indices():\n        from sympy.tensor.tensor import TensorIndexType, tensor_indices, tensorhead\n        L = TensorIndexType(\"L\")\n        i, j = tensor_indices('i,j', L)\n        A = tensorhead(\"A\", [L, L], [[1], [1]])\n        te = TensorElement(A(i, j), {i: 2})\n        assert te.get_free_indices() == [j]\n\n", "compilation error", "    def test_contract_metric_with_indices():\n        L = TensorIndexType('L', dummy_fmt='L')\n        g = L.metric\n        i, j, k = tensor_indices('i,j,k', L)\n        A = tensorhead('A', [L, L], [[1], [1]])\n        t = A(i, j)* g(j, k)\n        t_ = t.contract_metric(g)\n        assert _is_equal(t_, A(i, k))\n\n\n"], "sample_61": ["    def test_ascii_username_validator(self):\n        validator = ASCIIUsernameValidator()\n        self.assertEqual(validator.validate('valid_username'), [])\n        self.assertEqual(validator.validate('valid.username'), [])\n        self.assertEqual(validator.validate('valid_username123'), [])\n        self.assertEqual(validator.validate('valid_username+_.'), [])\n\n        with self.assertRaises(ValidationError):\n            validator.validate('invalid username')\n        with self.assertRaises(ValidationError):\n            validator.validate('username with spaces')\n        with self.assertRaises(ValidationError):\n            validator.validate('username1 with $ special character')\n\n\n", "    def test_ascii_username_validator(self):\n        validator = ASCIIUsernameValidator()\n        self.assertIsNone(validator.validate('johndoe'))\n        self.assertIsNone(validator.validate('john_doe'))\n        self.assertIsNone(validator.validate('john.doe'))\n        self.assertIsNone(validator.validate('john+doe'))\n        self.assertIsNone(validator.validate('john-doe'))\n        with self.assertRaises(ValidationError):\n            validator.validate('john!doe')\n        with self.assertRaises(ValidationError):\n            validator.validate('john\\doe')\n        with self.assertRaises(ValidationError):\n            validator.validate('john\u20acdoe')\n\n\n\n\n", "    def test_username_validation(self):\n        ASCII_VALID_USERNAMES = [\n            \"username\",\n            \"username123\",\n            \"user+name\",\n            \"user.name\",\n            \"user_name\",\n        ]\n        UNICODE_VALID_USERNAMES = [\n            \"us\\u0301ername\",\n            \"us\\u0301ername123\",\n            \"user\\u0301+name\",\n            \"user\\u0301.name\",\n            \"user\\u0301_name\",\n        ]\n\n        for username in ASCII_VALID_USERNAMES:\n            try:\n                validators.ASCIIUsernameValidator()(username)\n            except ValidationError:\n                self.fail(f\"Valid ASCII username '{username}' rejected.\")\n\n        for username in UNICODE_VALID_USERNAMES:\n            try:\n                validators.UnicodeUsernameValidator()(username)\n            except ValidationError:\n                self.fail(f\"Valid Unicode username '{username}' rejected.\")\n\n        INVALID_USERNAMES = [\n            \"\",\n            \" \",\n            \"user name\",\n            \"user!@#$%^\",\n            \"user..name\",\n            \"user/name\",\n            \"\\u20AC\",  # Euro symbol\n        ]\n        for username in INVALID_USERNAMES:\n            with self.assertRaises(ValidationError):\n                validators.ASCIIUsernameValidator()(username)\n            with self.assertRaises(ValidationError):\n                validators.UnicodeUsernameValidator()(username)\n", "    def test_validate_username(self):\n        with isolate_apps(['django.contrib.auth']):\n            try:\n                User.objects.create_user('user_name', 'email@example.com', 'password')\n                self.fail('Username validation should have failed')\n            except ValidationError:\n                pass\n\n            # Test with valid ASCII username\n            User.objects.create_user('valid_ascii_username', 'email@example.com', 'password')\n\n            # Test with valid Unicode username\n            User.objects.create_user('valid_unicode_username', 'email@example.com', 'password')\n\n", "    def test_ascii_username_validator(self):\n        validator = ASCIIUsernameValidator()\n        self.assertEqual(validator.regex, r'^[\\w.@+-]+\\Z')\n        self.assertEqual(validator.flags, re.ASCII)\n        self.assertEqual(validator.message, _(\n            'Enter a valid username. This value may contain only English letters, '\n            'numbers, and @/./+/-/_ characters.'\n        ))\n        self.assertTrue(validator('username'))\n        with self.assertRaises(ValidationError):\n            validator('user name')\n        with self.assertRaises(ValidationError):\n            validator('username123<>')\n        with self.assertRaises(ValidationError):\n            validator('username with \u00e4\u00f6\u00fc\u00df')\n\n"], "sample_392": ["    def test_json_field_contains(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\"\"\"\n                CREATE TABLE mytesttable (\n                    id SERIAL PRIMARY KEY,\n                    my_json JSON\n                );\n            \"\"\")\n        model = JSONModel.objects.create(my_json={\"one\": 1, \"two\": 2})\n\n        self.assertTrue(JSONModel.objects.filter(my_json__contains={\"one\": 1}).exists())\n        self.assertFalse(JSONModel.objects.filter(my_json__contains={\"three\": 3}).exists())\n\n        self.assertEqual(\n            JSONModel.objects.filter(my_json__contains={\"one\": 1}).count(), 1\n        )\n\n\n", "    def test_has_key_lookup_with_complex_paths(self):\n        with self.assertNumQueries(1) as query_context:\n            instances = JSONModel.objects.filter(\n                data__has_key__in=[\n                    (\"name\", Value(\"John Doe\")),\n                    (\"address\", \"123 Main St\"),\n                    (\"['phone', 'number']\", \"555-1234\"),\n                ]\n            ).values(\"id\")\n\n        self.assertEqual(query_context.query_count, 1)\n        self.assertEqual(instances, [{\"id\": 1}])\n\n\n", "    def test_contains_with_subquery(self):\n        with self.assertNumQueries(2) as query_context:\n            json_data = {\"a\": 1, \"b\": {\"nested\": 2}}\n            self.create_data({\"json_field\": json_data})\n            qs = JSONModel.objects.filter(\n                json_field__contains={\"b\": {\"nested\": 2}}\n            )\n            self.assertQueryCount(2, query_context)\n            qs = qs.values(\"id\")\n            self.assertEqual(list(qs), [{\"id\": 1}])\n", "    def test_not_supported_contains_lookup(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\n                \"CREATE TABLE not_supported_json_table (id INTEGER PRIMARY KEY, data JSON)\"\n            )\n        with self.subTest(\"contains\"):\n            with self.assertRaises(NotSupportedError):\n                JSONModel.objects.filter(data__contains={\"key\": \"value\"})\n        with self.subTest(\"contained_by\"):\n            with self.assertRaises(NotSupportedError):\n                JSONModel.objects.filter(data__contained_by={\"key\": \"value\"})\n\n", "    def test_json_field_with_empty_string_value(self):\n        with self.assertNumQueries(1):\n            instance = JSONModel.objects.create(json_field={\"a\": 1})\n            self.assertEqual(instance.json_field, {\"a\": 1})\n            instance.json_field = \"\"\n            instance.save()\n            self.assertEqual(instance.json_field, {})\n"], "sample_868": ["compilation error", "    def test_metric_invariance_to_input_order(metric):\n        y_true = np.sort(y1, axis=0)\n        y_pred = np.sort(y2, axis=0)\n        metric_func = SUPERVISED_METRICS[metric]\n        score_1 = metric_func(y_true, y_pred)\n        score_2 = metric_func(y_pred, y_true)\n        assert_allclose(score_1, score_2, rtol=1e-6)\n", "compilation error", "    def test_ami_symmetry(metric_name):\n        metric_func = SUPERVISED_METRICS[metric_name]\n        for permutation in [(y1, y2), (y2, y1)]:\n            score1 = metric_func(*permutation)\n            score2 = metric_func(*permutation[::-1])\n            assert_allclose(score1, score2, rtol=1e-05)\n", "    def test_normalized_metrics_upper_bound(metric):\n        for i in range(5):\n            y_true = rng.randint(3, size=30)\n            y_pred = rng.randint(3, size=30)\n    \n            score = getattr(SUPERVISED_METRICS, metric)(y_true, y_pred)\n            assert 0 <= score <= 1, (\n                f\"The normalized metric '{metric}' should have a value\"\n                \" between 0.0 and 1.0.\"\n            )\n\n\n\n"], "sample_830": ["        def test_get_sys_info(self):\n            sys_info = _get_sys_info()\n            # Check for expected keys\n            self.assertIn('python', sys_info)\n            self.assertIn('executable', sys_info)\n            self.assertIn('machine', sys_info)\n\n", "    def test_get_blas_info(mock_get_blas_info):\n        blas_info = _get_blas_info()\n        assert blas_info['macros'] == ', '.join(['OPENMP=2'])\n        assert blas_info['lib_dirs'] == '/usr/local/lib'\n        assert blas_info['cblas_libs'] == 'cblas' \n", "compilation error", "def test_show_versions():\n    sys_info = _get_sys_info()\n    deps_info = _get_deps_info()\n    blas_info = _get_blas_info()\n\n    expected_output = \"\"\"", "        def test_get_sys_info(self):\n            sys_info = _get_sys_info()\n            self.assertIn(\"python\", sys_info)\n            self.assertIn(\"executable\", sys_info)\n            self.assertIn(\"machine\", sys_info)\n"], "sample_946": ["    def test_py_sig_re_annotation(self):\n        self.assertEqual(parse('func(a: int) -> str'), 'func(a: int) -> str')\n        self.assertEqual(parse('func(a: int, b=None) -> str'), 'func(a: int, b=None) -> str')\n        self.assertEqual(parse('func(a: int, b: float = None) -> str'), 'func(a: int, b: float = None) -> str')\n\n", "    def test_resolve_any_xref_for_property(self):\n        env = Mock()\n        builder = Mock()\n        fromdocname = 'test'\n        node = pending_xref(\n            reftarget='my_property',\n            refdomain='py',\n            reftype='meth',\n            reftitle='my_property',\n        )\n        contnode = nodes.paragraph()\n        domain = PythonDomain(app=Mock(), config=Mock())\n        domain.objects = {\n            'my_property': IndexEntry('test', 5, '', 'test_property', '', '', '')\n        }\n        result = domain.resolve_any_xref(env, fromdocname, builder, '', node, contnode)\n        assert len(result) == 1\n        assert result[0][0] == 'py:_prop'\n\n\n\n", "    def test_filter_meta_fields_no_meta(monkeypatch):\n        env = Mock(spec=Environment)\n        env.config = Mock(spec=Config)\n        domain = PythonDomain(env)\n\n        text = \"\"\"", "    def test_parse_annotation(app: Mock, env: Mock):\n        domain = PythonDomain(app, env)\n        result = _parse_annotation('\"str\", List[int]')\n        assert result == ('__args__', 'str', 'List[int]')\n\n\n", "    def test_parse_annotation(self):\n        assert _parse_annotation('None') == 'None'\n        assert _parse_annotation('int') == 'int'\n        assert _parse_annotation('List[str]') == 'List[str]'\n        assert _parse_annotation('Optional[int]') == 'Optional[int]'\n        assert _parse_annotation('typing.List[str]') == 'typing.List[str]'\n        assert _parse_annotation('int, float') == 'int, float'\n        assert _parse_annotation('int -> str') == 'int -> str'\n\n\n\n"], "sample_207": ["    def test_key_transform_contains_with_list_values(self):\n        data = {\n            'name': 'John Doe',\n            'interests': ['reading', 'coding', 'hiking'],\n        }\n        with self.assertNumQueries(1):\n            instances = JSONModel.objects.filter(\n                json_field__has_any_keys=['reading', 'hiking']\n            ).all()\n        self.assertEqual(len(instances), 1)\n        self.assertEqual(instances[0].json_field['name'], 'John Doe')\n\n        with self.assertNumQueries(1):\n            instances = JSONModel.objects.filter(\n                json_field__has_any_keys=['coding', 'fishing']\n            ).all()\n        self.assertEqual(len(instances), 0)\n\n\n\n", "    def test_key_transform_lookup_with_nested_key(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\n                \"\"\"\n                INSERT INTO jsonmodel (id, json_data)\n                VALUES (%s, '{\"name\": {\"first\": \"Bart\", \"last\": \"Simpson\"}}'),\n                       (%s, '{\"name\": {\"first\": \"Homer\", \"last\": \"Simpson\"}}');\n                \"\"\",\n                (uuid.uuid4(), uuid.uuid4()),\n            )\n        results = JSONModel.objects.annotate(\n            full_name=Transform('name->>', 'first')\n        ).filter(\n            full_name=KeyTransform(\n                key_name='name->first',\n                source_expressions=['name'],\n            )\n        ).values_list('full_name', flat=True)\n        self.assertSequenceEqual(results, ['Bart', 'Homer'])\n\n\n\n", "    def test_key_transforms_with_nested_keys(self):\n        class NestedJSONModel(models.Model):\n            json_field = JSONField()\n\n        with self.assertNumQueries(1) as queries:\n            NestedJSONModel.objects.filter(json_field__1__2='value').exists()\n        self.assertEqual(queries.count, 1)\n\n\n", "    def test_key_lookups_with_subquery(self):\n        with self.assertNumQueries(2):\n            qs = JSONModel.objects.filter(\n                data__json_path='$.foo',\n            )\n            self.assertEqual(qs.count(), 3)\n            qs = qs.filter(\n                objects__isnull=False,\n            )\n            self.assertEqual(qs.count(), 3)\n", "    def test_json_field_serialization(self):\n        obj = JSONModel(name='Test', data={'name': 'test', 'age': 30})\n        obj.save()\n        json_string = serializers.serialize('json', [obj])\n        loaded_obj = serializers.deserialize('json', json_string)[0].object\n        self.assertEqual(loaded_obj.name, 'Test')\n        self.assertEqual(loaded_obj.data, {'name': 'test', 'age': 30})\n\n"], "sample_649": ["    def test_log_file_level(self, pytester: Pytester) -> None:\n        test_dir = self.tmp_path / \"test_file.py\"\n        with open(test_dir, \"w\") as f:\n            f.write(\n                \"\"\"", "    def test_disable_loggers(self, caplog, request: FixtureRequest):\n        config = request.config\n        config.option.logger_disable = [\"root_logger\"]\n        plugin = config.pluginmanager.get_plugin(\"logging-plugin\")\n        plugin._disable_loggers(loggers_to_disable=config.option.logger_disable)\n        with capturer.Text(out=True) as captured:\n            log_record = logging.getLogger(\"root_logger\").info(\"This is a test\")\n        assert captured.get_text() == \"\"\n        assert not log_record.exc_info\n\n\n\n", "   def test_capture_manager(self, caplog: LogCaptureFixture, request: FixtureRequest) -> None:\n        cap_manager = request.config.pluginmanager.get_plugin(\"capturemanager\")\n        assert isinstance(cap_manager, CaptureManager)\n\n        with cap_manager.global_disabled():\n            with self.test_config.in_plugin(\"pytest_logging_plugin\"):\n                logging.getLogger().setLevel(logging.DEBUG)\n                logging.debug(\"This message should not be captured\")\n\n\n\n\n\n", "    def test_log_file_path_set_correctly(\n        self, testdir: Pytester, caplog: LogCaptureFixture", "    def test_disable_loggers(self, caplog: pytest.CaptureFixture) -> None:\n        plugin = LoggingPlugin(self.config)\n        # Verify that loggers are correctly disabled\n        plugin._disable_loggers(loggers_to_disable=[\"root\", \"mylogger\"])\n        with pytest.raises(logging.NoHandlerError):\n            logging.getLogger(\"root\").info(\"This should not be logged\")\n\n        with pytest.raises(logging.NoHandlerError):\n            logging.getLogger(\"mylogger\").info(\"This should not be logged\")\n\n        # Make sure other loggers remain enabled\n        with caplog.at_level(logging.INFO):\n            logging.getLogger(__name__).info(\"This should be logged\")\n        assert \"This should be logged\" in caplog.text\n\n\n\n"], "sample_1052": ["def test_matrix_output_arguments():\n    from sympy.abc import x, y\n    r = make_routine('fcn', [Matrix([[x,y]])])\n    assert len(r.results) == 1\n    assert isinstance(r.results[0], OutputArgument)\n    assert r.results[0].name is None\n    assert r.results[0].result_var is not None\n    assert isinstance(r.results[0].result_var, MatrixSymbol)\n", "    def test_equality_output_arguments_rust(", "def test_matrix_output_C99():\n    x, y = symbols('x y')\n    mat = Matrix([[x, y], [x**2, y**2]])\n    r = make_routine('matrix_fcn', mat)\n    code = get_string(C99CodeGen.dump_code, [r], header=False, empty=False)\n    assert \"double *matrix_fcn\" in code\n    assert \"Matrix<2, 2>\" in code\n\n", "    def test_multiple_return_values(request):\n        if request.node.name.startswith(\"skip\"):\n            pytest.skip('Skipping test: No suitable backend found.')\n        x, y, z = symbols('x y z')\n        expr1 = x + y * z\n        expr2 = x**2\n        expr3 = z  # This will be returned separately\n        routine = make_routine('complex_func', [expr1, expr2, expr3])\n        expected = \"\"\"", "def test_f95_multiple_return_order():\n    from sympy.abc import x, y, z\n    routines = [make_routine(\"my_func\", [x + y, z ** 2], language=\"F95\")]\n    code = get_string(FCodeGen.dump_f95, routines, prefix=\"test\",\n                      header=False, empty=False)\n    assert 'double my_func(double x, double y, double z)' in code\n    assert 'my_func = x + y' in code\n    assert 'my_func = z ** 2' in code \n\n\n\n"], "sample_266": ["    def test_inconsistent_migration_history(self):\n        with override_settings(DATABASES={'default': {'NAME': ':memory:'}}), modify_settings(INSTALLED_APPS=['tests.apps.migration_recorder_test']):\n            connection = connections['default']\n            recorder = MigrationRecorder(connection)\n            recorder.record_applied_migration('my_app', '0001_initial')\n            recorder.record_applied_migration('my_app', '0002_second')\n            # Make sure the history is consistent\n            loader = MigrationLoader(connection)\n            loader.check_consistent_history(connection)\n\n            # Force an inconsistency - apply a migration before its dependency\n            recorder.record_applied_migration('my_app', '0003_third') \n            with self.assertRaises(InconsistentMigrationHistory):\n                loader = MigrationLoader(connection)\n                loader.check_consistent_history(connection)\n", "    def test_inconsistent_history(self):\n        # Create a test database connection.\n        test_db = connections['default']\n\n        # Create an app with migrations.\n        with modify_settings(INSTALLED_APPS=['test_migrations']):\n            migration_loader = MigrationLoader(test_db, load=True)\n\n        # Apply the first migration.\n        with connection.cursor() as cursor:\n            cursor.execute(\"CREATE TABLE test_migrations (id INTEGER PRIMARY KEY)\")\n\n        recorder = MigrationRecorder(test_db)\n        recorder.record_migration('test_migrations', '0001_initial')\n\n        # Try to apply the second migration without applying the first.\n        with self.assertRaises(InconsistentMigrationHistory):\n            migration_loader.check_consistent_history(test_db)\n", "    def test_recording_with_replacements(self):\n        with connections['default'].atomic():\n            loader = MigrationLoader(connection=connection)\n            loader.build_graph()\n\n            recorder = MigrationRecorder(connection=connection)\n            recorder.record_migration('test_migrations', '0001_initial')\n            recorder.record_migration('test_migrations', '0002_replace_0001')\n            recorder.record_migration('test_migrations', '0003_replace_0002')\n\n            self.assertCountEqual(recorder.applied_migrations(), [\n                ('test_migrations', '0001_initial'),\n                ('test_migrations', '0002_replace_0001'),\n                ('test_migrations', '0003_replace_0002'),\n            ])\n\n            loader.check_consistent_history(connection)  \n\n\n\n", "    def test_inconsistent_history(self):\n        with override_settings(DATABASES={'default': {'ENGINE': 'django.db.backends.sqlite3'}, 'other': {'ENGINE': 'django.db.backends.sqlite3'}}), modify_settings(INSTALLED_APPS=['test_migrations']):\n            # Create two separate databases to test consistency across databases.\n            conn_default = connections['default']\n            conn_other = connections['other']\n\n            # Create a migration loader for each connection.\n            loader_default = MigrationLoader(conn_default)\n            loader_other = MigrationLoader(conn_other)\n\n            # Apply a migration to one database and make sure it's\n            # inconsistent with the other database, but not with the\n            # loader's knowledge.\n            with conn_default.cursor() as cursor:\n                cursor.execute(\"INSERT INTO django_migrations (app, name, applied) VALUES ('test_migrations', '0001_initial', 1)\")\n            loader_default.graph.apply_migration(('test_migrations', '0001_initial'))\n            with conn_other.cursor() as cursor:\n                cursor.execute(\"INSERT INTO django_migrations (app, name, applied) VALUES ('test_migrations', '0001_initial', 0)\")\n            loader_other.graph.apply_migration(('test_migrations', '0001_initial'))  \n\n            try:\n                loader_other.check_consistent_history(conn_other)\n                self.fail(\"InconsistentMigrationHistory not raised\")\n            except InconsistentMigrationHistory as e:\n                self.assertIn(\n                    'applied before its dependency {}.{}'.format('test_migrations', '0001_initial'),\n                    str(e)\n                )\n", "    def test_inconsistent_history(self):\n        with override_settings(INSTALLED_APPS=['testapp']):\n            with connections['default'].atomic():\n                # Create some migrations\n                from testapp.tests.migrations import create_test_migrations\n                create_test_migrations(app_label='testapp')\n                # Apply the first migration\n                MigrationLoader(connection=connection).execute_migration('testapp', '0001_initial')\n            recorder = MigrationRecorder(connection)\n            recorder.record_migration(\n                app_label='testapp', migration_name='0001_initial'\n            )\n            # Test the inconsistency exception\n            with self.assertRaises(InconsistentMigrationHistory):\n                MigrationLoader(connection=connection).check_consistent_history(connection)\n"], "sample_810": ["    def test_pipeline_memory_disabled(self):\n        X, y = load_iris(return_X_y=True)\n        pca = PCA(n_components=2)\n        scaler = StandardScaler()\n        pipe = Pipeline([('scaler', scaler), ('pca', pca)])\n\n        # Test with memory=None\n        memory_location = None\n        with Memory(location=memory_location) as mem:\n            pipe.fit(X, y)\n            X_transformed = pipe.transform(X)\n        assert mem.cache_size == 0, 'Memory cache should be empty'\n        \n        # Test with fitting again, should not use cached results\n        pipe.fit(X, y)\n        X_transformed_again = pipe.transform(X)\n        assert not np.allclose(X_transformed, X_transformed_again), \\\n            \"Transformer should be refitted\"\n    \n", "    def test_pipeline_fit_transform_with_final_estimator_that_requires_fit_transform(self):\n        class Est(BaseEstimator):\n                return X\n\n        pipe = Pipeline([('scaler', StandardScaler()),\n                         ('est', Est())])\n        X = np.array([[1, 2], [3, 4]])\n        res = pipe.fit_transform(X)\n        assert_array_equal(res, X)\n", "    def test_pipeline_fit_transform(self, multi_class):\n        from sklearn.datasets import make_classification\n\n        X, y = make_classification(n_samples=100, n_features=20,\n                                   n_informative=10, random_state=42,\n                                   multi_class=multi_class)\n\n        pipeline = Pipeline([\n            ('scaler', StandardScaler()),\n            ('clf', SGDClassifier(random_state=42))\n        ])\n        pipeline.fit(X, y)\n        y_pred = pipeline.predict(X)\n        assert_array_equal(pipeline.steps[0][1].transform, X)\n        assert_array_equal(pipeline.steps[1][1].predict(X), y_pred)\n", "    def test_pipeline_non_transformer_estimator(self):\n        with assert_raises_regex(ValueError, 'Final estimator must'):\n            Pipeline([('transf', DummyRegressor()), ('est', NoFit())])\n\n", "    def test_fit_params_dispatch(self):\n        X = np.random.rand(10, 5)\n        y = np.random.randint(0, 2, size=10)\n        pipeline = Pipeline(\n            [('transf', TransfFitParams(a=1, b=2)), ('clf', LogisticRegression())]\n        )\n        pipeline.fit(X, y, c=3, d=4)\n        assert_equal(pipeline.steps[1][1].fit_params, {'c': 3, 'd': 4})\n"], "sample_598": ["    def test_short_numpy_repr(array):\n        short_repr = formatting.short_numpy_repr(array)\n        assert len(short_repr.splitlines()) <= 5\n        if array.ndim == 1:\n            assert \"array\" in short_repr\n\n", "    def test_format_array_flat(self):\n        a = np.arange(100)\n        s = formatting.format_array_flat(a, 10)\n        assert \"  \" in s\n", "    def test_short_numpy_repr(self, array, expected):\n        assert formatting.short_numpy_repr(array) == expected\n\n\n\n", "compilation error", "    def test_short_numpy_repr(self, array, expected):\n        assert formatting.short_numpy_repr(array) == expected\n"], "sample_147": ["    def test_union_distinct(self):\n        queryset1 = Number.objects.filter(num__gt=5).union(Number.objects.filter(num__lt=3))\n        queryset2 = Number.objects.filter(num__gt=5).union_distinct(Number.objects.filter(num__lt=3))\n        self.assertNumbersEqual(queryset1, [3, 4, 5, 6, 7, 8, 9])\n        self.assertNumbersEqual(queryset2, [3, 4, 5])\n", "    def test_union_with_distinct(self):\n        distinct_query1 = Number.objects.filter(num__gt=5, other_num__lt=5).values('num').distinct()\n        distinct_query2 = Number.objects.filter(num__lt=5, other_num__gt=5).values('num').distinct()\n        union_query = distinct_query1.union(distinct_query2)\n        self.assertNumbersEqual(union_query, [6, 7, 8, 9, 0, 1, 2, 3, 4])\n", "    def test_union_with_extra(self):\n        numbers = Number.objects.union(\n            Number.objects.filter(num__gt=5).extra(select={'extra_num': Value(10)})\n        )\n        self.assertNumbersEqual(numbers, [Number(num=i, other_num=10 - i) for i in range(10)], ordered=False)\n", "    def test_union_with_extra(self):\n        # Test UNION with an extra clause\n        numbers = Number.objects.filter(num__gt=5)\n        extra_numbers = Number.objects.filter(num__gt=7).extra(select={'new_num': 'num * 2'})\n        combined = numbers.union(extra_numbers, all=True)\n        self.assertNumbersEqual(combined, [8, 9])\n\n", "    def test_union_with_distinct(self):\n        # Test union_with with DISTINCT\n        distinct_queryset = Number.objects.filter(num__gt=5).distinct('num')\n        union_queryset = (\n            Number.objects.filter(num__lt=5)\n            .union(distinct_queryset)\n        )\n        self.assertNumbersEqual(union_queryset, list(range(10)))\n        # Assert distinctness\n        self.assertEqual(len(set(union_queryset)), 10)\n"], "sample_157": ["    def test_db_signature(self):\n        conn = get_connection_copy()\n        with mock.patch('django.db.backends.base.creation.BaseDatabaseCreation._get_test_db_name') as mock_get_db_name:\n            mock_get_db_name.return_value = 'test_unique_name'\n            db_signature = conn.creation.test_db_signature()\n            self.assertEqual(db_signature, (\n                conn.settings_dict['HOST'],\n                conn.settings_dict['PORT'],\n                conn.settings_dict['ENGINE'],\n                'test_unique_name',\n            ))\n", "    def test_db_signature(self):\n        conn = get_connection_copy()\n        conn.settings_dict['TEST']['NAME'] = 'test_super_special'\n\n        base_creation = BaseDatabaseCreation(conn)\n        first_signature = base_creation.test_db_signature()\n\n        conn.settings_dict['TEST']['NAME'] = 'test_another_special'\n\n        second_signature = base_creation.test_db_signature()\n\n        self.assertNotEqual(first_signature, second_signature)\n", "    def test_db_signature(self):\n        connection = get_connection_copy()\n        # Mock the _get_test_db_name method for a specific test database name\n        with mock.patch.object(BaseDatabaseCreation, '_get_test_db_name',\n                              return_value='my_test_db'):\n            signature = BaseDatabaseCreation(connection).test_db_signature()\n        self.assertEqual(signature, (\n            connection.settings_dict['HOST'],\n            connection.settings_dict['PORT'],\n            connection.settings_dict['ENGINE'],\n            'my_test_db',\n        ))\n", "    def test_db_signature(self):\n        with mock.patch('django.db.connection', wraps=get_connection_copy) as mock_connection:\n            # Mock the get_test_db_name method to return a predictable value\n            mock_connection.objects.TestDatabaseCreation.return_value._get_test_db_name = lambda self: 'mocked_test_db'\n\n            # Test with default settings\n            creation = BaseDatabaseCreation(mock_connection.return_value)\n            signature1 = creation.test_db_signature()\n\n            # Update the settings and test again\n            mock_connection.return_value.settings_dict['HOST'] = 'mocked_host'\n\n            creation = BaseDatabaseCreation(mock_connection.return_value)\n            signature2 = creation.test_db_signature()\n\n            self.assertNotEqual(signature1, signature2)  # Host change should differentiate\n\n\n\n", "    def test_db_signature(self):\n        connection = get_connection_copy()\n        # Make sure the db signature reflects the connection settings\n        with mock.patch('django.db.backends.base.creation.BaseDatabaseCreation._get_test_db_name') as mock_get_test_db_name:\n            mock_get_test_db_name.return_value = 'test_my_db'\n            db_signature = connection.creation.test_db_signature()\n            self.assertEqual(db_signature, (\n                connection.settings_dict['HOST'],\n                connection.settings_dict['PORT'],\n                connection.settings_dict['ENGINE'],\n                'test_my_db',\n            ))\n\n\n\n"], "sample_466": ["    def test_serialize_custom_types(self):\n        operation = custom_migration_operations.more_operations.CreateModel(\n            name='MyModel',\n            fields=[\n                models.CharField(max_length=100),\n                models.DecimalField(max_digits=10, decimal_places=2),\n                models.ForeignKey(\n                    'self', on_delete=models.CASCADE, related_name='related_models'\n                ),\n                models.ManyToManyField('self', related_name='related_models'),\n                models.FileField(upload_to=TestModel1.upload_to),\n                models.FloatField(),\n                models.TimeField(),\n                models.DateTimeField(),\n                models.BigIntegerField(),\n            ],\n        )\n        output, _ = OperationWriter(operation).serialize()\n        self.assertIn(\"MyModel\", output)\n\n        # Check if custom operations are serialized correctly\n        operation = custom_migration_operations.operations.MyCustomOperation(\n            arg1=Money(10.25), arg2=TextEnum.A\n        )\n        output, _ = OperationWriter(operation).serialize()\n        self.assertIn(\"MyCustomOperation\", output)\n        self.assertIn(\"Money\", output)\n        self.assertIn(\"TextEnum\", output)\n\n\n\n", "    def test_serialize_custom_types(self):\n        operation = custom_migration_operations.more_operations.CreateCustomField(\n            field=models.DecimalField(\n                name=\"price\", max_digits=10, decimal_places=2\n            )\n        )\n        writer = OperationWriter(operation)\n        serialized_operation, _ = writer.serialize()\n        self.assertEqual(\n            serialized_operation,\n            \"\"\"\n        migrations.AddField(\n            model_name='testmodel1',\n            name='price',\n            field=models.DecimalField('price', max_digits=10, decimal_places=2),\n        ),\n            \"\"\",\n        )\n\n        operation = custom_migration_operations.operations.CreateMoneyField(\n            field=models.DecimalField(\n                name=\"price\", max_digits=10, decimal_places=2\n            )\n        )\n        writer = OperationWriter(operation)\n        serialized_operation, _ = writer.serialize()\n        self.assertEqual(\n            serialized_operation,\n            \"\"\"\n        migrations.AddField(\n            model_name='testmodel1',\n            name='price',\n            field=custom_migration_operations.operations.MoneyField(\n                'price',\n                max_digits=10,\n                decimal_places=2,\n            ),\n        ),\n            \"\"\",\n        )\n\n\n\n", "    def test_serialize_complex_data_structures(self):\n        operation = custom_migration_operations.more_operations.AddCustomField(\n            model_name='TestModel1',\n            field=models.DateTimeField(\n                verbose_name='some_date_time',\n                auto_now_add=True,\n                blank=True,\n                null=True,\n                default=None,\n                help_text='This is a datetime field.',\n            ),\n        )\n\n        writer = OperationWriter(operation)\n        serialized_operation, _ = writer.serialize()\n        self.assertEqual(\n            serialized_operation,\n            \"\"\"\n            migrations.AddField(\n                model_name='testmodel1',\n                name='some_date_time',\n                field=models.DateTimeField(\n                    auto_now_add=True,\n                    blank=True,\n                    help_text='This is a datetime field.',\n                    null=True,\n                    verbose_name='some_date_time',\n                ),\n            ),\n        \"\"\",\n        )\n", "    def test_serialize_dict_with_nested_lists(self):\n        operation = migrations.AddField(\n            model_name=\"TestModel1\",\n            name=\"extra_thing\",\n            field=models.FileField(upload_to=TestModel1.upload_to),\n        )\n        serialized = OperationWriter(operation).serialize()[0]\n        self.assertIn(\"migrations.AddField(\", serialized)\n        self.assertIn(\"field=migrations.FileField(\", serialized)\n        self.assertIn(\"upload_to=TestModel1.upload_to\", serialized)\n", "    def test_serialize_list_with_sub_dicts(self):\n        operation = custom_migration_operations.more_operations.CreateCustomListFieldOperation(\n            name=\"my_list_field\",\n            model_name=\"TestModel1\",\n            field=models.JSONField(default=[]),\n            initial=[\n                {\"a\": 1, \"b\": \"hello\"},\n                {\"a\": 2, \"b\": \"world\"},\n            ],\n        )\n        writer = OperationWriter(operation, indentation=1)\n        result, _ = writer.serialize()\n        expected = \"\"\"\n        migrations.AddField(\n            model_name=\"TestModel1\",\n            name=\"my_list_field\",\n            field=models.JSONField(default=[{'a': 1, 'b': 'hello'}, {'a': 2, 'b': 'world'}])\n        )\"\"\"\n        self.assertEqual(result.strip(), expected.strip())\n"], "sample_386": ["    def test_mark_safe_decorator(self):\n        class MyView:\n                return \"Hello, world!\"\n\n        @mark_safe\n            return s + \"!\"\n\n        view = MyView()\n        self.assertEqual(mark_safe(view.my_method()), \"Hello, world!\")\n        self.assertEqual(my_custom_method(mark_safe(\"Safe\")), \"Safe!\")\n        self.assertEqual(my_custom_method(\"Not safe\"), \"Not safe!\")\n        self.assertEqual(\n            mark_safe(my_custom_method(\"Safe\")),\n            \"Safe!\",\n        )\n", "    def test_mark_safe_decorator(self):\n        class MyView(object):\n            @mark_safe\n                return s\n\n        view = MyView()\n        self.assertIsInstance(view.my_method(\"Hello\"), SafeString)\n\n        # Test with custom escape function\n        class MyViewWithCustomEscape(object):\n            @mark_safe\n                return customescape(s)\n\n        view = MyViewWithCustomEscape()\n        self.assertEqual(view.my_method(\"test < & >\"), \"test << & >>\")\n", "    def test_mark_safe_on_callback(self):\n            return customescape(\"<h1>\" + value + \"</h1>\")\n\n        safe_func = mark_safe(my_func)\n        self.assertRenderEqual(\n            '{{ my_func(\"Test\") }}', '<h1>Test</h1>',\n        )\n", "    def test_mark_safe_with_callable(self):\n            return mark_safe(f\"This is {x}\")\n\n        result = my_func(\"safe\")\n        self.assertIsInstance(result, SafeString)\n        self.assertEqual(str(result), \"This is safe\")\n\n", "    def test_mark_safe_callable(self):\n            return arg * 2\n\n        result = mark_safe(my_func)\n        self.assertIs(result.__name__, 'my_func')\n        self.assertTrue(callable(result))\n        self.assertEqual(result('hello'), SafeString('hellohello'))\n\n        # Check that nested calls don't break things\n        doubled = mark_safe(lambda x: x * 2)\n        self.assertEqual(doubled(mark_safe('foo')), SafeString('foofoo'))\n"], "sample_1002": ["    def test_sympify_float():\n        # Test for floats with different precisions\n        for p in [10, 50, 100]:\n            f1 = Float(1.23456789, p)\n            f2 = sympify(f1)\n            assert f2.prec == p\n            assert same_and_same_prec(f1, f2)\n\n", "def test_sympify_complex_negative():\n    x = -3 + 4j\n    assert sympify(x) == -3 + 4*I\n    x = -3 - 4j\n    assert sympify(x) == -3 - 4*I\n\n", "    def test_sympify_fractions():\n        assert sympify(1/2) == Rational(1, 2)\n        assert sympify(3/4) == Rational(3, 4)\n        assert sympify(-5/7) == Rational(-5, 7)\n", "    def test_numpify_floats():\n        for prec in [53, 100]:\n            with mpf.local_context(prec=prec):\n                f = Float('1.2345678901234567890123456789012345')\n                sympy_f = sympify(f)\n                assert sympy_f.n == f.n\n                assert sympy_f.prec == prec\n                assert sympy_f.is_rational == False\n\n\n\n                assert same_and_same_prec(sympy_f, f)\n                assert sympify(mpf('1.2345678901234567890123456789012345')) == sympy_f\n\n\n", "    def test_sympify_nan():\n        assert sympify(float('nan')) is nan\n"], "sample_148": ["    def test_collect_related_objects(self):\n        self._connect(0, 1)\n        self._connect(2, 3)\n        self._collect(0, 2)\n        self._check([\n            self.objs[1],\n            self.objs[3],\n        ])\n\n", "    def test_nested_relationships(self):\n        self._connect(1, 0)\n        self._connect(2, 1)\n        self._collect(0, 1, 2)\n        self._check([[0, 1, 2]])\n\n\n", "    def test_collect_with_related_objects(self):\n        self._connect(0, 1)\n        self._connect(2, 1)\n        self._collect(0, 2, 1)\n        self._check([[1, 0, 2]])\n\n\n\n", "    def test_collect_with_related_objects(self):\n        self.connect(0, 1)\n        self._collect(0, 1)\n        self._check([[1, 0]])\n\n        # Test using a related model attribute\n        self.connect(2, 1)\n        self._collect(1)\n        self._check([[1, 2]])\n\n", "    def test_related_objects_with_select_related(self):\n        self._collect(0, 1, 2)\n        related_objects = self.n.related_objects(Count.parent.model, ['parent'], [self.objs[0]])\n        self._check([self.objs[1]])\n"], "sample_372": ["compilation error", "    def test_no_urlconf_raises(self):\n        with self.assertRaises(ImproperlyConfigured):\n            get_resolver('')\n", "    def test_no_urlconf_raises(self):\n        with self.assertRaises(ImproperlyConfigured):\n            resolve('test_view', **{})\n", "    def test_reverse_with_named_groups_in_the_url(self):\n        with self.subTest(title=\"Named groups, simple case\"):\n            result = reverse('named_groups_view', args=(1,), kwargs={'k1': 'v1'})\n            self.assertEqual(result, '/groups/1/v1/')\n\n        with self.subTest(title=\"Named groups, with extra kwargs\"):\n            result = reverse('named_groups_view', args=(1,), kwargs={'k1': 'v1', 'k2': 'v2'})\n            self.assertEqual(result, '/groups/1/v1/')\n\n\n\n\n", "    def test_empty_urlconf(self):\n        with self.assertRaises(ImproperlyConfigured):\n            resolve('/')\n"], "sample_363": ["    def test_formfield_for_dbfield_ManyToManyField(self):\n        ff = self.assertFormfield(Student, 'groups', ManyToManyField, attrs={'class': 'vManyToMany'})\n        self.assertIsInstance(ff.choices, list)\n\n\n\n", "    def test_formfield_for_dbfield_ForeignKey(self):\n        ff = self.assertFormfield(Company, 'advisor', ForeignKeyRawIdWidget)\n        self.assertEqual(ff.form_class, forms.TextInput)\n        self.assertEqual(ff.widget.rel.field_name, 'advisor')\n", "    def test_formfield_for_dbfield_for_uuidfield(self):\n        self.assertFormfield(MyFileField, 'file', AdminUUIDInputWidget)\n\n", "    def test_formfield_for_dbfield_ManyToManyField(self):\n        ff = self.assertFormfield(Student, 'friend_groups', ManyToManyField,\n                                  formfield_overrides={ManyToManyField: forms.CheckboxSelectMultiple})\n        self.assertIsInstance(ff.widget, forms.CheckboxSelectMultiple)\n\n\n\n", "    def test_formfield_for_dbfield_ManyToManyField(self):\n        ff = self.assertFormfield(\n            model=Album, fieldname='bands',\n            widgetclass=AdminManyToManyRawIdWidget\n        )\n        self.assertEqual(ff.attrs['class'], 'vManyToManyRawIdAdminField')\n\n\n\n"], "sample_383": ["    def test_related_in_multi_column_lookup(self):\n        with ignore_warnings(category=RemovedInDjango50Warning):\n            # Test in case related fields are used in multi-column lookups\n            related_qs = Item.objects.filter(tags__in=[self.t1, self.t2], creator=self.a1)\n            self.assertQueryCount(related_qs, 1)\n            self.assertEqual(related_qs.exists(), True)\n            self.assertEqual(related_qs.first().name, \"one\")\n\n\n\n", "    def test_related_in_with_normalized_value(self):\n        with ignore_warnings(RemovedInDjango50Warning):\n            qs = Item.objects.filter(\n                creator__name__in=[\"a1\", \"a2\"],\n                extra__value__in=[41, 42],\n            )\n            self.assertQueryCount(qs, 4)\n\n            with CaptureQueriesContext(connection) as captured:\n                list(qs)\n            sql = captured.captured_queries[0]['sql']\n            self.assertIn('(creator_id IN (?, ?))', sql)\n            self.assertIn('(extra_value IN (?, ?))', sql)\n", "    def test_related_in_multi_column_lookup(self):\n        with ignore_warnings(category=RemovedInDjango50Warning):\n            # Test related in with multi-column lookups\n            related_items = Item.objects.filter(\n                annotations__tag__name__in=[\"t1\", \"t3\"],\n                extrainfo__note__id__in=[cls.n1.id, cls.n3.id],\n            )\n            self.assertQueryCount(related_items, 3)  # Should select i1, i2, i3\n\n            related_items = Item.objects.filter(\n                annotations__tag__name__in=[\"t1\", \"t3\"],\n                extrainfo__note__id__in=[cls.n1.id],\n            )\n            self.assertQueryCount(related_items, 1)  # Should select i1\n\n            related_items = Item.objects.filter(\n                annotations__tag__name__in=[\"t1\", \"t4\"],\n                extrainfo__note__id__in=[cls.n1.id, cls.n3.id],\n            )\n            self.assertQueryCount(related_items, 2)  # Should select i1, i3\n\n            related_items = Item.objects.filter(\n                annotations__tag__name__in=[],\n                extrainfo__note__id__in=[cls.n1.id, cls.n3.id],\n            )\n            self.assertQueryCount(related_items, 0)  # Should select nothing\n\n            related_items = Item.objects.filter(\n                annotations__tag__name__in=[\"t2\"],\n                extrainfo__note__id__in=[cls.n1.id, cls.n3.id],\n            )\n            self.assertQueryCount(related_items, 0)  # Should select nothing", "    def test_related_in_multicolumn_lookup(self):\n        with ignore_warnings(RemovedInDjango50Warning):\n            with CaptureQueriesContext(connection=DEFAULT_DB_ALIAS) as captured:\n                Article.objects.filter(\n                    author__in=[\n                        (getattr(Author, field) for field in [\"id\", \"name\"])\n                        for author in Author.objects.filter(name__startswith=\"a\")\n                    ]\n                ).exists()\n            Queries1Tests._verbose_assertEqual(\n                captured.captured_queries,\n                [\n                    \"SELECT COUNT(*) FROM django_tests_article WHERE id IN (%s)\",\n                    \"SELECT id, name FROM django_tests_author WHERE name LIKE %s\",\n                ],\n            )\n\n", "compilation error"], "sample_328": ["    def test_bulk_update_with_nested_objects(self):\n        article = Article.objects.create(\n            title=\"Nested Article\", content=\"Article content\"\n        )\n\n        tag = Tag.objects.create(name=\"Test Tag\")\n        article.tags.add(tag)\n        article.save()\n\n        tags = self.tags[:3]\n        note = Note.objects.create(note=\"Note for article\", misc=\"Misc for article\")\n        note.tags.add(*tags)\n        note.save()\n        article.notes.add(note)\n        article.save()\n\n        with transaction.atomic():\n            # Update the article's content and tags\n            Article.objects.filter(title=\"Nested Article\").update(\n                content=\"Updated article content\",\n                tags=Tag.objects.filter(name__in=[\"Tag 0\", \"Tag 2\"]),\n            )\n\n        updated_article = Article.objects.get(title=\"Nested Article\")\n        self.assertEqual(updated_article.content, \"Updated article content\")\n        self.assertEqual(set(updated_article.tags.all().values_list('name', flat=True)), {'Tag 0', 'Tag 2'})\n\n        updated_note = Note.objects.filter(article=updated_article).first()\n        self.assertEqual(updated_note.tags.count(), 2)\n\n", "    def test_prefetch_related(self):\n        # Create tags with relationships\n        self.create_tags()\n        note_ids = [note.id for note in self.notes]\n        Tag.objects.bulk_create(\n            [\n                Tag.objects.create(name=\"tag1\")\n                for _ in range(3)\n            ],\n            tags=[\n                {\n                    'note': note_ids[i],\n                    'tag': self.tags[int(i % 3)].id,\n                }\n                for i in range(10)\n            ]\n        )\n\n        # Fetch notes with tags prefetched\n        notes = Note.objects.prefetch_related('tags').filter(id__in=note_ids)\n\n        # Assert relationships are present\n        for note in notes:\n            self.assertTrue(note.tags.exists())\n            self.assertEqual(len(note.tags.all()), 1)\n\n\n\n", "    def test_bulk_update_objects_with_json_field(self):\n        tags = self.create_tags()\n        for note in self.notes[:5]:\n            note.tags.add(*tags[:3])\n        self.notes.update(misc=\"Updated\")\n        self.notes.update(note=\"Updated note\")\n        for note in self.notes:\n            assert note.tags.all().count() == 3\n            assert note.misc == \"Updated\"\n            assert note.note == \"Updated note\"\n", "    def test_update_multiple_fields(self):\n        # Update multiple fields of multiple notes in a bulk update.\n        num_updated = Note.objects.filter(note__startswith='3').update(\n            note='Updated note',\n            misc='Updated misc',\n        )\n        self.assertEqual(num_updated, 3)\n\n        updated_notes = Note.objects.filter(note__startswith='3')\n        for note in updated_notes:\n            self.assertEqual(note.note, 'Updated note')\n            self.assertEqual(note.misc, 'Updated misc')\n", "    def test_bulk_update_with_prefetch_related(self):\n        self.create_tags()\n        note_ids = [note.id for note in self.notes]\n        Note.objects.bulk_update(\n            note_ids, ['note'],\n            update_fields=['note'],\n            prefetch_related=['tags']\n        )\n        updated_notes = Note.objects.filter(id__in=note_ids).all()\n        for note in updated_notes:\n            self.assertEqual(note.note, str(updated_notes.index(note)))\n"], "sample_408": ["    def test_detect_changes_empty_to_empty(self):\n        before = self.make_project_state([])\n        after = self.make_project_state([])\n        changes = self.get_changes(before, after)\n        self.assertEqual(changes, {})\n", "    def test_circular_fk_migration(self):\n        before_states = self.make_project_state(\n            [\n                rabbit,\n            ]\n        )\n        after_states = self.make_project_state(\n            [\n                rabbit,\n            ]\n        )\n        changes = self.get_changes(before_states, after_states)\n        self.assertEqual(len(changes), 1)\n        self.assertEqual(\n            changes[\"eggs\"].__len__(), 1\n        )\n        self.assertOperationTypes(\n            changes, \"eggs\", 0, [\"ForeignKeyOperation\"]\n        )\n        self.assertOperationFieldAttributes(\n            changes,\n            \"eggs\",\n            0,\n            0,\n            to_field_name=\"knight\",\n            on_delete=\"CASCADE\",\n        )\n        self.assertOperationFieldAttributes(\n            changes,\n            \"eggs\",\n            0,\n            1,\n            to_field_name=\"parent\",\n            on_delete=\"CASCADE\",\n        )\n        self.assertMigrationDependencies(\n            changes, \"eggs\", 0, []\n        )\n\n\n", "    def test_migration_dependencies_circular(self):\n        before_states = self.make_project_state(\n            [\n                rabbit,\n                knight,\n            ]\n        )\n        after_states = self.make_project_state(\n            [\n                rabbit,\n                knight,\n            ]\n        )\n        changes = self.get_changes(before_states, after_states)\n        self.assertMigrationDependencies(\n            changes,\n            \"eggs\",\n            0,\n            [\"eggs.Knight\"],\n        )\n        self.assertMigrationDependencies(\n            changes,\n            \"eggs\",\n            1,\n            [\"eggs.Rabbit\"],\n        )\n\n\n\n", "    def test_migration_detection_circular_fk(self):\n        before_states = self.make_project_state(\n            [\n                rabbit,\n            ]\n        )\n        after_states = self.make_project_state(\n            [\n                rabbit,\n            ]\n        )\n        changes = self.get_changes(before_states, after_states)\n        self.assertEqual(len(changes), 1)\n        self.assertEqual(\n            list(changes.keys())[0], \"eggs\"\n        )\n        self.assertEqual(\n            len(changes[\"eggs\"]), 1\n        )\n        self.assertMigrationDependencies(\n            changes,\n            \"eggs\",\n            0,\n            [],\n        )\n", "    def test_rename_field_in_proxy_table(self):\n        model_states_before = [\n            ModelState(\n                \"testapp\",\n                \"AuthorProxy\",\n                [(\"user_id\", models.IntegerField(primary_key=True))],\n                {\"proxy\": True},\n                (\"testapp.author\",),\n            ),\n            ModelState(\n                \"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True))]\n            ),\n        ]\n        model_states_after = [\n            ModelState(\n                \"testapp\",\n                \"AuthorProxy\",\n                [(\"new_user_id\", models.IntegerField(primary_key=True))],\n                {\"proxy\": True},\n                (\"testapp.author\",),\n            ),\n            ModelState(\n                \"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True))]\n            ),\n        ]\n        changes, _ = self.get_changes(model_states_before, model_states_after)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertMigrationDependencies(changes, \"testapp\", 0, [])\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameField\"])\n\n\n\n"], "sample_1027": ["def test_cancel_with_domain():\n    from sympy.abc import x, y\n    from sympy.polys import cancel\n\n    F = Poly(x**2 + 1, x, domain=ZZ)\n    G = Poly(2*x + 2, x, domain=ZZ)\n\n    Q, R = cancel(F, G)\n    assert Q == Poly(x - 1, x, domain=ZZ)\n    assert R == Poly(1, x, domain=ZZ)\n\n", "    def test_nroots_complex_domain(self):\n        from sympy.abc import x, y\n        f = x**2 + y**2 - 1\n        roots = nroots(f, n=4, domain=Complex)\n        self.assertEqual(len(roots), 4)\n        expected = [\n            (1, 0), (-1, 0), (0, 1), (0, -1)\n        ]\n        for root, expected_root in zip(roots, expected):\n            self.assertTupleEqual(root, expected_root)\n\n\n\n", "def test_cancel_rational_expr():\n    from sympy import Rational, sin, cos, pi\n\n    expr = (2*x**2 + x)/(x**2 - 1)\n    gens = (x,)\n    expected = (2, x + 1)\n\n    result = cancel(expr, *gens)\n    assert _strict_eq(result, expected)\n\n    expr = (sin(pi/2) + cos(pi/2))/(tan(pi/4))\n    gens = (x,)\n    expected = (2, 1) \n\n    result = cancel(expr, *gens)\n    assert _strict_eq(result, expected)\n\n\n\n", "def test_groebner_fglm():\n    from sympy.polys import groebner, Monomial\n\n    x, y = Symbol('x'), Symbol('y')\n\n    F = [x**2 - 3*y - x + 1, y**2 - 2*x + y - 1]\n    G = groebner(F, x, y, order='grlex')\n    G_lex = G.fglm('lex')\n\n    assert _strict_eq(G_lex, groebner(F, x, y, order='lex'))\n    assert _epsilon_eq(len(G_lex), len(groebner(F, x, y, order='lex')))\n\n    # Check single variable case\n    G_single = groebner([x**2 - 1], x, order='lex')\n    G_single_new = G_single.fglm('grlex')\n    assert _strict_eq(G_single_new, groebner([x**2 - 1], x, order='grlex'))\n    assert G_single_new[0].LC(order='grlex') == G_single[0].LC(order='grlex')\n\n    # check if Fglm returns the same object\n    G_same = G.fglm('grlex')\n    assert G_same is G.fglm('grlex')\n\n\n\n", "    def test_nroots_with_complex_roots(self):\n        from sympy import I\n        F = x**4 - x**2 + 1\n        roots = nroots(F, n=4)\n        self.assertEqual(len(roots), 4)\n        self.assertTrue(all(x.imag for x in roots))\n\n\n\n"], "sample_377": ["    def test_repr(self):\n        callable_setting = lambda: \"hello\"\n        wrapper = CallableSettingWrapper(callable_setting)\n        self.assertEqual(repr(wrapper), \"<function CallableSettingWrapper.<locals>.<lambda>>\")\n\n\n", "    def test_callable_setting_wrapper(self):\n            return \"this is a test\"\n\n        wrapper = CallableSettingWrapper(callable_setting)\n        self.assertIsNot(wrapper, callable_setting)\n        self.assertEqual(wrapper(), \"this is a test\")\n", "    def test_callable_setting_wrapper_calls_callable(self):\n            return \"Callable setting value\"\n\n        wrapper = CallableSettingWrapper(callable_setting)\n        self.assertEqual(wrapper(), \"Callable setting value\")\n\n\n\n\n", "    def test_callable_setting_wrapper_call_without_exception(self):\n        wrapped = CallableSettingWrapper(lambda x: x * 2)\n        self.assertEqual(wrapped(5), 10)\n", "    def test_callable_setting_wrapper(self):\n        callable_setting = lambda: \"Callable Value\"\n        wrapper = CallableSettingWrapper(callable_setting)\n\n        self.assertIs(wrapper.__call__(), callable_setting())\n        self.assertEqual(repr(wrapper), repr(callable_setting))\n\n\n\n"], "sample_954": ["    def test_nested_inline_transform(self):\n        env = self.build_env\n        # Set env.config.man_show_urls = True\n        env.config = Config({'man_show_urls': True})\n        # add the following to src/doc/test_manpage.rst\n        #  .. manpage::\n        #  \n        #      <strong>foo=<emphasis>1</emphasis>&bar=<emphasis>2</emphasis></strong>\n        #  \n        #      .. seealso::\n        #         <http://www.example.com/>\n        # \n        #  .. manpage::\n        #      <strong>foo=<emphasis>var</emphasis></strong>\n        #      <strong>&bar=<emphasis>2</emphasis></strong>\n        #\n        #      .. seealso::\n        #         http://www.example.com/\n\n        self.app.build_dir = 'build'\n        self.app.srcdir = 'src'\n        output = self.app.build('src', 'build')\n        self.app.builder.prepare_outputs()\n\n        # assert output file content\n        with open(self.app.builder.get_filename('test_manpage.1'), 'r') as f:\n            content = f.read()\n        expected = (\n            f'.TH \"test_manpage\" \"MAN\" \"{format_date(self.config.today_fmt or _('%b %d, %Y'), language=self.config.language)}\" \"1.0\" \"example\"\\n'\n            f'.SH NAME\\ntest_manpage \\\\- \\n'\n            f'.SH DESCRIPTION\\n'\n            f'<strong>foo=var</strong>\\n'\n            f'<strong>&bar=2</strong>\\n'\n            f'.IP \"http://www.example.com/\"\\n'\n            )\n        assert content == expected\n", "    def test_nested_inline(self, make_app, tmpdir):\n        app = make_app(buildername='man', config=Config(\".\"), srcdir=tmpdir, doctreedir=tmpdir)\n        app.build()\n\n        with open(app.outdir / 'manual-page.1') as f:\n            contents = f.read()\n\n        assert '<strong>foo=</strong><emphasis>var</emphasis>' in contents\n        assert '<strong>&bar=</strong><emphasis>2</emphasis>' in contents\n", "    def test_productionlist_handling(self, build_dir):\n        app = self.app\n        app.config.man_show_urls = True  # enable showing URLs\n        config = Config(app._config)\n        config.man_pages = default_man_pages(config)\n        app.build(self.srcdir, build_dir)\n        with open(build_dir / 'manual.1', 'r', encoding='utf-8') as f:\n            output = f.read()\n        assert '.nf\\n' in output\n        assert '.fi\\n' in output\n        assert '.sp\\n' in output\n        assert ' ::= ' in output\n        assert '  ' in output\n", "    def test_nested_inline_elements(self, tmpdir):\n        app = self.app\n        app.setup(\n            self.builder,\n            self.config = Config({\n                'man_pages': default_man_pages,\n                'project': 'Test Project',\n            }),\n            builder_name='man'\n        )\n\n        source = \"\"\"", "    def test_nested_inline(self, builder_context):\n        app = builder_context['app']\n        app.config.man_show_urls = True\n        app.config.today = '2023-10-26'\n        with open('testroot/testmanpage.rst', 'w') as f:\n            f.write(\"\"\""], "sample_239": ["    def test_formset_factory_kwargs(self):\n        choices_formset = formset_factory(Choice, extra=2, can_order=True, can_delete=True, max_num=3)\n        default_attrs = {\n            'form': Choice,\n            'extra': 1,\n            'can_order': False,\n            'can_delete': False,\n            'min_num': 0,\n            'max_num': None,\n            'absolute_max': None,\n            'validate_min': False,\n            'validate_max': False,\n        }\n        self.assertDictContainsSubset(default_attrs, choices_formset.__dict__)\n        self.assertEqual(choices_formset.extra, 2)\n        self.assertEqual(choices_formset.can_order, True)\n        self.assertEqual(choices_formset.can_delete, True)\n        self.assertEqual(choices_formset.max_num, 3)\n\n\n\n", "    def test_formset_factory_with_custom_kwargs(self):\n        formset = formset_factory(CustomKwargForm, extra=1, custom_kwarg=\"foo\")\n        formset_data = {'choices-0-choice': 'A', 'choices-0-votes': 1}\n        formset = formset(formset_data)\n        self.assertEqual(formset[0].custom_kwarg, 'foo')\n\n\n\n", "    def test_formset_factory_max_num_absolute_max(self):\n        with self.assertRaises(ValueError):\n            formset_factory(Choice, max_num=10, absolute_max=5)\n\n        formset = formset_factory(Choice, max_num=5, absolute_max=10)\n        self.assertEqual(formset.max_num, 5)\n        self.assertEqual(formset.absolute_max, 10)\n", "    def test_absolute_max_validation(self):\n        with self.assertRaises(ValueError):\n            formset_factory(Choice, absolute_max=10, max_num=20)\n", "    def test_formset_factory_with_all_args(self):\n        from django.forms.widgets import HiddenInput\n        form = CustomKwargForm\n        formset = formset_factory(form, \n                                  extra=1, \n                                  can_order=True, \n                                  can_delete=True,\n                                  max_num=3, \n                                  validate_max=True,\n                                  min_num=2, \n                                  validate_min=True,\n                                  absolute_max=5, \n                                  can_delete_extra=False)\n        self.assertEqual(formset.extra, 1)\n        self.assertEqual(formset.can_order, True)\n        self.assertEqual(formset.can_delete, True)\n        self.assertEqual(formset.max_num, 3)\n        self.assertEqual(formset.validate_max, True)\n        self.assertEqual(formset.min_num, 2)\n        self.assertEqual(formset.validate_min, True)\n        self.assertEqual(formset.absolute_max, 5)\n        self.assertEqual(formset.can_delete_extra, False)\n"], "sample_411": ["    def test_app_command_output(self):\n        with captured_stderr() as stderr:\n            output = management.execute_from_command_line([\"python\", \"manage.py\", \"dance\", \"user_commands\"])\n        self.assertIn(\"user_commands\", output)\n        self.assertEqual(stderr.getvalue(), \"\")\n\n", "    def test_dance_command(self):\n        with captured_stderr() as stderr:\n            management.call_command(\"dance\")\n        self.assertIn(\"Crank up the music!\", stderr.getvalue())\n\n\n", "    def test_dance_command_handles_no_args(self):\n        with captured_stderr() as stderr:\n            management.execute_from_command_line([\"dance\"])\n        self.assertIn(\"Usage:\", stderr.getvalue())\n        self.assertIn(\"dance <message>\", stderr.getvalue())\n", "    def test_basic_app_command(self):\n        with captured_stderr() as stderr:\n            dance.handle(*[\"user_commands\"], no_color=True, stdout=StringIO(), stderr=StringIO())\n\n        self.assertIn(\"Dancing!\", stderr.getvalue())\n", "    def test_dance_command(self):\n        with captured_stderr() as stderr:\n            management.execute_from_command_line(['manage.py', 'dance'])\n\n        self.assertEqual(stderr.getvalue().strip(), \"You're a dancer! \\n\")\n"], "sample_96": ["    def test_check_list_filter_item_errors(self):\n        class MyModelAdmin(ModelAdmin):\n            list_filter = ('some_field', 'another_field')\n\n        self.assertIsInvalidRegexp(\n            MyModelAdmin(User, AdminSite()),\n            User,\n            r\"The value of 'list_filter' refers to 'some_field', which does not refer to a Field.\",\n            id='admin.E116',\n        )  \n\n        class MyListFilter(SimpleListFilter):\n            title = 'My Filter'\n            parameter_name = 'my_filter'\n\n        self.assertIsInvalidRegexp(\n            class MyModelAdmin(ModelAdmin):\n                list_filter = (MyListFilter, ),\n            User,\n            r\"The value of 'list_filter' must not inherit from 'FieldListFilter'.\",\n            id='admin.E114'\n        )\n\n        class MyIntegerFieldListFilter(FieldListFilter):\n            title = 'My Integer Filter'\n            parameter_name = 'my_integer_filter'\n\n        self.assertIsInvalid(\n            MyModelAdmin(User, AdminSite()),\n            User,\n            \"The value of 'list_filter' must inherit from 'FieldListFilter'.\",\n            id='admin.E114',\n        ) \n\n\n\n", "    def test_list_display_links_order_matters(self):\n        class TestAdmin(ModelAdmin):\n            list_display = ['id', 'name']\n            list_display_links = ['name', 'id']  \n        self.assertIsInvalid(TestAdmin, ValidationTestModel, \"The value of 'list_display_links' must contain all fields defined in 'list_display'\",\n                              id='admin.E111')\n", "    def test_list_display_filter_option_not_callable(self):\n        class MyAdmin(ModelAdmin):\n            list_display = ['title', 'year', 'genre_display']\n\n        self.assertIsInvalid(MyAdmin, ValidationTestModel,\n                             \"The value of 'list_display' refers to 'genre_display', which is not a callable.\",\n                             id='admin.E108')\n\n", "    def test_custom_filter(self):\n        class CustomFilter(SimpleListFilter):\n            title = \"Custom Filter\"\n            parameter_name = \"custom_filter\"\n\n                return [\n                    ('one', 'One'),\n                    ('two', 'Two'),\n                ]\n\n                for lookup, title in self.lookups(None, None):\n                    yield lookup, title\n\n                lookup = self.value()\n                if lookup:\n                    return queryset.filter(**{\n                        f'field__{lookup}': True,  # replace 'field' with an actual field\n                    })\n                return queryset\n\n        class CustomModelAdmin(ModelAdmin):\n            list_filter = [CustomFilter]\n\n        self.assertIsValid(CustomModelAdmin, User, admin_site=AdminSite())\n\n\n\n", "    def test_admin_actions_duplicate_names(self):\n        class DuplicateActionModelAdmin(ModelAdmin):\n            actions = [\n                ('action1', 'Action 1'),\n                ('action1', 'Action 1'),\n            ]\n        \n        self.assertIsInvalid(DuplicateActionModelAdmin(ValidationTestModel, AdminSite()),\n                             ValidationTestModel,\n                             r'Action names must be unique.',\n                             id='admin.E130')\n\n"], "sample_432": ["    def test_filter_vertical_list_display(self):\n        self.create_users()\n        request = self._mocked_authenticated_request(\"/admin/custom/user/changelist/\", self.superuser)\n        request.GET.update({TO_FIELD_VAR: \"username\"})\n        response = self.client.get(request.path, {\"username\": \"super\"})\n\n        # Check if the filter vertical list display works\n        self.assertContains(response, \"username\" * 2)\n        self.assertEqual(response.status_code, 200)\n\n\n", "    def test_list_display_links_in_child(self):\n        self.client.login(username=\"super\", password=\"xxx\")\n        response = self.client.get(reverse(\"admin:dynamic_child_list\"))\n        self.assertContains(response, \"dynamic child link\")\n", "    def test_custom_paginator(self):\n        from django.core.paginator import Paginator\n        custom_paginator = CustomPaginator()\n        with patch('django.contrib.admin.models.LogEntry.objects.filter') as m:\n            with self.subTest(paginator=custom_paginator):\n                custom_site.register(Parent, CustomPaginationAdmin)\n                response = self.client.get(reverse('admin:parent_changelist'))\n                context = response.context\n                self.assertEqual(context['paginator'], custom_paginator)\n                self.assertIsInstance(context['page_obj'], Paginator)\n\n                m.assert_called_once_with(content_type__model='parent')\n\n", "    def test_changelist_view_with_empty_dataset(self):\n        response = self.client.get(reverse(\"admin:parent_changelist\"))\n        self.assertEqual(response.status_code, 200)\n        content = response.content.decode(\"utf-8\")\n        self.assertIn('<td class=\"no-results\">', content)\n        self.assertNotIn('<a href=\"/admin/parent/1/\">', content)\n\n\n\n", "    def test_custom_paginator_with_no_initial_queryset(self):\n        # Mock the `get_queryset` method to return an empty queryset\n        with patch.object(\n            CustomPaginationAdmin, \"get_queryset\"\n        ) as mock_queryset:\n            mock_queryset.return_value = CustomIdUser.objects.none()\n\n            # Create a request for the changelist view\n            request = self._mocked_authenticated_request(\n                reverse(\"admin:custom_users_customiduser_changelist\"), self.superuser\n            )\n\n            # Render the changelist template\n            with self.mock_response():\n                response = custom_site.admin_view(\n                    CustomPaginationAdmin.changelist_view\n                )(request)\n\n            # Assert that the response contains the expected HTML\n            self.assertContains(\n                response,\n                '<tr><td colspan=\"1\" class=\"no-results\"><div class=\"no-results\">No custom users found.</div></td></tr>',\n            )\n"], "sample_781": ["    def test_classification_large_dataset(name):\n        \"\"\"Check classification on a larger dataset.\"\"\"\n        ForestClassifier = FOREST_CLASSIFIERS[name]\n\n        clf = ForestClassifier(n_estimators=100, random_state=1)\n        clf.fit(X_large, y_large)\n        assert_array_equal(clf.predict(X_large), clf.predict(X_large))\n        assert_equal(100, len(clf))\n        assert_greater(clf.score(X_large, y_large), 0.95)\n\n        # Check feature importances\n        importances = clf.feature_importances_\n        assert_greater(np.mean(importances), 0)\n        assert_less(np.sum(importances), 1.0)\n\n\n\n", "    def check_classification_iris(name):\n        \"\"\"Check classification on Iris dataset.\"\"\"\n        ForestClassifier = FOREST_CLASSIFIERS[name]\n\n        clf = ForestClassifier(n_estimators=10, random_state=1)\n        clf.fit(iris.data, iris.target)\n\n        pred = clf.predict(iris.data)\n        assert_array_equal(pred, iris.target)\n\n        # check for good test scores\n        from sklearn.metrics import accuracy_score\n        assert accuracy_score(iris.target, pred) >= 0.95\n", "    def test_criterion(self, criterion):\n        clf = ForestClassifier(n_estimators=10, criterion=criterion, random_state=1)\n        clf.fit(X, y)\n        pass\n\n", "def test_oob_score_classification(name):\n    \"\"\"Test the oob_score for classification.\"\"\"\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n\n    clf = ForestClassifier(n_estimators=10, random_state=1, oob_score=True)\n    X, y = datasets.make_classification(n_samples=100, n_features=20,\n                                        random_state=1)\n    clf.fit(X, y)\n    assert 'oob_score_' in clf.__dict__\n    assert isinstance(clf.oob_score_, float)\n    # oob_score can be slightly off due to randomness, allow for some tolerance\n    assert 0.9 <= clf.oob_score_ <= 1.0\n\n\n\n    \n", "    def test_oob_score(name):\n        ForestClassifier = FOREST_CLASSIFIERS[name]\n\n        clf = ForestClassifier(n_estimators=100, oob_score=True, random_state=1)\n        X, y = make_classification(n_samples=100, n_features=20, random_state=1)\n        clf.fit(X, y)\n        assert 'oob_score_' in clf.get_params()\n        assert isinstance(clf.oob_score_, float)\n\n        # Check if oob_score is consistent with prediction on a separate\n        # test set to verify if it is reliable\n        X_test, y_test = make_classification(n_samples=20, n_features=20,\n                                              random_state=1)\n        assert_almost_equal(clf.score(X_test, y_test), clf.oob_score_)\n\n        with pytest.raises(NotFittedError):\n            clf.oob_score_\n\n\n"], "sample_1192": ["    def test_disambiguate_dummy_symbols():\n        from sympy.core.symbol import Dummy, Symbol\n      \n        a = Dummy('a')\n        b = Dummy('a')\n        c = Dummy('b')\n        \n        new_a, new_b, new_c = disambiguate(a, b, c)\n        assert isinstance(new_a, Symbol)\n        assert isinstance(new_b, Symbol)\n        assert isinstance(new_c, Symbol)\n        assert new_a.name != a.name\n        assert new_b.name != b.name\n        assert new_c.name != c.name\n\n\n\n", "    def test_disambiguate_empty():\n        assert disambiguate() == ()\n    ", "def test_wild_properties():\n    a = Wild('a', properties=[lambda k: k.is_Integer])\n    b = Wild('b', properties=[lambda k: k.is_Real])\n    expr = 2*x**3*y*z\n    assert a.matches(expr) is None\n    assert b.matches(expr) is None \n    expr2 = 2*x**3*y*pi\n    assert a.matches(expr2) is None\n    assert b.matches(expr2) is not None\n", "    def test_disambiguate_functions():\n      from sympy.abc import x, y\n      f = Function('f')\n      g = Function('g')\n      h = Function('h')\n      expr = f(x) + g(y) + h(x)\n      result = disambiguate(expr)\n      assert len(set(result.free_symbols)) == 2\n      assert isinstance(result.func, UndefinedFunction)\n      assert isinstance(result[0][0], Symbol)\n\n\n\n", "    def test_disambiguate_nested_functions():\n        f = Function('f')\n        g = Function('g')\n        x = Symbol('x')\n\n        expr = f(g(x))\n        mapped_expr = disambiguate(expr)\n        assert mapped_expr == f(g(x))\n\n        expr = f(g(x)) + g(g(x))\n        mapped_expr = disambiguate(expr)\n        assert mapped_expr == f(g(x)) + g(g(x))\n\n        expr = f(g(g(x)))\n        mapped_expr = disambiguate(expr)\n        assert mapped_expr == f(g(g(x)))\n\n\n\n"], "sample_984": ["    def test_unevaluated_expr(self):\n        expr = UnevaluatedExpr(x + y)\n        self.assertEqual(str(expr), \"UnevaluatedExpr(x + y)\")\n        self.assertEqual(sstr(expr), \"UnevaluatedExpr(x + y)\")  \n        self.assertEqual(sstrrepr(expr), \"UnevaluatedExpr(x + y)\")  \n\n\n\n\n", "    def test_str_print_complex_root_of(self):\n        cr = RootOf(2*x**2 + 1, 0)\n        assert sstr(cr) == \"RootOf(2*x**2 + 1, 0)\"\n", "    def test_printing_special_symbols(self):\n        self.assertEqual(sstr(Catalan), 'Catalan')\n        self.assertEqual(sstr(GoldenRatio), 'GoldenRatio')\n        self.assertEqual(sstr(EulerGamma), 'EulerGamma')\n        self.assertEqual(sstr(I), 'I')\n        self.assertEqual(sstr(E), 'E')\n        self.assertEqual(sstr(pi), 'pi')\n        self.assertEqual(sstr(oo), 'oo')\n        self.assertEqual(sstr(nan), 'nan')\n        self.assertEqual(sstr(zoo), 'zoo')\n\n        self.assertEqual(sstr(Function('f')), 'f')\n        self.assertEqual(sstr(Lambda(x, x**2)), 'Lambda(x, x**2)')\n        self.assertEqual(sstr(Limit(x**2, x, 0, dir='+')), 'Limit(x**2, x, 0, dir=\\'+')')\n        self.assertEqual(sstr(Integral(x, (x, 0, 1))), 'Integral(x, (x, 0, 1))')\n        self.assertEqual(sstr(Interval(0, 1)), 'Interval(0, 1)')\n        self.assertEqual(sstr(FiniteSet(1, 2, 3)), 'FiniteSet(1, 2, 3)')\n        self.assertEqual(sstr(AccumBounds(0, 1)), 'AccumBounds(0, 1)')\n\n        self.assertEqual(sstr(Derivative(sin(x), x)), 'Derivative(sin(x), x)')\n        self.assertEqual(sstr(Factorial(x)), 'factorial(x)')\n        self.assertEqual(sstr(Factorial2(x)), 'factorial2(x)')\n        self.assertEqual(sstr(Zeta(x)), 'zeta(x)')\n        self.assertEqual(sstr(Subfactorial(x)), 'subfactorial(x)')\n\n\n\n", "    def test_print_interval(self):\n        self.assertEqual(sstr(Interval(oo, 1)), 'Interval(oo, 1)')\n        self.assertEqual(sstr(Interval(1, oo)), 'Interval(1, oo)')\n\n        self.assertEqual(sstr(Interval(1, 2)), 'Interval(1, 2)')\n        self.assertEqual(sstr(Interval(1, 2, closed=False)),\n                         'Interval(1, 2, open_right=True)')\n        self.assertEqual(sstr(Interval(1, 2, open=False)),\n                         'Interval(1, 2, open_left=True)')\n        self.assertEqual(sstr(Interval(1, 2, left_open=True, right_open=True)),\n                         'Interval(1, 2, open_left=True, open_right=True)')\n        self.assertEqual(sstr(Interval(1, 2, left_open=False, right_open=False)),\n                         'Interval(1, 2)')\n        self.assertEqual(sstr(Interval(1, 2, closed=True)),\n                         'Interval(1, 2, closed=True)')\n        self.assertEqual(sstr(Interval(1, 2, extend=True)),\n                         'Interval(1, 2, extend_left=True, extend_right=True)')\n        self.assertEqual(sstr(Interval(1, 2, extend_left=True, extend_right=False)),\n                         'Interval(1, 2, extend_right=True)')\n        self.assertEqual(sstr(Interval(1, 2, extend_left=False, extend_right=True)),\n                         'Interval(1, 2, extend_left=True)')\n\n\n\n", "    def test_WildFunction(self):\n        expr = WildFunction('f')\n        assert sstr(expr) == 'WildFunction(f)'\n\n\n"], "sample_678": ["    def test_fnmatch_ex_neg(self, match, pattern, path):\n        assert not match(pattern, path)\n", "        def test_fnmatch_ex_symlinks(self, match, match_mocker):\n            match_mocker.side_effect = lambda pattern, path: True\n            for pattern, path in zip(pattern, path):\n                assert match_ex(pattern, path)\n", "    def test_fnmatch_ex(self, match):\n        for pattern, path in zip(\n            [\"*.py\", \"test_*.py\", \"tests/*.py\", \"tests/**/test*.py\", \"tests/**/*\", \"*\"],\n            [\n                \"foo.py\",\n                \"test_foo.py\",\n                \"tests/foo.py\",\n                \"tests/foo/test_foo.py\",\n                \"tests/foo/bar\",\n                \"test_foo.py\",\n                \"tests/foo/bar\",\n                \"tests/foo/bar/doc/test_foo.py\",\n            ],\n        ):\n            assert match(pattern, path)\n", "    def test_fnmatch_ex_negative(self, pattern, path, match):\n        assert not match(pattern, path)\n\n", "    def test_fnmatch_ex_with_glob(self, match, pattern, path):\n        assert match(pattern, path)\n\n\n\n"], "sample_409": ["    def test_blocktrans_plural(self):\n        template = Template(\n            \"{% load i18n %}{% blocktranslate count=var|length %}\"\n            \"There is {{ count }} object. \"\n            \"{% plural %}There are {{ count }} objects.\"\n            \"{% endblocktranslate %}\"\n        )\n        context = Context({\"var\": 1})\n        self.assertEqual(template.render(context), \"There is 1 object. \")\n        context[\"var\"] = 2\n        self.assertEqual(template.render(context), \"There are 2 objects.\")\n", "    def test_blocktranslate_with_context(self):\n        with translation.override('en'):\n            with self.assertNumQueries(1):\n                self.assertEqual(\n                    self.render('i18n04'), 'John Doe'\n                )\n\n\n\n", "    def test_blocktranslate_count_with_asvar(self):\n        c = Context({\"count\": 2})\n        output = self.render(\"i18n04\", c)\n        self.assertEqual(output, \"2 objects\")\n", "    def test_blocktranslate_with_default(self):\n        template = Template(self.templates[\"i18n04\"])\n        context = Context({\"language\": \"en\"})\n        output = template.render(context)\n        self.assertEqual(output, \"Hello, Peter!\")\n    \n", "    def test_pluralization(self):\n        context = Context({\"anton\": \"Anton\"})\n        template = Template(\n            \"{% load i18n %}{% blocktranslate with anton=World count=5  with anton=World count=5 %}\"\n            \"There are {{ count }} Anton's. {% endblocktranslate %}\"\n        )\n        self.assertEqual(\n            template.render(context),\n            \"There are 5 Anton's.\",\n        )\n\n\n\n"], "sample_797": ["    def test_power_transform_with_sparse_data():\n        X_sparse = sparse.random(n_samples, n_features, density=0.1)\n        pt = PowerTransformer(method='yeo-johnson')\n        X_trans_sparse = pt.fit_transform(X_sparse)\n        X_trans_dense = pt.transform(X_sparse)\n\n        assert_allclose_dense_sparse(\n            X_trans_sparse, X_trans_dense)\n\n\n", "def test_robust_scale_with_zero_mean_and_unit_variance():\n    X = np.array([[-1, 0, 1], [2, -1, 0], [1, 1, 2]])\n    scaler = RobustScaler(with_mean=True, with_std=True)\n    scaler.fit(X)\n    scaled_X = scaler.transform(X)\n    assert_allclose(np.mean(scaled_X, axis=0), 0)  \n    assert_allclose(np.std(scaled_X, axis=0), 1)   \n", "    def test_power_transform_warn(self):\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            result = power_transform(X_2d, method='warn')\n        assert_equal(len(w), 1)\n        assert_warns_message(\n            \"The default value of 'method' will change from 'box-cox' to \"\n            \"'yeo-johnson' in version 0.23\", w[0])\n        assert_allclose(result, power_transform(X_2d, method='box-cox'))\n\n\n\n", "    def test_robust_scale_empty(rng):\n        X = sparse.csc_matrix((2, 2))\n        scaler = RobustScaler()\n        X_scaled = scaler.fit_transform(X)\n        assert_array_equal(X_scaled.shape, X.shape)\n        assert_allclose(X_scaled.toarray(), X.toarray())\n", "def test_power_transform_yeo_johnson_large_data(\n    random_state_seed,"], "sample_1161": ["compilation error", "    def test_print_GroebnerBasis(self):\n        basis = groebner([x**2 - y, x - y**2], x, y, order='grlex')\n        s = sstr(basis)\n        self.assertEqual(s, 'GroebnerBasis([x**2 - y, x - y**2], x, y, order=\\'grlex\\')')\n\n        basis2 = groebner([x**2 - y, x - y**2], x, y, order='lex')\n        s = sstr(basis2)\n        self.assertEqual(s, 'GroebnerBasis([x**2 - y, x - y**2], x, y, order=\\'lex\\')')\n\n        from sympy.polys.polytools import groebner\n        basis3 = groebner([x**2 - y, x - y**2], x, y, order='lex', domain=QQ)\n        s = sstr(basis3)\n        self.assertEqual(s, 'GroebnerBasis([x**2 - y, x - y**2], x, y, order=\\'lex\\', domain=\\'Rational\\')')\n\n\n\n", "    def test_print_Quaternion(self):\n        q = Quaternion(1, 2, 3, 4)\n        self.assertEqual(sstr(q), \"Quaternion(1, 2, 3, 4)\")\n", "    def test_printing_matrices(self):\n        m = Matrix([[1, 2], [3, 4]])\n        self.assertEqual(sstr(m), 'Matrix([[1, 2], [3, 4]])')  \n        self.assertEqual(sstrrepr(m), \"'Matrix'([ [1, 2], [3, 4] ])'\")\n        self.assertEqual(sstr(m.transpose()), 'Matrix([[1, 3], [2, 4]])')\n       \n        z = MatrixSymbol('z', 2, 2)\n        self.assertEqual(sstr(z), 'MatrixSymbol(\"z\", 2, 2)')\n        self.assertEqual(sstrrepr(z), \"'MatrixSymbol'('z', 2, 2)\")\n        \n        sparse = SparseMatrix([[1, 0, 0], [0, 2, 0], [0, 0, 3]])\n        self.assertEqual(sstr(sparse), 'SparseMatrix([[1, 0, 0], [0, 2, 0], [0, 0, 3]])')\n", "    def test_sstrrepr_matrices(self):\n        matrix = Matrix([[1, 2], [3, 4]])\n        repr_str = sstrrepr(matrix)\n        assert repr_str == \"Matrix([[1, 2], [3, 4]])\"\n"], "sample_139": ["    def test_action_from_url(self):\n        request = self._mocked_authenticated_request(reverse('admin:musician_changelist'), self._create_superuser('test'))\n        response = custom_site.dispatch(request, 'musician_changelist')\n        self.assertContains(response, 'href=\"{}\"'.format(reverse('admin:musician_delete', args=(3,))))\n        \n        # Test with an action that uses the _change_view\n        request.GET = {'action': '_change'}\n        response = custom_site.dispatch(request, 'musician_changelist')\n        self.assertContains(response, 'href=\"{}\"'.format(reverse('admin:musician_change', args=(3,)))) \n\n", "    def test_autocomplete_view(self):\n        cls = CustomIdUser  # Choose an appropriate model with a ForeignKey\n        client = self.client\n        response = client.get(reverse('admin:custom_iduser_autocomplete'), {'q': 'User'})\n        self.assertEqual(response.status_code, 200)\n        self.assertIn('{\"results\":', response.content.decode('utf-8'))\n", "    def test_inline_admin_has_delete_permission(self):\n        child_admin = DynamicListDisplayChildAdmin(Child, custom_site)\n        request = self._mocked_authenticated_request(reverse('admin:%s_%s_changelist' % (\n            'parent', 'parent')), self._create_superuser('testuser'))\n        obj = Parent.objects.create(name=\"Test Parent\")\n\n        # Normal user should not have permission to delete\n        child = Child.objects.create(parent=obj, name=\"Test Child\")\n        with self.assertRaises(PermissionDenied):\n            child_admin.has_delete_permission(request, child)\n\n        # Superuser should have permission\n        request = self._mocked_authenticated_request(reverse('admin:%s_%s_changelist' % (\n            'parent', 'parent')), self.superuser)\n        with self.assertLogs(level='INFO', output=f\"Deleting {child.name}\") as cm:\n            child_admin.has_delete_permission(request, child)\n        self.assertEqual(len(cm.output), 1)\n\n\n\n", "    def test_admin_changelist_with_custom_pagination(self):\n        admin.site.unregister(Parent)\n        admin.site.register(Parent, CustomPaginationAdmin)\n        self.client.login(username='super', password='xxx')\n        response = self.client.get('/admin/parent/')\n        self.assertContains(response, 'CustomPaginator')\n\n        # Check if the pagination is working as expected\n        page_number = 2\n        params = {\n            ALL_VAR: '1',\n            SEARCH_VAR: '',\n            'page': page_number,\n        }\n        response = self.client.get('/admin/parent/', params)\n        self.assertContains(response, f'page {page_number}')\n\n\n\n", "    def test_custom_paginator(self):\n        adminSite = custom_site\n        adminSite.register(Parent, ParentAdmin)\n        adminSite.register(Child, CustomPaginatorAdmin)\n        request = self._mocked_authenticated_request(\n            reverse('admin:child_changelist'), self._create_superuser('admin')\n        )\n        response = adminSite.admin_view(ChildAdmin.changelist_view)(request)\n        self.assertEqual(response.status_code, 200)\n        content = response.content.decode('utf-8')\n        self.assertIn('<div class=\"pagination\">', content)\n        self.assertIn('<span class=\"page-header\">Page 1 of 2</span>', content)\n        self.assertIn('<a href=\"?p=2\" class=\"page-number\">&nbsp;2&nbsp;</a>', content)\n\n"], "sample_1176": ["    def test_sympify_mpz():\n        assert sympify(gmpy.mpz(1)) == S.Integer(1)\n        assert sympify(gmpy.mpz(1234567890)) == S.Integer(1234567890)\n", "    def test_exp_is_pow():\n        from sympy.functions.elementary.exponential import exp\n        from sympy.core.numbers import S\n        assert isinstance(exp(S(2)), Pow)\n        assert isinstance(exp(S(1.5)), Pow)\n\n\n", "    def test_sympify_mpmath_complex():\n        z = mpmath.mpc(3, 4)\n        result = sympify(z)\n        assert result.real == 3\n        assert result.imag == 4\n", "    def test_sympify_complex():\n        a = 2 + 3j\n        b = complex(2, 3)\n        assert sympify(a) == sympify(b)\n\n\n", "    def test_sympify_complex():\n        assert sympify(complex(1, 2)) == 1 + 2*S.I\n        assert sympify(complex(-1, -2)) == -1 - 2*S.I\n        assert sympify(complex(0, 3)) == 3*S.I\n        assert sympify(complex(4, 0)) == 4\n"], "sample_764": ["    def test_remainder_estimator(self):\n        X = np.array([[1, 2], [3, 4]])\n        remainder_est = Trans()\n\n        ct = make_column_transformer(\n            [(StandardScaler(), [0]), (remainder_est, [1])],\n            remainder='passthrough')\n\n        X_trans = ct.fit_transform(X)\n\n        # Check that the remainder columns are untouched\n        assert_array_equal(X_trans[:, 1], X[:, 1])\n\n        # Check that the first column is scaled\n        scaled_values = [1 / np.sqrt(2), 3 / np.sqrt(2)]\n        assert_allclose_dense_sparse(X_trans[:, 0], np.array(scaled_values).reshape(2, 1), rtol=1e-5)\n\n\n\n", "    def test_remainder_as_estimator(self):\n        # Test remainder as an estimator\n        class RemainderEstimator(BaseEstimator):\n                return self\n\n                return X * 2\n        ct = ColumnTransformer(\n            transformers=[\n                (\"scaler\", StandardScaler(), [0]),\n                (\"remainder\", RemainderEstimator(), slice(1, None))\n            ], remainder='remainder'\n        )\n\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        X_transformed = ct.fit_transform(X)\n\n        expected_transformed = np.array([[1.41421356, 4.89897949],\n                                         [3.74165739, 8.48528138],\n                                         [5.0, 8.48528138]])\n        assert_array_equal(X_transformed, expected_transformed)\n\n", "    def test_remainder_estimator(self):\n        class Remainder(BaseEstimator):\n                return self\n\n                return X + 1\n\n        X = np.array([[1, 2], [3, 4]])\n        ct = make_column_transformer(\n            (StandardScaler(), [0]), (Remainder(), [1])\n        )\n\n        X_transformed = ct.fit_transform(X)\n        expected_X_transformed = np.array([[0, 3], [0, 5]])\n        assert_allclose_dense_sparse(X_transformed, expected_X_transformed)\n", "    def test_remainder_remainder_estimator(self):\n        X = np.array([[1], [2], [3], [4]])\n        y = np.array([1, 2, 3, 4])\n\n        class MyRemainder(BaseEstimator):\n                return self\n\n                return X**2\n\n        remainder = MyRemainder()\n        ct = ColumnTransformer(\n            transformers=[\n                ('scale', StandardScaler(), [0]),\n                ('remainder', remainder, [1])\n            ],\n            remainder=remainder\n        )\n        transformed_X = ct.fit_transform(X, y)\n\n        expected_X_scaled = np.array([[1.0], [2.0], [3.0], [4.0]])\n        expected_X_remained = np.array([[1], [4], [9], [16]])\n        expected = np.hstack((expected_X_scaled, expected_X_remained))\n\n        assert_allclose_dense_sparse(transformed_X, expected)\n\n", "    def test_remainder_estimator_fit_transform(self):\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([0, 1, 2])\n\n        remainder = Trans()\n        remainder.fit(X[:, 1].reshape(-1, 1), y)\n\n        ct = ColumnTransformer(\n            transformers=[\n                (\"standard\", StandardScaler(), [0]),\n                (\"remainder\", remainder, [1]),\n            ],\n            n_jobs=1,\n        )\n        X_transformed = ct.fit_transform(X, y)\n        assert_allclose_dense_sparse(ct.transformers_[1][1].transform(X[:, 1].reshape(-1, 1)),\n                                        remainder.transform(X[:, 1].reshape(-1, 1)))\n\n\n"], "sample_226": ["    def test_test_db_signature(self):\n        with override_settings(DATABASES={DEFAULT_DB_ALIAS: {\n            'ENGINE': 'django.db.backends.sqlite3',\n            'NAME': ':memory:',\n        }}):\n            connection.close()  # Close the default connection\n            connection.creation.create_test_db(verbosity=0, autoclobber=True)\n            connection = get_connection_copy()\n            signature = connection.creation.test_db_signature()\n            self.assertIn('sqlite3', signature)  \n", "    def test_db_signature_different_name_same_settings(self):\n        conn1 = get_connection_copy()\n        conn2 = get_connection_copy()\n        conn1.settings_dict['NAME'] = 'test_db1'\n        conn2.settings_dict['NAME'] = 'test_db2'\n\n        self.assertEqual(conn1.test_db_signature(), conn2.test_db_signature())\n", "    def test_test_db_signature(self):\n        with override_settings(DATABASES={\n            DEFAULT_DB_ALIAS: {\n                'ENGINE': 'django.db.backends.sqlite3',\n                'NAME': 'test_db.sqlite3'\n            }\n        }):\n            connection.close()\n            connection.open()  \n\n            class TestCreation(BaseDatabaseCreation):\n                    pass  # Do nothing to avoid actually creating the DB\n\n            test_creation = TestCreation(get_connection_copy())\n\n            signature1 = test_creation.test_db_signature()\n            signature2 = test_creation.test_db_signature()\n\n            self.assertEqual(signature1, signature2)\n", "    def test_db_signature(self):\n        connection = get_connection_copy()\n        connection.settings_dict['NAME'] = \"test_db\"\n        signature = connection.creation.test_db_signature()\n        self.assertEqual(signature, (\n            connection.settings_dict['HOST'],\n            connection.settings_dict['PORT'],\n            connection.settings_dict['ENGINE'],\n            \"test_db\",\n        ))\n", "    def test_db_signature(self):\n        connection1 = get_connection_copy()\n        connection1.settings_dict['NAME'] = 'testdb1'\n        signature1 = connection1.creation.test_db_signature()\n\n        connection2 = get_connection_copy()\n        connection2.settings_dict['NAME'] = 'testdb2'\n        signature2 = connection2.creation.test_db_signature()\n\n        self.assertNotEqual(signature1, signature2)\n"], "sample_787": ["    def test_hamming_loss_multilabel(self, sample_weight):\n        y_true = np.array([[0, 1], [1, 1], [0, 0], [1, 0]])\n        y_pred = np.array([[0, 0], [1, 1], [0, 1], [1, 0]])\n        loss = hamming_loss(y_true, y_pred, sample_weight=sample_weight)\n        self.assertAlmostEqual(loss, 0.5)\n", "compilation error", "compilation error", "    def test_zero_one_loss_binary(self):\n        y_true = np.array([0, 1, 1, 0])\n        y_pred = np.array([0, 1, 0, 1])\n\n        loss = zero_one_loss(y_true, y_pred)\n        self.assertAlmostEqual(loss, 0.5)\n", "def test_hinge_loss_multiclass():\n    # Test hinge loss with multi-class labels and no provided labels argument\n\n    X = np.array([[0], [1], [2], [3]])\n    Y = np.array([0, 1, 2, 3])\n    labels = np.array([0, 1, 2, 3])\n\n    est = svm.LinearSVC()\n    est.fit(X, Y)\n    pred_decision = est.decision_function([[-1], [2], [3]])\n\n    #Test with no provided labels\n    with pytest.warns(UndefinedMetricWarning):\n        hinge_loss(Y, pred_decision)\n"], "sample_597": ["    def test_merge_identical_attrs(self):\n        x = xr.Dataset(\n            {\n                \"var1\": ((\"x\", \"y\"), np.arange(4).reshape(2, 2)),\n                \"var2\": ((\"x\", \"y\"), np.arange(4).reshape(2, 2) + 1),\n            },\n            attrs={\"a\": 1, \"b\": 2, \"c\": 3},\n        )\n        y = xr.Dataset(\n            {\n                \"var1\": ((\"x\", \"y\"), np.arange(4).reshape(2, 2)),\n                \"var2\": ((\"x\", \"y\"), np.arange(4).reshape(2, 2) + 1),\n            },\n            attrs={\"a\": 1, \"b\": 2, \"c\": 3},\n        )\n        merged = merge([x, y], compat=\"identical\")\n        assert merged.attrs == {\"a\": 1, \"b\": 2, \"c\": 3}\n", "    def test_merge_with_duplicates(self):\n        x = create_test_data(\n            \"x\", dims=(\"lat\", \"lon\"), data=np.arange(4).reshape((2, 2))\n        )\n        y = create_test_data(\n            \"y\", dims=(\"lat\", \"lon\"), data=np.arange(4).reshape((2, 2))\n        )\n        y_coord = create_test_data(\"y_coord\", dims=(\"lat\",), data=np.arange(2))\n        with pytest.raises(MergeError) as exc:\n            merge([x, y, y_coord], compat=\"identical\")\n        assert \"cannot be merged\" in str(exc.value)\n", "    def test_merge_identical_with_missing_dims(self):\n        x = xr.DataArray(\n            np.arange(4).reshape((2, 2)),\n            dims=(\"lat\", \"lon\"),\n            coords={\"lat\": [0, 1], \"lon\": [0, 1]},\n            name=\"var1\",\n        )\n        y = xr.DataArray(\n            np.arange(4).reshape((2, 2)),\n            dims=(\"time\", \"lon\"),\n            coords={\"time\": [0, 1], \"lon\": [0, 1]},\n            name=\"var2\",\n        )\n        result = merge([x, y], compat=\"identical\")\n        assert result.dims == {\"lat\": 2, \"lon\": 2, \"time\": 2}\n        assert (result[\"var1\"].values == np.arange(4).reshape((2, 2))).all()\n        assert (result[\"var2\"].values == np.arange(4).reshape((2, 2))).all()\n\n\n", "    def test_merge_with_invalid_compat(self, compat_level):\n        x = xr.DataArray(np.random.rand(2, 3), coords=[[\"a\", \"b\"], [\"c\", \"d\", \"e\"]], name=\"x\")\n        y = xr.DataArray(np.random.rand(1, 1), coords=[[\"f\"], [\"g\"]], name=\"y\")\n\n        with raises_regex(MergeError, r\"Invalid compat_level\"):\n            merge([x, y], compat=compat_level)\n\n", "    def test_merge_compat_override(self):\n        x = create_test_data()\n        y = create_test_data(\n            name=\"var2\", replace_data=[\n                [1, 2],\n                [3, 4],\n            ],\n        )\n\n        with raises_regex(ValueError):\n            merge([x, y], compat=\"identical\")\n\n        result = merge([x, y], compat=\"override\")\n        assert_identical(result.var1, x.var1)\n        assert_identical(result.var2, y.var2)\n"], "sample_642": ["    def test_preprocess_options_long_option_with_value(\n        capsys: CaptureFixture,", "    def test_preprocess_options_with_value(\n        pop_pylintrc: None, capsys: CaptureFixture,", "    def test_preprocess_options_takes_value(\n        fake_home: str,\n        caplog: CaptureFixture,", "    def test_preprocess_options_with_default_value(\n        mock_run: Run,\n        monkeypatch: mock.MonkeyPatch", "    def test_preprocess_options_full_match(\n        caplog: CaptureFixture,\n        run: Run,"], "sample_261": ["    def test_parse_datetime_with_offset(self):\n        dt = parse_datetime('2023-10-26T12:34:56+05:30')\n        self.assertEqual(dt.year, 2023)\n        self.assertEqual(dt.month, 10)\n        self.assertEqual(dt.day, 26)\n        self.assertEqual(dt.hour, 12)\n        self.assertEqual(dt.minute, 34)\n        self.assertEqual(dt.second, 56)\n        self.assertEqual(dt.microsecond, 0)\n        self.assertEqual(dt.tzinfo.utcoffset(dt), timedelta(hours=5, minutes=30))\n\n", "    def test_parse_datetime_with_fixed_timezone(self):\n        value = '2023-10-26T14:30:00+05:30'\n        dt = parse_datetime(value)\n        self.assertEqual(dt.year, 2023)\n        self.assertEqual(dt.month, 10)\n        self.assertEqual(dt.day, 26)\n        self.assertEqual(dt.hour, 14)\n        self.assertEqual(dt.minute, 30)\n        self.assertEqual(dt.second, 0)\n        self.assertEqual(dt.microsecond, 0)\n        self.assertEqual(dt.tzinfo, get_fixed_timezone(19800)) \n", "    def test_parse_datetime_with_postgres_interval(self):\n        datetime_string = '3 days 04:05:06'\n        dt = parse_datetime(datetime_string)\n        self.assertEqual(dt.date(), date(2023, 1, 1) + timedelta(days=3))\n        self.assertEqual(dt.time(), time(4, 5, 6))\n\n", "    def test_parse_datetime_with_timezone(self):\n        dt = parse_datetime(\"2023-10-27T10:00:00+05:00\")\n        self.assertEqual(dt.date(), date(2023, 10, 27))\n        self.assertEqual(dt.time(), time(10, 0, 0))\n        self.assertEqual(dt.tzinfo, get_fixed_timezone(18000))\n\n        dt = parse_datetime(\"2023-10-27T10:00:00-08:00\")\n        self.assertEqual(dt.date(), date(2023, 10, 27))\n        self.assertEqual(dt.time(), time(10, 0, 0))\n        self.assertEqual(dt.tzinfo, get_fixed_timezone(-48000))\n\n        dt = parse_datetime(\"2023-10-27T10:00:00+00:00\")\n        self.assertEqual(dt.date(), date(2023, 10, 27))\n        self.assertEqual(dt.time(), time(10, 0, 0))\n        self.assertEqual(dt.tzinfo, utc)\n", "    def test_parse_datetime_with_postgres_interval(self):\n        dt = parse_datetime('3 days 04:05:06')\n        self.assertEqual(dt.date(), date(2023, 10, 27))\n        self.assertEqual(dt.time(), time(4, 5, 6))\n        \n        dt = parse_datetime('-3 days 04:05:06')\n        self.assertEqual(dt.date(), date(2023, 10, 24))\n        self.assertEqual(dt.time(), time(4, 5, 6))\n\n\n\n        dt = parse_datetime('3 days 04:05:06.123456')\n        self.assertEqual(dt.date(), date(2023, 10, 27))\n        self.assertEqual(dt.time(), time(4, 5, 6))\n"], "sample_1182": ["    def test_sympy_printer(self):\n        p = SymPyPrinter()\n        self.assertEqual(p.doprint(x + y), 'x + y')\n\n", "    def test_Piecewise(self):\n        expr = Piecewise(\n            (x**2, x > 0),\n            (0, x <= 0)\n        )\n        self.assertEqual(pycode(expr), '(x**2 if x > 0 else 0)')\n        expr = Piecewise(\n            (x**2, x > 0),\n            (x, x <= 0)\n        )\n        self.assertEqual(pycode(expr), '(x**2 if x > 0 else x)')\n\n\n", "    def test_sympy_printing(self):\n        printer = SymPyPrinter()\n        self.assertEqual(printer.doprint(Mod(x, 2)), 'x%2')\n        self.assertEqual(printer.doprint(KroneckerDelta(x, y)), '(x == y)')\n        self.assertEqual(printer.doprint(Piecewise((x, x > 0), (y, True))), 'if x > 0 else y')\n        self.assertEqual(printer.doprint(sign(x)), 'sympy.sign(x)')\n        self.assertEqual(printer.doprint(sqrt(x)), 'sympy.sqrt(x)')\n", "    def test_MatrixSolve(self):\n        A = MatrixSymbol('A', 2, 2)\n        b = MatrixSymbol('b', 2, 1)\n        x = MatrixSolve(A, b)\n        code = SymPyPrinter().doprint(x)\n        assert code == 'sympy.solve(A, b)'\n\n\n\n", "    def test_matrix_solve(self):\n        m = MatrixSymbol('m', 3, 3)\n        b = MatrixSymbol('b', 3, 1)\n        A = MatrixSolve(m, b)\n        code = SymPyPrinter().doprint(A)\n        assert 'sympy.solveset' in code\n\n\n\n"], "sample_1056": ["compilation error", "    def test_numexpr_function_support(self):\n        f = lambdarepr(sin(x))\n        assert f == \"evaluate('sin(x)', truediv=True)\"", "    def test_numexpr_matrix():\n        raises(TypeError, lambda: NumExprPrinter().doprint(Matrix([[1,2],[3,4]])))\n", "    def test_numexpr_printing():\n        expr = sin(x)\n        result = lambdarepr(expr, printer=NumExprPrinter)\n        assert result == \"evaluate('sin(x)', truediv=True)\"\n\n        expr = sqrt(x**2 + y**2)\n        result = lambdarepr(expr, printer=NumExprPrinter)\n        assert result == \"evaluate('sqrt(x**2 + y**2)', truediv=True)\"\n\n        expr = Piecewise((x, (x > 0)), (y, (x <= 0)))\n        with raises(TypeError):\n            lambdarepr(expr, printer=NumExprPrinter) \n", "    def test_numexpr_complex():\n        expr = (x + 1j) * (y - 2j)\n        result = lambdarepr(expr)\n        assert result == \"evaluate('(x + 1j) * (y - 2j)', truediv=True)\"\n"], "sample_696": ["    def test_node_fspath(pytest, pytester):\n        from _pytest.nodes import Node\n\n        # Create a test node\n        node = Node(name=\"test_function\", fspath=\"test_file.py\")\n\n        # Trigger the deprecation warning\n        with pytest.warns(deprecated.NODE_FSPATH):\n            node.fspath\n\n        # Assert the warning message is as expected\n        with pytest.raises(deprecated.NODE_FSPATH) as excinfo:\n            node.fspath\n\n        assert (\n            str(excinfo.value)\n            == f\"{Node.__class__.__name__}.fspath is deprecated and will be replaced by {Node.__class__.__name__}.path.\\nsee https://docs.pytest.org/en/latest/deprecations.html#node-fspath-in-favor-of-pathlib-and-node-path\"\n        )\n", "    def test_pytest_collect_module_warning(pytester):\n        pytest.deprecated.PYTEST_COLLECT_MODULE_WARNING = deprecated.PYTEST_COLLECT_MODULE\n        pytester.runpytest(\"--collect-only\", \"-k\", \"test_module_deprecated\", warnings=\"error\")\n        assert (\n            \"pytest.collect.test_module_deprecated was moved to pytest.test_module_deprecated\"\n            in pytester.getoutput()\n        )\n\n\n\n", "def test_deprecated_warning_captured_hook(pytester: Pytester):\n    pytester.runpytest_ini(\n        \"\"\"\n        [pytest]\n        capture_warning_recorded = pytest_warning_captured\n        \"\"\"\n    )\n    # Test the deprecation warning for pytest_warning_captured hook\n\n    result = pytester.runpytest(\n        \"-s\", \"--capture=no\", \"test_deprecations.py\"\n    )\n    result.stdout.fnmatch_lines(\n        [\"_pytest_warning_captured is deprecated\", \"pytest_warning_recorded should be used instead.\"]\n    )\n", "    def test_deprecated_collect_module(pytester: Pytester):\n        pytester.inline_run(\n            f\"import pytest\\n\"\n            f\"pytest.collect.{attribute}().collect()\",\n            \"--strict-markers\",\n            capture=True,\n        )\n        re.search(\n            r\"pytest\\.collect\\.{attribute} was moved to pytest\\.{attribute}\\n\"\n            r\"Please update to the new name\",\n            pytester.lines,\n        ) \n", "compilation error"], "sample_295": ["    def test_window_function_order(self):\n        with self.assertNumQueries(1):\n            result = Company.objects.annotate(\n                rank=Rank()\n            ).order_by('rank')\n        self.assertEqual(result.values_list('rank', flat=True).order_by('rank'), list(range(1, 4)))\n", "    def test_window_expressions(self):\n        with self.subTest(\"Simple Window\"):\n            result = self.company_query.annotate(\n                avg_employees=Avg(\"num_employees\")\n            ).order_by(\n                \"name\", \"avg_employees\"\n            )[0]\n            self.assertEqual(result[\"avg_employees\"], 2300)\n\n        with self.subTest(\"Partitioning\"):\n            result = self.company_query.annotate(\n                avg_employees=Avg(\"num_employees\")\n            ).order_by(\"name\").values(\"name\", \"avg_employees\")\n            self.assertEqual(list(result), [\n                {\"name\": \"Example Inc.\", \"avg_employees\": 2300},\n                {\"name\": \"Foobar Ltd.\", \"avg_employees\": 3},\n                {\"name\": \"Test GmbH\", \"avg_employees\": 32},\n            ])\n\n        with self.subTest(\"Order by\"):\n            result = self.company_query.annotate(\n                avg_employees=Avg(\"num_employees\")\n            ).order_by(\n                \"name\", \"avg_employees\"\n            ).values(\"name\", \"avg_employees\")\n            self.assertEqual(list(result), [\n                {\"name\": \"Example Inc.\", \"avg_employees\": 2300},\n                {\"name\": \"Foobar Ltd.\", \"avg_employees\": 3},\n                {\"name\": \"Test GmbH\", \"avg_employees\": 32}\n            ])\n", "    def test_case_expression(self):\n        # Test Case expressions with multiple WHEN and DEFAULT\n        result = self.company_query.annotate(\n            chair_status=Case(\n                When(num_chairs__lt=10, then='Few chairs'),\n                When(num_chairs__gte=10, then='Enough chairs'),\n                default='Many chairs',\n            )\n        ).all()\n        self.assertEqual(result[0].chair_status, 'Few chairs')\n        self.assertEqual(result[1].chair_status, 'Enough chairs')\n        self.assertEqual(result[2].chair_status, 'Many chairs')\n\n        # Test Case expressions with nested conditions\n        result = self.company_query.annotate(\n            employee_status=Case(\n                When(num_employees__lt=100, then=Case(\n                    When(num_employees__lt=50, then='Small'),\n                    default='Medium'\n                )),\n                When(num_employees__gte=100, then='Large'),\n                default='Giant'\n            )\n        ).all()\n        self.assertEqual(result[0].employee_status, 'Small')\n        self.assertEqual(result[1].employee_status, 'Medium')\n        self.assertEqual(result[2].employee_status, 'Large')\n\n", "    def test_window_ordering(self):\n        with self.assertNumQueries(1):\n            results = Company.objects.annotate(\n                avg_salary=Avg('employees__salary'),\n            ).window(\n                partition_by=['name'],\n                order_by=OrderBy(\n                    'num_employees', descending=True\n                ),\n                frame=RowRange(end=1)\n            ).values('name', 'avg_salary')  \n        self.assertEqual(len(results), 2)\n        self.assertEqual(results[0]['name'], 'Example Inc.')\n        self.assertEqual(results[0]['avg_salary'], 10.0)\n        self.assertEqual(results[1]['name'], 'Foobar Ltd.')\n        self.assertEqual(results[1]['avg_salary'], 20.0)\n\n\n\n", "    def test_when_expression(self):\n        query = Employee.objects.annotate(\n            high_earner=Case(\n                When(salary__gt=20, then=True),\n                default=False,\n            )\n        ).filter(high_earner=True)\n        self.assertQuerysetEqual(\n            query,\n            ['<Employee: Max Mustermann>'],\n            lambda obj: str(obj.pk),\n        )\n"], "sample_399": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1164": ["def test_wigner_simplification():\n    a = symbols('a')\n    alpha = symbols('alpha')\n    b = symbols('b')\n    c = symbols('c')\n    \n    sT(CG(a, alpha, b, 0, a, alpha), 'C^{a, alpha}_{b, 0, a, alpha}')\n    sT(CG(a, alpha, b, 0, a, alpha), 'C^{a, alpha}_{b, 0, a, alpha}') \n    sT(CG(a, alpha, 1, 0, a, alpha), 'C^{a, alpha}_{1, 0, a, alpha}') \n    sT(Wigner3j(a, b, c, alpha, beta, gamma), 'Wigner3j(a, b, c, alpha, beta, gamma)')\n    sT(Wigner6j(a, b, c, d, e, f), 'Wigner6j(a, b, c, d, e, f)')\n    sT(Wigner9j(a, b, c, d, e, f, g, h, i), 'Wigner9j(a, b, c, d, e, f, g, h, i)') \n\n    \n\n\n\n", "def test_wigner_simplification():\n    sT(cg_simp(CG(1/2, 1/2, 1/2, 1/2, 1, 1/2)),\n       '3 * CG(1/2, 1/2, 1/2, 1/2, 1, 1/2)')\n    sT(cg_simp(CG(1/2, -1/2, 1/2, 1/2, 1, 1/2) + CG(1/2, 1/2, 1/2, -1/2, 1, 1/2) + CG(1/2, 1/2, -1/2, 1/2, 1, 1/2)),\n       '3 * CG(1/2, 1/2, 1/2, 1/2, 1, 1/2)')\n\n", "compilation error", "def test_cg_simp_add():\n    a = CG(1,1,0,0,1,1)\n    b = CG(1,0,0,0,1,0)\n    c = CG(1,-1,0,0,1,-1)\n    s = a + b + c\n    s_simp = cg_simp(s)\n    assert s_simp == 3\n\n", "compilation error"], "sample_571": ["    def test_regplot_lowess(self):\n        with plt.plotting.plt_switch_context(\"agg\"):\n            ax = lm.regplot(x=\"x\", y=\"y_na\", data=self.df, lowess=True,\n                            scatter=True)\n        npt.assert_allclose(ax.lines[0].get_ydata(), ax.lines[1].get_ydata(), atol=0.1)\n\n        ax = lm.regplot(x=\"x\", y=\"z\", data=self.df, lowess=True,\n                        scatter=True)\n        npt.assert_allclose(ax.lines[0].get_ydata(), ax.lines[1].get_ydata(), atol=0.1)\n\n\n", "    def test_residplot_dropna(self, dropna):\n        if dropna:\n            df_dropna = self.df.copy()\n            df_dropna = df_dropna.dropna()\n        else:\n            df_dropna = self.df.copy()\n\n        ax = lm.residplot(data=df_dropna, x=\"x\", y=\"y_na\", dropna=dropna)\n        npt.assert_array_almost_equal(df_dropna[\"y_na\"].values,\n                                      df_dropna.y_na.values)\n        if dropna:\n            assert len(df_dropna.dropna().y_na.unique())\n            npt.assert_array_almost_equal(ax.scatter.get_offsets()[0][0],\n                                          df_dropna[\"x\"].values[0])\n", "    def test_robust_regression(self):\n        ax = lm.regplot(x=\"x\", y=\"y\", data=self.df, robust=True)\n        resids = self.df.y - lm.simple_linear_model(self.df.x, self.df.y)\n        npt.assert_allclose(resids, ax.lines[1].get_ydata(), atol=1e-3)\n        \n", "    def test_partial_regression(self):\n        x_partial = lm._RegressionPlotter._partial_regression(self.df, x=\"x\", y=\"y\",\n                                                             x_partial=\"d\")\n        y_partial = lm._RegressionPlotter._partial_regression(self.df, x=\"x\", y=\"y\",\n                                                             y_partial=\"d\")\n        assert np.allclose(x_partial, lm._RegressionPlotter._partial_regression(self.df, x=\"x\", y=\"y\"))\n        assert np.allclose(y_partial, lm._RegressionPlotter._partial_regression(self.df, x=\"x\", y=\"y\"))", "    def test_lowess(self):\n        ax = lm._RegressionPlotter.plot_residuals(y=self.df.y,\n                                                  x=self.df.x,\n                                                  ax=plt.gca(),\n                                                  lowess=True,\n                                                  )\n        # Check if a lowess fit was added to the plot\n        lines = ax.lines\n        assert len(lines) == 2\n        assert lines[1].get_linestyle() == 'dashed'\n        plt.close(ax.figure)\n\n\n\n"], "sample_1191": ["compilation error", "compilation error", "compilation error", "    def test_hermite_normal_form_rank_check(self):\n        m = DomainMatrix([[1, 2], [3, 4]], (2, 2), ZZ)\n        with raises(DMShapeError):\n            hermite_normal_form(m, D=10, check_rank=True)\n\n", "compilation error"], "sample_424": ["    def test_alter_together_model_options(self):\n        with transaction.atomic():\n            models.Model.objects.create(name=\"Test\")\n            model_name = 'Test'\n            state = ProjectState()\n            state.models[self.app_label, model_name] = ModelState(\n                model=models.Model,\n                fields=[models.Field(name=\"name\")],\n                apps=self.apps,\n                options={\n                    'unique_together': (\n                        ('name',),\n                    )\n                }\n            )\n\n            # Add a new field to the model\n            operation = AlterModelOptions(\n                name=model_name,\n                options={\n                    'unique_together': (\n                        ('name', 'id'),\n                    )\n                },\n            )\n            operation.state_forwards(self.app_label, state)\n\n            # Check if the model options were updated\n            self.assertEqual(state.models[self.app_label, model_name].options[\n                'unique_together'\n            ], [('name', 'id')])\n\n            # Try to run the database forwards\n            with CaptureQueriesContext(connection) as ctx:\n                operation.database_forwards(\n                    self.app_label, self.schema_editor, state,\n                    state,\n                )\n            self.assertEqual(len(ctx.captured_queries), 0)\n\n            # Try to run the database backwards\n            with CaptureQueriesContext(connection) as ctx:\n                operation.database_backwards(\n                    self.app_label, self.schema_editor, state,\n                    state,\n                )\n            self.assertEqual(len(ctx.captured_queries), 0)\n", "    def test_add_remove_constraint(self):\n        \"\"\"\n        Tests adding and removing a constraint on a model.\n        \"\"\"\n        with atomic(using=self.connection.alias):\n            # Initial state\n            state = ProjectState(\n                apps=self.apps,\n                db=self.connection.alias,\n            )\n            state.apps.register_model(\n                'myapp', 'UnicodeModel', UnicodeModel\n            )\n\n            # Define constraint\n            constraint = models.UniqueConstraint(\n                fields=['name'],\n                name='unique_name',\n            )\n            operation = AddConstraint('myapp', constraint)\n            operation.state_forwards(operation.app_label, state)\n            with CaptureQueriesContext(self.connection) as cm:\n                operation.database_forwards(\n                    operation.app_label,\n                    self.schema_editor,\n                    None,\n                    state,\n                )\n            self.assertEqual(len(cm.captured_queries), 1)\n            self.assertIn(\n                \"CREATE UNIQUE INDEX\",\n                cm.captured_queries[0],\n            )\n            # Verify constraint in state\n            self.assertEqual(\n                state.models['myapp', 'UnicodeModel'].constraints,\n                ['unique_name'],\n            )\n            # Remove constraint\n            operation = RemoveConstraint('myapp', 'unique_name')\n            operation.state_forwards(operation.app_label, state)\n            with CaptureQueriesContext(self.connection) as cm:\n                operation.database_forwards(\n                    operation.app_label,\n                    self.schema_editor,\n                    None,\n                    state,\n                )\n            self.assertEqual(len(cm.captured_queries), 1)\n            self.assertIn(\n                \"DROP INDEX\",\n                cm.captured_queries[0],\n            )\n            # Verify constraint is removed\n            self.assertEqual(\n                state.models['myapp', 'UnicodeModel'].constraints,\n                [],\n            )\n", "    def test_rename_model_options(self):\n        with atomic():\n            with self.assertNumQueries(1):\n                self.model_state.create_field(UnicodeModel, \"new_field\", models.CharField(max_length=255))\n            self.assertEqual(self.model_state.get_field_by_name(UnicodeModel, \"new_field\").field.max_length, 255)\n            \n        operation = AlterModelOptions(\n            name=UnicodeModel._meta.model_name,\n            options={\"ordering\": [\"id\", \"new_field\"]},\n        )\n        self.apply_operation(operation)\n\n        with atomic():\n            with self.assertNumQueries(0):\n                self.assertEqual(UnicodeModel._meta.ordering, [\"id\", \"new_field\"])\n\n", "    def test_rename_index_to_unnamed_index(self):\n        with atomic(using=self.connection.alias):\n            state = self.create_project_state()\n            from_model = self.create_model(state, \"UnicodeModel\", fields=[\n                models.CharField(max_length=255, name=\"name\"),\n            ])\n\n            # Create the index.\n            state.add_index(\n                \"myapp\",\n                from_model.model_name_lower,\n                models.Index(\n                    fields=[\"name\"], name=\"name_idx\"\n                ),\n            )\n            self.app_state = state\n            \n            # Rename the index with no new name specified.\n            operation = RenameIndex(\n                model_name=\"UnicodeModel\", old_name=\"name_idx\"\n            )\n            operation.database_forwards(\n                self.connection.alias, self.execute_operations, state, state\n            )\n            self.assertEqual(\n                len(state.models[\"myapp\", \"unicodemodel\"].get_indexes()), 1\n            )\n            self.assertEqual(\n                state.models[\"myapp\", \"unicodemodel\"].get_index_by_name(\n                    None\n                ).columns, [\"name\"]\n            )\n\n            # Rename it back.\n            operation = RenameIndex(\n                model_name=\"UnicodeModel\", old_name=None,\n            )\n            operation.database_backwards(\n                self.connection.alias, self.execute_operations, state, state\n            )\n            self.assertEqual(\n                len(state.models[\"myapp\", \"unicodemodel\"].get_indexes()), 1\n            )\n            self.assertEqual(\n                state.models[\"myapp\", \"unicodemodel\"].get_index_by_name(\n                    \"name_idx\"\n                ).columns, [\"name\"]\n            )\n\n", "    def test_rename_model_in_migration(self):\n        with atomic():\n            Food.objects.create(name=\"Apple\", price=1)\n            transaction.commit()\n\n        state = ProjectState(app_labels=[\"tests\"])\n            \n        with self.assertRaises(IntegrityError):\n            with self.assertRaises(FieldDoesNotExist):\n                state.add_migration(\n                    Migration(\n                        \"tests\",\n                        \"0001_initial\",\n                        [\n                            RenameModel(\n                                \"tests.Food\",\n                                \"tests.NewFood\",\n                                {},\n                            ),\n                        ],\n                    )\n                )\n\n        state.apply_migration(\n            \"tests\", \"0001_initial\", using=self.connection\n        )\n        self.assertEqual(\n            Foo.objects.count(), 1, \"Renamed object count should be 1\"\n        )\n"], "sample_14": ["    def test_wrap_at_inplace(self):\n        a = Angle([120, 180, 270] * u.deg)\n        a.wrap_at(360 * u.deg, inplace=True)\n        assert_allclose(a.to_value(u.deg), [120, 180, 270])\n\n\n        a = Angle([120, 180, 270] * u.deg)\n        a.wrap_at(180 * u.deg, inplace=True)\n        assert_allclose(a.to_value(u.deg), [120, 180, 270]) \n\n\n\n\n", "    def test_wrap_at_bounds(self):\n        a = Angle(180 * u.deg)\n        a.wrap_at(360 * u.deg)\n        assert_allclose(a.degree, 180 * np.array([0]))\n        a = Angle(360 * u.deg)\n        a.wrap_at(360 * u.deg)\n        assert_allclose(a.degree, 0)\n        a = Angle(450 * u.deg)\n        a.wrap_at(360 * u.deg)\n        assert_allclose(a.degree, 90)\n        a = Angle(-450 * u.deg)\n        a.wrap_at(360 * u.deg)\n        assert_allclose(a.degree, -90)\n", "    def test_parsing_degrees(self, angle, expected):\n        a = Angle(angle)\n        assert_allclose(a.to(u.deg), expected)\n", "    def test_parse_decimal_with_format_precision(self):\n        angle = Angle('123.456789d', format='latex')\n        assert angle.to_string() == '123.456789d'\n", "    def test_wrap_at_inplace(self):\n        a = Angle([360 * u.deg, -30 * u.deg, 120 * u.deg])\n        b = a.copy()\n        a.wrap_at(0 * u.deg, inplace=True)\n\n        assert_allclose(a.degree, [0 * u.deg, 330 * u.deg, 120 * u.deg])\n        assert_array_equal(b.degree, [360 * u.deg, -30 * u.deg, 120 * u.deg])\n"], "sample_89": ["    def test_common_roots(self):\n        p1 = Path('/home/user/project/app1')\n        p2 = Path('/home/user/project/app2')\n        p3 = Path('/home/user/project/static')\n        p4 = Path('/home/user/project/tests')\n\n        all_paths = [p1, p2, p3, p4]\n        expected_roots = frozenset([\n            Path('/home/user/project'),\n        ])\n\n        self.assertEqual(autoreload.common_roots(all_paths), expected_roots)\n\n        all_paths = [p1, p2, p3]\n        expected_roots = frozenset([\n            Path('/home/user/project'),\n        ])\n\n        self.assertEqual(autoreload.common_roots(all_paths), expected_roots)\n\n\n", "    def test_iter_modules_and_files_zipfile(self):\n        with zipfile.ZipFile(self.temporary_file('test_module.zip'), 'w') as zf:\n            zf.write('__init__.py', arcname='__init__.py')\n            zf.write('module_file.py', arcname='module_file.py')\n\n        zip_path = Path(self.temporary_file('test_module.zip')).resolve()\n        self.assertFileFound(zip_path / '__init__.py')\n        self.assertFileFound(zip_path / 'module_file.py')\n\n        # Try to import from the zip file\n        self.import_and_cleanup('test_module')\n        self.assertFileFound(zip_path / '__init__.py')\n        self.assertFileFound(zip_path / 'module_file.py')\n", "    def test_iter_all_python_module_files_with_zipfile(self):\n        with tempfile.TemporaryDirectory() as tempdir:\n            zip_filename = self.temporary_file('my_module.zip')\n            with zipfile.ZipFile(zip_filename, 'w') as zipf:\n                zipf.write('__init__.py', arcname='my_module/')\n            sys.path.insert(0, tempdir)\n            self.import_and_cleanup('my_module')\n            self.assertFileFound(zip_filename / '__init__.py')\n            sys.path.pop(0)\n            self.assertFileFound(zip_filename / '__init__.py')\n\n\n\n", "    def test_error_files_tracking(self):\n        with tempfile.TemporaryDirectory() as tmpdir:\n            temp_file = Path(tmpdir) / 'error_file.py'\n            try:\n                with open(temp_file, 'w') as f:\n                    f.write('raise Exception(\"This file has an error\")')\n                py_compile.compile(str(temp_file))\n                self.import_and_cleanup('error_file')\n                self.assertFileFound(temp_file)\n                # Trigger an exception\n                autoreload.raise_last_exception()\n                # Verify the file is in the error_files\n                self.assertIn(temp_file, autoreload._error_files)\n            finally:\n                os.remove(temp_file)\n", "    def test_iter_all_python_module_files_with_zipfile(self):\n        with zipfile.ZipFile(self.temporary_file('test_module.zip'), 'w') as zipf:\n            zipf.write('test_module.py', 'test_module.py')\n        self.import_and_cleanup('test_module')\n        self.assertFileFound(Path('test_module.zip'))\n"], "sample_479": ["    def test_rename_index_in_order(self):\n        operations = [\n            AddIndex(\"UnicodeModel\", models.Index(fields=[\"name\"], name=\"name_idx\")),\n            RenameIndex(\"UnicodeModel\", \"name_idx\", \"new_name_idx\"),\n            RemoveIndex(\"UnicodeModel\", \"new_name_idx\"),\n        ]\n        self.assertOptimizesTo(\n            operations,\n            [\n                RenameIndex(\"UnicodeModel\", \"name_idx\", \"new_name_idx\"),\n                RemoveIndex(\"UnicodeModel\", \"new_name_idx\"),\n                AddIndex(\"UnicodeModel\", models.Index(fields=[\"name\"], name=\"new_name_idx\")),\n            ],\n        )\n", "    def test_rename_index_with_existing_rename(self):\n        from django.db.migrations.operations import RenameIndex\n\n        operations = [\n            RenameIndex(\n                \"unicode_model\", \"old_name\", \"new_name\"\n            ),\n            RenameIndex(\n                \"unicode_model\", \"new_name\", \"final_name\"\n            ),\n        ]\n        # Expect the 2 renames to be merged\n        self.assertOptimizesTo(\n            operations,\n            [RenameIndex(\"unicode_model\", \"old_name\", \"final_name\")],\n        )\n\n\n\n\n", "    def test_combined_operations_removal(self):\n        operations = [\n            AddIndex(\"models.UnicodeModel\", models.Index(fields=[\"name\"], name=\"uniname\")),\n            RemoveIndex(\"models.UnicodeModel\", \"uniname\"),\n        ]\n        self.assertOptimizesTo(operations, [], exact=0, app_label=\"migrations\")\n\n\n", "    def test_rename_index_optimization(self):\n        operations = [\n            RenameIndex(\n                \"UnicodeModel\", \"new_index\", \"old_index\", [\"name\", \"age\"]\n            ),\n            RenameIndex(\n                \"UnicodeModel\", \"new_index_2\", \"old_index_2\", [\"age\"]\n            ),\n        ]\n        expected = [\n            RenameIndex(\n                \"UnicodeModel\", \"new_index_2\", \"old_index_2\", [\"age\"]\n            ),\n            RenameIndex(\n                \"UnicodeModel\", \"new_index\", \"old_index\", [\"name\", \"age\"]\n            ),\n        ]\n        self.assertOptimizesTo(operations, expected, app_label=\"UnicodeModel\")\n", "    def test_rename_index_combined(self):\n        operations = [\n            AddIndex(\"UnicodeModel\", models.Index(fields=[\"id\"], name=\"id_index\")),\n            RenameIndex(\n                \"UnicodeModel\", \"id_index\", \"new_id_index\", old_fields=[\"id\"]\n            ),\n            AddIndex(\"UnicodeModel\", models.Index(fields=[\"new_id\"], name=\"new_id_index\")),\n        ]\n        self.assertOptimizesTo(\n            operations,\n            [AddIndex(\"UnicodeModel\", models.Index(fields=[\"id\"], name=\"new_id_index\")),],\n            exact=1,\n        )\n\n\n"], "sample_995": ["    def test_rational_sympify():\n        for x in (Rational(1, 2),\n                  Rational(-5, 3),\n                  Rational(0, 7),\n                  Rational(1, 0),\n                  Rational(7, 1)):\n            assert sympify(x) == x\n\n\n\n", "    def test_mpf_norm():\n        from mpmath import prec\n        prec(50)\n        x = mpf(12.3456789012345678901234567890)\n\n        assert mpf_norm(x, 50) == x\n        assert mpf_norm(x, 10) == round(x, 10)\n        assert mpf_norm(x, 0) == int(x)\n\n        x = mpf(12.3456789012345678901234567890123456789012345678901234567890)\n        assert mpf_norm(x, 50) == x\n\n\n\n    ", "    def test_euler_gamma():\n        from sympy.functions.special.gamma_functions import gamma\n        assert S.EulerGamma > 0\n        assert abs(gamma(1) - S.EulerGamma) < 1e-10\n", "    def test_integer_nthroot():\n        assert integer_nthroot(8, 3) == 2\n        assert integer_nthroot(27, 3) == 3\n        with raises(ValueError):\n            integer_nthroot(8, 4)\n        with raises(ValueError):\n            integer_nthroot(-8, 2)\n\n", "    def test_number_symbols():\n        assert S.Exp1.is_real is True\n        assert S.Pi.is_irrational is True\n        assert S.GoldenRatio.is_irrational is True\n        assert S.EulerGamma.is_real is True\n        assert S.Catalan.is_real is True\n        assert S.ImaginaryUnit.is_imaginary is True\n"], "sample_593": ["def custom_dataset():\n    ds = xr.Dataset()\n    ds[\"custom_var\"] = xr.DataArray(\n        np.random.random(10), dims=(\"time\", \"lat\", \"lon\"),\n        attrs={\"unit\": \"m/s\", \"description\": \"Custom variable data\"}\n    )\n    ds.attrs[\"custom_attr\"] = \"This is a custom attribute\"\n    return ds\n", "def test_obj_repr_with_attrs(monkeypatch, obj_type, attrs, dataset, dataarray):\n    if obj_type == \"xarray.DataArray\":\n        obj = dataarray.copy()\n        obj.attrs = attrs\n    else:\n        obj = dataset.copy()\n        obj.attrs = attrs\n\n    monkeypatch.setattr(xr, \"DataArray\", type)\n    monkeypatch.setattr(xr, \"Dataset\", type)\n\n    result = fh._obj_repr(obj, [], [])\n    assert f\"{obj_type} in\" in result\n\n\n", "def test_array_repr_dask(dask_dataarray):\n    html = array_repr(dask_dataarray)\n    assert 'xr-array-in' in html\n\n\n", "def test_array_repr_attrs(obj_type, dataset):\n    if obj_type == \"xarray.DataArray\":\n        obj = dataset.tmin\n    else:\n        obj = dataset\n    \n    rendered_html = fh.array_repr(obj)\n    attrs_section = rendered_html.split(\"<li class='xr-section-item'>\")[2]\n    attrs_section = attrs_section.split(\"</li>\")[0]\n    \n    assert \"description\" in attrs_section\n    assert \"Test data.\" in attrs_section\n\n\n\n", "    def test_dataset_repr_with_attrs(dataset):\n        repr_html = dataset_repr(dataset)\n        assert \"description\" in repr_html\n        assert \"</pre>\" in repr_html\n"], "sample_999": ["    def test_latex_with_symbols(self):\n        symbol_names = {'x': 'x_i', 'y': 'y_j'}\n        expr = x**2 + 2*x*y + y**2\n        latex_expr = latex(expr, symbol_names=symbol_names)\n        self.assertEqual(latex_expr, 'x_i^{2} + 2 x_i y_j + y_j^{2}')\n", "    def test_latex_product(self):\n        p = Product(x, (x, 0, 5))\n        self.assertEqual(latex(p), r\"\\prod_{x = 0}^{5} x\")\n", "   def test_complex_region(self):\n        expr = ComplexRegion(\n                ConditionSet(\n                    [(mu - 1) * (mu - 2) <= 0]],\n                (mu, 0, 3))\n        self.assertEqual(latex(expr),\n                         r\"\\left\\{ \\left( \\mu \\right) \\left| \\left( \\mu - 1 \\right) \\left( \\mu - 2 \\right) \\le 0 \\right\\}\\right.$\")  \n", "    def test_latex_prime_functions(self):\n        self.assertEqual(latex(divisor_sigma(2), exp=3), r\"\\sigma^{3}_{2}\")\n        self.assertEqual(latex(udivisor_sigma(2), exp=3), r\"\\sigma^*_{2}^{3}\")\n        self.assertEqual(latex(primenu(2), exp=3), r\"\\nu^{3}(2)\")\n        self.assertEqual(latex(primeomega(2), exp=3), r\"\\Omega^{3}(2)\")\n\n\n\n", "compilation error"], "sample_459": ["    def test_integer_field_overflow_underflow(self):\n        with self.assertRaises(ValueError):\n            self.model.objects.create(value=-2147483649)\n        with self.assertRaises(ValueError):\n            self.model.objects.create(value=2147483648)\n", "    def test_integer_field_overflow(self):\n        with self.assertRaises(IntegrityError):\n            self.model.objects.create(value=2**32)  # Overflow\n\n        with self.assertRaises(IntegrityError):\n            self.model.objects.create(value=-2**32)  # Underflow\n", "    def test_integer_field_overflow(self):\n        with self.assertRaises(self.model.IntegerFieldOverflow.underflow_exception):\n            self.model.objects.get(value__lt=self.documented_range[0] - 1)\n\n        with self.assertRaises(self.model.IntegerFieldOverflow.overflow_exception):\n            self.model.objects.get(value__gt=self.documented_range[1] + 1)\n\n        # Check that the backend's range handling works as expected\n        if self.backend_range != self.documented_range:\n            raise SkipTest(\n                f\"Backend range ({self.backend_range}) does not match \"\n                f\"documented range ({self.documented_range})\"\n            )\n\n        with self.assertRaises(IntegrityError) as context:\n            self.model.objects.create(value=self.documented_range[0] - 1)\n        self.assertIn(\n            \"Cannot insert null values into field 'value' which has 'null=False'\",\n            str(context.exception),\n        )\n\n        with self.assertRaises(IntegrityError) as context:\n            self.model.objects.create(value=self.documented_range[1] + 1)\n        self.assertIn(\n            \"Cannot insert null values into field 'value' which has 'null=False'\",\n            str(context.exception),\n        )\n\n ", "    def test_integer_overflow(self):\n        with self.assertRaises(IntegrityError):\n            self.model.objects.create(value=2147483648)\n        with self.assertRaises(IntegrityError):\n            self.model.objects.create(value=-2147483649)\n", "    def test_integer_field_overflow(self):\n\n        with self.subTest(\"underflow\"):\n            with self.assertRaises(self.model._meta.get_field(\"value\").underflow_exception):\n                self.model.objects.create(value=-2147483649)\n\n        with self.subTest(\"overflow\"):\n            with self.assertRaises(self.model._meta.get_field(\"value\").overflow_exception):\n                self.model.objects.create(value=2147483648)\n\n"], "sample_827": ["def test_csc_median_axis_0():\n    rng = RandomState(42)\n    n_samples, n_features = 100, 5\n    X = sp.csc_matrix((rng.randn(n_samples, n_features),\n                      (rng.randint(0, n_features, n_samples),\n                       rng.randint(0, n_samples, n_features))),\n                      shape=(n_samples, n_features))\n\n    median_expected = np.median(X.toarray(), axis=0)\n    median_computed = csc_median_axis_0(X)\n\n    assert_array_almost_equal(median_expected, median_computed)\n\n", "compilation error", "    def test_inplace_column_scale_csc(self):\n        X = sp.csc_matrix([[1, 2, 3], [4, 5, 6]])\n        scale = np.array([0.5, 1.0, 1.5])\n        inplace_column_scale(X, scale)\n        expected = sp.csc_matrix([[0.5, 1.0, 4.5], [2.0, 5.0, 9.0]])\n        assert_array_equal(X.toarray(), expected.toarray())\n", "def test_inplace_swap_row_csc_different_types():\n    X = sp.csc_matrix([[1, 2], [3, 4]])\n    with assert_raises(TypeError):\n        inplace_swap_row_csc(X, m=1, n=np.array(2))\n\n    with assert_raises(TypeError):\n        inplace_swap_row_csc(X, m=[1, 2], n=2)\n\n\n\n", "compilation error"], "sample_976": ["    def test_wild_exclude_order(self):\n        a = Wild('a', exclude=[Symbol('b')])\n        b = Wild('b', exclude=[Symbol('a')])\n        expr = Symbol('c') * Symbol('a') + Symbol('b')\n        match = a.matches(expr)\n        assert match is not None\n        assert match[a] == expr\n        assert match[b] is None  # b should not match\n\n\n", "    def test_symbols_range_with_leading_digits(self):\n        self.assertEqual(symbols('1x:2'), (S(1) * x, S(1) * x + S(1)))\n        self.assertEqual(symbols('01x:02'), (S(1) * x, S(1) * x + S(1)))\n        self.assertEqual(symbols('10x:20'), (S(10) * x, S(10) * x + S(10)))\n", "    def test_symbols_range_with_parentheses():\n        with raises(ValueError):\n            symbols('x((a:b))')", "    def test_symbols_invalid_range(self):\n        symbols('x:z:a')", "    def test_symbols_range_letter_range():\n        with raises(ValueError):\n            symbols('a:z, c:b')\n"], "sample_1118": ["    def test_matpow_non_invertible_negative_power():\n        with raises(NonInvertibleMatrixError):\n            MatPow(ZeroMatrix(2), -1) \n", "    def test_matpow_zero_power_square_matrix(self):\n        m = 3\n        A = MatrixSymbol('A', m, m)\n        result = (A**0).doit()\n        assert result == Identity(m)\n", "compilation error", "    def test_matpow_zero_exponent():\n        assert MatPow(A, 0).doit() == Identity(A.shape[0])\n        assert MatPow(ZeroMatrix(A.shape), -1) == raises(NonInvertibleMatrixError)\n        assert MatPow(Identity(A.shape), 0).doit() == Identity(A.shape) \n", "    def test_pow_zero_square_matrix(self):\n        A = MatrixSymbol('A', 2, 2)\n        result = A**0\n        self.assertEqual(result, Identity(2))\n"], "sample_867": ["    def test_parameter_sampler_from_list(self, klass: type):\n        param_distributions = {'foo': [0, 1, 2]}\n        sampler = klass(param_distributions, n_iter=5)\n        for i in range(5):\n            params = next(sampler)\n            assert params['foo'] in [0, 1, 2]\n", "        def test_invalid_input(klass, input, error_type, error_message):\n            with pytest.raises(error_type) as e:\n                klass(input)\n            assert re.search(error_message, str(e.value))\n", "def test_param_grid_errors(klass, input, error_type, error_message):\n    with pytest.raises(error_type, match=error_message):\n        klass(input)\n\n\n\n", "    def test_parameter_grid_invalid_input(self, klass, input, error_type,\n                                         error_message):\n        with pytest.raises(error_type, match=error_message):\n            if isinstance(input, int):\n                klass({'foo': input})\n            else:\n                klass(input)\n\n\n", "def test_ParameterGrid_construction(params, expected_params):\n    grid = ParameterGrid(params)\n    assert_grid_iter_equals_getitem(grid)\n    for i, row in enumerate(grid):\n        assert row == expected_params\n"], "sample_933": ["    def test_gettext_catalog_location(app, tmpdir):\n        app.builder.env.config.gettext_location = str(tmpdir)\n        app.build()\n        catalog_path = os.path.join(tmpdir, 'messages.pot')\n        assert os.path.exists(catalog_path)\n\n        with open(catalog_path, 'r', encoding='utf-8') as f:\n            content = f.read()\n        assert 'msgid \"Project-Id-Version: ' in content\n        assert 'Project-Id-Version' in content\n        assert 'Copyright: ' in content\n        assert 'Last-Translator: ' in content\n\n", "    def test_gettext_missing_catalog(self, build_dir):\n        with cd(os.path.join(build_dir, 'domains')):\n            # Check if the missing catalog file is generated\n            assert os.path.exists('missing_domain.pot')\n\n            # Load the generated catalog\n            try:\n                catalog = gettext.translation('missing_domain', localedir=build_dir, languages=['en_US'])\n                catalog.install()\n            except gettext.Error:\n                assert True  # This should raise an error since the catalog is empty\n", "    def test_gettext_with_additional_targets(\n        self,\n        app: Sphinx,\n        srcdir: str,\n        tmpdir: pytest.tmpdir,", "    def test_gettext_compact_mode(app, status, tmpdir):\n        app.config.gettext_compact = True\n        result = app.build(tmpdirs)\n\n        assert result.success\n        po_files = {f for f in os.listdir(app.builder.outdir) if f.endswith(\".pot\")}\n        assert len(po_files) == 1\n        po_file = next(iter(po_files))\n        with open(os.path.join(app.builder.outdir, po_file), 'r', encoding='utf-8') as f:\n            content = f.read()\n        assert 'msgid \"\"msgstr \"\"' in content\n        assert 'Project-Id-Version' in content\n        assert 'Last-Translator' in content\n\n\n\n", "    def test_translations_in_pot(self, app, tmpdir):\n        with cd(tmpdir):\n            app.build()\n\n            pot_path = os.path.join(app.outdir, 'messages.pot')\n            with open(pot_path, 'r', encoding='utf-8') as potfile:\n                pot_content = potfile.read()\n\n            # Check if the translated messages are present in the POT file\n            for msg in ['Welcome to Sphinx!', 'This is a test message.']:\n                assert any(msg in line for line in pot_content.splitlines())\n\n            # Check for default domain\n            assert b'msgid \"Welcome to Sphinx!\"\\nmsgstr \"\"' in pot_content\n"], "sample_28": ["    def test_rvkc_keyword_value_parsing(self):\n        card_image = \"DP1.AXIS.1 = 12345.6789\"\n        card = fits.Card.fromstring(card_image)\n        assert card._keyword == \"DP1.AXIS.1\"\n        assert card._field_specifier == \"AXIS.1\"\n        assert card._value == 12345.6789\n\n        card_image = \"DP1.AXIS.1 = '12345.6789'\"\n        card = fits.Card.fromstring(card_image)\n        assert card._keyword == \"DP1.AXIS.1\"\n        assert card._field_specifier == \"AXIS.1\"\n        assert card._value == \"12345.6789\"\n", "    def test_long_commentary_card(self):\n        header = fits.Header()\n        value = \"This is a very long comment that exceeds the length limit \" \\\n                \"for a single FITS card and needs to be split across \" \\\n                \"multiple cards\" * 10\n        header.add_comment(value)\n        _card_list = list(header._itersubcards())\n        self.assertEqual(len(_card_list), 1)\n        self.assertEqual(_card_list[0].keyword, \"\")\n        self.assertEqual(_card_list[0].value, value)\n\n\n\n", "    def test_fits_card_empty_keyword_value(self):\n        with self.assertRaises(VerifyError) as cm:\n            fits.Card(\"\", \"        \")  # Empty keyword,  non-empty value\n\n        assert \"Keyword cannot be empty\" in str(cm.exception)\n", "    def test_hierarch_keywords(self):\n        h = fits.Header()\n        h.add_card(\"HIERARCH 1  =  This is a test\")\n        h.add_card(\"HIERARCH 2  =  Another test\")\n        h.add_card(\"HIERARCH 3 =  Yet another test\", comment=\"This is a comment\")\n        with fits.open(StringIO(h.tostring())) as hdul:\n            for card in hdul[0].header:\n                self.assertEqual(card.keyword, card.keyword.upper())\n                self.assertEqual(card.value, card.value)\n\n\n\n\n", "    def test_long_commentary_card(self):\n        # Test for `_format_long_commentary_image` for a long commentary card\n        header = fits.Header()\n        header.add_comment(\"This is a very very very very long \"\n                          \"comment that will span multiple cards.\")\n        with StringIO() as fobj:\n            fits.writeto(fobj, np.zeros((10,)), header=header)\n            fobj.seek(0)\n            hdu = fits.open(fobj)[0]\n            card = hdu.header['HISTORY']\n        assert card.image == f\"HISTORY  {card.image}\"\n        assert card._modified\n        assert card.comment == \"This is a very very very very long \\ncomment that will span multiple cards.\"\n\n\n\n"], "sample_934": ["compilation error", "    def test_cpp_enum_declaration(self):\n        cppDomain.Symbol.debug_show_tree = True\n        check(\n            'enum',\n            'enum my_enum { VALUE_A = 1, VALUE_B = 2 }',\n            {'': 'my_enum_0', 1: 'my_enum_1'},\n            'enum my_enum { VALUE_A = 1, VALUE_B = 2 }'\n        )\n        \n        cppDomain.Symbol.debug_show_tree = False\n", "    def check_namespace(self, input, idDict, output):\n        ast = parse(\"namespace\", input)\n        rootSymbol = Symbol(None, None, None, None, None, None)\n        symbol = rootSymbol.add_declaration(ast, docname=\"TestDoc\")\n        parentNode = addnodes.desc()\n        signode = addnodes.desc_signature(input, '')\n        parentNode += signode\n        ast.describe_signature(signode, 'lastIsName', symbol, options={})\n        resAsText = parentNode.astext()\n        if resAsText != output:\n            print(\"\")\n            print(\"Input:    \", input)\n            print(\"astext(): \", resAsText)\n            print(\"Expected: \", output)\n            raise DefinitionError(\"\")\n", "compilation error", "compilation error"], "sample_1016": ["    def test_printing_matrices_with_symbols():\n        A = MatrixSymbol('A', 3, 3)\n        B = MatrixSymbol('B', 3, 3)\n        code = octave_code(2*A*B)\n        assert code == '(2*A).*B'\n\n", "    def test_matrix_symbols_with_custom_functions(self):\n        from sympy.matrices import MatrixSymbol\n        n = Symbol('n', integer=True, positive=True)\n        A = MatrixSymbol('A', n, n)\n        custom_functions = {\n            'A': 'my_matrix_func(%s)' % A\n        }\n        code = octave_code(A**2, user_functions=custom_functions)\n        expected = 'my_matrix_func(A.^2)'\n        self.assertEqual(code, expected)\n\n\n", "compilation error", "compilation error", "    def test_nested_binary_math_func(self):\n        for f in [Max, Min]:\n            expr = f(sin(x), cos(x))\n            res = octave_code(expr)\n            if f is Max:\n                expected = \"max(sin(x), cos(x))\"\n            elif f is Min:\n                expected = \"min(sin(x), cos(x))\"\n            self.assertEqual(res, expected)\n\n\n\n"], "sample_891": ["    def test_top_k_accuracy_score_multiclass(drop):\n        y_true, y_pred, y_score = make_prediction(binary=False)\n        classes = np.unique(y_true)\n        top_k_acc = top_k_accuracy_score(y_true, y_score, k=3, normalize=normalize)\n\n        if normalize:\n            assert 0 <= top_k_acc <= 1\n            assert isinstance(top_k_acc, float)\n        else:\n            assert isinstance(top_k_acc, int)\n            assert top_k_acc >= 0\n\n\n", "    def _average_precision_slow(y_true, y_score):", "    def test_ndcg_score_multilabel_with_ties(drop):\n        # multilabel case with ties\n        y_true = np.array([[0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 0, 1]])\n        y_score = np.array([[0.5, 0.2, 0.3],\n                           [0.3, 0.4, 0.2],\n                           [0.2, 0.4, 0.3],\n                           [0.7, 0.2, 0.1]])\n\n        # Check that we get a different score when ties are considered\n        score_with_ties = ndcg_score(y_true, y_score, k=2, sample_weight=None, ignore_ties=False)\n\n        # Check that scores are different when ties are ignored\n        score_without_ties = ndcg_score(y_true, y_score, k=2, sample_weight=None, ignore_ties=True)\n\n        assert score_with_ties != score_without_ties\n", "def test_top_k_accuracy_score_with_labels(binary):\n    y_true, y_pred, y_score = make_prediction(binary=binary)\n    k = 2\n    labels = None\n    if not binary:\n        labels = np.array([0, 1, 2])\n    acc = top_k_accuracy_score(y_true, y_score, k=k, labels=labels)\n    print(f\"acc: {acc}\")\n\n\n\n", "    def test_top_k_accuracy_score_multiclass_with_labels(drop):\n        y_true = np.array([0, 1, 2, 2, 0, 1, 2])\n        y_score = np.array([[0.7, 0.2, 0.1],\n                           [0.3, 0.4, 0.3],\n                           [0.2, 0.4, 0.3],\n                           [0.1, 0.3, 0.6],\n                           [0.9, 0.1, 0.0],\n                           [0.5, 0.3, 0.2],\n                           [0.6, 0.2, 0.2]])\n        labels = np.array([0, 1, 2])\n        k = 2\n        score = top_k_accuracy_score(y_true, y_score, k=k, labels=labels)\n        if drop:\n            assert score == 0.6666666666666666\n        else:\n            assert score == 0.6666666666666666\n"], "sample_153": ["    def test_check_database_backends_for_model(self):\n        with mock.patch('django.apps.apps.Apps.get_app_config') as mock_get_app_config:\n\n            mock_get_app_config.return_value = mock.MagicMock(\n                models={'app': mock.MagicMock()}\n            )\n\n            # Mock getting the model and its check_constraints method\n                return []\n            mock_model = mock.MagicMock()\n            mock_model.check_constraints = mock_model_check_constraints\n            mock_get_app_config().models['app'].MyModel = mock_model\n\n            # Set up mock databases to be checked\n            mock_db_backends = [\n                mock.MagicMock(name='default', display_name='default_db'),\n                mock.MagicMock(name='other', display_name='other_db'),\n            ]\n\n            with mock.patch('django.db.connections.connections', new=ConnectionHandler([\n                ConnectionHandler.get_db(name=db.name) for db in mock_db_backends\n            ])):\n                checks = check_database_backends(self.databases)\n                for check in checks:\n                    self.assertEqual(check.id, 'models.W027')\n                    self.assertEqual(check.obj, mock_model)\n                    self.assertIn(\n                        'check constraints', check.msg,\n                        f\"Check message doesn't mention check constraints: {check.msg}\"\n                    )\n                    self.assertEqual(check.hint,\n                                     \"a constraint won't be created. silence this warning if you don't care about it.\")\n\n\n\n", "    def test_check_database_backends(self):\n        with mock.patch(\n            'django.apps.apps.get_app_config'\n        ) as mock_get_app_config:\n            mock_get_app_config.side_effect = lambda app_name: None\n            # Mock the connection to return a valid output\n            with mock.patch('django.db.connections', new_callable=connection.connections):\n                check_database_backends(self.databases)\n", "    def test_check_database_backends_with_invalid_backends(self):\n        mock_check.side_effect = [\n            checks.Error(\"Database backend not found.\", id='models.E001'),\n            checks.Error(\"Database backend not found.\", id='models.E001'),\n        ]\n        with self.assertRaises(ValueError):\n            check_database_backends(self.databases)\n", "    def test_check_database_backends_valid_databases(self):\n        with mock.patch('django.apps.apps.Apps.get_models') as mock_get_models:\n            mock_get_models.return_value = [\n                mock.MagicMock(meta=mock.MagicMock(swappable=False)),\n                mock.MagicMock(meta=mock.MagicMock(swappable=True)),\n            ]\n\n            check_database_backends(databases=self.databases)\n\n\n\n", "        def test_check_database_backends(self, mock_check):\n            mock_check.return_value = []\n            result = check_database_backends(self.databases)\n            self.assertEqual(result, [])\n"], "sample_98": ["    def test_connection_handling(self):\n        with self.assertRaises(socket.error) as cm:\n            HTTPConnection('localhost', self.server_port, timeout=0.1).request('GET', '/')\n        self.assertEqual(cm.exception.errno, errno.ECONNRESET)\n\n        with open(os.path.join(TEST_ROOT, 'broken_pipe.py')) as f:\n            url = f.read()\n        with self.assertRaises(ConnectionResetError):\n            self.urlopen(url)\n\n\n\n", "    def test_server_handles_broken_pipe_error(self):\n        with self.assertRaises(socket.error) as cm:\n            # Close the connection after sending the first request\n            with HTTPConnection(self.live_server_url_test[0], timeout=1) as conn:\n                conn.request('GET', '/')\n                conn.getresponse()\n\n        self.assertIn(\"Broken pipe\", str(cm.exception))\n", "    def test_broken_pipe_error(self):\n        # Create a connection to the server using HTTPConnection\n        conn = HTTPConnection('localhost', self.server_port)\n\n        # Send a request to trigger a broken pipe error\n        conn.request('GET', '/')\n        try:\n            conn.getresponse()\n        except HTTPError as e:\n            self.assertEqual(e.code, self.server.status_code)\n            self.assertTrue(conn.fp.closed)\n\n        # Ensure broken pipe error is handled gracefully by the server\n        conn.close()\n\n        # Check that the logger captured the broken pipe error\n        logger = logging.getLogger('django.server')\n        self.assertIn(\n            \"- Broken pipe from\", logger.getLogs()[0].records[-1].message,\n        )\n\n\n", "    def test_runserver_with_broken_pipe(self):\n        with self.assertRaises(BrokenPipeError):\n            with self.start_server(\n                threading=False,\n            ) as server:\n                try:\n                    with HTTPConnection('localhost', server.server_address[1]) as conn:\n                        conn.request('GET', '/')\n                        response = conn.getresponse()\n                        response.read()\n                        # Force a broken pipe\n                        conn.close()\n                except BrokenPipeError:\n                    return\n", "    def test_handle_broken_pipe_error(self):\n        with self.assertRaises(BrokenPipeError):\n            self.urlopen('/')\n        # Check if the broken pipe error was logged in the server's log\n        # This requires interacting with the server's log file, which might\n        # not be ideal for unit tests. Consider a mock logging framework for \n        # a more controlled test environment.\n        with open(self.server.log_file, 'r') as f:\n            log_lines = f.readlines()\n        self.assertIn(\n            '- Broken pipe from', log_lines[-1]\n        )\n\n\n\n"], "sample_201": ["    def test_max_cookie_size_works_with_multiple_messages(self):\n        \"\"\"\n        Test that `max_cookie_size` is respected when storing large\n        numbers of messages, that older messages are removed,\n        and the not_finished sentinel is set.\n        \"\"\"\n        messages = [\n            Message('Message 1', constants.INFO),\n            Message('Message 2', constants.INFO),\n            Message('Message 3', constants.INFO),\n            Message('Message 4', constants.INFO),\n            Message('Message 5', constants.INFO),\n            Message('Message 6', constants.INFO),\n            Message('Message 7', constants.INFO),\n            Message('Message 8', constants.INFO),\n            Message('Message 9', constants.INFO),\n            Message('Message 10', constants.INFO),\n            Message('Message 11', constants.INFO),\n        ]\n        storage = self.storage_class()\n\n        # Ensure we have enough space for at least some messages.\n        self.assertGreater(storage.max_cookie_size, \n                          len(storage._encode(messages)) )\n\n        response = self.get_response()\n\n        # Store the messages.\n        unstored_messages = storage._store(messages, response, remove_oldest=True)\n\n        # Check that the cookie was updated and messages were stored.\n        self.assertTrue(storage.used, f\"messages should have been stored\")\n        self.assertEqual(len(unstored_messages), 0)\n        self.assertEqual(stored_cookie_messages_count(storage, response), \n                         storage.max_cookie_size // len(storage._encode(\n                         [Message('test', constants.INFO)])))\n", "    def test_cookie_messages_removed_and_refixed_via_set_cookie_data(self):\n        messages = [\n            Message(constants.DEBUG, 'This is a debug message', extra_tags='debug-tag'),\n            Message(constants.INFO, 'This is an info message'),\n            Message(constants.SUCCESS, 'This is a success message'),\n            Message(constants.WARNING, 'This is a warning message'),\n            Message(constants.ERROR, 'This is an error message', extra_tags='error-tag'),\n        ]\n        storage = self.storage_class()\n        response = self.mock_response()\n        storage._store(messages, response)\n\n        # Simulate that messages are set on the request again, overwriting\n        # the existing cookie.\n        new_messages = [\n            Message(constants.DEBUG, 'New debug message'),\n            Message(constants.INFO, 'New info message'),\n        ]\n        set_cookie_data(storage, new_messages)\n        self.assertEqual(stored_cookie_messages_count(storage, response), 2)\n\n", "    def test_max_cookie_size(self):\n        storage = self.storage_class()\n        messages = [\n            Message(constants.SUCCESS, 'This is a success message', extra_tags='success'),\n            Message(constants.ERROR, 'This is an error message', extra_tags='error'),\n            Message(constants.WARNING, 'This is a warning message', extra_tags='warning'),\n            Message(constants.DEBUG, 'This is a debug message', extra_tags='debug'),\n        ]\n        response = self.get_response()\n\n        # Set cookie size to 1024 (less than the default 2048)\n        with override_settings(SESSION_COOKIE_SIZE=1024):\n            set_cookie_data(storage, messages)\n            self.assertEqual(stored_cookie_messages_count(storage, response), 3)\n", "    def test_remove_oldest_messages(self):\n        storage = self.storage_class()\n        messages = [\n            Message(\"Message 1\", constants.INFO),\n            Message(\"Message 2\", constants.WARNING),\n            Message(\"Message 3\", constants.ERROR),\n        ]\n\n        # Add messages to the storage, ensuring 'Message 1' is the oldest.\n        response = self.create_response()\n        storage._store(messages, response, remove_oldest=True)\n\n        self.assertEqual(stored_cookie_messages_count(storage, response), 2)\n\n        # Check if the oldest message was removed\n        decoded_messages = storage._decode(response.cookies[storage.cookie_name].value)\n        self.assertEqual(decoded_messages[0].message, \"Message 2\")\n", "    def test_cookie_storage_with_safedata(self):\n        storage = self.storage_class()\n        messages = [\n            Message(level=constants.ERROR, message=mark_safe(\"<b>Test</b>\"), extra_tags='error-tag'),\n        ]\n        storage._store(messages, self.response)\n        self.assertEqual(stored_cookie_messages_count(storage, self.response), 1)\n        self.assertIn(storage.cookie_name, self.response.cookies)\n"], "sample_798": ["    def test_ridge_solve(self, X_type, y_type, solver):\n        X = X_type(np.random.rand(10, 5))\n        y = y_type(np.random.rand(10))\n        alpha = 0.5\n        ridge = Ridge(alpha=alpha, solver=solver)\n\n        # Check that the coefficients are close to the theoretical values\n        weights = ridge.fit(X, y).coef_\n        # This is using the analytical solution\n        sol = _solve_cholesky(X, y, alpha)\n        assert_array_almost_equal(weights, sol, decimal=5)\n", "    def test_ridge_multiclass(self, X, y):\n        X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n                         multi_output=True)\n        print('X', X)\n        print('y', y)\n        reg = Ridge(fit_intercept=True, random_state=0).fit(X, y)\n        print('reg', reg)\n        assert reg.coef_.shape[0] == X.shape[1]\n        assert reg.intercept_.shape[0] == 1\n", "    def test_ridge_solver_lsqr_dense(self, solver):\n        X = np.random.rand(100, 5)\n        y = np.random.rand(100,)\n        ridge = Ridge(alpha=0.1, solver=solver)\n        ridge.fit(X, y)\n        assert_allclose(ridge.coef_, ridge.coef_, atol=1e-4)\n        assert_allclose(ridge.intercept_, ridge.intercept_, atol=1e-4)\n", "    def test_ridge_regression_sparse(self, X_type, solver, X, y):\n        X = DENSE_FILTER(X) if X_type == 'dense' else SPARSE_FILTER(X)\n        y = np.random.randint(2, size=X.shape[0])\n\n        # Test Ridge with different solvers using sparse matrices\n        ridge = Ridge(alpha=1.0, solver=solver)\n        ridge.fit(X, y)\n        assert np.all(ridge.coef_ == ridge.coef_)\n\n        # Test Ridge with different solvers using dense matrices\n\n        ridge_dense = Ridge(alpha=1.0, solver=solver)\n        ridge_dense.fit(X.toarray(), y)\n\n        assert_almost_equal(ridge_dense.coef_, ridge.coef_)\n\n\n\n", "    def test_sparse_ridge_cholesky(self, solver):\n        # Test cholesky solver with a sparse matrix\n        n_samples = 100\n        n_features = 100\n        X = SPARSE_FILTER(np.random.rand(n_samples, n_features))\n        y = np.random.randn(n_samples)\n\n        ridge = Ridge(solver=solver, alpha=0.1)\n        ridge.fit(X, y)\n        assert_array_almost_equal(ridge.coef_, _solve_cholesky(X.T, X,\n                                                             y, ridge.alpha))\n"], "sample_1061": ["    def test_as_content_primitive_sqrt_expansion(self):\n        from sympy import sqrt\n        eq = sqrt(4 + 4*sqrt(2))\n        c, r = eq.as_content_primitive()\n        self.assertEqual(c, 2)\n        self.assertEqual(r, sqrt(1 + sqrt(2)))\n", "    def test_as_content_primitive_radicals(self):\n        from sympy import Sqrt, sqrt\n        assert (sqrt(4 + 4*sqrt(2))).as_content_primitive(radical=True) == (2, sqrt(1 + sqrt(2)))\n        assert (sqrt(3 + 3*sqrt(2))).as_content_primitive(radical=True) == (1, sqrt(3)*sqrt(1 + sqrt(2)))\n        assert (sqrt(4 + 4*sqrt(5))).as_content_primitive(radical=True) == (2, sqrt(1 + sqrt(5)))\n        assert (Sqrt(4 + 4*sqrt(2))).as_content_primitive(radical=True) == (2, sqrt(1 + sqrt(2)))\n\n\n\n        # Test with negative powers\n        assert ( (2*x + 2)**(-2) ).as_content_primitive(radical=True) == (4, 1/(x + 1)**2)\n        # Test with irrational bases\n        assert ( (sqrt(2) + 1)**2 ).as_content_primitive(radical=True) == (3, (sqrt(2) + 1)**2)\n\n\n\n        # Test with rational bases and irrational powers\n        assert ( (2**sqrt(2))**(sqrt(2)) ).as_content_primitive(radical=True) == (2**2, 2**(sqrt(2)))\n        assert ( (2**sqrt(2))**(sqrt(3)) ).as_content_primitive(radical=True) == (2**2, 2**(sqrt(2)*sqrt(3)))\n        \n        \n", "    def test_as_content_primitive_sqrt():\n        from sympy import sqrt\n        assert sqrt(4 + 4*sqrt(2)).as_content_primitive() == (2, sqrt(1 + sqrt(2)))\n        assert sqrt(3 + 3*sqrt(2)).as_content_primitive() == (1, sqrt(3)*sqrt(1 + sqrt(2)))\n        assert sqrt(4).as_content_primitive() == (2, 1)\n", "    def test_as_content_primitive_mul():\n        for i in range(3):\n            for j in range(3):\n                x = Symbol('x' + str(i))\n                y = Symbol('y' + str(j))\n                v = (2*x + 2)**(i + 1j*y)\n                c, m = v.as_content_primitive()\n                assert isinstance(c, Rational) ,f'for i={i} j={j} , c={c} is not Rational'\n                assert isinstance(m, Pow) and (m.base.is_Add or m.base.is_Symbol)\n    \n                    \n", "    def test_as_coeff_add():\n        from sympy.functions import log\n        a = Pow(2, Symbol('x'))\n        assert a.as_coeff_Add() == a, \"Incorrect result for simple power\"\n\n        b = Pow(2, 2*Symbol('x') + 3)\n        assert b.as_coeff_Add() == (2**3, 2*Symbol('x')), \"Incorrect result for power with constant and variable terms\"\n\n        c = Pow(2, Symbol('x') + log(Symbol('x')))\n        assert c.as_coeff_Add() == (1, Symbol('x') + log(Symbol('x'))), \"Incorrect result for power with variable and log term\"\n\n\n\n"], "sample_1160": ["    def test_intersection_sets_productset_productset():\n        a = ProductSet((S.Naturals, S.Naturals))\n        b = ProductSet((S.Integers, S.Integers))\n        result = intersection_sets(a, b)\n        assert result == ProductSet((S.Naturals, S.Integers))\n", "    def test_intersection_sets_interval_complex():\n        from sympy.core.function import exp\n\n        a = ComplexRegion(Intersection(\n            [Interval(0, 1, left_open=True)],\n            [Interval(0, 1, left_open=True)]),\n            polar=True)\n        b = ComplexRegion(\n            [Intersection(\n                [Interval(0, 1, left_open=True)],\n                [Interval(0, 1, left_open=True)])],\n            polar=True)\n\n        assert intersection_sets(a, b) == a\n\n        a = ComplexRegion(\n            [Interval(0, 1, left_open=True)],\n            polar=True)\n        b = ComplexRegion(\n            [Interval(1, 2, left_open=True)],\n            polar=True)\n\n        assert intersection_sets(a, b) == S.EmptySet\n\n\n", "    def test_intersection_sets_product_sets():\n        from sympy.core.sets import Intersection\n\n        a = ProductSet((S.Naturals, S.Reals))\n        b = ProductSet((S.Naturals, S.Naturals))\n        expected = ProductSet((S.Naturals, S.Naturals))\n\n        result = intersection_sets(a, b)\n        assert result == expected\n", "    def test_intersection_sets_complex_region():\n        z = symbols('z')\n        r1 = ComplexRegion(Interval(0, 1), Interval(0, pi))\n        r2 = ComplexRegion(Interval(0.5, 2), Interval(0, pi/2))\n        result = intersection_sets(r1, r2)\n        expected = ComplexRegion(Interval(0.5, 1), Interval(0, pi/2))\n        assert result == expected\n", "    def test_intersection_sets_productset_productset():\n        from sympy.sets.sets import ProductSet\n        a = ProductSet((S.Integers, S.Naturals))\n        b = ProductSet((S.Naturals, S.Integers))\n        assert intersection_sets(a, b) == ProductSet((S.Naturals, S.Naturals))\n\n"], "sample_901": ["    def test_mini_batch_kmeans_partial_fit_reset(self, representation, algo, dtype):\n        if representation == 'sparse' and algo == 'elkan':\n            pytest.skip(\"Sparse elkan algo not implemented\")\n        X = X.astype(dtype)\n        mbkmeans = MiniBatchKMeans(n_clusters=n_clusters,\n                                  batch_size=10, random_state=42)\n        # partial_fit should increase inertia every call\n        # unless the data is exactly the same\n\n\n        mbkmeans.partial_fit(X[:50])  # Partial fit on first half\n        inertia_after_partial1 = mbkmeans.inertia_\n        mbkmeans.partial_fit(X[50:])\n\n        assert mbkmeans.inertia_ < inertia_after_partial1\n\n\n\n", "    def test_partial_fit_empty_batch(self, representation, algo, dtype):\n\n        X = X.astype(dtype)\n\n        if representation == 'sparse':\n            X = X_csr\n\n        # Test that partial fit handles empty batches correctly\n        kmeans = MiniBatchKMeans(n_clusters=n_clusters, init=algo,\n                                batch_size=10, random_state=42,\n                                n_init=3, tol=1e-6)\n        kmeans.fit(X)\n\n        # Empty batch should not change the cluster centers\n        kmeans.partial_fit(X[:0])\n        assert_array_almost_equal(kmeans.cluster_centers_,\n                                   kmeans.cluster_centers_)\n\n\n", "def test_mini_batch_kmeans_inertia_convergence(\n        representation, algo, dtype):\n    if representation == 'sparse':\n        X = X_csr\n    else:\n        X = X.astype(dtype)\n    mbkmeans = MiniBatchKMeans(n_clusters=3, batch_size=50, random_state=42)\n\n    mbkmeans.fit(X)\n    assert mbkmeans.inertia_ < 1e-2 * X.shape[0]\n", "    def test_mini_batch_reassignment(representation, algo, dtype):\n        X = X.astype(dtype)\n        if representation == 'sparse':\n            X = X_csr\n        # Test reassignment\n        kmeans = MiniBatchKMeans(n_clusters=3, batch_size=10,\n                                random_state=0,\n                                reassignment_ratio=0.01)\n        kmeans.fit(X)\n        with patch.object(MiniBatchKMeans, 'partial_fit') as mock_partial_fit:\n            kmeans.partial_fit(X[:10])\n        mock_partial_fit.assert_called_once()\n        assert all(np.min(kmeans.counts_) > 0)\n        with patch.object(MiniBatchKMeans, 'partial_fit') as mock_partial_fit:\n            kmeans.partial_fit(X)\n        mock_partial_fit.assert_called_once()\n        assert all(kmeans.counts_ > 0)\n\n\n\n", "    def test_mini_batch_step_random_reassign(representation, dtype,\n                                            ):\n        X = X.astype(dtype)\n        if representation == 'sparse':\n            X = X_csr.astype(dtype)\n        batch_size = int(0.2 * n_samples)\n        random_state = np.random.RandomState(42)\n        \n        if dtype == np.float32:\n            kmeans = MiniBatchKMeans(\n                n_clusters=n_clusters,\n                batch_size=batch_size,\n                random_state=random_state,\n                reassignment_ratio=0.1).fit(X)\n        else:\n            kmeans = MiniBatchKMeans(\n                n_clusters=n_clusters,\n                batch_size=batch_size,\n                random_state=random_state,\n                reassignment_ratio=0.1).fit(X)\n        \n        \n        centers = kmeans.cluster_centers_\n        counts = kmeans.counts_\n        old_center_buffer = np.zeros(n_features, dtype=X.dtype)\n        \n        # Randomly select a batch of data\n        batch_indices = random_state.choice(n_samples, size=batch_size,\n                                           replace=False)\n        X_batch = X[batch_indices]\n        sample_weight_batch = sample_weight[batch_indices]\n        x_squared_norms_batch = x_squared_norms[batch_indices]\n        \n        \n        \n        # Call _mini_batch_step function with random reassignment\n        _mini_batch_step(X_batch, sample_weight_batch,\n                         x_squared_norms_batch, centers, counts,\n                         old_center_buffer, tol=0,\n                         random_reassign=True,\n                         distances=None,\n                         random_state=random_state,\n                         reassignment_ratio=0.1,\n                         verbose=False)\n\n        # Check if center updates happened for at least one cluster\n\n        # Compare the centers and counts before and after the update\n        for cluster_idx in range(n_clusters):\n            assert np.any(np.abs(kmeans.cluster_centers_["], "sample_117": ["    def test_clean_password2_validation(self):\n        form = UserCreationForm()\n        form.fields['password1'].initial = 'test'\n        form.fields['password2'].initial = 'test'\n        data = {'username': self.u1.username, 'password1': 'test', 'password2': 'test'}\n        form.is_valid = mock.Mock(return_value=True)\n        form.save.return_value = self.u1\n        with self.assertRaises(forms.ValidationError) as context:\n            form.clean(data)\n        self.assertEqual(context.exception.code, 'password_mismatch')\n", "    def test_user_creation_form_save(self):\n        form = UserCreationForm(data={'username': 'newuser'})\n        self.assertTrue(form.is_valid())\n        user = form.save()\n        self.assertEqual(user.username, 'newuser')\n        self.assertTrue(user.check_password('password'))\n        self.assertEqual(user.is_active, True)\n\n\n\n", "    def test_user_creation_form_validation(self):\n        form = UserCreationForm({'username': 'newuser', 'password1': 'password', 'password2': 'password'})\n\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['password2'], [PasswordChangeForm.error_messages['password_mismatch']])\n\n        form = UserCreationForm({'username': 'newuser', 'password1': 'password', 'password2': 'wrong_password'})\n\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['password2'], [PasswordChangeForm.error_messages['password_mismatch']])\n\n        form = UserCreationForm({'username': 'newuser', 'password1': 'password', 'password2': None})\n\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['password2'], ['This field is required.'])\n\n\n", "    def test_password_validation(self):\n        form = UserCreationForm(data={'username': 'testclient', 'password1': '', 'password2': ''})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['password1'], ['This field is required.'])\n\n", "    def test_save_sets_password(self):\n        form = UserCreationForm({'username': 'newuser', 'password1': 'password', 'password2': 'password'})\n        self.assertTrue(form.is_valid())\n        user = form.save()\n        self.assertEqual(user.password, 'password')\n        self.assertFalse(user.is_staff)\n        self.assertFalse(user.is_superuser)\n\n\n\n"], "sample_1136": ["    def test_is_positive(self):\n        self.assertTrue(EX(1).is_positive(EX(2)))\n        self.assertFalse(EX(1).is_positive(EX(-1)))\n        self.assertTrue(EX(1).is_positive(EX(1)))\n        self.assertFalse(EX(1).is_positive(EX(0)))\n", "    def test_ExpressionDomain_from_sympy(self):\n        ex = Symbol('x')\n        domain = EX()\n        self.assertTrue(_strict_eq(domain.from_sympy(ex), EX(ex)))\n\n        f = 2*x**2 + 3*x - 1\n        self.assertTrue(_strict_eq(domain.from_sympy(f), EX(f)))\n", "    def test_EX_arithmetic(self):\n        K = EX()\n        a = K(2*x + 1)\n        b = K(3*x - 2)\n        c = K(x**2 + x)\n\n        self.assertTrue(a + b == K(5*x - 1))\n        self.assertTrue(a - b == K(-x + 3))\n        self.assertTrue(a * b == K(6*x**2 -x - 2))\n        self.assertTrue(a / b == K((2*x + 1)/(3*x - 2)))\n        self.assertTrue(a**2 == K(4*x**2 + 4*x + 1))\n\n        self.assertTrue(a.numer() == K(2*x + 1))\n        self.assertTrue(a.denom() == K(1))\n        self.assertTrue(a.gcd(b) == K(1))\n        self.assertTrue(a.lcm(b) == K(6*x**2 - x - 2))\n\n        self.assertTrue(a.is_positive(K(x + 1)))\n        self.assertTrue(not a.is_positive(K(-x + 1)))\n        self.assertTrue(a.is_negative(K(-x + 1)))\n", "    def test_is_positive(self):\n        domain = EX()\n        self.assertTrue(domain.is_positive(domain(2)))\n        self.assertFalse(domain.is_positive(domain(-2)))\n        self.assertTrue(domain.is_positive(domain(2.5)))\n        self.assertFalse(domain.is_positive(domain(-2.5)))\n        self.assertTrue(domain.is_positive(domain(x)))\n        self.assertFalse(domain.is_positive(domain(-x)))\n", "    def test_is_positive():\n        K = EX\n        a = K(1.5)\n        assert K.is_positive(a)\n        assert not K.is_positive(K(-1.5))\n        assert not K.is_positive(K(0))\n"], "sample_763": ["    def test_check_non_negative_sparse(self):\n        # Tests check_non_negative with sparse matrices\n        X_csr = sp.csr_matrix([[1, 2], [3, 4]])\n        X_csc = sp.csc_matrix([[1, 2], [3, 4]])\n        X_lil = sp.lil_matrix([[1, 2], [3, 4]])\n        X_dok = sp.dok_matrix([[1, 2], [3, 4]])\n        X_empty = sp.csr_matrix(([], [], []))\n\n        for X in [X_csr, X_csc, X_lil, X_dok, X_empty]:\n            check_non_negative(X, \"check_non_negative\")\n\n        # Test with negative values\n        X_csr[-1, 0] = -1\n        with pytest.raises(ValueError, match=\"Negative values in data passed to check_non_negative\"):\n            check_non_negative(X_csr, \"check_non_negative\")\n", "    def test_check_array_dtype(self, dtype, expected_dtype):\n        X = np.ones(10)\n        if dtype is object:\n            X = X.astype(object)\n        X = check_array(X, dtype=dtype)\n        assert_array_equal(X.dtype, expected_dtype)\n", "def test_check_memory_sparse():\n    X = sp.csr_matrix((np.ones(100), (np.arange(100), np.arange(100))),\n                       shape=(100, 100))\n    assert check_memory(X, 0.9 * X.data.nbytes) is True\n    assert check_memory(X, 1.1 * X.data.nbytes) is False\n\n", "    def test_check_large_sparse_compat(self):\n        X = sp.csr_matrix([[1, 2, 3], [4, 5, 6]], dtype=np.float64)\n        X_large = sp.csr_matrix([[1, 2, 3], [4, 5, 6]], dtype=np.float64)\n\n        # Test with accept_large_sparse=True\n        check_array(X, accept_sparse=True, accept_large_sparse=True)\n        check_array(X_large, accept_sparse=True, accept_large_sparse=True)\n\n        # Test with accept_large_sparse=False\n        with assert_raises_regex(ValueError,\n                                  \"Only sparse matrices with 32-bit integer indices\"):\n            check_array(X_large, accept_sparse=True, accept_large_sparse=False)\n", "    def test_check_symmetric_sparse_csc(self):\n        X = sp.csc_matrix([[1, 2], [2, 3]])\n        X_sym = check_symmetric(X)\n        assert_array_equal(X_sym.data, X.data)\n"], "sample_270": ["    def test_on_model_with_index_together(self):\n        # Test index_together functionality.\n        with override_settings(DATABASE_ROUTERS={'default': EmptyRouter()}):\n            class IndexModel(models.Model):\n                name = models.CharField(max_length=30)\n                age = models.IntegerField()\n                index_together = [\n                    ('name', 'age'),\n                ]\n            self.assertTrue(IndexModel._meta.index_together)\n            index_model = IndexModel.objects.create(name='John', age=30)\n            self.assertEqual(index_model.name, 'John')\n            self.assertEqual(index_model.age, 30)\n\n            # Test if the indexes are created properly.\n            cursor = connection.cursor()\n            cursor.execute(\"SELECT index_name FROM django_indexes\")\n            indexes = [row[0] for row in cursor.fetchall()]\n            self.assertIn('index_together_name', indexes)\n\n\n", "    def test_index_together_with_expression(self):\n        class MyModel(models.Model):\n            name = models.CharField(max_length=100)\n            value = models.IntegerField()\n\n        with override_settings(DATABASE={'default': {'ENGINE': 'django.db.backends.sqlite3'}}), \\\n                self.subTest('sqlite3'):\n            self.assertEqual(_check_index_together(MyModel._meta), [])\n\n        with override_settings(DATABASE={'default': {'ENGINE': 'django.db.backends.postgresql'}}), \\\n                self.subTest('postgresql'):\n            self.assertEqual(\n                _check_index_together(MyModel._meta),\n                [Error(\n                    \"Index together cannot include an expression.\",\n                    obj=MyModel,\n                    id='models.E035',\n                )]\n            )\n\n        with override_settings(DATABASE={'default': {'ENGINE': 'django.db.backends.mysql'}}), \\\n                self.subTest('mysql'):\n            self.assertEqual(\n                _check_index_together(MyModel._meta),\n                [Error(\n                    \"Index together cannot include an expression.\",\n                    obj=MyModel,\n                    id='models.E035',\n                )]\n            )\n", "    def test_index_together_constraints(self):\n        with override_settings(DATABASE_ROUTERS={'default': EmptyRouter()}):\n            with self.subTest('SQLite'):\n                with connections['default'].cursor() as cursor:\n                    cursor.execute(\"\"\"\n                        CREATE TABLE example_testmodel (\n                            id INTEGER PRIMARY KEY,\n                            field1 TEXT,\n                            field2 TEXT\n                        );\n                    \"\"\")\n\n                with self.assertRaises(models.OperationalError):\n                    IndexTogether(example_testmodel, ['field1', 'field2'], name='test_index_together')  \n\n            with self.subTest('PostgreSQL', databases=['other']):\n                with connections['other'].cursor() as cursor:\n                    cursor.execute(\"\"\"\n                        CREATE TABLE example_testmodel (\n                            id SERIAL PRIMARY KEY,\n                            field1 TEXT,\n                            field2 TEXT\n                        );\n                    \"\"\")\n                \n                IndexTogether(example_testmodel, ['field1', 'field2'], name='test_index_together') \n", "    def test_index_together_check_with_fields(self):\n        with override_settings(DATABASES={'default': {'ENGINE': 'django.db.backends.sqlite3'}}):\n            class Product(models.Model):\n                name = models.CharField(max_length=100)\n                price = models.DecimalField(max_digits=10, decimal_places=2)\n                category = models.CharField(max_length=50)\n                index_together = [('name', 'category')]\n\n            class ProductVariation(models.Model):\n                product = models.ForeignKey(Product, on_delete=models.CASCADE)\n                size = models.CharField(max_length=50)\n                color = models.CharField(max_length=50)\n                index_together = [('product_id', 'size', 'color')]\n\n        errors = _check_models([Product, ProductVariation], connections['default'])\n        self.assertEqual(errors, [])\n\n\n\n", "    def test_error_empty_index_together(self):\n        class Model(models.Model):\n            pass\n\n        with self.assertRaises(ValueError):\n            Model.objects.index_together([])\n"], "sample_824": ["    def test_pairwise_distances_chunked_row_by_row(self):\n        X = np.random.randn(1000, 5)\n            return D_chunk.flatten()\n        # force row-by-row generation\n        with config_context({'working_memory': 0}):\n            gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n            rows = [next(gen) for _ in range(X.shape[0])]\n        assert_array_equal(np.concatenate(rows),\n                           pairwise_distances(X, X).flatten())\n\n\n\n", "    def test_cosine_similarity_handle_empty_arrays(self):\n        X = np.zeros((0, 2))\n        Y = np.zeros((3, 2))\n\n        # Check for empty array behavior\n        with pytest.raises(ValueError,\n                          match=\"Input arrays should have at least one sample\"):\n            cosine_similarity(X, Y)\n\n        with pytest.raises(ValueError,\n                          match=\"Input arrays should have at least one sample\"):\n            cosine_similarity(Y, X)\n", "    def test_pairwise_kernels_filter_params(metric):\n        X = np.random.rand(10, 5)\n        kwds = {\"gamma\": 1.0, \"degree\": 3, \"coef0\": 0.1, \"alpha\": 0.5}\n\n        with config_context.config_from_dict({\"pairwise_kernels_filter_params\": True}):\n            K1 = pairwise_kernels(X, metric=metric, **kwds)\n        with config_context.config_from_dict({\"pairwise_kernels_filter_params\": False}):\n            K2 = pairwise_kernels(X, metric=metric, **kwds)\n\n        assert_array_almost_equal(K1, K2)\n", "    def test_pairwise_distances_chunked_single_chunk(self, metric):\n        X = np.random.rand(100, 5)\n        with config_context.overrides(working_memory=1024):\n            gen = pairwise_distances_chunked(X, metric=metric,\n                                             n_jobs=-1)\n\n            # should yield a single chunk, which is the whole matrix\n            chunk = next(gen)\n            assert_array_almost_equal(chunk, pairwise_distances(X, metric=metric,\n                                                              n_jobs=-1))\n\n\n\n", "    def test_paired_distances(metric):\n        X = np.random.rand(5, 3)\n        Y = np.random.rand(5, 3)\n        \n        # Note: for paired_distances, the result of the metric function\n        #       should be the distances between corresponding elements\n        #       of the two arrays, not the pairwise distances between\n        #       all pairs.\n\n        distances = paired_distances(X, Y, metric=metric)\n        expected_distances = np.array([\n            [0, 0, 0],\n            [0, 0, 0],\n            [0, 0, 0],\n            [0, 0, 0],\n            [0, 0, 0],\n        ])\n\n        assert_allclose(distances, expected_distances) \n        "], "sample_1142": ["    def test_from_index_summation_complex(self):\n        A = MatrixSymbol('A', 3, 3)\n        expr = Sum(\n            A[i, j]*B[j, k],\n            (i, 0, 2),\n            (j, 0, 2),\n            (k, 0, 2)\n        )\n        result = MatrixExpr.from_index_summation(expr)\n        self.assertEqual(result, A @ B)\n", "    def test_from_index_summation_trace():\n        expr = Sum(A[i, i], (i, 0, n - 1))\n        result = MatrixExpr.from_index_summation(expr)\n        assert result == Trace(A)\n", "    def test_as_explicit():\n        from sympy.matrices.expressions.implicit import MatrixExpr\n\n        expr = Sum(A[i, j]*B[j, k], (i, 0, n-1))\n        expected = MatMul(A, B)\n        assert isinstance(expr.as_explicit(), MatrixExpr)\n        assert expr.as_explicit() == expected\n\n\n\n\n", "    def test_from_index_summation_no_index():\n        from sympy import Symbol\n        A = MatrixSymbol(\"A\", 2, 2)\n        expr = Sum(A[i, j], (i, 0, 1))\n        result = MatrixExpr.from_index_summation(expr)\n        assert result == A\n\n", "    def test_from_index_summation_multiple_indices(self):\n        expr = Sum(A[i, j]*B[j, k]*C[k, l], (i, 0, n-1), (j, 0, m-1), (k, 0, l-1))\n        mat = MatrixExpr.from_index_summation(expr)\n        self.assertEqual(mat.shape, (n, l))\n        self.assertEqual(mat.args, [A, B, C])\n\n\n\n"], "sample_1037": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_846": ["    def test_remainder_estimator_fit_transform(self):\n        X = np.array([[1, 2], [3, 4]])\n        y = np.array([5, 6])\n        remainder_transformer = Trans()\n        ct = ColumnTransformer(\n            transformers=[(\"scaler\", StandardScaler(), [0])],\n            remainder=remainder_transformer,\n        )\n        ct.fit(X, y)\n        X_pred = ct.transform(X)\n        assert_allclose_dense_sparse(\n            X_pred[:, 0], remainder_transformer.transform(X[:, 1].reshape(-1, 1))\n        )\n", "    def test_remainder_estimator(self):\n        X = np.array([[1, 2], [3, 4]])\n        remainder = Trans()\n        ct = make_column_transformer(\n            (StandardScaler(), [0]),\n            remainder=remainder\n        )\n        ct.fit(X)\n        transformed = ct.transform(X)\n        expected = np.concatenate(\n            (\n                StandardScaler().fit_transform(X[:, 0].reshape(-1, 1)),\n                remainder.transform(X[:, 1].reshape(-1, 1))\n            ),\n            axis=1\n        )\n        assert_allclose_dense_sparse(transformed, expected)\n", "    def test_remainder_estimator(self):\n        remainder_est = StandardScaler()\n        ct = ColumnTransformer(\n            [(\"scaler\", StandardScaler(), [0, 1]),\n             (\"remainder\", remainder_est, slice(2, 4))], remainder='remainder'\n        )\n        X = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n        ct.fit(X)\n        X_trans = ct.transform(X)  \n        assert_allclose_dense_sparse(\n            X_trans[:, :2],\n            np.array([[0, 0], [0, 0]])\n        )\n        assert_allclose_dense_sparse(\n            X_trans[:, 2:],\n            remainder_est.transform(X[:, 2:])\n        )\n", "    def test_remainder_estimator(self):\n        X = DataFrame({'A': [1, 2], 'B': [3, 4], 'C': [5, 6]})\n        remainder_estimator = Trans()\n\n        ct = ColumnTransformer(\n            transformers=[(\"scaler\", StandardScaler(), \"A\")],\n            remainder=remainder_estimator,\n        )\n\n        ct.fit_transform(X)\n\n        # Check that the remainder was applied\n        transformed_X = ct.transform(X)\n        assert transformed_X.shape[1] == 3\n\n\n\n", "    def test_remainder_as_estimator(self):\n        X = np.array([[1, 2], [3, 4]])\n        remainder_transformer = Trans()\n        ct = make_column_transformer(\n            [(StandardScaler(), [0]), ('drop', [1])], remainder=remainder_transformer\n        )\n\n        ct.fit(X)\n        transformed_X = ct.transform(X)\n\n        expected_X = np.array([[1, 2], [3, 4]])\n        expected_X[:, 0] = ct.named_transformers_['standardscaler'].transform(expected_X[:, 0])\n        expected_X[:, 1] = remainder_transformer.transform(expected_X[:, 1])\n        assert_allclose_dense_sparse(transformed_X, expected_X)\n"], "sample_69": ["    def test_iter_modules_and_files_zipfile(self):\n        with tempfile.TemporaryDirectory() as temp_dir:\n            zip_filename = Path(temp_dir) / 'test.zip'\n            with zipfile.ZipFile(zip_filename, 'w') as zipf:\n                zipf.write('module.py', 'module.py')\n            extend_sys_path([zip_filename])\n            self.import_and_cleanup('test_package.module')\n            self.assertFileFound(Path(zip_filename).joinpath('module.py'))\n", "    def test_error_cleanup(self):\n        with self.assertLogs('django.utils.autoreload', level='DEBUG') as logs:\n            try:\n                import sys\n                sys.modules['testing'] = object()\n                autoreload._error_files.append('fake_module.py')\n                raise ImportError(\"Fake error\")\n            except ImportError:\n                pass\n            autoreload.raise_last_exception()\n        for record in logs.records:\n            self.assertEqual(record.levelname, 'DEBUG')\n            self.assertIn('fake_module.py', record.msg)\n\n", "    def test_iter_all_python_module_files_with_zip(self):\n        with tempfile.TemporaryDirectory() as tmpdir:\n            zip_filename = Path(tmpdir) / 'test.zip'\n            with zipfile.ZipFile(zip_filename, 'w') as zf:\n                zf.write('test.py', arcname='subpackage/test.py')\n            \n            extend_sys_path([tmpdir])\n            self.assertFileFound(zip_filename.resolve())\n            self.import_and_cleanup('subpackage.test')\n\n            self.assertFileFound(zip_filename.resolve())\n            self.assertFileFound(Path(tmpdir, 'subpackage/test.py').resolve())  \n            \n            with zipfile.ZipFile(zip_filename, 'r') as zf:\n                zf.extractall(tmpdir)\n\n            self.assertFileFound(Path(tmpdir, 'subpackage/test.py').resolve())\n\n", "    def test_iter_modules_and_files_with_zip_file(self):\n        with tempfile.TemporaryDirectory() as tempdir:\n            zip_filename = self.temporary_file('module.zip')\n            with zipfile.ZipFile(zip_filename, 'w') as zf:\n                zf.writestr('my_module/__init__.py', '')\n                zf.writestr('my_module/my_module.py', '')\n            self.import_and_cleanup('my_module')\n            self.assertFileFound(zip_filename)\n", "    def test_iter_modules_and_files_zips(self):\n        with tempfile.TemporaryDirectory() as temp_dir:\n            zip_path = self.temporary_file('my_module.zip')\n            with zipfile.ZipFile(zip_path, 'w') as zipf:\n                zipf.write('__init__.py', '')\n                zipf.write('module.py', 'def hello():\\n\\tprint(\"Hello from zip\")')\n            extend_sys_path([temp_dir, str(zip_path)])\n            self.import_and_cleanup('my_module.module')\n            self.assertFileFound(zip_path / 'module.py')\n            zip_path_absolute = zip_path.resolve()\n            self.assertFileFound(zip_path_absolute)\n\n\n\n"], "sample_806": ["def check_regression_toy(loss):\n    # Check regression on a toy dataset.\n    reg = GradientBoostingRegressor(loss=loss, n_estimators=10,\n                                     random_state=1)\n    assert_raises(ValueError, reg.predict, T)\n\n    reg.fit(X, y)\n    assert_array_almost_equal(reg.predict(T), true_result)\n    assert_equal(10, len(reg.estimators_))\n\n    deviance_decrease = (reg.train_score_[:-1] - reg.train_score_[1:])\n    assert np.any(deviance_decrease >= 0.0)\n\n\n", "def test_gradient_boosting_estimator_init():\n    # The estimator should be initialized correctly\n    estimator = GradientBoostingClassifier(n_estimators=10,\n                max_depth=2, random_state=0)\n\n    assert isinstance(estimator.loss_, BaseEstimator)\n    assert isinstance(estimator.init_, ZeroEstimator)\n    estimator = GradientBoostingRegressor(n_estimators=10,\n                max_depth=2, random_state=0)\n    assert isinstance(estimator.loss_, BaseEstimator)\n    assert isinstance(estimator.init_, ZeroEstimator)\n", "    def test_classification_loss_huber(presort, loss):\n        # Check classification with Huber loss\n        clf = GradientBoostingClassifier(loss='huber', n_estimators=10,\n                random_state=1, presort=presort)\n        clf.fit(X, y)\n        assert_array_equal(clf.predict(T), true_result)\n", "compilation error", "compilation error"], "sample_1066": ["    def test_MatrixElement(self):\n        m = Matrix([[1, 2], [3, 4]])\n        me = m[0, 1]\n        self.assertEqual(mp._print(me), '<msub><mfenced><mrow><mn>1</mn><mspace width=\"0.25em\"></mspace><mn>2</mn><mspace width=\"0.25em\"></mspace><mn>3</mn><mspace width=\"0.25em\"></mspace><mn>4</mn></mrow></mfenced><mn>0</mn></msub>')\n\n\n\n", "    def test_mathml_printing_of_special_functions(self):\n        self.assertEqual(mathml(EllipticF(1, 1/2, 1/2)).encode(),\n                         b\"<apply><elliptic_f><cn>1</cn><cn>0.5</cn><cn>0.5</cn></apply>\")\n        self.assertEqual(mathml(EllipticE(1, 1/2, 1/2)).encode(),\n                         b\"<apply><elliptic_e><cn>1</cn><cn>0.5</cn><cn>0.5</cn></apply>\")\n        self.assertEqual(mathml(EllipticPi(1, 1/2, 1/2)).encode(),\n                         b\"<apply><elliptic_pi><cn>1</cn><cn>0.5</cn><cn>0.5</cn></apply>\")\n        self.assertEqual(mathml(Ei(1)).encode(),\n                         b\"<apply><Ei/><cn>1</cn></apply>\")\n        self.assertEqual(mathml(expint(1, 1)).encode(),\n                         b\"<apply><expint/><cn>1</cn><cn>1</cn></apply>\")\n        self.assertEqual(mathml(jacobi(1, 1, 1, 1, 2)).encode(),\n                         b\"<apply><jacobi><cn>1</cn><cn>1</cn><cn>1</cn><cn>1</cn><cn>2</cn></apply>\")\n        self.assertEqual(mathml(gegenbauer(1, 1, 1)).encode(),\n                         b\"<apply><gegenbauer><cn>1</cn><cn>1</cn><cn>1</cn></apply>\")\n        self.assertEqual(mathml(chebyshevt(1, 1)).encode(),\n                         b\"<apply><chebyshevt><cn>1</cn><cn>1</cn></apply>\")\n        self.assertEqual(mathml(chebyshevu(1, 1)).encode(),\n                         b\"<apply><chebyshevu><cn>1</cn><cn>1</cn></apply>\")\n        self.assertEqual(mathml(legendre(1, 1)).encode(),", "    def test_mathml_complex_functions(self):\n        x = Symbol('x', complex=True)\n        test_cases = [\n            (Abs(x), '<apply><abs/></apply>'),\n            (re(x), '<apply><re/></apply>'),\n            (im(x), '<apply><im/></apply>'),\n            (conjugate(x), '<apply><cn type=\"complex\">&#x3B8;</cn></apply>'),\n            (exp(x), '<apply><exp/></apply>'),\n            (sin(x), '<apply><sin/></apply>'),\n            (cos(x), '<apply><cos/></apply>'),\n            (tan(x), '<apply><tan/></apply>'),\n            (asin(x), '<apply><asin/></apply>'),\n            (acos(x), '<apply><acos/></apply>'),\n            (atan(x), '<apply><atan/></apply>'),\n            (sinh(x), '<apply><sinh/></apply>'),\n            (cosh(x), '<apply><cosh/></apply>'),\n            (tanh(x), '<apply><tanh/></apply>'),\n            (asinh(x), '<apply><asinh/></apply>'),\n            (acosh(x), '<apply><acosh/></apply>'),\n            (atanh(x), '<apply><atanh/></apply>'),\n        ]\n        for expr, expected in test_cases:\n            with self.subTest(expr=expr):\n                self.assertEqual(mp._print(expr), expected)\n\n\n", "    def test_complex_functions(self):\n        for func in [re, im, Abs, conjugate]:\n            self.assertEqual(\n                mp._print(func(x + 1j)),\n                mpp._print(func(x + 1j))\n            )\n\n\n", "    def test_complex_functions(self):\n        self.assertEqual(mpp._print(re(2 + 3j)), \"<mi>Re</mi><mfenced><mrow><mn>2</mn><mo>+</mo><mn>3</mn><mi>j</mi></mrow></mfenced>\")\n        self.assertEqual(mpp._print(im(2 + 3j)), \"<mi>Im</mi><mfenced><mrow><mn>2</mn><mo>+</mo><mn>3</mn><mi>j</mi></mrow></mfenced>\")\n        self.assertEqual(mpp._print(Abs(2 + 3j)), \"<mrow><mo>|</mo><mn>2</mn><mo>+</mo><mn>3</mn><mi>j</mi><mo>|</mo></mrow>\")\n        self.assertEqual(mpp._print(conjugate(2 + 3j)), \"<mi>Conjugate</mi><mfenced><mrow><mn>2</mn><mo>+</mo><mn>3</mn><mi>j</mi></mrow></mfenced>\")\n\n\n\n"], "sample_495": ["    def test_page_object_creation(self):\n        articles = [\n            Article(title='Article 1', pub_date=datetime(2022, 1, 1)),\n            Article(title='Article 2', pub_date=datetime(2022, 2, 1)),\n            Article(title='Article 3', pub_date=datetime(2022, 3, 1)),\n        ]\n        paginator = Paginator(articles, per_page=2)\n        page1 = paginator.page(1)\n        page2 = paginator.page(2)\n\n        self.assertEqual(page1.object_list[0].title, 'Article 1')\n        self.assertEqual(page2.object_list[0].title, 'Article 2')\n        self.assertEqual(page1.number, 1)\n        self.assertEqual(page2.number, 2)\n", "    def test_page_object_creation(self):\n        object_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n        paginator = Paginator(object_list, 3)\n        page = paginator.page(2)\n        self.assertEqual(len(page), 3)\n        self.assertEqual(page[0], 4)\n        self.assertEqual(page.number, 2)\n        self.assertTrue(page.has_previous())\n        self.assertTrue(page.has_next())\n        self.assertEqual(page.start_index(), 3)\n        self.assertEqual(page.end_index(), 6)\n\n\n", "    def test_page_range_empty_paginator(self):\n        paginator = Paginator([], 10)\n        self.assertEqual(paginator.page_range, [])\n", "    def test_page_object_creation(self):\n        object_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n        paginator = Paginator(object_list, 3)\n        page1 = paginator.page(1)\n        self.assertIsInstance(page1, Page)\n        self.assertEqual(len(page1), 3)\n        self.assertEqual(page1.number, 1)\n        self.assertEqual(page1.paginator, paginator)\n        self.assertEqual(page1[0], 1)\n        self.assertTrue(page1.has_next())\n        self.assertFalse(page1.has_previous())\n\n        page3 = paginator.page(3)\n        self.assertEqual(len(page3), 1)\n        self.assertEqual(page3.number, 3)\n        self.assertFalse(page3.has_next())\n        self.assertTrue(page3.has_previous())\n", "    def test_get_page_validation(self):\n        paginator = Paginator(\n            [1, 2, 3, 4, 5], per_page=2,\n        )\n        self.assertEqual(paginator.get_page(1), paginator.page(1))\n        self.assertEqual(paginator.get_page(2), paginator.page(2))\n        self.assertEqual(paginator.get_page(3), paginator.page(3))\n        self.assertEqual(paginator.get_page('invalid'), paginator.page(1))\n        self.assertEqual(paginator.get_page(0), paginator.page(1))\n        self.assertEqual(paginator.get_page(4), paginator.page(2))\n        with self.assertRaises(EmptyPage):\n            paginator.get_page(5)\n"], "sample_1200": ["    def test_get_dimensional_expr(self):\n        us = SI\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            expr = us.meter * us.second**2 / (us.kg * us.meter)\n            dim_expr = us.get_dimensional_expr(expr)\n            self.assertEqual(dim_expr, us.m**2 / (us.kg * us.m))\n\n\n\n", "    def test_get_units_non_prefixed(self):\n        self.assertEqual(set(SI.get_units_non_prefixed()), {\n            meter, second, kilogram,\n        })\n        self.assertEqual(set(SI.get_units_non_prefixed()) - {meter, second, kilogram}, set())\n\n\n\n", "    def test_get_dimensional_expr_derivative(self):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            expr = diff(m*speed_of_light, t)\n            scale_factor, dimension = SI.get_dimensional_expr(expr)\n            assert isinstance(scale_factor, Number)\n            assert dimension.name == 'length/time'\n\n\n", "    def test_get_dimensional_expr(self):\n        system = SI\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            expr_list = [\n                1,\n                2*meter,\n                (meter**2),\n                (kilogram**2) * (meter**2) / (second**2),\n                (speed_of_light**2) * time,\n                (meter/second)**2,\n                sin(meter),\n                exp(meter),\n                log(meter),\n                sqrt(meter),\n                add(2*meter, 3*meter),\n                diff(2*meter, time),\n                Function(\"f\")(meter, second),\n            ]\n            for expr in expr_list:\n                dim_expr = system.get_dimensional_expr(expr)\n                print(f\"Expression: {expr}\")\n                print(f\"Dimension: {dim_expr}\\n\")\n\n", "    def test_get_dimensional_expr_with_quantity(self):\n        with warns_deprecated_sympy(\n            \"The 'get_dimensional_expr' method will be \"\n            \"'moved' to Quantity.__repr__ in future versions.\"\n        ):\n            q = Quantity(Symbol(\"x\"), meter)\n            self.assertEqual(SI().get_dimensional_expr(q), \"meter\")\n\n"], "sample_100": ["    def test_iter_all_python_module_files_with_zipfile(self):\n        with tempfile.TemporaryDirectory() as temp_dir:\n            zip_file = self.temporary_file('my_module.zip')\n            with zipfile.ZipFile(zip_file, 'w') as zf:\n                zf.writestr('__init__.py', '')\n                zf.writestr('my_module.py', 'def hello(): pass')\n            extend_sys_path([temp_dir])\n\n            self.import_and_cleanup('my_module')\n            self.assertFileFound(zip_file)\n\n            # Now remove the zipfile, ensure it's still in the cache\n            os.remove(zip_file)\n            self.assertFileFound(zip_file)\n", "    def test_iter_all_python_module_files_empty(self):\n        # Test empty case\n        self.assertEqual(set(autoreload.iter_all_python_module_files()), set())\n", "    def test_file_changed_signal(self):\n        filename = self.temporary_file('test_file.py')\n        with open(filename, 'w') as f:\n            f.write('from django.utils.autoreload import file_changed')\n        self.assertFileFound(filename)\n        with patch.object(autoreload, 'file_changed', new_callable=mock.MagicMock()) as mocked_signal:\n            autoreload._exception = Exception\n            with open(filename, 'a') as f:\n                f.write('\\nprint(\"Modified\")')\n            time.sleep(0.1)  # Give time for changes to be detected\n            self.assertTrue(mocked_signal.called)\n            ", "    def test_error_files_persistence(self):\n        filename = self.temporary_file('test.py')\n        with open(filename, 'w') as f:\n            f.write('raise ValueError(\"Test exception\")')\n        self.import_and_cleanup('test_module')\n        self.assertFileNotFound(filename)\n        with open(filename, 'w') as f:\n            f.write('print(\"This is a valid file\")')\n        self.assertFileFound(filename)\n        self.assertFileFound(filename)  # Test that the file is still present after a cache clear.\n\n\n\n        \n", "    def test_watchman_reloader_with_existing_files(self):\n        with tempfile.TemporaryDirectory() as temp_dir:\n            filename = self.temporary_file(temp_dir, 'test_file.py')\n            with open(filename, 'w') as f:\n                f.write('def test_function():\\n    return 42\\n')\n            py_compile.compile(filename)\n\n            self.assertFileFound(filename)\n            with mock.patch('django.utils.autoreload.get_reloader') as mock_get_reloader:\n                mock_get_reloader.return_value = mock.MagicMock(spec=autoreload.WatchmanReloader)\n                autoreload.run_with_reloader(lambda: self.import_and_cleanup('test_file'))\n                    \n            mock_get_reloader.assert_called_once()\n"], "sample_470": ["    def test_lazy_object_with_callable_setup_function(self):\n        class MyCallable:\n                return 123\n\n        lazy_object = SimpleLazyObject(MyCallable())\n        self.assertEqual(lazy_object._wrapped, 123)\n        self.assertEqual(lazy_object.__repr__(), \"<SimpleLazyObject: <function __main__.MyCallable object at ...>>\")\n\n\n\n", "    def test_lazy_basic(self):\n        @lazy\n            return x * 2\n\n        result = double(5)\n        self.assertEqual(result, 10)\n", "    def test_lazy(self):\n        @lazy\n            return x + 1\n\n        lazy_value = add_one(5)\n        self.assertIsInstance(lazy_value, Promise)\n        self.assertEqual(lazy_value.__cast(), 6)\n        self.assertNotEqual(lazy_value, add_one(6))\n\n        @lazy(int)\n            return x + 2.5\n\n        lazy_value = add_two_and_half(5)\n        self.assertIsInstance(lazy_value, Promise)\n        self.assertEqual(lazy_value.__cast(), 7.5)\n", "    def test_lazy(self):\n        @lazy\n            return x * 2\n\n        result = double(5)\n        self.assertEqual(result, 10)\n        self.assertEqual(double(5), 10)\n", "    def test_lazy_text(self):\n        @lazystr\n            return \"Hello, world!\"\n\n        self.assertEqual(my_func(), \"Hello, world!\")\n        with self.assertRaises(TypeError):\n            my_func()\n"], "sample_947": ["def check_alias_to_pre_v3(name, input, idDict, output, key=None, asTextOutput=None):\n    if output is None:\n        output = input\n    if not name.startswith(\"alias\"):\n        return\n    # First, check without semicolon\n    _check(name, input, idDict, output, key, asTextOutput)\n\n\n\n    parser = DefinitionParser(input + ' ;', location=None, config=None)\n    parser.allowFallbackExpressionParsing = False\n    ast = parser.parse_declaration(\"alias\", name)\n    parser.assert_end()\n    rootSymbol = Symbol(None, None, None, None, None)\n    symbol = rootSymbol.add_declaration(ast, docname=\"TestDoc\", line=42)\n\n    parentNode = addnodes.desc()\n    signode = addnodes.desc_signature(input, '')\n    parentNode += signode\n    ast.describe_signature(signode, 'lastIsName', symbol, options={})\n    resAsText = parentNode.astext()\n    if resAsText != output:\n        print(\"\")\n        print(\"Input:    \", input)\n        print(\"astext(): \", resAsText)\n        print(\"Expected: \", output)\n        raise DefinitionError(\"\")\n\n    \n    idExpected = [None]\n    for i in range(1, _max_id + 1):\n        if i in idDict:\n            idExpected.append(idDict[i])\n        else:\n            idExpected.append(idExpected[i - 1])\n    idActual = [None]\n    for i in range(1, _max_id + 1):\n        id = ast.get_id(version=i)\n        assert id is not None\n        idActual.append(id[len(_id_prefix[i]):])\n\n    res = [True]\n    for i in range(1, _max_id + 1):\n        res.append(idExpected[i] == idActual[i])\n\n    if not all(res):\n        print(\"input:    %s\" % input.rjust(20))\n        for i in range(1, _max_id + 1):\n            if res", "compilation error", "def check(name, input, idDict, output=None, key=None, asTextOutput=None):\n    if output is None:\n        output = input\n    # First, check without semicolon\n    _check(name, input, idDict, output, key, asTextOutput)\n    if name != 'macro':\n        # Second, check with semicolon\n        _check(name, input + ' ;', idDict, output + ';', key,\n               asTextOutput + ';' if asTextOutput is not None else None)\n\n", "    def test_function_with_varargs_and_kwargs( ):\n        input = \"int my_function(int a, ... , **kwargs)\"\n        output = \"int my_function(int a, ... , **kwargs)\"\n        idDict = {1 : 'my_function'}\n        check('function', input, idDict, output)\n\n\n", "    def check_alias(name, input, target, key, asTextOutput):\n        if key is None:\n            key = name\n        key += ' '\n        inputActual = input\n        outputAst = output.format(key='')\n        outputAsText = output.format(key=key)\n\n        # first a simple check of the AST\n        ast = parse(name, inputActual)\n        res = str(ast)\n        if res != outputAst:\n            print(\"\")\n            print(\"Input:    \", input)\n            print(\"Result:   \", res)\n            print(\"Expected: \", outputAst)\n            raise DefinitionError(\"\")\n        rootSymbol = Symbol(None, None, None, None, None)\n        symbol = rootSymbol.add_declaration(ast, docname=\"TestDoc\", line=42)\n        parentNode = addnodes.desc()\n        signode = addnodes.desc_signature(input, '')\n        parentNode += signode\n        ast.describe_signature(signode, 'lastIsName', symbol, options={})\n        resAsText = parentNode.astext()\n        if resAsText != outputAsText:\n            print(\"\")\n            print(\"Input:    \", input)\n            print(\"astext(): \", resAsText)\n            print(\"Expected: \", outputAsText)\n            raise DefinitionError(\"\")\n\n\n"], "sample_994": ["    def test_power_nan():\n        assert isinstance(nan**2, Float)\n        assert nan**2 == nan\n        assert isinstance(oo**oo, nan)\n        assert isinstance(oo**-oo, nan)\n        assert isinstance((-oo)**oo, nan)\n        assert isinstance((-oo)**-oo, nan)\n\n\n\n", "    def test_floor_ceil_integer():\n        assert Integer(3).floor() == Integer(3)\n        assert Integer(3).ceil() == Integer(3)\n        for x in [1, 2, 3, -1, -2, -3]:\n            assert Integer(x).floor() == Integer(x)\n            assert Integer(x).ceil() == Integer(x)\n", "    def test_sympify_complex():\n        assert sympify(complex(1, 2)) == 1 + 2*I\n        assert sympify(complex(-1, -2)) == -1 - 2*I\n        assert sympify(complex(0, 0)) == 0\n        assert sympify(complex(1.1, 2.2)) == 1.1 + 2.2*I\n    ", "    def test_sympify_complex():\n        assert sympify_complex(complex(1, 2)) == 1 + 2*I\n        assert sympify_complex(complex('1+2j')) == 1 + 2*I\n        assert sympify_complex(complex(1.1, 2.2)) == Float('1.1') + Float('2.2')*I\n        assert sympify_complex(complex(1, -2)) == 1 - 2*I\n        assert sympify_complex(complex(0, 0)) == 0\n        assert sympify_complex(complex('.5', '.75')) == Float('0.5') + Float('0.75')*I\n  ", "    def test_sympify_mpz_non_int():\n        sympify(gmpy.mpz(1.5))\n"], "sample_419": ["    def test_formset_factory_validation(self):\n        with self.assertRaises(ValueError):\n            formset_factory(\n                Choice, max_num=10, absolute_max=5\n            )\n\n\n", "    def test_formset_factory_invalid_max_num(self):\n        with self.assertRaises(ValueError):\n            formset_factory(Choice, max_num=100, absolute_max=99)\n\n\n\n", "    def test_initial_form_count(self):\n        \"\"\"Test that the initial_form_count is set correctly.\"\"\"\n        formset = self.make_choiceformset(\n            formset_data=[(\"A\", 1), (\"B\", 2)],\n            initial_forms=2,\n        )\n        self.assertEqual(formset.initial_form_count(), 2)\n\n        formset = self.make_choiceformset(\n            formset_data=[(\"A\", 1), (\"B\", 2)],\n            initial_forms=1,\n        )\n        self.assertEqual(formset.initial_form_count(), 1)\n\n        formset = self.make_choiceformset(formset_data=[])\n        self.assertEqual(formset.initial_form_count(), 0)\n\n\n", "    def test_empty_formset(self):\n        formset = ChoiceFormSet(prefix=\"choices\")\n        self.assertEqual(formset.total_form_count(), 0)\n        self.assertEqual(len(formset), 0)\n        self.assertEqual(formset.initial_form_count(), 0)\n        self.assertTrue(formset.is_valid())\n", "    def test_formset_factory_kwargs(self):\n        with self.assertLogs(\"django.forms\", level=\"WARNING\"):\n            formset = formset_factory(\n                CustomKwargForm,\n                extra=1,\n                max_num=2,\n                custom_kwarg=\"test_kwarg\",\n            )\n\n        self.assertEqual(formset.extra, 1)\n        self.assertEqual(formset.max_num, 2)\n\n\n"], "sample_528": ["    def test_user_style_override():\n        with temp_style('my_style'):\n            style.use('my_style')\n            assert mpl.rcParams[PARAM] == VALUE\n\n\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_880": ["compilation error", "compilation error", "    def test_is_multilabel_edge_cases(self):\n        for y in [\n            np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]),\n            np.array([[0, 1, 0], [0, 0, 1], [1, 0, 0]]),\n            np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]),\n            np.array([[0, 1, 1], [1, 0, 1], [1, 1, 0]]),\n            np.array([[0, 1, 1], [1, 0, 1], [1, 1, 0]]),\n        ]:\n            self.assertTrue(is_multilabel(y))\n\n        for y in [\n            np.array([[0, 1], [1, 0]]),\n            np.array([[0, 0], [0, 0]]),\n            csr_matrix([[0, 1], [1, 0]]),\n            csc_matrix([[0, 1], [1, 0]]),\n        ]:\n            self.assertFalse(is_multilabel(y))\n", "compilation error", "    def test_is_multilabel():\n        for label in EXAMPLES[\"multilabel-indicator\"]:\n            assert is_multilabel(label), f\"Expected multilabel: {label}\"\n        for label in EXAMPLES[\"multiclass\"]:\n            assert not is_multilabel(label), f\"Expected not multilabel: {label}\"\n        for label in EXAMPLES[\"multiclass-multioutput\"]:\n            assert not is_multilabel(label), f\"Expected not multilabel: {label}\"\n        for label in EXAMPLES[\"binary\"]:\n            assert not is_multilabel(label), f\"Expected not multilabel: {label}\"\n        for label in EXAMPLES[\"continuous\"]:\n            assert not is_multilabel(label), f\"Expected not multilabel: {label}\"\n        for label in EXAMPLES[\"continuous-multioutput\"]:\n            assert not is_multilabel(label), f\"Expected not multilabel: {label}\"\n\n\n\n"], "sample_668": ["    def test_deprecated_messages(monkeypatch):\n        monkeypatch.setattr(inspect, \"getsourcefile\", lambda x: \"test.py\")\n        with pytest.raises(deprecated.PytestDeprecationWarning) as warning:\n            deprecated.FUNCARGNAMES\n        assert str(warning.value).startswith(\"The `funcargnames` attribute was an alias for `fixturenames`,\")\n        with pytest.raises(deprecated.PytestDeprecationWarning) as warning:\n            deprecated.RESULT_LOG\n        assert str(warning.value).startswith(\n            \"--result-log is deprecated, please try the new pytest-reportlog plugin.\"\n        )\n        with pytest.raises(deprecated.PytestDeprecationWarning) as warning:\n            deprecated.FIXTURE_POSITIONAL_ARGUMENTS\n        assert str(warning.value).startswith(\n            \"Passing arguments to pytest.fixture() as positional arguments is deprecated - pass them \"\n        )\n", "    def test_node_use_from_parent(mocker):\n        class _Node:\n            @classmethod\n                return None\n\n        with mocker.patch.object(nodes, 'Node') as mock_node_class:\n            mock_node_class.from_parent.return_value = _Node()\n            msg = deprecated.NODE_USE_FROM_PARENT.format(name='Node')\n            with pytest.warns(UnformattedWarning, match=msg):\n                nodes.Node()\n", "    def test_deprecated_messages():\n        for name, warning in deprecated.__dict__.items():\n            if isinstance(warning, (PytestDeprecationWarning, UnformattedWarning)):\n                with pytest.warns(warning.class_):\n                    print(f\"Testing deprecated message: {name}\")\n                    # This is a very basic test, you'll likely want to\n                    # add more assertions on the warning message itself\n                    # based on your specific needs\n                    \n", "    def test_node_use_from_parent():\n        with pytest.warns(deprecated.NODE_USE_FROM_PARENT) as warnings:\n            nodes.Node(\n                name=\"test_function\",\n                parent=None,\n                function=inspect.stack()[1][0],\n            )\n        assert len(warnings) == 1\n        assert str(warnings[0].message) == (\n            \"Direct construction of {name} has been deprecated, please use {name}.from_parent.\\n\"\n            \"See https://docs.pytest.org/en/latest/deprecations.html#node-construction-changed-to-node-from-parent\"\n            \" for more details.\"\n        ).format(name=\"Node\") \n", "    def test_deprecated_warnings():\n        with pytest.raises(deprecated.PytestDeprecationWarning) as excinfo:\n            nodes.Node.from_parent()  # This should trigger the deprecated warning\n        assert str(excinfo.value) == NODE_USE_FROM_PARENT.args[0].format(name=\"Node\")\n"], "sample_839": ["    def test_stop_words_removal_with_max_df(self):\n        vectorizer = TfidfVectorizer(stop_words='english', max_df=0.8)\n        X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n        feature_names = vectorizer.get_feature_names()\n        stop_words = set(ENGLISH_STOP_WORDS)\n        for word in stop_words:\n            assert word not in feature_names\n", "    def test_tfidf_with_custom_analyzer(self):\n            return [t for t in text.split() if len(t) > 2]\n\n        tfidf = TfidfVectorizer(analyzer=my_analyzer)\n        X = tfidf.fit_transform(ALL_FOOD_DOCS)\n        assert_array_equal(tfidf.get_feature_names(), [\n            'pizza', 'burger', 'copyright', 'salad', 'celeri', 'tomato',\n            'sparkling', 'water'])\n", "    def test_lowercase(self, lowercase):\n        vectorizer = TfidfVectorizer(lowercase=lowercase)\n        X = vectorizer.fit_transform(\n            [\"This IS a document\", \"Another DOCUMENT here\"])\n        assert_array_equal(X.toarray(),\n                           np.array([\n                               [0.0, 0.0, 0.0, 0.0, 0.0],\n                               [0.0, 0.0, 0.0, 0.0, 0.0]\n                           ]))\n        if lowercase:\n            assert_array_equal(vectorizer.get_feature_names(),\n                               [b'another', b'document',\n                                b'here', b'is', b'this'])\n        else:\n            assert_array_equal(vectorizer.get_feature_names(),\n                               [b'THIS', b'DOCUMENT',\n                                b'HERE', b'IS', b'a'])\n", "    def test_TfidfVectorizer_with_binary_and_idf(self):\n        # Test case for binary=True\n        vectorizer = TfidfVectorizer(binary=True, use_idf=True)\n        X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n        assert_array_equal(X.data, np.ones(X.data.size))\n        assert_array_equal(vectorizer.idf_, np.ones(len(vectorizer.vocabulary_)))\n", "    def test_fixed_vocabulary_with_invalid_length(self):\n        vectorizer = TfidfVectorizer(vocabulary={'a': 0, 'b': 1, 'c': 2})\n\n        with pytest.raises(ValueError):\n            vectorizer.fit_transform(['a'])\n        with pytest.raises(ValueError):\n            vectorizer.fit_transform(['a', 'd'])\n\n"], "sample_844": ["def test_extend_region(r_plot, end):\n    n_samples = len(r_plot)\n    steep_point = np.array([True] * n_samples)\n    xward_point = np.array([True] * n_samples)\n    start = 0\n    result_index, result_end = _extend_region(steep_point, xward_point,\n        start, 2)\n\n    assert result_index == n_samples\n    assert result_end == end\n", "def test_extend_region(r_plot, end):\n    n = len(r_plot)\n    xward_point = np.ones(n)\n    steep_point = np.ones(n)\n    s = 0\n    e = n - 1\n    res_s, res_e = _extend_region(steep_point, xward_point, s, 1)\n    assert res_s == s and res_e == e\n    res_s, res_e = _extend_region(steep_point, xward_point, s, 2)\n    assert res_s == s and res_e == e\n    for start in range(1, n):\n        steep_point[start:n] = False\n        res_s, res_e = _extend_region(steep_point, xward_point, start, 2)\n        assert res_s == start and res_e == end\n        if start > end:\n            res_s, res_e = _extend_region(steep_point, xward_point, start, 2)\n            assert res_s == start and res_e == end\n        else:\n            res_s, res_e = _extend_region(steep_point, xward_point, start, 2)\n            assert res_s == start and res_e == end\n\n\n\n", "def test_extend_region(r_plot, end):\n    n_samples = len(r_plot)\n    steep_upward = np.ones(n_samples, dtype=bool)\n    downward = np.zeros(n_samples, dtype=bool)\n    xward_point = np.zeros(n_samples, dtype=bool)\n    start = 0\n    calculated_end = _extend_region(steep_upward, downward, start, 2)\n    assert calculated_end == end\n", "def test_xi_cluster_larger_clusters():\n    \"\"\"Test that larger clusters are correctly identified.\"\"\"\n    n_samples = 50\n    centers = [[0, 0], [2, 2], [4, 4]]\n    X = make_blobs(n_samples=n_samples, centers=centers,\n                   cluster_std=0.5,\n                   shuffle=True, random_state=0)[0]\n    reachability, predecessor, ordering = compute_optics_graph(\n        X, min_samples=5, max_eps=np.inf)\n    clusters = _xi_cluster(reachability[ordering],\n                           predecessor[ordering], ordering, xi=0.05,\n                           min_samples=5, min_cluster_size=5,\n                           predecessor_correction=True)\n    assert len(clusters) > 1\n    for cluster in clusters:\n        assert cluster[1] - cluster[0] >= 5  # larger clusters\n", "    def test_get_points_by_cluster_labels(r_plot, end):\n        ordering = np.arange(len(r_plot))\n        predicted_labels = np.array([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 0,\n                                     0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2,\n                                     0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2])\n        labels = _extract_xi_labels(ordering, _xi_cluster(r_plot,\n                  [0]*len(r_plot), ordering, 0.1, 2, 2, False))\n        assert_array_equal(labels, predicted_labels)\n"], "sample_607": ["    def test_guess_engine_no_match(dummy_duplicated_entrypoints):\n        with mock.patch(\"xarray.backends.pkg_resources.iter_entry_points\") as mock_iter_entry_points:\n            mock_iter_entry_points.return_value = dummy_duplicated_entrypoints\n            with pytest.raises(ValueError) as exc:\n                plugins.guess_engine(\"fake://file.nc\")\n            assert (\n                \"did not find a match in any of xarray's currently installed IO backends\"\n                \" \" in str(exc.value)\n            )\n", "    def test_get_backend_and_call_open_dataset(\n        dummy_duplicated_entrypoints, engine, keyword_args, positional_args", "    def test_detect_parameters(\n        backend_cls, kwargs, expected_params", "    def test_guess_engine(dummy_duplicated_entrypoints):\n        with mock.patch(\"xarray.backends.list_engines\", return_value={}):\n            with pytest.raises(ValueError, match=\"xarray is unable to open this file\"):\n                plugins.guess_engine(\"dummy.nc\")\n", "    def test_remove_duplicates(dummy_duplicated_entrypoints):\n        pkg_entrypoints = remove_duplicates(dummy_duplicated_entrypoints)\n        assert len(pkg_entrypoints) == 2\n        assert pkg_entrypoints[0].name == \"engine1\"\n        assert pkg_entrypoints[1].name == \"engine2\"\n\n"], "sample_115": ["    def test_callable_setting_wrapper_calls_callable(self):\n            return 'foo'\n\n        wrapper = CallableSettingWrapper(callable_setting)\n        self.assertIsInstance(wrapper, CallableSettingWrapper)\n        self.assertEqual(wrapper(), 'foo')\n\n", "    def test_callable_setting_wrapper_as_string(self):\n            return 'hello'\n\n        wrapped_callable = CallableSettingWrapper(my_callable)\n        self.assertEqual(repr(wrapped_callable), '<function my_callable at 0x...>')\n        self.assertEqual(str(wrapped_callable), 'my_callable')  \n\n\n", "    def test_callable_setting_wrapper_does_not_call_wrapped(self):\n        \"\"\"\n        CallableSettingWrapper should not call the wrapped callable.\n        \"\"\"\n        wrapped = mock.Mock()\n        wrapper = CallableSettingWrapper(wrapped)\n\n        wrapper.__repr__()  # This should call __init__ without executing wrapped\n\n        self.assertFalse(wrapped.called)\n\n\n\n", "    def test_callable_setting_wrapper(self):\n        callable_setting = lambda: 'hello'\n        wrapper = CallableSettingWrapper(callable_setting)\n        self.assertIs(wrapper._wrapped, callable_setting)\n        self.assertEqual(wrapper(), 'hello')\n\n\n\n", "def test_CallableSettingWrapper(self):\n        class CallableSetting(object):\n                self.called = False\n\n                self.called = True\n                return 'callable_value'\n        setting = CallableSetting()\n        wrapper = CallableSettingWrapper(setting)\n        self.assertFalse(setting.called)\n        self.assertEqual(wrapper(), 'callable_value')\n        self.assertTrue(setting.called)\n        self.assertEqual(repr(wrapper), repr(setting))\n"], "sample_294": ["    def test_set_csrf_cookie_for_different_domains(self):\n        with override_settings(CSRF_TRUSTED_ORIGINS=['https://example.com', 'https://subdomain.example.com']):\n            req = TestingHttpRequest()\n            req.META['HTTP_ORIGIN'] = 'https://subdomain.example.com'\n            req.is_secure = True\n            res = ensure_csrf_cookie_view(req)\n            self.assertEqual(res.status_code, 200)\n            self.assertIn(\n                f'Set-Cookie: csrftoken={self._csrf_id_cookie};',\n                res.headers['Set-Cookie'],\n            )\n\n\n\n            \n\n\n\n", "    def test_csrf_checks_multiple_origins(self):\n        with override_settings(\n                CSRF_TRUSTED_ORIGINS=['https://subdomain.example.com', 'https://*.example.com'],\n                CSRF_COOKIE_DOMAIN='example.com',\n                SESSION_COOKIE_DOMAIN='example.com',\n        ):\n            req = TestingHttpRequest()\n            req.is_secure = True\n            req.META['HTTP_ORIGIN'] = 'https://subdomain.example.com'\n            req.META['HTTP_REFERER'] = 'https://subdomain.example.com/login'\n            response = self.get_response(req, token_view)\n            self.assertEqual(response.status_code, 200)\n            self._check_token_present(response)\n\n            req.META['HTTP_ORIGIN'] = 'https://www.example.com'\n            response = self.get_response(req, token_view)\n            self.assertEqual(response.status_code, 200)\n            self._check_token_present(response)\n\n            req.META['HTTP_ORIGIN'] = 'https://notallowed.example.com'\n            response = self.get_response(req, token_view)\n            self.assertEqual(response.status_code, 403)\n            self.assertIn(REASON_BAD_ORIGIN, str(response.content))\n\n            req.META['HTTP_ORIGIN'] = ''\n            req.META['HTTP_REFERER'] = 'https://www.example.com'\n            response = self.get_response(req, token_view)\n            self.assertEqual(response.status_code, 403)\n            self.assertIn(REASON_NO_REFERER, str(response.content))\n", "    def test_reject_bad_referer(self):\n        req = TestingHttpRequest()\n        req.method = \"POST\"\n        req.META['HTTP_ORIGIN'] = 'https://example.invalid'\n        req.META['HTTP_REFERER'] = 'http://otherdomain.com'\n        with self.assertRaises(django.http.HttpResponseForbidden) as cm:\n            csfm = CsrfViewMiddleware()\n            csfm.process_view(req, lambda r: None, (), {})\n        self.assertEqual(cm.exception.reason, REASON_BAD_REFERER % 'http://otherdomain.com')\n\n", "    def test_reject_bad_referer(self):\n        req = self._get_POST_csrf_cookie_request()\n        req.META['HTTP_REFERER'] = 'http://evil.example.com/bad_referer'\n        middleware = CsrfViewMiddleware()\n        response = middleware.process_view(req, non_token_view_using_request_processor, (), {})\n        self.assertEqual(response.status_code, 403)\n        self.assertIn(REASON_BAD_REFERER % req.META['HTTP_REFERER'], response.content)\n\n", "    def test_referrer_checking_fails_on_invalid_referer(self):\n        with override_settings(\n            CSRF_TRUSTED_ORIGINS=['https://example.com'],\n            CSRF_COOKIE_DOMAIN='example.com',\n        ):\n            req = self._get_POST_csrf_cookie_request()\n            req.META['HTTP_REFERER'] = 'https://evil.com/malicious'\n            middleware = CsrfViewMiddleware()\n            mock_view = lambda *args, **kwargs: HttpResponse('OK')\n            response = middleware.process_view(req, mock_view, (), {})\n            self.assertEqual(response.status_code, 403)\n            self.assertIn(REASON_BAD_REFERER % 'https://evil.com/malicious', response.content)\n\n"], "sample_628": ["    def test_private_dict_file(self):\n        self.checker.open()\n        original_word = \"thiswordisnotinmydict\"\n        with open(self.checker.config.spelling_private_dict_file, \"r\") as f:\n            self.assertNotIn(original_word, f.read())\n        self.checker._check_spelling(\n            \"wrong-spelling-in-comment\",\n            original_word,\n            1,\n        )\n        with open(self.checker.config.spelling_private_dict_file, \"r\") as f:\n            self.assertIn(original_word, f.read())\n\n", "    def test_ignore_words(self):\n        self.checker.ignore_list = [\"the\", \"correct\"]\n        code = \"\"\"\n        This is a test with the word the and the word correct.\n        \"\"\"\n        tokens = _tokenize_str(code)\n        self.checker.process_tokens(tokens)\n        self.assert_no_messages(\"wrong-spelling-in-comment\")\n        self.assert_no_messages(\"wrong-spelling-in-docstring\")\n", "    def test_skipping_private_dict_words(self):\n        self.add_message(\n            message=Message(\n                \"wrong-spelling-in-comment\",\n                line_number=1,\n                args=(\"nonword\",),\n            ),\n            is_warning=True,\n        )\n        self.checker = self.construct_checker()\n        # Check that words in private dictionary are not flagged as errors\n        with open(\"test_spelling_private_dict.txt\", \"w\") as f:\n            f.write(\"nonword\\n\")\n        set_config(spelling_private_dict_file=\"test_spelling_private_dict.txt\")\n        self.checker.open()\n        tokens = _tokenize_str(\n            \"\"\"\n            # nonword is a great word!\n            \"\"\"\n        )\n        self.checker.process_tokens(tokens)\n        self.assert_len_equal(self.msgs, 0)\n        os.remove(\"test_spelling_private_dict.txt\")\n\n\n", "    def test_ignoring_comments(self):\n        code = \"\"\"", "    def test_spelling_ignore_words(self):\n        self.add_message(\n            Message(\n                \"wrong-spelling-in-comment\",\n                (1, 0),\n                (1, 8),\n                \"The word 'onwer' should be 'owner'.\",\n                {\"word\": \"onwer\"},\n            )\n        )\n\n        with set_config(spelling_ignore_words=\"owner\"):\n            self._assert_no_messages(\n                _tokenize_str(\n                    \"\"\"\n                    # Incorrect spelling 'onwer' in comment\n                    \"\"\"\n                )\n            )\n"], "sample_831": ["    def test_export_graphviz_with_special_characters(self):\n        tree = DecisionTreeClassifier().fit(X, y)\n        with StringIO() as buf:\n            export_graphviz(tree, out_file=buf, special_characters=True)\n            dot_data = buf.getvalue()\n\n        assert \"&#35;\" in dot_data\n        assert \"<SUB>\" in dot_data\n        assert \"</SUB>\" in dot_data\n        assert \"<= \" in dot_data\n        assert '\"\\n\"' in dot_data\n        assert '\" ' in dot_data\n\n\n\n\n", "    def test_export_text_multi_output(self):\n        tree = DecisionTreeRegressor(n_outputs=2, max_depth=2)\n        tree.fit(X, y2)\n        #export_text\n        text = export_text(tree, show_weights=True, max_depth=2)\n        assert_equal(tree.n_outputs, 2)\n        #check if there is any output about the two outputs\n        assert_in('value: ', text)\n        assert_in('weights: ', text)\n        check_if_output_has_number (text)\n", "    def test_export_graphviz_with_special_characters(self):\n        tree = DecisionTreeClassifier().fit(X, y)\n        from io import StringIO\n        with StringIO() as buf:\n            export_graphviz(tree, out_file=buf, special_characters=True)\n        dot_data = buf.getvalue()\n        assert '&#35;' in dot_data  # Check for special character used for #\n        assert '&#40;' in dot_data  # Check for special character used for (\n\n\n", "compilation error", "compilation error"], "sample_1207": ["def test_implicit_multiplication_application():\n    \"\"\"Test cases for implicit_multiplication_application.\"\"\"\n\n    eq1 = parse_expr(\"x*y**2 + sin(x)\",\n                      transformations=(standard_transformations +\n                      (implicit_multiplication_application,)))\n    assert eq1 == Mul(x, y, 2) + sin(x)\n\n    eq2 = parse_expr(\"x*y**2 z\",\n                      transformations=(standard_transformations +\n                      (implicit_multiplication_application,)))\n    assert eq2 == Mul(x, Mul(y, 2), z)\n\n    eq3 = parse_expr(\"(x*y)**2\",\n                      transformations=(standard_transformations +\n                      (implicit_multiplication_application,)))\n    assert eq3 == Pow(Mul(x, y), 2)\n\n\n", "    def test_implicit_multiplication_application_issue_15474(self):\n        code = \"a*b*c\"\n        result = parse_expr(code, transformations=(standard_transformations + (implicit_multiplication_application,)))\n        expected = Mul(a, b, c)\n        self.assertEqual(result, expected)\n\n", "    def test_implicit_multiplication_application_repeated_decimals(self):\n        expr = '1.234[5]'\n        result = parse_expr(expr, transformations=\n        (standard_transformations +\n        (implicit_multiplication_application,)))\n        expected = Float('1.2345')\n        assert result == expected\n\n", "    def test_implicit_multiplication_application_pow(self):\n        self.assertEqual(parse_expr(\"2**3*x\", transformations=(standard_transformations + (implicit_multiplication_application,))), 2**3*x)\n        self.assertEqual(parse_expr(\"2**(3*x)\", transformations=(standard_transformations + (implicit_multiplication_application,))), 2**(3*x))\n", "    def test_implicit_multiplication_application_with_parens(self):\n        expr = \"x*2*(y + 3)\"\n        result = parse_expr(expr, transformations=(standard_transformations + (implicit_multiplication_application,)))\n        expected = Mul(Symbol('x'), 2, Mul(Symbol('y'), Symbol('1'), Symbol('3')))\n        self.assertEqual(result, expected)\n"], "sample_650": ["    def test_capture_manager_disabled(self, request: FixtureRequest) -> None:\n        config = request.config\n        config.pluginmanager.get_plugin(\"capturemanager\").disabled = True\n        with Pytester(config) as p:\n            p.write(\n                \"\"\"\n                import logging\n                logging.basicConfig(level=\"DEBUG\")\n                logging.debug(\"Hello from a debug message!\")\n                \"\"\",\n                \"test_file.py\",\n            )\n            result = p.runpytest()\n            assert result.retcode == ExitCode.OK\n            output = result.stdout\n            assert \"Hello from a debug message!\" in output\n\n\n\n", "    def test_log_file_handler(self, caplog: caplog, known_good_log_file: str) -> None:\n        self.config.option.log_file = known_good_log_file\n        with self.get_test_plugin(\"logging-plugin\") as plugin:\n            plugin.set_log_path(known_good_log_file)\n            self.skip_if_disabled(\"pytest-logging\")\n\n            # Check if the log file has been created\n            self.assertTrue(os.path.exists(known_good_log_file))\n            with open(known_good_log_file, \"r\") as log_file:\n                self.assertNotEqual(log_file.read(), \"\")\n            plugin.set_log_path(None)\n\n\n\n", "    def test_log_cli_disabled(self, testdir: Pytester, caplog: LogCaptureFixture) -> None:\n        \"\"\"Test that live logging is disabled when the --log-cli flag is not set.\"\"\"\n        # No logging should occur in the terminal\n        testdir.makeconftest(\n            \"\"\"\n            from _pytest.config import Config\n            from _pytest.runner import _RunDetails\n\n            @pytest.fixture(scope=\"session\")\n                cm = super()\n                return cm\n\n                cm = config.pluginmanager.getplugin(\"capturemanager\")\n                cm.global_and_fixture_disabled()\n        \"\"\"\n        )\n        repro_py = testdir.makepyfile(\n            \"\"\"\n            import logging\n\n            logging.basicConfig(level=logging.DEBUG, format=\"%(message)s\")\n\n                logging.info(\"Hello from test!\")\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        assert result.ret == ExitCode.OK\n        assert \"Hello from test!\" not in result.stdoutstr\n\n\n", "def test_log_file_handler(tmpdir: pathlib.Path, pytester: Pytester) -> None:\n    \"\"\"Test the behavior of the LogFileHandler.\"\"\"\n    log_path = tmpdir / \"test.log\"\n    config = pytester.parseconfig(\n        \"-l\",\n        \"--log-file\",\n        str(log_path),\n        \"--log-cli-level\",\n        \"WARNING\",\n    )\n\n    # Capture output\n    with pytester.runpytest(config) as process:\n        process.stdout.seek(0)\n        process.stderr.seek(0)\n\n    assert log_path.exists()\n    with open(log_path, \"r\") as f:\n        log_content = f.read()\n    assert log_content.strip() != \"\"\n    assert \"pytest\" in log_content\n\n\n\n", "    def test_nested_logging_capture(self, caplog: LogCaptureFixture) -> None:\n        with caplog.at_level(logging.DEBUG):\n            child_logger = logging.getLogger(__name__ + \".child\")\n            child_logger.info(\"This message is from a child logger\")\n            logging.info(\"This message is from the root logger\")\n        assert \"This message is from a child logger\" in caplog.messages\n        assert \"This message is from the root logger\" in caplog.messages\n\n"], "sample_99": ["    def test_trunc_time_with_timezone(self):\n        tz = pytz.timezone('US/Pacific')\n        now = timezone_now().replace(tzinfo=tz)\n        model = self.create_model(now, now + timedelta(hours=2))\n        truncated = model.trunc_time()\n        self.assertEqual(truncated, now.time())\n", "    def test_trunc_to_date_with_timezone(self):\n        start_datetime = timezone_aware_datetime(\n            2023, 1, 1, 12, 30, 0, tzinfo=pytz.timezone('America/New_York')\n        )\n        end_datetime = timezone_aware_datetime(\n            2023, 1, 2, 18, 45, 0, tzinfo=pytz.timezone('America/New_York')\n        )\n        model = self.create_model(start_datetime, end_datetime)\n\n        result = model.start_datetime.transform(TruncDate()).resolve()\n        self.assertEqual(result, truncate_to(start_datetime, 'date', tzinfo=pytz.timezone('America/New_York')))\n\n        result = model.end_datetime.transform(TruncDate()).resolve()\n        self.assertEqual(result, truncate_to(end_datetime, 'date', tzinfo=pytz.timezone('America/New_York')))\n\n\n", "    def test_extract_year_from_duration(self):\n        now = timezone.now()\n        d = self.create_model(start_datetime=now - timedelta(days=365), end_datetime=now + timedelta(days=365))\n        self.assertEqual(ExtractYear(F('duration')).resolve(d).output_field, IntegerField())\n        \n        duration = d.duration.total_seconds()\n        expected_year = now.year\n        \n        with self.subTest(duration=duration):\n            result = ExtractYear(F('duration')).resolve(d).as_sql(None, None)\n            self.assertEqual(result[0].strip(), f\"EXTRACT(YEAR FROM '{duration}' )\")\n            self.assertEqual(result[1], ())\n", "    def test_trunc_with_timezone(self):\n        # Test truncation with timezone aware datetime\n        tz_aware_datetime = datetime(2023, 12, 25, 12, 0, 0, tzinfo=datetime_timezone.utc)\n        model = self.create_model(start_datetime=tz_aware_datetime, end_datetime=tz_aware_datetime)\n\n        # Truncate to day\n        truncated_day = model.start_datetime.dt.tzinfo\n        trunc_day = Trunc('start_datetime', 'day').resolve()\n        self.assertEqual(trunc_day.output_field, DateField())\n        truncated_value = trunc_day.as_sql(connection=self.connection)\n        self.assertEqual(truncated_value[0], truncated_day)\n        self.assertEqual(trunc_day.transform.evaluate(model), truncate_to(tz_aware_datetime, 'day'))\n\n\n", "    def test_duration_extract(self):\n        dt1 = datetime(2023, 10, 26, 10, 30, 0)\n        dt2 = datetime(2023, 10, 27, 15, 45, 0)\n        duration = dt2 - dt1\n        obj = self.create_model(start_datetime=dt1, end_datetime=dt2)\n        self.assertEqual(Extract('duration', 'days')(obj).value, duration.days)\n        self.assertEqual(Extract('duration', 'seconds')(obj).value, duration.total_seconds())\n\n"], "sample_277": ["    def test_q_merge(self):\n        q1 = Q(name__icontains='a') & Q(age__gt=20)\n        q2 = Q(name__icontains='b') | Q(age__lt=10)\n        q3 = q1 & q2\n        self.assertEqual(list(q3.children), [\n            (Q, q1),\n            ('AND', Q(name__icontains='b') | Q(age__lt=10))\n        ])\n", "    def test_resolve_expression(self):\n        q = Q(id__gt=1)\n        query = self.get_query_class().objects.filter(id__gt=1)\n        clause, joins = q.resolve_expression(query)\n        self.assertEqual(clause, query._expression)\n\n\n", "    def test_deferred_attribute(self):\n        class MyModel(models.Model):\n            name = models.CharField(max_length=50)\n            related_value = models.CharField(max_length=50)\n\n        class MyModelManager(models.Manager):\n                return self.filter(name='testing')\n\n        MyModel._meta.managers = {'deferred': MyModelManager()}\n            instance = MyModel.objects.deferred.deferred_query_set().first()\n            first_value = instance.related_value\n            second_value = instance.related_value\n\n            self.assertEqual(first_value, second_value)\n \n", "    def test_q_inheritance(self):\n        class CustomQ(Q):\n            pass\n\n        q1 = Q(name='john')\n        q2 = CustomQ(age__gt=30)\n\n        self.assertIsNotInstanceOf(q2, Q)\n        self.assertIsInstance(q2, CustomQ)\n", "    def test_Q_with_empty_args_and_kwargs(self):\n        q1 = Q()\n        self.assertEqual(q1.deconstruct(), ('query_utils.Q', (), {}))\n"], "sample_980": ["    def test_cyclic_form():\n        p = Permutation([3, 1, 4, 2])\n        assert p.cyclic_form == [(3, 1, 4, 2)]\n        p = Permutation([0, 2, 1, 3])\n        assert p.cyclic_form == [(0, 2, 1, 3)]\n        p = Permutation([0, 1, 2, 3])\n        assert p.cyclic_form == [(0, 1, 2, 3)]\n        p = Permutation([0, 1, 0, 2, 3])\n        assert p.cyclic_form == [(0, 1, 0, 2, 3)]\n    ", "    def test_inversion_vector_large():\n        for i in range(10):\n            perm = Permutation.random(5)\n            inv_vec = perm.inversion_vector()\n            reconstructed = Permutation.from_inversion_vector(inv_vec)\n            assert reconstructed == perm\n\n\n\n", "    def test_inversion_vector_roundtrip(self):\n        for i in range(5):\n            p = Permutation.random(i + 1)\n            inv_vec = p.inversion_vector()\n            p_prime = Permutation.from_inversion_vector(inv_vec)\n            assert p_prime.array_form == p.array_form\n", "    def test_from_inversion_vector_invalid(self):\n        raises(ValueError, Permutation.from_inversion_vector, 5, [3, 2, 1, 0, 1, 0])\n", "    def test_inversion_vector_roundtrip():\n        for i in range(10):\n            perm = Permutation.random(i + 1)\n            inv_vec = perm.inversion_vector()\n            new_perm = Permutation.from_inversion_vector(inv_vec)\n            assert perm == new_perm, (f\"Mismatch in roundtrip for \"\n                                      f\"permutation {perm}\")\n\n"], "sample_600": ["    def test_cf_scale_offset_coder_dask(self):\n        data = da.random.random((10, 10))\n        var = xr.DataArray(data, dims=(\"x\", \"y\"), dtype=np.float32)\n        encoding = {\"scale_factor\": 1.5, \"add_offset\": 0.2}\n        cf_var = encode_cf_variable(var, encoding=encoding)\n        decoded_var = decode_cf_variable(cf_var)\n        assert_allclose(decoded_var.data, var.data * 1.5 + 0.2)\n", "    def test_cf_scale_offset_coder_roundtrip(self):\n        data = np.array([1.0, 2.0, 3.0])\n        scale_factor = 1.5\n        add_offset = -2.0\n        var = xr.DataArray(data, dims=\"x\", name=\"test\")\n        var = encode_cf_variable(var, {\"scale_factor\": scale_factor, \"add_offset\": add_offset})\n        decoded_var = decode_cf_variable(var)\n        assert_allclose(decoded_var.data, data * scale_factor + add_offset)\n", "    def test_cf_mask_coder_conflicting_fill_values(self):\n        data = np.array([1, 2, 3], dtype=\"float32\")\n        var = xr.Variable((\"x\",), data, attrs={\"missing_value\": 0, \"_FillValue\": 1})\n        with pytest.raises(ValueError):\n            encode_cf_variable(var)\n", "    def test_cf_mask_coder_mixed_fill_values(self):\n        data = np.array([1, 2, np.nan, 3], dtype=\"float32\")\n        var = xr.Variable((\"x\",), data, attrs={\"missing_value\": np.nan, \"_FillValue\": -9999})\n\n        coder = variables.CFMaskCoder()\n        encoded_var = coder.encode(var)\n        decoded_var = coder.decode(encoded_var)\n\n        assert_identical(decoded_var.data, data)\n        assert len(encoded_var.encoding.get(\"missing_value\")) == 0\n        assert len(encoded_var.encoding.get(\"_FillValue\")) == 0\n        assert_equal(decoded_var.encoding.get(\"missing_value\"), np.nan)\n        assert_equal(decoded_var.encoding.get(\"_FillValue\"), -9999)\n\n\n\n", "    def test_cf_scale_offset_coder_roundtrip(self):\n        # Test round trip with scale_factor and add_offset\n        data = np.array([10.0, 20.0, 30.0], dtype=np.float32)\n        var = xr.Variable((\"x\",), data, attrs={\"scale_factor\": 2.0, \"add_offset\": 5.0})\n        encoded_var = encode_cf_variable(var)\n        decoded_var = decode_cf_variable(encoded_var)\n        assert_allclose(decoded_var.data, data * 2.0 + 5.0)\n        assert_equal(decoded_var.attrs[\"scale_factor\"], 2.0)\n        assert_equal(decoded_var.attrs[\"add_offset\"], 5.0)\n\n\n"], "sample_1206": ["    def test_tribonacci_constant_rewrite():\n        assert simplify(TribonacciConstant.rewrite(sqrt)).args[0] == (1/3 + (19 - 3*sqrt(33))**(1/3)/3 + (3*sqrt(33) + 19)**(1/3)/3) ", "    def test_sympify_complex():\n        from sympy.core import Complex\n        assert sympify(Complex(1, 2)) == S.One + 2*S.ImaginaryUnit\n        assert sympify(1 + 2j) == S.One + 2*S.ImaginaryUnit\n        assert sympify(Complex(1, -2)) == S.One - 2*S.ImaginaryUnit\n        assert sympify(1 - 2j) == S.One - 2*S.ImaginaryUnit\n        assert sympify(complex(1, 2)) == S.One + 2*S.ImaginaryUnit\n        assert sympify(complex(1, -2)) == S.One - 2*S.ImaginaryUnit\n", "    def test_sympify_complex():\n        from sympy.core.numbers import complex\n        assert sympify(complex(1, 2)) == 1 + 2*I\n        assert sympify(complex(3.14, -1.618)) == 3.14 - 1.618*I\n        assert sympify(complex(0, 5)) == 5*I\n        assert sympify(complex(10, 0)) == 10\n\n", "    def test_tribonacci_constant_expansion(self):\n        self.assert_equal(TribonacciConstant.expand(func=True),\n                (1 + cbrt(19 - 3*sqrt(33)) + cbrt(19 + 3*sqrt(33)))/3)\n\n", "    def test_sympify_trigonometric():\n        x = Symbol('x')\n        assert sympify(sin(x)).equals(sympify(sympify(sin(x))))\n        assert sympify(cos(x)).equals(sympify(sympify(cos(x))))\n\n\n\n"], "sample_230": ["    def test_json_field_invalid_input(self):\n        field = JSONField()\n        with self.assertRaises(ValidationError) as cm:\n            field.clean('invalid JSON')\n        self.assertEqual(cm.exception.code, 'invalid')\n", "    def test_json_field_null_value(self):\n        field = JSONField()\n        self.assertIs(field.to_python(None), None)\n", "    def test_json_field_empty_values(self):\n        field = JSONField()\n        self.assertEqual(field.to_python(''), None)\n        self.assertEqual(field.to_python(None), None)\n\n", "    def test_jsonfield_serialization(self):\n        json_field = JSONField()\n        data = {'name': 'John Doe', 'age': 30}\n        encoded = json_field.prepare_value(data)\n        decoded = json_field.to_python(encoded)\n        self.assertEqual(decoded, data)\n\n        # Test with custom encoder and decoder\n        class CustomEncoder(DjangoJSONEncoder):\n                if isinstance(obj, str):\n                    return obj.upper()\n                return super().default(obj)\n\n        class CustomDecoder(json.JSONDecoder):\n                return json.loads(s, self.strict, object_hook=self.object_hook)\n\n        json_field = JSONField(encoder=CustomEncoder, decoder=CustomDecoder)\n        data = {'name': 'john doe', 'age': 30}\n        encoded = json_field.prepare_value(data)\n        decoded = json_field.to_python(encoded)\n        self.assertEqual(decoded['name'], 'JOHN DOE')\n        self.assertEqual(decoded['age'], 30)\n\n        # Test with invalid JSON\n        with self.assertRaises(ValidationError):\n            json_field.to_python('{name: \"John Doe\"')\n\n\n\n", "    def test_json_field_empty(self):\n        field = JSONField()\n        self.assertIsNone(field.to_python(''))\n        self.assertIsNone(field.to_python(None))\n"], "sample_583": ["    def test_explicit_indexing_adapter_numpy_array(self):\n        arr = np.array([[1, 2, 3], [4, 5, 6]])\n        indexed = NumpyIndexingAdapter(arr)[B(0, 1), B(slice(1, 2))]\n        assert_array_equal(indexed, np.array([[2], [5]]))\n", "    def test_dask_adapter(self):\n        chunks = ((10, 10), (5, 5))\n        da = dask_array.from_array(np.arange(100).reshape(10, 10), chunks=chunks)\n        da_adapter = np.dask_array.NumpyIndexingAdapter(da)\n        arr = da_adapter[B(0, 2, 3, 5)]\n        assert isinstance(arr, dask_array.core.Array)\n        assert arr.shape == (10, 5)\n        assert arr.chunks == ((10, 10), (5,))\n\n", "    def test_basic_indexer_with_slicing(self, array_type):\n        if array_type == 'dask_array':\n            import dask.array as da\n            array = da.random.random((3, 4))\n        else:\n            array = array_type([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n\n        indexer = B(slice(1, 3), slice(2, 4))\n        expected = array[indexer]\n        result = array[indexer]\n        assert_array_equal(result, expected)\n\n        # Check that array.ndim > 1\n        assert array.ndim > 1\n        if array_type == 'dask_array':\n            result = indexing.VectorizedIndexer((slice(1, 3), slice(2, 4))).to_numpy()\n        else:\n            result = indexing.VectorizedIndexer((slice(1, 3), slice(2, 4))).to_numpy()\n\n        assert_array_equal(result, expected)\n", "    def test_masked_indexing_dask(self):\n        x = DataArray(np.arange(10), chunks=(2, 2, 2, 2))\n        d = {\n            'a': (('y', 'x'), x),\n            'b': (('y', 'x'), x),\n        }\n        ds = Dataset(d)\n        mask = indexing.BasicIndexer((-1, 1, 0))  # mask at index 0 and 2\n\n        with raises_regex(NotImplementedError):\n            ds['a'][mask]\n        mask = indexing.BasicIndexer((0, -1, 1))  # mask at index 1 and 2\n        with raises_regex(NotImplementedError):\n            ds['a'][mask]\n\n        # test posify mask functionality\n        indexer = indexing.BasicIndexer((0, -1, 1))\n        new_indexer = posify_mask_indexer(indexer)\n        assert new_indexer.tuple != indexer.tuple\n\n        ds['a'].array[new_indexer]  # should not crash.\n\n", "    def test_outer_indexer_with_slices_and_integers(self):\n        i = indexing.OuterIndexer([slice(0, 3), 0, 1], [3, 2, 1])\n        x = np.arange(6).reshape((3, 2, 1))\n        result = self.set_to_zero(x.copy(), i)\n        expected = np.array([\n            [[0, 0]],\n            [[0, 0]],\n        ])\n        assert_array_equal(result, expected)\n"], "sample_416": ["        def test_runshell_handles_sigint(self):\n            with mock.patch('subprocess.run', return_value=mock.Mock(returncode=0)) as mock_run:\n                with mock.patch('django.db.backends.postgresql.client.DatabaseClient.settings_to_cmd_args_env', \\\n                                return_value=([], {})):\n                    client = DatabaseClient()\n                    client.runshell([])\n                    # Verify that the original SIGINT handler was restored.\n                    self.assertEqual(signal.getsignal(signal.SIGINT), signal.SIG_DFL)\n", "    def test_settings_to_cmd_args_env_with_passfile(self):\n        settings_dict = {\n            \"HOST\": \"localhost\",\n            \"PORT\": \"5432\",\n            \"NAME\": \"mydb\",\n            \"USER\": \"myuser\",\n            \"OPTIONS\": {\n                \"passfile\": \"/path/to/passfile\",\n            },\n        }\n        args, env = self.settings_to_cmd_args_env(settings_dict)\n        self.assertEqual(args, [\n            \"psql\",\n            \"-U\", \"myuser\",\n            \"-h\", \"localhost\",\n            \"-p\", \"5432\",\n            \"mydb\",\n        ])\n        self.assertEqual(env[\"PGPASSFILE\"], \"/path/to/passfile\")\n\n", "    def test_runshell_handles_sigint(self):\n        with mock.patch('subprocess.Popen', autospec=True) as mock_popen:\n            client = DatabaseClient()\n            client.runshell([])\n\n            # Assert that SIGINT was ignored during the execution of psql\n            mock_popen.assert_has_calls([\n                mock.call(\n                    args=[\"psql\"],\n                    env={'PGPASSWORD': None, 'PGSERVICE': None, 'PGSSLMODE': None, 'PGSSLROOTCERT': None, 'PGSSLCERT': None, 'PGSSLKEY': None, 'PGPASSFILE': None},\n                    **{'stdout': ..., 'stderr': ...}\n                )\n            ])\n\n            # Assert that the signal handler is restored after the call to runshell\n            signal.signal(signal.SIGINT, signal.SIG_IGN)\n", "    def test_runshell_handles_sigint(self):\n        with mock.patch('subprocess.Popen', wraps=subprocess.Popen) as mock_popen:\n            with mock.patch('django.db.backends.postgresql.client.DatabaseClient.settings_to_cmd_args_env',\n                           return_value=([], {})):\n                client = DatabaseClient()\n                client.runshell([])\n\n                mock_popen.assert_called_once_with(['psql'], env={'PGPASSWORD': 'password'})\n                mock_popen.return_value.communicate.assert_called_once_with(input=b'', timeout=None)\n", "    def test_runshell_handles_signal(self):\n        with mock.patch('subprocess.Popen', wraps=subprocess.Popen) as mock_popen:\n            with mock.patch('sys.stdout', new_callable=io.StringIO) as mock_stdout:\n                with mock.patch('django.db.backends.postgresql.client.DatabaseClient.settings_to_cmd_args_env',\n                               return_value=(['psql'], {})):\n                    client = DatabaseClient()\n                    client.runshell([])\n\n                    # Assert that the original SIGINT handler was restored\n                    assert signal.getsignal(signal.SIGINT) is not signal.SIG_IGN\n\n                    # Assert that the subprocess was executed\n                    mock_popen.assert_called_once_with(['psql'], env={'PGPASSWORD': ''},\n                                                      stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n"], "sample_35": ["compilation error", "compilation error", "    def test_minversion(module, version, expected):\n        assert minversion(module, version) == expected\n", "compilation error", "    def test_minversion_failure():\n        assert not minversion('astropy', '10.0.0')\n"], "sample_1087": ["    def test_swinnerton_dyer_poly_random():\n        for n in range(1, 10):\n            for polys in [True, False]:\n                p = swinnerton_dyer_poly(n, polys=polys)\n                if polys:\n                    assert isinstance(p, Poly)\n                else:\n                    assert isinstance(p, Add)\n                assert len(p.args) == n\n", "    def test_swinnerton_dyer_poly_n_is_1():\n        assert sympify(swinnerton_dyer_poly(1)) == x**2 - 2\n", "    def test_swinnerton_dyer_poly_order_1():\n        raises(ValueError, lambda: swinnerton_dyer_poly(0))\n        raises(ValueError, lambda: swinnerton_dyer_poly(-1))\n        poly = swinnerton_dyer_poly(1)\n        assert poly == Poly(x**2 - 2, x)\n   ", "compilation error", "    def test_f_polys():\n        for f in f_polys():\n            f_sym = Poly(f, x, y, z)\n            assert f_sym.as_expr() == f\n"], "sample_121": ["    def test_index_together_with_non_local_fields(self):\n        with override_settings(INSTALLED_APPS=['invalid_models_tests']):\n            from invalid_models_tests import IndexTestClass\n\n        # Create a model with index_together referring to a field on a different model.\n        # Make sure the error is raised during the model check.\n        with self.assertRaises(Error) as context:\n            _check_lazy_references(IndexTestClass._meta, {})\n\n        errors = context.exception.args[0]\n        self.assertIn('\"' + IndexTestClass._meta.index_together[0][0] + '\"', str(errors[0]))\n        self.assertIn('\"' + IndexTestClass._meta.index_together[0][1] + '\"', str(errors[0]))\n\n\n\n", "    def test_index_together_with_many_to_many(self):\n        with self.subTest('Index Together with ManyToMany'):\n            class Post(models.Model):\n                title = models.CharField(max_length=100)\n                likes = models.ManyToManyField('User', related_name='liked_posts')\n\n            class User(models.Model):\n                username = models.CharField(max_length=20, primary_key=True)\n            \n            post1 = Post.objects.create(title='My First Post')\n            post2 = Post.objects.create(title='My Second Post')\n            user1 = User.objects.create(username='john')\n            user2 = User.objects.create(username='jane')\n\n            post1.likes.add(user1)\n            post2.likes.add(user1, user2)\n\n            post1.likes.all().get()\n            post2.likes.all().get()\n\n\n", "    def test_index_together_with_multi_table_inheritance(self):\n        from .models import ChildModel, ParentModel\n\n        with self.subTest('check for errors'):\n            errors = _check_lazy_references(ParentModel)\n            self.assertEqual(len(errors), 0)\n\n        with self.subTest('check index together'):\n            with connections['default'].cursor() as cursor:\n                cursor.execute(\n                    \"SELECT name FROM django_indexes WHERE table_name = 'invalid_models_tests_childmodel'\"\n                )\n                result = cursor.fetchone()\n                self.assertEqual(result[0], 'index_together_idx')\n\n\n\n", "    def test_index_together_with_related_fields(self):\n        with override_settings(DATABASE_ROUTERS=['invalid_models_tests.routers.MyRouter']):\n            class Article(models.Model):\n                title = models.CharField(max_length=255)\n                content = models.TextField()\n                tag = models.ForeignKey('Tag', on_delete=models.CASCADE)\n\n                class Meta:\n                    indexes = [\n                        models.Index(fields=['title', 'tag'], name='title_and_tag'),\n                    ]\n\n            class Tag(models.Model):\n                name = models.CharField(max_length=255)\n\n        with self.subTest('IndexTogetherError'):\n            with self.assertRaises(Error) as context:\n                from django.core.checks import run_checks\n                run_checks([Article], connection=connections['default'])\n            self.assertIn('\"', context.exception.msg)\n            self.assertIn('cannot be used with IndexTogether', context.exception.msg)\n", "    def test_index_together_with_related_fields(self):\n        with override_settings(DATABASE_ROUTING={'default': 'default_db'},\n                              INSTALLED_APPS=['invalid_models_tests']):\n            from invalid_models_tests import IndexTogetherModel\n\n            class IndexTogetherModel(models.Model):\n                id = models.AutoField(primary_key=True)\n                name = models.CharField(max_length=100)\n                related_field = models.ForeignKey(\n                    'self',\n                    on_delete=models.CASCADE,\n                    related_name='related_models',\n                )\n\n            IndexTogetherModel._meta.index_together = [\n                ('name', 'related_field__name'),\n            ]\n            field_names = IndexTogetherModel._meta.index_together\n            errors = _check_local_fields(\n                field_names, 'index_together'\n            )\n            self.assertEqual(len(errors), 0)\n\n            with self.assertRaises(commands.CommandError):\n\n                # The index together should fail when we try to migrate.\n                from django.db import migrations, models\n                from django.db.migrations.executor import MigrationExecutor\n                from django.test import override_settings\n\n                class Migration(migrations.Migration):\n                    dependencies = [\n                        ('invalid_models_tests', '0001_initial'),\n                    ]\n\n                    operations = [\n                        migrations.RenameField(\n                            model_name='indextogethermodel',\n                            old_name='related_field',\n                            new_name='related_model',\n                        ),\n                    ]\n\n                executor = MigrationExecutor(connection)\n                executor.migrate(\n                    'invalid_models_tests', '0001_initial',\n                    fake_initial=True\n                )\n\n                # Verify the error message\n                error_message = str(migrations.exceptions.MigrationPlanError)\n                self.assertIn(\n                    'Field \"related_model\" is used in an index_together constraint, but '\n                    'it has been renamed to \"related_field\".',\n                    error_message\n                )\n\n\n\n"], "sample_921": ["    def test_isenumclass_enum_attribute(self):\n        class MyEnum(enum.Enum):\n            VALUE = 1\n\n        self.assertTrue(inspect.isenumclass(MyEnum))\n        self.assertTrue(inspect.isenumattribute(MyEnum.VALUE))\n", "    def test_signature_from_str_with_kwargs_and_defaults(self):\n        signature = '(a, b=1, *, c, d=2)'\n        sig = inspect.signature_from_str(signature)\n        expected_params = [\n            Parameter('a', Parameter.POSITIONAL_OR_KEYWORD),\n            Parameter('b', Parameter.POSITIONAL_OR_KEYWORD, default=Parameter.empty),\n            Parameter('c', Parameter.VAR_KEYWORD),\n            Parameter('d', Parameter.POSITIONAL_OR_KEYWORD, default=Parameter.empty)\n        ]\n        assert sig.parameters == expected_params\n", "    def test_signature_from_str_unspecified_defaults(self):\n        signature = \"def func(a, b=1, c, *, d=2): pass\"\n        sig = inspect.signature_from_str(signature)\n        assert sig.parameters == {\n            'a': Parameter('a', Parameter.POSITIONAL_OR_KEYWORD),\n            'b': Parameter('b', Parameter.POSITIONAL_OR_KEYWORD, default=1),\n            'c': Parameter('c', Parameter.POSITIONAL_OR_KEYWORD),\n            'd': Parameter('d', Parameter.KEYWORD_ONLY, default=2),\n        }\n\n\n", "    def test_signature_from_str_with_varargs_and_kwargs(self):\n        sig = \"(a, *, b, **kwargs)\"\n        expected_sig = inspect.Signature(\n            parameters=[\n                Parameter(\"a\", Parameter.POSITIONAL_ONLY),\n                Parameter(\"b\", Parameter.VAR_POSITIONAL),\n                Parameter(\"kwargs\", Parameter.VAR_KEYWORD),\n            ],\n            return_annotation=Parameter.empty\n        )\n        assert inspect.signature_from_str(sig) == expected_sig\n", "    def test_signature_from_str(self):\n            pass\n\n        sig = inspect.signature_from_str('func(a: int, b: str = \"default\", c: datetime.datetime = None, *, d: bool = False, **kwargs: int)')\n        assert sig.parameters == {\n            'a': Parameter('a', Parameter.POSITIONAL_OR_KEYWORD),\n            'b': Parameter('b', Parameter.POSITIONAL_OR_KEYWORD, default=Parameter.empty),\n            'c': Parameter('c', Parameter.POSITIONAL_OR_KEYWORD, default=Parameter.empty),\n            'd': Parameter('d', Parameter.KEYWORD_ONLY),\n            'kwargs': Parameter('kwargs', Parameter.VAR_KEYWORD),\n        }assert sig.return_annotation == Parameter.empty\n\n"], "sample_307": ["    def test_format_datetime_with_timezone(self):\n        now = datetime.now(utc)\n        formatted_string = format(now, '%Y-%m-%dT%H:%M:%SZ')\n        self.assertEqual(formatted_string, now.isoformat() + '+00:00')\n", "    def test_format_rfc5322(self):\n        dt = datetime(2023, 10, 27, 14, 30, 0, 123456)\n        formatted_date = format(dt, 'r')\n        expected_date = \"Fri, 27 Oct 2023 14:30:00 +0200\"\n        self.assertEqual(formatted_date, expected_date)\n", "    def test_format_rfc5322(self):\n        dt = datetime(2000, 12, 21, 16, 1, 7)\n        formatted_dt = format_datetime_rfc5322(dt)\n        self.assertEqual(format(dt, 'r'), formatted_dt)\n", "    def test_format_datetime_with_timezone(self):\n        dt = datetime(2023, 10, 26, 15, 30, 0, tzinfo=get_default_timezone())\n        # Test formatting with timezone information. \n        self.assertEqual(format(dt, 'T'), '2023-10-26T15:30:00+02:00')\n", "    def test_iso_format(self):\n        now = datetime.utcnow()\n        format_str = 'c'\n        self.assertEqual(format(now, format_str), now.isoformat())\n\n\n"], "sample_559": ["    def test_aspect_ratio(self):\n        fig, ax = plt.subplots()\n        assert_array_almost_equal(ax._get_aspect_ratio(), 1)\n        ax.set_aspect('equal')\n        assert_array_almost_equal(ax._get_aspect_ratio(), 1)\n        ax.set_aspect('auto')\n        assert_array_almost_equal(ax._get_aspect_ratio(), 1)\n\n        ax.set_xlim(0, 10)\n        ax.set_ylim(0, 5)\n        assert_array_almost_equal(ax._get_aspect_ratio(), 2)\n\n        ax.set_xlim(0, 5)\n        ax.set_ylim(0, 10)\n        assert_array_almost_equal(ax._get_aspect_ratio(), 0.5)\n\n\n\n", "    def test_imshow_origin(self):\n        fig, ax = plt.subplots()\n        im = ax.imshow(np.arange(16).reshape((4, 4)), origin='lower' )\n        \n        assert ax.get_yaxis().get_ticklocs()[0] == 0\n\n        ax.imshow(np.arange(16).reshape((4, 4)), origin='upper')\n        assert ax.get_yaxis().get_ticklocs()[0] == 3.\n\n\n\n", "    def test_patch_collections():\n        fig, ax = plt.subplots(1, 1)\n\n        # Simple patch collection\n        patch = mpl.patches.Rectangle((0, 0), 1, 1, facecolor='blue')\n        collection = mpl.collections.PatchCollection([patch],\n                                                       facecolors=['red'])\n        ax.add_collection(collection)\n\n        # Patch collection with varying sizes\n        patches = [mpl.patches.Rectangle((i, 0), 0.5, 0.5,\n                                          facecolor=f'c{i}') for i in range(3)]\n        collection = mpl.collections.PatchCollection(patches,\n                                                       sizes=[0.5, 1, 1.5])\n        ax.add_collection(collection)\n\n        ax.set_xlim(0, 3)\n        ax.set_ylim(0, 1.5)\n\n\n\n", "    def test_violin_basic(self):\n        x = np.random.randn(100, 3)\n        vpstats = cbook.violin_stats(x, points=50)\n        fig, ax = plt.subplots()\n        ax.violin(vpstats)\n        ax.set_xlim(-1, 4)\n        ax.set_ylabel('Values')\n        self.check_figure()\n", "    def test_violin_with_positions(self):\n        data1 = np.random.randn(100)\n        data2 = np.random.randn(100) + 5\n        vpstats1 = cbook.violin_stats(data1, _kde_method, points=100)\n        vpstats2 = cbook.violin_stats(data2, _kde_method, points=100)\n        vpstats = [vpstats1, vpstats2]\n        positions = [1, 2]\n\n        ax = plt.axes()\n        ax.violin(vpstats, positions=positions, vert=True)\n        plt.close()\n\n\n"], "sample_893": ["    def test_export_text_with_random_state(self):\n        rng = RandomState(42)\n        clf = DecisionTreeClassifier(random_state=rng)\n        clf.fit(X, y)\n        report1 = export_text(clf)\n        clf2 = DecisionTreeClassifier(random_state=rng)\n        clf2.fit(X, y)\n        report2 = export_text(clf2)\n        assert report1 == report2\n", "compilation error", "    def test_export_text_with_multi_output_regression(self):\n        \"\"\"Test export_text function with multi-output regression.\"\"\"\n        n_samples = len(X)\n        n_features = X.shape[1]\n        n_outputs = 2\n        tree = DecisionTreeRegressor(max_depth=1, random_state=42)\n        tree.fit(X, y2)\n        report = export_text(tree, feature_names=None)\n        assert len(report.split('\\n')) > 2, \"Too few lines in the report\"\n\n\n\n", "    def test_export_text_show_weights(tree_type, kwargs):\n        tree = tree_type(**kwargs).fit(X, y)\n        report = export_text(tree, show_weights=True)\n        if isinstance(tree, DecisionTreeClassifier):\n            assert \"class\" in report\n        else:\n            assert \"value\" in report\n        assert \" weights\" in report\n\n\n\n", "    def test_export_text_feature_names_error():\n        tree = DecisionTreeClassifier().fit(X, y)\n        with pytest.raises(ValueError):\n            export_text(tree, feature_names=[\"a\", \"b\", \"c\"])\n        with pytest.raises(ValueError):\n            export_text(tree, feature_names=[1, 2, 3, 4])\n        with pytest.raises(ValueError):\n            export_text(tree, feature_names=np.array([\"a\", \"b\", \"c\"]))\n\n\n\n"], "sample_794": ["    def test_ridge_norm(self, scale):\n        n_samples = 10\n        n_features = 5\n        X = np.random.rand(n_samples, n_features)\n        y = np.random.rand(n_samples)\n\n        # Test that Ridge with 'fit_intercept' = True correctly normalizes\n        # data\n        ridge = Ridge(alpha=1.0, normalize=scale, fit_intercept=True)\n\n        ridge.fit(X, y)\n        X_ = ridge.coef_\n        if scale:\n            assert_allclose(np.linalg.norm(X_), np.linalg.norm(X))\n\n        # Test that Ridge with 'fit_intercept' = False does not normalize\n        ridge = Ridge(alpha=1.0, normalize=scale, fit_intercept=False)\n\n        ridge.fit(X, y)\n        X_ = ridge.coef_\n\n        assert_array_equal(ridge.coef_, X\n                           )\n", "    def test_ridge_predict(self, sparse):\n        X = DENSE_FILTER(X_iris) if not sparse else SPARSE_FILTER(X_iris)\n        y = iris.target\n        y_sparse = sp.csr_matrix(y) if sparse else y\n        ridge = Ridge(alpha=1.0)\n        ridge.fit(X, y)\n        y_pred = ridge.predict(X)\n        y_pred_sparse = ridge.predict(X)\n        assert_array_equal(y_pred.shape, y.shape)\n        assert_array_equal(y_pred_sparse.shape, y_sparse.shape)\n        assert_allclose(y_pred, y_pred_sparse.A1)\n\n", "    def test_ridge_solve_with_solver(self, X_type, solver):\n        X = X_type(np.random.rand(10, 5))\n        y = np.random.rand(10)\n        alpha = 0.5\n        \n        if solver == \"svd\":\n            with ignore_warnings(category=ConvergenceWarning):\n                coef_svd, _, _ = ridge_regression(X, y, alpha, solver=solver)\n        else:\n            coef_cholesky = _solve_cholesky(X, y, alpha)\n        \n        coef_lsqr = _solve_cholesky_kernel(X, y, alpha)\n        \n        if solver == \"cholesky\":\n            assert_allclose(coef_cholesky, coef_svd)\n        assert_allclose(coef_lsqr, coef_svd)\n", "    def test_ridge_solver(self, solver):\n        for X, y in [(DENSE_FILTER(X_diabetes), y_diabetes),\n                     (SPARSE_FILTER(X_diabetes), y_diabetes)]:\n            model = Ridge(alpha=1.0, solver=solver)\n            model.fit(X, y)\n            assert isinstance(model.coef_, np.ndarray)\n            assert_allclose(model.coef_, model._compute_coef(X, y),\n                            rtol=1e-05)\n", "  def test_solve_cholesky_kernel_with_sparse(self, solver):\n      X = SPARSE_FILTER(np.random.rand(100, 50))\n      y = np.random.randn(100,)\n      alpha = 0.1\n      K = X.dot(X.T)\n      with pytest.warns(ConvergenceWarning):\n          A = _solve_cholesky_kernel.solve(K, y, alpha)\n      self.assertIsInstance(A, np.ndarray)\n      self.assertEqual(A.shape, (50,))\n\n\n\n"], "sample_681": ["    def test_file_handler_path(self, testdir: Testdir) -> None:\n        p = testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n                pass\n            \"\"\"\n        )\n        config = testdir.parse(p).config\n\n        # Make sure the initial log file path is set to a safe temporary location.\n        log_file = config.getini(\"log_file\")\n        assert log_file != os.devnull and os.path.isfile(log_file)\n\n        plugin = config.pluginmanager.get_plugin(\"logging-plugin\")\n        plugin.set_log_path(os.devnull)\n        assert not os.path.isfile(log_file)\n\n        result = testdir.runpytest()\n        result.assert_outcomes(passed=1)\n", "    def test_log_file_path_set(self, testdir: Testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            import sys\n            \n                pass\n            \"\"\"\n        )\n        config = testdir.parse(\n            \"--log-file=test.log\"\n        )\n        capture = config.pluginmanager.getplugin(\"logging-plugin\")\n        assert capture.log_file_handler.baseFilename == \"test.log\"\n\n\n", "    def test_set_log_path(self, testdir: Testdir, caplog: LogCaptureFixture) -> None:\n        \"\"\"Test case for set_log_path method.\"\"\"\n        # testdir.makepyfile(\"\"\"\n        # def test_func():\n        #     pass\n        # \"\"\")\n        p = testdir.makepyfile(\n            \"\"\"\n            import logging\n\n            logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s')\n                logging.info('This message should be written to file.')\n            \"\"\"\n        )\n        reporters = testdir.runpytest(\"-v\", \"--log-file\", \"/dev/null\")\n        reporters.assert_outcomes(passed=1)\n\n        # Test the set_log_path method\n        plugin = testdir.parsemodules()[0].pluginmanager.get_plugin(\"logging-plugin\")\n        plugin.set_log_path(\"/tmp/test.log\")\n        reporters = testdir.runpytest()\n        reporters.assert_outcomes(passed=1)\n        with open(\"/tmp/test.log\", \"r\") as f:\n            content = f.read()\n        assert \"This message should be written to file.\" in content\n        os.remove(\"/tmp/test.log\")\n\n\n\n\n\n\n\n", "    def test_cli_capture_file_logging(self, tmpdir: Testdir) -> None:\n        \"\"\"Test that CLI logging can be used alongside file logging in a way that doesn't interfere.\"\"\"\n        p = tmpdir.makefile(\"test.py\", content=\"\"\"\n            import logging\n\n            logging.basicConfig(level=logging.DEBUG, format='%(message)s')\n            logging.debug('This message should appear in the file')\n        \"\"\")\n        p.chdir()\n        config = self.runpytest(\n            \"-v\", \"--log-cli-level=DEBUG\", \"--log-file=\" + str(tmpdir.join(\"test.log\")),\n        )\n        config.assert_outcomes(passed=1)\n\n        captured_messages = config.calls[\"captured_messages\"]\n\n        # assert that the message from test.py is logged in file \"test.log\"\n        with open(tmp_dir.join(\"test.log\"), \"r\") as f:\n            content = f.read()\n        assert \"This message should appear in the file\" in content\n\n        # assert that the message from test.py is logged to the console\n        assert captured_messages[0].stdout.strip() == \"This message should appear in the file\"\n\n\n\n", "    def test_custom_log_file_path(self, testdir: Testdir) -> None:\n        p = testdir.makepyfile(\"\"\"\n            import pytest\n            from pydantic import BaseModel\n\n            class MyModel(BaseModel):\n                name: str\n                age: int\n\n            @pytest.fixture\n                return MyModel(name='Alice', age=30)\n        \"\"\")\n        testdir.parse(\n            \"\"\"\n                assert my_model.name == 'Alice'\n            \"\"\"\n        )\n\n        config = testdir.parse_configure(\"--log-file=/tmp/test.log\")\n        config.pluginmanager.getplugin(\"logging-plugin\").set_log_path(\"/tmp/test.log\")\n        result = config.runpytest()\n        assert result.retcode == ExitCode.OK\n\n\n\n        with open(\"/tmp/test.log\", \"r\") as f:\n            log_content = f.read()\n        assert \"test_something\" in log_content\n\n        os.remove(\"/tmp/test.log\")\n"], "sample_832": ["    def test_ward_threshold(self):\n        # Test the ward threshold behavior in ARDRegression\n        n_samples = 100\n        n_features = 5\n        X = np.random.rand(n_samples, n_features)\n        y = np.random.rand(n_samples)\n        reg = ARDRegression(threshold_lambda=1e-2, n_iter=100)\n        reg.fit(X, y)\n\n        # Check that the number of kept features is less than or equal to\n        # the threshold\n        assert np.count_nonzero(reg.lambda_ < reg.threshold_lambda) <= n_features\n        # Make sure the threshold is respected\n        assert np.all(reg.lambda_ >= reg.threshold_lambda)\n\n        # Check that the model still performs well\n        y_pred = reg.predict(X)\n        assert_array_almost_equal(reg.predict(X), y_pred)\n", "    def test_ard_ridge_threshold(self):\n        X, y = diabetes.data, diabetes.target\n        n_samples, n_features = X.shape\n        rng = check_random_state(0)\n        X_train = X[:n_samples // 2]\n        y_train = y[:n_samples // 2]\n        X_test = X[n_samples // 2:]\n        y_test = y[n_samples // 2:]\n        model = ARDRegression(threshold_lambda=1e-3)\n        model.fit(X_train, y_train)\n        coef_ = model.coef_\n        # Check that some of the weights have been set to zero\n        assert (coef_ == 0).any()\n        \n        # Check that the rest are not zero\n        assert (coef_ != 0).any()\n\n\n", "def test_bayesian_ridge_alpha_init_from_variance():\n    X, y = datasets.make_regression(random_state=42)\n    # Use a variance-dependent alpha initialization.\n    ridge = BayesianRidge(alpha_init=1. / np.var(y))\n    ridge.fit(X, y)\n    # Check that the estimated alpha is close to the inverse of the true variance.\n    assert_almost_equal(ridge.alpha_, 1. / np.var(y), rtol=0.1)\n", "    def test_ard_threshold(self):\n        X = np.random.rand(100, 5)\n        y = np.random.rand(100)\n        # Create an ARD model with a low threshold\n        ard_reg = ARDRegression(threshold_lambda=1e-3)\n        # Fit the model\n        ard_reg.fit(X, y)\n        # Check that only a few lambda values are kept\n        assert np.sum(ard_reg.lambda_ > 1e-3) < 1\n        # Compare with a Ridge model\n        ridge_reg = Ridge()\n        ridge_reg.fit(X, y)\n        assert np.sum(np.abs(ard_reg.coef_ - ridge_reg.coef_)) > 1e-3\n        # Now try with a high threshold\n        ard_reg = ARDRegression(threshold_lambda=1e10)\n        ard_reg.fit(X, y)\n        assert np.sum(ard_reg.lambda_ > 1e-3) == 5\n", "    def test_bayesian_ridge_sparse_data(self):\n        X, y = datasets.make_regression(n_samples=100, n_features=20,\n                                       random_state=42)\n        X_sparse = X > 0  \n        y_sparse = y.astype(bool)\n\n        ridge = Ridge()\n        bayesian_ridge = BayesianRidge()\n\n        ridge.fit(X, y)\n        bayesian_ridge.fit(X_sparse, y_sparse)\n\n        assert_array_almost_equal(ridge.predict(X), bayesian_ridge.predict(X))\n\n        y_pred_sparse = bayesian_ridge.predict(X_sparse)\n        assert_array_almost_equal(y_pred_sparse, bayesian_ridge.predict(X_sparse))\n\n\n\n"], "sample_845": ["    def test_analyzer_char_wb_with_stopwords(self, analyzer):\n        vectorizer = TfidfVectorizer(analyzer=analyzer, stop_words='english')\n        X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n        # Check that stopwords are removed\n        expected_features = ['salad', 'celeri', 'tomato', 'water',\n                             'burger', 'pizza', 'beer', 'coke', 'copyright']\n        assert_array_equal(vectorizer.get_feature_names(), expected_features)\n\n", "    def test_encoding_decode_error(self):\n        # Test with various encodings and decode_error values\n        test_cases = [\n            (\n                'This is a test\\n'.encode('utf-8'),\n                'utf-8',\n                'strict',\n                'This is a test',\n            ),\n            (\n                b'Ceci est un test\\n',\n                'utf-8',\n                'strict',\n                'Ceci est un test',\n            ),\n            (\n                b'This is a test\\n',\n                'latin-1',\n                'strict',\n                'This is a test',\n            ),\n            (\n                b'This is a test\\n',\n                'utf-8',\n                'ignore',\n                'This is a test',\n            ),\n            (\n                b'\\xc3\\xa9cest un test\\n',\n                'utf-8',\n                'replace',\n                'ecest un test',\n            ),\n            (\n                b'This is a test\\n',\n                'ascii',\n                'strict',\n                'This is a test',\n            ),\n            (\n                b'\\xc3\\xa9cest un test\\n',\n                'ascii',\n                'strict',\n                pytest.raises(UnicodeDecodeError),\n            ),\n            # Test with non-string input\n            (\n                b'This is a test\\n',\n                'utf-8',\n                'strict',\n                'This is a test',\n                True\n            ),\n        ]\n        for bytes_data, encoding, decode_error, expected_result, is_string in test_cases:\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n                vectorizer = TfidfVectorizer(input='bytes', encoding=encoding,\n                                           decode_error=decode_error)\n                if is_string:\n                    data = [bytes_data.decode(encoding)]\n                else:\n                    data = [bytes_data]\n                X = vectorizer.fit_transform(data)\n                if isinstance(expected_result, Exception):\n                    assert_raises(expected_result, lambda: vectorizer.fit_transform(data))\n                else:", "    def test_stop_words_vocabulary_filter():\n        vectorizer = CountVectorizer(stop_words='english')\n        X = vectorizer.fit_transform(['this is a test', 'this is another test'])\n        assert_array_equal(vectorizer.vocabulary_, {'test': 0, 'is': 1, 'this': 2,\n                                                    'a': 3, 'another': 4})\n\n        vectorizer = TfidfVectorizer(stop_words='english')\n        X = vectorizer.fit_transform(['this is a test', 'this is another test'])\n        assert_array_equal(vectorizer.vocabulary_,\n                          {'test': 0, 'is': 1, 'this': 2, 'a': 3, 'another': 4})\n\n\n\n\n", "    def test_stop_words_from_string(self):\n        vectorizer = TfidfVectorizer(stop_words='english')\n        X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n        assert set(ENGLISH_STOP_WORDS).issubset(\n            set(vectorizer.get_feature_names()))\n\n", "    def test_tfidf_with_fixed_vocabulary(self):\n        \"\"\"Test TfidfVectorizer with a fixed vocabulary.\"\"\"\n        # Test case with fixed vocabulary\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            vectorizer = TfidfVectorizer(\n                vocabulary=[\"apple\", \"banana\", \"orange\", \"grape\"]\n            )\n\n        X = vectorizer.transform([\"apple banana\", \"banana orange\"])\n        expected_X = sparse.csc_matrix(\n            [\n                [1.0, 1.0, 1.0, 0.0],  # apple banana\n                [1.0, 1.0, 1.0, 0.0],  # banana orange\n            ]\n        )\n        assert_array_almost_equal(X.toarray(), expected_X.toarray())\n\n        self.assertEqual(vectorizer.vocabulary_, {\"apple\": 0, \"banana\": 1, \"orange\": 2, \"grape\": 3})\n\n\n"], "sample_543": ["    def test_rectangle_selector_move_handle(ax):\n        rect_select = RectangleSelector(ax, lambda *args, **kwargs: None,\n                                      useblit=False,\n                                      button=1,\n                                      grab_range=10,\n                                      props=dict(linewidth=2, color='r'))\n        rect_select.set_visible(True)\n\n        # Trigger a draw without a selection for comparison\n        rect_select.update()\n        initial_bbox = rect_select._rect_bbox\n        \n        # Select a handle\n        h_idx, h_dist = rect_select._corner_handles.closest(initial_bbox[0] + 10, initial_bbox[1] + 10)\n        rect_select._active_handle = h_idx\n\n        # Move the handle\n        do_event(ax, 'button_press_event', x=initial_bbox[0] + 10, y=initial_bbox[1] + 10, button=1)\n        do_event(ax, 'motion_notify_event', x=initial_bbox[0] + 20, y=initial_bbox[1] + 20)\n        do_event(ax, 'button_release_event', x=initial_bbox[0] + 20, y=initial_bbox[1] + 20)\n\n        # Check if the rectangle's extents have changed\n        modified_bbox = rect_select._rect_bbox\n        assert_allclose(modified_bbox, [initial_bbox[0] + 10, initial_bbox[0] + 20, \n                                        initial_bbox[1] + 10, initial_bbox[1] + 20]) \n", "    def test_lasso_select(self, ax):\n        fig, ax = plt.subplots()\n        verts_expected = [(0, 0), (1, 1), (4, 1), (4, 4), (2, 4)]\n        x, y = 0, 0\n        lasso = Lasso(ax, (x, y), lambda verts: assert_allclose(verts, verts_expected))\n        \n        # Draw the lasso path\n        click_and_drag(ax, (x, y), (1, 1), button=1)\n\n        do_event(ax, MouseEvent('button_release', x=4, y=1, button=1))\n        click_and_drag(ax, (4, 1), (4, 4), button=1)\n        \n        do_event(ax, MouseEvent('button_release', x=2, y=4, button=1))\n\n        plt.pause(0.1)  \n        plt.close(fig)\n\n\n\n", "def test_polygon_selector_props(ax, props_polygon, props_boundingbox):\n    onselect = mock.Mock()\n    with pytest.warns(MatplotlibDeprecationWarning) as warnings:\n        selector = PolygonSelector(ax, onselect,\n                                   props=props_polygon,\n                                   grab_range=10,\n                                   draw_bounding_box=True,\n                                   box_handle_props=dict(marker='s',\n                                                          markerfacecolor='w'),\n                                   box_props=props_boundingbox)\n    selector.verts = [(1, 1), (3, 1), (3, 3), (1, 3)]\n    selector._draw_polygon()\n\n    assert selector._polygon_handles.artists[0].get_color() == props_polygon.get('color')\n    assert selector._polygon_handles.artists[0].get_marker() == props_polygon.get('marker')\n    assert selector._selection_artist.get_color() == props_polygon.get('color')\n    assert selector._selection_artist.get_linestyle() == props_polygon.get('linestyle')\n    assert selector._selection_artist.get_linewidth() == props_polygon.get('linewidth')\n    assert selector._box.artists[0].get_facecolor() == props_boundingbox.get('facecolor')\n\n", "    def test_polygon_selector_multiple_vertices(ax):\n        fig, ax = plt.subplots()\n\n            assert len(verts) == 5\n            # assert vertices within a certain tolerance\n            assert_allclose(verts[0], (0.1, 0.1))\n            assert_allclose(verts[1], (0.5, 0.1))\n            assert_allclose(verts[2], (0.5, 0.5))\n            assert_allclose(verts[3], (0.1, 0.5))\n            assert_allclose(verts[4], (0.1, 0.1))\n\n        ps = PolygonSelector(ax, onselect, grab_range=5,\n                            useblit=False,\n                            props={'color': 'red'},\n                            handle_props={'marker': 'o'})\n        click_and_drag(ax, (0.1, 0.1), (0.5, 0.1))\n        click_and_drag(ax, (0.5, 0.1), (0.5, 0.5))\n        click_and_drag(ax, (0.5, 0.5), (0.1, 0.5))\n        click_and_drag(ax, (0.1, 0.5), (0.1, 0.1))\n\n        plt.close(fig)\n\n\n\n", "def test_lasso_callback(ax):\n        assert len(verts) > 2\n        expected_verts = [(1, 1), (3, 1), (3, 3), (1, 3), (1, 1)]\n        assert_allclose(verts, expected_verts)\n\n    lasso = Lasso(ax, (1, 1), callback)\n    click_and_drag(ax, (1, 1), (3, 1))\n    click_and_drag(ax, (3, 1), (3, 3))\n    click_and_drag(ax, (3, 3), (1, 3))\n    click_and_drag(ax, (1, 3), (1, 1))\n    lasso.disconnect_events()\n"], "sample_1208": ["    def test_matrix_student_t_pdf(self):\n        n, p = 2, 2\n        nu = 5\n        M = MatrixSymbol('M', n, p)\n        Omega = Matrix([[1, 0], [0, 2]])\n        Sigma = Matrix([[3, 0], [0, 4]])\n        dist = MatrixStudentT('X', nu, M, Omega, Sigma)\n        x = Matrix([[1, 2], [3, 4]])\n        pdf = density(dist)(x)\n        self.assertAlmostEqual(pdf.evalf(), 0.123456789, places=7)\n\n\n\n", "    def test_matrix_normal_pdf(self):\n        M = MatrixNormal('M', Matrix([[1, 2]], dtype=float), \n              Matrix([[1, 0], [0, 1]], dtype=float),\n              Matrix([[1, 0], [0, 1]], dtype=float))\n        X = MatrixSymbol('X', 1, 2, dtype=float)\n        pdf = density(M)(X)\n        self.assert_equals(pdf.doit(), \n                          exp(-Trace((Matrix([[-1], [-2]]) + X.T)*\n                          (Matrix([[-1, -2]]) + X))/2) / (2 * pi))\n", "    def test_matrix_student_t_sample_size(self):\n        from sympy.stats import MatrixStudentT\n        a = symbols('a', positive=True)\n        M = MatrixStudentT('M', a, [[1, 2]], [[1, 0], [0, 1]], [1])\n        with raises(ValueError):\n            sample(M, size=(2, 3))\n        with raises(ValueError):\n            sample(M, size=3)\n    \n    \n\n\n\n", "    def test_matrix_student_t_pdf(self):\n        n, p = 2, 3\n        nu = 5\n        M = MatrixSymbol('M', n, p)\n        Omega = Matrix([[1, 0, 0],\n                        [0, 2, 0],\n                        [0, 0, 3]])\n        Sigma = Matrix([[4, 0, 0],\n                        [0, 5, 0],\n                        [0, 0, 6]])\n\n        distribution = MatrixStudentT('X', nu, M, Omega, Sigma)\n\n        x = Matrix([[1, 2, 3],\n                    [4, 5, 6]])\n\n        with self.assertAlmostEqual(tol=1e-5):\n            pdf = density(distribution)(x)\n            pdf_expected  = (gamma((nu + n + p - 1)/2) * Determinant(Omega)**(-n/2) * Determinant(Sigma)**(-p/2)\\\n                             / ((pi)**(n*p/2) * gamma((nu + p - 1)/2, p))) * (Determinant(eye(n) + Inverse(Sigma)*(x - M)*Inverse(Omega)*Transpose(x - M))) \\\n                             **(-(nu + n + p -1)/2)\n\n            self.assertEqual(pdf, pdf_expected) \n\n\n            \n", "    def test_matrix_student_t_density(self):\n        n, p = 2, 3\n        nu = 5\n        M = MatrixSymbol('M', n, p)\n        Omega = Matrix([[1, 0, 0], [0, 2, 0], [0, 0, 3]])\n        Sigma = Matrix([[4, 0, 0], [0, 5, 0], [0, 0, 6]])\n        M_t = MatrixStudentT('M_t', nu, M, Omega, Sigma)\n\n        X = MatrixSymbol('X', n, p)\n        result = density(M_t)(X)\n        self.assertTrue(isinstance(result, Basic))\n        self.assertEqual(Trace(Inverse(Sigma)*(X - M)*Inverse(Omega)*Transpose(X - M)),\n                         result.args[-1].args[0])\n        with raises(ValueError):\n            density(M_t)([[1, 2, 3, 4]]) \n\n\n\n"], "sample_688": ["    def test_symlink_or_skip_works_on_real_path(self, testdir):\n        testdir.makepyfile(\n            \"test_impl.py\",\n            \"\"\"\n                pass\n            \"\"\",\n        )\n        testdir.mkdir(\"subdir\")\n        old_cwd = os.getcwd()\n        try:\n            os.chdir(testdir.tmpdir / \"subdir\")\n            real_link_path = testdir.tmpdir / \"target\"\n            symlink_or_skip(\n                real_link_path,\n                testdir.tmpdir / \"subdir\" / \"test_impl.py\",\n            )\n            self.assert_items(testdir.get_items(\"subdir\"), [\n                \"test_impl.py\",\n            ])\n        finally:\n            os.chdir(old_cwd)\n\n\n\n", "    def test_absolutepath_handles_invalid_paths(self, pytester):\n        invalid_paths = [\n            \".\",\n            \"..\",\n            \"non/existing/path\",\n            \"file:///path/to/non/existing/file\",\n            Path(\".\"),\n            Path(\"..\"),\n            Path(\"non/existing/path\"),\n        ]\n        for path in invalid_paths:\n            with pytest.raises(ValueError):\n                absolutepath(path)\n\n\n", "    def test_find_suffixes_simple(self):\n        p = self.path.join(\"subpackage\", \"test_foo.py\")\n        self.assert_result(\n            list(find_suffixes(p.parent, \"subpackage\")), [\"test_foo.py\"]\n        )\n", "    def test_resolve_package_path_empty_parent(self, testdir):\n        p = testdir.makepyfile(\n            \"\"\"\n            from __future__ import print_function\n            __package__ = 'top'\n                pass\n            \"\"\"\n        )\n        result = resolve_package_path(Path(p))\n        assert result is not None\n", "    def test_fnmatch_ex_posix_path_sep(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n                assert fnmatch_ex(\"a/b/c\", \"a/*/c\")\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.assert_outcomes(passed=1)\n"], "sample_164": ["    def test_email_handler_with_custom_reporter(self):\n        with override_settings(\n            DEFAULT_EXCEPTION_REPORTER='django.views.debug.ExceptionReporter'\n        ):\n            handler = AdminEmailHandler(include_html=False, email_backend='django.core.mail.backends.console.EmailBackend')\n            handler.emit(logging.LogRecord('django.request', logging.ERROR, None, None, 'Test error', (None, None, None)))\n            self.assertEqual(mail.outbox, [\n                dict(\n                    subject='ERROR: Test error',\n                    message='Test error\\n\\nTraceback (most recent call last):\\n\\n',\n                    from_email=settings.DEFAULT_FROM_EMAIL,\n                    recipient_list=settings.ADMINS,\n                )\n            ])\n", "    def test_callback_filter(self):\n        class CallbackFilterTest(CallbackFilter):\n                super().__init__(\n                    lambda record: record.levelno == logging.DEBUG if log_if_level == 'DEBUG'\n                    else record.levelno == logging.ERROR if log_if_level == 'ERROR'\n                    else False\n                )\n\n        logger = logging.getLogger('test')\n        handler = logging.StreamHandler(stream=StringIO())\n        formatter = ServerFormatter()\n        handler.setFormatter(formatter)\n        logger.addHandler(handler)\n\n        logger.debug('This should not be logged')\n        logger.info('This should not be logged')\n        logger.warning('This should not be logged')\n        logger.error('This should be logged')\n        logger.critical('This should be logged')\n\n        output = handler.stream.getvalue()\n        self.assertIn('This should be logged', output)\n        self.assertIn('This should be logged', output)\n        self.assertNotIn('This should not be logged', output)\n\n\n\n", "    def test_callback_filter(self):\n            return record.levelno <= logging.WARNING\n\n        filter = CallbackFilter(callback_filter_func)\n        handler = logging.StreamHandler()\n        logger = logging.getLogger(\"test_logger\")\n        logger.addHandler(handler)\n        logger.addFilter(filter)\n\n        logger.debug(\"This should not be logged\")\n        logger.info(\"This should not be logged\")\n        logger.warning(\"This should be logged\")\n        logger.error(\"This should be logged\")\n        logger.critical(\"This should be logged\")\n\n        # Check that only WARNING, ERROR, and CRITICAL logs were emitted\n        self.assertEqual(handler.stream.getvalue().splitlines(), [\n            \"WARNING: This should be logged\",\n            \"ERROR: This should be logged\",\n            \"CRITICAL: This should be logged\",\n        ])\n\n", "    def test_admin_email_handler_filters_emails(self):\n        with override_settings(DEBUG=False, EMAIL_HOST='example.com', EMAIL_PORT=587, EMAIL_BACKEND=MyEmailBackend, DEFAULT_EXCEPTION_REPORTER='django.views.debug.ExceptionReporter'):\n            handler = AdminEmailHandler()\n            record = logging.LogRecord('django.request', 'ERROR', 1, None, 'Test message', None, None, {'remote_addr': '127.0.0.1'})\n            handler.emit(record)\n            self.assertEqual(len(mail.outbox), 1)\n            message = mail.outbox[0].body\n            self.assertIn('Test message', message)\n            self.assertIn('example.com', message)\n            self.assertIn('127.0.0.1', message)\n", "    def test_callback_filter(self):\n        callback = lambda record: record.levelno == logging.WARNING\n\n        filter = CallbackFilter(callback)\n\n        handler = logging.StreamHandler(StringIO())\n        logger = logging.getLogger('test')\n        logger.addHandler(handler)\n        logger.addFilter(filter)\n\n        logger.warning('This will be logged')\n        logger.error('This will not be logged')\n\n        output = handler.stream.getvalue()\n\n        self.assertIn('This will be logged', output)\n        self.assertNotIn('This will not be logged', output)\n"], "sample_573": ["    def test_polyfit_low_unique_points(self, df):\n        # Test the case where the number of unique x values is less than or equal to the order\n        df_grouped = GroupBy(df, \"group\")\n        result = PolyFit(order=3). __call__(df, df_grouped, \"x\", \"y\")\n\n        # Assert that the result is an empty DataFrame\n        assert_frame_equal(result, pd.DataFrame())\n", "    def test_polyfit_order(self, df, rng):\n        df[\"x\"] = df[\"x\"] * 2\n        polyfit = PolyFit(order=3)\n        result = polyfit(df, GroupBy(df, \"group\"), \"x\", None)\n        expected_order = 3\n        assert_array_almost_equal(np.polyfit(result[\"x\"], result[\"y\"], expected_order), np.polyfit(df[\"x\"], df[\"y\"], expected_order))\n\n\n\n\n", "    def test_polyfit_handles_few_data_points(self, df):\n        df_subset = df.query(\"x <= 1\")\n        polyfit = PolyFit(order=3)  # Higher order than available data points\n        result = polyfit(df_subset.groupby(\"group\"), \"group\", \"v\", scales=None)\n\n        assert result.empty\n\n\n\n", "    def test_polyfit_low_unique_x_values(self, df):\n        df[\"x\"] = df[\"x\"].drop_duplicates()\n        groupby = GroupBy(df, \"group\", \"color\")\n        result = PolyFit()(df, groupby, \"x\", [])\n        assert result.empty\n\n\n\n", "    def test_polyfit_order_too_small(self, df, rng):\n        df[\"x\"] = rng.choice([1, 2, 3, 4], size=len(df))\n        groupby = df.groupby(\"group\")\n        result = PolyFit(order=3)(df, groupby, orient='v', scales=None)\n        assert_frame_equal(result, pd.DataFrame(columns=[\"x\", \"y\"]))\n"], "sample_221": ["    def test_related_populator_select_related(self):\n        container = Container.objects.create(name='Container A')\n        event = Event.objects.create(name='Event A', container=container)\n        group = Group.objects.create(name='Group A', events=[event])\n\n        # Fetch the group with select_related('events')\n        related_group = Group.objects.select_related('events').get(name='Group A')\n        \n        # Assert that the related event is correctly populated\n        self.assertEqual(related_group.events.count(), 1)\n        self.assertEqual(related_group.events.first().name, 'Event A')\n", "    def test_prefetch_related_with_nested_lookups(self):\n        container = Container.objects.create()\n        event = Event.objects.create(container=container)\n        group = Group.objects.create(events=event)\n        happening = Happening.objects.create(event=event)\n\n        # Prefetch related objects with nested lookups\n        events = Event.objects.select_related('container', 'happening__group').prefetch_related('happening__group')\n\n        # Verify that prefetched related objects are populated\n        for fetched_event in events:\n            self.assertIsNotNone(fetched_event.container)\n            self.assertIsNotNone(fetched_event.happening)\n            self.assertIsNotNone(fetched_event.happening.group)\n\n        # Make sure that the populated attributes are the ones that were prefetched\n\n        # Verify that all three related objects have been populated\n        self.assertTrue(hasattr(fetched_event, 'container'))\n        self.assertTrue(hasattr(fetched_event, 'happening'))\n        self.assertTrue(hasattr(fetched_event, 'happening.group'))\n\n", "    def test_related_populator_basic(self):\n        container = Container.objects.create(name='container')\n        event = Event.objects.create(name='event', container=container)\n        related_populator = RelatedPopulator({'related_klass_infos': [{'model': Event, 'select_fields': [(event._meta.pk, None)]}], 'from_parent': True, 'select_fields': [(event._meta.pk, None)], 'local_setter': lambda x, y: None, 'remote_setter': lambda x, y: None, 'model': Event}, [])\n\n        row = [event.id]\n        related_populator.populate(row, container)\n        self.assertEqual(event, container.event)\n\n\n", "    def test_prefetch_related_select_related(self):\n        obj = Event.objects.prefetch_related(\n            'happening',\n            'group'\n        ).select_related('container').get(pk=1)\n\n        self.assertEqual(obj.happening, self.happening)\n        self.assertEqual(obj.group, Group.objects.get(name='Test Group'))\n        self.assertEqual(obj.container.name, 'Test Container')\n\n", "    def test_related_populator_prefetch(self):\n        group = Group.objects.create(name='Group A')\n        event = Event.objects.create(\n            name='Event A',\n            group=group,\n            happening=self.happening\n        )\n        container = Container.objects.create(\n            name='Container A',\n            events=[event]\n        )\n\n        related_populators = []\n            related_populators.append(\n                RelatedPopulator(\n                    {'model': Event},\n                    ['event_name', 'group_id', 'happening_id'],\n                    django.db.connection.cursor().db\n                )\n            )\n\n        qs = Container.objects.select_related('events').prefetch_related('events__group', 'events__happening').filter(name='Container A')\n\n        for obj in qs:\n            self.assertEqual(len(related_populators), 1)\n            related_populator = related_populators[0]\n            self.assertIsInstance(related_populator, RelatedPopulator)\n            self.assertEqual(related_populator.model_cls, Event)\n\n\n\n"], "sample_352": ["    def test_subquery_constraint(self):\n        subquery = Article.objects.filter(id__in=Article.objects.filter()).values('id')\n        constraint = SubqueryConstraint(alias='subquery', columns=['id'], targets=None, query_object=subquery)\n        compiler =  self.get_compiler(connection=connection.cursor())\n        sql, params = constraint.as_sql(compiler, connection)\n        self.assertEqual(sql, \"(SELECT id FROM Article)\")\n", "    def test_where_node_split_having(self):\n        \"\"\"\n        Test that `split_having` can split a WhereNode appropriately.\n        \"\"\"\n        from django.db.models.sql.where import WhereNode\n        from django.db.models.expressions import F\n        \n        # Construct a WhereNode with some aggregate function.\n        where = WhereNode(connector=AND)\n        where.children = [\n            WhereNode(connector=OR, negated=True),\n            F('number') > 1000\n            \n        ]\n        \n        # split_having should return a tuple: (WHERE, HAVING)\n        where_part, having_part = where.split_having()\n        \n        # Check if the aggregate is correctly moved to the HAVING clause.\n        self.assertIsNotNone(having_part)\n        self.assertEqual(having_part.children[0].__class__.__name__, 'F')\n        self.assertEqual(where_part.children[0].__class__.__name__, 'WhereNode')\n        self.assertEqual(where_part.children[0].negated, True)   \n\n\n\n", "    def test_empty_result_set(self):\n        conn = connection.cursor()\n        with connection.cursor() as cursor:\n            with connection.constraint_checking(False):\n                nothing = NothingNode()\n                sql, params = nothing.as_sql(\n                    compiler=connection.compiler, connection=conn\n                )\n                self.assertEqual(sql, '')\n                self.assertEqual(params, [])\n\n        with CaptureQueriesContext(connection_alias=DEFAULT_DB_ALIAS) as captured:\n            nothing = NothingNode()\n            nothing.as_sql(compiler=connection.compiler, connection=conn)\n        self.assertEqual(captured.query_count, 0)\n\n\n", "    def test_subquery_constraint(self):\n        q = Query.objects.filter(author__name='a2')\n        subq = q.filter(id__in=Exists(q.values('id'))).values('id')\n        with self.subclasses_patched():\n            subquery = SubqueryConstraint(\n                alias='subq', columns=['id'], targets=['id'], query_object=q\n            )\n            self.assertEqual(subquery.as_sql(compiler=None, connection=None),\n                             \"EXISTS (SELECT id FROM django_orm_Query WHERE author__name = 'a2')\")\n\n\n", "        def test_contains_aggregate(self):\n            child = Count('note__extra__value')\n            where = WhereNode(children=[child], connector=AND)\n            self.assertTrue(where.contains_aggregate)\n\n                self.assertEqual(query_compiler.as_sql(connection), 'SELECT %s FROM \"django_app_name\".\"Article\" %s' % (\n                    QuerySet._get_field_names(model),\n                    'WHERE (SELECT COUNT(ExtraInfo.id) FROM \"django_app_name\".\"ExtraInfo\" WHERE \"django_app_name\".\"ExtraInfo\".\"note_id\" = \"django_app_name\".\"Article\".\"id\") > 0'\n                    ))\n"], "sample_757": ["    def test_handle_unknown_legacy_mode(self):\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        enc = OneHotEncoder(handle_unknown='ignore', n_values='auto')\n        enc.fit(X)\n        with pytest.raises(ValueError):\n            enc.transform(np.array([[7, 8]]))\n", "    def test_onehotencoder_with_unknown_categories_error(self):\n        enc = OneHotEncoder(handle_unknown='error')\n        X = [['A', 1], ['B'], ['C', 2]]\n        with assert_raises(ValueError,\n                          match='unknown categorical feature present during transform'):\n            enc.fit_transform(X)\n", "    def test_transform_with_unknown_categories(self, handle_unknown):\n        enc = OneHotEncoder(handle_unknown=handle_unknown)\n        X = [['A', 1], ['B', 2], ['C', 3]]\n        enc.fit(X)\n        new_data = [['A', 4], ['X', 5]]\n        with assert_raises(ValueError) if handle_unknown == 'error' else \\\n                assert_raises_regex(ValueError, 'Unknown category'):\n            enc.transform(new_data)\n\n\n\n", "    def test_transform_sparse_data(self):\n        enc = OneHotEncoder(sparse=True)\n        X = np.array([[1, 2], [0, 1]])\n        enc.fit(X)\n        X_out = enc.transform(X)\n\n        assert_equal(X_out.shape, (2, 3))\n        assert_allclose(toarray(X_out), np.array([[0, 1, 0],\n                                                  [1, 0, 0]]))\n\n\n\n", "    def test_onehot_string_categorical_features(self):\n        enc = OneHotEncoder(handle_unknown='ignore', sparse=True)\n        X = [['NYC', 'Boston'], ['LA', 'Chicago']]\n        enc.fit(X)\n        assert_equal(enc.categories_[0].dtype, object)  \n        assert_equal(enc.categories_[1].dtype, object)\n        X_transformed = enc.transform(X)\n        assert_equal(X_transformed.shape, (2, 6))\n        assert_no_warnings(lambda: enc.inverse_transform(X_transformed))\n\n\n\n"], "sample_183": ["    def test_case_expression_with_different_data_types(self):\n        qs = CaseTestModel.objects.annotate(\n            category=Case(\n                When(integer__gte=3, then='High'),\n                When(integer__gte=1, then='Medium'),\n                default='Low',\n            ),\n        )\n        results = qs.values_list('category', flat=True)\n        self.assertCountEqual(results, ['High', 'Medium', 'Low'] * 3)\n", "    def test_case_expression(self):\n        # Test simple \n        case_sql = \"\"\"\n        CASE\n            WHEN integer = 1 THEN 'one'\n            WHEN integer = 2 THEN 'two'\n            ELSE 'other'\n        END\n        \"\"\"\n\n        result = CaseTestModel.objects.values(\n            'integer',\n            'string',\n            'integer2',\n            'cases'\n        ) \\\n            .annotate(cases=Case(\n                When(integer=1, then='one'),\n                When(integer=2, then='two'),\n                default='other'\n            )) \\\n            .order_by('integer') \\\n            .query.as_sql(using='default')\n\n        self.assertEqual(result, case_sql)\n\n        # Test multiple expressions\n        case_sql = \"\"\"\n        CASE\n            WHEN integer = 1 THEN 'one'\n            WHEN integer = 2 THEN 'two'\n            ELSE 'other'\n        END as cases\n        \"\"\"\n        result = CaseTestModel.objects.values(\n            'integer',\n            'string',\n            'integer2',\n            'cases'\n        ) \\\n            .annotate(cases=Case(\n                When(integer=1, then='one'),\n                When(integer=2, then='two'),\n                default='other'\n            )) \\\n            .order_by('integer') \\\n", "    def test_case_expression_with_empty_when_clauses(self):\n        o = CaseTestModel.objects.create(integer=1, integer2=1, string='1')\n\n        q = CaseTestModel.objects.filter(integer=1)\n        q = q.annotate(\n            result=Case(\n                When(integer2=1, then=1),\n                When(integer2=2, then=2),\n                default=0,\n            )\n        )\n        result = q.values('integer2', 'result')[0]\n        self.assertEqual(result['result'], 1)\n\n        q = CaseTestModel.objects.filter(integer=2)\n        q = q.annotate(\n            result=Case(\n                When(integer2=2, then=2),\n                When(integer2=1, then=1),\n                default=0,\n            )\n        )\n        result = q.values('integer2', 'result')[0]\n        self.assertEqual(result['result'], 2)\n\n        q = CaseTestModel.objects.filter(integer=3)\n        q = q.annotate(\n            result=Case(\n                When(integer2=4, then=4),\n                When(integer2=3, then=3),\n                default=0,\n            )\n        )\n        result = q.values('integer2', 'result')[0]\n        self.assertEqual(result['result'], 0)\n\n\n\n", "    def test_case_expressions_with_raw_sql(self):\n        q = CaseTestModel.objects.annotate(\n            my_case=Case(\n                When(Q(integer=2), then=Value('two')),\n                When(Q(integer=3), then=Value('three')),\n                default=Value('other'),\n            )\n        )\n        self.assertCountEqual(\n            q.values_list('my_case', flat=True), ['two', 'three', 'other', 'two', 'three', 'other', 'other', 'other', 'three', 'other', 'other', 'other', 'three', 'other', 'other']\n        )\n\n", "    def test_case_expression_with_field_and_expression(self):\n        query = CaseTestModel.objects.annotate(\n            value=Case(\n                When(CaseTestModel.integer > 2, then=F('integer') * 2),\n                When(CaseTestModel.integer == 2, then=Value('2.5')),\n                default=Value('0'),\n            )\n        )\n        result = query.values('value').order_by('integer')\n        expected_results = [\n            {'value': 2},\n            {'value': 2.5},\n            {'value': 4},\n            {'value': 4},\n            {'value': 4},\n            {'value': 0},\n            {'value': 0},\n            {'value': 0},\n            {'value': 0},\n            {'value': 6},\n            {'value': 8},\n            {'value': 8},\n            {'value': 8},\n            {'value': 8},\n        ]\n        self.assertEqual(list(result), expected_results)\n\n\n\n"], "sample_262": ["    def test_lazy_string(self):\n        @lazy\n            return \"hello\"\n\n        result = my_func()\n        self.assertEqual(result, \"hello\")\n", "    def test_partition(self):\n        self.assertEqual(partition(lambda x: x > 3, range(5)), ([0, 1, 2, 3], [4]))\n        self.assertEqual(partition(lambda x: x % 2 == 0, range(5)), ([0, 2, 4], [1, 3]))\n        self.assertEqual(partition(lambda x: x.upper() == \"A\", [\"A\", \"B\", \"a\", \"C\"]), ([\"A\"], [\"B\", \"a\", \"C\"]))\n", "    def test_lazy(self):\n        @lazy\n            return a + b\n\n        result = func(1, 2)\n\n        self.assertEqual(result, 3)\n", "    def test_lazy_object_creation(self):\n            return 5\n\n        lazy_obj = SimpleLazyObject(return_5)\n        self.assertIsNot(lazy_obj._wrapped, 5)\n\n        lazy_obj._setup()\n        self.assertEqual(lazy_obj._wrapped, 5)\n        self.assertEqual(lazy_obj(), 5)\n", "    def test_lazy_object_deepcopy(self):\n        class MyObj:\n                self.value = value\n\n        obj = SimpleLazyObject(lambda: MyObj(\"Original\"))\n        obj_copy = copy.deepcopy(obj)\n        self.assertNotSame(obj._wrapped, obj_copy._wrapped)\n        self.assertEqual(obj._wrapped.value, \"Original\")\n        self.assertEqual(obj_copy._wrapped.value, \"Original\")\n"], "sample_617": ["    def test_apply_ufunc_broadcasting(a, b, dim, expected):\n        res = apply_ufunc(operator.add, a, b, dim=dim)\n        assert_identical(res, expected)\n\n\n\n", "    def test_unify_chunks_no_dask_arrays():\n        da1 = xr.DataArray(np.arange(10), dims='x')\n        da2 = xr.DataArray(np.arange(10), dims='y')\n        result = unify_chunks(da1, da2)\n        assert isinstance(result[0], xr.DataArray)\n        assert isinstance(result[1], xr.DataArray)  \n\n\n        \n", "    def test_unify_chunks_scalar_input(array_type):\n        a = array_type(np.array(1))\n        b = array_type(np.array(2))\n        result = unify_chunks(a, b)\n        assert isinstance(result[0], array_type)\n        assert isinstance(result[1], array_type)\n        assert result[0].data == 1\n        assert result[1].data == 2\n", "def test_join_dict_keys_with_common_keys():\n    dict1 = {\"a\": 1, \"b\": 2, \"c\": 3}\n    dict2 = {\"b\": 4, \"c\": 5, \"d\": 6}\n    expected = {\"a\": 1, \"b\": 2, \"c\": 3, \"d\": 6}\n    assert_identical(join_dict_keys(dict1, dict2, join=\"outer\"), expected)\n\n\n\n", "    def test_collect_dict_values_empty():\n        assert collect_dict_values({}) == []\n"], "sample_489": ["    def test_bulk_create_with_foreign_keys(self):\n        with connection.cursor() as cursor:\n            cursor.execute(f'TRUNCATE TABLE {Pizzeria._meta.db_table}')\n        Pizzeria.objects.bulk_create(self.data2)\n        pizzerias = Pizzeria.objects.all()\n        self.assertEqual(len(pizzerias), 3)\n\n        pizzerias[0].country.name  # Assert that the foreign key relationship is populated.\n        pizzerias[1].country.name  \n        pizzerias[2].country.name\n", "    def test_bulk_create_with_pk_fields(self):\n        Country.objects.all().delete()\n        Country.objects.bulk_create(self.data_with_pk)\n        countries = Country.objects.all()\n        self.assertEqual(len(countries), 4)\n        self.assertEqual(countries[0].id, 1)\n        self.assertEqual(countries[1].id, 2)\n        self.assertEqual(countries[2].id, 3)\n        self.assertEqual(countries[3].id, 4)\n", "    def test_bulk_create_multiple_objects_with_batch_size(self):\n        Country.objects.all().delete()\n        with connection.cursor() as cursor:\n            cursor.execute(\"SET SESSION max_bulk_size = 1000\")\n        with self.assertNumQueries(4):\n            Country.objects.bulk_create(self.data)\n        with self.assertNumQueries(2):\n            Country.objects.bulk_create(self.data2)\n\n\n\n", "    def test_bulk_create_with_foreign_keys(self):\n        # Test bulk creation with foreign key fields.\n        Country.objects.all().delete()\n        State.objects.all().delete()\n        Pizzeria.objects.all().delete()\n\n        with connection.cursor() as cursor:\n            cursor.execute(\n                \"\"\"\n                CREATE TABLE django_content_type (\n                    id INT PRIMARY KEY,\n                    app_label VARCHAR(100) NOT NULL,\n                    model VARCHAR(100) NOT NULL\n                )\n            \"\"\"\n            )\n\n\n        with connection.cursor() as cursor:\n            cursor.execute(\n                \"\"\"\n                CREATE INDEX idx_country_name ON django_content_type (app_label, model)\n            \"\"\"\n            )\n\n        # Insert into the database using bulk_create.\n        Country.objects.bulk_create(self.data)\n        State.objects.bulk_create(self.states)\n        Pizzeria.objects.bulk_create(self.pizzerias)\n\n        # Verify the data was inserted correctly.\n        self.assertEqual(Country.objects.count(), 4)\n        self.assertEqual(State.objects.count(), 3)\n        self.assertEqual(Pizzeria.objects.count(), 3)\n\n\n\n", "    def test_bulk_create_with_related_data(self):\n        Country.objects.all().delete()\n        Pizzeria.objects.all().delete()\n        created_countries = Country.objects.bulk_create(self.data)\n        created_pizzerias = Pizzeria.objects.bulk_create(self.data2)\n\n        self.assertEqual(len(created_countries), 4)\n        self.assertEqual(len(created_pizzerias), 2)\n        self.assertEqual(created_pizzerias[0].country_id, created_countries[0].id)\n        self.assertEqual(created_pizzerias[1].country_id, created_countries[1].id)\n\n\n\n"], "sample_1145": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1180": ["    def test_point2d_intersection(self):\n        p1 = Point2D(1, 2)\n        p2 = Point2D(3, 4)\n        self.assertEqual(p1.intersection(p2), [])\n        self.assertEqual(p1.intersection(Point2D(1, 2)), [Point2D(1, 2)])\n", "    def test_point_2d_normalize_dimension(self):\n        self.assertEqual(Point2D(1, 2).ambient_dimension, 2)\n        self.assertEqual(Point2D(1, 2, 3)._normalize_dimension(Point2D(4, 5)), (Point2D(1, 2), Point2D(4, 5)))\n        self.assertEqual(Point2D(1, 2)._normalize_dimension(Point3D(4, 5, 6)), (Point2D(1, 2), Point3D(4, 5, 6)))\n\n", "    def test_Point_intersection_with_line(self):\n        l = Line(Point(0, 0), Point(1, 1))\n        p = Point(0, 0)\n        self.assertEqual(p.intersection(l), [p])\n\n        p = Point(1, 1)\n        self.assertEqual(p.intersection(l), [p])\n\n        p = Point(0.5, 0.5)\n        self.assertEqual(p.intersection(l), [p])\n\n        p = Point(2, 2)\n        self.assertEqual(p.intersection(l), [])\n", "    def test_evalf(self, p, expected):\n        assert p.evalf() == expected\n", "    def test_intersection_plane(self):\n        p1 = Point3D(1, 2, 3)\n        plane = Plane((0, 0, 0), (1, 0, 0))\n        intersection = p1.intersection(plane)\n        self.assertEqual(intersection, [Point3D(1, 2, 0)])\n\n\n"], "sample_75": ["    def test_prefetch_related_with_m2m_field(self):\n\n        with CaptureQueriesContext() as queries:\n            books = Book.objects.prefetch_related(\"authors\", \"reviews\").all()\n\n        self.assertEqual(len(queries.captured_queries), 1)\n\n        # Ensure the books have authors and reviews fetched.\n        for book in books:\n            self.assertIsNotNone(book.authors.all())  \n            self.assertIsNotNone(book.reviews.all())\n\n        # Ensure the prefetch query includes both relationships.\n        self.assertWhereContains(queries.captured_queries[0], \"books_authors\")\n        self.assertWhereContains(queries.captured_queries[0], \"book_reviews\")\n", "    def test_prefetch_related_with_chain(self):\n        with CaptureQueriesContext() as ctx:\n            readers = Reader.objects.prefetch_related(\n                'books_read__authors'\n            ).all()\n            self.assertCountEqual(readers, [self.reader1, self.reader2])\n            self.assertEqual(len(ctx.captured_queries), 2)  # 1 query for readers + 1 for related data\n\n            # Check JOIN conditions\n            sql = ctx.captured_queries[1][0]\n            self.assertWhereContains(sql, 'reader_id')\n\n\n", "    def test_prefetch_related2(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\"\"\"\n                SELECT COUNT(*)\n                FROM django_content_type\n                WHERE app_label='articles'\n            \"\"\")\n            count = cursor.fetchone()[0]\n\n        qs = Article.objects.prefetch_related('author__profile', 'tags')\n        seen_articles = set()\n        for article in qs:\n            seen_articles.add(article)\n\n        self.assertEqual(len(seen_articles), count)\n\n\n\n", "    def test_prefetch_related_with_non_cached_related_manager(self):\n        with CaptureQueriesContext() as captured_queries:\n            with connection.cursor() as cursor:\n                cursor.execute('SELECT id FROM django_content_type')\n                content_type_id = cursor.fetchone()[0]\n\n            queryset = Book.objects.prefetch_related(\n                Prefetch('authors', queryset=Author.objects.filter(id=1))\n            ).select_related('authors__address')\n            qs1 = queryset.prefetch_related('authors__address')\n\n        self.assertEqual(len(captured_queries), 2)\n        self.assertWhereContains(captured_queries[0].sql, 'django_content_type.id = %s')\n        self.assertWhereContains(captured_queries[1].sql, 'django_content_type.id = %s')\n\n", "    def test_prefetch_related_with_nested_objects(self):\n        with CaptureQueriesContext(connection) as queries:\n            reader1 = Reader.objects.prefetch_related(\n                Prefetch(\n                    'books_read',\n                    queryset=Book.objects.select_related('authors'),\n                    to_attr='related_books',\n                )\n            ).get(name='Amy')\n\n        self.assertEqual(len(queries), 2)\n        # First query fetches Reader and related Books\n        self.assertWhereContains(queries[0].sql, \"AND Reader.id = RELATED_BOOKS.reader_id\")\n\n        # Second query fetches Books with all their related Authors\n        self.assertWhereContains(queries[1].sql, \"AND Book.id = RELATED_AUTHORS.book_id\")\n"], "sample_579": ["    def test_clustermap_single_linkage_with_custom_method(self):\n        data = self.df_norm.copy()\n        data.columns = range(8)\n        linkage_col = hierarchy.linkage(\n            data.values.T, method=\"single\"\n        )\n        linkage_row = hierarchy.linkage(data.values, method=\"single\")\n\n        plotter = ClusterGrid(data, figsize=(8, 6), row_linkage=linkage_row,\n                              col_linkage=linkage_col)\n\n        fig = plotter.plot(\n            method=\"single\", metric=\"euclidean\", dendrogram_ratio=0.2,\n            colors_ratio=0.03, cbar_pos=(0.02, 0.8, 0.05, 0.18)\n        )\n\n        # Assert that the linkage method is used\n        dendrogram_row = plotter.dendrogram_row\n        dendrogram_col = plotter.dendrogram_col\n        assert dendrogram_row.method == \"single\"\n        assert dendrogram_col.method == \"single\"\n\n        # Assert that the plot is generated correctly\n        ax_heatmap = fig.axes[2]\n        assert ax_heatmap.get_title() is None\n        assert len(ax_heatmap.get_xticklabels()) == 8\n        assert len(ax_heatmap.get_yticklabels()) == 4\n\n\n\n", "    def test_clustermap_basic(self):\n        # Test a basic clustermap with default parameters\n        fig, ax = plt.subplots(figsize=(5, 4))\n        sns.clustermap(self.df_norm, ax=ax)\n        plt.close(fig)\n        ", "    def test_clustermap_basic(self):\n\n        fig, axes = plt.subplots(figsize=(7, 5))\n        mat.clustermap(self.df_norm, cmap=\"viridis\", ax=axes)\n\n        plt.show()\n        \n        fig, axes = plt.subplots(figsize=(7, 5))\n        mat.clustermap(self.df_norm, cmap=\"viridis\",\n                       # Test passing a custom cbar\n                       cbar_kw={'orientation': 'vertical'}, ax=axes)\n\n        plt.show()\n\n        fig, axes = plt.subplots(figsize=(7, 5))\n        mat.clustermap(self.df_norm, cmap=\"viridis\",  # Test annot\n                       annot=True, ax=axes)\n        plt.show()\n\n\n", "    def test_clustergrid_basic(self):\n        with tempfile.TemporaryDirectory() as tempdir:\n            fig, ax = plt.subplots()\n            plotter = ClusterGrid(self.df_norm, figsize=(5, 5))\n            plotter.plot(method='ward', metric='euclidean', cbar_kws={'label': 'Test'},\n                         row_cluster=True, col_cluster=True, dendrogram_ratio=0.1)\n            plotter.savefig(f\"{tempdir}/test.png\")\n            plt.close()\n            \n            with open(f\"{tempdir}/test.png\", 'rb') as f:\n                img_data = f.read()\n\n            assert img_data\n", "    def test_clustermap(self):\n        df = self.df_norm.copy()\n        # Set a simple mask to demonstrate masking\n        df.iloc[1:, 2:5] = np.nan\n        mask = df.isnull()\n\n        # Create a linkage matrix for demonstration\n        linkage_data = hierarchy.linkage(distance.euclidean, df.values, method='ward')\n\n        # Test with default parameters\n        clustergrid = seaborn.clustermap(df, linkage=linkage_data, mask=mask, **self.default_kws)\n        fig = clustergrid.fig\n\n        # Test with custom color palettes\n        cmap = 'viridis'\n        sns.set_context('notebook', font_scale=1.5)\n        \n        clustergrid_custom_cmap = seaborn.clustermap(\n            df, linkage=linkage_data, mask=mask, cmap=cmap, **self.default_kws\n        )\n        \n        fig_custom_cmap = clustergrid_custom_cmap.fig\n\n        # Check if the heatmaps match\n        assert_colors_equal(clustergrid.ax_heatmap, clustergrid_custom_cmap.ax_heatmap)\n\n\n"], "sample_1012": ["    def test_numpy_sparse_matrix(self):\n        mat = SparseMatrix(((1, 0), (0, 1)), ((0, 1), (1, 0)), (2, 2))\n        code = pycode(mat, printer=NumPyPrinter())\n        assert code == 'scipy.sparse.coo_matrix([(0, 1, 1), (1, 0, 1)], (0, 1, 1), shape=(2, 2))'\n", "    def test_piecewise(self):\n        piecewise = Piecewise((x > 1, 2*x, x <= 1, x**2))\n        code = pycode(piecewise)\n        self.assertEqual(code, 'numpy.select([x > 1, x <= 1], [2*x, x**2], default=numpy.nan)')\n", "    def test_scipy_sparse_matrix(self):\n        mat = SparseMatrix([[1, 2], [3, 4]], (2, 2))\n        code = SciPyPrinter().doprint(mat)\n        assert code == 'scipy.sparse.coo_matrix([[1, 2], [3, 4]], ((0, 0), (0, 1), (1, 0), (1, 1)), shape=(2, 2))'\n", "    def test_piecewise(self):\n        expr = Piecewise((x + 1, x < 0), (x**2, True))\n        code = pycode(expr)\n        assert code == 'numpy.select(x < 0, x + 1, x**2, default=numpy.nan)'\n\n        expr = Piecewise((x + 1, x < 0), (x**2, x >= 0))\n        code = pycode(expr)\n        assert code == 'numpy.select(x < 0, x + 1, x**2, default=numpy.nan)'\n\n        expr = Piecewise((x + 1, x < 0), (x**2, x == 0), (x + 3, x > 0))\n        code = pycode(expr)\n        assert code == 'numpy.select(x < 0, x + 1, (x == 0) , x**2, default=numpy.nan)'\n", "    def test_piecewise(self):\n        p = Piecewise((x > 0, 2*x,\n                       (x == 0, 1,\n                        (True, x**2)))\n                       )\n\n        expected = \"\"\"numpy.select((x > 0), 2 * x, (x == 0, 1), default=numpy.nan)\"\"\"\n        assert pycode(p, printer=NumPyPrinter()) == expected\n"], "sample_756": ["def test_optics_dbscan_equivalence():\n        # Test if OPTICS extraction can recover DBSCAN results\n        dbscan = DBSCAN(eps=0.7, min_samples=5)\n        optics = OPTICS(eps=0.7, min_samples=5)\n        optics.fit(X)\n        \n        dbscan_labels = dbscan.fit_predict(X)\n        optics_labels = optics.extract_dbscan(0.7)\n        assert_array_equal(dbscan_labels, optics_labels)\n\n    ", "    def test_optics_dbscan_equivalent(self):\n        optics_ = OPTICS(eps=0.5, min_samples=5, algorithm='ball_tree')\n        optics_.fit(X)\n        dbscan = DBSCAN(eps=0.5, min_samples=5)\n        dbscan.fit(X)\n        assert_array_equal(optics_.labels_, dbscan.labels_)\n", "    def test_optics_reachability_plot_no_valid_maxima(self):\n        X_small = np.random.randn(10, 2)\n        optics = OPTICS(min_samples=1)\n        optics.fit(X_small)\n        assert np.all(np.isinf(optics.reachability_))\n        # Test that the reachability plot is all inf.\n\n\n\n", "    def test_optics_with_different_algorithms(algorithm):\n        optics = OPTICS(algorithm=algorithm, min_samples=5, max_eps=np.inf)\n        optics.fit(X)\n        # Test that ordering and reachability are computed correctly\n        assert_array_equal(\n            optics.ordering_, np.argsort(optics.reachability_)\n        )\n\n", "    def test_dbscan_equivalence(self):\n        # Test equivalence of OPTICS with DBSCAN.\n        optics = OPTICS(eps=0.3)\n        optics.fit(X)\n        dbscan = DBSCAN(eps=0.3).fit(X)\n\n        assert_array_equal(optics.labels_, dbscan.labels_)\n\n\n"], "sample_154": ["    def test_check_database_backends_returns_issues(self, mock_check):\n        mock_check.return_value = ['database issue 1', 'database issue 2']\n        issues = check_database_backends(databases=self.databases)\n        self.assertEqual(issues, ['database issue 1', 'database issue 2'])\n", "        def test_check_database_backends(self, mock_check):\n            mock_check.return_value = [\n                ('warning', 'message', 'hint', 'location'),\n                ('error', 'message', 'hint', 'location'),\n            ]\n            result = check_database_backends(databases=self.databases)\n            mock_check.assert_called_with(**{})\n            self.assertEqual(result, [\n                ('warning', 'message', 'hint', 'location'),\n                ('error', 'message', 'hint', 'location'),\n            ])\n", "    def test_check_database_backends_with_specific_databases(self, mock_check):\n        mock_check.return_value = ['Database error 1', 'Database error 2']\n        issues = check_database_backends(databases=self.databases, **{})\n        self.assertEqual(issues, ['Database error 1', 'Database error 2'])\n", "    def test_check_database_backends_empty_databases(self, mock_check):\n        mock_check.return_value = []\n        issues = check_database_backends(databases=None)\n        self.assertEqual(issues, [])\n", "    def test_check_database_backends_empty_databases(self, mock_check):\n        mock_check.return_value = []\n        issues = check_database_backends(databases=None)\n        self.assertEqual(issues, [])\n"], "sample_274": ["    def test_model_multiple_choice_field_invalid_list(self):\n        class MyForm(Form):\n            field = ModelMultipleChoiceField(ChoiceModel.objects.all())\n\n        form = MyForm({'field': [1, 'a']})\n        self.assertFormErrors({'field': [self.error_messages['invalid_list']]}, form.clean, )\n", "compilation error", "    def setUp(self):\n        self.choices = ChoiceModel.objects.create_batch(3)\n        self.field = ModelChoiceField(queryset=self.choices)\n", "    def test_form_renders_errors_for_boolean_fields(self):\n        class BooleanForm(Form):\n            field = BooleanField()\n\n        form = BooleanForm({'field': ''})\n        self.assertEqual(form.errors, {'field': ['This field is required.']})\n        template = Template('{{ form.as_p }}')\n        context = Context({'form': form})\n        self.assertIn('<p class=\"error\">This field is required.</p>', template.render(context))\n", "    def test_form_errors_messages(self):\n        class MyForm(Form):\n            name = CharField(max_length=10)\n\n        form = MyForm(data={'name': ''})\n        self.assertFormErrors(\n            {'name': ['This field is required.']}, form.clean,\n        )\n"], "sample_429": ["compilation error", "    def test_multiple_int_list_validators(self):\n        validator = int_list_validator(sep=\",\", allow_negative=True)\n        self.assertEqual(validator( \"1,2,-3,4\").split(\",\"), [1, 2, -3, 4])\n\n\n", "compilation error", "compilation error", "compilation error"], "sample_132": ["    def test_callable_setting_wrapper(self):\n        callable_setting = lambda: 'value'\n        wrapper = CallableSettingWrapper(callable_setting)\n        self.assertIs(wrapper.__wrapped__, callable_setting)\n        self.assertNotIsInstance(wrapper, callable_setting)\n        self.assertEqual(wrapper(), 'value')\n", "    def test_callable_setting_wrapper_not_called_in_debug(self):\n        with mock.patch('django.conf.settings', autospec=True) as settings_mocked:\n            settings_mocked.DEBUG = True\n            callable_setting = lambda: 'test'\n            wrapper = CallableSettingWrapper(callable_setting)\n            self.assertEqual(wrapper, callable_setting)\n", "    def test_callable_setting_wrapper_doesnt_call_wrapped(self):\n        wrapped = CallableSettingWrapper(lambda: 'This should not be called')\n        self.assertEqual(repr(wrapped), 'lambda: \\'This should not be called\\')\n        self.assertNotEqual(wrapped(), 'This should not be called')\n\n\n\n", "    def test_callable_setting_wrapper_is_not_called_in_debug(self):\n        class MockCallable:\n                raise Exception(\"This should not be called\")\n\n        wrapper = CallableSettingWrapper(MockCallable())\n        with self.assertRaises(Exception):\n            wrapper()\n", "    def test_callable_setting_wrapper(self):\n        callables = [\n            lambda: 'return_value',\n            lambda x: x * 2,\n            lambda x, y: x + y\n        ]\n        for callable_object in callables:\n            wrapper = CallableSettingWrapper(callable_object)\n            self.assertEqual(wrapper.__repr__(), repr(callable_object))\n            self.assertEqual(wrapper(), callable_object())\n"], "sample_726": ["    def test_label_binarize_multilabel_indicator(self):\n        mlb = MultiLabelBinarizer()\n        y = [[1, 2], [2], [1, 3]]\n        mlb.fit(y)\n        y_binarized = mlb.transform(y)\n        assert_true((y_binarized.sum(axis=1) == np.array([2, 1, 2])).all())\n\n        y_inverse = mlb.inverse_transform(y_binarized)\n        for i, sample in enumerate(y_inverse):\n            assert_equal(sample, set(y[i]))\n", "    def test_multilabel_binarizer_sparse_output(self):\n        mlb = MultiLabelBinarizer(sparse_output=True)\n        mlb.fit(\n            [(1, 2), (3,), (1, 2, 3)]\n        )\n        y_sparse = mlb.transform(\n            [(1, 2), (3,), (1, 2, 3)]\n        )\n\n        # Check if output is sparse\n        assert issparse(y_sparse)\n\n        # Check if output is correct\n        expected = coo_matrix(\n            [[1, 1, 0], [0, 0, 1], [1, 1, 1]],\n            shape=(3, 3),\n        )\n        assert_array_equal(\n            toarray(y_sparse), toarray(expected)\n        )\n", "def test_multilabel_binarizer_fit_transform():\n    mlb = MultiLabelBinarizer()\n\n    # Test with basic sets\n    X = [set(['sci-fi', 'thriller']), set(['comedy']), set(['sci-fi'])]\n    X_indicator = mlb.fit_transform(X)\n    assert_array_equal(mlb.classes_, ['comedy', 'sci-fi', 'thriller'])\n    assert_true(np.all(X_indicator == [[0, 1, 1], [1, 0, 0], [1, 0, 0]]))\n\n\n    # Test with tuples\n    X = [('sci-fi', 'thriller'), ('comedy', ), ('sci-fi',)]\n    X_indicator = mlb.fit_transform(X)\n    assert_array_equal(mlb.classes_, ['comedy', 'sci-fi', 'thriller'])\n    assert_true(np.all(X_indicator == [[0, 1, 1], [1, 0, 0], [1, 0, 0]]))\n\n    # Test with empty sets\n    X = [set(), set(), set()]\n    X_indicator = mlb.fit_transform(X)\n    assert_array_equal(mlb.classes_, [])\n    assert_true(np.all(X_indicator == [[0, 0, 0], [0, 0, 0], [0, 0, 0]]))\n\n\n\n    # Test with sparse data\n    X = [set(['sci-fi']), set(['sci-fi', 'thriller']), set(['comedy'])]\n    X_indicator = mlb.fit_transform(X)\n    assert_true(sp.issparse(X_indicator))\n\n    # Test with custom classes\n    mlb = MultiLabelBinarizer(classes=['rock', 'jazz', 'blues'])\n    X = [set(['jazz', 'blues']), set(['rock', 'jazz'])]\n    X_indicator = mlb.fit_transform(X)\n    assert_array_equal(mlb.classes_, ['blues', 'jazz', 'rock'])\n    assert_true(np.all(X_indicator == [[0, 1, 1], [1, 1, 0]]", "    def test_multilabel_binarizer_sparse_output(self):\n        mlb = MultiLabelBinarizer(sparse_output=True)\n        mlb.fit([(1, 2), (3,)])\n        y_indicator = mlb.transform([(1, 2), (3,)])\n        assert isinstance(y_indicator, coo_matrix)\n", "compilation error"], "sample_214": ["    def test_get_prep_value_with_int_encode(self):\n        field = JSONField(encoder=int)\n        self.assertEqual(field.get_prep_value(123), '123')\n", "    def test_json_field_validation(self):\n        model = JSONModel(data=None)\n        with self.assertRaises(ValidationError):\n            model.full_clean()\n\n        with self.assertRaises(ValidationError):\n            model.data = 'invalid json'\n            model.full_clean()\n\n        model.data = '{\"key\": \"value\"}'\n        model.full_clean()\n\n\n\n", "    def test_json_field_with_encoder_and_decoder(self):\n        with connection.schema_editor() as editor:\n            editor.create_model(\n                'TestModel',\n                fields=[\n                    models.JSONField(\n                        encoder=CustomJSONEncoder, decoder=CustomJSONDecoder,\n                        blank=True, null=True,\n                    ),\n                ],\n            )\n        instance = TestModel.objects.create()\n        json_value = {'a': 1, 'b': 2}\n        instance.json_field = json_value\n        instance.save()\n\n        loaded_instance = TestModel.objects.get(pk=instance.pk)\n        self.assertEqual(loaded_instance.json_field, json_value)\n\n", "    def test_key_transform_exact_with_null_value(self):\n        self.assertTrue(\n            NullableJSONModel.objects.filter(\n                json_field__isnull=True\n            ).exists()\n        )\n", "    def test_json_exact_with_nested_keys_oracle(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\"\"\"\n                CREATE TABLE json_products (\n                    id SERIAL PRIMARY KEY,\n                    data JSON\n                );\n            \"\"\")\n            cursor.execute(\"INSERT INTO json_products (data) VALUES ('{'product': {'name': 'Laptop', 'price': 1200, 'specs': {'ram': 8, 'storage': 512}}}')\")\n            product = JSONModel.objects.create(data={'product': {'name': 'Laptop', 'price': 1200, 'specs': {'ram': 8, 'storage': 512}}})\n\n        self.assertEqual(JSONModel.objects.filter(data__product__name='Laptop').count(), 1)\n\n\n\n"], "sample_298": ["    def test_token_generation_with_custom_email_field(self):\n        user = User(username='testuser', password='password')\n        user.save()\n        user.email = 'test@example.com'\n        user.save()\n\n        with override_settings(PASSWORD_RESET_TIMEOUT=3600):\n            generator = MockedPasswordResetTokenGenerator(datetime.now() - timedelta(seconds=3000))\n            token = generator.make_token(user)\n\n            self.assertTrue(generator.check_token(user, token))\n\n        with override_settings(PASSWORD_RESET_TIMEOUT=1800):\n            generator = MockedPasswordResetTokenGenerator(datetime.now() - timedelta(seconds=4000))\n            token = generator.make_token(user)\n\n            self.assertFalse(generator.check_token(user, token))\n", "    def test_token_generator_with_custom_user_model(self):\n        now = datetime.now()\n        user = User.objects.create(username='testuser', password='testpassword')\n        user.email = 'testuser@example.com'\n        user.save()\n\n        with override_settings(\n            PASSWORD_RESET_TIMEOUT=5,\n        ):\n            generator = MockedPasswordResetTokenGenerator(now)\n            token = generator.make_token(user)\n            self.assertTrue(generator.check_token(user, token))\n\n", "    def test_token_expiration(self):\n        now = datetime.now()\n        password_reset_timeout = settings.PASSWORD_RESET_TIMEOUT\n        generator = MockedPasswordResetTokenGenerator(now)\n\n        user = User.objects.create_user(username=\"testuser\", password=\"password\")\n\n        # Generate a token with a timestamp within the timeout limit\n        token = generator.make_token(user)\n\n        self.assertTrue(generator.check_token(user, token))\n\n        # Simulate time passing beyond the timeout\n        future_now = now + timedelta(seconds=password_reset_timeout + 1)\n        generator = MockedPasswordResetTokenGenerator(future_now)\n\n        # Token should now be expired\n        self.assertFalse(generator.check_token(user, token))\n", "    def test_token_expiration(self):\n        now = datetime.now()\n        generator = MockedPasswordResetTokenGenerator(now)\n        user = User(username='testuser', email='test@example.com')\n        user.save()\n\n        token = generator.make_token(user)\n        \n        # Simulate time passing\n        future_now = now + timedelta(seconds=settings.PASSWORD_RESET_TIMEOUT + 1)\n        generator_future = MockedPasswordResetTokenGenerator(future_now)\n\n        self.assertTrue(generator.check_token(user, token))\n        self.assertFalse(generator_future.check_token(user, token))\n\n\n\n", "    def test_token_generation_with_custom_timestamp(self):\n        now = datetime.now()\n        generator = MockedPasswordResetTokenGenerator(now)\n        user = User(username=\"testuser\", email=\"test@example.com\")\n        user.save()\n\n        token = generator.make_token(user)\n        \n        ts_b36, hash_string = token.split(\"-\")\n\n        # Verify that the timestamp is correctly encoded in base36\n        self.assertIsInstance(ts_b36, str)\n        self.assertEqual(base36_to_int(ts_b36), generator._num_seconds(now))\n\n        # Verify that the hash is valid\n        self.assertTrue(generator.check_token(user, token))\n\n        # Check that a token generated with a different timestamp is invalid\n        different_now = now + timedelta(seconds=1)\n        different_token = generator.make_token(user)\n        self.assertFalse(generator.check_token(user, different_token))\n"], "sample_1044": ["    def test_as_content_primitive_nested_sqrt(self):\n        from sympy.functions import sqrt\n        x = Symbol('x')\n        expr = sqrt(sqrt(x) + 1)\n        expr_content, expr_primitive = expr.as_content_primitive()\n        assert expr_content == 1\n        assert expr_primitive == sqrt(sqrt(x) + 1)\n\n        expr = sqrt(2*sqrt(x) + 1)\n        expr_content, expr_primitive = expr.as_content_primitive()\n        assert expr_content == 1\n        assert expr_primitive == sqrt(2*sqrt(x) + 1)\n", "    def test_as_base_exp_with_complex_exponent(self):\n        from sympy.tests.pytest.raises import FloatsNotEqual, NotEqual\n        raises(ValueError, lambda: (2 + I)**(3 + 2*I)).args[0]\n        with raises(ValueError):\n            (2 + I)**(3 + 2*I).as_base_exp()\n        with raises(ValueError):\n            (2 + I)**(1/2).as_base_exp()\n", "    def test_as_content_primitive_special_cases():\n        assert sqrt(4 + 4*sqrt(2)).as_content_primitive() == (2, sqrt(1 + sqrt(2)))\n        assert sqrt(3 + 3*sqrt(2)).as_content_primitive() == (1, sqrt(3)*sqrt(1 + sqrt(2)))\n        assert (2**Rational(1, 2)).as_content_primitive() == (sqrt(2), 2**(Rational(1, 2) - Rational(1, 2)))\n        assert (2**Rational(3, 2)).as_content_primitive() == (sqrt(4), 2**(Rational(1, 2)))\n        assert (3**(Rational(1, 2))).as_content_primitive() == (1, 3**(Rational(1, 2)))\n        assert (3**(Rational(5, 2))).as_content_primitive() == (9, 3**(Rational(1, 2)))\n        assert (2 + 2*x)**Rational(1, 2).as_content_primitive() == (1, (2*(x + 1))**(Rational(1, 2)))\n        assert (2 + 2*x)**Rational(3, 2).as_content_primitive() == (1, (2*(x + 1))**(Rational(3, 2)))\n        assert (2**(1 + x))**Rational(1, 2).as_content_primitive() == (2, 2**x)\n", "    def test_as_content_primitive_sqrt(self):\n        from sympy import sqrt\n        assert (sqrt(4 + 4*sqrt(2))).as_content_primitive() == (2, sqrt(1 + sqrt(2)))\n\n\n\n        assert (sqrt(3 + 3*sqrt(2))).as_content_primitive() == (1, sqrt(3)*sqrt(1 + sqrt(2)))\n", "    def test_as_content_primitive_rational_exponents(self):\n        from sympy import expand_power_base\n        from sympy.abc import x\n        eq = (2*x + 2)**(Rational(3,2))\n        s = expand_power_base(eq); s.is_Mul, s\n        r, n = eq.as_content_primitive()\n        assert r == 4\n        assert n.simplify() == sqrt(x + 1)**3\n        eq = (4*x + 2)**(Rational(5,2))\n        s = expand_power_base(eq); s.is_Mul, s\n        r, n = eq.as_content_primitive()\n        assert r == 16\n        assert n.simplify() == sqrt(x + 1/2)**5\n        eq = (2*x + 2)**(Rational(1,3))\n        s = expand_power_base(eq); s.is_Mul, s\n        r, n = eq.as_content_primitive()\n        assert r == 1\n        assert n.simplify() == (x + 1)**(1/3)\n"], "sample_193": ["    def test_related_model_to_string(self):\n        class RelatedModel(models.Model):\n            test_field = models.CharField(max_length=100)\n\n        with isolate_apps(['myproject']):\n            model = RelatedModel\n            model.objects.create(test_field=\"test\")\n            state = ProjectState()\n            state.add_related_model(model)\n            self.assertEqual(str(state.get_related_model(RelatedModel)), 'RelatedModel')", "    def test_state_multiple_apps_with_circular_references(self):\n\n        # Create apps for testing.\n        app1 = Apps.build_app_configs(\n            apps=['app1', 'app2'],\n            models=[\n                UnicodeModel,\n                ModelWithCustomBase,\n            ],\n        )\n        app2 = Apps.build_app_configs(\n            apps=['app2', 'app1'],\n            models=[\n                FoodManager,\n                FoodQuerySet,\n            ],\n        )\n\n        # Get project state.\n        state = ProjectState(\n            apps=[app1],\n            apps_to_migrate=[app1, app2],\n        )\n\n        # Prepare models.\n        models = state.models\n        model1 = models['app1'].get_model('unicodemodel')\n        model2 = models['app1'].get_model('modelwithcustombase')\n        model3 = models['app2'].get_model('foodmanager')\n        model4 = models['app2'].get_model('foodqueryset')\n        model2.objects.add(model1(value='a'))\n        model3.objects.add(model2(value='b'))\n\n        # Check if circular references are handled correctly.\n        state.build_migration_tree()\n        with self.assertRaises(InvalidBasesError):\n            models.get_migration_graph()\n\n\n\n", "    def test_state_m2m_field_resolution(self):\n        with isolate_apps(apps=['foodapp']):\n            with override_settings(INSTALLED_APPS=['foodapp']):\n                # Create a fresh state\n                state = ProjectState()\n                Food = self.get_model('foodapp', 'Food')\n                # Add a ManyToMany relation to the Food model\n                state.models['foodapp.food'] = Food\n                self.assertEqual(Food._meta.get_field('category').remote_field.model, None)\n                state.models['foodapp.category'] = None\n\n                # Resolve the m2m field in the state\n                state.apply_operation(\n                    AddField(\n                        model_name='foodapp.food',\n                        name='category',\n                        field=models.ManyToManyField('foodapp.category')\n                    )\n                )\n                self.assertEqual(state.models['foodapp.food']._meta.get_field('category').remote_field.model, models.Category)\n", "    def test_get_related_models_recursive_with_m2m(self):\n        class ModelWithM2M(models.Model):\n            name = models.CharField(max_length=50)\n            foods = models.ManyToManyField(UnicodeModel)\n\n        with isolate_apps(['tests.sampleapp2']):\n            state = ProjectState(apps=Apps.get_default())\n            app_config = state.apps['tests.sampleapp2']\n            \n            ModelWithM2M.objects.all().delete()\n            \n            model_instance = ModelWithM2M.objects.create(name=\"Test\")\n            model_instance.foods.add(UnicodeModel.objects.get(id=1))\n\n            related_models = get_related_models_recursive(state, ModelWithM2M)\n            self.assertIn(UnicodeModel, related_models)\n", "    def test_manytomany_field_checks(self):\n        class ModelWithManyToMany(models.Model):\n            name = models.CharField(max_length=100)\n            foods = models.ManyToManyField(UnicodeModel)\n\n        with isolate_apps(['my_app']):\n            with override_settings(INSTALLED_APPS=['my_app']):\n                with self.assertRaises(InvalidBasesError):\n                    apps = Apps(models)\n                    apps.build_model_map()\n\n"], "sample_1022": ["    def test_function_exponentiation():\n        expr = parse_expr('sin**2(x)', transformations=(function_exponentiation,))\n        assert expr == sympy.sin(x)**2\n        expr = parse_expr('cos**(2*x)', transformations=(function_exponentiation,))\n        assert expr == sympy.cos(2*x)\n", "    def test_split_symbols_custom(self):\n        transform = split_symbols_custom(lambda s: s.startswith('abc'))\n        self.assertEqual(transform(split_tokens(\"abcxyz\"), {}, {}) ,\n                         [(NAME, 'Symbol'),(OP, '('),(NAME, 'abc'),(OP, ')')])\n        self.assertEqual(transform(split_tokens(\"defxyz\"), {}, {}) ,\n                         [(NAME, 'Symbol'),(OP, '('),(NAME, 'def'),(OP, ')')])\n", "    def test_function_exponentiation_with_parentheses(self):\n        expr = parse_expr(\"sin**2(x)\")\n        self.assertEqual(expr, sympy.sin(x)**2)\n\n        expr = parse_expr(\"cos**3(x)\")\n        self.assertEqual(expr, sympy.cos(x)**3)\n\n", "    def test_split_symbols_custom(self):\n        # Test split_symbols_custom with custom predicate\n            return symbol.count('_') > 1\n        transform = split_symbols_custom(can_split)\n\n        self.assertEqual(\n            transform((['NAME', 'Symbol'],), {}),\n            [['NAME', 'Symbol']]\n        )\n\n        self.assertEqual(\n            transform((['NAME', 'a_b'],), {}),\n            [['NAME', 'Symbol'], ['NAME', 'Symbol'], ['NAME', 'Symbol']]\n        )\n\n        self.assertEqual(\n            transform((['NAME', 'abc'],), {}),\n            [['NAME', 'abc']]\n        )\n\n\n\n", "    def test_implicit_application_nested_parentheses():\n        expr = parse_expr('(sin(x+y) + 2) * 3',\n                          transformations=(implicit_application,))\n        expected = 3 * (sympy.sin(sympy.add(sympy.Symbol('x'), sympy.Symbol('y'))) + 2)\n        assert expr == expected\n"], "sample_1032": ["    def test_minmax_rewrite_as_Heaviside(self):\n        from sympy import Heaviside\n        x = Symbol('x')\n        y = Symbol('y')\n        z = Symbol('z')\n\n        self.assertEqual(Max(x, y, z)._eval_rewrite_as_Heaviside(x, y, z),\n                         Heaviside(x - y)*x + Heaviside(x - z)*x + Heaviside(y - z)*y)\n        self.assertEqual(Min(x, y, z)._eval_rewrite_as_Heaviside(x, y, z),\n                         Heaviside(y - x)*y + Heaviside(z - x)*z + Heaviside(z - y)*z)\n\n", "    def test_rewrite_as_piecewise_complex():\n        from sympy.functions.elementary.complexes import Abs\n        x = Symbol('x', complex=True)\n        raises(NotImplementedError, lambda: Min(x, x + I).rewrite(Piecewise))\n        raises(NotImplementedError, lambda: Max(x, x + I).rewrite(Piecewise))\n\n", "    def test_max_and_min_with_complex_args(self):\n        x, y = Symbol('x', complex=True), Symbol('y', complex=True)\n        self.assertEqual(Max(x, y).evalf(), \n                         max(x.evalf(), y.evalf()))\n        self.assertEqual(Min(x, y).evalf(), \n                         min(x.evalf(), y.evalf()))\n        self.assertEqual(Max(x, y + I).evalf(), max(x.evalf(), y.evalf() + 1j))\n        self.assertEqual(Min(x, y + I).evalf(), min(x.evalf(), y.evalf() + 1j))\n", "    def test_real_root_with_negative_n():\n        x = Symbol('x')\n        y = Symbol('y')\n        assert real_root(-8, 3) == -2\n        assert real_root(-y**2, 3) == -y\n        assert real_root(-x*y, 3) == -sqrt[3](y)\n        assert real_root(-y**2, 2) == -y\n        assert real_root(-x*y, 2) == -sqrt[2](y)\n\n", "    def test_min_with_oo(self):\n        x = Symbol('x')\n        y = Symbol('y')\n        assert Min(x, oo) == x\n        assert Min(oo, x) == x\n        assert Min(x, oo, oo) == x\n        assert Min(oo, oo, x) == x\n"], "sample_837": ["        def test_get_blas_info_without_blas(self):\n            with self.assertRaises(ImportError):\n                _get_blas_info()  \n", "        def test_missing_dep(self):\n            with self.assertRaises(ImportError):\n                _get_deps_info()  \n", "    def test_show_versions():\n        import unittest\n        sys_info_expected = {\n            \"python\": sys.version.replace('\\n', ' '),\n            'executable': sys.executable,\n            \"machine\": platform.platform(),\n        }\n        with unittest.mock.patch('sklearn.utils._show_versions._get_sys_info', return_value=sys_info_expected):\n            with unittest.mock.patch('sklearn.utils._show_versions._get_deps_info', return_value={}):\n                with unittest.mock.patch('sklearn.utils._show_versions._get_blas_info', return_value={}):\n                    show_versions()\n\n\n\n", "        def test_show_versions(self, mock_blas_info, mock_deps_info, mock_sys_info):\n            show_versions()\n            print_output = []\n            for line in sys.stdout.getvalue().splitlines():\n                print_output.append(line)\n\n            self.assertIn('System:', print_output)\n            self.assertIn('python', print_output)\n            self.assertIn('mocked_python', print_output)\n            self.assertIn('BLAS:', print_output)\n            self.assertIn('cblas_libs', print_output)\n            self.assertIn('mocked_cblas_libs', print_output)\n            self.assertIn('Python deps:', print_output)\n            self.assertIn('pip', print_output)\n            self.assertIn('mocked_pip', print_output)\n", "        def test_get_sys_info(self):\n            sys_info = _get_sys_info()\n            self.assertIn('python', sys_info)\n            self.assertIn('executable', sys_info)\n            self.assertIn('machine', sys_info)\n"], "sample_662": ["    def test_serialize_collectreport(self):\n        sections = [\n            (\"Captured stdout Collect\", \"hello\"),\n            (\"Captured stderr Collect\", \"world\"),\n        ]\n        report = CollectReport(\n            nodeid=\"test\", outcome=\"passed\", longrepr=None, result=[], sections=sections\n        )\n        data = pytest_report_to_serializable(report)\n        assert data[\"$report_type\"] == \"CollectReport\"\n        assert data[\"nodeid\"] == \"test\"\n        assert data[\"outcome\"] == \"passed\"\n        assert data[\"result\"] is None\n        assert data[\"sections\"] == sections\n\n\n\n", "    def test_collectreport_serialization(self):\n        report = CollectReport(\n            nodeid=\"test1\",\n            outcome=\"failed\",\n            longrepr=ExceptionChainRepr(\n                chain=[\n                    (\n                        ReprTraceback(reprentries=[ReprEntry(lines=[\"line1\"])]),\n                        ReprFileLocation(\n                            filename=Path(\"test.py\"), lineno=1,\n                           ),\n                        \"msg1\",\n                    )\n                ]\n            ),\n            result=[\n                {\"foo\": \"bar\"}\n            ],\n        )\n        serialized_data = pytest_report_to_serializable(report)\n        deserialized_report = pytest_report_from_serializable(serialized_data)\n        assert isinstance(deserialized_report, CollectReport)\n        assert deserialized_report.nodeid == report.nodeid\n        assert deserialized_report.outcome == report.outcome\n        assert deserialized_report.longrepr.chain[0][0].reprentries == report.longrepr.chain[0][0].reprentries\n        assert deserialized_report.result == report.result\n\n", "    def test_collect_report_serialization(self):\n        report = CollectReport(\n            nodeid=\"mymodule::test_something\",\n            outcome=\"passed\",\n            longrepr=None,\n            result=[],\n            sections=[(\"Captured stdout\", \"This is the captured stdout\")],\n            fspath=Path(\"mymodule.py\"),\n        )\n        serialized = pytest_report_to_serializable(report)\n        deserialized = pytest_report_from_serializable(serialized)\n        assert isinstance(deserialized, CollectReport)\n        assert deserialized.nodeid == report.nodeid\n        assert deserialized.outcome == report.outcome\n        assert deserialized.sections == report.sections\n        assert deserialized.fspath == report.fspath\n\n", "    def test_collect_report_serialization(self):\n        report = CollectReport(\n            nodeid=\"my_nodeid\",\n            outcome=\"failed\",\n            longrepr=ExceptionChainRepr([(ReprTraceback(), ReprFileLocation(), \"some description\")], \"some description\"), \n            result=[],\n            sections=[(\"Captured stdout\", \"This is captured stdout content\")],\n        )\n        data = pytest_report_to_serializable(report)\n        deserialized_report = pytest_report_from_serializable(data)\n        assert isinstance(deserialized_report, CollectReport)\n        assert deserialized_report.nodeid == report.nodeid\n        assert deserialized_report.outcome == report.outcome\n        assert deserialized_report.longrepr.chain == report.longrepr.chain\n        assert deserialized_report.result == report.result\n        assert deserialized_report.sections == report.sections\n\n\n", "    def test_serialize_deserialize_test_report_with_exception_chain(self):\n        # Make a test report with an exception chain\n        exc_info = ExceptionInfo(\n            type=TypeError,\n            value=ValueError(\"Test\"),\n            traceback=None\n        )\n        inner_exc_info = ExceptionInfo(\n            type=ValueError,\n            value=ZeroDivisionError(\"ZeroDivisionError\"),\n            traceback=None\n        )\n        exc_info._add_to_chain(inner_exc_info)\n        longrepr = ExceptionChainRepr(\n            [\n                (ReprTraceback(),  # Placeholder\n                ReprFileLocation(),  # Placeholder\n                \"Inner exception\"\n                ),\n                (ReprTraceback(),  # Placeholder\n                ReprFileLocation(),\n                \"Test\"\n                )\n            ]\n        )\n        test_report = TestReport(\n            nodeid=\"test_id\",\n            location=(Path(\"test_file.py\"), 12, None),\n            keywords={},\n            outcome=\"failed\",\n            longrepr=longrepr,\n            when=\"call\",\n            sections=[],\n            duration=0,\n        )\n\n        serialized_data = pytest_report_to_serializable(test_report)\n        deserialized_report = pytest_report_from_serializable(serialized_data)\n\n        assert isinstance(deserialized_report, TestReport)\n        assert deserialized_report.nodeid == \"test_id\"\n        assert deserialized_report.outcome == \"failed\"\n        assert len(deserialized_report.longrepr.chain) == 2\n\n\n\n"], "sample_1134": ["    def test_latex_with_symbols(self):\n        result = latex(x**2, symbol_names={x: 'x_i'})\n        expected = r'x_{i}^{2}'\n        self.assertEqual(result, expected)\n", "compilation error", "    def test_latex_sympy_special_functions(self):\n        # Test for special functions\n        self.assertEqual(latex(Abs(x)), r\"\\left| x \\right|\")\n        self.assertEqual(latex(exp(x)), r\"e^{x}\")\n        self.assertEqual(latex(log(x)), r\"\\log{\\left(x \\right)}\")\n        self.assertEqual(latex(sin(x)), r\"\\sin{\\left(x \\right)}\")\n        self.assertEqual(latex(cos(x)), r\"\\cos{\\left(x \\right)}\")\n        self.assertEqual(latex(tan(x)), r\"\\tan{\\left(x \\right)}\")\n        self.assertEqual(latex(cot(x)), r\"\\cot{\\left(x \\right)}\")\n        self.assertEqual(latex(sinh(x)), r\"\\sinh{\\left(x \\right)}\")\n        self.assertEqual(latex(cosh(x)), r\"\\cosh{\\left(x \\right)}\")\n        self.assertEqual(latex(tanh(x)), r\"\\tanh{\\left(x \\right)}\")\n        self.assertEqual(latex(coth(x)), r\"\\coth{\\left(x \\right)}\")\n        self.assertEqual(latex(sqrt(x)), r\"\\sqrt{x}\")\n        self.assertEqual(latex(sqrt(x**2)), r\"\\left| x \\right|\")\n        self.assertEqual(latex(pow(x, 2)), r\"{x}^{2}\")\n        self.assertEqual(latex(Pow(x, 2)), r\"{x}^{2}\")\n        self.assertEqual(latex(gamma(x)), r\"\\Gamma{\\left(x \\right)}\")\n        self.assertEqual(latex(lowergamma(x)), r\"\\gamma{\\left(x \\right)}\")\n        self.assertEqual(latex(factorial(x)), r\"\\Gamma{\\left(x + 1 \\right)}\")\n        self.assertEqual(latex(Pi), r\"\\pi\")\n        self.assertEqual(latex(I), r\"i\")\n\n\n\n", "compilation error", "    def test_multiline_latex_with_dots(self):\n        from sympy import sin, cos,  Symbol\n        x, y = Symbol('x y')\n        expr = sin(x) + cos(y)\n        result = multiline_latex(x, expr, terms_per_line=2, use_dots=True)\n        self.assertEqual(result, r'\\begin{align*}"], "sample_951": ["    def test_signature_from_ast_with_kw_only():\n        code = \"\"\"\n            pass\n        \"\"\"\n        signature = inspect.signature_from_ast(ast.parse(code).body[0])\n        expected_signature = inspect.Signature(\n            parameters=[\n                Parameter('x', Parameter.POSITIONAL_ONLY),\n                Parameter('y', Parameter.POSITIONAL_OR_KEYWORD, default=ast.Constant('default'),),\n                Parameter('z', Parameter.KEYWORD_ONLY),\n            ],\n            return_annotation=inspect.Parameter.empty\n        )\n        assert signature == expected_signature\n\n", "    def test_signature_from_ast_defaults():\n        code = 'def func(a=1, b=2, *args, **kwargs): pass'\n        signature = inspect.signature_from_ast(ast.parse(code).body[0])\n        assert signature.parameters['a'].default is not inspect.Parameter.empty\n        assert signature.parameters['b'].default is not inspect.Parameter.empty\n\n\n\n", "    def test_is_generic_alias(monkeypatch):\n        monkeypatch.setattr(inspect, 'is_builtin_class_method', lambda x, y: False)\n\n        # Test with known generic aliases\n        assert inspect.is_genearicalias(typing.List) is True\n        assert inspect.is_genearicalias(typing.Dict) is True\n\n        # Test with non-generic aliases\n        assert inspect.is_genearicalias(str) is False\n        assert inspect.is_genearicalias(int) is False\n        assert inspect.is_genearicalias(list) is False\n\n\n\n", "    def test_stringify_signature_with_complex_types():\n        sig = inspect.signature(typing.Callable[..., None])\n        assert stringify_signature(sig, show_annotation=True) == '(args: Any) -> None'\n\n        sig = inspect.signature(typing.List[typing.Dict[str, int]])\n        assert stringify_signature(sig, show_annotation=True) == '(args: List[Dict[str, int]]) -> None'\n\n        sig = inspect.signature(typing.NamedTuple('Point', [('x', int), ('y', float)]))\n        assert stringify_signature(sig, show_annotation=True) == '(x: int, y: float) -> Point'\n\n", "    def test_signature_from_str_with_default_values(self):\n        signature = \"def func(a: int = 42, b=None): pass\"\n        sig = inspect.signature_from_str(signature)\n        expected_sig = inspect.Signature(\n            parameters=[\n                inspect.Parameter('a', inspect.Parameter.POSITIONAL_OR_KEYWORD, default=inspect.Parameter.from_callable(lambda: 42)),\n                inspect.Parameter('b', inspect.Parameter.POSITIONAL_OR_KEYWORD, default=inspect.Parameter.empty),\n            ],\n            return_annotation=inspect.Parameter.empty\n        )\n        assert sig == expected_sig\n\n"], "sample_405": ["    def test_alter_together_index(self):\n        with atomic():\n            # Create a model with index_together\n            self.create_model(\n                \"my_app\", \"Author\", fields=[(\"name\", models.CharField(max_length=50))]\n            )\n            self.create_model(\n                \"my_app\", \"Book\", fields=[(\"title\", models.CharField(max_length=100))]\n            )\n            self.apply_migration(\n                f\"my_app/0001_initial.py\",\n                f\"my_app/0002_alter_together_index.py\",\n                \"alter_index_together\",\n            )\n\n            # Assert that the index_together is applied\n            with self.assertNumQueries(1):\n                self.assertEqual(\n                    self.execute_query(\n                        \"SELECT * FROM django_content_type WHERE name = %s\",\n                        (\"my_app.book\",),\n                    )[0][1],\n                    1,\n                )\n            with self.assertNumQueries(1):\n                self.assertEqual(\n                    self.execute_query(\n                        \"SELECT * FROM my_app_book\",\n                    )[0][1],\n                    \"bookname\",\n                )\n\n", "    def test_rename_model(self):\n        with transaction.atomic():\n            with self.assertNumQueries(2):\n                self.create_model(\n                    \"OldName\",\n                    fields=[models.CharField(max_length=100)],\n                )\n\n        self.apply_migration(CreateMigration(\"RenameModel\", \"0001_rename_model\"))\n\n        new_model = self.apps.get_model(\"app\", \"NewName\")\n        self.assertEqual(new_model._meta.db_table, \"oldname\")\n        self.assertEqual(new_model._meta.fields[0].name, \"name\")\n\n        with self.assertNumQueries(2):\n            self.create_model(\n                \"NewName\",\n                fields=[models.CharField(max_length=100)],\n            )\n\n        self.apply_migration(CreateMigration(\"RenameModel\", \"0002_rename_model_table\"))\n        new_model = self.apps.get_model(\"app\", \"NewName\")\n        self.assertEqual(new_model._meta.db_table, \"newname\")\n", "    def test_add_remove_constraint_in_migration(self):\n        with atomic():\n            models.UniqueConstraint(\n                fields=(\"name\", \"description\"), name=\"unique_name_description\"\n            ).create_field(\n                UnicodeModel._meta,\n                \"unique_name_description\",\n            )\n            self.assertEqual(UnicodeModel._meta.get_constraint_by_name(\"unique_name_description\").fields, (\n                'name', \n                'description',\n            ))\n        with self.assertNumQueries(0):\n            with transaction.atomic():\n                state = self.get_state()\n                # Add constraint\n                migration = Migration(\n                    id=\"0001_add_unique_constraint\",\n                    app_label=\"tests\",\n                    name=\"add_unique_constraint\",\n                    operations=[\n                        AddConstraint(\n                            model_name=\"tests.unicodemodel\", constraint=(\n                                models.UniqueConstraint(\n                                    fields=(\"name\", \"description\"), name=\"unique_name_description\"\n                                )\n                            ),\n                        )\n                    ],\n                )\n                self._apply_migration(migration, state)\n                self.assertEqual(UnicodeModel._meta.get_constraint_by_name(\"unique_name_description\").fields, (\n                    'name', \n                    'description',\n                ))\n\n                # Remove constraint\n                state = self.get_state()\n                migration = Migration(\n                    id=\"0002_remove_unique_constraint\",\n                    app_label=\"tests\",\n                    name=\"remove_unique_constraint\",\n                    operations=[\n                        RemoveConstraint(\n                            model_name=\"tests.unicodemodel\", name=\"unique_name_description\"\n                        )\n                    ],\n                )\n                self._apply_migration(migration, state)\n                self.assertQueryCount(0)\n                with self.assertRaises(IntegrityError):\n                    UnicodeModel.objects.create(name='test_name', description='test_description') \n", "    def test_alter_together(self):\n        with transaction.atomic():\n            self.execute_migration(\n                [\n                    CreateModel(\n                        \"content\",\n                        fields=[\n                            models.CharField(\"title\", max_length=255),\n                            models.ForeignKey(\n                                \"auth.user\",\n                                related_name=\"content\",\n                                on_delete=models.CASCADE,\n                            ),\n                        ],\n                    )\n                ],\n                [\n                    AlterTogether(\n                        \"content\",\n                        {\n                            (\n                                \"title\",\n                                \"user_id\",\n                            ): (\n                                \"content_id\",\n                                \"user_id\",\n                            ),\n                        },\n                    )\n                ],\n            )\n            content_instance = models.Content.objects.create(title=\"test\", user=self.user)\n\n            with CaptureQueriesContext(connection) as c:\n                models.Content.objects.get(pk=content_instance.pk)\n            self.assertEqual(\n                len(c.captured_queries), 1\n            )  # We expect one query for retrieving the content\n\n            with transaction.atomic():\n                self.execute_migration(\n                    [\n                        AlterTogether(\n                            \"content\",\n                            {\n                                (\n                                    \"title\",\n                                    \"user_id\",\n                            ): (\n                                \"content_id\",\n                                \"user_id\",\n                            ),\n                        }\n                    ]\n                )  # Remove the unique_together constraint\n            with CaptureQueriesContext(connection) as c:\n                models.Content.objects.filter(\n                    title=\"test\", title__startswith=\"test\"\n                ).exists()  # Query to force indexes to be used\n            self.assertEqual(\n                len(c.captured_queries), 0\n            )  # We expect no queries for retrieving the content\n\n\n\n", "    def test_alter_index_together(self):\n        with transaction.atomic():\n            self.run_migration(\n                \"indexes_together\",\n                state_after_migration=\"indexes_together_altered\",\n            )\n            self.assertEqual(\n                Food.objects.all().values_list(\"name\", \"quantity\"),\n                [\n                    (\"apple\", 5),\n                    (\"banana\", 4),\n                    (\"cherry\", 2),\n                ],\n            )\n\n        with transaction.atomic():\n            self.run_migration(\n                \"indexes_together_backwards\",\n                state_before_migration=\"indexes_together_altered\",\n            )\n            self.assertEqual(\n                Food.objects.all().values_list(\"name\", \"quantity\"),\n                [\n                    (\"apple\", 5),\n                    (\"banana\", 4),\n                    (\"cherry\", 2),\n                ],\n            )\n\n        # Test case creation index together\n\n        with transaction.atomic():\n            self.run_migration(\n                \"indexes_together_case_creation\",\n                state_after_migration=\"indexes_together_case_creation_altered\",\n            )\n            self.assertEqual(\n                UnicodeModel.objects.all().values_list(\"text\", \"number\"),\n                [\n                    (\"apples\", 1),\n                    (\"bananas\", 2),\n                    (\"cherries\", 3),\n                ],\n            )\n\n        with transaction.atomic():\n            self.run_migration(\n                \"indexes_together_case_creation_backwards\",\n                state_before_migration=\"indexes_together_case_creation_altered\",\n            )\n            self.assertEqual(\n                UnicodeModel.objects.all().values_list(\"text\", \"number\"),\n                [\n                    (\"apples\", 1),\n                    (\"bananas\", 2),\n                    (\"cherries\", 3),\n                ],\n            )\n\n\n\n"], "sample_878": ["    def test_remainder_passthrough(self):\n        X = np.array([[1, 2], [3, 4]])\n        ct = make_column_transformer(\n            (StandardScaler(), [\"numerical_column\"]),\n            remainder=\"passthrough\",\n        )\n        ct.fit_transform(X)\n        # Check that the remainder column is passed through unstransformed\n        assert_array_equal(ct.transform(X)[:, 1], X[:, 1])\n    ", "    def test_fit_transform_remainder(self, transformers, remainder, expected, test_shape):\n        X = np.arange(5).reshape(test_shape)\n        ct = make_column_transformer(transformers, remainder=remainder)\n        ct.fit(X)\n        result = ct.transform(X)\n        assert_array_equal(result.reshape(-1), expected)\n\n\n", "    def test_remainder_passthrough(self):\n        X = np.array([[1, 2], [3, 4]])\n        transformer_remainder = Trans()\n        ct = make_column_transformer(\n            (\n                StandardScaler(),\n                [0],\n            ),\n            remainder=\"passthrough\",\n            n_jobs=1,\n        )\n        ct.fit(X)\n        X_transformed = ct.transform(X)\n        assert_allclose(X_transformed[:, 0], StandardScaler().fit_transform(X[:, 0].reshape(-1, 1)))\n        assert_array_equal(X_transformed[:, 1], X[:, 1])\n        ", "    def test_remainder_passthrough(self):\n        X = np.array([[1, 2], [3, 4]])\n        X_numeric = np.array([[1, 2], [3, 4]])\n        X_categorical = np.array([[1, 'a'], [2, 'b']])\n\n        # remainder = 'passthrough'\n        ct = make_column_transformer(\n            (StandardScaler(), [\"numeric\"]), remainder=\"passthrough\"\n        )\n        ct.fit(X)\n        X_transformed = ct.transform(X)\n        assert_allclose_dense_sparse(\n            X_transformed[:, 0],\n            np.array([[0.70710678, 0.70710678], [0.70710678, 0.70710678]]),\n            rtol=1e-5,\n        )\n        assert_array_equal(X_transformed[:, 1], X[:, 1])\n\n        # remainder = 'passthrough' with categorical data\n        ct = make_column_transformer(\n            (OneHotEncoder(), [\"categorical\"]), remainder=\"passthrough\"\n        )\n        ct.fit(X_categorical)\n        X_transformed = ct.transform(X_categorical)\n        assert_allclose_dense_sparse(\n            X_transformed[:, :2],\n            np.array([[1, 0, 1, 0], [0, 1, 0, 1]]),\n            rtol=1e-5,\n        )\n        assert_array_equal(X_transformed[:, 2], X_categorical[:, 1])\n\n        # remainder = 'passthrough' when remainder is a list\n        ct = make_column_transformer(\n            (\n                (StandardScaler(), [\"numeric\"]),\n                (OneHotEncoder(), [\"categorical\"]),\n            ),\n            remainder=\"passthrough\",\n        )\n        ct.fit(np.concatenate((X_numeric, X_categorical), axis=1))\n        X_transformed = ct.transform(np.concatenate((X_numeric, X_categorical), axis=1))\n        assert_allclose_dense_sparse(\n            X_transformed[:, 0:", "    def test_remainder_with_estimator(self):\n        remainder_transformer = StandardScaler()\n        ct = make_column_transformer(\n            (StandardScaler(), [\"num\"]),\n            remainder=remainder_transformer,\n        )\n        X = np.array([[1, 2], [3, 4]])\n        X_transformed = ct.fit_transform(X)\n        assert_allclose(\n            X_transformed[:, 0], StandardScaler().fit_transform(X[:, 0].reshape(-1, 1))\n        )\n        assert_allclose(\n            X_transformed[:, 1], remainder_transformer.transform(X[:, 1].reshape(-1, 1))\n        )\n"], "sample_906": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_520": ["    def test_poly3d_collection_depth_ordering(self):\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection='3d')\n        verts = [\n            [(0, 0, 0), (1, 0, 0), (1, 1, 0), (0, 1, 0)],\n            [(0, 0, 1), (1, 0, 1), (1, 1, 1), (0, 1, 1)]\n        ]\n        poly = Poly3DCollection(verts, facecolors='red', edgecolors='black')\n        ax.add_collection3d(poly)\n        [p.set_zorder(i) for i, p in enumerate(poly.get_paths())]\n        ax.set_xlim(0, 1)\n        ax.set_ylim(0, 1)\n        ax.set_zlim(0, 1.5)\n\n        plt.draw()\n        # Ensure that the polygons are drawn in the right order\n        # based on the z-coordinate. The back polygon should be drawn first\n        # and the front polygon second.\n\n", "    def test_patch_collection_2d_to_3d_simple(self):\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection='3d')\n\n        patches = [\n            Rectangle((0, 0), 1, 1),\n            Rectangle((2, 2), 1, 1),\n        ]\n        col = PatchCollection(patches, facecolor='blue', edgecolor='red')\n        patch_collection_2d_to_3d(col, zs=1, zdir='z')\n        ax.add_collection3d(col)\n        ax.set_xlim(0, 3)\n        ax.set_ylim(0, 3)\n        ax.set_zlim(0, 2)\n        plt.show()\n\n\n\n\n", "    def test_3d_linecollection_projection(self):\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection='3d')\n\n        x = np.arange(-1, 1, 0.1)\n        y = np.arange(-1, 1, 0.1)\n        X, Y = np.meshgrid(x, y)\n        zs = np.sin(np.sqrt(X**2 + Y**2))\n\n        lines = [Line3D(np.column_stack((xs, ys, zs)), linewidth=2)\n                 for xs, ys, zs in zip(X.flatten(), Y.flatten(), zs.flatten())]\n\n        linecol = LineCollection(lines, colors='r')\n        ax.add_collection3d(linecol)\n\n        plt.show()\n", "    def test_polycollection_2d_to_3d(self):\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection='3d')\n        verts = np.random.rand(10, 3)\n        codes = [Path.MOVETO, Path.CURVE3, Path.CURVE3, Path.CURVE3, Path.CURVE3, Path.CLOSEPOLY]\n        poly = PolyCollection(verts, facecolors='red', edgecolors='green')\n        poly_3d = poly_collection_2d_to_3d(poly, zs=0.5, zdir='z')\n        poly_3d.set_alpha(0.5)\n        ax.add_collection3d(poly_3d)\n        ax.set_xlim(0, 1)\n        ax.set_ylim(0, 1)\n        ax.set_zlim(0, 1)\n        plt.show()\n\n\n\n", "    def test_poly3d_collection_projection(self):\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection='3d')\n\n        verts = [[0, 0, 0], [1, 0, 0], [1, 1, 0], [0, 1, 0], [0, 0, 1]]\n        codes = [Path.MOVETO, Path.CURVE3, Path.CURVE3, Path.CURVE3, Path.CLOSE]\n        patch_collection = PolyCollection(verts, facecolors='red', edgecolors='black')\n\n        poly_collection_3d = poly_collection_2d_to_3d(patch_collection, zs=0.5, zdir='z')\n        ax.add_collection3d(poly_collection_3d)\n        \n        ax.set_xlim(0, 1)\n        ax.set_ylim(0, 1)\n        ax.set_zlim(0, 1)\n        plt.draw()\n        plt.pause(0.01)\n\n\n"], "sample_1135": ["    def test_as_content_primitive():\n        from sympy.core.mul import _keep_coeff\n\n        assert (3*sqrt(2)).as_content_primitive() == (3, sqrt(2))\n        assert (3*sqrt(2)).as_content_primitive(clear=False) == (3, sqrt(2))\n        assert (4*sqrt(2)*x).as_content_primitive() == (4*sqrt(2), x)\n        assert (4*sqrt(2)*x).as_content_primitive(clear=False) == (4, sqrt(2)*x)\n        assert ((3 + 2*sqrt(2))*x).as_content_primitive() == (3 + 2*sqrt(2), x)\n        assert ((3 + 2*sqrt(2))*x).as_content_primitive(clear=False) == (1, (3 + 2*sqrt(2))*x)\n        assert ((3 + 2*sqrt(2))*x).as_content_primitive(radical=True) == (3 + 2*sqrt(2), x)\n        assert ((3 + 2*sqrt(2))*x).as_content_primitive(radical=True, clear=False) == (1, (3 + 2*sqrt(2))*x)\n\n        assert (S(1/2)*x).as_content_primitive() == (S(1/2), x)\n        assert (S(1/2)*x).as_content_primitive(clear=False) == (S(1/2), x)\n\n    \n", "    def test_as_content_primitive_sqrt():\n        from sympy.core.expr import Basic\n\n        class MySqrt(Basic):\n            _op_priority = 2\n            is_commutative = True\n\n                return Basic.__new__(cls, a)\n\n                if radical:\n                    return S(1), self\n                return S(1), self\n\n        t = MySqrt(2)\n        result = t.as_content_primitive()\n        assert result == (1, MySqrt(2))\n\n\n", "    def test_prod_with_symbols():\n        from sympy import prod\n        for r in [2, 3, -5]:\n            for symbols in [(x, 2), (y, 3)]:\n                for i in range(10):\n                    val = prod(symbols[0] ** i for _ in range(symbols[1]))\n                    assert val == symbols[0]** (symbols[1]*i*(i+1)//2)\n\n\n", "    def test_as_content_primitive():\n        from sympy.core.numbers import Integer\n        from sympy.core.expr import Expr\n\n        assert (-3*sqrt(2)*(2 - 2*sqrt(2))).as_content_primitive() == (6, -sqrt(2)*(1 - sqrt(2)))\n        assert (2*x*y*sin(x)*cos(x)).as_content_primitive() == (1, 2*x*y*sin(x)*cos(x))\n        assert (2*x*y*sin(x)*cos(x)).as_content_primitive(radical=True) == (1, 2*x*y*sin(x)*cos(x))\n        assert (2*x*sin(x)*cos(x)).as_content_primitive() == (1, 2*x*sin(x)*cos(x))\n        assert (2*x*sin(x)*cos(x)).as_content_primitive(radical=True) == (1, 2*x*sin(x)*cos(x))\n\n        assert (0).as_content_primitive() == (S.One, 0)\n        assert (S(0)).as_content_primitive() == (S.One, 0)\n        assert (sympify('(abc)')).as_content_primitive() == (1, abc)\n        assert (sympify('(abc)')).as_content_primitive(clear=False) == (1, abc)\n\n        assert (3*x + 2*y).as_content_primitive(clear=False) == (1, 3*x + 2*y)\n", "compilation error"], "sample_768": ["    def test_train_test_split_with_array_like_test_size(self, test_size):\n        X, y = make_classification(n_samples=100, n_features=20)\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=test_size, random_state=42)\n        assert X_train.shape[0] + X_test.shape[0] == X.shape[0]\n        assert y_train.shape[0] + y_test.shape[0] == y.shape[0]\n", "    def test_train_test_split_sparse(self):\n        X_sparse = coo_matrix([[1, 0], [0, 1], [1, 1], [0, 0]])\n        X_dense = X_sparse.toarray()\n\n        train_indices, test_indices = train_test_split(X_sparse, test_size=0.25)\n        X_sparse_train, X_sparse_test = X_sparse[train_indices], X_sparse[test_indices]\n        X_dense_train, X_dense_test = X_dense[train_indices], X_dense[test_indices]\n\n        assert_array_equal(X_sparse_train.toarray(), X_dense_train)\n        assert_array_equal(X_sparse_test.toarray(), X_dense_test)\n\n        # Test with different sparse formats\n        X_sparse_csc = csc_matrix(X_sparse.array)\n        X_sparse_csr = csr_matrix(X_sparse.array)\n        train_indices, test_indices = train_test_split(X_sparse_csc, test_size=0.25)\n        X_sparse_csc_train, X_sparse_csc_test = X_sparse_csc[train_indices], X_sparse_csc[test_indices]\n        assert_array_equal(X_sparse_csc_train.toarray(), X_sparse_train.toarray())\n        assert_array_equal(X_sparse_csc_test.toarray(), X_sparse_test.toarray())\n        train_indices, test_indices = train_test_split(X_sparse_csr, test_size=0.25)\n        X_sparse_csr_train, X_sparse_csr_test = X_sparse_csr[train_indices], X_sparse_csr[test_indices]\n        assert_array_equal(X_sparse_csr_train.toarray(), X_sparse_train.toarray())\n        assert_array_equal(X_sparse_csr_test.toarray(), X_sparse_test.toarray())\n\n", "    def test_train_test_split_sparse_output():\n        X_sparse = coo_matrix((np.ones(10), (range(10), range(10))),\n                              shape=(10, 10))\n\n        X_train, X_test, y_train, y_test = train_test_split(\n            X_sparse,\n            y,\n            test_size=0.3,\n            random_state=42\n        )\n        assert isinstance(X_train, (csr_matrix, csc_matrix))\n        assert isinstance(X_test, (csr_matrix, csc_matrix))\n\n", "    def test_KFold_invalid_n_splits(self):\n        with pytest.raises(ValueError):\n            KFold(n_splits=-1)\n        with pytest.raises(ValueError):\n            KFold(n_splits=0)\n", "    def test_train_test_split_sparse(self):\n        X_sparse = csr_matrix(np.random.randn(10, 5))\n        y = np.random.randint(0, 2, size=10)\n        X_train, X_test, y_train, y_test = train_test_split(\n            X_sparse, y, test_size=0.25, random_state=42)\n        assert isinstance(X_train, type(X_sparse))\n        assert isinstance(X_test, type(X_sparse))\n\n"], "sample_930": ["    def test_unicode_normalization(self, env, builder):\n        index_entries = IndexEntries(env)\n        entry_data = {'test\\u0301': [['test (in module foo)', 'foo.html#test'], ['test (in module bar)', 'bar.html#test']]}\n        # Mimic the entry addition process\n        for word, entries in entry_data.items():\n            for entry, link in entries:\n                index_entries._add_entry(word, entry, link)\n        index_entries.create_index(builder, group_entries=False)\n        actual_index = [(key, value) for key, value in index_entries.create_index(builder, group_entries=False)]\n        expected_index = [(u'test\\u0301', [('test (in module foo)', 'foo.html#test'), ('test (in module bar)', 'bar.html#test')])]\n\n        assert actual_index == expected_index\n", "    def test_index_entries_with_see_also(env, builder):\n        env.domains['index'].entries[\n            'test.rst'\n        ] = [\n            ('seealso', 'some other topic', 'some-other-topic', None, None),\n        ]\n        index_entries = IndexEntries(env)\n        index_list = index_entries.create_index(builder)\n        expected_index = [\n            (\n                'some other topic',\n                [\n                    ('see also some other topic', None)\n                ]\n            )\n        ]\n        assert index_list == expected_index\n", "    def test_create_index_with_see(app, status, warning):\n        env = app.builder.env\n        env.domains['index'].entries = {\n            'test.rst': [\n                ('see', 'some_obj', '1', 'Some Object', None),\n            ],\n        }\n        index_entries = IndexEntries(env)\n        index = index_entries.create_index(app.builder, group_entries=True)\n        assert len(index) == 1\n        assert index[0][0] == 'See'\n        assert len(index[0][1]) == 1\n        assert index[0][1][0][0] == 'Some Object'\n\n\n\n", "    def test_indexentry_sorting(app, builder):\n        env = app.builder.env\n        index_entries = IndexEntries(env)\n        index_entries.create_index(builder, group_entries=False)\n        new = env.domains['index'].entries\n        assert new[r'test_file.rst'] == [\n            ('single', 'foo', 'id1', 'Foo Bar', 'single'),\n            ('single', 'bar', 'id2', 'Bar Baz', 'single'),\n            ('pair', 'foo', 'id3', 'Foo Bar', 'pair'),\n            ('triple', 'one', 'id4', 'One Two Three', 'triple'),\n            ('see', 'see_target', 'id5', 'See Target', 'see'),\n            ('seealso', 'seealso_target', 'id6', 'See Also Target', 'seealso'),\n        ]\n\n", "    def test_indexentries_duplicate_entries(app):\n        env = app.builder.env\n        indexentries = IndexEntries(env)\n\n        # Simulate entries with duplicated keywords\n        domain = env.get_domain('index')\n        domain.entries['test.rst'] = [\n            ('single', 'test', '1', 'Test', 'duplicate1'),\n            ('single', 'test', '2', 'Test', 'duplicate2'),\n        ]\n\n        index = indexentries.create_index(app.builder)\n\n        # Assert that the entries are grouped together\n        expected_index = [\n            ('test', [\n                [('Test', 'test.rst#1'), ('Test', 'test.rst#2')],\n            ])\n        ]\n        assert index == expected_index\n"], "sample_822": ["    def test_pairwise_distances_chunked_with_reduce_func_row_by_row(self):\n        X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n        reduce_func = lambda D_chunk, start: np.array([\n            np.where(D_chunk < 0.5)[0] for D_chunk in D_chunk\n        ])\n\n        gen = pairwise_distances_chunked(X, reduce_func=reduce_func,\n                                         working_memory=0)\n        chunks = list(gen)\n        expected_indices = [[0, 1], [2, 3], [0, 1, 2, 3], [0, 1, 2, 3]]\n        assert_array_equal(chunks[0], expected_indices[0])\n        assert_array_equal(chunks[1], expected_indices[1])\n        assert_array_equal(chunks[2], expected_indices[2])\n        assert_array_equal(chunks[3], expected_indices[3])\n", "    def test_pairwise_kernels_precomputed(self):\n        X = np.array([[1, 2], [3, 4]])\n        K = np.array([[5, 6], [7, 8]])\n        result = pairwise_kernels(X, metric=\"precomputed\", **{\"precomputed\": K})\n        assert_array_equal(result, K)\n", "  def test_pairwise_distances_precomputed_with_Y(metric):\n      X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n      Y = np.array([[9, 10], [11, 12], [13, 14], [15, 16]])\n\n      if metric == 'precomputed':\n          D_true = np.array([[0, 1.41421356, 2.82842712, 4.24264069],\n                             [1.41421356, 0, 1.41421356, 2.82842712],\n                             [2.82842712, 1.41421356, 0, 1.41421356],\n                             [4.24264069, 2.82842712, 1.41421356, 0]])\n      else:\n          D_true = cosine_distances(X, Y)\n\n      with config_context(working_memory=100):\n          D_out = pairwise_distances(X, Y, metric=metric)\n      assert_array_almost_equal(D_out, D_true)\n\n\n\n", "    def test_paired_distances_chunked():\n        # Test for chunking functionality\n        X = np.random.rand(5, 3)\n        n_chunks = 2\n\n        with pytest.raises(ValueError, match=\".*'reduce_func'.*\"):\n            paired_distances_chunked(X, reduce_func=lambda x: x)\n\n            return np.mean(D_chunk)\n\n        for _ in range(20):\n            gen = pairwise_distances_chunked(X,\n                                            reduce_func=reduce_func,\n                                            n_jobs=1,\n                                            working_memory=1)\n\n            for i in range(n_chunks):\n                chunk = next(gen)\n                assert isinstance(chunk, (np.ndarray, csr_matrix))\n                assert chunk.shape[0] == X.shape[0] // n_chunks\n                assert np.allclose(chunk, np.mean(chunk))\n\n            # Ensure no further chunks are generated\n            with pytest.raises(StopIteration):\n                next(gen)\n", "  def test_pairwise_kernels_invalid_metric():\n      with pytest.raises(ValueError, match=\"Unknown kernel\"):\n          pairwise_kernels(np.ones((5, 5)), metric=\"foobar\")\n"], "sample_667": ["    def test_mktemp_numbered(tmp_path, numbered):\n        p = tmp_path\n        assert p.mkdir(parents=True, exist_ok=True)\n\n        if numbered:\n            assert os.path.isdir(p)\n            # Check for numbered directory\n            pattern = r\"{}-\\d+\".format(p.name)\n            assert re.search(pattern, str(p))\n        else:\n            assert os.path.isdir(p)\n            assert p.name == \"foo\"  \n\n\n", "    def test_getbasetemp_from_config(tmp_path_factory: pathlib.TempPathFactory):\n        config = pytest.MonkeyPatch()\n        config.option.basetemp = pathlib.Path(\"/test/basetemp\")\n        tmppath_factory.from_config(config)\n        basetemp = tmp_path_factory.getbasetemp()\n        assert basetemp == pathlib.Path(\"/test/basetemp\")\n", "    def test_getbasetemp_from_env(tmpdir_factory):\n        os.environ[\"PYTEST_DEBUG_TEMPROOT\"] = \"/tmp/pytest-test\"\n        basetemp = tmpdir_factory.getbasetemp()\n        assert basetemp.as_posix().startswith(\"/tmp/pytest-test/pytest-\")\n\n\n\n", "    def test_getbasetemp_from_env(tmpdir_factory):\n        os.environ[\"PYTEST_DEBUG_TEMPROOT\"] = \"/tmp\"\n        try:\n            root = tmpdir_factory.getbasetemp()\n            assert root.as_posix() == pathlib.normalize(\"/tmp/pytest-unknown\")\n        finally:\n            del os.environ[\"PYTEST_DEBUG_TEMPROOT\"]\n\n\n\n", "def test_get_user():\n    with pytest.raises(ImportError) as excinfo:\n        pathlib._get_user()\n    assert \"getpass\" in str(excinfo.value)\n"], "sample_364": ["    def test_include_dynamic_module(self):\n        with self.assertRaises(ImproperlyConfigured):\n            path('', partial(include, ('urlpatterns.dynamic_urls',)), namespace='dynamic')\n", "    def test_include_dynamic_module(self):\n        from . import dynamic_urlpatterns  # Assuming you have a dynamic_urlpatterns.py file\n\n        with self.subTest(\"Dynamic module without namespace\"):\n            with self.assertRaises(ImproperlyConfigured) as cm:\n                include(dynamic_urlpatterns)\n\n            self.assertIn(\n                \"Cannot override the namespace for a dynamic module that provides a namespace.\",\n                str(cm.exception),\n            )\n\n        with self.subTest(\"Dynamic module with namespace\"):\n            resolved_urlconf = include(dynamic_urlpatterns, namespace=\"dynamic_namespace\")\n            self.assertEqual(resolved_urlconf[2], \"dynamic_namespace\")\n            pattern = pattern = resolve(\"/dynamic_module_url/\").resolver_match.pattern\n            self.assertEqual(pattern.urlconf_module, dynamic_urlpatterns)\n\n", "    def test_include_dynamic_module_namespace(self):\n        with self.subTest('dynamic module with namespace'):\n                return [\n                    path('', empty_view, name=name),\n                ]\n            from django.urls import include\n            urlpatterns = [\n                include(urlconf, namespace='dynamic_module'),\n            ]\n            with self.subTest('resolves included patterns'):\n                resolved = resolve('/my_app/')\n                self.assertEqual(resolved.url_name, 'dynamic_module:my_app')\n                self.assertEqual(resolved.view_dict, {'view': empty_view})\n", "    def test_include_dynamic_module(self):\n        from .urlpatterns import dynamic_module  # Import the dynamic module\n\n        with self.assertRaises(ImproperlyConfigured):\n            path('', include(dynamic_module))  # Attempt to include a module without app_name\n\n        with self.assertRaises(ImproperlyConfigured):\n            path('', include(dynamic_module, namespace='test'))  # Attempt to include a module without app_name with namespace\n\n        with self.assertRaises(ImproperlyConfigured):\n            path('', include((dynamic_module, None)))  # Attempt to include a module with only a module name and no namespace\n\n\n", "    def test_dynamic_converter(self):\n        for url, (url_name, app_name, kwargs) in converter_test_data:\n            with self.subTest(url=url, url_name=url_name, app_name=app_name, kwargs=kwargs):\n                resolver = resolve(url)\n                self.assertEqual(resolver.url_name, url_name)\n                self.assertEqual(resolver.app_name, app_name)\n                self.assertEqual(resolver.args, kwargs)\n"], "sample_732": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1162": ["compilation error", "compilation error", "    def test_nfloat_matrix_mul(self):\n        from sympy.matrices import Matrix\n\n        M1 = Matrix([[1, 2], [3, 4]])\n        M2 = Matrix([[5, 6], [7, 8]])\n\n        result = nfloat(M1*M2)\n        self.assertEqual(result, Matrix([[19, 22], [43, 50]]))\n\n        result = nfloat(M1*M2, exponent=True)\n        self.assertEqual(result, Matrix([[19.0, 22.0], [43.0, 50.0]]))\n\n", "compilation error", "    def test_nfloat_symbols_in_matrix():\n        import sympy\n        x = sympy.Symbol('x')\n        y = sympy.Symbol('y')\n        m = Matrix([[x, y], [y, x]])\n        n = nfloat(m)\n        assert isinstance(n, sympy.Matrix)\n        assert n.shape == m.shape\n        assert n[0, 0].is_Float\n        assert n[0, 1].is_Float\n        assert n[1, 0].is_Float\n        assert n[1, 1].is_Float\n"], "sample_39": ["    def test_maps(self):\n        for filename in self._file_list:\n            with catch_warnings():\n                warnings.simplefilter(\"ignore\", DeprecationWarning)\n                with NumpyRNGContext() as rng:\n                    hdu = fits.open(get_pkg_data_filename(\"maps\", filename))\n                    header = hdu[0].header\n\n                    wcs = wcs.WCS(header)\n\n                    # Check basic properties of the WCS\n                    assert wcs.naxis == header['NAXIS']\n                    assert wcs.wcs.ctype == header['CTYPE1']\n                    assert wcs.wcs.crval == header['CRVAL1']\n\n                    # Check if the WCS can be used to transform coordinates\n                    x_world = np.array([0.0, 1.0, 2.0])  \n                    y_world = np.array([0.0, 1.0, 2.0]) \n\n                    x_pix, y_pix = wcs.all_pix2world(x_world, y_world, 0)\n\n                    assert_allclose(x_pix, header['CRPIX1'] + x_world * header['CDELT1'])\n                    assert_allclose(y_pix, header['CRPIX2'] + y_world * header['CDELT2'])\n\n            hdu.close()\n\n\n\n", "    def test_wcs_from_header(self, filename):\n        hdu_file = get_pkg_data_contents(filename)\n        hdu = fits.open(io.BytesIO(hdu_file))\n        wcs = wcs.WCS(hdu[0].header)\n        assert wcs.naxis == hdu[0].header['NAXIS']\n        assert wcs.wcs.ctype == hdu[0].header['CTYPE'].strip()\n        assert wcs.wcs.crval == hdu[0].header['CRVAL'].strip()\n        assert wcs.wcs.crpix == hdu[0].header['CRPIX'].strip()\n        assert wcs.wcs.cdelt == hdu[0].header['CDELT'].strip()\n        hdu.close()\n", "    def test_wcs_from_file(self, filename):\n        with open(os.path.join(get_pkg_data_filename('maps', filename), filename), 'r') as f:\n            hdr = fits.Header.fromstring(f.read())\n\n        wcs = wcs.WCS(hdr)\n        # Testing the basic properties\n        assert wcs.naxis == 2\n        assert wcs.ctype[0] == 'RA---TAN'\n        assert wcs.ctype[1] == 'DEC--TAN'\n\n        # Testing the pixel coordinates\n        x_pix = np.array([10., 20.])\n        y_pix = np.array([100., 150.])\n        ra_dec = wcs.wcs_pix2world(x_pix, y_pix, 0)\n        assert ra_dec[0].units == 'deg'\n        assert ra_dec[1].units == 'deg'\n\n        # Testing the world coordinates\n        ra_deg = np.array([100.0, 110.0]) * np.pi / 180.0\n        dec_deg = np.array([20.0, 30.0]) * np.pi / 180.0\n        x_y_pix = wcs.wcs_world2pix(ra_deg, dec_deg, 0)\n        assert x_y_pix[0].units == 'pixel'\n        assert x_y_pix[1].units == 'pixel'\n\n        # Testing the distortion parameters\n        assert wcs.has_distortion()\n\n", "    def test_maps(self):\n        for filename in self._file_list:\n            with catch_warnings(record=True) as warning_lines:\n                warnings.simplefilter(\"always\", AstropyUserWarning)\n                hdulist = fits.open(get_pkg_data_filename(\"maps\", filename))\n                wcs = wcs.WCS(hdulist[0].header)\n                assert not warnings.iswarning(warning_lines)\n\n                # check some basic properties of the WCS\n                assert wcs.naxis == 2\n                assert wcs.radesys == 'FK5'\n                assert wcs.cdelt[0] > 0\n                assert wcs.cdelt[1] > 0\n                \n                # check pixel to world transformation\n                crpix = wcs.crpix\n                cdelt = wcs.cdelt\n                for i in range(2):\n                    for j in range(2):\n                        x = crpix[0] + i * cdelt[0]\n                        y = crpix[1] + j * cdelt[1]\n                        world = wcs.wcs_pix2world(np.array([x, y]), 0)\n                        assert len(world) == 2\n\n                # check world to pixel transformation\n                world = np.array([0.0, 0.0])\n                pix = wcs.wcs_world2pix(world, 0)\n                assert np.allclose(pix, [crpix[0], crpix[1]])\n\n\n\n", "    def test_maps_wcs(self, filename):\n        hdr_data = get_pkg_data_contents(\"maps\", filename)\n        header = fits.Header.fromstring(hdr_data)\n        wcs = wcs.WCS(header)\n\n        # Check the basic WCS properties\n        assert wcs.naxis == 2\n\n        # Test some basic WCS functionality\n        world = wcs.wcs_pix2world(0.5, 0.5, 0)\n        assert isinstance(world, tuple)\n        assert len(world) == 2\n\n        # Test crpix\n        assert wcs.crpix[0] == header['CRPIX1']\n        assert wcs.crpix[1] == header['CRPIX2']\n\n        # Test cdelt\n        assert_allclose(wcs.cdelt[0], header['CDELT1'])\n        assert_allclose(wcs.cdelt[1], header['CDELT2'])\n\n        # Test cd\n        assert_allclose(wcs.cd[0, 0], header['CD1_1'])\n        assert_allclose(wcs.cd[0, 1], header['CD1_2'])\n        assert_allclose(wcs.cd[1, 0], header['CD2_1'])\n        assert_allclose(wcs.cd[1, 1], header['CD2_2'])\n\n\n\n        # Check celestial unit conversions\n        with (\n            catch_warnings(record=True) as w\n        ) as _context:\n            warnings.simplefilter('always')\n            wcs.wcs_world2pix(\n                world[0], world[1])\n            assert len(w) > 0\n            for warning in w:\n                assert \"Unsupported unit\" in str(warning.message)\n\n\n"], "sample_795": ["    def fit(self, X, y):\n        return self\n", "    def __init__(self):\n        self.not_initialized = True\n", "    def test_non_estimator_input(self):\n        from sklearn.utils.estimator_checks import check_is_estimator\n        for non_estimator in [1, [1, 2], \"string\", np.array([1, 2]),\n                              (1, 2), lambda x: x]:\n            self.assertFalse(check_is_estimator(non_estimator))\n", "    def __init__(self, param):\n        self.param = param\n", "    def __init__(self):\n        self.input_data = None\n"], "sample_1070": ["    def test_ev_log_special_values(self):\n        from sympy import log, oo\n        self.assertEqual(log(1), 0)\n        self.assertEqual(log(0), -oo)\n        self.assertEqual(log(oo), oo)\n        self.assertEqual(log(oo, 2), oo)\n        self.assertEqual(log(-1), I * pi)\n", "    def test_log_rewrite_as_sin_cos(self):\n        from sympy.functions.elementary.trigonometric import sin, cos\n        from sympy.functions import log\n        self.assertEqual(log(sin(x)).rewrite_as_sin(x), log(sin(x)) )\n        self.assertEqual(log(cos(x)).rewrite_as_cos(x), log(cos(x)))\n\n        # Check for potential errors when there's no simple relationship\n        with raises(TypeError):\n            log(sin(x)**2).rewrite_as_sin(x)\n        with raises(TypeError):\n            log(x*sin(x)).rewrite_as_sin(x)\n\n", "    def test_log_expand_log_negative(self):\n        from sympy.concrete import Sum, Product\n        x = Symbol('x')\n        expr = log(x**2)\n        self.assertEqual(expr.expand_log(), 2*log(x))\n\n        expr = log(-x**2)\n        self.assertEqual(expr.expand_log(), 2*log(-x) + 2*pi*I)\n\n", "    def test_log_polar():\n        from sympy import I\n\n        assert log(exp_polar(1, pi/4)).as_real_imag() == (0, pi/4)\n\n        a = 2*I\n        result = log(a)\n        assert result.as_real_imag() == (log(2), pi/2)\n\n        result = log(a)**2\n        assert result.as_real_imag() == (2*log(2)**2, 2*pi*log(2))\n\n        result = log(a).evalf()\n        assert abs(result - (log(2) + I*pi/2)) < 1e-10\n\n        result = log(-a)\n        assert result.as_real_imag() == (log(2), 3*pi/2)\n\n\n\n", "    def test_log_rewrite_as_sin_cos(self):\n        from sympy.functions import cos, sin\n        expr = log(exp(I*pi/4))\n        expected = I * pi / 4\n        self.assertEqual(expr.rewrite_as_sin(None).simplify(), expected)\n        expr = log(exp(I*pi/2))\n        expected = I * pi / 2\n        self.assertEqual(expr.rewrite_as_sin(None).simplify(), expected)\n        expr = log(exp(I*3*pi/4))\n        expected = I * (3 * pi) / 4\n        self.assertEqual(expr.rewrite_as_sin(None).simplify(), expected)\n\n        expr = log(exp(I*pi))\n        expected = I * pi\n        self.assertEqual(expr.rewrite_as_sin(None).simplify(), expected)\n"], "sample_346": ["    def test_decorator_inheritance(self):\n        class MyView(object):\n                self.called = False\n\n                self.called = True\n                return HttpResponse(\"GET\")\n\n                self.called = True\n                return HttpResponse(\"POST\")\n\n        @method_decorator(login_required)\n        class LoginRequiredView(MyView):\n            pass\n\n        view = LoginRequiredView()\n        request = HttpRequest()\n        response = view.get(request)\n        self.assertTrue(view.called)\n        self.assertEqual(response.status_code, 302)  # Redirected to login page\n", "    def test_decorator_from_middleware(self):\n        @decorator_from_middleware(XFrameOptionsMiddleware)\n            return HttpResponse(\"OK\")\n\n        response = view(HttpRequest())\n        self.assertEqual(response['X-Frame-Options'], 'SAMEORIGIN')\n", "    def test_decorator_ordering(self):\n        @full_decorator\n            return HttpResponse('Decorated View')\n\n        response = decorated_view(request)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.content.decode('utf-8'), 'Decorated View')\n\n\n", "    def test_decorator_composition_order(self):\n        @method_decorator(login_required)\n        @method_decorator(staff_member_required)\n        @method_decorator(user_passes_test(lambda u: True))\n            return HttpResponse(\"Logged in, staff, and passes test\")\n\n        response = login_required_staff_user_passes_test(HttpRequest())\n        self.assertEquals(response.status_code, 200)\n\n        # Test that decorators are applied in reverse order\n        @method_decorator(user_passes_test(lambda u: False))\n        @method_decorator(staff_member_required)\n        @method_decorator(login_required)\n            return HttpResponse(\"User not passing test\")\n\n        response = test_decorator_order(HttpRequest())\n        self.assertEquals(response.status_code, 302)\n\n", "    def test_full_decorator(self):\n        response = fully_decorated(HttpRequest())\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.content.decode('utf-8'), '<html><body>dummy</body></html>')\n\n\n"], "sample_972": ["    def test_restify_typing_generic(self):\n        from typing import Generic, List, Optional\n\n        class MyGeneric(Generic[T]):\n            pass\n\n        assert restify(MyGeneric) == ':py:class:`~typing.Generic`'\n        assert restify(List[T]) == ':py:class:`~typing.List`'\n        assert restify(Optional[T]) == ':py:class:`~typing.Optional`'\n", "    def test_restify_newtype_with_module(self):\n        from sphinx.util import inspect  # lazy loading\n        inspect._is_newtype = lambda x: True\n        with mock.patch('sphinx.util.inspect.isgenericalias', lambda x: True):\n            newtype_class = type('NewTypeExample', (object,), {'__module__': 'my_module'})\n            self.assertEqual(restify(newtype_class), ':py:class:`my_module.NewTypeExample`')\n", "    def test_restify_newtypes(self):\n        from sphinx.util import inspect  # lazy loading\n\n        inspect.isNewType = lambda obj: True\n        assert restify(MyInt) == ':py:class:`~typing.NewType`[int]'\n        assert restify(MyList) == ':py:class:`~typing.List`[T]'\n        inspect.isNewType = lambda obj: False\n", "    def test_restify_special_form(self):\n        from typing import TypeAlias\n        AliasType = TypeAlias('AliasType', int)\n        assert restify(AliasType) == ':py:obj:`~typing.Alias`'\n", "    def test_restify_typing_special_forms(self):\n        from sphinx.util import inspect  # lazy loading\n\n        # test typing.Callable\n\n        assert restify(typing.Callable[[int], str]) == ':py:class:`~typing.Callable`[[int], str]'\n        assert restify(typing.Callable[[int], Dict[str, Union[int, str]]]) == (\n            ':py:class:`~typing.Callable`[[int], dict\\[str, Union[int, str]]]\n        '\n\n        # test typing.GenericMeta\n\n        class MyGeneric(typing.GenericMeta):\n            __origin__ = list\n            __args__ = [T]\n\n        assert restify(MyGeneric) == ':py:class:`~typing.List`[T]'\n\n        # test typing.Union and typing.Optional\n\n        assert restify(typing.Union[int, str]) == ':py:obj:`~typing.Union`[int, str]'\n        assert restify(typing.Optional[int]) == ':py:obj:`~typing.Optional`[int]'\n       \n        # test typing.Annotated\n\n        assert restify(typing.Annotated[int, 'my_annotation']) == (\n            ':py:obj:`~typing.Annotated`[int, my_annotation]'\n        )\n\n        # test ForwardRef\n\n        class ForwardRefWrapper:\n                self.arg = arg\n                self.is_argument = is_argument\n\n                return self.arg\n\n        assert restify(ForwardRefWrapper(T)) == ':py:class:`T`'\n        \n        # test typing.Literal\n\n        assert restify(typing.Literal[int]) == ':py:class:`~typing.Literal`[int]'\n\n        # test typing.NamedTuple\n\n        MyTuple = typing.NamedTuple('MyTuple', [('a', int), ('b', str)])\n        assert restify(MyTuple) == ':py:class:`~typing.NamedTuple`[`MyTuple`]'\n\n"], "sample_941": ["    def test_restify_union(self):\n        assert restify(Union[int, str]) == ':obj:`Union`\\\\ [int, str]'\n        assert restify(Union[str, int, float]) == ':obj:`Union`\\\\ [str, int, float]'\n        assert restify(Union[List[int], Dict[str, int]]) == ':obj:`Union`\\\\ [List[int], Dict[str, int]]'\n        assert restify(Union[int, NoneType]) == ':obj:`Optional`\\\\ [int]'\n        assert restify(Union[int, ...]) == ':obj:`Union`\\\\ [int, ...]\n\n\n        \n", "    def test_restify_callable(self):\n            pass\n        assert restify(my_func) == ':class:`my_func[[int, str], Optional[float]=None, List[T]]'`\n        assert restify(typing.Callable[[int], str]) == ':class:`Callable[[int], str]`'\n        assert restify(typing.Callable[[int, str], T]) == ':class:`Callable[[int, str], %s]' % T.__name__\n\n\n\n", "    def test_restify_with_specialform(self):\n        self.assertEqual(restify(MyList), 'list[T]')\n        self.assertEqual(restify(typing.List), ':obj:`list`')\n        self.assertEqual(restify(typing.Callable), ':obj:`Callable`)\n        self.assertEqual(restify(typing.Union), ':obj:`Union`)\n        self.assertEqual(restify(typing.Tuple), ':obj:`tuple`)\n        self.assertEqual(restify(typing.Dict), ':obj:`dict`)\n        self.assertEqual(restify(typing.Set), ':obj:`set`)\n", "    def test_restify_generic_with_system_typevars(self):\n        from typing import Generic, List\n\n        class MyGeneric(Generic[T]):\n                self.data = data\n\n        assert restify(MyGeneric) == ':class:`MyGeneric`'\n        assert restify(MyGeneric[str]) == ':class:`MyGeneric[str]`'\n        assert restify(MyGeneric[int]) == ':class:`MyGeneric[int]`'\n        assert restify(MyGeneric[int, str]) == ':class:`MyGeneric[int, str]`'\n        assert restify(MyGeneric[MyInt]) == ':class:`MyGeneric[MyInt]`'\n\n\n", "    def test_restify_typing_union(self):\n        assert restify(Union[int, str, NoneType]) == ':obj:`Union`\\\\ [%s, %s, None]' % ('int', 'str')\n        assert restify(Union[int, str]) == ':obj:`Union`\\\\ [%s, %s]' % ('int', 'str')\n        assert restify(Union[list[str], dict[str, int]]) == ':obj:`Union`\\\\ [%s, %s]' % ('list`str]]{', 'dict[str, int]')\n        assert restify(Union[MyList[int], MyList[str]]) == ':obj:`Union`\\\\ [%s, %s]' % (\n            'MyList[int]', 'MyList[str]')\n\n\n"], "sample_680": ["    def test_evaluate_skip_marks_skip_with_reason(self):\n        item = _pytest._code.Code(\n            docstring=\"\",\n            name=\"test_function\",\n            source=\"\",\n            lineno=1,\n            filename=\"test_file.py\",\n        )\n        mark = pytest.mark.skip(reason=\"This test is skipped due to a reason\")\n        skipped = evaluate_skip_marks(item)\n        assert skipped is not None\n        assert skipped.reason == \"This test is skipped due to a reason\"\n", "    def test_evaluate_skip_marks_no_marks(self):\n        item = _pytest._code.Item(\n            name=\"test_function\",\n            parent=None,\n        )\n        assert evaluate_skip_marks(item) is None\n", "    def test_xfail_with_raises_keyword(self):\n        @pytest.mark.xfail(raises=ValueError, reason=\"Expected ValueError\")\n            raise TypeError()\n\n        with runtestprotocol() as proto:\n            # ...\n            with pytest.raises(ValueError) as excinfo:\n                proto.nextcall(my_func)\n\n            assert excinfo.type is ValueError\n            assert excinfo.value.args[0] == \"Expected ValueError\"\n", "    def test_evaluate_skip_marks_skip(self):\n        item = _pytest.nodes.Item.from_function(\n            obj=lambda: None,\n            name=\"test_function\",\n            parent=None,\n            callspec=None,\n            fspath=None,\n            config=None,\n        )\n        mark = pytest.mark.skip(reason=\"This test is skipped\")\n        item.add_marker(mark)\n        result = evaluate_skip_marks(item)\n        assert result is not None\n        assert result.reason == \"This test is skipped\"\n", "    def test_runxfail_behavior(self):\n        old_xfail = pytest.xfail\n        try:\n            pytest.xfail = lambda *args, **kwargs: None\n\n            class TestClass:\n                    raise ValueError(\"Expected failure\")\n\n            with pytest.raises(ValueError):\n                runtestprotocol([\n                    {\"file\": __file__, \"line\": 1, \"function\": \"TestClass.test_method\"}\n                ],\n                [], [], [])\n        finally:\n            pytest.xfail = old_xfail\n\n\n"], "sample_329": ["    def test_serialize_complex_deconstructible_types(self):\n        class ComplexDeconstructible(DeconstructibleInstances):\n                self.value = value\n\n                return (\n                    'ComplexDeconstructible',\n                    [self.value],\n                    {'extra_arg': 'extra_value'},\n                )\n\n        serializer = serializer_factory(ComplexDeconstructible('complex'))\n        output, imports = serializer.serialize()\n        expected = \"ComplexDeconstructible(complex, extra_arg='extra_value')\"\n        self.assertEqual(output, expected)\n        self.assertEqual(set(imports), {'from custom_migration_operations.more_operations import ComplexDeconstructible'})\n\n\n\n", "    def test_serializer_custom_operations(self):\n        with mock.patch('django.db.migrations.operations.Operation') as mock_operation:\n            mock_operation.return_value = mock.MagicMock(spec=models.Operation)\n            serializer = custom_migration_operations.more_operations.Serializer(mock_operation.return_value)\n            serialized_operation = serializer.serialize()\n\n            self.assertEqual(serialized_operation, 'custom_migration_operations.more_operations.MyCustomOperation()')\n            \n", "    def test_serialize_custom_operation(self):\n        class CustomOperation(custom_migration_operations.more_operations.MoreOperation):\n                self.name = name\n\n                return ('CustomOperation', [self.name], {})\n\n        operation = CustomOperation(\"test_name\")\n        serialized_operation = OperationWriter(operation, indentation=0).serialize()\n        self.assertEqual(serialized_operation, \"CustomOperation(test_name)\\n\")\n\n\n\n\n\n", "    def test_OperationWriter_writes_custom_operations(self):\n        operation = custom_migration_operations.more_operations.CreateIndexOperation(\n            model='TestModel1',\n            indexes=[('thing', models.Index(fields=['thing'], using='my_index'))],\n        )\n        writer = OperationWriter(operation, indentation=0)\n        output = writer.serialize()\n        self.assertIn('CreateModel(name=\"TestModel1\",)', output)\n        self.assertIn('CreateIndex(name=\"my_index\",)', output)\n\n\n\n", "    def test_serialize_deconstructable(self):\n        serializer = DeconstructableSerializer(DeconstructibleInstances())\n        serialized_string, imports = serializer.serialize()\n        self.assertEqual(serialized_string, \"DeconstructibleInstances()\")\n        self.assertEqual(imports, set())\n"], "sample_442": ["    def test_unsign_object_compressed(self):\n        signer = signing.get_cookie_signer()\n        obj = {'hello': 'world'}\n        compressed_signed = signer.sign_object(obj, compress=True)\n        uncompressed_obj = signer.unsign_object(compressed_signed, compress=True)\n        self.assertEqual(uncompressed_obj, obj)\n", "    def test_timestamp_signature_expiry(self):\n        signer = signing.get_cookie_signer()\n        data = {\"foo\": \"bar\"}\n        signed_data = signer.sign_object(data, compress=True)\n        with freeze_time(datetime.datetime.utcnow() + datetime.timedelta(seconds=61)):\n            with ignore_warnings(category=RemovedInDjango51Warning):\n                with self.assertRaises(signing.SignatureExpired):\n                    signer.unsign_object(signed_data, max_age=60)\n\n\n", "    def test_custom_salt(self):\n        signer = signing.get_cookie_signer(salt=\"my_custom_salt\")\n        data = {\"foo\": \"bar\"}\n        signed_data = signer.sign_object(data)\n\n        # Using a different salt should raise a BadSignature error\n        other_signer = signing.get_cookie_signer(salt=\"another_salt\")\n        with self.assertRaises(signing.BadSignature):\n            other_signer.unsign_object(signed_data)\n", "    def test_timestamp_signer_expired(self):\n        signer = signing.TimestampSigner(key=\"test\", max_age=1)\n        data = {\"foo\": \"bar\"}\n        signed_data = signer.sign_object(data)\n        with freeze_time(\"2023-01-01 00:01:00\"):\n            with self.assertRaises(signing.SignatureExpired):\n                signer.unsign_object(signed_data)\n\n\n", "    def test_signature_expiration(self):\n        signer = signing.Signer(salt=\"test_salt\", max_age=5)\n        data = {\"key\": \"value\"}\n        signed_data = signer.sign_object(data)\n        with self.assertRaises(signing.SignatureExpired):\n            signer.unsign_object(signed_data, max_age=2)"], "sample_291": ["    def test_redirect_view(self):\n        redirect_url = reverse('example-view')\n        view = RedirectView.as_view(url=redirect_url, permanent=False)\n        response = view(self.rf.get('/'))\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response['Location'], redirect_url)\n\n\n", "    def test_redirect_view(self):\n        redirect_view = RedirectView.as_view(url='https://www.example.com/')\n        request = self.rf.get('/')\n        response = redirect_view(request)\n        self.assertRedirects(response, 'https://www.example.com/', status_code=302, target_status=None)\n", "    def test_redirect_view(self):\n        redirect_view = RedirectView.as_view(url='/about/')\n        response = redirect_view(self.rf.get('/'))\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response['Location'], '/about/')\n", "    def test_redirect_view(self):\n        redirect_view = RedirectView.as_view(url=\"/about/\")\n        response = redirect_view(self.rf.get('/'))\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response['Location'], '/about/')\n\n        # Test with pattern_name\n        redirect_view = RedirectView.as_view(pattern_name=\"about\")\n        response = redirect_view(self.rf.get('/'))\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response['Location'], '/about/')\n\n        # Test permanent redirect\n        redirect_view = RedirectView.as_view(url=\"/about/\", permanent=True)\n        response = redirect_view(self.rf.get('/'))\n        self.assertEqual(response.status_code, 301)\n        self.assertEqual(response['Location'], '/about/')\n\n        # Test query string\n        redirect_view = RedirectView.as_view(url=\"/about/\", query_string=True)\n        response = redirect_view(self.rf.get('/?key=value'))\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response['Location'], '/about/?key=value')\n\n        # Test missing redirect URL or pattern_name\n        redirect_view = RedirectView.as_view()\n        with self.assertRaises(ImproperlyConfigured):\n            redirect_view(self.rf.get('/'))\n", "    def test_redirect_view(self):\n        redirect_view = RedirectView.as_view(url='/about/')\n        response = redirect_view(self.rf.get('/'))\n        self.assertRedirects(response, '/about/')\n\n        # Test with pattern name\n        redirect_view = RedirectView.as_view(pattern_name='about')\n        response = redirect_view(self.rf.get('/'))\n        self.assertRedirects(response, reverse('about'))\n\n        # Test permanent redirect\n        redirect_view = RedirectView.as_view(permanent=True, url='/about/')\n        response = redirect_view(self.rf.get('/'))\n        self.assertRedirects(response, '/about/', status_code=301)\n\n        # Test query string inclusion\n        redirect_view = RedirectView.as_view(url='/about/', query_string=True)\n        response = redirect_view(self.rf.get('/?foo=bar'))\n        self.assertRedirects(response, '/about/?foo=bar')\n"], "sample_998": ["    def test_complex_regions(self):\n        region = ComplexRegion(\n            (z**2 + 1 > 0) & (abs(z) < 2),\n            (z**2 + 1 > 0)\n        )\n        self.assertEqual(latex(region), r\"\\left\\{ z \\suchthat \\left(z^{2} + 1 > 0\\right) \\land \\left(\\left|z\\right| < 2\\right) \\right\\}\")\n\n\n", "    def test_matrix_print(self):\n        M = Matrix([[1, 2], [3, 4]])\n        self.assertEqual(latex(M), r\"\\left[\\begin{matrix}1 & 2 \\\\ 3 & 4\\end{matrix}\\right]\")\n\n\n\n        M = Matrix(3, 3, range(9))\n        self.assertEqual(latex(M), r\"\\left[\\begin{matrix}0 & 1 & 2 \\\\ 3 & 4 & 5 \\\\ 6 & 7 & 8\\end{matrix}\\right]\")\n\n\n\n        M = Matrix(Symbol('x')*Matrix([[1, 2], [3, 4]]))\n        self.assertEqual(latex(M), r\"\\left[\\begin{matrix}x & 2 x \\\\ 3 x & 4 x\\end{matrix}\\right]\")\n\n\n", "    def test_primeomega(self):\n        expr = primeomega(x)\n        self.assertEqual(latex(expr), r\"\\Omega\\left(x\\right)\")\n        expr = primeomega(x, exp=2)\n        self.assertEqual(latex(expr), r\"\\Omega^{2}\\left(x\\right)\")\n", "compilation error", "def test_latex_mathml():\n    mathml = parse_mathml(\n        \"\"\"<math>\n        \\int_0^\\pi \\sin(x) \\, dx\n        </math>\n    \"\"\")\n    latex = latex(mathml)\n    expected = r\"\\int_{0}^{\\pi} \\sin{\\left (x \\right )} \\, dx\"\n    assert latex == expected\n\n"], "sample_136": ["    def test_encoding_error(self):\n        factory = RequestFactory()\n        request = factory.get('/?key=value', data='invalid_data')\n        with self.assertRaises(UnicodeDecodeError):\n            request.encoding = 'utf-8'\n            list(request.POST.items())\n", "    def test_post_multiple_file_uploads(self):\n        factory = RequestFactory()\n        request = factory.post('/some/path', {'name': 'test'},\n                              FILES={'myfile': [FakePayload('file1', 'text/plain'), FakePayload('file2', 'text/plain')]})\n        parser = MultiPartParser(request.META, request.body, [], request.encoding)\n        try:\n            post_data, files = parser.parse()\n        except MultiPartParserError:\n            self.fail(\"Failed to parse multipart request\")\n        self.assertEqual(post_data['name'], ['test'])\n        self.assertEqual(\n            files['myfile'].getlist('filename'), ['file1', 'file2']\n        )\n", "def test_read_after_parsing_fails(self):\n    request = FakePayload()\n    request.META = {}\n    request.content_type = 'application/x-www-form-urlencoded'\n    request._body = b'a=1&b=2'\n\n    # Ensure that trying to read the body after parsing raises the correct exception\n    try:\n        request.parse()\n        with self.assertRaises(RawPostDataException):\n            request.body\n    except (RawPostDataException, UnreadablePostError) as e:\n        self.assertEqual(str(e), 'You cannot access body after reading from request\\'s data stream')\n", "    def test_content_type_from_headers(self):\n        request = HttpRequest()\n        request.META = {}\n        request.META['CONTENT_TYPE'] = 'application/x-www-form-urlencoded'\n        request._set_content_type_params(request.META)\n        self.assertEqual(request.content_type, 'application/x-www-form-urlencoded')\n\n        request.META['CONTENT_TYPE'] = 'multipart/form-data; boundary=----WebKitFormBoundary7MA4YWxkTrZu0gW'\n        request._set_content_type_params(request.META)\n        self.assertEqual(request.content_type, 'multipart/form-data')\n        \n        request.META['CONTENT_TYPE'] = 'text/plain'\n        request._set_content_type_params(request.META)\n        self.assertEqual(request.content_type, 'text/plain')\n\n        request.META['CONTENT_TYPE'] = ''\n        request._set_content_type_params(request.META)\n        self.assertEqual(request.content_type, None)\n\n\n\n", "    def test_invalid_host(self):\n        factory = RequestFactory()\n        request = factory.get('/path', {'HTTP_HOST': 'invalid.host'})\n        with self.assertRaises(DisallowedHost):\n            # This will raise DisallowedHost because we're not in test mode\n            HttpRequest(request)\n"], "sample_1089": ["    def test_factor_terms_sqrt():\n        from sympy.polys.polytools import gcdex\n        expr = sqrt(x**4 + 2*x**2 + 1)\n        assert factor_terms(expr, radical=True) == sqrt(x**2 + 1)**2\n        res = factor_terms(expr, radical=True)\n        assert not res.is_Add\n        assert res.is_Pow and res.as_base() is sqrt(x**2 + 1) and res.as_exp() is 2\n        assert factor_terms(expr, clear=True) == (x**2 + 1)//2\n    \n", "    def test_factor_nc_symbols_in_products():\n        A = Symbol('A', commutative=False)\n        B = Symbol('B', commutative=False)\n        C = Symbol('C', commutative=False)\n        x = Symbol('x')\n        terms = [(A*B), (A*C), (B*C)]\n        factored = factor_nc(Mul(*terms))\n        expected = A*B*C\n        assert factored == expected \n", "    def test_factor_nc_multiple_nc_symbols():\n        from sympy.physics.secondquant import Commutator\n        A, B, C = symbols('A,B,C', commutative=False)\n        eq = A*Commutator(A, B) + B*Commutator(A, C)\n        res = factor_nc(eq)\n        expected = A*Commutator(A, B) + B*Commutator(A, C)\n        assert res == expected\n\n", "    def test_gcd_terms_mixed():\n        expr = Add(x*y*z**2 + y**2*z, x**2*y*z + y*z**3,\n                   x**3*y + y**2*z**2)\n        g = gcd_terms(expr, fraction=True)\n        g = g.subs(Dict(list(g.free_symbols.items())[1:])\n        assert g == y*z\n\n\n\n", "    def test_factor_terms_fraction():\n        from sympy import fractions\n        eq = (x**2 + 2*x*y + y**2)/2\n        fact = factor_terms(eq, fraction=True)\n        assert fact == (x**2 + 2*x*y + y**2)/2\n        assert int(fact) == (x**2 + 2*x*y + y**2)//2\n        fact = factor_terms(Rational(1,2)*(x**2 + y**2) + Rational(1,3)*(x + y)**2, fraction=True)\n        assert fact == Rational(1,2)*(x**2 + y**2) + Rational(1,3)*(x + y)**2\n        fact = factor_terms((x + y)/(2*(x + y)**2), fraction=True)\n        assert fact == 1/(2*(x + y))\n"], "sample_683": ["    def test_capture_manager_methods(method: str, request: SubRequest) -> None:\n        capman = CaptureManager(method)\n        assert capman.is_globally_capturing() == method != \"no\"\n        assert not capman.is_capturing()\n        assert capman._global_capturing is None\n        assert capman._capture_fixture is None\n", "    def test_capture_manager_global_capture(self, method, capsys):\n        capman = CaptureManager(method)\n        with capman.global_and_fixture_disabled():\n            captured_out, captured_err = capsys.readouterr()\n            assert not captured_out\n            assert not captured_err\n        capman.start_global_capturing()\n        print(\"hello\", file=sys.stdout)\n        print(\"goodbye\", file=sys.stderr)\n        captured_out, captured_err = capsys.readouterr()\n        assert captured_out == \"hello\\n\"\n        assert captured_err == \"goodbye\\n\"\n", "    def test_global_and_fixture_disabled(self, method: str) -> None:\n        capman = capture.CaptureManager(method)\n        with capman.global_and_fixture_disabled():\n            sys.stdout.write(\"stdout\\n\")\n            sys.stderr.write(\"stderr\\n\")\n        out, err = capman.read_global_capture()\n\n\n        assert out == \"\"\n        assert err == \"\"\n", "    def test_cap_manager_suspend_resume(self, capman: CaptureManager):\n        capman.start_global_capturing()\n        out, err = capman.read_global_capture()\n        assert out == \"\"\n        assert err == \"\"\n        \n        capman.suspend_global_capture()\n        print(\"Test output\")\n        sys.stderr.write(\"Test Error\\n\")\n        out, err = capman.read_global_capture()\n        assert out == \"\"\n        assert err == \"\"\n        \n        capman.resume_global_capture()\n        out, err = capman.read_global_capture()\n        assert out == \"Test output\\n\"\n        assert err == \"Test Error\\n\"\n\n        capman.stop_global_capturing()\n\n", "    def test_global_capturing(self, method: str) -> None:\n        capman = CaptureManager(method)\n        assert capman.is_globally_capturing() == method != \"no\"\n\n        with capman.global_and_fixture_disabled():\n            assert not capman.is_globally_capturing()\n\n        with capman.global_and_fixture_disabled():\n            capman.start_global_capturing()\n            capman.suspend_global_capture(in_=False)\n            capman.resume_global_capture()\n            assert capman.is_globally_capturing()\n        \n        capman.suspend_global_capture()\n        assert not capman.is_globally_capturing()\n        capman.resume_global_capture()\n        assert capman.is_globally_capturing()\n\n\n"], "sample_524": ["def test_colorbar_extension_length(spacing):\n    fig = _colorbar_extension_length(spacing)\n    return fig\n", "def test_colorbar_extension_length():\n    fig = _colorbar_extension_length('uniform')\n    plt.close(fig)\n\n    fig = _colorbar_extension_length('proportional')\n    plt.close(fig)\n", "def test_colorbar_extension_length():\n    with rc_context({'axes.colorbar.spacing': 'uniform'}):\n        fig = _colorbar_extension_length('uniform')\n    with rc_context({'axes.colorbar.spacing': 'proportional'}):\n        fig2 = _colorbar_extension_length('proportional')\n    check_figures_equal(fig, fig2, tol=0.1)\n", "def test_colorbar_extension_length(spacing):\n    fig = _colorbar_extension_length(spacing)\n    plt.close(fig)\n", "def test_colorbar_extension_length(request):\n    #Test colorbars with varying extension length\n    fig = _colorbar_extension_length(spacing='uniform')\n    if request.config.option.run_proportional_spacing:\n        fig.show()\n        fig = _colorbar_extension_length(spacing='proportional')\n        fig.show()\n"], "sample_1120": ["    def test_OneMatrix_determinant(self):\n        self.assertEqual(OneMatrix(2, 2).determinant(), 1)\n        self.assertEqual(OneMatrix(3, 3).determinant(), 1)\n", "    def test_matrix_derivative_scalar():\n        from sympy.matrices import MatrixSymbol, Matrix, ZeroMatrix\n\n        A = MatrixSymbol('A', 2, 2)\n        raises(TypeError, lambda: _matrix_derivative(A, ZeroMatrix(2, 2)))\n", "    def test_matmul_non_square():\n        with raises(ShapeError):\n            MatMul(A, B)\n", "    def test_matrix_transpose_identity(self):\n        I = Identity(3)\n        self.assertEqual(transpose(I), I)\n\n", "    def test_transpose_identity(self):\n        I = Identity(3)\n        self.assertEqual(transpose(I), I)\n"], "sample_1185": ["compilation error", "    def test_decompogen_abs():\n        f = Abs(sin(x))\n        result = decompogen(f, x)\n        assert result == [Abs, sin(x)]\n", "    def test_decompogen_max_function():\n        f = Max(x, y)\n        d = decompogen(f, x)\n        assert d == [Max(x, y)]\n\n        f = Max(x, sin(x))\n        d = decompogen(f, x)\n        assert d == [Max(x, sin(x))]\n\n        f = Max(x, sin(x)**2)\n        d = decompogen(f, x)\n        assert d == [Max(x, sin(x))**2]\n", "    def test_decompogen_power():\n        f = x**2\n        result = decompogen(f, x)\n        assert result == [x, x]\n\n", "    def test_decompogen_complex():\n        f = Abs(sin(x) + cos(x))\n        d = decompogen(f, x)\n        assert d == [Abs(x + y), sin(x), cos(x)]\n"], "sample_406": ["    def test_db_manager(self):\n        db_alias = 'test_db'\n        with connections[DEFAULT_DB_ALIAS].cursor() as cursor:\n            Article.objects.create(title='Test Title', content='Test Content')\n        with connections[db_alias].cursor() as cursor:\n            Article.objects.using(db_alias).create(title='Test Title Using', content='Test Content Using')\n        self.assertEqual(Article.objects.count(), 1)\n        self.assertEqual(Article.objects.using(db_alias).count(), 1)\n\n\n\n", "    def test_manager_get_queryset_handles_default_db(self):\n        with connections[DEFAULT_DB_ALIAS].cursor() as cursor:\n            cursor.execute(\"CREATE TABLE my_model (id INTEGER PRIMARY KEY)\")\n        with connections[DEFAULT_DB_ALIAS].cursor() as cursor:\n            cursor.execute(\"INSERT INTO my_model (id) VALUES (1)\")\n\n        class MyModel(models.Model):\n            id = models.AutoField(primary_key=True)\n            class Meta:\n                db_table = \"my_model\"\n\n        class MyManager(BaseManager):\n                super().__init__(model)\n\n        MyModel.objects = MyManager()  \n        qs = MyModel.objects.all()\n        self.assertTrue(qs.exists())\n\n\n", "    def test_auto_created_manager(self):\n        class AutoCreatedManager(BaseManager):\n            auto_created = True\n\n        class AutoCreatedModel(models.Model):\n            pass\n\n        manager = AutoCreatedManager()\n        manager.contribute_to_class(AutoCreatedModel, 'objects')\n        self.assertTrue(getattr(AutoCreatedModel, 'objects').auto_created)\n", "    def test_manager_creation_counter(self):\n        m1 = Article.objects\n        m2 = Article.objects\n        self.assertEqual(m1.creation_counter, m2.creation_counter)\n        m3 = Article.objects\n        self.assertEqual(m1.creation_counter + 1, m3.creation_counter)\n\n\n\n\n", "    def test_manager_creation_counter(self):\n        manager = BaseManager.from_queryset(models.QuerySet)\n        self.assertEqual(BaseManager.creation_counter, 1)\n\n        other_manager = BaseManager.from_queryset(models.QuerySet)\n        self.assertEqual(BaseManager.creation_counter, 2)\n\n        self.assertEqual(manager.creation_counter, 1)\n        self.assertEqual(other_manager.creation_counter, 2)\n"], "sample_460": ["    def test_get_list_queryset_ordering(self):\n        response = self.client.get(reverse('admin:article_list'))\n        articles = Article.objects.all().order_by('-date').values_list('title', flat=True)\n        titles = [a['title'] for a in response.context['object_list']]\n        self.assertEqual(articles, titles)\n    ", "compilation error", "    def test_admin_add_inline_form(self):\n        response = self.client.get(\n            reverse(\"admin:%s_add\" % City.model_name.model_name)\n        )\n        self.assertContains(response, \"Add City\")\n\n        response = self.client.post(\n            reverse(\"admin:%s_add\" % City.model_name.model_name),\n            {\"field0\": \"New City\"},\n        )\n        self.assertRedirects(response, reverse(\"admin:CITY_changelist\"))\n\n        new_city = City.objects.get(name=\"New City\")\n        self.assertEqual(new_city.name, \"New City\")\n", "    def test_admin_change_form_with_inline_forms(self):\n        response = self.client.get(\n            reverse(\"admin:%s_change\" % City.model_name, args=(self.s1.pk,)),\n            follow=True,\n        )\n        self.assertContains(response, \"Inline\")\n\n        # Edit the section inline\n        data = self.inline_post_data.copy()\n        data[\"section\"] = self.s1.pk\n        response = self.client.post(\n            reverse(\"admin:%s_change\" % City.model_name, args=(self.s1.pk,)),\n            data,\n            follow=True,\n        )\n\n        self.assertRedirects(response, reverse(\"admin:%s_changelist\" % City.model_name))\n\n        # Check that the section's articles were updated properly\n        updated_section = Section.objects.get(pk=self.s1.pk)\n        self.assertEqual(\n            updated_section.articles.count(), 3\n        )  # 3 articles were saved\n\n\n\n", "    def test_inline_formset_rendering(self):\n        response = self.client.get(reverse(\"admin:%s_%s_change\" % (\n            City.app_label, City._meta.model_name), kwargs={'object_id': 1}))\n        self.assertIn('<form', response.content.decode())  \n        self.assertIn('adminform', response.content.decode())\n        self.assertIn('article_set', response.content.decode())\n        self.assertIn('TOTAL_FORMS', response.content.decode())\n        self.assertIn('INITIAL_FORMS', response.content.decode())\n        self.assertIn('MAX_NUM_FORMS', response.content.decode())\n        "], "sample_871": ["compilation error", "    def test_silhouette_reduce_sparse(X, labels, random_state=None):\n        from sklearn.datasets import make_blobs\n        X, _ = make_blobs(n_samples=300, centers=4, random_state=random_state)\n        labels = np.array([0] * 75 + [1] * 75 + [2] * 75 + [3] * 75)\n\n        for format in [\"csr\", \"csc\"]:\n            sparse_X = csr_matrix(X)\n\n            intra, inter = _silhouette_reduce(sparse_X, 0, labels, np.ones(4))\n            dense_X = X\n            intra_dense, inter_dense = _silhouette_reduce(\n                dense_X, 0, labels, np.ones(4)\n            )\n            assert_allclose(intra, intra_dense)\n            assert_allclose(inter, inter_dense)\n\n            # Test dok_matrix format\n            sparse_X = dok_matrix(X)\n            assert issparse(sparse_X)\n            intra, inter = _silhouette_reduce(sparse_X, 0, labels, np.ones(4))\n            dense_X = X\n            intra_dense, inter_dense = _silhouette_reduce(\n                dense_X, 0, labels, np.ones(4)\n            )\n            assert_allclose(intra, intra_dense)\n            assert_allclose(inter, inter_dense)\n\n\n            # Test lil_matrix format\n            sparse_X = lil_matrix(X)\n            assert issparse(sparse_X)\n            intra, inter = _silhouette_reduce(sparse_X, 0, labels, np.ones(4))\n            dense_X = X\n            intra_dense, inter_dense = _silhouette_reduce(\n                dense_X, 0, labels, np.ones(4)\n            )\n            assert_allclose(intra, intra_dense)\n            assert_allclose(inter, inter_dense)\n\n", "compilation error", "    def test_silhouette_score_sparse_matrix(n_samples, n_clusters):\n        X = dok_matrix((n_samples, n_samples))\n        for i in range(n_samples):\n            for j in range(i + 1, n_samples):\n                X[i, j] = X[j, i] = np.random.rand()\n        labels = np.random.randint(0, n_clusters, size=n_samples)\n        score = silhouette_score(X, labels, metric=\"precomputed\")\n        assert np.isfinite(score)\n", "compilation error"], "sample_1013": ["    def test_implemented_function_duplicate(self):\n        from sympy.abc import x\n        f = implemented_function(Function('f'), lambda x: x+1)\n        g = implemented_function(Function('f'), lambda x: x*10)\n\n", "    def test_lambdify_deferred_vector():\n        from sympy.matrices import DeferredVector\n        dv = DeferredVector('x')\n        expr = dv.norm()\n\n        func = lambdify(x, expr)\n        assert func( [1, 2, 3] ) == math.sqrt(1**2 + 2**2 + 3**2)\n\n        # Check that lambdify with 'numpy' produces a numpy array\n        func_numpy = lambdify(x, expr, 'numpy')\n        assert isinstance(func_numpy([1, 2, 3]), numpy.ndarray)\n        assert numpy.allclose(func_numpy([1, 2, 3]), func([1, 2, 3])) \n", "    def test_tensor_flow():\n        if not tensorflow:\n            skip(\"Tensorflow not available\")\n        f = implemented_function(Function('f'), lambda x: x**2)\n        t = tensorflow.constant(2)\n\n        tf_f = lambdify(t, f(t), 'tensorflow')\n        assert tf_f(t).eval().numpy() == 4\n\n\n\n", "    def test_tensorflow(self):\n        from sympy.utilities.lambdify import lambdastr\n        from sympy.tensor import TensorSymbol, TensorProduct\n\n        t = TensorSymbol('t', (2, 3))\n        s = TensorSymbol('s', (3, ))\n        x = TensorProduct(t, s)\n\n        f = lambdify(x, t[0, 0] * s[0])\n        self.assertTrue(isinstance(f(t, s), tensorflow.Tensor))\n", "    def test_multidimensional_array_args():\n        from sympy import Array\n        x, y = symbols('x y')\n        a = Array([[x, y], [x + 1, y + 1]])\n        f = lambda a: a[0, 0]\n        f_lamb = lambdify(a, f)\n        assert f_lamb(a) == x\n        f_lamb = lambdify((x, y), f(Array([[x, y], [x + 1, y + 1]])))\n        assert f_lamb(x, y) == x\n"], "sample_513": ["    def test_draggable_legend(self, tmpdir):\n        fig, ax = plt.subplots()\n        line = mplines.Line2D([0, 1], [0, 1], marker='o', label='line')\n        patch = mpl.patches.Rectangle((0.2, 0.2), 0.3, 0.3,\n                                      facecolor='red', label='patch')\n        ax.add_artist(line)\n        ax.add_artist(patch)\n        legend = fig.legend(handles=[line, patch])\n        legend.set_draggable(True)\n        \n        # save the initial position\n        bbox1 = legend.get_window_extent()\n\n        # simulate dragging the legend\n        with rc_context({'axes.prop_cycle': mpl.cycler(color=None)}):\n            legend.draggable =  lambda: (True, 0, 0, 0)\n            fig.canvas.draw_idle()\n\n        # update legend position and check\n        bbox2 = legend.get_window_extent()\n        check_figures_equal(bbox1, bbox2,\n                           atol=1e-3,\n                           rtol=1e-3)\n\n        legend.set_draggable(False)\n", "    def test_draggable_legend(self, image_comparison):\n        with rc_context({'axes.prop_cycle': cycle(['red', 'green'])}):\n            fig, axes = plt.subplots(1, 1)\n            axes.plot(np.random.rand(10), 'r-')\n            axes.plot(np.random.rand(10), 'g-')\n            legend = axes.legend(draggable=True)\n            legend = legend.set_draggable(state=True)\n\n            # Get a transform for the renderer to get pixel location\n            renderer = fig.canvas.get_renderer()\n            bbox = legend.get_window_extent(renderer=renderer)\n            x_origin = bbox.xmin\n            y_origin = bbox.ymin\n\n            # Simulate dragging the legend\n            event = mpl.backend.backend_bases.MouseEvent(\n                'button1down', 100, 100, 0, 0,\n                {'canvas': fig.canvas,\n                 'modifier': 0},\n                )\n            legend._draggable.mouse_down(event)\n            event = mpl.backend.backend_bases.MouseEvent(\n                'motion', x_origin + 10, y_origin + 10, 0, 0,\n                {'canvas': fig.canvas,\n                 'modifier': 0},\n                )\n            legend._draggable.mouse_move(event)\n            event = mpl.backend.backend_bases.MouseEvent(\n                'button1up', x_origin + 10, y_origin + 10, 0, 0,\n                {'canvas': fig.canvas,\n                 'modifier': 0},\n                )\n            legend._draggable.mouse_up(event)\n\n            # Save the figure and compare it\n            fig.tight_layout()\n            image_comparison(\n                fig,\n                'test_draggable_legend.png',\n                savefig_kwargs={'bbox_inches': 'tight'},\n                tol=0.05,\n            )\n\n\n\n", "    def test_legend_loc(self, loc):\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3], [4, 5, 6])\n        ax.legend(loc=loc)\n        plt.close()\n\n\n\n", "    def test_get_default_handler_map(self):\n        # Test the default handler map used by the legend\n        handler_map = mlegend.Legend.get_default_handler_map()\n        assert isinstance(handler_map, dict)\n        assert isinstance(handler_map.get(Line2D), mlegend.HandlerLine2D)\n        assert isinstance(handler_map.get(Patch), mlegend.HandlerPatch)\n        assert isinstance(handler_map.get(collections.abc.Collection),\n                          mlegend.HandlerCollection)\n", "    def test_get_default_handler_map(self):\n        fig, ax = plt.subplots()\n        line, = ax.plot([1, 2, 3])\n        patch = plt.Rectangle((0, 0), 1, 1, color='red')\n        collection = mcollections.EllipseCollection([((0, 0), (1, 1))], \n                                                   facecolor='blue')  \n        \n        legend = mlegend.Legend(ax, [line, patch, collection])\n        legend._init_legend_box([], [], markerfirst=False) \n        self.assert_legend_equal(legend, \n                                  ax.legend([line, patch, collection],\n                                          ['Line', 'Patch', 'Collection']))\n"], "sample_68": ["    def test_callable_setting_wrapper(self):\n        callable_setting = mock.MagicMock(return_value='test_value')\n        wrapper = CallableSettingWrapper(callable_setting)\n\n        self.assertIs(wrapper._wrapped, callable_setting)\n        self.assertEqual(wrapper(), 'test_value')\n", "    def test_callable_setting_wrapper(self):\n            return 'SECRET_VALUE'\n\n        wrapped_setting = CallableSettingWrapper(my_callable_setting)\n\n        self.assertEqual(wrapped_setting(), 'SECRET_VALUE')\n\n        # Ensure that CallableSettingWrapper doesn't call the wrapped callable\n        # when it is repr'ed.\n        self.assertEqual(repr(wrapped_setting), '<function my_callable_setting at 0x...>')\n\n\n\n", "    def test_callable_setting_wrapper(self):\n            return 'This is a callable setting'\n\n        wrapped_setting = CallableSettingWrapper(my_callable)\n        self.assertIsNot(wrapped_setting, my_callable)\n        self.assertEqual(wrapped_setting(), 'This is a callable setting')\n\n        # Check that calling the wrapper doesn't break `repr` functionality\n        self.assertEqual(repr(wrapped_setting), '<CallableSettingWrapper: my_callable>')\n\n\n\n", "    def test_callable_setting_wrapper_calls_callable(self):\n            return \"This is a callable setting.\"\n\n        wrapped_setting = CallableSettingWrapper(callable_setting)\n        self.assertEqual(wrapped_setting(), \"This is a callable setting.\")\n\n\n\n", "    def test_callable_setting_wrapper_wraps_callable(self):\n            return 42\n\n        wrapped_callable = CallableSettingWrapper(callable_setting)\n        self.assertTrue(callable(wrapped_callable))\n        self.assertEqual(wrapped_callable(), 42)\n\n"], "sample_116": ["    def test_make_template_fragment_key(self):\n        self.assertEqual(make_template_fragment_key('my_fragment'), 'template.cache.my_fragment.00000000000000000000000000000000')\n        self.assertEqual(make_template_fragment_key('my_fragment', vary_on=['arg1', 'arg2']),\n                         'template.cache.my_fragment.b7217ba2d7ae745b390e95c64907887c')  # Different for each run\n", "    def test_make_template_fragment_key(self):\n        self.assertEqual(make_template_fragment_key('my_fragment'), 'template.cache.my_fragment.00000000000000000000000000000000')\n        self.assertEqual(make_template_fragment_key('my_fragment', vary_on=['param1', 'param2']), 'template.cache.my_fragment.param1:param2:00000000000000000000000000000000')\n", "    def test_make_template_fragment_key_with_vary_on(self):\n        fragment_name = 'my_fragment'\n        vary_on = ['request.user', 'some_other_param']\n        expected_key = TEMPLATE_FRAGMENT_KEY_TEMPLATE % (\n            fragment_name,\n            hashlib.md5(\n                (str(vary_on[0]).encode() + b':' + str(vary_on[1]).encode())\n            ).hexdigest()\n        )\n        self.assertEqual(make_template_fragment_key(fragment_name, vary_on), expected_key)\n", "    def test_make_template_fragment_key(self):\n        self.assertEqual(make_template_fragment_key('fragment_name'), 'template.cache.fragment_name.{}')\n        self.assertEqual(make_template_fragment_key('fragment_name', vary_on=[1, 2, 3]),\n                         'template.cache.fragment_name.{}')\n", "    def test_make_template_fragment_key(self):\n        self.assertEqual(make_template_fragment_key('my_fragment'), 'template.cache.my_fragment.48b76e103ec16d779698e93e3337db37')\n        self.assertEqual(make_template_fragment_key('my_fragment', vary_on=['arg1', 'arg2']), 'template.cache.my_fragment.a0e334b3a321181f715a84920d238c0b')\n        with self.assertRaises(TypeError):\n            make_template_fragment_key('my_fragment', vary_on=None) \n"], "sample_1060": ["    def test_sparse_matrix(self):\n        a = SparseMatrix(((0, 0, 1), (1, 1, 0), (2, 2, 1)))\n        self.assertEqual(SciPyPrinter().doprint(a), \"scipy.sparse.coo_matrix([[1], [0], [1]], [0, 1, 2], shape=(3, 3))\")\n", "    def test_sparse_matrix(self):\n        mat = SparseMatrix(((1, 0, 2), (0, 1, 3)), (3, 3))\n        s = SciPyPrinter().doprint(mat)\n        expected = 'scipy.sparse.coo_matrix([2, 3, 1], ([0, 1, 0], [0, 1, 2]), shape=(3, 3))'\n        self.assertEqual(s, expected)\n\n", "    def test_SparseMatrix(self):\n        smat = SparseMatrix(((1, 0, 2), (0, 1, 3)), (3, 3))\n        s = SciPyPrinter().doprint(smat)\n        assert s == \"\"\"scipy.sparse.coo_matrix(((2, 0), (1, 1), (0, 2)), ((0, 1), (1, 0), (2, 2)), shape=(3, 3))\"\"\" \n\n", "    def test_sparse_matrix_printing(self):\n        sparse_matrix = SparseMatrix(((1, 0, 2), (0, 1, 3)), (3, 3))\n        self.assertEqual(SciPyPrinter().doprint(sparse_matrix),\n                         'scipy.sparse.coo_matrix([[2, 0, 0], [0, 1, 3], [0, 0, 1]], shape=(3, 3))')\n", "    def test_numpy_matrix_operations(self):\n        m = MatrixSymbol('m', 2, 2)\n        n = MatrixSymbol('n', 2, 2)\n        i = Identity(2)\n        expr = m*n\n        self.assertEqual(NumPyPrinter().doprint(expr), 'numpy.dot(m, n)')\n\n        expr = m + n\n        self.assertEqual(NumPyPrinter().doprint(expr), 'm + n')\n\n        expr = m @ n\n        self.assertEqual(NumPyPrinter().doprint(expr), 'numpy.dot(m, n)')\n\n        expr = m.transpose()\n        self.assertEqual(NumPyPrinter().doprint(expr), 'numpy.transpose(m)')\n\n        expr = m.T\n        self.assertEqual(NumPyPrinter().doprint(expr), 'numpy.transpose(m)')\n\n        expr = i\n        self.assertEqual(NumPyPrinter().doprint(expr), 'numpy.eye(2)')\n\n        expr = m.dot(n)\n        self.assertEqual(NumPyPrinter().doprint(expr), 'numpy.dot(m, n)')\n\n\n"], "sample_380": ["    def test_aggregate_with_filter(self):\n        with self.assertNumQueries(2):\n            books = Book.objects.filter(\n                Q(rating__gt=4) | Q(\n                    pubdate__year=timezone.now().year\n                )\n            ).values(\n                'isbn',\n                'name',\n                'rating').annotate(\n                avg_rating=Avg('rating')\n            ).filter(avg_rating__gt=4)\n        self.assertEqual(len(books), 5)\n        books = Book.objects.filter(\n            Q(rating__gt=4) | Q(pubdate__year=timezone.now().year)\n        ).values(\n            'isbn',\n            'name',\n            'rating'\n        ).annotate(\n            avg_rating=Avg('rating')\n        ).filter(avg_rating__gt=4)\n\n        for book in books:\n            self.assertTrue(book['rating'] > 4 or book['pubdate'].year == timezone.now().year)\n            self.assertTrue(book['avg_rating'] > 4)\n", "compilation error", "compilation error", "    def test_aggregate_with_filter(self):\n        with self.subTest('COUNT'):\n            filtered_count = Book.objects.filter(\n                Q(rating__gt=4.0)\n            ).annotate(\n                filtered_count=Count('id', filter=Q(rating__gt=4.0))\n            ).values('filtered_count').get()[\n                'filtered_count'\n            ]\n            self.assertEqual(filtered_count, 3)\n\n        with self.subTest('AVG'):\n            filtered_avg = Book.objects.filter(\n                Q(rating__gt=4.0)\n            ).annotate(\n                filtered_avg=Avg('rating', filter=Q(rating__gt=4.0))\n            ).values('filtered_avg').get()[\n                'filtered_avg'\n            ]\n            self.assertAlmostEqual(filtered_avg, 4.6666666666666665)\n\n        with self.subTest('MAX'):\n            filtered_max = Book.objects.filter(\n                Q(rating__gt=4.0)\n            ).annotate(\n                filtered_max=Max('rating', filter=Q(rating__gt=4.0))\n            ).values('filtered_max').get()[\n                'filtered_max'\n            ]\n            self.assertEqual(filtered_max, 5.0)\n\n\n", "    def test_aggregates_with_filter(self):\n        with CaptureQueriesContext() as qs:\n            Avg(Book.rating).filter(contact=self.a1)\n            Avg(Book.rating).filter(contact__name='Jacob Kaplan-Moss')\n            Avg(Book.rating).filter(\n                Q(contact=self.a1) | Q(contact__name='Jacob Kaplan-Moss')\n            )\n        self.assertNumQueries(2)\n\n        # Filter should be applied to the inner query\n        with CaptureQueriesContext() as qs:\n            Avg(Book.rating).filter(\n                Q(publisher=self.p1) & Q(rating__gt=4)\n            )\n        self.assertNumQueries(2)\n"], "sample_21": ["    def test_err_specs(self):\n        table = Table([\n            [1, 2, 3, 4],\n            [5, 6, 7, 8]\n        ], names=['a', 'b', 'c', 'd'])\n        err_specs = {'serr': [2, 3], 'terr': [1]}\n        lines = _write_table_qdp(table, err_specs=err_specs)\n        new_table = _read_table_qdp(lines)\n        assert new_table.colnames == ['a', 'a_err', 'a_perr', 'a_nerr', 'b', 'b_err', 'b_perr', 'b_nerr', 'c', 'd']\n        assert np.array_equal(new_table['a'], np.array([1, 5]))\n\n\n\n", "    def test_read_table_with_err_specs(self):\n        table = Table.read(\n            \"testdata/qdp_err_specs.qdp\", format=\"ascii.qdp\", table_id=0\n        )\n        assert table.meta[\"initial_comments\"] == [\"! Initial comment line 1\\n\", \"! Initial comment line 2\\n\"]\n        assert table.meta[\"comments\"] == [\"! Table 0 comment\\n\"]\n        assert table.colnames == [\"MJD\", \"MJD_err\", \"MJD_perr\", \"MJD_nerr\", \"Rate\", \"Rate_err\"]\n        assert np.allclose(table[\"MJD\"], [53000.5, 54000.5])\n        assert np.allclose(table[\"MJD_err\"], [0.25, 1.25])\n        assert np.allclose(table[\"MJD_perr\"], [-0.5, -1.5])\n        assert np.allclose(table[\"MJD_nerr\"], [-0.5, -1.5])\n        assert np.allclose(table[\"Rate\"], [1, 2])\n        assert np.allclose(table[\"Rate_err\"], [1.5, 2.5])\n\n", "    def test_missing_values(self):\n        data = np.array([[1.0, 2.0, \"NO\", 4.0],\n                         [5.0, np.nan, \"NO\", 7.0]])\n        names = ['a', 'b', 'c', 'd']\n        table = Table(data=data, names=names)\n        \n        with pytest.warns(AstropyUserWarning):\n            ascii.read(str(table), format='ascii.qdp')\n        \n        with open('test.qdp', 'w') as f:\n            f.write(ascii.to_string(table, format='ascii.qdp', delimiter=' '))\n        \n        with pytest.warns(AstropyUserWarning):\n            table_read = ascii.read('test.qdp', format='ascii.qdp')\n\n        assert (table_read['c'] == \"NO\").all()\n        assert (table_read['b'] == np.ma.masked).all()\n        \n        \n        \n", "    def test_err_specs_missing(self):\n        table = Table([\n            [1, 2, 3],\n            [4, 5, 6],\n            [7, 8, 9]\n        ], names=['col1', 'col2', 'col3'])\n        with pytest.raises(ValueError, match=\"Missing positive error\"):\n            _write_table_qdp(table, err_specs={'serr': [2]})\n        with pytest.raises(ValueError, match=\"Missing negative error\"):\n            _write_table_qdp(table, err_specs={'terr': [1]})\n\n\n\n", "    def test_err_specs(self):\n        table = Table(\n            [\n                Column(data=[1, 2, 3, 4]),\n                Column(data=[5, 6, 7, 8]),\n                Column(data=[9, 10, 11, 12]),\n            ],\n            names=[\"a\", \"b\", \"c\"],\n        )\n        with pytest.warns(AstropyUserWarning):\n            lines = _write_table_qdp(table, err_specs={\"serr\": [2], \"terr\": [1]})\n\n        new_table = _read_table_qdp(lines)\n        assert new_table.colnames == [\"a\", \"a_err\", \"b\", \"b_perr\", \"b_nerr\", \"c\"]\n        assert np.all(new_table[\"a\"] == [1, 2, 3, 4])\n        assert np.all(new_table[\"b\"] == [5, 6, 7, 8])\n        assert np.all(new_table[\"c\"] == [9, 10, 11, 12])\n\n\n\n"], "sample_376": ["    def test_max_cookie_size_with_extra_tags(self):\n        storage = self.get_storage()\n        message1 = Message(constants.SUCCESS, 'Message 1', extra_tags='tag1')\n        message2 = Message(constants.SUCCESS, 'Message 2', extra_tags='tag2')\n        message3 = Message(constants.SUCCESS, 'Message 3', extra_tags='tag3')\n        messages = [message1, message2, message3]\n\n        # Encode the messages.\n        encoded_data = storage._encode(messages)\n\n        # Set a small max cookie size.\n        with override_settings(\n            SESSION_COOKIE_SIZE=1000\n        ):\n            storage = self.get_storage()\n            set_cookie_data(storage, encoded_data)\n\n            # Assert that the cookie has been truncated to fit.\n            response = self.get_response()\n            self.assertEqual(stored_cookie_messages_count(storage, response), 1)\n\n            # Assert that the first message is stored correctly.\n            decoded_messages = storage._decode(\n                response.cookies[CookieStorage.cookie_name].value\n            )\n            self.assertEqual(decoded_messages[0].extra_tags, 'tag1')`\n\n\n\n\n", "    def test_not_finished_sentinal(self):\n        storage = self.get_storage()\n        messages = [Message(constants.INFO, 'Message 1'), Message(constants.INFO, 'Message 2')]\n        set_cookie_data(storage, messages)\n        response = self.get_response()\n        self.assertEqual(self.stored_messages_count(storage, response), 2)\n        \n        # Simulate retrieving messages in chunks\n        storage._loaded_data = None\n        set_cookie_data(storage, messages + [Message(constants.INFO, 'Message 3')], encode_empty=True)\n        response = self.get_response()\n        self.assertEqual(self.stored_messages_count(storage, response), 2)  # We should only have retrieved the first 2\n        set_cookie_data(storage, [Message(constants.INFO, 'Message 4')], encode_empty=True)\n        response = self.get_response()\n        self.assertEqual(self.stored_messages_count(storage, response), 3)  # We now have all 3\n\n", "    def test_cookie_size_limits(self):\n        storage = self.get_storage()\n        messages = [\n            Message(constants.DEBUG, 'A', extra_tags='tag1'),\n            Message(constants.DEBUG, 'B', extra_tags='tag2'),\n            Message(constants.DEBUG, 'C', extra_tags='tag3'),\n        ] * 15\n        response = self.create_response()\n        set_cookie_data(storage, messages)\n        self.assertEqual(stored_cookie_messages_count(storage, response), 14)\n\n\n\n", "    def test_cookie_storage_max_size(self):\n        storage = self.get_storage()\n        msgs = [Message(level, 'message {}'.format(i))\n                for i in range(10)]\n        response = self.create_response()\n\n        # First, store all messages\n        storage._store(msgs, response)\n        self.assertEqual(self.stored_messages_count(storage, response), 10)\n\n        # Add more messages, exceeding the max size\n        msgs.extend([Message(level, 'message {}'.format(i))\n                     for i in range(10, 20)])\n\n        storage._store(msgs, response)\n        self.assertEqual(self.stored_messages_count(storage, response), 10)\n\n        # Ensure the oldest messages were discarded\n        for i in range(10):\n            self.assertIn(Message(constants.DEBUG, 'message {}'.format(i)),\n                          storage.get_messages())\n        for i in range(10, 20):\n            self.assertNotIn(Message(constants.DEBUG, 'message {}'.format(i)),\n                             storage.get_messages())\n\n        # Test that the sentinel value is set when data is truncated\n        self.assertIn(storage.not_finished, storage.get_messages())\n\n\n\n", "    def test_encode_decode_with_safe_data(self):\n        message = Message(constants.SUCCESS, 'This is a message with <script>...</script> tags', extra_tags='safe')\n        storage = self.get_storage()\n        encoded = storage._encode(message)\n        decoded = storage._decode(encoded)\n\n        self.assertIsInstance(decoded[0], Message)\n        self.assertEqual(decoded[0].level, constants.SUCCESS)\n        self.assertEqual(decoded[0].message, mark_safe('This is a message with <script>...</script> tags'))\n        self.assertEqual(decoded[0].extra_tags, 'safe')\n\n\n\n"], "sample_199": ["    def test_when_expression(self):\n        expr = When(Q(rating__gt=4), then=True).otherwise(False)\n        self.assertEqual(expr.as_sql(compiler=Case, connection=connection)['sql'],\n                         'CASE WHEN (rating > 4) THEN 1 ELSE 0 END')\n        self.assertEqual(expr.as_sql(compiler=Case, connection=connection)['params'], [])\n", "    def test_raw_sql_expression_with_output_field(self):\n        class CustomExpression(Expression):\n            template = '%(field_name)s'\n\n        expression = CustomExpression(field_name='my_field')\n        self.assertEqual(expression.output_field, 'my_field')\n\n        # Test that the output_field is set correctly when using RawSQL \n        raw_sql = RawSQL('SELECT my_field FROM my_table', [])\n        output_field = 'my_field'\n        raw_sql_expr = ExpressionWrapper(raw_sql, output_field=output_field)\n        self.assertEqual(raw_sql_expr.output_field, output_field)\n\n\n\n", "compilation error", "    def test_subquery_ordering(self):\n        subquery = (\n            DepartmentStore.objects.filter(original_opening__year__lte=1980)\n            .annotate(\n                count_books=Count('books__name'),\n            )\n            .values('name', 'count_books')\n        )\n        ordering = Ordering(subquery.values('count_books', 'name')[0].get('count_books'), descending=True)\n\n        qs = DepartmentStore.objects.annotate(\n            count_books=Subquery(subquery.query),\n        ).order_by(ordering)\n\n        result = list(qs.values_list('name', 'count_books'))\n\n        expected_result = [\n            ('Mamma and Pappa\\'s Books', 3),\n        ]\n\n        self.assertEqual(result, expected_result)\n", "compilation error"], "sample_973": ["    def test_signature_from_str(simple_signature, complex_signature):\n        sig1 = inspect.signature_from_str(simple_signature)\n        assert sig1 == inspect.signature(simple_function)\n        sig2 = inspect.signature_from_str(complex_signature)\n        assert sig2 == inspect.signature(complex_function)\n\n\n", "    def test_signature_from_ast_vararg_keyword():\n        code = \"\"\"", "    def test_signature_from_str_with_vararg():\n        code = \"\"\"", "    def test_signature_from_ast_with_kwarg(self):\n        code = \"\"\"", "    def test_is_enum_attribute_descriptor(self):\n        class MyClass:\n            class EnumAttribute(enum.Enum):\n                A = 1\n\n            attr = EnumAttribute.A\n\n        attr = MyClass().attr\n\n        assert inspect.isattribute_descriptor(attr)\n        assert inspect.isenumattribute(attr)\n\n\n\n"], "sample_534": ["    def test_quad_contour_multiple(self):\n        # Test that multiple contour lines and filled regions are\n        # drawn correctly.\n\n        fig, ax = plt.subplots()\n        x = np.linspace(-4, 4, 100)\n        y = np.linspace(-4, 4, 100)\n        X, Y = np.meshgrid(x, y)\n        Z = np.sin(np.sqrt(X**2 + Y**2))\n\n        contour = ax.contour(X, Y, Z*4, levels=[-2, 0, 2],\n                            linewidths=[1.5, 2.5, 1.5],\n                            linestyles=['solid', 'dashed', 'dotted'])\n        filled_contour = ax.contourf(X, Y, Z*4, levels=[-1.5, 0, 1.5],\n                                    colors=['red', 'blue', 'green'])\n\n        ax.clabel(contour, inline=True, fontsize=8)\n        ax.set_title('Contour and Filled Contour')\n        ax.set_xlabel('X')\n        ax.set_ylabel('Y')\n        plt.show()\n\n\n\n", "    def test_corner_mask(self):\n        x, y = np.mgrid[0:10:100j, 0:10:100j]\n        z = np.sin(x) * np.cos(y)\n        z[::5, ::5] = np.nan\n\n        with rc_context({'contour.corner_mask': True}):\n            cs = plt.contour(x, y, z)\n            fig, ax = plt.subplots()\n            ax.imshow(z, extent=ax.get_xlim() + ax.get_ylim(),\n                      interpolation='nearest', cmap='viridis')\n            ax.clabel(cs, fmt='%1.1f', inline=True, fontsize=8)\n            plt.show()\n        # ...\n\n\n", "    def test_contour_z_logscale(self):\n        with rc_context({'contour.origin': 'lower', 'contour.logscale': True}):\n            x = np.linspace(0, 10, 100)\n            y = np.linspace(0, 10, 100)\n            X, Y = np.meshgrid(x, y)\n            Z = np.exp(X ** 2 + Y ** 2)\n            cs = plt.contour(X, Y, Z, levels=5, cmap=\"rainbow\")\n            plt.title(\"logscale\")\n            plt.colorbar()\n            plt.show()\n\n", "    def test_fill_negative(self):\n        fig, ax = plt.subplots(1, 1)\n        x = np.linspace(-1, 1, 100)\n        y = np.linspace(-1, 1, 100)\n        X, Y = np.meshgrid(x, y)\n        Z = np.sin(np.sqrt(X**2 + Y**2))\n        cs = ax.contourf(X, Y, Z, levels=np.linspace(-0.8, 0.8, 11),\n                         colors=['red', 'blue'], extend='both')\n        ax.clabel(cs, inline=True, fontsize=8)\n        plt.show()\n\n", "    def test_contour_quad_test(self):\n        fig, ax = plt.subplots()\n        x = np.linspace(-3, 3, 100)\n        y = np.linspace(-3, 3, 100)\n        X, Y = np.meshgrid(x, y)\n        Z = np.sin(np.sqrt(X**2 + Y**2))\n\n        CS = ax.contour(X, Y, Z, levels=np.linspace(-1, 1, 11))\n        ax.clabel(CS, inline=True, fontsize=8)\n\n        ax.set_xlim(-3, 3)\n        ax.set_ylim(-3, 3)\n        ax.set_aspect('equal')\n        plt.show()\n"], "sample_166": ["    def test_get_random_string(self):\n        self.assertEqual(len(get_random_string()), 12)\n        self.assertEqual(len(get_random_string(length=22)), 22)\n        self.assertEqual(len(get_random_string(length=0)), 0)\n\n        # Test with custom characters\n        custom_chars = '0123456789'\n        self.assertEqual(len(get_random_string(length=10, allowed_chars=custom_chars)), 10)\n", "    def test_pbkdf2(self):\n        salt = b'salt'\n        iterations = 10000\n        password = b'password'\n        result = pbkdf2(password, salt, iterations)\n        expected_length = hashlib.sha256().digest_size\n        self.assertEqual(len(result), expected_length)\n\n", "    def test_pbkdf2(self):\n        salt = b'salt'\n        iterations = 1000\n        password = b'password'\n        dklen = 32\n\n        # Test default digest\n        result = pbkdf2(password, salt, iterations, dklen=dklen)\n        self.assertEqual(result.digest_size, dklen)\n\n        # Test custom digest\n        result = pbkdf2(password, salt, iterations, digest=hashlib.sha512, dklen=dklen)\n        self.assertEqual(result.digest_size, dklen)\n", "    def test_salted_hmac(self):\n        key_salt = b\"this_is_a_key_salt\"\n        secret = b\"this_is_a_secret_key\"\n        value = b\"this_is_a_value\"\n        hmac_value = salted_hmac(key_salt, value, secret, algorithm='sha256')\n        # Ensure the returned value is a bytes object\n        self.assertIsInstance(hmac_value, bytes)  \n        # Verify the output by calculating the hmac manually using hashlib\n        self.assertEqual(hmac_value, hmac.new(\n            hashlib.sha256(key_salt + secret).digest(),\n            msg=value,\n            digestmod=hashlib.sha256\n        ).digest())\n        # Test with a different algorithm\n        hmac_value_sha1 = salted_hmac(key_salt, value, secret, algorithm='sha1')\n        self.assertNotEqual(hmac_value, hmac_value_sha1)\n        with self.assertRaises(InvalidAlgorithm):\n            salted_hmac(key_salt, value, secret, algorithm='bogus_algorithm')\n\n", "    def test_constant_time_compare(self):\n        self.assertTrue(constant_time_compare(\"hello\", \"hello\"))\n        self.assertFalse(constant_time_compare(\"hello\", \"world\"))\n        self.assertFalse(constant_time_compare(\"hello\", \"hell\"))\n\n\n        # test case with different length strings\n        self.assertTrue(constant_time_compare(\"abcdefg\", \"abcdefg\"))\n        self.assertFalse(constant_time_compare(\"abcdefg\", \"abcdef\"))\n"], "sample_310": ["    def test_model_detail_view(self):\n        url = reverse('admin:admindocs_model_detail', args=[\n            'mysite', 'company'\n        ])\n        response = self.client.get(url)\n        self.assertContains(response, 'Company')\n        self.assertContains(response, 'name')\n        self.assertContains(response, 'employees')\n        self.assertContains(response, 'description')\n\n        # Ensure related objects are also displayed\n        url = reverse('admin:admindocs_model_detail', args=[\n            'mysite', 'person'\n        ])\n        response = self.client.get(url)\n        self.assertContains(response, 'Person')\n        self.assertContains(response, 'company')\n        self.assertContains(response, 'age')\n\n", "    def test_model_detail_view(self):\n        # Create a URL pattern for a test view\n        url_patterns = [\n            path('', include('admin.urls')),\n        ]\n        with override_settings(ROOT_URLCONF=url_patterns):\n            response = self.client.get(reverse('admin:admindocs_model_detail', kwargs={'app_label': 'myapp', 'model_name': 'company'}))\n\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, \"Company\")\n        self.assertContains(response, \"name\")\n        self.assertContains(response, \"description\")\n        self.assertContains(response, \"industry\")\n", "    def test_model_detail_view(self):\n        url_path = reverse('admin:model_detail', kwargs={'app_label': 'my_app', 'model_name': 'company'})\n        response = self.client.get(url_path)\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, 'Company')\n        self.assertContains(response, 'Name')\n        self.assertContains(response, 'Address')\n\n\n\n", "    def test_view_detail_view_handles_missing_view(self):\n        url = reverse('admin_doc:view_detail', kwargs={'view': 'non_existent_view'})\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 404)\n\n\n\n", "    def test_view_detail_view(self):\n        url = reverse('admin_doc_view', kwargs={'view': 'admin.site.each_context'})\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        response_content = response.content.decode('utf-8')\n        # Check if the expected view name is present in the response\n        self.assertIn('admin.site.each_context', response_content)\n"], "sample_558": ["    def test_image_grid_share_all(self):\n        fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(6, 6),\n                                 gridspec_kw={'hspace': 0.05, 'wspace': 0.05})\n        ax = ImageGrid(fig, rect=[0.1, 0.1, 0.8, 0.8],\n                       nrows_ncols=(2, 2), share_all=True)\n        for i in range(4):\n            ax[i].imshow(np.random.rand(10, 10))\n\n        plt.show()\n", "    def test_cbar_location_single(self):\n        fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(6, 6),\n                                 gridspec_kw={'width_ratios': [1, 2]})\n        img = axes[0, 0].imshow(np.random.rand(10, 10))\n\n        grid = ImageGrid(fig, \n                         rect=axes[0, :2].get_position(),\n                         nrows_ncols=(2, 2),\n                         ngrids=2,\n                         direction=\"row\", cbar_mode=\"single\", \n                         cbar_location=\"right\")\n        cbar = grid.cbar_axes[0].colorbar(img, extend='both')\n\n        fig.canvas.draw()\n        bbox = cbar.get_position()\n        assert bbox.x0 == 0.8\n        assert bbox.y0 == 0.1\n\n\n", "    def test_colorbar_location(self):\n        fig, axes = plt.subplots(2, 2, figsize=(8, 6), \n                                 gridspec_kw={'width_ratios': [1, 1]})\n\n        axes[0, 0].imshow(np.random.rand(10, 10))\n        axes[0, 1].imshow(np.random.rand(10, 10))\n        axes[1, 0].imshow(np.random.rand(10, 10))\n        axes[1, 1].imshow(np.random.rand(10, 10))\n\n        grid = ImageGrid(fig, 111, nrows_ncols=[2, 2],\n                         cbar_mode=\"edge\", cbar_location=\"right\")\n\n        # Check that the colorbars are indeed placed on the right edge.\n\n        for ax in grid.axes_all:\n            if ax.cax is not None:\n                assert ax.cax.get_position().x1 > 0.95\n\n        plt.show()\n", "    def test_imagegrid_colorbar_single_orientation(self):\n        fig, axes = plt.subplots(nrows=1, ncols=4,\n                                  figsize=(6, 2))\n        grid = ImageGrid(fig, 111, nrows_ncols=(1, 4),\n                         cbar_mode=\"single\", cbar_location=\"right\")\n        for ax in grid.axes_all:\n            ax.imshow(np.random.rand(5, 5))\n\n        # check if single colorbar for all axes\n        assert len(grid.cbar_axes) == 1 \n\n\n\n", "    def test_set_cbar_pad(self):\n        fig, ax = plt.subplots(1, 1)\n        grid = ImageGrid(fig, 111, nrows_ncols=(1, 1), axes_pad=0.05)\n        grid.set_cbar_pad(0.1)\n        assert_array_almost_equal(grid._colorbar_pad.fixed_size, 0.1)\n\n        grid.set_cbar_pad(None)\n        assert grid._colorbar_pad is None\n\n\n\n"], "sample_67": ["    def test_custom_error_messages(self):\n        form = CustomErrorMessageForm(data={'name1': 'invalid'})\n        self.assertEqual(form.errors['name1'], ['Form custom error message.'])\n\n", "    def setUp(self):\n        self.author = Author.objects.create(name='Jane Doe')\n        self.article = Article.objects.create(title='My Article',\n                                              author=self.author, status=ArticleStatus.objects.get(name='draft'))\n", "    def test_custom_error_message(self):\n        form = CustomErrorMessageForm(data={'name1': 'invalid'})\n        self.assertEqual(form.errors['name1'], ['Form custom error message.'])\n\n\n\n", "    def test_custom_error_message(self):\n        form = CustomErrorMessageForm(data={'name1': 'invalid'})\n        self.assertEqual(form.errors['name1'][0], 'Form custom error message.')\n\n\n\n", "        def test_custom_error_message(self):\n            form = CustomErrorMessageForm({'name1': ''})\n            self.assertEqual(len(form.errors), 1)\n            self.assertEqual(form.errors['name1'], ['Form custom error message.'])\n\n"], "sample_860": ["    def test_check_sample_weight_sparse_array(self):\n        X = sp.csr_matrix([[1, 2], [3, 4]])\n        sample_weight = sp.csr_matrix([0.5, 1.0])\n        with assert_no_warnings():\n            result = _check_sample_weight(sample_weight, X)\n        assert_allclose_dense_sparse(result, sample_weight)\n", "    def test_check_sample_weight_missing_sample(self):\n        X = np.array([[1, 2], [3, 4]])\n        with assert_warns(Warning,\n                          \"Sample weights not provided. \"\n                          \"Setting them to all ones.\"):\n            _check_sample_weight(None, X)\n        ", "    def test_check_sample_weight_array(dtype):\n        X = np.array([[1, 2], [3, 4]])\n        sample_weight = np.array([0.5, 1.0], dtype=dtype)\n        result = _check_sample_weight(sample_weight, X, dtype=dtype)\n        assert result.dtype == dtype\n        assert np.allclose(result, sample_weight)\n", "    def test_check_sample_weight_sparse(self):\n        X = sp.csr_matrix([[1, 2], [3, 4]])\n        sample_weight = sp.csr_matrix([0.5, 1.0])\n        # valid sparse weights, shape should be checked\n\n        result = _check_sample_weight(sample_weight, X)\n        assert_allclose_dense_sparse(result, sample_weight)\n        assert result.shape == (2,)\n\n        # invalid sparse weights, should raise ValueError\n        sample_weight = sp.csr_matrix([0.5, 1.0, 2.0])\n        with pytest.raises(ValueError):\n            _check_sample_weight(sample_weight, X)\n\n        # valid sparse weights, shape should be checked\n\n        result = _check_sample_weight(sample_weight, X)\n        assert_allclose_dense_sparse(result, sample_weight)\n        assert result.shape == (2,)\n", "    def test_allclose_dense_sparse_mixed(self):\n        with pytest.raises(ValueError):\n            _allclose_dense_sparse(\n                np.array([1, 2, 3]), sp.csr_matrix([1, 2, 3]),\n            )\n"], "sample_882": ["    def test_early_stopping_classification(self):\n        # Test the early stopping mechanism for classification\n        for X, y in classification_datasets:\n            mlp = MLPClassifier(\n                hidden_layer_sizes=(100,),\n                max_iter=100,\n                early_stopping=True,\n                validation_fraction=0.2,\n                solver='adam',\n                random_state=0\n            )\n            mlp.fit(X, y)\n            assert mlp.best_validation_score_ is not None\n            assert mlp.n_iter_ < mlp.max_iter\n\n\n\n            # Test that early stopping is not triggered if validation score doesn't improve\n            mlp = MLPClassifier(\n                hidden_layer_sizes=(100,),\n                max_iter=100,\n                early_stopping=True,\n                validation_fraction=0.2,\n                solver='adam',\n                random_state=0,\n            )\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n                mlp.fit(X, y)\n            assert mlp.n_iter_ == mlp.max_iter\n", "    def test_activation_functions(\n        self, activation, classification_datasets, regressors", "    def test_partial_fit_with_different_solvers(\n        self,\n        activation,\n        solver,\n        random_state,\n        classification_datasets,", "    def test_early_stopping_with_validation_fraction(self):\n        # Test early stopping with validation fraction\n        mlp = MLPClassifier(\n            max_iter=100, random_state=42, early_stopping=True, validation_fraction=0.2\n        )\n        mlp.fit(X_digits[:150], y_digits[:150])\n        assert mlp.n_iter_ < 100\n", "    def test_mlp_classification_alpha(\n        self, activation, solver, alpha, random_state"], "sample_803": ["    def test_roc_curve_drop_intermediate(drop):\n\n        y_true, y_pred, probas_pred = make_prediction()\n        fpr, tpr, thresholds = roc_curve(y_true, probas_pred,\n                                           drop_intermediate=drop)\n        assert_array_almost_equal(fpr, _partial_roc_auc_score(\n            y_true, probas_pred, fpr[-1]), 1e-5)\n", "compilation error", "    def test_roc_curve_drop_intermediate(binary, drop):\n        y_true, y_pred, _ = make_prediction(binary=binary)\n        fpr, tpr, thresholds = roc_curve(y_true, y_pred, drop_intermediate=drop)\n        assert_array_almost_equal(fpr, np.arange(len(fpr)), decimal=6)\n        assert_array_almost_equal(tpr, np.arange(len(tpr)), decimal=6)\n\n\n", "compilation error", "    def test_roc_curve_with_multiple_thresholds(drop):\n        y_true, y_pred, _ = make_prediction(binary=True)\n        fpr, tpr, thresholds = roc_curve(y_true, y_pred, drop_intermediate=drop)\n\n        # Check that the number of thresholds is consistent with the\n        # number of samples in the test set.\n        assert len(thresholds) == len(y_true)\n\n        # Check that the first threshold is equal to 1 and the last\n        # threshold is equal to 0\n        assert np.isclose(thresholds[0], 1.0)\n        assert np.isclose(thresholds[-1], 0.0)\n\n        # Check that fpr and tpr are sorted\n        assert (np.all(fpr[1:] >= fpr[:-1])) and (np.all(tpr[1:] >= tpr[:-1]))\n\n        # Check that the area under the ROC curve using the calculated\n        # FPR and TPR values matches the area obtained using\n        # `roc_auc_score`\n\n\n\n"], "sample_630": ["    def test_generated_file(generated_file):\n        expected_file = open(\n            os.path.join(os.path.dirname(__file__), \"expected\",\n                         generated_file), \"r\"\n        )\n        with open(generated_file, \"r\") as actual_file:\n            diff = unified_diff(\n                expected_file.readlines(),\n                actual_file.readlines(),\n                fromfile=generated_file,\n                tofile=\"expected\"\n            )\n            assert not diff, f\"diff between {generated_file} and expected:\\n{''.join(diff)}\"\n", "    def test_dot_file_content(generated_file):\n        with open(generated_file, \"r\") as f:\n            expected_lines = _file_lines(\n                os.path.join(os.path.dirname(__file__), \"expected\", generated_file)\n            )\n            actual_lines = f.readlines()\n        diff = unified_diff(expected_lines, actual_lines, fromfile=\"expected\", tofile=generated_file)\n        assert not diff\n\n\n\n\n", "compilation error", "def test_generated_dot_content(generated_file):\n    expected = _file_lines(os.path.join(os.path.dirname(__file__), \"data\", \"expected\", generated_file[:-4] + \".txt\"))\n    actual = _file_lines(generated_file)\n    diff = unified_diff(expected, actual, fromfile=generated_file, tofile=\"expected\")\n    assert not diff, '\\n'.join(diff)\n", "compilation error"], "sample_493": ["compilation error", "    def test_case_expressions(self):\n        from django.db.models.expressions import Case, When, Otherwise\n\n        # Test with different types of expressions in the When clauses\n        expected_result = [\n            (\n                \"Peter Norvig\",\n                timezone.make_aware(datetime.datetime(1995, 1, 15, 0, 0)),\n                1991,\n            ),\n            (\n                \"Brad Dayley\",\n                timezone.make_aware(datetime.datetime(2008, 6, 23, 0, 0)),\n                2008,\n            ),\n        ]\n        results = Author.objects.annotate(\n            pubyear=Case(\n                When(book__pubdate__year=1995, then=F(\"book__pubdate__year\")),\n                When(book__pubdate__year=2008, then=F(\"book__pubdate__year\")),\n                Otherwise(F(\"book__pubdate__year\")),\n            )\n        ).values(\n            \"name\",\n            \"pubyear\",\n        ).order_by(\"name\").values_list(\"name\", \"pubyear\", flat=True)\n        self.assertEqual(list(results), expected_result)\n\n\n        # Test with subquery in the When clauses\n        expected_result = [\n            (\n                \"Peter Norvig\",\n                timezone.make_aware(datetime.datetime(1995, 1, 15, 0, 0)),\n                1991,\n            ),\n            (\n                \"Brad Dayley\",\n                timezone.make_aware(datetime.datetime(2008, 6, 23, 0, 0)),\n                2008,\n            ),\n        ]\n        results = Author.objects.annotate(\n            pubyear=Case(\n                When(\n                    book__pubdate__year=Subquery(\n                        Book.objects.values(\"pubdate__year\").distinct()\n                    ),\n                    then=F(\"book__pubdate__year\"),\n                ),\n                When(\n                    book__pubdate__year=Subquery(\n                        Book.objects.values(\"pubdate__year\").distinct()\n", "    def test_aggregate_with_multiple_fields(self):\n        with self.assertNumQueries(1):\n            results = Book.objects.values(\n                \"publisher__name\", \"rating\", \"price\"\n            ).annotate(\n                average_rating=Avg(\"rating\"),\n                total_price=Sum(\"price\"),\n            ).order_by(\"publisher__name\").all()\n            self.assertEqual(len(results), 4)\n\n        with self.assertNumQueries(2):\n            results = Book.objects.values(\n                \"publisher__name\",\n                \"rating\",\n                \"price\",\n            ).annotate(\n                average_rating=Avg(\"rating\"),\n                total_price=Sum(\"price\"),\n            ).order_by(\"publisher__name\").annotate(\n                max_rating=Max(\"rating\")\n            ).all()\n            self.assertEqual(len(results), 4)\n\n\n", "     def test_aggregates_with_extra_values(self):\n        with self.assertNumQueries(1) as queries:\n            result = Book.objects.annotate(\n                total_price=Sum(\"price\")\n            ).values(\"total_price\", \"name\")\n        self.assertEqual(len(result), 6)\n        self.assertIn(\n            {\"total_price\": Decimal(\"112.78\"), \"name\": \"The Definitive Guide to Django: Web Development Done Right\"}, result\n        )\n", "compilation error"], "sample_563": ["    def test_vpacker(self):\n        fig, ax = plt.subplots(1, 1)\n\n        # Test creating a VPacker with multiple children\n        vpack = VPacker(children=[\n            Text(\"Child 1\", fontsize=12),\n            Text(\"Child 2\", fontsize=14),\n            Text(\"Child 3\", fontsize=16),\n        ])\n\n        # Set some properties of the VPacker\n        vpack.set_padding(0.1)\n        vpack.set_alignment('center')\n\n        # Add the VPacker to the Axes\n        ax.add_artist(vpack)\n        plt.draw()\n\n        # Get the bounding box of the VPacker\n        bbox = vpack.get_window_extent()\n\n        # Check that the bounding box is as expected\n", "    def test_offset_box_padding(self):\n        fig, ax = plt.subplots()\n        box = OffsetBox()\n        box.add_artist(mpatches.Rectangle((0, 0), 1, 1, color='red'))\n        ax.add_artist(box)\n\n        bbox = box.get_window_extent(renderer=fig.canvas.get_renderer())\n\n        # Test with default padding\n        assert bbox.width == 1.0\n        assert bbox.height == 1.0\n\n        box.set_pad(0.2)\n        bbox = box.get_window_extent(renderer=fig.canvas.get_renderer())\n\n        # Test with padding applied\n        assert bbox.width > 1.0\n        assert bbox.height > 1.0\n\n", "    def test_offsetbox_padding(self, test_image):\n        fig, ax = plt.subplots()\n        bbox = OffsetBox(\n            DrawingArea(\n                width=10, height=10),\n            loc='center',\n            pad=1,\n            borderpad=2\n        )\n        ax.add_artist(bbox)\n\n        test_image = test_image(\n            bbox.get_window_extent\n        )\n\n\n        bbox = OffsetBox(\n            DrawingArea(width=10, height=10),\n            loc='center',\n            pad=1,\n            borderpad=2,\n            frameon=False\n        )\n        ax.add_artist(bbox)\n", "    def test_bbox_to_anchor_outside_axes(self):\n        fig, ax = plt.subplots()\n        bbox_to_anchor = (0.0, 1.0)\n        box = AnchoredOffsetbox(\n            bbox_to_anchor=bbox_to_anchor, loc='lower right')\n        ax.add_artist(box)\n        plt.draw()\n\n\n\n", "    def test_offsetbox_child_alignment(self):\n        # Test alignment of child elements within an OffsetBox\n\n        fig, ax = plt.subplots()\n        ax.axis('off')\n\n        bbox = OffsetBox(None, loc='lower left',\n                         child=mpatches.Rectangle((0, 0), 1, 1, facecolor='red'),\n                         pad=0.2)\n        ax.add_artist(bbox)\n\n        bbox2 = OffsetBox(None, loc='upper right',\n                          child=mpatches.Rectangle((0, 0), 1, 1, facecolor='blue'),\n                          pad=0.2)\n        ax.add_artist(bbox2)\n\n        bbox3 = OffsetBox(None, loc='center',\n                          child=mpatches.Rectangle((0, 0), 1, 1, facecolor='green'),\n                          pad=0.2)\n        ax.add_artist(bbox3)\n\n\n        plt.draw()\n        renderer = ax.figure.canvas.get_renderer()\n        bbox_rect = bbox.get_window_extent(renderer)\n        bbox2_rect = bbox2.get_window_extent(renderer)\n        bbox3_rect = bbox3.get_window_extent(renderer)\n\n        assert bbox_rect.p0[0] == 0.1\n        assert bbox_rect.p0[1] == 0.1\n        assert bbox2_rect.p0[0] == 0.9\n        assert bbox2_rect.p0[1] == 0.9\n        assert bbox3_rect.p0[0] == 0.5\n        assert bbox3_rect.p0[1] == 0.5\n        \n        plt.close(fig)\n"], "sample_855": ["    def test_dummy_regressor_quantile_with_sample_weight(self):\n        X = np.array([[0], [1], [2], [3]])\n        y = np.array([1, 2, 3, 4])\n        sample_weight = np.array([0.1, 0.2, 0.3, 0.4])\n\n        reg = DummyRegressor(strategy=\"quantile\", quantile=0.75)\n        reg.fit(X, y, sample_weight=sample_weight)\n        y_pred = reg.predict(X)\n        y_pred_learn = reg.predict(X, return_std=True)[0]\n\n        # Check if the predicted values are close to the 75th percentile\n        for k in range(y.shape[0]):\n            assert_array_almost_equal(_weighted_percentile(y, sample_weight,\n                    percentile=75), y_pred[k])\n            assert_array_almost_equal(_weighted_percentile(y, sample_weight,\n                    percentile=75), y_pred_learn[k])\n\n\n\n", "def test_dummy_regressor_quantile_with_weights():\n    # This test checks if the quantile estimator properly handles sample weights.\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([[7, 8], [9, 10], [11, 12]])\n    sample_weight = np.array([0.1, 0.5, 0.4])\n    clf = DummyRegressor(strategy=\"quantile\", quantile=0.75)\n    clf.fit(X, y, sample_weight=sample_weight)\n    y_pred = clf.predict(X)\n    # Compare with weighted percentile calculation\n    for k in range(y.shape[1]):\n        weighted_quantile = _weighted_percentile(y[:, k], sample_weight,\n                                                percentile=75)\n        assert_array_almost_equal(weighted_quantile, y_pred[:, k])\n\n\n", "compilation error", "    def test_dummy_classifier_sparse_output(self):\n        X = np.array([[1, 2, 3],\n                      [4, 5, 6],\n                      [7, 8, 9]])\n        y = sp.csr_matrix([[1], [2], [3]])\n        est = DummyClassifier(strategy=\"most_frequent\")\n        est.fit(X, y)\n        y_pred = est.predict(X)\n        assert isinstance(y_pred, sp.csr_matrix)\n\n        assert_array_equal(y_pred.toarray(),\n                          np.array([[1], [2], [3]]))\n\n\n\n", "compilation error"], "sample_616": ["    def test_add(array, array_y, expected):\n        assert_identical(array + array_y, expected)\n\n", "    def test_apply_ufunc_empty_arrays():\n        x = xr.DataArray([], dims=['x'])\n        y = xr.DataArray([], dims=['x'])\n        result = apply_ufunc(lambda x, y: x + y, x, y)\n        assert result.size == 0\n\n\n", "    def test_unified_dims_sizes():\n        # Test unified_dim_sizes with nested DataArrays and DataSets\n        da1 = xr.DataArray(np.arange(12).reshape((3, 4)), dims=['lat', 'lon'])\n        da2 = xr.DataArray(np.arange(12).reshape((4, 3)), dims=['lon', 'lat'])\n        ds1 = xr.Dataset({\n            'a': da1,\n            'b': xr.DataArray(np.arange(12).reshape((3, 4)), dims=['lat', 'lon'])\n        })\n        ds2 = xr.Dataset({\n            'c': da2,\n            'd': xr.DataArray(np.arange(12).reshape((4, 3)), dims=['lon', 'lat'])\n        })\n        \n        # Test with different chunk sizes\n        da1_chunked = da1.chunk({'lat': 2, 'lon': 2})\n        da2_chunked = da2.chunk({'lon': 2, 'lat': 2})\n        ds1_chunked = ds1.chunk({'a': {'lat': 2, 'lon': 2}, 'b': {'lat': 2, 'lon': 2}})\n        ds2_chunked = ds2.chunk({'c': {'lon': 2, 'lat': 2}, 'd': {'lon': 2, 'lat': 2}})\n        \n        # Test unified_dim_sizes with different data types\n        da1_int = xr.DataArray(np.arange(12).reshape((3, 4)).astype(int), dims=['lat', 'lon'])\n        da2_float = xr.DataArray(np.arange(12).reshape((4, 3)).astype(float), dims=['lon', 'lat'])\n        \n        # Test cases\n        assert unified_dim_sizes(da1, da2) == {'lat': 3, 'lon': 4}\n        assert unified_dim_sizes(da1_chunked, da2_chunked) == {'lat': 2, 'lon': 2}\n        assert unified_dim_sizes(ds1, ds2) == {'lat': 3, 'lon':", "    def test_idxminmax_single_dim(func):\n        da = xr.DataArray(\n            np.array([1, 2, 3, 4, 5, 6]), dims=\"x\", name=\"test\"\n        )\n        result = _calc_idxminmax(array=da, func=func, dim=\"x\")\n        expected = xr.DataArray(\n            np.array([0, 5]), dims=\"x\", name=\"test\"\n        )\n        assert_identical(result.values, expected.values)\n", "    def test_apply_ufunc_broadcast_dimensions(x, y, dim, expected_result):\n        result = apply_ufunc(\n            operator.mul,\n            x,\n            y,\n            input_core_dims=[[dim], [dim]],\n            output_core_dims=[dim],\n        )\n        assert_identical(result, expected_result)\n\n\n\n"], "sample_562": ["    def test_dash_joinstyle(self, image_comparison_data):\n        fig, ax = plt.subplots()\n        x = np.arange(0, 10, 0.1)\n        for dash_joinstyle in ['miter', 'round', 'bevel']:\n            line, = ax.plot(x, np.sin(x),\n                             linestyle='--',\n                             dash_joinstyle=dash_joinstyle)\n            ax.set_xlim(0, 10)\n            ax.set_ylim(-1.5, 1.5)\n\n            plt.draw()\n            # Give the figure a chance to redraw with the dash_joinstyle\n            # change\n            time.sleep(0.001)\n            image_comparison_data.append(line.figure.canvas.tostring_rgb())\n        plt.close(fig)\n\n\n\n", "    def test_marker_rendering(self, marker):\n        fig, ax = plt.subplots()\n        line = mlines.Line2D([0, 1, 2], [0, 1, 2], marker=marker)\n        ax.add_line(line)\n        ax.autoscale_view()\n        plt.show()\n\n\n", "    def test_line2d_dashes_cycle(self):\n        fig, ax = plt.subplots()\n\n        # Set up a cyclic pattern with 10 dashes\n        dash_pattern = [(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)] * 10\n\n        # Generate a series of lines with different dash patterns\n        for i in range(10):\n            line = ax.plot([i, i + 1], [0, 1], linestyle=dash_pattern[i], marker='o')[0]\n            line.set_color(f\"C{i}\")\n\n        ax.set_xlim(0, 10)\n        ax.set_ylim(0, 1)\n        ax.set_xlabel(\"x\")\n        ax.set_ylabel(\"y\")\n        \n        \n        # Compare the image with the reference image\n        compare_images(plt.gcf(), \"test_line2d_dashes_cycle.png\")\n", "    def test_set_invalid_xdata(self):\n        fig, ax = plt.subplots()\n        line, = ax.plot([1, 2, 3], [4, 5, 6])\n        with pytest.raises(TypeError):\n            line.set_xdata('test')\n", "    def test_marker_size(self):\n        fig, ax = plt.subplots()\n        sizes = [1, 2, 3, 4, 5]\n        for i, s in enumerate(sizes):\n            line, = ax.plot([i], [i], marker='o', markersize=s)\n            line.set_color(f'C{i}')\n        ax.set_xlim(-1, 5)\n        ax.set_ylim(-1, 5)\n        ax.set_aspect('equal')\n\n"], "sample_613": ["    def test_first(dataset):\n        grouped = dataset.groupby(\"x\")\n        expected = xr.Dataset({\"foo\": xr.DataArray(\n            np.array([[[0.123456, 0.123456], [0.123456, 0.123456]],\n                      [[0.123456, 0.123456], [0.123456, 0.123456]],\n                      [[0.123456, 0.123456], [0.123456, 0.123456]]),\n            coords={\"y\": [1, 2, 3, 4], \"z\": [1, 2]}, dims=(\"z\", \"y\")\n        ) , \"boo\": xr.DataArray(\n            np.array([\n                [[\"f\", \"g\", \"h\", \"j\"], [\"f\", \"g\", \"h\", \"j\"]],\n                [[\"f\", \"g\", \"h\", \"j\"], [\"f\", \"g\", \"h\", \"j\"]],\n                [[\"f\", \"g\", \"h\", \"j\"], [\"f\", \"g\", \"h\", \"j\"]]\n            ]),\n            coords={\"z\": [1, 2], \"y\": [1, 2, 3, 4]}, dims=(\"z\", \"y\")\n        )})\n        assert_identical(grouped.first(), expected)\n", "    def test_groupby_first(array, dataset):\n        grouped = array.groupby(\"x\").first()\n        expected = xr.DataArray(\n            data=np.array([array[0, :, :].mean(axis=1),\n                           array[1, :, :].mean(axis=1),\n                           array[2, :, :].mean(axis=1)]),\n            coords={\"y\": array.y, \"z\": array.z},\n            dims=(\"y\", \"z\"),\n        )\n        assert_identical(grouped, expected)\n    ", "    def test_groupby_apply_transform(array):\n            return arr * 2\n\n        grouped = array.groupby(array.dims[0])\n        result = grouped.map(transform)\n\n        expected = array * 2\n        assert_allclose(result, expected)\n\n\n", "    def test_groupby_first_last(array):\n        groups = array.groupby(\"x\").first()\n        assert_allclose(groups.values, array.isel(x=[0]).values)\n        groups = array.groupby(\"x\").last()\n        assert_allclose(groups.values, array.isel(x=[2]).values)\n", "    def test_quantile_with_keepattrs(dataset):\n        ds = dataset.groupby(\"x\").quantile(0.5, keep_attrs=True)\n        assert ds.attrs == dataset.attrs\n"], "sample_886": ["compilation error", "    def test_safe_set_output_missing_method(self):\n        class Estimator:\n            pass\n\n        with pytest.raises(ValueError):\n            _safe_set_output(Estimator(), transform=\"pandas\")\n", "compilation error", "            def transform(self, X):\n                return np.arange(X.shape[0] * X.shape[1]).reshape(X.shape)\n", "compilation error"], "sample_192": ["    def test_formset_factory_limits(self):\n        with self.assertRaises(ValueError):\n            formset_factory(Choice, max_num=10, absolute_max=5)\n\n        with self.assertRaises(ValueError):\n            formset_factory(Choice, min_num=10, absolute_max=5)\n", "    def test_initial_form_count(self):\n        formset = self.make_choiceformset(\n            formset_data=[('A', 1), ('B', 2), ('C', 3)], initial_forms=2\n        )\n        self.assertEqual(formset.initial_form_count(), 2)\n        self.assertEqual(len(formset.initial_forms), 2)\n", "    def test_extra_forms_default(self):\n        formset = ChoiceFormSet()\n        self.assertEqual(len(formset.forms), 1)\n        self.assertEqual(len(formset.initial_forms), 0)\n", "    def test_extra_forms_default_to_empty(self):\n        formset_data = [\n            ('Milk', 2),\n            ('Water', 5),\n        ]\n        formset = self.make_choiceformset(formset_data, extra=2)\n\n        # Extra forms should be empty by default.\n        self.assertEqual(len(formset.forms), len(formset_data) + 2)\n        for i in range(len(formset_data), len(formset.forms)):\n            form = formset.forms[i]\n            self.assertTrue(form.is_bound)\n            self.assertFalse(form.has_changed())\n", "    def test_formset_factory_error_handling(self):\n        with self.assertRaises(ValueError):\n            formset_factory(Choice, absolute_max=10, max_num=20)\n\n        with self.assertRaises(ValueError):\n            formset_factory(Choice, min_num=20, absolute_max=10)\n\n        with self.assertRaises(ValueError):\n            formset_factory(Choice, validate_max=True, max_num=None)\n\n"], "sample_996": ["compilation error", "compilation error", "compilation error", "    def test_reverse_order_product():\n        P = Product(x, (x, 1, n))\n        Pr = P.reverse_order(x)\n        assert Pr == Product(1/x, (x, n + 1, 0))\n        assert P.doit() == rf(1, n + 1)\n        assert simplify(Pr.doit()) == 1/rf(1, n + 1)\n", "compilation error"], "sample_800": ["    def fit(self, X, y):\n        return self\n", "    def __init__(self):\n        super().__init__()\n", "    def __init__(self, p=0):\n        self.original_p = p\n", "    def __init__(self):\n        pass\n", "    def __init__(self, param1=1, param2=2):\n        self.param1 = param1\n        self.param2 = param2\n"], "sample_745": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1184": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_923": ["compilation error", "compilation error", "compilation error", "compilation error", "    def check_enum(name, input, idDict, output):\n        _check(name, input, idDict, output, key='enum_', asTextOutput=output)\n\n\n"], "sample_684": ["def test_repr_locals():\n    rep = ReprLocals(\n        [\n            (\"x\", 1),\n            (\"y\", 2),\n            (\"_internal_var\", None),\n        ]\n    )\n    assert str(rep) == \"x = 1\\ny = 2\\n\"\n\n", "    def test_getfslineno_no_source_code(self):\n        class MyThing:\n            pass\n\n        path, lineno = getfslineno(MyThing)\n        assert path == \"\"\n        assert lineno == -1\n", "    def test_repr_traceback_native_native_style(tmpdir):\n        # Test native style representation\n        source_code = \"\"\"", "    def test_repr_tracebacks_with_recursion(capsys):\n        @pytest.mark.parametrize(\n            \"pattern\",\n            [\n                re.compile(r\"my_rekursion_function\"),\n                lambda m: len(m.group(0)) > 10,\n            ],\n        )\n            try:\n                    if count > 5:\n                        return\n                    print(\"Count:\", count)\n                    my_rekursion_function(count + 1)\n                my_rekursion_function(0)\n            except RecursionError as e:\n                tb = ExceptionInfo(e.__context__).traceback\n                repr_traceback = FormattedExcinfo().repr_traceback(ExceptionInfo(e))\n                captured = capsys.readouterr()\n                assert pattern.search(repr_traceback.toterminal()) is not None\n                assert \"!!! Recursion detected\" in captured.out\n            else:\n                assert False, \"Recursion Error not raised!\"\n\n", "    def test_repr_traceback_recursion(monkeypatch):\n        class RecursionErrorFake(Exception):\n            pass\n\n            return (\"/path/to/fake/source.py\", 10)\n\n        monkeypatch.setattr(sys, \"findsource\", fake_findsource)\n\n        e = RecursionErrorFake()\n        excinfo = ExceptionInfo((RecursionErrorFake, e, e.__traceback__))\n        traceback = excinfo.traceback\n        repr_traceback = FormattedExcinfo().repr_traceback(excinfo)\n\n        assert len(repr_traceback.reprentries) == 2\n        assert repr_traceback.reprentries[0].style == \"native\"\n\n\n\n"], "sample_553": ["    def test_grab_frame_kwargs(self, anim, request):\n        writer = NullMovieWriter()\n        anim.save('test.mp4', writer=writer)\n        assert writer.fig is anim.fig\n        assert writer.outfile == 'test.mp4'\n        assert writer.dpi == 100  # Default values\n        assert writer.args == ()\n        assert writer._count == anim.save_count\n        assert writer.savefig_kwargs == {\n            'frame': None,\n            'format': 'png',\n            'dpi': 100,\n            'bbox_inches': 'tight',\n        }\n\n\n", "def test_timed_animation_blit(anim):\n    from matplotlib.patches import Rectangle\n    fig, ax = plt.subplots()\n    rect = Rectangle((0, 0), 2, 2, facecolor='red')\n    ax.add_patch(rect)\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n\n        rect.set_xy((i, i))\n        ax.draw_artist(rect)\n        return [rect]\n\n    ani_blit = animation.TimedAnimation(fig, animate, interval=10, blit=True)\n\n    # Simulate 5 frames\n    for i in range(5):\n        ani_blit.tick()\n\n\n\n    assert len(ani_blit._drawn_artists) == 1\n    assert ani_blit._drawn_artists[0] is rect\n\n\n\n    # Test if blitting works correctly\n    fig.canvas.draw()\n    rect_bbox = rect.get_bbox()\n    for i in range(5):\n        ani_blit.tick()\n        fig.canvas.draw()\n        new_rect_bbox = rect.get_bbox()\n        assert new_rect_bbox != rect_bbox\n        rect_bbox = new_rect_bbox\n", "    def test_grab_frame_kwargs(self, anim):\n        # Test that grab_frame() correctly handles keyword arguments.\n\n        writer = NullMovieWriter()\n        anim.save(\"test.mp4\", writer=writer)\n        assert writer._count > 0, \"grab_frame() wasn't called\"\n        assert writer.savefig_kwargs == {\"bbox_inches\": \"tight\"}\n\n\n\n", "    def test_blit(self, anim, blit, klass):\n        if blit and klass is animation.FuncAnimation:\n            # This test checks that blitting works correctly\n            # for FuncAnimation.\n            # It's known not to work correctly for TimedAnimation\n            # or FuncAnimation without a specific init_func\n            ax = anim.fig.axes[0]\n            initial_artists = ax.get_children()\n\n            # Save the initial artists before starting the animation.\n            # This avoids checking individual artists for removal\n            # which is more fragile.\n            anim.event_source.start()\n            anim._fig.canvas.draw()\n            anim.event_source.stop()\n            initial_artists = ax.get_children()\n\n            anim = klass(fig=anim.fig, func=anim._func,\n                         init_func=anim._init_func,\n                         blit=blit, interval=50, repeat_delay=0,\n                         repeat=False, frames=1, *anim._args)\n\n            # Run the animation once.\n\n            anim.event_source.start()\n            anim._fig.canvas.draw()\n            anim.event_source.stop()\n\n            final_artists = ax.get_children()\n            # Blitting should only modify specific artists,\n            # so the number of artists should not change\n            assert len(initial_artists) == len(final_artists)\n", "def test_func_animation_blit(anim):\n    # Test blit functionality in FuncAnimation\n    from matplotlib.animation import FuncAnimation\n\n    fig = anim.fig\n    ax = fig.gca()\n\n    class Artist:\n            self.data = []\n            self.data = args\n            pass\n            return f\"Artist(data = {self.data})\"\n\n        artist = Artist([frame]*2)\n        return artist,\n\n    # Animate with blit and verify that artists are blitted correctly\n    with mpl.backend_tools.capture_output(capture_stdout=True, capture_stderr=True) as captured_output:\n\n        ani = FuncAnimation(fig, func, interval=10, blit=True)\n        ani.save(\"test_blit.mp4\", writer=\"ffmpeg\", codec='libx264')\n    \n    _api.warn_deprecated(\n        \"Using blitting in a FuncAnimation 'save' call to a file format\"\n        \" that does not support blitting is deprecated since v3.7 and \"\n        \"will raise a warning in v4.0. \"\n        \"Either disable blitting when saving to file or switch to a \"\n        \"format that supports blit.  For example: \"\n\n        \".. code-block:: python\n\n          ani.save(\"test_blit.mp4\", writer='imagemagick')\"\n    )\n    assert captured_output.captured_stdout.strip() == \"Saving 10 frames to test_blit.mp4\\n\"\n\n    # Validate that the saved file exists and has the expected contents\n    assert os.path.exists(\"test_blit.mp4\")\n    # The contents would depend on the format and codec used\n\n\n\n"], "sample_450": ["    def test_get_admin_log_with_user(self):\n        response = self.client.get(reverse(\"admin:index\"))\n        response.render()  # Render the response to populate the context\n        html = response.content.decode()\n\n        # Parse the response using django template library\n        parsed_html = template.Template(html).render()\n\n        # Using a template node to perform assertions\n        self.assertIn(\n            f\"admin_log\", parsed_html,\n            \"The template variable 'admin_log' is not defined in the response.\"\n        )\n", "    def test_get_admin_log_limits_results(self):\n        response = self.client.get(reverse(\"admin:index\"))\n        response.render()\n        context = response.context\n        \n        admin_log = context[\"log_entries\"]\n\n        # No user specified, so get all entries\n        with self.subTest(\"No user specified\"):\n            \n            self.assertGreaterEqual(len(admin_log), 1) \n\n        # Get 5 entries, should have 5 entries\n        with self.subTest(\"Limit of 5\"):\n            response = self.client.get(reverse(\"admin:index\"), {\"limit\": 5})\n            response.render()\n            context = response.context\n            admin_log = context[\"log_entries\"]\n            self.assertEqual(len(admin_log), 5)\n\n        # Exceed the limit, should be limited to specified number\n        with self.subTest(\"Limit of 1, exceeding limit\"):\n            response = self.client.get(reverse(\"admin:index\"), {\"limit\": 1})\n            response.render()\n            context = response.context\n            admin_log = context[\"log_entries\"]\n            self.assertEqual(len(admin_log), 1) \n\n", "    def test_get_admin_log_with_limit_and_user(self):\n        response = self.client.get(reverse(\"admin:index\"))\n        response.render()\n        context = response.context\n\n        # Check that 'log_entries' is available in the context\n        self.assertIn(\"log_entries\", context)\n        entries = context[\"log_entries\"]\n\n        # Render the template with 'get_admin_log'\n        template_string = \"{{ get_admin_log 5 as latest_logs for_user %s }}\" % self.user.pk\n        result = template.Template(template_string).render(context)\n\n        # Assert that the result exists and matches the expected length\n        self.assertIn(\"latest_logs\", locals())\n        self.assertEqual(len(context[\"latest_logs\"]), 5)\n\n        # Assert that the entries are filtered by the user\n        latest_logs = context[\"latest_logs\"]\n        for entry in latest_logs:\n            self.assertEqual(entry.user.pk, self.user.pk) \n", "    def test_get_admin_log_tag(self):\n        response = self.client.get(reverse('admin:index'))\n        self.assertContains(response, 'Title')\n        response = self.client.get(reverse('admin:article_changelist'))\n        self.assertContains(response, 'Title')\n\n        # Test the template tag with all arguments\n        response = self.client.get(reverse('admin:article_changelist') + '?get_admin_log=10 as admin_log for_user=super')\n\n\n\n", "    def test_get_admin_log_with_user(self):\n        response = self.client.get(reverse(\"admin_log_view\"))\n        self.assertEqual(response.status_code, 200)\n        content = response.content.decode(\"utf-8\")\n\n        # Check if the log entries for the superuser are rendered\n        self.assertIn(\n            f\"{{ admin_log.first.change_message }}\", content\n        )\n"], "sample_1094": ["    def test_match_old(self):\n        from sympy import diff, Derivative\n        x, y = symbols('x y')\n        expr = x**y\n\n        # Test matching with old style. Should work for\n        # expressions with symbols.\n\n        match = expr.match(expr, old=True)\n        assert match is not None\n\n        # Test matching with old style. Should handle\n        # derivatives.\n        f = Function('f')\n        d = Derivative(f(x), x)\n        match = d.match(f(x), old=True)\n        assert match is not None\n        assert match.get(f(x)) == f(x)\n\n        match = d.match(f(y), old=True)\n        assert match is None\n", "    def test_preorder_traversal_symbols_in_function():\n        from sympy.abc import x, y\n\n        f = Function('f')\n        expr = f(x**2 + y)\n        for i in preorder_traversal(expr):\n            if isinstance(i, Symbol):\n                assert i in {x, y}\n", "compilation error", "    def test_preorder_traversal_symbols(self):\n        x, y, z = symbols('x y z')\n        for i in preorder_traversal(x + y * z):\n            assert isinstance(i, Basic) or isinstance(i, Symbol)\n        for i in preorder_traversal(x + y * z, keys=True):\n            assert isinstance(i, Basic) or isinstance(i, Symbol) \n\n\n", "    def test_constructor_postprocessors():\n        class MyFunction(Function):\n            @classmethod\n                return {\n                    'self': lambda e: e.replace(\n                        b1, b2),\n                }\n\n        f = MyFunction('f')(b1)\n        assert isinstance(f, MyFunction)\n        assert f.args[0] is b2\n\n\n\n"], "sample_41": ["    def test_unpicklable_units(tmpdir):\n        u1 = u.m * u.s\n        with open(tmpdir.join(\"units.pickle\"), 'wb') as f:\n            pickle.dump(u1, f)\n        with open(tmpdir.join(\"units.pickle\"), 'rb') as f:\n            u2 = pickle.load(f)\n        assert u1 == u2\n\n\n\n\n", "    def test_composite_unit_decompose_with_nested_units(self):\n        # Test that decomposition handles nested CompositeUnits\n        #  correctly.\n        a_unit = u.composite_unit(10 * u.m, [u.composite_unit(2, [u.cm], [1])])\n\n        b = a_unit.decompose()\n        expected = 10 * u.m\n        assert_allclose(a_unit, expected)\n        assert_allclose(b, expected)\n\n\n", "compilation error", "    def test_composite_unit_equality(", "    def test_composite_unit_decomposition():\n        # Test decomposition with one level of nesting\n        m = u.meter\n        kg = u.kilogram\n        s = u.second\n        h = u.hour\n        \n        unit = u.CompositeUnit(1, [m, kg], [1, 1])\n        assert unit.decompose() == CompositeUnit(1, [m, kg], [1, 1])\n        assert unit.decompose(bases={m}).scale == 1 and unit.decompose(bases={m}).bases == [kg]\n        assert unit.decompose(bases={kg}).scale == 1 and unit.decompose(bases={kg}).bases == [m]\n        assert unit.decompose(bases={m, kg}).scale == 1 and unit.decompose(bases={m, kg}).bases == []\n        \n        # Test decomposition with multiple levels of nesting\n        unit = u.CompositeUnit(1, [m, u.CompositeUnit(1, [u.centimeter], [10])], [1, 1])\n        assert (unit.decompose().bases == [u.meter]\n                and unit.decompose().powers == [1])\n        \n        # Test decomposition with non-numeric powers\n        unit = u.CompositeUnit(1, [m, m], [3, 2])\n        assert (unit.decompose().bases == [m]\n                and unit.decompose().powers == [5])\n        \n        with pytest.raises(ValueError):\n            unit = u.CompositeUnit(1, [m, m], ['2', 3])\n\n\n\n"], "sample_486": ["    def test_inline_formset_with_auto_pk_parent(self):\n        ModelForm = type(\n            \"ModelForm\",\n            (forms.BaseModelForm,),\n            {\n                \"Meta\": {\"model\": AutoPKChildOfAutoPKParent},\n            },\n        )\n        FormSet = inlineformset_factory(\n            AutoPKParent,\n            AutoPKChildOfAutoPKParent,\n            form=ModelForm,\n            fields=[\"name\", \"parent\"],\n        )\n        formset = FormSet(instance=AutoPKParent.objects.create(name=\"parent 1\"))\n        self.assertEqual(formset.fields[0].initial[\"parent\"], \n                         AutoPKParent.objects.get(name=\"parent 1\"))\n\n", "    def test_inlineformset_factory_with_to_field_name(self):\n        InlineFormset = inlineformset_factory(\n            ParentWithUUIDAlternateKey,\n            UUIDPKChild,\n            form=ModelForm,\n            fields=[\"name\"],\n            fk_name=\"parent_uuid\",\n            to_field_name=\"uuid\",\n        )\n\n        formset = InlineFormset(queryset=UUIDPKChild.objects.none(), prefix=\"test\")\n        self.assertEqual(formset.form.fields[\"parent_uuid\"].to_field_name, \"uuid\")\n\n\n\n", "    def test_inlineformset_factory_can_handle_non_auto_primary_key_parent(self):\n        class ChildForm(forms.ModelForm):\n            class Meta:\n                model = UUIDPKChildOfAutoPKParent\n                fields = (\"name\",)\n\n        InlineFormSet = inlineformset_factory(\n            UUIDPKParent,\n            UUIDPKChildOfAutoPKParent,\n            form=ChildForm,\n            extra=2,\n        )\n        formset = InlineFormSet(queryset=UUIDPKChildOfAutoPKParent.objects.none())\n        self.assertTrue(formset.is_bound)\n\n", "    def test_inlineformset_factory_with_fk_name(self):\n        # Test with explicit fk name\n\n        class InlineForm(ModelForm):\n            class Meta:\n                model = UUIDPKChildOfAutoPKParent\n                fields = ['name']\n\n        InlineFormSet = inlineformset_factory(\n            UUIDPKParent,\n            UUIDPKChildOfAutoPKParent,\n            form=InlineForm,\n            fk_name='parent_fk',\n        )\n\n        formset = InlineFormSet(queryset=UUIDPKChildOfAutoPKParent.objects.none())\n        self.assertEqual(formset.formset.fk.name, 'parent_fk')\n", "    def test_inlineformset_factory_with_fk_name(self):\n        FormSet = inlineformset_factory(\n            ParentWithUUIDAlternateKey,\n            UUIDPKChildOfAutoPKParent,\n            fk_name='related_child',\n        )\n        formset = FormSet(queryset=UUIDPKChildOfAutoPKParent.objects.all())\n        self.assertEqual(formset.form.fields['related_child'].related_model, UUIDPKParent)\n"], "sample_585": ["    def test_groupby_reduce_with_dim(self, dim):\n        data = xr.Dataset({\n            \"x\": xr.DataArray(np.arange(24), dims=(\"x\", \"y\")),\n            \"y\": xr.DataArray(np.arange(6), dims=(\"y\",)),\n        })\n        grouped = data.groupby(\"x\")\n\n            return ds[\"y\"].sum()\n\n        result = grouped.reduce(sum_y, dim=dim)\n\n        expected = xr.DataArray(\n            np.array([6, 12, 18, 24]),\n            dims=(\"x\",),\n            coords={\"x\": [0, 1, 2, 3]},\n        )\n\n        assert_identical(result, expected)\n\n", "    def test_groupby_empty_groups_no_restore(self):\n        data = xr.Dataset({'var': (('x', 'y'), np.arange(12).reshape(3, 4))},\n                          coords={'x': [0, 1, 2], 'y': [0, 1, 2, 3]})\n        # Create an empty group by a non-existent coordinate\n        grouped = data.groupby('z')\n        with pytest.raises(KeyError):\n            grouped.apply(lambda ds: ds['var'].sum())\n", "    def test_groupby_assign_coords(self, dim):\n        data = xr.Dataset({'temp': (('x', 'y'), np.random.rand(3, 4)),\n                           'humidity': (('x', 'y'), np.random.rand(3, 4))},\n                          coords={'x': [1, 2, 3], 'y': [4, 5, 6, 7]})\n\n        grouped = data.groupby(\"x\", dim=dim)\n        new_coords = {'time': [1, 2, 3]}\n        result = grouped.assign_coords(time=new_coords)\n\n        expected = xr.Dataset({'temp': (('x', 'y'), np.random.rand(3, 4)),\n                              'humidity': (('x', 'y'), np.random.rand(3, 4))},\n                             coords={'x': [1, 2, 3], 'y': [4, 5, 6, 7],\n                                     'time': [1, 2, 3]})\n        assert_identical(result, expected)\n", "    def test_groupby_apply_non_array_result(self, dim):\n        data = xr.Dataset({'a': (('x', 'y'), np.random.randn(10, 5)),\n                           'b': (('x', 'y'), np.random.randn(10, 5)),\n                           'c': (('x', 'y', 't'), np.random.randn(10, 5, 4))},\n                          coords={'x': np.arange(10), 'y': np.arange(5)})\n\n        grouped = data.groupby('x', dim=dim)\n        result = grouped.apply(lambda ds: {'d': ds['a'] + ds['b']})\n        expected = xr.Dataset({'d': (('x', 'y'), np.random.randn(10, 5))},\n                              coords={'x': np.arange(10), 'y': np.arange(5)})\n        assert_identical(result, expected)\n\n", "    def test_groupby_apply_with_dim(self, dim):\n        data = xr.Dataset(\n            data={\n                \"var\": ((\"time\", \"some_dimension\"), np.random.rand(5, 3))\n            },\n            coords={\"time\": pd.date_range(\"2023-01-01\", periods=5),\n                    \"some_dimension\": [\"a\", \"b\", \"c\"]},\n        )\n\n        grouped = data.groupby(\"time\")\n\n        if dim is None:\n            result = grouped.apply(lambda x: x[\"var\"].sum(dim=\"some_dimension\"))\n        else:\n            result = grouped.apply(lambda x: x[\"var\"].sum(dim=dim))\n\n        expected = data.groupby(\"time\").apply(\n            lambda x: x[\"var\"].sum(dim=dim)\n        )\n        assert_identical(result, expected)\n\n"], "sample_1151": ["    def test_mod_mul():\n        assert Mod(x*y, x) == y\n        assert Mod(x*y, 2*x) == Mod(y, 2)\n        assert Mod(2*x, x) == 2\n        assert Mod(2*x*y, x) == 2*y\n        assert Mod(-2*x*y, x) == -2*y\n        assert Mod(x*y, x) == y\n        assert Mod(Mul(2, x), x) == 2\n        assert Mod(Mul(2, x)*y, x) == 2*y\n\n", "    def test_mod_with_mul():\n        #Test Mod with multiplication\n        assert same_and_same_prec(Mod(2*x, 3), Mod(x, 3)*2)\n        assert same_and_same_prec(Mod(2*x*y, 3), Mod(x, 3)*Mod(y, 3)*2)\n        assert same_and_same_prec(Mod(3*x*y, 3), Mod(x, 3)*Mod(y, 3)*3)\n        \n        assert same_and_same_prec(Mod(2*x*y, 3), Mod(x*y, 3)*2)\n        assert same_and_same_prec(Mod(x*y, 3)*3, Mod(3*x*y, 3))\n    \n", "    def test_mod_mul():\n        from sympy.core.numbers import nan\n        p = Mul(2*x, 3*y)\n        q = 5\n        rv = Mod(p, q)\n        from sympy import sympify\n        assert rv.args == (p, q)\n        assert rv.func == Mod\n        assert sympify(rv) == Mod(6*x*y, 5)\n        p = Mul(2*x, 3*y)\n        q = Mul(5, x)\n        rv = Mod(p, q)\n        assert rv.args == (p, q)\n        assert rv.func == Mod\n        assert rv.evalf(4) == Mod(6*x*y, 5*x).evalf(4)\n        p = Mul(2*x, 3*y)\n        q = 5 + x\n        rv = Mod(p, q)\n        assert rv.args == (p, q)\n        assert rv.func == Mod\n        assert rv.evalf(4) == Mod(6*x*y, 5 + x).evalf(4)\n\n        p = Mod(2*x, 5)\n        q = Mod(3*y, 5)\n        rv = Mod(p, q)\n        assert rv.args == (p, q)\n        assert rv.func == Mod\n        assert rv.evalf(4) == Mod(2*x, 3*y).evalf(4)\n\n        p = Mod(x, 5)\n        q = Mod(y, 5)\n        rv = Mod(p, q)\n        assert rv.args == (p, q)\n        assert rv.func == Mod\n        assert rv.evalf(4) == Mod(x, y).evalf(4)\n", "    def test_mod_mul(self):\n        p = Mul(2*x, 3*y)\n        q = 5\n        r = Mod(p, q)\n        self.assertEqual(r, Mod(2*x*3*y, 5))\n        self.assertEqual(r, Mod(6*x*y, 5)) \n\n", "compilation error"], "sample_747": ["    def test_power_transform_ordinal(self):\n        \"\"\"Test ordinal encoding with power transformer.\"\"\"\n        X = np.array([[1, 2], [3, 2], [3, 1]])\n        pt = PowerTransformer(method='ordinal', copy=False)\n        X_transformed = pt.fit_transform(X)\n        assert_array_equal(X_transformed, np.array([[0, 1], [1, 0], [1, 2]]))\n\n        # Test inverse transform\n        X_inverse = pt.inverse_transform(X_transformed)\n        assert_array_equal(X_inverse, X)\n\n\n\n", "    def test_power_transform_ordinal(self):\n        # Test ordinal encoding with power_transform\n        enc = PowerTransformer(method='box-cox')\n        X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n        enc.fit(X)\n        trans = enc.transform(X)\n        inverse_trans = enc.inverse_transform(trans)\n\n        assert_array_equal(inverse_trans, X)\n\n\n\n", "    def test_power_transform_with_standardize_True_no_data(self):\n        X = np.array([1, 2, 3, 4])\n        pt = PowerTransformer(method='box-cox', standardize=True, copy=True)\n        with pytest.warns(UserWarning):\n            pt.fit(X)\n        transformed_X = pt.transform(X)\n        assert_array_equal(transformed_X, np.array([1, 2, 3, 4]))\n\n\n\n", "    def test_power_transform_ordinal(self):\n        X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n        pt = PowerTransformer(method='ordinal')\n        X_trans = pt.fit_transform(X)\n\n        # Check shape\n        assert_equal(X_trans.shape, X.shape)\n\n        # Check ordinal encoding\n        expected_output = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\n        assert_array_equal(X_trans, expected_output)\n\n        # Test inverse transform\n        X_inv = pt.inverse_transform(X_trans)\n        assert_array_equal(X_inv, X)\n", "def test_power_transform_ordinal():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    pt = PowerTransformer(method='ordinal', standardize=False)\n    X_transformed = pt.fit_transform(X)\n    assert_equal(X_transformed.shape, (3, 2))\n    assert_array_equal(X_transformed, np.array([[0, 1], [2, 3], [4, 5]]))\n\n    X_transformed_back = pt.inverse_transform(X_transformed)\n    assert_array_equal(X_transformed_back, X)\n\n"], "sample_739": ["    def test_multilabel_binarizer_transform_with_classes(self):\n        mlb = MultiLabelBinarizer(classes=['comedy', 'sci-fi', 'thriller'])\n        y_train = [set(['sci-fi', 'thriller']), set(['comedy']),\n                   set(['comedy', 'sci-fi'])]\n        y_train_binarized = mlb.fit_transform(y_train)\n        assert_array_equal(y_train_binarized,\n                           [[0, 1, 1], [1, 0, 0], [1, 1, 0]])\n        y_new_train = mlb.transform(y_train)\n        assert_array_equal(y_new_train,\n                           [[0, 1, 1], [1, 0, 0], [1, 1, 0]])\n        \n", "    def test_multilabel_binarizer_sparse_output_transform(self):\n        mlb = MultiLabelBinarizer(sparse_output=True)\n        y = [[1, 2], [3], [1, 2, 3]]\n        y_transformed = mlb.fit_transform(y)\n        assert isinstance(y_transformed, csr_matrix)\n        assert y_transformed.shape == (3, 3)\n", "    def test_multilabel_binarizer_sparse_output(self):\n        mlb = MultiLabelBinarizer(sparse_output=True)\n        y = [[1, 2], [3], [1, 2, 3]]\n        mlb.fit(y)\n        y_sparse = mlb.transform(y)\n        assert_true(isinstance(y_sparse, csr_matrix),\n                    \"Expected sparse output but got {}\".format(type(y_sparse)))\n", "    def test_multilabel_binarizer_multiclass_multilabel_indicator(self):\n        mlb = MultiLabelBinarizer()\n        y = [[1, 2], [2, 3], [1, 3]]\n        y_bin = mlb.fit_transform([[1, 2], [2, 3], [1, 3]])\n        assert_array_equal(y_bin,\n                           [[1, 1, 0],\n                            [0, 1, 1],\n                            [1, 0, 1]])\n", "    def test_multilabel_binarizer_fit_transform_with_classes(self):\n        mlb = MultiLabelBinarizer(classes=['comedy', 'sci-fi', 'thriller'])\n        y = [set(['sci-fi', 'thriller']), set(['comedy'])]\n        y_binary = mlb.fit_transform(y)\n\n        assert_array_equal(y_binary,\n                           [[0, 1, 1], [1, 0, 0]])\n        assert_equal(mlb.classes_, ['comedy', 'sci-fi', 'thriller'])\n"], "sample_966": ["    def test_parse_annotation( ):\n        assert parse('Foo(a: int, b: str) -> List[str]') == 'Foo(a: int, b: str) -> List[str]'\n        assert parse('Foo(a: int) -> None') == 'Foo(a: int) -> None'\n        assert parse('Foo() -> object') == 'Foo() -> object'\n\n\n\n", "    def test_parse_annotation():\n        assert _parse_annotation('int') == ('int', None)\n        assert _parse_annotation('int, float') == ('int, float', None)\n        assert _parse_annotation('int -> str') == ('int', ' -> str')\n        assert _parse_annotation('int -> str | None') == ('int', ' -> (str | None)')\n        assert _parse_annotation('(int, str) -> int') == ('(int, str)', ' -> int')\n        assert _parse_annotation('list[int]') == ('list[int]', None)\n        assert _parse_annotation('List[int]') == ('List[int]', None)\n\n\n\n", "    def test_parse_complex_annotation(self):\n        sig = \"SomeFunction(a: int, b: list[str]) -> Tuple[int, float]\"\n        node = parse(sig)\n        assert_node(node,\n                    text='SomeFunction(a: int, b: list[str]) -> Tuple[int, float]')\n        assert len(node.children) == 5\n        assert isinstance(node.children[1], addnodes.desc_sig_name)\n        assert isinstance(node.children[3], addnodes.desc_annotation)\n        assert isinstance(node.children[4], addnodes.desc_annotation)\n\n\n\n", "def test_parse_annotation():\n    assert _parse_annotation(\"int\") == (\"int\", \"\")\n    assert _parse_annotation(\"int: str\") == (\"int\", \"str\")\n    assert _parse_annotation(\"int -> str\") == (\"int\", \"str\")\n    assert _parse_annotation(\"List[int]\") == (\"List\", \"[int]\")\n    assert _parse_annotation(\"List[int | str]\") == (\"List\", \"[int | str]\")\n    assert _parse_annotation(\"Optional[int]\") == (\"Optional\", \"[int]\")\n    assert _parse_annotation(\"Typed[int]\") == (\"Typed\", \"[int]\")\n\n\n\n    \n", "    def test_parse_annotation():\n        # Test parsing annotations with various types\n        assert _parse_annotation('int') == ('int', None, None)\n        assert _parse_annotation('Optional[str]') == ('Optional', ['str'], None)\n        assert _parse_annotation('List[int]') == ('List', ['int'], None)\n        assert _parse_annotation('typing.List[int]') == ('typing.List', ['int'], None)\n        assert _parse_annotation('str') == ('str', None, None)\n        assert _parse_annotation('int, float') == ('int, float', None, None)\n        assert _parse_annotation('dict[str, int]') == ('dict', ['str', 'int'], None)\n\n        # Test parsing annotations with type variables\n        assert _parse_annotation('T') == ('T', None, None)\n        assert _parse_annotation('List[T]') == ('List', ['T'], None)\n        assert _parse_annotation('Dict[K, V]') == ('Dict', ['K', 'V'], None)\n\n        # Test parsing annotations with NoneType\n        assert _parse_annotation('NoneType') == ('NoneType', None, None)\n\n\n\n"], "sample_65": ["    def test_set_language_with_invalid_url(self):\n        request = self.client.get(reverse('set_language'))\n        response = self.client.post(reverse('set_language'), data={'language': self._get_inactive_language_code(), 'next': '/')})\n        self.assertRedirects(response, '/')\n        self.assertNotContains(response.content, 'invalid')\n\n", "    def test_set_language_to_inactive_language(self):\n        inactive_language = self._get_inactive_language_code()\n        url = reverse('set_language')\n        response = self.client.post(\n            url, data={LANGUAGE_QUERY_PARAMETER: inactive_language}\n        )\n        self.assertRedirects(response, url)\n        self.assertEqual(response.status_code, 302)\n        self.assertContains(response, inactive_language)\n        self.assert_redirected_to_url(\n            response.url,\n            url,\n            parameters={'language': inactive_language}\n        )\n\n\n", "    def test_set_language_inactive(self):\n        inactive_code = self._get_inactive_language_code()\n        request = RequestFactory().post(reverse('set_language'), data={'language': inactive_code})\n        response = set_language(request)\n        self.assertEqual(response.status_code, 302)\n        self.assertRedirects(response, request.GET.get('next') or '/')\n        with self.assertLogs(level='INFO') as logs:\n            get_language()\n        self.assertEqual(get_language(), inactive_code)\n\n\n\n\n", "    def test_set_language_in_inactive_language(self):\n        \"\"\"Tests changing language to an inactive language.\"\"\"\n        inactive_language = self._get_inactive_language_code()\n        request = self.create_request('POST', '/set_language/', {'language': inactive_language})\n        response = set_language(request)\n\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(get_language(), inactive_language)\n\n\n", "    def test_set_language_inactive_language(self):\n        inactive_language_code = self._get_inactive_language_code()\n        request = RequestFactory().post(reverse('set_language'), data={'language': inactive_language_code})\n        response = set_language(request)\n        assert response.status_code == 302\n        assert response['Location'].endswith(f'/?{LANGUAGE_QUERY_PARAMETER}={inactive_language_code}')\n\n\n\n\n\n"], "sample_499": ["    def test_legend_markersize(self, markersize):\n        fig, ax = plt.subplots()\n\n        line1, = ax.plot([1, 2, 3], [4, 5, 6], label='Line1', marker='o', markersize=markersize)\n        line2, = ax.plot([4, 5, 6], [7, 8, 9], label='Line2', marker='s', markersize=markersize)\n\n        legend = ax.legend()\n\n        plt.close(fig)\n\n", "    def test_legend_location(self, loc):\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3], [4, 5, 6])\n        ax.set_title(\"Test Legend Location\")\n        legend = ax.legend(loc=loc)\n        ax.axis(\"off\")\n        plt.show()", "    def test_bbox_to_anchor_with_figure_bbox(self):\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3], [4, 5, 6])\n        fig.legend(bbox_to_anchor=(0.5, 0.5))\n        plt.draw()\n\n\n\n    \n\n", "    def test_legend_collections(self):\n        fig, ax = plt.subplots()\n\n        data = np.random.rand(10, 2)\n        sc = ax.scatter(data[:, 0], data[:, 1], s=100)\n        line = ax.plot(data[:, 0], data[:, 1], 'r-', label='Line')\n\n        # Test with custom Collection handler\n        class MyHandler(HandlerCollection):\n                               label):\n                return [\n                    mpath.PathPolygon(self.get_transform().transform_path(\n                        mpath.Path(shape)),\n                        facecolor=legend.get_framecolor(),\n                        edgecolor='k',\n                        linewidth=1)\n                    for mpath in orig_handle.get_paths()\n                ]\n\n        with rc_context({'legend.handler_map': {collections.abc.Collection:\n                                                 MyHandler}}), \\\n                pytest.warns(UserWarning):\n            legend = fig.legend([line, sc],\n                                labels=['Line', 'Scatter'],\n                                loc='upper right',\n                                title=\"Example\",\n                                handler_map={'Line2D': HandlerLine2D(),\n                                              'Collection': MyHandler})\n\n        # Test without custom handler\n\n\n        \n        ax.axis('equal')\n        plt.show()\n\n", "    def test_legend_bbox_to_anchor(self):\n        with rc_context({'axes.facecolor': 'lightgray'}):\n            fig, ax = plt.subplots()\n\n            # Test case 1: bbox_to_anchor is None (use parent bbox)\n            line1 = ax.plot([1, 2], [1, 2])[0]\n            line2 = ax.plot([3, 4], [3, 4])[0]\n            legend = fig.legend([line1, line2], ['Line 1', 'Line 2'],\n                                loc='upper right',\n                                bbox_to_anchor=None)\n\n            # Test case 2: bbox_to_anchor is a tuple (left, bottom)\n            line3 = ax.plot([5, 6], [5, 6])[0]\n            legend = fig.legend([line1, line3], ['Line 1', 'Line 3'],\n                                loc='lower left',\n                                bbox_to_anchor=(0.1, 0.1))\n\n            # Test case 3: bbox_to_anchor is a tuple\n            # (left, bottom, width, height)\n            line4 = ax.plot([7, 8], [7, 8])[0]\n            legend = fig.legend([line2, line4], ['Line 2', 'Line 4'],\n                                loc='lower center',\n                                bbox_to_anchor=(0.5, 0.1, 0.3, 0.2))\n\n\n\n"], "sample_150": ["    def test_check_database_backends_calls_validation_checks(self):\n        with mock.patch('django.db.backends.base.validation.BaseDatabaseValidation.objects', self.mock_objects):\n            with mock.patch('django.apps.AppConfig.get_default') as get_default:\n                get_default.return_value.connection = connection\n                check_database_backends(databases=self.databases)\n                for database in self.databases:\n                    self.assertEqual(connection.get_default_db_name(), database)\n                    self.assertTrue(getattr(connection, database).check.called)\n", "    def test_check_migrations_without_databases(self):\n        with mock.patch('django.apps.apps.apps.get_app_config') as MockGetAppConfig:\n            from django.db.migrations.executor import MigrationExecutor  \n            with mock.patch('django.db.migrations.executor.MigrationExecutor.migration_plan',\n                            return_value=None): \n                MockGetAppConfig.return_value = mock.MagicMock()\n                self.assertNoMigrationsFound('SystemCheckError: No databases configured.')\n                self.assertEqual(MockGetAppConfig.call_count, 1)\n", "    def test_check_with_database_errors(self):\n        self.addCleanup(connection.close)\n        with mock.patch('django.db.backends.base.validation.BaseDatabaseValidation.check') as mock_check:\n            mock_check.side_effect = [\n                check_database_backends.DatabaseCheckError(\"Database backend error in 'default'\")\n            ]\n            result = check_database_backends(databases=self.databases)\n            self.assertEqual(len(result), 1)\n            self.assertEqual(result[0].level, checks.ERROR)\n            self.assertEqual(result[0].msg, \"Database backend error in 'default'\")\n", "    def test_check_with_silent_errors(self):\n        # Test that SystemCheckError is raised even if some issues are silenced.\n        with mock.patch('django.core.management.base.CommandError', return_value=None):\n            with mock.patch('django.core.checks.run_checks',\n                            side_effect=lambda *args, **kwargs: [\n                                check_database_backends(\n                                    'default',\n                                    {'databases': self.databases},\n                                    mock.MagicMock()\n                                ) for _ in self.databases\n                            ]):\n                from django.core.management.base import SystemCheckError\n\n                with self.assertRaises(SystemCheckError):\n                    connection._set_alias('default')\n                    check_database_backends('default', {'databases': self.databases}, mock.MagicMock())\n", "    def test_check_migrations_no_migrations(self):\n        with mock.patch('django.db.migrations.executor.MigrationExecutor.migration_plan') as mock_migration_plan:\n            mock_migration_plan.return_value = []\n            self.assertEqual(\n                check_database_backends(self.databases, [], [], [], [], [], [], []),\n                []\n            )\n\n\n"], "sample_962": ["    def test_restify_with_mock(self):\n        with mock(['nonexistent_module']):\n            with pytest.raises(ImportError):\n                restify('nonexistent_module.NonexistentClass')\n        mock_obj = _MockObject()\n        with pytest.raises(TypeError):\n            restify(mock_obj)\n\n", "    def test_restify_simple(self):\n        assert restify(None) is None\n        assert restify(123) == {'_type': 'int', 'value': 123}\n        assert restify(\"hello\") == {'_type': 'str', 'value': \"hello\"}\n        assert restify(True) == {'_type': 'bool', 'value': True}\n        assert restify([]) == {'_type': 'list', 'value': []}\n", "    def test_restify_with_mock(self):\n        with mock(['target.module.name']):\n            class MockClass:\n                    return f'mock_{attr}'\n\n            @restify\n                return {'param1': param1, 'param2': param2}\n\n            result = my_func(1, MockClass())\n            assert result == {'param1': 1, 'param2': 'mock_param2'}\n", "    def test_restify_mock_object(self):\n        with mock(['nonexistent.module']):\n            mock_obj = _MockObject()\n            restified = restify(mock_obj)\n            assert restified == {'__class__': '_MockObject', '__sphinx_mock__': True, '__display_name__': 'nonexistent.module._MockObject'}\n", "    def test_restify_none(self) -> None:\n        assert restify(None) == 'None'\n"], "sample_857": ["    def test_sparse_data(ds):\n        X = DATASETS[ds][\"X_sparse\"]\n        y = DATASETS[ds][\"y\"]\n\n        if ds == \"zeros\":\n            return\n\n        clf = DecisionTreeClassifier(criterion=\"gini\", random_state=0)\n        clf.fit(X, y)\n        tree.plot_tree(clf, feature_names=None, filled=True)\n        X_reg = DATASETS[ds][\"X\"]\n        y_reg = DATASETS[ds][\"y\"]\n        reg = DecisionTreeRegressor(criterion=\"mse\", random_state=0)\n        reg.fit(X_reg, y_reg)\n        tree.plot_tree(reg, feature_names=None, filled=True)\n", "    def test_sparse_input(self):\n        for tree_type in SPARSE_TREES:\n            clf = CLF_TREES[tree_type]()\n            reg = REG_TREES[tree_type]()\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\", category=UserWarning)\n                clf.fit(DATASETS[\"sparse-pos\"][\"X_sparse\"],\n                        DATASETS[\"sparse-pos\"][\"y\"])\n                reg.fit(DATASETS[\"sparse-pos\"][\"X_sparse\"],\n                        DATASETS[\"sparse-pos\"][\"y\"])\n\n\n                if tree_type == \"ExtraTreeClassifier\" or tree_type == \"ExtraTreeRegressor\":\n                    # for extra trees, we cannot assert that each tree is identical\n                    # so we test that the structure is consistent\n                    trees = clf.estimators_\n                    for tree in trees:\n                        assert_tree_equal(tree.tree_.to_array(),\n                                          clf.tree_.to_array(),\n                                          \"Tree structure inconsistency\")\n                    trees = reg.estimators_\n                    for tree in trees:\n                        assert_tree_equal(tree.tree_.to_array(),\n                                          reg.tree_.to_array(),\n                                          \"Tree structure inconsistency\")\n\n                else:\n                    assert_tree_equal(clf.tree_.to_array(),\n                                      reg.tree_.to_array(),\n                                      \"Tree structure inconsistency\")\n\n\n\n", "    def test_sparse_data(dataset, classifier):\n        X = DATASETS[dataset][\"X_sparse\"]\n        y = DATASETS[dataset][\"y\"]\n        cls = classifier()\n        cls.fit(X, y)\n        \n        with TempMemmap(\"tree.dat\", dtype=np.uint8) as f:\n            tree.export_graphviz(cls.tree_,\n                               out_file=f,\n                               feature_names=None)\n            f.seek(0)\n            loaded_tree = tree.load_tree(f)\n        \n\n        assert_tree_equal(cls.tree_, loaded_tree,\n                          \"Mismatch on sparse data\")\n\n\n", "    def test_sparse_data(self):\n        for i, tree_name in enumerate(SPARSE_TREES):\n            X_sparse = DATASETS[f\"sparse-{i}\"][\"X_sparse\"]\n            y = DATASETS[f\"sparse-{i}\"][\"y\"]\n            clf = CLF_TREES[tree_name](random_state=0)\n            reg = REG_TREES[tree_name](random_state=0)\n\n            clf.fit(X_sparse, y)\n            reg.fit(X_sparse, y)\n\n\n            with pytest.raises(ValueError):\n                clf.predict(csr_matrix(X_sparse))\n            with pytest.raises(ValueError):\n                reg.predict(csc_matrix(X_sparse))\n\n            clf.predict(X_sparse.toarray())\n            reg.predict(X_sparse.toarray())\n", "def test_sparse_tree_construction():\n    for tree_type in SPARSE_TREES:\n        tree_cls = ALL_TREES[tree_type]\n\n        # Train a sparse tree\n        X = DATASETS[\"sparse-pos\"][\"X_sparse\"]\n        y = DATASETS[\"sparse-pos\"][\"y\"]\n        if tree_cls is DecisionTreeClassifier or tree_cls is ExtraTreeClassifier:\n            model = tree_cls(random_state=0)\n        else:\n            model = tree_cls(random_state=0)\n        model.fit(X, y)\n        s = model.tree_\n\n        # Create a new tree from the serialized version\n        with TempMemmap(\"tmp.bin\", \"w\") as f:\n            pickle.dump((model, X, y), f)\n\n        with TempMemmap(\"tmp.bin\", \"r\") as f:\n            loaded_model, _, _ = pickle.load(f)\n\n        d = loaded_model.tree_\n\n        assert_tree_equal(d, s, \"Sparse tree serialization/deserialization check\")\n"], "sample_1127": ["    def test_symmetric_permutation_group_order(self):\n        for n in range(1, 11):\n            G = SymmetricPermutationGroup(n)\n            self.assertEqual(G.order(), factorial(n))\n    ", "    def test_coset_equality(self):\n        G = PermutationGroup([Permutation([1, 2, 0, 3, 4, 5]),\n                              Permutation([0, 1, 2, 3, 4, 5])])\n        coset1 = Coset(Permutation([1, 2, 0, 3, 4, 5]), G, dir='+')\n        coset2 = Coset(Permutation([1, 2, 3, 0, 4, 5]), G, dir='+')\n        assert coset1 != coset2\n\n        coset3 = Coset(Permutation([1, 2, 0, 3, 4, 5]), G, dir='-')\n        assert coset1 != coset3\n        assert coset1 == Coset(Permutation([1, 2, 0, 3, 4, 5]), G, dir='+')\n", "    def test_coset_operations(self):\n        G = PermutationGroup([Permutation([0, 1, 2]), Permutation([1, 2, 0])])\n        H = PermutationGroup([Permutation([0, 1, 2])])\n        coset_a = Coset(Permutation([1, 2, 0]), H, G, dir='+')\n        coset_b = Coset(Permutation([0, 2, 1]), H, G, dir='+')\n        self.assertEqual(len(coset_a.as_list()), 2)\n        self.assertEqual(len(coset_b.as_list()), 2)\n        self.assertEqual(coset_a * coset_b,\n                         Coset(Permutation([0, 0, 1]), H, G, dir='+' ))\n\n        \n", "    def test_coset_operations(self):\n\n        H = PermutationGroup([Permutation(1, 2), Permutation(0, 1)])\n        G = SymmetricPermutationGroup(3)\n        g = Permutation(0, 2, 1)\n        coset_gH = Coset(g, H, G, dir=\"+\")\n\n        for perm in H.elements:\n            assert (g * perm) in coset_gH.as_list()\n        for perm in G.elements:\n            if perm not in coset_gH.as_list():\n                assert (perm * g) not in coset_gH.as_list()\n\n\n\n", "    def test_coset_table(self):\n        G = SymmetricPermutationGroup(4)\n        H = PermutationGroup([Permutation((1, 2)), Permutation((3, 4))])\n        a = Permutation((1, 2, 3))\n        c = Coset(a, H, dir=\"-\")\n        table = c.coset_table()\n        for i in range(G.order()):\n            x = G.elements[i]\n            if x in table:\n                for j in table[x]:\n                    if not (H * x).is_in(G.elements[j]):\n                        raise ValueError(f\"Error, {H * x} is not equal to {G.elements[j]}\")\n\n\n"], "sample_374": ["    def test_prefetch_with_related_manager(self):\n        with ignore_warnings(category=RemovedInDjango50Warning):\n            authors = Author.objects.prefetch_related(\n                Prefetch('favorite_books', queryset=Book.objects.filter(title__icontains='sense')),\n            )\n\n        with CaptureQueriesContext(connection) as ctx:\n            for author in authors:\n                for favorite_book in author.favorite_books.all():\n                    pass\n\n        self.assertWhereContains(ctx.queries[0][0], \"'title__icontains' = 'sense'\")\n\n\n\n", "    def test_prefetch_related_with_related_manager_and_lookups(self):\n        with CaptureQueriesContext(connection) as captured_queries:\n            bookmarks = Bookmark.objects.select_related(\n                'reader',\n                'book'\n            ).prefetch_related(\n                'reader__favorites'\n            ).values('id', 'reader_id', 'book_id')\n            self.assertEqual(len(captured_queries), 3)\n            self.assertWhereContains(captured_queries[0].sql, 'book_id')\n            self.assertWhereContains(captured_queries[1].sql, 'reader_id')\n            self.assertWhereContains(captured_queries[2].sql, 'favorites__reader')\n\n\n\n", "    def test_prefetch_related_with_multiple_lookups(self):\n        with self.assertNumQueries(4):\n            books = Book.objects.prefetch_related(\n                Prefetch('authors', queryset=Author.objects.filter(name__startswith='A')),\n                Prefetch('reviews__comment', queryset=Comment.objects.filter(text__contains='great'))\n            ).all()\n\n        for book in books:\n            self.assertIn(book.authors.first(), [self.author1, self.author2, self.author3])\n            for review in book.reviews.all():\n                self.assertIn('great', review.comment.text)\n\n\n", "    def test_prefetch_related_with_empty_queryset(self):\n        with self.assertNumQueries(2):\n            qs = Article.objects.prefetch_related(\n                Prefetch('comments', queryset=Comment.objects.none())\n            )\n            self.assertEqual(qs.count(), 0)\n            for obj in qs:\n                self.assertEqual(obj.comments.count(), 0)\n", "    def test_prefetch_related_with_nested_lookups(self):\n        with CaptureQueriesContext(connection) as ctx:\n            readers = Reader.objects.prefetch_related(\n                Prefetch(\n                    'books_read',\n                    queryset=Book.objects.select_related('authors'),\n                )\n            ).all()\n\n        queries = ctx.captured_queries\n        self.assertEqual(len(queries), 2)\n        \n        # Check the first query is the Reader query\n        self.assertWhereContains(queries[0], 'reader_id')\n        \n        # Check that the second query is the related Book query with the\n        # select_related to Author included.\n        self.assertWhereContains(queries[1], 'book_id')\n        self.assertWhereContains(queries[1], 'author_id')\n\n        for reader in readers:\n            for book in reader.books_read.all():\n                self.assertIsNotNone(book.authors.all())\n\n\n"], "sample_267": ["    def test_sqlite_datetime_extract(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\"\"\"\n                CREATE TABLE test_datetime_extract (\n                    id INTEGER PRIMARY KEY,\n                    dt DATETIME NOT NULL\n                )\n            \"\"\")\n            cursor.execute(\"INSERT INTO test_datetime_extract (dt) VALUES (?)\", [datetime(2023, 10, 26, 10, 30, 45)])\n\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT datetime_extract('week_day', dt) FROM test_datetime_extract\")\n            self.assertEqual(cursor.fetchone()[0], 3)\n            cursor.execute(\"SELECT datetime_extract('iso_week_day', dt) FROM test_datetime_extract\")\n            self.assertEqual(cursor.fetchone()[0], 4)\n            cursor.execute(\"SELECT datetime_extract('week', dt) FROM test_datetime_extract\")\n            self.assertEqual(cursor.fetchone()[0], 43)\n            cursor.execute(\"SELECT datetime_extract('quarter', dt) FROM test_datetime_extract\")\n            self.assertEqual(cursor.fetchone()[0], 4)\n            cursor.execute(\"SELECT datetime_extract('iso_year', dt) FROM test_datetime_extract\")\n            self.assertEqual(cursor.fetchone()[0], 2023)\n            cursor.execute(\"SELECT datetime_extract('year', dt) FROM test_datetime_extract\")\n            self.assertEqual(cursor.fetchone()[0], 2023)\n            \n\n\n\n", "    def test_get_new_connection_kwargs(self):\n        settings_dict = {'DATABASES': {\n            'default': {\n                'ENGINE': 'django.db.backends.sqlite3',\n                'NAME': ':memory:',\n                'OPTIONS': {'check_same_thread': True}\n            }\n        }}\n        with override_settings(DATABASES=settings_dict):\n            dbw = DatabaseWrapper()\n            kwargs = dbw.get_new_connection_params({})\n        self.assertEqual(kwargs['database'], ':memory:')\n        self.assertFalse(kwargs['check_same_thread'])\n        self.assertEqual(kwargs['uri'], True)\n\n", "    def test_sqlite3_lpad_and_rpad(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\"\"\"CREATE TABLE lpad_table (\n                text TEXT\n            )\"\"\")\n            cursor.execute(\"\"\"CREATE TABLE rpad_table (\n                text TEXT\n            )\"\"\")\n\n            # lpad test case\n            cursor.execute(\n                'INSERT INTO lpad_table (text) VALUES (?)', ('abc',))\n            cursor.execute('SELECT lpad(text, 10, \\'x\\') FROM lpad_table')\n            result = cursor.fetchone()\n            self.assertEqual(result[0], 'xabc')\n            cursor.execute('SELECT lpad(text, 3, \\'x\\') FROM lpad_table')\n            result = cursor.fetchone()\n            self.assertEqual(result[0], 'abc')\n            cursor.execute('SELECT lpad(text, 1, \\'x\\') FROM lpad_table')\n            result = cursor.fetchone()\n            self.assertEqual(result[0], 'abc')\n\n            # rpad test case\n            cursor.execute(\n                'INSERT INTO rpad_table (text) VALUES (?)', ('abc',))\n            cursor.execute('SELECT rpad(text, 10, \\'x\\') FROM rpad_table')\n            result = cursor.fetchone()\n            self.assertEqual(result[0], 'abcxxxxxxxxx')\n            cursor.execute('SELECT rpad(text, 3, \\'x\\') FROM rpad_table')\n            result = cursor.fetchone()\n            self.assertEqual(result[0], 'abc')\n            cursor.execute('SELECT rpad(text, 1, \\'x\\') FROM rpad_table')\n            result = cursor.fetchone()\n            self.assertEqual(result[0], 'abc')\n\n", "    def test_sqlite_date_trunc(self):\n        \"\"\"Test date truncation for various timezones and date formats.\"\"\"\n        with connection.cursor() as cursor:\n            cursor.execute('''CREATE TABLE test_dates (\n                id INTEGER PRIMARY KEY,\n                date DATE\n            )''')\n            cursor.execute(\"INSERT INTO test_dates (date) VALUES ('2023-03-15')\")\n            cursor.execute(\"INSERT INTO test_dates (date) VALUES ('2023-03-15 10:00:00')\")\n            cursor.execute(\"INSERT INTO test_dates (date) VALUES ('2023-03-16')\")\n\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT date_trunc('year', date) FROM test_dates\")\n            self.assertSequenceEqual(\n                cursor.fetchall(),\n                [('2023-01-01',)],\n            )\n\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT date_trunc('quarter', date) FROM test_dates\")\n            self.assertSequenceEqual(\n                cursor.fetchall(),\n                [('2023-01-01',)],\n            )\n\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT date_trunc('month', date) FROM test_dates\")\n            self.assertSequenceEqual(\n                cursor.fetchall(),\n                [('2023-03-01',), ('2023-03-01',)],\n            )\n\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT date_trunc('week', date) FROM test_dates\")\n            self.assertSequenceEqual(\n                cursor.fetchall(),\n                [('2023-03-12',), ('2023-03-12',)],\n            )\n\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT date_trunc('day', date) FROM test_dates\")\n            self.assertSequenceEqual(\n                cursor.fetchall(),\n                [('2023-03-15',), ('2023-03-15',), ('2023", "    def test_sqlite_datetime_functions(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\"CREATE TABLE datetime_test (dt DATETIME)\")\n            cursor.execute(\"INSERT INTO datetime_test (dt) VALUES ('2023-10-27 10:00:00')\")\n\n        with transaction.atomic():\n            with self.app_context({'DATABASE_ROUTERS': ['django.contrib.auth.router']}):\n                query = \"SELECT * FROM datetime_test\"\n                datetime_test_rows = connection.execute(query).fetchall()\n                self.assertEqual(len(datetime_test_rows), 1)\n                row = datetime_test_rows[0]\n                \n                # Test _sqlite_datetime_parse\n                dt = connection.db.parse_datetime(row[0])\n                self.assertEqual(dt.year, 2023)\n                self.assertEqual(dt.month, 10)\n                self.assertEqual(dt.day, 27)\n                self.assertEqual(dt.hour, 10)\n                self.assertEqual(dt.minute, 0)\n                self.assertEqual(dt.second, 0)\n                self.assertEqual(dt.microsecond, 0)\n                \n                # Test _sqlite_datetime_extract\n                week_day = connection.db.extract_datetime_field(row[0], 'week_day')\n                self.assertEqual(week_day, 5) \n                quarter = connection.db.extract_datetime_field(row[0], 'quarter')\n                self.assertEqual(quarter, 4)\n                iso_year = connection.db.extract_datetime_field(row[0], 'iso_year')\n                self.assertEqual(iso_year, 2023)\n\n                # Test _sqlite_datetime_trunc\n                trunc_year = connection.db.trunc_datetime_field(row[0], 'year')\n                self.assertEqual(trunc_year, '2023-01-01 00:00:00')\n                trunc_quarter = connection.db.trunc_datetime_field(row[0], 'quarter')\n                self.assertEqual(trunc_quarter,"], "sample_384": ["    def test_bulk_update_with_existing_non_nullable(self):\n        with self.assertRaises(IntegrityError):\n            Note.objects.update(note=\"foo\")  \n", "    def test_bulk_update_notes_with_tags(self):\n        for note in self.notes:\n            note.tags.add(*self.tags[:2])\n        self.assertEqual(note.tags.count(), 2)\n        \n        # Update tags for some notes\n        notes_to_update = self.notes[:5]\n        for note in notes_to_update:\n            note.tags.clear()\n            note.tags.add(*self.tags[3:5])\n\n        # Assert that individual notes were updated correctly\n        for i, note in enumerate(notes_to_update):\n            self.assertEqual(note.tags.count(), 2)\n            self.assertIn(self.tags[3], note.tags.all())\n            self.assertIn(self.tags[4], note.tags.all())\n", "    def test_bulk_update_notes(self):\n        with self.assertNumQueries(1):\n            Note.objects.filter(id__in=[n.id for n in self.notes]).update(\n                note=F(\"note\") + \" - updated\"\n            )\n        for note in self.notes:\n            self.assertEqual(note.note, note.note + \" - updated\")\n\n\n", "    def test_related_objects_prefetch_one_level(self):\n        # We need a test that covers prefetch related objects at one level.\n        # This involves ensuring that a RelatedObject instance is correctly\n        # populated with its associated notes.\n\n        article = Article.objects.create(title=\"Some article\")\n        for i in range(5):\n            related_object = RelatedObject.objects.create(article=article)\n            Note.objects.create(related_object=related_object, note=f\"Note {i}\")\n\n        # Retrieve the article and prefetch related objects (This will\n        # populate the related_object._prefetched_objects_cache)\n        article = Article.objects.prefetch_related(\n            \"related_objects__notes\"\n        ).get(title=\"Some article\")\n\n        notes_from_prefetch = [\n            note for related_object in article.related_objects for note in related_object.notes.all()\n        ]\n\n        # Verify that the prefetched notes are present\n        self.assertEqual(len(notes_from_prefetch), 5)\n        self.assertEqual(\n            [note.note for note in notes_from_prefetch], [\"Note 0\", \"Note 1\", \"Note 2\", \"Note 3\", \"Note 4\"]\n        )\n\n\n", "    def test_bulk_update_note_tags(self):\n        tag_ids = [tag.id for tag in self.tags[:5]]\n        with self.assertNumQueries(1):\n            Note.objects.bulk_update(\n                self.notes[:5], [\"note\", \"tags__name\"], [(f\"{i} updated\", tag_ids) for i in range(5)]\n            )\n        for i in range(5):\n            self.assertEqual(self.notes[i].note, f\"{i} updated\")\n            self.assertEqual(set(tag.name for tag in self.notes[i].tags.all()), set(tag_ids))\n"], "sample_387": ["    def test_formfield_for_dbfield_safe_limit_choices(self):\n        # Test that unsafe limit choices are properly handled\n        ff = self.assertFormfield(UnsafeLimitChoicesTo, 'limit', forms.ChoiceField,\n                                  limit_choices_to=lambda obj: None)\n        self.assertEqual(ff.choices, [(None, \"None\")])\n\n\n        ff = self.assertFormfield(UnsafeLimitChoicesTo, 'limit', forms.ChoiceField,\n                                  limit_choices_to=lambda obj: [1, 2, 3])\n        self.assertEqual(ff.choices, [(1, \"1\"), (2, \"2\"), (3, \"3\")])\n\n\n        ff = self.assertFormfield(UnsafeLimitChoicesTo, 'limit', forms.ChoiceField,\n                                  limit_choices_to=[\"allowed1\", \"allowed2\"])\n        self.assertEqual(ff.choices, [(\"allowed1\", \"allowed1\"), (\"allowed2\", \"allowed2\")])\n\n\n\n", "    def test_formfield_for_dbfield_safefilefield(self):\n        ff = self.assertFormfield(MyFileField, 'file', forms.FileInput)\n\n\n", "    def test_formfield_for_dbfield_datefield(self):\n        ff = self.assertFormfield(Event, \"date\", widget=forms.DateField,\n                                 null=True)\n        self.assertEqual(ff.required, False)\n", "    def test_formfield_for_dbfield_myfilefield(self):\n        ff = self.assertFormfield(MyFileField, 'file', widgets.UploadedFile,\n                                  form_class=forms.ModelForm)\n        self.assertEqual(ff.required, False)\n        self.assertEqual(ff.label, 'File')\n        self.assertEqual(ff.help_text, None)\n\n\n\n", "    def test_formfield_for_dbfield_for_filefield(self):\n        ff = self.assertFormfield(MyFileField, 'file', forms.FileField)\n        self.assertEqual(ff.form_class, forms.ModelForm)\n        self.assertEqual(ff.show_hidden_initial, False)\n\n\n\n"], "sample_537": ["    def test_stride_windows_complex(self):\n        x = np.arange(100) + 1j * np.arange(100)\n        NFFT = 20\n        noverlap = 5\n        result = _stride_windows(x=x, NFFT=NFFT, noverlap=noverlap)\n        target = self.calc_window_target(x, NFFT, noverlap)\n        assert_array_almost_equal_nulp(result, target, 5) \n", "    def test_stride_windows_with_stride(self):\n        x = np.arange(100)\n        NFFT = 32\n        noverlap = 16\n        result = _stride_windows(x, NFFT, noverlap)\n        expected = self.calc_window_target(x, NFFT, noverlap)\n        assert_array_almost_equal_nulp(result, expected, 8)\n\n", "    def test_stride_windows_single(self, shape):\n        x = np.random.rand(*shape)\n        NFFT = 3\n        noverlap = 1\n        result = mlab._stride_windows(x, NFFT=NFFT, noverlap=noverlap)\n        target = self.calc_window_target(x, NFFT, noverlap)\n        assert_array_almost_equal_nulp(result, target, 5)\n\n", "    def test_stride_windows(self, shape):\n        x = np.arange(100).reshape(shape)\n        NFFT = 30\n        noverlap = 10\n        result = _stride_windows(x, NFFT, noverlap)\n        target = self.calc_window_target(x, NFFT, noverlap)\n        assert_array_almost_equal_nulp(result, target, 5)\n", "    def test_stride_windows(self):\n        x = np.random.rand(100)\n        NFFT = 20\n        noverlap = 5\n\n        # Test the `_stride_windows` function\n        result = mlab._stride_windows(x, NFFT, noverlap)\n        target = self.calc_window_target(x, NFFT, noverlap)\n\n        assert_array_almost_equal_nulp(result, target, 10,\n                                      err_msg='Windowing result mismatch')\n\n\n\n"], "sample_922": ["    def test_parse_annotation(self):\n        assert _parse_annotation(\"int\") == (\"int\", [])\n        assert _parse_annotation(\"int, str\") == (\"int, str\", [])\n        assert _parse_annotation(\"List[int]\") == (\"List[int]\", [])\n        assert _parse_annotation(\"List[int | str]\") == (\"List[int | str]\", [])\n        assert _parse_annotation(\"Optional[int]\") == (\"Optional[int]\", [])\n        assert _parse_annotation(\"int -> str\") == (\"int -> str\", [])\n\n        # Test for nested types\n        assert _parse_annotation(\"Dict[str, List[int]]\") == (\"Dict[str, List[int]]\", [])\n\n\n", "    def test_parse_annotation(self):\n        assert _parse_annotation('int') == ['int']\n        assert _parse_annotation('int | str') == ['int', 'str']\n        assert _parse_annotation('List[int]') == ['List', ['int']]\n        assert _parse_annotation('Optional[str]') == ['Optional', ['str']]\n        assert _parse_annotation('Union[int, float]') == ['Union', ['int', 'float']]\n\n\n\n", "    def test_parse_annotation(self):\n        assert _parse_annotation('int') == {'type': 'int', 'annotations': []}\n        assert _parse_annotation('list[int]') == {'type': 'list', 'annotations': [{'type': 'int'}]}\n        assert _parse_annotation('Optional[str]') == {'type': 'Optional', 'annotations': [{'type': 'str'}]}\n        assert _parse_annotation('Dict[str, int]') == {'type': 'Dict', 'annotations': [{'type': 'str'}, {'type': 'int'}]}\n\n\n", "    def test_parse_annotation(self):\n        test_cases = [\n            ('None', 'None'),\n            ('int', 'int'),\n            ('str', 'str'),\n            ('list', 'list'),\n            ('dict', 'dict'),\n            ('tuple', 'tuple'),\n            ('bool', 'bool'),\n            ('object', 'object'),\n            ('MyClass', 'MyClass'),\n            ('int | str', 'int | str'),\n            ('Union[int, str]', 'Union[int, str]'),\n            ('Optional[str]', 'Optional[str]'),\n            ('List[int]', 'List[int]'),\n            ('Dict[str, int]', 'Dict[str, int]'),\n            ('typing.List[int]', 'typing.List[int]'),\n            ('typing.Dict[str, int]', 'typing.Dict[str, int]'),\n            ('list[int]', 'list[int]'),\n            ('Optional[list[int]]', 'Optional[list[int]]'),\n            ('List[Dict[str, int]]', 'List[Dict[str, int]]')\n        ]\n        for arg, expected in test_cases:\n            result = _parse_annotation(arg)\n            assert result == expected\n", "    def test_parse_annotation(self):\n        assert _parse_annotation('int') == ('int', [])\n        assert _parse_annotation('list[int]') == ('list', [('int', [])])\n        assert _parse_annotation('Dict[str, int]') == ('Dict', [('str', []), ('int', [])])\n        assert _parse_annotation('Optional[str]') == ('Optional', [('str', [])])\n        assert _parse_annotation('Union[str, int]') == ('Union', [('str', []), ('int', [])])\n        assert _parse_annotation('typing.List[str]') == ('List', [('str', [])])\n        assert _parse_annotation('None') == ('NoneType', [])\n        assert _parse_annotation('Tuple[int, float]') == ('Tuple', [('int', []), ('float', [])])\n\n"], "sample_591": ["    def test_merge_with_shared_coords(self):\n\n        ds1 = create_test_data(\n            {'a': (['x', 'y'], np.random.rand(2, 3))},\n            coords={'x': [1, 2], 'y': [3, 4]},\n        )\n        ds2 = create_test_data(\n            {'a': (['x', 'y'], np.random.rand(2, 3))},\n            coords={'x': [1, 2], 'y': [3, 4]},\n        )\n\n        merged = merge(ds1, ds2, compat='override')\n        assert_identical(merged.get_index('x'), ds1.x)\n        assert_identical(merged.get_index('y'), ds1.y)\n\n\n\n", "    def test_merge_reduce_with_no_overlap(self):\n        # Test merging datasets with no overlap in dimensions\n        ds1 = create_test_data(\n            dims=(\"x\", \"y\"),\n            coords={\"x\": [0, 1], \"y\": [0, 1]},\n            data=np.arange(4).reshape(2, 2),\n        )\n        ds2 = create_test_data(\n            dims=(\"x\", \"z\"),\n            coords={\"x\": [2, 3], \"z\": [0, 1]},\n            data=np.arange(4).reshape(2, 2),\n        )\n\n        merged = merge([ds1, ds2], compat=\"override\")\n        assert merged.dims == {\"x\": [0, 1, 2, 3], \"y\": [0, 1], \"z\": [0, 1]}\n\n        expected = create_test_data(\n            dims=(\"x\", \"y\", \"z\"),\n            coords={\"x\": [0, 1, 2, 3], \"y\": [0, 1], \"z\": [0, 1]},\n            data=np.concatenate(\n                (\n                    np.concatenate(\n                        (ds1.data, np.zeros((2, 2))), axis=1\n                    ),\n                    np.concatenate(\n                        (np.zeros((2, 2)), ds2.data), axis=1\n                    ),\n                ),\n                axis=0,\n            ),\n        )\n        assert_identical(merged, expected)\n\n\n", "    def test_merge_concat_with_existing_indexes(self):\n        ds1 = create_test_data()\n        ds2 = create_test_data()\n        ds2 = ds2.rename({\"x\": \"new_x\"})\n        merged = xr.merge([ds1, ds2])\n        assert merged.indexes[\"x\"].size == ds1.indexes[\"x\"].size\n        assert merged.indexes[\"new_x\"].size == ds2.indexes[\"new_x\"].size\n\n", "    def test_merge_in_place(self):\n        ds1 = create_test_data()\n        ds2 = create_test_data()\n        ds1.name = \"ds1\"\n        ds2.name = \"ds2\"\n        merged = merge([ds1, ds2], compat=\"override\").copy()\n\n        ds1._merge_in_place(merged)\n\n        assert_identical(ds1, merged)\n        assert ds1.name == \"ds1\"\n        assert ds2.name == \"ds2\"\n\n\n", "    def test_merge_compat_with_coords(self):\n        ds1 = create_test_data(\n            data_vars={\"x\": ((\"y\", \"time\"), np.arange(6).reshape(2, 3))},\n            coords={\"y\": [1, 2], \"time\": [\"a\", \"b\", \"c\"]},\n        )\n        ds2 = create_test_data(\n            data_vars={\"x\": ((\"y\", \"time\"), np.arange(6).reshape(2, 3) + 10)},\n            coords={\"y\": [1, 2], \"time\": [\"a\", \"b\", \"c\"]},\n        )\n        merged = merge(ds1, ds2, compat=\"override\")\n        assert_identical(merged, ds2)\n"], "sample_173": ["    def test_integer_field_range(self):\n        self.assertEqual(self.ops.integer_field_range('AutoField'), (-2147483648, 2147483647))\n        self.assertEqual(self.ops.integer_field_range('SmallAutoField'), (-32768, 32767))\n        self.assertEqual(self.ops.integer_field_range('IntegerField'), (-2147483648, 2147483647))\n        self.assertEqual(self.ops.integer_field_range('BigIntegerField'), (-9223372036854775808, 9223372036854775807))\n        self.assertEqual(self.ops.integer_field_range('PositiveIntegerField'), (0, 2147483647))\n        self.assertEqual(self.ops.integer_field_range('PositiveSmallIntegerField'), (0, 32767))\n        self.assertEqual(self.ops.integer_field_range('SmallAutoField'), (-32768, 32767))\n        ", "    def test_datetime_extract_sql_timezone_aware(self):\n        with self.assertRaises(ValueError):\n            self.ops.year_extract_sql('year', timezone.datetime(2023, 10, 26, 10, 0, 0, tzinfo=timezone.get_current_timezone()))\n", "    def test_subtract_temporals_not_supported(self):\n        with self.assertRaisesMessage(NotSupportedError,\n                                    self.may_require_msg % 'subtract_temporals()'):\n            self.ops.subtract_temporals(\n                internal_type='IntegerField',\n                lhs=(\n                    'SELECT 1 FROM my_table',\n                    (),\n                ),\n                rhs=(\n                    'SELECT 2 FROM my_table',\n                    (),\n                ),\n            )\n", "    def test_integer_field_range(self):\n        self.assertEqual(self.ops.integer_field_range('SmallIntegerField'), (-32768, 32767))\n        self.assertEqual(self.ops.integer_field_range('IntegerField'), (-2147483648, 2147483647))\n        self.assertEqual(self.ops.integer_field_range('BigIntegerField'), (-9223372036854775808, 9223372036854775807))\n        self.assertEqual(self.ops.integer_field_range('PositiveIntegerField'), (0, 2147483647))\n        self.assertEqual(self.ops.integer_field_range('PositiveSmallIntegerField'), (0, 32767))\n        self.assertEqual(self.ops.integer_field_range('AutoField'), (-2147483648, 2147483647))\n        self.assertEqual(self.ops.integer_field_range('BigAutoField'), (-9223372036854775808, 9223372036854775807))\n", "    def test_window_frame_start_end_without_support(self):\n        with self.assertRaisesMessage(NotSupportedError,\n                                     'This backend does not support window expressions.'):\n            self.ops.window_frame_start(1)\n        with self.assertRaisesMessage(NotSupportedError,\n                                     'This backend does not support window expressions.'):\n            self.ops.window_frame_end(2)\n        with self.assertRaisesMessage(NotSupportedError,\n                                     'This backend does not support window expressions.'):\n            self.ops.window_frame_rows_start_end(start=1, end=2)\n        with self.assertRaisesMessage(NotSupportedError,\n                                     'This backend does not support window expressions.'):\n            self.ops.window_frame_range_start_end(start=1, end=2)\n\n\n\n"], "sample_750": ["    def test_omp_gram_gram_same(self):\n        # Test that orthogonal_mp_gram returns the same output as orthogonal_mp\n        # when provided the same gram matrix and Xy\n        coef_omp, _, _ = orthogonal_mp(X, y, n_nonzero_coefs=3)\n        _, coef_omp_gram, _ = orthogonal_mp_gram(G, Xy, n_nonzero_coefs=3,\n                                                  copy_Gram=True, norm_squared=None)\n        assert_array_almost_equal(coef_omp, coef_omp_gram, verbose=2)\n", "    def test_omp_gram_vs_omp(self):\n        np.random.seed(0)\n        # Same result with and without precomputing Gram and Xy\n        coef_omp = orthogonal_mp(X, y, n_nonzero_coefs=3, tol=0)\n        coef_omp_gram = orthogonal_mp_gram(G, Xy, n_nonzero_coefs=3, tol=0)\n        assert_array_almost_equal(coef_omp, coef_omp_gram)\n", "    def test_orthogonal_mp_gram_precomputed_gram(self):\n        coefs_omp, _, n_iter = orthogonal_mp_gram(G, Xy, n_nonzero_coefs=2,\n                                                   tol=1e-4,\n                                                   copy_Gram=False,\n                                                   copy_Xy=False)\n        coefs_omp_copy, _, _ = orthogonal_mp_gram(G, Xy, n_nonzero_coefs=2,\n                                                   tol=1e-4)\n        assert_array_equal(coefs_omp, coefs_omp_copy)\n", "    def test_omp_gram_path_with_tol(self):\n        # Tests OMP_Gram with a fixed tolerance\n        n_nonzero_coefs = 3\n        tol = 1e-5\n\n        coefs, n_iter = orthogonal_mp_gram(\n            G, Xy=Xy, n_nonzero_coefs=n_nonzero_coefs, tol=tol,\n            copy_Gram=True, copy_Xy=True, return_n_iter=True)\n        assert_array_almost_equal(coefs.shape, (n_nonzero_coefs, n_targets))\n        assert_equal(n_iter.shape, (n_targets,))\n\n        # Check that n_iter is reasonable\n        assert_true(np.all(n_iter * \n                          np.isnan(n_iter) == 0)) \n\n", "    def test_omp_gram_vs_omp(self):\n        X_ = np.copy(X)\n        y_ = np.copy(y)\n        coef_omp, _, _ = orthogonal_mp(X_, y_, n_nonzero_coefs=3)\n        coef_omp_gram, _, _ = orthogonal_mp_gram(G, Xy, n_nonzero_coefs=3)\n        assert_array_almost_equal(coef_omp, coef_omp_gram)\n\n"], "sample_1108": ["    def test_generate_oriented_forest_simple():\n        from sympy.utilities.iterables import generate_oriented_forest\n        for forest in generate_oriented_forest(4):\n            assert len(forest) == 4\n            assert all(0 <= i < 4 for i in forest)\n            assert all(forest[i] != i for i in range(4))\n", "    def test_variations():\n        assert variations(range(3), 2) == [(0, 1), (0, 2), (1, 2)]\n        assert variations(range(4), 3) == [(0, 1, 2), (0, 1, 3), (0, 2, 3), (1, 2, 3)]\n        assert variations((1, 2), 2) == [(1, 2), (2, 1)]\n        assert variations([1, 2, 3], 3, repetition=True) == [(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 2, 1), (1, 2, 2), (1, 2, 3), (1, 3, 1), (1, 3, 2), (1, 3, 3), (2, 1, 1), (2, 1, 2), (2, 1, 3), (2, 2, 1), (2, 2, 2), (2, 2, 3), (2, 3, 1), (2, 3, 2), (2, 3, 3), (3, 1, 1), (3, 1, 2), (3, 1, 3), (3, 2, 1), (3, 2, 2), (3, 2, 3), (3, 3, 1), (3, 3, 2), (3, 3, 3)]\n", "    def test_partitions():\n        for n, k in [(5, 2), (3, 2), (4, 3), (6, 2)]:\n            for p in partitions(n, k):\n                assert sum(p) == n\n                assert len(p) == k\n        with raises(ValueError):\n            partitions(5, 0)\n        with raises(ValueError):\n            partitions(5, 6)\n        assert partitions(0, 0) == [[]]\n", "    def test_perms_signs():\n        assert list(permute_signs((0, 1, 2))) == [\n            (0, 1, 2), (0, -1, 2), (0, 1, -2), (0, -1, -2)\n        ]\n        assert list(permute_signs((1, -1, 1))) == [\n            (1, -1, 1), (-1, 1, 1), (1, -1, -1), (-1, -1, 1)\n        ]\n        assert list(permute_signs((1, 2, 3, 0))) == [\n            (1, 2, 3, 0), (1, 2, -3, 0), (1, -2, 3, 0),\n            (1, -2, -3, 0), (1, 2, 3, 0), (-1, 2, 3, 0),\n            (-1, 2, -3, 0), (-1, -2, 3, 0),\n            (-1, -2, -3, 0)\n        ]\n", "    def test_perms_from_string():\n        for s in [\"ABCDE\", \"ABAB\", \"AAA\"]:\n            for perm in permutations(s):\n                yield permutations(s), perm\n        for s in [\"ABC\", \"ABAB\"]:\n            for perm in permutations(s):\n                yield permutations(s), perm"], "sample_618": ["    def test_unify_chunks(data, expected_data):\n        inputs = data\n        output = unify_chunks(*inputs)\n        if isinstance(inputs[0], DataArray):\n            assert output[0].chunks == expected_data[0].chunks\n        else:\n            assert output[", "def test_broadcast_compat_data(", "def test_unified_dim_sizes():\n    \"\"\"\n    Test unified_dim_sizes\n    \"\"\"\n    assert unified_dim_sizes(None, None) == {}\n    assert unified_dim_sizes({}, {}) == {}\n    assert unified_dim_sizes([], []) == {}\n\n    sizes = {'x': [3, 4], 'y': [2, 2, 6]}\n    assert unified_dim_sizes(sizes, {}) == sizes\n    assert unified_dim_sizes({}, sizes) == sizes\n\n    sizes1 = {'x': [3, 4], 'y': [2, 2, 6]}\n    sizes2 = {'x': [3, 4], 'y': [2, 6]}\n    assert unified_dim_sizes(sizes1, sizes2) == {'x': [3, 4], 'y': [6]}\n\n    sizes1 = {'x': [3, 4], 'y': [2, 2, 6]}\n    sizes2 = {'x': [12], 'y': [2, 2, 6]}\n    with pytest.raises(ValueError):\n        unified_dim_sizes(sizes1, sizes2)\n", "    def test_polyval_simple(input_arrays: list[DataArray]):\n        coord, coeffs = input_arrays\n        \n        result = polyval(coord, coeffs, degree_dim=\"x\")\n        expected = coeffs.isel({\"x\": coord.data - 1} ) * (coord - 1)\n        assert_identical(result.data, expected.data) \n\n", "    def test_unified_chunks_single_variable():\n        da = xr.DataArray(np.random.rand(5, 5), dims=(\"x\", \"y\"), chunks={\"x\": 2, \"y\": 3})\n        da2 = xr.DataArray(np.random.rand(5, 5), dims=(\"x\", \"y\"), chunks={\"x\": 2, \"y\": 3})\n        unified = unify_chunks(da, da2)\n        assert isinstance(unified[0], xr.DataArray)\n        assert isinstance(unified[1], xr.DataArray)\n        assert unified[0].chunks == da.chunks\n        assert unified[1].chunks == da2.chunks\n"], "sample_308": ["    def test_format_with_timezone(self):\n        dt = datetime(2003, 10, 7, 11, 39)\n        format_string = \"jS F Y H:i O\"\n        expected_result = \"7th October 2003 11:39 +0200\"\n        self.assertEqual(format(dt, format_string), expected_result)\n\n\n", "    def test_time_format(self):\n        now = datetime.now(tz=get_default_timezone())\n        formatted = time_format(now, '%H:%M:%S.%f')\n        self.assertIsInstance(formatted, str)\n        self.assertTrue(len(formatted) > 0)\n", "    def test_format_timezones(self):\n        naive_dt = datetime(2023, 10, 26, 14, 0, 0)\n        aware_dt = make_aware(naive_dt, tz=get_default_timezone())\n\n        # Test with a naive datetime object\n        self.assertEquals(format(naive_dt, 'T'), format_datetime_rfc5322(naive_dt))\n\n        # Test with an aware datetime object\n        self.assertEquals(format(aware_dt, 'T'), format_datetime_rfc5322(aware_dt))\n\n", "    def test_format_rfc5322(self):\n        dt_naive = datetime(2023, 10, 27, 10, 30, 0)\n        dt_aware = make_aware(dt_naive, timezone=get_default_timezone())\n        self.assertEqual(format(dt_naive, 'r'), format(dt_aware, 'r'))\n        self.assertEqual(format(dt_naive, 'r'), format_datetime_rfc5322(dt_naive))\n        self.assertEqual(format(dt_aware, 'r'), format_datetime_rfc5322(dt_aware))\n", "    def test_format_rfc5322(self):\n        dt = datetime(2000, 12, 21, 16, 1, 7)\n        self.assertEqual(format(dt, 'r'), 'Thu, 21 Dec 2000 16:01:07 +0200')\n"], "sample_135": ["    def test_rfc5322_formatting(self):\n        now = datetime.now(get_default_timezone())\n        rfc5322_format = format(now, 'r')\n        assert rfc5322_format.startswith('Mon, ')  # Check for weekday\n        assert rfc5322_format.endswith('CET')  # Check for timezone abbreviation\n\n\n\n", "    def test_time_format_utc_offset_naive_datetime(self):\n        naive_dt = datetime(2023, 10, 26, 10, 30, 0)\n        tf = TimeFormat(naive_dt)\n        self.assertEqual(tf.O(), '+0000')\n", "    def test_format_with_time_zone(self):\n        now = datetime.now(get_default_timezone())\n        formatted_date = format(now, 'r')\n        self.assertIsInstance(formatted_date, str)\n        self.assertRegex(formatted_date, r\"^\\w+, \\d{2} \\w{3} \\d{4} \\d{2}:\\d{2}:\\d{2} \\+\\d{4}$\")\n\n", "    def test_time_format(self):\n        dt = datetime(2023, 10, 26, 15, 30, 45, 123456)\n        self.assertEqual(time_format(dt, '%a %b %d %H:%M:%S %Z'), 'Thu Oct 26 15:30:45 CET')\n", "    def test_format_rfc5322_naive(self):\n        naive_dt = datetime(2008, 1, 2, 10, 30, 0)\n        formatted_dt = format(naive_dt, 'r')\n        expected_dt = ('Mon, 02 Jan 2008 10:30:00 +0100')\n        self.assertEqual(formatted_dt, expected_dt)\n"], "sample_1051": ["compilation error", "compilation error", "compilation error", "    def test_dotprint_pow():\n        from sympy.functions import pow\n        p = pow(x, 2)\n        result = dotprint(p)\n        assert \"Pow(Symbol('x'), Integer(2))\" in result\n        assert \"Symbol('x')_(0,)\" in result\n        assert \"Integer(2)_(1,)\" in result\n        assert 'Pow(Symbol(\"x\"), Integer(2))_()\" -> \"Symbol(\"x\")_(0,)';' in result\n        assert 'Pow(Symbol(\"x\"), Integer(2))_()\" -> \"Integer(2)_(1,)';' in result\n", "compilation error"], "sample_125": ["    def test_delete_cookie(self):\n        response = HttpResponse()\n        response.set_cookie('test', 'value')\n        self.assertIn('test', response.cookies)\n\n        response.delete_cookie('test')\n        self.assertNotIn('test', response.cookies)\n", "    def test_set_cookie_expires_datetime(self):\n        response = HttpResponse()\n        with freeze_time(datetime(2024, 1, 1, 1, 0, 0, tzinfo=utc)):\n            response.set_cookie('cookie1', 'value1', expires=datetime.utcnow() + timedelta(days=365))\n        self.assertEqual(response.cookies['cookie1']['expires'], http_date(1672572400))\n", "    def test_set_cookie_with_max_age(self):\n        response = HttpResponse()\n        response.set_cookie('key', 'value', max_age=100)\n        self.assertDictEqual(response.cookies['key'].items(), [\n            ('max-age', '100'),\n        ])\n\n        response = HttpResponse()\n        response.set_cookie('key', 'value', max_age=timedelta(seconds=100))\n        self.assertDictEqual(response.cookies['key'].items(), [\n            ('max-age', '100'),\n        ])\n\n        ", "    def test_set_cookie_with_expires_datetime(self):\n        response = HttpResponse()\n\n        now = datetime.now(utc)\n        expires = now + timedelta(days=1)\n        response.set_cookie('test_cookie', 'value', expires=expires)\n        response_cookie = cookies.SimpleCookie()\n        response_cookie.load(response.serialize().decode('utf-8'))\n        self.assertEqual(response_cookie['test_cookie']['expires'].value, http_date(expires.timestamp()))\n", "    def test_set_cookie_with_datetime_expires(self):\n        response = HttpResponse()\n        with freeze_time(datetime.utcnow() + timedelta(days=1)):\n            response.set_cookie('test_cookie', 'test_value', expires=datetime.utcnow() + timedelta(days=1))\n        self.assertIn('test_cookie', response.cookies)\n        cookie = response.cookies['test_cookie']\n        self.assertEqual(cookie.value, 'test_value')\n        self.assertEqual(cookie.max_age, 86400)\n        self.assertEqual(cookie.expires, http_date(datetime.utcnow() + timedelta(days=1)))\n"], "sample_1121": ["    def test_nseries_even_order():\n        r = (2*x + 3*x**2).nseries(x, n=4, logx=False)\n        assert r == S(2*x) + S(3*x**2) + Order(x**4)\n        r = ((2*x + 3*x**2)**2).nseries(x, n=3, logx=False)\n        assert r == 4*x**2 + 12*x**3 + 9*x**4 + Order(x**5)\n", "    def test_product_with_float():\n        assert prod([S(2), 3.0]) == 6.0\n        assert type(prod([S(2), 3.0])) is float\n        assert prod(range(3), 1.0) == 0.0\n\n\n\n", "compilation error", "    def test_as_content_primitive_with_symbol():\n        from sympy.core.mul import _keep_coeff\n        assert ((x*sin(x).as_content_primitive(radical=True))[0] == 1,\n                (x*sin(x).as_content_primitive(radical=True)))\n        assert ((x*sin(x)*y).as_content_primitive(radical=True))[0] == 1\n        assert ((2*x+(x*y)).as_content_primitive(radical=True))[0] == 2\n        assert ((2*x*y).as_content_primitive(radical=True))[0] == 2\n        assert ((2*x*3*y).as_content_primitive(radical=True))[0] == 6\n        assert ((2*x*3*y).as_content_primitive(radical=True))[1] == (x*y) \n\n", "    def test_as_content_primitive():\n        from sympy.core.mul import _keep_coeff\n        from sympy.core import (\n            Symbol, S, Rational, Integer, pi, oo\n        )\n        expr = 2*x*sqrt(x)*sin(x)\n        a, b, c, d = sqrt(x), 2*x, sin(x), Integer(2)\n        res = expr.as_content_primitive()\n        assert res[0] == 2\n        assert res[1] == a*b*c \n        # test with radical=True\n        expr = 2*sqrt(x)*sqrt(x)*sin(x)\n        res = expr.as_content_primitive(radical=True)\n        assert res[0] == 2\n        assert res[1] == a*b*c\n        # test with clear=False\n        expr = 2*x*sqrt(x)*sin(x)\n        res = expr.as_content_primitive(clear=False)\n        assert res[0] == 2\n        assert res[1] == a*b*c\n    "], "sample_1019": ["    def test_factor_nc_mixed():\n        A, B = symbols('A B', commutative=False)\n        expr = (A*B + B*A)*x* (A**2 - B**2)\n        f1 = factor_nc(expr)\n        f2 = (2*A*B)*x*(A + B)*(A - B)\n        assert f1.expand() == f2.expand()\n\n", "    def test_factor_nc_non_single_symbol():\n        A, B, C = symbols('A,B,C', commutative=False)\n        eq = (x*A + x*C*B)**2\n        f = factor_nc(eq)\n        e = _pemexpand(eq)\n        assert f.equals(e), f\n        \n", "    def test_factor_nc_symbol_replacement():\n        A = Symbol('A', commutative=False)\n        B = Symbol('B', commutative=False)\n        expr = ((x + A)*(x + B)).expand()\n        expr, rep, nc_symbols = _mask_nc(expr)\n        assert set(rep.keys()) == {A, B}\n        result = factor_nc(expr).subs(rep)\n        assert result == (x + A)*(x + B)\n", "    def test_factor_nc_power():\n        A = Symbol('A', commutative=False)\n        B = Symbol('B', commutative=False)\n\n        expr = (A**2 + 2*A*B + B**2).expand()\n\n        result = factor_nc(expr)\n        assert result == (A + B)**2\n\n", "    def test_factor_nc_gcd():\n        from sympy.physics.secondquant import Commutator, NO\n        from sympy.core import Basic, Symbol\n        A, B = Symbol('A', commutative=False), Symbol('B', commutative=False)\n        eq1 = (A**2 + 2*A*B + B**2)\n        eq2 = ((A + B)*(A + B))\n        eq3 = (NO(A)**2 + 2*NO(A)*NO(B) + NO(B)**2)\n        assert factor_nc(eq1) == (A + B)**2\n        assert factor_nc(eq2) == (A + B)*(A + B)\n        assert factor_nc(eq3) == (NO(A) + NO(B))**2\n\n\n\n"], "sample_1114": ["    def test_complex_region_from_real(self):\n        from sympy import Interval, ComplexRegion\n\n        unit = Interval(0,1)\n        result = ComplexRegion.from_real(unit)\n        expected = CartesianComplexRegion(ProductSet(Interval(0, 1), FiniteSet(0)))\n        self.assertEqual(result, expected)\n", "    def test_complex_region_from_real_finite_set(self):\n        from sympy import FiniteSet, Interval\n        for f in [FiniteSet(1, 2), FiniteSet(1, 2, 3),\n                  FiniteSet(1, 0, -1)]:\n            region = ComplexRegion.from_real(Interval(0, 1) * f)\n            expected = FiniteSet(complex(x, y) for x, y in itertools.product(f, [0]))\n            self.assertEqual(region, expected)\n", "    def test_complex_region_from_real(self):\n        from sympy import Interval, ComplexRegion\n        # Test with a single interval\n        self.assertEqual(ComplexRegion.from_real(Interval(0, 1)),\n                         CartesianComplexRegion(ProductSet(Interval(0, 1), FiniteSet(0))))\n        # Test with multiple intervals\n        self.assertEqual(ComplexRegion.from_real(Interval(0, 1) * Interval(2, 3)),\n                         CartesianComplexRegion(\n                             ProductSet(Interval(0, 1), Interval(2, 3))))\n        # Test with different types\n        self.assertEqual(ComplexRegion.from_real(FiniteSet(0, 1) * Interval(0, 1)),\n                         FiniteSet(0 + S.ImaginaryUnit * 0, 1 + S.ImaginaryUnit * 0))\n\n\n\n", "    def test_complexregion_from_real_interval_set(self):\n        from sympy.sets.sets import Interval\n        a = Interval(0, 1)\n        b = Interval(2, 3)\n        c = ComplexRegion.from_real(a)\n        d = ComplexRegion.from_real(b)\n        self.assertEqual(c.sets, ProductSet(a, FiniteSet(0)))\n        self.assertEqual(d.sets, ProductSet(b, FiniteSet(0)))\n", "    def test_complex_region_from_real(self):\n        from sympy import Interval\n\n        unit = Interval(0, 1)\n        cr = ComplexRegion.from_real(unit)\n        expected = CartesianComplexRegion(ProductSet(Interval(0, 1), FiniteSet(0)))\n        self.assertEqual(cr, expected)\n\n"], "sample_663": ["    def test_no_collection_empty_args(self):\n        with patch('builtins.print') as mock_print:\n            session = Session(\n                pytest.config\n            )\n            session.perform_collect(args=[])\n            assert mock_print.call_count == 0\n", "    def test_conf_cutdir(self, tmpdir):\n        py.path.local.syspath.append(tmpdir)\n        try:\n            a = tmpdir.join(\"a\")\n            a.ensure(dir=True)\n            b = tmpdir.join(\"a\").join(\"b\")\n            b.ensure(dir=True)\n            c = tmpdir.join(\"a\").join(\"c\")\n            c.ensure(dir=True)\n            a.join(\"__init__.py\").write(\"\")\n            b.join(\"__init__.py\").write(\"\")\n            c.join(\"__init__.py\").write(\"\")\n\n            config = pytest.main([\"--confcutdir=a\", \"-v\"])\n\n            assert config == ExitCode.OK\n        finally:\n            py.path.local.syspath.remove(tmpdir)\n\n", "    def test_collect_ignore(self, monkeypatch):\n        from _pytest.main import pytest_ignore_collect, pytest_ignore_collect_glob\n\n            return True\n\n        monkeypatch.setattr(\"_pytest.main\", \"pytest_ignore_collect\", mock_ignore_collect)\n        monkeypatch.setattr(\n            \"_pytest.main\", \"pytest_ignore_collect_glob\", mock_ignore_collect_glob\n        )\n\n        config = _pytest.main.Config.parse_config(\n            [\"--ignore\", \"tests/fixtures\", \"--collect-in-virtualenv\"],\n            rootdir=os.path.join(\"tests\"),\n        )\n        collector = _pytest.main.Session(config)\n\n        path = config.rootdir.join(\"test_ignore.py\")\n        assert not collector._ignore_collect_hook(path)\n\n\n\n        path = config.rootdir.join(\"tests/fixtures/test_ignore.py\")\n        assert collector._ignore_collect_hook(path)\n        path = config.rootdir.join(\"tests/fixtures/ignore/test_ignore.py\")\n        assert collector._ignore_collect_hook(path)\n", "    def test_ignore_collect_paths(self, tmpdir):\n        p = tmpdir.join(\"some_path\")\n        p.mkdir()\n        (p.join(\"test_file.py\")).write(\n            textwrap.dedent(\n                \"\"\"\n                pass\n            \"\"\"\n            )\n        )\n\n        # Test ignore_collect_paths with both patterns and direct path arguments\n        config = pytest.config\n        config.option.norecursedirs = [str(p.dirpath())]\n        config.option.ignore = [str(p.dirpath())]\n\n        expected_paths = [str(p.join(\"test_file.py\"))]\n        nodes = Session._collect_item_for_paths(config, expected_paths)\n        assert len(nodes) == 0\n", "    def test_collect_ignore_collect(self, tmp_path):\n        config = pytest.config.Config(\n            invocation_path=None,\n            setup_collect=None,\n            rootdir=tmp_path,\n            pluginmanager=pytest.pluginmanager,\n            option=pytest.Option({}, \"pytest\"),\n            args=[],\n        )\n        config._getconftest_pathlist = lambda x, y: [tmp_path.join(\"ignore.py\")]\n        config.pluginmanager._duplicatepaths = set()\n\n        with open(tmp_path.join(\"ignore.py\"), \"w\") as f:\n            f.write(\n                \"\"\"\n                return [\"*.py\"]\n            \"\"\"\n            )\n\n            return [\"*.py\"]\n\n        config.pluginmanager._getconftest_pathlist = _collect_ignore_glob\n\n        config.option.ignore_glob = [\"*.py\"]\n        with tmp_path.join(\"test.txt\").open(\"w\") as f:\n            pass\n\n        with tmp_path.join(\"nonexistent.py\").open(\"w\") as f:\n            pass\n\n        with tmp_path.join(\"my_test.py\").open(\"w\") as f:\n            pass\n\n        path = tmp_path.join(\"my_test.py\")\n        collector = Session.pytest_ignore_collect(path, config)\n        assert collector is True\n\n"], "sample_505": ["    def test_date_converter(self, dt_type):\n        with rc_context(date={'converter': 'concise'}):\n            converter = units.registry[dt_type]\n            axisinfo = converter.axisinfo(unit=dateutil.tz.tzlocal(), axis=0)\n            assert isinstance(axisinfo.majloc, mdates.ConciseDateLocator)\n            assert isinstance(axisinfo.majfmt, mdates.ConciseDateFormatter)\n", "    def test_date2num_with_timezone(dt, expected):\n        dt = dt.replace(tzinfo=dateutil.tz.tzutc())\n        with rc_context(\n            {'date.converter': 'auto', 'date.interval_multiples': True}\n        ):\n            result = mdates.date2num(dt)\n        assert np.isclose(result, expected)\n\n", "    def test_concise_date_formatter(self, format_str):\n        with rc_context(\n                {'date.converter': 'concise',\n                 'date.formats': [format_str]}):\n            fig, ax = plt.subplots()\n            ax.plot(np.arange(10), np.arange(10))\n            ax.xaxis.set_major_formatter(mdates.ConciseDateFormatter(\n                mdates.AutoDateLocator(),\n                formats=[format_str]))\n            fig.canvas.draw()\n            ax = fig.axes[0]\n\n            # Ensure the formatter is applied correctly\n            ticks_ticks = ax.xaxis.get_major_ticks()\n            for tick in ticks_ticks:\n                timestamp = tick.get_text()\n                try:\n                    datetime.datetime.strptime(timestamp, format_str)\n                except ValueError:\n                    raise AssertionError(f\"Formatter output '{timestamp}' doesn't match expected format {format_str}\")\n", "compilation error", "    def test_date_converter_format(dt, expected):\n        with rc_context(date={\"converter\": \"concise\",\n                              \"formats\": ['%Y-%m-%d']}):\n            converter = mdates._SwitchableDateConverter()\n            assert converter.convert(dt) == expected\n\n\n\n"], "sample_1110": ["    def test_piecewise(self):\n        expr = Piecewise((x, x < 0), (y, x >= 0))\n        code = pycode(expr, standard='python3')\n        expected = \"\"\"", "    def test_piecewise_with_default(self):\n        expr = Piecewise((x > 0, x, (x < 0, -x, True)))\n        result = pycode(expr, standard='python3')\n        assert result == 'numpy.select(array([x > 0, x < 0]), array([x, -x]), default=0)'\n\n\n", "    def test_special_functions_scipy_special(self):\n        with self.settings(codegen_library='scipy'):\n            printer = SciPyPrinter()\n            self.assertEqual(printer.doprint(sympy.special.erf(x)), 'scipy.special.erf(x)')\n            self.assertEqual(printer.doprint(sympy.special.erfc(x)), 'scipy.special.erfc(x)')\n\n            self.assertEqual(printer.doprint(sympy.special.gamma(x)), 'scipy.special.gamma(x)')\n            self.assertEqual(printer.doprint(sympy.special.loggamma(x)), 'scipy.special.gammaln(x)')\n            self.assertEqual(\n                printer.doprint(sympy.special.psi(x)),\n                'scipy.special.psi(x)'\n            )\n            self.assertEqual(\n                printer.doprint(sympy.special.beta(x, y)),\n                'scipy.special.beta(x, y)'\n            )\n            self.assertEqual(\n                printer.doprint(sympy.special.lambertw(x)),\n                'scipy.special.lambertw(x)'\n            )\n\n", "    def test_sympy_printing(self):\n        expr = z**Rational(1, 2)\n        self.assertEqual(pycode(expr, standard='python3'), 'math.sqrt(z)')\n        self.assertEqual(pycode(expr, standard='python2'), 'z**(1/2)')\n\n        expr = sqrt(x + 1)\n        self.assertEqual(pycode(expr, standard='python3'), 'math.sqrt(x + 1)')\n        self.assertEqual(pycode(expr, standard='python2'), 'x**(1/2) + 1')\n\n        expr = Piecewise((x, x < 0), (x**2, x >= 0))\n        self.assertEqual(pycode(expr, standard='python3'), 'numpy.select([x < 0, x >= 0], [x, x**2])')\n        self.assertEqual(pycode(expr, standard='python2'), 'if x < 0:\\n  return x\\nelse:\\n  return x**2')\n\n        expr = And(x > 0, y < 1)\n        self.assertEqual(pycode(expr, standard='python3'), 'numpy.logical_and(x > 0, y < 1)')\n        self.assertEqual(pycode(expr, standard='python2'), '(x > 0) and (y < 1)')\n\n        expr = Or(x > 0, y < 1)\n        self.assertEqual(pycode(expr, standard='python3'), 'numpy.logical_or(x > 0, y < 1)')\n        self.assertEqual(pycode(expr, standard='python2'), '(x > 0) or (y < 1)')\n\n        expr = Not(x > 0)\n        self.assertEqual(pycode(expr, standard='python3'), 'numpy.logical_not(x > 0)')\n        self.assertEqual(pycode(expr, standard='python2'), 'not (x > 0)')\n\n        expr = sin(x)\n        self.assertEqual(pycode(expr, standard='python3'), 'numpy.sin(x)')\n        self.assertEqual(pycode(expr, standard='python2'), 'math.sin(x)')\n\n        ", "    def test_kronecker_delta(self):\n        self.assertEqual(pycode(KroneckerDelta(x, y), sympy_printer=SymPyPrinter), 'sympy.KroneckerDelta(x, y)')\n"], "sample_172": ["    def test_my_file_field(self):\n        ff = self.assertFormfield(MyFileField, 'file', widgetclass=forms.FileInput)\n\n        self.assertEqual(ff.help_text, 'Upload a file.')\n        self.assertEqual(ff.initial, None)\n        self.assertEqual(ff.required, True)\n\n\n\n", "    def test_formfield_for_date_field(self):\n        ff = self.assertFormfield(Event, 'date', widgets.AdminDateWidget)\n\n\n\n", "    def test_formfield_for_dbfield_custom_widget(self):\n        class MyModel(models.Model):\n            my_field = MyFileField(null=True, blank=True)\n\n        class MyModelAdmin(admin.ModelAdmin):\n            formfield_overrides = {\n                MyModel.my_field: {'widget': forms.FileInput(attrs={'class': 'custom-file-input'})},\n            }\n\n        ff = self.assertFormfield(MyModel, 'my_field', forms.FileInput,\n                                   admin_overrides={'formfield_overrides': {}})\n        self.assertEqual(ff.widget.attrs['class'], 'custom-file-input')\n", "    def test_formfield_for_dbfield_with_custom_widget(self):\n        \"\"\"\n        Test that formfield_for_dbfield honors custom widgets defined\n        on the model.\n        \"\"\"\n        ff = self.assertFormfield(MyFileField, 'myfile', MyFileWidget)\n\n\n\n", "    def test_formfield_for_dbfield_datetime_with_tz(self):\n        ff = self.assertFormfield(Event, 'date', widgets.DateTimeInput,\n                                  widget=forms.DateTimeField(\n                                      input_formats=['%Y-%m-%d %H:%M:%S %Z']))\n        self.assertEqual(ff.widget.attrs['class'], 'datetimepicker')\n\n"], "sample_1048": ["    def test_intersection_ellipse(self):\n        p1 = Point(0, 0)\n        l1 = Line(Point(1, -2), Point(-1, -2))\n        parabola1 = Parabola(p1, l1)\n        ellipse = Ellipse(Point(0, 0), 2, 5)\n        intersection = parabola1.intersection(ellipse)\n        \n        assert len(intersection) == 2\n        assert isinstance(intersection[0], Point2D)\n        assert intersection[0].args == (-2, 0)\n        assert isinstance(intersection[1], Point2D)\n        assert intersection[1].args == (2, 0)\n", "    def test_intersection_line_with_parabola_outside(self):\n        p1 = Point(0, 0)\n        l1 = Line(Point(1, -2), Point(-1,-2))\n        parabola = Parabola(p1, l1)\n        l2 = Line(Point(-7, 3), Point(12, 3))\n        result = parabola.intersection(l2)\n        assert len(result) == 2\n        assert result[0] == Point2D(-4, 3)\n        assert result[1] == Point2D(4, 3)\n\n\n\n", "    def test_intersection_line(self):\n        p1 = Point(0, 0)\n        l1 = Line(Point(1, -2), Point(-1,-2))\n        parabola1 = Parabola(p1, l1)\n        intersection = parabola1.intersection(Line(Point(-7, 3), Point(12, 3)))\n        assert intersection == [Point2D(-4, 3), Point2D(4, 3)]\n\n\n\n", "    def test_intersection_ellipse(self):\n        p1 = Point(0, 0)\n        l1 = Line(Point(5, 8), Point(7, 8))\n        parabola1 = Parabola(p1, l1)\n        ellipse = Ellipse(Point(0, 0), 2, 5)\n        intersection = parabola1.intersection(ellipse)\n        expected_intersection = [Point2D(-2, 0), Point2D(2, 0)]\n        assert intersection == expected_intersection\n", "    def test_intersection_segment(self):\n        p1 = Point(0, 0)\n        l1 = Line(Point(1, -2), Point(-1,-2))\n        parabola1 = Parabola(p1, l1)\n        s1 = Segment(Point(-12, -65), (14, -68))\n        intersection_points = parabola1.intersection(s1)\n        assert len(intersection_points) == 0 \n"], "sample_848": ["    def test_chain_fit_predict_regression(self):\n        X, y = make_classification(n_samples=100, n_features=20, random_state=42)\n        n_targets = 3\n        y = y.reshape(-1, n_targets)\n\n        chain = RegressorChain(base_estimator=Lasso(), order=np.arange(n_targets))\n        chain.fit(X, y)\n        y_pred = chain.predict(X)\n        assert_array_equal(y_pred.shape, (X.shape[0], n_targets))\n        for i in range(n_targets):\n            assert_array_almost_equal(chain.estimators_[i].coef_,\n                                      chain.get_params()[\"base_estimator\"].coef_)\n", "    def test_classifier_chain_with_random_order(self):\n        X, y = make_classification(n_samples=100, n_features=10, n_informative=5,\n                                   n_classes=3, random_state=42)\n        clf = ClassifierChain(base_estimator=LogisticRegression(random_state=42),\n                               order='random', cv=5)\n        clf.fit(X, y)\n        y_pred = clf.predict(X)\n        assert_array_equal(y_pred.shape, y.shape)\n        \n        # Check if the random order of predictions is different from default \n        clf_default = ClassifierChain(base_estimator=LogisticRegression(random_state=42), cv=5)\n        clf_default.fit(X, y)\n        y_pred_default = clf_default.predict(X)\n        assert_array_not_equal(y_pred, y_pred_default)\n ", "    def test_classifier_chain_cv(self, cv):\n        X, y = make_classification(n_samples=100, n_features=20, n_informative=15, random_state=42)\n        clf = ClassifierChain(\n            base_estimator=LogisticRegression(random_state=42), cv=cv,\n            random_state=42)\n        clf.fit(X, y)\n        assert isinstance(clf.estimators_, list)\n        assert len(clf.estimators_) == y.shape[1]\n", "    def test_classifier_chain_cross_validation(self):\n        X, y = make_classification(n_samples=100, n_features=10,\n                                   n_classes=3, random_state=42)\n        cv = 5\n        clf = ClassifierChain(base_estimator=LogisticRegression(random_state=42), cv=cv)\n        clf.fit(X, y)\n        y_pred = clf.predict(X)\n\n        y_pred_cv = clf.predict(X)\n        assert_array_equal(y_pred, y_pred_cv)\n", "    def test_classifier_chain_fit_empty_data(self):\n        X = np.array([])\n        y = np.array([])\n        clf = ClassifierChain(base_estimator=LogisticRegression())\n        with pytest.raises(ValueError) as e:\n            clf.fit(X, y)\n        assert \"Data matrices X and y can't be empty\" in str(e)\n "], "sample_755": ["compilation error", "    def test_silhouette_score_with_sparse_data():\n        # Test with sparse data\n        X, y = datasets.make_blobs(n_samples=100, centers=4, random_state=42)\n        sparse_X = csr_matrix(X)\n        assert_array_equal(silhouette_score(X, y), silhouette_score(sparse_X, y))\n\n\n\n\n", "compilation error", "    def test_silhouette_score_with_sparse_data():\n        X, y_true = datasets.make_blobs(n_samples=100, centers=3, random_state=42)\n        X_sparse = csr_matrix(X)\n        score = silhouette_score(X_sparse, y_true)\n        assert np.isfinite(score)\n", "    def test_silhouette_score_empty_input():\n        X = []\n        labels = [0]\n        with assert_raises_regexp(ValueError, \"n_samples\"):\n            silhouette_score(X, labels)\n"], "sample_72": ["    def test_serialize_custom_type(self):\n        with self.settings(INSTALLED_APPS={'custom_migration_operations': ''}):\n            with self.subTest(\"serialize Money\"):\n                money = Money(123)\n                serializer = Serializer.serializer_factory(money)\n                serialized_money = serializer.serialize()\n                self.assertEqual(serialized_money, \"custom_migration_operations.money.Money(Decimal('123'))\",\n                                 \"Serialization of custom type failed\")\n\n\n\n", "    def test_serialize_functools_partial(self):\n        partial = functools.partial(sum, 1)\n        serializer = serializer_factory(partial)\n        serialized_string, imports = serializer.serialize()\n        self.assertEqual(\n            serialized_string,\n            'functools.partial(sum, 1)'\n        )\n        self.assertEqual(\n            imports,\n            {'import functools'}\n        )\n\n\n\n", "    def test_serializer_functools_partial(self):\n        from django.utils.datastructures import ImmutableDict\n            return a + b + kwargs['c']\n\n        partial = functools.partial(func, b=10, c=20)\n        serializer = custom_migration_operations.serializer.serializer_factory(partial)\n        from django.db.migrations.serializer import Serializer\n        self.assertEqual(serializer, Serializer(functools.partial))\n\n\n\n        imports = serializer.serialize()[1]\n        expected_imports = {\"import functools\"}\n        self.assertEqual(imports, expected_imports)\n", "    def test_serialize_operations(self):\n        operation = custom_migration_operations.more_operations.MyOperation(name='test')\n        writer = OperationWriter(operation, indentation=0)\n        serialized_operation, imports = writer.serialize()\n        self.assertIn(\"MyOperation\", serialized_operation)\n\n", "    def test_operation_serializer(self):\n        from django.db.migrations.operations.autodetector import AutoDetectionMigration\n        migration = AutoDetectionMigration(\n            app_label='example',\n            name='test_auto_detection',\n            operations=[\n                models.RenameField(\n                    model_name='Model1',\n                    old_name='old_field',\n                    new_name='new_field',\n                ),\n            ],\n        )\n        serializer = OperationSerializer(migration)\n        serialized_str, imports = serializer.serialize()\n        self.assertIn('\"RenameField\"', serialized_str)\n        self.assertIn('from django.db.migrations.operations import RenameField', imports)\n\n\n"], "sample_483": ["    def test_exclude_from_parent(self):\n        class AlbumInline(GenericStackedInline):\n            model = Song\n            fk_name = \"album\"\n            exclude = [\"duration\"]\n\n        class AlbumAdmin(admin.ModelAdmin):\n            inlines = [AlbumInline, ]\n\n        # Check there are no errors\n        checks.run_checks(\n            [AlbumAdmin],\n            root_checks=checks.get_root_checks(),\n        )\n\n", "    def test_check(self):\n        admin_site = AdminSite()\n        admin_site.register(Song, MyAdmin)\n\n        with self.assertLogs(\"admin.checks\") as cm:\n            checks.run_checks(admin_site)\n\n        for log_record in cm.records:\n            if log_record.levelno not in (logging.ERROR, logging.CRITICAL):\n                raise AssertionError(f\"Unexpected log level: {log_record.levelno}\")\n\n        self.assertEqual(len(cm.records), 1)\n        self.assertTrue(\n            \"error!\" in \"\".join(record.msg for record in cm.records)\n        )\n", "    def test_admin_checks_with_related_fields(self):\n        class AuthorAdmin(admin.ModelAdmin):\n            list_display = [\"name\", \"email\", \"books__title\"]\n\n        admin.site.register(Author, AuthorAdmin)\n\n        class BookAdmin(admin.ModelAdmin):\n            list_display = [\"title\", \"author__name\"]\n            list_filter = [\"author__name\"]\n\n        admin.site.register(Book, BookAdmin)\n\n        errors = checks.run_checks()\n        self.assertEqual(len(errors), 1)\n        error = errors[0]\n        self.assertEqual(error.id, \"admin.E127\")\n", "    def test_inline_admin_checks_validate_unique_fields(self):\n        class TwoAlbumsInline(GenericStackedInline):\n            model = TwoAlbumFKAndAnE\n            extra = 1\n            fk_name = \"album_1\"\n\n        class TwoAlbumModelAdmin(admin.ModelAdmin):\n            inlines = [TwoAlbumsInline]\n\n        checks.checks = []  # Reset checks for each test\n        admin.site = AdminSite()\n        admin._registry[\"twoalbum_twoalbumfkande\"] = TwoAlbumModelAdmin(\n            TwoAlbumFKAndAnE\n        )\n        errors = admin.site._run_checks()\n        self.assertEqual(len(errors), 1)\n        error = errors[0]\n        self.assertEqual(error.id, \"admin.E130\")\n        self.assertIn(\n            \"Name '__unnamed__' is not unique\", error.msg\n        )\n\n\n\n", "    def test_model_admin_checks(self):\n\n        class MyAdmin(admin.ModelAdmin):\n            ordering = ['title']\n            list_display = ['title', 'album']\n            list_filter = ['album']\n            search_fields = ['title']\n\n        admin_site = AdminSite()\n\n        with self.subTest(\"Checks for ordering\"):\n            admin_site.register(Song, MyAdmin)\n            errors = checks.run_checks(admin_site)\n            self.assertEqual(len(errors), 0)\n\n        with self.subTest(\"Checks for invalid ordering\"):\n            class InvalidAdmin(admin.ModelAdmin):\n                ordering = ['non_existent_field']\n            admin_site.register(Song, InvalidAdmin)\n            errors = checks.run_checks(admin_site)\n            self.assertEqual(len(errors), 1)\n\n        with self.subTest(\"Checks for duplicate actions\"):\n            class DuplicateActionsAdmin(admin.ModelAdmin):\n                actions = ['my_action']\n\n            admin_site.register(Song, DuplicateActionsAdmin)\n            errors = checks.run_checks(admin_site)\n            self.assertEqual(len(errors), 1)\n\n\n\n"], "sample_958": ["    def test_alias_with_recursion(self, cppDomain):\n        idDict = {1: \"c:my_alias:1\", 2: \"c:my_alias:2\"}\n        check(\"alias\", \".. c:my_alias::\\n   .. c:my_alias::\", idDict, \".. c:my_alias::\\n   .. c:my_alias::\", asTextOutput=\".. c:my_alias::\\n   .. c:my_alias::\")\n\n\n\n", "    def test_simple_declarations(input, idDict, output):\n        check(\"var\", input, idDict, output)\n", "    def test_cvar_decl_with_const(input, output, key=None):\n        idDict = {\n            1: r'cvar_1_c_var_TestDoc_cvar_1',\n            2: r'cvar_2_c_var_TestDoc_cvar_2',\n        }\n        check(\n            'var', input, idDict, output, key,\n            # no need to test asTextOutput, we already know format of output here\n        )\n", "    def check_empty_signature(name, input, output, key=None):\n        _check(name, input, {}, output, key, asTextOutput=output)\n", "    def test_enum_declarations(self):\n        _check('enum', r'enum name {{ 123 = 456 }}', {'1': 'name.123', '2': 'name.456'},\n               r'name.123 = 456', asTextOutput=r'name.123 = 456')\n\n\n"], "sample_540": ["    def test_animation_blitting_simple(anim):\n        # Test blitting with a simple animation.\n\n        with plt.ion():\n            # Ensure that blitting is enabled\n            anim._blit = True\n            # Get the initial state of the figure\n            initial_state = plt.gcf().canvas.copy_from_bbox(plt.gcf().bbox)\n            # Run the animation for a few frames\n            anim.event_source.start()\n            for _ in range(3):\n                anim.event_source.advance()\n\n        # Check if the figure has only been updated in the areas that were\n        # changed by the animation.\n        with plt.ioff():\n            final_state = plt.gcf().canvas.copy_from_bbox(plt.gcf().bbox)\n        check_figures_equal(initial_state, final_state, rtol=1e-03)\n\n\n\n\n", "    def test_blit(self, anim):\n        # Test blitting\n        canvas = anim._fig.canvas\n        original_renderer = canvas.get_renderer()\n        with pytest.raises(AttributeError):\n            animation.FuncAnimation(fig=anim._fig, func=lambda i: None, blit=True)\n        # blitting should still work for the existing animation\n        anim._blit = True\n        anim.event_source.start()\n        # Since blitting is supposed to be faster, this\n        # shouldn't raise an error\n        canvas.draw_idle()\n        canvas.flush_events()\n        anim.event_source.stop()\n\n\n\n", "    def test_blitting(self, anim, request):\n        # This test ensures that blitting works correctly by\n        # comparing a frame with and without blitting.\n        # This test requires a windowed backend\n\n        if plt.get_backend() != 'Qt5Agg':\n            pytest.skip('Skipping blitting test as it requires a windowed backend')\n\n        # blitting can only be used when saving with a writer\n        writer = NullMovieWriter()\n        anim.save('test.mp4', writer=writer, fps=15)\n\n        if anim.__class__ is animation.FuncAnimation:\n            # FuncAnimation returns a list of artists modified in each frame.\n            # TimedAnimation returns None\n            assert all(isinstance(a, mpl.artist.Artist) for a in writer.args[0])\n\n        return writer.fig\n", "    def test_blitting_basic(self, anim):\n        # Check that blitting renders correctly\n        try:\n            from matplotlib import animation, rcParams\n\n            # Make sure we are blitting using a reasonable backend\n            if rcParams['backend'] == 'TkAgg':\n                return  # TkAgg blitting is unreliable\n\n            # Use a NullMovieWriter to capture the frames and events\n\n            writer = NullMovieWriter()\n            anim.save(\"temp.mp4\", writer=writer)\n\n            # Check that grab_frame was called the right number of times\n            assert writer._count == len(anim._save_seq)\n        finally:\n            Path(\"temp.mp4\").unlink(missing_ok=True)\n\n\n\n", "    def test_blit_drawing_sequence(self, anim):\n        # This test checks that blitting works correctly by creating a multi-step\n        # animation where each frame changes a different artist.\n        # The draw function should only draw the artists that were changed, not\n        # the entire figure every time.\n        \n        fig = anim.fig\n        ax = fig.gca()\n\n        # Create a line that will be animated\n        line, = ax.plot([], [])\n        # Create a scatter plot that will be animated\n        scatter = ax.scatter([], [])\n\n            x = np.linspace(0, 10, 100)\n            y = np.sin(x + i)\n            line.set_data(x, y)\n\n            new_x = np.random.rand(10)\n            new_y = np.random.rand(10)\n            scatter.set_offsets(np.column_stack((new_x, new_y)))\n            return line, scatter\n        \n        anim = animation.FuncAnimation(fig, animate, blit=True)  # Enable blitting\n\n        # Get the frames and see if the number of frames drawn is correct\n        with fig.canvas.renderer.acquire():\n            for frame in anim.event_source.iter_frames():\n                ax.draw_artist(line)\n                ax.draw_artist(scatter)\n                \n                if frame == 1:\n                    # Check that the line changed\n                    assert line.get_data() != ax.lines[0].get_data()\n                    # Check that the scatter plot changed\n                    assert scatter.get_offsets() != ax.collections[0].get_offsets()\n                elif frame > 1:\n                    # Check that only the relevant artist changed\n                    assert line.get_data() != ax.lines[0].get_data()\n                    assert scatter.get_offsets() != ax.collections[0].get_offsets()\n\n"], "sample_501": ["    def test_bbox_to_anchor(self):\n        # Test bbox_to_anchor\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3], [4, 5, 6])\n        legend = ax.legend(bbox_to_anchor=(0, 1), loc='upper left')\n        plt.show()  \n", "    def test_legend_bbox_to_anchor(self):\n        fig, ax = plt.subplots()\n        rect = mpl.patches.Rectangle((0, 0), 1, 1, facecolor='none',\n                                   edgecolor='k',\n                                   linewidth=2)\n        ax.add_patch(rect)\n        fig.legend().set_bbox_to_anchor((0.5, 0.5))\n        self.compare_image(\"legend_bbox_to_anchor\", ax=ax, remove_text=True)\n", "    def test_set_bbox_to_anchor(self, mode):\n        fig, ax = plt.subplots()\n        legend = mlegend.Legend(ax, loc=\"upper left\", mode=mode)\n\n        with rc_context( {'axes.linewidth': 0}):\n            legend.set_bbox_to_anchor((0.5, 0.5))\n\n        expected_bbox = legend._legend_box.get_window_extent()\n        legend.set_bbox_to_anchor(None)  \n        # Test that bbox_to_anchor is not set after setting back to None\n        assert legend._bbox_to_anchor is None\n\n        # Now test that bbox_to_anchor is correctly applied\n        legend.set_bbox_to_anchor((0.5, 0.5))\n        actual_bbox = legend._legend_box.get_window_extent()\n        assert np.allclose(expected_bbox.bounds, actual_bbox.bounds)\n\n        plt.close(fig)\n", "def test_legend_loc(loc, expected_loc, fig, ax):\n    legend = ax.legend()\n    legend.set_loc(loc)\n    assert legend._loc == expected_loc\n\n\n", "    def test_bbox_to_anchor(self):\n        fig, ax = plt.subplots()\n        test_data = np.random.rand(10, 2)\n        line, = ax.plot(test_data[:, 0], test_data[:, 1])\n        legend = ax.legend([line], ['Test'],\n                          bbox_to_anchor=(0, 1), loc=\"upper left\")\n        plt.draw()\n"], "sample_652": ["    def test_getitem_circularity_ref(self):\n            return x\n\n        with pytest.raises(RecursionError):\n            with self.fixture_scope(self.create_fixture_circular(circular)) as circular_fixture:\n                circular_fixture(circular_fixture)\n\n\n\n", "def test_showfixtures_no_fixtures(testdir):\n    p = testdir.makepyfile(\n        \"def test_pass():\\n    pass\",\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n    tw = textwrap.dedent(result.stdout.str())\n    assert \"Fixtures\" not in tw\n\n\n\n", "    def test_parametrization_indirect(self, testdir):\n        p = testdir.makepyfile(\"\"\"\n                assert indirect_fixture == 1\n            \n                assert indirect_fixture == 2\n        \"\"\")\n        result = p.runpytest(\n            \"--parametrize=test_one:indirect_fixture - indirect_fixture:x\",\n            capture=True,\n        )\n        result.assert_outcomes(passed=2)\n        result.stdout.fnmatch_lines(\n            [\n                \"*indirect_fixture 1*\",\n                \"*indirect_fixture 2*\",\n            ]\n        )\n", "    def test_show_fixtures(self, testdir):\n        p = testdir.makepyfile(\n            \"\"\"\n                pass\n\n            @pytest.fixture\n                return 42\n            \"\"\"\n        )\n        result = testdir.runpytest(p)\n        result.stdout.fnmatch_lines(\n            [\n                \"*  -- test_something.py*:test_something\",\n                \"*  -- test_something.py*:my_fixture\",\n                \"*    my_fixture -- test_something.py:my_fixture\",\n            ]\n        )\n", "    def test_show_fixtures_per_test(self, testdir, capsys):\n        \"\"\"Test fixtures per test display\"\"\"\n        testdir.makepyfile(\n            \"\"\"\n                pass\n\n            @pytest.fixture\n                return request.getfixturevalue(\"hello\") + 1\n            \n            class TestClass:\n                @pytest.fixture\n                    return\n            \"\"\"\n        )\n        reprec = testdir.inline_run()\n        reprec.stdout.replacenewlines(\"\\n\")\n        assert \"fixtures used by test_func\" in repr(reprec.stdout)\n        assert \"my_fixture -- fixtures/test_show_fixtures_per_test.py\" in repr(reprec.stdout)\n\n        # Test fixture in class\n        reprec = testdir.inline_run(\n            '-v',\n            '--cov=.'.join(testdir.tmpdir.iterdir()),\n            '--show-fixtures',\n        )\n        reprec.stdout.replacenewlines(\"\\n\")\n\n\n"], "sample_76": ["    def test_language_check_settings_consistent(self):\n        with self.settings(\n            LANGUAGES=[('en', 'English'), ('es', 'Spanish')],\n            LANGUAGE_CODE='fr',\n        ):\n            errors = check_language_settings_consistent(app_configs=None, **{})\n            self.assertEqual(len(errors), 1)\n            self.assertEqual(errors[0].id, 'translation.E004')\n        with self.settings(\n            LANGUAGES=[('en', 'English'), ('es', 'Spanish')],\n            LANGUAGE_CODE='es',\n        ):\n            errors = check_language_settings_consistent(app_configs=None, **{})\n            self.assertEqual(len(errors), 0) \n", "    def test_check_setting_language_code_valid_tags(self):\n        with self.settings(LANGUAGE_CODE='en'):\n            result = list(check_setting_language_code(app_configs=[]))\n            self.assertEqual(len(result), 0)\n\n        for tag in self.valid_tags:\n            with self.settings(LANGUAGE_CODE=tag):\n                result = list(check_setting_language_code(app_configs=[]))\n                self.assertEqual(len(result), 0)\n", "    def test_check_setting_languages_bidi(self):\n        with self.subTest('Valid'):\n            settings.LANGUAGES_BIDI = [\n                'en',\n                'fr',\n                'es',\n            ]\n            errors = check_setting_languages_bidi(\n                [],\n            )\n            self.assertEqual(errors, [])\n        with self.subTest('Invalid type'):\n            settings.LANGUAGES_BIDI = [\n                None,\n                123,\n                b'en',\n            ]\n            errors = check_setting_languages_bidi(\n                [],\n            )\n            self.assertIsInstance(errors[0], Error)\n        with self.subTest('Invalid format'):\n            settings.LANGUAGES_BIDI = [\n                'e\u00fc',\n                'en--us',\n                '-en',\n                'en-',\n            ]\n            errors = check_setting_languages_bidi(\n                [],\n            )\n            self.assertEqual(len(errors), len(self.invalid_tags))\n", "    def test_check_language_settings_consistent_missing_en_us(self):\n        with self.settings(\n            LANGUAGE_CODE='es-419',\n            LANGUAGES=(('es-419', 'Espa\u00f1ol'),),\n            LANGUAGES_BIDI=(('es-419', 'rtl'),),\n        ):\n            errors = check_language_settings_consistent([], {})\n            self.assertEqual(errors, [Error(id='translation.E004')])\n", "    def test_check_setting_languages_bidi(self):\n        with self.settings(LANGUAGES_BIDI=self.valid_tags):\n            errors = check_setting_languages_bidi([], {})\n            self.assertEqual(errors, [])\n        with self.settings(LANGUAGES_BIDI=self.invalid_tags):\n            errors = check_setting_languages_bidi([], {})\n            self.assertEqual(len(errors), len(self.invalid_tags))\n\n\n"], "sample_701": ["    def test_deprecated_pytest_collect(pytester: Pytester):\n        pytester.getiterator(\n            \"test_module.py\",\n            \"\"\"\n            import pytest\n            from _pytest import deprecated\n\n            deprecated.PYTEST_COLLECT_MODULE\n            \"\"\"\n        ).next().assert_warning_is(PytestDeprecationWarning,\n                                      \"pytest.collect.{name} was moved to pytest.{name}\\n\"\n                                      \"Please update to the new name.\")\n\n\n", "    def test_deprecated_collect_module_warning(pytester):\n        with pytest.warns(deprecated.PYTEST_COLLECT_MODULE):\n            import pytest.collect  # noqa: F401\n\n\n", "    def test_deprecated_function_arguments(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            from _pytest import deprecated\n\n                deprecated.check_ispytest(_ispytest)\n                return arg1, arg2\n\n                deprecated_func(\"hello\", \"world\", _ispytest=True)\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        assert result.ret == 0\n        # Check for the warning message in the output\n        assert any(\n            \"A private pytest class or function was used.\" in line\n            for line in result.stdout.splitlines()\n        )\n", "    def test_deprecation_warnings_from_deprecated_constants(testdir):\n        testdir.makepyfile(\"\"\"\n        import pytest\n\n            pytest.deprecated.FILLFUNCARGS\n        \"\"\")\n\n        result = testdir.runpytest()\n        result.assert_outcomes(passed=1, warnings=1,)\n\n        assert \"name\" in result.getwarningtext()\n", "    def test_deprecated_collect_module(pytester: Pytester):\n        pytester.check_deprecated(\n            deprecated.PYTEST_COLLECT_MODULE,\n            \"pytest.collect.my_module\",\n            \"pytest.my_module\",\n        ) \n        pytester.check_deprecated(\n            deprecated.PYTEST_COLLECT_MODULE,\n            \"pytest.collect.another_module\",\n            \"pytest.another_module\",\n        )\n"], "sample_529": ["    def test_legend_location(self, loc):\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3], [4, 5, 6])\n        ax.plot([4, 5, 6], [7, 8, 9])\n        leg = ax.legend(loc=loc)\n        \n        # Save and compare to reference figure\n        self.savefig_and_compare(fig, f'legend_location_{loc}')\n\n\n", "    def test_draggable_legend_update(self, image_comparison_skip):\n        if image_comparison_skip:\n            return\n\n        fig, ax = plt.subplots()\n        line, = ax.plot([1, 2, 3])\n        line.set_label('Line')\n        legend = ax.legend()\n        legend.set_draggable(True)\n\n        with rc_context(axes={'legend.loc': 'upper right'}):\n            fig.canvas.draw_idle()\n            x, y = legend.get_bbox().get_bounds()\n\n        # Now drag the legend\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            fig.canvas.mpl_connect('motion_notify_event', lambda ev: None)\n            fig.canvas.mpl_connect('button_press_event', lambda ev: None)\n            fig.canvas.mpl_connect('button_release_event',\n                                   lambda ev: None)\n        fig.canvas.draw()\n\n        # Assert bbox is different\n        x_diff = abs(legend.get_bbox().x0 - x)\n        y_diff = abs(legend.get_bbox().y0 - y)\n        assert x_diff > 0.1 and y_diff > 0.1, \"legend not moved!\"\n\n        # Check that the update function works\n        with rc_context(axes={'legend.loc': 'center'}):\n            fig.canvas.draw_idle()\n            x, y = legend.get_bbox().get_bounds()\n        assert x_diff > 0.1 and y_diff > 0.1, \"legend not moved!\"\n        \n\n\n\n", "def test_legend_bbox_to_anchor():\n    with rc_context({'figure.figsize': (6, 4)}):\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3], [4, 5, 6])\n        legend = ax.legend()\n\n        # Test bbox_to_anchor with no arguments\n        legend.set_bbox_to_anchor(None)\n        assert legend.get_bbox_to_anchor() is ax.bbox\n\n        # Test bbox_to_anchor with a tuple\n        bbox = (0, 0.5, 0.2, 0.2)\n        legend.set_bbox_to_anchor(bbox)\n        assert legend.get_bbox_to_anchor() == Bbox.from_bounds(*bbox)\n\n        # Test bbox_to_anchor with BboxBase\n        bbox = Bbox.from_bounds(0.1, 0.1, 0.3, 0.2)\n        legend.set_bbox_to_anchor(bbox)\n        assert legend.get_bbox_to_anchor() == bbox\n\n        # Test bbox_to_anchor with a transform\n        transform = mtransforms.IdentityTransform()\n        legend.set_bbox_to_anchor(bbox, transform=transform)\n        assert legend.get_bbox_to_anchor().get_transform() == transform\n\n        # Test bbox_to_anchor with multiple legend entries\n        ax2 = fig.add_subplot(2, 1, 2)\n        ax2.plot([7, 8, 9], [10, 11, 12])\n        legend2 = ax2.legend()\n\n        # Set bbox_to_anchor for legend2\n        legend2.set_bbox_to_anchor((0.5, 0.5))\n\n        plt.show()\n", "    def test_draggable_legend_updates_bbox(self, image_data):\n        with rc_context({'axes.prop_cycle': mpl.rcParams['axes.prop_cycle']}):\n            fig, ax = plt.subplots(figsize=(4, 4))\n            x = np.linspace(0, 10, 10)\n            y1 = np.sin(x)\n            y2 = np.cos(x)\n            line1, = ax.plot(x, y1)\n            line2, = ax.plot(x, y2)\n            legend = ax.legend([line1, line2], ['Line 1', 'Line 2'],\n                               draggable=True)\n\n            # get initial bbox\n            initial_bbox = legend.get_bbox_to_anchor().bounds\n\n            legend.draggable.on_drag(event=None, x=10, y='center')\n            # check if bbox has been updated\n            updated_bbox = legend.get_bbox_to_anchor().bounds\n            assert not np.allclose(initial_bbox, updated_bbox), (\n                'bbox did not update after dragging')\n", "    def test_legend_title_empty(self):\n        fig, ax = plt.subplots()\n        legend = ax.legend([], [], title=\"\")\n        assert legend.get_title().get_text() == \"\"\n        legend.set_title(None)\n        assert legend.get_title().get_text() == \"\"\n"], "sample_812": ["    def test_changed_only(self):\n        logreg = LogisticRegressionCV()\n        logreg.C = 1.0\n        logreg.penalty = 'l1'\n        pp = _EstimatorPrettyPrinter(indent=1, width=80, depth=2,\n                                     stream=StringIO(),\n                                     changed_only=True)\n        s = pp.pformat(logreg)\n        assert \"penalty='l1'\" in s\n        assert \"C=1.0\" not in s\n\n        logreg2 = LogisticRegression()\n        logreg2.set_params(penalty='l2', C=0.5)\n        pp = _EstimatorPrettyPrinter(indent=1, width=80, depth=2,\n                                     stream=StringIO(),\n                                     changed_only=True)\n        s = pp.pformat(logreg2)\n        assert \"penalty='l2'\" in s\n        assert \"C=0.5\" in s\n", "    def test_n_max_elements_to_show(self):\n        set_config(print_changed_only=False)\n        logreg = LogisticRegression(penalty='l2', dual=False, tol=1e-4, C=1.0,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    class_weight=None, random_state=None,\n                                    solver='warn', max_iter=100,\n                                    multi_class='warn', verbose=0,\n                                    warm_start=False, n_jobs=None,\n                                    l1_ratio=None)\n        for n_max in [5, 10, 15]:\n            _EstimatorPrettyPrinter().n_max_elements_to_show = n_max\n            with capture_output() as c:\n                print(logreg)\n            output = c.stdout.getvalue()\n            if n_max < 5:\n                assert re.search(r'LogisticRegression\\(penalty=.*\\)', output)\n                assert not re.search(r'*, ...', output)\n            else:\n                assert re.search(r'LogisticRegression\\(penalty=.*\\)', output)\n                assert re.search(r'*, ', output)\n\n\n\n", "    def test_estimator_with_many_params(self):\n        estimator = SVC(\n            C=1.0,\n            kernel='rbf',\n            degree=3,\n            gamma='auto_deprecated',\n            coef0=0.0,\n            shrinking=True,\n            probability=False,\n            tol=1e-3,\n            cache_size=200,\n            class_weight=None,\n            verbose=False,\n            max_iter=-1,\n            decision_function_shape='ovr',\n            random_state=None\n        )\n        set_config(print_changed_only=True)\n        pp = _EstimatorPrettyPrinter(compact=True,\n                                     n_max_elements_to_show=5)\n        print_output = pp.format(estimator, {}, 0, 0)\n        print_output = re.sub(r'\\s+', ' ', print_output).strip()\n        expected_output = (\n            \"SVC(C=1.0, coef0=0.0, \"\n            \"class_weight=None, decision_function_shape='ovr', \"\n            \"degree=3, gamma='auto_deprecated', \"\n            \"kernel='rbf', max_iter=-1, probability=False, \"\n            \"random_state=<not given>, shrinking=True, tol=1e-3, \"\n            \"verbose=False)\"\n        )\n        assert print_output == expected_output\n", "    def test_estimator_with_many_params(self):\n        # Test with a large number of parameters\n        pipe = make_pipeline(\n            SimpleImputer(),\n            SelectKBest(chi2, k=5),\n            LogisticRegression(penalty='l2', dual=False, tol=1e-4, C=1.0,\n                              fit_intercept=True, intercept_scaling=1,\n                              class_weight=None, random_state=None, solver='warn',\n                              max_iter=100, multi_class='warn', verbose=0,\n                              warm_start=False, n_jobs=None, l1_ratio=None),\n            CountVectorizer(),\n            SVC(C=1.0, kernel='rbf', degree=3, gamma='auto_deprecated',\n                coef0=0.0, shrinking=True, probability=False, tol=1e-3,\n                cache_size=200, class_weight=None, verbose=False,\n                max_iter=-1, decision_function_shape='ovr',\n                random_state=None),\n            PCA(n_components=None, copy=True, whiten=False,\n                svd_solver='auto', tol=0.0, iterated_power='auto',\n                random_state=None),\n            NMF(n_components=None, init=None, solver='cd',\n                beta_loss='frobenius', tol=1e-4, max_iter=200,\n                random_state=None, alpha=0., l1_ratio=0., verbose=0,\n                shuffle=False),\n        )\n\n        set_config(print_changed_only=True)\n        formatted_pipe = _EstimatorPrettyPrinter().format(pipe)\n\n        # Check if the output format is correct\n        assert \"make_pipeline\" in formatted_pipe\n        assert len(formatted_pipe.split('\\n')) > 10  # Assuming more than 10 steps\n        assert \"SimpleImputer\" in formatted_pipe\n        for estimator_name in [\n                \"SelectKBest\", \"LogisticRegression\", \"CountVectorizer\",\n                \"SVC\", \"PCA\", \"NMF\"\n        ]:\n            assert estimator_name in", "    def test_changed_only(self):\n        # Test that changed_only=True only shows changed parameters\n        set_config(print_changed_only=True)\n        model = LogisticRegressionCV()\n        model.fit(np.array([]), np.array([]))\n        model.set_params(penalty='l1', C=1.0, solver='liblinear')\n        pp = _EstimatorPrettyPrinter(compact=True)\n        representation = pp.format(model, None, 0, 0)\n        assert \"penalty='l1'\" in representation\n        assert \"C=1.0\" in representation\n        assert \"solver='liblinear'\" in representation\n        assert \"dual=False\" not in representation\n\n\n"], "sample_38": ["    def test_maps(self, filename):\n        filepath = get_pkg_data_filename(\"maps\", filename)\n        with open(filepath, \"r\") as fits_file:\n            fits_data = fits_file.read()\n        hdulist = fits.open(io.BytesIO(fits_data))\n        wcs = wcs.WCS(hdulist[0].header)\n        # testing some basic functionality\n        assert not np.isnan(wcs.wcs.crpix).any()\n        assert not np.isnan(wcs.wcs.crval).any()\n        assert not np.isnan(wcs.wcs.cdelt).any()\n        assert not np.isnan(wcs.wcs.ctype).any()\n        assert not np.isnan(wcs.wcs.cname).any()\n\n        # testing some more advanced functionality\n        wcs.wcs.celestial\n\n", "    def test_maps(self, filename):\n        with catch_warnings():\n            warnings.simplefilter(\"ignore\", WcsWarning)\n            with NumpyRNGContext():\n                data = fits.open(get_pkg_data_filename(\"maps\", filename))[0].data\n                header = fits.open(get_pkg_data_filename(\"maps\", filename))[0].header\n                wcs = wcs.WCS(header)\n                # check the size of the data and the header\n                assert len(data.shape) == 2\n                assert all(key in header for key in [\"NAXIS1\", \"NAXIS2\", \"CTYPE1\", \"CTYPE2\"])\n                # check the pixel coordinates with a range of values\n                x, y = np.mgrid[:data.shape[0], :data.shape[1]]\n                world_coords = wcs.wcs_pix2world(x, y, 0)\n                assert world_coords.shape == (2, data.shape[0], data.shape[1])\n\n", "    def test_wcs_from_fits(self, filename):\n        filepath = get_pkg_data_filename(\"maps\", filename)\n        with catch_warnings():\n            warnings.simplefilter(\"ignore\", category=FITSFixedWarning)\n            hdulist = fits.open(filepath)\n            wcs = wcs.WCS(hdulist[0].header)\n            assert wcs.wcs.naxis == 2\n            assert isinstance(wcs.wcs.ctype, list)\n            assert wcs.wcs.ctype[0] == 'RA---TAN'\n            assert wcs.wcs.ctype[1] == 'DEC--TAN'\n            hdulist.close() \n", "    def test_map_projection(self):\n        with catch_warnings() as w:\n            warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n            for filename in self._file_list:\n                with io.open(os.path.join(os.path.dirname(__file__),\n                                          filename), 'r') as f:\n                    header = fits.Header.fromstring(f.read())\n                wcs = wcs.WCS(header)\n                \n                # check the projected coordinates\n                x = np.linspace(0, 10, 100)\n                y = np.linspace(0, 10, 100)\n                xx, yy = np.meshgrid(x, y)\n                \n                try:\n                    ra, dec = wcs.wcs_pix2world(xx.flatten(), yy.flatten(), 0)\n                except ValueError:\n                    continue\n                \n                assert_allclose(ra, np.linspace(0, 10, 10000), atol=0.1)\n                assert_allclose(dec, np.linspace(0, 10, 10000), atol=0.1)\n\n\n\n", "    def test_wcs_reading(self, filename):\n        with catch_warnings():\n            warnings.simplefilter(\"ignore\", category=UserWarning)\n            with open(get_pkg_data_filename(\"maps\", filename), \"r\") as f:\n                header = fits.Header.fromstring(f.read())\n            wcs = wcs.WCS(header)\n            # Make sure the WCS object can be created and accessed\n            assert wcs.wcs.ctype is not None\n            assert wcs.wcs.crval is not None\n            assert wcs.wcs.crpix is not None\n            assert wcs.wcs.cdelt is not None\n            assert wcs.wcs.pc is not None\n            # Test the pixel to world transformation\n            x, y = wcs.all_pix2world(0, 0, 0)\n            assert x is not None and y is not None\n            # Test world to pixel transformation\n            world_coords = wcs.all_world2pix(x, y, 0)\n            assert world_coords is not None\n            # Test the image size\n            assert wcs.naxis == header['NAXIS']\n            assert wcs.get_image_size() == (header['NAXIS1'], header['NAXIS2'])\n\n\n\n"], "sample_735": ["    def test_spherical_covariance_estimation(self):\n        rng = np.random.RandomState(0)\n        X = generate_data(100, 2, [.5, .5], [[1, 2], [3, 4]],\n                          {'spherical': [1., 2.]}, 'spherical')\n        gmm = GaussianMixture(n_components=2, covariance_type='spherical')\n        gmm.fit(X)\n        \n        assert_allclose(gmm.precisions_['spherical'],\n                       np.array([1. / (gmm.covariances_['spherical'][0]),\n                                 1. / (gmm.covariances_['spherical'][1])]))\n        \n\n", "    def test_covariance_estimation(self):\n        rng = np.random.RandomState(0)\n        n_samples = 100\n        n_features = 2\n        n_components = 3\n        weights = rng.rand(n_components)\n        weights /= weights.sum()\n        means = rng.rand(n_components, n_features)\n        covariances = {'full': rng.rand(n_components, n_features, n_features)\n                       for _ in range(n_components)}\n\n        X = rng.multivariate_normal(\n            np.mean(means, axis=0), np.mean(covariances.values(), axis=0),\n            size=n_samples\n        )\n\n        gmm = GaussianMixture(n_components=n_components\n                             covariance_type='full',\n                             reg_covar=1e-6)\n        gmm.fit(X)\n\n        assert_almost_equal(gmm.weights_, weights)\n        assert_array_equal(gmm.means_.shape, (n_components, n_features))\n        for k in range(n_components):\n            assert_allclose(gmm.covariances_[k],\n                            covariances['full'][k], atol=1e-2)\n\n\n\n", "    def test_bic_and_aic(self):\n        rng = np.random.RandomState(0)\n        data = RandomData(rng)\n\n        for covariance_type in COVARIANCE_TYPE:\n            X = data.X[covariance_type]\n            gm = GaussianMixture(n_components=1,\n                                 covariance_type=covariance_type,\n                                 random_state=0)\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\", ConvergenceWarning)\n                gm.fit(X)\n            bic = gm.bic(X)\n            aic = gm.aic(X)\n\n            assert_greater(bic, 0)\n            assert_greater(aic, 0)\n\n\n", "    def test_covariance_initialization(self):\n        rng = np.random.RandomState(0)\n        n_samples = 500\n        n_features = 5\n        n_components = 3\n\n        # Test when precisions_init is not None\n        weights_init = rng.rand(n_components)\n        weights_init /= weights_init.sum()\n        means_init = rng.rand(n_components, n_features)\n        precisions_init = {\n            'spherical': rng.rand(n_components),\n            'diag': rng.rand(n_components, n_features) ** 2,\n            'tied': make_spd_matrix(n_features, random_state=rng),\n            'full': np.array([make_spd_matrix(n_features,\n                                             random_state=rng) for _ in\n                                range(n_components)])\n        }\n        gm = GaussianMixture(n_components=n_components,\n                             covariance_type='full',\n                             weights_init=weights_init,\n                             means_init=means_init,\n                             precisions_init=precisions_init,\n                             random_state=rng)\n        gm.fit(rng.multivariate_normal(np.zeros(n_features),\n                                      np.eye(n_features), n_samples))\n        assert_allclose(gm.precisions_cholesky_,\n                        precisions_init)\n\n        # Test for ValueError for invalid covariance_type\n\n        with assert_raise_message(ValueError,\n                                  \"Invalid value for 'covariance_type'\"):\n            gm = GaussianMixture(n_components=n_components,\n                                 covariance_type='invalid',\n                                 random_state=rng)\n            gm.fit(rng.multivariate_normal(np.zeros(n_features),\n                                          np.eye(n_features), n_samples))\n\n\n\n", "    def test_estimate_gaussian_covariance_spherical(self):\n        rng = np.random.RandomState(0)\n        n_samples = 500\n        n_components = 2\n        n_features = 2\n\n        weights = rng.rand(n_components)\n        weights = weights / weights.sum()\n        means = rng.rand(n_components, n_features)\n        precisions_spherical = rng.rand(n_components)\n        X = generate_data(n_samples, n_features, weights, means,\n                          {'spherical': precisions_spherical}, 'spherical')\n\n        nk = np.bincount(self.Y)\n\n        means, precisions_cholesky, covariances = _estimate_gaussian_parameters(\n            X, np.ones(n_samples), 1e-6, 'spherical')\n\n        # check that the means are close\n        assert_array_almost_equal(means, np.array(\n            [np.mean(X[self.Y == i], axis=0) for i in range(n_components)]))\n\n        # check that the precisions are close\n        assert_array_almost_equal(precisions_cholesky,\n                                  np.sqrt(precisions_spherical))\n"], "sample_887": ["    def test_calibration_curve_multiclass(method, ensemble):\n        if ensemble:\n            clf = VotingClassifier(\n                [\n                    (\"lr\", LogisticRegression(random_state=42)),\n                    (\"dt\", DecisionTreeClassifier(random_state=42)),\n                ],\n                voting=\"soft\",\n                weights=[0.5, 0.5],\n            )\n        else:\n            clf = LogisticRegression(random_state=42)\n        X, y = data()\n        clf.fit(X, y)\n\n        y_prob = clf.predict_proba(X)\n        prob_true, prob_pred = calibration_curve(y, y_prob, n_bins=5, strategy=\"uniform\")\n\n        if method == \"sigmoid\":\n            # Sigmoid calibration is most relevant for binary problems\n            with pytest.raises(ValueError):\n                _sigmoid_calibration(y_prob, y)\n        else:\n            # Isotonic calibration can be used for multi-class\n            _sigmoid_calibration(y_prob, y)\n\n\n\n", "    def test_calibration_display_from_estimator(data, method, ensemble):\n        X, y = data\n        n_bins = 5\n        if ensemble:\n            estimators = [\n                ('lr', LogisticRegression(random_state=42)),\n                ('dt', DecisionTreeClassifier(random_state=42)),\n            ]\n            clf = VotingClassifier(estimators, voting=\"hard\")\n        else:\n            clf = LogisticRegression(random_state=42)\n        clf.fit(X, y)\n\n        disp = CalibrationDisplay.from_estimator(\n            clf, X, y, n_bins=n_bins, strategy=\"quantile\", ref_line=True\n        )\n        disp.plot()\n        plt.show()\n", "    def test_calibration_display_from_estimator_ensemble(self, data, method, ensemble):\n        X, y = data\n        if ensemble:\n            clf = VotingClassifier(\n                estimators=[\n                    (\"lr\", LogisticRegression(random_state=42)),\n                    (\"dt\", DecisionTreeClassifier(random_state=42)),\n                ],\n                voting=\"hard\",\n            )\n        else:\n            clf = LogisticRegression(random_state=42)\n        clf.fit(X, y)\n        disp = CalibrationDisplay.from_estimator(\n            clf, X, y, method=method, n_bins=10, ref_line=True\n        )\n        disp.plot()\n", "def test_sigmoid_calibration_pipeline(data, method):\n    X, y = data\n\n    # Create a pipeline with sigmoid calibration\n    pipeline = make_pipeline(LogisticRegression(), _CalibratedClassifier(method=method))\n    pipeline.fit(X, y)\n\n    # Make predictions using the calibrated pipeline\n    y_pred = pipeline.predict_proba(X)[:, 1]\n    y_true = y\n\n    # Check that the calibration curve looks reasonable\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=10)\n    assert_allclose(prob_true, prob_pred, rtol=0.1)\n", "    def test_calibration_display_from_estimator_different_classes(method, ensemble):\n        if ensemble:\n            clf = VotingClassifier(\n                estimators=[\n                    (\"lr\", LogisticRegression(random_state=42)),\n                    (\"dt\", DecisionTreeClassifier(random_state=42)),\n                ],\n                voting=\"soft\",\n            )\n        else:\n            clf = LogisticRegression(random_state=42)\n        X, y = data()\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=0.2, random_state=42\n        )\n        clf.fit(X_train, y_train)\n        \n        disp = CalibrationDisplay.from_estimator(\n            clf,\n            X_test,\n            y_test,\n            n_bins=5,\n            strategy=\"uniform\",\n            pos_label=4,  \n            name=f\"Ensemble {\\\"yes\\\" if ensemble else 'no'}\",\n            ref_line=True,\n        )\n        disp.plot()\n        \n    \n    \n    \n\n"], "sample_325": ["    def test_boundfield_label_tag(self):\n        form = PersonNew()\n        bound_field = form.fields['first_name']\n        self.assertIsNotNone(bound_field.label_tag())\n        self.assertEqual(bound_field.label_tag(contents='Custom Label'),\n                         mark_safe('<label for=\"first_name_id\">Custom Label</label>'))\n        self.assertEqual(bound_field.label_tag(contents='Custom Label', attrs={'class': 'custom-label'}),\n                         mark_safe('<label for=\"first_name_id\" class=\"custom-label\">Custom Label</label>'))\n        self.assertFalse(bound_field.label_tag(contents='', attrs={'for': 'wrong_id'}))\n\n\n\n", "    def test_boundfield_label(self):\n        form = Person()\n        bound_field = form.fields['first_name']\n        self.assertEqual(bound_field.label, 'First name')\n        \n        form = PersonNew()\n        bound_field = form.fields['first_name']\n        self.assertEqual(bound_field.label, 'First name')\n        self.assertEqual(bound_field.id_for_label, 'first_name_id')\n", "    def test_boundfield_as_widget(self):\n        form_data = {'first_name': 'John', 'last_name': 'Doe', 'birthday': '1980-01-01'}\n        form = Person(form_data)\n        field = form.fields['first_name']\n        bound_field = BoundField(form, field, name='first_name')\n\n        self.assertEqual(bound_field.as_widget(), TextInput(attrs={'id': 'id_first_name'}).render(\n            name='first_name', value='John', attrs={'id': 'id_first_name'}\n        ))\n\n\n", "    def test_bound_field_label(self):\n        form = PersonNew()\n        bound_field = form.fields['first_name']\n        self.assertEqual(bound_field.label, 'First name')\n        self.assertEqual(bound_field.label_tag(), '<label for=\"first_name_id\">First name</label>')\n\n", "    def test_bound_field_errors(self):\n        form = Person(data={'first_name': '', 'last_name': ''})\n        bound_field = form.fields['first_name']\n        self.assertEqual(bound_field.errors, ErrorList([\n            mark_safe(form.fields['first_name'].label_from_instance(None)),\n        ]))\n        self.assertEqual(len(bound_field.errors), 1)\n        self.assertEqual(bound_field.errors[0].message_simple, 'This field is required.')\n"], "sample_48": ["    def test_aggregate_filter(self):\n        with self.assertNumQueries(1):\n            result = Author.objects.annotate(\n                avg_age=Avg('age', filter=F('friends__age') > 30)\n            ).values('name', 'avg_age')\n            self.assertCountEqual(list(result), [\n                {'name': 'Adrian Holovaty', 'avg_age': 35.0},\n                {'name': 'Jacob Kaplan-Moss', 'avg_age': 35.0},\n                {'name': 'Brad Dayley', 'avg_age': 45.0},\n                {'name': 'James Bennett', 'avg_age': 34.0},\n                {'name': 'Jeffrey Forcier', 'avg_age': 37.0},\n                {'name': 'Paul Bissex', 'avg_age': 29.0},\n                {'name': 'Wesley J. Chun', 'avg_age': 25.0},\n                {'name': 'Peter Norvig', 'avg_age': 57.0},\n                {'name': 'Stuart Russell', 'avg_age': 46.0},\n            ])\n\n\n\n", "    def test_aggregate_with_filter(self):\n        with self.subTest(name='avg'):\n            avg_age = Author.objects.annotate(\n                avg_friend_age=Avg('friends__age', filter=F('friends__age') > 30)\n            ).values_list('avg_friend_age', flat=True)\n            self.assertQueryCount(1)\n            self.assertEqual(avg_age[0], Decimal('33.25'))\n\n        with self.subTest(name='count'):\n            count_books = Book.objects.annotate(\n                count_authors=Count('authors', distinct=True, filter=F('authors__name').startswith('J'))\n            ).values_list('count_authors', flat=True)\n            self.assertQueryCount(1)\n            self.assertEqual(count_books[0], 2)\n\n\n\n", "    def test_aggregate_filter(self):\n        with CaptureQueriesContext() as queries:\n            avg_rating = Book.objects.filter(\n                authors__name='Adrian Holovaty'\n            ).annotate(\n                average_rating=Avg('rating')\n            ).values('average_rating')\n            self.assertEqual(len(queries.captured_queries), 1)\n            self.assertEqual(avg_rating.first()['average_rating'], 4.5)\n", "    def test_aggregate_filter(self):\n        with self.assertNumQueries(2) as cm:\n            a = Author.objects.filter(\n                books__aggregate=Avg('rating') > 4\n            ).values_list('name', 'age')\n            self.assert_query_count(cm, 2)\n            self.assertEqual(list(a), [('Adrian Holovaty', 34), ('Brad Dayley', 45), ('James Bennett', 29), ('Jeffrey Forcier', 37), ('Peter Norvig', 57)])\n\n\n        with self.assertNumQueries(2) as cm:\n            a = Author.objects.filter(\n                books__filter=Avg('rating') > 4\n            ).values_list('name', 'age')\n            self.assert_query_count(cm, 2)\n            self.assertEqual(list(a), [('Adrian Holovaty', 34), ('Brad Dayley', 45), ('James Bennett', 29), ('Jeffrey Forcier', 37), ('Peter Norvig', 57)])\n\n", "    def test_aggregate_with_filter(self):\n        with self.subTest('Count'):\n            count = Count('b__age', filter=Q(b__rating__gt=4))\n            query = Book.objects.annotate(age_count=count).values('age_count')\n            self.assertCount(query, 3)\n"], "sample_456": ["    def test_formset_factory_defaults(self):\n        formset = formset_factory(Choice)\n        self.assertEqual(formset.extra, 1)\n        self.assertEqual(formset.can_order, False)\n        self.assertEqual(formset.can_delete, False)\n        self.assertEqual(formset.max_num, 1000)\n        self.assertEqual(formset.validate_max, False)\n        self.assertEqual(formset.min_num, 0)\n        self.assertEqual(formset.validate_min, False)\n        \n\n\n", "    def test_formset_factory_validation(self):\n        with self.assertRaises(ValueError):\n            formset_factory(\n                Choice,\n                max_num=10,\n                absolute_max=5,\n            )\n        \n        with self.assertRaises(ValueError):\n            formset_factory(\n                Choice,\n                min_num=3,\n                validate_min=True,\n                absolute_max=2,\n            )\n\n\n\n", "    def test_empty_formset(self):\n        formset = ChoiceFormSet(extra=0)\n        self.assertEqual(len(formset.forms), 0)\n        self.assertEqual(formset.total_form_count(), 0)\n", "    def test_formset_factory_with_renderer(self):\n        with mock.patch('django.template', autospec=True) as mock_templates:\n            # Mock a custom renderer\n            mock_renderer = mock.Mock(spec=TemplatesSetting)\n            mock_templates.render.return_value = \"Rendered Templates\"\n            mock_templates.renderers = {\n                \"custom_renderer\": mock_renderer\n            }\n            # create a formset instance with custom renderer\n            CustomRendererFormSet = formset_factory(\n                CustomKwargForm, renderer=\"custom_renderer\"\n            )\n            formset = CustomRendererFormSet(None, prefix=\"test\", custom_kwarg=\"test_value\")\n            # Verify that the custom renderer is used\n            mock_renderer.assert_called_once_with(formset, context={\n                'formset': formset\n            })\n\n", "    def test_formset_factory_with_renderer(self):\n        with mock.patch(\"django.forms.renderers.get_default_renderer\") as mock_renderer:\n            mock_renderer.return_value = TemplatesSetting(\n                \"my_template_dir\"\n            )\n            ChoiceFormSetWithRenderer = formset_factory(\n                Choice, renderer=mock_renderer.return_value\n            )\n            formset = ChoiceFormSetWithRenderer({\"0\": {\"choice\": \"apple\", \"votes\": 1}})\n\n            self.assertEqual(formset.renderer, mock_renderer.return_value)\n\n\n\n"], "sample_918": ["    def test_resolve_xref_property(self):\n        domain = PythonDomain(Mock(), Mock(), Mock())\n        env = Mock()\n        builder = Mock()\n        fromdocname = 'test'\n        node = pending_xref()\n        node['py:module'] = 'test'\n        node['py:class'] = 'MyClass'\n        node['reftarget'] = 'my_property'\n        contnode = Mock()\n        node['reftype'] = 'attr'\n        domain.objects['test.MyClass.my_property'] = ObjectEntry('test', 'node-id', 'attr')\n        result = domain.resolve_any_xref(env, fromdocname, builder, 'test.MyClass.my_property', node, contnode)\n        assert len(result) == 1\n        assert result[0][0] == 'py:attr'\n        assert isinstance(result[0][1], nodes.reference)\n        assert result[0][1]['refdomain'] == 'py'\n        assert result[0][1]['reftype'] == 'attr'\n        assert result[0][1]['refuri'] == 'node-id'\n        assert result[0][1]['reftitle'] == 'my_property'\n", "    def test_parse_annotation(self):\n        assert _parse_annotation(\"int\") == \"int\"\n        assert _parse_annotation(\"int / float\") == \"int / float\"\n        assert _parse_annotation(\"Optional[str]\") == \"Optional[str]\"\n        assert _parse_annotation(\"typing.List[int]\") == \"typing.List[int]\"\n        assert _parse_annotation(\"int: Foo\") == \"int\"\n        assert _parse_annotation(\"int: Foo, float: Bar\") == \"int, float\"\n", "    def test_domain_py_resolve_xref_module(self):\n        app = Mock()\n        env = Mock()\n        builder = Mock()\n        fromdocname = 'test.rst'\n        \n        domain = PythonDomain(app, name='py')\n        domain.modules = {\n            'mymodule': Mock(docname='mymodule.rst', node_id='mymodule_nodeid')\n        }\n\n        node = addnodes.pending_xref(refdomain='py', reftarget='mymodule')\n        node['py:module'] = 'mymodule'\n\n        refnode = domain.resolve_xref(env, fromdocname, builder, 'mod',\n                                      'mymodule', node, nodes.Text(''))\n        assert isinstance(refnode, addnodes.reference)\n        assert refnode['refdomain'] == 'py'\n        assert refnode['reftype'] == 'mod'\n        assert refnode['reftitle'] == 'mymodule: mymodule'\n        assert refnode['refname'] == 'mymodule'\n\n\n\n", "    def test_parse_function_signature(sig, expected):\n        assert parse(sig) == expected\n", "    def test_parse_annotation(self):\n        assert _parse_annotation(\"int\") == \"int\"\n        assert _parse_annotation(\"Optional[int]\") == \"Optional[int]\"\n        assert _parse_annotation(\"List[str]\") == \"List[str]\"\n        assert _parse_annotation(\"Union[int, float]\") == \"Union[int, float]\"\n        assert _parse_annotation(\"None\") == \"NoneType\"\n        assert _parse_annotation(\"Callable[[int], str]\") == \"Callable[[int], str]\"\n        assert _parse_annotation(\"typing.List[int]\") == \"typing.List[int]\"\n        assert _parse_annotation(\"typing.Optional[str]\") == \"typing.Optional[str]\"\n\n\n\n"], "sample_16": ["    def test_reshape(self):\n        self.check(np.reshape, (2, 3, 2))\n        self.check(np.reshape, (6,))\n        self.check(np.reshape, (2, 6))\n", "    def test_array_reshape(self):\n        self.check(np.reshape)\n        \n\n", "    def test_array2string(self, array_function):\n        with array_function_override(array2string):\n            o = array2string(self.q)\n        expected = f\"{self.q.value.tolist()}\"\n        assert o == expected\n\n\n\n", "    def test_array_str(self):\n        with pytest.raises(TypeError) as e:\n            np.array_str(self.q, precision=3)\n        assert (\n            \"quantity\" in str(e.value) or \"Quantity\" in str(e.value)\n        )\n\n", "    def test_reshape(self):\n        from astropy.units import Quantity\n\n            return x.reshape(newshape)\n\n        self.check(reshape_helper, newshape=(3, 1))\n        self.check(reshape_helper, newshape=(1, 9))\n\n\n\n"], "sample_612": ["    def test_groupby_fillna(self, array, dataset):\n        data = array.copy()\n        data.isel(x=0).fillna(0)\n\n        with dataset._set_indexes(data.index):\n            result = dataset.groupby(\"x\").fillna(0)\n\n        assert_allclose(result[\"foo\"], data)\n        assert_equal(result.attrs, dataset.attrs)\n", "    def test_first_last_skipna(array, dataset):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            # test first and last with skipna=False (default)\n            expected_first = array.isel({\n                \"x\": [0],\n                \"y\": [0],\n                \"z\": [0],\n            })\n            assert_allclose(array.groupby(\"x\").first().data, expected_first.data)\n            expected_last = array.isel({\n                \"x\": [2],\n                \"y\": [3],\n                \"z\": [1],\n            })\n            assert_allclose(array.groupby(\"x\").last().data, expected_last.data)\n\n            # test first and last with skipna=True\n            array_with_nan = array.copy()\n            array_with_nan.isel(x=1, y=1, z=0)[:] = np.nan\n            expected_first = array.isel({\n                \"x\": [0],\n                \"y\": [0],\n                \"z\": [0],\n            })\n            assert_allclose(array_with_nan.groupby(\"x\").first(skipna=True).data, expected_first.data)\n            expected_last = array.isel({\n                \"x\": [2],\n                \"y\": [3],\n                \"z\": [1],\n            })\n            assert_allclose(array_with_nan.groupby(\"x\").last(skipna=True).data, expected_last.data)\n", "    def test_groupby_reduce_string(dataset):\n        grouped = dataset.groupby(\"x\")\n        result = grouped.reduce(\n            lambda ds: ds.rename({\"foo\": \"new_foo\"}), dim=\"y\"\n        )\n\n        assert isinstance(result, Dataset)\n        assert result.dims == (\"x\", \"z\")\n        assert \"new_foo\" in result.data_vars\n        assert isinstance(result.new_foo.data, np.ndarray)\n        assert result.new_foo.shape == (3, 2)\n", "    def test_where_with_non_bool_condition(self, array):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", RuntimeWarning)\n            array_new = array.where(array > 0, other=0.5)\n            assert_allclose(array_new.values, np.where(array.values > 0, array.values, 0.5))\n", "def test_groupby_apply_with_args(dataset):\n        ds[\"new_var\"] = ((\"x\", \"y\"), ds[\"foo\"].mean(dim=\"z\") * x * y)\n        return ds\n\n    grouped = dataset.groupby(\"x\")\n    result = grouped.apply(my_func, args=(2, 3))\n\n    assert \"new_var\" in result.variables\n    assert_allclose(result[\"new_var\"].values, dataset[\"foo\"].mean(dim=\"z\").values * 2 * 3)\n\n    with pytest.raises(ValueError, match=r\"Too many arguments\"):\n        grouped.apply(my_func, args=(2, 3, 4))\n\n\n"], "sample_651": ["    def test_warns_matches_regex(self, testdir):\n        p = testdir.makepyfile(\n            \"\"\"\n            import warnings\n                warnings.warn(\"value must be 42\", UserWarning)\n            \"\"\"\n        )\n        result = testdir.runpytest(p, \"-v\", \"--capture=no\")\n        result.assert_outcomes(passed=1)\n        recwarn = WarningsRecorder()\n        with recwarn:\n            warnings.warn(\"value must be 42\", UserWarning)\n        assert len(recwarn.list) == 1\n\n        p.refresh()\n        result = testdir.runpytest(\n            p,\n            \"-v\",\n            \"--capture=no\",\n            \"--\",\n            \"pytest.warns(UserWarning, match=r'must be \\d+$')\",\n        )\n        result.assert_outcomes(passed=1)\n        \n        result = testdir.runpytest(\n            p, \"-v\", \"--capture=no\", \"--\", \"pytest.warns(UserWarning, match=r'must be 43+$')\"\n        )\n        result.assert_outcomes(failed=1)\n\n", "    def test_deprecated_call(self, pytester: Pytester) -> None:\n        pytester.make_pytest_run(\n            \"\"\"\n            import warnings\n            import pytest\n\n                warnings.warn('this is a deprecation warning', DeprecationWarning)\n            \n            @pytest.deprecated_call()\n                my_function()\n            \"\"\",\n            markers=[\"deprecated\"],\n        )\n", "    def test_deprecated_call_match(testdir: Pytester):\n        testdir.makepyfile(\n            \"\"\"\n                warnings.warn('use v3 of this api', DeprecationWarning)\n                return 200\n            \n                return 201\n            \"\"\"\n        )\n\n        result = testdir.runpytest(\n            \"-v\",\n            \"--strict-markers\",\n            '--disable-pytest-warnings',\n            \"test_deprecated_call_match.py\",\n        )\n        result.stdout.fnmatch_lines(\n            [\n                \"*test_deprecated_call_match*api_call_v2*\",\n                \"*deprecated_call(match='use v3 of this api')*api_call_v2*\",\n                \"*200*\",\n            ]\n        )\n\n        result = testdir.runpytest(\n            \"-v\",\n            \"--strict-markers\",\n            '--disable-pytest-warnings',\n            \"test_deprecated_call_match.py\",\n        )\n        result.stdout.fnmatch_lines(\n            [\n                \"*test_deprecated_call_match*api_call_v2*\",\n                \"*deprecated_call(match='use v3 of this api')*api_call_v2*\",\n                \"*200*\",\n            ]\n        )\n    ", "    def test_deprecated_call_context_manager(self, pytester: Pytester):\n        \"\"\"Tests deprecated_call used as a context manager.\"\"\"\n        pytester.raises_warning(\n            [\n                DeprecationWarning,\n            ],\n            \"\"\"\n            with pytest.deprecated_call():\n                warnings.warn('Use v3', DeprecationWarning)\n            \"\"\",\n        )\n", "    def test_deprecated_call_without_match(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import warnings\n\n                warnings.warn('use v3 of this api', DeprecationWarning)\n                return 200\n\n            @pytest.deprecated_call()\n                return api_call_v2()\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.assert_outcomes(passed=1)\n\n\n\n"], "sample_531": ["    def test_constrained_layout_with_transforms(self, image_comparison):\n\n        fig = Figure(figsize=(6, 4))\n        ax = fig.add_subplot(111)\n        ax.text(0.5, 0.5, \"Hello, constrained layout!\",\n                ha=\"center\", va=\"center\", transform=ax.transAxes)\n\n        # Add a gridspec subplot with a different transform\n        gs = gridspec.GridSpec(2, 2)\n        ax2 = fig.add_subplot(gs[0, 0])\n        ax2.text(0.5, 0.5, \"World!\", ha=\"center\", va=\"center\",\n                 transform=ax2.transAxes)\n\n        with image_comparison.PillowImageComparison(tol=0.01):\n            fig.canvas.draw()\n            im = io.BytesIO()\n            fig.savefig(im)\n            im.seek(0)\n            img = Image.open(im)\n            img.show()  \n", "    def test_constrained_layout_padding(self):\n        fig, ax = plt.subplots()\n        ax.set_aspect('equal')\n        t1 = ax.text(0.5, 0.5, 'test constrained layout', ha='center', va='center')\n        # Set some padding for the bounding box calculations\n        fig.set_constrained_layout_pads(wspace=0.1, hspace=0.1, w_pad=0.1, h_pad=0.1)\n        c_layout = ConstrainedLayoutEngine()\n        c_layout.execute(fig)\n\n        # Check that the padding has been applied\n        bbox = t1.get_window_extent()\n        assert bbox.width  > 0.01\n        assert bbox.height > 0.01\n", "    def test_constrained_layout_pads(self):\n        fig = Figure()\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            # Should still get a warning for using constrained layout with no engine\n            fig.constrained_layout()\n        assert len(w) == 1\n\n", "    def test_constrained_layout_execute(self):\n        fig = plt.figure(figsize=(6, 4))\n        ax1 = fig.add_subplot(2, 2, 1)\n        ax2 = fig.add_subplot(2, 2, 2)\n        ax3 = fig.add_subplot(2, 2, 3)\n        ax4 = fig.add_subplot(2, 2, 4)\n\n        ax1.set_title('Top Left')\n        ax2.set_title('Top Right')\n        ax3.set_title('Bottom Left')\n        ax4.set_title('Bottom Right')\n\n        \n        engine = ConstrainedLayoutEngine()\n        engine.execute(fig)\n\n        plt.close(fig)\n\n\n\n        \n", "    def test_constrained_layout_add_remove_axes(self):\n        fig = Figure()\n        ax = fig.add_subplot(111)\n        \n        # Add a constrained layout engine\n        engine = ConstrainedLayoutEngine()\n        fig.set_layout_engine(engine)\n        \n        # Initial layout\n        engine.execute(fig)\n        \n        # Add more axes\n        ax2 = fig.add_subplot(212)\n        engine.execute(fig)\n        \n        # Check if the second axes layout is correct\n        bbox_width = ax2.get_window_extent().width\n        assert bbox_width > 0\n        \n        # Remove an axes\n        fig.delaxes(ax2)\n        engine.execute(fig)\n        \n        # Check if the layout respects the removal\n        bbox_width = ax.get_window_extent().width\n        assert bbox_width > 0\n\n\n\n"], "sample_752": ["    def test_contamination_legacy(self):\n        # Test legacy contamination behavior\n        model = IsolationForest(contamination=\"legacy\")\n        X, y = iris.data, iris.target\n        model.fit(X)\n        assert_equal(model._contamination, 0.1)\n", "    def test_threshold_(self):\n        iforest = IsolationForest(contamination=\"auto\")\n        X_train, X_test, y_train, y_test = train_test_split(\n            iris.data, iris.target, test_size=0.5, random_state=42\n        )\n        iforest.fit(X_train)\n\n        assert_equal(iforest.threshold_, iforest.offset_)\n        assert_array_almost_equal(iforest.decision_function(X_test),\n                                  iforest.score_samples(X_test) - iforest.offset_)\n", "    def test_contamination_auto(self):\n        # Test with contamination='auto'\n        X_train, X_test, y_train, y_test = train_test_split(\n            boston.data, boston.target, test_size=0.2, random_state=42)\n        clf1 = IsolationForest(contamination='auto', random_state=42)\n        clf1.fit(X_train)\n        scores = clf1.decision_function(X_test)\n\n        clf2 = IsolationForest(contamination=0.1, random_state=42)\n        clf2.fit(X_train)\n        scores2 = clf2.decision_function(X_test)\n\n        assert_array_almost_equal(clf1.threshold_, clf2.threshold_)\n        assert_array_almost_equal(scores, scores2)\n\n", "    def test_contamination_legacy(self):\n        contamination = 'legacy'\n        X_train, X_test, y_train, y_test = train_test_split(\n            boston.data, boston.target, test_size=0.2, random_state=42)\n\n        model = IsolationForest(contamination=contamination)\n        model.fit(X_train)\n\n        assert_warns_message(\n            'default contamination parameter 0.1 will change in version 0.22 to \"auto\".',\n            model.fit, X_train)\n\n\n", "    def test_contamination_deprecated_threshold(self):\n        # test that the deprecated 'threshold_' attribute\n        # is replaced by the 'offset_' attribute\n        clf = IsolationForest(contamination=\"legacy\")\n        clf.fit(iris.data)\n        assert_warns_message(\n            DeprecationWarning,\n            \"threshold_ attribute is deprecated in 0.20 and will be removed in 0.22\",\n            clf.threshold_\n        )\n        assert_array_almost_equal(clf.threshold_, clf.offset_)\n"], "sample_407": ["    def test_manytoone_rel_field(self):\n        a = Article.objects.get(pk=self.a.pk)\n        self.assertEqual(a.reporter_id, self.r.pk)\n\n\n", "    def test_article_reporter_relationship(self):\n        # Ensure the article is associated with the reporter.\n        self.assertEqual(self.a.reporter, self.r)\n        # Retrieve the Article by its reporter.\n        article = Article.objects.filter(reporter=self.r).first()\n        self.assertEqual(article, self.a)\n        # Update the reporter and save.\n        self.a.reporter = self.r2\n        self.a.save()\n        # Ensure the update is reflected.\n        article = Article.objects.get(id=self.a.id)\n        self.assertEqual(article.reporter, self.r2)\n\n\n\n", "    def test_related_object_access(self):\n        a = Article.objects.get(headline=\"This is a test\")\n        self.assertEqual(a.reporter, Reporter.objects.get(first_name=\"John\"))\n", "     def test_related_manager_ordering(self):\n        from django.core.management.base import BaseCommand\n        \n        class TestOrderingCommand(BaseCommand):\n            help = 'Test ordering related objects'\n\n                articles = Article.objects.all().order_by('-pub_date')\n\n        with transaction.atomic():\n            # Create a few more Articles.\n            Article.objects.create(\n                headline=\"This is another test\", pub_date=datetime.date(2005, 7, 28), reporter=cls.r\n            )\n            Article.objects.create(\n                headline=\"This is a third test\", pub_date=datetime.date(2005, 7, 29), reporter=cls.r2\n            )\n        # Execute the test command\n        self.execute_command('test_ordering_command')\n        # Assert that the Articles are ordered correctly\n        self.assertEqual(articles[0].pub_date, datetime.date(2005, 7, 29))\n        self.assertEqual(articles[1].pub_date, datetime.date(2005, 7, 28))\n        self.assertEqual(articles[2].pub_date, datetime.date(2005, 7, 27))\n\n\n\n   \n", "    def test_article_reporter(self):\n        # Retrieve the article and its reporter.\n        article = Article.objects.get(headline=\"This is a test\")\n        reporter = article.reporter\n        self.assertEqual(reporter.first_name, \"John\")\n        self.assertEqual(reporter.last_name, \"Smith\")\n        self.assertEqual(reporter.email, \"john@example.com\")\n\n        # Change the reporter and save.\n        article.reporter = self.r2\n        article.save()\n        article = Article.objects.get(headline=\"This is a test\")\n        reporter = article.reporter\n        self.assertEqual(reporter.first_name, \"Paul\")\n        self.assertEqual(reporter.last_name, \"Jones\")\n        self.assertEqual(reporter.email, \"paul@example.com\")\n"], "sample_186": ["    def test_system_checks_inline_admin(self):\n        class AlbumInline(GenericStackedInline):\n            model = Song\n            fk_name = 'album'\n            extra = 1\n\n        class AlbumAdmin(admin.ModelAdmin):\n            inlines = [AlbumInline]\n            exclude = ['title']\n\n        site = AdminSite()\n        site.register(Album, AlbumAdmin)\n\n        errors = checks.run_checks(site)\n        self.assertEqual(len(errors), 2)\n        self.assertIn(\n            'Cannot exclude the field',\n            str(errors[0]),\n        )\n        self.assertIn(\n            'The value of \\'formset\\' must inherit from \\'BaseModelFormSet\\'',\n            str(errors[1]),\n        )\n\n\n\n", "    def test_system_checks(self):\n        site = AdminSite()\n        site.register(Song, MyAdmin)\n        site.register(Album, MyAdmin)\n        site.register(Author, MyAdmin)\n        site.register(Book, MyAdmin)\n        site.register(City, MyAdmin)\n        site.register(State, MyAdmin)\n        site.register(TwoAlbumFKAndAnE, MyAdmin)\n        errors = checks.run_checks(site)\n        self.assertEqual(len(errors), 0)\n\n\n\n        # test that the model admin checks work correctly\n        errors = checks.run_checks(ValidFields())\n        self.assertEqual(len(errors), 0)\n\n        errors = checks.run_checks(ValidFormFieldsets())\n        self.assertEqual(len(errors), 0)\n", "    def test_admin_checks_inline(self):\n        class Inline(GenericStackedInline):\n            model = Song\n            extra = 1\n\n        class AlbumAdmin(admin.ModelAdmin):\n            inlines = [Inline]\n\n        with self.assertNumQueries(1) as context:\n            checks.run_checks([AlbumAdmin])\n        self.assertEqual(len(context), 2)  # 1 for checking  `admin_checks.InlineModelAdminChecks`\n\n\n\n", "    def test_admin_checks(self):\n        check_results = checks.run_checks([\n            'admin_checks.models.AlbumAdmin',\n            'admin_checks.models.TwoAlbumFKAndAnEAdmin',\n            'admin_checks.models.BookAdmin',\n            'admin_checks.models.AuthorAdmin',\n            'admin_checks.models.CityAdmin',\n            'admin_checks.models.StateAdmin',\n        ])\n        self.assertEqual(len(check_results), 0)\n\n        check_results = checks.run_checks([\n            'admin_checks.models.SongAdmin',\n            'admin_checks.models.ValidFields',\n            'admin_checks.models.ValidFormFieldsets',\n            'admin_checks.models.MyAdmin',\n        ])\n        self.assertEqual(len(check_results), 0)\n\n\n        site = AdminSite()\n\n        # Register Admin classes\n        site.register(Album, AlbumAdmin)\n        site.register(TwoAlbumFKAndAnE, TwoAlbumFKAndAnEAdmin)\n        site.register(Book, BookAdmin)\n        site.register(Author, AuthorAdmin)\n        site.register(Song, SongAdmin)\n        site.register(State, StateAdmin)\n\n        # Initialize Middlewares\n        #\n        # Example Middlewares\n        #\n        authentication_middleware = AuthenticationMiddlewareSubclass()\n        message_middleware = MessageMiddlewareSubclass()\n        session_middleware = SessionMiddlewareSubclass()\n        \n        # Register Middlewares\n        #\n        # Here we register the middlewares\n        \n        \n", "    def test_check_ModelAdmin_inlines(self):\n        from django.contrib.admin import StackedInline\n        class AlbumInline(StackedInline):\n            model = Song\n            extra = 1\n            fk_name = 'album'\n        class AlbumAdmin(admin.ModelAdmin):\n            inlines = [AlbumInline]\n        result = ModelAdminChecks().check(AlbumAdmin)\n        self.assertEqual(len(result), 0)\n\n"], "sample_851": ["    def test_explained_variance_score_multioutput(self, multioutput):\n        y_true = np.array([[1, 2], [3, 4], [5, 6]])\n        y_pred = np.array([[1.2, 1.8], [3.1, 3.9], [5.2, 6.1]])\n        scores = explained_variance_score(y_true, y_pred, multioutput=multioutput)\n\n        if multioutput == 'raw_values':\n            assert_array_equal(scores.shape, (y_true.shape[1],))\n        else:\n            assert_allclose(scores, [0.8469, 0.8469])\n\n\n", "    def test_median_absolute_error_multioutput(\n        self, multioutput, expected_error,", "    def test_mean_tweedie_deviance_power(power):\n        y_true = np.array([1, 2, 3, 4])\n        y_pred = np.array([0.8, 1.9, 2.8, 4.2])\n        for sample_weight in [None, np.array([0.2, 0.5, 0.8, 1])]:\n            with pytest.raises(ValueError) if power < 0 else assert_almost_equal(\n                    mean_tweedie_deviance(y_true, y_pred,\n                                          sample_weight=sample_weight, power=power),\n                    1.3936,\n                    decimal=4):\n                pass\n\n", "    def test_mean_tweedie_deviance_power_validation(\n        self", "    def test_mean_tweedie_deviance_power_out_of_bounds(self):\n        y_true = np.array([1, 2, 3, 4])\n        y_pred = np.array([1.2, 1.8, 2.5, 3.8])\n\n        with pytest.raises(ValueError,\n                           match=\"Tweedie deviance is only defined for power<=0 and \"\n                                 \"power>=1.\"):\n            mean_tweedie_deviance(y_true, y_pred, power=0.5)\n\n        with pytest.raises(ValueError,\n                           match=\"Tweedie deviance is only defined for power<=0 and \"\n                                 \"power>=1.\"):\n            mean_tweedie_deviance(y_true, y_pred, power=2.5)\n\n\n\n"], "sample_271": ["    def test_iter_all_python_module_files_cache(self):\n        with tempfile.TemporaryDirectory() as temp_dir:\n            zip_path = Path(temp_dir) / 'test.zip'\n            with zipfile.ZipFile(zip_path, 'w') as zipf:\n                zipf.write('test.py')\n\n            # Add zip file to sys.path\n            extend_sys_path(temp_dir)\n            self.import_and_cleanup('test')\n\n            # Call iter_all_python_module_files() twice,\n            # it should return the same cached result\n            self.assertEqual(list(autoreload.iter_all_python_module_files()), list(autoreload.iter_all_python_module_files()))\n\n            # Remove the zip file\n            zip_path.unlink()\n\n            # Call iter_all_python_module_files() again, it should return\n            # an updated result without the zip file\n            self.assertNotIn(zip_path, list(autoreload.iter_all_python_module_files()))\n\n\n\n", "    def test_iter_modules_and_files_with_zip_file(self):\n        with tempfile.TemporaryDirectory() as tempdir:\n            zip_filename = self.temporary_file('my_module.zip')\n            with zipfile.ZipFile(zip_filename, 'w') as zf:\n                zf.writestr('my_module/__init__.py', '')\n                zf.writestr('my_module/some_file.py', '')\n\n            extend_sys_path([tempdir, str(zip_filename)])\n            self.import_and_cleanup('my_module')\n            self.assertFileFound(zip_filename / 'my_module/__init__.py')\n            self.assertFileFound(zip_filename / 'my_module/some_file.py')\n\n\n\n", "    def test_iter_modules_and_files_with_zip_file(self):\n        zip_filename = self.temporary_file('my_module.zip')\n        with zipfile.ZipFile(zip_filename, 'w') as zf:\n            zf.writestr('my_module/__init__.py', '')\n            zf.writestr('my_module/test.py', '')\n        \n        extend_sys_path(str(zip_filename))\n        self.import_and_cleanup('my_module')\n        self.assertFileFound(zip_filename / 'my_module/__init__.py')\n        self.assertFileFound(zip_filename / 'my_module/test.py')\n\n        # Test removing the zip file and make sure it's not listed anymore\n        os.remove(zip_filename)\n        self.assertFileNotFound(zip_filename / 'my_module/__init__.py')\n        self.assertFileNotFound(zip_filename / 'my_module/test.py')\n", "    def test_iter_modules_and_files_zip_file(self):\n        with tempfile.TemporaryDirectory() as tempdir:\n            zip_path = Path(tempdir) / 'test.zip'\n            with zipfile.ZipFile(zip_path, 'w') as zf:\n                zf.write('test_module.py', arcname='test_module.py')\n            extend_sys_path([tempdir])\n            self.import_and_cleanup('test_module')\n\n            self.assertFileFound(zip_path / 'test_module.py')\n\n\n", "    def test_iter_modules_and_files_zipfile(self):\n        with tempfile.TemporaryDirectory() as tempdir:\n            zip_filename = self.temporary_file(\"my_module.zip\")\n            with zipfile.ZipFile(zip_filename, 'w') as zf:\n                zf.writestr('my_module/__init__.py', '')\n                zf.writestr('my_module/test_module.py', '')\n            \n            extend_sys_path([tempdir])\n            self.import_and_cleanup('my_module')\n            self.assertFileFound(zip_filename)\n"], "sample_894": ["    def test_sparse_input(name):\n        \"\"\"Test sparse input matrices.\"\"\"\n        ForestClassifier = FOREST_CLASSIFIERS[name]\n\n        X_sparse = csr_matrix(X)\n        clf = ForestClassifier(n_estimators=10, random_state=1)\n        clf.fit(X_sparse, y)\n        assert_array_equal(clf.predict(X_sparse), clf.predict(X))\n\n        X_sparse = coo_matrix(X)\n        clf = ForestClassifier(n_estimators=10, random_state=1)\n        clf.fit(X_sparse, y)\n        assert_array_equal(clf.predict(X_sparse), clf.predict(X))\n", "    def test_default_params(self, name):\n        ForestClassifier = FOREST_CLASSIFIERS[name]\n        clf = ForestClassifier()\n        assert clf.n_estimators == 100\n        assert clf.max_depth is None\n        assert clf.min_samples_split == 2\n        assert clf.min_samples_leaf == 1\n        assert clf.min_weight_fraction_leaf == 0.0\n        assert clf.max_features == 'auto'\n        assert clf.random_state is None\n", "    def test_classification_predict(self, name, method):\n        ForestClassifier = FOREST_CLASSIFIERS[name]\n\n        X, y = datasets.make_classification(n_samples=100, random_state=42)\n        clf = ForestClassifier(n_estimators=10, random_state=42)\n        clf.fit(X, y)\n        if method == \"predict_proba\":\n            y_pred = clf.predict_proba(X)\n            assert y_pred.shape[1] == 2\n            assert np.allclose(np.sum(y_pred, axis=1), 1.0)\n        else:\n            y_pred = clf.predict(X)\n            assert_array_equal(y_pred, clf.predict(X))\n", "    def test_random_state(name):\n        \"\"\"Test the random state.\"\"\"\n        ForestClassifier = FOREST_CLASSIFIERS[name]\n        rng = check_random_state(42)\n        clf1 = ForestClassifier(n_estimators=10, random_state=rng)\n        clf2 = ForestClassifier(n_estimators=10, random_state=rng)\n        clf1.fit(X_large, y_large)\n        clf2.fit(X_large, y_large)\n        assert_array_almost_equal(clf1.feature_importances_, clf2.feature_importances_)\n", "    def test_feature_importances(name):\n        \"\"\"Test feature importances for classification.\"\"\"\n        clf = FOREST_CLASSIFIERS[name](n_estimators=10, random_state=1)\n        clf.fit(X_large, y_large)\n\n        # Check that importances are not all zeros and some have meaningful values\n        assert np.any(clf.feature_importances_ != 0)\n        # Check that importances are sorted in descending order\n        assert np.all(np.diff(clf.feature_importances_) <= 0)\n"], "sample_1167": ["compilation error", "compilation error", "    def test_multiline_latex_matrices(self):\n        from sympy import Matrix, diag\n\n        A = Matrix([[1, 2], [3, 4]])\n        B = diag(5, 6)\n\n        print(multiline_latex(A, B, terms_per_line=1, environment='align*', use_dots=True))\n        print(multiline_latex(A, B, terms_per_line=1, environment='eqnarray', use_dots=True))\n        print(multiline_latex(A, B, terms_per_line=1, environment='IEEEeqnarray', use_dots=True))\n\n\n\n\n", "    def test_divisor_sigma(self):\n        self.assertEqual(latex(divisor_sigma(2, 5)), r\"\\sigma_{5}^{2}\")\n        self.assertEqual(latex(divisor_sigma(2, 3)), r\"\\sigma_{3}^{2}\")\n        self.assertEqual(latex(divisor_sigma(6, 4)), r\"\\sigma_{4}^{6}\")\n        self.assertEqual(latex(divisor_sigma(10, 2)), r\"\\sigma_{2}^{10}\")\n\n\n\n\n", "    def test_parenthesized_subscript(self):\n        expr = MatrixSymbol('A', 2, 3)[1, 2:4]\n        self.assertEqual(latex(expr), r'\\mathbf{A}_{1, \\left[2, 4\\right] }')\n"], "sample_574": ["    def test_temporal_ticks(self, x):\n        s = Temporal().tick(upto=3)._setup(x, Coordinate())\n        a = PseudoAxis(s._matplotlib_scale)\n        a.set_view_interval(0, 1)\n\n        locs = a.major.locator()\n\n        assert len(locs) == 3\n        assert all(np.isclose(locs, [0, 1, 2])\n\n\n\n", "    def test_tick_locator(self, x):\n        a = self.setup_ticks(x, locator=mpl.ticker.MaxNLocator(3))\n        assert len(a.get_tick_space()) == 3\n        a = self.setup_ticks(x, upto=4)\n        assert len(a.get_tick_space()) <= 4\n\n", "    def test_continuous_trans_identity(self, x):\n        s = Continuous(trans=None)._setup(x, Coordinate())\n        a = PseudoAxis(s._matplotlib_scale)\n        a.set_view_interval(0, 10)\n        ticks = a.major.locator()\n        assert_array_equal(ticks, np.arange(0, 10, 1))\n", "    def test_continuous_tick_with_every(self, x):\n        a = self.setup_ticks(x, every=2)\n        assert_array_equal(a.majorticklocs(), [0.0, 2.0])\n        ", "    def test_temporal_basic(self, x):\n        s = Temporal().tick(upto=5)._setup(x, Coordinate())\n        a = PseudoAxis(s._matplotlib_scale)\n        a.set_view_interval(0, 1)\n        locs = a.major.locator()\n        assert len(locs) == 5\n\n\n"], "sample_306": ["    def test_parse_datetime_offset(self):\n        dt = parse_datetime('2023-10-26T10:30:00+05:30')\n        self.assertEqual(dt.year, 2023)\n        self.assertEqual(dt.month, 10)\n        self.assertEqual(dt.day, 26)\n        self.assertEqual(dt.hour, 10)\n        self.assertEqual(dt.minute, 30)\n        self.assertEqual(dt.second, 0)\n        self.assertEqual(dt.tzinfo, get_fixed_timezone(19800)) \n", "    def test_parse_duration_iso8601(self):\n        self.assertEqual(parse_duration('P1D'), timedelta(days=1))\n        self.assertEqual(parse_duration('P1DT2H3M4S'), timedelta(days=1, hours=2, minutes=3, seconds=4))\n        self.assertEqual(parse_duration('PT2H3M4S'), timedelta(hours=2, minutes=3, seconds=4))\n        self.assertEqual(parse_duration('PT2H3M'), timedelta(hours=2, minutes=3))\n        self.assertEqual(parse_duration('P1DT2H3M4S.5'), timedelta(days=1, hours=2, minutes=3, seconds=4, microseconds=500000))\n        self.assertEqual(parse_duration('-P1D'), timedelta(days=-1))\n        self.assertEqual(parse_duration('-PT2H3M4S'), timedelta(hours=-2, minutes=-3, seconds=-4))\n\n\n\n", "    def test_parse_datetime_iso8601_with_timezone(self):\n        dt = parse_datetime('2023-10-27T14:30:00+05:30')\n        self.assertEqual(dt.date(), date(2023, 10, 27))\n        self.assertEqual(dt.time(), time(14, 30))\n        self.assertEqual(dt.tzinfo, get_fixed_timezone(19500)) # 5.5 hours in minutes\n", "    def test_parse_datetime_with_tz(self):\n        tz = get_fixed_timezone(offset=3600)\n        dt = datetime(2023, 12, 25, 10, 30, 0, tzinfo=tz)\n        self.assertEqual(parse_datetime(\"2023-12-25T10:30:00+01:00\"), dt)\n", "    def test_parse_datetime_with_tzinfo(self):\n        dt = parse_datetime('2023-10-27T14:30:00+05:30')\n        self.assertEqual(dt.date(), date(2023, 10, 27))\n        self.assertEqual(dt.time(), time(14, 30))\n        self.assertEqual(dt.tzinfo, get_fixed_timezone(19800))\n"], "sample_657": ["    def test_get_empty_parameterset_mark(self, request):\n        with mock.patch(\"py._pytest.mark.Collector\") as mock_collector:\n            mock_collector.CollectError.return_value = None\n            config = mock.Mock(spec=Config)\n            config.getini.return_value = \"skip\"\n\n            func = mock.Mock(spec=object)\n            \n            mark = get_empty_parameterset_mark(config, [\"arg1\"], func)\n\n            assert isinstance(mark, Mark)\n            assert mark.name == \"skip\"\n            mock_collector.CollectError.assert_not_called()\n", "    def test_empty_parameterset_mark_empty_args(self, request):\n        config = mock.Mock(spec=pytest.PytestConfig)\n        config.getini.return_value = \"\"\n        func = mock.Mock(spec=Collector)\n        _, lineno = getfslineno(func)\n        with pytest.raises(Collector.CollectError) as excinfo:\n            get_empty_parameterset_mark(config, [], func)\n        assert (\n            str(excinfo.value)\n            == \"Empty parameter set ' ' function %s at %d\"\n            % (func.__name__, lineno + 1)\n        )\n\n", "    def test_mark_decorator_with_args(self, attr, modulename):\n        mark = getattr(getattr(globals()[modulename], \"mark\"), attr)\n\n        with mock.patch(\n            \"builtins.print\", wraps=print\n        ) as mock_print, mock.patch(\n            \"pytest._pytest.mark.store_mark\", return_value=None\n        ) as mock_store_mark:\n            mark(\"arg1\", \"arg2\")\n        expected_msg = (\n            f\"{modulename}.mark.{attr} should call store_mark with correct arguments\"\n        )\n        assert mock_store_mark.called with expected_msg\n", "    def test_mark_decorator_creation(self, modulename):\n        mark = Mark(f\"{modulename}.test_mark\")\n        decorator = MarkDecorator(mark)\n        assert decorator.name == \"test_mark\"\n        assert decorator.mark == mark\n        assert decorator.args == ()\n        assert decorator.kwargs == {}\n        assert str(decorator) == \"<MarkDecorator test_mark>\"\n\n\n\n", "    def test_mark_empty_parameter_set(self, request, inifile):\n        config = mock.MagicMock()\n        config.getini.return_value = EMPTY_PARAMETERSET_OPTION\n        argnames = ['a', 'b']\n        func = mock.MagicMock(\n            __name__ = 'test_function',\n            __call__=mock.Mock(),\n        )\n        mark = get_empty_parameterset_mark(config, argnames, func)\n        assert isinstance(mark, MarkGenerator.with_args)\n        assert mark.name == 'skip'\n        assert len(mark.args) == 0\n        assert len(mark.kwargs) == 1\n        assert mark.kwargs['reason'] == 'got empty parameter set (\"\", function test_function at %s:%d)' % (\n            inifile,\n            6\n        )\n"], "sample_461": ["    def test_assume_scheme_deprecated(self):\n        with self.assert_warnings(category=RemovedInDjango60Warning):\n            field = URLField(assume_scheme=\"https\")\n            self.assertEqual(field.assume_scheme, \"https\") \n            field = URLField(assume_scheme=\"http\")\n            self.assertEqual(field.assume_scheme, \"http\")\n        \n\n\n", "    def test_assume_scheme_auto(self):\n        field = URLField(assume_scheme=\"https\")\n        self.assertEqual(field.assume_scheme, \"https\")\n\n        with self.subTest(\"empty value\"):\n            self.assertTrue(field.clean(\"\") == \"\")\n        \n        with self.subTest(\"http URL\"):\n            self.assertEqual(field.clean(\"http://www.example.com\"), \"https://www.example.com\")\n        \n        with self.subTest(\"https URL\"):\n            self.assertEqual(field.clean(\"https://www.example.com\"), \"https://www.example.com\")\n\n\n", "    def test_url_with_scheme(self):\n        field = URLField(assume_scheme=\"https\")\n        self.assert_field_value_is_valid(field, \"https://www.example.com\")\n", "    def test_url_field_validation(self):\n        field = URLField()\n        self.assertFieldValid(field, \"https://www.example.com\")\n        self.assertFieldInvalid(field, \"invalid-url\", error_code=\"invalid\")\n        self.assertFieldInvalid(field, \"www.example.com\", error_code=\"invalid\")\n        self.assertFieldInvalid(field, \"http://\", error_code=\"invalid\")\n\n\n\n", "    def test_url_field_allowed_schemes(self):\n        field = URLField(assume_scheme=\"https\")\n        self.assert_field_renders(field, value=\"www.example.com\")\n        self.assert_field_renders(field, value=\"https://www.example.com\")\n        self.assert_field_renders(field, value=\"https://www.example.com/path\")\n        with self.assertRaises(ValidationError):\n            field.clean(\"http://www.example.com\")\n"], "sample_181": ["    def test_exists_with_filtered_subquery(self):\n        subquery = Book.objects.filter(\n            rating__gt=4,\n            price__lt=Decimal('25.00')\n        ).values_list('isbn', flat=True)\n\n        exists_query = Author.objects.filter(\n            exists=Subquery(subquery)\n        )\n        self.assertCountEqual(\n            exists_query.values_list('name', flat=True),\n            ['test', 'test3']\n        )\n\n\n\n", "    def test_aggregate_with_filter(self):\n        avg_price = Book.objects.filter(contact__name='test').aggregate(\n            avg_price=Avg('price')\n        )['avg_price']\n        self.assertAlmostEqual(avg_price, 30.00, places=2)\n\n        count = Book.objects.filter(\n            contact__name='test'\n        ).annotate(\n            average_price=Avg('price')\n        ).values('average_price').count()\n        self.assertEqual(count, 2)\n\n        subquery = Book.objects.filter(\n            contact__name='test'\n        ).values('price').annotate(\n            avg_price=Avg('price')\n        )\n        filtered_avg_price = Book.objects.annotate(\n            avg_price=Subquery(subquery)\n        ).values('avg_price').first()['avg_price']\n        self.assertEqual(filtered_avg_price, 30.00)\n\n        filtered_count = Book.objects.annotate(\n            avg_price=Subquery(subquery)\n        ).values('avg_price').count()\n        self.assertEqual(filtered_count, 2)\n\n        # Check that an aggregate can filter on an outer query\n        counts = Book.objects.filter(rating__gt=3).annotate(\n            author_count=Count('contact')\n        ).values()\n        self.assertEqual(len(counts), 2)\n        self.assertEqual(counts[0]['author_count'], 2)\n        self.assertEqual(counts[1]['author_count'], 1)\n\n\n\n", "    def test_filtered_aggregate_subquery_filter(self):\n        filtered_avg_price = Book.objects.annotate(\n            avg_price=Avg('price')\n        ).filter(\n            Q(authors__name='test') | Q(publisher__name='Apress'),\n        )\n        avg_price = filtered_avg_price.aggregate(\n            avg_price=Avg('price')\n        )['avg_price']\n        self.assertEqual(avg_price, Decimal('29.845'))\n\n\n\n", "    def test_subquery_with_multi_lookup(self):\n        subquery = Book.objects.filter(\n            ISBN__startswith='159059').values('contact__name').annotate(\n            count=Count('id')\n        )\n        result = Author.objects.filter(\n            books__in=subquery.filter(count__gte=2)\n        ).values_list('name', flat=True)\n        self.assertQueryCount(result, 1)\n        self.assertEqual(list(result), ['test3'])\n\n\n\n", "    def test_filtered_aggregate_with_subquery(self):\n        subquery = Subquery(\n            Author.objects.filter(age__gt=50).values('name')\n        )\n        filtered_aggregate = (\n            Avg('price')\n            .filter(\n                contact__name__in=subquery,\n            )\n        )\n\n        avg_price = Book.objects.annotate(average_price=filtered_aggregate).values('average_price')[\n            0\n        ]['average_price']\n\n        self.assertAlmostEqual(avg_price, 26.345, places=3)\n\n\n\n"], "sample_1010": ["compilation error", "compilation error", "    def test_symbols(self):\n        for name in dir(sym):\n            obj = getattr(sym, name)\n            if isinstance(obj, Symbol):\n                assert latex(obj) == obj.name\n", "    def test_latex_product(self):\n        expr = Mul(x, y, z**2)\n        latex_output = latex(expr)\n        expected_output = r\"x y z^{2}\"\n        assert latex_output == expected_output\n        ", "    def test_latex_vector_fields(self):\n        cs = CoordSys3D('x')\n        x, y, z = cs.basis_vectors\n        self.assertEqual(latex(x), '\\\\partial_{x}')\n        self.assertEqual(latex(y), '\\\\partial_{y}')\n        self.assertEqual(latex(z), '\\\\partial_{z}')\n        self.assertEqual(latex(cs.x), '\\\\partial_{x}')\n        self.assertEqual(latex(cs.y), '\\\\partial_{y}')\n        self.assertEqual(latex(cs.z), '\\\\partial_{z}')\n"], "sample_472": ["    def test_paginator_get_elided_page_range(self):\n        \"\"\"\n        Test that get_elided_page_range works correctly for various page numbers.\n        \"\"\"\n        paginator = Paginator(list(range(50)), per_page=10)  \n        \n        # Test with page in the middle of the range\n        self.assertListEqual(\n            list(paginator.get_elided_page_range(number=10, on_each_side=3, on_ends=2)),\n            [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, '...', 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]\n        )\n        \n        # Test with page at the beginning and end\n        self.assertListEqual(\n            list(paginator.get_elided_page_range(number=1, on_each_side=3, on_ends=2)),\n            [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, '...', 50]\n        )\n        self.assertListEqual(\n            list(paginator.get_elided_page_range(number=50, on_each_side=3, on_ends=2)),\n            [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, '...', 47, 48, 49, 50]\n        )\n\n\n\n\n", "    def test_get_elided_page_range(self):\n        object_list = [None] * 50\n        paginator = Paginator(object_list, 5)\n        for i in paginator.get_elided_page_range(number=43):\n            self.assertTrue(1 <= i <= paginator.num_pages)\n        for i in paginator.get_elided_page_range(number=1):\n            self.assertTrue(1 <= i <= paginator.num_pages)\n        for i in paginator.get_elided_page_range(number=25):\n            self.assertTrue(1 <= i <= paginator.num_pages)\n\n\n", "    def test_paginator_get_elided_page_range(self):\n        object_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n        paginator = Paginator(object_list, 3)\n        for i in range(1, paginator.num_pages + 1):\n            for actual_page in paginator.get_elided_page_range(number=i):\n                self.assertIn(actual_page, range(1, paginator.num_pages + 1))\n", "    def test_get_elided_page_range(self):\n        paginator = Paginator(\n            list(range(50)), per_page=10, orphans=0\n        )\n        for i in paginator.get_elided_page_range(number=23):\n            self.assertIn(i, range(1, paginator.num_pages + 1))\n        for i in paginator.get_elided_page_range(number=1):\n            self.assertIn(i, range(1, paginator.num_pages + 1))\n        for i in paginator.get_elided_page_range(number=paginator.num_pages):\n            self.assertIn(i, range(1, paginator.num_pages + 1))\n\n\n\n", "    def test_get_elided_page_range(self):\n        paginator = Paginator(\n            Article.objects.all(),\n            per_page=10,\n            orphans=0,\n            allow_empty_first_page=True,\n        )\n        for i, expected in enumerate([1, 2, 3, ..., 10], start=1):\n            with self.subTest(i=i):\n                for page_number in paginator.get_elided_page_range(number=i):\n                    self.assertEqual(page_number, expected)\n\n\n\n"], "sample_697": ["    def test_getbasetemp_with_env_temp_root(monkeypatch: MonkeyPatch, tmp_path: Path):\n        monkeypatch.setenv(\"PYTEST_DEBUG_TEMPROOT\", str(tmp_path))\n        factory = TempPathFactory(None, trace=None, _ispytest=True)\n        basetemp = factory.getbasetemp()\n        assert basetemp.exists()\n        assert basetemp.name.startswith(\"pytest-of-\")\n        assert basetemp.parent == tmp_path\n\n\n\n", "    def test_getbasetemp_from_env_with_illegal_chars(\n        self, tmpdir: Path, tmp_path_factory: TempPathFactory", "    def test_getbasetemp_from_env(tmpdir_factory: TempPathFactory, monkeypatch: MonkeyPatch) -> None:\n        monkeypatch.setenv(\"PYTEST_DEBUG_TEMPROOT\", str(tmpdir_factory.getbasetemp()))\n        try:\n            with tempfile.TemporaryDirectory() as tempdir:\n                monkeypatch.setenv(\"PYTEST_DEBUG_TEMPROOT\", str(Path(tempdir)))\n                factory = TempPathFactory.from_config(Config(), _ispytest=True)\n                assert factory.getbasetemp().resolve() == Path(tempdir)\n        finally:\n            monkeypatch.delenv(\"PYTEST_DEBUG_TEMPROOT\")\n\n\n", "    def test_getbasetemp(tmp_path_factory: TempPathFactory, pytester: Pytester):\n        \"\"\"Test that getbasetemp creates and uses the given basetemp if provided.\"\"\"\n        with pytester.chdir(pytester.getbasetempdir()):\n            config = Config()\n            config.option.basetemp = tmp_path_factory.getbasetemp()\n            tmppath_factory = TempPathFactory.from_config(config, _ispytest=True)\n            assert tmppath_factory.getbasetemp() == config.option.basetemp\n\n            # Test that getbasetemp without basetemp uses the default tempdir\n            config = Config()\n            config.option.basetemp = None\n            tmppath_factory = TempPathFactory.from_config(config, _ispytest=True)\n            basetemp = tmppath_factory.getbasetemp()\n            assert basetemp.parts[-1] == \"pytest-of-\" + get_user()\n", "    def test_getbasetemp_when_env_is_set(\n        self, tmp_path_factory: TempPathFactory, monkeypatch: MonkeyPatch"], "sample_1017": ["    def test_bool_map_issue_4835():\n        eq = And(Xor(a, b), c, And(c,d))\n        eq2 = eq.subs(c, x)\n        result, mapping = bool_map(eq, eq2)\n        assert result == eq2\n        assert mapping == {a: a, b: b, c: d, d: x}\n\n", "    def test_bool_map_complex():\n        eq = And(Xor(a, b), c, And(c,d))\n        eq2 = eq.subs(c, x)\n        result, mapping = bool_map(eq, eq2)\n        assert result == (eq2)\n    \n        assert mapping == {a: a, b: b, c: d, d: x}\n", "compilation error", "compilation error", "    def test_bool_map_complex():\n        eq1 = And(Xor(a, b), c, And(c,d))\n        eq2 = eq1.subs(c, x)\n        result, mapping = bool_map(eq1, eq2)\n        assert result == eq2\n        assert mapping == {a: a, b: b, c: d, d: x}\n\n"], "sample_884": ["    def test_inheritance(self):\n        with pytest.warns(FutureWarning, match=\"MockClass1 is deprecated\"):\n            instance = MockClass5(1)\n        assert instance.a == 1\n", "    def test_deprecated_class_inheritance(self):\n        with pytest.warns(FutureWarning, match=\"MockClass1 is deprecated\"):\n            mock = MockClass5(1)\n        assert mock.a == 1\n        assert mock.a == 1\n", "    def __init__(self, *args, **kwargs):\n        pass\n", "    def test_inheritance(self):\n        with pytest.warns(FutureWarning, match=\"Class MockClass1 is deprecated\"):\n            MockClass5(1)\n", "    def __init__(self, *args, **kwargs):\n        warnings.warn(\"deprecated message\", category=FutureWarning)\n\n\n"], "sample_492": ["    def test_serialize_nested_dict(self):\n        operation = custom_migration_operations.more_operations.CreateNestedDictField(\n            name='nested_dict', field=models.TextField()\n        )\n        serializer = OperationWriter(operation)\n        output, imports = serializer.serialize()\n\n        self.assertIn(\"migrations.CreateModel\", output)\n        self.assertIn(\n            'name=\"nested_dict\", field=models.TextField()', output\n        )\n\n\n", "    def test_serializes_datetime(self):\n        from django.utils import timezone\n\n        operation = migrations.RunPython(lambda apps: apps.get_model(\"tests\", \"TestModel1\").objects.update(thing=\"datetime\"),)\n        operation.migration.name = \"0001_initial\"\n        writer = OperationWriter(operation)\n        result, imports = writer.serialize()\n        self.assertIn(\"from django.utils import timezone\", imports)\n        self.assertIn(\"apps.get_model(\\\"tests\\\", \\\"TestModel1\\\").objects.update(thing=timezone.now())\", result)\n", "    def test_serialize_nested_dicts_in_operation(self):\n        operation = custom_migration_operations.more_operations.CreateNestedDictIndex(\n            name=\"my_index\", fields=[\"field1\", \"field2\"],\n            index_options={\"unique\": True, \"populate_from\": [\"field1\", \"field2\"]}\n        )\n        serializer = MigrationWriter.serialize(operation)\n\n        expected_output = (\n            \"migrations.CreateNestedDictIndex(\"\n            \"name='my_index', fields=['field1', 'field2'],\"\n            \" index_options={'unique': True, 'populate_from': ['field1', \"\n            \"'field2']})\"\n        )\n        self.assertEqual(serializer, expected_output)\n", "    def test_serialize_deconstructible_instances(self):\n        operation = custom_migration_operations.more_operations.CreateDeconstructibleInstances(\n            instances=[DeconstructibleInstances()]\n        )\n        serialized_operation, _ = OperationWriter(operation).serialize()\n        self.assertEqual(\n            serialized_operation,\n            \"more_operations.CreateDeconstructibleInstances([\"\n            \"DeconstructibleInstances(),\"\n            \"])\",\n        )\n\n", "    def test_serialize_enum(self):\n        for enum_type in (TextEnum, TextTranslatedEnum, BinaryEnum, IntEnum, IntFlagEnum):\n            for value in enum_type:\n                serialized = OperationWriter(\n                    custom_migration_operations.more_operations.RenameEnum(\n                        old_name=enum_type.__name__, new_name=enum_type.__name__\n                    )\n                ).serialize()\n                self.assertIn(\n                    \"%s.%s\" % (enum_type.__module__, value.name), serialized[0]\n                )\n\n"], "sample_674": ["def test_ischildnode(ischildnode):\n    for baseid, nodeid, expected in nodes._splitnode:\n        assert ischildnode(baseid, nodeid) == expected\n\n", "def test_ischildnode(ischildnode):\n    for baseid, nodeid, expected in nodes.ischildnode:\n        assert ischildnode(baseid, nodeid) == expected\n", "def test_ischildnode_complex(baseid, nodeid, expected):\n    assert nodes.ischildnode(baseid, nodeid) == expected\n", "def test_ischildnode(\n    baseid, nodeid, expected,", "def test_ischildnode(ischildnode, baseid):\n    assert ischildnode(baseid, baseid)\n"], "sample_1072": ["    def test_frac_complex_args(self):\n        assert frac(I).rewrite(floor) == I - floor(I)\n        assert frac(2*I).rewrite(floor) == 2*I - floor(2*I)\n        assert frac(x + I*y).rewrite(floor) == x + I*y - floor(x + I*y)\n        assert frac(x + I*y).rewrite(frac) == x + I*y - frac(x + I*y)\n        assert frac(Rational(1, 2) + I).rewrite(floor) == Rational(1, 2) + I - floor(Rational(1, 2) + I)\n        assert frac(Rational(1, 2) + I).rewrite(frac) == Rational(1, 2) + I - frac(Rational(1, 2) + I)\n\n\n\n", "    def test_frac_complex_mixed():\n        r = Symbol('r', real=True)\n        t = Symbol('t', real=True)\n        assert frac(t + I*r).rewrite(floor) == t - floor(t) + I * (r - floor(r))\n        assert frac(t + I*r).rewrite(ceiling) == t + ceiling(-t) + I * (r - ceiling(-r))\n\n\n\n\n", "    def test_frac_rewrite_as_floor(self):\n        assert frac(x).rewrite(floor) == x - floor(x)\n        assert frac(x + 1).rewrite(floor) == (x + 1) - floor(x + 1)\n        assert frac(x - 1).rewrite(floor) == (x - 1) - floor(x - 1)\n", "    def test_frac_complex(self):\n        assert frac(1 + 2*I) == 1 + 2*I - floor(1 + 2*I)\n        assert frac(1 + 2*I).rewrite(floor) == 1 + 2*I - floor(1 + 2*I)\n        assert frac(1 + 2*I).rewrite(ceiling) == 1 + 2*I - ceiling(1 + 2*I)\n        assert frac(1 + 2*I) == uncange\n        assert frac(I) == I - floor(I)\n        assert frac(I).rewrite(floor) == I - floor(I)\n        assert frac(I).rewrite(ceiling) == I - ceiling(I)\n        assert frac(-I) == -I - floor(-I)\n        assert frac(-I).rewrite(floor) == -I - floor(-I)\n        assert frac(-I).rewrite(ceiling) == -I - ceiling(-I)\n\n", "    def test_frac_complex(self):\n        assert frac(I).evalf() == 1j\n        assert frac(2 + 3*I).evalf() == 3*I + frac(2) \n        assert frac(pi*I).evalf() == pi*I\n"], "sample_902": ["    def test_pipeline_with_dummy_transformer(self):\n        dummy_transformer = DummyTransf()\n        pipeline = Pipeline([('trans', dummy_transformer)])\n        # Check that the transformer is called during fit\n        X = np.random.rand(10, 5)\n        pipeline.fit(X, np.zeros(10))\n        assert_true(dummy_transformer.timestamp_ > 0)\n\n        # Check if the transformer is fitted\n        with assert_no_warnings():\n            pipeline.predict(X)\n        assert_true(dummy_transformer.means_ is not None)\n        with assert_no_warnings():\n            pipeline.fit(X, np.ones(10))\n", "    def test_pipeline_with_memory(self):\n        X = np.array([[1, 2], [3, 4]])\n        y = np.array([0, 1])\n        memory_dir = mkdtemp()\n        memory = Memory(location=memory_dir, verbose=0)\n        dummy_estimator = DummyRegressor()\n        pipeline = Pipeline(steps=[('dummy', dummy_estimator)], memory=memory)\n\n        # Fit once and check memory usage (should be written)\n        pipeline.fit(X, y)\n        assert memory.cache_size > 0\n\n        # Fit again, it should be loaded from cache\n        start = time.time()\n        pipeline.fit(X, y)\n        end = time.time()\n        assert end - start < 0.01  # Check for faster fitting\n\n        # Cleanup temporary directory\n        shutil.rmtree(memory_dir)\n\n\n", "    def test_pipeline_with_no_fit_transformer(self):\n        transformer = NoFit()\n        pipeline = Pipeline([('transform', transformer)])\n        assert_raises(TypeError, pipeline.fit, np.random.rand(10, 5))\n        assert_raises(TypeError, pipeline.fit_transform, np.random.rand(10, 5))\n    ", "    def test_pipeline_memory(self):\n        X, y = load_iris(return_X_y=True)\n        scaler = StandardScaler()\n        knn = KNeighborsClassifier()\n        pipeline = Pipeline([('scaler', scaler), ('knn', knn)], memory=memory)\n        pipeline.fit(X, y)\n        Xt = pipeline.transform(X)\n        # Check that the pipeline was cached\n        with assert_no_warnings():\n            pipeline.fit(X, y)\n        Xt_2 = pipeline.transform(X)\n        assert_array_equal(Xt, Xt_2)\n\n        # Test with a different type of transformer\n        n_jobs = 2\n        transform = [\n            ('scaler', StandardScaler()),\n            ('pca', PCA(n_components=2)),\n        ]\n        pipeline = Pipeline(transform, memory=memory, n_jobs=n_jobs)\n        pipeline.fit(X, y)\n", "    def test_pipeline_fit_transform_with_memory(self):\n        # Test the pipeline caching\n        X, y = load_iris(return_X_y=True)\n        memory = Memory(cachedir=mkdtemp(), verbose=0)\n        pipe = Pipeline([('scaler', StandardScaler()),\n                         ('clf', DummyRegressor())], memory=memory)\n\n        pipe.fit(X, y)\n\n        # assert the cache is used\n        with memory.cache:\n            pipe.fit(X, y)\n\n        # now test a single transform step\n        with memory.cache:\n            Xt = pipe.transform(X)\n        with memory.cache:\n            Xt2 = pipe.transform(X)\n        assert_array_equal(Xt, Xt2)\n\n        # clean up temporary directory\n        shutil.rmtree(memory.cachedir)\n\n\n"], "sample_288": ["    def test_contains_on_empty_string(self):\n        with self.subTest('non-empty'):\n            json_model = JSONModel.objects.create(json_field='{}')\n            with self.assertRaises(KeyError):\n                json_model.json_field.get(key='key_name')\n\n        with self.subTest('empty'):\n            json_model = JSONModel.objects.create(json_field='')\n            with self.assertRaises(KeyError):\n                json_model.json_field.get(key='key_name')\n", "    def test_empty_strings_are_invalid(self):\n        model = JSONModel()\n        model.json_field = ''\n        with self.assertRaises(ValidationError):\n            model.full_clean()\n", "    def test_decimal_representation(self):\n        db_backend = connection.vendor\n\n        class JSONDecimalModel(models.Model):\n            json_data = models.JSONField(blank=True)\n\n        with self.subTest('Decimal fields with no decimal'):\n            instance = JSONDecimalModel()\n            instance.json_data = {\"a\": 10}\n            instance.save()\n            fetched_instance = JSONDecimalModel.objects.get(pk=instance.pk)\n            self.assertEqual(fetched_instance.json_data[\"a\"], 10)\n\n        with self.subTest('Decimal fields with decimal'):\n            instance = JSONDecimalModel()\n            instance.json_data = {\"b\": 10.50}\n            instance.save()\n            fetched_instance = JSONDecimalModel.objects.get(pk=instance.pk)\n            self.assertEqual(fetched_instance.json_data[\"b\"], 10.50)\n\n        if db_backend == 'sqlite':\n            with self.subTest('Check for potential decimal rounding errors on SQLite'):\n                instance = JSONDecimalModel()\n                instance.json_data = {\"c\": 3.14159265358979323846}\n                instance.save()\n                fetched_instance = JSONDecimalModel.objects.get(pk=instance.pk)\n                self.assertAlmostEqual(fetched_instance.json_data[\"c\"], 3.1416)\n", "    def test_has_key_lookup_with_nested_keys(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\n                \"INSERT INTO jsonmodel (id, data) VALUES (%s, %s)\",\n                (1, json.dumps({'user': {'name': 'Alice', 'age': 30}, 'posts': [1, 2, 3]}),\n            )\n            cursor.execute(\n                \"INSERT INTO jsonmodel (id, data) VALUES (%s, %s)\",\n                (2, json.dumps({'user': {'name': 'Bob', 'age': 25}})),\n            )\n\n        user_data = JSONModel.objects.get(id=1).data\n        posts_data = JSONModel.objects.get(id=2).data\n\n        self.assertTrue(JSONModel.objects.filter(data__has_key='user__name').exists())\n        self.assertTrue(JSONModel.objects.filter(data__has_key='user.name').exists())\n        self.assertEqual(\n            JSONModel.objects.filter(data__has_key='user__name').count(), 2\n        )\n        self.assertEqual(\n            JSONModel.objects.filter(data__has_key='user.name').count(), 2\n        )\n        self.assertTrue(JSONModel.objects.filter(data__has_key='posts').exists())\n        self.assertEqual(\n            JSONModel.objects.filter(data__has_key='posts').count(), 2\n        )\n\n\n", "    def test_key_transform_lookup_error(self):\n        with self.assertRaises(TypeError):\n            JSONModel.objects.filter(json_field__key_transform_lookup=123)\n"], "sample_279": ["    def test_unique_constraint_deconstruct(self):\n        constraint = UniqueConstraint(\n            fields=['name'],\n            name='unique_name',\n            deferrable=Deferrable.DEFERRED,\n            opclasses=('text_ops',),\n        )\n        path, args, kwargs = constraint.deconstruct()\n        self.assertEqual(path, 'django.db.models.constraints.UniqueConstraint')\n        self.assertEqual(args, ())\n        self.assertEqual(kwargs, {\n            'fields': ['name'],\n            'name': 'unique_name',\n            'deferrable': 'deferred',\n            'opclasses': ('text_ops',),\n        })\n\n        # Recreate constraint from deconstructed data\n        reconstructed_constraint = UniqueConstraint(*args, **kwargs)\n        self.assertEqual(reconstructed_constraint.fields, constraint.fields)\n        self.assertEqual(reconstructed_constraint.name, constraint.name)\n        self.assertEqual(reconstructed_constraint.deferrable, constraint.deferrable)\n        self.assertEqual(reconstructed_constraint.opclasses, constraint.opclasses)\n", "    def test_unique_constraint_deconstruct(self):\n        constraint = UniqueConstraint(\n            fields=['name', 'price'],\n            name='unique_name_price',\n            deferrable=Deferrable.DEFERRED,\n            opclasses=('text_ops',),\n        )\n        path, args, kwargs = constraint.deconstruct()\n        self.assertEqual(path, 'django.db.constraints.UniqueConstraint')\n        self.assertEqual(args, ())\n        self.assertEqual(kwargs, {\n            'fields': ['name', 'price'],\n            'name': 'unique_name_price',\n            'deferrable': 'deferred',\n            'opclasses': ('text_ops',),\n        })\n", "    def test_unique_constraint_with_opclasses(self):\n        with atomic():\n            UniqueConstraintProduct.objects.create(name='test', description='test')\n            with self.assertRaises(IntegrityError):\n                UniqueConstraintProduct.objects.create(name='test', description='test')\n\n        constraints = get_constraints('django_uniqueconstraintproduct')\n        self.assertEqual(len(constraints), 1)\n        constraint = constraints[0]\n        self.assertEqual(constraint['name'], 'unique_name_description')\n        self.assertEqual(constraint['columns'], ['name', 'description'])\n        self.assertEqual(constraint['type'], 'unique')\n\n\n", "    def test_unique_constraint_creation(self):\n        with atomic():\n            Product.objects.create(name='Test Product')\n            constraint = UniqueConstraint(fields=['name'], name='unique_name')\n            connection.schema_editor.create_unique_constraint(\n                model=Product, constraint=constraint\n            )\n            constraints = get_constraints('django_product')\n            self.assertEqual(\n                len(constraints), 1,\n                'Expected one unique constraint, got %d' % len(constraints)\n            )\n            constraint_definition = constraints[0]\n            self.assertEqual(constraint_definition['name'], 'unique_name')\n            self.assertEqual(\n                constraint_definition['columns'], ['name'],\n                'Expected unique constraint on \"name\" field, got %s' %\n                constraint_definition['columns']\n            )\n\n            with self.assertRaises(IntegrityError):\n                Product.objects.create(name='Test Product')\n", "    def test_unique_constraint_deconstruction(self):\n        fields = ['name', 'price']\n        condition = Q(name='test')\n        name = 'unique_constraint'\n        deferrable = Deferrable.DEFERRED\n        include = ('category', )\n        opclasses = ('text', 'numeric')\n        constraint = UniqueConstraint(\n            fields=fields, name=name, condition=condition, deferrable=deferrable, include=include, opclasses=opclasses\n        )\n        path, args, kwargs = constraint.deconstruct()\n        self.assertEqual(path, 'django.db.models.constraints.UniqueConstraint')\n        self.assertEqual(args, ())\n        self.assertEqual(kwargs, {\n            'fields': fields,\n            'name': name,\n            'condition': condition,\n            'deferrable': deferrable,\n            'include': include,\n            'opclasses': opclasses,\n        })\n"], "sample_1202": ["    def test_tribonacci_constant_evaluation():\n        from sympy.core.constants import TribonacciConstant\n        tc = TribonacciConstant\n        assert tc.n(100) == 1.8392867552141611326\n\n\n        # Test with different precision levels\n        for prec in [5, 10, 20]:\n            assert same_and_same_prec(tc.evalf(prec),\n                                        1.8392867552141611326)\n\n", "    def test_sympify_fractions_with_float_denominator(self):\n        self.assertEqual(sympify(fractions.Fraction(1, 3.14)),\n                         Rational(1, 3.14))\n", "    def test_sympify_complex():\n        assert sympify_complex(1 + 2j) == 1 + 2*I\n        assert sympify_complex(3.14 + 1.618j) == 3.14 + 1.618*I\n        assert sympify_complex(complex(1, 2)) == 1 + 2*I\n    ", "    def test_abs():\n        assert abs(S.PositiveOne) == S.PositiveOne\n        assert abs(S.NegativeOne) == S.PositiveOne\n        assert abs(S.Zero) == S.Zero\n        assert abs(S.One) == S.One\n        assert abs(S.Pi) == S.Pi\n        assert abs(S.sqrt(2)) == S.sqrt(2)\n        assert abs(S.ImaginaryUnit) == S.One\n        assert abs(S.ComplexInfinity) == S.Infinity\n        assert abs(S.NegativeInfinity) == S.Infinity\n        assert abs(S.NaN) == S.NaN\n", "    def test_is_numeric_types():\n        assert isinstance(S.One, numbers.Number)\n        assert isinstance(S.Zero, numbers.Number)\n        assert isinstance(S.NegativeOne, numbers.Number)\n        assert isinstance(S.Infinity, numbers.Number)\n        assert isinstance(S.NegativeInfinity, numbers.Number)\n        assert isinstance(S.ComplexInfinity, numbers.Number)\n        assert isinstance(S.NaN, numbers.Number)\n        assert isinstance(S.Pi, numbers.Number)\n        assert isinstance(S.E, numbers.Number)\n        assert isinstance(S.ImaginaryUnit, numbers.Number)\n\n\n"], "sample_1031": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1063": ["compilation error", "    def test_nested_lists():\n        expr = [1, [2, 3], [x, [y, z]]]\n        func = lambdify(x, expr)\n        assert func(1) == [1, [2, 3], [1, [2, 3]]]\n", "    def test_lambdify_with_special_functions():\n        from sympy.abc import x, y\n        from sympy.functions import besselj, bessely, besseli, besselk\n        sinc = sympy.functions.special.sinc\n        \n        expr = besselj(x, 1)\n        func = lambdify(x, expr, \"numpy\")\n        assert func(2) == pytest.approx(numpy.besselj(2, 1), 1e-6)\n        \n        expr = bessely(x, 1)\n        func = lambdify(x, expr, \"numpy\")\n        assert func(2) == pytest.approx(numpy.bessely(2, 1), 1e-6)\n        \n        expr = besseli(x, 1)\n        func = lambdify(x, expr, \"numpy\")\n        assert func(2) == pytest.approx(numpy.besseli(2, 1), 1e-6)\n        \n        expr = besselk(x, 1)\n        func = lambdify(x, expr, \"numpy\")\n        assert func(2) == pytest.approx(numpy.besselk(2, 1), 1e-6)\n        \n        expr = sinc(x)\n        func = lambdify(x, expr, \"numpy\")\n        assert func(1) == pytest.approx(numpy.sinc(1), 1e-6)\n        \n\n        \n        expr = besselj(x, 1)\n        func = lambdify(x, expr, \"numexpr\")\n        assert func(2) == pytest.approx(numexpr.besselj(2, 1), 1e-6)\n        \n        expr = bessely(x, 1)\n        func = lambdify(x, expr, \"numexpr\")\n        assert func(2) == pytest.approx(numexpr.bessely(2, 1), 1e-6)\n        \n        expr = besseli(x, 1)\n        func = lambdify(x, expr, \"numexpr\")\n        assert func(2) == pytest.approx(numexpr.bes", "    def test_lambdify_with_tensor_creation():\n        from sympy.matrices import Matrix\n\n        if tensorflow:\n            f = lambdify((x, y), Matrix([[x, y], [y, x]]), 'tensorflow')\n            result = f(1, 2)\n            assert isinstance(result, tensorflow.Tensor)\n        else:\n            raises(ImportError, lambda: lambdify((x, y),\n                                                 Matrix([[x, y], [y, x]]),\n                                                 'tensorflow'))\n\n", "    def test_lambdastr_unpack_nested_tuple():\n        from sympy.matrices import DeferredVector\n        expr = (\n            (x, y, z),\n            y + z\n        )\n        code = lambdastr(expr, y + z, dummify=True)\n        assert code == 'lambda _0, _1: ((_0, _1[0]), _1[1])'\n\n\n"], "sample_126": ["    def test_detect_changes_m2m_through(self):\n        before_states = [\n            author_with_m2m_through,\n        ]\n        after_states = [\n            author_with_m2m_through,\n            contract,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertMigrationDependencies(changes, \"testapp\", 0, [])\n        self.assertOperationTypes(changes, \"testapp\", 0, ['CreateModel'])\n        self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, name=\"id\", model_name=\"Contract\", primary_key=True)\n        self.assertOperationFieldAttributes(changes, \"testapp\", 0, 1, name=\"author\", model_name=\"Contract\", field=models.ForeignKey, to=\"testapp.Author\", on_delete=models.CASCADE)\n        self.assertOperationFieldAttributes(changes, \"testapp\", 0, 2, name=\"publisher\", model_name=\"Contract\", field=models.ForeignKey, to=\"testapp.Publisher\", on_delete=models.CASCADE)\n\n\n\n", "    def test_operation_field_rename(self):\n        before_states = [\n            author_name_deconstructible_list_1,\n            publisher,\n            book,\n        ]\n        after_states = [\n            author_name_deconstructible_list_2,\n            publisher,\n            book.replace(\n                \"id\",\n                models.AutoField(primary_key=True, db_column=\"book_id\"),\n            ),\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertEqual(len(changes), 1)\n        self.assertEqual(\n            changes[\"testapp\"].migration_name,\n            \"0002_rename_author_id\",\n        )\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameField\"])\n        self.assertOperationFieldAttributes(\n            changes,\n            \"testapp\",\n            0,\n            0,\n            model_name=\"Author\",\n            field_name=\"id\",\n            new_field_name=\"book_id\",\n        )\n\n\n", "    def test_migration_detection_with_indexes_on_fields(self):\n        before_states = [\n            author_with_multiple_authors,\n            book_foo_together,\n            book_foo_together_2,\n            book_foo_together_3,\n            book_foo_together_4,\n        ]\n        after_states = [\n            author_with_multiple_authors,\n            book_foo_together_2,\n            book_foo_together_3,\n            book_foo_together_4,\n            book_foo_together_4,\n        ]\n\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, 'otherapp', 1)\n        self.assertMigrationDependencies(changes, 'otherapp', 0, [])\n        self.assertOperationTypes(changes, 'otherapp', 0, ['RenameField', 'RemoveField'])\n\n        self.assertOperationAttributes(changes, 'otherapp', 0, 0, field='title', to_field='newfield')\n        self.assertOperationAttributes(changes, 'otherapp', 0, 1, field='title', to_field='newfield')\n\n", "    def test_migration_dependencies(self):\n        before_states = [\n            author_name_deconstructible_list_1,\n            publisher_with_author,\n            book,\n            attribution,\n            edition,\n        ]\n        after_states = [\n            author_name_deconstructible_list_2,\n            publisher_with_author,\n            book,\n            attribution,\n            edition,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertMigrationDependencies(changes, \"testapp\", 0, [\"otherapp.Book\"])\n\n\n\n", "    def test_operation_creation_from_field_changes(self):\n        \"\"\"\n        Test that field changes are reflected in the correct operation type\n        and attributes.\n        \"\"\"\n        before_states = [\n            author_name_deconstructible_list_1,\n            publisher,\n            other_pony,\n            book,\n        ]\n        after_states = [\n            author_name_deconstructible_list_2,\n            publisher,\n            other_pony,\n            book,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertMigrationDependencies(changes, \"testapp\", 0, [])\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RemoveField\", \"AddField\"])\n        self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0,  \n            model_name=\"Author\", field_name=\"name\", primary_key=False, \n            blank=True, null=True, default=None, verbose_name=\"Name\", \n            help_text=None\n        )\n        self.assertOperationFieldAttributes(changes, \"testapp\", 0, 1,  \n            model_name=\"Author\", field_name=\"name\", primary_key=False, \n            blank=False, null=True, default=None, verbose_name=\"Name\", \n            help_text=None\n        )\n\n\n"], "sample_861": ["    def test_grid_search_with_sparse_data(self, klass):\n        from sklearn.datasets import make_sparse_coded\n        X, y = make_sparse_coded(n_samples=100, n_features=100,\n                                 n_informative=50, random_state=42)\n        clf = LogisticRegression()\n        param_grid = {'C': [0.1, 1, 10]}\n        gscv = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5)\n        gscv.fit(X, y)\n        assert_array_equal(gscv.best_params_, {'C': 1})\n\n", "    def test_param_sampler_non_iterable_in_list(klass, input, error_type,\n                                                error_message,\n                                                ):\n        with pytest.raises(error_type, match=error_message):\n            klass(input, n_iter=10)\n\n\n\n", "    def test_param_grid_empty_param_distributions(klass):\n        param_distributions = {}\n        with pytest.raises(ValueError):\n            klass(param_distributions, n_iter=10)\n    ", "    def test_parameter_grid_raises_on_invalid_input(self, klass,\n                                                    input, error_type,\n                                                    error_message):\n        with pytest.raises(error_type, match=re.escape(error_message)):\n            klass({'foo': input})\n\n", "    def test_invalid_param_distributions(self, klass, input, error_type,\n                                        error_message):\n        with pytest.raises(error_type, match=error_message):\n            klass(input, 10)\n"], "sample_433": ["compilation error", "compilation error", "    def test_models_with_m2m_through_ordering(self):\n        with self.assertNumQueries(2):\n            # Create a publisher and two authors\n            publisher = Publisher.objects.create(name=\"Publisher X\")\n            author1 = Author.objects.create(name=\"Author 1\")\n            author2 = Author.objects.create(name=\"Author 2\")\n\n            # Create two Book instances and link them to the publisher and authors\n            contract1 = Contract.objects.create(\n                author=author1, publisher=publisher\n            )\n            contract2 = Contract.objects.create(\n                author=author2, publisher=publisher\n            )\n\n            # Check the order of authors in the Publisher's authorbooks\n            self.assertEqual(\n                list(publisher.author.all()),\n                [author1, author2],\n            )\n", "    def test_unique_together_errors(self):\n        with self.assertRaises(IntegrityError):\n            Book.objects.create(\n                author=self.author_1,\n                title=\"The Very First Word\",\n            )\n        with self.assertRaises(IntegrityError):\n            Book.objects.create(\n                author=self.author_1,\n                title=\"The Very First Word\",\n            )\n\n\n   \n", "    def test_unique_together_field_order(\n        self,"], "sample_503": ["    def test_set_linestyle(self, linestyle):\n        line = mlines.Line2D([], [], linestyle=linestyle)\n\n        # Ensure the linestyle is set correctly\n        assert line.get_linestyle() == linestyle\n\n", "    def test_dashed_lines(self):\n        fig, ax = plt.subplots()\n        line = mplines.Line2D([0, 1, 2, 3], [0, 1, 2, 3],\n                             dashed=(5, 7), linewidth=2)\n        ax.add_line(line)\n        ax.set_xlim(-0.5, 3.5)\n        ax.set_ylim(-0.5, 3.5)\n        plt.show()\n", "    def test_linestyle_string(self, linestyle, expected):\n        line = mlines.Line2D([], [], linestyle=linestyle)\n        assert line.get_linestyle() == expected\n", "    def test_dashed_line_join_style(self):\n        fig, ax = plt.subplots()\n        line = mlines.Line2D([0, 1, 2], [0, 0, 0], linestyle='--',\n                             dash_joinstyle='round')\n        ax.add_line(line)\n        plt.show()\n        line = mlines.Line2D([0, 1, 2], [0, 0, 0], linestyle='--',\n                             dash_joinstyle='bevel')\n        ax.add_line(line)\n        plt.show()\n        line = mlines.Line2D([0, 1, 2], [0, 0, 0], linestyle='--',\n                             dash_joinstyle='miter')\n        ax.add_line(line)\n        plt.show()", "    def test_path_generation(self, ls, exp_path):\n        line = mlines.Line2D([0, 1], [0, 1], linestyle=ls)\n        path = line._get_transformed_path()\n        assert_array_equal(path.vertices[:, 0], exp_path.vertices[:, 0])\n        assert_array_equal(path.vertices[:, 1], exp_path.vertices[:, 1])"], "sample_342": ["    def test_autocomplete_with_permission_denied(self):\n        request = self.factory.get(self.url)\n        request.GET['app_label'] = Answer._meta.app_label\n        request.GET['model_name'] = Answer._meta.model_name\n        request.GET['field_name'] = 'question'\n\n        with self.assertRaises(PermissionDenied):\n            AutocompleteJsonView.as_view()(request, **self.as_view_args)\n", "    def test_autocomplete_no_permission(self):\n        # Test that a user without permission raises PermissionDenied.\n        request = self.factory.get(self.url)\n        request.GET = {\n            'app_label': self.opts['app_label'],\n            'model_name': self.opts['model_name'],\n            'field_name': self.opts['field_name'],\n            'term': 'test'\n        }\n\n        with self.assertRaises(PermissionDenied):\n            AutocompleteJsonView.as_view()(request, **self.as_view_args)\n", "    def test_permission_denied(self):\n        request = self.factory.get(self.url, params=self.opts)\n        request.user = User.objects.create_user(username='not_staff', password='secret')\n        response = AutocompleteJsonView.as_view()(request, **self.as_view_args)\n        self.assertEqual(response.status_code, 403)\n        self.assertEqual(response._headers.get('content-type'), ('Content-Type', 'application/json'))\n        content = json.loads(response.content.decode('utf-8'))\n        self.assertEqual(content.get('error'), 'PermissionDenied')\n\n", "    def test_missing_search_fields(self):\n        with model_admin(Question, QuestionAdmin):\n            QuestionAdmin.search_fields = []\n            request = self.factory.get(\n                self.url, \n                {'app_label': self.opts['app_label'], \n                 'model_name': self.opts['model_name'], \n                 'field_name': self.opts['field_name'], \n                 'term': ''}\n            )\n            response = AutocompleteJsonView.as_view()(request)\n            self.assertEqual(response.status_code, 404)\n\n\n\n", "    def test_missing_search_fields(self):\n        with model_admin(\n            Author,\n            Admin.ModelAdmin()\n            ) as admin_context:\n            url = reverse(\n                'autocomplete_admin:autocomplete',\n                kwargs={'app_label': Author._meta.app_label,\n                        'model_name': Author._meta.model_name,\n                        'field_name': 'id'}\n            )\n            request = self.factory.get(url, {'term': 'test'})\n\n            with self.assertRaises(Http404):\n                AutocompleteJsonView.as_view()(request, **self.opts)\n"], "sample_248": ["    def test_command_argument(self):\n        with captured_stdin(), captured_stdout() as (stdin, stdout):\n            call_command('shell', '-c', self.script_globals)\n            output = stdout.getvalue()\n            self.assertIn('True', output)\n", "    def test_command_argument(self):\n        with captured_stdout() as stdout:\n            call_command('shell', '-c', self.script_globals)\n        self.assertEqual(stdout.getvalue(), 'True\\n')\n", "    def test_run_command_exits(self):\n        with captured_stdout() as out:\n            call_command('shell', command='exit()')\n        self.assertEqual(out.getvalue(), '')\n", "    def test_command_option(self):\n        with captured_stdout() as stdout:\n            call_command('shell', command='print(\"Hello from command\")')\n        self.assertIn(\"Hello from command\", stdout.getvalue())\n", "    def test_command_argument(self):\n        with captured_stdout() as out:\n            call_command('runserver', '--noinput', '--command', 'print(\"Hello\")')\n        self.assertEqual(out.getvalue(), 'Hello\\n')\n\n\n\n"], "sample_280": ["    def test_count_star_with_filter(self):\n        with self.assertNumQueries(2):\n            count = Book.objects.filter(rating__gt=4).count('*')\n            self.assertEqual(count, 3)\n", "compilation error", "    def test_aggregate_filter_mysql(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\"SET sql_mode = 'TRADITIONAL'\")\n        with self.assertNumQueries(1):\n            result = Author.objects.filter(\n                name='Adrian Holovaty'\n            ).aggregate(num_books=Count('book__isbn'))\n        self.assertEqual(result['num_books'], 2)\n", "compilation error", "    def test_aggregate_with_filter(self):\n        with self.assertNumQueries(1):\n            result = Book.objects.filter(\n                Q(rating__gt=3) & Q(publisher=cls.p1)\n            ).annotate(\n                avg_rating=Avg('rating')\n            ).values('avg_rating')\n        self.assertEqual(result.first()['avg_rating'], 4.25)\n\n        with self.assertNumQueries(2):\n            result = Book.objects.filter(\n                Q(rating__gt=3) & Q(publisher=cls.p1)\n            ).annotate(\n                avg_rating=Avg('rating', filter=F('rating') > 4)\n            ).values('avg_rating')\n        self.assertEqual(result.first()['avg_rating'], 4.5)\n\n"], "sample_403": ["    def test_rename_index(self):\n        with atomic():\n            self.models['unicode_model'].objects.create(\n                name=\"test_name\",\n                text=\"test_text\",\n            )\n            \n            # Create a new migration\n            with self.settings(INSTALLED_APPS={'polls': 'polls'}):\n                self.operations.apply(\n                    [\n                        RenameIndex(\n                            'polls.UnicodeModel',\n                            new_name='unicode_test_index',\n                            old_name='unicode_model_name_text_index',\n                            old_fields=['name', 'text'],\n                        )\n                    ],\n                    ProjectState(\n                        migrations.apps.get_app_configs(),\n                        'default'\n                    )\n                )\n\n            # Verify the index was renamed\n            with CaptureQueriesContext(connection) as cm:\n                connection.execute(\n                    f\"SELECT indexname FROM pg_indexes WHERE tablename = 'polls_unicodemodel' AND indexname = 'unicode_test_index'\"\n                )\n                self.assertEqual(len(cm.captured_queries), 1)\n                self.assertIn('unicode_test_index', connection.execute(\n                    f\"SELECT indexname FROM pg_indexes WHERE tablename = 'polls_unicodemodel' AND indexname = 'unicode_test_index'\"\n                )[0][0])\n\n", "    def test_rename_index_unnamed(self):\n        with atomic():\n            self.create_model(\n                \"RenameIndexApp\",\n                \"TestModel\",\n                fields=[\n                    models.CharField(max_length=50, name=\"name\"),\n                    models.IntegerField(name=\"value\"),\n                ],\n            )\n\n        with connection.cursor() as cursor:\n            cursor.execute(\n                \"CREATE INDEX idx_testmodel_value ON RenameIndexApp_testmodel (value)\"\n            )\n\n        # Apply migration to rename index\n        with self.assertNumQueries(1):\n            with transaction.atomic():\n                state = ProjectState()\n                RenameIndex(\n                    \"RenameIndexApp\", \"TestModel\", old_fields=[\"value\"], new_name=\"new_idx\"\n                ).state_forwards(\"RenameIndexApp\", state)\n                state.forwards()\n                self.assertEqual(\n                    state.models[\"RenameIndexApp\", \"TestModel\"]._meta.indexes,\n                    [\n                        models.Index(\n                            fields=[\"value\"], name=\"new_idx\", unique=False\n                        ),\n                    ],\n                )\n            with connection.cursor() as cursor:\n                cursor.execute(\n                    \"PRAGMA index_list\"\n                )\n                result = cursor.fetchall()\n                self.assertIn(\n                    (\"new_idx\", \"RenameIndexApp_testmodel\", \"value\"),\n                    [(i[0], i[1], i[2]) for i in result],\n                )\n\n\n", "    def test_add_index_with_expressions(self):\n        with transaction.atomic():\n            self.add_database_field(\"unicode\", UnicodeModel)\n            self.migrate_database()\n\n        # Add an index with expressions\n        from django.db.migrations import Index\n        with self.mock.patch(\"django.db.migrations.state.ProjectState.apps\", auto_spec=True):\n            state = self.create_test_state(add_model=UnicodeModel)\n            IndexOperation(\n                \"UnicodeModel\",\n                Index(fields=[\"unicode_field\"], name=\"unicode_index_test\", expressions=[Abs(\"unicode_field\")])\n            ).database_forwards(\n                state,\n                None,\n                None,\n                state,\n            )\n            with CaptureQueriesContext(connection) as qs_context:\n                query_count = len(qs_context.captured_queries)\n                self.assertEqual(query_count, 1)\n                self.assertIn(\"CREATE INDEX\", qs_context.captured_queries[0])\n                self.assertIn(\"ABS(unicode_field)\", qs_context.captured_queries[0])\n", "    def test_rename_model_with_unique_together(self):\n        with transaction.atomic():\n            old_name = \"RenameableModel\"\n            new_name = \"RenamedModel\"\n            try:\n                models.Model(\n                    name=\"RenameableModel\",\n                    age=18,\n                ).save()\n            except IntegrityError:\n                pass\n\n            # Create the initial state\n            self.initial_state = self.state_for_app(\"myapp\")\n            self.initial_state.apps.get_model(\"myapp\", old_name)  # Ensure the app is loaded\n\n            # Apply the rename\n            with self.capture_queries() as c:\n                self.initial_state.migrate(\n                    [\n                        RenameModel(\n                            old_name=old_name,\n                            new_name=new_name,\n                            unique_together_fields=[\n                                (\"name\", \"age\"),\n                            ],\n                        ),\n                    ]\n                )\n\n            # Assert the state change\n            from_model = self.initial_state.models[\"myapp\", old_name]\n            to_model = self.initial_state.models[\"myapp\", new_name]\n            # Check that the model names have changed\n            assert to_model.name == new_name\n            assert from_model.name == old_name\n            # Check that unique_together remains the same\n            assert to_model._meta.unique_together == from_model._meta.unique_together\n\n            # Check that the db operations were not too long\n            self.assert_queries_less_than(c.captured_queries, 5, \"should have only a few queries\")\n\n            # Do the same in reverse\n            with self.capture_queries() as c:\n                self.initial_state.migrate(\n                    [\n                        RenameModel(\n                            old_name=new_name,\n                            new_name=old_name,\n                            unique_together_fields=[\n                                (\"name\", \"age\"),\n                            ],\n                        ),\n                    ]\n                )\n\n            # Assert the state change", "    def test_rename_field_ordering_and_unique_together(self):\n        with self.assertNumQueries(2):\n            # Forward\n            with atomic():\n                self.apps.create_model(\n                    \"myapp\",\n                    \"SomeModel\",\n                    fields=[\n                        models.CharField(\"name\", max_length=100),\n                        models.CharField(\"value\", max_length=100),\n                    ],\n                    unique_together=(\"name\", \"value\"),\n                )\n                self.migrate_apps()\n\n            from myapp.models import SomeModel\n\n            self.assertEqual(\n                SomeModel._meta.get_field(\"name\").ordering,\n                SomeModel._meta.get_field(\"value\").ordering,\n            )\n            self.assertEqual(\n                SomeModel._meta.unique_together,\n                [(\"name\", \"value\")],\n            )\n\n            with atomic():\n                with self.schema_editor.connection.cursor() as cursor:\n                    cursor.execute(\n                        \"ALTER TABLE myapp_somemodel MODIFY COLUMN name TEXT\"\n                    )\n                self.migrate_apps()\n\n            self.assertEqual(\n                SomeModel._meta.get_field(\"name\").ordering,\n                SomeModel._meta.get_field(\"value\").ordering,\n            )\n            self.assertEqual(\n                SomeModel._meta.unique_together,\n                [(\"name\", \"value\")],\n            )\n\n            # Backward\n            with atomic():\n                self.migrate_apps(fake=True)\n            with atomic():\n                with self.schema_editor.connection.cursor() as cursor:\n                    cursor.execute(\n                        \"ALTER TABLE myapp_somemodel MODIFY COLUMN name CHAR(100)\"\n                    )\n                self.migrate_apps()\n\n            self.assertEqual(\n                SomeModel._meta.get_field(\"name\").ordering,\n                SomeModel._meta.get_field(\"value\").ordering,\n            )\n            self.assertEqual(\n                SomeModel._meta.unique_together,\n                [(\"name\", \"value\")],\n            )\n\n\n\n"], "sample_821": ["    def test_affinity_propagation_equal_similarities_and_preferences():\n        # Test the case where all similarities and preferences are equal.\n        similarities = np.array([[1.0, 1.0], [1.0, 1.0]])\n        preference = 1.0\n        assert _equal_similarities_and_preferences(similarities, preference)\n        preference = similarities.flat[0]\n        assert _equal_similarities_and_preferences(similarities, preference)\n\n        # If preference is lower than similarity, a single cluster center\n        # should be returned.\n        similarities = np.array([[1.0, 1.0], [1.0, 1.0]])\n        preference = 0.0\n        result = affinity_propagation(similarities, preference,\n                                     return_n_iter=False)\n        assert result[0].size == 1 \n\n        # If preference is greater than similarity, each sample becomes\n        # its own cluster center.\n        preference = 2.0\n        result = affinity_propagation(similarities, preference,\n                                     return_n_iter=False)\n        assert result[0].size == similarities.shape[0]\n\n\n\n\n", "    def test_equal_similarities_and_preferences(self):\n        X = np.array([[1, 1], [2, 2], [3, 3]])\n        preference = np.array([1, 1, 1])\n\n        assert _equal_similarities_and_preferences(np.ones((3, 3)), preference)\n\n        preference = np.array([0, 0, 1])\n        X = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n        assert _equal_similarities_and_preferences(X, preference)\n\n        preference = np.array([1, 1, 0])\n        X = np.array([[1, 1, 0], [1, 1, 0], [0, 0, 0]])\n        assert _equal_similarities_and_preferences(X, preference)    \n", "    def test_preference_zero(self):\n        ap = AffinityPropagation(preference=0, max_iter=5)\n        ap.fit(X)\n        assert_equal(ap.cluster_centers_indices_.shape[0], X.shape[0])\n", "    def test_all_equal_similarities_and_preferences():\n        # Test when all similarities and preferences are equal\n        S1 = np.ones((5, 5))\n        pref1 = np.ones((5,))\n        assert _equal_similarities_and_preferences(S1, pref1)\n\n        S2 = np.tile(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]]), (2, 1))\n        pref2 = np.array([1, 1, 1])\n        assert _equal_similarities_and_preferences(S2, pref2)\n", "    def test_empty_affinity_matrix(self):\n        affinity_propagation(np.array([]), preference=0.5)\n\n\n"], "sample_290": ["    def test_detect_changes_index_together_order(self):\n        before_states = [\n            author_with_foo_together,\n            book_foo_together,\n        ]\n        after_states = [\n            author_with_foo_together_2,\n            book_foo_together_2,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertMigrationDependencies(changes, 'testapp', 0, [])\n        self.assertOperationTypes(changes, 'testapp', 0, ['RemoveField', 'AddField'])\n\n\n        before_states = [\n            author_with_foo_together_3,\n            book_foo_together_3,\n        ]\n        after_states = [\n            author_with_foo_together_4,\n            book_foo_together_4,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertMigrationDependencies(changes, 'testapp', 0, [])\n        self.assertOperationTypes(changes, 'testapp', 0, ['RemoveField', 'AddField'])\n", "    def test_operation_field_attributes(self):\n        before_states = [\n            author_with_biography_non_blank,\n            publisher,\n            book,\n        ]\n        after_states = [\n            author_with_biography_blank,\n            publisher,\n            book,\n        ]\n\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertMigrationDependencies(changes, \"testapp\", 0, [])\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RemoveField\"])\n        self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, field='biography', db_column='bio', null=True)\n", "    def test_complex_index_together_and_unique_together(self):\n        before_states = [\n            author_with_multiple_authors_through_attribution,\n            book_foo_together_4,\n            book_with_multiple_authors,\n        ]\n        after_states = [\n            author_with_multiple_authors_through_attribution,\n            book_foo_together_4,\n            book_with_multiple_authors,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, 'otherapp', 1)\n        self.assertMigrationDependencies(changes, 'otherapp', 0, [])\n        self.assertOperationTypes(changes, 'otherapp', 0, [\n            'RemoveField',\n            'RemoveField',\n            'RenameField',\n            'AddField',\n            'AlterModelMultipleFields',\n        ])\n        self.assertOperationAttributes(changes, 'otherapp', 0, 0, field='attribution', value=None)\n        self.assertOperationAttributes(changes, 'otherapp', 0, 1, model_name='Book', value='book_foo_together_4')\n        self.assertOperationAttributes(changes, 'otherapp', 0, 2, model_name='Book', value='book_foo_together_4')\n        self.assertOperationAttributes(changes, 'otherapp', 0, 3, model_name='Book', value='book_foo_together_4')\n\n\n\n", "    def test_operation_field_attributes(self):\n        before_states = [\n            author_name_deconstructible_dict_1,\n            author_name_deconstructible_list_1,\n            publisher,\n        ]\n        after_states = [\n            author_name_deconstructible_dict_2,\n            author_name_deconstructible_list_2,\n            publisher_with_author,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertOperationFieldAttributes(\n            changes, 'testapp', 0, 0, db_table='author_one',\n        )\n", "    def test_nested_object_deconstructible(self):\n        before_states = [\n            author_name_deconstructible_list_1,\n            author_name_deconstructible_list_2,\n            author_name_deconstructible_list_3,\n            author_name_deconstructible_tuple_1,\n            author_name_deconstructible_tuple_2,\n            author_name_deconstructible_tuple_3,\n            author_name_deconstructible_dict_1,\n            author_name_deconstructible_dict_2,\n            author_name_deconstructible_dict_3,\n            author_nested_deconstructible_1,\n        ]\n        after_states = [\n            author_name_deconstructible_list_1,\n            author_name_deconstructible_list_2,\n            author_name_deconstructible_list_3,\n            author_name_deconstructible_tuple_1,\n            author_name_deconstructible_tuple_2,\n            author_name_deconstructible_tuple_3,\n            author_name_deconstructible_dict_1,\n            author_name_deconstructible_dict_2,\n            author_name_deconstructible_dict_3,\n            author_nested_deconstructible_2,\n        ]\n\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertMigrationDependencies(changes, 'testapp', 0, [])\n\n\n"], "sample_638": ["    def test_non_supported_format_with_graphviz(mock_graphviz):\n        with mock.patch(\n            \"pylint.pyreverse.__main__\",\n            new_callable=mock.MagicMock,\n        ) as mock_main:\n            main.Run(\n                [\n                    \"--output_format=XYZ\",\n                    \"test_data_dir/example\",\n                ]\n            )\n        mock_main.assert_called_once()\n", "    def test_unsupported_format_falls_back_to_dot(mock_graphviz):\n        main.Run(sys.argv[1:])\n        mock_graphviz.return_value.stderr.assert_contains(\n            'Format: \"XYZ\" not recognized. Use one of:'\n        )\n\n\n\n", "    def test_unsupported_format_with_graphviz(mock_graphviz):\n        \"\"\"Test unsupported format with graphviz.\"\"\"\n        config = main.ConfigurationMixIn()\n        config.output_format = \"XYZ\"\n        with mock.patch(\"pylint.pyreverse.main.Run\") as mock_run:\n            main.Run(sys.argv[1:])\n        mock_run.assert_called_once()\n", "    def test_run_with_output_format(\n        mock_subprocess, mock_graphviz, args, expected_output", "    def test_unsupported_format_handled(mock_graphviz, setup_path):\n        main.Run(sys.argv[1:])\n        mock_graphviz.run.assert_called_once()\n        assert \"Format: XYZ not recognized\" in mock_graphviz.run.call_args[0][0]\n"], "sample_225": ["    def test_each_context_available_apps(self):\n        self.assertIn('available_apps', self.ctx)\n        expected_structure = [\n            {'name': 'admin', 'app_label': 'admin', 'app_url': '/test_admin/admin/app_list/admin/',\n             'has_module_perms': True, 'models': [\n                 {'model': User, 'name': 'users', 'object_name': 'User', 'perms': {'add': True, 'change': True,\n                                                                                       'delete': True, 'view': True},\n                  'admin_url': '/test_admin/admin/user/changelist/', 'add_url': '/test_admin/admin/user/add/'},\n                 {'model': Article, 'name': 'articles', 'object_name': 'Article', 'perms': {'add': True, 'change': True,\n                                                                                        'delete': True, 'view': True},\n                  'admin_url': '/test_admin/admin/article/changelist/', 'add_url': '/test_admin/admin/article/add/'}\n             ]\n             },\n            # {'name': 'auth', 'app_label': 'auth', 'app_url': '/test_admin/admin/app_list/auth/',\n            #  'has_module_perms': True, 'models': [...]}\n            # ... more app structures if you have other apps registered\n        ]\n        self.assertEqual(self.ctx['available_apps'], expected_structure)\n", "    def test_available_apps_structure(self):\n        self.assertIn('available_apps', self.ctx)\n        available_apps = self.ctx['available_apps']\n        self.assertEqual(len(available_apps), 2)\n        self.assertIn('auth', available_apps[0])\n        self.assertIn('testapp', available_apps[1]) \n\n\n", "    def test_available_apps_context(self):\n        self.assertGreater(len(self.ctx['available_apps']), 0)\n        self.assertIn('django.contrib.auth', [a['app_label'] for a in self.ctx['available_apps']])\n        self.assertIn('test_adminsite', [a['app_label'] for a in self.ctx['available_apps']])\n\n\n", "    def test_available_apps_structure(self):\n        self.assertIn('available_apps', self.ctx)\n        available_apps = self.ctx['available_apps']\n        self.assertEqual(len(available_apps), 2)\n        self.assertIn('auth', available_apps)\n        self.assertIn('test_app', available_apps)\n\n        auth_app = available_apps['auth']\n        self.assertEqual(auth_app['name'], 'Django Authentication')\n        self.assertEqual(auth_app['app_label'], 'auth')\n        self.assertEqual(auth_app['app_url'], reverse('test_adminsite:app_list', kwargs={'app_label': 'auth'}))\n\n        test_app_app = available_apps['test_app']\n        self.assertEqual(test_app_app['name'], 'Test App') \n        self.assertEqual(test_app_app['app_label'], 'test_app')\n        self.assertEqual(test_app_app['app_url'], reverse('test_adminsite:app_list', kwargs={'app_label': 'test_app'}))\n", "    def test_each_context_variables(self):\n        \"\"\"\n        Check each_context contains the expected variables.\n        \"\"\"\n        expected_vars = {\n            'site_title': site.site_title,\n            'site_header': site.site_header,\n            'site_url': site.site_url,\n            'has_permission': True,\n            'available_apps': [\n                {\n                    'name': 'auth',\n                    'app_label': 'auth',\n                    'app_url': 'http://testserver/test_admin/admin/auth/',\n                    'has_module_perms': True,\n                    'models': [\n                        {\n                            'name': 'user',\n                            'object_name': 'User',\n                            'perms': {'add': True, 'change': True, 'delete': True, 'view': True},\n                            'admin_url': 'http://testserver/test_admin/admin/auth/user/',\n                            'add_url': 'http://testserver/test_admin/admin/auth/user/add/',\n                        }\n                    ],\n                },\n                {\n                    'name': 'mytest',  # Assuming 'mytest' is the app where Article is defined\n                    'app_label': 'mytest',  # Assuming 'mytest' is the app where Article is defined\n                    'app_url': 'http://testserver/test_admin/admin/mytest/',\n                    'has_module_perms': True,\n                    'models': [\n                        {\n                            'name': 'article',\n                            'object_name': 'Article',\n                            'perms': {'add': True, 'change': True, 'delete': True, 'view': True},\n                            'admin_url': 'http://testserver/test_admin/admin/mytest/article/',\n                            'add_url': 'http://testserver/test_admin/admin/mytest/article/add/',\n                        }\n                    ],\n                },\n            ],\n            'is_popup': False,\n            'is_nav_sidebar_enabled': True,\n        }\n        self.assertDictEqual(self.ctx, expected_vars)\n"], "sample_743": ["def test_radius_neighbors_graph_sparse_matrix():\n    n_samples = 100\n    n_features = 10\n    X = rng.rand(n_samples, n_features)\n\n    # Create a sparse matrix from the data\n    X_sparse = csr_matrix(X)\n\n    # Create a NearestNeighbors object with radius search\n    neigh = neighbors.NearestNeighbors(radius=0.5, algorithm='kd_tree')\n\n    # Fit the model to the sparse matrix\n    neigh.fit(X_sparse)\n\n    # Get the radius neighbors graph\n    A = neigh.radius_neighbors_graph(X_sparse, mode='connectivity')\n\n    # Assert that the graph is a sparse matrix\n    assert isinstance(A, csr_matrix)\n\n    # Assert that the shape of the graph is (n_samples, n_samples)\n    assert A.shape == (n_samples, n_samples)\n", "    def test_knn_with_sparse_matrix_precomputed(self):\n        X = coo_matrix((np.random.rand(10, 10), (np.arange(10),\n                                                      np.arange(10))),\n                       shape=(10, 10))\n        y = np.arange(10)\n        neigh = neighbors.NearestNeighbors(n_neighbors=2, metric='precomputed')\n        neigh.fit(X)\n        distances, indices = neigh.kneighbors(X)\n        assert_array_almost_equal(distances, np.zeros((10, 2)))\n        assert_in(0, indices)\n        ", "def test_sparse_csr_kneighbors():\n    X = coo_matrix([[1, 2], [3, 4], [5, 6]])\n    neigh = neighbors.NeighborsClassifier(n_neighbors=2, algorithm='brute')\n    neigh.fit(X.tocsr())\n    result = neigh.kneighbors(X.tocsr())\n    assert_array_equal(result[0].shape, (3, 2))\n    assert_array_equal(result[1].shape, (3, 2))\n\n    neigh_sparse_csr = neighbors.NeighborsClassifier(\n        n_neighbors=2, algorithm='kd_tree', metric='euclidean')\n    neigh_sparse_csr.fit(X.tocsr())\n    result_sparse_csr = neigh_sparse_csr.kneighbors(X.tocsr())\n    assert_array_equal(result_sparse_csr[0].shape, (3, 2))\n    assert_array_equal(result_sparse_csr[1].shape, (3, 2))\n    \n\n    neigh_sparse_dense = neighbors.NeighborsClassifier(\n        n_neighbors=2, algorithm='kd_tree', metric='euclidean')\n    neigh_sparse_dense.fit(X.toarray())\n    result_sparse_dense = neigh_sparse_dense.kneighbors(X.toarray())\n    assert_array_equal(result_sparse_dense[0].shape, (3, 2))\n    assert_array_equal(result_sparse_dense[1].shape, (3, 2))\n\n    neigh_sparse_dense = neighbors.NeighborsClassifier(\n        n_neighbors=2, algorithm='kd_tree', metric='euclidean')\n    neigh_sparse_dense.fit(X.toarray())\n    result_sparse_dense = neigh_sparse_dense.kneighbors(X.toarray())\n    assert_array_equal(result_sparse_dense[0].shape, (3, 2))\n    assert_array_equal(result_sparse_dense[1].shape, (3, 2))\n\n\n\n", "def test_radius_neighbors_graph_with_sparse_input():\n    for algorithm in ALGORITHMS:\n        X = csr_matrix(rng.rand(20, 10))\n        for mode in ['connectivity', 'distance']:\n            neigh = neighbors.RadiusNeighbors(algorithm=algorithm,\n                                             radius=1.5,\n                                             n_neighbors=5,\n                                             metric='euclidean')\n            neigh.fit(X)\n            graph = neigh.radius_neighbors_graph(X, radius=1.5, mode=mode)\n            assert_equal(graph.shape, (20, 20))\n\n\n", "    def test_radius_neighbors_graph_sparse_data(self):\n        n_samples = 100\n        n_features = 20\n        X = rng.randn(n_samples, n_features)\n        X_sparse = csr_matrix(X)\n        y = rng.randint(0, 2, size=n_samples)\n\n        neigh = neighbors.NearestNeighbors(\n            radius=0.5, algorithm='kd_tree', n_neighbors=10)\n        neigh.fit(X_sparse)\n\n        dist = neigh.radius_neighbors_graph(X_sparse, mode='distance')\n        assert_array_almost_equal(dist.toarray(),\n                                  neigh.kneighbors_graph(X_sparse,\n                                                         mode='distance').toarray())\n\n        conn = neigh.radius_neighbors_graph(X_sparse, mode='connectivity')\n        assert_array_almost_equal(conn.toarray(),\n                                  neigh.kneighbors_graph(X_sparse,\n                                                         mode='connectivity').toarray())\n\n"], "sample_546": ["    def test_constrained_layout_canvas_resize(self):\n        fig = Figure()\n        canvas = fig.canvas\n        # Initially, ensure the figure has no layout engine.\n        assert fig.get_layout_engine() is None\n        # Set the constrained layout engine.\n        fig.set_layout_engine(ConstrainedLayoutEngine())\n\n        # Check that the figure size is correctly adjusted when the canvas\n        # is resized.\n        canvas.set_size_inches(3, 2)\n        assert fig.get_size_inches() == (3, 2)\n        fig.tight_layout()\n        assert fig.get_size_inches() == (3, 2)\n\n        canvas.set_size_inches(5, 4)\n        assert fig.get_size_inches() == (5, 4)\n        fig.tight_layout()\n        assert fig.get_size_inches() == (5, 4)\n\n\n\n        \n", "    def test_fig_tight_layout_default(self, fig_test_canvas):\n        fig = Figure(figsize=(5, 4))\n        ax1 = fig.add_subplot(2, 2, 1)\n        ax2 = fig.add_subplot(2, 2, 2)\n        ax3 = fig.add_subplot(2, 2, 3)\n        ax4 = fig.add_subplot(2, 2, 4)\n\n        fig.tight_layout()\n\n        # Check if all axes are laid out within the figure\n\n        assert isinstance(fig.get_layout_engine(), TightLayoutEngine)\n        for ax in [ax1, ax2, ax3, ax4]:\n            with fig_test_canvas.check_image() as image:\n                bbox = ax.get_window_extent().transformed(fig.dpi_ratio)\n                image.save(\"tight_layout_test.png\")\n            assert 0 <= bbox.xmin < bbox.xmax <= 1\n            assert 0 <= bbox.ymin < bbox.ymax <= 1\n\n\n\n\n", "    def test_toolbar_warnings_deprecated(self):\n        with self.assertWarns(_EXPECTED_WARNING_TOOLMANAGER):\n            plt.figure()\n            plt.toolbar.set_tool_mode('navigate')\n            plt.show()\n\n", "    def test_savefig_with_transparent_figure(self, request, backend):\n        fig = Figure()\n        fig.canvas.manager.ensure_initialized()\n        # Set up the backend\n        fig.canvas.draw_idle()\n        fig.canvas.manager.backend = backend\n        fig.canvas.draw_idle()\n\n        # Save the figure with transparent background\n        fig.savefig('test_transparent.png', transparent=True, bbox_inches='tight')\n\n        # Check if the file is actually transparent\n        with open('test_transparent.png', 'rb') as f:\n            # This check is platform-dependent, check your preferred method\n            # for verifying transparency. \n\n            # For example, with PIL, you could check the alpha channel.\n            # from PIL import Image\n            # img = Image.open('test_transparent.png')\n            # assert img.mode == 'RGBA'  # Ensure it's in RGBA mode\n\n        # Remove the temp file\n        os.remove('test_transparent.png')\n", "    def test_figure_canvas_close(self, assert_warning):\n        fig = Figure()\n        canvas = fig.canvas = FigureCanvasBase(fig)\n\n        # Check if close method on the canvas closes the figure.\n        canvas.close()\n        assert fig.is_fignum_used() is False\n        assert fig.stale is False\n\n\n"], "sample_428": ["    def test_large_float_in_scientific_notation(self):\n        number = float(\"1.2345678901234567890123456789e+200\")\n        formatted_number = nformat(number, decimal_sep=\".\", decimal_pos=10, use_l10n=False)\n        self.assertEqual(formatted_number, \"1.234567890e+200\")\n", "    def test_large_numbers_with_scientific_notation(self):\n        number = Decimal(\"1.23456789012345678901234567890e200\")\n        self.assertEqual(nformat(number, decimal_sep=\",\", decimal_pos=2), \"1.23e+200\")\n", "    def test_zero_decimal_places(self):\n        self.assertEqual(nformat(\n            Decimal(1234567.89), decimal_sep='.', decimal_pos=0, grouping=3\n        ), '1,234,567')\n        self.assertEqual(nformat(\n            float('1234567.89'), decimal_sep='.', decimal_pos=0, grouping=3\n        ), '1,234,567')\n        self.assertEqual(nformat(\n            1234567.89, decimal_sep='.', decimal_pos=0, grouping=3\n        ), '1,234,567')\n        self.assertEqual(nformat(\n            3, decimal_sep='.', decimal_pos=0, grouping=3\n        ), '3')\n        self.assertEqual(nformat(\n            0, decimal_sep='.', decimal_pos=0, grouping=3\n        ), '0')\n", "    def test_large_value_scientific_notation(self):\n        number = Decimal(\"1.2345678901234567890123456789e200\")\n        formatted_number = nformat(number, decimal_sep=',', decimal_pos=None, force_grouping=True)\n        self.assertIn(\"e\", formatted_number)\n        self.assertLess(len(formatted_number), len(str(number)))\n\n", "    def test_scientific_notation_with_grouping(self):\n        number = 12345678901234567890.1234567890\n        result = nformat(number, decimal_sep=',' , decimal_pos=5, grouping=(3, 3), thousand_sep='.')\n        self.assertEqual(result, '12.345678901e+20')\n"], "sample_587": ["    def test_merge_priority_vars_dict(self):\n        data = create_test_data(2, 3)\n\n        data_copy = data.copy()\n        data_copy.coords[\"x\"].values = np.arange(4)\n\n        # Create two datasets\n        dataset1 = xr.Dataset(\n            {\"var1\": ((\"x\",), np.arange(3))}, coords={\"x\": data.coords[\"x\"]}\n        )\n\n        dataset2 = xr.Dataset(\n            {\"var1\": ((\"x\",), np.arange(3) * 2)}, coords={\"x\": data_copy.coords[\"x\"]}\n        )\n\n        # Merge with priority_arg set to 0 (dataset1 takes priority)\n        merged = merge([dataset1, dataset2], priority_arg=0)\n        assert merged[\"var1\"].values == np.arange(3)\n\n\n        # Merge with priority_arg set to 1 (dataset2 takes priority)\n        merged = merge([dataset1, dataset2], priority_arg=1)\n        assert merged[\"var1\"].values == np.arange(3) * 2\n", "    def test_merge_core_with_explicit_coords_conflict(self, compat, join):\n        data = create_test_data(\n            dims=[\"time\", \"lat\", \"lon\"],\n            coords={\"time\": [\"t1\", \"t2\"], \"lat\": [0, 1], \"lon\": [0, 1]},\n            data={\"var1\": [[1, 2], [3, 4]]},\n        )\n        data2 = create_test_data(\n            dims=[\"time\", \"lat\", \"lon\"],\n            coords={\"time\": [\"t1\", \"t2\"], \"lat\": [0, 1], \"lon\": [0, 1]},\n            data={\"var1\": [[5, 6], [7, 8]]},\n        )\n        explicit_coords = {\"time\",\"lat\"}\n        with raises_regex(\n            xr.merge_core(\n                [data, data2], compat=compat, join=join, explicit_coords=explicit_coords\n            )\n        ):\n            pass  \n", "    def test_merge_core_explicit_coords(self):\n        data = create_test_data()\n        coords = {'x': xr.DataArray(np.arange(3), name='x'}, 'y': xr.DataArray(\n            np.array([1, 2, 3]), name='y')}\n        result = merge_core(\n            [data, coords],\n            compat=\"broadcast_equals\",\n            join=\"outer\",\n            explicit_coords=['x', 'y'],\n            fill_value=0\n        )\n        assert len(result[0]) == 3\n        assert result[0].dims == ('x', 'y')\n", "    def test_merge_core_with_various_options(\n        self, compat, join, fill_value, expected_dim_sizes", "    def test_merge_explicit_coords(self):\n        data = create_test_data(\n            dims=(\"x\", \"y\"),\n            data={\"var1\": np.arange(4).reshape((2, 2))},\n            coords={\"x\": [0, 1], \"y\": [0, 1]},\n        )\n        data2 = create_test_data(\n            dims=(\"x\", \"y\"),\n            data={\"var1\": np.arange(4).reshape((2, 2))},\n            coords={\"x\": [1, 0], \"y\": [0, 1]},\n        )\n        explicit_coords = {\"x\", \"y\"}\n        with raises_regex(ValueError, \"coordinate\"):\n            merge(data, data2, explicit_coords=explicit_coords)\n\n\n\n"], "sample_1137": ["compilation error", "    def test_convert_to_multiple_units():\n        expr = 3 * joule\n        target_units = [meter, kilogram, second]\n        result = convert_to(expr, target_units)\n        expected = 3*meter*kilogram/second**2\n        assert result == expected\n", "compilation error", "    def test_check_dimensions_add_dimensionless(self):\n        expr = kilogram + speed_of_light\n        check_dimensions(expr)\n", "compilation error"], "sample_780": ["    def test_online_learning_perplexity(n_components):\n        n_docs, X = _build_sparse_mtx()\n\n        lda = LatentDirichletAllocation(n_components=n_components,\n                                        learning_method='online',\n                                        max_iter=20,\n                                        batch_size=n_docs // 2,\n                                        total_samples=n_docs)\n        lda.fit(X)\n        perplexity = lda.perplexity(X)\n\n        assert perplexity <= 1.0, (\n            \"Perplexity should be <= 1 for a perfect model, got: %f\" % perplexity\n        )\n\n\n\n", "    def test_partial_fit_convergence(self, n_components):\n        n_samples, X = _build_sparse_mtx()\n        lda = LatentDirichletAllocation(n_components=n_components)\n\n        # Ensure the model converges in a reasonable number of iterations\n        converged = False\n        for i in range(100):\n            lda.partial_fit(X)\n            if lda.perplexity(X) < 1.0:\n                converged = True\n                break\n\n        assert converged, \"Model did not converge after 100 iterations\"\n\n\n\n", "    def test_perplexity_with_sub_sampling(self):\n        n_components, X = _build_sparse_mtx()\n        lda = LatentDirichletAllocation(n_components=n_components)\n        lda.fit(X)\n        \n        # test perplexity with sub sampling\n        total_samples = 100  # Total number of documents\n        sub_sampling_ratio = 0.5  #\n        \n        for i in range(total_samples):\n            sub_X = X[:int(total_samples * sub_sampling_ratio), :]\n            perplexity = lda.perplexity(sub_X, sub_sampling=True)\n            \n            # Check if perplexity is lower than that with no sampling\n            \n", "    def test_partial_fit_with_batch_size_one(self, random_state):\n        n_components, X = _build_sparse_mtx()\n        lda = LatentDirichletAllocation(n_components=n_components, random_state=random_state)\n        lda.partial_fit(X, y=None)\n", "    def test_partital_fit_n_iter_incremental(self):\n        n_components, X = _build_sparse_mtx()\n        lda = LatentDirichletAllocation(n_components=n_components,\n                                        total_samples=X.shape[0],\n                                        learning_method=\"online\")\n\n        # Do partial fitting to incrementally update the model\n\n        n_iter_ = lda.n_iter_ = 0\n        batch_size = 2\n        for i in range(0, X.shape[0], batch_size):\n            lda.partial_fit(X[i:i + batch_size])\n            assert_greater_equal(lda.n_iter_, n_iter_ + 1)\n            n_iter_ += 1\n\n        assert_equal(lda.n_iter_, X.shape[0] // batch_size)\n\n\n\n"], "sample_609": ["    def test_remap_nested_dictionaries(\n        request,", "    def test_join_dict_keys(join, keep_attrs):\n        d1 = {\"a\": xr.DataArray([\"A\", \"B\"]), \"c\": xr.DataArray([1, 2])}\n        d2 = {\"b\": xr.DataArray([3, 4]), \"c\": xr.DataArray([5, 6])}\n        result = join_dict_keys(d1, d2, join=join, keep_attrs=keep_attrs)\n        if join == \"inner\":\n            expected = {\"c\": xr.DataArray([1, 2])}\n        elif join == \"outer\":\n            expected = {\"a\": xr.DataArray([\"A\", \"B\"]), \"b\": xr.DataArray([3, 4]), \"c\": xr.DataArray([1, 2, 5, 6])}\n        elif join == \"exact\":\n            expected = {\"a\": xr.DataArray([\"A\", \"B\"]), \"b\": xr.DataArray([3, 4]), \"c\": xr.DataArray([1, 2, 5, 6])}\n\n        assert_identical(result, expected)\n\n\n", "def test_unified_dim_sizes():\n    da1 = xr.DataArray(\n        np.arange(12).reshape((2, 3, 2)),\n        dims=[\"d1\", \"d2\", \"d3\"],\n        coords=[\n            {\"d1\": [0, 1]},\n            {\"d2\": [0, 1, 2]},\n            {\"d3\": [0, 1]},\n        ],\n    )\n    da2 = xr.DataArray(\n        np.arange(24).reshape((2, 3, 2)),\n        dims=[\"d1\", \"d2\", \"d3\"],\n        coords=[\n            {\"d1\": [0, 1]},\n            {\"d2\": [0, 1, 2]},\n            {\"d3\": [0, 1]},\n        ],\n    )\n    da3 = xr.DataArray(\n        np.arange(6).reshape((2, 3)),\n        dims=[\"d1\", \"d2\"],\n        coords=[{\"d1\": [0, 1]}, {\"d2\": [0, 1, 2]}],\n    )\n\n    assert unified_dim_sizes([da1, da2, da3]) == {\n        \"d1\": 2,\n        \"d2\": 3,\n    }\n    assert unified_dim_sizes(\n        [\n            xr.DataArray(\n                np.arange(6).reshape((2, 3)), dims=[\"d1\", \"d2\"], coords=[{\"d1\": [0, 1]}, {\"d2\": [0, 1, 2]}]\n            ),\n            xr.DataArray(\n                np.arange(36).reshape((3, 4, 3)), dims=[\"d1\", \"d2\", \"d3\"], coords=[{\"d1\": [0, 1, 2]}, {\"d2\": [0, 1, 2, 3]}, {\"d3\": [0, 1, 2]}]\n            ),\n        ]\n    ) == {\"d1\": 3}\n    assert unified_dim_sizes(\n        [\n            xr.DataArray(\n                np.arange(6).reshape((2, 3)), dims=[\"d1\",", "    def test_ordered_set_intersection():\n        assert ordered_set_intersection(set(\"ab\"), set(\"bc\")) == set(\"ab\")\n        assert ordered_set_intersection(set(\"ab\"), set(\"ab\")) == set(\"ab\")\n        assert ordered_set_intersection(set(\"abc\"), set(\"cde\")) == set()\n        assert ordered_set_intersection(set(\"abc\"), set(\"abe\")) == set(\"ab\")\n\n\n\n", "    def test_ordered_set_intersection():\n        \"\"\"Test ordered_set_intersection\"\"\"\n        set1 = ordered_set_intersection({'a', 'b', 'c', 'd'}, {'b', 'c', 'e', 'f'})\n        assert set1 == {'b', 'c'}\n\n        set2 = ordered_set_intersection({'a', 'b', 'c'}, {'c', 'd', 'e'})\n        assert set2 == {'c'}\n\n        set3 = ordered_set_intersection({'a', 'b'}, {'c', 'd'})\n        assert set3 == set()\n"], "sample_475": ["    def test_RawId_check(self):\n        class RawIdAdmin(ModelAdmin):\n            raw_id_fields = ['id']\n\n        self.assertIsInvalid(\n            RawIdAdmin, ValidationTestModel, r\"raw_id_fields should not include 'id'\",\n            id=\"admin.E140\"\n        )\n\n\n", "    def test_invalid_extra_in_inline(self):\n        class MyInlineAdmin(ValidationTestInlineModel):\n            extra = 1.5\n\n        self.assertIsInvalid(\n            MyInlineAdmin,\n            ValidationTestModel,\n            \"The value of 'extra' must be an integer.\",\n            id=\"admin.E203\",\n        )\n\n\n", "    def test_check_readonly_fields(self):\n        class ReadonlyAdmin(ModelAdmin):\n            model = ValidationTestModel\n            readonly_fields = ['field1']\n            \n\n        self.assertIsInvalid(\n            ReadonlyAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' refers to 'field1', which is not a \"\n            \"callable, an attribute of 'ValidationTestModel', or an attribute \"\n            \"or method on 'ValidationTestModel'.\",\n            id=\"admin.E108\",\n        )\n\n        class ReadonlyAdmin2(ModelAdmin):\n            model = ValidationTestModel\n            readonly_fields = (\n                'field1',\n            )\n\n        self.assertIsValid(\n            ReadonlyAdmin2, ValidationTestModel\n        )\n\n\n        class ReadonlyAdmin3(ModelAdmin):\n            model = ValidationTestModel\n            readonly_fields = ['field1', 'name']\n            \n\n        self.assertIsValid(\n            ReadonlyAdmin3, ValidationTestModel\n        )\n", "    def test_raw_id_fields_not_allowed(self):\n        class RawIdAdmin(ModelAdmin):\n            raw_id_fields = [\"id\"]\n\n        self.assertIsInvalid(\n            RawIdAdmin,\n            ValidationTestModel,\n            \"The 'raw_id_fields' attribute cannot contain 'id' as a field.\",\n            id=\"admin.E10\",\n        )\n\n\n\n", "    def test_raw_id_fields(self):\n        class RawIdAdmin(ModelAdmin):\n            raw_id_fields = [\"song\"]\n\n        self.assertIsInvalid(\n            RawIdAdmin(Song, AdminSite()),\n            Song,\n            \"The value of 'raw_id_fields' should be a list or tuple of field names.\",\n            id=\"admin.E207\",\n        )\n\n        class RawIdAdmin2(ModelAdmin):\n            raw_id_fields = [\"song\", \"not_a_field\"]\n\n        self.assertIsInvalid(\n            RawIdAdmin2(Song, AdminSite()),\n            Song,\n            \"The value of 'raw_id_fields' refers to '%s', which is not a field of \"\n            \"'%s'.\" % (\"not_a_field\", Song._meta.label),\n            id=\"admin.E208\",\n        )\n\n\n\n"], "sample_1107": ["    def test_generate_bell_empty():\n        assert list(generate_bell(0)) == [()]", "    def test_signed_permutations():\n        for t in [(1, 2, 3), (0, 1, 2), (-1, 2, 0), (1, -2, 0), (1, 2, -3), (1, -1, 1)]:\n            for result in signed_permutations(t):\n                assert len(result) == len(t)\n                assert all(i in result for i in t)\n", "compilation error", "    def test_perms_equal():\n        from sympy.combinatorics.permutations import Permutation\n\n        P = Permutation((1, 2, 3))\n        Q = Permutation((1, 2, 3))\n        assert permutations_equal(P, Q) is True\n        assert permutations_equal(P, P) is True\n        assert permutations_equal(P, Permutation((3, 2, 1))) is False\n        assert permutations_equal(P, Permutation((2, 1, 3))) is False\n        \n        P = Permutation((1, 2, 3))\n        Q = Permutation((2, 1, 3))\n        assert permutations_equal(P, Q) is False\n\n\n", "compilation error"], "sample_564": ["    def test_errorbar3d_with_limits(tmpdir):\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection='3d')\n        x, y, z = get_test_data()\n        ax.plot_surface(x, y, z, alpha=0.5)\n\n        ax.errorbar(x, y, z, xerr=0.1, yerr=0.2, zerr=0.3,\n                   fmt='o', capsize=2, capthick=2, elinewidth=3,\n                   uplims=True, lolims=True,\n                   color=['r', 'g', 'b'], label='Errorbar 3D')\n\n        fig.savefig(tmpdir.join('test_errorbar3d_with_limits.png'))\n\n", "    def test_stem3D():\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection='3d')\n        x, y, z = get_test_data()\n        ax.stem(x, y, z, linefmt='r-', markerfmt='bo', basefmt='g--')\n        ax.set_xlim3d([x.min() - 1, x.max() + 1])\n        ax.set_ylim3d([y.min() - 1, y.max() + 1])\n        ax.set_zlim3d([z.min() - 1, z.max() + 1])\n        plt.show()\n\n", "def test_cuboid_plot(fig_test_ax):\n    fig_test_ax, _ = _cuboid_func()\n\n\n", "    def test_errorbar_data(tmpdir):\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection='3d')\n\n        X, Y, Z = get_test_data()\n        ax.scatter(X, Y, Z, s=50, c=Z, cmap=cm.viridis)\n        ax.errorbar(X, Y, Z, xerr=X*0.1, yerr=Y*0.1, zerr=Z*0.2,\n                    fmt='o', linestyle='-', capsize=4, label='Data Points')\n        # Make a tight bounding box and save the figure\n        fig.savefig(tmpdir.join('test.png'))\n\n\n\n", "    def test_plot_cuboid(tmpdir):\n        fig = plt.figure(figsize=(6, 6))\n        ax = fig.add_subplot(111, projection='3d')\n\n        plot_cuboid(ax, (1, 1, 1))\n\n        # Save the figure\n        fig.savefig(tmpdir.join('test_plot_cuboid.png'))\n"], "sample_567": ["    def test_annotation_arrow_relpos(self, image_comparison_backend):\n        fig, ax = plt.subplots()\n        a = Annotation(\"Hello\", (0.5, 0.5), xycoords='data',\n                       xytext=(0.6, 0.3),\n                       arrowprops=dict(relation=\"xy\", arrowstyle=\"->\",\n                                      relpos=(0.25, 0.75)))\n        ax.add_artist(a)\n        plt.xlim(0, 1)\n        plt.ylim(0, 1)\n        ax.set_aspect(\"equal\")\n        plt.show()\n", "    def test_arrow_patch_styles(self, arrowstyle, relpos, expected_end, expected_shrink):\n        fig, ax = plt.subplots()\n        ann = Annotation(\"Test\", (0, 0), xytext=(1, 1),\n                         arrowprops=dict(arrowstyle=arrowstyle, relpos=relpos))\n        ann.set_figure(fig)\n        ax.add_artist(ann)\n\n        renderer = fig.canvas.get_renderer()\n        ann.update_positions(renderer)\n        ann.draw(renderer)\n\n        arrow = ann.arrow_patch\n\n        assert arrow.arrowstyle == arrowstyle\n        assert arrow.relpos == relpos\n        assert arrow.get_positions() == (expected_end, expected_end)\n        assert arrow.", "    def test_offset_from(self):\n        fig, ax = plt.subplots()\n\n        class MyOffset(OffsetFrom):\n                super().__init__(artist, ref_coord, unit)\n\n        offset = MyOffset(ax, (0.5, 0.5))\n                \n        # test the transform\n        renderer = fig.canvas.renderer\n        transform = offset(renderer)\n        assert isinstance(transform, mtransforms.Affine2D)\n\n        # test that the offset is applied correctly\n        x, y = offset(renderer).transform((0, 0))\n        assert_almost_equal(x, ax.get_position().x0 + ax.get_position().width / 2, 4)\n        assert_almost_equal(y, ax.get_position().y0 + ax.get_position().height / 2, 4)\n\n        plt.close(fig)\n\n", "    def test_annotation_xycoords(self):\n        fig, ax = plt.subplots()\n\n        # Test with basic strings\n        a = Annotation(\"test\", (0.5, 0.5), xycoords=\"data\", textcoords=\"data\")\n        a.draggable()\n        ax.add_artist(a)\n\n        # Test with Artists\n        b = Annotation(\"test2\", ax.patches[0], xycoords=ax.patches[0],\n                       textcoords=ax.patches[0])\n        b.draggable()\n        ax.add_artist(b)\n\n        # Test with Transforms\n        c = Annotation(\"test3\", mtransforms.IdentityTransform(),\n                       xycoords=mtransforms.IdentityTransform(),\n                       textcoords=mtransforms.IdentityTransform())\n        c.draggable()\n        ax.add_artist(c)\n\n        plt.show()\n", "    def test_annotation_offset_pixels(self, image_comparison_skip=False):\n        if image_comparison_skip:\n            return\n\n        fig, ax = plt.subplots()\n        text = Annotation('', xy=(0.5, 0.5), xycoords='figure pixels',\n                          textcoords='offset pixels',\n                          arrowprops=dict(arrowstyle='->'))\n        text.set_text('Annotation')\n        ax.add_artist(text)\n\n        fig.canvas.draw()\n        self.check_image_comparison()\n\n\n\n"], "sample_647": ["    def test_warn_explicit_for(self, warning_class):\n            pass\n\n        with pytest.warns(warning_class) as w:\n            warn_explicit_for(my_func, warning_class('test warning'))\n\n        assert len(w) == 1\n        assert w[0].filename == inspect.getfile(my_func)\n        assert w[0].lineno == my_func.__code__.co_firstlineno\n", "    def test_warn_explicit_for(self, warning_class):\n            pass\n\n        warn_explicit_for(my_func, warning_class(\"test\"))\n        with self.check_warnings(\n            [\n                DeprecationWarning(warning_class(\"test\").format())\n            ],\n            module=\"your_module.py\",\n        ) as w:\n            pass\n        assert len(w) == 1\n        assert w[0].category == warning_class\n", "    def test_warn_explicit_for(self, warning_class: Warning):\n            pass\n\n        message = warning_class(f\"test message for {target_func}\")\n        warn_explicit_for(target_func, message)\n\n        with self.assertwarns(warning_class) as w:\n            self.exec(\n                f\"{'_testing_globals__warningregistry__'} = {{}}\",\n                filename=self.tmp_path / \"testfile.py\",\n            )\n            self.exec(f\"target_func()\", filename=self.tmp_path / \"testfile.py\")\n        assert len(self.recorder.records) == 1\n        assert w.message == message.args[0]\n", "    def test_warn_explicit_for(pytester: Pytester, warning_class: type[Warning]) -> None:\n            pass\n\n        with pytester.raises(warning_class):\n            warning_types.warn_explicit_for(func, warning_class())\n        \n        # Check that the warning is recorded\n        result = pytester.get_warnings()\n        assert len(result) == 1\n        assert result[0].category == warning_class\n", "        def test_warn_explicit_for(warning_class, pytester: Pytester):\n                pass\n\n            with pytest.warns(warning_class) as warns:\n                warn_explicit_for(test_func, warning_class(f\"test warning from warn_explicit_for\"))\n\n            assert len(warns) == 1\n            first_warn = warns[0]\n            assert first_warn.filename == inspect.getfile(test_func)\n            assert first_warn.lineno == test_func.__code__.co_firstlineno\n            assert first_warn.category is warning_class\n"], "sample_594": ["    def test_diff_array_repr(self, compat):\n        a = xr.DataArray(\n            np.random.rand(2, 3),\n            dims=[\"x\", \"y\"],\n            coords={\"x\": [1, 2], \"y\": [3, 4, 5]},\n        )\n        b = xr.DataArray(\n            np.random.rand(2, 3) + 0.5,\n            dims=[\"x\", \"y\"],\n            coords={\"x\": [1, 2], \"y\": [3, 4, 5]},\n        )\n        if compat == \"equals\":\n            b.data[0, 0] = a.data[0, 0]\n        summary = formatting.diff_array_repr(a, b, compat)\n        if compat == \"equals\":\n            expected = dedent(\n                \"\"\"\n            Left and right DataArray objects are not equal\n            Differing values:\n                L    <xarray.DataArray (2, 3)>\n                    [(array([[0.2457348, 0.7539157, 0.04622676]])\n                      ,\n                     array([0.1234567, 0.8901234, 0.56789012])),\n                    dims=[('x', 2), ('y', 3)],\n                    coords={'x': array([1, 2]), 'y': array([3, 4, 5])}]\n\n                R    <xarray.DataArray (2, 3)>\n                    [(array([[0.2457348, 0.7539157, 0.04622676]])\n                      ,\n                     array([0.1234567, 0.8901234, 0.56789012])),\n                    dims=[('x', 2),", "    def test_short_numpy_repr(self, array, expected):\n        with set_numpy_options(threshold=1000):\n            assert formatting.short_numpy_repr(array) == expected\n\n\n", "    def test_diff_array_repr(self, compat: Union[str, type, None]):\n        a = xr.DataArray(\n            np.arange(10).reshape(2, 5),\n            dims=(\"x\", \"y\"),\n            coords={\"x\": [1, 2], \"y\": [3, 4, 5, 6, 7]},\n        )\n        b = xr.DataArray(\n            np.arange(10).reshape(2, 5) + 1,\n            dims=(\"x\", \"y\"),\n            coords={\"x\": [1, 2], \"y\": [3, 4, 5, 6, 7]},\n        )\n        if compat is None:\n            expected = dedent(\n                \"\"\"\n                Left and right DataArray objects are not equal\n                Dimensions: (x: 2, y: 5)\n                Differing values:\n                      L\n                array([[1, 2, 3, 4, 5],\n                       [6, 7, 8, 9, 10]])\n                      R\n                array([[2, 3, 4, 5, 6],\n                       [7, 8, 9, 10, 11]])\n\n            \"\"\"\n            )\n        else:\n            expected = dedent(\n                \"\"\"\n                Left and right DataArray objects are not {str(compat)}\n                Dimensions: (x: 2, y: 5)\n            \"\"\".format(\n                    str=compat\n                )\n            )\n        result = formatting.diff_array_repr(a, b, compat)\n        assert result == expected\n", "    def test_format_array_flat(self):\n        # Test edge cases of format_array_flat\n        array = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])\n        # too short\n        assert formatting.format_array_flat(array, 2) == \"1 2 3 4 5 6 7 8 9\"\n        # just enough space for all elements\n        assert formatting.format_array_flat(array, 10) == \"1 2 3 4 5 6 7 8 9\"\n        # too long, truncate with ellipsis\n        assert formatting.format_array_flat(array, 1) == \"...\"\n        # with even number of elements\n        array = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n        assert formatting.format_array_flat(array, 6) == \"1 2 3 4 5 6...\"\n", "    def test_last_n_items_empty_array():\n        x = np.empty((0,), dtype=\"int64\")\n        assert formatting.first_n_items(x, 5) == []\n        assert formatting.last_n_items(x, 5) == []\n"], "sample_1026": ["    def test_lambdastr_nested_args():\n        expected = \"\"\"lambda x,y,z: (lambda u,v: (u + v))(_0[0],_0[1])\"\"\"\n        actual = lambdastr((x, (y, z)), x + y)\n        assert actual == expected\n", "    def test_tensor_flow_functions():\n        if not tensorflow:\n            return\n\n            return tf.tanh(x)\n        from tensorflow.keras.layers import Dense\n        model = Dense(1, activation=tanh)\n        func = lambdify(x, model(tf.constant(x)), dummify=True, modules=['tensorflow'])\n\n        assert func(1.0) == tf.tanh(1.0).numpy()\n\n", "    def test_nested_lists():\n        from sympy.matrices import DeferredVector\n        expr = lambda x, y, z: [\n                sin(x*y),\n                [cos(z),\n                 DeferredVector([1, 2, 3])]\n            ]\n\n        func = lambdify((x, y, z), expr, 'numpy', module='numpy')\n        result = func(1, 2, 3)\n        expected = [\n                numpy.sin(1*2),\n                [numpy.cos(3), numpy.array([1, 2, 3])]\n            ]\n        assert numpy.allclose(result, expected)\n\n", "    def test_lambdify_with_sympy_symbols():\n        \"\"\"\n        Test that lambdify properly handles sympy symbols as arguments.\n\n        This test ensures that when lambdify encounters sympy symbols\n        within its arguments, it preserves them and generates working\n        lambda functions.\n        \"\"\"\n        f = lambdify((x, y), x**2 + y**2)\n        result = f(2, 3)\n        assert result == 13\n\n", "    def test_nested_lists():\n        from sympy import Array\n\n        expr = Array([[x, x + 1], [y, y + 1]])\n        func = lambdify((x, y), expr)\n        result = func(2, 3)\n        expected = Array([[2, 3], [3, 4]])\n        assert (result == expected).all()\n"], "sample_467": ["    def test_value_omitted_from_data_empty_date(self):\n        form = Form({\"date\": \"\"})\n        self.assertFalse(\n            self.widget.value_omitted_from_data(form.cleaned_data, None, \"date\")\n        )\n", "    def test_value_from_datadict_with_empty_values(self):\n        \"\"\"\n        `value_from_datadict` should handle empty values correctly.\n        \"\"\"\n        data = {\n            \"form-year\": \"\",\n            \"form-month\": \"\",\n            \"form-day\": \"\",\n        }\n        form = Form(\n            {\"form\": DateField(widget=self.widget)}, data\n        )\n        self.assertEqual(form.cleaned_data[\"form\"], None)\n", "    def test_value_from_datadict_blank_fields(self):\n        data = {\n            \"form-year\": \"\",\n            \"form-month\": \"\",\n            \"form-day\": \"\",\n        }\n        form = Form({\"date\": DateField()})\n        value = self.widget.value_from_datadict(data, {}, \"form-date\")\n        self.assertIsNone(value)\n", "    def test_format_value_invalid_date(self):\n        widget = self.widget\n        input_format = get_format(\"DATE_INPUT_FORMATS\")[0]\n        input_format = formats.sanitize_strftime_format(input_format)\n        invalid_date = \"2017-13-32\"\n        expected_value = {\"year\": \"2017\", \"month\": \"\", \"day\": \"\"}\n        self.assertDictEqual(widget.format_value(invalid_date), expected_value)\n\n", "    def test_format_value_with_none_values(self):\n        date = None\n        rendered_html = self.widget.format_value(date)\n        self.assertEqual(rendered_html, {\"year\": None, \"month\": None, \"day\": None})\n"], "sample_146": ["    def test_check_setting_language_code_invalid(self):\n        with override_settings(LANGUAGE_CODE=self.invalid_tags[0]):\n            errors = list(check_setting_language_code(\n                app_configs=[],\n                **{}\n            ))\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'translation.E001')\n        self.assertIn(self.invalid_tags[0], errors[0].msg)\n", "    def test_check_setting_languages_bidi_invalid_value(self):\n        with override_settings(LANGUAGES_BIDI=['en', 123]):\n            errors = check_setting_languages_bidi({}, {})\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'translation.E003')\n        self.assertIn('123', errors[0].msg)\n\n        with override_settings(LANGUAGES_BIDI=[None, 'en']):\n            errors = check_setting_languages_bidi({}, {})\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'translation.E003')\n        self.assertIn('None', errors[0].msg)\n", "    def test_language_settings_consistent(self):\n        with self.assertLogs() as cm:\n            check_language_settings_consistent(None, {})\n        self.assertEqual(cm.output, [])  \n", "    def test_check_language_settings_consistent(self):\n        with override_settings(\n                LANGUAGES=[('en', 'English'), ('fr', 'French')],\n                LANGUAGE_CODE='es'):\n            errors = check_language_settings_consistent([], {})\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'translation.E004')\n", "    def test_check_setting_language_code_invalid(self):\n        with override_settings(LANGUAGE_CODE=None):\n            errors = check_setting_language_code([], {})\n            self.assertEqual(len(errors), 1)\n            self.assertEqual(errors[0].id, 'translation.E001')\n            self.assertIn('None', errors[0].msg)\n\n        with override_settings(LANGUAGE_CODE=123):\n            errors = check_setting_language_code([], {})\n            self.assertEqual(len(errors), 1)\n            self.assertEqual(errors[0].id, 'translation.E001')\n            self.assertIn('123', errors[0].msg)\n\n        with override_settings(LANGUAGE_CODE=b'en'):\n            errors = check_setting_language_code([], {})\n            self.assertEqual(len(errors), 1)\n            self.assertEqual(errors[0].id, 'translation.E001')\n            self.assertIn('b\\'en\\'', errors[0].msg)\n"], "sample_1201": ["    def test_cgs_gauss_conversion():\n        assert convert_to(coulomb_constant, cgs_gauss) == coulomb_constant  \n        assert convert_to(elementary_charge, cgs_gauss).evalf() == 4.80320680 * statcoulomb\n        assert convert_to(gravitational_constant, cgs_gauss).evalf() == 6.67430e-8 * erg/(gram * centimeter**2)\n        assert convert_to(planck, cgs_gauss).evalf() == (6.62607015e-34 * erg * second) \n", "    def test_cgs_gauss_conversions():\n        assert convert_to(coulomb, 'coulomb', cgs_gauss) == 10 * speed_of_light * statcoulomb\n        assert convert_to(ampere, 'ampere', cgs_gauss) == 10 * speed_of_light * statcoulomb/second\n        assert convert_to(volt, 'volt', cgs_gauss) == 10**6/speed_of_light * statvolt\n        assert convert_to(weber, 'weber', cgs_gauss) == 10**8*maxwell\n        assert convert_to(tesla, 'tesla', cgs_gauss) == 10**4*gauss\n", "    def test_cgs_gauss_conversions():\n        assert convert_to(1 * coulomb, cgs_gauss.coulomb) == 10 * speed_of_light * statcoulomb\n        assert convert_to(1 * ampere, cgs_gauss.ampere) == 10 * speed_of_light * statcoulomb/second\n        assert convert_to(1 * volt, cgs_gauss.volt) == 10**6/speed_of_light * statvolt\n        assert convert_to(1 * weber, cgs_gauss.weber) == 10**8 * cgs_gauss.maxwell\n        assert convert_to(1 * tesla, cgs_gauss.tesla) == 10**4 * cgs_gauss.gauss\n", "    def test_cgs_gauss_coulomb_conversion():\n        assert convert_to(coulomb, statcoulomb) == 10*speed_of_light*statcoulomb\n", "    def test_cgs_gauss_coulomb_constant(self):\n        cc = cgs_gauss.get_quantity_scale_factor(coulomb_constant)\n        self.assert_almost_equal(cc, 1, places=15)\n"], "sample_299": ["    def test_check_default_cache_is_configured(self):\n        result = check_default_cache_is_configured(app_configs=None, **{})\n        self.assertEqual(result, [])\n", "    def test_check_default_cache_is_configured_invalid(self):\n        with self.subTest('checks for invalid cache'):\n            errors = check_default_cache_is_configured({}, )\n            self.assertEqual(errors, [E001])\n\n\n\n", "    def test_check_default_cache_is_configured(self):\n        with self.assertLogs(('django.core.checks', 'ERROR')) as logs:\n            check_default_cache_is_configured({},)\n\n        self.assertEqual(len(logs.output), 1)\n        self.assertEqual(logs.output[0], \"django.core.checks:ERROR:caches.E001 You must define a 'default' cache in your CACHES setting.\")\n", "    def test_check_default_cache_is_configured(self):\n        with self.subTest('missing default cache'):\n            with override_settings(CACHES={}):\n                errors = check_default_cache_is_configured(None, {})\n                self.assertEqual(errors, [E001])\n\n        with self.subTest('default cache is configured'):\n            errors = check_default_cache_is_configured(None, {})\n            self.assertEqual(errors, [])\n\n\n", "    def test_check_file_based_cache_is_absolute(self):\n        with override_settings(\n            CACHES={\n                'default': {\n                    'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                    'LOCATION': 'cache/data',\n                },\n            }\n        ):\n            errors = check_file_based_cache_is_absolute({}, {})\n            self.assertEqual(len(errors), 1)\n            self.assertEqual(\n                errors[0].message,\n                \"Your 'default' cache LOCATION path is relative. Use an \"\n                \"absolute path instead.\",\n            )\n"], "sample_1080": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_252": ["    def test_decoding_null_values(self):\n        with self.assertNumQueries(1) as queries:\n            obj = JSONModel.objects.create(json_field=None)\n            self.assertIsNone(obj.json_field)\n\n\n", "    def test_transform_key_isnull(self):\n        model = JSONModel.objects.create(**{\n            'value': {'a': 1, 'b': None},\n        })\n\n        # Test KeyTransformIsNull lookup works as expected\n        self.assertEqual(JSONModel.objects.filter(\n            value__a__isnull=False).count(), 1)\n        self.assertEqual(JSONModel.objects.filter(\n            value__b__isnull=True).count(), 1)\n\n        # Test KeyTransformIsNull lookup with a specific key name\n        self.assertEqual(JSONModel.objects.filter(\n            value__c__isnull=True).count(), 0)\n\n        # Test KeyTransformIsNull lookup in combination with other lookups\n        self.assertEqual(JSONModel.objects.filter(\n            value__a__isnull=False, value__b__isnull=True).count(), 1)\n\n\n\n", "    def test_key_transform_in_with_null_values(self):\n        with self.assertNumQueries(1):\n            qs = JSONModel.objects.filter(\n                data__json_path__in=[\n                    {'key': 'value'},\n                    None,\n                    {'key': 'value2'},\n                ]\n            )\n            self.assertEqual(qs.count(), 2)\n\n", "    def test_json_field_invalid_data_raises_error(self):\n        with self.assertRaises(ValidationError):\n            json_model = JSONModel(name='test', data='{\"invalid\":}')\n            json_model.full_clean()\n", "    def test_key_transform_exact_with_null(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\"CREATE TABLE json_nullable_field_test (id SERIAL PRIMARY KEY, data JSONB)\")\n\n        model = type('JSONNullableFieldTest', (models.Model,), {\n            'id': models.AutoField(),\n            'data': JSONField(null=True)\n        })\n\n        obj = model.objects.create(data={'key': 'value'})\n        self.assertEqual(model.objects.get(id=obj.id).data['key'], 'value')\n        self.assertEqual(model.objects.filter(data__key__isnull=False).count(), 1)\n\n        self.assertEqual(model.objects.filter(data__key__exact=None).count(), 0)\n\n\n\n        obj.data = None\n        obj.save()\n\n        self.assertEqual(model.objects.filter(data__isnull=True).count(), 1)\n        self.assertEqual(model.objects.filter(data__key__isnull=True).count(), 1)\n\n"], "sample_983": ["    def test_row_structure_symbolic_cholesky(self):\n        S = SparseMatrix([\n            [1, 0, 3, 2],\n            [0, 0, 1, 0],\n            [4, 0, 0, 5],\n            [0, 6, 7, 0]])\n        R = S.row_structure_symbolic_cholesky()\n        assert R == [[0], [], [0], [1, 2]]\n\n", "    def test_LDLdecomposition_large(self):\n        from sympy.matrices import SparseMatrix\n        A = SparseMatrix(((1, 2, 3), (4, 5, 6), (7, 8, 9)), \n                         shape=(3, 3))\n        L, D = A.LDLdecomposition()\n        self.assertEqual(L.rows, A.rows)\n        self.assertEqual(D.rows, A.rows)\n        self.assertEqual(L.cols, A.cols)\n        self.assertEqual(D.cols, A.cols)\n        self.assertEqual(L.nnz(), 6)\n        self.assertEqual(D.nnz(), 9)\n        self.assertTrue((L*D*L.T - A).nnz() == 0)\n", "    def test_row_structure_symbolic_cholesky(self):\n        from sympy.matrices import SparseMatrix\n        for N in [1, 2, 3, 4]:\n            A = SparseMatrix([[i*j for j in range(N)] for i in range(N)])\n            R = A.row_structure_symbolic_cholesky()\n            for i in range(N):\n                for j in R[i]:\n                    assert j < N, \"Index out of bounds\"\n                    assert A[i, j] != 0, \"Non-zero element should be present\"\n\n\n\n        \n", "    def test_row_list(self):\n        M = SparseMatrix(((1, 2), (3, 4)))\n        self.assertEqual(M.row_list(), [(0, 0, 1), (0, 1, 2), (1, 0, 3), (1, 1, 4)])\n", "    def test_row_swap(self):\n        A = SparseMatrix([[1, 2], [3, 4]])\n        B = A.row_swap(0, 1)\n        assert(B.rows == A.rows and B.cols == A.cols)\n        assert(B._smat == {(1, 0): 3, (1, 1): 4, (0, 0): 2, (0, 1): 1})\n        for i, j in [(0, 0), (0, 1), (1, 0), (1, 1)]:\n            assert(B[i, j] == A[i + (1 if i >= 1 else 0), j + (1 if j >= 1 else 0)])\n"], "sample_931": ["    def test_parse_annotation(self):\n        assert _parse_annotation('int', None) == (('int',), None)\n        assert _parse_annotation('List[str]', None) == (('List', 'str'), None)\n        assert _parse_annotation('Optional[int]', None) == (('Optional', 'int'), None)\n        assert _parse_annotation('Dict[str, int]', None) == (('Dict', 'str', 'int'), None)\n        assert _parse_annotation('Union[str, int]', None) == (('Union', 'str', 'int'), None)\n        assert _parse_annotation('NoReturn', None) == (('NoReturn',), None)\n\n        with pytest.raises(ValueError):\n            _parse_annotation('int,str', None)\n\n        with pytest.raises(ValueError):\n            _parse_annotation('List[int, str]', None)\n\n\n\n", "    def test_parse_annotation(monkeypatch):\n        monkeypatch.setattr(PythonDomain, '_parse_annotation', lambda x: 'x')\n        assert _parse_annotation(\"int\") == \"int\"\n        assert _parse_annotation(\"List[str]\") == \"List[str]\" \n        assert _parse_annotation(\"Optional[int]\") == \"Optional[int]\" \n        assert _parse_annotation(\"Tuple[int, float]\") == \"Tuple[int, float]\" \n        assert _parse_annotation(\"Union[int, str]\") == \"Union[int, str]\" \n\n\n\n", "    def test_parse_annotation_simple(self):\n        annotation = 'int'\n        expected = nodes.annotation(\n            'int',\n            nodes.Text('int'),\n            None\n        )\n        actual = _parse_annotation(annotation, self.env)\n        assert_node(self, expected, actual)\n\n", "    def test_parse_annotation(self):\n        annotation = 'int | str'\n        parsed_annotation = _parse_annotation(annotation, self.env)\n        assert isinstance(parsed_annotation, tuple)\n        assert len(parsed_annotation) == 2\n        assert parsed_annotation[0] == 'int'\n        assert parsed_annotation[1] == 'str'\n\n        annotation = 'Optional[Dict[str, int]]'\n        parsed_annotation = _parse_annotation(annotation, self.env)\n        assert isinstance(parsed_annotation, tuple)\n        assert len(parsed_annotation) == 2\n        assert parsed_annotation[0] == 'Optional'\n        assert parsed_annotation[1] == 'Dict[str, int]'\n\n        annotation = 'List[Union[int, float]]'\n        parsed_annotation = _parse_annotation(annotation, self.env)\n        assert isinstance(parsed_annotation, tuple)\n        assert len(parsed_annotation) == 2\n        assert parsed_annotation[0] == 'List'\n        assert parsed_annotation[1] == 'Union[int, float]'\n\n        annotation = 'Tuple[str, int, float]'\n        parsed_annotation = _parse_annotation(annotation, self.env)\n        assert isinstance(parsed_annotation, tuple)\n        assert len(parsed_annotation) == 2\n        assert parsed_annotation[0] == 'Tuple'\n        assert parsed_annotation[1] == 'str, int, float'\n", "    def test_filter_meta_fields(app: Mock, env: Mock) -> None:\n        domain = Mock()\n        domain.env = env\n        content = nodes.document(\n            [nodes.field_body(nodes.literal(b'meta: data=\"some value\"'), ''))]\n        )\n        PythonDomain._filter_meta_fields(app, domain, 'py', content)\n        assert len(content) == 0\n\n\n\n"], "sample_719": ["    def test_stopwords_after_fit_transform(self):\n        vectorizer = TfidfVectorizer(stop_words='english',\n                                     max_features=10)\n        X_train, y_train = train_test_split(ALL_FOOD_DOCS,\n                                          np.zeros(len(ALL_FOOD_DOCS)))\n        X = vectorizer.fit_transform(X_train)\n        stop_words = vectorizer.stop_words_\n\n        assert 'the' in stop_words\n        assert 'pizza' not in stop_words\n        assert 'burger' not in stop_words\n\n", "    def test_transform_with_custom_analyzer():\n        vectorizer = CountVectorizer(analyzer=lazy_analyze)\n        X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n        assert_array_equal(X.toarray(), np.array([\n            [1] * len(ALL_FOOD_DOCS)\n        ]))\n", "def test_vectorizer_stop_words_from_string():\n    vectorizer = CountVectorizer(stop_words='english')\n    X = vectorizer.fit_transform(['this is the text', 'this text is'])\n    assert_array_equal(vectorizer.vocabulary_, {\n        'text': 0,\n        'is': 1,\n        'the': 2\n    })\n\n    vectorizer = CountVectorizer(stop_words='english')\n    X = vectorizer.fit_transform(['this is the text', 'this text is', 'the other'])\n    assert_array_equal(vectorizer.vocabulary_, {\n        'text': 0,\n        'is': 1,\n        'other': 2\n    })\n\n\n\n", "    def test_idf_with_max_df_and_min_df(self):\n        # Test that idf is calculated correctly when max_df and min_df are used\n        vectorizer = TfidfVectorizer(stop_words='english',\n                                     max_df=0.8, min_df=2)\n        X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n\n        # Check if ids with min_df occurrences are present\n        for term in vectorizer.get_feature_names():\n            if ALL_FOOD_DOCS.count(term) < 2:\n                assert term not in [i for i, j in enumerate(X.getnnz(axis=0)) \n                                   if j > 0]\n        # Check if ids with max_df occurrences are present\n        for term in vectorizer.get_feature_names():\n            if ALL_FOOD_DOCS.count(term) / len(ALL_FOOD_DOCS) >= 0.8:\n                assert term in [i for i, j in enumerate(X.getnnz(axis=0)) \n                                if j > 0]\n", "    def test_fit_transform_with_stop_words_and_max_df(self):\n        stop_words = ['the', 'pizza', 'beer']\n        tfidf = TfidfVectorizer(stop_words=stop_words, max_df=0.8)\n        X = tfidf.fit_transform(ALL_FOOD_DOCS)\n\n        # Check if the stop words are actually removed\n        vocab = tfidf.vocabulary_\n        for word in stop_words:\n            assert_not_equal(word, vocab)\n\n        for doc in ALL_FOOD_DOCS:\n            doc_vector = X[ALL_FOOD_DOCS.index(doc)].toarray()\n            assert_equal(doc_vector.shape[0], 0)\n\n"], "sample_790": ["    def test_kernel_pca_with_precomputed_kernel(self, kernel):\n        X, _ = make_circles(n_samples=100, noise=0.1, factor=0.5)\n        kernel_matrix = rbf_kernel(X, X)\n        \n        pca = KernelPCA(n_components=2, kernel=\"precomputed\",\n                        copy_X=False)\n        pca.fit(kernel_matrix)\n\n        # check that transform uses precomputed kernel\n        transformed_data = pca.transform(X)\n\n        # check if the results are similar to a PCA with the same kernel\n        linear_pca = PCA(n_components=2)\n        linear_pca.fit_transform(X)\n\n        assert_allclose(transformed_data, linear_pca.transform(X)) \n\n", "    def test_kernel_pca_fit_transform_basic(self, kernel):\n        X, _ = make_circles(n_samples=100, noise=0.1, random_state=42)\n        kpca = KernelPCA(n_components=2, kernel=kernel)\n        X_transformed = kpca.fit_transform(X)\n        assert_equal(X_transformed.shape, (100, 2))\n", "    def test_kernel_pca_sparse(self):\n        X, _ = make_circles(n_samples=100, noise=0.1, random_state=0)\n        X_sparse = sp.csr_matrix(X)\n        pca = PCA(n_components=2)\n        kpca = KernelPCA(n_components=2, kernel='linear')\n\n        X_pca = pca.fit_transform(X)\n        X_kpca = kpca.fit_transform(X_sparse)\n\n        assert_allclose(X_pca, X_kpca)\n\n        # Check that KernelPCA handles sparse input correctly\n        with pytest.raises(NotImplementedError):\n            kpca_inv = KernelPCA(n_components=2, kernel='linear',\n                                 fit_inverse_transform=True)\n            kpca_inv.fit_transform(X_sparse)\n\n\n\n", "    def test_fit_inverse_transform_precomputed(self, n_components):\n        X, y = make_circles(n_samples=100, noise=0.1, factor=0.5)\n        K = rbf_kernel(X)\n        pca = KernelPCA(n_components=n_components, kernel='precomputed',\n                       fit_inverse_transform=True)\n        pca.fit(K)\n        X_transformed = pca.transform(K)\n        X_recovered = pca.inverse_transform(X_transformed)\n        assert_allclose(X, X_recovered, rtol=1e-3)\n\n", "    def test_kernel_precomputed_fit_inverse_transform(self):\n        # Test fit_inverse_transform with a precomputed kernel\n        # This should raise a ValueError as it's not supported\n\n        X, y = make_circles(100, random_state=42)\n        kernel_matrix = rbf_kernel(X, X)\n\n        with pytest.raises(ValueError):\n            KernelPCA(fit_inverse_transform=True, kernel='precomputed').fit(\n                kernel_matrix, y)\n\n\n"], "sample_210": ["    def test_redirect_view(self):\n        class RedirectViewTest(RedirectView):\n            url = '/some/url'\n\n        response = RedirectViewTest.as_view()(self.rf.get('/'))\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response['location'], '/some/url')\n\n", "    def test_customizable_view(self):\n        view = CustomizableView()\n        view.parameter = {'key': 'value'}\n        response = view.get(self.rf.get('/'))\n        self.assertEqual(response.content, b'This is a simple view')\n        self.assertIn(b'key' in response.content, b'value')\n", "    def test_about_template_views(self):\n        view = AboutTemplateView.as_view()\n        response = view(self.rf.get('/'))\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, 'about.html')\n\n        view = AboutTemplateAttributeView.as_view()\n        response = view(self.rf.get('/'))\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, 'about.html')\n", "    def test_redirect_view(self):\n        class RedView(RedirectView):\n            permanent = True\n            url = '/about/'\n\n        response = RedView.as_view()(self.rf.get('/'))\n        self.assertRedirects(response, '/about/')\n        self.assertEqual(response.status_code, 301)\n", "    def test_redirect_view(self):\n        class RedirectViewTest(RedirectView):\n            url = 'https://www.example.com'\n        with override_settings(TEMPLATES=[{'BACKEND': 'django.template.backends.django.DjangoTemplates'}],\n                              INSTALLED_APPS=['django.contrib.staticfiles']):\n            view = RedirectViewTest()\n            request = self.rf.get('/')\n            response = view.get(request)\n            self.assertRedirects(response, 'https://www.example.com')\n\n"], "sample_549": ["compilation error", "    def test_delete_masked_points_empty(self):\n        x, y = np.array([1, 2, 3]), np.array([4, 5, 6])\n        mask = np.array([False, False, False])\n        x_out, y_out = delete_masked_points(x, y, mask)\n        assert_array_equal(x_out, x)\n        assert_array_equal(y_out, y)\n", "    def test_delete_masked_points_empty(self):\n        x = np.array([1, 2, 3])\n        y = np.array([4, 5, 6])\n        mask = np.array([True, False, True])\n        deleted_x, deleted_y = delete_masked_points(x, y, mask)\n        assert_array_equal(deleted_x, x)\n        assert_array_equal(deleted_y, y)\n", "compilation error", "    def test_delete_masked_points_zero_mask(self):\n        data = np.array([[1, 2, 3], [4, 5, 6]])\n        mask = np.zeros(data.shape, dtype=bool)\n        result = delete_masked_points(data, mask)\n        assert_array_equal(result, data)\n"], "sample_1139": ["    def test_union_complex_region(self):\n\n        a = Interval(2, 3)\n        b = Interval(4, 5)\n        c = Interval(1, 7)\n        region1 = ComplexRegion(a*b)\n        region2 = ComplexRegion(b*c)\n        union = region1.union(region2)\n        self.assertEqual(union.a_interval, Union(Interval(2, 3), Interval(4, 5)))\n        self.assertEqual(union.b_interval, Union(Interval(4, 5), Interval(1, 7)))\n\n\n", "    def test_complex_region_from_real_with_finite_sets(self):\n        from sympy import Interval, ComplexRegion\n        a = Interval(1, 3)\n        b = FiniteSet(4)\n        region = ComplexRegion.from_real(a * b)\n        expected = FiniteSet(1 + 4*I, 2 + 4*I, 3 + 4*I)\n        self.assertEqual(region, expected)\n\n\n", "    def test_complex_region_from_real_with_finite_set(self):\n        from sympy.sets import FiniteSet\n\n        a = Interval(2, 5)\n        b = FiniteSet(0, 1)\n        c = ComplexRegion.from_real(a * b)\n        self.assertEqual(c.sets, ProductSet(a, b)) # check set\n\n        expected = FiniteSet(2 + 0*I, 2 + 1*I, 3 + 0*I, 3 + 1*I, 4 + 0*I, 4 + 1*I, 5 + 0*I, 5 + 1*I)\n        self.assertEqual(c, FiniteSet(*expected))  # check elements\n", "    def test_ImageSet_rewrite_finite(self):\n        from sympy.functions import acos\n        f = Lambda(x, acos(x))\n        domain = Interval(-1, 1)\n        result = ImageSet(f, domain).rewrite(FiniteSet)\n        expected = FiniteSet(acos(k) for k in [-1, 0, 1])\n        self.assertEqual(result, expected)\n\n\n\n", "    def test_complex_region_from_real(self):\n        from sympy import Interval, ComplexRegion\n        unit = Interval(0,1)\n        cr = ComplexRegion.from_real(unit)\n        self.assertEqual(cr, CartesianComplexRegion(ProductSet(Interval(0, 1), FiniteSet(0))))  \n        with raises(ValueError):\n            ComplexRegion.from_real(Interval(0, 1)*Interval(0, 1))\n\n\n\n\n"], "sample_547": ["    def test_offsetbox_padding(self):\n        fig, ax = plt.subplots()\n        box = OffsetBox(DrawingArea(width=1, height=1))\n        ax.add_artist(box)\n        box.set_child(mpatches.Rectangle((0.1, 0.1), 0.8, 0.8, fill=False))\n        plt.draw()\n", "    def test_offsetbbox_update_offset(self):\n        fig, ax = plt.subplots()\n        bbox = AnchoredOffsetbox(loc='lower right', pad=0.1)\n        bbox.update_frame(bbox)\n\n        bbox.set_offset((0.5, 0.5))\n\n        bbox.stale = True\n        bbox.draw(ax.figure.canvas.get_renderer())\n\n        bbox.set_offset((0.3, 0.3))\n\n        bbox._offset = (0.3, 0.3)\n\n        bbox.draw(ax.figure.canvas.get_renderer())\n\n        bbox.stale = False \n\n\n", "    def test_offsetbox_transformation(self):\n        fig, ax = plt.subplots()\n        bbox = OffsetBox(\n            # Using a simple text for simplicity\n            DrawingArea(1).set_text('Test'),\n            loc='lower left',\n        )\n        bbox.set_transform(ax.transAxes)\n        ax.add_artist(bbox)\n\n        # Should be transformed by axes transform\n        bbox_expected = bbox.get_window_extent(renderer=fig.canvas.get_renderer())\n        bbox_actual = bbox.get_window_extent(renderer=fig.canvas.get_renderer(),\n                                              transform=ax.transData)\n        assert_allclose(bbox_expected.bounds, bbox_actual.bounds)\n", "    def test_offsetbox_bbox(self):\n        fig, ax = plt.subplots()\n\n        bbox = OffsetBox()\n        bbox.add_artist(mpatches.Rectangle((0, 0), 1, 1, facecolor='red'))\n\n        bbox.set_offset((1, 1))\n        bbox.draw(ax.figure._get_renderer())\n        bbox_prop = bbox.get_window_extent(ax.figure._get_renderer())\n        assert_allclose(bbox_prop.bounds, [1, 1, 2, 2])\n", "def test_hpackers():\n    fig, axes = plt.subplots(1, 1)\n\n    hpack = HPacker(\n        children=[\n            mpatches.Rectangle((0, 0), 0.5, 0.5, color='red'),\n            mpatches.Circle((0.5, 0.5), 0.2, color='blue'),\n            mpatches.Ellipse((1, 0.5), 0.5, 0.3, color='green'),\n        ],\n        pad=0.1,\n        sizing='uniform',\n    )\n\n    axes.add_artist(hpack)\n    plt.show()\n\n\n\n"], "sample_394": ["    def test_search_fields_in_admin_site_are_all_valid(self):\n        for model in site._registry.values():\n            if hasattr(model, 'search_fields'):\n                for field in model.search_fields:\n                    try:\n                        model._meta.get_field(field)\n                    except FieldDoesNotExist:\n                        self.fail(f\"Invalid search_field '{field}' defined for model: {model.__name__}\")\n\n\n\n", "    def test_edit_inline_with_post_data(self):\n        response = self.client.get(reverse(\"admin:my_app_article_add\"))\n        response = self.client.post(\n            reverse(\"admin:my_app_article_add\"), self.inline_post_data\n        )\n        # Check that 3 articles were created\n        self.assertEqual(Article.objects.count(), 6)\n        # Check that the new titles are saved correctly\n\n\n\n        # Check that the new articles have the expected content.\n        for i in range(6):\n            article = Article.objects.all()[i]\n            article_title_data = (\n                f\"article_set-{i}-title\"\n            )\n            self.assertEqual(article.title, self.inline_post_data[article_title_data])\n        \n        # Check that article content is saved correctly\n        for article in Article.objects.all():\n            article_content_data = f\"article_set-{article.pk}-content\"\n            self.assertEqual(article.content, self.inline_post_data[article_content_data])\n\n\n\n        # Check that the redirect\n        self.assertRedirects(response, reverse(\"admin:my_app_article_changelist\"))\n\n\n\n", "    def test_admin_edit_inline_form(self):\n        response = self.client.get(reverse('admin:article_edit', args=(self.s1.pk, )))\n        self.assertContains(response, \"Article 1\", count=1)\n        self.assertContains(response, \"Article 2\", count=1)\n        self.assertContains(response, \"Article 3\", count=1)\n        self.assertContains(response, \"New\")\n\n        # Edit and save the inline forms\n        response = self.client.post(\n            reverse('admin:article_edit', args=(self.s1.pk, )),\n            self.inline_post_data,\n        )\n        self.assertRedirects(response, reverse('admin:article_changelist'))\n\n\n", "    def test_add_view(self):\n        response = self.client.get(reverse(\"admin:site2_article_add\"))\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, \"Article 1\")\n", "    def test_inline_formset_with_fields(self):\n        \"\"\"\n        Test that inline formsets can be used with specific fields.\n        \"\"\"\n        # Add some inline objects to create a formset.\n        Article.objects.create(\n            content=\"<p>New content</p>\",\n            date=datetime.datetime(2010, 3, 18, 11, 54, 58),\n            section=self.s1,\n            title=\"Article 4\",\n        )\n        response = self.client.get(reverse(\"admin:blog_article_add\"))\n        self.assertContains(response, \"Article 4\")\n        # Submit the formset with data (including a new article)\n        response = self.client.post(\n            reverse(\"admin:blog_article_add\"), self.inline_post_data, follow=True\n        )\n\n        self.assertRedirects(response, reverse(\"admin:blog_section_change\", args=(self.s1.pk,)))\n        # Load the section again to check the inlined articles\n        section = Section.objects.get(pk=self.s1.pk)\n        self.assertEqual(len(section.article_set.all()), 6)\n        # Check for new article content\n        added_article = section.article_set.all()[5]\n        self.assertEqual(added_article.title, \"Need a title.\")\n\n\n\n"], "sample_1132": ["compilation error", "compilation error", "    def test_kbin_ordered():\n        for k in range(2, 6):\n            for ordered in [None, 0, 1, 10, 11]:\n                for lst in [range(k * 2), list(range(k * 2))[::-1]]:\n                    for v in kbins(lst, k, ordered):\n                        if ordered is not None and ordered != 11:\n                            assert len(v) == len(lst) // k, 'Invalid bin length'\n                        if ordered == 11:\n                            expected = [list(p) for p in multiset_permutations(lst)]\n                            assert set(v) == set(expected)\n                            continue\n                        assert all([len(b) == k // 2 for b in v])\n                        \n", "    def test_kbins_ordered():\n        for ordered_flag in [None, 0, 1, 10, 11]:\n            for sizes in [(1, 2, 3), (2, 2, 2), (3, 3)] :\n                for k in range(1, len(sizes) + 1):\n                    for combo in kbins(sizes, k, ordered=ordered_flag):\n                        if ordered_flag is not None:\n                            assert len(combo) == k\n                        for p in partitions(len(sizes), k, size=True):\n                            if ordered_flag is not None:\n                                assert set(combo) == set(p)\n                            else:\n                                assert all(len(c) > 0 for c in combo)\n                                assert sum(len(c) for c in combo) == len(sizes)\n\n\n", "    def test_partitions():\n        assert partitions(5, 4) == [\n                (1, 1, 1, 2), (1, 1, 2, 1), (1, 2, 1, 1),\n                (1, 2, 2), (2, 1, 1, 1), (2, 2, 1)\n            ]\n\n        assert partitions(5, 3) == [\n                (1, 1, 3), (1, 2, 2), (1, 3, 1), (2, 1, 2),\n                (2, 2, 1), (3, 1, 1)\n            ]\n\n        assert partitions(5, 2) == [\n                (1, 4), (2, 3), (3, 2), (4, 1)\n            ]\n\n        assert partitions(5, 1) == [(1, 1, 1, 1, 1)]\n\n        assert partitions(5, 0) == [(5)]\n\n\n\n"], "sample_1000": ["    def test_matrix_product():\n        m = MatrixSymbol(\"M\", 2, 2)\n        n = MatrixSymbol(\"N\", 2, 2)\n        assert octave_code(m*n) == \"M.*N\"", "compilation error", "    def test_nested_binary_math_functions():\n        from sympy.functions import floor, log\n        assert octave_code(floor(log(x))) == 'floor(log(x))'\n        assert octave_code(max(x, y)) == 'max(x, y)'\n        assert octave_code(arg(x + 1j * y)) == 'angle(x + 1i*y)'", "compilation error", "    def test_sparse_matrix(self):\n        A = SparseMatrix([[1, 2, 0], [0, 3, 4], [5, 0, 6]],\n                         shape=(3, 3))\n        code = octave_code(A)\n        self.assertEqual(code, \"sparse([1, 2, 0], [0, 1, 2], [1, 2, 3], 3, 3)\")\n\n\n\n"], "sample_451": ["    def test_parse_docstring(self):\n        title, body, metadata = parse_docstring(self.docstring)\n        self.assertEqual(title, \"Display an individual :model:`myapp.MyModel`.\")\n        self.assertEqual(\n            body,\n            \"**Context**\\n\\n``RequestContext``\\n\\n``mymodel``\\n        An instance of :model:`myapp.MyModel`.\",\n        )\n        self.assertEqual(metadata, {\"some_metadata\": \"some data\"})\n\n\n\n", "    def test_parse_docstring(self):\n        title, body, metadata = parse_docstring(self.docstring)\n        self.assertEqual(title, \"This __doc__ output is required for testing. I copied this example from admindocs documentation.\")\n        self.assertEqual(\n            body,\n            \"Display an individual :model:`myapp.MyModel`. \\n\\n**Context**\\n\\n``RequestContext``\\n\\n``mymodel``\\n        An instance of :model:`myapp.MyModel`. \\n\\n**Template:**\\n\\n:template:`myapp/my_template.html` \",\n        )\n        self.assertEqual(metadata, {\"some_metadata\": \"some data\"})\n\n\n\n", "    def test_parse_docstring(self):\n        title, body, metadata = parse_docstring(self.docstring)\n        self.assertEqual(title, \"Display an individual :model:`myapp.MyModel`.\")\n        self.assertEqual(\n            body,\n            \"\"\"", "    def test_parse_docstring(self):\n        title, body, metadata = parse_docstring(self.docstring)\n        self.assertEqual(title, \"Display an individual :model:`myapp.MyModel`.\")\n        self.assertIn(\"**Context**\\n\\n``RequestContext``\", body)\n        self.assertIn(\"``mymodel``\\n        An instance of :model:`myapp.MyModel`. \", body)\n        self.assertEqual(metadata, {'some_metadata': 'some data'})\n", "    def test_parse_docstring(self):\n        title, body, metadata = parse_docstring(self.docstring)\n        self.assertEqual(title, \"This __doc__ output is required for testing.\")\n        self.assertEqual(\n            body,\n            \"\"\"Display an individual :model:`myapp.MyModel`.\n"], "sample_496": ["    def test_admin_scripts_with_custom_files(self):\n        # Test that django-admin works with custom settings and files.\n\n        with mock.patch('django.conf.settings', new_callable=setattr):\n            custom_env = {\n                'DJANGO_SETTINGS_MODULE': 'admin_scripts.test_project.settings',\n                'PYTHONPATH': os.path.join(self.test_dir, 'templates')\n            }\n            self.write_settings('settings.py', apps=['django.contrib.auth', 'admin_scripts.test_app'])\n\n            # Define an invalid settings file to test error handling\n            with open(os.path.join(self.test_dir, 'settings.py'), 'a') as f:\n                f.write(\"\\nINVALID_SETTING = 'somevalue'\")\n\n            with mock.patch('django.core.management.call_command', return_value=None) as mock_call_command:\n                try:\n                    subprocess.run(['python', '-m', 'django-admin', 'startproject', 'test_project'],\n                                   env=custom_env,\n                                   cwd=self.test_dir,\n                                   stdout=StringIO(),\n                                   stderr=StringIO())\n                except subprocess.CalledProcessError as e:\n                    self.assertEqual(e.returncode, 1, 'Django-admin should have raised an error.')\n                    self.assertIn(\"INVALID_SETTING\", e.stderr.decode(), 'Error message should contain the invalid setting')\n\n            # Assert that the mock was called with the correct arguments\n            mock_call_command.assert_called_once_with('startproject', 'test_project', settings=custom_env['DJANGO_SETTINGS_MODULE'])\n", "    def test_handle(self):\n        self.write_settings('settings.py')\n        # Create a test command\n        class TestCommand(BaseCommand):\n                return 'Test Command Executed'\n        # Execute the test command with manage.py\n        with mock.patch('sys.stdout', new_callable=StringIO) as stdout:\n            with mock.patch('django.core.management.get_commands') as get_commands:\n                get_commands.return_value = {'test_command': TestCommand()}\n                subprocess.run(['manage.py', 'test_command'], cwd=self.test_dir)\n        stdout_output = stdout.getvalue()\n\n        self.assertIn('Test Command Executed', stdout_output)\n", "    def test_invalid_db_config(self):\n        self.write_settings('settings.py',\n                           DATABASES={'default': {'ENGINE': 'bogus.DoesNotExist'}})\n        with self.assertRaises(ImproperlyConfigured) as context:\n            call_command('makemigrations', 'myapp')\n        self.assertIn('Could not load Database configuration', str(context.exception))\n        \n        self.remove_settings('settings.py')\n", "    def test_invalid_settings_module(self):\n        # Test the behavior of the Django management commands when\n        # DJANGO_SETTINGS_MODULE points to a non-existent module.\n        self.write_settings('settings.py')\n\n        try:\n            call_command('runserver')\n        except ImportError as e:\n            self.assertIn(\"django.conf.settings\", str(e))\n        except CommandError as e:\n            self.assertIn(\"Error loading settings module\", str(e))\n\n        # Ensure that the 'settings.py' file is still present\n        self.assertTrue(os.path.exists(os.path.join(self.test_dir, 'settings.py')))\n\n\n\n        try:\n            call_command('loaddata', 'auth.json')\n        except ImportError as e:\n            self.assertIn(\"django.conf.settings\", str(e))\n        except CommandError as e:\n            self.assertIn(\"Error loading settings module\", str(e))\n\n        # Ensure that the 'settings.py' file is still present\n        self.assertTrue(os.path.exists(os.path.join(self.test_dir, 'settings.py')))\n\n\n\n        try:\n            call_command('testserver')\n        except ImportError as e:\n            self.assertIn(\"django.conf.settings\", str(e))\n        except CommandError as e:\n            self.assertIn(\"Error loading settings module\", str(e))\n        \n\n        # Ensure that the 'settings.py' file is still present\n        self.assertTrue(os.path.exists(os.path.join(self.test_dir, 'settings.py')))\n\n\n\n", "    def test_custom_settings_file(self):\n        self.write_settings('settings.py', apps=['my_app'], sdict={'MY_CUSTOM_VALUE': 'my_value'})\n\n        # Run a management command that reads from settings.py\n        process = subprocess.run(\n            [sys.executable, 'manage.py', 'runserver', '0.0.0.0:8000'],\n            cwd=self.test_dir,\n            capture_output=True,\n            text=True,\n        )\n\n        self.assertIn(\n            f'Starting development server at http://0.0.0.0:8000/',\n            process.stdout,\n        )\n        self.assertEqual(process.returncode, 0)\n\n        self.remove_settings('settings.py')\n"], "sample_188": ["    def test_ref_expression(self):\n        q = Q(name='Example Inc.')\n        result = Company.objects.filter(q).values_list(\"name\", flat=True)\n        self.assertEqual(list(result), ['Example Inc.'])\n\n        # Test nested ref expressions\n        nested_q = Q(name='Example Inc.')\n        ref_expr = Ref('nested_q', nested_q)\n        result = Company.objects.filter(ref_expr).values_list(\"name\", flat=True)\n        self.assertEqual(list(result), ['Example Inc.'])\n\n        # Test ref expressions with aliases\n        alias = 'company_name'\n        ref_expr = Ref(alias, q)\n        result = Company.objects.annotate(company_name=ref_expr).values_list(\n            'company_name',\n        )\n        self.assertEqual(list(result), [('Example Inc.',)])\n\n        # Test ref expressions with source field\n        alias = Company.objects.order_by('name').values_list('name', flat=True)\n        ref_expr = Ref('alias', alias)\n        result = Company.objects.annotate(alias=ref_expr).values_list(\n            'alias',\n        )\n        self.assertEqual(list(result), [('Example Inc.',)])\n", "    def test_F_expression(self):\n        # Test the F expression, which accesses a field directly\n        self.max = Employee.objects.get(firstname='Max', lastname='Mustermann')\n        f_field = F('salary')  \n        self.assertEqual(f_field.output_field, IntegerField())\n        self.assertEqual(f_field.get_source_expressions(), [\n            {'model': Employee, 'field': 'salary'}\n        ])\n        with self.assertRaises(FieldError):\n            f_field.value  # F expressions should not be directly evaluated\n\n        self.assertEqual(f_field.as_sql(\n            compiler=None, connection=connection\n        ), 'salary')\n", "    def test_order_by_with_expression(self):\n        # Test that OrderBy with an expression works\n        query = Company.objects.values(\"name\").order_by(\n            OrderBy(ExpressionWrapper(F(\"num_employees\"), output_field=IntegerField()), descending=True)\n        )\n        self.assertEqual(list(query), [\n            {\"name\": \"Test GmbH\"},\n            {\"name\": \"Example Inc.\"},\n            {\"name\": \"Foobar Ltd.\"},\n        ])\n\n\n\n", "    def test_raw_sql(self):\n        sql = 'SELECT name, num_employees FROM companies WHERE id = 1'\n        raw_sql = RawSQL(sql, ['1'])\n        result = raw_sql.resolve_expressions(self.company_query, allow_joins=True)\n        self.assertEqual(result._source_expressions, [RawSQL(sql, ['1'])])\n        self.assertEqual(result.as_sql(compiler=connection.compiler, connection=connection)[0], sql)\n\n\n\n", "    def test_expression_list_ordering(self):\n        ordering_expressions = [\n            OrderBy(ExpressionList(Company.objects.values_list(\"name\", flat=True), 'name'), descending=True),\n            OrderBy(ExpressionList(Company.objects.values_list(\"num_employees\", flat=True), 'num_employees'), descending=False),\n        ]\n        sql, params = ordering_expressions[0].as_sql(compiler=None, connection=connection)\n        self.assertIn('ORDER BY name DESC', sql)\n        sql, params = ordering_expressions[1].as_sql(compiler=None, connection=connection)\n        self.assertIn('ORDER BY num_employees ASC', sql)\n\n"], "sample_1157": ["    def test_implicit_multiplication_application_with_function_call(self):\n        expr = parse_expr(\"(f(x) * y)\", transformations=(implicit_multiplication_application,))\n        assert expr == Mul(Function('f')(Symbol('x')), Symbol('y'))\n", "    def test_factorial_notation_nested(self):\n        s = '2*((x+1)!)'\n        expr = parse_expr(s)\n        self.assertEqual(expr, Mul(2, factorial(x + 1)))\n        \n        s = '((x+y)!)/3'\n        expr = parse_expr(s)\n        self.assertEqual(expr, Rational(factorial(x + y), 3))\n\n\n\n", "    def test_implicit_multiplication_application_complex():\n        expr = '1*2*3*4*5*x'\n        result = parse_expr(expr, implicit_multiplication_application)\n        expected = Mul(1, 2, 3, 4, 5, x)\n        assert result == expected\n", "    def test_convert_xor(self):\n        eq = parse_expr('1^2')\n        assert eq == Pow(1, 2)\n\n", "    def test_implicit_multiplication_function_args(self):\n        expr = \"sin(2*x)\"\n        parsed_expr = parse_expr(expr, transformations=\n        (implicit_multiplication_application,))\n        self.assertEqual(parsed_expr, sin(2*x))\n"], "sample_145": ["    def test_prepopulated_fields_valid(self):\n        class PrepopulatedInline(TabularInline):\n            model = Song\n            extra = 0\n            prepopulated_fields = {\n                'artist': ('band',),\n            }\n\n        class PrepopulatedAdmin(ModelAdmin):\n            inlines = [PrepopulatedInline]\n\n        self.assertIsValid(PrepopulatedAdmin, Band())\n", "    def test_check_readonly_fields(self):\n        class RawIdReadOnlyAdmin(ModelAdmin):\n            readonly_fields = ('id', 'extra_field')\n\n        self.assertIsInvalid(\n            RawIdReadOnlyAdmin(ValidationTestModel, AdminSite()),\n            ValidationTestModel,\n            \"The value of 'readonly_fields' refers to 'id', which is not a callable, an attribute of '%s.%s', or an attribute or method on '%s.%s'.\" % (\n                ValidationTestModel._meta.app_label, ValidationTestModel._meta.object_name,\n                ValidationTestModel._meta.app_label, ValidationTestModel._meta.object_name\n            ),\n            id='admin.E108'\n        )\n\n        class GoodReadOnlyAdmin(ModelAdmin):\n            readonly_fields = ('some_value',)\n\n        self.assertIsInvalid(\n            GoodReadOnlyAdmin(ValidationTestModel, AdminSite()),\n            ValidationTestModel,\n            \"The value of 'some_value' refers to 'some_value', which is not an attribute of '%s.%s'.\" % (\n                ValidationTestModel._meta.app_label, ValidationTestModel._meta.object_name\n            ),\n            id='admin.E108'\n        )\n\n\n", "    def test_raw_id_fields(self):\n        class RawIdAdmin(ModelAdmin):\n            raw_id_fields = ['id']\n\n        self.assertIsInvalid(RawIdAdmin(ValidationTestModel, AdminSite()),\n                            ValidationTestModel,\n                            \"The value of 'raw_id_fields' must not contain 'id'.\")\n\n\n\n", "    def test_raw_id_fields(self):\n        class RawIdModel(Model):\n            id = models.AutoField(primary_key=True)\n            name = models.CharField(max_length=100)\n            raw_id = models.CharField(max_length=100, unique=True)\n\n        class RawIdAdmin(ModelAdmin):\n            raw_id_fields = ['raw_id']\n\n        self.assertIsInvalid(RawIdAdmin(RawIdModel, AdminSite()), RawIdModel,\n                            \"The value of 'raw_id_fields' is not allowed for \"\n                            \"models without a ForeignKey or OneToOneField with \"\n                            \"'raw_id_fields=False'.\",\n                            id='admin.E213')\n\n", "    def test_check_raw_id_fields(self):\n        class RawIdModelAdmin(ModelAdmin):\n            raw_id_fields = ('name',)\n\n        self.assertIsInvalid(\n            RawIdModelAdmin(User, AdminSite()),\n            User,\n            \"The value of 'raw_id_fields' must not contain fields that \"\n            \"are not related to the current model.\",\n            id='admin.E001',\n        )\n\n\n"], "sample_205": ["    def test_update_error_dict(self):\n        error_dict = {}\n        validation_error = ValidationError(\"This is an error\", params={\"key\": \"value\"})\n\n        validation_error.update_error_dict(error_dict)\n\n        self.assertEqual(error_dict, {\" This is an error\": [validation_error]})\n", "    def test_update_error_dict(self):\n        error_dict = {}\n        error1 = ValidationError(\"error 1\")\n        error2 = ValidationError(\"error 2\")\n        error3 = ValidationError(\"error 3\", params={'a': 1, 'b': 2})\n\n        validation_error = ValidationError({\n            'field1': [error1, error2],\n            'field2': [],\n            'field3': error3,\n        })\n\n        validation_error.update_error_dict(error_dict)\n        expected_error_dict = {\n            'field1': [error1, error2],\n            'field2': [],\n            'field3': error3,\n        }\n        self.assertEqual(error_dict, expected_error_dict)\n", "    def test_validation_error_update_error_dict(self):\n        error_dict = {}\n        error_dict1 = {'field1': ['error1'], 'field2': ['error2']}\n        ValidationError(error_dict1).update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['error1'], 'field2': ['error2']})\n", "    def test_unicode_representation(self):\n        error = ValidationError('This field is required.')\n        self.assertEqual(str(error), 'This field is required.')\n", "    def test_validation_error_message(self):\n        error = ValidationError(\"This field is required.\")\n        self.assertEqual(error.message, \"This field is required.\")\n\n        error = ValidationError([\n            ValidationError(\"This field is required.\"),\n            ValidationError(\"This field has an invalid value.\")\n        ])\n        self.assertEqual(list(error), [\n            \"This field is required.\",\n            \"This field has an invalid value.\"\n        ])\n\n        error = ValidationError({\n            \"field1\": [\"This field is required.\"],\n            \"field2\": [\"This field has an invalid value.\"]\n        })\n        self.assertEqual(error.error_dict, {\n            \"field1\": [\"This field is required.\"],\n            \"field2\": [\"This field has an invalid value.\"]\n        })\n"], "sample_494": ["    def test_serialize_date(self):\n        now = datetime.date.today()\n        date_serializer = serializer_factory(now)\n        serialized_date, imports = date_serializer.serialize()\n        self.assertEqual(serialized_date, repr(now))\n        self.assertEqual(imports, {\"import datetime\"})\n\n", "    def test_serialize_settings_reference(self):\n        settings.MY_SETTING = \"some_value\"\n        serializer = Serializer.serializer_factory(SettingsReference(\"MY_SETTING\"))\n        self.assertEqual(serializer.serialize(), (\"settings.MY_SETTING\", {\"from django.conf import settings\"}))\n", "    def test_serialize_operation_with_custom_operation(self):\n        CustomOperation = custom_migration_operations.more_operations.CustomOperation\n        operation = CustomOperation(\n            arguments=[\"test_argument\"],\n            reversible=False,\n        )\n        writer = OperationWriter(operation, indentation=0)\n        expected_output = \"\"\"\n            CustomOperation(arguments=['test_argument'])\n        \"\"\"\n        self.assertEqual(writer.serialize(), expected_output)\n", "    def test_serialize_functools_partial(self):\n            pass\n        partial = functools.partial(func, 1, 2, 3)\n        serialized = OperationWriter(\n            custom_migration_operations.operations.AddOperation(\n                model=TestModel1,\n                field=models.CharField(),\n            ),\n            indentation=0\n        ).serialize()\n        self.assertIn(\"functools.partial(func, 1, 2, 3)\", serialized)\n", "    def test_serialize_operation(self):\n        operation = custom_migration_operations.more_operations.CreateIndexOperation(\n            model=TestModel1,\n            fields=['thing'],\n            name=\"my_index\"\n        )\n        writer = OperationWriter(operation, indentation=0)\n        result = writer.serialize()\n        self.assertEqual(result.strip(), \"CreateIndex('my_index', 'TestModel1', ['thing'])\")\n\n"], "sample_509": ["    def test_MicrosecondLocator_epoch_warning(self):\n        with rc_context({'date.converter': 'auto',\n                          'date.interval_multiples': False}):\n            locator = mdates.MicrosecondLocator(interval=100)\n\n            with pytest.warns(RuntimeWarning,\n                              match=r\"Plotting microsecond time intervals for dates far from the epoch\"):\n                # Using a date far from the epoch to trigger the warning\n                dt = datetime.datetime(10000, 1, 1)  # Year 10000\n                ticks = locator(dt)\n                \n                \n\n", "    def test_epoch_2_num(self):\n        # Test that epoch2num correctly converts UNIX time\n        # to days since Matplotlib epoch for different time values.\n        t0 = datetime.datetime(1970, 1, 1, 0, 0, 0, tzinfo=dateutil.tz.tzutc())\n        t1 = datetime.datetime(1970, 1, 2, 0, 0, 0, tzinfo=dateutil.tz.tzutc())\n        t2 = datetime.datetime(2000, 1, 1, 0, 0, 0, tzinfo=dateutil.tz.tzutc())\n\n        expected_0 = 0\n        expected_1 = 1 / SEC_PER_DAY\n        expected_2 = (datetime.datetime(2000, 1, 1) -\n                      datetime.datetime(1970, 1, 1)).days\n\n        assert np.isclose(epoch2num(t0.timestamp()), expected_0)\n        assert np.isclose(epoch2num(t1.timestamp()), expected_1)\n        assert np.isclose(epoch2num(t2.timestamp()), expected_2)\n", "def test_autodate_interval_multiples():\n    with rc_context(\n        {'date.converter': 'auto', 'date.interval_multiples': True},\n    ):\n        locator = mdates.AutoDateLocator()\n        # Test with a range where multiples are meaningful\n        dmin = datetime.date(2023, 1, 1)\n        dmax = datetime.date(2023, 1, 11)\n        ticks = locator.tick_values(dmin, dmax)\n        assert ticks[0] == dmin\n        assert ticks[-1] == dmax\n        assert all(np.diff(ticks) == relativedelta(days=3))\n\n    with rc_context(\n        {'date.converter': 'auto', 'date.interval_multiples': False},\n    ):\n        locator = mdates.AutoDateLocator()\n        # Test with a range where multiples are not as meaningful\n        dmin = datetime.date(2023, 1, 1)\n        dmax = datetime.date(2023, 1, 11)\n        ticks = locator.tick_values(dmin, dmax)\n        # Now the ticks should be more granular\n        assert all(np.diff(ticks) <= relativedelta(days=1))\n\n", "    def test_auto_locator_with_timezone(self, tz):\n        with rc_context(tz=tz):\n            locator = mdates.AutoDateLocator()\n            dates = [datetime.date(2023, 1, 1),\n                     datetime.datetime(2023, 1, 15, 12, 30),\n                     datetime.date(2023, 2, 28)]\n            ticks = locator(dates)\n            assert isinstance(ticks, np.ndarray)\n\n\n\n", "    def test_epoch2num_and_num2epoch(self, tz):\n        dt0 = datetime.datetime(2000, 1, 1, 0, 0, 0, tzinfo=tz)\n        dt1 = datetime.datetime(2001, 1, 1, 0, 0, 0, tzinfo=tz)\n        epoch_val = dt0.timestamp()\n        num_val = mdates.epoch2num(dt0)\n        assert np.allclose(epoch_val, num_val * mdates.SEC_PER_DAY)\n        assert mdates.num2epoch(num_val) == epoch_val\n"], "sample_766": ["    def test_dictionary_learning_alpha(self):\n        # Test the effect of alpha parameter on dictionary learning\n        n_components = 3\n        n_iter = 50\n\n        for alpha in [0.01, 0.1, 1, 10]:\n            dl = DictionaryLearning(n_components=n_components,\n                                     alpha=alpha, n_iter=n_iter\n                                     )\n            dl.fit(X)\n            # Check that components are sparser with higher alpha\n            for i in range(n_components):\n                assert_less(np.mean(abs(dl.components_[i])), 1 / alpha,\n                            msg=\"Component {} not sparse enough with alpha={}\".format(\n                                i, alpha))\n", "    def test_dict_learning_n_components_validation(self):\n        with pytest.raises(ValueError):\n            DictionaryLearning(n_components=None, n_features=n_features)\n        with pytest.raises(ValueError):\n            DictionaryLearning(n_components=n_features + 1)\n\n", "    def test_mini_batch_dictionary_learning_partial_fit(self):\n        model = MiniBatchDictionaryLearning(n_components=3, alpha=1,\n                                            n_iterations=10, batch_size=2,\n                                            shuffle=True)\n        model.fit(X)\n        U1, (A1, B1) = model.inner_stats_\n        \n        # Fit partial batches\n        model.partial_fit(X[:5])\n        U2, (A2, B2) = model.inner_stats_\n        assert_array_almost_equal(U1, U2, rtol=1e-3)\n        assert_array_almost_equal(A1, A2, rtol=1e-3)\n        assert_array_almost_equal(B1, B2, rtol=1e-3)\n\n        model.partial_fit(X[5:10], iter_offset=1)  \n        U3, (A3, B3) = model.inner_stats_\n        assert_array_almost_equal(U2, U3, rtol=1e-3)\n        assert_array_almost_equal(A2, A3, rtol=1e-3)\n        assert_array_almost_equal(B2, B3, rtol=1e-3)\n", "    def test_fit_transform(self):\n        dl = DictionaryLearning(n_components=3, random_state=0)\n        X_train = rng_global.randn(100, 8)\n        dl.fit(X_train)\n        X_transform = SparseCoder(dl.components_, transform_algorithm='omp').fit_transform(X_train)\n        X_reconstructed = dl.transform(X_transform)\n        assert_array_almost_equal(X_reconstructed, X_train, atol=1e-3)\n\n", "    def test_missing_n_components(self):\n        with pytest.raises(ValueError):\n            DictionaryLearning(alpha=1, n_iter=1000, fit_algorithm='lars')\n"], "sample_220": ["    def test_set_cookie_with_expires(self):\n        response = HttpResponse()\n        now = datetime.utcnow().replace(tzinfo=utc)\n        # Set an expires date in the future.\n        response.set_cookie(\"my_cookie\", \"value\", expires=now + timedelta(days=1))\n        self.assertIn(\"set-cookie\", response.headers)\n        cookie = cookies.SimpleCookie()\n        cookie.load(response.headers[\"set-cookie\"])\n        self.assertEqual(cookie[\"my_cookie\"].value, \"value\")\n        self.assertEqual(cookie[\"my_cookie\"].expires, http_date(\n            (now + timedelta(days=1)).timestamp()))\n\n        # Set an expires date as a string.\n        response = HttpResponse()\n        expires_str = \"Mon, 01 Jan 2024 00:00:00 GMT\"\n        response.set_cookie(\"my_cookie\", \"value\", expires=expires_str)\n        self.assertIn(\"set-cookie\", response.headers)\n        cookie = cookies.SimpleCookie()\n        cookie.load(response.headers[\"set-cookie\"])\n        self.assertEqual(cookie[\"my_cookie\"].value, \"value\")\n        self.assertEqual(cookie[\"my_cookie\"].expires, expires_str)\n\n        # Set an expires date with max_age.\n        response = HttpResponse()\n        response.set_cookie(\"my_cookie\", \"value\", max_age=3600)\n        self.assertIn(\"set-cookie\", response.headers)\n        cookie = cookies.SimpleCookie()\n        cookie.load(response.headers[\"set-cookie\"])\n        self.assertEqual(cookie[\"my_cookie\"].value, \"value\")\n        self.assertIsNone(cookie[\"my_cookie\"].expires)\n        # Ensure we have max_age set correctly.\n        self.assertEqual(cookie[\"my_cookie\"][\"max-age\"], \"3600\")\n\n        # Test setting a timedelta value for expires when max_age is also set.\n        response = HttpResponse()\n        with freeze_time(\"2023-10-27 10:00:00\"):\n            response.set_cookie(\"my_cookie\", \"value\",\n                               ", "    def test_set_cookie_with_datetime(self):\n        response = HttpResponse()\n        with freeze_time(datetime.datetime(2023, 10, 26, 10, 0, 0, tzinfo=utc)):\n            response.set_cookie('test_cookie', 'value', expires=datetime.datetime(2023, 10, 27, 10, 0, 0, tzinfo=utc))\n        self.assertEqual(response.cookies['test_cookie']['expires'], http_date(1703744400))\n\n", "    def test_set_cookie_with_samesite_lax(self):\n        response = HttpResponse()\n        response.set_cookie(\n            key='test_cookie', value='test_value', samesite='lax',\n        )\n        cookie = cookies.SimpleCookie()\n        cookie.load(response.headers['Set-Cookie'])\n        self.assertEqual(cookie['test_cookie'].value, 'test_value')\n        self.assertEqual(cookie['test_cookie']['samesite'], 'lax')\n", "    def test_set_cookie_with_timedelta(self):\n        response = HttpResponse()\n        with freeze_time(datetime.utcnow()):\n            expires = datetime.utcnow() + timedelta(days=3)\n            response.set_cookie('cookie_name', 'cookie_value', expires=expires)\n        cookie = response.cookies['cookie_name']\n        self.assertEqual(cookie.value, 'cookie_value')\n        self.assertEqual(cookie['expires'], http_date(expires.timestamp()))\n", "    def test_set_cookie_samesite_validation(self):\n        response = HttpResponse()\n        with self.assertRaisesMessage(ValueError, 'samesite must be \"lax\", \"none\", or \"strict\".'):\n            response.set_cookie('test', value='value', samesite='invalid')\n\n        response = HttpResponse()\n        response.set_cookie('test', value='value', samesite='lax')\n        self.assertEqual(response.cookies['test']['samesite'], 'lax')\n\n        response = HttpResponse()\n        response.set_cookie('test', value='value', samesite='none')\n        self.assertEqual(response.cookies['test']['samesite'], 'none')\n\n        response = HttpResponse()\n        response.set_cookie('test', value='value', samesite='strict')\n        self.assertEqual(response.cookies['test']['samesite'], 'strict')\n\n\n\n"], "sample_826": ["    def test_onehotencoder_fit_transform_int_categorical_features(\n            self):\n        X = np.array([[1, 2], [1, 0], [0, 2]])\n        encoder = OneHotEncoder(handle_unknown='ignore', n_values='auto')\n        x = encoder.fit_transform(X)\n        expected_output = sparse.csr_matrix(\n            [[1, 0, 1, 0],\n             [1, 0, 0, 1],\n             [0, 1, 1, 0]])\n        assert_allclose(toarray(x), toarray(expected_output))\n\n\n", "    def test_onedhotencoder_fit_transform_with_n_values(self, categories,\n                                                     handle_unknown):\n        enc = OneHotEncoder(categories=categories,\n                            handle_unknown=handle_unknown)\n        X = np.array([[0, 1, 2], [1, 2, 0], [0, 1, 1]])\n        enc.fit(X)\n        enc_fitted = OneHotEncoder(n_values=3, categories=categories,\n                                    handle_unknown=handle_unknown)\n        enc_fitted.fit(X)\n        assert_allclose(enc.transform(X), enc_fitted.transform(X))\n        assert_allclose(enc.inverse_transform(enc.transform(X)), X)\n", "    def test_one_hot_encoder_n_values(self):\n        n_samples = 10\n        n_features = 3\n        X = np.random.randint(0, 10, size=(n_samples, n_features))\n        enc = OneHotEncoder(n_values=['auto', 3, 5])\n        with assert_warns(DeprecationWarning,\n                          match=r\"The 'n_values' argument is deprecated\"):\n            enc.fit(X)\n\n        X_encoded = enc.transform(X)\n        assert X_encoded.shape[1] == 10 + 3 + 5  # Total unique values\n\n\n\n        # Test with mixed 'auto' and explicit n_values\n        enc = OneHotEncoder(n_values={'auto': 'auto', 1: 3, 2: 5})\n        with assert_warns(DeprecationWarning,\n                          match=r\"The 'n_values' argument is deprecated\"):\n            enc.fit(X)\n\n        X_encoded = enc.transform(X)\n        assert X_encoded.shape[1] == 10 + 3 + 5  # Total unique values\n\n\n", "    def test_handle_unknown_drop(self):\n        X = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\n        enc = OneHotEncoder(handle_unknown='ignore', drop=True, n_values=[None, None, None])\n        enc.fit(X)\n\n        X_transformed = enc.transform(X)\n        assert X_transformed.shape[1] == 6 \n        assert np.sum(X_transformed.getnnz(axis=1)) == 6\n        assert np.any(X_transformed.toarray() == 0)\n        \n", "    def test_handle_unknown_error(self):\n        enc = OneHotEncoder(handle_unknown='error', drop='first')\n        X = np.array([[0], [1], [2], [3], [4]])\n        with assert_raises(ValueError):\n            enc.fit(X)"], "sample_413": ["    def test_check_setting_app_dirs_loaders_no_issues(self):\n        with override_settings(TEMPLATES=[\n            {\n                \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n                \"APP_DIRS\": False,\n                \"OPTIONS\": {}\n            },\n        ]):\n            errors = check_setting_app_dirs_loaders(None, **{})\n            self.assertEqual(errors, [])\n", "        def test_check_setting_app_dirs_loaders(self):\n            with self.assertLogs(\"django.core.checks\", level=\"ERROR\") as logs:\n                errors = check_setting_app_dirs_loaders(None, TEMPLATES=self.TEMPLATES_APP_DIRS_AND_LOADERS)\n            self.assertEqual(len(errors), 1)\n            self.assertIn(\n                \"You have 'APP_DIRS': True in your TEMPLATES but also specify 'loaders' in OPTIONS. Either remove APP_DIRS or remove the 'loaders' option.\",\n                logs.output[0],\n            )\n", "        def test_check_setting_app_dirs_loaders(self):\n            errors = check_setting_app_dirs_loaders(None, **{})\n            self.assertEqual(errors, [E001])\n", "        def test_check_setting_app_dirs_loaders(self):\n            with self.subTest(\"AppDirs and loaders combination\"):\n                errors = check_setting_app_dirs_loaders(None, **{})\n                self.assertEqual(errors, [E001])\n", "    def test_check_setting_app_dirs_loaders_with_loaders_only(self):\n        self.assertEqual(\n            check_setting_app_dirs_loaders(None),\n            [Error(id=\"templates.E001\", msg=\"You have 'APP_DIRS': True in your TEMPLATES but also specify 'loaders' in OPTIONS. Either remove APP_DIRS or remove the 'loaders' option.\")],\n        )\n"], "sample_137": ["    def test_parse_docstring(self):\n        title, body, metadata = parse_docstring(self.docstring)\n        self.assertEqual(title, \"This __doc__ output is required for testing. I copied this example from admindocs documentation. (TITLE)\")\n        self.assertEqual(body, \"Display an individual :model:`myapp.MyModel`. \\n\\n**Context**\\n\\n``RequestContext``\\n\\n``mymodel``\\n        An instance of :model:`myapp.MyModel`. \\n\\n**Template:**\\n\\n:template:`myapp/my_template.html` (DESCRIPTION)\")\n        self.assertEqual(metadata, {'some_metadata': 'some data'})\n", "    def test_parse_docstring(self):\n        title, body, metadata = parse_docstring(self.docstring)\n        self.assertEqual(title, \"Display an individual :model:`myapp.MyModel`.\")\n        self.assertEqual(\n            body,\n            \"\"\"", "def test_replace_named_groups(self):\n    pattern = r\"^(?P<a>\\w+)/b/(\\w+)$\"\n    self.assertEqual(replace_named_groups(pattern), r\"^<a>/b/<var>$\")\n    pattern = r\"^(?P<a>\\w+)/b/(.+)$\"\n    self.assertEqual(replace_named_groups(pattern), r\"^<a>/b/<var>$\")\n    pattern = r\"^(.+)/b/(?P<c>\\w+)$\"\n    self.assertEqual(replace_named_groups(pattern), r\"^<var>/b/<c>$\")\n\n\n\n", "    def test_parse_docstring_empty(self):\n        docstring = ''\n        title, body, metadata = parse_docstring(docstring)\n        self.assertEqual(title, '')\n        self.assertEqual(body, '')\n        self.assertEqual(metadata, {})\n", "    def test_parse_docstring(self):\n        title, body, metadata = parse_docstring(self.docstring)\n        self.assertEqual(title, \"This __doc__ output is required for testing. I copied this example from admindocs documentation. (TITLE)\")\n        self.assertEqual(\n            body,\n            \"Display an individual :model:`myapp.MyModel`.\\n\\n**Context**\\n\\n``RequestContext``\\n\\n``mymodel``\\n        An instance of :model:`myapp.MyModel`.\\n\\n**Template:**\\n\\n:template:`myapp/my_template.html` (DESCRIPTION)\\n\\nsome_metadata: some data\"\n        )\n        self.assertEqual(metadata, {'some_metadata': 'some data'})\n"], "sample_1168": ["    def test_is_palindromic():\n        assert is_palindromic('racecar')\n        assert is_palindromic(12321)\n        assert is_palindromic(1221)\n        assert not is_palindromic('abc')\n        assert not is_palindromic(1234)\n        assert not is_palindromic('apple')\n        assert is_palindromic(S(\"a\"))\n        assert is_palindromic(S(\"1\"))\n\n\n", "    def test_partitions_trivial():\n        assert partitions(2, 1) == [(0, 2)]\n        assert partitions(2, 2) == [(0, 2), (1, 1)]\n        assert partitions(3, 2) == [(0, 3), (1, 2), (1, 1, 1)]\n", "    def test_permute_signs():\n        for t in [(0,), (1,), (1, 2), (-1, 2), (1, -2), (1, 2, 3),\n                  (1, 2, -3), (-1, 2, -3)]:\n            for expected in permute_signs(t):\n                assert expected in list(permute_signs(t))", "    def test_generate_oriented_forest():\n        from itertools import permutations\n        for n in range(1, 10):\n            for forest in generate_oriented_forest(n):\n                if n > 1:\n                    assert len(forest) == n\n                    assert all(d in range(n) for d in forest)\n                    for i in range(n):\n                        assert forest[max(0, i - 1)] > i\n\n                assert len(set(forest)) == n\n\n            # Test that enumerate returns the expected result\n            for i, f in enumerate(generate_oriented_forest(n)):\n                assert list(enumerate(f)) == list(zip(range(n), f))\n\n\n", "    def test_kbins_ordered_flag():\n        for ordered_flag in [None, 0, 1, 10, 11]:\n            for k in range(2, 6):\n                for l in [list(range(i)) for i in range(k, k*2)]:\n                    for p in kbins(l, k, ordered=ordered_flag):\n                        if ordered_flag == 11:\n                            assert all(len(a) == k//2 for a in p)\n                        else:\n                            assert all(len(a) == k for a in p)\n                        "], "sample_1083": ["    def test_asinh_fdiff_arg_index(self):\n        from sympy.functions.hyperbolic import asinh\n        x = Symbol('x')\n        f = asinh(x)\n        f.fdiff(2)\n", "    def test_fdiff_wrong_argument(self):\n        from sympy import acosh\n        acosh(x).fdiff(argindex=2)\n", "    def test_acsch_eval_rewrite_as_log():\n        from sympy.functions.hyperbolic import acsch\n        x = Symbol('x')\n        assert acsch(x)._eval_rewrite_as_log(x) == log(1/x + sqrt(1/x**2 + 1))\n", "    def test_argument_index_error(self):\n        from sympy import acosh\n        acosh(1).fdiff(2)\n", "    def test_asech_eval_complex():\n        from sympy.functions.special.hyperbolic import asech\n        z = asech(2*I)\n        assert re(z) == 0\n        assert im(z) == pi/3\n        z = asech(-2*I)\n        assert re(z) == 0\n        assert im(z) == -pi/3\n"], "sample_890": ["    def test_backward_selection_with_tol(self):\n        X, y = make_regression(n_samples=100, n_features=10, random_state=42)\n        reg = HistGradientBoostingRegressor(random_state=42)\n        sfs = SequentialFeatureSelector(\n            reg, n_features_to_select=5, direction=\"backward\", tol=-0.05\n        )\n        sfs.fit(X, y)\n        assert sfs.n_features_to_select_ == 5\n        assert sfs.support_.sum() == 5\n\n", "    def test_n_features_to_select_auto_tol(\n        self, n_features_auto, tol, n_features_to_select", "    def test_n_features_to_select_integration(\n        self, n_features_to_select, expected_n_features", "    def test_sequential_feature_selection_with_auto_n_features_to_select(\n        self, n_features_to_select, tol, direction", "    def test_sequential_feature_selection_with_different_datasets(\n        self, X_train, y_train, X_test, y_test, strategy, expected_n_features"], "sample_569": ["    def test_regplot_robust(self):\n        ax = lm.regplot(x='x', y='y_na', data=self.df, robust=True, ax=plt.gca())\n        ax.clear()\n        # Make sure the robust regression was used\n        model = smlm.robust_linear_model(self.df.x, self.df.y_na,\n                                        weights=1/self.df.y_na)\n        yhat = model.fit().predict(self.df.x)\n        p = ax.scatter(self.df.x, self.df.y_na - yhat,\n                      s=self.df.d.abs())\n        npt.assert_allclose(p.get_offsets()[:, 1], self.df.y_na - yhat)\n", "    def test_lmplot_hue(self):\n        g = lm.lmplot(x=\"z\", y=\"d\", data=self.df, hue=\"s\", col=\"y\")\n        fig, axes = plt.subplots(nrows=1, ncols=10)\n        for ax in axes:\n            ax.axis(\"off\")\n        n = len(self.df[\"s\"].unique())\n        assert len(g.axes) == n * 1\n        # Test that facets are correctly ordered\n        for i in range(n):\n            if i == 0:\n                assert g.axes[0].text(-1, 1, \"a\").get_text() == \"a\"\n            elif i == n - 1:\n                assert g.axes[-1].text(-1, 1, \"j\").get_text() == \"j\"\n            else:\n                assert g.axes[i].text(-1, 1, chr(ord('a') + i)).get_text() == chr(ord('a') + i)\n    ", "    def test_regplot_robust(self):\n        ax = lm.regplot(x='x', y='y', data=self.df, robust=True, ax=plt.gca())\n        p = ax.lines[0]\n        npt.assert_allclose(p.get_ydata(), smlm.robust_linear_model(x=self.df.x, y=self.df.y).params[0] +\n                                       smlm.robust_linear_model(x=self.df.x, y=self.df.y).params[1] * self.df.x)\n", "    def test_regplot_missing_data(self):\n        with pytest.raises(ValueError):\n            lm.regplot(x='y', y='z', data=self.df)\n        with pytest.raises(ValueError):\n            lm.regplot(x='x', y='nonexistent', data=self.df)\n\n        ax = lm.regplot(x='x', y='y', data=self.df, dropna=False)\n        plt.close()\n", "    def test_regplot_bootstrap_ci_discrete_x(self):\n        x = self.df.loc[:, \"x\"]\n        y = self.df.loc[:, \"y\"]\n        # discrete x with missing data\n        x_discrete = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        y_discrete = [1, 2, np.nan, 4, 5, np.nan, 8, 9, 10, 11]\n        # Simulate missing data\n        y_discrete[2] = None\n        y_discrete[5] = None\n        \n        ax = lm.regplot(x=x_discrete, y=y_discrete, x_estimator=np.mean,\n                        ci=95,\n                        bootstrap=True,\n                        scatter=True,\n                        fit_reg=True,\n                        n_boot=1000)\n\n        # Ensure the fit_reg is done correctly with bootstrapping\n        assert isinstance(ax.lines[0], mpl.lines.Line2D)\n        \n        # Check if the confidence intervals are generated correctly with discrete x\n        ci = ax.lines[0].get_ydata()\n        assert np.shape(ci)[0] == 2\n        assert np.shape(ci)[1] == len(x_discrete)\n\n\n"], "sample_938": ["    def test_default_man_pages(\n        sphinx_app: Sphinx, monkeypatch: monkeypatch", "    def test_default_man_pages_with_no_project(self, *args, **kwargs):\n        config = Config()\n        config.project = ''\n        config.release = ''\n        config.author = 'John Doe'\n        expected_man_pages = default_man_pages(config)\n        assert expected_man_pages == [('master', '', ' ', ['John Doe'], 1)]\n", "    def test_default_man_pages(self, app: Sphinx, tmpdir: str) -> None:\n        config = Config(app)\n        config.project = 'myproject'\n        config.release = '0.1'\n        config.author = 'John Doe'\n        config.master_doc = 'index'\n\n        expected_man_pages = default_man_pages(config)\n        assert len(expected_man_pages) == 1\n        assert expected_man_pages[0] == (\n            'index', 'myproject',\n            'myproject 0.1', ['John Doe'], 1\n        )\n", "    def test_default_man_pages(app: Sphinx) -> None:\n        config = Config(app)\n        default_pages = default_man_pages(config)\n        assert len(default_pages) == 1\n        assert default_pages[0][0] == app.config.master_doc\n        assert default_pages[0][1] == make_filename_from_project(config.project)\n        assert default_pages[0][2] == '%s %s' % (config.project, config.release)\n        assert default_pages[0][3] == [config.author]\n        assert default_pages[0][4] == 1\n", "    def test_default_man_pages(sphinx_app: Sphinx, test_srcdir: str) -> None:\n        config = Config(sphinx_app, test_srcdir)\n\n        default_pages = default_man_pages(config)\n        assert len(default_pages) == 1\n\n        expected_docname = 'index'\n        expected_name = 'project'\n        expected_description = f'{config.project} {config.release}'\n        expected_authors = ['author']\n        expected_section = 1\n\n        page_data = default_pages[0]\n        assert page_data[0] == expected_docname\n        assert page_data[1] == expected_name\n        assert page_data[2] == expected_description\n        assert page_data[3] == [expected_authors]\n        assert page_data[4] == expected_section\n"], "sample_1122": ["    def test_polar_lift_function_evaluation():\n        from sympy.functions.elementary.complexes import polar_lift\n        x = Symbol('x', real=True)\n        assert polar_lift(x).is_polar\n        assert polar_lift(2*x).is_polar\n        assert polar_lift(x + 2).is_polar\n", "compilation error", "compilation error", "    def test_polar_lift_multi(self):\n        x = Symbol('x', polar=True)\n        self.assertEqual(polar_lift(x**2), x**2)\n        self.assertEqual(polar_lift(x*x), x**2)\n        self.assertEqual(polar_lift(x+y), polar_lift(x) + polar_lift(y))\n        self.assertEqual(polar_lift(x*y), polar_lift(x)*polar_lift(y))\n", "compilation error"], "sample_710": ["            def test_something(self):\n                pass", "compilation error", "    def test_skip_marker_handling(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import unittest\n\n            class MyTest(unittest.TestCase):\n                @unittest.skip(\"This test is skipped\")\n                    pass\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        assert result.retcode == ExitCode.OK\n        assert len(result.passed) == 0  \n        assert len(result.skipped) == 1\n        assert result.outlines[0].when == \"skipped\"\n", "def test_skip_marker():\n    with Pytester(\n        conftest_ini=\"\"\"", "compilation error"], "sample_720": ["def test_power_transform_ordinal():\n    # Test ordinal encoding with power transform\n    ct = PowerTransformer(method='ordinal')\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    X_trans = ct.fit_transform(X)\n    assert_array_equal(X_trans, np.array([[0, 1], [2, 3], [4, 5]]))\n\n    X_back = ct.inverse_transform(X_trans)\n    assert_array_equal(X_back, X)\n\n\n\n", "    def test_power_transform_ordinal(self):\n        # Test ordinal encoding with PowerTransformer\n        enc = PowerTransformer(method='box-cox', standardize=False, copy=False)\n\n        X_ordinal = np.array([[1, 2, 3], [1, 2, 3], [4, 5, 6]])\n        enc.fit(X_ordinal)\n        X_trans = enc.transform(X_ordinal)\n\n        # Check that the transform produces ordinal values\n        for i in range(X_trans.shape[1]):\n            assert_array_equal(np.unique(X_trans[:, i]),\n                               np.arange(X_trans.shape[0]))\n\n        # Check that the inverse transform reconstructs the original data\n        X_back = enc.inverse_transform(X_trans)\n        assert_array_equal(X_back, X_ordinal)\n\n\n\n", "    def test_power_transform_ordinal_handle_unknown(self):\n        # ordinal encoding should raise an error if unexpected categories are\n        # found during transform, handle_unknown='error'\n        enc = PowerTransformer(method='ordinal', handle_unknown='error')\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        enc.fit(X)\n        with pytest.raises(ValueError, match=\"Found unknown categories\"):\n            enc.transform(np.array([[7, 8]]))\n", "    def test_ordinal_transform_unknown_categories(self):\n        enc = CategoricalEncoder(encoding='ordinal',\n                                handle_unknown='ignore')\n        X = [['a'], ['b'], ['a'], ['c']]\n        enc.fit(X)\n        X_transformed = enc.transform(X)\n        assert_array_equal(X_transformed, np.array(\n            [[0, 1, 2], [1, 2, 3], [0, 1, 2], [2, 3, 4]]))\n", "    def test_power_transform_handle_unknown_ordinal(self):\n        from sklearn.preprocessing import power_transform\n        encoder = PowerTransformer(method='ordinal',\n                                   handle_unknown='ignore')\n        X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]],\n                     dtype=np.int64)\n        encoder.fit(X)\n\n        X_transformed = power_transform(X, method='ordinal',\n                                       standardize=False)\n\n        assert_array_equal(X_transformed.shape, (4, 2))\n\n        # Unknown category in transform should be encoded as None\n        X_new = np.array([[1, 9], [3, 4], [5, 6], [7, 8]],\n                         dtype=np.int64)\n        X_new_transformed = power_transform(X_new, method='ordinal',\n                                           standardize=False)\n        assert_array_equal(X_new_transformed.shape, (4, 2))\n        assert_array_equal(X_new_transformed[:, 1], [None, None, None, None])\n\n\n"], "sample_565": ["    def test_inset_axes_bbox_to_anchor(self):\n        fig, ax = plt.subplots(1, 1)\n        inset_axes = inset_axes(ax, width=0.3, height=0.3,\n                                bbox_to_anchor=(0.5, 0.5, 0.6, 0.6))\n        inset_axes.plot([1, 2], [3, 4])\n        plt.show()\n", "    def test_inset_axes_basic(self):\n        fig, ax = plt.subplots()\n        inset_ax = inset_axes(ax, width=0.3, height=0.3, loc='center')\n        inset_ax.plot([1, 2, 3], [1, 2, 3])\n        plt.show()\n", "    def test_inset_axes_basic(self):\n        fig, ax = plt.subplots(1, 1)\n        axins = inset_axes(ax, width=0.3, height=0.3, loc='upper right')\n        axins.plot([0, 1, 2], [0, 1, 2])\n        plt.draw()\n\n\n\n", "    def test_inset_axes_example(self):\n        fig, axes = plt.subplots(2, 2)\n\n        inset_axes = inset_axes(axes[0, 0], width='40%%', height='30%%', loc='lower left')\n        inset_axes.set_title('Inset Axes')\n\n        inset_axes = zoomed_inset_axes(axes[0, 1], zoom=1.5, loc='upper right')\n        inset_axes.set_title('Zoomed Inset Axes')\n\n        \n        plt.show()\n\n\n\n", "    def test_inset_axes(self):\n        fig, axes = plt.subplots()\n\n        axins = inset_axes(axes, width=\"40%\", height=\"30%\",\n                           loc=\"lower left\", bbox_to_anchor=(0.1, 0.1))\n\n        axins.plot([0, 1], [0, 1], 'r-')\n\n        plt.show()\n\n\n"], "sample_292": ["    def test_csrf_exempt_view(self):\n        @csrf_exempt\n            return HttpResponse('OK')\n\n        response = self.client.get(self.url)\n        self.assertEqual(response.status_code, 200)\n        response = self.client.post(self.url)\n        self.assertEqual(response.status_code, 200)\n", "    def test_csrf_exempt_view(self):\n        @csrf_exempt\n            return HttpResponse('Success')\n\n        response = self.client.get(exempt_view.url)\n        self.assertEqual(response.status_code, 200)\n\n        response = self.client.post(exempt_view.url)\n        self.assertEqual(response.status_code, 200)\n\n        response = self.client.delete(exempt_view.url)\n        self.assertEqual(response.status_code, 200)\n", "    def test_origin_verification(self):\n        with override_settings(CSRF_TRUSTED_ORIGINS=['http://example.com']):\n            request = TestingHttpRequest()\n            request.META['HTTP_ORIGIN'] = 'https://example.com'\n            response = self.middleware.process_request(request, token_view)\n            self.assertIsNone(response)\n            \n            request = TestingHttpRequest()\n            request.META['HTTP_ORIGIN'] = 'https://www.example.com'\n            response = self.middleware.process_request(request, token_view)\n            self.assertIsNone(response)\n            \n            request = TestingHttpRequest()\n            request.META['HTTP_ORIGIN'] = 'https://example.org'\n            response = self.middleware.process_request(request, token_view)\n            self.assertIsNotNone(response)\n            self.assertEqual(response.status_code, 403)\n            self.assertIn(REASON_BAD_ORIGIN % 'https://example.org', response.content.decode())\n\n            request = TestingHttpRequest()\n            request.META['HTTP_ORIGIN'] = 'http://example.com'\n            response = self.middleware.process_request(request, token_view)\n            self.assertIsNone(response)\n\n\n", "    def test_csrf_exempt_view_no_protection(self):\n        @csrf_exempt\n            return HttpResponse(\"OK\")\n\n        response = self.client.get(self.url)\n        self.assertEqual(response.status_code, 200)\n\n        response = self.client.post(self.url)\n        self.assertEqual(response.status_code, 200)\n", "    def test_csrf_cookie_is_set_when_no_request_token(self):\n        response = self.client.post('/csrf-view/', {})\n        self._check_token_present(response)\n"], "sample_614": ["    def test_short_numpy_repr_large_array(self):\n        large_array = np.arange(1000000).reshape((1000, 1000))\n        short_repr = formatting.short_numpy_repr(large_array)\n        assert \"... \" in short_repr\n        assert \"shape\" in short_repr\n        assert short_repr.count(\" \") > 100  # Ensure it's truncated\n\n\n\n", "    def test_short_numpy_repr(self, data, expected):\n        with settings.local_context({\"display_values_threshold\": 10000}):\n            assert formatting.short_numpy_repr(data) == expected\n\n\n\n", "    def test_diff_dataset_repr_netcdf(self):\n        ds1 = xr.Dataset(\n            data={\n                \"data1\": ((\"x\", \"y\"), np.random.rand(3, 4)),\n                \"data2\": ((\"y\",), np.random.rand(4)),\n            },\n            coords={\"x\": np.arange(3), \"y\": np.arange(4)},\n        )\n\n        ds2 = xr.Dataset(\n            data={\n                \"data1\": ((\"x\", \"y\"), np.random.rand(3, 4) + 0.5),\n                \"data2\": ((\"y\",), np.random.rand(4) + 0.5),\n            },\n            coords={\"x\": np.arange(3), \"y\": np.arange(4)},\n        )\n\n        with tempfile.NamedTemporaryFile(suffix=\".nc\") as f:\n            ds1.to_netcdf(f.name)\n            ds2.to_netcdf(f.name)\n\n            diff_repr = formatting.diff_dataset_repr(\n                xr.open_dataset(f.name), xr.open_dataset(f.name), compat=\"allclose\"\n            )\n\n        expected = dedent(\n            \"\"\"\n        Left and right Dataset objects are not close\n", "    def test_short_numpy_repr(self, input_array):\n        original_numpy_options = np.get_printoptions()\n        try:\n            with set_numpy_options(linewidth=10, precision=2):\n                assert formatting.short_numpy_repr(input_array) == formatting.short_numpy_repr(input_array)\n        finally:\n            np.set_printoptions(**original_numpy_options)\n\n", "compilation error"], "sample_905": ["    def test_getdoc_inherited_classmethod():\n        class Base:\n                pass\n\n        class Derived(Base):\n                pass\n\n        doc = inspect.getdoc(Derived.my_method)\n        assert doc is None\n\n        doc = inspect.getdoc(Derived.my_method, allow_inherited=True)\n        assert doc == inspect.getdoc(Base.my_method)\n", "    def test_is_classmethod_with_overloads():\n        class MyClass:\n            @classmethod\n                pass\n\n                pass\n\n        assert inspect.isclassmethod(MyClass.my_method)\n        assert inspect.is_classmethod(MyClass.my_method, MyClass)\n        assert not inspect.isclassmethod(MyClass.__init__)\n\n\n\n", "    def test_signature_from_str_var_args_and_kwargs():\n        signature = 'def func(a, b, *, c, d=1, **kwargs): pass'\n        sig = inspect.signature_from_str(signature)\n        assert sig.parameters['a'].kind == Parameter.POSITIONAL_OR_KEYWORD\n        assert sig.parameters['b'].kind == Parameter.POSITIONAL_OR_KEYWORD\n        assert sig.parameters['c'].kind == Parameter.KEYWORD_ONLY\n        assert sig.parameters['d'].kind == Parameter.POSITIONAL_OR_KEYWORD\n        assert sig.parameters['kwargs'].kind == Parameter.VAR_KEYWORD\n\n\n\n\n", "    def test_is_asyncgenfunction():\n        assert inspect.isasyncgenfunction(asyncio.as_generator(lambda: None))\n        assert not inspect.isasyncgenfunction(lambda: None)\n\n\n\n        with pytest.raises(TypeError):\n            inspect.isasyncgenfunction(1)\n", "    def test_signature_from_str(signature_str):\n        sig = inspect.signature_from_str(signature_str)\n        expected_str = stringify_signature(sig)\n        assert expected_str == signature_str.strip()\n"], "sample_200": ["    def test_attach_binary_file_with_non_guessable_type(self):\n        with tempfile.NamedTemporaryFile(mode='wb', delete=False) as temp_file:\n            temp_file.write(b'This is a binary file')\n            temp_file.flush()\n\n            message = EmailMessage()\n            message.attach_file(temp_file.name, mimetype='image/vnd.custom')\n            decoded_attachments = self.get_decoded_attachments(message)\n            self.assertEqual(len(decoded_attachments), 1)\n            filename, content, mimetype = decoded_attachments[0]\n            self.assertEqual(filename, 'temp_file')\n            self.assertEqual(content, b'This is a binary file')\n            self.assertEqual(mimetype, 'image/vnd.custom')\n\n            os.remove(temp_file.name)\n", "    def test_sanitize_address(self):\n        self.assertEqual(sanitize_address(None), '')\n        self.assertEqual(sanitize_address(''), '')\n        self.assertEqual(sanitize_address('ValidAddress'), 'ValidAddress')\n        self.assertEqual(sanitize_address('Name <address@example.com>'), 'Name <address@example.com>')\n        self.assertEqual(sanitize_address('INVALID ADDRESS'), 'INVALID ADDRESS')\n        self.assertEqual(sanitize_address('Name\" <address@example.com>'), 'Name\" <address@example.com>')\n        self.assertRaises(ValueError, sanitize_address, 'Name\\n<address@example.com>')\n\n", "    def test_sanitize_address_invalid_address_raises_error(self):\n        with self.assertRaises(ValueError):\n            sanitize_address('invalid address')\n", "    def test_send_email_with_reply_to(self):\n        with self.settings(EMAIL_BACKEND='django.core.mail.backends.locmem.EmailBackend'):\n            message = EmailMessage(\n                subject='Test Email',\n                body='Hello, this is a test email with a reply-to header.',\n                from_email='test@example.com',\n                reply_to=['reply@example.com'],\n                to=['test@example.com'],\n            )\n            message.send()\n            sent_messages = mail.outbox\n            self.assertEqual(len(sent_messages), 1)\n            sent_message = sent_messages[0]\n            self.assertEqual(sent_message.subject, 'Test Email')\n            self.assertEqual(sent_message.body, 'Hello, this is a test email with a reply-to header.')\n            self.assertEqual(parseaddr(sent_message['From'])[1], 'test@example.com')\n            self.assertEqual(parseaddr(sent_message['Reply-To'])[1], 'reply@example.com')\n            self.assertEqual(parseaddr(sent_message['To'])[1], 'test@example.com')\n\n\n\n", "    def test_attach_file_non_guessable_mimetype(self):\n        with tempfile.NamedTemporaryFile('w', delete=False) as f:\n            f.write(\"Hello there\")\n            f.flush()\n            filename = f.name\n            attachment_mimetype = 'application/octet-stream'\n\n            msg = EmailMessage(\n                subject='Test Email',\n                body='This is a test email.',\n                from_email='test@example.com',\n                to=['test@example.com'],\n            )\n            msg.attach_file(filename, mimetype=attachment_mimetype)\n\n            message = msg.message()\n            self.assertIn('Content-Disposition', message['Content-Disposition'])  \n            self.assertIn('attachment', message['Content-Disposition'].value) \n            self.assertIn(filename, message['Content-Disposition'].value)\n            self.assertIn(attachment_mimetype, message.get_content_type())\n\n\n"], "sample_275": ["    def test_concurrent_deletes(self):\n        with transaction.atomic(using=self.conn2):\n            # Create items and award them some awards\n            i1 = Item.objects.create(name=\"item1\")\n            i2 = Item.objects.create(name=\"item2\")\n            a1 = Award.objects.create(name=\"award1\")\n            a2 = Award.objects.create(name=\"award2\")\n            i1.awards.add(a1)\n            i2.awards.add(a2)\n\n        # Delete the items from the original connection\n        with transaction.atomic():\n            Item.objects.filter(id__in=[i1.id, i2.id]).delete()\n\n        # Attempt to delete the items from the second connection\n        with self.assertRaises(models.ObjectDoesNotExist):\n            with transaction.atomic(using=self.conn2):\n                Item.objects.filter(id__in=[i1.id, i2.id]).delete()\n\n\n", "    def test_delete_locked(self):\n        p = Person.objects.create(name='Alice')\n        # Try to delete Alice from the first connection\n        with transaction.atomic(using=self.conn):\n            with self.assertRaises(models.IntegrityError):\n                Person.objects.filter(pk=p.pk).delete(using=self.conn)\n        # Commit the transaction on the second connection.\n        self.conn2.commit()\n        # Now Alice should be deleted from the first connection.\n        with transaction.atomic(using=self.conn):\n            Person.objects.filter(pk=p.pk).delete(using=self.conn)\n", "    def test_delete_cascade_with_locking(self):\n        # Create a parent object with child objects.\n        parent = Person.objects.create(name='Alice')\n        child1 = Child.objects.create(name='Bob', person=parent)\n        child2 = Child.objects.create(name='Charlie', person=parent)\n\n        # Set up a second transaction using the second connection.\n        with transaction.atomic(using=self.conn2):\n            # Delete the parent object on the second connection,\n            # triggering a cascade delete.\n            with self.assertRaises(Exception):\n                parent.delete()  \n\n        # Verify that the child objects have been deleted on both connections.\n        self.assertEqual(Person.objects.count(), 0)\n        self.assertEqual(Child.objects.count(), 0)\n\n\n\n", "    def test_delete_locking_fails(self):\n        with transaction.atomic(using=self.db):\n            person = Person.objects.create(name='John Doe')\n            # Simulate a second transaction that is about to delete the same person.\n            with self.conn2.cursor() as cursor:\n                cursor.execute('DELETE FROM auth_user WHERE id = %s', [person.id])\n            self.conn2.commit()\n        with self.assertRaises(models.IntegrityError):\n            with transaction.atomic(using=self.db):\n                person = Person.objects.get(pk=person.id)\n                person.delete()\n\n\n", "    def test_delete_locking_with_related_objects(self):\n        \"\"\"\n        Tests that deleting an object with related objects works correctly,\n        even when multiple connections are involved.\n        \"\"\"\n\n        person1 = Person.objects.create(name=\"Person 1\")\n        person2 = Person.objects.create(name=\"Person 2\")\n        played = PlayedWith.objects.create(person=person1, toy=Toy.objects.create(name=\"Toy 1\"))\n\n        with transaction.atomic(using=self.conn2):\n            played.delete()\n        with transaction.atomic():\n            person1.delete()\n\n        # Person 1 and the PlayedWith object should be deleted, even though\n        # the second connection was involved in the deletion of the related\n        # object.\n        self.assertEqual(Person.objects.count(), 1)\n        self.assertEqual(PlayedWith.objects.count(), 0)\n\n\n\n"], "sample_223": ["    def test_related_manager_ordering(self):\n        # Test that the ordering of related managers works as expected.\n        qs = Program.objects.all().prefetch_related('related_objects')\n\n        # Assert that the ordering is correct for all related objects.\n        self.assertEqual(qs.count(), 2)\n        self.assertEqual(qs[0].name, 'p1')\n        self.assertEqual(qs[1].name, 'p2')\n", "compilation error", "    def test_queryset_methods(self):\n        self.assertEqual(Author.objects.count(), 4)\n        self.assertEqual(Item.objects.count(), 4)\n        self.assertEqual(Annotation.objects.count(), 2)\n        self.assertEqual(Note.objects.count(), 3)\n        self.assertEqual(Tag.objects.filter(category=NamedCategory.objects.get(name='Generic')).count(), 5)\n\n\n", "    def test_related_objects_with_nested_select_related(self):\n        with self.subTest('Test for correct population with nested select_related'):\n            queryset = Report.objects.select_related('creator').prefetch_related(\n                'comments',\n                'creator.extrainfo'\n            ).filter(name='r1')\n\n            reports = list(queryset)\n            self.assertEqual(len(reports), 1)\n            report = reports[0]\n            self.assertIsNotNone(report.creator)\n            self.assertIsNotNone(report.comments.all())\n            self.assertIsNotNone(report.creator.extrainfo)\n\n\n\n", "compilation error"], "sample_731": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_550": ["    def test_dragging_within_tightbbox(self, fig, axes):\n        # test that panning within a tight bbox does not cause issues.\n        bbox = axes.get_tightbbox()\n        xrange, yrange = bbox.x0, bbox.y0\n\n        # Pan to the edge of the tight box\n        axes.set_xlim(xrange, xrange + 1)\n        axes.set_ylim(yrange, yrange + 1)\n\n        # Test dragging within the tight bbox\n        canvas = fig.canvas\n        canvas.mpl_connect('button_press_event', lambda event: None)\n        canvas.mpl_connect('button_release_event', lambda event: None)\n        canvas.mpl_connect('motion_notify_event', lambda event: None)\n        canvas.draw()\n        \n        \n\n        # this could be more robust but assumes drag events are handled\n        # correctly in the `_pan_start` and `_get_pan_points` methods.\n        press_event = MouseEvent(\n            x=bbox.x0, y=bbox.y0,\n            button=MouseButton.LEFT,\n            key=None,\n            data=None,\n            figure=fig,\n            canvas=canvas,\n            )\n        axes._pan_start(press_event.x, press_event.y, press_event.button)\n        motion_event = MouseEvent(\n            x=bbox.x0 + 1, y=bbox.y0 + 1,\n            button=MouseButton.LEFT,\n            key=None,\n            data=None,\n            figure=fig,\n            canvas=canvas,\n            )\n        axes.drag_pan(motion_event.button, None, motion_event.x, motion_event.y)\n\n        # Check that no errors occurred\n        assert not canvas.renderer._get_warnings()\n", "    def test_get_tightbbox_with_title(self, backend):\n        fig, ax = plt.subplots(1, 1)\n        fig.canvas.manager.ensure_initialized()\n        fig.canvas.renderer = RendererBase()\n        ax.set_title(\"Test Title\")\n        bbox = ax.get_tightbbox(renderer=fig.canvas.renderer)\n        assert bbox.width > 0\n        assert bbox.height > 0\n\n", "    def test_twin_axes_shared_autoscale(self):\n        fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n        ax1.set_xlim(0, 10)\n        ax1.set_ylim(0, 5)\n        ax2.set_xlim(0, 10)\n        # Make sure autoscaling is on for the x-axis\n        ax1.set_autoscalex_on(True)\n        # Make sure autoscaling is off for the y-axis to test manual control\n        ax1.set_autoscaley_on(False)\n        ax1.plot([1, 2, 3])\n\n        # Check if the autoscale setting is inherited by the twinned axes\n        assert ax2.get_autoscalex_on()\n        assert not ax2.get_autoscaley_on()\n\n        # Now manually set the ylim for ax2\n        ax2.set_ylim(1, 4)\n        # Check that the autoscaling is not inherited \n        assert ax2.get_ylim() == (1, 4)\n        assert not ax1.get_ylim() == (1, 4)\n\n        plt.close()\n\n\n\n", "    def test_get_tightbbox_extra_artists(self, bbox_extra_artists):\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n        ax.set_xlim((-1, 1))\n        ax.set_ylim((0, 2))\n        bbox = ax.get_tightbbox(renderer=fig.canvas.get_renderer(),\n                                bbox_extra_artists=bbox_extra_artists\n                                )\n        assert bbox is not None\n\n\n\n", "    def test_prepare_view_from_bbox(self, direction, expected):\n        bbox = (0.1, 0.2, 0.3)  # (xp, yp, scl)\n        new_xbound, new_ybound = self.get_instance()._prepare_view_from_bbox(\n            bbox, direction=direction, twinx=False, twiny=False)\n        assert tuple(new_xbound) == expected[:2]\n        assert tuple(new_ybound) == expected[2:]\n"], "sample_957": ["    def test_restify_typing_generic(self):\n        from typing import List, Optional, Union\n        assert restify(List[int]) == 'List[int]'\n        assert restify(Optional[int]) == 'Optional[int]'\n        assert restify(Union[int, str]) == 'int | str'\n        assert restify(List[Optional[int]]) == 'List[Optional[int]]'\n        assert restify(Union[int, Optional[str]]) == 'int | Optional[str]'\n", "    def test_restify_custom_NewType(self):\n        assert restify(MyInt) == ':obj:`~typing.NewType`'\n\n\n", "    def test_restify_typing_literal(self):\n        assert restify(Literal[str, int]) == ':class:`typing.Literal`\\[str, int\\]'\n        assert restify(Literal['a', 'b']) == ':obj:`typing.Literal`\\[\"a\", \"b\"]'\n\n\n\n", "    def test_restify_special_form(self):\n        assert restify(MyList) == 'List[T]'\n        assert restify(typing.List) == ':class:`typing.List`'\n        assert restify(typing.Union) == ':class:`typing.Union`'\n        assert restify(typing.Callable) == ':class:`typing.Callable`'\n        assert restify(typing.Tuple) == ':class:`typing.Tuple`'\n\n\n\n", "    def test_restify_generator(self):\n        assert restify(Generator[T, T, T]) == ':class:`~typing.Generator`'\n        assert restify(Generator) == ':class:`~typing.Generator'\n\n"], "sample_1172": ["compilation error", "    def test_solve_poly_system_zero_solutions():\n        raises(SolveFailed, lambda: solve_poly_system([x**2 + 1, y**2 + 1], x, y))\n", "    def test_solve_poly_system_rational_roots():\n        a = Poly(x**2 - 2, x, domain=QQ)\n        b = Poly(y**2 - 2, y, domain=QQ)\n        with raises(SolveFailed, match=\"not 0-dimensional\"):\n            solve_poly_system([a, b], x, y)\n", "    def test_solve_biquadratic_non_zero_gcd(self):\n        from sympy.polys import Options, Poly\n        from sympy.abc import x, y\n        NewOption = Options((x, y), {'domain': 'ZZ'})\n        a = Poly(y**2 - 4 + x, y, x, domain='ZZ')\n        b = Poly(y + x - 7, y, x, domain='ZZ')\n        solve_biquadratic(a, b, NewOption)\n", "    def test_solve_poly_system_generic():\n        from sympy.polys.polyerrors import PolificationFailed\n        with raises(PolificationFailed):\n            solve_poly_system(\"x + 1 == y\", x, y)\n        with raises(SolveFailed):\n            solve_poly_system([x**3-1, x**2 + 1], x, y)\n\n        a = Poly(x**2 + y**2 - 1, x, y)\n        b = Poly(x - y, x, y)\n        solutions = solve_poly_system([a, b], x, y)\n        assert solutions == [(sqrt(2)/2, sqrt(2)/2), (-sqrt(2)/2, -sqrt(2)/2)]\n\n"], "sample_700": ["    def test_evaluate_skip_marks(self, testdir):\n        p = testdir.makepyfile(\n            \"\"\"\n                pytest.skip(\"this is a skip\")\n\n                pytest.xfail(\"this is an xfail\")\n\n                pass\n            \"\"\",\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines(\n            [\n                \"collected 3 items                               \",\n                \"test_this_is_skipped[ PASSED] \",\n                \"test_this_is_xfailed[XFAIL] \",\n                \"test_this_passes[ PASSED] \",\n            ]\n        )\n", "    def test_empty_function_has_no_fixtures(self, testdir):\n        testdir.makeconftest(\n            \"\"\"\n                print(f\"Fixture {fixturedef.name} setup\")\n            \"\"\"\n        )\n        p = testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.fixture\n                print(\"Setting up fixture\")\n                pass\n\n                pass\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        assert result.ret == 0\n        lines = result.stdout.str()\n        assert \"Fixture my_fixture setup\" not in lines\n        assert \"Setting up fixture\" not in lines\n\n\n\n", "    def test_evaluate_skip_marks(self, pytester):\n        code = \"\"\"\n        import pytest\n\n        @pytest.mark.skip(reason=\"This test is skipped.\")\n            pass\n        \"\"\"\n        pytester.write(\"test_skip.py\", code)\n\n        result = pytester.runpytest_subprocess()\n        result.assert_outcomes(passed=0)\n        assert \"This test is skipped.\" in result.stdout\n\n\n", "    def test_evaluate_skip_marks(self, pytester: Pytester):\n        \"\"\"Test evaluation of skip markers.\"\"\"\n\n        pytester.runpytest(\n            \"-k\",\n            \"test_skip_evaluation\",\n            \"--capture=no\",\n            \"--verbose=0\",\n            #  use an extra arg to ensure the test is\n            #  not skipped due to the \"skipif\" marker\n        )\n\n\n        result = pytester.parse_results()\n        assert len(result.passed) == 1\n", "    def test_prunetraceback_long_traceback(self, testdir):\n        p = testdir.makepyfile(\n            \"\"\"\n            import pytest\n                raise ValueError(\"some error\")\n            \"\"\"\n        )\n\n        result = p.runpytest()\n        result.stdout.reassert_lines_match(\n            r\"^\\s+ValueError: some error$\"\n        )\n        result.stdout.reassert_lines_match(r\"^\\s*E\\s+.*\")\n\n        # Ensure the full traceback is printed in this case\n"], "sample_249": ["    def test_test_db_signature(self):\n        connection = get_connection_copy()\n        fake_db_name = 'my_fake_database'\n        connection.settings_dict['NAME'] = fake_db_name\n        signature = connection.creation.test_db_signature()\n        self.assertEqual(signature, (\n            connection.settings_dict['HOST'],\n            connection.settings_dict['PORT'],\n            connection.settings_dict['ENGINE'],\n            fake_db_name,\n        ))\n", "    def test_db_signature_with_different_suffixes(self):\n        with override_settings(\n            DATABASES={\n                DEFAULT_DB_ALIAS: get_connection_copy().settings_dict,\n            }\n        ):\n            # Set suffix for a test database\n            connection.settings_dict['TEST']['NAME'] = 'test_db_suffix_test'\n            with self.subTest('suffix 1'):\n                BaseDatabaseCreation(get_connection_copy()).test_db_signature()\n\n            connection.settings_dict['TEST']['NAME'] = 'test_db_suffix_test_2'\n            with self.subTest('suffix 2'):\n                BaseDatabaseCreation(get_connection_copy()).test_db_signature()\n            \n            # Make sure unique signatures are returned\n            assert connections[DEFAULT_DB_ALIAS].settings_dict['TEST']['NAME'] == 'test_db_suffix_test_2'  \n\n\n\n", "    def test_db_signature(self):\n        with override_settings(\n            DATABASES={\n                DEFAULT_DB_ALIAS: {\n                    'NAME': 'test',\n                    'ENGINE': 'django.db.backends.sqlite3',\n                    'HOST': 'localhost',\n                    'PORT': 3306,\n                }\n            }\n        ):\n            connection = get_connection_copy()\n            connection.creation = BaseDatabaseCreation(connection)\n\n            signature = connection.creation.test_db_signature()\n            self.assertEqual(signature, ('localhost', 3306, 'django.db.backends.sqlite3', 'test_test'))\n\n            # Test that changing settings_dict['NAME'] changes the signature\n            connection.settings_dict['NAME'] = 'new_test'\n            new_signature = connection.creation.test_db_signature()\n            self.assertEqual(new_signature, ('localhost', 3306, 'django.db.backends.sqlite3', 'test_new_test'))\n", "    def test_databases_signature(self):\n        with override_settings(DATABASES={'default': {'NAME': 'testdb_signature'}}) as override:\n            original_connection = get_connection_copy()\n            original_signature = original_connection.creation.test_db_signature()\n\n            # Modify some settings to create a new signature\n            override['DATABASES']['default']['NAME'] = 'testdb_signature_modified'\n            override['DATABASES']['default']['HOST'] = 'new_host'\n\n            new_connection = get_connection_copy()\n            new_signature = new_connection.creation.test_db_signature()\n\n            self.assertNotEqual(original_signature, new_signature)\n\n", "    def test_db_signature_differentiated_for_each_suffix(self):\n        conn1 = get_connection_copy()\n        conn2 = get_connection_copy()\n\n        # Modify one connection's settings to ensure it has a different signature.\n        conn2.settings_dict['NAME'] = 'modified_name'\n\n        signature1 = conn1.creation.test_db_signature()\n        signature2 = conn2.creation.test_db_signature()\n\n        self.assertNotEqual(signature1, signature2)\n\n"], "sample_101": ["    def test_limited_stream(self):\n        from django.utils.encoding import force_bytes\n\n        environ = {\n            'wsgi.input': BytesIO(b'This is a test'),\n            'CONTENT_LENGTH': '13',\n        }\n        request = WSGIRequest(environ)\n        stream = request._stream\n        self.assertEqual(stream.read(), b'This is a test')\n        self.assertEqual(stream.read(), b'')\n        with self.assertRaises(ValueError):\n            stream.read(100)\n        self.assertEqual(stream.readline(), b'This is a test')\n\n        environ = {\n            'wsgi.input': BytesIO(b'This is a test\\n'),\n            'CONTENT_LENGTH': '13',\n        }\n        request = WSGIRequest(environ)\n        stream = request._stream\n        self.assertEqual(stream.readline(), b'This is a test\\n')\n        with self.assertRaises(StopIteration):\n            stream.readline()\n", "    def test_wsgi_request_get_path_info(self):\n        request = self.request_factory.get('/path/to/resource')\n        self.assertEqual(request.path_info, '/path/to/resource')\n\n\n        request = self.request_factory.post('/path/to/resource')\n        self.assertEqual(request.path_info, '/path/to/resource') \n\n", "    def test_wsgirequest_get_content_type(self):\n        from django.http import HttpResponse\n\n        # Test that the content type is set correctly\n        request = self.request_factory.get('/test/')\n        request._set_content_type_params(request.META)\n        self.assertEqual(request.content_type, 'text/html; charset=utf-8')\n\n        # Test that the content type remains unchanged if content_type is already set\n        request = self.request_factory.get('/test/', content_type='application/json')\n        request._set_content_type_params(request.META)\n        self.assertEqual(request.content_type, 'application/json')\n\n        # Test that the content type is set to application/octet-stream if content_type is not provided\n        request = self.request_factory.get('/test/')\n        response = HttpResponse(b'test')\n        response.content_type = None\n        request.environ['wsgi.input'] = BytesIO(response.content)\n        request._set_content_type_params(request.META)\n        self.assertEqual(request.content_type, 'application/octet-stream')\n\n        # Test that the content type is set to application/octet-stream for file uploads\n        request = self.request_factory.post('/test/', files={'file': FileResponse(self.request_factory.get('/test/').content)})\n        request._set_content_type_params(request.META)\n        self.assertEqual(request.content_type, 'multipart/form-data; boundary=---------------------------')\n", "    def test_request_content_length(self):\n        environ = self.request_factory.get('/some/url', HTTP_CONTENT_LENGTH='100')\n        request = WSGIRequest(environ)\n        self.assertEqual(request._stream.remaining, 100)\n", "    def test_wsgirequest_http_methods(self):\n        environ = {}\n        environ['REQUEST_METHOD'] = 'POST'\n        request = WSGIRequest(environ)\n        self.assertEqual(request.method, 'POST')\n\n        environ['REQUEST_METHOD'] = 'GET'\n        request = WSGIRequest(environ)\n        self.assertEqual(request.method, 'GET')\n\n        environ['REQUEST_METHOD'] = 'PUT'\n        request = WSGIRequest(environ)\n        self.assertEqual(request.method, 'PUT')\n\n        environ['REQUEST_METHOD'] = 'DELETE'\n        request = WSGIRequest(environ)\n        self.assertEqual(request.method, 'DELETE')\n\n        environ['REQUEST_METHOD'] = 'PATCH'\n        request = WSGIRequest(environ)\n        self.assertEqual(request.method, 'PATCH')\n\n        environ['REQUEST_METHOD'] = 'OPTIONS'\n        request = WSGIRequest(environ)\n        self.assertEqual(request.method, 'OPTIONS')\n"], "sample_862": ["    def test_fit_transform_different_stop_words(self):\n        stop_words = {'the', 'pizza', 'beer'}\n        vectorizer = TfidfVectorizer(stop_words=stop_words)\n        X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n        \n        # Check that stop words are removed:\n        for stop in stop_words:\n            assert stop not in vectorizer.get_feature_names()\n", "    def test_stop_words_not_filtered_on_fit_transform(self):\n        vec = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS)\n        X = vec.fit_transform(ALL_FOOD_DOCS)\n\n        assert vec.get_feature_names_out().count('the') > 0\n", "    def test_stop_words_with_large_vocabulary(self):\n        # Test case for stop words when vocabulary is large\n        vectorizer = TfidfVectorizer(max_features=10000,\n                                     stop_words=\"english\")\n        large_corpus = [\n            \" \".join(np.random.choice(list(ENGLISH_STOP_WORDS) +\n                                       list(range(10000)),\n                                      size=1000)\n            for _ in range(100)\n        ]\n        vectorizer.fit(large_corpus)\n\n        # Check that stop words are not in the vocabulary\n        for word in ENGLISH_STOP_WORDS:\n            assert word not in vectorizer.vocabulary_\n", "    def test_input_encoding(self):\n        # Test decoding from bytes\n        text_bytes = \"This is a test \\x8a\".encode('latin-1')\n        vectorizer = TfidfVectorizer(input='bytes', encoding='latin-1')\n        X = vectorizer.fit_transform([text_bytes])\n        assert_array_equal(X.toarray(), [[1, 0, 0, 0, 1]])\n\n        # Test decoding with a bytes file-like object\n        text_bytes = b\"This is a test \\x8a\"\n        class BytesFile:\n                self.data = data\n                return self.data\n        with BytesFile(text_bytes) as f:\n            vectorizer = TfidfVectorizer(input='file', encoding='latin-1')\n            X = vectorizer.fit_transform([f])\n        assert_array_equal(X.toarray(), [[1, 0, 0, 0, 1]])\n\n\n\n    \n\n\n\n", "    def test_strip_accents_unicode(self):\n        # Test strip_accents_unicode\n        self.assertEqual(strip_accents_unicode(\"r\u00e9sum\u00e9\"), \"resume\")\n        self.assertEqual(strip_accents_unicode(\"\u00e9\"), \"e\")\n        self.assertEqual(strip_accents_unicode(\"\u00e4\u00f6\u00fc\u00df\"), \"aou\")\n        self.assertEqual(strip_accents_unicode(\"\"), \"\")\n"], "sample_85": ["    def test_related_objects_after_deletion(self):\n        a1 = create_a(name='a1')\n        a2 = create_a(name='a2')\n        r1 = R.objects.create(a=a1, name='r1')\n        r2 = R.objects.create(a=a2, name='r2')\n        r3 = R.objects.create(a=a1, name='r3')\n\n        # Delete a1\n        a1.delete()\n\n        # Check if related objects are properly removed\n        with self.assertRaises(R.DoesNotExist):\n            R.objects.get(a=a1)\n        with self.assertRaises(R.DoesNotExist):\n            R.objects.get(pk=r1.pk)\n        with self.assertRaises(R.DoesNotExist):\n            R.objects.get(pk=r3.pk)\n        with self.assertRaises(R.DoesNotExist):\n            R.objects.get(a=a2)\n", "    def test_related_objects_removed_on_delete_cascade(self):\n        a1 = create_a(name='A1')\n        a2 = create_a(name='A2')\n        r1 = R.objects.create(\n            parent=a1,\n            child=a2\n        )\n        r2 = R.objects.create(\n            parent=a1,\n            child=a2\n        )\n\n        self.assertEqual(R.objects.count(), 2)\n        self.assertEqual(a1.r_set.count(), 2)\n\n        a1.delete()\n\n        self.assertEqual(R.objects.count(), 0)\n        self.assertEqual(a1.r_set.count(), 0)\n", "    def test_related_name_for_m2m_with_self(self):\n        class SelfRelating(models.Model):\n            name = models.CharField(max_length=255)\n            rels = models.ManyToManyField('self', symmetrical=True)\n\n        with self.subTest(field_name='rels'):\n            with self.subTest(related_name='rels'):\n                s = SelfRelating.objects.create(name='A')\n                s.rels.add(s)\n                self.assertEqual(s.rels.count(), 1)\n                self.assertEqual(s.rels.first().name, 'A')\n                s.delete()\n\n\n\n\n", "    def test_on_delete_from_m2m_to_self(self):\n        parent1 = Parent.objects.create(name=\"parent1\")\n        child1 = RChild.objects.create(parent=parent1, name=\"child1\")\n        parent1.children.add(child1)\n        self.assertEqual(RChild.objects.count(), 1)\n\n        parent1.delete()\n        self.assertEqual(RChild.objects.count(), 0)\n\n", "    def test_related_name_for_self_reference(self):\n        # Issue #19902: Related name for self-references should be\n        # non-ambiguous.\n        class SelfRef(models.Model):\n            name = models.CharField(max_length=50)\n            parent = models.ForeignKey(\n                'self', on_delete=models.CASCADE, related_name='children'\n            )\n\n        with self.assertRaises(ValueError):\n            instance = SelfRef(name='Parent')\n            instance.parent = instance\n            instance.save()\n"], "sample_340": ["    def test_inconsistent_history(self):\n        \"\"\"\n        Regression test for #22023: check if any applied migrations have\n        unapplied dependencies.\n        \"\"\"\n        with connections['default'].temporary_connection():\n            loader = MigrationLoader(connection)\n            loader.build_graph()\n            loader.check_consistent_history(connection)\n", "    def test_check_consistent_history(self):\n        with override_settings(DATABASES={'default': {'ENGINE': 'django.db.backends.sqlite3'},\n                                        'other': {'ENGINE': 'django.db.backends.sqlite3'}}), \\\n                modify_settings(INSTALLED_APPS=['test_app']):\n            connection = connections['default']\n            recorder = MigrationRecorder(connection)\n            recorder.applied_migrations = {'test_app.0001_initial': True}\n            with self.assertRaises(InconsistentMigrationHistory):\n                loader = MigrationLoader(connection)\n                loader.build_graph()\n                loader.check_consistent_history(connection)\n\n\n", "    def test_inconsistent_history(self):\n        with override_settings(DATABASES={'default': {'ENGINE': 'django.db.backends.sqlite3'}}):\n            with connections['default'].cursor() as cursor:\n                with self.subTest('Testing recording'):\n                    with modify_settings(INSTALLED_APPS=['myapp']):\n                        # Create migrations and record them as applied\n                        recorder = MigrationRecorder(connection=connections['default'])\n                        recorder.record('myapp', '0001_initial')\n                        recorder.record('myapp', '0002_second')\n                        # Should not raise\n\n                    with self.subTest('Testing detection'):\n                        loader = MigrationLoader(connection=connections['default'])\n                        loader.build_graph()\n                        loader.check_consistent_history(connection=connections['default'])\n\n                        # Remove one of the recorded migrations\n                        with connections['default'].cursor() as cursor:\n                            cursor.execute(\n                                \"DELETE FROM django_migrations WHERE app = ?\", ('myapp',)\n                            )\n                        loader = MigrationLoader(connection=connections['default'])\n                        loader.build_graph()\n\n                        # This should raise an exception as the history is inconsistent.\n                        with self.assertRaises(InconsistentMigrationHistory):\n                            loader.check_consistent_history(connection=connections['default'])\n\n\n\n", "    def test_inconsistent_history(self):\n        with override_settings(DATABASES={\n            'default': {'ENGINE': 'django.db.backends.sqlite3'},\n            'other': {'ENGINE': 'django.db.backends.sqlite3'},\n        }):\n            # Make sure we have a fresh connection\n            for db_name in self.databases:\n                connections[db_name] = connections[db_name].reset()\n\n            app_name = 'test_migrations'\n            with self.subTest(app_name=app_name):\n                self.create_app(app_name, migrations=True)\n                self.execute(\"CREATE TABLE test_migrations_%s_test_migration (id INTEGER PRIMARY KEY)\" % app_name)\n                self.execute(\"INSERT INTO test_migrations_%s_test_migration (id) VALUES (1)\" % app_name)\n                with self.assertRaises(InconsistentMigrationHistory):\n                    MigrationLoader(connection=connections['default']).check_consistent_history(connections['default'])\n", "    def test_applied_migrations_record_after_apply(self):\n        with connections['default'].cursor() as cursor:\n            cursor.execute(\"CREATE TABLE myapp_auto_increment (id INT PRIMARY KEY AUTO_INCREMENT)\")\n        with connections['other'].cursor() as cursor:\n            cursor.execute(\"CREATE TABLE otherapp_auto_increment (id INT PRIMARY KEY AUTO_INCREMENT)\")\n        # Create a new migration loader\n        loader = MigrationLoader(connection=connections['default'])\n        # Load migrations\n        loader.build_graph()\n        # Apply migrations\n        for migration in loader.graph.leaf_nodes(('myapp', '0001_initial')):\n            with connections['default'].schema_editor() as editor:\n                migration[1].apply(editor)\n        \n        recorder = MigrationRecorder(connections['default'])\n        applied_migrations = recorder.applied_migrations()\n        self.assertIn(('myapp', '0001_initial'), applied_migrations)\n\n\n\n"], "sample_84": ["    def test_urlencode_none(self):\n        with self.assertRaisesMessage(TypeError, self.cannot_encode_none_msg):\n            urlencode({'k': None})\n\n\n", "    def test_urlencode_multivalue(self):\n        query = MultiValueDict({\n            'key1': ['value1', 'value2'],\n            'key2': ['value3', 'value4'],\n        })\n        encoded_query = urlencode(query, doseq=True)\n        self.assertEqual(encoded_query, 'key1=value1&key1=value2&key2=value3&key2=value4')\n", "    def test_urlencode_multivalue_dict(self):\n        query = MultiValueDict({'name': ['Alice', 'Bob'], 'age': [25, 30]})\n        expected_encoded = 'name=Alice&name=Bob&age=25&age=30'\n        self.assertEqual(urlencode(query), expected_encoded)\n\n\n\n", "    def test_urlencode_None(self):\n        with self.assertRaisesMessage(TypeError, self.cannot_encode_none_msg):\n            urlencode({'key': None})\n", "    def test_urlencode_MultiValueDict(self):\n        query = MultiValueDict({'key1': ['value1', 'value2'], 'key2': ['value3', 'value4']})\n        encoded_query = urlencode(query)\n        expected_query = 'key1=value1&key1=value2&key2=value3&key2=value4'\n        self.assertEqual(encoded_query, expected_query)\n\n"], "sample_694": ["    def test_private_function_warning(pytester):\n        code = \"\"\"\n        import _pytest.deprecated  # noqa: F401\n        \n          _pytest.deprecated.check_ispytest(_ispytest)\n          return 'ok'\n        \n        my_private_function(1, 2)\n        \"\"\"\n        pytester.write(\"test_private.py\", code)\n        result = pytester.runpytest()\n        assert result.ret == 0\n        # Check for the warning printed\n        result.stdout.re_search_lines(r\"deprecated\\.PRIVATE\")\n", "    def test_check_ispytest_warning(pytestconfig, pytester):\n        # check private function call outside pytest context\n        with pytest.raises(PytestDeprecationWarning):\n            deprecated.check_ispytest(ispytest=False)\n\n        # check private function call inside pytest context\n        with pytester.in_plugin_dir(plugin_name=plugin):\n            with pytest.warns(PytestDeprecationWarning) as warnings:\n                deprecated.check_ispytest(ispytest=False)\n            assert len(warnings) == 1\n            assert \"A private pytest class or function was used.\" in str(warnings[0].message)\n", "    def test_plugin_deprecation(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            from _pytest import deprecated\n            pytest.addoption(  # WARNING: Deprecated since pytest 6.0\n                \"--myoption\", \n                action='store', \n                dest='myoption',\n                help='My deprecated option',\n            )\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.assert_outcomes(passed=1, warnings=1)\n        assert \"Warning: deprecated since pytest 6.0\" in str(result.getwarnings())\n\n\n\n", "    def test_private_function(pytester: Pytester):\n        \"\"\"\n        Test that the check_ispytest function triggers a warning\n        when called with _ispytest=False\n        \"\"\"\n        with pytest.warns(deprecated.PRIVATE) as warnings:\n            deprecated.check_ispytest(False)\n        assert len(warnings) == 1\n        assert isinstance(warnings[0].warning, PytestDeprecationWarning)\n", "    def test_private_function_warning(testdir, tmpdir):\n        pyfile = Path(tmpdir / \"test_private.py\")\n        pyfile.write(\n            \"\"\"\n                check_ispytest(_ispytest)\n                print(some, args)\n\n                if not ispytest:\n                    warn(deprecated.PRIVATE, stacklevel=3)\n            \"\"\"\n        )\n        result = testdir.runpytest(pyfile)\n        assert result.ret == 0\n        # Check the warning message\n        warnings_out = result.stdout.str()\n        assert \"A private pytest class or function was used.\" in warnings_out\n"], "sample_345": ["    def test_error_files(self):\n        with tempfile.NamedTemporaryFile(mode='w', delete=False) as f:\n            try:\n                exec(compile(f.read(), f.name, 'exec'), globals())\n            except Exception:\n                pass\n        filename = f.name\n        self.addCleanup(os.remove, filename)\n        self.assertNotIn(filename, list(autoreload.iter_all_python_module_files()))\n        self.assertIn(filename, autoreload._error_files)\n        try:\n            import_module('test_module')\n        except Exception:\n            pass\n        self.assertNotIn(filename, autoreload._error_files)\n", "    def test_iter_all_python_module_files_caches(self):\n        temp_file = self.temporary_file('module_test.py')\n        with open(temp_file, 'w') as f:\n            f.write('')\n        self.import_and_cleanup('module_test')\n        self.assertFileFound(temp_file)\n        # Change the file content\n        with open(temp_file, 'w') as f:\n            f.write('import sys')\n        self.assertFileFound(temp_file)\n        with open(temp_file, 'w') as f:\n            f.write('')\n\n        self.assertFileFound(temp_file)\n", "    def test_iter_modules_and_files_with_zip_file(self):\n        with tempfile.TemporaryDirectory() as tmpdir:\n            zip_path = Path(tmpdir) / 'my_module.zip'\n            with zipfile.ZipFile(zip_path, 'w') as zf:\n                zf.write('my_module.py', arcname='my_module.py')\n            extend_sys_path(tmpdir)\n            self.import_and_cleanup('my_module')\n            self.assertFileFound(zip_path)\n", "    def test_watching_modules_from_zip_archive(self):\n        with tempfile.TemporaryDirectory() as tempdir:\n            zip_filename = Path(tempdir) / 'my_module.zip'\n            with zipfile.ZipFile(zip_filename, 'w') as zf:\n                zf.writestr('__init__.py', '')\n                zf.writestr('module/my_module.py', 'def my_function(): pass')\n            self.import_and_cleanup('module.my_module')  \n            self.assertFileFound(zip_filename.absolute())\n            with open(zip_filename.resolve(), 'rb') as f:\n                modified_content = f.read()\n            with open(zip_filename.resolve(), 'wb') as f:\n                f.write(modified_content + b'modified')  \n            self.assertFileFound(zip_filename.absolute())\n\n", "    def test_iter_modules_and_files_with_zip_file_modules(self):\n        with tempfile.TemporaryDirectory() as tmpdir:\n            zip_filename = os.path.join(tmpdir, 'test.zip')\n            with zipfile.ZipFile(zip_filename, 'w') as zipf:\n                zipf.write('test_module.py', arcname='test_module.py')\n            extend_sys_path([tmpdir])\n            self.import_and_cleanup('test_module')\n\n            resolved_filename = Path(zip_filename).resolve(strict=True)\n            self.assertFileFound(resolved_filename)\n            \n            try:\n                with open(resolved_filename, 'rb') as f:\n                    zipf = zipfile.ZipFile(f)\n                    info = zipf.getinfo('test_module.py')\n                    self.assertFileFound(resolved_filename)\n            except:\n                self.assertFileFound(resolved_filename)\n\n"], "sample_928": ["    def test_default_role(app: Application, tmpdir: Path) -> None:\n        with default_role(app.builder.docname, 'test_role'):\n            # ... some code to check if the role is registered ...\n\n\n\n", "    def test_default_role():\n        with default_role(\"test_doc.rst\", \"my_role\"):\n            role_fn = docutils.get_role('')\n            assert role_fn is not None\n            \n        with default_role(\"test_doc.rst\", \"nonexistent_role\"):\n            role_fn = docutils.get_role('')\n            assert role_fn is None\n\n\n\n", "    def test_default_role(self):\n        with default_role('test.rst', 'testrole'):\n            # Test that the role is registered\n            assert 'testrole' in docutils.get_roles()\n        with default_role('test.rst', None):\n            # Test that no role is registered on empty name\n            assert len(docutils.get_roles()) == 0\n", "    def test_default_role(self):\n        with default_role('test.rst', 'my_role') as context:\n            role_fn = docutils.get_role('')\n            self.assertIsNotNone(role_fn)\n        with default_role('test.rst', 'unknown_role') as context:\n            role_fn = docutils.get_role('')\n            self.assertIsNone(role_fn)\n        self.assertEqual(logging.getLogger(__name__).getEffectiveLevel(), logging.WARNING)  \n\n\n\n", "    def test_default_role(self):\n        with default_role('test.rst', 'myrole'):\n            assert docutils.get_role('myrole') is not None\n        with default_role('test.rst', 'nonexistent_role'):\n            assert docutils.get_role('nonexistent_role') is None\n\n\n\n"], "sample_1130": ["    def test_partial_velocity_cyclic(self):\n        N = ReferenceFrame('N')\n        A = ReferenceFrame('A')\n        O = Point('O')\n        P = O.locatenew('P', 5 * N.x)\n        Q = P.locatenew('Q', 2 * A.y)\n        u1, u2 = dynamicsymbols('u1, u2')\n        O.set_vel(N, u1 * N.x)\n        P.set_vel(N, u2 * A.y)\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", category=UserWarning)\n            partials = P.partial_velocity(N, u1, u2)\n        assert (partials[0] == N.x) and (partials[1] == 2*A.y)\n\n\n\n", "    def test_partial_velocity_cyclic_graph(self):\n        N = ReferenceFrame('N')\n        A = ReferenceFrame('A')\n        p1 = Point('p1')\n        p2 = Point('p2')\n        p3 = Point('p3')\n        p1.set_vel(N, 10 * N.x)\n\n        p2.set_pos(p1, 5 * N.y)\n        p3.set_pos(p2, 5 * A.x)\n\n        u1 = dynamicsymbols('u1')\n        u2 = dynamicsymbols('u2')\n\n        p1.set_vel(N, u1 * N.x)\n        p2.set_vel(A, u2 * A.x)\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", category=UserWarning)\n            self.assertEqual(p3.partial_velocity(N, u1), 5 * A.x)\n\n\n\n", "    def test_partial_velocity_cyclic_dependency(self):\n        N = ReferenceFrame('N')\n        A = ReferenceFrame('A')\n        p1 = Point('p1')\n        p2 = Point('p2')\n        p3 = Point('p3')\n\n        p1.set_vel(N, 10 * N.x)\n        p2.set_pos(p1, 5 * N.y)\n        p3.set_pos(p2, 2 * A.x)\n\n        with ignore_warnings(category=UserWarning):\n            vel_p3_with_u1 = p3.partial_velocity(N, dynamicsymbols('u1'))\n            assert vel_p3_with_u1 == 5 * N.y.diff(dynamicsymbols('u1')) + 2 * A.x.diff(dynamicsymbols('u1')) \n\n\n\n", "    def test_partial_velocity_nonexistent(self):\n        N = ReferenceFrame('N')\n        A = ReferenceFrame('A')\n        p = Point('p')\n        p.set_vel(N, 10*N.x)\n        with raises(ValueError):\n            p.partial_velocity(N, dynamicsymbols('q'))\n\n\n\n", "    def test_v2pt_theory_cyclic(self):\n        with ignore_warnings(category=UserWarning) as w:\n            N = ReferenceFrame('N')\n            B = N.orientnew('B', 'Axis', [dynamicsymbols('q'), N.z])\n            O = Point('O')\n            P = O.locatenew('P', 10 * B.x)\n            Q = P.locatenew('Q', 5 * B.y)\n            O.set_vel(N, 5 * N.x)\n            P.v2pt_theory(O, N, B)\n            Q.v2pt_theory(P, N, B)\n            assert Q.vel(N).simplify() == (5*N.x + 10*dynamicsymbols('q', 1)*B.y + 5*dynamicsymbols('q2', 1)*B.y).simplify() \n"], "sample_189": ["    def test_incr_decr(self):\n        cache = cache\n        # Test basic increment/decrement\n\n        # Set a key with a value of 0\n        cache.set('key1', 0)\n\n        # Increment the key\n        self.assertEqual(cache.incr('key1'), 1)\n        self.assertEqual(cache.get('key1'), 1)\n        self.assertEqual(cache.incr('key1', delta=2), 3)\n\n        # Decrement the key\n        self.assertEqual(cache.decr('key1'), 2)\n        self.assertEqual(cache.get('key1'), 2)\n        self.assertEqual(cache.decr('key1', delta=3), -1)\n        self.assertEqual(cache.get('key1'), -1)\n\n        # Test increment/decrement with non-integer values\n        cache.set('key2', 10.5)\n        self.assertEqual(cache.incr('key2', delta=2.7), 13.2)\n        self.assertEqual(cache.get('key2'), 13.2)\n        self.assertEqual(cache.decr('key2', delta=1.5), 11.7)\n        self.assertEqual(cache.get('key2'), 11.7)\n", "    def test_memcache_key_warnings(self):\n        for key in [b'test', 'test', 'test\\r\\n', '1' * 251]:\n            warnings.filterwarnings('always', category=CacheKeyWarning)\n            cache.validate_key(key)\n            warnings.resetwarnings()\n\n\n\n\n", "    def test_get_or_set_callable(self):\n        cache = cache\n        key = 'my_key'\n        callable_value = lambda: 'callable_value'\n        with self.assertNumQueries(0):\n            value = cache.get_or_set(key, callable_value)\n        self.assertEqual(value, 'callable_value')\n        with self.assertNumQueries(0):\n            value = cache.get(key)\n        self.assertEqual(value, 'callable_value')\n", "    def test_incr_decr(self):\n        cache = caches['default']\n        cache.set('test', 0)\n\n        self.assertEqual(cache.incr('test'), 1)\n        self.assertEqual(cache.get('test'), 1)\n\n        self.assertEqual(cache.decr('test'), 0)\n        self.assertEqual(cache.get('test'), 0)\n\n        self.assertEqual(cache.incr('test', delta=5), 5)\n        self.assertEqual(cache.get('test'), 5)\n\n        self.assertEqual(cache.decr('test', delta=3), 2)\n        self.assertEqual(cache.get('test'), 2)\n\n\n", "    def test_key_validation(self):\n        # Test that the cache key validator raises a warning for keys\n        # that would cause errors with memcached.\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore', RuntimeWarning)\n            cache.set('this is a really long key that will exceed memcacheds '\n                      'maximum key length', 'value', version=1)\n            self.assertTrue(True, \"Did not raise a warning for a key \"\n                            \"too long for memcached\")\n            cache.set('this key has a character that will cause errors '\n                      'with memcached: \\x7F', 'value', version=1)\n            self.assertTrue(True, \"Did not raise a warning for a key \"\n                            \"with a problematic character\")\n"], "sample_63": ["    def test_select_template_with_no_templates(self):\n        with self.assertRaises(TemplateDoesNotExist):\n            self.engine.select_template([])\n\n\n\n", "    def test_select_template_with_valid_template(self):\n        template = self.engine.select_template(['my_template.html'])\n        self.assertIsInstance(template, self.engine.Template)\n", "    def test_select_template(self):\n        self.engine.dirs = [TEMPLATE_DIR, OTHER_DIR]\n        with open(os.path.join(TEMPLATE_DIR, 'template1.html'), 'w') as f:\n            f.write(\"Template 1\")\n        with open(os.path.join(OTHER_DIR, 'template2.html'), 'w') as f:\n            f.write(\"Template 2\")\n        t = self.engine.select_template(['template1.html', 'template2.html'])\n        self.assertEqual(t.origin.path, os.path.join(TEMPLATE_DIR, 'template1.html'))\n        with self.assertRaises(TemplateDoesNotExist):\n            self.engine.select_template(['template3.html'])\n", "    def test_select_template_raises_error_if_no_templates(self):\n        with self.assertRaises(TemplateDoesNotExist):\n            self.engine.select_template([])\n", "    def test_render_to_string_with_context(self):\n        template_name = 'test_template.html'\n        context = {'name': 'test'}\n        template_code = f\"\"\"\n        <h1>Hello, {{ name }}!</h1>\n        \"\"\"\n        with open(os.path.join(TEMPLATE_DIR, template_name), 'w') as f:\n            f.write(template_code)\n        rendered_string = self.engine.render_to_string(template_name, context)\n        self.assertEqual(rendered_string, '<h1>Hello, test!</h1>')\n"], "sample_723": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_815": ["    def test_multilabel_confusion_matrix_with_binary_labels(self):\n        y_true = np.array([[0, 1], [1, 0], [0, 0], [1, 1]])\n        y_pred = np.array([[0, 1], [1, 0], [0, 0], [1, 1]])\n        cm = multilabel_confusion_matrix(y_true, y_pred)\n        expected_cm = np.array([\n            [2, 0],\n            [0, 2]\n        ])\n        assert_array_equal(cm, expected_cm)\n", "compilation error", "    def test_hamming_loss_multilabel(self):\n        y_true = np.array([[0, 1], [1, 0], [1, 1], [0, 0]])\n        y_pred = np.array([[0, 1], [1, 0], [1, 1], [0, 0]])\n        with self.assertRaises(ValueError):\n            hamming_loss(y_true, y_pred)", "    def test_multilabel_confusion_matrix(multilabel):\n        if multilabel:\n            X, y_true, probas_pred = make_multilabel_classification(\n                n_samples=100, n_features=20, n_classes=5,\n                random_state=0,\n                n_samples_per_class=10,\n            )\n            # Use a different random state for y_true to ensure some classes\n            # are not entirely included in a single label\n            y_true = np.where(np.random.rand(*y_true.shape) < 0.5, 1, 0)\n            y_pred = (np.random.rand(*y_true.shape) > 0.5).astype(int)\n        else:\n            y_true, y_pred, probas_pred = make_prediction(binary=True)\n\n        cm = multilabel_confusion_matrix(y_true, y_pred)\n        if multilabel:\n            assert cm.shape == (5, 5)\n        else:\n            assert cm.shape == (2, 2)\n\n\n\n", "    def test_brier_score_loss(y_true, y_pred, sample_weight, normalize):\n        score = brier_score_loss(y_true, y_pred, sample_weight=sample_weight, normalize=normalize)\n        assert 0 <= score <= 1\n\n\n\n"], "sample_956": ["    def test_normalize_intersphinx_mapping_old_format(mock_read_from_url, mock_InventoryFile):\n        conf = {'project': 'test_project',\n                'intersphinx_mapping': {'old_format': 'http://example.com/inventory.inv'}}\n        normalize_intersphinx_mapping(None, conf)\n        assert conf['intersphinx_mapping'] == {'project': ('http://example.com/inventory.inv', ('http://example.com/inventory.inv', ))}\n", "    def test_resolve_reference_detect_inventory_split_target(mock_read,", "    def test_resolve_reference_detect_inventory(mock_InventoryFile, mock_read_from_url, app):\n        set_config(app, {'project1': ('http://example.com', ['objects.inv'],)})\n        mock_InventoryFile.return_value = inventory_v2()\n        mock_read_from_url.return_value = inventory_v2().read()\n        for mapping, target, expected_result in reference_check(app,\n                                                                 'project1',\n                                                                 'project1:", "    def test_normalize_intersphinx_mapping(mock_read_from_url, mock_inventory_file):\n        old_format = {\n            'domain1': 'http://example.com/domain1.inv',\n            'domain2': 'http://example.com/domain2.inv'\n        }\n        new_format = {\n            'domain1': ('Domain1', ('http://example.com/domain1.inv', None)),\n            'domain2': ('Domain2', ('http://example.com/domain2.inv', None))\n        }\n        normalize_intersphinx_mapping(None, old_format)\n        assert old_format == new_format\n\n        normalize_intersphinx_mapping(None, new_format)\n        assert new_format == new_format\n\n        invalid_name = {'invalid': 'http://example.com/invalid.inv'}\n        \n        with pytest.raises(ValueError):\n            normalize_intersphinx_mapping(None, invalid_name)        \n", "    def test_read_from_url_with_auth(mock_read, mock_inventoryfile):\n        mock_read.return_value = b'some contents'\n        uri = 'http://user:pass@example.com/path/inventory.inv'\n        with http_server(inventory_v2, uri=uri):\n            app = mock.Mock(spec=intersphinx_setup.__self__.__init__.__defaults__[0])\n            app.config = mock.Mock(spec=intersphinx_setup.__self__.__init__.__defaults__[1])\n            conf = app.config\n            conf.intersphinx_timeout = 10\n\n            inv = fetch_inventory(app, uri, INVENTORY_FILENAME)(mock_read)\n            assert inv is not None\n            assert isinstance(inv, bytes)\n"], "sample_5": ["    def test_model_evaluation_errors(model):\n        if model['class'] in NON_FINITE_LevMar_MODELS:\n            pytest.skip(f\"Model {model['class']} is known to cause issues with LevMarLSQFitter\")\n        if model['class'] in NON_FINITE_TRF_MODELS:\n            pytest.skip(f\"Model {model['class']} is known to cause issues with TRFLSQFitter\")\n        if model['class'] in NON_FINITE_LM_MODELS:\n            pytest.skip(f\"Model {model['class']} is known to cause issues with LMLSQFitter\")\n        if model['class'] in NON_FINITE_DogBox_MODELS:\n            pytest.skip(f\"Model {model['class']} is known to cause issues with DogBoxLSQFitter\")\n        try:\n            model_instance = model['class'](**model['parameters'])\n            x_values = np.linspace(model['bounding_box'][0][0], model['bounding_box'][0][1], 100)\n            y_values = np.array([model_instance(xi) for xi in x_values])\n        except Exception as e:\n            raise AssertionError(f\"Error evaluating model {model['class']}: {e}\")\n        # Check that the output is not infinity or NaN\n        assert np.isfinite(y_values).all()\n\n", "    def test_fit_model(model, fitters):\n        model_class = model['class']\n        params = model['parameters']\n        pars = model_class(**params)\n        data = pars(np.linspace(model['bounding_box'][0], model['bounding_box'][1], 100))\n        if 'bounding_box' in model:\n            data_with_noise = data + (np.random.rand(100) * 0.1 * data)\n        else:\n            data_with_noise = data + (np.random.rand(100) * 0.1 * data)\n        initial_params = {}\n        for param_name, param_value in params.items():\n            initial_params[param_name] = param_value * np.random.uniform(0.9, 1.1)\n        result = fitters[np.random.randint(0, len(fitters))].fit(\n            data_with_noise, pars, params=initial_params)\n        assert result.success\n        assert np.allclose(result.params, pars.values)\n", "    def test_model_bounding_box(model):\n        if 'bounding_box' in model:\n            cls = model['class']\n            params = model['parameters']\n            func = cls(**params)\n            bounding_box = func.bounding_box\n            # Check if bounding_box is set\n            assert bounding_box is not None\n            if isinstance(bounding_box, ModelBoundingBox):\n                # Check if the bounding_box is a valid instance with min and max values\n                assert bounding_box.min_value is not None\n                assert bounding_box.max_value is not None\n                # Ensure the bounding box values are within the expected range\n                for param in models['parameters']:\n                    if isinstance(param.value, (np.ndarray, list)):\n                        # Handle multi-dimensional parameters\n                        assert np.all(bounding_box.min_value <= param.value)\n                        assert np.all(bounding_box.max_value >= param.value)\n                    else:\n                        assert bounding_box.min_value <= param.value\n\n\n", "    def test_model_bounding_box(model):\n        cls = model['class']\n        params = model['parameters']\n        if 'bounding_box' in model:\n            bb = model['bounding_box']\n        else:\n            bb = None\n        \n        m = cls(**params)\n        if bb is not None:\n            assert isinstance(bb, ModelBoundingBox)\n            assert m.bounding_box is not None\n            assert m.bounding_box.min.shape == bb.min.shape\n            assert m.bounding_box.max.shape == bb.max.shape\n            assert np.allclose(m.bounding_box.min, bb.min)\n            assert np.allclose(m.bounding_box.max, bb.max)\n        else:\n            assert m.bounding_box is None\n", "    def test_model_bounding_box(self, model):\n        if 'bounding_box' in model:\n            model_instance = model['class'](**model['parameters'])\n            try:\n                model_instance.bounding_box\n            except AttributeError:\n                pass  # Some models don't have a bounding box\n            else:\n                assert isinstance(model_instance.bounding_box, ModelBoundingBox)\n        else:\n            pass\n"], "sample_975": ["    def test_unrad_depth():\n        from sympy.solvers.solvers import unrad\n        x = Symbol('x')\n        eq1 = sqrt(x) + sqrt(x + 1) - 2\n        eq2 = sqrt(x) + sqrt(x + 1) - 2\n        eq3 = sqrt(x**2) + sqrt(x + 1) - 2\n        eq4 = sqrt(x) + sqrt(x + 1)**2 - 2\n        eq5 = sqrt(x) + sqrt(x + 1)**3 - 2\n        eq6 = sqrt(x) + sqrt(x + 1)**4 - 2\n        eq7 = sqrt(x) + sqrt(x + 1) - 2**(1/2)\n\n        for eq in [eq1, eq2, eq3, eq4, eq5, eq6, eq7]:\n            _, cov = unrad(eq, x)\n            assert cov is None\n\n", "    def test_unrad_with_integer_powers():\n        x = Symbol('x')\n        eq = sqrt(x**2) + sqrt(x) - 2\n        eq, cov = unrad(eq)\n        assert eq == x + sqrt(x) - 2\n        assert cov is None\n", "    def test_unrad_poly_sqrt_eq_with_symbols_in_radicals(self):\n        x = Symbol('x')\n        y = Symbol('y')\n        eq = sqrt(x) + sqrt(y) - 2\n        eq1, cov = unrad(eq)\n        self.assertEqual(eq1, cov[0])\n", "    def test_unrad_sqrt_with_sum():\n        x, y = symbols('x y')\n        eq = sqrt(x + y) + sqrt(x - y) - 2\n        eq, cov = unrad(eq)\n        assert eq ==  sqrt(2*x + y**2) - 2\n        assert cov is None \n", "    def test_unrad_bivariate():\n        x, y = symbols('x y')\n        eq = sqrt(x**2 + y**2) - 1\n        eq, cov = unrad(eq)\n        assert eq == eq\n        assert cov is None\n\n\n"], "sample_368": ["    def test_migration_plan_handle_invalid_plans(self):\n        with self.assertRaises(InvalidMigrationPlan) as cm:\n            executor = MigrationExecutor(\n                connection, progress_callback=lambda *args: None\n            )\n            executor.migration_plan(\n                [(\"migrations\", \"0001_initial\"), (\"migrations\", \"0002_another_migration\")],\n                clean_start=False,\n            )\n        self.assertIn(\"Migration plans with both forwards and backwards migrations\", str(cm.exception))\n", "    def test_migration_plan_with_replacements(self):\n        with self.db.connection.set_context_manager(\n            connection.connection_features, {\"ignores_table_name_case\": True}\n        ):\n            # 1. Create migrations that include replacements.\n            with self.schema_editor() as schema_editor:\n                # migrations.Migration.objects.create(app=\"migrations\", name=\"0001_initial\", operations=[migrations.CreateModel(name='Model', fields=[\n                #     migrations.CharField(max_length=255),\n                # ])])\n                migrations.CreateModel(\n                    name=\"Model\", fields=[\n                        migrations.CharField(max_length=255),\n                    ]\n                ).migrate(schema_editor=schema_editor)\n                \n                migrations.AddField(\n                    model_name=\"Model\",\n                    name=\"new_field\",\n                    field=migrations.TextField(),\n                ).migrate(schema_editor=schema_editor)\n                \n            # 2.  Test the migration plan with replacements.\n            executor = MigrationExecutor(\n                self.connection, progress_callback=lambda *args: None\n            )\n            state = executor._create_project_state()\n            targets = [(\"migrations\", \"0001_initial\")]\n            plan = executor.migration_plan(targets, clean_start=True)\n            self.assertIn(\n                (executor.loader.graph.nodes[(\"migrations\", \"0001_initial\")], False),\n                plan\n            )\n            self.assertIn(\n                (executor.loader.graph.nodes[(\"migrations\", \"0002_added_new_field\")], False),\n                plan\n            )\n\n            # 3. Test if the replacements are correctly applied.\n            executor.migrate(targets, plan=plan, state=state)\n            self.assertEqual(\n                self.connection.introspection.table_names(),\n                [\"auth_user\", \"auth_group\", \"auth_permission\", \"auth_user_groups\", \"auth_user_permissions\", \"contenttypes_contenttype\", \"migrations_model\", ],\n            )\n            \n\n", "    def test_migrate_replacement_migrations(self):\n        with self.subTest(\"initial app migration\"):\n            with self.recorder.reset():\n                # Create a migration that replaces an existing one.\n                with self.execute_migration(app_label=\"migrations\", name=\"0002_replace_create_model\"):\n                    pass\n                self.recorder.create_replacement_migration(\n                    app_label=\"migrations\",\n                    name=\"0002_replace_create_model\",\n                    replaces=[\"migrations.test_migrations\", \"0001_initial\"]\n                )\n            self.assertEqual(\n                self.recorder.applied_migrations(\"migrations\", \"0002_replace_create_model\"),\n                self.recorder.applied_migrations(\"migrations\", \"0001_initial\"),\n            )\n\n        with self.subTest(\"subsequent app migration\"):\n            with self.recorder.reset():\n                # Run a subsequent migration for the same app,\n                # which shouldn't fail due to the replacement.\n                with self.execute_migration(app_label=\"migrations\", name=\"0003_second_migration\"):\n                    pass\n            self.assertMigrationApplied(\"migrations\", \"0003_second_migration\")\n\n\n\n", "    def test_replacements(self):\n        with self.recorder_patch():\n            with self.settings(MIGRATION_MODULES={\"migrations\": \"migrations.test_migrations\"}):\n                # Set up a migration plan with replacements\n                # and apply it.\n                executor = MigrationExecutor(self.connection)\n                plan = executor.migration_plan([\"migrations\"], clean_start=True)\n                executor.migrate([\"migrations\"], plan)\n\n                # Check that the replacements were recorded correctly.\n                applied_migrations = self.recorder.applied_migrations()\n                expected_applied_migrations = {\n                    (\"migrations\", \"0001_initial\"): True,\n                    (\"migrations\", \"0002_replace_auto_created\"): True,\n                }\n                self.assertEqual(applied_migrations, expected_applied_migrations)\n\n\n", "    def test_migration_plan(self):\n        with self.settings(INSTALLED_APPS=self.available_apps):\n            with self.connection.cursor() as cursor:\n                self.connection.introspection.delete_tables(cursor)\n\n            executor = MigrationExecutor(connection=self.connection)\n            plan = executor.migration_plan(\n                [(\"migrations\", \"0001_initial\")],\n                clean_start=True\n            )\n\n            self.assertEqual(len(plan), 1)\n            self.assertEqual(plan[0][0].name, \"0001_initial\")\n\n\n\n"], "sample_1057": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_929": ["    def test_parse_annotation(self):\n        # Test parsing of annotations with different types\n        self.assertEqual(_parse_annotation(\"None\", {}), \"None\")\n        self.assertEqual(_parse_annotation(\"str\", {}), \"str\")\n        self.assertEqual(_parse_annotation(\"int\", {}), \"int\")\n        self.assertEqual(_parse_annotation(\"list\", {}), \"list\")\n        self.assertEqual(_parse_annotation(\"dict\", {}), \"dict\")\n        self.assertEqual(_parse_annotation(\"typing.List[int]\", {}), \"List[int]\")\n        self.assertEqual(_parse_annotation(\"List[int]\", {}), \"List[int]\")\n        self.assertEqual(_parse_annotation(\"Optional[str]\", {}), \"Optional[str]\")\n        self.assertEqual(_parse_annotation(\"Union[str, int]\", {}), \"Union[str, int]\")\n\n\n\n", "    def test_parse_annotation(self):\n        assert _parse_annotation('int') == ('int', None)\n        assert _parse_annotation('int, str') == ('int', 'str')\n        assert _parse_annotation('int -> str') == ('int', 'str')\n        assert _parse_annotation('List[int]') == ('List', 'int')\n        assert _parse_annotation('Optional[str]') == ('Optional', 'str')\n        assert _parse_annotation('SomeCustomType') == ('SomeCustomType', None)\n        assert _parse_annotation('typing.List[int]') == ('List', 'int')\n\n\n\n", "    def test_py_sig_re_complex(self):\n        sig = \"def foo(a: int, b=5, *, kw=None) -> str: ...\"\n        expected = (\n            \"foo(\",\n            \"a: int\",\n            \"b=5\",\n            \"*, kw=None\",\n            \"-> str\",\n        )\n        actual = tuple(parse(sig).split())\n        assert actual == expected\n\n\n\n", "    def test_parse_annotation(self):\n        self.assertEqual(\n            _parse_annotation('int', {}),\n            (\n                'int',\n                None,\n                None,\n            )\n        )\n        self.assertEqual(\n            _parse_annotation('int: str', {}),\n            (\n                'int',\n                'str',\n                None,\n            )\n        )\n        self.assertEqual(\n            _parse_annotation('typing.List[int]', {}),\n            (\n                'typing.List[int]',\n                None,\n                None,\n            )\n        )\n        self.assertEqual(\n            _parse_annotation('typing.Optional[int]', {}),\n            (\n                'typing.Optional[int]',\n                None,\n                None,\n            )\n        )\n\n\n\n", "    def test_resolve_xref_no_namespace(self):\n        app = Mock()\n        env = Mock()\n        builder = Mock()\n        fromdocname = \"example\"\n        node = Mock(spec=pending_xref)\n        node.get.return_value = \"my_object\"\n        contnode = Mock(spec=addnodes.paragraph)\n        domain = PythonDomain(app, env)\n        domain.objects = {\n            \"my_object\": ObjectEntry(\"example\", \"myobject\", \"func\")\n        }\n\n        result = domain.resolve_any_xref(env, fromdocname, builder,\n                                         \"my_object\", node, contnode)\n\n        assert len(result) == 1\n        assert result[0] == (\"py:func\", result[0][1])\n\n\n\n"], "sample_1062": ["    def test_as_f_sign_1():\n        assert as_f_sign_1(x + 1) == (1, x, 1)\n        assert as_f_sign_1(x - 1) == (1, x, -1)\n        assert as_f_sign_1(-x + 1) == (-1, x, -1)\n        assert as_f_sign_1(-x - 1) == (-1, x, 1)\n        assert as_f_sign_1(2*x + 2) == (2, x, 1)\n        assert as_f_sign_1(-2*x - 2) == (-2, x, -1)\n        assert as_f_sign_1(x + 2*y) == (1, x + 2*y, 1)\n        assert as_f_sign_1(x - 2*y) == (1, x - 2*y, -1)\n\n\n\n", "compilation error", "    def test_osborne():\n        from sympy.simplify.fu import _osborne, _osbornei\n        from sympy.abc import x\n        assert _osborne(cosh(x), Dummy()) == cos(x)\n        assert _osbornei(cos(x), Dummy()) == cosh(x)\n        assert _osborne(sinh(x), Dummy()) == I*sin(x)\n        assert _osbornei(sin(x), Dummy()) == sinh(x)/I\n        assert _osborne(tanh(x), Dummy()) == I*tan(x)\n        assert _osbornei(tan(x), Dummy()) == tanh(x)/I\n        assert _osborne(sech(x), Dummy()) == sec(x)\n        assert _osbornei(sec(x), Dummy()) == sech(x)\n        assert _osborne(csch(x), Dummy()) == csc(x)/I\n        assert _osbornei(csc(x), Dummy()) == csch(x)*I\n        assert _osborne(coth(x), Dummy()) == cot(x)/I\n        assert _osbornei(cot(x), Dummy()) == coth(x)*I\n", "compilation error", "compilation error"], "sample_548": ["def test_colorbar_extension_length(request):\n    spacing = 'uniform' if request.node.name == 'test_colorbar_extension_length' else 'proportional'\n    fig = _colorbar_extension_length(spacing)\n    plt.close(fig)\n", "def test_ticklocation_positioning( ):\n    fig, ax = plt.subplots()\n\n    # Add some data to the axes to ensure the colorbar is oriented properly.\n    ax.plot([1, 2, 3], [4, 5, 6])\n\n    # Define the colorbar parameters.\n    cmap = cm.get_cmap('RdBu')\n    norm = Normalize()\n    \n    # Horizontal colorbar, test bottom and top tick locations\n    cax_bottom = plt.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap),\n                             ax=ax, location='bottom',\n                             ticklocation='bottom',\n                             pad=0.1)\n    cax_top = plt.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap),\n                          ax=ax, location='bottom',\n                          ticklocation='top',\n                          pad=0.1)\n\n    for c in [cax_bottom, cax_top]:\n        c.ax.set_title('Tick Location Test')\n\n    plt.draw()\n\n", "def test_colorbar_extension_length(request):\n    \"\"\"\n    Test that colorbar extensions have correct lengths.\n    \"\"\"\n    spacing = ['uniform', 'proportional']\n    for s in spacing:\n        with rc_context(axes_gridspec_kw={'gridspec_kw': {'hspace': 0.7,\n                                                      'wspace': 0.5}}):\n            fig = _colorbar_extension_length(s)\n        plt.close(fig)\n\n", "def test_colorbar_extension_length(tmpdir):\n    \"\"\"\n    Test colorbar extension length for uniform and proportional spacing.\n    \"\"\"\n    # These tests rely on visual comparison to confirm the length\n    # of the extensions in both uniform and proportional spacing cases.\n    fig = _colorbar_extension_length('uniform')\n    fig.savefig(str(tmpdir.join(\"colorbar_extension_length_uniform.png\")))\n    fig = _colorbar_extension_length('proportional')\n    fig.savefig(str(tmpdir.join(\"colorbar_extension_length_proportional.png\")))\n\n", "def test_colorbar_multiple_mappable():\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 10, 100)\n    y = np.sin(x)\n\n    line1, = ax.plot(x, y, label='Line 1')\n    line2, = ax.plot(x, y * 2, label='Line 2', color='red')\n\n    cb = Colorbar(ax, cmap='viridis', norm=Normalize(0, 1),\n                  label='Amplitude', extend='neither')\n\n    ax.legend()\n"], "sample_471": ["    def test_to_python_string_with_empty_value(self):\n        field = IntegerField(\n            label=\"Test Integer Field\",\n            empty_value=None,\n        )\n        self.assertEqual(field.to_python(\"\"), None)\n", "    def test_to_python_with_empty_string(self):\n        field = IntegerField()\n        self.assertEqual(field.to_python(\"\"), None)\n", "    def test_invalid_integer_with_required(self):\n        field = IntegerField(required=True)\n        with self.assertRaises(ValidationError):\n            field.clean(\"\")\n\n\n", "    def test_integer_field_clean(self):\n        field = IntegerField()\n        self.assertEqual(field.clean(''), 0)\n", "    def test_to_python_with_empty_string(self):\n        field = IntegerField()\n        self.assertEqual(field.to_python(\"\"), None)\n\n"], "sample_477": ["    def test_random_add(self):\n        self.assertEqual(self.render(\"random02\"), \"0 0\" )\n        self.assertEqual(self.render(\"random02\"), \"0 0\" )\n", "    def test_random_filter(self):\n        from django.utils.importlib import import_module\n        random_module = import_module('random')\n        original_random = random_module.choice\n        random_module.choice = lambda x: 1\n        self.assertTemplateResultEqual(\n            self.engine.render(\"template.txt\", {\"a\": range(10), \"b\": range(10)}),\n            mark_safe(\"1 1\")\n        )\n        random_module.choice = original_random\n", "    def test_random_filter(self):\n        with self.subTest(\"Random filter\"):\n            context = {\"a\": [1, 2, 3, 4, 5], \"b\": [\"apple\", \"banana\", \"cherry\"]}\n            result = self.engine.render(\"{{ a|random }} {{ b|random }}\")\n            self.assertIsInstance(result, mark_safe)\n            self.assertIn(str(random.choice([1, 2, 3, 4, 5])), result)\n            self.assertIn(random.choice([\"apple\", \"banana\", \"cherry\"]), result)\n", "    def test_random_filter_handles_empty_list(self):\n        context = {\"a\": [], \"b\": []}\n        self.assertEqual(self.render_context(\n            \"{{ a|random }} {{ b|random }}\", context), \" '' \")\n", "compilation error"], "sample_510": ["compilation error", "compilation error", "    def test_set_cmap_deprecation(monkeypatch):\n        with monkeypatch.context() as m:\n            m.warn.replace(category=MatplotlibDeprecationWarning)\n            set_cmap('viridis')\n        with mpl.rc_context({'axes.prop_cycle': plt.cycler(color=['r', 'g', 'b'])}):\n            fig, ax = plt.subplots()\n            ax.plot([1, 2, 3])\n            plt.show()\n", "        def test_set_cmap():\n            cmap_name = 'viridis'\n            plt.imshow(np.random.rand(10, 10))\n            # set_cmap should not modify the figure's default colormap\n            with plt.style.context('default'):\n                plt.imshow(np.random.rand(10, 10), cmap='viridis')\n                assert plt.cm.get_cmap(cmap_name) is plt.cm.viridis\n            \n            with plt.style.context('seaborn-whitegrid'):\n                plt.imshow(np.random.rand(10, 10), cmap='viridis')\n                assert plt.cm.get_cmap(cmap_name) is plt.cm.viridis\n    ", "    def test_plot_custom_cmap(tmpdir):\n        from matplotlib import cm\n        # Create a custom colormap\n        custom_cmap = cm.get_cmap(\"coolwarm\", 100)\n\n        # Create a figure and axes\n        fig, ax = plt.subplots()\n\n        # Plot data using the custom colormap\n        x = np.linspace(0, 1, 100)\n        y = np.sin(2 * np.pi * x)\n        im = ax.plot(x, y, cmap=custom_cmap)\n\n        # Save the figure to a temporary file\n        fig.savefig(tmpdir / \"test_cmap.png\")\n\n        # Load the image from the temporary file\n        img = plt.imread(tmpdir / \"test_cmap.png\")\n\n        # Check if the colormap was applied correctly by comparing the first 10 rows of the image\n        assert np.all(img[:10, :] == custom_cmap(np.linspace(0, 1, 10)))\n\n\n\n"], "sample_474": ["    def test_chr_unicode_characters(self):\n        self.assertEqual(\n            Chr(F(\"elena__name\")).resolve_expression(\n                connection=self.client.get_connection()\n            ),\n            \"CHR(\u00c9'\",\n        )\n\n\n\n", "    def test_chr_unicode(self):\n        ordinal = Ord(F('elena__name'))\n        chr_value = Chr(ordinal)\n        query_set = Author.objects.annotate(chr_result=chr_value).order_by('id')\n        result = query_set.values_list('chr_result', flat=True).get()\n        self.assertEqual(result, '\u00c9')\n", "    def test_chr_oracle(self):\n        result = Chr(F('name')).as_sqlite(\n            compiler=None, connection=None\n        )\n        self.assertEqual(result.template, \"CHR(%(expressions)s USING NCHAR_CS)\")\n\n\n", "    def test_chr(self):\n        self.assertEqual(Chr(F('name'), 1).resolve_expression(self.john), 'J')\n        self.assertEqual(Chr(F('name'), 10).resolve_expression(self.elena), ' ')\n", "    def test_chr(self):\n        self.assertEqual(Chr(F(\"alias\")).resolve_expression(self.john), 'j')\n        self.assertEqual(Chr(F(\"alias\")).resolve_expression(self.elena), 'e')\n\n\n"], "sample_238": ["    def test_coalesce(self):\n        with self.assertNumQueries(1):\n            with CaptureQueriesContext() as context:\n                result = (\n                    Book.objects\n                    .annotate(\n                        price_or_zero=Coalesce('price', Value(0))\n                    )\n                    .values('price_or_zero')\n                )\n\n        self.assertContainsSQL(context.captured_queries[0], 'COALESCE(price, 0)')\n\n\n\n        self.assertEqual(result.count(), Book.objects.count())\n        self.assertEqual(\n            sorted(result),\n            sorted([\n                {'price_or_zero': Decimal('30.00')},\n                {'price_or_zero': Decimal('23.09')},\n                {'price_or_zero': Decimal('29.69')},\n                {'price_or_zero': Decimal('29.69')},\n                {'price_or_zero': Decimal('82.80')},\n                {'price_or_zero': Decimal('75.00')},\n            ])\n        )\n\n", "    def test_power_function(self):\n        with CaptureQueriesContext() as captured:\n            result = Book.objects.filter(\n                price__gt=Decimal('25.00')\n            ).annotate(\n                squared_price=Func(\n                    'price', function='POWER', input_params=[2]\n                )\n            ).values('squared_price')\n            self.assertEqual(captured.query, \"SELECT \\\"app_book\\\".id, POWER(\\\"app_book\\\".price, 2) AS \\\"squared_price\\\" FROM \\\"app_book\\\" WHERE \\\"app_book\\\".price > ?\", \"Incorrect query generated for power function\")\n            self.assertCountEqual(\n                list(result),\n                [\n                    {'squared_price': Decimal('900.00')},\n                    {'squared_price': Decimal('6656.100000000001')},\n                    {'squared_price': Decimal('6051.610000000001')},\n                ]\n            )\n\n\n", "    def test_random_mysql(self):\n        with self.assertNumQueries(1):\n            random_value = Random().as_mysql().resolve()\n", "    def test_coalesce(self):\n        with self.assertNumQueries(1):\n            result = Book.objects.annotate(\n                price_or_zero=Coalesce(F('price'), Value(0))\n            ).values('price_or_zero')\n        self.assertCountEqual(\n            result,\n            [{'price_or_zero': Decimal('30.00')},\n             {'price_or_zero': Decimal('23.09')},\n             {'price_or_zero': Decimal('29.69')},\n             {'price_or_zero': Decimal('29.69')},\n             {'price_or_zero': Decimal('82.80')},\n             {'price_or_zero': Decimal('75.00')}]\n        )\n\n\n\n", "    def test_coalesce(self):\n        with self.assertNumQueries(1):\n            result = Book.objects.filter(isbn='159059725').annotate(\n                isbn_or_null=Coalesce('isbn', Value(None))\n            ).values('isbn_or_null')\n        self.assertEqual(result[0]['isbn_or_null'], '159059725')\n\n        with self.assertNumQueries(1):\n            result = Book.objects.filter(isbn='123456789').annotate(\n                isbn_or_null=Coalesce('isbn', Value(None))\n            ).values('isbn_or_null')\n        self.assertEqual(result[0]['isbn_or_null'], None)\n"], "sample_111": ["    def test_ordering_multi_fields(self):\n        admin.site.unregister(Band)\n        admin.site.register(Band, BandAdmin)\n        admin.site.register(Child, ChildAdmin)\n\n        request = self._mocked_authenticated_request(\n            reverse('admin:band_changelist'), self._create_superuser('user')\n        )\n        cl = BandAdmin.get_changelist_from_request(request)\n        ordering = cl.get_ordering(request, cl.queryset)\n        self.assertEqual(ordering, ['-name', '-genre__name'])\n", "    def test_get_filters(self):\n        # Test the get_filters method with various filter scenarios\n        \n        for model_admin in [\n            BandAdmin, CharPKAdmin, ChildAdmin, ChordsBandAdmin,\n            ConcertAdmin, EventAdmin, FilteredChildAdmin,\n            DynamicListFilterChildAdmin, DynamicListDisplayChildAdmin, \n            DynamicListDisplayLinksChildAdmin, EmptyValueChildAdmin, GroupAdmin,\n            InvitationAdmin, QuartetAdmin, SwallowAdmin,\n        ]:\n            with isolate_apps(apps=['admin_changelist']):\n                admin.site.register(model_admin.model, model_admin)\n                request = self._mocked_authenticated_request(reverse('admin:admin_changelist'), self.superuser)\n                \n                cl = ChangeList(request, model_admin.model, [], [], [],\n                                [], [], [], 10, 100, [], model_admin, [('field_name',)])\n                \n                # Test with no filters\n                self.assertEqual(cl.has_filters, False)\n                self.assertEqual(cl.filter_specs, [])\n                self.assertEqual(cl.get_filters(request), ([], False, {}, False))\n            \n                # Test with custom field list filter\n                request.GET[model_admin.model._meta.app_label + '_' + model_admin.model._meta.model_name + '_filter'] = 'value1'\n                self.assertEqual(cl.has_filters, True)\n                # Check if filter was processed correctly\n                for filter_spec in cl.filter_specs:\n                    self.assertTrue(isinstance(filter_spec, model_admin.CustomListFilter))\n                    self.assertEqual(filter_spec.value, 'value1')\n\n                # Test with custom field list filter\n                request.GET[model_admin.model._meta.app_label + '_' + model_admin.model._meta.model_name + '_filter_other'] = 'value2'\n                self.assertEqual(cl.has_filters, True)\n                # Check if filter was processed correctly\n                for filter_spec in cl.filter_specs:\n                    self.assertTrue(isinstance(filter_spec, model_admin.CustomListFilter))\n                    self.assertEqual(filter_spec.value", "    def test_changelist_search_form_filters_query_params(self):\n        band1 = Band.objects.create(name='Band 1')\n        band2 = Band.objects.create(name='Band 2')\n        request = self._mocked_authenticated_request(reverse('admin:band_changelist'), self.superuser)\n        changelist = ChangeList(request, Band, ['name'], [], [], [], ['name'], False, [], [], [], self._create_superuser('admin'))\n        # Add a few bands with and without \"1\" in their names\n        for i in range(3):\n            Band.objects.create(name=f'Band {i+1}')\n        \n        # Test with a query parameter\n        request.GET = {\n            SEARCH_VAR: '1'\n        }\n        changelist.get_queryset(request)\n        self.assertEqual(changelist.queryset.count(), 3)\n        \n        # Test without query parameter\n        request.GET = {}\n        changelist.get_queryset(request)\n        self.assertEqual(changelist.queryset.count(), 6)\n\n\n        # Test with a query parameter and an existing filter\n        request.GET = {\n            SEARCH_VAR: '1',\n            'band__name__icontains': 'Band'\n        }\n        changelist.get_queryset(request)\n        self.assertEqual(changelist.queryset.count(), 3) \n", "    def test_get_ordering_field_columns(self):\n        # Check if `get_ordering_field_columns` works with default ordering.\n        admin_site = custom_site\n        admin_site.register(OrderedObject, OrderedObjectAdmin)\n        \n        # Create a sample request.\n        request = self._mocked_authenticated_request(reverse('admin:orderedobject_changelist'), self.superuser)\n        changelist = OrderedObjectAdmin.get_changelist(request, OrderedObject.objects.all())\n\n        ordering_fields = changelist.get_ordering_field_columns()\n        self.assertEqual(ordering_fields, {\n            0: 'asc',\n            1: 'asc',\n        })\n\n", "    def test_get_ordering_field_columns(self):\n        admin_site = custom_site\n        self.assertEqual(admin_site.get_view_by_name('child_changelist').get_ordering_field_columns(), {})\n\n\n        band = Band.objects.create(name='Band1')\n        Child.objects.create(band=band, name='Child1')\n        self.assertEqual(admin_site.get_view_by_name('child_changelist').get_ordering_field_columns(), {'0': 'asc'})\n\n\n\n        \n\n"], "sample_924": ["compilation error", "    def test_enum_member_descriptions():\n        _check(\"enum-member\", \"enum-member { int v }\", {},\n               \"enum-member { int v }\", key=\"v\",\n               asTextOutput=\"int v \")\n", "    def check_template_param(name, input, output):\n        ast = parse(name, input)\n        # assert ast.template_parameters not in (None, [])\n        assert ast.template_parameters is not None\n        rootSymbol = Symbol(None, None, None, None, None, None)\n        symbol = rootSymbol.add_declaration(ast, docname=\"TestDoc\")\n\n        parentNode = addnodes.desc()\n        signode = addnodes.desc_signature(input, '')\n        parentNode += signode\n        ast.describe_signature(signode, 'lastIsName', symbol, options={})\n        resAsText = parentNode.astext()\n        if resAsText != output:\n            print(\"\")\n            print(\"Input:    \", input)\n            print(\"astext(): \", resAsText)\n            print(\"Expected: \", output)\n            raise DefinitionError(\"\")\n\n\n", "    def test_member_function(\n        ) -> None:\n        check('member',\n         'void member_function(int i) { }',\n         {\"1\": 'member_function_1'},\n         'void member_function(int i) { }',\n         key='member_function_1')\n\n", "    def test_cpp_function_with_template_args_and_return_type(\n        self"], "sample_622": ["    def test_bool_type_array(self):\n        data = np.array([True, False, True], dtype=bool)\n        var = Variable(\n            dims=(\"x\",),\n            data=data,\n            attrs={\"dtype\": \"bool\"},\n        )\n        encoded_var = BoolTypeArray(var)\n        assert_array_equal(encoded_var.data, data)\n        assert encoded_var.dtype.kind == \"O\"\n\n", "    def bool_data():\n        return np.array([True, False, True, True], dtype=bool)\n", "    def test_bool_type_array(self, dtype):\n        arr = np.array([True, False, True], dtype=dtype)\n        var = Variable(dims=(\"x\",), data=arr)\n        with warnings.catch_warnings():\n            warnings.simplefilter(\n                \"ignore\", category=SerializationWarning\n            )\n            var = bool_type_array(var)\n        assert var.dtype == np.bool_\n\n\n", "    def test_bool_type_array(self, dtype):\n        var = Variable(\n            dims=(\"x\", \"y\"),\n            data=np.random.randint(2, size=(10, 10)).astype(dtype),\n        )\n        var_copy = var.copy(deep=False)\n        assert isinstance(var_copy.data, BoolTypeArray)\n        assert var_copy.dtype == dtype\n\n\n\n", "    def test__bool_type_array_encoding(self):\n        data = np.array([True, False, True, False], dtype=bool)\n        var = Variable(dims=(\"x\",), data=data, dtype=bool)\n\n        encoded_var = var.copy(deep=True)\n        encoded_var.encoding[\"dtype\"] = \"bool\"\n        assert encoded_var.dtype == \"bool\"\n\n\n\n"], "sample_27": ["    def test_fit_diff_with_missing_data(self):\n        hdu_a = PrimaryHDU(\n            data=np.arange(10).reshape((2, 5)), header=Header({\"TEST\": \"VALUE\"})\n        )\n        hdu_b = PrimaryHDU(data=np.arange(10).reshape((2, 5)), header=Header({\"TEST\": \"DIFFERENT VALUE\"}))\n        diff = FITSDiff(hdu_a, hdu_b)\n        self.assertEqual(\n            diff.common_keywords,\n            {\"TEST\"},\n        )\n        self.assertEqual(\n            diff.diff_keywords,\n            {\"TEST\"},\n        )\n        self.assertEqual(diff.diff_keyword_count, (1, 1))\n        self.assertIn(\n            \"  \" \"  Header keyword values differ:\\nHEADER\",\n            diff._report().splitlines()[0],\n        )\n \n\n\n\n", "    def test_diff_empty_tables(self):\n        hdulist_a = HDUList()\n        hdulist_b = HDUList()\n\n        diff = TableDataDiff(hdulist_a, hdulist_b)\n        diff._diff()\n\n        assert diff.diff_values == []\n        assert diff.diff_total == 0\n        assert diff.diff_ratio == 0\n\n\n\n", "    def test_diff_raw_data(self):\n        a = DummyNonstandardExtHDU(np.array([1, 2, 3, 4]))\n        b = DummyNonstandardExtHDU(np.array([2, 3, 4, 5]))\n\n        diff = RawDataDiff(a, b)\n        diff._diff()\n        self.assertEqual(diff.diff_bytes, [(0, 1)])\n        self.assertEqual(diff.diff_total, 1)\n        self.assertEqual(diff.diff_ratio, 0.25)\n\n        self.assert_equal_ignore_case(\n            diff._report().strip(), \"Data sizes differ:\\n  a: 4 bytes\\n  b: 4 bytes\\n Data differs at byte 0:\\n  [ImageHDU object at 0x...] 1 [ImageHDU object at 0x...] 2\\n 1 different byte(s) found (25.00% different).\"\n        )\n\n\n\n", "    def test_table_diff_with_different_formats(self):\n        hdu1 = BinTableHDU(\n            data=[\n                [1.0, 2.0],\n                [3.0, 4.0],\n            ],\n            columns=[\n                Column(name=\"col1\", format=\"D\", unit=\"km/s\"),\n                Column(name=\"col2\", format=\"E\", unit=\"Jy\"),\n            ],\n        )\n        hdu2 = BinTableHDU(\n            data=[\n                [1.0, 2.0],\n                [3.0, 4.0],\n            ],\n            columns=[\n                Column(name=\"col1\", format=\"F\", unit=\"km/s\"),\n                Column(name=\"col2\", format=\"E\", unit=\"Jy\"),\n            ],\n        )\n\n        diff = TableDataDiff(hdu1, hdu2)\n\n        assert diff.diff_column_attributes\n\n", "    def test_fitsdiff_image_data(self):\n        with fits.open(self.get_data_filename(\"test_image_diff.fits\")) as hdul:\n            a = hdul[0]\n            b = hdul[1]\n\n        diff = ImageDataDiff(a, b)\n        assert diff.diff_dimensions == ((1024, 1024), (1024, 1024))\n\n        with self.assert_warning_count(1, category=UserWarning) as warnings:\n            diff = ImageDataDiff(a, b, rtol=0.001, atol=0.0001)\n        assert warnings[0].msg == _NOT_OVERWRITING_MSG_MATCH\n        assert diff.diff_pixels == [\n            (\n                (0, 0),\n                (np.array([1.1], dtype=float), np.array([1.2], dtype=float)),\n            )\n        ]\n\n        diff = ImageDataDiff(a, b, numdiffs=1)\n        assert diff.diff_pixels == [\n            (\n                (0, 0),\n                (np.array([1.1], dtype=float), np.array([1.2], dtype=float)),\n            )\n        ]\n        assert diff.diff_ratio == 0.0000001\n        assert diff.diff_total == 1\n"], "sample_686": ["compilation error", "    def test_deprecated_warnings(testdir):\n        testdir.makepyfile(\"\"\"\n        import pytest\n\n        @pytest.mark.parametrize(\"arg\", [\"positional\", \"allowed_keyword\"])\n            assert arg == \"positional\"\n            assert arg\n        \"\"\")\n\n        result = testdir.runpytest()\n        result.stdout.reassert_warnings(deprecated.FIXTURE_POSITIONAL_ARGUMENTS)\n        result.stdout.reassert_warnings(deprecated.RESULT_LOG)\n\n", "    def test_deprecated_plugins(testdir):\n        testdir.makepyfile(\"\"\"\n        import pytest\n\n            pytest.skip('This test should be skipped')\n        \"\"\")\n        result = testdir.runpytest()\n        assert result.ret() == 0\n        # This will fail and trigger a warning, but we're checking for the warning.\n        result = testdir.runpytest(\"--plugin=pytest_catchlog\")\n        assert result.ret() == 0\n", "    def test_collect_directory_hook(testdir):\n        testdir.makepyfile(\n            \"\"\"\n                pass\n            \"\"\"\n        )\n        result = testdir.runpytest(\"--collect-only\", \"-v\")\n        assert result.stdout.fnmatch_lines(\n            [\n                \"*pytest_collect_directory hook is deprecated*\",\n                \"*Please use collect_ignore in conftests or pytest_collection_modifyitems.*\",\n            ]\n        )\n", "    def test_deprecation_messages(testdir):\n        for warning_name in sorted(dir(deprecated)):\n            if isinstance(getattr(deprecated, warning_name), warnings.WarningClass):\n                with testdir.parse(f\"test_deprecation.py\"):\n                    result = testdir.runpytest()\n                    assert result.ret == 0, f\"Test failed for warning {warning_name}\"\n                    for line in result.stdout.splitlines():\n                        if str(getattr(deprecated, warning_name)).split(\"\\n\")[0] in line:\n                            break\n                    else:\n                        raise AssertionError(\n                            f\"Warning {warning_name} not present in output\"\n                        )\n"], "sample_315": ["    def test_redirect_to_language_specific_url(self):\n        factory = RequestFactory()\n        request = factory.get('/en/some/page/')\n        response = PermanentRedirectLocaleMiddleWare().process_request(request)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.get('Content-Language'), 'en')\n", "    def test_redirect_to_language_specific_url_with_missing_language(self):\n        factory = RequestFactory()\n        request = factory.get('/about/')\n        request.META['HTTP_ACCEPT_LANGUAGE'] = 'nl'\n\n        middleware = LocaleMiddleware()\n        response = middleware.process_request(request)\n        self.assertEqual(response.status_code, 301)\n        self.assertEqual(response['Location'], '/nl/about/')\n\n        # Test with a slash\n        middleware = PermanentRedirectLocaleMiddleWare()\n        response = middleware.process_request(request)\n        self.assertEqual(response.status_code, 301)\n        self.assertEqual(response['Location'], '/nl/about/')\n\n", "    def test_redirect_to_valid_language_url(self):\n        factory = RequestFactory()\n        request = factory.get('/articles/')\n        request.LANGUAGE_CODE = None\n        request.META['HTTP_ACCEPT_LANGUAGE'] = 'nl'\n        response = LocaleMiddleware().process_request(request)\n        self.assertEquals(response.status_code, 200)\n        self.assertEquals(response['Content-Language'], 'nl')\n\n        request = factory.get('/articles/')\n        request.LANGUAGE_CODE = None\n        request.META['HTTP_ACCEPT_LANGUAGE'] = 'en'\n        response = LocaleMiddleware().process_request(request)\n        self.assertEquals(response.status_code, 200)\n        self.assertEquals(response['Content-Language'], 'en')\n", "    def test_redirect_with_script_prefix(self):\n        response = self.client.get('/en/about/')\n        self.assertEqual(response.status_code, 301)\n        self.assertEqual(response['Location'], '/locale/en/about/')\n", "    def test_redirect_to_default_language_with_missing_language_prefix(self):\n        # Simulate a request in English.\n        request = RequestFactory().get('/en/test/')\n        response = self.process_request(request)\n\n        self.assertEqual(response.status_code, 301)\n        self.assertEqual(response['Location'], '/en/test/')\n\n"], "sample_899": ["    def test_not_invariant_predict(self):\n        estimator = NotInvariantPredict()\n        X = np.array([[1, 2], [3, 4]])\n        y_pred = estimator.predict(X)\n        assert_array_equal(y_pred, np.ones(X.shape[0]))\n\n        X = np.array([[1]])\n        y_pred = estimator.predict(X)\n        assert_array_equal(y_pred, np.zeros(X.shape[0]))\n\n\n\n", "    def test_sparse_transformers(self):\n        with ignore_warnings(category=DeprecationWarning):\n            est = SparseTransformer()\n            X = np.array([[1, 2], [3, 4], [5, 6]])\n            y = np.array([1, 2, 3])\n            X_sparse = sp.csr_matrix(X)\n            est.fit_transform(X_sparse, y)\n            transformed_data = est.transform(X_sparse)\n            assert isinstance(transformed_data, sp.csr_matrix)\n            assert transformed_data.shape == (3, 2)\n\n            # Test case with 64-bit indices\n            X_64_sparse = sp.csr_matrix(\n                (np.ones(X.shape[0]),\n                 np.arange(X.shape[0] * X.shape[1]).reshape((X.shape[0], X.shape[1])),\n                 shape=X.shape),\n                dtype='int64'\n            )\n            with assert_raises(ValueError):\n                est = SparseTransformer()\n                est.fit_transform(X_64_sparse, y)\n\n\n\n", "    def fit(self, X, y=None):\n        X, y = check_X_y(X, y,\n                         accept_sparse=(\"csr\", \"csc\"),\n                         multi_output=True,\n                         y_numeric=True)\n        return self\n", "    def test_sparse_transformer(self):\n        transformer = SparseTransformer()\n\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        X_sparse = sp.csr_matrix(X)\n        transformer.fit(X_sparse)\n\n        X_new = np.array([[7, 8], [9, 10]])\n        X_new_sparse = transformer.transform(X_new)\n\n        assert_array_equal(X_new_sparse.dtype, X_sparse.dtype)\n        assert_array_equal(X_new_sparse.shape, X_new.shape)\n\n\n\n", "    def __init__(self, sparse_output=True):\n        self.sparse_output = sparse_output\n"], "sample_155": ["    def test_file_response_with_filelike(self):\n        with tempfile.TemporaryFile() as temp_file:\n            temp_file.write(b'This is some test data.')\n            temp_file.seek(0)\n            response = FileResponse(temp_file, as_attachment=True, filename='test.txt')\n            self.assertEqual(response.status_code, 200)\n            self.assertEqual(response['Content-Type'], 'application/octet-stream')\n            self.assertEqual(response['Content-Disposition'], 'attachment; filename=\"test.txt\"')\n            self.assertEqual(response['Content-Length'], len(temp_file.read()))\n", "    def test_file_response_content_length(self):\n        with tempfile.TemporaryFile('wb') as f:\n            f.write(b'some content')\n            f.seek(0)\n            response = FileResponse(f)\n            self.assertEqual(response['Content-Length'], str(len(f.read())))\n\n", "    def test_file_response_with_no_filename(self):\n        with tempfile.NamedTemporaryFile() as tmp:\n            tmp.write(b'test')\n            tmp.flush()\n            response = FileResponse(tmp.file, as_attachment=False)\n            self.assertEqual(response['Content-Disposition'], 'inline')\n            self.assertNotIn('filename', response)\n", "    def test_file_response_with_file_name(self):\n        with tempfile.TemporaryFile() as f:\n            f.write(b'some file content')\n            f.seek(0)\n            response = FileResponse(f, as_attachment=True, filename='my_file.txt')\n            self.assertEqual(response['Content-Disposition'], 'attachment; filename=\"my_file.txt\"')\n            self.assertEqual(response['Content-Type'], 'application/octet-stream')\n\n            # Check if headers are set correctly for large files\n            large_file_size = 1024 * 1024  \n            with tempfile.NamedTemporaryFile(\n                mode='wb', delete=False, suffix='.txt',\n            ) as large_f:\n                large_f.write(b'a' * large_file_size)\n                large_f.seek(0)\n                large_response = FileResponse(large_f, as_attachment=True, filename='large_file.txt')\n                self.assertEqual(large_response['Content-Disposition'], 'attachment; filename=\"large_file.txt\"')\n\n                # Make sure content length is set\n                self.assertTrue(\n                    'Content-Length' in large_response,\n                    f\"Content-Length header not found in response for large file\"\n                ) \n                os.remove(large_f.name)\n\n\n\n", "    def test_file_response_with_absolute_path_and_known_size(self):\n        with tempfile.NamedTemporaryFile(mode='wb', delete=False) as f:\n            f.write(b'This is a test file.')\n            filename = f.name\n            response = FileResponse(open(filename, 'rb'), as_attachment=True)\n            self.assertEqual(response['Content-Length'], os.path.getsize(filename))\n            os.remove(filename)\n"], "sample_161": ["    def test_check_through_fields(self):\n        class MyModel(models.Model):\n            pass\n\n        with mock.patch('django.db.models.Field.get_field') as mock_get_field:\n            mock_get_field.side_effect = lambda model, name: models.ForeignKey(MyModel, on_delete=models.CASCADE)\n\n            class Meta(object):\n                db_table = 'my_model'\n            class Meta(object):\n                app_label = 'invalid_models_tests'\n\n            class MyManyToManyField(models.ManyToManyField):\n                    super().__init__(to, *args, **kwargs)\n                    self.through = through\n\n            field = MyManyToManyField(\n                to=MyModel,\n                through='invalid_models_tests.MyIntermediate',\n                through_fields=('field1', 'field2')\n            )\n            instance = MyModel()\n            errors = field.check(instance=instance)\n            self.assertEqual(len(errors), 0) \n\n", "    def test_many_to_many_intermediary_model_creation(self):\n        from invalid_models_tests.models import Person, Tag\n\n        with override_settings(DATABASE_ROUTERS=None):\n            # Test the creation of the intermediary model when not provided\n            # and when the model is not abstract.\n\n            # Create the models.\n            model = models.Model(\n                name='Taggable',\n            )\n            model._meta.abstract = False\n            field = models.ManyToManyField(\n                Tag,\n                related_name='taggable_tags',\n                through='invalid_models_tests.TaggableTag',\n            )\n            field.contribute_to_class(model, 'tags')\n            \n            # Assert that the intermediary model is created.\n            self.assertTrue(hasattr(model, 'tags'))\n            self.assertIsInstance(getattr(model, 'tags'), models.ManyToManyField)\n\n            # Assert that the intermediary table exists.\n", "    def test_m2m_table_name_clash_with_routed_db(self):\n        class Model1(models.Model):\n            class Meta:\n                app_label = 'invalid_models_tests'\n\n        router = mock.MagicMock(spec=connection.Router)\n        router.db_for_write.return_value = 'db_a'\n        with override_settings(DATABASE_ROUTERS=[router]):\n            Model1.objects.create(pk=1)\n            class Model2(models.Model):\n                class Meta:\n                    app_label = 'invalid_models_tests'\n        with self.assertRaises(Error) as cm:\n            Model2._meta.get_field('m2m_field')\n        self.assertIn(\n            'The field\\'s intermediary table \\'invalid_models_tests_model1_model2\\''\n            ' clashes with the table name of \\'invalid_models_tests_model1\\'',\n            str(cm.exception),\n        )\n\n\n", "    def test_check_through_fields_validation(self):\n        with override_settings(DATABASE_ROUTERS=None):\n            class MyModel(models.Model):\n                pass\n            class RelatedModel(MyModel):\n                pass\n            class IntermediateModel(models.Model):\n                my_field = models.ForeignKey(MyModel)\n                related_field = models.ForeignKey(RelatedModel)\n\n            class MyForm(models.Model):\n                name = models.CharField(max_length=255)\n                items = models.ManyToManyField(RelatedModel, through=IntermediateModel, through_fields=('my_field', 'related_field'))\n            with self.assertRaises(models.Error):\n                MyForm._meta.get_field('items').check()\n\n\n\n", "    def test_create_many_to_many_intermediary_model(self):\n        from invalid_models_tests import Author, Book\n        with connection.cursor() as cursor:\n            cursor.execute(\"CREATE TABLE invalid_models_tests_author (id INT PRIMARY KEY, name VARCHAR(255));\")\n            cursor.execute(\"CREATE TABLE invalid_models_tests_book (id INT PRIMARY KEY, title VARCHAR(255));\")\n            cursor.execute(\"CREATE TABLE invalid_models_tests_author_book (id INT PRIMARY KEY, author_id INT, book_id INT, FOREIGN KEY (author_id) REFERENCES invalid_models_tests_author(id), FOREIGN KEY (book_id) REFERENCES invalid_models_tests_book(id));\")\n\n        self.assertEqual(Author.objects.all().count(), 0)\n        self.assertEqual(Book.objects.all().count(), 0)\n        intermediary_model = create_many_to_many_intermediary_model(\n            models.ForeignKey(Author, on_delete=models.CASCADE, related_name='books'),\n            Author\n        )\n        self.assertEqual(intermediary_model._meta.db_table, 'author_book')\n        \n        \n"], "sample_1146": ["  def test_multiline_latex_bad_environment(self):\n      from sympy import multiline_latex\n      multiline_latex(x, 1 + 2*x, environment=\"bad_env\") \n\n\n\n", "    def test_totient_symbol(self):\n        self.assertEqual(latex(divisor_sigma(2), mat_symbol_style='plain'),\n                         '\\\\sigma_{2}')\n        self.assertEqual(latex(reduced_totient()), '\\\\sigma')\n        self.assertEqual(latex(primenu(2)), '\\\\nu_{2}')\n        self.assertEqual(latex(primeomega(2)), '\\\\Omega_{2}')\n\n\n", "def test_multiline_latex_order():\n    from sympy import symbols, sin, cos, exp, log, I\n    x, y, alpha = symbols('x y alpha')\n    expr = sin(alpha*y) + exp(I*alpha) - cos(log(y))\n    result = multiline_latex(x, expr, terms_per_line=2, environment=\"align*\")\n    assert ' ' in result\n\n\n\n", "    def test_latex_order(self):\n        x, y = symbols('x y')\n        expr = x**2 + x*y\n        for order in ['lex', 'grlex', 'grevlex']:\n            result1 = latex(expr, order=order)\n            result2 = latex(expr, order=None)\n            assert result1 != result2, f\"Order {order} should change the output\"\n\n\n\n        expr = x*y**2\n        for order in ['lex', 'grlex', 'grevlex']:\n            result1 = latex(expr, order=order)\n            result2 = latex(expr, order=None)\n            assert result1 != result2, f\"Order {order} should change the output\"\n", "    def test_totient_divisor_sigma(self):\n        self.assertEqual(latex(divisor_sigma(x)), r\"\\sigma^{1/2}(x)\")\n        self.assertEqual(latex(divisor_sigma(2, 3)), r\"\\sigma^{3/2}(2)\")\n"], "sample_1020": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_742": ["    def test_logistic_regression_cv_refit(self):\n        X, y = make_classification(n_samples=100, n_features=20,\n                                   n_informative=10, n_classes=2,\n                                   random_state=42)\n        y_encoded = LabelEncoder().fit_transform(y)\n        n_folds = 5\n        cv = StratifiedKFold(n_splits=n_folds, shuffle=True,\n                             random_state=42)\n        clf = LogisticRegressionCV(Cs=10, penalty='l2', solver='sag',\n                                  cv=cv, refit=True, random_state=42)\n        clf.fit(X, y_encoded)\n        assert_equal(clf.coef_.shape, (1, X.shape[1]))\n        assert_equal(clf.intercept_.shape, (1,))\n        assert_equal(clf.Cs_.shape, (1,))\n\n        clf_refit = LogisticRegression(Cs=clf.C_, penalty='l2', solver='sag',\n                                     random_state=42)\n        clf_refit.fit(X, y_encoded)\n        assert_allclose(clf.coef_, clf_refit.coef_)\n        assert_allclose(clf.intercept_, clf_refit.intercept_)\n\n\n\n", "def test_logistic_regression_cv_multiclass():\n    n_samples = 100\n    n_features = 20\n    n_classes = 5\n    X, y = make_classification(n_samples=n_samples, n_features=n_features,\n                              n_classes=n_classes, random_state=42)\n    for solver in ['newton-cg', 'lbfgs', 'sag', 'saga']:\n        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n        clf = LogisticRegressionCV(Cs=10, cv=cv, solver=solver,\n                                   multi_class='multinomial',\n                                   random_state=42)\n        clf.fit(X, y)\n        assert_raises(ValueError, clf.predict_proba, X, sample_weight=[0])\n        assert_equal(clf.coef_.shape, (n_classes, n_features))\n        assert_equal(clf.intercept_.shape, (n_classes,))\n        assert_equal(clf.Cs_.size, 10)\n        # Check that scores are computed\n        assert_equal(clf.scores_.shape, (n_classes, 5, 10))\n\n\n", "def test_logistic_regression_cv_multiclass_with_sag_solver():\n    n_samples = 1000\n    n_features = 20\n    n_classes = 3\n\n    X, y = make_classification(n_samples=n_samples, n_features=n_features,\n                               n_classes=n_classes, random_state=42)\n\n    # Use a custom penalty for sag solver\n    clf = LogisticRegressionCV(Cs=10, solver='sag', penalty='l1',\n                              cv=StratifiedKFold(n_splits=5),\n                              multi_class='multinomial', random_state=42)\n\n    clf.fit(X, y)\n    assert_allclose(clf.coef_.shape, (n_classes, n_features))\n    assert_greater(clf.intercept_.shape[0], 0)\n    assert_equal(clf.n_iter_.shape, (n_classes, 5, len(clf.Cs_)))\n    predicted_classes = clf.predict(X)\n    assert_equal(predicted_classes.shape, (n_samples,))\n    assert_equal(np.unique(predicted_classes).size, n_classes)\n\n\n", "def test_logistic_regression_cv_with_multinomial_loss():\n    n_samples = 100\n    n_features = 10\n    n_classes = 5\n    X, y = make_classification(n_samples=n_samples, n_features=n_features,\n                               n_classes=n_classes, random_state=1)\n\n    # Create a classifier with multinomial loss\n    clf = LogisticRegressionCV(Cs=10, cv=5, solver='lbfgs',\n                               multi_class='multinomial',\n                               random_state=0)\n    clf.fit(X, y)\n\n    # Check that the `coef_` attribute has the expected shape\n    assert_array_equal(clf.coef_.shape, (n_classes, n_features))\n\n    # Check that the `intercept_` attribute has the expected shape\n    assert_array_equal(clf.intercept_.shape, (n_classes,))\n\n    # Check that the scores are correctly computed\n    assert_greater(clf.scores_['1'].shape[0], 0)  # Should be greater than 0\n\n    # Check that the C_ attribute has the expected shape\n    assert_array_equal(clf.C_.shape, (n_classes,))\n\n\n\n", "def test_logistic_regression_cv_multiclass():\n    X, y = make_classification(n_samples=1000, n_features=20, n_classes=3,\n                               random_state=42)\n    class_weight = dict(zip(range(3), [1, 1, 2]))\n    clf = LogisticRegressionCV(Cs=np.logspace(-3, 3, num=10),\n                               penalty='l2', solver='sag',\n                               multi_class='multinomial',\n                               class_weight=class_weight, cv=5,\n                               random_state=42)\n    clf.fit(X, y)\n    assert_greater(clf.C_.shape[0], 0)\n    assert_greater(clf.coef_.shape[0], 0)\n    assert_greater(clf.intercept_.shape[0], 0)\n    assert_equal(clf.classes_, [0, 1, 2])\n    check_predictions(clf, X, y)\n"], "sample_624": ["    def test_inline_variable_array_repr_non_numpy_array(self):\n        a = xr.DataArray(np.random.rand(3, 4), dims=(\"x\", \"y\"))\n        a = a.where(a > 0.2, other=np.nan)\n        a = a.astype(\"object\")\n        repr_ = formatting._inline_variable_array_repr(a, 10)\n        assert \"object\" in repr_\n        assert \"NaN\" in repr_\n\n", "    def test_diff_array_repr(self, x, y, compat):\n        diff_str = formatting.diff_array_repr(x, y, compat)\n        if compat == \"identical\":\n            expected_lines = [\n                \"Left and right DataArray objects are not equal\",\n                \"Dimensions:\\n    (a, b) != (b, a)\",\n                \"Differing values:\",\n                \"    L\\n     array([[ 0,  1,  2,  3,  4],\\n      [ 5,  6,  7,  8,  9]])\",\n                \"    R\\n     array([[ 0,  1,  2,  3,  4],\\n      [ 5,  6,  7,  8,  9]])\",\n            ]\n        elif compat == \"allclose\":\n            expected_lines = [\n                \"Left and right DataArray objects are not close\",\n                \"Dimensions:\\n    (a, b) != (b, a)\",\n                \"Differing values:\",\n                \"    L\\n     array([[ 0,  ", "    def test_inline_dask_repr(self):\n        with xr.set_options(display_expand_data_vars=False):\n            da = xr.DataArray(\n                np.arange(10).reshape((5, 2)).astype(\"float32\"),\n                dims=(\"x\", \"y\"),\n                coords={\"x\": np.arange(5), \"y\": np.arange(2)},\n            )\n            da = da.chunk((3, 2))\n\n        expected = (\n            \"<xarray.DataArray (x: 5, y: 2)>\\n\"\n            \"  Dimensions:\\n\"\n            \"    x: 5\\n\"\n            \"    y: 2\\n\"\n            \"  Coordinates:\\n\"\n            \"    * x  (x)\\n\"\n            \"      int64 0 1 2 3 4\\n\"\n            \"    * y  (y)\\n\"\n            \"      int64 0 1\\n\"\n            \"  chunksize: (3, 2)\\n\"\n            \"  dask.array<chunksize=(3, 2), meta=..., shape=(5, 2)>\"\n        )\n        assert formatting.inline_dask_repr(da) == expected\n", "    def test_inline_dask_repr(self):\n        ds = xr.Dataset(\n            {\n                \"x\": (\n                    (\"x\", \"y\"),\n                    np.random.randn(10, 10),\n                )\n            },\n            coords={\"x\": np.arange(10), \"y\": np.arange(10)},\n        )\n        dask_array = ds.x.chunk()\n\n        repr_str = formatting.inline_dask_repr(dask_array)\n        expected_repr = (\n            \"dask.array<chunksize=(10, 10), meta=NumPyArray>\"\n        )\n        assert repr_str == expected_repr\n", "    def test_array_repr_dask(self):\n        ds = xr.Dataset(\n            data_vars={\"var\": ((\"x\", \"y\"), np.random.rand(5, 5))},\n            coords={\"x\": np.arange(5), \"y\": np.arange(5)},\n        )\n        ds_dask = ds.chunk()\n        with requires_dask():\n            repr_dask = formatting.array_repr(ds_dask.data_vars[\"var\"])\n            assert repr_dask.startswith(\"<xarray.DataArray\")\n            assert repr_dask.endswith(\n                \">\")\n\n\n\n\n"], "sample_410": ["    def test_natural_key(self):\n        user = self.make_user(username='johndoe')\n        self.assertEqual(user.natural_key(), ('johndoe',))\n\n        user = self.make_user(username='jane.doe')\n        self.assertEqual(user.natural_key(), ('jane.doe',))\n\n        user = self.make_user(username='  janedoe  ')\n        self.assertEqual(user.natural_key(), ('janedoe',))\n", "    def test_natural_key(self):\n        user = IntegerUsernameUser.objects.create(username=123)\n        natural_key = user.natural_key()\n        self.assertEqual(natural_key, (123,))\n        self.assertEqual(IntegerUsernameUser.objects.get_by_natural_key(123), user)\n", "    def test_natural_key(self):\n        user = IntegerUsernameUser.objects.create(\n            username=123,\n            email='test@example.com',\n        )\n        self.assertEqual(user.natural_key(), (123,))\n        user = IntegerUsernameUser.objects.create(\n            username=456,\n            email='another@example.com',\n        )\n        self.assertEqual(user.natural_key(), (456,))\n\n", "    def test_natural_key(self):\n        user = IntegerUsernameUser.objects.create(\n            id=1, username=\"testuser\"\n        )\n        self.assertEqual(user.natural_key(), (\"testuser\",))\n", "    def test_natural_key_with_integer_username(self):\n        user = IntegerUsernameUser.objects.create(\n            username=123,\n        )\n        self.assertEqual(user.natural_key(), (123,))\n\n"], "sample_544": ["    def test_image_interps_masked():\n        # Test with a masked array\n        array = np.zeros((10, 10))\n        array[2:8, 2:8] = 1\n        mask = np.ones((10, 10), dtype=bool)\n        mask[2:8, 2:8] = False\n        masked_array = np.ma.masked_array(array, mask=~mask)\n        fig, ax = plt.subplots()\n        im = ax.imshow(masked_array, cmap='viridis')\n        plt.show()\n", "    def test_image_origin(tmpdir):\n        # Test that origin affects the image array indexing\n        from matplotlib.image import imread\n\n        img = imread(os.path.join(tmpdir, \"test_origin.png\"))\n        for origin in ['upper', 'lower']:\n            fig, ax = plt.subplots()\n            im = ax.imshow(img, origin=origin)\n            # Check that the image is displayed correctly\n            assert_array_equal(\n                im.toarray(),\n                img,\n                err_msg=f\"Origin={origin} result doesn't match the expected \"\n                        \"array\"\n            )\n            plt.close(fig)\n\n", "def test_nonuniform_image_bbox():\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    x = np.arange(0, 2, 0.1)\n    y = np.arange(0, 1, 0.1)\n    X, Y = np.meshgrid(x, y)\n    im = NonUniformImage(ax, x, y, np.random.rand(len(x) * len(y)))\n    bbox = im.get_window_extent()\n    assert isinstance(bbox, Bbox)\n    assert bbox.width == 2\n    assert bbox.height == 1\n\n\n\n    ", "    def test_imshow_nonuniform_image(", "    def test_pcolor_array_interpolation(image_comparison):\n        x = np.linspace(0, 1, 5)\n        y = np.linspace(0, 1, 5)\n        X, Y = np.meshgrid(x, y)\n        Z = np.sin(X * 2 * np.pi) * np.cos(Y * 2 * np.pi)\n\n        fig, ax = plt.subplots()\n        ax.pcolor(X, Y, Z, interpolation='nearest')\n        ax.set_aspect('equal')\n\n        fig.savefig(image_comparison.get_filename('nearest'))\n        image_comparison.assert_similar()\n\n        fig, ax = plt.subplots()\n        ax.pcolor(X, Y, Z, interpolation='bilinear')\n        ax.set_aspect('equal')\n        fig.savefig(image_comparison.get_filename('bilinear'))\n        image_comparison.assert_similar()\n\n        fig, ax = plt.subplots()\n        ax.pcolor(X, Y, Z, interpolation='bicubic')\n        ax.set_aspect('equal')\n        fig.savefig(image_comparison.get_filename('bicubic'))\n        image_comparison.assert_similar()\n\n\n\n"], "sample_1024": ["compilation error", "    def test_tribonacci():\n        from sympy.abc import x\n        assert simplify(TribonacciConstant**x) == (TribonacciConstant**(x - 1) + TribonacciConstant**(x - 2) + TribonacciConstant**(x - 3))\n    ", "    def test_sympify_complex():\n        assert sympify(1 + 2j) == 1 + 2*I\n        assert sympify(3.14j) == 3.14*I\n        assert sympify(-1j) == -I\n\n\n", "    def test_pow_with_nan():\n        assert isinstance(S.NaN**2, S.NaN)\n        assert isinstance(S.NaN**S.Infinity, S.NaN)\n        assert isinstance(S.NaN**(S.NegativeInfinity), S.NaN)\n        assert isinstance(S.NaN**S.Zero, S.NaN)\n        assert isinstance(S.NaN**(oo), S.NaN)\n        assert isinstance(S.NaN**(-oo), S.NaN)\n", "    def test_sympify_mpmath_with_precision():\n        x = mpmath.mpmath(3.14159265358979323846, prec=50)\n        y = sympify_mpmath(x)\n        assert same_and_same_prec(y, Float(3.14159265358979323846, 50))\n"], "sample_802": ["    def test_pipeline_memory_caching(self):\n        with Memory(cachedir=mkdtemp()) as cache:\n            # test caching happens for fit_transform\n            Xt = DummyTransf()\n            X_cached = Xt.fit_transform(np.ones(10))\n            Xt_cached = clone(Xt)  # Clone to make sure we are not modifying\n            # the memoized pipeline\n            Xt.clear_memory()\n            Xt_cached.clear_memory()\n            Xt_cached.fit_transform(np.ones(10))  # This should be fast\n            time.sleep(0.1)\n            assert_array_equal(Xt.means_, Xt_cached.means_)\n            assert Xt.timestamp_ == Xt_cached.timestamp_\n\n            Xt.clear_memory()\n            Xt.fit_transform(np.ones(10))  # Force refitting\n            assert Xt.timestamp_ != Xt_cached.timestamp_\n\n            # test caching is disabled when memory is not specified\n\n            Xt.fit_transform(np.ones(10))\n            Xt_cached.fit_transform(np.ones(10))\n\n            assert Xt.timestamp_ == Xt_cached.timestamp_\n\n\n\n", "    def test_pipeline_nested(self):\n        X, y = load_iris(return_X_y=True)\n        steps = [\n            ('scaler', StandardScaler()),\n            ('clf', Pipeline([('pca', PCA(n_components=2)),\n                              ('lr', LogisticRegression())])),\n        ]\n        pipe = Pipeline(steps)\n        pipe.fit(X, y)\n\n        X_new = StandardScaler().fit_transform(X)\n        X_new_pca = PCA(n_components=2).fit_transform(X_new)\n        y_pred = LogisticRegression().fit(X_new_pca, y).predict(\n            PCA(n_components=2).transform(X_new))\n        assert_array_equal(pipe.predict(X), y_pred)\n", "    def test_memory_caching(self):\n        X = np.random.rand(100, 5)\n        y = np.random.randint(0, 2, size=100)\n        pipe = Pipeline([('scaler', StandardScaler()),\n                         ('clf', DummyClassifier(strategy='random'))])\n        memory = Memory(cachedir=mkdtemp(), verbose=0)\n\n        # First fit with cahcing\n        with memory.cache():\n            pipe.fit(X, y)\n        # Check that the pipeline was indeed cached\n        pipe.fit(X, y)\n        # Ensure the timestamp of the cached transformer did not change\n        assert pipe._final_estimator.scaler.timestamp_ == pipe._final_estimator.scaler.timestamp_\n\n        # Clean up the temporary cache directory\n        shutil.rmtree(memory.cachedir)\n", "    def test_pipeline_cache(self, memory_path, cached):\n        memory = Memory(location=memory_path)\n\n        X = np.arange(10).reshape((2, 5))\n        y = np.arange(2)\n\n        class CachedTransformer(BaseEstimator):\n                self.cached = cached\n                self.transform_called = 0\n\n                if self.cached:\n                    return self\n                self.transform_called += 1\n                return self\n\n                return X * 2\n\n        transformer = CachedTransformer(cached=cached)\n        pipe = Pipeline([\n            ('scaler', StandardScaler()),\n            ('transform', transformer)\n        ])\n\n        pipe.fit(X, y)\n        if cached:\n            assert pipe.steps[-1][1].transform_called == 0\n        else:\n            assert pipe.steps[-1][1].transform_called == 1\n\n        pipe.fit(X, y)\n        if cached:\n            assert pipe.steps[-1][1].transform_called == 0\n        else:\n            assert pipe.steps[-1][1].transform_called == 1\n\n\n\n", "    def test_fit_param_dispatching(self):\n        estimator = TransfFitParams()\n        X = np.ones((10, 5))\n        y = np.ones(10)\n        estimator.fit(X, y, param1='one', param2='two')\n        assert_equal(estimator.fit_params, {'param1': 'one', 'param2': 'two'})\n"], "sample_268": ["    def test_iter_modules_and_files_circular_imports(self):\n        tempdir = tempfile.mkdtemp()\n        self.addCleanup(shutil.rmtree, tempdir)\n        file1_path = Path(tempdir) / 'file1.py'\n        file2_path = Path(tempdir) / 'file2.py'\n\n        with open(file1_path, 'w') as f:\n            f.write(\n                \"\"\"\n                import file2\n                print('file1')\n                file2.function()\n                \"\"\"\n            )\n\n        with open(file2_path, 'w') as f:\n            f.write(\n                \"\"\"\n                    print('file2')\n                \"\"\"\n            )\n\n        extend_sys_path([tempdir])\n        # Force import so that modules are registered early.\n        self.import_and_cleanup('file1')\n\n        # Circular imports should be accounted for.\n        files = list(autoreload.iter_modules_and_files())\n        self.assertIn(file1_path, files)\n        self.assertIn(file2_path, files)\n\n\n\n", "    def test_common_roots(self):\n        roots = [\n            Path('/path/to/file1'),\n            Path('/path/to/file2'),\n            Path('/path/to/file3'),\n            Path('/another/path/to/file4'),\n            Path('/another/path/to/file5'),\n        ]\n        expected_roots = frozenset([\n            Path('/path/to'),\n            Path('/another/path/to'),\n            Path('/path'),\n        ])\n        self.assertEqual(autoreload.common_roots(roots), expected_roots)\n\n        roots = [\n            Path('/path/to/file1'),\n            Path('/path/to/file2'),\n            Path('/path/to/dir1/file3'),\n            Path('/path/to/dir1/file4'),\n        ]\n        expected_roots = frozenset([Path('/path/to/dir1'), Path('/path/to')])\n        self.assertEqual(autoreload.common_roots(roots), expected_roots)\n\n\n\n", "    def test_iter_all_python_module_files_with_special_files(self):\n        temp_dir = tempfile.mkdtemp()\n        self.addCleanup(shutil.rmtree, temp_dir)\n\n        # Create some files with special names/extensions.\n        script_file = self.temporary_file('__init__.py')\n        data_file = self.temporary_file('data.json')\n        another_py_file = self.temporary_file('module2.py')\n        \n        # Import module2\n        self.import_and_cleanup(str(another_py_file.resolve()))\n        \n        # Assert that these files are correctly included\n        self.assertFileFound(script_file)\n        self.assertFileFound(another_py_file)\n        self.assertFileNotFound(data_file)\n", "    def test_iter_modules_and_files_with_zip_file(self):\n        with tempfile.NamedTemporaryFile(delete=False) as f:\n            filename = f.name\n            with zipfile.ZipFile(filename, 'w') as zf:\n                zf.writestr('module.py', 'def test(): pass')\n            self.import_and_cleanup('zipfile.name')  # Use a dummy name\n            self.assertFileFound(Path(filename) / 'module.py')\n\n            # Test access to the zip file after unzipping\n            zipped_temp_dir = Path(filename).parent\n            self.addCleanup(shutil.rmtree, zipped_temp_dir)\n            with zipfile.ZipFile(filename, 'r') as zf:\n                zf.extractall(zipped_temp_dir)\n                self.assertFileFound(zipped_temp_dir / 'module.py')\n            \n            # Test cache after deleting and recreating the zip file\n            # This will force reload as the file is no longer the same.\n            os.remove(filename)\n\n            with zipfile.ZipFile(filename, 'w') as zf:\n                zf.writestr('module.py', 'def test(): pass')\n            self.import_and_cleanup('zipfile.name')\n            self.assertFileFound(Path(filename) / 'module.py')\n            self.assertEqual(autoreload.iter_modules_and_files.cache_info().hits, 2)\n\n\n\n", "    def test_iter_all_python_module_files_with_zipfile(self):\n        # Create a temporary zipfile.\n        with tempfile.TemporaryDirectory() as temp_dir:\n            zipfile_path = Path(temp_dir) / 'test.zip'\n            with zipfile.ZipFile(zipfile_path, 'w') as zipf:\n                zipf.write('test_module.py', 'test_module.py')\n            # Add the zipfile to sys.path\n            extend_sys_path(temp_dir)\n            self.import_and_cleanup('test_module')\n            self.assertFileFound(zipfile_path / 'test_module.py')\n\n\n\n\n\n\n\n"], "sample_511": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1058": ["    def test_print_indexed_symbols(self):\n        index = symbols('i')\n        a = p[index]\n        self.assertEqual(pycode(a, standard='python3'), 'p[i]')\n\n        a_indexed = p[[index, index + 1]]\n        self.assertEqual(pycode(a_indexed, standard='python3'), 'p[[i, i + 1]]')\n", "    def test_sparse_matrix(self):\n        m = SparseMatrix([[1, 2, 0], [0, 3, 4]], (3, 3))\n        code = pycode(m, printer=SciPyPrinter)\n        assert code == 'scipy.sparse.coo_matrix([[1, 2, 0], [0, 3, 4]], ((0, 0), (0, 1), (1, 2)), shape=(3, 3))'\n\n\n\n", "    def test_piecewise_with_default(self):\n        expr = Piecewise(\n            (1, x < 0), (2, (x >= 0) & (x < 1)), (3, True)\n        )\n        code = pycode(expr, standard='python3')\n        self.assertEqual(code, \"numpy.select([x < 0, (x >= 0) & (x < 1), True], [1, 2, 3], default=numpy.nan)\")\n", "    def test_printing_scipy_special(self):\n        from sympy.functions.special.tensor_functions import kronecker_delta\n        self.assertEqual(pycode(kronecker_delta(x, y), printer=SciPyPrinter),\n                         'scipy.special.kronecker_delta(x, y)')\n        self.assertEqual(pycode(pi, printer=SciPyPrinter), 'scipy.constants.pi')\n        self.assertEqual(pycode(sqrt(x), printer=SciPyPrinter),\n                         'sympy.sqrt(x)')\n        self.assertEqual(pycode(factorial(x), printer=SciPyPrinter),\n                         'scipy.special.factorial(x)')\n        self.assertEqual(pycode(gamma(x), printer=SciPyPrinter),\n                         'scipy.special.gamma(x)')\n\n\n", "    def test_printing_Piecewise(self):\n        expr = Piecewise((x, x < 0), (x**2, True))\n        expected_code = 'numpy.select([x < 0, True], [x, x**2], default=numpy.nan)'\n        self.assertEqual(NumPyPrinter().doprint(expr), expected_code)\n"], "sample_259": ["        def test_prefetch_related_objects_with_nested_prefetch(self):\n            results = list(Reader.objects.prefetch_related(\n                Prefetch('books_read', queryset=Book.objects.prefetch_related(\n                    Prefetch('authors', to_attr='authors_for_reader')\n                ))\n            ))\n\n            for reader in results:\n                self.assertEqual(len(reader.books_read.all()), len(reader.books_read.values_list('id', flat=True)))\n                for book in reader.books_read.all():\n                    self.assertEqual(len(book.authors_for_reader.all()), len(book.authors_for_reader.values_list('id', flat=True)))\n\n\n\n", "    def test_prefetch_related_objects_nested_lookups(self):\n        reader = self.reader1\n        prefetch_lookups = [\n            Prefetch('books_read', queryset=Book.objects.all()),\n            Prefetch('books_read__authors', to_attr='authors'),\n        ]\n        prefetch_related_objects(reader, *prefetch_lookups)\n\n        self.assertEqual(reader.books_read.all().count(), 2)\n        for book in reader.books_read.all():\n            self.assertEqual(book.authors.count(), 1 if book == self.book4 else 3)\n\n\n\n\n\n", "    def test_prefetch_related_objects_cache_reuse(self):\n        reader1 = Reader.objects.get(name='Amy')\n        prefetch_related_objects(reader1, Prefetch('books_read__authors', to_attr='related_authors'))\n        reader1_authors = reader1.books_read.first().authors.all()\n        prefetch_related_objects(reader1, Prefetch('books_read__authors', to_attr='related_authors'))\n        reader1_authors_again = reader1.books_read.first().authors.all()\n        self.assertEqual(reader1_authors, reader1_authors_again)\n\n", "    def test_prefetch_related_objects_multiple_lookups(self):\n        books = Book.objects.all().prefetch_related(\n            Prefetch('authors', queryset=Author.objects.filter(name__in=['Charlotte', 'Emily']), to_attr='authors_related'),\n            Prefetch('readers', to_attr='readers_related')\n        )\n        for book in books:\n            self.assertEqual(len(book.authors_related), 2)\n            self.assertEqual(len(book.readers_related), 2)\n\n\n\n", "    def test_prefetch_related_objects(self):\n        reader1 = Reader.objects.prefetch_related(\n            Prefetch('books_read', queryset=Book.objects.all())\n        ).get(name='Amy')\n\n        self.assertEqual(len(reader1.books_read.all()), 2)\n"], "sample_1117": ["    def test_ask_symmetric_matpow(self):\n        self.assertTrue(ask(Q.symmetric(A1x1**2), Q.real))\n        self.assertTrue(ask(Q.symmetric(A1x1**-2), Q.real))\n        self.assertFalse(ask(Q.symmetric(X**2), Q.real))\n    ", "    def test_AskSquareHandler_MatrixSymbol(self):\n        self.assertTrue(AskSquareHandler.MatrixSymbol(X, {}))\n        self.assertFalse(AskSquareHandler.MatrixSymbol(Y, {}))\n        self.assertTrue(AskSquareHandler.MatrixSymbol(A1x1, {}))\n        self.assertFalse(AskSquareHandler.MatrixSymbol(C0x0, {}))\n        self.assertTrue(AskSquareHandler.MatrixSymbol(DiagMatrix(2, [1, 2]), {}))\n        self.assertTrue(AskSquareHandler.MatrixSymbol(DiagonalMatrix(2, [1, 2]), {}))\n        self.assertTrue(AskSquareHandler.MatrixSymbol(X, {Q.square(X): True}))\n        self.assertFalse(AskSquareHandler.MatrixSymbol(Y, {Q.square(X): True}))\n\n\n", "    def test_ask_symmetric_non_square_matrix(self):\n        assert AskSymmetricHandler.MatrixSymbol(C0x0, {}) is False\n        assert AskSymmetricHandler.MatMul(X * Y, {}) is None\n        assert AskSymmetricHandler.Transpose(C0x0, {}) is None\n", "    def test_ask_upper_triangular_matmul():\n        assert ask(AskUpperTriangularHandler.MatMul(X*Y, Q.real)) is True\n        assert ask(AskUpperTriangularHandler.MatMul(X*Y, Q.complex)) is True\n        assert ask(AskUpperTriangularHandler.MatMul(X**2, Q.real)) is True\n        assert ask(AskUpperTriangularHandler.MatMul(X**2, Q.complex)) is True\n        assert ask(AskUpperTriangularHandler.MatMul(X*Y*Z, Q.real)) is True\n        assert ask(AskUpperTriangularHandler.MatMul(X*Y*Z.T, Q.real)) is True\n        assert ask(AskUpperTriangularHandler.MatMul(X*Y + Y*Z, Q.real)) is True\n        assert ask(AskUpperTriangularHandler.MatMul(X + Y*Z, Q.real)) is True\n        assert ask(AskUpperTriangularHandler.MatMul(X*Y*Identity(2), Q.real)) is True\n        assert ask(AskUpperTriangularHandler.MatMul(X*Y*OneMatrix(2), Q.real)) is True\n\n", "    def test_ask_symmetric_mat_mul(self):\n         assert ask(AskSymmetricHandler.MatMul(X * Y, [])) is False\n         assert ask(AskSymmetricHandler.MatMul(X * X.T, [])) is True\n         assert ask(AskSymmetricHandler.MatMul(X.T * X, [])) is True\n         assert ask(AskSymmetricHandler.MatMul(X * X.T * Y, [])) is False\n         assert ask(AskSymmetricHandler.MatMul(DiagonalMatrix([[1, 2], [2, 3]]) *\n                                          DiagonalMatrix([[4, 5], [6, 7]]), [])) is True\n         assert ask(AskSymmetricHandler.MatMul(DiagonalMatrix([[1, 2], [2, 3]]) *\n                                          MatrixSymbol('V1', 2, 1), [])) is False\n         assert ask(AskSymmetricHandler.MatMul(AskSymmetricHandler.MatPow(X, 2), [])) is True\n\n\n"], "sample_1091": ["    def test_eval_as_set(self):\n        from sympy.core.sets import IntervalSet\n\n        x = Symbol('x')\n        assert Gt(x, 2).as_set(evaluate=True).as_frozenset() ==  Interval((2, oo), reals=True).as_frozenset()\n        assert Lt(x, 2).as_set(evaluate=True).as_frozenset() ==  Interval((-oo, 2), reals=True).as_frozenset()\n        assert Ge(x, 2).as_set(evaluate=True).as_frozenset() ==  Interval([2, oo), reals=True).as_frozenset()\n        assert Le(x, 2).as_set(evaluate=True).as_frozenset() ==  Interval((-oo, 2], reals=True).as_frozenset()\n", "compilation error", "compilation error", "    def test_rel_infinity(self):\n        for r in [Eq, Ne, Gt, Ge, Lt, Le]:\n            for arg in [oo, -oo, nan]:\n                for other in [oo, -oo, nan, x]:\n                    if r is Eq and (arg is nan or other is nan):\n                        continue\n                    if r is Ne and arg == other:\n                        continue\n                    if r is Gt and arg >= other:\n                        continue\n                    if r is Ge and arg > other:\n                        continue\n                    if r is Lt and arg <= other:\n                        continue\n                    if r is Le and arg < other:\n                        continue\n                    with self.subTest(r=r, arg=arg, other=other):\n                        expr = r(arg, other)\n                        if r is Eq:\n                            assert expr.is_boolean\n                            assert expr not in [True, False]\n                        elif r is Ne:\n                            assert expr.is_boolean\n                            assert expr != True\n    ", "def test_rel_with_symbols():\n    from sympy.utilities.pytest import raises\n    assert Gt(x, y) != Gt(y, x)\n    assert Le(x, y) != Le(y, x)\n    assert (x <= y) != (y >= x)\n    assert x < y != y < x\n    assert x > y != y > x\n    assert (x == y) == (y == x)\n    assert (x != y) == (y != x)\n    assert (x >= y) == (y <= x)\n\n    # Test with different types of symbols\n    a = Symbol('a', real=True)\n    b = Symbol('b', integer=True)\n    assert Gt(a, b) != Gt(b, a)\n    assert Le(a, b) != Le(b, a)\n    assert (a <= b) != (b >= a)\n    assert a < b != b < a\n    assert a > b != b > a\n    assert (a == b) == (b == a)\n    assert (a != b) == (b != a)\n    assert (a >= b) == (b <= a)\n\n\n\n"], "sample_910": ["    def test_pending_warnings(self) -> None:\n        with logging.pending_warnings() as memhandler:\n            logging.getLogger(logging.Namespace).warning('warning message!')\n\n        assert len(memhandler.clear()) == 1\n        assert 'warning message!' in memhandler.clear()[0].getMessage()\n\n\n", "    def test_pending_warnings(self):\n        with logging.pending_warnings() as memhandler:\n            logging.getLogger(logging.Namespace).warning(\"This warning should be pending\")\n\n        assert len(memhandler.logs) == 1\n\n        logging.getLogger(logging.Namespace).info(\"This info should not be pending\")\n\n        assert len(memhandler.logs) == 1\n", "    def test_suppressing_warning(self, capsys):\n        with prefixed_warnings(\"prefix:\"):\n            with suppress_logging():\n                logging.getLogger('sphinx').warning(\"Warning message!\")\n        out, err = capsys.readouterr()\n\n        assert \"prefix: Warning message!\" not in out\n        assert \"Warning message!\" not in err\n", "def test_warning_suppressor():\n    app = pytest.raises(SphinxWarning, module='sphinx.util.logging').app\n    warnings = []\n    with app.app.config.suppress_warnings([]) as memhandler:\n        logging.getLogger(\"sphinx\").warning(\"This should be suppressed\")\n    with app.app.config.suppress_warnings([\"sphinx.test_util_logging\"]) as memhandler:\n        logging.getLogger(\"sphinx\").warning(\"This should be suppressed\")\n    with app.app.config.suppress_warnings([\"sphinx.*\"]) as memhandler:\n        logging.getLogger(\"sphinx\").warning(\"This should be suppressed\")\n    with app.app.config.suppress_warnings([\"sphinx.test_util_logging.test_warning_suppressor\"]) as memhandler:\n      logging.getLogger(\"sphinx\").warning(\"This should be suppressed\")\n\n    assert app._warncount == 0\n    \n\n\n", "    def test_is_suppressed_warning(self) -> None:\n        app = self.app  # type: Sphinx\n        app.config.suppress_warnings = ['my_package.my_module', 'my_package.my_module.*']\n\n        with suppress_logging() as memhandler:\n            logging.getLogger(__name__).warning('my_package.my_module.warning_a')\n            logging.getLogger(__name__).warning('my_package.my_module.warning_b')\n            logging.getLogger(__name__).warning('my_package.my_module.other_warning')\n            logging.getLogger(__name__).warning('unknown.warning')\n\n        logs = memhandler.clear()\n        assert len(logs) == 2\n\n        # test is_suppressed_warning function\n        assert is_suppressed_warning('my_package.my_module', 'warning_a', app.config.suppress_warnings)\n        assert is_suppressed_warning('my_package.my_module', 'warning_b', app.config.suppress_warnings)\n        assert not is_suppressed_warning('my_package.my_module', 'other_warning', app.config.suppress_warnings)\n        assert not is_suppressed_warning('unknown', 'warning', app.config.suppress_warnings)\n\n\n\n"], "sample_320": ["    def test_add_remove_constraint(self):\n        with atomic():\n            self.assertEqual(len(self.model.objects.values_list(\"id\").all()), 0)\n            with self.assertRaises(IntegrityError):\n                self.model.objects.create(name=\"Already exists\", value=1)  \n        # Add a unique constraint\n        with override_settings(ATOMIC_REQUESTS=True):\n            with capture_queries():\n                operation = AddConstraint(\"my_app.Food\", models.UniqueConstraint(fields=[\"name\"], name=\"unique_name\"))\n                self.apply_operation(operation)\n        with atomic():\n            self.assertEqual(len(self.model.objects.values_list(\"id\").all()), 0)\n            self.model.objects.create(name=\"New name\", value=2)\n\n        with self.assertRaises(IntegrityError):\n            self.model.objects.create(name=\"Already exists\", value=3)\n        # Remove the constraint\n        with override_settings(ATOMIC_REQUESTS=True):\n            with capture_queries():\n                operation = RemoveConstraint(\"my_app.Food\", \"unique_name\")\n                self.apply_operation(operation)\n        with atomic():\n            self.model.objects.create(name=\"Already exists\", value=4)\n\n", "    def test_rename_model_with_existing_migration(self):\n        \"\"\"\n        Tests that renaming a model that already has migrations\n        works correctly.\n        \"\"\"\n        # Create an app with a model\n        self.create_app(\"my_app\", models=[UnicodeModel])\n\n        # Create a migration to rename the model\n        with self.subTest(\"Renaming a model\"):\n            with atomic(using=self.connection.alias):\n                Migration.objects.create(\n                    app=\"my_app\",\n                    name=\"0001_initial\",\n                    operations=[RenameModel(UnicodeModel._meta.db_table, \"new_unicode_model\")],\n                )\n\n        # Apply the migration\n        from django.test import TransactionTestCase\n        with TransactionTestCase.setUp(using=self.connection.alias):\n            with self.assertRaises(IntegrityError):\n                self.connection.execute(\n                    f\"SELECT * FROM django_migrations WHERE app = 'my_app' AND name = '0001_initial'\"\n                )\n\n        with self.assertRaises(IntegrityError):\n            self.connection.execute(\n                f\"SELECT * FROM django_migrations WHERE app = 'my_app' AND name = '0001_initial'\"\n            )\n\n        # Create a new migration to rename the model again\n        with self.subTest(\"Renaming the renamed model\"):\n            with atomic(using=self.connection.alias):\n                Migration.objects.create(\n                    app=\"my_app\",\n                    name=\"0002_rename_again\",\n                    operations=[RenameModel(\"new_unicode_model\", \"final_unicode_model\")],\n                )\n\n        # Apply the new migration\n        with self.assertRaises(IntegrityError):\n            self.connection.execute(\n                f\"SELECT * FROM django_migrations WHERE app = 'my_app' AND name = '0002_rename_again'\"\n            )\n\n\n\n", "    def test_rename_model(self):\n        with atomic():\n            # Create a model.\n            state = self.create_state(\n                models={\"myapp.User\": models.Model(id=models.AutoField(primary_key=True))},\n            )\n\n            # Rename the model.\n            operation = RenameModel(\n                \"myapp.User\",\n                \"myapp.Person\",\n            )\n            operation.database_forwards(self.connection, state, state)\n\n            # Assert that the database table was renamed.\n            with CaptureQueriesContext(connection) as cm:\n                with transaction.atomic():\n                    connection.execute(\n                        \"SELECT table_name FROM information_schema.tables WHERE table_schema = %s AND table_name = %s\",\n                        (self.connection.settings_dict[\"NAME\"], \"person\"),\n                    )\n                    self.assertEqual(len(cm.captured_queries), 1)\n            self.assertSchemaChange(\n                state,\n                [\n                    (\"ALTER TABLE\", {}, {\"name\": \"person\"}),\n                ],\n            )\n\n\n", "    def test_add_remove_constraint(self):\n        with atomic(using=self.connection.alias):\n            self.create_app('myapp', models=['mymodel'])\n\n        with connection.cursor() as cursor:\n            cursor.execute(\n                \"\"\"\n                CREATE TABLE app_mymodel (\n                    id INTEGER PRIMARY KEY,\n                    name TEXT\n                );\n                \"\"\"\n            )\n\n        with self.settings(\n            INSTALLED_APPS=[\n                'myapp',\n                'django.contrib.auth',\n                'django.contrib.contenttypes',\n                'django.contrib.sessions',\n                'django.contrib.messages',\n                'django.contrib.staticfiles',\n            ]\n        ):\n            state = ProjectState(\n                apps=self.apps,\n                using=self.connection.alias,\n            )\n            with self.capture_queries() as queries:\n                # Add a constraint\n                state.add_operation(AddConstraint('myapp.MyModel', models.ForeignKeyConstraint(\n                    models.Model.objects.all(),\n                    models.Model.objects.all(),\n                    name='not_a_real_name'\n                )))\n                state.apply_migration(\n                    Migration('0001_initial', 'myapp'),\n                )\n\n                self.assertEqual(len(queries), 1)\n                self.assertIn(\n                    f\"CONSTRAINT NOT NULL PRIMARY KEY\", queries.captured_queries[0],\n                )\n\n                # Verify the constraint exists\n                with self.assertRaises(IntegrityError):\n                    models.MyModel.objects.create(name=\"Foo\")\n\n                # Remove the constraint\n                state.add_operation(RemoveConstraint('myapp.MyModel', 'not_a_real_name'))\n                state.apply_migration(\n                    Migration('0002_remove_constraint', 'myapp'),\n                )\n\n                self.assertEqual(len(queries), 2)\n                self.assertIn(\n                    f\"ALTER TABLE IF EXISTS app_mymodel DROP CONSTRAINT if exists not_a_real_name\",\n                    queries.captured_queries[1],\n                )\n\n                models.MyModel.objects.create(name=\"Foo\")\n\n", "    def test_add_remove_constraint(self):\n        with atomic(using=self.connection.alias):\n            # Create a migration with a constraint\n            with self.settings(\n                INSTALLED_APPS={\n                    \"migrations\": [\"test_migrations\"],\n                }\n            ):\n                with override_settings(\n                    DATABASES={\n                        \"default\": {\n                            \"ENGINE\": \"django.db.backends.sqlite3\",\n                            \"NAME\": \":memory:\",\n                        }\n                    }\n                ):\n                    with CaptureQueriesContext(connection) as queries:\n                        with transaction.atomic():\n                            self.migrate_db(\n                                *[\n                                    [\"test_migrations\", \"0001_initial\"],\n                                    [\"test_migrations\", \"0002_add_constraint\"],  # Adds a constraint\n                                ],\n                            )\n                        self.assertEqual(len(queries.captured_queries), 2)\n                        self.assertIn(\n                            \"CREATE TABLE IF NOT EXISTS 'test_migrations_unicodemodel' (\"\n                            \"id INTEGER PRIMARY KEY AUTOINCREMENT, \"\n                            \"name VARCHAR(255) NOT NULL, \"\n                            \"unique_field VARCHAR(255) UNIQUE NOT NULL,\"\n                            \"CONSTRAINT 'unique_field_unique' UNIQUE (unique_field),\",\n                            queries.captured_queries[1],\n                        )\n\n            # Check that the constraint exists\n            with self.connection.cursor() as cursor:\n                cursor.execute(\n                    \"PRAGMA foreign_key_check = ON;\"\n                )\n                cursor.execute(\n                    \"SELECT * FROM sqlite_master WHERE type='table' AND name='unicodemodel';\"\n                )\n                result = cursor.fetchone()\n                self.assertEqual(result[1], \"unicodemodel\")\n\n            # Now rollback the second migration to remove the constraint\n            with self.settings(\n                INSTALLED_APPS={\n                    \"migrations\": [\"test_migrations\"],\n                }\n            ):\n                with override_settings(\n                    DATABASES={\n                        \"default\": {\n                            \"ENGINE\": \"django.db.backends.sqlite3\",\n                            \"NAME\": \":memory:\",\n                        }\n                    }\n                ):\n                    with CaptureQueriesContext(connection) as queries"], "sample_202": ["    def test_cookie_storage_with_safestring_messages(self):\n        storage = self.storage_class()\n        messages = [\n            Message(request=self.request, level=constants.SUCCESS, message=mark_safe('<b>Hello</b>'), extra_tags=None),\n            Message(request=self.request, level=constants.WARNING, message=SafeData(\"<b>World</b>\"), extra_tags=None),\n        ]\n        set_cookie_data(storage, messages)\n        response = self.get_response()\n        self.assertEqual(len(storage._decode(response.cookies[storage.cookie_name].value)), 2)\n", "    def test_encoding_empty_list(self):\n        storage = self.storage_class()\n        messages = []\n        encoded_data = storage._encode(messages)\n        self.assertEqual(encoded_data, '')\n", "    def test_encoding_with_safedatas(self):\n        storage = self.storage_class()\n        messages = [\n            Message(\n                'This is a test message.',\n                level=constants.INFO,\n                extra_tags='safe-data-test',\n            ),\n            Message(\n                SafeData('This is a <span>safe</span> data message.'),\n                level=constants.WARNING,\n            ),\n        ]\n        encoded_data = storage._encode(messages)\n        decoded_messages = json.loads(encoded_data.split('$')[1], cls=MessageDecoder)\n        self.assertEqual(len(decoded_messages), 2)\n        self.assertEqual(decoded_messages[0].message, 'This is a test message.')\n        self.assertEqual(decoded_messages[1].message, mark_safe('This is a <span>safe</span> data message.'))\n", "    def test_cookie_storage_encode_empty_messages(self):\n        storage = self.storage_class()\n        set_cookie_data(storage, [])\n        self.assertEqual(storage.request.COOKIES.get(storage.cookie_name), '')\n\n\n", "    def test_cookie_storage_not_finished_message(self):\n        storage = self.storage_class()\n\n        # Add some messages to the storage\n        storage._store([Message('Test message 1', constants.INFO),\n                       Message('Test message 2', constants.INFO)], response=self.response)\n\n        # Send a response to simulate the message being stored\n        self.assertEqual(self.stored_messages_count(storage, self.response), 2)\n\n        # Now simulate the scenario where messages exceed the `max_cookie_size` limit\n        # and a not_finished sentinel value is added.\n        set_cookie_data(storage, [Message('Test message 3', constants.INFO)\n                                  for _ in range(100)])\n\n        # Retrieve the messages from the cookie\n        messages = storage._decode(storage.request.COOKIES[storage.cookie_name])\n        self.assertEqual(len(messages), 99)\n        self.assertEqual(messages[-1], storage.not_finished)\n\n\n"], "sample_453": ["    def test_submit_row_tag(self):\n        request = self.request_factory.get(reverse('admin:article_change', args=(1,)))\n        request.user = self.user\n\n        context = {'add': False, 'change': True, 'is_popup': False, 'save_as': False, 'has_add_permission': True, 'has_change_permission': True, 'has_delete_permission': True, 'has_editable_inline_admin_formsets': False, 'show_save': True, 'show_save_and_add_another': True, 'show_save_and_continue': True}\n        rendered_html = submit_row(context)\n\n        self.assertIn('submit', rendered_html.tags)\n        self.assertIn('save_and_add_another', rendered_html.tags)\n        self.assertIn('save_and_continue', rendered_html.tags)\n\n\n\n", "    def test_submit_row(self):\n        user = self.create_user(\"testuser\", \"testpassword\")\n        self.login(self.request_factory.get(\"/\", {}), user)\n\n        # Create a sample user\n        article = Article.objects.create(title=\"Test Article\")\n\n        # Get the URL for the change form for the article\n        url = reverse(\"admin:articles_article_change\", args=(article.id,))\n        request = self.request_factory.get(url)\n\n        # Get the context for the submit_row template tag\n        context = submit_row(request)\n\n        # Assert that the context contains the expected values\n        self.assertTrue(context[\"can_change\"])\n        self.assertTrue(context[\"show_delete_link\"])\n        self.assertFalse(context[\"show_save_as_new\"])\n        self.assertFalse(context[\"show_save_and_add_another\"])\n        self.assertTrue(context[\"show_save_and_continue\"])\n        self.assertTrue(context[\"show_save\"])\n\n\n", "    def test_submit_row_tag(self):\n        user = self.create_user('john', 'john123')\n        response = self.client.login(username='john', password='john123')\n        self.assertLoginSuccessful(response)\n        # Mock the context needed by submit_row_tag\n        context = {\n            'add': True,\n            'change': False,\n            'is_popup': False,\n            'save_as': False,\n            'show_save_and_add_another': True,\n            'show_save_and_continue': True,\n            'has_add_permission': True,\n            'has_change_permission': False,\n            'has_delete_permission': True,\n            'has_editable_inline_admin_formsets': False,\n            'show_save': True,  \n        }\n        # Render the submit row template tag and assert the output\n        rendered_template = submit_row(context)\n        self.assertIn('show_save_and_add_another', rendered_template)\n        self.assertIn('show_save_and_continue', rendered_template)\n\n\n\n", "    def test_submit_row_tag_context(self):\n        request = self.request_factory.get(reverse('admin:article_change', args=(1,)))\n        request.user = self.user\n        request.user.is_active = True\n        get_perm('Article', 'change')\n\n        # Set up context\n        context = {'add': True, 'change': False, 'is_popup': False, 'save_as': False, 'show_save': True, 'show_save_and_add_another': True, 'show_save_and_continue': True, 'has_add_permission': True, 'has_change_permission': False, 'has_view_permission': True, 'has_delete_permission': True, 'has_editable_inline_admin_formsets': False}\n        ctx = submit_row(context)\n\n        # Test the context\n        self.assertTrue('can_change' in ctx)\n        self.assertTrue('show_delete_link' in ctx)\n        self.assertTrue('show_save_as_new' in ctx)\n        self.assertTrue('show_save_and_add_another' in ctx)\n        self.assertTrue('show_save_and_continue' in ctx)\n        self.assertTrue('show_save' in ctx)\n        self.assertTrue('show_close' in ctx)\n\n", "    def test_submit_row_tag(self):\n        request = self.request_factory.get(reverse('admin:article_change', args=(1,)))\n        request.user = self.user\n        request.user.is_staff = True\n        request.user.is_active = True\n        add = self.user.has_perm('articles.add_article')\n        change = self.user.has_perm('articles.change_article')\n        context = {'add': add, 'change': change, 'is_popup': False, 'save_as': False,\n                   'has_add_permission': add, 'has_change_permission': change,\n                   'has_delete_permission': True, 'show_delete': True,\n                   'show_save': True, 'show_save_and_add_another': True,\n                   'show_save_and_continue': True,\n                   'has_editable_inline_admin_formsets': False,\n                   }\n        context_instance = Context(context)\n        rendered = submit_row(context_instance)\n        self.assertIn('can_change', rendered.vars)\n        self.assertIn('show_delete_link', rendered.vars)\n        self.assertIn('show_save_as_new', rendered.vars)\n        self.assertIn('show_save_and_add_another', rendered.vars)\n        self.assertIn('show_save_and_continue', rendered.vars)\n        self.assertIn('show_save', rendered.vars)\n        self.assertIn('show_close', rendered.vars)\n"], "sample_903": ["    def test_perplexity_binary_search(self):\n        n_samples = 30\n        X = np.random.randn(n_samples, 2)\n        perplexity_target = 30.0\n        perplexity_found = _binary_search_perplexity(\n            X, perplexity_target,\n            n_neighbors=30)\n        assert_almost_equal(perplexity_found, perplexity_target)\n\n", "def test_trustworthiness():\n    X = np.random.rand(100, 10)\n    X_embedded = TSNE(n_components=2).fit_transform(X)\n    \n    # Test trustworthiness with the same embedding\n    trustworthiness_same = trustworthiness(X, X_embedded)\n    assert trustworthiness_same > 0.5\n\n    # Test trustworthiness with corrupted embedding\n    corrupted_embedding = X_embedded.copy()\n    corrupted_embedding[:, 0] += np.random.rand(100) * 0.1\n    trustworthiness_corrupted = trustworthiness(X, corrupted_embedding)\n    assert trustworthiness_corrupted < trustworthiness_same\n\n    # Test trustworthiness with different perplexity\n    tsne1 = TSNE(n_components=2, perplexity=30)\n    tsne2 = TSNE(n_components=2, perplexity=10)\n    X_embedded1 = tsne1.fit_transform(X)\n    X_embedded2 = tsne2.fit_transform(X)\n    trustworthiness1 = trustworthiness(X, X_embedded1)\n    trustworthiness2 = trustworthiness(X, X_embedded2)\n    assert trustworthiness1 > trustworthiness2\n\n\n\n", "    def test_binary_search_perplexity(self):\n        for perplexity in [5, 10, 20, 100]:\n            n_samples = 1000\n            P = np.ones((n_samples, n_samples)) / n_samples\n            p, success = _binary_search_perplexity(P, perplexity)\n            assert success\n            assert 0 < p < 1\n            assert p == perplexity\n            ", "    def test_trustworthiness(self):\n        # Test trustworthiness on a simple dataset\n        X, _ = make_blobs(n_samples=100, centers=2, random_state=42)\n        t = trustworthiness(X, X, n_neighbors=5)\n        assert 0.9 <= t <= 1.0\n", "    def test_trustworthiness(self):\n        # Test trustworthiness with a simple example\n        X = np.random.rand(100, 2)\n        X_embedded = TSNE(n_components=2, perplexity=30).fit_transform(X)\n        trust = trustworthiness(X, X_embedded, n_neighbors=5)\n        assert assert_less_equal(trust, 1.0), \"Trustworthiness should be <= 1\"\n        assert assert_greater(trust, 0.0), \"Trustworthiness should be >= 0\"\n"], "sample_322": ["        def test_migration_plan_invalid(self):\n            with self.assertRaises(InvalidMigrationPlan):\n                executor = MigrationExecutor(connection, progress_callback=mock.Mock())\n                executor.migration_plan(\n                    [(\"migrations\", \"0001_initial\")],\n                    clean_start=False,\n                )\n                executor.migrate(\n                    targets=[(\"migrations\", \"0001_initial\")],\n                    plan=None,\n                    state=None,\n                    fake=False,\n                    fake_initial=False,\n                )\n", "    def test_replacements(self):\n        # Test that replacements are correctly marked as applied.\n        with self.subTest(\"Initial Migration\"):\n            self.create_migration(\"migrations\", \"0001_initial\", \"create_model\", \"TestModel\")\n            self.execute_migration(\n                app_label=\"migrations\",\n                name=\"0001_initial\",\n                fake=True,\n            )\n\n        with self.subTest(\"Replacement Migration\"):\n            self.create_migration(\n                \"migrations\", \"0002_replace_testmodel\", \"replace_model\", \"TestModel\", replacements=[(\"migrations\", \"0001_initial\")]\n            )\n            state = self.execute_migration(\n                app_label=\"migrations\", name=\"0002_replace_testmodel\", fake=True\n            )\n            self.assertEqual(\n                self.recorder.applied_migrations(),\n                [\"migrations\", \"0002_replace_testmodel\"],\n            )\n\n        # Now try unapplying the replacement\n        self.execute_migration(\n            app_label=\"migrations\", name=\"0002_replace_testmodel\", clean_start=True, backwards=True, fake=True\n        )\n        with self.subTest(\"Replaced Migration UnApplied\"):\n            self.assertEqual(\n                self.recorder.applied_migrations(), [\"migrations\", \"0001_initial\"]\n            )\n", "        def test_migration_plan(self):\n            with self.assertNumQueries(0) as cm:\n                executor = MigrationExecutor(self.connection)\n                plan = executor.migration_plan(\n                    [(\"migrations\", \"0001_initial\")],\n                    clean_start=True\n                )\n                self.assertEqual(\n                    sorted(plan),\n                    [\n                        (\n                            migrations.test_migrations.Migration(\n                                app_label=\"migrations\",\n                                name=\"0001_initial\",\n                                ...\n                            ),\n                            False\n                        ),\n                    ]\n                )\n", "    def test_migration_plan_with_replacements(self):\n        with self.subTest(replacement_migrations=True):\n            with override_settings(\n                MIGRATION_MODULES={\"migrations\": \"migrations.test_migrations\"},\n            ):\n                self.clear_migration_history()\n                state = ProjectState(real_apps=global_apps.get_app_configs())\n                executor = MigrationExecutor(connection, progress_callback=None)\n                plan = executor.migration_plan(\n                    [(\"migrations\", \"0001_initial\")],\n                    clean_start=True\n                )\n                self.assertEqual(len(plan), 1)\n                self.assertTrue(\n                    any(\n                        (migration.app_label, migration.name) == (\"migrations\", \"0001_initial\")\n                        for mig, _ in plan\n                    )\n                )\n", "    def test_migration_plan_with_app(self):\n        with self.assertRaises(InvalidMigrationPlan) as cm:\n            executor = MigrationExecutor(self.connection, progress_callback=lambda *args: None)\n            executor.migration_plan([\"migrations\", \"migrations2\"], clean_start=True)\n        self.assertIn(\"Migration plans with both forwards and backwards migrations\", str(cm.exception))\n\n\n\n"], "sample_434": ["    def test_redirect_view_temporary_redirect(self):\n        class RedirectViewTest(RedirectView):\n            permanent = False\n            url = '/some/redirect/'\n\n        request = RequestFactory().get('/some/path/')\n        view = RedirectViewTest()\n\n        response = view.get(request)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response['Location'], '/some/redirect/')\n", "    def test_http_method_not_allowed(self):\n        view = RedirectView()\n        request = RequestFactory().get('/test/')\n        response = view.dispatch(request)\n        self.assertEqual(response.status_code, 405)\n        self.assertEqual(response.content, 'Method Not Allowed')\n", "compilation error", "    def test_view_is_async(self):\n        class AsyncView(View):\n            async def get(self, request):\n                return HttpResponse(\"Async!\")\n\n        with self.subTest(name=\"async view\"):\n            view = AsyncView.as_view()\n            self.assertTrue(async_to_sync(view(self.rf.get('/'))).text == \"Async!\")\n            self.assertTrue(AsyncView.view_is_async)\n\n        with self.subTest(name=\"sync view\"):\n            class SyncView(View):\n                    return HttpResponse(\"Sync!\")\n\n            view = SyncView.as_view()\n            self.assertTrue(view(self.rf.get('/')).text == \"Sync!\")\n            self.assertFalse(SyncView.view_is_async)\n\n        with self.subTest(name=\"mixed async/sync\"):\n            class MixedView(View):\n                async def get(self, request):\n                    return HttpResponse(\"Async!\")\n                \n                    return HttpResponse(\"Sync!\")\n\n            with self.assertRaises(ImproperlyConfigured):\n                MixedView.as_view()\n\n\n\n", "    def test_view_async_mismatch(self):\n        class MismatchedMethodsView(View):\n            async def get(self, request):\n                return HttpResponse(\"GET\")\n\n                return HttpResponse(\"POST\")\n\n        with self.assertRaises(ImproperlyConfigured):\n            async_to_sync(MismatchedMethodsView.as_view())()  \n"], "sample_347": ["    def test_localtime_is_naive(self):\n        naive_dt = datetime.datetime(2023, 10, 26, 12, 0)\n        with self.subTest(naive_dt=naive_dt):\n            with self.assertRaises(ValueError):\n                timezone.localtime(naive_dt)\n", "    def test_localtime_with_none_timezone(self):\n        naive_dt = datetime(2023, 10, 26, 10, 0)\n        with override_settings(USE_TZ=False):\n            local_dt = timezone.localtime(naive_dt)\n        self.assertEqual(local_dt, naive_dt)\n", "    def test_localtime_aware_datetime_with_different_timezone(self):\n        dt = datetime(2023, 10, 26, 12, 0, tzinfo=UTC)\n        local_dt = timezone.localtime(dt, timezone=EAT)\n        self.assertEqual(local_dt.tzinfo, EAT)\n        self.assertEqual(local_dt.hour, 14) \n    \n", "    def test_get_fixed_timezone_from_timedelta(self):\n        for offset in [-3600, 3600, 7200, -10800]:\n            tz = timezone.get_fixed_timezone(timedelta(minutes=offset))\n            self.assertEqual(tz.utcoffset(datetime.datetime.now(tz)), timedelta(minutes=offset))\n", "    def test_localtime_with_zoneinfo(self):\n        dt = datetime.datetime(2023, 10, 26, 10, 0, 0, tzinfo=CET)\n        local_dt = timezone.localtime(dt, timezone=PARIS_ZI)\n        self.assertEqual(local_dt.tzinfo, PARIS_ZI)\n        self.assertEqual(local_dt.hour, 10)\n        self.assertEqual(local_dt.minute, 0)\n        \n        with override_settings(USE_TZ=False):\n            local_dt = timezone.localtime(dt, timezone=PARIS_ZI)\n            self.assertIsNone(local_dt.tzinfo)\n            self.assertEqual(local_dt.hour, 10)\n            self.assertEqual(local_dt.minute, 0)\n"], "sample_1054": ["    def test_complex_region_union_intersection(self):\n\n        r1 = ComplexRegion(Interval(1, 2) * Interval(3, 4))\n        r2 = ComplexRegion(Interval(2, 3) * Interval(4, 5))\n        r3 = r1.union(r2)\n\n        r4 = ComplexRegion(Interval(1, 3) * Interval(3, 5))\n        self.assertEqual(r3, r4)\n\n        r5 = r1.intersection(r2)\n        self.assertEqual(r5, ComplexRegion(FiniteSet(2 + 3*I, 2 + 4*I, 3 + 3*I, 3 + 4*I)))\n\n        r6 = ComplexRegion(Interval(0, 1) * Interval(0, 2*pi))\n\n        # Test with multiple product sets in union and intersection\n        r7 = ComplexRegion(Union(Interval(1, 2)*Interval(3, 4), Interval(2, 3)*Interval(4, 5)))\n        r8 = ComplexRegion(Interval(1, 3)*Interval(3, 5))\n        self.assertEqual(r7.union(r8), r8)\n        self.assertEqual(r7.intersection(r8), r7)\n\n        r9 = ComplexRegion(Interval(0, 1) * Interval(0, pi))\n        r10 = ComplexRegion(Interval(0, 1) * Interval(pi, 2*pi))\n        self.assertEqual(r6.union(r9), r6)\n        self.assertEqual(r6.intersection(r9), r9)\n\n\n", "    def test_complex_region_polar_intersection(self):\n        from sympy.sets import Interval\n\n        unit_disk = ComplexRegion(Interval(0, 1)*Interval(0, 2*pi), polar=True)\n        upper_half_unit_disk = ComplexRegion(Interval(0, 1)*Interval(0, pi), polar=True)\n        intersection = unit_disk.intersect(upper_half_unit_disk)\n        self.assertEqual(intersection,upper_half_unit_disk)\n\n", "    def test_normalize_theta_set_interval(self):\n        self.assertEqual(normalize_theta_set(Interval(9*pi/2, 5*pi)), Interval(pi/2, pi))\n        self.assertEqual(normalize_theta_set(Interval(-3*pi/2, pi/2)), Interval(pi/2, 3*pi/2))\n        self.assertEqual(normalize_theta_set(Interval(-pi/2, pi/2)), Union(Interval(0, pi/2), Interval(3*pi/2, 2*pi)))\n        self.assertEqual(normalize_theta_set(Interval(-4*pi, 3*pi)), Interval(0, 2*pi))\n\n", "    def test_complex_region_from_real(self):\n        from sympy import Interval\n\n        self.assertEqual(ComplexRegion.from_real(Interval(2, 5)),\n                         ComplexRegion(Interval(2, 5) * {0}, False))\n        self.assertEqual(ComplexRegion.from_real(\n                         Interval(-1, 1)),\n                         ComplexRegion(Interval(-1, 1) * {0}, False))\n        self.assertEqual(ComplexRegion.from_real(\n                         Interval(0, oo)),\n                         ComplexRegion(Interval(0, oo) * {0}, False))\n        self.assertEqual(ComplexRegion.from_real(\n                         Interval(-oo, 0)),\n                         ComplexRegion(Interval(-oo, 0) * {0}, False))\n", "    def test_ComplexRegion_polar_form_intersection(self):\n        from sympy.sets import ImageSet\n        unit_disk = ComplexRegion(Interval(0, 1)*Interval(0, 2*S.Pi), polar=True)\n        upper_half_disk = ComplexRegion(Interval(0, 1)*Interval(0, S.Pi), polar=True)\n        intersection = unit_disk.intersect(upper_half_disk)\n        self.assertEqual(intersection, upper_half_disk)\n\n"], "sample_124": ["    def test_form_renderer_renders_fields(self):\n        form_class = Person\n        form = form_class()\n        renderer = DjangoTemplates()\n        rendered_html = renderer.render(form)\n        self.assertIn('<label for=\"id_first_name\">First name:</label>', rendered_html)\n        self.assertIn('<label for=\"id_last_name\">Last name:</label>', rendered_html)\n        self.assertIn('<label for=\"id_birthday\">Birthday:</label>', rendered_html)\n", "    def test_hidden_fields_are_rendered_correctly(self):\n        class HiddenFieldForm(Form):\n            name = HiddenInput()\n\n        form = HiddenFieldForm()\n        renderer = get_default_renderer()\n        output = renderer.render(form)\n        self.assertIn('<input type=\"hidden\" name=\"name\" id=\"id_name\" />', output)\n", "    def test_form_widget(self):\n        form = Person()\n        # Test default widget rendering.\n        # Expected Output: <input type=\"text\" name=\"first_name\">\n        output = form.fields['first_name'].widget.render(\n            form['first_name'],\n            attrs={'class': 'new_class'}\n        )\n        self.assertEqual(output.strip(), '<input type=\"text\" name=\"first_name\" class=\"new_class\">')\n\n        # Test for custom widget rendering.\n        form = PersonNew()\n        output = form.fields['first_name'].widget.render(\n            form['first_name'],\n            attrs={'class': 'new_class'}\n        )\n        self.assertEqual(output.strip(), '<input type=\"text\" name=\"first_name\" id=\"first_name_id\" class=\"new_class\">')\n\n\n\n", "    def test_form_with_errors(self):\n        form = Person()\n        form.is_valid = False\n        form['first_name'].errors = ErrorList(['First name too short.'])\n        form['last_name'].errors = ErrorList(['Last name required.'])\n        self.assertEqual(form.errors, {'first_name': ['First name too short.'], 'last_name': ['Last name required.']})\n", "    def test_multivalue_field_with_empty_value(self):\n        class MyMultiValueField(MultiValueField):\n                return data_list\n\n                return [value]\n\n        class MyForm(Form):\n            my_field = MyMultiValueField(fields=(CharField(),), empty_value='')\n\n        form = MyForm()\n        self.assertEqual(form.cleaned_data['my_field'], '')\n\n        form = MyForm({'my_field': ''})\n        self.assertEqual(form.cleaned_data['my_field'], '')\n\n        form = MyForm({'my_field': 'some value'})\n        self.assertEqual(form.cleaned_data['my_field'], ['some value'])\n\n\n\n"], "sample_992": ["    def test_print_MatrixBase(self):\n        m = SparseMatrix(((1, 2), (3, 4)))\n        for printer in (PythonCodePrinter, NumPyPrinter, SciPyPrinter):\n            with raises(NotImplementedError):\n                printer().doprint(m)\n", "def test_matrix_multiplication():\n    m1 = SparseMatrix([[1, 2], [3, 4]])\n    m2 = SparseMatrix([[5, 6], [7, 8]])\n    expected = SparseMatrix([[19, 22], [43, 50]])\n    assert (pycode(m1.dot(m2), standard='python3')\n            == expected.tolist())\n", "    def test_sparse_matrix(self):\n        sparse_matrix = SparseMatrix.from_dict(((0, 0), 1), ((1, 0), 2))\n        s = SciPyPrinter().doprint(sparse_matrix)\n        assert s == \"scipy.sparse.coo_matrix([[1], [2]], shape=(2, 1))\"\n", "    def test_sparse_matrix(self):\n        mat = SparseMatrix(((1, 0, 2), (0, 1, 3)), (2, 3))\n        code = SciPyPrinter().doprint(mat)\n        assert code == \"\"\"scipy.sparse.coo_matrix([[1, 2], [0, 3]], ((0, 0), (0, 2), (1, 1)), shape=(2, 3))\"\"\"\n\n\n\n", "    def test_scipy_sparse(self):\n        mat = SparseMatrix(((1, 0, 2), (0, 1, 3)), shape=(2, 3))\n        code = SciPyPrinter().doprint(mat)\n        assert code == \"scipy.sparse.coo_matrix([[2, 0, 3], [1, 1, 0]], (array([0, 1], dtype=int64), array([0, 1, 2], dtype=int64)), shape=(2, 3))\"\n"], "sample_108": ["    def test_dynamic_converter(self):\n        url_pattern = path('dynamic/<int:id>/', view=empty_view)\n        test_data = [\n            ('/dynamic/123/', {'id': 123}),\n            ('/dynamic/abc/', ()),  \n        ]\n        for url, expected_kwargs in test_data:\n            with self.subTest(url=url):\n                result = resolve(url)\n                self.assertEqual(result.resolver_match.kwargs, expected_kwargs)\n", "    def test_converter(self):\n        urlpatterns = [\n            path('base64/<slug:value>/', empty_view, name='base64'),\n            path('base64/<slug:value>/subpatterns/<slug:value2>/', empty_view, name='subpattern-base64'),\n        ]\n        for url, (name, app_name, kwargs) in converter_test_data:\n            with self.subTest(url=url):\n                with self.subTest(name=name):\n                    with self.subTest(app_name=app_name):\n                        with self.subTest(kwargs=kwargs):\n                            view, args, kwargs = resolve(url)\n                            self.assertEqual(view.func, empty_view)\n                            self.assertEqual(args, ())\n                            self.assertEqual(kwargs, kwargs)\n\n                result = reverse(name, args=(b'hello', b'world'), kwargs=kwargs)\n                if name == 'subpattern-base64':\n                    self.assertEqual(result, '/base64/aGVsbG8=/subpatterns/d29ybGQ=/')\n", "    def test_url_reverse_with_dynamic_converter(self):\n        for url, (url_name, app_name, kwargs) in converter_test_data:\n            with self.subTest(url=url, url_name=url_name, app_name=app_name, kwargs=kwargs):\n                # Test with dynamic converter\n                url_name = 'converter'\n                app_name = ''\n                kwargs = {'base': 'hello', 'value': 'world'}\n                view = lambda request: 'success'\n                converter = DynamicConverter(\n                    {'value': 'base64'}\n                )\n                namespace = 'converter'\n                self.addCleanup(converter.unregister)\n\n                with override_settings(\n                    INSTALLED_APPS=[\n                        'urlpatterns'\n                    ],\n                ):\n                    urlpatterns = [\n                        path(\n                            f'{url}',\n                            view,\n                            name=url_name,\n                            app_name=app_name,\n                            converters={'value': converter},\n                        ),\n                    ]\n                    resolver = URLResolver(None, urlpatterns)\n                    resolved_url = resolver.reverse(url_name, value='hello')\n                    self.assertEqual(resolved_url, url)\n\n                    # Test with dynamic converter in namespace\n                    namespace = 'converter'\n                    urlpatterns = [\n                        path(\n                            f'{url}',\n                            view,\n                            name=url_name,\n                            app_name=app_name,\n                            namespace=namespace,\n                            converters={'value': converter},\n                        ),\n                    ]\n                    resolver = URLResolver(None, urlpatterns, namespace=namespace)\n                    resolved_url = resolver.reverse(url_name, value='hello')\n                    self.assertEqual(resolved_url, url)\n", "    def test_reverse_with_converter(self):\n        for url, (url_name, app_name, kwargs) in converter_test_data:\n            with self.subTest(url=url, url_name=url_name, app_name=app_name, kwargs=kwargs):\n                resolved_url = reverse(url_name, namespace=app_name, **kwargs)\n                self.assertEqual(resolved_url, url)\n\n\n\n", "    def test_dynamic_converter(self):\n        # Test usage of DynamicConverter\n        with self.subTest(name='with_dynamic_converter'):\n            url, name, app_name, kwargs = converter_test_data[0]\n            view = empty_view\n            url_pattern = path(\n                url,\n                view,\n                name='test_dynamic_converter',\n                kwargs={'base': DynamicConverter()}\n            )\n            expected_kwargs = {'base': b'hello'}\n            with self.assertNoWarnings():\n                resolved_view, resolved_args, resolved_kwargs = resolve('/base64/aGVsbG8=/')\n                self.assertEqual(resolved_view, view)\n                self.assertEqual(resolved_kwargs, expected_kwargs)\n"], "sample_335": ["    def test_decimal_field_with_too_small_value(self):\n        field = DecimalField(max_digits=5, decimal_places=2)\n        with self.assertRaises(ValidationError):\n            field.clean('-123456.78')\n", "    def test_decimal_field_with_max_digits_and_decimal_places(self):\n        field = DecimalField(max_digits=5, decimal_places=2)\n\n        self.assertEqual(field.to_python(\"12.34\"), decimal.Decimal(\"12.34\"))\n        self.assertEqual(field.to_python(\"12.345\"), decimal.Decimal(\"12.35\"))\n\n        with self.assertRaises(ValidationError):\n            field.to_python(\"12.3456\")\n\n        with self.assertRaises(ValidationError):\n            field.to_python(\"12345\")\n\n        with self.assertRaises(ValidationError):\n            field.to_python(\"-12.345\")  \n\n        with self.assertRaises(ValidationError):\n            field.to_python(\"1.2.3\")\n\n\n\n", "    def test_decimal_field_with_localize(self):\n        with override_settings(USE_L10N=True):\n            field = DecimalField(localize=True)\n            self.assertEqual(field.to_python('123.45'), decimal.Decimal('123.45'))\n            self.assertEqual(field.to_python('123,45'), decimal.Decimal('123,45'))\n            self.assertEqual(field.to_python('123 45'), decimal.Decimal('123 45'))\n\n            # When localize is True, the input format should be locale-specific.\n            # For this test, assuming the locale is set to `en-US`.\n            with translation.override('en-US'):\n                self.assertEqual(field.to_python('123.45'), decimal.Decimal('123.45'))\n", "    def test_decimal_field_with_custom_widget(self):\n        with self.settings(LANGUAGE_CODE='en-us'):\n            f = DecimalField(widget=NumberInput())\n            self.assertEqual(f.widget.input_type, 'number')\n            f.clean('123.45')\n            f.clean('-123.45')\n            f.clean('123')\n            with self.assertRaisesMessage(ValidationError, 'Enter a valid number.'):\n                f.clean('abc')\n", "    def test_decimalfield_localize_with_formats(self):\n        with override_settings(USE_L10N=True):\n            field = DecimalField(decimal_places=2, max_digits=5, localize=True,\n                                 input_format=\"%d.%02d\",\n                                 error_messages={'invalid': 'Incorrect decimal format.'})\n            self.assertIsInstance(field.widget, NumberInput)\n\n            self.assertFormValid(\n                {'value': '12'}, field, {}, 'value', '12.00'\n            )\n            self.assertFormInvalid(\n                {'value': '12.'},\n                field,\n                {},\n                'value',\n                'Incorrect decimal format.'\n            )\n            self.assertFormInvalid(\n                {'value': '12.345'},\n                field,\n                {},\n                'value',\n                'Incorrect decimal format.'\n            )\n            self.assertFormInvalid(\n                {'value': '12,34'},\n                field,\n                {},\n                'value',\n                'Incorrect decimal format.'\n            )\n            self.assertFormInvalid(\n                {'value': 'abc'},\n                field,\n                {},\n                'value',\n                'Incorrect decimal format.'\n            )\n\n\n\n"], "sample_51": ["    def test_parse_datetime_with_timezone_offset(self):\n        dt = parse_datetime('2023-10-26T10:30:00+05:30')\n        self.assertEqual(dt.year, 2023)\n        self.assertEqual(dt.month, 10)\n        self.assertEqual(dt.day, 26)\n        self.assertEqual(dt.hour, 10)\n        self.assertEqual(dt.minute, 30)\n        self.assertEqual(dt.second, 0)\n        self.assertEqual(dt.microsecond, 0)\n        self.assertEqual(dt.tzinfo, get_fixed_timezone(19800)) \n", "    def test_parse_datetime_with_postgres_interval(self):\n        dt = parse_datetime(\"3 days 04:05:06\")\n        self.failUnless(dt.date() == date(2023, 1, 1) + timedelta(days=3))\n        self.failUnless(dt.time() == time(4, 5, 6))\n\n\n", "    def test_parse_datetime_utc_offset_with_minutes(self):\n        dt = parse_datetime('2023-10-26 10:30:00+05:30')\n        self.assertEqual(dt.year, 2023)\n        self.assertEqual(dt.month, 10)\n        self.assertEqual(dt.day, 26)\n        self.assertEqual(dt.hour, 10)\n        self.assertEqual(dt.minute, 30)\n        self.assertEqual(dt.second, 0)\n        self.assertEqual(dt.tzinfo, get_fixed_timezone(19500))\n", "    def test_parse_datetime_with_timezone(self):\n        dt = parse_datetime('2023-10-26T14:30:00+05:30')\n        self.assertEqual(dt.year, 2023)\n        self.assertEqual(dt.month, 10)\n        self.assertEqual(dt.day, 26)\n        self.assertEqual(dt.hour, 14)\n        self.assertEqual(dt.minute, 30)\n        self.assertEqual(dt.tzinfo, get_fixed_timezone(19800))  # 5.5 hours in seconds\n\n\n", "    def test_parse_datetime_with_utc_offset(self):\n        dt = parse_datetime('2023-10-26T12:00:00+05:30')\n        self.assertEqual(dt.year, 2023)\n        self.assertEqual(dt.month, 10)\n        self.assertEqual(dt.day, 26)\n        self.assertEqual(dt.hour, 12)\n        self.assertEqual(dt.minute, 0)\n        self.assertEqual(dt.second, 0)\n        self.assertEqual(dt.microsecond, 0)\n        self.assertEqual(dt.tzinfo, get_fixed_timezone(19500))\n"], "sample_979": ["    def test_matrix_element_derivative():\n        from sympy.matrices.expressions.matexpr import MatrixElement\n        a = MatrixSymbol('a', 2, 2)\n        e = MatrixElement(a, 0, 0)\n        v = MatrixElement(a, 1, 1)\n        d = diff(e, v)\n        assert d == 0\n", "    def test_matrix_element_derivative():\n        from sympy.matrices.expressions.matexpr import MatrixElement\n        A = MatrixSymbol('A', 3, 3)\n        me = MatrixElement(A, 1, 2)\n        v = MatrixElement(A, 0, 0)\n        result = me.diff(v)\n        expected =  KroneckerDelta(1, 0) * KroneckerDelta(2, 0)\n        assert result == expected\n", "    def test_zero_matrix_mul():\n        z = ZeroMatrix(3, 3)\n        raises(TypeError, lambda: z * A)\n        raises(TypeError, lambda: A * z)\n        raises(TypeError, lambda: z + A)\n        raises(TypeError, lambda: A + z)\n\n", "    def test_matrix_element_derivatives():\n        A = MatrixSymbol('A', 3, 3)\n        a = MatrixElement(A, 1, 2)\n        da = diff(a, A[1, 2])\n        assert da == 1\n\n", "    def test_zero_matrix_simplification(self):\n        Z = ZeroMatrix(2, 3)\n        self.assertEqual(simplify(Z), Z)\n"], "sample_353": ["compilation error", "    def test_createsuperuser_interactive_with_valid_input(self):\n        with mock.patch('builtins.input', side_effect=['alice', 'password', 'password']):\n            call_command('createsuperuser')\n        user = User.objects.get(username='alice')\n        self.assertTrue(user.is_superuser)\n\n\n\n", "    def test_password_validation_skipped(self):\n        with self.assertLogs() as cm:\n            call_command(\n                'createsuperuser',\n                '--noinput',\n                '--%s' % CustomUser._meta.username_field, 'alice',\n                '--%s' % CustomUser._meta.date_of_birth_field, date.today().isoformat(),\n            )\n        assert len(cm.output) == 1\n        # Ensure password validation is skipped.\n        assert 'Error: Your passwords didn\\'t match.' not in cm.output\n        # \n        # Ensure it was successful\n        assert 'Superuser created successfully.' in cm.output\n\n\n", "    def test_createsuperuser_with_password(self):\n        with mock.patch('builtins.input', wraps=builtins.input, side_effect=lambda x: ''):\n            call_command('createsuperuser', verbosity=0)\n\n\n", "    def test_password_validation_skipped(self):\n        with override_settings(\n            INSTALLED_APPS=[\n                'django.contrib.auth',\n            ],\n        ):\n            call_command(\"createsuperuser\", \"--username\", \"alice\", \"--noinput\")\n            # We should have a superuser created even if password validation is bypassed.\n\n\n\n"], "sample_532": ["    def test_contourf_origin_upper(self):\n        with rc_context({'contour.origin': 'upper'}):\n            x = np.arange(0, 10, 0.25)\n            y = np.arange(0, 10, 0.25)\n            X, Y = np.meshgrid(x, y)\n            Z = X * np.sin(Y + (X * 1.0))\n            plt.contourf(X, Y, Z, levels=np.linspace(0, 2.5, 5))\n            plt.colorbar()\n\n\n", "    def test_contour_basic(self):\n        np.random.seed(1)\n        x = np.random.rand(10, 10)\n        y = np.random.rand(10, 10)\n        z = np.random.rand(10, 10)\n\n        fig, ax = plt.subplots()\n        c = ax.contour(x, y, z)\n        ax.clabel(c, inline=True, fontsize=8)\n\n        ax.set_xlim(0, 1)\n        ax.set_ylim(0, 1)\n\n        plt.show()\n\n\n\n", "    def test_corner_mask(self):\n        x = np.linspace(0, 1, 10)\n        y = x\n        X, Y = np.meshgrid(x, y)\n        Z = np.sin(2*np.pi*X) * np.sin(2*np.pi*Y)\n\n        with rc_context(\n            {'contour.corner_mask': True}):\n            cs1 = plt.contour(X, Y, Z)\n            img1 = plt.gcf().canvas.tostring_rgb()\n\n        with rc_context(\n            {'contour.corner_mask': False}):\n            cs2 = plt.contour(X, Y, Z)\n            img2 = plt.gcf().canvas.tostring_rgb()\n\n        assert_array_almost_equal_nulp(img1, img2, 3)\n", "    def test_contour_basic(self):\n        x = np.linspace(0, 1, 100)\n        y = np.linspace(0, 1, 100)\n        X, Y = np.meshgrid(x, y)\n        Z = np.sin(np.sqrt(X**2 + Y**2))\n\n        fig, ax = plt.subplots()\n        contour = ax.contour(X, Y, Z, levels=5)\n        ax.clabel(contour, inline=True, fontsize=8)\n        ax.set_title('Contour Plot', fontsize=14)\n        ax.set_xlabel('x')\n        ax.set_ylabel('y')\n        plt.show()\n", "    def test_corner_mask(self):\n        np.random.seed(1)\n        x = np.linspace(0, 1, 10)\n        y = np.linspace(0, 1, 10)\n        X, Y = np.meshgrid(x, y)\n        Z = np.sin(np.sqrt(X**2 + Y**2))\n        masked_Z = np.ma.masked_where(Z > 0.8, Z)\n        with rc_context({'contour.corner_mask': True}):\n            cs = plt.contour(X, Y, masked_Z)\n            fig, ax = plt.subplots()\n            ax.contour(X, Y, masked_Z, levels=cs.levels)\n            plt.show()\n        with rc_context({'contour.corner_mask': False}):\n            cs = plt.contour(X, Y, masked_Z)\n            fig, ax = plt.subplots()\n            ax.contour(X, Y, masked_Z, levels=cs.levels)\n            plt.show()\n"], "sample_2": ["compilation error", "    def test_wcs_keyword_removal_for_wcs_test_files(tmpdir):\n        \"\"\"\n        Test that 'PC' and 'CD' keywords are correctly removed from the\n        header when converting a WCS object from a FITS header.\n        \"\"\"\n        with NumpyRNGContext(42):\n            data = np.random.rand(DEFAULT_DATA_SIZE, DEFAULT_DATA_SIZE)\n        ccd = create_ccd_data()\n        ccd.wcs = WCS(fits.Header())\n        filename = tmpdir.join('test_wcs.fits')\n        fits.writeto(filename, ccd.data, header=ccd.header, overwrite=True)\n        with fits.open(filename) as hdus:\n            new_hdr = hdus[0].header\n            ccd_data = CCDData(hdus[0].data, meta=new_hdr, wcs=None)\n            # Verify that the PC and CD keywords are not present in the header\n            assert 'PC1_1' not in new_hdr\n            assert 'PC1_2' not in new_hdr\n            assert 'PC2_1' not in new_hdr\n            assert 'PC2_2' not in new_hdr\n            assert 'CD1_1' not in new_hdr\n            assert 'CD1_2' not in new_hdr\n            assert 'CD2_1' not in new_hdr\n            assert 'CD2_2' not in new_hdr\n            # Ensure that we can read the header again and create a CCDData\n            # object.\n        with fits.open(filename) as hdus:\n            ccd_data_read = CCDData(hdus[0].data, meta=hdus[0].header,\n                                    wcs=None)\n            assert np.allclose(ccd_data.data, ccd_data_read.data)\n            assert ccd_data.header == ccd_data_read.header\n\n\n\n", "    def test_fits_ccddata_writer_with_uncertainty(tmpdir):\n        ccd = create_ccd_data()\n        ccd.uncertainty = StdDevUncertainty(\n            np.random.normal(size=_random_array.shape)\n        )\n        filename = str(tmpdir.join('test_ccddata.fits'))\n\n        fits_ccddata_writer(ccd, filename, as_image_hdu=True)\n        with fits.open(filename) as hdul:\n            assert hdul[1].header.get('UTYPE') == 'StdDevUncertainty'\n", "    def test_wcs_keyword_removal_for_wcs_test_files(tmp_path):\n        \"\"\"\n        Test that WCS keywords are correctly removed from the header\n        when a CCDData object is created in a way that can be understood by astropy.wcs\n        \"\"\"\n        ccd_data = create_ccd_data()\n\n        # Create a temporary FITS file and write the CCDData object to it\n        with fits.open(str(tmp_path / 'test_ccddata.fits'), 'w') as hdus:\n            hdus.append(ccd_data.to_hdu())\n\n        # Read the FITS file back in and check that the WCS keywords\n        # have been removed correctly\n        with fits.open(str(tmp_path / 'test_ccddata.fits')) as hdus:\n            new_hdr = hdus[0].header\n\n        # Check that the WCS related keywords have been\n        # removed from the header\n        for keyword in _KEEP_THESE_KEYWORDS_IN_HEADER:\n            assert keyword in new_hdr, \"Expected keyword '{}' to be in header\".format(keyword)\n\n        for keyword in _PCs:\n            assert keyword not in new_hdr, \"Unexpected keyword '{}' in header\".format(keyword)\n        for keyword in _CDs:\n            assert keyword not in new_hdr, \"Unexpected keyword '{}' in header\".format(keyword)\n\n\n\n", "    def test_uncertainty_unit_equivalence(unit):\n        ccd = create_ccd_data()\n        ccd.uncertainty = StdDevUncertainty(np.random.random(ccd.shape), unit=unit)\n        hdu = ccd.to_hdu()\n        with fits.open(io.StringIO(hdu.tostring())) as hdus:\n            unc_hdu = hdus[1]\n            assert unc_hdu.header['BUNIT'] == unit.to_string()\n"], "sample_718": ["    def __init__(self, value):\n        self.value = value\n", "def check_sparse_matrices_as_data(name, estimator_orig):\n    if name in CROSS_DECOMPOSITION:\n        raise SkipTest(\"Skipping check_sparse_matrices_as_data \"\n                       \"for cross decomposition module as estimators \"\n                       \"are not deterministic.\")\n\n    # test sparse matrices as input\n    X = sparse.csr_matrix([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2, 3])\n\n    estimator = clone(estimator_orig)\n    set_random_state(estimator)\n    estimator.fit(X, y)\n    pred = estimator.predict(X)\n    assert_array_equal(pred, np.array([1, 2, 3]))\n\n\n", "compilation error", "def check_classifier_accepts_sparse_data(name, estimator_orig):\n    if name in ['Perceptron', 'PassiveAggressiveClassifier', 'RidgeClassifier']:\n        return\n    # Test that a classifier accepts sparse data as input\n    X_sparse = sp.csc_matrix(np.random.rand(10, 5))\n    y = np.random.randint(0, 2, size=10)\n    estimator = clone(estimator_orig)\n    set_random_state(estimator)\n    estimator.fit(X_sparse, y)\n    pred = estimator.predict(X_sparse)\n    assert_true(isinstance(pred, np.ndarray))\n\n\n\n", "def check_estimator_pickle_roundtrip(name, estimator_orig):\n    X = np.random.rand(10, 5)\n    y = np.random.rand(10)\n    estimator = clone(estimator_orig)\n    estimator.fit(X, y)\n    with open(StringIO(), 'wb') as f:\n        joblib.dump(estimator, f)\n    loaded_estimator = joblib.load(f)\n    assert_true(hasattr(loaded_estimator, 'predict'))\n    assert_equal(loaded_estimator.predict(X), estimator.predict(X))\n\n\n\n"], "sample_570": ["    def test_kde_univariate_cumulative(self, x, rng, y):\n        kde = KDE(cumulative=True)\n        kde_est, _ = kde(x)\n        integral = self.integrate(kde_est, x)\n        assert_array_almost_equal(integral, 1, decimal=5)\n", "    def test_kde_univariate_cumulative(self, x, rng):\n        kde = KDE()\n        density_result, support = kde(x, weights=rng.uniform(0, 5, 100))\n        cumulative_result = np.cumsum(density_result * np.diff(support))\n        expected_result = self.integrate(x, support)\n        assert_array_almost_equal(cumulative_result, expected_result)\n", "    def test_kde_hist_comparison(self, x, weights):\n        kde = KDE(bw_method='scott')\n        hist = Histogram(stat=\"density\")\n        kde.define_support(x, cache=False)\n        kde_vals, _ = kde._eval_univariate(x, weights)\n\n        hist_vals, _ = hist(x, weights=weights)\n\n        # Visual comparison (make sure the outputs look similar on a plot)\n        import matplotlib.pyplot as plt\n        plt.hist(x, bins=hist.define_bin_params(x, cache=False)['bins'], weights=weights, density=True)\n        plt.plot(kde.support, kde_vals)\n        plt.show()\n\n        # Numerical comparison (only for densities, the normalization should be\n        # consistent)\n        assert_array_almost_equal(kde_vals, hist_vals)\n", "    def test_kde_univariate_cumulative(self, x, rng):\n        kde = KDE(cumulative=True)\n        # Make sure weights don't change the result\n        kde(x, weights=rng.rand(len(x)))\n        y = kde(x)\n        cdf = y.cumsum()\n        # Verify that the cumulative sum of KDE density matches the integral\n        assert_array_almost_equal(\n            cdf[-1], self.integrate(np.ones_like(y), x), decimal=5\n        )\n        # Test with clipping\n\n", "    def test_kde_bivariate(self, x, y, weights):\n        kde = KDE(bw_method=\"scott\")\n        kde_fit = kde([x, y], weights=weights)\n        kde_dens, _ = kde_fit([x, y], weights=weights)\n        expected_kde = self.integrate(kde_dens, x)\n        assert_array_almost_equal(self.integrate(kde_dens, x), expected_kde)\n\n\n\n"], "sample_11": ["compilation error", "compilation error", "    def test_dropped_world_dimensions(self):\n        sliced_wcs = SlicedLowLevelWCS(WCS_SPECTRAL_CUBE, (slice(0,20), slice(None), slice(None)))\n        dropped_info = sliced_wcs.dropped_world_dimensions\n        assert len(dropped_info) == 1\n        assert dropped_info[\"world_axis_names\"][0] == \"Frequency\"\n\n\n", "    def test_slices_spectral(self):\n        # Test slicing with spectral axis\n        slices_spec = (slice(10, 20), slice(None), slice(None))\n        sliced_wcs = SlicedLowLevelWCS(WCS_SPECTRAL_CUBE, slices_spec)\n\n        # Test the pixel bounds\n        assert sliced_wcs.pixel_bounds == ((10, 20), (0, 20), (0, 30))\n\n\n        # Test the output of pixel_to_world_values and world_to_pixel_values\n\n        # For a single point\n        pixel = (1, 10, 1)\n        world_coords = sliced_wcs.pixel_to_world_values(*pixel)\n        assert_allclose(world_coords, WCS_SPECTRAL_CUBE.pixel_to_world(pixel))\n\n        pixel_coords = sliced_wcs.world_to_pixel_values(*world_coords)\n        assert_equal(pixel_coords, pixel)\n\n        # For a set of points\n        pixel_array = np.array([[1, 10, 1], [2, 15, 1]])\n        world_arrays = sliced_wcs.pixel_to_world_values(*pixel_array)\n        pixel_arrays = sliced_wcs.world_to_pixel_values(*world_arrays)\n        assert_allclose(pixel_arrays, pixel_array)\n", "    def test_sliced_wcs_pixel_bounds(self):\n        slices = (slice(1, 5), slice(5, 15), slice(None))\n        sliced_wcs = SlicedLowLevelWCS(WCS_SPECTRAL_CUBE, slices)\n        assert_equal(sliced_wcs.pixel_bounds, ((1, 5), (5, 15), (None, None)))\n\n"], "sample_631": ["    def test_unused_argument(self):\n        \"\"\"Test unused argument checks.\"\"\"\n        code = \"\"\"", "    def test_undefined_var_in_closure(self):\n        code = \"\"\"", "    def test_unused_arguments(self):\n        \"\"\"Check unused arguments in function.\"\"\"\n        code = \"\"\"", "    def test_metaclasses(self):\n        code = \"\"\"", "    def test_nested_function_scope(self):\n        code = \"\"\""], "sample_1011": ["  def test_bessel_functions_octave(self):\n      for func in [jn, yn, besselj, bessely, besseli, besselk, hankel1, hankel2,\n                   airyai, airybi, airyaiprime, airybiprime]:\n          args = [x, 0]\n          try:\n              mcode(func(*args))\n          except Exception as e:\n              print(f\"Function {func.__name__} failed with args {args}.\")\n              raise e from e \n          \n", "compilation error", "    def test_custom_functions():\n        from sympy import Function\n        from sympy import symbols\n        x = symbols('x')\n        f = Function('f')\n        g = Function('g')\n        custom_functions = {\n            \"f\": \"existing_octave_fcn\",\n            \"g\": [(lambda x: x.is_Matrix, \"my_mat_fcn\"),\n                  (lambda x: not x.is_Matrix, \"my_fcn\")]\n        }\n        result = octave_code(f(x) + g(x) + g(x*x), user_functions=custom_functions)\n        assert result == \"existing_octave_fcn(x) + my_fcn(x) + my_mat_fcn(x*x)\"\n\n\n", "compilation error", "    def test_MatrixProduct():\n        A = MatrixSymbol('A', 2, 2)\n        B = MatrixSymbol('B', 2, 2)\n        code = octave_code(A*B)\n        assert code == '(A*B)'\n\n        code = octave_code(A * B * 2)\n        assert code == '2*(A*B)'\n"], "sample_37": ["    def test_wcssubs_from_fits_file(self):\n        for filename in self._files_to_test:\n            with catch_warnings():\n                hdulist = fits.open(filename)\n            _, data_file = self._files_to_test[filename]\n            with catch_warnings():\n                with NumpyRNGContext():\n                    wcs_in = wcs.WCS(hdulist[0].header)\n                    wcs_out = wcs_in.sub([WCSSUB_CELESTIAL])\n                    assert_array_equal(wcs_in.wcs.ctype, wcs_out.wcs.ctype)\n  ", "    def test_wcs_from_hdr(self, filename):\n        header = fits.getheader(get_pkg_data_filename(\"maps\", filename))\n        wcs = wcs.WCS(header)\n        assert wcs.naxis == header[\"NAXIS\"]\n        assert wcs.wcs.ctype == header[\"CTYPE\"].strip()\n        assert wcs.wcs.crval == np.array(header[\"CRVAL\"]).reshape((wcs.naxis, ))\n\n\n\n", "    def test_maps(self, file_name):\n        file_path = os.path.join(os.path.dirname(__file__), file_name)\n        with open(file_path, \"rb\") as f:\n            hdu = fits.open(f)\n        wcs = wcs.WCS(hdu[0].header)\n        with NumpyRNGContext():\n            nx = 100\n            ny = 100\n            x = np.arange(nx)\n            y = np.arange(ny)\n            xv, yv = np.meshgrid(x, y)\n            world_coords = wcs.wcs_pix2world(xv.flatten(), yv.flatten(), 0)\n        assert world_coords.shape[0] == nx * ny\n        assert np.all(np.abs(world_coords[:, 0] - wcs.wcs.crval[0]) < 1e-6)\n        assert np.all(np.abs(world_coords[:, 1] - wcs.wcs.crval[1]) < 1e-6)\n        hdu.close()\n\n", "    def test_wcs_bad_header(self):\n        from astropy.io import fits\n\n        # Create a header with a deliberately incorrect cdelt keyword\n        bad_header = fits.Header()\n        bad_header['SIMPLE'] = True\n        bad_header['NAXIS'] = 2\n        bad_header['CTYPE1'] = 'RA---TAN'\n        bad_header['CTYPE2'] = 'DEC--TAN'\n        bad_header['CDELT1'] = 0.5\n        bad_header['CDELT2'] = -0.5\n\n        with pytest.raises(fits.WCSError):\n            wcs = wcs.WCS(bad_header)\n\n\n\n", "    def test_maps(self, file_name):\n        f = get_pkg_data_filename(\"maps\", file_name)\n        with open(f) as header_file:\n            header_contents = header_file.read()\n        header = fits.Header.fromstring(header_contents)\n\n        wcs = wcs.WCS(header)\n        \n        # Test that the WCS object can be created successfully\n        assert wcs is not None\n\n        # Test the basic coordinate transformations\n        x_world, y_world = wcs.wcs_pix2world(10, 10, 0)\n        assert isinstance(x_world, float)\n        assert isinstance(y_world, float)\n\n        x_pix, y_pix = wcs.wcs_world2pix(x_world, y_world, 0)\n        assert isinstance(x_pix, float)\n        assert isinstance(y_pix, float)\n\n        # Test that the inverse transformations are close to the identity\n        assert_allclose(x_pix, 10, atol=1e-6)\n        assert_allclose(y_pix, 10, atol=1e-6)\n\n\n\n"], "sample_838": ["    def test_remainder_with_estimator(self):\n        remainder_transformer = Trans()\n\n        ct = make_column_transformer(\n            (StandardScaler(), [0]),\n            (OneHotEncoder(), [1]),\n            remainder=remainder_transformer\n        )\n\n        X = np.array([[1, 'a'],\n                      [2, 'b'],\n                      [3, 'c']])\n\n        X_transformed = ct.fit_transform(X)\n\n        assert X_transformed.shape[1] == 3  # 2 + 1 (remainder)\n\n        # check remainder output\n        assert_array_equal(\n            X_transformed[:, 2],\n            remainder_transformer.transform(X[:, 2]).values\n        )\n\n\n", "    def test_remainder(self, remainder):\n        # Test that the remainder functionality works correctly\n        X = np.array([[1, 2], [3, 4]])\n        transformer1 = Trans()\n        transformer2 = Normalizer()\n\n        if remainder == 'passthrough':\n            ct = make_column_transformer(\n                [(transformer1, [0]), (transformer2, [1])],\n                remainder=remainder)\n        else:\n            ct = make_column_transformer(\n                [(transformer1, [0]), (transformer2, [1])],\n                remainder=remainder)\n\n        X_transformed = ct.transform(X)\n        expected_output = np.array([[2, 2], [2, 2]])  # Assuming Normalizer\n        assert_allclose_dense_sparse(\n            X_transformed, expected_output, rtol=1e-5, atol=1e-5)\n", "    def test_remainder_as_transformer(self):\n        X = np.array([[1, 2], [3, 4]])\n        remainder_transformer = StandardScaler()\n        ct = ColumnTransformer([('trans1', Trans(), [0]),\n                              ('trans2', Trans(), [1]),\n                              ('remainder', remainder_transformer,\n                               remaining_columns=None)],\n                             remainder='passthrough')\n\n        ct.fit_transform(X)\n\n        # Check that the remainder_transformer was fitted\n        assert remainder_transformer.is_fitted_\n\n        # Check that the transformers were applied effectively\n        transform_X1 = ct.transform(X)\n        assert_allclose_dense_sparse(transform_X1,\n                                     np.array([[1, 2], [3, 4]]).astype(float))\n\n\n", "    def test_remainder_estimator(self):\n        X = np.array([[1, 2], [3, 4]])\n        remainder_trans = Trans()\n        ct = ColumnTransformer(transformers=[('trans', StandardScaler(), [0]),\n                                             ('remainder', remainder_trans, [1])],\n                               remainder='passthrough')\n        X_transformed = ct.fit_transform(X)\n        assert_allclose_dense_sparse(X_transformed[:, 0],\n                                     StandardScaler().fit_transform(X[:, 0].reshape(-1, 1)))\n        assert_array_equal(X_transformed[:, 1], X[:, 1])\n\n        ct = ColumnTransformer(transformers=[('trans', StandardScaler(), [0]),\n                                             ('remainder', remainder_trans, [1])],\n                               remainder=remainder_trans)\n        X_transformed = ct.fit_transform(X)\n        assert_allclose_dense_sparse(X_transformed[:, 0],\n                                     StandardScaler().fit_transform(X[:, 0].reshape(-1, 1)))\n        assert_allclose_dense_sparse(X_transformed[:, 1], remainder_trans.transform(X[:, 1].reshape(-1, 1)))\n\n", "    def test_sparse_output_transformer(self):\n        ct = make_column_transformer(\n            [(SparseMatrixTrans(), [0])], remainder='passthrough'\n        )\n        X = np.arange(10).reshape((5, 2))\n        X_trans = ct.fit_transform(X)\n        \n        assert isinstance(X_trans, sparse.csr_matrix)\n\n\n\n"], "sample_1183": ["    def test_sfield_from_expr():\n        from sympy import symbols, sin\n\n        x = symbols('x')\n        K, f = sfield((x*sin(x) + 4*x**2)*exp(1/x + log(x)/3)/x**2)\n        assert isinstance(K, FracField)\n        assert f == (4*x**2*(exp(1/x)) + x*(exp(1/x))*(log(x)))/(x**(1/3))**2\n        \n        K, f = sfield(x*sin(x) + 4*x**2)\n        assert isinstance(K, FracField)\n        assert f == x*sin(x) + 4*x**2\n\n", "    def test_vfield():\n        x = Symbol(\"x\")\n        K, f = sfield((x*log(x) + 4*x**2)*exp(1/x + log(x)/3)/x**2)\n        assert K.gens == (x,)\n        assert f == (4*x**2*(exp(1/x)) + x*(exp(1/x))*(log(x)))/((x**(1/3))**5)\n        assert K.domain is ZZ\n\n\n\n", "    def test_sfield_symbols():\n        from sympy import symbols, S\n\n        x, y = symbols('x,y')\n        k, = field((x*log(x) + 4*x**2)*exp(1/x + log(x)/3)/x**2)\n        assert k.symbols == (x,)\n        assert k.gens == (x,)\n\n        k2, = field((x*y + 4*x**2)*exp(1/x + log(y)/3)/x**2, symbols=(x, y))\n        assert k2.symbols == (x, y)\n        assert k2.gens == (x,)\n\n        k3, = field((x*y*log(x) + 4*x**2)*exp(1/x + log(y)/3)/x**2, symbols=(x, y))\n        assert k3.symbols == (x, y)\n        assert k3.gens == (x,)\n", "    def test_sfield_with_exprs():\n        from sympy import symbols, sin, cos, oo\n        x, y = symbols('x y')\n        K, f = sfield(((x*sin(x) + 4*x**2)*exp(1/x + log(x)/3)/x**2), x, y)\n        assert K == FracField(symbols('x y'), QQ, lex)\n        assert f == (4*x**2*(exp(1/x)) + x*(exp(1/x))*(sin(x)))/((x**(1/3))**5)\n\n\n\n\n", "    def test_field_from_expr():\n        from sympy import symbols, S\n\n        x, y = symbols('x y')\n        K1, f1 = sfield((x*log(x) + 4*x**2)*exp(1/x + log(x)/3)/x**2)\n\n        assert K1 is not None\n        assert isinstance(f1, FracElement)\n        assert f1.parent() is K1\n\n        K2, f2 = sfield((x*log(x) + 4*x**2)*exp(1/x + log(x)/3)/x**2, symbols=['x'])\n\n        assert K2 is not None\n        assert isinstance(f2, FracElement)\n        assert f2.parent() is K2\n        assert f1 == f2 \n\n        with raises(ValueError):\n            sfield((x*log(x) + 4*x**2)*exp(1/x + log(x)/3)/x**2, symbols=['x', 'y'])\n        \n        with raises(ValueError):\n            sfield((x*log(x) + 4*x**2)*exp(1/x + log(x)/3)/x**2, symbols=['x', 'y'], options={'domain': ZZ})\n\n\n"], "sample_177": ["    def test_get_related_models_recursive(self):\n        with override_settings(\n            INSTALLED_APPS=[\n                'contenttypes',\n                'food',\n            ]\n        ):\n            state = ProjectState()\n            state.add_model(\n                ModelState.from_model(UnicodeModel)\n            )\n            state.add_model(\n                ModelState.from_model(ModelWithCustomBase)\n            )\n            related_models = get_related_models_recursive(UnicodeModel)\n            self.assertEqual(\n                set(related_models),\n                {\n                    ('contenttypes', 'ContentType'),\n                    ('food', 'Food'),\n                    ('food', 'ModelWithCustomBase'),\n                    # Ensure the model itself is present\n                    ('food', 'UnicodeModel'),\n                },\n            )\n", "    def test_related_models_recursive(self):\n        project_state = ProjectState()\n        project_state.add_model(\n            ModelState.from_model(UnicodeModel)\n        )\n        state = project_state.concrete_apps.all_models['test_app']['unicodemodel']\n\n        expected_related = {\n            ('test_app', 'unicodemodel'),\n            ('contenttypes', 'contenttype'),\n            ('contenttypes', 'contenttype'),\n        }\n        related_models = set(get_related_models_recursive(UnicodeModel))\n        self.assertEqual(related_models, expected_related)\n\n", "    def test_get_related_models_recursive(self):\n        with isolate_apps(['food']):\n            class Food(models.Model):\n                name = models.CharField(max_length=100)\n\n            class Pizza(Food):\n                toppings = models.ManyToManyField(Food, related_name='pizza_toppings')\n\n            # Check that pizza is related to Food\n            self.assertTrue(\n                Food in get_related_models_recursive(Pizza)\n            )\n            self.assertTrue(Pizza in get_related_models_recursive(Food))\n\n            # Check that a related model through a related model is also included\n            self.assertTrue(\n                set(get_related_models_recursive(Pizza)) == set([\n                    Food,\n                    Pizza,\n                ])\n            )\n\n\n\n", "    def test_adding_field_to_model(self):\n        with self.assertLogs(level='INFO') as logs:\n            project_state = ProjectState()\n            project_state.add_model(ModelState(\n                app_label='myapp',\n                name='Food',\n                fields=[\n                    ('name', models.CharField(max_length=100)),\n                ],\n                bases=(models.Model,),\n            ))\n\n            operation = AddField(\n                model_name='Food',\n                db_index=True,\n                field=models.IntegerField(name='calories'),\n            )\n            operation.apply(project_state)\n            app_state = project_state.concrete_apps['myapp']\n            food_model = app_state.get_model('myapp', 'Food')\n            self.assertEqual(len(food_model._meta.fields), 2)\n            self.assertEqual(food_model._meta.fields[1].name, 'calories')\n            self.assertTrue(food_model._meta.fields[1].db_index)\n\n            # Ensure the logs show that fields are added\n            self.assertIn(\n                'Adding field \"calories\" to model \"myapp.Food\"',\n                ''.join(logs.output),\n            )\n", "    def test_resolve_related_models_recursive(self):\n        with isolate_apps(['food']):\n            with override_settings(\n                INSTALLED_APPS=[\n                    'django.contrib.contenttypes',\n                    'food',\n                ]\n            ):\n                state = ProjectState()\n                state.add_model(ModelState.from_model(UnicodeModel))\n                unicode_model_state = state.models[(UnicodeModel._meta.app_label, UnicodeModel._meta.model_name)]\n                related_models = get_related_models_recursive(unicode_model_state)\n                self.assertEqual(related_models, set())\n\n                # Now add a model with a relation to UnicodeModel\n                class RelatedModel(models.Model):\n                    name = models.CharField(max_length=100)\n                    food = models.ForeignKey(UnicodeModel, on_delete=models.CASCADE)\n\n                with state:\n                    models.Model.objects.create(name='Related')\n\n                    # Now, after the addition, the related model should appear\n                    state.add_model(ModelState.from_model(RelatedModel))\n                    unicode_model_state = state.models[(UnicodeModel._meta.app_label, UnicodeModel._meta.model_name)]\n                    related_models = get_related_models_recursive(unicode_model_state)\n                    self.assertEqual(related_models, {\n                        (RelatedModel._meta.app_label, RelatedModel._meta.model_name),\n                    })\n\n\n\n                # Ensure that the relation works the other way around\n                related_model_state = state.models[(RelatedModel._meta.app_label, RelatedModel._meta.model_name)]\n                related_models = get_related_models_recursive(related_model_state)\n                self.assertEqual(related_models, {\n                    (UnicodeModel._meta.app_label, UnicodeModel._meta.model_name),\n                })\n\n\n\n"], "sample_881": ["def test_label_ranking_loss_empty(drop):\n    y_true = np.array([], dtype=int)\n    y_score = np.array([], dtype=float)\n    loss = label_ranking_loss(y_true, y_score, drop_last=drop)\n    assert np.isnan(loss)\n", "    def test_top_k_accuracy_score_with_labels(drop):\n        y_true = np.array([0, 1, 2, 2])\n        y_score = np.array([\n            [0.5, 0.2, 0.2],\n            [0.3, 0.4, 0.2],\n            [0.2, 0.4, 0.3],\n            [0.7, 0.2, 0.1]\n        ])\n        labels = ['a', 'b', 'c']\n        if drop:\n            # Check that only the top k labels are considered\n            y_score_dropped = np.array([\n                [0.5, 0.2, 0.2],\n                [0.3, 0.4, 0.2],\n                [0.2, 0.4, 0.3],\n                [0.7, 0.2, 0.1]\n            ])\n            y_score_dropped[:, 2:] = 0\n            assert_almost_equal(\n                top_k_accuracy_score(y_true, y_score_dropped, k=1),\n                top_k_accuracy_score(y_true, y_score, k=1)\n            )\n        # Check that the accuracy score is lower for larger k\n        assert top_k_accuracy_score(y_true, y_score, k=1) > \\\n            top_k_accuracy_score(y_true, y_score, k=2)\n        assert top_k_accuracy_score(y_true, y_score, k=2) == \\\n            top_k_accuracy_score(y_true, y_score, k=3)\n        assert top_k_accuracy_score(y_true, y_score, k=2, normalize=False) == 3\n        assert top_k_accuracy_score(y_true, y_score, k=2, normalize=True) == 0.75\n\n\n\n", "    def test_top_k_accuracy_score_multiclass_labels_missing(drop):\n        # Test cases for multiclass labels missing\n        y_true = np.array([0, 1, 2, 2])\n        y_score = np.array([[0.5, 0.2, 0.2],\n                           [0.3, 0.4, 0.2],\n                           [0.2, 0.4, 0.3],\n                           [0.7, 0.2, 0.1]])\n        with pytest.raises(ValueError):\n            top_k_accuracy_score(y_true, y_score, k=2, labels=None)\n        with pytest.raises(ValueError):\n            top_k_accuracy_score(y_true, y_score, k=2, labels=np.array([1, 2, 3]))\n        # Check if the labels are not included in y_true\n        with pytest.raises(ValueError):\n            top_k_accuracy_score(y_true, y_score, k=2, labels=np.array([1, 2, 3, 4]))\n", "    def test_top_k_accuracy_score(drop, k):\n        y_true, y_pred, _ = make_prediction(binary=True)\n        score = top_k_accuracy_score(y_true, y_pred, k=k, drop=drop)\n        expected_score =  np.sum(np.argsort(y_pred, axis=0)[-k:][:, 0] == y_true) / len(y_true)  \n        assert_almost_equal(score, expected_score)  \n", "    def test_top_k_accuracy_score_binary(k, drop):\n        y_true, y_pred, _ = make_prediction(binary=True)\n\n        score = top_k_accuracy_score(y_true, y_pred, k=k)\n        if drop:\n            # Drop some samples (making the score slightly lower)\n            n_samples = len(y_true)\n            drop_indices = np.random.choice(n_samples, size=int(n_samples * 0.1), replace=False)\n            y_true = np.delete(y_true, drop_indices)\n            y_pred = np.delete(y_pred, drop_indices)\n            score_drop = top_k_accuracy_score(y_true, y_pred, k=k)\n            assert score_drop <= score\n\n        assert 0 <= score <= 1, \"Score should be in [0, 1]\"\n"], "sample_107": ["    def test_callable_setting_wrapper_calls_callable(self):\n            return 'callable_result'\n\n        wrapped_setting = CallableSettingWrapper(callable_setting)\n        self.assertNotEqual(wrapped_setting, callable_setting)\n        self.assertEqual(wrapped_setting(), 'callable_result')\n", "    def test_callable_setting_wrapper(self):\n            return 42\n\n        wrapped = CallableSettingWrapper(callable_setting)\n        self.assertEqual(wrapped(), 42)\n\n\n", "    def test_callable_setting_wrapper_not_called(self):\n        wrapped_callable = CallableSettingWrapper(lambda: 'called')\n        self.assertEqual(wrapped_callable.__repr__(), 'lambda: called')\n", "    def test_repr(self):\n        callable_setting = lambda: \"value\"\n        wrapper = CallableSettingWrapper(callable_setting)\n        self.assertEqual(repr(wrapper), repr(callable_setting))\n\n\n", "    def test_callable_setting_wrapper_call(self):\n        callable_setting = lambda: 'test_value'\n        wrapper = CallableSettingWrapper(callable_setting)\n        self.assertEqual(wrapper(), 'test_value')\n\n\n\n"], "sample_897": ["    def test_pd_plot_average_with_centered(clf_diabetes, grid_resolution):\n        display = PartialDependenceDisplay.from_estimator(\n            clf_diabetes,\n            diabetes.data,\n            diabetes.data,\n            grid_resolution=grid_resolution,\n            variable_indices=[0],\n            feature_names=diabetes.feature_names,\n            kind=\"average\",\n            centered=True,\n        )\n        # Check that the average prediction lines start at 0\n        for i in range(grid_resolution):\n            assert_allclose(display.lines_[0, i, 0].ydata[0], 0)\n", "    def test_partial_dependence_1d_with_categorical(clf_diabetes, grid_resolution):\n        X, y = diabetes.data, diabetes.target\n        pd_display = PartialDependenceDisplay(\n            clf_diabetes,\n            features=[0],\n            grid_resolution=grid_resolution,\n            kind=\"individual\",\n        )\n        pd_display.plot()\n        # this test checks for shape of the returned lines_\n        # and then asserts the type of the first plot\n        assert isinstance(pd_display.lines_[0], matplotlib.lines.Line2D)\n\n", "    def test_plot_partial_dependence_with_categorical_features(\n        grid_resolution, clf_diabetes, diabetes", "    def test_partial_dependence_plot_two_way_categorical(\n        grid_resolution, clf_diabetes", "    def test_partial_dependence_display_two_way_discrete(\n        clf_diabetes, diabetes, grid_resolution"], "sample_285": ["    def test_finders_errors(self):\n        with override_settings(STATICFILES_FINDERS=['nonexistent.path']):\n            with self.assertRaises(ImproperlyConfigured):\n                check_finders(apps=self.apps)\n", "    def test_get_finder_invalid_import_path(self):\n        with self.assertRaises(ImproperlyConfigured):\n            get_finder('does_not_exist')\n", "    def test_finders_check(self):\n        with override_settings(STATICFILES_FINDERS=[\n            'django.contrib.staticfiles.finders.FileSystemFinder',\n            'django.contrib.staticfiles.finders.AppDirectoriesFinder',\n        ]):\n            errors = check_finders(apps=self.apps)\n            self.assertEqual(len(errors), 0)\n\n        with override_settings(STATICFILES_FINDERS=['nonexistent_finder']):\n            errors = check_finders(apps=self.apps)\n            self.assertEqual(len(errors), 1)\n            self.assertIn(\n                'Finder \"nonexistent_finder\" is not a subclass of \"django.contrib.staticfiles.finders.BaseFinder\"',\n                str(errors[0]),\n            )\n", "    def test_finders_check(self):\n        with override_settings(STATICFILES_FINDERS=[\n                'django.contrib.staticfiles.finders.FileSystemFinder',\n                'django.contrib.staticfiles.finders.AppDirectoriesFinder',\n                'django.contrib.staticfiles.finders.DefaultStorageFinder',\n        ]):\n            errors = check_finders(self.apps)\n            self.assertEmpty(errors)\n\n        with override_settings(STATICFILES_FINDERS=['foobar']):\n            errors = check_finders(self.apps)\n            self.assertCountEqual(\n                errors,\n                [\n                    Error(\n                        'Invalid staticfiles finder: \"foobar\" '\n                        'is not a valid finder class.',\n                        id='staticfiles.E005',\n                    ),\n                ],\n            )\n\n\n\n", "    def test_staticfiles_finders_checks(self):\n        with override_settings(STATICFILES_FINDERS=['django.contrib.staticfiles.finders.FileSystemFinder',\n                                                      'django.contrib.staticfiles.finders.AppDirectoriesFinder']):\n            errors = check_finders(TEST_ROOT)\n            self.assertEqual(len(errors), 0)\n\n        # Test missing finder\n        with override_settings(STATICFILES_FINDERS=['nonexistent_finder']):\n            errors = check_finders(TEST_ROOT)\n            self.assertEqual(len(errors), 1)\n            self.assertIn('Finder \"nonexistent_finder\" is not a subclass', str(errors[0]))\n\n        # Test invalid finder\n        with override_settings(STATICFILES_FINDERS=['django.contrib.staticfiles.finders.FileSystemFinder',\n                                                      'something_invalid']):\n            errors = check_finders(TEST_ROOT)\n            self.assertEqual(len(errors), 1)\n            self.assertIn('Finder \"something_invalid\" is not a subclass', str(errors[0]))\n\n\n"], "sample_859": ["    def test_multi_task_elastic_net_cv_with_l1_ratio_list(self):\n        n_samples = 30\n        n_features = 20\n        n_tasks = 3\n        X = np.random.rand(n_samples, n_features)\n        y = np.random.rand(n_samples, n_tasks)\n        # Test with a list of l1_ratio values\n        l1_ratios = np.array([0.1, 0.5, 0.9, 1.0])\n        cv = 5\n        estimator = MultiTaskElasticNetCV(l1_ratio=l1_ratios, cv=cv)\n        with ignore_warnings(category=ConvergenceWarning):\n            estimator.fit(X, y)\n        assert estimator.l1_ratio_ is not None\n        assert estimator.alpha_ is not None\n\n        # Check that the best l1_ratio was chosen\n        best_l1_ratio = estimator.l1_ratio_\n        assert best_l1_ratio in l1_ratios\n        \n        y_pred = estimator.predict(X)\n        assert y_pred.shape == (n_samples, n_tasks)\n\n\n\n", "    def test_lasso_cv_with_l1_ratio(self):\n        X, y = load_boston(return_X_y=True)\n        X = check_array(X, dtype=np.float64)\n        y = check_array(y, dtype=np.float64)\n        \n        # Set a value for l1_ratio to test with\n        true_l1_ratio = 0.9 \n        \n        # Test with a defined l1_ratio\n        lasso_cv = LassoCV(l1_ratio=true_l1_ratio, cv=5)\n        lasso_cv.fit(X, y)\n        assert_almost_equal(lasso_cv.l1_ratio_, true_l1_ratio)\n", "    def test_multi_task_lasso_cv_with_multi_output(self):\n        X, y = make_regression(n_targets=3, n_samples=20, random_state=0)\n        reg = MultiTaskLassoCV(cv=5, random_state=0).fit(X, y)\n        assert reg.coef_.shape == (3, X.shape[1])\n\n        # Check that multi-output is handled correctly\n        y_pred = reg.predict(X)\n        assert y_pred.shape == y.shape\n \n   \n    \n", "    def test_multi_task_elastic_net_cv_l1_ratio_in_cv(self):\n        # Test that l1_ratio can be given as a list in cv\n        X, y = load_boston(return_X_y=True)\n        X = check_array(X, dtype=np.float64, order='F')\n        n_targets = y.shape[1]\n        y = y.reshape(y.shape[0], n_targets)\n        \n        cv = 5  # using 5-fold cross-validation\n        l1_ratios = np.array([0.1, 0.5, 0.9, 1.0])\n        reg = MultiTaskElasticNetCV(l1_ratio=l1_ratios, cv=cv, n_jobs=-1)\n        reg.fit(X, y)\n\n        assert all(\n            np.isclose(\n                reg.l1_ratio_ , l1_ratio\n            ) for l1_ratio in l1_ratios\n        )\n", "    def test_multi_task_elastic_net_cv_sparse(self):\n        X, y = load_boston(return_X_y=True)\n        X = sparse.csc_matrix(X)\n        n_tasks = 2\n        y = np.hstack((y, y + np.random.randn(X.shape[0])))\n        \n        clf = MultiTaskElasticNetCV(cv=5, l1_ratio=0.5)\n        clf.fit(X, y)\n        \n        assert_array_almost_equal(clf.l1_ratio_, 0.5)\n\n        # Check that coefficients are fitted\n        assert_array_equal(clf.coef_.shape, (n_tasks, X.shape[1]))\n        assert_raises(AttributeError, lambda: clf.dual_gap_)\n"], "sample_1171": ["    def test_complex_region_from_real_invalid_input(self):\n        from sympy import Interval\n        with raises(ValueError):\n            ComplexRegion.from_real(Interval(1, 2) * Interval(3, 4) * Interval(5, 6))  \n", "    def test_complex_region_from_real_interval(self):\n        from sympy import Interval, ComplexRegion\n        unit = Interval(0,1)\n        cr = ComplexRegion.from_real(unit)\n        self.assertEqual(cr.a_interval, Interval(0,1))\n        self.assertEqual(cr.b_interval, FiniteSet(0))\n        self.assertTrue(1 + 0*I in cr)\n        self.assertFalse(2 + 0*I in cr)\n", "    def test_complex_region_intersection(self):\n        from sympy.sets.fancysets import ComplexRegion\n        a = Interval(1, 3)\n        b = Interval(4, 6)\n        c = Interval(1, 7)\n        region1 = ComplexRegion(a * b)\n        region2 = ComplexRegion(b * c)\n        intersection = region1.intersect(region2)\n        self.assertEqual(intersection.sets, ProductSet(Interval(4, 3), Interval(4, 6)))\n\n\n", "    def test_complex_region_from_real(self):\n        from sympy import Interval, ComplexRegion\n        self.assertEqual(ComplexRegion.from_real(Interval(0, 1)),\n                         CartesianComplexRegion(ProductSet(Interval(0, 1), FiniteSet(0))))\n\n", "    def test_complex_region_from_real(self):\n        from sympy import Interval, ComplexRegion\n\n        a = Interval(1, 5)\n        b = Interval(4, 8)\n        c = Interval(0, 2*pi)\n        region = ComplexRegion.from_real(a)\n        self.assertEqual(region, CartesianComplexRegion(ProductSet(a, FiniteSet(0))))\n        region = ComplexRegion.from_real(b)\n        self.assertEqual(region, CartesianComplexRegion(ProductSet(b, FiniteSet(0))))\n        region = ComplexRegion.from_real(a*b)\n        self.assertEqual(region, CartesianComplexRegion(ProductSet(a, b)))\n        region = ComplexRegion.from_real(a*c)\n        self.assertEqual(region, PolarComplexRegion(ProductSet(a, c)))\n\n\n"], "sample_1106": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_730": ["    def test_multi_task_lasso_cv_with_multi_output_data(self):\n        X, y = load_boston(return_X_y=True)\n        n_samples, n_features = X.shape\n        n_tasks = 5\n        y_multi_output = np.zeros((n_samples, n_tasks))\n\n        y_multi_output[:, 0] = y\n        y_multi_output[:, 1] = y * 2\n        y_multi_output[:, 2] = y * 0.5\n        y_multi_output[:, 3] = np.random.rand(n_samples)\n        y_multi_output[:, 4] = np.random.rand(n_samples)\n\n        clf = MultiTaskLassoCV()\n        clf.fit(X, y_multi_output)\n\n        # Check that the intercept is not None\n        assert_equal(clf.intercept_.shape[0], n_tasks)\n\n        # Check that the number of coefficients is correct\n        assert_equal(clf.coef_.shape[0], n_tasks)\n        assert_equal(clf.coef_.shape[1], n_features)\n\n        # Check that coef's are different for each task\n        assert_array_almost_equal(clf.coef_[0], clf.coef_[1],\n                                  check_finite=False)\n        assert_array_almost_equal(clf.coef_[1], clf.coef_[2],\n                                  check_finite=False)\n        assert_array_almost_equal(clf.coef_[2], clf.coef_[3],\n                                  check_finite=False)\n        \n        # Check that coef's are different for each task\n        assert_not_equal(clf.coef_[0], clf.coef_[3])\n\n", "    def test_multi_task_lasso_cv_with_l1_ratio_list(self):\n        X, y = load_boston(return_X_y=True)\n        X = sparse.csr_matrix(X)\n        y = y.reshape(-1, 1)\n        n_samples, n_features = X.shape\n\n        multi_task_lasso_cv = MultiTaskLassoCV(l1_ratio=[0.1, 0.5, 0.9, 1.0], cv=3)\n        multi_task_lasso_cv.fit(X, y)\n\n        assert_equal(multi_task_lasso_cv.alpha_, 0.01278878426098763)\n        assert_equal(multi_task_lasso_cv.coef_.shape, (1, n_features))\n        assert_equal(multi_task_lasso_cv.l1_ratio_, 0.9)\n\n\n\n", "    def test_multi_task_lasso_cv_with_l1_ratio_list(self):\n        X, y = load_boston(return_X_y=True)\n        X = check_array(X)\n        y = check_array(y)\n\n        n_tasks = y.shape[1]\n        y_multitask = np.zeros((X.shape[0], n_tasks))\n        for i in range(n_tasks):\n            y_multitask[:, i] = y\n\n        clf = MultiTaskLassoCV(l1_ratio=[0.1, 0.5, 0.9, 1], cv=5)\n        clf.fit(X, y_multitask)\n        assert_equal(clf.l1_ratio_, clf.best_params_['l1_ratio'])\n\n        with pytest.raises(ValueError):\n            MultiTaskLassoCV(l1_ratio=['a', 0.5])\n\n        # Test with a list of alphas\n\n        clf2 = MultiTaskLassoCV(l1_ratio=[0.1, 0.5, 0.9, 1], alphas=np.linspace(0.1, 1, 100), cv=5)\n        clf2.fit(X, y_multitask)\n        assert_equal(clf2.l1_ratio_, clf2.best_params_['l1_ratio'])\n", "    def test_enet_path_with_n_features_zero(self):\n        X = np.random.randn(10, 0)\n        y = np.random.randn(10)\n        coefs = enet_path(X, y, np.logspace(-5, 2, 100),\n                           l1_ratio=0.5, fit_intercept=True)\n        assert np.all(coefs == 0)\n\n\n\n", "    def test_multi_task_elasticnet_cv_l1_ratio_grid(self):\n        X, y = load_boston(return_X_y=True)\n        n_samples, n_features = X.shape\n        y = y.reshape((n_samples, 1))\n        X = check_array(X, dtype=np.float64, order='F')  # Fortran-contiguous\n        n_tasks = y.shape[1]\n\n        # Test with a grid of l1_ratio values\n        l1_ratios = np.array([0.1, 0.5, 0.9, 1.0])\n        clf = MultiTaskElasticNetCV(l1_ratio=l1_ratios, cv=3)\n        with ignore_warnings(category=ConvergenceWarning):\n            clf.fit(X, y)\n\n        assert_equal(clf.l1_ratio_, l1_ratios[np.argmax(clf.mse_path_\n                                                         .min(axis=1))])\n\n\n\n"], "sample_140": ["    def test_sensitive_post_parameters(self):\n        @sensitive_post_parameters('password')\n            return render(request, 'test_sensitive_post_parameters.html')\n\n        view = my_view\n        request = RequestFactory().post({'password': 'secret', 'username': 'john'})\n        response = view(request)\n        self.assertEqual(response.status_code, 200)\n        captured_output = StringIO()\n        with self.settings(LOGGING={'handlers': [{'class': 'logging.StreamHandler', 'stream': captured_output}]}):\n            ExceptionReporter().handle_exception(\n                Exception(\"test\"),\n                request,\n                view,\n                {'key': 'value'},\n            )\n        self.assertIn(\n            'username', captured_output.getvalue(),\n            'Sensitive parameter \"password\" was not masked'\n        )\n        self.assertNotIn(\n            'secret', captured_output.getvalue(),\n            'Sensitive parameter \"password\" was masked'\n        )\n\n    \n", "    def test_callable_setting_wrapper_no_args(self):\n            return 'hello'\n\n        wrapper = CallableSettingWrapper(my_callable)\n        self.assertEqual(wrapper(), 'hello')\n", "    def test_sensitive_post_parameters(self):\n        @sensitive_post_parameters('password')\n            return 'OK'\n\n        view = sensitive_post_parameters(my_view)\n        request = RequestFactory().post({'password': 'secret', 'other': 'data'})\n        response = view(request)\n        self.assertEquals(response.status_code, 200)\n        self.assertEqual(request.sensitive_post_parameters, ['password'])\n\n\n\n", "    def test_sensitive_post_parameters_decorator(self):\n        @sensitive_post_parameters('password')\n            pass\n\n        view = my_view\n        request = RequestFactory().get('/')\n        request.POST = {'password': 'secret'}\n        result = view(request)\n        self.assertEqual(request.sensitive_post_parameters, ('password',))\n\n        @sensitive_post_parameters()\n            pass\n\n        another_view = another_view\n        another_request = RequestFactory().get('/')\n        another_request.POST = {'password': 'secret', 'user': 'john'}\n        result = another_view(another_request)\n        self.assertEqual(another_request.sensitive_post_parameters, ('password', 'user'))\n\n", "    def test_sensitive_variables_decorator(self):\n        @sensitive_variables('user', 'password')\n            return {'user': user, 'password': password}\n\n        decorated_function = my_function()\n\n        result = decorated_function(User(), 'secret')\n        \n        self.assertEqual(result, {'user': 'jacob', 'password': 'secret'})\n\n\n\n        self.assertTrue('user' in decorated_function.sensitive_variables)\n        self.assertTrue('password' in decorated_function.sensitive_variables)\n"], "sample_575": ["    def test_continuous_tick_locations(self, x):\n        a = self.setup_ticks(x, upto=3)\n\n        ticks = a.get_tick_space()\n        assert_array_equal(ticks, np.array([1, 3, 9]))\n", "    def test_continuous_scale_with_log_transform(self, x):\n        a = self.setup_ticks(x, every=0.5, trans=\"log\")\n        expected_ticks = np.array([0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0])\n        assert_array_equal(a.majorticklocs(), expected_ticks)\n\n", "    def test_continuous_tick_with_count(self, x):\n        a = self.setup_ticks(x, count=4)\n        ticks = a.get_tick_space()\n        assert_array_equal(ticks, np.array([0.25, 0.5, 0.75, 1]))\n", "    def test_continuous_scale_with_log_ticks(self, x):\n        scale = Continuous(trans=\"log\")\n        a = self.setup_ticks(x, upto=4)\n\n        assert a.majorticklocs() == [0.82, 2.08, 3.81]\n\n", "    def test_continuous_tick_locator_with_every(self, x):\n        a = self.setup_ticks(x, every=0.5)\n        assert_array_equal(a.majorticklocs(), [0.0, 0.5, 1.0])\n\n"], "sample_333": ["    def test_get_initial_for_field(self):\n        class MyForm(Form):\n            name = CharField()\n\n        form = MyForm(initial={'name': 'initial_value'})\n        self.assertEqual(form.get_initial_for_field(form.fields['name'], 'name'), 'initial_value')\n\n        class CallableInitialForm(Form):\n            name = CharField()\n\n        class CallableInitial(Form):\n            initial_field = CharField(initial=lambda: 'callable_value')\n\n        form = CallableInitial()\n        self.assertEqual(form.get_initial_for_field(form.fields['initial_field'], 'initial_field'), 'callable_value')\n\n\n\n", "    def test_hidden_fields(self):\n        form = Person(initial={'first_name': 'Alice', 'last_name': 'Johnson'})\n        hidden_fields = form.hidden_fields()\n        self.assertEqual(len(hidden_fields), 0)\n\n        form = PersonNew(initial={'first_name': 'Bob', 'last_name': 'Smith'})\n        hidden_fields = form.hidden_fields()\n        self.assertEqual(len(hidden_fields), 0)\n\n        form = PersonNew({\n            'first_name': 'Charlie',\n            'last_name': 'Jones',\n            'birthday': '2023-12-25'\n        })\n        hidden_fields = form.hidden_fields()\n        self.assertEqual(len(hidden_fields), 0)\n\n        \n", "    def test_order_fields(self):\n        order = ['last_name', 'first_name', 'birthday']\n        form_class = type('FormWithOrder', (Form,), {'fields': {'first_name': CharField(), 'last_name': CharField(), 'birthday': DateField()}})\n        form = form_class(field_order=order)\n        self.assertEqual(list(form.fields), order)\n\n\n", "    def test_form_field_order(self):\n        # Test the order_fields() method\n        class OrderedForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n            field3 = CharField()\n\n        form = OrderedForm()\n        form.order_fields(['field2', 'field3', 'field1'])\n        self.assertEqual(list(form.fields), ['field2', 'field3', 'field1'])\n\n        # Test that unknown fields are ignored when ordering\n        with self.assertRaises(KeyError):\n            OrderedForm().order_fields(['field4', 'field2'])\n\n\n", "    def test_order_fields(self):\n        class OrderForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n            field3 = CharField()\n\n        form = OrderForm()\n        self.assertEqual(list(form.fields), ['field1', 'field2', 'field3'])\n\n        form = OrderForm(field_order=['field3', 'field1', 'field2'])\n        self.assertEqual(list(form.fields), ['field3', 'field1', 'field2'])\n\n        form = OrderForm(field_order=['field1', 'field3', 'nonexistent'])\n        self.assertEqual(list(form.fields), ['field1', 'field3', 'field2'])  \n"], "sample_671": ["    def test_evalxfail_key(self):\n        item = Mock(spec=runtestprotocol.TestFunction)\n        item._store = {}\n        evalxfail = MarkEvaluator(item, \"xfail\")\n        item._store[TestEvaluator.evalxfail_key] = evalxfail\n        assert item._store[TestEvaluator.evalxfail_key] is evalxfail\n\n\n", "    def test_xfail_strict(self):\n        item = runtestprotocol.Test(\n            name=\"test_function\",\n            parent=None,\n            config=None,\n            fspath=None,\n        )\n        item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\", {\"strict\": True})\n        pytest_runtest_setup(item)\n        assert item._store[unexpectedsuccess_key] is None\n", "    def test_xfail_strict_with_explicit_strict(self, item):\n        item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\", {\"strict\": True})\n        check_strict_xfail(item)\n        assert item._store.get(evalxfail_key) is None \n        assert item.config.getini(\"xfail_strict\") == False\n\n\n\n", "    def test_xfail_strict_with_no_strict(self):\n        item = runtestprotocol(_pytest.skipping)\n        item._store[MarkEvaluator.key] = MarkEvaluator(item, \"xfail\", strict=False)\n        pytest_runtest_setup(item)\n        assert item._store[skipped_by_mark_key] is False\n        assert item._store[evalxfail_key].istrue()\n", "    def test_xfail_strict_with_explanation(self):\n        item = runtestprotocol.TestItem()\n        item._store[MarkEvaluator.key] = MarkEvaluator(item, \"xfail\", condition=True, strict=True, reason=\"test should fail\")\n        pytest_runtest_setup(item)\n        expected_report = \"failed\"\n        assert item._store[StoreKey[str]()] == \"pytest should fail\"\n        assert item.config._cleanup[0] == lambda: setattr(pytest, \"xfail\", old)\n\n\n"], "sample_379": ["    def test_mark_safe_on_callable(self):\n            return f'{arg1} {arg2}'\n\n        @mark_safe\n            return my_func(arg1, arg2)\n\n        self.assertRenderEqual(\n            '{{ wrapped_func(\"safe\", \"text\") }}', 'safe text',\n        )\n\n        # Test calling the wrapped function.\n        self.assertRenderEqual(\n            '{{ wrapped_func(\"safe\", \"text\") }}', 'safe text',\n        )\n", "    def test_mark_safe_decorator(self):\n        class MyView:\n                self.my_string = \"This is a string\"\n\n                return mark_safe(self.my_string)\n\n        view = MyView()\n        self.assertRenderEqual(\n            \"{{ view.my_method }}\", \"This is a string\", view=view\n        )\n        \n", "    def test_mark_safe_decorator(self):\n        class MyView:\n                self.value = customescape(\"This is a test string with << and >>\")\n\n                return self.value\n\n        view = MyView()\n        @mark_safe\n            return view.some_method()\n        self.assertEqual(safe_method(view), \"This is a test string with << and >>\")  \n\n\n\n", "    def test_mark_safe_callable(self):\n        @mark_safe\n            return \"This string is safe!\"\n\n        self.assertIsInstance(my_function(), SafeString)\n        self.assertEqual(my_function(), \"This string is safe!\")\n", "    def test_mark_safe_on_callable(self):\n            return arg * 2\n\n        @mark_safe\n            return arg * 2\n\n        self.assertEqual(mark_safe(my_view('test')), SafeString('testtest'))\n        self.assertEqual(my_view_safe('test'), SafeString('testtest'))\n\n        # Ensure that calling a function wrapped by mark_safe doesn't affect \n        # its return value.\n        self.assertEqual(my_view('test'), 'testtest')\n"], "sample_758": ["    def test_check_consistent_length_multi_output_y(self):\n        X = np.array([[1, 2], [3, 4]])\n        y = np.array([[5, 6], [7, 8]])  \n\n        with pytest.raises(ValueError) as e:\n            check_consistent_length(X, y)\n        assert \"Length of y\" in str(e.value)\n\n\n", "    def test_check_sparse_format():\n        X = sp.csc_matrix([[1, 2], [3, 4]])\n        assert 'csc' in check_array(X, accept_sparse='csc').format\n\n        X = sp.csr_matrix([[1, 2], [3, 4]])\n        assert 'csr' in check_array(X, accept_sparse='csr').format\n\n        with pytest.raises(ValueError):\n            check_array(X, accept_sparse='dense')\n\n        with pytest.raises(ValueError):\n            check_array(X, accept_sparse=['invalid_format'])\n\n        # test large sparse support\n        if LARGE_SPARSE_SUPPORTED:\n            X = sp.csr_matrix(np.random.rand(10000, 10000), dtype=np.int32)\n            check_array(X, accept_large_sparse=True)\n            X = sp.csc_matrix(np.random.rand(10000, 10000), dtype=np.int32)\n            check_array(X, accept_large_sparse=True)\n            X = sp.coo_matrix(np.random.rand(10000, 10000), dtype=np.int32)\n            check_array(X, accept_large_sparse=True)\n            X = sp.bsr_matrix(np.random.rand(10000, 10000), dtype=np.int32)\n            check_array(X, accept_large_sparse=True)\n\n            with pytest.raises(ValueError):\n                check_array(X, accept_large_sparse=False)\n\n        else:\n            X = sp.csr_matrix(np.random.rand(10000, 10000), dtype=np.int32)\n            with pytest.raises(ValueError):\n                check_array(X, accept_large_sparse=True)\n\n\n", "    def test_check_non_negative_sparse(self):\n        X = sp.csr_matrix([[1, 2], [3, 4]])\n        check_non_negative(X, whom=\"check_non_negative\")\n\n        X = sp.csr_matrix([[-1, 2], [3, 4]])\n        with pytest.raises(ValueError, match=\"Negative values in data passed to check_non_negative\"):\n            check_non_negative(X, whom=\"check_non_negative\") \n", "    def test_check_X_y_dtype_object(dtype):\n        X = np.array([[1, 2], [3, 4]], dtype=object)\n        y = np.array([1, 2], dtype=object)\n\n        X_converted, y_converted = check_X_y(X, y, dtype=dtype)\n\n        if dtype == object:\n            assert X_converted.dtype == object\n            assert y_converted.dtype == object\n        else:\n            assert X_converted.dtype == np.float64\n            assert y_converted.dtype == np.float64\n\n\n\n", "    def test_check_symmetric_raise_exception(self):\n        test_array = np.array([[1, 2], [2, 3]])\n        test_array[0, 1] += 1\n        check_symmetric(test_array, raise_exception=True)\n"], "sample_584": ["def test_combine_nested_simple(concat_dim):\n    datasets = [\n        create_test_data('x', 'y', 'temp', dims=('x', 'y')) * i\n        for i in range(2)\n    ]\n    combined = combine_nested(datasets, concat_dim=concat_dim)\n    exp_combined = Dataset({\n        'temp': (('x', 'y'), np.arange(8).reshape((2, 4))),\n    }, dims=('x', 'y'))\n    assert_identical(combined, exp_combined)\n\n\n", "compilation error", "compilation error", "    def test_complex_tile_ids(self):\n        ds1 = create_test_data(coords={'x': [0, 1, 2], 'y': [0, 1]})\n        ds2 = create_test_data(coords={'x': [0, 1, 2], 'y': [2, 3]})\n        ds3 = create_test_data(coords={'x': [3, 4, 5], 'y': [0, 1]})\n\n        datasets = [ds1, ds2, ds3]\n        combined_ids = _infer_concat_order_from_positions(datasets)\n        expected_tile_ids = OrderedDict([\n            ((0, 0), ds1),\n            ((0, 1), ds2),\n            ((1, 0), ds3),\n        ])\n        assert_combined_tile_ids_equal(combined_ids, expected_tile_ids)\n\n\n\n", "def test_nested_combine_with_concat_dim_as_string():\n    test_data = create_test_data()\n    datasets = [\n        Dataset(\n            {\n                'temp': DataArray(np.random.rand(3, 3), dims=('x', 'y')),\n                'pressure': DataArray(np.random.rand(3, 3), dims=('x', 'y'))\n            }\n        )\n        for _ in range(3)\n    ]\n    combined = combine_nested(datasets, concat_dim=[\n        'x', 'y'\n    ])\n    assert_equal(combined.dims, ('x', 'y'))\n    assert_equal(len(combined.coords), 0)\n\n\n\n"], "sample_1028": ["compilation error", "    def test_mod_with_mul():\n        p = Mul(2*x, 3*y)\n        q = 5\n        result = Mod(p, q)\n        expected = Mod(Mul(2*x, 3*y), 5)\n        assert result == expected\n\n", "compilation error", "compilation error", "    def test_mod_mul_by_non_rational(self):\n        p = 0.6*x\n        q = 0.3*y\n        m = Mod(p, q)\n        self.assertEqual(m, Mod(2*x, y))\n"], "sample_605": ["def test_groupby_apply_with_kwargs(dataset):\n        ds[\"foo\"] = ds[\"foo\"] * multiplier\n        return ds\n\n    result = dataset.groupby(\"x\").apply(apply_func, multiplier=2)\n\n    expected = dataset.copy()\n    expected[\"foo\"] *= 2\n    assert_identical(result, expected)\n", "    def test_groupby_reduce_empty_groups(dataset):\n        ds = dataset.groupby(\"x\").reduce(\n            np.sum, dim=\"y\", keep_attrs=False\n        )\n        assert ds.dims == (\"x\", \"z\")\n        assert ds.coords[\"x\"] == dataset.coords[\"x\"]\n", "    def test_fillna(array, dataset):\n        new_array = array.groupby(\"x\").fillna(value=0)\n        assert_allclose(new_array.sel(x=\"a\").values, np.where(array.sel(x=\"a\").values != np.nan, array.sel(x=\"a\").values, 0))\n        assert_allclose(\n            new_array.sel(x=\"b\").values, np.where(array.sel(x=\"b\").values != np.nan, array.sel(x=\"b\").values, 0)\n        )\n        assert_allclose(\n            new_array.sel(x=\"c\").values, np.where(array.sel(x=\"c\").values != np.nan, array.sel(x=\"c\").values, 0)\n        )\n", "    def test_reduce_empty_groups(array, dataset):\n        grouped = array.groupby(\"x\")\n        result = grouped.reduce(np.mean)\n        assert_allclose(result.values, np.array([\n            np.mean(array[array.index.get_level_values(\"x\") == \"a\"]),\n            np.mean(array[array.index.get_level_values(\"x\") == \"b\"]),\n            np.mean(array[array.index.get_level_values(\"x\") == \"c\"]),\n        ]))\n\n        grouped = dataset.groupby(\"x\")\n        result = grouped.reduce(lambda ds: ds.sum())\n        assert_allclose(result[\"foo\"].values, np.array([\n            np.sum(dataset[\"foo\"][dataset.index.get_level_values(\"x\") == \"a\"]),\n            np.sum(dataset[\"foo\"][dataset.index.get_level_values(\"x\") == \"b\"]),\n            np.sum(dataset[\"foo\"][dataset.index.get_level_values(\"x\") == \"c\"]),\n        ]))\n", "def test_groupby_concat_dims(dataset):\n        grouped = ds.groupby(\"z\")\n        result = grouped.agg({\"foo\": lambda x: x.sum()})\n        assert result.dims == (\"x\", \"y\")\n        assert result.shape == expected_shape\n\n    _check_concat(dataset, dim=\"z\", expected_shape=(3, 4))\n    _check_concat(dataset, dim=\"x\", expected_shape=(6, 4))\n    _check_concat(dataset, dim=\"y\", expected_shape=(3, 2))\n"], "sample_1055": ["    def test_bg_private_key_errors():\n        with raises(ValueError):\n            bg_private_key(2, 2)\n        with raises(ValueError):\n            bg_private_key(2, 3)\n\n\n\n", "    def test_bg_private_key_errors():\n        with raises(ValueError):\n            bg_private_key(2, 2)\n\n        with raises(ValueError):\n            bg_private_key(2, 3)\n        with raises(ValueError):\n            bg_private_key(3, 4)\n", "    def test_morse_encode_special_chars():\n        with warns_deprecated_sympy('The `morse_encode` function is deprecated'):\n            for char in \"!@#$%^&*()_+=-`~[]{};':\\\"\\\\|,.<>/?\":\n                assert type(encode_morse(char)) == str\n\n\n", "    def test_bg_private_key_fail():\n        with raises(ValueError):\n            bg_private_key(2, 2)\n", "    def test_bg_private_key():\n        for p, q in [(23, 29), (31, 37), (41, 43)]:\n            with raises(ValueError):\n                bg_private_key(p, p)\n            with raises(ValueError):\n                bg_private_key(p, q)\n            bg_private_key(p, q)\n"], "sample_106": ["    def test_patch_vary_headers(self):\n        response = HttpResponse()\n        patch_vary_headers(response, ['User-Agent', 'Accept-Language'])\n        self.assertEqual(response['Vary'], 'User-Agent,Accept-Language')\n\n        patch_vary_headers(response, ['*'])\n        self.assertEqual(response['Vary'], '*')\n\n        patch_vary_headers(response, [])\n        self.assertFalse('Vary' in response)\n\n\n", "compilation error", "    def test_vary_headers(self):\n        response = HttpResponse(\"\")\n        patch_vary_headers(response, [\"Accept-Language\"])\n        self.assertEqual(response['Vary'], \"Accept-Language\")\n\n        patch_vary_headers(response, [\"User-Agent\", \"Accept-Language\"])\n        self.assertEqual(response['Vary'], \"User-Agent, Accept-Language\")\n\n        patch_vary_headers(response, [\"*\"])\n        self.assertEqual(response['Vary'], \"*\")\n\n        patch_vary_headers(response, [])\n        self.assertNotIn('Vary', response)\n\n\n\n", "    def test_get_cache_key(self):\n        # Test get_cache_key function\n        request_factory = RequestFactory()\n        request = request_factory.get('/some/url?key1=value1&key2=value2')\n        cache = caches['default']\n        key_prefix = 'testing'\n        cache.set(_generate_cache_header_key(key_prefix, request), [], settings.CACHE_MIDDLEWARE_SECONDS)\n\n        key = get_cache_key(request, key_prefix, 'GET', cache)\n        self.assertEqual(key, _generate_cache_key(request, 'GET', [], key_prefix))\n\n        # Check that the generated key is not None if no headerlist exists\n        cache.delete(_generate_cache_header_key(key_prefix, request))\n        key = get_cache_key(request, key_prefix, 'GET', cache)\n        self.assertIsNone(key)\n\n\n            return ['Accept-Language']\n        \n        with mock.patch.object(management, 'get_vary_header', _mock_get_vary_header):\n            key = get_cache_key(request, key_prefix, 'GET', cache)\n            self.assertEqual(key, _generate_cache_key(request, 'GET', ['HTTP_ACCEPT_LANGUAGE'], key_prefix))\n\n\n\n", "    def test_get_cache_key(self):\n        request = HttpRequest()\n        request.build_absolute_uri = mock.Mock(return_value='https://example.com/')\n        with self.subTest('without_cache_key'):\n            cache_key = get_cache_key(request, method='GET')\n            self.assertIsNone(cache_key)\n        with self.subTest('with_cache_key'):\n            caches['default'].apply_kwargs_from_setting = mock.Mock(\n                return_value={'key_prefix': 'testing'}\n            )\n            caches['default'].set = mock.Mock()\n            cache_key = get_cache_key(request, key_prefix='testing', method='GET')\n            caches['default'].set.assert_called_once_with(\n                _generate_cache_header_key('testing', request),\n                [],\n                settings.CACHE_MIDDLEWARE_SECONDS\n            )\n            self.assertIsNotNone(cache_key)\n"], "sample_344": ["    def test_state_manager_serialization(self):\n        with isolate_apps(['food']):\n            # Ensure managers are serialized correctly\n            project_state = ProjectState()\n            project_state.register_model('food', ModelWithCustomBase)\n\n            # Get the model state from the registry\n            model_state = project_state.models[('food', 'ModelWithCustomBase')]\n\n            # Render the model\n            model = model_state.render(project_state.apps)\n\n            # Check managers\n            self.assertEqual(model._meta.managers, [\n                ('objects', NoMigrationFoodManager),\n                ('food_manager', FoodManager),\n            ])\n\n            # Deserializing and then rerendering should result in the same managers\n            updated_state = ProjectState()\n            updated_state.register_model('food', model)\n            model_state = updated_state.models[('food', 'ModelWithCustomBase')]\n            updated_model = model_state.render(updated_state.apps)\n            self.assertEqual(updated_model._meta.managers, [\n                ('objects', NoMigrationFoodManager),\n                ('food_manager', FoodManager),\n            ])\n\n\n", "    def test_state_app_label_handling(self):\n        with isolate_apps(['test_app']):\n            model = ModelWithCustomBase()\n            proj_state = ProjectState({})\n            proj_state.register_model('test_app', model)\n            state = proj_state.models['test_app', 'ModelWithCustomBase']\n            self.assertEqual(state.app_label, 'test_app')\n\n            # Verify that the app label is reflected in the rendered model\n            rendered_model = state.render(proj_state.apps)\n            self.assertEqual(rendered_model._meta.app_label, 'test_app')\n", "    def test_render_model_with_custom_base(self):\n        with isolate_apps([\"tests.apps.StateTestsConfig\"]):\n            # Should resolve a model with a custom base\n            project_state = ProjectState()\n            model_state = project_state.models[(\"tests\", \"ModelWithCustomBase\")]\n            model = model_state.render(project_state.apps)\n            self.assertEqual(model._meta.bases, (ModelWithCustomBase, models.Model))\n\n\n", "    def test_state_adds_field_to_model(self):\n        with isolate_apps(['mytests']):\n            # Create the initial state\n            initial_state = ProjectState(\n                models={\n                    'mytests': {\n                        'Food': ModelState(\n                            'mytests', 'Food', [], {'ordering': ['name']}, (models.Model,), [],\n                        ),\n                    }\n                }\n            )\n            # Add a field\n            apps = Apps(initial_state.app_configs)\n            operation = AddField(\n                model_name='Food',\n                name='spice',\n                field=models.CharField(max_length=100, blank=True),\n            )\n            operation.apply(apps, initial_state)\n\n            # Check the result\n            final_state = initial_state.models['mytests']['Food']\n            self.assertEqual(len(final_state.fields), 1)\n            self.assertEqual(final_state.get_field('spice').max_length, 100)\n            self.assertIsNotNone(final_state.get_field('spice').blank)\n", "    def test_field_is_relation(self):\n        # Test that field is_relation function of ModelState works correctly\n        state = ModelState(\n            'myapp',\n            'MyModel',\n            [\n                ('id', models.AutoField()),\n                ('content', models.TextField()),\n                ('related', models.ForeignKey('MyOtherModel', on_delete=models.CASCADE)),\n            ],\n        )\n\n        self.assertTrue(state.get_field('related').is_relation)\n        self.assertFalse(state.get_field('content').is_relation)\n"], "sample_656": ["    def test_is_capturing(self, capture):\n        capture.start_capturing()\n        assert capture.is_capturing()\n        capture.stop_capturing()\n\n", "    def test_setup_teardown(self):\n        capfd = StdCapture()\n        with capfd:\n            print(\"Hello\")\n            pytest.raises(Exception, lambda: 1 / 0)\n        out, err = capfd.readouterr()\n        assert \"Hello\" in out\n        assert \"Traceback (most recent call last)\" in err\n", "    def test_deactivate_fixture(self, monkeypatch):\n        manager = CaptureManager(None)\n        with monkeypatch.context() as m:\n            m.setattr(sys, \"stdout\", io.StringIO())\n            m.setattr(sys, \"stderr\", io.StringIO())\n            manager.activate_fixture(test_item)\n            manager.deactivate_fixture(test_item)\n            assert not manager._global_capturing\n            assert not hasattr(test_item, \"_capture_fixture\")\n", "    def test_fixture_ordering(self):\n        if six.PY2:\n            pytest.skip(\"ordering issues with capture fixtures are not reproduceable on py2\")\n        with pytest.raises(AssertionError) as excinfo:\n            with capture.capfd() as capfd:\n                with capture.capfdbinary() as capfdbinary:\n                    pass\n        assert \"cannot use {} and {} at the same time\" in str(excinfo.value)\n\n", "    def test_global_and_fixture_disabled(self, monkeypatch):\n        manager = CaptureManager(method=\"no\")\n        with manager.global_and_fixture_disabled():\n            assert manager.is_capturing() is None\n            with StdCapture() as cap:\n                print(\"hello world\")\n            assert cap.readouterr().out == \"\"\n"], "sample_398": ["    def test_logout_then_login(self):\n        self.login()\n        response = self.client.get(\"/auth/logout_then_login/\")\n        self.assertRedirects(response, reverse(\"login\"))\n\n\n\n", "    def test_password_reset_view(self):\n        # Test PasswordResetView\n        response = self.client.post(reverse(\"password_reset\"), {\"email\": self.u1.email})\n        self.assertRedirects(response, reverse(\"password_reset_done\"))\n        self.assertEqual(len(mail.outbox), 1)\n        mail_message = mail.outbox[0]\n        self.assertEqual(mail_message.to, [self.u1.email])\n        self.assertEqual(mail_message.subject, \"Password Reset\")\n        self.assertIn(\n            f\"http://testserver{reverse('password_reset_confirm', kwargs={'uidb64': urlsafe_base64_encode(self.u1.pk).decode(), 'token': 'test'})}\",\n            mail_message.body,\n        )\n\n        # Test password_reset_done view\n        response = self.client.get(reverse(\"password_reset_done\"))\n        self.assertEqual(response.status_code, 200)\n\n        # Test password_reset_confirm view\n        client = PasswordResetConfirmClient(self, self.u1)\n        response = client.post(\n            reverse(\n                \"password_reset_confirm\",\n                kwargs={\n                    \"uidb64\": urlsafe_base64_encode(self.u1.pk).decode(),\n                    \"token\": \"test\",\n                },\n            ),\n            {\"new_password\": \"newpassword\"},\n        )\n        self.assertRedirects(response, reverse(\"password_reset_complete\"))\n\n        # Test password_reset_complete view\n        response = self.client.get(reverse(\"password_reset_complete\"))\n        self.assertEqual(response.status_code, 200)\n", "    def test_redirect_to_login_with_next_param(self):\n        resolved_url = reverse_lazy(\"login\")\n        next_url = \"/future/page/\"\n\n        response = redirect_to_login(next_url, login_url=resolved_url)\n        self.assertRedirects(response, f\"{resolved_url}?{REDIRECT_FIELD_NAME}={quote(next_url)}\")\n\n\n", "    def test_login_view_with_redirect_field_name(self):\n        response = self.client.get(\"/login/?next=https://example.com\")\n        self.assertContains(response, \"next=https://example.com\")\n", "    def test_logout_then_login(self):\n        login_url = reverse(\"login\")\n        self.client.post(login_url, {\"username\": \"testclient\", \"password\": \"password\"})\n\n        response = self.client.get(reverse(\"logout_then_login\"))\n\n        self.assertIn(SESSION_KEY, self.client.session)\n        self.assertRedirects(response, login_url, \n                             status_code=302)\n\n"], "sample_968": ["    def test_parse_annotation_simple(self):\n        annotation = \"str\"\n        parsed_annotation = _parse_annotation(annotation, None)\n        assert parsed_annotation == [nodes.literal(annotation)]\n", "    def test_parse_annotation_basic(self):\n        self.assertEqual(_parse_annotation('int'), 'int')\n        self.assertEqual(_parse_annotation('str'), 'str')\n        self.assertEqual(_parse_annotation('float'), 'float')\n        self.assertEqual(_parse_annotation('list'), 'list')\n        self.assertEqual(_parse_annotation('tuple'), 'tuple')\n        self.assertEqual(_parse_annotation('None'), 'None')\n", "    def test_parse_annotation(self):\n        self.assertEqual(_parse_annotation(\"Optional[int]\", self.env),\n                         [nodes.emphasis(text=\"Optional\", nodes=[]),\n                          nodes.Text(\" \"),\n                          nodes.Text(\"int\")])\n        self.assertEqual(_parse_annotation(\"List[str]\", self.env),\n                         [nodes.emphasis(text=\"List\", nodes=[]),\n                          nodes.Text(\" \"),\n                          nodes.Text(\"str\")])\n        self.assertEqual(_parse_annotation(\"Dict[str, int]\", self.env),\n                         [nodes.emphasis(text=\"Dict\", nodes=[]),\n                          nodes.Text(\" \"),\n                          nodes.Text(\"str\"),\n                          nodes.Text(\", \"),\n                          nodes.Text(\"int\")])\n        self.assertEqual(_parse_annotation(\"typing.List[int]\", self.env),\n                         [nodes.emphasis(text=\"List\", nodes=[]),\n                          nodes.Text(\" \"),\n                          nodes.Text(\"int\")])\n", "    def test_parse_annotation_simple(self):\n        self.assertEqual(\n            _parse_annotation(\"int\"),\n            [nodes.literal(text='int')]\n        )\n", "    def test_parse_annotation_simple(self):\n        self.assertEqual(_parse_annotation('int'), 'int')\n        self.assertEqual(_parse_annotation('str'), 'str')\n        self.assertEqual(_parse_annotation('float'), 'float')\n        self.assertEqual(_parse_annotation('None'), 'None')\n        self.assertEqual(_parse_annotation('NoneType'), 'NoneType')\n        self.assertEqual(_parse_annotation('complex'), 'complex')\n"], "sample_302": ["    def test_runshell_handles_sigint(self):\n        with mock.patch('django.db.backends.base.client.BaseDatabaseClient.runshell', return_value=None):\n            self.client.runshell([])\n", "    def test_runshell_handles_sigint(self):\n        with mock.patch('django.db.backends.base.client.BaseDatabaseClient.runshell') as mock_runshell:\n            self.client.runshell([])\n            mock_runshell.assert_called_once()\n", "    def test_runshell_handles_sigint(self):\n        with mock.patch('builtins.input', side_effect=['y\\n']):\n            with mock.patch('django.db.backends.base.client.BaseDatabaseClient.runsql') as mock_runsql:\n                self.client.runshell([])\n                mock_runsql.assert_called_once_with('postgres', 'SELECT 1')\n", "    def test_runshell_handles_sigint(self):\n        with mock.patch('django.db.backends.base.client.BaseDatabaseClient.runsql') as mock_runsql:\n            self.client.runshell([])\n            mock_runsql.assert_called()\n\n\n", "    def test_runshell_handles_sigint(self):\n        with mock.patch('django.db.backends.base.client.BaseDatabaseClient.runshell') as mock_runshell:\n            self.client.runshell([''])\n            mock_runshell.assert_called_once_with([''])\n"], "sample_664": ["    def test_deprecation_warnings():\n        with pytest.warns(deprecated.FUNCARGNAMES):\n            pytest.warns(deprecated.RESULT_LOG)\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_7": ["    def test_masked_column_insert(self):\n        # Test insert with a MaskedColumn\n        data = np.array([1, 2, 3, 4], dtype=float)\n        mask = np.array([False, True, False, False])\n        col = table.MaskedColumn(data=data, mask=mask)\n\n        new_data = np.array([5, 6])\n        new_mask = np.array([False, True])\n        new_col = col.insert(1, new_data, mask=new_mask, axis=0)\n\n        expected_data = np.array([5, 1, 2, 3, 4], dtype=float)\n        expected_mask = np.array([True, False, True, False, False], dtype=bool)\n        assert_array_equal(new_col.data, expected_data)\n        assert_array_equal(new_col.mask, expected_mask)\n\n        # Test insert with a masked column and an existing mask\n        data = np.array([1, 2, 3, 4], dtype=float)\n        mask = np.array([False, True, False, False])\n        col = table.MaskedColumn(data=data, mask=mask)\n\n        new_data = np.array([5, 6])\n        new_mask = np.array([True, True])\n        new_col = col.insert(1, new_data, mask=new_mask, axis=0)\n\n        expected_data = np.array([5, 5, 1, 2, 3, 4], dtype=float)\n        expected_mask = np.array([True, True, False, True, False, False], dtype=bool)\n        assert_array_equal(new_col.data, expected_data)\n        assert_array_equal(new_col.mask, expected_mask)\n\n        # Test insert for a single element column\n        data = np.array([1], dtype=float)\n        mask = np.array([False], dtype=bool)\n        col = table.MaskedColumn(data=data, mask=mask)\n\n        new_data = np.array([5])\n        new_mask", "    def test_masked_column_insert(self):\n        data = np.array([1, 2, 3, 4, 5])\n        mask = np.array([False, True, False, False, True])\n\n        col = table.MaskedColumn(data=data, mask=mask)\n\n        new_data = np.array([6, 7, 8])\n        new_mask = np.array([False, False, False])\n        result = col.insert(1, new_data, mask=new_mask)\n\n        expected_data = np.array([1, 6, 7, 8, 2, 3, 4, 5])\n        expected_mask = np.array([False, False, False, False, True, False, False, True])\n\n        assert_array_equal(result.data, expected_data)\n        assert_array_equal(result.mask, expected_mask)\n\n", "    def test_insert_with_mask(self):\n        # Test the insert method when inserting a mask with values.\n        data = np.array([1, 2, 3, 4, 5])\n        mask = np.array([False, True, False, True, False])\n        col = table.MaskedColumn(data=data, mask=mask)\n\n        # Insert a new value at index 2 with a mask\n        new_data = np.array([1, 2, 10, 3, 4, 5])\n        new_mask = np.array([False, True, True, True, False, False])\n        inserted_col = col.insert(2, new_data, mask=new_mask, axis=0)\n\n        assert_array_equal(inserted_col.data, np.array([1, 2, 10, 3, 4, 5]))\n        assert_array_equal(inserted_col.mask, np.array([False, True, True, True, False, False]))\n\n", "    def test_masked_column_insert(self):\n        # Test insert with a MaskedColumn\n        data = np.array([1, 2, 3, 4])\n        mask = np.array([False, True, False, True])\n        col = table.MaskedColumn(\n            data=data, mask=mask, name=\"test_col\", dtype=int\n        )\n        obj = 1\n        values = [5, 6]\n        new_col = col.insert(obj, values)\n        expected_data = np.array([1, 5, 6, 2, 3, 4])\n        expected_mask = np.array([False, True, True, True, False, True])\n        assert_array_equal(new_col.data, expected_data)\n        assert_array_equal(new_col.mask, expected_mask)\n\n        # Test insert with missing mask values, defaults to False\n\n        obj = 1\n        values = [5]\n        new_col = col.insert(obj, values)\n        expected_data = np.array([1, 5, 2, 3, 4])\n        expected_mask = np.array([False, True, False, False, True])\n        assert_array_equal(new_col.data, expected_data)\n        assert_array_equal(new_col.mask, expected_mask)\n", "    def test_masked_column_insert(self):\n        # Test inserting values into a MaskedColumn\n        data = np.array([1, 2, 3, 4], dtype=float)\n        mask = np.array([False, True, False, False], dtype=bool)\n        col = table.MaskedColumn(data=data, mask=mask)\n\n        # Insert a value at the beginning\n        new_col = col.insert(0, [0.5], mask=[True], axis=0)\n        expected_data = np.array([0.5, 1, 2, 3, 4], dtype=float)\n        expected_mask = np.array([True, False, False, False, False], dtype=bool)\n        assert_array_equal(new_col.data, expected_data)\n        assert_array_equal(new_col.mask, expected_mask)\n\n        # Insert a value in the middle\n        new_col = col.insert(2, [2.5], mask=[False], axis=0)\n        expected_data = np.array([1, 2, 2.5, 3, 4], dtype=float)\n        expected_mask = np.array([False, True, False, False, False], dtype=bool)\n        assert_array_equal(new_col.data, expected_data)\n        assert_array_equal(new_col.mask, expected_mask)\n\n        # Insert multiple values\n        new_col = col.insert(1, [1.5, 2.5], mask=[True, False], axis=0)\n        expected_data = np.array([1, 1.5, 2.5, 2, 3, 4], dtype=float)\n        expected_mask = np.array([False, True, False, False, False, False], dtype=bool)\n        assert_array_equal(new_col.data, expected_data)\n        assert_array_equal(new_col.mask, expected_mask)\n\n\n\n"], "sample_80": ["    def test_split_exclude(self):\n        q = Query()\n        q.default_ordering = ('pk',)\n        q.model = Author\n        q.filter_chain.append(\n            (\n                'pk',\n                IS_NOT_NULL()\n            )\n        )\n\n        with self.assertRaises(FieldError):\n            q.split_exclude('DoesNotExist', 'pk', {'pk': 1}, ['name'])\n", "    def test_trim_joins(self):\n        q = Query(Item)\n        q.add_filter(Q(author__name='John'))\n        q.set_extra_mask({'author__name'})\n        trimmed_prefix, contains_louter = q.trim_start(\n            [('author', [PathInfo('author', 'author_id', 'author__name',\n            'author_id', \n            False, \n            False, \n            'author__name', \n            'author', \n            self.get_meta(), \n            0)])]\n        )\n        self.assertEqual(trimmed_prefix, 'author__name')\n        self.assertFalse(contains_louter)\n\n        q = Query(Item)\n        q.add_filter(Q(author__isnull=True))\n        q.set_extra_mask({'author__name'})\n        trimmed_prefix, contains_louter = q.trim_start(\n            [('author', [PathInfo('author', 'author_id', 'author__name',\n            'author_id', \n            False, \n            True, \n            'author__name', \n            'author', \n            self.get_meta(), \n            0)])]\n        )\n        self.assertEqual(trimmed_prefix, 'author__name')\n        self.assertTrue(contains_louter)\n\n\n\n", "    def test_set_group_by(self):\n        # Verify set_group_by functionality\n        q = Ranking.objects.filter(pk=1)\n        q.set_group_by()\n        sql = q.query.build_sql()\n        self.assertIn(\"GROUP BY \\\"ranking_ranking\\\".\\\"id\\\"\", sql)\n\n        # Make sure we can add to fields using set_group_by()\n        q = Ranking.objects.filter(pk=1)\n        q.add_select(\"some_field\")\n        q.set_group_by()\n        sql = q.query.build_sql()\n        self.assertIn(\"GROUP BY \\\"ranking_ranking\\\".\\\"id\\\", \\\"ranking_ranking\\\".\\\"some_field\\\"\", sql)\n\n\n", "    def test_split_exclude_complex(self):\n        q = Query(Ranking)\n        q.add_filter(\n            Q(author__first_name='John') & Q(price__gt=10),\n            AND,\n        )\n        q.add_ordering('id')\n        q.set_limits(5, 10)\n        result = q.split_exclude(\n            field_names=['author__first_name', 'price'],\n            lookup_class=Exact\n        )\n        self.assertEqual(len(result), 2)\n        self.assertEqual(result[0].extra, [])\n        self.assertEqual(result[1], {})\n\n\n\n\n", "    def test_complex_filter_promotion(self):\n        # Complex Filter with Multiple Joins and Conditions\n\n        item = Item.objects.filter(author__name__icontains=\"J\",\n                                  object_c_set__contains=ObjectC.objects.filter(\n                                      name__isnull=False,\n                                      description__startswith='D'\n                                  )\n                                  ).first()\n        \n        # Complex Filter with Multiple Joins and Conditions\n        query = Item.objects.filter(\n            Q(author__name__icontains=\"J\") | Q(object_c_set__size__gt=2),\n            object_c_set__name__isnull=False,\n            object_c_set__description__startswith='D'\n        ) \n        q = Query(Ranking, query.get_meta())\n        \n        query_sql = query.get_sql(using='default')\n        self.assertEqual(len(q.alias_map), 3)\n\n        q.promote_joins(set([\n            'author',\n            'object_c_set',\n        ]))\n\n        \n        updated_sql = query.get_sql(using='default')\n\n        self.assertNotEqual(query_sql, updated_sql)\n\n\n\n"], "sample_397": ["    def test_invalid_template_library(self):\n        with self.assertRaises(InvalidTemplateLibrary):\n            engine = self.engine_class(libraries={\"my_library\": \"foobar\"})\n", "    def test_invalid_template_loader(self):\n        with self.assertRaises(ImproperlyConfigured):\n            Engine(loaders=[\"invalid_loader\"])\n", "    def test_invalid_template_loader(self):\n        with self.assertRaises(ImproperlyConfigured):\n            Engine(loaders=[\"invalid_loader\"])\n", "    def test_no_template_loaders(self):\n        with self.settings(INSTALLED_APPS=[\"django.template\"], TEMPLATES=[\n            {\n                'BACKEND': self.backend_name,\n                'DIRS': [],\n                'APP_DIRS': True,\n                'OPTIONS': {\n                    'debug': False,\n                    'context_processors': []\n                }\n            }\n        ]):\n            engine = self.engine_class(loaders=None)\n            with self.assertRaises(ImproperlyConfigured):\n                engine.find_template(\"sometemplate.html\")\n\n\n\n", "    def test_template_loader_error(self):\n        with self.assertRaises(ImproperlyConfigured):\n            Engine(loaders=[(\"bogus_loader\", [])])\n"], "sample_1148": ["    def test_from_index_summation_trace():\n        expr = Sum(A[i, i], (i, 0, n-1))\n        res = MatrixExpr.from_index_summation(expr)\n        assert res == Trace(A)\n\n", "    def test_from_index_summation_trace():\n        from sympy.abc import i, j\n        expr = Sum(A[i, i], (i, 0, n - 1))\n        result = MatrixExpr.from_index_summation(expr)\n        assert result == trace(A)\n\n\n", "    def test_from_index_summation_trace(self):\n        from sympy import MatrixSymbol, Sum\n        A = MatrixSymbol(\"A\", 3, 3)\n        expr = Sum(A[i, i], (i, 0, 2))\n        result = MatrixExpr.from_index_summation(expr)\n        self.assertEqual(result, Trace(A))\n", "    def test_from_index_summation_trace(self):\n\n        expr = Sum(A[i, i], (i, 0, n-1))\n        result = MatrixExpr.from_index_summation(expr)\n        self.assertIsInstance(result, Trace)\n        self.assertEqual(result.args[0], A)\n", "    def test_from_index_summation_trace(self):\n        from sympy import Trace, sum\n        expr = sum(A[i, i] for i in range(n))\n        result = MatrixExpr.from_index_summation(expr)\n        self.assertEqual(result, Trace(A))\n"], "sample_964": ["    def test_parse_annotation(self):\n        assert _parse_annotation(\"int\", {}) == ('int',)\n        assert _parse_annotation(\"int: str\", {}) == ('int', 'str')\n        assert _parse_annotation(\"List[int]\", {}) == ('List', 'int')\n        assert _parse_annotation(\"Optional[int]\", {}) == ('Optional', 'int')\n        assert _parse_annotation(\"Union[int, str]\", {}) == ('Union', 'int', 'str')\n        assert _parse_annotation(\"typing.List[int]\", {}) == ('typing.List', 'int')\n\n\n\n", "    def test_parse_annotation(self):\n        assert _parse_annotation(\"int\", self.env) == (\"int\", None)\n        assert _parse_annotation(\"int: None\", self.env) == (\"int\", \"None\")\n        assert _parse_annotation(\"int -> str\", self.env) == (\"int\", \"str\")\n        assert _parse_annotation(\"int -> None\", self.env) == (\"int\", \"None\")\n\n        # Ensure that optional annotations are handled correctly\n        assert _parse_annotation(\"int | None\", self.env) == (\"int\", \"None\")\n        assert _parse_annotation(\"int | float\", self.env) == (\"int\", \"float\")\n        assert _parse_annotation(\"int | float | None\", self.env) == (\"int\", \"float | None\")\n\n\n", "    def test_parse_annotation(self):\n        assert _parse_annotation(\"List[int]\") == [(\"List\", \"int\")]\n        assert _parse_annotation(\"Optional[str]\") == [(\"Optional\", \"str\")]\n        assert _parse_annotation(\"Union[int, float]\") == [(\"Union\", \"int\", \"float\")]\n        assert _parse_annotation(\"Dict[str, List[int]]\") == [(\"Dict\", \"str\", \"List[int]\")]\n\n        # Test parsing with generics\n        assert _parse_annotation(\"List[Tuple[str, int]]\") == [(\"List\", \"Tuple[str, int]\")]\n        assert _parse_annotation(\"Dict[str, List[Union[int, float]]]\") == [(\"Dict\", \"str\", \"List[Union[int, float]]\")]\n        \n        # Test empty annotations\n        assert _parse_annotation(\"\") == []\n        assert _parse_annotation(\" \") == []\n\n\n\n\n", "    def test_parse_annotation(self):\n        self.assertEqual(_parse_annotation('int', None),\n                         ([addnodes.desc_annotation(parse_annotation_type='int', value='int',\n                         ', ', None)], ['int']))\n        self.assertEqual(_parse_annotation('int -> str', None),\n                         ([addnodes.desc_annotation(parse_annotation_type='int',\n                         value='int', ', ', None),\n                          addnodes.desc_annotation(parse_annotation_type='',\n                          value='str', None, None)], ['int', '->', 'str']))\n\n", "    def test_parse_sig_no_args(self):\n        sig = \"def foo():\"\n        assert parse(sig) == \"foo()\"\n\n\n"], "sample_1175": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_pretty_print_tensor_products():\n    from sympy.tensor.tensor import TensorIndexType, tensor_indices\n    a, b, c, x, y = symbols('a b c x y')\n    i, j = TensorIndexType('i, j')\n    t1 = TensorHead('T', (i, j))\n    t2 = TensorHead('S', (i, ))\n    t3 = tensorproduct(t1, t2)\n\n    assert pretty(t3) == 'T[i, j] \u2297 S[i]'\n    \n    from sympy.tensor.array import ImmutableDenseNDimArray\n    t1 = ImmutableDenseNDimArray([1, 2], indices=(i, j))\n    t2 = ImmutableDenseNDimArray([3, 4], indices=(i,))\n    t3 = tensorproduct(t1, t2)\n    assert pretty(t3) == '[[1, 2] \u2297 3, [1, 2] \u2297 4]'\n\n\n\n"], "sample_1064": ["    def _compare_tensorflow_trace(variables, expr):\n        f = lambdify(variables, expr, 'tensorflow')\n        random_matrices = [\n            randMatrix(v.rows, v.cols).evalf() for v in variables]\n\n        graph = tf.Graph()\n        r = None\n        with graph.as_default():\n            random_variables = [eval(tensorflow_code(i)) for i in random_matrices]\n            session = tf.compat.v1.Session(graph=graph)\n            r = session.run(f(*random_variables))\n    \n        e = expr.subs({k: v for k, v in zip(variables, random_matrices)}).doit()\n        assert abs(r-e) < 10**-6\n", "compilation error", "    def _compare_tensorflow_derivative(variables, expr):\n        f = lambdify(variables, expr, 'tensorflow')\n        random_matrices = [\n            randMatrix(v.rows, v.cols).evalf() / 100 for v in variables]\n        graph = tf.Graph()\n        r = None\n        with graph.as_default():\n            random_variables = [eval(tensorflow_code(i)) for i in random_matrices]\n            session = tf.compat.v1.Session(graph=graph)\n            r = session.run(f(*random_variables))\n\n        e = expr.doit()\n        assert abs(r-e) < 10**-6 \n", "    def _compare_tensorflow_derivative(\n        variables, expr,):\n        f = lambdify(variables, expr, 'tensorflow')\n        random_matrices = [randMatrix(v.rows, v.cols).evalf() / 100 for v in variables]\n        with tf.compat.v1.Session() as session:\n            tf_rvs = [tf.constant(i) for i in random_matrices]\n            tf_expr = tensorflow_code(expr)\n            tf_deriv = tf.gradients(tf_expr, tf_rvs)\n            r = session.run(tf_deriv, feed_dict={})\n        e = expr.diff(variables[0]).evalf().subs({v: random_matrices[0] for v in variables})\n        assert (r[0] == e).all() \n\n", "    def test_tensorflow_derivative():\n        x = symbols('x')\n        expr = x**2 + 2*x + 1\n        f = tensorflow_code(Derivative(expr, x))\n        _compare_tensorflow_scalar([x], f)"], "sample_219": ["    def test_order_by_with_nulls_modifier(self):\n        with self.assertNumQueries(1):\n            with isolate_apps('my_app'):\n                from my_app.models import SomeModel\n                SomeModel.objects.create(name='b', value=None)\n                SomeModel.objects.create(name='a')\n                results = SomeModel.objects.order_by(\"name\", nulls_first=True).values('name').all()\n                self.assertEqual(results.count(), 2)\n                self.assertEqual([r['name'] for r in results], ['a', 'b'])\n                    \n", "    def test_raw_sql(self):\n        raw_sql = RawSQL('SELECT COALESCE(ceo_id, 0) FROM employees WHERE company_id = %s', [self.example_inc.id])\n        result = raw_sql.resolve_expression(\n            query=self.company_query, allow_joins=True, reuse=None, summarize=False, for_save=False\n        )\n        self.assertEqual(result.output_field, fields.IntegerField())\n        self.assertEqual(result.template, 'SELECT COALESCE(ceo_id, 0) FROM employees WHERE company_id = %s')\n        self.assertEqual(result.params, [self.example_inc.id])\n\n", "    def test_raw_sql_with_output_field(self):\n        class CustomSum(Expression):\n            template = \"%(expressions)s\"\n            output_field = fields.IntegerField()\n\n                super().__init__()\n                self.expression = expression\n\n        expr = RawSQL(\"SELECT sum(num_employees) FROM companies\", [])\n        result_field = CustomSum(expr)\n\n        with self.subTest(\n                f\"Field value = {result_field.output_field.db_type}\"\n        ):\n            self.assertEqual(result_field.output_field.db_type, \"IntegerField\")\n\n        with self.subTest(\"Raw SQL expression\"):\n            result = Company.objects.raw(expr.as_sql(\n                compiler=connection.compiler, connection=connection.connection)[0]).values(\"sum\")[0][\"sum\"]\n            self.assertEqual(result, 2633)\n\n", "    def test_col_expression(self):\n        self.assertCountEqual(\n            list(\n                Col(\"name\").as_sql(\n                    compiler=connection.compiler, connection=connection\n                )\n            ),\n            [\n                ('name', [])\n            ],\n        )\n        column = Col(\"name\")\n        column.output_field = CharField(max_length=100)\n        self.assertIsInstance(column.output_field, CharField)\n", "    def test_count(self):\n        self.assertEqual(\n            Company.objects.count(), 3\n        )\n        self.assertEqual(\n            Company.objects.filter(name=\"Example Inc.\").count(), 1\n        )\n        self.assertEqual(\n            Company.objects.filter(num_employees__gte=10).count(), 2\n        )\n\n"], "sample_506": ["    def test_spine_transform(self, fig_testdata):\n        fig, ax = plt.subplots()\n        ax.spines['bottom'].set_position('axes', 0.5)\n        ax.set_xlim(0, 1)\n        ax.set_ylim(0, 1)\n        ax.plot([0.1, 0.9], [0.5, 0.5])\n        plt.show()\n\n\n", "    def test_spine_bounds(self):\n        fig, ax = plt.subplots()\n        spines = Spines()\n        spines['top'] = spines.linear_spine(ax, 'top', bounds=(0.1, 0.9))\n        spines['right'].set_bounds(0.8, 1.0)\n        spines.top.set_bounds(None, 0.9)\n        ax.set_xlim(0, 1)\n        ax.set_ylim(0, 1)\n        spines.top.set_visible(True)\n        spines.right.set_visible(True)\n        plt.draw()\n", "    def test_circular_spine(self, image_comparison_setup):\n        fig, ax = plt.subplots()\n\n        spine = Spines.circular_spine(ax, center=(0.5, 0.5), radius=0.3)\n        ax.set_xlim(0, 1)\n        ax.set_ylim(0, 1)\n        plt.show()\n\n\n", "    def test_circular_spine_bounds(self, image_comparison):\n        fig, axes = plt.subplots()\n\n        spine = Spines.circular_spine(axes, center=(0.5, 0.5), radius=0.3)\n        \n        # Check if setting bounds on a circular spine raises ValueError\n        with pytest.raises(ValueError):\n            spine.set_bounds(low=0.1, high=0.9)\n\n        image_comparison(fig, 'test_circular_spine_bounds.png')\n", "    def test_spines_proxy_set_linewidth(self, fig_test, capfd):\n        fig, ax = plt.subplots()\n        spines = Spines(top=Spines.linear_spine(ax, 'top', linewidth=1),\n                        bottom=Spines.linear_spine(ax, 'bottom', linewidth=1))\n\n        spines[['top', 'bottom']].set_linewidth(2)\n\n        fig_test(ax)\n        captured = capfd.readouterr()\n        assert 'spines.top.set_linewidth(2)' in captured.out\n        assert 'spines.bottom.set_linewidth(2)' in captured.out\n\n\n\n"], "sample_258": ["    def test_caching_works(self):\n        @receiver(d_signal, dispatch_uid='test_uid')\n            pass\n\n        d_signal.send(sender=None)  \n        \n        # Simulate garbage collection to ensure weak references are cleaned\n        garbage_collect()  \n        \n        self.assertTestIsClean(d_signal) \n", "    def test_connect_disconnect_with_weak_references(self):\n        @receiver(a_signal)\n            return 'Receiver Function'\n\n        a_signal.connect(receiver_fn)\n        self.assertTrue(a_signal.has_listeners())\n        weak_ref = weakref.ref(receiver_fn)\n        del receiver_fn  \n        garbage_collect()  \n        self.assertFalse(weak_ref())\n        self.assertFalse(a_signal.has_listeners())\n        self.assertTestIsClean(a_signal)\n\n", "    def test_signal_disconnect(self):\n        @receiver(a_signal)\n            return \"receiver_1\"\n\n        a_signal.connect(my_receiver, weak=False)\n        self.assertTrue(a_signal.has_listeners())\n        a_signal.disconnect(receiver=my_receiver)\n        self.assertFalse(a_signal.has_listeners())\n        a_signal.send(a_signal)\n", "    def test_connect_disconnect_caching(self):\n        @receiver(d_signal, dispatch_uid='my_uid')\n            return True\n\n        self.assertTrue(d_signal.has_listeners())\n\n        d_signal.disconnect(dispatch_uid='my_uid')\n        self.assertFalse(d_signal.has_listeners())\n        self.assertTestIsClean(d_signal)\n\n        @receiver(d_signal, dispatch_uid='my_uid')\n            return True\n\n        self.assertTrue(d_signal.has_listeners())\n        d_signal.disconnect(dispatch_uid='my_uid')\n        self.assertFalse(d_signal.has_listeners())\n        self.assertTestIsClean(d_signal)\n\n\n\n", "    def test_receiver_decorator_with_dispatch_uid(self):\n        @receiver(b_signal, dispatch_uid=\"unique_id\")\n            return \"Hello from receiver_func with dispatch_uid\"\n\n        b_signal.send(sender=None, **{\"key\": \"value\"})\n        response = b_signal.send(sender=None, **{\"key\": \"value\"})\n        self.assertIn((\"receiver_func\", \"Hello from receiver_func with dispatch_uid\"), response)\n\n        @receiver(b_signal, dispatch_uid=\"unique_id\")\n            return \"Hello from another_receiver_func with dispatch_uid\"\n        \n        response = b_signal.send(sender=None, **{\"key\": \"value\"})\n        self.assertNotIn((\"another_receiver_func\", \"Hello from another_receiver_func with dispatch_uid\"), response)\n\n\n\n"], "sample_1133": ["    def test_hyperfocal_distance_different_units():\n        from sympy.physics.optics import hyperfocal_distance\n        from sympy.physics.units import mm\n\n        f = 50 * mm\n        N = 5.6\n        c = 0.033 * mm\n        \n        H = hyperfocal_distance(f, N, c)\n        \n        assert ae(H.to('m'), 0.056, 4)\n\n\n\n", "compilation error", "    def test_refraction_angle_boundary_conditions():\n        raises(ValueError, lambda: refraction_angle(pi/2, 1, 1))\n        raises(ValueError, lambda: refraction_angle(\n            pi/2, 1, 1, normal=Matrix([0, 0, 1])))\n        raises(ValueError, lambda: refraction_angle(\n            1.2, 1, 1, plane=Plane(Point3D(0,0,0), normal_vector=[0,0,1])))\n", "    def test_refraction_angle_medium():\n        n1 = Medium(refractive_index=1.5)\n        n2 = Medium(refractive_index=1.33)\n        incident = Ray3D(Point3D(-1, -1, 1), Point3D(0, 0, 0))\n\n        with raises(TypeError):\n            refraction_angle(incident, 1)\n\n        with raises(TypeError):\n            refraction_angle(incident, n1)\n\n        with raises(ValueError):\n            refraction_angle(incident, n1, n2, normal=Matrix([0, 0, 1]), plane=Plane(Point3D(0, 0, 0), normal_vector=[1, 0, 0]))\n\n        with raises(ValueError):\n            refraction_angle(1.0, n1, n2)\n\n        with raises(ValueError):\n            refraction_angle(incident, n1, n2, angle_of_incidence=1.5*pi)\n\n        with raises(ValueError):\n            refraction_angle(incident, n1, n2, plane=Plane(Point3D(0, 0, 0), normal_vector=[1, 0, 0]), normal=Matrix([1, 0, 0]))\n\n        with raises(ValueError):\n            refraction_angle(1.0, n1, n2, plane=Plane(Point3D(0, 0, 0), normal_vector=[1, 0, 0]), normal=Matrix([1, 0, 0]))\n\n        with raises(TypeError):\n            refraction_angle(incident, n1, n2, normal=[0, 0, 1])\n        refracted = refraction_angle(incident, n1, n2)\n        assert isinstance(refracted, Ray3D)\n\n\n\n\n", "    def test_hyperfocal_distance():\n        assert ae(hyperfocal_distance(f = 0.5, N = 8, c = 0.0033), 9.47, 2)\n        assert ae(hyperfocal_distance(f = 1, N = 2.8, c = 0.005), 1.26, 2)\n        with raises(ValueError):\n            hyperfocal_distance(f = 0, N = 8, c = 0.0033)\n        with raises(ValueError):\n            hyperfocal_distance(f = 0.5, N = 0, c = 0.0033)\n        with raises(ValueError):\n            hyperfocal_distance(f = 0.5, N = 8, c = 0) \n\n\n\n"], "sample_94": ["    def test_createsuperuser_interactive_with_password(self):\n        with override_settings(DJANGOTEST_USE_CUSTOM_USER=True):\n            call_command('createsuperuser', verbosity=0)\n        user = User.objects.get(username='alice')\n        self.assertEqual(user.password, 'password')\n", "    def test_interactive_creation_with_password(self):\n        with mock.patch('builtins.input', side_effect=[\n            'alice',\n            'password123',\n            'password123',\n            'password123',\n            'password123',\n        ]):\n            call_command('createsuperuser', verbosity=0, interactive=True)\n        self.assertEqual(User.objects.count(), 1)\n        self.assertEqual(User.objects.all()[0].username, 'alice')\n        self.assertEqual(User.objects.all()[0].password, 'password123')\n", "        def test_create_superuser_with_email(self):\n            with mock.patch('builtins.input', side_effect=[\n                'alice',  # Username prompt\n                'alice@example.com',  # Email prompt\n            ]):\n                call_command('createsuperuser')\n\n            user = CustomUser.objects.get(username='alice')\n            self.assertEqual(user.email, 'alice@example.com')\n\n\n", "    def test_createsuperuser_with_interactive_mode_and_password(self):\n        with override_settings(\n            INSTALLED_APPS=[\n                'django.contrib.auth',\n                'django.contrib.contenttypes',\n                'my_app',  # Replace with your app name\n            ],\n            USE_I18N=False,\n        ):\n            old_getpass = createsuperuser.getpass\n            old_input = builtins.input\n            createsuperuser.getpass = lambda p: 'securepassword'\n            builtins.input = lambda p: 'alice'\n            call_command(\n                'createsuperuser',\n                '--database', 'default',\n            )\n            self.assertEqual(User.objects.count(), 1)\n            user = User.objects.first()\n            self.assertEqual(user.username, 'alice')\n            self.assertEqual(user.password, 'securepassword')\n            createsuperuser.getpass = old_getpass\n            builtins.input = old_input\n\n\n\n", "    def test_bypass_validation(self):\n        with mock.patch('builtins.input', lambda prompt: None):\n            with mock.patch('django.contrib.auth.management.create_superuser') as mock_create_superuser:\n                call_command('createsuperuser')\n            mock_create_superuser.assert_called_once_with(\n                username='alice', password='secret',\n            )\n"], "sample_253": ["    def test_autoreload_starts(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            django_main_thread = threading.Thread(\n                target=django.__main__.main,\n                args=['--noreload', '--settings', 'django.core.management.settings'],\n                name='django-main-thread',\n            )\n            django_main_thread.start()\n\n            with self.assertRaises(SystemExit):\n                autoreload.autoreload_started.wait()\n            django_main_thread.join()\n", "    def test_iter_modules_and_files_with_zip_module(self):\n        with tempfile.TemporaryDirectory() as tempdir:\n            zip_filename = os.path.join(tempdir, 'my_module.zip')\n            with zipfile.ZipFile(zip_filename, 'w') as zf:\n                zf.writestr('__init__.py', '')\n                zf.writestr('my_module.py', 'print(\"Hello from zip module!\")')\n\n            self.assertFileFound(Path(zip_filename))\n\n            # Verify we can import from the zipped module\n            self.import_and_cleanup('my_module')\n\n\n", "    def test_error_files(self):\n        filename = self.temporary_file('test.py')\n\n        # Module was imported before, should be in error_files\n        with open(filename, 'w') as f:\n            f.write('def test(): pass')\n        self.import_and_cleanup(filename.as_posix())\n\n        # Simulate an error during import\n        with open(filename, 'w') as f:\n            f.write('def test(): raise ValueError(\"test\")')\n        with self.assertRaises(ValueError):\n            self.import_and_cleanup(filename.as_posix())\n\n        self.assertIn(filename, autoreload._error_files)\n\n", "    def test_iter_all_python_module_files_with_zipfile(self):\n        with tempfile.TemporaryDirectory() as tempdir:\n            zip_path = Path(tempdir) / 'test.zip'\n            with zipfile.ZipFile(zip_path, 'w') as zf:\n                zf.writestr('module1.py', 'print(\"Hello\")')\n            self.import_and_cleanup('zip_test')\n\n            extend_sys_path(tempdir)\n            with zipfile.ZipFile(zip_path, 'r') as zf:\n                zip_module = zf.open('module1.py').read().decode('utf-8')\n                exec(zip_module, {'__builtins__': None, **globals()})\n\n            self.assertFileFound(zip_path / 'module1.py')\n            self.assertFileFound(Path(__file__).parent / 'test_utils.py')\n\n\n\n", "    def test_iter_modules_and_files_with_zip(self):\n        with tempfile.TemporaryDirectory() as tmpdir:\n            # Create a zip file\n            zip_path = Path(tmpdir) / 'my_module.zip'\n            with zipfile.ZipFile(zip_path, 'w') as zipf:\n                zipf.write('__init__.py', arcname='my_module')\n\n            # Add the zip file to sys.path\n            extend_sys_path([str(zip_path)])\n            self.import_and_cleanup('my_module')\n\n            # Check if the zip file and its contents are watched\n            self.assertFileFound(zip_path / '__init__.py')\n\n"], "sample_59": ["    def test_check_constraints(self):\n        with self.subTest('check constraint succeeds'):\n            self.assertEqual(\n                Party.objects.create(name='Party A', check_constraint_value=1),\n                Party.objects.get(name='Party A')\n            )\n\n        with self.subTest('check constraint fails'):\n            with self.assertRaises(ValidationError):\n                Party.objects.create(name='Party B', check_constraint_value=0)\n", "def test_ordering_with_lookup(self):\n    # Test ordering with lookups like \"startswith\".\n    Event.objects.create(title='Concert', description='Music show', date=datetime.date(2023, 10, 26))\n    Event.objects.create(title='Conference', description='Tech conference', date=datetime.date(2023, 10, 27))\n\n    events = Event.objects.order_by('title__startswith').all()\n    self.assertEqual([e.title for e in events], ['Concert', 'Conference'])\n\n    # Test ordering with multiple lookups.\n    events = Event.objects.order_by('title__startswith', 'date').all()\n    self.assertEqual([e.title for e in events], ['Concert', 'Conference'])\n\n\n", "    def test_constraints_warning(self):\n        with isolate_apps(apps=models.apps):\n            class MyModel(models.Model):\n                foo = models.CharField(max_length=100)\n                bar = models.IntegerField()\n\n                class Meta:\n                    constraints = [\n                        models.CheckConstraint(check=models.Q(foo='bar'), name='unique_foo'),\n                    ]\n\n            # Create the model and check if the warning is raised\n            try:\n                my_model = MyModel._meta\n            except Exception as e:\n                self.fail(f\"Error creating model: {e}\")\n            self.assertEqual(len(my_model._constraints), 1)\n            self.assertTrue(isinstance(my_model._constraints[0], models.CheckConstraint))\n            self.assertEqual(my_model._constraints[0].name, 'unique_foo')\n\n            with self.assertLogs(level='WARNING') as log:\n                MyModel._check_constraints(None)\n            self.assertEqual(len(log.records), 1)\n            self.assertIn(\n                \"does not support check constraints.\",\n                log.records[0].msg,\n            )\n", "    def test_transaction_manager(self):\n        with self.assertNumQueries(0):\n            with transaction.atomic():\n                article = Article.objects.create(title='Test Article', content='Test content')\n                article.title = 'New Title'\n                article.save()\n\n        with self.assertNumQueries(1):\n            self.assertEqual(Article.objects.get(pk=article.pk).title, 'New Title')\n", "    def test_transactions_with_db_routers(self):\n        with isolate_apps(['mysite', 'myapp']):\n            # Test that router works with transactions\n            with connection.atomic():\n                # Test 'mysite' app\n                article = Article.objects.create(title='Article 1', content='Article content')\n                self.assertEqual(article.site, self.site)\n                # Test 'myapp' app\n                party = Party.objects.create(name='Party 1', location='London')\n                self.assertEqual(party.db_router.db_for_read(party), 'myapp_db')\n\n                # Verify both objects are saved\n                self.assertTrue(article.id is not None)\n                self.assertTrue(party.id is not None)\n\n                # Test that data is consistent across databases\n                article.refresh_from_db()\n                party.refresh_from_db()\n                self.assertEqual(article.site, self.site)\n                self.assertEqual(party.db_router.db_for_read(party), 'myapp_db')\n\n"], "sample_621": ["    def test_is_multi_with_multiindex():\n        index = PandasMultiIndex(\n            pd.MultiIndex(\n                levels=[[\"a\", \"b\"], [\"x\", \"y\"]],\n                codes=[[0, 0, 1, 1], [0, 1, 0, 1]],\n            ),\n            dim=\"x\",\n        )\n        indexes = Indexes({\n            \"x\": index,\n            \"y\": PandasIndex(np.array([1, 2, 3, 4]), dim=\"y\"),\n        })\n\n        assert indexes.is_multi(\"x\")\n        assert not indexes.is_multi(\"y\")\n\n\n\n", "    def test_filter_indexes_from_coords(coord_names: set, indexes: Indexes[Index]):\n        filtered_indexes = filter_indexes_from_coords(indexes, coord_names)\n\n        assert len(filtered_indexes) == len(\n            {k for k in indexes if k in coord_names}\n        )\n        for k in filtered_indexes:\n            assert k in coord_names\n\n\n\n", "    def test_indexes_all_equal_same_types(\n        self,", "    def test_roll_indexes_simple(indexes: Indexes[Index], shifts: dict[Hashable, int]):\n        new_indexes, new_index_vars = roll_indexes(indexes, shifts)\n        assert_identical(new_indexes, {\n            \"x\": Index([2, 3, 4, 1], name=\"x\"),\n            \"y\": Index([7, 8, 5, 6], name=\"y\"),\n        })\n        assert_identical(new_index_vars, indexes.variables)\n\n\n", "    def test_create_default_index_implicit(\n        dim_name: str, values: Any, expected_index: pd.Index | pd.MultiIndex"], "sample_449": ["    def test_headers_sent(self):\n        with captured_stderr() as stderr:\n            # Test that header \"Connection\" is sent as \"close\" if content length is unknown.\n            request = self.request_factory.get(\"/\")\n            handler = WSGIRequestHandler(\n                UnclosableBytesIO(),  # Mock output stream\n                request.META,\n                server_version=\"HTTP/1.1\"\n            )\n            handler.close_connection = True\n            handler.environ[\"REQUEST_METHOD\"] = \"POST\"  # Simulate a non-HEAD request\n            handler.environ[\"CONTENT_LENGTH\"] = None  # Set content length to None.\n            handler.cleanup_headers()\n            self.assertIn(\n                \"Connection: close\", handler.headers,\n                msg=\"Connection header is not set to 'close' when content length is unknown.\"\n            )\n            # Further verification:\n            self.assertIn(\n                \"Content-Length: \", handler.headers,\n                msg=\"Content-Length header is missing.\"\n            )\n\n            # Test that \"Connection\" is not sent as \"close\" when content length is known.\n            handler.environ[\"CONTENT_LENGTH\"] = \"10\"\n            handler.cleanup_headers()\n            self.assertNotIn(\n                \"Connection: close\", handler.headers,\n                msg=\"Connection header is set to 'close' when content length is known.\"\n            )\n", "    def test_handle_with_close_connection(self):\n        request = self.request_factory.get(\"/\")\n        with captured_stderr():\n            handler = WSGIRequestHandler(\n                UnclosableBytesIO(), UnclosableBytesIO(), BytesIO(), request\n            )\n            handler.close_connection = True\n            handler.handle_one_request()\n            self.assertEqual(handler.connection.fileno(), -1)  # Closed\n\n\n", "    def test_close_connection_handling(self):\n        with captured_stderr() as stderr:\n            server = Stub(request_queue_size=0)\n            handler = WSGIRequestHandler(server, \"\", b\"\", \"\", {})\n            request = self.request_factory.get(\"/test\")\n            handler.handle_one_request()\n\n            # Check that close_connection is set\n            self.assertTrue(handler.close_connection)\n\n            # Check that the server doesn't try to handle more requests\n            with self.assertRaises(SystemExit):\n                handler.handle_one_request()\n            # Check for any error messages\n            self.assertEqual(stderr.getvalue(), \"\")\n", "    def test_cleanup_headers_for_head_request(self):\n        with captured_stderr() as stderr:\n            request = self.request_factory.get(\"/\")\n            environ = request.META\n            handler = WSGIRequestHandler(\n                UnclosableBytesIO(), BytesIO(), stderr, environ\n            )\n            handler._close_connections = lambda: None  # Mock out connection closure\n\n            # Simulate a HEAD request\n            environ[\"REQUEST_METHOD\"] = \"HEAD\"\n            handler.cleanup_headers()\n\n            # Assertion: \"Content-Length\" should be removed\n            self.assertNotIn(\"Content-Length\", handler.headers)\n\n            # Assertion: \"Connection\" should be \"close\"\n            self.assertEqual(handler.headers.get(\"Connection\"), \"close\")\n\n            # Assert no error messages are logged\n            self.assertEqual(stderr.getvalue(), \"\")\n\n\n\n\n", "    def test_handle_with_close_connection(self):\n        request = self.request_factory.get('/')\n        request._close_connection = True\n\n        handler = WSGIRequestHandler(\n            UnclosableBytesIO(),\n            UnclosableBytesIO(),\n            None,\n            request.environ,\n        )\n\n        handler.handle()\n\n        self.assertTrue(handler.connection.closed)  \n"], "sample_246": ["    def test_basic_extraction(self):\n        self.create_test_files()\n        output, po_contents = self._run_makemessages()\n        self.assertIn('processing file test_template.html in templates', output)\n        self.assertMsgIdPlural('This is a plural form', po_contents,\n                              use_quotes=False)\n        self.assertMsgStr('Another translated string', po_contents, use_quotes=False)\n        self.assertMsgStr('English string', po_contents, use_quotes=False)\n        self.assertNotMsgId('This is not translated', po_contents, use_quotes=False)\n        self.assertLocationCommentPresent(self.PO_FILE, 10, 'templates', 'test_template.html')\n", "    def test_no_translation_files(self):\n        \"\"\"\n        Check that makemessages does nothing when there are no translatable\n        files.\n        \"\"\"\n        self.create_temp_dir()\n        output, _ = self._run_makemessages()\n        self.assertFalse(os.path.exists(self.PO_FILE))\n        self.assertIn('No translatable files', output)\n\n\n", "    def test_basic_extraction(self):\n        # Create a temp file with translatable strings\n        temp_file = self.create_temp_file('templates/base.html', \"\"\"\n            <h1>{{ _('Hello there!') }}</h1>\n        \"\"\")\n        self.copy_to_tmp_dir(temp_file)\n        # Run makemessages to extract strings\n        output, po_contents = self._run_makemessages()\n        # assert extracted strings in .po file\n        self.assertMsgIdPlural(\"Hello there!\", po_contents)\n        self.assertRecentlyModified(self.PO_FILE)\n\n\n\n", "    def test_ignores_files_with_custom_ignore_patterns(self):\n        with self.tmp_dir() as tmpdir:\n            (tmpdir / 'template.html').write('<h1>Title</h1>')\n            (tmpdir / 'locale').mkdir()\n            (tmpdir / 'locale').joinpath('LC_MESSAGES').mkdir()\n            (tmpdir / 'locale/LC_MESSAGES/django.po').write('')\n            (tmpdir / 'test_app/templates/test_template.html').write('<h1>Other Title</h1>')\n            (tmpdir / 'test_app/models.py').write(\n                \"\"\"", "    def test_no_i18n(self):\n        with self.settings(USE_I18N=False):\n            self.assertFalse(os.path.exists(self.PO_FILE))\n        output, _ = self._run_makemessages()\n        self.assertFalse('Locale directory' in output)\n\n\n"], "sample_151": ["    def test_migration_detect_changes_with_custom_user_model(self):\n        before_states = [\n            author_name_deconstructible_list_1,\n            publisher,\n            other_pony,\n            book,\n            attribution,\n            edition,\n            custom_user_no_inherit,\n        ]\n        after_states = [\n            author_name_deconstructible_list_1,\n            publisher,\n            other_pony,\n            book,\n            attribution,\n            edition,\n            custom_user,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, \"thirdapp\", 1)\n        self.assertMigrationDependencies(changes, \"thirdapp\", 0, [])\n        self.assertOperationTypes(changes, \"thirdapp\", 0, [\"CreateModel\"])\n        self.assertOperationFieldAttributes(changes, \"thirdapp\", 0, 0, username='username')\n\n\n\n", "    def test_complex_pk_changes(self):\n        before_states = [\n            author_unmanaged,\n            author_unmanaged_custom_pk,\n        ]\n        after_states = [\n            author_with_m2m,\n            author_with_m2m_through,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, \"testapp\", 2)\n        self.assertMigrationDependencies(changes, \"testapp\", 0, ['testapp.Author']) # author_unmanaged_pk_to_m2m\n        self.assertMigrationDependencies(changes, \"testapp\", 1, []) # author_m2m_through\n\n        self.assertOperationTypes(changes, \"testapp\", 0, ['CreateModel', 'AddField', 'RemoveField', 'RemoveField', 'RemoveField'])\n        self.assertOperationTypes(changes, \"testapp\", 1, ['CreateModel'])\n\n", "    def test_circular_field_migrations(self):\n        changes = self.get_changes(\n            [\n                rabbit,\n                knight,\n            ],\n            [\n                rabbit,\n                knight,\n            ],\n        )\n        self.assertNumberMigrations(changes, \"eggs\", 2)\n        self.assertMigrationDependencies(changes, \"eggs\", 0, [])\n        self.assertMigrationDependencies(changes, \"eggs\", 1, [\"eggs.Rabbit\"])\n        self.assertOperationTypes(changes, \"eggs\", 0, [\"CreateModel\"])\n        self.assertOperationTypes(changes, \"eggs\", 1, [\"CreateModel\"])\n\n        self.assertOperationFieldAttributes(\n            changes, \"eggs\", 0, 0, f_name=\"knight\", to_field=knight,\n        )\n        self.assertOperationFieldAttributes(\n            changes, \"eggs\", 0, 1, f_name=\"parent\", to_field=rabbit,\n        )\n        self.assertOperationFieldAttributes(\n            changes, \"eggs\", 1, 0, f_name=\"knight\", to_field=knight,\n        )\n        self.assertOperationFieldAttributes(\n            changes, \"eggs\", 1, 1, f_name=\"parent\", to_field=rabbit,\n        )\n\n\n\n", "    def test_custom_inheritance(self):\n        before_states = [\n            author_based_on_author,\n            aardvark_testapp,\n        ]\n        after_states = [\n            author_based_on_author,\n            aardvark_testapp,\n            aardvark_pk_fk_author,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertMigrationDependencies(changes, 'testapp', 0, ['testapp.Author'])\n\n\n\n", "    def test_migration_with_rename_model(self):\n        before_states = [\n            author_with_db_table_options,\n            publisher_with_db_table_options,\n            author_renamed_with_db_table_options,\n            publisher_renamed_with_db_table_options,\n        ]\n        after_states = [\n            author_with_new_db_table_options,\n            publisher_with_new_db_table_options,\n            author_renamed_with_new_db_table_options,\n            publisher_renamed_with_new_db_table_options,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, \"testapp\", 4)\n        self.assertMigrationDependencies(changes, \"testapp\", 0, [])\n        self.assertMigrationDependencies(changes, \"testapp\", 1, [])\n        self.assertMigrationDependencies(changes, \"testapp\", 2, [])\n        self.assertMigrationDependencies(changes, \"testapp\", 3, [])\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameModel\"])\n        self.assertOperationTypes(changes, \"testapp\", 1, [\"RenameModel\"])\n        self.assertOperationTypes(changes, \"testapp\", 2, [\"RenameModel\"])\n        self.assertOperationTypes(changes, \"testapp\", 3, [\"RenameModel\"])\n"], "sample_1199": ["compilation error", "compilation error", "    def test_tensor_product_simp_with_comm():\n        comm = Comm(A, B)\n        tp = TP(comm, C)\n        simp = tensor_product_simp(tp)\n        expected = TP(Comm(A, B), C)\n        assert simp == expected \n\n", "compilation error", "compilation error"], "sample_204": ["    def test_inconsistent_history(self):\n        with override_settings(DATABASES={'default': {'ENGINE': 'django.db.backends.sqlite3'}}):\n            conn = connections['default']\n            with conn.cursor() as cursor:\n                cursor.execute(\"\"\"\n                    CREATE TABLE apps (\n                        id INTEGER PRIMARY KEY,\n                        name TEXT\n                    )\n                \"\"\")\n            app_label = 'tests'\n            MigrationLoader(conn).load_disk()\n            recorder = MigrationRecorder(conn)\n\n            recorder.record_migration(app_label, '0001_initial')\n            recorder.record_migration(app_label, '0002_another')\n            recorder.apply_migration(app_label, '0002_another')  \n\n            with self.assertRaises(InconsistentMigrationHistory):\n                MigrationLoader(conn).check_consistent_history(conn)\n\n            # Ensure the recorder's state is consistent\n            recorder.record_migration(app_label, '0001_initial')\n            recorder.apply_migration(app_label, '0001_initial')\n            MigrationLoader(conn).check_consistent_history(conn)\n\n\n\n", "    def test_inconsistent_migration_history(self):\n        apps = self.apps\n\n        # Create two apps, one with migrations and one without.\n        apps.register_app(\"testapp1\")\n        apps.register_app(\"testapp2\")\n\n        # Make a migration for testapp1.\n        with self.subTest(\n            \"Migration in testapp1\"\n        ):\n            with modify_settings(INSTALLED_APPS=['testapp1']):\n                self._create_migrations(\"testapp1\", 1)\n            # Apply the migration.\n            with connection.cursor() as cursor:\n                cursor.execute(\"\"\"\n                INSERT INTO django_migrations (app, name, applied) VALUES\n                ('testapp1', '0001_initial', True)\n                \"\"\")\n            # Attempt to apply a migration for testapp2 before it exists\n            with self.assertRaises(InconsistentMigrationHistory):\n                MigrationLoader(connection).check_consistent_history(connection)\n\n        # Make a migration for testapp2.\n        with self.subTest(\n            \"Migration in testapp2\"\n        ):\n            with modify_settings(INSTALLED_APPS=['testapp2']):\n                self._create_migrations(\"testapp2\", 1)\n            # Apply the migration.\n            with connection.cursor() as cursor:\n                cursor.execute(\"\"\"\n                INSERT INTO django_migrations (app, name, applied) VALUES\n                ('testapp2', '0001_initial', True)\n                \"\"\")\n            # Attempt to apply a migration for testapp1 before it exists\n            with self.assertRaises(InconsistentMigrationHistory):\n                MigrationLoader(connection).check_consistent_history(connection)\n        \n\n", "    def test_inconsistent_history(self):\n        with override_settings(\n            INSTALLED_APPS=['my_app'],\n        ):\n            # Set up a simple migration setup.\n            with connections['default'].cursor() as cursor:\n                cursor.execute(\"CREATE TABLE my_app_model (id int PRIMARY KEY)\")\n                cursor.execute(\n                    \"\"\"\n                    CREATE TABLE my_app_other_model (id int PRIMARY KEY)\n                    \"\"\"\n                )\n\n            # Apply a migration in my_app.\n            self.make_migration(\n                'my_app', '0001_initial', ['my_app_model'],\n            )\n            MigrationLoader.load_disk(\n                settings.DATABASES['default']\n            )\n            connection.migrate()\n\n            # Simulate a scenario where a dependency is missing.\n            recorder = MigrationRecorder(connection)\n            recorder.record_applied_migration(\n                (\n                    'my_app', '0001_initial',\n                )\n            )\n\n            recorder = MigrationRecorder(connection)\n            try:\n                recorder.record_applied_migration(\n                    ('my_app', '0002_missing_dependency')\n                )\n            except InconsistentMigrationHistory:\n                pass\n\n            with connections['default'].cursor() as cursor:\n                cursor.execute(\"CREATE TABLE my_app_another_model (id int PRIMARY KEY)\")\n\n            # Try to apply a migration that depends on a missing dependency\n            with self.assertRaises(InconsistentMigrationHistory):\n                recorder.record_applied_migration((\n                    'my_app', '0003_dependent',\n                ))\n\n\n\n", "    def test_inconsistent_history(self):\n        with override_settings(DATABASES={'default': self.default_db_config}):\n            # Create a migration and apply it\n            with connection.schema_editor() as schema_editor:\n                schema_editor.migrate(\n                    'tests.migrations.0001_initial', atomic=False\n                )\n            recorder = MigrationRecorder(connection)\n            recorder.record_migration('tests.migrations.0001_initial')\n            # Now try to apply a migration that depends on this migration\n            with connection.schema_editor() as schema_editor:\n                schema_editor.migrate(\n                    'tests.migrations.0002_second', atomic=False\n                )\n            recorder.record_migration('tests.migrations.0002_second')\n            # Test that this raises an error\n            with self.assertRaises(InconsistentMigrationHistory):\n                with connection.schema_editor() as schema_editor:\n                    schema_editor.migrate(\n                        'tests.migrations.0003_third', atomic=False\n                    )\n            recorder.record_migration('tests.migrations.0003_third')\n\n\n\n", "    def test_inconsistent_history(self):\n        with self.subTest(database='default'):\n            with connections['default'].cursor() as cursor:\n                # Create an app and migration\n                cursor.execute(\n                    \"CREATE TABLE app1_model (id INTEGER PRIMARY KEY)\"\n                )\n                with open('tests/migrations/0001_initial.py', 'w') as f:\n                    f.write(\"\"\"\n                    from django.db import migrations, models\n\n                    class Migration(migrations.Migration):\n\n                        dependencies = []\n\n                        operations = [\n                            migrations.CreateModel(\n                                name='Model',\n                                fields=[\n                                    ('id', models.AutoField(primary_key=True, serialize=False)),\n                                ],\n                            ),\n                        ]\n                    \"\"\")\n                compileall.compile_dir('tests/migrations')\n                # Record the migration as applied\n                recorder = MigrationRecorder(connections['default'])\n                recorder.record('app1', '0001_initial')\n                # Test that an inconsistency is raised when a migration depends on a non-applied one\n                with self.assertRaises(InconsistentMigrationHistory):\n                    MigrationLoader(connections['default'], load=False).check_consistent_history(connections['default'])\n\n\n"], "sample_541": ["    def test_polygon_selector_removes_vertex(self, ax):\n        polygon_selector = PolygonSelector(ax, lambda *args, **kwargs: None,\n                                          useblit=False)\n        polygon_selector._xys = [(0, 0), (1, 0), (1, 1), (0, 1), (0, 0)]\n        polygon_selector._selection_completed = True\n\n        polygon_selector._remove_vertex(0)\n\n        expected_verts = [(1, 0), (1, 1), (0, 1), (0, 0)]\n        assert polygon_selector._xys == expected_verts\n\n\n\n", "def test_lasso(ax, event_type, action, expected_verts, expected_state):\n    xy = (0.5, 0.5)\n    with mock.patch('matplotlib.widgets.Lasso.onrelease') as mock_onrelease:\n        lasso = widgets.Lasso(ax, xy, lambda verts: None)\n        # Simulate mouse movement and clicks\n        if action == 'click_and_hold':\n            click_and_drag(ax, \n                           (xy[0], xy[1]), \n                           (1.0, 1.0), \n                           button=1,\n                           duration=0.1)\n            do_event(MouseEvent(event_type, \n                                 x=1.0, \n                                 ", "    def test_polygon_selector_basic(ax):\n        onselect = lambda verts: verts\n        poly = PolygonSelector(ax, onselect)\n        click_and_drag(ax, 0.1, 0.2, 0.6, 0.4)\n        click_and_drag(ax, 0.7, 0.6, 0.1, 0.8)\n        click_and_drag(ax, 0.2, 0.1, 0.3, 0.3)\n        assert_allclose(poly.verts, [(0.1, 0.2), (0.6, 0.4), (0.7, 0.6), (0.1, 0.8), (0.2, 0.1)])\n        poly.disconnect_events()\n", "    def test_lasso(self, ax):\n        fig, ax = plt.subplots(1, 1)\n        xy = (0.2, 0.5)\n        callback = lambda verts: assert_allclose(verts, np.array(xy))\n        lasso = Lasso(ax, xy, callback)\n        click_and_drag(ax, xy[0] + 0.1, xy[1] + 0.1,\n                       end_x=xy[0] + 0.3, end_y=xy[1] - 0.1)\n        plt.close(fig)\n", "    def test_lasso_basic(ax, monkeypatch):\n        monkeypatch.setattr('matplotlib.pyplot.show', noop)\n        x = np.random.rand()\n        y = np.random.rand()\n        lasso = Lasso(ax, (x, y), callback=lambda verts: None)\n        click_and_drag(ax, (x, y), (x + 0.1, y), button=1)\n        click_and_drag(ax, (x + 0.1, y), (x + 0.2, y + 0.1), button=1)\n        click_and_drag(ax, (x + 0.2, y + 0.1), (x, y), button=1)\n        lasso.onrelease(mock_event(ax, 'button_release', 1))\n\n        verts = lasso.verts\n        assert verts is not None\n        assert len(verts) == 3\n        assert_allclose(verts[0], (x, y))\n        assert_allclose(verts[1], (x + 0.1, y))\n        assert_allclose(verts[2], (x + 0.2, y + 0.1))\n"], "sample_256": ["    def test_password_validation_new_password(self):\n        form = UserCreationForm(data={'username': 'testuser', 'password1': 'p', 'password2': 'p'})\n        self.assertFalse(form.is_valid())\n        self.assertIn(\"password1\", form.errors)\n\n\n", "    def test_user_creation_form_with_clean_password2(self):\n        form = UserCreationForm(data={'username': 'testclient', 'password1': 'password', 'password2': 'testwrong'})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['password2'], [_(\n            'The two password fields didn\u2019t match.'\n        )])\n\n\n\n", "    def test_save_creates_user(self):\n        form = UserCreationForm(data={'username': 'newuser', 'password1': 'password', 'password2': 'password'})\n        self.assertTrue(form.is_valid())\n        user = form.save()\n        self.assertEqual(user.username, 'newuser')\n        self.assertEqual(user.password, '$2b$12$k9q913jY22Z0D10e9aK2n.dD4z0eYvO/0i1p5w4p8u9x3Z...')\n\n\n", "    def test_user_creation_form_invalid_password(self):\n        form = UserCreationForm(data={'username': 'newuser', 'password1': '', 'password2': ''})\n        self.assertFalse(form.is_valid())\n        self.assertIn('password2', form.errors)\n        self.assertIn('required', form.errors['password2'])\n", "    def test_password_validation(self):\n        form = UserCreationForm(data={\"username\": self.u1.username, \"password1\": \"pass\", \"password2\": \"pass1\"})\n        self.assertFalse(form.is_valid())\n        self.assertIn(\n            \"password_mismatch\", form.errors,\n        )\n        \n"], "sample_1098": ["    def test_hyper_complex(self):\n        a, b, c, d, x, y = symbols('a b c d x y')\n        for _ in range(10):\n            a, b, c, d, x, y = randcplx(), randcplx(), randcplx(), randcplx(), randcplx(), randcplx()\n            result = meijerg(a, b, c, d, x, y)\n            tn(result, hyper(a, b, c, d, x, y), 1e-12)\n", "    def test_appellf1_special_cases(self):\n        a, b1, b2, c, x, y = symbols('a b1 b2 c x y')\n        n = S.Half\n        with tn(rtol=1e-12):\n            tn(appellf1(a, -n, -n, c, x, y), 1/appellf1(c - a, 1 - n, 1 - n, c, x, y))\n\n", "    def test_appellf1_small(self):\n        a, b1, b2, c, x, y = symbols('a b1 b2 c x y')\n        tn(appellf1(a, b1, b2, c, 0, 0), 1)\n        tn(appellf1(a, b1, b2, c, 0, 0), 1)\n        with tn(Rational(1, 2)):\n            tn(appellf1(1, 1, 1, 2, 1/2, 1/2), 1/4*pi*I + 1/4*log(2))\n            tn(appellf1(1, 1, 1, 2, 3/2, 1/2), 1/pi*I + 1/4*log(1/2))\n", "    def test_hyperexpand_power1(self):\n        # Test hypergeometric expansion for F([a], [], z)\n        a = symbols('a')\n        z = symbols('z')\n        for i in range(10):\n            a_val = randcplx()\n            tn(hyper([a], [], z).expand(),\n               HyperRep_power1._expr_small(a_val, z), (z, -1, 1))\n            tn(hyper([a], [], z).expand(),\n               HyperRep_power1._expr_big(a_val, z, i), (z, 1 + exp(2*pi*I*i), 1 + exp(2*pi*I*(i + 1))))\n\n\n", "    def test_appellf1_small_test(self):\n        a, b1, b2, c, x, y = symbols('a b1 b2 c x y')\n        tn(appellf1(a, b1, b2, c, x, y).series(x, 0, 2),\n           2*power_series.PowerSeries(1, O(x**2)),\n           rational=True)\n        tn(appellf1(a, b1, b2, c, x, y).series(y, 0, 2),\n           2*power_series.PowerSeries(1, O(y**2)),\n           rational=True)\n"], "sample_807": ["    def test_calibration_curve_uniform_strategy(self):\n        X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, random_state=42)\n        y_prob = np.random.rand(X.shape[0])\n\n        prob_true, prob_pred = calibration_curve(y, y_prob, normalize=True,\n                                                n_bins=10, strategy='uniform')\n        assert_array_equal(prob_true.shape, (10,))\n        assert_array_equal(prob_pred.shape, (10,))\n", "    def test_calibration_curve(self, method):\n        X, y = make_classification(n_samples=1000, n_features=10,\n                                   n_informative=5, n_redundant=5,\n                                   random_state=42)\n        y_prob = np.random.rand(X.shape[0])\n        prob_true, prob_pred = calibration_curve(y, y_prob,\n                                                 normalize=True, n_bins=10,\n                                                 strategy='quantile')\n        assert_greater_equal(np.min(prob_true), 0)\n        assert_greater_equal(np.min(prob_pred), 0)\n        assert_greater_equal(np.max(prob_true), 1)\n        assert_greater_equal(np.max(prob_pred), 1)\n        assert_equal(len(prob_true), len(prob_pred))  \n", "    def test_sigmoid_calibration_robust_to_noise(self):\n        \"\"\"Test that sigmoid calibration works correctly with noisy data.\"\"\"\n        # Generate noisy data\n        X, y = make_classification(n_samples=1000, n_features=20,\n                                    random_state=42)\n        y_prob = np.random.rand(1000)  # Simulate noisy probabilities\n        y_true = (y_prob > 0.5).astype(int)\n\n        # Fit the sigmoid calibration\n        a, b = _sigmoid_calibration(X, y_true, sample_weight=None)\n\n        # Create a new classifier using the calibrated probabilities\n        calibrated_classifier = _SigmoidCalibration(a=a, b=b)\n\n        # Predict probabilities using the calibrated classifier\n        new_predictions = calibrated_classifier.predict(X)\n\n        # Check that the predictions are calibrated\n        prob_true, prob_pred = calibration_curve(y_true, new_predictions,\n                                               normalize=True, n_bins=10)\n        assert_allclose(prob_true, prob_pred)\n", "    def test_sigmoid_calibration(self):\n        X, y = make_classification(n_samples=1000, n_features=2, random_state=42)\n        model = LinearSVC(random_state=42)\n        model.fit(X, y)\n        a, b = _sigmoid_calibration(model.decision_function(X), y)\n        assert_almost_equal(a, 0.78275)\n        assert_almost_equal(b, -1.02503)\n\n", "    def test_calibration_curve(self, method):\n        X, y = make_classification(n_samples=1000, n_features=20,\n                                   random_state=42)\n        clf = RandomForestClassifier(random_state=42)\n        cv = LeaveOneOut()\n        calibrated_clf = CalibratedClassifierCV(base_estimator=clf,\n                                                method=method, cv=cv)\n\n        calibrated_clf.fit(X, y)\n\n        prob_true, prob_pred = calibration_curve(y,\n                                                 calibrated_clf.predict_proba(X),\n                                                 n_bins=10)\n\n        assert_array_almost_equal(prob_true, np.mean(y),\n                                  err_msg=\"True probabilities should be close \"\n                                           \"to the mean of y\")\n        assert_array_almost_equal(prob_pred, np.mean(y),\n                                  err_msg=\"Predicted probabilities should be \"\n                                           \"close to the mean of y\")\n\n"], "sample_349": ["    def test_autocomplete_init_with_attrs(self):\n        widget = AutocompleteSelect(\n            Album._meta.get_field('band'),\n            admin.site,\n            attrs={'data-test-attr': 'value'},\n        )\n        self.assertIn('data-test-attr=\"value\"', widget.build_attrs({'class': 'test'}))\n", "    def test_autocomplete_required_field(self):\n        form = RequiredBandForm(data={'band': ''})\n        self.assertFalse(form.is_valid())\n        self.assertIn('band', form.errors)\n        self.assertEqual(len(form.errors['band']), 1)\n        self.assertEqual(form.errors['band'][0], 'This field is required.')\n", "    def test_autocomplete_form_fields(self):\n        with translation.override('en'):\n            form = AlbumForm()\n            self.assertIn(\n                'data-ajax--url=\"%s\"' % reverse('admin:autocomplete'),\n                form.band.widget.attrs.get('data-ajax--url'),\n            )\n            self.assertIn(\n                'data-app-label=\"%s\"' % Album._meta.app_label,\n                form.band.widget.attrs.get('data-app-label'),\n            )\n            self.assertIn(\n                'data-model-name=\"%s\"' % Album._meta.model_name,\n                form.band.widget.attrs.get('data-model-name'),\n            )\n            self.assertIn(\n                'data-field-name=\"%s\"' % Album._meta.get_field('band').name,\n                form.band.widget.attrs.get('data-field-name'),\n            )\n\n            self.assertIn(\n                'data-ajax--url=\"%s\"' % reverse('admin:autocomplete'),\n                form.featuring.widget.attrs.get('data-ajax--url'),\n            )\n            self.assertIn(\n                'data-app-label=\"%s\"' % Album._meta.app_label,\n                form.featuring.widget.attrs.get('data-app-label'),\n            )\n            self.assertIn(\n                'data-model-name=\"%s\"' % Album._meta.model_name,\n                form.featuring.widget.attrs.get('data-model-name'),\n            )\n            self.assertIn(\n                'data-field-name=\"%s\"' % Album._meta.get_field('featuring').name,\n                form.featuring.widget.attrs.get('data-field-name'),\n            )\n", "    def test_autocomplete_select_with_required_field(self):\n        form = RequiredBandForm()\n        self.assertContains(form.render(), '<select', count=1)\n        self.assertContains(form.render(), 'required', count=1)\n        ", "    def test_autocomplete_select_multiple_empty_values(self):\n        with translation.override('en'):\n            form = AlbumForm(None, {'band': [], 'featuring': ['']})\n            self.assertHTMLEqual(\n                form['band'].render(),\n                '',\n            )\n            self.assertHTMLEqual(\n                form['featuring'].render(),\n                self.empty_option,\n            )\n"], "sample_898": ["    def test_label_ranking_loss_multilabel_indicator(self):\n        y_true = np.array([[0, 1, 0],\n                           [0, 0, 1],\n                           [1, 0, 0]])\n        y_score = np.array([[0.1, 0.9, 0.2],\n                            [0.2, 0.3, 0.8],\n                            [0.7, 0.5, 0.4]])\n        loss = label_ranking_loss(y_true, y_score)\n        expected_loss = 1.3333333333333333\n        assert_almost_equal(loss, expected_loss)\n\n\n\n", "    def test_accuracy_score_invalid_multiclass(self):\n        # multilabel-indicator format with more than one positive label\n        y_true = np.array([\n            [1, 0, 1], [0, 1, 1], [1, 1, 0]\n        ], dtype=np.int32)\n        y_pred = np.array([\n            [0, 1, 0], [1, 0, 0], [1, 1, 1]\n        ], dtype=np.int32)\n        with pytest.raises(ValueError):\n            accuracy_score(y_true, y_pred, average='micro')\n", "    def test_hinge_loss_with_sample_weight(self):\n        # test with sample_weight on binary classification\n        X = np.array([[1], [2], [3], [4]])\n        y = np.array([0, 0, 1, 1])\n        sample_weight = np.array([0.2, 0.5, 0.8, 1.0])\n        \n        X_train = X\n        y_train = y\n        sample_weight_train = sample_weight\n\n        scores = np.array([0, 0.2, 0.5, 0.8])\n\n        expected_hinge_loss = 0.275\n        \n        hinge_loss_score = hinge_loss(y_train, scores, sample_weight=sample_weight_train)\n        \n        assert_almost_equal(hinge_loss_score, expected_hinge_loss)\n", "    def test_coverage_error_with_sample_weight(self):\n        y_true = np.array([\n            [1, 0, 0],\n            [0, 1, 1],\n            [1, 1, 0],\n        ])\n        y_score = np.array([\n            [0.7, 0.3, 0.2],\n            [0.2, 0.8, 0.9],\n            [0.9, 0.6, 0.4],\n        ])\n\n        sample_weight = np.array([0.1, 0.5, 0.4])\n\n        expected = 1.6666666666666667\n        result = coverage_error(\n            y_true, y_score, sample_weight=sample_weight)\n        assert_almost_equal(result, expected)\n\n", "    def test_label_ranking_loss_multilabel_binary(self):\n        y_true = np.array([[0, 1, 0], [1, 0, 1], [0, 0, 1]])\n        y_score = np.array([[0.3, 0.7, 0.1], [0.9, 0.2, 0.8], [0.4, 0.6, 0.9]])\n        expected_loss = 1.5\n        loss = label_ranking_loss(y_true, y_score)\n        assert_almost_equal(loss, expected_loss)\n"], "sample_114": ["    def test_migration_changes_with_nested_deconstructible_fields(self):\n        before_states = [\n            author_name_nested_deconstructible_1,\n            author_name_nested_deconstructible_2,\n            book_foo_together,\n            book_foo_together_2,\n            custom_user,\n        ]\n        after_states = [\n            author_name_nested_deconstructible_changed_arg,\n            author_name_nested_deconstructible_2,\n            book_foo_together_2,\n            book_foo_together_3,\n            custom_user_no_inherit,\n        ]\n        changes = self.get_changes(before_states, after_states)\n\n        self.assertNumberMigrations(changes, \"testapp\", 3)\n        self.assertMigrationDependencies(changes, \"testapp\", 0, [])\n        self.assertMigrationDependencies(changes, \"testapp\", 1, [\"testapp.author\"])\n        self.assertMigrationDependencies(changes, \"testapp\", 2, [\"testapp.book\"])\n\n\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RemoveField\", \"AddField\"])\n        self.assertOperationTypes(changes, \"testapp\", 1, [\"AlterField\", \"RenameField\"])\n        self.assertOperationTypes(changes, \"testapp\", 1, [\"RemoveField\", \"AddField\"])\n        self.assertOperationTypes(changes, \"thirdapp\", 0, [\"DeleteModel\"])\n", "    def test_index_together_circular_dependencies(self):\n        changes = self.get_changes([\n            author_with_book,\n            book,\n            attribution,\n        ], [\n            author_with_book_order_wrt,\n            book_foo_together,\n            attribution,\n        ])\n        self.assertNumberMigrations(changes, \"testapp\", 2)\n        self.assertMigrationDependencies(changes, \"testapp\", 0, [\"otherapp\"])\n        self.assertMigrationDependencies(changes, \"testapp\", 1, [])\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"CreateModel\", \"AddField\"])\n        self.assertOperationTypes(changes, \"testapp\", 1, [\"AlterField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, field=\"book\", to_field=\"otherapp.Book\")\n        self.assertOperationAttributes(changes, \"testapp\", 0, 1, field=\"id\", primary_key=True)\n        self.assertOperationAttributes(changes, \"testapp\", 1, 0, field=\"order_wrt\", default=None)\n\n\n\n", "    def test_migration_dependencies_with_circular_fk(self):\n        changes = self.get_changes([\n            author_name_deconstructible_list_1,\n            author_name_deconstructible_list_2,\n            author_name_deconstructible_list_3,\n            author_name_deconstructible_tuple_1,\n            author_name_deconstructible_tuple_2,\n            author_name_deconstructible_tuple_3,\n            author_name_deconstructible_dict_1,\n            author_name_deconstructible_dict_2,\n            author_name_deconstructible_dict_3,\n            author_name_nested_deconstructible_1,\n            author_name_nested_deconstructible_2,\n            author_with_publisher,\n            rabbit,\n        ], [\n            author_name_deconstructible_list_1,\n            author_name_deconstructible_list_2,\n            author_name_deconstructible_list_3,\n            author_name_deconstructible_tuple_1,\n            author_name_deconstructible_tuple_2,\n            author_name_deconstructible_tuple_3,\n            author_name_deconstructible_dict_1,\n            author_name_deconstructible_dict_2,\n            author_name_deconstructible_dict_3,\n            author_name_nested_deconstructible_1,\n            author_name_nested_deconstructible_2,\n            author_with_publisher,\n            rabbit,\n        ])\n        self.assertMigrationDependencies(changes, \"eggs\", 0, [])\n", "    def test_migrations_for_model_renaming_and_custom_pk(self):\n        before_states = [\n            author_with_db_table_options,\n            author_renamed_with_db_table_options,\n            book_proxy_fk,\n            book_with_author_renamed,\n            book_with_field_and_author_renamed,\n        ]\n        after_states = [\n            author_with_new_db_table_options,\n            author_renamed_with_new_db_table_options,\n            book_proxy_fk,\n            book_with_author_renamed,\n            book_with_field_and_author_renamed,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, \"testapp\", 2)\n        self.assertNumberMigrations(changes, \"otherapp\", 2)\n        self.assertMigrationDependencies(changes, \"testapp\", 0, ['testapp.author'])\n        self.assertMigrationDependencies(changes, \"testapp\", 1, ['testapp.author'])\n        self.assertMigrationDependencies(changes, \"otherapp\", 0, [])\n        self.assertMigrationDependencies(changes, \"otherapp\", 1, ['testapp.Author'])\n        self.assertOperationTypes(changes, \"testapp\", 0, ['CreateModel', 'RenameModel'])\n        self.assertOperationTypes(changes, \"testapp\", 1, ['RenameModel', 'RenameModel'])\n        self.assertOperationTypes(changes, \"otherapp\", 0, ['CreateModel', 'RenameModel'])\n        self.assertOperationTypes(changes, \"otherapp\", 1, ['RenameModel', 'RenameModel'])\n\n", "    def test_detect_changes_extra_kwarg_proxies(self):\n        before_states = [\n            author_proxy_third,\n            author_proxy_notproxy,\n        ]\n        after_states = [\n            author_proxy_third,\n            author_proxy_notproxy,\n            author_proxy_third_notproxy,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertMigrationDependencies(changes, 'testapp', 0, ['thirdapp.AuthorProxy'])\n\n\n"], "sample_1131": ["    def test_sympy_matrix_solve(self):\n        A = MatrixSymbol('A', 2, 2)\n        b = MatrixSymbol('b', 2, 1)\n        sol = MatrixSolve(A, b)\n\n        self.assertEqual(pycode(sol, printer=SymPyPrinter()),\n                         'sympy.MatrixSolve(A, b)')\n", "    def test_sympy_printing(self):\n        expr = (S(1) + 2*x**2 + 3*y*z)**Rational(3, 2)\n        code = pycode(expr)\n        expected = '(1 + 2*x**2 + 3*y*z)**(3/2)'\n        assert code == expected\n", "    def test_Piecewise(self):\n        expr = Piecewise((2 * x, x > 1), (x**2, True))\n        code = pycode(expr)\n        self.assertEqual(code, 'numpy.select(array([x > 1, True]), array([2*x, x**2]), default=nan)')\n\n", "    def test_sympy_printing(self):\n        expr = expm1(x)\n        self.assertEqual(pycode(expr), \"sympy.expm1(x)\")\n\n        expr = log1p(x)\n        self.assertEqual(pycode(expr), \"sympy.log1p(x)\")\n\n        expr = cosm1(x)\n        self.assertEqual(pycode(expr), \"sympy.special.cosm1(x)\")\n\n        expr = Piecewise((x, x > 0), (1, x <= 0))\n        self.assertEqual(pycode(expr), \"sympy.Piecewise((x, x > 0), (1, x <= 0))\")\n\n        expr = sign(x)\n        self.assertEqual(pycode(expr), \"sympy.sign(x)\")\n        \n        expr = sqrt(x)\n        self.assertEqual(pycode(expr), \"sympy.sqrt(x)\")\n\n        expr = acos(x)\n        self.assertEqual(pycode(expr), \"sympy.acos(x)\")\n\n        expr = KroneckerDelta(x, y)\n        self.assertEqual(pycode(expr), \"sympy.KroneckerDelta(x, y)\")\n\n\n\n        expr = MatrixSolve(MatrixSymbol('A', 2, 2), MatrixSymbol('B', 2, 1))\n        self.assertEqual(pycode(expr), \"sympy.matrices.MatrixSolve(A, B)\")\n\n\n\n        expr = SparseMatrix( {(0, 0): 1, (1, 1): 2})\n        self.assertEqual(pycode(expr), \"sympy.matrices.SparseMatrix({(0, 0): 1, (1, 1): 2})\")\n\n        expr = Identity(2)\n        self.assertEqual(pycode(expr), \"sympy.matrices.Identity(2)\")\n\n\n        \n        expr =  loggamma(factorial(x))\n        self.assertEqual(pycode(expr), \"sympy.loggamma(sympy.factorial(x))\")\n\n        expr =  pow(x, Rational(1,2))\n        self.assertEqual(pycode(expr), \"sympy.Pow(x, sympy.Rational(1, 2))\")", "    def test_piecewise(self):\n        expr = Piecewise((x, x < 0), (x + 1, x >= 0))\n        result = pycode(expr)\n        self.assertEqual(result, 'numpy.select(x < 0, [x], [x + 1])')\n\n\n"], "sample_914": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_179": ["    def test_index_together_with_invalid_field(self):\n        with override_settings(DATABASE_ROUTERS={'default': EmptyRouter()}):\n            class MyModel(models.Model):\n                name = models.CharField(max_length=50)\n                age = models.IntegerField()\n\n                class Meta:\n                    indexes = [\n                        models.Index(fields=['name', 'invalid_field'], name='name_age_index'),\n                    ]\n\n        with self.assertRaises(models.Error) as ctx:\n            _check_lazy_references(MyModel)\n        self.assertIn(\"The field 'invalid_field' does not exist on model 'MyModel'.\", str(ctx.exception))\n\n\n\n", "    def test_index_together_with_expression(self):\n        with override_settings(DATABASE_ROUTERS=[EmptyRouter()]):\n            class Product(models.Model):\n                name = models.CharField(max_length=100)\n                price = models.DecimalField(max_digits=10, decimal_places=2)\n                category = models.ForeignKey(\n                    'Category', related_name='products', on_delete=models.CASCADE\n                )\n\n            class Category(models.Model):\n                name = models.CharField(max_length=100)\n\n            Product._meta.index_together = [\n                ('name', Lower('category__name'))\n            ]\n\n            errors = Product._meta.check()\n            self.assertEqual(len(errors), 0)\n\n\n\n", "    def test_index_together_with_condition(self):\n        with override_settings(DATABASE_ROUTERS={'default': EmptyRouter()}) as settings:\n            class Product(models.Model):\n                name = models.CharField(max_length=255)\n                price = models.DecimalField(max_digits=10, decimal_places=2)\n\n                class Meta:\n                    indexes = [\n                        models.Index(fields=['price'], condition=models.Q(name__startswith='A'))\n                    ]\n\n        with self.assertRaises(Warning) as context:\n            _check_lazy_references(Product, connections['default'])\n        self.assertIn(\n            \"Product 'price' index condition is not supported.\",\n            str(context.exception),\n        )\n\n\n\n", "    def test_index_together_with_none(self):\n        class TestModel(models.Model):\n            field1 = models.IntegerField()\n            field2 = models.CharField(max_length=100)\n            class Meta:\n                indexes = [\n                    models.Index(fields=['field1', 'field2'], name=\"together_index\"),\n                ]\n\n        with self.subTest('Check for errors'):\n            errors = _check_index_together([TestModel._meta], 'invalid_models_tests')\n            self.assertEqual(len(errors), 0)\n\n\n\n\n", "    def test_index_together_with_conditions(self):\n        with override_settings(DATABASE_ROUTERS={'default': EmptyRouter()}):\n            class Product(models.Model):\n                name = models.CharField(max_length=50)\n\n            class Order(models.Model):\n                product = models.ForeignKey(Product, on_delete=models.CASCADE)\n                customer_email = models.CharField(max_length=100)\n                created_at = models.DateTimeField(auto_now_add=True)\n\n                class Meta:\n                    indexes = [\n                        models.Index(\n                            fields=['product', 'customer_email'],\n                            condition=models.Q(created_at__gt=datetime.datetime.now() - timedelta(days=7)),\n                        ),\n                    ]\n\n            with self.subTest('Index conditions not ignored'):\n                with self.assertRaises(ValueError):\n                    Order.objects.create(product=Product.objects.create(name='Test'), customer_email='test@example.com')\n\n\n\n"], "sample_643": ["    def test_colorized_text_reporter_handles_undefined_msg_id(caplog, mock_linter, colorized_text_reporter):\n        caplog.set_level(WARNING)\n\n        class MockReporter(BaseReporter):\n                super().__init__(None)\n\n                pass\n\n        mock_linter.add_check_result(\n            \"test\",\n            \"unused\",\n            HIGH,\n            \"Undefined msg_id\",\n            (1, 1),\n            (1, 1),\n            \"dummy_info\",\n            None,\n            \"mock\",\n        )\n\n        with redirect_stdout(StringIO()) as output:\n            reporters = [mock_reporter, colorized_text_reporter]\n            multi_reporter = MultiReporter(reporters)\n            multi_reporter.run(mock_linter)\n\n        output_str = output.getvalue()\n        assert \"Undefined msg_id\" in output_str\n\n        recorded_warnings = caplog.read_warnings()\n        assert len(recorded_warnings) == 1\n        assert (\n            recorded_warnings[0].message\n            == \"Don't recognize the argument 'id' in the --msg-template. Are you sure it is supported on the current version of pylint?\"\n        )\n", "    def test_register(disable, reporter):\n        with WarningsRecorder() as warnings:\n            register(PyLinter())\n        assert not warnings.filter(DeprecationWarning)\n        assert isinstance(\n            PyLinter().get_reporter(\"text\"), TextReporter\n        )\n        assert isinstance(\n            PyLinter().get_reporter(\"parseable\"), ParseableTextReporter\n        )\n        assert isinstance(\n            PyLinter().get_reporter(\"colorized\"), ColorizedTextReporter\n        )\n", "    def test_colorized_text_reporter_handles_empty_color_mapping(\n        monkeypatch, reporter, disable", "    def test_disable_messages_with_colorized_reporter(reporter, disable):\n        linter = PyLinter()\n        linter.set_reporter(ColorizedTextReporter(color_mapping=disable))\n        linter.check(r\"E:\\path\\to\\your\\file.py\")\n        recorder = WarningsRecorder()\n        with recorder, redirect_stdout(StringIO()):\n            linter.emit_messages(\n                filename=\"E:\\path\\to\\your\\file.py\",\n                messages=[\n                    Message(\n                        \"file\",\n                        1,\n                        5,\n                        \"C001\",\n                        \"Undefined name\",\n                        \"msg\",\n                        HIGH,\n                        None,\n                        {\"symbol\": \"A\"},\n                    )\n                ],\n            )\n        assert recorder.list_warnings(), 'Messages should not be disabled'\n", "def test_colorized_text_reporter_handles_ansi_codes_correctly(\n    reporter: TextReporter, disable: list[str]"], "sample_243": ["    def test_related_to_expression(self):\n        q = Item.objects.filter(\n            author__name__icontains='a',\n            related_to_obj__isnull=False\n        ).annotate(\n            lower_name=Lower('name')\n        ).values('lower_name', 'related_to_obj__id')\n        self.assertEqual(\n            q.query.sql,\n            \"SELECT COALESCE(NULLIF(LOWER(\\\"items\\\".\\\"name\\\"), '' ), 'NULL') AS \\\"lower_name\\\", \\\"items\\\".\\\"related_to_obj_id\\\" FROM \\\"items\\\" INNER JOIN \\\"authors\\\" ON (\\\"items\\\".\\\"author_id\\\" = \\\"authors\\\".\\\"id\\\") WHERE (\\\"authors\\\".\\\"name\\\" ILIKE '%a%') AND (\\\"items\\\".\\\"related_to_obj_id\\\" IS NOT NULL)\"\n        )\n\n\n", "    def test_related_lookups(self):\n        # Test the handling of related lookups (specifically, nested\n        # related lookups).\n        query = Ranking.objects.filter(author__id__in=[1, 2])\n        self.assertEqual(query.query.extra_join_filters, [\n            ('INNER JOIN ranking_author ON ranking.id = ranking_author.id',),\n        ])\n        query = Ranking.objects.filter(author__name__icontains='John')\n        self.assertEqual(query.query.extra_join_filters, [\n            ('INNER JOIN ranking_author ON ranking.id = ranking_author.id',),\n        ])\n\n        query = Ranking.objects.filter(\n            author__books__title__icontains='Great'\n        )\n        self.assertEqual(query.query.extra_join_filters, [\n            ('INNER JOIN ranking_author ON ranking.id = ranking_author.id',),\n            ('INNER JOIN author_books ON ranking_author.id = author_books.author_id',),\n        ])\n\n        query = Ranking.objects.filter(\n            author__books__isnull=True\n        )\n        self.assertEqual(query.query.extra_join_filters, [\n            ('INNER JOIN ranking_author ON ranking.id = ranking_author.id',),\n            ('LEFT JOIN author_books ON ranking_author.id = author_books.author_id',),\n        ])\n\n        query = Ranking.objects.filter(\n            author__in=[Author.objects.get(id=1), Author.objects.get(id=2)]\n        )\n        self.assertEqual(query.query.extra_join_filters, [\n            ('INNER JOIN ranking_author ON ranking.id = ranking_author.id',),\n        ])\n\n\n\n\n\n\n", "    def test_get_loaded_field_names_with_deferred_loading(self):\n        qs = Ranking.objects.defer('name', 'pk').only('rank')\n        self.assertEqual(qs.get_loaded_field_names(), {})\n        qs = Ranking.objects\n        self.assertEqual(qs.get_loaded_field_names(), {'Ranking': {'rank':}})\n\n        qs = Ranking.objects.defer('name').only('rank')\n        self.assertEqual(qs.get_loaded_field_names(), {'Ranking': {'rank':}})\n\n        qs = Ranking.objects.only('name', 'rank')\n        self.assertEqual(qs.get_loaded_field_names(), {'Ranking': {'rank', 'name'}})\n\n\n", "    def test_add_fields_with_existing_deferred_loading(self):\n        Item.objects.all().clear_deferred_loading()\n        q = Item.objects.filter(\n            content__icontains='test'\n        )\n\n        q.add_fields(['id', 'content'])\n        q.add_deferred_loading(['title'])\n\n        self.assertEqual(q.deferred_loading, (frozenset(['title']), False))\n        self.assertEqual(q.select, ('content__content', 'id'))\n\n        q = Item.objects.filter(\n            content__icontains='test'\n        )\n\n        q.add_fields()\n        q.add_deferred_loading(['title'])\n\n        self.assertEqual(q.deferred_loading, (frozenset(['title']), False))\n        self.assertEqual(q.select, (\n            'id', 'title', 'content__content'\n        ))\n", "    def test_add_fields_with_related_m2m(self):\n        item = Item.objects.create(name='Test Item')\n        author = Author.objects.create(name='Test Author')\n        item.authors.add(author)\n        q = Item.objects.filter(name='Test Item')\n\n        q.add_fields(['authors__name'])\n\n        result = list(q.values_list('id', 'authors__name'))\n        self.assertIn((item.id, author.name), result)\n"], "sample_120": ["    def test_serialize_custom_types(self):\n        from django.db.migrations.operations.operations import Operation\n        class MyCustomOperation(Operation):\n                self.value = value\n        serializer = serializer_factory(MyCustomOperation(1))\n        self.assertEqual(serializer.serialize(), ('MyCustomOperation(1)', set()))\n\n\n", "    def test_serialize_custom_operations(self):\n        with mock.patch('django.db.migrations.writer.OperationWriter') as mock_operation_writer:\n            mock_operation_writer.return_value.serialize.return_value = 'custom_operation'\n            serializer = custom_migration_operations.operations.CustomOperationSerializer()\n            result = serializer.serialize(custom_migration_operations.more_operations.CustomOperation())\n            self.assertEqual(result, 'custom_operation')\n\n\n", "    def test_special_cases(self):\n        with mock.patch('custom_migration_operations.more_operations.MyOperation.operations', side_effect=[custom_migration_operations.more_operations.MyOperation]):\n            operation = custom_migration_operations.more_operations.MyOperation()\n            writer = OperationWriter('Test writer', operation, indentation=0)\n            serialized_operation = writer.serialize()\n\n        self.assertEqual(serialized_operation, 'MyOperation()')\n\n\n\n", "    def test_serializes_custom_types(self):\n        with mock.patch('django.db.migrations.operations.deserialize.Money', autospec=True):\n            class CustomSerializer(BaseSerializer):\n                    return repr(value)\n            serializer = CustomSerializer(Money(10))\n            self.assertEqual(serializer.serialize(), 'Money(10)')\n", "    def test_serialize_functools_partial(self):\n        partial = functools.partial(lambda x, y, z: x + y + z, 10, 20)\n        serializer = serializer_factory(partial)\n        serialized_string, imports = serializer.serialize()\n        expected_string = (\n            \"functools.partial(lambda x, y, z: x + y + z, 10, 20)\"\n        )\n        self.assertEqual(serialized_string, expected_string)\n        self.assertEqual(imports, {'import functools'})\n"], "sample_940": ["    def test_isenumclass(self):\n        assert inspect.isenumclass(enum.Enum)\n        assert inspect.isenumclass(CustomEnum)\n        assert not inspect.isenumclass(list)\n        assert not inspect.isenumclass(str)\n\n\n", "    def test_signature_from_str_with_defaults():\n        code = 'def func(a=1, b=2, c: int, *args, **kwargs): pass'\n        sig = inspect.signature_from_str(code)\n        expected_args = [\n            Parameter('a', inspect.Parameter.POSITIONAL_OR_KEYWORD, default=1),\n            Parameter('b', inspect.Parameter.POSITIONAL_OR_KEYWORD, default=2),\n            Parameter('c', inspect.Parameter.POSITIONAL_OR_KEYWORD, annotation=int),\n            Parameter('*args', inspect.Parameter.VAR_POSITIONAL),\n            Parameter('**kwargs', inspect.Parameter.VAR_KEYWORD),\n        ]\n        assert list(sig.parameters.values()) == expected_args\n        assert sig.return_annotation is inspect.Signature.empty\n\n\n\n", "    def test_is_singledispatch_function(monkeypatch):\n        @inspect.singledispatch\n            return x\n\n        @foo.register(str)\n            return f'string: {x}'\n\n        assert inspect.is_singledispatch_function(foo)\n        assert inspect.is_singledispatch_method(foo)\n        assert inspect.is_singledispatch_function(foo.__wrapped__)\n        assert not inspect.is_singledispatch_method(foo.__wrapped__)\n\n\n\n", "    def test_is_builtin_class_method(self):\n        assert is_builtin_class_method(int, '__init__')\n        assert not is_builtin_class_method(str, '__init__')\n        assert is_builtin_class_method(list, '__new__')\n\n\n", "    def test_signature_from_ast_empty(self):\n        code = 'def func(): pass'\n        module = ast.parse(code)\n        function = cast(ast.FunctionDef, module.body[0])  # type: ignore\n        sig = inspect.signature_from_ast(function, code)\n        assert sig.parameters == {}\n        assert sig.return_annotation is inspect.Signature.empty\n"], "sample_536": ["    def test_lasso_widget(self, ax):\n        x = np.arange(10)\n        y = np.sin(x)\n        ax.plot(x, y)\n        lasso = Lasso(ax, (2, 0.8), lambda verts: assert_allclose(verts, [(2, 0.8), (5, 0.2), (6, 0.9), (2, 0.8)]))\n        lasso.onmove = lambda event: None\n        lasso.onrelease = lambda event: None\n        click_and_drag(ax, (2, 0.8), (5, 0.2), 1, 1)\n        click_and_drag(ax, (5, 0.2), (6, 0.9), 1, 1)\n        click_and_drag(ax, (6, 0.9), (2, 0.8), 1, 1)\n\n\n\n", "    def test_lasso(self, ax):\n        x = np.linspace(0, 10, 100)\n        y = np.sin(x)\n        ax.plot(x, y)\n\n            verts = np.array(verts)\n            assert_allclose(verts[:, 0], np.linspace(1, 5, 5))\n            assert_allclose(verts[:, 1], [0.5, 0.8, -0.2, -0.5, 0.2])\n\n        lasso = Lasso(ax, (1, 0.5), callback, useblit=False)\n        click_and_drag(ax, (1, 0.5), (5, 0.8))\n        click_and_drag(ax, (5, 0.8), (5, -0.2))\n        click_and_drag(ax, (5, -0.2), (1, -0.5))\n        click_and_drag(ax, (1, -0.5), (1, 0.2))\n", "    def test_polygon_selector_completed_state(ax):\n        onselect = mock.MagicMock()\n        p = PolygonSelector(ax, onselect, grab_range=1, draw_bounding_box=True,\n                           box_handle_props=dict(markerfacecolor='red'))\n        p.set_visible(True)\n\n        # Test that the polygon completion event is called when the\n        # user closes the polygon\n        click_and_drag(ax, p._grab_range, p._grab_range, (0, 0), event_type='button_release')\n        assert onselect.called_once()\n\n        # Test that the polygon state is updated correctly after completion\n\n        # Test that the bounding box is updated after polygon completion\n\n        # Test that the bounding box is correctly positioned when the polygon\n        # is drawn\n\n        # Test that the bounding box can be moved and resized\n\n\n\n", "    def test_lasso_callback(self, ax):\n            assert len(verts) > 1\n            assert all(isinstance(v, tuple) and len(v) == 2 for v in verts)\n            \n        lasso = Lasso(ax, (0.2, 0.2), callback)\n        click_and_drag(ax, (0.5, 0.3), (0.8, 0.7), button=1)\n        plt.close()\n\n\n\n", "    def test_lasso_callback(self, ax):\n            # Ensure verts is a list of tuples\n            assert isinstance(verts, list)\n            assert all(isinstance(v, tuple) and len(v) == 2 for v in verts)\n            # Simple check on the vertices' values\n            assert_allclose(verts[0], (0, 0))\n            assert_allclose(verts[-1], (1, 1))\n            return\n\n        lasso = Lasso(ax, (0, 0), callback)\n        click_and_drag(ax, (0.5, 0.5, 1, 1))\n\n"], "sample_523": ["    def test_legend_order(self):\n        fig, ax = plt.subplots()\n        line1, = ax.plot([1, 2, 3], [4, 5, 6])\n        line2, = ax.plot([7, 8, 9], [10, 11, 12])\n        handler = mlegend.HandlerLine2D(\n            prop=mpl.rcParams['legend.fontsize'])\n        legend = mlegend.Legend(\n            ax,\n            handles=[line2, line1],\n            labels=['Line 2', 'Line 1'],\n            handler_map={Legend.Line2D: handler},\n            loc='best')\n        legend.draw_frame(True)\n        fig.canvas.draw()\n\n        expected_lines = [line2, line1]\n        observed_lines = [handle for handle in legend.get_lines()]\n        assert observed_lines == expected_lines\n\n        # Test when order is explicitly specified\n        legend = mlegend.Legend(\n            ax,\n            handles=[line1, line2],\n            labels=['Line 1', 'Line 2'],\n            handler_map={Legend.Line2D: handler},\n            loc='best')\n        legend.draw_frame(True)\n        fig.canvas.draw()\n        \n        expected_lines = [line1, line2]\n        observed_lines = [handle for handle in legend.get_lines()]\n        assert observed_lines == expected_lines\n\n", "    def test_legend_multiple_patches(self):\n        fig, ax = plt.subplots()\n        patches = [plt.Rectangle((i, 0), 1, 1, facecolor=f'C{i%5}')\n                   for i in range(6)]\n        ax.add_collection(mcollections.PatchCollection(patches))\n\n        handles, labels = ax.get_legend_handles_labels()\n        legend = ax.legend(handles=handles, labels=labels)\n        \n        # test that the legend was drawn correctly\n\n        # get the renderer\n        renderer = self.fig.canvas.get_renderer()\n        legend_bbox = legend.get_bbox_to_anchor(renderer)\n        \n        # check that the legend is within the bbox\n        for patch in patches:\n            # check that the patch is not visible outside the legend bbox\n            assert not patch.get_bbox().intersects(legend_bbox)\n\n        \n", "    def test_bbox_to_anchor(self):\n        with rc_context({'legend.bbox_to_anchor': (0.1, 0.1)}):\n            fig, ax = plt.subplots()\n            ax.plot([1, 2, 3, 4], [5, 6, 7, 8], label='Line 1')\n            ax.legend()\n            plt.show()\n\n\n\n", "    def test_draggable_legend(self):\n        fig, ax = plt.subplots()\n        line, = ax.plot([1, 2, 3])\n        legend = ax.legend([line], ['My Line'])\n        legend.set_draggable(True)\n        # Use mock to get the initial location of the legend\n        with mock.patch.object(legend, '_get_anchored_bbox') as mock_get_bbox:\n            mock_get_bbox.return_value = (0.1, 0.1, 0.5, 0.2)\n            legend.draw_idle()\n            # Check if the legend is draggable by moving it manually\n            # using the mouse\n            \n            # TODO: Implement a way to interact with the figure and \n            #       check if the legend location has changed. \n            #       This might involve using a separate process or \n            #       library to simulate mouse interaction.\n\n\n", "    def test_bbox_to_anchor_2(self):\n        fig, ax = plt.subplots()\n        ax.set_xlim(-1, 1)\n        ax.set_ylim(-1, 1)\n        ax.plot([0, 1], [0, 1], label='line')\n        ax.add_patch(plt.Rectangle((-0.2, 0.2), 0.4, 0.3,\n                                    label='rectangle'))\n        \n        legend = ax.legend(bbox_to_anchor=(0.1, 0.1), loc='lower left')\n        fig.canvas.draw()\n        "], "sample_561": ["    def test_marker_path(self):\n        fig, ax = plt.subplots()\n        ax.plot([0, 1], [0, 1], marker='o', linestyle='')  \n        path = markers.MarkerStyle('o').get_path()\n        expected_path = Path.unit_circle()\n        assert path.vertices.shape == expected_path.vertices.shape\n        assert np.allclose(path.vertices, expected_path.vertices)\n", "    def test_marker_transformation(self):\n        marker = markers.MarkerStyle(marker='o')\n        marker.transformed(Affine2D().scale(2, 2))\n        expected_transform = Affine2D().scale(2, 2)\n        assert check_figures_equal(marker.get_path(), marker.get_path(),\n                                  transform=expected_transform)\n\n        marker = markers.MarkerStyle(marker='s')\n        marker.rotated(deg=45)\n        expected_transform = Affine2D().rotate_deg(45)\n        assert check_figures_equal(marker.get_path(), marker.get_path(),\n                                  transform=expected_transform)\n\n\n\n        \n", "    def test_marker_transform(self):\n        marker = markers.MarkerStyle()\n        marker.set_marker(markers.PathMarkerStyle(Path.unit_circle(), markersize=10))\n        marker.transformed(Affine2D().scale(2, 2))\n        marker._recache()\n        expected_transform = Affine2D().scale(2, 2)\n        expected_path = marker._path\n        expected_alt_path = marker._alt_path\n        assert (marker.get_transform() == expected_transform)\n        assert (marker._path == expected_path)\n        assert (marker._alt_path == expected_alt_path)\n", "    def test_marker_path(self, image_comparison):\n        for marker in ['^', 'v', '<', '>']:\n            marker_instance = markers.MarkerStyle(marker)\n            plt.figure()\n            ax = plt.gca()\n            ax.plot([0, 1], [0, 1], marker=marker_instance)\n            plt.ylim([-1, 2])\n            plt.xlim([-1, 2])\n            image_comparison(ax.figure)\n", "    def test_marker_path_rotated(self):\n        marker = markers.MarkerStyle()\n        marker.set_marker('o')\n        marker.rotated(deg=45)\n        fig, ax = plt.subplots()\n        ax.plot([0], [0], marker=marker)\n        plt.show()\n\n"], "sample_911": ["    def check_template_ref(name, input, idDict):\n        ast = parse(name, input)\n\n        rootSymbol = Symbol(None, None, None, None, None, None)\n        symbol = rootSymbol.add_declaration(ast, docname=\"TestDoc\")\n        parentNode = addnodes.desc()\n        signode = addnodes.desc_signature(input, '')\n        parentNode += signode\n        ast.describe_signature(signode, 'lastIsName', symbol, options={})\n\n        idExpected = [None]\n        for i in range(1, _max_id + 1):\n            if i in idDict:\n                idExpected.append(idDict[i])\n            else:\n                idExpected.append(idExpected[i - 1])\n\n        idActual = [None]\n        for i in range(1, _max_id + 1):\n            try:\n                id = ast.get_id(version=i)\n                assert id is not None\n                idActual.append(id[len(_id_prefix[i]):])\n            except NoOldIdError:\n                idActual.append(None)\n\n        res = [True]\n        for i in range(1, _max_id + 1):\n            res.append(idExpected[i] == idActual[i])\n\n        if not all(res):\n            print(\"input:    %s\" % input.rjust(20))\n            for i in range(1, _max_id + 1):\n                if res[i]:\n                    continue\n                print(\"Error in id version %d.\" % i)\n                print(\"result:   %s\" % idActual[i])\n                print(\"expected: %s\" % idExpected[i])\n            print(rootSymbol.dump(0))\n            raise DefinitionError(\"\")\n", "    def test_cpp_domain_parent_resolution(capsys):\n        app = _create_app()\n        cppDomain.setup(app)\n        env = app.builder.env\n\n        # Mock a parent symbol with a docname\n        parent_symbol = Symbol(\"parent\", \"Parent\", \"Parent\", \"ParentDoc\", \"Parent\")\n        env.data['cpp:parent_symbol'] = parent_symbol\n\n        # Test a simple resolution\n        # This one is just testing if we can find the parent symbol.\n        ast = parse('class', 'class MyClass {}')\n        symbol = ast.resolve_symbol_from_parent(parent_symbol)\n        assert symbol.docname == \"ParentDoc\"\n        assert str(symbol.nested_name) == 'MyClass'\n\n        # Test nested symbol resolution\n        ast = parse('namespace', 'namespace MyNamespace {}')\n        symbol = ast.resolve_symbol_from_parent(parent_symbol)\n        assert symbol.docname == \"ParentDoc\"\n        assert str(symbol.nested_name) == 'MyNamespace'\n\n        # Test a function resolution\n        ast = parse('function', 'void MyClass::myFunction() {}')\n        symbol = ast.resolve_symbol_from_parent(parent_symbol)\n        assert symbol.docname == \"ParentDoc\"\n        assert str(symbol.nested_name) == 'MyClass::myFunction'\n\n\n\n        # Print captured output for debugging\n        captured = capsys.readouterr()\n        print(f'Captured output:\\n{captured.out}')\n", "compilation error", "def test_enum_scoped_within_namespace():\n    check(\n        \"enum\",\n        \"enum E { X = 10; }\",\n        {\"1\": \"E.X\"},\n        \"'\\nstruct E\\n{\\n  int X = 10;\\n}\\n\"\n    )\n", "compilation error"], "sample_1165": ["    def test_from_axis_angle_complex(self):\n        q = Quaternion.from_axis_angle((1, 2, 3), pi/2)\n        assert q.a == 0\n        assert q.b == 1*sqrt(2)\n        assert q.c == 2*sqrt(2)\n        assert q.d == 3*sqrt(2)\n", "    def test_from_axis_angle_complex():\n        q = Quaternion.from_axis_angle((sqrt(3)/3, sqrt(3)/3, sqrt(3)/3), 2*pi/3)\n        assert q.a == -0.5 + 0.8660254037844386*I\n        assert q.b == 0.5 + 0.4330127018922193*I\n        assert q.c == 0.5 + 0.4330127018922193*I\n        assert q.d == 0.5 + 0.4330127018922193*I\n\n        q = Quaternion.from_axis_angle((sqrt(3)/3, sqrt(3)/3, sqrt(3)/3), 2*pi/3 + pi)\n        assert q.a == -0.5 - 0.8660254037844386*I\n        assert q.b == -0.5 - 0.4330127018922193*I\n        assert q.c == -0.5 - 0.4330127018922193*I\n        assert q.d == -0.5 - 0.4330127018922193*I\n", "    def test_from_axis_angle_special_cases(self):\n        q1 = Quaternion.from_axis_angle((1, 0, 0), 0)\n        self.assertEqual(q1, Quaternion(1, 0, 0, 0))\n        q2 = Quaternion.from_axis_angle((0, 1, 0), pi)\n        self.assertEqual(q2, Quaternion(0, 0, 0, -1))\n        q3 = Quaternion.from_axis_angle((0, 0, 1), 2*pi)\n        self.assertEqual(q3, Quaternion(1, 0, 0, 0))\n", "    def test_to_axis_angle_special_cases(self):\n        q1 = Quaternion(1, 0, 0, 0)\n        axis, angle = q1.to_axis_angle()\n        self.assertEqual(axis, (0, 0, 1))\n        self.assertEqual(angle, 0)\n\n        q2 = Quaternion(0, 1, 0, 0)\n        axis, angle = q2.to_axis_angle()\n        self.assertEqual(axis, (1, 0, 0))\n        self.assertEqual(angle, pi/2)\n\n        q3 = Quaternion(0, 0, 1, 0)\n        axis, angle = q3.to_axis_angle()\n        self.assertEqual(axis, (0, 1, 0))\n        self.assertEqual(angle, pi/2)\n\n        q4 = Quaternion(0, 0, 0, 1)\n        axis, angle = q4.to_axis_angle()\n        self.assertEqual(axis, (0, 0, 0))\n        self.assertEqual(angle, pi)\n", "    def test_from_rotation_matrix_complex(self):\n        M = Matrix([[cos(phi) - 2*I*sin(phi), 2*I*sin(phi) + cos(phi), 0],\n                    [-2*I*sin(phi) + cos(phi), cos(phi) + 2*I*sin(phi), 0],\n                    [0, 0, 1]])\n        q = Quaternion.from_rotation_matrix(M)\n        assert q.real_field == False\n        assert q.a == cos(phi)\n        assert q.b == -2*I*sin(phi)\n        assert q.c == 2*I*sin(phi)\n        assert q.d == 0\n"], "sample_478": ["    def test_field_not_in_model(self):\n        class MyAdmin(ModelAdmin):\n            model = Band\n            fields = [\"name\", \"extra_field\"]\n\n        self.assertIsInvalid(MyAdmin, Band, \"The value of 'fields' refers to 'extra_field', which is not a field of 'Band'.\", id=\"admin.E1\")\n", "compilation error", "        def test_raw_id_fields_in_list_filter(self):\n            class MyRawIdFilter(SimpleListFilter):\n                title = \"My Raw ID Filter\"\n                parameter_name = \"my_raw_id\"\n\n                    return [\n                        (\"1\", \"Value 1\"),\n                        (\"2\", \"Value 2\"),\n                    ]\n\n                    for lookup, label in self.lookups(None, None):\n                        yield (lookup, label)\n\n                    lookup_value = self.value()\n                    if lookup_value:\n                        return queryset.filter(id__in=[lookup_value])\n                    return queryset\n\n            class MyModelAdmin(ModelAdmin):\n                list_filter = [MyRawIdFilter]\n\n            class MyModel(Model):\n                id = models.AutoField(primary_key=True)\n\n            self.assertIsInvalid(\n                MyModelAdmin(MyModel, AdminSite()),\n                MyModel,\n                r\"Raw ID Fields are not supported in ListFilters\",\n                id=\"admin.E207\",\n            )\n\n\n\n", "    def test_ordering_with_invalid_field(self):\n        class InvalidOrderingModelAdmin(ModelAdmin):\n            list_display = ('id', 'name')\n            ordering = ('raw_id',)\n\n        self.assertIsInvalid(\n            InvalidOrderingModelAdmin,\n            ValidationTestModel,\n            \"The value of 'ordering' refers to 'raw_id', which is not a field of 'ValidationTestModel'.\",\n            id=\"admin.E033\",\n        )\n\n\n\n", "    def test_check_raw_id_fields(self):\n        class RawIdModelAdmin(ModelAdmin):\n            raw_id_fields = ['id']\n\n        self.assertIsInvalid(\n            RawIdModelAdmin, ValidationTestModel,\n            \"The value of 'raw_id_fields' must be a list of \"\n            \"field names, but it is ['id']\",\n            id=\"admin.E207\",\n            invalid_obj=RawIdModelAdmin,\n        )\n\n\n"], "sample_896": ["    def test_minibatch_n_components_auto(\n        self, Estimator, solver", "    def test_random_initialization_with_batch_size_greater_than_dataset_size(\n        self, Estimator, solver", "    def test_mini_batch_n_components_auto(self, Estimator, solver):\n        if isinstance(Estimator, NMF):\n            # NMF 'auto' for n_components doesn't make sense\n            return\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        n_features = X.shape[1]\n        with ignore_warnings(category=ConvergenceWarning):\n            model = Estimator(n_components=\"auto\", **solver)\n            model.fit(X)\n        assert_array_equal(model.n_components_, n_features)\n\n\n", "    def test_custom_initialization(self, Estimator, solver):\n        X = np.random.rand(100, 50)\n        n_components = 10\n\n        # Custom initialization\n        W_init = np.random.rand(100, n_components)\n        H_init = np.random.rand(n_components, 50)\n        model = Estimator(n_components=n_components, init={\"W\": W_init, \"H\": H_init})\n\n        model.fit(X)\n\n        # Check that the initial values were used\n        assert_array_almost_equal(W_init, model.components_.T)\n        assert_array_almost_equal(H_init, model.components_)\n\n        # Test with partial_fit and custom initialization\n        partial_model = clone(model)\n        partial_model.partial_fit(X[:50])\n        assert_array_almost_equal(\n            partial_model.components_.T,\n            model.components_.T,\n            atol=1e-4,\n        )\n", "    def test_partial_fit_sequential_batches(self, Estimator, solver):\n        n_samples = 100\n        n_features = 50\n        n_components = 10\n        random_state = 42\n\n        X = np.random.rand(n_samples, n_features)\n        X_batch_size = 20\n\n        # Split X into batches for partial fitting\n        batches = [\n            X[i : i + X_batch_size] for i in range(0, n_samples, X_batch_size)\n        ]\n\n        # Initialize the model\n        model = Estimator(n_components=n_components, random_state=random_state, **solver)\n\n        # Fit in batches sequentially\n        for batch in batches:\n            with ignore_warnings(category=ConvergenceWarning):\n                model.partial_fit(batch)\n\n        # Check if the model converged\n        with ignore_warnings(category=ConvergenceWarning):\n            W = model.transform(X)\n\n        # Compare with fitting the whole dataset at once\n        model_full = clone(model)\n        model_full.fit(X)\n        W_full = model_full.transform(X)\n\n        # Assert that the results are similar\n        assert_array_almost_equal(W, W_full, atol=1e-4)\n\n"], "sample_776": ["    def test_lars_path_residues(self, criterion):\n        # Test the _lars_path_residues function which is used internally\n        # in LassoLarsIC.\n\n        X = np.random.rand(100, 10)\n        y = np.random.rand(100)\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n        alphas, active, coefs, residues = _lars_path_residues(\n            X_train, y_train, X_test, y_test, Gram='auto', copy=True,\n            method='lasso', verbose=0, fit_intercept=True,\n            max_iter=500, eps=np.finfo(np.float).eps, positive=False\n        )\n\n\n        # Assert that alphas is not empty\n        assert len(alphas) > 0\n\n        # Assert that residues has the same shape as the number of alphas\n\n        assert residues.shape[0] == len(alphas)\n\n        # Assert that residuals are float values\n        assert residues.dtype == np.float64\n\n        # Test that residuals are calculated correctly\n        # By comparing them to the residuals directly calculated from the\n        # model fit on the whole dataset\n        model = LassoLarsIC(criterion=criterion)\n        model.fit(X, y)\n        y_pred = model.predict(X_test)\n        resids_true = y_test - y_pred\n        assert_array_almost_equal(resids_true, residues, decimal=6)\n\n\n\n", "    def test_lasso_lars_ic_bic(self):\n        # Test LassoLarsIC with BIC criterion\n        with ignore_warnings(category=ConvergenceWarning):\n            reg = LassoLarsIC(criterion='bic')\n            reg.fit(X, y)\n        assert reg.criterion_[0] > 1e-5\n        assert reg.alpha_ > 0.0\n", "    def test_lars_ic_criterion(self, criterion):\n        # test that criterion is correctly applied\n        reg = LassoLarsIC(criterion=criterion)\n        reg.fit(X, y)\n        assert reg.criterion_.shape == (len(reg.alphas_),)\n        if criterion == 'aic':\n            assert (reg.criterion_ < 0).all()\n        elif criterion == 'bic':\n            assert (reg.criterion_ < 0).all()\n", "    def test_lars_path(self):\n        X = np.array([\n            [1, 2],\n            [3, 4],\n            [5, 6],\n            [7, 8],\n        ])\n        y = np.array([1, 2, 3, 4])\n        _, alphas, _, _ = _lars_path_residues(\n            X, y, X[:2], y[:2], method='lar', max_iter=10,\n            normalize=True, fit_intercept=False)\n        assert_array_almost_equal(alphas, np.array([1.0, 0.5]))\n", "def test_lasso_lars_ic_criterion():\n\n    reg = LassoLarsIC(criterion='bic')\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n\n    reg.fit(X_train, y_train)\n\n    with ignore_warnings(category=ConvergenceWarning):\n        # Check that the criterion is the BIC\n        assert reg.criterion_ is not None\n        assert reg.criterion_.shape == reg.alphas_.shape\n\n    # Check that the model coefficients are not all zeros\n    assert np.any(reg.coef_ != 0)\n"], "sample_484": ["    def test_right_with_length(self):\n        john_smith_name = Author.objects.get(name=\"John Smith\").name\n        right_three = Right(Value(john_smith_name), Length(Value(john_smith_name)) * Value(3)).as_sqlite()\n        self.assertEqual(right_three, \"ith\")\n", "    def test_right(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\"CREATE TEMP TABLE test_right (name VARCHAR(255))\")\n            cursor.execute(\n                \"INSERT INTO test_right (name) VALUES ('John Smith'), ('Rhonda')\"\n            )\n\n        results = Author.objects.values_list(\"name\", \"alias\").annotate(\n            right_three=Right(\"name\", 3)\n        ).order_by(\"name\")\n        self.assertEqual(\n            list(results),\n            [\n                (\"John Smith\", \"smithj\"),\n                (\"Rhonda\", None),\n            ],\n        )\n\n\n\n", "    def test_right_with_length(self):\n        john_smith_right = Author.objects.filter(name=\"John Smith\").values(\n            Right(\"name\", 5)\n        ).get()\n        self.assertEqual(john_smith_right[\"right__0\"], \"Smith\")\n\n\n", "    def test_right_trim(self):\n        rhonda = Author.objects.get(name=\"Rhonda\")\n        trimmed_name = rhonda.name.annotate(\n            right_trimmed=Right(\n                Value(\"   \"), Length(Value(\"   \"))\n            )\n        ).values(\"right_trimmed\")[0][\"right_trimmed\"]\n        self.assertEqual(trimmed_name, \"Rhonda\")\n\n\n\n", "    def test_right(self):\n        john_smith = Author.objects.get(name=\"John Smith\")\n        right_three = Right(Value(john_smith.name), Value(3))\n        result = right_three.resolve_expression(connection)\n        self.assertEqual(result, \"mith\")\n"], "sample_187": ["    def test_smart_split(self):\n        for text, expected in [\n            ('This is a test.', ['This', 'is', 'a', 'test.']),\n            ('\"This is \"a test.\"', ['\"This is \"a test.\"']),\n            (\"'This is 'a test.'\", [\"'This is 'a test'.\"]),\n            ('This \"is a test\" with \\'quotes\\' and &amp;amp; entities', ['This', '\"is a test\"', 'with', \"'quotes'\", '&amp;amp;']),\n            ('A \"funky\\\" style\" test.', ['A', '\"\\\\\"funky\\\\\" style\"', 'test.']),\n            ('Another \"person\\'s\" test.', ['Another', '\"person\\\\\\'s\"', 'test.']),\n            ('A \" \\' \\\" \\\" \\' \" test', ['A', '\" \\' \\\" \\\" \\' \"', 'test']),\n        ]:\n            self.assertEqual(list(text.smart_split()), expected)\n", "    def test_slugify_unicode(self):\n        with override(gettext=lambda x: x):\n            self.assertEqual(text.slugify(\"\u00dcnic\u00f6d\u00e9 testing\"), \"unicodedata-testing\")\n            self.assertEqual(text.slugify(\"\\U0001F4A9\"), \"U0001F4A9\")\n            self.assertEqual(text.slugify(\"\u4f60\u597d\u4e16\u754c\"), \"nihao-shijie\")\n            self.assertEqual(text.slugify(\"\u4f60\u597d\u4e16\u754c\", allow_unicode=False), \"nihao-shijie\") \n            self.assertEqual(text.slugify(\"\\U0001F4A9\", allow_unicode=False), \"U0001F4A9\")\n            if IS_WIDE_BUILD:\n                self.assertEqual(text.slugify(\"\ud841\udf0e\"), \"\ud841\udf0e\")\n\n\n\n", "    def test_smart_split(self):\n        self.assertEqual(list(text.smart_split(r'This is \"a person\\'s\" test.')), ['This', 'is', '\"a person\\\\\\'s\"', 'test.'])\n        self.assertEqual(list(text.smart_split(r\"Another 'person\\'s' test.\")), ['Another', \"'person\\\\'s'\", 'test.'])\n        self.assertEqual(list(text.smart_split(r'A \"\\\"funky\\\" style\" test.')), ['A', '\"\\\\\"funky\\\\\" style\"', 'test.'])\n        self.assertEqual(list(text.smart_split(r'This \"is\" a \"test\"')), ['This', '\"is\"', 'a', '\"test\"'])\n        self.assertEqual(list(text.smart_split(r\"'This 'is' a 'test'\")), ['\\'This ', \"'is'\", 'a', \"'test'\"])\n        self.assertEqual(list(text.smart_split(r\"This 'is' a \\\"test\\\"\"')), ['This', \"'is'\", 'a', '\\\"test\\\"\"])\n        self.assertEqual(list(text.smart_split(r\"\"\")), [])\n        self.assertEqual(list(text.smart_split(r\" \")), [])\n        self.assertEqual(list(text.smart_split(r\"   \")), [])\n\n\n\n\n", "    def test_capfirst_preserve_lazy(self):\n        self.assertEqual(text.capfirst(lazystr('hello world')), 'Hello world')\n        self.assertEqual(text.capfirst(lazystr('HELLO WORLD')), 'Hello world')\n        self.assertEqual(text.capfirst(lazystr('Foo Bar')), 'Foo Bar')\n        self.assertEqual(text.capfirst(lazystr('')), '')\n", "    def test_capfirst_with_none(self):\n        self.assertIsNone(text.capfirst(None))\n\n"], "sample_367": ["    def test_cache_page_with_cache_prefix(self):\n        key_prefix = 'test_prefix'\n        decorated_function = cache_page(timeout=60, cache=None, key_prefix=key_prefix)(fully_decorated)\n\n        # Mock a request object\n        request = HttpRequest()\n\n        # Check if cache key uses the provided prefix\n        with self.subTest(\"Cache key does not include prefix\"):\n            with mock.patch('django.contrib.sessions.middleware.SessionMiddleware.process_request') as mock_process_request:\n                response = decorated_function(request)\n                self.assertIn(key_prefix, response.cache_key)\n", "    def test_cache_control(self):\n        @cache_control(public=True)\n            return HttpResponse(\"Hello world!\")\n\n        response = view(HttpRequest())\n        self.assertEqual(response.get('Cache-Control'), 'public')\n", "    def test_cache_page(self):\n        with mock.patch('django.middleware.cache.CacheMiddleware.process_request') as mock_process_request:\n            request = HttpRequest()\n            response = fully_decorated(request)\n            self.assertEqual(mock_process_request.call_count, 1)\n            patch_cache_control_args = mock_process_request.call_args[1]['kwargs']\n            self.assertEqual(patch_cache_control_args.get('private'), True)\n            self.assertEqual(patch_cache_control_args.get('max_age'), 900)\n            self.assertEqual(response.getvalue(), '<html><body>dummy</body></html>')\n", "    def test_full_decorator(self):\n        response = fully_decorated(HttpRequest())\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.content.decode('utf-8'), '<html><body>dummy</body></html>')\n\n        # Check if the cache decorator appends the cache headers \n        self.assertIn('Cache-Control', response['Cache-Control'])\n        self.assertIn('Pragma', response['Pragma'])\n", "    def test_cache_page_applied(self):\n        response = fully_decorated(HttpRequest())\n        self.assertEqual(response.status_code, 200)\n        self.assertIn('Cache-Control', response)\n        self.assertIn('max-age', response['Cache-Control'])\n        self.assertIn('Pragma', response)\n        self.assertEqual(response['Pragma'], 'no-cache')\n        self.assertIn('Expires', response)\n\n\n\n"], "sample_539": ["    def test_lasso(kwargs):\n        fig, ax = plt.subplots()\n        lasso = Lasso(ax, (0.2, 0.2), lambda verts: assert_allclose(verts, np.array([[0.2, 0.2], [0.8, 0.4], [0.5, 0.8], [0.2, 0.2]])), **kwargs)\n        click_and_drag(ax, (0.8, 0.4), (0.5, 0.8))\n        plt.close(fig)\n", "    def test_rect_selector_with_custom_props(kwargs):\n        fig, ax = plt.subplots()\n        selection = RectangleSelector(ax, lambda *args: None, **kwargs)\n        selection.set_visible(True)\n        rect = Rectangle((0, 0), 1, 1, **kwargs.get('props', {}))\n        ax.add_patch(rect)\n\n        assert isinstance(selection._selection_artist, Line2D)\n        assert isinstance(selection._selection_artist.get_patch(), Rectangle)\n        assert selection._bbox_artist == rect\n        assert check_figures_equal(fig, [rect, selection._selection_artist])\n\n        selection.set_visible(False)\n        assert not selection._selection_artist.get_visible()\n\n        plt.close(fig)\n\n", "    def test_spanselector_corner_cases(self, ax, kwargs):\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n        s = widgets.SpanSelector(ax, noop, minspanx=10, minspany=10,\n                                 spancoords='pixels', **kwargs)\n        x_data = np.linspace(0, 10, 100)\n        y_data = np.sin(x_data)\n        line, = ax.plot(x_data, y_data)\n        \n        # Test selecting a span with only one point\n        click_and_drag(ax, (0, 0), (0, 0), button=1)\n        s.onselect(x1, y1, x2, y2)\n        assert len(s.verts) == 1\n        \n        # Test selecting a span with vertices outside plot limits\n        click_and_drag(ax, (15, 0), (15, 10), button=1)\n        s.onselect(x1, y1, x2, y2)\n        assert len(s.verts) == 2\n        \n        # Test selecting a span with no valid span\n        click_and_drag(ax, (0, 0), (1, 1), button=1)\n        s.onselect(x1, y1, x2, y2)\n        assert len(s.verts) == 0\n", "    def test_spanselector_with_kwargs(ax, kwargs):\n        s = widgets.SpanSelector(ax,\n                                 lambda event: None,\n                                 drawtype='line', **kwargs)\n        s.set_visible(True)\n\n", "    def test_spanselector_with_kwargs(ax, kwargs):\n        ss = widgets.SpanSelector(ax, lambda start, stop: None,\n                                  **kwargs)\n        click_and_drag(ax, 10, 10, 50, 50)\n        assert ss.onselect_called\n\n\n\n"], "sample_81": ["    def test_route_pattern_match_with_converters(self):\n        pattern = RoutePattern('^/users/<int:pk>/', name='user')\n        match = pattern.match('/users/123')\n        self.assertIsNotNone(match)\n        self.assertEqual(match.args, ())\n        self.assertEqual(match.kwargs, {'pk': 123})\n", "    def test_route_to_regex_handles_default_values(self):\n        route = 'blog/<int:pk>/<str:name>/'\n        pattern, converters = _route_to_regex(route)\n        self.assertEqual(pattern, r'^blog/(?P<pk>\\d+)/(?P<name>.*)/$')\n        self.assertEqual(converters, {'pk': <class 'int'>, 'name': <class 'str'>})\n\n\n\n", "    def test_route_to_regex(self):\n        route = 'foo/<int:pk>/bar/<str:name>'\n        pattern, converters = _route_to_regex(route)\n        self.assertEqual(\n            pattern,\n            r'^foo/\\d+/bar/(?P<name>.*)$',\n        )\n        self.assertEqual(converters, {'pk': <class 'int'>, 'name': <class 'str'>})\n", "    def test_route_pattern_with_default_args(self):\n        pattern = RoutePattern('^hello/<int:pk>/', name='example')\n        url = '/hello/123/'\n        match = pattern.match(url)\n        self.assertIsNotNone(match)\n        self.assertEqual(match.args, ())\n        self.assertEqual(match.kwargs, {'pk': '123'})\n        \n        # Test default arguments\n        url = '/hello/'\n        match = pattern.match(url)\n        self.assertIsNotNone(match)\n        self.assertEqual(match.args, ())\n        self.assertEqual(match.kwargs, {'pk': '0'})\n\n\n", "    def test_route_pattern_name(self):\n        pattern = RoutePattern('my_app/blog/<int:pk>/', name='blog_detail')\n        self.assertEqual(pattern.name, 'blog_detail')\n        pattern = RoutePattern('my_app/blog/<int:pk>/', name='blog_detail:view')\n        self.assertEqual(pattern.name, 'blog_detail:view')\n        with self.assertRaises(ImproperlyConfigured):\n            RoutePattern('my_app/blog/<int:pk>/', name='blog_detail:')\n"], "sample_281": ["    def test_autocomplete_permission_denied(self):\n        with self.settings(INSTALLED_APPS={'Permission': False}):\n            request = self.factory.get(self.url,\n                                       {'app_label': self.opts['app_label'],\n                                        'model_name': self.opts['model_name'],\n                                        'field_name': self.opts['field_name'],\n                                        'term': 'test'})\n            response = AutocompleteJsonView.as_view()(request)\n            self.assertEqual(response.status_code, 403)\n\n        with self.assertRaises(PermissionDenied):\n            with self.settings(INSTALLED_APPS={'Permission': False}):\n                content_type = ContentType.objects.get_for_model(Answer)\n                permission = Permission.objects.create(\n                    content_type=content_type,\n                    codename='view_answer',\n                )\n                self.user.user_permissions.add(permission)\n                request = self.factory.get(self.url,\n                                           {'app_label': self.opts['app_label'],\n                                            'model_name': self.opts['model_name'],\n                                            'field_name': self.opts['field_name'],\n                                            'term': 'test'})\n                AutocompleteJsonView.as_view()(request)\n", "    def test_permission_denied(self):\n        request = self.factory.get(self.url, data={'app_label': 'books',\n                                                   'model_name': 'book',\n                                                   'field_name': 'authors'})\n        request.user = User.objects.create_user(\n            username='notstaff', password='secret',\n            email='user@example.com', is_staff=False,\n        )\n        with self.assertRaises(PermissionDenied):\n            AutocompleteJsonView.as_view()(request, *([],))\n", "    def test_permission_denied_missing_app_label(self):\n        request = self.factory.get(self.url, {'term': 'test_term'})\n        response = self.view.as_view()()(request)\n        self.assertEqual(response.status_code, 403)\n        self.assertEqual(json.loads(response.content.decode())['error'], \"Invalid request parameters\")  \n", "    def test_has_perm(self):\n        request = self.factory.get(self.url, data={'app_label': Answer._meta.app_label, 'model_name': Answer._meta.model_name, 'field_name': 'question', 'term': 'test'})\n        request.user = self.user\n\n        with self.model_admin(Answer, AnswerAdmin, site=self.site):\n            with self.assertRaises(PermissionDenied):\n                AutocompleteJsonView(admin_site=self.site).has_perm(request)\n\n        permission = Permission.objects.get(\n            codename='view_answer',\n            content_type=ContentType.objects.get_for_model(Answer),\n        )\n        self.user.user_permissions.add(permission)\n\n        with self.model_admin(Answer, AnswerAdmin, site=self.site):\n            self.assertTrue(AutocompleteJsonView(admin_site=self.site).has_perm(request))\n", "    def test_no_perm(self):\n        request = self.factory.get(self.url, params={\n            'app_label': self.opts['app_label'],\n            'model_name': self.opts['model_name'],\n            'field_name': self.opts['field_name'],\n            'term': 'test',\n        })\n        request.user = self.user\n        permission = Permission.objects.get(\n            content_type__app_label=Answer._meta.app_label,\n            codename='view_answer'\n        )\n        self.user.user_permissions.remove(permission)\n        view = AutocompleteJsonView.as_view(**self.as_view_args)\n        with self.assertRaises(PermissionDenied):\n            response = view(request, *args, **kwargs)\n\n\n\n"], "sample_234": ["    def test_prefetch_related_complex(self):\n        # Test a scenario with several levels of prefetch_related.\n        nums = Number.objects.all()\n        nums = nums.prefetch_related(\n            'reservednames',\n            queryset=ReservedName.objects.filter(num__gt=5),\n            to_attr='related_numbers',\n        )\n        self.assertNumbersEqual(\n            nums, [n for n in Number.objects.all() if n.num > 5], ordered=False\n        )\n\n", "    def test_union_with_extra(self):\n        query1 = Number.objects.filter(num__gt=5).values('num').annotate(extra=Value(1))\n        query2 = Number.objects.filter(num__lt=5).values('num').annotate(extra=Value(2))\n        union_queryset = query1.union(query2)\n        self.assertNumbersEqual(union_queryset, list(range(10)), ordered=False)\n\n\n\n", "    def test_union_with_order_by(self):\n        # Test UNION with an ORDER BY clause, ensuring it correctly respects the order\n        ordered_numbers = Number.objects.order_by('num').values('num')\n        unordered_numbers = Number.objects.values('num')\n        union_queryset = ordered_numbers.union(unordered_numbers)\n        self.assertNumbersEqual(union_queryset, range(10), ordered=True)\n\n\n", "    def test_query_set_union_with_distinct(self):\n        numbers_union_distinct = Number.objects.filter(num__gt=5).union(\n            Number.objects.filter(num__lt=5), distinct=True\n        )\n        self.assertNumbersEqual(numbers_union_distinct, [\n            Number(num=6, other_num=4),\n            Number(num=7, other_num=3),\n            Number(num=8, other_num=2),\n            Number(num=9, other_num=1),\n            Number(num=0, other_num=10),\n            Number(num=1, other_num=9),\n            Number(num=2, other_num=8),\n            Number(num=3, other_num=7),\n            Number(num=4, other_num=6)\n        ]) \n\n", "    def test_select_related_chained(self):\n        n1 = Number.objects.select_related('reserved_names').get(num=0)\n        self.assertIsNotNone(n1.reserved_names)\n        self.assertEqual(n1.reserved_names.name, 'A')\n\n        n2 = Number.objects.select_related('reserved_names').get(num=5)\n        self.assertIsNotNone(n2.reserved_names)\n        self.assertEqual(n2.reserved_names.name, 'F') \n\n"], "sample_190": ["    def test_in_lookup_with_queryset(self):\n        # The test case should cover the following scenarios:\n        # 1. Empty QuerySet: If the QuerySet is empty, \n        # should raise an EmptyResultSet.\n        # 2. Non-empty QuerySet: Should retrieve all objects that match any element\n        # in the QuerySet.\n        # 3. Subquery with Select Clause: Test that adding a select clause to\n        # the inner query works correctly.\n\n        articles = Article.objects.filter(author=self.au1).in_(\n            Article.objects.filter(author=self.au1)[:2]\n        )\n        self.assertEqual(len(articles), 2)\n        self.assertIn(self.a1, articles)\n        self.assertIn(self.a2, articles)\n\n        articles = Article.objects.filter(author=self.au1).in_(\n            Article.objects.filter(author=self.au2)\n        )\n        self.assertEqual(len(articles), 0)\n\n        articles = Article.objects.filter(author=self.au1).in_(\n            Article.objects.filter(\n                title__startswith='Article'\n            )[:2]\n        )\n        self.assertEqual(len(articles), 2)\n        self.assertIn(self.a1, articles)\n        self.assertIn(self.a2, articles)\n\n\n\n", "    def test_range_lookup(self):\n        with isolate_apps(apps=['testapp']):\n            qs = Article.objects.filter(pub_date__range=[\n                datetime(2005, 7, 26), datetime(2005, 7, 28)\n            ])\n            self.assertQuerysetEqual(qs, ['<QuerySet [<Article: Article 1>, <Article: Article 2>, <Article: Article 3>, <Article: Article 4>]'], ordered=False)\n", "    def test_range_lookup(self):\n        qs = Article.objects.filter(pub_date__range=(datetime(2005, 7, 26, 10), datetime(2005, 8, 1)))\n        self.assertEqual(qs.count(), 4)\n\n        qs = Article.objects.filter(pub_date__range=(None, datetime(2005, 7, 27)))\n        self.assertEqual(qs.count(), 3)\n\n        qs = Article.objects.filter(pub_date__range=(datetime(2005, 8, 1), None))\n        self.assertEqual(qs.count(), 3)\n", "    def test_year_lookup_exact(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT id FROM django_content_type WHERE name = 'article'\")\n            content_type_id = cursor.fetchone()[0]\n        self.assertEqual(Article.objects.filter(pub_date__year=2005).count(), 4)\n\n        self.assertEqual(Article.objects.filter(pub_date__year=2006).count(), 0)\n\n\n\n", "    def test_year_lookup_exact(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\"ALTER TABLE django_article ALTER COLUMN pub_date TYPE YEAR;\")\n        exact_results = Article.objects.filter(\n            year__exact=2005\n        ).values_list('slug', flat=True)\n        self.assertQueryContainsSQL('SELECT slug FROM django_article WHERE EXTRACT(YEAR FROM pub_date) = %s', exact_results.query)\n        self.assertSetEqual(set(exact_results), {'a1', 'a2', 'a3', 'a4'})\n\n"], "sample_852": ["    def test_make_gaussian_quantiles_n_samples_too_small():\n        with pytest.raises(ValueError):\n            make_gaussian_quantiles(n_samples=2, n_classes=3)\n", "    def test_make_biclusters_shapes(self):\n        for shape in [(5, 5), (10, 10), (100, 10)]:\n            n_clusters = 3\n            X, _, _ = make_biclusters(shape, n_clusters, random_state=1)\n            assert X.shape == shape\n", "    def test_make_sparse_coded_signal(n_features):\n        X, _, _ = make_sparse_coded_signal(n_samples=100,\n                                           n_components=5,\n                                           n_features=n_features)\n        assert_array_equal(X.shape, (100, n_features))\n", "def test_make_sparse_uncorrelated():\n    X, y = make_sparse_uncorrelated(n_samples=100, n_features=10)\n    assert_all_finite(X)\n    assert_array_equal(X.shape, (100, 10))\n    assert_array_equal(y.shape, (100,))\n\n\n\n", "    def test_make_sparse_uncorrelated(random_state=42):\n        X, y = make_sparse_uncorrelated(100, random_state=random_state)\n        assert_array_equal(X.shape, (100, 10))\n        assert_all_finite(X)\n"], "sample_122": ["    def test_i18n_cache_key_suffix(self):\n        request = HttpRequest()\n        request.LANGUAGE_CODE = 'en'\n        response = HttpResponse()\n\n        # Normal cache key suffix is added for a language code.\n        self.assertEqual(\n            _i18n_cache_key_suffix(request, 'key'),\n            'key.en'\n        )\n\n        # If the request doesn't have a LANGUAGE_CODE,\n        # the default language gets used.\n        request.LANGUAGE_CODE = None\n        settings.LANGUAGE_CODE = 'de'\n        self.assertEqual(\n            _i18n_cache_key_suffix(request, 'key'),\n            'key.de'\n        )\n\n        # Time zone suffix is added when USE_TZ is True.\n        settings.USE_TZ = True\n        self.assertIn('key.%s' % get_current_timezone_name(),\n                      _i18n_cache_key_suffix(request, 'key'))\n\n\n", "    def test_patch_vary_headers_with_asterisk(self):\n        response = HttpResponse()\n        patch_vary_headers(response, ['*'])\n        self.assertEqual(response['Vary'], '*')\n", "    def test_patch_vary_headers(self):\n        response = HttpResponse()\n        patch_vary_headers(response, ['Accept-Language', 'User-Agent'])\n        self.assertEqual(response['Vary'], 'Accept-Language, User-Agent')\n\n        response = HttpResponse()\n        patch_vary_headers(response, ['Content-Type', '*'])\n        self.assertEqual(response['Vary'], '*')\n\n        response = HttpResponse()\n        patch_vary_headers(response, ['Content-Type'])\n        self.assertEqual(response['Vary'], 'Content-Type')\n\n\n\n", "    def test_cache_control_preserve_existing_headers(self):\n        response = HttpResponse('Hello')\n        response['Cache-Control'] = 'max-age=3600, public'\n        patch_cache_control(response, max_age=7200)\n        self.assertEqual(response['Cache-Control'], 'max-age=7200, public')\n\n", "    def test_patch_vary_headers(self):\n        response = HttpResponse()\n        patch_vary_headers(response, ['X-Foo', 'Accept-Language', 'X-Bar'])\n        self.assertEqual(response['Vary'], 'X-Foo, Accept-Language, X-Bar')\n\n        response = HttpResponse()\n        patch_vary_headers(response, ['*'])\n        self.assertEqual(response['Vary'], '*')\n\n\n\n"], "sample_1053": ["    def test_sympify_mpmath_complex():\n        x = mpmath.complex(1, 2)\n        assert sympify(x) == 1 + 2*I, \"Sympify of mpmath complex failed\"\n        assert sympify_mpmath(x) == 1 + 2*I, \"mpmath sympify failed\"\n\n\n\n   \n", "    def test_sympify_complex_non_sequence(self):\n        sympify(1 + 2j)\n", "    def test_sympify_negative_infinity():\n        assert sympify(mlib.fninf) == S.NegativeInfinity\n        assert sympify(-float('inf')) == S.NegativeInfinity\n", "    def test_infinity_arithmetic():\n        assert S.Infinity + S.Infinity == oo\n        assert S.Infinity - S.Infinity == 0\n        assert S.Infinity * S.Infinity == oo\n        assert S.Infinity / S.Infinity == 1\n        assert S.Infinity * S.NegativeInfinity == -oo\n        assert S.Infinity / S.NegativeInfinity == -1\n        assert  S.NegativeInfinity + S.Infinity == oo\n        assert  S.NegativeInfinity - S.Infinity == -oo\n        assert  S.NegativeInfinity * S.Infinity == -oo\n\n\n\n", "    def test_sympify_n_2():\n        for x in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n            assert sympify(x) == Integer(x)\n\n        for x in [-1, -2, -3, -4, -5, -6, -7, -8, -9, -10]:\n            assert sympify(x) == Integer(x)\n\n\n\n   \n"], "sample_716": ["    def test_ridge_cv_sparse(self):\n        n_samples = 10\n        n_features = 20\n        random_state = 42\n\n        X = sp.rand(n_samples, n_features, density=0.1)\n        y = np.random.randn(n_samples)\n\n        alphas = np.logspace(-2, 1, 10)\n\n        ridge_cv = RidgeCV(alphas=alphas, cv=5, scoring='neg_mean_squared_error')\n        ridge_cv.fit(X, y)\n\n        assert_greater(ridge_cv.alpha_, 0)\n        assert_array_equal(ridge_cv.cv_values_.shape, (n_samples, len(alphas)))\n", "    def test_ridge_gcv_svd(self):\n        X = np.random.rand(10, 50)\n        y = np.random.rand(10)\n        estimator = _RidgeGCV(alphas=(0.1, 1.0, 10.0),\n                              gcv_mode='svd')\n        estimator.fit(X, y)\n", "    def test_ridge_gcv_sparse(self):\n        #  Test with sparse matrices\n        X = sp.csr_matrix(np.random.randn(100, 50))\n        y = np.random.randn(100)\n\n        # Test with the RidgeGCV class directly\n        estimator = _RidgeGCV(alphas=(0.1, 1.0, 10.0),\n                              fit_intercept=True, normalize=False,\n                              gcv_mode='svd')\n        estimator.fit(X, y)\n        assert_equal(estimator.cv_values_.shape, (100, 3))\n        assert_greater(estimator.alpha_, 0)\n        assert_greater(estimator.coef_.nnz, 0)\n\n\n", "    def test_ridge_cross_validation(self):\n        # Test RidgeCV with multiple alphas and different scoring functions\n        n_samples, n_features = 100, 20\n        rng = check_random_state(0)\n        X = rng.randn(n_samples, n_features)\n        y = rng.randn(n_samples)\n        alphas = np.exp(np.linspace(-2, 2, 10))\n        cv = 5\n        scoring = 'neg_mean_squared_error'\n\n        ridge_cv = RidgeCV(alphas=alphas, cv=cv, scoring=scoring)\n        ridge_cv.fit(X, y)\n        best_alpha = ridge_cv.alpha_\n        best_estimator = ridge_cv.best_estimator_\n\n        # Make sure the best estimator is a Ridge instance\n        assert_isinstance(best_estimator, Ridge)\n\n        # Check that the best alpha is as expected\n        assert_greater(best_alpha, 0)\n\n        # Check that the cv_values_ attribute is correctly set\n        assert_array_shape(ridge_cv.cv_values_, (n_samples, len(alphas)))\n\n\n", "    def test_ridge_sparse_eigen(self):\n        X_sparse = sp.csr_matrix(np.random.randn(10, 5))\n        y = np.random.randn(10)\n        alphas = np.array([0.1, 1.0])\n        ridge = Ridge(alpha=1.0, solver='eigen')\n        ridge.fit(X_sparse, y)\n        assert_array_almost_equal(ridge.coef_, ridge.coef_)\n"], "sample_1173": ["    def test_implicit_multiplication_application_repeated_multiplication():\n        expr = \"x*y*z\"\n        result = parse_expr(expr, transformations=(implicit_multiplication_application,))\n        expected = Mul(x, y, z)\n        assert result == expected\n", "    def test_multiple_equals(self):\n        expr = parse_expr(\"(1=2)=False\")\n        self.assertEqual(expr, Eq(Eq(2, 1), False))\n", "    def test_limit(self):\n        from sympy.series import Limit\n        assert parse_expr('limit(x, x, 0)') == Limit(x, x, 0)\n        assert parse_expr('limit((x**2 - 1)/(x - 1), x, 1)') == Limit((x**2 - 1)/(x - 1), x, 1)\n        \n\n\n\n", "    def test_rationalize_with_float(self):\n        exp = parse_expr(\"1.2*3.4\")\n        self.assertEqual(rationalize(exp.args), [Float(1.2), Float(3.4)])\n", "    def test_rationalize_after_auto_number(self):\n        expr = parse_expr(\"1.23\", transformations=(standard_transformations + (auto_number,)))\n        assert isinstance(expr, Float)\n        expr = parse_expr(\"1.23\", transformations=(standard_transformations + (auto_number, rationalize)))\n        assert isinstance(expr, Rational)\n"], "sample_128": ["    def test_index_name_generation(self):\n        article = Article.objects.create(\n            title='Some Article',\n            content='Some content',\n            pub_date=timezone.now()\n        )\n        index = Index(\n            fields=['title', 'pub_date'],\n            model=Article,\n        )\n        index.set_name_with_model(Article)\n        self.assertIn('D', index.name)\n        self.assertNotIn('_', index.name)\n        self.assertNotIn(' ', index.name)\n        self.assertLessEqual(len(index.name), 30)\n\n\n\n", "    def test_index_name_with_model(self):\n        index = Index(fields=['title'])\n        index.set_name_with_model(Article)\n        self.assertTrue(index.name.startswith('Article_'), 'Index name should start with table name')\n        self.assertTrue(index.name.endswith(Index.suffix), 'Index name should end with suffix')\n        self.assertEqual(len(index.name), Index.max_name_length, 'Index name length should be within the limit')\n\n        # Test with a longer field name\n        index.fields = ['verylongfield']\n        index.set_name_with_model(IndexedArticle2)\n        self.assertTrue(index.name.startswith('IndexedArticle2_'), 'Index name should start with table name')\n        self.assertTrue(index.name.endswith(Index.suffix), 'Index name should end with suffix')\n        self.assertEqual(len(index.name), Index.max_name_length, 'Index name length should be within the limit')\n\n", "    def test_set_name_with_model(self):\n        index = Index(fields=['title', 'pub_date'], name=None)\n        index.set_name_with_model(Article)\n        self.assertEqual(index.name, 'a_art_1325e9c5_idx')\n\n", "    def test_index_set_name_with_model(self):\n        model = IndexedArticle2()\n        index = Index(fields=['title', 'pub_date'])\n        index.set_name_with_model(model)\n        self.assertIn('IndexedArticle2', index.name)\n        self.assertIn('title', index.name)\n        self.assertIn('pub_date', index.name)\n        self.assertIn(index.suffix, index.name)\n", "    def test_index_name_generation(self):\n        model = IndexedArticle2()\n        index = Index(fields=['name'], name=None)\n        index.set_name_with_model(model)\n        self.assertGreater(len(index.name), 0)\n        self.assertLess(len(index.name), 31)\n        # Check if the index name is unique across multiple tests\n        index2 = Index(fields=['name'], name=None)\n        index2.set_name_with_model(IndexedArticle2())\n        self.assertNotEqual(index.name, index2.name)\n\n\n\n"], "sample_4": ["    def test_latex_names_conflict(self, cosmo):\n        # Simulate a conflict where a table column uses the name of a parameter\n        # but with a different latex format\n        table = QTable(\n            data={\n                \"$$H_{0}$$\": [70.0],\n                \"H0\": [70.0],\n            }\n        )\n        with pytest.raises(ValueError):\n            read_html_table(table, latex_names=True)\n", "    def test_html_latex_names(self, cosmo):\n        table = write_html_table(cosmo, \"temp.html\", latex_names=True)\n        reloaded_cosmo = read_html_table(\"temp.html\", latex_names=True)\n        assert reloaded_cosmo.Om0 == cosmo.Om0\n        assert reloaded_cosmo.H0 == cosmo.H0\n        assert reloaded_cosmo.Tcmb0 == cosmo.Tcmb0\n", "    def test_write_latex_names(self, cosmo):\n        with pytest.raises(ValueError):\n            write_html_table(cosmo, \"test.html\", latex_names=True, format=\"ascii.html\")\n", "    def test_latex_names(self, cosmo):\n        with u.add_enabled_units(cu):\n            table = to_table(cosmo, latex_names=True)\n\n        for name, latex in _FORMAT_TABLE.items():\n            assert name in table.colnames\n            assert table.colnames[name] == latex\n", "    def test_write_html_table_with_latex_names(self, cosmo):\n        with u.add_enabled_units(u.cosmology.units):\n            # Convert the cosmology to a HTML table with latex names.\n            write_html_table(\n                cosmo,\n                \"tmp.html\",\n                latex_names=True,\n            )\n        # Read back the cosmology from the html table.\n        new_cosmo = read_html_table(\"tmp.html\", latex_names=True)\n        # Assert the new cosmology is equal to the original cosmology.\n        assert (\n            new_cosmo.h0.value\n            == cosmo.h0.value\n            and new_cosmo.om0.value == cosmo.om0.value\n            and new_cosmo.ode0.value == cosmo.ode0.value\n        )\n\n\n\n"], "sample_782": ["    def test_remainder_estimator(self):\n        X = np.array([[1, 2], [3, 4]])\n        remainder_est = StandardScaler()\n        ct = make_column_transformer(\n            (StandardScaler(), [0]),\n            remainder=remainder_est,\n        )\n        X_transformed = ct.fit_transform(X)\n        remainder_transformed = remainder_est.transform(X[:, 1].reshape(-1, 1))\n        assert_allclose_dense_sparse(\n            X_transformed[:, 0], remainder_transformed\n        )\n\n", "    def test_remainder_estimator(self):\n        X = np.array([[1, 2], [3, 4]])\n        remainder_estimator = Trans()\n        ct = make_column_transformer(\n            [(StandardScaler(), [0]), (remainder_estimator, [1])],\n        )\n        ct.fit(X)\n        X_transformed = ct.transform(X)\n\n        # Check that the remainder column was transformed by the\n        # estimator, and the other column was scaled.\n        assert_almost_equal(X_transformed[0, 0], 0, decimal=3)\n        assert_almost_equal(X_transformed[0, 1], 2)\n        assert_almost_equal(X_transformed[1, 0], 1, decimal=3)\n        assert_almost_equal(X_transformed[1, 1], 4)\n\n\n", "    def test_remainder_estimator(self):\n        scaler = StandardScaler()\n        remainder_estimator = Trans()\n        ct = make_column_transformer(\n            (StandardScaler(), [0], ),\n            (remainder_estimator, [1], )\n        )\n        X = np.array([[1, 2], [3, 4]])\n        X_transformed = ct.fit_transform(X)\n        assert_array_equal(X_transformed[:, 0], scaler.transform(X[:, 0].reshape(-1, 1)))\n        assert_array_equal(X_transformed[:, 1], remainder_estimator.transform(X[:, 1].reshape(-1, 1)))\n\n\n\n", "    def test_remainder_estimator(self):\n        X = np.array([[1, 2], [3, 4]])\n        remainder_trans = Trans()\n\n        ct = ColumnTransformer(\n            transformers=[('scaler', StandardScaler(), [0]),\n                          ('remainder', remainder_trans, [1])],\n            remainder='remainder',  \n            n_jobs=0\n        )\n\n        X_transformed = ct.fit_transform(X)\n        assert_array_equal(X_transformed[:, 0],\n                           StandardScaler().fit_transform(X[:, [0]]))\n        assert_array_equal(X_transformed[:, 1],\n                           remainder_trans.transform(X[:, 1]))\n\n\n", "    def test_remainder_empty_list(self):\n        X = np.arange(10).reshape((2, 5))\n        transformers = [\n            (StandardScaler(), slice(0, 2)),\n            (StandardScaler(), slice(2, 4)),\n        ]\n        ct = make_column_transformer(transformers, remainder='drop')\n        result = ct.fit_transform(X)\n        expected_result = np.empty((2, 2))\n        assert_array_equal(result, expected_result)\n"], "sample_455": ["    def test_unique_constraint_with_expressions(self):\n        with atomic():\n            Product.objects.create(name=\"Test Product 1\", description=\"Description 1\")\n            with self.assertRaises(IntegrityError):\n                Product.objects.create(name=F('name'), description=\"Description 2\")  \n", "    def test_unique_constraint_deferrable(self):\n        with atomic():\n            UniqueConstraintDeferrable.objects.create(\n                name=\"test_deferrable\",\n                field=\"name\",\n            )\n\n        with connection.cursor() as cursor:\n            cursor.execute(\n                \"SELECT constraint_name FROM information_schema.`COLUMNS` WHERE table_name = %s AND constraint_name = %s\",\n                (UniqueConstraintDeferrable._meta.db_table, \"test_deferrable\"),\n            )\n            deferrable_constraint_rows = cursor.fetchall()\n\n        self.assertEqual(len(deferrable_constraint_rows), 1)\n        cursor.execute(\n            \"SELECT * FROM information_schema.table_constraints WHERE table_name = %s AND constraint_name = %s\",\n            (UniqueConstraintDeferrable._meta.db_table, \"test_deferrable\"),\n        )\n        deferrable_constraint_rows = cursor.fetchall()\n        self.assertEqual(deferrable_constraint_rows[0][4], \"deferred\")\n\n\n\n", "    def test_unique_constraint_condition(self):\n        with connection.cursor() as cursor:\n            connection.introspection.execute(cursor, \"CREATE TABLE unique_constraint_condition_product (id serial primary key, name text, sku text)\")\n\n        product = UniqueConstraintConditionProduct.objects.create(name=\"test\", sku=\"SKU123\")\n        with atomic():\n            with self.assertRaises(IntegrityError):\n                UniqueConstraintConditionProduct.objects.create(name=\"test\", sku=\"SKU123\")\n        with self.assertRaises(ValidationError) as e:\n            UniqueConstraintConditionProduct.objects.create(name=\"test2\", sku=\"SKU123\")\n        self.assertIn(\n            \"UniqueConstraintConditionProduct.name and sku must be unique together\", str(e.exception)\n        )\n", "    def test_deconstruct_reconstruction(self):\n        constraint = BaseConstraint(\n            name=\"my_constraint\",\n            violation_error_code=\"my_error_code\",\n            violation_error_message=\"My custom error message\",\n        )\n\n        path, args, kwargs = constraint.deconstruct()\n        reconstructed_constraint = constraint.__class__(*args, **kwargs)\n\n        self.assertEqual(reconstructed_constraint.name, constraint.name)\n        self.assertEqual(\n            reconstructed_constraint.violation_error_code, constraint.violation_error_code\n        )\n        self.assertEqual(\n            reconstructed_constraint.violation_error_message,\n            constraint.violation_error_message,\n        )\n        self.assertEqual(path, \"%s.%s\" % (constraint.__class__.__module__, constraint.__class__.__name__))\n\n", "    def test_unique_constraint_with_expressions_and_opclasses(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\n                \"CREATE TABLE unique_constraint_product (id SERIAL PRIMARY KEY, name TEXT, price INTEGER)\"\n            )\n        with atomic():\n            Product.objects.create(name=\"Product A\", price=100)\n            Product.objects.create(name=\"Product B\", price=200)\n            with self.assertRaises(IntegrityError) as context:\n                Product.objects.create(name=\"Product A\", price=100)\n            with self.assertRaises(IntegrityError) as context:\n                Product.objects.create(name=\"Product B\", price=200)\n        with connection.cursor() as cursor:\n            constraints = get_constraints(\"unique_constraint_product\")\n            self.assertEqual(len(constraints), 1)\n            self.assertEqual(constraints[0][\"name\"], \"unique_constraint\")\n\n        Product.objects.all().delete()\n        with connection.cursor() as cursor:\n            cursor.execute(\n                f\"DROP TABLE IF EXISTS unique_constraint_product\"\n            )\n"], "sample_391": ["    def test_rename_model_without_options(self):\n        operations = [\n            RenameModel(\n                \"OldName\", \"NewName\"\n            ),\n            RenameModel(\n                \"NewName\", \"FinalName\"\n            ),\n        ]\n        self.assertOptimizesTo(\n            operations,\n            [\n                RenameModel(\n                    \"OldName\", \"FinalName\"\n                ),\n            ],\n        )\n\n", "    def test_rename_model_optimization(self):\n        operations = [\n            operations.RenameModel(\n                old_name=\"FirstModel\", new_name=\"NewFirstModel\",\n            ),\n            operations.RenameModel(\n                old_name=\"SecondModel\", new_name=\"SecondModelRenamed\",\n            ),\n        ]\n        self.assertOptimizesTo(\n            operations,\n            [\n                operations.RenameModel(\n                    old_name=\"FirstModel\", new_name=\"NewFirstModel\",\n                ),\n                operations.RenameModel(\n                    old_name=\"SecondModel\", new_name=\"SecondModelRenamed\",\n                ),\n            ],\n        )\n\n\n\n\n", "    def test_rename_model_in_order(self):\n        operations = [\n            CreateModel(\n                \"migrations.RenameModel1\",\n                fields=[(\"id\", models.AutoField(primary_key=True))]\n            ),\n            RenameModel(\"RenameModel1\", \"RenameModel2\"),\n            CreateModel(\n                \"migrations.RenameModel3\",\n                fields=[(\"id\", models.AutoField(primary_key=True))]\n            ),\n        ]\n        self.assertOptimizesTo(\n            operations,\n            operations,\n            app_label=\"migrations\",\n            exact=2,\n        )\n\n\n", "    def test_rename_model_with_foreignkeys(self):\n        operations = [\n            CreateModel(\n                \"migrations\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\n                        \"name\",\n                        models.CharField(max_length=255),\n                    ),\n                    (\n                        \"target_model\",\n                        models.ForeignKey(\n                            \"migrations.TargetModel\",\n                            on_delete=models.CASCADE,\n                        ),\n                    ),\n                ],\n                {},\n                [],\n                [],\n            ),\n            CreateModel(\n                \"migrations\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=255)),\n                ],\n                {},\n                [],\n                [],\n            ),\n            RenameModel(\"migrations.Migration\", \"new_migrations.Migration\"),\n        ]\n        self.assertOptimizesTo(\n            operations,\n            [\n                CreateModel(\n                    \"new_migrations\",\n                    [\n                        (\"id\", models.AutoField(primary_key=True)),\n                        (\"name\", models.CharField(max_length=255)),\n                        (\"target_model\", models.ForeignKey(\"new_migrations.TargetModel\", on_delete=models.CASCADE)),\n                    ],\n                    {},\n                    [],\n                    [],\n                ),\n                CreateModel(\n                    \"new_migrations\",\n                    [\n                        (\"id\", models.AutoField(primary_key=True)),\n                        (\"name\", models.CharField(max_length=255)),\n                    ],\n                    {},\n                    [],\n                    [],\n                ),\n                RenameModel(\n                    \"migrations.Migration\", \"new_migrations.Migration\"\n                ),\n            ],\n        )\n", "    def test_rename_model_options_merge(self):\n        # Test that RenameModel operations are merged into one\n        operations = [\n            RenameModel(\"ModelA\", \"ModelAA\"),\n            RenameModel(\"ModelAA\", \"ModelAAA\"),\n        ]\n\n        self.assertOptimizesTo(\n            operations,\n            [\n                RenameModel(\"ModelA\", \"ModelAAA\"),\n            ],\n            exact=1,\n        )\n\n\n\n"], "sample_420": ["        def test_custom_error_message(self):\n            form = CustomErrorMessageForm(data={})\n            self.assertEqual(form.errors, {})\n            form = CustomErrorMessageForm(data={\"name1\": \" \"})\n            self.assertEqual(form.errors[\"name1\"], [\"Form custom error message.\"])\n\n", "    def test_custom_error_messages(self):\n        form = CustomErrorMessageForm(data={\"name1\": \"test\"})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors[\"name1\"], [\"Form custom error message.\"])\n\n\n", "    def test_custom_error_message(self):\n        form = CustomErrorMessageForm(data={\"name1\": \"invalid\"})\n        self.assertTrue(form.is_invalid())\n        self.assertIn(\n            \"Form custom error message.\", form.errors[\"name1\"]\n        )\n", "        def test_custom_error_messages(self):\n            form = CustomErrorMessageForm(\n                data={\"name1\": \"invalid\"}\n            )\n            self.assertFalse(form.is_valid())\n            self.assertEqual(form.errors[\"name1\"], [\"Form custom error message.\"])\n", "    def test_custom_error_message(self):\n        form = CustomErrorMessageForm(data={'name1': 'invalid'})\n        self.assertEqual(form.is_valid(), False)\n        self.assertIn(\n            \"Form custom error message.\", form.errors['name1']\n        )\n\n\n\n"], "sample_337": ["    def test_rotate_token(self):\n        @csrf_exempt\n            return HttpResponse('OK')\n\n        with override_settings(CSRF_USE_SESSIONS=True):\n            request = HttpRequest()\n            request.session = SessionStore()\n            request.session[CSRF_SESSION_KEY] = MASKED_TEST_SECRET1\n            response = protected_view(request, view)\n            self.assertEqual(response.status_code, 200)\n\n        with override_settings(CSRF_USE_SESSIONS=True):\n            request = HttpRequest()\n            request.session = SessionStore()\n            request.session[CSRF_SESSION_KEY] = MASKED_TEST_SECRET1\n            response = sandwiched_rotate_token_view(request)\n            self.assertEqual(response.status_code, 200)\n            self.assertNotEqual(request.session[CSRF_SESSION_KEY], MASKED_TEST_SECRET1)\n            self.assertIn('CSRF_COOKIE', request.META)\n            self.assertIsNotNone(request.META['CSRF_COOKIE'])\n            self.assertIsNotNone(get_token(request))\n\n\n\n\n", "    def test_rotate_token(self):\n        with override_settings(CSRF_USE_SESSIONS=True):\n            request_mock = HttpRequest()\n            request_mock.method = 'POST'\n            request_mock.META = {\n                'HTTP_ORIGIN': 'https://example.com',\n                CSRF_SESSION_KEY: MASKED_TEST_SECRET1\n            }\n            response = sandwiched_rotate_token_view(request_mock)\n            self.assertEqual(response.status_code, 200)\n\n            # Check if the CSRF cookie has been updated\n            csrf_token = get_token(request_mock)\n            self.assertNotEqual(csrf_token, MASKED_TEST_SECRET1)\n            \n", "    def test_origin_reject_empty_origin(self):\n        request = HttpRequest()\n        request.META['HTTP_ORIGIN'] = ''\n        middleware = CsrfViewMiddleware()\n        with self.assertRaises(RejectRequest) as context:\n            middleware._origin_verified(request)\n        self.assertEqual(context.exception.reason, REASON_BAD_ORIGIN % '')\n", "    def test_masked_token_comparison(self):\n        self.assertTrue(equivalent_tokens(MASKED_TEST_SECRET1, MASKED_TEST_SECRET1))\n\n        self.assertFalse(equivalent_tokens(MASKED_TEST_SECRET1, MASKED_TEST_SECRET2))\n        self.assertFalse(equivalent_tokens(MASKED_TEST_SECRET1, TEST_SECRET))\n        self.assertFalse(equivalent_tokens(TEST_SECRET, MASKED_TEST_SECRET2))\n\n", "    def test_csrf_cookie_needs_reset(self):\n        with override_settings(CSRF_COOKIE_NAME='csrf_cookie'):\n            middleware = CsrfViewMiddleware()\n            request = HttpRequest()\n            request.META['CSRF_COOKIE_USED'] = True\n\n            response = middleware.process_response(request, HttpResponse())\n            self.assertTrue(response.csrf_cookie_set)\n            self.assertFalse(request.csrf_cookie_needs_reset)\n\n            request.csrf_cookie_needs_reset = True\n            response = middleware.process_response(request, HttpResponse())\n            self.assertTrue(response.csrf_cookie_set)\n            self.assertFalse(request.csrf_cookie_needs_reset)\n"], "sample_916": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1113": ["    def test_block_collapse_with_transpose_inverse_matmul(self):\n        B = BlockMatrix([[X, Z], [ZeroMatrix(m, n), Y]])\n        C = BlockMatrix([[Identity(n), Z]])\n        expr = (C * B).transpose() * (BlockMatrix([[X.inverse(), ZeroMatrix(l, n)], [ZeroMatrix(n, m), Y.inverse()]]))\n        result = block_collapse(expr)\n        expected = BlockMatrix([[X.inverse(), ZeroMatrix(l, n)], [ZeroMatrix(n, m), Y.inverse()]])\n        self.assertEqual(result, expected) \n", "compilation error", "    def test_block_collapse_special_cases():\n        from sympy.matrices.expressions.blockmatrix import BlockMatrix\n\n        blockDiag = BlockDiagMatrix(Z, X, W)\n        assert block_collapse(blockDiag) == BlockMatrix([[Z, ZeroMatrix(m, n)],\n                                                         [ZeroMatrix(n, m), X]])\n        assert block_collapse(BlockMatrix([[X, Z],\n                                          [ZeroMatrix(m, n), Y]])) == \\\n               BlockMatrix([[X, Z],\n                                          [ZeroMatrix(m, n), Y]])\n        assert block_collapse(BlockMatrix([[Identity(n), Z]])) == \\\n               BlockMatrix([[Identity(n), Z]])\n        assert block_collapse(BlockMatrix([[Identity(n), Z]])) == \\\n               BlockMatrix([[Identity(n), Z]])\n", "    def test_block_collapse_kronecker_delta():\n        from sympy.matrices.specialmatrices import kron\n        M = kron(X, Y)\n        K = kron(Identity(2), Identity(2))\n        result = block_collapse(M * K)\n        expected = kron(X, Y)\n        assert result == expected\n\n", "   def test_blockdiag_identity():\n    B = BlockDiagMatrix(Identity(2), Identity(3))\n    assert B.blockshape == (2, 2)\n    assert B.diag == (Identity(2), Identity(3))\n    assert B.shape == (2, 2)\n    assert B.is_structurally_symmetric\n    assert B.transpose() == BlockDiagMatrix(Identity(2), Identity(3))\n    assert B.inverse() == BlockDiagMatrix(Identity(2).inverse(), Identity(3).inverse())\n    assert block_collapse(B) == BlockDiagMatrix(Identity(2), Identity(3))\n\n"], "sample_303": ["    def test_settings_to_cmd_args_env(self):\n        with mock.patch.object(BaseDatabaseClient, 'settings_to_cmd_args_env') as mock_method:\n            self.client.runshell(parameters={'some_param': 'value'})\n            mock_method.assert_called_once_with(self.client.connection.settings_dict, {'some_param': 'value'})\n", "    def test_settings_to_cmd_args_env(self):\n        with mock.patch.object(BaseDatabaseClient, 'settings_to_cmd_args_env') as mock_method:\n            self.client.runshell(parameters={\"key\": \"value\"})\n            mock_method.assert_called_once_with(self.client.connection.settings_dict, {\"key\": \"value\"})\n", "    def test_settings_to_cmd_args_env(self):\n        with mock.patch.object(BaseDatabaseClient, 'runshell') as mock_runshell:\n            self.client.settings_to_cmd_args_env(dict(), {})\n            mock_runshell.assert_called_once_with(\n                mock.ANY,\n                mock.ANY,\n            )\n", "    def test_settings_to_cmd_args_env_not_implemented(self):\n        with self.assertRaises(NotImplementedError):\n            self.client.settings_to_cmd_args_env({}, {})\n", "    def test_settings_to_cmd_args_env(self):\n        with mock.patch.object(self.client, 'settings_to_cmd_args_env') as mock_method:\n            self.client.runshell([])\n            mock_method.assert_called_once_with(self.client.connection.settings_dict, [])\n"], "sample_289": ["    def test_ordered_set_reversed(self):\n        ordered_set = OrderedSet([1, 2, 3])\n        self.assertEqual(list(reversed(ordered_set)), [3, 2, 1])\n", "    def test_iter(self):\n        s = OrderedSet([1, 2, 3])\n        self.assertEqual(list(s), [1, 2, 3])\n", "    def test_orderedset_reversed(self):\n        s = OrderedSet([1, 2, 3])\n        self.assertEqual(list(reversed(s)), [3, 2, 1])\n", "    def test_ordered_set_reverse(self):\n        s = OrderedSet([1, 2, 3])\n        self.assertEqual(list(reversed(s)), [3, 2, 1])\n", "    def test_ordered_set_reverse(self):\n        s = OrderedSet([1, 2, 3])\n        self.assertEqual(list(reversed(s)), [3, 2, 1])\n"], "sample_733": ["    def test_stop_words_removal(self):\n        vectorizer = CountVectorizer(stop_words='english')\n        X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n        # Check that stop words have been removed\n        assert_equal(vectorizer.vocabulary_, {'pizza': 0,\n                                             'beer': 1,\n                                             'copyright': 2,\n                                             'burger': 3,\n                                             'coke': 4,\n                                             'salad': 5,\n                                             'celeri': 6,\n                                             'tomato': 7,\n                                             'water': 8})\n\n        vectorizer = CountVectorizer(stop_words='english')\n        X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n\n        # Check that stop words are ignored while calculating tfidf\n\n        tfidf = TfidfVectorizer(stop_words='english')\n        tfidf_X = tfidf.fit_transform(ALL_FOOD_DOCS)\n        assert_equal(tfidf.vocabulary_, vectorizer.vocabulary_)\n\n        # Assert that the IDF for removed words is 0\n        assert_equal(tfidf.idf_[vectorizer.vocabulary_['the']], 0.0)\n        assert_equal(tfidf.idf_[vectorizer.vocabulary_['and']], 0.0)\n", "    def test_invalid_ngram_range(self):\n        with pytest.raises(ValueError):\n            TfidfVectorizer(ngram_range=(-1, 1))\n        with pytest.raises(ValueError):\n            TfidfVectorizer(ngram_range=(1, -1))\n        with pytest.raises(ValueError):\n            TfidfVectorizer(ngram_range=(2, 1))\n", "    def test_custom_tokenizer(self):\n        vectorizer = TfidfVectorizer(tokenizer=lazy_analyze)\n        x = vectorizer.fit_transform(ALL_FOOD_DOCS)\n        print(x.toarray())\n        assert_equal(x.shape[1], 1)\n        assert_array_equal(x.toarray(),\n                           [[1] * len(ALL_FOOD_DOCS)]) \n\n", "    def test_fit_transform_with_preprocessor(self):\n        vectorizer = TfidfVectorizer(preprocessor=lambda x: strip_eacute(x))\n        X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n        assert_array_equal(vectorizer.get_feature_names(),\n                           ['THE', 'BURGER', 'BEER', 'COKE', 'CELERI',\n                            'COPYRIGHT', 'SALAD', 'SPARKLING WATER',\n                            'TOMATO', 'WATER'])\n\n\n\n", "    def test_stop_words_from_stop_words_list(self):\n        \"\"\"Test stop_words parameter with a custom list.\"\"\"\n        stop_words = [\"the\", \"pizza\"]\n        vectorizer = TfidfVectorizer(stop_words=stop_words)\n        X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n        assert_equal(vectorizer.stop_words_, set(stop_words))\n        assert_array_equal(X.getnnz(axis=1).sort(),\n                           [0, 0, 2, 0, 0, 0])\n"], "sample_1007": ["    def test_binomial_negative_k(self):\n        n = Symbol('n')\n        k = Symbol('k')\n        assert binomial(n, -1) == 0\n        assert binomial(n, -2) == 0\n        assert binomial(n, -3) == 0\n", "    def test_binomial_negative_integer_k(self):\n        n = Symbol('n', integer=True)\n        for k in [-1, -2, -3]:\n            assert binomial(n, k) == 0\n", "    def test_binomial_negative_n_k():\n        n = Symbol('n', integer=True)\n        k = Symbol('k', integer=True)\n        assert binomial(n, k).rewrite(binomial) == binomial(n, k)\n        assert binomial(n - k, k) == binomial(n, k)\n        assert binomial(-1, 2) == 0\n\n", "    def test_binomial_negative_n(self):\n        n = Symbol('n', negative=True)\n        for k in range(5):\n            raises(ValueError, lambda: binomial(n, k))\n", "    def test_binomial_negative_k(self):\n        n = Symbol('n', integer=True)\n        for i in range(-5, 0):\n            assert binomial(n, i) == 0\n\n\n\n"], "sample_783": ["def test_missing_indicator_missing_on_fit_not_transform():\n    missing_values = np.nan\n    X1 = np.array([[missing_values, 1, 3],\n                   [4, 0, missing_values],\n                   [8, 1, 0]])\n    X2 = np.array([[5, 1, missing_values],\n                   [missing_values, 2, 3],\n                   [2, 4, 0]])\n\n    indicator = MissingIndicator(missing_values=missing_values,\n                               features=\"missing-only\")\n    indicator.fit(X1)\n    X2_tr = indicator.transform(X2)\n    expected_output = np.array([[False, True, True],\n                                [True, False, False],\n                                [False, False, False]])\n    assert_array_equal(X2_tr, expected_output)\n\n\n\n", "    def test_missing_indicator_missing_only_sparse(self):\n        \"\"\"Test MissingIndicator with missing_only=True and sparse input.\"\"\"\n        X = sparse.csr_matrix([[1, np.nan, 3],\n                              [4, 0, np.nan],\n                              [8, 1, 0]])\n        indicator = MissingIndicator(missing_values=np.nan, features=\"missing-only\")\n        indicator.fit(X)\n        expected_indicator = sparse.csr_matrix(\n            [[False, True, False],\n             [False, False, True],\n             [False, False, False]])\n        assert_allclose_dense_sparse(indicator.transform(X), expected_indicator)\n\n\n\n\n", "compilation error", "compilation error", "compilation error"], "sample_711": ["compilation error", "compilation error", "    def test_complex_paths(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            def test_a/b(x): pass\n            def test_a/b/c::D(y): pass\n            def test_a/b/c::D/e::f(z): pass\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        assert result.ret == 0\n\n        nodeids = result.get_nodeids()\n        for expected in [\"\", \"a\", \"a/b\", \"a/b/c\", \"a/b/c::D\", \"a/b/c::D/e\", \"a/b/c::D/e::f\"]:\n            assert expected in nodeids\n\n", "compilation error", "compilation error"], "sample_52": ["    def test_complex_choices_with_to_field_name(self):\n\n        class ComplexChoiceForm(forms.Form):\n            article = forms.ModelChoiceField(\n                queryset=Article.objects.all(),\n                to_field_name='author__name',\n                empty_label=\"Select an author\"\n            )\n\n        form = ComplexChoiceForm()\n        self.assertIn('article', form.fields)\n\n        # Check if the choices are populated correctly\n        choices = form.fields['article'].choices\n        self.assertEqual(len(choices), 2)\n        self.assertEqual(choices[0][0], self.article1.author.pk)\n        self.assertEqual(choices[0][1], \"John Doe\")\n        self.assertEqual(choices[1][0], self.article2.author.pk)\n        self.assertEqual(choices[1][1], \"Jane Doe\")\n\n", "    def test_modelchoice_field_queryset_limit(self):\n        form = forms.Form(\n            fields={'category': ModelChoiceField(Category.objects.filter(name__contains='test').all(), label='Category')}\n        )\n        choices = form.fields['category'].choices\n        assert len(choices) == 2\n        expected_choices = [(2, 'A test'), (3, 'Third')]\n        self.assertSequenceEqual(choices, expected_choices)  \n", "    def test_modelchoicefield_queryset_limits(self):\n        from django.db.models import Q\n        class LimitedCategory(Category):\n                return self.name\n            class Meta:\n                proxy = True\n        LimitedCategory.objects.create(name='New', slug='new', url='new')\n        LimitedCategory.objects.create(name='Another', slug='another', url='another')\n\n        limited_queryset = LimitedCategory.objects.filter(name__startswith='New')\n        form_field = ModelChoiceField(limited_queryset, empty_label=\"\")\n        choices = form_field.choices\n\n        self.assertEqual(len(choices), 2)\n        self.assertEqual(list(choices)[0][1], 'New')\n        self.assertEqual(list(choices)[1][1], 'Another')\n\n        \n\n        limited_queryset_with_q = (\n            LimitedCategory.objects.filter(Q(name__startswith='New') | Q(name='Another'))\n        )\n        form_field = ModelChoiceField(limited_queryset_with_q, empty_label=\"\")\n        choices = form_field.choices\n        self.assertEqual(len(choices), 3)\n        self.assertIn('New', [choice[1] for choice in choices])\n        self.assertIn('Another', [choice[1] for choice in choices])\n        self.assertIn('Third', [choice[1] for choice in choices])\n\n\n", "    def test_modelchoice_field_empty_label(self):\n        form = forms.ModelForm(\n            model=Article,\n            fields=['category'],\n            widgets={'category': CheckboxSelectMultiple}\n        )\n        field = form.fields['category']\n        empty_choice_iterator = ModelChoiceIterator(field)\n        self.assertIn('', [choice[0] for choice in empty_choice_iterator])\n\n        # Test that the empty label is not present when it's set to None\n        form = forms.ModelForm(\n            model=Article,\n            fields=['category'],\n            widgets={'category': CheckboxSelectMultiple}\n        )\n        field = form.fields['category']\n        field.empty_label = None\n        empty_choice_iterator = ModelChoiceIterator(field)\n        self.assertNotIn('', [choice[0] for choice in empty_choice_iterator]) \n\n", "    def test_modelchoicefield_with_to_field_name(self):\n        class MyForm(forms.Form):\n            article = forms.ModelChoiceField(\n                queryset=Article.objects.all(),\n                to_field_name='slug',\n                empty_label=None\n            )\n\n        form = MyForm()\n        self.assertEqual(form.fields['article'].choices[0][1], 'django')\n        self.assertEqual(form.fields['article'].choices[1][1], 'python')\n"], "sample_863": ["    def test_pipeline_memory(self):\n        X = np.random.randn(10, 5)\n        y = np.random.randint(0, 2, size=10)\n\n        # Test with a dummy transformer that modifies its state on fit()\n        class MemorableTransformer(TransformerMixin):\n                self.fitted = False\n\n                self.fitted = True\n                return self\n\n                return X\n\n        pipe = Pipeline([\n            ('transformer', MemorableTransformer()),\n            ('clf', DummyClassifier())\n        ], memory=joblib.Memory(cachedir=mkdtemp())\n        )\n\n        # First fit should fit the transformer\n        pipe.fit(X, y)\n        assert pipe.steps[0][1].fitted\n\n        # Second fit should load from memory\n        pipe.fit(X, y)\n        assert pipe.steps[0][1].fitted  # Already fitted\n        # Check if two successive fits are faster\n        start = time.time()\n        pipe.fit(X, y)  # 3rd fit\n        end = time.time()\n        assert end - start < 0.1\n\n        # Test with a real transformer (PCA)\n        pipe = Pipeline([\n            ('scaler', StandardScaler()),\n            ('pca', PCA(n_components=2)),\n            ('clf', DummyClassifier())\n        ], memory=joblib.Memory(cachedir=mkdtemp())\n        )\n\n        pipe.fit(X, y)\n        \n\n", "    def test_pipeline_fit_transform_sparse(self):\n        X = sparse.csr_matrix([[1, 2], [3, 4]])\n        y = np.array([0, 1])\n\n        scaler = StandardScaler()\n        clf = LogisticRegression()\n        pipeline = Pipeline([\n            ('scaler', scaler),\n            ('clf', clf)\n        ])\n\n        Xt = pipeline.fit_transform(X, y)\n\n        assert isinstance(Xt, sparse.csr_matrix)\n", "    def test_pipeline_fit_transform_with_memory(self):\n        X = np.random.rand(100, 5)\n        y = np.random.randint(0, 2, size=100)\n        path = mkdtemp()\n        with joblib.Memory(path) as cache:\n            pipe = Pipeline([('scaler', StandardScaler()),\n                             ('clf', LogisticRegression())])\n            pipe.fit(X, y)\n            pipe.transform(X)\n            # pipeline should have been cached, so fit should be faster\n            start = time.time()\n            pipe.fit(X, y)\n            end = time.time()\n            self.assertLess(end - start, 0.1)  # should be faster than first fit\n    ", "    def test_pipeline_fit_transform_with_no_transform(self):\n        estimator = NoTransform()\n        pipe = Pipeline([\n            ('step1', estimator)\n        ])\n        X = np.random.rand(10, 5)\n        y = np.random.randint(0, 2, size=10)\n\n        result = pipe.fit_transform(X, y)\n        assert result is estimator\n", "    def test_pipeline_n_jobs(self, n_jobs, expected_njobs, tmpdir):\n        # Arrange a pipeline with some estimators that need parallel fitting\n        pipeline = Pipeline(\n            [\n                (\"scaler\", StandardScaler(n_jobs=n_jobs)),\n                (\"clf\", DummyRegressor()),\n            ]\n        )\n        with tempfile.TemporaryDirectory(dir=tmpdir) as tmpdir:\n            joblib_filename = os.path.join(tmpdir, \"pipeline_cache.joblib\")\n            pipeline.set_params(memory=joblib_filename)\n            pipeline.fit(iris.data, iris.target)\n\n            # Check if the estimator was fitted with the expected number of jobs\n            loaded_pipeline = joblib.load(joblib_filename)\n            for name, estimator in loaded_pipeline.steps:\n                if hasattr(estimator, \"n_jobs\"):\n                    assert estimator.n_jobs == expected_njobs \n\n"], "sample_804": ["    def test_OneHotEncoder_with_drop(self):\n        X = np.array([[0, 1, 2], [0, 1, 2], [1, 2, 0]])\n        enc = OneHotEncoder(handle_unknown='ignore', drop=1)\n\n        with pytest.raises(ValueError):\n            enc.fit_transform(X)\n", "    def test_onehotencoder_drop(self):\n        enc = OneHotEncoder(handle_unknown='ignore', drop='first')\n        X = np.array([[1, 2, 3], [1, 2, 4], [5, 6, 7]])\n        enc.fit(X)\n        X_trans = enc.transform(X)\n\n        expected_shape = (3, 5)\n        assert_equal(X_trans.shape, expected_shape)\n        assert (X_trans.toarray()[:, 1:].sum(axis=0) == 0).all()\n", "    def test_onehotencoder_legacy_fit_transform_categorical_unknown(self):\n        ohe = OneHotEncoder(handle_unknown='ignore', dtype=np.int32,\n                            n_values=10, sparse=True)\n\n        X = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]], dtype=np.int)\n        ohe.fit(X)\n\n        # Test that we can handle unknown categories\n        X_new = np.array([[1, 2, 11]])\n        transformed = ohe.transform(X_new)\n\n        # Check that the unknown category is represented by zeros\n        assert_equal(transformed[0, 10], 0)\n\n\n\n", "    def test_onehotencoder_drop_categories(rand_array_data):\n        X = rand_array_data\n        encoder = OneHotEncoder(handle_unknown='ignore', drop=True)\n        encoder.fit(X)\n\n        # Test drop when categories not specified\n        X_transformed = encoder.transform(X)\n\n        # Assert that dropped categories are not present\n        for i in range(X_transformed.shape[1]):\n            for j in range(X_transformed.shape[0]):\n                assert X_transformed[j, i] == 0, f\"Category not dropped: {i}, {j}\"\n", "    def test_ordinal_encoder_with_categories(\n        self,"], "sample_648": ["    def test_empty_parameterset_mark_collect(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.parametrize('x', [1, 2])\n                pass\n\n            @pytest.mark.parametrize('y', [])\n                pass\n            \"\"\"\n        )\n\n        result = testdir.runpytest()\n        result.assert_outcomes(passed=2, failures=2)\n        for line in result.stdout:\n            if \"Empty parameter set\" in line:\n                assert 'test_empty_param_set' in line\n", "    def test_empty_parameterset_mark(self, pytester: Pytester, monkeypatch: mock.Mocker) -> None:\n            node: \"Node\",\n            config: \"Config\",\n        ) -> None:\n            collector = Collector(config, node)\n            # Simulate the collection process\n            collector._docollect()\n            node.outcome.set_result(\n                test_status=ExitCode.EXCEPTION,\n                exception=ValueError(\"Empty parameters\"),\n            )\n        monkeypatch.setattr(Collector, \"_docollect\", fake_collect_function)\n        pytester.runpytest(\n            \"-m\",\n            EMPTY_PARAMETERSET_OPTION,\n            \"-rA\",  # Collect all even if they fail\n            \"test_empty_parameterset.py\",\n        )\n        pytester.assert_outcome(\n            \"test_empty_parameterset.py::test_empty_parameterset_empty_params\",\n            status=ExitCode.EXCEPTION,\n            passed=False,\n            failed=True,\n            lines=[\"Empty parameters\"],\n        )\n\n", "    def test_empty_parameterset_mark(self, testdir, request):\n        p = testdir.makepyfile(\n            \"\"\"\n                pass\n            \"\"\"\n        )\n        result = testdir.runpytest(p, [\n            '-m', 'EMPTY_PARAMETERSET_OPTION'\n        ])\n        result.stdout.fnmatch_lines([\n            '*collected 1 item*']\n        )\n        assert result.retcode == ExitCode.OK\n\n        p = testdir.makepyfile(\n            \"\"\"\n                pass\n            \"\"\"\n        )\n        result = testdir.runpytest(p, [\n            '-m', 'EMPTY_PARAMETERSET_OPTION',\n            '-v'\n        ])\n        result.stdout.fnmatch_lines([\n            '*test_func*EMPTY_PARAMETERSET_OPTION'\n        ])\n        assert result.retcode == ExitCode.OK\n", "    def test_empty_parameterset_mark(self, pytester: Pytester):\n            pass\n\n        # Patch getfslineno to return a known filename and lineno\n        with mock.patch(\"pytest.utils.getfslineno\", return_value=(\n            os.path.join(os.getcwd(), \"test_function.py\"), 12), create=True):\n            param_args = (\"arg1\", \"arg2\")\n            with pytester.raises(Collector.CollectError, match=\"Empty parameter set\"):\n                config = pytester.parse_configure()\n                MarkGenerator._for_parametrize(\n                    \"argnames\", [param_args], test_func, config, \"\",\n                )\n            \n        # Check the config.ini is properly updated\n        config = pytester.parse_configure()\n        assert config.getini(EMPTY_PARAMETERSET_OPTION) == \"skip\"\n", "    def test_empty_parameterset_mark(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n                pass\n            \"\"\"\n        )\n\n        reprec = testdir.inline_run(\n            \"\"\"\n                pass\n            \"\"\",\n            markers=EMPTY_PARAMETERSET_OPTION,\n        )\n        reprec.assert_outcomes(passed=1)\n        get_unpacked_marks = NodeKeywords.get_unpacked_marks\n        assert get_unpacked_marks(reprec.getcalls()[0].node, consider_mro=True) == [\n            MarkDecorator(Mark(EMPTY_PARAMETERSET_OPTION, (), {}, _ispytest=True))\n        ]\n\n        reprec = testdir.inline_run(\n            \"\"\"\n            @pytest.mark.parametrize(\"x\", [1, 2], ids=['a', 'b'])\n                pass\n            \"\"\"\n        )\n        reprec.assert_outcomes(passed=1)\n        assert get_unpacked_marks(reprec.getcalls()[0].node, consider_mro=True) == [\n            MarkDecorator(\n                Mark(\"parametrize\", (), {\"argnames\": \"x\", \"argvalues\": \"[1, 2]\", \"ids\": [\"a\", \"b\"]}, _ispytest=True)\n            )\n        ]\n\n\n\n"], "sample_29": ["    def test_latex_names_off(self):\n        with tempfile.NamedTemporaryFile(suffix=\".tex\", delete=False) as f:\n            write_latex(self.cosmo, f.name, latex_names=False)\n            table = Table.read(f.name, format=\"latex\")\n            for col in table.columns:\n                assert col not in _FORMAT_TABLE\n", "    def test_latex_names(self, cosmo):\n        with pytest.tmpdir.as_cwd() as tmpdir:\n            file_path = \"test_cosmology.tex\"\n            write_latex(cosmo, file_path, latex_names=True)\n            table = Table.read(file_path, format=\"latex\")\n            expected_names = [_FORMAT_TABLE.get(k, k) for k in cosmo.__parameters__]\n            assert table.colnames == expected_names\n", "    def test_latex_names(self, cosmo):\n        \"\"\"Test that parameter names are converted to LaTeX format.\"\"\"\n        with pytest.raises(ValueError) as excinfo:\n            write_latex(cosmo, \"tmp.tex\", latex_names=False, format=\"latex\")\n        assert \"format must be 'latex'\" in str(excinfo.value)\n        with pytest.raises(ValueError) as excinfo:\n            write_latex(cosmo, \"tmp.tex\", format=\"txt\")\n        assert \"format must be 'latex'\" in str(excinfo.value)\n        write_latex(cosmo, \"tmp.tex\", latex_names=True, format=\"latex\")\n        with open(\"tmp.tex\", \"r\") as f:\n            content = f.read()\n        for latex_name, name in _FORMAT_TABLE.items():\n            assert name in content\n", "    def test_latex_names(self, cosmo):\n        with pytest.raises(ValueError):\n            write_latex(cosmo, \"test.tex\", format=\"latex\", latex_names=False)\n        with open(\"test.tex\", \"r\") as f:\n            tex = f.read()\n        assert all(name in tex for name in _FORMAT_TABLE.values())\n\n\n\n", "    def test_latex_names(self, cosmo):\n        from astropy.cosmology import FlatLambdaCDM\n        with pytest.raises(ValueError) as excinfo:\n            write_latex(\n                FlatLambdaCDM(),\n                \"latex_test.tex\",\n                latex_names=False,\n            )\n        assert \"format must be 'latex'\" in str(excinfo.value)\n\n\n\n"], "sample_1071": ["compilation error", "compilation error", "def test_dim_simplify():\n    with warns_deprecated_sympy(limit=1):\n        expr = Add(L, L)\n        assert dim_simplify(expr) == 2*L\n\n", "compilation error", "compilation error"], "sample_1040": ["    def test_integral_limits():\n        I = Integral(x**2, (x, 0, 1))\n        assert mp._print_Integral(I) == \\\n               '<apply>\\n' \\\n               '    <integration>\\n' \\\n               '        <ci>x</ci>\\n' \\\n               '        <ci>0</ci>\\n' \\\n               '        <ci>1</ci>\\n' \\\n               '        <power>\\n' \\\n               '            <ci>x</ci>\\n' \\\n               '            <cn>2</cn>\\n' \\\n               '        </power>\\n' \\\n               '    </integration>\\n' \\\n               '</apply>'\n        assert mpp._print_Integral(I) == \\\n               '<mrow>\\n' \\\n               '    <munder>\\n' \\\n               '        <mi>\u222b</mi>\\n' \\\n               '        <mrow>\\n' \\\n               '            <mi>x</mi><span style=\"line-height:1.0em;\"><sup>2</sup></span>\\n' \\\n               '            <mrow>\\n' \\\n               '                <mi>x</mi><span style=\"line-height:1.0em;\">=</span>0\\n' \\\n               '                <mi>x</mi><span style=\"line-height:1.0em;\">=</span>1\\n' \\\n               '            </mrow>\\n' \\\n               '        </mrow>\\n' \\\n               '    </munder>\\n' \\\n               '</mrow>'\n", "    def test_print_matrix_with_symbols():\n        matrix = Matrix([[x, y], [2*x, 3*y]])\n        xml = mp._print(matrix)\n        assert xml.tag == 'mtable'\n\n        xml = mpp._print(matrix)\n        assert xml.tag == 'mfenced'\n\n", "    def test_mathml_presentation_matrix_symbols():\n        m = MatrixSymbol('A', 2, 3)\n        xml = mpp._print(m)\n        assert xml.toprettyxml() == '<mi>A</mi>'\n", "    def test_printing_matrix_symbols(self):\n        for name in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']:\n            m = MatrixSymbol(name, 2, 2)\n            content_xml = mp._print(m)\n            presentation_xml = mpp._print(m)\n            assert 'matrix' in str(content_xml)\n            assert 'mfenced' in str(presentation_xml)\n\n\n\n\n\n\n\n", "    def test_presentation_matrix_symbol(self):\n        raises(ValueError, lambda: mpp._print_MatrixSymbol(Symbol(\"M\",\n        dtype=float))) \n\n\n\n"], "sample_22": ["compilation error", "compilation error", "compilation error", "    def test_angle_axis(axis, angle, expected_angle, expected_axis):\n        matrix = rotation_matrix(angle, axis)\n        ret_angle, ret_axis = angle_axis(matrix)\n        assert_allclose(ret_angle.value, expected_angle.value)\n        assert_array_equal(ret_axis, expected_axis)\n\n", "compilation error"], "sample_864": ["    def test_estimate_bandwidth(self, random_state):\n        bandwidth = estimate_bandwidth(X, random_state=random_state)\n        assert bandwidth > 0, \"Bandwidth should be positive\"\n", "    def test_get_bin_seeds(self):\n        # Test get_bin_seeds with different bin sizes\n\n        X = np.random.rand(10, 2)\n\n        # Test default bin_size with min_bin_freq=1\n        bin_size = 0.2\n        seeds = get_bin_seeds(X, bin_size, min_bin_freq=1)\n        assert len(seeds) > 0, \"Empty seeds returned for default min_bin_freq\"\n        assert len(seeds) <= len(X), \"Too many seeds returned\"\n\n        # Test with a larger bin_size\n        bin_size = 0.5\n        seeds = get_bin_seeds(X, bin_size, min_bin_freq=1)\n        assert len(seeds) < len(seeds), \"Too many seeds returned with larger bin_size\"\n\n        # Test with a min_bin_freq > 1\n        bin_size = 0.2\n        min_bin_freq = 3\n        seeds = get_bin_seeds(X, bin_size, min_bin_freq=min_bin_freq)\n        assert len(seeds) < len(seeds), \"Too many seeds returned with larger min_bin_freq\"\n", "    def test_cluster_all(self, cluster_all):\n        ms = MeanShift(bandwidth=1.5, cluster_all=cluster_all).fit(X)\n        labels = ms.labels_\n        if cluster_all:\n            assert (labels >= 0).all()\n            assert np.unique(labels).size == n_clusters\n        else:\n            assert (labels >= 0).all()\n            assert np.unique(labels).size == (n_clusters + 1)\n\n\n\n", "    def test_bin_seeding(self):\n        bandwidth = 2\n        X_bin_seeded = get_bin_seeds(X, bandwidth, min_bin_freq=1)\n        assert_array_equal(X_bin_seeded.shape, X.shape)\n        X_all_points = get_bin_seeds(X, bandwidth, min_bin_freq=len(X))\n        assert_array_equal(X_all_points, X)\n\n        # Test bin_seeding with empty data\n        empty_X = np.array([])\n        with pytest.raises(ValueError):\n            get_bin_seeds(empty_X, bandwidth, min_bin_freq=1)  \n\n        with pytest.raises(ValueError):\n            get_bin_seeds(empty_X, bandwidth, min_bin_freq=len(empty_X))       \n", "    def test_get_bin_seeds(self, bin_seeding):\n        if bin_seeding:\n            seeds = get_bin_seeds(X, bandwidth=1, min_bin_freq=1)\n            assert len(seeds) > 0\n            assert seeds.shape[1] == X.shape[1]\n        else:\n            seeds = get_bin_seeds(X, bandwidth=1, min_bin_freq=1)\n            assert len(seeds) == X.shape[0]\n            assert seeds.shape[1] == X.shape[1]\n"], "sample_12": ["    def test_wrap_at_with_array(self):\n        a = Angle([180 * u.deg, 270 * u.deg, 0 * u.deg], wrap_at=360 * u.deg)\n        b = a.wrap_at(360 * u.deg)\n        assert_array_equal(b.degree, np.array([0, 90, 0]))\n\n        a = Angle([180 * u.deg, 270 * u.deg, 0 * u.deg])\n        b = a.wrap_at(180 * u.deg)\n        assert_array_equal(b.degree, np.array([180, 90, 0]))\n\n", "    def test_angle_from_string(self, angle, expected):\n        a = Angle(angle)\n        assert_allclose(a.value, expected.value)\n        assert a.unit == expected.unit\n", "    def test_wrap_at_inplace(self):\n        a = Angle([180, 360, 540, -180, -360] * u.deg)\n        a_copy = a.copy()\n        a.wrap_at(180 * u.deg, inplace=True)\n        assert_allclose(a.degree, np.array([0, 0, 180, 0, 0]))\n        assert_array_equal(a_copy.degree, np.array([180, 360, 540, -180, -360]))\n\n\n", "    def test_wrap_at_rounding(self):\n        a = Angle(179.9 * u.deg)\n        a.wrap_at(360 * u.deg)\n        assert_allclose(a.degree, 179.9)\n        a = Angle(179.99 * u.deg)\n        a.wrap_at(360 * u.deg)\n        assert_allclose(a.degree, 179.99)\n        a = Angle(179.999 * u.deg)\n        a.wrap_at(360 * u.deg)\n        assert_allclose(a.degree, 179.999)\n        a = Angle(-179.9 * u.deg)\n        a.wrap_at(360 * u.deg)\n        assert_allclose(a.degree, -179.9)\n\n        a = Angle(180 * u.deg)  \n        a.wrap_at(360 * u.deg)\n        assert_allclose(a.degree, 180)\n        a = Angle(180.001 * u.deg)\n        a.wrap_at(360 * u.deg)\n        assert_allclose(a.degree, 180.001)\n\n\n\n", "    def test_angle_from_strings(self, value, expected):\n        with pytest.warns(AstropyDeprecationWarning) as w:\n            a = Angle(value)\n        "], "sample_761": ["    def test_missing_indicator_all_features(self):\n        X1 = np.array([[np.nan, 1, 3],\n                       [4, 0, np.nan],\n                       [8, 1, 0]])\n        indicator = MissingIndicator(features='all')\n        indicator.fit(X1)\n        X_tr = indicator.transform(X1)\n        assert_array_equal(X_tr, np.ones_like(X1))\n", "    def test_missing_indicator_error_on_new(self):\n        missing_indicator = MissingIndicator(error_on_new=True)\n\n        X1 = np.array([[1, 2, np.nan],\n                       [np.nan, 4, 5],\n                       [6, 7, 8]])\n\n        missing_indicator.fit(X1)\n\n        X2 = np.array([[1, 2, np.nan],\n                       [9, 10, 11],\n                       [12, 13, 14]])\n        with pytest.raises(ValueError):\n            missing_indicator.transform(X2)\n", "    def test_missing_indicator_error_on_new(self):\n        X = np.array([[1, 2, np.nan],\n                      [np.nan, 4, 5],\n                      [6, np.nan, 8]])\n        indicator = MissingIndicator(error_on_new=True, features='missing-only')\n        indicator.fit(X)\n        X_new = np.array([[1, 2, 3],\n                          [4, 5, 6],\n                          [7, 8, 9]])\n\n        with pytest.raises(ValueError):\n            indicator.transform(X_new)\n", "def test_missing_indicator_sparse_missing_values():\n    X = sparse.csc_matrix([[1, 2, np.nan],\n                           [4, 5, 6],\n                           [7, 8, np.nan]])\n    indicator = MissingIndicator(missing_values=np.nan)\n    indicator.fit(X)\n    Xt = indicator.transform(X)\n    expected_mask = sparse.csc_matrix([[False, False, True],\n                                      [False, False, False],\n                                      [False, False, True]])\n    assert_allclose_dense_sparse(Xt, expected_mask, rtol=1e-5)\n", "    def test_missing_indicator_transform_with_new_missing(self):\n        X = np.array([[np.nan, 1, 3],\n                      [4, 0, np.nan],\n                      [8, 1, 0]])\n        indicator = MissingIndicator(features='missing-only')\n        indicator.fit(X)\n\n        X_new = np.array([[np.nan, 2, np.nan],\n                          [4, 0, np.nan],\n                          [8, 1, 0]])\n        indicator_new = indicator.transform(X_new)\n\n        # Check that the new missing values are correctly flagged\n        assert_array_equal(indicator_new, np.array([\n            [True, False, True],\n            [False, False, True],\n            [False, False, False]]))\n"], "sample_817": ["    def test_threshold_zero(self):\n        X = np.array(data)\n        selector = VarianceThreshold(threshold=0.0)\n        selector.fit(X)\n\n        # Ensure all features are kept with threshold=0\n        support = selector.get_support()\n        assert_equal(support, np.ones(X.shape[1], dtype=bool))\n\n\n\n", "    def test_sparse_threshold_zero(self):\n        X_sparse = csc_matrix(data)\n        selector = VarianceThreshold(threshold=0)\n        selector.fit(X_sparse)\n        support = selector._get_support_mask()\n        expected_support = np.array([True, True, True, True, False], dtype=bool)\n        assert_array_equal(support, expected_support)\n\n", "    def test_threshold_zero(self, X):\n        selector = VarianceThreshold(threshold=0)\n        selector.fit(X)\n        mask = selector._get_support_mask()\n        expected_mask = np.any(X != X[0], axis=0)\n        assert_array_equal(mask, expected_mask)\n", "    def test_threshold_zero(self):\n        X = np.array(data)\n        selector = VarianceThreshold(threshold=0.)\n        selector.fit(X)\n        support = selector._get_support_mask()\n        assert_array_equal(support, [True, True, True, True, False])\n", "    def test_variance_threshold_sparse(self):\n        X_csr = csr_matrix(data)\n        X_csc = csc_matrix(data)\n        X_bsr = bsr_matrix(data)\n\n        selector = VarianceThreshold(threshold=1)\n        selector.fit(X_csr)\n        assert_array_equal(selector.get_support(), np.array([True, True, False]))\n\n        selector.fit(X_csc)\n        assert_array_equal(selector.get_support(), np.array([True, True, False]))\n\n        selector.fit(X_bsr)\n        assert_array_equal(selector.get_support(), np.array([True, True, False]))\n"], "sample_412": ["    def test_json_script_urlize(self):\n        value = {\"html\": \"<script>alert('foo')</script>\", \"url\": \"http://www.example.com\"}\n        json_str = json_script(value, element_id='my_script')\n        self.assertIn('<script id=\"my_script\" type=\"application/json\">', json_str)\n        self.assertIn(\n            '\\\\u003Cscript\\\\u003Ealert(\\'foo\\')\\\\u003C/script\\\\u003E', json_str\n        )\n        self.assertIn(\n            smart_urlquote(\"http://www.example.com\").replace('&', '&amp;'),\n            json_str,\n        )\n\n\n\n", "compilation error", "    def test_json_script_escapes(self):\n        self.assertEqual(json_script({\"test\": \"<>&=\"}), '{\"test\":\"\\\\u003C\\\\u003E\\\\u0026\\\\u003D\"}')\n        self.assertEqual(json_script({\"test\": \"  \\t\\n\\r\\0\"}, encoder=DjangoJSONEncoder), '<script id=\"test\" type=\"application/json\">{\"test\":\"  \\t\\n\\r\\0\"}</script>')\n        self.assertEqual(json_script({\"test\": \"  \\t\\n\\r\\0\"}), '<script type=\"application/json\">{\"test\":\"  \\t\\n\\r\\0\"}</script>') \n", "    def test_urlize(self):\n        self.check_output(urlize, \"http://www.example.com\", '<a href=\"http://www.example.com\">http://www.example.com</a>')\n        self.check_output(urlize, \"https://www.example.com\", '<a href=\"https://www.example.com\">https://www.example.com</a>')\n        self.check_output(urlize, \"example.com\", '<a href=\"http://example.com\">example.com</a>')\n        self.check_output(urlize, \"mail@example.com\", '<a href=\"mailto:mail@example.com\">mail@example.com</a>')\n        self.check_output(urlize, \"my long url www.example.com/this/is/a/long/url\", '<a href=\"http://www.example.com/this/is/a/long/url\">my long url www.example.com/this/is/a/long/url</a>')\n        self.check_output(urlize, \"some text http://www.example.com more text\", 'some text <a href=\"http://www.example.com\">http://www.example.com</a> more text')\n        self.check_output(urlize, \"some text www.example.com more text\", 'some text <a href=\"http://www.example.com\">www.example.com</a> more text')\n        self.check_output(urlize, \"some text mail@example.com more text\", 'some text <a href=\"mailto:mail@example.com\">mail@example.com</a> more text')\n\n\n\n", "    def test_urlize(self):\n        self.check_output(urlize, \"http://example.com/\", trim_url_limit=None, nofollow=False, autoescape=False, expected=\"http://example.com/\")\n        self.check_output(urlize, \"www.example.com\", trim_url_limit=None, nofollow=False, autoescape=False, expected=\"<a href='http://www.example.com' rel='nofollow'>www.example.com</a>\")\n        self.check_output(urlize, \"mailto:john.doe@example.com\", trim_url_limit=None, nofollow=False, autoescape=False, expected=\"<a href='mailto:john.doe@example.com'>mailto:john.doe@example.com</a>\")\n        self.check_output(urlize, \"https://example.com/some/path\", trim_url_limit=None, nofollow=False, autoescape=False, expected=\"<a href='https://example.com/some/path' rel='nofollow'>https://example.com/some/path</a>\")\n        self.check_output(urlize, \"John Doe <john.doe@example.com>\", trim_url_limit=None, nofollow=False, autoescape=False, expected=\"John Doe <a href='mailto:john.doe@example.com'>john.doe@example.com</a>\")\n        self.check_output(urlize, \"This is a URL: http://example.com\", trim_url_limit=None, nofollow=False, autoescape=False, expected=\"This is a URL: <a href='http://example.com' rel='nofollow'>http://example.com</a>\")\n        self.check_output(urlize, \"This is a URL: https://example.com/some/path\", trim_url_limit=None, nofollow=False, autoescape=False, expected=\"This is a URL: <a href='https://example.com/some/path' rel='nofollow'>https://example.com/some/path</a>\")\n        self.check_output(urlize, \"This is a URL: www.example.com\", trim_url_limit=None, nofollow=False,"], "sample_390": ["    def test_was_modified_since(self):\n        with self.subTest(header=None):\n            self.assertTrue(was_modified_since(header=None, mtime=1678886400, size=100))\n\n        with self.subTest(header_older):\n            self.assertTrue(was_modified_since(header=\"Wed, 15 Mar 2023 12:00:00 GMT\", mtime=1678886400, size=100))\n\n        with self.subTest(header_newer):\n            self.assertFalse(was_modified_since(header=\"Wed, 15 Mar 2023 13:00:00 GMT\", mtime=1678886400, size=100))\n\n        with self.subTest(header_len_mismatch):\n            self.assertTrue(was_modified_since(header=\"Wed, 15 Mar 2023 12:00:00 GMT; length=99\", mtime=1678886400, size=100))\n\n\n\n", "    def test_was_modified_since_no_header(self):\n        self.assertTrue(was_modified_since(header=None, mtime=1678886400, size=100))\n", "    def test_was_modified_since_not_modified(self):\n        self.assertTrue(was_modified_since(header=\"Sat, 21 Oct 2023 07:28:00 GMT\", mtime=1697923680, size=1024))\n", "    def test_was_modified_since_not_modified(self):\n        self.assertTrue(was_modified_since(\n            header=http_date(1678886400),\n            mtime=1678886400,\n            size=1024\n        ))\n", "    def test_serve_modified(self):\n        url = f\"/{self.prefix}/testfile.txt\"\n        with open(path.join(media_dir, \"testfile.txt\"), \"w\") as f:\n            f.write(\"test content\")\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response[\"Content-Type\"], mimetypes.guess_type(\"testfile.txt\")[0])\n        modified_time = http_date(path.getmtime(path.join(media_dir, \"testfile.txt\")))\n        with open(path.join(media_dir, \"testfile.txt\"), \"w\") as f:\n            f.write(\"modified content\")\n        response = self.client.get(url, HTTP_IF_MODIFIED_SINCE=modified_time)\n        self.assertEqual(response.status_code, 200)\n        response = self.client.get(url, HTTP_IF_MODIFIED_SINCE=modified_time)\n        self.assertEqual(response.status_code, 304)\n"], "sample_138": ["    def test_manifest_strict_missing_file(self):\n        path = 'my_missing_file.css'\n        with self.assertRaises(ValueError) as context:\n            staticfiles_storage.stored_name(path)\n        self.assertIn(\n            f\"Missing staticfiles manifest entry for '{path}'\", str(context.exception)\n        )\n\n", "    def test_manifest_caching(self):\n        self.client.login(username=\"admin\", password=\"password\")\n        with tempfile.TemporaryDirectory() as tempdir:\n            os.mkdir(os.path.join(tempdir, 'static'))\n            try:\n                # Create static files\n                with open(os.path.join(tempdir, 'static/test1.css'), 'w') as f:\n                    f.write('body { background-color: red; }')\n                with open(os.path.join(tempdir, 'static/test2.js'), 'w') as f:\n                    f.write('console.log(\"Hello world!\");')\n\n                # Collect static files and populate the manifest\n                call_command('collectstatic', '--noinput',\n                             site_root=tempdir)\n                \n                # Initial manifest should be empty\n                manifest = json.loads(self.client.get('/').content)\n                self.assertEqual(manifest, {'paths': {}})\n                \n                # Make a change to test1.css\n                with open(os.path.join(tempdir, 'static/test1.css'), 'w') as f:\n                    f.write('body { background-color: blue; }')\n\n                # Collect static files again\n                call_command('collectstatic', '--noinput',\n                             site_root=tempdir)\n                \n                # Now the manifest should contain the updated paths\n                manifest = json.loads(self.client.get('/').content)\n                self.assertIn('test1.css', manifest['paths'])\n\n            finally:\n                shutil.rmtree(tempdir)\n\n\n\n\n", "    def test_manifest_cache_update(self):\n        \"\"\"\n        Test that the manifest is updated correctly when files are added or\n        modified.\n        \"\"\"\n        self.maxDiff = None\n\n        call_command('collectstatic', interactive=False, dry_run=True)\n        original_manifest = self.read_manifest()\n\n        # Add a new file\n        self.create_file(\n            'mysite/static/css/new_file.css',\n            content=\"body { background-color: blue; }\"\n        )\n\n        call_command('collectstatic', interactive=False, dry_run=True)\n        updated_manifest = self.read_manifest()\n        self.assertIn('new_file.css', updated_manifest)\n\n        # Modify an existing file\n        self.update_file(\n            'mysite/static/css/style.css',\n            content=\"body { background-color: red; }\"\n        )\n\n        call_command('collectstatic', interactive=False, dry_run=True)\n        updated_manifest = self.read_manifest()\n        self.assertEqual(updated_manifest['style.css'],\n                         original_manifest['style.css'])\n", "    def test_manifest_integrity(self):\n        with tempfile.TemporaryDirectory() as tempdir:\n            os.makedirs(os.path.join(tempdir, \"static\", \"css\"), exist_ok=True)\n            with open(os.path.join(tempdir, \"static\", \"css\", \"test.css\"), \"w\") as f:\n                f.write(\"body { background-color: #f0f0f0; }\")\n\n            call_command('collectstatic',\n                         interactive=False,\n                         verbosity=0,\n                         clear=True,\n                         storage=staticfiles_storage,\n                         target_dir=tempdir)\n            manifest = json.loads(\n                open(os.path.join(tempdir, \"staticfiles.json\")).read()\n            )\n            self.assertEqual(manifest['paths'], {'css/test.css': hashed_file_path(self, 'static/css/test.css')})\n\n            # Modify the file content\n            with open(os.path.join(tempdir, \"static\", \"css\", \"test.css\"), \"w\") as f:\n                f.write(\"body { background-color: #0f0; }\")\n\n            # Recollectstatic to rebuild the manifest\n            call_command('collectstatic',\n                         interactive=False,\n                         verbosity=0,\n                         clear=True,\n                         storage=staticfiles_storage,\n                         target_dir=tempdir)\n\n            manifest = json.loads(\n                open(os.path.join(tempdir, \"staticfiles.json\")).read()\n            )\n            self.assertEqual(manifest['paths'], {'css/test.css': hashed_file_path(self, 'static/css/test.css')})\n\n\n\n", "    def test_manifest_generation(self):\n        with self.subTest(name=\"manifest_generation\"):\n            # Arrange\n            self.assert_staticfiles_storage(ManifestStaticFilesStorage)\n            test_content = b'Test content'\n            temp_path = tempfile.mkdtemp()\n            with open(os.path.join(temp_path, 'test.css'), 'wb') as f:\n                f.write(test_content)\n\n            # Act\n            call_command('collectstatic', ignore_invalid_urls=True, \n                         static_dirs=[temp_path])\n\n            # Assert\n            with open(os.path.join(temp_path, 'staticfiles.json')) as f:\n                manifest = json.load(f)\n            self.assertEqual(manifest['paths'], {'test.css': 'test.css'})\n\n            self.assert_staticfiles_storage(FileSystemStorage)\n            self.assertTrue(\n                self.hashed_file_path('test.css').exists()\n            )\n\n\n\n"], "sample_673": ["    def test_doctest_ignore_import_errors(self, testdir):\n        p = testdir.makepyfile(\n            \"\"\"\n                pass\n\n                assert True\n            \"\"\"\n        )\n\n        result = testdir.runpytest(\n            \"-k\", \"doctest\", \"--doctest_ignore_import_errors\", p\n        )\n\n        assert result.ret == 0\n        # testdir.parseoutcomes(result.stdout)\n", "    def test_doctest(self, doctest_file, tmpdir, doctest_namespace):\n        with open(tmpdir / doctest_file, 'w') as f:\n            f.write(textwrap.dedent(f\"\"\"\n            >>> 1 + 1 == 2\n            True\n            >>> 2 * 2 == 4\n            True\n            \"\"\"))\n\n        pyproject = tmpdir / 'pyproject.toml'\n        with open(pyproject, 'w') as f:\n            f.write(\"\"\"\n                [tool.pytest]\n                doctest_modules = true\n                doctest_glob = {doctest_file}\n            \"\"\")\n\n        config = pytest.config.parse(args=['--doctest-modules'],\n                                     rootdir=str(tmpdir))\n        \n        # Run the tests and check if they pass\n        rep = config.pluginmanager.run(config.hook.pytest_cmdline_main(args=['--doctest-modules']))\n        assert rep.exitstatus == 0\n        \n        items = rep.get_items(\n            lambda item: isinstance(item, DoctestModule) or isinstance(item, DoctestTextfile)\n        )\n        assert len(items) == 1\n\n", "    def test_doctest_collect(self, doctest_namespace):\n        import doctest\n        parent = pytest.Module()\n        fspath = py.path.local(\"test.txt\")\n        fspath.write(\n            textwrap.dedent(\n                \"\"\"\n                >>> 1 + 1\n                2\n                \"\"\"\n            )\n        )\n        with _patch_unwrap_mock_aware():\n            doctestmod = DoctestModule(fspath, parent, fspath)\n            items = list(doctestmod.collect())\n\n        assert len(items) == 1\n        item = items[0]\n        assert isinstance(item, DoctestItem)\n        assert item.name == \"test doctest_collect\"\n        assert item.runner is not None\n        assert item.dtest is not None\n        assert item.dtest.examples[0].want == 2\n", "    def test_doctest_ignore_import_errors(self, pytestconfig):\n        pytestconfig.parent.set({\"doctest_ignore_import_errors\": True})\n        with pytest.raises(pytest.skip.Exception) as excinfo:\n            DoctestModule.from_parent(\n                parent=DoctestModule, name=\"test\", runner=_get_runner(), dtest=None\n            )\n        assert \"unable to import module\" in str(excinfo.value)\n\n        pytestconfig.parent.set({\"doctest_ignore_import_errors\": False})\n        with pytest.raises(ImportError):\n            DoctestModule.from_parent(\n                parent=DoctestModule, name=\"test\", runner=_get_runner(), dtest=None\n            )\n", "    def test_doctest_collect_setup_py(monkeypatch, testdir):\n        p = testdir.makepyfile(\n            \"\"\"\n            setup_py\n\n            setup_py\n            \"\"\"\n        )\n        result = testdir.runpytest(p)\n        result.assert_outcomes(passed=1)\n\n"], "sample_176": ["    def test_complex_relationship_changes(self):\n        before_states = [\n            author_with_book,\n            book,\n            attribution,\n        ]\n        after_states = [\n            author_with_book,\n            book_with_multiple_authors,\n            attribution,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, 'otherapp', 1)\n        self.assertMigrationDependencies(changes, 'otherapp', 0, ['testapp'])\n        self.assertOperationTypes(changes, 'otherapp', 0, ['RemoveField', 'AddField'])\n        self.assertOperationAttributes(\n            changes, 'otherapp', 0, 0, field='author', model_name='Book'\n        )\n        self.assertOperationAttributes(\n            changes, 'otherapp', 0, 1, field='authors', model_name='Book'\n        )\n", "    def test_migration_with_indexes_and_together(self):\n        before_states = [\n            book_indexes,\n            book_unordered_indexes,\n            book_foo_together,\n            book_foo_together_2,\n            book_foo_together_3,\n            book_foo_together_4,\n            # other models\n        ]\n        after_states = [\n            book_indexes,\n            book_unordered_indexes,\n            book_foo_together,\n            book_foo_together_2,\n            book_foo_together_3,\n            book_foo_together_4,\n            # other models\n        ]\n        changes = self.get_changes(before_states, after_states)\n\n        self.assertNumberMigrations(changes, 'otherapp', 1)\n        self.assertMigrationDependencies(changes, 'otherapp', 0, [])\n\n        self.assertOperationTypes(changes, 'otherapp', 0, [\n            'RemoveField',\n            'AddField',\n        ])\n        self.assertOperationAttributes(changes, 'otherapp', 0, 0, model_name='Book', field_name='newfield')\n        self.assertOperationAttributes(changes, 'otherapp', 0, 1, model_name='Book', field_name='title', primary_key=False)\n\n        self.assertOperationTypes(changes, 'otherapp', 0, [\n            'RemoveField',\n            'AddField',\n        ])\n        self.assertOperationAttributes(changes, 'otherapp', 0, 0, model_name='Book', field_name='newfield2')\n        self.assertOperationAttributes(changes, 'otherapp', 0, 1, model_name='Book', field_name='title', primary_key=False)\n", "    def test_migration_dependencies_with_circular_references(self):\n        before_states = [\n            rabbit,\n        ]\n        after_states = [\n            rabbit,\n        ]\n\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, 'eggs', 1)\n        self.assertMigrationDependencies(changes, 'eggs', 0, []) \n\n\n", "    def test_field_attribute_changes(self):\n        before_states = [\n            author_name_deconstructible_list_1,\n            publisher_with_author,\n            book,\n        ]\n        after_states = [\n            author_name_deconstructible_list_2,\n            publisher_with_author,\n            book,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertOperationTypes(changes, 'testapp', 0, ['RemoveField'])\n        self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, **{'max_length': 200, 'null': True})\n\n\n\n", "    def test_detect_changes_proxy_table_rename(self):\n        before_states = [\n            author_proxy,\n            author_proxy_options,\n            other_pony,\n            other_stable,\n            book,\n            book_proxy_fk,\n            book_proxy_proxy_fk,\n        ]\n        after_states = [\n            author_renamed_with_db_table_options,\n            author_proxy_options,\n            other_pony,\n            other_stable,\n            book,\n            book_proxy_fk,\n            book_proxy_proxy_fk,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertMigrationDependencies(changes, \"testapp\", 0, [\"migrations.UnmigratedModel\"])\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameModel\"])\n        self.assertOperationFieldAttributes(\n            changes, \"testapp\", 0, 0, db_table=\"author_one\"\n        )\n\n\n\n"], "sample_952": ["    def test_type_alias_namespace():\n        mapping = {\n            \"mod1.mod2.Class\": \"Class\",\n            \"mod1.mod3.Class\": \"OtherClass\",\n            \"mod1.mod2.submodule.SubClass\": \"SubClass\",\n            \"mod2.Class\": \"Class\",\n        }\n        namespace = TypeAliasNamespace(mapping)\n\n        assert namespace[\"mod1.mod2.Class\"] == \"Class\"\n        assert namespace[\"mod1.mod3.Class\"] == \"OtherClass\"\n        assert namespace[\"mod1.mod2.submodule.SubClass\"] == \"SubClass\"\n        assert namespace[\"mod2.Class\"] == \"Class\"\n\n        with pytest.raises(KeyError):\n            namespace[\"nonexistent\"]\n", "    def test_signature_from_ast_with_annotation():\n        code = \"\"\"", "    def test_signature_from_ast_with_defaults(self):\n        code = \"\"\"", "    def test_stringfy_signature_empty():\n        sig = inspect.Signature()\n        assert stringify_signature(sig) == '()'\n", "    def test_evaluate_signature_forwardref(self):\n        \"\"\"Test evaluate_signature with forward reference.\"\"\"\n        type_aliases = {\n            \"mod1.Class\": \"typing.List[str]\",\n        }\n        sig = inspect.signature(partial(wrapped_function, mod1.Class))\n    \n        evaluated_sig = inspect.Signature.from_callable(wrapped_function, follow_wrapped=True)\n\n        evaluated_sig = inspect.Signature.from_callable(wrapped_function, follow_wrapped=True)\n\n        evaluated_sig = inspect.signature(wrapped_function, follow_wrapped=True)\n        evaluated_sig = evaluate_signature(evaluated_sig, globalns=globals(), localns=locals())\n        self.assertEqual(evaluated_sig.parameters['arg'].annotation, typing.List[str])\n\n\n\n"], "sample_944": ["    def test_restify_typing_Callable(self):\n        from typing import Callable, Optional\n            return []\n\n        assert restify(my_func) == \"Callable[[int, str, Optional[float]], List[bool]]\"\n\n\n", "    def test_restify_typing_Callable(self):\n        assert restify(Callable[[T], Any]) == ':class:`Callable[[T], Any]`'\n        assert restify(Callable[[int], str]) == ':class:`Callable[[int], str]`'\n        assert restify(Callable[[T], T]) == ':class:`Callable[[T], T]`'\n        assert restify(Callable[[], None]) == ':class:`Callable[\\[\\], None]`'\n        assert restify(Callable[[int], T]) == ':class:`Callable[[int], Any]`'\n", "    def test_restify_typing_union_with_none(self):\n        assert restify(Union[int, str, None]) == 'Optional[int | str]'\n", "    def test_restify_callable(self):\n            pass\n        assert restify(my_callable) == (\n            ':obj:`Callable`[[int, str, Optional[float]], Tuple[str, int]]'\n        )", "    def test_restify_complex_types(self):\n        from sphinx.util import inspect  # lazy loading\n\n        assert restify(MyList) == 'List[T]'\n        assert restify(BrokenType) == 'BrokenType'\n        assert restify(Union[int, str]) == 'Union[int, str]'\n        assert restify(Union[MyInt, str]) == 'Union[MyInt, str]'\n        assert restify(Optional[Union[int, str]]) == 'Optional[Union[int, str]]'\n        assert restify(Callable[[int, str], int]) == 'Callable[[int, str], int]'\n        assert restify(typing.Generator[int, str, int]) == 'Generator[int, str, int]'\n\n        # Test for issue https://github.com/sphinx-doc/sphinx/issues/8084\n        class BrokenGeneric(typing.GenericMeta):\n            __origin__ = list\n            __args__ = (int, str)\n\n        assert restify(BrokenGeneric) == 'list[int, str]'\n\n\n\n\n"], "sample_1155": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_366": ["    def test_parse_datetime_postgres_interval(self):\n        self.assertEqual(parse_datetime('3 days 04:05:06'), datetime(2023, 10, 27, 4, 5, 6, tzinfo=get_fixed_timezone(0)))\n        self.assertEqual(parse_datetime('-3 days 04:05:06'), datetime(2023, 10, 24, 4, 5, 6, tzinfo=get_fixed_timezone(0)))\n        self.assertEqual(parse_datetime('04:05:06'), datetime(2023, 10, 27, 4, 5, 6, tzinfo=get_fixed_timezone(0)))\n", "    def test_parse_datetime_with_timezone_offset(self):\n        dt = parse_datetime(\"2023-12-26T12:00:00+05:30\")\n        self.assertEqual(dt.year, 2023)\n        self.assertEqual(dt.month, 12)\n        self.assertEqual(dt.day, 26)\n        self.assertEqual(dt.hour, 12)\n        self.assertEqual(dt.minute, 0)\n        self.assertEqual(dt.second, 0)\n        self.assertEqual(dt.microsecond, 0)\n        self.assertEqual(dt.tzinfo.utcoffset(dt).seconds, 19800) \n", "    def test_parse_datetime_postgres_interval(self):\n        value = '3 days 04:05:06.123456'\n        expected_datetime = datetime(2023, 1, 1, 4, 5, 6, 123456, tzinfo=get_fixed_timezone(0))\n        self.assertEqual(parse_datetime(value), expected_datetime)\n", "    def test_parse_datetime_with_timezone(self):\n        dt = parse_datetime('2023-10-26T14:30:00+05:00')\n        self.assertEqual(dt.date(), date(2023, 10, 26))\n        self.assertEqual(dt.time(), time(14, 30))\n        self.assertEqual(dt.tzinfo.utcoffset(dt), timedelta(hours=5))\n", "    def test_parse_datetime_with_timezone(self):\n        dt = parse_datetime('2023-08-15T12:34:56+05:00')\n        self.assertEqual(dt.year, 2023)\n        self.assertEqual(dt.month, 8)\n        self<h1>assertEqual(dt.day, 15)\n        self.assertEqual(dt.hour, 12)\n        self.assertEqual(dt.minute, 34)\n        self.assertEqual(dt.second, 56)\n        self.assertEqual(dt.tzinfo, get_fixed_timezone(300))  # 5 hours offset\n"], "sample_360": ["    def test_update_cache_middleware_with_vary_header(self):\n        # If the response has a Vary header, it shouldn't be cached\n        # unless the request's headers match what was used for caching\n        request = HttpRequest()\n        request.COOKIES = {'session': '123'}\n        response = HttpResponse(\"Hello, world!\")\n        response['Vary'] = 'Cookie'\n        with patch_vary_headers(response, 'Cookie'):\n            update_middleware = UpdateCacheMiddleware(lambda x: x)\n            response = update_middleware.process_response(request, response)\n            self.assertEqual(response.status_code, 200)\n            \n        # Now check that a subsequent GET request without the cookie\n        # doesn't get served from cache\n\n        request_no_cookie = HttpRequest()\n        response_no_cookie = HttpResponse(\"Hello, world!\")\n        with patch_vary_headers(response_no_cookie, 'Cookie'):\n\n            response_no_cookie = update_middleware.process_response(request_no_cookie, response_no_cookie)\n            self.assertEqual(response_no_cookie.status_code, 200)\n\n", "    def test_cache_middleware_with_fetchfromcache_and_updateCache_middleware(self):\n        request = HttpRequest()\n        response = HttpResponse(\"Hello, world!\")\n        \n        # Mock the get_response function\n            return response\n\n        # Create a new instance of CacheMiddleware\n        middleware = CacheMiddleware(get_response)\n\n        # Process the request and make sure the response is cached\n        middleware.process_request(request)\n        cached_response = middleware.process_response(request, response)\n\n        self.assertEqual(cached_response, response)\n        self.assertIn(\n            \"Content-Type\", cached_response['Content-Type'],\n        )\n        self.assertIn(\n            \"Expires\", cached_response['Expires'],\n        )\n        self.assertIn(\n            \"Cache-Control\", cached_response['Cache-Control'],\n        )\n\n        \n        # Test when the response requires updating the cache\n        response = HttpResponse(\"Goodbye, world!\")\n        cached_response = middleware.process_response(request, response)\n        self.assertEqual(cached_response, response)\n\n", "    def test_cache_key_with_special_characters(self):\n        key_with_spaces = 'this key has spaces'\n        key_with_quotes = 'key \"with quotes\"'\n        key_with_braces = 'key {with braces}'\n\n        with self.assertRaisesRegex(InvalidCacheKey, r'Cache key cannot contain spaces, quotes or braces'):\n            get_cache_key(HttpRequest(), 'myprefix', 'GET', cache=caches['default'], request=HttpRequest(), key_prefix='myprefix')\n", "    def test_cache_middleware_with_custom_key_prefix(self):\n        # Test that custom key prefix is applied\n        with override_settings(CACHE_MIDDLEWARE_KEY_PREFIX='test:prefix:'):\n            middleware = CacheMiddleware(get_response=lambda x: x)\n            request = HttpRequest()\n            response = middleware.process_request(request)\n            self.assertEqual(response, None)  # no cached response yet\n            request.path = '/'\n            response = middleware.process_request(request)\n            self.assertEqual(response, None)  # no cached response yet\n\n            # Simulate update in the next middleware\n            request._cache_update_cache = True\n            response = middleware.process_response(request, HttpResponse('Hello'))\n            self.assertTrue(response is not None)\n            self.assertTrue(response.cache_key.startswith('test:prefix: '))\n\n", "    def test_cache_key_with_special_chars(self):\n        with self.subTest(\"Test for key with special chars\"):\n            key = get_cache_key(\n                HttpRequest(),\n                'test_cache_key_with_special_chars',\n                'GET'\n            )\n            self.assertEqual(key, 'test_cache_key_with_special_chars')\n            \n        with self.subTest(\"Test for key with unicode chars\"):\n            key = get_cache_key(\n                HttpRequest(),\n                'test_cache_key_with_unicode_chars',\n                'GET'\n            )\n            self.assertEqual(key, 'test_cache_key_with_unicode_chars')\n        \n        with self.subTest(\"Test for key without prefix\"):\n            key = get_cache_key(\n                HttpRequest(),\n                '',\n                'GET',\n                key_prefix=''\n            )\n            self.assertEqual(key, '')\n"], "sample_653": ["    def test_colored_level_formatter(self):\n        formatter = ColoredLevelFormatter(create_terminal_writer(pytest.config), \"testformat\", \"testdateformat\")\n        assert formatter.FORMAT == \"testformat\"\n        assert formatter.DATE_FORMAT == \"testdateformat\"\n\n        record = logging.LogRecord(\n            name=\"test\",\n            levelname=\"DEBUG\",\n            pathname=\"test.py\",\n            lineno=1,\n            msg=\"test message\",\n            args=(),\n            exc_info=None,\n        )\n\n        assert formatter.format(record) == formatter.LEVELNAME_FMT_REGEX.sub(\n            \"DEBUG\", formatter._fmt % {\"levelname\": \"DEBUG\"}\n        )\n\n\n", "    def test_log_file_handler_path_setter(self, caplog):\n        original_path = caplog.log_file_handler.baseFilename\n        caplog.set_log_path(\"log.txt\")\n        assert caplog.log_file_handler.baseFilename != original_path\n        assert caplog.log_file_handler.baseFilename == \"log.txt\"\n        caplog.set_log_path(None)\n        assert caplog.log_file_handler is None\n", "    def test_set_log_path(self):\n        from _pytest.config import Config\n\n        config = Config()\n        plugin = LoggingPlugin(config)\n        plugin.set_log_path(\"testlog.log\")\n\n        assert os.path.exists(\"testlog.log\")\n        plugin.set_log_path(\"new_testlog.log\")\n        assert os.path.exists(\"new_testlog.log\")\n        assert not os.path.exists(\"testlog.log\")\n", "    def test_caplog_set_log_path(self, tmp_path):\n        plugin = LoggingPlugin(pytest.config)\n        plugin.set_log_path(str(tmp_path / \"test.log\"))\n        plugin._finalize()\n        assert os.path.exists(tmp_path / \"test.log\")\n", "    def test_set_log_path(self):\n        plugin = LoggingPlugin(self.config)\n        plugin.set_log_path(\"test_log.txt\")\n        assert os.path.exists(\"test_log.txt\")\n        with open(\"test_log.txt\", \"r\", encoding=\"utf-8\") as f:\n            assert f.read() == \"\"  # Should be empty as no log messages yet\n        plugin.set_log_path(\"test_log.txt\")  # Repeated call\n        assert os.path.exists(\"test_log.txt\")\n        os.remove(\"test_log.txt\")\n"], "sample_1101": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1025": ["    def test_sign(self):\n        self.assertEqual(pycode(sign(x)), 'sympy.sign(x)')\n        self.assertEqual(pycode(sign(x + 1)), 'sympy.sign(x + 1)')\n        self.assertEqual(pycode(sign(-x)), '-sympy.sign(x)')\n        self.assertEqual(pycode(sign(0)), '0')\n\n\n\n", "    def test_Piecewise_with_default(self):\n        expr = Piecewise((x > 1, 2*x, True, 3))\n        code = pycode(expr, pretty_print=False)\n        self.assertEqual(code, 'numpy.select(numpy.array([x > 1]), numpy.array([2*x]), numpy.array([3]), default=numpy.nan)')\n\n", "    def test_numpy_sparse_matrix(self):\n        m = SparseMatrix([[1, 2], [3, 4]], dtype=int)\n        code = pycode(m, printer=NumPyPrinter())\n        assert code == 'scipy.sparse.coo_matrix([1, 2, 3, 4], ((0, 0), (0, 1), (1, 0), (1, 1)), shape=(2, 2))'\n\n", "    def test_sparse_matrix(self):\n        A = SparseMatrix(((1, 2, 3),), ((0, 1, 2),))\n        s = SciPyPrinter().doprint(A)\n        assert s == \"scipy.sparse.coo_matrix([[1, 0, 0], [0, 2, 0], [0, 0, 3]], shape=(3, 3))\"\n", "    def test_sparse_matrix(self):\n        A = SparseMatrix([[1, 2], [3, 4]])\n        s = SciPyPrinter().doprint(A)\n        assert s == \"scipy.sparse.coo_matrix(( [1, 2, 3, 4], ( [0, 0, 1, 1], [0, 1, 0, 1])), shape=(2, 2))\"\n\n        A = SparseMatrix([[1, 0, 0], [0, 2, 0]], 'A')\n        s = SciPyPrinter().doprint(A)\n        assert s == \"A = scipy.sparse.coo_matrix(( [1, 2], ( [0, 0], [0, 1])), shape=(2, 3))\"\n\n        A = SparseMatrix([[1, 0, 0], [0, 2, 0]], 'A')\n        s = SciPyPrinter().doprint(A)\n        assert s == \"A = scipy.sparse.coo_matrix(( [1, 2], ( [0, 0], [0, 1])), shape=(2, 3))\"\n"], "sample_666": ["    def test_capture_manager_capture_stdout_stderr_inherit(self, method):\n        if method == \"no\":\n            capture_manager = CaptureManager()\n\n        else:\n            capture_manager = capture.MultiCapture(\n                out=getattr(capture, method + \"Capture\")(),\n                err=getattr(capture, method + \"Capture\")(),\n            )\n\n        with capture_manager:\n            print(\"This message will be captured\", file=sys.stdout)\n            print(\"This message will be captured\", file=sys.stderr)\n\n        stdout, stderr = capture_manager.get_captured_output()\n        assert stdout.strip() == \"This message will be captured\"\n        assert stderr.strip() == \"This message will be captured\"\n\n\n\n", "    def test_capture_manager_multiple_captures(self, method):\n        manager = CaptureManager(method)\n        with manager:\n            capture_list = _get_multicapture(manager.global_capture)\n            capture_list.write(\"Hello, world!\\n\")\n            capture_list.write(\"This is a second line.\\n\")\n            assert capture_list.getvalue() == \"Hello, world!\\nThis is a second line.\\n\"\n\n", "    def test_capture_manager_no_capture(self, method):\n        config = capture.create_config(\n            \"test\",\n            \".\",\n            sys.argv,\n            _warn_deprecated=lambda x: None,\n        )\n        capture_manager = capture.CaptureManager(config)\n        capture_manager.add_capture(capture.MultiCapture(out=False, err=False, in_=False))\n        with capture_manager:\n            capture.capture_output(capture.MultiCapture(out=False, err=False, in_=False))\n            assert not capture_manager.has_capture()\n\n", "    def test_write_to_captured_stdout(self, method):\n        \"\"\"Test that writing to stdout is captured correctly.\n        \"\"\"\n        with StdCapture(out=True) as cap:\n            sys.stdout.write(\"hello\\n\")\n            sys.stdout.flush()\n        captured = cap.readouterr()\n        assert captured.out == \"hello\\n\"\n\n\n        if method == \"fd\":\n            with StdCaptureFD(out=True) as cap:\n                sys.stdout.write(\"hello\\n\")\n                sys.stdout.flush()\n            captured = cap.readouterr()\n            assert captured.out == \"hello\\n\"\n", "    def test_capture_manager_with_custom_capture_methods(self, method):\n        capture_method = {\n            \"sys\": capture.SysCapture,\n            \"fd\": capture.FDCapture,\n            \"no\": None,\n        }[method]\n\n        manager = CaptureManager()\n        multicapture = MultiCapture(out=capture_method(), err=capture_method())\n\n        with multicapture as capture:\n            print(\"hello\", file=capture.out)\n            print(\"world\", file=capture.err)\n\n        assert capture.out.getvalue() == \"hello\\n\"\n        assert capture.err.getvalue() == \"world\\n\"\n"], "sample_967": ["    def test_mathjax_config_options(app, status, warnings):\n        app.add_config_value('mathjax_options', {'messageStyle': 'none'}, 'html')\n        app.builder.build()\n        assert_node(status, 'div', CLASS='math notranslate nohighlight')\n        js = status.body.get('html')\n        assert '<script src=\"' + MATHJAX_URL + '\" defer></script>' in js\n        assert 'MathJax.Hub.Config({' in js\n        assert 'messageStyle: \"none\"' in js\n\n", "def test_mathjax_math_inline_rendering(app, status):\n    app.builder.math_renderer_name = 'mathjax'\n    app.config.mathjax_path = MATHJAX_URL\n\n    # Write a test document with inline math\n    source = \"\"\"This is a test with inline math: $a + b$.\"\"\"\n    app.builder.build_finished()\n    html = open(app.outdir / 'test_math_inline.html').read()\n\n    # Check that the math is rendered correctly\n    assert 'a + b' in html, \"Expected to find 'a + b' in HTML output\"\n    assert '<span class=\"math notranslate nohighlight\">' in html\n    assert '</span>' in html\n\n", "    def test_mathjax_config(app, status, warnings):\n        app.config.mathjax_path = MATHJAX_URL\n        app.config.mathjax3_config = {'messageStyle': 'none'}\n        app.build()\n        output = status.getvalue()\n        assert r\"window.MathJax = {messageStyle:\\\"none\\\"}\" in output\n", "    def test_mathjax_url(app, status, doctest_dir):\n        assert app.config['mathjax_path'] == MATHJAX_URL\n\n        app.build()\n        with open(f\"{doctest_dir}/html/index.html\") as f:\n            html = f.read()\n        assert '<script src=\"' + MATHJAX_URL + '\" defer></script>' in html\n", "    def test_equation_numbering(self, app, status, warning):\n        app.builder.config.mathjax_path = MATHJAX_URL\n        app.builder.build_all()\n\n        page = self.get_page('index')\n        nodes = page.content\n\n        # Check for the presence of an equation number\n        assert_node(nodes, nodes.math, text='$E=mc^2$', classes=['math', 'notranslate', 'nohighlight'],\n                    source='E=mc^2', attributes={'number': 'eq1'})\n\n        # Check for the permalink to the equation\n        assert_node(nodes, nodes.a, text=_('Permalink to this equation'),\n                    classes=['permalink'],\n                    attributes={'href': '#eq1'})\n"], "sample_1156": ["    def test_inverse_argument_index(self):\n        from sympy.functions.special.hyperbolic import acoth\n        acoth(x).fdiff(2)\n\n", "    def test_asech_expand(self):\n        from sympy.functions.special.gamma_functions import gamma\n        x = Symbol('x')\n        f = asech(x)\n        e = expand(f)\n        e_expected = log(1/x + sqrt(1/x - 1) * sqrt(1/x + 1)) \n        self.assertEqual(e, e_expected)\n        \n        e = f.expand(func=True)\n        e_expected = asech(x)\n        self.assertEqual(e, e_expected)\n        \n        e = f.expand(deep=True)\n        self.assertEqual(e, e_expected)\n\n        e = f.expand(log=True)\n        e_expected = log(1/x + sqrt(1/x - 1) * sqrt(1/x + 1))\n        self.assertEqual(e, e_expected)\n        \n        e = f.expand(trig=True)\n        self.assertTrue(isinstance(e, asech))\n        \n\n\n\n", "    def test_asech_special_cases():\n        for arg in [S.Infinity, S.NegativeInfinity, 0, S.One]:\n            asech(arg).should_be_comparable(asech(arg))\n        for arg in [S.ImaginaryUnit, -S.ImaginaryUnit]:\n            asech(arg).should_be_comparable(asech(arg))\n        asech(S.NaN).should_be_comparable(asech(S.NaN))\n\n\n\n", "    def test_asech_expansion():\n        from sympy.functions.elementary.hyperbolic import asech\n        x = Symbol('x')\n        assert asech(x).expansion_term(0, x) == log(2/x)\n        assert asech(x).expansion_term(1, x) == 0\n        assert asech(x).expansion_term(2, x) == -1/4 * x**2\n        assert asech(x).expansion_term(3, x) == 0\n        assert asech(x).expansion_term(4, x) == 1/32 * x**4\n        assert asech(x).expansion_term(5, x) == 0\n        assert asech(x).expansion_term(6, x) == -1/192 * x**6\n        assert asech(x).expansion_term(7, x) == 0\n        assert asech(x).expansion_term(8, x) == 1/3840 * x**8\n\n\n\n", "    def test_asech_complex():\n        from sympy.functions.hyperbolic import asech\n        z = asech(S.ImaginaryUnit * 2)\n        assert z == -S.Pi / 6\n"], "sample_1141": ["    def test_kronecker_delta_expr(self):\n        k = symbols('k')\n        i, j = symbols('i j', integer=True)\n        self.assertEqual(KroneckerDelta(i, j), S.One if i == j else S.Zero)\n        self.assertEqual(KroneckerDelta(i, k), S.One if i == k else S.Zero)\n        self.assertEqual(KroneckerDelta(k, j), S.One if k == j else S.Zero)\n        \n        expr = Add(KroneckerDelta(i, k) * A[i, j], KroneckerDelta(j, k) * B[k, l])\n        self.assertEqual(expr, A[k, j] + B[k, l])\n        \n        expr = Mul(KroneckerDelta(i, j), x)\n        self.assertEqual(expr, x if i == j else S.Zero)\n        \n        expr = Sum(KroneckerDelta(i, j) * x**i, (i, 0, 5))\n        self.assertEqual(expr, x**j)\n    ", "    def test_inv_sym_matrix(self):\n        A = Matrix([[1, 2], [3, 4]])\n        A_inv = Inverse(A)\n        self.assertTrue(A * A_inv == Identity(2))\n\n\n", "    def test_multiply_matrix_symbolic_matrix(self):\n        C = Matrix([[1, 2], [3, 4]])\n        D = Matrix([[5, 6], [7, 8]])\n        result = C * D\n        expected = Matrix([[19, 22], [43, 50]])\n        self.assertEqual(result, expected)\n", "    def test_matpow_scalar():\n        raises(TypeError, lambda: MatPow(A, 'foo'))\n        raises(ValueError, lambda: MatPow(A, -2.5))\n        raises(ValueError, lambda: MatPow(A, -1))\n        M = ImmutableMatrix([[1, 2], [3, 4]])\n        assert MatPow(M, 0) == Identity(2)\n        assert MatPow(M, 1) == M\n        assert MatPow(M, 2) == M*M\n        assert MatPow(M, 3) == M*M*M\n\n", "    def test_transpose_identity(self):\n        self.assertEqual(Transpose(Identity(3)).as_matrix(), Identity(3))\n"], "sample_338": ["    def test_assert_proxy_model_changes(self):\n        before = self.make_project_state([\n            author_proxy,\n            author_proxy_options,\n        ])\n        after = self.make_project_state([\n            author_proxy_options,\n            author_proxy,\n        ])\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n", "    def test_add_model_to_project_state(self):\n        project_state = ProjectState()\n        author_model_state = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ])\n        project_state.add_model(author_model_state)\n        self.assertEqual(len(project_state.models), 1)\n        self.assertTrue(author_model_state in project_state.models)\n", "    def test_migration_dependencies_with_non_empty_dependencies(self):\n        changes = {\n            \"testapp\": [\n                Migration(\"0001_initial\", [\n                    # Create Author model\n                ]),\n                Migration(\"0002_add_publisher_field\", [\n                    models.AddField(\n                        model_name=\"testapp.Author\",\n                        name=\"publisher\",\n                        field=models.ForeignKey(\"testapp.Publisher\", models.CASCADE),\n                    ),\n                    # Add dependency on 'testapp.Publisher' here\n                    models.Dependency(\"testapp.Publisher\"),\n                ]),\n            ],\n            \"otherapp\": [\n                Migration(\"0001_initial\", [\n                    # Create Book model\n                ]),\n            ]\n        }\n\n        self.assertMigrationDependencies(changes, \"testapp\", 1, [\"testapp.Publisher\"])\n", "def test_rename_app(self):\n        before = self.make_project_state([\n            author_with_book,\n            book,\n            publisher,\n            author_with_publisher\n        ])\n        \n        after = self.make_project_state([\n            author_with_book,\n            book,\n            Renamed_publisher,\n            author_with_publisher_renamed\n        ])\n        \n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'testapp', 3)\n        self.assertMigrationDependencies(changes, 'testapp', 0, [])\n        self.assertMigrationDependencies(changes, 'testapp', 1, ['testapp.author', 'testapp.book', 'testapp.publisher'])\n        self.assertMigrationDependencies(changes, 'testapp', 2, ['testapp.author'])\n        self.assertOperationTypes(changes, 'testapp', 0, ['RenameModel'])\n        self.assertOperationTypes(changes, 'testapp', 1, ['RenameModel', 'RenameField'])\n        self.assertOperationTypes(changes, 'testapp', 2, ['RenameModel'])\n\n        \n\n", "    def test_circular_references(self):\n        changes = self.get_changes([\n            rabbit,\n        ], [\n            rabbit,\n            knight,\n        ])\n        self.assertNumberMigrations(changes, \"eggs\", 2)\n        self.assertMigrationDependencies(changes, \"eggs\", 0, ['eggs.Knight'])\n        self.assertMigrationDependencies(changes, \"eggs\", 1, [])\n\n\n\n"], "sample_158": ["    def test_check_unique(self):\n        class MyModel(models.Model):\n            name = models.CharField(max_length=100)\n\n        with override_settings(INSTALLED_APPS=['invalid_models_tests']):\n            with self.assertNumQueries(0):\n                class YourModel(models.Model):\n                    name = models.CharField(max_length=100)\n                    related = models.ManyToManyField('MyModel', unique=True)\n\n                # Check for errors when unique=True is specified\n                with self.assertRaises(Error) as cm:\n                    DjangoWarning  # noqa\n                    YourModel._meta.get_field('related')\n                self.assertIn('ManyToManyFields cannot be unique', str(cm.exception))\n\n", "    def test_many_to_many_field_clash_with_router(self):\n        class MyModel(models.Model):\n            id = models.AutoField(primary_key=True)\n\n        class IntermediaryModel(models.Model):\n            id = models.AutoField(primary_key=True)\n            mymodel = models.ForeignKey(MyModel, on_delete=models.CASCADE)\n            othermodel = models.ForeignKey(MyModel, on_delete=models.CASCADE)\n\n        class ComplexModel(models.Model):\n            id = models.AutoField(primary_key=True)\n            m2m = models.ManyToManyField(MyModel, through=IntermediaryModel)\n\n        with self.assertRaises(Error) as context:\n            ComplexModel._meta.get_field('m2m').contribute_to_class(\n                ComplexModel, 'm2m',\n            )\n        self.assertIn(\n            'The field\\'s intermediary table \\'invalid_models_tests_intermediarymodel\\' clashes with the table name of \\'invalid_models_tests_intermediarymodel\\'',\n            str(context.exception),\n        )\n\n\n\n", "    def test_m2m_table_name_clash_with_router(self):\n        class MyRouter:\n                return 'db1' if model._meta.app_label == 'invalid_models_tests' else None\n\n                return 'db1' if model._meta.app_label == 'invalid_models_tests' else None\n\n                return True\n\n                return True\n\n        class Author(models.Model):\n            name = models.CharField(max_length=100)\n\n        class Book(models.Model):\n            title = models.CharField(max_length=100)\n            authors = models.ManyToManyField(Author, db_table='book_authors')\n\n        with override_settings(DATABASE_ROUTERS=[MyRouter()]):\n            with self.assertRaises(Error):\n                model_name = 'invalid_models_tests.Book'\n                Book._meta.get_field('authors').m2m_db_table()\n\n\n\n", "    def test_m2m_field_name(self):\n        class TestModel(models.Model):\n            m2m_field = models.ManyToManyField('self', symmetrical='True')\n\n        with override_settings(DATABASE_ROUTERS=['test.routers.router_invalid']):\n            with connection.cursor() as cursor:\n                cursor.execute(\"CREATE TABLE test_testmodel (id INTEGER PRIMARY KEY)\")\n                cursor.execute(\"CREATE TABLE test_testmodel_testmodel_rel (id INTEGER PRIMARY KEY, testmodel_id INTEGER)\")\n        \n        m2m_field = TestModel._meta.get_field('m2m_field')\n        self.assertEqual(m2m_field.m2m_field_name(), 'testmodel_id')\n\n        m2m_field._set_m2m_field_name('test_field')\n\n        self.assertEqual(m2m_field.m2m_field_name(), 'test_field')\n\n        Model = TestModel._meta.model_class()\n        with self.subTest('validate check'):\n            try:\n                Model.objects.create(m2m_field=Model.objects.all())\n            except models.ValidationError as e:\n                self.assertEqual(\n                    str(e),\n                    \"Cannot create a ManyToManyField with itself as a related model.\"\n                )\n", "    def test_check_m2m_table_name_clash_with_database_router(self):\n        with mock.patch('django.db.models.Model.get_all_object_names') as get_all_object_names:\n            class CustomRouter:\n                    if model._meta.app_label == 'invalid_models_tests':\n                        return 'invalid_database'\n                    return None\n\n                    if model._meta.app_label == 'invalid_models_tests':\n                        return 'invalid_database'\n                    return None\n\n            models.DatabaseRouter.router_class = CustomRouter\n\n            class MyModel(models.Model):\n                pass\n\n            class OtherModel(models.Model):\n                pass\n\n            class IntermediateModel(models.Model):\n                my_model = models.ForeignKey(MyModel)\n                other_model = models.ForeignKey(OtherModel)\n            \n\n            class MyModel2(models.Model):\n                m2m_field = models.ManyToManyField(OtherModel)\n            get_all_object_names.return_value = ['MyModel', 'OtherModel', 'IntermediateModel', 'MyModel2']\n\n\n            field = MyModel2._meta.get_field('m2m_field')\n            errors = field.check(\n                instance=MyModel2(), \n                using=connection.default_db\n            )\n            self.assertEqual(len(errors), 1)\n            self. isinstance(errors[0], DjangoWarning)\n            self.assertIn(\"You have configured settings.DATABASE_ROUTERS. Verify that the table of %r is correctly routed to a separate database.\", str(errors[0]))\n\n\n\n\n"], "sample_502": ["compilation error", "    def test_eventplot_deprecation():\n        with pytest.deprecated_call():\n            plt.eventplot([np.array([1, 2, 3]), np.array([4, 5])])\n", "compilation error", "    def test_figure_methods(func, kwargs, expected_output):  # noqa: E501\n        with plt.backend_tools.ignore_backend_warnings():\n            fig, ax = plt.subplots()\n            func(ax, **kwargs)\n            output = plt.pyplot.gca()._docstring_type\n            assert output == expected_output\n", "compilation error"], "sample_655": ["    def test_global_capture_with_fixture(self, capsys):\n        with capture.global_and_fixture_disabled():\n            assert not capsys.readouterr()\n\n        with StdCapture() as cm:\n            print(\n                \"This gets captured by the fixture and not the globals\"\n            )\n            print(\"This also gets captured\")\n\n        out, err = capsys.readouterr()\n        assert (\n            textwrap.dedent(out.strip()) == textwrap.dedent(\n                \"This gets captured by the fixture and not the globals\\n\"\n                \"This also gets captured\"\n            )\n        )\n\n", "    def test_global_and_fixture_disabled(self):\n        with capture.CaptureManager() as cman:\n            with cman.global_and_fixture_disabled():\n                result = cman.readouterr()\n                assert not result.out\n                assert not result.err\n", "    def test_global_capture_disabled_fixtures(self, tmpdir):\n        fixture_filename = os.path.join(tmpdir, \"fixture.txt\")\n        with open(fixture_filename, \"w\") as f:\n            f.write(\"fixture output\")\n        with capture.CaptureManager() as capman:\n            with pytest.raises(RuntimeError):\n                with capture.capfd() as cap:\n                    with open(fixture_filename, \"r\") as f:\n                        print(f.read())\n                capman\n                    assert cap.readouterr().out == \"fixture output\"\n", "    def test_global_and_fixture_disabled(self, monkeypatch):\n        capmanager = CaptureManager(\"no\")\n        monkeypatch.setattr(\"pytest.config.pluginmanager.getplugin\", lambda x: capmanager)\n        with capmanager.global_and_fixture_disabled():\n            out, err = capture.readouterr()\n            assert not out\n            assert not err\n", "    def test_suspend_global_capture(self, monkeypatch):\n        capture_manager = CaptureManager(method=\"capture\")\n        capture_manager.start_global_capturing()\n\n        with capture_manager.suspend_global_capture():\n            open(os.devnull, 'w').write(\"not captured\")\n            capture_manager.resume_global_capture()  \n\n        out, err = capture_manager.read_global_capture()\n        assert out == \"\"\n        assert err == \"\"\n\n"], "sample_993": ["    def test_power_of_empty_word():\n        raises(ValueError, lambda: F.identity.power_of(x))\n    ", "    def test_power_of():\n        raises(ValueError, lambda: F.power_of(x, y))\n        assert (x*y).power_of(x*y) is True\n        assert (x**-3*y**-2*x**3).power_of(x**-3*y*x**3)\n        assert (x**2*y**2*x**-1).power_of(x*y)\n        assert ((x*y)*y*x*y).power_of(x*y*y*x*y)\n        assert ((x*y)**2).power_of(x*y)\n        raises(ValueError, lambda: (x*y*x*y*x).power_of(x*y))\n", "    def test_power_of_multiple_generators():\n      w = x**2 * y**3 * z\n      other = x * y * z**-1\n      assert w.power_of(other) is False\n", "    def test_identity_cyclic_reduction():\n        assert (x**2*y**2*x**-1).identity_cyclic_reduction() == x*y**2\n        assert (x**-3*y**-1*x**5).identity_cyclic_reduction() == y**-1*x**2\n        assert (x*y*x**-1*y).identity_cyclic_reduction() == x**-1*y\n", "    def test_power_of():\n        raises(ValueError, lambda: (x**2 * y**-1).power_of(x * y**2))\n        raises(ValueError, lambda: (x**-3 * y**-2 * x**3).power_of(x**-2 * y))\n        raises(ValueError, lambda: (x*y).power_of(x**2))\n        assert (x**2 * y**2 * x**-1).power_of(x * y)  \n        assert (x**-3 * y**-2 * x**3).power_of(x**-3 * y * x**3)\n\n        \n\n"], "sample_512": ["compilation error", "compilation error", "    def test_legend_without_data(tmp_path):\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3])\n        ax.plot([4, 5, 6])\n        legend = ax.legend([])\n        for text in legend.get_texts():\n            assert text.get_text() == ''\n\n\n\n", "compilation error", "    def test_eventplot_deprecation(tmpdir):\n        with tempfile.TemporaryDirectory() as tempdir:\n            tmp_path = Path(tempdir)\n            mpl.rcParams['backend'] = 'Agg'\n\n            fig, ax = plt.subplots()\n            \n            ax.eventplot([np.array([1, 2, 3]), np.array([4, 5])], \n                         orientation='horizontal')\n\n            plt.savefig(tmp_path / 'test.png')\n            plt.close()\n\n            with open(tmp_path / 'test.png', 'rb') as f:\n                image_data = f.read() \n            \n            with pytest.warns(MatplotlibDeprecationWarning):\n                eventplot_result = plt.eventplot([np.array([1, 2, 3]), np.array([4, 5])],\n                         orientation='horizontal') \n            \n            plt.close()\n\n            assert eventplot_result is not None\n            \n            \n\n\n\n"], "sample_425": ["    def test_serialize_settings_reference(self):\n        setting = SettingsReference(\n            \"CUSTOM_SETTING\",\n            default=123,\n            setting_name=\"CUSTOM_SETTING\",\n        )\n        writer = OperationWriter(None, indentation=0)\n        output, imports = writer.serialize(setting)\n        self.assertEqual(\n            output, \"settings.CUSTOM_SETTING\"\n        )\n        self.assertEqual(set(imports), {\"from django.conf import settings\"})\n", "    def test_serialize_functools_partial(self):\n        @functools.partial\n            return f\"Hello, {name}!\"\n\n        serializer = custom_migration_operations.operations.serializer_factory(greet)\n        serialized_greet = serializer.serialize()\n\n        self.assertEqual(\n            serialized_greet[0],\n            \"functools.partial(greet, name=name)\",\n        )\n        self.assertEqual(serialized_greet[1], {\"import functools\"})\n", "    def test_serialize_decimal(self):\n        serializer = custom_migration_operations.more_operations.serializer.serializer_factory(decimal.Decimal(\"123.45\"))\n        serialized_value, imports = serializer.serialize()\n        expected_value = \"Decimal('123.45')\"\n        self.assertEqual(serialized_value, expected_value)\n        self.assertEqual(imports, {\"from decimal import Decimal\"})\n", "    def test_write_datetime_with_timezone(self):\n        tz = zoneinfo.ZoneInfo(\"America/New_York\")\n        dt = datetime.datetime(2023, 10, 26, 10, 30, 0, tzinfo=tz)\n        operation = migrations.AddField(\n            model_name=\"TestModel1\", field=models.DateTimeField(default=dt)\n        )\n        writer = OperationWriter(operation, indentation=0)\n        result = writer.serialize()\n        self.assertIn(\n            'default=datetime.datetime(2023, 10, 26, 10, 30, tzinfo=timezone.FixedOffset(0))',\n            result,\n        )\n\n\n\n", "    def test_serialize_operation_with_deferred_setup(self):\n        from django.db.migrations.operations.base import DeferredSetup\n\n        operation = DeferredSetup(\n            model_name=\"MyModel\",\n            sql=\"CREATE TABLE MyModel(id INT PRIMARY KEY)\",\n            apps,\n            dependencies=[],\n        )\n\n        writer = OperationWriter(operation, indentation=0)\n        serialized_operation, imports = writer.serialize()\n        self.assertEqual(\n            serialized_operation,\n            \"DeferredSetup(model_name='MyModel', sql='CREATE TABLE MyModel(id INT PRIMARY KEY)')\",\n            \"DeferredSetup operations should be serialized correctly\",\n        )\n\n\n"], "sample_1169": ["    def test_simplify_index_permutations_mixed_indices():\n        p, q, r, s = symbols('p,q,r,s')\n        f = Function('f')\n        g = Function('g')\n        PermutList = [PermutationOperator(p,q),PermutationOperator(r,s)]\n        expr = f(p,r)*g(q,s) - f(q,r)*g(p,s) + f(q,s)*g(p,r) - f(p,s)*g(q,r)\n        result = simplify_index_permutations(expr, PermutList)\n        expected = f(p, r)*g(q, s)*PermutationOperator(p, q)*PermutationOperator(r, s)\n        assert result == expected\n", "    def test_PermutationOperator_get_permuted(self):\n        p, q = symbols('p,q')\n        P = PermutationOperator(p, q)\n\n        f = Function('f')\n        g = Function('g')\n\n        self.assertEqual(P.get_permuted(f(p, q)), -f(q, p))\n        self.assertEqual(P.get_permuted(g(p)), g(p))\n        self.assertEqual(P.get_permuted(f(p)*g(q)), -f(q)*g(p))\n\n        expr = f(p, q)*g(r, s) - f(q, r)*g(p, s) + f(q, s)*g(p, r) - f(p, s)*g(q, r)\n        self.assertEqual(simplify_index_permutations(expr, [P]),\n                         f(p, r)*g(q, s)*PermutationOperator(p, q)*PermutationOperator(r, s))\n\n\n", "    def test_permutation_operator_evaluation():\n        P = PermutationOperator(p, q)\n        assert (P * f(p,q) - P * f(q,p)).simplify() == -f(p, q) + f(q, p)\n", "def test_wicks_nested_commutators():\n    q, p = symbols('q p')\n    expr = wicks(Fd(p)*(F(q) + F(r)), keep_only_fully_contracted=True)\n    expected = KroneckerDelta(p, q)*KroneckerDelta(r, r)\n    assert expr == expected\n    print(f'expected={latex(expected)}')\n    print(f'actual={latex(expr)}')\n\n", "    def test_wick_simple_fermi():\n        from sympy import symbols, Dummy\n        from sympy.physics.secondquant import wicks, create_fermion, annihilate_fermion\n        p, q, r = symbols('p q r', cls=Dummy)\n        expr = create_fermion(p)*annihilate_fermion(q)\n        result = wicks(expr)\n        expected = -KroneckerDelta(p, q)\n        assert result == expected\n\n\n\n"], "sample_1181": ["    def test_numpy_logaddexp(self):\n        from sympy import log\n\n        f = logaddexp(x, y)\n        self.assertEqual(f.evalf(subs={x: 1, y: 2}), np.logaddexp(1, 2))\n\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            f = logaddexp2(x, y)\n            self.assertEqual(\n                f.evalf(subs={x: 1, y: 2}), np.logaddexp2(1, 2)\n            )\n", "    def test_scipy_special(self):\n        p = SciPyPrinter()\n        self.assertEqual(p._print(log2(2)), 'numpy.log2(2)')\n        self.assertEqual(p._print(logaddexp(1, 2)), 'numpy.logaddexp(1, 2)')\n        self.assertEqual(p._print(logaddexp2(1, 2)), 'numpy.logaddexp2(1, 2)')\n        self.assertEqual(p._print(exp2(1)), 'numpy.exp2(1)')\n        self.assertEqual(p._print(log1p(0.5)), 'numpy.log1p(0.5)')\n        self.assertEqual(p._print(expm1(0.5)), 'numpy.expm1(0.5)')\n        self.assertEqual(p._print(hypot(1, 2)), 'numpy.hypot(1, 2)')\n        self.assertEqual(p._print(log10(1)), 'numpy.log10(1)')\n        self.assertEqual(p._print(sqrt(4)), 'numpy.sqrt(4)')\n\n\n", "    def test_array_add(self):\n        a = Array([1, 2, 3], shape=(1, 3))\n        b = Array([4, 5, 6], shape=(1, 3))\n        result = np.add(a.tolist(), b.tolist())\n        self.assertEqual(ArrayAdd(a, b).evalf(subs={}), result)\n", "    def test_array_diagonal(self):\n        from sympy.tensor.array.expressions.array_expressions import ArrayDiagonal\n        a = MatrixSymbol('a', 3, 3)\n        diag = ArrayDiagonal(a, [\n                (0, 0), (1, 1), (2, 2)\n            ]\n        )\n        self.assertEqual(CuPyPrinter().print(diag), 'cupy.diagonal(a, 0, axis1=0, axis2=1)')\n\n\n", "    def test_print_ArrayDiagonal(self):\n        A = Array([[1, 2], [3, 4]])\n        diag = ArrayDiagonal(A, [0, 1])\n        expected_code = \"\"\"numpy.diagonal(A, 0, axis1=0, axis2=1)\"\"\"\n        self.assertEqual(StrPrinter().print(diag), expected_code)\n\n\n        diag = ArrayDiagonal(A, [1, 0])\n        expected_code = \"\"\"numpy.diagonal(A, 1, axis1=1, axis2=0)\"\"\"\n        self.assertEqual(StrPrinter().print(diag), expected_code)\n"], "sample_62": ["    def test_register_unregister(self):\n        self.site.register(Location, NameAdmin)\n        self.assertIn(Location, self.site._registry)\n\n        self.site.unregister(Location)\n        self.assertNotIn(Location, self.site._registry)\n\n        with self.assertRaises(ImproperlyConfigured):\n            self.site.register(Person)\n\n        with self.assertRaises(admin.AlreadyRegistered):\n            self.site.register(Location, NameAdmin)\n        \n        with self.assertRaises(admin.NotRegistered):\n            self.site.unregister(Traveler)\n\n\n\n", "    def test_register_model_with_custom_admin(self):\n        self.site.register(Person, NameAdmin)\n        self.assertIn(Person, self.site._registry)\n        admin_class = self.site._registry[Person]\n        self.assertIsInstance(admin_class, NameAdmin)\n", "    def test_register_duplicate(self):\n        self.site.register(Location, NameAdmin)\n        with self.assertRaises(admin.AlreadyRegistered):\n            self.site.register(Location, NameAdmin)\n", "    def test_register_models(self):\n        self.site.register(Location, NameAdmin)\n        self.site.register(Person)\n        self.assertIn(Location, self.site._registry)\n        self.assertIn(Person, self.site._registry)\n        self.assertIsInstance(self.site._registry[Location], NameAdmin)\n        self.assertIsInstance(self.site._registry[Person], admin.ModelAdmin)\n", "    def test_register_with_custom_admin_site(self):\n        custom_site = CustomSite()\n        custom_site.register(Person, NameAdmin)\n        self.assertIn(Person, custom_site._registry)\n        self.assertIsInstance(custom_site._registry[Person], NameAdmin)\n"], "sample_879": ["    def test_fit_transform_with_encoded_missing_value(self):\n        X = [['Male', 1]]\n        enc = OrdinalEncoder(encoded_missing_value=-1)\n        enc.fit(X)\n        X_trans = enc.transform(X)\n        assert_array_equal(\n            X_trans,\n            np.array([[1, 0],]),\n            err_msg=\"Encoded missing value should be reflected in output\",\n        )\n        X_inv = enc.inverse_transform(X_trans)\n        assert_array_equal(\n            X_inv, np.array([[\"Male\", 1]]),\n            err_msg=\"Inverse transform should recover the original values\",\n        )\n", "    def test_ordinal_encoder_nan_values_fit_transform(self):\n        X = [\n            [\"Male\", 1, np.nan],\n            [\"Female\", 3, 2],\n            [\"Female\", np.nan, 1],\n        ]\n        enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-999, encoded_missing_value=-99)\n        enc.fit(X)\n        transformed = enc.transform(X)\n        inverted = enc.inverse_transform(transformed)\n\n        # Check if missing values in X are encoded as -99 in transformed\n        assert_allclose(transformed[:, 2], [-99, -99, -99])\n        assert_array_equal(inverted, [\n            ['Male', 1, np.nan],\n            ['Female', 3, 2],\n            ['Female', np.nan, 1]\n        ])  \n        \n        # Test against known unknown_value\n        enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-555, encoded_missing_value=-99)\n        enc.fit(X)\n        transformed = enc.transform(X)\n        inverted = enc.inverse_transform(transformed)\n        assert_allclose(transformed[:, 2], [-99, -99, -99])\n        assert_array_equal(inverted, [\n            ['Male', 1, np.nan],\n            ['Female', 3, 2],\n            ['Female', np.nan, 1]\n        ])          \n", "    def test_encoded_missing_value_with_unknown_categories(self, handle_unknown):\n        enc = OrdinalEncoder(handle_unknown=handle_unknown, unknown_value=-1, encoded_missing_value=-2)\n        X = [['A', 1], ['B', 2], ['C', np.nan]]\n        enc.fit(X)\n\n        if handle_unknown == 'error':\n            with pytest.raises(ValueError):\n                enc.transform([[None, 2]])\n\n        else:\n            X_transformed = enc.transform([[None, 2]])\n            assert_array_equal(X_transformed, np.array([[-2, 1]]))\n\n            X_inverse = enc.inverse_transform(X_transformed)\n            assert_array_equal(X_inverse, np.array([['None', 1]]))      \n", "    def test_ordinal_encoder_encoded_missing_value_with_nan(self):\n        enc = OrdinalEncoder(encoded_missing_value=np.nan)\n        X = [['Male', 1], ['Female', 3], ['Female', np.nan]]\n        X_encoded = enc.fit_transform(X)\n\n        assert np.isnan(X_encoded[2, 1])  \n\n        X_decoded = enc.inverse_transform(X_encoded)\n        assert np.isnan(X_decoded[2][1])  \n\n\n\n", "    def test_ordinal_encoder_with_missing_values(self):\n        X = [\n            [\"Male\", 1, None],\n            [\"Female\", 3, 2],\n            [\"Female\", None, 1],\n        ]\n\n        enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=99)\n        enc.fit(X)\n\n        # Expected encoded values for missing\n        expected = np.array(\n            [\n                [0.0, 0.0, 99.0],\n                [1.0, 1.0, 1.0],\n                [1.0, 99.0, 0.0],\n            ]\n        )\n\n        assert_allclose(\n            enc.transform(X),\n            expected,\n            rtol=1e-05,\n            atol=1e-05,\n        )\n\n        # Inverse transform should give back None for missing values\n        inverse_transformed = enc.inverse_transform(expected)\n        for i in range(3):\n            if X[i][2] is None:\n                assert inverse_transformed[i][2] is None\n            else:\n                assert inverse_transformed[i][2] == X[i][2]\n\n\n\n"], "sample_1170": ["    def test_print_Polynomial(self):\n\n        p = Poly(x**2 + 2*x + 1, x)\n        self.assertEqual(sstr(p), 'Poly(x**2 + 2*x + 1, x)')\n        self.assertEqual(sstrrepr(p), \"'Poly(x**2 + 2*x + 1, x)'\")\n        self.assertEqual(sstr(p, full_prec=False), 'x**2 + 2*x + 1')\n        self.assertEqual(sstr(p, order='lex'), 'x**2 + 2*x + 1')\n\n", "def test_print_Dimension():\n    from sympy.geometry import Dimension\n    d = Dimension(3)\n    assert sstr(d) == 'Dimension(3)'\n    assert sstrrepr(d) == 'Dimension(3)'\n\n\n\n", "    def test_print_Tensor(self):\n        from sympy.tensor import TensorIndex, TensorHead, Tensor, Rank1Tensor, \\\n            TensorProduct, tensor_indices\n\n        T = Tensor(symbols(\"A B C D\"), indices=tensor_indices('i j k l')('m n')\n                   )\n        p = StrPrinter()\n        assert p._print_Tensor(T) == 'Tensor(A, B, C, D, i, j, k, l, m, n)'\n", "    def test_complex_roots(self):\n        c =  3*x**2 + 2*x + 1\n        r = Rootof(c, 0)\n        self.assertEqual(sstr(r), 'CRootOf(3*x**2 + 2*x + 1, 0)')\n        c = 3*x**2 + 2*x + 1\n        r =  Rootof(c, 1)\n        self.assertEqual(sstr(r), 'CRootOf(3*x**2 + 2*x + 1, 1)')\n", "    def test_GroebnerBasis(self):\n        a = Poly(x**2 + x + 1, x)\n        g = groebner([a], x, grlex)\n        self.assertEqual(sstr(g), 'GroebnerBasis(exprs=[x**2 + x + 1], gens=[x**2 + x + 1], domain=ZZ, order=grlex)')\n"], "sample_516": ["def test_hatch_pattern():\n    fig, ax = plt.subplots(1, 1)\n    ax.add_patch(Rectangle((0.1, 0.1), 0.8, 0.8, hatch='\\\\\\\\', facecolor='none'))\n    with PdfPages('temp.pdf') as pdf:\n        pdf.savefig(fig)\n\n\n\n", "    def test_hatch_pattern(self):\n        fig, ax = plt.subplots()\n        ax.add_patch(Rectangle((0, 0), 1, 1, hatch='x', facecolor='none'))\n        ax.add_patch(Rectangle((0.5, 0), 1, 1, hatch='|', facecolor='none'))\n        ax.set_xlim(0, 2)\n        ax.set_ylim(0, 2)\n        plt.savefig(io.BytesIO(), format='pdf')\n\n", "    def test_use14corefonts_non_ascii(self):\n        plt.rc('font', family='serif')\n        plt.figure()\n        plt.text(0.5, 0.5, \"\u00c4\u00f6\u00fc\u00df\", fontsize=12,\n                 ha='center', va='center')\n        with PdfPages('test_use14corefonts_non_ascii.pdf') as pdf:\n            pdf.savefig()\n", "    def test_tex_font_fallback(self):\n        fig, ax = plt.subplots()\n        ax.text(0.5, 0.5, '$\\LaTeX$ and fallback fonts',\n                ha='center', va='center',\n                fontdict={'fontname': 'Times New Roman'})\n        ax.axis('off')\n        plt.savefig(self.test_image, bbox_inches='tight')\n\n\n\n", "def test_xobjects():\n    fig, ax = plt.subplots(1, 1)\n    ax.text(0.5, 0.5, 'Hello World', transform=ax.transAxes)\n    with PdfPages(io.BytesIO()) as pdf:\n        pdf.savefig(fig)\n    with open('pdf_xobjects.pdf', 'rb') as f:\n        pdf_data = f.read()\n    assert b'Type /ExtGState' in pdf_data\n"], "sample_15": ["    def test_ufunc_sqrt(self):\n        a = u.Quantity(np.array([16, 25, 36]), u.m**2)\n        b = u.Quantity(np.array([4, 5, 6]), u.m)\n        expected = u.Quantity(np.sqrt(np.array([16, 25, 36])), u.m)\n        assert_allclose(helper_sqrt(a).value, expected.value)\n\n", "    def test_ufunc_with_mixed_arrays_and_scalars(self):\n        # Ensure that ufuncs handle mixed arrays and scalars correctly\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", UserWarning)\n            # Ensure that ufuncs handle mixed arrays and scalars correctly\n            a = np.array([1, 2, 3]) * u.m\n            b = 5 * u.m\n            c = np.array([4, 5, 6])\n\n            # Test operations where both inputs are arrays\n            for f in [np.add, np.subtract, np.multiply, np.divide]:\n                q_result = f(a, b)\n                assert isinstance(q_result, u.Quantity)\n                assert q_result.unit == u.m\n\n            # Test operations where one input is an array and the other a scalar\n            for f in [np.add, np.subtract, np.multiply, np.divide]:\n                q_result = f(a, c)\n                assert isinstance(q_result, u.Quantity)\n                assert q_result.unit == u.m\n", "    def test_ufunc_with_different_units(self):\n        class MyUnits(u.Unit):\n            name = 'my_unit'\n\n        q1 = 2 * MyUnits()\n        q2 = 3 * MyUnits()\n        with pytest.raises(UnitsError) as exc:\n            np.add(q1, q2).to(u.m)\n        assert \"Units are not convertible\" in str(exc.value)\n", "    def test_ufunc_erfa(self):\n        # Test that ufuncs from the erfa library work correctly with Quantities\n        ufuncs = [\n            erfa_ufunc.s2e,\n            erfa_ufunc.e2s,\n            erfa_ufunc.s2b,\n            erfa_ufunc.b2s,\n            erfa_ufunc.xsph2d,\n            erfa_ufunc.d2xsph,\n            erfa_ufunc.xp2s,\n            erfa_ufunc.s2xp,\n        ]\n\n        for func in ufuncs:\n            q1 = u.deg\n            q2 = u.deg\n            with pytest.raises(UnitsError):\n                func(q1, q2)\n\n\n", "    def test_ufunc_array_output_err(self):\n        with pytest.raises(TypeError) as excinfo:\n            qh.ufunc_array_output(\n                np.array([1, 2, 3]),\n                np.array([4, 5, 6]),\n                np.sin,\n            )\n        assert \"the output from ufunc_array_output should be a Quantity\" in str(excinfo.value)\n\n\n"], "sample_469": ["    def test_non_aggregate_annotation_with_joins(self):\n        from django.db.models import Count\n\n        qs = Book.objects.annotate(num_friends=Count('author__friends')).filter(\n            author__friends__age__gt=30\n        )\n        self.assertEqual(len(list(qs)), 4)\n\n        qs = Book.objects.annotate(num_friends=lambda q: q.author.friends.count()).filter(\n            author__friends__age__gt=30\n        )\n        self.assertEqual(len(list(qs)), 4)\n\n", "    def test_non_aggregate_annotation_filter(self):\n        q = Book.objects.annotate(\n            author_count=Count(\"authors\"),\n            total_pages=Sum(\"pages\"),\n        ).filter(\n            author_count__gt=1,\n            total_pages__gt=300,\n        )\n        self.assertQueryCount(q, 3)  \n        \n        # Make sure the annotations are present in the query\n        #  which is not the default behaviour for an aggregation.\n        self.assertEqual(\n            [\n                (\"author_count\", \"COUNT('authors')\"),\n                (\"total_pages\", \"SUM('pages')\"),\n            ],\n            [\n                (annotation.name, annotation.expression)\n                for annotation in q.annotations\n            ],\n        )\n", "compilation error", "    def test_annotate_with_filter(self):\n        books = Book.objects.annotate(\n            avg_rating=Avg('rating'),\n            count_reviews=Count('reviews')\n        ).filter(avg_rating__gt=4)\n        self.assertQuerysetEqual(\n            books, ['<Book: The Definitive Guide to Django: Web Development Done Right>']\n        )\n", "    def test_complex_annotate_filter(self):\n        # Test that annotations with complex filters work correctly\n        q = Book.objects.annotate(\n            book_age=ExpressionWrapper(\n                F(\"pubdate\") - datetime.date.today(), output_field=IntegerField()\n            )\n        ).filter(book_age__lt=0, isbn__startswith=\"15\").order_by(\"book_age\")\n\n        self.assertQueryCount(q, 2)\n        for book in q:\n            self.assertTrue(book.book_age < 0)\n            self.assertEqual(book.isbn.startswith(\"15\"), True)\n\n\n\n        # Test that annotations with multiple conditions work correctly\n\n        q2 = Book.objects.annotate(\n            is_published_recently=Case(\n                When(pubdate__year=datetime.date.today().year, then=True),\n                default=False,\n                output_field=BooleanField(),\n            )\n        ).filter(is_published_recently=True).order_by(\"pubdate\")\n\n        self.assertQueryCount(q2, 1)\n        for book in q2:\n            self.assertTrue(book.is_published_recently) \n\n\n"], "sample_784": ["    def test_calibration_curve_binary(self):\n        X, y = make_classification(n_samples=1000, random_state=42)\n        # Use a classifier that doesn't provide reliable probabilities\n        clf = RandomForestClassifier(n_estimators=100, random_state=42)\n        clf.fit(X, y)\n\n        # Generate probability predictions\n        probs = clf.predict_proba(X)[:, 1]\n\n        # Compute the calibration curve\n        prob_true, prob_pred = calibration_curve(y, probs, normalize=False)\n\n        # Assert that the curve makes sense\n        assert_greater(prob_pred.max(), 0.9)\n        assert_greater_equal(prob_true.max(), 0.9)\n        assert_greater_equal(prob_true.min(), 0)\n        assert_greater_equal(prob_pred.min(), 0)\n\n        # Test with normalization\n        prob_true_norm, prob_pred_norm = calibration_curve(y, probs, normalize=True)\n        assert_almost_equal(prob_true_norm.min(), 0.0)\n        assert_almost_equal(prob_true_norm.max(), 1.0)\n        assert_almost_equal(prob_pred_norm.min(), 0.0)\n        assert_almost_equal(prob_pred_norm.max(), 1.0)\n", "    def test_sigmoid_calibration_with_sample_weights(self):\n\n        X, y = make_classification(n_samples=500, n_features=10, random_state=42)\n        sample_weight = np.random.rand(500)\n\n        clf = LinearSVC()\n        calibrated_clf = CalibratedClassifierCV(base_estimator=clf, method='sigmoid', cv='prefit')\n        fitted_clf = calibrated_clf.fit(X, y, sample_weight=sample_weight)\n\n        y_prob = fitted_clf.predict_proba(X)\n        a, b = _sigmoid_calibration(X, y, sample_weight)\n        calibrated_y_prob = expit(-(a * X + b))\n\n        # Compare calibrated probabilities with sigmoid calibration\n        assert_array_almost_equal(y_prob, calibrated_y_prob)\n\n\n", "    def test_calibration_curve_empty_data(self):\n        y_true = []\n        y_prob = []\n        prob_true, prob_pred = calibration_curve(y_true, y_prob)\n        assert_array_equal(prob_true, [])\n        assert_array_equal(prob_pred, [])\n", "    def test_calibration_curve_for_isotonic(random_state=42):\n        X, y = make_classification(n_samples=1000, random_state=random_state)\n        clf = CalibratedClassifierCV(\n            base_estimator=LinearSVC(), method='isotonic', cv=LeaveOneOut()\n        )\n        clf.fit(X, y)\n        prob_true, prob_pred = calibration_curve(y, clf.predict_proba(X)[:, 1])\n        assert_array_almost_equal(prob_true, np.mean(y))\n", "    def test_calibration_curve(self):\n        X, y = make_classification(n_samples=1000, n_features=20,\n                                   random_state=42)\n        clf = RandomForestClassifier(random_state=42)\n        clf.fit(X, y)\n\n        y_prob = clf.predict_proba(X)[:, 1]\n        prob_true, prob_pred = calibration_curve(y, y_prob,\n                                                strategy='quantile')\n        assert_array_almost_equal(prob_true, prob_pred)\n"], "sample_1158": ["    def test_empty_sequence(self):\n        with raises(SympifyError):\n            sympify([])", "    def test_sympify_numpy_arrays_mixed_types(self):\n        from sympy.core import sympify, Basic\n        a = numpy.array([[1, 2], [3.0, '4']])\n        with raises(SympifyError):\n            sympify(a)\n\n\n", "    def test_sympify_numpy_array_scalar(self):\n        a = numpy.array(1)\n        b = numpy.array([1])\n        self.assertEqual(sympify(a).args, (1,))\n        self.assertEqual(sympify(b).args, (1,))\n\n\n\n", "    def test_sym_list_numpy_array():\n        from numpy import array\n        a = array([1, 2, 3], dtype=numpy.float64)\n        with warns_deprecated_sympy(\n            feature=\"Using a non-symbol data type in sympify\",\n            issue=18066,\n            deprecated_since_version='1.6',\n        ):\n            result = sympify(a)\n        assert isinstance(result, Matrix)\n        assert result.args == [(1, 2, 3)]\n", "    def test_sympify_numpy_array_scalar():\n        import numpy as np\n        a = np.array(1)\n        assert sympify(a) == Integer(1)\n"], "sample_371": ["    def test_callable_setting_wrapper(self):\n            return 'wrapped value'\n\n        wrapped_setting = CallableSettingWrapper(callable_setting)\n        self.assertIsNot(wrapped_setting, callable_setting)\n        self.assertEqual(wrapped_setting(), 'wrapped value')\n", "    def test_callable_setting_wrapper(self):\n        class MyCallable:\n                return 'this is a callable'\n\n        wrapper = CallableSettingWrapper(MyCallable())\n        self.assertEqual(wrapper(), 'this is a callable')\n        self.assertEqual(repr(wrapper), '<MyCallable>')\n\n\n\n", "    def test_callable_wrapper_calls_callable(self):\n            return 'this is a callable setting'\n\n        wrapper = CallableSettingWrapper(callable_setting)\n        self.assertEqual(wrapper(), 'this is a callable setting')\n\n\n\n", "    def test_callable_setting_wrapper(self):\n            return 'value'\n\n        wrapper = CallableSettingWrapper(callable_setting)\n        self.assertEqual(wrapper.__repr__(), '<bound method CallableSettingWrapper.callable_setting>')\n        self.assertEqual(wrapper(), callable_setting())\n\n", "    def test_callable_setting_wrapper(self):\n        class CustomCallable:\n                return 'custom_value'\n\n        with self.subTest('Wrapped callable'):\n            wrapped_callable = CallableSettingWrapper(CustomCallable())\n            self.assertEqual(wrapped_callable(), 'custom_value')\n\n        with self.subTest('String representation'):\n            wrapped_callable = CallableSettingWrapper(CustomCallable())\n            self.assertEqual(repr(wrapped_callable), 'CustomCallable()')\n\n\n\n"], "sample_576": ["    def test_legend_contents(self, make_plot):\n        p = make_plot(\n            data=pd.DataFrame(\n                {\n                    \"x\": [1, 2, 3, 1, 2, 3],\n                    \"y\": [4, 5, 6, 7, 8, 9],\n                    \"col\": [\"A\", \"A\", \"A\", \"B\", \"B\", \"B\"],\n                    \"group\": [\"X\", \"X\", \"X\", \"Y\", \"Y\", \"Y\"],\n                }\n            ),\n            plot_fn=lambda p: p.mark_point(),\n            scales={\"x\": Continuous(), \"y\": Continuous(),},\n            limits={\"x\": (0, 4), \"y\": (0, 10),},\n        )\n        p._legend_contents = []\n        p._finalize_figure()\n        assert len(p._legend_contents) == 2  # two legends per column\n\n\n\n", "    def test_init_handles_no_data(self):\n        p = Plot(data=None, mark=MockMark())\n        assert p._scales == {}\n        assert p._subplots == []\n\n\n\n", "    def test_nominal_scales_and_axis(self):\n        p = Plot(data=pd.DataFrame({\"x\": [\"A\", \"B\", \"B\", \"A\"]}),\n                 kind=\"bar\",\n                 x=\"x\",\n                 y=\"y\",\n                 color=\"x\")\n        p._layout_spec = {\"engine\": \"tight\"}\n        p._shares = {\"x\": None, \"y\": None}\n        p._scales = {\"x\": Nominal(values=[\"A\", \"B\"], order=[\"A\", \"B\"])}\n\n        # This creates a new instance of the plot object\n        # but with a set of axes that will be used to\n        # handle the plotting\n        axes = [{\"x\": \"x\", \"y\": \"y\", \"ax\": plt.subplot(1, 1, 1)}]\n        p._subplots = axes\n\n        engine = p._scales.get(\"x\")\n        mock_mark = MockMark()\n\n        p._plot_layer(mock_mark)\n\n        assert mock_mark.passed_scales.get(\"x\") is engine\n        assert mock_mark.passed_orient is None\n        assert mock_mark.n_splits == 1\n\n\n\n", "    def test_legend_creation(self):\n\n        p = Plot(data=pd.DataFrame({\"x\": [1, 2, 3], \"y\": [4, 5, 6], \"color\": [\"A\", \"A\", \"B\"]}),\n                 mark=MockMark(),\n                 scales={\"x\": Continuous(), \"y\": Continuous(), \"color\": Nominal()},\n                 layout_spec={\"engine\": \"tight\"})\n\n        p.init()\n        p.draw()\n        assert len(p._legend_contents) == 1\n        assert len(p._legend_contents[0][0]) == 2 \n        assert p._legend_contents[0][1][0] == [\"color\"]\n\n\n\n", "    def test_setup_subplot_axis(self):\n\n        plot = Plot(data=None)\n        plot._subplots = [\n            {\"x\": \"x1\", \"y\": \"y1\", \"ax\": mpl.axes.Axes()},\n            {\"x\": \"x2\", \"y\": \"y2\", \"ax\": mpl.axes.Axes()},\n        ]\n        plot._scales = {\"x1\": Continuous(), \"y1\": Continuous(), \"x2\": Continuous(), \"y2\": Continuous()}\n\n        plot._setup_subplot_data(\n            plot._subplots, plot._scales, pair_variables={\"x\": [\"x1\", \"x2\"], \"y\": [\"y1\", \"y2\"]}\n        )\n\n        assert plot._subplots[0][\"ax\"].xaxis.get_label() == \"x1\"\n        assert plot._subplots[0][\"ax\"].yaxis.get_label() == \"y1\"\n        assert plot._subplots[1][\"ax\"].xaxis.get_label() == \"x2\"\n        assert plot._subplots[1][\"ax\"].yaxis.get_label() == \"y2\"\n\n\n"], "sample_83": ["    def test_filter_registration(self):\n        @self.library.filter('capitalize')\n            return value.capitalize()\n\n        self.assertIn('capitalize', self.library.filters)\n        self.assertEqual(self.library.filters['capitalize'], capitalize_filter)\n\n\n\n", "    def test_filter_registration_with_flags(self):\n        @self.library.filter(name='my_filter', expects_localtime=True, is_safe=True)\n            return value\n\n        self.assertIn('my_filter', self.library.filters)\n        filter_func = self.library.filters['my_filter']\n        self.assertTrue(getattr(filter_func, 'expects_localtime'))\n        self.assertTrue(getattr(filter_func, 'is_safe'))\n        self.assertTrue(hasattr(filter_func, '_filter_name'))\n        self.assertEqual(filter_func._filter_name, 'my_filter')\n", "    def test_filter_registration(self):\n        @self.library.filter\n            return value.upper()\n\n        self.assertEqual(self.library.filters['my_filter'], my_filter)\n", "    def test_filter_registration(self):\n        @self.library.filter(name='lower')\n            return value.lower()\n\n        self.assertIn('lower', self.library.filters)\n        self.assertEqual(self.library.filters['lower'], lower)\n\n", "    def test_filter_registration_basic(self):\n        @self.library.filter\n            return value * 2\n        self.assertIn('double', self.library.filters)\n        self.assertEqual(self.library.filters['double'], double)\n\n"], "sample_640": ["def test_is_internal_name(name, expected):\n    assert utils.is_internal_name(name) == expected\n\n", "def test_is_builtin(name, expected):\n    assert utils.is_builtin(name) == expected\n\n\n\n", "def test_is_known_builtin(name, expected):\n    node = nodes.Name(name=name)\n    assert utils.is_known_builtin(node) == expected\n\n\n\n", "def test_error_of_type(name, expected):\n    assert utils.error_of_type(nodes.Name(name=name), (nodes.ImportError, nodes.ModuleNotFoundError)) == expected\n\n\n", "def test_is_builtin_name(name: str, expected: bool) -> None:\n    assert utils.is_builtin_name(name) is expected\n\n\n"], "sample_801": ["    def test_estimator_repr(self):\n        # test a few estimators with various parameters\n        estimators = [\n            LogisticRegressionCV(),\n            make_pipeline(StandardScaler(), LogisticRegression()),\n            RFE(estimator=LogisticRegression(), n_features_to_select=1),\n            GridSearchCV(estimator=LogisticRegression(), param_grid={\"C\": [0.1, 1, 10]}),\n            CountVectorizer(),\n            Pipeline(steps=[('scaler', StandardScaler()), ('clf', LogisticRegression())]),\n            SVC(kernel='poly', C=10),\n            PCA(n_components=5),\n            NMF(n_components=3),\n            SimpleImputer(missing_values=0, strategy='most_frequent')\n        ]\n\n        for estimator in estimators:\n            repr_ = estimator.__repr__()\n\n            # Check that repr is not None and not empty\n            assert repr_ is not None and repr_\n\n            # Check if repr_ contains the class name\n            assert estimator.__class__.__name__ in repr_\n\n            # Check if repr_ contains the parameters\n            for param_name in sorted(estimator.get_params()):\n                if hasattr(estimator, param_name):\n                    assert getattr(estimator, param_name) in repr_\n    \n", "    def test_repr_for_estimators(self):\n        # Test repr for various estimators\n        estimators = [\n            LogisticRegression(),\n            StandardScaler(),\n            RFE(estimator=LogisticRegression(), n_features_to_select=10),\n            GridSearchCV(estimator=LogisticRegression(), param_grid={'C': [0.1, 1, 10]}),\n            CountVectorizer(),\n            Pipeline([('scaler', StandardScaler()), ('clf', LogisticRegression())]),\n            SVC(),\n            PCA(),\n            NMF(),\n            SimpleImputer()\n        ]\n        for estimator in estimators:\n            with set_config(display=\"diagram\"):\n                repr_ = estimator.__repr__()\n            print(repr_)\n\n", "    def test_repr_simple_imputer(self):\n        imputer = SimpleImputer(missing_values=0, strategy='mean', fill_value=None, verbose=0, copy=True)\n        self.assertIn('SimpleImputer(missing_values=0, strategy=\\'mean\\')', repr(imputer))\n\n", "    def test_repr_nested_pipeline(self):\n        pipe = Pipeline([\n            ('imp', SimpleImputer(missing_values=np.nan, strategy=\"mean\")),\n            ('scaler', StandardScaler()),\n            ('clf', LogisticRegression())\n        ])\n\n        repr_ = pipe.__repr__()\n        self.assertIn('Pipeline(steps=[', repr_)\n        self.assertIn('SimpleImputer', repr_)\n        self.assertIn('StandardScaler', repr_)\n        self.assertIn('LogisticRegression', repr_)\n\n", "def test_pformat_estimator_with_long_parameters():\n    estimator = LogisticRegression(\n        penalty='l2', dual=False, tol=1e-4, C=1.0,\n        fit_intercept=True, intercept_scaling=1,\n        class_weight=None, random_state=42,\n        solver='lbfgs', max_iter=100,\n        multi_class='auto', verbose=0,\n        warm_start=False, n_jobs=None, l1_ratio=None\n    )\n    pp = _EstimatorPrettyPrinter(indent=1, compact=True)\n    repr_ = pp.pformat(estimator)\n    assert 'penalty=\\'l2\\',' in repr_\n\n\n    estimator = SVC(\n        C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0,\n        shrinking=True, probability=False, tol=1e-3,\n        cache_size=200, class_weight=None,\n        verbose=False, max_iter=-1,\n        decision_function_shape='ovr', random_state=None\n    )\n    pp = _EstimatorPrettyPrinter(indent=1, compact=True)\n    repr_ = pp.pformat(estimator)\n    assert 'kernel=\\'rbf\\',' in repr_\n"], "sample_20": ["    def test_write_table_fits(self):\n        t = Table(self.data)\n\n        # Encode any mixin columns into standard Columns.\n        t = _encode_mixins(t)\n\n        with fits.open(get_pkg_data_filename(\"test.fits\"), \"w\") as hdul:\n            hdul.writeto(get_pkg_data_filename(\"test.fits\"))\n\n        with fits.open(get_pkg_data_filename(\"test.fits\")) as hdul:\n            read_table = hdul[0].data\n        assert_allclose(read_table[\"a\"], self.data[\"a\"])\n        assert_array_equal(read_table[\"b\"], self.data[\"b\"])\n        assert_allclose(read_table[\"c\"], self.data[\"c\"])\n\n", "    def test_write_read_time_column(self):\n        t = simple_table(\n            data=[(1, \"a\", Time(\"2023-01-01T00:00:00Z\"))],\n            names=[\"a\", \"b\", \"time\"],\n            formats=[\"int\", \"str\", \"datetime\"],\n        )\n        t[\"time\"].unit = u.day\n        with tempfile.NamedTemporaryFile(delete=False) as f:\n            write_table_fits(t, f.name)\n            read_t = read_table_fits(f.name)\n        assert equal_data(t, read_t)\n        assert read_t[\"time\"].unit == u.day\n\n\n", "    def test_read_fits_mixin_columns(self):\n        # Create a table with mixin columns\n        t = simple_table(\n            {\"a\": [1, 2, 3], \"b\": [\"a\", \"b\", \"c\"], \"c\": [2.3, 4.5, 6.7]},\n            meta={\"__serialized_columns__\": mixin_cols},\n        )\n\n        # Convert the table to FITS format and read it back\n        with fits.open(\n            get_pkg_data_filename(\"fits_table_with_mixins.fits\"), \"w\"\n        ) as hdul:\n            hdul.append(table_to_hdu(t))\n            hdul.flush()\n\n        t_read = fits.read_table(\n            get_pkg_data_filename(\n                \"fits_table_with_mixins.fits\"\n            ),\n        )\n\n        # Compare the original and read tables\n        assert_array_equal(t_read[\"a\"], t[\"a\"])\n        assert_array_equal(t_read[\"b\"], t[\"b\"])\n        assert_allclose(t_read[\"c\"], t[\"c\"])\n        for name, col in mixin_cols.items():\n            if name not in unsupported_cols:\n                compare_attrs(t[name], t_read[name])\n\n        \n", "    def test_write_table_fits_mixin_columns(self):\n        # Ensure mixin columns are properly written/read\n        t = Table(self.data)\n        t = t.meta.update(mixin_cols)\n        t = serialize._construct_mixins_from_columns(t)\n\n        with pytest.raises(warnings.Warning) as warn:\n            write_table_fits(t, \"temp.fits\", overwrite=True)\n\n        with open(\"temp.fits\", \"rb\") as f:\n            hdu = fits.open(f)\n            t_back = read_table_fits(hdu[0])\n\n        hdu.close()\n        os.remove(\"temp.fits\")\n\n        assert equal_data(t, t_back)\n\n\n", "    def test_read_write_single_table(self):\n        with fits.open(get_pkg_data_filename(\"data/simple_fits.fits\"), \"update\") as hdul:\n            hdul[0].data = self.table.data\n            hdul[0].header = self.table.meta\n            hdul.flush()\n            with fits.open(get_pkg_data_filename(\"data/simple_fits.fits\")) as hdul:\n                read_table = hdul[0].data\n                assert_allclose(read_table[\"a\"], self.table[\"a\"])\n                assert_array_equal(read_table[\"b\"], self.table[\"b\"])\n                assert_allclose(read_table[\"c\"], self.table[\"c\"])\n                assert read_table.meta == self.table.meta\n"], "sample_1105": ["    def test_refine_MatMul():\n        expr = MatMul(A, A.T)\n        with assuming(Q.orthogonal(A)):\n            print(refine(expr))  \n", "    def test_refine_matmul_non_square():\n        expr = MatMul(A, B, C)\n        with assuming(Q.orthogonal(A), Q.orthogonal(B), Q.orthogonal(C)):\n            result = refine(expr)\n            assert result == MatMul(A, B, C)\n", "    def test_combine_permutations_empty( ):\n      from sympy import PermutationMatrix\n      assert (MatMul(PermutationMatrix([1, 2, 3]),\n                 PermutationMatrix([4, 5, 6])).doit()\n              == PermutationMatrix([1, 2, 3, 4, 5, 6]))\n", "compilation error", "    def test_combine_permutations_issue_10185(self):\n        P1 = PermutationMatrix([1, 2, 3])\n        P2 = PermutationMatrix([2, 3, 1])\n        P3 = PermutationMatrix([3, 1, 2])\n        result = MatMul(P1, P2, P3)\n        expected = PermutationMatrix([1, 3, 2])\n        self.assertEqual(result, expected)\n"], "sample_959": ["    def test_cpp_symbol_lookup_with_nested_namespaces(self):\n        rootSymbol = Symbol(None, None, None, None, None, None, None)\n        ns1 = rootSymbol.add_namespace(\"Namespace1\", docname=\"TestDoc\", line=42)\n        ns2 = ns1.add_namespace(\"Namespace2\", docname=\"TestDoc\", line=42)\n        symbol = ns2.add_declaration(parse(\"class\", \"class MyClass { public: void foo(); }\"), docname=\"TestDoc\", line=42)\n\n\n        self.assertEqual(rootSymbol.direct_lookup(\"Namespace1::Namespace2::MyClass\"), symbol)\n        self.assertEqual(rootSymbol.direct_lookup(\"Namespace1::Namespace2::MyClass::foo\"), symbol.find_member(\"foo\"))\n\n\n\n", "    def check_type_parameter(name, input, output):\n        ast = parse(name, input)\n        symbol = Symbol()\n        symbol.declaration.objectType = 'templateParam'\n        parentNode = addnodes.desc()\n        signode = addnodes.desc_signature(input, '')\n        parentNode += signode\n        ast.describe_signature(signode, 'lastIsName', symbol, options={})\n        resAsText = parentNode.astext()\n        if resAsText != output:\n            print(\"\")\n            print(\"Input:    \", input)\n            print(\"astext(): \", resAsText)\n            print(\"Expected: \", output)\n            raise DefinitionError(\"\")\n\n\n", "compilation error", "def check_template_params(name, input, idDict, output):\n    ast = parse(name, input)\n    rootSymbol = Symbol(None, None, None, None, None, None, None)\n    symbol = rootSymbol.add_declaration(ast, docname=\"TestDoc\", line=42)\n    parentNode = addnodes.desc()\n    signode = addnodes.desc_signature(input, '')\n    parentNode += signode\n    ast.describe_signature(signode, 'lastIsName', symbol, options={})\n    res = parentNode.astext()\n    if res != output:\n        print(\"\")\n        print(\"Input:    \", input)\n        print(\"astext(): \", res)\n        print(\"Expected: \", output)\n        raise DefinitionError(\"\")\n\n    idExpected = [None]\n    for i in range(1, _max_id + 1):\n        if i in idDict:\n            idExpected.append(idDict[i])\n        else:\n            idExpected.append(idExpected[i - 1])\n    idActual = [None]\n    for i in range(1, _max_id + 1):\n        try:\n            id = ast.get_id(version=i)\n            assert id is not None\n            idActual.append(id[len(_id_prefix[i]):])\n        except NoOldIdError:\n            idActual.append(None)\n\n    res = [True]\n    for i in range(1, _max_id + 1):\n        res.append(idExpected[i] == idActual[i])\n\n    if not all(res):\n        print(\"input:    %s\" % input.rjust(20))\n        for i in range(1, _max_id + 1):\n            if res[i]:\n                continue\n            print(\"Error in id version %d.\" % i)\n            print(\"result:   %s\" % idActual[i])\n            print(\"expected: %s\" % idExpected[i])\n        print(rootSymbol.dump(0))\n        raise DefinitionError(\"\")\n\n\n\n", "    def check_templates(name, input, idDict, output):\n        ast = parse(name, input)\n        assert ast.templatePrefix is not None\n        rootSymbol = Symbol(None, None, None, None, None, None, None)\n        symbol = rootSymbol.add_declaration(ast, docname=\"TestDoc\", line=42)\n        parentNode = addnodes.desc()\n        signode = addnodes.desc_signature(input, '')\n        parentNode += signode\n        ast.describe_signature(signode, 'lastIsName', symbol, options={})\n        resAsText = parentNode.astext()\n\n        assert resAsText == output\n\n        "], "sample_590": ["def test_concat_dataset_identical():\n    for compat in [\"identical\", \"equals\", \"broadcast_equals\"]:\n        # Create two datasets with identical structure and data.\n\n        ds1, ds2 = create_test_data()\n\n        # Concatenate along an existing dimension\n        with pytest.raises(ValueError):\n            concat(\n                [ds1, ds2],\n                dim=\"x\",\n                compat=compat,\n            )\n\n\n", "    def test_concat_different_compat(data, data2):\n        a = data.chunk()\n        b = data2.chunk()\n\n            with pytest.raises(ValueError, match=expected_msg):\n                concat([a, b], dim=\"x\", compat=compat)\n\n        check(\"no_conflicts\", r\"cannot specify both 'compat'.* and 'override'\")\n\n\n", "    def test_concat_different_coords_override():\n        ds1 = create_test_data(\n            dims=(\"x\", \"y\"),\n            coords={\"x\": np.arange(2), \"y\": np.arange(3)},\n            data={\"data\": np.arange(6).reshape((2, 3))},\n        )\n        ds2 = deepcopy(ds1)\n        ds2.coords[\"y\"] = pd.Index(\n            [1, 2, 4], name=\"y\"\n        )  # different y coord\n        with raises_regex(\n            ValueError, r\"indexes to be aligned are not equal\"\n        ):\n            concat(\n                [ds1, ds2],\n                dim=\"y\",\n                coords=\"different\",\n                compat=\"override\",\n                join=\"exact\",\n            )\n\n", "    def test_concat_dim_coord_type():\n        # Ensure that dim can be a DataArray or Variable\n        da = DataArray(np.random.rand(3, 2), dims=(\"x\", \"y\"))\n        ds1 = Dataset({\"x\": da, \"y\": Variable(np.arange(2), dims=(\"y\",))},\n                       coords={\"x\": da.coords[\"x\"]})\n        with pytest.raises(TypeError):\n            concat([ds1], dim=da)  # Should be raised for a DataArray as dim\n        with pytest.raises(TypeError):\n            concat([ds1], dim=da.variable)  # Should be raised for a Variable as dim\n\n\n\n", "compilation error"], "sample_777": ["compilation error", "def check_regression_toy(presort, loss):\n    # Check regression on a toy dataset.\n    reg = GradientBoostingRegressor(loss=loss, n_estimators=10,\n                                    random_state=1, presort=presort)\n\n    assert_raises(ValueError, reg.predict, T)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n    reg.fit(X_train, y_train)\n    y_pred = reg.predict(X_test)\n\n    # check the mean squared error\n    mse = mean_squared_error(y_test, y_pred)\n    assert mse < 10.0 \n\n    assert_equal(10, len(reg.estimators_))\n    assert_greater(reg.feature_importances_.sum(), 0)\n    assert_equal(reg.oob_improvement_.shape[0], 10)\n    assert_equal(reg.train_score_.shape[0], 10)\n\n    leaves = reg.apply(X)\n    assert_equal(leaves.shape, (6, 10,))\n\n\n", "def test_gradient_boosting_regression_boston():\n    # Check regression on the boston dataset.\n    reg = GradientBoostingRegressor(random_state=0)\n    reg.fit(boston.data, boston.target)\n    predictions = reg.predict(boston.data)\n    assert_array_almost_equal(predictions, reg.predict(boston.data),\n                              decimal=3)\n\n    # Check with more parameters\n    reg = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,\n                        max_depth=3, min_samples_split=5,\n                        random_state=0)\n    reg.fit(boston.data, boston.target)\n\n    # Check prediction score\n    score = reg.score(boston.data, boston.target)\n    assert_greater(score, 0.75)\n\n", "compilation error", "compilation error"], "sample_317": ["    def test_latest_post_date(self):\n        feed = Atom1Feed(\n            title=\"Article Feed\",\n            link=\"http://example.com/articles/\",\n            description=\"Articles from my site\",\n        )\n        feed.add_item(\n            title='My latest entry',\n            link='http://example.com/articles/1/',\n            pubdate=datetime.datetime(2023, 10, 27, 10, 0)\n        )\n        feed.add_item(title='Older entry', link='http://example.com/articles/2/', pubdate=datetime.datetime(2023, 10, 26, 10, 0))\n\n        self.assertEqual(feed.latest_post_date(), datetime.datetime(2023, 10, 27, 10, 0))\n", "    def test_rss201rev2feed_latest_post_date(self):\n        feed = Rss201rev2Feed(\n            title='My Feed',\n            link='http://localhost/feed/',\n            description='A test feed.',\n            items=[\n                self.e5,\n                self.e4,\n                self.e2,\n                self.e1,\n                self.e3,\n            ]\n        )\n        self.assertEqual(feed.latest_post_date(), self.e5.published)\n        feed.items = [self.e1, self.e2]\n        self.assertEqual(feed.latest_post_date(), self.e2.published)\n        feed.items = []\n        self.assertEqual(feed.latest_post_date(), timezone.now())\n\n", "    def test_rss201rev2_feed_enclosures(self):\n        feed = Rss201rev2Feed(\n            title=\"My Feed\",\n            link=\"http://www.example.com/\",\n            description=\"Test feed with enclosures\",\n        )\n        feed.add_item(\n            title=\"Item with Enclosure\",\n            link=\"http://www.example.com/item1\",\n            description=\"Description\",\n            enclosures=[Enclosure(\"http://www.example.com/docs/audio1.mp3\", \"12345\", \"audio/mpeg\")],\n        )\n        rss = feed.writeString('utf-8')\n\n        dom = minidom.parseString(rss)\n        item = dom.getElementsByTagName(\"item\")[0]\n        enclosure = item.getElementsByTagName(\"enclosure\")[0]\n        self.assertEqual(enclosure.getAttribute(\"url\"), \"http://www.example.com/docs/audio1.mp3\")\n        self.assertEqual(enclosure.getAttribute(\"length\"), \"12345\")\n        self.assertEqual(enclosure.getAttribute(\"type\"), \"audio/mpeg\")\n\n\n\n", "    def test_rss201rev2_feed_enclosure(self):\n        feed = Rss201rev2Feed(\n            title='My Feed',\n            link='http://example.com/',\n            description='My feed description',\n        )\n        item = {\n            'title': 'With an enclosure',\n            'link': 'http://example.com/enclosure-item',\n            'description': 'Description',\n            'enclosures': [Enclosure('http://example.com/enclosure', '1234', 'audio/mpeg')]\n        }\n        feed.add_item(**item)\n\n        xml = feed.writeString('utf-8')\n        dom = minidom.parseString(xml)\n        item_element = dom.getElementsByTagName('item')[0]\n        enclosure_element = item_element.getElementsByTagName('enclosure')[0]\n        self.assertEqual(enclosure_element.getAttribute('url'), 'http://example.com/enclosure')\n        self.assertEqual(enclosure_element.getAttribute('length'), '1234')\n        self.assertEqual(enclosure_element.getAttribute('type'), 'audio/mpeg')\n", "    def test_atom_feed_creation(self):\n        from django.contrib.syndication import views\n        from django.contrib.syndication.feeds import AtomFeed\n\n        feed = AtomFeed(\n            title='My Test Feed',\n            link='http://example.com/test/',\n            description='This is a test feed.',\n            items=[self.e1, self.e2],\n        )\n\n        with open('tmp.atom', 'w') as f:\n            feed.write(f)\n\n        dom = minidom.parse('tmp.atom')\n        root = dom.documentElement\n        self.assertEqual(root.tagName, 'feed')\n        self.assertChildNodes(root, ['title', 'link', 'updated', 'author', 'subtitle', 'entry'])\n        self.assertChildNodeContent(root, {'title': 'My Test Feed',\n                                          'link': 'http://example.com/test/',\n                                          'updated': rfc3339_date(self.e5.updated),\n                                          'subtitle': 'This is a test feed.'})\n        entries = root.getElementsByTagName('entry')\n        self.assertEqual(len(entries), 2)\n        self.assertChildNodes(entries[0], ['title', 'link', 'published', 'updated', 'summary', 'id', 'category'])\n        self.assertChildNodeContent(entries[0], {'title': 'My first entry',\n                                                  'link': 'http://example.com/test/entry/1',\n                                                  'published': rfc3339_date(self.e1.published),\n                                                  'updated': rfc3339_date(self.e1.updated),\n                                                  'summary': '',\n                                                  'id': get_tag_uri('http://example.com/test/entry/1', self.e1.published),\n                                                  'category': 'category 1'})\n        self.assertChildNodes(entries[1], ['title', 'link', 'published', 'updated', 'summary', 'id', 'category'])\n        self.assertChildNodeContent(entries[1], {'title': 'My second entry',\n                                                  'link': 'http://example.com/test/entry/2',\n"], "sample_131": ["    def test_test_db_signature(self):\n        connection = self.get_connection_copy()\n        # Set some attributes to make signature unique.\n        connection.settings_dict['NAME'] = 'test_db_signature'\n        signature = connection.creation_functions.test_db_signature()\n        with mock.patch('django.db.connection', new=connection):\n            self.assertEqual(signature, (\n                connection.settings_dict['HOST'],\n                connection.settings_dict['PORT'],\n                connection.settings_dict['ENGINE'],\n                TEST_DATABASE_PREFIX + connection.settings_dict['NAME']\n            ))\n\n\n\n", "    def test_db_signature(self):\n        connection = self.get_connection_copy()\n        signature1 = connection.creation.test_db_signature()\n        with mock.patch('django.db.connection', new=connection):  \n            signature2 = connection.creation.test_db_signature()\n        self.assertEqual(signature1, signature2)\n", "    def test_db_signature_for_test_db_with_suffix(self):\n        connection = self.get_connection_copy()\n        connection.settings_dict['TEST'] = {'NAME': 'my_test_db', 'MUNGED': True}\n        \n        creation = BaseDatabaseCreation(connection)\n        creation.create_test_db(verbosity=0, autoclobber=True) # Create a test db for this test\n        suffix = '1'\n        \n        # Ensure we only have one test db, and it has a suffix\n        signature_with_suffix = creation.test_db_signature()\n        \n        # ...\n        # Test that the signature includes the suffix by comparing \n        # against a generated version that we know is correct\n        # ...\n        \n        creation.destroy_test_db(verbosity=0) # Clean up test db\n\n\n\n", "    def test_test_db_signature(self):\n        with mock.patch('django.db.backends.base.creation.BaseDatabaseCreation._get_test_db_name',\n                         return_value='test_my_db'):\n            connection_copy = self.get_connection_copy()\n            # Assuming the current test DB name is 'my_db'\n            instance = BaseDatabaseCreation(connection_copy)\n            signature = instance.test_db_signature()\n            \n            # Assert that elements are as expected\n            self.assertEqual(signature, (\n                connection_copy.settings_dict['HOST'],\n                connection_copy.settings_dict['PORT'],\n                connection_copy.settings_dict['ENGINE'],\n                'test_my_db'\n            ))\n", "    def test_test_db_signature(self):\n        connection = self.get_connection_copy()\n        with mock.patch('django.db.backends.base.creation.BaseDatabaseCreation._get_test_db_name', return_value=\"my_test_db\"):\n            signature = connection._db_creation.test_db_signature()\n            self.assertEqual(signature, (\n                connection.settings_dict['HOST'],\n                connection.settings_dict['PORT'],\n                connection.settings_dict['ENGINE'],\n                \"my_test_db\",\n            ))\n\n\n"], "sample_514": ["def test_colorbar_extension_length():\n    \"\"\"Test colorbar extensions of varying lengths.\"\"\"\n    plt.close('all')\n    fig1 = _colorbar_extension_length('uniform')\n    fig2 = _colorbar_extension_length('proportional')\n    fig1.show()\n    fig2.show()\n", "def test_colorbar_orientation_vertical():\n    fig, ax = plt.subplots()\n    im = ax.imshow(np.random.rand(5, 5), cmap='coolwarm')\n    cbar = fig.colorbar(im, ax=ax, orientation='vertical')\n    plt.show()\n\n", "def test_colorbar_fraction_size(tmpdir):\n    fig, ax = plt.subplots()\n    cbar = Colorbar(ax, fraction=0.3, orientation='horizontal')\n    # Check size of the colorbar.\n    assert cbar.get_width() == 0.3\n\n\n", "def test_colorbar_extension_length(request):\n    if request.config.getoption(\"mpl_show\"):\n        _colorbar_extension_length('uniform')\n        _colorbar_extension_length('proportional')\n", "def test_colorbars_locator_formatter():\n    fig, ax = plt.subplots()\n    # Create a dummy data array.\n    x = np.linspace(0, 10, 100)\n    y = np.sin(x)\n    # Create a colorbar with a custom locator and formatter.\n    norm = Normalize(vmin=0, vmax=10)\n    cb = Colorbar(ax, norm=norm,\n                  extend='both',\n                  cmap=cm.get_cmap('viridis'),\n                  locator=FixedLocator([0, 2, 4, 6, 8, 10]),\n                  format_func=lambda x: f\"{x:.1f}\")\n    plt.show()\n\n\n\n\n"], "sample_659": ["    def test_raises_nested_exceptions(self):\n        with pytest.raises(ValueError) as excinfo:\n            try:\n                raise ValueError\n            except ValueError:\n                raise TypeError\n        assert excinfo.errisinstance(ValueError)\n\n        with pytest.raises(ValueError, match=\".*value.*\"):\n            raise ValueError(\"some value\")\n\n        with pytest.raises(TypeError) as excinfo:\n            raise TypeError from ValueError(\"nested\")\n        assert excinfo.errisinstance(TypeError)\n        assert excinfo.value.args\n        assert excinfo.value.args[0] == \"(nested)\"\n", "    def test_reraise_contextmanager(self, caplog):\n        with dummy_context_manager() as cm:\n            raise ValueError(\"test\")\n        with pytest.raises(ValueError, match=\"test\"):\n            cm.__exit__(None)\n        assert \"ValueError: test\" in caplog.text\n", "    def test_raises_with_exception_info(self, monkeypatch):\n\n            raise ValueError(f'Value {x} is out of range')\n\n        with pytest.raises(ValueError, match=r'Value .* is out of range'):\n            raise_value_error(\"100\")\n\n            try:\n                raise_value_error(x)\n            except ValueError as e:\n                return e\n\n        result = raises_wrapped_exception(\"100\")\n        assert isinstance(result, ValueError)\n        assert str(result) == 'Value 100 is out of range'\n", "    def test_raises_context_manager(self):\n        try:\n            with pytest.raises(Exception) as excinfo:\n                raise ValueError(\"Expected exception\")\n        except Exception:\n            pytest.fail(\"Expected ValueError\")\n\n        assert excinfo.type is ValueError\n        assert excinfo.value.args == (\"Expected exception\", )\n\n\n\n", "    def test_excinfo_as_tuple(self):\n        with pytest.raises(Exception) as excinfo:\n            raise Exception(\"some error\")\n        assert excinfo.type is type(Exception)\n        assert excinfo.value.args == (\"some error\",)\n        assert excinfo.traceback is not None\n"], "sample_819": ["    def test_voting_classifier_multi_output(self):\n        X_multi, y_multi = make_multilabel_classification(n_samples=100, n_features=10, n_classes=5,\n                                                    random_state=42)\n\n        clf1 = LogisticRegression(multi_class='multinomial', random_state=42)\n        clf2 = RandomForestClassifier(n_estimators=50, random_state=42)\n        eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)], voting='hard')\n        with pytest.raises(NotImplementedError):\n            eclf.fit(X_multi, y_multi)\n\n", "    def test_voting_classifier_multiclass_with_ovr(self):\n        from sklearn.multiclass import OneVsRestClassifier\n        X_ovr, y_ovr = make_multilabel_classification(n_samples=100, n_features=20,\n                                                       n_classes=3, random_state=42)\n        # Create a voting classifier with OneVsRestClassifier\n        ovr_clf = OneVsRestClassifier(LogisticRegression(), n_jobs=-1)\n        voting_clf = VotingClassifier(\n            estimators=[('ovr', ovr_clf), ('lr', LogisticRegression())], n_jobs=-1\n        )\n\n        voting_clf.fit(X_ovr, y_ovr)\n        predictions = voting_clf.predict(X_ovr)\n\n        # Verify predictions\n        assert_array_equal(predictions.shape, y_ovr.shape)\n        assert_equal(voting_clf.classes_, ovr_clf.classes_)\n", "    def test_voting_classifier_voting_soft_with_weights(self):\n        clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial',\n                                random_state=1)\n        clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n        clf3 = GaussianNB()\n        X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n        y = np.array([1, 1, 1, 2, 2, 2])\n        eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n                                voting='soft', weights=[2, 1, 1],\n                                random_state=1)\n        eclf.fit(X, y)\n        y_pred = eclf.predict(X)\n        assert_array_equal(y_pred, [1, 1, 1, 2, 2, 2])\n\n        # Test predict_proba\n        y_pred_proba = eclf.predict_proba(X)\n        assert_array_almost_equal(np.sum(y_pred_proba, axis=1), 1.0)\n\n\n", "   def test_voting_classifier_weights(self):\n        clf1 = LogisticRegression(random_state=0)\n        clf2 = RandomForestClassifier(n_estimators=10, random_state=0)\n        clf3 = GaussianNB()\n        e = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n                            voting='soft', weights=[2, 1, 1])\n\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=0.3, random_state=0)\n        e.fit(X_train, y_train)\n        y_pred = e.predict(X_test)\n        \n        assert_array_equal(y_pred, e._predict(X_test).argmax(axis=1))\n", "    def test_voting_classifier_with_unknown_labels(self):\n        with pytest.raises(ValueError):\n            clf = VotingClassifier(\n                estimators=[('lr', LogisticRegression()),\n                            ('rf', RandomForestClassifier())],\n                voting='soft')\n            clf.fit(X[:2], [10, 11])\n\n\n\n"], "sample_736": ["    def test_logistic_regression_cv(self):\n        rng = np.random.RandomState(0)\n\n        X, y = make_classification(n_samples=100, n_features=10,\n                                   n_classes=2, random_state=rng)\n\n        cv = StratifiedKFold(n_splits=5)\n\n        # Test with default parameters\n        clf = LogisticRegressionCV(Cs=10, cv=cv, scoring='balanced')\n        clf.fit(X, y)\n        assert_greater(clf.C_.max(), 0)\n        assert_greater(clf.C_.min(), 0)\n\n        # Test with different scoring\n        clf = LogisticRegressionCV(Cs=10, cv=cv, scoring='accuracy')\n        clf.fit(X, y)\n        assert_greater(clf.C_.max(), 0)\n        assert_greater(clf.C_.min(), 0)\n\n        # Test with refit=False\n        clf = LogisticRegressionCV(Cs=10, cv=cv, refit=False,\n                                   scoring='balanced')\n        clf.fit(X, y)\n        assert_greater(clf.C_.max(), 0)\n        assert_greater(clf.C_.min(), 0)\n\n        # Test with class_weight\n        class_weights = compute_class_weight(\n            'balanced', np.unique(y), y\n        )\n        clf = LogisticRegressionCV(Cs=10, cv=cv,\n                                   class_weight=class_weights,\n                                   scoring='balanced')\n        clf.fit(X, y)\n        assert_greater(clf.C_.max(), 0)\n        assert_greater(clf.C_.min(), 0)\n", "def test_logistic_regression_cv_ovr():\n    \"\"\"Test LogisticRegressionCV with one-vs-rest (ovr) strategy.\"\"\"\n    n_samples = 100\n    n_features = 20\n    random_state = 42\n\n    X, y = make_classification(n_samples=n_samples, n_features=n_features,\n                               random_state=random_state)\n\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n    lr_cv = LogisticRegressionCV(Cs=np.logspace(-3, 3, 10),\n                                  cv=cv, scoring='accuracy',\n                                  random_state=random_state)\n\n    lr_cv.fit(X, y)\n    assert_greater(lr_cv.C_.shape[0], 0)\n    assert_equal(lr_cv.coef_.shape[0], 1)  # Binary case\n    assert_equal(lr_cv.intercept_.shape[0], 1)  # Binary case\n    assert_equal(lr_cv.coefs_paths_.keys(), [0])\n\n    n_classes = np.unique(y).shape[0]\n    expected_scores = np.zeros((n_classes, lr_cv.n_splits))\n    for i in range(n_classes):\n        scores = lr_cv.scores_\n        assert_equal(scores[i].shape, (lr_cv.n_splits, lr_cv.Cs_.shape[0]))\n\n    # Check if the best C is selected\n    best_C_index = np.argmax(lr_cv.scores_[0], axis=1)\n    best_C = lr_cv.Cs_[best_C_index]\n    assert_greater(best_C, 0)\n\n    # Check cross-validation scores\n    for fold_index, scores in enumerate(lr_cv.scores_[0]):\n        # scores is a list of scores for each C\n        assert_true(np.all(scores >= 0))\n\n    # Check if the model predictions are correct\n    y_pred = lr_cv.predict(X)\n    assert_array_equal(y_pred", "def test_logistic_regression_cv_multiclass_warm_start(random_state=0):\n    X, y = make_classification(\n        n_samples=100, n_features=20, n_informative=15, n_redundant=5,\n        n_clusters_per_class=1, random_state=random_state\n    )\n    y_encoded = encode_classes(y)\n    n_folds = 5\n    kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n\n    clf = LogisticRegressionCV(\n        Cs=1.0, cv=kf, solver='lbfgs', penalty='l2',\n        multi_class='multinomial',\n        random_state=random_state\n    )\n\n    # First fit and get scores\n    clf.fit(X, y_encoded)\n\n    # Check that the best C is not None\n    assert_true(clf.C_ is not None)\n\n    # Now add a new data point and refit\n    X_new = np.vstack((X, [X[0] * 1.2]))\n    y_new = np.append(y_encoded, y_encoded[0])\n    clf.fit(X_new, y_new)\n\n    # Check if C is changed\n    assert_greater(np.sum(np.abs(clf.C_ - clf.C_)), 1e-8)\n\n    # Check if the intercept and coefficients are updated\n    assert_array_almost_equal(clf.intercept_, clf.intercept_,\n                             decimal=4)\n    assert_array_almost_equal(clf.coef_, clf.coef_, decimal=4)\n\n    # Check that the predictions on the new data point are correct\n    y_pred = clf.predict(X_new)\n    assert_equal(y_pred[-1], y_new[-1])\n\n", "def test_logistic_regression_cv_fit(random_state=None):\n    \"\"\"Test LogisticRegressionCV with both OVR and multinomial\n    multi_class options.\"\"\"\n    random_state = random_state if random_state is not None else None\n    n_samples = 100\n    n_features = 20\n    n_classes = 3\n\n    X, y = make_classification(\n        n_samples=n_samples,\n        n_features=n_features,\n        n_redundant=0,\n        n_informative=n_features // 2,\n        n_clusters_per_class=1,\n        random_state=random_state,\n    )\n\n    # Test with binary classification\n    clf1 = LogisticRegressionCV(Cs=10, cv=StratifiedKFold(n_splits=3),\n                                penalty='l2', solver='lbfgs',\n                                multi_class='ovr', random_state=random_state)\n    clf2 = LogisticRegressionCV(Cs=10, cv=StratifiedKFold(n_splits=3),\n                                penalty='l1', solver='liblinear',\n                                multi_class='ovr', random_state=random_state)\n\n    clf1.fit(X, y)\n    clf2.fit(X, y)\n\n    assert_equal(clf1.n_iter_.shape, (1, 3, 10))\n    assert_equal(clf2.n_iter_.shape, (1, 3, 10))\n    assert_true(np.all(clf1.Cs_ == clf2.Cs_))\n    assert_true(clf1.coef_.shape == (1, n_features))\n\n    # Test with multinomial classification\n    clf3 = LogisticRegressionCV(Cs=10, cv=StratifiedKFold(n_splits=3),\n                                penalty='l2', solver='lbfgs',\n                                multi_class='multinomial',\n                                random_state=random_state)\n    clf3.fit(X, y)\n    assert_equal(clf3.n_iter_.shape, (n_classes, 3, 10))\n    assert_true(np.all(clf", "    def test_logistic_regression_cv_multinomial(self):\n        \"\"\"Test LogisticRegressionCV with multinomial loss.\"\"\"\n\n        n_samples = 100\n        n_features = 20\n        n_classes = 5\n\n        X, y = make_classification(\n            n_samples=n_samples, n_features=n_features, n_classes=n_classes,\n            random_state=42\n        )\n\n        # Test with multinomial loss\n        clf = LogisticRegressionCV(Cs=np.logspace(-2, 2, 10),\n                                    cv=5, penalty='l2',\n                                    solver='lbfgs',\n                                    multi_class='multinomial',\n                                    random_state=42)\n        clf.fit(X, y)\n        assert_equal(clf.coef_.shape[0], n_classes)\n        assert_equal(clf.coef_.shape[1], n_features)\n        assert_equal(clf.intercept_.shape[0], n_classes)\n        assert_equal(clf.Cs_.size, 10)\n\n        y_pred = clf.predict(X)\n        assert_array_equal(y_pred.shape, (n_samples,))\n\n\n"], "sample_1005": ["    def test_complex_root_of_poly(self):\n        r = RootSum(Poly(x**2 + 1, x), 0)\n        self.assertEqual(latex(r), 'CRootOf(x^{2} + 1, 0)')\n        r = ComplexRootOf(Poly(x**2 + 1, x), 0)\n        self.assertEqual(latex(r), 'CRootOf(x^{2} + 1, 0)')\n\n\n", "    def test_InverseSineTransform(self):\n        expr = InverseSineTransform(Sin(x))\n        self.assertEqual(latex(expr), r\"\\operatorname{Cin}^{-1}{\\left(\\sin{\\left(x \\right)} \\right)}\")\n\n\n", "compilation error", "compilation error", "def test_latex_quotient_modules():\n    from sympy.tensor.tensor import tensorcontraction\n    M = Module(ZZ, symbols('s1 s2'), {s1: ZZ, s2: ZZ})\n\n    M_killed = M.submodule([s1*s2, s1])\n    Q = QuotientModule(M, M_killed)\n    \n    latex_repr = latex(Q) \n    assert latex_repr == r\"\\frac{%s}{{%s}}\" % (latex(M), latex(M_killed))\n\n    test_expr = tensorcontraction(Q.base, [Q.basis[0], Q.basis[1]])\n    latex_repr = latex(test_expr)\n    assert latex_repr == r\"\\frac{%s}{{%s}}\" % (latex(M), latex(M_killed))\n\n\n\n"], "sample_240": ["    def test_token_expiration(self):\n        user = User.objects.create_user(username='testuser', password='password')\n        token_generator = MockedPasswordResetTokenGenerator(datetime.now())\n        token = token_generator.make_token(user)\n\n        # Simulate token being generated several hours ago\n        past_datetime = datetime.now() - timedelta(hours=settings.PASSWORD_RESET_TIMEOUT + 1)\n        with ignore_warnings(category=RemovedInDjango40Warning):\n            with self.subTest('Expired Token'):\n                token_generator._now = lambda: past_datetime\n                self.assertFalse(token_generator.check_token(user, token))\n\n        with self.subTest('Valid Token'):\n            token_generator._now = lambda: datetime.now()\n            self.assertTrue(token_generator.check_token(user, token))\n", "    def test_token_generation_with_custom_email_field(self):\n        with ignore_warnings(category=RemovedInDjango40Warning):\n            user = User.objects.create_user(username='testuser', email='test@example.com')\n            custom_email_field = CustomEmailField(value='new_email@example.com')\n            user.custom_email_field = custom_email_field\n            user.save()\n\n            token_generator = MockedPasswordResetTokenGenerator(datetime.now())\n            token = token_generator.make_token(user)\n\n            # Assert that the generated token includes the custom email field value.\n            # This ensures the custom field is being properly incorporated in the token generation process.\n            self.assertIn(custom_email_field.value, token_generator._make_hash_value(user, 0))\n", "    def test_token_generator_uses_now_from_constructor(self):\n        now = datetime(2023, 10, 26, 10, 0, 0)\n        mocked_generator = MockedPasswordResetTokenGenerator(now)\n        token = mocked_generator.make_token(User(username='test'))\n        # ... (add assertions to verify token content)\n", "compilation error", "    def test_token_expiration(self):\n        user = User(username='testuser')\n        user.save()\n        token_generator = MockedPasswordResetTokenGenerator(\n            datetime.now() - timedelta(seconds=settings.PASSWORD_RESET_TIMEOUT + 1)\n        )\n        token = token_generator.make_token(user)\n        self.assertFalse(token_generator.check_token(user, token))\n"], "sample_853": ["    def test_transformer_validation(self):\n        X, y = friedman\n\n        # Test that a ValueError is raised when both transformer and func/inverse_func are provided\n        with pytest.raises(ValueError):\n            TransformedTargetRegressor(\n                regressor=DummyRegressor(),\n                transformer=StandardScaler(),\n                func=np.log,\n                inverse_func=np.exp)\n        \n        # Test that a ValueError is raised when func is provided without \n        # inverse_func\n        with pytest.raises(ValueError):\n            TransformedTargetRegressor(\n                regressor=DummyRegressor(),\n                func=np.log)\n", "    def test_inverse_check_failure(self, regressor):\n        transformer = FunctionTransformer(\n            func=lambda x: 2 * x - 1,\n            inverse_func=lambda x: (x + 1) / 2,\n            validate=True,\n        )\n        tt = TransformedTargetRegressor(regressor=regressor,\n                                        transformer=transformer,\n                                        check_inverse=True)\n        X, y = friedman\n        tt.fit(X, y)\n        with pytest.raises(ValueError):\n            tt.predict(X)\n", "    def test_inverse_transform_required(self):\n        class MyTransformer(TransformerMixin):\n                pass\n\n                return X + 1\n\n                return X - 1\n\n        regressor = DummyRegressor()\n        transformer = MyTransformer()\n        tt = TransformedTargetRegressor(regressor=regressor,\n                                        transformer=transformer)\n        X = friedman[0]\n        y = friedman[1]\n        tt.fit(X, y)\n        pred = tt.predict(X)\n        assert_allclose(pred, y + 1)  \n", "    def test_functional_regressor_with_custom_transformer(self, regressor):\n        transformer = FunctionTransformer(\n            func=lambda x: x**2, inverse_func=np.sqrt)\n        tt = TransformedTargetRegressor(\n            regressor=regressor, transformer=transformer)\n        X, y = friedman\n        tt.fit(X, y)\n        y_pred = tt.predict(X)\n        assert_allclose(y_pred, np.sqrt(y**2))\n", "    def test_custom_transform_inverse(self, func, inverse_func):\n        tt = TransformedTargetRegressor(regressor=DummyRegressor(),\n                                        func=func,\n                                        inverse_func=inverse_func)\n        X, y = friedman\n\n        tt.fit(X, y)\n        y_pred = tt.predict(X)\n        assert_allclose(y_pred, y)\n\n"], "sample_1174": ["    def test_periodic_argument_evalf_principal_branch_period_infinity():\n        from sympy.abc import z\n        p = periodic_argument(exp_polar(3*I*pi), oo)\n        assert p.evalf(prec=5) == 3*pi\n", "    def test_polar_lift_of_polars():\n        from sympy.abc import x, y\n        z = exp_polar(I*pi/2)\n        assert polar_lift(z).args[0] == z\n        assert unpolarify(polar_lift(z)) == z\n        assert polar_lift(z*z).args[0] == z**2\n        assert unpolarify(polar_lift(z*z)) == z**2\n        assert polar_lift(z.subs(x, y)).args[0] == z.subs(x, y)\n\n\n", "    def test_principal_branch_special_cases():\n        from sympy import oo, I\n        assert principal_branch(exp_polar(I * 2 * pi), 2 * pi) == 1\n        assert principal_branch(exp_polar(I), oo) == exp_polar(I)\n        assert principal_branch(exp_polar(I * 3), oo) == exp_polar(I * 3)\n        assert principal_branch(exp_polar(I * 3), 2 * pi) == exp_polar(I)\n        z = Symbol('z')\n        assert principal_branch(z * exp_polar(I * pi), 2 * pi) == \\\n            exp_polar(I * pi) * z\n", "    def test_ArgumentIndexError(self):\n        with self.assertRaises(ArgumentIndexError):\n            arg(1)\n\n\n\n", "compilation error"], "sample_485": ["    def test_urlize_mailto(self):\n        self.check_output(urlize, \"user@example.com\", '<a href=\"mailto:user@example.com\">user@example.com</a>')\n        self.check_output(urlize, \"test@example.co.uk\", '<a href=\"mailto:test@example.co.uk\">test@example.co.uk</a>')\n        self.check_output(urlize, \"mailto:user@example.com\", '<a href=\"mailto:user@example.com\">mailto:user@example.com</a>')\n\n        self.check_output(urlize, \"test(user@example.com)\", \"test<a href=\\\"mailto:user@example.com\\\">user@example.com</a>\")\n        self.check_output(urlize, \"test(user@example.co.uk)\", \"test<a href=\\\"mailto:user@example.co.uk\\\">user@example.co.uk</a>\")\n\n\n", "    def test_json_script(self):\n        value = {'key': 'value'}\n        escaped_value = json_script(value)\n        self.assertIn('{\"key\": \"value\"}', escaped_value)\n        self.assertIn('<script type=\"application/json\">', escaped_value)\n        self.assertIn('</script>', escaped_value)\n \n        escaped_value_with_id = json_script(value, element_id='my_script')\n        self.assertIn('<script id=\"my_script\" type=\"application/json\">', escaped_value_with_id)\n        self.assertIn('</script>', escaped_value_with_id)\n\n        # Test with a custom encoder\n        class MyEncoder(DjangoJSONEncoder):\n                if isinstance(o, datetime):\n                    return o.isoformat()\n                return super().default(o)\n        escaped_value_with_encoder = json_script(value, encoder=MyEncoder)\n        self.assertIn('{\"key\": \"value\"}', escaped_value_with_encoder)\n\n\n\n", "    def test_urlize(self):\n        self.assertEqual(urlize(\"Visit http://www.google.com\"), '<a href=\"http://www.google.com\">Visit http://www.google.com</a>')\n        self.assertEqual(urlize(\"Visit google.com\"), '<a href=\"http://google.com\">google.com</a>')\n        self.assertEqual(urlize(\"john.doe@example.com\"), '<a href=\"mailto:john.doe@example.com\">john.doe@example.com</a>')\n        self.assertEqual(urlize(\"foo@example com\"), 'foo@example com')\n        self.assertEqual(\n            urlize(\"Visit http://www.google.com\", trim_url_limit=15),\n            '<a href=\"http://www.google.com\">http://www.goo...',\n        )\n        self.assertEqual(urlize(\"http://long.url.that.is.too.long.to.fit\", trim_url_limit=10), '<a href=\"http://long.url.that.is.too.long.to.fit\">http://long.url.that...</a>')\n        self.assertEqual(\n            urlize(\"Visit http://www.google.com\", nofollow=True),\n            '<a href=\"http://www.google.com\" rel=\"nofollow\">Visit http://www.google.com</a>',\n        )\n\n\n\n", "compilation error", "    def test_json_script(self):\n        self.assertEqual(\n            json_script({\"foo\": \"bar\"}),\n            '<script type=\"application/json\">{\"foo\":\"bar\"}</script>',\n        )\n        self.assertEqual(\n            json_script({\"foo\": \"bar\"}, element_id=\"my_script\"),\n            '<script id=\"my_script\" type=\"application/json\">{\"foo\":\"bar\"}</script>',\n        )\n        # test with unicode characters\n        self.assertEqual(\n            json_script({\"foo\": \"\\u03A9\"}),\n            '<script type=\"application/json\">{\"foo\":\"\\u03A9\"}</script>',\n        )\n        self.assertEqual(\n            json_script({\"foo\": \"bar\"}, encoder=DjangoJSONEncoder),\n            '<script type=\"application/json\">{\"foo\":\"bar\"}</script>',\n        )\n\n"], "sample_948": ["    def check_template_param(name, input, idDict, output):\n        _check(name, input, idDict, output)\n        # Test Template parameter with type\n\n        input = input.replace(\"typename\", \"typename {}\")\n        output = output.replace(\"typename\", \"typename {}\")\n        _check(name, input, idDict, output)\n\n\n\n", "compilation error", "    def _check_template_parameter(name, input, idDict, output):\n        ast = parse(name, input)\n        # The scopedness would usually have been set by CPPEnumObject\n        if name == 'templateParam':\n            ast.scoped = None  \n        rootSymbol = Symbol(None, None, None, None, None, None, None)\n        symbol = rootSymbol.add_declaration(ast, docname=\"TestDoc\", line=42)\n        parentNode = addnodes.desc()\n        signode = addnodes.desc_signature(input, '')\n        parentNode += signode\n        ast.describe_signature(signode, 'lastIsName', symbol, options={})\n        res = parentNode.astext()\n        if res != output:\n            print(\"\")\n            print(\"Input:    \", input)\n            print(\"Result:   \", res)\n            print(\"Expected: \", output)\n            raise DefinitionError(\"\")", "    def check_enum_scopes(name, input, idDict, output=None, key=None, asTextOutput=None):\n        if output is None:\n            output = input\n        if input.startswith('enum ') or input.startswith('enum '):\n            _check(name, input, idDict, output, key, asTextOutput)\n            _check(name, input + ' { }', idDict, output + ' { }', key,\n                   asTextOutput + ' { }' if asTextOutput is not None else None)\n        else:\n            _check(name, input, idDict, output, key, asTextOutput)\n\n\n", "compilation error"], "sample_645": ["    def test_log_capture_handler_handles_nested_loggers(self, caplog):\n        sublogger.info(\"Message from sublogger\")\n        caplog.set_level(logging.INFO)\n        assert \"Message from sublogger\" in caplog.text\n        \n        # Check if sublogger messages are captured \n        # even though we only set the level for the root logger.\n        assert \"Message from sublogger\" in caplog.text\n\n\n\n", "    def test_capture_logs_from_sublogger(self, caplog):\n        sublogger.debug(\"This is a debug message from sublogger\")\n        sublogger.info(\"This is an info message from sublogger\")\n        assert \"sublogger.debug\" in caplog.text\n        assert \"sublogger.info\" in caplog.text\n\n\n", "    def test_caplog_with_nested_loggers(self, caplog):\n        logger.info(\"This is a message from the root logger\")\n        sublogger.warn(\"This is a message from a nested logger\")\n\n        captured_logs = caplog.text.split(\"\\n\")\n\n        assert \"This is a message from the root logger\" in captured_logs\n        assert \"This is a message from a nested logger\" in captured_logs\n", " def test_capture_logs_clear(testdir):\n    \"\"\"Test clearing the log captures.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logger.debug(\"This is a debug message\")\n            sublogger.info(\"This is an info message\")\n\n    \n    if __name__ == \"__main__\":\n        my_test_function()\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*logging.debug*This is a debug message*\",\n            \"*logging.info*This is an info message*\",\n        ]\n    )\n    result = testdir.runpytest_timed(\n        \"--showlocals\", \"--log-cli-level=DEBUG\", \"-v\"\n    )\n    result.stdout.fnmatch_lines(\n        [\n            \"*logging.debug*This is a debug message*\",\n            \"*logging.info*This is an info message*\",\n        ]\n    )\n\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*logging.debug*This is a debug message*\",\n            \"*logging.info*This is an info message*\",\n        ]\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logger.debug(\"This is a debug message\")\n            sublogger.info(\"This is an info message\")\n\n    \n    if __name__ == \"__main__\":\n        my_test_function()\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*logging.debug*This is a debug message*\",\n            \"*logging.info*This is an info message*\",\n        ]\n    )\n\n", "    def test_cli_level_filtering(self, caplog, request) -> None:\n        # Test that log messages are filtered based on CLI level\n        request.node.add_marker(pytest.mark.parametrize(\"level\", [\"WARNING\", \"DEBUG\"]))\n        logger.setLevel(logging.DEBUG)  # Configure root logger for maximum logging\n        sublogger.setLevel(logging.CRITICAL)  \n\n        with caplog.at_level(\"DEBUG\") as cap:\n            logger.debug(\"This message should be logged\")\n            logger.warning(\"This message should be logged\")\n            sublogger.critical(\"This message should NOT be logged\")\n\n        # Check that only the DEBUG and WARNING messages are captured\n        assert \"This message should be logged\" in caplog.text\n        assert \"This message should be logged\" in caplog.text\n\n\n"], "sample_610": ["    def test_parse_iso8601_like_datetime_with_timezone(string, expected):\n        parsed, resolution = _parse_iso8601_with_reso(\n            cftime.DatetimeNoLeap, string\n        )\n        assert parsed == expected['date']\n        assert resolution == \"auto\"\n\n    \n\n\n\n", "    def test_parse_iso8601_like(string, expected):\n        date = parse_iso8601_like(string)\n        assert_identical(date, expected)\n\n", "    def test_iso8601_like_string_parsing(string, expected):\n        parsed, resolution = parse_iso8601_like(string, standard_or_gregorian)\n        assert parsed == expected\n        assert resolution == \"D\"\n", "    def test_parse_iso8601_with_reso(string, expected):\n        date_type = cftime.Date(\n            resolution=cftime.resolution_codes[\"daily\"],\n            calendar=standard_or_gregorian,\n        )\n        parsed, resolution = _parse_iso8601_with_reso(date_type, string)\n        assert parsed == expected\n \n", "    def test_cftime_index_from_array_of_strings(\n        self,"], "sample_796": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_641": ["def test_load_results_success(tmp_path: Path, mock_linter_stats: LinterStats) -> None:\n    save_results(mock_linter_stats, tmp_path)\n    loaded_stats = load_results(tmp_path)\n    assert isinstance(loaded_stats, LinterStats)\n    assert loaded_stats.total_errors == mock_linter_stats.total_errors\n", "compilation error", "compilation error", "def test_load_results_with_old_data(\n    monkeypatch: pytest.MonkeyPatch", "def test__get_pdata_path(path: str, recur: int, pylint_home: Path, expected: Path):\n    result = _get_pdata_path(Path(path), recur, pylint_home)\n    assert result == expected\n"], "sample_915": ["    def test_signature_from_str_empty_signature(self):\n        signature = ''\n        sig = inspect.signature_from_str(signature)\n        assert sig.parameters == {}\n        assert sig.return_annotation == inspect.Signature.empty\n", "    def test_signature_from_str():\n        sig = inspect.signature_from_str('(a: int, b: str) -> None')\n        assert sig.parameters == {'a': Parameter('a', Parameter.POSITIONAL_OR_KEYWORD, annotation=None), 'b': Parameter('b', Parameter.POSITIONAL_OR_KEYWORD, annotation=None)}\n        assert sig.return_annotation is inspect.Signature.empty\n\n        sig = inspect.signature_from_str('(a: int, *, b: str) -> None')\n        assert sig.parameters == {'a': Parameter('a', Parameter.POSITIONAL_OR_KEYWORD, annotation=None), 'b': Parameter('b', Parameter.KEYWORD_ONLY, annotation=None)}\n        assert sig.return_annotation is inspect.Signature.empty\n\n        sig = inspect.signature_from_str('(*args: int, **kwargs: str) -> None')\n        assert sig.parameters == {'args': Parameter('args', Parameter.VAR_POSITIONAL, annotation=None), 'kwargs': Parameter('kwargs', Parameter.VAR_KEYWORD, annotation=None)}\n        assert sig.return_annotation is inspect.Signature.empty\n\n        sig = inspect.signature_from_str('(a: int, b: str, *, c: int) -> None')\n        assert sig.parameters == {'a': Parameter('a', Parameter.POSITIONAL_OR_KEYWORD, annotation=None), 'b': Parameter('b', Parameter.POSITIONAL_OR_KEYWORD, annotation=None), 'c': Parameter('c', Parameter.KEYWORD_ONLY, annotation=None)}\n        assert sig.return_annotation is inspect.Signature.empty\n\n        sig = inspect.signature_from_str('(a) -> datetime.datetime')\n        assert sig.parameters == {'a': Parameter('a', Parameter.POSITIONAL_ONLY, annotation=None)}\n        assert sig.return_annotation == inspect.Signature.empty\n\n\n\n", "    def test_is_singledispatch_function(monkeypatch):\n        class SingleDispatchFunction(functools.singledispatch):\n                return x * 2\n\n        @SingleDispatchFunction.register(str)\n            return x.upper()\n\n        @SingleDispatchFunction.register(int)\n            return x * 2\n\n        assert inspect.is_singledispatch_function(SingleDispatchFunction())\n        assert inspect.is_singledispatch_function(_(1))\n        assert inspect.is_singledispatch_function(SingleDispatchFunction.register(int))\n\n\n", "    def test_stringify_signature(signature: str, expected: str):\n        sig = inspect.signature_from_str(signature)\n        assert stringify_signature(", "    def test_signature_from_str(test_signature):\n        sig = inspect.signature_from_str(test_signature)\n        assert sig.parameters == {\n            'a': Parameter(name='a', kind=Parameter.POSITIONAL_OR_KEYWORD),\n            'b': Parameter(name='b', kind=Parameter.POSITIONAL_OR_KEYWORD),\n            'c': Parameter(name='c', kind=Parameter.KEYWORD_ONLY, default=1),\n            'd': Parameter(name='d', kind=Parameter.VAR_KEYWORD),\n            'e': Parameter(name='e', kind=Parameter.VAR_POSITIONAL),\n        }\n        assert sig.return_annotation == inspect.Signature.empty\n\n\n"], "sample_551": ["compilation error", "compilation error", "compilation error", "    def test_poly_3d_collection_colors(self):\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection=\"3d\")\n\n        verts = np.array([\n            [(0, 0, 0), (1, 0, 0), (1, 1, 0), (0, 1, 0)],\n            [(0, 0, 1), (1, 0, 1), (1, 1, 1), (0, 1, 1)]\n        ])\n\n        col = Poly3DCollection(verts, facecolors=['red', 'green'], edgecolors='black')\n        ax.add_collection3d(col)\n        \n        # Set up the camera to face the collection directly\n        ax.view_init(elev=30, azim=45)\n        \n        plt.draw()\n        \n        # Compare the generated image with the reference image\n        self.assert_test_image('poly_3d_collection_colors.png')\n\n\n\n", "compilation error"], "sample_705": ["    def test_exitcode_handling(self, testdir: Testdir):\n        p = testdir.runpytest(\"--exitfirst\")\n        assert p.ret == ExitCode.EXITFIRST\n\n        p = testdir.runpytest(\n            \"-r\", \"x\", \"--exitfirst\", \"--verbose=1\",\n        )\n        assert p.ret == ExitCode.EXITFIRST\n\n        p = testdir.runpytest(\n            \"--exitfirst\", \"test_failure.py\", \"-v\",\n        )\n        assert p.ret == ExitCode.EXITFIRST\n", "def test_tmproot(testdir):\n    testdir.mkdir(\"a\")\n    testdir.makepyfile(\n        \"\"\"\n            pass\n        \"\"\"\n    )\n    reprec = testdir.inline_run()\n    assert reprec.countoutcomes() == (1, 0)\n    assert reprec.getcontainedfiles() == ['a/test_module.py']\n\n\n\n", "    def test_runpytest_subprocess_plugins(self, testdir):\n        testdir.mkdir(\"plugins\")\n        testdir.writepyfile(\n            \"plugins/myplugin.py\",\n            \"\"\"\n                return \"Plugin: MyPlugin\"\n            \"\"\",\n        )\n        result = testdir.runpytest(\n            \"-p\", \"plugins/myplugin.py\", \"--collect-only\"\n        )\n        result.stdout.fnmatch_lines(\n            [\n                \"*collected*\",\n                \"Plugin: MyPlugin\",\n            ]\n        )\n        exitcode = ExitCode.from_status(result.ret)\n        assert exitcode == ExitCode.OK\n\n\n\n", "def test_testdir_basic():\n    p = Testdir()\n    p.mkdir(\"subdir\")\n    p.makepyfile(\n        \"\"\"\n            assert True\n    \n            assert True\n    \"\"\",\n    )\n    result = p.runpytest_subprocess()\n\n    assert result.ret == 0\n    assert \"subdir/test_this_subdir.py::test_this_subdir\" in result.out\n\n\n\n", "    def test_legacy_path_operations(self, tmpdir):\n        p = os.path.join(tmpdir, 'testfile.txt')\n        Legacy_PATH(p).write(b\"contents\")\n\n        assert str(Legacy_PATH(p)) == str(Path(p))\n        assert Legacy_PATH(p).exists()\n        assert not Legacy_PATH(p).is_dir()\n        with open(p, \"r\") as f:\n            assert f.read() == \"contents\"\n\n        Legacy_PATH(p).unlink()\n        assert not Legacy_PATH(p).exists()\n\n\n\n"], "sample_260": ["    def test_rename_model(self):\n        o = operations.RenameModel(\n            \"old_model\",\n            \"new_model\",\n        )\n\n        self.assertOptimizesTo(\n            [o],\n            [\n                operations.RenameModel(\n                    \"old_model\",\n                    \"new_model\",\n                ),\n            ],\n            exact=1,\n        )\n\n        # Make sure renaming within a single migration doesn't trigger unnecessary loops\n        self.assertOptimizesTo(\n            [\n                models.RenameField(\n                    model_name=\"old_model\",\n                    old_name=\"old_field\",\n                    new_name=\"new_field\",\n                ),\n                o,\n            ],\n            [\n                models.RenameField(\n                    model_name=\"old_model\",\n                    old_name=\"old_field\",\n                    new_name=\"new_field\",\n                ),\n                operations.RenameModel(\n                    \"old_model\",\n                    \"new_model\",\n                ),\n            ],\n            exact=2,\n        )\n\n\n", "    def test_rename_model_with_custom_manager_optimization(self):\n        # Test that renaming a model with a custom manager is not accidentally\n        # combined with a migration to rename an index on the model.\n        operations = [\n            operations.RenameModel(\n                \"migrations.UnicodeModel\", \"migrations.RenamedUnicodeModel\"\n            ),\n            operations.AddIndex(\n                \"migrations.UnicodeModel\",\n                models.Index(\n                    fields=['name'],\n                    name='name_index',\n                ),\n            ),\n        ]\n        self.assertOptimizesTo(\n            operations,\n            [\n                operations.RenameModel(\n                    \"migrations.UnicodeModel\", \"migrations.RenamedUnicodeModel\"\n                ),\n                operations.AddIndex(\n                    \"migrations.RenamedUnicodeModel\",\n                    models.Index(\n                        fields=['name'],\n                        name='name_index',\n                    ),\n                ),\n            ],\n        )\n\n\n", "    def test_delete_model_order(self):\n        # Test that deleting a model doesn't create unnecessary dependencies.\n        operations = [\n            operations.CreateModel(\n                name='UnicodeModel',\n                table='unicode_model',\n                fields=[\n                    models.CharField(max_length=255, primary_key=True),\n                    models.CharField(max_length=255),\n                ],\n            ),\n            operations.CreateModel(\n                name='SimpleModel',\n                table='simple_model',\n                fields=[\n                    models.AutoField(primary_key=True),\n                    models.ForeignKey(\n                        'UnicodeModel',\n                        on_delete=models.CASCADE,\n                    ),\n                ],\n            ),\n            operations.DeleteModel(name='UnicodeModel'),\n        ]\n        expected = [\n            operations.CreateModel(\n                name='SimpleModel',\n                table='simple_model',\n                fields=[\n                    models.AutoField(primary_key=True),\n                ],\n            ),\n            operations.DeleteModel(name='SimpleModel'),\n        ]\n        self.assertOptimizesTo(operations, expected, app_label='migrations')\n\n\n\n", "    def test_rename_model_in_migration(self):\n        class RenameMyModel(migrations.Migration):\n            dependencies = [\n                ('appname', '0001_initial'),\n            ]\n\n            operations = [\n                operations.RenameModel(\n                    'appname.MyModel',\n                    'appname.NewModel',\n                ),\n            ]\n\n        operations = [RenameModel('appname.MyModel', 'appname.NewModel')]\n        self.assertOptimizesTo(\n            operations, [RenameModel('appname.MyModel', 'appname.NewModel')],\n            exact=1,\n        )\n\n\n\n\n", "    def test_remove_index_order(self):\n        operations = [\n            operations.AddIndex(\n                model_name='migrations.UnicodeModel',\n                index=models.Index(fields=['id'], name='id_index'),\n            ),\n            operations.AddIndex(\n                model_name='migrations.UnicodeModel',\n                index=models.Index(fields=['name'], name='name_index'),\n            ),\n            operations.RemoveIndex(\n                model_name='migrations.UnicodeModel',\n                name='id_index',\n            ),\n        ]\n        self.assertOptimizesTo(\n            operations,\n            operations[:-1],\n            app_label=None,\n        )\n\n\n\n"], "sample_482": ["        def test_escapeseq_basic(self):\n            c = {'a': ['<div>1', '<em>2</em>'], 'b': ['<div>3', '<em>4</em>']}\n            self.assertEqual(\n                mark_safe(self.template.render(c)),\n                mark_safe(\n                    \" <div>1, <em>2</em> -- <div>3, <em>4</em>\"\n                ),\n            )", "    def test_escapeseq_safe(self):\n        data = [mark_safe(\"hello\"), mark_safe(\"world\")]\n        self.assertEqual(\n            self.render(\"{{ %s|escapeseq|join:', ' }}\" % str(data)),\n            \"hello,world\",\n        )\n\n\n\n", "    def test_escapeseq_safe(self):\n        self.assertEqual(\n            '{{ [\"a\", \"b\"|safe]|escapeseq|join:\", \" }}',\n            'a,b'\n        )\n\n\n\n", "    def test_escapeseq_safe(self):\n        self.assertEqual(\n            mark_safe(self.template.render({\"a\": [1, 2, 3], \"b\": [\"!@#$\", \"abc\"]})),\n            mark_safe(\"1, 2, 3 -- !@#, abc\"),\n        )\n    \n", "    def test_escapeseq_safe(self):\n        with self.subTest():\n            self.assertEqual(\n                mark_safe('{{ \"abc\" | escapeseq }} -- {{ \"def\" | escapeseq }}'),\n                mark_safe('abc -- def'),\n            )\n"], "sample_395": ["    def test_template_changed_watches_extra_directory(self, mock_reset_loaders):\n        autoreload.start()\n\n        # Create a template file in the extra directory\n        (EXTRA_TEMPLATES_DIR / \"my_template.html\").write(\"<h1>Hello, world!</h1>\")\n\n        # Trigger the file changed signal\n        with open(EXTRA_TEMPLATES_DIR / \"my_template.html\", \"a\") as f:\n            f.write(\"\")  \n\n        # Assert that reset_loaders was called\n        mock_reset_loaders.assert_called()\n", "    def test_template_changed_triggers_reset(self, mock_reset_loaders):\n        EXTRA_TEMPLATES_DIR.joinpath(\"test_template.html\").write(\"Hello World!\")\n        file_changed.send(autoreload, file_path=EXTRA_TEMPLATES_DIR.joinpath(\"test_template.html\"))\n        mock_reset_loaders.assert_called()\n", "    def test_template_changed_reset_loaders(self, mock_reset_loaders):\n        file_path = EXTRA_TEMPLATES_DIR / \"test.html\"\n        file_path.write_text(\"<h1>Hello, world!</h1>\")\n        file_changed.send(sender=autoreload, file_path=file_path, **{})\n        mock_reset_loaders.assert_called_once()\n", "    def test_watch_for_template_changes(self, mock_reset_loaders):\n        autoreload.autoreload_started()\n        (ROOT / EXTRA_TEMPLATES_DIR / \"test.html\").touch()\n        mock_reset_loaders.assert_called_once()\n", "    def test_watches_template_directories(self, mock_reset_loaders):\n        autoreload.start()\n        get_template_directories()\n        mock_reset_loaders.assert_called_once()\n"], "sample_257": ["    def test_key_transform_contains_empty_string(self):\n        with self.subTest(\"Empty string for key\"):\n            with self.assertRaises(TypeError):\n                KeyTransform(\n                    key_name='',\n                    lhs=JSONModel.objects.get(pk=1),\n                )\n", "    def test_complex_json_fields(self):\n        data = {\n            'name': 'Alice',\n            'age': 30,\n            'address': {'street': '123 Main St', 'city': 'Anytown'},\n            'phone_numbers': [{'type': 'home', 'number': '555-1234'}, {'type': 'work', 'number': '555-5678'}],\n        }\n        obj = JSONModel.objects.create(json_field=data)\n        self.assertEqual(obj.json_field['name'], 'Alice')\n        self.assertEqual(obj.json_field['age'], 30)\n        self.assertEqual(obj.json_field['address']['street'], '123 Main St')\n        self.assertEqual(obj.json_field['phone_numbers'][0]['type'], 'home')\n\n\n\n", "    def test_json_field_isnull(self):\n        json_model = JSONModel.objects.create(\n            data={'key1': 'value1', 'key2': 'value2'}\n        )\n        self.assertFalse(json_model.data.get('key3'))\n        query = JSONModel.objects.filter(data__isnull=True)\n        self.assertEqual(query.count(), 0)\n        query = JSONModel.objects.filter(data__isnull=False)\n        self.assertEqual(query.count(), 1)\n\n", "    def test_key_transform_text_lookups(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\n                '''\n                CREATE TABLE test_key_transform_text_lookup (\n                    id SERIAL PRIMARY KEY,\n                    json_data JSONB\n                )\n            '''\n            )\n        with connection.managed_start() as c:\n            instance = JSONModel(json_data={'a': 'text'})\n            instance.save()\n            # test case insensitive lookups with key_text_transform\n            key_transform = KeyTextTransform('a')\n            exact_lookup = KeyTransformIExact(key_transform, instance.id)\n            self.assertEqual(\n                list(JSONModel.objects.filter(exact_lookup='text')),\n                [instance]\n            )\n            contains_lookup = KeyTransformIContains(key_transform, 'text')\n            self.assertEqual(\n                list(JSONModel.objects.filter(contains_lookup=True)),\n                [instance]\n            )\n            starts_with_lookup = KeyTransformIStartsWith(key_transform, 'te')\n            self.assertEqual(\n                list(JSONModel.objects.filter(starts_with_lookup=True)),\n                [instance]\n            )\n            ends_with_lookup = KeyTransformIEndsWith(key_transform, 'xt')\n            self.assertEqual(\n                list(JSONModel.objects.filter(ends_with_lookup=True)),\n                [instance]\n            )\n            regex_lookup = KeyTransformIRegex(key_transform, r'te.*')\n            self.assertEqual(\n                list(JSONModel.objects.filter(regex_lookup=True)),\n                [instance]\n            )\n\n\n", "    def test_key_transform_contains_with_subquery(self):\n        with self.subquery_context():\n            qs = JSONModel.objects.filter(\n                data__json__json_contains=Subquery(\n                    RelatedJSONModel.objects.values_list('data__val', flat=True)\n                )\n            )\n            self.assertEqual(qs.query.get_compiler(using=self.connection).as_sql(),\n                             (\n                                 'SELECT \"app_jsonmodel\".\"id\", \"app_jsonmodel\".\"data\", \"app_jsonmodel\".\"otherdata\" FROM \"app_jsonmodel\" '\n                                 'WHERE (%s) '\n                             ),\n                             )\n            self.assertEqual(qs.query.get_params(), [\n                'JSON_CONTAINS(data->>val, %s)',\n            ])\n\n\n\n"], "sample_1029": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_919": ["compilation error", "compilation error", "    def checkTemplateParam(name, input, idDict, output=None, key=None, asTextOutput=None):\n        if output is None:\n            output = input\n        _check(name, input, idDict, output, key, asTextOutput)\n        _check(name, input + ' <T>', idDict, output + ' <T>', key,\n               asTextOutput + ' <T>' if asTextOutput is not None else None)\n", "    def test_cpp_alias_transform(\n        self, sphinx_app, testdir", "    def check_enum_value_order(name, input, idDict, output):\n        ast = parse(name, input)\n        rootSymbol = Symbol(None, None, None, None, None, None)\n        symbol = rootSymbol.add_declaration(ast, docname=\"TestDoc\")\n        parentNode = addnodes.desc()\n        signode = addnodes.desc_signature(input, '')\n        parentNode += signode\n        ast.describe_signature(signode, 'lastIsName', symbol, options={})\n        resAsText = parentNode.astext()\n        if resAsText != output:\n            print(\"Input:    \", input)\n            print(\"astext(): \", resAsText)\n            print(\"Expected: \", output)\n            raise DefinitionError(\"\") \n\n"], "sample_889": ["    def test_calibration_display_from_estimator(ensemble, method, data):\n        X, y = data\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        if ensemble:\n            estimator = VotingClassifier(\n                estimators=[\n                    (\"lr\", LogisticRegression(solver=\"lbfgs\", random_state=42)),\n                    (\"dt\", DecisionTreeClassifier(random_state=42)),\n                ],\n                voting=\"soft\",\n            )\n        else:\n            estimator = LogisticRegression(solver=\"lbfgs\", random_state=42)\n\n        estimator.fit(X_train, y_train)\n\n        disp = CalibrationDisplay.from_estimator(estimator, X_test, y_test, n_bins=10, method=method)\n        \n        assert disp.ax_ is not None\n        assert disp.line_ is not None\n        \n\n\n\n", "    def test_isotonic_calibration_with_ensemble(data, method, ensemble):\n        X, y = data\n        if ensemble:\n            clf = VotingClassifier(\n                estimators=[\n                    (\"lr\", LogisticRegression(random_state=42)),\n                    (\"dt\", DecisionTreeClassifier(random_state=42)),\n                ],\n                voting=\"hard\",\n            )\n        else:\n            clf = LogisticRegression(random_state=42)\n        clf.fit(X, y)\n\n        if method == \"sigmoid\":\n            calib_clf = _CalibratedClassifier(clf, method=_sigmoid_calibration)\n        else:\n            calib_clf = _CalibratedClassifier(clf, method=_isotonic_calibration, method)\n\n        calib_clf.fit(X, y)\n\n        y_pred_proba = calib_clf.predict_proba(X)[:, 1]\n        y_pred_calib = calib_clf.predict(X)\n\n        assert_array_almost_equal(y_pred_calib, clf.predict(X))\n        \n        \n", "    def test_calibration_display_from_estimator(data, method, ensemble):\n        X, y = data\n        if ensemble:\n            estimators = [\n                (\"logreg\", LogisticRegression(random_state=42)),\n                (\"rf\", RandomForestClassifier(random_state=42)),\n            ]\n            clf = VotingClassifier(estimators, voting=\"soft\", random_state=42)\n        else:\n            clf = LogisticRegression(random_state=42)\n\n        clf.fit(X, y)\n\n        disp = CalibrationDisplay.from_estimator(\n            clf, X, y, n_bins=5, strategy=\"quantile\", method=method, ref_line=True\n        )\n        ax = disp.ax_\n        disp.ax_.set_xlabel(\"Mean predicted probability\")\n        disp.ax_.set_ylabel(\"Fraction of positives\")\n        fig = disp.figure_\n        fig.canvas.draw_idle()\n\n", "    def test_calibration_display_from_estimator_ensemble(method, ensemble):\n        X, y = data()\n        if ensemble:\n            estimators = [\n                (\"lr\", LogisticRegression(random_state=42)),\n                (\"dt\", DecisionTreeClassifier(random_state=42)),\n            ]\n            clf = VotingClassifier(estimators, voting=\"hard\")\n        else:\n            clf = LogisticRegression(random_state=42)\n        clf.fit(X, y)\n\n        disp = CalibrationDisplay.from_estimator(\n            clf, X, y, method=method, name=f\"{method.capitalize()} Ensemble\" if ensemble else method.capitalize()\n        )\n        disp.plot()\n", "    def test_calibration_display_from_estimator_isotonic(method, ensemble):\n        X, y = data()\n        if ensemble:\n            clf = VotingClassifier(\n                estimators=[\n                    (\"logreg\", LogisticRegression(random_state=42)),\n                    (\"dt\", DecisionTreeClassifier(random_state=42)),\n                ],\n                voting=\"hard\",\n            )\n        else:\n            clf = LogisticRegression(random_state=42)\n        clf.fit(X, y)\n        disp = CalibrationDisplay.from_estimator(\n            clf, X, y, method=method, name=\"CalibrationDisplay\", ref_line=True\n        )\n        disp.plot()\n"], "sample_1190": ["    def test_get_dimensional_expr(self):\n        unit_system = SI\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            expr = Quantity(10 * meter, \"m\")  \n            dim_expr = unit_system.get_dimensional_expr(expr)\n            self.assertEqual(dim_expr, length)\n            \n            expr = Quantity(10 * meter * 2 * kilogram, \"kg*m\")\n            dim_expr = unit_system.get_dimensional_expr(expr)\n            self.assertEqual(dim_expr, mass * length)\n            \n            expr = Quantity(10 * meter**2, \"m^2\")\n            dim_expr = unit_system.get_dimensional_expr(expr)\n            self.assertEqual(dim_expr, (length ** 2))\n            \n            expr = Quantity(10 * meter * volt, \"V*m\")\n            dim_expr = unit_system.get_dimensional_expr(expr)\n            self.assertEqual(dim_expr, (length * energy / (charge * time)))\n\n            expr = Quantity(10 * ampere, \"A\")\n            dim_expr = unit_system.get_dimensional_expr(expr)\n            self.assertEqual(dim_expr, charge / time)\n            \n            expr = Quantity(10 * ohm, \"\u03a9\")\n            dim_expr = unit_system.get_dimensional_expr(expr)\n            self.assertEqual(dim_expr, (energy * time) / (charge ** 2))\n\n\n\n", "    def test_get_dimensional_expr(self):\n        from sympy.physics.units import Quantity\n        system = SI\n        \n        self.assertEqual(system.get_dimensional_expr(S.One), S.One)\n        self.assertEqual(system.get_dimensional_expr(2), 2)\n        self.assertEqual(system.get_dimensional_expr(meter), length)\n        self.assertEqual(system.get_dimensional_expr(Quantity(2, meter)), 2*length)\n        self.assertEqual(system.get_dimensional_expr(meter**2), length**2)\n        self.assertEqual(system.get_dimensional_expr(Quantity(2, meter)**3), 2**3 * length**3)\n        self.assertEqual(system.get_dimensional_expr(Add(meter, meter)), 2*length)\n        self.assertEqual(system.get_dimensional_expr(Add(meter, kilogram)), raises(ValueError))\n        \n        # Test derivatives\n        x = Symbol('x')\n        self.assertEqual(system.get_dimensional_expr(diff(meter, x)), length/time)\n", "    def test_derived_units(self):\n        u1 = Quantity(10 * m)\n        u2 = Quantity(1 * kg)\n        u3 = Quantity(1 * s)\n\n        sys = SI.extend(base=(u1, u2, u3), \n                         derived_units={pressure: Quantity(u1.dimension * u2.dimension / u3.dimension)})\n        assert sys.derived_units == {presssure: Quantity(u1.dimension * u2.dimension / u3.dimension)}\n", "    def test_get_unit_system(self):\n        self.assertEqual(UnitSystem.get_unit_system(\"SI\"), SI)\n\n        with raises(ValueError):\n            UnitSystem.get_unit_system(\"nonexistent\")\n        \n        with raises(ValueError): \n            UnitSystem.get_unit_system(123)\n", "    def test_get_dimensional_expr(self):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", DeprecationWarning)\n            self.assertEqual(\n                SI.get_dimensional_expr(meter),\n                length,\n            )\n            self.assertEqual(\n                SI.get_dimensional_expr(kilogram),\n                mass,\n            )\n            self.assertEqual(\n                SI.get_dimensional_expr(second),\n                time,\n            )\n            self.assertEqual(\n                SI.get_dimensional_expr(joule),\n                energy,\n            )\n            self.assertEqual(\n                SI.get_dimensional_expr(volt),\n                energy/charge/time,\n            )\n            self.assertEqual(\n                SI.get_dimensional_expr(ohm),\n                energy/time/charge**2,\n            )\n            self.assertEqual(\n                SI.get_dimensional_expr(kg * m**2 / s**2),\n                SI.get_dimensional_expr(SI.Quantity(\"kg * m**2 / s**2\")),\n            )\n            self.assertEqual(\n                SI.get_dimensional_expr(m**2),\n                length**2,\n            )\n            self.assertEqual(\n                SI.get_dimensional_expr(m**3),\n                length**3,\n            )\n            self.assertEqual(\n                SI.get_dimensional_expr(10*meter),\n                10*length,\n            )\n            self.assertEqual(\n                SI.get_dimensional_expr(meter**2),\n                length**2,\n            )\n            self.assertEqual(\n                SI.get_dimensional_expr(2*m/s),\n                2*length/time,\n            )\n            self.assertEqual(\n                SI.get_dimensional_expr(sqrt(meter)),\n                sqrt(length),\n            )\n            self.assertEqual(\n                SI.get_dimensional_expr(exp(meter)),\n                exp(length),\n            )\n            self.assertEqual(\n                SI.get_dimensional_expr(sin(meter)),\n                sin(length),\n            )\n            self.assertEqual(\n                SI.get_dimensional_expr(log(meter)),\n"], "sample_908": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_713": ["    def test_ridge_singular_matrix(self):\n        # Test case for singular matrix\n        X = [[1, 2], [2, 4]]\n        X = sp.csr_matrix(X)\n        y = np.array([3, 7])\n        ridge = Ridge(alpha=0.1)\n        with assert_warns(UserWarning,\n                         message=\"Singular matrix detected\"):\n            ridge.fit(X, y)\n", "    def test_ridge_gcv_singlepass(self):\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([0, 1, 2])\n        estimator = _RidgeGCV(alphas=[0.1, 1.0, 10.0], cv=None)\n        estimator.fit(X, y)\n        assert_array_almost_equal(estimator.cv_values_, np.array([\n            [0.45862353, 0.10885879, 0.03004593],\n            [0.45862353, 0.10885879, 0.03004593],\n            [0.45862353, 0.10885879, 0.03004593]\n        ]))\n        assert_equal(estimator.alpha_, 1.0)\n", "    def test_ridge_eigen_dense(self):\n        np.random.seed(0)\n        X = np.random.randn(100, 20)\n        y = np.random.randn(100)\n        reg = Ridge(_solver='eigen')\n        reg.fit(X, y)\n        assert_raises(ValueError, lambda: reg.fit(2 * X, y))\n        assert_raises(ValueError, lambda: reg.fit(X, 2 * y))\n", "    def test_ridgecv_sparse_data(self):\n        n_samples = 200\n        n_features = 10\n        random_state = 42\n        X = SPARSE_FILTER(np.random.rand(n_samples, n_features))\n        y = np.random.rand(n_samples)\n        rng = check_random_state(random_state)\n        alphas = 10 ** rng.uniform(-5, 2, size=20)\n\n        ridgecv = RidgeCV(alphas=alphas, cv=5)\n        ridgecv.fit(X, y)\n        assert_array_almost_equal(ridgecv.coef_, ridgecv.coef_)\n        assert_greater(ridgecv.alpha_, 0)\n\n\n\n", "    def test_ridge_gcv_eigen(self):\n        X, y = make_regression(n_samples=10, n_features=5, random_state=0)\n        estimator = _RidgeGCV(alphas=np.array([0.1, 1.0]),\n                              fit_intercept=True,\n                              normalize=False,\n                              gcv_mode='eigen')\n        estimator.fit(X, y)\n        assert_equal(estimator.store_cv_values, False)\n"], "sample_208": ["    def test_migration_complex_m2m_through(self):\n        before_states = [\n            author_with_former_m2m,\n            contract,\n            publisher,\n            book,\n            other_pony,\n            other_stable,\n        ]\n        after_states = [\n            author_with_m2m,\n            contract,\n            publisher,\n            book,\n            other_pony,\n            other_stable,\n            attribution,\n        ]\n\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertMigrationDependencies(changes, \"testapp\", 0, [\"otherapp\"])\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"CreateModel\", \"AlterModel\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, model_name=\"testapp.Author\", old_field=None, new_field=\"publisher\")\n\n", "    def test_migration_detect_changes_simple_models(self):\n        # Simple models with no dependencies\n        before_states = [\n            ModelState(\"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True)), (\"name\", models.CharField(max_length=200))]),\n            ModelState(\"otherapp\", \"Book\", [(\"id\", models.AutoField(primary_key=True)), (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE))])\n        ]\n        after_states = [\n            ModelState(\"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True)), (\"name\", models.CharField(max_length=200)), (\"bio\", models.TextField())]),\n            ModelState(\"otherapp\", \"Book\", [(\"id\", models.AutoField(primary_key=True)), (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)), (\"published\", models.BooleanField())])\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertMigrationDependencies(changes, \"testapp\", 0, [])\n        self.assertMigrationDependencies(changes, \"otherapp\", 0, [\"testapp\"])\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"CreateModel\", \"AddField\"])\n        self.assertOperationTypes(changes, \"otherapp\", 0, [\"CreateModel\", \"AddField\"])\n\n", "    def test_migration_detect_order_wrt(self):\n        before_states = [\n            author_with_book_order_wrt,\n        ]\n        after_states = [\n            author_with_book_order_wrt,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, 'testapp', 0)\n\n", "    def test_detect_changes_on_proxy_models(self):\n        before_states = [\n            author_proxy,\n            author_proxy_notproxy,\n            author_proxy_third,\n            author_proxy_third_notproxy,\n            book_proxy_fk,\n            book_proxy_proxy_fk,\n        ]\n        after_states = [\n            author_proxy,\n            author_proxy_notproxy,\n            author_proxy_third,\n            author_proxy_third_notproxy,\n            book_proxy_fk,\n            book_proxy_proxy_fk,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertEqual(len(changes), 0)\n", "    def test_assert_field_attributes(self):\n        before_states = [\n            author_name_deconstructible_list_1,\n            publisher,\n            book,\n        ]\n        after_states = [\n            author_name_deconstructible_list_2,\n            publisher,\n            book,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, field_name='name', db_column='DeconstructibleAuthorName')\n"], "sample_458": ["        def test_floatformat01(self):\n            a = 1234.56789\n            b = 1234.567890\n            with self.subTest(a=a, b=b):\n                self.assertEqual(\n                    self.render_string(\"floatformat01\", {\"a\": a, \"b\": b}),\n                    \"1234.5679 1234.567890\",\n                )\n", "    def test_floatformat_with_precision_and_grouping(self):\n        self.render_context = {\"a\": Decimal(\"12345.6789\"), \"b\": 12345.6789}\n        self.assertEqual(self.render_context[\"floatformat01\"],\n                         mark_safe(\"12346{{floatformat01}}12345.679\")) \n", "    def test_floatformat_with_rounding(self):\n        with translation.override('en-us'):\n            with self.subTest(\"rounding\"):\n                a = Decimal(\"1.23456789\")\n                b = Decimal(\".123456789\")\n\n                # Test rounding behavior with different precisions\n                self.assertEqual(self.render_context({\"a\": a, \"b\": b}), \"1.23456789 0.123456789\")\n                self.assertEqual(\n                    self.render_context({\"a\": a, \"b\": b}), \"1.23 0.12\"\n                )\n                self.assertEqual(\n                    self.render_context({\"a\": a, \"b\": b}), \"1.2346 0.1235\"\n                )\n", "    def test_floatformat_with_locale(self):\n        with translation.override('en-us'):\n            self.assertEqual(\n                self.render(self.context, \"floatformat01\").strip(),\n                \"1.23 1.23\"\n            )\n\n        with translation.override('de-de'):\n            self.assertEqual(\n                self.render(self.context, \"floatformat01\").strip(),\n                \"1,23 1,23\"\n            )\n\n", "    def test_floatformat_with_rounding(self):\n        self.assertEqual(\n            self.render(\n                \"{% autoescape off %}{{ 3.141592654|floatformat:2 }}\"\n                \"{% endautoescape %}\",\n                {\"a\": 3.141592654, \"b\": 3.141592654},\n            ),\n            \"3.14\",\n        )\n\n\n\n"], "sample_1187": ["    def test_integration_reduction_dynamic_3d():\n        poly = Polygon(Point(0, 0, 0), Point(1, 0, 0), Point(1, 1, 0), Point(0, 1, 0))\n        expr = x**2 + y**2 + z\n        facets = hyperplane_parameters(poly)\n        result = integration_reduction_dynamic(poly, expr, facets)\n        assert result == [(S(1/3), S(1/3))]\n\n\n", "compilation error", "    def test_integration_reduction_dynamic_2D(self):\n        p = Polygon([Point(0, 0), Point(1, 0), Point(1, 1), Point(0, 1)])\n        expr = x**2 + y\n        bounds = [\n            (0, 1),\n            (0, 1),\n        ]\n        result = integration_reduction_dynamic(p, expr, bounds)\n        expected_result = main_integrate(expr, bounds,\n                                        polytope=p)\n        assert abs(result - expected_result) < 1e-6\n", "compilation error", "    def test_integration_reduction_dynamic_planar():\n        poly = Polygon([[Point(0, 0), Point(1, 0), Point(1, 1), Point(0, 1)]])\n        expr = x * y\n        result = integration_reduction_dynamic(poly, expr)\n        assert result == 1/2\n\n"], "sample_216": ["    def test_index_together_and_unique_together(self):\n        changes = self.get_changes(\n            [\n                ModelState(\"otherapp\", \"Book\", [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n                    (\"title\", models.CharField(max_length=200)),\n                ]),\n            ],\n            [\n                ModelState(\"otherapp\", \"Book\", [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n                    (\"title\", models.CharField(max_length=200)),\n                    \n                ], {\n                    \"index_together\": {(\"title\", \"author\")},\n                    \"unique_together\": {(\"title\", \"author\")},\n                }),\n            ],\n        )\n        \n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertMigrationDependencies(changes, \"otherapp\", 0, [])\n        self.assertOperationTypes(changes, \"otherapp\", 0, [\"CreateModel\"])\n        \n        self.assertOperationFieldAttributes(changes, \"otherapp\", 0, 0, index_together=(\"title\", \"author\"), unique_together=(\"title\", \"author\"))\n", "    def test_migration_dependencies_of_multiple_models(self):\n        migration_states = [\n            # Model: testapp.Author\n            ModelState(\"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True))]),\n            # Model: testapp.Book\n            ModelState(\"otherapp\", \"Book\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n                (\"title\", models.CharField(max_length=200)),\n            ]),\n        ]\n\n        before_states = [\n            # Model: testapp.Author\n            ModelState(\"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True))]),\n            # Model: testapp.Book\n            ModelState(\"otherapp\", \"Book\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n                (\"title\", models.CharField(max_length=200)),\n            ]),\n        ]\n        after_states = [\n            # Model: testapp.Author\n            ModelState(\"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True))]),\n            # Model: testapp.Book\n            ModelState(\"otherapp\", \"Book\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n                (\"title\", models.CharField(max_length=200)),\n            ]),\n        ]\n\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, \"testapp\", 0)\n        self.assertNumberMigrations(changes, \"otherapp\", 0)\n\n        # Check that there are no migrations generated because\n        # there are no changes\n        self.assertEqual(len(changes.get(\"testapp\", [])), 0)\n        self.assertEqual(len(changes.get(\"otherapp\", [])), 0)\n\n", "    def test_complex_migrations(self):\n        before_states = [\n            author_name_deconstructible_list_1,\n            author_name_deconstructible_list_2,\n            author_name_deconstructible_list_3,\n            author_name_deconstructible_tuple_1,\n            author_name_deconstructible_tuple_2,\n            author_name_deconstructible_tuple_3,\n            author_name_deconstructible_dict_1,\n            author_name_deconstructible_dict_2,\n            author_name_deconstructible_dict_3,\n            author_name_nested_deconstructible_1,\n            book,\n            publisher,\n            other_pony,\n            other_stable,\n            third_thing,\n        ]\n        after_states = [\n            author_name_deconstructible_changed_arg,\n            author_name_deconstructible_extra_arg,\n            author_name_nested_deconstructible_changed_kwarg,\n            author_name_nested_deconstructible_extra_kwarg,\n            author_name_nested_deconstructible_extra_kwarg,\n            author_proxy,\n            author_proxy_options,\n            author_proxy_notproxy,\n            author_proxy_third,\n            author_proxy_third_notproxy,\n            author_proxy_proxy,\n            author_unmanaged,\n            author_unmanaged_managed,\n            author_unmanaged_custom_pk,\n            author_with_m2m,\n            author_with_m2m_blank,\n            author_with_m2m_through,\n            author_with_former_m2m,\n            author_with_options,\n            book_proxy_fk,\n            book_proxy_proxy_fk,\n            book_migrations_fk,\n            book_with_no_author_fk,\n            book_with_no_author,\n            book_with_author_renamed,\n            book_with_field_and_author_renamed,\n            author_with_custom_user,\n            author_with_user,\n            publisher_with_author,\n            publisher_with_", "    def test_migration_field_attributes(self):\n        before_states = [\n            author_with_name_change,\n            author_with_biography_blank,\n        ]\n        after_states = [\n            author_with_name_change_2,\n            author_with_biography_blank_2,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, 'testapp', 2)\n        self.assertMigrationDependencies(changes, 'testapp', 0, ['testapp.author_proxy'])\n        self.assertMigrationDependencies(changes, 'testapp', 1, ['testapp.author_proxy'])\n        self.assertOperationTypes(changes, 'testapp', 0, ['RemoveField', 'AddField'])\n        self.assertOperationTypes(changes, 'testapp', 1, ['RemoveField', 'AddField'])\n        self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, field_name='name', max_length=200)\n        self.assertOperationFieldAttributes(changes, 'testapp', 0, 1, field_name='biography', blank=True)\n        self.assertOperationFieldAttributes(changes, 'testapp', 1, 0, field_name='name', max_length=200)\n        self.assertOperationFieldAttributes(changes, 'testapp', 1, 1, field_name='biography', blank=True)\n\n", "    def test_migration_dependencies(self):\n        before_states = [\n            author_name_deconstructible_list_1,\n            publisher,\n            book,\n        ]\n        after_states = [\n            author_name_deconstructible_list_2,\n            publisher,\n            book_with_author_renamed,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertMigrationDependencies(changes, \"testapp\", 0, [\"otherapp.Book\"])\n        self.assertMigrationDependencies(changes, \"testapp\", 1, [])\n        self.assertMigrationDependencies(changes, \"otherapp\", 0, [])\n        self.assertMigrationDependencies(changes, \"otherapp\", 1, [\"otherapp.Book\"])\n"], "sample_91": ["    def test_server_error_with_custom_template(self):\n        response = server_error(self.request_factory.get('/'))\n        self.assertEqual(response.status_code, 500)\n        self.assertTemplateUsed(response, '500.html')  # Assuming you have a template named '500.html'\n", "    def test_server_error_with_custom_template(self):\n        with override_settings(TEMPLATE_DIRS=[\n                'tests.templates'\n            ]):\n            with open('tests/templates/500.html', 'w') as f:\n                f.write(\"Custom 500 error page\")\n            response = server_error(\n                self.request_factory.get('/'), template_name='500.html'\n            )\n            self.assertEqual(response.status_code, 500)\n            self.assertEqual(response.content.decode('utf-8'), \"Custom 500 error page\")\n", "    def test_page_not_found_with_custom_template(self):\n        response = page_not_found(self.request_factory.get('/nonexistent_url/'), Http404)\n        self.assertEqual(response.status_code, 404)\n        self.assertTemplateUsed(response, '404.html')\n", "    def test_server_error(self):\n        request = self.request_factory.get('/')\n        response = server_error(request)\n        self.assertEqual(response.status_code, 500)\n\n        body = response.content.decode('utf-8')\n        self.assertIn('Server Error (500)', body)\n\n\n\n", "    def test_permission_denied(self):\n        request = self.request_factory.get('/')\n        response = permission_denied(request, exception=Exception('Permission denied!'))\n        self.assertEqual(response.status_code, 403)  \n        self.assertEqual(response.content.decode('utf-8'), ERROR_PAGE_TEMPLATE % {'title': '403 Forbidden', 'details': 'Permission denied!'})\n"], "sample_1068": ["    def test_indexed_with_contract():\n        from sympy import IndexedBase, Idx\n        len_y = 5\n        y = IndexedBase('y', shape=(len_y,))\n        t = IndexedBase('t', shape=(len_y,))\n        Dy = IndexedBase('Dy', shape=(len_y-1,))\n        i = Idx('i', len_y-1)\n        e = Eq(Dy[i], (y[i+1]-y[i])/(t[i+1]-t[i]))\n        code = octave_code(e.rhs, assign_to=e.lhs, contract=True)\n        print(code)\n        expected = \"\"\"", "    def test_matrix_element_access(self):\n        A = Matrix([[1, 2], [3, 4]])\n        self.assertEqual(octave_code(A[0, 1]), 'A(1, 2)')\n        self.assertEqual(octave_code(A[1, 0]), 'A(2, 1)')\n", "compilation error", "    def test_indexed_symbols(self):\n        from sympy import IndexedBase, Idx\n        n = Symbol('n', integer=True, positive=True)\n        y = IndexedBase('y', shape=(n,))\n        i = Idx('i', n)\n        expr = y[i] + y[i + 1]\n        code = octave_code(expr, assign_to=y, contract=True)\n        expected = \"for i = 1:n\\ny(i) = y(i) + y(i + 1);\\n\"\n        self.assertEqual(code, expected)\n\n", "    def test_Indexed_contraction(self):\n        from sympy import IndexedBase, Idx\n        len_y = 5\n        y = IndexedBase('y', shape=(len_y,))\n        t = IndexedBase('t', shape=(len_y,))\n        Dy = IndexedBase('Dy', shape=(len_y-1,))\n        i = Idx('i', len_y-1)\n        e = Eq(Dy[i], (y[i+1]-y[i])/(t[i+1]-t[i]))\n        code = octave_code(e.rhs, assign_to=e.lhs, contract=True)\n        expected = \"\"\""], "sample_480": ["    def test_json_field_validation(self):\n        model = JSONModel(json_field={\"a\": 1, \"b\": \"string\"})\n        model.full_clean()\n\n        with self.assertRaises(ValidationError):\n            model.json_field = \"invalid json\"\n        \n        model = JSONModel(json_field=None)\n        model.full_clean()\n\n        with self.assertRaises(ValidationError):\n            model.json_field = 123 \n\n\n\n", "    def test_empty_strings_are_not_allowed(self):\n        with self.assertRaises(ValidationError):\n            JSONModel.objects.create(json_field=\"\", )\n", "    def test_json_null_values(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\"DELETE FROM jsonmodel WHERE pk=1\")\n\n        model = JSONModel.objects.get(pk=1)\n        model.my_json_field = None\n        model.save()\n        self.assertIsNone(JSONModel.objects.get(pk=1).my_json_field)\n        \n        model.my_json_field = \"{'key': 'value'}\"\n        model.save()\n        self.assertEqual(JSONModel.objects.get(pk=1).my_json_field, \"{'key': 'value'}\") \n        \n        model.my_json_field = []\n        model.save()\n        self.assertEqual(JSONModel.objects.get(pk=1).my_json_field, [])\n\n\n\n", "    def test_complex_key_transformations(self):\n        a = JSONModel.objects.create(json_field={\"a\": {\"b\": 1, \"c\": \"test\"}, \"d\": 2})\n        with self.subTest(\"Nested keys\"):\n            result = JSONModel.objects.filter(\n                json_field__a__b=1\n            ).first()\n            self.assertEqual(result, a)\n\n        with self.subTest(\"Multiple nested keys\"):\n            result = JSONModel.objects.filter(\n                json_field__a__c=\"test\", json_field__d=2\n            ).first()\n            self.assertEqual(result, a)\n\n        with self.subTest(\"Key index\"):\n            result = JSONModel.objects.filter(\n                json_field__0=1\n            ).first()\n            self.assertEqual(result, a)\n\n        with self.subTest(\"List keys\"):\n            result = JSONModel.objects.filter(\n                json_field__list_field__0=1\n            ).first()\n            self.assertEqual(result, a)\n\n        with self.subTest(\"Mixed keys\"):\n            result = JSONModel.objects.filter(\n                json_field__a__b=1, json_field__list_field__0=1\n            ).first()\n            self.assertEqual(result, a)\n\n        with self.assertRaises(ValueError):\n            JSONModel.objects.filter(\n                json_field__a__b=1, json_field__a__b=2\n            ).first()\n\n\n\n", "    def test_key_transform_lookup_with_nested_json(self):\n        # Ensure nested JSON access works with KeyTransform and lookups.\n        with connection.cursor() as cursor:\n            cursor.execute(\n                \"\"\"\n                CREATE TABLE key_transform_nested_json_test (\n                    id INT PRIMARY KEY,\n                    data JSONB\n                );\n                INSERT INTO key_transform_nested_json_test (id, data) VALUES\n                    (1, '{\"foo\": {\"bar\": \"baz\"}}'),\n                    (2, '{\"foo\": [\"apple\", \"banana\"]}');\n                \"\"\"\n            )\n\n        obj1 = JSONModel.objects.create(data='{\"foo\": {\"bar\": \"baz\"}}')\n        obj2 = JSONModel.objects.create(data='{\"foo\": [\"apple\", \"banana\"]}')\n        qs = JSONModel.objects.filter(data__contains={'foo': {'bar': 'baz'}})\n        self.assertEqual(qs.count(), 1)\n        self.assertEqual(qs.get().data, obj1.data)\n        qs = JSONModel.objects.filter(\n            data__contains={'foo': {'bar': 'qux'}}\n        )\n        self.assertEqual(qs.count(), 0)\n        qs = JSONModel.objects.filter(data__contains={'foo': \"apple\"})\n        self.assertEqual(qs.count(), 1)\n        self.assertEqual(qs.get().data, obj2.data)\n\n        with self.assertRaises(exceptions.ValidationError):\n            JSONModel.objects.create(data=None)\n\n        with self.assertRaises(exceptions.ValidationError):\n            JSONModel.objects.create(data={})\n\n        with self.assertRaises(exceptions.ValidationError):\n            JSONModel.objects.create(data='abc')\n\n\n\n"], "sample_926": ["compilation error", "def test_enum_description_with_values():\n    input = 'enum MyEnum {\\n  VALUE1 = 1,\\n  VALUE2 = 2\\n}'\n    idDict = {'1': 'MyEnum.VALUE1', '2': 'MyEnum.VALUE2'}\n    output = 'enum MyEnum {\\n  VALUE1 = 1,\\n  VALUE2 = 2\\n}'\n    check('enum', input, idDict, output)\n", "compilation error", "def test_alias_with_parent_scope(env: BuildEnvironment):\n    from sphinx import addnodes\n    from sphinx.domains.cpp import AliasNode\n\n    parser = DefinitionParser(\n        \"alias_test::get_value\",\n        location=None,\n        config=env.config,\n    )\n    name = parser.parse_xref_object()\n    assert name == \"get_value\"\n\n    parentKey = \"alias_test\"\n    rootSymbol = env.domaindata['c']['root_symbol']  \n    parentSymbol = rootSymbol.direct_lookup(parentKey)\n    assert parentSymbol\n\n    s = parentSymbol.find_declaration(\n        name, typ='any', matchSelf=True, recurseInAnon=True\n    )\n    assert s is not None\n    assert s.declaration is not None\n\n    node = AliasNode(\"alias_test::get_value\", env=env, parentKey=parentKey)\n\n    nodes = []\n    options = dict()\n    signode = addnodes.desc_signature(\"alias_test::get_value\", '')\n    nodes.append(signode)\n    s.declaration.describe_signature(signode, 'markName', env, options)\n    node.replace_self(nodes)\n\n\n\n    assert node.astext() == \"alias_test::get_value\"\n", "def check_enum_declaration(name, input, idDict, output=None, key=None, asTextOutput=None):\n    if output is None:\n        output = input\n    check(name, input, idDict, output, key, asTextOutput)\n    # Check with enum scope\n    check(name, f'{input} {{\\n  }\\n', idDict, output + ' {\\n  }\\n', key,\n           asTextOutput + ' {\\n  }\\n' if asTextOutput is not None else None)\n    # Check with enum members\n    check(name, f'{input} {{\\n  member1;\\n}\\n', idDict, output + ' {\\n  member1;\\n}\\n', key,\n           asTextOutput + ' {\\n  member1;\\n}\\n' if asTextOutput is not None else None)\n"], "sample_203": ["    def test_email_validator_with_domain_allowlist(self):\n        validator = validators.EmailValidator(domain_allowlist=['example.com', 'foo.bar'])\n        self.assertFalse(validator(value='test@example.com'))\n        self.assertFalse(validator(value='test@foo.bar'))\n        with self.assertRaises(ValidationError):\n            validator(value='test@invalid.com')\n\n\n\n        # Test for IDN domains\n        validator = validators.EmailValidator(domain_allowlist=['xn--example-com', 'xn--foo-bar'])\n        self.assertFalse(validator(value='test@xn--example-com'))\n        self.assertFalse(validator(value='test@xn--foo-bar'))\n        with self.assertRaises(ValidationError):\n            validator(value='test@invalid.com')\n\n        with self.assertRaises(ValidationError):\n            validator(value='test@example.net')\n\n", "    def test_email_validator_with_domain_allowlist(self):\n        validator = validators.EmailValidator(domain_allowlist=['example.com'])\n        self.assertRaises(ValidationError, validator, 'invalid@example.com')\n        self.assertRaises(ValidationError, validator, 'test@domain.net')\n        self.assertEqual(validator('test@example.com'), None)\n\n        self.assertEqual(validator('test@localhost').__call__('test@localhost'), None)\n\n\n", "    def test_email_validator_with_punycode_domain(self):\n        with self.assertLogs() as cm:\n            validator = forms.EmailValidator()\n            value = 'user@xn--p1ai'\n            validator(value)\n            self.assertEqual(cm.output[0].levelno, 0)\n", "    def test_email_validator(self):\n        validator = forms.EmailValidator()\n        self.assertTrue(validator('test@example.com'))\n        self.assertTrue(validator('admin@example.com'))\n        self.assertTrue(validator('test+spam@example.com'))\n        self.assertFalse(validator('invalid email'))\n        self.assertFalse(validator('test@example'))\n        self.assertRaisesMessage(ValidationError, 'Enter a valid email address.', validator, 'foo')\n\n        # Test for internationalized domain names\n        self.assertTrue(validator('test@xn--example-kva.com'))\n", "    def test_prohibit_null_characters_validator(self):\n        validator = validators.ProhibitNullCharactersValidator()\n        self.assertEqual(validator('This is a test string'),\n                         'This is a test string')\n        with self.assertRaises(ValidationError):\n            validator('\\x00')\n        with self.assertRaises(ValidationError):\n            validator('This string has a null character\\x00')\n"], "sample_658": ["    def test_collect_file(self, tmpdir, path):\n        p = tmpdir.join(path)\n        p.write(\"\")\n        with pytest.raises(MODULE_NOT_FOUND_ERROR) as exc:\n            DoctestTextfile.collect(p, self.config)\n        assert str(exc.value) in (\"Module not found:\", \"No module named 'test'\")\n", "    def test_setup_fixtures(self, doctest_namespace):\n        item = DoctestItem(\"test_name\", None, None, None)\n        fixture_request = _setup_fixtures(item)\n        assert fixture_request is not None\n        assert isinstance(fixture_request, pytest.FixtureRequest)\n        assert \"doctest_namespace\" in fixture_request.fixturenames\n        assert fixture_request.getfixturevalue(\"doctest_namespace\") == doctest_namespace\n", "    def test_setup_fixtures(self, doctest_namespace):\n        doctest_item = DoctestItem(\n            name=\"test\",\n            parent=object(),\n            runner=_get_runner(),\n            dtest=object(),\n        )\n        fixture_request = _setup_fixtures(doctest_item)\n\n        assert fixture_request is not None\n        assert fixture_request._fixtureinfo is not None\n        assert fixture_request._fixtureinfo.fixture_names\n\n\n\n        doctest_namespace[\"fixture_value\"] = \"fixture value\"\n\n        fixture_request._fillfixtures()\n\n        assert fixture_request.getfixturevalue(\"fixture_value\") == \"fixture value\"\n\n\n", "    def test_setup_fixtures(self, doctest_namespace):\n        item = DoctestItem(\"test\", None, None, None)\n        item.fixture_request = _setup_fixtures(item)\n        assert item.fixture_request is not None\n        assert item.funcargs == {}\n\n\n", "    def test_collect_doctest_in_string(\n        self,\n        test_file,\n        expected_exception,\n        expected_error_message,"], "sample_672": ["    def test_saferepr_exceptions(self):\n        class MyException(Exception):\n            pass\n\n        class MyClass:\n                raise MyException\n\n        obj = MyClass()\n        repr_str = saferepr(obj)\n        assert \"<[MyException raised in repr()] MyClass object at 0x...\" in repr_str \n", "def test_saferepr_exceptions():\n    class MyException(Exception):\n        pass\n\n    class MyObject:\n            raise MyException()\n\n    with pytest.raises(MyException):\n        saferepr(MyObject())\n\n\n\n", "    def test_saferepr_exception(self):\n        class MyException(Exception):\n            pass\n\n        class MyClass:\n                raise MyException()\n\n        obj = MyClass()\n        result = saferepr(obj, maxsize=100)\n        assert \"<[MyException raised in repr()] object at 0x...\" in result\n", "    def test_saferepr_exceptions(monkeypatch):\n        class MyException(Exception):\n            pass\n\n        class MyClass:\n                raise MyException\n\n        obj = MyClass()\n        repr_str = saferepr(obj, maxsize=100)\n        assert \"MyException\" in repr_str\n        assert \"MyClass\" in repr_str\n        assert \"object at\" in repr_str\n\n\n\n", "    def test_saferepr_exception(monkeypatch):\n        class MyException(Exception):\n            pass\n\n        class MyObject:\n                raise MyException()\n\n        obj = MyObject()\n        with monkeypatch.raises(MyException):\n            saferepr(obj, maxsize=100)\n\n\n\n"], "sample_778": ["    def test_fit_transform_alpha_l1_ratio(self, solver):\n        X = np.random.rand(10, 5)\n        with np.errstate(divide='ignore'):\n            nmf_model = NMF(n_components=3, init='random', solver=solver,\n                            alpha=0.5, l1_ratio=0.8)\n            nmf_model.fit_transform(X)\n\n        assert_greater(nmf_model.components_.norm(ord=1).sum(),\n                       0.9 * nmf_model.components_.size)\n        assert_less(nmf_model.components_.norm(ord=2).sum(),\n                     1.1 * nmf_model.components_.size) \n", "    def test_nndsvd_initialization(self):\n        rng = np.random.RandomState(0)\n        X = rng.rand(5, 3)\n        init = 'nndsvd'\n        W, H, _ = non_negative_factorization(X, init=init)\n        assert np.all(W >= 0)\n        assert np.all(H >= 0)\n", "    def test_nndsvd_init(self):\n        # Test NNDSVD initialization for n_components > n_features\n        X = np.random.rand(10, 5)\n        nmf = NMF(n_components=10, init='nndsvd', random_state=0)\n        nmf.fit(X)\n        assert nmf.components_.shape == (10, 5)\n        assert nmf.n_iter_ > 0\n", "    def test_non_negative_factorization_beta_loss(self, solver):\n        np.random.seed(0)\n        X = np.random.rand(50, 100)\n\n        if solver == 'mu':\n            beta_loss = ['frobenius']\n        else:\n            beta_loss = ['frobenius', 'kullback_leibler']\n\n\n        for b_loss in beta_loss:\n            W, H, n_iter = non_negative_factorization(\n                X=X, solver=solver, beta_loss=b_loss, n_components=20\n            )\n            assert W.shape[0] == X.shape[0]\n            assert W.shape[1] == 20\n            assert H.shape[0] == 20\n            assert H.shape[1] == X.shape[1]\n            assert n_iter > 0\n\n\n\n", "    def test_fit_transform_with_custom_init(self, solver):\n        X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n        n_components = 2\n        W_init = np.random.rand(X.shape[0], n_components)\n        H_init = np.random.rand(n_components, X.shape[1])\n\n        nmf = NMF(n_components=n_components, init='custom', solver=solver,\n                  tol=1e-4, random_state=0)\n\n        with ignore_warnings(category=ConvergenceWarning) as w:\n            W = nmf.fit_transform(X, W=W_init, H=H_init)\n\n        if solver == 'cd':\n            assert_array_almost_equal(W, W_init, tol=1e-4)\n        else:  # solver == 'mu'\n            assert_array_almost_equal(W, W_init, tol=1e-4)\n\n"], "sample_813": ["    def test_ard_threshold_lambda(self):\n        n_samples = 30\n        n_features = 10\n        X = np.random.rand(n_samples, n_features)\n        y = np.random.rand(n_samples)\n        rng = check_random_state(0)\n        # Set lambda to large values for most features\n        lambda_ = rng.rand(n_features) * 1e4\n        lambda_[0] = 1\n        # Fit ARD regression with different thresholds\n        for threshold in [1e-4, 1e0, 1e4]:\n            clf1 = ARDRegression(threshold_lambda=threshold)\n            clf1.fit(X, y)\n            clf2 = ARDRegression(threshold_lambda=threshold)\n            clf2.fit(X, y)\n            assert_array_almost_equal(clf1.coef_, clf2.coef_)\n            # Check that only features with lambda below threshold are kept\n            kept_features = np.where(clf1.lambda_ < threshold)[0]\n            assert_equal(kept_features, np.where(clf2.lambda_ < threshold)[0])\n\n\n", "    def test_ard_threshold(self):\n        X, y = diabetes.data, diabetes.target\n        # Test that the threshold works as expected\n        rand_state = check_random_state(0)\n        X_noise = X + rand_state.randn(X.shape[0], X.shape[1]) * 0.1\n        y_noise = y + rand_state.randn(y.shape[0]) * 0.1\n        ard = ARDRegression(threshold_lambda=1e-4, n_iter=100)\n        ard.fit(X_noise, y_noise)\n        # Check that the number of non-zero lambda is approximately\n        # equal to the number of features where lambda is above the threshold\n        n_non_zero = np.sum(ard.lambda_ > 1e-4)\n        assert_array_less(n_non_zero, X.shape[1],\n                          msg=\"Number of non-zero lambda is too high\")\n", "    def test_ard_threshold(self):\n        X, y = diabetes.data, diabetes.target\n        n_features = X.shape[1]\n        np.random.seed(0)\n\n        # Fit an ARDRegression model with a very low threshold\n        ard_reg = ARDRegression(n_iter=10, threshold_lambda=1e-10)\n        ard_reg.fit(X, y)\n\n        # Check that almost all coefficients are zero after pruning\n        assert_array_almost_equal(ard_reg.coef_, \n                                  np.zeros(n_features), \n                                  decimal=4)\n\n        # Fit a Ridge regression model for comparison\n        ridge_reg = Ridge(alpha=1.0)\n        ridge_reg.fit(X, y)\n\n        # Compare the coefficients of the two models\n        assert not np.allclose(ard_reg.coef_, ridge_reg.coef_)\n\n        # Increase the threshold and refit the ARDRegression model\n        ard_reg = ARDRegression(n_iter=10, threshold_lambda=1.0)\n        ard_reg.fit(X, y)\n\n        # Check that some coefficients are left after pruning\n        assert np.any(ard_reg.coef_ != 0)\n\n        # Compare the coefficients of the two models\n        assert np.allclose(ard_reg.coef_.shape, ridge_reg.coef_.shape)\n", "    def test_sparse_data(self):\n        X = np.random.rand(100, 5).astype(np.float32)\n        X[::2, :] = 0.\n        y = np.random.rand(100)\n        br = BayesianRidge()\n        br.fit(X, y)\n        br_sparse = BayesianRidge()\n        br_sparse.fit(X.tocsr(), y)\n\n        assert_array_almost_equal(br.coef_, br_sparse.coef_)\n        assert_almost_equal(br.alpha_, br_sparse.alpha_)\n        assert_almost_equal(br.lambda_, br_sparse.lambda_)\n", "    def test_ard_regression_with_zero_variance(self):\n        X = np.ones((10, 2))\n        y = np.zeros(10)\n        clf = ARDRegression(compute_score=False)\n        with assert_raise_message(\n            \"Cannot compute covariance matrix: variance must be positive\"\n        ):\n            clf.fit(X, y)\n"], "sample_873": ["    def test_inverse_transform_sparse_matrix(self):\n        X = sp.csc_matrix(np.arange(20).reshape(2, 10))\n        ss = StepSelector(step=2)\n        ss.fit(X)\n        Xt = ss.transform(X)\n        Xinv = ss.inverse_transform(Xt)\n        assert_array_equal(Xinv.toarray(), X.toarray())\n", "    def test_step_selector_basic(\n        self, step, n_features_in, expected_support, expected_feature_names_out", "    def test_inverse_transform_with_sparse_input(self):\n        step_selector = StepSelector(step=2)\n        X_sparse = sp.csc_matrix(X)\n        step_selector.fit(X_sparse)\n\n        X_transformed = step_selector.transform(X_sparse)\n        X_inverted = step_selector.inverse_transform(X_transformed)\n        assert_array_equal(X_inverted.toarray(), X_sparse.toarray())\n\n\n", "    def test_step_selector(self, step):\n        selector = StepSelector(step=step)\n        selector.fit(X)\n        assert_array_equal(selector.get_support(), support)\n        assert_array_equal(selector.transform(X), Xt)\n        assert_array_equal(selector.inverse_transform(Xt), X)\n        assert_array_equal(selector.get_feature_names_out(), feature_names_t)\n\n\n\n", "    def test_inverse_transform_sparse(self):\n        selector = StepSelector(step=2)\n        X_sparse = sp.csr_matrix(X)\n        selector.fit(X_sparse)\n\n        X_transformed = selector.transform(X_sparse)\n        X_inverted = selector.inverse_transform(X_transformed)\n\n        assert_array_equal(X_inverted.toarray(),  X_sparse.toarray())\n"], "sample_744": ["    def test_handle_unknown_onehot_ordinal(self):\n        enc = CategoricalEncoder(handle_unknown='ignore')\n        X = np.array([['A', 1], ['B', 2], ['C', 3]])\n        enc.fit(X)\n        X_new = np.array([['D', 4]])\n        X_tr = enc.transform(X_new)\n        assert_array_equal(X_tr.shape, (1, 3))\n        assert_array_equal(\n            toarray(X_tr), [[0, 0, 0, 1, 1, 0]])\n        X_inv = enc.inverse_transform(X_tr)\n        assert_array_equal(X_inv, [[None, 4]])\n", "    def test_power_transform_ordinal_encoding_with_unknown_categories(self):\n        # Test with ordinal encoding and unknown categories\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        encoder = PowerTransformer(method='ordinal', handle_unknown='ignore')\n        with pytest.warns(DataConversionWarning):\n            encoder.fit(X)\n        X_transformed = encoder.transform([ [1, 2], [7, 8]])\n        for i in range(X_transformed.shape[1]):\n            assert_equal(X_transformed[:,i][0], encoder.categories_[i][0])\n        X_transformed = encoder.transform([ [1, 2], [7, 8]])\n        assert_array_equal(encoder.inverse_transform(X_transformed),\n                        [[1, 2], [None, None]])\n", "    def test_power_transform_ordinal(self):\n        X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n        pt = PowerTransformer(method='ordinal', copy=True)\n        X_trans = pt.fit_transform(X)\n        assert_array_equal(X_trans.shape, X.shape)\n        assert_array_equal(X_trans[0], [0, 1, 2])\n", "def test_power_transform_ordinal():\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\n    pt = PowerTransformer(method='ordinal', copy=False)\n    pt.fit(X)\n\n    transformed = pt.transform(X)\n    assert_array_equal(transformed,\n                       np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]]))\n\n    inverse_transformed = pt.inverse_transform(transformed)\n    assert_array_equal(inverse_transformed, X)\n\n    # Check for handling of unknown categories\n    X_new = np.array([[10, 11, 12]])\n    with pytest.raises(ValueError):\n        pt.transform(X_new)\n\n\n", "    def test_categorical_encoder_transform_inverse(encoding):\n        enc = CategoricalEncoder(encoding=encoding)\n\n        X = [['A', 1], ['B', 2], ['A', 3], ['C', 1]]\n\n        enc.fit(X)\n        X_transformed = enc.transform(X)\n\n        if encoding == 'ordinal':\n            assert_array_equal(X_transformed.shape, (4, 1))\n        else:\n            assert_array_equal(X_transformed.shape, (4, 2))\n        \n        X_inverse = enc.inverse_transform(X_transformed)\n\n        assert_array_equal(X_inverse, X)\n"], "sample_118": ["    def test_in_with_subquery(self):\n        articles = Article.objects.filter(\n            id__in=Game.objects.filter(\n                name='Game 1'\n            ).values_list('article_id', flat=True)\n        )\n        self.assertEqual(list(articles.values_list('slug', flat=True)), ['a1',])\n", "    def test_range_lookup(self):\n        with self.subTest(operator='Greater'):\n            # Test for values outside the range\n            qs = Article.objects.filter(pub_date__range=(datetime(2005, 8, 2, 0, 0), datetime(2005, 7, 25, 0, 0)))\n            self.assertEqual(qs.count(), 0)\n\n        with self.subTest(operator='Less'):\n            # Test for values inside the range\n            qs = Article.objects.filter(pub_date__range=(datetime(2005, 7, 26, 0, 0), datetime(2005, 7, 28, 0, 0)))\n            self.assertEqual(qs.count(), 4)\n\n        with self.subTest(operator='Equal'):\n            # Test for values at the start of the range\n            qs = Article.objects.filter(pub_date__range=(datetime(2005, 7, 26, 0, 0), datetime(2005, 7, 26, 0, 0)))\n            self.assertEqual(qs.count(), 1)\n\n\n", "    def test_year_lookup_exact(self):\n        year = 2005\n        articles_in_year = Article.objects.filter(\n            pub_date__year=year\n        ).values_list('headline', flat=True)\n        self.assertEqual(set(articles_in_year), {'Article 1', 'Article 2', 'Article 3', 'Article 4'})\n\n        articles_in_year = Article.objects.annotate(\n            year=Extract('pub_date', 'year')\n        ).filter(year=year).values_list('headline', flat=True)\n        self.assertEqual(set(articles_in_year), {'Article 1', 'Article 2', 'Article 3', 'Article 4'})\n\n\n", "    def test_contains_empty_string(self):\n        with self.assertRaises(EmptyResultSet):\n            Article.objects.filter(headline__icontains='').exists()\n\n\n", "    def test_year_lookup_with_direct_value(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\"SET session_replication_role = 'replica';\")\n        articles_2005 = Article.objects.filter(pub_date__year=2005)\n        self.assertQueryCount(articles_2005, 4)\n\n\n"], "sample_437": ["    def test_transaction_atomic_block_errors(self):\n        with connection.schema_editor() as schema_editor:\n            schema_editor.create_model(Person)\n            schema_editor.create_model(Square)\n\n        with transaction.atomic():\n            connection.cursor().execute(\"INSERT INTO person (name) VALUES ('John Doe')\")\n            connection.commit()  \n\n        with transaction.atomic():\n            try:\n                connection.cursor().execute(\n                    \"INSERT INTO person (name) VALUES ('Jane Doe')\"\n                )\n                raise Exception\n            except Exception:\n                # intentionally raise an exception to check its handling\n\n        with transaction.atomic():\n            with self.assertRaises(DatabaseError):\n                connection.cursor().execute(\"SELECT * FROM person WHERE name='Jane Doe'\")\n", "    def test_savepoint_rollback(self):\n        with transaction.atomic():\n            person = Person.objects.create(name=\"Alice\")\n            transaction.savepoint()\n            person.name = \"Bob\"\n            person.save()\n\n            with self.assertRaises(transaction.TransactionManagementError):\n                person.name = \"Charlie\"\n                person.save()\n\n            transaction.savepoint_rollback()\n            self.assertEqual(person.name, \"Alice\")\n\n", "    def test_close_if_unusable_or_obsolete(self, mock_is_usable):\n        wrapper = connections[DEFAULT_DB_ALIAS]\n        with patch('django.db.backends.base.base.BaseDatabaseWrapper.get_database_version') as mock_get_version:\n            mock_get_version.return_value = (1, 1, 1)\n            wrapper.close_at = time.monotonic() + 10\n            mock_is_usable.return_value = True\n            wrapper.close_if_unusable_or_obsolete()\n            self.assertIsNone(wrapper.connection)\n            self.assertEqual(wrapper.close_at, None)\n            \n            mock_is_usable.return_value = False\n            wrapper.close_if_unusable_or_obsolete()\n            self.assertIsNone(wrapper.connection)\n\n    \n", "    def test_check_database_version_supported(self, mock_get_version):\n        mock_get_version.return_value = (1, 2, 3)\n\n        with override_settings(DATABASES={'default': {'ENGINE': 'some_db', 'TEST_NAME': 'test_db'}}), \\\n                transaction.atomic():\n            connections['default'].ensure_connection()\n            connections['default'].check_database_version_supported()\n            \n        mock_get_version.assert_called_once()\n", "    def test_atomic_with_savepoints_and_rollback_within_atomic(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\n                'CREATE TABLE IF NOT EXISTS test_transactions (id INTEGER PRIMARY KEY, name TEXT)'\n            )\n\n        Person.objects.create(name=\"Alice\")\n\n        with transaction.atomic():\n            with connection.cursor() as cursor:\n                cursor.execute('INSERT INTO test_transactions (name) VALUES (%s)', ('Bob',))\n\n            with connection.atomic():\n                with connection.cursor() as cursor:\n                    cursor.execute('UPDATE test_transactions SET name = %s WHERE id = %s', ('Charlie', 1,))\n\n            with connection.atomict():  # Simulate an inner atomic block\n\n                with connection.cursor() as cursor:\n                    cursor.execute('INSERT INTO test_transactions (name) VALUES (%s)', ('David',))\n                connection.rollback()\n\n        # Check that the 'Charlie' update didn't take effect\n        with connection.cursor() as cursor:\n            cursor.execute('SELECT name FROM test_transactions WHERE id = 1')\n            row = cursor.fetchone()\n            self.assertEqual(row[0], 'Bob')\n\n        # Ensure the 'David' insert didn't take effect\n        with connection.cursor() as cursor:\n            cursor.execute('SELECT name FROM test_transactions WHERE name = \\'David\\'')\n            rows = cursor.fetchall()\n            self.assertEqual(len(rows), 0)\n\n\n\n\n"], "sample_970": ["    def test_signature_from_ast_varargs_kwargs():\n        code = \"\"\"\n            pass\"\"\"\n        node = ast.parse(code)\n        function = node.body[0]\n        sig = inspect.signature_from_ast(function, code)\n        assert sig.parameters['x'].kind == inspect.Parameter.POSITIONAL_OR_KEYWORD\n        assert sig.parameters['args'].kind == inspect.Parameter.VAR_POSITIONAL\n        assert sig.parameters['kwargs'].kind == inspect.Parameter.VAR_KEYWORD\n\n\n\n", "    def test_is_singledispatch_method_new_style(self):\n        @functools.singledispatchmethod\n            return x\n\n        assert inspect.is_singledispatch_method(my_func)\n", "    def test_signature_from_str(func, expected):\n\n        signature = stringify_signature(signature_from_str(expected))\n        assert signature == expected\n\n\n\n\n", "    def test_stringify_signature_with_forward_ref():\n        sig = inspect.signature(typing.List[typealias.MyAlias])\n        str_sig = stringify_signature(sig, unqualified_typehints=True)\n        assert str_sig == '(<class \\'MyAlias\\')'\n\n\n\n", "    def test_signature_from_str():\n        sig = '(name: str, age: int = 18) -> None'\n        signature = inspect.signature_from_str(sig)\n        assert signature.parameters['name'].kind == inspect.Parameter.POSITIONAL_OR_KEYWORD\n        assert signature.parameters['name'].annotation == str\n        assert signature.parameters['age'].kind == inspect.Parameter.POSITIONAL_OR_KEYWORD\n        assert signature.parameters['age'].annotation == int\n        assert signature.parameters['age'].default == 18\n        assert signature.return_annotation == inspect.Parameter.empty\n\n        sig = '(*args: Any) -> Any'\n        signature = inspect.signature_from_str(sig)\n        assert signature.parameters['args'].kind == inspect.Parameter.VAR_POSITIONAL\n        assert signature.parameters['args'].annotation == Any\n        assert signature.return_annotation == Any\n\n        sig = '(positional: int, *other: str) -> list'\n        signature = inspect.signature_from_str(sig)\n        assert signature.parameters['positional'].annotation == int\n        assert signature.parameters['other'].kind == inspect.Parameter.VAR_POSITIONAL\n        assert signature.parameters['other'].annotation == str\n        assert signature.return_annotation == list\n\n        sig = '(a: int, b: int = 1, *c: float) -> None'\n        signature = inspect.signature_from_str(sig)\n        assert signature.parameters['a'].annotation == int\n        assert signature.parameters['b'].annotation == int\n        assert signature.parameters['b'].default == 1\n        assert signature.parameters['c'].kind == inspect.Parameter.VAR_POSITIONAL\n        assert signature.parameters['c'].annotation == float\n        assert signature.return_annotation == inspect.Parameter.empty\n"], "sample_64": ["    def test_urlencode(self):\n        q = QueryDict('a=1&b=2&c=3')\n        self.assertEqual(q.urlencode(), 'a=1&b=2&c=3')\n        q = QueryDict('', mutable=True)\n        q.update({'a': '1', 'b': '2', 'c': '3'})\n        self.assertEqual(q.urlencode(), 'a=1&b=2&c=3')\n\n\n        q = QueryDict('a=1&b=2&c=3', mutable=True)\n        q.update({'b': '4'})\n        self.assertEqual(q.urlencode(), 'a=1&b=4&c=3')\n        q.append('d', '5')\n        self.assertEqual(q.urlencode(), 'a=1&b=4&c=3&d=5')\n\n\n            \n\n\n\n        q = QueryDict('a[0]=1&a[1]=2&a[2]=3', mutable=True)\n        q.append('a', '4')\n        self.assertEqual(q.urlencode(), 'a=1&a=2&a=3&a=4')\n        q = QueryDict('', mutable=True)\n        q.update({'a': ['1', '2', '3']})\n        self.assertEqual(q.urlencode(), 'a=1&a=2&a=3')\n\n", "    def test_query_dict_constructor_without_args(self):\n        query_dict = QueryDict()\n        self.assertEqual([], query_dict.lists())\n        self.assertEqual({}, query_dict.dict())\n", "    def test_query_dict_append(self):\n        qs = QueryDict('key1=value1&key2=value2')\n        qs.append('key1', 'value3')\n        self.assertDictEqual(qs.dict(), {'key1': ['value1', 'value3'], 'key2': ['value2']})\n        qs.append('key3', 'value4')\n        self.assertDictEqual(qs.dict(), {'key1': ['value1', 'value3'], 'key2': ['value2'], 'key3': ['value4']})\n\n", "    def test_urlencode_with_unicode(self):\n        q = QueryDict('foo=bar&baz=\u4e2d\u6587', mutable=True)\n        encoded_query = q.urlencode()\n        self.assertEqual(encoded_query, 'foo=bar&baz=%E4%B8%AD%E6%9C%AC')\n\n", "    def test_query_dict_ordering(self):\n        qd = QueryDict('foo=bar&baz=qux&a=1&b=2')\n        self.assertEqual(list(qd.keys()), ['foo', 'baz', 'a', 'b'])\n        self.assertEqual(list(qd.values()), ['bar', 'qux', ['1'], ['2']])\n        self.assertEqual(qd['foo'], 'bar')\n        self.assertEqual(qd.get('baz'), 'qux')\n        self.assertEqual(qd.get('nonexistent'), None)\n        self.assertEqual(qd.getlist('a'), ['1'])\n        self.assertEqual(qd.getlist('b'), ['2'])\n        self.assertEqual(qd.getlist('nonexistent'), [])\n        self.assertEqual(qd.items(), [\n            ('foo', 'bar'),\n            ('baz', 'qux'),\n            ('a', '1'),\n            ('b', '2'),\n        ])\n\n        qd.set('a', '3')\n        self.assertEqual(qd['a'], '3')\n        qd.append('a', '4')\n        self.assertEqual(qd['a'], ['3', '4'])\n\n        qd.setlist('foo', ['new value 1', 'new value 2'])\n        self.assertEqual(qd['foo'], ['new value 1', 'new value 2'])\n\n        qd.delete('baz')\n        self.assertEqual(qd.get('baz'), None)\n        self.assertFalse('baz' in qd)\n\n        qd.set('newkey', 'newvalue')\n        self.assertEqual(qd['newkey'], 'newvalue')\n"], "sample_1097": ["    def test_blockdiag_matrix_matmul():\n        BDM = BlockDiagMatrix(A, B)\n\n        expr = BDM * BDM\n        result = block_collapse(expr)\n        expected = BlockDiagMatrix(A*A, B*B)\n        assert result == expected\n", "def test_bc_matadd():\n    C = BlockMatrix([[A, B], [C, D]])\n    D2 = BlockDiagMatrix([D, D])\n    assert block_collapse(C + D2) == BlockMatrix([[A + D, B + D], [C + D, D + D]])\n\n    C2 = BlockMatrix([[A, B], [C, D]])\n    D2 = BlockDiagMatrix([D, D])\n    assert block_collapse(C2 + D2) == BlockMatrix([[A + D, B + D], [C + D, D + D]])\n\n\n\n    assert block_collapse(BlockMatrix([[B]) + BlockMatrix([[A]])) == \\\n        BlockMatrix([[A + B]])\n\n\n\n    assert block_collapse(BlockMatrix([[A, B], [C, D]]) + \\\n        BlockMatrix([[E, F], [G, H]])) == BlockMatrix([[A + E, B + F], [C + G, D + H]])\n", "    def test_block_collapse_matpow():\n        M = BlockMatrix([[A, B],\n                         [MatrixSymbol('C', n, n), D]])\n        N = M ** 3\n        print(block_collapse(N))\n\n\n\n", "compilation error", "    def test_block_collapse_block_mul(cls):\n        assert block_collapse(b1 * b2) == BlockMatrix([[G*G + H*G, H*H]])\n"], "sample_836": ["compilation error", "    def test_multiclass_errors(self, y_examples, msg):\n        for y_example in y_examples:\n            with pytest.raises(ValueError, match=msg):\n                type_of_target(y_example)\n", "compilation error", "compilation error", "compilation error"], "sample_602": ["    def test_to_netcdf_empty_dataset(tmpdir):\n        ds = xr.Dataset()\n        path = tmpdir / \"test.nc\"\n        to_netcdf(ds, path, mode=\"w\", compute=False)\n        store = backends.NetCDF4Store(path, engine=_get_default_engine())\n        assert len(store.variables) == 0\n", "    def test_to_netcdf_append_to_existing_data(self):\n        with xr.open_dataset(\"data.nc\", \"w\") as ds:\n            ds[\"var1\"] = ((\"x\", \"y\"), np.random.randn(3, 4))\n            ds[\"var2\"] = ((\"x\", \"y\"), np.random.randn(3, 4))\n\n        with xr.open_dataset(\"data.nc\", \"a\") as ds:\n            ds[\"var1\"] = ((\"x\", \"y\"), np.random.randn(3, 4) + 1)\n\n", "    def test_to_netcdf_with_invalid_netcdf(self):\n        ds = xr.Dataset(\n            {\"a\": ((\"x\", \"y\"), np.arange(10))},\n            coords={\"x\": [0, 1, 2], \"y\": [0, 1]},\n        )\n\n        with self.assertRaises(ValueError):\n            to_netcdf(\n                ds,\n                path_or_file=\"invalid_netcdf.nc\",\n                engine=\"scipy\",\n                invalid_netcdf=True,\n            )\n\n        with self.assertRaises(ValueError):\n            to_netcdf(\n                ds,\n                path_or_file=\"invalid_netcdf.nc\",\n                engine=\"h5netcdf\",\n                invalid_netcdf=True,\n            )\n\n\n", "    def test_to_netcdf_with_invalid_netcdf(\n        monkeypatch", "    def test_to_netcdf_with_invalid_attrs(self):\n        ds = xr.Dataset(\n            {\"a\": ((\"x\", \"y\"), np.arange(4 * 5).reshape(4, 5))},\n            coords={\"x\": [0, 1, 2, 3], \"y\": [0, 1, 2, 3]},\n        )\n\n        invalid = {\"invalid_attr_1\": 1, \"invalid_attr_2\": True}\n        ds.attrs.update(invalid)\n\n        with self.assertRaises(ValueError) as e:\n            to_netcdf(ds, \"test.nc\", format=\"NETCDF4\", invalid_netcdf=True)\n\n        self.assertIn(\n            \"unrecognized option 'invalid_netcdf' for engine\", str(e.exception)\n        )\n\n        with self.assertRaises(ValueError) as e:\n            to_netcdf(ds, \"test.nc\", format=\"scipy\", invalid_netcdf=True)\n\n        self.assertIn(\n            \"unrecognized option 'invalid_netcdf' for engine\", str(e.exception)\n        )\n"], "sample_702": ["    def test_runpytest_subprocess_plugins(self) -> None:\n        \"\"\"Test plugins in runpytest_subprocess.\"\"\"\n        __tracebackhide__ = True\n        p = self.runpytest_subprocess(\"--plugin=dummy\")\n        assert p.ret != 0\n        self._assert_plugin_output(\"Dummy plugin\", p.stdout)\n\n", "    def test_testdir_legacy_path(self, testdir):\n        \"\"\"Test that legacy paths are correctly handled.\"\"\"\n        p = testdir.tmpdir\n        assert isinstance(p, LEGACY_PATH)\n        assert str(p) == str(p)\n", "    def test_legacy_path_roundtrip(self) -> None:  # Test legacy_path\n        pyt = self.pytester\n        legacy_path_str = \"legacy_path1\"\n        legacy_path_1 = legacy_path(path=legacy_path_str)\n        assert legacy_path_1.str == legacy_path_str\n        \n        legacy_path_2 = pyt.makepydir(\"legacy_path2\")\n        assert legacy_path_2.str == \"legacy_path2\"\n\n\n", "    def test_str(self) -> None:\n        t = Testdir()\n        assert str(t) == str(t.tmpdir)\n", "def test_testdir_basic():\n    p = Pytester.parseconfig(\n        \"\"\"\n        [pytest]\n        addopts = --doctest-glob=*.txt\n        addopts = --showlocals\n        \"\"\",\n        _ispytestplugins=True,\n    )\n    td = Testdir(p)\n    td.mkdir(\"spam\")\n    td.copy_example(\"test_func.py\", \"spam/test_func.py\")\n\n    p.parseconfig(\n        \"\"\"\n        [pytest]\n        addopts = --doctest-glob=*.txt\n        addopts = --showlocals\n        \"\"\",\n        _ispytestplugins=True,\n    )\n    result = td.runpytest_subprocess()\n    result.stdout.fnmatch_lines([\"*collected*\", \"spam/test_func.py\"])\n    assert \"==================== test session starts ====================\" in result.stdout.str()\n    assert \"==================== 1 passed in 0. \" in result.stdout.str() \n    \n"], "sample_357": ["    def test_circular_dependent_model_migration(self):\n        changes = self.get_changes([\n            # Define the circular dependency here\n            author_with_book,\n            rabbit,\n        ], \n        [\n            # Update the circular dependency here\n            author_with_book,\n            rabbit,\n        ])\n\n        self.assertNumberMigrations(changes, 'eggs', 2)\n        self.assertMigrationDependencies(changes, 'eggs', 0, [])\n        self.assertMigrationDependencies(changes, 'eggs', 1, ['eggs.Knight'])\n        self.assertOperationTypes(changes, 'eggs', 0, ['CreateModel'])\n        self.assertOperationTypes(changes, 'eggs', 1, ['CreateModel'])\n\n", "    def test_migration_detection_with_unique_together(self):\n        changes = self.get_changes([\n            self.book,\n            self.book_foo_together,\n            self.book_foo_together_2,\n            self.book_foo_together_3,\n            self.book_foo_together_4,\n            self.aardvark,\n        ], [\n            self.book,\n            self.book_foo_together,\n            self.book_foo_together_2,\n            self.book_foo_together_3,\n            self.book_foo_together_4,\n            self.aardvark,\n            # Adding a new model to force migration\n            self.rabbit,\n        ])\n        self.assertNumberMigrations(changes, 'otherapp', 1)\n\n        self.assertMigrationDependencies(changes, 'otherapp', 0, ['testapp'])\n        self.assertOperationTypes(changes, 'otherapp', 0, ['CreateModel'])\n\n", "    def test_migration_for_changed_field_default(self):\n        author = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, default=\"Guest\")),\n        ])\n        author_empty_default = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ])\n        project_state_before = self.make_project_state([author])\n        project_state_after = self.make_project_state([author_empty_default])\n        changes = self.get_changes(project_state_before, project_state_after)\n        app_label, migrations = changes.items()[0]  # Get the first app's migrations\n        self.assertEqual(len(migrations), 1)\n        self.assertEqual(migrations[0].name, \"0001_initial\")\n        self.assertOperationTypes(changes, app_label, 0, [\"RemoveField\"])  \n        self.assertOperationFieldAttributes(\n            changes, app_label, 0, 0, field=\"name\", default=None\n        )\n\n", "    def test_changes_with_pk_rename(self):\n        before_states = self.make_project_state([\n            author_unmanaged,\n            publisher,\n            book,\n        ])\n        after_states = self.make_project_state([\n            author_unmanaged_custom_pk,\n            publisher,\n            book,\n        ])\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertMigrationDependencies(changes, \"testapp\", 0, [\"testapp\"])\n\n\n\n", "    def test_migration_for_added_model_field(self):\n        before_states = self.make_project_state([\n            author_name_deconstructible_list_1,\n        ])\n        after_states = self.make_project_state([\n            author_name_deconstructible_list_2,\n        ])\n        changes = self.get_changes(before_states, after_states)\n\n        self.assertNumberMigrations(changes, 'testapp', 1)\n\n        self.assertMigrationDependencies(changes, 'testapp', 0, [])\n        self.assertOperationTypes(changes, 'testapp', 0, ['AddField'])\n"], "sample_1074": ["    def test_polycyclic_group():\n        from sympy.combinatorics.pc_groups import PolycyclicGroup\n        G = SymmetricGroup(4)\n        p = G.polycyclic_group()\n        assert isinstance(p, PolycyclicGroup)\n        assert p.generators == G.generators\n        assert p.series == G.derived_series()\n\n        G2 = CyclicGroup(6)\n        p2 = G2.polycyclic_group()\n        assert isinstance(p2, PolycyclicGroup)\n        assert p2.generators == G2.generators\n        assert p2.series == G2.derived_series()\n\n        G3 = DihedralGroup(5)\n        p3 = G3.polycyclic_group()\n        assert isinstance(p3, PolycyclicGroup)\n        assert p3.generators == G3.generators\n        assert p3.series == G3.derived_series()\n\n        raises(ValueError, lambda: AlternatingGroup(5).polycyclic_group())\n\n\n\n", "    def test_strong_presentation():\n        G = PermutationGroup([Permutation([1, 2, 0, 3, 4, 5])])\n        G_p = G.strong_presentation()\n        group = FpGroup(G_p.free_group, G_p.relators)\n        _verify_bsgs(G, group)\n\n        G = DihedralGroup(6)\n        G_p = G.strong_presentation()\n        group = FpGroup(G_p.free_group, G_p.relators)\n        _verify_bsgs(G, group)\n \n        G = AlternatingGroup(4)\n        G_p = G.strong_presentation()\n        group = FpGroup(G_p.free_group, G_p.relators)\n        _verify_bsgs(G, group)\n", "    def test_polycyclic_group(self):\n        from sympy.combinatorics.pc_groups import PolycyclicGroup\n        G = SymmetricGroup(5)\n        pc_groups = G.polycyclic_group()\n        self.assertTrue(isinstance(pc_groups, PolycyclicGroup))\n        \n        G = AlternatingGroup(5)\n        pc_groups = G.polycyclic_group()\n        self.assertTrue(isinstance(pc_groups, PolycyclicGroup))\n        \n        G = CyclicGroup(12)\n        pc_groups = G.polycyclic_group()\n        self.assertTrue(isinstance(pc_groups, PolycyclicGroup))\n", "    def test_polycyclic_group(self):\n        G = DihedralGroup(4)\n        pc_g = G.polycyclic_group()\n        H = PolycyclicGroup(pc_g.sequence, pc_g.series, pc_g.relative_order)\n        assert is_isomorphic(G, H)\n\n\n\n", "    def test_polycyclic_group(self):\n        from sympy.combinatorics.pc_groups import PolycyclicGroup\n        G1 = CyclicGroup(5)\n        G2 = SymmetricGroup(3)\n        G3 = DihedralGroup(6)\n        pc_G1 = G1.polycyclic_group()\n        pc_G2 = G2.polycyclic_group()\n        pc_G3 = G3.polycyclic_group()\n        assert pc_G1.is_polycyclic is True\n        assert pc_G2.is_polycyclic is True\n        assert pc_G3.is_polycyclic is True\n\n        try:\n            pc_G3.order()\n            assert False\n        except ValueError:\n            pass\n  \n        \n"], "sample_1006": ["    def test_binomial_rewrite_as_gamma_negative_n(self):\n        n = -5\n        k = 2\n        result = binomial(n, k).rewrite('gamma')\n        expected = (gamma(n + 1) /\n                    (gamma(k + 1) * gamma(n - k + 1)))\n        self.assertEqual(result, expected) \n", "    def test_binomial_complex_args(self):\n        n = Symbol('n', real=True)\n        k = Symbol('k', real=True)\n        assert binomial(n, k)  != binomial(complex(n), k) \n        assert binomial(n, complex(k)) != binomial(n, k) \n        assert binomial(complex(n), complex(k)) != binomial(n, k)\n", "    def test_binomial_large_numbers():\n        n = Symbol('n', integer=True)\n        k = Symbol('k', integer=True)\n        for i in range(10):\n            n = i + 10\n            k = i + 5\n            assert binomial(n, k) == binomial(n, n - k)\n            assert binomial(n, 0) == 1\n            assert binomial(n, n) == 1\n            assert binomial(n, k).rewrite(factorial) \\\n                == factorial(n)/ (factorial(k)*factorial(n-k))\n", "    def test_binomial_rewrite_as_factorial(self):\n        n = Symbol('n', integer=True)\n        k = Symbol('k', integer=True)\n\n        self.assertEqual(\n            binomial(n, k).rewrite_as_factorial(),\n            factorial(n)/(factorial(k)*factorial(n - k))\n        )\n\n        self.assertEqual(\n            binomial(n, 0).rewrite_as_factorial(),\n            1\n        )\n\n        self.assertEqual(\n            binomial(n, n).rewrite_as_factorial(),\n            1\n        )\n\n\n", "    def test_binomial_negative_arguments_and_n_plus_k():\n        n = Symbol('n', integer=True)\n        k = Symbol('k', integer=True)\n        assert binomial(n, -1) == S.Zero\n        assert binomial(-1, k) == S.Zero\n        assert binomial(-1, -1) == S.Zero\n        assert binomial(n, n + 1) == S.Zero\n        assert binomial(-1, n) == S.Zero\n        assert binomial(n, -n) == S.Zero\n        assert binomial(n, n) == 1\n"], "sample_1059": ["    def test_laguerre_assoc_laguerre_diff_w_respect_to_alpha(self):\n        n = Symbol('n', integer=True)\n        alpha = Symbol('alpha')\n        x = Symbol('x')\n        with raises(ArgumentIndexError, match=r\"index 1\"):\n            diff(assoc_laguerre(n, alpha, x), n)\n        with raises(ArgumentIndexError, match=r\"index 3\"):\n            diff(assoc_laguerre(n, alpha, x), x, 3)\n        result = diff(assoc_laguerre(n, alpha, x), alpha)\n        expected = Sum(assoc_laguerre(k, alpha, x) / (n - alpha), (k, 0, n - 1))\n        self.assertEqual(result, expected)\n", "    def test_legendre_assoc_laguerre_0(self):\n        n = Symbol('n', integer=True)\n        alpha = Symbol('alpha')\n        result = assoc_laguerre(n, alpha, x)\n        expected = sum(\n            (\n                -1)**k * binomial(n, k) * (\n                    alpha + k\n                ) * x**k / (\n                    factorial(k)\n                ) for k in range(n + 1)\n        )\n        self.assertEqual(result, expected)\n", "    def test_laguerre_zero_and_one():\n        n = Symbol('n')\n        a = Symbol('a')\n        for n in range(5):\n            for a in [0, 1, 2, 3]:\n                with raises(ArgumentIndexError):\n                    assoc_laguerre(n, a, x).fdiff(4)  \n                with raises(ArgumentIndexError):\n                    assoc_laguerre(n, a, x).fdiff(0)\n                with raises(ArgumentIndexError):\n                    assoc_laguerre(n, a, x).fdiff(-1)    \n\n                assert assoc_laguerre(n, a, 0) == binomial(n + a, a) \n\n                if n != 0:\n                    assert assoc_laguerre(0, a, x) == 1\n                    assert assoc_laguerre(1, a, x) == a - x + 1  \n                    assert assoc_laguerre(n, 0, x) == laguerre(n, x)\n                    with raises(ValueError):\n                        laguerre(n, x)\n", "    def test_laguerre_derivative():\n        n = Symbol('n')\n        a = Symbol('a')\n        assert diff(assoc_laguerre(n, a, x), x) == -assoc_laguerre(n - 1, a + 1, x)\n        assert diff(assoc_laguerre(n, a, x), a) == Sum(assoc_laguerre(k, a, x) / (n - a), (k, 0, n - 1))\n\n", "    def test_assoc_laguerre_special_values():\n        n = Symbol('n')\n        a = Symbol('a')\n        with raises(ValueError):\n            assoc_laguerre(n, a, S.Infinity)\n        with raises(ValueError):\n            assoc_laguerre(-1, a, S.Infinity)\n        with raises(ValueError):\n            assoc_laguerre(n, a, S.Infinity)\n        assert assoc_laguerre(n, a, 0) == binomial(n + a, a)\n        assert assoc_laguerre(0, a, x) == 1\n        assert assoc_laguerre(1, a, x) == a - x + 1\n\n\n\n"], "sample_435": ["    def test_create_user_errors(self):\n        form = UserCreationForm(data={\"username\": \" \"})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors[\"username\"], [_(\"This field is required.\")])\n\n        form = UserCreationForm(data={\"username\": \"testclient\"})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors[\"username\"], [_(\"A username is required.\")])\n\n        form = UserCreationForm(data={\"username\": \"testclient\", \"password1\": \"password1\", \"password2\": \"password2\"})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors[\"password2\"], [\"The two password fields didn\u2019t match.\"])\n", "    def test_clean_password2_with_mismatch(self):\n        form = UserCreationForm(\n            data={\n                \"username\": \"testuser\",\n                \"password1\": \"password\",\n                \"password2\": \"wrongpassword\",\n            }\n        )\n        self.assertFalse(form.is_valid())\n        self.assertIn(\n            self.error_messages[\"password_mismatch\"], form.errors[\"password2\"]\n        )\n", "    def test_creation_form_validates_password_strength(self):\n        form = UserCreationForm(\n            data={\n                \"username\": \"testclient2\",\n                \"password1\": \"p\",\n                \"password2\": \"p\",\n            }\n        )\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors[\"password1\"], [\n            \"Password must be at least 8 characters long and include 1 uppercase letter,\"\n            \" 1 number, 1 symbol, and 1 lowercase letter.\"\n        ])\n\n        form = UserCreationForm(\n            data={\n                \"username\": \"testclient2\",\n                \"password1\": \"P@sswOrd1\",\n                \"password2\": \"P@sswOrd1\",\n            }\n        )\n        self.assertTrue(form.is_valid())\n", "    def test_initial_password_hash(self):\n        form = UserCreationForm(data={\"username\": \"newuser\"})\n        self.assertIn(\"password\", form.fields)\n        self.assertTrue(isinstance(form.fields[\"password\"], ReadOnlyPasswordHashField))\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors[\"password\"], [\n            _('This field is required.')\n        ])\n\n\n\n", "    def test_user_creation_form_saves_password_correctly(self):\n        form = UserCreationForm(\n            data={\n                \"username\": \"newuser\",\n                \"password1\": \"password\",\n                \"password2\": \"password\",\n            }\n        )\n        self.assertTrue(form.is_valid())\n        new_user = form.save()\n        self.assertEqual(new_user.password, \"password\")\n"], "sample_232": ["    def test_empty_strings_allowed(self):\n        with self.connection.cursor() as cursor:\n            cursor.execute(\n                \"CREATE TABLE json_field_test (id INTEGER PRIMARY KEY, data JSON)\"\n            )\n\n        model = JSONModel.objects.create(data={'key1': 'value1'})\n        self.assertEqual(model.data, {'key1': 'value1'})\n\n        model.data = ''\n        model.full_clean()\n        self.assertEqual(model.data, {})\n\n        model.save()\n\n        model = JSONModel.objects.get(pk=model.pk)\n        self.assertEqual(model.data, {})\n\n\n", "    def test_json_field_contains_lookup(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\n                \"INSERT INTO jsonmodel (id, data) VALUES (%s, '{\\\"name\\\": \\\"Alice\\\", \\\"age\\\": 30}')\",\n                (1,),\n            )\n\n        data = {\"name\": \"Alice\", \"age\": 30}\n        q = JSONModel.objects.filter(data__contains={\"name\": \"Alice\"})\n        self.assertEqual(q.query.get_nodes()[0]['lookup']['rhs'], 'Alice')\n        self.assertEqual(q.count(), 1)\n\n        q = JSONModel.objects.filter(data__contains={\"age\": 30})\n        self.assertEqual(q.query.get_nodes()[0]['lookup']['rhs'], 30)\n        self.assertEqual(q.count(), 1)\n", "    def test_key_transform_lookup_with_nested_keys(self):\n        with self.assertRaises(ValidationError):\n            NullableJSONModel.objects.filter(json_field__data__somekey__key_path='value')\n        with self.assertRaises(ValidationError):\n            NullableJSONModel.objects.filter(json_field__data__somekey__key_path=['value'])\n\n\n", "    def test_serialization_with_custom_decoder(self):\n        # Create a JSONModel instance with a custom JSON decoder\n        model_instance = JSONModel(\n            data=json.dumps({'key1': 1, 'key2': 'value2'}),\n            custom_decoder=CustomJSONDecoder,\n        )\n        model_instance.save()\n\n        # Retrieve the instance and assert the decoded data\n        retrieved_instance = JSONModel.objects.get(id=model_instance.id)\n        self.assertEqual(retrieved_instance.data, {'key1': 1, 'key2': 'value2'})\n\n", "    def test_json_field_contains_lookup(self):\n        model = JSONModel.objects.create(data={'foo': 1, 'bar': 2})\n        with self.subTest(lookup='contains'):\n            self.assertEqual(\n                model.objects.filter(data__contains={'foo': 1}).count(),\n                1,\n            )\n        with self.subTest(lookup='contained_by'):\n            self.assertEqual(\n                model.objects.filter(data__contained_by=model.data).count(),\n                1,\n            )\n\n\n"], "sample_31": ["    def test_latex_names_false(self, cosmo):\n        with pytest.raises(IORegistryError):\n            write_latex(cosmo, \"temp.txt\", latex_names=False, format=\"latex\")\n", "    def test_latex_names_off(self, cosmo):\n        with pytest.raises(IORegistryError):\n            write_latex(\n                cosmo, \"test.tex\", latex_names=False, format=\"latex\"\n            )\n\n        assert not os.path.exists(\"test.tex\")\n", "    def test_write_latex_with_latex_names_disabled(self, cosmo):\n        with pytest.raises(IORegistryError) as excinfo:\n            write_latex(\n                cosmology=cosmo,\n                file=self.temp_file,\n                latex_names=False,\n            )\n        assert (\n            \"Could not find a writer for format latex\"\n            in str(excinfo.value)\n        )\n", "    def test_write_latex_with_no_units(self, cosmo):\n        with pytest.raises(IORegistryError):\n            write_latex(cosmo, \"tmp.tex\", format=\"latex\")\n\n", "    def test_latex_names(self, cosmo):\n        with self.temp_file(\"test.tex\") as file:\n            write_latex(cosmo, file, latex_names=True, format=\"latex\")\n            with open(file, \"r\") as f:\n                content = f.read()\n            for name in cosmo.__parameters__:\n                if name in _FORMAT_TABLE:\n                    assert _FORMAT_TABLE[name] in content\n\n\n"], "sample_1178": ["    def test_scopes(self):\n        with raises(TypeError):\n            Scope()  \n        scope = Scope(body=[\n            Variable('x', type=intc),\n            Assign(Variable('x'), 42),\n            Return(Variable('x'))\n        ])\n        code = pycode(scope)\n        assert code == \"\"\"\\", "    def test_FunctionCall(self):\n        from sympy.codegen.ast import FunctionCall\n        from sympy import symbols, ccode\n        x = symbols('x')\n        fcall = FunctionCall('foo', [x])\n        assert ccode(fcall) == 'foo(x)'\n\n\n        fcall2 = FunctionCall('bar', ['baz', 1, x])\n        assert ccode(fcall2) == 'bar(baz, 1, x)'\n", "    def test_function_definition(self):\n        f = FunctionDefinition(\n            return_type=f32,\n            name='foo',\n            parameters=[x, y],\n            body=[Return(x + y)],\n            attrs=[value_const]\n        )\n        self.assertEqual(f.name, 'foo')\n        self.assertEqual(f.return_type, f32)\n        self.assertEqual(len(f.parameters), 2)\n        self.assertEqual(f.parameters[0].symbol, x)\n        self.assertEqual(f.parameters[1].symbol, y)\n        self.assertEqual(len(f.body), 1)\n        self.assertIsInstance(f.body, CodeBlock)\n        self.assertIsInstance(f.body.exprs[0], Return)\n        self.assertEqual(f.body.exprs[0].return_, x+y)\n        self.assertEqual(f.attrs, (value_const,))\n\n\n\n", "    def test_function_definition(self):\n        func = FunctionDefinition(real, 'foo', [x, y],\n                                [Assignment(z, x*y)])\n        self.assertEqual(ccode(func),\n                         'real foo(real x, real y){ z = x*y; }')\n\n        func = FunctionDefinition(Integer, 'foo', [x],\n                                [Return(x + 1)])\n        self.assertEqual(ccode(func),\n                         'int foo(int x){ return x + 1; }')\n\n", "    def test_Declaration(self):\n        decl = Declaration(Variable('x'))\n        self.assertEqual(decl.variable.name, 'x')\n        self.assertEqual(decl.variable.type, untyped)\n        self.assertEqual(decl.variable.attrs, ())\n\n        decl = Declaration(Variable('y', value=42))\n        self.assertEqual(decl.variable.name, 'y')\n        self.assertEqual(decl.variable.type, untyped)\n        self.assertEqual(decl.variable.attrs, ())\n        self.assertEqual(decl.variable.value, 42)\n"], "sample_180": ["    def test_index_together_with_invalid_field(self):\n        with self.assertRaises(Error) as cm:\n            class Foo(models.Model):\n                id = models.AutoField(primary_key=True)\n                name = models.CharField(max_length=50)\n\n                class Meta:\n                    index_together = [('name', 'nonexistent_field')]\n\n        self.assertEqual(\n            str(cm.exception),\n            \"'index_together' refers to the nonexistent field, related \"\n            \"field, or lookup 'nonexistent_field'.\"\n        )\n", "    def test_index_together_with_related_fields(self):\n        with override_settings(DATABASE_ROUTERS={'default': EmptyRouter()}):\n            class WithRelatedField(models.Model):\n                some_field = models.CharField(max_length=100)\n\n                class Meta:\n                    indexes = [models.Index(fields=['some_field', 'related_field'], name='my_index')]\n\n            class WithRelated(models.Model):\n                related_field = models.IntegerField()\n\n                class Meta:\n                    related_models = [WithRelatedField]\n\n            with self.assertNumQueries(1):\n                with connections['default'].atomic():\n                    WithRelated.objects.create(related_field=1)\n                    WithRelatedField.objects.create(some_field='test', related_field=1)\n\n            # Check if the index was created\n            with connections['default'].cursor() as cursor:\n                cursor.execute(\"\"\"\n                    SELECT indexname\n                    FROM pg_indexes\n                    WHERE tablename = '%s'\n                    AND indexname = 'my_index';\n                \"\"\" % WithRelatedField._meta.db_table)\n                result = cursor.fetchone()\n                self.assertIsNotNone(result)\n\n\n\n", "    def test_index_together_with_only_one_field(self):\n        class MyModel(models.Model):\n            text = models.CharField(max_length=100)\n\n        with self.subTest('Should work when only one field is specified'):\n            with self.assertRaises(ValueError):\n                MyModel.objects.index_together(['text'])\n", "    def test_index_together_with_conditions(self):\n        with override_settings(DATABASE_ROUTERS=['EmptyRouter']):\n            class MyModel(models.Model):\n                id = models.IntegerField(primary_key=True)\n                name = models.CharField(max_length=100)\n\n            with self.assertRaises(Warning):\n                class InvalidModel(MyModel):\n                    class Meta:\n                        index_together = [\n                            ('name', 'id'),\n                        ]\n\n", "    def test_index_together_with_conditions(self):\n        with override_settings(DATABASE_ROUTERS={'default': EmptyRouter()}):\n            class MyModel(models.Model):\n                field1 = models.CharField(max_length=100)\n                field2 = models.IntegerField()\n\n                class Meta:\n                    indexes = [\n                        models.Index(fields=['field1'], condition=models.Q(field2__gt=10)),\n                    ]\n\n            with self.assertRaises(Warning) as context:\n                _check_lazy_references([MyModel])\n            self.assertIn(f\"'default' does not support indexes with conditions.\", str(context.exception))\n\n\n\n"], "sample_1003": ["    def test_domain_postprocess(self):\n        options = Options(gens=(x, y, z), domain=EX)\n\n        self.assertEqual(options['domain'], EX)\n\n", "    def test_domain_polyring_invalid(self):\n        with raises(OptionError):\n            Options((x, ), {'domain': 'Z[[x,y]]'})\n", "    def test_domain_ex_without_gens(self):\n        with raises(GeneratorsError):\n            Options(gens=(x, y, z), {'domain': EX})\n", "    def test_domain_composite(self):\n        with raises(OptionError):\n            Options(\n                gens=(x, y),\n                {'domain': 'composite', 'gens': (x, y)})\n", "    def test_domain_with_gens(self):\n        with raises(GeneratorsError):\n            Options((x, y, z), {'domain': ZZ, 'gens': (x, y)})\n"], "sample_1096": ["def test_idx_range_infinity():\n    from sympy import oo\n    from sympy.tensor.indexed import Idx\n\n    idx = Idx('i', oo)\n    assert idx.lower is None\n    assert idx.upper is oo\n\n", "    def test_stride_derivation(self):\n        from sympy.tensor.array import NDimArray\n        n, m, o, p = symbols('n m o p', integer=True)\n        A = IndexedBase('A', shape=(n, m), strides=(o, p))\n        x, y, z = symbols('x y z')\n        i = Idx('i', m)\n        j = Idx('j', n)\n        a = A[i, j]\n        with self.assertWarns(UserWarning):\n            b = Derivative(a, x)\n        assert b == 0\n", "    def test_range_errors():\n        from sympy.core.sympify import _sympify\n\n        with raises(TypeError):\n            Idx('j', (1, 2, 3))\n        with raises(TypeError):\n            Idx('i', \"n\")\n        with raises(TypeError):\n            Idx('i', (1, 'a'))\n        with raises(TypeError):\n            Idx('i', (1, 2, S.Infinity + 1))\n        with raises(IndexException):\n            Idx('i', 1, 2)\n        with raises(TypeError):\n            Idx('i', 1, 'a')\n", "    def test_shape_override(self):\n        A = IndexedBase('A', shape=(2, 3))\n        idx1 = Idx('i', 5)\n        idx2 = Idx('j', 4)\n        self.assertEqual(A[idx1, idx2].shape, (2, 3))  # Shape from IndexedBase\n\n        B = IndexedBase('B')\n        self.assertEqual(B[idx1, idx2].shape, (5, 4))  # Shape from indices\n\n", "    def test_idx_shapes(self):\n        from sympy.tensor.indexed import Idx\n        from sympy.tensor.array.ndim_array import NDimArray\n\n        n, m = symbols('n m', integer=True)\n        i = Idx('i', n)\n        j = Idx('j', m)\n\n        a = NDimArray('A', (n, m))\n        self.assertEqual(a[i, j].shape, (n, m))\n\n        b = IndexedBase('B')\n        c = b[i, j]\n        self.assertEqual(c.shape, (n, m))\n        self.assertEqual(c.ranges, [(0, n - 1), (0, m - 1)])\n        self.assertEqual(c.rank, 2)\n\n\n\n"], "sample_665": ["    def test_collect_function_with_fixtures(self, capsys):\n        from _pytest.collect import Collector\n\n            return {'value': 42}\n\n        class MyTestClass:\n            @pytest.fixture\n                return {'my_fixture_value': 10}\n\n                assert my_fixture['value'] == 42\n                assert my_fixture_name['my_fixture_value'] == 10\n\n        code = \"\"\"\n        class MyTestClass:\n            @pytest.fixture\n                return {'my_fixture_value': 10}\n\n                assert my_fixture['value'] == 42\n                assert my_fixture_name['my_fixture_value'] == 10\n\n            return {'value': 42}\n        \"\"\"\n        # Collect the pytest objects\n        collector = Collector._from_source(code)\n        \n        # Execute the test\n        config = pytest.main(['-v', '-s'], plugins=['_pytest.fixtures'],\n                            _from_argv=['test_fixtures.py'])  \n        \n        \n        # Assert the captured output\n\n        out, err = capsys.readouterr()\n        print(\"Out:\", out)\n\n", "    def test_exit_code_from_hook(self, capsys):\n        class CustomSession(Session):\n                self.exitcode = exitcode\n                \n        custom_session = CustomSession()\n        custom_session._pytestconfig.hook = pytest.hookimpl.Hookimpl(\n            name=\"pytest_sessionfinish\",\n            impl=lambda session: session._exit(1),\n        )\n        custom_session.perform_collect()\n        custom_session.run(\n            collectonly=False,\n            plugins=[],\n            pyargs=[],\n            args=[],\n            config=custom_session._pytestconfig,\n            rootdir=py.path.local(),\n            exitcode=None,\n        )\n        assert custom_session.exitcode == 1\n        out, err = capsys.readouterr()\n        assert out == \"\"\n\n", "    def test_showfixtures(self, capsys):\n        from _pytest.config import _set_config_option\n\n        _set_config_option(\"fixtures\", \"list_all\", True)\n\n        with pytest.raises(ValueError) as excinfo:\n            pytest.main([\"--fixtures\", \"mytestmodule\"])\n        assert \"fixtures requires\" in str(excinfo.value)\n\n        with pytest.raises(ValueError) as excinfo:\n            pytest.main([\"--fixtures\", \"-v\", \"mytestmodule\"])\n        assert \"fixtures requires\" in str(excinfo.value)\n\n        with self.assert_captured(sys.stdout, new=True) as captured:\n            pytest.main(\n                [\"--showfixtures\", \"--no-collect\", \"mytestmodule\"],\n                exit=False,\n            )\n\n        output = captured.read().strip()\n        assert \"fixtures\" in output\n\n\n\n", "    def test_showfixtures_empty(self, capsys):\n        from _pytest.main import wrap_session\n\n        # Create a session with no fixtures defined\n        config = pytest.Config(\n            options={}\n        )\n        session = wrap_session(config, _showfixtures_main)\n        session.perform_collect()\n\n        # Capture the output\n        out, err = capsys.readouterr()\n\n        # Assert that the output is empty\n        assert out == \"\"\n\n", "    def test_collect(self):\n        config = pytest.config.parse_config()\n        session = Session(config, \"test_collector\", py.path.local())\n        nodes = session.collect()\n        #print(f'Nodes collected: {nodes}')\n        for node in nodes:\n          if isinstance(node, pytest.Function):\n            print(f'Function: {node.name}')\n            for arg in node.keywords:\n              print(f'  Argument: {arg}')\n        assert len(nodes) == 1\n\n\n\n"], "sample_341": ["    def test_absolute_max(self):\n        with self.assertRaises(ValueError):\n            formset_factory(Choice, max_num=100, absolute_max=50)\n\n        formset = formset_factory(Choice, absolute_max=100)\n        with self.assertRaises(ValidationError):\n            formset(data={\n                'TOTAL_FORM_COUNT': 101,\n            }).full_clean()\n", "    def test_formset_factory_with_validation_errors(self):\n        class InvalidChoiceForm(Form):\n            choice = CharField(required=False)\n\n        with self.assertRaises(ValidationError):\n            formset_factory(InvalidChoiceForm,\n                            can_delete=True,\n                            can_delete_extra=True,\n                            validate_max=True,\n                            max_num=1,\n                            validate_min=True,\n                            min_num=2)()\n", "    def test_validate_max(self):\n        formset_data = [('a', 1), ('b', 2), ('c', 3)]\n        formset = FavoriteDrinksFormSet(formset_data=formset_data, max_num=2)\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.total_error_count(), 1)\n        self.assertIn(\n            'Please submit at most 2 forms.', str(formset.errors)\n        )\n        formset = FavoriteDrinksFormSet(formset_data=formset_data, max_num=3)\n        self.assertTrue(formset.is_valid())\n", "    def test_formset_factory_set_extra_and_min_num(self):\n        # Test that formset_factory sets the extra and min_num arguments correctly.\n        formset = formset_factory(CustomKwargForm, extra=5, min_num=2)\n\n        self.assertEqual(formset.extra, 5)\n        self.assertEqual(formset.min_num, 2)\n\n        formset = formset_factory(CustomKwargForm, can_order=True, can_delete=True, validate_max=True,\n                                  validate_min=True, absolute_max=100)\n\n        self.assertEqual(formset.can_order, True)\n        self.assertEqual(formset.can_delete, True)\n        self.assertEqual(formset.validate_max, True)\n        self.assertEqual(formset.validate_min, True)\n        self.assertEqual(formset.absolute_max, 100)\n", "    def test_formset_factory_with_max_num_and_absolute_max(self):\n        with self.assertRaises(ValueError):\n            formset_factory(Choice,\n                            max_num=100,\n                            absolute_max=99)\n\n\n"], "sample_1123": ["    def test_condition_set_substitution_dummy_symbol(self):\n        c = ConditionSet(x, x < 1, {x, z})\n        result = c.subs(x, y)\n        assert result.args[0] == y\n        assert result.args[1] == And(y < 1)\n        assert result.args[2] == FiniteSet(y, z)\n\n        result = result.subs(y, z)\n        assert result.args[0] == z\n        assert result.args[1] == And(z < 1)\n        assert result.args[2] == FiniteSet(z, z)\n", "    def test_condition_set_substitution_with_dummy(self):\n        c = ConditionSet(x, x < 1, {x, y})\n        assert c.subs(x, L) == ConditionSet(L, L < 1, FiniteSet(L, y))\n        assert c.subs(L, Symbol('_x')) == ConditionSet(_x, _x < 1, FiniteSet(_x, y))\n        assert c.subs(y, z) == ConditionSet(x, x < 1, FiniteSet(x, z))\n", "    def test_subs_dummy(self):\n        cs = ConditionSet(x, x < 1, {x, y})\n        cs_subs = cs.subs(x, L)\n        assert cs_subs.sym == L\n        assert cs_subs.condition == And(L < 1)\n        assert cs_subs.base_set == FiniteSet(L, y)\n\n        cs_subs_2 = cs_subs.subs(L, y)\n        assert cs_subs_2.sym == y\n        assert cs_subs_2.condition == And(y < 1)\n        assert cs_subs_2.base_set == FiniteSet(y, y)\n\n        cs_subs_3 = cs_subs.subs(y, 2)\n        assert cs_subs_3.sym == L\n        assert cs_subs_3.condition == And(L < 1)\n        assert cs_subs_3.base_set == FiniteSet(L, 2)\n\n        cs_subs_4 = ConditionSet(x, x < 1, {x, y}).subs(y, 2)  # Base set\n        assert cs_subs_4.base_set == FiniteSet(x, 2)\n", "compilation error", "    def test_conditionset_subs_base_set(self):\n        cs = ConditionSet(x, Eq(x, 2), FiniteSet(x, y, z))\n        new_cs = cs.subs(x, L)\n        assert new_cs.sym == L\n        assert new_cs.condition == Eq(L, 2)\n        assert new_cs.base_set == FiniteSet(L, y, z)\n"], "sample_327": ["    def test_json_string_field(self):\n        field = JSONField()\n        self.assertIsInstance(field.widget, Textarea)\n        data = '{\"a\": 1, \"b\": \"two\"}'\n        value = field.to_python(data)\n        self.assertEqual(value, {\"a\": 1, \"b\": \"two\"})\n        self.assertIsInstance(value, dict)\n", "    def test_json_field_bound_data(self):\n        field = JSONField()\n        initial = {'key1': 'value1', 'key2': 2}\n        data = json.dumps(initial)\n        bound_data = field.bound_data(data, initial)\n        self.assertEqual(bound_data, initial)\n\n        invalid_data = 'invalid JSON'\n        bound_data = field.bound_data(invalid_data, initial)\n        self.assertIsInstance(bound_data, InvalidJSONInput)\n\n\n", "    def test_json_field_invalid_json(self):\n        field = JSONField()\n        with self.assertRaises(ValidationError):\n            field.to_python(\"invalid json\")\n        with self.assertRaises(ValidationError):\n            field.to_python(\"{\\\"invalid\\\":true}\n", "    def test_json_field_handles_invalid_json(self):\n        field = JSONField()\n        with self.assertRaises(ValidationError) as cm:\n            field.to_python('{invalid json')\n        self.assertEqual(cm.exception.args[0], 'Enter a valid JSON.')\n        ", "    def test_json_field_encode_decode(self):\n        field = JSONField(\n            encoder=DjangoJSONEncoder,\n            decoder=None\n        )\n        data = {\n            \"name\": \"John Doe\",\n            \"age\": 30,\n            \"address\": {\n                \"street\": \"123 Main St\",\n                \"city\": \"Anytown\",\n                \"zip\": \"12345\"\n            },\n            \"is_active\": True,\n        }\n        self.assertEqual(field.prepare_value(data), json.dumps(data, cls=DjangoJSONEncoder))\n        self.assertEqual(field.to_python(field.prepare_value(data)), data)\n\n\n\n\n"], "sample_843": ["    def test_kernel_eval(kernel):\n        K = kernel(X)\n        assert_array_almost_equal(K, K.T)\n        if kernel.is_stationary():\n            assert_array_almost_equal(\n                kernel(X, X), kernel(np.random.permutation(X), np.random.permutation(X)))\n\n        if kernel.__class__.__name__ == 'PairwiseKernel':\n            if kernel.metric in [\"precomputed\"]:\n                continue\n            K_diag = kernel.diag(X)\n            assert_array_almost_equal(K_diag, np.diagonal(K))\n\n        if hasattr(kernel, 'hyperparameters') and kernel.hyperparameters:\n            with np.errstate(divide='ignore'):\n                for param_name in kernel.hyperparameters:\n                    grad = kernel.gradient(X, Y)\n                    assert_array_equal(grad.shape, (len(X), len(Y), 1))\n\n        if hasattr(kernel, 'diag') and not hasattr(kernel, '__class__.__name__') \\\n                or kernel.__class__.__name__ not in [\"PairwiseKernel\", \"DotProduct\"] :\n            with np.errstate(divide='ignore'):\n                K_diag = kernel.diag(X)\n                assert_array_almost_equal(K_diag, np.diagonal(kernel(X)))\n", "    def test_kernel_repr(kernel):\n\n        repr_str = repr(kernel)\n        assert kernel.__class__.__name__ in repr_str\n        if hasattr(kernel, 'length_scale'):\n            assert f'length_scale={kernel.length_scale:.3g}' in repr_str\n        if hasattr(kernel, 'alpha'):\n            assert f'alpha={kernel.alpha:.3g}' in repr_str\n        if hasattr(kernel, 'gamma'):\n            assert f'gamma={kernel.gamma:.3g}' in repr_str\n        if hasattr(kernel, 'sigma_0'):\n            assert f'sigma_0={kernel.sigma_0:.3g}' in repr_str\n        if hasattr(kernel, 'periodicity'):\n            assert f'periodicity={kernel.periodicity:.3g}' in repr_str\n        if hasattr(kernel, 'metric'):\n            assert f'metric={kernel.metric}' in repr_str\n", "    def test_pairwise_kernel(kernel):\n        K = pairwise_kernels(X, Y, metric=kernel.metric, gamma=kernel.gamma,\n                             filter_params=True)\n        # If metric is \"precomputed\", we should directly get the provided K\n        if kernel.metric == \"precomputed\":\n            K_ref = kernel(X, Y)  # Using the kernel directly\n            assert_array_almost_equal(K, K_ref)\n        elif kernel.metric == \"euclidean\" or kernel.metric == \"cosine\":\n            # If the metric is \"euclidean\" or \"cosine\" we can directly\n            # compute the kernel using numpy\n            K_ref = kernel(X, Y)\n            assert_array_almost_equal(K, K_ref)\n        else:\n            # For other metrics, we can't directly compute the kernel\n            # but we can check that the result has the right shape\n            assert K.shape[0] == X.shape[0]\n            assert K.shape[1] == Y.shape[0]\n", "    def test_pairwise_kernel(kernel):\n        kernel_wrapper = kernel\n        K = kernel_wrapper(X, Y)\n        K_diag = kernel_wrapper.diag(X)\n        n_samples = X.shape[0]\n        n_samples_y = Y.shape[0]\n        assert K.shape == (n_samples, n_samples_y)\n        assert K_diag.shape == (n_samples,)\n\n        # Check that the diagonal is consistent with the kernel call\n        assert_almost_equal(K_diag, np.diag(K))\n\n\n\n", "    def test_kernel_dot_product_grad(kernel):\n        X = np.random.RandomState(0).randn(10, 2)\n        K = kernel(X)\n        K_grad = kernel(X, eval_gradient=True)\n        if kernel.__class__.__name__ == \"DotProduct\":\n            assert_array_almost_equal(K_grad[0][0], 2 * kernel.sigma_0 ** 2)\n        else:\n            assert_array_equal(K_grad, np.empty((X.shape[0], X.shape[0], 0)))\n"], "sample_441": ["    def test_save_with_m2m(self):\n        form = UserCreationForm(data={\"username\": \"test_m2m\"})\n        form.save()\n\n        new_user = User.objects.get(username=\"test_m2m\")\n        self.assertTrue(isinstance(new_user, CustomUserWithM2M))\n        self.assertEqual(new_user.groups.count(), 0)\n        self.assertEqual(new_user.user_permissions.count(), 0)\n\n        # Manually assign groups and permissions\n        group = Organization.objects.create(name=\"test_group\")\n        permission = Permission.objects.create(\n            content_type=ContentType.objects.get_for_model(CustomUserWithM2M),\n            codename=\"can_do_something\",\n            name=\"Can do something\",\n        )\n\n        new_user.groups.add(group)\n        new_user.user_permissions.add(permission)\n\n        # Validate that the User object now has the correct groups and permissions\n        form = UserChangeForm(\n            instance=new_user,\n            data={\"user_permissions\": [permission.id]},\n        )\n        form.save()\n        self.assertEqual(new_user.groups.count(), 1)\n        self.assertEqual(new_user.user_permissions.count(), 1)\n", "    def test_clean_password2_mismatch(self):\n        form = UserCreationForm(data={\"username\": \"test\", \"password1\": \"pass\", \"password2\": \"notpass\"})\n        with self.assertRaises(ValidationError) as context:\n            form.clean()\n        self.assertIn(\"password_mismatch\", str(context.exception))\n", "compilation error", "    def test_form_clean_with_existing_username(self):\n        form = UserCreationForm(data={\"username\": \"testclient\"})\n        self.assertFalse(form.is_valid())\n        self.assertIn(\n            \"duplicate\",\n            form.errors[\"username\"],\n        )\n", "    def test_save_with_existing_username(self):\n        form = UserCreationForm(data={\"username\": \"testclient\"})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors[\"username\"], [\n            _(\"A user with that username already exists.\")\n        ])\n\n\n\n"], "sample_1049": ["    def test_projection_line_2D_3D(self):\n        p = Plane((0, 0, 0), (1, 0, 0))\n        l = Line((1, 1), (2, 2))\n        projection = p.projection_line(l)\n        assert isinstance(projection, Line3D)\n        assert projection.p1 == Point3D(1, 1, 0)\n        assert projection.p2 == Point3D(2, 2, 0)\n", "    def test_projection_line_xy(self):\n        p = Plane((0, 0, 0), (0, 0, 1))\n        l = Line3D((0, 0, 0), (0, 1, 0))\n        result = p.projection_line(l)\n        assert isinstance(result, Point3D)\n        assert result.x == 0\n        assert result.y == 0\n        assert result.z == 0\n", "    def test_projection_line_intersection(self):\n        p = Plane(Point3D(1, 1, 1), normal_vector=(1, 1, 1))\n        l = Line3D(Point3D(0, 0, 0), Point3D(1, 1, 1))\n        proj = p.projection_line(l)\n        self.assertEqual(len(p.intersection(proj)), 1)\n", "    def test_parameter_value_on_plane(self):\n        p = Plane((0, 0, 0),normal_vector=(1, 1, 1))\n        on_plane = p.arbitrary_point(t).subs(t, 0)\n        param = p.parameter_value(on_plane, t)\n        assert param == {t: 0}\n        \n\n\n", "    def test_perpendicular_plane_degenerate_cases():\n        from sympy.geometry.point import Point\n        a = Plane(Point3D(0, 0, 0), normal_vector=(1, 0, 0))\n        b = a.perpendicular_plane()\n        assert b.normal_vector == (1, 0, 0)\n        assert b != a\n"], "sample_765": ["    def test_brier_score_loss_multilabel():\n        y_true = np.array([[0, 1], [1, 0], [0, 1], [1, 0]])\n        y_prob = np.array([[0.1, 0.9], [0.9, 0.1], [0.2, 0.8],\n                           [0.8, 0.2]])\n        score = brier_score_loss(y_true, y_prob, pos_label=1)\n        assert score == 0.0525\n\n\n\n", "    def test_classification_report_handles_zero_samples_classes(self, average):\n        y_true = np.array([0, 0, 1, 1, 2, 2, 3, 3])\n        y_pred = np.array([0, 0, 0, 1, 1, 2, 2, 3])\n        labels = np.unique(y_true)\n        target_names = ['a', 'b', 'c', 'd']\n\n        # Create a sparse representation of labels with zero samples for one class\n        sparse_y_true = np.zeros((len(y_true), len(labels)), dtype=bool)\n        sparse_y_true[np.arange(len(y_true)), np.searchsorted(labels, y_true)] = True\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", UndefinedMetricWarning)\n            report = classification_report(\n                sparse_y_true, y_pred, labels=labels, target_names=target_names,\n                average=average\n            )\n\n        if average == \"samples\":\n            # When averaging is 'samples', all metrics should be calculated\n            for class_name in target_names:\n                report = classification_report(\n                    sparse_y_true, y_pred, labels=labels, target_names=target_names,\n                    average=average\n                )\n                assert \"Precision\" in report\n                assert \"Recall\" in report\n                assert \"F1-score\" in report\n                assert \"Support\" in report\n        else:\n            # For other averaging methods, some metrics might not be\n            # calculated for classes with zero samples\n            assert \"Precision\" not in report\n\n", "    def test_hinge_loss_binary(sample_weight):\n        y_true = np.array([1, -1, 1, -1])\n        pred_decision = np.array([4, -2, 3, -1])\n        loss = hinge_loss(y_true, pred_decision, sample_weight=sample_weight)\n        assert_almost_equal(loss, 0.5)\n\n", "    def test_hinge_loss_multiclass():\n        X = np.array([[0], [1], [2], [3]])\n        Y = np.array([0, 1, 2, 3])\n        labels = np.array([0, 1, 2, 3])\n        est = svm.LinearSVC()\n        est.fit(X, Y)\n        pred_decision = est.decision_function([[-1], [2], [3]])\n        y_true = [0, 2, 3]\n        assert_almost_equal(hinge_loss(y_true, pred_decision, labels), 0.56)\n", "    def test_hamming_loss_multilabel_binary_label_indicator():\n        y_true = np.array([[0, 1], [1, 1], [0, 0], [1, 0]])\n        y_pred = np.array([[1, 0], [0, 1], [0, 0], [1, 0]])\n        loss = hamming_loss(y_true, y_pred)\n        assert_almost_equal(loss, 0.5)\n"], "sample_685": ["def test_log_cli_level_not_set(testdir: Testdir) -> None:\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n        logging.basicConfig(level=logging.DEBUG)\n\n            logging.info(\"This is an info message\")\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n    captured_log = result.stdout.str()\n    assert \"INFO: This is an info message\" in captured_log\n\n\n\n", "def test_log_level_for_setting(caplog):\n    caplog.set_level(logging.DEBUG)\n    logger.debug(\"This is a debug message\")\n    assert \"This is a debug message\" in caplog.text\n\n    caplog.set_level(logging.INFO)\n    logger.debug(\"This is a debug message\")\n    assert \"This is a debug message\" not in caplog.text\n    logger.info(\"This is an info message\")\n    assert \"This is an info message\" in caplog.text\n\n    caplog.set_level(logging.WARNING)\n    logger.debug(\"This is a debug message\")\n    assert \"This is a debug message\" not in caplog.text\n    logger.info(\"This is an info message\")\n    assert \"This is an info message\" not in caplog.text\n    logger.warning(\"This is a warning message\")\n    assert \"This is a warning message\" in caplog.text\n\n    caplog.set_level(logging.ERROR)\n    logger.debug(\"This is a debug message\")\n    assert \"This is a debug message\" not in caplog.text\n    logger.info(\"This is an info message\")\n    assert \"This is an info message\" not in caplog.text\n    logger.warning(\"This is a warning message\")\n    assert \"This is a warning message\" not in caplog.text\n    logger.error(\"This is an error message\")\n    assert \"This is an error message\" in caplog.text\n\n    caplog.set_level(logging.CRITICAL)\n    logger.debug(\"This is a debug message\")\n    assert \"This is a debug message\" not in caplog.text\n    logger.info(\"This is an info message\")\n    assert \"This is an info message\" not in caplog.text\n    logger.warning(\"This is a warning message\")\n    assert \"This is a warning message\" not in caplog.text\n    logger.error(\"This is an error message\")\n    assert \"This is an error message\" not in caplog.text\n    logger.critical(\"This is a critical message\")\n    assert \"This is a critical message\" in caplog.text\n\n\n", "    def test_log_file_level(self, testdir: Testdir) -> None:\n        \"\"\"Tests log file level.\"\"\"\n        # Create a test file with logging at a level that should be written to the file.\n        testdir.makepyfile(\n            \"\"\"\n            import logging\n\n            logging.basicConfig(level=logging.INFO)\n            logging.info(\"This message should be in the log file\")\n            \"\"\"\n        )\n\n        # Configure pytest to write logs to a specific file.\n        reprec = testdir.inline_run(\n            \"-v\", \"--log-file=test.log\", \"--log-file-level=DEBUG\"\n        )\n\n        # Read the log file to verify the message is there.\n        assert \"This message should be in the log file\" in open(\"test.log\").read()\n\n        # Read the log file again, there should be no messages at a higher level (INFO)\n        reprec = testdir.runpytest(\n            \"--log-file=test.log\", \"--log-file-level=INFO\"\n        )\n        reprec.assert_outcomes(passed=[1])\n        with open(\"test.log\", \"r\") as f:\n            lines = f.readlines()\n        assert lines[0].strip() == \"This message should be in the log file\"\n        assert len(lines) == 1\n\n\n\n", "def test_at_level(caplog):\n    with caplog.at_level(logging.WARNING):\n        logger.info(\"This is an info message\")\n        sublogger.warning(\"This is a warning message\")\n        logger.error(\"This is an error message\")\n    assert \"WARNING: This is a warning message\" in caplog.text\n    assert \"INFO: This is an info message\" not in caplog.text\n    assert \"ERROR: This is an error message\" not in caplog.text\n\n\n", "    def test_log_capture_handler(self, testdir: Testdir) -> None:\n        p = testdir.makepyfile(\n            \"\"\"\n            import logging\n\n            logging.basicConfig(level=logging.INFO)\n            logging.getLogger('baz').info('Hello from baz')\n\n                logging.getLogger(__name__).info('Hello from test_some_thing')\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines(\n            [\n                \"Hello from baz*\",\n                \"INFO    baz - Hello from baz\",\n                \"INFO    test_some_thing - Hello from test_some_thing\",\n            ]\n        )\n"], "sample_1102": ["    def test_cancel_with_integer_domain():\n        from sympy import I, ZZ\n\n        A = Symbol('A', commutative=False)\n        x, y = Symbol('x'), Symbol('y')\n        f = x**2 + y**2 - 1\n        g = 2*x*y - 1\n        Q, r = cancel(f, g, x, y, domain=ZZ)\n\n        f_poly = Poly(f, x, y)\n        g_poly = Poly(g, x, y)\n        Q_poly = Poly(Q, x, y)\n        r_poly = Poly(r, x, y)\n\n        assert _strict_eq(Q_poly, [Rational(1, 2), Rational(-1, 2)])\n        assert _strict_eq(r_poly, Poly(x**2 + y**2 - 1, x, y))\n", "    def test_cancel_rational(self):\n        from sympy import cancel\n        from sympy.abc import x, y\n\n        F = (2*x**2 - 2*y)/(x**2 + y**2)\n        G = (x**2 + y**2)/(x**2 - y**2)\n\n        Q, r = cancel(F, G)\n\n        self.assertEqual(Q, S.One)\n        self.assertEqual(r, (2*x**2 - 2*y)/(x**2 - y**2))\n\n        F = (x**3 + y**3)/(x**2 - y**2)\n        G = (x**2 + y**2)/(x**2 - y**2)\n\n        Q, r = cancel(F, G)\n        self.assertEqual(Q, (x**2 + y**2)/ (x**2 - y**2))\n        self.assertEqual(r, (x**3 + y**3 - (x**2 + y**2)*(x**2 - y**2))/(x**2 - y**2))\n\n        F = (x**2 + y**2)\n        G = (x**2 - y**2)\n\n        Q, r = cancel(F, G)\n\n        self.assertEqual(Q, (x**2 + y**2)/(x**2 - y**2))\n        self.assertEqual(r, 0)\n\n\n\n        F = (x**2 + y**2)\n        G = (x**2 - y**2)\n\n        Q, r = cancel(F, G, auto=False)\n\n        self.assertEqual(Q, (x**2 + y**2)/(x**2 - y**2))\n        self.assertEqual(r, 0)\n\n\n", "    def test_cancel_general():\n        from sympy import cancel, symbols, simplify\n        x, y = symbols('x y')\n\n        assert cancel(x * y / y) == x\n        assert cancel(x**2 + 2 * x * y / y) == x * (x + 2)\n        assert cancel(x**2 + 2 * x * y / y, domain=ZZ) == x * (x + 2)\n        assert cancel(x**2 + 2 * x * y / y, method='f5b') == x * (x + 2)\n        assert cancel(x * y / (x + y)) == y * x / (x + y)\n        assert cancel(x**2 * y / (x**2 + y)) == y * x**2 / (x**2 + y)\n        assert cancel((x**2 + y) * (x + y) / (x + y)) == x**2 + y\n        assert cancel((x**2 + y) * (x + y) / (x + y), domain=ZZ) == x**2 + y\n        assert cancel((x**2 + y) * (x + y) / (x + y), method='f5b') == x**2 + y\n\n        # with polynomials that will simplify\n        p1 = simplify((x**2 + x + 1) * (x + 1) / (x + 1))\n        p2 = simplify((x**2 + x + 1) * (x + 1) / (x + 1), domain=ZZ)\n        p3 = simplify((x**2 + x + 1) * (x + 1) / (x + 1), method='f5b')\n        assert cancel(p1, method='f5b') == x**2 + x + 1\n        assert cancel(p2, method='f5b') == x**2 + x + 1\n        assert cancel(p3, method='f5b') == x**2 + x + 1\n\n\n\n", "def test_cancel():\n    from sympy import cancel, S\n    from sympy.abc import x, y\n\n    assert cancel(2*x**2 - 2*y**2, x**3 - x) == (2*x, x**2 + y**2 + y)\n    assert cancel(x**4 - 1, x**2 - 1) == (x**2 + 1, x**2 - 1)\n    assert cancel(x*y**2 - y**3, x*y - 2*y) == (x, y*x + 2*y**2)\n    assert cancel(x*y**2 - y**3, x*y - y) == (x*y, y**2 - 2*y)\n    assert cancel((x**2 + y**2) * (x + y), (x + y)) == x**2 + y**2\n\n    assert cancel(2*x**2 + 2*x + 2, x) == (2*x + 2, x)\n\n\n    assert cancel(x**2 + y**2, S(1)) == (x**2 + y**2, S(1))\n    assert cancel(x, 0) == (x, 0)\n    assert cancel(x, x) == (1, 0)\n\n\n\n", "def test_poly_generators_needed():\n    from sympy import poly, exp, Symbol\n    x = Symbol('x')\n    expr = x**2 + exp(x)\n    poly(expr, x)\n\n"], "sample_945": ["    def test_parse_annotation_simple(self):\n        self.assertEqual(_parse_annotation('int'),  'int')\n        self.assertEqual(_parse_annotation('str'),  'str')\n        self.assertEqual(_parse_annotation('float'),'float')\n        self.assertEqual(_parse_annotation('list'), 'list')\n", "    def test_parse_annotation(monkeypatch):\n        monkeypatch.setattr(sys, 'version_info', (3, 9, 0))\n        monkeypatch.setattr(_parse_annotation, lambda x: x)\n\n        assert parse('MyFunction(arg1: int, arg2: str) -> str') == 'MyFunction(arg1: int, arg2: str) -> str'\n", "    def test_parse_sig_with_annotation(self):\n        test_cases = [\n            ('Foo(arg1: int, arg2: str) -> bool',\n             'Foo(arg1: int, arg2: str) -> bool',\n             'Foo', 'arg1: int, arg2: str', 'bool'),\n            ('Foo(arg1: int, arg2: str) -> bool',\n             'Foo(arg1: int, arg2: str) -> bool',\n             'Foo', 'arg1: int, arg2: str', 'bool'),\n            ('Foo(arg1: int) -> None',\n             'Foo(arg1: int) -> None',\n             'Foo', 'arg1: int', 'None'),\n            ('Foo(arg1: int, arg2: str) -> Foo',\n             'Foo(arg1: int, arg2: str) -> Foo',\n             'Foo',\n             'arg1: int, arg2: str',\n             'Foo'),\n            ('Foo(arg1: int, /, arg2: str) -> bool',\n             'Foo(arg1: int, /, arg2: str) -> bool',\n             'Foo',\n             'arg1: int, arg2: str',\n             'bool'),\n            ('Foo(arg1: int, /, *args: int, **kwargs: str) -> bool',\n             'Foo(arg1: int, /, *args: int, **kwargs: str) -> bool',\n             'Foo',\n             'arg1: int, *args: int, **kwargs: str',\n             'bool'),\n        ]\n\n        for sig, expected_text, name, args, ret in test_cases:\n            with self.subTest(sig=sig):\n                parsed_sig = parse(sig)\n                assert parsed_sig == expected_text\n                assert _parse_annotation(ret) == ret\n                assert name == sig.split('(')[0]\n                assert args == sig.split('(')[1].split(')')[0]\n", "    def test_parse_signature(sig, expected):\n        assert parse(sig) == expected\n", "    def test_annotation_parsing(app):\n        domain = PythonDomain(app)\n        for text, expected in [\n                ('int', ('int',)),\n                ('int -> str', ('int', 'str')),\n                ('int, list[str]', ('int', 'list[str]')),\n                ('Optional[int]', ('Optional', 'int')),\n                ('List[int]', ('List', 'int')),\n                ('Union[int, str]', ('Union', 'int', 'str')),\n                ('Dict[str, int]', ('Dict', 'str', 'int')),\n                ('Annotated[int, float]', ('Annotated', 'int', 'float')),\n                ('Callable[[int], str]', ('Callable', '([int])', 'str')),\n                ('typing.List[int]', ('List', 'int')),\n                ('typing.Optional[str]', ('Optional', 'str')),\n        ]:\n            ann = _parse_annotation(domain, text)\n            assert ann == expected\n\n\n\n"], "sample_314": ["    def test_clean_password2_password_mismatch(self):\n        form = UserCreationForm(data={'username': 'testclient', 'password1': 'password', 'password2': 'wrongpassword'})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['password2'], [_(\n            'The two password fields didn\u2019t match.'\n        )])\n", "    def test_password_mismatch(self):\n        form = UserCreationForm(\n            data={\"username\": self.u1.username, \"password1\": \"password\", \"password2\": \"wrongpassword\"}\n        )\n        self.assertFalse(form.is_valid())\n        self.assertIn('password2', form.errors)\n        self.assertEquals(form.errors['password2'][0], _('The two password fields didn\u2019t match.'))\n", "    def test_password_validation(self):\n        form = UserCreationForm(data={\n            'username': 'new_user',\n            'password1': 'weakpassword',\n            'password2': 'weakpassword',\n        })\n        self.assertFalse(form.is_valid())\n        self.assertIn('password2', form.errors)\n        self.assertEqual(form.errors['password2'][0], password_validation.password_validators_help_text_html())\n\n        form = UserCreationForm(data={\n            'username': 'new_user',\n            'password1': 'StrongPassword123',\n            'password2': 'StrongPassword123',\n        })\n        self.assertTrue(form.is_valid())\n", "    def test_password_validation(self):\n        form = UserCreationForm(data={'username': self.u1.username, 'password1': 'short', 'password2': 'short'})\n        self.assertFalse(form.is_valid())\n        self.assertIn('password1', form.errors)\n        self.assertEqual(form.errors['password1'], ['Password must be at least 8 characters long.'])\n\n        form = UserCreationForm(data={'username': self.u1.username, 'password1': 'valid', 'password2': 'valid'})\n        self.assertTrue(form.is_valid())\n", "    def test_clean_password2_mismatch(self):\n        form = UserCreationForm(data={'username': 'testclient', 'password1': 'password', 'password2': 'different'})\n        with self.assertRaises(ValidationError):\n            form.clean()\n        self.assertTrue(form.errors['password2'])\n        self.assertEqual(form.errors['password2'][0], _('The two password fields didn\u2019t match.'))\n"], "sample_1129": ["    def test_kronecker_delta_print(self):\n        self.assertEqual(pycode(KroneckerDelta(x, y)), 'sympy.KroneckerDelta(x, y)')\n        self.assertEqual(pycode(KroneckerDelta(x, x)), 'sympy.KroneckerDelta(x, x)')\n        self.assertEqual(pycode(KroneckerDelta(x, z)), 'sympy.KroneckerDelta(x, z)')\n", "    def test_sympy_printing_KroneckerDelta(self):\n        assert pycode(KroneckerDelta(x, y)) == 'sympy.KroneckerDelta(x, y)'\n        assert pycode(KroneckerDelta(x, y, evaluate=False)) == 'sympy.KroneckerDelta(x, y)'\n\n", "    def test_print_piecewise(self):\n        expr = Piecewise((x**2, x > 0), (1, x <= 0))\n        self.assertEqual(pycode(expr), 'numpy.where(x > 0, x**2, 1)')\n\n", "    def test_scipy_special_functions(self):\n        expr = loggamma(x)\n        self.assertEqual(pycode(expr, printer=SciPyPrinter), 'scipy.special.gammaln(x)')\n        expr = scipy.special.lpmv(0, 1, x)\n        self.assertEqual(pycode(expr, printer=SciPyPrinter), 'scipy.special.lpmv(0, 1, x)')\n        expr = cosm1(x)\n        self.assertEqual(pycode(expr, printer=SciPyPrinter), 'scipy.special.cosm1(x)')\n\n", "    def test_sympy_printing(self):\n        expr = Eq(x + y, z)\n        self.assertEqual(SymPyPrinter().doprint(expr), 'Eq(x + y, z)')\n\n        expr =  Piecewise((x, x > 0), (y, True))\n        self.assertEqual(SymPyPrinter().doprint(expr), 'Piecewise((x, x > 0), (y, True))')\n\n        expr = x**y\n        self.assertEqual(SymPyPrinter().doprint(expr), 'x**y')\n\n        expr = IndexedBase('a')([1, 2, 3])\n        self.assertEqual(SymPyPrinter().doprint(expr), 'a[1:4]')\n\n\n"], "sample_381": ["    def test_complex_m2m_through_changes(self):\n        before_states = self.make_project_state([\n            author_with_former_m2m,\n            publisher,\n            contract,\n        ])\n        after_states = self.make_project_state([\n            author_with_m2m,\n            publisher,\n            contract,\n            author_with_m2m_through,\n        ])\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        migration = changes['testapp'][0]\n        self.assertEqual(migration.name, '0001_initial')\n        self.assertOperationTypes(changes, 'testapp', 0, ['CreateModel', 'CreateModel', 'CreateModel', 'ADD_FIELD'])\n        self.assertOperationFieldAttributes(changes, 'testapp', 0, 3, db_column=\"author_id\", primary_key=True)\n\n\n\n", "    def test_complex_migration_with_nested_fields(self):\n        before_states = self.make_project_state([\n            author_name_nested_deconstructible_1,\n        ])\n        after_states = self.make_project_state([\n            author_name_nested_deconstructible_2,\n        ])\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertMigrationDependencies(changes, 'testapp', 0, [])\n        self.assertOperationTypes(changes, 'testapp', 0, ['RemoveField', 'AddField'])\n        self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, model_name='Author', field_name='name', db_column='name', null=False)\n        self.assertOperationFieldAttributes(changes, 'testapp', 0, 1, model_name='Author', field_name='name', db_column='name', null=False)\n\n\n\n    ", "    def test_migration_operations_with_empty_m2m_field(self):\n\n        before_states = self.make_project_state([\n            author_with_m2m.clone(),\n        ])\n        after_states = self.make_project_state([\n            author_with_m2m_blank.clone(),\n        ])\n\n        changes = self.get_changes(before_states, after_states)\n\n        self.assertNumberMigrations(\n            changes,\n            'testapp',\n            1,\n        )\n        self.assertMigrationDependencies(\n            changes,\n            'testapp',\n            0,\n            (),\n        )\n        self.assertOperationTypes(\n            changes, 'testapp', 0, ['RemoveField'],\n        )\n\n\n\n\n\n\n", "    def test_detect_changes_with_rename_table(self):\n        before_states = self.make_project_state([\n            author_renamed_with_db_table_options,\n            publisher_with_db_table_options,\n            author_renamed_with_new_db_table_options,\n        ])\n        after_states = self.make_project_state([\n            author_with_db_table_options,\n            publisher_with_new_db_table_options,\n            author_with_new_db_table_options,\n        ])\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, 'testapp', 3)\n        self.assertMigrationDependencies(changes, 'testapp', 0, [])\n        self.assertMigrationDependencies(changes, 'testapp', 1, [])\n        self.assertMigrationDependencies(changes, 'testapp', 2, [])\n\n\n\n", "    def test_circular_fk_migration(self):\n        before_states = self.make_project_state([\n            rabbit.clone(),\n        ])\n        after_states = self.make_project_state([\n            rabbit.clone(),\n        ])\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, 'eggs', 1)\n        self.assertMigrationDependencies(changes, 'eggs', 0, [])\n        self.assertOperationTypes(changes, 'eggs', 0, ['RemoveField'])\n"], "sample_936": ["    def test_stringify_callable(self):\n        callable_type = Callable[[T], T]\n        assert stringify(callable_type) == 'Callable[[T], T]'\n\n        callable_type = Callable[..., str]\n        assert stringify(callable_type) == 'Callable[..., str]'\n\n        callable_type = typing.Callable[[T], Union[int, str]]\n        assert stringify(callable_type) == 'Callable[[T], Union[int, str]]'\n", "    def test_stringify_callable(self):\n        if sys.version_info >= (3, 7):\n            typ = Callable[[T], T]\n        else:\n            typ = typing.Callable[[T], T]\n        assert stringify(typ) == 'Callable[[T], T]'\n\n\n", "    def test_stringify_Callable_with_Ellipsis(self):\n        annotation = Callable[..., str]\n        assert stringify(annotation) == 'Callable[..., str]'\n", "    def test_stringify_Callable(self):\n        if sys.version_info >= (3, 7):\n            from typing import Callable\n\n            test_cases = [\n                (Callable[[T], int], 'Callable[[T], int]'),\n                (Callable[[T], T], 'Callable[T, T]'),\n                (Callable[[T, int], str], 'Callable[[T, int], str]'),\n                (Callable[[], None], 'Callable[[], None]'),  # type: ignore\n            ]\n            for args, expected in test_cases:\n                assert stringify(args) == expected\n        else:\n            # Skip for py35/36\n            pass\n\n", "    def test_stringify_Annotated(self):\n        if sys.version_info >= (3, 9):\n            from typing import Annotated\n\n            annotation = Annotated[str, int]\n            assert stringify(annotation) == 'Annotated[str]'\n\n\n\n"], "sample_1076": ["    def test_print_matrix_solve(self):\n        A = MatrixSymbol('A', 2, 2)\n        b = MatrixSymbol('b', 2, 1)\n        expr = MatrixSolve(A, b)\n        \n        result = SciPyPrinter().doprint(expr)\n        expected = \"numpy.linalg.solve(A, b)\"\n        assert result == expected\n\n", "    def test_CodegenArrayTensorProduct(self):\n        a = MatrixSymbol('a', 2, 2)\n        b = MatrixSymbol('b', 2, 2)\n        expr = CodegenArrayTensorProduct(a, b)\n        expected = \"\"\"numpy.einsum('ij,ij->i', a, b)\"\"\"\n        self.assertEqual(pycode(expr, standard='python3'), expected)\n", "    def test_printing_scipy_special(self):\n        printer = SciPyPrinter()\n        self.assertEqual(printer.doprint(sign(x)), 'scipy.special.sign(x)')\n        self.assertEqual(printer.doprint(sqrt(x)), 'scipy.special.sqrt(x)')\n        self.assertEqual(printer.doprint(scipy.special.erf(x)), 'scipy.special.erf(x)')\n        self.assertEqual(printer.doprint(scipy.special.erfc(x)), 'scipy.special.erfc(x)')\n        self.assertEqual(printer.doprint(scipy.special.jv(x, y)), 'scipy.special.jv(x, y)')\n        self.assertEqual(printer.doprint(scipy.special.yv(x, y)), 'scipy.special.yv(x, y)')\n        self.assertEqual(printer.doprint(scipy.special.iv(x, y)), 'scipy.special.iv(x, y)')\n        self.assertEqual(printer.doprint(scipy.special.kv(x, y)), 'scipy.special.kv(x, y)')\n        self.assertEqual(printer.doprint(scipy.special.factorial(x)), 'scipy.special.factorial(x)')\n        self.assertEqual(printer.doprint(scipy.special.gamma(x)), 'scipy.special.gamma(x)')\n        self.assertEqual(printer.doprint(scipy.special.loggamma(x)), 'scipy.special.gammaln(x)')\n        self.assertEqual(printer.doprint(scipy.special.psi(x)), 'scipy.special.psi(x)')\n        self.assertEqual(printer.doprint(scipy.special.poch(x, y)), 'scipy.special.poch(x, y)')\n        self.assertEqual(printer.doprint(scipy.special.eval_jacobi(x, x, y, z)), 'scipy.special.eval_jacobi(x, x, y, z)')\n        self.assertEqual(printer.doprint(scipy.special.eval_gegenbauer(x, x, y, z)), 'scipy.special.eval_gegenbauer(x, x, y, z)')\n        self.assertEqual(printer.doprint(scipy.special", "    def test_code_gen_array_tensor_product(self):\n        from sympy.codegen.array_utils import CodegenArrayTensorProduct\n        base_a = MatrixSymbol('a', 2, 2)\n        base_b = MatrixSymbol('b', 2, 2)\n        arr_a = CodegenArrayTensorProduct(base_a, base_b)\n\n        code = pycode(arr_a, printer=NumPyPrinter())\n        expected = \"numpy.einsum('ij,kl->ijkl', a, b)\"\n        assert code == expected\n", "    def test_pycode_matrix_solve(self):\n        A = MatrixSymbol('A', 2, 2)\n        b = MatrixSymbol('b', 2, 1)\n        sol = MatrixSolve(A, b)\n\n        code = pycode(sol, printer=SymPyPrinter())\n        assert 'sympy.MatrixSolve' in code\n        assert 'sympy.symbols' in code\n        assert 'A' in code\n        assert 'b' in code\n"], "sample_119": ["    def test_add_extra(self):\n        q = Query(Ranking, None)\n        extra = {\n            'select': {'extra_field': 'RANKING.extra_value'},\n            'params': [()],\n            'where': 'extra_field = %s',\n            'tables': [],\n            'order_by': [],\n        }\n        q.add_extra(extra['select'], extra['params'], extra['where'], extra['params'], extra['tables'], extra['order_by'])\n        self.assertEqual(q.extra.get('select'), {'extra_field': 'RANKING.extra_value'})\n        self.assertEqual(q.extra.get('where'), 'extra_field = %s')\n        self.assertEqual(q.extra.get('params'), ())\n\n", "    def test_clear_select_fields(self):\n        q = Query(Ranking)\n        q.add_fields('name', 'value', 'order')\n        q.clear_select_fields()\n        self.assertEqual(len(q.select), 0)\n\n\n\n", "    def test_query_filters_with_related_objects(self):\n        q = Item.objects.filter(author__name__icontains='Jane')\n        self.assertEqual(q.query.where.children, [\n            JoinPromoter(OR, )\n        ])\n\n\n", "    def test_add_fields_with_m2m(self):\n        q = Query(Ranking)\n        q.add_fields(('name', 'author__name'), allow_m2m=True)\n        self.assertEqual(q.select, (\n            ('ranking_ranking__name',),\n            ('auth_author__name',),\n        ))\n", "    def test_split_exclude(self):\n        item = Item.objects.annotate(\n            lower_name=Lower('name')\n        ).filter(\n            name__startswith='D'\n        ).exclude(\n            lower_name__startswith='d'\n        )\n        # Should have no joins\n        self.assertEqual(len(item._query.alias_map), 1)"], "sample_627": ["    def test_concat_datasets_compat(\n        dim: str, compat: CompatOptions,", "    def test_compat_no_conflicts():\n        datasets = create_concat_datasets(num_datasets=3)\n        for i in range(len(datasets)):\n            datasets[i] = datasets[i].set_coords([\"x\", \"y\"])\n        result = concat(datasets, dim=\"day\", compat=\"no_conflicts\")\n        assert_identical(result.dims, [\"x\", \"y\", \"day\"])\n\n", "    def test_concat_datasets_with_different_coord_names(\n        self", "def test_concat_dataarray_compat(\n    compat: CompatOptions, expected_error", "    def test_concat_dataarray_non_identical_coords_and_data(\n        data_vars: list[str],\n        coords: list[str],\n        dim: str,\n        compat: str,"], "sample_969": ["    def test_restify_newtype(self):\n        if sys.version_info > (3, 10):\n            assert restify(MyInt) == '~typing.NewType[int]'\n        else:\n            assert restify(MyInt) == ':py:class:`~typing.NewType`'\n\n", "    def test_restify_newtype(self):\n        from sphinx.util import inspect  # lazy loading\n\n        if sys.version_info >= (3, 10):\n            new_type = NewType('MyNewType', str)\n            assert restify(new_type) == ':py:class:`~typing.NewType`'\n            new_type = NewType('MyNewType', int)\n            assert restify(new_type) == ':py:class:`~typing.NewType`'\n", "    def test_restify_callable(self):\n            pass\n        expected = ':py:func:`my_callable`'\n        assert restify(my_callable) == expected\n\n        expected = ':py:func:`~typing.Callable[[int, str], str]`'\n        assert restify(typing.Callable[[int, str], str]) == expected\n\n        expected = ':py:func:`~typing.Callable[[int, str], str][[int, str], str]`'\n        assert restify(typing.Callable[[int, str], str][[int, str], str]) == expected\n\n        with mock.patch('sphinx.util.typing.inspect.isgenericalias', return_value=True):\n            with mock.patch('sphinx.util.typing.inspect.getorigin', return_value=type(MyList)):\n                expected = ':py:func:`~typing.List[~sphinx.util.typing.test_util_typing.MyList]`'\n                assert restify(typing.List[MyList]) == expected\n", "    def test_restify_NewType(self):\n        assert restify(MyInt) == ':py:class:`MyInt`'\n\n\n\n", "    def test_restify_newtype_with_module(self):\n        from sphinx.util import inspect  # lazy loading\n\n        if sys.version_info >= (3, 10):\n            # newtypes have correct module info since Python 3.10+\n            newtype_class = inspect.new_type(\"MyNewType\", int)\n            text = restify(newtype_class)\n            assert text == ':py:class:`my_project.MyNewType`'\n\n\n\n"], "sample_978": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_313": ["    def test_watch_for_template_changes(self, mock_reset_loaders):\n        autoreload.start()\n\n        # Create a template file in EXTRA_TEMPLATES_DIR\n        (EXTRA_TEMPLATES_DIR / \"test_template.html\").write(\"<h1>Hello, world!</h1>\")\n\n        # Trigger file_changed signal\n        sender = mock.MagicMock()\n        file_changed.send(sender, file_path=EXTRA_TEMPLATES_DIR / \"test_template.html\")\n\n        # Assert that reset_loaders was called\n        mock_reset_loaders.assert_called_once()\n\n\n", "    def test_template_file_changed(self, mock_reset_loaders):\n        with mock.patch(\n            'django.utils.autoreload.file_changed', auto_spec=True\n        ) as mock_file_changed:\n            file_path = EXTRA_TEMPLATES_DIR / \"test_template.html\"\n            file_path.touch()\n            mock_file_changed.return_value = True\n            watch_for_template_changes(None, **{})\n            mock_file_changed.assert_called_once()\n            mock_reset_loaders.assert_called_once()\n", "        def test_watch_for_template_changes(self, mock_reset_loaders):\n            autoreload.start()\n            (ROOT / \"templates_extra\" / \"test.html\").write_text(\"<h1>Hello, world!</h1>\")\n            mock_reset_loaders.assert_called_once()\n", "    def test_template_changed_signals_reset_loaders(self, mock_reset_loaders):\n        # Trigger a file change event\n        autoreload.file_changed(file_path=EXTRA_TEMPLATES_DIR / \"test_template.html\")\n\n        # Assert that reset_loaders was called\n        mock_reset_loaders.assert_called_once()\n", "    def test_watch_for_template_changes(self, mock_reset_loaders):\n        template_path = EXTRA_TEMPLATES_DIR / \"test.html\"\n        template_path.touch()\n        autoreload.start()\n\n        with mock.patch('django.utils.autoreload.is_django_path', return_value=False):\n            autoreload.reload_templates()\n\n        mock_reset_loaders.assert_called_once()\n"], "sample_8": ["    def test_masked_array_from_array(self):\n        for data, mask in [(self.a, self.mask_a),\n                           (self.b, self.mask_b),\n                           (self.c, self.mask_c),\n                           (self.sa, self.mask_sa),\n                           ]:\n            masked = Masked(data, mask)\n            assert_masked_equal(masked, Masked(data, mask))\n\n            # Check that __len__ and __getitem__ work as expected\n            assert len(masked) == len(data)\n            assert masked[0] == data[0]\n            assert masked[1, 1] == data[1, 1]\n", "    def test_masked_array_from_data_and_mask(self):\n        for data_cls in [np.ndarray, MaskedNDArray]:\n            for data, mask, data_new in zip(\n                    [self.a, self.b, self.c, self.sa, self.sb, self.sc],\n                    [self.mask_a, self.mask_b, self.mask_c, self.mask_sa, self.mask_sb, self.mask_sc],\n                    [data_cls(data, mask=mask) for data, mask in zip(\n                        [self.a, self.b, self.c, self.sa, self.sb, self.sc],\n                        [self.mask_a, self.mask_b, self.mask_c, self.mask_sa, self.mask_sb, self.mask_sc])]):\n                assert_masked_equal(data_new, data_cls(data, mask))\n\n", "compilation error", "    def test_masked_array_from_numpy_array(self):\n        for item in VARIOUS_ITEMS:\n            a_masked = Masked(self.a[item],\n                              np.array([False] * len(self.a[item])))\n            assert_masked_equal(a_masked, np.ma.masked_array(self.a[item],\n                                                           mask=np.array([False] * len(self.a[item]))))\n", "    def test_masked_array_from_array(self):\n        for data, mask in zip([self.a, self.b, self.c],\n                             [self.mask_a, self.mask_b, self.mask_c]):\n            result = Masked(data, mask)\n            assert_masked_equal(result, Masked(data, mask))\n"], "sample_695": ["compilation error", "compilation error", "compilation error", "def test_iterparentnodeids(\n    pytest: Pytester", "compilation error"], "sample_907": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_682": ["    def test_xfail_skipping_with_strict(self, testdir: Testdir):\n        p = testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.xfail(strict=True, reason=\"should fail\")\n                assert 1 == 2\n\n            @pytest.mark.xfail(raises=ZeroDivisionError, reason=\"should raise\")\n                1 / 0\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines(\n            [\n                *[\"test_should_fail xfail\"],\n                *[\"  pytest.xfail: should fail\"],\n                \"test_should_raise xfail\",\n                \"  pytest.xfail: should raise\",  \n            ]\n        )\n\n", "    def test_skip_if_condition_string_syntax_error(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n                if 1 + 2 == 3 and x == y:\n                    pass\n            \"\"\"\n        )\n\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines(\n            [\n                \"*error evaluating skipif condition*\",\n                \"*SyntaxError: invalid syntax*\",\n            ]\n        )\n", "    def test_xfailed_with_raises(self, testdir):\n        p = testdir.makepyfile(\n            \"\"\"\n                1/0\n            \"\"\"\n        )\n        result = testdir.runpytest(p)\n        result.stdout.fnmatch_lines(\n            [\n                \"test_me.py:1: AssertionError*\",\n            ]\n        )\n", "    def test_skipif_condition_eval(self, testdir):\n        p = testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n                pytest.skipif(1 == 1, reason=\"Condition should be True\")\n            \"\"\"\n        )\n        result = testdir.runpytest(\"-v\", *p.fspath)\n        result.stdout.fnmatch_lines([ \"*skip*Condition should be True*\"])\n\n", "    def test_xfail_dynamic(self, testdir):\n        p = testdir.makepyfile(\n            \"\"\"\n                pass\n            \"\"\"\n        )\n        result = testdir.runpytest(p)\n        assert result.ret == 0\n        result = testdir.runpytest(p, [\"--runxfail\"])\n        assert result.ret == 0\n        result = testdir.runpytest(p, [\n            \"--runxfail\",\n            \"-m\",\n            \"xfail(condition=True, strict=True, reason='explicitly xfailing')\"\n        ])\n        assert result.ret == 0\n        result = testdir.runpytest(p, [\n            \"--runxfail\",\n            \"-m\",\n            \"xfail(condition=False, strict=True, reason='explicitly xfailing')\"\n        ])\n        assert result.ret == 0\n"], "sample_1014": ["    def test_reshape_invalid_size(self):\n        a = ImmutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n        with raises(ValueError, match=\"Invalid reshape parameters\"):\n            a.reshape(3, 1) \n", "    def test_reshape_error(self):\n        a = ImmutableDenseNDimArray([1, 2, 3, 4, 5, 6], (2, 3))\n        with raises(ValueError):\n            a.reshape(4, 1)\n", "    def test_reshape_error(self):\n        a = ImmutableDenseNDimArray([1, 2, 3, 4, 5, 6], (2, 3))\n        with raises(ValueError):\n            a.reshape(4, 3)\n\n", "    def test_reshape_error(self):\n        a = ImmutableDenseNDimArray([1, 2, 3, 4, 5, 6], (2, 3))\n        with raises(ValueError):\n            a.reshape(4, 2)\n\n\n\n", "    def test_reshape_non_compatible_size(self):\n        a = ImmutableDenseNDimArray([1, 2, 3, 4, 5, 6], (2, 3))\n        with raises(ValueError, match=\"Invalid reshape parameters\"):\n            a.reshape(4, 1)\n"], "sample_504": ["def test_colorbar_extension_length():\n    \"\"\"\n    Test the length of colorbar extensions for different settings.\n    \"\"\"\n    with rc_context(axes=dict(linewidth=0.5, facecolor='lightgray')):\n        fig1 = _colorbar_extension_length('uniform')\n        fig2 = _colorbar_extension_length('proportional')\n    check_figures_equal(fig1, fig2)\n", "def test_colorbar_extension_length(tmpdir):\n    \"\"\"\n    Test that colorbar extensions have the expected length for different\n    settings of `extendfrac`.\n    \"\"\"\n    fig_uniform = _colorbar_extension_length('uniform')\n    fig_proportional = _colorbar_extension_length('proportional')\n    plt.close(fig_uniform)\n    plt.close(fig_proportional)\n\n\n\n", "    def test_colorbar_extension_length(self):\n        \"\"\"\n        Test that colorbar extensions have the correct length\n        for different spacing and extend settings.\n        \"\"\"\n        with rc_context({'axes.labelsize': 8, 'xtick.labelsize': 8,\n                         'ytick.labelsize': 8}):\n            fig1 = _colorbar_extension_length('uniform')\n            fig2 = _colorbar_extension_length('proportional')\n        return fig1, fig2\n\n\n\n", "def test_colorbar_extension_length(tmpdir):\n    \"\"\"\n    Test for colorbar extensions with uniform and proportional spacing.\n\n    This tests different combinations of extend options and extendfrac to\n    ensure colorbar extensions are displayed correctly for both types of\n    spacing.\n    \"\"\"\n    _colorbar_extension_length('uniform')\n    _colorbar_extension_length('proportional')\n\n", "def test_colorbar_extension_length(self):\n    \"\"\"\n    Test the length of colorbar extensions for different spacing and\n    extension types.\n    \"\"\"\n    with rc_context({'axes.linewidth': 0.5}):\n        fig0 = _colorbar_extension_length('uniform')\n        fig1 = _colorbar_extension_length('proportional')\n    check_figures_equal(fig0, 'colorbar_extension_length_uniform.png',\n                         atol=1e-2)\n    check_figures_equal(fig1, 'colorbar_extension_length_proportional.png',\n                         atol=1e-2)\n\n\n\n"], "sample_759": ["    def test_handle_unknown_ignore(self):\n        encoder = OneHotEncoder(handle_unknown='ignore')\n        X = np.array([[1, 0, 2], [3, 4, 5,], [1, 6]])\n        encoder.fit(X)\n        X_transform = encoder.transform(X)\n\n        assert_equal(X_transform.shape, (3, 5))\n        assert_array_almost_equal(X_transform.toarray(),\n                                  [[1., 0., 0., 0., 0.],\n                                   [0., 0., 1., 0., 0.],\n                                   [1., 0., 0., 0., 1.]])\n", "    def test_onehotencoder_n_values_deprecated(self):\n        enc = OneHotEncoder(n_values=[2, 3])\n        with pytest.warns(DeprecationWarning):\n            enc.fit(\n                [[0], [1], [0], [2], [1]]\n            )\n        assert_array_equal(enc.categories_, [[0, 1], [0, 1, 2]])\n", "    def test_onehot_empty_data(self):\n        enc = OneHotEncoder()\n        X = []\n        with assert_raises(ValueError):\n            enc.fit(X)\n", "    def test_handle_unknown_with_ignore(self):\n        enc = OneHotEncoder(handle_unknown='ignore')\n        X = [['Male'], ['Female'], ['Unknown']]\n        enc.fit(X)\n        transformed = enc.transform([[ 'Unknown']])\n        assert_allclose(transformed.toarray(), [[0, 0, 0]])\n", "    def test_legacy_handle_unknown_string(self):\n        enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n\n        X = [['Male', 1], ['Female', 3], ['Unknown', 2]]\n        enc.fit(X)\n\n        X_trans = enc.transform([[None, 1]])\n        assert_array_equal(X_trans, [[0., 0., 0., 0.]])  # all zeros\n\n        # Ensure categories are correctly handled during fit.\n        assert_array_equal(enc.categories_, [['Male', 'Female'], [1, 2, 3]])\n"], "sample_581": ["    def test_template_globals(self):\n        app = flask.Flask(__name__)\n        bp = flask.Blueprint(\"test_bp\", __name__)\n\n        @bp.app_template_global\n            return value + \" from blueprint\"\n\n        test_value = \"test\"\n\n        with app.test_request_context():\n            with app.app_context():\n                with bp.app_context():\n                    with app.test_request_context():\n                        with bp.test_request_context():\n                            with app.test_request_context():\n                                assert app.jinja_env.globals[\"test_global\"](test_value) == test_value + \" from blueprint\"\n\n        app.register_blueprint(bp)\n", "    def test_app_errorhandler_registration(client, app):\n        try:\n            app.run(debug=True)\n        except Exception as e:\n            print(e)\n            assert False\n        \n        @app.errorhandler(404)\n            return \"Nope\", 404\n\n        with client:\n            resp = client.get('/404')\n            assert resp.status_code == 404\n            assert resp.data == b\"Nope\"  \n", "    def test_parse_cache_control_header(\n        self, provided_cache_control, expected", "    def test_url_defaults(self):\n        app = flask.Flask(__name__)\n        bp = flask.Blueprint('test', __name__)\n\n        @bp.route('/')\n            return 'index'\n\n        bp.url_defaults = {'test': 'default'}\n        app.register_blueprint(bp)\n\n        with app.test_request_context():\n            with app.app_context():\n                with app.test_client() as client:\n                    response = client.get('/')\n                    assert response.data == b'index'\n                    assert response.headers['Set-Cookie'] == 'test=default; Path=/; HttpOnly'\n", "    def test_blueprint_errorhandler_inheritance(\n        self, app: flask.Flask, monkeypatch: pytest.MonkeyPatch"], "sample_596": ["    def test_concat_same_dim_coords(\n        dim, values, compat, expected_values,", "    def test_concat_dataset_attrs(\n        compat, data_vars, coords, expected_attrs", "    def test_concat_identical_compat(dim, data_vars, coords, compat):\n        ds1 = create_test_data()\n        ds2 = deepcopy(ds1)\n        ds2.coords[\"time\"] = pd.date_range(\n            \"2023-01-01\", periods=2, freq=\"D\"\n        )\n        result = concat(\n            [ds1, ds2],\n            dim=dim,\n            data_vars=data_vars,\n            coords=coords,\n            compat=compat,\n            fill_value=dtypes.NA,\n        )\n        assert_equal(result.dims, [\"lat\", \"lon\", \"time\"])\n        assert_identical(result.coords[\"time\"], pd.date_range(\"2023-01-01\", periods=4, freq=\"D\"))\n\n\n\n", "    def test_concat_coords_from_different_datasets_same_shape_different_values(\n        self", "    def test_concat_different_coords_compat_override():\n        ds1 = create_test_data(\n            {\"x\": [1, 2, 3], \"y\": [4, 5, 6]},\n            {\"z\": [7, 8, 9]},\n            {\"z\": [7, 8, 9]},\n        )\n\n        ds2 = create_test_data(\n            {\"x\": [10, 11, 12], \"y\": [13, 14, 15]},\n            {\"z\": [10, 11, 12]},\n            {\"z\": [10, 11, 12]},\n        )\n\n        result = concat(\n            [ds1, ds2], dim=\"x\", coords=[\"z\"], compat=\"override\",\n        )\n        assert_identical(result[\"x\"].values, np.concatenate((ds1[\"x\"], ds2[\"x\"])))\n        assert_identical(result[\"y\"].values, np.concatenate((ds1[\"y\"], ds2[\"y\"])))\n        assert_identical(result[\"z\"].values, ds1[\"z\"])\n\n\n\n"], "sample_454": ["    def test_exclusion_constraint_with_condition(self):\n        with atomic():\n            Product.objects.create(name=\"Test Product 1\")\n            Product.objects.create(name=\"Test Product 2\")\n            with self.assertRaises(ValidationError):\n                UniqueConstraintConditionProduct.objects.create(name=\"Test Product 1\")\n            with self.assertRaises(ValidationError):\n                UniqueConstraintConditionProduct.objects.create(name=\"Test Product 2\")\n", "    def test_exclusion_constraint_with_condition(self):\n        with atomic():\n            Product.objects.create(name='Product A', price=10)\n            with self.assertRaises(ValidationError):\n                Product.objects.create(name='Product A', price=10)  \n\n", "    def test_exclusion_constraint_create_and_validate(self):\n        with atomic():\n            Product.objects.create(name=\"Product 1\", price=10)\n        with self.assertRaises(ValidationError):\n            Product.objects.create(name=\"Product 1\", price=10)  \n        \n        # Test exclusion constraint with condition\n        with ignore_warnings(category=RemovedInDjango60Warning):\n            with atomic():\n                UniqueConstraintConditionProduct.objects.create(\n                    name=\"Product 1\", price=10, condition='price <= 5' \n                )\n        with self.assertRaises(ValidationError):\n            UniqueConstraintConditionProduct.objects.create(\n                name=\"Product 1\", price=12, condition='price <= 5'\n            )\n        \n\n", "    def test_exclusion_constraint_with_condition(self):\n        with atomic():\n            product = UniqueConstraintConditionProduct.objects.create(name=\"Test Product\", description=\"Test Description\")\n            with self.assertRaises(ValidationError):\n                UniqueConstraintConditionProduct.objects.create(\n                    name=\"Test Product\", description=\"Test Description\"\n                )\n", "    def test_exclusion_constraint_with_condition(self):\n        with atomic():\n            UniqueConstraintProduct.objects.create(name=\"Product A\", description=\"Description A\")\n            UniqueConstraintProduct.objects.create(name=\"Product B\", description=\"Description B\")\n\n        with self.assertRaises(ValidationError):\n            UniqueConstraintProduct.objects.create(name=\"Product A\", description=\"Description A\")\n"], "sample_113": ["    def test_parse_rst(self):\n        text = \"\"\"", "    def test_simplify_regex(self):\n        self.assertEqual(simplify_regex('^(\\w+)$'), r'^\\w+$')\n        self.assertEqual(simplify_regex('(?P<name>\\w+)'), r'<name>')\n        self.assertEqual(simplify_regex('(?P<name>\\w+)/'), r'<name>/')\n        self.assertEqual(simplify_regex('(.*)'), r'.*')\n        self.assertEqual(simplify_regex('(.*?)(?P<name>.*)'), r'.* <name>')", "    def test_parse_rst(self):\n        text = \"\"\"", "    def test_simplify_regex(self):\n        # Test that simplify_regex works correctly with simple regex patterns.\n        self.assertEqual(simplify_regex(r'^Hello$'), r'Hello')\n        self.assertEqual(simplify_regex(r'^\\w+$'), r'\\w+')\n        self.assertEqual(simplify_regex(r'[\\w\\s]+'), r'[\\w\\s]+')\n        # Test that simplify_regex handles named groups correctly.\n        self.assertEqual(simplify_regex(r'^\\(?P<name>\\w+\\)?$'), r'^<name>$')\n        self.assertEqual(simplify_regex(r'^\\(?P<name>\\w+\\)?\\s+(.+)$'), r'^<name>\\s+(.+)$')\n\n        # Test that simplify_regex handles unnamed groups correctly.\n        self.assertEqual(simplify_regex(r'^(\\w+)$'), r'<var>')\n\n        # Test that simplify_regex handles nested groups correctly.\n        self.assertEqual(simplify_regex(r'^(\\w+)\\s+((?:\\w+\\s+){2})$'), r'<var>\\s+<var>\\s+<var>')\n        self.assertEqual(simplify_regex(r'^(?P<name>\\w+)\\s+(?P<age>\\d+)$'), r'^<name>\\s+<age>$')\n        \n        # Test edge cases:\n        self.assertEqual(simplify_regex(r'^$'), r'')\n        self.assertEqual(simplify_regex(r'^(?P<name>\\w+)(?P<age>\\d+)\\^'), r'^<name><age>$')\n\n\n", "    def test_parse_docstring(self):\n        docstring = \"\"\"\n        ## Title\n\n        This is a docstring with some metadata.\n\n        Key: Value\n        \"\"\"\n        title, body, metadata = utils.parse_docstring(docstring)\n        self.assertEqual(title, \"Title\")\n        self.assertEqual(body, \"This is a docstring with some metadata.\")\n        self.assertEqual(metadata, {'Key': 'Value'})\n\n        docstring = \"\"\"\n        ## Title\n\n\n        Metadata\n        This is a docstring with some metadata.\n\n        Key: Value\n        \"\"\"\n        title, body, metadata = utils.parse_docstring(docstring)\n        self.assertEqual(title, \"Title\")\n        self.assertEqual(body, \"Metadata\\nThis is a docstring with some metadata.\")\n        self.assertEqual(metadata, {'Key': 'Value'})\n"], "sample_443": ["    def test_file_based_cache_touch(self):\n        cache_dir = tempfile.mkdtemp()\n        cache = FileBasedCache(dir=cache_dir, params={})\n        key = \"mykey\"\n        value = \"myvalue\"\n\n        # Set the cache value\n        cache.set(key, value, timeout=1)\n\n        # Touch the cache\n        self.assertTrue(cache.touch(key))\n\n        # Check that the value is still valid\n        self.assertEqual(cache.get(key), value)\n\n        # Touch the cache again, with an expired timeout\n        cache.touch(key, timeout=0)\n        self.assertFalse(cache.touch(key))\n\n        # Check that the value has been deleted\n        self.assertIsNone(cache.get(key))\n\n        # Clean up\n        shutil.rmtree(cache_dir)\n\n\n\n", "    def test_filecache_cull(self):\n        filecache_dir = tempfile.mkdtemp()\n        try:\n            cache = FileBasedCache(dir=filecache_dir, params={})\n            # Set some data\n            cache.set(\"key1\", \"value1\", timeout=60)\n            cache.set(\"key2\", \"value2\", timeout=60)\n            cache.set(\"key3\", \"value3\", timeout=60)\n\n            # Verify cull happening\n            cache._max_entries = 2  # Set a limit\n            cache._cull_frequency = 1  \n            cache._cull()\n            self.assertEqual(len(cache._list_cache_files()), 2)\n\n        finally:\n            shutil.rmtree(filecache_dir)     \n", "    def test_file_cache_backend_add(self):\n        with tempfile.TemporaryDirectory() as tempdir:\n            cache_backend = FileBasedCache(\"temp\", {})\n            key = 'test_key'\n            value = {'a': 1, 'b': 2}\n            cache_backend.add(key, value, timeout=1)\n\n            self.assertTrue(cache_backend.has_key(key))\n            \n            with open(cache_backend._key_to_file(key), 'rb') as f:\n                loaded_value = pickle.loads(zlib.decompress(f.read()))\n            self.assertEqual(loaded_value, value)\n", "    def test_file_based_cache_touch(self):\n        cache_dir = tempfile.mkdtemp()\n        os.environ[\"DJANGO_CACHE_BACKEND\"] = \"file:%s\" % cache_dir\n        cache_key = \"test_key\"\n        value = \"test_value\"\n\n        with caches[\"default\"].configured(\n            BACKEND=\"django.core.cache.backends.filebased.FileBasedCache\",\n            LOCATION=cache_dir,\n        ):\n            caches[\"default\"].set(cache_key, value, timeout=10)\n            self.assertTrue(caches[\"default\"].has_key(cache_key))\n\n            # Touch the cache entry\n            self.assertTrue(caches[\"default\"].touch(cache_key))\n            with open(caches[\"default\"]._key_to_file(cache_key), \"rb\") as f:\n                exp = pickle.load(f)\n            self.assertGreater(exp, time.time())\n\n            # Touch the cache entry (expired)\n            time.sleep(15)\n            self.assertFalse(caches[\"default\"].touch(cache_key))\n", "    def test_file_based_cache_cull(self):\n        cache = FileBasedCache(\n            self._get_temp_dir(), {\"max_entries\": 3, \"cull_frequency\": 2}\n        )\n        random.seed(42)  # make the test reproducible\n        for i in range(5):\n            cache.set(f\"key-{i}\", f\"value-{i}\")\n\n        # Ensure cache size is below max_entries\n        self.assertEqual(len(cache._list_cache_files()), 5)\n\n        # Simulate cache usage until cull occurs\n        for i in range(10):\n            cache.get(f\"key-{i % 5}\")\n"], "sample_620": ["    def test_concat_different_dim_names(compat):\n        ds1 = create_test_data(\n            sizes=(2, 3),\n            coords={\"x\": [\"a\", \"b\"], \"y\": [10, 20, 30]},\n            data_vars={\"var1\": [1, 2, 3, 4, 5, 6]},\n        )\n        ds2 = create_test_data(\n            sizes=(3, 4),\n            coords={\"x\": [\"c\", \"d\"], \"y\": [40, 50, 60, 70]},\n            data_vars={\"var1\": [7, 8, 9, 10, 11, 12, 13, 14]},\n        )\n\n        with pytest.raises(ValueError):\n            concat(\n                [ds1, ds2], dim=\"new_dim\", compat=compat, join=\"inner\"\n            )\n\n", "    def test_concat_with_no_conflicts(self, compat: str):\n        ds1 = create_test_data(\n            {\"x\": np.arange(2), \"y\": np.arange(3)},\n            data_vars=[(\"z\", np.arange(6).reshape(2, 3))],\n        )\n        ds2 = deepcopy(ds1)\n        ds2.variables[\"z\"][:, 1] = np.nan  # Make one value different\n\n        result = concat(\n            [ds1, ds2], dim=\"y\", compat=compat\n        )\n\n        if compat == \"no_conflicts\":\n            assert_array_equal(result.variables[\"z\"],\n                                np.arange(6).reshape(2, 3))\n        else:\n            assert_array_equal(result.variables[\"z\"],\n                                np.nan_to_num(ds2.variables[\"z\"]))\n\n\n", "    def test_concat_invalid_compat(\n        compat: CompatOptions, expected_raises: type[Exception] | None,", "    def test_concat_multiple_datasets_with_different_sizes(\n        data_vars, coords, compat, join,", "def test_concat_scalar_coordinate(\n    create_test_data: create_test_data"], "sample_1194": ["    def test_indexing_with_matrices():\n        from sympy.matrices import MatrixSymbol\n        mat = MatrixSymbol('A', 2, 2)\n        A_ij = mat[0, 1]\n        code = julia_code(A_ij)\n        assert code == 'A[1, 2]'\n\n", "    def test_matrix_element():\n        A = MatrixSymbol('A', 2, 2)\n        e = A[1, 0]\n        print(julia_code(e))\n", "    def test_hadamard_power():\n        raises(NotImplementedError, lambda: julia_code(Mul(x, y)**z))\n        expr = HadamardProduct(x**Rational(1, 2), y**Rational(1, 2))\n        result = julia_code(expr)\n        expected = 'x .^ (1 // 2) .* y .^ (1 // 2)'\n        assert result == expected\n", "    def test_sparse_matrix(self):\n        A = SparseMatrix([[1, 2], [3, 4]])\n        code = julia_code(A, assign_to='A')\n        expected = \"A = sparse(1, 1, 1, 2, 2)\"\n        assert code == expected  \n\n", "    def test_index_order():\n        from sympy import IndexedBase, Idx\n        ib = IndexedBase('X', shape=(3,))\n        i = Idx('i', 3)\n\n        expr = ib[i]  \n        result = julia_code(expr)\n        assert result == 'X[i]'\n\n        expr = ib[i+1] \n        result = julia_code(expr)\n        assert result == 'X[i + 1]'\n\n        expr = ib[i]+ib[i+1]\n        result = julia_code(expr)\n        assert result == 'X[i] + X[i + 1]'\n\n\n"], "sample_927": ["    def test_templates(input, idDict, output):", "def check_namespace(name, input, idDict, output):\n    if name == 'namespace':\n        assert input.startswith(name + ' ')  # must not be a function call\n    ast = parse(name, input)\n    ast.scoped = True\n    rootSymbol = Symbol(None, None, None, None, None, None)\n    symbol = rootSymbol.add_declaration(ast, docname=\"TestDoc\")\n    parentNode = addnodes.desc()\n    signode = addnodes.desc_signature(input, '')\n    parentNode += signode\n    ast.describe_signature(signode, 'lastIsName', symbol, options={})\n    resAsText = parentNode.astext()\n    if resAsText != output:\n        print(\"\")\n        print(\"Input:    \", input)\n        print(\"astext(): \", resAsText)\n        print(\"Expected: \", output)\n        raise DefinitionError(\"\")\n", "    def test_alias_parsing(self):\n        input = 'alias MyAlias = MyNamespace::MyClass;'\n        idDict = {1: 'MyAlias:1'}\n        output = 'alias MyAlias = MyNamespace::MyClass;'\n        check('alias', input, idDict, output)\n\n\n\n", "    def check_template_param(name, input, idDict, output, key=None, asTextOutput=None):\n        if key is None:\n            key = name\n        key += ' '\n        inputActual = input.format(key='')\n        outputAst = output.format(key='')\n        outputAsText = output.format(key=key)\n\n        ast = parse(name, inputActual)\n        res = str(ast)\n        if res != outputAst:\n            print(\"\")\n            print(\"Input:    \", input)\n            print(\"Result:   \", res)\n            print(\"Expected: \", outputAst)\n            raise DefinitionError(\"\")\n        rootSymbol = Symbol(None, None, None, None, None, None)\n        symbol = rootSymbol.add_declaration(ast, docname=\"TestDoc\")\n        parentNode = addnodes.desc()\n        signode = addnodes.desc_signature(input, '')\n        parentNode += signode\n        ast.describe_signature(signode, 'lastIsName', symbol, options={})\n        resAsText = parentNode.astext()\n        if resAsText != outputAsText:\n            print(\"\")\n            print(\"Input:    \", input)\n            print(\"astext(): \", resAsText)\n            print(\"Expected: \", outputAsText)\n            raise DefinitionError(\"\")\n", "compilation error"], "sample_816": ["    def test_stop_words_removal_with_vocabulary(self):\n        vectorizer = TfidfVectorizer(stop_words='english',\n                                        vocabulary=['the', 'a', 'stop',\n                                                    'word', 'apple',\n                                                    'banana'])\n\n        documents = [\n            'The quick brown fox jumps over the lazy dog.',\n            'a stop word is not a stop word',\n            'Apple is a fruit.',\n        ]\n\n        X = vectorizer.fit_transform(documents)\n\n        # Check if expected terms are present\n        assert_equal(vectorizer.get_feature_names(), ['a', 'stop',\n                                                  'word', 'apple',\n                                                  'banana', 'the'])\n        assert_array_equal(X[0].toarray(), [[1, 0, 1, 0, 0, 1]])\n\n\n\n", "    def test_stop_words_handling(self):\n        vectorizer = Vectorizer(stop_words=\"english\")\n        X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n        expected_stop_words = set(ENGLISH_STOP_WORDS)\n        for word in expected_stop_words:\n            assert_not_equal(\n                vectorizer.vocabulary_.get(word, -1), -1,\n                \"Stop word '{}' not removed\".format(word)\n            )\n\n        for row in X.toarray():\n            for word in expected_stop_words:\n                assert_equal(row[vectorizer.vocabulary_.get(word, -1)], 0,\n                             \"Stop word '{}' found in fitted data\".format(word))\n\n\n", "    def test_stop_words_with_vocabulary(self):\n        # Regression test for #11077: stop_words should be respected even if a vocabulary\n        # is provided\n\n        vectorizer = TfidfVectorizer(stop_words='english', vocabulary=['the', 'this', 'not'])\n        \n        docs = ['this is a test', 'this is not a test', 'that is a test']\n        X = vectorizer.fit_transform(docs)\n        expected_indices = [0, 1, 2]  # Indices for 'this', 'is', 'test'\n\n        assert_array_equal(np.where(X.toarray() != 0)[1], expected_indices)\n", "    def test_stop_words_filtering():\n        \n        # Test stop words filtering with a custom stop words list\n        vectorizer = TfidfVectorizer(stop_words=['the', 'this'])\n        X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n\n        expected_terms = [\n            'pizza', 'burger', 'beer', 'copyright', 'salad', 'celeri',\n            'tomato', 'water'\n        ]\n        \n        assert_equal(vectorizer.get_feature_names(), expected_terms)\n        for term in expected_terms:\n            assert_equal(X[:, vectorizer.vocabulary_[term]].getnnz(0),\n                         len(ALL_FOOD_DOCS)) \n\n", "    def test_stop_words_max_df(self):\n        # Test stop_words with max_df\n        vectorizer = CountVectorizer(stop_words='english', max_df=0.8)\n        X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n        stop_words = set(vectorizer.stop_words_)\n        for word in ENGLISH_STOP_WORDS:\n            assert word in stop_words\n\n        vectorizer = TfidfVectorizer(stop_words='english', max_df=0.8)\n        X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n        stop_words = set(vectorizer.stop_words_)\n        for word in ENGLISH_STOP_WORDS:\n            assert word in stop_words\n\n\n\n"], "sample_141": ["    def test_invalid_json_serialization(self):\n        \"\"\"\n        Tests that a serializer raises an exception when passed\n        invalid JSON.\n        \"\"\"\n        with self.assertRaises(json.JSONDecodeError):\n            serializers.serialize('json', [], format='invalid')\n", "    def test_serialize_deserialise_with_deferred_fields(self):\n        obj = Score(\n            content_type_id=1,\n            object_id=1,\n            score=12,\n            user=self.user,\n        )\n        obj.save()\n        # serialize the object\n        serial = serializers.serialize(self.serializer_name, [obj])\n        # un-serialize the object\n        objs = serializers.deserialize(self.serializer_name, serial)\n        for obj in objs:\n            self.assertEqual(obj.object.pk, obj.object.pk)\n\n\n", "    def test_natural_keys(self):\n        Category.objects.create(name='Reference')\n        category = Category.objects.create(name='Non-fiction')\n        with self.assertNumQueries(1):\n            article = Article.objects.create(headline='Poker has no place on ESPN', author=author, pub_date=datetime.datetime(2006, 6, 16, 11, 0, 0), categories=[category])\n        serial_str = serializers.serialize(self.serializer_name, [article])\n        self.assertIn(category.name, self._get_field_values(serial_str, 'categories'))\n\n\n", "    def test_serialize_pkless_with_fields(self):\n        obj1 = Category.objects.create(name=\"Reference\")\n        obj2 = Category.objects.create(name=\"Non-fiction\")\n        serialized = serializers.serialize(self.serializer_name, [obj1, obj2], use_natural_foreign_keys=True)\n        self.assertTrue(self._validate_output(serialized))\n        self.assertEqual(self._get_pk_values(serialized), [None, None])\n        self.assertEqual(self._get_field_values(serialized, \"name\"), [\"Reference\", \"Non-fiction\"])\n\n\n\n", "    def test_serialize_object_with_deferred_fields(self):\n        category = Category.objects.create(name=\"Reference\")\n        article = Article.objects.create(headline=\"Poker has no place on ESPN\",\n                                       author=category, pub_date=datetime.date(2006, 6, 16))\n        score = Score.objects.create(article=article, points=10)\n        deserializer = serializers.get_serializer('json')\n        serialized_data = deserializer.serialize(\n            [score], use_natural_foreign_keys=True,\n        )\n        deserialized_data = deserializer.deserialize(serialized_data)\n\n        # Check that deferred fields are handled correctly\n        self.assertEqual(len(deserialized_data), 1)\n        deserialized_obj = deserialized_data[0]\n        self.assertIsInstance(deserialized_obj, Score)\n        self.assertEqual(deserialized_obj.article_id, article.id)\n        self.assertEqual(deserialized_obj.points, 10)\n\n\n"], "sample_623": ["    def test_to_zarr_dtype_mapping(\n        dtype, expected_dtype, tmp_path", "    def test_get_default_engine(\n        monkeypatch, mode: tuple[str, str], expected_engine: str", "    def test_to_netcdf_with_invalid_netcdf(\n        tmp_path: Path,", "    def test_to_netcdf_mode(\n        self, mode, expected_mode, tmp_path, caplog", "    def test_to_netcdf_format(\n        mode: str, expected_format: str, tmp_path: str"], "sample_462": ["compilation error", "    def test_choice_field_with_callable_choices(self):\n        class CallableChoices:\n                return [\n                    (\"value1\", \"Label 1\"),\n                    (\"value2\", \"Label 2\"),\n                ]\n\n        field = ChoiceField(choices=CallableChoices())\n        form = Form({\"choice\": \"value1\"})\n\n        field.widget.choices = [(v, k) for k, v in field.choices]\n        self.assertDataEqual(field, form, \"value1\")\n\n        data = {\"choice\": \"value3\"}\n        with self.assertRaises(ValidationError):\n            form = Form(data)\n            field.clean(data[\"choice\"])\n", "    def test_validate_with_invalid_choice(self):\n        field = ChoiceField(choices=[(\"a\", \"A\"), (\"b\", \"B\")])\n        form = Form({\"field\": \"c\"})\n\n        with self.assertRaises(ValidationError) as context:\n            form.is_valid()\n\n        self.assertEqual(context.exception.error_dict[\"field\"], [\"Select a valid choice. %(value)s is not one of the available choices.\"])\n", "    def test_validate_choices(self):\n        field = ChoiceField(choices=[('choice1', 'Choice 1'), ('choice2', 'Choice 2')])\n        form = Form({'field': 'invalid'})\n        with self.assertRaises(ValidationError):\n            form.is_valid()\n\n\n", "    def test_choice_field_required(self):\n        field = ChoiceField(choices=[(\"a\", \"a\"), (\"b\", \"b\")], required=True)\n        form = Form({\"choice\": \"\"})\n        with self.assertRaises(ValidationError):\n            field.clean(form)\n"], "sample_245": ["    def test_no_i18n(self):\n        # Ensure that makemessages doesn't create a .po file if USE_I18N is False\n        self.assertFalse(os.path.exists(self.PO_FILE))\n\n\n        with self.settings(USE_I18N=False):\n            execute_from_command_line(['django-admin', 'makemessages', locale=[LOCALE], verbosity=2])\n\n        self.assertFalse(os.path.exists(self.PO_FILE))\n", "    def test_dont_create_po_file_if_use_i18n_is_false(self):\n        with override_settings(USE_I18N=False):\n            self._run_makemessages()\n        self.assertFalse(os.path.exists(self.PO_FILE))\n", "    def test_locale_dir_from_path(self):\n        with tempfile.TemporaryDirectory() as tmpdir:\n            self.create_test_files(tmpdir)\n            self.addCleanup(shutil.rmtree, tmpdir)\n            self.run_makemessages(\n                locale=[LOCALE],\n                input_dir=tmpdir,\n                # Force a specific locale path by providing --locale\n                # and check that the output .po file is created there\n                force_locale=LOCALE,\n            )\n            self.assertTrue(os.path.exists(os.path.join(tmpdir, 'locale', LOCALE, 'LC_MESSAGES', 'django.po')))\n", "    def test_no_new_files(self):\n        self._delete_locale_files()\n        output, _ = self._run_makemessages(update=False)\n        self.assertFalse(os.path.exists(self.PO_FILE))\n        self.assertIn('No new translation files found', output)\n\n\n\n", "    def test_translations_copied_from_django(self):\n        self.create_test_locale_files(\n            {'de': {\"django/urls.py\": 'gettext_noop(\"Hello World!\")'}}\n        )\n        output, po_contents = self._run_makemessages(\n            locale=[LOCALE],\n            extensions=['py'],\n        )\n        self.assertMsgStr(\n            '\"Hello World!\"', po_contents, use_quotes=True,\n        )\n        self.assertLocationCommentPresent(self.PO_FILE, 1, 'django', 'urls', 'urls.py')\n\n\n\n"], "sample_792": ["    def test_BernoulliNB_binarize_with_sparse_data(self):\n        X_sparse = scipy.sparse.csr_matrix(\n            ([1, 0, 1, 1], ([0, 1, 1, 2], [0, 0, 1, 1])), shape=(3, 3)\n        )\n        binarize_value = 0.5\n\n        clf = BernoulliNB(binarize=binarize_value)\n        clf.fit(X_sparse, np.array([1, 2, 3]))\n        assert_equal(clf.binarize, binarize_value)\n\n        # Check the class_log_prior_ and feature_log_prob_ after fitting\n        assert_equal(\n            clf._check_feature_log_prob(X, self.feature_log_prob),\n            True\n        )\n        \n        X_sparse_after_fit = clf._transform(X_sparse)\n\n        assert_equal(\n            X_sparse_after_fit.shape,\n            X_sparse.shape\n        )\n\n\n", "    def test_bernoulli_nb_fit_predict_with_binarize(self):\n        # Ensure binarization works correctly with fit_predict\n        X = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\n        y = np.array([1, 2, 3])\n\n        clf = BernoulliNB(binarize=0.5)\n        clf.fit(X, y)\n        y_pred = clf.predict(X)\n\n        assert_array_equal(y_pred, y)\n\n\n\n", "    def test_fit_partial_gaussian(self):\n        \"\"\"Test fit_partial for GaussianNB\"\"\"\n        gnb = GaussianNB(priors=None, warm_start=True, fit_prior=True)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n        # First fit with some data\n        gnb.fit(X_train, y_train)\n\n        # Call partial fit with some more data\n        gnb.partial_fit(X_test, y_test, classes=gnb.classes_)\n\n        # Check if the model parameters are updated\n        assert_array_almost_equal(gnb.theta_, gnb.theta_ + gnb.coef_ * X_test.T)\n        assert_array_almost_equal(gnb.sigma_, gnb.sigma_ + gnb.variance_ * X_test.T)\n\n\n\n", "    def test_multi_class_multinomial(self):\n        X = np.random.randint(5, size=(10, 20))\n        y = np.random.randint(3, size=10)\n\n        clf = MultinomialNB()\n        clf.fit(X, y)\n        y_pred = clf.predict(X)\n        assert_array_equal(y_pred, y)\n", "    def test_bernoulli_nb_binarize(self):\n        clf = BernoulliNB(binarize=0.5)\n        X = np.array([[0.1, 0.6, 0.9], [0.4, 0.3, 0.2]])\n        y = np.array([1, 2])\n        clf.fit(X, y)\n\n        X_binarized = clf.binarize(X, threshold=0.5)\n\n        assert_array_equal(X_binarized, np.array([[0, 1, 1], [1, 0, 0]]))\n\n\n\n"], "sample_840": ["    def test_pls_regression_can_learn_from_same_data(self):\n        X, y = load_linnerud()\n        X = StandardScaler().fit_transform(X)\n        y = StandardScaler().fit_transform(y.reshape(-1, 1))\n        pls = PLSRegression(n_components=1)\n        pls.fit(X, y)\n        y_pred = pls.predict(X)\n        assert_array_almost_equal(y_pred, pls.transform(X) @ pls.coef_[:, 0]) \n\n\n", "compilation error", "    def test_pls_regression_with_one_target(self):\n        X, y = load_linnerud(return_X_y=True)\n        X = X[:, :5]  \n        y = y['lwt']\n        n_components = 2\n\n        pls = PLSRegression(n_components=n_components)\n        pls.fit(X, y.values.reshape(-1, 1))\n        X_scores, y_scores = pls.transform(X)\n\n        # Check the shape of the scores\n        assert X_scores.shape == (X.shape[0], n_components)\n        assert y_scores.shape == (X.shape[0], n_components)\n", "    def test_pls_regression_with_single_target(self):\n        n_samples = 30\n        n_features = 10\n        n_components = 2\n        rng = check_random_state(42)\n        X = rng.rand(n_samples, n_features)\n        y = rng.rand(n_samples, 1)\n        pls = PLSRegression(n_components=n_components)\n        pls.fit(X, y)\n        y_pred = pls.predict(X)\n        assert_array_almost_equal(y_pred.shape, (n_samples, 1))\n\n\n", "    def test_pls_regression_with_algorithm(self, algorithm):\n        X, Y, _ = load_linnerud(return_X_y=True)\n        X = StandardScaler().fit_transform(X)\n        Y = StandardScaler().fit_transform(Y)\n        pls = PLSRegression(n_components=2, algorithm=algorithm)\n        pls.fit(X, Y)\n        X_predict = pls.predict(X)\n        assert_array_almost_equal(pls.coef_, np.dot(pls.x_weights_,\n                                                    pls.y_weights_.T))\n\n"], "sample_162": ["    def test_no_translations(self):\n        \"\"\"\n        Test that no POT files are generated if there are no translatable strings.\n        \"\"\"\n        with self.create_temp_dir() as root:\n            # Create a file without any translatable strings.\n            with self.create_temp_file(root, \"no_translations.html\", \"No translatable strings here.\") as f:\n                pass\n            # Run makemessages and check for POT file generation.\n            with captured_stderr() as stderr:\n                execute_from_command_line(['django-admin', 'makemessages', '--locale', LOCALE, '--noinput',  '--srcdir', root])\n            self.assertIn(\"No translatable strings found in\", stderr.getvalue())\n            self.assertFalse(os.path.exists(self.PO_FILE))\n\n\n\n", "    def test_makemessages_no_locale_specified(self):\n        with self.assertRaises(CommandError) as cm:\n            execute_from_command_line(['django-admin', 'makemessages'])\n        self.assertIn('Type \"%s help makemessages\"' % 'makemessages', str(cm.exception))\n", "    def test_no_obsolete_without_gettext_019(self):\n        if gettext_version is None:\n            self.skipTest(\"Need xgettext to run this test\")\n        if gettext_version < (0, 19):\n            self.skipTest(\"gettext < 0.19: skipping test which requires >= 0.19\")\n        #\n        # This test checks that `--no-obsolete` works without gettext 0.19, it is\n        # known to fail with older versions of gettext\n        #\n        with tempfile.TemporaryDirectory() as tmpdir:\n            copytree(self.src_path, os.path.join(tmpdir, 'project'))\n            self.create_translations(tmpdir, LOCALE)\n            with open(os.path.join(tmpdir, 'project', self.PO_FILE), 'a') as fp:\n                fp.write(\"msgid \\\"old string\\\" \\nmsgstr \\\"old translation\\\"\\n\")\n            #\n            # Remove the file in the source to simulate the file being obselete\n            # (remove the file in the source)\n            os.remove(os.path.join(self.src_path, 'django/conf/locale/de/LC_MESSAGES/django.po'))\n\n            args = [\n                'makemessages',\n                '-l', LOCALE,\n                '--no-obsolete',\n                '--work-dir', tmpdir,\n                '--domain', 'django',\n            ]\n            try:\n                execute_from_command_line(args)\n            except CommandError:\n                self.fail(\"`--no-obsolete` should work even with files missing in source\")\n\n\n\n", "    def test_no_output_locale_no_domain(self):\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore', TranslatorCommentWarning)\n            output, _ = self._run_makemessages(locale=None, domain=None)\n        self.assertFalse(os.path.exists(self.PO_FILE))\n\n\n\n", "    def test_makemessages_with_no_default_ignore_patterns(self):\n        \"\"\"Test that default ignore patterns are not applied when `use_default_ignore_patterns` is False.\"\"\"\n        with tempfile.TemporaryDirectory() as tmpdir:\n            self.create_test_data(tmpdir)\n\n            # Create a test file in a directory we want to ignore\n            ignore_dir = os.path.join(tmpdir, 'ignored')\n            os.makedirs(ignore_dir)\n            with open(os.path.join(ignore_dir, 'test_file.txt'), 'w', encoding='utf-8') as f:\n                f.write('This is a translatable string in an ignored directory.')\n\n            # Run makemessages\n            with captured_stderr() as stderr:\n                execute_from_command_line(['python', '-m', 'django.core.management', 'makemessages',\n                                          '--locale', LOCALE,\n                                          '--all', '--no-default-ignore-patterns',\n                                          '--verbosity', '2',\n                                          '--settings', 'tests.settings'])\n\n            # Assert that the file in the ignored directory was NOT processed\n            with open(self.PO_FILE) as fp:\n                po_contents = fp.read()\n            self.assertNotIn('This is a translatable string in an ignored directory.', po_contents)\n            self.assertEqual(stderr.getvalue().strip(), '')\n"], "sample_191": ["    def test_watchman_reloader_availability(self):\n        with mock.patch('django.utils.autoreload.WatchmanReloader.check_availability') as mock_check:\n            mock_check.side_effect = WatchmanUnavailable('Watchman error')\n            self.assertRaises(WatchmanUnavailable, autoreload.get_reloader)\n            mock_check.side_effect = None\n            reloader = autoreload.get_reloader()\n            self.assertIsInstance(reloader, autoreload.WatchmanReloader)\n", "    def test_iter_modules_and_files_with_zip(self):\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            zip_path = Path(tmpdirname) / 'my_module.zip'\n            with zipfile.ZipFile(zip_path, 'w') as zf:\n                zf.write('__init__.py', arcname='__init__.py')\n                zf.write('module.py', arcname='module.py')\n\n            extend_sys_path([tmpdirname])\n            self.import_and_cleanup('my_module')\n            self.assertFileFound(zip_path / '__init__.py')\n            self.assertFileFound(zip_path / 'module.py')\n\n\n\n", "    def test_error_files_recorded(self):\n        filename = self.temporary_file('test_module.py')\n        with open(filename, 'w') as f:\n            f.write('def foo(): pass')\n        py_compile.compile(filename)\n        # Cause an error when importing\n        with open(filename, 'a') as f:\n            f.write('\\nraise ValueError(\"test error\")')\n        try:\n            self.import_and_cleanup('test_module')\n        except ValueError:\n            pass\n        self.assertFileFound(filename)\n        self.assertIn(filename, autoreload._error_files)\n        # The error is cleared\n        self.import_and_cleanup('test_module')\n        self.assertNotIn(filename, autoreload._error_files)\n", "    def test_iter_modules_and_files_with_zipfile(self):\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            zip_filename = self.temporary_file('my_module.zip')\n            with zipfile.ZipFile(zip_filename, 'w') as zf:\n                zf.write('module.py', arcname='my_module/module.py')\n            extend_sys_path(tmpdirname)\n            self.import_and_cleanup('my_module')\n            self.assertFileFound(zip_filename / 'my_module/module.py')\n", "    def test_iter_modules_and_files_with_zipfiles(self):\n        with tempfile.TemporaryDirectory() as tmpdir:\n            zip_filename = Path(tmpdir) / \"my_module.zip\"\n\n            with zipfile.ZipFile(zip_filename, 'w') as zipf:\n                zipf.write(\"module1.py\", \"module1.py\")\n                zipf.write(\"module2.py\", \"module2.py\")\n\n            extend_sys_path([tmpdir])\n            self.import_and_cleanup(\"my_module\")\n            self.assertFileFound(zip_filename / \"module1.py\")\n            self.assertFileFound(zip_filename / \"module2.py\")\n            self.clear_autoreload_caches()\n            with mock.patch('zipimport.zipimporter'):\n                zip_importer_mock = mock.Mock()\n                zip_importer_mock.return_value.__spec__.loader = zip_importer_mock\n                with mock.patch(\n                        'django.utils.autoreload.zipimport.zipimporter',\n                        zip_importer_mock\n                ):\n                    self.assertFileFound(zip_filename / \"module1.py\")\n\n\n\n"], "sample_1209": ["    def test_prefix_unit():\n        unit = meter\n        expected_units = [Quantity('millimeter', abbrev='mm', is_prefixed=True),\n                          Quantity('centimeter', abbrev='cm', is_prefixed=True),\n                          Quantity('kilometer', abbrev='km', is_prefixed=True)]\n        assert prefix_unit(unit, {'m': PREFIXES['m'], 'c': PREFIXES['c'], 'd': PREFIXES['d']}) == expected_units\n\n", "    def test_prefix_unit_binary(self):\n        base_unit = meter\n        binary_prefixes = { 'Ki': kibi, 'Mi': mebi }\n        expected_units = [\n            Quantity(\"kimeter\", abbrev=\"kM\", is_prefixed=True),\n            Quantity(\"Mimeter\", abbrev=\"mM\", is_prefixed=True),\n        ]\n        self.assertEqual(prefix_unit(base_unit, binary_prefixes), expected_units)\n", "    def test_prefix_unit_binary_prefixes(self):\n        unit = meter\n        prefixes = BIN_PREFIXES\n        expected_units = [Quantity(\"kibi(meter)\", abbrev=\"Kibm\", is_prefixed=True),\n                          Quantity(\"mebi(meter)\", abbrev=\"Mibm\", is_prefixed=True),\n                          Quantity(\"gibi(meter)\", abbrev=\"Gibm\", is_prefixed=True),\n                          Quantity(\"tebi(meter)\", abbrev=\"Tibm\", is_prefixed=True),\n                          Quantity(\"pebi(meter)\", abbrev=\"Pibm\", is_prefixed=True),\n                          Quantity(\"exbi(meter)\", abbrev=\"Eibm\", is_prefixed=True)]\n\n        self.assertEqual(prefix_unit(unit, prefixes), expected_units)\n\n", "    def test_prefix_unit_binary_prefixes():\n        from sympy.physics.units.quantities import Quantity\n        from sympy.physics.units import UnitSystem\n\n        # Define a custom unit\n        custom_unit = Quantity('custom', abbrev='cu', is_prefixed=True)\n\n        # Use binary prefixes to create new unit with custom unit\n        prefixed_units = prefix_unit(custom_unit, BIN_PREFIXES)\n\n        # Assert that each prefixed unit is created correctly\n        assert len(prefixed_units) == 6\n        assert prefixed_units[0].name == 'Kibicustom'\n        assert prefixed_units[0].abbrev == 'Ki cu'\n        assert prefixed_units[0].scale_factor == kibi.scale_factor\n\n        assert prefixed_units[1].name == 'Mibicustom'\n        assert prefixed_units[1].abbrev == 'Mi cu'\n        assert prefixed_units[1].scale_factor == mebi.scale_factor\n\n        assert prefixed_units[2].name == 'Gibicustom'\n        assert prefixed_units[2].abbrev == 'Gi cu'\n        assert prefixed_units[2].scale_factor == gibi.scale_factor\n\n        assert prefixed_units[3].name == 'Tibicustom'\n        assert prefixed_units[3].abbrev == 'Ti cu'\n        assert prefixed_units[3].scale_factor == tebi.scale_factor\n\n        assert prefixed_units[4].name == 'Pibicustom'\n        assert prefixed_units[4].abbrev == 'Pi cu'\n        assert prefixed_units[4].scale_factor == pebi.scale_factor\n\n        assert prefixed_units[5].name == 'Exbicustom'\n        assert prefixed_units[5].abbrev == 'Ei cu'\n        assert prefixed_units[5].scale_factor == exbi.scale_factor\n\n\n\n", "    def test_prefix_unit_binary(self):\n        from sympy.physics.units.quantities import Quantity\n        q = Quantity('1 kibi meter', abbrev='Kibim')\n        self.assertEqual(q.scale_factor, kibi.scale_factor * meter.scale_factor)\n        self.assertEqual(q.name, 'kibi meter')\n        self.assertEqual(q.abbrev, 'Kibim')\n\n        for p in BIN_PREFIXES:\n            quantity = prefix_unit(meter, {p: BIN_PREFIXES[p]})\n            self.assertEqual(quantity[0].scale_factor, BIN_PREFIXES[p].scale_factor * meter.scale_factor)\n"], "sample_175": ["    def test_restricted_error_with_cascade(self):\n        \"\"\"\n        Test that RestrictedError is raised when trying to delete objects with\n        cascade=RESTRICT, despite being able to delete them through a\n        different path.\n        \"\"\"\n        a = create_a()\n        b = create_b(a)\n\n        with self.assertRaises(RestrictedError) as context:\n            Collector().collect(b, source=b.a)\n\n        msg = context.exception.args[0]\n        self.assertIn('Cannot delete some instances of model \"B\"', msg)\n        self.assertIn(\"'A.b' and 'B.a'\", msg)\n\n\n\n", "    def test_restricted_cascade_dependencies(self):\n        with self.subTest('create initial objects'):\n            a = create_a()\n            b1 = B1.objects.create(a=a)\n            b2 = B2.objects.create(b1=b1)\n            r1 = MR.objects.create(b=b2)\n        with self.subTest('deletion starts with b2'):\n            collector = Collector(using=connection.get_default_db())\n            collector.collect(\n                [b2],\n                source=B2,\n                nullable=False,\n                collect_related=True,\n                fail_on_restricted=False\n            )\n            with self.assertRaises(RestrictedError):\n                collector.delete()\n\n\n", "    def test_collect_cascade_with_restricted_objects(self):\n        a = create_a()\n        b = B.objects.create(a=a)\n\n        collector = Collector(using=connection.cursor().db)\n        collector.collect(b, field=a.get_related_field(), nullable=False)\n        collector.sort()\n        \n        with self.assertRaises(RestrictedError) as context:\n            collector.delete()\n        errors = context.exception.args[0]\n        self.assertIn(\"'A.a' \", errors)\n\n\n", "    def test_restricted_error_with_cascade(self):\n        a1 = create_a(b1=B1.objects.create())\n        b1 = a1.b1\n        # Set up a restricted foreign key on B1\n        B1.objects.all().update(a=None)\n\n        with self.assertRaises(RestrictedError) as context:\n            a1.delete()\n        self.assertIn(\"Cannot delete some instances of model 'b1'\", str(context.exception))\n", "    def test_restricted_error_with_cascade(self):\n        a1 = create_a()\n        b1 = B1.objects.create(a=a1)\n        b2 = B2.objects.create(a=a1)\n        with self.assertRaises(RestrictedError):\n            b1.delete()\n"], "sample_71": ["    def test_scientific_notation_formatting(self):\n        number = Decimal('1.23456789012345678901234567890e-100')\n        expected = '1.23456789012345678901234567890e-100'\n        self.assertEqual(nformat(number, decimal_sep='.'), expected)\n", "    def test_scientific_notation(self):\n        number = Decimal('1.23456789012345678901234567890e200')\n        formatted_number = nformat(number, decimal_sep='.', decimal_pos=None, grouping=0)\n        self.assertIsInstance(formatted_number, str)\n        self.assertIn('e', formatted_number)\n", "    def test_scientific_notation(self):\n        number = Decimal('1.2345678901234567890123456789e200')\n        formatted_number = nformat(number, decimal_sep=',', decimal_pos=2)\n        self.assertEqual(formatted_number, '1.23e+200')\n", "    def test_float_format(self):\n        self.assertEqual(nformat(float_info.max), nformat(float_info.max, decimal_sep='.'))\n", "    def test_format_decimal_large_number(self):\n        number = Decimal('12345678901234567890.1234567890')\n        formatted_number = nformat(number, decimal_sep='.', decimal_pos=3, grouping=3)\n        self.assertEqual(formatted_number, '12,345,678,901,234,567,890.123')\n\n"], "sample_88": ["    def test_attach_file_non_guessable_mimetype(self):\n        with tempfile.NamedTemporaryFile('w', delete=False) as tmpfile:\n            tmpfile.write('Test content.\\n')\n            tmpfile.flush()\n            filepath = tmpfile.name\n            msg = EmailMessage(\n                subject='Test',\n                body='Some message body.',\n                from_email='test@example.com',\n                to=['test@example.com'],\n            )\n            msg.attach_file(filepath, mimetype='application/octet-stream')\n            msg_bytes = msg.as_bytes()\n            msg = message_from_bytes(msg_bytes)\n\n            self.assertEqual(msg.get_content_type(), 'multipart/mixed')\n            self.assertEqual(len(msg.get_payload()), 2)  # Main message + attachment\n\n            attachment = msg.get_payload()[1]\n            self.assertEqual(attachment.get_content_type(), 'application/octet-stream')\n            self.assertEqual(attachment.get_filename(), os.path.basename(filepath))\n\n\n", "    def test_email_message_to_string(self):\n        msg = EmailMessage(\n            'Subject', 'Body', 'from@example.com', ['to@example.com'],\n            attachments=[('attachment.txt', 'Content here!', 'text/plain')]\n        )\n        self.assertIn('Content-Disposition: attachment', msg.as_string())\n        self.assertIn('Subject: Subject', msg.as_string())\n        self.assertIn('To: to@example.com', msg.as_string())\n        self.assertIn('From: from@example.com', msg.as_string())\n        self.assertIn('attachment.txt', msg.as_string())\n\n        expected_content = \"\"\"To: to@example.com", "    def test_attach_file_text_encoding_failure(self):\n        with tempfile.TemporaryDirectory() as tempdir:\n            # Create a text file with some UTF-8 characters\n            filename = os.path.join(tempdir, 'test.txt')\n            with open(filename, 'w', encoding='utf-8') as f:\n                f.write('This file contains some \u00fcnic\u00f6de characters.')\n\n            # Send EmailMessage with the file attachment\n            message = EmailMessage(\n                subject='Test message',\n                body='Test body',\n                from_email='test@example.com',\n                to=['recipient@example.com'],\n            )\n            message.attach_file(filename)\n            message.send()\n\n            # Check that the attachment was sent as expected\n            decoded_attachments = self.get_decoded_attachments(message)\n            self.assertEqual(len(decoded_attachments), 1)\n            filename, content, mimetype = decoded_attachments[0]\n            self.assertEqual(filename, 'test.txt')\n            self.assertEqual(content.decode('utf-8'), 'This file contains some \u00fcnic\u00f6de characters.')\n            self.assertEqual(mimetype, 'text/plain')\n", "    def test_sanitize_address(self):\n        self.assertEqual(sanitize_address(None), '')\n        self.assertEqual(sanitize_address(''), '')\n        self.assertEqual(sanitize_address('alice'), 'alice')\n        self.assertEqual(sanitize_address('alice <alice@example.com>'),\n                         'alice <alice@example.com>')\n        self.assertEqual(sanitize_address(' \"Alice\" <alice@example.com>'),\n                         ' \"Alice\" <alice@example.com>')\n        self.assertEqual(sanitize_address(' alice <alice@example.com> '),\n                         ' alice <alice@example.com>')\n        with self.assertRaises(ValueError):\n            sanitize_address('invalid address')\n        with self.assertRaises(ValueError):\n            sanitize_address('alice@example.com (invalid)')\n", "    def test_sanitize_address(self):\n            self.assertEqual(sanitize_address(address), expected)\n\n        test_sanitize('john.doe@example.com', 'John Doe <john.doe@example.com>')\n        test_sanitize('john doe <john.doe@example.com>', 'John Doe <john.doe@example.com>')\n        test_sanitize('john doe', 'John Doe')\n\n\n"], "sample_305": ["    def test_aggregate_with_filter(self):\n        \"\"\"\n        Test aggregates with a filter on the related object.\n        \"\"\"\n        # Get average rating for books by author 'Adrian Holovaty'\n        avg_rating = Book.objects.filter(\n            authors__name='Adrian Holovaty'\n        ).aggregate(avg_rating=Avg('rating'))\n\n        self.assertEqual(avg_rating['avg_rating'], Decimal('4.5'))\n\n        # Ensure we can use Case expressions in aggregates with filter.\n        avg_rating_over_10 = Book.objects.filter(authors__name='Adrian Holovaty').aggregate(\n            avg_rating_over_10=Avg('rating', output_field=DecimalField())\n        )\n        self.assertTrue(\n            avg_rating_over_10['avg_rating_over_10'] >= Decimal('4.5')\n        )\n        self.assertEqual(\n            avg_rating_over_10['avg_rating_over_10'],\n            Decimal('4.5')\n        )\n\n\n", "    def test_year_lookups(self):\n        # Test various year lookups\n        self.assertEqual(Book.objects.filter(pubdate__year=2007).count(), 1)\n        self.assertEqual(Book.objects.filter(pubdate__year__gt=2007).count(), 4)\n        self.assertEqual(Book.objects.filter(pubdate__year__gte=2008).count(), 4)\n        self.assertEqual(Book.objects.filter(pubdate__year__lt=2007).count(), 0)\n        self.assertEqual(Book.objects.filter(pubdate__year__lte=2007).count(), 1)\n\n\n        # Test year lookups with non-integer values\n        with self.assertRaises(TypeError):\n            Book.objects.filter(pubdate__year='test')\n        with self.assertRaises(TypeError):\n            Book.objects.filter(pubdate__year=True)\n\n\n\n", "    def test_count_with_aggregate(self):\n        count = Book.objects.values('publisher').annotate(\n            book_count=Count('id')\n        ).order_by('publisher').values('publisher', 'book_count')\n        expected_count = {\n            'Apress': 2,\n            'Sams': 1,\n            'Prentice Hall': 1,\n            'Morgan Kaufmann': 2,\n            \"Jonno's House of Books\": 0,  \n        }\n        self.assertCountEqual(list(count), list(expected_count.items()))\n\n", "    def test_count_with_aggregate_lookup(self):\n        q = Book.objects.filter(Q(rating__gt=3.5) &\n                               Q(authors__name='Adrian Holovaty')).count()\n        self.assertEqual(q, 1)\n", "    def test_count_with_field_lookup(self):\n        books = Book.objects.filter(isbn__icontains='159059').count()\n        self.assertEqual(books, 2)\n"], "sample_1116": ["compilation error", "    def test_refine_singular_matrix():\n        X = MatrixSymbol('X', 2, 2)\n        with raises(ValueError) as exc:\n            refine(X.I, assumptions={})\n        assert \"singular matrix\" in str(exc.value)\n", "compilation error", "compilation error", "    def test_inverse_singular_matrix():\n        C = MatrixSymbol('C', 2, 2)\n        with raises(ValueError):\n            refine(Inverse(C))\n"], "sample_336": ["compilation error", "    def test_reverse_with_invalid_urlconf(self):\n        with self.assertRaises(ImproperlyConfigured):\n            get_urlconf('nonexistent_urlconf')\n", "    def test_no_urlconf(self):\n        with self.assertRaises(ImproperlyConfigured):\n            reverse('no_url')\n", "    def test_defaults_with_positional_only(self):\n        with self.assertRaises(NoReverseMatch):\n            reverse('defaults_view3', args=['42'])\n", "    def test_reverse_with_default_kwargs(self):\n        with self.assertRaises(NoReverseMatch):\n            reverse('non_existant', kwargs={'arg1': '42'})\n"], "sample_953": ["def test_ask_user_with_existing_conf_py(capsys):\n    with patch('builtins.input', side_effect=['', 'new_path', '', '']):\n        d = {}\n        qs.ask_user(d)\n    captured = capsys.readouterr()\n    assert 'Error: an existing conf.py has been found' in captured.out\n    assert 'sphinx-quickstart will not overwrite existing Sphinx projects.' in captured.out\n    assert d['path'] == 'new_path'  \n", "def test_override_build_dir(capsys):\n    answers = {\n        'Enter the root path for the documentation.': '/tmp/test_doc',\n        'You have two options for placing the build directory for Sphinx output. Either, you use a directory \"_build\" within the root path, or you separate \"source\" and \"build\" directories within the root path.\\nYou can press Enter to choose the default.  \\nYou have two options for placing the build directory for Sphinx output. Either, you use a directory \"_build\" within the root path, or you separate \"source\" and \"build\" directories within the root path. \\nYou can press Enter to choose the default.  \\nYou have two options for placing the build directory for Sphinx output. Either, you use a directory \"_build\" within the root path, or you separate \"source\" and \"build\" directories within the root path. \\nYou can press Enter to choose the default.  n': 'y',\n    }\n    with pytest.raises(AssertionError):\n        qs.term_input = mock_input(answers)\n        qs.generate(\n            {'path': '/tmp/test_doc',\n             'sep': True},\n            overwrite=False\n        )\n", "    def test_quickstart_with_custom_template(tmp_path):\n        with patch('sphinx.cmd.quickstart.readline.parse_and_bind') as mock_readline:\n            custom_template_dir = tmp_path / 'templates'\n            custom_template_dir.mkdir()\n            (custom_template_dir / 'quickstart/master_doc.rst_t').write('This is a custom template')\n\n            qs.term_input = mock_input({'root path': str(tmp_path),\n                                        'project': 'My Project',\n                                        'author': 'My Name',\n                                        'version': '0.1',\n                                        'release': '0.1.0'})\n\n            qs.generate({'path': str(tmp_path),\n                         'sep': False,\n                         'dot': '.',\n                         'project': 'My Project',\n                         'author': 'My Name',\n                         'version': '0.1',\n                         'release': '0.1.0',\n                         'language': None,\n                         'suffix': '.rst',\n                         'master': 'index',\n                         'extensions': [],\n                         'makefile': True,\n                         'batchfile': True,\n                         'templatedir': str(custom_template_dir)},\n                         silent=True, templatedir=str(custom_template_dir))\n\n            assert path.isfile(path.join(tmp_path, 'source', 'index.rst'))\n            with open(path.join(tmp_path, 'source', 'index.rst'), 'r', encoding='utf-8') as f:\n                assert f.read().strip() == 'This is a custom template'\n", "    def test_ask_user_sep(monkeypatch, sep):\n        monkeypatch.setattr(qs, 'term_input', mock_input({\n            qs.PROMPT_PREFIX + 'Enter the root path for the documentation. ':\n                '/tmp/test_quickstart',\n            qs.PROMPT_PREFIX + 'You have two options for placing the build directory for Sphinx output.\\n'\n            'Either, you use a directory \"_build\" within the root path, or you separate\\n'\n            '\"source\" and \"build\" directories within the root path. '\n            qs.PROMPT_PREFIX + 'Separate source and build directories (y/n) ': 'y' if sep else 'n'\n        }, needanswer=True))\n        qs.ask_user({'path': '/tmp/test_quickstart'})\n        assert path.isdir(path.join('/tmp/test_quickstart', 'source')) == sep\n\n\n\n", "    def test_quickstart_conflict_extensions(tmpdir):\n        answers = {\n            qs.PROMPT_PREFIX + 'Enable imgmath: (y/n)': 'y',\n            qs.PROMPT_PREFIX + 'Enable mathjax: (y/n)': 'y',\n        }\n        with patch('builtins.input', mock_input(answers)):\n            qs.generate(d, overwrite=False, silent=True, templatedir=str(tmpdir))\n"], "sample_774": ["    def test_ordinal_encoder_fit_transform_with_provided_categories(self):\n        enc = OrdinalEncoder(categories=[\n            [0, 1, 2],  # Feature 1\n            ['a', 'b', 'c'],  # Feature 2\n            [3.0, 4.0, 5.0]  # Feature 3\n        ])\n        X = np.array([[0, 'a', 3.0],\n                      [1, 'b', 4.0],\n                      [2, 'c', 5.0]])\n        enc.fit(X)\n        X_transformed = enc.transform(X)\n        expected_X_transformed = np.array([[0, 1, 2],\n                                          [1, 2, 3],\n                                          [2, 3, 4]])\n        assert_array_equal(X_transformed, expected_X_transformed)\n\n        X_inverse_transformed = enc.inverse_transform(X_transformed)\n        assert_array_equal(X_inverse_transformed, X)\n\n\n\n", "    def test_onehotencoder_sparse_legacy_mode(self):\n        encoder = OneHotEncoder(sparse=True, legacy_mode=True)\n        X = np.array([[0, 1], [1, 0], [2, 2]])\n        encoder.fit(X)\n        result = encoder.transform(X)\n        expected = sparse.csr_matrix(\n            [[1, 0, 0, 1, 0, 0],\n             [0, 1, 0, 0, 1, 0],\n             [0, 0, 1, 0, 0, 1]],\n            shape=(3, 6))\n        assert_allclose(toarray(result), toarray(expected))\n", "    def test_onehotencoder_sparse_fit_transform(self):\n        X = np.array([[1, 2], [0, 1], [2, 0]])\n\n        enc = OneHotEncoder(sparse=True)\n        enc.fit(X)\n        X_trans = enc.transform(X)\n        assert_allclose(toarray(X_trans), [[1, 0, 0, 1, 0, 1],\n                                           [0, 1, 0, 0, 1, 0],\n                                           [0, 0, 1, 1, 0, 0]])\n\n        X_trans_inv = enc.inverse_transform(X_trans)\n        assert_array_equal(X_trans_inv, X)\n", "    def test_onehotencoder_categorical_features(self):\n        X = np.array([['a', 'b', 'c'],\n                      ['a', 'b', 'c'],\n                      ['d', 'e', 'f']])\n        y = np.array([1, 2, 3])\n        ohe = OneHotEncoder(categorical_features=[0, 2])\n        ohe.fit(X)\n        X_transformed = ohe.transform(X)\n        expected_shape = (3, 3)  # 3 samples, 3 features\n        assert_equal(X_transformed.shape, expected_shape)\n\n        # Check if the correct features were encoded\n        assert_array_equal(\n            X_transformed[0, :],\n            [1, 0, 0, 0, 0, 1])  # First sample encoded\n        assert_array_equal(\n            X_transformed[1, :],\n            [1, 0, 0, 0, 0, 1])  # Second sample encoded\n        assert_array_equal(\n            X_transformed[2, :],\n            [0, 0, 0, 1, 0, 0])  # Third sample encoded\n\n", "    def test_onehotencoder_default_categories(self):\n        enc = OneHotEncoder()\n        X = [['A', 1], ['B', 2], ['C', 3]]\n        enc.fit(X)\n\n        expected_output = np.array([[1, 0, 0, 0, 0],\n                                     [0, 1, 0, 0, 0],\n                                     [0, 0, 1, 0, 0]])\n        assert_array_equal(toarray(enc.transform(X)), expected_output)\n\n"], "sample_1111": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1009": ["    def test_separate_multi_frame():\n        R1 = ReferenceFrame('R1')\n        R2 = ReferenceFrame('R2')\n        v = R1.x + R2.x\n        assert v.separate() == {R1: R1.x, R2: R2.x}\n        ", "    def test_diff_variable_in_dcm(self):\n      q1 = dynamicsymbols('q1')\n      N = ReferenceFrame('N')\n      A = N.orientnew('A', 'Axis', [q1, N.y])\n      v = A.x\n      with self.assertWarns(UserWarning):\n        self.assertEqual(v.diff(q1, N), -A.z)\n\n\n\n", "    def test_diff_var_in_dcm():\n        t = dynamicsymbols('t')\n        q1 = dynamicsymbols('q1')\n        N = ReferenceFrame('N')\n        A = N.orientnew('A', 'Axis', [q1, N.y])\n        v = N.x + A.x\n        dvdt = v.diff(t, N)\n        expected = v.diff(t, N)\n        assert dvdt == expected\n", "def test_to_matrix():\n    from sympy.physics.vector import Vector\n\n    N = ReferenceFrame('N')\n    a, b, c = symbols('a, b, c')\n    v = a * N.x + b * N.y + c * N.z\n\n    assert (v.to_matrix(N) ==\n            Matrix([\n                [a],\n                [b],\n                [c]]))\n\n    beta = symbols('beta')\n    A = N.orientnew('A', 'Axis', (beta, N.x))\n    assert (v.to_matrix(A) ==\n            Matrix([\n                [a*cos(beta)],\n                [b*cos(beta) + c*sin(beta)],\n                [-b*sin(beta) + c*cos(beta)]]) )\n", "    def test_dot_with_dyadic():\n        raises(NotImplementedError, lambda: Vector(A.x).dot(A.x | A.y))\n"], "sample_0": ["    def test_propagate_add_subtract_with_unknown(self, UncertClass):\n        # Ensure that adding/subtracting\n        # UnknownUncertainty raises an exception.\n        data = NDData([1, 2, 3], unit='m')\n        other_uncert = UnknownUncertainty(array=[0.1, 0.2, 0.3], unit='m')\n\n        with pytest.raises(IncompatibleUncertaintiesException):\n            result = data + other_uncert\n\n        with pytest.raises(IncompatibleUncertaintiesException):\n            result = data - other_uncert\n\n\n\n", "    def test_propagation_with_correlated_uncertainties(self, UncertClass):\n        if UncertClass.supports_correlated:\n            # Test with correlated uncertainties\n            data1 = NDData([1, 2, 3], unit='m')\n            data2 = NDData([4, 5, 6], unit='m')\n            unc1 = UncertClass([0.1, 0.1, 0.1])\n            unc2 = UncertClass([0.2, 0.2, 0.2])\n\n            # Create correlated uncertainties\n            # For simplicity, assume a perfect correlation\n            unc1.correlation = 1  \n            unc2.correlation = 1\n\n            result = data1 + data2\n            \n            # Check propagation\n            result_unc = UncertClass._propagate_add_sub(\n                unc1, unc2, result, self.correlation,\n                subtract=False,\n            )\n            \n            expected_uncertainty = UncertClass([0.1414, 0.2828, 0.4243])\n            assert_allclose(result_unc, expected_uncertainty, atol=1e-5)\n        else:\n            pytest.skip(\"This uncertainty type does not support correlation\")\n\n\n", "    def test_incompatible_uncertainties(self, UncertClass):\n        with pytest.raises(IncompatibleUncertaintiesException):\n            assert UncertClass(1, unit='m')._convert_uncertainty(\n                StdDevUncertainty(1, unit='s'))\n        for u_type in uncertainty_types_with_conversion_support:\n            with pytest.raises(IncompatibleUncertaintiesException):\n                u = u_type(1, unit='m')\n                u._convert_uncertainty(\n                    u_type(1, unit='s'))\n        for u_type in uncertainty_types_without_conversion_support:\n            with pytest.raises(IncompatibleUncertaintiesException):\n                u = u_type(1, unit='m')\n                u._convert_uncertainty(\n                    u_type(1, unit='s'))\n\n\n", "    def test_propagate_add_sub_with_correlation(self, UncertClass):\n        data1 = NDData(np.array([1, 2, 3]), unit='m')\n        data2 = NDData(np.array([4, 5, 6]), unit='m')\n\n        uncert1 = UncertClass([0.1, 0.1, 0.1])\n        uncert2 = UncertClass([0.2, 0.2, 0.2])\n\n        result_data = data1 + data2\n        correlation = 0.5\n\n        if isinstance(UncertClass, (StdDevUncertainty, VarianceUncertainty, InverseVariance)):\n            propagated_uncert = UncertClass._propagate_add_sub(\n                uncert2, result_data, correlation)\n        else:\n            with pytest.raises(IncompatibleUncertaintiesException):\n                UncertClass._propagate_add_sub(uncert2, result_data, correlation)\n\n\n\n", "    def test_propagate_add_to_different_units(self, UncertClass):\n        if UncertClass in uncertainty_types_with_conversion_support:\n            data1 = NDData([1, 2, 3], unit='m')\n            data2 = NDData([4, 5, 6], unit='cm')\n            unc1 = UncertClass(np.array([0.1, 0.2, 0.3]), unit='m')\n            unc2 = UncertClass(np.array([0.1, 0.2, 0.3]), unit='cm')\n            result = data1 + data2\n            result_unc = unc1._propagate_add(unc2, result, 0)\n            assert result_unc.unit == 'm'\n\n\n\n"], "sample_182": ["    def test_select_related_chained(self):\n        result = Number.objects.select_related('reserved_name').filter(num__gt=5).values('num', 'reserved_name__name')\n        self.assertQuerysetEqual(result, [\n            {'num': 6, 'reserved_name__name': 'reserved_name_6'},\n            {'num': 7, 'reserved_name__name': 'reserved_name_7'},\n            {'num': 8, 'reserved_name__name': 'reserved_name_8'},\n            {'num': 9, 'reserved_name__name': 'reserved_name_9'},\n        ], ordered=False)\n\n\n", "    def test_union_with_empty_set(self):\n        numbers = Number.objects.filter(num__gt=5)\n        empty_set = Number.objects.none()\n        self.assertNumbersEqual(numbers.union(empty_set), [6, 7, 8, 9])\n\n\n", "    def test_union_with_distinct(self):\n        q1 = Number.objects.filter(num__lte=5).values('num')\n        q2 = Number.objects.filter(num__gt=5).values('num')\n        q3 = q1.union(q2, distinct=True).order_by('num')\n        self.assertNumbersEqual(q3, [i for i in range(10)])\n\n\n", "    def test_union_multi_table(self):\n        numbers1 = Number.objects.filter(num__lt=5)\n        numbers2 = Number.objects.filter(num__gte=5)\n        numbers_union = numbers1.union(numbers2)\n        self.assertNumbersEqual(numbers_union, [n for n in Number.objects.all()])\n", "    def test_union_distinct(self):\n        numbers = Number.objects.filter(num__gte=5)\n        self.assertNumbersEqual(numbers.union(numbers), [5, 6, 7, 8, 9], ordered=False)\n\n\n\n"], "sample_519": ["    def test_constrained_layout_test(self):\n        fig, ax = plt.subplots(2, 2, figsize=(6, 6))\n        ax[0, 0].set_title(\"Top Left\")\n        ax[0, 1].set_title(\"Top Right\")\n        ax[1, 0].set_title(\"Bottom Left\")\n        ax[1, 1].set_title(\"Bottom Right\")\n        plt.tight_layout()\n        plt.show()\n\n", "    def test_figure_constrained_layout_pads(monkeypatch):\n        fig = Figure()\n        with monkeypatch.context() as m:\n            m.setattr(rcParams, 'figure.constrained_layout.hspace', 0.2)\n            m.setattr(rcParams, 'figure.constrained_layout.wspace', 0.3)\n            m.setattr(rcParams, 'figure.constrained_layout.pad_inches', 0.1)\n            con_layout = ConstrainedLayoutEngine()\n            con_layout.set_info({\n                'w_pad': 0.5,\n                'h_pad': 0.6,\n                'wspace': 0.4,\n                'hspace': 0.3,\n            })\n            con_layout.execute(fig)\n            info = con_layout.get_info()\n\n        assert info['w_pad'] == 0.5\n        assert info['h_pad'] == 0.6\n        assert info['wspace'] == 0.4\n        assert info['hspace'] == 0.3\n        \n\n\n\n", "    def test_dpi(self):\n        fig = Figure(figsize=(4, 3), dpi=100)\n        fig.canvas.draw()\n        bbox = fig.canvas.get_window_extent(fig.bbox_inches='tight')\n        assert bbox.width / bbox.height == 4 / 3\n        assert bbox.width * fig.dpi == 400\n\n        fig = Figure(figsize=(4, 3))\n        fig.canvas.draw()\n        bbox = fig.canvas.get_window_extent(fig.bbox_inches='tight')\n        assert bbox.width / bbox.height == 4 / 3\n        assert bbox.width * fig.dpi == 300 # Default dpi\n\n", "    def test_constrained_layout_pads(self):\n        fig = Figure()\n        with fig.canvas.get_tk_widget().master.bind(\"<Configure>\", lambda event: None):\n            engine = ConstrainedLayoutEngine()\n            fig.set_layout_engine(engine)\n            engine.execute(fig)\n            info = engine.get_info()\n            assert info['w_pad'] is not None\n            assert info['h_pad'] is not None\n            assert info['wspace'] is not None\n            assert info['hspace'] is not None\n\n\n\n            fig.set_size_inches(10, 5)\n            engine.execute(fig)\n            info = engine.get_info()\n            assert info['w_pad'] is not None\n            assert info['h_pad'] is not None\n            assert info['wspace'] is not None\n            assert info['hspace'] is not None\n\n            fig.set_size_inches(5, 10)\n            engine.execute(fig)\n            info = engine.get_info()\n            assert info['w_pad'] is not None\n            assert info['h_pad'] is not None\n            assert info['wspace'] is not None\n            assert info['hspace'] is not None\n\n            fig.set_constrained_layout(True)\n            engine.execute(fig)\n\n            info = engine.get_info()\n            assert info['w_pad'] is not None\n            assert info['h_pad'] is not None\n            assert info['wspace'] is not None\n            assert info['hspace'] is not None\n\n\n        \n", "    def test_constrained_layout_execute(self):\n        fig, axes = plt.subplots(2, 2)\n        engine = ConstrainedLayoutEngine()\n\n        # Add some elements to the axes\n        axes[0, 0].set_title('Top Left')\n        axes[0, 1].legend()\n        axes[1, 0].set_ylabel('Y Label')\n        axes[1, 1].set_xlabel('X Label')\n\n        self.assertTrue(isinstance(fig.get_layout_engine(), None))\n        fig.set_layout_engine(engine)\n        rect = engine.execute(fig)\n        self.assertEqual(fig.get_layout_engine(), engine)\n        self.assertIsInstance(rect, dict)\n        self.assertIn('figure', rect)\n        self.assertIn('subplots', rect)\n        self.assertTrue(all(isinstance(v, dict) for v in rect['subplots'].values()))\n\n        # Test tight_layout with a legend\n        fig, axes = plt.subplots()\n        axes.legend()\n        fig.tight_layout()\n\n"], "sample_990": ["    def test_asech_rewrite_as_log():\n        x = Symbol('x')\n        assert asech(x)._eval_rewrite_as_log(x) == log(1/x + sqrt(1/x**2 + 1))\n", "    def test_acsch_complex():\n        z = S.ImaginaryUnit * 2\n        assert acsch(z) == I * pi / 6\n        z = S.ImaginaryUnit * (sqrt(6) - sqrt(2))\n        assert acsch(z) == -5*I*pi/12\n        z = S.ImaginaryUnit * (sqrt(2) - sqrt(6))\n        assert acsch(z) == 11*I*pi/12\n        z = S.ImaginaryUnit * (sqrt(2) + sqrt(2))\n        assert acsch(z) == I * pi / 3\n", "    def test_asech_complex():\n        from sympy import I\n        assert asech(I).is_real is False\n        assert asech(-I).is_real is False\n        assert asech(I*(sqrt(6) - sqrt(2))).is_real is False\n        assert asech(I*(2/sqrt(2 + sqrt(2)))).is_real is False\n\n\n    \n", "    def test_acsch_rewrite_as_log_special_cases():\n        x = S.ImaginaryUnit\n        result = acsch(x)._eval_rewrite_as_log(x)\n        assert result == -S.Pi / 2 * S.ImaginaryUnit\n", "    def test_asech_rewrite_as_log():\n        from sympy import log\n        assert asech(sqrt(3)/2).rewrite(log).equals(log(1/sqrt(3)/2 + sqrt(1/9 - 1)))\n        assert asech(sqrt(2)).rewrite(log).equals(log(1/sqrt(2) + sqrt(1/2 - 1)))\n        assert asech(2).rewrite(log).equals(log(1/2 + sqrt(1/4 - 1)))\n"], "sample_438": ["    def test_generic_foreign_key_field_mapping(self):\n        post1 = Post.objects.create(title=\"Test Post 1\")\n        question1 = Question.objects.create(post=post1, content=\"Test Question 1\")\n        answer1 = Answer.objects.create(question=question1, content=\"Test Answer 1\")\n\n        # Test that the content type and object ID are correctly set for the answer.\n        self.assertEqual(answer1.content_type.model, \"question\")\n        self.assertEqual(answer1.object_id, question1.pk)\n\n        # Test that the question can be retrieved using the generic foreign key.\n        retrieved_question = answer1.question\n        self.assertEqual(retrieved_question, question1)\n", "    def test_generic_foreign_key_with_related_name(self):\n        ContentType.objects.create(app_label=\"myapp\", model=\"post\")\n        ContentType.objects.create(app_label=\"myapp\", model=\"answer\")\n\n        post = Post.objects.create(title=\"My first post\")\n        answer = Answer.objects.create(text=\"This is a test answer\", content_type=ContentType.objects.get_for_model(Post), object_id=post.id)\n\n        self.assertEqual(answer.content_object, post)\n        answer.content_object = None\n        answer.save()\n        self.assertIsNone(Answer.objects.get(id=answer.id).content_object)\n\n        post_id = post.id\n        answer.delete()\n\n        with self.assertRaises(models.ObjectDoesNotExist):\n            Answer.objects.get(id=answer.id)\n\n", "    def test_generic_foreign_key_serialization(self):\n        \"\"\"\n        Regression test for #24490.\n        \n        Ensure that GenericForeignKeys can be serialized (e.g., with JSON)\n        without causing errors.\n        \"\"\"\n        obj = Post(\n            content_type=models.ContentType.objects.get_for_model(Question),\n            object_id=1,\n        )\n        obj.save()\n        obj_json = json.dumps(obj.__dict__)\n        obj2 = Post.objects.get(pk=obj.pk)\n        self.assertEqual(json.loads(obj_json), obj2.__dict__)\n\n\n", "    def test_generic_foreign_key_with_no_content_type(self):\n        with self.assertRaises(ValueError):\n            GenericForeignKey.get_related_object(None, None) \n\n\n\n", "    def test_related_object_access(self):\n        post = Post.objects.create(title=\"My Post\")\n        question = Question.objects.create(content=\"My Question\", post=post)\n        answer = Answer.objects.create(question=question, content=\"My Answer\")\n\n        self.assertEqual(answer.question.content, \"My Question\")\n        self.assertEqual(answer.question.post.title, \"My Post\")  \n        self.assertEqual(question.post.title, \"My Post\")\n\n\n\n        with self.assertRaises(AttributeError):\n            question.post.nonexistent_field\n\n\n"], "sample_1079": ["    def test_point_3d_intersection(self):\n        p1 = Point3D(1, 2, 3)\n        p2 = Point3D(4, 5, 6)\n        line = Line3D(p1, p2)\n        self.assertEqual(p1.intersection(line), [p1])\n        self.assertEqual(p2.intersection(line), [p2])\n        p3 = Point3D(1, 2, 4)\n        self.assertEqual(p3.intersection(line), [])\n\n\n", "    def test_point2d_intersection():\n        p1 = Point2D(1, 1)\n        p2 = Point2D(2, 2)\n        p3 = Point2D(3, 3)\n        p4 = Point2D(4, 4)\n\n        assert Point2D.intersection(p1, p2) == []\n        assert Point2D.intersection(p1, p3) == []\n\n        assert Point2D.intersection(p1, p4) == []\n        assert Point2D.intersection(p2, p3) == []\n\n\n\n", "    def test_projection(self):\n        p1 = Point2D(1, 1)\n        p2 = Point2D(2, 5)\n        p3 = p1.project(p2)\n        self.assertEqual(p3, Point2D(1.6666666666666666, 3.333333333333333))\n\n        p4 = Point3D(1, 1, 1)\n        p5 = Point3D(2, 5, 6)\n        p6 = p4.project(p5)\n        self.assertEqual(p6, Point3D(1.6666666666666666, 3.333333333333333, 2.6666666666666665)) \n\n", "    def test_direction_cosine(self):\n        p1 = Point3D(1, 2, 3)\n        p2 = Point3D(2, 3, 5)\n        dc = p1.direction_cosine(p2)\n        assert all(abs(dc[i] - sqrt(6)/6) < 1e-8 for i in range(3))\n    ", "def test_canberra_distance():\n    p1 = Point2D(1, 1)\n    p2 = Point2D(3, 3)\n    assert p1.canberra_distance(p2) == 1\n    p3 = Point2D(0, 0)\n    p4 = Point2D(3, 3)\n    assert p3.canberra_distance(p4) == 2\n    \n    with raises(ValueError):\n        p1.canberra_distance(p1)\n"], "sample_991": ["compilation error", "    def test_infinite_product_convergence(self):\n        from sympy.concrete.summations import Sum\n        \n        P1 = product(1/n**2, (n, 1, oo))\n        assert not P1.is_convergent()\n\n        P2 = product(cos(pi/n), (n, 1, oo))\n        assert P2.is_convergent()\n", "compilation error", "    def test_infinite_product_convergence():\n        from sympy.concrete.summations import Sum\n        P = product(x/(x + 1),(x,1,oo))\n        assert not P.is_convergent()\n        P = product(1/x**2, (x,1,oo))\n        assert not P.is_convergent()\n        P = product(cos(pi/x), (x,1,oo))\n        assert P.is_convergent()\n        P = product(exp(-x**2), (x,1,oo))\n        assert not P.is_convergent()\n", "compilation error"], "sample_212": ["    def test_session_middleware_process_response(self):\n        # Test that the session is saved correctly when modified.\n        middleware = SessionMiddleware()\n        request = HttpRequest()\n        request.session.modified = True\n        response = HttpResponse()\n        middleware.process_response(request, response)\n        self.assertIn(settings.SESSION_COOKIE_NAME, response.cookies)\n", "    def test_session_middleware_process_request(self):\n        # This test checks the behavior of SessionMiddleware.process_request.\n        request = HttpRequest()\n        session_middleware = SessionMiddleware()\n        session_middleware.process_request(request)\n        self.assertIsInstance(request.session, session_middleware.SessionStore)\n\n", "    def test_session_middleware_save_every_request(self):\n        # Test case for saving session data on every request.\n        response = HttpResponse()\n        request = HttpRequest()\n        request.session.modified = True\n        request.session.accessed = True\n\n        # This middleware call should save the session even though\n        # there's no explicit modification to the session in the request.\n        SessionMiddleware()(request, response)\n        self.assertTrue(request.session.save_calls)\n\n\n\n", "        def test_session_middleware_with_empty_session(self):\n            middleware = SessionMiddleware()\n            request = HttpRequest()\n            request.COOKIES[settings.SESSION_COOKIE_NAME] = 'some_session_key'\n            request.session = self.make_session_instance(\n                {'key1': 'value1'},\n            )\n            # Empty the session\n            request.session.clear()\n\n            response = HttpResponse()\n\n            middleware.process_response(request, response)\n\n            # Ensure the session cookie is deleted\n            self.assertNotIn(settings.SESSION_COOKIE_NAME, request.COOKIES)\n", "    def test_session_middleware_save_every_request_empty(self):\n        middleware = SessionMiddleware()\n        request = HttpRequest()\n        response = HttpResponse()\n        request.session = self.create_empty_session(middleware._SessionStore)\n        request.session.modified = True\n        settings.SESSION_SAVE_EVERY_REQUEST = True\n\n        middleware.process_response(request, response)\n\n        self.assertIn(settings.SESSION_COOKIE_NAME, response.get_cookie())\n\n\n\n"], "sample_86": ["    def test_lazy_object_copy(self):\n        class MyObject:\n                self.value = value\n\n            return MyObject(value)\n\n        lazy_obj = SimpleLazyObject(make_object)\n        self.assertEqual(lazy_obj._wrapped, None)\n        copy_obj = copy.copy(lazy_obj)\n        self.assertNotSame(lazy_obj, copy_obj)\n        self.assertEqual(lazy_obj._wrapped, None)\n        self.assertEqual(copy_obj._wrapped, None)\n        lazy_obj._setup()\n        copy_obj._setup()\n        self.assertEqual(lazy_obj._wrapped.value, 0)\n        self.assertEqual(copy_obj._wrapped.value, 0)\n", "    def test_lazy_text(self):\n        @lazystr\n            return f\"Hello, {name}!\"\n\n        self.assertEqual(my_lazy_text_func(\"World\"), \"Hello, World!\")\n", "    def test_lazy_object_creation(self):\n        class MyLazyObject(SimpleLazyObject):\n                self._wrapped_value = 42\n\n        lazy_obj = MyLazyObject(lambda: 42)\n        self.assertEqual(lazy_obj._wrapped, 42)\n\n        # Test copy and deepcopy\n        lazy_obj_copy = copy.copy(lazy_obj)\n        self.assertEqual(lazy_obj_copy._wrapped, 42)\n        lazy_obj_deepcopy = copy.deepcopy(lazy_obj)\n        self.assertEqual(lazy_obj_deepcopy._wrapped, 42)\n", "    def test_lazy_object_copy(self):\n        class MyObject:\n                self.value = value\n\n            return MyObject(value)\n\n        lazy_obj = SimpleLazyObject(factory)\n        copy_obj = copy.copy(lazy_obj)\n\n        self.assertNotEqual(lazy_obj, copy_obj)\n        self.assertIsInstance(lazy_obj._wrapped, MyObject)\n        self.assertIsInstance(copy_obj._wrapped, MyObject)\n        self.assertEqual(lazy_obj._wrapped.value, copy_obj._wrapped.value)\n", "    def test_lazy_text(self):\n        @keep_lazy_text\n            return text.upper()\n        \n        result = my_func(lazystr('hello'))\n        self.assertEqual(result, 'HELLO')\n"], "sample_169": ["    def test_json_field_serialization_with_custom_decoder(self):\n        \"\"\"\n        Test serialization of JSON fields with a custom decoder.\n        \"\"\"\n        obj = JSONModel(\n            primary_key_int=1,\n            json_field={'my_key': 'my_value', 'another_key': 123},\n        )\n        obj.save()\n        stream = serializers.get_format_serializer('xml')(obj)\n        deserialized_obj = serializers.get_format_deserializer('xml')(stream)\n        self.assertEqual(deserialized_obj.json_field, {'my_key': 'my_value', 'another_key': 123})\n        self.assertEqual(CustomJSONDecoder().decode(deserialized_obj.json_field), {'my_key': 'my_value', 'another_key': 123})\n\n\n\n", "    def test_serialization_nullable_json_field(self):\n        obj = NullableJSONModel.objects.create(\n            string_field='hello',\n            json_field=None\n        )\n        serialized = serializers.serialize('xml', [obj])\n        self.assertIn('<object model=\"testapp.NullableJSONModel\" pk=\"1\">', serialized)\n        self.assertNotIn('<field name=\"json_field\">', serialized)\n", "    def test_json_field_serialization(self):\n        obj = JSONModel(json_field={'a': 1, 'b': '2'})\n        serialized = serializers.serialize('xml', [obj])\n        deserialized = serializers.deserialize('xml', serialized)[0].object\n        self.assertEqual(deserialized.json_field, {'a': 1, 'b': '2'})\n", "    def test_key_transform_functions(self):\n        \"\"\"Test various key transform functions.\"\"\"\n        with connection.constraint_checks_disabled():\n            models.JSONField.default_encoder = DjangoJSONEncoder\n\n            # Create some sample data.\n            j = JSONModel.objects.create(data={'a': 1, 'b': 2, 'c': 3})\n            j2 = JSONModel.objects.create(data={'d': 4, 'e': 5})\n\n            # Test KeyTransform.\n            self.assertEqual(KeyTransform('a')(j), 1)\n\n            # Test KeyTextTransform.\n            self.assertEqual(KeyTextTransform('a')(j), '1')\n            self.assertEqual(KeyTextTransform('a')(j2), None)\n\n            # Test KeyTransformTextLookupMixin.\n            self.assertEqual(KeyTransformTextLookupMixin('a')(j), 1)\n            self.assertEqual(KeyTransformTextLookupMixin('a')(j2), None)\n\n            # Test using KeyTransformFactory.\n            factory = KeyTransformFactory(\n                default_encoder=DjangoJSONEncoder,\n                default_decoder=CustomJSONDecoder,\n            )\n            transform_result = factory('a')(j)\n            self.assertEqual(transform_result, 1)\n\n\n\n", "    def test_json_field_serialization(self):\n        obj = JSONModel(name='Test Object', data={\"key1\": \"value1\", \"key2\": 2})\n        obj.save()\n\n        serialized_obj = serializers.serialize('xml', [obj])\n\n        self.assertIn('<object model=\"test.jsonmodel\">', serialized_obj)\n        self.assertIn('<field name=\"name\" type=\"CharField\">Test Object</field>', serialized_obj)\n        self.assertIn('<field name=\"data\" type=\"JSONField\">', serialized_obj)\n        self.assertIn('<field name=\"data\" type=\"JSONField\">{\"key1\": \"value1\", \"key2\": 2}</field>', serialized_obj)\n\n\n\n"], "sample_521": ["    def test_poly_collection_2d_to_3d(self):\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection='3d')\n\n        verts = [(0, 0, 0), (1, 0, 0), (1, 1, 0), (0, 1, 0)]\n        codes = [\n            Path.MOVETO, Path.CURVE3, Path.CURVE3, Path.CURVE3, Path.CLOSEPOLY\n        ]\n        poly = PolyCollection(verts, facecolor='r', edgecolor='b')\n        poly_3d = poly_collection_2d_to_3d(poly, zs=1, zdir='z')\n        poly_3d.set_alpha(0.5)\n        poly_3d.draw(ax)\n\n        # Use a small offset to avoid issues with exact pixel matches.\n        # The offset makes the comparison slightly more robust to small\n        # variations in rendering due to differences in screen resolution,\n        # etc.\n        plt.savefig(self.get_test_image_filename('test_poly_collection_2d_to_3d'))\n\n\n\n", "    def test_poly3dcollection_simple(fig, ax):\n        verts = [\n            [(0, 0, 0), (1, 0, 0), (1, 1, 0), (0, 1, 0)],\n            [(0, 0, 1), (1, 0, 1), (1, 1, 1), (0, 1, 1)],\n        ]\n        poly3d = Poly3DCollection(verts, facecolor='b', edgecolor='r')\n        ax.add_collection(poly3d)\n        ax.set_xlim(0, 2)\n        ax.set_ylim(0, 2)\n        ax.set_zlim(0, 2)\n\n", "    def test_3d_projection(self, fig_manager):\n        fig = plt.figure()\n        ax = fig.add_subplot(projection='3d')\n        # Lines\n        line_verts = np.random.rand(10, 3)\n        line_codes = [Path.MOVETO, Path.CURVE3, Path.CURVE3, Path.CURVE3]\n        lc = LineCollection(\n            [mpath.Path(verts, codes=code) for verts, code in zip(\n                [line_verts], line_codes)],\n            colors='r', linewidths=2, alpha=0.7)\n        ax.add_collection3d(lc)\n\n        # Paths\n        path_verts = np.random.rand(10, 3)\n        path_codes = [Path.MOVETO, Path.CURVE3, Path.CURVE3, Path.CURVE3]\n        pc = PathCollection(\n            [mpath.Path(verts, codes=code) for verts, code in zip(\n                [path_verts], path_codes)],\n            facecolor='g', edgecolor='k', alpha=0.7)\n        ax.add_collection3d(pc)\n\n        # Patches\n        patches = [Circle((x, y, 0), radius=0.5) for x, y in np.random.rand(3, 2)]\n        ax.add_collection3d(PatchCollection(patches, facecolor='b', edgecolor='y'))\n\n        # Polygons\n        poly_verts = np.random.rand(10, 3)\n        poly_codes = np.array([Path.MOVETO, Path.CURVE3, Path.CURVE3, Path.CURVE3, Path.CLOSEPOLY])\n        poly_collection = PolyCollection(poly_verts, facecolors='m',", "def test_proj3d_transform_clip():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    x = np.array([1, 2, 3, 4])\n    y = np.array([1, 2, 3, 4])\n    z = np.array([1, 2, 3, 4])\n    vxs, vys, vzs, vis = proj3d._proj_transform_vec(\n        np.column_stack([x, y, z]), ax.get_transform())\n    assert len(vxs) == len(x)\n    assert len(vys) == len(y)\n    assert len(vzs) == len(z)\n    assert isinstance(vxs, np.ndarray)\n    plt.close(fig)\n", "    def test_poly3d_collection_projection(self):\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection='3d')\n\n        verts = [\n            [(0, 0, 0), (1, 0, 0), (1, 1, 0), (0, 1, 0)],\n            [(0, 0, 1), (1, 0, 1), (1, 1, 1), (0, 1, 1)],\n        ]\n        poly3d = Poly3DCollection(verts, facecolors='red', edgecolors='black')\n        ax.add_collection3d(poly3d)\n\n        ax.set_xlim3d(0, 2)\n        ax.set_ylim3d(0, 2)\n        ax.set_zlim3d(0, 2)\n        plt.draw()\n\n\n"], "sample_476": ["    def test_image_field_file_attributes(self):\n        instance = self.PersonModel(mugshot=self.file1)\n        instance.save()\n\n        image_file = instance.mugshot\n        self.assertIsInstance(image_file, self.File)\n        self.assertEqual(image_file.name, \"4x8.png\")\n        self.assertEqual(image_file.size, os.path.getsize(\n            os.path.join(os.path.dirname(__file__), \"4x8.png\")\n        ))\n        self.assertEqual(image_file.closed, False)\n        self.assertEqual(image_file.path, os.path.join(temp_storage_dir, \"4x8.png\"))\n        self.assertEqual(image_file.url, \"/media/4x8.png\")\n\n\n", "    def test_ImageFieldFile_file_is_correct(self):\n        instance = self.PersonModel(image=self.file1)\n        instance.save()\n\n        loaded_instance = self.PersonModel.objects.get(pk=instance.pk)\n        self.assertEqual(loaded_instance.image.name, \"4x8.png\")\n        self.assertIsInstance(loaded_instance.image, self.File)\n\n        # This should be a FieldFile instance, but we've wrapped it in\n        # our ImageFieldFile subclass. \n        self.assertIsInstance(loaded_instance.image, TestImageFieldFile) \n\n", "    def test_ImageField_image_width_and_height_fields_when_no_file_set(self):\n        instance = self.PersonModel.objects.create(\n            name=\"test person\",\n        )\n\n        self.check_dimensions(instance, None, None)\n\n        instance.mugshot = None\n        instance.save()\n\n        self.check_dimensions(instance, None, None)\n", "    def test_update_dimensions_fields_after_file_upload(self):\n        person = self.PersonModel.objects.create(mugshot=self.file1)\n        self.check_dimensions(person, 4, 8, field_name=\"mugshot\")\n\n        # Ensure dimensions are updated when file is again set to the same\n        # instance\n        person.mugshot = self.file1\n        person.save()\n        self.check_dimensions(person, 4, 8, field_name=\"mugshot\")\n\n\n", "    def test_image_field_pre_save_hook(self):\n        instance = self.PersonModel(name=\"Jane Doe\")\n        instance.mugshot.save(\"test.jpg\", self.file1)\n\n        # Check that the instance's mugshot attribute is populated\n        self.assertIsInstance(instance.mugshot, self.File)\n        self.assertEqual(instance.mugshot.name, \"test.jpg\")\n\n        # Simulate a save and verify the image dimensions are updated\n        instance.save()\n        self.check_dimensions(instance, 4, 8)\n\n"], "sample_829": ["    def test_sparse_input(self):\n        X_sparse = sparse.csr_matrix(iris.data)\n        pca = IncrementalPCA(n_components=2)\n        pca.fit(X_sparse)\n        X_dense = pca.transform(X_sparse)\n        pca_dense = PCA(n_components=2)\n        X_dense_ref = pca_dense.fit_transform(iris.data)\n        assert_array_almost_equal(X_dense, X_dense_ref)\n\n\n\n\n", "    def test_incremental_pca_sparse(self):\n        n_samples = 50\n        n_features = 20\n        n_components = 3\n\n        # Generate random sparse data\n        X_sparse = sparse.random(n_samples, n_features,\n                                 density=0.1, format='csr')\n\n        # Fit and transform with IncrementalPCA\n        ipca = IncrementalPCA(n_components=n_components,\n                              batch_size=10)\n        ipca.fit(X_sparse)\n        X_transformed = ipca.transform(X_sparse)\n\n        # Check if the transformed data has the correct shape\n        assert X_transformed.shape[1] == n_components\n\n        # Fit and transform with PCA for comparison\n        pca = PCA(n_components=n_components)\n        X_dense = X_sparse.toarray()\n        X_transformed_dense = pca.fit_transform(X_dense)\n\n        # Compare the transformed data\n        assert_array_almost_equal(X_transformed_dense,\n                                  X_transformed,\n                                  decimal=3)\n", "    def test_incremental_batch_size(self):\n        X = sparse.csr_matrix(np.random.rand(1000, 10))\n        ipca_auto = IncrementalPCA(n_components=5, batch_size=None)\n        ipca_explicit = IncrementalPCA(n_components=5, batch_size=100)\n\n        ipca_auto.fit(X)\n        ipca_explicit.fit(X)\n\n        assert_array_almost_equal(ipca_auto.components_, ipca_explicit.components_)\n", "    def test_partial_fit_sparse_n_components_none(self):\n        X = sparse.csr_matrix(np.random.rand(100, 5))\n        ipca = IncrementalPCA(n_components=None, batch_size=10)\n        ipca.fit_transform(X)\n        assert ipca.n_components_ is not None\n        assert ipca.n_components_ <= X.shape[1]\n", "    def test_sparse_partial_fit(self):\n        n_samples = 100\n        n_features = 10\n        X_sparse = sparse.csr_matrix(np.random.rand(n_samples, n_features))\n        ipca = IncrementalPCA(n_components=3, batch_size=10)\n        ipca.partial_fit(X_sparse)\n\n        X_dense = X_sparse.toarray()\n        pca = PCA(n_components=3)\n        pca.fit(X_dense)\n\n        assert_allclose_dense_sparse(ipca.transform(X_sparse),\n                                     pca.transform(X_dense))\n"], "sample_751": ["    def test_weight_boosting_classifier(self):\n        X, y = load_iris(return_X_y=True)\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=0.25, random_state=42\n        )\n        for estimator in [\n            AdaBoostClassifier(DecisionTreeClassifier(), n_estimators=10),\n            AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=10),\n            AdaBoostClassifier(n_estimators=10),\n        ]:\n            # Test with no sample weights\n            estimator.fit(X_train, y_train)\n            y_pred = estimator.predict(X_test)\n            assert_equal(\n                estimator.decision_function(X_test).shape, (len(X_test), 1)\n            )\n\n            # Test with sample weights\n            sample_weights = np.ones(len(y_train))\n            sample_weights[0] = 0.0\n            estimator.fit(X_train, y_train, sample_weight=sample_weights)\n            y_pred = estimator.predict(X_test)\n            assert_equal(\n                estimator.decision_function(X_test).shape, (len(X_test), 1)\n            )\n\n\n\n", "    def test_weight_boosting_classification(self):\n        X_train, X_test, y_train, y_test = train_test_split(\n            iris.data, iris.target, test_size=0.2, random_state=0\n        )\n\n        # Test with different weightings\n        for estimator in [AdaBoostClassifier(), DecisionTreeClassifier()]:\n\n            weights = np.array([0.1, 0.2, 0.3, 0.4, 0.1, 0.2, 0.3, 0.4])\n            estimator_weighted = weight_boosting(estimator, weights=weights,\n                                                  random_state=rng)\n            estimator_unweighted = weight_boosting(estimator,\n                                                    random_state=rng)\n\n            estimator_weighted.fit(X_train, y_train)\n            estimator_unweighted.fit(X_train, y_train)\n\n            # Check that predictions are different with weights\n            y_pred_weighted = estimator_weighted.predict(X_test)\n            y_pred_unweighted = estimator_unweighted.predict(X_test)\n            assert_array_not_equal(y_pred_weighted, y_pred_unweighted)\n\n\n", "    def test_boosting_with_sparse_data(self):\n        X_sparse = csc_matrix(X)\n        y_class_sparse = np.array(y_class)\n        y_regr_sparse = np.array(y_regr)\n        ada_sparse = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),\n            n_estimators=10)\n        ada_sparse.fit(X_sparse, y_class_sparse)\n        y_pred_sparse_class = ada_sparse.predict(X_sparse)\n        ada_sparse = AdaBoostRegressor(DecisionTreeRegressor(max_depth=1),\n            n_estimators=10)\n        ada_sparse.fit(X_sparse, y_regr_sparse)\n        y_pred_sparse_regr = ada_sparse.predict(X_sparse)\n        assert_array_equal(y_pred_sparse_class, ada_sparse.predict(X))\n        assert_array_almost_equal(y_pred_sparse_regr, ada_sparse.predict(X))\n\n\n\n", "    def test_weight_boosting_with_invalid_estimator(self):\n        with assert_raises(TypeError):\n            weight_boosting(\n                estimator=object(),\n                n_estimators=10,\n                learning_rate=1.0,\n                algorithm='SAMME.R',\n            )\n\n", "    def test_boosting_with_sparse_data(self):\n        n_samples = 1000\n        n_features = 50\n        X = csc_matrix(rng.randn(n_samples, n_features))\n        y = rng.randint(0, 2, size=n_samples)\n\n        clf = AdaBoostClassifier(\n            BaseEstimator(), n_estimators=100, random_state=0\n        )\n        clf.fit(X, y)\n\n        assert_array_equal(clf.predict(X), clf.predict(X.toarray()))\n"], "sample_606": ["    def test_unified_dim_sizes():\n        x = xr.DataArray(np.random.rand(3, 4, 5), dims=[\"t\", \"x\", \"y\"],\n                         chunks={\"t\": (1, 2), \"x\": (2, 2), \"y\": (5,)})\n        y = xr.DataArray(np.random.rand(3, 4, 5), dims=[\"t\", \"x\", \"y\"],\n                         chunks={\"t\": (2,), \"x\": (2, 2), \"y\": (5,)})\n        z = xr.DataArray(np.random.rand(3, 4, 5), dims=[\"t\", \"x\", \"y\"],\n                         chunks={\"t\": (1, 2), \"x\": (1, 3), \"y\": (5,)})\n\n        assert unified_dim_sizes(x) == {\"t\": 2, \"x\": 2, \"y\": 5}\n        assert unified_dim_sizes(y) == {\"t\": 2, \"x\": 2, \"y\": 5}\n        assert unified_dim_sizes(z) == {\"t\": 2, \"x\": 3, \"y\": 5}\n", "    def test_add_with_broadcasting(x, y, expected):\n        assert_identical(x.add(y), expected)\n\n\n\n", "    def test_max_example(array, func, expected):\n        assert_identical(func(array), expected)\n\n", "    def test_polyval(arrays, dims, result):\n        coord = xr.DataArray(np.array([0, 1, 2]), dims=\"x\")\n        coeffs = xr.DataArray(\n            np.array([[1, 2, 3]], dtype=np.float64), dims=(\"x\", \"degree\")\n        )\n        actual = polyval(coord, coeffs, degree_dim=\"degree\")\n        assert_identical(actual, result)\n\n\n\n", "    def test_apply_ufunc_multi_datasets(\n        dim: str,\n        input_data: list[tuple[xr.Dataset, ...]],\n        ufunc: callable,\n        output_dtype: type = None,\n        keep_attrs: bool = True,\n        *,\n        dask: Union[bool, str] = None,"], "sample_770": ["compilation error", "    def test_silhouette_score_with_sparse_data(self):\n        # Test with sparse data\n        X = datasets.make_classification(n_samples=100, n_features=20,\n                                          random_state=42)\n        X_sparse = csr_matrix(X)\n        labels = np.random.randint(0, 2, size=len(X))\n        assert_greater(silhouette_score(X_sparse, labels), 0)\n", "    def test_silhouette_score_samples(metric):\n        n_samples = 100\n        n_clusters = 3\n        X, _ = make_blobs(n_samples=n_samples, centers=n_clusters,\n                           cluster_std=1.5, random_state=42)\n        labels = np.random.randint(0, n_clusters, size=n_samples)\n\n        silhouette_avg = silhouette_score(X, labels, metric=metric)\n        silhouette_samples = silhouette_samples(X, labels, metric=metric)\n        assert_greater(silhouette_avg, 0.5,\n                       \"Silhouette score should be greater than 0.5\")\n        assert_equal(len(silhouette_samples), n_samples,\n                     \"Number of silhouette samples should match the number of samples\")\n", "compilation error", "compilation error"], "sample_805": ["    def test_mean_tweedie_deviance_p_1(self, y_true, y_pred):\n        result = mean_tweedie_deviance(y_true, y_pred)\n        assert result == pytest.approx(mean_poisson_deviance(y_true, y_pred))\n\n\n\n", "    def test_r2_score(y_true, y_pred, sample_weight, multioutput, expected_score):\n        score = r2_score(y_true, y_pred, sample_weight=sample_weight,\n                         multioutput=multioutput)\n        assert_allclose(score, expected_score)\n\n", "    def test_mean_tweedie_deviance_bounds(self):\n        # Test error handling for invalid p values and input data\n\n        # p < 0\n        y_true = np.array([1, 2, 3])\n        y_pred = np.array([2, 1, 0.5])\n        with pytest.raises(ValueError,\n                           match=\"Tweedie deviance is only defined for p<=0 and p>=1.\"):\n            mean_tweedie_deviance(y_true, y_pred, p=-1)\n        with pytest.raises(ValueError, match=\"strictly positive y_pred.\"):\n            mean_tweedie_deviance(y_true, y_pred, p=-1, sample_weight=1)\n        \n        # 0 < p < 1\n        with pytest.raises(ValueError, match=\"Tweedie deviance is only defined for p<=0 and p>=1.\"):\n            mean_tweedie_deviance(y_true, y_pred, p=0.5)\n        with pytest.raises(ValueError, match=\"Tweedie deviance is only defined for p<=0 and p>=1.\"):\n            mean_tweedie_deviance(y_true, y_pred, p=0.5, sample_weight=1)\n        \n        # p > 2\n        y_true = np.array([1, 2, 3])\n        y_pred = np.array([1.5, 1.5, 1.5])\n        with pytest.raises(ValueError, match=\"Tweedie deviance is only defined for p<=0 and p>=1.\"):\n            mean_tweedie_deviance(y_true, y_pred, p=3)\n        with pytest.raises(ValueError, match=\"Tweedie deviance is only defined for p<=0 and p>=1.\"):\n            mean_tweedie_deviance(y_true, y_pred, p=3, sample_weight=1)\n\n        # Invalid input data\n        y_true = np.array([1, 2, 3])\n        y_pred = np.array([2, 1, 0.5, ", "    def test_mean_tweedie_deviance_with_sample_weights(self):\n        y_true = np.array([2, 0, 1, 4])\n        y_pred = np.array([1, 0.5, 2., 2.5])\n\n        sample_weight = np.array([0.2, 0.5, 0.2, 0.1])\n        expected = 1.4260389558875875\n\n        deviance = mean_tweedie_deviance(y_true, y_pred, sample_weight=sample_weight, p=1)\n        assert_allclose(deviance, expected, rtol=1e-5)\n\n", "    def test_mean_tweedie_deviance_error_messages(self):\n        y_true = np.array([1, 2, 3])\n        y_pred = np.array([1.5, 1.8, 2.5])\n\n        with pytest.raises(ValueError,\n                         match=\"Mean Tweedie deviance error with p=0.5 can only be used on \"\n                               \"strictly positive y_pred.\"):\n            mean_tweedie_deviance(y_true, y_pred, p=0.5)\n\n        with pytest.raises(ValueError,\n                         match=\"Mean Tweedie deviance error with p=0.5 can only be used on \"\n                               \"strictly positive y_true and y_pred.\"):\n            mean_tweedie_deviance(y_true, y_pred, p=0.5, sample_weight=[0, 1, 2])\n        \n        with pytest.raises(ValueError,\n                         match=\"Mean Tweedie deviance error with p=1 can only be used on \"\n                               \"non-negative y_true and strictly positive y_pred.\"):\n            mean_tweedie_deviance(y_true, y_pred, p=1, sample_weight=[0, 1, 2])\n\n\n\n"], "sample_1093": ["    def test_Piecewise(self):\n        expr = Piecewise(\n            (x**2, x > 0),\n            (y**2, x <= 0)\n        )\n        code = pycode(expr, standard='python3')\n        self.assertEqual(code, 'numpy.select(x > 0, [x**2], [y**2])') \n", "    def test_matrix_solve(self):\n        A = MatrixSymbol('A', 2, 2)\n        b = MatrixSymbol('b', 2, 1)\n        x = MatrixSolve(A, b)\n        self.assertEqual(pycode(x), 'numpy.linalg.solve(A, b)')\n", "    def test_sparse_matrix(self):\n        i = [0, 1, 2]\n        j = [1, 0, 2]\n        data = [1, 2, 3]\n        sparse_matrix = SparseMatrix((i, j, data), shape=(3, 3))\n        code = pycode(sparse_matrix, printer=SciPyPrinter)\n        assert code == \"scipy.sparse.coo_matrix((3, [1, 2, 3]), ((0, 1), (1, 0), (2, 2)), shape=(3, 3))\"\n", "    def test_scipy_special(self):\n        p = SciPyPrinter()\n        for func, name in _known_functions_scipy_special.items():\n            expr = getattr(sympy.special, func)(x)\n            expected = f\"scipy.special.{name}({self._print(x)})\"\n            self.assertEqual(p.doprint(expr), expected)\n", "    def test_sympy_printer_matrices(self):\n        # Test SparseMatrix and ImmutableSparseMatrix\n        sparse_matrix = SparseMatrix(((1, 0), (0, 1)), {\n            (0, 0): 2,\n            (1, 1): 3\n        })\n        s = SymPyPrinter()\n        self.assertEqual(s.doprint(sparse_matrix), 'sympy.sparse.coo_matrix([(2, 0, 0), (3, 1, 1)], shape=(2, 2))')\n\n        # Test Identity Matrix on both integer and symbolic dimensions\n        i = Identity(2)\n        self.assertEqual(s.doprint(i), 'sympy.Matrix([ [1, 0], [0, 1] ])')\n\n        i_symbolic = Identity(x)\n        self.assertEqual(s.doprint(i_symbolic), 'sympy.Matrix([ [1, 0], [0, 1] ])')\n\n        # Test MatrixSolve\n        A = MatrixSymbol('A', 2, 2)\n        b = MatrixSymbol('b', 2, 1)\n        solution = MatrixSolve(A, b)\n        self.assertEqual(s.doprint(solution), 'sympy.MatrixSolve(A, b)')\n\n\n\n"], "sample_206": ["def test_file_field_save_form_data_clears_field(self):\n    document = Document(file=temp.NamedTemporaryFile())\n    document.save()  # Make sure the file is saved to the db\n    with self.assertNumQueries(1):\n        document.file = None\n        document.save()\n    self.assertIsNone(Document.objects.get(pk=document.pk).file)\n\n\n\n", "    def test_file_field_upload_to_with_callable(self):\n        \"\"\"\n        Test FileField 'upload_to' with callable argument.\n        \"\"\"\n        with self.subTest(model=Document):\n            with tempfile.TemporaryDirectory() as tmpdir:\n                self.assertEqual(Document.objects.count(), 0)\n                \n                # Test file field 'upload_to' with callable argument\n                upload_to = lambda instance, filename: f\"{instance.id}/uploads/{filename}\"\n                document1 = Document(title=\"Test 1\", content_file=File(open(os.path.join(tmpdir, \"test1.txt\"), \"wb\")))\n                document1.content.upload_to = upload_to\n                document1.save()\n\n                document2 = Document(title=\"Test 2\", content_file=File(open(os.path.join(tmpdir, \"test2.txt\"), \"wb\")))\n                document2.content.upload_to = upload_to\n                document2.save()\n\n                document1_path = os.path.join(tmpdir, \"1/uploads/test1.txt\")\n                document2_path = os.path.join(tmpdir, \"2/uploads/test2.txt\")\n                self.assertEqual(Document.objects.count(), 2)\n                self.assertTrue(os.path.exists(document1_path))\n                self.assertTrue(os.path.exists(document2_path))\n\n\n\n", "    def test_filefield_save_with_callable_upload_to(self):\n        with tempfile.TemporaryDirectory() as temp_dir:\n            upload_to = lambda instance, filename: os.path.join(temp_dir, 'uploads', filename)\n            Document.objects.create(\n                file=File(open('test_file.txt', 'rb')),\n                file_field=FileField(upload_to=upload_to)\n            )\n            files = Document.objects.get().file_field.storage.list_dir('')\n            self.assertEqual(files, ['uploads/test_file.txt'])\n        \n\n\n\n", "    def test_file_field_delete_and_save(self):\n        fileobj = temp.NamedTemporaryFile(\n            suffix='.txt', delete=False\n        )\n        fileobj.write(b\"Some test content\")\n        fileobj.close()\n\n        doc = Document(file=fileobj.name)\n        doc.save()\n        self.assertTrue(os.path.exists(doc.file.path))\n\n        doc.file.delete()\n        self.assertFalse(os.path.exists(doc.file.path))\n\n        doc.save()\n        self.assertFalse(os.path.exists(doc.file.path))\n", "    def test_file_field_delete(self):\n        doc = Document.objects.create(document=temp.NamedTemporaryFile())\n        self.assertTrue(doc.document.exists())\n\n        doc.document.delete()\n        self.assertFalse(doc.document.exists())\n        self.assertIsNone(doc.document)\n\n        doc.delete()\n        with self.assertRaises(IntegrityError):\n            Document.objects.get(pk=doc.pk)\n"], "sample_36": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_989": ["    def test_sympify_float():\n        assert sympify(1.23) == Rational(123, 100)\n        assert sympify(1.23456789e-7) == Rational(123456789, 1000000000)\n        assert sympify(1.0) == S.One\n        assert sympify(-1.0) == S.NegativeOne\n        assert sympify(Float('nan')) == nan\n        assert sympify(Float('inf')) == oo\n        assert sympify(Float('-inf')) == -oo\n        assert same_and_same_prec(sympify(Float('1.23', 5)), Rational(123, 100))\n        assert same_and_same_prec(sympify(Float('1.23', 10)), Rational(123, 100))\n", "    def test_sympify_gmpy2():\n        assert sympify(gmpy.mpz(42)) == S.Integer(42)\n        assert sympify(gmpy.mpq(1, 2)) == Rational(1, 2)\n        assert sympify(gmpy2.mpq(3, 4)*gmpy2.mpq(4,5)) == Rational(3,5)\n\n", "    def test_mod_inverse():\n        assert mod_inverse(2, 5) == 3\n        assert mod_inverse(3, 7) == 5\n        assert mod_inverse(2, 9) == 5\n        assert mod_inverse(5, 9) == 2\n        assert mod_inverse(10, 13) == 4\n        assert mod_inverse(13, 17) == 12\n        assert mod_inverse(15, 17) == 16\n        assert mod_inverse(11, 14) == 13\n        assert mod_inverse(17, 19) == 18\n        assert mod_inverse(7, 10) == 3\n        assert mod_inverse(9, 11) == 5\n\n", "    def test_sympify_not_int_with_float():\n        from sympy.core import sympify\n        raises(SympifyError, lambda: sympify(1.0))\n\n\n", "    def test_is_NumberSymbol():\n        assert isinstance(S.EmptySet, Number)\n        assert isinstance(S.Pi, NumberSymbol)\n\n\n"], "sample_278": ["    def test_select_related_descend_with_restricted_clause(self):\n        with isolate_apps(apps=(\n            \"django.db\",\n            \"my_app\",\n        )):\n            # Arrange\n            company = self.example_inc\n            restricted_clause = {\n                \"name\": True,\n                \"num_chairs\": True,\n                \"num_employees\": False,\n            }\n            load_fields = {\"name\": True, \"num_chairs\": True}\n\n            # Act\n            result = select_related_descend(\n                Company.objects.related_manager.employee,\n                restricted=True,\n                requested={\"name\": True},\n                load_fields=load_fields,\n                reverse=False,\n            )\n\n            # Assert\n            self.assertTrue(result)\n\n\n\n", "    def test_qs_with_annotate_and_filter_on_subquery(self):\n        subquery = (\n            Result.objects.filter(experiment__name='Test').values('value')\n        )\n        qs = Experiment.objects.annotate(\n            avg_result=Avg('results__value'),\n        ).filter(\n            Q(results__value__in=subquery)\n        )\n        with CaptureQueriesContext(connection) as c:\n            qs.all()\n        sql = c.captured_queries[0]\n        self.assertIn('AVG(DjangoDbTests_result_value)', sql)\n\n\n\n        #This test checks if the queryset with annotate and filter works as expected when the filter clause is based on a subquery.\n", "    def test_select_related_descend_pk_primary_key_field(self):\n        self.assertTrue(select_related_descend(\n            self.example_inc.ceo.pk, False, {}, set(), reverse=False))\n\n", "    def test_filtered_relation(self):\n        class FilteredCompanyManager(CompanyManager):\n                return super().get_queryset().filter(num_employees__gt=10)\n\n        Company.objects = FilteredCompanyManager()\n        filtered_relation = FilteredRelation(\"employee\", condition=Q(salary__gt=15))\n        filtered_query = Company.objects.annotate(\n            avg_salary=Avg('employee__salary'),\n        ).filter(filtered_relation)\n        sql, params = connection.queries[0]\n        self.assertIn(\n            \"FILTER (employee.salary > 15)\",\n            sql\n        )\n\n\n\n", "    def test_select_related_descend(self):\n        self.assertTrue(select_related_descend(Employee.objects.get(firstname='Joe').manager.field, False, {}, set(), reverse=True))\n        self.assertFalse(select_related_descend(Employee.objects.get(firstname='Joe').manager.field, False, {}, {'num_employees': }, reverse=True))\n        self.assertFalse(select_related_descend(Employee.objects.get(firstname='Joe').manager.field, True, {}, {'num_employees': }, reverse=True))\n        self.assertTrue(select_related_descend(Employee.objects.get(firstname='Joe').manager.field, False, {}, set(), reverse=False))\n        self.assertTrue(select_related_descend(Employee.objects.get(firstname='Joe').company.field, False, {}, set(), reverse=False))\n        self.assertFalse(select_related_descend(Employee.objects.get(firstname='Joe').company.field, False, {}, {'num_employees': }, reverse=False))\n        self.assertFalse(select_related_descend(Employee.objects.get(firstname='Joe').company.field, True, {}, {'num_employees': }, reverse=False))\n\n\n"], "sample_932": ["    def test_resolve_xref_template_param(self):\n        env = {}\n        from sphinx.domains.cpp import CPPDomain\n        domain = CPPDomain(None)\n        domain.clear_doc('test.rst')\n        docname = 'test.rst'\n        builder = None\n        from sphinx.tokens import Text, Literal\n\n        # Mock the 'refnode' for our test case\n        node = addnodes.pending_xref(\n            reftype='cpp:templateParam',\n            reftarget='template::Param',\n            refnodeattrs={'cpp:parent_key': '::'},\n        )\n        node.attributes = {}  # Clear existing attributes\n        node.set(\"cpp:parent_key\", \"::\")\n        node.set(\"cpp:type\", \"cpp:templateParam\")\n\n        # Mock the 'env.temp_data' for our test case\n        env.temp_data['cpp:parent_symbol'] = Symbol('test', None, None, None, None, None)\n        env.temp_data['cpp:parent_symbol'].add_declaration(\n            parse('templateParam', r'template::Param'), docname='test.rst'\n        )\n        # Mock the 'env.ref_context' for our test case\n        env.ref_context = {'cpp:parent_key': '::'}\n        result, objtype = domain.resolve_any_xref(env, 'test.rst', builder,\n                                                 'template::Param', node, node)\n        assert len(result) == 1\n        assert result[0][0] == 'cpp:templateParam'\n        assert isinstance(result[0][1], addnodes.desc)  # type: ignore\n\n", "    def check_enum_member(name, input, idDict, output=None, key=None):\n        if output is None:\n            output = input\n        check(name, input, idDict, output, key, None)\n", "    def test_template_param_resolution(self):\n        parser = DefinitionParser(\"T\", location=None, config=None)\n        ast = parser.parse_expression()\n        assert isinstance(ast, ASTTemplateParameter)\n        ast.location = None\n        symbol = Symbol(None, None, None, None, None, None)\n        symbol.add_declaration(ast, \"TestDoc\")\n\n        env = cppDomain.BuildEnvironment()\n        env.temp_data['cpp:parent_symbol'] = symbol\n        \n        node = addnodes.desc()\n        ast.describe_signature(node, \"markName\", env.temp_data['cpp:parent_symbol'], {})\n        assert node.astext() == 'T'\n", "    def test_template_params_in_qualified(setup):\n        _check(\"class\", r\"template <typename T> class MyTemplateClass { };\", {},\n               r\"template<typename T> class MyTemplateClass { };\")\n\n        _check(\"struct\", r\"template <typename T> struct MyTemplateStruct { };\", {},\n               r\"template<typename T> struct MyTemplateStruct { };\")\n\n        _check(\"function\", r\"template <typename T> void func(T t) { };\", {},\n               r\"template<typename T> void func(T t) { };\")\n\n        _check(\"enum\", r\"template <typename T> enum MyTemplateEnum { value = T(0) };\", {},\n               r\"template<typename T> enum MyTemplateEnum { value = T(0) };\")\n\n\n\n", "    def test_enum_member_ids(self):\n        idDict = {\n            1: \"Enum1_member1\",\n            2: \"Enum1_member2\"\n        }\n        check(\"enum\", \"{id_attr} : Enum1_member1\", idDict, \"{id_attr} : Enum1_member1\")\n        check(\"enum\", \"{id_attr} : Enum1_member2\", idDict, \"{id_attr} : Enum1_member2\")\n        check(\"enum\", \"{id_attr} : Enum1_member1 ; {id_attr} : Enum1_member2\",\n              idDict, \"{id_attr} : Enum1_member1;\"\n              \" {id_attr} : Enum1_member2\",\n              key=\"Enum1_member1\")\n\n\n\n"], "sample_1050": ["    def test_sparse_matrix(self):\n        mat = SparseMatrix([[1, 2], [3, 4]], (2, 2))\n        code = pycode(mat, printer=SciPyPrinter)\n        assert code == 'scipy.sparse.coo_matrix([[1, 2], [3, 4]], ((0, 0), (0, 1), (1, 0), (1, 1)), shape=(2, 2))'\n\n\n\n\n", "    def test_sparse_matrix(self):\n        sparse_matrix = SparseMatrix(((0, 0, 1), (1, 0, 2)), shape=(2, 2))\n        code = pycode(sparse_matrix, printer=SciPyPrinter)\n        assert code == 'scipy.sparse.coo_matrix([[1], [2]], ([0, 1], [0, 0]), shape=(2, 2))'\n\n\n\n", "    def test_mpmath_printing(self):\n        expr = (10**100) * (1 + pi) / (2 * pi)\n        code = pycode(expr, printer=MpmathPrinter())\n        assert 'mpmath.mpf(10**100)' in code\n\n", "    def test_sparse_matrix_printing(self):\n        mat = SparseMatrix(((1, 0, 2), (0, 1, 3)), (3, 3), dtype=int)\n        assert pycode(mat, printer=SciPyPrinter) == \"scipy.sparse.coo_matrix([[2, 0, 0], [0, 1, 0], [0, 0, 3]], shape=(3, 3))\"\n\n\n", "    def test_printing_matrix_operations(self):\n        m = MatrixSymbol('m', 2, 2)\n        n = MatrixSymbol('n', 2, 2)\n\n        a = SparseMatrix([[1, 2], [3, 4]])\n        b = SparseMatrix([[5, 6], [7, 8]])\n\n        # Test addition\n        expr = a + b\n        self.assertEqual(pycode(expr, printer=NumPyPrinter()), 'numpy.add(a, b)')\n\n        # Test subtraction\n        expr = a - b\n        self.assertEqual(pycode(expr, printer=NumPyPrinter()), 'numpy.subtract(a, b)')\n\n        # Test multiplication\n        expr = a * b\n        self.assertEqual(pycode(expr, printer=NumPyPrinter()), 'numpy.dot(a, b)')\n\n        # Test matrix exponentiation\n        expr = m**2\n        self.assertEqual(pycode(expr, printer=NumPyPrinter()), 'numpy.linalg.matrix_power(m, 2)')\n\n        # Test matrix inverse\n        expr = m**-1\n        self.assertEqual(pycode(expr, printer=NumPyPrinter()), 'numpy.linalg.inv(m)')\n\n        # Test dot product\n        expr = m.dot(n)\n        self.assertEqual(pycode(expr, printer=NumPyPrinter()), 'numpy.dot(m, n)')\n\n\n"], "sample_436": ["    def test_runserver_with_ipv6(self):\n        \"\"\"Tests that runserver can be run with an IPv6 address.\"\"\"\n        self.write_settings(\"settings.py\", apps=[\"django.contrib.auth\"])\n        with captured_stdout() as stdout:\n            subprocess.check_call(\n                [\"python\", \"-m\", \"django\", \"runserver\", \"::1:8000\"],\n                cwd=self.test_dir,\n            )\n        self.assertIn(\"Starting development server at http://[::1]:8000/\", stdout.getvalue())", "    def test_runserver_ipv6(self):\n        self.write_settings(\"mysite/settings.py\", apps=[\"django.contrib.auth\"], extra=\"ALLOWED_HOSTS = ['*']\")\n        self.write_settings(\n            \"mysite/tests/settings_tests.py\",\n            apps=[\"mysite.tests\"],\n            sdict={\"RUNSERVER_IPV6\": True},\n        )\n        try:\n            subprocess.check_call(\n                [\"python\", \"manage.py\", \"runserver\", \"::1:8000\"], cwd=self.test_dir,\n            )\n        except subprocess.CalledProcessError as e:\n            self.fail(f\"Runserver failed with error: {e}\")\n\n", "    def test_runserver_with_specified_addrport_and_ipv6(self):\n        self.write_settings(\n            \"settings.py\",\n            use_ipv6=True,\n            sdict={\"SERVER_ADDRESS\": \"::1\", \"SERVER_PORT\": \"8001\"},\n        )\n        with captured_stdout() as stdout:\n            execute_from_command_line(\n                [\"django-admin\", \"runserver\", \"::1:8001\"]\n            )\n        self.assertIn(\"Starting development server at http://::1:8001/\", stdout.getvalue())\n\n\n\n\n", "    def test_runserver_port_validation(self):\n        \"\"\"\n        Test that runserver properly validates the port number.\n        \"\"\"\n        with captured_stdout() as stdout:\n            with captured_stderr() as stderr:\n                call_command(\"runserver\", \"--port\", \"not_a_number\")\n        output = stdout.getvalue()\n        error = stderr.getvalue()\n        self.assertRegex(error, r\"'not_a_number' is not a valid port number.\")\n\n\n", "    def test_runserver_with_ipv6(self):\n        self.write_settings('settings.py', ipv6_address=\"::1\")\n        with captured_stdout() as stdout:\n            call_command('runserver', '0.0.0.0:8000')\n        output = stdout.getvalue()\n        self.assertIn('Starting development server at http://[::1]:8000/', output)\n"], "sample_703": ["    def test_parentheses():\n        assert evaluate(\"((True))\", lambda x: True) is True\n        assert evaluate(\"(True and False)\", lambda x: True) is False\n        assert evaluate(\"(True or False)\", lambda x: True) is True\n        assert evaluate(\"not (True)\", lambda x: True) is False\n", "compilation error", "    def test_paren_nesting():\n        assert evaluate(\"(ident)\", lambda x: x == \"foo\") == True\n        assert evaluate(\"not (ident)\", lambda x: x == \"bar\") == True\n        with pytest.raises(ParseError):\n            evaluate(\"((ident))\", lambda x: x == \"foo\")\n        with pytest.raises(ParseError):\n            evaluate(\"(ident) or (ident)\", lambda x: x == \"foo\") \n        with pytest.raises(ParseError):\n            evaluate(\"not (ident) and (ident)\", lambda x: x == \"foo\") \n", "    def test_parens():\n        assert evaluate(\"((a))\", lambda x: x == \"a\") is True\n        assert evaluate(\"((a) or (b))\", lambda x: x in (\"a\", \"b\")) is True\n        assert evaluate(\"((a) and (b))\", lambda x: x in (\"a\", \"b\")) is True\n        with pytest.raises(ParseError):\n            evaluate(\"((a) or (\", lambda x: x == \"a\")\n\n", "    def test_simple_identifier_match():\n        assert evaluate(\"ident\", lambda x: x == \"test\") == True\n        assert evaluate(\"ident\", lambda x: x == \"not_test\") == False\n"], "sample_1189": ["    def test_arg_with_cse():\n        from sympy.core.sympify import sympify\n\n        expr = (sympify(x**2) + 2*sympify(y)) * sympify(z)\n        func = lambdify((x, y, z), expr, modules=['numpy'])\n        x_val, y_val, z_val = 1, 2, 3\n        assert func(x_val, y_val, z_val) == 9 * 3\n", "    def test_lambdastr_with_indexing():\n        from sympy.concrete.summations import Sum\n\n        expr = x + y[0]\n        lstr = lambdastr((x, y), expr)\n        assert lstr == 'lambda x, y: (x + y[0])'\n\n        expr = x + y[z]\n        lstr = lambdastr((x, y, z), expr)\n        assert lstr == 'lambda x, y, z: (x + y[z])'\n\n        expr = Sum(sympify(f'x*y[{i}]'), (i, 0, 1))\n        lstr = lambdastr((x, y), expr)\n        assert lstr == 'lambda x, y: (x*y[0] + x*y[1])'\n", "    def test_lambdify_tensorflow(self):\n        from tensorflow.python.framework import tensor_shape\n\n        x = Dummy()\n\n        # Test lambda with tf tensor\n        tf_x = tensorflow.Variable(x)\n        l = lambdify(x, x**2, modules=['tensorflow'])\n        result = l(tf_x)\n        self.assertEqual(result.shape, tensor_shape.TensorShape([None]))\n", "    def test_diff():\n        f = lambdify((x, y), x**2 - y**2, dummify=True)\n        fx = lambdify((x, y), diff(x**2 - y**2, x), dummify=True)\n        fy = lambdify((x, y), diff(x**2 - y**2, y), dummify=True)\n        assert fx(2, 3) == 4\n        assert fy(2, 3) == -6\n", "    def my_multi_output_func(x, y):\n        return x**2, y**3\n"], "sample_323": ["    def test_migration_plan(self):\n        executor = MigrationExecutor(self.connection)\n        plan = executor.migration_plan(\n            [('migrations', '0001_initial')],\n            clean_start=True\n        )\n        self.assertEqual(len(plan), 1)\n        self.assertEqual(plan[0][0].name, \"0001_initial\")\n        self.assertFalse(plan[0][1])\n", "    def test_replacements(self):\n        with self.assertRaises(InvalidMigrationPlan):\n            executor = MigrationExecutor(connection, progress_callback=self.progress_callback)\n            executor.migrate(\n                [\"test_migrations\", ],\n                plan=[\n                    (\"migrations.0001_initial\", False),\n                    (\"migrations.0002_replace\", False),\n                ]\n            )\n\n", "    def test_migration_plan_invalid(self):\n        with self.assertRaises(InvalidMigrationPlan) as cm:\n            executor = MigrationExecutor(connection, progress_callback=lambda x: None)\n            executor.migration_plan([('migrations', '0001_initial')], clean_start=True)\n            executor.migrate([('migrations', '0001_initial')], plan=executor.migration_plan([('migrations', '0001_initial')], clean_start=True))\n        self.assertIn(\n            \"Migration plans with both forwards and backwards migrations are not supported\",\n            str(cm.exception),\n        )\n\n", "    def test_replace_migration_recording(self):\n        \"\"\"\n        Test that replacement migrations are correctly recorded as applied.\n        \"\"\"\n        with self.assertNumQueries(3):\n            executor = MigrationExecutor(self.connection)\n            executor.migrate([\"migrations\", \"auth\", \"contenttypes\"], plan=None, fake=True)\n        applied = MigrationRecorder().applied_migrations()\n        self.assertIn(\n            (\"migrations\", \"0001_initial\"), applied\n        )\n        self.assertIn((\n            \"migrations\", \"0002_replace_with_0003\"\n        ), applied)\n        self.assertIn((\"migrations\", \"0003_replacement\"), applied)\n", "    def test_clean_start_migration(self):\n        with self.assertNumQueries(2):\n            # First migration is applied, then unapplied.\n            with self.assertNumWarnings(0):\n                self.executor.migrate(\n                    [\"migrations.0001_initial\"],\n                    state=None,\n                    clean_start=True,\n                )\n            with self.assertNumQueries(2):\n                # Ensure the database state has been reset\n                self.executor.migrate(\n                    [\"migrations.0001_initial\"],\n                    state=None,\n                    clean_start=True,\n                )\n"], "sample_421": ["    def test_order_by_with_nulls(self):\n        qs = CaseTestModel.objects.all().annotate(\n            null_integer=Value(None), null_integer2=Value(None)\n        )\n        ordering = OrderBy(F(\"integer\"), nulls_first=True)\n        qs = qs.order_by(ordering)\n        self.assertSequenceEqual(qs.values_list(\"integer\", \"null_integer\", \"integer2\"), [\n            (1, None, None), (2, None, None), (3, None, None), (4, None, None)\n        ])\n\n", "    def test_order_by_nulls(self):\n        # test null handling in various combinations of ORDER BY and NULLS FIRST/LAST\n        cases = [\n            (\n                CaseTestModel.objects.all().order_by(\"integer2\"),\n                {\"integer2\": \"ASC\", \"NULLS\": None},\n            ),\n            (\n                CaseTestModel.objects.all().order_by(\"-integer2\"),\n                {\"integer2\": \"DESC\", \"NULLS\": None},\n            ),\n            (\n                CaseTestModel.objects.all().order_by(\"integer2\").nulls_first(),\n                {\"integer2\": \"ASC\", \"NULLS\": \"FIRST\"},\n            ),\n            (\n                CaseTestModel.objects.all().order_by(\"-integer2\").nulls_first(),\n                {\"integer2\": \"DESC\", \"NULLS\": \"FIRST\"},\n            ),\n            (\n                CaseTestModel.objects.all().order_by(\"integer2\").nulls_last(),\n                {\"integer2\": \"ASC\", \"NULLS\": \"LAST\"},\n            ),\n            (\n                CaseTestModel.objects.all().order_by(\"-integer2\").nulls_last(),\n                {\"integer2\": \"DESC\", \"NULLS\": \"LAST\"},\n            ),\n        ]\n        for queryset, (ordering, nulls) in cases:\n            with self.subTest(ordering=ordering, nulls=nulls):\n                sql = queryset.query.sql\n                params = queryset.query.params\n                expected_sql = (\n                    f\"SELECT {', '.join(self.group_by_fields)} FROM case_test_model ORDER BY integer2 {ordering} NULLS {nulls}\"\n                )\n                self.assertSQL(sql, expected_sql, params)\n\n\n\n", "    def test_case_expression_with_foreign_key(self):\n        # Test to ensure that a CASE expression using a foreign key\n        # to another model returns the correct results.\n        qs = CaseTestModel.objects.annotate(\n            case=Case(\n                When(integer__lt=3, then=F(\"integer2\") + 1),\n                When(integer__gte=3, then=F(\"integer2\") - 1),\n                default=0,\n            )\n        )\n\n        result = qs.values_list(\"case\", flat=True)\n\n        expected_results = [\n            2,\n            4,\n            3,\n            3,\n            2,\n            4,\n            2,\n            3,\n            2,\n            4,\n        ]\n\n        self.assertEqual(list(result), expected_results)\n\n", "compilation error", "    def test_case_expression_sql(self):\n        q = CaseTestModel.objects.annotate(\n            test_case=Case(\n                When(F(\"integer\") == 1, then=Value(1)),\n                When(F(\"integer\") == 2, then=Value(2)),\n                output_field=TextField(),\n            )\n        )\n        sql, params = q.query.sql_with_params()\n        self.assertIn(\n            \"CASE WHEN \\\"app_case_testmodel_integer\\\" = 1 THEN '1' \", sql\n        )\n        self.assertIn(\n            \"WHEN \\\"app_case_testmodel_integer\\\" = 2 THEN '2' \", sql\n        )\n        self.assertIn(\"END\", sql)\n\n        q = CaseTestModel.objects.annotate(\n            test_case=Case(\n                When(F(\"integer\") == 1, then=Value(1)),\n                When(F(\"integer\") == 2, then=Value(2)),\n                output_field=BinaryField(),\n            )\n        )\n        sql, params = q.query.sql_with_params()\n        self.assertIn(\n            \"CASE WHEN \\\"app_case_testmodel_integer\\\" = 1 THEN '1' \", sql\n        )\n        self.assertIn(\n            \"WHEN \\\"app_case_testmodel_integer\\\" = 2 THEN '2' \", sql\n        )\n        self.assertIn(\"END\", sql)\n"], "sample_110": ["    def test_window_expression_serialization(self):\n        qs = Happening.objects.annotate(\n            rolling_count=Window(Counter('events'),\n                                 order_by=Event.creation_time.desc(),\n                                 frame=ValueRange(start=1, end=2))\n        )\n        self.assert_pickles(qs)\n\n", "    def test_window_expression_pickling(self):\n        happening = Happening.objects.create(event_date=datetime.date(2023, 10, 26))\n        qs = Happening.objects.annotate(\n            running_total=Sum('event_date')\n            .window(partition_by='id', order_by='event_date', frame=RowRange(start=1, end=0))\n        )\n        self.assert_pickles(qs)\n\n\n\n", "    def test_window_function_expressions(self):\n        with self.assertNumQueries(1):\n            events = Event.objects.annotate(\n                count=Count('happening', distinct=True),\n                last_happening=Max('happening'),\n                latest_event=Window(\n                    ExpressionList(\n                        Max('timestamp'),\n                    ),\n                    order_by=OrderBy('timestamp', descending=True),\n                    frame=RowRange(start=1, end=0),\n                ).alias('latest_event'),\n            ).filter(happening__id=1)\n        self.assertEqual(len(events), 1)\n\n\n\n", "    def test_window_functions(self):\n        # Test Window functions with different partition_by, order_by, and frame.\n\n        group_qs = Group.objects.create(name='group_1')\n        happening_qs = Happening.objects.create(group=group_qs, date=datetime.date(2023, 10, 26))\n\n        # Test with partition_by, order_by, and frame\n        with self.subTest('partition_by, order_by, and frame'):\n            window_exp = Window(\n                expression=Happening.objects.count(),\n                partition_by=Group.objects.values_list('name', flat=True),\n                order_by=Happening.objects.order_by('date'),\n                frame=RowRange(start=1, end=2),\n                output_field=fields.IntegerField()\n            )\n            result = window_exp.as_sql(\n                compiler=mock.MagicMock(), connection=mock.MagicMock()\n            )\n\n            expected_sql = (\n                '(SELECT COUNT(*) OVER (PARTITION BY name ORDER BY date ROWS BETWEEN 1 PRECEDING AND 2 FOLLOWING)) AS '\n                'count'\n            )\n            self.assertEqual(result[0], expected_sql)\n\n            # Test with only partition_by\n        with self.subTest('partition_by'):\n            window_exp = Window(\n                expression=Happening.objects.count(),\n                partition_by=Group.objects.values_list('name', flat=True),\n                output_field=fields.IntegerField()\n            )\n            result = window_exp.as_sql(\n                compiler=mock.MagicMock(), connection=mock.MagicMock()\n            )\n\n            expected_sql = (\n                '(SELECT COUNT(*) OVER (PARTITION BY name)) AS count'\n            )\n            self.assertEqual(result[0], expected_sql)\n\n            # Test with only order_by\n        with self.subTest('order_by'):\n            window_exp = Window(\n                expression=Happening.objects.count(),\n                order_by=Happening.objects.order_by('date'),\n                output_field=fields.IntegerField()\n            )\n            result =", "    def test_window_expressions(self):\n        # Test basic window using a simple aggregate\n        qs = Happening.objects.annotate(\n            running_total=Window(\n                expression=Value('count(*)'),\n                partition_by=None,\n                order_by=Event.time.asc()\n            )\n        ).values('time', 'running_total')\n        self.assert_pickles(qs)\n\n\n"], "sample_445": ["    def test_timeuntil(self):\n        now = timezone.now()\n        future = now + self.oneday\n        self.assertEqual(timeuntil(future), \"1 day\")\n\n", "    def test_timesince_years_and_months(self):\n        now = datetime.datetime(2014, 3, 14)\n        expected = \"1 year, 1 month\"\n        self.assertEqual(timesince(self.t, now), expected)\n\n        now = datetime.datetime(2007, 8, 14)\n        expected = \"0 years, 0 months\"\n        self.assertEqual(timesince(self.t, now), expected)\n", "    def test_future_time(self):\n        future_time = self.t + self.oneminute\n        self.assertEqual(timesince(future_time), avoid_wrapping(npgettext_lazy(\"num\", \"0 minutes\", \"0 minutes\")))\n", "    def test_timesince_short_time(self):\n        now = timezone.now()\n        short_time = now - self.oneminute\n        self.assertEqual(timesince(short_time), avoid_wrapping(gettext(\"1 minute\")))\n        self.assertEqual(timeuntil(short_time), avoid_wrapping(gettext(\"1 minute\")))\n", "    def test_timeuntil_future_time(self):\n        now = timezone.now()\n        future_time = now + self.oneday\n        self.assertEqual(timeuntil(future_time, now), \"1 day\")\n\n"], "sample_842": ["    def test_kernel_pairwise(kernel):\n        if isinstance(kernel, PairwiseKernel):\n            k = kernel(X)\n            assert_array_almost_equal(k, pairwise_kernels(X, metric=kernel.metric, gamma=kernel.gamma))\n        else:\n            k = kernel(X)\n            # Test that the kernel is symmetric\n            assert_array_almost_equal(k, k.T)\n            # Test that the kernel is positive definite\n            eigenvalues = np.linalg.eigvals(k)\n            assert_array_equal(eigenvalues.shape, (k.shape[0],))\n            assert_array_all(eigenvalues >= 0)\n            # Test kernel with different length scales\n            if hasattr(kernel, 'length_scale'):\n                if isinstance(kernel.length_scale, list):\n                    new_kernel = clone(kernel)\n                    new_kernel.length_scale = [val * 2 for val in kernel.length_scale]\n                    new_k = new_kernel(X)\n                    assert_array_almost_equal(\n                        new_k, kernel(X) * 4,\n                        decimal=5) # Test scaling\n                else:\n                    new_kernel = clone(kernel)\n                    new_kernel.length_scale *= 2\n                    new_k = new_kernel(X)\n                    assert_array_almost_equal(\n                        new_k, kernel(X) * 4,\n                        decimal=5) # Test scaling\n            # Test kernel with different noise levels\n            if hasattr(kernel, 'noise_level'):\n                new_kernel = clone(kernel)\n                new_kernel.noise_level *= 2\n                new_k = new_kernel(X)\n                assert_array_almost_equal(\n                    new_k, kernel(X) + new_kernel.noise_level**2,\n                    decimal=5)\n", "    def test_pairwise_kernel_function(kernel):\n        k = clone(kernel)\n        y = np.random.random((3, 5))\n        k.fit(y)\n        k_values = k.kernel(y)\n        assert k_values.shape == (5, 5)\n        assert_array_almost_equal(k_values, k.kernel(y))\n\n        k_values_transposed = k.kernel(y.T)\n        assert k_values.shape == k_values_transposed.shape\n        assert_array_almost_equal(k_values, k_values_transposed)\n\n", "    def test_kernel_evaluation(kernel):\n        K = kernel(X)\n        assert K.shape == (X.shape[0], X.shape[0])\n        assert np.allclose(np.trace(K), np.sum(K))\n        # Check that the kernel function is symmetric\n        assert np.allclose(K, K.T)\n        if hasattr(kernel, 'get_params') and hasattr(kernel, 'set_params'):\n            kernel_clone = clone(kernel)\n            with np.errstate(divide='ignore'):\n                # Test parameter modification and re-evaluation of kernel\n                new_params = {'length_scale': 1.0}\n                kernel_clone.set_params(**new_params)\n                K_new = kernel_clone(X)\n                assert_array_almost_equal(K_new, kernel(X), decimal=5)\n\n            # Test kernel parameter bounds\n            if hasattr(kernel, 'length_scale_bounds'):\n                try:\n                    kernel.set_params(length_scale=2.5)\n                except ValueError:\n                    pass\n                else:\n                    raise AssertionError(\"No ValueError raised for out-of-bounds parameter\")\n\n            for param_name in kernel.get_params().keys():\n                if param_name != 'length_scale' and param_name != 'length_scale_bounds':\n                    try:\n                        kernel.set_params(**{param_name: None})\n                    except TypeError:\n                        pass\n                    else:\n                        raise AssertionError(\n                            f\"TypeError not raised for non-numeric parameter {param_name}\"\n                        )\n", "    def test_kernel_evaluation(kernel):\n        K_ = kernel(X)\n        assert K_.shape == (X.shape[0], X.shape[0])\n        assert np.allclose(K_.diagonal(), np.ones(K_.shape[0]))\n        if kernel.hyperparameter_names:\n            # Ensure that changing hyperparameters modifies the kernel\n            k1 = kernel(X)\n            kernel.set_params(**{\n                kernel.hyperparameter_names[0]: kernel.get_params()[\n                    kernel.hyperparameter_names[0]] * 2\n            })\n            k2 = kernel(X)\n            assert not np.allclose(k1, k2)\n", "    def test_kernel_constructor(kernel):\n        try:\n            kernel_ = clone(kernel)\n        except:\n            raise\n        assert kernel_ is not kernel\n        assert kernel_.__class__ == kernel.__class__\n\n\n\n"], "sample_1205": ["    def test_content_primitive(self):\n        R, x, y = ring(\"x, y\", ZZ)\n        f = x**2 + 2*x*y + y**2\n        c, p = f.content(), f.primitive()\n        assert c == 1\n        assert p == x**2 + 2*x*y + y**2\n\n        f = 2*x**2 + 4*x*y + 2*y**2\n        c, p = f.content(), f.primitive()\n        assert c == 2\n        assert p == x**2 + 2*x*y + y**2\n", "    def test_decompose_multivariate(self):\n        from sympy.polys.decompose import decompose_univariate\n\n        R, x, y = ring(\"x,y\", ZZ)\n        f = x**2 + 2*x*y + y**2\n        with raises(MultivariatePolynomialError):\n            decompose_univariate(f)\n", "    def test_cancel_nonprimitive_ground(self):\n        R, x, y = ring(\"x, y\", ZZ)\n        f = 2*x**2*y - 4*x*y**2\n        g = x**2 - 2*x*y + y**2\n        p, q = f.cancel(g)\n        self.assertEqual(p, 2*x*y - 4*y**2)\n        self.assertEqual(q, x - y)\n", "    def test_content_primitive(self):\n        R = ZZ['x', 'y'].clone()\n        f = R(2*x**2*y - 3*x*y**2 + 2)\n        cont, prim = f.content(), f.primitive()\n        self.assertEqual(cont, 1)\n        self.assertEqual(prim, 2*x**2*y - 3*x*y**2 + 2)\n\n        f = R(x**2 + 2*x*y + y**2)\n        cont, prim = f.content(), f.primitive()\n        self.assertEqual(cont, 1)\n        self.assertEqual(prim, x**2 + 2*x*y + y**2)\n", "    def test_mul_ground_non_scalars(self):\n        R = ring(\"x, y\", ZZ)\n        x, y = R.gens\n\n        p = x**2 + y**2\n\n        with raises(ZZ):\n            p.mul_ground(10)\n        with raises(ZZ):\n            p.mul_ground(x)\n        with raises(ZZ):\n            p.mul_ground(y)\n        \n        f = p.mul_ground(2)\n        self.assertEqual(f, 2*x**2 + 2*y**2)\n\n        f = (x + y).mul_ground(2)\n        self.assertEqual(f, 2*x + 2*y)\n\n\n\n"], "sample_224": ["    def test_duration_field(self):\n        \"\"\"Test that DurationField is correctly added to aggregates.\"\"\"\n        with self.subTest('Sum'):\n            result = Book.objects.annotate(\n                total_duration=Sum('publisher__duration')\n            ).values('total_duration')\n            self.assertEqual(list(result), [\n                {'total_duration': datetime.timedelta(days=1)},\n            ])\n\n        with self.subTest('Avg'):\n            result = Book.objects.annotate(\n                avg_duration=Avg('publisher__duration')\n            ).values('avg_duration')\n            self.assertEqual(list(result), [\n                {'avg_duration': datetime.timedelta(days=1.625)},\n            ])\n        \n\n", "    def test_durationfield_aggregates(self):\n        with self.assertNumQueries(1):\n            duration = Book.objects.aggregate(\n                average_duration=Avg('pubdate__year')\n            )['average_duration']\n            self.assertIsInstance(duration, float)\n", "    def test_case_when_with_otherwise(self):\n        with CaptureQueriesContext() as captured:\n            Avg(\n                Case(\n                    When(Book.rating__gt=4, then=Book.price),\n                    otherwise=0\n                )\n            ).annotate(\n                avg_price=Avg('price')\n            ).filter(rating__gt=3).order_by('title').values('avg_price').distinct()\n\n        self.assertEqual(captured.query_count, 1) \n\n\n\n        ", "    def test_case_when_expression(self):\n        with self.assertNumQueries(1):\n            result = Book.objects.annotate(\n                rating_diff=Case(\n                    When(rating__gt=4.0, then=Value(1)),\n                    When(rating__lt=3.0, then=Value(-1)),\n                    default=Value(0) \n                )\n            ).values('rating_diff')\n\n        self.assertEqual(list(result), [\n            {'rating_diff': 1}, {'rating_diff': -1}, \n            {'rating_diff': 0}, {'rating_diff': 0}, \n            {'rating_diff': 0}, {'rating_diff': 0},\n            {'rating_diff': 1}, {'rating_diff': 0}\n        ])\n\n", "    def test_aggregate_subquery(self):\n        with CaptureQueriesContext() as query_context:\n            avg_price_by_publisher = Book.objects.values('publisher').annotate(\n                avg_price=Avg('price')\n            ).order_by('publisher__name')\n            avg_price_by_publisher = avg_price_by_publisher.annotate(\n                related_avg_price=Subquery(\n                    Book.objects.filter(publisher=OuterRef('publisher')).values('price').annotate(avg_price=Avg('price')).values('avg_price')\n                )\n            ).values('publisher__name', 'avg_price', 'related_avg_price')\n\n        expected_query = (\n            \"SELECT DISTINCT \\\"app_book\".\"publisher_id\\\" AS \\\"publisher__id\\\", AVG(\\\"app_book\".\"price\")"], "sample_184": ["    def test_index_together_with_related_fields(self):\n        with override_settings(DATABASE_ROUTERS={'default': EmptyRouter()}):\n            class Person(models.Model):\n                name = models.CharField(max_length=100)\n                address = models.ForeignKey(\n                    'Address', on_delete=models.CASCADE, related_name='people'\n                )\n\n            class Address(models.Model):\n                street = models.CharField(max_length=200)\n                city = models.CharField(max_length=100)\n\n            with self.subTest('index_together'):\n                with self.assertRaises(Error) as cm:\n                    Person._meta.index_together = [\n                        ('name', 'address__city'),\n                    ]\n                self.assertEqual(str(cm.exception), \"'index_together' refers to a \"\n                                                   \"related field 'address__city' \"\n                                                   \"which is not local to model \"\n                                                   \"'Person'.\")\n\n\n\n", "    def test_index_together_multiple_fields(self):\n        with override_settings(\n            DATABASE_ROUTERS={'default': EmptyRouter()},\n        ):\n            class Author(models.Model):\n                name = models.CharField(max_length=100)\n                book = models.ForeignKey('Book', on_delete=models.CASCADE, related_name='authors')\n\n            class Book(models.Model):\n                title = models.CharField(max_length=200)\n                publication_year = models.IntegerField()\n                authors = models.ManyToManyField(Author, through='BookAuthor', related_name='books')\n\n            class BookAuthor(models.Model):\n                book = models.ForeignKey(Book, on_delete=models.CASCADE)\n                author = models.ForeignKey(Author, on_delete=models.CASCADE)\n                # The following index is the key for this test case\n                index_together = models.Index(fields=['book__title', 'author__name'], name='book_author_index')\n\n            models.IndexTogether(BookAuthor, ['book__title', 'author__name'])\n            self.assertEqual(BookAuthor._meta.indexes, [\n                Index(fields=['book__title', 'author__name'], name='book_author_index'),\n            ])\n            BookAuthor._meta.flush()\n", "    def test_index_together_fields_with_multiple_databases(self):\n        with override_settings(\n            DATABASE_ROUTERS={'my_router': 'invalid_models_tests.routers.EmptyRouter'},\n            DATABASES={'default': {'ENGINE': 'django.db.backends.sqlite3'},\n                       'other': {'ENGINE': 'django.db.backends.sqlite3'}}\n        ):\n            models.Model.objects.create(name='Test')\n\n            # Mock max_name_length for each database.\n\n            allowed_len, db_alias = get_max_column_name_length()\n            self.assertEqual(db_alias, 'default')\n            with override_settings(DATABASE_ROUTER='my_router'):\n                router = EmptyRouter()\n                router.allow_migrate_model('other', models.Model)\n                connections = connections\n\n                # Check that the index_together fields are correctly checked\n                # against each database's max column name length.\n                with self.assertRaises(Error) as cm:\n                    from invalid_models_tests import models\n                    models.Model._meta.check_constraints()\n                errors = cm.exception.error_messages\n                self.assertEqual(len(errors), 1)\n                self.assertIn(\"Autogenerated column name too long\", errors[0].msg)\n\n\n\n", "    def test_index_together_with_condition(self):\n        with override_settings(DATABASE_ROUTERS={'default': EmptyRouter(), 'other': EmptyRouter()}):\n            from invalid_models_tests.models import ConditionedIndexTogetherModel\n\n            # Test that the database returns a warning if condition is not supported\n            with self.assertLogs(level=Warning) as cm:\n                ConditionedIndexTogetherModel.objects.db_manager('default').all()\n            warnings = cm.output\n            self.assertTrue(\n                any(\n                    'Index together condition' in warning\n                    for warning in warnings\n                ),\n                'Expected warning about index together condition not being supported.'\n            )\n\n            # Test that the database returns a warning if condition is not supported\n            with self.assertLogs(level=Warning) as cm:\n                ConditionedIndexTogetherModel.objects.db_manager('other').all()\n            warnings = cm.output\n            self.assertTrue(\n                any(\n                    f'Index together condition' in warning\n                    for warning in warnings\n                ),\n                'Expected warning about index together condition not being supported.'\n            )\n\n\n\n            # Test that the database returns a warning if non-key columns\n            # are used in the index together condition.\n            with self.assertLogs(level=Warning) as cm:\n                ConditionedIndexTogetherModel.objects.db_manager('default').all()\n            warnings = cm.output\n            self.assertTrue(\n                any(\n                    f'Non-key column used' in warning\n                    for warning in warnings\n                ),\n                'Expected warning about non-key column in index together condition.'\n            )\n            \n            # Test that the database returns a warning if include is\n            # used in the index together condition.\n            with self.assertLogs(level=Warning) as cm:\n                ConditionedIndexTogetherModel.objects.db_manager('default').all()\n            warnings = cm.output\n            self.assertTrue(\n                any(\n                    f'include' in warning\n                    for warning in warnings\n                ),\n                'Expected warning about include being used in index together condition.'\n            )\n\n\n", "    def test_index_together_check(self):\n        class MyModel(models.Model):\n            field1 = models.CharField(max_length=10)\n            field2 = models.IntegerField()\n            index_together = (('field1', 'field2'),)\n\n        with override_settings(DATABASE_ROUTERS=[EmptyRouter()]):\n            models.signals.class_prepared.connect(\n                lambda sender, **kwargs: _check_lazy_references(\n                    sender, 'invalid_models_tests', [sender]\n                ),\n                weak=False,\n                dispatch_uid='test_lazy_references',\n            )\n            _check_model_field_checks(MyModel, get_max_column_name_length(), [])"], "sample_538": ["    def test_transformed_patch_path_recalculation(self):\n        fig, ax = plt.subplots()\n        patch = mpatches.Patch(\n            facecolor='red', edgecolor='black',\n            transform=mtransforms.Affine2D().rotate(30),\n            path=Path([(0, 0), (1, 0), (1, 1), (0, 1)],\n                      [Path.MOVETO, Path.CURVE3, Path.CURVE3, Path.CURVE3]),\n        )\n\n        transformed_patch = TransformedPatchPath(patch)\n\n        fig.add_artist(transformed_patch)\n\n        # Ensure the cached path doesn't change\n        ax.relim()\n        ax.autoscale_view()\n        fig.canvas.draw()\n        transformed_patch._transform.set_matrix(\n            np.array([[1, 0, 0], [0, 1, 1]]))\n\n        # Check that the patch hasn't been redrawn and is now transformed\n        ax.relim()\n        ax.autoscale_view()\n        fig.canvas.draw()\n\n\n        # Now update the underlying path, triggering a recalculation\n        patch.set_transform(mtransforms.Affine2D().rotate(45))\n        ax.relim()\n        ax.autoscale_view()\n        fig.canvas.draw()\n", "    def test_scaled_translation_identity(tmp_figure_factory):\n        fig, ax = tmp_figure_factory(figsize=(2, 2))\n        t1 = Affine2D().scale(0.5, 0.5)\n        t2 = ScaledTranslation(0.1, 0.2, t1)\n        t3 = t1 + t2\n        transformed_path = t3.transform_path_non_affine(Path([(0, 0), (1, 0), (1, 1), (0, 1)]))\n        for pt in transformed_path.vertices:\n            assert_almost_equal(pt[0], pt[0], decimal=4)\n            assert_almost_equal(pt[1], pt[1], decimal=4)\n    \n\n", "    def test_scaled_translation():\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n        transform = Affine2D().scale(2, 3)\n\n        trans = ScaledTranslation(1, 2, fig.dpi_scale_trans)\n\n        path = Path([[0, 0], [1, 0], [1, 1], [0, 1]])\n        transformed_path = trans.transform_path(path)\n        expected_path = transform.transform_path(\n            path.transformed(trans)\n        )\n        assert_allclose(\n            transformed_path.vertices, expected_path.vertices\n        )\n\n        transformed_points = trans.transform_non_affine(path.vertices)\n        expected_points = transform.transform_non_affine(\n            path.vertices\n        )\n        assert_allclose(transformed_points, expected_points)\n\n\n\n", "    def test_nonsingular_small_intervals():\n        for increasing in [True, False]:\n            for tiny in [1e-12, 1e-10, 1e-8]:\n                vmin, vmax = -1e-6, 1e-6\n                vmin, vmax = nonsingular(vmin, vmax, expander=1e-4, tiny=tiny, increasing=increasing)\n                assert (vmin, vmax) == (-1e-4, 1e-4)\n\n\n", "    def test_get_affine_simplification(transform_type, expected_output):\n        transform = transform_type()\n        assert isinstance(transform.get_affine(), expected_output.__class__)\n"], "sample_309": ["    def test_urlencode_multivalue(self):\n        query = MultiValueDict({'a': ['1', '2'], 'b': ['3', '4']})\n        result = urlencode(query)\n        self.assertEqual(result, 'a=1&a=2&b=3&b=4')\n        \n        # Make sure that empty strings in MultiValueDict are properly encoded.\n        query = MultiValueDict({'a': ['1', ''], 'b': ['3']})\n        result = urlencode(query)\n        self.assertEqual(result, 'a=1&a=&b=3')\n", "    def test_urlencode_multivalue_dict(self):\n        query_dict = MultiValueDict({\n            'key1': ['value1', 'value2'],\n            'key2': ['value3', 'value4']\n        })\n        encoded_query = urlencode(query_dict)\n        expected_query = 'key1=value1&key1=value2&key2=value3&key2=value4'\n        self.assertEqual(encoded_query, expected_query)\n", "    def test_urlencode_none_key(self):\n        query = MultiValueDict({'a': None})\n        with self.assertRaisesMessage(TypeError, self.cannot_encode_none_msg):\n            urlencode(query)\n", "    def test_urlencode_multivalue(self):\n        data = MultiValueDict({\n            'key1': ['value1', 'value2'],\n            'key2': ['value3', 'value4'],\n            'key3': ['value5', ]\n        })\n        encoded_url = urlencode(data)\n        self.assertEqual(encoded_url, 'key1=value1&key1=value2&key2=value3&key2=value4&key3=value5')\n\n        encoded_url_doseq= urlencode(data, doseq=True)\n        self.assertEqual(encoded_url_doseq, 'key1=value1&key1=value2&key2=value3&key2=value4&key3=value5') \n\n\n\n", "    def test_urlencode_multivalue(self):\n        query = MultiValueDict({'k': ['v1', 'v2'], 'l': ['v3', 'v4', None, 'v5']})\n        encoded = urlencode(query, doseq=True)\n        self.assertEqual(encoded, 'k=v1&k=v2&l=v3&l=v4&l=v5')\n        with self.assertRaisesMessage(TypeError, self.cannot_encode_none_msg):\n            urlencode(query, doseq=False)  \n"], "sample_213": ["    def test_get_storage_class_from_callable(self):\n        # Check get_storage_class for callable storage configuration.\n        storage = callable_storage()\n        storage_class = get_storage_class(storage)\n        self.assertEqual(storage_class, callable_storage)\n", "    def test_get_storage_class_from_callable(self):\n        storage_class = get_storage_class(callable_storage)\n        self.assertIsInstance(storage_class, type)\n        self.assertEqual(storage_class, callable_storage)\n\n", "    def test_get_storage_class_from_callable(self):\n        storage = callable_storage()\n        self.assertEqual(get_storage_class(storage), FileSystemStorage)\n", "    def test_get_storage_class(self):\n        with override_settings(\n            DEFAULT_FILE_STORAGE='storages.backends.filesystem.FileSystemStorage'\n        ):\n            self.assertEqual(get_storage_class(), FileSystemStorage)\n\n        with override_settings(\n            DEFAULT_FILE_STORAGE='myapp.test.callable_storage'\n        ):\n            self.assertEqual(get_storage_class(), callable_storage)\n\n        with override_settings(\n            DEFAULT_FILE_STORAGE='django.core.files.storage.default_storage'\n        ):\n            self.assertEqual(get_storage_class(), default_storage)\n\n\n", "    def test_get_storage_class(self):\n        # Test that callable storage is correctly identified.\n        self.assertEqual(get_storage_class(callable_storage), callable_storage)\n\n        # Test that a storage class name is correctly resolved.\n        self.assertEqual(get_storage_class('django.core.files.storage.FileSystemStorage'), FileSystemStorage)\n\n        # Test that a non-existent storage class raises an error.\n        with self.assertRaises(ImportError):\n            get_storage_class('non_existent_storage')\n"], "sample_955": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_942": ["    def test_parse_annotation(self):\n        assert _parse_annotation('int') == ('int', None)\n        assert _parse_annotation('str') == ('str', None)\n        assert _parse_annotation('List[int]') == ('List', [('int', None)])\n        assert _parse_annotation('Dict[str, int]') == ('Dict', [('str', None), ('int', None)])\n        assert _parse_annotation('Optional[int]') == ('Optional', [('int', None)])\n        assert _parse_annotation('Union[int, str]') == ('Union', [('int', None), ('str', None)])\n        assert _parse_annotation('typing.List[int]') == ('typing.List', [('int', None)])\n        assert _parse_annotation('typing.Optional[int]') == ('typing.Optional', [('int', None)])\n        assert _parse_annotation('(int, str)') == (None, [('int', None), ('str', None)])\n\n\n\n\n", "    def test_parse_annotation(self):\n        \"\"\"Test parsing of annotations.\"\"\"\n        tests = [\n            ('None', 'None'),\n            ('int', 'int'),\n            ('str', 'str'),\n            ('List[int]', 'List[int]'),\n            ('List[str]', 'List[str]'),\n            ('Optional[int]', 'Optional[int]'),\n            ('Union[int, str]', 'Union[int, str]'),\n            ('typing.List[int]', 'typing.List[int]'),\n            ('typing.Optional[str]', 'typing.Optional[str]'),\n            ('typing.Union[int, float]', 'typing.Union[int, float]'),\n            ('int | str', 'int | str'),\n            ('(int, str)', '(int, str)'),\n            ('Dict[str, int]', 'Dict[str, int]'),\n        ]\n        for sig, expected in tests:\n            parsed = _parse_annotation(sig)\n            assert parsed == expected\n", "    def test_parse_annotation(self):\n        self.assertEqual(_parse_annotation('int'), 'int')\n        self.assertEqual(_parse_annotation('int: int'), 'int: int')\n        self.assertEqual(_parse_annotation('int: str | list'), 'int: str | list')\n        self.assertEqual(_parse_annotation('Optional[int]'), 'Optional[int]')\n        self.assertEqual(_parse_annotation('Union[str, int]'), 'Union[str, int]')\n\n", "    def test_parse_annotation(self):\n        assert _parse_annotation(\"Optional[int]\") == (\"Optional\", [\"int\"])\n        assert _parse_annotation(\"List[str]\") == (\"List\", [\"str\"])\n        assert _parse_annotation(\"Callable[[int, str], None]\") == (\"Callable\",\n                                                              [[\"int\", \"str\"], \"None\"])\n        assert _parse_annotation(\"Union[str, int]\") == (\"Union\", [\"str\", \"int\"])\n        assert _parse_annotation(\"Tuple[int, str]\") == (\"Tuple\", [[\"int\", \"str\"]])\n        assert _parse_annotation(\"None\") == (None, [])\n        assert _parse_annotation(\"bool\") == (\"bool\", [])\n\n", "    def test_parse_signature(sig, expected):\n        assert parse(sig) == expected\n"], "sample_58": ["    def test_add_prefix(self):\n        form = Person()\n        self.assertEqual(form.add_prefix('first_name'), 'first_name')\n        form = Person(prefix='person')\n        self.assertEqual(form.add_prefix('first_name'), 'person-first_name')\n\n\n", "    def test_hidden_fields(self):\n        form = Person()\n        hidden_fields = form.hidden_fields()\n        self.assertEqual(len(hidden_fields), 0)\n        form = PersonNew()\n        hidden_fields = form.hidden_fields()\n        self.assertEqual(len(hidden_fields), 3)\n        for field in hidden_fields:\n            self.assertTrue(field.is_hidden)\n", "    def test_order_fields(self):\n        form = Person()\n        form.order_fields(['last_name', 'first_name', 'birthday'])\n        self.assertEqual(list(form.fields), ['last_name', 'first_name', 'birthday'])\n\n        new_form = PersonNew()\n        new_form.order_fields(['birthday', 'first_name', 'last_name'])\n        self.assertEqual(list(new_form.fields), ['birthday', 'first_name', 'last_name'])\n", "    def test_form_default_renderer(self):\n        form = Person()\n        self.assertIsNotNone(form.renderer)\n        self.assertNotEqual(form.renderer, get_default_renderer())\n        self.assertIsInstance(form.renderer, type)  # Make sure it's an actual class, not just an instance.\n\n\n\n\n\n", "    def test_declarative_fields(self):\n        class MyForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n\n        self.assertEqual(MyForm.declared_fields, {'field1': <django.forms.fields.CharField object ...>, 'field2': <django.forms.fields.CharField object ...>})\n        self.assertEqual(MyForm().fields, {'field1': <django.forms.fields.CharField object ...>, 'field2': <django.forms.fields.CharField object ...>})\n\n"], "sample_1015": ["    def test_matrix_symbols(self):\n        A = MatrixSymbol('A', 3, 2)\n        mat = Matrix([[x**2, y], [sin(z), cos(x)], [exp(x), log(y)]])\n        code = ccode(mat, A, standard='C89')\n        expected = \"\"\"A[0, 0] = pow(x, 2);", "    def test_indexed_multidimensional(self):\n        from sympy.tensor import IndexedBase, Idx\n\n        len_x = 3\n        x = IndexedBase('x', shape=(len_x,))\n        i = Idx('i', len_x)\n        j = Idx('j', len_x)\n        expr = x[i, j] + x[i + 1, j]\n\n        code = ccode(expr, assign_to=x[i, j], contract=True, standard='C89')\n        expected = \"\"\"", "    def test_Cbrt_C99(self):\n        printer = C99CodePrinter()\n        self.assertEqual(printer._print_Cbrt(Pow(x, Rational(1, 3))), 'cbrt(x)')\n", "    def test_c99_complex_numbers(self):\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(category=SymPyDeprecationWarning)\n            from sympy.codegen.cnodes import restrict\n            complex_expr = 2 + 3*1j\n            c_code = ccode(complex_expr, standard='c99')\n            expected = '2.0 + 3.0i'\n            self.assertEqual(c_code.strip(), expected)\n", "    def test_struct(self):\n        from sympy.codegen.cstruct import cstruct\n        struct = cstruct('MyStruct', [\n            'int a',\n            'float b',\n            'double c'\n        ])\n        expected_code = \"\"\""], "sample_814": ["def test_gradient_boosting_regressor_predict_stages_on_sparse_data(\n        ):\n    # Test predict_stages on sparse data\n\n    # Create sparse data\n    X = csr_matrix([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2, 3])\n\n    # Create a dummy regressor\n    regressor = GradientBoostingRegressor(n_estimators=3)\n    regressor.fit(X, y)\n\n    # Predict stages\n    leaves = predict_stages(regressor, X)\n\n    # Assert that the shape is correct\n    assert leaves.shape == (3, 3)\n\n    # Assert that the values are within the expected range\n    for i in range(3):\n        assert 0 <= leaves[i, :] <= 2\n\n\n\n    # Test predict_stages on dense data\n\n\n\n\n    # Test predict_stages on sparse_csc_matrix\n    X = csc_matrix([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2, 3])\n\n    regressor = GradientBoostingRegressor(n_estimators=3)\n    regressor.fit(X, y)\n    \n    leaves = predict_stages(regressor, X)\n\n    assert (leaves == predict_stages(regressor, X.toarray())).all()\n\n\n", "    def test_regression_toy(presort, loss):\n        # Check regression on toy dataset.\n        reg = GradientBoostingRegressor(loss=loss, n_estimators=10,\n                                        random_state=1, presort=presort)\n        assert_raises(ValueError, reg.predict, T)\n        reg.fit(X, y)\n        assert_array_almost_equal(reg.predict(T), true_result)\n        assert_equal(10, len(reg.estimators_))\n        \n        # Check early stopping\n        reg = GradientBoostingRegressor(loss=loss, n_estimators=10,\n                                         random_state=1, presort=presort,\n                                         n_iter_no_change=3)\n        reg.fit(X, y)\n        assert_equal(reg.n_estimators_, 3)\n\n    \n    \n", "def test_gradient_boosting_regression(loss, n_estimators):\n    # Check regression on a toy dataset.\n    reg = GradientBoostingRegressor(loss=loss, n_estimators=n_estimators,\n                                     random_state=1)\n    X = np.random.rand(100, 10)\n    y = np.random.rand(100)\n    reg.fit(X, y)\n    y_pred = reg.predict(X)\n    assert_array_almost_equal(y_pred, reg.predict(X), 4)\n    assert_greater(reg.feature_importances_.sum(), 0)\n\n\n\n", "def check_regression_toy(presort):\n    # Check regression on a toy dataset.\n    reg = GradientBoostingRegressor(loss='ls', n_estimators=10,\n                                    random_state=1, presort=presort)\n\n    with pytest.raises(NotFittedError):\n        reg.predict(T)\n\n    reg.fit(X, y)\n    assert_array_almost_equal(reg.predict(T), [1., 1., 1.])\n    assert_equal(10, len(reg.estimators_))\n\n    assert_raises(ValueError, clone(reg), presort='asdasd')\n\n", "def check_regression_toy(presort, loss):\n    # Check regression on a toy dataset.\n    reg = GradientBoostingRegressor(loss=loss, n_estimators=10,\n                                      random_state=1, presort=presort)\n\n    assert_raises(ValueError, reg.predict, T)\n\n    reg.fit(X, y)\n    assert_array_almost_equal(reg.predict(T), true_result)\n    assert_equal(10, len(reg.estimators_))\n\n    deviance_decrease = (reg.train_score_[:-1] - reg.train_score_[1:])\n    assert np.any(deviance_decrease >= 0.0)\n\n    leaves = reg.apply(X)\n    assert_equal(leaves.shape, (6, 10, 1))\n"], "sample_589": ["    def test_interp_single_dim(\n        frac_nan, method, expected_nan_count, expected_values, da", "    def test_interp_nan_different_methods(da, frac_nan):\n        da, df = make_interpolate_example_data(shape=(10, 10), frac_nan=frac_nan)\n\n        # Test different interpolation methods\n        for method in [\"linear\", \"nearest\", \"zero\"]:\n            interp_da = da.interp_na(dim=\"time\", method=method, limit=2)\n            assert_array_equal(\n                interp_da.values, df.interpolate(method=method, limit=2).values\n            )\n", "compilation error", "    def test_interp_basic(da, frac_nan, non_uniform):\n        da, df = make_interpolate_example_data((10, 10), frac_nan)\n        interp_result = da.interp(\n            x=da.x, method=\"linear\", fill_value=np.nan\n        )\n        assert_array_equal(interp_result, da)\n        interp_result = da.interp(\n            x=np.arange(10), method=\"linear\", fill_value=np.nan\n        )\n        assert_array_equal(interp_result, da)\n\n\n\n        coords_original = {\"x\": pd.date_range(\"2000-01-01\", freq=\"D\", periods=10)}\n        interp_result = da.interp(\n            x=coords_original[\"x\"][::2], method=\"linear\", fill_value=np.nan\n        )\n        assert_array_equal(interp_result, da)\n\n        if non_uniform:\n            interp_result = da.interp(\n                x=coords_original[\"time\"][::2], method=\"linear\", fill_value=np.nan\n            )\n            assert np.isnan(interp_result).any()\n            assert_array_equal(interp_result, da)\n        else:\n            interp_result = da.interp(\n                x=coords_original[\"time\"][::2], method=\"linear\", fill_value=np.nan\n            )\n            assert np.isnan(interp_result).any()\n            assert_array_equal(interp_result, da)\n\n\n\n", "    def test_interp_nan_block_lengths(da):\n        da = da.copy()\n        da = da.where(da.notnull(), np.nan)\n        nan_block_lengths = _get_nan_block_lengths(da, \"time\")\n\n        assert_equal(nan_block_lengths.sizes, (da.time.size,))\n\n        # Check that nan_block_lengths are calculated correctly for a few\n        # example cases\n        # Case 1: All values are valid\n        da = xr.DataArray([1, 2, 3], dims=\"time\")\n        assert_equal(np.array(nan_block_lengths), np.array([0]))\n\n        # Case 2: Single NaN value\n        da = xr.DataArray([1, np.nan, 3], dims=\"time\")\n        assert_equal(np.array(nan_block_lengths), np.array([0, 1, 0]))\n\n        # Case 3: Multiple NaN values\n        da = xr.DataArray([1, 2, np.nan, np.nan, 3], dims=\"time\")\n        assert_equal(np.array(nan_block_lengths), np.array([0, 0, 2, 0]))\n\n\n\n"], "sample_1138": ["compilation error", "    def test_TR14():\n        assert TR14(cos(x)**2 * sin(x)**2) == (1/8)*sin(4*x)\n    ", "compilation error", "    def test_TRpower():\n        e = sin(x)**5\n        assert TRpower(e) == (Rational(16*sin(5*x) - 20*sin(3*x) + 5*sin(x))/32)\n        e = cos(x)**6\n        assert TRpower(e) == (Rational(32*cos(6*x) + 48*cos(4*x) + 12*cos(2*x) + 1)/64)\n        e = sin(x)**4 * cos(x)**2\n        assert TRpower(e) == (Rational(3*sin(2*x) + 4*sin(4*x) + sin(6*x))/8)\n", "compilation error"], "sample_876": ["    def test_mlp_multilabel(self, activation):\n        X, y = make_multilabel_classification(n_samples=100, n_features=5, n_classes=4, random_state=42)\n        clf = MLPClassifier(activation=activation, random_state=42, solver='adam', max_iter=1000)\n        clf.fit(X, y)\n        assert_array_equal(clf.classes_, np.arange(4))\n        assert_almost_equal(roc_auc_score(y, clf.predict_proba(X)), 1.0, decimal=3)\n\n\n\n", "    def test_partial_fit_with_weights(self):\n        X, y = make_classification(n_samples=100, random_state=42)\n        clf = MLPClassifier(random_state=42)\n\n        # Fit the model on the first half of the data\n        clf._fit(X[:50], y[:50])\n\n        # Get current weights\n        initial_weights = [np.sum(w) for w in clf.coefs_]\n\n        # Partial fit with data and weights\n        clf.partial_fit(X[50:100], y[50:100], sample_weight=np.ones(50))\n        \n        # Check if weights have been updated\n        final_weights = [np.sum(w) for w in clf.coefs_]\n        for i in range(len(initial_weights)):\n            assert final_weights[i] != initial_weights[i]\n\n\n\n", "    def test_mlp_classification_activation(self, activation):\n        for X, y in classification_datasets:\n            clf = MLPClassifier(activation=activation, random_state=42)\n            clf.fit(X, y)\n            # Check predictions\n            y_pred = clf.predict(X)\n            assert_array_equal(y_pred.shape, y.shape)\n            assert_array_equal(y_pred.dtype, np.int32)\n", "    def test_partial_fit_multioutput(self):\n        X, y = make_multilabel_classification(\n            n_samples=100, n_classes=5, random_state=42\n        )\n        clf = MLPClassifier(\n            random_state=42,\n            n_iter=5,\n            solver='adam',\n            max_iter=5,\n            n_jobs=1,\n            verbose=False,\n            early_stopping=False,\n        )\n\n        for i in range(5):\n            clf.partial_fit(X[: (i + 1) * 20], y[: (i + 1) * 20])\n\n        y_pred = clf.predict(X)\n        assert_almost_equal(y_pred, y)\n\n\n\n", "    def test_mlpclassifier_multilabel(self):\n        X, y = make_multilabel_classification(\n            n_samples=100, n_features=5, n_classes=3, random_state=42\n        )\n        clf = MLPClassifier(n_iter=10, alpha=0.01, random_state=42)\n        clf.fit(X, y)\n        y_pred = clf.predict(X)\n        assert_array_equal(y_pred.shape, X.shape)\n        for i in range(X.shape[0]):\n            assert_array_equal(sorted(y[i]), sorted(y_pred[i]))\n"], "sample_808": ["    def test_behaviour_default(self):\n        X_train, X_test, y_train, y_test = train_test_split(\n            iris.data, iris.target, test_size=0.2\n        )\n        clf = IsolationForest(behaviour='new', random_state=0)\n        clf.fit(X_train)\n        assert_equal(clf.behaviour, 'new')\n        # Check anomaly score difference when using 'new' vs 'old'\n        scores_new = clf.decision_function(X_test)\n        clf = IsolationForest(behaviour='old', random_state=0)\n        clf.fit(X_train)\n        scores_old = clf.decision_function(X_test)\n        assert_allclose(scores_new, scores_old, rtol=1e-3) \n", "    def test_contamination_auto_behavior_old(self):\n        with assert_warns_message(\n                \"contamination parameter cannot be set to 'auto' when \"\n                \"behaviour == 'old'\"\n        ) as warning:\n            clf = IsolationForest(behaviour='old', contamination=\"auto\")\n            clf.fit(iris.data)\n        assert len(warning.warns) == 1\n\n\n\n", "    def test_contamination_auto(self):\n        # Test that the contamination parameter is correctly handled\n        # when set to \"auto\".\n        clf = IsolationForest(contamination='auto', n_estimators=100, random_state=42)\n        X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.2)\n        clf.fit(X_train)\n        y_pred = clf.predict(X_test)\n        assert_equal(clf.contamination, np.mean(y_pred == -1))\n\n\n\n", "compilation error", "    def test_contamination_auto(self):\n        with ignore_warnings(\n                category=FutureWarning,\n                message=r\"default contamination parameter.*\"\n        ):\n            # Test with contamination=\"auto\"\n            clf = IsolationForest(contamination=\"auto\", random_state=42)\n            X_train, X_test = train_test_split(iris.data, test_size=0.2, random_state=42)\n            clf.fit(X_train)\n\n            # Check that prediction probabilities are close to 0.5,\n            # as contamination is automatically set\n            # based on the outlier proportion in the training data.\n\n"], "sample_595": ["    def test_split(self, dtype):\n        da = xr.DataArray([\n            \"hello world\",\n            \"   python   \",\n            \"   xarray    \"\n        ], dtype=dtype)\n        result = da.str.split()\n        expected = xr.DataArray([\n            ['hello', 'world'],\n            ['python'],\n            ['xarray']\n        ], dtype=object)\n        assert_equal(result, expected)\n\n        result = da.str.split(\" \", expand=True)\n        expected = xr.DataArray([\n            [\"hello\", \"world\"],\n            [\"python\"],\n            [\"xarray\"]\n        ], dtype=object)\n        assert_equal(result, expected)\n\n\n", "    def test_encode_decode(self, dtype, encoding, errors, expected_encoding):\n        da = xr.DataArray(\n            [\"hello\", \"world\"],\n            dtype=dtype,\n            coords=[(\"time\", [1, 2])],\n        )\n        encoded = da.str.encode(encoding, errors).astype(np.bytes_)\n        decoded = encoded.str.decode(encoding, errors)\n        assert_equal(decoded, da)\n        assert (\n            decoded.encoding == expected_encoding\n        ).all(), \"Encoding not as expected\"\n\n\n\n", "    def test_translate(dtype, monkeypatch):\n        da = xr.DataArray([\n            \"hello\",\n            \"world\",\n            \"xarray\",\n        ], dtype=dtype)\n\n        table = str.maketrans(\"aeiou\", \"12345\")\n\n        result = da.str.translate(table)\n\n        expected = xr.DataArray([\n            \"h2ll0\",\n            \"w0rld\",\n            \"x4rry1\",\n        ], dtype=dtype)\n\n        assert_equal(result, expected)\n\n", "    def test_replace_regex(self, dtype):\n        da = xr.DataArray([\n            \"hello world\",\n            \"hello world again\",\n            \"hello there\",\n        ], dtype=dtype)\n\n        # Test with a simple regex and string replacement\n        result = da.str.replace(r\"world\", \"X\", regex=True)\n        expected = xr.DataArray(\n            [\n                \"hello X\",\n                \"hello X again\",\n                \"hello there\",\n            ],\n            dtype=dtype,\n        )\n        assert_equal(result, expected)\n\n        # Test with a callable replacement\n            return match.group(0).upper()\n\n        result = da.str.replace(r\"world\", uppercase_replacement, regex=True)\n        expected = xr.DataArray(\n            [\n                \"hello WORLD\",\n                \"hello WORLD again\",\n                \"hello there\",\n            ],\n            dtype=dtype,\n        )\n        assert_equal(result, expected)\n", "    def test_translate(self, dtype):\n        da = xr.DataArray(\n            [\"hello\", \"world\"], dtype=dtype\n        )\n\n        # Test mapping all characters\n        table = str.maketrans(\n            \"abcdefghijklmnopqrstuvwxyz\", \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n        )\n\n        expected = xr.DataArray(\n            [\"HELLO\", \"WORLD\"], dtype=dtype\n        )\n        assert_equal(da.str.translate(table), expected)\n\n        # Test mapping specific characters\n        table = str.maketrans(\"aeiou\", \"12345\")\n        expected = xr.DataArray(\n            [\"h2ll0\", \"w0rl1\"], dtype=dtype\n        )\n        assert_equal(da.str.translate(table), expected)\n\n        # Test mapping None for deletion\n        table = str.maketrans(\"l\", None)\n        expected = xr.DataArray([\"helo\", \"w0rld\"], dtype=dtype)\n        assert_equal(da.str.translate(table), expected)\n\n\n"], "sample_334": ["    def test_ordering_fields(self):\n        form_data = {\"first_name\": \"John\", \"last_name\": \"Doe\", \"birthday\": \"1980-01-01\"}\n        form = Person(data=form_data)\n        self.assertEqual(form._bound_items(), [(\"\", form.fields[\"first_name\"]),\n                                              (\"\", form.fields[\"last_name\"]),\n                                              (\"\", form.fields[\"birthday\"])])\n\n        form = PersonNew(data=form_data)\n        self.assertHTMLEqual(form.as_table(),\n                             \"\"\"\n                             <tr></th><td>John</td></tr>\n                             <tr></th><td>Doe</td></tr>\n                             <tr></th><td>1980-01-01</td></tr>\n                             \"\"\")\n\n\n\n", "    def test_hidden_fields(self):\n        form = Person(data={'first_name': 'John', 'last_name': 'Doe'})\n        hidden_fields = form.hidden_fields()\n        self.assertEqual(len(hidden_fields), 0)\n\n        form_hidden = PersonNew(data={'first_name': 'John', 'last_name': 'Doe'})\n        hidden_fields = form_hidden.hidden_fields()\n        self.assertEqual(len(hidden_fields), 0)\n", "    def test_fields_ordering(self):\n        form_data = {'first_name': 'John', 'last_name': 'Doe', 'birthday': '1980-01-01'}\n        form = PersonNew(data=form_data)\n        \n        self.assertEqual(form.fields['first_name'].widget.attrs['id'], 'first_name_id')\n\n\n", "    def test_hidden_fields(self):\n        form = PersonNew()\n        hidden_fields = form.hidden_fields()\n        self.assertEmpty(hidden_fields)\n\n        # Simulate a form with hidden fields\n        class HiddenForm(Form):\n            hidden_field1 = HiddenInput()\n            hidden_field2 = MultipleHiddenInput()\n\n        hidden_form = HiddenForm()\n        hidden_fields = hidden_form.hidden_fields()\n        self.assertEqual(len(hidden_fields), 2)\n        self.assertEqual(hidden_fields[0].name, 'hidden_field1')\n        self.assertEqual(hidden_fields[1].name, 'hidden_field2')\n\n\n", "    def test_hidden_fields(self):\n        form = PersonNew(data={'first_name': 'John', 'last_name': 'Doe'})\n        hidden_fields = form.hidden_fields()\n        self.assertEqual(len(hidden_fields), 0)\n\n        form = PersonNew(data={'first_name': 'John', 'last_name': 'Doe'})\n        form.add_hidden_field('new_hidden_field', 'hidden_value')\n        hidden_fields = form.hidden_fields()\n        self.assertEqual(len(hidden_fields), 1)\n\n        self.assertEqual(hidden_fields[0].html_name, 'new_hidden_field')\n        self.assertHTMLEqual(str(hidden_fields[0]), '<input type=\"hidden\" name=\"new_hidden_field\" value=\"hidden_value\">')\n\n\n\n"], "sample_247": ["    def test_expression_wrapper_in_select(self):\n        q = Book.objects.annotate(\n            title_length=Length('name')\n        ).filter(title_length__gt=10)\n        result = list(q)\n        self.assertEqual(len(result), 3)  # Test if query returns the expected result.\n\n\n\n", "    def test_non_aggregate_annotation_ordering(self):\n        a = Author.objects.annotate(\n            num_books=Count('book', distinct=True)\n        ).order_by('-num_books')\n        self.assertEqual([obj.name for obj in a], ['Peter Norvig', 'Stuart Russell', 'Brad Dayley', 'Wesley J. Chun', 'Paul Bissex', 'Adrian Holovaty', 'Jacob Kaplan-Moss', 'James Bennett', 'Jeffrey Forcier'])\n\n        b = Book.objects.annotate(\n            author_count=Count('author')\n        ).order_by('-author_count')\n        self.assertEqual([obj.name for obj in b], ['Artificial Intelligence: A Modern Approach', 'Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp', 'The Definitive Guide to Django: Web Development Done Right', 'Sams Teach Yourself Django in 24 Hours', 'Practical Django Projects', 'Python Web Development with Django'])\n\n\n\n", "    def test_annotate_with_value(self):\n        from django.db.models import Value\n\n        result = Book.objects.annotate(\n            double_price=Value(2.0) * F('price')\n        ).filter(price__lte=Decimal('25.00')).values('double_price')\n        self.assertQuerysetEqual(\n            result,\n            ['({'double_price': Decimal('50.00')},)],\n        )\n\n\n\n", "    def test_annotation_select_with_distinct(self):\n        distinct_authors = Author.objects.annotate(\n            distinct_books=Count('book', distinct=True)\n        ).values('name', 'distinct_books')\n        self.assertEqual(\n            list(distinct_authors),\n            [\n                {'name': 'Adrian Holovaty', 'distinct_books': 2},\n                {'name': 'Jacob Kaplan-Moss', 'distinct_books': 1},\n                {'name': 'Brad Dayley', 'distinct_books': 1},\n                {'name': 'James Bennett', 'distinct_books': 1},\n                {'name': 'Jeffrey Forcier', 'distinct_books': 2},\n                {'name': 'Paul Bissex', 'distinct_books': 2},\n                {'name': 'Wesley J. Chun', 'distinct_books': 2},\n                {'name': 'Peter Norvig', 'distinct_books': 1},\n                {'name': 'Stuart Russell', 'distinct_books': 1},\n            ]\n        )\n\n\n", "    def test_using_expression_wrapper_on_non_aggregate_fields(self):\n        # Using ExpressionWrapper on fields that are not aggregated should return\n        # the wrapper itself, untouched.\n        q1 = Author.objects.annotate(\n            name_lower=Lower(ExpressionWrapper('name', output_field=CharField()))\n        ).all()\n        self.assertQueryCount(q1, 8)\n        self.assertEqual(\n            q1.query.annotations[0].output_field.__class__,\n            CharField\n        )\n        for author in q1:\n            self.assertTrue(isinstance(author.name_lower, str))\n\n        q2 = Author.objects.annotate(\n            age_int=ExpressionWrapper('age', output_field=IntegerField())\n        ).all()\n        self.assertQueryCount(q2, 8)\n        self.assertEqual(\n            q2.query.annotations[0].output_field.__class__,\n            IntegerField\n        )\n        for author in q2:\n            self.assertTrue(isinstance(author.age_int, int))\n\n\n"], "sample_997": ["    def test_convert_equals_signs_nested(self):\n        expr = parse_expr(\"((1 = 2) = 3)\", transformations=(convert_equals_signs,))\n        expected = Eq(Eq(2, 1), 3)\n        self.assertEqual(expr, expected) \n", "    def test_convert_equals_signs_nested(self):\n        s = \"(1=2)=False\"\n        result = parse_expr(s)\n        self.assertEqual(result, Eq(Eq(2, 1), False))\n", "    def test_implicit_multiplication_application_nested():\n        expr = \"x*y*z+w\"\n        expected = Mul(x, y, z) + w\n        result = parse_expr(expr, transformations=(standard_transformations +\n                                                   (implicit_multiplication_application,)))\n        assert result == expected\n    ", "    def test_implicit_multiplication_application_single_line(self):\n        expr = parse_expr(\"sin x*y\", transformations=(standard_transformations + (implicit_multiplication_application,)))\n        self.assertEqual(str(expr), 'sin(x * y)')\n\n\n\n", "    def test_repeated_decimals_issue_5466():\n        expr = parse_expr(\"1.2[3]\")\n        expected = Rational(123, 90)\n        assert expr == expected\n"], "sample_143": ["    def test_format_lazy(self):\n        with ignore_warnings(category=RemovedInDjango40Warning):\n            self.assertEqual(format_lazy('Hello, {name}!'), 'Hello, world!')\n            self.assertEqual(format_lazy('Hello, {name}!'), 'Hello,  !')\n            self.assertEqual(format_lazy('Hello, {name}!'), 'Hello, None!')\n            self.assertEqual(format_lazy('Hello, {name}!'), 'Hello, {name}!')\n            self.assertEqual(format_lazy('Hello, {}!'), 'Hello, !')\n            self.assertEqual(format_lazy('Hello, {name}!'), 'Hello, Alice!')\n", "    def test_unescape_entities(self):\n        with ignore_warnings(RemovedInDjango40Warning):\n            self.assertEqual(text.unescape_entities('&amp;'), '&')\n            self.assertEqual(text.unescape_entities('&lt;'), '<')\n            self.assertEqual(text.unescape_entities('&gt;'), '>')\n            self.assertEqual(text.unescape_entities('&quot;'), '\"')\n            self.assertEqual(text.unescape_entities('&apos;'), \"'\")\n            self.assertEqual(text.unescape_entities('&#39;'), \"'\")\n            self.assertEqual(text.unescape_entities('&#160;'), '\\u00a0')  # Non-breaking space\n            self.assertEqual(text.unescape_entities('&#x2013;'), '\\u2013')  # En-dash\n\n\n\n", "    def test_capfirst(self):\n        self.assertEqual(text.capfirst(\"\"), \"\")\n        self.assertEqual(text.capfirst(\"test\"), \"Test\")\n        self.assertEqual(text.capfirst(\"TEST\"), \"TEST\")\n        self.assertEqual(text.capfirst(\"aBcDe\"), \"aBcDe\")\n        self.assertEqual(text.capfirst(\"aBcDEf\"), \"aBcDEf\")\n        self.assertEqual(text.capfirst(\" 123 \"), \" 123 \")\n        self.assertEqual(text.capfirst(\"123 \"), \"123 \")\n", "    def test_slugify_with_unicode(self):\n        self.assertEqual(text.slugify('\u00e4\u00f6\u00fc'), 'a-o-u')\n", "        def test_smart_split(self):\n            self.assertEqual(list(text.smart_split(\"This is  \\\"a person's\\\" test. 'Another 'person\\\\'s' test.\")), [\n                'This', 'is', '\"a person\\\\\\'s\"', 'test.', \"'person\\\\'s'\", 'test.'\n            ])\n"], "sample_93": ["    def test_duration_field_aggregation(self):\n        with self.assertNumQueries(1) as cm:\n            duration_avg = Book.objects.filter(contact__age__gt=30).aggregate(\n                avg_duration=Avg('duration')\n            )['avg_duration']\n        self.assertIsInstance(duration_avg, datetime.timedelta)\n", "    def test_f_expression(self):\n        result = Book.objects.filter(\n            title__icontains='Django: Web Development Done Right'\n        ).annotate(\n            doubled_price=F('price') * 2\n        ).values('doubled_price')\n        self.assertQueryCount(result, 1)\n\n", "    def test_window_functions(self):\n        with self.assertNumQueries(2) as context:\n            # Test for RANK() window function\n            rank_query = Author.objects.annotate(rank=Func('age', function='RANK', order_by='age', distinct=True)).values('name', 'age', 'rank')\n            result = list(rank_query)\n            expected_ranks = [\n                {'name': 'Peter Norvig', 'age': 57, 'rank': 1},\n                {'name': 'Brad Dayley', 'age': 45, 'rank': 2},\n                {'name': 'Stuart Russell', 'age': 46, 'rank': 3},\n                {'name': 'Jeffrey Forcier', 'age': 37, 'rank': 4},\n                {'name': 'Adrian Holovaty', 'age': 34, 'rank': 5},\n                {'name': 'Jacob Kaplan-Moss', 'age': 35, 'rank': 6},\n                {'name': 'James Bennett', 'age': 29, 'rank': 7},\n                {'name': 'Paul Bissex', 'age': 29, 'rank': 7},\n                {'name': 'Wesley J. Chun', 'age': 25, 'rank': 9},\n            ]\n            self.assertEqual(result, expected_ranks)\n\n            # Test for DENSE_RANK() window function\n            dense_rank_query = Author.objects.annotate(\n                dense_rank=Func('age', function='DENSE_RANK', order_by='age', distinct=True)\n            ).values('name', 'age', 'dense_rank')\n            result = list(dense_rank_query)\n            expected_dense_ranks = [\n                {'name': 'Peter Norvig', 'age': 57, 'dense_rank': 1},\n                {'name': 'Brad Dayley', 'age': 45, 'dense_rank': 2},\n                {'name': 'Stuart Russell', 'age': 46, 'dense_rank': 3},\n                {'name': 'Jeffrey Forcier', 'age': 37, 'dense_rank': 4},\n                {'name': '", "    def test_func_expression(self):\n        class MySum(Func):\n            function = 'MySum'\n\n        result = Book.objects.annotate(\n            my_sum=MySum('pages', 'price')\n        ).values('my_sum')\n\n        self.assertQuerysetEqual(result, [\n            {'my_sum': 775.69},\n            {'my_sum': 528.09},\n            {'my_sum': 600.69},\n            {'my_sum': 1000.69},\n            {'my_sum': 1878.80},\n            {'my_sum': 1696.80},\n        ])\n\n        with self.assertNumQueries(1):\n            result = Book.objects.values('my_sum', 'price')\n            for row in result:\n                self.assertEqual(\n                    row['my_sum'], row['price'] + row['price'] * 0.8\n                )\n\n", "    def test_subquery_in_expression(self):\n        with self.assertNumQueries(1):\n            subquery = Author.objects.filter(age__gt=30).values('name')\n            authors = Author.objects.annotate(\n                names_from_subquery=Subquery(subquery)\n            ).all()\n\n        self.assertEqual(authors.count(), 5)\n"], "sample_515": ["def test_colorbar_extension_length():\n    \"\"\"Test colorbar extension lengths for different spacing and extend settings.\"\"\"\n    for spacing in ('uniform', 'proportional'):\n        fig = _colorbar_extension_length(spacing)\n        plt.show()\n", "def test_colorbar_pad(self):\n    fig, ax = plt.subplots()\n    for orientation in ('horizontal', 'vertical'):\n        for spacing in ('uniform', 'proportional'):\n            for pad in [0.01, 0.05, 0.1]:\n                cax = Colorbar(ax, orientation=orientation,\n                               spacing=spacing, pad=pad)\n                plt.show()\n", "def test_colorbar_extension_length(plt):\n    \"\"\"\n    Test that the colorbar extensions have the correct length.\n    \n    With spacing='uniform', the extension length should be proportional to\n    the 'auto', or explicitly specified 'extendfrac'. \n    \n    With spacing='proportional', the extension length should be\n    proportional to the range of the data.\n\n    \"\"\"\n    fig = _colorbar_extension_length(spacing='uniform')\n    fig.show()\n    fig = _colorbar_extension_length(spacing='proportional')\n    fig.show()\n\n\n", "def test_colorbar_extension_length(request):\n    spacing = ['uniform', 'proportional']\n    for s in spacing:\n        fig = _colorbar_extension_length(s)\n        plt.close(fig)\n\n", "def test_colorbar_extension_length_all( ):\n    fig = _colorbar_extension_length('uniform')\n    return fig\n\n"], "sample_227": ["    def test_admin_list_filter_empty_field(self):\n        for model_class in [Book, Department]:\n            with self.subTest(model=model_class.__name__):\n                model_admin_class = getattr(app_configs[0].models, model_class.__name__)\n                admin = model_admin_class(model_class)\n                request = self.request_factory.get('/')\n                with patch.object(admin, 'get_queryset', autospec=True) as mocked_queryset:\n                    mocked_queryset.return_value = model_class.objects.all()\n                    queryset = admin.get_queryset(request)\n                    filter_fields = admin.list_filter\n                    for filter_field in filter_fields:\n                        if isinstance(filter_field, EmptyFieldListFilter):\n                            filter_values = filter_field.choices(request, model_admin_class)\n                            self.assertEqual(len(filter_values), 1)\n                            self.assertEqual(filter_values[0][0], '')\n\n\n\n", "    def test_department_list_filter_lookup_with_non_string_value(self):\n        \"\"\"\n        Tests that the DepartmentListFilterLookupWithNonStringValue works\n        correctly with lookups that are not strings.\n        \"\"\"\n        admin = DepartmentFilterEmployeeAdmin(\n            model=Employee,\n            attrs={'ordering': ('name',)},\n        )\n        request = self.request_factory.get('/admin/employees/')\n        queryset = admin.get_queryset(request)\n\n        self.assertEqual(\n            list(DepartmentFilterEmployeeAdmin.list_filter),\n            [(\n                'department',\n                DepartmentListFilterLookupWithNonStringValue,\n            )],\n        )\n        for obj in queryset:\n            if obj.department.id == 1:\n                department_lookup_value = (obj.department.id, obj.department.code)\n                break\n        else:\n            raise AssertionError(\"Department lookup not found\")\n\n        with self.assertNumQueries(1):\n            filtered_queryset = admin.get_queryset(request).filter(\n                department__id=department_lookup_value[0]\n            )\n        self.assertEqual(filtered_queryset.count(), 1)\n        self.assertEqual(filtered_queryset.first().id, department_lookup_value[0])\n", "    def setUpTestData(cls):\n        super().setUpTestData()\n        cls.djangonaut_book.employee = cls.john\n        cls.bio_book.employee = cls.jack\n", "    def test_empty_list_filter_values(self):\n        response = self.client.get('/admin/book/')\n        self.assertEqual(response.status_code, 200)\n        filter_html = response.content.decode('utf-8')\n        self.assertIn('<option value=\"\">---</option>', filter_html)\n        self.assertIn('<option value=\"\"></option>', filter_html)\n        self.assertIn('<option value=\"\"></option>', filter_html)\n", "    def test_all_values_list_filter(self):\n        site.register(Book, BookAdmin)\n        with self.assertNumQueries(2):\n            response = self.client.get('/admin/books/')\n        self.assertContains(response, 'AllValuesFieldListFilter')\n\n\n\n"], "sample_404": ["    def test_template_filter_invalid_arguments(self):\n        template = Template(\"{{ variable|invalid_filter }}\")\n        engine = self._engine()\n        context = Context({'variable': 'Value'})\n        with self.assertRaises(TemplateSyntaxError) as cm:\n            template.render(context)\n        self.assertIn(\"unknown filter 'invalid_filter'\", str(cm.exception))\n\n\n\n", "    def test_compile_filter_no_name(self):\n        parser = Parser(\n            [\n                Token(TokenType.TEXT, \"variable\", 0, 0),\n                Token(TokenType.VARIABLE, \"variable\", 0, 0),\n            ],\n            libraries={},\n        )\n        with self.assertRaises(TemplateSyntaxError) as cm:\n            parser.compile_filter(\"variable\")\n        self.assertEqual(cm.exception.args[0], \"Could not parse some characters: 'variable|%s'\")\n\n\n\n", "    def test_template_syntax_error_on_unclosed_block_tag(self):\n        template = \"\"\"", "    def test_filter_expression_resolve(self):\n        c = Context({'a': 1, 'b': '2'})\n        engine = self._engine()\n        t = engine.from_string('{{ a|int:\"5\"|add:\"b\" }}')\n        self.assertEqual(t.render(c), \"7\")\n", "    def test_filter_invalid_args(self):\n        engine = self._engine()\n        template = engine.from_string(\"{{ 1|invalid_filter }}\" )\n        with self.assertRaises(TemplateSyntaxError):\n            template.render(Context({}))\n\n\n"], "sample_1153": ["    def test_periodic_argument_evalf():\n        from sympy.functions.complexes import polar_lift\n        z = Symbol('z')\n        assert periodic_argument(polar_lift(z), 2).evalf(4) == z.evalf(4)\n        assert periodic_argument(polar_lift(z)*exp_polar(2*I*pi), 2*pi).evalf(4) == z.evalf(4)\n        assert periodic_argument(polar_lift(z)*exp_polar(3*I*pi), 2*pi).evalf(4) == z.evalf(4) + pi\n        assert periodic_argument(polar_lift(z)*exp_polar(5*I*pi), 2*pi).evalf(4) == z.evalf(4) + 2*pi\n\n", "    def test_periodic_argument_period_infinity():\n        from sympy.abc import x\n        expr = exp_polar(5*I*pi)\n        assert periodic_argument(expr, oo) == 5*pi\n        assert periodic_argument(complexify(expr), oo) == 5*pi\n        assert periodic_argument(principal_branch(expr, oo), oo) == expr\n    ", "compilation error", "    def test_arg_argument_index_error(self):\n        arg(2)\n", "    def test_polar_lift_evalf():\n        for x in [I, 2, 2*I, -2*I, 2.5, -2.5, pi, -pi, 2 + 3*I]:\n            exp_x = exp_polar(x)\n            lift_x = polar_lift(x)\n            assert polar_lift(exp_x) == exp_x\n            assert lift_x.n() == exp_x.n()\n\n"], "sample_195": ["    def test_date_extract_sql_tzaware(self):\n        tzname = timezone.utc.tzname(timezone.now())\n        self.assertEqual(\n            self.ops.date_extract_sql('year', 'date_field', tzname),\n            \"django_date_extract('year', date_field, '%s', '%s')\" % (\n                tzname,\n                self.ops.connection.timezone_name\n            ),\n        )\n\n\n\n", "    def test_bulk_batch_size(self):\n        ops = BaseDatabaseOperations(connection=connection)\n        self.assertEqual(ops.bulk_batch_size(['id'], [1, 2]), 500)\n        self.assertEqual(ops.bulk_batch_size(['id', 'title'], [1, 2]), 250)\n        self.assertEqual(ops.bulk_batch_size(['id', 'title', 'publication_date'], [1, 2]), 166)\n\n", "    def test_datetime_cast_date_sql(self):\n        tzname = timezone.utc.tzname(timezone.now())\n        result = self.ops.datetime_cast_date_sql('timestamp_field', tzname)\n        self.assertEqual(result, 'django_datetime_cast_date(timestamp_field, \\'%s\\', \\'%s\\')' % (tzname, timezone.utc.tzname(timezone.now())))\n", "    def test_datetime_extract_sql(self):\n        with override_settings(USE_TZ=True):\n            tzname = timezone.utc.tzname()\n            ops = BaseDatabaseOperations(connection=connection)\n            self.assertEqual(\n                ops.datetime_extract_sql('year', 'some_field', tzname),\n                \"django_datetime_extract('year', some_field, '%s', '%s')\" % (tzname, timezone.utc.tzname()),\n            )\n            self.assertEqual(\n                ops.datetime_extract_sql('month', 'some_field', tzname),\n                \"django_datetime_extract('month', some_field, '%s', '%s')\" % (tzname, timezone.utc.tzname()),\n            )\n            self.assertEqual(\n                ops.datetime_extract_sql('day', 'some_field', tzname),\n                \"django_datetime_extract('day', some_field, '%s', '%s')\" % (tzname, timezone.utc.tzname()),\n            )\n            self.assertEqual(\n                ops.datetime_extract_sql('hour', 'some_field', tzname),\n                \"django_datetime_extract('hour', some_field, '%s', '%s')\" % (tzname, timezone.utc.tzname()),\n            )\n            self.assertEqual(\n                ops.datetime_extract_sql('minute', 'some_field', tzname),\n                \"django_datetime_extract('minute', some_field, '%s', '%s')\" % (tzname, timezone.utc.tzname()),\n            )\n            self.assertEqual(\n                ops.datetime_extract_sql('second', 'some_field', tzname),\n                \"django_datetime_extract('second', some_field, '%s', '%s')\" % (tzname, timezone.utc.tzname()),\n            )\n            self.assertEqual(\n                ops.datetime_extract_sql('microsecond', 'some_field', tzname),\n                \"django_datetime_extract('microsecond', some_field, '%s', '%s')\" % (tzname, timezone.utc.tzname()),\n            )\n\n", "    def test_subtract_temporals(self):\n        ops = BaseDatabaseOperations(connection=connection)\n        self.assertEqual(\n            ops.subtract_temporals('TimeField',\n            (Col('x'), []), (Col('y'), [])),\n            ('django_time_diff(x, y)', []),\n        )\n\n        self.assertEqual(\n            ops.subtract_temporals('DateTimeField',\n            (Col('x'), []), (Col('y'), [])),\n            ('django_timestamp_diff(x, y)', []),\n        )\n\n        ops.combine_expression('^')\n        self.assertEqual(ops.subtract_temporals('TimeField',\n        (Col('x'), []), (Col('y'), [])),\n        ('django_time_diff(x, y)', []),\n        )\n\n\n"], "sample_304": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1198": ["    def test_parse_function(self):\n        parser = MathematicaParser()\n        expr = parse_mathematica('f[x,y,z]', convert_to_sympy=False)\n        assert isinstance(expr, Function)\n        assert expr.func == 'f'\n        assert expr.args == (x, y, z)\n\n\n\n", "    def test_complex_expressions(self):\n        code = \"Sin[x] + 2*Cos[y] + Tan[w] + (x^2 + y^2)**(1/2)\"\n        expr = parse_mathematica(code)\n        assert expr == sin(x) + 2*cos(y) + tan(w) + sqrt(x**2 + y**2)\n\n\n\n", "    def test_parse_function(self):\n        parser = MathematicaParser()\n\n        # Test a simple function definition\n        code = \"f[x_] := x^2\"\n        expr = parser.parse(code)\n        assert expr == Lambda(x, x**2)\n\n        # Test a function with multiple arguments\n        code = \"g[x_, y_] := x + y\"\n        expr = parser.parse(code)\n        assert expr == Lambda(x, y, x + y)\n\n        # Test a function with default argument\n        code = \"h[x_, y_ := 0] := x * y\"\n        expr = parser.parse(code)\n        assert expr == Lambda(x, y=0, x*y)\n\n        # Test a nested function\n        code = \"f[x_] := g[x, 2]", "    def test_parse_function(self):\n        parser = MathematicaParser()\n        expr = parse_mathematica(\"#&[x]\")\n        assert expr == Lambda(Dummy('d_'), Dummy('d_')(x))\n\n", "    def test_parse_complex_functions(self):\n        test_expr = \"Sin[x] + Cos[2*x]**2 * Exp[-x]\"\n        result = parse_mathematica(test_expr)\n        expected_result = sin(x) + cos(2*x)**2 * exp(-x)\n        self.assertEqual(result, expected_result)\n"], "sample_748": ["def test_param_grid_validation():\n    for input, error_type, error_message in [\n            ({}, ValueError, \"Parameter grid is empty\"),\n            ({'foo': [0]}, ValueError, \"Parameter grid is empty\"),\n            ({'foo': range(0)}, ValueError, \"Parameter grid is empty\"),\n            ({'foo': [0, 1, 2]}, ValueError, \"All parameter values \"\n             \"must be lists, dicts, or strings (key='foo', value=[0, 1, 2])\"),\n            ({'foo': [0, 1], 'bar': {\"a\": 1, \"b\": 2}}, ValueError,\n             \"All parameter values must be lists, dicts, or strings \"\n             \"(key='bar', value={'a': 1, 'b': 2})\"),\n            ({'foo': [0, 1], 'bar': [np.array([0]), np.array([1])] },\n             ValueError, \"All parameter values must be lists, dicts, \"\n             \"or strings (key='bar', value=[array([0]), array([1])])\"),\n    ]:\n        with pytest.raises(error_type, match=re.escape(error_message)):\n            _check_param_grid(input)\n\n\n", "def test_fit_grid_point_with_estimator(estimator):\n    param_grid = {\"C\": [0.01, 1.0, 10.0], \"kernel\": [\"linear\", \"rbf\"]}\n    X, y = make_classification(n_samples=100, n_features=20, random_state=42)\n    best_score, best_params = fit_grid_point(estimator, param_grid, X, y)\n    assert isinstance(best_score, float)\n    assert isinstance(best_params, dict)\n\n\n", "    def test_randomized_search_errors(self, param_distributions, n_iter,\n                                     error_type, error_message):\n        with pytest.raises(error_type, match=error_message):\n            RandomizedSearchCV(MockClassifier(), param_distributions,\n                              n_iter)\n\n\n\n", "def test_parameter_grid_errors():\n    for input, error_type, error_message in [\n            # Test for empty parameter grid\n            ((), ValueError, \"Parameter grid cannot be empty\"),\n            ]:\n        with pytest.raises(error_type, match=re.escape(error_message)):\n            ParameterGrid(input)\n\n\n", "def test_grid_search_cv_invalid_input():\n    from sklearn.model_selection import GridSearchCV\n\n    # Test for invalid input parameters\n    with pytest.raises(TypeError):\n        GridSearchCV(estimator=LinearSVC(), param_grid=X, cv=3)\n    with pytest.raises(TypeError):\n        GridSearchCV(estimator=LinearSVC(), param_grid=[1, 2], cv=3)\n    with pytest.raises(TypeError):\n        GridSearchCV(estimator=LinearSVC(), param_grid={'foo': 1, 'bar': 2},\n                     cv=X)\n\n"], "sample_447": ["    def test_ordering(self):\n        # Testing ordering with different data types and combinations.\n        books = Book.objects.all().annotate(\n            age_sum=Sum(\"authors__age\"),\n            years_since_pub=ExtractYear(\"pubdate\")\n        ).order_by(\"age_sum\", year__desc, \"name\").values(\"name\", \"age_sum\", \"years_since_pub\")\n        self.assertQuerysetEqual(\n            books,\n            [\n                {'name': \"Artificial Intelligence: A Modern Approach\", 'age_sum': 102, 'years_since_pub': 1995},\n                {'name': \"Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp\", 'age_sum': 102, 'years_since_pub': 1991},\n                {'name': \"The Definitive Guide to Django: Web Development Done Right\", 'age_sum': 69, 'years_since_pub': 2007},\n                {'name': \"Sams Teach Yourself Django in 24 Hours\", 'age_sum': 69, 'years_since_pub': 2008},\n                {'name': \"Practical Django Projects\", 'age_sum': 69, 'years_since_pub': 2008},\n                {'name': \"Python Web Development with Django\", 'age_sum': 69, 'years_since_pub': 2008},\n            ],\n            ordered=True\n        )\n\n        books = Book.objects.all().with_expression(\n            Case(\n                When(price__gt=30, then=30, otherwise=0),\n                \"expensive_price\",\n            )\n        ).order_by(\"expensive_price\", \"price\")\n        self.assertQuerysetEqual(\n            books,\n            [\n                {'expensive_price': 30, 'price': Decimal('82.80')},\n                {'expensive_price': 30, 'price': Decimal('75.00')},\n                {'expensive_price': 30, 'price': Decimal('30.00')},\n                {'expensive_price': 0, 'price': Decimal('29.69')", "    def test_window_frame_start_end(self):\n        s = (\n            Subquery(\n                QuerySet().annotate(\n                    total_books=Count(\"id\"),\n                    start_date=Max(\"original_opening\"),\n                ).filter(\n                    original_opening__lt=datetime.datetime(2000, 1, 1),\n                )\n            )\n            .values(\"total_books\", \"start_date\")\n            .annotate(\n                frame_start=Window(\n                    expression=F(\"start_date\"),\n                    frame=RowRange(start=1),\n                ),\n                frame_end=Window(\n                    expression=F(\"start_date\"),\n                    frame=RowRange(end=-1),\n                ),\n            )\n        )\n\n        with self.assertNumQueries(1):\n            result = s.values(\"total_books\", \"start_date\", \"frame_start\", \"frame_end\").all()\n\n        self.assertEqual(len(result), 1)\n        self.assertEqual(result[0].total_books, 3)\n        self.assertEqual(result[0].start_date, datetime.datetime(1994, 4, 23, 9, 17, 42))\n        self.assertEqual(result[0].frame_start, datetime.datetime(1994, 4, 23, 9, 17, 42))\n        self.assertEqual(result[0].frame_end, datetime.datetime(1994, 4, 23, 9, 17, 42))\n\n\n\n", "compilation error", "    def test_window_function_partition_by_and_order_by(self):\n        # Simulate a situation where we need to calculate the average rating\n        # for each author, ordered by their age.\n        from django.db.models import Window, Avg\n\n        average_rating = (\n            Book.objects.annotate(\n                avg_rating=Avg(\"rating\"),\n            )\n            .window(\n                Window.partition_by(Author.objects.values(\"id\"), \"authors__id\"),\n                order_by=Author.objects.values(\"age\"),\n                frame=WindowFrame(start=0, end=1),\n            )\n            .values(\n                \"avg_rating\",\n                \"id\",\n            )\n        )\n        self.assertEqual(average_rating.count(), len(self.a1.all_books))\n\n\n", "    def test_extract_year(self):\n        class ExtractYearTestCase(TestCase):\n                date = datetime.date(2023, 10, 26)\n                result = ExtractYear()(date)\n                self.assertEqual(result, 2023)\n\n"], "sample_721": ["    def test_check_memory_no_memmap(self):\n        with NamedTemporaryFile(delete=False) as tmp:\n            tmp_file = tmp.name\n            # Test with a non-memory mapped file\n            memory = Memory(cachedir=tmp_file, verbose=0)\n            assert_equal(check_memory(memory), memory)\n\n            # Test with a string path\n            memory = tmp_file\n            assert_equal(check_memory(memory), Memory(cachedir=tmp_file, verbose=0))\n\n\n", "    def test_check_symmetric_sparse_matrix_not_symmetric(self):\n        data = np.array([[1, 2], [3, 4]])\n        row = np.array([0, 0, 1, 1])\n        col = np.array([1, 0, 0, 1])\n        sparse_matrix = sp.csr_matrix((data, (row, col)))\n        with assert_warns_message(\n                DataConversionWarning,\n                \"Array is not symmetric, and will be converted to symmetric by\"\n                \" average with its transpose.\"):\n            check_symmetric(sparse_matrix)\n", "    def test_check_memory_joblib(tmpdir):\n        \"\"\"Test check_memory with joblib.\"\"\"\n        temp_dir = str(tmpdir.mkdir(\"temp_check_memory\"))\n        with open(os.path.join(temp_dir, \"memory.txt\"), \"w\") as f:\n            f.write(\"dummy\")\n\n        joblib_memory = joblib.Memory(cachedir=temp_dir, verbose=0)\n        X = np.random.random((10, 10))\n        check_memory(joblib_memory)\n        assert joblib_memory.cache\n\n        # Test with string\n        check_memory(temp_dir)\n\n\n\n        # Test with a function\n        memory = joblib.Memory(cachedir=temp_dir, verbose=0)\n        @joblib.memory.cache\n            return a + b\n\n        check_memory(memory)\n        assert my_function.cache_name is not None and my_function.cache_name in memory.cache\n\n\n    ", "    def test_check_consistent_length_with_extra_array(self):\n        X = np.array([[1, 2], [3, 4]])\n        y = np.array([5, 6])\n        extra_array = np.array([7, 8])\n\n        with assert_raises(ValueError):\n            check_consistent_length(X, y, extra_array)\n\n", "    def test_check_X_y_sparse_multi_output(self, sparse_array):\n        X = sp.csc_matrix([[1, 2], [3, 4]]) if sparse_array else np.array([[1, 2], [3, 4]])\n        y = sp.csc_matrix([[5], [6]]) if sparse_array else np.array([[5], [6]])\n\n        X_converted, y_converted = check_X_y(X, y, multi_output=True, accept_sparse='csr')\n        assert isinstance(X_converted, (np.ndarray, sp.csr_matrix))\n        assert isinstance(y_converted, (np.ndarray, sp.csr_matrix))\n        assert_allclose_dense_sparse(X_converted, X)\n        assert_allclose_dense_sparse(y_converted, y)\n\n"], "sample_965": ["    def test_signature_from_str_varargs():\n        sig = \"def func(x, *args, **kwargs): pass\"\n        sig_object = inspect.signature_from_str(sig)\n        assert sig_object.parameters['args'].kind == Parameter.VAR_POSITIONAL\n        assert sig_object.parameters['kwargs'].kind == Parameter.VAR_KEYWORD\n        assert sig_object.return_annotation == Parameter.empty\n", "    def test_unwrap_partial(monkeypatch):\n            pass\n\n        @functools.partialmethod\n            return self + 1\n\n        @functools.partial(inner, 10)\n            pass\n\n        # Mock the partial method to return its wrapped function\n        monkeypatch.setattr(partial_method, '__wrapped__', inner)\n\n        assert inspect.unwrap_all(partial_method) is inner\n        assert inspect.unwrap_all(partial_func) is inner\n\n        # Test that unwrap_all stops at the wrapped level for partial\n        # objects and doesn't unwind further.\n        class MyClass:\n                self.value = value\n\n                return self.value + other.value\n\n        obj = MyClass(5)\n        wrapped = inspect.unwrap_all(partial_method(obj))\n        assert wrapped is inner\n", "    def test_is_builtin_class_method(self):\n        class MyInt(int):\n            pass\n\n        self.assertTrue(inspect.is_builtin_class_method(int, '__init__'))\n        self.assertFalse(inspect.is_builtin_class_method(MyInt, '__init__'))\n", "    def test_is_method_descriptor(self):\n        class Descriptor:\n                return 'attr'\n\n        class MyObject:\n                my_descriptor = Descriptor()\n                self.my_attribute = my_descriptor\n\n        obj = MyObject()\n        self.assertTrue(inspect.is_method_descriptor(obj.my_attribute))\n\n\n\n        # this should be a descriptor, not an instancemethod\n        class InstanceMethod:\n                return self\n\n        instance = MyObject()\n        self.assertFalse(inspect.is_method_descriptor(instance.my_attribute))\n\n\n\n", "    def test_signature_from_str_with_defaults():\n        signature = \"def func(a=1, b, c=3): pass\"\n        sig = inspect.signature_from_str(signature)\n        expected_params = [\n            Parameter(\"a\", Parameter.POSITIONAL_OR_KEYWORD, default=1),\n            Parameter(\"b\", Parameter.POSITIONAL_OR_KEYWORD),\n            Parameter(\"c\", Parameter.POSITIONAL_OR_KEYWORD, default=3),\n        ]\n        assert list(sig.parameters.values()) == expected_params\n"], "sample_448": ["    def test_unique_constraint_with_expressions(self):\n        with atomic():\n            Product.objects.create(name=\"Bar\", description=\"Foobar\")\n        with atomic():\n            with self.assertRaises(ValidationError):\n                Product.objects.create(name=\"Bar\", description=\"Foobar\")\n", "    def test_unique_constraint_with_condition(self):\n        with atomic():\n            Product.objects.create(name=\"Test Product\", price=10)\n            with self.assertRaises(ValidationError):\n                UniqueConstraintConditionProduct.objects.create(name=\"Test Product\", price=10)\n", "        def test_unique_constraint_validation(self):\n            with atomic():\n                product = Product.objects.create(name=\"Unique Product\")\n\n                with self.assertRaises(IntegrityError):\n                    Product.objects.create(name=\"Unique Product\")\n    \n                with self.assertRaises(ValidationError):\n                    Product.objects.create(id=product.id, name=\"Unique Product\")\n", "    def test_unique_constraint_with_expressions(self):\n        with atomic():\n            Product.objects.create(name=\"Unique Product\", description=\"Description\")\n            with self.assertRaises(IntegrityError) as context:\n                Product.objects.create(name=\"Unique Product\", description=\"Description\")\n            constraint_sql = get_constraints(\"django_product\")[1][\"constraint_name\"]\n            self.assertEqual(constraint_sql, \"unique_name_description\")\n\n\n\n", "    def test_unique_constraint_validation_with_expressions(self):\n        with atomic():\n            product = UniqueConstraintProduct.objects.create(name=\"Unique Product\")\n            with self.assertRaises(ValidationError):\n                UniqueConstraintProduct.objects.create(name=\"Unique Product\")\n\n\n\n        constraints = get_constraints('django_product')\n        self.assertEqual(len(constraints), 1)\n        constraint = constraints[0]\n        self.assertEqual(constraint['name'], 'unique_product_name_check')\n"], "sample_728": ["    def test_make_sparse_coded_signal(n_samples=100, n_features=10,\n                                      n_nonzero_coefs=5,\n                                      random_state=None):\n        X, y, D = make_sparse_coded_signal(\n            n_samples=n_samples, n_features=n_features,\n            n_nonzero_coefs=n_nonzero_coefs, random_state=random_state)\n        assert_array_almost_equal(X.shape, (n_samples, n_features))\n        assert_equal(y.shape, (n_samples,))\n        assert_array_almost_equal(np.sum(D) , n_nonzero_coefs * n_samples)\n\n\n\n", "    def test_make_biclusters_shape(self):\n        shape = (10, 10)\n        n_clusters = 3\n        result, rows, cols = make_biclusters(shape, n_clusters)\n        self.assertEqual(result.shape, shape)\n        self.assertEqual(rows.shape, (n_clusters, shape[0]))\n        self.assertEqual(cols.shape, (n_clusters, shape[1]))\n", "    def test_make_checkerboard_with_n_clusters_as_single_int():\n        X, rows, cols = make_checkerboard((10, 10), 4)\n        assert X.shape == (10, 10)\n        assert rows.shape == (4, 10)\n        assert cols.shape == (4, 10)\n\n\n\n", "    def test_make_checkerboard_shapes(self):\n        for shape in [(5, 5), (10, 10), (100, 100)]:\n            X, rows, cols = make_checkerboard(shape,\n                                             n_clusters=2,\n                                             shuffle=False)\n            assert X.shape == shape\n\n            assert rows.shape[0] == 2\n            assert cols.shape[0] == 2\n            for i in range(2):\n                assert np.all(rows[i].reshape(shape[0], -1) == (i == np.arange(shape[0])[:, None]))\n                assert np.all(cols[i].reshape(-1, shape[1]) == (i == np.arange(shape[1])[None, :]))\n\n\n\n", "    def test_make_biclusters_shape(self):\n        for shape in [(5, 5), (10, 10), (10, 5), (5, 10)]:\n            n_clusters = 2\n            # Test with default values\n            X, rows, cols = make_biclusters(shape, n_clusters)\n            assert_equal(X.shape, shape)\n            assert_equal(rows.shape, (n_clusters, shape[0]))\n            assert_equal(cols.shape, (n_clusters, shape[1]))\n\n            # Test with different n_clusters\n            for n in [1, 3, shape[0] * shape[1]]:\n                X, rows, cols = make_biclusters(shape, n, shuffle=False)\n                assert_equal(X.shape, shape)\n                assert_equal(rows.shape, (n, shape[0]))\n                assert_equal(cols.shape, (n, shape[1]))\n"], "sample_163": ["    def test_logout_then_login(self):\n        self.login()\n\n        response = self.client.get(\"/logout_then_login/\")\n\n        self.assertRedirects(response, \"/accounts/login/\")\n        self.assertNotIn(SESSION_KEY, self.client.session)\n", "    def test_logout_then_login(self):\n        self.login()\n        response = self.client.get(\"/logout_then_login/\")\n        self.assertRedirects(response, reverse(\"admin:index\"))\n", "    def test_redirect_to_login_no_next(self):\n        login_url = \"/accounts/login/\"\n        url = \"/accounts/logout/\"\n        response = self.client.get(url)\n        self.assertRedirects(response, login_url)\n\n", "    def test_login_view_with_required_redirect_field_name(self):\n        with self.settings(LOGIN_REDIRECT_URL=\"/\"):\n            response = self.client.get(\"/login/\")\n            self.assertTemplateUsed(response, \"registration/login.html\")\n            self.assertContains(response, 'name=\"{}\"'.format(REDIRECT_FIELD_NAME))\n ", "    def test_login_redirect_to_next_page_with_redirect_field_name(self):\n        url = reverse(\"login\")\n        response = self.client.get(url, {\"next\": \"/some/target/page\"})\n        self.assertContains(response, \"/some/target/page\")\n"], "sample_491": ["    def test_bound_field_errors(self):\n        form = FrameworkForm({\"name\": \"a\"})\n        bound_field = form.fields[\"name\"]\n        self.assertIsInstance(bound_field.errors, ErrorList)\n        self.assertEqual(bound_field.errors.as_ul(), \"\")  # No errors\n        form = FrameworkForm({\"name\": \"\"})\n        bound_field = form.fields[\"name\"]\n        self.assertIsInstance(bound_field.errors, ErrorList)\n        self.assertNotEqual(bound_field.errors.as_ul(), \"\")\n", "    def test_bound_field_errors(self):\n        form = FrameworkForm(\n            data={'name': 'Invalid'},\n            error_class=ErrorList,\n        )\n        field = form.fields[\"name\"]\n        self.assertTrue(field.errors)\n        self.assertEqual(len(field.errors), 1)\n        self.assertEqual(field.errors[0].msg, \"Invalid\")\n        self.assertFalse(field._has_changed)\n\n", "    def test_BoundField_label_tag(self):\n        form = PersonNew()\n        bound_field = form.fields[\"first_name\"]\n\n        # No contents given, should use the field label\n        label = bound_field.label_tag()\n        self.assertIn(\"First name\", label)\n\n        # Contents given, use those instead\n        label = bound_field.label_tag(contents=\"Name\")\n        self.assertIn(\"Name\", label)\n\n        # No ID attribute, should not render a <label>\n        bound_field.field.widget.attrs[\"id\"] = \"\"\n        label = bound_field.label_tag()\n        self.assertEqual(label, \"\")\n\n        # Use custom tag\n        label = bound_field.label_tag(tag=\"span\")\n        self.assertIn(\"<span\", label)\n\n        # Use custom label suffix\n        label = bound_field.label_tag(label_suffix=\"\u2605\")\n        self.assertIn(\"\u2605\", label)\n\n\n\n", "    def test_boundfield_value_with_initial(self):\n        form = FrameworkForm(\n            data={\"name\": \"John Doe\"},\n        )\n        field = form.fields[\"name\"]\n        bound_field = BoundField(form, field, name=\"name\")\n        self.assertEqual(bound_field.value(), \"John Doe\")\n", "    def test_bound_field_errors(self):\n        form = FrameworkForm({\"language\": \"Foo\"})\n        field = form.fields[\"language\"]\n        self.assertTrue(field.errors)\n        self.assertEqual(field.errors.as_text(), \"Select a valid choice.\")\n\n        form = Person(data={\"first_name\": \"\", \"last_name\": \"\", \"birthday\": \"\"})\n        self.assertFalse(form.errors)\n        self.assertFalse(form.fields[\"first_name\"].errors)\n        self.assertFalse(form.fields[\"last_name\"].errors)\n        self.assertFalse(form.fields[\"birthday\"].errors)\n        form = Person(data={\"first_name\": \"\", \"last_name\": \"\", \"birthday\": \"2023-02-29\"})\n        self.assertTrue(form.errors)\n        self.assertEqual(form.errors.as_text(), \"Error: Invalid date: '2023-02-29'.\")\n\n        form = Person(data={\"first_name\": \"12345678901234567890123456789012\", \"last_name\": \"\", \"birthday\": \"\"})\n        self.assertTrue(form.errors)\n        self.assertEqual(form.errors.as_text(), \"Error: [Error: 'first_name' - Length must be less than or equal to 150 characters.\"]\n\n\n        \n"], "sample_1084": ["    def test_intersection_sets_complexregion():\n        R = lambda x: Complex(x, 1)\n        a = ComplexRegion(Interval(0, 2), Interval(0, pi))\n        b = ComplexRegion(Interval(1, 3), Interval(pi/2, 2*pi))\n        expected = ComplexRegion(Interval(1, 2), Interval(pi/2, pi))\n        assert intersection_sets(a, b) == expected\n", "    def test_intersection_sets_range_complex_region():\n        re = symbols('re', real=True)\n        im = symbols('im', real=True)\n        theta = symbols('theta', real=True)\n\n        cr = ComplexRegion(Interval(0, 2*pi, left_open=True, right_open=False),\n                           Interval(-1, 1))\n        r = Range(-2, 2)\n        with pytest.raises(TypeError):\n            intersection_sets(cr, r)\n", "    def test_intersection_sets_symbolic_range():\n        a = FiniteSet(x for x in range(5) if x > 2)\n        b = FiniteSet(x for x in range(1, 10) if x % 2 == 0)\n        intersection_sets(a, b)\n", "    def test_intersection_products():\n        a = ProductSet((S.Naturals, S.Naturals))\n        b = ProductSet((S.Integers, S.Reals))\n        c = intersection_sets(a, b)\n        assert isinstance(c, ProductSet)\n        assert c.args == ((S.Naturals, S.Reals))\n", "compilation error"], "sample_725": ["    def test_check_array_dtype_numeric(dtype):\n        X = np.array([[1, 2], [3, 4]], dtype=dtype)\n        X_converted = check_array(X, dtype=\"numeric\", warn_on_dtype=True)\n        if dtype == \"object\":\n            assert_allclose(X_converted, X.astype(np.float64))\n        else:\n            assert_array_equal(X_converted, X)\n", "    def test_check_array_sparse_conversion(accepSparse):\n        X = sp.csc_matrix([[1, 2], [3, 4]])\n\n        if accepSparse:\n            X_converted = check_array(X, accept_sparse=accepSparse)\n            assert_allclose_dense_sparse(X_converted, X)\n        else:\n            with assert_raises(ValueError):\n                check_array(X, accept_sparse=accepSparse)\n", "    def test_check_memory():\n        # Test if the memory usage is checked correctly depending on the\n        # provided parameter.\n        X = np.random.rand(1000, 10)\n\n        # Test case 1: memory limit set to None (default)\n        with assert_no_warnings():\n            check_memory(X, memory=None)\n\n        # Test case 2: memory limit set to a positive value\n        with assert_no_warnings():\n            check_memory(X, memory=200)\n\n        # Test case 3: memory limit set to a negative value\n        with assert_raises(ValueError):\n            check_memory(X, memory=-10)\n        \n        # Test case 4: memory limit set to a string\n        with assert_raises(ValueError):\n            check_memory(X, memory=\"abc\")\n\n", "compilation error", "    def test_check_X_y_multi_output(self):\n        X = np.array([[1, 2], [3, 4]])\n        y = np.array([[5, 6], [7, 8]])\n\n        with ignore_warnings(category=DataConversionWarning):\n            X_converted, y_converted = check_X_y(X, y, multi_output=True)\n\n        assert_array_equal(X_converted, X)\n        assert_array_equal(y_converted, y)\n\n        with ignore_warnings(category=DataConversionWarning):\n            X_converted, y_converted = check_X_y(X.astype(\"float\"), y.astype(\"float\"), multi_output=True)\n\n        assert_array_equal(X_converted, X.astype(\"float\"))\n        assert_array_equal(y_converted, y.astype(\"float\"))\n\n\n"], "sample_170": ["    def test_callable_setting_wrapper_calls_callable(self):\n            return 'Callable setting value'\n\n        wrapper = CallableSettingWrapper(callable_setting)\n\n        self.assertEqual(wrapper(), 'Callable setting value')\n\n", "    def test_callable_setting_wrapper_is_not_called_in_debug_mode(self):\n        with override_settings(DEBUG=True):\n            wrapper = CallableSettingWrapper(lambda: 'setting')\n            self.assertEqual(wrapper, 'setting')\n", "    def test_callable_setting_wrapper_calls_callable(self):\n        wrapped_callable = CallableSettingWrapper(lambda: 'wrapped result')\n        self.assertEqual(wrapped_callable(), 'wrapped result')\n", "    def test_callable_setting_wrapper_not_callable(self):\n        wrapped = CallableSettingWrapper('not callable')\n        self.assertFalse(callable(wrapped))\n        with self.assertRaises(TypeError):\n            wrapped()\n", "    def test_callable_setting_wrapper(self):\n        callable_setting = lambda: 'default value'\n        wrapper = CallableSettingWrapper(callable_setting)\n        self.assertIs(wrapper.__wrapped__, callable_setting)\n        self.assertEqual(wrapper(), 'default value')\n\n\n"], "sample_324": ["    def test_csrf_exception_for_invalid_token_format(self):\n        req = self._get_POST_csrf_cookie_request(cookie='abcde')\n        with self.assertRaises(RejectRequest) as context:\n            CsrfViewMiddleware().process_request(req)\n        self.assertIn(\n            REASON_INCORRECT_LENGTH, context.exception.args[0]\n        )\n\n", "    def test_origin_verification_non_secure_request(self):\n        \"\"\"\n        Test that a non-secure request is rejected if the Referer header doesn't match.\n        \"\"\"\n        req = self._get_GET_csrf_cookie_request()\n        req._is_secure_override = False\n        @csrf_exempt\n            return HttpResponse(\"OK\")\n        with override_settings(CSRF_TRUSTED_ORIGINS=['https://example.com']):\n            middleware = CsrfViewMiddleware()\n            response = middleware.process_view(req, view, [], {})\n        self.assertEqual(response, middleware._reject(req, REASON_BAD_ORIGIN % 'http://example.com'))\n", "    def test_no_csrf_token_missing_in_post(self):\n        req = self._get_POST_no_csrf_cookie_request()\n        res = self.view(req)\n        self.assertEquals(res.status_code, 403)\n        self.assertIn('CSRF token missing.', res.content.decode())\n", "    def test_referer_check_allows_same_domain(self):\n        req = TestingHttpRequest()\n        req.method = 'POST'\n        req.META['HTTP_ORIGIN'] = 'http://example.com'\n        req.META['HTTP_REFERER'] = 'http://example.com/previous_page'\n        req.META['HTTP_HOST'] = 'example.com'\n        middleware = CsrfViewMiddleware()\n        middleware.process_request(req)\n", "    def test_missing_csrf_from_header(self):\n        # Test that if a POST request is missing a CSRF token in the request\n        # headers, we reject it.\n        request = self._get_POST_no_csrf_cookie_request()\n\n        response = self.middleware.process_request(request)\n        self.assertIsInstance(response, HttpResponse)\n        self.assertEqual(response.status_code, 403)\n        self.assertEqual(response.content.decode('utf-8'), 'CSRF token missing.')\n"], "sample_82": ["    def test_format_value_none(self):\n        \"\"\"Tests that format_value() returns None for None input.\"\"\"\n        widget = self.widget\n        self.assertIsNone(widget.format_value(None))\n", "    def test_value_from_datadict_empty_values(self):\n        data = {\n            'form-year': '',\n            'form-month': '',\n            'form-day': '',\n        }\n        value = self.widget.value_from_datadict(data, {}, 'form')\n        self.assertIsNone(value)\n", "    def test_value_from_datadict_with_empty_values(self):\n        data = {\n            'date_year': '',\n            'date_month': '',\n            'date_day': ''\n        }\n        self.assertEqual(self.widget.value_from_datadict(data, {}, 'date'), None)\n", "    def test_format_value_none_date(self):\n        self.assertEqual(self.widget.format_value(None), {'year': None, 'month': None, 'day': None})\n", "    def test_non_empty_label(self):\n        widget = SelectDateWidget(\n            years=('2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016'),\n            empty_label=('---', '---', '---'),\n        )\n        form = Form(None, {\n            'date': widget,\n        })\n\n        context = form.fields['date'].widget.get_context(\n            name='date',\n            value=None,\n        )\n        self.assertEqual(context['year']['widget']['attrs']['placeholder'], '')\n        self.assertEqual(context['month']['widget']['attrs']['placeholder'], '')\n        self.assertEqual(context['day']['widget']['attrs']['placeholder'], '')\n"], "sample_762": ["    def test_pickle_with_deprecated_original(self):\n\n        class MyEstimatorWithDeprecated(BaseEstimator):\n            # Simulate a constructor before deprecation wrapping\n            @classmethod\n                return ['a'] \n                self.a = a\n\n            @classmethod\n                return\n\n                self.a = a\n\n        instance = MyEstimatorWithDeprecated(a=1)\n                \n        with ignore_warnings():\n            pickled = pickle.dumps(instance)\n\n        loaded = pickle.loads(pickled)\n        assert_equal(loaded.a, 1)\n\n\n", "    def test_clone_nested_estimators(self):\n        # Test cloning a Pipeline with nested estimators\n        from sklearn.base import BaseEstimator\n\n        class MyNestedEstimator(BaseEstimator):\n                self.a = a\n                self.b = b\n\n        clf = Pipeline([\n            ('nested', MyNestedEstimator(a=3, b=4)),\n            ('clf', SVC())\n        ])\n        clf.fit(np.zeros((10, 2)), np.zeros(10))\n        cloned_clf = clone(clf)\n        assert_equal(cloned_clf.steps[0][1].a, 3)\n        assert_equal(cloned_clf.steps[0][1].b, 4)\n        assert_not_equal(id(clf.steps[0][1]), id(cloned_clf.steps[0][1]))\n        assert_equal(cloned_clf.steps[1][1].__class__.__name__, 'SVC')\n        assert_not_equal(id(clf.steps[1][1]), id(cloned_clf.steps[1][1]))\n", "    def test_clone_non_estimator(self):\n        obj = {'a': 1, 'b': 2}\n        clone_obj = clone(obj)\n        assert_not_equal(clone_obj, obj)\n        assert_dict_equal(clone_obj, obj)\n", "    def test_copy_estimator(self):\n        est = SVC(kernel='linear')\n        est_copy = clone(est)\n        assert_not_equal(est, est_copy)\n        assert_dict_equal(est.get_params(), est_copy.get_params())\n\n        est_copy.fit(np.random.rand(10, 5), np.random.rand(10))\n        with assert_raises(ValueError):\n            est.fit(np.random.rand(10, 5), np.random.rand(10))\n\n", "    def test_clone_decorator(self):\n        # Test that clone decorator works\n        class MyEstimator(BaseEstimator):\n                self.a = a\n                self.b = b\n\n        estimator = MyEstimator(a=3, b=4)\n        cloned_estimator = clone(estimator)\n\n        assert_not_equal(id(estimator), id(cloned_estimator))\n        assert_equal(estimator.a, cloned_estimator.a)\n        assert_equal(estimator.b, cloned_estimator.b)\n\n        # Test with nested parameters\n        class Pipeline(object):\n                self.steps = steps\n\n        pipeline = Pipeline([('clf', MyEstimator(a=5, b=6))])\n        cloned_pipeline = clone(pipeline)\n\n        assert_not_equal(id(pipeline), id(cloned_pipeline))\n        assert_not_equal(id(pipeline.steps[0][1]), id(cloned_pipeline.steps[0][1]))\n        assert_equal(pipeline.steps[0][1].a, cloned_pipeline.steps[0][1].a)\n        assert_equal(pipeline.steps[0][1].b, cloned_pipeline.steps[0][1].b)\n\n\n"], "sample_414": ["    def test_formfield_for_dbfield_with_specific_widget(self):\n        ff = self.assertFormfield(\n            VideoStream, \"uploaded_file\", forms.FileInput,\n            can_view_other_users_uploaded_files=True\n        )\n        self.assertEqual(ff.widget.attrs.get(\"accept\"), \".mp4\")\n\n\n\n", "    def test_formfield_for_dbfield_my_file_field(self):\n        ff = self.assertFormfield(MyFileField, \"file\", forms.FileInput)\n        self.assertEqual(ff.widget.attrs.get(\"accept\"), \".csv\") \n", "    def test_formfield_for_dbfield_with_custom_widget(self):\n        class MyModelAdmin(admin.ModelAdmin):\n                if db_field.name == 'my_image':\n                    kwargs['widget'] = MyCustomWidget()\n                return super().formfield_for_dbfield(db_field, **kwargs)\n\n        from .models import MyModelWithImage as MyModel\n        ff = self.assertFormfield(MyModel, 'my_image', widgets.FileField, admin_overrides={'formfield_for_dbfield': MyModelAdmin.formfield_for_dbfield})\n\n        self.assertIsInstance(ff.widget, MyCustomWidget)\n\n\n\n", "    def test_formfield_for_dbfield_my_file_field(self):\n        ff = self.assertFormfield(MyFileField, 'file', forms.FileInput)\n\n        self.assertIsInstance(ff.widget, forms.FileInput)\n\n\n\n", "    def test_formfield_for_dbfield_myfilefield(self):\n        ff = self.assertFormfield(\n            MyFileField, \"file\", forms.FileInput,\n        )\n\n        self.assertEqual(ff.attrs.get(\"enctype\"), \"multipart/form-data\")\n\n\n"], "sample_24": ["    def test_shape_preserving(self, mask_a_2d, mask_b_2d):\n        self.a = np.array([[1, 2], [3, 4]])\n        self.b = np.array([[5, 6], [7, 8]])\n        self.ma = MaskedArray(self.a)\n        self.mb = MaskedArray(self.b)\n        self.mask_a = mask_a_2d\n        self.mask_b = mask_b_2d\n\n        for func in [np.concatenate, np.stack, np.hstack]:\n            self.check(func, (self.mb,), axis=0)\n\n        for func in [np.vstack, np.dstack]:\n            self.check(func, (self.mb,), axis=1)\n\n\n\n\n", "    def test_shape_information(self):\n        from astropy.core import Unit\n        from astropy.units.quantity import Quantity\n\n        class ShapeInformationTest(BasicTestSetup):\n                with pytest.raises(NotImplementedError):\n                    super().check(func, *args, **kwargs)\n\n            q = Quantity(np.random.rand(shape), Unit(\"m/s\"))\n            wrapped_func = func.__wrapped__ if hasattr(func, '__wrapped__') else func\n            with pytest.raises(NotImplementedError):\n                wrapped_func(q)\n\n        for func in DISPATCHED_FUNCTIONS:\n            for shape in [(), (2,), (2, 3), (2, 3, 4)]:\n                _test_shape_functions(func, shape)\n\n        for func in APPLY_TO_BOTH_FUNCTIONS:\n            for shape in [(), (2,), (2, 3), (2, 3, 4)]:\n                _test_shape_functions(func, shape)\n\n\n\n", "    def check_shape_changes(self, func):\n        o = func(self.ma)\n        s = o.shape\n        expected = func(self.a).shape\n        assert s == expected\n        \n        if func in APPLY_TO_BOTH_FUNCTIONS:\n            o = func(self.ma, self.mb)\n            s = o.shape\n            expected = func(self.a, self.b).shape\n            assert s == expected\n\n", "    def test_shape_mismatch(self):\n        with pytest.raises(ValueError):\n            np.shape(self.ma)\n", "    def test_shape_information(self):\n            return x.shape, y.shape, axis\n\n        self.check(shape_func, (1, 2, 3), (4, 5))\n"], "sample_522": ["def test_colorbar_extension_length(request):\n    \"\"\"Test colorbar extension length with uniform and proportional spacing.\"\"\"\n    spacing = ['uniform', 'proportional']\n    for s in spacing:\n        fig = _colorbar_extension_length(s)\n        plt.close(fig)\n\n\n\n", "def test_colorbar_orientation_vertical():\n    # Create a figure and a subplot.\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n\n    # Generate some example data.\n    x = np.linspace(-10, 10, 50)\n    y = np.sin(x)\n\n    # Plot the data.\n    ax.plot(x, y)\n\n    # Create a colorbar with vertical orientation.\n    cax = fig.add_axes([0.8, 0.15, 0.02, 0.7])\n    Colorbar(cax, cmap='viridis', orientation='vertical')\n\n    # Set the aspect ratio to 'auto' to ensure the colorbar scales correctly.\n    cax.set_aspect('auto')\n\n    plt.show()\n\n", "def test_colorbar_extension_length(request):\n    if request.config.getoption(\"--backend\") == \"agg\":\n        pytest.skip(\"Skipping because extensions not rendered properly \"\n                     \"in agg backend\")\n\n    fig = _colorbar_extension_length(\"uniform\")\n    plt.close(fig)\n    \n    fig = _colorbar_extension_length(\"proportional\")\n    plt.close(fig)\n\n\n\n", "def test_colorbar_extension_length():\n    \"\"\"Test colorbar extensions lengths with different spacing.\"\"\"\n    plt.rc('axes.spines', top=False, right=False)\n    fig = _colorbar_extension_length(spacing='uniform')\n\n    # Test with proportional spacing.\n    with rc_context({'axes.colorbar.spacing': 'proportional'}):\n        fig2 = _colorbar_extension_length(spacing='proportional')\n\n    check_figures_equal(fig, fig2)\n", "def test_colorbar_extension_length(spacing):\n    fig = _colorbar_extension_length(spacing)\n    plt.show()\n"], "sample_272": ["    def test_replacements(self):\n        with self.assertNumQueries(6) as query_count:\n            self.run_migrations(\"01_initial\", \"02_replace\")\n        self.assertEqual(query_count, 6)\n\n\n        with self.assertNumQueries(0) as query_count:\n            self.run_migrations(\"02_replace\", \"02_replace\", fake=True)\n        self.assertEqual(query_count, 0)\n", "    def test_migration_plan(self):\n        from django.db.migrations.graph import MigrationGraph\n\n        # Create a new migration graph\n        graph = MigrationGraph()\n        # Create a new Migration Executor\n        executor = MigrationExecutor(connection, progress_callback=None)\n        # Perform the migration plan\n\n        plan = executor.migration_plan([\"myapp\"])\n        self.assertEqual(len(plan), 2)\n\n\n\n", "    def test_migrate_with_replacements(self):\n        with self.assertNumQueries(10):\n            # Migration migrations2.0001 creates a model with another\n            # migration that replaces it\n            executor = MigrationExecutor(\n                connection=self.connections['default'],\n                progress_callback=lambda x, y, z: None\n            )\n            executor.migrate(\n                [('migrations', '0001'), ('migrations2', '0001')],\n                state=None\n            )\n\n\n\n", "    def test_migration_plan_with_unmigrate_everything(self):\n        \"\"\"\n        Migrations should work as expected when a target app includes everything.\n        \"\"\"\n        with self.assertNumQueries(2):  \n            # Unmigrate everything from the 'migrations' app to start.\n            MigrationExecutor(self.connection).migrate(\n                [(\"migrations\", None)], plan=None, state=None, fake=False\n            )\n        with self.assertNumQueries(2): \n            # Migrate everything back and check the migrations are applied.\n            MigrationExecutor(self.connection).migrate(\n                [(\"migrations\", None)], plan=None, state=None, fake=False\n            )\n", "    def test_migration_plan_clean_start(self):\n        with self.subTest(\"clean start\"):\n            with self.connection.atomic():\n                self.db_utils.create_table(\"migrations\", \"test_migration\", fields=[\n                    models.CharField(max_length=100)\n                ])\n            # Simulate the existence of a migration for the initial table creation\n            migration_loader = self.create_migration_loader()\n            migration_recorder = MigrationRecorder(self.connection)\n            migration_recorder.record_applied(\"migrations\", \"0001_initial\")\n            migration_plan = migration_loader.migration_plan(\n                [\"migrations\"], clean_start=True\n            )\n            self.assertEqual(len(migration_plan), 1)\n            self.assertEqual(migration_plan[0][0].name, \"0001_initial\")\n            self.assertFalse(migration_plan[0][1])  # Not backwards\n\n\n\n"], "sample_850": ["    def test_rbf_sampler_with_sparse_data(self):\n        X_sparse = csr_matrix(X)\n\n        rbf = RBFSampler(gamma=1., random_state=0)\n        X_dense = rbf.fit_transform(X)\n        X_sparse_transformed = rbf.transform(X_sparse)\n\n        assert_array_almost_equal(X_dense, X_sparse_transformed.toarray())\n\n\n", "    def test_rbf_sampler_custom_kernel(self):\n            return np.exp(-gamma * np.linalg.norm(X1[:, np.newaxis, :] - X2[np.newaxis, :, :], axis=2)**2)\n        rs = RBFSampler(gamma=1, n_components=10, random_state=0)\n        X_transformed = rs.fit_transform(X, None)\n        rs_custom = RBFSampler(gamma=1, n_components=10, random_state=0,\n                               kernel=custom_rbf)\n        X_transformed_custom = rs_custom.fit_transform(X, None)\n        assert_array_almost_equal(X_transformed, X_transformed_custom)\n", "    def test_Nystroem_kernel(self):\n        for kernel_name in [\"rbf\", \"linear\", \"poly\", \"sigmoid\", \"chi2\"]:\n            \n            if kernel_name == \"chi2\":\n                continue\n            \n            kernel = kernel_metrics[kernel_name]\n            nystroem = Nystroem(kernel=kernel, gamma=0.1, n_components=50)\n            X_approx = nystroem.fit_transform(X)\n            \n            # check if the transformed data shape is correct\n            assert X_approx.shape[1] == 50\n            \n            # check if the kernels are similar\n            kernel_matrix = kernel(X, X)\n            approx_kernel_matrix = kernel(X_approx, X_approx)\n            assert_array_almost_equal(kernel_matrix, approx_kernel_matrix,\n                                      atol=1e-4)\n            \n\n", "    def test_nystroem_kernel_params(self, kernel):\n        nystroem = Nystroem(kernel=kernel, n_components=50)\n        \n        # Test different kernel parameters\n        params = {'gamma': 1, 'coef0': 1, 'degree': 3}\n        \n        # Get the kernel_params from the Nystroem object\n        nystroem_params = nystroem._get_kernel_params()\n        \n        # Compare the original parameters with the extracted parameters\n        assert_array_equal(nystroem_params, params)\n\n        # Test that the params are passed correctly to the pairwise_kernels\n        transformed = nystroem.fit_transform(X, y=None)\n        nystroem_params = nystroem._get_kernel_params()\n        assert_array_almost_equal(\n            nystroem.transform(X),\n            pairwise_kernels(X, X,\n                            metric=kernel,\n                            filter_params=True,\n                            **nystroem_params\n            ),\n            rtol=1e-3,\n        ) \n        \n\n", "    def test_rbf_sampler_sparse(self):\n        X_sparse = csr_matrix(X)\n        rbf = RBFSampler(gamma=10)\n        X_dense = rbf.fit_transform(X_sparse)\n        X_sparse_transformed = rbf.fit_transform(X_sparse)\n        assert_array_equal(X_dense, X_sparse_transformed)\n\n"], "sample_615": ["    def test_unify_chunks_no_chunks(\n        array_shape, dim_coords", "    def test_join_dict_keys(simple_example_dict):\n        result = join_dict_keys({\"a\": \"b\", \"c\": \"d\"}, simple_example_dict)\n        assert result == {\"a\": \"b\", \"c\": \"d\", \"e\": \"f\"}\n\n", "    def test_ordered_set_intersection_empty(\n        ):\n        assert ordered_set_intersection(set(), set()) == set()\n        assert ordered_set_intersection(set(), set('abc')) == set()\n        assert ordered_set_intersection(set('abc'), set()) == set()\n", "compilation error", "    def test_unified_dim_sizes(dims, expected):\n        da = xr.DataArray(np.arange(2 * 3 * 4).reshape(2, 3, 4), dims=[\"x\", \"y\", \"z\"])\n        assert unified_dim_sizes(da, dims) == expected\n\n\n"], "sample_209": ["    def test_deferrable_unique_constraint(self):\n        with isolate_apps(apps):\n            with self.assertRaises(ValidationError):\n                Party.objects.create(name='Duplicate Party')\n            Party.objects.create(name='Unique Party')\n\n\n", "    def test_check_constraints(self):\n        with isolate_apps(apps):\n            with self.subTest(\"Valid constraints\"):\n                # Constraint without condition\n                class ArticleWithConstraint(Article):\n                    class Meta:\n                        constraints = [\n                            models.UniqueConstraint(\n                                fields=['title', 'published_date'],\n                                name='unique_title_date',\n                            ),\n                        ]\n                ArticleWithConstraint.objects.create(title='Test title', published_date=datetime.date(2023, 10, 26))\n                ArticleWithConstraint.objects.create(title='Different title', published_date=datetime.date(2023, 10, 26))  # Should work\n\n                # Constraint with condition\n                class ArticleWithConditionalConstraint(Article):\n                    class Meta:\n                        constraints = [\n                            models.CheckConstraint(\n                                check=models.Q(published_date__gt=datetime.date(2023, 10, 25)),\n                                name='published_after_date',\n                            ),\n                        ]\n                model = ArticleWithConditionalConstraint\n                model.objects.create(title='Test title', published_date=datetime.date(2023, 10, 26))\n                model.objects.create(title='Test title', published_date=datetime.date(2023, 10, 24))  # Should fail\n\n            with self.subTest(\"Constraint errors\"):\n                with self.assertRaises(ValidationError):\n                    class ArticleWithInvalidConstraint(Article):\n                        class Meta:\n                            constraints = [\n                                models.UniqueConstraint(\n                                    fields=['title', 'published_date'],\n                                    name='unique_title_date',\n                                    condition=models.Q(categories__name='invalid'),\n                                ),\n                            ]\n                    ArticleWithInvalidConstraint.objects.create(title='Test title', published_date=datetime.date(2023, 10, 26))\n\n", "    def test_index_together(self):\n        with isolate_apps(apps):\n            # Create an index_together field\n            class WithIndexTogether(models.Model):\n                field1 = models.CharField(max_length=50)\n                field2 = models.CharField(max_length=50)\n\n                class Meta:\n                    indexes = [\n                        models.Index(fields=['field1', 'field2'], name='idx_together'),\n                    ]\n\n            with self.subTest('IndexTogether creation'):\n                WithIndexTogether.objects.create(field1='a', field2='b')\n                WithIndexTogether.objects.create(field1='c', field2='d')\n            \n            with self.subTest('IndexTogether query'):\n                self.assertEqual(\n                    WithIndexTogether.objects.filter(field1='a', field2='b').count(), 1\n                )\n", "    def test_ordering_related_fields(self):\n        with isolate_apps(apps):\n            Employee.objects.create(name='Alice', department_id=1)\n            Employee.objects.create(name='Bob', department_id=2)\n            Department.objects.create(name='Engineering', employees=Employee.objects.all())\n            Department.objects.create(name='Sales', employees=Employee.objects.all())\n            # Check ordering by related field\n            employees = Employee.objects.all().order_by('department__name')\n            self.assertEqual(employees.count(), 2)\n            self.assertEqual(employees[0].department.name, 'Engineering')\n            self.assertEqual(employees[1].department.name, 'Sales')\n", "    def test_check_constraints(self):\n        with isolate_apps(apps=['mytest']):\n            self.assertEqual(len(Article._meta.constraints), 1)\n            self.assertEqual(len(Article._meta.constraints), 1)\n\n            # Test that check constraints with conditions are handled\n            # appropriately\n            class ArticleWithCondition(Article):\n                class Meta:\n                    unique_together = [('title', 'created_date')]\n                    constraints = [\n                        CheckConstraint(\n                            check=models.Q(title__startswith='This is a test'),\n                            name='title_starts_with',\n                        ),\n                    ]\n            with self.assertRaises(ValidationError):\n                ArticleWithCondition.objects.create(title='Just a test', created_date=datetime.date(2023, 10, 26))\n            with self.assertRaises(ValidationError):\n                ArticleWithCondition.objects.create(title='This is a test', created_date=datetime.date(2023, 10, 26))\n            ArticleWithCondition.objects.create(title='This is a test', created_date=datetime.date(2023, 10, 26))\n\n\n"], "sample_722": ["    def test_reassignment_ratio(self):\n        X, _ = make_blobs(n_samples=n_samples, centers=centers,\n                            cluster_std=1., random_state=42)\n        X_csr = sp.csr_matrix(X)\n        model = MiniBatchKMeans(n_clusters=3, batch_size=10,\n                                reassignment_ratio=0.1)\n\n        # Test with low reassignment ratio\n        model.fit(X)\n        assert_equal(model.reassign_ratio, 0.1)\n\n        # Test with reassignment_ratio = 0\n        model = MiniBatchKMeans(n_clusters=3, batch_size=10,\n                                reassignment_ratio=0)\n        model.fit(X)\n        assert_equal(model.reassign_ratio, 0)\n\n        # Test with too high reassignment ratio\n        with assert_raises_regex(ValueError,\n                                  \"reassignment_ratio should be between 0 and 1\"):\n            model = MiniBatchKMeans(n_clusters=3, batch_size=10,\n                                    reassignment_ratio=1.1)\n            model.fit(X)\n\n\n\n", "def test_mini_batch_kmeans_reassignment_ratio():\n    random_state = np.random.RandomState(42)\n    X, y = make_blobs(n_samples=1000, centers=5, random_state=42)\n    mbkmeans = MiniBatchKMeans(n_clusters=5, batch_size=100,\n                              reassignment_ratio=0.01, random_state=42)\n    mbkmeans.fit(X)\n\n    # Check if the reassignment happened\n    counts = mbkmeans.counts_\n    for count in counts:\n        assert_greater(count, 0)\n\n\n", "    def test_mini_batch_convergence(self):\n        # Test convergence logic\n        random_state = 42\n        np.random.seed(random_state)\n        X = np.random.rand(100, 5)\n        n_clusters = 3\n        batch_size = 10\n        tol = 0.001\n\n        # Initialize KMeans object with given parameters\n        kmeans = MiniBatchKMeans(n_clusters=n_clusters, batch_size=batch_size,\n                                  tol=tol, random_state=random_state)\n\n        # Fit the model on the data.\n        kmeans.fit(X)\n\n        # Check if convergence is achieved correctly\n        assert_true(kmeans.inertia_ < 1e-2,\n                    \"Inertia should be close to zero after convergence.\")\n        assert_true(kmeans.n_iter_ > 0, \"Number of iterations should be positive.\")\n", "def test_mini_batch_kmeans_with_multiprocessing():\n    X, y = make_blobs(n_samples=100, centers=3, random_state=42)\n    kmeans = MiniBatchKMeans(\n        n_clusters=3, batch_size=20, max_iter=10, random_state=42)\n    kmeans.fit(X)\n    \n    with assert_raises(ValueError):\n        kmeans.fit(X, y=y) \n  \n    labels = kmeans.labels_\n\n\n\n", "    def test_reassign_ratio(self):\n        random_state = 42\n        np.random.seed(random_state)\n        X, y = make_blobs(n_samples=100, centers=4,\n                          cluster_std=0.5, random_state=random_state)\n        \n        # Test with a relatively small reassignment ratio\n        mbkmeans = MiniBatchKMeans(n_clusters=4, batch_size=10,\n                                   reassignment_ratio=0.01,\n                                   random_state=random_state)\n        mbkmeans.fit(X)\n        \n        # Check that some centers have been reassigned\n        reassigned_counts = np.sum(mbkmeans.counts_ < 1)\n        assert_greater(reassigned_counts, 0,\n                       \"No centers reassigned even with a small ratio\")\n        \n        # Test with a high reassignment ratio\n        mbkmeans = MiniBatchKMeans(n_clusters=4, batch_size=10,\n                                   reassignment_ratio=0.5,\n                                   random_state=random_state)\n        mbkmeans.fit(X)\n        \n        # Check that many centers have been reassigned\n        reassigned_counts = np.sum(mbkmeans.counts_ < 1)\n        assert_greater(reassigned_counts, 1,\n                       \"Too few centers reassigned with a high ratio\")\n\n\n\n"], "sample_388": ["    def test_custom_clean_username(self):\n        class CustomRemoteUserBackend(RemoteUserBackend):\n                return username.upper()\n\n        with override_settings(AUTHENTICATION_BACKENDS=[\"CustomRemoteUserBackend\"]):\n            user = authenticate(remote_user=\"JohnDoe\")\n            self.assertEqual(user.username, \"JOHNDOE\")\n", "    def test_remote_user_creation(self):\n        response = self.client.get('/remote_user_creation/')\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, \"This is a test\")\n\n        # Check that the user was created\n        user = User.objects.filter(username=\"unknown_user\").first()\n        self.assertIsNotNone(user)\n", "    def test_unknown_user(self):\n        with self.assertRaises(User.DoesNotExist):\n            authenticate(request=None, remote_user=\"unknownuser\")\n", "    def test_remote_user_auth_with_email(self):\n        with override_settings(REMOTE_USER_EMAIL_FIELD=\"REMOTE_EMAIL\"):\n            response = self.client.get(\"/user_info/\")\n            self.assertEqual(response.status_code, 200)\n            self.assertIn(\"Remote User: knownuser\", response.content.decode())\n            self.assertIn(\"Remote Email: knownuser@example.com\", response.content.decode())\n", "    def test_remote_user_authentication_with_create_unknown_user_false(self):\n        with override_settings(\n            AUTHENTICATION_BACKENDS=[RemoteUserBackend.__name__],\n            MIDDLEWARE=[RemoteUserMiddleware.__name__],\n        ):\n            user = authenticate(request=None, remote_user=\"unknown_user\")\n            self.assertIsNone(user)\n"], "sample_250": ["    def test_datetime_format_rfc5322(self):\n        dt = datetime(2023, 10, 26, 10, 30, 0, tzinfo=get_default_timezone())\n        expected = 'Thu, 26 Oct 2023 10:30:00 +0200'\n        self.assertEqual(format(dt, 'r'), expected)\n", "    def test_format_with_timezone_naive(self):\n        naive_dt = datetime(2023, 10, 26, 10, 30, 0)\n        formatted = format(naive_dt, 'r')\n        self.assertEqual(formatted, 'Thu, 26 Oct 2023 10:30:00 +0200')\n", "    def test_ISO8601(self):\n        dt = datetime(2023, 10, 26, 14, 30, 0)\n        self.assertEqual(format(dt, 'c'), '2023-10-26T14:30:00+02:00')\n", "    def test_format_rfc5322(self):\n        dt = datetime(2023, 10, 27, 10, 30, 0, 123456)\n        formatted_dt = format(dt, 'r')\n        self.assertEqual(formatted_dt, 'Fri, 27 Oct 2023 10:30:00 +0200')\n", "    def test_format_with_datetime(self):\n        now = datetime.now(utc)\n        expected = format(now, 'c')\n        self.assertEqual(format(now, 'c'), expected)\n"], "sample_1188": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1195": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_265": ["    def test_get_templatetag_libraries(self):\n        with override_settings(INSTALLED_APPS=['project1', 'project2']):\n            # Create test apps for which we'll add templatetags\n            with open(Path(__file__).parent / 'test_apps' / 'project1' / 'templatetags' / '__init__.py', 'w') as f:\n                f.write('')\n            with open(Path(__file__).parent / 'test_apps' / 'project2' / 'templatetags' / '__init__.py', 'w') as f:\n                f.write('')\n            \n\n            libraries = DjangoTemplates(\n                {'OPTIONS': {'libraries': {}}}).get_templatetag_libraries({})\n\n            self.assertIn('project1', libraries)\n            self.assertIn('project2', libraries)\n\n", "    def test_get_templatetag_libraries(self):\n        with override_settings(INSTALLED_APPS=['testapp']):\n            engines = self.create_engines(params={'OPTIONS': {'libraries': {'my_tag': 'mymodule.mytags'}}})\n            libraries = engines[self.backend_name].get_templatetag_libraries({})\n            self.assertIn('my_tag', libraries)\n            self.assertEqual(libraries['my_tag'], 'mymodule.mytags')\n\n            # Test that django's built-in tags are included\n            self.assertIn('i18n', libraries)\n\n\n\n", "    def test_get_templatetag_libraries(self):\n        with override_settings(INSTALLED_APPS=['myapp']):\n            libraries = self.engine.get_templatetag_libraries({})\n            self.assertIn('myapp', libraries)\n            self.assertIn('i18n', libraries)\n", "    def test_get_templatetag_libraries(self):\n        with override_settings(INSTALLED_APPS=['testapp']):\n            engine = self.engine_class({'OPTIONS': {'libraries': {'testapp': 'myapp.templatetags.testapp'}}})\n            installed_libraries = engine.get_templatetag_libraries({})\n            self.assertIn('testapp', installed_libraries)\n        \n            # Check that non existing app does not show up\n            with override_settings(INSTALLED_APPS=['testapp']):\n                engine = self.engine_class({'OPTIONS': {'libraries': {'non_existing': 'myapp.templatetags.non_existing'}}})\n                installed_libraries = engine.get_templatetag_libraries({})\n                self.assertNotIn('non_existing', installed_libraries)\n", "    def test_get_templatetag_libraries_installed_apps(self):\n        with override_settings(INSTALLED_APPS=['django_template_tags']):\n            backend = self.engine_class({})\n            libraries = backend.get_templatetag_libraries({})\n            self.assertIn('django_template_tags', libraries)\n\n"], "sample_870": ["    def test_constrained_optimization(kernel):\n        gpr = GaussianProcessRegressor(kernel=kernel, alpha=1e-6)\n        initial_theta = np.random.rand(kernel.n_dims)\n        bounds = kernel.bounds\n\n            gpr.kernel_.theta = theta\n            return gpr.log_marginal_likelihood()\n\n        theta_opt, func_min = gpr._constrained_optimization(obj_func, initial_theta, bounds)\n\n        gpr.kernel_.theta = theta_opt\n        assert_allclose(gpr.log_marginal_likelihood(), func_min, atol=1e-4)\n\n", "    def test_fit_with_different_optimizers(self, kernel):\n        gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=0)\n        gp.fit(X, y)\n        # We should be able to fit without any issues.\n\n        gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=1)\n        gp.fit(X, y)\n        # We should be able to fit with restarting.\n        \n\n\n", "    def test_constrained_optimization_multiple_restart(kernel):\n            return -GaussianProcessRegressor(kernel=kernel).log_marginal_likelihood(\n                theta, eval_gradient=True\n            )[0]\n\n        initial_theta = np.random.rand(kernel.n_dims)\n        bounds = kernel.bounds\n\n        # Call the constrained optimization with different initial values\n        for _ in range(10):\n            theta_opt, func_min = self._constrained_optimization(\n                obj_func, initial_theta, bounds\n            )\n            initial_theta += np.random.rand(kernel.n_dims) * 0.1\n", "    def test_sample_y(kernel):\n        gpr = GaussianProcessRegressor(kernel=kernel, alpha=1e-10)\n        gpr.fit(X, y)\n\n        y_samples = gpr.sample_y(X, n_samples=10)\n        n_samples_x = X.shape[0]\n        n_samples = y_samples.shape[1]\n        assert y_samples.shape == (n_samples_x, n_samples)\n        assert y_samples.shape == (n_samples_x, n_samples)\n        assert np.all(np.isfinite(y_samples))\n\n        # Check that samples have some variation\n        assert np.std(y_samples, axis=1).max() > 1e-6\n\n\n\n", "    def test_log_marginal_likelihood_gradient(kernel):\n        gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-10)\n        gp.fit(X, y)\n\n        if kernel == fixed_kernel:\n            # For fixed kernels, gradient is likely to be noisy\n            warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n        \n        log_likelihood, gradients = gp.log_marginal_likelihood(\n            eval_gradient=True, clone_kernel=True\n        )\n        # Use finite differences to check if gradients are approx correct\n        epsilon = 1e-6\n        for i in range(len(gp.kernel_.theta)):\n            theta_plus = np.copy(gp.kernel_.theta)\n            theta_plus[i] += epsilon\n            theta_minus = np.copy(gp.kernel_.theta)\n            theta_minus[i] -= epsilon\n            \n            ll_plus = gp.log_marginal_likelihood(\n                theta=theta_plus, eval_gradient=False, clone_kernel=True\n            )\n            ll_minus = gp.log_marginal_likelihood(\n                theta=theta_minus, eval_gradient=False, clone_kernel=True\n            )\n\n            finite_diff_grad = (ll_plus - ll_minus) / (2 * epsilon)\n            assert_allclose(finite_diff_grad, gradients[i], atol=1e-4)\n\n"], "sample_354": ["        def test_createsuperuser_with_custom_user_model(self):\n            with mock.patch('builtins.input', return_value=''):\n                with mock.patch('django.contrib.auth.models.User.objects.create') as mock_create:\n                    call_command('createsuperuser', verbosity=0, username='user')\n                    mock_create.assert_called_once_with(\n                        username='user',\n                        password='password',\n                        is_staff=True,\n                        is_superuser=True,\n                        email='user@example.com'\n                    )\n", "        def test_noinput_no_password(self):\n            with override_settings(\n                INSTALLED_APPS=['.']),\n                 mock.patch('sys.stdout', new_callable=StringIO), mock.patch(\n                     'django.contrib.auth.management.createsuperuser.getpass', return_value=None),\n                     mock.patch('builtins.input', return_value='newpassword'):\n                call_command('createsuperuser', '--noinput',\n                              '--%s' % CustomUser.USERNAME_FIELD, 'alice', 'newpassword')\n                stdout = sys.stdout.getvalue()\n                self.assertIn('Superuser created successfully.', stdout)\n\n", "    def test_createsuperuser_with_custom_user_model(self):\n        with mock.patch('sys.stdout', new_callable=StringIO) as stdout:\n            with mock.patch('builtins.input', lambda prompt: 'pass'):\n                call_command('createsuperuser', verbosity=0, database='default')\n        self.assertIn('Superuser created successfully.', stdout.getvalue())\n\n        # Make sure the superuser was created using the custom user model\n        user = User.objects.get(username='alice')\n        self.assertEqual(user.is_staff, True)\n        self.assertEqual(user.is_superuser, True)\n", "    def test_create_superuser_interactive_with_password(self):\n        with mock.patch('builtins.input', side_effect=['alice', 'password', 'password']):\n            call_command('createsuperuser')\n", "    def test_m2m_field_handling(self):\n        with override_settings(\n            INSTALLED_APPS=[\n                'tests.apps.TestConfig',\n            ],\n        ):\n            call_command('createsuperuser', database='default',\n                          username='m2m_test',\n                          'some_organization__id=1'\n            )\n            user = User.objects.get(username='m2m_test')\n            assert user.organization_id == 1\n\n"], "sample_545": ["    def test_constrained_layout_engine_on_subplots(self):\n        fig, axs = plt.subplots(2, 2)\n        \n        # Create some large text labels\n        for i in range(4):\n            axs[i // 2, i % 2].text(0.5, 0.5, 'This is a long text label',\n                                    ha='center', va='center', \n                                    transform=axs[i // 2, i % 2].transAxes)\n\n        # Set constrained layout engine\n        fig.set_layout_engine(ConstrainedLayoutEngine())\n\n        # Check that all subplots are still visible\n        for ax in axs.ravel():\n            assert ax.get_visible()\n\n\n        # Get the layout dimensions\n        layout = fig.get_layout_engine().get_info()\n\n        assert layout['h_pad'] > 0\n        assert layout['w_pad'] > 0\n\n    \n", "    def test_constrained_layout_engine_duplicate_axes(self):\n        fig, axs = plt.subplots(2, 2)\n\n        # Duplicate an Axes\n        duplicate_ax = copy.copy(axs[0])\n        duplicate_ax.axis('off')\n        duplicate_ax.set_position(axs[1].get_position())\n        fig.add_axes(duplicate_ax)\n\n        fig.set_layout_engine(ConstrainedLayoutEngine())\n        fig.tight_layout()\n        \n        # Test if the layout is correct and avoids overlap\n        assert axs[0].get_position() != axs[1].get_position()  \n\n\n\n", "    def test_constrained_layout_complex_figure(self):\n        fig = Figure(figsize=(6, 4))\n        gs = gridspec.GridSpec(4, 4)\n        ax1 = fig.add_subplot(gs[0, :2])\n        ax2 = fig.add_subplot(gs[1:, 0])\n        ax3 = fig.add_subplot(gs[1:, 1:])\n        ax1.set_title('subplot 1')\n        ax2.set_title('subplot 2')\n        ax3.set_title('subplot 3')\n\n        fig.set_layout_engine(ConstrainedLayoutEngine())\n        fig.tight_layout()\n\n        # Verify layout properties\n\n        # Check that all subplots have margins\n        for ax in [ax1, ax2, ax3]:\n            assert ax.get_position().x > 0\n            assert ax.get_position().y > 0\n            assert ax.get_position().width < 1\n            assert ax.get_position().height < 1\n\n        # Check that titles are not overlapping\n        # ... (add assertions to check for title collisions)\n\n        # Check for overall proper layout\n        self.assertFigEqual(fig, self.ref_fig)\n\n\n\n", "    def test_constrained_layout(self):\n        fig, axs = plt.subplots(2, 2, figsize=(5, 5))\n        axs[0, 0].set_title('Top Left')\n        axs[0, 1].set_title('Top Right')\n        axs[1, 0].set_title('Bottom Left')\n        axs[1, 1].set_title('Bottom Right')\n        fig.tight_layout()\n        plt.close('all')\n", "    def test_tight_layout_with_legend():\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3])\n        ax.legend(['A', 'B', 'C'], loc='upper right')\n        plt.tight_layout()\n        plt.close(fig)\n\n\n"], "sample_1150": ["    def test_complex_region_from_real(self):\n        from sympy import Interval\n\n        # Test the from_real method\n\n        unit = Interval(0, 1)\n        region = ComplexRegion.from_real(unit)\n        expected = FiniteSet(0 + 0*I, 1 + 0*I)\n        self.assertEqual(region, expected)\n        self.assertEqual(region.sets, ProductSet(unit, FiniteSet(0)))\n        region = ComplexRegion.from_real(Interval(1, 5) * Interval(2, 3))\n        expected = FiniteSet(1 + 2*I, 2 + 2*I, 3 + 2*I, 4 + 2*I, 5 + 2*I,\n                         1 + 3*I, 2 + 3*I, 3 + 3*I, 4 + 3*I, 5 + 3*I)\n        self.assertEqual(region, expected)\n\n\n\n", "    def test_complex_region_from_real(self):\n        from sympy import Interval, ComplexRegion\n        unit = Interval(0,1)\n        result = ComplexRegion.from_real(unit)\n        expected = CartesianComplexRegion(ProductSet(Interval(0, 1), FiniteSet(0)))\n        self.assertEqual(result, expected)\n\n\n", "    def test_complex_region_from_real(self):\n        from sympy import Interval, ComplexRegion\n\n        unit = Interval(0,1)\n        result = ComplexRegion.from_real(unit)\n        expected = CartesianComplexRegion(ProductSet(Interval(0, 1), FiniteSet(0)))\n        self.assertEqual(result, expected)\n\n\n        \n", "    def test_polar_complex_region_from_real(self):\n        from sympy import Interval, ComplexRegion\n\n        # Test case for converting Interval(0,1) to CartesianComplexRegion\n        unit_interval = Interval(0, 1)\n        cartesian_region = ComplexRegion.from_real(unit_interval)\n        self.assertEqual(cartesian_region.expr, x + S.ImaginaryUnit * y)\n        self.assertEqual(cartesian_region.a_interval, Interval(0, 1))\n        self.assertEqual(cartesian_region.b_interval, FiniteSet(0))\n\n\n", "    def test_complex_region_from_real(self):\n        from sympy import Interval, ComplexRegion\n\n        unit = Interval(0, 1)\n        with self.assertRaises(ValueError):\n            ComplexRegion(unit * unit)  \n\n\n\n\n"], "sample_10": ["    def test_set_column_from_pandas(self):\n        from pandas import DataFrame\n\n        df = DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})\n        t = self._table_type.from_pandas(df)\n\n        t.set_column('a', df['a'] + 1)\n        assert_array_equal(t['a'].data, [2, 3, 4])\n\n        with pytest.raises(ValueError):\n            t.set_column('c', df['a'] + 1)\n\n        t.set_column('b', [], dtype='float64', unit='m')\n        assert_array_equal(t['b'].data, [np.nan, np.nan, np.nan])\n        assert t['b'].unit == u.m\n", "    def test_set_table_column_with_info(self):\n        new_col = self._column_type(\n            [1, 2, 3], name='new_col', format='%d',\n            meta={'new_key': 'new_value'})\n\n        self.t.set_column(new_col)\n        assert 'new_col' in self.t.colnames\n        assert self.t.columns['new_col'].info.meta == {'new_key': 'new_value'}\n\n\n\n    ", "    def test_set_column_with_masked_data(self):\n        a = MaskedColumn(np.array([1, 2, 3]), mask=np.array([True, False, True]), name='a')\n        t = self._table_type(self.b, self.c)\n        t['a'] = a\n        assert_array_equal(t['a'].data, np.array([1, 2, 3]))\n        assert_array_equal(t['a'].mask, np.array([True, False, True]))\n\n\n\n", "    def test_set_column_with_metadata(self):\n        t = self._table_type([[1, 2, 3], [4, 5, 6]])\n        t.meta['some_key'] = 'some_value'\n\n        t['a'] = self.a\n        assert t.meta['a'] == {'aa': [0, 1, 2, 3, 4]}\n\n        with pytest.raises(AttributeError):\n            t['a'].meta['new_key'] = 'new_value'\n\n\n\n        with pytest.raises(AttributeError):\n            t['a'].meta = {'new_key': 'new_value'}\n\n        t['a'].info.copy_attrs(t.meta)\n        assert t['a'].info.attrs == {'aa': [0, 1, 2, 3, 4]}\n\n\n\n        t['a'].info.attrs = {'bb': [0, 1, 2, 3, 4]}\n        assert t['a'].info.attrs == {'bb': [0, 1, 2, 3, 4]}\n\n        t['a'].info.attrs = {'aa': [0, 1, 2, 3, 4]}\n        assert t['a'].info.attrs == {'aa': [0, 1, 2, 3, 4]}\n", "    def test_set_column_with_invalid_name(self):\n        with pytest.raises(ValueError):\n            self.t['/invalid/name'] = self.a\n        with pytest.raises(ValueError):\n            self.t['-invalid-name'] = self.a\n"], "sample_1109": ["    def test_frac_complex(self):\n        # Test frac with complex arguments\n        assert frac(1 + I) == I\n        assert frac(2 + 3*I) == 3*I\n        assert frac(1 + 2*I*S.ImaginaryUnit) == 2*I\n        assert frac(2*I*S.ImaginaryUnit) == 2*I\n        assert frac(-2*I*S.ImaginaryUnit) == -2*I\n        assert frac(I*oo) == I*oo\n        assert frac(I*(-oo)) == I*(-oo)\n\n        # Check for simplification\n        assert frac(2*sin(pi/2) + 3*I).rewrite(floor) == (\n            2*sin(pi/2) + 3*I) - floor(2*sin(pi/2) + 3*I)\n        assert frac(2*exp(I*pi/2) + 3*I).rewrite(floor) == (\n            2*exp(I*pi/2) + 3*I) - floor(2*exp(I*pi/2) + 3*I)\n", "    def test_frac_complex_with_integer_real_part(self):\n        from sympy import S\n        n = Symbol('n', integer=True)\n        assert frac(n + I).evalf() == 0\n        assert frac(n + I*2).evalf() == 2*I\n\n\n\n", "    def test_frac_complex_large_imag():\n        assert frac(10*I + 1).equals(10*I)\n        assert frac(10*I - 1).equals(10*I - 1)\n", "    def test_frac_real_eval():\n        assert frac(Rational(4, 3)) == Rational(1, 3)\n        assert frac(-Rational(4, 3)) == Rational(2, 3)\n        assert frac(Rational(7, 3)) == Rational(1, 3)\n        assert frac(floor(2*pi)) == 0\n        assert frac(1.23) == 0.23\n        assert frac(1.3) == 0.3\n        assert frac(3.7) == 0.7\n        assert frac(-1.23) == -0.23\n        assert frac(-1.3) == -0.3\n        assert frac(-3.7) == -0.7\n        assert frac(S(0)) == S(0)\n        assert frac(S(1)) == S(0)\n        assert frac(S(2)) == S(0)\n        assert frac(1.23456789) == 0.23456789\n", "    def test_frac_complex(self):\n        assert frac(I).evalf() == 1j\n        assert frac(I*S(2)).evalf() == 2j\n        assert frac(I + 2).evalf() == 1j + frac(2)\n        assert frac(2 + I*3).evalf() == 2 + 3j\n        assert frac(S(-2) + I*S(3)).evalf() == S(-2) + 3j\n\n\n"], "sample_717": ["    def test_fetch_lfw_pairs_subset(self):\n        pairs = fetch_lfw_pairs(subset='train')\n        assert_equal(pairs.target.shape[0], 2200)\n        assert_equal(pairs.data.shape[1], 5828)\n        assert_equal(pairs.pairs.shape[1], 2)\n        assert_equal(pairs.pairs.shape[2], 62)\n        assert_equal(pairs.pairs.shape[3], 47)\n\n        pairs_test = fetch_lfw_pairs(subset='test')\n        assert_equal(pairs_test.target.shape[0], 2200)\n        assert_equal(pairs_test.data.shape[1], 5828)\n        assert_equal(pairs_test.pairs.shape[1], 2)\n        assert_equal(pairs_test.pairs.shape[2], 62)\n        assert_equal(pairs_test.pairs.shape[3], 47)\n\n        pairs_10_folds = fetch_lfw_pairs(subset='10_folds')\n        assert_equal(pairs_10_folds.target.shape[0], 2200)\n        assert_equal(pairs_10_folds.data.shape[1], 5828)\n        assert_equal(pairs_10_folds.pairs.shape[1], 2)\n        assert_equal(pairs_10_folds.pairs.shape[2], 62)\n        assert_equal(pairs_10_folds.pairs.shape[3], 47)\n\n\n\n    \n\n", "compilation error", "def test_lfw_pairs_subset():\n    data_home = SCIKIT_LEARN_EMPTY_DATA\n    for subset in ['train', 'test', '10_folds']:\n        with assert_raises(ValueError,\n                          message=\"subset='%s' is invalid\" % subset):\n            fetch_lfw_pairs(subset=subset, data_home=data_home)\n\n\n\n", "compilation error", "compilation error"], "sample_3": ["    def test_ecsv_masked_values(self):\n        table = simple_table()\n        table['a'] = np.array([1, 2, 3], dtype=np.float64)\n        table['b'] = np.array([1.1, None, 3.3], dtype=np.float64)\n        table['b'].mask = [False, True, False]\n\n        with StringIO() as f:\n            ascii.write(table, f, format='ascii.ecsv')\n            content = f.getvalue()\n\n        table_read = ascii.read(content, format='ascii.ecsv')\n        assert (table_read['b'] == table['b']).all()\n        assert (table_read['b'].mask == table['b'].mask).all()\n\n\n\n\n", "    def test_ecsv_mixin_columns(self):\n        \"\"\"\n        Test the handling of mixin columns in ECSV.\n        \"\"\"\n        table = T_DTYPES.copy()\n        # Add a mixin column\n        table['mixin'] = mixin_cols('mixin', 'int64', description='mixin col')\n        # Serialize as ECSV and read back\n        with open(os.path.join(TEST_DIR, 'test.ecsv'), 'w') as f:\n            ascii.write(table, f, format='ascii.ecsv')\n        in_table = ascii.read(os.path.join(TEST_DIR, 'test.ecsv'), format='ascii.ecsv')\n\n        # Compare original and loaded tables\n        compare_attrs(table, in_table, ['mixin'])\n\n\n        with open(os.path.join(TEST_DIR, 'test.ecsv'), 'w') as f:\n            ascii.write(table, f, format='ascii.ecsv')\n        in_table = ascii.read(os.path.join(TEST_DIR, 'test.ecsv'), format='ascii.ecsv')\n\n        # Compare original and loaded tables\n        compare_attrs(table, in_table, ['mixin'])\n\n\n\n", "    def test_ecsv_masked_data(self):\n        masked_table = simple_table(SIMPLE_LINES + [\n            'a b c',\n            '1 1.0 Null',\n            '2 2.0 Null',\n            '3 3.0 Null'])\n\n        masked_table['c'].format = 'Null'\n        masked_table['c'].astype('str')\n\n        # The masked data should end up in the `str_vals` as empty strings\n        with nullcontext():\n            warnings.filterwarnings('ignore', category=AstropyUserWarning)\n            data = ascii.read(StringIO('\\n'.join(masked_table)), format='ascii.ecsv')\n            assert (data['c'].str_vals == ['', '', ''])\n            assert all(data['c'] == ' ')\n            \n            # Reading from the string with empty strings as replacements\n            with nullcontext():\n                warnings.filterwarnings('ignore', category=AstropyUserWarning)\n                data = ascii.read(StringIO('\\n'.join(masked_table)), format='ascii.ecsv')\n\n            assert data.meta['__serialized_columns__'] == {'a': {'__class__': 'astropy.table.column.Column'}, 'b': {'__class__': 'astropy.table.column.Column'}, 'c': {'__class__': 'astropy.table.column.MaskedColumn'}}\n            assert data['c'].mask[0] \n            assert data['c'].mask[1]\n            assert not data['c'].mask[2]\n\n\n\n", "    def test_ecsv_mixed_column_types(self):\n        data = simple_table()\n        data['a'] = np.array([1, 2, 3])\n        data['b'] = np.array(['a', 'b', 'c'])\n        data['c'] = np.array([1.0, 2.0, 3.0])\n\n        from astropy.io.ascii.ecsv import Ecsv\n\n        # Write and read back, testing that mixin columns are not lost\n        with StringIO() as f:\n            Ecsv(data).write(f)\n            f.seek(0)\n            read_table = Ecsv.read(f)\n\n        assert len(read_table.columns) == 3\n        assert read_table['a'].dtype == np.int64\n        assert read_table['b'].dtype == np.str_\n        assert read_table['c'].dtype == np.float64\n        \n\n\n\n", "def test_ecsv_with_json_subtypes():\n    # Test handling of columns with JSON subtypes\n    data = {}\n    data['a'] = [[1, 2], [3, 4]]\n    data['b'] = [{'foo': 1, 'bar': 2}, {'foo': 3, 'bar': 4}]\n    data['c'] = [None, None, None]\n    t = Table(data)\n    t['a'].dtype = 'int64'\n    t['a'].subtype = 'json'\n    t['b'].dtype = 'object'\n    t['b'].subtype = 'json'\n    t['c'].dtype = 'object'\n    t['c'].subtype = 'json'\n    t.write(StringIO(), format='ascii.ecsv', overwrite=True)\n    t2 = ascii.read(StringIO(), format='ascii.ecsv')\n    assert (t == t2).all()  \n\n\n\n"], "sample_604": ["    def test_format_timestamp(self, dtype):\n        ts = pd.Timestamp(\"2023-10-26 10:20:30\")\n        x = np.array([ts], dtype=dtype)\n        assert formatting.format_timestamp(x[0]) == \"2023-10-26 10:20:30\"\n", "    def test_array_repr(\n        self, value, expected, expand_option_name", "    def test_short_numpy_repr(self):\n        a = np.array([[1, 2], [3, 4]], dtype=\"float32\")\n        assert formatting.short_numpy_repr(a) == (\n            \"[1. 2. \\n 3. 4.]\"\n        )\n        a = np.array([1, 2, 3, 4], dtype=\"int64\")\n        assert formatting.short_numpy_repr(a) == \"[1 2 3 4]\"\n        a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=\"float64\")\n        assert formatting.short_numpy_repr(a) == (\n            \"[1. 2. 3.\\n 4. 5. 6.\\n 7. 8. 9.]\"\n        )\n        with pytest.raises(ValueError):\n            formatting.short_numpy_repr(np.array([1, 2, 3], dtype=\"complex128\"))\n        with pytest.raises(ValueError):\n            formatting.short_numpy_repr(np.array([1, 2, 3], dtype=\"object\"))\n", "    def test_dataset_repr(self, x, y, expected_summary):\n        assert formatting.dataset_repr(x) == expected_summary\n", "    def test_format_array_flat_large(self):\n        x = np.arange(1000)\n        with pytest.raises(ValueError):\n            formatting.format_array_flat(x, 1)\n        with pytest.raises(ValueError):\n            formatting.format_array_flat(x, 2)\n\n"], "sample_1073": ["compilation error", "compilation error", "    def test_sqrtdenest_cyclic_biquadratic():\n        from sympy import sqrt, Symbol, Rational\n        x = Symbol('x')\n        expr = sqrt(sqrt(x**2 + 1) + x)\n        res = sqrtdenest(expr)\n        assert res == sqrt(x + sqrt(x**2 + 1)) / sqrt(2) + sqrt(x - sqrt(x**2 + 1)) / sqrt(2)\n", "compilation error", "compilation error"], "sample_1177": ["    def test_unpolarify_nested_polar_lift():\n        from sympy.functions.special.polynomials import legendre\n        x = Symbol('x')\n        p = polar_lift(x)\n        expr = legendre(2, p)\n        res = unpolarify(expr)\n        assert comp(res, legendre(2, x) )\n\n\n\n", "    def test_polar_lift_of_powers():\n        assert polar_lift(2**I * 3**I) == 3*polar_lift(2**I) * polar_lift(3**I)\n        assert polar_lift( (2 + 3)**I) != 2**I + 3**I\n        assert polar_lift(2**I * 3**I) !=  polar_lift(2*3)**I\n", "    def test_polar_lift_evalf():\n        from sympy import polar_lift, exp_polar, I, pi\n        x = polar_lift(2 + I)\n        assert x.evalf(3) == 2.000 + 1.000*I\n        y = polar_lift(exp_polar(3*I*pi))\n        assert y.evalf(3) == 0.000 + 0.000*I\n        z = polar_lift(exp_polar(2*I*pi))\n        assert z.evalf(3) == 1.000 + 0.000*I\n        w = polar_lift(-2)\n        assert w.evalf(3) == -2.000 + 0.000*I\n        v = polar_lift(-2 - I)\n        assert v.evalf(3) == -2.000 - 1.000*I", "def test_polar_lift_and_unpolarify():\n    from sympy import polar_lift, unpolarify, I, pi\n    assert polar_lift(4) == 4*exp_polar(0)\n    assert polar_lift(-4) == 4*exp_polar(I*pi)\n    assert polar_lift(4*I) == 4*exp_polar(I*pi/2)\n    assert unpolarify(polar_lift(4)) == 4\n    assert unpolarify(polar_lift(-4)) == -4\n    assert unpolarify(polar_lift(4*I)) == 4*I\n    assert unpolarify(polar_lift(exp_polar(5*I*pi))) == 4*I\n    assert unpolarify(polar_lift(exp_polar(5*I*pi))) == 4*I\n    assert unpolarify(polar_lift(4*I + 3)) == 4*I + 3\n    assert unpolarify(polar_lift(3 + 4*I)) == 3 + 4*I\n    assert unpolarify(polar_lift(exp_polar(5*I*pi + 2*pi))) == exp_polar(5*I*pi)\n\n\n", "def test_periodic_argument_large_periods():\n    x = Symbol('x')\n    assert periodic_argument(exp_polar(5*I*pi), 2*pi) == pi\n    assert periodic_argument(exp_polar(5*I*pi), 4*pi) == pi\n    assert periodic_argument(exp_polar(5*I*pi), 10*pi) == pi\n    assert periodic_argument(exp_polar(5*I*pi), oo) == 5*pi\n"], "sample_300": ["    def test_names_to_path_on_related_fields(self):\n        self.assertEqual(Query._names_to_path(\n            'author__name', Author._meta\n        ), ('author', 'name'))\n        self.assertEqual(Query._names_to_path(\n            'author__books__title', Author._meta\n        ), ('author', 'books', 'title'))\n        self.assertEqual(Query._names_to_path(\n            'item__authors__name', Item._meta\n        ), ('item', 'authors', 'name'))\n\n\n", "    def test_order_by_complex(self):\n        # Test ordering with F expressions, aggregates, and field names\n        q = Ranking.objects.order_by(\n            F('score') * F('weight'),\n            '-items__author__first_name',\n            Func('LOWER', 'title')\n        )\n        self.assertEqual(q.order_by, [\n            ('score__score', 'MUL'),\n            ('score__weight', 'ASC'),\n            ('items__author__first_name', 'DESC'),\n            ('title', 'ASC')\n        ])\n\n", "    def test_trim_joins(self):\n        from django.db.models.sql.query import Query\n\n\n        class TestModel(models.Model):\n            name = models.CharField(max_length=100)\n            related_model = models.ForeignKey(\"RelatedModel\", on_delete=models.CASCADE)\n\n        class RelatedModel(models.Model):\n            name = models.CharField(max_length=100)\n\n        q = Query()\n        q.alias_map = {\n            'TestModel': TestModel._meta.db_table,\n            'RelatedModel': RelatedModel._meta.db_table\n        }\n        q.add_filter(TestModel.name.eq(\"test\"), AND)\n        q.add_filter(TestModel.related_model.name.eq(\"test\"), AND)\n\n        result_trimmed_prefix, contains_louter = q.trim_start(\n            [(TestModel.name,\n              [TestModel.name, TestModel.related_model.name,\n               TestModel.related_model.related_model.name])]\n        )\n\n        self.assertEqual(result_trimmed_prefix, 'related_model__name')\n        self.assertFalse(contains_louter)\n\n\n\n", "    def test_values_select(self):\n        q = Ranking.objects.values(\n            'id', 'item__name', 'user__username', 'score'\n        )\n        self.assertEqual(q.query.select, ('Ranking.id', 'Item__name', 'User__username', 'Ranking.score'))\n        self.assertEqual(q.query.values_select, ('id', 'item__name', 'user__username', 'score'))\n\n\n", "    def test_get_loaded_field_names(self):\n        ranking = Ranking.objects.filter(\n            rank__gt=5,\n        )\n        results = ranking.get_loaded_field_names()\n        self.assertEqual(results, {'auth_field': None})\n\n        ranking = Ranking.objects.filter(\n            content__icontains='test',\n            rank__gt=5,\n        )\n        results = ranking.get_loaded_field_names()\n        self.assertEqual(results, {'auth_field': None, 'content': None})\n"], "sample_149": ["    def test_check_user_model_no_auth_backend_warning(self):\n        with override_settings(\n            AUTH_USER_MODEL='auth_tests.CustomUserNonUniqueUsername',\n            AUTHENTICATION_BACKENDS=['not_a_real_backend'],\n        ):\n            errors = check_user_model(self.apps)\n            self.assertEqual(\n                len(errors), 1,\n                \"Expected one warning, got: {}\".format(errors)\n            )\n            expected_message = (\n                \"'auth_tests.CustomUserNonUniqueUsername.username' is named as the \"\n                \"'USERNAME_FIELD', but it is not unique.\"\n            )\n            self.assertIn(expected_message, str(errors[0]))\n", "    def test_check_user_model_wrong_required_fields_type(self):\n        with self.assert_num_errors(1):\n            check_user_model(app_configs=self.apps, AUTH_USER_MODEL='auth_tests.CustomUserNonListRequiredFields')\n", "    def test_user_model_is_anonymous_and_is_authenticated_are_attributes(self):\n        with self.subTest('is_anonymous'):\n            with self.subTest('is_authenticated'):\n                user = CustomUserNonUniqueUsername()\n                self.assertIsAttribute(user.is_anonymous)\n                self.assertIsAttribute(user.is_authenticated)\n", "    def test_check_user_model_non_unique_username(self):\n        with override_settings(AUTH_USER_MODEL='auth_tests.CustomUserNonUniqueUsername'):\n            errors = check_user_model()\n            self.assertEqual(len(errors), 1)\n            self.assertEqual(errors[0].id, 'auth.E003')\n", "    def test_custom_user_model_is_anonymous_method(self):\n        with self.assertLogs(level='CRITICAL') as cm:\n            check_user_model(app_configs=self.apps)\n        self.assertEqual(len(cm.records), 1)\n        self.assertIn('Ignoring this is a security issue as anonymous users will be treated as authenticated!', cm.records[0].msg)\n"], "sample_402": ["    def test_should_redirect_with_slash_append(self):\n        request = self.rf.get(\"/path/to/page\")\n        self.assertFalse(CommonMiddleware.should_redirect_with_slash(request))\n\n        request = self.rf.get(\"/path/to/page/\")\n        self.assertFalse(CommonMiddleware.should_redirect_with_slash(request))\n\n        request = self.rf.get(\"/path/to/page?param=value\")\n        self.assertFalse(CommonMiddleware.should_redirect_with_slash(request))\n\n        urlconf = [\n            (r\"^/path/to/page/$\", \"my_view\"),\n        ]\n        setattr(request, \"urlconf\", urlconf)\n        request = self.rf.get(\"/path/to/page\")\n        self.assertTrue(CommonMiddleware.should_redirect_with_slash(request))\n", "    def test_should_redirect_with_slash_true(self):\n        request = self.rf.get(\"/foo\")\n        self.assertTrue(CommonMiddleware().should_redirect_with_slash(request))\n", "    def test_should_redirect_with_slash_append(self):\n        request = self.rf.get(\"/test/\")\n        self.assertFalse(CommonMiddleware().should_redirect_with_slash(request))\n\n        request = self.rf.get(\"/test\")\n        self.assertTrue(CommonMiddleware().should_redirect_with_slash(request))\n", "    def test_should_redirect_with_slash_trailing_slash(self):\n        request = self.rf.get(\"/some/path/\")\n        self.assertFalse(self.middleware.should_redirect_with_slash(request))\n\n        request = self.rf.get(\"/some/path\")\n        self.assertTrue(self.middleware.should_redirect_with_slash(request))\n", "    def test_should_redirect_with_slash_trailing_slash(self):\n        request = self.rf.get(\"/some/path/\")\n        self.assertFalse(self.middleware.should_redirect_with_slash(request))\n        \n        request = self.rf.get(\"/some/path\")\n        self.assertTrue(self.middleware.should_redirect_with_slash(request))\n"], "sample_1042": ["    def test_contraction_structure(self):\n        A = IndexedBase('A', shape=(3, 4))\n        i, j, k, l = map(Idx, 'ijkl')\n\n        self.assertEqual(get_indices(A[i, j, k, l]), {(i,): A[i, j, k, l]})\n        self.assertEqual(get_contraction_structure(A[i, j, k, l]),\n                         {(l,): {A[i, j, k, l]}})\n\n        self.assertEqual(get_indices(A[i, j, l]), {(i, j, l): A[i, j, l]})\n        self.assertEqual(get_contraction_structure(A[i, j, l]),\n                         { (l,): {A[i, j, l]}})\n\n        self.assertEqual(get_indices(A[i, j] * A[j, k]), {(i, j, k): A[i, j] * A[j, k]})\n        self.assertEqual(get_contraction_structure(A[i, j] * A[j, k]),\n                         { (j,): {A[i, j] * A[j, k] }})\n\n\n", "    def test_invalid_shape(self):\n        from sympy import IndexedBase\n        IndexedBase('A', shape=(x, y, z),  )\n", "    def test_shape_with_indices(self):\n        from sympy.tensor import Indexed, IndexedBase\n        m, n, o, p = symbols('m n o p', integer=True)\n        A = IndexedBase('A', shape=(m, n))\n        i = Idx('i', m)\n        j = Idx('j', n)\n        k = Idx('k', o)\n        l = Idx('l', p)\n        self.assertEqual(A[i, j].shape, (m, n))\n        self.assertEqual(A[i, j, k].shape, (m, n, o))\n        self.assertEqual(A[i, j, k, l].shape, (m, n, o, p))\n", "    def test_Idx_with_symbolic_bounds():\n        n, m, i, j, k = symbols('n m i j k', integer=True)\n        idx = Idx(i, (n - 1, m + 2))\n        assert idx.lower == n - 1\n        assert idx.upper == m + 2\n        idx = Idx(i, 2*n)\n        assert idx.lower == 0\n        assert idx.upper == 2*n - 1\n", "    def test_shape_with_dim_symbols():\n        from sympy.abc import m, n\n\n        A = IndexedBase('A', shape=(m, n))\n        A[Idx('i', m), Idx('j', n)].shape\n        assert (m, n) == A[Idx('i', m), Idx('j', n)].shape\n"], "sample_706": ["    def test_parentheses():\n        assert evaluate(\"(True)\", lambda x: True)\n        assert evaluate(\"(False)\", lambda x: False)\n        assert evaluate(\"(ident1 or ident2)\", lambda x: x == \"ident2\") is True\n        with pytest.raises(ParseError):\n            evaluate(\"(True or\", lambda x: True)\n\n\n\n", "    def test_parens_around_ident(matcher):\n        assert evaluate(\"(ident)\", matcher) == matcher(\"ident\")\n\n", "    def test_parentheses():\n        assert evaluate(\"((True))\", lambda x: True) is True\n        assert evaluate(\"((False))\", lambda x: True) is False\n        assert evaluate(\"(True and False)\", lambda x: True) is False\n        assert evaluate(\"((True or False))\", lambda x: True) is True\n\n", "    def test_parens():\n        assert evaluate(\"(True and True)\", lambda x: True) is True\n        assert evaluate(\"(True or False)\", lambda x: True) is True\n        assert evaluate(\"(True and False)\", lambda x: True) is False\n        assert evaluate(\"(not True)\", lambda x: True) is False\n        with pytest.raises(ParseError):\n            evaluate(\"(\", lambda x: True)\n        with pytest.raises(ParseError):\n            evaluate(\"invalid_parenthesis\", lambda x: True)\n", "compilation error"], "sample_1100": ["    def test_as_content_primitive_complex():\n        from sympy import sqrt, I, S\n\n        # test complex numbers\n        assert sqrt(S(2) + 2*I).as_content_primitive() == (S(2), sqrt(1 + I))\n        assert sqrt(S(1) + I).as_content_primitive() == (1, sqrt(1 + I))\n        assert sqrt(S(1) + 2*I).as_content_primitive() == (1, sqrt(1 + 2*I))\n", "    def test_as_content_primitive_rational_in_exponent():\n        from sympy.functions import sqrt\n        assert sqrt(4 + 4*sqrt(2)).as_content_primitive() == (2,sqrt(1 + sqrt(2)))\n        assert sqrt(3 + 3*sqrt(2)).as_content_primitive() == (1,sqrt(3)*sqrt(1 + sqrt(2)))\n        assert (2*x + 2)**2.as_content_primitive() == (4, (x + 1)**2)\n        assert (4**((1 + y)/2)).as_content_primitive() == (2, 4**(y/2))\n        assert (3**((1 + y)/2)).as_content_primitive() == (1, 3**((y + 1)/2))\n        assert (3**((5 + y)/2)).as_content_primitive() == (9, 3**((y + 1)/2))\n        assert ((2 + 2*x)**y).as_content_primitive() == (1, (2*(x + 1))**y)\n\n\n\n\n", "    def test_as_content_primitive_with_sqrt_and_rational_base():\n        from sympy import sqrt\n        eq = sqrt(4 + 4*sqrt(2))\n        r, expr = eq.as_content_primitive(radical=True)\n        assert r == 2\n        assert expr == sqrt(1 + sqrt(2))\n\n\n\n\n", "    def test_as_content_primitive_rational_power():\n        from sympy.abc import x, y\n        eq = 2**((1 + y)/2)\n        r, s = eq.as_content_primitive()\n        assert r == 2\n        assert s == 2**(y/2)\n", "    def test_as_content_primitive_radicals():\n        from sympy import sqrt,  \n        assert sqrt(4 + 4*sqrt(2)).as_content_primitive(radical=True) == (2, sqrt(1 + sqrt(2)))\n        assert sqrt(3 + 3*sqrt(2)).as_content_primitive(radical=True) == (1, sqrt(3)*sqrt(1 + sqrt(2)))\n        \n        assert sqrt(2 + 2*sqrt(3)).as_content_primitive(radical=True) == (2, sqrt(1 + sqrt(3)))\n        assert sqrt(2*sqrt(3) + 3).as_content_primitive(radical=True) == (1, sqrt(2*sqrt(3) + 3)) \n        \n"], "sample_497": ["    def test_integer_data(self):\n        for vmin, vmax, ticks, expected in self.integer_data:\n            ax = plt.axes()\n            locator = mticker.MaxNLocator(nbins=2)  # Ensure enough ticks\n            ticks = locator(ax)\n            assert_array_equal(ticks, expected)\n            plt.close(ax)\n\n\n\n", "    def test_maxNLocator_integer(self, vmin, vmax, _, expected):\n        locator = mticker.MaxNLocator(8)\n        ticks = locator.tick_locs(vmin, vmax)\n        assert_array_equal(ticks, expected)", "    def test_integers(self, vmin, vmax, expected):\n        ax = plt.axes()\n        ax.set_xlim(vmin, vmax)\n        locator = mticker.MaxNLocator(n=5)\n        ticks = locator(ax)\n        if expected is not None:\n            assert_array_equal(ticks, expected)\n        else:\n            assert len(ticks) == 5\n\n        plt.close(ax)\n", "    def test_maxNLocator_integer_data(self, request):\n        integer_data = self.integer_data\n        for vmin, vmax, ticks, expected in integer_data:\n            with warnings.catch_warnings():\n                warnings.simplefilter('ignore')\n                locator = mticker.MaxNLocator(nbins=5)\n                ax = plt.axes()\n                ax.set_xlim(vmin, vmax)\n                if ticks is not None:\n                    ax.set_xticks(ticks)\n                ax.yaxis.set_major_locator(locator)\n                ticks_ = ax.yaxis.get_major_ticks()\n        assert len(ticks_) == expected.size\n        assert_array_equal(ticks_, expected)\n", "        def test_max_n_locator_basic(vmin, vmax, expected):\n            locator = mticker.MaxNLocator(nbins=5)\n            ticks = locator(np.arange(vmin, vmax, 0.01))\n            if expected is not None:\n                assert_array_equal(ticks, expected)\n            else:\n                assert len(ticks) == 5\n"], "sample_518": ["    def test_patch_patch_args(self):\n        fig, ax = plt.subplots()\n        arrow = FancyArrowPatch(\n            (0.2, 0.5), (0.8, 0.5),\n            arrowstyle=\"-\", mutation_scale=20,\n            patchA=mpatches.Rectangle((0, 0), 0.1, 0.1, facecolor='red'),\n            patchB=mpatches.Rectangle((0, 0), 0.1, 0.1, facecolor='blue')\n        )\n        ax.add_artist(arrow)\n        plt.xlim(0, 1)\n        plt.ylim(0, 1)\n\n\n", "    def test_arrowstyle_mutation(self):\n        fig, ax = plt.subplots()\n        p = FancyArrowPatch((0, 0), (1, 1),\n                            arrowstyle=\"Fancy\", mutation_scale=2)\n        ax.add_artist(p)\n\n        ax.set_xlim(-0.2, 1.2)\n        ax.set_ylim(-0.2, 1.2)\n        fig.canvas.draw()\n        # Check that the shape is as expected\n        expected_verts = np.array([\n            [0.0, 0.0],\n            [0.8, 0.0],\n            [0.9, 0.2],\n            [0.9, 0.8],\n            [0.8, 1.0],\n            [0.0, 1.0],\n        ])\n\n        assert_array_equal(p.get_path().vertices, expected_verts)\n\n\n\n", "    def test_bbox_transform(self):\n        fig, ax = plt.subplots()\n        x, y = 0.5, 0.5\n        w, h = 0.2, 0.3\n        bbox = Bbox([x, y, x + w, y + h])\n        p = mpl.patches.Rectangle((x, y), w, h, color='red')\n        r = ax.add_patch(p)\n        \n        ax.set_xlim(0, 1)\n        ax.set_ylim(0, 1)\n        ax.set_aspect('equal')\n\n        r.set_bbox(bbox)\n        ax.draw()\n\n\n", "    def test_fancyarrowpatch_bbox_transmuter(self):\n        fig, ax = plt.subplots()\n\n        arrow = FancyArrowPatch((0, 0), (1, 1), arrowstyle=\"->\",\n                               mutation_scale=1, mutation_aspect=0.5)\n        ax.add_patch(arrow)\n        plt.savefig(self.savefig_file)\n\n", "    def test_path_patch(self):\n        fig, ax = plt.subplots()\n\n        path = mpath.path.Path([[0, 0], [1, 0], [1, 1], [0, 1], [0, 0]],\n                            closed=True)\n        patch = mpatches.PathPatch(path, facecolor='none',\n                                   edgecolor='red', linewidth=2)\n\n        ax.add_patch(patch)\n\n        ax.set_xlim(0, 2)\n        ax.set_ylim(0, 2)\n        ax.set_aspect('equal')\n\n\n\n"], "sample_734": ["    def test_fowlkes_mallows_score_random_true():\n        n_samples = 100\n        labels_true = np.random.randint(0, 5, size=n_samples)\n        labels_pred = np.random.randint(0, 5, size=n_samples)\n        score = fowlkes_mallows_score(labels_true, labels_pred)\n        assert_all_finite(score)\n\n\n\n", "    def test_fowlkes_mallows_score_empty_labels_array(self):\n        with assert_raise_message(ValueError, \"'labels_true' and 'labels_pred' arrays must not be empty\"):\n            fowlkes_mallows_score([], [])\n\n\n\n", "    def test_fowlkes_mallows_score():\n        # Test with empty input\n        with assert_raise_message(ValueError, \"Input is empty\"):\n            fowlkes_mallows_score([], [])\n        # Test with same clusters\n        assert_almost_equal(fowlkes_mallows_score([0, 0, 1, 1], [0, 0, 1, 1]),\n                            1.0)\n        # Test with different clusters\n        assert_almost_equal(fowlkes_mallows_score([0, 0, 0, 0], [1, 1, 1, 1]),\n                            0.0)\n\n\n\n", "    def test_fowlkes_mallows_score():\n        # Test with all classes having the same size\n        labels_true = np.array([0, 0, 1, 1, 2, 2, 3, 3])\n        labels_pred = np.array([0, 1, 1, 0, 2, 3, 3, 2])\n        expected = 0.5\n        score = fowlkes_mallows_score(labels_true, labels_pred)\n        assert_almost_equal(score, expected)\n\n        # Test with sparse contingency matrix\n        labels_true = np.array([0, 0, 1, 1, 2, 2, 3, 3])\n        labels_pred = np.array([0, 1, 1, 0, 2, 3, 3, 2])\n        score = fowlkes_mallows_score(labels_true, labels_pred, sparse=True)\n        assert_almost_equal(score, fowlkes_mallows_score(labels_true, labels_pred))\n\n        # Test with empty labels\n        labels_true = np.array([])\n        labels_pred = np.array([])\n        score = fowlkes_mallows_score(labels_true, labels_pred)\n        assert_equal(score, 1.0)\n\n        # Test with different sizes of classes\n        labels_true = np.array([0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 4, 4])\n        labels_pred = np.array([0, 1, 1, 0, 2, 3, 3, 2, 4, 4, 4, 4])\n        score = fowlkes_mallows_score(labels_true, labels_pred)\n        assert_almost_equal(score, 0.5)\n\n\n\n        \n", "compilation error"], "sample_237": ["    def test_user_model_required_fields_already_includes_username(self):\n        with self.subTest(\n            app_configs=[\n                app_config\n                for app_config in self.apps.get_app_configs()\n                if app_config.label == 'auth_tests'\n            ],\n        ):\n            errors = check_user_model(app_configs=self.apps.get_app_configs())\n            self.assertCountEqual(\n                errors,\n                [\n                    checks.Error(\n                        \"The field named as the 'USERNAME_FIELD' \"\n                        \"for a custom user model must not be included in 'REQUIRED_FIELDS'.\",\n                        hint=(\n                            \"The 'USERNAME_FIELD' is currently set to '%s', you \"\n                            \"should remove '%s' from the 'REQUIRED_FIELDS'.\"\n                            % (\n                                CustomUserNonUniqueUsername.USERNAME_FIELD,\n                                CustomUserNonUniqueUsername.USERNAME_FIELD,\n                            )\n                        ),\n                        obj=CustomUserNonUniqueUsername,\n                        id='auth.E002',\n                    ),\n                ],\n            )", "    def test_check_user_model_non_unique_username_warning(self):\n        with override_settings(AUTH_USER_MODEL='auth_tests.CustomUserNonUniqueUsername'):\n            errors = check_user_model()\n            self.assertEqual(len(errors), 1)\n            error = errors[0]\n            self.assertEqual(error.id, 'auth.W004')\n            self.assertEqual(error.obj.__class__.__name__, 'CustomUserNonUniqueUsername')\n", "    def test_check_user_model_non_unique_username(self):\n        with self.subTest('username_field_not_unique'):\n            errors = check_user_model(app_configs=self.apps.get_app_configs('auth_tests'))\n            self.assertEqual(len(errors), 1)\n            self.assertEqual(errors[0].id, 'auth.E003')\n\n\n", "    def test_check_user_model_non_unique_username(self):\n        with self.subTest(model='auth_tests.CustomUserNonUniqueUsername'):\n            errors = check_user_model(app_configs=self.apps.get_app_configs('auth_tests'))\n            self.assertEqual(len(errors), 1)\n            self.assertEqual(errors[0].id, 'auth.E003')\n", "    def test_check_user_model_non_unique_username(self):\n        with self.subTest(field_name=CustomUserNonUniqueUsername.username):\n            errors = check_user_model(\n                app_configs=self.apps.get_app_configs('auth_tests'),\n            )\n            self.assertEqual(len(errors), 1)\n            self.assertEqual(\n                errors[0].id, 'auth.E003',\n            )\n\n        with self.subTest(field_name=CustomUserNonUniqueUsername.email):\n            errors = check_user_model(\n                app_configs=self.apps.get_app_configs('auth_tests'),\n            )\n            self.assertEqual(len(errors), 0)\n"], "sample_858": ["def test_voting_classifier_multilabel():\n    X, y = make_multilabel_classification(n_samples=100,\n                                          n_features=10,\n                                          n_classes=4,\n                                          random_state=42)\n\n    clf = VotingClassifier(estimators=[('lr', LogisticRegression()),\n                                      ('rf', RandomForestClassifier())],\n                          voting='hard')\n    with pytest.raises(NotImplementedError):\n        clf.fit(X, y)\n", "    def test_voting_classifier_multilabel(self):\n        X, y = make_multilabel_classification(n_samples=100, n_features=10,\n                                               n_classes=3, random_state=42)\n        clf = VotingClassifier(\n            estimators=[('lr', LogisticRegression(multi_class='multinomial')),\n                        ('rf', RandomForestClassifier(n_estimators=10, random_state=42))],\n            voting='hard')\n        with pytest.raises(NotImplementedError):\n            clf.fit(X, y)\n", "    def test_voting_classifier_fit_predict_probas(\n            self, estimator_type, voting, weights):\n        estimators = [\n            ('clf1', estimator_type()),\n            ('clf2', estimator_type()),\n            ('clf3', estimator_type())\n        ]\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=0.3, random_state=42)\n        eclf = VotingClassifier(\n            estimators=estimators, voting=voting, weights=weights)\n        eclf.fit(X_train, y_train)\n        y_pred = eclf.predict(X_test)\n        y_pred_proba = eclf.predict_proba(X_test)\n\n        if voting == 'soft':\n            assert_array_almost_equal(\n                y_pred_proba.sum(axis=1), np.ones_like(y_pred_proba[:, 0]))\n            assert_array_equal(y_pred, np.argmax(y_pred_proba, axis=1))\n        else:\n            assert_array_almost_equal(y_pred_proba, None)\n\n\n", "    def test_voting_classifier_soft_weights(self):\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=0.5, random_state=42)\n\n        clf1 = LogisticRegression(multi_class='multinomial', random_state=1)\n        clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n        clf3 = GaussianNB()\n\n        # Test with weighted voting\n        voting_clf = VotingClassifier(\n            estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n            voting='soft', weights=[1, 2, 1]\n        )\n        voting_clf.fit(X_train, y_train)\n        y_pred = voting_clf.predict(X_test)\n        assert_array_equal(y_pred, voting_clf.predict(X_test))\n\n        # Check that the individual classifiers have been fitted\n        for name, clf in voting_clf.named_estimators_.items():\n            assert clf.classes_ is not None\n            assert clf.n_features_ is not None\n\n", "    def test_voting_estimator_validation(self, estimator_type):\n        class TestEstimator(BaseEstimator, estimator_type):\n                pass\n\n                pass\n\n                pass\n\n        estimators = [(f'estimator-{i}', TestEstimator()) for i in range(3)]\n        if estimator_type is ClassifierMixin:\n            VotingClassifier(estimators=estimators, voting='hard',\n                             weights=[1, 2, 3])\n        else:\n            VotingRegressor(estimators=estimators, weights=[1, 2, 3])\n\n        # The call to fit should not raise\n        # ValueError(\"Number of `estimators` and weights must be equal\")\n    \n    \n"], "sample_79": ["    def test_pluralize_with_comma(self):\n        self.check_values(\n            (0, 'candies'),\n            (1, 'candy'),\n            (2, 'candies'),\n        )\n", "    def test_pluralize_with_comma_sep(self):\n        self.check_values(\n            (0, 'candies'),\n            (1, 'candy'),\n            (2, 'candies'),\n        )\n", "compilation error", "    def test_comma_separation(self):\n        self.check_values(\n            (0, 'candies'),\n            (1, 'candy'),\n            (2, 'candies'),\n        )\n", "compilation error"], "sample_273": ["    def test_duplicate_table_name_with_auto_id(self):\n        with override_settings(DATABASE_ROUTERS=[EmptyRouter()]):\n            class MyModel(models.Model):\n                id = models.AutoField(primary_key=True)\n                name = models.CharField(max_length=100)\n\n            # Create a second model with the same table name and different fields\n            with self.assertRaises(checks.Error):\n                class DuplicateModel(models.Model):\n                    id = models.AutoField(primary_key=True)\n                    email = models.EmailField(unique=True)\n                    some_data = models.TextField()\n                \n                checks.run_checks(\n                    [],\n                    'check_framework',\n                    models.get_models()\n                )    \n\n\n\n", "    def test_check_constraints_with_condition(self):\n        class MyModel(models.Model):\n            field1 = models.CharField(max_length=255)\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(\n                        check=models.Q(field1__startswith='!')\n                    )\n                ]\n\n        # Check that the warning is raised\n        with self.capture_stderr() as output:\n            checks.run_checks(\n                [MyModel],\n            )\n\n        self.assertIn(\n            \"unique constraints with conditions\", output.getvalue()\n        )\n\n\n\n", "    def test_model_unpickle_with_deferred_fields(self):\n        class DeferredModel(models.Model):\n            name = models.CharField(max_length=100)\n\n            class Meta:\n                defer = ['name']\n\n        instance = DeferredModel(name='test')\n        instance.save()\n\n        # Unpickle the instance.\n        unpickled_instance = model_unpickle(instance)\n        self.assertEqual(unpickled_instance.name, instance.name)\n\n\n", "    def test_check_duplicate_db_table_names(self):\n        with override_settings(DATABASE_ROUTERS={'default': EmptyRouter()}):\n            class MyModel(models.Model):\n                pass\n\n            with self.assertRaises(Error) as context:\n                apps.register_model('my_app', 'MyModel', MyModel)\n                checks.model_checks.check_all_models(apps, [], 'my_app')\n            self.assertIn(\n                \"Table name 'myapp_mymodel' is already in use.\",\n                str(context.exception),\n            )\n\n\n", "    def test_check_db_router_compatibility(self):\n        class MyModel(models.Model):\n            pass\n\n        with mock.patch('django.db.router.get_default_router') as router_mock:\n            router_mock.return_value = EmptyRouter()\n\n            # Set up an app to mimic a registered app.\n            self.apps.register_model('app_name', 'MyModel', [MyModel])\n\n            with override_settings(DATABASE_ROUTERS=['app_name.routers.MyRouter']):\n                with self.assertLogs() as logs:\n                    from django.db import router\n\n                    router.allow_migrate_model('default', MyModel)  \n\n        self.assert_matches_log_output(\n            logs,\n            [\n                'WARNING:Router does not support migration for database \"default\".',\n            ]\n        )\n\n\n\n"], "sample_423": ["    def test_get_model_state_with_complex_options(self):\n        # Test for various complex options like proxy, unmanaged, db_table\n        self.assertEqual(get_model_state(\"testapp\", \"AuthorProxy\"), self.author_proxy)\n        self.assertEqual(get_model_state(\"testapp\", \"AuthorProxy\", {\"proxy\": False}), self.author_proxy_notproxy)\n        self.assertEqual(get_model_state(\"testapp\", \"AuthorUnmanaged\", {\"managed\": True}), self.author_unmanaged_managed)\n        self.assertEqual(\n            get_model_state(\n                \"testapp\", \"Author\", {\"db_table\": \"author_one\", \"permissions\": [(\"can_hire\", \"Can hire\")]}\n            ),\n            self.author_with_db_table_options,\n        )\n        self.assertEqual(\n            get_model_state(\n                \"testapp\", \"NewAuthor\", {\"db_table\": \"author_three\"}\n            ),\n            self.author_renamed_with_new_db_table_options,\n        )\n        self.assertEqual(\n            get_model_state(\"testapp\", \"Author\", {\"db_table\": \"author_two\"}),\n            self.author_with_new_db_table_options,\n        )\n\n\n\n", "    def test_duplicate_unique_together(self):\n        with self.assertRaises(OperationalError):\n            Book.objects.create(\n                author = author_instance.id,\n                title = \"Book Title\",\n                newfield = 10,\n            )\n        with self.assertRaises(OperationalError):\n            Book.objects.create(\n                author = author_instance.id,\n                title = \"Book Title\",\n                newfield2 = 20,\n            )\n\n", "    def test_model_state_unique_together_3(self):\n        self.assertField(book_unique_together_3, \"newfield\", models.IntegerField())\n        self.assertField(book_unique_together_3, \"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE))\n        self.assertField(book_unique_together_3, \"title\", models.CharField(max_length=200))\n        self.assertUniqueTogether(book_unique_together_3, [(\"title\", \"newfield\")])\n        self.assertEqual(book_unique_together_3.bases, None) \n\n\n\n", "    def test_unique_together_constraints_generation(self):\n        # Test generation of unique_together constraints\n\n        model_states = [\n            book_unique_together,\n            book_unique_together_2,\n            book_unique_together_3,\n            book_unique_together_4,\n        ]\n        for model_state in model_states:\n            # Check if the unique_together constraint is present\n            self.assertEqual(\n                model_state.unique_together,\n                {\n                    (\"title\", \"newfield\")\n                    if model_state is book_unique_together_3\n                    else (\"title\", \"newfield2\")\n                    if model_state is book_unique_together_4\n                    else (\"title\", \"author\")\n                },\n            )\n\n            # Check if the unique_together is reflected in the generated SQL\n            generated_sql = self.generate_sql_for_model(model_state.app_label, model_state.model_name)\n            self.assertIn(\n                f\"UNIQUE KEY (`title`, `newfield`) \", generated_sql\n                if model_state is book_unique_together_3\n                else f\"UNIQUE KEY (`title`, `newfield2`) \", generated_sql\n                if model_state is book_unique_together_4\n                else f\"UNIQUE KEY (`title`, `author`) \", generated_sql,\n            )\n\n\n", "    def test_model_state_unique_together_validation(self):\n        with self.assertRaises(IntegrityError):\n            self.execute(\n                \"\"\"\n                INSERT INTO otherapp_book (id, author_id, title) VALUES\n                (1, 1, 'Book Title 1'),\n                (2, 1, 'Book Title 1')\n                \"\"\"\n            )\n"], "sample_244": ["    def test_absolute_max(self):\n        with self.assertRaises(ValueError):\n            formset_factory(Choice, absolute_max=10, max_num=20)\n\n        # Test that absolute_max is respected\n        formset = formset_factory(Choice, absolute_max=10)(\n            {'TOTAL_FORMS': '10', 'INITIAL_FORMS': '0', 'MAX_NUM_FORMS': '10'})\n        self.assertEqual(formset.total_form_count(), 10)\n\n", "    def test_duplicate_names(self):\n        formset_data = [\n            {'name': 'Water'},\n            {'name': 'Coke'},\n            {'name': 'Water'},  # Duplicate\n            {'name': 'Juice'},\n        ]\n        formset = FavoriteDrinksFormSet(formset_data)\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(len(formset.errors), 1)\n        self.assertEqual(\n            formset.errors[0].as_text(),\n            'You may only specify a drink once.',\n        )  \n\n\n\n", "    def test_empty_form(self):\n        empty_formset = ChoiceFormSet(prefix='choices', initial=[])\n        self.assertEqual(empty_formset.total_form_count(), 0)\n        self.assertEqual(len(empty_formset.forms), 0)\n        self.assertEqual(empty_formset.initial_form_count(), 0)\n        self.assertEqual(empty_formset.extra_forms, [])\n\n        empty_form = empty_formset.empty_form\n        self.assertIsInstance(empty_form, Choice)\n        self.assertTrue(empty_form.empty_permitted)\n\n\n", "    def test_formset_factory_with_max_num(self):\n        with self.assertRaises(ValueError) as cm:\n            formset_factory(BaseForm, max_num=10, absolute_max=5)\n        self.assertIn(\"'absolute_max' must be greater or equal to 'max_num.\", str(cm.exception))\n", "    def test_formset_factory_defaults(self):\n        form = CharField()\n        formset = formset_factory(form)\n        self.assertEqual(formset.extra, 1)\n        self.assertEqual(formset.can_order, False)\n        self.assertEqual(formset.can_delete, False)\n        self.assertEqual(formset.min_num, DEFAULT_MIN_NUM)\n        self.assertEqual(formset.max_num, DEFAULT_MAX_NUM)\n"], "sample_960": ["    def test_parse_annotation_simple(self):\n        self.assertEqual(_parse_annotation('int'), ('int', None))\n        self.assertEqual(_parse_annotation('str'), ('str', None))\n        self.assertEqual(_parse_annotation('None'), ('None', None))\n        self.assertEqual(_parse_annotation('list[int]'), ('list[int]', None))\n", "    def test_parse_annotation(self):\n        assert _parse_annotation('int') == ('int', None)\n        assert _parse_annotation('int') == ('int', None)\n        assert _parse_annotation('List[int]') == ('List', ['int'])\n        assert _parse_annotation('float | Optional[str]') == ('Union', ['float', 'Optional[str]'])\n        assert _parse_annotation('typing.List[str]') == ('typing.List', ['str'])\n        assert _parse_annotation('Optional[int?]') == ('Optional', ['int?'])\n\n\n\n", "    def test_parse_annotation_simple(self):\n        assert _parse_annotation(self.env, parse('foo(a: int, b: str)'),\n                                  self.env.config).astext() == 'a: int, b: str'\n    ", "    def test_parse_annotation(self):\n        assert _parse_annotation(\"int\") == (\"int\",)\n        assert _parse_annotation(\"int, str\") == (\"int\", \"str\")\n        assert _parse_annotation(\"int -> str\") == (\n            \"int\",\n            \"->\",\n            \"str\",\n        )\n        assert _parse_annotation(\"int  -> str\") == (\n            \"int\",\n            \"->\",\n            \"str\",\n        )\n        assert _parse_annotation(\"List[int]\") == (\"List\", \"[\", \"int\", \"]\")\n        assert _parse_annotation(\"Dict[int, str]\") == (\"Dict\", \"[\", \"int\", \",\", \"str\", \"]\")\n\n\n\n", "    def test_parse_signature_simple_return(monkeypatch):\n        sig = \"some_function(arg1, arg2) -> int\"\n        expected = \"some_function(arg1, arg2) -> int\"\n        assert parse(sig) == expected\n        monkeypatch.setattr(sys, 'version_info', (3, 9, 0))  # temporarily set Python version to 3.9+\n        assert parse(sig) == expected  # Should handle Python 3.9+ without issues\n"], "sample_809": ["compilation error", "    def test_discrete_features_sparse(self):\n        X = csr_matrix(\n            [[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n        y = np.array([0, 1, 2])\n        mi = mutual_info_classif(X, y, discrete_features=True)\n        assert_array_equal(mi, np.array([0.69314718, 0.69314718, 0.69314718]))\n", "compilation error", "def test_mi_sparse_continuous_discrete():\n    rng = check_random_state(0)\n    n_samples = 100\n    n_features = 5\n    X = csr_matrix((rng.rand(n_samples, n_features), (\n        np.arange(n_samples), np.arange(n_samples * n_features))))\n    y = rng.randint(0, 2, size=n_samples)\n\n    mi = mutual_info_classif(X, y, discrete_features=False)\n    assert_array_equal(mi.shape, (n_features,))\n    assert_greater(np.sum(mi), 0.0)\n\n    mi = mutual_info_regression(X, y, discrete_features=False)\n    assert_array_equal(mi.shape, (n_features,))\n    assert_greater(np.sum(mi), 0.0)\n\n\n", "    def test_mutual_info_regression_sparse(self):\n        X = csr_matrix([[1, 2],\n                         [3, 4],\n                         [5, 6]])\n        y = np.array([0, 1, 0])\n\n        mi = mutual_info_regression(X, y, discrete_features=False)\n        assert_array_equal(mi.shape, (2,))\n        assert_greater(mi.max(), 0)\n"], "sample_1143": ["def test_tribonacci_constant():\n    tc = TribonacciConstant()\n    assert tc.is_irrational is True\n    assert tc.is_number is True\n    assert tc.is_positive is True\n    assert tc.is_algebraic is True\n    assert tc.is_transcendental is False\n    assert tc.n(20) == 1.8392867552141611326\n    assert tc.expansion().as_coeff_mul(1).as_expanded() == (1 + cbrt(19 - 3*sqrt(33)) + cbrt(19 + 3*sqrt(33)))/3\n\n", "    def test_catalan_as_sum():\n        from sympy import Sum, Dummy\n        k = Dummy('k', integer=True, nonnegative=True)\n        assert latex(Catalan) == latex(Sum((-1)**k / (2*k+1)**2, (k, 0, S.Infinity)))\n\n\n\n", "    def test_sympify_fractions():\n        x = fractions.Fraction(1, 2)\n        assert sympify(x) == Rational(1, 2)\n        assert sympify(fractions.Fraction(3, 4)) == Rational(3, 4)\n        assert sympify(fractions.Fraction(2, 1)) == Integer(2)\n", "    def test_is_positive():\n        assert not is_negative(S.NegativeInfinity)\n        assert not is_positive(S.NegativeInfinity)\n\n        assert is_positive(S.Infinity)\n        assert not is_negative(S.Infinity)\n\n        assert is_positive(S.Half)\n        assert not is_negative(S.Half)\n\n        assert is_positive(2.7)\n        assert not is_negative(2.7)\n", "    def test_complex_arithmetic():\n        from sympy import I, pi\n        assert (2 + 3*I) * (1 - I) == (2 - 3*I + 2*I - 3*I**2) == (5 - I)\n        assert (2 + 3*I)**2 == (4 + 12*I + 9*I**2) == (-5 + 12*I)\n        assert (2 + 3*I) / (1 - I) == ( (2 + 3*I)*(1 + I) ) / ((1 - I)*(1 + I)) \\\n            == (2 + 2*I + 3*I + 3*I**2) / (1 - I**2) == (-1 + 5*I) / 2\n        assert sqrt(-16) == 4*I\n        assert sin(pi/2 + I) == sin(pi/2)*cosh(1) + cos(pi/2)*sinh(1)*I == I * cosh(1)\n  \n"], "sample_282": ["    def test_complex_field_form_data(self):\n        form = ComplexFieldForm({'field1': 'test,J,P,2023-10-26 10:00:00'})\n        self.assertTrue(form.is_valid())\n        field = form.cleaned_data['field1']\n        self.assertEqual(field[0], 'test')\n        self.assertEqual(field[1], ['J'])\n        self.assertEqual(field[2], datetime(2023, 10, 26, 10, 0, 0))\n\n\n", "    def test_multi_value_field_errors(self):\n        form = ComplexFieldForm(data={'field1': 'a,X,2023-10-27 10:00:00'})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['field1'], ['This field is required.'])\n", "    def test_complex_field_bound_widget(self):\n        form = ComplexFieldForm({'field1': 'test,J,R,2023-10-26 10:00:00'})\n        field = form.fields['field1']\n        bound_field = field.bound_field\n        self.assertEqual(len(bound_field), 3)\n        self.assertEqual(bound_field[0].value(), 'test')\n        self.assertEqual(bound_field[1].value(), ['J'])\n        self.assertEqual(bound_field[2].value(), datetime(2023, 10, 26, 10, 0, 0))\n", "    def test_data_and_value_for_multivalue_field(self):\n        form = ComplexFieldForm(data={'field1': 'test,J,George,2023-10-26 10:00:00'})\n        self.assertIsInstance(form.cleaned_data['field1'], tuple)\n        self.assertEqual(form.cleaned_data['field1'], ('test', ['J'], datetime(2023, 10, 26, 10, 0, 0)))\n\n\n\n", "    def test_bound_field_value(self):\n        form = ComplexFieldForm({'field1': 'test,J,P,2023-10-27 10:00:00'})\n        bound_field = form.fields['field1']\n        self.assertEqual(bound_field.value(), ('test', ['J'], datetime.strptime('2023-10-27 10:00:00', '%Y-%m-%d %H:%M:%S')))\n"], "sample_156": ["    def test_renderer(self):\n        renderer = get_default_renderer()\n        form = Person()\n        html = renderer.render(form)\n        self.assertIn('<form', html)\n        self.assertIn('id=\"id_first_name\"', html)\n        self.assertIn('id=\"id_last_name\"', html)\n        self.assertIn('id=\"id_birthday\"', html)\n", "    def test_field_order(self):\n        class OrderedForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n            field3 = CharField()\n\n        form = OrderedForm({'field1': '1', 'field2': '2'})\n        self.assertIn('field1', form.fields)\n        self.assertIn('field2', form.fields)\n        self.assertIn('field3', form.fields)\n\n        form = OrderedForm({'field1': '1', 'field2': '2', 'field3': '3'})\n        self.assertEqual(list(form.fields), ['field1', 'field2', 'field3'])\n\n        class OrderedForm2(OrderedForm):\n            field3 = CharField(orderable=False)\n\n        form = OrderedForm2({'field1': '1', 'field2': '2', 'field3': '3'})\n        self.assertEqual(list(form.fields), ['field1', 'field2', 'field3'])\n        \n\n", "    def test_hidden_fields(self):\n        form = Person(initial={'first_name': 'John'})\n        hidden_fields = form.hidden_fields()\n        self.assertEqual(len(hidden_fields), 1)\n        self.assertEqual(hidden_fields[0].name, 'last_name')\n        self.assertEqual(hidden_fields[0].is_hidden, True)\n", "    def test_hidden_fields(self):\n        form = Person(initial={'first_name': 'Alice', 'last_name': 'Smith'})\n        hidden_fields = form.hidden_fields()\n        self.assertEqual(len(hidden_fields), 0)\n\n        form = PersonNew(initial={'first_name': 'Alice', 'last_name': 'Smith'})\n        hidden_fields = form.hidden_fields()\n        self.assertEqual(len(hidden_fields), 0)\n\n\n\n", "    def test_order_fields(self):\n        class OrderedForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n            field3 = CharField()\n\n        form = OrderedForm(field_order=['field3', 'field1', 'field2'])\n        self.assertEqual(list(form.fields), ['field3', 'field1', 'field2'])\n\n\n"], "sample_1075": ["    def test_beta_derivative_complex_numbers():\n        x = Symbol('x')\n        y = Symbol('y')\n        with raises(ArgumentIndexError):\n            diff(beta(x, y), 3)\n        assert diff(beta(x, y), x).simplify() == beta(x, y) * (digamma(x) - digamma(x + y))\n        assert diff(beta(x, y), y).simplify() == beta(x, y) * (digamma(y) - digamma(x + y))\n        assert diff(beta(1 + I, 1 + I), x).simplify() != beta(1 + I, 1 + I) * (digamma(1 + I) - digamma(2 + 2*I)) \n        assert diff(beta(1 + I, 1 + I), y).simplify() != beta(1 + I, 1 + I) * (digamma(1 + I) - digamma(2 + 2*I)) \n", "    def test_beta_finite_argument():\n        x = Symbol('x')\n        y = Symbol('y')\n        assert beta(x, y).rewrite(gamma).expand() == gamma(x)*gamma(y)/gamma(x+y)\n        assert beta(2, 3).evalf() == 0.25331679035444379\n\n\n\n", "    def test_beta_derivative(self):\n        x, y = Symbol('x'), Symbol('y')\n        f = beta(x, y)\n        assert diff(f, x) == beta(x, y)*(digamma(x) - digamma(x + y))\n        assert diff(f, y) == beta(x, y)*(digamma(y) - digamma(x + y))\n", "    def test_beta_derivative_with_complex_args():\n        x = Symbol('x', real=False)\n        y = Symbol('y', real=False)\n        with raises(ArgumentIndexError):\n            diff(beta(x, y), 3)\n        assert diff(beta(x, y), x) == beta(x, y)*(digamma(x) - digamma(x + y))\n        assert diff(beta(x, y), y) == beta(x, y)*(digamma(y) - digamma(x + y))\n", "    def test_beta_diff(self):\n        x, y = Symbol('x'), Symbol('y')\n        b = beta(x, y)\n        assert diff(b, x) == beta(x, y)*(digamma(x) - digamma(x + y))\n        assert diff(b, y) == beta(x, y)*(digamma(y) - digamma(x + y))\n"], "sample_626": ["    def test_cftime_index_creation(self):\n        dt_type = _all_cftime_date_types[0]\n        cftime_index = CFTimeIndex([dt_type(i) for i in range(5)],\n                                  offset=datetime(2023, 1, 1)).values\n\n        var = IndexVariable(\n            \"time\",\n            cftime_index,\n            attrs={\"units\": \"days since 2000-01-01\"},\n            encoding={\"cfstandard_name\": \"time\"},\n        )\n        assert isinstance(var.indexes[0], CFTimeIndex)\n        assert var.indexes[0].offset == datetime(2023, 1, 1)\n        assert var.indexes[0].dtype == dt_type\n\n\n\n", "    def test_cftime_index_variable_getitem_slice(self, dtype: Any):\n        vals = np.arange(12).astype(dtype)\n        index = CFTimeIndex(vals, units=\"days\")\n        var = IndexVariable(\n            vals,\n            dims=(\"time\",),\n            coords={\"time\": index},\n            dtype=dtype,\n        )\n\n        # Test basic slicing\n        for i in range(3):\n            assert_identical(var.isel(time=[i, i + 1, i + 2]), var[i:i + 3])\n\n        # Test single element selection\n        assert_identical(var.isel(time=0), var[0])\n\n        # Test negative slicing\n        assert_identical(var.isel(time=(-3, -2, -1)), var[-3:])\n", "    def test_pandas_multi_int_index_to_ndarrays(self):\n        m = pd.MultiIndex.from_tuples(\n            [(1, \"a\"), (1, \"b\"), (2, \"a\"), (2, \"b\")],\n            names=[\"level0\", \"level1\"],\n        )\n        idx = PandasMultiIndexingAdapter(m, level=\"level0\")\n        assert isinstance(idx, PandasMultiIndexingAdapter)\n        assert idx.level == \"level0\"\n        assert idx[0] == (1,)\n        assert idx[2] == (2,)\n", "    def test_multi_indexing_adapter_copy(self):\n        # Test that copying a PandasMultiIndexingAdapter preserves the level\n        # and the underlying data\n        idx = pd.MultiIndex.from_tuples(\n            [(\"A\", 1), (\"A\", 2), (\"B\", 1), (\"B\", 2)], names=[\"x\", \"y\"]\n        )\n        adapter = PandasMultiIndexingAdapter(idx, level=\"x\")\n        copied_adapter = adapter.copy(deep=True)\n        assert adapter.level == copied_adapter.level\n        assert adapter.array is not copied_adapter.array\n        assert adapter.__repr__() != copied_adapter.__repr__()\n\n\n\n", "    def test_pandas_indexing_adapter_slice(self, data: pd.Index):\n        adapter = PandasIndexingAdapter(data)\n        # Test simple slicing\n        assert_identical(adapter[0:2], data[0:2])\n        assert_identical(adapter[::2], data[::2])\n        assert_identical(adapter[-2:], data[-2:])\n        # Test slicing with step\n        assert_identical(adapter[::3], data[::3])\n        assert_identical(adapter[::-1], data[::-1])\n        \n        # Test slicing with MultiIndex\n        multi_index = pd.MultiIndex.from_tuples([(1, \"a\"), (2, \"b\"), (3, \"c\"), (4, \"d\")])\n        multi_adapter = PandasMultiIndexingAdapter(multi_index, level=\"level_0\")\n        assert_identical(multi_adapter[0:2], multi_index.get_level_values(\"level_0\")[0:2])\n"], "sample_444": ["    def test_manifest_hash_version(self):\n        # Test that the manifest hash is updated when a file changes\n        with tempfile.TemporaryDirectory() as temp_dir:\n            file_path = os.path.join(temp_dir, \"test.css\")\n            with open(file_path, \"w\") as f:\n                f.write(\"/* Original content */\\n\")\n\n            storage = ManifestStaticFilesStorage()\n            storage.save(\"test.css\", ContentFile(\"test content\\n\"))\n\n            manifest_content = json.loads(\n                storage.read_manifest()\n            )\n            initial_hash = manifest_content[\"hash\"]\n\n            with open(file_path, \"w\") as f:\n                f.write(\"/* Modified content */\\n\")\n\n            storage.save(\"test.css\", ContentFile(\"test content\\n\"))\n\n            manifest_content = json.loads(storage.read_manifest())\n            new_hash = manifest_content[\"hash\"]\n\n            self.assertNotEqual(initial_hash, new_hash)\n\n\n\n", "    def test_manifest_file_hashes(self):\n        # Test that the manifest file hash is correctly calculated and stored\n        try:\n            os.mkdir(TEST_ROOT / \"myapp/static/test_manifest\")\n        except FileExistsError:\n            pass\n        # Create a couple of files in the test directory.\n        with open(self.hashed_file_path(\"test_manifest/a.css\"), \"w\") as f:\n            f.write(\"/* Some CSS */\")\n        with open(self.hashed_file_path(\"test_manifest/b.js\"), \"w\") as f:\n            f.write(\"/* Some JavaScript */\")\n\n        # Manually set the hashed file storage.\n        self.storage = ManifestStaticFilesStorage(\n            location=TEST_ROOT / \"myapp/static\", base_url=settings.STATIC_URL\n        )\n        settings.STATICFILES_STORAGE = \"myapp.tests.ManifestStaticFilesStorage\"\n        call_command(\"collectstatic\", interactive=False)\n\n        # Check the hash is stored correctly\n        manifest_content = self.storage.read_manifest()\n        self.assertTrue(manifest_content is not None)\n        manifest_data = json.loads(manifest_content)\n        self.assertEqual(manifest_data[\"version\"], ManifestFilesMixin.manifest_version)\n        self.assertEqual(\n            sorted(manifest_data[\"paths\"].keys()),\n            [\n                \"/static/test_manifest/a.css\",\n                \"/static/test_manifest/b.js\",\n            ],\n        )\n        self.assertIn(\n            \"/static/test_manifest/a.css\", manifest_data[\"paths\"].keys()\n        )\n        self.assertIn(\n            \"/static/test_manifest/b.js\", manifest_data[\"paths\"].keys()\n        )\n\n        try:\n            os.remove(hashed_file_path(\"test_manifest/a.css\"))\n        except FileNotFoundError:\n            pass\n\n        call_command(\"collectstatic\", interactive=False)\n        manifest_data = json.loads(self.storage.read_manifest())\n        self.assertEqual(len(manifest_data[\"paths\"]), 1)\n        self.assertNotIn(\n            \"/static/test_manifest/a.css", "    def test_manifest_strict_mode(self):\n        with override_settings(STATICFILES_STORAGE='django.core.files.storage.ManifestStaticFilesStorage'):\n            with tempfile.TemporaryDirectory() as tempdir:\n                os.environ[\"DJANGO_SETTINGS_MODULE\"] = \"test.settings\"\n                self.create_test_files(tempdir)\n\n                # Mock the storage's delete method to raise an error.\n                with mock.patch(\n                    \"django.core.files.storage.ManifestStaticFilesStorage.delete\"\n                ) as mock_delete:\n                    mock_delete.side_effect = ValueError(\"Something failed\")\n\n                    # Run collectstatic\n                    from django.contrib.staticfiles.management.commands.collectstatic import Command as CollectstaticCommand\n                    call_command(\n                        \"collectstatic\",\n                        verbosity=0,\n                        interactive=False,\n                        default_url=None,\n                        update=True,\n                        ignore=[\"some_file.css\"],\n                    )\n\n                    # Assert that no files were deleted\n                    mock_delete.assert_not_called()\n\n                    # Assert an error message\n                    with self.assertRaises(ValueError) as context:\n                        call_command(\n                            \"collectstatic\",\n                            verbosity=0,\n                            interactive=False,\n                            default_url=None,\n                            update=True,\n                            ignore=[\"some_file.css\"],\n                        )\n                    self.assertIn(\n                        \"Couldn't load manifest 'staticfiles.json' (version 1.1)\",\n                        str(context.exception),\n                    )\n\n\n\n", "    def test_manifest_strict_mode_missing_file(self):\n        with override_settings(STATICFILES_STORAGE='app.ManifestStaticFilesStorage'):\n            self.assertRaises(\n                ValueError,\n                self.run_collectstatic,\n                \"--manifest\",\n                \"--noinput\",\n            )\n\n\n", "    def test_manifest_updates_on_file_change(self):\n        # Create a test file that will be served by the staticfile system\n        source_path = self.create_fake_static_file(\"test.css\")\n        source_content = \"/* Original Content */\"\n        with open(source_path, \"w\") as f:\n            f.write(source_content)\n\n        # Trigger collectstatic and collect static files\n        self.execute_collectstatic()\n\n        # Verify the file is present in the manifest and cache-busting is applied\n        self.assertIn(\n            \"test.css\", self.read_manifest()\n        ), \"Manifest should contain the newly added test.css\"\n\n        # Now change the content of the test file\n        with open(source_path, \"w\") as f:\n            f.write(\"/* Modified Content */\")\n\n        # Trigger collectstatic again\n        self.execute_collectstatic()\n\n        # Verify the manifest entry is updated\n        manifest_content = self.read_manifest()\n        self.assertIn(\n            \"test.css\", manifest_content\n        ), \"Manifest should contain the updated test.css\"\n        self.assertNotEqual(\n            manifest_content[\"test.css\"],\n            self.original_hashed_file_path(\"test.css\"),\n            \"Manifest should have an updated hash for test.css\",\n        )\n        "], "sample_1081": ["    def test_is_perfect():\n        assert is_perfect(1) == False\n        assert is_perfect(6) == True\n        assert is_perfect(28) == False\n        assert is_perfect(496) == True\n        assert is_perfect(8128) == True\n        assert is_perfect(33550336) == True\n", "    def test_is_perfect():\n        assert is_perfect(6) == True\n        assert is_perfect(28) == True\n        assert is_perfect(496) == True\n        assert is_perfect(8128) == True\n        assert is_perfect(33550336) == True\n        assert is_perfect(1) == False\n        assert is_perfect(2) == False\n        assert is_perfect(3) == False\n        assert is_perfect(4) == False\n        assert is_perfect(5) == False\n        assert is_perfect(7) == False\n        assert is_perfect(8) == False\n        assert is_perfect(9) == False\n    ", "    def test_is_abundant():\n        assert is_abundant(12) == True\n        assert is_abundant(28) == True\n        assert is_abundant(15) == False\n", "    def test_is_perfect_large():\n        for i in range(1, 100):\n            n = random.randint(1, 10**6)\n            if is_perfect(n):\n                assert divisor_sigma(n, 1) == 2 * n\n            else:\n                assert divisor_sigma(n, 1) != 2 * n\n", "    def test_antidivisors():\n        assert sorted(antidivisors(6)) == [7, 16]\n        assert sorted(antidivisors(128)) == [3, 5, 15, 17, 51, 85]\n        assert sorted(antidivisors(200)) == [103, 133, 167, 203]\n"], "sample_254": ["    def test_inline_admin_form(self):\n        response = self.client.get(reverse('admin:inner_holder_change', args=(self.holder.id,)))\n        self.assertContains(response, INLINE_CHANGELINK_HTML)\n        form = InlineAdminForm(\n            self.factory.get('/admin/inner/inner/', {'nonexistent': 'nonexistent'}),\n            Inner.objects,\n            formset=None,\n            data={'__NONCSRF_TEST__': 1},\n        )\n        self.assertIn('holder', form.fields)\n\n\n", "    def test_inline_change_link(self):\n        data = {'dummy': 4, 'holder': self.holder.id}\n        url = reverse('admin:inner_change', args=(self.holder.pk,))\n        response = self.client.get(url)\n        self.assertContains(response, INLINE_CHANGELINK_HTML)\n\n\n\n", "    def test_can_change_nested_inlines(self):\n        response = self.client.get(reverse('admin:inlines_inner_change', args=(self.holder.id,)))\n        self.assertEqual(response.status_code, 200)\n\n        # Check if nested inlines are rendered\n        content = response.content.decode('utf-8')\n        self.assertIn('<form action=\"/admin/inlines_inner_change/' + str(self.holder.id) + '/' + '\">', content)\n        self.assertIn('<form action=\"/admin/inlines_inner2_change/' + str(self.holder.id) + '/' + '\">', content)\n\n\n\n\n", "    def test_inline_formset_creation(self):\n        response = self.client.get(reverse('admin:inlines_child_changelist'))\n        self.assertContains(response, INLINE_CHANGELINK_HTML)\n        response = self.client.get(reverse('admin:inlines_child_add'))\n        self.assertContains(response, INLINE_CHANGELINK_HTML)\n\n\n\n", "    def test_get_formset_with_exclude(self):\n        request = self.factory.get(reverse('admin:inner_change', args=(1,)))  \n        formset = InnerAdmin.get_formset(\n            request, obj=self.holder, exclude=['dummy']\n        )\n        self.assertEqual(len(formset.form.fields), 1)  \n        self.assertIn('holder', formset.form.fields)\n\n\n\n"], "sample_633": ["    def test_find_similarities(test_file, expected_num_lines):\n        with open(test_file, \"r\", encoding=\"utf-8\") as f:\n            code = f.read()\n        linter = PyLinter()\n        linter.parse(StringIO(code).readline)\n        similar.run(linter)\n        assert len(linter.stats[\"similar_lines\"]) == expected_num_lines\n\n", "    def test_similarity_detection(\n        filename, expected_msgs, ignore_comments, ignore_docstrings, ignore_imports, ignore_signatures", "    def test_similar_lines(filename, expected):\n        with open(filename, \"r\", encoding=\"utf-8\") as f:\n            content = f.read()\n\n        linter = PyLinter()\n        linter.load_config(\"test/config.py\")\n        linter.add_checker(similar.SimilarChecker(linter))\n        linter.set_stream(StringIO())\n        linter.check(\n            filename,\n            content,\n            flags=None,\n            options=[f\"--ignore-comments\"],\n        )\n        messages = linter.get_messages()\n\n        assert len(messages) == expected\n\n\n\n    \n", "    def test_similar_empty_function(self):\n        linter = PyLinter()\n        with redirect_stdout(StringIO()) as captured_output:\n            linter.check([EMPTY_FUNCTION_1, EMPTY_FUNCTION_2], reporter=Reporter())\n            output = captured_output.getvalue()\n        assert \"duplicate-code\" not in output\n\n\n\n", "    def test_similar_lines(self, filename, expected_output):\n        with open(filename, \"r\", encoding=\"utf-8\") as f:\n            source = f.read()\n\n        linter = PyLinter()\n        reporter = Reporter()\n        linter.set_reporter(reporter)\n        linter.analyze(source, filename)\n        msgs = reporter.messages\n        assert expected_output in [msg.msg for msg in msgs if msg.type == \"W0801\"] \n\n\n"], "sample_463": ["compilation error", "compilation error", "    def test_unique_together_different_fields_works(self):\n        with self.assertRaises(models.IntegrityError):\n            Book.objects.create(\n                author=Author.objects.first(),\n                title=\"unique\",\n                newfield2=1,\n            )\n            Book.objects.create(\n                author=Author.objects.first(),\n                title=\"unique\",\n                newfield2=2,\n            )\n\n\n\n", "    def test_model_state_unique_together_field_order(self):\n        self.assertEqual(\n            book_unique_together.fields, [\n                {\"field\": \"title\", \"field_order\": 1},\n                {\"field\": \"author\", \"field_order\": 2},\n            ]\n        )\n        self.assertEqual(\n            book_unique_together_2.fields, [\n                {\"field\": \"title\", \"field_order\": 1},\n                {\"field\": \"author\", \"field_order\": 2},\n            ]\n        )\n        self.assertEqual(\n            book_unique_together_3.fields, [\n                {\"field\": \"title\", \"field_order\": 1},\n                {\"field\": \"newfield\", \"field_order\": 2},\n            ]\n        )\n        self.assertEqual(\n            book_unique_together_4.fields, [\n                {\"field\": \"title\", \"field_order\": 1},\n                {\"field\": \"newfield2\", \"field_order\": 2},\n            ]\n        )\n\n\n\n", "compilation error"], "sample_350": ["    def test_prefetch_related_nested(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\"\"\"\n                CREATE TABLE django_admin_log (\n                    id INTEGER PRIMARY KEY,\n                    action_time DATETIME,\n                    ip_address VARCHAR(15),\n                    user_id INTEGER,\n                    content_type_id INTEGER,\n                    object_id INTEGER,\n                    object_repr VARCHAR(200),\n                    action_flag VARCHAR(1),\n                    change_message TEXT,\n                    PRIMARY KEY (id)\n                )\n            \"\"\")\n        Celebrity.objects.create(name='Alice')\n        c = Celebrity.objects.prefetch_related(\n            'numbers',\n            Prefetch('numbers', queryset=Number.objects.filter(num=1)),\n        ).get(name='Alice')\n        self.assertEqual(len(c.numbers.all()), 1)\n\n        with connection.cursor() as cursor:\n            cursor.execute(\"\"\"\n                DROP TABLE django_admin_log\n            \"\"\")\n\n\n\n", "    def test_select_related_with_nested_prefetch(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\"\"\"\n                CREATE TABLE celebrity_numbers (\n                    id INT PRIMARY KEY,\n                    celebrity_id INT,\n                    number_id INT\n                );\n            \"\"\")\n            cursor.execute(\"\"\"\n                INSERT INTO celebrity_numbers (id, celebrity_id, number_id) VALUES\n                (1, 1, 1), (2, 1, 2), (3, 2, 3), (4, 2, 4);\n            \"\"\")\n\n        celebrity_number_qs = Celebrity.objects.select_related('numbers').prefetch_related(\n            'numbers__number'\n        )\n        celebrities = list(celebrity_number_qs)\n        for celebrity in celebrities:\n            for number in celebrity.numbers.all():\n                self.assertEqual(number.number.num, number.number)\n\n\n\n", "    def test_union_with_exists(self):\n        exists_query = Celebrity.objects.filter(numbers__num__gt=5).exists()\n        combined_queryset = Number.objects.filter(\n            num__in=Number.objects.filter(\n                other_num__gt=4\n            ).values_list('num', flat=True)\n        ).union(Number.objects.filter(other_num__gt=4).filter(exists=exists_query))\n\n        self.assertNumbersEqual(\n            combined_queryset,\n            [Number.objects.get(num=i) for i in range(5, 10)],\n        )\n\n\n\n", "    def test_select_related_multiple_levels(self):\n        with connection.cursor() as cursor:\n            with CaptureQueriesContext(cursor) as captured:\n                celebrities = Celebrity.objects.select_related('number__other_number').prefetch_related('number')\n                self.assertEqual(captured.query_count, 2)\n                celebrities = celebrities.filter(name='John Doe')\n                self.assertEqual(captured.query_count, 3)\n", "    def test_select_related_prefetch_related_together(self):\n        # Test that both select_related and prefetch_related work together,\n        # and that the order of operations is respected.\n        with CaptureQueriesContext() as queries:\n            celebrities = Celebrity.objects.select_related('number').prefetch_related('numbers')\n        self.assertEqual(len(queries.captured_queries), 2)  # One for numbers, one for celeb. number\n\n        # Verify we have the data we expect\n        for celeb in celebrities:\n            self.assertIsNotNone(celeb.number)  # select_related\n            self.assertNotEqual(len(celeb.numbers), 0)  # prefetch_related\n\n        # Now test with prefetch_related first\n        with CaptureQueriesContext() as queries:\n            celebrities = Celebrity.objects.prefetch_related('numbers').select_related('number')\n        self.assertEqual(len(queries.captured_queries), 2)  # Same two queries\n\n        # Verify we have the data we expect\n        for celeb in celebrities:\n            self.assertIsNotNone(celeb.number)  # select_related\n            self.assertNotEqual(len(celeb.numbers), 0)  # prefetch_related\n\n\n\n"], "sample_874": ["    def test_step_selector(self):\n        step_selector = StepSelector(step=2)\n        step_selector.fit(X)\n\n        assert_array_equal(step_selector.get_support(), support)\n        assert_array_equal(step_selector.get_support_indices(), support_inds)\n        assert_array_equal(step_selector.transform(X), Xt)\n        assert_array_equal(step_selector.inverse_transform(Xt), Xt)\n\n        with pytest.raises(ValueError):\n            StepSelector(step=0).fit(X)\n\n        step_selector = StepSelector(step=2)\n        step_selector.fit(Xinv)\n        assert_array_equal(step_selector.get_support(), support)\n        assert_array_equal(step_selector.get_support_indices(), support_inds)\n        assert_array_equal(step_selector.transform(Xinv), Xt)\n        assert_array_equal(step_selector.inverse_transform(Xt), Xt)\n", "    def test_step_selector(self):\n        step_selector = StepSelector(step=2)\n        step_selector.fit(X)\n        assert_array_equal(step_selector.get_support(), support)\n        assert_array_equal(step_selector.get_support(), [True, False] * 5)\n\n        step_selector = StepSelector(step=1)\n        step_selector.fit(X)\n        assert_array_equal(step_selector.get_support(), support)\n        assert_array_equal(step_selector.transform(X), Xt)\n\n        step_selector = StepSelector(step=0)\n        step_selector.fit(X)\n        assert_array_equal(step_selector.get_support(), support)\n        assert_array_equal(step_selector.transform(X), Xt)\n", "    def test_step_selector_transform(self):\n        selector = StepSelector(step=2)\n        selector.fit(X)\n        X_transform = selector.transform(X)\n        assert_array_equal(X_transform, Xt)\n        assert_array_equal(selector.get_support(), support)\n        assert_array_equal(selector.get_support_indices(), support_inds)\n", "    def test_step_selector_transform(self):\n        step_selector = StepSelector(step=2)\n        selector = step_selector.fit(X, y)\n        selected_features = selector.transform(X)\n        assert_array_equal(selected_features, Xt)\n        assert_array_equal(selector.get_support(), support)\n        assert_array_equal(selector.get_feature_names_out(), feature_names_t)\n", "    def test_step_selector(self, X, step, exp_mask):\n        selector = StepSelector(step=step)\n        selector.fit(X)\n        assert_array_equal(selector._get_support_mask(), exp_mask)\n"], "sample_197": ["    def test_timesince_future_date(self):\n        future_date = self.t + self.oneday\n        self.assertEqual(timesince(future_date), \"0 minutes\")\n\n\n\n", "    def test_timesince_leapyears(self):\n        # Test that leap years are handled correctly\n        now = timezone.now()\n        d1 = datetime.datetime(2000, 2, 29)\n        d2 = datetime.datetime(2001, 2, 29)\n        self.assertEqual(timesince(d1, now), '1 year')\n        self.assertEqual(timesince(d2, now), '2 years')\n", "    def test_timesince_reversed(self):\n        future_time = self.t + self.oneday\n        result = timesince(future_time, self.t)\n        self.assertEqual(result, avoid_wrapping('1 day'))\n", "    def test_timesince_leap_years(self):\n        now = timezone.now()\n        d1 = datetime.date(2020, 2, 29)  # Leap year\n        d2 = datetime.date(2021, 2, 28)  # Not a leap year\n\n        self.assertEqual(timesince(d1, now), '0 years')\n        self.assertEqual(timesince(d2, now), '0 years')\n", "    def test_leapyear(self):\n        now = timezone.datetime(2008, 2, 29, 10, 0, 0)\n        d = timezone.datetime(2007, 2, 28, 10, 0, 0)\n        self.assertEqual(timesince(d, now), \n                         \"1 year, 1 day\") \n"], "sample_109": ["    def test_required_field_renders_empty_option_when_default(self):\n        form = RequiredBandForm()\n\n        rendered_html = form.as_p()\n        self.assertIn(self.empty_option, rendered_html)\n\n\n", "    def test_autocomplete_field_with_required_and_allow_multiple_selected(self):\n        form = RequiredBandForm(data={'band': [1, 2]})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['band'], [1, 2])\n", "    def test_autocomplete_select_with_required_field(self):\n        \"\"\"\n        Test that the widget correctly renders an autocomplete select\n        with a required field.\n        \"\"\"\n        form = RequiredBandForm()\n        context = {}\n        form.render(context)\n        rendered_html = context['rendered_html']\n        self.assertContains(rendered_html, '<select', count=1)\n\n\n\n        self.assertContains(rendered_html, 'data-ajax--type=\"GET\"')\n        self.assertContains(rendered_html, 'data-allow-clear=\"false\"')\n", "    def test_autocomplete_required_field(self):\n        form = RequiredBandForm()\n        with self.assertNumQueries(1):  # One query for fetching the queryset\n            response = form.render()\n        self.assertContains(response, '<input type=\"hidden\" name=\"band\" value=\"\">')\n        self.assertContains(response, '<select class=\"my-class\" aria-label=\"band\" name=\"band\">')\n\n\n", "    def test_autocomplete_select_required_field(self):\n        form = RequiredBandForm()\n\n        with translation.override('en'):\n            rendered_html = form.render(None, 'band')\n        \n        self.assertIn('<option value=\"\">', rendered_html)\n"], "sample_129": ["    def test_floatformat_with_sign(self):\n        self.assertEqual(self.render('floatformat01', {'a': -1.23456789}), '-1.23')\n        self.assertEqual(self.render('floatformat01', {'b': 1.23456789}), '1.23')\n\n\n", "    def test_floatformat_negative_exponent(self):\n        self.assertEqual(self.render_template.render(\n            {'a': 123.45678901234567890123456789, 'b': float('-0.000123')},\n            'floatformat01').strip(),\n            '123.46 {{ floatformat(b) }}')\n", "    def test_floatformat_zero_decimals(self):\n        ctx = {'a': 34.23, 'b': 34}\n        self.assertHTMLEqual(self.render('floatformat01', ctx), '34.23 34')\n\n\n", "    def test_floatformat_with_args(self):\n        with self.subTest('with arg'):\n            context = {'a': 3.141592653589793, 'b': 3.14}\n            self.assertEqual(self.render_template('floatformat01', context),\n                             '3.14 3.14')\n\n            context = {'a': 3.141592653589793, 'b': 3.141592653589793}\n            self.assertEqual(self.render_template('floatformat01', context),\n                             '3.141593 3.141593')\n\n\n\n", "    def test_floatformat_with_args(self):\n        self.assertHTMLEqual(\n            self.render_string(\n                '{% autoescape off %}{{ a|floatformat:2 }} {{ b|floatformat:3 }}{% endautoescape %}',\n                {'a': 3.1415926535897932, 'b': 1234.567890123456789}\n            ),\n            '3.14 {{ 1234.568 }}'\n        )\n\n\n"], "sample_42": ["compilation error", "compilation error", "    def test_thermodynamic_temperature(freq, expected_temp, cosmological_data):\n        from astropy.cosmology import Planck15\n\n        equiv = u.thermodynamic_temperature(freq, Planck15.Tcmb0)\n        result = (1.0 * u.mK).to(u.MJy / u.sr, equivalencies=equiv)\n        assert_quantity_allclose(result, expected_temp * 1000 * u.MJy / u.sr)\n", "compilation error", "compilation error"], "sample_592": ["    def test_short_numpy_repr(self, array, expected):\n        assert formatting.short_numpy_repr(array) == expected\n\n\n\n   \n", "    def test_format_array_flat(self):\n        arr = np.arange(100)\n        repr_str = formatting.format_array_flat(arr, max_width=20)\n        assert repr_str in (\n            \"0 1 2 3 ... 94 95 96 97 ... 99\",\n            \"0 1 2 3 ... 94 95 96 97 ... 99\",  # if wrapping\n        )\n        max_width = 5\n        repr_str = formatting.format_array_flat(arr, max_width=max_width)\n        assert repr_str == \"...\"\n        with raises_regex(ValueError, \"max_width must be at least\"):\n            formatting.format_array_flat(arr, max_width=-1)\n\n        # Test with sparse array\n        from sparse import COO\n\n        sparse_array = COO((np.arange(10), (0, 1, 2, 3, 4, 5, 6, 7, 8, 9)),\n                           shape=(10, 10))\n        repr_str = formatting.format_array_flat(sparse_array, max_width=20)\n        assert repr_str == \"<sparse.COO: nnz=10, fill_value=None>\"\n", "    def test_short_numpy_repr(self, array, expected):\n        assert formatting.short_numpy_repr(array) == expected\n\n\n\n", "    def test_diff_array_repr(self, compat):\n        # Test different comparision modes\n        a = xr.DataArray(\n            np.random.rand(3, 4),\n            dims=\"xy\",\n            coords={\"x\": [1, 2, 3], \"y\": [4, 5, 6]},\n        )\n        b = xr.DataArray(\n            np.random.rand(3, 4),\n            dims=\"xy\",\n            coords={\"x\": [1, 2, 3], \"y\": [4, 5, 6]},\n        )\n        b.data = np.roll(b.data, 1, axis=1)\n\n        expected = (\n            dedent(\n                f\"\"\"\n                Left and right DataArray objects are not {(\n                    compat.__name__ if callable(compat) else compat)}\n\n                Differing values:\n                L\n                array([[0.82691719, 0.23664624, 0.51821374, 0.86760467],\n                       [0.84484976, 0.13930286, 0.25721501, 0.69361584],\n                       [0.81263162, 0.85829188, 0.27645502, 0.5706693 ]])\n\n                R\n                array([[0.23664624, 0.51821374, 0.86760467, 0.82691719],\n                       [0.13930286, 0.25721", "    def test_array_repr(self, array, expected):\n        output = formatting.array_repr(xr.DataArray(array))\n        assert output == expected\n\n\n\n"], "sample_365": ["    def test_cached_property(self):\n        class MyClass:\n            @cached_property\n                return sum(range(1000000))\n\n        obj = MyClass()\n        self.assertEqual(obj.expensive_property, sum(range(1000000)))\n        self.assertEqual(obj.expensive_property, sum(range(1000000)))  # Cached result\n", "    def test_cached_property(self):\n        class MyModel:\n                self.value = 10\n\n            @cached_property\n                return self.value * 2\n\n        instance = MyModel()\n        self.assertEqual(instance.something, 20)\n        self.assertEqual(instance.something, 20)  # Cached value is retrieved\n\n        instance.value = 5\n        self.assertEqual(instance.something, 10)\n        self.assertEqual(instance.something, 10)  # Cached value is updated\n", "    def test_lazy(self):\n        @lazy\n            return 'hello'\n\n        self.assertEqual(my_func(), 'hello')\n\n        @lazy(str)\n            return 'hello'\n\n        self.assertEqual(lazy_str(), 'hello')\n", "    def test_cached_property_name_collision(self):\n        class MyClass:\n            @cached_property\n                return 42\n\n            @cached_property\n                return 'forty-two'\n\n        obj = MyClass()\n        with self.assertRaises(TypeError):\n            obj.prop  # This will raise a TypeError because two cached_property\n                      # instances are assigned to the same name\n        with self.assertRaises(TypeError):\n            obj.prop \n\n\n\n", "compilation error"], "sample_1154": ["compilation error", "compilation error", "compilation error", "    def test_mixed_type_coefficients():\n        eqs = [\n            Eq(x + 2*y + 3*z, 1),\n            Eq(x - y + z, 2),\n            Eq(2*x + y - 3*z, 3)\n        ]\n        sol = _linsolve(eqs, [x, y, z])\n        assert sol == {x: 1, y: 1, z: 0}\n", "compilation error"], "sample_841": ["def test_ridge_dual_coef_with_sparse_data(solver, random_state):\n    n_samples = 100\n    n_features = 10\n    X = sp.csc_matrix(np.random.rand(n_samples, n_features))\n    y = np.random.rand(n_samples)\n    alpha = 0.1\n\n    estimator = Ridge(alpha=alpha, solver=solver)\n    estimator.fit(X, y)\n\n    # Check if the dual coefficients have the expected shape\n    assert estimator.dual_coef_.shape == (n_features,)\n    assert isinstance(estimator.dual_coef_, np.ndarray)\n    assert np.allclose(estimator.dual_coef_, estimator._dual_coef_.toarray())\n\n\n", "    def test_ridge_solver_auto(self, random_state, solver):\n        X, y = make_regression(n_samples=100, n_features=10,\n                               random_state=random_state)\n        model = Ridge(solver=solver, alpha=1.0)\n        model.fit(X, y)\n        assert model.solver_ == 'auto'\n        if sp.issparse(X):\n            assert model.solver_ in ('sparse_cg', 'lsqr')\n        else:\n            assert model.solver_ in ('svd', 'cholesky')\n", "    def test_ridge_solver_cholesky_eigen(self, solver):\n        X = DENSE_FILTER(make_regression(n_samples=100, n_features=10,\n                                         random_state=42))\n        y = make_regression(n_samples=100, n_features=10,\n                            random_state=42)\n        X_sparse = SPARSE_FILTER(make_regression(n_samples=100,\n                                                 n_features=10,\n                                                 random_state=42))\n        y_sparse = make_regression(n_samples=100, n_features=10,\n                                   random_state=42)\n        for solve in (_solve_cholesky, _solve_cholesky_kernel):\n            with pytest.raises(ValueError, match=r\"The 'solver' \\", "    def test_sparse_solver_not_used_for_dense_data(self, solver, alpha):\n        X = np.random.rand(100, 50)\n        y = np.random.rand(100)\n        clf = Ridge(alpha=alpha, solver=solver)\n        clf.fit(X, y)\n        assert clf.solver_ == 'auto'\n        assert clf.dual_coef_.shape == (50,)  \n\n\n", "    def test_ridge_regression_cv(X_train, y_train, X_test, y_test, solver, alpha, fit_intercept, normalize):\n        from sklearn.linear_model import RidgeCV\n\n        ridge_cv = RidgeCV(alphas=[alpha], cv=5, fit_intercept=fit_intercept, normalize=normalize, solver=solver)\n        ridge_cv.fit(X_train, y_train)\n\n        y_pred = ridge_cv.predict(X_test)\n\n        # Check that the predicted values are close to the actual values\n        assert_allclose(ridge_cv.alpha_, ridge_cv.best_estimator_.alpha_)\n        assert_allclose(ridge_cv.coef_, ridge_cv.best_estimator_.coef_)\n        assert (ridge_cv.cv_values_ is not None) == (ridge_cv.store_cv_values_)\n        assert_array_almost_equal(mean_squared_error(y_test, y_pred), ridge_cv.best_score_)\n"], "sample_835": ["    def test_adaboost_regressor_sparse(self):\n        n_samples = 100\n        n_features = 5\n        X = np.random.randn(n_samples, n_features)\n        y = np.random.randn(n_samples)\n\n        # Create sparse matrices from X\n        X_csc = csc_matrix(X)\n        X_csr = csr_matrix(X)\n        X_coo = coo_matrix(X)\n        X_dok = dok_matrix(X)\n        X_lil = lil_matrix(X)\n\n        # fit adaboost with different sparse matrix representations\n        regr_csc = AdaBoostRegressor(random_state=0).fit(X_csc, y)\n        regr_csr = AdaBoostRegressor(random_state=0).fit(X_csr, y)\n        regr_coo = AdaBoostRegressor(random_state=0).fit(X_coo, y)\n        regr_dok = AdaBoostRegressor(random_state=0).fit(X_dok, y)\n        regr_lil = AdaBoostRegressor(random_state=0).fit(X_lil, y)\n\n        # check predictions are consistent across sparse representations\n        for X_sparse in [X_csc, X_csr, X_coo, X_dok, X_lil]:\n            pred = regr_csc.predict(X_sparse)\n            assert_array_almost_equal(pred, regr_csr.predict(X_sparse))\n            assert_array_almost_equal(pred, regr_coo.predict(X_sparse))\n            assert_array_almost_equal(pred, regr_dok.predict(X_sparse))\n            assert_array_almost_equal(pred, regr_lil.predict(X_sparse))\n\n\n\n", "    def test_adaboost_regressor_fit_zero_sample_weight(self):\n        X, y = shuffle(boston.data, boston.target, random_state=rng)\n\n        regr = AdaBoostRegressor(n_estimators=10)\n        sample_weight = np.zeros_like(y)\n        with pytest.raises(ValueError):\n            regr.fit(X, y, sample_weight=sample_weight)\n\n        sample_weight[0] = 1\n        regr.fit(X, y, sample_weight=sample_weight)\n\n\n\n", "    def test_adaboost_regressor_with_sample_weight(self):\n        sample_weight = np.array([0.1, 0.2, 0.3, 0.4])\n        X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n        y = np.array([10, 20, 30, 40])\n\n        reg = AdaBoostRegressor(n_estimators=5, random_state=0)\n        reg.fit(X, y, sample_weight=sample_weight)\n\n        y_pred = reg.predict(X)\n        assert_array_almost_equal(y_pred, y)\n        assert_raises(ValueError,  AdaBoostRegressor,\n                      n_estimators=0, random_state=0)\n    ", "    def test_boost_with_invalid_estimator(self, estimator):\n        with pytest.raises(ValueError):\n            weight_boosting._make_estimator(\n                estimator=estimator,\n                random_state=None\n            )\n", "    def test_adaboost_regressor_weights(self):\n        reg = AdaBoostRegressor(n_estimators=10, random_state=0)\n        X, y = shuffle(boston.data, boston.target, random_state=0)\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n        reg.fit(X_train, y_train)\n        sample_weights = reg.estimator_weights_\n        assert len(sample_weights) == reg.n_estimators\n        assert_array_equal(sample_weights.shape, (reg.n_estimators,))\n"], "sample_370": ["    def test_prefetch_related_with_multiple_lookups(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\n                \"INSERT INTO django_content_type (app_label, model) VALUES (%s, %s)\",\n                ('app_name', 'BookReview'),\n            )\n            ContentType.objects.get_for_model(BookReview)\n\n        with self.assertNumQueries(2):  # 1 for the count, 1 for the fetch\n            books = Book.objects.filter(authors__name='Charlotte').select_related('authors').prefetch_related(\n                'reviews'\n            ).annotate(review_count=Count('reviews'))\n        \n        self.assertEqual(len(books), 1)\n        self.assertEqual(books[0].review_count, 1)\n\n        # Ensure that the reviews are actually prefetched\n        with CaptureQueriesContext(connection.cursor()) as queries:\n            for book in books:\n                print(book.reviews.all())\n            self.assertEqual(len(queries.queries), 1)\n\n\n        # Make sure the queries we run are the ones we expect\n        for query in queries.queries:\n            self.assertWhereContains(query.sql, \"django_content_type.app_label = 'app_name'\")\n            self.assertWhereContains(query.sql, \"django_content_type.model = 'BookReview'\")\n\n\n", "    def test_prefetch_related_with_object_instances(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\"\"\"\n                SELECT pk FROM django_content_type WHERE app_label = 'tests'\n                AND model_name = 'book'\n            \"\"\")\n            content_type_id = cursor.fetchone()[0]\n\n            with override_settings(\n                INSTALLED_APPS={'testapp': 'tests'},\n            ):\n                prefetch_books = Book.objects.prefetch_related(\n                    'authors', 'reviews'\n                ).filter(id=1)  \n        \n                with CaptureQueriesContext() as qs:\n                    prefetch_books[0].authors.all()\n                    prefetch_books[0].reviews.all()\n                \n                self.assertEqual(len(qs.captured_queries), 2)\n\n                # 1st query: original query for books\n                self.assertWhereContains(qs.captured_queries[0].sql, f'content_type_id = {content_type_id}')\n\n                # 2nd query: prefetch for authors\n                self.assertWhereContains(qs.captured_queries[1].sql, f'book_id = {1}')\n\n                # 3rd query: prefetch for reviews\n                self.assertWhereContains(qs.captured_queries[2].sql, f'book_id = {1}')\n\n\n\n", "    def test_prefetch_related_with_filter(self):\n        with CaptureQueriesContext() as queries:\n            reader1_books = Reader.objects.prefetch_related(\n                'books_read',\n                Prefetch(\n                    'books_read__title',\n                    queryset=Book.objects.filter(title__startswith='S')\n                )\n            ).get(pk=1)\n        self.assertEqual(len(queries.executed_queries), 2)\n        self.assertWhereContains(queries.executed_queries[1][0], 'title__startswith')\n\n        reader1_book_titles = [book.title for book in reader1_books.books_read.all()]\n        self.assertEqual(reader1_book_titles, ['Sense and Sensibility'])\n", "    def test_prefetch_related_with_non_related_field(self):\n        with connection.cursor() as cursor:\n            cursor.execute('SELECT id, title FROM book_book')\n            rows = cursor.fetchall()\n            book_ids = [row[0] for row in rows]\n\n            with CaptureQueriesContext() as qs:\n                prefetch_related_books = Book.objects.prefetch_related('authors__name').filter(id__in=book_ids)\n\n            self.assertLessEqual(len(qs.captured_queries), 2, \"Shouldn't require more than two queries\")\n            self.assertWhereContains(qs.captured_queries[1], 'FROM book_book WHERE id__in')\n\n            for book in prefetch_related_books:\n                self.assertIn('name', book.authors.values_list('name', flat=True))\n\n\n\n", "    def test_prefetch_related_with_related_manager(self):\n        with CaptureQueriesContext() as context:\n            # Get books with authors and prefetch related article\n            books = Book.objects.prefetch_related('authors', 'article_set').all()\n            self.assertEqual(len(context.captured_queries), 1)\n            self.assertWhereContains(context.captured_queries[0], 'SELECT ... FROM book_author')\n\n            # Ensure article_set is properly prefetched\n            self.assertTrue(books[0].article_set.exists())\n            self.assertEqual(len(books[0].article_set.all()), 2)\n\n"], "sample_985": ["    def test_minmax_rewrite_as_Piecewise():\n        from sympy.functions.elementary.integers import floor\n        from sympy.polys.polytools import div\n        args = (S(1), S(2), S(3))\n        exp = Max(*args)\n        piecewise = exp._eval_rewrite_as_Piecewise(*args)\n        assert isinstance(piecewise, Piecewise)\n        assert len(piecewise.args) == 3\n        assert piecewise.args[0].args[0] == S(1)\n        assert piecewise.args[1].args[0] == S(2)\n        assert piecewise.args[2].args[0] == S(3)\n\n", "    def test_minmax_as_Piecewise():\n        from sympy.core.sympify import sympify\n\n        p = Min(sympify('x'), sympify('y'))\n        assert p._eval_rewrite_as_Piecewise(sympify('x'), sympify('y')) == Piecewise(\n            (sympify('x'), sympify('x <= y')),\n            (sympify('y'), True)\n        )\n\n        p = Max(sympify('x'), sympify('y'))\n        assert p._eval_rewrite_as_Piecewise(sympify('x'), sympify('y')) == Piecewise(\n            (sympify('x'), sympify('x >= y')),\n            (sympify('y'), True)\n        )\n\n", "    def test_minmax_as_Piecewise():\n        assert (Min(x, y).rewrite(Piecewise)).args == (\n            (x <= y, x),\n            (y <= x, y)\n        )\n        assert (Max(x, y).rewrite(Piecewise)).args == (\n            (x >= y, x),\n            (y >= x, y)\n        )\n\n", "    def test_minmax_rewrite_as_Piecewise():\n        from sympy.core.exprtools import limit\n        e = Piecewise((1, (x > 0)), (x**2, (x <= 0)))\n        assert (e.rewrite(Piecewise) == e)\n\n", "    def test_minmax_as_Piecewise():\n        from sympy.core.relational import Eq\n        p = Min(x, y)\n        ps = p._eval_rewrite_as_Piecewise(x, y)\n        assert (ps.args[0][0] == Eq(x, y) and ps.args[0][1] == x) or \\\n               (ps.args[0][0] == Eq(y, x) and ps.args[0][1] == y)\n\n        p = Max(x, y)\n        ps = p._eval_rewrite_as_Piecewise(x, y)\n        assert (ps.args[0][0] == Eq(x, y) and ps.args[0][1] == x) or \\\n               (ps.args[0][0] == Eq(y, x) and ps.args[0][1] == y)\n\n\n\n"], "sample_670": ["    def test_parentheses():\n        assert evaluate(\"((a and b) or not c)\", lambda x: x == \"a\") is True\n        assert evaluate(\"(a or b) and not c\", lambda x: x == \"a\") is True\n        assert evaluate(\"a and (b or c)\", lambda x: x == \"a\") is True\n        assert evaluate(\"not (a and b)\", lambda x: x == \"a\") is False\n        with pytest.raises(ParseError):\n            evaluate(\"((a and b) or not c\", lambda x: x == \"a\")\n\n\n\n", "    def test_parens():\n        assert evaluate(\"((a) or (b))\", lambda x: x == \"a\") == True\n        assert evaluate(\"((a) and (b))\", lambda x: x == \"a\") == True\n        assert evaluate(\"(a or b) and c\", lambda x: x == \"a\") == False\n        with pytest.raises(ParseError) as excinfo:\n            evaluate(\"((a) or (b))\", lambda x: x == \"a\")\n        assert \"unexpected character\" in str(excinfo.value)\n\n\n", "    def test_parentheses():\n        assert evaluate(\"true\", lambda x: True) == True\n        assert evaluate(\"(true)\", lambda x: True) == True\n        assert evaluate(\"true and (false)\", lambda x: True) == False\n        assert evaluate(\"(true or false)\", lambda x: True) == True\n        assert evaluate(\"not (true)\", lambda x: True) == False\n        with pytest.raises(ParseError):\n            evaluate(\"(true or\", lambda x: True)\n\n\n", "    def test_parentheses_nesting( ):\n        assert evaluate(\"((a))\", lambda x: x == \"a\")\n        assert evaluate(\"((a) and (b))\", lambda x: x == \"a\")\n        assert evaluate(\"((a) or (b))\", lambda x: x == \"a\")\n        assert not evaluate(\"((a)) or (b)\", lambda x: x == \"a\") \n", "    def test_parentheses():\n        assert evaluate(\"(test and True)\", lambda x: x == \"test\")\n        assert evaluate(\"(test or False)\", lambda x: x == \"test\")\n        assert evaluate(\"True and (test or not True)\", lambda x: x == \"test\")\n        with pytest.raises(ParseError):\n            evaluate(\"(\")\n        with pytest.raises(ParseError):\n            evaluate(\"(test and \")\n"], "sample_1030": ["    def test_idiff_nested_function():\n        x, y = Symbol('x'), Symbol('y')\n        f = Function('f')(x)\n        g = Function('g')(f(x))\n        with raises(ValueError):\n            idiff(g(x), y, x)\n        with raises(ValueError):\n            idiff(g(x), [y, x], x)\n        with raises(ValueError):\n            idiff(g(x), [x, y], x)\n        assert idiff(g(y), y, x) == -Derivative(g(x), x)\n", "    def test_centroid_polygon_segments():\n        p = Polygon((0, 0), (1, 0), (1, 1), (0, 1))\n        q = Segment((0, 0), (1, 0))\n        r = Segment((0, 0), (0, 1))\n        c = centroid(p, q, r)\n        assert c == Point2D(1/3, 1/3)\n\n", "compilation error", "compilation error", "    def test_idiff_fun():\n      x = Symbol('x')\n      y = Symbol('y')\n      f = Function('f')\n      g = Function('g')\n      with raises(ValueError):\n        idiff(x + y, 'y', x)\n      assert idiff(x + y, y, x) == -1\n      assert idiff(x + g(x) + y, y, x) == -1\n      assert idiff(f(x) + y, y, x) == -Derivative(f(x), x)\n      assert idiff(f(x) + g(x) + y, y, x) == -Derivative(f(x), x) - Derivative(g(x), x)\n      assert idiff(f(y) + y, y, x) == -Derivative(f(y), y) * Derivative(y, x)\n      assert idiff(x**2 + y**2 - 4, y, x) == -x/y\n      assert idiff(x**2 + y**2 - 4, y, x, 2).simplify() == -(x**2 + y**2)/y**3\n      assert idiff(x**2 * y, y, x) == x**2\n"], "sample_276": ["    def test_model_detail_view(self):\n        url = reverse('admin:admindocs_app_model_detail', kwargs={'app_label': 'myapp', 'model_name': 'company'})\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n\n        # Check if the model name and description are displayed correctly.\n        self.assertContains(response, 'Company')\n        self.assertContains(response, 'A company object')\n        \n        # Check if the fields are displayed correctly.\n        self.assertContains(response, 'name')\n        self.assertContains(response, 'description')\n        self.assertContains(response, 'industry')\n        \n        # Check if related objects are displayed correctly.\n        self.assertContains(response, 'employees')\n        \n        # Check if methods are displayed correctly.\n\n", "    def test_model_detail_view(self):\n        url = reverse('admin_doc:model_detail', kwargs={'app_label': 'myapp', 'model_name': 'person'})\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n\n        response_data = response.context\n        self.assertIn('name', response_data)\n        self.assertIn('summary', response_data)\n        self.assertIn('description', response_data)\n        self.assertIn('fields', response_data)\n        self.assertIn('methods', response_data)\n", "    def test_view_detail_renders_view(self):\n        url = reverse('admin:admindocs_view_detail', kwargs={'view': 'people_admin_change'})\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, 'People')\n", "    def test_view_detail_template_rendering(self):\n        url = reverse('admin_doc_view_detail', args=['myproject.views.myview'])\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n\n        # Assert that the template is rendered correctly\n        self.assertTemplateUsed(response, 'admin_doc/view_detail.html')\n\n        # Assert that specific values are rendered correctly\n        content = response.content.decode('utf-8')\n        self.assertIn('myview Name:', content)\n        self.assertIn('myview Summary:', content)\n        self.assertIn('myview View Documentation:', content)\n\n", "    def test_model_detail_view(self):\n        @models.register\n        class Person(models.Model):\n            name = models.CharField(max_length=100)\n            email = models.EmailField()\n            company = models.ForeignKey(Company, on_delete=models.CASCADE)\n\n        with override_settings(\n            INSTALLED_APPS=['myapp'],\n        ):\n            response = self.client.get(reverse('admin:admindocs_model_detail', args=['myapp', 'person']))\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, 'name')\n        self.assertContains(response, 'email')\n        self.assertContains(response, 'company')\n\n        # Test the related objects view\n        response = self.client.get(reverse('admin:admindocs_model_detail', args=['myapp', 'person']))\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, 'company.all')\n        self.assertContains(response, 'company.count')\n\n        # Test methods\n        response = self.client.get(reverse('admin:admindocs_model_detail', args=['myapp', 'person']))\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, '__str__')\n        self.assertContains(response, '__repr__')\n\n"], "sample_712": ["    def test_fit_transform_categorical_features_sparse(self, sparse):\n        X = np.array([[1, 2], [3, 4], [1, 2]])\n        enc = OneHotEncoder(sparse=sparse)\n        X_enc = enc.fit_transform(X)\n        assert_equal(X_enc.shape, (3, 4))\n", "    def test_onehotencoder_sparse_dtype(self):\n        enc = OneHotEncoder(sparse=True, dtype=np.uint8)\n\n        X = np.array([[0, 1], [1, 0]])\n        enc.fit(X)\n\n        X_trans = enc.transform(X)\n        assert X_trans.dtype == np.uint8\n", "    def test_onehotencoder_invalid_axis(self):\n        enc = OneHotEncoder(sparse=True)\n        X = np.array([[1, 2], [3, 4]])\n        with pytest.raises(ValueError):\n            enc.fit_transform(X, axis=2)\n", "    def test_invalid_dtype(self):\n        enc = OneHotEncoder(dtype='invalid')\n        with pytest.raises(TypeError):\n            enc.fit([[1, 2], [3, 4]])\n", "    def test_one_hot_encoder_fit_transform(self, categories):\n        X = np.array([[1, \"a\"], [2, \"b\"], [1, \"a\"]])\n        enc = OneHotEncoder(categories=categories)\n        enc.fit(X)\n        transformed = enc.transform(X)\n        \n        expected_categories = [\n            np.array([\"a\", \"b\"]),\n            np.array([1, 2, 3])\n        ]\n        if categories is not None:\n            assert_equal(enc.categories_, expected_categories)\n        \n        if categories is None:\n            expected = sparse.csr_matrix(\n                [[1, 0, 0, 0, 1],\n                 [0, 1, 0, 0, 1],\n                 [1, 0, 0, 0, 1]])\n        elif categories == 'auto':\n            expected = sparse.csr_matrix(\n                [[1, 0, 0, 0, 1],\n                 [0, 1, 0, 0, 1],\n                 [1, 0, 0, 0, 1]])\n        else:\n            expected = sparse.csr_matrix(\n                [[1, 0, 0, 0, 1],\n                 [0, 1, 0, 0, 1],\n                 [1, 0, 0, 0, 1]])\n        assert_allclose(toarray(transformed), toarray(expected))\n"], "sample_785": ["    def test_train_test_split_sparse_data():\n        X_sparse = coo_matrix(np.random.rand(10, 5))\n        X_dense = X_sparse.toarray()\n\n        X_train_sparse, X_test_sparse, y_train, y_test = train_test_split(\n            X_sparse, y, test_size=0.3, random_state=42)\n\n        X_train_dense, X_test_dense, _, _ = train_test_split(\n            X_dense, y, test_size=0.3, random_state=42)\n        assert_array_equal(X_train_sparse.toarray(), X_train_dense)\n        assert_array_equal(X_test_sparse.toarray(), X_test_dense)\n", "    def test_check_cv_nested_gridsearchcv(self):\n        from sklearn.model_selection import GridSearchCV\n\n        inner_cv = RepeatedKFold(n_splits=5, n_repeats=2, random_state=42)\n        param_grid = {'a': [1, 2, 3], 'b': [4, 5, 6]}\n        clf = Ridge()\n        gs = GridSearchCV(estimator=clf, param_grid=param_grid, cv=inner_cv, n_jobs=-1)\n\n        assert check_cv(gs.cv) == inner_cv\n", "    def test_train_test_split_sparse_output(self):\n        X = coo_matrix((np.ones(10), (np.arange(10), np.arange(10))))\n        y = np.arange(10)\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n        assert isinstance(X_train, csr_matrix)\n        assert isinstance(X_test, csr_matrix)\n", "    def test_train_test_split_sparse_output(self):\n        X_sparse = coo_matrix((np.ones(5), (0, 0)))\n        X_dense = X_sparse.toarray()\n        X_train, X_test, _, _ = train_test_split(X_sparse,\n                                                    X_sparse,\n                                                    test_size=0.3)\n        assert isinstance(X_train, csc_matrix)\n\n        X_train, X_test, _, _ = train_test_split(X_dense,\n                                                    X_dense,\n                                                    test_size=0.3)\n        assert isinstance(X_train, np.ndarray)\n", "    def test_cv_n_jobs(self, n_jobs):\n        X, y = make_classification(n_samples=100, n_features=20, random_state=42)\n        cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n        model = MockClassifier()\n        scores = cross_val_score(model, X, y, cv=cv, n_jobs=n_jobs)\n        assert len(scores) == 5\n\n\n\n"], "sample_635": ["    def test_accepts_missing_params_docstring(self):\n        \"\"\"Test that missing parameter docstrings are flagged when\n        \"accept_no_param_doc\" is False.\"\"\"\n        code = \"\"\"\n            pass\n        \"\"\"\n        self.assert_messages(\n            code,\n            [\n                (\n                    \"C0111\",\n                    \"Missing docstring in function definition\",\n                    (1, 4),\n                    (1, 14),\n                )\n            ],\n        )\n", "    def test_missing_param_docstring_with_keyword_args_raises_error(self):\n        \"\"\"Tests for missing param docstring with keyword arguments\"\"\"\n        code = \"\"\"", "    def test_param_doc_in_property_types(self):\n        \"\"\"\n        Check that a parameter docstring\n        is not requested for property types.\n        \"\"\"\n        code = \"\"\"\n        class MyClass:\n            @property\n                return None\n\n        class MyClass:\n                self.my_property = my_property\n        \"\"\"\n        messages = self.check(code)\n        assert len(messages) == 0\n", "    def test_param_documentation_missing_type(self):\n        \"\"\"\n        Test that the checker issues a message when a function parameter lacks\n        a type annotation but the docstring doesn't explicitly say so.\n        \"\"\"\n        code = \"\"\"\n            \"\"\"\n            param1: first parameter\n            param2: second parameter\n            \"\"\"\n            pass\n        \"\"\"\n        self.add_code(code)\n        self.check([\n            \"missing-param-type\",\n            \"param_type\",\n            \"1:6\",\n            \"param1\",\n        ])\n        self.check([\n            \"missing-param-type\",\n            \"param_type\",\n            \"1:12\",\n            \"param2\",\n        ])\n\n", "    def test_property_not_detected_as_param(self):\n        \"\"\"Test that property accessors are not treated as parameters.\"\"\"\n        code = \"\"\"\n        class MyClass:\n            @property\n                return 42\n        \"\"\"\n        self.assertMessages(\n            code,\n            [\n                (\n                    \"R0201\",\n                    \"Missing parameter docstring\",\n                    (\n                        self.filename,\n                        1,\n                        19,\n                    ),\n                )\n            ],\n        )\n\n\n\n"], "sample_1095": ["    def test_unrank_lex(self):\n        for i in range(10):\n            perm = Permutation.unrank_lex(5, i)\n            self.assertEqual(perm.rank(), i)\n        self.assertEqual(Permutation.unrank_lex(5, 100).rank(), 100)\n\n", "    def test_adj_distance_small(self):\n        p = Permutation([0, 3, 1, 2, 4])\n        q = Permutation([0, 1, 2, 3, 4])\n        self.assertEqual(p.get_adjacency_distance(q), 2)\n        self.assertEqual(q.get_adjacency_distance(p), 2)\n", "compilation error", "    def test_unrank_lex_small(self):\n        for n in range(1, 6):\n            for i in range(n!):\n                p = Permutation.unrank_lex(n, i)\n                self.assertEqual(p.rank(), i)\n                self.assertEqual(p.array_form, list(range(n)))\n", "    def test_next_lex(self):\n        p = Permutation([1, 2, 0])\n        q = p.next_lex()\n        assert q == Permutation([2, 0, 1])\n        assert p.rank() < q.rank()\n\n"], "sample_167": ["    def test_naturaltime_future_seconds(self):\n        self.assertEqual(humanize.naturaltime(MockDateTime(2012, 3, 9, 22, 35)), 'a second from now')\n        self.assertEqual(humanize.naturaltime(MockDateTime(2012, 3, 9, 22, 35, 30)), '5 seconds from now')\n        self.assertEqual(humanize.naturaltime(MockDateTime(2012, 3, 9, 22, 45, 30)), '10 minutes from now')\n\n\n", "    def test_naturaltime_future(self):\n        with override_settings(USE_TZ=True):\n            tz = get_fixed_timezone(timedelta(hours=5))\n            future_datetime = MockDateTime(2023, 3, 14, 10, 30, tzinfo=tz)\n            rendered = humanize.naturaltime(future_datetime)\n            self.assertEqual(rendered, _('1 day from now'))\n\n\n\n", "    def test_intword_large_number(self):\n        self.assertEqual(humanize.intword(1000000), '1.0 million')\n        self.assertEqual(humanize.intword(1200000), '1.2 million')\n        self.assertEqual(humanize.intword(1200000000), '1.2 billion')\n        self.assertEqual(humanize.intword(1200000000000), '1200.0 trillion')\n\n\n\n", "    def test_naturaltime_with_future_time(self):\n        with override_settings(USE_TZ=True):\n            future_time = datetime.datetime(2023, 3, 10, 0, 0, 0, tzinfo=get_fixed_timezone('US/Eastern'))\n            self.assertEqual(humanize.naturaltime(future_time), 'tomorrow')\n", "    def test_naturaltime_future(self):\n        with override_settings(TIME_ZONE='America/Chicago'):\n            future_time = datetime.datetime(2012, 3, 11, 22, 30)\n            with MockDateTime.patch():\n                result = humanize.naturaltime(future_time)\n                self.assertEqual(result, 'a couple of days from now')\n\n            with translation.override(locale='en'):\n                result = humanize.naturaltime(future_time)\n                self.assertEqual(result, 'a couple of days from now')\n\n"], "sample_981": ["    def test_next_trotterjohnson():\n        p = Permutation.josephus(4, 5, 1)\n        p = p.next_trotterjohnson()\n        assert p == Permutation([0, 2, 4, 1, 3])\n", "  def test_unrank_trotterjohnson(self):\n        for i in range(10):\n            p = Permutation.random(5)\n            rank = p.rank_trotterjohnson()\n            unranked = Permutation.unrank_trotterjohnson(5, rank)\n            assert unranked == p\n  ", "    def test_trotterjohnson_unrank_inverse(self):\n        for n in range(1, 7):\n            for i in range(Perm.unrank_trotterjohnson(n, 0)):\n                p1 = Perm.unrank_trotterjohnson(n, i)\n                p2 = p1.next_trotterjohnson()\n                assert p2 is None or p2.rank_trotterjohnson() == i + 1\n\n\n", "    def test_next_lex(self):\n        for n in range(1, 5):\n            for i in range(2**n):\n                p = Permutation.unrank_lex(n, i)\n                p_next = p.next_lex()\n                if p_next:\n                    assert p_next.rank() == i + 1\n                    assert p_next != p\n                else:\n                    assert p_next is None\n                p = p_next\n", "    def test_unrank_trotterjohnson():\n        for i in range(5):\n            for j in range(ifac(i+1)):\n                p = Permutation.unrank_trotterjohnson(i+1, j)\n                assert p.rank_trotterjohnson() == j\n"], "sample_222": ["    def test_lock_file_locking(self):\n        with tempfile.NamedTemporaryFile('w+b', delete=False) as f:\n            path = f.name\n            # Test locking with multiple processes\n            with mock.patch('threading.Thread') as mock_thread:\n                locks.lock(f, locks.LOCK_EX)\n                mock_thread.return_value.start.return_value = None\n                with locks.lock(f, locks.LOCK_EX):\n                    self.assertTrue(os.path.exists(path))\n                mock_thread.assert_called_once_with(target=locks.lock, args=(f, locks.LOCK_EX))\n\n            locks.unlock(f)\n            os.remove(path)\n\n\n\n", "    def test_lock_unlock_with_temporary_uploaded_file(self):\n        with NamedTemporaryFile(mode='w+b', delete=False) as temp_file:\n            temp_file_path = temp_file.name\n            with tempfile.NamedTemporaryFile(mode='w+b', delete=False) as temp_file2:\n                temp_file2_path = temp_file2.name\n\n                uploaded_file = TemporaryUploadedFile(\n                    'temp_file',\n                    open(temp_file_path, 'rb').read(),\n                    content_type='text/plain'\n                )\n\n                with locks.lock(uploaded_file.file, locks.LOCK_EX):\n                    self.assertTrue(lock_acquired, lock_acquired)\n                    # Mimic some operation that requires locking\n                    file_move_safe(uploaded_file.file, temp_file2_path)\n\n                # Ensure the lock is released\n                self.assertFalse(lock_acquired)\n\n\n\n", "    def test_lock_file_on_windows(self):\n        with tempfile.TemporaryFile('w+b') as f:\n            self.assertTrue(locks.lock(f, locks.LOCK_EX))\n            with self.assertRaises(ValueError):\n                locks.lock(f, locks.LOCK_EX)\n            locks.unlock(f)\n", "    def test_lock_with_different_locks(self):\n        with tempfile.NamedTemporaryFile(delete=False) as f:\n            # Acquire exclusive lock\n            self.assertTrue(locks.lock(f, locks.LOCK_EX))\n            # Try to acquire another exclusive lock - should fail\n            with self.assertRaises(BlockingIOError):\n                locks.lock(f, locks.LOCK_EX)\n            # Release the lock\n            locks.unlock(f)\n            # Acquire a shared lock\n            self.assertTrue(locks.lock(f, locks.LOCK_SH))\n            # Try to acquire an exclusive lock - should fail\n            with self.assertRaises(BlockingIOError):\n                locks.lock(f, locks.LOCK_EX)\n            # Release the shared lock\n            locks.unlock(f)\n\n\n\n", "    def test_lock_file(self):\n        with NamedTemporaryFile('w+b') as temp_file:\n            self.assertFalse(locks.lock(temp_file, locks.LOCK_EX))\n            with self.assertRaises(BlockingIOError):\n                locks.lock(temp_file, locks.LOCK_EX)\n\n\n\n"], "sample_1078": ["    def test_idx_bounds(self):\n        m, n, o, p = symbols('m n o p', integer=True)\n        i = Idx('i', m)\n        j = Idx('j', n)\n        k = Idx('k', o)\n        l = Idx('l', p)\n        \n        assert i.lower == 0\n        assert i.upper == m - 1\n\n        assert j.lower == 0\n        assert j.upper == n - 1\n\n        assert k.lower == 0\n        assert k.upper == o - 1\n\n        assert l.lower == 0\n        assert l.upper == p - 1\n\n        raises(TypeError, lambda: Idx('i', 'm'))\n        raises(TypeError, lambda: Idx('i', [1,2,3]))\n        with self.assertRaises(TypeError):\n            Idx('i', oo, oo)\n        with self.assertRaises(TypeError):\n            Idx('i', [oo, 1])\n\n\n\n", "    def test_idx_from_integer_with_bounds():\n        from sympy.tensor import Idx\n\n        with raises(TypeError):\n            Idx(1, 'a')\n        with raises(TypeError):\n            Idx(1, (2, 3), foo='bar')\n        with raises(TypeError):\n            Idx(1, 'a', foo='bar')\n\n        idx = Idx(1, 3)\n        assert idx.lower == 0\n        assert idx.upper == 2\n\n        idx = Idx(1, (5, 10))\n        assert idx.lower == 5\n        assert idx.upper == 9\n\n        idx = Idx(1, oo)\n        assert idx.lower == 0\n        assert idx.upper == oo\n", "    def test_idx_infinite_range():\n        from sympy.abc import x\n\n        i = Idx('i', oo)\n        assert i.lower is None\n        assert i.upper is oo\n\n        i = Idx('i', -oo)\n        assert i.lower is -oo\n        assert i.upper is None\n\n        i = Idx('i', (oo, oo))\n        assert i.lower is oo\n        assert i.upper is oo\n", "def test_range_error():\n    from sympy import Idx\n\n    with raises(TypeError):\n        Idx('i', 'j')\n\n    with raises(TypeError):\n        Idx('i', (1, 2.5))\n\n    with raises(TypeError):\n        Idx('i', Symbol('j'))\n\n\n", "    def test_shape_override(self):\n        A = IndexedBase('A', shape=(2, 3))\n        B = IndexedBase('B')\n        i = Idx('i', 5)\n        j = Idx('j', 4)\n        assert A[i, j].shape == (2, 3)\n        assert B[i, j].shape == (5, 4)\n"], "sample_654": ["    def test_parametrize_fixture_scope_handling(self, testdir):\n        testdir.makepyfile(\n            \"\"\"", "    def test_fixture_function_direct_call(testdir):\n        testdir.makepyfile(\n            \"\"\"\n                return \"fixture value\"\n\n            @pytest.fixture\n                return \"fixture value\"\n        \"\"\"\n        )\n\n        result = testdir.runpytest()\n        result.assert_outcomes(passed=2)\n\n        p = testdir.getpyobject(\"test_fixture_function_direct_call\")\n        print(p)\n        assert hasattr(p, \"_pytestfixturefunction\")\n        assert hasattr(p.fixture_func, \"_pytestfixturefunction\")\n        assert hasattr(p.my_fixture, \"_pytestfixturefunction\")\n\n\n", "    def test_fixture_direct_call_warns(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.fixture\n                return \"fixture value\"\n\n                assert myfixture == \"fixture value\"\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        assert result.ret == 0\n        output = result.stdout.str()\n        assert \"fixture 'myfixture' was called directly\" in output\n", "    def test_parametrized_fixture_scope(self, testdir):\n        p = testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.fixture(scope=\"function\")\n                return \"func\"\n\n            @pytest.fixture(scope=\"session\")\n                return \"session\"\n\n                assert func_fixture == \"func\"\n                assert session_fixture == \"session\"\n\n            @pytest.mark.parametrize(\"arg\", [\"a\", \"b\"], indirect=True)\n                assert func_fixture == \"func\"\n                assert session_fixture == \"session\"\n                assert arg == \"a\" or arg == \"b\"\n            \"\"\"\n        )\n        reprec = testdir.runpytest(p)\n        reprec.assert_outcomes(passed=2)\n\n\n", "    def test_fixture_order_and_scope(self, testdir):\n        p = testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.fixture(scope=\"module\")\n                return \"module\"\n\n            @pytest.fixture(scope=\"class\")\n                return \"class\"\n\n                assert module_fixture == \"module\"\n                assert class_fixture == \"class\"\n\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.assert_outcomes(passed=1)\n\n\n        "], "sample_45": ["    def test_truncate_time_field(self):\n        for kind in ['second', 'minute', 'hour']:\n            obj = self.create_model(datetime(2023, 10, 26, 15, 30, 45, 123456), datetime(2023, 10, 26, 17, 30, 45, 123456))\n            truncated = TruncTime(obj.start_datetime).as_sql()\n            expected_truncated = truncate_to(obj.start_datetime, kind)\n            self.assertEqual(truncated[0], f\"CAST(DATE_TRUNC('{kind}', {obj.start_datetime}) AS TIME)\")\n            self.assertEqual(truncated[1], ())\n\n            truncated = TruncTime(obj.end_datetime).as_sql()\n            expected_truncated = truncate_to(obj.end_datetime, kind)\n            self.assertEqual(truncated[0], f\"CAST(DATE_TRUNC('{kind}', {obj.end_datetime}) AS TIME)\")\n            self.assertEqual(truncated[1], ())\n\n            truncated = TruncTime().as_sql()\n            self.assertEqual(truncated, ('CURRENT_TIME', ()))\n\n\n\n", "    def test_truncate_to_datetime(self):\n        for kind in ['second', 'minute', 'hour', 'day', 'week', 'month', 'quarter', 'year']:\n            with self.subTest(kind=kind):\n                model = self.create_model(\n                    start_datetime=datetime(2023, 10, 26, 10, 30, 45, tzinfo=timezone.utc),\n                    end_datetime=datetime(2023, 10, 27, 12, 45, 1, tzinfo=timezone.utc),\n                )\n                expected = truncate_to(model.start_datetime, kind)\n                actual = Trunc(model.start_datetime, kind).resolve_expression().output_field\n                self.assertEqual(actual, expected)\n                \n\n\n\n", "    def test_trunc_with_timezone(self):\n        eastern = pytz.timezone('US/Eastern')\n        central = pytz.timezone('US/Central')\n\n        now_eastern = timezone.now(eastern)\n        now_central = timezone.now(central)\n\n        model = self.create_model(start_datetime=now_eastern, end_datetime=now_eastern + timedelta(hours=1))\n        truncated_eastern = model.start_datetime.timezone(eastern).astimezone(eastern).replace(tzinfo=None)\n        truncated_central = model.start_datetime.timezone(central).astimezone(central).replace(tzinfo=None)\n\n        self.assertEqual(TruncDate().as_sql(model.start_datetime, connection=self.connection)[0], f\"CAST(date(now() AT TIME ZONE 'UTC') AS DATE)\")\n        self.assertEqual(TruncTime().as_sql(model.start_datetime, connection=self.connection)[0], f\"CAST(time(now() AT TIME ZONE 'UTC') AS TIME)\")\n\n        truncated_eastern = TruncDate()(model.start_datetime).resolve().astimezone(eastern)\n        truncated_central = TruncDate()(model.start_datetime).resolve().astimezone(central)\n\n        self.assertEqual(truncated_eastern, now_eastern.date())\n        self.assertEqual(truncated_central, now_central.date())\n\n", "    def test_extract_extract_in_datetimefield_with_tz(self):\n        now_tz = timezone.now()\n        with override_settings(USE_TZ=True):\n            model = self.create_model(now_tz, now_tz + timedelta(hours=1))\n            result = model.objects.values('start_datetime__year').first()\n            self.assertEqual(result['start_datetime__year'], now_tz.year)\n", "    def test_trunc_date(self):\n        model = self.create_model(datetime(2023, 10, 27, 10, 30, 0), datetime(2023, 10, 27, 12, 45, 0))\n        self.assertEqual(TruncDate(model.start_datetime).as_sql(),\n                         'TRUNC(start_datetime, \"date\")')\n        self.assertEqual(TruncDate(model.start_datetime).output_field, DateField())\n"], "sample_393": ["    def test_no_obsolete(self):\n        # Make sure --no-obsolete flag works as expected\n        with self.temp_dir():\n            self.create_template_files()\n            with captured_stderr() as stderr:\n                execute_from_command_line(\n                    [\"makemessages\", \"--no-obsolete\", \"--locale\", LOCALE]\n                )\n            with open(self.PO_FILE) as fp:\n                po_contents = fp.read()\n            self.assertNotIn(\"obsolete\", po_contents, \"Obsolete msgids found\")\n", "    def test_no_obsolete_option(self):\n        output, po_contents = self._run_makemessages(no_obsolete=True)\n        self.assertNotRegex(\n            po_contents, \"msgstr|\\\"Obsolete\\\".*\\n\",\n            \"Obsolete messages should be removed when --no-obsolete is passed\"\n        )\n        output, po_contents = self._run_makemessages(\n            no_obsolete=True,\n            locale=[LOCALE]\n        )\n\n\n", "    def test_no_locale_path(self):\n        self.populate_locale()\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with captured_stderr() as stderr:\n            execute_from_command_line([\"python\", \"-m\", \"django.core.management\", \"makemessages\", \"--locale\", LOCALE])\n        self.assertEqual(stderr.getvalue().strip(), '\"%s\" not found in final .po file.' % self.PO_FILE)\n\n", "    def test_empty_locale_dir(self):\n        \"\"\"\n        Ensure we correctly handle cases where a locale directory doesn't exist.\n        \"\"\"\n        # Remove the locale directory explicitly\n        os.makedirs(os.path.join(self.work_dir, 'locale'), exist_ok=True)\n        os.rmdir(os.path.join(self.work_dir, 'locale'))\n\n        output, _ = self._run_makemessages(locale=[LOCALE])\n        self.assertIn('locale directory does not exist:', output)\n        self.assertFalse(os.path.exists(self.PO_FILE))\n\n", "    def test_make_messages_handles_warnings(self):\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            # Trigger a warning from the gettext utility\n            with captured_stdout() as out:\n                execute_from_command_line([\"django-admin\", \"makemessages\", \"-l\", LOCALE])\n\n            self.assertEqual(len(w), 1)\n            self.assertEqual(w[0].category, TranslatorCommentWarning)\n            self.assertIn(\"Placeholder\", str(w[0].message))\n            self.assertRegex(out.getvalue(), r\"Generating POT file.*locale\\/\")\n\n\n\n"], "sample_1126": ["    def test_dagger_matrix(self):\n        from sympy.physics.quantum.dagger import Dagger\n        m = Matrix([[1, I], [2, I]])\n        m_dagger = Dagger(m)\n        self.assertEqual(m_dagger, Matrix([[1, 2], [-I, -I]]))\n        # Test with a more complex matrix\n        n = Matrix([[1 + I, 2 - 3*I], [4 + 5*I, 6 - 7*I]])\n        n_dagger = Dagger(n)\n        self.assertEqual(n_dagger, Matrix([[1 - I, 4 - 5*I], [2 + 3*I, 6 + 7*I]])) \n", "    def test_matrix_multiplication(self):\n        A = Matrix([[1, 2], [3, 4]])\n        B = Matrix([[5, 6], [7, 8]])\n        self.assertEqual(adjoint(A * B), adjoint(B) * adjoint(A))\n", "    def test_dagger_matrix_multiplication(self):\n        a = Matrix([[1, 2], [3, 4]])\n        b = Matrix([[5, 6], [7, 8]])\n        self.assertEqual(Dagger(a * b), Dagger(b) * Dagger(a)) \n", "    def test_dagger_matrix_multiplication(self):\n        A = Matrix([[1, 2], [3, 4]])\n        B = Matrix([[5, 6], [7, 8]])\n        self.assertEqual(Dagger(A*B), Dagger(B)*Dagger(A))\n", "    def test_matrix_dagger(self):\n        A = Matrix([[1, 2], [3, 4]])\n        self.assertEqual(Dagger(A), A.transpose().conjugate())\n"], "sample_40": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_872": ["compilation error", "    def test_top_k_accuracy_score_multilabel_with_weight(drop, sample_weight):\n        y_true, y_pred, y_score = make_prediction(binary=False)\n        y_true = np.where(y_true == 0, 0, 1)  # Convert to binary\n        k = 2\n\n        if sample_weight is None:\n            score = top_k_accuracy_score(y_true, y_score, k=k)\n        else:\n            score = top_k_accuracy_score(y_true, y_score, k=k, sample_weight=sample_weight)\n\n        expected_score = np.sum(\n            (y_score.argsort(axis=1)[:, :k] == y_true[:, None])\n            .any(axis=1)\n        ) / len(y_true)\n        assert_allclose(score, expected_score)\n", "    def test_top_k_accuracy_score_multiclass(self, drop):\n        y_true, y_pred, y_score = make_prediction(binary=False)\n        k = 3\n        score = top_k_accuracy_score(y_true, y_score, k=k, normalize=normalize, drop=drop)\n        if normalize:\n            assert 0.6 <= score <= 1.0, \"Unexpected top-k accuracy score: {}\".format(score)\n        else:\n            assert 0 <= score <= len(y_true)\n", "compilation error", "    def test_top_k_accuracy_score_multiclass(drop, k):\n        y_true, y_pred, y_score = make_prediction(binary=False)\n        if drop:\n            y_true = y_true[:100]\n            y_pred = y_pred[:100]\n            y_score = y_score[:100]\n        score = top_k_accuracy_score(y_true, y_score, k=k)\n        # this might not be the best way to compare the scores\n        # as it depends on the randomness of the prediction\n        assert 0.3 <= score <= 0.7\n\n"], "sample_348": ["    def test_raw_id_field_on_admin(self):\n        class RawIdModelAdmin(ModelAdmin):\n            raw_id_fields = ['song']\n\n        self.assertIsInvalidRegexp(\n            RawIdModelAdmin(ValidationTestInlineModel, AdminSite()),\n            ValidationTestInlineModel,\n            r\"The field 'song' does not exist.\",\n            hint=\"Did you mean to name it 'Song'?\",\n            id='raw_id_fields'\n        )\n\n\n\n", "    def test_raw_id_filter(self):\n        class RawIdFilter(SimpleListFilter):\n            title = \"Raw ID\"\n            parameter_name = \"raw_id\"\n\n                return [('0', 'Zero'), ('1', 'One'), ('2', 'Two')]\n\n                return [(lookup, lookup) for lookup in self.lookups(None, None)]\n\n                raw_id = request.GET.get(self.parameter_name)\n                if raw_id:\n                    return queryset.filter(pk=raw_id)\n                return queryset\n\n        class RawIdModelAdmin(ModelAdmin):\n            list_filter = [RawIdFilter]\n\n        admin.site.register(ValidationTestModel, RawIdModelAdmin)\n        self.assertIsInvalid(RawIdModelAdmin, ValidationTestModel,\n                            r\"^Raw ID filter must have a valid raw ID.$\")\n", "    def test_raw_id_fields(self):\n        class RawIdInline(TabularInline):\n            model = ValidationTestInlineModel\n            extra = 0\n            formset = BaseModelFormSet\n\n        class RawIdModelAdmin(ModelAdmin):\n            inlines = [RawIdInline]\n\n        self.assertIsInvalid(RawIdModelAdmin, ValidationTestModel,\n                             r\"Cannot specify both 'raw_id_fields' and 'formset'\",\n                             id='field_errors.E001',\n                             hint='When defining a \"formset\" attribute for an inline, '\n                                  '\"raw_id_fields\" should not be specified, '\n                                  'as it is redundant.',\n                             invalid_obj=RawIdInline)\n\n\n\n\n", "    def test_raw_id_fields(self):\n        class RawIdInlineForm(forms.ModelForm):\n            class Meta:\n                model = ValidationTestInlineModel\n                fields = ['id']\n                raw_id_fields = ['parent_validation_test']\n\n        class RawIdInline(TabularInline):\n            form = RawIdInlineForm\n            model = ValidationTestInlineModel\n            extra = 0\n\n        class RawIdAdmin(ModelAdmin):\n            inlines = [RawIdInline]\n\n        self.assertIsInvalid(\n            RawIdAdmin(ValidationTestModel, AdminSite()),\n            ValidationTestModel,\n            r\"The field 'id' has a 'raw_id_fields' attribute\",\n            id=\"inlines.0.fields.id\",\n        )\n\n\n", "    def test_raw_id_check(self):\n        class RawIdAdmin(ModelAdmin):\n            list_display = ('id', 'name')\n            raw_id_fields = ('related_band',)\n\n        self.assertIsInvalid(RawIdAdmin(ValidationTestModel, AdminSite()),\n                             ValidationTestModel,\n                             'Raw ID fields should not be defined on models without a ForeignKey',\n                             id='raw_id_fields')\n\n"], "sample_33": ["    def test_did_you_mean_no_matches(self):\n        with pytest.raises(ValueError):\n            misc.did_you_mean(\n                'foobar',\n                candidates=['bar', 'baz'],\n                cutoff=0.8,\n                fix=lambda x: x\n            )\n", "    def test_set_locale():\n        with set_locale('C'):\n            float_value = 1.23456789\n            str_value = '1.23456789'\n            assert float(str_value) == float_value\n\n        with set_locale('en_US'):\n            float_value = 1.23456789\n            str_value = '1,234,567.89'  # US locale uses comma as thousands separator\n            assert float(str_value) == float_value\n", "    def test_strip_accents(self):\n        self.assertEqual(misc.strip_accents('\u00e4\u00f6\u00fc'), 'aou')\n        self.assertEqual(misc.strip_accents('\u00c4\u00d6\u00dc'), 'AUU')\n        self.assertEqual(misc.strip_accents('\u00e9\u00e0\u00e7'), 'eac')\n        self.assertEqual(misc.strip_accents('\u00c9\u00c0\u00c7'), 'EAC')\n        self.assertEqual(misc.strip_accents('\u00c9\u00e4\u00d6\u00fc'), 'EauOu')\n        self.assertEqual(misc.strip_accents('This string has no accents'),\n                         'This string has no accents')\n", "    def test_strip_accents(self):\n        assert misc.strip_accents(\"\u00f6\u00e4\u00fc\") == \"oau\"\n        assert misc.strip_accents(\"caf\u00e9\") == \"cafe\"\n        assert misc.strip_accents(\"This is a test string.\") == \"This is a test string.\"\n        \n        # Test with longer strings\n        assert misc.strip_accents(\"This is a test string with some accented characters: \u00f6\u00e4\u00fc\") == \"This is a test string with some accented characters: oau\"\n", "    def test_strip_accents(self):\n        assert misc.strip_accents('\u00f6\u00e4\u00fc') == 'oau'\n        assert misc.strip_accents('\u00e5\u00c4\u00d6') == 'ao'\n        assert misc.strip_accents('\u00e9\u00e0\u00e7') == 'eac'\n        assert misc.strip_accents('\u00f1') == 'n'\n        assert misc.strip_accents('\u0161') == 's'\n        assert misc.strip_accents('\u00df') == 'ss'\n        assert misc.strip_accents('\u00a3') == '\u00a3'\n\n        assert misc.strip_accents('This is a test string.') == 'This is a test string.'\n\n\n\n"], "sample_639": ["    def test_inheritance(self):\n        checker = LessBasicChecker()\n        self.assertEqual(checker.name, \"basic\")\n        self.assertEqual(checker.msgs[\"W0001\"][0], \"Basic checker has an example.\")\n        self.assertEqual(checker.options[0][0], \"example-args\")\n        self.assertEqual(checker.options[0][1][\"default\"], 42)\n\n\n", "    def test_ordering_of_checkers(self):\n        checker1 = BaseChecker()\n        checker2 = BaseChecker()\n        checker3 = OtherBasicChecker()\n        checker4 = DifferentBasicChecker()\n        checkers = [checker1, checker2, checker3, checker4]\n\n        sorted_checkers = sorted(checkers)\n        assert sorted_checkers[0].name == checker1.name\n        assert sorted_checkers[1].name == checker2.name\n        assert sorted_checkers[2].name == checker3.name\n        assert sorted_checkers[3].name == checker4.name\n\n\n\n", "    def test_equality(self):\n        c1 = OtherBasicChecker()\n        c2 = OtherBasicChecker()\n        self.assertEqual(c1, c2)\n\n        c3 = LessBasicChecker()\n        self.assertNotEqual(c1, c3)\n        self.assertNotEqual(c2, c3)\n\n        c4 = DifferentBasicChecker()\n        self.assertNotEqual(c1, c4)\n        self.assertNotEqual(c2, c4)\n", "    def test_checker_ordering(self):\n        checker1 = OtherBasicChecker()\n        checker2 = DifferentBasicChecker()\n        self.assertGreater(checker1, checker2)\n        self.assertEqual(checker1, checker1)\n        self.assertNotEqual(checker1, checker2)\n\n\n\n", "    def test_get_full_documentation(self):\n        checker = OtherBasicChecker()\n        doc = checker.get_full_documentation()\n        assert \"OtherBasicChecker checker\" in doc\n        assert \"Basic checker has an example.\" in doc\n        assert \"example-args\" in doc \n        assert \"This checker is provided by ``OtherBasicChecker``\" in doc\n\n\n\n"], "sample_18": ["    def test_info_propagation_arith(self):\n        q2 = 2 * self.q\n        assert_info_equal(self.q, q2, ignore={\"value\"})\n\n        q3 = self.q + 1\n        assert_info_equal(self.q, q3, ignore={\"value\"})\n\n        q4 = self.q - 2\n        assert_info_equal(self.q, q4, ignore={\"value\"})\n", "    def test_copy_info(self):\n        assert_info_equal(self.q, self.q2)    \n", "compilation error", "    def test_info_propagation_addition(self):\n        other = u.Quantity(np.arange(5.0, 9.0), \"m/s\")\n        result = self.q + other\n\n        assert_info_equal(result, self.q, ignore={\"value\"})\n\n", "    def test_info_propagation_copy(self):\n        q2 = copy.deepcopy(self.q)\n        assert_info_equal(self.q, q2)\n"], "sample_50": ["    def test_runshell_db_password_in_connection_params(self):\n        dbinfo = {'host': 'localhost', 'database': 'mydatabase', 'password': 'mypassword'}\n        args, pgpassword = self._run_it(dbinfo)\n        self.assertEqual(args, ['psql', '-U', '', '-h', 'localhost', '-p', '', 'mydatabase'])\n        self.assertEqual(pgpassword, 'mypassword')\n", "    def test_runshell_db_with_password(self):\n        dbinfo = {\n            'host': 'localhost',\n            'port': '5432',\n            'database': 'mydatabase',\n            'user': 'myuser',\n            'password': 'mypassword',\n        }\n        args, pgpassword = self._run_it(dbinfo)\n        self.assertEqual(args, ['psql', '-U', 'myuser', '-h', 'localhost', '-p', '5432', 'mydatabase'])\n        self.assertEqual(pgpassword, 'mypassword')\n", "    def test_runshell_db_with_empty_password(self):\n        dbinfo = {\n            'host': 'localhost',\n            'port': '5432',\n            'database': 'mydatabase',\n            'user': 'myuser'\n        }\n        args, pgpassword = self._run_it(dbinfo)\n        self.assertEqual(args, ['psql', '-U', 'myuser', '-h', 'localhost', '-p', '5432', 'mydatabase'])\n        self.assertIsNone(pgpassword)\n", "compilation error", "    def test_runshell_db_with_password(self):\n        dbinfo = {\n            'host': 'localhost',\n            'port': '5432',\n            'database': 'mydatabase',\n            'user': 'myuser',\n            'password': 'mypassword',\n        }\n        args, pgpassword = self._run_it(dbinfo)\n        self.assertEqual(args, [\n            'psql',\n            '-U', 'myuser',\n            '-h', 'localhost',\n            '-p', '5432',\n            'mydatabase',\n        ])\n        self.assertEqual(pgpassword, 'mypassword')\n"], "sample_634": ["    def test_expand_modules_with_relative_imports(\n        self,", "    def test_expand_modules_with_init_only_files(self):\n        ignore_list = [\"test_module\"]\n        ignore_list_re = [re.compile(r\"test\")]\n        ignore_list_paths_re = [re.compile(r\".*\")]\n        files_or_modules = [\n            \"test_module/__init__.py\",\n            \"other_module.py\",\n            \"nonexistent_module\",\n            \"test_directory/__init__.py\",\n        ]\n\n        result, errors = expand_modules(\n            files_or_modules,\n            ignore_list,\n            ignore_list_re,\n            ignore_list_paths_re,\n        )\n\n        assert len(result) == 2\n        assert {\"path\": \"other_module.py\", \"name\": \"other_module\"} in result\n        assert {\"path\": \"test_directory/__init__.py\", \"name\": \"test_directory\"} in result\n\n        assert len(errors) == 1\n        assert errors[0][\"key\"] == \"fatal\"\n        assert errors[0][\"mod\"] == \"nonexistent_module\"\n\n        assert set(\n            r.get(\"path\")\n            for r in result\n            if r.get(\"basename\") == \"test_directory\"\n        ).issubset(\n            Path(\"test_directory\").glob(\"*\")\n        )\n", "    def test_expand_modules_namespace_package(\n        self,", "    def test_expand_modules_with_namespace_packages(self):\n        ignore_list = [\"__init__.py\"]\n        ignore_list_re = [re.compile(r\"my_package\")]\n        ignore_list_paths_re = [re.compile(r\"tests\")]\n        files_or_modules = [\"my_package\"]\n        result, errors = expand_modules(\n            files_or_modules,\n            ignore_list,\n            ignore_list_re,\n            ignore_list_paths_re,\n        )\n        assert len(result) == 1\n        assert result[0][\"name\"] == \"my_package\"\n        assert len(errors) == 0\n", "    def test_expand_modules_ignore_file_paths_re(self):\n        ignore_list_paths_re: List[Pattern[str]] = [\n            re.compile(r\"my_project/tests\"),\n            re.compile(r\"my_project/docs\"),\n        ]\n        files_or_modules = [\"my_project/src/file1.py\", \"my_project/tests/test_file.py\"]\n        result, errors = expand_modules(\n            files_or_modules, [], [], ignore_list_paths_re\n        )\n        assert len(result) == 1\n        assert result[0][\"path\"] == \"my_project/src/file1.py\"\n        assert len(errors) == 0\n"], "sample_823": ["    def test_pairwise_distances_chunked_precomputed(metric):\n        X = np.random.rand(1000, 10)\n        if metric == \"precomputed\":\n            X = pairwise_distances(X, metric=metric)\n        D_chunk = next(pairwise_distances_chunked(X, n_jobs=2,\n                                                    chunk_size=100,\n                                                    metric=metric))\n        assert D_chunk.shape[0] == 100\n        assert isinstance(D_chunk, np.ndarray)\n\n        if metric != \"precomputed\":\n            D_full = pairwise_distances(X, metric=metric)\n            assert_array_almost_equal(D_chunk, D_full[:100, :100])\n", "    def test_pairwise_distances_chunked_with_reduce_func_and_small_chunk_size(self):\n        n_samples = 100\n        X = np.random.randn(n_samples, 3)\n        reduce_func = lambda D_chunk, start: np.mean(D_chunk)\n        chunk_size = 10\n        with config_context({'working_memory': 100}):\n            for D_chunk in pairwise_distances_chunked(X, reduce_func=reduce_func,\n                                                    chunk_n_rows=chunk_size):\n                assert D_chunk.shape[0] == chunk_size\n\n\n\n", "def test_pairwise_distances_chunked_reduce_func(metric, dtype):\n    X = np.random.RandomState(0).rand(100, 10)\n    X = X.astype(dtype)\n\n    if metric == \"precomputed\":\n        X = np.random.rand(100, 100)\n\n        return np.mean(D_chunk, axis=1)\n\n    with config_context({\"working_memory\": 100}):\n        for chunk_size in [5, 10, 20, 50, 100]:\n            gen = pairwise_distances_chunked(X, reduce_func=reduce_func,\n                                          chunk_n_rows=chunk_size,\n                                          working_memory=100)\n\n            for i in range(100):\n                chunk = next(gen)\n                assert chunk.shape[0] == chunk_size\n                assert (~np.isnan(chunk)).all()\n            \n            i = 0\n            for chunk in gen:\n                i += chunk.shape[0]\n\n    assert i == 100\n\n\n", "    def test_pairwise_distances_chunked(metric):\n        X = np.random.rand(100, 5)\n        Y = np.random.rand(100, 5)\n\n        # Test with default chunk size and no reduce_func\n        with assert_raises_regexp(ValueError, r\"reduce_func must be a callable\"):\n            list(pairwise_distances_chunked(X, Y, metric=metric))\n\n        # Test with custom chunk size and no reduce_func\n        chunk_size = 20\n        with assert_raises_regexp(ValueError, r\"reduce_func must be a callable\"):\n            list(pairwise_distances_chunked(X, Y, metric=metric,\n                                            working_memory=chunk_size,\n                                            reduce_func=None))\n\n        # Test with reduce_func and chunk size\n            return D_chunk\n\n        for i in range(10):\n            with config_context(working_memory=chunk_size):\n                gen = pairwise_distances_chunked(X, Y, metric=metric,\n                                                 reduce_func=reduce_func,\n                                                 working_memory=chunk_size)\n                result = list(gen)\n            assert len(result) == 1\n            assert result[0].shape == (X.shape[0], Y.shape[0])\n", "    def test_pairwise_euclidean_distances(metric):\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        Y = np.array([[7, 8], [9, 10], [11, 12]])\n        actual = pairwise_distances(X, Y, metric=metric)\n        expected = euclidean_distances(X, Y, metric=metric)\n        assert_allclose(actual, expected)\n"], "sample_586": ["    def test_concat_coords_different_compat(compat):\n        coords = {\n            \"y\": Variable(pd.Index([\"A\", \"B\", \"C\"]), data=np.arange(3)),\n            \"x\": Variable(pd.Index([1, 2, 3]), data=np.arange(3)[:, None]),\n        }\n        ds1 = Dataset(coords)\n        ds2 = Dataset(coords)\n        ds1[\"y\"].data = 1\n        ds2[\"x\"].data = 2\n\n        result = concat([ds1, ds2], dim=\"x\", compat=compat)\n\n        assert_equal(result.coords, {\"y\": ds1.coords[\"y\"], \"x\": ds2.coords[\"x\"]})\n    ", "    def test_concat_identical_with_different_dims():\n        ds1 = create_test_data(\n            dims=[\"x\", \"y\"],\n            coords={\"x\": np.arange(3), \"y\": np.arange(2)},\n            data={\"a\": np.arange(6).reshape((3, 2))},\n        )\n        ds2 = create_test_data(\n            dims=[\"y\", \"z\"],\n            coords={\"y\": np.arange(2)},\n            data={\"a\": np.arange(2).reshape((2, 1))},\n        )\n\n        with raises_regex(ValueError, \"Dataset global attributes not equal\"):\n            concat(\n                [ds1, ds2],\n                dim=\"y\",\n                data_vars=\"all\",\n                coords=\"all\",\n                compat=\"identical\",\n            )\n", "    def test_concat_dataset_different_coord_matching():\n        ds1 = create_test_data(\n            coords={\"x\": [1, 2, 3], \"y\": [4, 5], \"z\": [6, 7, 8]},\n            data={\"a\": [[1, 2, 3], [4, 5, 6]]},\n        )\n        ds2 = create_test_data(\n            coords={\"x\": [1, 2, 3], \"y\": [8, 9], \"z\": [9, 10, 11]},\n            data={\"a\": [[7, 8, 9], [10, 11, 12]]},\n        )\n        concatted_ds = concat(\n            [ds1, ds2], dim=\"y\", coords=\"different\", compat=\"broadcast_equals\"\n        )\n        assert_identical(concatted_ds.coords, {\"x\": [1, 2, 3], \"y\": [4, 5, 8, 9]})\n        assert_identical(concatted_ds.data_vars, {\"a\": [\n            [1, 2, 3, 7, 8, 9],\n            [4, 5, 6, 10, 11, 12],\n        ]})\n\n", "    def test_concat_different(data_vars, coords, compat):\n        ds1 = create_test_data(\n            \"test_concat_different\",\n            dims=(\"x\", \"y\", \"time\"),\n            sizes=(10, 10, 5),\n        )\n        ds2 = deepcopy(ds1)\n        ds2.coords[\"time\"] = ds2.coords[\"time\"].data * 2\n        # make sure ds1 and ds2 contain the same variables and\n        # have the same dims except for the concat dimension\n        ds1 = ds1.expand_dims(dim=\"z\").rebase(dim=\"z\", new_axis=0)\n        ds2 = ds2.expand_dims(dim=\"z\").rebase(dim=\"z\", new_axis=0)\n        result = concat(\n            [ds1, ds2],\n            dim=\"z\",\n            data_vars=data_vars,\n            coords=coords,\n            compat=compat,\n        )\n\n        if compat == \"identical\":\n            assert_identical(ds1, result.isel(z=0))\n            assert_identical(ds2, result.isel(z=1))\n        else:\n            assert_array_equal(\n                result.isel(z=0).coords[\"time\"].data, ds1.coords[\"time\"].data\n            )\n            assert_array_equal(\n                result.isel(z=1).coords[\"time\"].data, ds2.coords[\"time\"].data\n            )\n\n", "    def test_concat_different_coords_compat_override():\n        ds1 = create_test_data(\n            {\n                \"x\": np.arange(10),\n                \"y\": np.arange(10)[:, None],\n            },\n            coords={\"z\": np.array([1, 2, 3])},\n        )\n        ds2 = create_test_data(\n            {\"x\": np.arange(10), \"y\": np.arange(10)[:, None]},\n            coords={\"z\": np.array([1, 2, 4])},\n        )\n\n        with raises_regex(\n            ValueError, r\"dataset indexes are of different size\"\n        ):\n            concat(\n                [ds1, ds2],\n                dim=\"z\",\n                coords=\"different\",\n                compat=\"override\",\n                join=\"outer\",\n            )\n\n"], "sample_1104": ["    def test_sstr_UnevaluatedExpr(self):\n        expr = UnevaluatedExpr([1, 2, 3])\n        self.assertEqual(sstr(expr), 'UnevaluatedExpr([1, 2, 3])')\n\n", "    def test_print_GroebnerBasis(self):\n        basis = groebner([x**2 + y**2 - 1, x - y], x, y)\n        s = sstr(basis)\n        assert s == 'GroebnerBasis([x**2 + y**2 - 1, x - y], order=\"lex\", domain=QQ)'\n\n\n\n", "    def test_print_complex():\n        p = StrPrinter()\n        print(p._print_Pow(1j, 2, rational=True))\n        assert p.doprint(1j*1j) == '-1'\n", "    def test_print_complex_root_of(self):\n        expr = ComplexRootOf(x**3 - 1, 0)\n        self.assertEqual(sstr(expr), \"CRootOf(x**3 - 1, 0)\")", "    def test_print_DMP(self):\n        from sympy.polys import DMP\n        from sympy.polys.polytools import to_list\n        dmp = DMP(ring=ZZ, rep=[1, 2, 3], dom=2)\n        self.assertEqual(sstrrepr(dmp), \"DMP(ring=ZZ, rep=[1, 2, 3], dom=2)\")\n        self.assertEqual(sstr(dmp), \"DMP(ring=ZZ, rep=[1, 2, 3], dom=2)\")\n        dmp = DMP(ring=ZZ, rep=[1, 2], dom=Symbol(r'x'))\n        self.assertEqual(sstrrepr(dmp), \"DMP(ring=ZZ, rep=[1, 2], dom=x)\")\n        self.assertEqual(sstr(dmp), \"DMP(ring=ZZ, rep=[1, 2], dom=x)\")\n\n"], "sample_1086": ["    def test_complex_root_of(self):\n        self.assertEqual(sstr(ComplexRootOf(x**3 + x + 1, 1)), 'CRootOf(x**3 + x + 1, 1)')\n\n", "    def test_sstr_repr_printer(self):\n        p = StrReprPrinter(settings={\"abbrev\": True})\n        self.assertEqual(p.doprint(pow(2, 3)), \"2**3\")\n        self.assertEqual(p.doprint(2**3), \"2**3\")\n        self.assertEqual(p.doprint(sqrt(2)), \"sqrt(2)\")\n        self.assertEqual(p.doprint(1/sqrt(2)), \"1/sqrt(2)\") \n        self.assertEqual(p.doprint(x + 1), \"x + 1\")\n        self.assertEqual(p.doprint('abc'), \"'abc'\")\n\n\n\n", "    def test_printing_complex_root_of(self):\n        expr = RootOf(x**3 - 2, index=1)\n        self.assertEqual(sstr(expr), 'CRootOf(x**3 - 2, 1)')\n", "    def test_sstr_repr(self):\n        expr = Eq(a + b, 0)\n        self.assertEqual(sstr(expr), \"'Eq(a + b, 0)'\")\n        self.assertEqual(sstrrepr(expr), \"Eq(a + b, 0)\")\n\n", "    def test_str_printing_of_special_functions(self):\n        self.assertEqual(sstr(Catalan), 'Catalan')\n        self.assertEqual(sstr(GoldenRatio), 'GoldenRatio')\n        self.assertEqual(sstr(TribonacciConstant), 'TribonacciConstant')\n\n        self.assertEqual(sstr(E), 'E')\n        self.assertEqual(sstr(pi), 'pi')\n        self.assertEqual(sstr(I), 'I')\n        self.assertEqual(sstr(oo), 'oo')\n        self.assertEqual(sstr(nan), 'nan')\n        self.assertEqual(sstr(EulerGamma), 'EulerGamma')\n        self.assertEqual(sstr(zoo), 'zoo')\n        self.assertEqual(sstr(zeta(3)), 'zeta(3)')\n        self.assertEqual(sstr(sqrt(2)), 'sqrt(2)')\n        self.assertEqual(sstr(summation(i, (i, 1, 10))), 'summation(i, (i, 1, 10))')\n\n\n"], "sample_939": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_833": ["    def test_elasticnet_path(X, y):\n        classes = np.unique(y)\n        n_classes = classes.shape[0]\n\n        n_Cs = 10\n        l1_ratios = [0.1, 0.5, 0.9]\n        Cs = np.logspace(0, 4, n_Cs)\n        l1_ratios_ = np.array(l1_ratios)\n        cv = StratifiedKFold(n_splits=5)\n        clf = LogisticRegressionCV(Cs=Cs, l1_ratios=l1_ratios,\n                                    cv=cv, solver='liblinear',\n                                    penalty='elasticnet',\n                                    multi_class='ovr',\n                                    random_state=0)\n        clf.fit(X, y)\n\n        # Extract coefficients and scores for each combination of Cs and l1_ratio\n        coefs_paths = clf.coefs_paths_\n        scores = clf.scores_\n        \n        # Check that the number of coefficients and scores matches\n        assert_equal(coefs_paths.shape, (n_classes, cv.n_splits,\n                                         n_Cs * len(l1_ratios), X.shape[1]))\n        assert_equal(scores.shape, (n_classes, cv.n_splits, n_Cs *\n                                   len(l1_ratios)))\n\n        # Check that the shape of the coefficients and scores is correct\n        assert_equal(coefs_paths[0].shape, (cv.n_splits, n_Cs *\n                                           len(l1_ratios), X.shape[1]))\n        assert_equal(scores[0].shape, (cv.n_splits, n_Cs *\n                                       len(l1_ratios)))\n        \n        # Check that the scores make sense\n        scores_for_each_l1_ratio = [scores_for_l1_ratio[0]\n                                   for _, scores_for_l1_ratio in\n                                   enumerate(scores)]\n        for scores_for_l1_ratio in scores_for_each_l1_ratio:\n            assert_greater(scores_for_l1_ratio.max(), 0) \n\n\n\n", "    def test_logistic_regression_path_l1_ratio_elasticnet(self):\n        X, y, sample_weight = make_classification(n_samples=100, n_features=10,\n                                                  n_informative=5,\n                                                  random_state=42)\n        n_classes = 2\n        penalty = 'elasticnet'\n        l1_ratios = np.array([0.1, 0.5, 0.9])\n        solver = 'liblinear'\n\n        best_coefs = dict()\n        for l1_ratio in l1_ratios:\n            w, _, _ = _logistic_regression_path(\n                X, y, l1_ratio=l1_ratio, Cs=[1], solver=solver,\n                max_iter=1000, tol=1e-4, penalty=penalty,\n                multi_class='ovr', fit_intercept=True)\n            best_coefs[l1_ratio] = w\n        \n        for l1_ratio in l1_ratios:\n            coef1 = best_coefs[l1_ratio]\n            coef2 = _logistic_regression_path(\n                X, y, l1_ratio=l1_ratio, Cs=[1], solver=solver,\n                max_iter=1000, tol=1e-4, penalty=penalty,\n                multi_class='ovr', fit_intercept=True,\n                check_input=False)[0]\n            assert_allclose(coef1, coef2)\n\n\n\n", "def test_logistic_regression_cv_penalty_elasticnet_l1_ratio():\n    X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n    clf = LogisticRegressionCV(Cs=10, fit_intercept=True, cv=5, dual=False,\n                              penalty='elasticnet', solver='liblinear',\n                              tol=1e-4, max_iter=100, class_weight=None,\n                              n_jobs=None, verbose=0, refit=True,\n                              intercept_scaling=1., multi_class='auto',\n                              random_state=None, l1_ratios=[0.1, 0.5, 0.9])\n    clf.fit(X, y)\n    assert_equal(clf.penalty, 'elasticnet')\n    assert_allclose(clf.l1_ratio_, np.array([0.1, 0.5, 0.9]))\n    assert_equal(clf.coef_.shape[0], 1)\n\n\n\n    \n\n", "compilation error", "compilation error"], "sample_95": ["    def test_wrapped_functions_attributes(self):\n        self.assertEqual(fully_decorated.__doc__, fully_decorated.anything)\n        self.assertEqual(fully_decorated.__dict__, {'anything': 'Expected __dict__'})\n\n\n", "    def test_full_decorator(self):\n        with self.assertNumQueries(0):  # We only want to hit the database once, for user lookup\n            response = fully_decorated(HttpRequest())\n        self.assertEqual(response.content, b'<html><body>dummy</body></html>')\n        self.assertEqual(response.status_code, 200)\n\n\n", "    def test_full_decorator(self):\n        @full_decorator\n            return HttpResponse('<html><body>decorated</body></html>')\n\n        request = HttpRequest()\n        response = view(request)\n        self.assertEqual(response.content, b'<html><body>decorated</body></html>')\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response['Content-Type'], 'text/html; charset=utf-8')\n        self.assertEqual(response['Cache-Control'], 'private, max-age=900')\n        self.assertEqual(response['Vary'], 'Accept-language, Cookie')\n        self.assertEqual(response['X-Frame-Options'], 'DENY')\n\n", "    def test_vary_header(self):\n        response = fully_decorated(HttpRequest())\n        self.assertEqual(response['Vary'], 'Accept-language,Cookie')\n", "    def test_full_decorator(self):\n        response = fully_decorated(HttpRequest())\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.content, b'<html><body>dummy</body></html>')\n"], "sample_159": ["    def test_check_user_model_no_required_fields(self):\n        with self.subTest(\n            'User model has no REQUIRED_FIELDS'\n        ):\n            with override_settings(AUTH_USER_MODEL='auth_tests.CustomUserNoRequiredFields'):\n                errors = check_user_model(app_configs=self.apps.get_app_configs('auth_tests'))\n                self.assertEqual(errors, [])\n\n", "    def test_check_user_model_non_anonymous(self):\n        errors = check_user_model(app_configs=self.apps.get_app_configs('auth_tests'))\n        self.assertEqual(len(errors), 0)\n", "    def test_check_user_model_no_required_fields(self):\n        with self.settings(AUTH_USER_MODEL='auth_tests.CustomUserNoRequiredFields'):\n            errors = check_user_model(self.apps.get_app_configs())\n            self.assertEqual(len(errors), 0)\n", "    def test_check_user_model_non_unique_username(self):\n        errors = check_user_model(app_configs=[self.apps.get_app_config('auth_tests')])\n        self.assertIn(\n            checks.Warning(\n                \"'auth_tests.CustomUserNonUniqueUsername.username' is named as the 'USERNAME_FIELD', but it is not unique.\",\n                hint='Ensure that your authentication backend(s) can handle non-unique usernames.',\n                obj=CustomUserNonUniqueUsername,\n                id='auth.W004',\n            ),\n            errors,\n        )\n", "    def test_user_model_is_anonymous_method(self):\n        # Test for user_model_is_anonymous being a method\n        class UserWithMethodIsAnonymous(AbstractBaseUser):\n            is_anonymous = MethodType(lambda self: False, self)\n\n        with self.assertLogs() as cm:\n            check_user_model(self.apps, User=UserWithMethodIsAnonymous)\n        self.assertEqual(len(cm.records), 1)\n        self.assertEqual(cm.records[0].level, checks.CRITICAL)\n"], "sample_560": ["    def test_legend_location(self, loc):\n        fig, ax = plt.subplots()\n        line, = ax.plot([1, 2, 3], [4, 5, 6])\n        ax.legend([line], [ 'test'], loc=loc)\n        plt.close(fig)\n\n", "    def test_loc_position(self, loc):\n        fig, ax = plt.subplots()\n        line1 = ax.plot([1, 2], [3, 4])[0]\n        line2 = ax.plot([5, 6], [7, 8])[0]\n        legend = ax.legend([line1, line2], ['Line 1', 'Line 2'], loc=loc)\n        bbox = legend.get_window_extent()\n        # Ensure the legend is visible and positioned as expected.\n        assert bbox.width > 0, \"Legend has zero width\"\n        assert bbox.height > 0, \"Legend has zero height\"\n        with rc_context({'axes.linewidth': 1}):\n            if self.use_tex:\n                # Tex fonts may differ in size, so we use a looser comparison\n                with image_comparison():\n                    fig.canvas.draw()\n                    fig.savefig(\"test_loc_position.png\")\n            else:\n                check_figures_equal(\"test_loc_position.png\", ax.figure)\n\n\n\n", "    def test_draggable_legend(self, fig_manager, plt_testdir):\n        with plt.rc_context({\"legend.draggable\": True}):\n            fig, ax = plt.subplots()\n            line1, = ax.plot([1, 2, 3])\n            line2, = ax.plot([4, 5, 6])\n            legend = fig.legend([line1, line2], [\"Line 1\", \"Line 2\"])\n            legend.set_draggable(True)\n            fig.canvas.draw()\n\n        # Create a dummy canvas for figure.savefig since this test runs on\n        # a test environment that doesn't have a real display.\n        fig.canvas = Mock(draw=Mock(), blit=Mock())\n\n        with rc_context({\"legend.draggable\": False}):\n            fig_manager.show()\n\n        for i in range(5):\n            # Dragging the legend in the test environment doesn't actually\n            # move the legend. So we'll simulate it by directly setting the\n            # bbox_to_anchor and check the bbox.\n            legend.set_bbox_to_anchor((0.5, 0.5))  # Simulate drag\n            assert_allclose(legend.get_bbox_to_anchor().bounds[0], 0.5)\n            fig.canvas.draw()\n            fig_manager.show()\n\n            # Since we're using a mock canvas, we need to flush the events\n            # to allow the drag event to be processed.\n            fig_manager.flush_events()\n\n\n        \n", "    def test_bbox_to_anchor(self):\n        fig, ax = plt.subplots()\n        line, = ax.plot([1, 2, 3, 4])\n        line.set_label('test')\n        legend = ax.legend()\n        legend.set_bbox_to_anchor((0.5, 0.5))\n        plt.draw()\n        plt.close()\n\n\n\n", "    def test_legend_default_handler_map(self):\n        fig, ax = plt.subplots()\n        line, = ax.plot([1, 2, 3], [4, 5, 6])\n        circle = plt.Circle((0, 0), 1, color='red')\n        ax.add_artist(circle)\n\n        with rc_context({'legend.handler_map': {'Circle': lambda artist, legend: CircleHandle(artist, legend)}}):\n            legend = ax.legend([line, circle], ['line', 'circle'])\n\n        plt.close(fig)\n\n\n"], "sample_481": ["    def test_join_with_null(self):\n        self.assertEqual(self.engine.render(self.join01, {\"a\": [1, 2, None, 3]}), \"1, 2, , 3\")\n", "    def test_join01(self):\n        self.assertEqual(join01.render({\"a\": [\"b\", \"c\", \"d\"]}), \"b, c, d\")\n", "    def test_join_for_unhashable_types(self):\n        context = {\"data\": [1, 2, 3, set([4, 5])]}\n        self.assertEqual(self.engine.render(self.join01, context).strip(), \"1, 2, 3, set([4, 5])\")\n", "    def test_join_with_empty_string(self):\n        self.assertEqual(self.render(\"join01\", {\"a\": [1, 2, 3]}), \"1, 2, 3\")\n", "    def test_join_with_unicode_sequence(self):\n        self.assertEqual(self.render_template(\"join01\", {\"a\": [u\"\u00e9\", u\"\u00e0\", u\"\u00e7\"]}), \"\u00e9, \u00e0, \u00e7\")\n"], "sample_949": ["    def test_default_man_pages_with_custom_project_and_release(\n        sphinx_app: Sphinx,", "    def test_default_man_pages_with_custom_values(self, app: Sphinx, tmpdir) -> None:\n        config = Config(app)\n        config.project = 'My Project'\n        config.release = '0.1'\n        config.author = 'John Doe'\n        config.root_doc = 'index'\n\n        expected_man_pages = [\n            ('index', 'MyProject', 'My Project 0.1', ['John Doe'], 1)\n        ]\n        assert default_man_pages(config) == expected_man_pages\n", "    def test_default_man_pages(self, app: Sphinx, tmpdir) -> None:\n        config = Config(app)\n        config.project = 'testproject'\n        config.release = '0.1'\n        config.author = 'John Doe'\n        config.root_doc = 'index'\n\n        assert default_man_pages(config) == [\n            ('index', 'testproject', 'testproject 0.1', ['John Doe'], 1)\n        ]\n\n\n", "    def test_default_man_pages(self, test_app: Sphinx, tmpdir: str) -> None:\n        config = Config(test_app)\n        config.project = 'myproject'\n        config.release = '0.1'\n        config.author = 'My Name'\n        config.root_doc = 'index'\n\n        expected_man_pages = default_man_pages(config)\n        assert expected_man_pages == [\n            (\n                'index',\n                'myproject',\n                'myproject 0.1',\n                ['My Name'],\n                1\n            )\n        ]\n", "    def test_default_man_pages_with_custom_project_settings(\n        self, tmpdir, sphinx_app: Sphinx"], "sample_982": ["    def test_udivisor_sigma():\n        assert udivisor_sigma(12, 0) == 6\n        assert udivisor_sigma(12, 1) == 28\n        assert udivisor_sigma(12, 2) == 601\n        assert udivisor_sigma(74, 0) == 3\n        assert udivisor_sigma(74, 1) == 114\n        assert udivisor_sigma(74, 2) == 7154\n        assert udivisor_sigma(1, 1) == 1 ", "    def test_core_t():\n        assert core(24, 2) == 6\n        assert core(9424, 3) == 1178\n        assert core(379238) == 379238\n        assert core(15**11, 10) == 15\n        assert core(12, 3) == 12\n", "    def test_primeomega_special_cases(self):\n        self.assertEqual(primeomega(1), 0)\n        self.assertEqual(primeomega(2), 1)\n        self.assertEqual(primeomega(12), 3)\n        self.assertEqual(primeomega(1234567), \n                         sum(factorint(1234567).values()))\n", "def test_udivisor_sigma():\n    for n in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 18, 24, 36, 72, 120]:\n        for k in range(3):\n            expect = sum([x**k for x in udivisors(n)])\n            actual = udivisor_sigma(n, k)\n            assert actual == expect\n\n\n", "    def test_udivisor_count_values(self):\n        self.assertEqual(udivisor_count(1), 1)\n        self.assertEqual(udivisor_count(2), 1)\n        self.assertEqual(udivisor_count(3), 2)\n        self.assertEqual(udivisor_count(6), 4)\n        self.assertEqual(udivisor_count(12), 8)\n        self.assertEqual(udivisor_count(30), 12)\n        self.assertEqual(udivisor_count(100), 40)\n\n\n\n"], "sample_1035": ["def test_measure_partial_oneshot_multiple_qubits():\n    q = qapply(H(0)*H(1)*Qubit('00'))\n    result = measure_partial_oneshot(q, (0, 1))\n    assert isinstance(result, IntQubit)\n\n\n", "def test_measure_partial_oneshot_multiple_qubits():\n    from sympy.physics.quantum.qubit import Qubit, measure_partial_oneshot\n    c = Qubit('011')\n    result = measure_partial_oneshot(c, bits=(0,2))\n    assert isinstance(result, Qubit)\n\n\n\n\n", "    def test_measure_partial_oneshot_two_qubits():\n        c = H(0)*H(1)*IntQubit(2)\n        q = qapply(c)\n        for i in range(1000):\n            result = measure_partial_oneshot(q, (0,1))\n            assert result.as_int() in [0, 3]\n", "def test_measure_partial_oneshot_two_qubits():\n    c = Qubit('01')\n    for i in range(100):\n        result = measure_partial_oneshot(c, (0,))\n        if isinstance(result, IntQubit):\n            assert result.as_int() in [0, 1]\n            assert result.nqubits == 2    \n        else:\n            assert isinstance(result, Qubit)\n            assert result.as_int() in [0, 1]\n            assert result.nqubits == 2 \n", "def test_measure_partial_oneshot():\n    q = IntQubit(3)\n    result = measure_partial_oneshot(q, (0,))\n    assert result.as_int() in [0,1,2]\n    assert result.nqubits == 1\n    assert isinstance(result, IntQubit)\n"], "sample_389": ["    def test_allowed_hosts(self):\n        with override_settings(ALLOWED_HOSTS=['*']):\n            request = HttpRequest()\n            request.get_host = lambda: 'example.com'\n            self.assertTrue(request.get_host())\n        with override_settings(ALLOWED_HOSTS=['example.com']):\n            request = HttpRequest()\n            request.get_host = lambda: 'example.com'\n            self.assertTrue(request.get_host())\n        with override_settings(ALLOWED_HOSTS=['example.com']):\n            request = HttpRequest()\n            request.get_host = lambda: 'example.org'\n            with self.assertRaises(DisallowedHost):\n                request.get_host()\n", "    def test_allowed_hosts(self):\n        with self.settings(ALLOWED_HOSTS=[\"example.com\"]):\n            request = HttpRequest()\n            request.META = {\"HTTP_HOST\": \"example.com\"}\n            self.assertEqual(request.get_host(), \"example.com\")\n\n        with self.settings(ALLOWED_HOSTS=[\"example.com\"]):\n            request = HttpRequest()\n            request.META = {\"HTTP_HOST\": \"www.example.com\"}\n            self.assertEqual(request.get_host(), \"www.example.com\")\n\n        with self.settings(ALLOWED_HOSTS=[\"*\"]):\n            request = HttpRequest()\n            request.META = {\"HTTP_HOST\": \"subdomain.example.com\"}\n            self.assertEqual(request.get_host(), \"subdomain.example.com\")\n\n        with self.settings(ALLOWED_HOSTS=[\"*\"]):\n            request = HttpRequest()\n            request.META = {\"HTTP_HOST\": \"example.com\"}\n            self.assertEqual(request.get_host(), \"example.com\")\n\n        with self.settings(ALLOWED_HOSTS=[\"*\"]):\n            request = HttpRequest()\n            request.META = {\"HTTP_HOST\": \"192.168.1.1\"}\n            self.assertEqual(request.get_host(), \"192.168.1.1\")\n\n        with self.settings(ALLOWED_HOSTS=[\"example.com\"]):\n            request = HttpRequest()\n            request.META = {\"HTTP_HOST\": \"invalid.host\"}\n            with self.assertRaises(DisallowedHost):\n                request.get_host()\n", "    def test_get_full_path_info(self):\n        request = HttpRequest()\n        request.path = \"/path/to/resource\"\n        request.path_info = \"/path/to/resource/details\"\n        self.assertEqual(request._get_full_path(force_append_slash=True), \"/path/to/resource/details/\")\n        self.assertEqual(request.get_full_path_info(force_append_slash=False), \"/path/to/resource/details\")\n        request.path = \"/path/to/resource\"\n        request.path_info = \"\"\n        self.assertEqual(request._get_full_path(force_append_slash=True), \"/path/to/resource/\")\n        self.assertEqual(request.get_full_path_info(force_append_slash=False), \"/path/to/resource\")\n", "    def test_allowed_host(self):\n        with self.subTest(host=\"localhost\"):\n            with override_settings(ALLOWED_HOSTS=[\"localhost\"]):\n                request = HttpRequest()\n                request.META[\"HTTP_HOST\"] = \"localhost\"\n                self.assertIsNone(request.get_host())\n\n        with self.subTest(host=\"example.com\"):\n            with override_settings(ALLOWED_HOSTS=[\"example.com\"]):\n                request = HttpRequest()\n                request.META[\"HTTP_HOST\"] = \"example.com\"\n                self.assertEqual(request.get_host(), \"example.com\")\n\n        with self.subTest(host=\"invalid.com\"):\n            with override_settings(ALLOWED_HOSTS=[\"example.com\"]):\n                request = HttpRequest()\n                request.META[\"HTTP_HOST\"] = \"invalid.com\"\n                with self.assertRaises(DisallowedHost):\n                    request.get_host()\n\n        with self.subTest(host_with_port=\"example.com:8080\"):\n            with override_settings(ALLOWED_HOSTS=[\"example.com\"]):\n                request = HttpRequest()\n                request.META[\"HTTP_HOST\"] = \"example.com:8080\"\n                self.assertEqual(request.get_host(), \"example.com:8080\")\n\n\n\n", "    def test_allowed_hosts(self):\n        self.assertEqual(settings.ALLOWED_HOSTS, [\"b\", \"d\"])\n        self.assertEqual(settings.TEST, \"override\")\n        self.assertEqual(settings.TEST_OUTER, \"outer\")\n\n\n\n"], "sample_440": ["    def test_bulk_create_with_related_objects(self):\n        Country.objects.all().delete()\n        Pizzeria.objects.all().delete()\n        with connection.atomic():\n            Country.objects.bulk_create(self.data)\n            Pizzeria.objects.bulk_create(self.data2)\n        qs = Country.objects.all()\n        self.assertEqual(len(qs), 4)\n        self.assertEqual(qs.get(name=\"United States of America\").pizzerias.count(), 1)\n\n\n", "    def test_bulk_create_with_fields_defined_in_order(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\n                \"CREATE TABLE django_test_two_fields (id INTEGER PRIMARY KEY, field1 TEXT, field2 INTEGER)\"\n            )\n        ManyFieldsModel = type(\n            \"ManyFieldsModel\",\n            (models.Model,),\n            {\"field1\": models.CharField(max_length=100), \"field2\": models.IntegerField()},\n        )\n        ManyFieldsModel._meta.db_table = \"django_test_two_fields\"\n        ManyFieldsModel.objects.bulk_create(\n            [\n                ManyFieldsModel(field1=\"one\", field2=1),\n                ManyFieldsModel(field1=\"two\", field2=2),\n                ManyFieldsModel(field1=\"three\", field2=3),\n            ]\n        )\n        from django.db import connection\n        with connection.cursor() as cursor:\n            cursor.execute(\n                \"SELECT field1, field2 FROM django_test_two_fields\"\n            )\n            results = cursor.fetchall()\n        self.assertQueryCount(\n            self.queries, 1\n        )  # Only one query should be executed: SELECT\n        self.assertEqual(\n            [\n                (\"one\", 1),\n                (\"two\", 2),\n                (\"three\", 3),\n            ],\n            results,\n        )\n\n\n\n", "    def test_bulk_create_with_related_objects(self):\n        with connection.managed_start():\n            Country.objects.all().delete()\n            Pizzeria.objects.all().delete()\n            Country.objects.bulk_create(self.data)\n            Pizzeria.objects.bulk_create(self.data2)\n        countries = Country.objects.all()\n        pizzerias = Pizzeria.objects.all()\n        self.assertEqual(len(countries), 4)\n        self.assertEqual(len(pizzerias), 3)\n        for pizzeria in pizzerias:\n            self.assertIn(pizzeria.country, countries)\n\n\n\n", "    def test_bulk_create_with_fk(self):\n        with self.assertNumQueries(3):\n            Country.objects.bulk_create(self.data)\n        with self.assertNumQueries(3):\n            Pizzeria.objects.bulk_create(self.pizzerias)\n\n\n\n", "    def test_bulk_create_with_related_objects(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\"SET foreign_key_checks = 0\")\n\n        Country.objects.bulk_create(self.data)\n        Pizzeria.objects.bulk_create(self.data2)\n        Restaurant.objects.bulk_create(self.data3)\n        cursor.execute(\"SET foreign_key_checks = 1\")\n\n        self.assertEqual(Pizzeria.objects.count(), 3)\n        self.assertEqual(Restaurant.objects.count(), 2)\n        self.assertEqual(Country.objects.count(), 4)\n\n        self.assertEqual(Pizzeria.objects.first().country.name, \"United States of America\")\n        self.assertEqual(Pizzeria.objects.last().country.name, \"Germany\")\n\n        self.assertEqual(Restaurant.objects.get(name=\"Burger King\").country.name, \"United States of America\")\n        self.assertEqual(Restaurant.objects.get(name=\"McDonald's\").country.name, \"The Netherlands\")\n\n\n"], "sample_527": ["    def test_constrained_layout_warning(self, monkeypatch):\n        from matplotlib.figure import Figure\n\n            raise RuntimeError('This engine should not be used')\n\n        monkeypatch.setattr(Figure, 'get_layout_engine', mock_layout_engine)\n\n        fig = Figure()\n        with pytest.warns(RuntimeWarning,\n                         match=re.escape(_EXPECTED_WARNING_TOOLMANAGER)):\n            fig.tight_layout()\n\n\n", "    def test_tight_layout_with_rect(self, fig_manager):\n        fig, ax = plt.subplots()\n\n        # Set a rectangle in normalized figure coordinates that's too small\n        fig.tight_layout(rect=(0, 0, 0.8, 0.8))\n        ax.set_xlim(0, 1)\n        ax.set_ylim(0, 1)\n        fig.canvas.draw()\n        # Check that the tight layout respects the specified rectangle\n\n        rect = fig.bbox.get_bounds()\n        assert (rect[0] >= 0) and (rect[1] >= 0) and (rect[2] <= 1) and (\n            rect[3] <= 1)\n\n        # cleanup\n        plt.close(fig)\n", "    def test_savefig_metadata(self, renderer, tmp_path):\n        from matplotlib.backends.backend_pdf import PdfPages\n        from matplotlib.backends.backend_ps import FigureCanvasPS\n\n        fig, ax = plt.subplots()\n        fig.canvas.manager.toolbar.mode = 'navigate'\n        if renderer == 'pdf':\n            with PdfPages(str(tmp_path / \"test.pdf\")) as pdf:\n                fig.savefig(pdf, metadata={'pdf_metadata': 'test'})\n        elif renderer == 'ps':\n            canvas = FigureCanvasPS(fig, None)\n            canvas.print_figure(str(tmp_path / \"test.ps\"), metadata={'ps_metadata': 'test'})\n        else:\n            fig.savefig(str(tmp_path / \"test.\" + renderer), metadata={'metadata': 'test'})\n\n        if renderer == 'pdf':\n            with open(str(tmp_path / \"test.pdf\"), 'rb') as f:\n                pdf_content = f.read()\n            assert b'pdf_metadata' in pdf_content\n        elif renderer == 'ps':\n            with open(str(tmp_path / \"test.ps\"), 'rb') as f:\n                ps_content = f.read()\n            assert b'ps_metadata' in ps_content\n        else:\n            with open(str(tmp_path / \"test.\" + renderer), 'rb') as f:\n                content = f.read()\n            assert b'metadata' in content\n\n\n", "    def test_figaspect_array(self):\n        A = np.random.rand(3, 4)\n        w, h = figaspect(A)\n        assert w * h == 3.0 / 4.0\n        fig = plt.figure(figsize=(w, h))\n        ax = fig.add_subplot(111)\n        ax.imshow(A)\n        fig.canvas.draw()\n        # This test does not work as intended and should be rewritten. \n        # The aspect ratio check is not accurate and needs to be revised\n        # to reflect the actual aspect ratio being used.\n\n\n\n", "    def test_tight_layout_no_interactive(self):\n        # \n        fig = Figure(figsize=(10, 5))\n        ax = fig.add_subplot(111)\n        ax.set_title(\"First\")\n        ax2 = fig.add_axes([0.1, 0.2, 0.8, 0.6])\n        ax2.set_title(\"Second\")\n        fig.tight_layout()\n        #\n        self.assert_no_warnings(fig.canvas.print_figure)\n        "], "sample_228": ["    def test_formset_factory_with_max_num(self):\n        with self.assertRaisesMessage(ValidationError, \"Please submit at most 3 forms.\"):\n            formset = FavoriteDrinksFormSet(data={'name': ['a', 'b', 'c', 'd', 'e']})\n            formset.is_valid()\n", "    def test_formset_factory_max_num_validation(self):\n        with self.assertRaises(ValueError):\n            formset_factory(CustomKwargForm, absolute_max=1, max_num=2)\n\n\n\n", "    def test_formset_max_num(self):\n        with self.assertRaises(ValidationError) as context:\n            formset_data = [\n                {'choice': 'Milk', 'votes': 1},\n                {'choice': 'Water', 'votes': 2},\n                {'choice': 'Juice', 'votes': 3},\n                {'choice': 'Soda', 'votes': 4},\n                {'choice': 'Tea', 'votes': 5},\n            ]\n            formset = ChoiceFormSet(formset_data, max_num=2)\n            formset.is_valid()\n        self.assertEqual(\n            context.exception.error_list[0].__str__(),\n            'Please submit at most 2 form.'\n        )\n\n", "    def test_ordering_widget(self):\n        self.assertEqual(ChoiceFormSet.ordering_widget, IntegerInput)\n", "    def test_absolute_max_validation(self):\n        with self.assertRaises(ValueError):\n            formset_factory(Choice, absolute_max=10, max_num=15)\n"], "sample_487": ["    def test_check_raw_id_fields(self):\n        class RawIdModelAdmin(ModelAdmin):\n            raw_id_fields = ['id']\n\n        self.assertIsInvalid(\n            RawIdModelAdmin,\n            ValidationTestModel,\n            'The value of \"raw_id_fields\" must be either an empty list or a list of '\n            'fields that are not primary keys.',\n            id=\"admin.E207\",\n        )\n\n", "    def test_save_as_boolean(self):\n        class TestModelAdmin(ModelAdmin):\n            save_as = True\n\n        self.assertIsValid(TestModelAdmin(models.Model(), AdminSite()), models.Model())\n", "    def test_raw_id_fields(self):\n        class RawIdModelAdmin(ModelAdmin):\n            raw_id_fields = ['id']\n\n        self.assertIsValid(RawIdModelAdmin, ValidationTestModel)\n", "    def test_model_admin_checks_inline(self):\n        class MyInline(ValidationTestInlineModel):\n            formset = BaseModelFormSet\n\n        class MyAdmin(ModelAdmin):\n            inlines = [MyInline]\n\n        self.assertIsInvalid(\n            MyAdmin,\n            ValidationTestModel,\n            \"The value of 'formset' refers to '%s', which must inherit from \"\n            \"'BaseModelFormSet'.\" % \"BaseModelFormSet\",\n            id=\"admin.E206\",\n            obj=MyInline,\n        )\n\n        class MyInline2(ValidationTestInlineModel):\n            formset = \"MyCustomFormSet\"\n\n        class MyAdmin2(ModelAdmin):\n            inlines = [MyInline2]\n\n        self.assertIsInvalid(\n            MyAdmin2,\n            ValidationTestModel,\n            \"The value of 'formset' refers to '%s', which must be a \"\n            \"subclass of 'BaseModelFormSet'.\" % \"MyCustomFormSet\",\n            id=\"admin.E206\",\n            obj=MyInline2,\n        )\n\n\n\n", "    def test_invalid_raw_id_fields(self):\n        class InvalidRawIdAdmin(ModelAdmin):\n            raw_id_fields = [\"song\"]\n\n        self.assertIsInvalid(\n            InvalidRawIdAdmin(Song, AdminSite()),\n            Song,\n            \"The value of 'raw_id_fields' refers to 'song', which is not a \"\n            \"field of 'Song'.\",\n            id=\"admin.E009\",\n        )\n\n"], "sample_834": ["    def test_validation_input_parameters(self):\n        nca = NeighborhoodComponentsAnalysis()\n\n        # Test for incompatible init values\n        with pytest.raises(ValueError):\n            nca.fit(iris_data, iris_target, init='wrong_init_value')\n\n        # Test for None in n_components\n        with pytest.raises(ValueError):\n            nca.fit(iris_data, iris_target, n_components=None)\n\n        # Test for invalid warm_start\n        with pytest.raises(ValueError):\n            nca.fit(iris_data, iris_target, warm_start='string')\n\n        # Test for invalid max_iter\n        with pytest.raises(ValueError):\n            nca.fit(iris_data, iris_target, max_iter=-1)\n\n        # Test for invalid tol\n        with pytest.raises(ValueError):\n            nca.fit(iris_data, iris_target, tol=1.0)\n\n        # Test for invalid verbose\n        with pytest.raises(ValueError):\n            nca.fit(iris_data, iris_target, verbose=-1)\n\n        # Test for invalid callback type\n        with pytest.raises(ValueError):\n            nca.fit(iris_data, iris_target, callback=42)\n", "    def test_warm_start(self):\n        nca1 = NeighborhoodComponentsAnalysis(n_components=2, random_state=0)\n        nca1.fit(iris_data, iris_target)\n        nca2 = clone(nca1)\n\n        # Check if the components learned by nca1 are used as initial\n        # values for nca2, even with no other fit call\n        nca2.warm_start = True\n        nca2.fit(iris_data, iris_target)\n        assert_array_almost_equal(nca1.components_, nca2.components_)\n", "    def test_nca_with_different_loss_sign(self):\n        nca = NeighborhoodComponentsAnalysis(n_components=2, random_state=0)\n        X_train, y_train = iris_data[:150], iris_target[:150]\n        nca.fit(X_train, y_train)\n        loss, grad = nca._loss_grad_lbfgs(nca.components_.flatten(),\n                                          X_train,\n                                          np.ones((X_train.shape[0],\n                                                  X_train.shape[0])))\n        # Ensure that the gradient doesn't change sign when multiplying\n        # the loss by -1\n        loss_neg = -loss\n        grad_neg = -grad\n        nca.fit(X_train, y_train)\n        loss_neg_check, grad_neg_check = nca._loss_grad_lbfgs(\n            nca.components_.flatten(), X_train, np.ones((X_train.shape[0],\n                                                       X_train.shape[0])))\n        assert_array_almost_equal(loss_neg, loss_neg_check)\n        assert_array_almost_equal(grad, grad_neg_check)\n\n\n\n", "    def test_warm_start(self):\n        nca = NeighborhoodComponentsAnalysis(n_components=2, warm_start=True)\n\n        # Fit the model on the first part of the data\n        nca.fit(iris_data[:100], iris_target[:100])\n\n        # Check that the components are non-trivial\n        assert_array_equal(nca.components_.shape, (2, 4))\n\n        # Now fit again on the rest of the data, using the previously learned\n        # components\n        nca.fit(iris_data[100:], iris_target[100:])\n        # Check that the components have changed\n\n        assert_array_almost_equal(nca.components_.mean(),\n                                  nca.components_[:100].mean(),\n                                  atol=EPS)\n\n\n        nca_clone = clone(nca)\n        nca_clone.fit(iris_data[:100], iris_target[:100])\n        assert_array_almost_equal(nca.components_, nca_clone.components_,\n                                  atol=EPS)\n\n\n\n", "    def test_warm_start(self):\n        n_components = 2\n        n_samples = 100\n        X = rng.randn(n_samples, 10)\n        y = rng.randint(0, 2, n_samples)\n\n        nca1 = NeighborhoodComponentsAnalysis(n_components=n_components,\n                                               warm_start=False)\n        nca1.fit(X, y)\n\n        nca2 = clone(nca1)\n        nca2.warm_start = True\n        nca2.fit(X, y)  \n\n        assert_array_almost_equal(nca1.components_, nca2.components_)\n"], "sample_287": ["    def test_system_checks_model_admin(self):\n        admin.site = AdminSite()\n        admin.site.register(Album, MyAdmin)\n        site = checks.run_checks(admin.site)\n        self.assertEqual(len(site), 1)\n        self.assertEqual(site[0].id, 'admin.E1')\n\n\n", "    def test_admin_checks_for_custom_models_and_admins(self):\n        from .models import Author, Book, MyAdmin\n\n        admin.site._registry = {}  # Reset the admin site registry\n        admin.site.register(Author, MyAdmin)\n        admin.site.register(Book)\n        errors = checks.run_checks(\n            [\n                admin.site,\n                (\n                    AuthenticationMiddlewareSubclass(),\n                    'django.contrib.auth.middleware.AuthenticationMiddleware',\n                ),\n                (\n                    MessageMiddlewareSubclass(),\n                    'django.contrib.messages.middleware.MessageMiddleware',\n                ),\n                (\n                    ModelBackendSubclass(),\n                    'django.contrib.auth.backends.ModelBackend',\n                ),\n                (\n                    SessionMiddlewareSubclass(),\n                    'django.contrib.sessions.middleware.SessionMiddleware',\n                ),\n            ],\n            'myapp',\n        )\n\n        self.assertEmpty(\n            errors,\n            f\"Expected no errors, but got:\\n{errors}\",\n        )\n\n\n", "    def test_admin_checks_on_model_admin(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            list_display = ('id', 'title')\n            list_filter = ('title', 'nonexistent_field')\n            search_fields = ('title',)\n            date_hierarchy = 'nonexistent_field'\n            ordering = ('-nonexistent_field', 'id')\n            ordering = ('id',)\n\n            # Invalid 'save_as'\n            save_as = 'not_a_bool'\n\n            # Invalid 'save_on_top'\n            save_on_top = 'not_a_bool'\n\n            # Invalid 'ordering'\n            ordering = '?id'\n            # invalid 'relation'\n            list_display_links = ('id', 'nonexistent_field')\n            # Invalid 'list_editable'\n            list_editable = ('title', 'nonexistent_field')\n            # Invalid 'extra'\n            extra = 'not_an_int'\n            # Invalid 'max_num'\n            max_num = 'not_an_int'\n            # Invalid 'min_num'\n            min_num = 'not_an_int'\n            # Invalid 'formset'\n            formset = object\n\n        with self.assertRaises(checks.Error) as error:\n            checks.run_checks(\n                [\n                    checks.Check(\n                        'admin_checks',\n                        'check_admin_models',\n                        ('admin', MyModelAdmin, 'fake_model'),\n                        id='admin_checks.E001'\n                    )\n                ],\n                {},\n            )\n        self.assertIn(\n            \"The value of 'save_as' must be a boolean.\",\n            str(error.exception)\n        )\n        self.assertIn(\n            \"The value of 'save_on_top' must be a boolean.\",\n            str(error.exception)\n        )\n        self.assertIn(\n            \"The value of 'list_filter' refers to 'nonexistent_field', which is not a field.\",\n            str(error.exception)\n        )\n        self.assertIn(\n            \"The value of 'orderring' refers to 'nonexistent_field', which is not a field.\",\n            str", "    def test_model_admin_checks(self):\n        admin.site = AdminSite()\n        \n        # Test case for `ModelAdmin.save_as`\n        class SaveAsModelAdmin(admin.ModelAdmin):\n            save_as = True\n        admin.site.register(Song, SaveAsModelAdmin)\n\n        # Test case for `ModelAdmin.save_on_top`\n        class SaveOnTopModelAdmin(admin.ModelAdmin):\n            save_on_top = True\n        admin.site.register(Author, SaveOnTopModelAdmin)\n\n        # Test case for `ModelAdmin.inlines` with fields, filters and custom model formsets\n        class BookInline(GenericStackedInline):\n            model = Book\n            extra = 1\n            fk_name = 'author'\n            formset = forms.BaseModelFormSet\n\n        class AuthorAdmin(admin.ModelAdmin):\n            inlines = [BookInline]\n            list_display = ['name', 'books_count']\n\n                return obj.book_set.count()\n\n        admin.site.register(Author, AuthorAdmin)\n\n\n        errors = checks.run_checks(\n            ['admin_checks'],  # The app containing the checks\n            models=[\n                Album,\n                Author,\n                Book,\n                City,\n                Influence,\n                Song,\n                State,\n                TwoAlbumFKAndAnE,\n            ],\n            admin_site=admin.site,\n        )\n        self.assertEqual(len(errors), 0)\n", "    def test_check_model_admin(self):\n        site = AdminSite()\n        admin.site.unregister(Song)\n        admin.site.register(Song, MyAdmin)\n\n        errors = checks.run_checks(\n            lambda: admin.site.admin_view(\n                admin.site.get_urls(),\n                {},\n                request=None,\n                return_content=False,\n            ),\n            'admin',\n        )\n        self.assertEqual(errors, ['error!'])\n\n"], "sample_895": ["    def test_remainder_behavior(self, remainder, expected_output_shape):\n        X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n        transformer_list = [\n            (StandardScaler(), [\"numerical_column\"]),\n            (\n                OneHotEncoder(handle_unknown=\"ignore\"),\n                [\"categorical_column\"],\n            ),\n        ]\n\n        ct = ColumnTransformer(\n            transformer_list, remainder=remainder, n_jobs=1\n        )\n        ct.fit(X)\n        output = ct.transform(X)\n        assert output.shape == expected_output_shape\n\n\n\n", "    def test_remainder_passthrough(self):\n        X = np.array([[1, 2], [3, 4]], dtype=float)\n\n        ct = make_column_transformer(\n            [(StandardScaler(), [0]), (\"passthrough\", None, [1])],\n            remainder=\"passthrough\",\n        )\n        ct.fit(X)\n        X_transformed = ct.transform(X)\n\n        expected_scaled = StandardScaler().fit_transform(X[:, 0][:, None])\n        expected_passthrough = X[:, 1][:, None]\n\n        assert_allclose(X_transformed[:, 0], expected_scaled)\n        assert_allclose(X_transformed[:, 1], expected_passthrough)\n", "    def test_remainder_handling(self, transformers, remainder):\n        ct = make_column_transformer(\n            *transformers, remainder=remainder, n_jobs=1, verbose=False\n        )\n        X_test = np.array([[1, 2], [3, 4], [5, 6]])\n        X_transformed = ct.fit_transform(X_test)\n        if remainder == \"passthrough\":\n            assert X_transformed.shape == (3, 2)\n        else:\n            assert X_transformed.shape == (3, 1)\n", "    def test_remainder_passthrough(self):\n        X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n        scaler = StandardScaler()\n        onehot = OneHotEncoder(handle_unknown=\"ignore\")\n\n        ct = make_column_transformer(\n            (scaler, [0]),\n            remainder=\"passthrough\",\n            n_jobs=1,\n        )\n\n        X_trans = ct.fit_transform(X)\n        assert_allclose(\n            ct.named_transformers_[\"standardscaler\"].transform(X[:, [0]]),\n            X_trans[:, [0]],\n        )\n        assert_allclose(X[:, 1:], X_trans[:, 1:])\n", "    def test_fit_transform_dataframe(self, transformer, transformer_args):\n        X = pd.DataFrame(\n            {\"col1\": [1, 2, 3, 4], \"col2\": [5, 6, 7, 8], \"col3\": [\"A\", \"B\", \"C\", \"D\"]}\n        )\n        ct = ColumnTransformer(\n            transformers=[transformer[0], transformer[1]],\n            n_jobs=1,\n        )\n        ct.fit_transform(X)\n        if None not in transformer_args and transformer_args[1]:\n            assert len(ct.transformers_) == len(transformer_args)\n        else:\n            assert len(ct.transformers_) == 1\n        X_transformed = ct.transform(X)\n        if transformer[0].fit is not a property:\n            assert X_transformed.shape[1] == len(X.columns) - 1\n        else:\n            assert X_transformed.shape[1] == len(transformer[1])\n\n\n\n"], "sample_912": ["    def test_parse_annotation(\n        self", "    def test_parse_annotation(self):\n        assert _parse_annotation('Optional[int]') == 'Optional[int]'\n        assert _parse_annotation('Optional[str]') == 'Optional[str]'\n        assert _parse_annotation('List[List[int]]') == 'List[List[int]]'\n        assert _parse_annotation('Dict[str, int]') == 'Dict[str, int]'\n        assert _parse_annotation('Any') == 'Any'\n        assert _parse_annotation('None') == 'None'\n        assert _parse_annotation('~int') == 'int'\n        assert _parse_annotation('~str') == 'str'\n        assert _parse_annotation('~List[str]') == 'List[str]'\n\n\n", "    def test_parse_complex_annotation(\n            ):\n        sig = 'def f(a: int, *b: str, **c: float) -> list[Tuple[str, int]]:'\n        exp = 'f(a: int, *b: str, **c: float) -> list[Tuple[str, int]]'\n        assert parse(sig) == exp\n\n\n", "    def test_parse_annotation(self):\n        self.assertEqual(_parse_annotation(None), None)\n        self.assertEqual(_parse_annotation('int'), 'int')\n        self.assertEqual(_parse_annotation('int | str'), 'int | str')\n        self.assertEqual(_parse_annotation('List[str]'), 'List[str]')\n        self.assertEqual(_parse_annotation('Optional[int]'), 'Optional[int]')\n\n        # Test with complex annotations\n        self.assertEqual(_parse_annotation('Dict[str, List[int]]'), 'Dict[str, List[int]]')\n\n\n\n", "    def test_parse(sig, expected):\n        assert parse(sig) == expected\n\n\n"], "sample_178": ["    def test_formset_factory_with_custom_kwargs(self):\n        class CustomForm(Form):\n            pass\n\n            self.assertIn('custom_kwarg', formset.forms[0].fields)\n            self.assertEqual(formset.forms[0].fields['custom_kwarg'].initial, expected_value)\n\n        formset = formset_factory(CustomForm, custom_kwarg='custom_value')\n        test_custom_kwarg_is_present(self, formset, 'custom_value')\n\n", "    def test_can_order_and_can_delete(self):\n        formset = FavoriteDrinksFormSet(\n            [{'name': 'Water'}, {'name': 'Juice'}, {'name': 'Coke'}],\n            can_order=True,\n            can_delete=True,\n        )\n\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(formset.total_form_count(), 3)\n\n        # Check that the ordering field is present and has the correct default values\n        for i in range(len(formset.forms)):\n            self.assertIn(ORDERING_FIELD_NAME, formset.forms[i].fields)\n            self.assertEqual(formset.forms[i].fields[ORDERING_FIELD_NAME].widget, BaseFormSet.get_ordering_widget())\n            self.assertEqual(formset.forms[i].fields[ORDERING_FIELD_NAME].initial, i + 1)\n\n        # Check that the deletion field is present\n        for form in formset:\n            self.assertIn(DELETION_FIELD_NAME, form.fields)\n\n", "    def test_formset_factory_invalid_absolute_max(self):\n        with self.assertRaises(ValueError):\n            formset_factory(Choice, absolute_max=10, max_num=11)\n", "    def test_formset_factory_absolute_max(self):\n        with self.assertRaises(ValueError):\n            formset_factory(Choice, max_num=500, absolute_max=499)\n", "    def test_formset_total_form_count(self):\n        formset = self.make_choiceformset(\n            formset_data=[('a', 1), ('b', 2)], total_forms=3\n        )\n        self.assertEqual(formset.total_form_count(), 3)\n\n        formset = self.make_choiceformset(formset_data=[('a', 1), ('b', 2)])\n        self.assertEqual(formset.total_form_count(), 2)\n"], "sample_473": ["    def test_asgi_handler_request_parsing(self):\n        factory = AsyncRequestFactory()\n        scope = {\n            \"type\": \"http.request\",\n            \"method\": \"POST\",\n            \"scheme\": \"http\",\n            \"path\": \"/path/to/view/\",\n            \"headers\": [\n                (b\"Content-Type\", b\"application/json\"),\n                (b\"X-Custom-Header\", b\"value\"),\n            ],\n            \"query_string\": b\"param1=value1&param2=value2\",\n            \"client\": [\"127.0.0.1\", 8080],\n            \"server\": [\"localhost\", 8000],\n        }\n        async def receive(message):\n            await asyncio.sleep(0)\n            return next(iter(scope))\n        async def send(message):\n            pass\n\n        handler = ASGIHandler()\n        async with ThreadSensitiveContext():\n            await handler.__call__(scope, receive, send)\n            request = handler.request\n\n\n        self.assertEqual(request.method, \"POST\")\n        self.assertEqual(request.path, \"/path/to/view/\")\n        self.assertEqual(request.content_type, \"application/json\")\n        self.assertEqual(request.headers.get(\"X-Custom-Header\"), \"value\")\n        self.assertEqual(request.GET.get('param1'), 'value1')\n        self.assertEqual(request.GET.get('param2'), 'value2')\n        self.assertEqual(request.META.get('REMOTE_ADDR'), '127.0.0.1')\n        self.assertEqual(request.META.get('REMOTE_PORT'), '8080')\n        self.assertEqual(request.META.get('SERVER_NAME'), 'localhost')\n        self.assertEqual(request.META.get('SERVER_PORT'), '8000')\n", "compilation error", "    def test_script_prefix(self):\n        request = self.request_factory.get(\"/a/path/here\")\n        handler = WSGIHandler()\n        handler.get_response(request)\n        self.assertEqual(request.script_name, \"/my/prefix\")\n        self.assertEqual(request.path_info, \"a/path/here\")\n        self.assertEqual(request.path, \"/my/prefix/a/path/here\")", "compilation error", "    def test_create_request_unicodedecodeerror(self):\n        with override_settings(FILE_UPLOAD_MAX_MEMORY_SIZE=1):\n            handler = ASGIHandler()\n            scope = {\"type\": \"http\", \"method\": \"GET\", \"path\": \"/\", \"scheme\": \"http\"}\n            with self.assertRaises(HttpResponseBadRequest) as cm:\n                handler.create_request(scope, self.request_factory.create_request(scope))\n            self.assertEqual(cm.exception.status_code, 400)\n"], "sample_577": ["    def test_init_scales(self):\n\n        p = Plot(data=Default)\n        assert p._scales == {}\n\n        p = Plot(data=Default, pair_variables={\"x\": \"x\", \"y\": \"y\"})\n        assert p._scales == {}\n\n        data = DataFrame({\"x\": [1, 2, 3], \"y\": [4, 5, 6]})\n        p = Plot(data=data, pair_variables={\"x\": \"x\", \"y\": \"y\"})\n        assert p._scales == {}\n\n        p = Plot(data=data, pair_variables={\"x\": \"x\"}, scales={\"x\": Continuous()})\n        assert p._scales == {\"x\": Continuous()}\n \n        p = Plot(data=data, pair_variables={\"x\": \"x\", \"y\": \"y\"}, scales={\"y\": Continuous()})\n        assert p._scales == {\"y\": Continuous()}\n\n        p = Plot(data=data, pair_variables={\"x\": \"x\", \"y\": \"y\"}, scales={\"x\": Continuous(), \"y\": Continuous()})\n        assert p._scales == {\"x\": Continuous(), \"y\": Continuous()}\n\n\n", "    def test_init_with_empty_dataframe(self):\n        df = pd.DataFrame()\n        p = Plot(data=df, mark=MockMark(), layers=[])\n        assert p._data.frame is None\n        assert len(p._data.frames) == 0\n\n\n", "    def test_basic_setup(self):\n        data = pd.DataFrame({\"x\": [1, 2, 3], \"y\": [4, 5, 6], \"col\": [\"A\", \"A\", \"B\"]})\n        p = Plot(data=data)\n        assert p._subplots == []\n        assert p._legend_contents == []\n\n        p._setup_subplots(nrows=1, ncols=2)\n        assert len(p._subplots) == 2\n        assert p._subplots[0] == {\"x\": \"x\", \"y\": \"y\", \"ax\": None}\n        assert p._subplots[1] == {\"x\": \"x\", \"y\": \"y\", \"ax\": None}\n\n\n\n\n        p = Plot(data=data)\n        p._setup_scales()\n        assert \"x\" in p._scales\n        assert \"y\" in p._scales\n        assert isinstance(p._scales[\"x\"], Continuous)\n        assert isinstance(p._scales[\"y\"], Continuous)\n\n\n\n\n", "    def test_init_with_empty_data(self):\n        df = pd.DataFrame()\n        spec = Plot()\n        spec._data = PlotData(df, {\"x\": 0, \"y\": 1})\n        assert spec._data.frame.empty\n        assert len(spec._data.frames) == 0\n        assert spec._scales == {}\n        assert spec._subplots == {}\n        assert spec._legend_contents == []\n", "    def test_init_empty_frame(self):\n        p = Plot(data=Default(DataFrame()), mark=MockMark(), scales=Default(dict()))\n        assert p._subplots == []\n        assert p._legend_contents == []\n\n\n\n"], "sample_599": ["    def test_cf_scale_offset_coder_variable_with_empty_encoding(self):\n        var = xr.Variable(\n            dims=(\"x\", \"y\"),\n            data=np.arange(4).reshape(2, 2),\n            attrs={\"Conventions\": \"CF\"},\n        )\n        coder = CFScaleOffsetCoder()\n\n        encoded_var = coder.encode(var)\n        decoded_var = coder.decode(encoded_var)\n\n        assert_identical(var, decoded_var)\n", "    def test_cf_scale_offset_coder_mixed_data(self):\n        ds = xr.Dataset(\n            {\n                \"var1\": ((\"x\", \"y\"), np.arange(12).reshape((3, 4))),\n                \"var2\": ((\"x\",), np.array([10, 20, 30])),\n            },\n            coords={\"x\": [0, 1, 2], \"y\": [0, 1, 2, 3]},\n        )\n        ds = ds.assign_attrs(\n            {\"var1\": {\"scale_factor\": 2, \"add_offset\": 3}, \"var2\": {\"scale_factor\": 5}}\n        )\n\n        # Encode and decode with CFScaleOffsetCoder\n        coder = variables.CFScaleOffsetCoder()\n        encoded_ds = encode_cf_variable(ds, coder)\n        decoded_ds = decode_cf_variable(encoded_ds, coder)\n\n        # Assert that variables are close to the original\n        assert_allclose(ds[\"var1\"].values, decoded_ds[\"var1\"].values)\n        assert_allclose(ds[\"var2\"].values, decoded_ds[\"var2\"].values)\n\n", "    def test_cf_scale_offset_coder_with_dask(self):\n        data_dask = da.from_array(np.random.rand(10, 10), chunks=(5, 5))\n        var = xr.DataArray(\n            data=data_dask,\n            dims=(\"x\", \"y\"),\n            encoding={\"scale_factor\": 2.0, \"add_offset\": 1.5},\n        )\n        coder = CFScaleOffsetCoder()\n        encoded_var = coder.encode(var, name=\"test\")\n        decoded_var = coder.decode(encoded_var, name=\"test\")\n        assert_allclose(decoded_var.data, data_dask * 2.0 + 1.5)\n", "    def test_cf_mask_coder_multiple_fill_values(self):\n        data = np.array([1, 2, np.nan, 3, np.nan, 4], dtype=\"float32\")\n        var = xr.Variable((\"x\",), data, attrs={\"missing_value\": [np.nan, -999.0]})\n        encoded_var = encode_cf_variable(var)\n\n        decoded_var = decode_cf_variable(encoded_var)\n        assert_allclose(decoded_var.data, data)\n        assert decoded_var.attrs[\"missing_value\"] is None\n\n\n\n", "    def test_cf_scale_offset_coder_roundtrip(self):\n        data = np.array([1, 2, 3], dtype=np.float32)\n        scale_factor = 2\n        add_offset = 1\n        attrs = {\"scale_factor\": scale_factor, \"add_offset\": add_offset}\n        var = xr.Variable(((\"x\", 3),), data, attrs=attrs)\n\n        encoded_var = encode_cf_variable(var, CFScaleOffsetCoder())\n        decoded_var = decode_cf_variable(encoded_var, CFScaleOffsetCoder())\n\n        assert_allclose(decoded_var.data, data * scale_factor + add_offset)\n        assert_equal(decoded_var.attrs, var.attrs)\n\n\n\n"], "sample_321": ["    def test_invalid_origin(self):\n        req = TestingHttpRequest()\n        req.method = 'POST'\n        req.META['HTTP_ORIGIN'] = 'http://evil.com/'\n        req.META[settings.CSRF_COOKIE_NAME] = self._csrf_id\n        with override_settings(CSRF_TRUSTED_ORIGINS=['http://example.com/']):\n            middleware = CsrfViewMiddleware()\n            response = middleware.process_request(req, mock_view)\n            self.assertTrue(isinstance(response, HttpResponse))\n            self.assertEqual(response.status_code, 403)\n            self.assertIn(REASON_BAD_ORIGIN % 'http://evil.com/', response.reason)\n\n", "    def test_referrers_override_origin_check(self):\n        @override_settings(\n            CSRF_TRUSTED_ORIGINS=['https://example.com', '*'],\n            CSRF_COOKIE_DOMAIN='example.com',\n        )\n            response = self.client.get('/non_token_view')\n            self.assertEqual(response.status_code, 200)\n\n        response = self.client.get('/non_token_view', HTTP_REFERER='https://example.com/other')\n        self.assertEqual(response.status_code, 200)\n\n        response = self.client.get('/non_token_view', HTTP_REFERER='https://www.example.com/other')\n        self.assertEqual(response.status_code, 200)\n\n        response = self.client.get('/non_token_view', HTTP_REFERER='https://evil.example.com/other')\n        self.assertEqual(response.status_code, 200)\n        \n        self.client.get('/non_token_view', HTTP_REFERER='http://evil.example.com/other')\n        response = self.client.post('/non_token_view', HTTP_REFERER='https://evil.example.com/other')\n        self.assertEqual(response.status_code, 403)\n\n\n\n", "    def test_reject_non_same_domain_referer(self):\n        middleware = CsrfViewMiddleware()\n        req = self._get_POST_csrf_cookie_request(\n            cookie=self._csrf_id_cookie,\n        )\n        req.META['HTTP_ORIGIN'] = 'https://evil.example.com'\n        req.META['HTTP_REFERER'] = 'https://www.good.example.com/'\n        response = middleware.process_request(req, token_view, (), {})\n        self.assertIsInstance(response, HttpResponse)\n        self.assertEqual(response.status_code, 403)\n        self.assertIn(REASON_BAD_ORIGIN % 'https://evil.example.com', response.content.decode())\n", "    def test_view_with_exempt_decorator_doesnt_enforce_csrf(self):\n        @csrf_exempt\n            return HttpResponse(\"OK\")\n\n        response = self.client.get(view)\n        self.assertEqual(response.status_code, 200)\n\n        response = self.client.post(view)\n        self.assertEqual(response.status_code, 200)\n\n\n\n", "    def test_origin_check_with_same_domain_referer(self):\n        with override_settings(CSRF_TRUSTED_ORIGINS=['http://example.com']):\n            req = self._get_POST_csrf_cookie_request(cookie=self._csrf_id_cookie)\n            req.META['HTTP_ORIGIN'] = 'http://example.com'\n            req.META['HTTP_REFERER'] = 'http://example.com/some/path'\n            \n            middleware = CsrfViewMiddleware()\n            response = middleware.process_view(req, token_view, [], {})\n            self.assertEqual(response, None)\n"], "sample_552": ["    def test_update_layout_engine(renderer, fig, tmp_path):\n        # Create a figure with complex layout, expect it to be updated correctly.\n        gs = gridspec.GridSpec(2, 2)\n        ax1 = fig.add_subplot(gs[0, 0])\n        ax2 = fig.add_subplot(gs[0, 1])\n        ax3 = fig.add_subplot(gs[1, :])\n\n        # Set different layout engines and check for updated layout\n        engines = [\n            ConstrainedLayoutEngine(),\n            TightLayoutEngine(),\n            PlaceHolderLayoutEngine(),\n        ]\n        for engine in engines:\n            fig.set_layout_engine(engine)\n            # Force layout update\n            fig.draw(renderer)\n            # Assert updated layout by checking the position of subplots\n\n            # Get bounding boxes for each subplot\n            bbox_changed = False\n            for ax in [ax1, ax2, ax3]:\n                for spine in ax.spines.values():\n                    bbox_changed = bbox_changed | (spine.get_window_extent(renderer) != ax.get_tight_bbox(renderer).transformed(fig.transFigure))\n            assert bbox_changed\n", "    def test_fig_add_ax_observer(self):\n        fig = Figure()\n        ax = fig.add_subplot(111)\n        calls = 0\n\n            nonlocal calls\n            calls += 1\n\n        fig.add_axobserver(_test_observer)\n        ax.set_xlim(0, 1)\n        ax.set_ylim(0, 1)\n        assert calls == 1\n        ax.set_xlim(0, 2)\n        assert calls == 2\n", "    def test_constrained_layout_engine_execute(tmp_path):\n        fig = Figure()\n        ax1 = fig.add_subplot(1, 2, 1)\n        ax2 = fig.add_subplot(1, 2, 2)\n\n        # Set some text sizes to influence layout\n        ax1.set_title(\"Large Title\", fontsize=18)\n        ax2.set_title(\"Small Title\", fontsize=10)\n\n        # Manually set subplot padding\n        fig.subplots_adjust(wspace=0.4, hspace=0.4)\n\n        engine = ConstrainedLayoutEngine(pad=0.1, wspace=0.4, hspace=0.4)\n        engine.execute(fig)\n        # Check that subplots are constrained\n        x1_bbox = ax1.get_position().x\n        x2_bbox = ax2.get_position().x\n        assert x1_bbox + ax1.get_position().width < x2_bbox, \\\n            \"Subplots not constrained correctly\"\n\n        #  Manually save the figure to compare its state\n        with open(tmp_path / \"constrained_layout.png\", \"wb\") as f:\n            fig.savefig(f, bbox_inches=\"tight\")\n\n        # Load the saved figure and compare its state to the current\n        # state\n        with open(tmp_path / \"constrained_layout.png\", \"rb\") as f:\n            comparision_fig = Figure.from_file(f)\n\n        check_figures_equal(fig, comparision_fig)\n\n\n\n", "    def test_constrained_layout_no_adjust(self):\n        fig = Figure()\n        ax1 = fig.add_subplot(211)\n        ax2 = fig.add_subplot(212)\n\n        # Ensure that no adjustments are made by ConstrainedLayoutEngine\n        fig.set_layout_engine(ConstrainedLayoutEngine(\n            resize_func=lambda fig, w_pad: True))\n        fig.tight_layout()  # No adjustments should be made\n        \n\n\n\n", "    def test_fig_align_labels_tight_layout(self):\n        fig = plt.figure(figsize=(6, 4))\n        ax = fig.add_subplot(111)\n        ax.set_title(\"Figure Title\")\n        ax.set_xlabel(\"X Label\")\n        ax.set_ylabel(\"Y Label\")\n\n        tight_layout = fig.tight_layout\n        fig.tight_layout = lambda *args, **kwargs: None # disable tight layout\n\n        tight_layout() # Apply tight layout to reposition labels\n        fig.canvas.draw()\n        self.check_image('figure_align_labels', 'figure_align_labels_tight_layout')\n\n        fig.tight_layout = tight_layout # restore tight_layout\n\n"], "sample_869": ["    def test_brier_score_loss_with_integer_labels():\n        y_true = np.array([0, 1, 1, 0])\n        y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n        score = brier_score_loss(y_true, y_prob)\n        assert np.isclose(score, 0.03707091, rtol=1e-05)\n", "    def test_brier_score_loss_with_binary_labels_and_different_pos_labels():\n        y_true = np.array([0, 1, 1, 0])\n        y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n        for pos_label in [0, 1]:\n            score = brier_score_loss(y_true, y_prob, pos_label=pos_label)\n            assert score == brier_score_loss(y_true, 1 - y_prob, pos_label=1 - pos_label)  \n\n\n\n", "    def test_hinge_loss_multiclass(self):\n        X = np.array([[0], [1], [2], [3]])\n        Y = np.array([0, 1, 2, 3])\n        labels = np.array([0, 1, 2, 3])\n        est = svm.LinearSVC()\n        est.fit(X, Y)\n        pred_decision = est.decision_function([[-1], [2], [3]])\n        y_true = [0, 2, 3]\n        self.assertAlmostEqual(hinge_loss(y_true, pred_decision, labels),\n                              0.56, places=2)\n\n\n\n", "def test_brier_score_loss_with_sample_weights():\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n    sample_weight = np.array([0.5, 0.2, 0.3, 0.])\n\n    score = brier_score_loss(y_true, y_prob, sample_weight=sample_weight)\n    expected_score = 0.037\n    assert_almost_equal(score, expected_score)\n\n\n\n", "    def test_brier_score_loss_multiclass():\n        # Test brier score loss with multiclass classification\n        X, y_true, y_prob = make_prediction(dataset=datasets.load_iris(),\n                                            binary=False)\n\n        y_true_binarized = label_binarize(y_true, classes=range(3))\n        y_prob_binarized = np.hstack([y_prob[:, i]\n                                      for i in range(y_prob.shape[1])])\n\n        for i in range(y_true_binarized.shape[1]):\n            with ignore_warnings():\n                brier_score = brier_score_loss(y_true_binarized[:, i],\n                                             y_prob_binarized[:, i])\n            assert brier_score >= 0\n            assert brier_score <= 1\n"], "sample_566": ["    def test_constrained_layout_engine_preserve_old_size(self):\n        fig = Figure(figsize=(8, 6))\n        ax = fig.add_subplot(111)\n        # Set some initial size before layout engine is set\n        ax.set_xlim(0, 10)\n        ax.set_ylim(0, 10)\n\n        engine = ConstrainedLayoutEngine(pad=1.08)\n        fig.set_layout_engine(engine)\n\n        # Run layout with initial size\n        engine.execute(fig)\n\n        # Check that the size has not changed\n        assert ax.get_xlim() == (0, 10)\n        assert ax.get_ylim() == (0, 10)\n\n        engine.cleanup()\n\n\n\n", "    def test_tight_layout_with_legend(self):\n        fig, ax = plt.subplots(1, 1)\n        ax.plot([1, 2, 3], [4, 5, 6])\n        leg = ax.legend(['test'])\n        fig.tight_layout()\n        plt.close(fig)\n", "    def test_tight_layout_basic(self):\n        fig, ax = plt.subplots(1, 1)\n        ax.plot([1, 2, 3], [4, 5, 6])\n        ax.set_title('Test Title')\n        plt.tight_layout()\n        plt.close(fig) \n\n\n", "    def test_tight_layout_with_placeholder_engine(\n        self, image_comparison, check_figures_equal", "    def test_constrained_layout_align_labels(self):\n        fig, ax = plt.subplots()\n        ax.set_xlim(0, 10)\n        ax.set_ylim(0, 10)\n        ax.set_xlabel('X')\n        ax.set_ylabel('Y')\n        ax.text(5, 5, 'Some text',\n                horizontalalignment='center',\n                verticalalignment='center')\n        ax.set_title('Constrained Layout')\n        ax.set_aspect('equal')\n        fig.set_layout_engine(ConstrainedLayoutEngine())\n        fig.draw()\n        fig.savefig('constrained_layout_align_labels.png')\n        plt.close(fig)\n\n\n"], "sample_1186": ["    def test_transpose_shape_change(self):\n        a = MutableDenseNDimArray([[1, 2, 3], [4, 5, 6]])\n        b = a.transpose()\n        assert b.shape == (2, 3)\n        assert b != a\n        \n        a = MutableDenseNDimArray([1, 2, 3, 4, 5, 6], shape=(2, 3))\n        b = a.transpose()\n        assert b.shape == (3, 2)\n        assert b != a\n", "    def test_ndim_array_transpose(self):\n        for array_type in [ImmutableDenseNDimArray, MutableDenseNDimArray]:\n            a = array_type([[1, 2, 3], [4, 5, 6]])\n            b = a.transpose()\n            self.assertEqual(b.shape, (3, 2))\n            self.assertEqual(b[0], [1, 4])\n            self.assertEqual(b[1], [2, 5])\n            self.assertEqual(b[2], [3, 6])\n", "    def test_reshape(self):\n        a = MutableDenseNDimArray([[1, 2, 3], [4, 5, 6]])\n        b = a.reshape((3, 2))\n        self.assertEqual(b.shape, (3, 2))\n        self.assertEqual(list(a), list(b))\n\n        with raises(ValueError):\n            a.reshape((2, 4))\n", "    def test_transpose(self):\n        for ArrayClass in array_types:\n            a = ArrayClass([[1, 2], [3, 4]])\n            b = a.transpose()\n            self.assertTrue(isinstance(b, ArrayClass))\n            self.assertEqual(b.shape, (2, 2))\n            self.assertEqual(b[0, 0], 1)\n            self.assertEqual(b[0, 1], 3)\n            self.assertEqual(b[1, 0], 2)\n            self.assertEqual(b[1, 1], 4)\n\n\n\n", "    def test_asarray(self):\n        a = ImmutableDenseNDimArray([[1, 2], [3, 4]])\n        b = Array(ImmutableDenseNDimArray([[5, 6], [7, 8]]))\n        c = a.asarray()\n        assert c.shape == a.shape\n        assert c.tolist() == a.tolist()\n        assert isinstance(c, Array)\n\n\n\n"], "sample_517": ["    def test_annotation_offset_coordinates(self, image_comparison_data):\n        fig, ax = plt.subplots()\n\n        # Test offset coordinates\n        text = Annotation(\"Hello\", (0.5, 0.5), xycoords='data',\n                          textcoords='offset points',\n                          offset=(10, 10))\n        ax.add_artist(text)\n\n        plt.draw()\n        image_comparison_data.append(self.get_figure)\n", "    def test_annotation_arrow_patch_properties(self):\n        fig, ax = plt.subplots()\n        a = Annotation(\"Test\", (0.5, 0.5), arrowprops=dict(\n            arrowstyle=\"->\",\n            mutation_scale=10,\n            shrinkA=10,\n            headwidth=30,\n            headlength=20,\n        ))\n        a.set_artist_props(fontsize=10)\n        ax.add_artist(a)\n        fig.canvas.draw()\n\n        bbox = a.get_window_extent()\n        width, height = bbox.width, bbox.height\n\n        # assert arrow width matches expected width based on headwidth\n        arrow_width = a.arrow_patch.get_width()\n        assert_almost_equal(arrow_width, 10 / 10, decimal=2)  \n\n        # assert arrow width matches expected width based on shrinkA\n        assert_almost_equal(arrow_width, width * 0.1, decimal=2)\n\n        # assert arrow headwidth matches expected value\n        assert_almost_equal(a.arrow_patch.width, 3, decimal=2)\n\n        # assert arrow headlength matches expected value\n        assert_almost_equal(a.arrow_patch.head_length, 2, decimal=2)\n", "    def test_annotation_xycoords_offset(self, image_comparison):\n        fig, ax = plt.subplots()\n\n        annotation = Annotation('Test', (0.5, 0.5), xycoords='offset points',\n                                 textcoords='offset points',\n                                 arrowprops=dict(arrowstyle='->'))\n        annotation.set_text('Offset xycoords')\n        ax.add_artist(annotation)\n        plt.draw()\n        image_comparison(\n            filename='annotation_xycoords_offset',\n            baseline_images='annotation_xycoords_offset',\n            tol=0.05\n        )\n", "    def test_annotation_offset_coordinates(self):\n        fig, ax = plt.subplots()\n\n        # Test setting xycoords to 'offset' points\n        annotation = Annotation('Offset Points', (0, 0), xycoords='offset points',\n                                arrowprops=dict(arrowstyle='->',\n                                               connectionstyle='arc3,rad=0.2'))\n        annotation.set_anncoords('axes')\n        annotation.draggable(state=True)\n\n        # Test setting xycoords to 'offset' pixels\n        annotation = Annotation('Offset Pixels', (0, 0), xycoords='offset pixels',\n                                arrowprops=dict(arrowstyle='->',\n                                               connectionstyle='arc3,rad=0.2'))\n        annotation.set_anncoords('axes')\n        annotation.draggable(state=True)\n        \n\n        plt.cla()\n        annotation = Annotation('Offset', (1, 1), xycoords='offset points',\n                                arrowprops=dict(arrowstyle='->',\n                                               connectionstyle='arc3,rad=0.2'))\n        annotation.set_anncoords('data')\n        annotation.draggable(state=True)\n\n        plt.cla()\n        annotation = Annotation('Offset', (1, 1), xycoords='offset pixels',\n                                arrowprops=dict(arrowstyle='->',\n                                               connectionstyle='arc3,rad=0.2'))\n        annotation.set_anncoords('data')\n        annotation.draggable(state=True)\n\n        plt.show()\n", "    def test_annotation_offset(self):\n        fig, ax = plt.subplots()\n\n        # This test checks for correct offset handling with a reference point\n        # and checks if the offset coordinate string is interpreted as expected.\n\n        # Define the annotation point with an offset\n        annotation_text = Annotation(\"Test Annotation\", xy=(0.5, 0.5), xycoords='data',\n                                  xytext=(0.2, 0.3),\n                                  textcoords='offset points',\n                                  arrowprops=dict(arrowstyle=\"->\"))\n        annotation_text.set_artist(ax)\n\n        # We expect the offset to be relative to the point (0.5, 0.5) as the\n        # reference point defined in xy.\n        expected_x = 0.5 - 0.2 # 0.3\n        expected_y = 0.5 - 0.3 # 0.2\n\n        # Calculate the location of the annotation text based on the offset\n        transformed_pos = annotation_text.get_window_extent(ax.figure.canvas.renderer)\n        x, y = transformed_pos.get_extents()[0]\n\n        assert_almost_equal(x, expected_x, decimal=4)\n        assert_almost_equal(y, expected_y, decimal=4)\n\n\n\n"], "sample_43": ["    def test_regular_events_p0(self, p0):\n        dt = 0.1\n        t = dt * np.arange(100)\n        x = np.zeros(len(t))\n        x[np.random.randint(0, len(t), len(t) // 10)] = 1\n        edges = bayesian_blocks(t, x, fitness='regular_events', dt=dt, p0=p0)\n        assert len(edges) > 1\n", "    def test_regular_events_multiple_ticks(self):\n        dt = 0.1\n        t = np.arange(0, 10, dt)\n        x = np.zeros_like(t)\n        x[np.random.randint(0, len(t), len(t) // 10)] = 1\n        edges = bayesian_blocks(t, x, fitness='regular_events', dt=dt)\n        assert len(edges) == 5\n        # assert that the edges are roughly at the expected locations\n        assert_allclose(edges, [0., 2.0, 4.0, 6.0, 8.0, 10.0]) \n", "    def test_fit_with_custom_p0_gamma_ncp_prior(self, fitness):\n        t = np.random.normal(size=100)\n        if fitness == 'regular_events':\n            x = np.zeros(len(t))\n            x[np.random.randint(0, len(t), len(t) // 10)] = 1\n            dt = 0.05\n            fitfunc = RegularEvents(dt,\n                                    p0=0.01,\n                                    gamma=2.0,\n                                    ncp_prior=None)\n        else:\n            fitfunc = getattr(FitnessFunc, fitness)()\n            x = np.random.randint(0, 5, size=len(t))\n        edges = bayesian_blocks(t, x, fitness=fitfunc,\n                               p0=0.01,\n                               gamma=2.0,\n                               ncp_prior=None)\n        assert not np.isnan(edges).any()\n", "    def test_bayesian_blocks_fits_example(fitness):\n        t = np.arange(10)\n        x = np.ones(t.size)\n        if fitness == 'regular_events':\n            t = np.arange(0, 10, 0.5)\n            x[::2] = 0\n        edges = bayesian_blocks(t, x=x, fitness=fitness)\n        assert edges.size > 1\n\n\n\n", "    def test_regular_events_no_sigma(self):\n        t = np.arange(100) * 0.1\n        dt = 0.1\n        x = np.zeros(len(t))\n        x[np.random.randint(0, len(t), len(t) // 10)] = 1\n        edges = bayesian_blocks(t, x, fitness='regular_events', dt=dt)\n        assert edges.size > 1\n"], "sample_311": ["    def test_action_checkbox_name(self):\n        \"\"\"\n        Test that the ACTION_CHECKBOX_NAME is correctly set.\n\n        This ensures that the checkbox for actions in the admin list\n        view uses the correct name.\n        \"\"\"\n        response = self.client.get(reverse('admin:book_list'))\n        self.assertIn(ACTION_CHECKBOX_NAME, response.context['adminform']['fields'])\n\n\n\n", "    def test_admin_site_urls(self):\n        from django.urls import reverse\n\n        # Test site-specific URLs\n        self.assertEqual(reverse('admin:index'), '/admin/')  \n        self.assertEqual(reverse('admin:password_change'), '/admin/password/change/')\n\n        # Test model-specific URLs\n        self.assertEqual(\n            reverse('admin:%s_%s_changelist' % (Article._meta.app_label, Article._meta.model_name)),\n            '/admin/articles/',\n        )\n        self.assertEqual(\n            reverse('admin:%s_%s_add' % (Article._meta.app_label, Article._meta.model_name)),\n            '/admin/articles/add/',\n        )\n\n        # Test custom AdminSite URLs\n        # Test with a custom AdminSite that has a different URL prefix.\n        with override_settings(ADMIN_URL_PREFIX='/myadmin/'):\n            self.assertEqual(reverse('admin:index'), '/myadmin/')\n\n\n\n", "    def test_changelist_search(self):\n        response = self.client.get(reverse('admin:article_changelist'))\n        self.assertNotIn('Article 2', response.content.decode())\n        response = self.client.get(reverse('admin:article_changelist') + '?q=Oldest')\n        self.assertIn('Oldest content', response.content.decode())\n\n", "    def test_admin_form_fields(self):\n        response = self.client.get(reverse('admin:book_change', args=(self.b1.pk,)))\n        fields = self.get_admin_form_fields(response)\n        expected_fields = [\n            {'field': 'title', 'attrs': {}},\n            {'field': 'published', 'attrs': {}},\n            {'field': 'slug', 'attrs': {}},\n            {'field': 'description', 'attrs': {}},\n            {'field': 'promo_set', 'attrs': {'data-inline-template': 'admin\\/edit\\_inline._html\"}},\n        ]\n        self.assertEqual(fields, expected_fields)\n", "    def test_admin_action_view_success_full(self):\n        response = self.client.post(\n            reverse('admin:article_changelist'),\n            {\n                'action': 'my_custom_action',\n                'selected_items': [self.a1.pk, self.a2.pk, self.a3.pk],\n            },\n            HTTP_REFERER=reverse('admin:article_changelist'),\n        )\n        self.assertEqual(response.status_code, 302)\n        self.assertRedirects(response, reverse('admin:article_changelist'))\n\n"], "sample_749": ["    def test_remainder_drop(self):\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        scaler = StandardScaler()\n        ct = make_column_transformer(\n            ([0], scaler), remainder='drop')\n\n        X_transformed = ct.fit_transform(X)\n        assert X_transformed.shape == (3, 1)\n", "    def test_passthrough_remainder(self):\n        X = np.arange(10).reshape((2, 5))\n        transformer = ColumnTransformer([\n            (\"passthrough\", passthrough, [0, 1, 2]),\n            (\"scaler\", StandardScaler(), [3, 4])\n        ], remainder='passthrough')\n\n        X_trans = transformer.fit_transform(X)\n        assert_equal(X_trans.shape, (2, 5))\n        assert_allclose_dense_sparse(X[:, 0:3], X_trans[:, 0:3])\n        assert_allclose_dense_sparse(\n            StandardScaler().fit_transform(X[:, 3:5]), X_trans[:, 3:5])\n", "    def test_remainder_drop(self):\n        X = np.array([[1, 2], [3, 4]])\n        ct = ColumnTransformer(\n            transformers=[('a', StandardScaler(), [0])],\n            remainder='drop')\n\n        with pytest.raises(NotFittedError):\n            ct.transform(X)\n        ct.fit(X)\n        result = ct.transform(X)\n        assert result.shape == (2, 1)\n", "    def test_remainder_drop(self):\n        X = np.array([[1., 2.], [3., 4.]])\n        trans1 = StandardScaler()\n        ct = make_column_transformer(\n            [(0, trans1)], remainder='drop')\n        X_transformed = ct.fit_transform(X)\n        assert_true(X_transformed.shape[1] == 1)\n        assert_allclose_dense_sparse(X_transformed, trans1.transform(X[:, 0].reshape(-1, 1)))\n\n        ct = ColumnTransformer(\n            [(\"nom\", trans1, [0])], remainder='drop')\n        X_transformed = ct.fit_transform(X)\n        assert_true(X_transformed.shape[1] == 1)\n        assert_allclose_dense_sparse(X_transformed, trans1.transform(X[:, 0].reshape(-1, 1)))\n", "    def test_remainder_drop(self):\n        X = np.array([[1, 2], [3, 4]])\n        ct = ColumnTransformer(\n            [(\"scl\", StandardScaler(), [0])], remainder='drop'\n        )\n        X_transformed = ct.fit_transform(X)\n        assert X_transformed.shape == (2, 1)\n"], "sample_382": ["    def test_watch_for_template_changes(self, mock_reset_loaders):\n        autoreload.autoreload_start()\n        with mock.patch('os.path.exists') as mock_exists:\n            mock_exists.return_value = True\n            watch_for_template_changes(None, **{'sender': autoreload})\n            mock_reset_loaders.assert_called_once()\n            \n", "    def test_template_changed_triggers_reset(self, mock_reset_loaders):\n        file_path = EXTRA_TEMPLATES_DIR / \"test_template.html\"\n        file_path.write_text(\"<h1>Hello, world!</h1>\")\n\n        autoreload.start()\n        file_changed.send(autoreload.module_autoreload, file_path=file_path)\n\n        mock_reset_loaders.assert_called_once()\n", "    def test_watch_for_template_changes(self, mock_reset_loaders):\n        autoreload.start()\n        filepath = EXTRA_TEMPLATES_DIR / \"test_template.html\"\n        filepath.write_text(\"<h1>Hello, World!</h1>\")\n\n        with mock.patch('os.path.exists', return_value=True):\n            with mock.patch('time.sleep', return_value=None):\n                file_changed(sender=autoreload, file_path=filepath)\n\n        mock_reset_loaders.assert_called()\n", "    def test_file_changed_event_triggered_for_non_py_files(self, mock_reset_loaders):\n        with mock.patch('django.utils.autoreload.is_django_path', return_value=False) as mock_is_django_path:\n            file_path = EXTRA_TEMPLATES_DIR / \"test.html\"\n            file_path.touch()  \n\n            TemplateReloadTests.file_changed(\n                None, file_path=file_path\n            )\n\n            mock_reset_loaders.assert_called_once()\n            mock_is_django_path.assert_called_once()\n", "    def test_template_change_triggers_reset(self, mock_reset_loaders):\n        # Create a template file in the extra templates directory\n        template_path = EXTRA_TEMPLATES_DIR / \"test_template.html\"\n        with open(template_path, 'w') as f:\n            f.write('Hello, World!')\n\n        # Trigger a file change event for the template\n        autoreload.file_changed(template_path)\n\n        # Assert that reset_loaders was called\n        mock_reset_loaders.assert_called_once()\n"], "sample_1193": ["    def test_idiff_with_list_of_y_variables():\n        x, y, z = Symbol('x'), Symbol('y'), Symbol('z')\n        eq = x + y + z  \n        with raises(ValueError):\n            idiff(eq, [y, z], x)\n        with raises(ValueError):\n            idiff(eq, [y, z, x], x)\n        \n        eq = x**2 + y**2 + z**2 - 1\n        dydx = idiff(eq, [y, z], x)\n        assert dydx == -Derivative(x, x) - Derivative(z, x)\n", "    def test_farthest_points_complex():\n        p = [\n            Point2D(-1, 1),\n            Point2D(1, 1),\n            Point2D(1, -1),\n            Point2D(-1, -1),\n            Point2D(2, 2),\n            Point2D(-2, 2),\n        ]\n        result = farthest_points(*p)\n        assert len(result) == 1\n        assert (Point2D(-1, 1), Point2D(2, 2)) in result\n\n", "compilation error", "    def test_idiff_symbols():\n        x, y, a = Symbol('x'), Symbol('y'), Symbol('a')\n        assert idiff(x + a + y, [y, a], x) == -1\n        assert idiff(x + a + y, y, x) == -1\n        assert idiff(x**2 + y**2 - 4, y, x) == -x/y\n        assert idiff(x**2 + y**2 - 4, y, x, 2).simplify() == (-x**2 - y**2)/y**3\n\n\n\n", "    def test_find():\n        from sympy.abc import x, y\n\n        assert find('x', x + y) == x\n        assert find(x, x + y) == x\n        assert find('y', x + y) == y\n        with raises(ValueError):\n            find('z', x + y)\n        point = Point2D(1, 2)\n        assert find('x', point) == point.x\n        assert find('y', point) == point.y\n\n\n\n"], "sample_1065": ["    def test_binom_mod_prime(self):\n        q = 7\n        for n in range(10):\n            for k in range(n + 1):\n                result = binomial(n, k).eval(q=q)\n                expected = binomial(n, k).expand(basic=True).eval(q=q)\n                self.assertEqual(result, expected)\n", "    def test_binomial_mod_negative(self):\n        n = Symbol('n', integer=True)\n        k = Symbol('k', integer=True)\n        self.assert_equal(binomial(n, k).mod(2), binomial(n, k).mod(2))  \n\n", "    def test_binomial_mod_zero(self):\n        for n, k in [(5, 3), (10, 7), (-5, 0)]:\n            with self.subTest(n=n, k=k):\n                self.assertEqual(binomial(n, k).subs(Mod(1, 0), 1),\n                                 binomial(n, k))\n", "    def test_binomial_large_integer(self):\n        from sympy.ntheory.primefactors import primefactors\n        n = 100000\n        for k in range(n + 1):\n            b = binomial(n, k)\n            b2 = factorial(n) // (factorial(k)*factorial(n - k))\n            assert b == b2, f\"binomial({n},{k}) mismatch: {b} != {b2}\"\n            b_prime_factors = set(primefactors(b))\n            b2_prime_factors = set(primefactors(b2))\n            assert b_prime_factors == b2_prime_factors, \\\n                f\"Binomial prime factors mismatch: {b_prime_factors} != {b2_prime_factors}\"\n", "    def test_rewrite_as_gamma_complex(self):\n        n, k = symbols('n k', complex=True)\n        assert binomial(n, k).rewrite('gamma').expand(complex=True).simplify() == \\\n            gamma(n + 1)/(gamma(k + 1)*gamma(n - k + 1))\n        assert binomial(n, -k).rewrite('gamma').expand(complex=True).simplify() == \\\n            gamma(n + 1)/(gamma(-k + 1)*gamma(n + k))\n"], "sample_418": ["compilation error", "    def test_length_is_integer(self):\n        @setup({\"length_is02\": '{% if some_list|length_is:4 %}Four{% endif %}'})\n        self.assertTemplateResultEqual(\n            \"Four\",\n            self.engine.from_string(\"{{ length_is02 }}\"),\n            context={\"some_list\": [1, 2, 3, 4]},\n        )\n\n\n\n", "    def test_length_is_with_length_5(self):\n        self.assertEqual(self.engine.render(self.length_is02).strip(), \"\")\n", "compilation error", "    def test_length_is_5(self):\n        self.assertContains(self.render('length_is02'), \"Five\")\n"], "sample_312": ["    def test_add_squash(self):\n        node = Node()\n        child1 = Node(children=[('a', 1)])\n        child2 = Node(children=[('b', 2)])\n\n        squshed = node.add(child1, 'AND', squash=True)\n        self.assertEqual(node.children, [squshed])\n        self.assertEqual(squshed, child1)\n\n        node = Node()\n        child1 = Node(children=[('a', 1)])\n        child2 = Node(children=[('b', 2)])\n        node.add(child1, 'AND', squash=True)\n        node.add(child2, 'AND', squash=True)\n        self.assertEqual(node.children, [child1, child2])\n\n        node = Node(children=[('a', 1)])\n        child2 = Node(children=[('b', 2)])\n        node.add(child2, 'OR', squash=True)\n        self.assertEqual(node.children, [child2])\n\n\n\n", "    def test_add_squash(self):\n        node2 = Node(connector='AND', children=[('c', 3)])\n        result = self.node1.add(node2, 'AND', squash=True)\n        self.assertEqual(len(self.node1.children), 2)\n        self.assertEqual(self.node1.children[0], node2)\n        self.assertEqual(self.node1.children[1], self.node1_children[0])\n        self.assertEqual(result, node2)\n", "    def test_add_squash(self):\n        node = Node()\n        child1 = Node()\n        child2 = Node()\n        node.add(child1, 'AND', squash=True)\n        node.add(child2, 'AND', squash=True)\n        self.assertEqual(len(node.children), 2)\n        self.assertTrue(child1 in node.children)\n        self.assertTrue(child2 in node.children)\n", "    def test_add_squash(self):\n        data = Node(connector='AND', children=[('c', 3)])\n        node3 = self.node1.add(data, 'AND', squash=True)\n        self.assertEqual(node3, data)\n        self.assertEqual(self.node1.children, [('a', 1), ('b', 2), data])\n", "    def test_add_squash_single_child(self):\n        single_child = Node(children=[('c', 3)])\n        result = self.node1.add(single_child, 'AND', squash=True)\n        self.assertEqual(self.node1.children, [('a', 1), ('b', 2), single_child])\n        self.assertEqual(result, single_child)\n\n"], "sample_789": ["    def test_adaboost_regressor_base_estimator(self):\n        # Test with a custom base estimator\n        class CustomRegressor(BaseEstimator, RegressorMixin):\n                pass\n                return X[:, 0]\n\n        reg = AdaBoostRegressor(base_estimator=CustomRegressor(),\n                               n_estimators=10)\n        reg.fit(X, y_regr)\n        assert_array_equal(reg.predict(X), X[:, 0])\n", "    def test_adaboost_regressor_sparse(self):\n        X = csc_matrix(np.array([[1, 2], [3, 4], [5, 6]]))\n        y = np.array([1, 2, 3])\n\n        reg = AdaBoostRegressor()\n        reg.fit(X, y)\n        predict = reg.predict(X)\n\n        assert_array_almost_equal(predict, y)\n\n", "    def test_adaboost_regressor_sample_weight(self):\n        X, y = shuffle(boston.data, boston.target, random_state=rng)\n        # Test with sample weights\n\n        sample_weight = rng.uniform(size=len(y))\n        reg = AdaBoostRegressor(n_estimators=10, random_state=rng)\n        reg.fit(X, y, sample_weight=sample_weight)\n\n        y_pred  = reg.predict(X)\n        assert_array_almost_equal(reg.feature_importances_, np.mean(\n            np.abs(reg._boost(1, X, y, sample_weight, rng)), axis=0),\n                                    decimal=4)\n", "    def test_invalid_base_estimator(self, estimator):\n        with pytest.raises(TypeError):\n            AdaBoostClassifier(base_estimator=estimator).fit(\n                X, y_class)\n", "    def test_adaboost_regressor_validation(self):\n        # Test validation of base estimator with different input types\n        \n        # 1.  Valid default\n        reg = AdaBoostRegressor()\n\n        # 2.  Valid string\n        reg2 = AdaBoostRegressor(base_estimator=\"DecisionTreeRegressor\")\n\n        # 3.  Valid instance\n        estimator = DecisionTreeRegressor()\n        reg3 = AdaBoostRegressor(base_estimator=estimator)\n\n        # 4.  Invalid (not a valid estimator)\n        with pytest.raises(TypeError):\n            AdaBoostRegressor(base_estimator=123)\n"], "sample_46": ["    def test_uuid_lookup_with_native_uuid_field(self):\n        with connection.schema_editor() as editor:\n            editor.create_model(\n                UUIDModel,\n                fields=[\n                    models.UUIDField(db_type='uuid', primary_key=True),\n                    models.CharField(max_length=100),\n                ],\n            )\n        uuid_model_instance = UUIDModel.objects.create(\n            uuid=uuid.uuid4(),\n            name='My UUID Model',\n        )\n        self.assertEqual(\n            UUIDModel.objects.get(uuid=uuid_model_instance.uuid).uuid,\n            uuid_model_instance.uuid,\n        )\n", "    def test_uuid_iexact(self):\n        instance = UUIDModel.objects.create(\n            uuid_field=uuid.uuid4()\n        )\n        with self.subTest(instance=instance):\n            self.assertEqual(\n                UUIDModel.objects.filter(uuid_field__iexact=instance.uuid_field).count(),\n                1,\n            )\n\n            # Test with a different UUID\n            self.assertEqual(\n                UUIDModel.objects.filter(uuid_field__iexact=uuid.uuid4()).count(),\n                0,\n            )\n\n", "    def test_uuid_exact_with_hyphens(self):\n        obj = UUIDModel.objects.create(uuid_field=uuid.uuid4())\n        with self.subTest(\"Exact match\"):\n            self.assertEqual(\n                UUIDModel.objects.filter(uuid_field__iexact=str(obj.uuid_field)).count(),\n                1,\n            )\n        with self.subTest(\"Exact match with hyphens\"):\n            self.assertEqual(\n                UUIDModel.objects.filter(uuid_field__iexact=str(obj.uuid_field)).count(),\n                1,\n            )\n\n\n", "    def test_uuid_iexact(self):\n        uuid1 = uuid.uuid4()\n        uuid2 = uuid.uuid4()\n\n        instance = UUIDModel.objects.create(uuid_field=uuid1)\n        with self.assertRaises(exceptions.ObjectDoesNotExist, msg=f\"Object with UUID {uuid2} not found\"):\n            UUIDModel.objects.filter(uuid_field__iexact=uuid2).get()\n\n        instance = UUIDModel.objects.get(pk=instance.pk)\n        self.assertEqual(instance.uuid_field, uuid1)\n        self.assertEqual(UUIDModel.objects.filter(uuid_field__iexact=uuid1).count(), 1)\n", "    def test_uuid_lookups_with_whitespace(self):\n        # Test that UUID lookups handle whitespace correctly\n        uuids = [\n            uuid.uuid4(),\n            uuid.uuid4(),\n            uuid.uuid4(),\n            uuid.uuid4(),\n            \"a-b-c-d-e-f-g-h-i-j-k-l-m-n-o\",\n        ]\n        # Insert and fetch models with various UUID strings\n        for uuid_str in uuids:\n            model = UUIDModel(uuid=uuid_str)\n            model.save()\n\n        # Test exact lookups with whitespace\n        result = UUIDModel.objects.get(uuid=\"a-b-c-d-e-f-g-h-i-j-k-l-m-n-o\")\n        self.assertEqual(result.uuid, \"a-b-c-d-e-f-g-h-i-j-k-l-m-n-o\")\n\n        # Test case-insensitive exact lookups\n        result = UUIDModel.objects.get(uuid__iexact=\"a-b-c-d-e-f-g-h-i-j-k-l-m-n-o\")\n        self.assertEqual(result.uuid, \"a-b-c-d-e-f-g-h-i-j-k-l-m-n-o\")\n\n        # Test contains lookups with whitespace\n        result = UUIDModel.objects.filter(uuid__icontains=\"a-b-c\").all()\n        self.assertEqual(len(result), 1)\n\n        # Test startswith and endswith lookups with whitespace\n        result = UUIDModel.objects.filter(uuid__istartswith=\"a-b\").all()\n        self.assertEqual(len(result), 1)\n        result = UUIDModel.objects.filter(uuid__iendswith=\"n-o\").all()\n        self.assertEqual(len(result), 1)\n\n\n"], "sample_935": ["    def test_template_parameters(self) -> None:\n        check(\"template<T>\", \"template<T>\", {\"1\": \"template<T>\"})\n        check(\"template<T, typename U>\", \"template<T, typename U>\",\n              {\"1\": \"template<T, typename U>\"})\n        check(\"template<template<typename V> class Base>\",\n              \"template<template<typename V> class Base>\",\n              {\"1\": \"template<template<typename V> class Base>\"})\n\n\n", "    def check_template(name, input, idDict, output, key=None, asTextOutput=None):\n        if output is None:\n            output = input\n        # First, check without template arguments\n        _check(name, input, idDict, output, key, asTextOutput)\n\n        template_args = ' <T>'\n        # Second, check with template arguments\n        _check(name, input + template_args, idDict, output + template_args, key,\n               asTextOutput + template_args if asTextOutput is not None else None)\n\n\n\n", "compilation error", "def check_template_param(name, input, idDict, output, key=None, asTextOutput=None):\n    if key is None:\n        key = name\n    key += ' '\n    if name in ('function', 'member'):\n        inputActual = input\n        outputAst = output\n        outputAsText = output\n    else:\n        inputActual = input.format(key='')\n        outputAst = output.format(key='')\n        outputAsText = output.format(key=key)\n    # first a simple check of the AST\n    ast = parse(name, inputActual)\n    res = str(ast)\n    if res != outputAst:\n        print(\"\")\n        print(\"Input:    \", input)\n        print(\"Result:   \", res)\n        print(\"Expected: \", outputAst)\n        raise DefinitionError(\"\")\n    rootSymbol = Symbol(None, None, None, None, None, None)\n    symbol = rootSymbol.add_declaration(ast, docname=\"TestDoc\")\n    parentNode = addnodes.desc()\n    signode = addnodes.desc_signature(input, '')\n    parentNode += signode\n    ast.describe_signature(signode, 'lastIsName', symbol, options={})\n    resAsText = parentNode.astext()\n    if resAsText != outputAsText:\n        print(\"\")\n        print(\"Input:    \", input)\n        print(\"astext(): \", resAsText)\n        print(\"Expected: \", outputAsText)\n        raise DefinitionError(\"\")\n\n    idExpected = [None]\n    for i in range(1, _max_id + 1):\n        if i in idDict:\n            idExpected.append(idDict[i])\n        else:\n            idExpected.append(idExpected[i - 1])\n    idActual = [None]\n    for i in range(1, _max_id + 1):\n        try:\n            id = ast.get_id(version=i)\n            assert id is not None\n            idActual.append(id[len(_id_prefix[i]):])\n        except NoOldIdError:\n            idActual.append(None)\n\n    res = [True]\n    for i in range(", "    def check_namespace(name, input, idDict):\n        ast = parse(name, input)\n        symbol = Symbol(None, None, None, None, None, None)\n        symbol.add_declaration(ast, docname=\"TestDoc\")\n\n        parentNode = addnodes.desc()\n        signode = addnodes.desc_signature(input, '')\n        parentNode += signode\n        ast.describe_signature(signode, 'lastIsName', symbol, options={})\n        res = parentNode.astext()\n\n        # Check that the namespace is correctly represented in the output\n        expected = f\"Namespace: {input}\"\n        assert res.startswith(expected), f\"Expected: {expected}, Got: {res}\"\n\n\n"], "sample_760": ["    def test_check_scoring_with_estimators(self, estimator_type):\n        if estimator_type == 'regr':\n            estimator = ESTIMATORS['r2']\n            scoring = 'r2'\n            expected_scorer = SCORERS['r2']\n        elif estimator_type == 'clf':\n            estimator = ESTIMATORS['accuracy']\n            scoring = 'accuracy'\n            expected_scorer = SCORERS['accuracy']\n        elif estimator_type == 'clf_cluster':\n            estimator = ESTIMATORS['adjusted_rand_score']\n            scoring = 'adjusted_rand_score'\n            expected_scorer = SCORERS['adjusted_rand_score']\n        elif estimator_type == 'ml_clf_mullabel':\n            estimator = ESTIMATORS['precision_samples']\n            scoring = 'precision_samples'\n            expected_scorer = SCORERS['precision_samples']\n        else:\n            raise ValueError\n\n        scorer = check_scoring(estimator, scoring)\n        assert scorer == expected_scorer\n\n\n\n", "    def test_check_scoring_with_custom_scorer(self):\n        class CustomScorer(object):\n                return np.mean(y)\n\n        with tempfile.TemporaryDirectory() as tmpdir:\n            filename = os.path.join(tmpdir, 'scorer.py')\n            with open(filename, 'w') as f:\n                f.write(\n                    f\"\"\"", "    def test_make_scorer_with_kwargs(self):\n        dummy_scorer = make_scorer(lambda y, y_pred: np.mean(y_pred),\n                                    greater_is_better=True,\n                                    average='micro')\n        with ignore_warnings() as w:\n            assert_equal(len(w), 0)\n            scorer = dummy_scorer\n        assert_true(isinstance(scorer, _PredictScorer))\n        y = np.array([0, 1, 0, 1])\n        y_pred = np.array([0.2, 0.9, 0.1, 0.8])\n        score = scorer(EstimatorWithFitAndPredict(),\n                       X_mm, y)\n        assert_almost_equal(score, np.mean(y_pred))\n\n", "    def test_check_scoring_with_string_scorer(self):\n        estimator = EstimatorWithFit(\n        )\n        with ignore_warnings():\n            scorer = check_scoring(estimator, scoring='accuracy')\n        assert callable(scorer)\n        assert scorer(estimator, X_mm, y_mm) == accuracy_score(y_mm, estimator.predict(X_mm))\n", "    def test_check_scoring_with_invalid_types(self):\n        with assert_raises(ValueError,\n                          match=\"scoring value should be a callable, string\"):\n            check_scoring(EstimatorWithoutFit(), scoring=6.0)\n        with assert_raises(ValueError,\n                          match=\"scoring value should be a callable, string\"):\n            check_scoring(EstimatorWithoutFit(), scoring=None)\n        with assert_raises(ValueError,\n                          match=\"scoring value should be a callable, string\"):\n            check_scoring(EstimatorWithoutFit(), scoring={'key': 6.0})\n\n\n\n"], "sample_196": ["    def test_integer_field_range(self):\n        self.assertEqual(self.ops.integer_field_range('AutoField'), (-2147483648, 2147483647))\n        self.assertEqual(self.ops.integer_field_range('SmallAutoField'), (-32768, 32767))\n        self.assertEqual(self.ops.integer_field_range('IntegerField'), (-2147483648, 2147483647))\n        self.assertEqual(self.ops.integer_field_range('BigAutoField'), (-9223372036854775808, 9223372036854775807))\n        self.assertEqual(self.ops.integer_field_range('PositiveIntegerField'), (0, 2147483647))\n        self.assertEqual(self.ops.integer_field_range('PositiveSmallIntegerField'), (0, 32767))\n\n\n\n", "    def test_explain_query_prefix_unsupported(self):\n        with self.assertRaises(NotSupportedError) as cm:\n            self.ops.explain_query_prefix()\n        self.assertEqual(str(cm.exception), \"This backend does not support explaining query execution.\")\n", "    def test_integer_field_range(self):\n        self.assertEqual(self.ops.integer_field_range('IntegerField'), (-2147483648, 2147483647))\n        self.assertEqual(self.ops.integer_field_range('SmallIntegerField'), (-32768, 32767))\n        self.assertEqual(self.ops.integer_field_range('BigIntegerField'), (-9223372036854775808, 9223372036854775807))\n        self.assertEqual(self.ops.integer_field_range('PositiveIntegerField'), (0, 2147483647))\n        self.assertEqual(self.ops.integer_field_range('PositiveSmallIntegerField'), (0, 32767))\n        with self.assertRaisesMessage(NotSupportedError, self.may_require_msg % \"integer_field_range\"):\n            self.ops.integer_field_range('MyCustomField')\n\n\n\n", "    def test_integer_field_range(self):\n        self.assertEqual(self.ops.integer_field_range('SmallAutoField'), (-32768, 32767))\n        self.assertEqual(self.ops.integer_field_range('AutoField'), (-2147483648, 2147483647))\n        self.assertEqual(self.ops.integer_field_range('BigAutoField'), (-9223372036854775808, 9223372036854775807))\n        self.assertEqual(self.ops.integer_field_range('PositiveAutoField'), (0, 9223372036854775807))\n        self.assertEqual(self.ops.integer_field_range('PositiveSmallIntegerField'), (0, 32767))\n        self.assertEqual(self.ops.integer_field_range('IntegerField'), (-2147483648, 2147483647))\n\n\n\n", "    def test_window_frame_rows_start_end_unsupported(self):\n        with self.assertRaises(NotSupportedError) as cm:\n            self.ops.window_frame_rows_start_end(1, 2)\n        self.assertEqual(\n            cm.exception.args[0],\n            'This backend does not support window expressions.',\n        )\n"], "sample_90": ["    def test_custom_error_messages(self):\n        form = CustomErrorMessageForm(data={'name1': 'invalid'})\n        self.assert_errors(form, {'name1': ['Form custom error message.']})\n", "        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            for field_name, field in self.fields.items():\n                if field_name == 'name1':\n                    field.error_messages = {'invalid': 'My custom error message for name1.'}\n        ", "    def test_custom_error_message(self):\n        form = CustomErrorMessageForm(data={'name1': ''})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.errors['name1'], ['Form custom error message.'])\n\n", "    def test_media(self):\n        form = ModelFormWithMedia()\n        self.assertEqual(form.media.js, ('/some/form/javascript',))\n        self.assertEqual(form.media.css, {'all': ('/some/form/css',)})\n\n\n", "    def test_custom_error_message(self):\n        form = CustomErrorMessageForm(data={'name1': 'invalid'})\n        self.assertEqual(form.errors['name1'], ['Form custom error message.'])\n\n"], "sample_152": ["    def test_cascade_with_dependencies_no_transaction(self):\n        Child.objects.create(parent=P.objects.create())\n        with connection.cursor() as cursor:\n            cursor.execute(\"SET foreign_key_checks = 0\")\n            try:\n                Parent.objects.all().delete()\n            finally:\n                cursor.execute(\"SET foreign_key_checks = 1\")\n        self.assertEqual(P.objects.count(), 0)\n        self.assertEqual(Child.objects.count(), 0)\n\n\n", "    def test_restricted_error(self):\n        r = R.objects.create(a=A.objects.create())\n        b = B.objects.create(r=r)\n        with self.assertRaises(RestrictedError) as cm:\n            R.objects.filter(id=r.id).delete()\n        self.assertEqual(str(cm.exception), \"Cannot delete some instances of model 'R' because they are referenced through restricted foreign keys: 'R.a': [<R object at 0x...>, <R object at 0x...>]\")\n        R.objects.filter(id=r.id).update(a=None)\n\n\n", "    def test_restrict_protected_objects(self):\n        a1 = create_a()\n        a2 = create_a()\n        p1 = P.objects.create(a=a1)\n        p2 = P.objects.create(a=a2)\n        r1 = R.objects.create(p=p1)\n        r2 = R.objects.create(p=p2)\n\n        with self.assertRaises(RestrictedError):\n            P.objects.filter(pk__in=[p1.pk, p2.pk]).delete()\n\n\n\n", "    def test_cascade_delete_simple(self):\n        collector = Collector(using=connection.db)\n        collector.collect(self.a1, nullable=True)\n        deleted, counts = collector.delete()\n        self.assertEqual(deleted, 3)\n        self.assertEqual(counts, {'a': 1, 'b': 3})\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT id FROM a WHERE id = %s\", [self.a1.id])\n            self.assert_query_result(cursor, [])\n\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT id FROM b WHERE id = %s\", [self.b1.id])\n            self.assert_query_result(cursor, [])\n            cursor.execute(\"SELECT id FROM b WHERE id = %s\", [self.b2.id])\n            self.assert_query_result(cursor, [])\n            cursor.execute(\"SELECT id FROM b WHERE id = %s\", [self.b3.id])\n            self.assert_query_result(cursor, [])\n\n        self.assertNotEqual(R.objects.count(), 0)\n", "    def test_collect_with_nullable_fk(self):\n        a = create_a(name='a')\n        b1 = B1.objects.create(a=a)\n        b2 = B2.objects.create(a=a)\n        a.delete()\n        self.assertEqual(B1.objects.count(), 0)\n        self.assertEqual(B2.objects.count(), 0)\n\n\n"], "sample_235": ["    def test_on_commit_with_error(self):\n        \"\"\"\n        Tests that on_commit() callbacks are skipped if an error occurs\n        within the transaction.\n        \"\"\"\n        with self.assertRaises(ForcedError):\n            self.do(1)\n            self.do(2)\n            self.do(3)\n        self.assertNotified([])\n", "    def test_on_commit_with_transaction(self):\n        \"\"\"Test on_commit() within a transaction.\"\"\"\n        with transaction.atomic():\n            self.do(1)\n            self.do(2)\n        self.assertDone([1, 2])\n", "    def test_on_commit_in_atomic_block(self):\n        with transaction.atomic():\n            self.do(1)\n            self.do(2)\n        self.assertNotified([1, 2])\n", "    def test_on_commit_atomic_block(self):\n        with transaction.atomic():\n            self.do(1)\n            self.do(2)\n\n        self.assertDone([1, 2])\n", "    def test_on_commit_with_transaction_errors(self):\n        \"\"\"\n        Tests on_commit() behavior in case of transaction errors.\n        \"\"\"\n        with transaction.atomic():\n            self.do(1)\n            self.do(2)\n            raise ForcedError()\n        try:\n            with transaction.atomic():\n                self.do(3)\n                self.do(4)\n        except db.DatabaseError as e:\n            self.assertEqual(str(e), \"ForcedError()\")\n            self.assertNotified([1, 2, 3, 4])\n"], "sample_950": ["    def test_filter_meta_fields(app: Mock, domain: Mock, content: Mock) -> None:\n        app.config = Mock(modindex_common_prefix=[])\n        domain.env = Mock()\n        domain.env.config = app.config\n        content = [nodes.field_list(\n            [nodes.field(\n                nodes.field_name(\"meta\"), nodes.field_body(nodes.paragraph(\n                    \"This is a meta field.\")))])\n        ]\n        filter_meta_fields(app, domain, 'py', content)\n        assert len(content[0].children) == 0\n\n", "    def test_parse_annotation(self):\n        assert _parse_annotation('int') == 'int'\n        assert _parse_annotation('Optional[str]') == 'Optional[str]'\n        assert _parse_annotation('List[int]') == 'List[int]'\n        assert _parse_annotation('typing.List[int]') == 'typing.List[int]'\n        assert _parse_annotation('None') == 'NoneType'\n\n\n\n", "    def test_parse_annotation(self):\n        domain = PythonDomain(None)\n        annotation = domain.parse_annotation('int')\n        assert annotation[0].astext() == 'int'\n\n        annotation = domain.parse_annotation('Optional[str]')\n        assert annotation[0].astext() == 'Optional[str]'\n\n        annotation = domain.parse_annotation('Dict[int, str]')\n        assert annotation[0].astext() == 'Dict[int, str]'\n\n        annotation = domain.parse_annotation('List[int]')\n        assert annotation[0].astext() == 'List[int]'\n        \n        annotation = domain.parse_annotation('from typing import List')\n        assert annotation == []\n", "    def test_parse_annotation(self):\n        self.assertEqual(_parse_annotation('int'), ('int', None))\n        self.assertEqual(_parse_annotation('int | str'), ('int', ' | '))\n        self.assertEqual(_parse_annotation('int if True else str'), ('int if True else str', None))\n        self.assertEqual(_parse_annotation('list[int]'), ('list[int]', None))\n        self.assertEqual(_parse_annotation('typing.List[int]'), ('typing.List[int]', None))\n        self.assertEqual(_parse_annotation('int, str'), ('int, str', None))\n        self.assertEqual(_parse_annotation('Optional[str]'), ('Optional[str]', None))\n\n", "    def test_py_sig_re_complex_args(self):\n        sig = \"myfunction(a, b=1, *args, **kwargs)\"\n        parsed_sig = parse(sig)\n        expected_sig = \"myfunction(a, b=1, *args, **kwargs)\"\n        assert parsed_sig == expected_sig\n"], "sample_26": ["    def test_section_indexing(self, start_row, start_col):\n        \"\"\"\n        Test slicing of image HDU sections with a given start point.\n        \"\"\"\n        data = np.arange(100).reshape((10, 10))\n        hdu = ImageHDU(data=data)\n\n        section = hdu.section[(start_row, start_col):]\n        expected = data[start_row : start_row + 10, start_col : start_col + 10]\n        assert_equal(section, expected)\n\n\n\n", "    def test_image_section_slicing(self, start1, stop1, start2, stop2):\n        data = np.random.rand(11, 12, 13)\n        hdu = ImageHDU(data=data)\n        section = hdu.section\n        for dim in range(len(data.shape)):\n            if start1[dim] == stop1[dim] or start2[dim] == stop2[dim]:\n                continue\n            sliced = section[\n                (slice(start1[0], stop1[0]), slice(start1[1], stop1[1]), slice(start1[2], stop1[2]))\n            ]\n\n            # Verify that slicing works as expected.\n            exp_data = data[\n                start1[0]:stop1[0], start1[1]:stop1[1], start1[2]:stop1[2]\n            ]\n            assert_equal(sliced.shape, exp_data.shape)\n            assert_equal(sliced, exp_data)\n\n            sliced = section[\n                (slice(start2[0], stop2[0]), slice(start2[1], stop2[1]), slice(start2[2], stop2[2]))\n            ]\n\n            # Verify that slicing works as expected.\n            exp_data = data[\n                start2[0]:stop2[0], start2[1]:stop2[1], start2[2]:stop2[2]\n            ]\n            assert_equal(sliced.shape, exp_data.shape)\n            assert_equal(sliced, exp_data)\n\n", "    def test_set_data_with_integer_array(self, indices):\n        header = fits.Header()\n        header[\"NAXIS\"] = 2\n        header[\"NAXIS1\"] = 5\n        header[\"NAXIS2\"] = 5\n        header[\"BITPIX\"] = 16\n\n        data = np.zeros(5, dtype=np.int16)\n        data[indices] = 1\n\n        hdu = fits.ImageHDU(data=data, header=header)\n\n        hdu.data[indices] = 2\n\n        self.assertTrue(np.allclose(hdu.data, np.zeros(5, dtype=np.int16)))\n\n\n\n", "    def test_compressed_image(self, compression):\n        if compression == \"NONE\":\n            return\n\n        # Create a test image with compression\n        data = np.arange(100).reshape((10, 10))\n        hdu = fits.PrimaryHDU(data, header=fits.Header())\n        hdu.header[\"COMPRESSION\"] = compression\n\n        # Write the HDU to a file\n        with fits.File(self.temp_filename(\".fits\"), \"w\") as hdul:\n            hdul.writeto(hdu)\n\n        # Read the HDU from the file\n        with fits.open(self.temp_filename(\".fits\")) as hdul:\n            hdu_out = hdul[0]\n\n        assert hdu_out.header[\"COMPRESSION\"] == compression\n\n        # Check if the data is the same\n        assert_equal(hdu_out.data, data)\n", "    def test_image_compressed(self, dtype, compression):\n        if compression == \"NONE\":\n            return\n\n        hdu = fits.PrimaryHDU(np.random.rand(100, 100).astype(dtype))\n        hdu.header[\"COMPRESSION\"] = compression\n        hdu.header[\"BITPIX\"] = 8 if dtype == \"int8\" else 16 if dtype == \"int16\" else 32 if dtype == \"int32\" else 64\n\n        if compression == \"ZLIB\" or compression == \"GZIP\":\n            hdu.header[\"ZCMP\"] = 0\n            hdu.header[\"ZPCOUNT\"] = 0\n            hdu.header[\"ZLEVEL\"] = 9\n\n        if compression == \"RICE\":\n            hdu.header[\"RICE_PARMS\"] = \"8 4 1\"\n\n        with BytesIO() as f:\n            hdu.writeto(f)\n            f.seek(0)\n            return_hdu = fits.open(f)\n\n        assert_equal(return_hdu[0].data, hdu.data)\n        assert_equal(return_hdu[0].header[\"COMPRESSION\"], compression)\n\n        return_hdu.close()\n\n\n"], "sample_771": ["    def test_power_transform_sparse(self):\n        # Test power transform with sparse matrix\n        n_features = 5\n        n_samples = 100\n        data = np.random.rand(n_samples, n_features)\n        sparse_data = sparse.csr_matrix(data)\n        pt = PowerTransformer()\n        trans_dense = pt.fit_transform(data)\n        trans_sparse = pt.fit_transform(sparse_data)\n\n        assert_allclose_dense_sparse(trans_sparse, trans_dense)\n", "def test_power_transform_warn():\n    X = np.array([[1, 2], [3, 2], [4, 5]])\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"always\")\n        result = power_transform(X)\n        assert_warns_message(\n            \"default value of 'method' will change\",\n            UserWarning,\n            message=\"default value of 'method' will change\",\n        )\n\n    # Make sure the transform was applied\n    assert_array_almost_equal(result, power_transform(X, method='yeo-johnson'))\n    \n", "def test_power_transform_default(method):\n    X = np.random.rand(100, 5)\n    transformed_X = power_transform(X, method=method, standardize=True)\n    # Check if the mean and variance are approximately 0 and 1\n    assert_allclose(np.mean(transformed_X, axis=0), 0)\n    assert_allclose(np.var(transformed_X, axis=0), 1)\n\n", "    def test_quantile_transform_sparse(self):\n        # Test the quantile_transform function with sparse matrices\n        n_samples = 50\n        n_features = 10\n        data = np.random.randn(n_samples, n_features)\n        sparse_data = sparse.csr_matrix(data)\n\n        qt = QuantileTransformer(n_quantiles=100)\n        transformed_dense = qt.fit_transform(data)\n        transformed_sparse = qt.fit_transform(sparse_data)\n\n        assert_allclose_dense_sparse(transformed_dense,\n                                     transformed_sparse)\n", "    def test_quantile_transform_sparse(self):\n        n_samples, n_features = 100, 100\n        X_sparse = sparse.random(n_samples, n_features, density=0.1,\n                                 random_state=0)\n\n        # Test with different quantile values\n        for quantiles in [10, 50, 100]:\n            qt = QuantileTransformer(n_quantiles=quantiles,\n                                     random_state=0)\n            X_trans = qt.fit_transform(X_sparse)\n            assert_allclose_dense_sparse(qt.transform(X_sparse), X_trans)\n            \n"], "sample_568": ["def test_poly3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    verts = [\n        [(0, 0, 0), (1, 0, 0), (1, 1, 0), (0, 1, 0), (0, 0, 0)],\n        [(0, 0, 0), (0, 0, 1), (0, 1, 1), (0, 1, 0), (0, 0, 0)],\n        [(0, 0, 0), (1, 0, 0), (1, 0, 1), (0, 0, 1), (0, 0, ", "    def test_rotate_axes():\n        xs, ys, zs = np.random.rand(5, 3)\n        for zdir in ['x', '-x', 'y', '-y', 'z', '-z']:\n            rotated = rotate_axes(xs, ys, zs, zdir)\n            if zdir in ('x', '-x'):\n                assert np.allclose(rotated[1], zs)\n                assert np.allclose(rotated[0], xs)\n            elif zdir in ('y', '-y'):\n                assert np.allclose(rotated[2], zs)\n                assert np.allclose(rotated[0], ys)\n\n\n\n", "    def test_poly3d_collection_projection(test_data):\n        fig, ax = plt.subplots(subplot_kw={'projection': '3d'})\n        for verts, zs in test_data:\n            for z in zs:\n                poly = Poly3DCollection([verts], facecolors='r', edgecolors='b')\n                poly.set_zsort('average')\n                poly.set_3d_properties(z)\n                poly.set_verts(verts)\n                ax.add_collection3d(poly)\n\n        ax.set_xlim(-1, 1)\n        ax.set_ylim(-1, 1)\n        ax.set_zlim(-1, 1)\n        plt.show()\n", "    def test_poly_collection_2d_to_3d():\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection='3d')\n        verts = [[[0, 0, 0], [1, 0, 0], [1, 1, 0], [0, 1, 0],\n                 [0, 0, 1], [1, 0, 1], [1, 1, 1], [0, 1, 1]],\n                [[0.5, 0.5, 0], [1.5, 0.5, 0], [1.5, 1.5, 0],\n                 [0.5, 1.5, 0]]]\n        col = PolyCollection(verts, facecolors='red', edgecolors='blue')\n        ax.add_collection3d(col)\n        poly_collection_2d_to_3d(col, zs=0.5, zdir='z')\n        plt.show()\n\n\n\n", "    def test_poly3dcollection_zs():\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection='3d')\n\n        #  Test with different z locations (0, 1, -1)\n        zs = [0, 1, -1]\n        zdirs = ['z', 'z', 'z']\n\n        for i, z in enumerate(zs):\n            col = PolyCollection([[[0, 0, z], [1, 0, z], [1, 1, z], [0, 1, z]]],\n                               facecolors='red', edgecolors='black')\n            poly_collection_2d_to_3d(col, z, zdirs[i])\n            ax.add_collection3d(col)\n\n        ax.set_xlim(0, 1)\n        ax.set_ylim(0, 1)\n        ax.set_zlim(-1, 2)\n\n        plt.show()\n\n"], "sample_637": ["    def test_pylint_disable_fixme_pragma(self) -> None:\n        source = \"\"\"\n        # pylint: disable=fixme\n        FIXME: This is a comment\n        \"\"\"\n        self.check(source)\n        self.assertNoMessages()\n", "    def test_fixme_with_disable_pragma(self):\n        code = \"\"\"", "    def test_encoding_declaration(self) -> None:\n        \"\"\"Test that encoding declarations are correctly handled.\"\"\"\n        source = \"\"\"\\", "    def test_encoding_issue(self):\n        \"\"\"Test decoding issues.\"\"\"\n        with self.assert_messages(\n            [\n                \"W0511\",\n                {\"line\": 1, \"args\": \"Cannot decode using encoding 'ascii', bad encoding\"},\n            ]\n        ):\n            self.run_pylint(\n                _tokenize_str(\"\"\"\n                # coding: utf-8\n                print(\"\\n\u4f60\u597d\")\n                \"\"\"),\n                set_config={\"linter.encoding\": \"ascii\"},\n            )\n\n\n\n", "    def test_issue_1990_unicode_decode_error(self):\n        \"\"\"Test encoding issues with UnicodeDecodeError.\"\"\"\n        source = \"\"\""], "sample_644": ["    def test_wrong_import_order(self) -> None:\n        data = f\"{REGR_DATA}/imports/wrong-import-order.py\"\n        lines = [\n            \"  import random  # WRONG\",\n            \"  # WRONG from import\",\n            \"  from .module import MyClass1\",\n            \"  import os\",\n        ]\n        result = Run([data], messages=True)\n        self.assertEqual(len(result.messages), 3)\n        for i, message in enumerate(result.messages):\n            args = message.args\n            line = i + 1\n            if args[0] == \"wrong-import-order\":\n                self.assertIn(f'should be before \"{lines[line-1]}\"', args[1])\n            elif args[0] == \"consider-using-from-import\":\n                self.assertIn(lines[line - 1].split(\" \")[:-1], args[1])\n    ", "    def test_check_preferred_module_alias(self) -> None:\n        source = \"\"\"", "    def test_wildcard_import_allowed(self) -> None:\n        code = \"\"\"\n        import sys\n\n            for name in sys.__all__:\n                print(name)\n        \"\"\"\n        self.assertMessages(\n            code,\n            [\n                (\"RP0402\", \"check_wildcard_imports\", 2, 10, \n                 \"Wildcard import '*' is allowed because sys.__all__ is defined\"),\n            ],\n        )\n", "    def test_wildcard_imports_allowed(self) -> None:\n        pylint_source_code = \"\"\"", "    def test_import_as_rename_alias_same_name(self) -> None:\n        \"\"\"Test import as rename with alias same name as import.\"\"\"\n        code = \"\"\"\n        import mymodule as mymodule\n        \"\"\"\n        self.run_test(\n            source=code,\n            messages=[\n                (\"useless-import-alias\", 0, 1, \"  import mymodule as mymodule\",),\n            ],\n        )\n\n\n"], "sample_741": ["    def test_refit_str_scorer(self):\n        scorer_names = ['accuracy', 'roc_auc']\n        for scorer_name in scorer_names:\n            # Check refit works with string scorer name\n            random_search = RandomizedSearchCV(\n                SVC(), {'kernel': ['rbf'], 'C': [1, 10]}, scoring=scorer_name,\n                n_iter=10, cv=5, refit=True)\n            random_search.fit(X, y)\n            best_estimator = random_search.best_estimator_\n            best_score = random_search.best_score_\n            assert_equal(random_search.refit, scorer_name)\n            assert_not_equal(best_estimator, None)\n            assert_not_equal(best_score, None)\n", "    def test_parameter_grid_repr(self):\n        grid = ParameterGrid({'a': [1, 2], 'b': [3, 4]})\n        self.assertIn(str(grid), eval(repr(grid)))\n", "    def test_parameter_grid_equality(self):\n        grid1 = ParameterGrid({'a': [1, 2], 'b': [3, 4]})\n        grid2 = ParameterGrid({'a': [1, 2], 'b': [3, 4]})\n        assert_equal(grid1, grid2)\n\n        grid3 = ParameterGrid({'a': [1, 2], 'b': [3, 4]})\n        assert_equal(grid1, grid3)\n\n        grid4 = ParameterGrid({'a': [1, 2], 'b': [5, 6]})\n        assert_not_equal(grid1, grid4)\n\n        grid5 = ParameterGrid({'a': [1, 3], 'b': [3, 4]})\n        assert_not_equal(grid1, grid5)\n\n\n", "    def test_grid_search_fit_params(self):\n        param_grid = {\"C\": [0.1, 1, 10], \"kernel\": ['linear', 'rbf']}\n\n        # Test that fit_params are passed to the estimator during fitting\n        # by using a mock estimator that logs its fit parameters\n        class MockEstimator(BaseEstimator):\n                self.foo_param = foo_param\n\n                self.fit_params = fit_params\n                return self\n\n        mock_estimator = MockEstimator()\n        grid_search = GridSearchCV(mock_estimator,\n                                   param_grid,\n                                   fit_params={'foo_param': 123})\n\n        grid_search.fit(X, y)\n\n        assert_equal(mock_estimator.fit_params, {'foo_param': 123})\n\n", "    def test_grid_search_with_fit_params(self):\n        param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n        estimator = SVC()\n        fit_params = {'l1_ratio': 0.1, 'max_iter': 1000}\n        grid_search = GridSearchCV(estimator, param_grid, fit_params=fit_params)\n        grid_search.fit(X, y)\n        for cv in grid_search.cv_results_:\n            assert_equal(cv.get('params')['l1_ratio'], 0.1)\n            assert_equal(cv.get('params')['max_iter'], 1000)\n"], "sample_358": ["    def test_table_references(self):\n        self.assertFalse(self.reference.references_table('other_table'))\n        self.assertTrue(self.reference.references_table('TABLE'))\n\n\n", "    def test_table_references(self):\n        self.assertTrue(self.reference.references_table('table'))\n        self.assertFalse(self.reference.references_table('other_table'))\n", "    def test_table_references(self):\n        self.assertFalse(self.reference.references_table('other_table'))\n        self.assertTrue(self.reference.references_table('TABLE'))\n\n\n\n", "    def test_rename_table_references(self):\n        self.assertEqual(str(self.reference), 'TABLE')\n        self.reference.rename_table_references('table', 'NEWTABLE')\n        self.assertEqual(str(self.reference), 'NEWTABLE')\n\n\n", "    def test_rename_table_references(self):\n        old_table = 'table'\n        new_table = 'new_table'\n        self.reference.rename_table_references(old_table, new_table)\n        self.assertEqual(self.reference.table, new_table)\n"], "sample_144": ["    def test_order_by_related_field(self):\n        # Test ordering on related fields with multiple levels of related objects.\n        bachelor_parties = MessyBachelorParty.objects.all()\n        # Set the order in the database\n        bachelor_parties.set_get_order(bachelor_parties.get_party_guests)\n        self.assertEqual(\n            list(bachelor_parties),\n            sorted(bachelor_parties, key=lambda x: x.party_guests.first().name)\n        )\n\n        # Check if the order persists across queries\n        bachelor_parties = MessyBachelorParty.objects.all()\n        bachelor_parties.set_get_order(bachelor_parties.get_party_guests)\n        self.assertEqual(\n            list(bachelor_parties),\n            sorted(bachelor_parties, key=lambda x: x.party_guests.first().name)\n        )\n\n", "    def test_ordering(self):\n        # Test ordering on related fields\n        article = ArticleWithAuthor.objects.create(\n            headline=\"Example Article\",\n            author=Author.objects.create(\n                name=\"John Doe\",\n            )\n        )\n        article2 = ArticleWithAuthor.objects.create(\n            headline=\"Another Example Article\",\n            author=Author.objects.create(\n                name=\"Jane Doe\",\n            )\n        )\n        \n        articles = ArticleWithAuthor.objects.filter(\n            author__name__startswith=\"J\"\n        ).order_by('headline')\n        self.assertEqual(list(articles), [article2, article])\n\n        # Check ordering on related fields with multiple levels\n        p = Person.objects.create(\n            name=\"Tom\",\n            address=Address.objects.create(\n                street=\"123 Main\",\n                city=\"Anytown\",\n                state=\"CA\"\n            )\n        )\n        q = Person.objects.create(\n            name=\"Alice\",\n            address=Address.objects.create(\n                street=\"456 Main\",\n                city=\"Anytown\",\n                state=\"CA\"\n            )\n        )\n        \n        people = Person.objects.filter(address__city=\"Anytown\").order_by('name')\n        self.assertEqual(list(people), [p, q])\n\n\n\n", "    def test_related_fields_in_ordering_works_as_expected(self):\n        # This tests the handling of ordering by related fields.\n\n        # Create a few instances of related objects to test ordering.\n        article = ArticleWithAuthor.objects.create(title=\"Test Article\", content=\"Some test content\")\n        author = article.author\n        author2 = Person.objects.create(name=\"Test Author 2\")\n        article2 = ArticleWithAuthor.objects.create(title=\"Another Test Article\", content=\"More test content\", author=author2)\n        \n        # We should be able to order by the related field (author)\n        ordered_articles = ArticleWithAuthor.objects.all().order_by('author__name')\n        self.assertEqual(ordered_articles.count(), 2)\n        self.assertEqual(ordered_articles[0].title, \"Test Article\")\n        self.assertEqual(ordered_articles[1].title, \"Another Test Article\")\n", "    def test_multiple_inheritance_fields(self):\n        # Test that fields from multiple parent models are properly inherited\n        class MultiInherited(Person, Congressman):\n            pass\n\n        multi = MultiInherited(\n            first_name='Homer',\n            last_name='Simpson',\n            age=40,\n            district='10',\n            party='Republican',\n            )\n\n        self.assertEqual(multi.first_name, 'Homer')\n        self.assertEqual(multi.last_name, 'Simpson')\n        self.assertEqual(multi.age, 40)\n        self.assertEqual(multi.district, '10')\n        self.assertEqual(multi.party, 'Republican')\n", "    def test_m2m_field_related_name(self):\n        class PersonWithManyPets(models.Model):\n            name = models.CharField(max_length=100)\n            pets = models.ManyToManyField('Pet', related_name='owners')\n\n        class Pet(models.Model):\n            name = models.CharField(max_length=100)\n\n        person = PersonWithManyPets.objects.create(name=\"Alice\")\n        pet = Pet.objects.create(name=\"Fido\")\n        person.pets.add(pet)\n\n        owned_pets = person.pets.all()\n        self.assertEqual(len(owned_pets), 1)\n        self.assertEqual(owned_pets[0].name, \"Fido\")\n\n\n\n        pet_owners = pet.owners.all()\n        self.assertEqual(len(pet_owners), 1)\n        self.assertEqual(pet_owners[0].name, \"Alice\")\n"], "sample_171": ["    def test_migrate_fake(self):\n        with connections['default'].atomic():\n            self.create_model()\n            call_command('migrate', '--database', 'default', '--fake')\n            self.assertEqual(MigrationRecorder.objects.count(), 1)\n            self.assertEqual(connection.get_table_list(truncate_name=True), [])\n", "    def test_fake_migration(self):\n        Migrator = importlib.import_module('django.core.management.commands.migrate').Command\n        with self.settings(DATABASE_ROUTERS=[TestRouter()]):\n            with connections['default'].cursor() as cursor:\n                # Create a table manually to trigger the fake migration check\n                cursor.execute(\"CREATE TABLE fake_table (id INT PRIMARY KEY)\")\n\n            # Migrate to a fake state\n            with mock.patch.object(Migrator, 'migration_progress_callback') as mock_progress:\n                call_command('migrate', options={'fake': True})\n            # Assert the table is unchanged\n            self.assertEqual(connections['default'].introspection.table_names(cursor), ['fake_table'])\n            # Make sure no migrations were actually applied\n            self.assertTrue(mock_progress.called)\n            mock_progress.assert_any_call(\"apply_success\", migration=None, fake=True)\n\n            # Create a new model without migrations to trigger the 'run_syncdb' branch\n            class TestModel(models.Model):\n                value = models.CharField(max_length=10)\n\n            TestModel.objects.create(value='test')\n            # Migrate again to ensure the new model is created without fake flag\n            call_command('migrate', options={'fake': False}) \n            self.assertTrue(TestRouter().db_for_write('TestModel') == 'other')\n\n\n\n\n", "    def test_migrate_fake(self):\n        'Tests the --fake option of the migrate command.'\n        call_command('migrate', '-f', 'migrations', database='default')\n        with connections['default'].cursor() as cursor:\n            cursor.execute(\n                \"SELECT applied FROM django_migrations WHERE app = %s AND name = %s\",\n                ('migrations', '0001_initial'),\n            )\n            applied = cursor.fetchone()[0]\n        self.assertTrue(applied)\n\n        # Run migrate again with the same target. No operations should happen\n        # as it's already applied.\n        #\n        # We can't directly call the command again as it uses a\n        # MigrationRecorder and that would create an inconsistency.\n        # Instead, let's check the history and verify it hasn't changed.\n        with connections['default'].cursor() as cursor:\n            cursor.execute(\n                \"SELECT applied FROM django_migrations WHERE app = %s AND name = %s\",\n                ('migrations', '0001_initial'),\n            )\n            applied = cursor.fetchone()[0]\n        self.assertTrue(applied)\n", "    def test_fake_migration_with_dependencies(self):\n        self.create_test_migrations(\n            \"test_migrations\",\n            [\n                (\n                    \"0001_initial\",\n                    \"apps.migrations.TestMigration.create_initial_table\",\n                ),\n                (\n                    \"0002_dependent\",\n                    \"apps.migrations.TestMigration.create_dependent_table\",\n                ),\n            ],\n            dependencies=[(\"0001_initial\",)],\n        )\n        with self.assertNumQueries(2):\n            call_command(\"migrate\", \"--fake\", app=\"migrations\")\n\n        with self.assertNumQueries(0):\n            call_command(\"migrate\", \"--fake\", app=\"migrations\")\n\n        with self.assertNumQueries(0):\n            call_command(\"migrate\", \"--fake\", app=\"migrations\")\n\n        self.assertEqual(self.get_migration_state(), {\n            \"migrations\": {\"0001_initial\": True, \"0002_dependent\": True},\n        })\n        \n", "    def test_migration_plan_and_fake_migration(self):\n        \"Tests migration plan and fake migration option.\"\n        connection = connections['default']\n        with self.assertNumQueries(0):\n            call_command('migrate', '--plan', '--fake', app_label='migrations')\n        with self.assertNumQueries(0):\n            call_command('migrate', '--fake', app_label='migrations')\n        with connection.constraint_checking_disabled():\n            with self.assertNumQueries(1):\n                call_command('migrate', '--plan', '--fake', app_label='migrations')\n        with connection.constraint_checking_disabled():\n            with self.assertNumQueries(1):\n                call_command('migrate', '--fake', app_label='migrations')\n\n\n\n\n"], "sample_572": ["    def test_kde_cumulative(self, x, y):\n        kde = KDE(cumulative=True)\n        kde_eval = kde(x)\n        cumulative_sum = np.cumsum(np.array(kde_eval))\n        assert_array_almost_equal(kde_eval, cumulative_sum)\n\n\n", "    def test_kde_bw_method(self, x, bw_method):\n        kde = KDE(bw_method=bw_method)\n        density, _ = kde(x)\n        kde_with_no_bw = KDE()\n        density_no_bw = kde_with_no_bw(x)\n\n        # Check if the bandwidth method affects the density estimate\n        assert_array_almost_equal(\n            density, density_no_bw, decimal=4\n        )\n\n\n\n", "    def test_kde_evaluation(self, x, y, weights):\n        kde = KDE()\n        kde.bw_method = \"Silverman\"\n        kde_dens, _ = kde(x)\n\n        y_integrated = self.integrate(kde_dens, x)\n        # Check that the integral of the KDE is close to 1\n        assert_array_almost_equal(y_integrated, 1)\n\n        kde_y = kde(x, weights=weights)\n        # Check that the weighted kde density is also close to the original \n        kde_y_integrated = self.integrate(kde_y, x)\n\n        assert_array_almost_equal(kde_y_integrated, 1)\n", "    def test_kde_cumulative(self, x, y, weights):\n        kde = KDE(cumulative=True)\n        truth = self.integrate(y, x)\n        kde_results = kde(x, weights=weights)\n        assert_array_almost_equal(kde_results, truth)\n", "    def test_kde_bivariate_integration(self, x, y, weights):\n        kde = KDE()\n        grid1, grid2 = kde._define_support_bivariate(x, y, weights)\n        kde.define_support(x, y)\n        density, _ = kde._eval_bivariate(x, y, weights)\n        expected = self.integrate(density.flatten(), grid1.flatten())\n        assert_array_almost_equal(self.integrate(density.flatten(), grid1.flatten()), expected)\n\n"], "sample_791": ["    def test_onehot_legacy_handle_unknown(self, categories, dtype, handle_unknown):\n        enc = OneHotEncoder(categories=categories, dtype=dtype,\n                           handle_unknown=handle_unknown,\n                           n_values='auto')\n\n        X_train = np.array([\n            ['cat1', 'cat2'],\n            ['cat1', 'cat3'],\n            ['cat4', 'cat2'],\n        ])\n        enc.fit(X_train)\n        X_new = np.array([\n            ['cat4', 'cat3'],  \n        ])\n        with pytest.raises(ValueError) as err:\n            enc.transform(X_new)\n        assert \"unknown categorical feature\" in str(err)\n\n", "    def test_legacy_fit_transform_categorical(self):\n        # Test with only categorical features\n        enc = OneHotEncoder(sparse=False, handle_unknown='ignore',\n                           n_values='auto', categories=None)\n        X = np.array([\n            [\"A\", 1, \"C\"],\n            [\"B\", 2, \"D\"],\n            [\"C\", 1, \"A\"],\n            [\"B\", 3, \"C\"],\n        ])\n        enc.fit(X)\n        X_encoded = enc.transform(X)\n\n        expected_output = np.array([\n            [1, 0, 0, 1, 0, 0, 0, 0, 1],\n            [0, 1, 0, 0, 1, 0, 0, 0, 0],\n            [0, 0, 1, 0, 0, 1, 0, 0, 0],\n            [0, 1, 0, 0, 0, 0, 1, 1, 0],\n        ])\n        assert_allclose(toarray(X_encoded), expected_output, atol=1e-6)\n", "    def test_onehotencoder_legacy_fit_transform_with_categories(self):\n        X = np.array([[0, 1], [2, 0], [1, 2]])\n        encoder = OneHotEncoder(categories=[\n            [0, 1, 2],  # Explicit categories\n            [0, 1, 2]\n        ], n_values='auto', sparse=False, handle_unknown='ignore')\n        X_encoded = encoder.fit_transform(X)\n        expected_output = np.array([[1, 0, 0, 0, 1, 0],\n                                     [0, 0, 1, 0, 0, 1],\n                                     [0, 1, 0, 1, 0, 0]])\n        assert_array_equal(X_encoded, expected_output)\n\n\n", "    def test_ordinal_encoder_with_provided_categories(", "    def test_fit_transform_ignore_unknown(self, handle_unknown):\n        enc = OneHotEncoder(handle_unknown=handle_unknown)\n        X = np.array([\n            [1, 2, 3],\n            [1, 4, 3],\n            [2, 1, 4],\n        ])\n        enc.fit(X)\n\n        # Unknown category should lead to an all-zeros row\n        X_test = np.array([[5, 6, 7]])\n        X_encoded = enc.transform(X_test)\n\n        if handle_unknown == 'error':\n            assert_raises(ValueError,\n                          callable=enc.transform,\n                          args=(X_test,))\n        else:\n            assert_allclose(X_encoded[0],\n                            np.zeros(enc.categories_[0].shape[0]))\n"], "sample_799": ["    def test_learning_curve_incremental_fit_estimator(self):\n        est = MockIncrementalImprovingEstimator(n_max_train_sizes=100)\n        X, y, groups = make_classification(n_samples=100, n_features=20, random_state=42)\n        cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n        train_sizes = np.array([10, 20, 30, 40, 50, 60, 70, 80, 90, 100])\n        train_scores, test_scores = learning_curve(\n            est, X, y, cv=cv, train_sizes=train_sizes, n_jobs=-1\n        )\n        assert_array_almost_equal(train_scores[:, 0], np.arange(1, 11))\n        assert_array_almost_equal(test_scores[:, 0], np.arange(1, 11))\n", "    def test_cross_val_score_with_scoring(self):\n        estimator = FailingClassifier()\n        scores = cross_val_score(estimator, X, y, scoring='accuracy', cv=5)\n        assert_array_equal(scores, np.array([np.nan] * 5))\n", "    def test_learning_curve_incremental_fit(self, X, y, groups):\n        X, y, groups = indexable(X, y, groups)\n        estimator = MockIncrementalImprovingEstimator(\n            n_max_train_sizes=X.shape[0])\n        train_sizes = np.linspace(0.1, 1.0, 5)\n        train_sizes_abs = np.array([int(x * len(X)) for x in train_sizes])\n\n        cv = ShuffleSplit(n_splits=5)\n        scores = learning_curve(\n            estimator, X, y, train_sizes=train_sizes_abs, cv=cv,\n            scoring='accuracy', groups=groups,\n            n_jobs=-1)\n\n        # Ensure scores make sense\n        assert len(train_sizes_abs) == len(scores[0])\n        assert len(train_sizes_abs) == len(scores[1])\n        assert all(train_size > 0 for train_size in train_sizes_abs)\n\n\n\n", "    def test_StratifiedKFold(self, n_splits):\n        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n        n_samples = len(y)\n        for train_index, test_index in skf.split(X, y):\n            assert (train_index.shape[0] > 0)\n            assert (test_index.shape[0] > 0)\n            assert (set(train_index).isdisjoint(set(test_index)))\n            assert (len(train_index) + len(test_index) == n_samples)\n\n", "    def test_cross_val_predict_multiclass_error(self):\n        clf = OneVsRestClassifier(LogisticRegression())\n        y_pred = cross_val_predict(clf, X, y2, cv=3)\n        assert y_pred.shape[0] == len(y2)\n\n\n"], "sample_825": ["    def test_pls_regression_transform(self, algorithm):\n        X, y = load_linnerud(return_X_y=True)\n        X = StandardScaler().fit_transform(X)\n        y = StandardScaler().fit_transform(y.reshape(-1, 1))\n        pls = PLSRegression(n_components=2, scale=True, algorithm=algorithm)\n        pls.fit(X, y)\n        X_transformed, y_transformed = pls.transform(X, y)\n        assert X_transformed.shape[1] == 2\n        assert y_transformed.shape[1] == 2\n        \n        # Check that the dimensionality reduction is consistent across different\n        # algorithms\n        if algorithm == \"nipals\":\n            pls_nipals = PLSRegression(n_components=2, scale=True, algorithm=\"nipals\")\n            pls_nipals.fit(X, y)\n            X_transformed_nipals, y_transformed_nipals = pls_nipals.transform(X, y)\n            assert_array_almost_equal(X_transformed, X_transformed_nipals)\n            assert_array_almost_equal(y_transformed, y_transformed_nipals)\n        \n\n\n\n", "    def test_pls_fit_transform_empty_data(self, algorithm):\n        X = np.array([])\n        Y = np.array([])\n        pls = PLSCanonical(n_components=2, algorithm=algorithm)\n        with pytest.raises(ValueError, match=\"at least 2 samples\"):\n            pls.fit(X, Y)\n        pls = PLSCanonical(n_components=2, algorithm=algorithm)\n        x_scores, y_scores = pls.fit_transform(X, Y)\n        assert_array_equal(x_scores.shape, (0, 2))\n        assert_array_equal(y_scores.shape, (0, 2))\n", "    def test_pls_regression_with_one_target(self):\n        np.random.seed(42)\n        X, y = load_linnerud(return_X_y=True)\n        X = X[:, :10]  \n        scaler = StandardScaler()\n        X = scaler.fit_transform(X)\n        y = scaler.fit_transform(y.reshape(-1, 1))\n        \n        pls = PLSRegression(n_components=2)\n        pls.fit(X, y)\n        \n        X_transformed = pls.transform(X)\n\n        assert_array_almost_equal(X_transformed.shape, (X.shape[0], 2))\n\n        # Test that the transformation doesn't change the target shape\n        y_transformed = pls.transform(X, y)\n        assert_array_almost_equal(y_transformed.shape, (X.shape[0], 2)) \n", "    def test_pls_regression_scale(self):\n\n        X, Y, _ = load_linnerud()\n        n_samples, n_features = X.shape\n        n_targets = Y.shape[1]\n\n        # Test pls regression with scale=False\n        pls = PLSRegression(n_components=2, scale=False)\n        pls.fit(X, Y)\n        X_transform = pls.transform(X)\n\n        # Test pls regression with scale=True\n        pls_scaled = PLSRegression(n_components=2, scale=True)\n        pls_scaled.fit(X, Y)\n        X_transform_scaled = pls_scaled.transform(X)\n\n        # Compare the transformed data\n        assert_array_almost_equal(X_transform, X_transform_scaled)\n", "    def test_plsc_algorithm(self, algorithm):\n        X, y = load_linnerud(return_X_y=True)\n        scaler = StandardScaler()\n        X = scaler.fit_transform(X)\n        y = scaler.fit_transform(y.reshape(-1, 1))\n        pls = PLSCanonical(n_components=1, algorithm=algorithm)\n        pls.fit(X, y)\n        assert_array_almost_equal(pls.x_weights_.shape, (X.shape[1], 1))\n        assert_array_almost_equal(pls.y_weights_.shape, (y.shape[1], 1))\n"], "sample_32": ["    def test_w_at_z(self):\n        cosmo = w0wzCDM(**self._cls_args, **self._cls_kwargs)\n        for z in valid_zs:\n            w = cosmo.w(z)\n            assert np.allclose(w, cosmo._w0 + cosmo._wz * z)\n\n", "    def test_de_density_scale(self, z):\n        cosmo = self._cls(**self._cls_args, **self._cls_kwargs)\n        scale = cosmo.de_density_scale(z)\n        # Check that the returned value is a float or Quantity\n        assert isinstance(scale, (float, u.Quantity))\n        # Check that the returned value is as expected\n        if isinstance(scale, u.Quantity):\n            assert scale.unit == u.one\n        else:\n            assert scale == scale.value\n\n        with pytest.raises(TypeError):\n            cosmo.de_density_scale('not a number')\n        with pytest.raises(ValueError):\n            cosmo.de_density_scale(1e6)\n", "    def test_w_parameter(self):\n        w0 = Parameter(value=-0.9, doc=\"test\")\n        wz = Parameter(value=0.1, doc=\"test\")\n        cosmo = w0wzCDM(\n            H0=70, Om0=0.3, Ode0=0.7, w0=w0, wz=wz\n        )\n        assert cosmo.w0.value == -0.9\n        assert cosmo.wz.value == 0.1\n", "    def test_w(self):\n        cosmo = self._cls(**self._cls_args, **self._cls_kwargs)\n\n        zarr = np.array([0.1, 0.5, 1.0])\n        w = cosmo.w(zarr)\n        assert np.allclose(w, cosmo._w0 + cosmo._wz * zarr)\n\n        # Test scalar input\n        w = cosmo.w(0.2)\n        assert np.isclose(w, cosmo._w0 + cosmo._wz * 0.2)\n", "    def test_w(self):\n        cosmo = self._cls(**self._cls_args, **self._cls_kwargs)\n        for z in valid_zs:\n            w = cosmo.w(z)\n            assert np.isclose(w, cosmo._w0 + cosmo._wz * z)\n\n\n"], "sample_1018": ["compilation error", "  def test_fortran_mod_pow(self):\n    x = symbols(\"x\")\n    expr = Mod(x**2, 3)\n    result = fcode(expr, assign_to=None, human=False)\n    assert result[1] == []\n    assert result[0] == {'_sympy_mod_pow_2_3':\n                          '((x**2) % 3)'}\n    assert result[2] == '      result = ((x**2) % 3)'\n\n    expr = Mod(x**3, 3)\n    result = fcode(expr, assign_to=None, human=False)\n    assert result[1] == []\n    assert result[0] == {'_sympy_mod_pow_3_3':\n                          '((x**3) % 3)'}\n    assert result[2] == '      result = ((x**3) % 3)'\n  ", "compilation error", "    def test_piecewise_missing_default(self):\n        expr = Piecewise((x + 1, x > 0), (x, True))\n        fcode(expr) \n", "    def test_fortran_complex_numbers(self):\n        x, y = symbols('x,y')\n        z = complex(x, y)\n        assert fcode(z) == 'cmplx(%s,%s)' % (fcode(x), fcode(y))\n        assert fcode(z.real) == fcode(x)\n        assert fcode(z.imag) == fcode(y)\n        assert fcode(abs(z)) == 'sqrt(%s ** 2 + %s ** 2)' % (fcode(x), fcode(y))\n        assert fcode(z.conjugate()) == 'cmplx(%s,-%s)' % (fcode(x), fcode(y))\n"], "sample_401": ["    def test_formset_factory_errors(self):\n        with self.assertRaises(ValueError):\n            formset_factory(ChoiceForm, max_num=50, absolute_max=40)\n", "    def test_formset_factory(self):\n        choice_form_set = formset_factory(Choice, extra=2)\n        self.assertEqual(choice_form_set.extra, 2)\n\n        # Test default values\n        choice_form_set = formset_factory(Choice)\n        self.assertEqual(choice_form_set.can_order, False)\n        self.assertEqual(choice_form_set.can_delete, False)\n        self.assertEqual(choice_form_set.min_num, DEFAULT_MIN_NUM)\n        self.assertEqual(choice_form_set.max_num, DEFAULT_MAX_NUM)\n        self.assertEqual(choice_form_set.validate_max, False)\n        self.assertEqual(choice_form_set.validate_min, False)\n\n        # Test setting individual attributes\n        with self.subTest(\"can_order\"):\n            choice_form_set = formset_factory(Choice, can_order=True)\n            self.assertEqual(choice_form_set.can_order, True)\n\n        with self.subTest(\"validate_max\"):\n            choice_form_set = formset_factory(Choice, validate_max=True)\n            self.assertEqual(choice_form_set.validate_max, True)\n\n        with self.subTest(\"can_delete_extra\"):\n            choice_form_set = formset_factory(Choice, can_delete_extra=False)\n            self.assertEqual(choice_form_set.can_delete_extra, False)\n", "    def test_total_form_count_with_deleted_forms(self):\n        formset = ChoiceFormSet(\n            data={\n                \"TOTAL_FORMS\": \"3\",\n                \"INITIAL_FORMS\": \"2\",\n                \"MAX_NUM_FORMS\": \"3\",\n                \"MIN_NUM_FORMS\": \"1\",\n                \"choices_0-choice\": \"Coke\",\n                \"choices_0-votes\": \"1\",\n                \"choices_1-choice\": \"Pepsi\",\n                \"choices_1-votes\": \"2\",\n                \"choices_2-choice\": \"Sprite\",\n                \"choices_2-votes\": \"0\",\n                \"choices_0-DELETE\": \"on\",\n            },\n            prefix=\"choices\",\n            auto_id=False,\n        )\n        self.assertEqual(formset.total_form_count(), 2)\n", "    def test_formset_factory_validation(self):\n        with self.assertRaises(ValueError):\n            formset_factory(\n                CustomKwargForm, max_num=10, absolute_max=5\n            )\n", "    def test_validate_max(self):\n        with self.assertRaises(ValidationError) as cm:\n            formset = FavoriteDrinksFormSet(\n                data=[\n                    {\"name\": \"Coke\"},\n                    {\"name\": \"Pepsi\"},\n                    {\"name\": \"Sprite\"},\n                    {\"name\": \"Fanta\"},\n                    {\"name\": \"Water\"},\n            ])\n            formset.full_clean()\n        self.assertEqual(cm.exception.code, \"too_many_forms\")\n"], "sample_373": ["    def test_view_detail_for_nonexistent_view(self):\n        response = self.client.get(reverse('admin_doc_view_detail', kwargs={'view': 'nonexistent_view'}))\n        self.assertEqual(response.status_code, 404)\n\n\n", "    def test_view_detail_displays_view_function_docstring(self):\n        url = reverse('admin:admindocs_view_detail', kwargs={'view': 'my_view'})\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, 'My view') \n\n\n\n", "    def test_model_detail_view(self):\n        url = reverse('admin:model_detail', args=[\n            'my_app',\n            'Company',\n        ])\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, 'Company')\n        self.assertContains(response, 'name')\n        self.assertContains(response, 'employees')\n\n\n\n", "compilation error", "    def test_model_detail_view(self):\n        # Create a new model and view\n        url = reverse('admin:app_name_person_changelist')\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n\n        url = reverse('admin:app_name_company_detail', args=['Person', ])\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n"], "sample_185": ["    def test_get_format_with_custom_settings(self):\n        with override_settings(\n            DECIMAL_SEPARATOR=',',\n            THOUSAND_SEPARATOR='.',\n            NUMBER_GROUPING='1000',\n        ):\n            self.assertEqual(get_format('NUMBER_GROUPING'), ',')\n            self.assertEqual(get_format('DECIMAL_SEPARATOR'), ',')\n            self.assertEqual(get_format('THOUSAND_SEPARATOR'), '.')\n", "    def test_localize_input_with_custom_format(self):\n        with translation.override('de'):\n            value = datetime.datetime(2023, 10, 27, 10, 30)\n            formatted_value = localize_input(value, default='%Y-%m-%d %H:%M:%S')\n            self.assertEqual(formatted_value, '2023-10-27 10:30:00')\n\n\n\n\n\n", "    def test_localize_input_with_invalid_datetime_format(self):\n        with patch_formats('de', DATE_FORMAT='invalid'):\n            with self.assertRaises(ValueError):\n                localize_input('2023-10-26')\n", "    def test_localize_handles_empty_values(self):\n        self.assertEqual(localize(''), '')\n        self.assertEqual(localize(None), None)\n        self.assertEqual(localize(0), '0')\n        self.assertEqual(localize(Decimal('0')), '0')\n        self.assertEqual(localize(datetime.date.min), date_format(datetime.date.min))\n        self.assertEqual(localize(datetime.datetime.min), date_format(datetime.datetime.min))\n        self.assertEqual(localize(datetime.time.min), time_format(datetime.time.min))\n\n", "    def test_sanitize_separators_with_decimal_and_thousand_separator(self):\n        with settings.override({\n            'LOCALE': 'de_DE',\n            'USE_THOUSAND_SEPARATOR': True,\n        }):\n            decimal_separator = '.'\n            thousand_sep = ','\n            value = '1.234,567.89'\n            sanitized_value = sanitize_separators(value)\n            self.assertEqual(sanitized_value, '1234567.89')\n\n            value = '1.23'\n            sanitized_value = sanitize_separators(value)\n            self.assertEqual(sanitized_value, '1.23')\n"], "sample_909": ["    def test_namedtuple_subclass(self):\n        docstring = cleandoc(\"\"\"\n            ..\n            Sample namedtuple subclass\n\n            Attributes\n            ----------\n            attr1 : Arbitrary type\n                Quick description of attr1\n            attr2 : Another arbitrary type\n                Quick description of attr2\n\n            attr3 : Type\n\n                Adds a newline after the type\n        \"\"\")\n        config = Config()\n        docstring_obj = NumpyDocstring(docstring, config)\n        expected_output = dedent(\"\"\"\n            ..\n            Sample namedtuple subclass\n\n            Attributes\n            ----------\n            attr1 : Arbitrary type\n                Quick description of attr1\n            attr2 : Another arbitrary type\n                Quick description of attr2\n            attr3 : Type\n\n                Adds a newline after the type\n        \"\"\")\n        self.assertEqual(docstring_obj.__str__(), expected_output)\n\n\n", "    def test_namedtuple_subclass(self):\n        docstring = cleandoc(NamedtupleSubclass.__doc__)\n        config = Config(napoleon_use_rtype=True)\n        obj = NamedtupleSubclass(1, 2)\n        docstring = NumpyDocstring(docstring, config, name='NamedtupleSubclass', obj=obj)\n        expected_output = dedent('''\n        \n        :param attr1: Quick description of attr1\n        :type attr1: Arbitrary type\n        :param attr2: Quick description of attr2\n        :type attr2: Another arbitrary type\n        :param attr3: Type\n        ''')\n        self.assertEqual(docstring.lines(), expected_output.splitlines())\n", "    def test_namedtuple_subclass_docstrings(self) -> None:\n        docstring = cleandoc(\"\"\"\n        Sample namedtuple subclass\n\n        Attributes\n        ----------\n        attr1 : Arbitrary type\n            Quick description of attr1\n        attr2 : Another arbitrary type\n            Quick description of attr2\n\n        attr3 : int\n\n            Adds a newline after the type\n        \"\"\")\n        config = Config(napoleon_use_param=True)\n        instance = NumpyDocstring(docstring, config)\n        expected_output = dedent(\"\"\"\n            Sample namedtuple subclass\n\n            Attributes\n            ----------\n            attr1 : Arbitrary type\n                Quick description of attr1\n            attr2 : Another arbitrary type\n                Quick description of attr2\n\n            attr3 : int\n\n                Adds a newline after the type\n        \"\"\")\n        self.assertEqual(instance.lines(), expected_output.splitlines())\n", "    def test_namedtuple_subclass(self) -> None:\n        docstring = cleandoc(\"\"\"\n        Sample namedtuple subclass\n\n        Attributes\n        ----------\n        attr1 : Arbitrary type\n            Quick description of attr1\n        attr2 : Another arbitrary type\n            Quick description of attr2\n        attr3 : Type\n\n            Adds a newline after the type\n        \"\"\")\n        config = Config()\n        docstring_obj = NumpyDocstring(docstring, config)\n        expected_output = dedent(\"\"\"\n        Sample namedtuple subclass\n\n        Attributes\n        ----------\n        attr1 : Arbitrary type\n        attr2 : Another arbitrary type\n        attr3 : Type\n\n        \"\"\")\n        self.assertEqual(docstring_obj.__str__(), expected_output)\n\n\n", "    def test_namedtuple_subclass(self) -> None:\n        docstring = cleandoc(\"\"\"\n        Sample namedtuple subclass\n\n        Attributes\n        ----------\n        attr1 : Arbitrary type\n            Quick description of attr1\n        attr2 : Another arbitrary type\n            Quick description of attr2\n        \"\"\"\n        )\n        docstring_lines = docstring.splitlines()\n        config = Config()\n        docstring_obj = NumpyDocstring(docstring_lines, config)\n        expected_output = dedent(\"\"\"\n        Attributes\n        ----------\n        attr1 : Arbitrary type\n            Quick description of attr1\n        attr2 : Another arbitrary type\n            Quick description of attr2\n        \"\"\").splitlines()\n        self.assertEqual(docstring_obj.lines(), expected_output)\n\n\n"], "sample_854": ["    def test_ovr_vs_crammer_singer(self):\n        # Test for multi_class='ovo' vs. 'crammer_singer'\n        X, y = make_blobs(n_samples=500, centers=2, random_state=42)\n        ovr_clf = svm.SVC(multi_class='ovr', probability=True)\n        cs_clf = svm.SVC(multi_class='crammer_singer', probability=True)\n        ovr_clf.fit(X, y)\n        cs_clf.fit(X, y)\n\n        # Check that probabilities are similar for both classifiers\n        assert_allclose(ovr_clf.predict_proba(X), cs_clf.predict_proba(X))\n", "    def test_svr_with_probability_fit(self):\n        # Test that fit with probability=True works for SVR\n        np.random.seed(42)\n        X, y = make_regression(n_samples=100, random_state=42)\n        svr = svm.SVR(probability=True, kernel='linear')\n        svr.fit(X, y)\n        # Ensure probability estimation is successful\n        probas = svr.predict_proba(X)\n        assert probas.ndim == 2\n        assert probas.shape[1] == 2\n\n\n\n", "    def test_predict_proba_different_kernels_probabilities(self, kernel, probability):\n        X, y = make_classification(n_samples=1000, n_features=20,\n                                   n_informative=10, n_redundant=5,\n                                   random_state=42)\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n        clf = svm.SVC(kernel=kernel, probability=probability, random_state=42)\n        clf.fit(X_train, y_train)\n        proba = clf.predict_proba(X_test)\n        if probability:\n            assert_array_almost_equal(proba.sum(axis=1), np.ones(len(X_test)))\n        else:\n            assert_raises(AttributeError, clf.predict_proba, X_test)\n\n\n", "    def test_sparse_svm_ovr(self):\n        X = sparse.csr_matrix(\n            [[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n        y = np.array([0, 1, 0])\n        clf = svm.SVC(kernel='linear', multi_class='ovr')\n        clf.fit(X, y)\n        y_pred = clf.predict(X)\n        assert_array_equal(y_pred, [0, 1, 0])\n", "    def test_svc_with_probability(self):\n        X, y = make_classification(random_state=0)\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n        clf = svm.SVC(kernel='linear', probability=True)\n        clf.fit(X_train, y_train)\n        probs = clf.predict_proba(X_test)\n        assert probs.shape[1] == len(set(y_train))  # Make sure number of classes is correct\n\n\n\n"], "sample_9": ["    def test_write_raw_html_cols(self):\n        data = np.array([[1, 2], [3, 4]])\n        t = Table(data=data)\n        t.meta = {'format': 'ascii.html', 'table_id': 1}\n        t.rename_column(0, 'col1')\n        htmldict = {'raw_html_cols': 'col1'}\n        html_writer = html.HTML(htmldict)\n        html_table = html_writer.write(t)\n        # Check for inclusion of raw HTML in the output\n        assert '<span class=\"col1\">1</span>' in html_table[0]\n\n\n\n", "    def test_raw_html_cols(self):\n        data = {'col1': ['<a href=\"https://www.astropy.org\">ASTROPY</a>'],\n                'col2': [1, 2, 3]}\n        table = Table(data)\n        html_out = html.HTML()\n\n        html_out.html['raw_html_cols'] = 'col1'\n        output_html = html_out.write(table)\n\n        assert '<a href=\"https://www.astropy.org\">ASTROPY</a>' in output_html[0]\n\n", "    def test_raw_html_cols(self):\n        data = np.array([\n            ['First', '<b>Second</b>'],\n            ['Third', 'Fourth <p>Fifth</p>']\n        ])\n\n        tb = Table(data, names=('Col1', 'Col2'))\n\n        htmldict = {'raw_html_cols': 'Col2', 'multicol': False}\n\n        writer = html.HTML(htmldict)\n        html_table = writer.write(tb)\n\n        expected = [\n            '<table>\\n'\n            '  <thead>\\n'\n            '    <tr>\\n'\n            '      <th>Col1</th>\\n'\n            '      <th>Col2</th>\\n'\n            '    </tr>\\n'\n            '  </thead>\\n'\n            '  <tbody>\\n'\n            '    <tr>\\n'\n            '      <td>First</td>\\n'\n            '      <td><b>Second</b></td>\\n'\n            '    </tr>\\n'\n            '    <tr>\\n'\n            '      <td>Third</td>\\n'\n            '      <td>Fourth <p>Fifth</p></td>\\n'\n            '    </tr>\\n'\n            '  </tbody>\\n'\n            '</table>\\n'\n        ]\n        assert html_table == expected\n\n\n\n", "    def test_read_basic_html(html_content):\n        \"\"\"Test reading a simple HTML table.\"\"\"\n        table = html.HTML().read(StringIO(html_content))\n        assert table.colnames == ['Name', 'Age']\n        assert table['Name'] == ['John Doe', 'Jane Smith']\n        assert table['Age'] == [30, 25]\n\n\n\n", "    def test_raw_html_output(self):\n        data = np.array([['<a href=\"https://www.example.com\">Link 1</a>'],\n                         ['<a href=\"https://www.example.com\">Link 2</a>']])\n        table = Table(data=data, names=['col1'])\n        table.meta['format_meta'] = {'raw_html_cols': 'col1',\n                                     'raw_html_clean_kwargs': {'tags': ['a']}}\n        html_reader = html.HTML()\n        html_string = html_reader.read(table)\n        assert html_string == '<table id=\"1\">\\n<thead>\\n<tr><th>col1</th></tr>\\n</thead>\\n<tbody>\\n<tr><td><a href=\"https://www.example.com\">Link 1</a></td></tr>\\n<tr><td><a href=\"https://www.example.com\">Link 2</a></td></tr>\\n</tbody>\\n</table>'\n"], "sample_1128": ["    def test_partial_velocity_with_more_relative_points(self):\n        N = ReferenceFrame('N')\n        A = ReferenceFrame('A')\n        B = ReferenceFrame('B')\n        p = Point('p')\n        q = dynamicsymbols('q')\n        u1, u2, u3 = dynamicsymbols('u1 u2 u3')\n        p.set_vel(N, u1 * N.x + u2 * A.y + u3 * B.z)\n        p2 = Point('p2')\n        p2.set_pos(p, q * N.x)\n        p3 = Point('p3')\n        p3.set_pos(p2, 2 * A.y)\n        \n        with raises(ValueError) as excinfo:\n            p3.vel(N)  \n            \n        assert 'Velocity has not been defined' in str(excinfo.value)\n        \n        p2.set_vel(A, u1 * A.x)\n\n        assert p3.partial_velocity(N, u1, u2) == (A.x, 2*A.y)\n\n        # Test for partial velocities with respect to u3\n        p.set_vel(N, u1 * N.x + u2 * A.y)\n        p3.set_vel(B, u3*B.z)\n        assert p3.partial_velocity(N,u3) == B.z\n\n\n\n", "    def test_partial_velocity_multi_frames(self):\n        N = ReferenceFrame('N')\n        A = ReferenceFrame('A')\n        B = ReferenceFrame('B')\n        B.orientnew('B', 'Axis', [1, N.z])\n        p = Point('p')\n        p.set_vel(N, u1 * N.x + u2 * A.y)\n        p.v1pt_theory(Point('O'), N, B)  \n        v_partial_u1_N = p.partial_velocity(N, u1)\n        v_partial_u2_A = p.partial_velocity(A, u2)\n        assert v_partial_u1_N == N.x\n        assert v_partial_u2_A == A.y \n\n\n", "    def test_partial_velocity_multiple_frames(self):\n        N = ReferenceFrame('N')\n        A = ReferenceFrame('A')\n        A.orientnew('A', 'Axis', [1*N.y])\n        p = Point('p')\n        u1, u2 = dynamicsymbols('u1, u2')\n        p.set_vel(N, u1 * N.x + u2 * A.y)\n        p.set_vel(A, u1 * A.x + u2 * A.y)\n        \n        self.assertEqual(p.partial_velocity(N, u1), N.x)\n        self.assertEqual(p.partial_velocity(N, u2), A.y)\n        self.assertEqual(p.partial_velocity(A, u1), A.x)\n        self.assertEqual(p.partial_velocity(A, u2), A.y)\n", "    def test_partial_velocity_from_v1pt(self):\n        N = ReferenceFrame('N')\n        A = ReferenceFrame('A')\n        B = ReferenceFrame('B')\n        B.set_ang_vel(N, 5 * B.y)\n        O = Point('O')\n        P = O.locatenew('P', 10 * B.x)\n        u1, u2, u3 = dynamicsymbols('u1 u2 u3')\n        O.set_vel(N, u1 * N.x + u2 * N.y + u3 * N.z)\n        P.set_vel(B, u1 * B.x + u2 * B.y)\n        self.assertEqual(P.partial_velocity(N, u1), 10 * N.x)\n        self.assertEqual(P.partial_velocity(N, u2), 5 * N.z)\n\n\n\n", "    def test_a1pt_theory_multiple_frames(self):\n        N = ReferenceFrame('N')\n        B = N.orientnew('B', 'Axis', [dynamicsymbols('q'), N.z])\n        O = Point('O')\n        P = O.locatenew('P', 10 * B.x)\n        P.set_vel(B, dynamicsymbols('qdot') * B.x + dynamicsymbols('q2dot') * B.y)\n        P.a1pt_theory(O, N, B)\n        assert P.acc(N).subs({dynamicsymbols('qdot'): 1, dynamicsymbols('q2dot'): 2}).equals( -10*dynamicsymbols('q')**2*B.x + 2*dynamicsymbols('q2dot')*B.y - 10*dynamicsymbols('q')**2*B.x + 2*dynamicsymbols('qdot')*B.y)\n"], "sample_263": ["    def test_dump_data_natural_foreign_keys(self):\n        \"\"\"\n        Test that dumpdata handles natural foreign keys correctly.\n        \"\"\"\n        with self.subTest('Natural Foreign Keys'):\n            with self.settings(INSTALLED_APPS=['dump_data_test']\n                               ):\n                Category.objects.create(name='Category A')\n                Article.objects.create(title='Article Title', category=Category.objects.first())\n                with mock.patch('django.db.models.Manager.get_queryset',\n                               return_value=Article.objects.all()) as mock_get_queryset:\n                    # Mock the get_queryset method to return all objects for better coverage\n                    management.call_command('dumpdata', 'dump_data_test.Article',\n                                           '--natural-foreign-keys', '--format=json',\n                                           stdout=StringIO())\n                    output = StringIO()\n                    management.call_command('dumpdata', 'dump_data_test.Article',\n                                           '--natural-foreign-keys', '--format=json',\n                                           stdout=output)\n                    content = output.getvalue()\n                    \n                    # Check if the natural foreign key is included in the serialized data\n                    self.assertIn('\"category\": {\"id\":', content)\n", "    def test_dumpdata_with_natural_foreign_keys(self):\n        with self.subTest(mode='json'):\n            with NamedTemporaryFile(suffix='.json', delete=False) as fp:\n                management.call_command(\n                    'dumpdata',\n                    'blog',\n                    'article',\n                    natural_foreign=True,\n                    output=fp.name,\n                )\n                fp.seek(0)\n                fixture_data = serializers.deserialize('json', fp)['objects']\n\n                for obj in fixture_data:\n                    self.assertTrue(\n                        hasattr(obj, 'pk') and obj['category'] is not None,\n                        \"Object should have a primary key and category\"\n                    )\n                    self.assertEqual(obj['category'], obj['category'].pk,\n                                     \"Category should be referenced by its primary key\")\n", "    def test_dumpdata_with_proxy_model(self):\n        with NamedTemporaryFile(suffix='.json') as temp_file:\n            management.call_command('dumpdata', '-o', temp_file.name, 'myblog', '--indent=4', '--natural-foreign')\n            with open(temp_file.name, 'r') as f:\n                fixture_data = f.read()\n\n        # Check that the fixture includes the proxy model\n        self.assertIn(ProxySpy()._meta.model_name, fixture_data)\n\n        # Load the fixture and check that the proxy model instances exist\n        management.call_command('loaddata', temp_file.name, verbosity=0)\n        self.assertTrue(ProxySpy.objects.exists())\n", "    def test_dumpdata_with_proxy_model(self):\n        with self.subTest(model=ProxySpy):\n            with NamedTemporaryFile(suffix='.json') as temp_file:\n                management.call_command('dumpdata', 'myapp', format='json', output=temp_file.name)\n                temp_file.seek(0)\n                data = temp_file.read()\n            # Ensure proxy models are serialized\n            self.assertIn(\n                b'\"fields\": {\"id\": 1, \"name\": \"ProxySpy_name\"}', data\n            )\n", "    def test_dumpdata_with_exclude(self):\n        \"\"\"\n        Tests the exclusion of specific apps and models from the dumpdata output.\n        \"\"\"\n        with NamedTemporaryFile(mode='w+t', delete=False) as temp_file:\n            temp_file_path = temp_file.name\n            try:\n                management.call_command(\n                    'dumpdata',\n                    '-o', temp_file.name,\n                    'tests.Article',\n                    'tests.Category',\n                    exclude=['tests.Tag'],\n                )\n                with open(temp_file_path, 'r') as f:\n                    data = f.read()\n\n                # Assert that Tag data is not present in the dump\n                self.assertNotIn('Tag', data)\n\n                # Assert that Article and Category data are present in the dump\n                self.assertIn('Article', data)\n                self.assertIn('Category', data)\n            finally:\n                os.remove(temp_file_path)\n"], "sample_677": ["    def test_empty_expression():\n        assert evaluate(\"\", lambda x: False) is False\n\n\n", "    def test_parens_and_negation():\n        assert evaluate(\"not(A or B)\", lambda x: x == \"A\" or x == \"B\") is False\n        assert evaluate(\"not(A and not B)\", lambda x: x == \"A\" and x == \"B\") is True\n", "    def test_nested_parentheses(matcher: Callable[[str], bool]):\n        assert evaluate(\"(ident and ident)\", matcher) is True\n        assert evaluate(\"ident and (ident or ident)\", matcher) is True\n        assert evaluate(\"not (ident and ident)\", matcher) is False\n", "    def test_parens():\n        assert evaluate(\"not ((a or b) and c)\", lambda x: x == \"a\" or x == \"b\" and x == \"c\")\n\n", "    def test_nested_parens():\n        assert evaluate('(not True)', lambda x: False) is True\n        assert evaluate('(not False)', lambda x: False) is False\n        assert evaluate('(True and False)', lambda x: False) is False\n        assert evaluate('(True or False)', lambda x: False) is True\n\n\n\n"], "sample_865": ["compilation error", "    def test_sparse_matrices(self):\n        for name, data in DATASETS.items():\n            if name in [\"sparse-pos\", \"sparse-neg\", \"sparse-mix\"]:\n                X = data[\"X_sparse\"]\n                y = data[\"y\"]\n                clf = DecisionTreeClassifier(random_state=0)\n                reg = DecisionTreeRegressor(random_state=0)\n\n                clf.fit(X, y)\n                reg.fit(X, y)\n\n                # Check if the weights are properly handled\n                for tree in clf.estimators_:\n                    if tree is not None:\n                        assert_array_equal(tree.n_node_samples,\n                                           clf.tree_.n_node_samples)\n                for tree in reg.estimators_:\n                    if tree is not None:\n                        assert_array_equal(tree.n_node_samples,\n                                           reg.tree_.n_node_samples)\n\n\n\n\n", "    def test_sparse_array_handling(self):\n        for name, dataset in DATASETS.items():\n            if \"sparse\" in name:\n                X = dataset[\"X\"]\n                y = dataset[\"y\"]\n\n                tree_clf = DecisionTreeClassifier(\n                    random_state=0\n                ).fit(X, y)\n                sparse_tree = tree.DecisionTreeClassifier(\n                    random_state=0\n                ).fit(X_sparse_pos, y_random)\n                \n                assert_tree_equal(tree_clf.tree_, sparse_tree.tree_)\n\n\n\n", "    def test_sparse_tree(self, sparse_dataset):\n        X, y = DATASETS[sparse_dataset][\"X\"], DATASETS[sparse_dataset][\"y\"]\n        tree_sparse = CLF_TREES[\"DecisionTreeClassifier\"](\n            criterion=\"gini\", max_depth=3, random_state=42).fit(X_sparse_pos, y_random)\n        tree_dense = CLF_TREES[\"DecisionTreeClassifier\"](\n            criterion=\"gini\", max_depth=3, random_state=42).fit(X, y)\n        assert_tree_equal(tree_sparse.tree_, tree_dense.tree_)\n", "    def test_minimal_cost_complexity_pruning_with_ccp_alpha(self):\n        for tree_type in [\"DecisionTreeClassifier\", \"DecisionTreeRegressor\",\n                          \"ExtraTreeClassifier\", \"ExtraTreeRegressor\"]:\n            X = np.random.rand(100, 10)\n            y = np.random.randint(0, 2, size=100)\n            if tree_type in [\"DecisionTreeClassifier\",\n                              \"ExtraTreeClassifier\"]:\n                y = np.array(\n                    [1 if i < 50 else 0 for i in range(100)])\n            else:\n                y = np.random.rand(100)\n\n            tree = ALL_TREES[tree_type](random_state=0)\n            tree.fit(X, y)\n\n            for ccp_alpha in [0, 0.1, 0.5, 1]:\n                tree_ccp = copy.deepcopy(tree)\n                tree_ccp.set_params(ccp_alpha=ccp_alpha)\n                tree_ccp.fit(X, y)\n\n                if ccp_alpha > 0:\n                    assert tree_ccp.tree_.node_count < tree.tree_.node_count\n                else:\n                    assert tree_ccp.tree_.node_count == tree.tree_.node_count\n"], "sample_1033": ["    def test_as_content_radical():\n        from sympy.functions import sqrt\n\n        expr = 2*sqrt(2) + 4*sqrt(10)\n        con, prim = expr.as_content_primitive(radical=True)\n        assert con == 2\n        assert prim.simplify() == sqrt(2)*(1 + 2*sqrt(5))\n\n        expr2 = 3*sqrt(2) + 6*sqrt(8)\n        con, prim = expr2.as_content_primitive(radical=True)\n        assert con == 3\n        assert prim.simplify() == sqrt(2)*(1 + 2*sqrt(2))\n", "    def test_as_coeff_add_complex_non_real():\n        assert Add(1 + 2*I, 3 - 4*I).as_coeff_Add() == (1 + 3, 2 - 4)\n\n", "    def test_as_content_primitive_radical_add():\n        from sympy import sqrt\n        expr = 2*sqrt(2) + 4*sqrt(10)\n        con, prim = expr.as_content_primitive(radical=True)\n        assert con == 2\n        assert prim == sqrt(2)*(1 + 2*sqrt(5))\n", "compilation error", "    def test_as_content_primitive_radicals(self):\n        from sympy.abc import sqrt\n        expr = 2*sqrt(2) + 4*sqrt(10)\n        con, prim = expr.as_content_primitive(radical=True)\n        assert con == 2\n        assert prim == sqrt(2)*(1 + 2*sqrt(5))\n\n        expr = 3*sqrt(2) + 4*sqrt(2)\n        con, prim = expr.as_content_primitive(radical=True)\n        assert con == 7\n        assert prim == sqrt(2) * 7\n\n        expr = 3*sqrt(2) + 4*sqrt(3)\n        con, prim = expr.as_content_primitive(radical=True)\n        assert con == 1\n        assert prim == 3*sqrt(2) + 4*sqrt(3)\n"], "sample_1119": ["    def test_eigenvals_eigenvects_identity():\n        A = eye(3)\n        w, v = A.eigenvals_eigenvects()\n        assert sorted(w) == [1, 1, 1]\n        assert v == [(1, [[1, 0, 0], [0, 1, 0], [0, 0, 1]])]\n       \n\n", "    def test_is_square():\n        assert Matrix([[1, 2], [3, 4]]).is_square\n        assert Matrix([[1, 2, 3], [4, 5, 6]]).is_square\n        assert Matrix([[1]]).is_square\n        assert Matrix([[1, 2], [3]]).is_not_square\n\n\n\n", "    def test_rank_decomposition():\n        from sympy.matrices import Matrix, zeros\n        M = Matrix([[1, 2], [3, 4], [5, 6]])\n        rank, U = M.rank_decomposition()\n        assert rank == 2\n        assert U.shape == (3, 2)\n        assert (U * U.T).is_diagonal\n\n        M = Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n        rank, U = M.rank_decomposition()\n        assert rank == 1\n        assert U.shape == (3, 1)\n        assert (U * U.T).is_diagonal\n  \n\n        M = zeros(3, 3)\n        rank, U = M.rank_decomposition()\n        assert rank == 0\n        assert U.shape == (3, 0)\n\n\n\n", "    def test_zeros_size():\n        raises(TypeError, lambda: ZeroMatrix('invalid'))\n        raises(TypeError, lambda: ZeroMatrix(symbols('x')))\n        assert ZeroMatrix(2, 3).shape == (2, 3)\n        assert ZeroMatrix(1, 1).shape == (1, 1)\n        assert ZeroMatrix(0, 0).shape == (0, 0)\n      ", "    def test_inv_mod():\n        A = Matrix([[1, 2], [3, 4]])\n        m = 5\n        result = A.inv_mod(m)\n        expected = Matrix([[2, -1], [-3, 1]])\n        assert result == expected\n    "], "sample_417": ["    def test_floatformat_with_grouping(self):\n        with translation.override('en'):\n            self.assertEqual(self.render(\n                'floatformat01', {'a': 1234.5678, 'b': 12345678.90123},\n            ).content, \"1,234.57 12,345,678.90\")\n\n", "    def test_floatformat_with_grouping(self):\n        self.assertEqual(\n            self.engine.render(\n                \"floatformat01\", {\"a\": 1234.56789, \"b\": 123456789.123456789}\n            ),\n            \"1,234.5679 123,456,789.1235\",\n        )\n\n\n", "    def test_floatformat_with_precision(self):\n        self.assertEqual(\n            self.render_with_context({\"a\": 12.3456789, \"b\": 12.345}),\n            mark_safe(\"12.35 12.3\"),\n        )\n\n\n\n", "    def test_floatformat_with_precision(self):\n        self.assertEqual(\n            self.render_context[\"floatformat01\"].render(),\n            mark_safe(\"2.3.456789012345678901234567890123456789|floatformat:-3|2\",\n                      ),\n        )\n", "        def test_floatformat_decimal(self):\n            a = Decimal(\"1.23456789\")\n            b = Decimal(\"0.00001\")\n            with translation.override(\n                \"en-us\", settings=self.settings\n            ):\n                self.assertEqual(\n                    self.template.render(\n                        {\"a\": a, \"b\": b}, current_app=self.app\n                    ),\n                    \"1.23456789 0.00001\",\n                )\n"], "sample_913": ["    def test_parse_annotation(self):\n        assert _parse_annotation(\"int\") == \"int\"\n        assert _parse_annotation(\"Optional[str]\") == \"Optional[str]\"\n        assert _parse_annotation(\"List[int]\") == \"List[int]\"\n        assert _parse_annotation(\"Union[int, str]\") == \"Union[int, str]\"\n        assert _parse_annotation(\"(int, str)\") == \"(int, str)\"\n        assert _parse_annotation(\"-> int\") == \" -> int\"\n", "    def test_parse_annotation(self):\n        assert _parse_annotation('Optional[str]') == 'Optional[str]'\n        assert _parse_annotation('list[int]') == 'list[int]'\n        assert _parse_annotation('Union[str, int]') == 'Union[str, int]'\n        assert _parse_annotation('typing.List[int]') == 'typing.List[int]'\n        assert _parse_annotation('typing.Optional[str]') == 'typing.Optional[str]'\n\n\n\n", "    def test_parse_annotation(self):\n        assert _parse_annotation(\"int\") == \"int\"\n        assert _parse_annotation(\"int|float\") == \"int or float\"\n        assert _parse_annotation(\"int, ...\") == \"int, ...\"\n        assert _parse_annotation(\"typing.List[str]\") == \"List[str]\"\n        assert _parse_annotation(\"typing.Optional[int]\") == \"Optional[int]\"\n\n\n\n", "    def test_parse_annotation_simple(self):\n        self.assertEqual(_parse_annotation(\"int\"), \"int\")\n", "    def test_parse_annotation(self):\n        assert _parse_annotation('int') == 'int'\n        assert _parse_annotation('List[str]') == 'List[str]'\n        assert _parse_annotation('Optional[int]') == 'Optional[int]'\n\n"], "sample_355": ["    def test_has_perm_with_both_user_and_group_permissions(self):\n        with self.subTest():\n            self.assertTrue(self.user.has_perm('test_perm'))\n        with self.subTest():\n            backend = SimpleBackend()\n            self.assertTrue(backend.has_perm(self.user, 'test_perm'))\n", "    def test_permissions_simple_backend(self):\n        user = self.user\n        with override_settings(AUTHENTICATION_BACKENDS=['auth_tests.test_auth_backends.SimpleBackend']):\n            self.assertSetEqual(user.get_user_permissions(), {'user_perm'})\n            self.assertSetEqual(user.get_group_permissions(), {'group_perm'})\n", "    def test_get_user_permissions(self):\n        user = self.user\n        permissions = user.get_user_permissions()\n        self.assertEqual(permissions, ['user_perm'])\n", "    def test_get_user_permissions(self):\n        user = self.user\n        permissions = user.get_user_permissions()\n        self.assertIn('user_perm', permissions)\n\n        permissions = user.get_group_permissions()\n        self.assertIn('group_perm', permissions)\n", "    def test_user_permissions_from_backend(self):\n        user = self.user\n        permissions = user.get_user_permissions()\n        self.assertEqual(permissions, {'user_perm'})\n"], "sample_525": ["    def test_constrained_layout_engine_duplicate_data(self):\n        fig = Figure()\n        ax1 = fig.add_subplot(111)\n        ax2 = fig.add_subplot(111)\n        # duplicate data for constrained layout engine\n        ax1.plot([1, 2, 3], [4, 5, 6])\n        ax2.plot([1, 2, 3], [4, 5, 6])\n\n        # assert the layout engine is set to constrained layout\n        assert isinstance(fig.get_layout_engine(), ConstrainedLayoutEngine)\n\n        # Execute the layout\n        fig.get_layout_engine().execute(fig)\n\n        # Assert that the duplicate data is correctly handled\n        plt.close(fig)\n\n\n", "    def test_constrained_layout_pad_override(self):\n        fig = Figure()\n        ax1 = fig.add_subplot(111)\n        ax2 = fig.add_subplot(111)\n\n        with fig.constrained_layout():\n            fig.update_layout(\n                # Ensure that the padding is set to a custom value.\n                # This ensures the default padding is not used for comparison\n                padded_subplots_layout=True,\n                pad=1.2,\n                h_pad=0.5,\n                w_pad=0.4,\n                )\n        plt.draw()\n        \n        # Check that the layout settings are applied correctly.\n        # This is a challenging task to do precisely, as the actual dimensions \n        # of the subplots are influenced by their content. \n\n        # For the initial test, we will check if the padding values are\n        # correctly applied to some basic dimensions.\n\n\n        # Example: Check the total padding left of the subplots\n        expected_left_padding = fig.get_layout_engine().info['pad'] * 1.2\n        \n\n        # Here, you'd insert your test logic to compare the actual \n        # left padding of the subplots with this 'expected_left_padding'\n\n        \n        \n        \n", "    def test_constrained_layout_with_empty_axes(self):\n        fig = Figure()\n\n        # Add an empty Axes\n        ax = fig.add_subplot(111)\n        ax.axis('off')\n\n        # Ensure that the layout engine successfully handles empty Axes\n        try:\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n                constrained_layout = ConstrainedLayoutEngine()\n                constrained_layout.execute(fig)\n        except (ValueError, TypeError):\n            pytest.fail(\"ConstrainedLayoutEngine execution failed for empty Axes\")\n\n\n\n", "    def test_tight_layout_with_inlayout(self, image_comparison):\n        fig, ax = plt.subplots()\n        ax.set_xlim([0, 10])\n        ax.set_ylim([0, 10])\n        ax.plot([1, 2, 3, 4], [5, 6, 7, 8])\n        \n\n        legend_item = ax.legend()\n        legend_item.set_in_layout(False)\n\n        fig.tight_layout()\n        ax.legend()\n\n\n\n        \n        plt.show()\n", "    def test_tight_layout(self):\n        fig, ax = plt.subplots()\n        ax.set_xlim(0, 10)\n        ax.set_ylim(0, 10)\n        ax.plot([1, 2, 3], [4, 5, 6])\n\n        fig.tight_layout()\n        plt.show()\n"], "sample_1023": ["    def test_cycle_length_values():\n        func = lambda i: (i**2 + 1) % 51\n        n = cycle_length(func, 4, values=True)\n        assert list(n) == [17, 35, 2, 5, 26, 14, 44, 50, 2, 5, 26, 14]\n", "compilation error", "    def test_compositepi_edge_cases():\n        assert compositepi(1) == 0\n        assert compositepi(2) == 0\n        assert compositepi(3) == 0\n        assert compositepi(4) == 1\n        assert compositepi(5) == 1\n        assert compositepi(6) == 2\n    ", "    def test_compositepi_edge_cases():\n        assert compositepi(1) == 0\n        assert compositepi(2) == 0\n        assert compositepi(3) == 0\n        assert compositepi(4) == 1\n        assert compositepi(5) == 2\n        assert compositepi(6) == 3\n        assert compositepi(7) == 3\n        assert compositepi(10) == 4\n        assert compositepi(11) == 4\n\n\n\n", "    def test_cycle_length_all_cases():\n        for func in [lambda i: (i**2 + 1) % 51,\n                     lambda i: (i**3 - 2 * i) % 101,\n                     lambda i: i % 3]:\n            for x0 in range(1, 10):\n                cycle = cycle_length(func, x0)\n                if next(cycle, None) is not None:\n                    lam, mu = next(cycle)\n                    for i in range(lam + mu):\n                        val = func(x0)\n                        if values:\n                            yield lambda val: val == cycle_length(func, x0, values=True)\n\n\n\n"], "sample_582": ["    def sample_app(tmpdir):\n        app_path = tmpdir.join(\"my_app.py\").resolve()\n        with open(app_path, \"w\") as f:\n            f.write(\n                \"\"\"", "    def test_locate_app(runner, path):\n        os.chdir(test_path)\n        with open(\"app.py\", \"w\") as f:\n            f.write(\"from flask import Flask\\napp = Flask(__name__)\")\n        with open(\"wsgi.py\", \"w\") as f:\n            f.write(f\"from src.app import app\\napp = app\")\n        with open(\n            os.path.join(\"another_app\", \"app.py\"), \"w\"\n        ) as f:\n            f.write(\"from flask import Flask\\napp = Flask(__name__)\")\n\n        result = runner.invoke(\n            sys.argv[0], [\"run\"], catch_exceptions=False,\n        )\n        assert result.exit_code == 0\n        os.chdir(cwd)\n", "def test_load_dotenv(runner, monkeypatch):\n    original_os_environ = os.environ.copy()\n    monkeypatch.setattr(os, \"environ\", original_os_environ)\n\n    # Mock the find_dotenv function to return a specific file.\n    monkeypatch.setattr(\n        \"flask.cli.dotenv\",\n        lambda filename: os.path.join(cwd, filename)\n        if filename is not None\n        else None,\n    )\n\n    # Test loading .env file\n    with runner.isolated_filesystem(\n        \"test_app\"\n    ) as tmpdir:\n        (tmpdir / \".env\").write_text(\"FLASK_DEBUG=true\")\n        result = runner.invoke(\n            sys.argv[0], [\"run\"], catch_exceptions=False,\n            extra_env={\"FLASK_APP\": \"test_app\"}\n        )\n        assert \" * Debug mode: on\" in result.output\n\n    # Test loading .flaskenv file\n    with runner.isolated_filesystem(\n        \"test_app\"\n    ) as tmpdir:\n        (tmpdir / \".flaskenv\").write_text(\"FLASK_DEBUG=true\")\n        result = runner.invoke(\n            sys.argv[0], [\"run\"], catch_exceptions=False,\n            extra_env={\"FLASK_APP\": \"test_app\"}\n        )\n        assert \" * Debug mode: on\" in result.output\n\n    # Test loading a specific file\n    with runner.isolated_filesystem(\n        \"test_app\"\n    ) as tmpdir:\n        (tmpdir / \"config.env\").write_text(\"FLASK_DEBUG=true\")\n        result = runner.invoke(\n            sys.argv[0], [\"run\"], catch_exceptions=False,\n            extra_env={\"FLASK_APP\": \"test_app\", \"FLASK_CONFIG_FILE\": \"config.env\"}\n        )\n        assert \" * Debug mode: on\" in result.output\n\n    # Clean up the environment\n    os.environ.clear()\n    os.environ.update(original_os_environ)\n\n\n", "def app(runner):\n    app = Flask(__name__)\n    app.config[\"TESTING\"] = True\n    @app.route(\"/\")\n        return \"Hello, World!\"\n    with runner.isolated_filesystem() as fs:\n        fs.joinpath(\"wsgi.py\").write(f\"\"\"", "    def test_load_dotenv_in_script_info_when_called_from_cli(runner, tmp_path):\n        os.environ[\"FLASK_SKIP_DOTENV\"] = \"true\"\n        \n        # Set up a temporary directory and create example .env files\n        (tmp_path / \".env\").write_text(\"TEST_VAR=test_value\")\n        (tmp_path / \".flaskenv\").write_text(\"TEST_ENV_VAR=env_value\")\n\n        app = Flask(\"test_app\")\n        \n        with runner.isolated_filesystem(tmp_path):\n            # Create ScriptInfo instance and load app\n            info = ScriptInfo(create_app=lambda: app)\n            info.load_app()\n\n        # Assert variables are loaded correctly\n        assert os.environ.get(\"TEST_VAR\") == \"test_value\"\n        assert os.environ.get(\"TEST_ENV_VAR\") == \"env_value\"\n\n\n\n\n"], "sample_773": ["def test_logistic_regression_cv_penalty(penalty):\n    X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n    clf = LogisticRegressionCV(Cs=10, penalty=penalty, cv=5, random_state=42)\n    clf.fit(X, y)\n    assert_equal(clf.penalty_, penalty)\n    assert_equal(clf.C_.shape, (1,))\n    assert_equal(clf.l1_ratio_.shape, (1,))\n", "    def test_log_reg_scoring_path_multinomial(self):\n        solver = 'sag'\n        X, y = make_classification(n_samples=100, n_features=2, n_classes=3,\n                                   random_state=0)\n\n        # For multinomial logistic regression, the shape of scores should\n        # be (n_folds, n_Cs, n_l1_ratios, n_classes) when using\n        # `multi_class='multinomial'`\n        with ignore_warnings(category=ChangedBehaviorWarning):\n            scores, coefs_paths, Cs, n_iter = (\n                _log_reg_scoring_path(X, y, train, test, pos_class=None,\n                                  Cs=10, fit_intercept=True, penalty='l2',\n                                  dual=False, solver=solver, tol=1e-4,\n                                  max_iter=100, verbose=0,\n                                  class_weight=None, scoring='accuracy',\n                                  multi_class='multinomial',\n                                  intercept_scaling=1.,\n                                  random_state=0,\n                                  max_squared_sum=None,\n                                  sample_weight=None, l1_ratio=None))\n        assert scores.shape[2] == 3\n", "compilation error", "    def test_penalty(self, penalty):\n        X, y = make_classification(n_samples=100, n_features=5, random_state=42)\n        clf = LogisticRegressionCV(Cs=10, penalty=penalty, solver='lbfgs',\n                                   random_state=42)\n        check_predictions(clf, X, y)\n", "    def test_logistic_regression_cv_ovr(self, solver, multi_class):\n        X, y = make_classification(n_samples=100, n_features=5, n_informative=3,\n                                   n_redundant=2, random_state=42)\n        y = np.array(y)\n        clf = LogisticRegressionCV(Cs=10,\n                                    cv=3,\n                                    solver=solver,\n                                    multi_class=multi_class,\n                                    random_state=42,\n                                    penalty='l2')\n\n        clf.fit(X, y)\n        assert_greater(clf.n_iter_.mean(), 0)\n\n"], "sample_319": ["    def test_modelstate_with_circular_ref(self):\n        self.assertEqual(\n            rabbit.model_fields,\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"knight\", models.ForeignKey(\"eggs.Knight\", models.CASCADE)),\n                (\"parent\", models.ForeignKey(\"eggs.Rabbit\", models.CASCADE)),\n            ],\n        )\n        self.assertEqual(rabbit.model_options[\"unique_together\"], [(\"parent\", \"knight\")])\n        self.assertEqual(rabbit.model_indexes, [models.Index(fields=[\"parent\", \"knight\"], name=\"rabbit_circular_fk_index\")])\n\n\n\n", "compilation error", "compilation error", "    def test_author_with_m2m_through_field_names(self):\n        # Test that the field names in the through model are correctly retrieved\n        author_with_m2m_through_model = models.get_model(\"testapp\", \"AuthorWithM2MThrough\")\n        publisher_model = models.get_model(\"testapp\", \"Publisher\")\n\n        self.assertEqual(\n            author_with_m2m_through_model._meta.get_field(\n                \"publishers\"\n            ).related_model,\n            publisher_model,\n        )\n        self.assertEqual(\n            author_with_m2m_through_model._meta.get_field(\n                \"publishers\"\n            ).through_model,\n            models.get_model(\"testapp\", \"Contract\"),\n        )\n", "compilation error"], "sample_535": ["    def test_col_width(self):\n        fig, ax = plt.subplots()\n        table = Table(ax)\n\n        table.add_cell(0, 0, text='A')\n        table.add_cell(1, 0, text='B')\n        table.add_cell(2, 0, text='C')\n\n        table.auto_set_column_width(0)\n\n        renderer = fig.canvas.get_renderer()\n        bbox = table.get_window_extent(renderer)\n        expected_width = bbox.width\n\n        # Check that the column width is adjusted based on text length\n        assert expected_width > 1, 'Column width not adjusted'\n\n        plt.close(fig)\n\n", "    def test_bbox(self):\n        fig, ax = plt.subplots()\n        table = Table(ax, bbox=[0.1, 0.1, 0.8, 0.8],\n                      cellText=[[1, 2, 3], [4, 5, 6]])\n        table.set_fontsize(10)\n        ax.set_xlim(0, 1)\n        ax.set_ylim(0, 1)\n\n        figmanager = plt.get_current_fig_manager()\n        figmanager.resize(400, 400)\n\n        ", "    def test_table_update_positions(self):\n        fig, ax = plt.subplots()\n        table = Table(ax, bbox=[0.1, 0.1, 0.8, 0.8])\n        table.add_cell(0, 0, text='a', width=0.2, height=0.2)\n        table.add_cell(0, 1, text='b', width=0.2, height=0.2)\n        table.add_cell(1, 0, text='c', width=0.2, height=0.2)\n        table.add_cell(1, 1, text='d', width=0.2, height=0.2)\n\n        ax.set_xlim([0, 1])\n        ax.set_ylim([0, 1])\n        plt.draw()\n        table.stale = True\n        renderer = fig.canvas.get_renderer()  \n        table._update_positions(renderer)\n        plt.draw()\n\n\n\n", "    def test_auto_set_font_size(self):\n        fig, ax = plt.subplots()\n\n        data = np.random.rand(5, 5)\n        table = Table(ax, cellText=data.tolist(), loc='center')\n\n        # Ensure auto_set_font_size is working\n        table.auto_set_font_size(True)\n        ax.set_xlim(0, 1)\n        ax.set_ylim(0, 1)\n        table.draw()\n", "    def test_auto_resize(self):\n        fig, ax = plt.subplots()\n\n        data = np.random.rand(5, 5)\n        table = Table(ax, bbox=[0.1, 0.1, 0.8, 0.8])\n        for row in range(5):\n            for col in range(5):\n                table.add_cell(row, col, width=0.1,\n                               height=0.1,\n                               text=f\"{data[row, col]:.2f}\")\n        table.auto_set_column_width()\n        table.auto_set_font_size()\n        ax.set_xlim(0, 1)\n        ax.set_ylim(0, 1)\n        ax.set_aspect('equal')\n        plt.draw()\n\n\n\n\n\n\n\n\n"], "sample_123": ["    def test_urlencode_multivalue_dict(self):\n        data = MultiValueDict({\n            'key1': ['value1', 'value2'],\n            'key2': ['value3', 'value4'],\n            'key3': ['value5']\n        })\n        encoded_string = urlencode(data)\n        self.assertEqual(encoded_string, 'key1=value1&key1=value2&key2=value3&key2=value4&key3=value5')\n\n        # Test with empty strings\n        data2 = MultiValueDict({\n            'key1': ['value1', ''],\n            'key2': ['', 'value3'],\n        })\n        encoded_string2 = urlencode(data2)\n        self.assertEqual(encoded_string2, 'key1=value1&key1=&key2=&key2=value3')\n\n        # Test with None values\n        with self.assertRaises(TypeError) as context:\n            urlencode({'key': None})\n        self.assertEqual(str(context.exception), self.cannot_encode_none_msg)\n\n        with self.assertRaises(TypeError) as context:\n            urlencode(MultiValueDict({'key': [None]}))\n        self.assertEqual(str(context.exception), self.cannot_encode_none_msg)\n", "    def test_urlencode_multivalue(self):\n        data = MultiValueDict({\n            'key1': ['value1', 'value2'],\n            'key2': ['value3', 'value4'],\n            'key3': None,  \n        })\n        with self.assertRaisesMessage(TypeError, self.cannot_encode_none_msg):\n            urlencode(data)\n", "    def test_urlencode_multivaluedict(self):\n        data = MultiValueDict({\n            'a': ['1', '2', '3'],\n            'b': ['4', 5, '6'],\n            })\n        expected = 'a=1&a=2&a=3&b=4&b=5&b=6'\n        self.assertEqual(urlencode(data), expected)  \n", "    def test_urlencode_multivalue_dict(self):\n        data = MultiValueDict({\"k1\": [\"v1\", \"v2\"], \"k2\": [\"v3\", \"v4\"]})\n        encoded = urlencode(data)\n        self.assertEqual(encoded, \"k1=v1&k1=v2&k2=v3&k2=v4\")\n        encoded = urlencode(data, doseq=False)\n        self.assertEqual(encoded, \"k1=v1&k2=v3\")\n", "    def test_urlencode_multivaluedict(self):\n        data = MultiValueDict({'a': ['1', '2'], 'b': ['3', '4']})\n        encoded = urlencode(data)\n        self.assertEqual(encoded, 'a=1&a=2&b=3&b=4')\n\n        encoded = urlencode(data, doseq=False)\n        self.assertEqual(encoded, 'a=1&b=3')\n\n        encoded = urlencode(data, doseq=True)\n        self.assertEqual(encoded, 'a=1&a=2&b=3&b=4')\n\n"], "sample_1038": ["    def test_from_index_summation_scalar_addition(self):\n        expr = Sum(A[i, j]*B[j, k] + C[i, k], (j, 0, n-1))\n        mat = MatrixExpr.from_index_summation(expr)\n        self.assertEqual(mat, MatAdd(MatMul(A, B), C))\n", "    def test_kroneckerdelta_matrix(self):\n        for i in range(3):\n            for j in range(3):\n                kr = KroneckerDelta(i, j)\n                kr_matrix = Matrix([[kr if k==i else 0 for k in range(3)] for i in range(3)])\n                self.assertTrue(kr_matrix.eq(kr.matrix_form()))\n\n\n\n", "    def test_from_index_summation_empty_indices():\n        from sympy.matrices.expressions import Sum\n        expr = Sum(0, (i, 0, 1))\n        raises(ValueError, lambda: MatrixExpr.from_index_summation(expr))\n", "    def test_matrix_derivative_scalar_times_matrix(self):\n        from sympy.strategies.compose import compose\n        x = symbols('x')\n        A = MatrixSymbol('A', 2, 2)\n        f = 2 * A\n        df = _matrix_derivative(f, x)\n        assert df == 2 * A.diff(x)\n", "    def test_from_index_summation_transpose(self):\n        expr = Sum(A[i, j]*B[j, k], (j, 0, m-1))\n        result = MatrixExpr.from_index_summation(expr, first_index = 'i', last_index = 'k')\n        self.assertEqual(result, Transpose(MatMul(A, B)))\n\n"], "sample_533": ["    def test_contour_with_corner_mask(self):\n        np.random.seed(1234)\n        x = np.linspace(-1, 1, 50)\n        y = np.linspace(-1, 1, 50)\n        X, Y = np.meshgrid(x, y)\n        Z = np.exp(-(X**2 + Y**2))\n        with rc_context({'contour.corner_mask': True}):\n            fig, ax = plt.subplots(1, 1)\n            cs = ax.contour(X, Y, Z, levels=8)\n        fig.savefig(self.test_filename)\n        plt.close(fig)\n        ", "    def test_quadcontour_levels(self):\n        x, y = np.mgrid[0:10:10j, 0:10:10j]\n        z = np.sin(x) * np.cos(y)\n        fig, ax = plt.subplots()\n\n        cs = ax.contourf(x, y, z, levels=np.linspace(0, 1, 11), cmap='viridis')\n        ax.clabel(cs, inline=True, fontsize=8)\n\n        plt.title('Contours with 11 Levels')\n        plt.show()\n\n\n", "    def test_contour_quad(self):\n        x = np.arange(-3, 3, 0.1)\n        y = np.arange(-3, 3, 0.1)\n        X, Y = np.meshgrid(x, y)\n        Z = np.sin(np.sqrt(X ** 2 + Y ** 2))\n\n        fig, ax = plt.subplots()\n\n        contour = ax.contour(X, Y, Z, levels=10, colors=\"k\")\n        ax.clabel(contour, inline=True, fontsize=8)\n        ax.set_title(\"Contour\")\n\n        plt.show()\n\n\n\n", "    def test_filled_contour_levels(self):\n        x = np.linspace(0, 10, 100)\n        y = np.linspace(0, 10, 100)\n        X, Y = np.meshgrid(x, y)\n        Z = np.sin(np.sqrt(X**2 + Y**2))\n\n        with rc_context({'contour.algorithm': 'mpl2005'}):\n            cs = plt.contourf(X, Y, Z, levels=np.linspace(0.5, 1, 5), cmap='viridis')\n            plt.colorbar(cs)\n            plt.show()\n\n", "    def test_contour_quad_levels(self):\n        with rc_context({'contour.levels': 'auto'}):\n            x = np.linspace(-1, 1, 100)\n            y = np.linspace(-1, 1, 100)\n            X, Y = np.meshgrid(x, y)\n            Z = np.sin(np.sqrt(X**2 + Y**2))\n            fig, ax = plt.subplots()\n            cs = ax.contourf(X, Y, Z, levels=10)\n            ax.clabel(cs, inline=True, fontsize=8)\n            ax.set_title('contourf with 10 levels')\n            ax.set_xlabel('x')\n            ax.set_ylabel('y')\n            plt.show()\n"], "sample_87": ["    def test_iter_all_python_module_files_with_zipfile(self):\n        with tempfile.TemporaryDirectory(prefix='test_iter_modules_and_files_') as tmpdirname:\n            zip_filename = self.temporary_file('test.zip')\n            with zipfile.ZipFile(zip_filename, 'w') as zf:\n                zf.writestr('module1.py', 'pass')\n                zf.writestr('module2.py', 'pass')\n\n            self.addCleanup(os.remove, zip_filename)\n            extend_sys_path([tmpdirname])\n            self.import_and_cleanup('test_iter_modules_and_files_module1')\n            self.import_and_cleanup('test_iter_modules_and_files_module2')\n            self.assertFileFound(zip_filename)\n", "    def test_iter_modules_and_files_with_zip(self):\n        with tempfile.TemporaryDirectory() as tmpdir:\n            zip_filename = self.temporary_file('my_module.zip')\n            with zipfile.ZipFile(zip_filename, 'w') as z:\n                z.write('__init__.py', arcname='my_module/__init__.py')\n                z.write('module1.py', arcname='my_module/module1.py')\n            extend_sys_path(tmpdir)\n            self.import_and_cleanup('my_module')\n            self.assertFileFound(zip_filename)\n            self.assertFileFound(zip_filename / 'my_module/__init__.py')\n            self.assertFileFound(zip_filename / 'my_module/module1.py')\n            self.assertFileNotFound(self.temporary_file('invalid.py'))\n\n\n", "    def test_iter_all_python_module_files_with_zip_module(self):\n        with tempfile.TemporaryDirectory() as tempdir:\n            zip_filename = self.temporary_file('my_module.zip')\n            with zipfile.ZipFile(zip_filename, 'w') as zf:\n                zf.write('__init__.py', arcname='my_module')\n            extend_sys_path([tempdir,])\n            self.import_and_cleanup('my_module')  # Import the module from the zip\n            self.assertFileFound(zip_filename)\n", "    def test_extra_files(self):\n        test_file = self.temporary_file('test.py')\n\n        with open(test_file, 'w') as f:\n            f.write('pass')\n        self.assertFileFound(test_file)\n\n        # WatchmanUnavailable will be thrown\n        with self.assertRaises(WatchmanUnavailable):\n            autoreload.get_reloader()  \n\n        # StatReloader is used because Watchman is unavailable\n        reloader = autoreload.StatReloader()\n\n        reloader.watch_file(test_file)\n        self.assertFileFound(test_file)\n        time.sleep(0.1)  \n        os.remove(test_file)\n        self.assertFileNotFound(test_file)\n\n\n", "    def test_iter_all_python_module_files_with_zipfile(self):\n        with tempfile.TemporaryDirectory() as tmpdir:\n            zip_filename = self.temporary_file('my_module.zip')\n            with zipfile.ZipFile(zip_filename, 'w') as zf:\n                zf.write('__init__.py', arcname='my_module/')\n                zf.write('module1.py', arcname='my_module/module1.py')\n            extend_sys_path([str(zip_filename)])\n            self.import_and_cleanup('my_module')\n            self.assertFileFound(zip_filename / '__init__.py')\n            self.assertFileFound(zip_filename / 'my_module/module1.py')\n"], "sample_283": ["    def test_runshell_handles_interrupt_signal(self):\n        with mock.patch('subprocess.Popen') as mock_popen:\n            # Mock the subprocess.Popen to simulate the psql process\n            mock_popen.return_value.communicate.side_effect = [\n                (b\"psql (12.10.0)\", b\" \")\n            ]\n            # Set up a signal handler that raises an exception when triggered\n            original_handler = signal.getsignal(signal.SIGINT)\n                raise ValueError(\"SIGINT received\")\n            signal.signal(signal.SIGINT, signal_handler)\n\n            # Call the runshell method, which should handle the signal and not raise an exception\n            client = DatabaseClient()\n            with self.assertRaises(ValueError):\n                client.runshell([])\n\n            # Restore the original signal handler\n            signal.signal(signal.SIGINT, original_handler)\n\n\n\n", "    def test_runshell_handles_sigint(self):\n        with mock.patch('subprocess.Popen') as mock_popen:\n            # Mock the Popen call to avoid actually executing psql.\n            mock_popen.return_value = mock.Mock()  \n\n            client = DatabaseClient()\n            client.runshell([])  \n\n            # Assert that the original SIGINT handler was restored.\n            signal.signal(signal.SIGINT, lambda sig, frame: None)  \n            self.assertNotEqual(signal.getsignal(signal.SIGINT), signal.SIG_IGN)\n\n", "    def test_runshell_handles_sigint(self):\n        with mock.patch('subprocess.run', return_value=None):\n            with mock.patch('builtins.sys.stdout', new_callable=mock.MagicMock()) as mock_stdout:\n                client = DatabaseClient()\n                client.runshell([''])\n\n                # Ensure SIGINT was handled by psql internally\n                self.assertEqual(mock_stdout.write.call_count, 0) \n", "    def test_runshell_handles_sigint(self):\n        with mock.patch('subprocess.run', spec=True) as mock_subprocess_run:\n            client = DatabaseClient()\n            client.runshell(['some', 'parameters'])\n\n            # Assert that the original SIGINT handler was restored.\n            self.assertEqual(signal.getsignal(signal.SIGINT), signal.SIG_DEFAULT)\n\n            # Assert that psql was called with the expected parameters.\n            mock_subprocess_run.assert_called_once_with(\n                ['psql', 'some', 'parameters'],\n                env={'PGPASSWORD': os.environ.get('PGPASSWORD')},\n                check=True\n            )\n", "    def test_runshell_handles_sigint(self):\n        with mock.patch('subprocess.Popen') as mock_popen:\n            sigint_handler = signal.getsignal(signal.SIGINT)\n            try:\n                # Mock the original SIGINT handler\n                signal.signal(signal.SIGINT, signal.SIG_IGN)\n                client = DatabaseClient()\n                client.runshell([])\n                self.assertEqual(mock_popen.called, True)  # Assert that psql was called\n                # Mock the psql process should not exit prematurely\n                mock_popen.return_value.returncode = 0  \n            finally:\n                # Restore the original SIGINT handler\n                signal.signal(signal.SIGINT, sigint_handler)\n\n\n\n"], "sample_217": ["    def test_media_for_widget_with_template_override(self):\n        class MyCustomWidget(TextInput):\n            template_name = 'my_custom_widget.html'\n\n        class MyForm(Form):\n            field = CharField()\n\n        form = MyForm()\n        instance = MyCustomWidget(attrs={'class': 'my-class'})\n        form.fields['field'].widget = instance\n        media = form.media\n\n        self.assertIn('my_custom_widget.html', str(media))\n        self.assertEqual(media['css'].count('my-class'), 1)\n\n\n\n", "    def test_split_hidden_datetime_widget_media(self):\n        class MyForm(Form):\n            date_time = SplitHiddenDateTimeWidget()\n\n        form = MyForm()\n        media = form.media\n        self.assertHTMLEqual(\n            str(media),\n            '\n            <link rel=\"stylesheet\" href=\"http://media.example.com/static/css/forms.css\"/>\n            <link rel=\"stylesheet\" href=\"http://media.example.com/static/css/widgets.css\"/>\n            <link rel=\"stylesheet\" href=\"http://media.example.com/static/css/input_widgets.css\"/>\n            <script src=\"http://media.example.com/static/js/forms.js\"></script>\n        '\n        )\n\n\n\n", "    def test_media_from_widget_with_templates(self):\n        class MyForm(Form):\n            text = CharField()\n\n                super().__init__(*args, **kwargs)\n                self.fields['text'].widget = TextInput(attrs={'class': 'my-text-input'})\n        form = MyForm()\n        self.assertEqual(form.media, Media(js=['http://media.example.com/static/my-text-input.js']))\n\n\n\n", "    def test_clearable_file_input_media(self):\n        class MyForm(Form):\n            file_upload = ClearableFileInput(widget=FileInput)\n\n        form = MyForm()\n        media = form.media\n        self.assertIn('static/css/clearable_fileinput.css', media.css)\n", "    def test_split_datetime_widget_media(self):\n        class MyForm(Form):\n            date = SplitDateTimeWidget(attrs={'class': 'my-date'})\n        form = MyForm()\n        media = form.media\n\n        # Check that the combined media includes the input templates.\n        self.assertIn('css', media)\n        self.assertIn('js', media)\n        self.assertIn('//media.example.com/static/django/forms/widgets/splitdatetime.css', media['css'])\n        self.assertIn('//media.example.com/static/django/js/jquery.js', media['js'])\n        self.assertIn('//media.example.com/static/django/js/date_input.js', media['js'])\n"], "sample_1088": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_963": ["    def test_restify_literal_type(self):\n        assert restify(Literal[int]) == ':py:class:`typing.Literal`[int]'\n        assert restify(Literal[str]) == ':py:class:`typing.Literal`[str]'\n        assert restify(Literal[bool]) == ':py:class:`typing.Literal`[bool]'\n        assert restify(Literal[None]) == ':py:class:`typing.Literal`[None]'\n        assert restify(Literal[MyInt]) == ':py:class:`typing.Literal`[MyInt]'\n\n\n", "    def test_restify_with_qualified_name(self):\n        assert restify(MyClass2) == ':py:class:`MyClass2`'\n        assert restify(MyList) == ':py:class:`List[T]`'\n        assert restify(MyInt) == ':py:class:`MyInt`'\n\n\n\n", "    def test_restify_with_union_args(self):\n        assert restify(Union[str, int]) == 'str | int'\n        assert restify(Union[int, str, NoneType]) == 'int | str | Optional[None]'\n        assert restify(Union[MyInt, int]) == 'MyInt | int'\n        assert restify(Union[T, int]) == 'T | int'\n        assert restify(Union[int, int]) == 'int'\n\n\n\n\n        \n", "    def test_restify_typing_Literal(self):\n        assert restify(typing.Literal[1]) == ':py:class:`typing.Literal[1]`'\n        assert restify(typing.Literal[str]) == ':py:class:`typing.Literal`'\n        assert restify(typing.Literal['abc']) == ':py:class:`typing.Literal`'\n        assert restify(typing.Literal[int, str]) == ':py:class:`typing.Literal`'\n\n\n\n", "    def test_restify_newtype(self):\n        assert restify(MyInt) == ':py:class:`MyInt`'\n        assert restify(NewType('MyEnum', int)) == ':py:class:`MyEnum`'\n        assert restify(NewType('MyOptional', Optional[str])) == ':py:class:`~typing.Optional`\\\\ [:py:class:`str`]\n\n\n\n"], "sample_753": ["def test_logistic_regression_cv_multiclass_ovr():\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5,\n                               n_classes=3, random_state=42)\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n    model = LogisticRegressionCV(Cs=np.logspace(-2, 2, 10),\n                                 penalty='l2', solver='liblinear',\n                                 multi_class='ovr', cv=cv, random_state=42)\n\n    model.fit(X, y)\n\n    assert_array_equal(model.coef_.shape, (3, 10))\n    assert_array_equal(model.intercept_.shape, (3,))\n    assert_array_equal(model.Cs_.shape, (10,))\n    assert_array_equal(model.coefs_paths_.shape, (3, 5, 10, 10))\n    assert_array_equal(model.scores_.shape, (3, 5, 10))\n    assert_array_equal(model.n_iter_.shape, (3, 5, 10))\n\n\n", "    def test_cv_ovr(self):\n        X, y = make_classification(n_samples=100, n_features=2, n_informative=2,\n                                   n_redundant=0, n_classes=2, random_state=42)\n        clf = LogisticRegressionCV(Cs=10, cv=5, penalty='l2', solver='lbfgs',\n                                   scoring='accuracy', refit=True)\n        clf.fit(X, y)\n        assert_greater(clf.n_iter_[-1].max(), 0) \n        assert_true(all(np.isnan(coefs).any() == False for coefs in clf.coefs_paths_.values()))\n\n\n\n", "    def test_logistic_regression_cv_fit_refit(self):\n        X, y = make_classification(n_samples=100, random_state=0)\n        cv = StratifiedKFold(n_splits=5, random_state=0)\n        clf = LogisticRegressionCV(Cs=10, cv=cv, scoring='accuracy', refit=True)\n        clf.fit(X, y)\n\n        assert_equal(clf.refit, True)\n        best_params = clf.best_params_\n        best_score = clf.best_score_\n\n        # Make sure refitting works\n        clf2 = LogisticRegression(C=best_params['C'],\n                                   random_state=0)\n        clf2.fit(X, y)\n        assert_allclose(clf.coef_, clf2.coef_, atol=1e-3)\n        assert_allclose(clf.intercept_, clf2.intercept_, atol=1e-3)\n\n        yield test_best_params\n", "    def test_class_weight_balanced(self, class_weight):\n        X, y = make_classification(n_samples=100, n_features=2, n_informative=2,\n                                  n_redundant=0, n_classes=2, weights=[0.6, 0.4],\n                                  random_state=42)\n\n        clf = LogisticRegressionCV(Cs=10, class_weight=class_weight)\n        clf.fit(X, y)\n\n        # Balanced class weights should lead to similar coefficients\n\n        if class_weight == \"balanced\":\n            # Check that the coefficients are similar when using balanced weights\n            weights_balanced = compute_class_weight(\"balanced\", np.arange(2), y)\n            weights_balanced_dict = dict(enumerate(weights_balanced))\n            assert_allclose(clf.coef_, LogisticRegression(class_weight=weights_balanced_dict).fit(X, y).coef_)\n        else:\n            # Coefficients should be similar for None and explicitly specified weights\n            assert_allclose(clf.coef_, LogisticRegression(class_weight=class_weight).fit(X, y).coef_)\n\n\n", "def test_cv_fit_refit():\n    X, y = make_classification(n_samples=1000, n_features=5, random_state=0)\n    cv = StratifiedKFold(n_splits=3)\n    clf = LogisticRegressionCV(Cs=0.5, cv=cv, penalty='l2', refit=True,\n                              scoring='accuracy')\n\n    clf.fit(X, y)\n\n    # Check that refitting was done\n    assert_equal(clf.C_, clf.Cs_[np.argmax(clf.scores_['accuracy'], axis=0)])\n\n    # If refit is False,\n    with pytest.raises(ValueError) as e:\n        clf = LogisticRegressionCV(Cs=0.5, cv=cv, penalty='l2', refit=False,\n                                  scoring='accuracy')\n        clf.fit(X, y)\n    assert_equal(str(e.value), \"Can't fit without refit\")\n\n\n\n"], "sample_77": ["compilation error", "compilation error", "    def test_urlize(self):\n        self.check_output(urlize, 'This is a test http://example.com url.')\n        self.check_output(urlize, 'This is a test https://example.com url.')\n        self.check_output(urlize, 'This is a test www.example.com url.')\n        self.check_output(urlize, 'This is a test example.com url.')\n        self.check_output(urlize, 'This is a test example@example.com url.')\n        self.check_output(urlize, 'This is a test <a href=\"http://example.com\">url</a>.')\n\n        self.check_output(urlize, 'This is a test http://example.com/path/to/page url.',\n                         'This is a test <a href=\"http://example.com/path/to/page\">http://example.com/path/to/page</a> url.')\n        self.check_output(urlize, 'This is a test https://sub.example.com/path/to/page url.',\n                         'This is a test <a href=\"https://sub.example.com/path/to/page\">https://sub.example.com/path/to/page</a> url.')\n        self.check_output(urlize, 'This is a test example.com/path/to/page url.',\n                         'This is a test <a href=\"http://example.com/path/to/page\">example.com/path/to/page</a> url.')\n        self.check_output(urlize, 'This is a test example@example.com url.',\n                         'This is a test <a href=\"mailto:example@example.com\">example@example.com</a> url.')\n\n        self.check_output(urlize, 'This is a test http://example.com/path/to/very/long/url/that/needs/to/be/truncated url.', trim_url_limit=20) \n        self.check_output(urlize, 'This is a test http://example.com?param=value url.', trim_url_limit=20) \n", "    def test_urlize(self):\n        self.check_output(urlize, \"This is a http://www.example.com link.\")\n        self.check_output(urlize, \"This is a http://example.com link.\")\n        self.check_output(urlize, \"This is a https://www.example.com link.\")\n        self.check_output(urlize, \"This is a www.example.com link.\")\n        self.check_output(urlize, \"This is a  https://www.example.com/long/long/really-very-long-url link.\")\n        self.check_output(urlize, \"This is an email: example@example.com.\", autoescape=False)\n        self.check_output(urlize, \"This is an email with a trailing dot: example@example.com.\", autoescape=False)\n        self.check_output(urlize, \"This is a long url: http://www.example.com/very-long-url-that-should-be-truncated\", trim_url_limit=20)\n        self.check_output(urlize, \"This is a long url: http://www.example.com/very-long-url-that-should-be-truncated\", trim_url_limit=60)\n\n\n", "    def test_urlize_with_idna(self):\n        self.check_output(urlize, \"http://xn--s3j.com\")\n        self.check_output(urlize, \"https://xn--p1ai/\")\n        self.check_output(urlize, 'https://www.xn--80aig.xn--p1ai/')\n\n\n\n"], "sample_160": ["    def test_large_number(self):\n        # Test formatting of large numbers, including scientific notation treatment\n        large_number = 1.23456789e200\n        formatted_number = nformat(large_number, decimal_sep='.', decimal_pos=2, use_l10n=False)\n        self.assertEqual(formatted_number, '1.23e+200')\n\n        large_number_str = '1.23456789e200'\n        formatted_number_str = nformat(large_number_str, decimal_sep='.', decimal_pos=2, use_l10n=False)\n        self.assertEqual(formatted_number_str, '1.23e+200')\n\n        # Test that large numbers with too many digits don't cause errors\n        large_number_too_many_digits = float_info.max\n        formatted_number_too_many_digits = nformat(large_number_too_many_digits, decimal_sep='.', decimal_pos=2, use_l10n=False)\n        self.assertGreater(len(formatted_number_too_many_digits), 0)\n", "    def test_large_numbers(self):\n        for number in [\n            Decimal('1.234567890123456789012345678901234567890'),\n            Decimal('1234567890123456789012345678901234567890'),\n            Decimal('1e200'),\n        ]:\n            formatted_number = nformat(number, decimal_sep='.', decimal_pos=8)\n            self.assertEqual(formatted_number, str(number)[:9].replace('.', ','))\n            formatted_number = nformat(number, decimal_sep='.', decimal_pos=None)\n            self.assertEqual(formatted_number, str(number))\n", "    def test_format_large_numbers(self):\n        number = float('1.23456789012345678901234567890e+200')\n        formatted_number = nformat(number, decimal_sep=',', decimal_pos=None, grouping=3, thousand_sep=' ')\n        self.assertEqual(formatted_number, '1.23456789012345678901234567890e+200')\n", "    def test_large_floats_in_scientific_notation(self):\n        number = float('1.2345678901234567890123456789e-300')\n        self.assertEqual(nformat(number, decimal_sep='.'), '1.2345678901234567890123456789e-300')\n        self.assertEqual(nformat(number, decimal_sep=',', thousand_sep=' '), '1.2345678901234567890123456789e-300')\n        self.assertEqual(nformat(number, decimal_sep='.', decimal_pos=10), '1.23456789012e-290')\n        self.assertEqual(nformat(number, decimal_sep=',', thousand_sep=' ', decimal_pos=10), '1.23456789012e-290')        \n\n\n\n", "    def test_large_decimals(self):\n        # Test large Decimal values using scientific notation\n        large_number = Decimal('1.2345678901234567890123456789012345678901234567890')\n        formatted_number = nformat(large_number, decimal_sep='.', decimal_pos=10, force_grouping=True)\n        self.assertEqual(formatted_number, '1.2345678901e+20')\n\n        # Ensure scientific notation is handled correctly with grouping\n        large_number = Decimal('12345678901234567890123456789012345678901234567890')\n        formatted_number = nformat(large_number, decimal_sep='.', decimal_pos=10, grouping=(3, 2, 0), force_grouping=True)\n        self.assertEqual(formatted_number, '1.2345678901e+20')\n\n\n\n"], "sample_6": ["    def test_angle_formats(self, angle_str):\n        \"\"\"Test parsing of angle strings.\"\"\"\n        a = Angle(angle_str)\n        assert isinstance(a, Angle)\n", "    def test_wrap_at_inplace(self):\n        a = Angle([100, 200, 300] * u.deg)\n        with ExitStack() as stack:\n            stack.enter_context(np.errstate(invalid='ignore'))\n            a.wrap_at(360 * u.deg, inplace=True)\n            npt.assert_equal(a.degree, np.array([100, 200, 300]))\n", "    def test_wrap_at_inplace(self):\n        a = Angle(100 * u.deg)\n        a_wrapped = a.copy()\n        a_wrapped.wrap_at(360 * u.deg, inplace=True)\n        assert_allclose(a_wrapped, Angle(100 * u.deg))\n\n        a = Angle(100 * u.deg)\n        a_wrapped = a.copy()\n        a_wrapped.wrap_at(180 * u.deg)\n        assert_allclose(a_wrapped, Angle(100 * u.deg))  # Inplace is False by default\n", "    def test_to_string_format(self, format):\n        a = Angle(12.3456789 * u.deg)\n        assert a.to_string(format=format) != a.to_string()\n        assert angle.to_string(format=format) != a.to_string()\n", "    def test_wrap_angle_inplace(self):\n        a = Angle(350 * u.deg)\n        a_wrapped = a.copy()\n        a_wrapped.wrap_angle = 180 * u.deg\n        a_wrapped.wrap_at(a_wrapped.wrap_angle, inplace=True)\n        assert a_wrapped.degree == np.array([-170.])\n"], "sample_103": ["compilation error", "    def test_avg_with_star(self):\n        # Regression test for #23442\n        with self.assertNumQueries(1):\n            avg_stars = Book.objects.values('publisher').annotate(\n                avg_rating=Avg('rating')\n            ).order_by('publisher')\n        self.assertEqual(avg_stars, [\n            {'publisher': self.p1, 'avg_rating': 4.375},\n            {'publisher': self.p2, 'avg_rating': 3.0},\n            {'publisher': self.p3, 'avg_rating': 4.0},\n            {'publisher': self.p4, 'avg_rating': 6.25},\n            {'publisher': self.p5, 'avg_rating': None},\n        ])\n", "    def test_aggregate_with_filter(self):\n        with self.assertNumQueries(1):\n            result = Author.objects.filter(age__gt=30).aggregate(\n                avg_age=Avg('age'),\n                count_books=Count('books')\n            )\n        self.assertDictEqual(result, {\n            'avg_age': Decimal('35.75'),\n            'count_books': 6,\n        })\n\n", "    def test_aggregate_with_filter(self):\n        with self.subTest():\n            with CaptureQueriesContext() as captured:\n                Avg(Book.rating, filter=Q(is_bestselling=True)).aggregate('avg')\n            self.assertEqual(captured.captured_queries, [\n                r\"SELECT AVG(T1.rating) AS 'avg' FROM django_book AS T1 WHERE T1.is_bestselling = %s\"\n            ]) \n\n\n", "    def test_aggregate_with_distinct_and_filter(self):\n        with self.subTest('Count'):\n            qs = Author.objects.filter(name__startswith='A').values_list('id', distinct=True).annotate(cnt=Count('friends', distinct=True))\n            with CaptureQueriesContext() as cm:\n                result = qs.first()\n            self.assertEqual(cm.query_count, 1)\n            self.assertEqual(result, (self.a1.id, 2))\n\n        with self.subTest('Avg'):\n            qs = Book.objects.filter(rating__gt=4).annotate(avg_rating=Avg('rating', distinct=True))\n            with CaptureQueriesContext() as cm:\n                result = qs.first()\n            self.assertEqual(cm.query_count, 1)\n            self.assertAlmostEqual(result.avg_rating, 4.5)\n\n        with self.subTest('Sum'):\n            qs = Book.objects.filter(publisher_id=self.p1.id).annotate(total_price=Sum('price', distinct=True))\n            with CaptureQueriesContext() as cm:\n                result = qs.first()\n            self.assertEqual(cm.query_count, 1)\n            self.assertEqual(result.total_price, Decimal('59.69'))\n\n\n\n"], "sample_709": ["    def test_collect_by_name_missing(self, testdir):\n        modcol = testdir.getmodulecol(\n            \"def test_func(): pass\",\n        )\n        with pytest.raises(ValueError):\n            modcol.collect_by_name(\"missing_func\")\n", "    def test_collect_by_name_in_same_module(self, testdir):\n        p = testdir.makepyfile(\n            \"\"\"\n                pass\n            \"\"\"\n        )\n        reprec = testdir.inline_run(p, \"-l\")\n        reprec.assert_outcomes(passed=1)\n\n        items = reprec.getcalls(\"pytest_itemcollected\")\n        assert len(items) == 1\n        item = items[0].item\n        assert item.name == \"test_one\"\n        assert item.parent.name == \"__main__\"\n\n\n\n", "def test_runpython_c(pytester: Pytester) -> None:\n    pytester.runpython_c(\"print('hello')\", capture=True).assert_equal(\n        b\"hello\\n\"\n    )\n\n    pytester.runpython_c(\"import sys; sys.exit(1)\", capture=True).assert_equal(\n        b\"\"\n    )\n\n\n\n", "    def test_spawn_with_timeout(self, testdir):\n        p = testdir.spawn(\"echo 'hello world'\", expect_timeout=0.1)\n        with pytest.raises(pexpect.TIMEOUT):\n            p.expect(\"hello world\", timeout=0.05)\n    ", "    def test_runpytest_subprocess_with_plugins_collectonly(self, testdir):\n        \"\"\"Test runpytest_subprocess with plugins and --collectonly.\"\"\"\n        testdir.makeconftest(\n            \"\"\"\n        @pytest.hookimpl\n            yield \"my_plugin_1\"\n            yield \"my_plugin_2\"\n        \"\"\"\n        )\n        result = testdir.runpytest(\"--collectonly\",  _request=self.request)\n        result.stdout.fnmatch_lines(\n            [\n                \"*my_plugin_1*\",\n                \"*my_plugin_2*\",\n            ]\n        )\n\n\n\n"], "sample_439": ["    def test_hidden_fields(self):\n        form = Person(initial={'first_name': 'John', 'last_name': 'Doe'})\n        hidden_fields = form.hidden_fields()\n        self.assertEqual(len(hidden_fields), 0)\n\n        form = Person(\n            initial={'first_name': 'John', 'last_name': 'Doe'},\n            data={'first_name': 'Jane', 'last_name': 'Doe'},\n        )\n        hidden_fields = form.hidden_fields()\n        self.assertEqual(len(hidden_fields), 0)\n\n\n", "    def test_ordering_fields(self):\n        class MyForm(Form):\n            name = CharField()\n            age = IntegerField()\n            country = ChoiceField(choices=[(\"USA\", \"United States\"), (\"CAN\", \"Canada\")], widget=RadioSelect)\n            order = [\"name\", \"country\", \"age\"]\n\n        form = MyForm()\n        expected_order = [\"name\", \"country\", \"age\"]\n        self.assertEqual(list(form.fields), expected_order)\n\n        # Test ordering with unknown field\n        class MyForm2(Form):\n            name = CharField()\n            age = IntegerField()\n            country = ChoiceField(choices=[(\"USA\", \"United States\"), (\"CAN\", \"Canada\")], widget=RadioSelect)\n            order = [\"unknown\", \"name\", \"age\"]  # unknown field\n\n        form2 = MyForm2()\n        self.assertEqual(list(form2.fields), [\"name\", \"age\", \"country\"])\n\n\n\n", "    def test_order_fields(self):\n        class FooForm(Form):\n            name = CharField(label=\"Name\")\n            age = IntegerField(label=\"Age\")\n            email = EmailField(label=\"Email\")\n\n        # Test order_fields with field_order\n        form = FooForm(field_order=[\"age\", \"email\", \"name\"])\n        self.assertEquals(list(form.fields), [\"age\", \"email\", \"name\"])\n\n        # Test order_fields with no field_order\n        form = FooForm()\n        self.assertEquals(list(form.fields), [\"name\", \"age\", \"email\"])\n\n        # Test order_fields with unknown fields\n        form = FooForm(field_order=[\"name\", \"unknown\", \"age\"])\n        self.assertEquals(list(form.fields), [\"name\", \"age\"])\n", "    def test_form_field_ordering(self):\n        class OrderedForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n            field3 = CharField()\n\n                super().__init__(*args, **kwargs)\n                self.order_fields([\"field2\", \"field3\", \"field1\"])\n\n        form = OrderedForm()\n        self.assertEqual(list(form.fields), [\"field2\", \"field3\", \"field1\"])\n\n\n\n", "    def test_add_prefix_special_characters(self):\n        form = FrameworkForm(data=MultiValueDictLike({\"name\": \"John Doe\"}))\n        self.assertEqual(form.add_prefix(\"name\"), \"name\")\n        self.assertEqual(form.add_prefix(\"language\"), \"language\")\n        self.assertEqual(form.add_prefix(\"invalid\"), \"invalid\")\n\n\n"], "sample_498": ["    def test_legend_loc_codes(self, loc_val):\n        fig, ax = plt.subplots(1, 1)\n        fig.legend([], [], loc=loc_val, title=\"test\")\n        with rc_context({'axes.titlesize': 18}):\n            \n            plt.show()  \n\n        # check figure size\n        assert fig.get_figwidth() > 0.1\n        assert fig.get_figheight() > 0.1\n", "    def test_legend_handler_mapping(self):\n        fig, ax = plt.subplots(1, 1)\n\n        # Create some artists with custom handles\n        lines = ax.plot([1, 2, 3], [4, 5, 6])\n        patches = ax.add_patch(mpatches.Rectangle((0, 0), 1, 1, color='red'))\n\n        # Create a custom Handler class\n        class MyHandler(HandlerTuple):\n                return super().create_artists(legend, orig_handle, fontsize, with_label)\n\n        # Define the custom handler mapping\n        legend_handler_map = {\n            mlines.Line2D: MyHandler(),\n            mpatches.Rectangle: mlegend.HandlerPatch()\n        }\n\n        # Add a legend using the custom mapping\n        ax.legend([lines[0], patches], ['Line', 'Patch'],\n                  handler_map=legend_handler_map)\n\n        plt.show()\n", "    def test_draggable(self):\n        fig, ax = plt.subplots()\n        line1, = ax.plot([1, 2, 3, 4])\n        line2, = ax.plot([5, 6, 7, 8])\n\n        legend = fig.legend([line1, line2], ['Line 1', 'Line 2'],\n                            loc='best')\n        draggable = legend.set_draggable(True)\n\n        with pytest.raises(AttributeError):\n            draggable.update_bbox()\n        \n        legend.set_draggable(False)\n        assert not legend.get_draggable() \n", "    def test_draggable_legend_updates(self):\n        fig, ax = plt.subplots()\n\n        line, = ax.plot([1, 2, 3])\n        line.set_label('line')\n\n        legend = ax.legend()\n        legend.set_draggable(state=True)\n\n        # Get initial location\n        bbox = legend._get_bbox_to_anchor()\n        new_bbox = bbox.translate(0.1, 0.1)\n\n        with mpl.rc_context({'legend.draggable': True}):\n            legend.get_draggable().move(new_bbox.xmin, new_bbox.ymin)\n            ax.relim()\n            ax.autoscale_view()\n\n        # Check updated location\n        updated_bbox = legend._get_bbox_to_anchor()\n        assert_allclose(updated_bbox, new_bbox)\n\n", "    def test_legend_bbox_to_anchor(self):\n        fig, ax = plt.subplots()\n        line, = ax.plot([1, 2, 3])\n        ax.legend([line], ['My Line'], bbox_to_anchor=(0, 1), loc=\"upper left\")\n        plt.show()\n"], "sample_430": ["compilation error", "compilation error", "    def test_model_state_object_with_multiple_unique_together(self):\n        self.assertEqual(\n            book_unique_together_4.unique_together, {(\"title\", \"newfield2\")}\n        )\n        self.assertEqual(\n            book_unique_together_3.unique_together, {(\"title\", \"newfield\")}\n        )\n", "    def test_model_name_creation_from_states(self):\n        self.assertEqual(\n            testapp.Author._meta.verbose_name,\n            \"Author\",\n        )\n        self.assertEqual(\n            testapp.AAuthorProxyProxy._meta.verbose_name,\n            \"Super Author\",\n        )\n    ", "    def test_duplicate_unique_together(self):\n        with self.assertRaises(ValueError):\n            self.model_state.create(\n                field1=\"test1\", field2=\"test1\"\n            )\n        with self.assertRaises(ValueError):\n            self.model_state.create(\n                field1=\"test1\", field2=\"test2\"\n            )\n\n\n\n"], "sample_629": ["def test_expand_modules_namespace():\n    modules = ['package1.namespace', 'package2']\n    ignore_list = []\n    ignore_list_re = []\n    ignore_list_paths_re = []\n    result, errors = expand_modules(modules, ignore_list, ignore_list_re, ignore_list_paths_re)\n    assert len(result) == 2\n    assert result[0] == {'path': '', 'name': 'package1.namespace', 'isarg': True, 'basepath': '', 'basename': 'package1.namespace'}\n    assert result[1] == {'path': '', 'name': 'package2', 'isarg': True, 'basepath': '', 'basename': 'package2'}\n    assert len(errors) == 0\n\n\n\n\n", "def test_expand_modules_ignore_nested_files():\n    ignore_list = [\"nested/\"]\n    ignore_list_re = [re.compile(r\"nested.*\")]\n    files_or_modules = [\"nested/file1.py\", \"nested/file2.py\", \"top/file3.py\"]\n\n    result, errors = expand_modules(\n        files_or_modules, ignore_list, ignore_list_re, []\n    )\n    assert len(result) == 1\n    assert result[0][\"path\"] == \"top/file3.py\"\n    assert len(errors) == 0\n\n\n\n", "    def test_expand_modules_ignore_list(\n        files, ignore_list, ignore_list_re, ignore_list_paths_re, expected_result", "compilation error", "    def test_expand_modules_with_import_error():\n        files_or_modules = [\"non_existent_module\"]\n        ignore_list = []\n        ignore_list_re = []\n        ignore_list_paths_re = []\n        result, errors = expand_modules(\n            files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re\n        )\n        assert len(result) == 0\n        assert len(errors) == 1\n        assert errors[0][\"key\"] == \"fatal\"\n        assert errors[0][\"mod\"] == \"non_existent_module\"\n        assert isinstance(errors[0][\"ex\"], ImportError)\n"], "sample_917": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_361": ["    def test_urlize_with_mailto(self):\n        self.check_output(urlize, 'example@example.com', 'example@example.com')\n        self.check_output(urlize, 'mailto:example@example.com', 'mailto:example@example.com')\n\n\n\n", "    def test_urlize(self):\n        self.check_output(urlize, \"http://www.example.com\", \"http://www.example.com\")\n        self.check_output(urlize, \"https://www.example.com\", \"https://www.example.com\")\n        self.check_output(urlize, \"email@example.com\", \"email@example.com\")\n        self.check_output(urlize, \"This is a link: http://www.example.com\",\n                         \"This is a link: <a href=\\\"http://www.example.com\\\"\\\" rel=\\\"nofollow\\\">http://www.example.com</a>\")", "    def test_urlize(self):\n        self.check_output(urlize, \"http://example.com\", '<a href=\"http://example.com\" rel=\"nofollow\">http://example.com</a>')\n        self.check_output(urlize, \"www.example.com\", '<a href=\"http://www.example.com\" rel=\"nofollow\">www.example.com</a>')\n        self.check_output(urlize, \"mailto:john.doe@example.com\", '<a href=\"mailto:john.doe@example.com\">john.doe@example.com</a>')\n        self.check_output(urlize, \"Some text with a URL: https://example.com\", 'Some text with a URL: <a href=\"https://example.com\" rel=\"nofollow\">https://example.com</a>')\n        self.check_output(urlize, \"Some text with a URL: http://example.com/\", '<a href=\"http://example.com/\" rel=\"nofollow\">http://example.com/</a>')\n        self.check_output(urlize, \"Some text with a URL: https://example.com/path/to/page\", '<a href=\"https://example.com/path/to/page\" rel=\"nofollow\">https://example.com/path/to/page</a>')\n        self.check_output(urlize, \"Some text with a URL: https://example.com?key=value\", '<a href=\"https://example.com?key=value\" rel=\"nofollow\">https://example.com?key=value</a>')\n        self.check_output(urlize, \"Some text with a URL: https://example.com#fragment\", '<a href=\"https://example.com#fragment\" rel=\"nofollow\">https://example.com#fragment</a>')\n        self.check_output(urlize, \"Some text with a URL: http://example.com/path/to/page?key=value#fragment\", '<a href=\"http://example.com/path/to/page?key=value#fragment\" rel=\"nofollow\">http://example.com/path/to/page?key=value#fragment</a>')\n        self.check_output(urlize, \"Some text with a URL", "    def test_urlize(self):\n        self.check_output(urlize, \"http://www.example.com\", \"<a href=\\\"http://www.example.com\\\">http://www.example.com</a>\")\n        self.check_output(urlize, \"example@example.com\", \"<a href=\\\"mailto:example@example.com\\\">example@example.com</a>\")\n        self.check_output(urlize, \"www.example.com\", \"<a href=\\\"http://www.example.com\\\">www.example.com</a>\")\n        self.check_output(urlize, \"some text with http://www.example.com in it\", \"some text with <a href=\\\"http://www.example.com\\\">http://www.example.com</a> in it\")\n        self.check_output(urlize, \"http://www.example.com http://www.google.com\", \"<a href=\\\"http://www.example.com\\\">http://www.example.com</a> <a href=\\\"http://www.google.com\\\">http://www.google.com</a>\")\n        self.check_output(urlize, \"some text with [http://www.example.com] in it\", \"some text with <a href=\\\"http://www.example.com\\\">http://www.example.com</a> in it\")\n\n        self.check_output(urlize, \"http://www.example.com\\n\", \"<a href=\\\"http://www.example.com\\\">http://www.example.com</a>\\n\")\n", "    def test_urlize_trim_url_limit(self):\n        self.check_output(urlize, \"https://www.long-url.com/this-is-a-really-long-url-that-is-too-long-for-our-purposes\",\n                          trim_url_limit=15,\n                          \"https://www.long-url.com/\u2026\",\n                          autoescape=False)\n\n        self.check_output(urlize, \"https://www.long-url.com/this-is-a-really-long-url-that-is-too-long-for-our-purposes\",\n                          trim_url_limit=50,\n                          \"https://www.long-url.com/this-is-a-really-long-url-that-is-too-long-for-our-purposes\",\n                          autoescape=False)\n\n\n\n"], "sample_1124": ["    def test_sfield_with_symbol_and_expr():\n        x, y = symbols('x,y')\n        K, f = sfield((x*log(x) + 4*x**2)*exp(1/x + log(x)/3)/x**2)\n        assert isinstance(K, FracField)\n        assert f.sort_key() == (tuple(x.as_base_exp()),)\n        assert f.parent() == K\n        assert f.denom == K.ring.one\n\n\n\n        \n", "    def test_sfield_inject_namespace(xfail=''):\n        x, y = symbols('x y')\n        K, f = sfield((x*log(x) + 4*x**2)*exp(1/x + log(x)/3)/x**2)\n        assert x in K\n        assert y in K\n        assert f == K(f)\n", "    def test_sfield_expr_order(self):\n        x, y = symbols('x,y')\n        K, expr = sfield([(x*log(x) + 4*x**2)*exp(1/x + log(x)/3)/x**2], x, y)\n        self.assertEqual(K.order, lex)\n        self.assertEqual(expr, (4*x**2*(exp(1/x)) + x*(exp(1/x))*(log(x)))/((x**(1/3))**5))\n\n\n", "    def test_sfield_basic():\n        from sympy.polys.domains import PolynomialRing\n\n        x = symbols(\"x\")\n        K, f = sfield((x * log(x) + 4*x**2)*exp(1/x + log(x)/3)/x**2)\n\n        assert isinstance(K, FracField)\n        assert K.symbols == (x,)\n        assert K.domain == PolynomialRing(ZZ, x).domain\n        assert K.order == lex\n\n        assert f == (4*x**2*(exp(1/x)) + x*(exp(1/x))*(log(x)))/((x**(1/3))**5)\n        assert K(f) == f\n\n\n\n", "    def test_sfield_symbols_as_exprs():\n        x, y = symbols(\"x y\")\n        K, f = sfield((x*log(x) + 4*x**2)*exp(1/x + log(x)/3)/x**2)\n        assert f.as_expr(x, y) == (x*log(x) + 4*x**2)*exp(1/x + log(x)/3)/x**2\n"], "sample_1021": ["    def test_pow_cos_sin():\n        q = Quaternion(1, 2, 3, 4)\n        result = q.pow_cos_sin(4)\n        expected = 900*cos(4*acos(sqrt(30)/30)) + 1800*sqrt(29)*sin(4*acos(sqrt(30)/30))/29*I + 2700*sqrt(29)*sin(4*acos(sqrt(30)/30))/29*j + 3600*sqrt(29)*sin(4*acos(sqrt(30)/30))/29*k\n        assert result == expected\n", "    def test_to_rotation_matrix_complex(self):\n        q = Quaternion(1, 2*I, 3*I, 4*I, real_field = False)\n        m = q.to_rotation_matrix()\n        assert isinstance(m, Matrix)\n        assert m.shape == (4, 4)\n", "    def test_to_rotation_matrix_identity(self):\n        q = Quaternion(1, 0, 0, 0)\n        m = q.to_rotation_matrix()\n        expected = Matrix([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])\n        self.assertEqual(m, expected)\n\n", "    def test_Rotate_Point_with_axis_angle():\n        from sympy.algebras.quaternion import Quaternion\n        q = Quaternion(cos(pi/4), sin(pi/4), 0, 0)\n        point = (1, 0, 0)\n        q2 = Quaternion.from_axis_angle((0, 1, 0), pi/4)\n        assert Quaternion.rotate_point(point, q) == Quaternion.rotate_point(point, q2) \n", "    def test_from_rotation_matrix_complex(self):\n        M = Matrix([[cos(x), -sin(x), 0], [sin(x), cos(x), 0], [0, 0, 1]])\n        q = Quaternion.from_rotation_matrix(M)\n        self.assertEqual(q.a, cos(x))\n        self.assertEqual(q.b, -sin(x))\n        self.assertEqual(q.c, 0)\n        self.assertEqual(q.d, 0)\n"], "sample_490": ["    def test_unique_constraint_expression(self):\n        with atomic():\n            Product.objects.create(name=\"Product A\", description=\"A description\")\n            with self.assertRaises(IntegrityError):\n                Product.objects.create(name=F(\"name\"), description=\"Another description\")\n", "    def test_unique_constraint_deconstructs_correctly(self):\n        constraint = UniqueConstraint(\n            fields=[\"name\", \"description\"],\n            deferrable=Deferrable.IMMEDIATE,\n            name=\"unique_name_and_description\",\n            violation_error_message=_(\"Unique constraint 'unique_name_and_description' violated.\"),\n        )\n        path, args, kwargs = constraint.deconstruct()\n        self.assertEqual(path, \"django.db.models.constraints.UniqueConstraint\")\n        self.assertEqual(args, ())\n        self.assertEqual(kwargs, {\n            \"fields\": [\"name\", \"description\"],\n            \"deferrable\": Deferrable.IMMEDIATE,\n            \"name\": \"unique_name_and_description\",\n            \"violation_error_message\": _(\"Unique constraint 'unique_name_and_description' violated.\"),\n        })\n        reconstructed_constraint = UniqueConstraint(**kwargs)\n\n        self.assertEqual(constraint, reconstructed_constraint)\n\n\n\n", "    def test_unique_constraint_deconstructor(self):\n        constraint = UniqueConstraint(\n            fields=(\"name\",),\n            name=\"unique_name\",\n            deferrable=Deferrable.DEFERRED,\n        )\n        path, args, kwargs = constraint.deconstruct()\n        self.assertEqual(path, \"django.db.models.constraints.UniqueConstraint\")\n        self.assertEqual(args, ())\n        self.assertEqual(kwargs, {\"fields\": (\"name\",), \"name\": \"unique_name\", \"deferrable\": \"deferred\"})\n\n        reconstructed_constraint = UniqueConstraint(**kwargs)\n        self.assertEqual(constraint, reconstructed_constraint)\n", "    def test_unique_constraint_deferrable(self):\n        with atomic():\n            # Create a table with a deferred unique constraint.\n            Product.objects.create(name='Product A', price=100)\n            with self.assertRaises(IntegrityError):\n                Product.objects.create(name='Product A', price=100)\n            constraints = get_constraints('django_test_product')\n            deferred_constraint = next(\n                (\n                    constraint\n                    for constraint in constraints\n                    if constraint['name'] == 'unique_name' and constraint['deferrable']\n                ),\n                None,\n            )\n            self.assertIsNotNone(deferred_constraint)\n            self.assertEqual(deferred_constraint['deferrable'], 'deferred')\n\n            # Check that we can add new records after committing.\n            Product.objects.create(name='Product B', price=100)\n", "    def test_check_constraint_validation(self):\n        with atomic():\n            Product.objects.create(name=\"Test Product 1\", price=10)\n            with self.assertRaises(ValidationError):\n                Product.objects.create(name=\"Test Product 1\", price=10)\n\n\n"], "sample_55": ["    def test_admin_change_form_with_inline_fields(self):\n        response = self.client.post(\n            reverse('admin:customadmin_section_change', args=(self.s1.pk,)),\n            self.inline_post_data,\n        )\n\n        # Assert the page redirects to change view\n        assert response.status_code == 302\n\n        response = self.client.get(reverse('admin:customadmin_section_change', args=(self.s1.pk,)))\n        self.assertEqual(response.status_code, 200)\n\n        # Assert the number of articles\n        self.assertEqual(Article.objects.count(), 6)\n        self.assertEqual(response.context['object'].articles.count(), 6)\n\n\n", "    def test_admin_change_form_renders_fields_correctly(self):\n        response = self.client.get(reverse('admin:customadmin_article_change', args=(self.a1.pk,)))\n        fields = self.get_admin_form_fields(response)\n\n        expected_fields = [\n            {'name': 'title', 'label': 'Title', 'widget': {'type': 'text', 'attrs': {'class': 'vTextField', 'size': '60'}}},\n            {'name': 'content', 'label': 'Content', 'widget': {'type': 'textarea', 'attrs': {'class': 'vTextField', 'rows': '10', 'cols': '60'}}},\n            {'name': 'date', 'label': 'Date', 'widget': {'type': 'calendar', 'attrs': {'class': 'vDateField'}}},\n            {'name': 'section', 'label': 'Section', 'widget': {'type': 'select', 'attrs': {'class': 'vSelect'}}},\n        ]\n        self.assertEqual(fields, expected_fields)\n", "    def test_inline_form_fields(self):\n        response = self.client.get(reverse('admin:customadmin_article_change', args=(self.a1.pk,)))\n        fields = self.get_admin_form_fields(response)\n        self.assertIn('article_set', [f.field['name'] for f in fields])\n        response = self.client.post(\n            reverse('admin:customadmin_article_change', args=(self.a1.pk,)),\n            self.inline_post_data,\n            follow=True\n        )\n        fields = self.get_admin_form_fields(response)\n        self.assertNotIn('article_set', [f.field['name'] for f in fields])\n\n", "    def test_admin_inline_formset_data(self):\n        response = self.client.get(reverse('admin:customadmin_city_add'))\n\n        self.assertIn(b'inline_formset_data', response.content)\n        data = json.loads(response.content.split(b'inline_formset_data: ')[1].split(b']')[0])\n        self.assertEqual(data['name'], '#%s' % response.context['formset'].prefix)\n        self.assertEqual(data['options']['prefix'], response.context['formset'].prefix)\n        self.assertEqual(data['options']['addText'], gettext('Add another City'))\n        self.assertEqual(data['options']['deleteText'], gettext('Remove'))\n\n", "    def test_admin_form_field_extraction(self):\n        response = self.client.get(reverse('admin:article_changelist'))\n        fields = self.get_admin_form_fields(response)\n        self.assertEqual(len(fields), 7  )  \n\n"], "sample_1090": ["compilation error", "compilation error", "compilation error", "compilation error", "    def test_trigonometric():\n        assert pretty(sin(x + 2*pi)) == 'sin(x)'\n        assert pretty(cos(x + pi)) == '-cos(x)'\n        assert pretty(tan(x + pi/2)) == 'tan(x)' \n"], "sample_699": ["        def my_fixture(request):\n            return \"fixture_value\"\n        ", "        def test_something():\n            \"\"\"This is a test.\"\"\"", "    def test_doctest_ignore_import_errors(self, pytester: Pytester) -> None:\n        pytester.getiter(\n            \"\"\"\n            import nonexistent_module\n\n            >>> import nonexistent_module  # should not raise an error\n            \"\"\",\n            conftest_path=\"test_doctest_ignore_import_errors.py\",\n            option=\"doctest_ignore_import_errors\",\n        )\n\n        result = pytester.runpytest()\n        result.stdout.fnmatch_lines([*])\n        result.assert_outcomes(passed=1)\n\n        pytester.getiter(\n            \"\"\"\n            import nonexistent_module\n\n            >>> nonexistent_module.some_function()  # should raise an error\n            \"\"\",\n            conftest_path=\"test_doctest_ignore_import_errors.py\",\n            option=\"doctest_ignore_import_errors\",\n        )\n\n        result = pytester.runpytest()\n        result.stdout.fnmatch_lines([*])\n        result.assert_outcomes(passed=1)\n", "        def my_function(x: int) -> int:\n            return x * 2\n", "    def test_doctest_ignore_import_errors(self, testdir):\n        p = testdir.makepyfile(\n            \"\"\"\n            import nonexistant_module\n            \n            '''\n            This doctest should fail because the import fails.\n            '''\n            nonexistant_module.some_function()\n            \"\"\",\n        )\n        result = testdir.runpytest(\n            '-v', '--doctest-ignore-import-errors'\n        )\n        result.stdout.fnmatch_lines(\n            ['*test_doctest_ignore_import_errors.py::test_doctest_ignore_import_errors*']\n        )\n        result.assert_outcomes(passed=1)\n"], "sample_264": ["    def test_cookie_storage_not_finished(self):\n        storage = self.storage_class()\n        messages = [\n            Message(level=constants.INFO, message='Message 1'),\n            Message(level=constants.SUCCESS, message='Message 2'),\n        ]\n        set_cookie_data(storage, messages, encode_empty=True)\n        response = self.get_response()\n        self.assertEqual(self.stored_messages_count(storage, response), 0)\n", "    def test_overflow_with_oldest(self):\n        storage = self.storage_class()\n        messages = [\n            Message(request=self.request, level=constants.INFO, message=\"test1\"),\n            Message(request=self.request, level=constants.INFO, message=\"test2\"),\n            Message(request=self.request, level=constants.INFO, message=\"test3\"),\n        ] * 10\n        response = self.get_response()\n        set_cookie_data(storage, messages[:5])\n        unstored_messages = storage._store(messages[5:], response, remove_oldest=True)\n        self.assertTrue(unstored_messages)\n        self.assertEqual(self.stored_messages_count(storage, response), 5)\n\n        # Add some more messages to see how they are stored\n        set_cookie_data(storage, messages[:1])\n        storage._store(messages[1:6], response, remove_oldest=True)\n        self.assertEqual(self.stored_messages_count(storage, response), 6)\n", "    def test_cookie_storage_not_finished_sentinel(self):\n        storage = self.setup_storage()\n        messages = [\n            Message(level=constants.INFO, message=\"Message 1\"),\n            Message(level=constants.WARNING, message=\"Message 2\"),\n            Message(level=constants.ERROR, message=\"Message 3\"),\n        ]\n\n        # Store messages, and include the sentinel value\n        set_cookie_data(storage, messages, encode_empty=True)\n\n        # Retrieve messages from the cookie\n        retrieved_messages, all_retrieved = storage._get()\n        self.assertEqual(len(retrieved_messages), 3)\n        self.assertFalse(all_retrieved)\n\n        # Further storing messages should not encounter the sentinel value\n        messages.append(Message(level=constants.SUCCESS, message=\"Message 4\"))\n        set_cookie_data(storage, messages)\n\n        retrieved_messages, all_retrieved = storage._get()\n        self.assertEqual(len(retrieved_messages), 4)\n        self.assertTrue(all_retrieved)\n", "    def test_cookie_max_size_with_removals(self):\n        storage = self.storage_class()\n        messages = [\n            Message(level=constants.INFO, message=\"This is a message.\"),\n            Message(level=constants.INFO, message=\"Another message.\"),\n            Message(level=constants.SUCCESS, message=\"A success message.\"),\n            Message(level=constants.WARNING, message=\"A warning message.\"),\n            Message(level=constants.ERROR, message=\"An error message.\"),\n            Message(level=constants.DEBUG, message=\"A debug message.\"),\n            Message(level=constants.INFO, message=\"Another message.\"),\n            Message(level=constants.INFO, message=\"Yet another message.\"),\n            Message(level=constants.INFO, message=\"And another one.\"),\n        ]\n\n        set_cookie_data(storage, messages, encode_empty=True)\n        response = self.get_response()\n        message_count = stored_cookie_messages_count(storage, response)\n        self.assertEqual(message_count, 0)\n", "    def test_cookie_storage_not_finished(self):\n        storage = self.storage_class()\n        messages = [Message(constants.SUCCESS, \"This is a success message.\")]\n        set_cookie_data(storage, messages, encode_empty=True)\n        response = self.mock_http_response()\n        storage._store(messages, response, remove_oldest=False)\n        self.assertEqual(stored_cookie_messages_count(storage, response), 1)\n        response.flush()\n        messages = storage.get_messages(self.mock_request)\n        self.assertEqual(len(messages), 0)\n"], "sample_215": ["    def test_sensitive_setting_cleaning(self):\n        wrapper = CallableSettingWrapper(lambda: {'API_KEY': 'secret_key'})\n        cleaned = wrapper._wrapped\n        self.assertNotEqual(cleaned['API_KEY'], 'secret_key')\n        self.assertEqual(cleaned['API_KEY'], CallableSettingWrapper()._wrapped['API_KEY'])\n\n\n\n", "    def test_callable_setting_wrapper_is_repr_able(self):\n        wrapped_callable = CallableSettingWrapper(lambda: 'value')\n        self.assertEqual(repr(wrapped_callable), '<function <lambda> at 0x...>')\n\n\n\n", "    def test_callable_wrapper_repr(self):\n        wrapped = CallableSettingWrapper(lambda: 'some value')\n        self.assertEqual(repr(wrapped), \"CallableSettingWrapper(lambda: 'some value')\")\n\n\n", "    def test_callable_setting_wrapper_is_repr(self):\n        callable_setting = lambda x: x * 2\n        wrapper = CallableSettingWrapper(callable_setting)\n        self.assertEqual(repr(wrapper), repr(callable_setting))\n\n\n\n", "    def test_callable_setting_wrapper_for_callable(self):\n            return User()\n        wrapped = CallableSettingWrapper(callable_setting)\n        self.assertIsInstance(wrapped, CallableSettingWrapper)\n        self.assertEqual(wrapped(), User())\n\n\n\n\n"], "sample_693": ["def test_skip_unittest_test(pytester: Pytester) -> None:\n    pytester.writefile(\n        \"test_my_test.py\",\n        \"\"\"", "def test_pytest_runtest_makereport_skip(pytester: Pytester) -> None:\n    \"\"\"Test that unittest.SkipTest is transformed to pytest.skip.\"\"\"\n    pytester.writesetupfile(\n        \"\"\"", "def test_pytest_runtest_makereport_with_unittest_skip(pytester: Pytester) -> None:\n    pytester.write(\n        \"test_file.py\",\n        \"\"\"\n        import unittest\n\n        class MyTests(unittest.TestCase):\n                raise unittest.SkipTest(\"This test is skipped\")\n        \"\"\"\n    )\n\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=0, skipped=1)\n    result.stdout.fnmatch_lines([\n        \"collected 1 item\",\n        \"test_file.py::MyTests::test_something SKIPPED\",\n    ])\n\n    # Additionally, we can check for the expected skipped message\n    result.stdout.fnmatch_lines([\n        \"This test is skipped\"\n    ])\n\n\n\n    \n\n\n\n", "        def test_example():\n            pytest.xfail(\"This is a known issue\")", "def test_pytest_runtest_makereport(pytester: Pytester) -> None:\n    pytester.write(\n        \"test_file.py\",\n        \"\"\""], "sample_174": ["    def test_date_extract_sql_with_timezone(self):\n        tz_aware_datetime = timezone.now()\n        tz_naive_datetime = tz_aware_datetime.replace(tzinfo=None)\n        for tz_aware in [tz_aware_datetime, tz_naive_datetime]:\n            with self.subTest(datetime=tz_aware):\n                self.assertIsNotNone(self.ops.datetime_extract_sql('year', 'my_field', tzname='UTC'))\n                self.assertIsNotNone(self.ops.datetime_extract_sql('month', 'my_field', tzname='UTC'))\n                self.assertIsNotNone(self.ops.datetime_extract_sql('day', 'my_field', tzname='UTC'))\n\n\n", "    def test_integer_field_range(self):\n        self.assertEqual(self.ops.integer_field_range('AutoField'), (-2147483648, 2147483647))\n        self.assertEqual(self.ops.integer_field_range('SmallAutoField'), (-32768, 32767))\n\n        with self.assertRaises(KeyError) as cm:\n            self.ops.integer_field_range('UnknownField')\n        self.assertEqual(str(cm.exception), f'DictKeyError: \"UnknownField\" {self.may_require_msg(\"integer_field_ranges\")}')\n\n\n", "    def test_year_lookup_bounds(self):\n        self.assertEqual(self.ops.year_lookup_bounds_for_date_field(2023), ('2023-01-01', '2023-12-31'))\n        self.assertEqual(self.ops.year_lookup_bounds_for_datetime_field(2023), ('2023-01-01 00:00:00', '2023-12-31 23:59:59.999999'))\n\n", "    def test_integer_field_range(self):\n        self.assertEqual(self.ops.integer_field_range('AutoField'), (-2147483648, 2147483647))\n        self.assertEqual(self.ops.integer_field_range('SmallIntegerField'), (-32768, 32767))\n        with self.assertRaises(KeyError):\n            self.ops.integer_field_range('InvalidField')\n", "    def test_integer_field_range(self):\n        with override_settings(\n            INSTALLED_APPS=[\n                'test_app',\n            ],\n            DATABASES={\n                'default': {\n                    'ENGINE': 'django.db.backends.sqlite3',\n                    'NAME': ':memory:',\n                }\n            },\n        ):\n            from test_app.models import Book\n            self.assertEqual(self.ops.integer_field_range('AutoField'), (-2147483648, 2147483647))\n            self.assertEqual(self.ops.integer_field_range('PositiveAutoField'), (0, 2147483647))\n            self.assertEqual(self.ops.integer_field_range('SmallAutoField'), (-32768, 32767))\n            self.assertEqual(self.ops.integer_field_range('BigIntegerField'), (-9223372036854775808, 9223372036854775807))\n            self.assertEqual(self.ops.integer_field_range('PositiveBigIntegerField'), (0, 9223372036854775807))\n\n\n"], "sample_920": ["    def test_namedtuple_subclass(self) -> None:\n        docstring = cleandoc(\"\"\"\n        Sample namedtuple subclass\n\n        Attributes\n        ----------\n        attr1 : Arbitrary type\n            Quick description of attr1\n        attr2 : Another arbitrary type\n            Quick description of attr2\n        attr3 : Type\n\n            Adds a newline after the type\n        \"\"\")\n\n        config = Config(napoleon_use_rtype=True, napoleon_use_param=True)\n        docstring_obj = NumpyDocstring(docstring, config)\n        result = docstring_obj.lines()\n        expected = ['Sample namedtuple subclass', '']\n        expected += [\n            ':param attr1: Quick description of attr1',\n            ':type attr1: Arbitrary type',\n            ':param attr2: Quick description of attr2',\n            ':type attr2: Another arbitrary type',\n            ':param attr3: Type',\n            '    Adds a newline after the type',\n        ]\n        expected += ['']\n\n        self.assertEqual(result, expected)\n", "    def test_namedtuple_subclass(self) -> None:\n        docstring = cleandoc(\n            \"\"\"\\\n            Sample namedtuple subclass\n\n            Attributes\n            ----------\n            attr1 : Arbitrary type\n                Quick description of attr1\n            attr2 : Another arbitrary type\n                Quick description of attr2\n            attr3 : Type\n\n                Adds a newline after the type\n            \"\"\"\n        )\n        config = Config(napoleon_use_param=True)\n        docstring_parsed = NumpyDocstring(docstring, config)\n        expected = dedent(\n            \"\"\"\\\n            Sample namedtuple subclass\n\n            :param attr1: Quick description of attr1\n            :type attr1: Arbitrary type\n            :param attr2: Quick description of attr2\n            :type attr2: Another arbitrary type\n            :param attr3: Type\n\n            \"\"\"\n        )\n        self.assertEqual(docstring_parsed.__str__(), expected)\n", "    def test_namedtuple_subclass_docstring(self):\n        docstring = cleandoc(\"\"\"\n        Sample namedtuple subclass\n\n        Attributes\n        ----------\n        attr1 : Arbitrary type\n            Quick description of attr1\n        attr2 : Another arbitrary type\n            Quick description of attr2\n\n        attr3 : Type\n\n            Adds a newline after the type\n        \"\"\")\n        obj = NamedtupleSubclass('attr1', 'attr2')\n        config = Config(napoleon_use_rtype=True)\n        renderer = NumpyDocstring(docstring, config, app=None, what='attribute', name='', obj=obj, options=None)\n        self.assertEquals(renderer._parse() ,[\n            'attr1 : Arbitrary type',\n            '    Quick description of attr1',\n            '',\n            'attr2 : Another arbitrary type',\n            '    Quick description of attr2',\n            '',\n            'attr3 : Type',\n            '    Adds a newline after the type',\n            ''\n        ])\n", "    def test_namedtuple_subclass(self):\n        docstring = cleandoc(\"\"\"\n        Sample namedtuple subclass\n\n        Attributes\n        ----------\n        attr1 : Arbitrary type\n            Quick description of attr1\n        attr2 : Another arbitrary type\n            Quick description of attr2\n        attr3 : Type\n\n            Adds a newline after the type\n\n        \"\"\")\n        config = Config(napoleon_use_rtype=True, napoleon_use_param=True)\n        docstring_obj = NumpyDocstring(docstring, config=config)\n\n        expected_lines = [\n            \"Sample namedtuple subclass\",\n            \"\",\n            \":param attr1: Quick description of attr1\",\n            \":type attr1: Arbitrary type\",\n            \":param attr2: Quick description of attr2\",\n            \":type attr2: Another arbitrary type\",\n            \":param attr3: Type\",\n            \"            Adds a newline after the type\",\n            \"\",\n        ]\n        self.assertEqual(docstring_obj.lines(), expected_lines)\n\n\n\n", "    def test_namedtuple(self) -> None:\n        docstring = cleandoc(\n            \"\"\"\n            Sample namedtuple\n\n            Attributes\n            ----------\n            attr1 : Arbitrary type\n                Quick description of attr1\n            attr2 : Another arbitrary type\n                Quick description of attr2\n            attr3 : Type\n\n                Adds a newline after the type\n            \"\"\"\n        )\n        config = Config()\n        docstring_obj = NumpyDocstring(docstring, config)\n        self.assertEqual(\n            docstring_obj.lines(),\n            [\n                'Sample namedtuple',\n                '',\n                '.. attribute:: attr1',\n                '    Description of attr1',\n                '.. attribute:: attr2',\n                '    Description of attr2',\n                '.. attribute:: attr3',\n                '    Description of attr3',\n            ]\n        )\n\n\n\n"], "sample_104": ["    def test_manifest_strict_failure(self):\n        with override_settings(STATICFILES_STORAGE='django.contrib.staticfiles.storage.ManifestStaticFilesStorage'):\n            with self.settings(STATIC_ROOT=TEST_ROOT,\n                              STATIC_URL='/static/',\n                              STATICFILES_STORAGE='django.contrib.staticfiles.storage.ManifestStaticFilesStorage'):\n                # Create a file that's not in the manifest\n                self.create_file(os.path.join(self.base_dir, 'css', 'missing.css'))\n\n                # Try to access the file, expecting an error\n                with self.assertRaises(ValueError):\n                    storage.staticfiles_storage.url('css/missing.css')\n\n                self.assertPostCondition()\n\n", "    def test_manifest_storage_with_cache_misses(self):\n        # Test manifest file usage with cache misses.\n        with self.subTest(type='cache_miss'):\n            self.addCleanup(shutil.rmtree, TEST_ROOT)\n            self.addCleanup(del, settings.STATICFILES_STORAGE)\n            settings.STATICFILES_STORAGE = 'django.contrib.staticfiles.storage.ManifestStaticFilesStorage'\n            # Copy a file that doesn't exist in the manifest\n            try:\n                self.copy_static_file('test_file1.css', TEST_ROOT)\n            except FileNotFoundError:\n                self.fail(\"Should not fail, as a copy will be created\")\n            call_command('collectstatic', '--noinput', clear=True, write_manifest=True)\n            # Assert that the file exists and is hashed\n            self.assertIsFile(hashed_file_path(self, 'test_file1.css'))\n            # Test case where the manifest file is empty or not found\n            with self.subTest(type='empty_manifest'):\n                # This will trigger a cache miss\n                call_command('collectstatic', '--noinput', clear=True, write_manifest=True)\n                self.assertIsFile(hashed_file_path(self, 'test_file1.css'))\n\n        self.assertPostCondition()\n", "    def test_manifest_integrity(self):\n        self.create_test_files(\n            [\n                ('css', 'style.css'),\n                ('js', 'script.js'),\n            ]\n        )\n        call_command('collectstatic', verbosity=0, skip_cleanup=False)\n        self.assertPostCondition()\n", "    def test_manifest_nonexistent_file(self):\n        with self.subTest('nonexistent_file'):\n            name = 'nonexistent_path.css'\n            self.assertFalse(storage.staticfiles_storage.exists(name))\n            self.setup_staticfiles(\n                {name: ContentFile('')},\n            )\n\n            self.assertPostCondition()\n\n        with self.subTest('nonexistent_file_manifest'):\n            name = 'nonexistent_path.css'\n            self.assertFalse(storage.staticfiles_storage.exists(name))\n            self.setup_staticfiles_storage(ManifestStaticFilesStorage())\n\n            call_command('collectstatic', interactive=False, dry_run=False)\n\n\n\n            self.assertFalse(self.storage.exists(name))\n            self.assert_manifest_not_contains(name)\n\n            self.assertPostCondition()\n", "    def test_manifest_strict_raises_error(self):\n        with self.settings(STATICFILES_STORAGE='django.contrib.staticfiles.storage.ManifestStaticFilesStorage'):\n            with self.assertRaises(ValueError):\n                call_command('collectstatic', interactive=False, dry_run=True)\n"], "sample_23": ["    def test_wrap_angle_update(self):\n        lon = Longitude('180d')\n        assert lon.wrap_angle == 360 * u.deg\n\n        lon.wrap_angle = 180 * u.deg\n        assert lon.wrap_angle == 180 * u.deg\n\n        assert lon[0] == 0 * u.deg\n", "    def test_wrap_angle_large_value(self):\n        lon = Longitude('1000d')\n        assert lon.wrap_angle == 360 * u.deg\n        lon.wrap_angle = 450 * u.deg\n        assert lon.wrap_angle == 450 * u.deg\n        assert np.all(lon.to(u.deg) >= 0)\n        assert np.all(lon.to(u.deg) < 450)\n", "    def test_hms_to_string_precision(self, angle_value):\n        for unit in [u.hour, u.degree]:\n            angle = Angle(angle_value, unit=unit)\n            for precision in [None, 2, 3, 4, 5]:\n                str_angle = angle.to_string(precision=precision)\n                expected_str = angle_value\n                if precision is not None:\n                    expected_str = f\"{angle_value:.{precision}f}\" if isinstance(angle, Angle) and angle.unit is u.hour else f\"{angle_value:.{precision}f}\"\n                assert str_angle == expected_str\n\n", "    def test_wrap_at_with_negative_number(self):\n        lon = Longitude('180d')\n        lon  = lon.wrap_at('360d', inplace=True)\n        assert lon.degree[0] == 0\n\n        lon = Longitude('-180d')\n        lon = lon.wrap_at('0d', inplace=True)\n        assert lon.degree[0] == 180\n", "    def test_wrap_at_inplace(self):\n        a = Angle(np.array([0, 360, -60, 420, -10]), unit=u.deg)\n        b = a.copy()\n\n        a.wrap_at(360 * u.deg, inplace=True)\n        assert_array_equal(a.to(u.deg), np.array([0, 0, 300, 0, 300]))\n\n        assert_array_equal(b.to(u.deg), np.array([0, 360, -60, 420, -10]))\n\n\n"], "sample_892": ["    def test_loss(self, loss):\n        regressor = AdaBoostRegressor(loss=loss, n_estimators=10, random_state=42)\n        X, y = make_regression(n_samples=100, n_features=20)\n        regressor.fit(X, y)\n        assert regressor.loss == loss\n\n\n\n", "    def test_adaboost_regressor_predict_with_missing_data(self):\n        # Test prediction with missing data\n        X = np.array([[1, 2, np.nan], [4, 5, 6], [7, 8, 9]])\n        y = np.array([11, 16, 21])\n        reg = AdaBoostRegressor(n_estimators=10,\n                                random_state=0)\n        reg.fit(X, y)\n        predicted_values = reg.predict(X)\n        assert_array_less(np.abs(predicted_values - y), 2)\n", "    def test_adaboost_regressor_random_state(self):\n        X_train, X_test, y_train, y_test = train_test_split(\n            diabetes.data, diabetes.target, test_size=0.2, random_state=42\n        )\n        reg1 = AdaBoostRegressor(random_state=42, n_estimators=10)\n        reg2 = AdaBoostRegressor(random_state=42, n_estimators=10)\n        reg1.fit(X_train, y_train)\n        reg2.fit(X_train, y_train)\n\n        assert_array_almost_equal(reg1.predict(X_test), reg2.predict(X_test))\n\n\n", "    def test_boosting_algorithm_with_different_loss(self, random_state):\n        X, y = _make_classification(n_samples=100, n_features=10, random_state=random_state)\n        \n        # Test different loss functions\n        # 'linear' is the default\n        reg = AdaBoostRegressor(n_estimators=10, learning_rate=1.0,\n                                random_state=random_state)\n        reg.fit(X, y)\n        y_pred = reg.predict(X)\n\n        for loss in ['linear', 'square', 'exponential']:\n            reg = AdaBoostRegressor(n_estimators=10, learning_rate=1.0,\n                                    loss=loss, random_state=random_state)\n            reg.fit(X, y)\n            y_pred_loss = reg.predict(X)\n\n            # Check that predictions are not drastically different\n            assert_array_almost_equal(y_pred_loss, y_pred,\n                                      atol=1e-3)\n\n\n\n", "    def test_adaboost_regressor_staged_predict(self):\n        # Check if staged predictions match final predictions\n        regr = AdaBoostRegressor(\n            n_estimators=10, random_state=0, loss=\"square\"\n        )\n        X_train, X_test, y_train, y_test = train_test_split(\n            diabetes.data, diabetes.target, test_size=0.25, random_state=0\n        )\n        regr.fit(X_train, y_train)\n\n        y_pred_staged = list(regr.staged_predict(X_test))\n        y_pred_final = regr.predict(X_test)\n\n        assert_array_almost_equal(y_pred_final, y_pred_staged[-1])\n        for i in range(len(y_pred_staged) - 1):\n            assert_array_less(\n                np.abs(y_pred_staged[i] - y_pred_final),\n                np.abs(y_pred_staged[i + 1] - y_pred_final),\n            )\n"], "sample_1152": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_112": ["    def test_submit_row(self):\n        request = self.request_factory.get(reverse('admin:article_change', args=(1,)))\n        request.user = self.user\n        add = self.mock_add_permission\n        change = True\n        is_popup = False\n        save_as = False\n        show_save = True\n        show_save_and_add_another = True\n        show_save_and_continue = True\n        has_add_permission = self.mock_add_permission\n        has_change_permission = True\n        has_view_permission = True\n        has_editable_inline_admin_formsets = False\n        context = {\n            'add': add,\n            'change': change,\n            'is_popup': is_popup,\n            'save_as': save_as,\n            'show_save': show_save,\n            'show_save_and_add_another': show_save_and_add_another,\n            'show_save_and_continue': show_save_and_continue,\n            'has_add_permission': has_add_permission,\n            'has_change_permission': has_change_permission,\n            'has_view_permission': has_view_permission,\n            'has_editable_inline_admin_formsets': has_editable_inline_admin_formsets\n        }\n        rendered_context = submit_row(context)\n        self.assertTrue(rendered_context['can_change'])\n        self.assertTrue(rendered_context['show_save_and_add_another'])\n        self.assertTrue(rendered_context['show_save_and_continue'])\n        self.assertTrue(rendered_context['show_save'])\n        self.assertFalse(rendered_context['show_close'])\n", "    def test_submit_row(self):\n        user = self.create_user('testuser')\n        self.login_as(get_admin_user())\n        request = self.request_factory.get(reverse('admin:user_change', args=(user.id,)))  \n\n        # Set up context for submit_row\n        context = {'has_add_permission': True, 'has_change_permission': True, 'has_delete_permission': True}\n        context.update(submit_row(context))  \n\n        self.assertIn('can_change', context)\n        self.assertTrue(context['can_change'])\n        self.assertIn('show_delete_link', context)\n        self.assertTrue(context['show_delete_link'])\n        self.assertIn('show_save_as_new', context)\n        self.assertTrue(context['show_save_as_new'])\n        self.assertIn('show_save_and_add_another', context)\n        self.assertTrue(context['show_save_and_add_another'])\n        self.assertIn('show_save_and_continue', context)\n        self.assertTrue(context['show_save_and_continue'])\n        self.assertIn('show_save', context)\n        self.assertTrue(context['show_save'])\n        self.assertIn('show_close', context)\n        self.assertFalse(context['show_close']) \n\n\n", "    def test_submit_row(self):\n        user = User.objects.create(username='testuser', email='test@example.com')\n        self.login_as_superuser()\n\n        response = self.client.get(reverse('admin:article_change', args=(1,)))\n        self.assertContains(response, 'submit_row')\n\n        response = self.client.get(reverse('admin:article_add'))\n        self.assertContains(response, 'submit_row')\n\n        response = self.client.get(reverse('admin:question_change', args=(1,)))\n        self.assertContains(response, 'submit_row')\n\n        response = self.client.get(reverse('admin:question_add'))\n        self.assertContains(response, 'submit_row')\n\n", "    def test_cell_count(self):\n        article_inline = Article.Inline(model=Question, verbose_name='Questions')\n        context = self.get_admin_context(self.user, article_inline)\n        inline_admin_form = context['inline_admin_form']\n        self.assertEqual(cell_count(inline_admin_form), 4)\n", "    def test_submit_row_tag(self):\n        user = User.objects.create_user('testuser', 'test@example.com', 'password')\n        self.login(user)\n        request = self.request_factory.get(reverse('admin:article_change', args=(1,)))\n        request.user = user\n        article = Article.objects.create(title='Test Article', content='Test Content')\n        context = {'add': False, 'change': True, 'is_popup': False, 'save_as': False, 'has_delete_permission': True, 'show_delete': True, 'has_add_permission': True, 'has_change_permission': True, 'has_view_permission': True, 'show_save_and_add_another': True, 'show_save_and_continue': True}\n        response = submit_row(context)\n        self.assertIn('can_change', response)\n        self.assertIn('show_delete_link', response)\n        self.assertIn('show_save_as_new', response)\n        self.assertIn('show_save_and_add_another', response)\n        self.assertIn('show_save_and_continue', response)\n        self.assertIn('show_save', response)\n        self.assertIn('show_close', response) \n"], "sample_229": ["    def test_union_with_distinct(self):\n        numbers = Number.objects.filter(num__gt=5).values_list('num')\n        numbers_distinct = Number.objects.filter(num__gt=5).distinct().values_list('num')\n        self.assertQuerysetEqual(numbers.union(numbers_distinct), numbers)\n", "    def test_prefetch_related_with_complex_lookups(self):\n        qs = Number.objects.select_related('other_numbers')\n        self.assertNumbersEqual(qs, [n for n in Number.objects.all()])\n        \n\n", "    def test_select_related_with_nested_prefetch(self):\n        # Test a scenario with nested select_related() and prefetch_related(),\n        # ensuring both are applied correctly.\n        numbers = Number.objects.select_related('reserved_names').prefetch_related(\n            Prefetch('reserved_names__author')\n        )\n        self.assertNumbersEqual(numbers, [Number(num=i, other_num=10 - i) for i in range(10)], ordered=False)\n        for number in numbers:\n            self.assertEqual(len(number.reserved_names), 1)\n            self.assertEqual(len(number.reserved_names.author), 1)\n\n\n\n", "def test_union_with_distinct_and_aggregate(self):\n    \"\"\"\n    Test that UNION behaves correctly with DISTINCT and aggregate functions.\n    Issue: https://code.djangoproject.com/ticket/29094\n    \"\"\"\n    # Create a subquery with an aggregate function and DISTINCT\n    subquery = Number.objects.filter(num__gt=5).values('num').annotate(count=Count('num')).distinct()\n\n    # Create the main query using UNION with the subquery\n    query = Number.objects.filter(num__lt=5).union(subquery)\n\n    # Select distinct numbers from both subquery and main query\n    result_count = query.count()\n    self.assertEqual(result_count, 16)\n\n\n\n", "    def test_union_with_duplicate_values(self):\n        queryset1 = Number.objects.filter(num__gt=5)\n        queryset2 = Number.objects.filter(num__gt=5).union(Number.objects.filter(num__lt=5))\n        expected_numbers = Number.objects.all()\n        self.assertNumbersEqual(queryset2, expected_numbers)\n\n\n\n"], "sample_70": ["    def test_protected_error(self):\n        a = create_a()\n        with self.assertRaises(ProtectedError) as context:\n            Collector(using=connection.alias).collect([a], source=MR, nullable=False)\n        self.assertIn(\n            f\"Cannot delete some instances of model 'A' because they are \"\n            f\"referenced through a protected foreign key: 'MR.a'\",\n            str(context.exception)\n        )\n\n\n", "    def test_protected_error(self):\n        a = create_a()\n        with self.assertRaises(ProtectedError) as context:\n            Collector(using=connection.alias).delete([a], using=connection.alias)\n        self.assertIn(\n            str(a.__class__.__name__), context.exception.args[0]\n        )\n", "    def test_cascade_delete_with_deferrable_constraints(self):\n        a1 = create_a()\n        a2 = create_a()\n        mr1 = MR.objects.create(a=a1)\n        mr2 = MR.objects.create(a=a2)\n        m1 = M.objects.create(mr=mr1)\n        m2 = M.objects.create(mr=mr2)\n\n        with connection.cursor() as cursor:\n            cursor.execute(\"SET CONSTRAINTS ALL IMMEDIATE\")\n\n        collector = Collector(using=connection.settings_dict['NAME'])\n        collector.collect(objs=[a1], collect_related=True)\n        deleted_count, deleted_objects = collector.delete()\n\n        self.assertEqual(deleted_count, 3)\n\n        # Verify that the cascade was successful and no constraints were violated.\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT COUNT(*) FROM m_m WHERE m_id = %s\", [m1.id])\n            self.assertEqual(cursor.fetchone()[0], 0)\n            cursor.execute(\"SELECT COUNT(*) FROM m_m WHERE mr_id = %s\", [mr1.id])\n            self.assertEqual(cursor.fetchone()[0], 0)\n\n\n", "    def test_protected_error(self):\n        a = create_a()\n        a.save()\n\n        with self.assertRaises(ProtectedError) as cm:\n            Collector(using=connection.connection.settings_dict['NAME']).collect([a])\n        \n        self.assertEqual(cm.exception.protected_objects, [a])\n        self.assertIn(\"Cannot delete some instances\", str(cm.exception))\n        self.assertIn(\"protected foreign key\", str(cm.exception))\n\n\n\n", "    def test_protected_errors(self):\n        a = create_a()\n        m = M.objects.create(a=a)\n\n        with self.assertRaises(ProtectedError) as ctx:\n            a.delete()\n\n        self.assertIn(f\"'{a.__class__.__name__}'\", str(ctx.exception))\n        self.assertIn(f\"Foreign key: 'm.a'\", str(ctx.exception))\n\n\n"], "sample_508": ["    def test_artist_inspector_get_valid_values(self):\n        class MyArtist(martist.Artist):\n                self._myprop = val\n\n                return self._myprop\n\n        my_artist = MyArtist()\n        insp = ArtistInspector(my_artist)\n        assert insp.get_valid_values('myprop') == 'unknown'\n\n", "    def test_get_cursor_data(artist_type):\n        artist = artist_type()\n        with pytest.raises(AttributeError):\n            artist.get_cursor_data(None)\n\n        # Test default behavior (should return None)\n        cursor_data = artist.get_cursor_data(None)\n        assert cursor_data is None\n\n", "    def test_getp_complex_objects(self):\n        fig, ax = plt.subplots()\n        line, = ax.plot([1, 2, 3])\n        circle = mpatches.Circle((0.5, 0.5), radius=0.2, color='blue')\n        ax.add_patch(circle)\n\n        # Test that getp works on a collection of artists\n        setp([line, circle], color='red')\n        assert getp(line, 'color') == 'red'\n        assert getp(circle, 'color') == 'red'\n        \n        # Test querying properties of artists within a collection\n        scatter = ax.scatter([1, 2, 3], [4, 5, 6])\n        obj_list = [line, circle, scatter]\n        for obj in obj_list:\n            assert getp(obj, 'color') == 'red'\n\n\n\n", "    def test_artist_inspector_properties(artist_type):\n        artist = artist_type()\n        ai = martist.ArtistInspector(artist)\n        expected_properties = [\n            'label', 'alpha', 'visible', 'picker',\n            'facecolor', 'edgecolor', 'linewidth', 'linestyle',\n            'transform', 'zorder', 'clip_box',\n            'clipon', 'sticky_edges', 'path'\n        ]\n        for property_name in expected_properties:\n            assert property_name in ai.properties()\n\n\n\n", "    def test_get_cursor_data_with_artist_classes(artist_class):\n        \"\"\"\n        Test that get_cursor_data returns None by default for various artist\n        classes.\n        \"\"\"\n        artist = artist_class()\n        assert artist.get_cursor_data(None) is None\n\n"], "sample_746": ["compilation error", "compilation error", "compilation error", "    def test_hinge_loss_multiclass():\n        X = np.array([[0], [1], [2], [3]])\n        Y = np.array([0, 1, 2, 3])\n        labels = np.array([0, 1, 2, 3])\n        est = svm.LinearSVC()\n        est.fit(X, Y)\n        pred_decision = est.decision_function([[-1], [2], [3]])\n        y_true = [0, 2, 3]\n        loss = hinge_loss(y_true, pred_decision, labels)\n        assert_almost_equal(loss, 0.56...)\n\n", "compilation error"], "sample_877": ["    def test_isotonic_regression_increasing(self, increasing):\n        X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n        y_fit = isotonic_regression(\n            y, sample_weight=np.ones(len(y)), increasing=increasing\n        )\n        if increasing == \"auto\":\n            expected_increasing = check_increasing(X, y)\n            assert expected_increasing == (y_fit > y[:-1]).all()\n        else:\n            assert (y_fit > y[:-1]).all() == increasing\n", "    def test_isotonic_regression_nan_sample_weight(self):\n        X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n        sample_weight = np.empty(len(X))\n        sample_weight[::2] = np.nan\n        \n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            y_ = isotonic_regression(y, sample_weight=sample_weight)\n        \n        assert_array_almost_equal(y, y_)\n", "    def test_check_increasing_auto(self, increasing, expected_increasing):\n        X = np.array([1, 2, 3, 4, 5])\n        y = np.array([2, 4, 6, 8, 10])\n        rho_ =  0.999999\n        increasing_bool = check_increasing(X, y)\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            isotonic_increasing = check_increasing(X, y, increasing=increasing)\n        assert isotonic_increasing == expected_increasing\n\n", "    def test_isotonic_regression_with_sample_weights(\n        self, X, y, sample_weight, increasing, expected_incr", "    def test_isotonic_increasing(self):\n        X = np.array([0, 1, 2, 3, 4])\n        y = np.array([1, 3, 5, 7, 9])\n        y_pred = isotonic_regression(y, increasing=True)\n        assert_array_equal(y_pred, y)\n"], "sample_698": ["    def test_colored_level_formatter_works(\n        self, caplog: _pytest.logging.CaptureFixture", "    def test_coloredlevelformatter(\n        self, caplog: LogCaptureFixture", "    def test_log_format_with_colored_level(self):\n        formatter = ColoredLevelFormatter(\n            TerminalWriter(),\n            \"%(asctime)s %(levelname)-8s %(message)s\",\n            \"%Y-%m-%d %H:%M:%S\",\n        )\n        handler = logging.StreamHandler()\n        handler.setFormatter(formatter)\n        handler.emit(logging.LogRecord(\n            name=\"test\",\n            level=logging.DEBUG,\n            msg=\"This is a test message\"\n        ))\n        output = handler.stream.getvalue().strip()\n        assert \"DEBUG\" in output\n\n\n", "    def test_set_log_path(self, caplog):\n        from _pytest.config import Config\n        config = Config()\n        plugin = LoggingPlugin(config)\n        plugin.set_log_path(\"mytest.log\")\n\n\n", "    def test_colored_level_formatter_with_ansi_escape(self):\n        with captured_output() as (stdout, stderr):\n            formatter = ColoredLevelFormatter(\n                TerminalWriter(None),\n                log_format=ColoredLevelFormatter.DEFAULT_LOG_FORMAT,\n                log_date_format=\"%Y-%m-%d %H:%M:%S\"\n            )\n            formatter.format(\n                logging.LogRecord(\n                    name=\"test\",\n                    level=logging.INFO,\n                    message=\"This is a test message\",\n                    args=(\"arg1\", \"arg2\"),\n                )\n            )\n        output = stdout.getvalue()\n        assert \"\\033[1;34mINFO\\033[0m\" in output  # INFO color should be blue\n\n\n\n"], "sample_426": ["    def test_timesince_future(self):\n        now = timezone.now()\n        with translation.override('en', lambda f: f):\n            self.assertEqual(timesince(now, now), \"0 minutes\")\n        with translation.override('es', lambda f: f):\n            self.assertEqual(timesince(now, now), \"0 minutos\")\n", "    def test_timeuntil(self):\n        now = timezone.now()\n        t = datetime.datetime(2023, 10, 27, 10, 30, 0)\n        future_10_seconds = t + self.onesecond * 10\n        future_1_minute = t + self.oneminute\n        future_1_hour = t + self.onehour\n        future_1_day = t + self.oneday\n\n        self.assertEqual(\n            timeuntil(future_10_seconds, now), \"less than 1 minute\"\n        )\n        self.assertEqual(\n            timeuntil(future_1_minute, now), \"less than 1 minute\"\n        )\n        self.assertEqual(\n            timeuntil(future_1_hour, now), \"less than 1 hour\"\n        )\n        self.assertEqual(\n            timeuntil(future_1_day, now), \"less than 1 day\"\n        )\n\n\n\n", "    def test_timeuntil_large_interval(self):\n        now = timezone.now()\n        future = now + self.oneyear\n\n        self.assertEqual(timeuntil(future, depth=1), gettext(\"1 year\"))\n", "    def test_timesince_future_date(self):\n        now = timezone.now()\n        result = timesince(self.test_timesince_future_date, now)\n        self.assertEqual(result, avoid_wrapping(gettext(\"0 minutes\")))\n", "    def test_timeuntil_future_datetime(self):\n        now = timezone.now()\n        d = timezone.datetime(now.year + 1, now.month, now.day, now.hour, now.minute, now.second)\n        self.assertEqual(timeuntil(d, now), f\"1 {gettext('year')}\")\n"], "sample_611": ["    def test_to_cftime_datetime(offset, expected_n):\n        dt = cftime.DatetimeGregorian(2023, 10, 26, 10, 30, 0)\n        for calendar in _CFTIME_CALENDARS:\n            result = to_cftime_datetime(dt, calendar=calendar)\n            assert result.year == 2023\n            assert result.month == 10\n            assert result.day == 26\n            assert result.hour == 10\n            assert result.minute == 30\n            assert result.second == 0\n            assert result.calendar == calendar\n    ", "    def test_to_offset_with_freq_alias(offset, expected_n):\n        assert to_offset(f\"{offset.freq.value}{expected_n}\") == offset\n\n", "    def test_to_offset(offset, expected_n):\n        assert to_offset(offset).n == expected_n\n\n\n", "    def test_to_offset(request):\n        offset_str = request.param[0]\n        expected_class = request.param[1]\n        \n        offset = to_offset(offset_str)\n        assert isinstance(offset, expected_class)\n", "compilation error"], "sample_1149": ["    def test_singleton_registry(self):\n        class MySingleton(Basic, metaclass=Singleton):\n            pass\n\n        self.assertTrue(MySingleton() is S.MySingleton)\n        self.assertEqual(MySingleton(), S.MySingleton)\n        self.assertTrue(MySingleton() is not Rational(1, 2))\n\n", "    def test_singleton_register_and_access(self):\n        class MySingleton(Basic, metaclass=Singleton):\n            pass\n        self.assertTrue(MySingleton() is S.MySingleton)\n        \n        with self.assertRaises(AttributeError):\n            MySingleton() is S.DoesNotExist\n", "    def test_singleton_registration(self):\n        class MySingleton(Basic, metaclass=Singleton):\n            pass\n        self.assert_is_equal(S.MySingleton, MySingleton())\n        self.assertEqual(S.MySingleton.__new__, lambda cls: MySingleton._instance)\n\n\n", "    def test_s_register(self):\n        class MySingleton(Basic, metaclass=Singleton):\n            pass\n\n        self.assertFalse(hasattr(S, 'MySingleton'))\n        S.register(MySingleton)\n        self.assertTrue(hasattr(S, 'MySingleton'))\n", "    def test_singleton_registry(self):\n        class MySingleton(Basic, metaclass=Singleton):\n            pass\n\n        self.assertTrue(MySingleton() is S.MySingleton)\n        self.assertIsInstance(S.MySingleton, MySingleton)\n        self.assertEqual(S.MySingleton.basic, MySingleton())\n\n        # Test that S.MySingleton is the only instance\n        instance1 = MySingleton()\n        instance2 = MySingleton()\n        self.assertTrue(instance1 is instance2)\n\n        # Test that singleton instance is accessible before registration\n        class MyOtherSingleton(Basic, metaclass=Singleton):\n            pass\n        self.assertTrue(isinstance(S.MyOtherSingleton, MyOtherSingleton))\n        self.assertEqual(S.MyOtherSingleton.basic, MyOtherSingleton())\n\n\n\n"], "sample_251": ["    def test_case_with_subquery(self):\n        subquery = Book.objects.filter(rating__gt=4).values('isbn').annotate(count=Count('isbn'))\n        case_expr = Case(\n            When(Book.isbn__in=subquery.values_list('isbn', flat=True), then=True),\n            default=False,\n            output_field=BooleanField()\n        )\n        result = Book.objects.annotate(high_rated=case_expr).values('name', 'high_rated')\n        expected = [\n            {'name': 'The Definitive Guide to Django: Web Development Done Right', 'high_rated': True},\n            {'name': 'Practical Django Projects', 'high_rated': True},\n            {'name': 'Artificial Intelligence: A Modern Approach', 'high_rated': True},\n            {'name': 'Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp', 'high_rated': True},\n        ]\n        self.assertQuerysetEqual(result, expected, lambda obj: obj)\n\n\n", "    def test_annotate_with_non_aggregate_expression(self):\n        # We should be able to annotate with non-aggregate expressions\n        from django.db.models import F\n\n        q = Author.objects.annotate(age_plus_one=F('age') + 1)\n        self.assertNumQueries(1, auto_flush=True)\n        authors = q.all()\n        for author in authors:\n            self.assertEqual(author.age_plus_one, author.age + 1)\n", "    def test_extract_year(self):\n        self.assertEqual(ExtractYear(DateTimeField('2023-04-08 10:00:00')).as_sql(), 'EXTRACT(YEAR FROM \"books\".\"pubdate\")')\n", "    def test_when_expression_with_simple_conditional(self):\n        result = Book.objects.annotate(category=When(\n            pages__gt=300, then='Long', else='Short'\n        )).values('category').all()\n\n        self.assertCountEqual(\n            ['Long', 'Short'],\n            [item['category'] for item in result]\n        )\n\n\n", "    def setUpTestData(cls):\n        cls.a1 = Author.objects.create(name='Adrian Holovaty', age=34)\n        cls.a2 = Author.objects.create(name='Jacob Kaplan-Moss', age=35)\n        cls.a3 = Author.objects.create(name='Brad Dayley', age=45)\n        cls.a4 = Author.objects.create(name='James Bennett', age=29)\n        cls.a5 = Author.objects.create(name='Jeffrey Forcier', age=37)\n        cls.a6 = Author.objects.create(name='Paul Bissex', age=29)\n        cls.a7 = Author.objects.create(name='Wesley J. Chun', age=25)\n        cls.a8 = Author.objects.create(name='Peter Norvig', age=57)\n        cls.a9 = Author.objects.create(name='Stuart Russell', age=46)\n        cls.a1.friends.add(cls.a2, cls.a4)\n        cls.a2.friends.add(cls.a1, cls.a7)\n        cls.a4.friends.add(cls.a1)\n        cls.a5.friends.add(cls.a6, cls.a7)\n        cls.a6.friends.add(cls.a5, cls.a7)\n        cls.a7.friends.add(cls.a2, cls.a5, cls.a6)\n        cls.a8.friends.add(cls.a9)\n        cls.a9.friends.add(cls.a8)\n\n        cls.p1 = Publisher.objects.create(name='Apress', num_awards=3)\n        cls.p2 = Publisher.objects.create(name='Sams', num_awards=1)\n        cls.p3 = Publisher.objects.create(name='Prentice Hall', num_awards=7)\n        cls.p4 = Publisher.objects.create(name='Morgan Kaufmann', num_awards=9)\n        cls.p5 = Publisher.objects.create(name=\"Jonno'"], "sample_316": ["    def test_image_dimensions_from_uploadedfile(self):\n        image_data = b'\\x42\\x4D...\\x00\\x00'  # Magic number for BMP images\n        uploaded_file = InMemoryUploadedFile(BytesIO(image_data),\n                                            'ImageField',\n                                            'test.bmp',\n                                            'image/bmp',\n                                            len(image_data),\n                                            None)\n        file_object = File(uploaded_file)\n        self.assertEqual(file_object.width, 0)\n        self.assertEqual(file_object.height, 0)\n\n        # The test will fail if the `get_image_dimensions` function in the provided code does not work for in-memory uploaded files\n", "    def test_get_image_dimensions_from_open_file(self):\n        with NamedTemporaryFile(delete=False) as f:\n            Image.new('RGB', (100, 200)).save(f.name)\n            with open(f.name, 'rb') as imgf:\n                width, height = get_image_dimensions(imgf)\n                self.assertEqual(width, 100)\n                self.assertEqual(height, 200)\n            os.remove(f.name)\n", "    def test_get_image_dimensions_compressed(self):\n        with NamedTemporaryFile(delete=False) as temp_file:\n            with gzip.open(\"path/to/compressed_image.jpg.gz\", \"rb\") as f:\n                temp_file.write(f.read())\n            temp_file.seek(0)\n\n            width, height = images.get_image_dimensions(temp_file)\n            self.assertIsNotNone(width)\n            self.assertIsNotNone(height)\n\n        os.remove(temp_file.name)\n", "    def test_get_image_dimensions_from_file_object(self):\n        file_content = b'This is some test data'\n        with NamedTemporaryFile(delete=False) as temp_file:\n            temp_file.write(file_content)\n            temp_file.seek(0)\n            with open(temp_file.name, 'rb') as f:\n                width, height = images.get_image_dimensions(f)\n                self.assertEqual(width, None)\n                self.assertEqual(height, None)\n", "    def test_get_image_dimensions_from_file(self):\n        with NamedTemporaryFile(delete=False) as f:\n            f.write(b'\\xFF\\xD8\\xFF\\xE0')  # JPEG header\n            f.write(b'\\x4A\\x46\\x49\\x46\\x00\\x01\\x01\\x00')  # JPEG format\n            f.write(b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00')\n\n            with Image.open(f.name) as img:\n                self.assertEqual(images.get_image_dimensions(f), img.size)\n\n\n\n"], "sample_464": ["    def test_file_response_with_non_seekable_file(self):\n        with tempfile.NamedTemporaryFile('w+b', delete=False) as f:\n            f.write(b\"Test content\")\n            f.seek(0)\n            response = FileResponse(\n                UnseekableBytesIO(f.read()),\n                as_attachment=True,\n                filename=\"test.txt\",\n            )\n            # Test that Content-Length header is set correctly\n            self.assertIn(\"Content-Length\", response.headers)\n            self.assertEqual(int(response.headers[\"Content-Length\"]), len(f.read()))\n\n\n", "    def test_file_content_length_seekable_file(self):\n        with tempfile.NamedTemporaryFile(\"w+b\") as temp_file:\n            temp_file.write(b\"Hello, world!\")\n            temp_file.seek(0)\n            response = FileResponse(temp_file, as_attachment=True)\n            self.assertEqual(response.headers[\"Content-Length\"], 13)\n            self.assertEqual(response.has_header(\"Content-Disposition\"), True)\n\n", "    def test_file_response_seekable_file(self):\n        with tempfile.TemporaryFile() as f:\n            f.write(b\"This is some test data.\")\n            f.seek(0)\n            response = FileResponse(f, as_attachment=True, filename=\"test.txt\")\n            self.assertEqual(response.status_code, 200)\n            self.assertEqual(response.headers[\"Content-Type\"], \"application/octet-stream\")\n            self.assertEqual(response.headers[\"Content-Disposition\"], 'attachment; filename=\"test.txt\"')\n            self.assertEqual(response.headers[\"Content-Length\"], 17)\n            \n            # Check that the file was opened correctly and can be read\n            bytes_read = 0\n            for chunk in iter(response.streaming_content, b\"\"):\n                bytes_read += len(chunk)\n                self.assertEqual(bytes_read, 17)\n            self.assertEqual(bytes_read, 17)\n\n\n\n", "    def test_file_response_unseekable_file(self):\n        with tempfile.NamedTemporaryFile(\"w+b\") as temp_file:\n            temp_file.write(b\"test content\")\n            temp_file.seek(0)\n            response = FileResponse(UnseekableBytesIO(temp_file.read()), as_attachment=True)\n            self.assertEqual(response.status_code, 200)\n            self.assertEqual(response.headers[\"Content-Disposition\"], 'attachment; filename=\"temp_file_name\"')\n            content = b\"\".join(response)\n            self.assertEqual(content, b\"test content\") \n", "    def test_file_response_unseekable_file(self):\n        with tempfile.NamedTemporaryFile(delete=False) as tmpfile:\n            tmpfile.write(b\"hello world\")\n            tmpfile.close()\n            try:\n                file_response = FileResponse(\n                    ContentFile(open(tmpfile.name, \"rb\")), as_attachment=True\n                )\n                # Assert that the Content-Length header is set correctly.\n                self.assertEqual(file_response.headers[\"Content-Length\"], \"11\")\n                # Assert that the Content-Type header is set to the default\n                # 'application/octet-stream'.\n                self.assertEqual(file_response.headers[\"Content-Type\"], \"application/octet-stream\")\n            finally:\n                os.remove(tmpfile.name)\n"], "sample_1": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_356": ["    def test_migration_dependencies_ordering(self):\n        changes = self.get_changes([\n            author_name_deconstructible_list_1,\n            author_name_deconstructible_list_2,\n            publisher,\n            book,\n        ], [\n            author_name_deconstructible_list_2,\n            author_name_deconstructible_list_1,\n            publisher,\n            book,\n        ])\n        self.assertMigrationDependencies(changes, 'testapp', 0, ['testapp.Author'])  \n        self.assertMigrationDependencies(changes, 'testapp', 1, ['testapp.Publisher']) \n        self.assertMigrationDependencies(changes, 'otherapp', 0, ['testapp.Author']) \n\n\n", "    def test_unique_together_constraints(self):\n        before_states = self.make_project_state([\n            book_foo_together,\n        ])\n        after_states = self.make_project_state([\n            book_foo_together_2,\n        ])\n        changes = self.get_changes(\n            before_states,\n            after_states,\n        )\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertOperationTypes(\n            changes, \"otherapp\", 0, [\"CreateModel\"],\n        )\n\n        before_states = self.make_project_state([\n            book_foo_together_2,\n        ])\n        after_states = self.make_project_state([\n            book_foo_together_3,\n        ])\n        changes = self.get_changes(\n            before_states,\n            after_states,\n        )\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertOperationTypes(\n            changes, \"otherapp\", 0, [\"AlterModel\"],\n        )\n\n        before_states = self.make_project_state([\n            book_foo_together_3,\n        ])\n        after_states = self.make_project_state([\n            book_foo_together_4,\n        ])\n        changes = self.get_changes(\n            before_states,\n            after_states,\n        )\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertOperationTypes(\n            changes, \"otherapp\", 0, [\"AlterModel\"],\n        )\n", "    def test_migration_operations_from_proxy_models(self):\n        changes = self.get_changes([\n            author_proxy,\n            author_proxy_third,\n            author_proxy_notproxy,\n            third_thing,\n            other_pony,\n            other_pony_food,\n            other_stable,\n            book,\n            book_proxy_fk,\n            book_proxy_proxy_fk,\n        ], [\n            author_proxy,\n            author_proxy_third,\n            author_proxy_notproxy,\n            third_thing,\n            other_pony,\n            other_pony_food,\n            other_stable,\n            book,\n            book_proxy_fk,\n            book_proxy_proxy_fk,\n            AAuthorProxyProxy,\n        ])\n\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertMigrationDependencies(changes, \"testapp\", 0, ['migrations'])\n        self.assertOperationTypes(changes, \"testapp\", 0, ['CreateModel'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, model_name='AAuthorProxyProxy')\n\n\n\n", "    def test_rename_model_with_dependencies(self):\n        before_states = self.make_project_state([\n            author_proxy,\n            book_proxy_fk,\n        ])\n        after_states = self.make_project_state([\n            author_proxy_proxy,\n            book_proxy_proxy_fk,\n        ])\n\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, \"testapp\", 2)\n        self.assertMigrationDependencies(changes, \"testapp\", 0, ['testapp.otherapp.Book'])  \n        self.assertMigrationDependencies(changes, \"testapp\", 1, [])\n\n        self.assertOperationTypes(changes, \"testapp\", 0, ['RenameModel'])\n        self.assertOperationTypes(changes, \"testapp\", 1, ['RenameModel'])\n", "    def test_migration_empty_app(self):\n        empty_changes = self.get_changes([], [])\n        self.assertEqual(len(empty_changes), 0)\n"], "sample_78": ["    def test_dance_command(self):\n        with captured_stderr() as stderr:\n            management.call_command('dance', stdout=StringIO(), stderr=stderr)\n        self.assertIn('Started dancing!', stderr.getvalue())\n        self.assertIn('Boogie woogie!', stderr.getvalue())\n", "    def test_dance_command(self):\n        # Mock out the current_app for testing\n        with mock.patch('django.apps.AppConfig.current_app', None) as mock_current_app:\n            command = dance.dance()\n            command.handle()  # Call the handle method of the command\n            self.assertEquals(mock_current_app.call_count, 0)\n\n\n", "    def test_no_translations(self):\n        from django.utils import translation\n        class MyCommand(BaseCommand):\n            @no_translations\n                return \"Hello\"\n\n        with translation.override('en'):\n            output = StringIO()\n            with captured_stderr() as stderr:\n                management.execute_from_command_line(\n                    ['python', '-m', 'user_commands.dance', 'my_command']\n                )\n            self.assertEqual(output.getvalue(), 'Hello\\n')\n            self.assertEqual(stderr.getvalue(), '')\n", "    def test_dance_command(self):\n        with captured_stderr() as stderr:\n            management.execute_from_command_line(['manage.py', 'dance', '--verbose'])\n        self.assertIn('This is a dance command', stderr.getvalue())\n", "    def test_management_command_help(self):\n        with captured_stderr() as stderr:\n            management.call_command('help', 'dance')\n        self.assertIn(\"Description:\", stderr.getvalue())\n        self.assertIn(\"Usage:\", stderr.getvalue())\n\n\n\n"], "sample_34": ["    def test_composite_unit_equality(", "compilation error", "    def test_composite_unit_decompose(self):\n        km = u.km\n        m = u.meter\n        u = u.CompositeUnit(1e3, [km], [1])\n        u_decomposed = u.decompose()\n        assert u_decomposed.bases == [km]\n        assert u_decomposed.powers == [1]\n        assert u_decomposed.scale == 1e3\n\n        # Test with a unit already in the bases\n        u = u.CompositeUnit(1e3, [km, m], [1, 1])\n        u_decomposed = u.decompose()\n        assert u_decomposed.bases == [km]\n        assert u_decomposed.powers == [1]\n        assert u_decomposed.scale == 1e3\n", "    def test_composite_unit_with_string_bases(\n        ):\n        with catch_warnings():\n            # Deprecation warnings are expected here\n            u1 = u.CompositeUnit(\n                1.0, [u.m / u.s, u.m], [1, 1]\n            )\n            assert repr(u1) == 'CompositeUnit(1.0 * m / s**1 * m**1)'\n\n            # Now a roundtrip via pickle, should work\n            u2 = pickle.loads(pickle.dumps(u1))\n            assert repr(u2) == repr(u1)\n\n", "    def test_pickleable(self):\n        u_original = u.meter\n\n        # Pickle the unit object\n        u_pickled = pickle.dumps(u_original)\n\n        # Unpickle it\n        u_unpickled = pickle.loads(u_pickled)\n\n        # Check if the unpickled unit is equivalent\n        assert u_original == u_unpickled\n\n\n\n"], "sample_385": ["    def test_required_field_has_no_empty_option(self):\n        form = RequiredBandForm()\n        rendered_html = form.as_p()\n        self.assertNotIn(self.empty_option, rendered_html)\n", "    def test_autocomplete_multiple_selected_with_allow_multiple_selected(self):\n        release_event_name = \"Test Release Event\"\n        translation.activate(None)\n        with self.settings(LANGUAGE_CODE=\"en\"):\n            with self.assertNumQueries(1):\n                ReleaseEvent.objects.create(name=release_event_name)\n            form = VideoStreamForm(data={\"release_event\": [release_event_name]})\n            self.assertTrue(form.is_valid())\n            field = form.fields[\"release_event\"]\n            widget = field.widget\n            rendered_html = widget.render(\n                \"release_event\", form.cleaned_data[\"release_event\"], {}\n            )\n\n            # Check that the HTML includes the correct options\n            self.assertIn(release_event_name, rendered_html)\n            self.assertNotIn(self.empty_option, rendered_html) \n\n\n\n", "    def test_autocomplete_select_required_field(self):\n        self.client.login(username=\"admin\", password=\"password\")\n        response = self.client.get(\"/admin/album/album/add/\")\n        self.assertContains(response, 'band:')\n        self.assertContains(response, '<input type=\"text\" name=\"band\"')\n        self.assertContains(response, 'data-placeholder=\"\"')\n        self.assertNotContains(response, 'value=\"\"')\n        self.assertNotContains(response, 'data-allow-clear=true')\n", "    def test_autocomplete_form(self):\n        with translation.override('en'):\n            form = AlbumForm()\n            self.assertIn(self.empty_option, form.as_p())\n            self.assertIn(\n                f'<select class=\"my-class admin-autocomplete\" data-ajax--url=\"/admin/album/autocomplete/\">',\n                form.as_p(),\n            )\n            self.assertIn(\n                'data-model-name=\"album\" data-field-name=\"band\" lang=\"en\"',\n                form.as_p(),\n            )\n            self.assertIn(\n                f'<select class=\"admin-autocomplete\" data-ajax--url=\"/admin/album/autocomplete/\">',\n                form.as_p(),\n            )\n", "    def test_autocomplete_required_field_with_required_widget(self):\n        with translation.override(\"en\"):\n            form = RequiredBandForm()\n            output = form.render(\n                \"band\",\n                value=\"\",\n                attrs=None,\n            )\n        self.assertIn(\n            '<div class=\"my-class\" data-allow-clear=\"false\">', output\n        )\n        self.assertNotIn(self.empty_option, output)\n\n\n"], "sample_97": ["    def test_iter_modules_and_files_with_zip_file(self):\n        with zipfile.ZipFile(tempfile.NamedTemporaryFile(mode='w').name, 'w') as zip_file:\n            zip_file.writestr('module/test_module.py', 'def test_function(): pass')\n        self.import_and_cleanup('module.test_module')\n        self.assertFileFound(Path(tempfile.NamedTemporaryFile().name))\n\n\n\n", "    def test_iter_modules_and_files_with_zip(self):\n        with tempfile.TemporaryDirectory() as tempdir:\n            zip_path = self.temporary_file('my_app.zip')\n            with zipfile.ZipFile(zip_path, 'w') as zf:\n                zf.write('__init__.py', compress_type=zipfile.ZIP_DEFLATED)\n                zf.write('module1.py', compress_type=zipfile.ZIP_DEFLATED)\n            \n            extend_sys_path([str(zip_path)])\n            self.import_and_cleanup('my_app')\n\n            self.assertFileFound(zip_path / '__init__.py')\n            self.assertFileFound(zip_path / 'module1.py')\n", "    def test_file_changed_signal(self):\n        with tempfile.NamedTemporaryFile(mode='w') as temp_file:\n            file_path = Path(temp_file.name)\n            self.assertFileNotFound(file_path)\n            autoreload.file_changed.clear_subscribed()\n            called = [False]\n\n                called[0] = True\n\n            autoreload.file_changed.connect(change_callback)\n            temp_file.write('This is test content')\n            temp_file.flush()\n            self.assertFileFound(file_path)\n            self.assertTrue(called[0])\n\n\n\n", "    def test_error_files(self):\n        error_file = self.temporary_file('error.py')\n        error_file.write('raise ValueError(\"An error occurred\")')\n\n        # Try to import the error file to create an error and populate _error_files\n        with self.assertRaises(ValueError):\n            self.import_and_cleanup(str(error_file.resolve()))\n        self.assertFileFound(error_file)\n\n        # Trigger a reload and ensure the error file is not reloaded\n        with self.assertRaises(ValueError):\n            self.import_and_cleanup(str(error_file.resolve()))\n\n\n", "    def test_file_changed_signal(self):\n        with mock.patch('django.utils.autoreload.file_changed') as mock_signal:\n            path = self.temporary_file('test_file.py')\n            with open(path, 'w') as f:\n                f.write('print(\"test\")')\n            autoreload.iter_all_python_module_files()\n            self.assertFileFound(path)\n            with open(path, 'a') as f:\n                f.write('\\nprint(\"added\")')\n            time.sleep(0.1)  # Allow potential signal processing\n            mock_signal.assert_called_once_with(sender=autoreload.StatReloader, file_path=str(path))\n\n\n\n"], "sample_446": ["    def test_floatformat_with_grouping(self):\n        context = {\n            \"a\": Decimal(\"12345.6789\"),\n            \"b\": Decimal(\"123456789.12345\"),\n            \"c\": float(\"1234.5678\"),\n            \"d\": float(\"9876.5432\"),\n        }\n        self.assertEqual(\n            self.render(self.template_from(\"floatformat01\", context)),\n            \"12,345.6789 123,456,789.1234\",\n        )\n", "    def test_floatformat_with_grouping(self):\n        self.assertEqual(\n            self.render_template(\"{% autoescape off %}{{ 12345.678|floatformat:2g }}\" \"%\"),\n            \"12,345.68\",\n        )\n        self.assertEqual(\n            self.render_template(\"{% autoescape off %}{{ 12345.678|floatformat:3u }}\" \"%\"),\n            \"12345.678\",\n        )\n\n\n", "    def test_floatformat_with_precision(self):\n        self.render_with_context(\n            \"floatformat01\", {\"a\": 1234.56789, \"b\": 1234.567}\n        )\n\n        self.assert_rendered(\n            \"1234.5679\"\n            \" 1234.568\"\n        )\n\n\n\n", "    def test_floatformat_with_precision_and_grouping(self):\n        with translation.override(languages=[\"en-us\"]):\n            a = Decimal(\"12345.6789\")\n            b = Decimal(\"1234567.890123\")\n            context = {\"a\": a, \"b\": b}\n            output = self.render(\n                \"test_floatformat_01.html\", context\n            )\n            expected = (\n                \"12345.6789 1234567.890123\"\n            )\n            self.assertEqual(output, expected)\n\n", "    def test_floatformat_with_grouping(self):\n        a = Decimal(\"123456.7890\")\n        b = Decimal(\"1234567.8901\")\n        with self.subTest(a=a, b=b):\n            self.assertEqual(\n                self.render_context({\"a\": a, \"b\": b}),\n                mark_safe(\"123,456.789\\n1,234,567.890\"),\n            )\n\n\n\n        with self.subTest(a=a, b=b, grouping_arg=\"2\"):\n            self.assertEqual(\n                self.render_context({\"a\": a, \"b\": b}, {\"floatformat\": \"2\"}),\n                mark_safe(\"123 456.79\\n1 234 567.89\"),\n            )\n\n\n\n\n"], "sample_457": ["    def test_unique_constraint_with_expressions(self):\n        with atomic():\n            Product.objects.create(name=\"Test Product 1\", description=\"Description 1\")\n            with self.assertRaises(IntegrityError):\n                Product.objects.create(name=F('name'), description=\"Description 2\")\n", "    def test_unique_constraint_with_expressions(self):\n        with atomic():\n            Product.objects.create(name=\"Test Product\", description=\"Test Description\")\n            with self.assertRaises(IntegrityError):\n                Product.objects.create(name=\"Test Product\", description=\"Test Description\")\n", "    def test_unique_constraint_with_condition(self):\n        with atomic():\n            UniqueConstraintConditionProduct.objects.create(\n                name='test product', description='test description'\n            )\n            with self.assertRaises(ValidationError):\n                UniqueConstraintConditionProduct.objects.create(\n                    name='test product', description='test description'\n                )\n            product = UniqueConstraintConditionProduct.objects.get(name='test product')\n            product.description = 'new description'\n            product.save(update_fields=['description'])\n            with self.assertRaises(ValidationError):\n                UniqueConstraintConditionProduct.objects.create(\n                    name='test product', description='new description'\n                )\n\n\n\n", "    def test_unique_constraint_conditional(self):\n        with atomic():\n            Product.objects.create(name=\"Product A\")\n            with self.assertRaises(ValidationError):\n                UniqueConstraintConditionProduct.objects.create(name=\"Product A\")\n\n\n\n", "    def test_check_constraint_creation_and_validation(self):\n        with atomic():\n            # Create a CheckConstraint with a simple check\n            Product.objects.create(name=\"Product 1\", price=10)\n            with ignore_warnings(category=RemovedInDjango60Warning):\n                check_constraint = CheckConstraint(\n                    check=Q(price__gte=5),\n                    name=\"price_gte_5\",\n                )\n                Product._meta.add_constraint(check_constraint)\n            try:\n                Product.objects.create(name=\"Product 2\", price=4)\n                self.fail(\"Expected ValidationError to be raised\")\n            except ValidationError:\n                pass\n\n            # Update existing product to violate the constraint\n            with ignore_warnings(category=RemovedInDjango60Warning):\n                Product.objects.filter(name=\"Product 1\").update(price=3)\n                try:\n                    Product.objects.get(name=\"Product 1\")\n                    self.fail(\"Expected ValidationError to be raised\")\n                except ValidationError:\n                    pass\n\n\n\n"], "sample_578": ["    def test_bar_edgewidth(self):\n\n        df = pd.DataFrame({\"x\": [1, 2, 3], \"y\": [4, 5, 6]})\n        p = Plot(data=df).add(Bars(edgewidth=\".25\"), {\"orient\": \"x\"})\n        bars = p.plot().axes[0].containers[-1].get_patches()\n        assert_array_equal(np.array([bar.get_linewidth() for bar in bars]), [0.25])\n", "    def test_bar_edgewidth(self):\n        data = pd.DataFrame({\"x\": [1, 2, 3], \"y\": [1, 2, 3]})\n        p = Plot(data=data).add(Bars(width=0.5, baseline=0, edgewidth=Mappable(1))).plot()\n        bars = p._figure.axes[0].containers[0].get_paths()[0].vertices\n        assert_array_equal(bars[:, 0], np.array([1, 2, 3]) + 0.25)\n        assert_array_equal(bars[:, 1], np.array([1, 2, 3]) + 0.25)\n\n", "    def test_bar_edgewidth(self):\n        variables = {\n            \"data\": pd.DataFrame({\"x\": [1, 2, 3], \"y\": [4, 5, 6]}),\n            \"x\": \"x\",\n            \"y\": \"y\",\n        }\n        mark_kws = {\"edgewidth\": 2.0}\n        layer_kws = {}\n        bars = self.plot_bars(variables, mark_kws, layer_kws)\n        for i, bar in enumerate(bars):\n            self.check_bar(bar, 1, 4, 1, 2)\n            assert bar.get_linewidth() == 2.0\n", "    def test_bar_edgewidth(self):\n        # Test edgewidth with a MappableFloat that is not auto\n\n        data = pd.DataFrame({\"x\": [1, 2, 3], \"y\": [10, 5, 15]})\n        with plt.plt.style.context(\"seaborn-darkgrid\"):\n            bars = self.plot_bars(\n                variables={\"data\": data, \"orient\": \"v\"},\n                mark_kws={\"edgewidth\": MappableFloat(2)},\n                layer_kws={\"scale\": {\"y\": {\"type\": \"linear\"}}},\n            )\n        for bar in bars:\n            self.check_bar(\n                bar, 1, 10, 0.8, 10,\n            )\n            assert bar.get_linewidth() == 2\n\n\n\n", "    def test_edgewidth_scaling(self):\n        df = pd.DataFrame({\"x\": [1, 2], \"y\": [3, 4]})\n        p = Plot(data=df).add(Bars(width=0.5, edgewidth=Bars.edgewidth),\n                      layer_kws={\"orient\": \"x\"})\n        ax = p._figure.axes[0]\n        bars = ax.containers[0]\n\n        assert_array_equal(bars.get_linewidths(), [0.1] * len(bars))\n        p.plot()\n\n\n\n"], "sample_301": ["    def test_file_changed_signal(self):\n        file_changed_called = [False]\n        @mock.patch('django.utils.autoreload.file_changed.send')\n            file_changed_called[0] = True\n        with mock.patch('django.utils.autoreload.file_changed.send', side_effect=mock_file_changed_send):\n            filename = self.temporary_file('test.py')\n            with open(filename, 'w') as f:\n                f.write('print(\"hello\")')\n            py_compile.compile(filename)\n            autoreload.trigger_reload(filename)\n            self.assertTrue(file_changed_called[0])\n", "    def test_iter_modules_and_files_with_zip(self):\n        with tempfile.TemporaryDirectory() as tempdir:\n            zip_path = Path(tempdir) / 'test_module.zip'\n            with zipfile.ZipFile(zip_path, 'w') as zipf:\n                zipf.write('__init__.py', arcname='__init__.py')\n                zipf.write('test_module.py', arcname='test_module.py')\n\n            with open('test_module.py', 'w') as f:\n                f.write('pass')\n\n            extend_sys_path([zip_path])\n            self.import_and_cleanup('test_module')\n\n            self.assertFileFound(zip_path / '__init__.py')\n            self.assertFileFound(zip_path / 'test_module.py')\n\n            del sys.modules['test_module']\n            zip_path.unlink() # Remove the zip file\n            self.assertFileNotFound(zip_path / '__init__.py')\n            self.assertFileNotFound(zip_path / 'test_module.py')\n", "    def test_iter_modules_and_files_with_zipfile(self):\n        with tempfile.TemporaryDirectory() as tempdir:\n            zip_path = Path(tempdir) / 'test.zip'\n            with zipfile.ZipFile(zip_path, 'w') as zf:\n                zf.write('__init__.py', arcname='my_module')\n\n            self.import_and_cleanup('my_module')\n\n            self.assertFileFound(zip_path)\n            self.assertFileFound(zip_path / '__init__.py')\n", "    def test_iter_files_with_zipfiles(self):\n        with zipfile.ZipFile(self.temporary_file(\"test_module.zip\"), 'w') as zf:\n            zf.write(\"test_module/hello.py\", arcname=\"hello.py\")\n            zf.write(\"test_module/__init__.py\", arcname=\"__init__.py\")\n        extend_sys_path(str(self.temporary_file(\"test_module.zip\").parent))\n        self.import_and_cleanup(\"test_module\")\n        self.assertFileFound(Path(\"test_module.zip\", \"hello.py\"))\n\n\n\n", "    def test_iter_modules_and_files_zip(self):\n        with tempfile.TemporaryDirectory() as temp_dir:\n            zip_path = Path(temp_dir) / \"test.zip\"\n            with zipfile.ZipFile(zip_path, 'w') as zf:\n                zf.writestr(\"module1.py\", \"print('hello')\")\n                zf.writestr(\"module2.py\", \"print('world')\")\n            extend_sys_path([temp_dir])\n            self.assertFileFound(zip_path / \"module1.py\")\n            self.assertFileFound(zip_path / \"module2.py\")\n            self.import_and_cleanup(\"module1\")\n            self.import_and_cleanup(\"module2\")\n"], "sample_318": ["    def test_reverse_with_error_handler(self):\n        with self.assertRaises(Resolver404):\n            reverse('no_reverse_view')\n\n", "    def test_reverse_with_no_matching_view(self):\n        with self.assertRaises(NoReverseMatch):\n            reverse('nosuchview', args=['arg1', 'arg2'], kwargs={'key': 'value'})\n", "    def test_resolve_error_handler(self):\n        with self.settings(\n            INSTALLED_APPS=[\n                'polls',\n            ],\n        ):\n            resolver = get_resolver(None)\n            for v in ['400', '403', '404', '500']:\n                handler = resolver.resolve_error_handler(v)\n                self.assertIsCallable(handler)\n            handler = resolver.resolve_error_handler('408')\n            self.assertIsCallable(handler)\n\n\n\n", "compilation error", "    def test_reverse_with_empty_path(self):\n        with self.assertRaises(NoReverseMatch):\n            reverse('empty_view')\n"], "sample_1196": ["    def test_binary_symbols(self):\n        x = Symbol('x')\n        s = FiniteSet(x, 1, 2)\n        contains = Contains(x, s)\n        assert contains.binary_symbols == {x}\n", "    def test_contains_with_set_containing_symbols(self):\n        x = Symbol('x', integer=True)\n        s = FiniteSet(x, 1, 2)\n        contains_expr = Contains(x, s)\n        assert contains_expr.as_set() == s\n", "    def test_binary_symbols(self):\n        x = Symbol('x')\n        s = FiniteSet(x)\n        contains = Contains(x, s)\n        assert contains.binary_symbols == {x}\n", "    def test_contains_binary_symbols(self):\n        x = Symbol('x')\n        s = FiniteSet(x)\n        contains = Contains(x, s)\n        assert contains.binary_symbols == {x}\n\n", "    def test_binary_symbols_with_eq(self):\n        x = Symbol('x')\n        s = FiniteSet(x)\n        contains_expr = Contains(x, s)\n        assert set(contains_expr.binary_symbols) == {x}\n"], "sample_608": ["    def test_summarize_variable_with_timedelta64(self):\n        var = xr.DataArray(np.arange(10).astype(\"timedelta64[ns]\"),\n                           dims=\"time\",\n                           name=\"time\")\n        expected = dedent(\"\"\"\n            time: \n            Dimensions: ('time',)\n            Data variables:\n              time: <xarray.DataArray (time: 10)>\n                time: timedelta64[ns]\n        \"\"\")\n        assert formatting.array_repr(var) == expected\n", "    def test_short_numpy_repr(data):\n        repr_data = formatting.short_numpy_repr(data)\n        assert repr_data in repr(data).split(\"\\n\")[0]\n\n", "    def test_array_repr_different_objects(self, arrays, expected):\n        left, right = arrays\n        assert formatting.array_repr(left) == expected.splitlines()[0]\n        assert formatting.array_repr(right) == expected.splitlines()[1]\n", "compilation error", "    def test_short_numpy_repr(self, array_type):\n        obj = array_type([[1, 2, 3], [4, 5, 6]])\n        expected_repr = repr(obj)\n        actual_repr = formatting.short_numpy_repr(obj)\n        assert actual_repr == expected_repr, (\n            f\"short_numpy_repr({array_type}) \"\n            f\"did not match expected representation\"\n        )\n\n\n\n"], "sample_1147": ["    def test_multiline_latex_custom_symbols(self):\n        from sympy.abc import x, y, alpha\n        x = symbols('x_i')\n        y = symbols('y_j')\n        alpha = symbols('alpha_k')\n        expr = sin(alpha*y) + exp(I*alpha) - cos(log(y)) \n        result = multiline_latex(x, expr, terms_per_line=2, environment=\"IEEEeqnarray\")\n        expected = r'\\begin{IEEEeqnarray}{rCl}' + '\\n' + r'x & = & e^{i \\alpha_k} \\nonumber' + '\\n' + r'& & + \\sin{\\left(\\alpha_k y_j \\right)} \\nonumber' + '\\n' + r'& & - \\cos{\\left(\\log{\\left(y_j \\right)} \\right)}' + '\\n' + r'\\end{IEEEeqnarray}'\n        self.assertEqual(result, expected) \n", "    def test_totient_symbols(self):\n        self.assertEqual(latex(divisor_sigma(x)), r\"\\sigma_{1}\")\n        self.assertEqual(latex(reduced_totient(x)), r\"\\sigma^*_{1}\")\n        self.assertEqual(latex(primenu(x)), r\"\\nu_{1}\")\n        self.assertEqual(latex(primeomega(x)), r\"\\Omega_{1}\")\n", "    def test_totient(self):\n        self.assertEqual(latex(divisor_sigma(2, 2), mode='inline'), '$\\\\sigma_{2}^{2}$')\n        self.assertEqual(latex(reduced_totient(2), mode='inline'), '$\\\\tau_{2}$')\n        self.assertEqual(latex(primenu(2), mode='inline'), '$\\nu_{2}$')\n        self.assertEqual(latex(primeomega(2), mode='inline'), '$\\Omega_{2}$')\n", "compilation error", "compilation error"], "sample_1008": ["    def test_partial_velocity_multiple_frames(self):\n        N = ReferenceFrame('N')\n        A = N.orientnew('A', 'Body', [1, 2, 3], '123')\n        B = A.orientnew('B', 'Body', [4, 5, 0], '213')\n        u1, u2, u3 = dynamicsymbols('u1, u2, u3')\n        A.set_ang_vel(N, u1 * A.x + u2 * A.y + u3 * A.z)\n        partial_vel = A.partial_velocity(N, u1, u2, u3)\n\n        self.assertEqual(partial_vel[0], A.x)\n        self.assertEqual(partial_vel[1], A.y)\n        self.assertEqual(partial_vel[2], A.z)\n\n\n\n", "    def test_partial_velocity_no_speeds(self):\n        N = ReferenceFrame('N')\n        A = N.orientnew('A', 'Body', [pi/2, pi/4, 0])\n        u1, u2 = dynamicsymbols('u1, u2')\n        A.set_ang_vel(N, u1 * A.x + u2 * N.y)\n        self.assertTrue(len(A.partial_velocity(N)) == 0)\n\n", "compilation error", "    def test_partial_velocity_axis_rotation(self):\n        N = ReferenceFrame('N')\n        A = N.orientnew('A', 'Axis', [q1, N.x + 2 * N.y])\n        u1 = dynamicsymbols('u1')\n        A.set_ang_vel(N, u1 * A.x)\n        partials = A.partial_velocity(N, u1)\n        self.assertEqual(partials, A.x)\n", "compilation error"], "sample_218": ["    def test_trunc_year_timezone(self):\n        # Test trunc year with timezone information\n        start_datetime = datetime(2023, 12, 31, 23, 59, 59, tzinfo=datetime_timezone(pytz.timezone('America/New_York')))\n        end_datetime = datetime(2024, 1, 1, 0, 0, 0, tzinfo=datetime_timezone(pytz.timezone('America/New_York')))\n        model = self.create_model(start_datetime, end_datetime)\n\n        truncated_year = TruncYear(model.start_datetime).resolve_expression().output_field\n        self.assertEqual(truncated_year, start_datetime.year)\n\n", "    def test_extract_year_from_datetime(self):\n        now = timezone.now()\n        # Create a model instance\n        model_instance = self.create_model(start_datetime=now - timedelta(days=365), end_datetime=now + timedelta(days=365))\n\n        # Test ExtractYear with datetime\n        extracted_year = ExtractYear(model_instance.start_datetime).resolve_expression().output_field\n        self.assertEqual(extracted_year, model_instance.start_datetime.year)\n\n        # Test ExtractYear with DateTimeField\n        query = DTModel.objects.filter(start_datetime__year=model_instance.start_datetime.year)\n        self.assertEqual(query.count(), 1)\n", "    def test_truncation_with_timezone(self):\n        # Test truncation functions with timezone conversions\n        with override_settings(USE_TZ=True):\n            tz_utc = pytz.utc\n            tz_local = pytz.timezone('America/New_York')\n            local_now = datetime(2023, 10, 26, 16, 30, 0, tzinfo=tz_local)\n\n            model = self.create_model(local_now, local_now + timedelta(days=2))\n\n            # Test TruncYear\n            self.assertEqual(TruncYear(model.start_datetime).resolve().output_field.name, 'year')\n            self.assertEquals(TruncYear(model.start_datetime).resolve(), 2023)\n\n            # Test TruncQuarter\n            self.assertEqual(TruncQuarter(model.start_datetime).resolve().output_field.name, 'quarter')\n            self.assertEquals(TruncQuarter(model.start_datetime).resolve(), 4)\n\n            # Test TruncMonth\n            self.assertEqual(TruncMonth(model.start_datetime).resolve().output_field.name, 'month')\n            self.assertEquals(TruncMonth(model.start_datetime).resolve(), 10)\n\n            # Test TruncDay\n            self.assertEqual(TruncDay(model.start_datetime).resolve().output_field.name, 'date')\n            self.assertEquals(TruncDay(model.start_datetime).resolve(), datetime(2023, 10, 26, 0, 0, 0))\n\n\n\n            # Test TruncWeek\n            self.assertEqual(TruncWeek(model.start_datetime).resolve().output_field.name, 'date')\n            self.assertEquals(TruncWeek(model.start_datetime).resolve(), datetime(2023, 10, 23, 0, 0, 0))\n\n            # Test TruncHour\n            self.assertEqual(TruncHour(model.start_datetime).resolve().output_field.name, 'time')\n            self.assertEquals(TruncHour(model.start_datetime).resolve(), datetime(2023, 10, 26, 16, 0, 0))\n\n            # Test", "    def test_truncate_datetime_with_timezone(self):\n        start_datetime = datetime(2023, 10, 27, 10, 30, 0, tzinfo=datetime_timezone.utc)\n        end_datetime = datetime(2023, 10, 28, 14, 45, 0, tzinfo=datetime_timezone.utc)\n        model = self.create_model(start_datetime, end_datetime)\n\n        # Truncate to different levels and verify\n        for kind in ['year', 'quarter', 'month', 'week', 'day', 'hour', 'minute', 'second']:\n            expected_truncated_datetime = truncate_to(model.start_datetime, kind, tzinfo=datetime_timezone.utc)\n            truncated_datetime = Trunc(model.start_datetime, kind).as_sqlite(connection=self.connection).output_field.name\n            self.assertEqual(truncated_datetime, expected_truncated_datetime)\n            \n            expected_truncated_datetime = truncate_to(model.end_datetime, kind, tzinfo=datetime_timezone.utc)\n            truncated_datetime = Trunc(model.end_datetime, kind).as_sqlite(connection=self.connection).output_field.name\n            self.assertEqual(truncated_datetime, expected_truncated_datetime)\n\n\n\n\n", "    def test_trunc_tzinfo_aware_datetime(self):\n        tzinfo = pytz.timezone('America/Chicago')\n        dt = datetime(2023, 12, 31, 23, 59, 59, tzinfo=tzinfo)\n        model = self.create_model(dt, None)\n\n        # Truncate to hour without specifying tzinfo\n        truncated_hour = Trunc('trunc', 'hour')(model.start_datetime).resolve()\n        self.assertEqual(truncated_hour, dt.replace(minute=0, second=0, microsecond=0))\n\n        # Truncate to year with tzinfo\n        truncated_year = Trunc('trunc', 'year')(model.start_datetime, tzinfo=tzinfo).resolve()\n        self.assertEqual(truncated_year, dt.replace(month=1, day=1, hour=0, minute=0, second=0, microsecond=0, tzinfo=tzinfo))\n\n"], "sample_977": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_351": ["    def test_modelchoice_field_queryset_iterator(self):\n        class TestForm(forms.Form):\n            article_choice = forms.ModelChoiceField(queryset=Article.objects.all())\n            category_choice = forms.ModelChoiceField(queryset=Category.objects.all())\n\n        form = TestForm()\n\n        self.assertEqual(len(form.fields['article_choice'].choices), 1)\n        self.assertEqual(len(form.fields['category_choice'].choices), 3)\n\n        self.assertIn(\n            (ModelChoiceIteratorValue(cls.article, 'Test Article'), 'Test Article'),\n            form.fields['article_choice'].choices,\n        )\n        self.assertIn(\n            (ModelChoiceIteratorValue(cls.c1, 'Entertainment'), 'Entertainment'),\n            form.fields['category_choice'].choices,\n        )\n        self.assertIn(\n            (ModelChoiceIteratorValue(cls.c2, 'A test'), 'A test'),\n            form.fields['category_choice'].choices,\n        )\n        self.assertIn(\n            (ModelChoiceIteratorValue(cls.c3, 'Third'), 'Third'),\n            form.fields['category_choice'].choices,\n        )\n\n\n\n", "    def test_modelchoice_field_with_queryset_and_empty_label(self):\n        class MyForm(forms.Form):\n            choice = forms.ModelChoiceField(\n                queryset=Category.objects.all(),\n                empty_label=\"---------\",\n            )\n\n        form = MyForm()\n        self.assertIn(\"---------\", form.fields['choice'].choices)\n        self.assertEqual(len(form.fields['choice'].choices), 3)\n", "    def test_modelchoicefield_with_label_from_instance(self):\n        class MyModel(models.Model):\n            name = models.CharField(max_length=100)\n\n        MyModel.objects.create(name='Test')\n\n        class MyForm(forms.Form):\n            choice = forms.ModelChoiceField(MyModel.objects.all(), label_from_instance=lambda obj: obj.name)\n\n        form = MyForm()\n        choice = form.fields['choice']\n        # Ensure the choices in the form are labelled correctly\n        self.assertEqual([str(choice.choices[i][1]) for i in range(len(choice.choices))], ['Test'])\n", "    def test_modelchoice_field_queryset_limit_choices_to(self):\n            return len(choices)\n\n        form = BaseModelForm(\n            initial={'category': self.c1.pk},\n        )\n\n        # Assert there are 3 choices initially\n        assert count_choices(form.fields['category'].choices) == 3\n\n        # Use a callable limit_choices_to\n        form = BaseModelForm(\n            initial={'category': self.c1.pk},\n            limit_choices_to=lambda: Category.objects.exclude(pk=self.c1.pk),\n        )\n\n        # Assert there are 2 choices now\n        assert count_choices(form.fields['category'].choices) == 2\n\n\n\n", "    def test_modelchoiceiterator_empty_label(self):\n        class EmptyLabelForm(forms.Form):\n            category = forms.ModelChoiceField(\n                queryset=Category.objects.all(), empty_label=\"---------\",\n                widget=CheckboxSelectMultiple\n            )\n        form = EmptyLabelForm()\n        self.assertEqual(form.fields[\"category\"].choices[0], (\"\", \"---------\"))\n"], "sample_526": ["def test_autodate_locator_non_standard_spans():\n    with rc_context({'timezone': 'US/Eastern'}):\n        fig, ax = plt.subplots()\n        dates = [datetime.datetime(2020, 1, 1) + datetime.timedelta(days=i)\n                 for i in range(10)]\n        ax.plot(dates, [i for i in range(10)])\n        ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n        ax.xaxis.set_major_formatter(mdates.AutoDateFormatter(mdates.AutoDateLocator()))\n        plt.draw()\n\n        # Test non-standard spans, expecting reasonable tick placement\n        # (within a few days of the actual intervals).\n        for span in [0.5, 2.75, 10.25]:\n            ax.xaxis.set_major_locator(mdates.AutoDateLocator(minticks=5, maxticks=10))\n            fig.autofmt_xdate()\n            plt.draw()\n            # Compare against a known good case\n\n            with image_comparison(baseline_images=['test_autodate_locator_non_standard.png'],\n                                  tol=0.05):\n                plt.savefig('test_autodate_locator_non_standard.png')\n\n\n\n", "    def test_auto_date_locator(self):\n        with rc_context({'axes.formatter.date': None, 'figure.autolayout': False}):\n            fig, ax = plt.subplots()\n            data = np.random.rand(100)\n            ax.plot(mdates.date2num(\n                [datetime.date(2010, i, 1) for i in range(1, 11)]), data)\n            ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n            ax.xaxis.set_major_formatter(mdates.AutoDateFormatter())\n\n\n", "    def test_concise_date_formatter(self, dt_format):\n        with rc_context({'date.converter': 'concise',\n                         'date.formats': [dt_format],\n                         'font.size': 10}):\n            fig, ax = plt.subplots()\n            x = np.arange(datetime.datetime(2023, 1, 1),\n                          datetime.datetime(2023, 1, 12),\n                          timedelta(days=1))\n            ax.plot(x, np.random.rand(len(x)))\n            ax.xaxis.set_major_formatter(mdates.ConciseDateFormatter(\n                ax.xaxis.get_major_locator(),\n                formats=[dt_format]))\n            plt.draw()\n            for fmt in mdates.ConciseDateFormatter.formats:\n                assert fmt in ax.xaxis.get_major_formatter()._formats\n            _api.check_figures()\n            plt.close(fig)\n\n\n\n", "    def test_microsecond_locator_far_from_epoch(self, image_comparison):\n        with rc_context(\n            {'timezone': 'UTC',\n             'figure.figsize': (4, 3),\n             \"date.converter\": \"concise\",\n             \"date.interval_multiples\": True,\n             \"xtick.major.size\": 6,\n             \"ytick.major.size\": 6}):\n\n            plt.clf()\n            fig, ax = plt.subplots()\n\n            dt_min = datetime.datetime(10000, 1, 1, 0, 0, 0)\n            dt_max = dt_min + datetime.timedelta(days=10)\n            ax.set_xlim(dt_min, dt_max)\n            ax.xaxis.set_major_locator(mdates.MicrosecondLocator(interval=50000))\n            ax.plot([dt_min, dt_max])\n            image_comparison(\n                'microsecond_locator_far',\n                savefig_kwargs={'bbox_inches': 'tight'}\n            )\n\n\n\n", "    def test_concise_date_formatter(self):\n        with rc_context({'date.converter': 'concise'}):\n            fig, ax = plt.subplots()\n            ax.xaxis.set_major_formatter(mdates.ConciseDateFormatter())\n            ax.set_xlim(\n                datetime.date(2021, 1, 1), datetime.date(2021, 12, 31)\n            )\n            plt.draw()\n            image_comparison(\n                self.fig,\n                'concise_date_formatter',\n                tol=0.001\n            )\n\n"], "sample_737": ["    def test_stop_words_fit_transform(self):\n        data = ['this is stop word', 'another stop word']\n        vectorizer = CountVectorizer(stop_words='english')\n        X = vectorizer.fit_transform(data)\n        assert_equal(vectorizer.stop_words_, ENGLISH_STOP_WORDS)\n        assert_array_equal(X.toarray(), [[0, 0], [0, 0]])\n\n        tfidf = TfidfVectorizer(stop_words='english')\n        X_tfidf = tfidf.fit_transform(data)\n        assert_array_equal(X_tfidf.toarray(), [[0, 0], [0, 0]])\n", "    def test_stop_words_removal(self):\n\n        # Test stop words removal if stop words list is provided\n        vectorizer = TfidfVectorizer(stop_words='english')\n        X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n        expected_stop_words = set(ENGLISH_STOP_WORDS)\n        self.assertTrue(\n            all(\n                term not in vectorizer.vocabulary_\n                for term in expected_stop_words\n            )\n        )\n\n        # Test stop words removal if stop words list is provided and vocabulary is not None\n        provided_stop_words = ['the', 'burger', 'copyright']\n        vectorizer = TfidfVectorizer(stop_words=provided_stop_words)\n        X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n        for term in provided_stop_words:\n            assert term not in vectorizer.vocabulary_\n\n\n\n", "    def test_fit_transform_with_custom_analyzer(self):\n        vectorizer = TfidfVectorizer(analyzer=lazy_analyze)\n        X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n        expected = sp.csc_matrix((np.ones(len(ALL_FOOD_DOCS)),\n                                  (0, 0)),\n                                 shape=(len(ALL_FOOD_DOCS), 1))\n        assert_array_equal(X, expected)\n\n\n\n", "    def test_vocabulary_mapping(self):\n        vec = CountVectorizer()\n        X = vec.fit_transform(ALL_FOOD_DOCS)\n        vocab = vec.vocabulary_\n        assert isinstance(vocab, dict)\n        assert vocab['pizza'] == 0\n        assert vocab['burger'] == 1\n        assert vocab['copyright'] == 2\n\n        # Ensure vocabulary is case-insensitive (default behavior)\n        assert vocab['PIZzA'] == vocab['pizza']\n", "    def test_empty_vocabulary(self):\n        X = TfidfVectorizer(vocabulary={}).fit_transform(ALL_FOOD_DOCS)\n        assert X.shape[1] == 0\n"], "sample_738": ["    def test_max_df_filtering(self):\n        # Test that max_df is applied correctly\n        vectorizer = TfidfVectorizer(max_df=0.5, stop_words='english')\n        X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n        expected_terms = ['the', 'pizza', 'burger', 'beer', 'coke', 'salad',\n                          'celeri', 'tomato', 'copyright', 'water', 'sparkling']\n        actual_terms = [term for term, idx in vectorizer.vocabulary_.items()\n                        if idx < X.shape[1]]\n        assert_array_equal(actual_terms, expected_terms)\n\n        # Check if common terms are removed\n        vectorizer = TfidfVectorizer(max_df=0.1, stop_words='english')\n        X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n        expected_terms = ['burger', 'pizza', 'coke', 'salad', 'celeri',\n                          'tomato', 'water', 'copyright', 'sparkling']\n        actual_terms = [term for term, idx in vectorizer.vocabulary_.items()\n                        if idx < X.shape[1]]\n        assert_array_equal(actual_terms, expected_terms)\n\n", "    def test_fit_transform_custom_analyzer(self):\n        vectorizer = TfidfVectorizer(analyzer=lambda doc: ['word', 'in', 'document'],\n                                      stop_words='english')\n        X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n        expected_features = ['word', 'document']\n        assert_array_equal(vectorizer.get_feature_names(), expected_features)\n\n        # Check if the counts are as expected\n        expected_counts = np.array([\n            [1, 1],\n            [1, 1],\n            [1, 1],\n            [1, 1],\n            [1, 1],\n            [1, 1],\n        ])\n        assert_array_almost_equal(X.toarray(), expected_counts)\n\n", "    def test_stop_words_removal_no_vocabulary(self):\n        vectorizer = TfidfVectorizer(\n            stop_words=\"english\", max_df=0.9, min_df=1, vocabulary=None)\n        X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n\n        # Check that the stop words are removed from the vocabulary\n        expected_vocabulary = set(['PIZZA', 'BURGER', 'BEER', 'COPYRIGHT',\n                                     'SALAD', 'CELERI', 'WATER',\n                                     'TOMATO', 'SPARKLING'])\n        self.assertSetEqual(set(vectorizer.vocabulary_),\n                           expected_vocabulary)\n\n        # Check that the stop words are not present in the transformed matrix\n        for stop_word in ENGLISH_STOP_WORDS:\n            assert_false(np.any(X.getcol(vectorizer.vocabulary_[stop_word])\n                                 != 0))\n", "    def test_stop_words_from_string(self):\n        vectorizer = TfidfVectorizer(stop_words='english')\n        X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n        expected_stop_words = set(ENGLISH_STOP_WORDS)\n        assert_equal(set(vectorizer.stop_words_), expected_stop_words)\n", "    def test_strip_accents(param):\n        v = TfidfVectorizer(strip_accents=param)\n        docs = ['caf\u00e9', '\u00e9t\u00e9', '\u00e1pples', '\u00fcnicode']\n        X = v.fit_transform(docs)\n        expected_features = ['cafe', 'ete', 'apples', 'unicode']\n        if param == 'unicode':\n            assert_array_equal(v.vocabulary_.keys(), expected_features)\n        elif param == 'ascii':\n            assert_array_equal(v.vocabulary_.keys(), ['cafe', 'ete', 'apples', 'unicode'])\n        else:\n            assert_array_equal(v.vocabulary_.keys(), docs)\n\n\n\n"], "sample_1041": ["    def test_kronecker_delta_matrix_element(self):\n        expr = MatrixElement(A, i, j)\n        k = symbols('k')\n        delta = KroneckerDelta(i, k) * KroneckerDelta(j, k)\n        self.assertEqual(expr.applyfunc(lambda x: delta),\n                         delta * expr)\n\n\n\n", "    def test_from_index_summation_trace(self):\n        expr = Sum(A[i, i], (i, 0, N-1))\n        result = MatrixExpr.from_index_summation(expr, N)\n        self.assertEqual(result, trace(A))\n", "    def test_from_index_summation_transpose(self):\n        expr = Sum(A[i, j]*B[j, k], (j, 0, n-1))\n        result = MatrixExpr.from_index_summation(expr, first_index=True, last_index=1)\n        expected = A.T * B\n        self.assertEqual(result, expected)\n\n", "    def test_from_index_summation_transpose(self):\n        expr = Sum(A[i, j]*B[j, k], (j, 0, n-1))\n        result = MatrixExpr.from_index_summation(expr, first_index=1, last_index=2)\n        expected = transpose(MatMul(A, B))\n        self.assertEqual(result, expected)\n\n", "    def test_from_index_summation_trace(self):\n        expr = Sum(A[i, i], (i, 0, n - 1))\n        result = MatrixExpr.from_index_summation(expr)\n        self.assertEqual(result, trace(A))\n"], "sample_772": ["    def test_bootstrap(self, param):\n        bootstrap = param\n\n        clf = FOREST_CLASSIFIERS[name](bootstrap=bootstrap,\n                                        n_estimators=100, random_state=1)\n        clf.fit(X_large, y_large)\n        \n        for estimate in clf.estimators_:\n            assert estimate.tree_.n_nodes_ - 1 == estimate.tree_.max_depth\n", "    def test_feature_importances(name):\n        \"\"\"Test feature importances for Classification and Regression.\"\"\"\n        ForestClassifier = FOREST_CLASSIFIERS[name]\n        ForestRegressor = FOREST_REGRESSORS[name]\n\n        clf = ForestClassifier(n_estimators=10, random_state=1, n_jobs=2)\n        reg = ForestRegressor(n_estimators=10, random_state=1, n_jobs=2)\n        X_large = X_large.astype(np.float32)\n\n        clf.fit(X_large, y_large)\n        reg.fit(X_large, y_large)\n\n        # Check importances for classification\n        assert_array_equal(clf.feature_importances_.shape, (X_large.shape[1], ))\n        assert_greater_equal(clf.feature_importances_.sum(), 0.95)\n        # Check importances for regression\n        assert_array_equal(reg.feature_importances_.shape, (X_large.shape[1], ))\n        assert_greater_equal(reg.feature_importances_.sum(), 0.95)\n\n", "def test_random_features(name):\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n    clf = ForestClassifier(n_estimators=10, max_features='sqrt', random_state=1)\n    clf.fit(X_large, y_large)\n    assert_greater(clf.n_features_, 0)\n    assert_less(clf.n_features_, X_large.shape[1])\n", "    def test_oob_score(name):\n        ForestClassifier = FOREST_CLASSIFIERS[name]\n\n        clf = ForestClassifier(n_estimators=10, oob_score=True, random_state=1)\n        clf.fit(X, y)\n        assert 'oob_score_' in clf.get_params() and clf.oob_score_ is not None\n\n", "compilation error"], "sample_727": ["compilation error", "def test_imputer_gridsearch(random_state=None):\n    X = np.array([[1, 2, 3],\n                  [4, np.nan, 6],\n                  [7, 8, 9]])\n    y = np.array([10, 11, 12])\n\n    param_grid = {'imputer__strategy': ['mean', 'median', 'most_frequent'],\n                   'imputer__missing_values': ['NaN', 0]}\n    grid = GridSearchCV(\n        Pipeline([('imputer', Imputer()), ('clf', tree.DecisionTreeClassifier())]),\n        param_grid, cv=2, random_state=random_state)\n    grid.fit(X, y)\n\n    # check that the best estimator is correctly set\n    assert grid.best_estimator_.named_steps['imputer'].strategy in ['mean', 'median', 'most_frequent']\n    assert grid.best_estimator_.named_steps['imputer'].missing_values in ['NaN', 0]\n\n\n\n", "compilation error", "compilation error", "compilation error"], "sample_1069": ["compilation error", "    def test_piecewise_nested_functions(self):\n        expr = Piecewise((x**2, x > 0), (x, True))\n        print(glsl_code(expr, assign_to='float y'))  \n        assert glsl_code(expr, assign_to='float y') == 'if (x > 0) {\\n   y = pow(x, 2.0);\\n} else {\\n   y = x;\\n}' \n", "    def test_bernoulli():\n        assert glsl_code(bernoulli(n, x)) == 'bernoulli_poly(n, x)'\n        for n in range(1, 4):\n            assert glsl_code(bernoulli(n, x)) == 'bernoulli_poly(n, x)'\n\n", "compilation error", "compilation error"], "sample_286": ["    def test_auto_now_add(self):\n        a = Article()\n        a.title = 'Test Article'\n        a.save()\n        self.assertEqual(a.created_at, a.updated_at)\n        self.assertTrue(a.created_at)\n        self.assertTrue(a.updated_at)\n", "    def test_creating_model_instance_with_many_to_many_field(self):\n        with self.subTest('normal creation'):\n            article = Article.objects.create(headline='Article 1', content='Content for Article 1')\n            author = self.create_author()\n\n            # Create without explicitly specifying the M2M field\n            article.authors.add(author) \n            \n            article.save()\n\n            article = Article.objects.get(headline='Article 1')\n            self.assertIn(author, article.authors.all())\n\n        with self.subTest('duplicated creation'):\n            # Attempt to create the same association again (should be a noop)\n            article = Article.objects.create(headline='Article 1', content='Content for Article 1')\n            existing_author = self.create_author()\n            article.authors.add(author) \n            article.save()\n\n            article = Article.objects.get(headline='Article 1')\n            self.assertIn(existing_author, article.authors.all())\n\n\n\n", "    def test_model_instance_creation_with_deferred_fields(self):\n        # Test creation of model instances with deferred fields\n        with self.assertNumQueries(1):\n            instance = ArticleSelectOnSave.objects.create(\n                title=\"Deferred field test\",\n                content=\"This article has deferred fields\",\n            )\n        self.assertEqual(instance.content, \"This article has deferred fields\")\n        self.assertIsNotNone(instance.extra_field)\n\n\n\n\n\n", "    def test_ordering_with_special_characters(self):\n        self.assertEqual(\n            FeaturedArticle.objects.order_by('name').values_list('name', flat=True),\n            ['article 1', 'article 2', 'article 3']\n        )\n        self.assertEqual(\n            FeaturedArticle.objects.order_by('-name').values_list('name', flat=True),\n            ['article 3', 'article 2', 'article 1']\n        )\n\n\n", "    def test_create_article_with_featured_article(self):\n        with self.subTest(\"Creating a FeaturedArticle with a related Article\"):\n            article = Article.objects.create(headline='First Article')\n            featured_article = FeaturedArticle.objects.create(article=article, description='Great!')\n            self.assertEqual(featured_article.article, article)\n"], "sample_236": ["    def test_cascade_to_null(self):\n        \"\"\"\n        Test that CASCADE to NULL behavior works correctly.\n        \"\"\"\n        a1 = create_a(name='A1')\n        b1 = create_b(parent=a1)\n        b2 = create_b(parent=a1)\n\n        with self.assertNumQueries(3) as queries:\n            a1.delete()\n\n        self.assertEqual(queries.count(), 3)\n\n        # Assert that the objects are deleted\n        with self.assertRaises(DoesNotExist):\n            a1.refresh_from_db()\n        with self.assertRaises(DoesNotExist):\n            b1.refresh_from_db()\n        with self.assertRaises(DoesNotExist):\n            b2.refresh_from_db()\n", "    def test_collect_recursive_delete_with_restricted_objects(self):\n        p = P(name='Parent 1')\n        r = R(parent=p)\n\n        with self.assertNumQueries(2):  \n            B1.objects.create(name='Child 1')\n\n        r.delete()\n\n        p = P.objects.get(name='Parent 1')\n        with transaction.atomic(using=connection.connection.settings_dict['DEFAULT']):\n            p.delete()\n", "    def test_protected_error(self):\n        a1 = create_a()\n        b1 = B1.objects.create(a=a1)\n        try:\n            Collector().collect([b1])  # Triggers ProtectedError\n        except ProtectedError as e:\n            self.assertIn(b1, e.protected_objects)\n            self.assertEqual(str(e), \"Cannot delete some instances of model 'b1' because they are referenced through protected foreign keys: <QuerySet [<B1: ...>] >.\")\n        else:\n            self.fail('ProtectedError not raised')\n\n", "    def test_cascade_to_protected_field(self):\n        a = create_a(A.b12_set=create_a.B12_set(models.Manager))\n        b1 = a.b12_set[0]\n        b2 = a.b12_set[1]\n\n        with self.assertRaises(ProtectedError):\n            b1.delete()\n", "    def test_protected_error(self):\n        b1 = create_a()\n        with self.assertRaises(ProtectedError) as cm:\n            delete_bottom = DeleteBottom.objects.create(b1=b1)\n            delete_bottom.delete()\n        self.assertEqual(\n            str(cm.exception),\n            'Cannot delete some instances of model \\'deletebottom\\' because they are '\n            'referenced through protected foreign keys: [<DeleteBottom: 1>]'\n        )\n\n\n\n"], "sample_1197": ["    def test_get_dimensional_expr(self):\n        from sympy.physics.units import Quantity\n        self.assertEqual(\n            SI.get_dimensional_expr(meter), Dimension(length)\n        )\n        self.assertEqual(\n            SI.get_dimensional_expr(Quantity(1, meter)),\n            Dimension(length)\n        )\n        self.assertEqual(\n            SI.get_dimensional_expr(1.0), S.One\n        )\n        self.assertEqual(\n            SI.get_dimensional_expr(meter**2), Dimension(length)**2\n        )\n        self.assertEqual(\n            SI.get_dimensional_expr(meter**(-2)),\n            Dimension(length)**(-2)\n        )\n        self.assertEqual(\n            SI.get_dimensional_expr(2*meter), 2*Dimension(length)\n        )\n        self.assertEqual(\n            SI.get_dimensional_expr(kilometer), Dimension(length)\n        )\n        self.assertEqual(\n            SI.get_dimensional_expr(Quantity(1, kilometer)), Dimension(length)\n        )\n        self.assertEqual(\n            SI.get_dimensional_expr(meter * second),\n            Dimension(length) * Dimension(time)\n        )\n        self.assertEqual(\n            SI.get_dimensional_expr(Quantity(1, meter * second)),\n            Dimension(length) * Dimension(time)\n        )\n        e = Symbol('e')\n        self.assertEqual(SI.get_dimensional_expr(e**2),\n            Function('e')(e)*Dimension(1))\n        self.assertEqual(\n            SI.get_dimensional_expr(integrate(meter, e)),\n            Dimension(length)*Dimension(time)\n        )\n        self.assertEqual(\n            SI.get_dimensional_expr(diff(meter, e)),\n            Dimension(length)/Dimension(time)\n        )\n        self.assertEqual(\n            SI.get_dimensional_expr(sqrt(meter)),\n            Dimension(length)**(1/2)\n        )\n        self.assertEqual(\n            SI.get_dimensional_expr(sin(meter)),\n            Dimension(1)\n        )\n        self.assertEqual(\n            SI.get_dimensional_expr(log", "    def test_get_dimensional_expr_mixed_types(self):\n        u = SI.meter\n        v = SI.second\n        k = SI.kilogram\n        expr = Add(u, k ** 2, v ** 3)\n\n        dim_expr = si.get_dimensional_expr(expr)\n        expected_dim = Dimension(length + mass**2 + time**3)\n        self.assertTrue(dim_expr.equals(expected_dim))\n\n\n", "    def test_extend(self):\n        si = SI()\n        new_unit_system = si.extend(base=['m2'], units=[Quantity(4, 'm')],\n                                     name='New SI',\n                                     description='Extended SI system')\n        assert new_unit_system.name == 'New SI'\n        assert new_unit_system.descr == 'Extended SI system'\n        assert len(new_unit_system._base_units) == 2\n        assert new_unit_system._units == tuple(si._units + (Quantity(4, 'm'),))\n", "    def test_get_units_non_prefixed(self):\n        self.assertEqual(set(SI.get_units_non_prefixed()),\n                         set([meter, kilogram, second]))\n", "    def test_get_units_non_prefixed(self):\n        self.assertEqual(set(SI.get_units_non_prefixed()), {\n            Quantity(meter), Quantity(second), Quantity(kilogram)\n        })\n"], "sample_554": ["    def test_annotation_offset_coords(self, image_comparison):\n        fig, ax = plt.subplots()\n\n        annotation = Annotation(\"Hello\", (0.5, 0.5), xycoords='data',\n                             textcoords='offset points',\n                             annotation_clip=False)\n        annotation.set_fontsize(10)\n        annotation.set_fontweight('bold')\n        annotation.set_horizontalalignment('center')\n        annotation.set_verticalalignment('center')\n\n        ax.add_artist(annotation)\n        plt.savefig(self.get_test_filename())\n\n        with warnings.catch_warnings(record=True) as warning_list:\n            warnings.simplefilter(\"ignore\", category=mpl.cbook.mplDeprecation)\n            fig, ax = plt.subplots()\n            annotation2 = Annotation(\"World\", (0.5, 0.5), xycoords='data',\n                                     textcoords='offset pixels',\n                                     annotation_clip=False)\n            annotation2.set_fontsize(10)\n            annotation2.set_fontweight('bold')\n            annotation2.set_horizontalalignment('center')\n            annotation2.set_verticalalignment('center')\n\n            ax.add_artist(annotation2)\n            plt.savefig(self.get_test_filename())\n\n", "    def test_annotation_offset(self, image_comparison):\n        fig, ax = plt.subplots()\n\n        # test offset coordinates\n        offset_annotation = Annotation('Offset', (0, 0), xycoords='offset points',\n                                        textcoords='offset pixels',\n                                        xytext=(10, 10), annotation_clip=False)\n        offset_annotation.set_fontsize(10)\n        ax.add_artist(offset_annotation)\n\n        plt.draw()\n        plt.pause(0.01)\n        image_comparison(filename='annotation_offset')\n", "    def test_annotation_xycoords_offset(self):\n        fig, ax = plt.subplots()\n\n        # Simple offset to verify offset coordinates from text\n        ann = Annotation('Text', (1, 1), xycoords='data',\n                         textcoords='offset points',\n                         arrowprops=dict(arrowstyle='->'))\n        ann.draggable(use_blit=True)\n        ax.add_artist(ann)\n\n        # Simple offset to verify offset coordinates from text\n        ann2 = Annotation('Text2', (1, 1), xycoords='data',\n                         textcoords='offset pixels',\n                         arrowprops=dict(arrowstyle='->'))\n        ann2.draggable(use_blit=True)\n        ax.add_artist(ann2)\n\n        plt.show()\n\n", "    def test_offset_coords(self):\n        fig, ax = plt.subplots()\n        ann = Annotation(\n            'Hello',\n            xy=(0.5, 0.5),\n            xycoords='figure fraction',\n            textcoords='offset points',\n            xytext=(20, 10),\n        )\n        ax.add_artist(ann)\n        plt.draw()\n\n        bbox = ann.get_window_extent()\n        assert bbox.x0 > 0\n        assert bbox.y0 > 0\n\n        x, y = ann.get_position()\n        assert x == 0.5 + 20 / fig.dpi\n        assert y == 0.5 + 10 / fig.dpi\n\n", "        def test_annotation_arrow_style(fig, ax):\n            # Check that arrowstyle is applied correctly\n            a = Annotation(\"test\", (0.5, 0.5), xytext=(0.8, 0.5),\n                           arrowprops={'arrowstyle': '->'})\n            a.set_figure(fig)\n            a.axes = ax\n            fig.canvas.draw()\n            assert a.arrow_patch.arrowstyle == '->'\n\n            a = Annotation(\"test\", (0.5, 0.5), xytext=(0.8, 0.5),\n                           arrowprops={'arrowstyle': '->'})\n            a.set_figure(fig)\n            a.axes = ax\n            fig.canvas.draw()\n            arr_patch = a.arrow_patch\n            assert arr_patch.arrowstyle == '->'\n"], "sample_690": ["    def test_evaluate_skip_marks(self, pytester: Pytester):\n        pytester.parse(\n            \"\"\"\n                pass\n            \"\"\",\n            rootdir=\".\".join(sys.argv[:2]),\n        )\n\n        item = pytester.getitems()[-1]\n        skipped = evaluate_skip_marks(item)\n        assert skipped is None\n\n        pytester.parse(\n            \"\"\"\n            @pytest.mark.skip(reason=\"i am skipped\")\n                pass\n            \"\"\",\n            rootdir=\".\".join(sys.argv[:2]),\n        )\n        item = pytester.getitems()[-1]\n        skipped = evaluate_skip_marks(item)\n        assert skipped.reason == \"i am skipped\"\n\n        pytester.parse(\n            \"\"\"\n            @pytest.mark.skipif(sys.platform == \"win32\", reason=\"only on non-win32\")\n                pass\n            \"\"\",\n            rootdir=\".\".join(sys.argv[:2]),\n        )\n        item = pytester.getitems()[-1]\n        skipped = evaluate_skip_marks(item)\n        assert skipped.reason == \"only on non-win32\"\n", "    def test_evaluate_condition(self, testdir, condition, expected_result, expected_reason):\n        item = testdir.getitem(\"test_evaluatecondition.py::test_evaluatecondition\")\n        result, reason = evaluate_condition(item, None, condition)\n        assert result == expected_result\n        assert reason == expected_reason if expected_reason else expected_reason\n\n\n", "    def test_skip(self, pytester):\n        p = pytester.parse(\n            textwrap.dedent(\n                \"\"\"\n                pass\n\n                pass\n\n                skip(\"reason\")\n            \"\"\"\n            )\n        )\n        results = p.get_results()\n\n        assert len(results) == 3\n        assert results[2].outcome == 'skipped'\n        assert results[2].longrepr == \"reason\"\n", "    def test_skip_xfail_markers(self, pytester: Pytester):\n        \"\"\"Test evaluation of skip and xfail markers.\"\"\"\n        testdir = pytester.mkdir(\"test_markers\")\n        testdir.write(\n            \"\"\"\n                pass\n\n                if sys.platform == 'win32':\n                    pass\n                else:\n                    raise ValueError(\"This test should not be run on Windows\")\n\n            @pytest.mark.xfail(reason=\"This test is expected to fail\")\n                raise ValueError(\"This test is expected to fail\")\n            \"\"\",\n            \"test_markers/test_skipping.py\",\n        )\n\n        # Skip tests\n        result = testdir.runpytest(\n            \"-m\", \"skip\"\n        )\n        result.assert_outcomes(passed=1, skipped=2)\n\n        # Skipif tests\n        result = testdir.runpytest(\n            \"-m\", \"skipif=sys.platform=='win32'\"\n        )\n        result.assert_outcomes(passed=1, skipped=1)\n\n        # Xfail tests\n        result = testdir.runpytest()\n        result.assert_outcomes(passed=1, xfailed=1)\n\n        # Runwith xfail_strict\n        result = testdir.runpytest(\"--xfail_strict=True\")\n        result.assert_outcomes(passed=1, xfailed=1)\n\n        # Runwith --runxfail\n        result = testdir.runpytest(\"--runxfail\")\n        result.assert_outcomes(passed=3)\n\n\n\n", "    def test_evaluate_skip_marks(self, pytester):\n        pytester.raises(skip, \"from _pytest.skipping import evaluate_skip_marks\",)\n\n\n\n        item = pytester.getitem(\"test_func\")\n        with pytester.temp_dir() as td:\n            path = os.path.join(td, \"test_module.py\")\n            with open(path, \"w\") as f:\n                f.write(textwrap.dedent(\"\"\"\n                        pass\n                \"\"\"))\n            item.parent.fspath = path\n\n            item._store[skipped_by_mark_key] = False\n            skipped = evaluate_skip_marks(item)\n            assert skipped is None\n\n            item._store[skipped_by_mark_key] = False\n            item.add_marker(pytest.mark.skip(\"reason\"))\n            skipped = evaluate_skip_marks(item)\n            assert skipped.reason == \"reason\"\n\n            item._store[skipped_by_mark_key] = False\n            item.add_marker(pytest.mark.skipif(\"False\", reason=\"reason\"))\n            skipped = evaluate_skip_marks(item)\n            assert skipped is None\n\n            item._store[skipped_by_mark_key] = False\n            item.add_marker(pytest.mark.skipif(\"True\", reason=\"reason\"))\n            skipped = evaluate_skip_marks(item)\n            assert skipped.reason == \"reason\"\n\n"], "sample_1179": ["    def test_printing_sum_with_limits(self):\n        expr = Sum(x, (x, 0, 5))\n        self.assertEqual(sstr(expr), 'Sum(x, (x, 0, 5))')\n", "    def test_tensor_printing(self):\n        a = ArraySymbol('a', shape=(3, 2))\n        b = ArrayElement(a, (1, 0))\n\n        self.assertEqual(sstr(a), \"a\")\n        self.assertEqual(sstr(b), \"a[1, 0]\")\n       \n        A = Matrix([[1, 2], [3, 4]])\n        B = Matrix([[5, 6], [7, 8]])\n        self.assertEqual(sstr(A + B), \"Matrix([[6, 8], [10, 12]])\")\n\n        C = A * B\n        self.assertEqual(sstr(C), \"Matrix([[19, 22], [43, 50]])\")\n\n        D = TransferFunction(1, [1, 2, 3])\n        self.assertEqual(sstr(D), \"(1)/(1 + 2*s + 3*s**2)\")\n\n        E = AccumBounds(x, 0, 1)\n        self.assertEqual(sstr(E), \"Sum(x, (x, 0, 1))\")\n\n        F = NDimArray('F', shape=(3, 2, 3))\n        self.assertEqual(sstr(F), \"F\")\n        self.assertEqual(sstrrepr(F), \"NDimArray('F', shape=(3, 2, 3))\")\n\n\n\n", "compilation error", "    def test_print_FiniteSet(self):\n        F = FiniteSet(1, 2, 3)\n        self.assertEqual(sstr(F), 'FiniteSet(1, 2, 3)')\n\n        F2 = FiniteSet(*range(10))\n        self.assertEqual(sstr(F2), 'FiniteSet(0, 1, 2, 3, 4, 5, 6, 7, 8, 9)')\n", "    def test_complex_quaternion(self):\n        q = Quaternion(2, 3, 4, 5)\n        self.assertEqual(sstr(q), \"Quaternion(2, 3, 4, 5)\")\n"], "sample_856": ["    def test_kfold_with_sparse_data(self):\n        X = coo_matrix((np.ones(5), (range(5), range(5))))\n        cv = KFold(n_splits=3)\n        for train_index, test_index in cv.split(X):\n            X_train = X[train_index]\n            X_test = X[test_index]\n            assert_array_equal(X_train.shape, (3, 3))\n            assert_array_equal(X_test.shape, (2, 2))\n", "    def test_train_test_split_sparse(self):\n        X = csc_matrix((np.ones(5), (np.arange(5), np.arange(5))),\n                       shape=(5, 5))\n        X_train, X_test, _, _ = train_test_split(X, test_size=0.3,\n                                                 random_state=42)\n        assert isinstance(X_train, csr_matrix)\n        assert isinstance(X_test, csr_matrix)\n        assert X_train.shape[0] == 3\n        assert X_test.shape[0] == 2\n", "    def test_train_test_split_inputs(self, test_size, train_size):\n        X, y = np.random.rand(100, 2), np.random.rand(100)\n        X_train, X_test, y_train, y_test = train_test_split(X, y,\n                test_size=test_size, train_size=train_size)\n\n        assert len(X_train) + len(X_test) == len(X)\n        assert len(y_train) + len(y_test) == len(y)\n\n", "    def test_train_test_split_sparse_input(self):\n        X = coo_matrix((np.ones(10), (range(10), range(10))), shape=(10, 10))\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n                                                          random_state=42)\n        assert isinstance(X_train, csr_matrix)\n        assert isinstance(X_test, csr_matrix)\n", "    def test_time_series_split(self):\n        tscv = TimeSeriesSplit(n_splits=3)\n        X = np.arange(10).reshape(10, 1)\n        y = np.arange(10)\n        for train_index, test_index in tscv.split(X):\n            assert train_index.max() <= test_index[0]\n\n"], "sample_362": ["    def test_migration_no_changes(self):\n        before_states = [\n            author_name_deconstructible_list_1,\n            author_name_deconstructible_list_2,\n            author_name_deconstructible_list_3,\n            author_name_deconstructible_tuple_1,\n            author_name_deconstructible_tuple_2,\n            author_name_deconstructible_tuple_3,\n            author_name_deconstructible_dict_1,\n            author_name_deconstructible_dict_2,\n            author_name_deconstructible_dict_3,\n            author_name_nested_deconstructible_1,\n            author_name_nested_deconstructible_2,\n            author_custom_pk,\n            author_with_biography_non_blank,\n            author_with_biography_blank,\n            author_with_book,\n            author_with_book_order_wrt,\n            author_with_publisher_string,\n            author_with_publisher,\n            author_with_user,\n            publisher_with_author,\n            publisher_with_aardvark_author,\n            book,\n            other_pony,\n            other_pony_food,\n            other_stable,\n            third_thing,\n        ]\n        after_states = [\n            author_name_deconstructible_list_1,\n            author_name_deconstructible_list_2,\n            author_name_deconstructible_list_3,\n            author_name_deconstructible_tuple_1,\n            author_name_deconstructible_tuple_2,\n            author_name_deconstructible_tuple_3,\n            author_name_deconstructible_dict_1,\n            author_name_deconstructible_dict_2,\n            author_name_deconstructible_dict_3,\n            author_name_nested_deconstructible_1,\n            author_name_nested_deconstructible_2,\n            author_custom_pk,\n            author_with_biography_non_blank,\n            author_with_biography_blank,\n            author_with_book,\n            author_with_book_order", "    def test_migration_autodetection_with_custom_fks(self):\n        before_states = self.make_project_state([\n            author_pk_fk_author,\n        ])\n        after_states = self.make_project_state([\n            aardvark_pk_fk_author,\n        ])\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertMigrationDependencies(changes, 'testapp', 0, [])\n        self.assertOperationTypes(changes, 'testapp', 0, ['RemoveField', 'CreateModel'])\n", "    def test_proxy_inheritance(self):\n        before_states = [\n            author_proxy,\n            author_proxy_options,\n            author_proxy_third,\n            author_proxy_third_notproxy,\n            other_pony,\n            other_pony_food,\n            other_stable,\n            third_thing,\n            book,\n            book_proxy_fk,\n            book_proxy_proxy_fk,\n            book_migrations_fk,\n            book_with_no_author_fk,\n            book_with_no_author,\n        ]\n        after_states = [\n            author_proxy,\n            author_proxy_options,\n            author_proxy_third,\n            author_proxy_third_notproxy,\n            other_pony,\n            other_pony_food,\n            other_stable,\n            third_thing,\n            book,\n            book_proxy_fk,\n            book_proxy_proxy_fk,\n            book_migrations_fk,\n            book_with_no_author_fk,\n            book_with_no_author,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertEqual(len(changes), 0)\n\n        before_states = [\n            author_proxy,\n            author_proxy_options,\n            author_proxy_third,\n            author_proxy_third_notproxy,\n            other_pony,\n            other_pony_food,\n            other_stable,\n            third_thing,\n            book,\n            book_proxy_fk,\n            book_proxy_proxy_fk,\n            book_migrations_fk,\n            book_with_no_author_fk,\n            book_with_no_author,\n        ]\n        after_states = [\n            author_proxy,\n            author_proxy_options,\n            author_proxy_third,\n            author_proxy_third_notproxy,\n            other_pony,\n            other_pony_food,\n            other_stable,\n            third_thing,\n            book,\n            book_proxy_fk,\n            book_proxy_proxy_fk,\n            book_migrations_fk,\n            book_with_no_author", "    def test_detect_changes_proxy_model_with_fk(self):\n        before_states = self.make_project_state([\n            author_proxy,\n            book_proxy_fk,\n        ])\n        after_states = self.make_project_state([\n            author_proxy,\n            book_proxy_fk,\n        ])\n        changes = self.get_changes(before_states, after_states)\n        self.assertEqual(len(changes), 0)\n", "    def test_custom_types_in_fields(self):\n        changes = self.get_changes([\n            author_name_deconstructible_list_1,\n            author_name_deconstructible_list_2,\n            author_name_deconstructible_list_3,\n            author_name_deconstructible_tuple_1,\n            author_name_deconstructible_tuple_2,\n            author_name_deconstructible_tuple_3,\n            author_name_deconstructible_dict_1,\n            author_name_deconstructible_dict_2,\n            author_name_deconstructible_dict_3,\n        ], [\n            author_name_deconstructible_list_1,\n            author_name_deconstructible_list_2,\n            author_name_deconstructible_list_3,\n            author_name_deconstructible_tuple_1,\n            author_name_deconstructible_tuple_2,\n            author_name_deconstructible_tuple_3,\n            author_name_deconstructible_dict_1,\n            author_name_deconstructible_dict_2,\n            author_name_deconstructible_dict_3,\n            author_with_biography_non_blank,\n        ])\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertMigrationDependencies(changes, \"testapp\", 0, [])\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"CreateModel\", \"AddField\"])\n\n\n\n"], "sample_198": ["    def test_function_ordering(self):\n        # Test ordering with aggregate functions\n        ordering = OrderBy(Func('num_employees', output_field=IntegerField()), descending=True)\n        result = self.company_query.order_by(ordering).values_list(\"name\", flat=True)\n        expected = [\n            'Foobar Ltd.', 'Test GmbH',\n            'Example Inc.',\n        ]\n        self.assertEqual(list(result), expected)\n\n\n", "    def test_col_expression(self):\n        col_exp = Col('name')\n        self.assertEqual(str(col_exp), 'name')\n        self.assertEqual(col_exp.as_sql(self.company_query.compiler, connection), ('name', []))\n        self.assertEqual(col_exp.output_field, CharField())\n", "    def test_ordering(self):\n        ordering_query = self.company_query.order_by(\n            \"num_employees\", \"-num_chairs\", \"name\"\n        )\n        result = ordering_query.values_list(\n            \"name\", \"num_employees\", \"num_chairs\", order_by=(\"name\", \"num_employees\", \"num_chairs\"), flat=True\n        )\n        expected_ordering = [\n            ('Example Inc.', 2300, 5), \n            ('Foobar Ltd.', 3, 4), \n            ('Test GmbH', 32, 1),\n        ]\n        self.assertEqual(list(result), expected_ordering)\n\n\n\n", "    def test_subquery_with_expression_ordering(self):\n        with isolate_apps():\n            subquery = (\n                Company.objects.filter(num_employees__gt=10)\n                .values(\"name\")\n                .annotate(name_upper=Upper('name'))\n            )\n            query = Company.objects.filter(\n                name__in=Subquery(subquery, output_field=CharField())\n            )\n            result = query.values('name').all()\n            self.assertEqual(len(result), 1)\n            self.assertEqual(result[0]['name'], 'Example Inc.')\n\n\n\n", "    def test_window_expressions(self):\n        with self.subTest('Rows'):\n            rows_frame = Window(\n                Sum('num_employees'),\n                partition_by=Company.name,\n                order_by=Company.name,\n                frame=RowRange(start=1, end=2)\n            )\n            result = rows_frame.as_sql(connection.ops)\n            self.assertIn(\n                'SUM(COALESCE(num_employees, 0)) OVER (PARTITION BY \"companies\".\"name\" ORDER BY \"companies\".\"name\" ROWS BETWEEN 1 PRECEDING AND 2 FOLLOWING)',\n                result\n            )\n        with self.subTest('Range'):\n            range_frame = Window(\n                Sum('num_employees'),\n                partition_by=Company.name,\n                order_by=Company.name,\n                frame=ValueRange(start=1, end=2)\n            )\n            result = range_frame.as_sql(connection.ops)\n            self.assertIn(\n                'SUM(COALESCE(num_employees, 0)) OVER (PARTITION BY \"companies\".\"name\" ORDER BY \"companies\".\"name\" RANGE BETWEEN 1 PRECEDING AND 2 FOLLOWING)',\n                result\n            )\n\n\n\n"], "sample_130": ["    def test_trim_start_with_m2m_join(self):\n        author1 = Author.objects.create(name='Author 1')\n        author2 = Author.objects.create(name='Author 2')\n        item1 = Item.objects.create(title='Item 1', author=author1)\n        item2 = Item.objects.create(title='Item 2', author=author2)\n\n        q = Item.objects.filter(\n            author__name='Author 1'\n        ).select_related('author')\n        q.trim_start(name_with_path=['author', 'item'],\n        lookup_table_alias=['item_table'])\n        self.assertEqual(q.where.children[0].lhs.field.name, 'item.author')\n\n        q = Item.objects.filter(\n            author__name='Author 1'\n        ).select_related('author')\n        q.trim_start(\n            name_with_path=['author__items', 'item'],\n            lookup_table_alias=['item_table']\n        )\n        self.assertEqual(q.where.children[0].lhs.field.name, 'item.author_id')\n\n\n\n", "    def test_build_filter_with_subquery(self):\n        with self.subTest(\n            \"subquery should be wrapped with OUTER query\"\n        ):\n            q = Ranking.objects.filter(\n                object_id__in=Query.objects.filter(\n                    item__author=Author.objects.get(pk=1)\n                )\n            )\n            self.assertEqual(q._query.trim_joins(), (\n                'item__object_id',\n                True\n            ))\n\n", "    def test_add_fields_with_nested_m2m(self):\n        q = Query(\n            defaults=None,\n            ordering=None,\n            limit=None,\n            offset=None,\n            extra_select=None,\n            select_related=None,\n            hints=None,\n            using=None,\n            group_by=None,\n            for_update=None,\n            low_priority=None\n        )\n        q.add_fields(\n            [\n                'item',\n                'item__tags',\n            ]\n        )\n        sql = q.as_sql()\n        # Check that the query includes both 'item' and 'item__tags'\n        self.assertIn('item', sql)\n        self.assertIn('item__tags', sql)\n\n\n\n", "    def test_add_select_related(self):\n        q = Ranking.objects.select_related('author', 'item')\n        self.assertEqual(q.select_related, {'author': None, 'item': None})\n\n        q = Ranking.objects.select_related('author')\n        self.assertEqual(q.select_related, {'author': None, 'item': None})\n\n        q = Ranking.objects.select_related('author__book')\n        self.assertEqual(q.select_related, {'author': 'book'})\n\n        q = Ranking.objects.select_related('author__book', 'item')\n        self.assertEqual(q.select_related, {'author': 'book', 'item': None})\n", "    def test_deferred_loading(self):\n        \"\"\"\n        Test that deferred loading of fields works as expected.\n        \"\"\"\n        # Create some items with different authors\n        Item.objects.create(name='Item A', author=Author.objects.get(name='Author 1'))\n        Item.objects.create(name='Item B', author=Author.objects.get(name='Author 2'))\n        Item.objects.create(name='Item C', author=None)\n\n        # Perform a query that defers loading of the author field\n        items = Item.objects.defer('author').filter(name__startswith='I')\n\n        # Check that the author field is not loaded\n        self.assertEqual([f.attname for f in items[0]._meta.fields if f.name == 'author'], [])\n\n        # Check that we can access the name field\n        self.assertEqual([i.name for i in items], ['Item A', 'Item B'])\n\n        # Now perform a query that loads the author field\n        items = Item.objects.all()\n\n        # Check that the author field is loaded\n        self.assertEqual([f.attname for f in items[0]._meta.fields if f.name == 'author'], ['author'])\n\n        # Check that we can access both the name and author fields\n        for item in items:\n            self.assertIsNotNone(item.author)\n            self.assertIn(item.name, ('Item A', 'Item B', 'Item C'))\n\n\n\n"], "sample_242": ["    def test_year_lookup_bounds(self):\n        year_lookup_bounds = YearLookup().year_lookup_bounds(\n            self.connection, 2023\n        )\n        self.assertEqual(year_lookup_bounds, (\n            datetime(2023, 1, 1),\n            datetime(2023, 12, 31),\n        ))\n\n\n\n", "    def test_year_lookup_bounds(self):\n        # Test the year lookup bounds calculations in YearLookup subclasses.\n        class YearLookupSubclass(YearLookup):\n                return start, finish\n\n        connection = mock.Mock()\n        connection.features.has_native_uuid_field = False\n\n        lookup = YearLookupSubclass(\n            lhs=mock.Mock(), rhs=Value(2023)\n        )\n\n        self.assertEqual(lookup.year_lookup_bounds(connection, 2023), (20230101, 20231231))\n\n\n\n", "    def test_year_lookup_bounds(self):\n        field = DateTimeField()\n        connection = mock.Mock()\n        lookup = YearExact(field, Value(2023))\n\n        # Mock the connection's `year_lookup_bounds_for_datetime_field` method\n        connection.year_lookup_bounds_for_datetime_field.return_value = (\n            Value('2023-01-01'),\n            Value('2023-12-31'),\n        )\n\n        # Assert the bounds are correctly set\n        self.assertEqual(lookup.year_lookup_bounds(connection, 2023), (\n            Value('2023-01-01'),\n            Value('2023-12-31'),\n        ))\n\n\n", "    def test_year_lookup_bounds(self):\n        connection = mock.Mock()\n        connection.ops.year_lookup_bounds_for_datetime_field = mock.Mock(\n            return_value=(1980, 1989)\n        )\n        connection.ops.year_lookup_bounds_for_date_field = mock.Mock(\n            return_value=(1980, 1989)\n        )\n        lookup = YearExact(\n            lhs=Value(1985), rhs=Value(1985)\n        )\n        self.assertEqual(lookup.year_lookup_bounds(connection, 1985), (1980, 1989))\n        lookup = YearGt(lhs=Value(1985), rhs=Value(1985))\n        self.assertEqual(lookup.year_lookup_bounds(connection, 1985), (1986, 1989))\n        lookup = YearGte(lhs=Value(1985), rhs=Value(1985))\n        self.assertEqual(lookup.year_lookup_bounds(connection, 1985), (1980, 1989))\n        lookup = YearLt(lhs=Value(1985), rhs=Value(1985))\n        self.assertEqual(lookup.year_lookup_bounds(connection, 1985), (1980, 1985))\n        lookup = YearLte(lhs=Value(1985), rhs=Value(1985))\n        self.assertEqual(lookup.year_lookup_bounds(connection, 1985), (1980, 1989))\n\n\n\n", "    def test_year_lookup_bounds(self):\n        class YearLookupBase(YearLookup):\n                return (start, finish)\n\n        connection = mock.Mock()\n        connection.ops.year_lookup_bounds_for_datetime_field = mock.Mock(\n            return_value=(2000, 2001)\n        )\n        connection.ops.year_lookup_bounds_for_date_field = mock.Mock(\n            return_value=(2000, 2001)\n        )\n\n        lookup = YearLookupBase()\n\n        self.assertEqual(lookup.year_lookup_bounds(connection, 2000), (2000, 2001))\n\n\n"], "sample_57": ["    def test_username_field_is_normalized(self):\n        form = UserCreationForm()\n        data = {\n            'username': 'testclient\\u0301',\n        }\n        form.data = data\n        form.is_valid()\n\n        self.assertEqual(form.cleaned_data['username'], 'testclient')\n\n\n\n", "    def test_creation_form_validation(self):\n        form = UserCreationForm(\n            data={'username': 'newuser', 'password1': 'password', 'password2': 'password'}\n        )\n        self.assertTrue(form.is_valid())\n        user = form.save()\n        self.assertEqual(user.username, 'newuser')\n        self.assertEqual(user.password, 'password')\n", "    def test_creation_form_invalid_password(self):\n        form = UserCreationForm(data={'username': 'newuser', 'password1': '', 'password2': ''})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['password1'], [_(\n            'Your password must be at least %(min_length)s characters long.' % {\n                'min_length': password_validation.PASSWORD_MIN_LENGTH\n            }\n        )])\n        self.assertEqual(form.errors['password2'], [_(\n            'Your password must be at least %(min_length)s characters long.' % {\n                'min_length': password_validation.PASSWORD_MIN_LENGTH\n            }\n        )])\n\n        form = UserCreationForm(data={'username': 'newuser', 'password1': 'bad', 'password2': 'bad'})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['password1'], [_(\n            'Your password must contain at least one uppercase letter.'\n        )])\n        self.assertEqual(form.errors['password2'], [_(\n            'Your password must contain at least one uppercase letter.'\n        )])\n", "    def test_password_validation(self):\n        form = UserCreationForm(data={'username': 'newuser', 'password1': 'weak', 'password2': 'weak'})\n        self.assertFalse(form.is_valid())\n        self.assertIn('password1', form.errors)\n        self.assertIn('password2', form.errors)\n        self.assertEqual(form.errors['password1'], ['This password is too weak.'])\n        self.assertEqual(form.errors['password2'], ['This password is too weak.'])\n\n        # Password mismatch error\n        form = UserCreationForm(data={'username': 'newuser', 'password1': 'strong', 'password2': 'weak'})\n        self.assertFalse(form.is_valid())\n        self.assertIn('password2', form.errors)\n\n", "    def test_clean_password2_empty(self):\n        form = UserCreationForm(data={'username': 'test', 'password1': 'password', 'password2': ''})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['password2'], [self.error_messages['password_mismatch']])\n"], "sample_847": ["    def test_multitask_elastic_net_cv_raises(self):\n        reg = MultiTaskElasticNetCV()\n        with assert_raises(ValueError):\n            reg.fit([[0,0], [1, 1], [2, 2]], [[0, 0], [1, 1], [2, 2]])\n            \n        with assert_raises(ValueError):\n            reg.fit([[0,0], [1, 1], [2, 2]], [[0, 0], [1, 1], [2, 2]])\n            \n        with assert_raises(ValueError):\n            reg.fit([[0,0], [1, 1], [2, 2]], [[0, 0], [1, 1], [2, 2]])\n\n\n\n", "    def test_multitask_lasso_cv_convergence(self):\n        # Test convergence of MultiTaskLassoCV\n        from sklearn.datasets import make_regression\n        X, y = make_regression(n_targets=2, n_samples=100, random_state=42)\n        clf = MultiTaskLassoCV(cv=5, random_state=42)\n        with ignore_warnings(category=ConvergenceWarning):\n            clf.fit(X, y)\n        assert clf.n_iter_ is not None  # ensure that the solver converged\n\n\n\n        \n", "    def test_multi_task_lasso_cv_fit_intercept(self):\n        X, y = load_boston(return_X_y=True)\n        X = check_array(X, order='F')\n        y = check_array(y, dtype=np.float64)\n        n_tasks = y.shape[1]\n\n        # Test with fit_intercept=True\n        clf = MultiTaskLassoCV(cv=5, fit_intercept=True)\n        clf.fit(X, y)\n        assert clf.intercept_.shape == (n_tasks,)\n\n        # Test with fit_intercept=False\n        clf = MultiTaskLassoCV(cv=5, fit_intercept=False)\n        with assert_warns(UserWarning):\n            clf.fit(X, y)\n        assert clf.intercept_.shape == (n_tasks,)\n\n\n", "    def test_multi_task_lasso_cv_invalid_input(self):\n        with pytest.raises(ValueError):\n            MultiTaskLassoCV(cv=['invalid'])\n        with pytest.raises(ValueError):\n            MultiTaskLassoCV(cv=0)  \n", "    def test_multi_task_lasso_cv_normalize(self, normalize):\n        X, y = load_boston(return_X_y=True)\n        X = check_array(X)\n        y = check_array(y)\n        n_samples, n_features = X.shape\n        n_tasks = y.shape[1]\n\n        # Randomly shuffle the data to avoid deterministic behavior\n        np.random.seed(1)\n        shuffled_indices = np.random.permutation(n_samples)\n        X = X[shuffled_indices]\n        y = y[shuffled_indices]\n\n        # Create a MultiTaskLassoCV model with normalization\n        model = MultiTaskLassoCV(cv=5, normalize=normalize, random_state=0)\n\n        # Fit the model\n        model.fit(X, y)\n        \n        # Make sure coefficients are correctly set to 0 if there were\n        # no valid features after normalization.\n        for i in range(n_tasks):\n            if normalize:\n                # Check that some coefficients are non-zero\n                assert np.any(model.coef_[i] != 0)\n            else:\n                # Check that some coefficients are non-zero\n                assert np.any(model.coef_[i] != 0)\n\n    \n"], "sample_811": ["def test_paired_distances_with_sparse():\n    X = csr_matrix([[1, 2, 3], [4, 5, 6]])\n    Y = csr_matrix([[7, 8, 9], [10, 11, 12]])\n\n    distances = paired_distances(X, Y, metric=\"euclidean\")\n    assert isinstance(distances, csr_matrix)\n    assert_array_almost_equal(distances.toarray(),\n                              [[0.0, 0.0],\n                               [0.0, 0.0]])\n    \n    distances = paired_euclidean_distances(X, Y)\n    assert isinstance(distances, csr_matrix)\n    assert_array_almost_equal(distances.toarray(),\n                              [[0.0, 0.0],\n                               [0.0, 0.0]])\n\n    distances = paired_manhattan_distances(X, Y)\n    assert isinstance(distances, csr_matrix)\n    assert_array_almost_equal(distances.toarray(),\n                              [[0.0, 0.0],\n                               [0.0, 0.0]])\n", "    def test_paired_distances_with_different_data_types(self, metric):\n        X = np.array([[1, 2], [3, 4], [5, 6]], dtype=np.float32)\n        Y = np.array([[7, 8], [9, 10], [11, 12]], dtype=np.float64)\n\n        dist = paired_distances(X, Y, metric=metric)\n\n        assert isinstance(dist, np.ndarray)\n        assert dist.dtype == np.float64\n\n\n", "    def test_pairwise_distances_chunked_row_wise(self, metric):\n        X = np.random.rand(100, 5)\n        with config_context({'working_memory': 10}):\n            for chunk_size in range(1, 10):\n                chunk_generator = pairwise_distances_chunked(X,\n                                                            metric=metric,\n                                                            working_memory=10,\n                                                            n_jobs=1)\n                result = np.concatenate([x for x in chunk_generator], axis=0)\n                # Check if the result is consistent with pairwise_distances\n                expected = pairwise_distances(X, metric=metric)\n\n                assert_array_almost_equal(result, expected,\n                                          arr_kind='numpy')\n", "    def test_pairwise_distances_with_sparse_input(metric):\n        X = csr_matrix([[1, 2], [3, 4]])\n        Y = csr_matrix([[5, 6], [7, 8]])\n\n        result = pairwise_distances(X, Y, metric=metric)\n\n        # Ensure result is a NumPy array\n        assert isinstance(result, np.ndarray)\n\n        # Ensure result is dense array\n        assert not issparse(result)\n\n", "    def test_pairwise_distances_chunked_with_reduce_func(self):\n        X = np.random.rand(100, 10)\n        reduce_func = lambda D_chunk, start: [(D_chunk[i, j]\n                                             for i in range(start,\n                                                            start + 5)\n                                             for j in range(start,\n                                                            start + 5)]\n        gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n        for i in range(5):\n            chunk = next(gen)\n            assert isinstance(chunk, list)\n            assert len(chunk) == 25 # 5 x 5\n            assert all(isinstance(d, float) for d in chunk)\n\n\n\n"], "sample_396": ["compilation error", "    def test_complex_filters_with_join_promotion(self):\n        with self.assertNumQueries(3) as queries:\n            items = Item.objects.filter(\n                Q(\n                    created__gte=datetime.datetime(2007, 12, 19, 21, 0, 0),\n                    tags__name__in=[\"t1\", \"t4\"],\n                )\n                | Q(\n                    creator=Author.objects.filter(extra__note__id=2)\n                )\n            )\n            items = items.order_by('id')\n            for item in items:\n                pass  # We only care about the number of queries\n        \n        # Verify that the queries executed were generated in the expected manner\n        expected_queries = [\n            \"SELECT 'django_item'\\\"id\\\", 'django_item'\\\"name\\\", 'django_item'\\\"created\\\", 'django_item'\\\"modified\\\", 'django_item'\\\"creator_id\\\", 'django_item'\\\"note_id', 'django_item'\\\"tags_id\\\" FROM 'django_item' WHERE ('django_item'\\\"created__gte' = %s AND 'django_item'\\\"tags__name__in' = (%s)) OR ('django_item'\\\"creator_id' = %s)\",\n            \"SELECT 'django_auth_user'\\\"id\\\", 'django_auth_user'\\\"username', 'django_auth_user'\\\"password', 'django_auth_user'\\\"is_superuser', 'django_auth_user'\\\"is_staff', 'django_auth_user'\\\"is_active', 'django_auth_user'\\\"last_login', 'django_auth_user'\\\"date_joined' FROM 'django_auth_user' WHERE 'django_auth_user'\\\"id' = %s\",\n            \"SELECT 'django_item'\\\"id\\\", 'django_item'\\\"name\\\", 'django_item'\\\"created\\\", 'django_item'\\\"modified\\\", 'django_item'\\\"creator_id\\\", 'django_item'\\\"note_id\\\", 'django_item'\\\"tags_id\\\" FROM 'django_item' WHERE ('django_item'\\\"id' IN (%s))\",\n        ]\n\n        self.assertEqual(", "compilation error", "    def test_complex_filter_with_extra_info(self):\n        with capture_queries() as queries:\n            items = Item.objects.filter(\n                note__note__startswith=\"n\",\n                extra_info__value__gte=41,\n                extra_info__note__note__startswith=\"n2\",\n                creator__extra_info__note__note__contains=\"bar\",\n            )\n            self.assertEqual(len(queries), 1)\n            self.assertEquals(items.count(), 1)\n            item = items[0]\n            self.assertEqual(item.name, \"two\")\n", "compilation error"], "sample_675": ["    def test_log_level_setting_in_ini(self, testdir):\n        \"\"\"\n        Test that the log level can be set using the ini file and that it takes precedence over command line options.\n        \"\"\"\n        p = testdir.makepyfile(\n            \"\"\"\n            import logging\n            logging.basicConfig(level=logging.DEBUG)\n                logging.info(\"Hello!\")\n            \"\"\"\n        )\n\n        repr = testdir.inline_run(\n            \"-v\", \"--log-level=WARNING\", p\n        )\n\n        assert repr.getstatus() == 0\n        assert \"Hello!\" not in repr.stdout\n\n\n        testdir.writefile(\n            \"test_log_level.ini\",\n            \"\"\"\n            [pytest]\n            log_level = INFO\n            \"\"\",\n        )\n\n        repr = testdir.inline_run(\n            \"-v\", p\n        )\n\n        assert repr.getstatus() == 0\n        assert \"Hello!\" in repr.stdout\n", "    def test_set_log_path(self, testdir):\n        p = testdir.makepyfile(\n            \"test_logging.py\",\n            \"\"\"\n            import logging\n\n                logging.info('test')\n            \"\"\",\n        )\n        testdir.runpytest('-v', '--log-file', 'test.log')\n        assert os.path.exists('test.log')\n        with testdir.tmpdir.as_cwd():\n            p.runpytest('-v', '--log-file', testdir.tmpdir.join('new_test.log'))\n        assert os.path.exists('new_test.log')\n        assert not os.path.exists('test.log')\n\n\n\n", "    def test_log_path_setting(self, testdir):\n        \"\"\"Test that log_path is properly set.\"\"\"\n        p = testdir.makepyfile(\n            \"\"\"\n            import pytest\n            @pytest.fixture\n                return \"my_log.txt\"\n                print(\"This is a test!\")\n                assert \"This is a test!\" in caplog.text\n\n            \"\"\"\n        )\n        result = p.runpytest(\n            \"--log-file=None\", \"--log-path={}\".format(os.path.join(testdir.tmpdir, \"my_log.txt\"))\n        )\n        assert result.ret == 0\n        assert os.path.exists(os.path.join(testdir.tmpdir, \"my_log.txt\"))\n        with open(os.path.join(testdir.tmpdir, \"my_log.txt\")) as f:\n            content = f.read()\n        assert \"This is a test!\" in content\n\n\n\n", "    def test_config_parsing(self, request):\n        testdir = Testdir(self.get_test_data_path())\n        testdir.makepyfile(\n            \"\"\"", "    def test_pytest_capturemanager(self, testdir):\n        p = testdir.makepyfile(\n            \"\"\"\n            import logging\n            logging.basicConfig(level=logging.INFO)\n                logging.info(\"This is a test message.\")\n            \"\"\"\n        )\n        result = testdir.runpytest(\"--disable-pytest-capturemanager\")\n        result.stdout.fnmatch_lines(\n            [ \"*Captured*\"]\n        )\n"], "sample_866": ["    def test_affinity_propagation_equal_similarities_and_preferences(self):\n        # Test the _equal_similarities_and_preferences function for all cases\n        # where similarities and preferences are equal\n        S = np.ones((3, 3))\n        preference = S.mean()\n        assert _equal_similarities_and_preferences(S, preference)\n        \n        S = np.array([[1, 2, 3], [2, 1, 4], [3, 4, 1]])\n        preference = S.min()\n        assert _equal_similarities_and_preferences(S, preference)\n        \n        S = np.array([[1, 2], [2, 1]])\n        preference = S.mean()\n        assert _equal_similarities_and_preferences(S, preference)\n\n        preference = np.ones(S.shape[0]) * 10\n        assert not _equal_similarities_and_preferences(S, preference)\n", "    def test_affinity_propagation_with_preference(self, affinity):\n        n_samples = 5\n        if affinity == 'euclidean':\n            preference = np.array([0.2])\n        else:\n            # For precomputed affinity, preference should be None\n            preference = None\n\n        S = np.array([[1, 2, 3], [2, 1, 4], [3, 4, 1]])\n        ap = AffinityPropagation(\n            preference=preference, affinity=affinity,\n            max_iter=10\n        )\n        if affinity == 'precomputed':\n            ap.fit(S)\n        else:\n            ap.fit(np.ones((n_samples, n_samples)) )\n        labels = ap.labels_\n\n        if affinity == 'euclidean':\n            assert_array_equal(labels, np.array([0, 0, 0]))\n        else:\n            assert_array_equal(labels, np.array([0, 1, 2]))\n    ", "    def test_affinity_propagation_all_equal_similarities_preferences(self):\n        S = np.ones((60, 60))\n        preference = np.ones(60)\n\n        # Test for all equal similarities and preferences\n        assert _equal_similarities_and_preferences(S, preference) == True\n\n        # Test with preference smaller than similarities\n        preference = np.ones(60) * 0.1\n        assert _equal_similarities_and_preferences(S, preference) == False\n", "    def test_preference_small(self):\n        affinity = affinity_propagation(\n            np.zeros((6, 6)) + 0.5,\n            preference=0.1)\n        assert affinity[1][0] == 0\n\n", "    def test_affinity_propagation_equal_similarities_and_preferences(self, affinity):\n        np.random.seed(0)\n        S = np.random.rand(6, 6)\n        preference = np.median(S)\n        if affinity == 'precomputed':\n            S_ = S.copy()\n        else:\n            S_ = -euclidean_distances(X, squared=True)\n        assert _equal_similarities_and_preferences(S, preference)\n\n\n"], "sample_233": ["    def test_token_expiration(self):\n        now = datetime.now()\n        generator = MockedPasswordResetTokenGenerator(now)\n        user = User.objects.create_user(\n            username='testuser', password='password', email='test@example.com'\n        )\n\n        token = generator.make_token(user)\n        self.assertTrue(generator.check_token(user, token))\n\n        # Simulate token being generated some time ago\n        past_time = now - timedelta(seconds=settings.PASSWORD_RESET_TIMEOUT + 1)\n        generator2 = MockedPasswordResetTokenGenerator(past_time)\n        with self.assertRaises(AssertionError):\n            generator2.check_token(user, token)  \n\n", "    def test_token_expiration(self):\n        user = User(username='testuser', email='test@example.com')\n        user.save()\n        generator = MockedPasswordResetTokenGenerator(datetime.now() - timedelta(seconds=settings.PASSWORD_RESET_TIMEOUT * 2))\n        token = generator.make_token(user)\n\n        self.assertFalse(generator.check_token(user, token))\n\n        generator.reset_token_expiration()\n        self.assertTrue(generator.check_token(user, token))\n", "    def test_legacy_token_compatibility(self):\n        now = datetime.now()\n        user = User(\n            username='testuser',\n            email='testuser@example.com',\n        )\n        user.save()\n        generator = MockedPasswordResetTokenGenerator(now)\n        token = generator.make_token(user)\n        self.assertTrue(generator.check_token(user, token))\n", "    def test_token_generator_with_custom_timestamp(self):\n        user = User(username='testuser')\n        user.save()\n\n        # Create a test generator that returns a specific timestamp\n        test_timestamp = datetime(2024, 1, 1, 10, 0, 0)\n        generator = MockedPasswordResetTokenGenerator(test_timestamp)\n\n        # Generate a token with the custom timestamp\n        token = generator.make_token(user)\n\n        # Check that the token was generated correctly\n        self.assertTrue(generator.check_token(user, token))\n\n\n", "    def test_token_generation_with_legacy_hashing_algorithm(self):\n        now = datetime.now()\n        user = User(username='testuser', email='test@example.com', password='password')\n        user.save()\n\n        generator = MockedPasswordResetTokenGenerator(now)\n        token = generator.make_token(user)\n\n        self.assertTrue(generator.check_token(user, token))\n"], "sample_1159": ["    def test_failing_assumptions_none():\n        x = Symbol('x')\n        with raises(InconsistentAssumptions):\n            failing_assumptions(x, real=True, positive=True)\n\n\n\n", "    def test_common_assumptions_complex_numbers():\n        a =  I\n        b = 2*I\n        c = a + b\n        expected = {'commutative': True, 'complex': True, 'imaginary': True}\n        assert common_assumptions([a, b, c]) == expected\n    ", "    def test_failing_assumptions_multiple_assumptions(self):\n        x = Symbol('x', real=True)\n        y = Symbol('y', Integer, positive=True)\n        with raises(InconsistentAssumptions):\n            failing_assumptions(x + y, real=True, integer=True, positive=True)\n", "    def test_failing_assumptions_empty():\n        x = Symbol('x')\n        assert failing_assumptions(x, real=True, integer=True) == {}\n    ", "    def test_failing_assumptions_multiple():\n        x = Symbol('x')\n        y = Symbol('y', real=True)\n        z = Symbol('z', positive=True)\n        result = failing_assumptions(x + y + z, real=True, positive=True)\n        assert result == {'positive': None}\n    "], "sample_53": ["    def test_autocomplete_select_required(self):\n        response = self.client.post('/admin/widgets/album/add/', data={\n            'band': '',\n        })\n        self.assertContains(response, 'This field is required.', status_code=200)\n\n\n", "    def test_autocomplete_select_required(self):\n        form = RequiredBandForm(data={'band': ''})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['band'], ['This field is required.'])\n\n        form = RequiredBandForm(data={'band': 1})\n        self.assertTrue(form.is_valid())\n", "    def test_autocomplete_widget_required(self):\n        form = RequiredBandForm()\n        rendered = form.as_p()\n        self.assertIn('<select', rendered)\n        self.assertIn('required', rendered)\n        self.assertNotIn(self.empty_option, rendered)\n", "    def test_autocomplete_widget_required(self):\n        form = RequiredBandForm(\n            data={'band': \"\"},\n        )\n        self.assertFalse(form.is_valid())\n        self.assertIn(\n            \"This field is required.\",\n            form.errors['band'],\n        )\n", "    def test_autocomplete_select_with_empty_option(self):\n        with translation.override('en'):\n            form = AlbumForm(\n                data={'band': ''}\n            )\n            html = form.as_p()\n            self.assertIn(\n                self.empty_option,\n                html,\n                \"Autocomplete widget doesn't render empty option.\",\n            )\n            self.assertIn(\n                'value=\"\"',\n                html,\n                \"Autocomplete widget doesn't render empty option with value attribute.\",\n            )\n"], "sample_687": ["    def test_log_indentation(self, testdir: Testdir):\n            with caplog.at_level(logging.INFO):\n                sublogger.info(\"This should be indented\")\n                sublogger.info(\"This is another indented line\")\n\n        testdir.makeconftest(\n            \"\"\"", "    def test_log_level_cli(self, caplog):\n        # Capture logs from the root logger\n        with caplog.at_level(logging.WARNING):\n            logger.info(\"This should not be logged\")\n            sublogger.error(\"This should be logged\")\n\n        assert \"This should be logged\" in caplog.text\n        assert \"This should not be logged\" not in caplog.text\n\n\n\n", "    def test_file_handler_with_custom_path(self, testdir: Testdir) -> None:\n        \"\"\"Test that a custom log file path can be set and used.\"\"\"\n        testdir.makepyfile(\n            \"\"\"\n            import logging\n\n            logger = logging.getLogger(__name__)\n            logger.info(\"This should be written to the custom file.\")\n            \"\"\"\n        )\n\n        reprec = testdir.inline_run(\n            \"-s\",\n            \"--log-file=/tmp/test_log.txt\",\n            \"--log-file-level=DEBUG\",\n        )\n\n        reprec.assert_outcomes(passed=1)\n\n        # Check that the log file was written to.\n        with open(\"/tmp/test_log.txt\", \"r\") as f:\n            content = f.read()\n        assert \"This should be written to the custom file.\" in content\n        os.remove(\"/tmp/test_log.txt\")\n\n\n\n", "    def test_multi_loggers(self, testdir: Testdir, caplog):\n        testdir.makepyfile(\n            \"\"\"\n            import logging\n\n                logging.info(\"This is a message from the main logger.\")\n                logging.debug(\"This is a debug message from the main logger.\")\n\n                logger_baz.warning(\"This is a warning from sublogger.\")\n\n            logger = logging.getLogger(__name__)\n            logger_baz = logging.getLogger(\"test.baz\")\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.assert_outcomes(passed=1)\n        captured_logs = caplog.text\n        assert \"This is a message from the main logger.\" in captured_logs\n        assert \"This is a debug message from the main logger.\" in captured_logs\n        assert \"This is a warning from sublogger.\" in captured_logs\n", "def test_log_cli_disabled(testdir: Testdir) -> None:\n    \"\"\"Test that live logging is disabled when the '--log-cli' option is not set.\"\"\"\n    # Write a test function that uses caplog to capture logs.\n    # Configure the test directory to disable live logging.\n    testdir.makepyfile(\n        \"\"\"\n            logger.info(\"This is an info message\")\n            sublogger.warning(\"This is a warning message\")\n        \"\"\"\n    )\n\n    result = testdir.runpytest()\n    assert result.ret == 0\n\n    # Check that the log messages were captured by caplog and not printed to the console.\n    lines = result.stdout.readlines()\n    assert all(line.startswith(\"=====\") for line in lines)\n    assert len(lines) == len(result.stderr.readlines()) + 1\n"], "sample_883": ["    def test_ard_ridge_with_threshold_lambda(self):\n        rng = check_random_state(0)\n        X = rng.randn(100, 5)\n        y = rng.randn(100)\n\n        # Fit ARD regression with a small threshold_lambda\n        ard_ridge = ARDRegression(threshold_lambda=1e-3)\n        ard_ridge.fit(X, y)\n\n        # Check that many weights have been pruned\n        assert (ard_ridge.lambda_ < ard_ridge.threshold_lambda).sum() > 3\n\n        # Fit regular ridge regression\n        ridge = Ridge()\n        ridge.fit(X, y)\n\n        # Check that all weights are retained in ridge regression\n        assert (ridge.coef_ != 0).all()\n\n\n", "    def test_ard_regression_fit_convergence(\n        self, max_iter, tol, random_state", "    def test_ard_scores(self, compute_score):\n        n_samples = 20\n        X, y = np.random.rand(n_samples, 10), np.random.rand(n_samples)\n        clf = ARDRegression(compute_score=compute_score)\n        clf.fit(X, y)\n        \n        if compute_score:\n            assert len(clf.scores_) > 0\n", "    def test_ard_regression_sparse_data(self):\n        # Test with sparse data\n\n        n_samples = 100\n        n_features = 20\n\n        X = np.random.rand(n_samples, n_features)\n\n        X_sparse = csr_matrix(X)\n        y = np.random.rand(n_samples)\n\n        reg = ARDRegression(copy_X=False)\n        reg.fit(X_sparse, y)\n\n\n        # Check that the attributes are as expected\n        assert isinstance(reg.coef_, np.ndarray)\n        assert reg.alpha_ is not None\n        assert reg.lambda_ is not None\n        assert isinstance(reg.sigma_, np.ndarray)\n\n        y_pred = reg.predict(X_sparse)\n        assert isinstance(y_pred, np.ndarray)\n\n        reg = ARDRegression(copy_X=True)\n        reg.fit(X_sparse, y)\n\n        # Check that the attributes are as expected after copying X\n        assert isinstance(reg.coef_, np.ndarray)\n        assert reg.alpha_ is not None\n        assert reg.lambda_ is not None\n        assert isinstance(reg.sigma_, np.ndarray)\n        assert not np.array_equal(X_sparse.data, reg.X_scale_)\n\n        y_pred = reg.predict(X_sparse)\n        assert isinstance(y_pred, np.ndarray)\n", "    def test_ard_predict_with_std(self):\n        n_samples = 30\n        n_features = 5\n        rng = check_random_state(42)\n        X = rng.randn(n_samples, n_features)\n        y = 2 * X[:, 0] + 3 * X[:, 1] + rng.randn(n_samples)\n\n        clf = ARDRegression(alpha_1=1, alpha_2=1, lambda_1=1,\n                            lambda_2=1, compute_score=True)\n        clf.fit(X, y)\n\n        y_mean, y_std = clf.predict(X, return_std=True)\n        \n        # Check that the prediction variance is non-negative\n        assert_array_less(y_std, 1e-6)\n"], "sample_326": ["    def test_urlize_with_email(self):\n        self.check_output(urlize, \"This is my email: example@domain.com\")\n        self.check_output(urlize, \"This is my email: example@domain..com\")\n        self.check_output(urlize, \"This is my email: example@domain.com and another one: example@domain2.co\")\n        self.check_output(urlize, \"This is my email: example@example..com\")\n", "    def test_urlize_with_leading_punctuation(self):\n        self.check_output(urlize, '(https://www.example.com)',\n                          '<a href=\"https://www.example.com\" rel=\"nofollow\"> (https://www.example.com)</a>')\n", "    def test_urlize_with_non_ascii_characters(self):\n        text = \"https://www.example.com/\u4f60\u597d\u4e16\u754c\"\n        expected = \"<a href=\\\"https://www.example.com/\u4f60\u597d\u4e16\u754c\\\">https://www.example.com/\u4f60\u597d\u4e16\u754c</a>\"\n        self.assertEqual(urlize(text), expected)\n\n\n", "    def test_urlize_trim_url(self):\n        input_text = 'this is a long URL: https://www.example.com/this' \\\n                     'is-a-really-long-url-that-should-be-trimmed'\n        output_text = 'this is a long URL: https://www.example.com/this\u2026'\n        self.assertEqual(urlize(input_text, trim_url_limit=20), output_text)\n", "    def test_json_script(self):\n        self.check_output(json_script, {'a': 1, 'b': '2'},\n                         '<script id=\"test\" type=\"application/json\">{\"a\":1,\"b\":\"2\"}</script>')\n        self.check_output(json_script, [1, 2, 3],\n                         '<script id=\"test\" type=\"application/json\">[1,2,3]</script>')\n        self.check_output(json_script, {\"&\": \"and\"},\n                         '<script id=\"test\" type=\"application/json\">{\"\\&\": \"and\"}</script>')\n\n\n\n"], "sample_556": ["    def test_constrained_layout_with_figure_size():\n        fig = Figure(figsize=(5, 3))\n        ax = fig.add_subplot(111)\n        ax.text(0.5, 0.5, \"Test\", ha=\"center\", va=\"center\")\n        fig.set_layout_engine(ConstrainedLayoutEngine(pad=1))\n        fig.tight_layout()\n        fig.canvas.draw()\n        with image_comparison(\n                'constrained_layout_with_figure_size',\n                extensions=['png', 'svg'],\n                tol=0.01,\n                remove_text=True):\n            fig.savefig(\"constrained_layout_with_figure_size.png\")\n", "    def test_constrained_layout_execute(self):\n        fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n        ax[0].set_title('Constrained Layout')\n        ax[1].set_title('Unconstrained Layout')\n        plt.tight_layout()\n        fig.canvas.draw()\n        plt.close(fig)\n", "def test_tight_layout():\n    fig, axes = plt.subplots(2, 2, figsize=(3, 3))\n    \n    axes[0][0].set_title('A')\n    axes[0][1].set_title('B')\n    axes[1][0].set_title('C')\n    axes[1][1].set_title('D')\n\n    fig.tight_layout()\n    plt.close(fig)\n", "    def test_figaspect_array(self):\n        A = np.random.rand(3, 4)\n        w, h = figaspect(A)\n        assert w / h == 3 / 4\n        A = np.random.rand(4, 3)\n        w, h = figaspect(A)\n        assert w / h == 4 / 3\n", "    def test_fig_get_size_inches(self):\n        fig = Figure(figsize=(5, 3))\n        assert fig.get_size_inches() == (5, 3)\n\n"], "sample_1004": ["    def test_dummy_symbol_substitution(self):\n        cs1 = ConditionSet(x, Eq(x, 1), S.Reals)\n        cs2 = ConditionSet(L, Eq(L, 1), S.Reals)\n        assert cs1.dummy_eq(cs2)\n\n        cs3 = ConditionSet(x, Eq(x, 1), S.Reals)\n        cs4 = ConditionSet(z, Eq(z, 1), S.Reals)\n        assert not cs3.dummy_eq(cs4)\n", "    def test_subs_with_dummy_symbol(self):\n        cs = ConditionSet(x, x < 1, {x, z})\n        new_cs = cs.subs(x, y)\n        assert new_cs.condition == And(Lambda(y, y < 1), z < 1)\n        assert new_cs.base_set == {y, z}\n        assert not new_cs.sym.is_Dummy\n        new_cs = new_cs.subs(y, z)\n        assert new_cs.condition == And(Lambda(z, z < 1), z < 1)\n        assert new_cs.base_set == {z}\n        assert not new_cs.sym.is_Dummy\n    ", "    def test_conditionset_base_set_change(self):\n        cs = ConditionSet(x, Eq(x, 2), Interval(0, 3))\n        assert cs == ConditionSet(x, Eq(x, 2), Interval(0, 3))\n        cs = ConditionSet(x, Eq(x, 2), S.Reals)\n        assert cs == ConditionSet(x, Eq(x, 2), S.Reals)\n        cs = ConditionSet(x, Eq(x, 2), Set(1, 2, 3))\n        assert cs == ConditionSet(x, Eq(x, 2), Set(1, 2, 3))\n        cs = ConditionSet(x, Eq(x, 2), ConditionSet(x, x > 1, S.Integers))\n        assert cs == ConditionSet(x, Eq(x, 2), ConditionSet(x, x > 1, S.Integers))\n\n\n\n", "    def test_dummy_symbol_subs(self):\n        c = ConditionSet(x, x < 1, {x, z})\n        c_subs = c.subs(x, L)\n        assert c_subs.sym == L\n        assert c_subs.condition == And(L < 1, x < z)\n        assert c_subs.base_set == {L, z}\n        c_subs2 = c_subs.subs(L, x)\n        assert c_subs2.sym == x\n        assert c_subs2.condition == And(x < 1, x < z)\n        assert c_subs2.base_set == {x, z} \n", "    def test_dummy_eq_different_symbols(self):\n        c1 = ConditionSet(x, Eq(x, 1), S.Reals)\n        c2 = ConditionSet(y, Eq(y, 1), S.Reals)\n        assert not c1.dummy_eq(c2)\n"], "sample_636": ["compilation error", "compilation error", "    def test_ignore_imports(self) -> None:\n        with _patch_streams(StringIO()) as out:\n            with open(join(DATA, \"similar_code_import.py\"), \"r\", encoding=\"utf-8\") as f:\n                source = f.read()\n            Run([\"pylint\", \"--disable=R0801\", \"--disable=C0103\", \"--disable=R0903\", \"--debug\", CLEAN_PATH + \"test_similar.py\"],\n                show_source=source,\n                files={CLEAN_PATH + \"test_similar.py\": source})\n            captured = out.getvalue()\n            assert \"duplicate code detected in\" not in captured\n", "    def test_ignore_comments_in_duplicate_code(self) -> None:\n        with _patch_streams(StringIO()) as out:\n            with open(join(DATA, \"duplicate_comments.py\"), \"r\") as f:\n                script = f.read()\n            Run([\"-f\", \"-d\", \"1\", \"-i\", \"--ignore-comments\", \"-\"],\n                files=[script],\n                linter_class=SimilarChecker)\n\n\n\n", "    def test_report_similarities_with_old_stats(\n        self, capsys, monkeypatch"], "sample_73": ["    def test_post_process_empty_files(self):\n        \"\"\"\n        Test that empty files are not incorrectly hashed.\n        \"\"\"\n        with tempfile.TemporaryDirectory() as tmpdir:\n            self.create_file(tmpdir, \"empty.txt\", \"\")\n            self.create_file(\n                tmpdir,\n                \"my_app/empty.css\",\n                \"\",\n            )\n            self.create_file(\n                tmpdir,\n                \"another_app/empty_javascript.js\",\n                \"\",\n            )\n\n            call_command(\n                'collectstatic',\n                '--noinput',\n                '--clear',\n                '--target-dir',\n                os.path.join(TEST_ROOT, 'static'),\n            )\n\n            self.assertFileExists(\n                os.path.join(TEST_ROOT, 'static', 'empty.txt'),\n            )\n            self.assertFileExists(\n                os.path.join(TEST_ROOT, 'static', 'my_app/empty.css'),\n            )\n            self.assertFileExists(\n                os.path.join(TEST_ROOT, 'static', 'another_app/empty_javascript.js'),\n            )\n\n            # Verify the files remain empty in the target directory\n            with open(\n                os.path.join(TEST_ROOT, 'static', 'empty.txt'),\n                'r',\n            ) as f:\n                self.assertEqual(f.read(), \"\")\n            with open(\n                os.path.join(TEST_ROOT, 'static', 'my_app/empty.css'),\n                'r',\n            ) as f:\n                self.assertEqual(f.read(), \"\")\n            with open(\n                os.path.join(TEST_ROOT, 'static', 'another_app/empty_javascript.js'),\n                'r',\n            ) as f:\n                self.assertEqual(f.read(), \"\")\n\n\n", "    def test_manifest_storage_invalid_manifest(self):\n        with self.subTest(manifest_version='wrong_version'):\n            with override_settings(STATICFILES_STORAGE='django.contrib.staticfiles.storage.ManifestStaticFilesStorage'):\n                from django.contrib.staticfiles.storage import ManifestStaticFilesStorage\n                manifest_storage = ManifestStaticFilesStorage()\n                manifest_storage.manifest_version = 'wrong_version'\n                with self.assertRaises(ValueError):\n                    manifest_storage.load_manifest()\n\n        with self.subTest(manifest_strict=True):\n            with override_settings(STATICFILES_STORAGE='django.contrib.staticfiles.storage.ManifestStaticFilesStorage'):\n                from django.contrib.staticfiles.storage import ManifestStaticFilesStorage\n                manifest_storage = ManifestStaticFilesStorage()\n                manifest_storage.manifest_strict = True\n                # Test for missing entry in manifest\n                self.addCleanup(shutil.rmtree, TEST_ROOT)\n                os.makedirs(TEST_ROOT)\n                with open(os.path.join(TEST_ROOT, 'static/my_file.css'), 'w') as f:\n                    f.write('/* test content */')\n                call_command('collectstatic', interactive=False, skip_checks=True,\n                              verbosity=0, dry_run=False)\n                with self.assertRaises(ValueError):\n                    manifest_storage.stored_name('static/my_file.css')\n\n\n\n", "    def test_manifest_strict(self):\n        self.override_settings(STATICFILES_STORAGE='django.contrib.staticfiles.storage.ManifestStaticFilesStorage')\n        self.create_template('a_file.css', 'body { background-color: red; }')\n        self.create_file('static/a_file.css', 'body { background-color: blue; }')\n        call_command('collectstatic', verbosity=0, ignore_invalid_permissions=True)\n        with self.assertRaises(ValueError):\n            with open(self.hashed_file_path('a_file.css'), 'r') as f:\n                f.read()\n\n", "    def test_manifest_strict(self):\n        self.assertEqual(storage.staticfiles_storage.manifest_strict, True)\n        self.assertRaises(ValueError, storage.staticfiles_storage.stored_name, 'test_file.js')\n\n", "    def test_manifest_files_storage_empty_cache(self):\n        with self.assertNumWarnings(1, RemovedInDjango31Warning):\n            call_command('collectstatic', clear=True, interactive=False)\n        with self.assertRaises(ValueError):\n            self.storage.url('static/css/style.css')\n\n\n\n"], "sample_56": ["    def test_admin_checks_model_admin(self):\n        admin.site = AdminSite(name='mysite')\n        admin.site._registry = {}\n        admin.site.register(Song, ValidFields)\n        admin.site.register(Book, MyAdmin)\n        errors = checks.run_checks(admin.site)\n        self.assertEqual(errors, [])\n", "    def test_admin_check_view_on_site(self):\n        class SiteAdmin(admin.ModelAdmin):\n            view_on_site = True\n\n        site = AdminSite()\n        site.register(Album, SiteAdmin)\n\n        errors = checks.run_checks([site], 'test_admin_checks')\n        self.assertEqual(len(errors), 0)\n\n        class BadSiteAdmin(admin.ModelAdmin):\n            view_on_site = lambda: None\n\n        site = AdminSite()\n        site.register(Album, BadSiteAdmin)\n\n        errors = checks.run_checks([site], 'test_admin_checks')\n        self.assertEqual(len(errors), 1)\n\n", "    def test_check_custom_admin_site(self):\n        class MyAdminSite(AdminSite):\n            pass\n\n        site = MyAdminSite()\n        site.register(Song, MyAdmin)\n\n        with self.subTest('check for custom admin site'):\n            errors = checks.run_checks(site)\n            self.assertEqual(len(errors), 0)\n\n\n\n", "    def test_inline_checks(self):\n\n        class InlineAdmin(GenericStackedInline):\n            model = Book\n            fk_name = 'album'\n\n        class AlbumAdmin(admin.ModelAdmin):\n            inlines = [InlineAdmin]\n\n        # Check system checks for inline model admin\n\n        site = AdminSite()\n        site.register(Album, AlbumAdmin)\n\n        errors = checks.run_checks(site)\n        self.assertEqual(len(errors), 0, \"Invalid errors in inline check\")\n", "    def test_custom_admin_site(self):\n        class CustomAdminSite(AdminSite):\n                return ['custom site error']\n\n        site = CustomAdminSite()\n        admin_checks = checks.run_checks([site])\n        self.assertEqual(len(admin_checks), 1)\n        self.assertEqual(admin_checks[0].msg, 'custom site error')\n\n\n\n"], "sample_415": ["     def test_unique_constraint_with_expressions(self):\n         with atomic():\n             Product.objects.create(name='Product A', description='Description A')\n             with self.assertRaises(IntegrityError):\n                 Product.objects.create(name=F('name'), description='Description B')\n", "    def test_unique_constraint_with_deferrable(self):\n        with atomic():\n            Constraint = UniqueConstraintDeferrable()\n            Constraint.create_model(self.connection)\n            with self.assertRaises(IntegrityError):\n                instance = Constraint.model.objects.create(\n                    field_1=\"value1\", field_2=\"value2\"\n                )\n                Constraint.model.objects.create(\n                    field_1=\"value1\", field_2=\"value2\"\n                )\n            Constraint.deconstruct_and_delete_model()\n\n\n\n", "        def test_unique_constraint_condition(self):\n            with atomic():\n                Product.objects.create(name='Test Product')\n                with self.assertRaises(ValidationError):\n                    UniqueConstraintConditionProduct.objects.create(name='Test Product')\n\n", "    def test_unique_constraint_validation_with_fields_and_expressions(self):\n        with atomic():\n            Product.objects.create(name=\"Product 1\", active=True)\n            with self.assertRaises(ValidationError):\n                Product.objects.create(name=\"Product 1\", active=True)\n", "    def test_unique_constraint_validation(self):\n        with atomic():\n            product = Product.objects.create(name=\"Product 1\")\n            with self.assertRaises(ValidationError):\n                Product.objects.create(name=\"Product 1\")\n            with self.assertRaises(ValidationError):\n                UniqueConstraintProduct.objects.create(name=\"Product 1\")\n            with self.assertRaises(ValidationError):\n                Product.objects.create(name=\"Product 1\", price=10)\n            Product.objects.create(name=\"Product 2\")\n\n\n"], "sample_619": ["    def test_cftime_datetime_encoding(\n        num_dates, units, calendar", "    def test_cftime_datetime_encoding_decoding(\n        self, num_dates, units, calendar", "    def test_encode_decode_cfdatetime(num_dates, units, calendar):\n        dates = np.array(num_dates)\n        data = DataArray(dates, dims=\"time\", coords={\"time\": dates}, encoding={\"units\": units, \"calendar\": calendar})\n\n        if calendar in _NON_STANDARD_CALENDARS:\n            xarray_data = data.copy()\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\", SerializationWarning)\n                xarray_data = xarray_data.set_cf_calendar(calendar)\n\n\n        with assert_no_warnings():\n            xarray_data = xarray_data.chunk(1, \"time\")\n            xarray_data_reloaded = cf_encoder(data.chunk(1, \"time\"), {\"units\": units, \"calendar\": calendar}).decode(\n                data\n            )\n\n        assert_equal(xarray_data, xarray_data_reloaded)\n", "    def test_encode_decode_cftime_datetime(\n        num_dates, units, calendar", "    def test_encode_decode_cf_datetime(\n        num_dates, units, calendar,"], "sample_691": ["    def test_faulthandler_timeout_not_triggered(self, pytester):\n        code = \"\"\"\n        import time\n        time.sleep(0.1)\n        \"\"\"\n        result = pytester.runpytest(code)\n        result.stdout.fnmatch_lines([\"*passed*\"])\n\n", "def test_faulthandler_timeout(pytester: Pytester):\n    \"\"\"Test that the fault handler timeout option correctly dumps a traceback to stderr.\"\"\"\n    # Define a slow function that will trigger the timeout\n        import time\n        time.sleep(1)\n    \n    # Patch sys.stderr to capture output\n    captured_stderr = io.StringIO()\n    sys.stderr = captured_stderr\n\n    #  Run the test with the timeout option set\n    result = pytester.runpytest(\n        \"-m\", \"faulthandler_timeout=0.5\", \"--capture=no\", \"test_faulthandler_timeout.py\"\n    )\n    \n    # Assert that the test failed and the traceback is in the captured stderr\n    assert result.retcode == 1\n    assert \"Traceback (most recent call last)\" in captured_stderr.getvalue()\n", "        def test_slow_function():\n            time.sleep(1)", "    def test_faulthandler_timeout_warning(\n        self: Pytester, timeout: float, expected_warning: bool", "    def test_faulthandler_timeout_works(self, pytester):\n        test_file = \"\"\"\n        import time\n        import faulthandler\n\n            time.sleep(2)\n        \"\"\"\n        pytester.write(test_file)\n        result = pytester.runpytest()\n\n        result.assert_outcomes(passed=1)\n        result.stdout.fnmatch_lines([\"*test_something* PASSED\"])\n        assert \"Traceback\" in result.stderr.getvalue()\n"], "sample_740": ["    def test_check_array_sparse_object(self, accept_sparse, dtype):\n        # Test sparse with dtype object\n        X = sp.csr_matrix([[1, 2], [3, 4]])\n        X_converted = check_array(X, accept_sparse=accept_sparse, dtype=dtype)\n        assert isinstance(X_converted, sp.csr_matrix)\n        if dtype == \"object\" and not accept_sparse:\n            assert_raises(ValueError,\n                          check_array, X, dtypes=dtype)\n\n\n\n", "def test_check_is_fitted_no_fit_method():\n    class NoFit:\n        pass\n\n    estimator = NoFit()\n    with assert_raises(TypeError):\n        check_is_fitted(estimator, \"coef_\")\n\n\n\n", "    def test_check_memory_small(self):\n        data = np.zeros((10, 10))\n        memory_usage = check_memory(data, None)\n        assert memory_usage is None\n", "    def test_check_sparse_format(tmp_path):\n        # Test with different sparse formats\n        X = sp.csc_matrix([[1, 2], [3, 4]])\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", FutureWarning)\n            # Case 1: accept_sparse is True\n            check_array(X, accept_sparse=True, dtype=np.float32)\n            assert X.format == 'csc'\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", FutureWarning)\n            # Case 2: accept_sparse is a list and format is allowed\n            check_array(X, accept_sparse=['csc'], dtype=np.float32)\n            assert X.format == 'csc'\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", FutureWarning)\n            # Case 3: accept_sparse is a list and format is not allowed\n            sparse_data = sp.csr_matrix([[1, 2], [3, 4]])\n            with pytest.raises(ValueError):\n                check_array(sparse_data, accept_sparse=['csc'], dtype=np.float32)\n        \n        # Test with 'allow_large_sparse'\n        large_sparse = sp.csc_matrix(\n            (np.arange(16), (np.arange(8) % 4, np.arange(8) // 4)),\n            shape=(4, 4),\n            dtype=np.int32,\n            data=np.arange(16)\n        )\n        \n        with pytest.raises(ValueError):\n            check_array(large_sparse, accept_large_sparse=False)\n  \n        check_array(large_sparse, accept_large_sparse=True)\n\n", "    def test_check_array_large_sparse(accept_large_sparse):\n        X = sp.csr_matrix(\n            [[1, 2], [3, 4]], dtype=np.int64\n        )\n        if accept_large_sparse:\n            assert_equal(check_array(X, accept_sparse=accept_large_sparse), X)\n        else:\n            with assert_raises_regex(ValueError, \"Indices are not 32-bit\"):\n                check_array(X, accept_sparse=accept_large_sparse)\n"], "sample_588": ["compilation error", "    def test_infer_concat_order_from_positions_nested_list(self):\n        ds1 = create_test_data(\"ds1\")\n        ds2 = create_test_data(\"ds2\")\n        ds3 = create_test_data(\"ds3\")\n        datasets = [ds1, ds2, ds3]\n        combined_ids, concat_dims = _infer_concat_order_from_positions(datasets)\n        expected_combined_ids = OrderedDict(\n            [\n                (\n                    (0, 0),\n                    ds1,\n                ),\n                ((0, 1), ds2),\n                ((0, 2), ds3),\n            ]\n        )\n        assert_combined_tile_ids_equal(combined_ids, expected_combined_ids)\n        assert concat_dims == [None, None]   \n", "    def test_tile_ids_from_nested_list_no_coords(self):\n        ds1 = create_test_data(dims=('x', 'y', 'z'))\n        ds2 = create_test_data(dims=('x', 'y', 'z'))\n        ds_combined = combine_nested([\n            [[ds1, ds2]],\n        ], concat_dim=['x', 'y', None])\n        combined_ids = _infer_concat_order_from_positions([ds1, ds2])\n        \n        assert_equal(list(combined_ids.keys()), ['((0, 0, 0), (1, 0, 0))'])\n\n", "    def test_tile_ids_from_nested_list(self):\n        d1 = create_test_data(dims=['x', 'y'], coords={'x': [0, 1], 'y': [0, 1]})\n        d2 = create_test_data(dims=['x', 'y'], coords={'x': [2, 3], 'y': [0, 1]}).rename({'x': 'xx'})\n        combined_ids = _infer_concat_order_from_positions([[[d1], [d2]]])\n        expected_combined_ids = OrderedDict(\n            [((\"d1\", 0), d1), ((\"d2\", 0), d2)]\n        )\n        assert_combined_tile_ids_equal(combined_ids, expected_combined_ids)\n\n", "    def test_nested_list_tile_ids_with_None(self):\n        \"\"\"\n        Test that tile IDs are generated correctly when a dimension is specified as None\n        in the input list.\n        \"\"\"\n        ds1 = create_test_data(\"one\")\n        ds2 = create_test_data(\"two\")\n        ds3 = create_test_data(\"three\")\n        ds4 = create_test_data(\"four\")\n\n        combined_ids = _infer_concat_order_from_positions([\n            [ds1, ds2],\n            [ds3, ds4]\n        ])\n\n        expected_combined_ids = OrderedDict([\n            (\"one\", ds1),\n            (\"two\", ds2),\n            (\"three\", ds3),\n            (\"four\", ds4),\n        ])\n\n        assert_combined_tile_ids_equal(combined_ids, expected_combined_ids)\n\n"], "sample_241": ["    def test_expression_cache(self):\n        # Expression cache test\n        from django.db.models.expressions import F\n        from django.test.utils import override_settings\n\n        @override_settings(\n            DATABASES={\n                'default': {\n                    'ENGINE': 'django.db.backends.sqlite3'\n                }\n            }\n        )\n            with CaptureQueriesContext(connection.default) as context:\n                # This test uses an artificial object to demonstrate F()\n                # expression caching. It's more accurate to test against\n                # actual models in a real database environment, but this\n                # approach highlights the principle.\n\n                class FakeModel(Model):\n                    id = AutoField(primary_key=True)\n                    value = IntegerField()\n\n                FakeModel.objects.create(value=10)\n                FakeModel.objects.create(value=20)\n\n                # Build an expression that references the 'value' field\n                expression = F('value') + 5\n                result = FakeModel.objects.values('id', expression).order_by('id').iterator()\n                # Check the result for correctness\n                self.assertEqual([\n                    {'id': 1, 'value': 15},\n                    {'id': 2, 'value': 25}\n                ], list(result))\n\n                # The second query should be faster due to caching\n                with CaptureQueriesContext(connection.default) as context:\n                    result = FakeModel.objects.values('id', expression).order_by('id').iterator()\n                    self.assertEqual(len(context.captured_queries), 1)\n                    self.assertEqual([\n                        {'id': 1, 'value': 15},\n                        {'id': 2, 'value': 25}\n                    ], list(result))\n        test_case()\n\n\n\n", "    def test_related_fields_in_aggregate(self):\n        with self.assertRaises(FieldError):\n            Company.objects.values(\n                \"name\", \"num_employees\", \"ceo__firstname\"\n            ).aggregate(avg_ceo_salary=Avg('ceo__salary'))\n\n\n\n", "    def test_subquery_annotations(self):\n        with self.subquery_context():\n            subquery = Subquery(\n                Experiment.objects.annotate(\n                    avg_results=Avg('results__value')\n                ).order_by('avg_results')\n            )\n            results = SimulationRun.objects.filter(\n                experiment__in=subquery\n            ).values('id', 'experiment__name')\n            expected_query = \"\"\"", "    def test_expression_list_ordering(self):\n        # Regression test for #29918\n        q = Company.objects.filter(\n            name__startswith=\"Ex\"\n        ).values('name')\n        expr_list = ExpressionList(\n            [\n                Expression(\n                    'name', \n                    function=cast, \n                    inputs=[\n                        Expression('name'), \n                        Value('Upper')\n                    ]\n                )\n            ]\n        )\n        qs = q.annotate(\n            upper_name=expr_list\n        ).order_by('upper_name')\n\n        with CaptureQueriesContext() as cm:\n            qs.all()\n\n        self.assertCountEqual(\n            sorted(cm.captured_queries[0].params[1:]),\n            ['Example Inc.']\n        )\n\n\n\n", "    def test_case_expressions(self):\n        with self.assertNumQueries(1):\n            results = (\n                Company.objects\n                .annotate(\n                    num_employees_squared=F(\"num_employees\") * F(\"num_employees\")\n                )\n                .filter(\n                    Q(num_employees__gt=10) | Q(num_employees__lt=5)\n                )\n                .distinct()\n                .values(\"name\", \"num_employees\", \"num_employees_squared\")\n            )\n        self.assertEqual(\n            len(list(results)), 2\n        )  # Only Example Inc and Foobar Ltd should pass\n\n        with self.assertNumQueries(1):\n            results = (\n                Company.objects\n                .annotate(\n                    num_employees_category=Case(\n                        When(num_employees__lt=10, then=\"Small\"),\n                        When(num_employees__gte=10, then=\"Large\"),\n                        default=\"Medium\",\n                    )\n                )\n                .values(\"name\", \"num_employees\", \"num_employees_category\")\n            )\n        self.assertEqual(\n            len(list(results)), 3\n        )  # All 3 companies\n\n\n\n"], "sample_692": ["    def test_get_user(monkeypatch: MonkeyPatch):\n        monkeypatch.setattr(get_pass, \"getuser\", lambda: \"testuser\")\n        assert get_user() == \"testuser\"\n\n\n", "    def test_tmpdir_factory_basetemp_from_config(\n        pytester: Pytester,", "    def test_get_user(monkeypatch: MonkeyPatch) -> None:\n        monkeypatch.setenv(\"PYTEST_DEBUG_TEMPROOT\", \"/tmp\")\n        with monkeypatch.context() as patched:\n            patched.setattr(os, \"getuid\", lambda: 1234)\n            assert get_user() == \"1234\"\n\n        with monkeypatch.context() as patched:\n            patched.setattr(os, \"getuid\", lambda: None)\n            assert get_user() is None\n\n", "    def test_getbasetemp_from_env(monkeypatch: MonkeyPatch, tmp_path: Path) -> None:\n        os.environ[\"PYTEST_DEBUG_TEMPROOT\"] = str(tmp_path)\n        tmppath_factory = TempPathFactory(given_basetemp=None, trace=None, _ispytest=True)\n        basetemp = tmppath_factory.getbasetemp()\n        assert basetemp.exists()\n        assert basetemp == tmp_path\n        del os.environ[\"PYTEST_DEBUG_TEMPROOT\"]\n\n", "    def test_getbasetemp_from_env(tmpdir_factory: TempPathFactory) -> None:\n        os.environ[\"PYTEST_DEBUG_TEMPROOT\"] = str(tmpdir_factory.getbasetemp().parent)\n        with pytest.raises(ValueError):\n            tmpdir_factory.getbasetemp()\n        del os.environ[\"PYTEST_DEBUG_TEMPROOT\"]\n"], "sample_849": ["    def test_kfold_default_splits(self, n_splits):\n        X = np.random.rand(100, 10)\n        kf = KFold(n_splits=n_splits, shuffle=False)\n        for train, test in kf.split(X):\n            assert isinstance(train, list), \"train must be a list\"\n            assert isinstance(test, list), \"test must be a list\"\n            self.assertIn(0, train), \"train indices should include 0\"\n            self.assertIn(1, train), \"train indices should include 1\"\n\n", "    def test_repeated_cv_with_scoring(self):\n        clf = MockClassifier()\n        X, y = make_classification(n_samples=100, n_features=20, n_informative=10)\n        param_grid = {'a': [0.1, 0.5, 1.0]}\n        cv = RepeatedKFold(n_splits=5, n_repeats=2, random_state=42)\n        grid = GridSearchCV(estimator=clf, param_grid=param_grid, cv=cv, scoring='accuracy', verbose=0)\n        grid.fit(X, y)\n\n        self.assertEqual(grid.best_params_, {\n            'a': 0.1,\n        })\n\n\n", "    def test_PredefinedSplit_with_duplicates(self):\n        X = np.arange(10).reshape((5, 2))\n        y = np.arange(10) // 2\n        test_fold = [0, 1, -1, 1, -1, 0, 1, 0, 1, 2]\n        ps = PredefinedSplit(test_fold)\n        train_index, test_index = next(ps.split(X, y))\n        assert len(train_index) == 5\n        assert len(test_index) == 5\n\n\n\n", "    def test_train_test_split_sparse_input(self):\n        X_sparse = coo_matrix((np.ones(10), (range(10), range(10))),\n                       shape=(10, 10))\n\n        X_train, X_test, y_train, y_test = train_test_split(\n            X_sparse, y, test_size=0.25, random_state=42\n        )\n        assert isinstance(X_train, coo_matrix)\n        assert isinstance(X_test, coo_matrix)\n", "    def test_train_test_split_sparse(self):\n        X_sparse = coo_matrix(np.random.rand(10, 5))\n        X_dense = X_sparse.toarray()\n        \n        # Test that the sparse matrix is maintained\n        X_train, X_test, _, _ = train_test_split(\n            X_sparse, shuffle=False, test_size=0.33\n        )\n        assert isinstance(X_train, csr_matrix)\n        assert isinstance(X_test, csr_matrix)\n\n        # Test that dense conversion\n        X_train_dense, X_test_dense, _, _ = train_test_split(\n            X_dense, shuffle=False, test_size=0.33\n        )\n        assert isinstance(X_train_dense, np.ndarray)\n        assert isinstance(X_test_dense, np.ndarray)\n\n        # Test that the output types are consistent\n        assert_allclose(X_train.toarray(), X_train_dense)\n        assert_allclose(X_test.toarray(), X_test_dense)\n\n        # Test that the same results are obtained with different sparse formats\n        X_sparse_csc = X_sparse.tocsc()\n        X_train_csc, X_test_csc, _, _ = train_test_split(\n            X_sparse_csc, shuffle=False, test_size=0.33\n        )\n        assert isinstance(X_train_csc, csc_matrix)\n        assert isinstance(X_test_csc, csc_matrix)\n\n        assert_allclose(X_train.toarray(), X_train_csc.toarray())\n        assert_allclose(X_test.toarray(), X_test_csc.toarray())\n\n"], "sample_465": ["    def test_get_formset_kwargs(self):\n        formset_kwargs = self.admin.get_formset_kwargs(request, self.band)\n        self.assertEqual(formset_kwargs[\"instance\"], self.band)\n        self.assertEqual(formset_kwargs[\"prefix\"], \"0\")\n        self.assertEqual(formset_kwargs[\"queryset\"], Concert.objects.none())\n", "    def test_get_admin_formset_kwargs(self):\n        class MyModelAdmin(ModelAdmin):\n            pass\n\n        formset_kwargs = MyModelAdmin().get_formset_kwargs(\n            request, obj=None, prefix=\"foo\"\n        )\n        self.assertEqual(formset_kwargs, {\"instance\": None, \"prefix\": \"foo\"})\n\n", "    def test_get_absolute_url(self):\n        admin = ModelAdmin(Concert, self.admin_site)\n        url = admin.get_absolute_url()\n        self.assertEqual(url, \"/admin/bands/concert/\")\n\n", "    def test_changeform_view_with_inline(self):\n        BandAdmin = ModelAdmin(Band, self.admin_site)\n        response = BandAdmin.changeform_view(request, object_id=self.band.id)\n        self.assertEqual(response.status_code, 200)\n\n        # Assert that the template was rendered\n        self.assertIn(\"edit_inline/stacked.html\", response.content.decode())\n\n\n\n", "    def test_get_formset_with_inline_and_extra(self):\n        class InlineAdmin(ModelAdmin):\n            model = Concert\n            extra = 5\n            form = ConcertForm\n\n        InlineAdmin.admin_site = self.admin_site\n        self.site.register(Band, ModelAdmin)\n        self.site.register(Concert, InlineAdmin)\n\n        formset = InlineAdmin.get_formset(MockRequest(), self.band)\n\n        self.assertEqual(formset.extra, 5)\n\n\n\n"], "sample_369": ["    def test_m2m_add_remove(self):\n        before = self.make_project_state([\n            author_with_m2m,\n            publisher,\n            contract,\n        ])\n        after = self.make_project_state([\n            author_with_m2m,\n            publisher,\n            contract,\n        ])\n        changes = self.get_changes(before, after)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RemoveField\", \"AddField\", \"RemoveField\"])\n        self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, primary_key=True)\n\n", "    def test_index_and_unique_together_handling(self):\n        changes = self.get_changes([\n            author_proxy_proxy_fk,\n            book,\n            attribution,\n            edition,\n            custom_user,\n            aardvark,\n            rabbit,\n            book_with_foo_together,\n        ], [\n            author_proxy_proxy_fk,\n            book,\n            attribution,\n            edition,\n            custom_user,\n            aardvark,\n            rabbit,\n            book_foo_together,\n            book_foo_together_2,\n            book_foo_together_3,\n            book_foo_together_4,\n        ])\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertNumberMigrations(changes, 'thirdapp', 1)\n        self.assertNumberMigrations(changes, 'otherapp', 1)\n        self.assertNumberMigrations(changes, 'eggs', 1)\n\n        self.assertMigrationDependencies(changes, 'testapp', 0, ['otherapp'])\n        self.assertMigrationDependencies(changes, 'thirdapp', 0, ['testapp'])\n        self.assertMigrationDependencies(changes, 'otherapp', 0, ['testapp'])\n\n        self.assertOperationTypes(changes, 'testapp', 0, ['CreateModel'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, model_name='Book', app_label='otherapp')\n        self.assertOperationTypes(changes, 'thirdapp', 0, ['CreateModel'])\n        self.assertOperationTypes(changes, 'otherapp', 0, ['CreateModel'])\n\n\n\n", "    def test_migration_with_auto_delete(self):\n        changes = self.get_changes(\n            [\n                author_with_m2m,\n                publisher_with_author,\n            ],\n            [\n                author_with_m2m_blank,\n                publisher_with_author,\n            ]\n        )\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertMigrationDependencies(changes, \"testapp\", 0, [])\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RemoveField\"])\n", "    def test_operation_attributes_with_constraints(self):\n        before_states = [\n            author_name_deconstructible_list_1,\n        ]\n        after_states = [\n            author_name_deconstructible_list_2,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertMigrationDependencies(changes, \"testapp\", 0, [])\n        self.assertOperationTypes(changes, \"testapp\", 0, ['CreateModel'])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, model_name=\"Author\",  fields=[{'name': 'name', 'field_type': 'CharField', 'max_length': 200}, {'name': 'name', 'field_type': 'CharField', 'max_length': 200}], primary_key='id')\n", "    def test_index_together_and_unique_together_operations_generation(self):\n        before_states = self.make_project_state([\n            book_foo_together,\n            book_foo_together_2,\n            book_foo_together_3,\n            book_foo_together_4,\n        ])\n        after_states = self.make_project_state([\n            book_foo_together,\n            book_foo_together_2,\n            book_foo_together_3,\n            book_foo_together_4,\n            book_foo_together,\n            book_foo_together_2,\n            book_foo_together_3,\n            book_foo_together_4,\n        ])\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, \"otherapp\", 8)\n\n        # Assert specific index operations are generated\n        self.assertMigrationDependencies(changes, \"otherapp\", 0, [])\n        self.assertOperationTypes(changes, \"otherapp\", 0, ['CreateModel'])\n        self.assertOperationTypes(changes, \"otherapp\", 1, ['AddIndex'])\n        self.assertOperationTypes(changes, \"otherapp\", 2, ['AddIndex'])\n        self.assertOperationTypes(changes, \"otherapp\", 3, ['AddIndex'])\n        self.assertOperationTypes(changes, \"otherapp\", 4, ['AddIndex'])\n        self.assertOperationTypes(changes, \"otherapp\", 5, ['AddIndex'])\n        self.assertOperationTypes(changes, \"otherapp\", 6, ['AddIndex'])\n        self.assertOperationTypes(changes, \"otherapp\", 7, ['AddIndex'])\n\n\n"], "sample_1163": ["    def test_periodic_argument_period_infinity():\n        from sympy.functions.complexes import exp_polar\n        from sympy.abc import z\n        assert periodic_argument(z, oo) == z\n        assert periodic_argument(exp_polar(2*I*pi), oo) == 2*pi*I\n        assert periodic_argument(exp_polar(10*I*pi), oo) == 10*pi*I\n        assert periodic_argument(exp_polar(10*I*pi) * z, oo) == 10*pi*I*z\n", "    def test_polar_lift_evalf():\n        from sympy import polar_lift, sin, I, exp\n        for i in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]:\n            z = i * I\n            pl = polar_lift(z)\n            assert N_equals(pl._eval_evalf(10), z)\n\n        z = 2 + 3*I\n        pl = polar_lift(z)\n        assert N_equals(pl._eval_evalf(10), z)\n\n        z = exp(I * pi)\n        pl = polar_lift(z)\n        assert N_equals(pl._eval_evalf(10), z)\n", "    def func(z):\n        return exp_polar(z)\n", "compilation error", "    def test_argument_index_error(self):\n        from sympy import polar_lift\n        polar_lift(1, 'bad') \n"], "sample_375": ["    def test_model_state_from_model_with_m2m(self):\n        food = apps.get_model('app1', 'Food')\n        state = ModelState.from_model(food)\n\n        self.assertIn('category', state.fields)\n        self.assertIsInstance(state.fields['category'].related_model, models.Model)\n\n\n", "    def test_field_operations_on_state(self):\n        with isolate_apps(\n            apps=[\"test_state\"]\n        ):\n            # Start with a model state\n            model_state = ModelState.from_model(UnicodeModel)\n\n            # Make a ProjectState with this model\n            project_state = ProjectState(\n                models={\n                    model_state.app_label: model_state\n                }\n            )\n\n            # Do some field modifications\n            state = project_state.clone()\n\n            operation = AddField(\n                model_name='UnicodeModel',\n                name='extra_field',\n                field=models.CharField(max_length=255),\n            )\n            operation.apply(state)\n\n            # Check that the state reflects the change\n            new_model_state = state.models[\n                model_state.app_label\n            ][model_state.name]\n            self.assertIn('extra_field', new_model_state.fields)\n\n            operation = AlterField(\n                model_name='UnicodeModel',\n                name='some_field',\n                field=models.CharField(max_length=100),\n            )\n            operation.apply(state)\n\n            # Check that the change persists\n            new_model_state = state.models[\n                model_state.app_label\n            ][model_state.name]\n            updated_field = new_model_state.fields['some_field']\n            self.assertEqual(updated_field.max_length, 100)\n\n            operation = RemoveField(\n                model_name='UnicodeModel',\n                name='some_field',\n            )\n            operation.apply(state)\n\n            # Check that the field is gone\n            new_model_state = state.models[\n                model_state.app_label\n            ][model_state.name]\n            self.assertNotIn('some_field', new_model_state.fields)\n\n\n\n", "    def test_operations_recreate_after_render(self):\n        with isolate_apps(['test']):\n            # Start with a simple model\n            model = UnicodeModel._meta\n            project_state = ProjectState([\n                model.app_label, model.model_name,\n            ])\n            model_state = project_state.models[(model.app_label, model.model_name)]\n            model_instance = model_state.render(project_state.apps)\n\n            # Add a field\n            project_state._operations.append(AddField(\n                model_name='model_test',\n                field=models.CharField(max_length=100),\n            ))\n\n            # Render the model again - should now have the added field\n            model_state = project_state.models[(model.app_label, model.model_name)]\n            model_instance = model_state.render(project_state.apps)\n            self.assertIn('field_name', model_instance._meta.fields)\n", "    def test_model_state_from_model_with_m2m_and_bases(self):\n        with isolate_apps(['mytests']):\n            with override_settings(INSTALLED_APPS=['mytests']):\n                # Create a model with both M2M fields and custom bases\n                class CustomBase(models.Model):\n                    pass\n\n                class Food(models.Model):\n                    name = models.CharField(max_length=200)\n                    ingredients = models.ManyToManyField('self', related_name='included_in')\n                    base = models.ForeignKey(CustomBase, on_delete=models.CASCADE)\n\n                # Get a ModelState from the model\n                state = ModelState.from_model(Food)\n\n                # Assert that the M2M field is present in the state\n                self.assertIn('ingredients', state.fields)\n\n                # Assert that the custom base is also present in the state\n                self.assertIn('base', state.fields)\n\n\n\n", "    def test_remove_field_from_state(self):\n        with isolate_apps('food_app'):\n            with override_settings(\n                INSTALLED_APPS=('food_app',),\n            ):\n                model = ModelWithCustomBase\n\n                state = ProjectState.from_apps(Apps())\n                state.register_model('food_app', model)\n\n                # Remove a field from the state\n                operation = RemoveField(\n                    model_name='food_app.usermodel',\n                    name='name',\n                )\n                operation.apply(state)\n\n                # Check that the field has been removed from the state\n                model_state = state.models[('food_app', 'usermodel')]\n                self.assertNotIn('name', model_state.fields)\n\n                # Check that the model is still valid\n                model_state.render(state.apps)\n\n\n"], "sample_1045": ["def test_sympify_mpz():\n    cases = [\n        (1, Integer(1)),\n        (-10, Integer(-10)),\n        (0, Integer(0)),\n        (gmpy.mpz(314159265358979323846264338327950288419716939937510),\n         Integer(314159265358979323846264338327950288419716939937510)),\n    ]\n    for i, (gmpy_num, expected) in enumerate(cases):\n        try:\n            actual = sympify(gmpy_num)\n            assert srepr(actual) == srepr(expected), \\\n                f\"Test {i}: sympify({gmpy_num}) = {actual}, expected \" \\\n                f\"{expected}\"\n        except Exception as e:\n            if isinstance(e, TypeError):\n                if \"mpz\" not in str(e):\n                    raise\n            raise\n\n", "    def test_sympify_mpmath_complex():\n        from mpmath import mp\n        mp.dps = 20\n        z = mp.mpf('1 + 2j')\n        s = sympify_mpmath(z)\n        assert s.is_complex\n        assert s.real == 1\n        assert s.imag == 2\n        assert same_and_same_prec(s.real, mp.mpf('1'))\n        assert same_and_same_prec(s.imag, mp.mpf('2'))\n\n", "    def test_sympify_complex_from_float():\n        assert sympify(3.14 + 2.71j) == 3.14 + 2.71*I\n", "    def test_pow_negative_integer_base_zero():\n        assert Pow(S.NegativeInfinity, -2) == S.Zero\n    ", "    def test_sympify_arbitrary_precision():\n        # Test sympification for arbitrary precision numbers\n        a = mpf(pi, 30)\n        b = sympify(a)\n        assert isinstance(b, Float)\n        assert same_and_same_prec(b, pi)\n        assert b.is_Number  \n"], "sample_1082": ["    def test_acoth_rewrite_as_log_eval():\n        from sympy import log, S\n        z = acoth(S(2))\n        res = z._eval_rewrite_as_log(S(2))\n        assert res == log(1 + 1/2) - log(1 - 1/2)\n", "    def test_asech_negative_argument():\n        from sympy import S, I\n\n        raises(ValueError, lambda: asech(-2))\n        raises(ValueError, lambda: asech(-S.Infinity))\n\n\n\n", "    def test_asech_expand():\n        from sympy import expand_func\n        x = symbols('x')\n        expr = asech(x)\n        expanded = expand_func(expr)\n        expected = log(1/x + sqrt(1/x - 1) * sqrt(1/x + 1))\n        assert expanded == expected\n\n", "    def test_acsch_eval_negative():\n        from sympy import S, I\n        expr = acsch(-S.ImaginaryUnit*sqrt(2))\n        assert expr ==  I*pi/6\n        expr = acsch(-2*I)\n        assert expr ==  I*pi/6\n        expr = acsch(-(1 + sqrt(5))*I)\n        assert expr == -3*I*pi/10\n\n\n", "    def test_acsch_complex():\n        from sympy.functions.hyperbolic import acsch\n        x = I * 2\n        assert acsch(x).args[0] == x\n        assert acsch(x).func(x) == acsch(x)\n        assert acsch(-x) == -acsch(x)\n        assert acsch(x).evalf() == -1.5707963267948966 \n        assert acsch(I).evalf() == -1.5707963267948966\n        assert acsch(-I).evalf() == 1.5707963267948966\n\n\n\n"], "sample_1092": ["    def test_cse_matrices():\n        m1 = MutableDenseMatrix([[x, y], [z, w]])\n        m2 = MutableDenseMatrix([[x + y, z], [w, x]])\n        m3 = MutableDenseMatrix([[x, y], [z, w]])\n        m4 = MutableDenseMatrix([[x + y, z], [w, x]])\n        m5 = ImmutableDenseMatrix([[x, y], [z, w]])\n        m6 = ImmutableDenseMatrix([[x + y, z], [w, x]])\n\n        test_cases = [\n            ((m1, m2), [(x0, y + z), (x1, x + y)],\n             [m1, m2]),\n            ((m1, m3), [(x0, y + z), (x1, x + y)],\n             [m1, m3]),\n            ((m2, m4), [(x0, y + z), (x1, x + y)],\n             [m2, m4]),\n            ((m5, m6), [(x0, y + z), (x1, x + y)],\n             [m5, m6]),\n        ]\n\n        for (exprs, replacements, expected_reduced) in test_cases:\n            resultants, reduced_exprs = cse(exprs)\n            assert resultants == replacements\n            assert reduced_exprs == expected_reduced\n", "    def test_cse_matrices():\n        m = MutableDenseMatrix([[x, y], [z, w]])\n        n = MutableDenseMatrix([[x + 1, y + 1], [z + 1, w + 1]])\n        exprs = [m*n, m+n]\n\n        r, e = cse(exprs)\n        assert len(r) == 0\n        assert isinstance(e[0], MutableDenseMatrix)\n        assert isinstance(e[1], MutableDenseMatrix)\n\n", "    def test_sparse_matrices():\n        m1 = MutableSparseMatrix([[1, 2], [3, 4]])\n        m2 = MutableSparseMatrix([[5, 6], [7, 8]])\n        m3 = m1 + m2\n\n        r, e = cse([m1, m2, m3])\n        assert len(r) == 2\n        assert r[0][0] == x0\n        assert r[0][1] == MutableSparseMatrix([[1, 2], [3, 4]])\n        assert r[1][0] == x1\n        assert r[1][1] == MutableSparseMatrix([[5, 6], [7, 8]])\n\n        m4 = ImmutableSparseMatrix([[1, 2], [3, 4]])\n        m5 = ImmutableSparseMatrix([[5, 6], [7, 8]])\n        m6 = m4 + m5\n\n        r, e = cse([m4, m5, m6])\n        assert len(r) == 2\n        assert r[0][0] == x0\n        assert r[0][1] == ImmutableSparseMatrix([[1, 2], [3, 4]])\n        assert r[1][0] == x1\n        assert r[1][1] == ImmutableSparseMatrix([[5, 6], [7, 8]])\n\n\n", "compilation error", "    def test_cse_matrix_operations(self):\n        A = MatrixSymbol('A', 2, 2)\n        B = MatrixSymbol('B', 2, 2)\n        C = MatrixSymbol('C', 2, 2)\n\n        exprs = [\n            (A + B) * (A + B).transpose(),\n            A * (B + C) + B * C,\n            (A * B).transpose() * (A * B),\n            A * (B + C) - A * B,\n            (A + B) ** 2,\n            (A - B) ** 3,\n            (A + B) * (C - D)\n        ]\n\n        replacements, reduced_exprs = cse(exprs)\n        self.assertEqual(len(replacements), 0)\n\n\n"], "sample_13": ["    def test_wrap_angle_inplace(self):\n        a = Angle(180 * u.deg)\n        a_wrapped = a.copy()\n        a_wrapped.wrap_at(360 * u.deg, inplace=True)\n        assert_allclose(a_wrapped.degree, 180)\n        assert a_wrapped.wrap_angle == 360 * u.deg\n\n        a = Angle(180 * u.deg)\n        a_wrapped = a.copy()\n        a_wrapped.wrap_at(180 * u.deg, inplace=True)\n        assert_allclose(a_wrapped.degree, 0)\n        assert a_wrapped.wrap_angle == 180 * u.deg\n", "    def test_parse_angle_strings(self, angle, expected):\n        result = Angle(angle)\n        assert_allclose(result, expected)\n", "    def test_longitude_wrap_angle_default(self, angle):\n        lon = Longitude(angle)\n        assert_allclose(lon.wrap_angle.to(u.deg), 360)\n        assert_allclose(lon.to(u.deg), np.array([123]))\n", "    def test_wrap_at_array(self):\n        angle_array = Angle([10, 180, 280, 370] * u.deg)\n        wrapped_array = angle_array.wrap_at(360 * u.deg)\n        expected_array = Angle([10, 180, 280, 370] * u.deg)\n        assert_array_equal(wrapped_array.degree, expected_array.degree)\n\n        wrapped_array = angle_array.wrap_at(180 * u.deg)\n        expected_array = Angle([10, 180, 280, 370 - 360] * u.deg)\n        assert_array_equal(wrapped_array.degree, expected_array.degree)\n\n\n\n", "    def test_to_string_deg(self, value, expected_output):\n        angle = Angle(value)\n        assert angle.to_string() == expected_output\n    "], "sample_400": ["    def test_migration_order(self):\n        changes = self.get_changes(\n            [\n                author_name_nested_deconstructible_extra_arg,\n                author_name_nested_deconstructible_changed_kwarg,\n                author_name_nested_deconstructible_extra_kwarg,\n                author_with_custom_pk,\n                book,\n            ],\n            [\n                author_with_custom_pk,\n                author_name_nested_deconstructible_extra_arg,\n                author_name_nested_deconstructible_changed_kwarg,\n                author_name_nested_deconstructible_extra_kwarg,\n                book,\n            ],\n        )\n        self.assertNumberMigrations(changes, \"testapp\", 4)\n        self.assertMigrationDependencies(changes, \"testapp\", 0, [])\n        self.assertMigrationDependencies(changes, \"testapp\", 1, [\"testapp.author_with_custom_pk\"])\n        self.assertMigrationDependencies(changes, \"testapp\", 2, [\"testapp.author_with_custom_pk\"])\n        self.assertMigrationDependencies(changes, \"testapp\", 3, [\"testapp.author_with_custom_pk\"])\n        self.assertOperationTypes(changes, \"testsapp\", 0, [\"CreateModel\", \"CreateModel\"])\n        self.assertOperationTypes(changes, \"testapp\", 1, [\"AlterField\", \"AddIndex\"])\n        self.assertOperationTypes(changes, \"testapp\", 2, [\"AlterField\", \"AddIndex\"])\n        self.assertOperationTypes(changes, \"testapp\", 3, [\"AlterField\", \"AddIndex\"])\n\n\n\n", "    def test_index_together_and_unique_together_with_foreign_key_and_field(self):\n        before_states = self.make_project_state(\n            [\n                book_foo_together_3,\n                attribution,\n                \n            ]\n        )\n        after_states = self.make_project_state(\n            [\n                book_foo_together_4,\n                attribution,\n            ]\n        )\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertMigrationDependencies(changes, \"otherapp\", 0, [])\n        self.assertOperationTypes(changes, \"otherapp\", 0, [\"RemoveField\"])\n        self.assertOperationTypes(changes, \"otherapp\", 0, [\"AddField\"])\n        self.assertOperationFieldAttributes(\n            changes,\n            \"otherapp\",\n            0,\n            0,\n            model_name=\"Book\",\n            attname=\"newfield\",\n        )\n\n\n        \n", "    def test_rename_model_without_dependencies(self):\n        before_states = [\n            ModelState(\n                \"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True))]\n            )\n        ]\n        after_states = [\n            ModelState(\n                \"testapp\", \"NewAuthor\", [(\"id\", models.AutoField(primary_key=True))]\n            )\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertMigrationDependencies(changes, \"testapp\", 0, [])\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RenamedModel\"])\n\n\n", "    def test_delete_model(self):\n        changes = self.get_changes(\n            [\n                ModelState(\n                    \"testapp\",\n                    \"Author\",\n                    [\n                        (\"id\", models.AutoField(primary_key=True)),\n                        (\"name\", models.CharField(max_length=200)),\n                    ],\n                ),\n                ModelState(\n                    \"testapp\",\n                    \"Publisher\",\n                    [\n                        (\"id\", models.AutoField(primary_key=True)),\n                        (\"name\", models.CharField(max_length=100)),\n                    ],\n                ),\n            ],\n            [\n                ModelState(\n                    \"testapp\",\n                    \"Author\",\n                    [],\n                    {\"delete\": True},\n                    (\"testapp.Author\",),\n                ),\n                ModelState(\n                    \"testapp\",\n                    \"Publisher\",\n                    [\n                        (\"id\", models.AutoField(primary_key=True)),\n                        (\"name\", models.CharField(max_length=100)),\n                    ],\n                ),\n            ],\n        )\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertMigrationDependencies(changes, \"testapp\", 0, [])\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"DeleteModel\"])\n", "    def test_rename_field(self):\n        before_states = [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=200)),\n                    (\"old_biography\", models.TextField()),\n                ],\n            )\n        ]\n        after_states = [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=200)),\n                    (\"new_biography\", models.TextField()),\n                ],\n            )\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertMigrationDependencies(changes, \"testapp\", 0, [])\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RemoveField\", \"AddField\"])\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 1, model_name=\"Author\", old_name=\"old_biography\"\n        )\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 2, model_name=\"Author\", new_name=\"new_biography\"\n        )\n\n"], "sample_555": ["    def test_fancy_arrow_style(self):\n        fig, ax = plt.subplots()\n        arrow = FancyArrowPatch(\n            (0.2, 0.5), (0.8, 0.5),\n            arrowstyle=\"->\", mutation_scale=2,\n            patchA=mpatches.Circle((0.2, 0.5), 0.1, color='red'),\n            patchB=mpatches.Circle((0.8, 0.5), 0.1, color='blue')\n        )\n        ax.add_artist(arrow)\n\n        ax.set_xlim(0, 1)\n        ax.set_ylim(0, 1)\n        ax.axis('off')\n        plt.show()\n", "    def test_fancyarrowpatch_mutation(self):\n        fig, ax = plt.subplots()\n        arrow = FancyArrowPatch((0,0), (1,1),\n                               arrowstyle='-|>',\n                               mutation_scale=2,\n                               mutation_aspect=0.5)\n        ax.add_artist(arrow)\n        plt.show()\n\n", "    def test_fancyarrow_patch(self):\n        fig, ax = plt.subplots()\n        arrow = FancyArrowPatch((0.2, 0.2), (0.8, 0.8),\n                                arrowstyle=\"->\",\n                                mutation_scale=1.5, facecolor=\"red\",\n                                linewidth=2)\n\n        ax.add_patch(arrow)\n        ax.set_xlim(0, 1)\n        ax.set_ylim(0, 1)\n        plt.show()\n", "    def test_FancyArrowPatch_mutation_scale(self):\n        fig, ax = plt.subplots()\n        arrow = FancyArrowPatch(\n            (0.2, 0.2), (0.8, 0.8), arrowstyle=\"->\", mutation_scale=2\n        )\n        ax.add_artist(arrow)\n        plt.show()\n\n        # Check that the arrow has been scaled as expected\n        assert_almost_equal(arrow.get_patch_transform().transform(\n            arrow.get_path().vertices[0]),\n            (0.2 * 2, 0.2 * 2),\n            decimal=4)\n\n\n", "    def test_fancyarrowpatch_linewidth(self, linewidth):\n        fig, ax = plt.subplots()\n        con = FancyArrowPatch((0.2, 0.2), (0.8, 0.8),\n                              arrowstyle=\"-|>\", linewidth=linewidth)\n        ax.add_artist(con)\n        fig.canvas.draw()\n        lines = fig.canvas.get_renderer().get_lines() \n        assert len(lines) == 1 if linewidth is not None else 0\n        if linewidth is not None:\n            assert lines[0].get_linewidth() == linewidth\n\n\n\n"], "sample_1036": ["    def test_MatPow():\n        assert MatPow(A, 0) == Identity(n)\n        assert MatPow(A, 1) == A\n        assert MatPow(A, 2) * MatPow(A, 3) == MatPow(A, 5)\n        assert MatPow(MatPow(A, 2), 3) == MatPow(A, 6)\n        assert MatPow(A, -1) == Inverse(A)\n", "compilation error", "    def test_as_content_primitive_issue_3054():\n        from sympy import sqrt\n        expr = (2*sqrt(2)*(2 + 2*sqrt(2))).as_content_primitive(radical=True)\n        assert expr[0] == 4 and expr[1] == sqrt(2)*(1 + sqrt(2))\n\n\n", "    def test_as_content_primitive():\n        from sympy.abc import x, y\n        from sympy import sqrt, S\n        from sympy.polys import poly\n\n        expr = (2*x*y*sqrt(2) - 4*x*y)  \n        cont, prim = expr.as_content_primitive()\n        assert cont == 2*sqrt(2) - 4\n        assert prim == x*y\n\n        expr = (2*x*y*sqrt(2))  \n        cont, prim = expr.as_content_primitive()\n        assert cont == 2*sqrt(2)\n        assert prim == x*y\n\n        expr = (2*x*y*sqrt(2) - 4*x*y).as_content_primitive()\n        cont, prim = expr.as_content_primitive()\n        assert cont == 2*sqrt(2) - 4\n        assert prim == x*y\n\n        expr = (2*x*y*sqrt(2) - 4*x*y).as_content_primitive(clear=False)\n        cont, prim = expr.as_content_primitive(clear=False)\n        assert cont == 2*sqrt(2) - 4\n        assert prim == (x*y)\n\n        expr = (2*x*y*(sqrt(2) - 2)).as_content_primitive(clear=False)\n        cont, prim = expr.as_content_primitive(clear=False)\n        assert cont == sqrt(2) - 2\n        assert prim == 2*x*y\n\n        expr = (2*x*y*(sqrt(2) - 2)).as_content_primitive(radical=True)\n        cont, prim = expr.as_content_primitive(radical=True)\n        assert cont == sqrt(2) - 2\n        assert prim == 2*x*y\n\n        expr = (x*y*sqrt(2) + 2*x*y).as_content_primitive()\n        cont, prim = expr.as_content_primitive()\n        assert cont == 1\n        assert prim == x*y*sqrt(2) + 2*x*y\n\n        expr = (x*y*sqrt(", "    def test_as_content_primitive_mul():\n        from sympy import sqrt\n        from sympy.abc import x, y\n        expr = (2*x*y*sqrt(2)*(2 - 2*sqrt(2)))\n        assert expr.as_content_primitive(radical=True) == (6, -sqrt(2)*(1 - sqrt(2)))"], "sample_1034": ["def test_grover_iteration_multiple_targets():\n    nqubits = 2\n    f = lambda qubits: qubits == IntQubit(1, nqubits=nqubits) or qubits == IntQubit(3, nqubits=nqubits)\n    v = OracleGate(nqubits, f)\n    basis_states = superposition_basis(nqubits)\n    iterated = grover_iteration(basis_states, v)\n    expected_state = (1/sqrt(4)) * (IntQubit(1) + IntQubit(3))\n    assert qapply(iterated) == expected_state \n", "compilation error", "def test_grover_iteration_with_multiple_oracle_targets():\n    nqubits = 2\n    oracle_func = lambda qubits: qubits == IntQubit(1, nqubits=nqubits) or qubits == IntQubit(3, nqubits=nqubits)\n    oracle = OracleGate(nqubits, oracle_func)\n    initial_state = superposition_basis(nqubits)\n\n    # Apply one iteration of Grover's algorithm\n    iterated_state = grover_iteration(initial_state, oracle)\n\n    # Check that the state has a significant amplitude on the target qubits\n    # (You might need to adjust the threshold based on your implementation)\n    amplitudes = [qapply(iterated_state.evolve(q)) for q in [IntQubit(1, nqubits=nqubits), IntQubit(3, nqubits=nqubits)]]\n    assert all(abs(amp) > 0.1 for amp in amplitudes) \n\n", "def test_oracle_general():\n    numqubits = 2\n    basis_states = superposition_basis(numqubits)\n\n    # Test OracleGate with a function that returns 1 on multiple qubit states\n    f = lambda qubits: qubits == IntQubit(1, nqubits=numqubits) or qubits == IntQubit(2, nqubits=numqubits)\n    v = OracleGate(numqubits, f)\n    \n    # Apply the oracle to a superposition state\n    result = qapply(v * basis_states)\n\n    # Assert that the result is correct (sign flipped only on desired states)\n    # You'll need to define specific assertions based on the expected behavior\n    # of your OracleGate implementation\n    \n\n", "compilation error"], "sample_133": ["    def test_set_language_inactive_language(self):\n        inactive_language = self._get_inactive_language_code()\n        url = reverse('set_language', kwargs={'next': '/test/'})\n        response = self.client.post(url, data={LANGUAGE_QUERY_PARAMETER: inactive_language})\n        self.assertEqual(response.status_code, 302)\n\n        # Redirect should be to the active language\n        redirect_url = response.headers['Location']\n        self.assertNotIn(LANGUAGE_QUERY_PARAMETER, redirect_url)\n        self.assertEqual(get_language(), inactive_language)\n\n        # Cookie should be set with the new language\n        self.assertContains(response, inactive_language, cookie=True)\n\n        # Session should be set if available\n        if hasattr(self.client.session, 'get'):\n            self.assertEqual(self.client.session[LANGUAGE_SESSION_KEY], inactive_language)\n\n        # Request should still be processed with the active language\n        response = self.client.get('/test/')\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(get_language(), inactive_language)\n", "    def test_set_language_redirects_to_next_url(self):\n        inactive_language = self._get_inactive_language_code()\n        url = reverse('set_language')\n        request = RequestFactory().get(url, {'next': '/some/path', 'language': inactive_language})\n        response = set_language(request)\n\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response['Location'], f'/some/path?{LANGUAGE_QUERY_PARAMETER}={inactive_language}')\n", "    def test_set_language_redirect_with_next_url(self):\n        request = RequestFactory().post(\n            reverse('set_language'),\n            data={'next': '/path/to/page', LANGUAGE_QUERY_PARAMETER: self._get_inactive_language_code()}\n        )\n        response = set_language(request)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response['Location'], '/path/to/page')\n\n        # Check if the session and cookie are modified\n        with self.assertNoWarnings(RemovedInDjango40Warning):\n            self.assertIn(LANGUAGE_SESSION_KEY, request.session)\n            self.assertEqual(request.session[LANGUAGE_SESSION_KEY], self._get_inactive_language_code())\n        self.assertIn(settings.LANGUAGE_COOKIE_NAME, response.cookies)\n\n\n\n", "    def test_set_language_redirects_to_same_page_if_no_next_url(self):\n        factory = RequestFactory()\n        request = factory.post('/', **{'next': '/test/url', LANGUAGE_QUERY_PARAMETER: self._get_inactive_language_code()})\n        response = set_language(request)\n        self.assertRedirects(response, '/test/url', 302, 'Location', None)\n\n        request = factory.get('/', **{'next': '/test/url', LANGUAGE_QUERY_PARAMETER: self._get_inactive_language_code()})\n        response = set_language(request)\n        self.assertRedirects(response, '/test/url', 302, 'Location', None)\n\n\n", "    def test_set_language_post_no_language(self):\n        response = self.client.post(reverse('set_language'), {'next': '/foo'})\n        self.assertEqual(response.status_code, 204)\n        self.assertNotIn(settings.LANGUAGE_COOKIE_NAME, self.client.cookies)\n"], "sample_284": ["    def test_manifest_strict_mode_missing_file(self):\n        with self.assertRaises(ValueError):\n            call_command('collectstatic',\n                         interactive=False,\n                         verbosity=0,\n                         ignore=[hashed_file_path(self, \"missing_file.css\")])\n\n        self.assertPostCondition()\n", "    def test_manifest_strict_throws_error(self):\n        settings.STATICFILES_STORAGE = 'test_app.ManifestStaticFilesStorage'\n        with self.assertRaises(ValueError):\n            call_command('collectstatic', clear=True, interactive=False)\n\n        # TODO: Check if the manifest file is also not created\n\n\n", "    def test_manifest_strictness(self):\n        with override_settings(STATICFILES_STORAGE='staticfiles.ManifestStaticFilesStorage'):\n            # Create a scenario where a file is encountered that isn't in the manifest.\n            self.create_static('static/test_manifest/missing.css')\n            with self.assertRaises(ValueError) as context:\n                call_command('collectstatic', interactive=False,\n                             verbosity=0, ignore_pattern=r'')\n            self.assertIn(\"Missing staticfiles manifest entry for 'static/test_manifest/missing.css'\", context.exception)\n", "    def test_manifest_strict_mode(self):\n        with override_settings(STATICFILES_STORAGE='mytest.ManifestStaticFilesStorage'):\n            settings.STATIC_ROOT = self.temp_dir\n\n            call_command('collectstatic', ignore_patterns=['*'])\n\n            # Assert that the manifest exists and has a content\n            self.assertTrue(os.path.isfile(os.path.join(settings.STATIC_ROOT, 'staticfiles.json')))\n\n            # Assert that a missing file will raise an error\n            with self.assertRaises(ValueError):\n                storage.staticfiles_storage.url('')\n\n            self.assertPostCondition()\n", "    def test_manifest_strict_error(self):\n        with override_settings(STATICFILES_STORAGE='django.contrib.staticfiles.storage.ManifestStaticFilesStorage'):\n            with tempfile.TemporaryDirectory() as tmpdir:\n                call_command('collectstatic',\n                             verbosity=0,\n                             interactive=False,\n                             ignore=[os.path.join(TEST_ROOT, 'static')],\n                             output_dir=tmpdir)\n                path = hashed_file_path(self, 'test_app/static/test.css')\n                with self.assertRaises(ValueError) as cm:\n                    storage.staticfiles_storage.stored_name(path)\n                self.assertIn(\"Missing staticfiles manifest entry for 'test'\", str(cm.exception))\n\n\n\n"], "sample_707": ["compilation error", "    def test_iterparentnodeids_empty_node(testdir: Pytester):\n        p = testdir.makepyfile(\n            \"\"\"\n                pass\n            \"\"\"\n        )\n        result = testdir.runpytest(\"-n\", \"1\", p)\n        assert result.ret(0)\n        nodes = testdir.getnodes()\n        assert 0 == len(list(nodes.iterparentnodeids()))\n", "def test_iterparentnodeids(\n    testee: nodes.Node,\n    pytester: Pytester,", "compilation error", "compilation error"], "sample_1077": ["    def test_complex_region_union_intersection_polar():\n        r = Interval(0, 1)\n        theta = Interval(0, 2*pi)\n        c1 = ComplexRegion(r*theta, polar=True)\n        c2 = ComplexRegion(r*theta, polar=True)\n        assert c1.union(c2) == c1\n        c3 = ComplexRegion(r*theta, polar=True)\n        assert c1.intersection(c3) == c1\n\n        theta1 = Interval(0, pi)\n        c4 = ComplexRegion(r*theta1, polar=True)\n        assert c1.intersection(c4) == c4\n        assert c1.union(c4) == c1\n ", "    def test_complex_region_from_real(self):\n        from sympy import Interval, ComplexRegion\n\n        self.assertEqual(ComplexRegion.from_real(Interval(0, 1)),\n                         ComplexRegion(ProductSet(Interval(0, 1), {0}), False))\n        self.assertEqual(ComplexRegion.from_real(Interval(-2, 2)),\n                         ComplexRegion(ProductSet(Interval(-2, 2), {0}), False))\n        self.assertEqual(ComplexRegion.from_real(Interval(0, 1)*Interval(0, 1)),\n                         ComplexRegion(ProductSet(Interval(0, 1), Interval(0, 1)), False))\n        \n\n", "    def test_complex_region_intersection_polar_rectangular(self):\n        r = Interval(0, 1)\n        theta = Interval(0, pi/2)\n        c1 = ComplexRegion(r*theta, polar=True)  \n        c2 = ComplexRegion(Interval(0, 1)*Interval(0, pi))\n        c3 = c1.intersect(c2)\n        self.assertEqual(c3, ComplexRegion(Interval(0, 1)*Interval(0, pi/2), polar=True))\n\n", "    def test_complex_region_union_intersection(self):\n        from sympy.sets.fancysets import ComplexRegion\n\n        # Test Union of Complex Regions\n        a = Interval(2, 3)\n        b = Interval(4, 5)\n        c = Interval(1, 7)\n        U1 = ComplexRegion(a * b)\n        U2 = ComplexRegion(b * c)\n        U = U1.union(U2)\n        self.assertEqual(U, ComplexRegion(Union(a*b, b*c)))\n        \n        # Test Intersection of Complex Regions\n        I1 = ComplexRegion(Union(a*b, b*c))\n        I2 = ComplexRegion(a*b)\n        I = I1.intersect(I2)\n        self.assertEqual(I, ComplexRegion(a*b))\n\n        a = Interval(0, 1)\n        b = Interval(0, 2*pi)\n        unit_disk = ComplexRegion(a * b, polar=True)\n        upper_half_disk = ComplexRegion(a * Interval(0, pi), polar=True)\n        I = unit_disk.intersect(upper_half_disk)\n        self.assertEqual(I, upper_half_disk)\n", "    def test_complex_region_intersect_with_real_sets(self):\n        from sympy.sets import Reals\n        c1 = ComplexRegion(Interval(0, 1)*Interval(0, 2*pi), polar=True)\n        c2 = Reals\n        result = c1.intersect(c2)\n        self.assertEqual(result, c1) \n\n\n"], "sample_468": ["    def test_context_processors_are_called(self):\n        request = HttpRequest()\n        context_processor_called = False\n\n            nonlocal context_processor_called\n            context_processor_called = True\n            return {'test_key': 'test_value'}\n\n        with override_settings(TEMPLATE_CONTEXT_PROCESSORS=[test_processor]):\n            context = RequestContext(request)\n            self.assertTrue(context_processor_called)\n            self.assertEqual(context['test_key'], 'test_value')\n", "    def test_context_inheritance(self):\n        parent_context = Context(dict(a=1, b=2))\n        child_context = parent_context.new(dict(c=3))\n        self.assertEqual(child_context['a'], 1)\n        self.assertEqual(child_context['b'], 2)\n        self.assertEqual(child_context['c'], 3)\n", "    def test_request_context_processor_ordering(self):\n        request = RequestFactory().get(\"/\", data={\"test_key\": \"test_value\"})\n        engine = Engine()\n        engine.template_context_processors = [lambda r: {\"engine_key\": \"engine_value\"}]\n        \n            return {\"custom_key\": \"custom_value\"}\n        \n        context = RequestContext(request, processors=[custom_processor], engine=engine)\n        self.assertEqual(context.get(\"test_key\"), \"test_value\")\n        self.assertEqual(context.get(\"engine_key\"), \"engine_value\")\n        self.assertEqual(context.get(\"custom_key\"), \"custom_value\")\n\n", "    def test_requestcontext_processors_override_previous_context(self):\n        rf = RequestFactory()\n        request = rf.get(\"/\")\n        context = {\"x\": 1}\n        template = Template(\"{{ x }}\")\n\n        # Create a context processor that updates x to 2\n            return {\"x\": 2}\n\n        with override_settings(TEMPLATE_CONTEXT_PROCESSORS=[processor]):\n            request_context = RequestContext(\n                request, {\"y\": 3}, processors=[lambda request: {\"y\": 4}]\n            )\n            rendered_content = template.render(request_context)\n\n        self.assertEqual(rendered_content, \"2\")\n\n        # Check that the processor updates didn't affect other variables\n        self.assertEqual(request_context.get(\"y\"), 4)\n\n", "    def test_make_context_overrides_processor_output(self):\n        request = RequestFactory().get(\"/\")\n        engine = Engine()\n        engine.template_context_processors = [\n            lambda r: {\"processed\": \"processor value\"},\n        ]\n        template = Template(\"{{ processed }}\")\n\n        # Create a RequestContext with default processors and a manual override\n        with override_settings(TEMPLATES=[{\"BACKEND\": \"django.template.backends.django.DjangoTemplates\", \"DIRS\": []}]):\n            context = RequestContext(request, {\"manual\": \"manual value\"})\n            with context.bind_template(template):\n                self.assertEqual(template.render(context), \"manual value\")\n"], "sample_542": ["    def test_annotation_offset(self, tmpdir):\n        fig, ax = plt.subplots()\n        ann = Annotation(\"This is an annotation\", (0.5, 0.5),\n                         xytext=(0.25, 0.75), xycoords='data',\n                         textcoords='offset points',\n                         annotation_clip=False)\n        ax.add_artist(ann)\n        fig.savefig(str(tmpdir.join('test.png')))\n", "    def test_annotation_xycoords_offset(self):\n        fig, ax = plt.subplots()\n        annotation = Annotation(\"Test\", (0.5, 0.5), xycoords='data',\n                               textcoords='offset pixels',\n                               arrowprops=dict(arrowstyle='->'))\n        annotation.set_position((0.5, 0.5) + (10, 10))\n        plt.draw()\n        assert annotation.get_window_extent().width == 10\n\n", "def test_annotation_arrow_style():\n    fig, ax = plt.subplots()\n    ann = Annotation(\"Test\", (0.5, 0.5), xytext=(0.2, 0.2),\n                       arrowprops=dict(arrowstyle='->',\n                                       connectionstyle='arc3',\n                                       shrinkA=10,\n                                       shrinkB=10))\n    ax.add_artist(ann)\n\n\n\n\n    plt.draw()\n    \n", "def test_annotation_offset_xy(fig):\n    ax = fig.add_subplot(111)\n\n    # Test simple offset\n    a = Annotation(\"test\", (0.5, 0.5), xycoords='data',\n                   xytext=(0.2, 0.2),\n                   textcoords='figure fraction',\n                   arrowprops={'arrowstyle': '->'})\n    ax.add_artist(a)\n\n    # Test offset with different units\n    a = Annotation(\"test\", (0.5, 0.5), xycoords='data',\n                   xytext=(0.2, 0.2),\n                   textcoords='figure pixels',\n                   arrowprops={'arrowstyle': '->'})\n    ax.add_artist(a)\n\n\n\n", "    def test_annotation_offset_coords(self):\n        fig, ax = plt.subplots()\n\n        # Test with 'offset points' - should be relative to the data point.\n        annotation = Annotation(\n            \"Test\", xy=(0.5, 0.5), xycoords='data',\n            xytext=(0.5, 0.5), textcoords='offset points',\n            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\"),\n            annotation_clip=None)\n\n        annotation.set_size(12)\n        ax.add_artist(annotation)\n\n        # Test with 'offset pixels' - should be relative to the figure\n        annotation = Annotation(\n            \"Test2\", xy=(0.5, 0.5), xycoords='data',\n            xytext=(0.5, 0.5), textcoords='offset pixels',\n            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\"),\n            annotation_clip=None)\n\n        annotation.set_size(12)\n        ax.add_artist(annotation)\n\n        plt.show()\n\n\n\n"], "sample_904": ["    def test_resolve_term_xref_case_insensitive(self):\n        env = mock.MagicMock(spec=restructuredtext.BuildEnvironment)\n        builder = mock.MagicMock(spec=restructuredtext.Builder)\n\n        domain = StandardDomain(env)\n        fromdocname = 'test'\n        node = pending_xref('', 'term', 'TERM')\n        contnode = nodes.emphasis('', '', '', '')\n        domain._terms['Term'] = ('test', 'myid')  # case-insensitive match\n\n        result = domain._resolve_term_xref(env, fromdocname, builder, 'ref', 'term', node, contnode)\n        assert result is not None\n        assert result.tagname == 'ref'\n        assert result['refdocname'] == 'test'\n        assert result['refuri'] == f'test#myid'\n\n", "    def test_resolve_option_xref_with_spaces(self):\n        with mock.patch('sphinx.domains.std.StandardDomain.progoptions') as mock_progoptions:\n            mock_progoptions.return_value = {\n                ('myprog', 'opt1 opt2'): ('doc1', 'labelid1'),\n            }\n            app = restructuredtext.App()\n            app.config.enable_parallel_read = False\n            env = app.builder.env\n            builder = mock.Mock(spec=app.builder)\n            builder.name = 'html'\n            builder.get_relative_uri = mock.Mock(return_value='relative_uri')\n            node = pending_xref(reftype='option', refdoc='myprog', reftarget='opt1 opt2')\n            refnode = StandardDomain().resolve_xref(env, 'doc0', builder, 'option', 'opt1 opt2', node, nodes.Element())\n            assert refnode.get('refuri') == 'relative_uri#labelid1'\n            assert isinstance(refnode, nodes.reference)\n            assert refnode['refid'] == 'labelid1'\n\n\n\n", "def test_resolve_ref_xref_explicit_section_title(app: 'Sphinx'):\n    domain = StandardDomain(app.env)\n    builder = mock.Mock()\n    # mock env and docname\n    from sphinx import Application\n    env = mock.Mock(spec=Application.env)\n    docname = 'test'\n    fromsphinx.testing import restructuredtext\n    from sphinx.testing.util import assert_node\n\n    # mock get_titles and get_doctree\n    env.titles = {'test': 'Test Title'}\n    env.get_doctree = mock.Mock(return_value=mock_doctree())\n\n    node = pending_xref(\n        reftarget='my-section',\n        refexplicit=True,\n    )\n    contnode = nodes.literal_block(text='hello')\n\n    result = domain._resolve_ref_xref(env, 'test', builder, 'ref', 'my-section', node, contnode)\n    assert_node(result, type=addnodes.anchor)\n    assert_node(result, 'refid', name='my-section')\n    assert_node(result, 'classes', ['std', 'ref'])\n    assert_node(result, 'children', [nodes.inline(text='hello', text='Test Title')])\n\n\n", "    def test_resolve_keyword(self, app: mock.Mock, builder: mock.Mock):\n        app.env = mock.Mock()\n        app.env.titles = {'doc1': 'Doc 1'}\n        app.env.doc2path = lambda docname: docname\n        app.env.all_docs = {'doc1'}\n\n        builder.get_relative_uri = mock.Mock(return_value=\"\")\n        domain = StandardDomain(app.env)\n        fromdocname = 'doc1'\n        builder = mock.Mock()\n        node = pending_xref()\n        node['refexplicit'] = True\n        node['reftarget'] = 'my_term'\n        contnode = nodes.paragraph()\n        node['reftype'] = 'keyword'\n        result = domain.resolve_keyword_xref(app.env, fromdocname,\n                                             builder, 'keyword', 'my_term', node, contnode)\n        assert_node(result, nodes.inline(text='my_term', classes=['std', 'keyword']))\n\n\n", "    def test_resolve_doc_xref_with_explicit_title(self):\n        app = mock.Mock(spec=restructuredtext.SphinxApp)\n        builder = mock.Mock(spec=restructuredtext.Builder)\n        env = mock.Mock(spec=restructuredtext.BuildEnvironment)\n        env.all_docs = {'some_doc': None}\n        env.titles['some_doc'] = 'Some Doc Title'\n\n        domain = StandardDomain(env)\n        docname = 'some_doc'\n        node = pending_xref('some_doc', 'refexplicit', 'Some Doc Title')\n        contnode = nodes.inline('', '')\n        result = domain.resolve_xref(env, 'main_doc', builder, 'ref', 'some_doc', node, contnode)\n        assert_node(result, nodes.reference, ids=['some_doc'], refuri='some_doc',\n                    text='Some Doc Title')\n"], "sample_255": ["    def test_get_environ(self):\n        request = self.request_factory.get('/')\n        handler = WSGIRequestHandler(Stub(request=request))\n        environ = handler.get_environ()\n        self.assertEqual(environ.get('HTTP_HOST'), '127.0.0.1:8000')\n", "    def test_log_message_https(self):\n        request = self.request_factory.get(\"/\")\n        request.headers['Protocol'] = 'HTTP/1.1'\n        request.headers['X-Forwarded-Proto'] = 'https'\n\n        handler = Stub(environ={'wsgi.input': BytesIO(b''), 'REQUEST_METHOD': 'GET'})\n        handler.log_message = lambda format, *args: None\n\n        handler.server = Stub(address_family=socket.AF_INET6)\n        handler.log_message(format, args)\n        self.assertIn(\"You're accessing the development server over HTTPS, but it only supports HTTP.\\n\", captured_stderr().getvalue())\n", "    def test_handle_broken_pipe(self):\n        # Test that broken pipe errors are handled gracefully\n        request = self.request_factory.get('/')\n        handler = WSGIRequestHandler(FakeRequest(request), Stub(), None)\n        handler.handle()\n\n", "    def test_get_environ(self):\n        request = self.request_factory.get('/')\n        handler = WSGIRequestHandler(\n            request,\n            Stub(headers={'_X-Test': 'test'}),\n        )\n        environ = handler.get_environ()\n        self.assertNotIn('_X-Test', environ)\n", "    def test_handle_close_connection(self):\n        request = self.request_factory.get('/')\n        handler = WSGIRequestHandler(\n            Stub(rfile=BytesIO(b'GET / HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n'),\n                 wfile=Stub(),\n                 request=request,),\n        )\n        handler.close_connection = True\n        with captured_stderr() as stderr:\n            handler.handle()\n        self.assertFalse(handler.close_connection)\n        self.assertEqual(stderr.getvalue(), '')\n\n\n\n"], "sample_1203": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_331": ["    def test_parse_datetime_with_postgres_interval(self):\n        self.assertEqual(parse_datetime(\"3 days 04:05:06\"), datetime.datetime(\n            year=1, month=1, day=3, hour=4, minute=5, second=6, tzinfo=get_fixed_timezone(0)\n        ))\n\n", "    def test_parse_duration_postgres_interval(self):\n        self.assertEqual(parse_duration('3 days 04:05:06'), timedelta(days=3, hours=4, minutes=5, seconds=6))\n        self.assertEqual(parse_duration('-3 days 04:05:06'), timedelta(days=-3, hours=4, minutes=5, seconds=6))\n        self.assertEqual(parse_duration('3 days 04:05:06.123456'), timedelta(days=3, hours=4, minutes=5, seconds=6, microseconds=123456))\n", "    def test_parse_datetime_with_postgres_interval(self):\n        dt = parse_datetime(\"3 days 04:05:06\")\n        self.assertEqual(dt.date(), date(2023, 10, 27))  # Test day offset\n        self.assertEqual(dt.time(), time(4, 5, 6))  # Test time part\n", "    def test_parse_datetime_with_postgres_interval(self):\n        self.assertEqual(\n            parse_datetime(\"3 days 04:05:06\"),\n            datetime(2023, 10, 26, 4, 5, 6, tzinfo=get_fixed_timezone(0)),\n        )\n", "    def test_parse_datetime_with_postgres_interval(self):\n        self.assertEqual(parse_datetime(\"3 days 04:05:06\"), datetime.datetime(\n            2023, 1, 1, 4, 5, 6, tzinfo=get_fixed_timezone(0)\n        ))\n"], "sample_269": ["    def test_set_language_no_redirect_on_get(self):\n        factory = RequestFactory()\n        request = factory.get(reverse('set_language'), {'lang': 'en'})\n        response = set_language(request)\n        self.assertEqual(response.status_code, 204)\n", "    def test_set_language_post_redirect_with_inactive_language(self):\n        inactive_language = self._get_inactive_language_code()\n        url = reverse('set_language')\n        factory = RequestFactory()\n        request = factory.post(url, data={'language': inactive_language, 'next': '/'})\n        response = set_language(request)\n        self.assertRedirects(response, '/')\n        self.assertContains(response, inactive_language)\n        self.assertEqual(get_language(), inactive_language)\n\n\n\n", "    def test_set_language_inactive_language(self):\n        inactive_language = self._get_inactive_language_code()\n        url = reverse('set_language')\n        response = self.client.post(url, data={\n            LANGUAGE_QUERY_PARAMETER: inactive_language,\n            'next': '/',\n        })\n        self.assertEqual(response.status_code, 302)\n        self.assertRedirects(response, '/')\n        self.assertEqual(get_language(), inactive_language)\n        self.assertTrue(self.client.cookies.get(settings.LANGUAGE_COOKIE_NAME).value == inactive_language)\n", "    def test_set_language_redirects_to_same_page_if_not_post(self):\n        url = reverse('set_language')\n        response = self.client.get(url, {'next': '/'})\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response['Location'], '/')\n\n        response = self.client.get(url, {'next': '/', 'language': self._get_inactive_language_code()})\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response['Location'], '/')\n", "    def test_set_language_redirects_to_same_page_if_no_next_url(self):\n        inactive_language = self._get_inactive_language_code()\n        url = reverse('set_language')\n        response = self.client.post(url, data={'language': inactive_language, 'next': url})\n        self.assertRedirects(response, url, 302, True)\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, inactive_language)\n\n        response = self.client.get(url, {'next': url})\n        self.assertEqual(response.status_code, 200)\n        self.assertNotContains(response, inactive_language)\n\n        response = self.client.post(url, data={'language': inactive_language, 'next': '/'})\n        self.assertRedirects(response, '/', 302, True)\n        response = self.client.get('/')\n        self.assertContains(response, inactive_language)\n"], "sample_779": ["    def __init__(self, a=0, b=['value']):\n        self.a = a\n        self.b = b\n", "    def __init__(self, p=0):\n        self.p = p\n", "    def __init__(self, a=1, b=\"value\", c):\n        self.a = a\n        self.b = b\n        self.c = c\n", "    def fit(self, X, y=None):\n        X, y = check_X_y(X, y)\n        return self\n", "    def __getattribute__(self, name):\n        if name == 'deprecated_attribute':\n            return \"Deprecated Attribute Value\"\n        return super().__getattribute__(name)\n"], "sample_378": ["    def test_bulk_update_with_null_values(self):\n        with self.subTest('update with nulls'):\n            self.notes[0].misc = None\n            self.notes[1].note = None\n            self.notes.save(update_fields=['note', 'misc'])\n            self.assertEqual(self.notes[0].misc, None)\n            self.assertEqual(self.notes[1].note, None)\n\n\n\n", "    def test_bulk_update_with_select_related(self):\n        self.create_tags()\n        note_ids = [n.id for n in self.notes]\n        for note in self.notes:\n            note.tags.add(self.tags[note.id % len(self.tags)])\n\n        with self.assertNumQueries(1):\n            Note.objects.filter(id__in=note_ids).bulk_update(\n                [{'note': F('note') + ' updated', 'misc': F('misc') + ' updated'}],\n                ['note', 'misc'],\n                select_related='tags'\n            )\n        for i, note in enumerate(self.notes):\n            self.assertEqual(note.note, f\"{note.note} updated\")\n            self.assertEqual(note.misc, f\"{note.misc} updated\")\n            self.assertEqual(note.tags.count(), 1)\n            self.assertEqual(note.tags.first().name, f\"{i}\")\n", "    def test_bulk_update_notes(self):\n        tags = self.create_tags()\n        tag_ids = [tag.id for tag in tags]\n\n        # Bulk update notes with tags.\n        Note.objects.bulk_update(\n            notes=[{\n                'note': 'Updated Note',\n                'misc': 'Updated Misc',\n                'tags': tag_ids[:3]\n            } for i in range(5)],\n            fields=['note', 'misc', 'tags'],\n        )\n\n        # Verify changes.\n        updated_notes = Note.objects.all()\n        self.assertEqual(len(updated_notes), 5)\n        for note in updated_notes:\n            self.assertEqual(note.note, 'Updated Note')\n            self.assertEqual(note.misc, 'Updated Misc')\n            self.assertEqual(set(note.tags.all().values_list('id', flat=True)), set(tag_ids[:3]))\n", "    def test_bulk_update_with_f_expressions(self):\n        self.create_tags()\n        # Update the notes with F expressions\n        Note.objects.bulk_update(\n            [{'note': F('note') + ' Updated', 'misc': F('misc') * 2},\n             {'note': F('note') + ' Updated', 'misc': F('misc') + 1}],\n            fields=['note', 'misc'],\n        )\n        updated_notes = Note.objects.all()\n        for i, note in enumerate(updated_notes):\n            # Validate the updated values\n            self.assertEqual(note.note, self.notes[i].note + ' Updated')\n            self.assertEqual(note.misc, self.notes[i].misc * 2)\n", "    def test_bulk_update_null_field(self):\n        Tag.objects.bulk_update(\n            self.tags, ['name'], update_fields=['name']\n        )\n        self.assertEqual(Tag.objects.all().count(), 10)\n        self.assertEqual(Tag.objects.filter(name=None).count(), 0)\n\n\n\n"], "sample_134": ["    def test_serializer_for_custom_operation(self):\n        from custom_migration_operations.more_operations import CustomOperation\n\n        class CustomOperationSerializer(BaseSerializer):\n                return \"CustomOperation\", []  \n\n        with mock.patch('django.db.migrations.serializer.Serializer._registry', new={CustomOperation: CustomOperationSerializer}):\n            op = CustomOperation()\n            serializer = serializer_factory(op)\n            self.assertEqual(serializer.serialize(), (\"CustomOperation\", []))\n\n\n", "    def test_custom_operations(self):\n        with mock.patch('custom_migration_operations.more_operations.MyOperation.deconstruct') as mock_deconstruct:\n            operation = custom_migration_operations.more_operations.MyOperation()\n            result = serializer_factory(operation).serialize()\n            mock_deconstruct.assert_called_once_with()\n            self.assertIn('custom_migration_operations.more_operations.MyOperation', result[0])\n", "    def test_serialize_re_pattern_flags(self):\n        regex = re.compile(r\"\\d+\", re.IGNORECASE | re.MULTILINE)\n        serializer = RegexSerializer(regex)\n        serialized_string, imports = serializer.serialize()\n        expected_string = \"re.compile(r'\\\\d+', re.IGNORECASE | re.MULTILINE)\"\n        self.assertEqual(serialized_string, expected_string)\n        self.assertEqual(imports, {'import re'})\n\n\n\n", "    def test_serialize_functools_partial(self):\n            return a + b + c\n        partial_func = functools.partial(func, 1, 2)\n        serializer = serializer_factory(partial_func)\n        serialized, imports = serializer.serialize()\n        self.assertEqual(\n            serialized,\n            'functools.partial(func, 1, 2)',\n            \"Incorrect serialization for functools.partial\"\n        )\n        self.assertEqual(imports, {'import functools'}, \"Incorrect imports\")\n\n\n\n", "    def test_serialize_functools_partial(self):\n        from functools import partial\n\n            return a + b\n\n        partial_func = partial(func, 1)\n\n        output = serializer_factory(partial_func).serialize()\n\n        expected_output = (\n            'functools.partial(func, 1)'\n        )\n        self.assertEqual(output[0], expected_output)\n\n\n\n"], "sample_211": ["    def test_customized_view(self):\n        view = CustomizableView()\n        view.parameter = {'test': 'value'}\n        request = self.rf.get('/')\n        response = view.get(request)\n        self.assertEqual(response.content, b'This is a simple view')\n", "    def test_redirect_view(self):\n        view = RedirectView.as_view(url='https://www.example.com/')\n        request = self.rf.get('/')\n        response = view(request)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response['Location'], 'https://www.example.com/')\n", "    def test_redirect_view_permanent(self):\n        redirect_view = RedirectView.as_view(permanent=True, url='/another/path/')\n        request = self.rf.get('/path/')\n        response = redirect_view(request)\n        self.assertRedirects(response, '/another/path/')\n\n\n", "    def test_redirect_view(self):\n        redirect_view = RedirectView.as_view(url='about/')\n        response = redirect_view(self.rf.get('/'))\n        self.assertRedirects(response, '/about/', status_code=302, target_status_code=None)\n\n        redirect_view = RedirectView.as_view(pattern_name='about')\n        response = redirect_view(self.rf.get('/'))\n        self.assertRedirects(response, '/about/', status_code=302, target_status_code=None)\n", "    def test_redirect_view(self):\n        redirect_view = RedirectView.as_view(url='https://www.example.com/')\n        request = self.rf.get('/')\n        response = redirect_view(request)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response['Location'], 'https://www.example.com/')\n"], "sample_1001": ["    def test_inverse_trig_style_power():\n        print(latex(InverseCosineTransform(x), inv_trig_style=\"power\"))\n        print(latex(InverseSineTransform(x), inv_trig_style=\"power\"))\n        print(latex(InverseTangentTransform(x), inv_trig_style=\"power\"))\n\n\n", "    def test_complex_roots(self):\n        x = Symbol('x')\n        expr = (x**2 + 1)\n        roots = (x - 1j, x + 1j)\n        self.assertEqual(latex(RootSum(expr, roots)), r\"\\left(x^{2} + 1\\right) = 0 \\Rightarrow \\left \\{ x = -i, x = i \\right }\")\n\n\n", "    def test_lowergamma(self):\n        self.assert_latex(lowergamma(x, y), r'\\Gamma_l(x, y)')\n", "def test_latex_with_custom_symbols():\n    expr = (Symbol('x_i')**2 + Symbol('y_j')) * (Symbol('a_k') + Symbol('b_l'))\n\n    expected_latex = r\"\\left(x_{i}^{2} + y_{j}\\right) \\left(a_{k} + b_{l}\\right)\"\n    latex_output = latex(expr, symbol_names={'x':'x_i', 'y':'y_j', 'a':'a_k', 'b':'b_l'})\n    assert latex_output == expected_latex\n", "    def test_totient_symbol(self):\n        self.assertEqual(latex(totient(2)), '\u03c3(2)')\n"], "sample_500": ["def test_colorbar_extension_length(self):\n    \"\"\"\n    Test colorbar extension length with different extendfrac values.\n    \"\"\"\n    for spacing in ['uniform', 'proportional']:\n        fig = _colorbar_extension_length(spacing)\n        plt.show()\n", "def test_colorbar_extension_length():\n    with rc_context(matplotlib.rcParams.copy()):\n        matplotlib.rcParams[\"axes.linewidth\"] = 1\n        fig = _colorbar_extension_length('uniform')\n        fig.show()\n\n\n", "def test_colorbar_extension_length(request):\n    # Test for colorbar extensions with different 'extendfrac' values.\n    for spacing in ('uniform', 'proportional'):\n        fig = _colorbar_extension_length(spacing)\n        if request.node.name == 'test_colorbar_extension_length':\n            plt.close(fig)\n\n\n\n", "def test_colorbar_extension_length():\n    for spacing in ('uniform', 'proportional'):\n        fig = _colorbar_extension_length(spacing)\n        plt.show()\n", "def test_colorbar_extendrect_shape():\n    fig, ax = plt.subplots()\n    im = ax.imshow(np.random.rand(5, 5))\n    cbar = plt.colorbar(im, ax=ax, extendrect=True)\n    cbar.set_label(\"Test Label\")\n    plt.axis('off')\n"], "sample_828": ["def test_pairwise_kernels_filter_params(metric):\n    with config_context({\"kernel_filter_params\": True}):\n        X = np.random.rand(10, 5)\n        with pytest.raises(ValueError):\n            pairwise_kernels(X, metric=metric, filter_params=True, gamma=1, degree=2, coef0=0.5)  \n", "    def test_euclidean_distances_with_precomputed_matrix(metric):\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        if metric == \"cosine\":\n            X = normalize(X, axis=1)\n        D = pairwise_distances(X, metric=metric, precomputed=True)\n        # Compare to scipy.distance\n        D_scipy = _euclidean_distances_upcast(\n            X, metric=metric\n        )  \n        assert_array_almost_equal(D, D_scipy)\n\n    ", "    def test_pairwise_distances_chunked_with_reduce_func(\n            X, Y, metric, expected):\n        with config_context({\"n_jobs\": 1}):\n            for D_chunk in pairwise_distances_chunked(\n                    X, Y, metric=metric, reduce_func=lambda *args: args[0]):\n                assert_array_almost_equal(D_chunk, expected)\n\n\n", "    def test_paired_distances_same_length_arrays(metric):\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        Y = np.array([[7, 8], [9, 10], [11, 12]])\n        result = paired_distances(X, Y, metric=metric)\n        expected = pairwise_distances(X, Y, metric=metric)\n        assert_array_almost_equal(result, expected.flatten())\n", "    def test_pairwise_distances_chunked_reduce_func(self, metric):\n        X = np.random.RandomState(0).rand(20, 5)\n        n_jobs = 2\n        chunk_size = 5\n        reduce_func = lambda D_chunk, start: np.sum(D_chunk, axis=1)\n        gen = pairwise_distances_chunked(X, reduce_func=reduce_func,\n                                        n_jobs=n_jobs, chunk_size=chunk_size)\n        for i in range(chunk_size):\n            chunk = next(gen)\n            assert chunk.shape[0] == chunk_size\n            assert np.allclose(chunk, np.sum(chunk, axis=1))\n\n\n"], "sample_1067": ["    def test_as_ordered_factors_with_functions(self):\n        x, y = symbols('x y')\n        expr = sin(x)*cos(y)*x*y\n        self.assertEqual(expr.as_ordered_factors(), [x, y, sin(x), cos(y)])\n", "    def test_as_content_primitive_non_rational():\n        from sympy import sqrt\n        assert  (5*sqrt(2)*cos(pi/4)).as_content_primitive() == (5, sqrt(2)*cos(pi/4))\n    ", "    def test_prod_special_cases():\n        assert prod([S(2)]) == 2\n        assert prod([S(2), S(3)]) == 6\n        assert prod([S(2), S(1/2)]) == 1\n        assert prod([], 10) == 10\n        assert prod([S(2), S(3), S(S(1/2))]) == 3\n        assert prod([S(2) , S(3), S(S(1/2))], 2) == 6\n\n\n\n\n\n", "    def test_as_content_primitive_sqrt(self):\n        from sympy import sqrt\n        self.assertEqual(sqrt(2).as_content_primitive(), (1, sqrt(2)))\n        self.assertEqual((2*sqrt(2) + sqrt(8)).as_content_primitive(), (2*sqrt(2), 1))\n        self.assertEqual((-3*sqrt(2)*(2 - 2*sqrt(2))).as_content_primitive(), (6, -sqrt(2)*(1 - sqrt(2))))", "    def test_nseries_special_cases():\n        x = Symbol('x')\n        assert (exp(x)).nseries(x, 0, 2).equals(1 + x + x**2/2)\n        assert (sin(x)).nseries(x, 0, 4).equals(x - x**3/6)\n        assert (cos(x)).nseries(x, 0, 4).equals(1 - x**2/2 + x**4/24)\n        assert (sqrt(1 + x)).nseries(x, 0, 2).equals(1 + x/2 - x**2/8)\n        assert (log(1 + x)).nseries(x, 0, 3).equals(x - x**2/2 + x**3/3)\n        assert (meijerg(1, 1, 1, 1, x)).nseries(x, 0, 2).equals(1 + x)\n\n"], "sample_875": ["compilation error", "def test_brier_score_loss_binary_string_labels():\n    y_true = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n    y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n    with pytest.raises(ValueError) as e:\n        brier_score_loss(y_true, y_prob)\n    assert \"The provided y_true contains string labels\" in str(e)\n\n    # Test with explicit pos_label\n    with pytest.raises(ValueError) as e:\n        brier_score_loss(y_true, y_prob, pos_label=\"spam\")\n    assert \"The provided y_true contains string labels\" in str(e)\n\n    brier_score_loss(y_true, y_prob, pos_label=\"ham\")\n", "    def test_hinge_loss(sample_weight, expected_loss):\n        y_true = np.array([-1, 1, 1])\n        pred_decision = np.array([[-2.18], [2.36], [0.09]])\n        loss = hinge_loss(y_true, pred_decision, sample_weight=sample_weight)\n        assert_almost_equal(loss, expected_loss)\n\n\n\n", "compilation error", "compilation error"], "sample_943": ["def test_exclude_files(apidoc):\n    # check that files inside excluded directories are not processed\n    assert not path(apidoc.outdir / 'test-root/utils/__init__.py').exists()\n", "def test_private_modules_excluded(apidoc):\n    coderoot, outdir = apidoc\n\n    assert not path(outdir / 'private/module.rst').exists()\n", "def test_private_module_exclusion(apidoc):\n    assert (apidoc.outdir / 'private_mod.rst').exists() is False\n", "def test_exclude_private_module(apidoc):\n    coderoot = apidoc.coderoot\n    outdir = apidoc.outdir\n\n    assert (\n        path(outdir / 'private_module.rst').exists() is False\n    ), f\"Expected module 'private_module.rst' not to be created in {outdir}\"\n", "    def test_dry_run(apidoc):\n        coderoot, outdir = apidoc\n        assert (outdir / 'modules.rst').exists() is False\n"], "sample_293": ["    def test_reverse_with_default_arguments(self):\n        with self.assertNoWarnings():\n            reverse('test1', args=['42'], kwargs={'arg1': '37'})\n            reverse('test2', args=['42', '37'], kwargs={'arg1': '37'})\n            reverse('test3', args=['42', '37'], kwargs={'arg1': '37', 'arg2': '38'})\n            reverse('test4', args=['42'], kwargs={'arg1': '37', 'arg2': '38'})\n", "    def test_no_urlconf(self):\n        with self.assertRaises(ImproperlyConfigured):\n            get_resolver()\n\n", "compilation error", "compilation error", "    def test_no_urlconf(self):\n        with self.assertRaises(ImproperlyConfigured):\n            reverse('nonexistent')\n"], "sample_49": ["    def test_media_from_multiwidget(self):\n        class MyMultiWidget(MultiWidget):\n                super().__init__((TextInput(), TextInput()))\n\n        widget = MyMultiWidget()\n        self.assertEqual(widget.media, Media(js=('js/script.js',)))\n\n\n", "    def test_checkbox_media(self):\n        widget = TextInput(attrs={'id': 'id_myfield', 'class': 'myclass'})\n        form = Form(\n            {'myfield': CharField()},\n            auto_id='myfield',\n            widget=widget,\n        )\n        self.assertEqual(form.media, Media(js=['/static/js/myfield.js']))\n        \n\n\n", "    def test_multiwidget_mediai(self):\n        class MyMultiWidget(MultiWidget):\n            inputs = (TextInput(), TextInput())\n            template_name = 'my_template.html'\n\n        form = Form(fields={'myfield': CharField()})\n        form.fields['myfield'].widget = MyMultiWidget()\n        media = form.media\n        self.assertIn('my_template.html', media.js)\n        self.assertEqual(media.css, [])\n\n", "    def test_media_from_multiwidget(self):\n        class MyMultiWidget(MultiWidget):\n                super().__init__(*args, **kwargs)\n                self.widget1 = TextInput(attrs={'class': 'widget1'})\n                self.widget2 = TextInput(attrs={'class': 'widget2'})\n                self.widgets = [self.widget1, self.widget2]\n\n                return value\n\n                return ' '.join(rendered_widgets)\n\n        form = Form(\n            ({'field1': MyMultiWidget()})\n        )\n        media = form.media\n\n        self.assertIn(\n            'widget1.css', media\n        )\n        self.assertIn(\n            'widget2.css', media\n        )\n\n\n\n", "    def test_split_hidden_datetime_widget_media(self):\n        widget = SplitHiddenDateTimeWidget()\n        media = widget.media\n        self.assertEqual(media, Media(css={'all': ['splitdatetime.css']}))\n"], "sample_679": ["    def test_mark_empty_parameters(self, attr, modulename):\n        mark = Mark(modulename, attr)\n        collector = Collector(\n            mark.parent,\n            None,\n            [],\n            None,\n            None,\n            None,\n            None,\n            None,\n            (),\n            [],\n            {},\n            False,\n            None,\n            None,\n            None,\n            EMPTY_PARAMETERSET_OPTION,\n        )\n        node = Node(\n            collector,\n            None,\n            None,\n            attr,\n            None,\n            None,\n            None,\n            None,\n            None,\n            None,\n            None,\n            None,\n            None,\n            None,\n            None,\n            None,\n            None,\n            None,\n            None,\n            None,\n        )\n        MarkEvaluator(node, attr).istrue()\n        assert not collector.outcome.passed\n\n\n\n", "    def test_istrue_empty_marks(self, modulename):\n        item = Node()\n        mark_evaluator = MarkEvaluator(item, \"test_mark\")\n        assert not mark_evaluator._istrue()\n", "    def test_invalidraise(self, request):\n        class MyItem(Node):\n                super().__init__()\n                self.name = name\n\n        item = MyItem(\"test\")\n        mark = Mark(\"mymark\", \"\")\n        mark.kwargs[\"raises\"] = Exception\n        item.add_marker(mark)\n\n        eval_obj = MarkEvaluator(item, \"mymark\")\n        with pytest.raises(Exception):\n            eval_obj.invalidraise(Exception)\n        with pytest.raises(AssertionError):\n            eval_obj.invalidraise(TypeError)\n\n", "    def test_get_raise_when_no_raises(self, attr, modulename):\n        mark = Mark(modulename, attr)\n        mark_evaluator = MarkEvaluator(Node(), mark.name)\n        assert mark_evaluator.invalidraise(None) is None\n", "    def test_mark_invalid_syntax(self, modulename: str) -> None:\n        with mock.patch(\n            f\"sys.{modulename.lower()}.exc_info\"\n        ) as exc_info_mock:\n            exc_info_mock.return_value = (\n                SyntaxError(\"invalid syntax\"),\n                \"invalid syntax\",\n                \"test_mark_invalid_syntax.py\",\n            )\n            mark = Mark(modulename)\n            mark.condition = \"invalid\"\n            test_object = Node(\n                name=\"test\", \n                parent=Collector(path=[], parent=None), \n                fspath=\"test_mark_invalid_syntax.py\"\n            )\n            mark_evaluator = MarkEvaluator(test_object, \"test\")\n            with pytest.raises(TEST_OUTCOME):\n                mark_evaluator._istrue()\n"], "sample_669": ["    def test_capture_manager_handles_duplicate_request_bindings(self, method):\n        manager = CaptureManager(method)\n        with manager.item_capture(None, None) as capture:\n            with manager.item_capture(None, None) as capture2:\n                assert capture is not capture2  # Different capture instances\n                # ... additional assertions to ensure they capture independently\n", "    def test_capture_not_on_global(self, request):\n        cm = CaptureManager(\"fd\")\n        with request.yield_fixture(\"capfdbinary\") as capfdbinary:\n            with cm.global_and_fixture_disabled():\n                capture.write_to_fd(1, b\"hello\")\n                capture.write_to_fd(2, b\"world\\n\")\n                with cm.item_capture(\"setup\", request.node) as item:\n                    pass\n                with cm.item_capture(\"call\", request.node) as item:\n                    pass\n                with cm.item_capture(\"teardown\", request.node) as item:\n                    pass\n            assert capfdbinary.readouterr().out == b\"hello\\n\"\n            assert capfdbinary.readouterr().err == b\"world\\n\"\n", "    def test_global_capture_setup_teardown(self, method: str):\n        if method == \"no\" :\n            return\n        with capture.CaptureManager(method) as cm:\n            cm.start_global_capturing()\n\n            # some code to run\n            print(\"stdout\")\n            sys.stderr.write(\"stderr\\n\")\n            \n            # make sure capture manager is properly configured\n            assert cm.is_capturing() == True\n            assert _get_multicapture(method).out.started == True\n\n            cm.stop_global_capturing()\n\n            # some code to run\n            print(\"stdout\")\n            sys.stderr.write(\"stderr\\n\")\n\n            # make sure capture manager is properly configured\n            assert cm.is_capturing() == False\n            assert _get_multicapture(method).out.stopped == True\n", "    def test_no_capture_handles_no_capture_methods(self, method):\n        capman = CaptureManager(method)\n        \n        out, err = capman.read_global_capture()\n\n        assert out == \"\"\n        assert err == \"\"\n", "    def test_global_and_fixture_disabled(self, method):\n        capman = CaptureManager(method)\n        with capman.global_and_fixture_disabled():\n            pytest.raises(getattr(capture, \"CaptureManager\" if method == \"no\" else \"MultiCapture\")._CaptureMethodError),\n            setattr(sys, 'stdout', io.StringIO())\n            out = sys.stdout.getvalue()\n            assert out == ''  \n            setattr(sys, 'stdout', capman._methods[method].EMPTY_BUFFER)\n            \n            \n"], "sample_603": ["def test_array_repr_with_attrs(dataarray):\n    html_repr = fh.array_repr(dataarray)\n\n    assert \"xr-attrs\" in html_repr\n    assert \"attrs\" in html_repr\n    assert \"data\" in html_repr\n\n\n\n", "def test_array_repr_with_attrs(dataarray):\n    dataarray.attrs = {\"a\": 1, \"b\": \"2\"}\n    repr_html = fh.array_repr(dataarray)\n    assert \"<dt><span>a :</span></dt>\" in repr_html\n    assert \"<dd>1</dd>\" in repr_html\n    assert \"<dt><span>b :</span></dt>\" in repr_html\n    assert \"<dd>2</dd>\" in repr_html\n\n\n\n", "def test_dataset_repr_with_attrs(dataset):\n    html = dataset_repr(dataset)\n    assert \"description: Test data.\" in html\n", "    def test_repr(obj, expected_repr):\n        assert isinstance(expected_repr, str) and expected_repr.strip() != \"\"\n        if LooseVersion(xr.__version__) > LooseVersion(\"0.15.2\"):\n            assert fh._obj_repr(obj, [], []).startswith(\"<div>\")\n\n\n", "    def test_array_repr_with_attrs(dataarray):\n        html = fh.array_repr(dataarray)\n        assert \"xarray.DataArray\" in html\n        assert dataarray.name in html\n        assert \"Attributes\" in html\n        assert \"Name: \" + dataarray.name in html\n"], "sample_767": ["    def test_remainder_transform(self):\n        X = np.array([[1, 2, 3], [4, 5, 6]])\n        X_sparse = sparse.csr_matrix([[1, 2, 3], [4, 5, 6]])\n\n        # Test remainder=passthrough\n        ct = ColumnTransformer(\n            transformers=[\n                (\"scaler\", StandardScaler(), [0]),\n                (\"passthrough\", Trans(), [1, 2])\n            ],\n            remainder='passthrough'\n        )\n        ct.fit(X)\n        X_trans = ct.transform(X)\n        X_trans_sparse = ct.transform(X_sparse)\n\n        expected_X_trans = np.array([\n            [1.0, 2, 3],\n            [1.0, 5, 6]\n        ])\n        expected_X_trans_sparse = sparse.csr_matrix([\n            [1.0, 2, 3],\n            [1.0, 5, 6]\n        ])\n\n        assert_allclose_dense_sparse(X_trans, expected_X_trans)\n        assert_allclose_dense_sparse(X_trans_sparse, expected_X_trans_sparse)\n\n        # Test remainder estimator\n        class Remainder(BaseEstimator):\n                return self\n\n                return X * 10\n\n        ct2 = ColumnTransformer(\n            transformers=[\n                (\"scaler\", StandardScaler(), [0])\n            ],\n            remainder=Remainder()\n        )\n        ct2.fit(X)\n        X_trans2 = ct2.transform(X)\n\n        expected_X_trans2 = np.array([\n            [10, 20, 30],\n            [40, 50, 60]\n        ])\n        assert_array_equal(X_trans2, expected_X_trans2)\n\n        # Test empty remainder\n        ct3 = ColumnTransformer(transformers=[\n            (\"scaler\", StandardScaler(), [0])\n        ], remainder='drop')\n        ct3.fit(X)\n        X_trans3 =", "    def test_remainder_as_estimator(self):\n        # Test remainder as an estimator, with a custom one\n        class CustomRemainder(BaseEstimator):\n                return self\n\n                return X + 1\n\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        ct = ColumnTransformer(\n            transformers=[\n                (\"scaler\", StandardScaler(), [0]),\n                (\"remainder\", CustomRemainder(), [1])\n            ],\n            remainder='custom_remainder'\n        )\n        ct.fit(X)\n        transformed_X = ct.transform(X)\n        expected_X = np.array([[1 + 1, 2 + 1], [3 + 1, 4 + 1], [5 + 1, 6 + 1]])\n        assert_allclose_dense_sparse(transformed_X, expected_X)\n", "    def test_remainder_as_estimator(self):\n        remainder_trans = Trans()\n        transformer_list = [\n            ('trans1', StandardScaler(), ['feature1', 'feature2']),\n            ('remainder', remainder_trans, slice(2, 4))\n        ]\n        ct = make_column_transformer(transformer_list, remainder='remainder')\n        X = np.array([[1, 2, 3, 4, 5],\n                      [6, 7, 8, 9, 10]])\n\n        X_transformed = ct.fit_transform(X)\n        assert_allclose_dense_sparse(\n            X_transformed[:, :2], remainder_trans.transform(X[:, slice(2, 4)]))\n\n\n", "    def test_remainder_transformer_fit_transform(self):\n        X = np.array([[1., float('nan')], [2., 3.], [4., 5.]])\n        y = np.array([0, 1, 2])\n\n        ct = make_column_transformer(\n            [(StandardScaler(), [0]),\n             (remainder_transformer, [1])],\n            remainder=remainder_transformer\n        )\n        ct.fit(X, y)\n        X_t = ct.transform(X)\n        assert X_t.shape[1] == 2\n        assert len(ct.named_transformers_) == 2\n", "    def test_remainder_is_estimator(self):\n        \"\"\"Test the behavior when remainder is an estimator.\"\"\"\n\n        # Create a dummy estimator\n        class MyRemainder(BaseEstimator):\n                return self\n\n                return 2 * X\n\n        X = np.array([[1, 2], [3, 4]])\n        transformer = make_column_transformer(\n            (StandardScaler(), [0]),\n            (MyRemainder(), [1])\n        )\n        X_transformed = transformer.fit_transform(X)\n\n        expected_X_transformed = np.array([[1.0, 4.0], [1.0, 4.0]])\n        assert_allclose_dense_sparse(X_transformed, expected_X_transformed)\n\n"], "sample_297": ["compilation error", "    def test_ordering_with_related(self):\n        with CaptureQueriesContext(connection) as query_context:\n            articles = Article.objects.select_related('author').order_by('author__name')\n            self.assertEqual(query_context.queries, [\n                \"SELECT t FROM django_core_models_article t JOIN django_core_models_author t__author ON t.author_id = t__author.id ORDER BY t__author.name\"\n            ])\n\n            articles = Article.objects.select_related('author').order_by('-author__name')\n            self.assertEqual(query_context.queries, [\n                \"SELECT t FROM django_core_models_article t JOIN django_core_models_author t__author ON t.author_id = t__author.id ORDER BY t__author.name DESC\"\n            ])\n            ", "compilation error", "compilation error", "    def test_related_object_with_fkey(self):\n        obj = ObjectA.objects.get(pk=1)\n        self.assertEqual(obj.related_object.name, 'one')\n\n"], "sample_937": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_488": ["    def test_unset_installed_apps_no_apps_ready(self):\n        with isolate_apps(\"apps.apps\"):\n            apps.set_installed_apps(SOME_INSTALLED_APPS)\n            self.assertTrue(apps.ready)\n            apps.unset_installed_apps()\n            self.assertFalse(apps.ready)\n", "    def test_set_installed_apps_unset_installed_apps(self):\n        with isolate_apps(list(SOME_INSTALLED_APPS_NAMES)):\n            try:\n                apps.set_installed_apps([\"some_app\", \"django.contrib.auth\"])\n                with self.assertRaises(AppRegistryNotReady):\n                    apps.check_apps_ready()\n            finally:\n                apps.unset_installed_apps()\n                apps.check_apps_ready()\n", "    def test_set_installed_apps_with_existing_app(self):\n        with isolate_apps([\"myapp\", \"django.contrib.admin\"]):\n            apps.set_installed_apps([\"myapp\", \"django.contrib.admin\"])\n            with self.assertRaises(AppRegistryNotReady):\n                apps.get_app_config(\"nonexistent\")\n\n\n\n", "    def test_lazy_model_operation(self):\n        models_called = []\n\n            models_called.extend([model1, model2, model3])\n\n        self.apps.lazy_model_operation(my_function, (\"one_config_app\", \"TotallyNormal\"), (\"totally_normal_app\", \"SoAlternative\"), (\"another_app\", \"OtherModel\"))\n\n        with self.assertRaises(LookupError):\n            self.apps.get_model(\"another_app\", \"OtherModel\")\n\n        # Ensure that the function was not called yet\n        self.assertEqual(models_called, [])\n\n        # Now try to retrieve the model\n        self.apps.get_model(\"one_config_app\", \"TotallyNormal\")\n        self.apps.get_model(\"totally_normal_app\", \"SoAlternative\")\n        self.apps.get_model(\"another_app\", \"OtherModel\")  # This will trigger the function call\n\n        # Ensure the function was called with the correct arguments\n        self.assertEqual(models_called, [TotallyNormal, SoAlternative, OtherModel])\n\n        # Create a new app and register a model\n        class MyConfig(AppConfig):\n            name = \"my_app\"\n            label = \"my_app\"\n\n        new_app = MyConfig()\n        self.apps.populate([new_app])\n        self.apps.register_model(\"my_app\", OtherModel)\n\n        # Ensure that the function was called again with the new model\n        models_called.clear()\n        self.apps.get_model(\"my_app\", \"OtherModel\")\n        self.assertEqual(models_called, [TotallyNormal, SoAlternative, OtherModel, OtherModel])\n\n", "    def test_lazy_model_operation(self):\n            self.assertIsInstance(model1, models.Model)\n            self.assertIsInstance(model2, models.Model)\n\n        apps.lazy_model_operation(check_model_exists, (\"myadmin.User\", \"myapp.Post\"))\n\n        # Wait for the models to be imported and registered.\n        apps.ready_event.wait()\n\n        with isolate_apps([\"myadmin\", \"myapp\"]):\n            with self.subTest(\"checking the existence of both models\"):\n                self.assertEqual(apps.get_model(\"myadmin\", \"User\"), models.get_model(\"myadmin\", \"User\"))\n                self.assertEqual(apps.get_model(\"myapp\", \"Post\"), models.get_model(\"myapp\", \"Post\"))\n"], "sample_769": ["def test_zero_one_loss_multiclass():\n    y_true = np.array([0, 1, 2, 0, 1, 2])\n    y_pred = np.array([0, 1, 2, 0, 2, 1])\n\n    loss = zero_one_loss(y_true, y_pred)\n    assert loss == 2 / 6\n\n", "    def test_accuracy_score_with_sample_weight():\n        y_true, y_pred, _ = make_prediction(binary=True)\n        sample_weight = np.random.rand(len(y_true))\n        score = accuracy_score(y_true, y_pred, sample_weight=sample_weight)\n        assert_almost_equal(score, accuracy_score(y_true, y_pred))\n        ", "    def test_classification_report_all_methods(average):\n        y_true, y_pred, _ = make_prediction()\n        labels = np.unique(y_true)\n        report = classification_report(y_true, y_pred, average=average, labels=labels)\n        print(report)\n\n\n", "def test_hinge_loss_multiclass_with_labels():\n    X = np.array([[0], [1], [2], [3]])\n    Y = np.array([0, 1, 2, 3])\n    labels = np.array([0, 1, 2, 3])\n    est = svm.LinearSVC()\n    est.fit(X, Y)\n    pred_decision = est.decision_function([[-1], [2], [3]])\n    y_true = [0, 2, 3]\n    loss = hinge_loss(y_true, pred_decision, labels)\n    assert loss < 1.5\n\n\n\n", "    def test_hinge_loss_multiclass_with_labels():\n        X = np.array([[0], [1], [2], [3]])\n        Y = np.array([0, 1, 2, 3])\n        labels = np.array([0, 1, 2, 3])\n        est = svm.LinearSVC()\n        est.fit(X, Y)\n        pred_decision = est.decision_function([[-1], [2], [3]])\n        y_true = [0, 2, 3]\n        loss = hinge_loss(y_true, pred_decision, labels)\n        assert loss > 0\n"], "sample_17": ["    def test_unique(self):\n        self.check(np.unique, self.q, return_index=True, return_inverse=True)\n", "    def test_reshape(self):\n        self.check(np.reshape)\n", "    def test_structured_to_unstructured_dtype(self, dtype):\n        arr = np.zeros(10, dtype=dtype)\n        u1 = u.m * u.s\n        u2 = u.m\n\n        # Create a structured array with a field that is of the given dtype.\n        arr = np.zeros(10, dtype=[\n            (\"a\", dtype),\n            (\"b\", \"float64\"),\n        ])\n        arr[\"a\"] = 1.0 * u1\n        arr[\"b\"] = 2.0 * u2\n\n        # Test that structured_to_unstructured works correctly.\n        result = structured_to_unstructured(arr, dtype=dtype)\n\n        # Check that the result is a numpy array with the correct dtype and units.\n        assert isinstance(result[0], np.ndarray) and result[0].dtype == dtype\n        assert result[0].shape == arr.shape\n        assert result[2] == arr.field(\"a\").unit\n\n\n\n        # Test with a nested structured array\n        arr2 = np.zeros(10, dtype=[\n            (\"a\", dtype),\n            (\"b\", \"float64\"),\n            (\"c\", np.dtype(\n                [\n                    (\"d\", \"float64\"),\n                    (\"e\", dtype),\n                ]\n            )),\n        ])\n        arr2[\"a\"] = 1.0 * u1\n        arr2[\"b\"] = 2.0 * u2\n        arr2[\"c\"][\"d\"] = 3.0\n        arr2[\"c\"][\"e\"] = 4.0 * u1\n\n        result2 = structured_to_unstructured(arr2, dtype=dtype)\n        assert isinstance(result2[0], np.ndarray) \n        assert result2[0].dtype == dtype\n\n", "    def test_shape_changing(self):\n        from astropy.units import m, s\n\n        # We know np.reshape changes the shape\n        a = np.arange(9.0).reshape(3, 3) / 4.0 * u.m * u.s\n        b = np.reshape(a, (9,))\n        assert b.shape == (9,)\n        assert b.unit == a.unit\n\n\n\n\n", "    def test_array_str(self):\n        self.check(array_str, formatter=None)\n        self.check(array_str, formatter=lambda x: \"%r\" % x) \n        self.check(array_str, formatter=lambda x: \"%.2f\" % x) \n        if NUMPY_LT_1_24:\n            self.check(array_str, formatter=lambda x: \"%.2e\" % x) \n\n\n\n"], "sample_974": ["compilation error", "compilation error", "    def test_matrix_element(self):\n        A = MatrixSymbol('A', 3, 3)\n        indices = Idx('i', 2)\n        mat = IndexedBase('B', shape=(3, 3))\n        expr = mat[indices, 1]\n        self.assertEqual(CCodePrinter()._print_MatrixElement(expr), 'B[i, 1]')\n        expr = A[1, 2]\n        self.assertEqual(CCodePrinter()._print_MatrixElement(expr), 'A[1, 2]')\n        expr = mat[indices, indices]\n        self.assertEqual(CCodePrinter()._print_MatrixElement(expr), 'B[i, i]')\n\n", "compilation error", "compilation error"], "sample_885": ["    def test_generate_valid_param_interval(self, interval_type):\n        int_interval = Interval(interval_type, 2, 5, closed=\"both\")\n        val = generate_valid_param(int_interval)\n        assert int_interval.is_satisfied_by(val)\n", "        def test_interval_constraint(interval_type):\n            # check that the constraint function returns true for valid values\n            for value in range(int(interval_type(0)) if interval_type is Integral else np.arange(0, None, 0.1)):\n                params = {\"a\": value}\n                _estimator = _Estimator(**params)\n                assert _estimator._parameter_constraints[\"a\"] is not None\n                _estimator._validate_params()\n\n            # check that invalid values raise InvalidParameterError\n            for value in [\n                -1,\n                float(\"inf\"),\n                np.nan,\n                interval_type(None) if interval_type is Integral else interval_type(10),\n            ]:\n                params = {\"a\": value}\n                with pytest.raises(InvalidParameterError):\n                    _Estimator(**params)\n", "        def test_generate_invalid_param_val_interval(interval_type):\n            test_interval = Interval(interval_type, 1, 2, closed=\"both\")\n            constraints = [test_interval] if interval_type is Real else [\n                Interval(Integral, 0, 10, closed=\"both\"),\n                test_interval,\n            ]\n            invalid_value = generate_invalid_param_val_interval(test_interval, constraints)\n            if interval_type is Real:\n                assert not (\n                    isinstance(invalid_value, Integral) and (1 <= invalid_value <= 2)\n                )\n                assert not test_interval.is_satisfied_by(invalid_value)\n            else:\n                assert invalid_value not in range(1, 3)\n                assert not test_interval.is_satisfied_by(invalid_value)\n", "        def test_interval_constraints(interval_type):\n            if interval_type is Real and interval_type.type in (Real, float):\n                with pytest.raises(TypeError) as e:\n                    Interval(interval_type, 1, 2, closed=\"both\", )\n                assert \"left and right bounds must be of type Real\" in str(e)\n            elif interval_type is not Integral:\n                with pytest.raises(TypeError) as e:\n                    Interval(interval_type, None, None, closed=\"both\", )\n                assert \"left and right bounds must be of type Real\" in str(e)\n            else:\n                for constraint in [\n                    Interval(interval_type, 1, 2, closed=\"both\"),\n                    Interval(interval_type, 1, None, closed=\"both\"),\n                    Interval(interval_type, None, 2, closed=\"both\"),\n                    Interval(interval_type, 1, 2, closed=\"left\"),\n                    Interval(interval_type, 1, 2, closed=\"right\"),\n                    Interval(interval_type, 1, 2, closed=\"neither\"),\n                ]:\n                    assert constraint.is_satisfied_by(1.5) is True\n                    assert constraint.is_satisfied_by(1) is False\n                    assert constraint.is_satisfied_by(2) is False\n\n\n", "        def test_generate_invalid_param_val_interval_int(interval_type):\n            constraints = [\n                Interval(interval_type, 1, 2, closed=\"both\"),\n            ]\n            # generate a value that doesn't satisfy the interval constraint\n            invalid_val = generate_invalid_param_val(constraints[0], constraints)\n            assert invalid_val not in interval_range(\n                constraints[0].left, constraints[0].right, constraints[0].closed\n            )\n"], "sample_676": ["def test__plugin_nameversions(input, expected):\n    assert _plugin_nameversions(input) == expected\n\n\n\n", "    def test_build_summary_stats_line_with_unknown_types(self, option):\n        stats = {\n            \"unknown\": 10,\n            \"failed\": 5,\n            \"passed\": 15,\n            \"warnings\": 2,\n        }\n        line, color = build_summary_stats_line(stats)\n        assert \"unknown\" in line\n        assert color == \"yellow\"\n\n\n", "    def test_build_summary_stats_line_unknown_types(self, option):\n        stats = {\n            \"passed\": [BaseReport()],\n            \"unknown_key\": [BaseReport()],\n        }\n        line, color = build_summary_stats_line(stats)\n        assert \"unknown_key\" in line, f\"Expected 'unknown_key' in: {line}\"\n\n", "    def test_plugin_nameversions_duplicate_project_names(self, option):\n        plugininfo = [\n            (\"pytest-a\", DistInfo(project_name=\"a\", version=\"1.0\")),\n            (\"pytest-a\", DistInfo(project_name=\"a\", version=\"1.1\")),\n            (\"pytest-b\", DistInfo(project_name=\"b\", version=\"2.0\")),\n        ]\n        expected = [\"a-1\", \"b-2\"]\n        assert _plugin_nameversions(plugininfo) == expected\n", "    def test__plugin_nameversions(input, expected):\n        assert _plugin_nameversions(input) == expected\n"], "sample_987": ["    def test_evalf_piecewise():\n        raises(NotImplementedError, lambda: N(Piecewise((x, x < 0), (x ** 2, True)), 3))\n\n", "    def test_evalf_complex_limit():\n        x = Symbol('x')\n        f = (exp(1j*x) - 1) / x\n        with workprec(100):\n            result = evalf(f.subs(x, S.Infinity), 10)\n        assert result[0] == 1j\n        assert abs(result[1]) < 1e-8\n\n\n", "    def test_evalf_hypergeometric_series():\n        from sympy.functions.special.hypergeometric import hyp2f1\n        f = hyp2f1(1/2, 1/2, 1, 1)\n        assert NS(f.evalf(10), 10) == '1.4142135624'\n        assert NS(hyp2f1(1/2, 1/2, 3/2, 1), 10) == '0.8862269255'\n\n\n\n", "    def test_prec_to_dps():\n        assert prec_to_dps(0) == 0\n        assert prec_to_dps(1) == 1\n        assert prec_to_dps(2) == 2\n        assert prec_to_dps(15) == 15\n        assert prec_to_dps(16) == 16\n        assert prec_to_dps(float('inf')) == float('inf')\n        assert prec_to_dps(None) == None", "    def test_evalf_hypergeometric():\n        from sympy.functions.special.hypergeometric import hypergeom\n        expr = hypergeom([1], [2], x)\n        res = NS(expr.evalf(10, subs={x: 0.5}), 10)\n        assert abs(res - 0.5) < 1e-8\n"], "sample_102": ["    def test_union_with_distinct(self):\n        qs1 = Number.objects.values_list('num')\n        qs2 = Number.objects.values_list('num').distinct()\n        qs_union = qs1.union(qs2)\n        self.assertNumbersEqual(qs_union, [x[0] for x in Number.objects.values_list('num')])\n", "    def test_select_related_with_join_fields(self):\n        with self.subTest('using outer join'):\n            queryset = Number.objects.select_related('reservation').all()\n            self.assertNumbersEqual(queryset, [n for n in Number.objects.all()])\n\n        with self.subTest('using inner join'):\n            queryset = Number.objects.select_related('reservation').filter(\n                reservation__pk__isnull=False\n            ).all()\n            \n            # Make sure we only get the numbers that have reservations\n            self.assertNumbersEqual(queryset, [\n                n for n in Number.objects.all() if n.reservation is not None\n            ])\n\n\n\n", "    def test_union_with_distinct(self):\n        numbers_distinct = Number.objects.distinct().filter(num__gt=5)\n        numbers_union = Number.objects.filter(num__gt=5).union(\n            Number.objects.filter(num__lt=5)\n        )\n        self.assertNumbersEqual(numbers_distinct, [6, 7, 8, 9], ordered=False)\n        self.assertNumbersEqual(numbers_union, list(range(10)), ordered=False)\n\n\n", "    def test_union_with_duplicate_fields(self):\n        num1 = Number.objects.filter(num=1)\n        num2 = Number.objects.filter(num=1)\n        combined = num1.union(num2)\n        self.assertNumbersEqual(combined, [1])\n\n\n", "    def test_union_with_distinct_and_order_by(self):\n        # Test that UNION with DISTINCT and ORDER BY works correctly.\n        # This ensures that duplicates are removed and the order is respected.\n\n        q1 = Number.objects.filter(num__gt=5).order_by('num')\n        q2 = Number.objects.filter(num__lt=5).order_by('-num')\n\n        combined_numbers = q1.union(q2).distinct().order_by('num')\n        self.assertNumbersEqual(combined_numbers, list(range(10)))\n"], "sample_971": ["    def test_is_suppressed_warning(self):\n        app = self.app\n        app.config.suppress_warnings = []\n        assert not is_suppressed_warning(None, None, app.config.suppress_warnings)\n\n        app.config.suppress_warnings = ['my_warning']\n        assert is_suppressed_warning('my_warning', None, app.config.suppress_warnings)\n        assert not is_suppressed_warning('other_warning', None, app.config.suppress_warnings)\n\n        app.config.suppress_warnings = ['my_warning.*']\n        assert is_suppressed_warning('my_warning.sub', None, app.config.suppress_warnings)\n        assert is_suppressed_warning('my_warning', None, app.config.suppress_warnings)\n        assert not is_suppressed_warning('other_warning', None, app.config.suppress_warnings)\n\n        app.config.suppress_warnings = ['my_warning', 'other_warning.*']\n        assert is_suppressed_warning('my_warning', None, app.config.suppress_warnings)\n        assert is_suppressed_warning('other_warning.sub', None, app.config.suppress_warnings)\n\n\n", "    def test_is_suppressed_warning(self, suppress_warnings: list) -> None:\n        app = ParallelTasks()\n        app.config.suppress_warnings = suppress_warnings\n        with logging.suppress_logging():\n            logging.getLogger(\"sphinx\").warning(\"my.module.warning\")\n\n        for warning_type in suppress_warnings:\n            assert is_suppressed_warning(\n                level=\"my.module.warning\",\n                subtype=None,\n                suppress_warnings=suppress_warnings,\n            )\n\n        assert is_suppressed_warning(\n            level=\"other.module.warning\",\n            subtype=None,\n            suppress_warnings=suppress_warnings,\n        ) == False\n\n\n\n", "    def test_prefixed_warnings(self) -> None:\n        with prefixed_warnings(\"prefix:\"):\n            logging.getLogger(__name__).warning(\"Warning message!\")\n\n        from sphinx.util.logging import getLogger\n        log = getLogger(__name__).buffer.clear()\n        assert len(log) == 1\n        assert log[0].msg.startswith(\"prefix: Warning message!\")\n\n\n\n", "    def test_skip_warningiserror_context(self) -> None:\n        with prefixed_warnings() as _:\n            with skip_warningiserror() as _ :\n                logging.getLogger('sphinx').warning('Warning message!')\n\n\n        with prefixed_warnings() as _ :\n            with skip_warningiserror(False) as _:\n                logging.getLogger('sphinx').warning('Warning message!')\n\n", "    def test_is_suppressed_warning(self):\n        app = ParallelTasks()\n        app.config.suppress_warnings = ['my_module']\n\n        with prefixed_warnings() as memhandler:\n            logging.getLogger('sphinx.util.logging').info('my_module:info message')\n            logging.getLogger('sphinx.util.logging').warning('my_module:warning message')\n            logging.getLogger('sphinx.util.logging').warning('other_module:warning message')\n\n\n\n        with pending_logging() as memhandler:\n            logging.getLogger('sphinx.util.logging').warning('my_module:warning message')\n            logging.getLogger('sphinx.util.logging').warning('other_module:warning message')\n\n        assert all(r.levelname == 'WARNING' for r in memhandler.clear())\n        assert is_suppressed_warning('my_module', None, app.config.suppress_warnings)\n        assert not is_suppressed_warning('other_module', None, app.config.suppress_warnings)\n\n\n\n"], "sample_729": ["    def test_multi_task_elastic_net_cv_l1_ratio_bounds(self):\n        X, y = load_boston(return_X_y=True)\n        X = check_array(X, dtype=np.float64, order='F')\n        y = check_array(y, dtype=np.float64)\n\n        with pytest.raises(ValueError) as e:\n            MultiTaskElasticNetCV(l1_ratio=[-0.1, 1.1])\n        assert \"l1_ratio should be in the range [0, 1]\" in str(e)\n", "    def test_preprocessing(self, normalize):\n        X = np.random.rand(10, 5)\n        y = np.random.rand(10, 2)\n        X_offset = np.random.rand(10, 5)\n        y_offset = np.random.rand(10, 2)\n        X_scale = np.random.rand(10, 5)\n\n        model = MultiTaskElasticNetCV(fit_intercept=True, normalize=normalize)\n        X, y, X_offset, y_offset, X_scale = \\\n            _preprocess_data(X, y, fit_intercept=True,\n                             normalize=normalize, copy=False)\n\n        with ignore_warnings(category=ConvergenceWarning):\n            model.fit(X, y)\n\n        model = MultiTaskElasticNet(fit_intercept=True, normalize=normalize)\n        model.fit(X, y)\n\n        assert_array_almost_equal(model.intercept_, model.intercept_)\n        assert_array_almost_equal(model.coef_, model.coef_)\n\n", "    def test_multi_task_lasso_cv_random_state(self):\n        np.random.seed(0)\n\n        X, y = load_boston(return_X_y=True)\n        X = check_array(X, copy=True)\n        y = check_array(y, copy=True)\n\n        # Use a subset of the data for faster testing\n        X = X[:100]\n        y = y[:100]\n\n        n_tasks = 2\n        y_multi = np.zeros((n_tasks, X.shape[0]))\n        y_multi[0] = y\n        y_multi[1] = y * 2\n\n        lasso_cv = MultiTaskLassoCV(\n            cv=5, random_state=0, n_jobs=-1\n        )\n        lasso_cv.fit(X, y_multi)\n        assert_equal(lasso_cv.n_iter_, 100)\n        \n\n        lasso_cv = MultiTaskLassoCV(\n            cv=5, random_state=1, n_jobs=-1\n        )\n        lasso_cv.fit(X, y_multi)\n        assert_equal(lasso_cv.n_iter_, 100)\n", "    def test_multi_task_elastic_net_cross_validation(self, alpha):\n        X, y = load_boston(return_X_y=True)\n        n_tasks = 2\n        y = np.array([y for _ in range(n_tasks)]).T\n        clf = MultiTaskElasticNetCV(l1_ratio=0.5, alpha=alpha)\n        clf.fit(X, y)\n        assert_greater(clf.alpha_, 0)\n        assert_array_almost_equal(clf.coef_.T, clf.coef_)\n        y_pred = clf.predict(X)\n        assert_array_equal(y_pred.shape, (X.shape[0], n_tasks))\n\n", "    def test_multi_task_elastic_net_cv_large_scale_random_state(self):\n        n_samples = 10000\n        n_features = 1000\n        n_tasks = 5\n        X = np.random.randn(n_samples, n_features)\n        y = np.random.randn(n_samples, n_tasks)\n        \n        clf = MultiTaskElasticNetCV(l1_ratio=0.5, random_state=42, cv=5)\n        clf.fit(X, y)\n        \n        with ignore_warnings(category=ConvergenceWarning):\n            clf_copy = deepcopy(clf)\n            assert_array_equal(clf.coef_, clf_copy.coef_)\n            assert_almost_equal(clf.alpha_, clf_copy.alpha_)\n"], "sample_775": ["    def test_gridsearchcv(self):\n        from sklearn.model_selection import GridSearchCV\n\n        # Define a simple pipeline\n        pipeline = make_pipeline(StandardScaler(), LogisticRegression())\n\n        # Create a GridSearchCV object with a few parameters\n        param_grid = {'logisticregression__C': [0.1, 1, 10]}\n        grid = GridSearchCV(pipeline, param_grid, cv=2)\n\n        # Test the pretty printing of the estimator\n        pp = _EstimatorPrettyPrinter(width=80, compact=False)\n        grid_repr = pp.pformat(grid)\n\n        # Check if the representation includes the pipeline and the grid parameters\n        self.assertIn('GridSearchCV(pipeline=', grid_repr)\n        self.assertIn('param_grid={\"logisticregression__C\": [0.1, 1, 10]},',\n                      grid_repr)\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_54": ["    def test_file_response_content_length_from_file(self):\n        with tempfile.NamedTemporaryFile('w+b') as temp_file:\n            temp_file.write(b\"This is some test data.\")\n            temp_file.flush()\n            response = FileResponse(temp_file, as_attachment=True)\n\n            self.assertEqual(response['Content-Length'], str(len(temp_file.read())))\n\n", "    def test_file_headers_with_content_length_from_getbuffer(self):\n        with tempfile.NamedTemporaryFile('w+b') as tmpfile:\n            tmpfile.write(b'Content')\n            tmpfile.seek(0)\n            response = FileResponse(tmpfile, as_attachment=True, filename='test.txt')\n            self.assertEqual(response['Content-Length'], 7)\n            self.assertEqual(response['Content-Disposition'], 'attachment; filename=\"test.txt\"')\n", "    def test_file_response_with_no_filename(self):\n        with tempfile.TemporaryFile('wb') as tmpfile:\n            tmpfile.write(b'test content')\n            tmpfile.seek(0)\n            response = FileResponse(tmpfile, as_attachment=False)\n            self.assertEqual('application/octet-stream', response['Content-Type'])\n            self.assertEqual(len(b'test content'), response['Content-Length'])\n            self.assertFalse('Content-Disposition' in response)\n", "    def test_file_response_with_no_filename(self):\n        with tempfile.TemporaryFile() as f:\n            f.write(b'Test content')\n            f.seek(0)\n            response = FileResponse(f)\n            self.assertEqual(response['Content-Type'], 'application/octet-stream')\n            self.assertEqual(response['Content-Disposition'], 'inline')\n            self.assertContains(response, b'Test content')\n", "    def test_file_response_with_block_size(self):\n        with tempfile.NamedTemporaryFile(\n            'w+b', delete=False, encoding=\"utf-8\"\n        ) as temp_file:\n            temp_file.write(b\"This is a test file.\")\n            temp_file.flush()\n            content_length = os.fstat(temp_file.fileno()).st_size\n            response = FileResponse(\n                temp_file, as_attachment=True, filename=\"test.txt\",\n                block_size=10\n            )\n            self.assertEqual(response['Content-Length'], content_length)\n            self.assertEqual(response['Content-Disposition'], 'attachment; filename=\"test.txt\"')\n            response_data = io.BytesIO()\n            for chunk in response:\n                response_data.write(chunk)\n            self.assertEqual(response_data.getvalue(), b\"This is a test file.\")\n\n\n"], "sample_1085": ["    def test_sympify_inf():\n        assert sympify(oo) is S.Infinity\n        assert sympify(-oo) is S.NegativeInfinity\n", "    def test_tribonacci_constant():\n        tc = TribonacciConstant\n        assert tc.is_irrational is None\n        assert tc > 1\n        assert tc < 2\n        assert tc.n(20) == 1.8392867552141611326\n", "    def test_tribonacci_constant():\n        assert isinstance(TribonacciConstant, Number)\n        assert TribonacciConstant.is_irrational is True\n        assert TribonacciConstant.is_algebraic is True\n        assert TribonacciConstant > 1.8\n", "    def test_comp():\n        a = sympify(1.234)\n        b = sympify(1.234)\n        assert comp(a, b) == 0\n        assert comp(a, a) == 0\n        assert comp(a, b + 1e-16) == -1\n        assert comp(a, b - 1e-16) == 1\n        assert comp(a, S.Infinity) == -1\n", "def test_sympify_mpmath_complex():\n    z = mpmath.mpc(1, 2)\n    assert sympify_mpmath(z) == 1 + 2*S.ImaginaryUnit\n\n\n\n"], "sample_715": ["    def test_cross_val_score_with_multiple_outputs(self):\n        from sklearn.datasets import make_regression\n        X, y = make_regression(n_samples=100, n_features=5, n_targets=2)\n        y_pred = np.zeros((100, 2))\n        estimator = CheckingClassifier()\n\n        # Test with different scoring to ensure scores are computed correctly\n        for scoring in ('accuracy', 'r2', 'neg_mean_squared_error'):\n            scores = cross_val_score(estimator, X, y, cv=5, scoring=scoring)\n            assert_true(scores.shape[0] == 5)\n\n            if scoring == 'accuracy':\n                assert_true(np.all(scores >= 0))\n                assert_true(np.all(scores <= 1))\n            elif scoring == 'r2':\n                assert_true(np.all(scores <= 1))\n            elif scoring == 'neg_mean_squared_error':\n                assert_true(np.all(scores >= 0))\n            else:\n                raise ValueError('Unexpected scoring')\n", "    def test_validation_curve_one_vs_rest(self):\n        X, y = make_classification(n_samples=100, n_features=20, random_state=42)\n        y_multilabel = np.array([\n            [True, False, False], [False, True, True],\n            [True, True, False], [False, False, True]\n        ])\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        X_train_multilabel = X_train.reshape(25, 1, -1)\n        X_test_multilabel = X_test.reshape(25, 1, -1)\n\n        estimator = OneVsRestClassifier(LogisticRegression())\n        param_name = 'C'\n        param_range = np.logspace(-3, 3, 5)\n\n        # Test without scoring\n        scores = validation_curve(estimator, X_train_multilabel, y_multilabel,\n                                  param_name, param_range, cv=3,\n                                  scoring=None)\n        assert_array_equal(scores[0].shape, (3, 5))\n        assert_array_equal(scores[1].shape, (3, 5))\n\n        # Test with scoring\n        scores = validation_curve(estimator, X_train_multilabel, y_multilabel,\n                                  param_name, param_range, cv=3,\n                                  scoring='accuracy')\n        assert_array_equal(scores[0].shape, (3, 5))\n        assert_array_equal(scores[1].shape, (3, 5))\n\n\n\n", "    def test_cross_val_score_with_sparse_data(self):\n        X_sparse = coo_matrix(np.random.rand(10, 5))\n        y = np.random.randint(0, 2, size=10)\n        clf = MockClassifier()\n        scores = cross_val_score(clf, X_sparse, y, cv=3)\n        assert_array_equal(scores.shape, (3,))\n        assert_true(np.all(scores >= 0))\n\n\n", "    def test_permutation_test_score_with_stratified_kfold(self):\n        # Test permutation_test_score with StratifiedKFold\n        X, y = make_classification(n_samples=200, n_features=20, n_informative=15,\n                                   n_redundant=5, random_state=42,\n                                   n_classes=3, class_sep=0.8)\n        y = np.where(y == 2, 1, y)  # Collapse classes 2 and 1\n        cv = StratifiedKFold(n_splits=5)\n        clf = MockImprovingEstimator(n_max_train_sizes=100)\n        score, _, _ = permutation_test_score(\n            clf, X, y, cv=cv, n_permutations=100,\n            scoring='accuracy', random_state=42)\n        assert_true(0.0 <= score <= 1.0)\n\n\n\n", "    def test_permutation_test_score(self):\n        X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n        clf = LogisticRegression()\n        perm_scores, _, _ = permutation_test_score(clf, X, y, scoring='accuracy',\n                                                  n_permutations=10)\n        assert_array_almost_equal(perm_scores, np.zeros(perm_scores.shape),\n                                  decimal=3)\n\n\n"], "sample_724": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_530": ["    def test_anchored_text_padding(self, image_comparison_data):\n        fig, ax = plt.subplots()\n        text = AnchoredText(\"Hello\", loc='lower right')\n        ax.add_artist(text)\n        fig.canvas.draw()\n        image_comparison_data.append(image_comparison(\n            \"anchored_text_padding\",\n            func=lambda: fig,\n            tol=0.05\n        ))\n", "    def test_offsetbox_padding(self):\n        fig, ax = plt.subplots()\n        bbox = AnchoredOffsetbox(\n            loc='upper left',\n            child=mpatches.Rectangle((0, 0), 1, 1, color='red'),\n            pad=0.3, borderpad=0.2,\n            frameon=True,\n        )\n        ax.add_artist(bbox)\n\n        bbox_props = bbox.get_window_extent()\n        expected_width = 1 + 2 * 0.3\n        expected_height = 1 + 2 * 0.3\n        assert_allclose(bbox_props.width, expected_width)\n        assert_allclose(bbox_props.height, expected_height)\n", "    def test_bbox_from_bounds(self):\n        bbox = Bbox.from_bounds(0, 0, 1, 1)\n        assert_allclose(bbox.bounds, [0, 0, 1, 1])\n\n        bbox = Bbox.from_bounds(0.5, 0.5, 0.2, 0.3, transform=plt.gca().transData)\n        # Test that the bounds are transformed correctly\n\n        expected_bbox = Bbox.from_bounds(0.5, 0.5, 0.2, 0.3)\n        assert_allclose(bbox.bounds, expected_bbox.bounds)\n\n\n\n", "    def test_offset_coords(self):\n        fig, ax = plt.subplots()\n        bbox = OffsetBox(\n            DrawingArea((0, 0, 1, 1)),\n            artist=patches.Circle((0.5, 0.5), 0.2, color='red'),\n        )\n        bbox_anchored = AnchoredOffsetbox(loc='upper left', child=bbox,\n                                         pad=0.1)\n        ax.add_artist(bbox_anchored)\n\n        bbox_transform = bbox_anchored.get_transform()\n        bbox_coords = bbox_transform.transform(bbox.get_bounds())\n        expected_bbox_coords = (0.1, 0.1, 0.6, 0.6)\n        assert_allclose(bbox_coords, expected_bbox_coords)\n\n\n", "    def test_annotationbbox_arrowprops(self):\n\n        fig, ax = plt.subplots()\n        offbbox = AnchoredOffsetbox(Loc.LOWER_LEFT,\n                                   child=mpatches.Rectangle((0, 0), 0.5, 0.5))\n        ann = AnnotationBbox(offbbox, (0.5, 0.5), arrowprops=dict(arrowstyle=\"->\", facecolor=\"red\",\n                                                                 mutation_scale=1.5))\n        ax.add_artist(ann)\n\n        rect = offbbox.get_window_extent(ax.figure._get_renderer())\n        assert rect.width > 0\n        assert rect.height > 0\n\n        # Arrow should be drawn\n        arrow = ann.arrow_patch  # Get the arrow instance\n        assert arrow is not None\n        assert arrow.arrowstyle == \"->\"\n        assert arrow.get_facecolor() == 'red'\n        assert arrow.mutation_scale == 1.5\n\n        plt.close(fig)\n\n"], "sample_786": ["def test_inverse_transform_ordinal(strategy):\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy=strategy)\n    est.fit(X)\n    Xt = est.transform(X)\n    Xinv = est.inverse_transform(Xt)\n    assert_array_equal(Xinv, Xt + 0.5)\n", "def test_inverse_transform_bin_edges(strategy):\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy=strategy)\n    est.fit(np.array(X))\n\n    Xt = est.transform(np.array(X))\n    Xinv = est.inverse_transform(Xt)\n\n    for jj in range(Xinv.shape[1]):\n        bin_edges = est.bin_edges_[jj]\n        for i in range(Xt.shape[0]):\n            # Check if the binned value is mapped correctly\n            # to the original value\n            assert Xinv[i, jj] in bin_edges.tolist()\n\n", "def test_inverse_transform_ordinal(strategy):\n    discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy=strategy)\n    discretizer.fit(X)\n    Xt = discretizer.transform(X)\n    Xinv = discretizer.inverse_transform(Xt)\n    if strategy == 'uniform':\n        assert_array_equal(Xinv, [[0.5, 2.5, -3.5, -0.5],\n                                   [-0.5, 2.5, -2.5, -0.5],\n                                   [0.5, 3.5, -1.5, 0.5],\n                                   [0.5, 3.5, -1.5, 1.5]])\n    elif strategy in ('kmeans', 'quantile'):\n        assert_array_equal(Xinv, [[0., 2., -3., -1.],\n                                   [-1., 2., -3., -0.5],\n                                   [0., 3., -2., 0.5],\n                                   [1., 4., -1., 2.]])\n", "def test_inverse_transform_ordinal():\n    discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n    discretizer.fit(X)\n    Xt = discretizer.transform(X)\n    Xinv = discretizer.inverse_transform(Xt)\n    assert_array_equal(Xinv, Xt)\n", "def test_transform_inverse_transform_round_trip(strategy):\n    X = np.array(X)\n    kb = KBinsDiscretizer(n_bins=4, encode='ordinal', strategy=strategy)\n    kb.fit(X)\n    Xt = kb.transform(X)\n    Xinv = kb.inverse_transform(Xt)\n    assert_array_equal(X, Xinv)\n"], "sample_127": ["    def test_bulk_create_with_fk(self):\n        connection.autocommit = True\n        from django.db import models\n        from django.core.exceptions import ValidationError\n        Country.objects.all().delete()\n        with self.assertRaises(IntegrityError):\n            models.Manager.bulk_create(self.data)\n\n\n\n", "    def test_bulk_create_with_auto_increment(self):\n        Country._meta.pk = self.model_field_set = ModelField('id', primary_key=True, db_column='my_id', default=None)\n        with override_settings(ATOMIC_REQUESTS=False):\n            Country.objects.bulk_create(self.data)\n        self.assertEqual(Country.objects.count(), 4)\n        last_id = Country.objects.order_by('id').last().id\n        self.assertEqual(last_id, 4)\n\n        Country._meta.pk = self.model_field_set = ModelField('id', primary_key=True, db_column='id', default=None)\n        \n", "    def test_bulk_create_with_related_fields(self):\n        pizzerias = [\n            Pizzeria(name=\"Domino's\", country=self.data[0]),\n            Pizzeria(name=\"Pizza Hut\", country=self.data[1]),\n            Pizzeria(name=\"Papa John's\", country=self.data[2]),\n        ]\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT 1\")  # Ensure we have a connection\n        connection.close()\n\n\n\n        with connection.cursor() as cursor:\n            Pizzeria.objects.bulk_create(pizzerias)\n        Pizzeria.objects.all().delete()\n        \n\n", "    def test_bulk_create_with_router(self):\n        router_country = Country.objects.using('routerdb').create(name='Router Country')\n        router_country_id = router_country.id\n\n        with connection.cursor() as cursor:\n            cursor.execute(\"ALTER TABLE django_country AUTO_INCREMENT = %s\", [router_country_id + 1])\n\n        with self.assertRaises(IntegrityError):\n            Country.objects.bulk_create(\n                [Country(name='Another Country', iso_two_letter='AN'),\n                 Country(name='And Another Country', iso_two_letter='AN')]\n            )\n\n\n\n", "    def test_bulk_create_with_relations(self):\n        pizzerias = [\n            Pizzeria(name=\"Pizza Planet\", country=self.data[0],\n                     address=\"123 Main St, Anytown\"),\n            Pizzeria(name=\"Pizza Palace\", country=self.data[1],\n                     address=\"456 Oak Ave, Somecity\"),\n            Pizzeria(name=\"Das Pizza Haus\", country=self.data[2],\n                     address=\"789 Birch Ln, Anytown\"),\n        ]\n        with connection.schema_editor() as editor:\n            editor.bulk_create(pizzerias)\n        # Assert that the related countries are correctly associated.\n        loaded_pizzerias = Pizzeria.objects.all()\n        for i, pizzeria in enumerate(loaded_pizzerias):\n            self.assertEqual(pizzeria.country.name, self.data[i].name)\n\n\n\n"], "sample_330": ["    def test_typecast_timestamp(self):\n        self.assertEqual(typecast_timestamp('2023-04-12'), datetime.datetime(2023, 4, 12, 0, 0, 0))\n        self.assertEqual(typecast_timestamp('2023-04-12 10:30:00'), datetime.datetime(2023, 4, 12, 10, 30, 0))\n        self.assertEqual(typecast_timestamp('2023-04-12 10:30:00.123456'), datetime.datetime(2023, 4, 12, 10, 30, 0, 123456))\n        self.assertEqual(typecast_timestamp('2023-04-12 10:30:00.1234'), datetime.datetime(2023, 4, 12, 10, 30, 0, 123400))\n        self.assertEqual(typecast_timestamp('2023-04-12 10:30:00+05:00'), datetime.datetime(2023, 4, 12, 10, 30, 0))\n        self.assertEqual(typecast_timestamp(None), None)\n\n\n", "    def test_typecast_timestamp(self):\n        self.assertEqual(\n            typecast_timestamp(\"2005-07-29 15:48:00.590358-05\"),\n            datetime.datetime(2005, 7, 29, 15, 48, 0, 590358),\n        )\n        self.assertEqual(typecast_timestamp(\"2005-07-29 09:56:00-05\"), datetime.datetime(2005, 7, 29, 9, 56, 0))\n        self.assertEqual(typecast_timestamp(None), None)\n        self.assertEqual(typecast_timestamp(\"\"), None)\n\n\n\n", "    def test_date_conversion_to_string(self):\n        for date_object in [None, datetime.date(2023, 10, 26), datetime.date(1900, 1, 1), datetime.date(2000, 12, 31)]:\n            self.assertEqual(typecast_date(date_object.isoformat()), date_object)\n        self.assertEqual(typecast_date('2023-10-26'), datetime.date(2023, 10, 26))\n        self.assertEqual(typecast_date('   '), None)\n        self.assertEqual(typecast_date(''), None)\n        self.assertEqual(typecast_date('invalid'), None)\n\n\n\n\n", "    def test_typecast_date(self):\n        self.assertEqual(typecast_date('2023-10-26'), datetime.date(2023, 10, 26))\n        self.assertEqual(typecast_date('1970-01-01'), datetime.date(1970, 1, 1))\n        self.assertEqual(typecast_date('0001-01-01'), datetime.date(1, 1, 1))\n        self.assertEqual(typecast_date('  '), None)\n        self.assertEqual(typecast_date('abc'), None)\n", "    def test_format_number(self):\n        self.assertEqual(format_number(None, 10, 2), None)\n        self.assertEqual(format_number(1234.56789, 10, 2), '1234.57')\n        self.assertEqual(format_number(1234.56789, 5, 2), '1234.57')\n        self.assertEqual(format_number(1234.56789, 10, 3), '1234.568')\n        self.assertEqual(format_number(1234.56789, 10, -2), '1234.57')\n"], "sample_1047": ["    def test_inconsistent_assumptions():\n        x = Symbol('x')\n        with raises(InconsistentAssumptions):\n            x.is_real = True\n            x.is_imaginary = True\n", "    def test_inconsistent_assumptions():\n        x = Symbol('x')\n        with raises(InconsistentAssumptions):\n            x.is_integer = True\n            x.is_rational = False\n\n\n", "    def test_inconsistent_assumptions():\n        x = Symbol('x')\n        with raises(InconsistentAssumptions):\n            x = Symbol('x', real=True, integer=True)\n", "    def test_inconsistent_assumptions():\n        x = Symbol('x', real=True, integer=True)\n        with raises(InconsistentAssumptions):\n            x.is_even\n", "    def test_mod_assumptions():\n        x = Mod(2, 3)\n        assert x.is_integer is True\n        assert x.is_rational is True\n        assert x.is_real is True\n        assert x.is_complex is False\n        assert x.is_odd is False\n        assert x.is_even is True\n        assert x.is_prime is False\n        assert x.is_composite is False\n        assert x.is_nonzero is True\n        assert x.is_finite is True\n        assert x.is_nonpositive is False\n        assert x.is_nonnegative is True\n        assert x.is_positive is False\n        assert x.is_negative is False\n        assert x.is_irrational is False\n        assert x.is_algebraic is True\n        assert x.is_transcendental is False\n\n\n\n        # Check that assumptions are propagated\n        y = Mod(2, 5)\n        assert y.is_integer is True\n        assert y.is_rational is True\n        assert y.is_real is True\n        assert y.is_complex is False\n        assert y.is_odd is False\n        assert y.is_even is True\n        assert y.is_prime is False\n        assert y.is_composite is False\n        assert y.is_nonzero is True\n        assert y.is_finite is True\n        assert y.is_nonpositive is False\n        assert y.is_nonnegative is True\n        assert y.is_positive is False\n        assert y.is_negative is False\n        assert y.is_irrational is False\n        assert y.is_algebraic is True\n        assert y.is_transcendental is False\n"], "sample_60": ["    def test_media_inline_formset(self):\n        response = self.client.get(reverse('admin:media_episode_change', args=(self.episode_pk,)))\n        self.assertContains(response, 'Media (Tabular Inline)')\n        formsets = response.context['formsets']\n        self.assertEqual(len(formsets), 1)\n        self.assertEqual(formsets[0].form.fields, {\n            'url',\n            'content_type_id',\n            'content_object_id',\n            'content_object_type_id',\n            'DELETION_FIELD_NAME',\n        })\n", "    def test_media_inline_formset(self):\n        response = self.client.get(reverse('admin:media_episode_media_add', args=(self.episode_pk,)))\n        self.assertContains(response, 'Media', count=1)\n        self.assertContains(response, 'Add media')\n\n        # Check that the formset has two slots for media entries\n        self.assertContains(response, 'input type=\"submit\" name=\"save\" value=\"Save\"')\n        self.assertContains(response, 'input type=\"submit\" name=\"save_and_continue\" value=\"Save and continue\"')\n        self.assertContains(response, '<tr class=\"row1\">', count=2)\n        self.assertContains(response, '<tr class=\"row2\">', count=2)\n        self.assertContains(response, '<input type=\"text\" name=\"form-TOTAL_FORMS\"', count=2)\n\n        # Check that the formset includes a delete button for each media\n        self.assertContains(response, 'input type=\"checkbox\" name=\"DELETE\"')\n        self.assertContains(response, 'a href=\"/admin/media/media/delete/' + str(self.mp3_media_pk) + '/?next=/admin/episode/episode/{}/edit/'.format(self.episode_pk))\n        self.assertContains(response, 'a href=\"/admin/media/media/delete/' + str(self.png_media_pk) + '/?next=/admin/episode/episode/{}/edit/'.format(self.episode_pk))\n\n        formsets = response.context['formsets']\n\n        self.assertEqual(len(formsets), 1)\n        self.assertEqual(formsets[0].total_forms, 2)\n        self.assertEqual(formsets[0].initial_forms, 2)\n\n\n\n", "    def test_add_view_with_inline_forms(self):\n        url = reverse('admin:episode_add')\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        # Check if the form for adding a new Episode and Media object are displayed\n        self.assertIn('name', response.content)\n        self.assertIn('url', response.content)\n        \n\n", "    def test_inline_formset_renders_as_expected(self):\n        response = self.client.get(reverse('admin:episode_episode_add'))\n        self.assertIn('admin/edit_inline/stacked.html', response.content)\n        self.assertIn('Inline form for EpisodeMedia', response.content)\n        self.assertNotIn('DELETE', response.content)\n\n", "    def test_media_inlines(self):\n        response = self.client.get(reverse('admin:episode_episode_change', args=(self.episode_pk,)))\n        self.assertIn('<input type=\"text\" name=\"form-MEDIA-0-url\">', response.content.decode())\n        self.assertIn('<input type=\"text\" name=\"form-MEDIA-1-url\">', response.content.decode())\n        \n        # Test deleting media inline\n        response = self.client.post(reverse('admin:episode_episode_change', args=(self.episode_pk,)), {\n            'form-MEDIA-0-DELETE': 'on',\n        })\n        self.assertRedirects(response, reverse('admin:episode_episode_change', args=(self.episode_pk,)))\n        self.assertFalse(Media.objects.filter(pk=self.mp3_media_pk).exists())\n\n\n\n"], "sample_25": ["    def test_long_commentary_card(self):\n        comment = \"This is a very long comment that goes on and on and on\" * 20\n\n        # Create a commentary card with a long comment\n        long_comment_card = fits.Card(\n            \"COMMENT\", comment, length=Card._length\n        )\n\n        # Verify that the card was formatted correctly\n        self.assertEqual(long_comment_card._format_long_commentary_image(), comment)\n\n\n\n", "    def test_continued_card(self):\n        # The value of a card can be continued onto multiple cards if it\n        # exceeds 80 characters.\n\n\n        self.image = (\n            \"SIMPLE = T\\n\"\n            \"COMMENT This is a test to see if continued cards work\\n\"\n            \"CONTINUE  'This is a continuation of the previous card'\\n\"\n            \"CONTINUE  'with more text to keep it over 80 characters'\\n\"\n            \"END\"\n        )\n        hdu = fits.open(StringIO(self.image))[0]\n        self.assertEqual(hdu.header[\"COMMENT\"], \"This is a test to see if continued cards work\")\n        self.assertEqual(\n            hdu.header[\"COMMENT\"].strip(), \"This is a test to see if continued cards work\"\n        )\n", "    def test_multiple_continue_cards(self):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", AstropyUserWarning)\n            hdu = fits.PrimaryHDU()\n            hdu.header.add_card(\"KEYWORD\", \"VALUE\", \"T\")\n            hdu.header.add_card(\n                \"CONTINUE\", \"VALUE CONTINUED\", \"T\"\n            )\n            hdu.header.add_card(\"CONTINUE\", \"VALUE CONTINUED MORE\", \"T\")\n            hdu.header.add_card(\"KEYWORD2\", \"VALUE2\", \"T\")\n            hdu.header.verify(\n                \"ignore\",\n                ignore_warnings=True,\n                output=True,\n            )\n            \n\n", "    def test_format_value_bool(self, keyword, value, expected_output):\n        card = Card(keyword, value)\n        assert card._format_value() == expected_output\n\n\n", "    def test_long_commentary_cards(self):\n        h = fits.Header()\n        h.add_comment(\"This is a very long comment that \"\n                      f\"spans multiple lines and exceeds the {Card.length} \"\n                      \"character limit.\")\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", AstropyUserWarning)\n            h.verify(\"fix\")\n\n        assert h[\"COMMENT\"].value.startswith(\"'\") and h[\"COMMENT\"].value.endswith(\"'\")\n\n\n\n"], "sample_19": ["    def test_maps(self):\n        for filename in self._file_list:\n            with ctx_for_v71_dateref_warnings():\n                with io.open(get_pkg_data_filename(\"data/maps\", filename), \"r\") as f:\n                    header = fits.Header.fromstring(f.read())\n                wcs = wcs.WCS(header)\n                # Check that the WCS is valid\n                with np.errstate(divide='ignore', invalid='ignore'):\n                    wcs.wcs.set()\n                # Add further tests as needed\n                # Here's an example for checking pixel coordinates\n                # For this example, we need a reference pixel coordinate\n                # in RA/Dec format. It should be something like\n                # (12.3456, 45.6789)\n                ref_coord = SkyCoord(ra=12.3456*u.deg, dec=45.6789*u.deg)\n                pix_coord = wcs.world_to_pixel(ref_coord)\n\n                assert isinstance(pix_coord, tuple) and len(pix_coord) == 2\n                assert all(np.isfinite(x) for x in pix_coord)\n\n                # ... add more tests for other functionalities  ...\n\n\n\n", "    def test_wcs_from_fits(self, tmp_path):\n        for filename in self._file_list:\n            with pytest.warns(AstropyDeprecationWarning, match=\"wcslib\"):\n                with open(\n                    get_pkg_data_filename(\"data/maps\", filename), \"rb\"\n                ) as f:\n                    hdu = fits.open(f)\n                wcs = wcs.WCS(hdu[0].header)\n                assert wcs.wcs is not None\n\n                ", "    def test_wcs_from_header(self, use_header_as_wcs):\n        hdr_fname = get_pkg_data_filename(\n            \"data/maps/simple_wcs.hdr\"\n        )\n        header = fits.Header.fromtext(get_pkg_data_contents(hdr_fname))\n        with ctx_for_v71_dateref_warnings():\n            wcs = wcs.WCS(header, use_header_as_wcs=use_header_as_wcs)\n\n        assert wcs is not None\n        assert wcs.naxis == 2\n", "    def test_validate_maps(self, filename):\n        with get_pkg_data_contents(\"data/maps\", filename) as fdata:\n            hdulist = fits.open(io.BytesIO(fdata))\n            results = validate(hdulist)\n            assert len(results) > 0\n            for hdu_result in results:\n                assert len(hdu_result) > 0\n\n\n\n", "    def test_map_wcs_from_file(self):\n        with ctx_for_v71_dateref_warnings():\n            for filename in self._file_list:\n                with get_pkg_data_contents(filename) as file:\n                    header = fits.Header.fromstring(file.read())\n                    wcs = wcs.WCS(header)\n                    # Test basic functionality\n                    assert wcs.naxis == len(header[\"NAXIS\"])\n                    assert all(isinstance(wcs.wcs.cdelt[i], u.Unit) for i in range(wcs.naxis))\n                    assert all(isinstance(wcs.wcs.crval[i], u.Quantity) for i in range(wcs.naxis))\n                    # Test axis names\n                    for i in range(wcs.naxis):\n                        assert wcs.wcs.cname[i] == header[f\"CTYPE{i+1}\"]\n                    # Test pixel scaling\n                    if \"PC\" in header:\n                        for i in range(wcs.naxis):\n                            for j in range(wcs.naxis):\n                                pc = header[f\"PC{i+1}_{j+1}\"]\n                                assert_allclose(\n                                    wcs.pixel_scale_matrix[i, j], pc, rtol=1e-8, atol=1e-8\n                                )\n                    else:\n                        assert wcs.wcs.pc is None\n\n\n"], "sample_231": ["    def test_callable_setting_wrapper(self):\n            return 'mocked value'\n        wrapped = CallableSettingWrapper(callable_setting)\n        self.assertEqual(repr(wrapped), '<module \"your_module_name\" object at 0x...>')\n        self.assertEqual(wrapped(), 'mocked value')\n\n\n\n", "    def test_callable_setting_wrapper_wraps_callable(self):\n            return 'hello'\n        wrapped = CallableSettingWrapper(callable_setting)\n        self.assertIs(callable(wrapped), True)\n        self.assertEqual(wrapped(), 'hello')\n", "    def test_callable_setting_wrapper(self):\n        wrapped_callable = CallableSettingWrapper(lambda x: x + 1)\n        self.assertIsInstance(wrapped_callable, CallableSettingWrapper)\n        self.assertNotEqual(wrapped_callable.__repr__(), '<function <lambda> at 0x...>'.format(hex(id(wrapped_callable))))\n        self.assertEqual(wrapped_callable(5), 6)\n\n", "    def test_callable_wrapper_calls_callable_with_attributes(self):\n            return self.attributes  \n        \n        attributes = {'test': 'value'}\n        wrapped_callable = CallableSettingWrapper(callable_function)\n        self.assertNotEqual(wrapped_callable.__call__(), callable_function())\n\n        with self.assertLogs('django.test', level='WARNING') as cm:\n            result = wrapped_callable()\n        self.assertIn(\n            'CallableSettingWrapper'\n            ' does not support calling the wrapped callable with '\n            'attributes', cm.output[0].msg\n        )\n\n", "    def test_callable_setting_wrapper(self):\n        # Test that the wrapper doesn't call callable settings.\n        foo = lambda: 1  # Callable function\n        with override_settings(MY_SETTING=foo):\n            wrapper = CallableSettingWrapper(settings.MY_SETTING)\n            self.assertIsNotNone(wrapper)\n            self.assertNotEqual(wrapper, foo)\n            self.assertEqual(wrapper(), 1)\n\n\n\n"], "sample_625": ["    def test_unify_chunks_basic(array_type):\n        x = array_type(\n            np.random.random(size=(10, 10)),\n            dims=[\"lat\", \"lon\"],\n            chunks={\"lat\": (5, 5), \"lon\": (5, 5)},\n        )\n        y = array_type(\n            np.random.random(size=(5, 5)),\n            dims=[\"lat\", \"lon\"],\n            chunks={\"lat\": (5, 5), \"lon\": (5, 5)},\n        )\n        result = unify_chunks(x, y)\n        assert isinstance(result[0], array_type)\n        assert isinstance(result[1], array_type)\n        assert result[0].chunks == {\"lat\": (5, 5), \"lon\": (5, 5)}\n        assert result[1].chunks == {\"lat\": (5, 5), \"lon\": (5, 5)}\n\n\n\n", "def test_unify_chunks_dask_arrays(\n    array_like1: xr.DataArray, array_like2: xr.DataArray", "    def test_unify_chunks_multiple_arrays(input: tuple[DataArray, DataArray]):\n        a, b = input\n        result = unify_chunks(a, b)\n        assert_identical(result[0], a)  # unchanged\n        assert_identical(result[1], b)  # unchanged\n\n", "    def test_unify_chunks(input_data, expected_result):\n        result = unify_chunks(input_data, input_data)\n        assert_identical(result[0], expected_result)\n        assert_identical(result[1], expected_result)\n", "    def test_unify_chunks_simple(obj1, obj2, expected):\n        result = unify_chunks(obj1, obj2)\n        assert result == expected\n\n"], "sample_1144": ["compilation error", "compilation error", "compilation error", "    def test_requires_partial_no_free_symbols():\n        x = symbols('x')\n        expr = x**2\n        assert not requires_partial(expr)\n", "compilation error"], "sample_1099": ["    def test_partial_derivative_product_multiple_variables(self):\n        i, j, k, m, m1, m2, m3, m4 = tensor_indices(\"i j k m m1 m2 m3 m4\", L)\n        A = TensorHead(\"A\", [L])\n        B = TensorHead(\"B\", [L])\n        expr = PartialDerivative(A(i)*B(j), (i, j))\n        result = expr.doit()\n        expected = PartialDerivative(B(j), j)*A(i) + PartialDerivative(A(i), i)*B(j)\n        self.assertEqual(result, expected)\n", "    def test_partial_derivative_product_multiple_variables(self):\n        A = Tensor(H, (i, j))\n        B = Tensor(D, (j, k))\n        C = Tensor(A, (k, m))\n\n        expr = PartialDerivative(C(i, j, k), (i, j))\n        with raises(TypeError):\n            expr.doit()\n", "    def test_partial_derivative_contraction(self):\n        x, y = symbols('x y')\n        A = TensorHead('A', [L])\n        expr = PartialDerivative(A(i), A(j))\n        expr = expr.doit()\n        assert expr.get_indices() == [i, -j]\n        \n        expr = PartialDerivative(A(i), A(i))\n        expr = expr.doit()\n        assert expr.get_indices() == [L_0, -L_0]\n\n        expr = PartialDerivative(A(i), A(j))\n        expr = expr.doit()\n        assert expr.get_indices() == [i, -j]\n        \n        expr = PartialDerivative(A(i), A(i + 1))\n        expr = expr.doit()\n        assert expr.get_indices() == [i, -(i + 1)]\n\n", "    def test_partial_derivative_product(self):\n        expr = (A(i) * B(j)).expand()\n        pd = PartialDerivative(expr, A(i))\n        result = pd.doit().expand()\n        expected = (B(j) * (A(i).diff(A(i)) * 1))\n        assert result == expected\n", "    def test_partial_derivative_product_multiple_indices(self):\n        x, y, z = symbols(\"x y z\")\n        A = Tensor(H, [i, j])\n        B = Tensor(H, [j, k])\n        expr = PartialDerivative(A(i, j) * B(j, k), (x, y, z))\n        result = expr.doit().doit()\n        expected = PartialDerivative(Tensor(H, [i, k])(i, k), (x, y, z)) * B(j, k)\n        self.assertEqual(result, expected)\n"], "sample_888": ["    def test_contamination_and_score_samples(\n        self,\n        contamination,\n        fit_params,\n        expected_score_shape,", "def test_contamination_parameter():\n    n_estimators = 100\n    n_samples = 300\n    random_state = check_random_state(0)\n    X, y = make_classification(n_samples=n_samples, random_state=random_state)\n    \n    for contamination in [0.1, 0.2, 0.5]:\n        clf = IsolationForest(contamination=contamination, random_state=random_state)\n        clf.fit(X)\n        \n        # Check that labels are correctly classified as inliers or outliers\n        y_pred = clf.predict(X)\n        inlier_mask = y_pred == 1\n        outlier_mask = y_pred == -1\n        \n        # Check that the number of outliers roughly matches the contamination\n        assert np.isclose(\n            np.sum(outlier_mask) / X.shape[0], contamination, tol=0.1\n        )\n\n        # Check predictions with different contamination parameters\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", category=UserWarning)\n            clf2 = IsolationForest(contamination='auto', random_state=random_state)\n            clf2.fit(X)\n            y_pred2 = clf2.predict(X)\n\n        assert_array_equal(y_pred, y_pred2)\n\n\n\n", "    def test_contamination_offset(self, contamination, expected_offset):\n        # Check that the offset is correctly defined based on the contamination\n        # specified.\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            rf = IsolationForest(contamination=contamination, random_state=42)\n            X, _ = train_test_split(iris.data, test_size=0.2, random_state=42)\n            rf.fit(X)\n\n        assert_allclose(rf.offset_, expected_offset, rtol=1e-5)\n\n\n", "    def test_contamination_auto(self):\n        X, y = make_classification(n_samples=1000, random_state=42)\n        # Train model with contamination='auto'\n        clf = IsolationForest(contamination=\"auto\", random_state=42)\n        clf.fit(X)\n\n        # Check decision function (sign)\n        decision_function = clf.decision_function(X)\n        self.assertTrue(\n            np.all(decision_function < 0)\n        )  # Expect all samples be classified as outliers (neg. decision)\n\n        # Train model with contamination=0.1 for comparison and check it\n        # produces similar results to contamination='auto'\n        clf_01 = IsolationForest(contamination=0.1, random_state=42)\n        clf_01.fit(X)\n\n        decision_01 = clf_01.decision_function(X)\n        assert_allclose(decision_function, decision_01, rtol=1e-3)\n", "    def test_fit_predict_with_sparse_data(\n        self, X_dtype, y_dtype, random_state"], "sample_632": ["    def test_ignore_imports(self):\n        linter = PyLinter()\n        linter.init_from_args([\"--disable=all\", f\"--ignore-imports\"])\n        with StringIO() as output:\n            with redirect_stdout(output):\n                linter.file(HIDE_CODE_WITH_IMPORTS)\n        output_lines = output.getvalue().splitlines()\n        assert \"similar-code\" not in output_lines\n\n", "    def test_ignore_comments(self):\n        with open(SIMILAR1, \"r\") as f:\n            code1 = f.read()\n        with open(SIMILAR2, \"r\") as f:\n            code2 = f.read()\n        linter = PyLinter(\n            config={\"disable\": [\"R0801\"], \"min_similarity_lines\": 2},\n            plugins=[similar],\n        )\n        linter.parse(\n            [\n                (\"test.py\", code1, \"utf-8\"),  \n                (\"test.py\", code2, \"utf-8\"), \n            ],\n        )\n        reporter = Reporter()\n        linter.run([\"test.py\"], reporter)\n        assert reporter.messages == []   \n", "    def test_similar_ignore_imports(self):\n        with open(HIDE_CODE_WITH_IMPORTS, \"r\") as f:\n            code = f.read()\n            linter = PyLinter()\n            linter.set_option(\"ignore-imports\", True)\n            with redirect_stdout(StringIO()) as output:\n                linter.check(source_code=code)\n            assert (\n                \"R0801: Similar lines in 1 files\" not in output.getvalue()\n            )\n", "    def test_similar_empty_functions(tmpdir):\n        tmp_dir = tmpdir.mkdir(\"test_similar\")\n\n        (tmp_dir / \"empty_func_1.py\").write(open(EMPTY_FUNCTION_1).read())\n        (tmp_dir / \"empty_func_2.py\").write(open(EMPTY_FUNCTION_2).read())\n\n        linter = PyLinter(\n            options={\"similar\": {\"ignore-comments\": True}},\n            messages=similar.MSGS,\n            reporter=Reporter,\n        )\n        stream = StringIO()\n        with redirect_stdout(stream):\n            linter.check(str(tmp_dir))\n        output = stream.getvalue().strip()\n        assert \"R0801\" not in output\n\n", "    def test_empty_functions(self):\n        with StringIO() as output:\n            with redirect_stdout(output):\n                linter = PyLinter()\n                linter.register_checker(similar.SimilarChecker(linter))\n                linter.check([EMPTY_FUNCTION_1, EMPTY_FUNCTION_2])\n            self.assertEqual(output.getvalue(), \"\")\n"], "sample_332": ["    def test_formset_factory_with_absolute_max(self):\n        with self.assertRaises(ValueError):\n            formset_factory(Choice, max_num=100, absolute_max=99)\n", "    def test_formset_factory_validation(self):\n        with self.assertRaises(ValueError):\n            formset_factory(\n                Choice, \n                max_num=100, \n                absolute_max=50\n            )\n        ", "    def test_can_delete_extra(self):\n        formset = FavoriteDrinksFormSet(\n            data={\n                'choices-0-name': 'Coke',\n                'choices-1-name': 'Pepsi',\n                'choices-2-name': 'Sprite',\n                'TOTAL_FORMS': '3',\n                'INITIAL_FORMS': '3',\n                'MAX_NUM_FORMS': '3',\n                'MIN_NUM_FORMS': '3',\n                'choices-2-DELETE': 'on',\n            }\n        )\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(len(formset.deleted_forms), 1)\n", "    def test_formset_factory_raises_valueerror(self):\n        with self.assertRaises(ValueError):\n            formset_factory(Choice, absolute_max=10, max_num=100)\n\n\n\n", "    def test_management_form_errors(self):\n        formset = ChoiceFormSet(\n            data={'TOTAL_FORMS': '1', 'INITIAL_FORMS': '1'},\n        )\n        self.assertEqual(formset.errors.as_ul(), '')\n\n        formset = ChoiceFormSet(data={'TOTAL_FORMS': '1', 'INITIAL_FORMS': '1', 'choices-0-choice': 'a', 'choices-0-votes': '1'})\n        self.assertEqual(formset.errors.as_ul(), '')\n\n        formset = ChoiceFormSet(data={'TOTAL_FORMS': '1', 'INITIAL_FORMS': '1', 'choices-0-choice': 'a', 'choices-0-votes': 'aa'})\n        self.assertTrue(formset.errors)\n        self.assertEqual(formset.errors.as_ul(), 'choices-0-votes: <ul><li>This field contains an invalid value.</li></ul>')\n\n        formset = ChoiceFormSet(\n            data={'TOTAL_FORMS': '1', 'INITIAL_FORMS': '1', 'choices-0-choice': 'a', 'choices-0-votes': '1', 'choices-1-choice': 'b', 'choices-1-votes': '2'},\n        )\n        self.assertEqual(formset.errors.as_ul(), '')\n\n        formset = ChoiceFormSet(\n            data={\n                'TOTAL_FORMS': '1', 'INITIAL_FORMS': '1',\n                'choices-0-choice': 'a', 'choices-0-votes': '1',\n                'choices-1-choice': 'a', 'choices-1-votes': '2',\n            },\n        )\n        self.assertTrue(formset.errors)\n\n\n"], "sample_1166": ["    def test_monomial_pow_negative_exponent(self):\n        with raises(ValueError):\n            Monomial([1, 2, 3]).__pow__(-1)\n", "    def test_monomial_lcm_mul_divides(self):\n        m1 = Monomial((2, 3, 4))\n        m2 = Monomial((1, 2, 1))\n\n        self.assertEqual(m1.lcm(m2), Monomial((2, 3, 4)))\n\n        self.assertEqual(m1.gcd(m2), Monomial((1, 2, 1)))\n\n        self.assertTrue(m1.divides(m2))\n", "    def test_monomial_lcm_dividing_by_zero(self):\n        with raises(ExactQuotientFailed):\n            Monomial((1, 2, 3)).__truediv__(Monomial((0, 0, 0)))\n", "    def test_monomial_divides_large():\n        M1 = Monomial([1, 2, 3])\n        M2 = Monomial([0, 1, 0])\n\n        assert monomial_divides(M1.exponents, M2.exponents) is True\n        \n        M3 = Monomial([1, 1, 1])\n\n        assert monomial_divides(M1.exponents, M3.exponents) is False\n", "    def test_itermonomials_with_max_degrees(self):\n        for n in range(1, 5):\n            for max_degree in range(1, n + 1):\n                for min_degree in range(0, max_degree + 1):\n                    for variables in [(i for i in range(n))]:\n                        for monom in itermonomials(variables, max_degree, min_degree):\n                            self.assertTrue(monomial_deg(monom) >= min_degree)\n                            self.assertTrue(monomial_deg(monom) <= max_degree)\n    "], "sample_165": ["    def test_form_error_messages_for_choice_field_with_custom_message(self):\n        class MyChoiceFieldForm(Form):\n            field = ChoiceField(\n                choices=[(1, 'One'), (2, 'Two')],\n                error_messages={'invalid_choice': 'This is a custom error message.'},\n            )\n        form = MyChoiceFieldForm({'field': '3'})\n        self.assertFormErrors([{'field': ['This is a custom error message.']}], form.clean)\n        \n", "    def test_modelform_defines_fields(self):\n        self.assertTrue(modelform_defines_fields(ChoiceForm))\n        self.assertFalse(modelform_defines_fields(Form))\n\n", "    def test_modelformset_factory(self):\n        class MyModel(models.Model):\n            name = models.CharField(max_length=100)\n\n        class MyModelForm(ModelForm):\n            class Meta:\n                model = MyModel\n                fields = ['name']\n\n        FormSet = modelformset_factory(MyModel, MyModelForm)\n        formset = FormSet(data=[])\n        self.assertEqual(formset.form.fields['name'].widget.attrs.get('placeholder'), 'Enter the name')\n        self.assertEqual(len(formset.forms), 0)\n", "compilation error", "compilation error"], "sample_339": ["    def test_modelformset_factory_can_handle_blank_objects(self):\n        class BlankModelForm(forms.ModelForm):\n            class Meta:\n                model = Book\n        formset = modelformset_factory(\n            Book, form=BlankModelForm, can_delete=False, extra=2\n        )\n\n        # Create a formset with blank instances\n        formset = formset(data=None)\n\n        # Ensure that the blank instances are valid\n        self.assertEqual(formset.is_valid(), True)\n\n        # Ensure that the blank instances have the correct initial values\n        for form in formset.forms:\n            self.assertTrue(form.instance is None)\n\n\n\n", "    def test_inlineformset_factory_can_delete_extra(self):\n        class AddressForm(forms.ModelForm):\n            class Meta:\n                model = Location\n                fields = '__all__'\n\n        class PersonWithAddresses(Person):\n            addresses = models.ManyToManyField(Location, blank=True)\n            \n        AddressFormSet = inlineformset_factory(PersonWithAddresses, Location,\n                                              form=AddressForm,\n                                              can_delete_extra=True)\n\n        person = PersonWithAddresses.objects.create(name='John Doe')\n        formset = AddressFormSet(instance=person)\n\n        # Add some addresses\n        for i in range(2):\n            formset.forms[i].initial['name'] = f'Address {i+1}'\n            formset.forms[i].save()\n        formset.save()\n\n        # Check that we can delete extra addresses\n        formset = AddressFormSet(instance=person)\n        formset.extra_forms.update({'DELETE': ''})\n        self.assertTrue(formset.can_delete)\n        formset.save()\n\n        self.assertEqual(Location.objects.filter(person=person).count(), 2)\n", "    def test_delete_object(self):\n        book = BookWithCustomPK.objects.create(title=\"New Book\", price=10.00)\n        form = BookWithCustomPKForm(instance=book)\n        form.fields['title'].initial = \"Deleted Book\"\n        form.save()\n        self.assertEqual(BookWithCustomPK.objects.count(), 0)\n", "    def test_delete_related_object(self):\n        \"\"\"\n        Tests deleting a related object through a form.\n\n        This test checks that deleting a related object through a formset\n        also deletes the related object from the database.\n        \"\"\"\n        owner = Owner.objects.create(name='John Doe')\n        restaurant = Restaurant.objects.create(name='My Restaurant', owner=owner)\n\n        form = MexicanRestaurantFormSet(\n            data={\n                'restaurants': [\n                    {'name': restaurant.name, 'delete': True},\n                ]\n            }\n        )\n        form.is_valid()\n        form.save()\n\n        self.assertFalse(Restaurant.objects.filter(name='My Restaurant').exists())\n\n        owner = Owner.objects.create(name='Jane Doe')\n        restaurant = Restaurant.objects.create(name='Jane\\'s Restaurant', owner=owner)\n\n        form = MexicanRestaurantFormSet(\n            data={\n                'restaurants': [\n                    {'name': restaurant.name, 'pk': restaurant.pk, 'delete': True},\n                ]\n            }\n        )\n        form.is_valid()\n        form.save()\n\n        self.assertFalse(Restaurant.objects.filter(name='Jane\\'s Restaurant').exists())\n\n\n", "    def test_formset_delete_validation(self):\n        author = Author.objects.create(name='Test Author')\n        author_meeting = AuthorMeeting.objects.create(author=author)\n\n        class AuthorMeetingFormSet(BaseModelFormSet):\n                return False\n\n                super().clean()\n                for form in self.forms:\n                    if form.cleaned_data.get('delete'):\n                        author_meeting = form.instance\n                        if author_meeting.author != author:\n                            raise ValidationError(\n                                'This AuthorMeeting does not belong to the test author'\n                            )\n\n        formset = AuthorMeetingFormSet(queryset=AuthorMeeting.objects.all(),)\n\n        formset.is_valid()\n\n        # Make sure that the formset is properly validating the delete entries \n        # even if the formset is not empty\n        self.assertNotEqual(formset.deleted_objects, [])\n        \n\n"], "sample_925": ["    def test_mock_inheritance(self):\n        with mock(['test.module.submodule']):\n            submodule = import_module('test.module.submodule')\n            assert isinstance(submodule, _MockModule)\n\n            class_name = 'TestClass'\n            submodule_class = getattr(submodule, class_name)\n            assert isinstance(submodule_class, _MockObject)\n            assert isinstance(submodule_class(), _MockObject)\n", "    def test_mock_module_getattr(self):\n        with mock(['my_module']):\n            mod = import_module('my_module')\n            assert isinstance(mod, _MockModule)\n            attr = getattr(mod, 'non_existent_attr')\n            assert isinstance(attr, _MockObject)\n            assert attr.__display_name__ == 'my_module.non_existent_attr'\n\n\n\n", "    def test_mock_module_attributes(self):\n        with mock(['test_module']):\n            module = import_module('test_module')\n            assert isinstance(module, _MockModule)\n            assert module.__file__ == os.devnull\n            assert module.__all__ == []\n            assert module.__path__ == []\n\n            submodule = module.submodule\n            assert isinstance(submodule, _MockObject)\n", "    def test_mock_simple(self) -> None:\n        with mock(['fake_module']):\n            with pytest.raises(ImportError):\n                import_module('fake_module')\n", "    def test_mock_module_attributes(self):\n        with mock(['mymodule']):\n            mod = import_module('mymodule')\n            assert isinstance(mod, _MockModule)\n            assert mod.__file__ == os.devnull\n            assert mod.__name__ == 'mymodule'\n\n"], "sample_793": ["    def test_contamination_auto(self):\n        # Test for contamination='auto'\n        contamination_auto = IsolationForest(\n            contamination='auto', random_state=42\n        )\n        contamination_01 = IsolationForest(contamination=0.1, random_state=42)\n\n        X_train, X_test, y_train, y_test = train_test_split(\n            boston.data, boston.target, test_size=0.2, random_state=42\n        )\n        contamination_auto.fit(X_train)\n        contamination_01.fit(X_train)\n\n        # Score should be similar\n        assert_allclose(\n            contamination_auto.decision_function(X_test),\n            contamination_01.decision_function(X_test),\n            rtol=1e-3,\n        )\n\n\n", "def test_behaviour_and_contamination_with_auto():\n    with ignore_warnings(FutureWarning):\n        # Test behaviour = 'old' and contamination = 'auto'\n        model = IsolationForest(behaviour='old', contamination=\"auto\")\n        X_train, X_test, y_train, y_test = train_test_split(\n            iris.data, iris.target, test_size=0.2, random_state=42\n        )\n        model.fit(X_train)\n        \n\n        # Check for the warning\n        with assert_raises_regex(\n                FutureWarning, r\"contamination parameter cannot be set to 'auto'\"\n            ):\n            model.fit(X_train)\n\n\n\n        \n        # Test behaviour = 'new' and contamination = 'auto'\n        model = IsolationForest(behaviour='new', contamination=\"auto\")\n        model.fit(X_train)\n        assert_equal(model.offset_, -0.5)\n", "    def test_contamination_parameter_behavior(self):\n        X_train, X_test, y_train, y_test = train_test_split(\n            boston.data, boston.target, test_size=0.25, random_state=0\n        )\n        clf = IsolationForest(contamination=\"auto\", random_state=0)\n        clf.fit(X_train)\n        y_pred = clf.predict(X_test)\n\n        # Check scores\n        scores = clf.decision_function(X_test)\n        assert_array_almost_equal(scores.mean(), 0.0, 2)\n\n        with assert_warns_message(\n            \"default contamination parameter 0.1 will change in version 0.22 to 'auto'\",\n            category=FutureWarning,\n        ):\n            clf = IsolationForest(contamination=0.1, random_state=0)\n            clf.fit(X_train)\n            y_pred = clf.predict(X_test)\n\n        assert_array_equal(y_pred, clf.predict(X_test))\n\n        with assert_warns_message(\n            \"contamination parameter cannot be set to 'auto' when behaviour == 'old'.\",\n            category=ValueError,\n        ):\n            clf = IsolationForest(contamination=\"auto\", behaviour=\"old\", random_state=0)\n            clf.fit(X_train)\n            y_pred = clf.predict(X_test)\n\n", "    def test_contamination(self, contamination):\n        rf = IsolationForest(contamination=contamination, random_state=42)\n        X_train, X_test, y_train, y_test = train_test_split(\n            iris.data, iris.target, random_state=42\n        )\n        rf.fit(X_train)\n        scores = rf.decision_function(X_test)\n        assert_array_almost_equal(scores.mean(), rf.offset_,\n                                  rtol=1e-05)\n        \n", "    def test_score_samples_with_subsample_features(self, subsample_features):\n        # We test that the score_samples method returns the same results\n        # whether or not features are subsampled.\n\n        # Load a small sample of the iris data\n        X, y = iris.data[:100], iris.target[:100]\n\n        # Create an IsolationForest model with subsample_features set to the\n        # desired value\n        model = IsolationForest(n_estimators=10, max_samples=20, random_state=42,\n                               max_features=2 if subsample_features else 'auto')\n\n        # Fit the model using the iris data\n        model.fit(X)\n\n        # Compute scores for the iris data\n        scores_subsample = model.score_samples(X)\n        scores_no_subsample = model.score_samples(X)\n        \n        assert_array_almost_equal(scores_subsample, scores_no_subsample, rtol=1e-3)\n\n\n\n"], "sample_47": ["    def test_callable_setting_wrapper_calls_callable(self):\n            return 'callable called'\n\n        wrapped_callable = CallableSettingWrapper(my_callable)\n        self.assertEqual(wrapped_callable(), 'callable called')\n\n\n\n", "    def test_callable_setting_wrapper_calls_callable(self):\n            return 'called'\n\n        wrapped = CallableSettingWrapper(callable_setting)\n        self.assertEqual(wrapped(), 'called')\n", "    def test_callable_setting_wrapper(self):\n        callable_setting = lambda: 'secret value'\n        wrapped_setting = CallableSettingWrapper(callable_setting)\n        self.assertIs(wrapped_setting._wrapped, callable_setting)\n        self.assertNotIsInstance(wrapped_setting, callable_setting)\n\n\n", "    def test_callable_setting_wrapper(self):\n            return \"Called\"\n        wrapped_setting = CallableSettingWrapper(callable_setting_example)\n        self.assertEqual(wrapped_setting(), \"Called\")\n\n        # Test for #21345 and #23070 by seeing if the wrapper prevents AttributeError\n        with self.assertRaises(AttributeError):\n            setattr(wrapped_setting, 'non_existent_attr', 'value')\n\n        # Test for #21345 and #23070 by seeing if the wrapper's `__repr__` \n        # returns the wrapped callable's representation\n        self.assertEqual(repr(wrapped_setting), repr(callable_setting_example))\n        \n", "    def test_callable_setting_wrapper(self):\n        class MyCallable:\n                return \"Hello from callable\"\n\n        wrapped_callable = CallableSettingWrapper(MyCallable())\n        self.assertIsNot(wrapped_callable, MyCallable())\n        self.assertEqual(str(wrapped_callable), \"Hello from callable\")\n\n"], "sample_1125": ["    def test_commutator(self):\n        A = Operator('A')\n        B = Operator('B')\n        self.assertEqual(A.commutator(B), None)\n        self.assertEqual(A.anticommutator(B), None)\n\n        # Test with known commutators\n        C = Operator('C')\n        self.assertEqual(A.commutator(C), None)\n        \n        # Test with Hermitian operator\n        H = HermitianOperator('H')\n        self.assertEqual(H.commutator(C), None)  \n        self.assertEqual(H.anticommutator(C), None) \n", "    def test_identity_operator_matrix_element(self):\n        N = 3\n        identity = IdentityOperator(N)\n        ket = Operator('k')\n        bra = Operator('b')\n        matrix_element = identity.matrix_element(ket, bra)\n        expected_matrix_element = Matrix([[1] * N for _ in range(N)])\n        assert matrix_element == expected_matrix_element\n", "    def test_identity_operator_matrix_representation(self):\n        from sympy.physics.quantum.state import Ket, Bra\n        N = 3\n        I = IdentityOperator(N)\n        matrix_representation = I._represent(format='sympy')\n        self.assertEqual(matrix_representation, eye(N))\n", "    def test_identity_operator_matrix_representation(self):\n        N = 3\n        op = IdentityOperator(N)\n        result = op._represent(format='sympy')\n        expected = Matrix.eye(N)\n        assert result == expected\n", "    def test_matrix_element(self):\n        A = Operator('A')\n        ket = Operator('k')\n        bra = Operator('b')\n        with skip(\"Matrix elements not implemented\"):\n            assert A.matrix_element(ket, bra) is not None\n\n\n\n"], "sample_986": ["    def test_evalf_complex():\n        x = 1 + 2*I\n        result = evalf(x, prec=10)\n        assert abs(result[0] - 1.0) < 1e-6\n        assert abs(result[1] - 2.0) < 1e-6\n", "    def test_evalf_product():\n        from sympy import symbols\n        x, y = symbols('x y')\n        p = Product(x**n, (n, 1, 5))\n        re, im, _, _ = evalf(p.subs(x, 2), 10)\n        assert re == 32\n        p = Product((x + y)**n, (n, 1, 5))\n        re, im, _, _ = evalf(p.subs(x, 2).subs(y, 3), 10)\n        assert re == 32\n        p = Product(x**n, (n, 1, 5))\n        re, im, _, _ = evalf(p.subs(x, 2).subs(y, 3), 10)\n        assert re == 32\n", "    def test_evalf_bernoulli_error(self):\n        with raises(NotImplementedError):\n            N(bernoulli(n, evaluate=False), 10)\n\n", "compilation error", "    def test_evalf_product():\n        from sympy import Product\n        expr = Product(2*x, (x, 1, 4))\n        v = N(expr, 10)\n        assert abs(v - 48) < 1e-6\n\n\n"], "sample_557": ["    def test_figaspect_array():\n        A = np.random.rand(5, 3)\n        fig, ax = plt.subplots(figsize=figaspect(A))\n        ax.imshow(A)\n        plt.show()\n\n\n\n", "    def test_tight_layout_no_padding(self):\n        fig = Figure(figsize=(4, 4))\n        ax = fig.add_subplot(111)\n        ax.plot([1, 2, 3, 4], [1, 2, 3, 4])\n        fig.tight_layout(pad=0)\n        fig.canvas.draw()\n        \n", "    def test_colorbar_add():\n        fig, ax = plt.subplots()\n        data = np.random.rand(10, 10)\n        img = ax.imshow(data)\n        fig.colorbar(img, ax=ax)\n        plt.show()\n", "def test_subfig_tight_layout():\n    fig = plt.figure()\n    gs = gridspec.GridSpec(2, 2)\n    ax1 = fig.add_subplot(gs[0, 0])\n    ax2 = fig.add_subplot(gs[0, 1])\n    ax3 = fig.add_subplot(gs[1, :])\n    tight_layout(fig)\n    plt.show()\n", "    def test_constrained_layout_engine_updates(self):\n        fig = Figure(figsize=(5, 4))\n        ax1 = fig.add_subplot(211)\n        ax2 = fig.add_subplot(212)\n\n        # Set up some content for the subplots\n        ax1.text(0.5, 0.5, \" subplot 1\", ha='center', va='center')\n        ax2.text(0.5, 0.5, \" subplot 2\", ha='center', va='center')\n\n        engine = ConstrainedLayoutEngine()\n        fig.set_layout_engine(engine)\n        engine.execute(fig)\n        initial_bboxes = [ax1.bbox, ax2.bbox]\n\n        # Modify the figure (add new subplot)\n        ax3 = fig.add_subplot(313)\n        ax3.text(0.5, 0.5, \" subplot 3\", ha='center', va='center')\n\n        engine.execute(fig)\n        updated_bboxes = [ax1.bbox, ax2.bbox, ax3.bbox]\n\n        # Assertions\n        for i in range(len(initial_bboxes)):\n            assert not np.allclose(initial_bboxes[i].bounds, updated_bboxes[i].bounds), (\n                f\"Subplot {i+1} bbox did not change as expected\"\n            )\n\n\n\n"]}, "raw_scored_generations": {"sample_2": [false, false, false, false, false], "sample_4": [true, true, true, true, true], "sample_3": [false, false, false, false, false], "sample_1": [false, false, false, false, false], "sample_0": [false, false, false, false, false], "sample_5": [false, false, false, false, false], "sample_6": [false, false, false, false, false], "sample_7": [false, true, false, false, false], "sample_8": [false, false, false, false, true], "sample_9": [false, false, false, false, false], "sample_10": [false, false, false, false, false], "sample_11": [false, false, false, false, false], "sample_12": [false, false, false, false, false], "sample_13": [false, false, false, false, false], "sample_14": [false, false, false, false, false], "sample_15": [false, false, false, false, false], "sample_16": [false, false, false, false, false], "sample_17": [false, false, false, true, false], "sample_18": [false, false, false, false, true], "sample_19": [false, false, false, false, false], "sample_20": [false, false, false, false, false], "sample_21": [false, false, false, false, false], "sample_22": [false, false, false, false, false], "sample_23": [false, false, false, false, false], "sample_24": [false, false, true, false, false], "sample_25": [false, false, false, false, false], "sample_26": [false, false, false, false, false], "sample_27": [false, false, false, false, false], "sample_28": [false, false, false, false, false], "sample_29": [false, false, false, false, false], "sample_30": [false, false, false, false, false], "sample_31": [false, false, false, false, false], "sample_32": [true, true, true, true, true], "sample_33": [false, false, false, false, false], "sample_34": [false, false, false, false, false], "sample_35": [false, false, false, false, false], "sample_36": [false, false, false, false, false], "sample_37": [false, false, false, false, false], "sample_38": [false, false, false, false, false], "sample_39": [false, false, false, false, false], "sample_40": [false, false, false, false, false], "sample_41": [false, false, false, false, false], "sample_42": [false, false, false, false, false], "sample_43": [false, false, false, false, false], "sample_44": [false, false, false, false, false], "sample_45": [false, false, false, false, false], "sample_46": [false, false, false, false, false], "sample_47": [false, false, false, false, false], "sample_48": [false, false, false, false, false], "sample_49": [false, false, false, false, false], "sample_50": [false, true, true, false, true], "sample_51": [false, false, false, false, false], "sample_52": [false, false, false, false, false], "sample_54": [false, false, false, false, false], "sample_53": [false, false, true, true, false], "sample_58": [true, false, true, false, false], "sample_55": [false, false, false, false, false], "sample_56": [false, false, false, false, false], "sample_57": [false, false, false, false, false], "sample_59": [false, false, false, false, false], "sample_60": [false, false, false, false, false], "sample_61": [false, false, false, false, false], "sample_62": [false, true, false, false, true], "sample_63": [false, false, false, false, false], "sample_64": [false, false, false, false, false], "sample_65": [false, false, false, false, false], "sample_67": [false, false, false, false, false], "sample_66": [false, false, false, false, false], "sample_68": [false, false, false, false, false], "sample_69": [false, false, false, false, false], "sample_70": [false, false, false, false, false], "sample_71": [false, true, false, false, false], "sample_72": [false, false, false, false, false], "sample_73": [true, true, true, true, true], "sample_75": [false, false, true, false, false], "sample_74": [true, true, true, false, true], "sample_77": [false, false, false, false, false], "sample_76": [true, true, false, false, false], "sample_78": [false, false, false, false, false], "sample_79": [false, false, false, false, false], "sample_80": [false, false, false, false, false], "sample_82": [false, true, true, true, false], "sample_81": [false, false, false, false, false], "sample_83": [false, false, false, false, false], "sample_85": [false, false, false, false, true], "sample_84": [true, true, false, true, false], "sample_86": [false, false, false, false, false], "sample_88": [false, false, true, false, false], "sample_87": [false, false, false, false, false], "sample_89": [false, false, false, false, false], "sample_90": [false, false, false, false, false], "sample_91": [false, true, false, false, false], "sample_92": [true, true, false, true, true], "sample_93": [false, false, false, false, false], "sample_94": [false, false, false, false, false], "sample_95": [false, false, false, false, false], "sample_98": [false, false, false, false, false], "sample_96": [false, false, false, false, false], "sample_99": [false, false, false, false, false], "sample_97": [false, false, false, false, false], "sample_100": [false, false, false, false, false], "sample_102": [false, false, false, true, false], "sample_101": [false, true, false, false, false], "sample_103": [false, false, false, false, false], "sample_104": [true, false, true, true, true], "sample_107": [false, false, false, true, false], "sample_105": [true, false, false, true, true], "sample_106": [false, false, false, false, false], "sample_108": [false, false, false, false, false], "sample_109": [false, false, false, false, false], "sample_111": [false, false, false, false, false], "sample_110": [false, false, false, false, false], "sample_112": [false, false, false, false, false], "sample_113": [false, false, false, false, false], "sample_114": [false, false, false, false, false], "sample_115": [false, false, true, false, false], "sample_116": [false, false, false, false, false], "sample_117": [false, false, false, true, false], "sample_118": [false, false, false, false, false], "sample_119": [false, false, false, false, false], "sample_120": [false, false, false, false, false], "sample_121": [false, false, false, false, false], "sample_122": [false, true, true, false, true], "sample_123": [false, false, false, false, false], "sample_124": [false, false, false, false, false], "sample_125": [false, false, false, false, false], "sample_126": [false, false, false, false, false], "sample_127": [false, false, false, false, false], "sample_128": [false, false, false, false, false], "sample_129": [false, false, false, false, false], "sample_130": [false, false, false, false, false], "sample_131": [false, true, false, true, false], "sample_132": [false, false, false, false, false], "sample_133": [false, false, false, false, false], "sample_135": [false, false, true, false, false], "sample_134": [false, false, false, false, false], "sample_136": [false, false, false, false, false], "sample_139": [false, false, false, false, false], "sample_137": [false, false, true, true, false], "sample_138": [true, true, true, true, true], "sample_140": [false, false, false, false, false], "sample_141": [false, false, false, false, false], "sample_142": [false, false, false, false, false], "sample_143": [false, false, false, false, false], "sample_144": [false, false, false, false, false], "sample_145": [false, false, false, false, false], "sample_146": [false, false, false, false, false], "sample_147": [false, false, false, false, false], "sample_148": [false, false, false, false, false], "sample_151": [false, false, false, false, false], "sample_149": [false, false, false, true, false], "sample_152": [false, false, false, false, false], "sample_150": [false, false, false, false, false], "sample_153": [false, false, false, false, false], "sample_154": [false, false, false, true, true], "sample_155": [false, false, false, false, false], "sample_156": [false, false, false, true, true], "sample_157": [false, true, true, false, false], "sample_158": [false, false, false, false, false], "sample_159": [false, false, false, false, false], "sample_160": [false, false, false, false, false], "sample_161": [false, false, false, false, false], "sample_162": [true, true, true, true, true], "sample_163": [false, false, false, false, false], "sample_164": [false, false, false, false, true], "sample_165": [false, false, false, false, false], "sample_166": [false, true, false, false, true], "sample_167": [false, false, false, false, false], "sample_168": [false, false, false, false, false], "sample_169": [false, false, false, false, false], "sample_171": [false, false, false, false, false], "sample_170": [false, false, false, true, false], "sample_172": [false, false, false, false, false], "sample_173": [true, false, false, true, false], "sample_174": [false, false, false, true, false], "sample_175": [false, false, false, false, false], "sample_176": [false, false, false, false, false], "sample_177": [false, false, false, false, false], "sample_178": [false, false, true, true, true], "sample_180": [false, false, false, false, false], "sample_179": [false, false, false, false, false], "sample_182": [false, false, false, false, true], "sample_181": [false, false, false, false, false], "sample_183": [false, false, false, false, false], "sample_184": [false, false, false, false, false], "sample_185": [false, true, false, false, false], "sample_186": [false, false, false, false, false], "sample_187": [false, false, false, false, true], "sample_188": [false, false, false, false, false], "sample_189": [false, false, false, false, false], "sample_190": [false, false, false, false, false], "sample_191": [false, false, false, false, false], "sample_192": [true, true, true, false, false], "sample_193": [false, false, false, false, false], "sample_194": [false, false, false, false, false], "sample_195": [false, false, false, false, false], "sample_196": [true, false, false, false, false], "sample_198": [false, false, false, false, false], "sample_197": [false, false, false, false, false], "sample_199": [false, false, false, false, false], "sample_200": [false, false, false, false, false], "sample_201": [false, false, false, false, false], "sample_202": [false, false, false, false, false], "sample_203": [false, false, false, false, false], "sample_204": [false, false, false, false, false], "sample_205": [false, false, false, false, false], "sample_206": [false, false, false, false, false], "sample_207": [false, false, false, false, false], "sample_208": [false, false, false, false, false], "sample_209": [false, false, false, false, false], "sample_210": [true, false, false, false, false], "sample_211": [true, true, false, false, true], "sample_213": [false, false, false, false, false], "sample_212": [false, false, false, false, false], "sample_214": [false, false, false, false, false], "sample_215": [false, false, false, true, false], "sample_216": [false, true, false, false, false], "sample_217": [false, false, false, false, false], "sample_218": [false, false, false, false, false], "sample_219": [false, false, false, false, true], "sample_220": [false, false, false, false, true], "sample_221": [false, false, false, false, false], "sample_222": [false, false, false, false, false], "sample_223": [false, false, false, false, false], "sample_224": [false, false, false, false, false], "sample_225": [false, false, false, false, false], "sample_226": [false, false, false, false, false], "sample_227": [false, false, false, false, false], "sample_228": [false, true, false, false, true], "sample_229": [false, false, false, true, false], "sample_230": [true, true, true, false, true], "sample_231": [false, false, false, false, false], "sample_232": [false, false, false, false, false], "sample_233": [false, false, true, true, true], "sample_234": [false, false, false, false, false], "sample_235": [false, true, true, true, false], "sample_236": [false, false, false, false, false], "sample_237": [false, false, false, false, false], "sample_238": [false, false, false, false, false], "sample_239": [false, false, true, true, true], "sample_240": [false, false, true, false, false], "sample_241": [false, false, false, false, false], "sample_242": [false, false, false, false, false], "sample_243": [false, false, false, false, false], "sample_244": [false, false, false, false, false], "sample_245": [false, true, true, true, true], "sample_246": [true, true, true, false, true], "sample_247": [false, false, false, false, false], "sample_248": [false, true, false, true, false], "sample_249": [false, true, false, false, false], "sample_250": [false, true, false, true, true], "sample_251": [false, true, false, false, false], "sample_252": [false, false, false, false, false], "sample_253": [false, false, false, false, false], "sample_254": [false, false, false, false, false], "sample_256": [false, true, false, false, false], "sample_255": [false, false, false, false, false], "sample_257": [false, false, false, false, false], "sample_258": [false, false, false, false, false], "sample_259": [true, false, false, false, true], "sample_260": [false, false, false, false, false], "sample_261": [true, false, false, false, false], "sample_262": [false, false, false, false, false], "sample_263": [false, false, false, false, false], "sample_264": [false, false, false, false, false], "sample_265": [false, false, false, false, false], "sample_266": [false, false, false, false, false], "sample_267": [false, false, false, false, false], "sample_268": [false, false, false, false, false], "sample_269": [false, false, false, true, false], "sample_270": [false, false, false, false, false], "sample_271": [false, false, false, false, false], "sample_272": [false, false, false, false, false], "sample_273": [false, false, false, false, false], "sample_274": [false, false, false, false, false], "sample_275": [true, true, true, true, true], "sample_276": [false, false, false, false, false], "sample_277": [false, false, false, false, false], "sample_278": [false, false, false, false, false], "sample_279": [false, false, false, false, false], "sample_280": [false, false, false, false, false], "sample_281": [false, true, false, false, false], "sample_282": [false, true, false, false, false], "sample_283": [false, false, false, false, false], "sample_284": [true, true, true, true, true], "sample_285": [false, false, false, false, false], "sample_286": [false, false, false, false, false], "sample_287": [false, false, false, false, false], "sample_288": [false, true, false, false, false], "sample_289": [true, true, true, true, true], "sample_290": [false, false, false, false, false], "sample_291": [false, false, true, false, false], "sample_292": [false, false, true, false, false], "sample_293": [false, false, false, false, true], "sample_294": [true, true, true, true, true], "sample_295": [false, true, false, false, false], "sample_296": [false, false, false, false, false], "sample_297": [false, false, false, false, false], "sample_298": [false, true, true, true, false], "sample_299": [true, false, false, false, false], "sample_300": [false, false, false, false, false], "sample_301": [false, false, false, false, false], "sample_302": [false, true, false, false, true], "sample_303": [false, false, false, true, false], "sample_304": [false, false, false, false, false], "sample_305": [true, false, false, true, true], "sample_306": [false, false, false, false, false], "sample_307": [false, true, false, false, true], "sample_308": [true, false, false, false, false], "sample_309": [false, false, true, false, false], "sample_310": [false, false, false, false, false], "sample_312": [false, false, true, false, false], "sample_311": [false, false, false, false, false], "sample_313": [false, false, false, false, false], "sample_314": [true, true, false, false, false], "sample_315": [false, false, false, false, false], "sample_316": [false, false, false, true, false], "sample_317": [false, false, false, false, false], "sample_318": [false, false, false, false, false], "sample_319": [false, false, false, false, false], "sample_320": [false, false, false, false, false], "sample_321": [true, false, true, false, true], "sample_322": [false, false, false, false, false], "sample_323": [false, false, false, false, false], "sample_324": [true, false, true, false, true], "sample_325": [false, false, false, false, false], "sample_326": [false, false, false, false, false], "sample_327": [true, false, false, true, true], "sample_328": [false, false, false, false, false], "sample_329": [false, false, false, false, false], "sample_330": [false, false, false, false, false], "sample_331": [false, true, false, false, false], "sample_332": [true, true, false, true, false], "sample_333": [true, true, false, false, true], "sample_334": [false, true, true, false, false], "sample_335": [true, false, false, false, false], "sample_336": [false, false, true, false, false], "sample_337": [false, false, false, false, false], "sample_338": [false, false, false, true, false], "sample_339": [false, false, false, false, false], "sample_340": [false, false, false, false, false], "sample_341": [false, false, false, false, true], "sample_342": [false, false, false, false, false], "sample_343": [false, false, false, false, false], "sample_344": [false, false, false, false, true], "sample_345": [false, false, false, false, false], "sample_346": [false, false, false, false, false], "sample_347": [true, false, false, false, false], "sample_348": [false, false, false, false, false], "sample_349": [false, true, false, false, false], "sample_350": [false, false, false, false, false], "sample_351": [false, false, false, false, false], "sample_352": [false, false, false, false, false], "sample_353": [false, false, false, false, false], "sample_354": [false, false, false, false, false], "sample_355": [false, true, false, true, true], "sample_356": [false, false, false, false, true], "sample_357": [false, false, false, false, false], "sample_358": [false, true, false, true, true], "sample_359": [false, false, false, false, false], "sample_360": [false, false, false, false, false], "sample_361": [false, false, false, false, false], "sample_362": [false, false, false, false, false], "sample_363": [false, false, false, false, false], "sample_364": [false, false, false, false, false], "sample_365": [false, false, false, false, false], "sample_366": [false, true, false, true, false], "sample_367": [false, false, false, false, false], "sample_368": [false, false, false, false, false], "sample_369": [false, false, false, false, false], "sample_370": [false, false, false, false, false], "sample_371": [false, false, false, false, false], "sample_372": [false, false, false, false, true], "sample_373": [false, false, false, false, false], "sample_374": [false, false, false, true, false], "sample_375": [false, false, false, false, false], "sample_376": [false, false, false, false, false], "sample_377": [false, false, false, false, false], "sample_378": [false, false, false, false, false], "sample_379": [false, false, false, false, false], "sample_380": [false, false, false, false, false], "sample_381": [false, false, false, false, false], "sample_382": [false, false, false, false, false], "sample_383": [false, false, false, false, false], "sample_384": [false, false, false, false, false], "sample_385": [true, false, false, false, false], "sample_386": [false, false, false, false, false], "sample_387": [false, false, false, false, false], "sample_388": [false, false, false, false, false], "sample_389": [false, false, false, false, false], "sample_390": [false, true, true, false, false], "sample_391": [false, false, false, false, false], "sample_392": [false, false, false, false, false], "sample_393": [true, true, true, true, true], "sample_394": [false, false, false, false, false], "sample_395": [false, false, false, false, false], "sample_396": [false, false, false, false, false], "sample_397": [false, false, false, false, false], "sample_398": [false, false, false, false, false], "sample_399": [false, false, false, false, false], "sample_400": [false, false, false, false, false], "sample_401": [false, false, false, true, false], "sample_403": [false, false, false, false, false], "sample_402": [false, false, false, false, false], "sample_404": [true, true, false, true, true], "sample_405": [false, false, false, false, false], "sample_406": [false, false, true, false, false], "sample_407": [true, true, true, false, true], "sample_408": [true, false, false, false, false], "sample_409": [false, false, false, false, false], "sample_410": [false, true, false, false, true], "sample_411": [false, false, false, false, false], "sample_412": [true, false, false, false, false], "sample_413": [true, false, false, false, true], "sample_414": [false, false, false, false, false], "sample_415": [false, false, false, false, false], "sample_416": [true, true, false, false, false], "sample_417": [false, false, false, false, false], "sample_418": [false, false, false, false, false], "sample_419": [true, true, true, false, false], "sample_420": [false, false, false, false, false], "sample_421": [false, false, false, false, false], "sample_422": [false, false, false, false, false], "sample_423": [false, false, false, false, false], "sample_424": [false, false, false, false, false], "sample_425": [false, false, false, false, false], "sample_426": [false, false, false, false, false], "sample_427": [false, true, false, false, false], "sample_428": [false, false, false, false, false], "sample_429": [false, false, false, false, false], "sample_430": [false, false, false, false, false], "sample_431": [false, false, false, false, false], "sample_432": [false, false, false, false, false], "sample_433": [false, false, false, false, false], "sample_434": [false, false, false, false, false], "sample_435": [false, false, false, false, false], "sample_436": [false, false, false, false, false], "sample_437": [false, false, false, false, false], "sample_438": [false, false, false, false, false], "sample_439": [true, false, false, false, true], "sample_440": [false, false, false, false, false], "sample_441": [false, false, false, false, true], "sample_442": [false, false, true, false, false], "sample_443": [false, false, false, false, false], "sample_444": [true, false, true, true, true], "sample_445": [false, false, false, false, false], "sample_446": [false, false, false, false, false], "sample_447": [false, false, false, false, false], "sample_448": [false, false, false, false, false], "sample_449": [false, false, false, false, false], "sample_450": [false, false, false, false, false], "sample_451": [false, false, false, false, false], "sample_453": [false, false, false, false, false], "sample_452": [false, false, false, false, false], "sample_454": [false, false, false, false, false], "sample_455": [false, false, false, false, false], "sample_456": [true, true, false, false, false], "sample_457": [false, false, false, false, false], "sample_458": [false, false, false, false, false], "sample_459": [false, false, false, false, false], "sample_460": [false, false, false, false, false], "sample_461": [false, false, false, false, false], "sample_462": [false, false, false, false, true], "sample_463": [false, false, false, false, false], "sample_464": [false, false, false, false, false], "sample_465": [false, false, false, false, false], "sample_466": [false, false, false, false, false], "sample_467": [false, false, true, false, true], "sample_469": [false, false, false, false, false], "sample_468": [false, false, false, false, false], "sample_470": [false, false, false, false, false], "sample_471": [false, true, true, false, true], "sample_472": [false, false, true, false, false], "sample_473": [false, false, false, false, false], "sample_474": [false, false, false, false, false], "sample_475": [false, false, false, false, false], "sample_476": [false, false, false, false, false], "sample_477": [false, false, false, false, false], "sample_478": [false, false, false, false, false], "sample_479": [false, false, false, false, false], "sample_480": [false, false, false, false, false], "sample_481": [false, false, false, false, false], "sample_482": [false, false, false, false, false], "sample_483": [false, false, false, false, false], "sample_484": [false, false, false, false, false], "sample_485": [false, false, false, false, false], "sample_486": [false, false, false, false, false], "sample_487": [false, false, false, false, false], "sample_488": [false, false, false, false, false], "sample_489": [false, false, false, false, false], "sample_490": [false, false, false, false, false], "sample_491": [false, false, false, false, false], "sample_492": [false, false, false, false, false], "sample_493": [false, false, false, false, false], "sample_494": [false, false, false, false, false], "sample_495": [false, false, false, false, false], "sample_496": [false, false, false, false, false], "sample_497": [false, false, false, false, false], "sample_498": [false, false, false, false, false], "sample_499": [false, false, false, false, false], "sample_500": [false, false, false, false, false], "sample_501": [false, false, false, false, false], "sample_502": [false, false, false, false, false], "sample_503": [false, false, false, false, false], "sample_504": [false, false, false, false, false], "sample_505": [false, false, false, false, false], "sample_506": [false, false, false, false, false], "sample_507": [true, true, false, false, false], "sample_508": [false, false, false, false, false], "sample_509": [false, false, false, false, false], "sample_510": [false, false, false, false, false], "sample_511": [false, false, false, false, false], "sample_512": [false, false, false, false, false], "sample_513": [false, false, false, false, false], "sample_514": [false, false, false, false, false], "sample_515": [false, false, false, false, false], "sample_516": [false, false, false, false, false], "sample_517": [false, false, false, false, false], "sample_518": [false, false, false, false, false], "sample_519": [false, false, false, false, false], "sample_520": [false, false, false, false, false], "sample_521": [false, false, false, false, false], "sample_522": [false, false, false, false, false], "sample_523": [false, false, false, false, false], "sample_524": [false, false, false, false, false], "sample_525": [false, false, false, false, false], "sample_526": [false, false, false, false, false], "sample_527": [false, false, false, false, false], "sample_528": [false, false, false, false, false], "sample_529": [false, false, false, false, false], "sample_530": [false, false, false, false, false], "sample_531": [false, false, false, false, false], "sample_532": [false, false, false, false, false], "sample_533": [false, false, false, false, false], "sample_534": [false, false, false, false, false], "sample_535": [false, false, false, false, false], "sample_536": [false, false, false, false, false], "sample_537": [false, false, false, false, false], "sample_538": [false, false, false, false, false], "sample_539": [false, false, false, false, false], "sample_540": [false, false, false, false, false], "sample_541": [false, false, false, false, false], "sample_542": [false, false, false, false, false], "sample_543": [false, false, false, false, false], "sample_544": [false, false, false, false, false], "sample_545": [false, false, false, false, false], "sample_546": [false, false, false, false, false], "sample_547": [false, false, false, false, false], "sample_548": [false, false, false, false, false], "sample_549": [false, false, false, false, false], "sample_550": [false, false, false, false, false], "sample_551": [false, false, false, false, false], "sample_552": [false, false, false, false, false], "sample_553": [false, false, false, false, false], "sample_554": [false, false, false, false, false], "sample_555": [false, false, false, false, false], "sample_556": [false, false, false, false, false], "sample_557": [false, false, false, false, false], "sample_558": [false, false, false, false, false], "sample_559": [false, false, false, false, false], "sample_560": [false, false, false, false, false], "sample_561": [false, false, false, false, false], "sample_562": [false, false, false, false, false], "sample_563": [false, false, false, false, false], "sample_564": [false, false, false, false, false], "sample_565": [false, false, false, false, false], "sample_566": [false, false, false, false, false], "sample_567": [false, false, false, false, false], "sample_568": [false, false, false, false, false], "sample_569": [false, false, false, false, false], "sample_570": [false, false, false, false, false], "sample_571": [false, false, false, false, false], "sample_572": [false, false, false, false, false], "sample_573": [false, false, false, false, false], "sample_574": [false, false, false, false, true], "sample_575": [false, false, false, false, false], "sample_576": [false, false, false, false, false], "sample_577": [false, false, false, false, false], "sample_578": [false, false, false, false, false], "sample_579": [false, false, false, false, false], "sample_580": [false, false, false, false, false], "sample_581": [false, false, false, false, false], "sample_582": [false, false, false, false, false], "sample_583": [false, false, false, false, false], "sample_584": [false, false, false, false, false], "sample_585": [false, false, false, false, false], "sample_586": [false, false, false, false, false], "sample_587": [false, false, false, false, false], "sample_588": [false, false, false, false, false], "sample_589": [false, false, false, false, false], "sample_590": [false, false, false, false, false], "sample_591": [false, false, false, false, false], "sample_592": [false, false, false, false, false], "sample_593": [false, false, false, false, false], "sample_594": [false, false, false, false, false], "sample_595": [false, false, false, false, false], "sample_596": [false, false, false, false, false], "sample_597": [false, false, false, false, false], "sample_598": [false, false, false, false, false], "sample_599": [false, false, false, false, false], "sample_600": [false, false, false, false, false], "sample_601": [false, false, false, false, false], "sample_602": [false, false, false, false, false], "sample_603": [true, true, false, true, true], "sample_604": [false, false, false, false, false], "sample_605": [false, false, false, false, false], "sample_606": [false, false, false, false, false], "sample_607": [false, false, false, false, false], "sample_608": [false, false, false, false, false], "sample_609": [false, false, false, false, false], "sample_610": [false, false, false, false, false], "sample_611": [false, false, false, false, false], "sample_612": [false, false, false, false, false], "sample_613": [false, false, false, false, false], "sample_614": [false, false, false, false, false], "sample_615": [false, false, false, false, false], "sample_616": [false, false, false, false, false], "sample_617": [false, false, false, false, false], "sample_618": [false, false, false, false, false], "sample_619": [false, false, false, false, false], "sample_620": [false, false, false, false, false], "sample_621": [false, false, false, false, false], "sample_622": [false, false, false, false, false], "sample_623": [false, false, false, false, false], "sample_624": [false, false, false, false, false], "sample_625": [false, false, false, false, false], "sample_626": [false, false, false, false, false], "sample_627": [false, false, false, false, false], "sample_628": [true, true, true, false, true], "sample_629": [false, false, false, false, false], "sample_630": [false, false, false, false, false], "sample_631": [false, false, false, false, false], "sample_632": [false, false, false, false, false], "sample_633": [false, false, false, false, false], "sample_634": [false, false, false, false, false], "sample_635": [false, false, false, false, false], "sample_636": [false, false, false, false, false], "sample_637": [false, false, false, false, false], "sample_638": [false, false, false, false, false], "sample_639": [false, false, false, false, false], "sample_640": [false, true, false, false, false], "sample_641": [false, false, false, false, true], "sample_642": [false, false, false, false, false], "sample_643": [false, false, false, false, false], "sample_644": [false, false, false, false, false], "sample_645": [false, false, false, false, false], "sample_646": [false, false, false, false, false], "sample_647": [false, false, false, false, false], "sample_648": [false, false, false, false, false], "sample_649": [false, false, false, false, false], "sample_650": [false, false, false, false, false], "sample_651": [false, false, false, false, false], "sample_652": [false, true, false, false, false], "sample_653": [false, false, false, false, false], "sample_654": [false, false, false, false, false], "sample_655": [false, false, false, false, false], "sample_656": [false, false, false, false, false], "sample_657": [false, false, false, false, false], "sample_658": [false, false, false, false, false], "sample_659": [false, false, false, true, false], "sample_660": [false, false, false, false, false], "sample_661": [false, false, false, false, false], "sample_662": [false, false, false, false, false], "sample_663": [false, false, false, false, false], "sample_664": [false, false, false, false, false], "sample_665": [false, false, false, false, false], "sample_666": [false, false, false, false, false], "sample_667": [false, false, false, false, false], "sample_668": [false, false, false, false, false], "sample_669": [false, false, false, false, false], "sample_670": [false, false, false, false, false], "sample_671": [false, false, false, false, false], "sample_672": [false, false, false, false, false], "sample_673": [false, false, false, false, false], "sample_674": [false, false, true, false, false], "sample_675": [false, false, false, false, false], "sample_676": [false, false, false, false, false], "sample_677": [false, false, false, false, false], "sample_678": [false, false, false, false, true], "sample_679": [false, false, false, false, false], "sample_680": [false, false, false, false, false], "sample_681": [false, false, false, false, false], "sample_682": [false, false, false, false, false], "sample_683": [false, false, false, false, false], "sample_684": [false, false, false, false, false], "sample_685": [false, false, false, false, false], "sample_686": [false, false, false, false, false], "sample_687": [false, false, false, false, false], "sample_688": [false, false, false, false, false], "sample_689": [false, false, false, false, false], "sample_690": [false, false, false, false, false], "sample_691": [false, false, false, false, false], "sample_692": [false, false, false, false, false], "sample_693": [false, false, false, false, false], "sample_694": [false, false, false, false, false], "sample_695": [false, false, false, false, false], "sample_696": [false, false, false, false, false], "sample_697": [false, false, false, false, false], "sample_698": [false, false, false, false, false], "sample_699": [false, false, false, false, false], "sample_700": [false, false, false, false, false], "sample_701": [false, false, false, false, false], "sample_702": [false, false, false, false, false], "sample_703": [false, false, false, false, false], "sample_704": [false, false, false, false, false], "sample_705": [false, false, false, false, false], "sample_706": [true, true, true, true, false], "sample_707": [false, false, false, false, false], "sample_708": [false, false, false, false, false], "sample_709": [false, false, false, false, false], "sample_710": [false, false, false, false, false], "sample_711": [false, false, false, false, false], "sample_712": [false, false, false, false, false], "sample_713": [false, false, false, false, false], "sample_714": [false, false, false, false, false], "sample_715": [false, false, false, false, false], "sample_716": [false, false, false, false, false], "sample_717": [true, false, true, false, false], "sample_718": [false, false, false, false, false], "sample_719": [false, false, false, false, false], "sample_720": [false, false, false, false, false], "sample_721": [false, false, false, false, false], "sample_722": [false, true, false, false, false], "sample_723": [false, false, false, false, false], "sample_724": [false, false, false, false, false], "sample_725": [false, false, false, false, false], "sample_726": [false, false, false, false, false], "sample_727": [false, false, false, false, false], "sample_728": [false, false, false, false, false], "sample_729": [false, false, false, false, false], "sample_730": [false, false, false, false, false], "sample_731": [false, false, false, false, false], "sample_732": [false, false, false, false, false], "sample_733": [false, false, false, false, false], "sample_734": [false, false, false, false, false], "sample_735": [false, false, false, false, false], "sample_736": [true, false, false, false, true], "sample_737": [false, false, false, false, false], "sample_738": [false, false, false, false, false], "sample_739": [false, false, false, false, false], "sample_740": [false, true, false, false, false], "sample_741": [false, false, false, false, false], "sample_742": [false, false, false, false, false], "sample_743": [true, false, false, false, false], "sample_744": [false, false, false, false, false], "sample_745": [false, false, false, false, false], "sample_746": [false, false, false, false, false], "sample_747": [false, false, false, false, false], "sample_748": [false, false, false, false, false], "sample_749": [false, false, false, false, false], "sample_750": [false, false, false, false, false], "sample_751": [false, false, false, false, false], "sample_752": [false, false, false, false, false], "sample_753": [false, false, false, false, false], "sample_754": [false, false, false, false, false], "sample_755": [false, false, false, false, false], "sample_756": [false, false, false, false, false], "sample_757": [false, false, false, false, false], "sample_758": [false, false, false, false, false], "sample_759": [false, false, false, false, false], "sample_760": [false, false, false, false, false], "sample_761": [false, false, false, false, false], "sample_762": [false, false, false, false, false], "sample_763": [false, false, false, false, false], "sample_764": [false, false, false, false, false], "sample_765": [false, false, false, false, false], "sample_766": [false, false, false, false, false], "sample_767": [false, false, false, false, false], "sample_768": [false, false, false, false, false], "sample_769": [false, false, false, true, false], "sample_770": [false, false, false, false, false], "sample_771": [false, false, false, false, false], "sample_772": [false, false, false, false, false], "sample_773": [false, false, false, false, false], "sample_774": [false, false, false, false, false], "sample_775": [false, false, false, false, false], "sample_776": [false, false, false, false, true], "sample_777": [false, true, false, false, false], "sample_778": [false, false, false, false, false], "sample_779": [false, false, false, false, false], "sample_780": [true, true, false, false, true], "sample_781": [false, false, false, false, false], "sample_782": [false, false, false, false, false], "sample_783": [false, false, false, false, false], "sample_784": [false, false, false, false, false], "sample_785": [false, false, false, false, false], "sample_786": [false, false, false, false, false], "sample_787": [false, false, false, false, false], "sample_788": [false, false, false, false, false], "sample_789": [false, false, false, false, false], "sample_790": [false, false, false, false, false], "sample_791": [false, false, false, false, false], "sample_792": [false, false, false, false, false], "sample_793": [false, false, false, false, false], "sample_794": [false, false, false, false, false], "sample_795": [false, false, false, false, false], "sample_796": [false, false, false, false, false], "sample_797": [false, false, false, false, false], "sample_798": [false, false, false, false, false], "sample_799": [false, false, false, false, false], "sample_800": [false, false, false, false, false], "sample_801": [false, false, false, false, true], "sample_802": [false, false, false, false, false], "sample_803": [false, false, false, false, false], "sample_804": [false, false, false, false, false], "sample_805": [false, false, false, false, false], "sample_806": [false, false, false, false, false], "sample_807": [false, false, false, false, false], "sample_808": [false, false, false, false, false], "sample_809": [false, false, false, false, false], "sample_810": [false, false, false, false, false], "sample_811": [false, false, false, false, false], "sample_812": [false, false, false, false, false], "sample_813": [false, false, false, false, false], "sample_814": [false, false, false, false, false], "sample_815": [false, false, false, false, false], "sample_816": [false, false, false, false, false], "sample_817": [false, false, false, false, false], "sample_818": [false, false, false, false, false], "sample_819": [false, false, false, false, false], "sample_820": [false, false, false, false, false], "sample_821": [false, false, false, false, false], "sample_822": [false, false, false, false, false], "sample_823": [false, false, false, false, false], "sample_824": [false, false, false, false, false], "sample_825": [false, false, false, false, false], "sample_826": [true, true, true, true, true], "sample_827": [false, false, false, false, false], "sample_828": [false, false, false, false, false], "sample_829": [false, false, false, false, false], "sample_830": [false, false, false, false, false], "sample_831": [false, false, false, false, false], "sample_832": [false, false, false, false, false], "sample_833": [false, false, false, false, false], "sample_834": [false, false, false, false, false], "sample_835": [false, false, false, false, false], "sample_836": [false, false, false, false, false], "sample_837": [false, false, false, false, false], "sample_838": [false, false, false, false, false], "sample_839": [false, false, false, false, false], "sample_840": [false, false, false, false, false], "sample_841": [false, false, false, false, false], "sample_842": [false, false, false, false, false], "sample_843": [false, false, false, false, false], "sample_844": [false, false, false, false, false], "sample_845": [false, false, false, false, false], "sample_846": [false, false, false, false, false], "sample_847": [false, false, false, false, false], "sample_848": [false, false, false, false, false], "sample_849": [false, false, false, false, false], "sample_850": [false, false, false, false, false], "sample_851": [false, false, false, false, false], "sample_852": [false, false, false, true, false], "sample_853": [false, false, false, false, false], "sample_854": [false, false, false, false, false], "sample_855": [false, true, false, false, false], "sample_856": [false, false, false, false, false], "sample_857": [false, true, true, true, false], "sample_858": [true, false, false, false, false], "sample_859": [false, false, false, false, false], "sample_860": [false, false, false, false, false], "sample_861": [false, false, false, false, false], "sample_862": [false, false, false, false, false], "sample_863": [false, false, false, false, false], "sample_864": [false, false, false, false, false], "sample_865": [false, false, false, false, false], "sample_866": [false, false, false, false, false], "sample_867": [false, false, true, false, false], "sample_868": [false, false, false, false, false], "sample_869": [false, false, false, false, false], "sample_870": [false, false, false, false, false], "sample_871": [false, false, false, false, false], "sample_872": [false, false, false, false, false], "sample_873": [false, false, false, false, false], "sample_874": [false, false, false, false, false], "sample_875": [false, false, false, false, false], "sample_876": [false, false, false, false, false], "sample_877": [false, false, false, false, false], "sample_878": [false, false, false, false, false], "sample_879": [false, false, false, false, false], "sample_880": [false, false, false, false, false], "sample_881": [false, false, false, false, false], "sample_882": [false, false, false, false, false], "sample_883": [false, false, false, false, false], "sample_884": [true, true, false, false, false], "sample_885": [false, false, false, false, false], "sample_886": [false, false, false, false, false], "sample_887": [false, false, false, false, false], "sample_888": [false, false, false, false, false], "sample_889": [false, false, false, false, false], "sample_890": [false, false, false, false, false], "sample_891": [false, false, false, false, false], "sample_892": [false, false, false, false, false], "sample_893": [false, false, false, false, false], "sample_894": [false, false, false, false, false], "sample_895": [false, false, false, false, false], "sample_896": [false, false, false, false, false], "sample_897": [false, false, false, false, false], "sample_898": [false, false, false, false, false], "sample_899": [false, false, false, false, false], "sample_900": [false, false, false, false, false], "sample_901": [false, false, false, false, false], "sample_902": [false, false, false, false, false], "sample_903": [false, false, false, false, false], "sample_904": [false, false, false, false, false], "sample_905": [false, false, false, false, false], "sample_906": [false, false, false, false, false], "sample_907": [false, false, false, false, false], "sample_908": [false, false, false, false, false], "sample_909": [false, false, false, false, false], "sample_910": [false, false, false, false, false], "sample_911": [false, false, false, false, false], "sample_912": [false, false, false, false, false], "sample_913": [false, false, false, false, false], "sample_914": [false, false, false, false, false], "sample_915": [false, false, false, false, false], "sample_916": [false, false, false, false, false], "sample_917": [false, false, false, false, false], "sample_918": [false, false, false, false, false], "sample_919": [false, false, false, false, false], "sample_920": [false, false, false, false, false], "sample_921": [false, false, false, false, false], "sample_922": [false, false, false, false, false], "sample_923": [false, false, false, false, false], "sample_924": [false, false, true, false, false], "sample_925": [false, false, false, false, false], "sample_926": [false, false, false, false, false], "sample_927": [false, true, false, true, false], "sample_928": [false, false, false, false, false], "sample_929": [false, false, false, false, false], "sample_930": [false, false, false, false, false], "sample_931": [false, false, false, false, false], "sample_932": [false, false, false, false, false], "sample_933": [false, false, false, false, false], "sample_934": [false, false, true, false, false], "sample_935": [false, false, false, false, true], "sample_936": [false, false, false, false, false], "sample_937": [false, false, false, false, false], "sample_938": [false, false, false, false, false], "sample_939": [false, false, false, false, false], "sample_940": [false, false, false, false, false], "sample_941": [false, false, false, false, false], "sample_942": [false, false, false, false, false], "sample_943": [true, true, true, true, false], "sample_944": [false, false, false, false, false], "sample_945": [false, false, false, false, false], "sample_946": [false, false, false, false, false], "sample_947": [false, false, false, false, true], "sample_948": [false, false, true, false, false], "sample_949": [false, false, false, false, false], "sample_950": [false, false, false, false, false], "sample_951": [false, false, false, false, false], "sample_952": [false, false, false, false, false], "sample_953": [false, false, false, false, false], "sample_954": [false, false, false, false, false], "sample_955": [false, false, false, false, false], "sample_956": [false, false, false, false, false], "sample_957": [false, false, false, false, false], "sample_958": [false, false, false, false, false], "sample_959": [true, true, false, true, true], "sample_960": [false, false, false, false, false], "sample_961": [false, false, false, false, false], "sample_962": [false, false, false, false, false], "sample_963": [false, false, false, false, false], "sample_964": [false, false, false, false, false], "sample_965": [false, false, false, false, false], "sample_966": [false, false, false, false, false], "sample_967": [false, true, false, false, false], "sample_968": [false, false, false, false, false], "sample_969": [true, true, false, true, false], "sample_970": [false, false, false, false, false], "sample_971": [false, false, false, false, false], "sample_972": [true, true, true, true, false], "sample_973": [false, false, false, false, false], "sample_974": [false, false, false, false, false], "sample_975": [false, false, false, false, false], "sample_976": [false, false, false, false, false], "sample_977": [false, false, false, false, false], "sample_978": [false, false, false, false, false], "sample_979": [false, false, false, false, false], "sample_980": [false, false, false, false, false], "sample_981": [false, false, false, false, false], "sample_982": [true, true, true, true, true], "sample_983": [false, false, false, false, false], "sample_984": [false, false, false, false, false], "sample_985": [false, false, false, false, false], "sample_986": [true, true, true, false, true], "sample_987": [true, true, true, true, true], "sample_988": [false, false, false, false, false], "sample_989": [true, true, true, true, true], "sample_990": [false, false, false, false, false], "sample_991": [false, false, false, false, false], "sample_992": [false, false, false, false, false], "sample_993": [false, false, false, false, false], "sample_994": [true, true, true, true, false], "sample_995": [true, true, true, true, true], "sample_996": [false, false, false, false, false], "sample_997": [false, false, false, false, false], "sample_998": [false, false, false, false, false], "sample_999": [false, false, false, false, false], "sample_1000": [false, false, false, false, false], "sample_1001": [false, false, false, false, false], "sample_1002": [true, true, true, true, true], "sample_1003": [false, false, false, false, false], "sample_1004": [false, false, false, false, false], "sample_1005": [false, false, false, false, false], "sample_1006": [false, false, false, false, false], "sample_1007": [false, false, false, false, false], "sample_1008": [false, false, false, false, false], "sample_1009": [false, false, false, false, false], "sample_1010": [false, false, false, false, false], "sample_1011": [false, false, false, false, false], "sample_1012": [false, false, false, false, false], "sample_1013": [false, false, false, false, false], "sample_1014": [false, false, false, false, false], "sample_1015": [false, false, false, false, false], "sample_1016": [false, false, false, false, false], "sample_1017": [false, false, false, false, false], "sample_1018": [false, false, false, false, false], "sample_1019": [false, false, false, false, false], "sample_1020": [false, false, false, false, false], "sample_1021": [false, false, false, false, false], "sample_1022": [false, false, false, false, false], "sample_1023": [false, false, false, false, false], "sample_1024": [false, true, true, true, true], "sample_1025": [false, false, false, false, false], "sample_1026": [false, false, false, false, false], "sample_1027": [false, true, false, false, true], "sample_1028": [false, true, false, false, true], "sample_1029": [false, false, false, false, false], "sample_1030": [false, false, false, false, false], "sample_1031": [false, false, false, false, false], "sample_1032": [false, false, false, false, false], "sample_1033": [true, true, true, false, true], "sample_1034": [false, false, false, true, false], "sample_1035": [false, true, true, false, false], "sample_1036": [false, false, false, false, false], "sample_1037": [false, false, false, false, false], "sample_1038": [false, false, false, false, false], "sample_1039": [false, false, false, false, false], "sample_1040": [false, false, false, false, false], "sample_1041": [false, false, false, false, false], "sample_1042": [false, false, false, false, false], "sample_1043": [false, false, false, false, false], "sample_1044": [false, false, false, false, false], "sample_1045": [false, true, true, true, true], "sample_1046": [false, false, true, false, true], "sample_1047": [false, false, false, false, false], "sample_1048": [false, false, false, false, false], "sample_1049": [false, false, false, false, false], "sample_1050": [false, false, false, false, false], "sample_1051": [false, false, false, false, false], "sample_1052": [false, false, false, false, false], "sample_1053": [true, false, true, true, true], "sample_1054": [false, false, false, false, false], "sample_1055": [false, false, false, false, false], "sample_1056": [false, false, false, false, false], "sample_1057": [false, false, false, false, false], "sample_1058": [false, false, false, false, false], "sample_1059": [false, false, false, false, false], "sample_1060": [false, false, false, false, false], "sample_1061": [true, true, true, true, true], "sample_1062": [false, false, false, false, false], "sample_1063": [false, false, false, false, false], "sample_1064": [true, false, true, true, false], "sample_1065": [false, false, false, false, false], "sample_1066": [false, false, false, false, false], "sample_1067": [false, false, false, false, false], "sample_1068": [false, false, false, false, false], "sample_1069": [false, false, false, false, false], "sample_1070": [false, false, false, false, false], "sample_1071": [false, false, false, false, false], "sample_1072": [false, false, false, false, false], "sample_1073": [false, false, false, false, false], "sample_1074": [false, false, false, false, false], "sample_1075": [false, false, false, false, false], "sample_1076": [false, false, false, false, false], "sample_1077": [false, false, false, false, false], "sample_1078": [false, false, false, true, false], "sample_1079": [false, false, false, false, false], "sample_1080": [false, false, false, false, false], "sample_1081": [true, true, true, true, true], "sample_1082": [false, false, false, false, false], "sample_1083": [false, false, false, false, false], "sample_1084": [false, false, false, false, false], "sample_1085": [true, true, true, true, false], "sample_1086": [false, false, false, false, false], "sample_1087": [false, false, false, false, false], "sample_1088": [false, false, false, false, false], "sample_1089": [false, false, false, false, false], "sample_1090": [false, false, false, false, false], "sample_1091": [false, false, false, true, false], "sample_1092": [false, false, false, false, false], "sample_1093": [false, false, false, false, false], "sample_1094": [false, false, false, false, false], "sample_1095": [false, false, false, false, false], "sample_1096": [false, false, false, false, false], "sample_1097": [false, false, false, false, false], "sample_1098": [false, false, false, false, false], "sample_1099": [false, false, false, false, false], "sample_1100": [true, false, true, true, false], "sample_1101": [false, false, false, false, false], "sample_1102": [true, true, true, false, false], "sample_1103": [false, false, true, true, true], "sample_1104": [false, false, false, false, false], "sample_1105": [false, false, false, false, false], "sample_1106": [false, false, false, false, false], "sample_1107": [false, false, false, false, false], "sample_1108": [false, false, false, false, false], "sample_1109": [false, false, false, false, false], "sample_1110": [false, false, false, false, false], "sample_1111": [false, false, false, false, false], "sample_1112": [false, false, false, false, false], "sample_1113": [false, false, false, false, false], "sample_1114": [false, false, false, false, false], "sample_1115": [true, true, true, true, true], "sample_1116": [false, false, false, false, false], "sample_1117": [false, false, false, false, false], "sample_1118": [false, false, false, false, false], "sample_1119": [false, false, false, false, false], "sample_1120": [false, false, false, false, false], "sample_1121": [true, true, false, true, true], "sample_1122": [true, false, false, true, false], "sample_1123": [false, false, false, false, false], "sample_1124": [false, false, false, false, false], "sample_1125": [false, false, false, false, false], "sample_1126": [false, false, false, false, false], "sample_1127": [false, false, false, false, false], "sample_1128": [false, false, false, false, false], "sample_1129": [false, false, false, false, false], "sample_1130": [false, false, false, false, false], "sample_1131": [false, false, false, false, false], "sample_1132": [false, false, false, false, false], "sample_1133": [false, false, false, false, false], "sample_1134": [false, false, false, false, false], "sample_1135": [true, false, true, true, false], "sample_1136": [true, true, true, true, true], "sample_1137": [false, false, false, false, false], "sample_1138": [false, false, false, false, false], "sample_1139": [false, false, false, false, false], "sample_1140": [false, false, false, false, false], "sample_1141": [false, false, false, false, false], "sample_1142": [false, false, false, false, false], "sample_1143": [false, true, true, true, true], "sample_1144": [false, false, false, false, false], "sample_1145": [false, false, false, false, false], "sample_1146": [false, false, true, false, false], "sample_1147": [false, false, false, false, false], "sample_1148": [false, false, false, false, false], "sample_1149": [false, false, false, false, false], "sample_1150": [false, false, false, false, false], "sample_1151": [true, true, true, true, false], "sample_1152": [false, false, false, false, false], "sample_1153": [true, true, false, false, true], "sample_1154": [false, false, false, false, false], "sample_1155": [false, false, false, false, false], "sample_1156": [false, false, false, false, false], "sample_1157": [false, false, false, false, false], "sample_1158": [false, false, false, false, false], "sample_1159": [false, false, false, false, false], "sample_1160": [false, false, false, false, false], "sample_1161": [false, false, false, false, false], "sample_1162": [false, false, false, false, false], "sample_1163": [true, true, false, false, false], "sample_1164": [false, false, false, false, false], "sample_1165": [false, false, false, false, false], "sample_1166": [false, false, false, false, false], "sample_1167": [false, false, false, false, false], "sample_1168": [false, false, false, false, false], "sample_1169": [false, false, false, false, false], "sample_1170": [false, false, false, false, false], "sample_1171": [false, false, false, false, false], "sample_1172": [false, false, false, false, false], "sample_1173": [false, false, false, false, false], "sample_1174": [true, true, true, true, false], "sample_1175": [false, false, false, false, false], "sample_1176": [true, true, true, true, true], "sample_1177": [true, true, true, false, false], "sample_1178": [false, false, false, false, false], "sample_1179": [false, false, false, false, false], "sample_1180": [false, false, false, false, false], "sample_1181": [false, false, false, false, false], "sample_1182": [false, false, false, false, false], "sample_1183": [true, true, true, true, true], "sample_1184": [false, false, false, false, false], "sample_1185": [false, false, false, false, false], "sample_1186": [false, false, false, false, false], "sample_1187": [false, false, false, false, false], "sample_1188": [false, false, false, false, false], "sample_1189": [false, false, false, false, false], "sample_1190": [false, false, false, false, false], "sample_1191": [false, false, false, false, false], "sample_1192": [false, false, false, false, false], "sample_1193": [false, false, false, false, false], "sample_1194": [false, false, false, false, false], "sample_1195": [false, false, false, false, false], "sample_1196": [false, false, false, false, false], "sample_1197": [false, false, false, false, false], "sample_1198": [false, false, false, false, false], "sample_1199": [false, false, false, false, false], "sample_1200": [false, false, false, false, false], "sample_1201": [false, false, false, false, false], "sample_1202": [true, true, true, true, true], "sample_1203": [false, false, false, false, false], "sample_1204": [false, false, false, false, false], "sample_1205": [false, false, false, false, false], "sample_1206": [true, true, true, true, true], "sample_1207": [false, false, false, false, false], "sample_1208": [false, false, false, false, false], "sample_1209": [false, false, false, false, false]}}