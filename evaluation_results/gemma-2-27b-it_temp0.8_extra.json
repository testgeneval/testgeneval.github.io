{"raw_generations": {"sample_792": ["def test_partial_fit_with_different_classes_in_each_call():\n    X = [[0, 1], [1, 0]]\n    y = [0, 1]\n    clf = MultinomialNB()\n    clf.partial_fit(X[:1], y[:1], classes=[0])\n    clf.partial_fit(X[1:], y[1:], classes=[1])\n    assert_array_equal(clf.classes_, [0, 1])\n\n", "compilation error", "    def test_predict_log_proba_consistency():\n        # Test if predict_log_proba returns consistent results with\n        # predict_proba after applying the logarithm to the probabilities.\n        clf = MultinomialNB()\n        clf.fit(X, y)\n        log_proba = clf.predict_log_proba(X)\n        proba = clf.predict_proba(X)\n        assert_array_almost_equal(log_proba, np.log(proba), decimal=10)\n", "    def test_bnb_zero_counts(self):\n        X = np.array([[1, 0, 1], \n                      [0, 1, 1], \n                      [1, 0, 0]])\n        y = np.array([0, 1, 0])\n        clf = BernoulliNB(alpha=1.0)\n        clf.fit(X, y)\n        # Check feature probabilities for zero counts\n        assert_array_almost_equal(clf.feature_log_prob_[0, 0],\n                                   np.log(1.0 / 3))\n        assert_array_almost_equal(clf.feature_log_prob_[1, 2],\n                                   np.log(1.0 / 2))\n\n", "compilation error"], "sample_275": ["    def test_delete_with_complex_prefetch_related(self):\n        \"\"\"\n        Regression for #22118: Ensure that prefetching related objects with\n        complex lookup chains doesn't cause errors during deletion.\n        \"\"\"\n        parent = House.objects.create(address='Parent House')\n        child1 = OrderedPerson.objects.create(name='Child 1', lives_in=parent)\n        child2 = OrderedPerson.objects.create(name='Child 2', lives_in=parent)\n        Toy.objects.create(name='Block', owners=[child1, child2])\n        House.objects.filter(address='Parent House').prefetch_related(\n            'orderedperson__toys__owners'\n        ).delete()\n", "    def test_delete_queryset_with_select_related(self):\n        h1 = House.objects.create(address='Foo')\n        h2 = House.objects.create(address='Bar')\n        p1 = Child.objects.create(name='Alice', house=h1)\n        p2 = Child.objects.create(name='Bob', house=h2)\n        Child.objects.select_related('house').filter(name__in=['Alice']).delete()\n        self.assertFalse(Child.objects.filter(name='Alice').exists())\n        self.assertTrue(Child.objects.filter(name='Bob').exists())\n", "    def test_ticket_19102_prefetch_related(self):\n        with self.assertNumQueries(1):\n            Login.objects.filter(\n                pk=self.l1.pk\n            ).filter(\n                orgunit__name__isnull=False\n            ).order_by(\n                'description'\n            ).prefetch_related('orgunit').delete()\n        self.assertFalse(Login.objects.filter(pk=self.l1.pk).exists())\n        self.assertTrue(Login.objects.filter(pk=self.l2.pk).exists())\n", "    def test_delete_with_prefetch_related(self):\n        \"\"\"\n        Test that deleting model instances with prefetch_related works as expected.\n        \"\"\"\n        p1 = Person.objects.create(name='Alice')\n        p2 = Person.objects.create(name='Bob')\n        t1 = Toy.objects.create(name='Toy1')\n        t2 = Toy.objects.create(name='Toy2')\n        p1.toys.add(t1, t2)\n        p2.toys.add(t1)\n        \n        persons = Person.objects.prefetch_related('toys').all()\n        persons.delete()\n\n        self.assertEqual(Person.objects.count(), 0) \n        self.assertEqual(Toy.objects.count(), 0) \n", "    def test_delete_queryset_with_values(self):\n        \"\"\"\n        Test that .delete() on a QuerySet with .values() raises the expected\n        error.\n\n        This is a regression test for a bug that was introduced in Django 3.2.\n\n        Refs #25585.\n        \"\"\"\n        msg = 'Cannot call delete() after .values() or .values_list()'\n        with self.assertRaisesMessage(TypeError, msg):\n            Book.objects.values('id').delete()\n\n"], "sample_408": ["    def test_operation_with_custom_name_suggest_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    \"Person\", fields=[], name=\"custom_person_create\"\n                ),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"custom_person_create\")\n", "    def test_custom_name_with_suffix(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\"Person\", fields=[]),\n            ]\n            name = \"custom_name\"\n\n        migration = Migration(\"0001_custom_name\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"custom_name\")\n", "    def test_duplicate_name_suggestion(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\"Person\", fields=[]),\n            ]\n\n        migration1 = Migration(\"0001_person\", \"test_app\")\n        migration2 = Migration(\"0002_person\", \"test_app\")\n        self.assertEqual(migration1.suggest_name(), \"person\")\n        self.assertEqual(migration2.suggest_name(), \"person_2\")\n", "    def test_custom_operation_name_suggestion(self):\n        class CustomOperation(migrations.Operation):\n                super().__init__()\n\n                return \"Custom operation\"\n\n        class Migration(migrations.Migration):\n            operations = [CustomOperation()]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"custom_operation\")\n\n", "    def test_squashed_migration_with_identical_operations(self):\n        class MigrationOne(migrations.Migration):\n            operations = [migrations.CreateModel(\"Person\", fields=[])]\n\n        class MigrationTwo(migrations.Migration):\n            operations = [migrations.CreateModel(\"Person\", fields=[])]\n\n        migration = Migration(\"0002_squashed_0001\", \"test_app\")\n        migration.dependencies = [\n            migrations.MigrationDependency(\"test_app\", \"0001_initial\"),\n            migrations.MigrationDependency(\"test_app\", MigrationOne.name),\n            migrations.MigrationDependency(\"test_app\", MigrationTwo.name),\n        ]\n\n        self.assertEqual(migration.suggest_name(), \"person\")\n\n"], "sample_1130": ["compilation error", "def test_point_pos_from_different_frames():\n    q = dynamicsymbols('q')\n    N = ReferenceFrame('N')\n    B = N.orientnew('B', 'Axis', [q, N.z])\n    O = Point('O')\n    P = Point('P')\n    P.set_pos(O, B.x)\n    assert O.pos_from(P) == -B.x\n", "compilation error", "compilation error", "def test_auto_vel_different_frame_warning_arises():\n    t = dynamicsymbols._t\n    q1, q2 = dynamicsymbols('q1 q2')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    O = Point('O')\n    P = Point('P')\n\n    O.set_vel(N, q1 * N.x)\n    P.set_pos(O, q2 * B.y)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"error\")\n        raises(UserWarning ,lambda: P.vel(B))\n"], "sample_766": ["def test_dict_learning_n_iter_no_convergence():\n    n_components = 5\n    dico = DictionaryLearning(n_components, n_iter=2, random_state=0)\n    with pytest.warns(ConvergenceWarning):\n        dico.fit(X)\n", "def test_dict_learning_online_n_iter():\n\n    rng = np.random.RandomState(0)\n    n_components = 8\n    \n    # check that n_iter is effective in limiting the number of iterations\n    dico_10 = MiniBatchDictionaryLearning(n_components, n_iter=10,\n                                         random_state=rng).fit(X)\n    dico_20 = MiniBatchDictionaryLearning(n_components, n_iter=20,\n                                         random_state=rng).fit(X)\n\n    assert dico_10.n_iter_ < dico_20.n_iter_\n", "def test_dict_learning_lars_empty_input():\n    with pytest.raises(ValueError, match=\"Input data must not be empty\"):\n        DictionaryLearning(n_components=1).fit(np.array([]))\n\n", "def test_dict_learning_online_warm_start():\n    rng = np.random.RandomState(0)\n    X = rng.randn(100, 50)\n    n_components = 10\n\n    dico = MiniBatchDictionaryLearning(n_components, n_iter=5, random_state=0)\n    dico.fit(X)\n\n    # Warm start with existing code\n    dico.set_params(warm_start=True, n_iter=5)\n    old_components = dico.components_.copy()\n    dico.fit(X)\n\n    assert_array_almost_equal(dico.components_, old_components, decimal=2)\n", "    def test_dict_learning_online_n_iter(self):\n        n_components = 5\n        dico = MiniBatchDictionaryLearning(n_components, n_iter=1, random_state=0)\n        dico.fit(X)\n        assert dico.components_.shape == (n_components, n_features)\n\n"], "sample_803": ["compilation error", "compilation error", "compilation error", "compilation error", "    def test_coverage_error_empty():\n        assert_almost_equal(coverage_error([], []), 0)\n        assert_almost_equal(coverage_error([[]], [[]]), 0)\n\n"], "sample_257": ["    def test_deep_lookup_expression_transform(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.annotate(\n                deep_key=KeyTransform('a', 'value__baz'),\n            ).filter(deep_key=Value('b')),\n            [self.objs[7]],\n        )\n", "    def test_isnull_key_with_nested_transform(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__baz__a__isnull=False),\n            [self.objs[7]],\n        )\n", "    def test_key_transform_with_function_and_annotation(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.annotate(\n                length=Length(KeyTransform('c', 'value')),\n            ).filter(length=2),\n            [self.objs[3], self.objs[4]],\n        )\n", "    def test_empty_string_key(self):\n        obj = NullableJSONModel.objects.create(value={'': 'a'})\n        self.assertEqual(NullableJSONModel.objects.filter(value__=''='a').get(), obj)\n", "    def test_key_transform_expression_with_subquery(self):\n        obj = NullableJSONModel.objects.create(value={'a': 1, 'b': 2})\n        subquery = NullableJSONModel.objects.filter(pk=OuterRef('pk')).values('value')\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.annotate(\n                subquery_value=subquery,\n                key=KeyTransform('a', 'subquery_value'),\n                expr=ExpressionWrapper(\n                    F('key') * 2, output_field=IntegerField()\n                ),\n            ).filter(expr=4),\n            [obj],\n        )\n"], "sample_1206": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1161": ["compilation error", "def test_printing_str_array_expressions_with_substitutions():\n    A = ArraySymbol(\"A\", 2, 3, 4)\n    i, j, k = symbols('i j k')\n    assert sstr(A[i, j, k].subs({i: 1, j: 2, k: 3})) == \"A[1, 2, 3]\"\n", "compilation error", "compilation error", "compilation error"], "sample_1120": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_transpose_properties():\n    A = MatrixSymbol('A', 2, 3)\n    B = MatrixSymbol('B', 3, 2)\n    assert (A.T).T == A\n    assert (A + B).T == A.T + B.T\n    assert (A*B).T == B.T*A.T\n"], "sample_1153": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_610": ["compilation error", "def test_infer_freq_nonstandard_freq(freq):\n    indx = pd.date_range(\"2000-01-01\", periods=3, freq=freq)\n    assert xr.infer_freq(indx) == freq\n    assert xr.infer_freq(xr.DataArray(indx)) == freq\n\n", "def test_infer_freq_combined_freq(freq, expected):\n    indx = xr.cftime_range(\"2000-01-01\", periods=3, freq=freq)\n\n    assert xr.infer_freq(indx) == expected\n", "compilation error", "def test_cftimeindex_to_datetimeindex_preserve_nanoseconds():\n    date_type = cftime.DatetimeNoLeap\n    index = xr.CFTimeIndex(\n        [date_type(2000, 1, 1, 0, 0, 0, 123456789)], calendar=calendar\n    )\n    expected = pd.DatetimeIndex(\n        [pd.Timestamp(\"2000-01-01 00:00:00.123456789\")],\n    )\n    result = index.to_datetimeindex()\n    assert result.equals(expected)\n    assert result.dtype == object  \n"], "sample_569": ["    def test_lmplot_col_wrap_hue_legend(self):\n        g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, hue=\"h\", col=\"u\",\n                      col_wrap=4)\n        assert len(g._legend.get_texts()) == 2\n\n", "    def test_lmplot_markers_list(self):\n\n        g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, hue=\"h\",\n                      markers=[\"o\", \"+\", \"x\"])\n        assert g.hue_kws == {\"marker\": [\"o\", \"+\", \"x\"]}\n\n        with pytest.raises(ValueError):\n            lm.lmplot(x=\"x\", y=\"y\", data=self.df, hue=\"h\",\n                      markers=[\"o\", \"+\", \"x\", \"*\"])\n\n", "    def test_lmplot_palette(self):\n\n        with pytest.raises(ValueError):\n            lm.lmplot(x=\"x\", y=\"y\", data=self.df, hue=\"h\", palette=\"bad_palette\")\n\n        g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, hue=\"h\", palette=\"Set1\")\n        assert g.hue_kws == {\"palette\": \"Set1\"}\n\n", "    def test_lmplot_legend_position(self):\n        g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, hue=\"h\", legend_position=\"top\")\n        assert g._legend.get_position().points[1] == 1\n\n", "    def test_regplot_xlabel_ylabel(self):\n        f, ax = plt.subplots()\n        lm.regplot(x=\"x\", y=\"y\", data=self.df, xlabel=\"X Label\", ylabel=\"Y Label\")\n        assert ax.get_xlabel() == \"X Label\"\n        assert ax.get_ylabel() == \"Y Label\"\n\n"], "sample_1009": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_Vector_cross():\n    N = ReferenceFrame('N')\n    v1 = N.x + N.y\n    v2 = N.x + N.z\n\n    assert v1.cross(v2) == N.y - N.z\n    assert v2.cross(v1) == -N.y + N.z\n\n    v3 = 2 * N.x + 3 * N.y + 4 * N.z\n    v4 = 5 * N.x + 6 * N.y + 7 * N.z\n\n    assert v3.cross(v4) == (\n        21 * N.x - 14 * N.y - 6 * N.z\n    )\n    assert v4.cross(v3) == (\n        -21 * N.x + 14 * N.y + 6 * N.z\n    )\n\n    A = ReferenceFrame('A')\n    A.set_euler_angles(0, pi/4, 0)\n    v5 = A.x + A.y\n    v6 = A.x + A.z\n    assert v5.cross(v6) != (A.y - A.z)\n\n    v5 = v5.express(N)\n    v6 = v6.express(N)\n    assert v5.cross(v6) == N.y - N.z\n\n"], "sample_854": ["compilation error", "def test_oneclasssvm_empty_data():\n    # Test behaviour when an empty data array is provided\n\n    X = np.array([])\n    y = np.array([])\n\n    clf = svm.OneClassSVM()\n\n    msg = \"Training data is empty.\"\n    with pytest.raises(ValueError, match=msg):\n        clf.fit(X, y)\n", "def test_linear_svc_multiclass_predict():\n    # Test prediction for LinearSVC with multiclass and different decision\n    # function shapes\n    rng = np.random.RandomState(0)\n    X = rng.randn(100, 3)\n    y = rng.randint(0, 3, size=100)\n\n    for shape in ['ovr', 'ovo']:\n        clf = svm.LinearSVC(multi_class='crammer_singer',\n                            decision_function_shape=shape,\n                            random_state=0).fit(X, y)\n        assert clf.predict(X).shape == (100,)\n        assert all(p in [0, 1, 2] for p in clf.predict(X))\n", "compilation error", "    def test_gamma_auto_deprecated(self, kernel):\n        X, y = make_classification(n_samples=100, random_state=0)\n        clf = svm.SVC(kernel=kernel, gamma='auto_deprecated')\n        with pytest.warns(FutureWarning):\n            clf.fit(X, y)\n\n"], "sample_178": ["    def test_empty_formset_delete_fields(self):\n        class DeleteForm(Form):\n            name = CharField()\n            DELETE = BooleanField(required=False)\n\n        DeleteFormSet = formset_factory(DeleteForm, extra=1, can_delete=True)\n        formset = DeleteFormSet(data={'form-TOTAL_FORMS': '1',\n                                   'form-INITIAL_FORMS': '0',\n                                   'form-0-name': 'Test',\n                                   'form-0-DELETE': 'on'})\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(formset.cleaned_data, [{'name': 'Test', 'DELETE': True}])\n        formset = DeleteFormSet(data={'form-TOTAL_FORMS': '1',\n                                   'form-INITIAL_FORMS': '0',\n                                   'form-0-name': 'Test'})\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(formset.cleaned_data, [{'name': 'Test', 'DELETE': False}])\n\n", "    def test_empty_formset_save(self):\n        \"\"\" save() on an empty formset shouldn't raise errors.\"\"\"\n        class MockModel(object):\n                pass\n        \n        class MockModelForm(ModelForm):\n            class Meta:\n                model = MockModel\n        \n        EmptyFormset = formset_factory(MockModelForm, extra=0)\n        formset = EmptyFormset()\n        formset.save() \n", "    def test_empty_formset_validation_with_management_form_data(self):\n        data = {\n            'form-TOTAL_FORMS': '0',\n            'form-INITIAL_FORMS': '0',\n        }\n        formset = ArticleFormSet(data)\n        self.assertTrue(formset.is_valid())\n", "    def test_formset_with_zero_max_num_forms(self):\n        ChoiceFormSet = formset_factory(Choice, max_num=0)\n        formset = ChoiceFormSet(\n            {'choices-TOTAL_FORMS': '1', 'choices-INITIAL_FORMS': '0', 'choices-0-choice': 'Test', 'choices-0-votes': '10'},\n            prefix='choices',\n        )\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.errors, [{'choices-0-choice': ['You must not add more than 0 forms.']}])\n\n", "    def test_formset_with_empty_forms(self):\n        data = {\n            'form-TOTAL_FORMS': '3',\n            'form-INITIAL_FORMS': '0',\n            'form-0-title': 'Test1',\n            'form-0-pub_date': '2023-10-27',\n            'form-1-title': 'Test2',\n            'form-1-pub_date': '2023-10-28',\n        }\n\n        formset = ArticleFormSet(data)\n\n        # Should be valid despite having only two forms filled\n        self.assertTrue(formset.is_valid())\n\n        # Check cleaned data only includes filled forms\n        self.assertEqual(formset.cleaned_data, [{'title': 'Test1', 'pub_date': datetime.date(2023, 10, 27)}, {'title': 'Test2', 'pub_date': datetime.date(2023, 10, 28)}])\n\n"], "sample_189": ["    def test_cache_control_no_cache(self):\n        view = cache_page(3)(cache_control(no_cache=True)(hello_world_view))\n        request = self.factory.get('/view/')\n        response = view(request, '1')\n        self.assertEqual(response.content, b'Hello World 1')\n        response = view(request, '2')\n        self.assertEqual(response.content, b'Hello World 2')\n", "    def test_cache_page_with_etag(self):\n        # Test that cache_page correctly handles ETag headers.\n        @cache_page(1)\n            return HttpResponse('hello')\n\n        request = self.factory.get('/view/')\n        response1 = view(request)\n        etag1 = response1['Etag']\n        \n        response2 = view(request)\n        etag2 = response2['Etag']\n        self.assertEqual(etag1, etag2)\n\n", "    def test_cache_control_no_cache(self):\n        view = cache_page(3)(cache_control(no_cache=True)(hello_world_view))\n        request = self.factory.get('/view/')\n        response = view(request, '1')\n        self.assertEqual(response.content, b'Hello World 1')\n        response = view(request, '2')\n        self.assertEqual(response.content, b'Hello World 2')\n", "    def test_cache_control_no_cache(self):\n        view = cache_page(3)(cache_control(no_cache=True)(hello_world_view))\n\n        request = self.factory.get('/view/')\n        response = view(request, '1')\n        self.assertEqual(response.content, b'Hello World 1')\n        response = view(request, '2')\n        self.assertEqual(response.content, b'Hello World 2')\n", "    def test_cache_middleware_doesnt_cache_streaming_response(self):\n        request = self.factory.get(self.path)\n        get_cache_data = FetchFromCacheMiddleware(empty_response).process_request(request)\n        self.assertIsNone(get_cache_data)\n\n            return StreamingHttpResponse(['Check for cache with streaming content.'])\n\n        UpdateCacheMiddleware(get_stream_response)(request)\n\n        get_cache_data = FetchFromCacheMiddleware(empty_response).process_request(request)\n        self.assertIsNone(get_cache_data)\n"], "sample_1172": ["compilation error", "compilation error", "    def test_solve_biquadratic_with_complex_roots():\n        x0, y0, x1, y1, r = symbols('x0 y0 x1 y1 r')\n\n        f_1 = (x - x0)**2 + (y - y0)**2 - r**2\n        f_2 = (x - x1)**2 + (y - y1)**2 - r**2\n\n        result = solve_poly_system([f_1, f_2], x, y)\n\n        assert len(result) == 2 and all(len(r) == 2 for r in result)\n        assert all(r.count(I) == 1 for r in flatten(result))\n\n", "compilation error", "compilation error"], "sample_564": ["def test_get_axis_position_from_axis(axis, pos):\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n\n    actual = ax.get_axis_position(axis)\n    assert actual == [False, False, False]\n    ax._axis3don = True\n    ax._axis_position = [True, True, True]\n\n    actual = ax.get_axis_position(axis)\n    assert actual[pos] is True\n    \n", "def test_list_to_array(input_data, expected_result):\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    ax.scatter(input_data, [1, 1, 1], [1, 1, 1])\n    assert ax.get_xlim3d()[0:2] == expected_result\n\n", "def test_scatter_masked_color_image():\n    \"\"\"\n    Test color parameter usage with non-finite coordinate arrays.\n\n    GH#26236\n    \"\"\"\n\n    x = [np.nan, 1, 2,  1]\n    y = [0, np.inf, 2,  1]\n    z = [0, 1, -np.inf, 1]\n    colors = [\n        [0.0, 0.0, 0.0, 1],\n        [0.0, 0.0, 0.0, 1],\n        [0.0, 0.0, 0.0, 1],\n        [0.0, 0.0, 0.0, 1]\n    ]\n\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    ax.scatter(x, y, z, c=colors)\n", "    def test_set_projection(projection):\n        fig = plt.figure()\n        ax = fig.add_subplot(projection='3d')\n        ax.set_projection(projection)\n        assert ax.get_projection() == projection\n", "def test_scatter_masked_marker_size():\n    \"\"\"\n    Test markersize parameter usage with non-finite coordinate arrays.\n\n    GH#26236\n    \"\"\"\n\n    x = [np.nan, 1, 2,  1]\n    y = [0, np.inf, 2,  1]\n    z = [0, 1, -np.inf, 1]\n    sizes = [100, 50, 200, 150]\n\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    path3d = ax.scatter(x, y, z, s=sizes)\n\n    # Assert sizes' equality\n    assert len(path3d.get_sizes()) == len(path3d.get_offsets())\n"], "sample_900": ["def test_warm_start_random_state():\n    X = X_iris\n    y = y_iris\n    \n    clf = MLPClassifier(hidden_layer_sizes=2, solver='sgd',\n                        warm_start=True, random_state=1).fit(X, y)\n    \n    clf.fit(X, y)\n    \n    # Make sure the same random state is used for subsequent fits\n    previous_coefs = clf.coefs_\n    clf.fit(X, y)\n    assert np.allclose(previous_coefs, clf.coefs_)\n\n", "compilation error", "def test_best_validation_score_early_stopping():\n    X = X_digits_binary[:100]\n    y = y_digits_binary[:100]\n\n    mlp = MLPClassifier(early_stopping=True, validation_fraction=0.2)\n    mlp.fit(X, y)\n    assert mlp.best_validation_score_ >= 0\n    assert len(mlp.validation_scores_) > 0\n\n", "    def test_multilabel_classification_with_sparse_indicator_matrix():\n        X, Y = make_multilabel_classification(n_samples=50, random_state=0,\n                                             return_indicator=True)\n        Y_sparse = csr_matrix(Y)\n\n        clf = MLPClassifier(solver='lbfgs', hidden_layer_sizes=30,\n                            random_state=0)\n        clf.fit(X, Y)\n        pred_proba = clf.predict_proba(X)\n        assert_array_equal(pred_proba > 0.5, Y)\n\n        clf.fit(X, Y_sparse)\n        pred_proba_sparse = clf.predict_proba(X)\n        assert_array_equal(pred_proba_sparse > 0.5, Y)\n        assert_almost_equal(pred_proba, pred_proba_sparse)\n", "    def test_output_activation_function():\n        # Test that the output activation function is applied correctly.\n\n        X = np.array([[0.1, 0.2], [0.3, 0.4]])\n        y = np.array([0, 1])\n        mlp = MLPClassifier(hidden_layer_sizes=(2,), activation='relu',\n                            solver='lbfgs',\n                            output_activation='logistic')\n        mlp.fit(X, y)\n        assert mlp.predict_proba(X).max() <= 1.0\n        assert mlp.predict_proba(X).min() >= 0.0\n\n"], "sample_73": ["    def test_manifest_static_files_caching(self):\n        \"\"\"\n        Test that the static files manifest is cached and reused correctly\n        across multiple calls to collectstatic\n        \"\"\"\n        finders.get_finder.cache_clear()\n        err = StringIO()\n\n        # First collectstatic run.\n        call_command('collectstatic', interactive=False, verbosity=0, stderr=err)\n        manifest_content_before = storage.staticfiles_storage.read_manifest()\n\n        # Second collectstatic run.\n        call_command('collectstatic', interactive=False, verbosity=0, stderr=err)\n        manifest_content_after = storage.staticfiles_storage.read_manifest()\n        self.assertEqual(manifest_content_before, manifest_content_after)\n", "    def test_manifest_strict_missing_file(self):\n        missing_file_name = 'cached/missing.css'\n        configured_storage = storage.staticfiles_storage\n        configured_storage.manifest_strict = True\n        self.assertNotIn(missing_file_name, configured_storage.hashed_files)\n\n        # File name not found in manifest\n        with self.assertRaisesMessage(ValueError, \"Missing staticfiles manifest entry for '%s'\" % missing_file_name):\n            self.hashed_file_path(missing_file_name)\n\n        # File doesn't exist on disk\n        with self.assertRaisesMessage(ValueError, \"The file '%s' could not be found with %r.\" % (missing_file_name, configured_storage._wrapped)):\n            self.hashed_file_path(missing_file_name)\n", "    def test_manifest_clear(self):\n        self.run_collectstatic()\n        manifest_content = storage.staticfiles_storage.load_manifest()\n        self.assertTrue(manifest_content)\n\n        storage.staticfiles_storage.manifest_strict = False\n\n        # delete the original file form the app, collect with clear\n        original_file = os.path.join(settings.STATIC_ROOT, 'cached', 'styles.5e0040571e1a.css')\n        os.unlink(original_file)\n        self.run_collectstatic(clear=True)\n\n        manifest_content = storage.staticfiles_storage.load_manifest()\n        self.assertFalse(manifest_content)\n\n", "    def test_manifest_cache_invalidation(self):\n        filename = 'test/styles.css'\n        with open(self._get_filename_path(filename), 'w') as f:\n            f.write('/* test */')\n\n        with self.modify_settings(STATICFILES_DIRS={'append': self._temp_dir}):\n            finders.get_finder.cache_clear()\n            # Initial collectstatic\n            call_command('collectstatic', interactive=False, verbosity=0)\n            self.assertTrue(os.path.exists(os.path.join(settings.STATIC_ROOT, 'test', 'styles.deploy12345.css')))\n\n            # Update the file contents\n            with open(self._get_filename_path(filename), 'w') as f:\n                f.write('/* updated */')\n\n            # collectstatic should trigger cache invalidation\n            call_command('collectstatic', interactive=False, verbosity=0)\n            self.assertFalse(os.path.exists(os.path.join(settings.STATIC_ROOT, 'test', 'styles.deploy12345.css')))\n            self.assertTrue(os.path.exists(os.path.join(settings.STATIC_ROOT, 'test', 'styles.deploy12345.css')))\n\n", "    def test_manifest_strict_missing_file(self):\n        missing_file_name = 'cached/missing.css'\n        configured_storage = storage.staticfiles_storage\n        configured_storage.manifest_strict = True\n        self.assertNotIn(missing_file_name, configured_storage.hashed_files)\n\n        # File name not found in manifest\n        with self.assertRaisesMessage(ValueError, \"Missing staticfiles manifest entry for '%s'\" % missing_file_name):\n            self.hashed_file_path(missing_file_name)\n\n"], "sample_1169": ["compilation error", "compilation error", "def test_canonical_ordering_AntiSymmetricTensor_with_repeated_indices():\n    v = symbols(\"v\")\n\n    c, d = symbols(('c','d'), above_fermi=True, cls=Dummy)\n    k, l = symbols(('k','l'), below_fermi=True, cls=Dummy)\n    \n    assert AntiSymmetricTensor(v, (k, k), (c, d)) == 0\n    assert AntiSymmetricTensor(v, (k, l), (c, c)) == 0\n", "compilation error", "compilation error"], "sample_304": ["    def test_validate_comma_separated_integer_list_with_leading_and_trailing_whitespace(self):\n        with self.assertRaises(ValidationError):\n            validate_comma_separated_integer_list(' 1, 2,3 ')\n", "    def test_null_character_validation_with_custom_message(self):\n        validator = ProhibitNullCharactersValidator(message='Field contains null character.')\n        with self.assertRaisesMessage(ValidationError, 'Field contains null character.'):\n            validator('\\x00something')\n", "    def test_file_extension_empty_list(self):\n        with self.assertRaises(ValueError):\n            FileExtensionValidator([])  \n", "    def test_domain_whitelist_deprecated(self):\n        validator = EmailValidator(domain_whitelist=['mydomain'])\n        self.assertEqual(validator.domain_allowlist, ['mydomain'])\n        self.assertIsNone(validator('email@mydomain'))\n\n", "    def test_int_list_validator_allow_negative_empty_string(self):\n        validator = int_list_validator(allow_negative=True)\n        with self.assertRaises(ValidationError):\n            validator('')\n\n"], "sample_423": ["    def test_unique_together_update(self):\n        initial_author = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"age\", models.IntegerField()),\n            ],\n            {\n                \"unique_together\": {(\"name\", \"age\")},\n            },\n        )\n        author_updated_constraints = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"age\", models.IntegerField()),\n            ],\n            {\n                \"unique_together\": {(\"age\",)},\n            },\n        )\n        changes = self.get_changes([initial_author], [author_updated_constraints])\n\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"testapp\",\n            0,\n            [\"AlterUniqueTogether\"],\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            0,\n            name=\"author\",\n            unique_together={(\"age\",)},\n        )\n\n", "    def test_add_unique_together_constraint(self):\n        before = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"age\", models.IntegerField()),\n            ],\n        )\n        after = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"age\", models.IntegerField()),\n            ],\n            {\n                \"unique_together\": {(\"name\", \"age\")},\n            },\n        )\n        changes = self.get_changes([before], [after])\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"testapp\",\n            0,\n            [\"AddConstraint\"],\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            0,\n            name=\"author\",\n            constraint=models.UniqueConstraint(fields=[\"name\", \"age\"], name=\"author_name_age_unique\"),\n        )\n\n", "    def test_add_field_to_existing_model_with_order_with_respect_to(self):\n        initial_author = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"book\", models.ForeignKey(\"otherapp.Book\", models.CASCADE)),\n                (\"age\", models.IntegerField()),\n            ],\n            options={\n                \"order_with_respect_to\": \"book\",\n            },\n        )\n\n        author_with_new_field = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"book\", models.ForeignKey(\"otherapp.Book\", models.CASCADE)),\n                (\"age\", models.IntegerField()),\n                (\"new_field\", models.CharField(max_length=200)),\n            ],\n            options={\n                \"order_with_respect_to\": \"book\",\n            },\n        )\n        changes = self.get_changes([initial_author], [author_with_new_field])\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"testapp\",\n            0,\n            [\"AddField\"],\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            0,\n            model_name=\"author\",\n            name=\"new_field\",\n        )\n\n", "    def test_migration_name_with_underscore(self):\n        class Migration(migrations.Migration):\n            operations = [migrations.AddField(\"Person\", \"name\", fields=[models.CharField(max_length=200)])]\n        \n        migration = Migration(\"0001_initial\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person_add_name\")\n", "    def test_create_and_alter_model_with_same_name(self):\n        before = [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=200)),\n                ],\n            ),\n        ]\n        after = [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=200)),\n                    (\"age\", models.IntegerField()),\n                ],\n            ),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AddField\"])\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 0, name=\"Author\", field=\"age\"\n        )\n\n"], "sample_898": ["compilation error", "    def test_sample_weight_sparse_matrix():\n        random_state = check_random_state(0)\n        n_samples = 10\n        n_classes = 5\n        y_true = random_state.randint(0, n_classes, size=(n_samples,))\n        y_pred = random_state.randint(0, n_classes, size=(n_samples,))\n        sample_weight = np.zeros(n_samples)\n        sample_weight[[2, 5]] = 1\n\n        for name in ALL_METRICS:\n            if name in METRICS_WITHOUT_SAMPLE_WEIGHT:\n                continue\n            metric = ALL_METRICS[name]\n            if name in (THRESHOLDED_METRICS + MULTILABELS_METRICS +\n                       MULTIOUTPUT_METRICS):\n                continue\n            if name in REGRESSION_METRICS:\n                y_true = random_state.rand(n_samples)\n                y_pred = random_state.rand(n_samples)\n            y_true_sparse = sp.csr_matrix((y_true, (np.arange(n_samples),\n                                                   y_true)))\n            y_pred_sparse = sp.csr_matrix((y_pred, (np.arange(n_samples),\n                                                   y_pred)))\n            score_dense = metric(y_true, y_pred, sample_weight=sample_weight)\n            score_sparse = metric(y_true_sparse, y_pred_sparse,\n                                 sample_weight=sample_weight)\n            assert_almost_equal(score_dense, score_sparse)\n\n", "    def test_no_averaging_labels_empty():\n        # test labels argument when not using averaging\n        # with empty labels\n        y_true_multilabel = np.array([[1, 1, 0, 0], [1, 1, 0, 0]])\n        y_pred_multilabel = np.array([[0, 0, 1, 1], [0, 1, 1, 0]])\n        y_true_multiclass = np.array([0, 1, 2])\n        y_pred_multiclass = np.array([0, 2, 3])\n        labels = np.array([3, 0, 1, 2, 4]) \n\n        _, inverse_labels = np.unique(labels, return_inverse=True)\n\n        for name in METRICS_WITH_AVERAGING:\n            for y_true, y_pred in [[y_true_multiclass, y_pred_multiclass],\n                                   [y_true_multilabel, y_pred_multilabel]]:\n                if name not in MULTILABELS_METRICS and y_pred.ndim > 1:\n                    continue\n\n                metric = ALL_METRICS[name]\n                with pytest.warns(UserWarning):\n                   score_labels = metric(y_true, y_pred, labels=labels, average=None)\n\n\n", "compilation error", "    def test_jaccard_score_empty_prediction():\n        y_true = np.array([[1, 0, 1], [0, 1, 0], [1, 1, 1]])\n        y_pred = np.zeros_like(y_true)\n\n        with pytest.raises(ValueError, match=\"Prediction labels are empty\"):\n            jaccard_score(y_true, y_pred, average=\"samples\")\n"], "sample_996": ["def test_issue_14036_product():\n    a, n = symbols('a n')\n    assert product(1 - a**2 / (n*pi)**2, (n, 1, oo)).doit() == \\\n        piecewise(\n        (1 - a**2/(4*pi**2))/(1 - a**2/(pi**2)), Eq(a**2, pi**2)),\n        (1/(1 - a**2/pi**2)), True", "def test_Product_rewrite_Sum():\n    assert Product(1 / (1 + x**2)**(1/2), (x, 1, n)).rewrite(Sum) == \\\n        exp(Sum(-log(1 + x**2)**(1/2), (x, 1, n)))\n", "def test_product_zero_terms():\n    assert product(x, (x, 1, 0)) == 1\n    assert product(x, (x, 3, 2)) == 1\n    assert product(x, (x, a, a - 1)) == 1\n", "def test_issue_14255():\n    n = Symbol('n')\n    k = Symbol('k')\n    assert product(1/k**n, (k, 1, oo)).doit() == S.Infinity\n", "def test_Product_with_free_symbols():\n    x, y = symbols('x, y', real=True)\n    p = Product(x**(2*n), (n, 1, 5))\n    assert p.subs(x, y) == Product(y**(2*n), (n, 1, 5))\n"], "sample_681": ["def test_log_file_cli_subdirectories_are_successfully_created_and_named_based_on_test_file(testdir):\n    path = testdir.makepyfile(\"\"\" def test_logger(): pass \"\"\")\n    expected = os.path.join(os.path.dirname(str(path)), \"foo\", \"bar\")\n    result = testdir.runpytest(\"--log-file=foo/bar/test_logger.log\")\n    assert os.path.exists(expected)\n\n\n\n    assert \"test_logger.log\" in os.listdir(expected)\n    assert result.ret == ExitCode.OK\n\n", "def test_log_file_level_with_different_from_cli_ini(testdir):\n    log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level = WARNING\n        \"\"\".format(\n            log_file\n        )\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_file_handler.level == logging.WARNING\n            logging.getLogger('catchlog').info(\"This log message won't be shown\")\n            logging.getLogger('catchlog').warning(\"This log message will be shown\")\n            print('PASSED')\n    \"\"\"\n    )\n\n    result = testdir.runpytest(\"-s\", \"--log-file-level=INFO\")\n\n    result.stdout.fnmatch_lines([\"test_log_file_with_different_from_cli_ini.py PASSED\"])\n\n    assert result.ret == 0\n    assert os.path.isfile(log_file)\n    with open(log_file) as rfh:\n        contents = rfh.read()\n        assert \"This log message will be shown\" in contents\n        assert \"This log message won't be shown\" not in contents\n", "def test_log_level_override_from_ini(testdir):\n    log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level = WARNING\n        log_level = INFO\n        \"\"\".format(\n            log_file\n        )\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        logging.getLogger().debug(\"debug message\")\n        logging.getLogger().info(\"info message\")\n        logging.getLogger().warning(\"warning message\")\n            pass\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n\n    assert result.ret == 0\n    assert os.path.isfile(log_file)\n    with open(log_file, encoding=\"utf-8\") as rfh:\n        contents = rfh.read()\n        assert \"debug message\" not in contents\n        assert \"info message\" in contents\n        assert \"warning message\" in contents\n\n", "    def test_log_file_cli_relative_paths(testdir):\n        log_file = os.path.join(\"logs\", \"pytest.log\")\n        testdir.makepyfile(\"\"\" def test_logger(): pass \"\"\")\n        result = testdir.runpytest(f\"--log-file={log_file}\")\n        assert os.path.exists(log_file)\n        assert result.ret == ExitCode.OK\n", "def test_log_file_not_created_when_empty(testdir):\n    log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level = INFO\n        \"\"\".format(\n            log_file\n        )\n    )\n\n\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n\n            logging.getLogger().debug(\"This debug message won't be logged\")\n            logging.getLogger().info(\"This info message won't be logged\")\n\n        \"\"\"\n    )\n    result = testdir.runpytest()\n\n    assert result.ret == 0\n    assert not os.path.isfile(log_file)\n"], "sample_20": ["    def test_empty_table_with_meta(tmp_path):\n        filename = tmp_path / \"test.fits\"\n        t = Table(data=[], meta={'COMMENT': 'This is a comment'})\n        t.write(filename, overwrite=True)\n        t2 = Table.read(filename)\n        assert 'COMMENT' in t2.meta  \n        assert t2.meta['COMMENT'] == 'This is a comment' \n", "    def test_fits_mixins_qtable_to_table_no_astropy_native(tmp_path):\n        filename = tmp_path / \"test_simple.fits\"\n        names = sorted(mixin_cols)\n\n        t = QTable([mixin_cols[name] for name in names], names=names)\n        t.write(filename, format=\"fits\")\n        t2 = Table.read(filename, format=\"fits\", astropy_native=False)\n\n        assert t.colnames == t2.colnames\n\n        for name, col in t.columns.items():\n            col2 = t2[name]\n\n            # Special-case Time, which does not yet support round-tripping\n            # the format.\n            if isinstance(col2, Time):\n                col2.format = col.format\n\n            attrs = compare_attrs[name]\n            compare_class = True\n\n            if isinstance(col.info, QuantityInfo):\n                # Downgrade Quantity to Column + unit\n                assert type(col2) is Column\n                # Class-specific attributes like `value` or `wrap_angle` are lost.\n                attrs = [\"unit\"]\n                compare_class = False\n                # Compare data values here (assert_objects_equal doesn't know how in this case)\n                assert np.all(col.value == col2)\n\n            assert_objects_equal(col, col2, attrs, compare_class)\n", "    def test_roundtrip_with_nulls(self, colname, tmp_path):\n        t = Table(\n            {\n                \"a\": [1, 2, np.nan],\n                \"b\": [3.0, np.nan, 5.0],\n            }\n        )\n        filename = tmp_path / \"test.fits\"\n        t.write(filename, overwrite=True)\n        t2 = Table.read(filename)\n        assert np.isnan(t2[colname][2])\n\n\n\n", "def test_empty_table(tmp_path):\n    filename = tmp_path / \"empty.fits\"\n    t = Table()\n    t.write(filename, format=\"fits\")\n    t2 = Table.read(filename, format=\"fits\")\n    assert t2.colnames == []\n    assert len(t2) == 0\n", "    def test_write_fits_with_empty_table(self, tmp_path):\n        filename = tmp_path / \"test_empty_table.fits\"\n        t = Table()\n        with pytest.warns(AstropyUserWarning, match=\"The table is empty\"):\n            t.write(filename, format='fits')\n        assert os.path.exists(filename)\n"], "sample_168": ["    def test_no_deletion_with_dependent_objects_and_interactive_false(self):\n        \"\"\"\n        non-interactive mode doesn't delete stale content types if there are\n        dependent objects.\n        \"\"\"\n        post = Post.objects.create(title='post', content_type=self.content_type)\n        # A related object is needed to show that a custom collector with\n        # can_fast_delete=False is needed.\n        ModelWithNullFKToSite.objects.create(post=post)\n        with captured_stdout() as stdout:\n            call_command('remove_stale_contenttypes', interactive=False, verbosity=2)\n        self.assertEqual(Post.objects.count(), 1)\n        output = stdout.getvalue()\n        self.assertIn('- Content type for contenttypes_tests.Fake', output)\n        self.assertIn('- 1 contenttypes_tests.Post object(s)', output)\n        self.assertIn('- 1 contenttypes_tests.ModelWithNullFKToSite', output)\n        self.assertNotIn('Deleting stale content type', output)\n        self.assertEqual(ContentType.objects.count(), self.before_count + 1)\n\n", "    def test_no_confirmation_for_empty_stale_content_types(self):\n        \"\"\"Checks if confirmation is skipped when no stale content types are found.\"\"\"\n        with mock.patch('builtins.input') as mock_input:\n            with captured_stdout() as stdout:\n                call_command('remove_stale_contenttypes', verbosity=2, interactive=True)\n        mock_input.assert_not_called()\n        self.assertEqual(stdout.getvalue(), '')\n        self.assertEqual(ContentType.objects.count(), self.before_count)\n\n", "    def test_no_stale_content_types(self):\n        \"\"\"\n        No stale content type is deleted if there aren't any.\n        \"\"\"\n        with captured_stdout() as stdout:\n            call_command('remove_stale_contenttypes', interactive=False, verbosity=2)\n        self.assertEqual(stdout.getvalue(), '')\n        self.assertEqual(ContentType.objects.count(), self.before_count + 1)\n", "    def test_no_database_specified(self):\n        \"\"\"\n        Test that the command works when no database is specified.\n\n        \"\"\"\n        with self.assertNumQueries(1) as context:\n            call_command('remove_stale_contenttypes', verbosity=0)\n        self.assertEqual(context.calls, 1)\n", "    def test_no_confirmation_prompt_when_no_stale_types(self):\n        \"\"\"No confirmation prompt is shown if there are no stale content types.\"\"\"\n        with captured_stdout() as stdout:\n            call_command('remove_stale_contenttypes', interactive=False, verbosity=2)\n        self.assertEqual(stdout.getvalue(), '')\n"], "sample_401": ["    def test_formset_with_initial_data(self):\n        data = {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Zero\",\n            \"choices-0-votes\": \"0\",\n            \"choices-1-choice\": \"One\",\n            \"choices-1-votes\": \"1\",\n        }\n        initial_data = [\n            {\"choice\": \"Initial Choice\", \"votes\": 10},\n            {\"choice\": \"Other Choice\", \"votes\": 5},\n        ]\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet(data, initial=initial_data, auto_id=False, prefix=\"choices\")\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(formset.cleaned_data, [\n            {\"choice\": \"Zero\", \"votes\": 0},\n            {\"choice\": \"One\", \"votes\": 1},\n        ])\n\n", "    def test_empty_formset_cleaned_data(self):\n        formset = formset_factory(Choice, extra=0)()\n        self.assertEqual(formset.cleaned_data, [])\n\n", "    def test_empty_formset_errors(self):\n        data = {\n            \"form-TOTAL_FORMS\": \"0\",\n            \"form-INITIAL_FORMS\": \"0\",\n        }\n        formset = ArticleFormSet(data)\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(formset.errors, {})\n\n", "    def test_formset_with_initial_data(self):\n        data = {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Zero\",\n            \"choices-0-votes\": \"0\",\n            \"choices-1-choice\": \"One\",\n            \"choices-1-votes\": \"1\",\n        }\n        initial_data = [\n            {\"choice\": \"Initial Zero\", \"votes\": 10},\n            {\"choice\": \"Initial One\", \"votes\": 20},\n        ]\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet(data, initial=initial_data, auto_id=False, prefix=\"choices\")\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(formset.cleaned_data, [\n            {\"choice\": \"Zero\", \"votes\": 0},\n            {\"choice\": \"One\", \"votes\": 1},\n        ])\n", "    def test_all_valid_empty(self):\n        data = {\"form-TOTAL_FORMS\": \"0\", \"form-INITIAL_FORMS\": \"0\"}\n        formset1 = ArticleFormSet(data)\n        formset2 = ArticleFormSet(data)\n        self.assertIs(all_valid((formset1, formset2)), True)\n"], "sample_1059": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_420": ["    def test_modelform_factory_with_formfield_callback(self):\n            if isinstance(db_field, models.CharField):\n                return forms.CharField(widget=forms.Textarea)\n            return db_field.formfield(**kwargs)\n\n        Form = modelform_factory(Person, formfield_callback=callback)\n        self.assertEqual(type(Form.base_fields[\"name\"].widget), forms.Textarea)\n\n", "    def test_exclude_fields_with_formfield_callback(self):\n            return forms.CharField(widget=forms.Textarea)\n\n        class BaseForm(forms.ModelForm):\n            class Meta:\n                model = Person\n                fields = \"__all__\"\n                formfield_callback = callback\n\n        NewForm = modelform_factory(Person, form=BaseForm)\n\n        class InheritedForm(NewForm):\n            class Meta:\n                exclude = (\"name\",)\n\n        self.assertNotIn(\"name\", InheritedForm.base_fields)\n        \n", "    def test_modelform_factory_with_exclude(self):\n        form = modelform_factory(Person, exclude=(\"id\",))\n        self.assertEqual(list(form.base_fields), [\"name\"])\n", "    def test_modelform_factory_with_exclude(self):\n        form = modelform_factory(Person, exclude=[\"name\"])\n        self.assertEqual(list(form.base_fields), [])\n", "    def test_modelform_factory_with_exclude(self):\n        form = modelform_factory(Person, exclude=['name'])\n        self.assertEqual(list(form.base_fields), [])\n"], "sample_10": ["    def test_read_write_absolute_path():\n        with open('test.csv', 'w') as f:\n            f.write('a\\n1')\n        t1 = Table.read('test.csv')\n        assert np.all(t1['a'] == [1])\n        t1.write('test.csv')\n        t2 = Table.read('test.csv')\n        assert np.all(t2['a'] == [1])\n\n        os.remove('test.csv')\n", "def test_table_attribute_failure_type():\n    # Test for issue #13493, where a TypeError was being raised\n    # instead of a ValueError when creating a Table with a TableAttribute\n    # that was not a subclass of TableAttribute\n\n    class MyBadAttribute: \n        pass\n\n    with pytest.raises(ValueError,\n                       match='MyBadAttribute is not a subclass of TableAttribute'):\n        class MyTable(Table):\n            foo = MyBadAttribute()\n\n", "def test_slice_assignment_with_table():\n    t = Table([[1, 2], [3, 4]], names=['a', 'b'])\n    t2 = Table([[5, 6], [7, 8]], names=['a', 'b'])\n    t[:1] = t2\n    assert np.all(t['a'] == [5, 3])\n    assert np.all(t['b'] == [6, 4])\n", "    def test_sort_with_column_attribute():\n        t = Table([[3, 1], [2, 4]], names=['a', 'b'], descriptions=['desc1', 'desc2'],\n                  meta={'a': [1, 2], 'b': [3, 4], 'c': [5, 6]})\n        t.sort('a')\n\n        assert t['a'] == [1, 2]\n        assert t['b'] == [4, 3]\n\n", "def test_table_attribute_error():\n    class NoTableAttribute(Table):\n        foo = 'a'\n    with pytest.raises(AttributeError):\n        NoTableAttribute()\n"], "sample_53": ["    def test_widget_with_custom_placeholder(self):\n        class CustomPlaceholderForm(forms.Form):\n            band = ModelChoiceField(\n                queryset=Album.objects.all(),\n                widget=AutocompleteSelect(\n                    Album._meta.get_field('band').remote_field, admin.site, attrs={'data-placeholder': 'Choose a band'}\n                ),\n            )\n        form = CustomPlaceholderForm()\n        output = form.as_table()\n\n        self.assertIn(\n            '<input type=\"hidden\" name=\"band\" id=\"id_band\"><input type=\"text\" name=\"band\" id=\"id_band_raw\" class=\"admin-autocomplete\" data-ajax--cache=\"true\" data-ajax--type=\"GET\" data-ajax--url=\"/admin_widgets/band/autocomplete/\" data-theme=\"admin-autocomplete\" data-allow-clear=\"false\" data-placeholder=\"Choose a band\">',\n            output\n        )\n\n", "    def test_value_omitted_from_data(self):\n        form = AlbumForm({'band': ''})\n        self.assertTrue(form['band'].value_omitted_from_data({}, {}, 'band'))\n", "    def test_empty_value(self):\n        form = NotRequiredBandForm(data={'band': ''})\n        self.assertTrue(form.is_valid())\n        self.assertIsNone(form.cleaned_data['band'])\n", "    def test_render_options_empty_value(self):\n        \"\"\"An empty value should not render an option.\"\"\"\n        form = NotRequiredBandForm()\n        output = form.as_table()\n        self.assertNotIn('<option value=\"\"></option>', output)\n", "    def test_media_empty_languages(self):\n        rel = Album._meta.get_field('band').remote_field\n        with translation.override(''):\n            self.assertEqual(\n                AutocompleteSelect(rel, admin.site).media._js,\n                [\n                    'admin/js/vendor/jquery/jquery.min.js',\n                    'admin/js/vendor/select2/select2.full.min.js',\n                    'admin/js/jquery.init.js',\n                    'admin/js/autocomplete.js',\n                ],\n            )\n\n"], "sample_1110": ["def test_SymPyPrinter_not_supported():\n    from sympy import besselj, bessely\n    \n    expr1 = besselj(1, x)\n    expr2 = bessely(2, x)\n\n    prntr = SymPyPrinter()\n    assert prntr.doprint(expr1) == 'besselj(1, x)'\n    assert prntr.doprint(expr2) == 'bessely(2, x)'\n\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_923": ["def test_build_domain_cpp_with_add_function_parentheses_is_True(app, status, warning):\n    app.builder.build_all()\n    f = 'roles.html'\n    t = (app.outdir / f).read_text()\n    assert len(re.findall(r'paren_5::operator\\(\\)\\(\\)', t)) == 1\n    assert len(re.findall(r'paren_6::operator\\(\\)\\(\\)', t)) == 1\n\n\n\n", "    def test_cpp17_if_constexpr(app, status, warning):\n        app.builder.build_all()\n        ws = filter_warnings(warning, \"cpp17-if-constexpr\")\n        assert len(ws) == 1\n        assert \"WARNING: cpp17:if constexpr: reference target not found: foo\" in ws[0]\n", "    def check(spec, text, file):\n        pattern = '<li><p>%s<a .*?><code .*?><span .*?>%s</span></code></a></p></li>' % spec\n        res = re.search(pattern, text)\n        if not res:\n            print(\"Pattern\\n\\t%s\\nnot found in %s\" % (pattern, file))\n            assert False", "def test_build_domain_cpp_conceptmaps(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"conceptmaps\")\n    assert len(ws) == 0\n\n", "def test_build_domain_cpp_backslash_ok(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"backslash\")\n    assert len(ws) == 0\n"], "sample_1031": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_284": ["    def test_manifest_strict_false(self):\n        # Test file does not exist in the directory.\n        missing_file_name = 'cached/missing.css'\n        self.assertNotIn(missing_file_name, storage.staticfiles_storage.hashed_files)\n\n        configured_storage = storage.staticfiles_storage\n        configured_storage.manifest_strict = False\n\n        # File doesn't exist on disk\n        self.hashed_file_path(missing_file_name)\n\n", "    def test_manifest_file_permissions(self):\n        call_command('collectstatic', **self.command_params)\n        manifest_path = os.path.join(settings.STATIC_ROOT, storage.staticfiles_storage.manifest_name)\n        manifest_mode = os.stat(manifest_path).st_mode & 0o777\n        self.assertEqual(manifest_mode, 0o666 & ~self.umask)\n", "    def test_manifest_strict_mode(self):\n        self.run_collectstatic(clear=True)\n        # File doesn't exist on disk\n        original_file_name = 'cached/missing.css'\n        with self.assertRaisesMessage(ValueError, \"Missing staticfiles manifest entry for '%s'\" % original_file_name):\n            self.hashed_file_path(original_file_name)\n\n\n", "    def test_collectstatic_ignore_patterns(self):\n        finders.get_finder.cache_clear()\n        err = StringIO()\n        with self.assertRaises(Exception):\n            call_command('collectstatic', interactive=False, verbosity=0, stderr=err)\n        self.assertIn(\"Post-processing 'faulty.css' failed!\", err.getvalue())\n", "    def test_manifest_exists_after_file_changes(self):\n        # Create initial static files.\n        file_contents = (\n            ('foo.png', 'foo'),\n            ('bar.css', 'url(\"foo.png\")'),\n        )\n        for filename, content in file_contents:\n            with open(self._get_filename_path(filename), 'w') as f:\n                f.write(content)\n\n        with self.modify_settings(STATICFILES_DIRS={'append': self._temp_dir}):\n            finders.get_finder.cache_clear()\n            # First collectstatic run.\n            call_command('collectstatic', interactive=False, verbosity=0)\n            manifest_path = storage.staticfiles_storage.path(storage.staticfiles_storage.manifest_name)\n            self.assertTrue(os.path.exists(manifest_path))\n\n            # Change the contents of the png files.\n            with open(self._get_filename_path('foo.png'), 'w+b') as f:\n                f.write(b\"new content of file to change its hash\")\n\n            # The manifest should be updated after a second collectstatic.\n            call_command('collectstatic', interactive=False, verbosity=0)\n            self.assertTrue(os.path.exists(manifest_path))\n            # Check if the manifest has changed.\n            with open(manifest_path) as f:\n                manifest_content = json.load(f)\n            self.assertIn('\"foo.png\"', manifest_content)\n            self.assertNotEqual(manifest_content['foo.png']['hash'], 'acbd18db4cc2f94c295772080bd8e468')\n\n"], "sample_426": ["    def test_timeuntil_with_future_date(self):\n        future_date = datetime.datetime(2024, 1, 1)\n        self.assertEqual(timeuntil(future_date), \"1\\xa0year\")\n", "    def test_timesince_midnight(self):\n        midnight = datetime.datetime(2023, 10, 26, 0, 0, 0)\n        tests = [\n            (midnight + datetime.timedelta(minutes=1), \"1\\xa0minute\"),\n            (midnight + datetime.timedelta(minutes=59), \"59\\xa0minutes\"),\n            (midnight + datetime.timedelta(hours=1), \"1\\xa0hour\"),\n            (midnight + datetime.timedelta(hours=23, minutes=59), \"23\\xa0hours, 59\\xa0minutes\"),\n            (midnight + datetime.timedelta(days=1), \"1\\xa0day\"),\n            (midnight + datetime.timedelta(days=2), \"2\\xa0days\"),\n        ]\n        for value, expected in tests:\n            with self.subTest(value=value):\n                self.assertEqual(timesince(midnight, value), expected)\n", "    def test_timeuntil_with_different_timezones(self):\n        \"\"\"When using two different timezones.\"\"\"\n        now = datetime.datetime.now(timezone.utc)\n        now_tz = timezone.make_aware(now, timezone.get_default_timezone())\n        now_tz_i = timezone.localtime(now_tz, timezone.get_fixed_timezone(195))\n\n        self.assertEqual(timeuntil(now), \"0\\xa0minutes\")\n        self.assertEqual(timeuntil(now_tz), \"0\\xa0minutes\")\n        self.assertEqual(timeuntil(now_tz_i), \"0\\xa0minutes\")\n        self.assertEqual(timeuntil(now_tz, now_tz_i), \"0\\xa0minutes\")\n", "    def test_timesince_before_epoch(self):\n        t = datetime.datetime(1960, 8, 14, 13, 46, 0)\n        self.assertEqual(timesince(t, self.t), \"0\\xa0minutes\")\n", "    def test_timeuntil_leap_year_february(self):\n        t = datetime.date(2024, 2, 29)\n        now = datetime.datetime(2024, 2, 28, 18, 0, 0)\n        self.assertEqual(timeuntil(t, now), \"1\\xa0day\")\n        self.assertEqual(timesince(now, t), \"1\\xa0day\")\n"], "sample_938": ["def test_image(app, status, warning):\n    app.build()\n    content = (app.outdir / 'sphinx_image.1').read_text()\n    assert '.. image:: sphinx.png' in content\n\n", "def test_no_man_pages_config(app, status, warning):\n    app.config['man_pages'] = []\n    app.builder.build_all()\n    assert not (app.outdir / 'sphinxtests.1').exists()\n\n", "def test_empty_manpage(app, status, warning):\n    app.build()\n    assert (app.outdir / 'empty.1').exists()\n    content = (app.outdir / 'empty.1').read_text()\n    assert content == ''\n", "def test_reference_target(app, status, warning):\n    app.build()\n    content = (app.outdir / 'python.1').read_text()\n    assert ':ref:`example-label'` in content\n", "def test_images(app, status, warning):\n    app.build()\n    content = (app.outdir / 'sphinx-images.1').read_text()\n    assert '.. image:: logo.png' in content\n"], "sample_639": ["def test_base_checker_message_definition() -> None:\n    basic = OtherBasicChecker()\n    definition = basic.get_message_definition(\"W0001\")\n    assert definition.msgid == \"W0001\"\n    assert definition.msg == \"Basic checker has an example.\"\n    assert definition.symbol == \"basic-checker-example\"\n    assert definition.description == \"Used nowhere and serves no purpose.\"\n\n    with pytest.raises(InvalidMessageError) as excinfo:\n        basic.get_message_definition(\"W0002\")\n    assert \"MessageDefinition for 'W0002' does not exists. Choose from ['W0001']\" in str(excinfo.value)\n\n", "def test_base_checker_message_definition() -> None:\n    basic = OtherBasicChecker()\n    message_definition = basic.get_message_definition(\"W0001\")\n    assert message_definition.msgid == \"W0001\"\n    assert message_definition.symbol == \"basic-checker-example\"\n    assert message_definition.msg == \"Basic checker has an example.\"\n\n", "def test_base_checker_messages():\n    checker = OtherBasicChecker()\n    assert checker.get_message_definition(\"W0001\").msg == \"Basic checker has an example.\"\n    with pytest.raises(InvalidMessageError):\n        checker.get_message_definition(\"W00042\")\n", "def test_base_checker_get_message_definition() -> None:\n    basic = OtherBasicChecker()\n    definition = basic.get_message_definition(\"W0001\")\n    assert definition.msgid == \"W0001\"\n    assert definition.msg == \"Basic checker has an example.\"\n    assert definition.symbol == \"basic-checker-example\"\n    assert definition.description == \"Used nowhere and serves no purpose.\"\n\n\n", "def test_base_checker_message_definition() -> None:\n    basic = OtherBasicChecker()\n\n    message_definition = basic.get_message_definition(\"W0001\")\n\n    assert message_definition.msgid == \"W0001\"\n    assert message_definition.msg == \"Basic checker has an example.\"\n    assert message_definition.symbol == \"basic-checker-example\"\n    assert message_definition.description == \"Used nowhere and serves no purpose.\"\n\n    with pytest.raises(InvalidMessageError) as excinfo:\n        basic.get_message_definition(\"W0003\")\n\n    assert \"MessageDefinition for 'W0003' does not exists\" in str(excinfo.value)\n"], "sample_1064": ["compilation error", "compilation error", "compilation error", "def test_tensorflow_Piecewise():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    x = Symbol('x')\n\n    expr = Piecewise((0, x < 0), (1, x >= 0))\n    assert tensorflow_code(expr) == \\\n        'tensorflow.where(tensorflow.math.greater_equal(x, 0), 1, 0)'\n    _compare_tensorflow_scalar((x,), expr, rng=lambda: random.random()*2 - 1)\n\n", "    def test_tensorflow_piecewise():\n        if not tf:\n            skip(\"TensorFlow not installed\")\n\n        expr = Piecewise((x**2, x < 0), (x+1, x >= 0))\n        f = lambdify(x, expr, 'tensorflow')\n        values = [f(-1), f(0), f(1)]\n        tf_values = session.run(f(-1), f(0), f(1))\n\n        assert values == tf_values\n\n"], "sample_316": ["    def test_imagefile_dimensions(self):\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image = images.ImageFile(fh)\n            self.assertEqual(image.width, 540)\n            self.assertEqual(image.height, 405)\n", "    def test_image_file_width_and_height(self):\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image_file = images.ImageFile(fh)\n            self.assertEqual(image_file.width, 540)\n            self.assertEqual(image_file.height, 405)\n", "    def test_image_file_width_height(self):\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image = images.ImageFile(fh)\n            self.assertEqual(image.width, 540)\n            self.assertEqual(image.height, 405)\n", "    def test_image_file_dimensions(self):\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image = images.ImageFile(fh)\n            self.assertEqual(image.width, 540)\n            self.assertEqual(image.height, 405)\n", "    def test_image_file_width_and_height(self):\n        with open(os.path.join(os.path.dirname(__file__), \"test.png\"), 'rb') as fh:\n            image_file = images.ImageFile(fh)\n\n            self.assertEqual(image_file.width, 540)\n            self.assertEqual(image_file.height, 405)\n"], "sample_404": ["    def test_multiple_extends(self):\n        \"\"\"\n        #27368 -- Multiple `{% extends %}` tags should raise an error.\n        \"\"\"\n        engine = self._engine()\n        template = engine.from_string(\n            \"{% extends 'base1.html' %}{% extends 'base2.html' %}\"\n        )\n\n        with self.assertRaises(TemplateSyntaxError) as e:\n            template.render(Context())\n        if self.debug_engine:\n            self.assertEqual(\n                e.exception.template_debug[\"during\"],\n                \"{% extends 'base2.html' %}\",\n            )\n", "    def test_string_format_with_empty_args(self):\n        \"\"\"\n        String format should work correctly when the format string is empty.\n        \"\"\"\n        template = self._engine().from_string('{{ \"\"|format:\"s\" }}')\n        self.assertEqual(template.render(Context({})), \"\")\n", "    def test_escaped_variable(self):\n        \"\"\"\n        Verify that `{{ var|escape }}` correctly escapes variables.\n        \"\"\"\n        template = self._engine().from_string(\"{{ var|escape }}\")\n        self.assertEqual(template.render(Context({\"var\": '<script>alert()</script>'})), '&lt;script&gt;alert()&lt;/script&gt;')\n", "    def test_empty_filter_expression(self):\n        \"\"\"\n        Verifies that an empty filter expression is parsed correctly without\n        raising errors.\n        \"\"\"\n        engine = self._engine()\n        template = engine.from_string(\"{{ ''| }}\")\n        self.assertEqual(template.render(Context()), \"\")\n", "    def test_multiple_extends_blocks(self):\n        \"\"\"\n        Ensure multiple extends blocks in a template raise an error.\n        \"\"\"\n        template_string = \"\"\"\n        {% extends 'base.html' %}\n        {% block content %}\n            Content\n        {% endblock %}\n        {% extends 'another.html' %}\n        {% block sidebar %}\n            Sidebar\n        {% endblock %}\n        \"\"\"\n        with self.assertRaises(TemplateSyntaxError):\n            self._engine().from_string(template_string)\n\n"], "sample_727": ["    def test_imputation_sparse_zero_missing_values():\n        # Test imputation with sparse matrices and missing_values=0\n        X = sparse.csr_matrix([[1, 0, 0], [0, 2, 0], [0, 0, 3]])\n        imputer = Imputer(missing_values=0, strategy=\"mean\")\n        X_imputed = imputer.fit_transform(X)\n        assert_array_almost_equal(X_imputed.toarray(), X.toarray())\n\n", "def test_imputation_errors():\n\n    X = np.array([[1, 2], [3, 4], [np.nan, 6]])\n    imputer = Imputer()\n\n    # Test invalid strategy\n    assert_raises(ValueError, imputer.fit, X, strategy=\"invalid\")\n    # Test inconsistent data type\n    X_mixed = np.array([['a', 2], [3, 4], [np.nan, 6]])\n    assert_raises(TypeError, imputer.fit, X_mixed)\n", "def test_imputation_errors():\n    # Test that Imputer raises errors in invalid cases.\n    X = np.array([[1, 2], [3, np.nan]])\n\n    # Test that missing_values must be a number or 'NaN'\n    imputer = Imputer(missing_values=\"foo\", strategy='mean')\n    with pytest.raises(TypeError):\n        imputer.fit(X)\n\n    # Test that invalid strategies raise a ValueError\n    imputer = Imputer(missing_values=np.nan, strategy='foo')\n    with pytest.raises(ValueError):\n        imputer.fit(X)\n\n    # Test that axis must be 0 or 1\n    imputer = Imputer(missing_values=np.nan, strategy='mean', axis=2)\n    with pytest.raises(ValueError):\n        imputer.fit(X)\n", "def test_imputation_axis_warning():\n    X = np.array([[1, 2, np.nan], [4, np.nan, 6], [7, 8, 9]])\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\")\n        imputer = Imputer(missing_values=np.nan, axis=None)\n        imputer.fit(X)\n        assert len(w) == 1\n        assert \"axis\" in str(w[0].message)\n", "def test_imputation_zero_variance():\n    # Test imputation when a feature has zero variance.\n    X = np.array([[1, 0, 0],\n                  [2, 0, 0],\n                  [3, 0, 0]])\n\n    X_true = np.array([[1, 0, 0],\n                       [2, 0, 0],\n                       [3, 0, 0]])\n\n    _check_statistics(X, X_true, \"mean\", [np.nan, 0, 0], 0)\n    _check_statistics(X, X_true, \"median\", [np.nan, 0, 0], 0)\n\n    X = sparse.csc_matrix(X)\n    _check_statistics(X, X_true, \"mean\", [np.nan, 0, 0], 0)\n    _check_statistics(X, X_true, \"median\", [np.nan, 0, 0], 0)\n"], "sample_1167": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_209": ["    def test_auto_now_add_default(self):\n        \"\"\"\n        Regression test for #23838: Models with auto_now_add=True\n        should default to the current time on creation, even if no value\n        is explicitly provided.\n        \"\"\"\n        a = Article.objects.create(headline=\"Foo\", article_text=\"Bar\")\n        self.assertIsNotNone(a.pub_date)\n", "    def test_field_change_after_save(self):\n        department = Department.objects.create(id=1, name='department1')\n        worker = Worker.objects.create(name='worker', department=department)\n        original_department = worker.department\n        worker.department = None\n        worker.save()\n        self.assertIsNone(worker.department)\n        self.assertEqual(original_department.id, 1)\n", "    def test_inheritance_model_save_ordering(self):\n        m1 = Model1.objects.create(pkey=100)\n        \n        m2 = Model2(model1=m1, value='m2')\n        m2.save()\n        \n        m3 = Model3(model2=m2, value='m3')\n        m3.save()\n\n        self.assertEqual(Model2.objects.get(model1=m1).value, 'm2')\n        self.assertEqual(Model3.objects.get(model2__model1=m1).value, 'm3')\n\n", "    def test_get_order_method(self):\n        # Test the get_order method for related models\n        worker1 = Worker.objects.create(name='worker1', department=Department.objects.create(name='dept1'))\n        worker2 = Worker.objects.create(name='worker2', department=Department.objects.create(name='dept2'))\n        department1 = worker1.department\n        department2 = worker2.department\n\n        self.assertEqual(department1.get_worker_order(), [worker1.pk])\n        self.assertEqual(department2.get_worker_order(), [worker2.pk])\n", "    def test_related_field_copy(self):\n        \"\"\"\n        Regression test for #27280: deepcopy of model instances doesn't copy\n        related fields correctly.\n        \"\"\"\n        dept = Department.objects.create(name='department1')\n        worker = Worker.objects.create(department=dept, name='worker')\n        copied_worker = copy.deepcopy(worker)\n        self.assertNotEqual(copied_worker.department, worker.department)\n        self.assertEqual(copied_worker.department.name, 'department1')\n"], "sample_236": ["    def test_fast_delete_related_objects_exist(self):\n        u = User.objects.create(\n            avatar=Avatar.objects.create()\n        )\n        a = Avatar.objects.get(pk=u.avatar_id)\n        # 1 query to fast-delete the user\n        # 1 query to delete the avatar\n        self.assertNumQueries(2, a.delete)\n        self.assertFalse(User.objects.exists())\n        self.assertFalse(Avatar.objects.exists())\n\n", "    def test_fast_delete_related_object_with_signal(self):\n        \"\"\"\n        #26401 - Fast delete should respect deletion signals connected to related objects.\n        \"\"\"\n        origin = Origin.objects.create()\n        referrer = Referrer.objects.create(origin=origin)\n        deleted = []\n\n            deleted.append(instance)\n\n        models.signals.pre_delete.connect(receiver, sender=SecondReferrer)\n        try:\n            origin.delete()\n            self.assertEqual(len(deleted), 1)\n            self.assertEqual(deleted[0], referrer)\n        finally:\n            models.signals.pre_delete.disconnect(receiver, sender=SecondReferrer)\n", "    def test_fast_delete_empty_related_set(self):\n        \"\"\"\n        #26016 - Ensure fast-delete doesn't raise an error when the related\n        set is empty.\n        \"\"\"\n        origin = Origin.objects.create()\n        # No Referrers related to this origin, so fast-delete should work.\n        with self.assertNumQueries(1):\n            origin.delete()\n", "    def test_fast_delete_with_delete_signals(self):\n        u = User.objects.create(\n            avatar=Avatar.objects.create()\n        )\n        a = Avatar.objects.get(pk=u.avatar_id)\n\n        calls = []\n\n            calls.append(instance.__class__.__name__)\n\n        models.signals.pre_delete.connect(receiver, sender=User)\n        models.signals.pre_delete.connect(receiver, sender=Avatar)\n\n        self.assertNumQueries(2, a.delete)\n        self.assertEqual(calls, ['User', 'Avatar'])\n        models.signals.pre_delete.disconnect(receiver, sender=User)\n        models.signals.pre_delete.disconnect(receiver, sender=Avatar)\n\n", "    def test_fast_delete_custom_pk(self):\n        obj = CustomPK.objects.create(primary_key='a')\n        with self.assertNumQueries(1):\n            obj.delete()\n"], "sample_260": ["    def test_optimize_through_m2m_through_table(self):\n        \"\"\"\n        Test optimization when CreateModel, AlterField, and DeleteModel\n        operations are applied to a model with a ManyToManyField using a\n        through table.\n\n        This test checks that the optimizer correctly handles the\n        relationship between two models, \"Parent\" and \"Child\", which are\n        connected through a ManyToManyField with a through table \"ParentChild\".\n\n        The test asserts that the following sequence of operations can be\n        optimized:\n\n        1. CreateModel: Create the \"Parent\" model.\n        2. CreateModel: Create the \"Child\" model.\n        3. CreateModel: Create the \"ParentChild\" through table model.\n        4. AlterField: Modify a field in \"ParentChild\".\n        5. DeleteModel: Delete the \"ParentChild\" model.\n\n        The optimized result should only include the CreateModel\n        operations for \"Parent\" and \"Child\" models.\n        \"\"\"\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    \"Parent\", [(\"name\", models.CharField(max_length=255))]\n                ),\n                migrations.CreateModel(\n                    \"Child\", [(\"name\", models.CharField(max_length=255))]\n                ),\n                migrations.CreateModel(\n                    \"ParentChild\",\n                    [\n                        (\n                            \"parent\",\n                            models.ForeignKey(\n                                \"Parent\", models.CASCADE, related_name=\"parentchild_parent\"\n                            ),\n                        ),\n                        (\n                            \"child\",\n                            models.ForeignKey(\n                                \"Child\", models.CASCADE, related_name=\"parentchild_child\"\n                            ),\n                        ),\n                    ],\n                ),\n                migrations.AlterField(\"ParentChild\", \"parent\", models.CharField(max_length=255)),\n                migrations.DeleteModel(\"ParentChild\"),\n            ],\n            [\n                migrations.CreateModel(\n                    \"Parent\", [(\"name\", models.CharField(max_length=255))]\n                ),\n                migrations.CreateModel(\n                    \"Child\", [(\"name\", models.CharField(max_length=255))]\n                ),\n            ],\n        )\n", "    def test_optimize_through_delete_model(self):\n        \"\"\"\n        Test that DeleteModel operations are properly handled when they are\n        nested within other operations. Specifically, the test ensures that\n        DeleteModel operations that depend on models being created or altered\n        in previous operations are correctly optimized.\n        \"\"\"\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\"Foo\", [(\"name\", models.CharField(max_length=255))]),\n                migrations.CreateModel(\"Bar\", [(\"size\", models.IntegerField())]),\n                migrations.AddField(\"Foo\", \"bar\", models.ForeignKey(\"Bar\", models.CASCADE)),\n                migrations.DeleteModel(\"Foo\"),\n                migrations.DeleteModel(\"Bar\"),\n            ],\n            [],\n        )\n", "    def test_create_model_add_field_with_default(self):\n        \"\"\"\n        AddField with a default value should optimize into CreateModel.\n        \"\"\"\n        managers = [('objects', EmptyManager())]\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={'verbose_name': 'Foo'},\n                    bases=(UnicodeModel,),\n                    managers=managers,\n                ),\n                migrations.AddField(\"Foo\", \"age\", models.IntegerField(default=42)),\n            ],\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[\n                        (\"name\", models.CharField(max_length=255)),\n                        (\"age\", models.IntegerField(default=42)),\n                    ],\n                    options={'verbose_name': 'Foo'},\n                    bases=(UnicodeModel,),\n                    managers=managers,\n                ),\n            ],\n        )\n", "    def test_optimize_through_create_alter_field_with_same_name_in_separate_models(self):\n        \"\"\"\n        Test a scenario where a field is created, then altered, in two separate models.\n        Ensure optimization doesn't incorrectly merge them.\n        \"\"\"\n        managers = [('objects', EmptyManager())]\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel('Foo', [('name', models.CharField(max_length=255))],\n                                     options={'verbose_name': 'Foo'},\n                                     bases=(UnicodeModel,), managers=managers),\n                migrations.CreateModel('Bar', [('name', models.CharField(max_length=255))],\n                                     options={'verbose_name': 'Bar'},\n                                     bases=(UnicodeModel,), managers=managers),\n                migrations.AlterField('Foo', 'name', models.IntegerField()),\n\n            ],\n            [\n                migrations.CreateModel('Foo', [('name', models.IntegerField())],\n                                     options={'verbose_name': 'Foo'},\n                                     bases=(UnicodeModel,), managers=managers),\n                migrations.CreateModel('Bar', [('name', models.CharField(max_length=255))],\n                                     options={'verbose_name': 'Bar'},\n                                     bases=(UnicodeModel,), managers=managers),\n\n            ],\n        )\n", "    def test_optimize_dependent_operations(self):\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel('Foo', [('name', models.CharField(max_length=255))]),\n                migrations.CreateModel('Bar', [('foo', models.ForeignKey('Foo', models.CASCADE))]),\n                migrations.DeleteModel('Bar'),\n                migrations.DeleteModel('Foo'),\n            ],\n            [],\n        )\n"], "sample_867": ["    def test_grid_search_refit_precomputed_metric(self):\n        # Test refit with precomputed metric\n\n        n_samples = 50\n        n_splits = 2\n        X, y = make_classification(n_samples=n_samples, random_state=0)\n\n        # defaults to euclidean metric (minkowski p = 2)\n        clf = KNeighborsClassifier()\n        grid_params = {'n_neighbors': [5, 10]}\n        cv = GridSearchCV(clf, grid_params, cv=n_splits, refit=True)\n        cv.fit(X, y)\n\n        # precompute euclidean metric to validate _pairwise is working\n        X_precomputed = euclidean_distances(X)\n        clf = KNeighborsClassifier(metric='precomputed')\n        grid_params = {'n_neighbors': [5, 10]}\n        cv = GridSearchCV(clf, grid_params, cv=n_splits, refit=True)\n        cv.fit(X_precomputed, y)\n\n        # Ensure refitted estimator uses the best parameters\n        assert cv.best_params_['n_neighbors'] == cv.best_estimator_.n_neighbors\n        \n        # predict using the refitted estimator should have same output\n        preds_original = cv.predict(X)\n        preds_precomputed = cv.predict(X_precomputed)\n\n        assert (preds_original == preds_precomputed).all()\n", "    def test_grid_search_with_transform_and_inverse_transform():\n        # test grid search with estimators that have transform and inverse_transform\n        # methods.\n\n        class MockTransformer(TransformerMixin):\n                return self\n\n                return X * 2\n\n                return X / 2\n\n        X, y = make_classification(n_samples=100, n_features=10, random_state=0)\n\n        clf = Pipeline([\n            ('transformer', MockTransformer()),\n            ('classifier', LogisticRegression())\n        ])\n        param_grid = {'classifier__C': [0.1, 1, 10]}\n\n        grid_search = GridSearchCV(clf, param_grid, cv=3)\n        grid_search.fit(X, y)\n        \n        # check that inverse_transform is called during predict_proba\n        X_transformed = grid_search.best_estimator_.named_steps['transformer']\\\n            .transform(X)\n        \n        X_transformed_back = grid_search.best_estimator_.named_steps['transformer']\\\n            .inverse_transform(X_transformed)\n\n        assert_array_almost_equal(X, X_transformed_back)\n\n", "def test_grid_search_refit_best_estimator():\n    # Ensure refitting works with refit=True and best_estimator_ accessible\n    X, y = make_classification(n_samples=100, n_features=10, random_state=0)\n    clf = LinearSVC(random_state=0)\n    param_grid = {'C': [0.1, 1, 10]}\n    grid = GridSearchCV(clf, param_grid, refit=True)\n    grid.fit(X, y)\n    assert isinstance(grid.best_estimator_, LinearSVC)\n    assert grid.best_estimator_.C in param_grid['C']\n\n", "def test_grid_search_with_sparse_input():\n    # Test GridSearchCV with sparse input data\n    rng = np.random.RandomState(0)\n    X = sparse.csr_matrix(rng.rand(10, 5))\n    y = rng.randint(0, 2, size=10)\n\n    param_grid = {'C': [0.1, 1.0]}\n    clf = LogisticRegression(solver='lbfgs', max_iter=100)\n    grid_search = GridSearchCV(clf, param_grid, cv=3)\n    grid_search.fit(X, y)\n", "def test_grid_search_precomputed_metric_with_sparse_data():\n    # Test case for issue #19503\n    # Ensure GridSearchCV works correctly with precomputed distance metrics\n    # and sparse data.\n\n    from scipy.sparse import csr_matrix\n\n    n_samples = 20\n    n_features = 10\n    n_splits = 2\n    X_dense = np.random.rand(n_samples, n_features)\n    X_sparse = csr_matrix(X_dense)\n\n    y = np.random.randint(0, 2, size=n_samples)\n    grid_params = {'n_neighbors': [3, 5]}\n\n    # Precompute distance matrix with dense data\n    distance_matrix = euclidean_distances(X_dense)\n    clf = KNeighborsClassifier(metric='precomputed')\n    cv = GridSearchCV(clf, grid_params, cv=n_splits)\n    cv.fit(distance_matrix, y)\n\n    # Precompute distance matrix with sparse data\n    distance_matrix_sparse = pairwise_distances(X_sparse, metric='euclidean')\n    clf_sparse = KNeighborsClassifier(metric='precomputed')\n    cv_sparse = GridSearchCV(clf_sparse, grid_params, cv=n_splits)\n    cv_sparse.fit(distance_matrix_sparse, y)\n\n    assert np.allclose(cv.best_score_, cv_sparse.best_score_)\n    assert cv.best_params_ == cv_sparse.best_params_\n"], "sample_698": ["def test_coloredlogformatter_with_encoding() -> None:\n    logfmt = \"%(filename)-25s %(lineno)4d %(levelname)-8s %(message)s\"\n\n    record = logging.LogRecord(\n        name=\"dummy\",\n        level=logging.INFO,\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=\"Test Message with \u00fcnicode\",\n        args=(),\n        exc_info=None,\n    )\n\n    tw = TerminalWriter()\n    tw.hasmarkup = True\n    formatter = ColoredLevelFormatter(tw, logfmt)\n    output = formatter.format(record)\n    assert output == (\n        \"dummypath                   10 \\x1b[32mINFO    \\x1b[0m Test Message with \u00fcnicode\"\n    )\n\n", "def test_coloredlogformatter_with_empty_loglevel() -> None:\n    logfmt = \"%(filename)-25s %(lineno)4d %(levelname)-8s %(message)s\"\n    record = logging.LogRecord(\n        name=\"dummy\",\n        level=logging.DEBUG,\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=\"Test Message\",\n        args=(),\n        exc_info=None,\n    )\n    tw = TerminalWriter()\n    tw.hasmarkup = True\n    formatter = ColoredLevelFormatter(tw, logfmt)\n    output = formatter.format(record)\n    assert output == (\n        \"dummypath                   10 \\x1b[36mDEBUG   \\x1b[0m Test Message\"\n    )\n    tw.hasmarkup = False\n    formatter = ColoredLevelFormatter(tw, logfmt)\n    output = formatter.format(record)\n    assert output == (\"dummypath                   10 DEBUG     Test Message\")\n", "def test_coloredlogformatter_level_error() -> None:\n    logfmt = \"%(levelname)s %(message)s\"\n\n    record = logging.LogRecord(\n        name=\"dummy\",\n        level=logging.ERROR,\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=\"Test Message\",\n        args=(),\n        exc_info=None,\n    )\n\n    tw = TerminalWriter()\n    tw.hasmarkup = True\n    formatter = ColoredLevelFormatter(tw, logfmt)\n    output = formatter.format(record)\n    assert output == (\n        \"\\x1b[31mERROR\\x1b[0m Test Message\"\n    )\n\n    tw.hasmarkup = False\n    formatter = ColoredLevelFormatter(tw, logfmt)\n    output = formatter.format(record)\n    assert output == (\"ERROR Test Message\")\n", "    def test_colored_short_level_no_color():\n        logfmt = \"%(levelname).1s %(message)s\"\n\n        record = logging.LogRecord(\n            name=\"dummy\",\n            level=logging.INFO,\n            pathname=\"dummypath\",\n            lineno=10,\n            msg=\"Test Message\",\n            args=(),\n            exc_info=None,\n        )\n\n        tw = TerminalWriter()\n        tw.hasmarkup = False\n        formatter = ColoredLevelFormatter(tw, logfmt)\n        output = formatter.format(record)\n        assert output == (\"I Test Message\")\n\n", "def test_coloredlogformatter_no_colors() -> None:\n    logfmt = \"%(filename)-25s %(lineno)4d %(levelname)-8s %(message)s\"\n\n    record = logging.LogRecord(\n        name=\"dummy\",\n        level=logging.INFO,\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=\"Test Message\",\n        args=(),\n        exc_info=None,\n    )\n\n    tw = TerminalWriter()\n    tw.hasmarkup = False\n    formatter = ColoredLevelFormatter(tw, logfmt)\n    output = formatter.format(record)\n    assert output == (\"dummypath                   10 INFO     Test Message\")\n"], "sample_1021": ["compilation error", "compilation error", "def test_quaternion_multiplication_complex_real():\n    a = symbols(\"a\", complex=True)\n    b = symbols(\"b\", real=True)\n\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(a, b, 0, 0)\n\n    assert q1 * q2 == Quaternion((1 + 2*b)*a + 3*b,\n                                  2*a,\n                                  3*a,\n                                  4*a)\n\n    q3 = Quaternion(0, 0, b, 0)\n    assert q1 * q3 == Quaternion( 3*b,\n                                  4*b,\n                                  -b,\n                                  0)\n", "    def test_quaternion_multiplication_associativity():\n        q1 = Quaternion(1, 2, 3, 4)\n        q2 = Quaternion(5, 6, 7, 8)\n        q3 = Quaternion(9, 10, 11, 12)\n        assert (q1 * q2) * q3 == q1 * (q2 * q3)\n", "compilation error"], "sample_1180": ["def test_point_equality():\n    p1 = Point(1, 2)\n    p2 = Point(1, 2)\n    p3 = Point(3, 4)\n    assert p1 == p2\n    assert p1 != p3\n    assert not (p1 != p2)\n    assert not (p1 == p3)\n", "compilation error", "    def test_point_equality():\n        p1 = Point(1, 2)\n        p2 = Point(1, 2)\n        p3 = Point(3, 4)\n        assert p1 == p2\n        assert p1 != p3\n", "compilation error", "compilation error"], "sample_504": ["    def test_colorbar_with_pcolormesh_and_discrete_norm(self):\n        fig, ax = plt.subplots()\n        data = np.arange(12).reshape(3, 4)\n        pc = ax.pcolormesh(data, cmap='viridis', norm=mcolors.DiscreteNorm(bins=5))\n        fig.colorbar(pc, ax=ax)\n", "def test_colorbar_with_axes_get_position(orientation):\n    fig, ax = plt.subplots()\n    im = ax.imshow([[1, 2], [3, 4]])\n    cbar = fig.colorbar(im, ax=ax, orientation=orientation)\n    cbar_pos = cbar.ax.get_position()\n    ax_pos = ax.get_position()\n    if orientation == \"vertical\":\n        assert cbar_pos.x1 >= ax_pos.x1\n    else:\n        assert cbar_pos.y1 >= ax_pos.y1\n", "    def test_colorbar_labelsize():\n        fig, ax = plt.subplots()\n        pc = ax.pcolormesh(np.arange(100).reshape(10, 10))\n        cbar = fig.colorbar(pc)\n        cbar.set_label('Test Label', fontsize=16)\n        assert cbar.ax.get_ylabel('Test Label').get_fontsize() == 16\n", "def test_colorbar_tick_params_labelsize():\n    fig, ax = plt.subplots()\n    pc = ax.imshow(np.arange(100).reshape(10, 10))\n    cbar = fig.colorbar(pc)\n    cbar.ax.tick_params(labelsize=15)\n", "def test_colorbar_tick_params():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.arange(100).reshape(10, 10))\n    cb = fig.colorbar(pc)\n    cb.ax.tick_params(direction='in', length=10, width=2, colors='red')\n    assert cb.ax.yaxis.get_tick_params()['direction'] == 'in'\n\n"], "sample_219": ["    def test_output_field_preservation(self):\n        expr = ExpressionWrapper(F('field'), output_field=CharField())\n        self.assertIsInstance(expr.output_field, CharField)\n", "    def test_wrapped_field_deconstruction(self):\n        value = F('field')\n        wrapped = ExpressionWrapper(value, output_field=IntegerField())\n\n        path, args, kwargs = wrapped.deconstruct()\n        self.assertEqual(path, 'django.db.models.expressions.ExpressionWrapper')\n        self.assertEqual(args, (value,))\n        self.assertEqual(kwargs['output_field'].deconstruct(), IntegerField().deconstruct())\n\n", "    def test_expressionwrapper_output_field(self):\n        value = Value('f')\n        expr = ExpressionWrapper(Lower(value), output_field=IntegerField())\n        self.assertIsInstance(expr.output_field, IntegerField)\n", "    def test_expression_wrapper_resolve_output_field(self):\n        value = 3\n        expr = ExpressionWrapper(Value(value), output_field=CharField())\n        self.assertIsInstance(expr.output_field, CharField)\n", "    def test_resolve_expression_with_output_field(self):\n        value = Value('f')\n        value.output_field = None\n        expr = ExpressionWrapper(Lower(value), output_field=IntegerField())\n        self.assertEqual(expr.output_field, IntegerField())\n"], "sample_297": ["    def test_ticket_24830(self):\n        r1 = RecursiveModel.objects.create(name='r1')\n        r2 = RecursiveModel.objects.create(name='r2', parent=r1)\n        r3 = RecursiveModel.objects.create(name='r3', parent=r1)\n        r4 = RecursiveModel.objects.create(name='r4', parent=r2)\n        qs = RecursiveModel.objects.filter(\n            Q(parent__parent__name='r1') | Q(parent__name='r1')\n        )\n        self.assertSequenceEqual(qs.order_by('name'), [r2, r3, r4])\n", "    def test_ticket_23622_isnull(self):\n        a1 = Ticket23605A.objects.create()\n        Ticket23605B.objects.create(modela_fk=a1, field_b0=123, field_b1=True)\n        a2 = Ticket23605A.objects.create()\n\n        qx = (\n            Q(ticket23605b__isnull=False) &\n            Q(ticket23605b__pk__in=Ticket23605B.objects.order_by('modela_fk', '-field_b1').distinct('modela_fk'))\n        )\n\n        self.assertSequenceEqual(Ticket23605A.objects.filter(qx), [a1])\n", "    def test_ticket_22971(self):\n        a1 = Parent.objects.create()\n        c1 = Child.objects.create(parent=a1)\n        c2 = Child.objects.create(parent=a1)\n        c3 = Child.objects.create(parent=a1)\n        qs = Parent.objects.annotate(child_count=Count('child')).filter(child_count__gt=1)\n        self.assertSequenceEqual(qs, [a1])\n        qs = qs.exclude(child__in=[c2])\n        self.assertSequenceEqual(qs, [])\n", "    def test_ticket_23622_multi_filter(self):\n        \"\"\"\n        Test ticket_23622 with multiple filters on the related field in a\n        subquery.\n        \"\"\"\n        a1 = Ticket23605A.objects.create()\n        a2 = Ticket23605A.objects.create()\n        c1 = Ticket23605C.objects.create(field_c0=0.0)\n        Ticket23605B.objects.create(\n            modela_fk=a1, field_b0=123,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a1, field_b0=23,\n            field_b1=False,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a1, field_b0=234,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a1, field_b0=12,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a2, field_b0=567,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a2, field_b0=76,\n            field_b1=False,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a2, field_b0=7,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=", "    def test_ticket_24935(self):\n        \"\"\"\n        Regression test for #24935: Ensure that ordering by a field in a\n        subquery works correctly when the subquery is filtered by the same field,\n        and when the main query also filters by that field.\n        \"\"\"\n        o1 = Order.objects.create(created=datetime.date.today())\n        o2 = Order.objects.create(created=datetime.date.today() - datetime.timedelta(days=1))\n        oi1 = OrderItem.objects.create(order=o1, quantity=10)\n        oi2 = OrderItem.objects.create(order=o2, quantity=5)\n\n        qs = Order.objects.filter(\n            Q(orderitem__quantity__gt=5),\n            orderitem__order=Subquery(OrderItem.objects.filter(order__created=F('order__created')).order_by('-quantity'))\n        ).distinct()\n        self.assertSequenceEqual(qs, [o1])\n"], "sample_347": ["    def test_get_current_timezone_name_no_default(self):\n        with override_settings(TIME_ZONE=''):\n            with self.assertRaises(ValueError):\n                timezone.get_current_timezone_name()\n", "    def test_make_naive_zoneinfo_fold(self):\n        # Test if fold is correctly applied when making naive\n        ambiguous = datetime.datetime(2015, 10, 25, 2, 30)\n\n        std = timezone.make_naive(datetime.datetime(2015, 10, 25, 2, 30, fold=0, tzinfo=PARIS_ZI), PARIS_ZI)\n        dst = timezone.make_naive(datetime.datetime(2015, 10, 25, 2, 30, fold=1, tzinfo=PARIS_ZI), PARIS_ZI)\n\n        self.assertEqual(std, datetime.datetime(2015, 10, 25, 2, 30))\n        self.assertEqual(dst, datetime.datetime(2015, 10, 25, 2, 30))\n", "    def test_localtime_zoneinfo_dst(self):\n        paris = timezone.get_default_timezone()\n        naive = datetime.datetime(2023, 10, 29, 1, 30)\n        with self.assertRaises(pytz.AmbiguousTimeError): \n            timezone.localtime(naive, timezone=paris)\n        late = timezone.localtime(naive, timezone=paris)\n        early = timezone.localtime(naive.replace(fold=1), timezone=paris)\n        self.assertEqual(late.tzinfo, paris)\n        self.assertEqual(early.tzinfo, paris)\n\n        self.assertEqual(late.astimezone(timezone.utc), datetime.datetime(2023, 10, 28, 23, 30, tzinfo=timezone.utc))\n        self.assertEqual(early.astimezone(timezone.utc), datetime.datetime(2023, 10, 29, 0, 30, tzinfo=timezone.utc))\n", "    def test_get_fixed_timezone_invalid_input(self):\n        with self.assertRaises(ValueError):\n            timezone.get_fixed_timezone(\"invalid\")\n", "    def test_make_naive_zoneinfo_no_tzinfo(self):\n        naive = datetime.datetime(2015, 1, 1, 0, 0, 1)\n        with self.assertRaises(ValueError):\n            timezone.make_naive(naive, PARIS_ZI)\n\n"], "sample_817": ["    def test_sparse_variance_threshold():\n        # Test VarianceThreshold with sparse matrices and different thresholds.\n        sparse_data = csr_matrix(data)\n        for threshold in [0.1, 1.0, 2.0]:\n            selector = VarianceThreshold(threshold=threshold).fit(sparse_data)\n            # Transform the data and check the shape\n            transformed_data = selector.transform(sparse_data)\n            assert transformed_data.shape[1] <= sparse_data.shape[1]\n", "    def test_threshold_with_sparse_matrix():\n        # Test VarianceThreshold with sparse matrix and custom threshold.\n        for X in [csr_matrix(data), csc_matrix(data), bsr_matrix(data)]:\n            selector = VarianceThreshold(threshold=0.5).fit(X)\n            assert_array_equal([0, 1, 3], selector.get_support(indices=True))\n", "def test_sparse_threshold():\n    # Test VarianceThreshold on sparse matrices with a custom threshold\n    X = csr_matrix([[0, 1, 2],\n                   [0, 2, 2],\n                   [1, 1, 2],\n                   [0, 2, 0]])\n    selector = VarianceThreshold(threshold=0.6)\n    selector.fit(X)\n    assert_array_equal([False, True], selector.get_support(indices=True))\n", "def test_get_support():\n    # Test VarianceThreshold.get_support() method\n    sel = VarianceThreshold().fit(data)\n    support = sel.get_support()\n    assert_equal(support, [False, True, True, True, True])\n\n    sel = VarianceThreshold(threshold=.4).fit(data)\n    support = sel.get_support()\n    assert_equal(support, [False, True, False, True, True])\n\n", "    def test_sparse_variance_threshold():\n        # Test sparsity is preserved with VarianceThreshold.\n\n        X = csr_matrix(data)\n        X = VarianceThreshold(threshold=.4).fit_transform(X)\n        assert isinstance(X, csr_matrix)\n\n\n"], "sample_501": ["    def test_legend_frameon_after_set_visible():\n        fig, ax = plt.subplots()\n        ax.plot([1, 2], [3, 4], label='line')\n        leg = ax.legend()\n        leg.set_visible(False)\n        leg.set_frameon(True)\n        assert leg.get_frame() is not None\n        leg.set_visible(True)\n        assert leg.get_frame().get_visible()\n", "def test_legend_ncol_tight_layout():\n    # Test tight_layout() behavior with legend.ncol > 1.\n\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='line1')\n    ax.plot(range(10), label='line2')\n    ax.plot(range(10), label='line3')\n    leg = ax.legend(ncol=3)\n\n    fig.tight_layout()\n    assert fig.bbox_inches.width > leg.get_window_extent().width\n\n", "def test_legend_title_fontproperties():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line')\n    leg = ax.legend(title='Title', title_fontproperties={'size': 20})\n    assert leg.get_title().get_fontsize() == 20\n", "    def test_legend_patch_artist():\n        # Test that a Patch artist can be used with the legend.\n        fig, ax = plt.subplots()\n        rect = plt.Rectangle((0.1, 0.1), 0.5, 0.5, facecolor='blue')\n        ax.add_patch(rect)\n        ax.legend([rect], [\"Rectangle\"])\n", "    def test_legend_box_position():\n        fig, ax = plt.subplots()\n        ax.plot([0, 1], [0, 1], label='line')\n        leg = ax.legend()\n        leg.set_bbox_to_anchor([0.5, 0.5])\n        assert leg.get_bbox_to_anchor() == (0.5, 0.5)\n\n"], "sample_657": ["    def test_pytest_param_with_mark_and_id(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.parametrize(\n                \"input, expected\",\n                [\n                    pytest.param(1, 2, marks=pytest.mark.slow, id=\"slow test\"),\n                    pytest.param(2, 4, id=\"default\"),\n                ],\n            )\n                assert input * 2 == expected\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.assert_outcomes(passed=2)\n        result.stdout.fnmatch_lines([\"*slow test PASSED*\"])\n        result.stdout.fnmatch_lines([\"*default PASSED*\"])\n\n", "    def test_mark_decorator_scope_and_inheritance(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            class TestClass(object):\n                @pytest.mark.a\n                    pass\n\n                class TestNestedClass(object):\n                    @pytest.mark.b\n                        pass\n\n                        pass\n        \"\"\"\n        )\n        reprec = testdir.inline_run('-m', 'a', '--show-markers')\n        passed, skipped, failed = reprec.listoutcomes()\n        assert len(passed) == 1\n        assert len(skipped) == 0\n        assert len(failed) == 0\n        assert 'a' in passed[0].nodeid\n\n        reprec = testdir.inline_run('-m', 'b', '--show-markers')\n        passed, skipped, failed = reprec.listoutcomes()\n        assert len(passed) == 1\n        assert len(skipped) == 0\n        assert len(failed) == 0\n        assert 'b' in passed[0].nodeid\n", "    def test_marker_expression_with_complex_condition(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.parametrize(\"value\", [0, 1, 2, 3])\n                if value > 1 and value < 3:\n                    pass\n                else:\n                    pytest.skip(\"Skipping for value: {}\".format(value))\n            \"\"\"\n        )\n\n        rec = testdir.inline_run()\n        rec.assertoutcome(skipped=2, passed=2)\n", "    def test_pytest_parametrize_idfn_error_handling(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n                raise ValueError(f\"Error processing fixture value: {fixture_value}\")\n\n            @pytest.mark.parametrize('x', [1, 2], ids=idfn)\n                pass\n            \"\"\"\n        )\n        result = testdir.runpytest()\n\n    \n", "    def test_pytest_param_none_id_preserved(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.parametrize(\"arg\", [pytest.param(None)], ids=[\"my_id\"])\n                print(arg)\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.assert_outcomes(passed=1)\n        result.stdout.fnmatch_lines([\"*my_id*\"])\n\n"], "sample_748": ["    def test_grid_search_refit_with_sparse_data():\n        X = sparse.csr_matrix([[1, 2], [3, 4]])\n        y = [0, 1]\n        clf = LogisticRegression()\n        gs = GridSearchCV(clf, {'C': [0.1, 1.0]}, refit=True)\n        gs.fit(X, y)\n        assert_true(hasattr(gs, 'best_estimator_'))\n        assert_true(isinstance(gs.best_estimator_, LogisticRegression))\n\n", "    def test_grid_search_with_precomputed_kernel():\n        # Test grid search with a precomputed kernel\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([0, 1, 0])\n        precomputed_kernel = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n        svc = SVC(kernel='precomputed')\n        grid_search = GridSearchCV(svc, {'C': [0.1, 1]}, cv=2)\n        grid_search.fit(precomputed_kernel, y)\n        assert grid_search.best_score_ is not None\n\n", "def test_grid_search_with_precomputed_kernel():\n    # Test GridSearchCV with precomputed kernel\n\n    X = np.arange(20).reshape(5, -1)\n    y = [0, 0, 1, 1, 1]\n    # Precompute kernel\n    kernel = \"precomputed\"\n    X_train = np.dot(X, X.T)\n\n    clf = SVC(kernel=kernel)\n    grid_search = GridSearchCV(clf, {'C': [0.1, 1, 10]}, cv=2)\n    grid_search.fit(X_train, y)\n    assert grid_search.best_score_ is not None\n\n", "compilation error", "        def __call__(self, estimator, X, y):\n            raise ValueError(\"Failing scorer\")\n\n"], "sample_473": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_497": ["    def test_scalarformatter_rounding(self):\n        formatter = mticker.ScalarFormatter()\n        tests = [(12.34567, 2, '12.35'),\n                 (12.34567, 1, '12.3'),\n                 (12.34567, 0, '12'),\n                 (-12.34567, 2, '-12.35'),\n                 (-12.34567, 1, '-12.3'),\n                 (-12.34567, 0, '-12')]\n        for val, decimals, expected in tests:\n            assert formatter.format_scalar(val, decimals) == expected\n", "    def test_scalarformatter_format(self, kwargs, expected_label):\n        formatter = mticker.ScalarFormatter(**kwargs)\n        assert formatter.format_data(1.0) == expected_label\n", "    def test_remove_overlap_dates_with_None(self):\n        import numpy as np\n        import matplotlib.dates as mdates\n\n        t = np.arange(\"2018-11-03\", \"2018-11-06\", dtype=\"datetime64\")\n        x = np.ones(len(t))\n\n        fig, ax = plt.subplots()\n        ax.plot(t, x)\n\n        ax.xaxis.set_major_locator(mdates.DayLocator())\n        ax.xaxis.set_major_formatter(mdates.DateFormatter('\\n%a'))\n\n        ax.xaxis.set_minor_locator(mdates.HourLocator((0, 6, 12, 18)))\n        ax.xaxis.set_minor_formatter(mdates.DateFormatter('%H:%M'))\n        # force there to be extra ticks\n        ax.xaxis.get_minor_ticks(15)\n\n        # set but with None\n        ax.xaxis.remove_overlapping_locs = None\n\n        # check that getter/setter exists\n        current = ax.xaxis.remove_overlapping_locs\n        assert (current == ax.xaxis.get_remove_overlapping_locs())\n        plt.setp(ax.xaxis, remove_overlapping_locs=current)\n        new = ax.xaxis.remove_overlapping_locs\n        assert (new == ax.xaxis.remove_overlapping_locs)\n\n        # check that the getter/setter is properly set to None\n        assert ax.xaxis.remove_overlapping_locs is None\n", "    def test_EngFormatter_neg_zero(self):\n        fmt = mticker.EngFormatter()\n        assert fmt(-0.0) == '0'\n        assert fmt(-0) == '0'\n", "    def test_Locator_formatter_inheritance():\n        \"\"\"Test that Locator and Formatter inherit expected methods.\"\"\"\n        for cls in [matplotlib.ticker.Locator, matplotlib.ticker.Formatter]:\n            assert hasattr(cls, '__init__')\n            assert hasattr(cls, '__call__')\n            assert hasattr(cls, '__repr__')\n        # ... more tests for specific expected methods in Locators/Formatters\n"], "sample_422": ["    def setUpTestData(cls):\n        cls.author1 = Author.objects.create(name=\"Author1\")\n        cls.book1 = Book.objects.create(title=\"Book1\", first_author=cls.author1)\n        cls.author2 = Author.objects.create(name=\"Author2\")\n        cls.book2 = Book.objects.create(title=\"Book2\", first_author=cls.author2)\n\n        cls.genre1 = Genre.objects.create(name=\"Genre1\")\n        cls.genre2 = Genre.objects.create(name=\"Genre2\")\n        cls.book1.genres.add(cls.genre1)\n        cls.book2.genres.add(cls.genre2)\n", "    def test_prefetch_related_empty_list(self):\n        with self.assertNumQueries(1):\n            books = Book.objects.filter(pk__in=[]).prefetch_related(\n                Prefetch(\"authors\", queryset=Author.objects.all())\n            )\n        self.assertEqual(books.count(), 0)\n\n", "    def test_prefetch_related_empty_queryset(self):\n        with self.assertNumQueries(1):\n            books = Book.objects.filter(pk__in=[]).prefetch_related(\"authors\")\n        self.assertEqual(list(books), [])\n", "    def test_prefetch_related_empty_results(self):\n        author = Author.objects.create(name=\"Jane Austen\")\n        with self.assertNumQueries(2):\n            book = Book.objects.create(title=\"Sense and Sensibility\", first_time_authors=author)\n        with self.assertNumQueries(0):\n            self.assertEqual(book.first_time_authors.all(), [author])\n\n\n\n", "    def test_prefetch_related_ordering(self):\n        with self.assertNumQueries(3):\n            books = list(\n                Book.objects.prefetch_related(\n                    Prefetch(\"authors\", queryset=Author.objects.order_by(\"name\"), to_attr=\"ordered_authors\"),\n                ).order_by(\"title\")\n            )\n        for book in books:\n            with self.subTest(book=book):\n                self.assertEqual(\n                    book.ordered_authors.values_list(\"name\", flat=True),\n                    sorted(book.authors.values_list(\"name\", flat=True))\n                )\n"], "sample_753": ["def test_logreg_predict_proba_empty():\n    # Test that predict_proba handles empty inputs correctly.\n\n    X = np.array([])\n    y = np.array([])\n    lr = LogisticRegression()\n    lr.fit(X, y) \n    with pytest.warns(UserWarning):\n        proba = lr.predict_proba(X)\n    assert_array_equal(proba.shape, (0, 1)) \n", "    def test_lbfgs_sparse_matrix():\n        # Test that lbfgs solver works with sparse matrices\n        X, y = make_classification(n_samples=100, n_features=20,\n                                   random_state=0)\n        X_sparse = sparse.csr_matrix(X)\n        clf = LogisticRegression(solver='lbfgs', max_iter=100)\n        clf.fit(X_sparse, y)\n\n\n", "    def test_logreg_predict_proba_binary():\n        # Test predict_proba for binary classification with liblinear solver\n\n        X, y = make_classification(n_samples=50, n_features=20, random_state=0)\n        clf = LogisticRegression(solver='liblinear')\n        clf.fit(X, y)\n\n        probas = clf.predict_proba(X)\n        assert_almost_equal(probas.shape, (50, 2))\n        assert_almost_equal(np.sum(probas, axis=1), np.ones(50))\n", "    def test_logreg_coef_stability():\n        # Test that the coefficients are stable across different random\n        # initializations and that the warning about the convergence is\n        # not raised for n_iter = 1.\n        X, y = make_classification(n_samples=100, n_features=20, random_state=0)\n        clf = LogisticRegression(solver='sag', random_state=0, max_iter=1)\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            clf.fit(X, y)\n        coefs1 = clf.coef_.copy()\n        clf = LogisticRegression(solver='sag', random_state=42, max_iter=1)\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            clf.fit(X, y)\n        coefs2 = clf.coef_.copy()\n        assert_allclose(coefs1, coefs2, rtol=1e-5)\n", "    def test_logreg_intercept_scaling_ovr():\n        # Test that intercept scaling works correctly for ovr with\n        # different solver when fit_intercept=True\n        X, y = iris.data, iris.target\n        for solver in ['liblinear', 'newton-cg', 'saga', 'lbfgs']:\n            for intercept_scaling in [0.1, 1, 10]:\n                clf = LogisticRegression(solver=solver,\n                                         intercept_scaling=intercept_scaling,\n                                         fit_intercept=True,\n                                         multi_class='ovr', random_state=42)\n                clf.fit(X, y)\n                assert_array_almost_equal(\n                    clf.coef_,\n                    LogisticRegression(solver=solver,\n                                       intercept_scaling=1,\n                                       fit_intercept=True,\n                                       multi_class='ovr',\n                                       random_state=42).fit(X, y).coef_)\n"], "sample_738": ["    def test_vectorizers_stop_words_empty_list(self):\n        # test that empty list for stop_words is handled correctly\n        stop_words = []\n        for vec in [CountVectorizer(stop_words=stop_words),\n                    TfidfVectorizer(stop_words=stop_words),\n                    HashingVectorizer(stop_words=stop_words)]:\n            vec.fit(JUNK_FOOD_DOCS)\n            assert_true(vec.get_feature_names())\n", "    def test_countvectorizer_fit_transform_empty_document():\n        corpus = [\"\", \"This is a document\", \"\"]\n        vectorizer = CountVectorizer()\n        result = vectorizer.fit_transform(corpus)\n        assert_equal(result.shape[0], len(corpus))\n        assert_equal(result.shape[1], len(vectorizer.get_feature_names()))\n", "    def test_tfidfvectorizer_lowercase():\n        # test that lowercase=False works as advertised\n\n        # create a corpus with mixed capitalized words\n        corpus = [\n            \"The quick BROWN Fox jumps over THE lazy dog.\",\n            \"Another Sentence with SOME capitalized Words.\"\n        ]\n\n        # create a TfidfVectorizer without lowercase\n        vect = TfidfVectorizer(lowercase=False)\n        X = vect.fit_transform(corpus)\n \n        # check that the vocabulary contains both cases\n        assert \"The\" in vect.get_feature_names()\n        assert \"THE\" in vect.get_feature_names()\n\n        # check that the IDF reflects the case sensitivity\n        assert vect.idf_[vect.vocabulary_[\"The\"]] != vect.idf_[vect.vocabulary_[\"THE\"]]\n", "    def test_vectorizer_empty_vocabulary(self):\n        # Test behavior when vocabulary is empty\n        vect = CountVectorizer(vocabulary=[])\n        with pytest.raises(ValueError) as excinfo:\n            vect.fit(JUNK_FOOD_DOCS)\n        assert \"Vocabulary is empty\" in str(excinfo.value)\n\n", "    def test_vectorizer_empty_vocabulary():\n        # Test the behavior of CountVectorizer and TfidfVectorizer when\n        # presented with an empty vocabulary.\n        for vec_class in [CountVectorizer, TfidfVectorizer]:\n            vec = vec_class(vocabulary=[])\n            \n            with pytest.raises(ValueError) as excinfo:\n                vec.fit(ALL_FOOD_DOCS)\n            assert \"Vocabulary cannot be empty\" in str(excinfo.value)\n\n"], "sample_376": ["    def test_not_finished_sentinel(self):\n        storage = self.storage_class(self.get_request())\n        response = self.get_response()\n\n        messages = [\n            Message(constants.INFO, 'message 1'),\n            Message(constants.INFO, 'message 2'),\n            Message(constants.INFO, 'message 3'),\n        ]\n        set_cookie_data(storage, messages + [CookieStorage.not_finished])\n        self.assertEqual(stored_cookie_messages_count(storage, response), 3)\n        self.assertFalse(storage.get_messages())\n        self.assertEqual(list(storage), messages)\n        \n        storage.update(response)\n        self.assertEqual(stored_cookie_messages_count(storage, response), 0)\n", "    def test_empty_cookie_encoding(self):\n        \"\"\"\n        Test the scenario where an empty list of messages is passed to\n        the encoder. This should create a valid cookie.\n        \"\"\"\n        storage = self.storage_class(self.get_request())\n        set_cookie_data(storage, [], encode_empty=True)\n        # Check if the cookie was set\n        self.assertIn(CookieStorage.cookie_name, storage.request.COOKIES)\n        # Check if the cookie's content is valid\n        self.assertIsNotNone(storage._decode(storage.request.COOKIES[CookieStorage.cookie_name]))\n", "    def test_cookie_size(self):\n        \"\"\"\n        Verify that the cookie size is within the allowed limit.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n        # Generate a large number of messages to exceed the cookie size limit.\n        messages = [\n            Message(constants.INFO, f\"message {i}\")\n            for i in range(100)\n        ]\n        for message in messages:\n            storage.add(constants.INFO, message.message)\n\n        storage.update(response)\n\n        cookie = response.cookies.get(storage.cookie_name)\n        self.assertIsNotNone(cookie)\n        self.assertLessEqual(len(cookie.value), CookieStorage.max_cookie_size)\n", "    def test_empty_cookie(self):\n        \"\"\"\n        Test that an empty cookie doesn't result in an error when trying to\n        decode messages.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n        set_cookie_data(storage, [], encode_empty=True)\n        self.assertEqual(self.stored_messages_count(storage, response), 0)\n", "    def test_empty_cookie(self):\n        \"\"\"\n        Ensure the encoder handles empty message lists correctly when\n        encoding a cookie.\n\n        Also check that the storage doesn't attempt to decode an empty string\n        and handles this gracefully.\n        \"\"\"\n        storage = self.storage_class(self.get_request())\n        # Test encoding\n        encoded = storage._encode([])\n        self.assertEqual(encoded, b'')\n        # Test empty cookie\n        set_cookie_data(storage, [], encode_empty=True)\n        self.assertEqual(list(storage), [])\n"], "sample_795": ["    def test_check_estimator_deprecated_methods():\n        class TestEstimatorWithDeprecatedMethod(BaseEstimator):\n            @deprecated(\"Deprecated for testing purposes\")\n                return self\n\n        check_estimator(\"TestEstimatorWithDeprecatedMethod\",\n                        TestEstimatorWithDeprecatedMethod())\n", "def test_check_estimator_deprecation_warning():\n    class DeprecatedEstimator(BaseEstimator):\n        @deprecated(\"Deprecated for testing purposes.\")\n            return self\n\n    with warnings.catch_warnings(record=True) as w:\n        check_estimator(DeprecatedEstimator())\n        assert len(w) == 1\n        assert \"Deprecated for testing purposes\" in str(w[0].message)\n\n", "    def test_check_estimator_class_weight_default():\n        # check that estimators with default class_weight='balanced'\n        # correctly handle it (e.g., do not raise)\n\n        check_class_weight_balanced_linear_classifier(\n            'estimator_name', SVC(class_weight='balanced'))\n", "    def test_check_estimator_with_deprecation_warning():\n        # check that check_estimator can handle estimators with DeprecationWarning\n        # without raising an AssertionError\n        @deprecated(\"Deprecated for testing purposes\")\n        class DeprecatedEstimator(BaseEstimator):\n                return self\n\n        check_estimator(DeprecatedEstimator())\n", "    def test_check_estimator_sparse_input():\n        # check if estimators handle sparse input correctly\n        random_state = np.random.RandomState(0)\n        X = sp.csr_matrix(random_state.rand(10, 5))\n        y = random_state.randint(0, 2, size=10)\n        msg = \"Estimator should handle sparse input correctly\"\n        assert_raises_regex(AssertionError, msg, check_estimator,\n                            LinearRegression(), X=X, y=y)\n        \n\n"], "sample_28": ["    def test_header_fromstring_with_trailing_whitespace():\n        \"\"\"\n        Test parsing headers from strings with trailing whitespace.\n        \"\"\"\n        header_string = \"\"\"", "    def test_header_set_duplicate_keywords(self):\n        h = fits.Header()\n        h['FOO'] = 1\n        with pytest.raises(ValueError):\n            h['FOO'] = 2\n", "    def test_header_fromstring():\n        h = fits.Header.fromstring(\n            \"\"\"", "    def test_empty_header():\n        \"\"\"Test creating and working with an empty Header object.\"\"\"\n        h = fits.Header()\n        assert len(h) == 0\n        assert list(h) == []\n        assert h.cards == []\n        assert h.ascard is None\n        assert h.comment == ''\n        assert str(h) == ''\n        h['TEST'] = 123\n        assert len(h) == 1\n        assert list(h.keys) == ['TEST']\n        assert h['TEST'] == 123\n\n", "    def test_header_get_item_wildcard_with_comment(self):\n        hdu = fits.PrimaryHDU(header=self._test_header)\n        self._test_header.set(\"DP1.COMMENT\", \"This is a comment\", after=\"DP1.NAXIS\")\n        assert hdu.header[\"DP1.*\"] == [\n            fits.Card(\"DP1\", value=2.0, comment=\"NAXIS: 2\"),\n            fits.Card(\"DP1.COMMENT\", value=\"This is a comment\"),\n            fits.Card(\"DP1\", value=2, comment=\"NAUX: 2\"),\n            fits.Card(\"DP1\", value=0, comment=\"AUX.1.COEFF.0: 0\"),\n            fits.Card(\"DP1\", value=1, comment=\"AUX.1.POWER.0: 1\"),\n            fits.Card(\"DP1\", value=0.00048828125, comment=\"AUX.1.COEFF.1: 0.00048828125\"),\n            fits.Card(\"DP1\", value=1, comment=\"AUX.1.POWER.1: 1\"),\n        ]\n"], "sample_342": ["    def test_empty_search_field(self):\n        self.selenium.get(self.live_server_url + reverse('autocomplete_admin:admin_views_answer_add'))\n        elem = self.selenium.find_element_by_css_selector('.select2-selection')\n        elem.click()\n        results = self.selenium.find_element_by_css_selector('.select2-results')\n        self.assertTrue(results.is_displayed())\n        option = self.selenium.find_element_by_css_selector('.select2-results__option')\n        self.assertEqual(option.text, 'No results found')\n", "    def test_field_does_not_exist_permission_denied(self):\n        request = self.factory.get(self.url, {'term': 'is', **self.opts, 'field_name': 'does_not_exist'})\n        request.user = self.superuser\n        with self.assertRaises(PermissionDenied):\n            AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "    def test_search_results_limited_by_permission(self):\n        \"\"\"\n        Users require the change permission for the related model to see all\n        autocomplete results.\n        \"\"\"\n        class QuestionAdmin(admin.ModelAdmin):\n            search_fields = ['question']\n\n        group = Group.objects.create(name='LimitedViewers')\n        user = User.objects.create_user(\n            username='limited_user', password='secret', email='limited@example.com',\n        )\n        user.groups.add(group)\n\n        Permission.objects.filter(codename='view_question').update(\n            group=group\n        )\n        q1 = Question.objects.create(question='Question 1')\n        q2 = Question.objects.create(question='Question 2')\n        with model_admin(Question, QuestionAdmin):\n            with self.subTest(user=user):\n                self.selenium.get(self.live_server_url + reverse('autocomplete_admin:admin_views_answer_add'))\n                elem = self.selenium.find_element_by_css_selector('.select2-selection')\n                elem.click()\n                results = self.selenium.find_elements_by_css_selector('.select2-results__option')\n                self.assertEqual(len(results), 2)\n        with model_admin(Question, QuestionAdmin):\n            with self.subTest(user=self.superuser):\n                self.selenium.get(self.live_server_url + reverse('autocomplete_admin:admin_views_answer_add'))\n                elem = self.selenium.find_element_by_css_selector('.select2-selection')\n                elem.click()\n                results = self.selenium.find_elements_by_css_selector('.select2-results__option')\n                self.assertEqual(len(results), PAGINATOR_SIZE + 1)\n", "    def test_permission_denied_for_add_another(self):\n        class PKChildNoAdd(PKChild):\n            class Meta:\n                permissions = []\n\n        with model_admin(PKChild, PKChildAdmin):\n            self.selenium.get(self.live_server_url + reverse('admin:admin_views_pkchild_add'))\n            self.selenium.find_element_by_link_text('Add another').click()\n            self.assertTrue(self.selenium.find_element_by_css_selector('.errorlist').is_displayed())\n", "    def test_permission_denied_when_field_is_hidden(self):\n        Author.objects.create(name=\"Frida Kahlo\", alive=False)\n        opts = {\n            'app_label': Answer._meta.app_label,\n            'model_name': Answer._meta.model_name,\n            'field_name': 'author',\n        }\n        request = self.factory.get(self.url, {'term': 'frida', **opts})\n        request.user = self.user\n        with self.assertRaises(PermissionDenied):\n            AutocompleteJsonView.as_view(**self.as_view_args)(request)\n"], "sample_19": ["def test_swapaxes_different_val_roundtrip():\n    w = wcs.WCS(naxis=3)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\", \"FREQ\"]\n    w.wcs.crpix = [32.5, 16.5, 1.0]\n    w.wcs.crval = [5.63, -72.05, 1.0]\n    w.wcs.pc = [[5.9e-06, 1.3e-05, 0.0], [-1.2e-05, 5.0e-06, 0.0], [0.0, 0.0, 1.0]]\n    w.wcs.cdelt = [1.0, 1.0, 1.0]\n    w.wcs.set()\n    axes_order = [3, 2, 1]\n    axes_order0 = list(i - 1 for i in axes_order)\n    ws = w.sub(axes_order)\n    imcoord = np.array([3, 5, 7])\n    imcoords = imcoord[axes_order0]\n    val_ref = w.wcs_pix2world([imcoord], 0)[0]\n    val_swapped = ws.wcs_pix2world([imcoords], 0)[0]\n\n    # check round-tripping:\n\n    assert np.allclose(\n        w.wcs_world2pix([val_swapped], 0)[0][axes_order0], imcoords, rtol=0, atol=1e-8\n    )\n", "    def test_swapaxes_diff_val(self):\n        w = wcs.WCS(naxis=3)\n        w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\", \"FREQ\"]\n        w.wcs.crpix = [32.5, 16.5, 1.0]\n        w.wcs.crval = [5.63, -72.05, 1.0]\n        w.wcs.pc = [[5.9e-06, 1.3e-05, 0.0], [-1.2e-05, 5.0e-06, 0.0], [0.0, 0.0, 1.0]]\n        w.wcs.cdelt = [1.0, 1.0, 1.0]\n        w.wcs.set()\n        axes_order = [2, 3, 1]\n        ws = w.sub(axes_order)\n        imcoord = np.array([3, 5, 7]) \n        imcoords = imcoord[axes_order - 1]\n        val_ref = w.wcs_pix2world([imcoord], 0)[0]\n        val_swapped = ws.wcs_pix2world([imcoords], 0)[0]\n\n        # check original axis and swapped give same results\n        assert not np.allclose(val_ref, val_swapped)\n\n", "    def test_swapaxes_different_val_roundtrip():\n        w = wcs.WCS(naxis=3)\n        w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\", \"FREQ\"]\n        w.wcs.crpix = [32.5, 16.5, 1.0]\n        w.wcs.crval = [5.63, -72.05, 1.0]\n        w.wcs.pc = [[5.9e-06, 1.3e-05, 0.0], [-1.2e-05, 5.0e-06, 0.0], [0.0, 0.0, 1.0]]\n        w.wcs.cdelt = [1.0, 1.0, 1.0]\n        w.wcs.set()\n        axes_order = [3, 2, 1]\n        axes_order0 = list(i - 1 for i in axes_order)\n        ws = w.sub(axes_order)\n        imcoord = np.array([3, 5, 7])\n        imcoords = imcoord[axes_order0]\n        val_ref = w.wcs_pix2world([imcoord], 0)[0]\n        val_swapped = ws.wcs_pix2world([imcoords], 0)[0]\n        # intentionally use different values from original\n        imcoord2 = np.array([10, 15, 20])\n        imcoords2 = imcoord2[axes_order0]\n        val_ref2 = w.wcs_pix2world([imcoord2], 0)[0]\n        val_swapped2 = ws.wcs_pix2world([imcoords2], 0)[0]\n        # check original axis and swapped give different results\n        assert not np.allclose(val_ref[axes_order0], val_swapped, rtol=0, atol=1e-8)\n        assert not np.allclose(val_ref2[axes_order0], val_swapped2, rtol=0, atol=1e-8)\n\n        # check", "compilation error", "    def test_swapaxes_different_val_roundtrip(self):\n        w = wcs.WCS(naxis=3)\n        w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\", \"FREQ\"]\n        w.wcs.crpix = [32.5, 16.5, 1.0]\n        w.wcs.crval = [5.63, -72.05, 1.0]\n        w.wcs.pc = [[5.9e-06, 1.3e-05, 0.0], [-1.2e-05, 5.0e-06, 0.0], [0.0, 0.0, 1.0]]\n        w.wcs.cdelt = [1.0, 1.0, 1.0]\n        w.wcs.set()\n        axes_order = [3, 2, 1]\n        axes_order0 = list(i - 1 for i in axes_order)\n        ws = w.sub(axes_order)\n        imcoord = np.array([3, 5, 7])\n        imcoords = imcoord[axes_order0]\n        val_ref = w.wcs_pix2world([imcoord], 0)[0]\n        val_swapped = ws.wcs_pix2world([imcoords], 0)[0]\n\n        # check round-tripping:\n        assert np.allclose(w.wcs_world2pix([val_swapped], 0)[0], imcoords, rtol=0, atol=1e-8)\n\n"], "sample_118": ["    def test_in_with_empty_inner_queryset(self):\n        self.assertQuerysetEqual(\n            Article.objects.filter(author__in=Article.objects.none()),\n            []\n        )\n", "    def test_nested_outerref_lhs_relatedfield(self):\n        tag = Tag.objects.create(name=self.au1.alias)\n        tag.articles.add(self.a1)\n        qs = Tag.objects.annotate(\n            has_author_alias_match=Exists(\n                Article.objects.annotate(\n                    author_exists=Exists(\n                        Author.objects.filter(alias=OuterRef('articles__author__alias'))\n                    ),\n                ).filter(author_exists=True)\n            ),\n        )\n        self.assertEqual(qs.get(has_author_alias_match=True), tag)\n", "    def test_exact_transform_with_function_rhs(self):\n        Article.objects.create(headline='Article 1', pub_date=datetime(2005, 11, 20))\n        Article.objects.create(headline='Article 2', pub_date=datetime(2005, 11, 21))\n        self.assertCountEqual(\n            Article.objects.filter(headline__exact=Func(F('headline'), function='TRIM')),\n            Article.objects.filter(headline__in=['Article 1', 'Article 2'])\n        )\n", "    def test_related_field_lookup_with_transform(self):\n        article = Article.objects.create(headline='Article headline', author=self.au1, slug='article-slug')\n        self.assertTrue(Article.objects.filter(author__name__startswith='Author').exists())\n        self.assertTrue(Article.objects.filter(author__name__endswith='1').exists())\n        self.assertEqual(\n            Article.objects.filter(author__name__startswith='Author').first(),\n            article\n        )\n        self.assertEqual(\n            Article.objects.filter(author__name__endswith='1').first(),\n            article\n        )\n", "    def test_isnull_with_lhs_alias(self):\n        qs = Article.objects.annotate(\n            author_alias=F('author__name'),\n        ).filter(\n            author_alias__isnull=False,\n        )\n        self.assertEqual(len(qs), 7)\n\n"], "sample_305": ["    def test_self_ref_fk_with_filter(self):\n        t1 = SelfRefFK.objects.create(name='t1')\n        t2 = SelfRefFK.objects.create(name='t2', parent=t1)\n        SelfRefFK.objects.create(name='t3', parent=t1)\n        self.assertQuerysetEqual(\n            SelfRefFK.objects.filter(parent=t1).annotate(num_children=Count('children')).order_by('name'),\n            [('t2', 0), ('t3', 0)],\n            lambda x: (x.name, x.num_children)\n        )\n", "    def test_ticket_24748_with_filter(self):\n        t1 = SelfRefFK.objects.create(name='t1')\n        SelfRefFK.objects.create(name='t2', parent=t1)\n        SelfRefFK.objects.create(name='t3', parent=t1)\n        self.assertQuerysetEqual(\n            SelfRefFK.objects.filter(parent__isnull=False).annotate(num_children=Count('children')).order_by('name'),\n            [('t2', 0), ('t3', 0)],\n            lambda x: (x.name, x.num_children)\n        )\n\n", "    def test_double_self_join(self):\n        t1 = SelfRefFK.objects.create(name='t1')\n        t2 = SelfRefFK.objects.create(name='t2', parent=t1)\n        t3 = SelfRefFK.objects.create(name='t3', parent=t1)\n        SelfRefFK.objects.create(name='t4', parent=t2)\n        qs = SelfRefFK.objects.annotate(\n            num_children=Count('children'),\n            num_grandchildren=Count('children__children')\n        ).order_by('name')\n        self.assertQuerysetEqual(\n            qs,\n            [('t1', 2, 1), ('t2', 0, 0), ('t3', 0, 0)],\n            lambda x: (x.name, x.num_children, x.num_grandchildren)\n        )\n   \n", "    def test_self_referential_fk_on_reverse_relation(self):\n        t1 = SelfRefFK.objects.create(name='t1')\n        t2 = SelfRefFK.objects.create(name='t2', parent=t1)\n        t3 = SelfRefFK.objects.create(name='t3', parent=t1)\n        self.assertQuerysetEqual(\n            t1.children.annotate(num_children=Count('children')),\n            [('t2', 0), ('t3', 0)],\n            lambda x: (x.name, x.num_children)\n        )\n", "    def test_ticket_24748_exclude(self):\n        t1 = SelfRefFK.objects.create(name='t1')\n        SelfRefFK.objects.create(name='t2', parent=t1)\n        SelfRefFK.objects.create(name='t3', parent=t1)\n        self.assertQuerysetEqual(\n            SelfRefFK.objects.exclude(children__isnull=True).annotate(num_children=Count('children')).order_by('name'),\n            [('t1', 2)],\n            lambda x: (x.name, x.num_children)\n        )\n"], "sample_1026": ["def test_lambdify_with_evaluate_flag():\n    f = lambdify(x, x**2, evaluate=False)\n    assert isinstance(f(3), Expr)\n    assert f(3).evalf() == 9\n\n", "def test_lambdify_MatrixSymbol_issue_15578_transpose():\n    if not numpy:\n        skip(\"numpy not installed\")\n    A = MatrixSymbol('A', 2, 2)\n    A0 = numpy.array([[1, 2], [3, 4]])\n    f = lambdify(A, A.T)\n    assert numpy.allclose(f(A0), A0.T)\n", "def test_issue_19531():\n    f = lambdify((x, y), Piecewise((x, y > 0), (y, True)), \"numpy\")\n    assert f(1, 2) == 1\n    assert f(-1, -2) == -2\n", "def test_Piecewise_issue_17110():\n    x = Symbol('x')\n    f = Piecewise((1, x < 0), (2, True))\n    func = lambdify(x, f)\n    assert func(-1) == 1\n    assert func(1) == 2\n", "compilation error"], "sample_925": ["def test_mock_subclassing():\n    with mock(['sphinx.unknown']):\n        class Subclass(sys.modules['sphinx.unknown'].SomeClass):\n            pass\n        obj = Subclass()\n        assert isinstance(obj, Subclass)\n\n\n\n", "def test_mock_inheritance():\n    modname = 'sphinx.unknown'\n    with mock([modname]):\n        # Mocking a module with a base class\n        import_module(modname)\n        import_module(f'{modname}.base')\n        with pytest.raises(ImportError):\n            import_module(f'{modname}.derived')\n\n        # Mocking the base class\n        with mock([f'{modname}.base']):\n            import_module(f'{modname}.derived')\n            assert (f'{modname}.derived' in sys.modules)\n", "def test_mock_inheritance():\n    with mock(['test_module']):\n        import test_module\n        assert hasattr(test_module, 'ChildClass')\n        assert issubclass(test_module.ChildClass, test_module.ParentClass)\n\n", "def test_mock_with_existing_modules():\n    with mock(['sphinx.ext.autodoc.mock']):\n        # This should still work as the mock is for existing module\n        from sphinx.ext.autodoc.mock import _MockModule\n        assert isinstance(_MockModule, type)\n", "def test_mock_mocked_modules_are_invalidated_on_exit():\n    modname = 'sphinx.unknown'\n    with mock([modname]):\n        import_module(modname)\n        assert modname in sys.modules\n\n    assert modname not in sys.modules\n"], "sample_480": ["    def test_key_text_transform_from_lookup_empty_string(self):\n        qs = NullableJSONModel.objects.annotate(\n            empty_string=KT(\"value__empty_string\")\n        ).filter(empty_string__isnull=True)\n        self.assertSequenceEqual( qs, self.objs)\n", "    def test_key_text_transform_expression_invalid(self):\n        msg = \"Expression must contain key or index transforms.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            KT(F('value'), 1)\n        with self.assertRaisesMessage(ValueError, msg):\n            KT(Value(\"value\"))\n", "    def test_key_transform_with_null_value(self):\n        obj = NullableJSONModel.objects.create(value={\"a\": None, \"b\": 1})\n        self.assertEqual(\n            NullableJSONModel.objects.filter(\n                **{\"value__a__isnull\": False}\n            ).get(),\n            obj,\n        )\n\n", "    def test_key_transform_with_subquery(self):\n        subquery = NullableJSONModel.objects.filter(pk=OuterRef(\"pk\")).values(\n            \"value\"\n        )\n        qs = NullableJSONModel.objects.annotate(\n            subquery_value=Subquery(subquery)\n        ).filter(KT(\"subquery_value__baz__a\") == \"b\")\n        self.assertSequenceEqual(qs, [self.objs[7]])\n\n", "    def test_key_text_transform_from_lookup_isnull(self):\n        qs = NullableJSONModel.objects.annotate(b=KT(\"value__bax__foo\")).filter(\n            b__isnull=False,\n        )\n        self.assertSequenceEqual(qs, [self.objs[7]])\n"], "sample_372": ["    def test_include_with_trailing_slash(self):\n        url_patterns = [\n            path('test/', views.empty_view, name='test'),\n        ]\n        with self.assertRaisesMessage(ImproperlyConfigured, \"The included URLconf 'urlpatterns_reverse.tests.urls' does not appear to have any patterns\"):\n            include(url_patterns, namespace='test/')\n", "    def test_include_with_dot_in_namespace(self):\n        msg = 'Namespaces cannot contain dots.'\n        with self.assertRaisesMessage(ImproperlyConfigured, msg):\n            include(self.url_patterns, 'inc.app')\n", "    def test_include_empty_patterns(self):\n        self.assertEqual(include([]), ([], None, None))\n", "    def test_include_app_name_with_namespace(self):\n        self.assertEqual(\n            include(self.app_urls, namespace='custom-namespace'),\n            (self.app_urls, 'inc-app', 'custom-namespace')\n        )\n", "    def test_include_module_with_namespace(self):\n        with override_settings(ROOT_URLCONF='urlpatterns_reverse.urls_with_namespace'):\n            self.assertEqual(resolve('/inc-ns1/test3/inner/42/37/').view_name, 'inc-ns1:test3-inner-view')\n"], "sample_915": ["    def test_getdoc_inherited_method_docstring(app):\n        class Foo:\n                \"\"\"docstring.\"\"\"\n        \n        class Bar(Foo):\n            pass\n            \n        assert inspect.getdoc(Bar.meth, getattr, True, Bar, \"meth\") == \"docstring.\"\n", "    def test_ismethod(app):\n        from target.methods import Base, Inherited\n\n        assert inspect.ismethod(Base.meth) is False\n        assert inspect.ismethod(Base().meth) is True\n        assert inspect.ismethod(Inherited.meth) is False\n        assert inspect.ismethod(Inherited().meth) is True\n", "def test_stringify_signature_with_overloading(app):\n    from target.overloading import Overloaded\n\n    sig = inspect.signature(Overloaded.overload_me)\n    assert stringify_signature(sig) == '(*args, **kwargs)'\n", "    def test_getdoc_from_instance(app):\n\n        from target.methods import Base\n\n        b = Base()\n\n        assert inspect.getdoc(b.meth, getattr, False, Base, \"meth\") == \"method docstring\"\n        assert inspect.getdoc(b.meth, getattr, True, Base, \"meth\") == \"method docstring\"\n", "    def test_getdoc_with_bound_method():\n        class Foo:\n                \"\"\"docstring.\"\"\"\n        obj = Foo()\n        assert inspect.getdoc(obj.meth, getattr, False, Foo, \"meth\") == \"docstring.\"\n        assert inspect.getdoc(obj.meth, getattr, True, Foo, \"meth\") == \"docstring.\"\n\n"], "sample_496": ["    def test_custom_startproject_template_with_dot_in_path(self):\n        \"\"\"The startproject management command is able to use a project template\n        with dots in the path.\n        \"\"\"\n        template_path = os.path.join(custom_templates_dir, 'project.template')\n        args = ['startproject', '--template', template_path, 'dottestproject']\n        testproject_dir = os.path.join(self.test_dir, 'dottestproject')\n        self.addCleanup(shutil.rmtree, testproject_dir, True)\n\n        out, err = self.run_django_admin(args)\n        self.assertNoOutput(err)\n        self.assertTrue(os.path.isdir(testproject_dir))\n        self.assertTrue(os.path.exists(os.path.join(testproject_dir, 'manage.py')))\n", "    def test_diffsettings_output_context(self):\n        \"\"\"\n        --output=context emits settings diff in context mode.\n        \"\"\"\n        self.write_settings('settings_to_diff.py', sdict={'FOO': '\"bar\"', 'BAR': '\"baz\"'})\n        self.addCleanup(self.remove_settings, 'settings_to_diff.py')\n        args = ['diffsettings', '--settings=settings_to_diff', '--output=context']\n        out, err = self.run_manage(args)\n        self.assertNoOutput(err)\n        self.assertInOutput(out, '*** settings_to_diff.py')\n        self.assertInOutput(out, '--- settings_to_diff.py')\n        self.assertInOutput(out, '+ FOO = \\'bar\\'')\n        self.assertInOutput(out, '+ BAR = \\'baz\\'')\n", "    def test_custom_command(self):\n        self.write_settings('settings.py', apps=['admin_scripts'])\n        self.addCleanup(self.remove_settings, 'settings.py')\n        args = ['custom_command', '--option_a', 'value']\n        out, err = self.run_manage(args)\n        self.assertNoOutput(err)\n        self.assertOutput(out, \"EXECUTE:CustomCommand options=[('option_a', 'value')]\")\n", "    def test_suggestions_with_typo_and_multiple_commands(self):\n        args = ['runservert', '--settings=test_project.settings']\n        out, err = self.run_django_admin(args)\n        self.assertNoOutput(out)\n        self.assertOutput(err, \"Unknown command: 'runservert'. Did you mean runserver?\")\n", "    def test_invalid_app_label(self):\n        \"\"\"startapp errors out when given an invalid app label.\"\"\"\n        bad_label = 'test-app'\n        args = ['startapp', bad_label]\n        out, err = self.run_django_admin(args)\n        self.assertOutput(\n            err,\n            \"Error: 'test-app' is not a valid app label. Please make \"\n            \"sure the name is a valid identifier and doesn't \"\n            \"contain hyphens.\"\n        )\n\n"], "sample_45": ["    def test_trunc_with_different_output_field(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        if settings.USE_TZ:\n            start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n            end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n        self.create_model(end_datetime, start_datetime)\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(truncated_date=Trunc('start_datetime', 'day', output_field=DateField()), truncated_time=Trunc('start_datetime', 'hour', output_field=TimeField())).order_by('start_datetime'),\n            [\n                (start_datetime, start_datetime.date(), start_datetime.time().replace(minute=0, second=0, microsecond=0)),\n                (end_datetime, end_datetime.date(), end_datetime.time().replace(minute=0, second=0, microsecond=0)),\n            ],\n            lambda m: (m.start_datetime, m.truncated_date, m.truncated_time)\n        )\n", "    def test_trunc_with_explicit_timezone_and_output_field(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n        melb = pytz.timezone('Australia/Melbourne')\n        utc = timezone.utc\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated_melb=Trunc('start_datetime', 'day', output_field=DateTimeField(), tzinfo=melb),\n                truncated_utc=Trunc('start_datetime', 'day', output_field=DateTimeField(), tzinfo=utc)\n            ).order_by('start_datetime'),\n            [\n                (start_datetime,\n                 truncate_to(start_datetime.astimezone(melb), 'day', melb),\n                 truncate_to(start_datetime, 'day', tzinfo=utc)),\n                (end_datetime,\n                 truncate_to(end_datetime.astimezone(melb), 'day', melb),\n                 truncate_to(end_datetime, 'day', tzinfo=utc)),\n            ],\n            lambda m: (m.start_datetime, m.truncated_melb, m.truncated_utc)\n        )\n", "    def test_trunc_with_explicit_tzinfo_and_output_field(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n\n        melb = pytz.timezone('Australia/Melbourne')\n\n        qs = DTModel.objects.annotate(\n            truncated=Trunc('start_datetime', 'day', output_field=DateTimeField(), tzinfo=melb)\n        ).order_by('start_datetime')\n\n        for obj in qs:\n            self.assertEqual(obj.truncated.tzinfo, melb)\n", "compilation error", "compilation error"], "sample_663": ["def test_collect_skip_on_conftest_error(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n            pass\n        \"\"\"\n    )\n    conftest = testdir.makefile(\".ini\", content=\"\"\"\n        [pytest]\n        addopts = --strict\n        \"\"\")\n    result = testdir.runpytest(str(conftest), \"-v\")\n    result.stdout.fnmatch_lines([\"*SKIP*\"])\n\n", "    def test_collect_symlink_to_non_existant_package(testdir):\n        \"\"\"Test that collecting a symlink to a non-existent package doesn't cause errors (#5176).\"\"\"\n        sub = testdir.mkdir(\"sub\")\n        real = testdir.makepyfile(\n            real=\"\"\"\n            assert request.node.nodeid == \"real.py::test_nodeid\"\n        \"\"\"\n        )\n        symlink = sub.join(\"non_existent_package.py\")\n        symlink.mksymlinkto(real)\n        result = testdir.runpytest(\"-v\", str(sub))\n        result.stdout.fnmatch_lines([\"*1 passed in*\"])\n        assert result.ret == 0  \n", "    def test_collect_symlink_out_of_tree_different_rootdir(testdir):\n        \"\"\"Test collection of symlink via out-of-tree rootdir when rootdir is different.\"\"\"\n        sub = testdir.tmpdir.join(\"sub\")\n        real = sub.join(\"test_real.py\")\n        real.write(\n            textwrap.dedent(\n                \"\"\"\n                    # Should not contain sub/ prefix.\n                    assert request.node.nodeid == \"test_real.py::test_nodeid\"\n                \"\"\"\n            ),\n            ensure=True,\n        )\n\n        out_of_tree = testdir.tmpdir.join(\"out_of_tree\").ensure(dir=True)\n        symlink_to_sub = out_of_tree.join(\"symlink_to_sub\")\n        symlink_to_sub.mksymlinkto(sub)\n\n        result = testdir.runpytest(\n            \"-vs\", \"--rootdir=%s\" % testdir.tmpdir, symlink_to_sub\n        )\n        result.stdout.fnmatch_lines(\n            [\n                # Should not contain \"sub/\"!\n                \"test_real.py::test_nodeid PASSED\"\n                \n            ]\n        )\n        assert result.ret == 0\n\n", "    def test_collect_symlink_to_nonexistent_file(testdir):\n        sub = testdir.mkdir(\"sub\")\n        sub.ensure(\"test_broken.py\").mksymlinkto(\"nonexistent.py\")\n\n        result = testdir.runpytest(\"-v\", str(sub))\n        result.stdout.fnmatch_lines([\"*ERROR collecting test_symlink_to_nonexistent_file.py*\"])\n\n        # The test should fail, but not crash due to the broken symlink.\n        assert result.ret == ExitCode.INTERRUPTED\n", "        def fix():\n            return 1\n"], "sample_387": ["    def test_ManyToManyField_using_to_field(self):\n        from selenium.webdriver.common.by import By\n        from selenium.webdriver.support.ui import Select\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_event_add\")\n        )\n\n        main_window = self.selenium.current_window_handle\n\n        # Add a band\n        self.selenium.find_element(By.ID, \"add_id_bands\").click()\n        self.wait_for_and_switch_to_popup()\n        self.selenium.find_element(By.ID, \"id_name\").send_keys(\"Test Band\")\n        self.selenium.find_element(\n            By.CSS_SELECTOR, \".submit-row > input[type=submit]\"\n        ).click()\n        self.selenium.switch_to.window(main_window)\n\n        select = Select(self.selenium.find_element(By.ID, \"id_bands\"))\n        select.select_by_visible_text(\"Test Band\")\n\n        # Click the Change Band button to change it\n        self.selenium.find_element(By.ID, \"change_id_bands\").click()\n        self.wait_for_and_switch_to_popup()\n\n        self.selenium.find_element(By.ID, \"id_name\").clear()\n        self.selenium.find_element(By.ID, \"id_name\").send_keys(\"Updated Test Band\")\n        self.selenium.find_element(\n            By.CSS_SELECTOR, \".submit-row > input[type=submit]\"\n        ).click()\n\n        self.selenium.switch_to.window(main_window)\n        select = Select(self.selenium.find_element(By.ID, \"id_bands\"))\n        select.select_by_visible_text(\"Updated Test Band\")\n\n        # Go ahead and submit the form to make sure it works\n        self.selenium.find_element(By.CSS_SELECTOR, \"button[type='submit']\").click()\n        self.wait_for_text(\"li.success", "    def test_ManyToManyField_using_to_field(self):\n        from selenium.webdriver.common.by import By\n        from selenium.webdriver.support.ui import Select\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_membership_add\")\n        )\n\n\n", "    def test_ForeignKey_using_to_field_add_inline(self):\n        from selenium.webdriver.common.by import By\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_school_add\")\n        )\n\n        main_window = self.selenium.current_window_handle\n\n        # Click the Add Student button\n        self.selenium.find_element(By.ID, \"add_id_students\").click()\n        self.wait_for_and_switch_to_popup()\n\n        # Fill out the student form\n        name_field = self.selenium.find_element(By.ID, \"id_name\")\n        name_field.send_keys(\"New Student\")\n\n        save_button_css_selector = \".submit-row > input[type=submit]\"\n        self.selenium.find_element(By.CSS_SELECTOR, save_button_css_selector).click()\n        self.selenium.switch_to.window(main_window)\n\n        # The new student should be selected in the \"Students\" field\n        student_option = self.selenium.find_element(\n            By.CSS_SELECTOR, \"#id_students option:last-child\"\n        )\n        self.assertEqual(student_option.text, \"New Student\")\n        self.assertTrue(student_option.is_selected())\n\n        # Click the \"Save\" button on the school form\n        self.selenium.find_element(By.XPATH, '//input[@value=\"Save\"]').click()\n        self.wait_for_text(\"li.success\", \"The school was added successfully.\")\n\n        # Verify the student is associated with the school\n        school = School.objects.get(pk=1)\n        self.assertEqual(school.students.count(), 1)\n        self.assertEqual(school.students.first().name, \"New Student\")\n", "    def test_ManyToManyField_using_to_field(self):\n        from selenium.webdriver.common.by import By\n        from selenium.webdriver.support.ui import Select\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_event_add\")\n        )\n\n        main_window = self.selenium.current_window_handle\n        # Click the Add Band button to add new\n        self.selenium.find_element(By.ID, \"add_id_bands\").click()\n        self.wait_for_and_switch_to_popup()\n        band_name_field = self.selenium.find_element(By.ID, \"id_name\")\n        band_name_field.send_keys(\"NewBand\")\n\n        save_button_css_selector = \".submit-row > input[type=submit]\"\n        self.selenium.find_element(By.CSS_SELECTOR, save_button_css_selector).click()\n        self.selenium.switch_to.window(main_window)\n        # The field now contains the new band\n        self.selenium.find_element(By.CSS_SELECTOR, \"#id_bands option[value=NewBand]\")\n\n        self.selenium.find_element(By.ID, \"view_id_bands\").click()\n        self.wait_for_value(\"#id_name\", \"NewBand\")\n        self.selenium.back()\n\n        select = Select(self.selenium.find_element(By.ID, \"id_bands\"))\n        select.select_by_visible_text(\"NewBand\")\n        # Click the Change Band button to change it\n        self.selenium.find_element(By.ID, \"change_id_bands\").click()\n        self.wait_for_and_switch_to_popup()\n\n        band_name_field = self.selenium.find_element(By.ID, \"id_name\")\n        band_name_value = \"ChangedNewBand\"\n        band_name_field.clear()\n        band_name_field.send_keys(band_name_value)\n\n        save_button_", "    def test_ManyToManyField_using_to_field(self):\n        from selenium.webdriver.common.by import By\n        from selenium.webdriver.support.ui import Select\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_profile_add\")\n        )\n\n        main_window = self.selenium.current_window_handle\n        # Click the Add Group button to add new\n        self.selenium.find_element(By.ID, \"add_id_groups\").click()\n        self.wait_for_and_switch_to_popup()\n        group_name_field = self.selenium.find_element(By.ID, \"id_name\")\n        group_name_field.send_keys(\"newgroup\")\n\n        save_button_css_selector = \".submit-row > input[type=submit]\"\n        self.selenium.find_element(By.CSS_SELECTOR, save_button_css_selector).click()\n        self.selenium.switch_to.window(main_window)\n        # The field now contains the new group\n        self.selenium.find_element(By.CSS_SELECTOR, \"#id_groups option[value=newgroup]\")\n\n        self.selenium.find_element(By.ID, \"view_id_groups\").click()\n        self.wait_for_value(\"#id_name\", \"newgroup\")\n        self.selenium.back()\n\n        select = Select(self.selenium.find_element(By.ID, \"id_groups\"))\n        select.select_by_value(\"newgroup\")\n        # Click the Change Group button to change it\n        self.selenium.find_element(By.ID, \"change_id_groups\").click()\n        self.wait_for_and_switch_to_popup()\n\n        group_name_field = self.selenium.find_element(By.ID, \"id_name\")\n        group_name_value = \"changednewgroup\"\n        group_name_field.clear()\n        group_name_field.send_keys(group_name_value)\n\n        save_button_css_"], "sample_752": ["    def test_iforest_threshold():\n        X = np.array([[1, 1], [1, 2], [2, 1],\n                       [3, 3], [3, 4], [4, 3]])\n        y = np.array([1, 1, 1, -1, -1, -1])\n        clf = IsolationForest(random_state=0)\n        clf.fit(X, y)\n        dec_func = clf.decision_function(X)\n        assert_almost_equal(clf.threshold_, np.median(dec_func[y == 1]))\n        assert_array_equal(clf.predict(X), y)\n", "def test_iforest_random_state():\n    \"\"\" Tests the randomness of IsolationForest with fixed random_state.\n\n    Checks if the output of the model is consistent when using the same\n    random_state. This ensures that the model is deterministic given a fixed seed.\n    \"\"\"\n    rng = check_random_state(0)\n    X = np.random.rand(100, 3)\n\n    clf1 = IsolationForest(random_state=rng).fit(X)\n    clf2 = IsolationForest(random_state=rng).fit(X)\n\n    # Check if the predicted scores are the same for both models\n    assert_array_almost_equal(clf1.decision_function(X),\n                              clf2.decision_function(X))\n", "def test_iforest_contamination_parameter():\n    # Test that contamination parameter works as expected\n    X = iris.data\n    y = iris.target\n\n    # Test contamination=0.1\n    clf = IsolationForest(contamination=0.1, random_state=0).fit(X)\n    n_outliers = np.sum(clf.predict(X) == -1)\n    assert_equal(n_outliers, int(0.1 * len(X)))\n\n    # Test contamination='auto'\n    clf = IsolationForest(contamination='auto', random_state=0).fit(X)\n    n_outliers_auto = np.sum(clf.predict(X) == -1)\n    assert_not_equal(n_outliers_auto, n_outliers)\n", "def test_iforest_predict_proba():\n\n    X_train = np.array([[0, 1], [1, 2], [2, 1], [2, 2]])\n    clf = IsolationForest(contamination=0.1, random_state=42).fit(X_train)\n    proba = clf.predict_proba(X_train)\n    #Check if the probabilities sum to 1\n    assert_array_almost_equal(proba[:, 0] + proba[:, 1], np.ones(len(X_train)))\n\n\n\n", "def test_iforest_sparse_input_with_deprecation():\n    from sklearn.datasets import make_classification\n    rng = check_random_state(0)\n    X, _ = make_classification(n_samples=100,\n                              n_features=10,\n                              random_state=rng,\n                              n_informative=5,\n                              sparse=True)\n    clf = IsolationForest(contamination='auto').fit(X)\n    assert_warns_message(DeprecationWarning,\n                         \"threshold_ attribute is deprecated in 0.20 and will\"\n                         \" be removed in 0.22.\",\n                         getattr, clf, \"threshold_\")\n\n"], "sample_761": ["    def test_imputer_sparse_add_indicator(self, strategy):\n        rng = np.random.RandomState(0)\n        n_samples = 100\n        n_features = 10\n        X_sparse = sparse.csr_matrix(rng.rand(n_samples, n_features))\n        X_sparse[rng.rand(n_samples, n_features) < 0.2] = np.nan\n\n        imputer = SimpleImputer(missing_values=np.nan, strategy=strategy,\n                                add_indicator=True)\n        X_trans = imputer.fit_transform(X_sparse)\n\n        assert sparse.issparse(X_trans)\n        assert X_trans.shape[1] == n_features + n_features\n\n", "    def test_imputation_add_indicator_sparse_matrix_feature_types(self):\n        X_sparse = sparse.csc_matrix([\n            [1, np.nan, 'a'],\n            [2, 3, 'b'],\n            [np.nan, 4, 5],\n            [1, 2, 9]\n        ], dtype=object)\n        with pytest.raises(ValueError, match=\"dtype\"):\n            SimpleImputer(missing_values=np.nan, add_indicator=True).fit_transform(\n                X_sparse)\n\n", "    def test_missing_indicator_add_indicator_error(self):\n        X = np.array([[np.nan, 1], [2, np.nan]])\n        with pytest.raises(ValueError, match=\"add_indicator can only be used\"):\n            MissingIndicator(add_indicator=True)\n", "def test_simple_imputer_strategy(X, strategy, expected_output):\n    imputer = SimpleImputer(missing_values=np.nan, strategy=strategy)\n    imputer.fit(X)\n    assert_allclose(imputer.transform(X), expected_output)\n", "    def test_missing_indicator_addindicator_sparse(self):\n        X = sparse.csr_matrix([[1, np.nan, 3],\n                              [4, 5, np.nan],\n                              [7, np.nan, 9]])\n\n        imputer = SimpleImputer(missing_values=np.nan, add_indicator=True)\n        X_trans = imputer.fit_transform(X)\n\n        assert sparse.issparse(X_trans)\n        assert X_trans.shape == (3, 5)\n"], "sample_804": ["    def test_one_hot_encoder_sparse_output_dtype(self):\n        X = np.array([[1, 2], [3, 4]], dtype='int64')\n        enc = OneHotEncoder(sparse=True, dtype=np.float32)\n        X_tr = enc.fit_transform(X)\n        assert X_tr.dtype == np.float32\n", "    def test_one_hot_encoder_error_on_strings_with_drop(self):\n        enc = OneHotEncoder(drop='first', handle_unknown='ignore')\n        X = [['abc', 'a'], ['def', 'b'], ['ghi', 'a']]\n        with pytest.raises(ValueError, match=\"Setting 'drop' to a list\"):\n            enc.fit(X) \n", "    def test_one_hot_encoder_pandas_drop_manual():\n        cats_to_drop = ['def', 12, 3, 56]\n        pd = pytest.importorskip('pandas')\n        enc = OneHotEncoder(drop=cats_to_drop)\n        X = pd.DataFrame({\n            'A': ['abc', 'def', 'def'],\n            'B': [12, 12, 12],\n            'C': [2, 1, 3],\n            'D': [55, 55, 56]\n        })\n        trans = enc.fit_transform(X).toarray()\n        exp = [[1, 0, 1, 1],\n               [0, 1, 0, 1],\n               [0, 0, 0, 0]]\n        assert_array_equal(trans, exp)\n        dropped_cats = [cat[feature]\n                        for cat, feature in zip(enc.categories_,\n                                                    enc.drop_idx_)]\n        assert_array_equal(dropped_cats, cats_to_drop)\n        assert_array_equal(np.array(X, dtype=object),\n                           enc.inverse_transform(trans))\n", "compilation error", "    def test_ordinal_encoder_sparse(self):\n        X = [['abc', 2, 55], ['def', 1, 55]]\n        enc = OrdinalEncoder(sparse=True)\n        exp = sparse.csr_matrix([[0, 1, 0],\n                                 [1, 0, 0]])\n        assert_array_equal(enc.fit_transform(X), exp)\n"], "sample_630": ["def test_infer_node_3(mock_infer, mock_get_annotation):\n    \"\"\"Return set(annotation) when an annotation has been returned\n    \"\"\"\n    mock_get_annotation.return_value = 'str'\n    node = astroid.extract_node(\"a: str = 'mystr'\")\n    mock_infer.return_value = \"x\"\n    assert infer_node(node) == {'str'}\n    assert mock_infer.called\n", "def test_infer_node_3(mock_infer, mock_get_annotation):\n    \"\"\"Return the result of get_annotation when InferenceError is raised and an\n    annotation has been returned\n    \"\"\"\n    mock_get_annotation.return_value = \"annotation\"\n    node = astroid.extract_node(\"a: str = 'mystr'\")\n    mock_infer.side_effect = astroid.InferenceError\n    assert infer_node(node) == {\"annotation\"}\n    assert mock_infer.called\n\n", "def test_get_annotation_assignattr_no_type(init_method, label):\n    \"\"\"AssignAttr with no type\"\"\"\n    assign = rf\"\"\"\n        class A:\n            {init_method}\n    \"\"\"\n    node = astroid.extract_node(assign)\n    instance_attrs = node.instance_attrs\n    for _, assign_attrs in instance_attrs.items():\n        for assign_attr in assign_attrs:\n            got = get_annotation(assign_attr)\n            assert isinstance(assign_attr, astroid.AssignAttr)\n            assert got is None, f\"got {got} instead of None for value {node}\"\n", "def test_infer_node_3(mock_infer, mock_get_annotation):\n    \"\"\" Return a set of the annotation if it exists and ignore InferenceError \"\"\"\n    mock_get_annotation.return_value = astroid.node_classes.Name(\"Optional[str]\")\n    node = astroid.extract_node(\"a: Optional[str] = 'mystr'\")\n    mock_infer.side_effect = astroid.InferenceError\n    assert infer_node(node) == {'Optional[str]'}\n    assert mock_infer.called\n\n", "def test_infer_node_3(mock_get_annotation):\n    \"\"\"Return set(annotation.name) when InferenceError is not raised and an\n    annotation has been returned\n    \"\"\"\n    mock_get_annotation.return_value = astroid.nodes.Name(\"str\")\n    node = astroid.extract_node(\"a: str = 'mystr'\")\n    assert infer_node(node) == set(\"str\")\n"], "sample_198": ["    def test_output_field_mismatch(self):\n        with self.assertRaises(ValueError):\n            ExpressionWrapper(Value('f'), output_field=IntegerField())\n", "    def test_expressionwrapper_deconstruct(self):\n        value = ExpressionWrapper(Value('name'), output_field=CharField())\n        path, args, kwargs = value.deconstruct()\n        self.assertEqual(path, 'django.db.models.expressions.ExpressionWrapper')\n        self.assertEqual(args, (Value('name'), CharField()))\n        self.assertEqual(kwargs, {})\n", "    def test_expression_wrapper_deconstruct(self):\n        expr = ExpressionWrapper(F('field'), output_field=IntegerField())\n        path, args, kwargs = expr.deconstruct()\n        self.assertEqual(path, 'django.db.models.expressions.ExpressionWrapper')\n        self.assertEqual(args, (F('field'),))\n        self.assertEqual(kwargs['output_field'].deconstruct(), IntegerField().deconstruct())\n\n", "    def test_expression_wrapper_with_alias(self):\n        expr = ExpressionWrapper(F('col'), output_field=CharField(), alias='my_alias')\n        self.assertEqual(expr.alias, 'my_alias')\n", "    def test_output_field_inheritance(self):\n        class MyField(IntegerField):\n            pass\n        expr = ExpressionWrapper(Value(3), output_field=MyField())\n        self.assertIsInstance(expr.output_field, MyField)\n"], "sample_165": ["    def test_modelmultiplechoicefield_empty_values(self):\n        f = ModelMultipleChoiceField(queryset=ChoiceModel.objects.all())\n        self.assertFormErrors(['Select a valid choice. None is not one of the available choices.'], f.clean, [None])\n", "    def test_modelmultiplechoicefield_empty_iterable(self):\n        e = {\n            'required': 'REQUIRED',\n            'invalid_choice': '%(value)s IS INVALID CHOICE',\n            'invalid_list': 'NOT A LIST OF VALUES',\n        }\n        f = ModelMultipleChoiceField(queryset=ChoiceModel.objects.all(), error_messages=e)\n        self.assertFormErrors(['REQUIRED'], f.clean, [])\n", "    def test_modelchoicefield_empty_label(self):\n        f = ModelChoiceField(queryset=ChoiceModel.objects.all(), empty_label='Select...')\n        self.assertEqual(f.choices[0][1], 'Select...')\n", "    def test_modelchoicefield_empty_label(self):\n        f = ModelChoiceField(queryset=ChoiceModel.objects.all(), empty_label=None)\n        self.assertEqual(f.choices[0][1], '')\n", "    def test_model_multiplechoicefield_empty_label(self):\n        f = ModelMultipleChoiceField(queryset=ChoiceModel.objects.all(), empty_label=None)\n\n        self.assertEqual(\n            f.widget.render(\n                'name', f.value,\n                attrs={'multiple': True, 'size': 3},\n            ),\n            f.widget.render(\n                'name', f.value,\n                attrs={'multiple': True, 'size': 3},\n            ),\n        )\n\n"], "sample_291": ["    def test_object_passed_to_get_context_data(self):\n        test_view = views.CustomSingleObjectView()\n        test_view.object = {'name': 'Object Name'}\n        context = test_view.get_context_data()\n        self.assertEqual(context['object'], test_view.object)\n\n", "    def test_object_at_custom_name_in_get_context_data(self):\n        test_view = views.CustomSingleObjectView()\n        context = test_view.get_context_data()\n        self.assertEqual(context['object'], test_view.object)\n\n", "    def test_get_context_data_object_name(self):\n        test_view = views.CustomSingleObjectView()\n        context = test_view.get_context_data()\n        self.assertEqual(context['object'], test_view.object)\n        self.assertEqual(context['custom_key'], 'custom_value')\n", "    def test_get_object_by_pk(self):\n        test_view = views.CustomSingleObjectView()\n        request = self.rf.get('/')\n        test_view.kwargs = {'pk': 1}\n        test_view.dispatch(request)\n        self.assertEqual(test_view.object.pk, 1)\n\n", "    def test_single_object_template_mixin_empty_object_list(self):\n        test_view = views.CustomSingleObjectMixinView()\n        test_view.queryset = []\n        with self.assertRaises(Http404):\n            test_view.get(self.rf.get('/'))\n\n"], "sample_972": ["    def test_stringify_GenericAlias():\n        from typing import GenericAlias\n        # Example of a GenericAlias: typing._GenericAlias\n        assert stringify(GenericAlias(list, str)) == 'list[str]'\n        assert stringify(GenericAlias(list, str), 'smart') == 'list[str]'\n\n", "    def test_stringify_type_union_operator_nested():\n        assert stringify(List[int | str]) == \"List[int | str]\"\n        assert stringify(List[int | str], \"smart\") == \"~typing.List[int | str]\" \n", "    def test_stringify_type_union_operator_with_None():\n        assert stringify(int | None) == \"int | None\"  \n        assert stringify(int | None, \"smart\") == \"int | None\"\n\n        assert stringify(str | None) == \"str | None\"   \n        assert stringify(str | None, \"smart\") == \"str | None\"\n        \n        assert stringify(list[int] | None) == \"list[int] | None\"\n        assert stringify(list[int] | None, \"smart\") == \"list[int] | None\"\n", "def test_stringify_type_hints_generic_alias():\n    from typing import Generic, TypeVar\n\n    T = TypeVar('T')\n\n    class MyGeneric(Generic[T]):\n        pass\n\n    assert stringify(MyGeneric[int]) == 'tests.test_util_typing.MyGeneric[int]'\n    assert stringify(MyGeneric[int], \"smart\") == '~tests.test_util_typing.MyGeneric[int]'\n", "    def test_stringify_type_ForwardRef():\n        from typing import ForwardRef  # type: ignore\n        assert stringify(ForwardRef(\"myint\")) == \"myint\"\n        assert stringify(ForwardRef(\"myint\"), \"smart\") == \"myint\"\n"], "sample_1117": ["compilation error", "compilation error", "def test_BlockMatrix():\n    A = MatrixSymbol('A', 2, 2)\n\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n\n    block_matrix = BlockMatrix([[A, B], [C, A]])\n    assert ask(Q.invertible(block_matrix), Q.invertible(A) & Q.invertible(A) & Q.square(A))\n    assert ask(Q.symmetric(block_matrix), Q.symmetric(A) & Q.symmetric(B))\n", "compilation error", "def test_matrix_element_sets_transpose():\n    X = MatrixSymbol('X', 4, 4)\n    assert ask(Q.integer_elements(X.T), Q.integer_elements(X))\n"], "sample_233": ["    def test_token_invalidating_last_login(self):\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n        # Simulate a password reset by updating last_login\n        user.last_login = datetime.now()\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n", "    def test_token_with_changed_password(self):\n        \"\"\"\n        Updating the user password invalidates the token.\n        \"\"\"\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n        user.set_password('newpw')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n", "    def test_token_with_changed_password(self):\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        user.set_password('newpassword')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n", "    def test_token_with_password_change(self):\n        \"\"\"Updating the user password invalidates the token.\"\"\"\n        user = User.objects.create_user('changepwuser', 'test5@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n        user.set_password('newtestpw')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n", "    def test_token_with_changed_password(self):\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        user.set_password('newpassword')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n"], "sample_1121": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_issue_19009():\n    # test that a function with a complex argument doesn't incorrectly evaluate\n    # to a non-complex result when the argument is a complex symbol\n    z = symbols('z', complex=True)\n    assert isinstance(sin(z), Expr)\n    assert isinstance(cos(z), Expr)\n    assert isinstance(tan(z), Expr)\n"], "sample_710": ["def test_unittest_setup_teardown_order(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n        from unittest.mock import Mock\n\n        event_log = []\n\n        class MyTestCase(unittest.TestCase):\n                event_log.append('setUp')\n            \n                event_log.append('test_something')\n\n                event_log.append('tearDown')\n\n            MyTestCase().test_something()\n            assert event_log == ['setUp', 'test_something', 'tearDown']\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n", "    def test_traceback_pruning_with_setup(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import unittest\n\n            class MyTestCase(unittest.TestCase):\n                    unittest.TestCase.__init__(self, test_method)\n\n                    assert False\n\n            class TestIt(MyTestCase):\n                    pass\n            \"\"\"\n        )\n        reprec = pytester.inline_run()\n        passed, skipped, failed = reprec.countoutcomes()\n        assert passed == 0\n        assert failed == 1\n        assert reprec.ret == 1\n", "    def test_do_cleanups_on_method_teardown_failure(pytester: Pytester) -> None:\n        testpath = pytester.makepyfile(\n            \"\"\"\n            import unittest\n            class MyTestCase(unittest.TestCase):\n                values = []\n                        self.values.append(1)\n                    self.addCleanup(cleanup)\n                    assert False\n                assert MyTestCase.values == [1]\n        \"\"\"\n        )\n        reprec = pytester.inline_run(testpath)\n        passed, skipped, failed = reprec.countoutcomes()\n        assert failed == 1\n        assert passed == 1\n\n", "    def test_do_cleanups_on_failure(pytester: Pytester) -> None:\n        testpath = pytester.makepyfile(\n            \"\"\"\n            import unittest\n            class MyTestCase(unittest.TestCase):\n                values = []\n                        self.values.append(1)\n                    self.addCleanup(cleanup)\n                    assert False\n                assert MyTestCase.values == [1]\n        \"\"\"\n        )\n        reprec = pytester.inline_run(testpath)\n        passed, skipped, failed = reprec.countoutcomes()\n        assert failed == 1\n        assert passed == 1\n\n", "    def test_do_class_cleanups_on_success_with_setup_teardown(pytester: Pytester) -> None:\n        testpath = pytester.makepyfile(\n            \"\"\"\n            import unittest\n            class MyTestCase(unittest.TestCase):\n                values = []\n                @classmethod\n                        cls.values.append(1)\n                    cls.addClassCleanup(cleanup)\n                @classmethod\n                    cls.values.append(2)\n                    pass\n                    pass\n                assert MyTestCase.values == [1, 2]\n        \"\"\"\n        )\n        reprec = pytester.inline_run(testpath)\n        passed, skipped, failed = reprec.countoutcomes()\n        assert failed == 0\n        assert passed == 3\n"], "sample_808": ["def test_iforest_n_jobs():\n    rng = check_random_state(0)\n    X = rng.randn(100, 2)\n    # test parallel prediction\n    clf = IsolationForest(n_jobs=2).fit(X)\n    result = clf.predict(X)\n    clf.set_params(n_jobs=1)\n    result2 = clf.predict(X)\n    assert_array_equal(result, result2)\n", "def test_iforest_predict_proba():\n    X_train = [[0.0], [1.0], [2.0]]\n    clf = IsolationForest()\n    clf.fit(X_train)\n    proba = clf.predict_proba([[0.5]])\n    assert proba.shape == (1, 2)\n\n    # check that probabilities sum to 1\n    assert_allclose(proba[0][0] + proba[0][1], 1.0)\n", "    def test_iforest_n_jobs():\n   \n        X_train = [[1, 1], [1, 2], [2, 1]]\n        clf = IsolationForest(n_estimators=10, n_jobs=2)\n        \n        clf.fit(X_train)\n\n", "def test_iforest_predict_proba():\n    X_train = [[1, 1], [1, 2], [2, 1], [3, 3]]\n    clf = IsolationForest(contamination='auto').fit(X_train)\n    proba = clf.predict_proba([[2., 2.]])\n    assert proba.shape == (1, 2)\n    assert proba[0, 0] < proba[0, 1]  # outlier probability should be smaller\n\n", "def test_iforest_input_validation():\n    # Check input validation for fit\n    X = [[1, 1], [1, 2], [2, 1]]\n    clf = IsolationForest()\n\n    # Test for empty input\n    with pytest.raises(ValueError):\n       clf.fit([])\n\n    # Test for non-2d input\n    with pytest.raises(ValueError):\n        clf.fit([1, 2, 3])\n\n    # Test for inconsistent number of features\n    with pytest.raises(ValueError):\n        clf.fit([[1, 1], [1, 2, 3]])\n\n    # Test for non-numerical input\n    with pytest.raises(TypeError):\n       clf.fit([['a', 1], [1, 2]])\n\n"], "sample_386": ["    def test_safe_string_addition_with_different_types(self):\n        self.assertIsInstance(SafeString(\"a\") + \"b\", SafeString)\n        self.assertIsInstance(\"a\" + SafeString(\"b\"), SafeString)\n        self.assertIsInstance(SafeString(\"a\") + SafeString(\"b\"), SafeString)\n\n        with self.assertRaises(TypeError):\n            SafeString(\"a\") + 1\n", "    def test_mark_safe_empty_string(self):\n        s = mark_safe(\"\")\n        self.assertIsInstance(s, SafeString)\n        self.assertEqual(s, \"\")\n", "    def test_safe_string_subclassing(self):\n        class MySafeString(SafeString):\n                super().__init__(value)\n                self.my_attr = value\n\n        s = MySafeString(\"a&b\")\n        self.assertEqual(s.my_attr, \"a&b\")\n        self.assertRenderEqual(\"{{ s }}\", \"a&b\", s=s)\n", "    def test_safe_string_addition_with_non_string(self):\n        s = mark_safe(\"Hello\")\n        with self.assertRaises(TypeError):\n            s + 123\n", "    def test_safe_string_addition_with_different_types(self):\n        self.assertEqual(SafeString(\"foo\") + \"bar\", \"foobar\")\n        self.assertEqual(\"foo\" + SafeString(\"bar\"), \"foobar\")\n        self.assertEqual(SafeString(\"foo\") + SafeString(\"bar\"), SafeString(\"foobar\"))\n"], "sample_82": ["    def test_selectdate_invalid_date(self):\n        self.widget.years = ('2000',)\n        self.check_html(self.widget, 'mydate', '2001-02-30', html=(\n            \"\"\"\n            <select name=\"mydate_month\" id=\"id_mydate_month\">\n                <option value=\"\">---</option>\n                <option value=\"1\">January</option>\n                <option value=\"2\" selected>February</option>\n                <option value=\"3\">March</option>\n                <option value=\"4\">April</option>\n                <option value=\"5\">May</option>\n                <option value=\"6\">June</option>\n                <option value=\"7\">July</option>\n                <option value=\"8\">August</option>\n                <option value=\"9\">September</option>\n                <option value=\"10\">October</option>\n                <option value=\"11\">November</option>\n                <option value=\"12\">December</option>\n            </select>\n\n            <select name=\"mydate_day\" id=\"id_mydate_day\">\n                <option value=\"\">---</option>\n                <option value=\"1\">1</option>\n                <option value=\"2\">2</option>\n                <option value=\"3\">3</option>\n                <option value=\"4\">4</option>\n                <option value=\"5\">5</option>\n                <option value=\"6\">6</option>\n                <option value=\"7\">7</option>\n                <option value=\"8\">8</option>\n                <option value=\"9\">9</option>\n                <option value=\"10\">10</option>\n                <option value=\"11\">11</option>\n                <option value=\"12\">12</option>\n                <option value=\"13\">13</option>\n                <option value=\"14\">14</option>\n                <option value=\"15\">15</option>\n                <option value=\"16\">16</option>\n                <option value=\"17\">17</option>\n                <option value=\"18\">18</option>\n", "    def test_initial_value_with_empty_string(self):\n        self.assertEqual(\n            self.widget.value_from_datadict({'year': '', 'month': '', 'day':''}, {}, 'field'),\n            None,\n            )\n", "    def test_empty_value(self):\n        self.assertEqual(self.widget.value_from_datadict({}, {}, 'field'), None)\n        self.assertEqual(self.widget.value_from_datadict({'field_year': '', 'field_month': '', 'field_day': ''}, {}, 'field'), None)\n", "    def test_selectdate_empty_label_with_none(self):\n        w = SelectDateWidget(years=('2014',), empty_label=None)\n\n        # Rendering the default state with empty_label set to None.\n\n        self.assertHTMLEqual(\n            w.render('mydate', ''),\n            \"\"\"\n            <select name=\"mydate_month\" id=\"id_mydate_month\">\n                <option selected value=\"\">---</option>\n                <option value=\"1\">January</option>\n                <option value=\"2\">February</option>\n                <option value=\"3\">March</option>\n                <option value=\"4\">April</option>\n                <option value=\"5\">May</option>\n                <option value=\"6\">June</option>\n                <option value=\"7\">July</option>\n                <option value=\"8\">August</option>\n                <option value=\"9\">September</option>\n                <option value=\"10\">October</option>\n                <option value=\"11\">November</option>\n                <option value=\"12\">December</option>\n            </select>\n\n            <select name=\"mydate_day\" id=\"id_mydate_day\">\n                <option selected value=\"\">---</option>\n                <option value=\"1\">1</option>\n                <option value=\"2\">2</option>\n                <option value=\"3\">3</option>\n                <option value=\"4\">4</option>\n                <option value=\"5\">5</option>\n                <option value=\"6\">6</option>\n                <option value=\"7\">7</option>\n                <option value=\"8\">8</option>\n                <option value=\"9\">9</option>\n                <option value=\"10\">10</option>\n                <option value=\"11\">11</option>\n                <option value=\"12\">12</option>\n                <option value=\"13\">13</option>\n                <option value=\"14\">14</option>\n                <option value=\"15\">15</option>\n                <option value=\"16\">16</option>\n                <option value=\"17\">17</option", "    def test_selectdate_disabled(self):\n        w = SelectDateWidget(years=('2018', '2019'), disabled=True)\n\n        self.check_html(\n            w,\n            'mydate',\n            '',\n            html=(\n                \"\"\"\n                <select name=\"mydate_month\" id=\"id_mydate_month\" disabled>\n                    <option selected value=\"\">---</option>\n                    <option value=\"1\">January</option>\n                    <option value=\"2\">February</option>\n                    <option value=\"3\">March</option>\n                    <option value=\"4\">April</option>\n                    <option value=\"5\">May</option>\n                    <option value=\"6\">June</option>\n                    <option value=\"7\">July</option>\n                    <option value=\"8\">August</option>\n                    <option value=\"9\">September</option>\n                    <option value=\"10\">October</option>\n                    <option value=\"11\">November</option>\n                    <option value=\"12\">December</option>\n                </select>\n\n                <select name=\"mydate_day\" id=\"id_mydate_day\" disabled>\n                    <option selected value=\"\">---</option>\n                    <option value=\"1\">1</option>\n                    <option value=\"2\">2</option>\n                    <option value=\"3\">3</option>\n                    <option value=\"4\">4</option>\n                    <option value=\"5\">5</option>\n                    <option value=\"6\">6</option>\n                    <option value=\"7\">7</option>\n                    <option value=\"8\">8</option>\n                    <option value=\"9\">9</option>\n                    <option value=\"10\">10</option>\n                    <option value=\"11\">11</option>\n                    <option value=\"12\">12</option>\n                    <option value=\"13\">13</option>\n                    <option value=\"14\">14</option>\n                    <option value=\"15\">15</option>\n                    <option value=\"16\">16</option>\n                    <option value=\"17\">17</option>\n                    <option value=\"1"], "sample_375": ["    def test_proxy_model_fk(self):\n        A = self.create_model(\"A\")\n        B = self.create_model(\"B\", bases=(A,), proxy=True, foreign_keys=[models.ForeignKey('C', models.CASCADE)])\n        C = self.create_model(\"C\")\n        self.assertRelated(A, [B, C])\n        self.assertRelated(B, [A, C])\n        self.assertRelated(C, [A, B])\n", "    def test_generic_fk_proxy_base(self):\n        A = self.create_model(\"A\", foreign_keys=[GenericForeignKey()])\n        B = self.create_model(\"B\", bases=(A,), proxy=True)\n        self.assertRelated(A, [B])\n        self.assertRelated(B, [])\n", "    def test_proxy_with_m2m(self):\n        A = self.create_model(\"A\", foreign_keys=[models.ManyToManyField('B')])\n        B = self.create_model(\"B\")\n        P = self.create_model(\"P\", bases=(A,), proxy=True)\n        self.assertRelated(A, [B, P.a_1.rel.through])\n        self.assertRelated(B, [A, P])\n        self.assertRelated(P, [B])\n\n", "    def test_proxy_model_with_inherited_fk(self):\n        class Author(models.Model):\n            name = models.CharField(max_length=100)\n\n        class Book(models.Model):\n            title = models.CharField(max_length=200)\n            author = models.ForeignKey(Author, on_delete=models.CASCADE)\n\n        class SpecialBook(Book):\n            class Meta:\n                proxy = True\n                \n        self.assertRelated(Author, [Book, SpecialBook])\n\n", "    def test_circular_base_with_fk(self):\n        A = self.create_model(\"A\")\n        B = self.create_model(\"B\", bases=(A,))\n        C = self.create_model(\"C\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n        D = self.create_model(\"D\", bases=(C,))\n        self.assertRelated(A, [B, C, D])\n        self.assertRelated(B, [A, C, D])\n        self.assertRelated(C, [A, B, D])\n        self.assertRelated(D, [A, B, C])\n"], "sample_100": ["    def test_watch_dir_with_absolute_path_string(self):\n        self.reloader.watch_dir(str(self.tempdir), '*.py')\n        watched_files = list(self.reloader.watched_files())\n        self.assertIn(self.existing_file, watched_files)\n\n", "    def test_notify_file_changed_does_not_trigger_duplicate_events(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]) as mocked_files:\n            mocked_files.__iter__.return_value = iter([self.existing_file])\n            self.reloader.notify_file_changed(self.existing_file)\n            self.reloader.notify_file_changed(self.existing_file)\n            self.assertEqual(mocked_files.call_count, 1)\n", "    def test_start_django_with_no_reloader(self, mocked_echo):\n        fake_main = mock.MagicMock()\n        autoreload.start_django(None, fake_main)\n        self.assertEqual(mocked_echo.call_count, 1)\n", "    def test_watch_files_with_recursive_glob_and_hidden_files(self):\n        hidden_file = self.ensure_file(self.tempdir / '.hidden_file.py')\n        self.reloader.watch_dir(self.tempdir, '**/*.py')\n        watched_files = list(self.reloader.watched_files())\n        self.assertIn(self.existing_file, watched_files)\n        self.assertIn(hidden_file, watched_files)\n", "    def test_should_stop_returns_false_if_should_stop_is_false(self):\n        self.reloader.should_stop = False\n        self.assertFalse(self.reloader.should_stop())\n"], "sample_1150": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_202": ["    def test_empty_cookie(self):\n        storage = self.get_storage()\n        response = self.get_response()\n        set_cookie_data(storage, [], encode_empty=True)\n        self.assertEqual(stored_cookie_messages_count(storage, response), 0)\n\n", "    def test_empty_cookie(self):\n        storage = self.get_storage()\n        response = self.get_response()\n        # Empty cookie should not result in a cookie with an empty value being set\n        self.assertEqual(stored_cookie_messages_count(storage, response), 0)\n        storage.update(response)\n        self.assertEqual(stored_cookie_messages_count(storage, response), 0)\n\n", "    def test_empty_cookie(self):\n        \"\"\"\n        If the cookie is empty (i.e., we haven't stored any\n        messages), we should return an empty list and indicate\n        that all messages were retrieved.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n        self.assertEqual(list(storage), [])\n\n        # Set the cookie to an empty string (simulating an empty cookie).\n        storage.request.COOKIES = {CookieStorage.cookie_name: \"\"}\n\n        messages, all_retrieved = storage._get()\n        self.assertEqual(messages, [])\n        self.assertTrue(all_retrieved)\n", "    def test_empty_cookie(self):\n        storage = self.storage_class(self.get_request())\n        set_cookie_data(storage, [], encode_empty=True)\n        response = self.get_response()\n        self.assertEqual(\n            response.cookies['messages']['value'],\n            storage._encode( [], encode_empty=True),\n        )\n", "    def test_empty_cookie(self):\n        \"\"\"\n        Ensure that retrieving messages from an empty cookie returns an empty\n        list (#24582).\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n        self.assertEqual(list(storage), [])  # Ensure empty list is returned\n\n\n\n"], "sample_220": ["    def test_delete_cookie_path(self):\n        response = HttpResponse()\n        response.set_cookie('c', path='/path/')\n        response.delete_cookie('c', path='/path/')\n        self.assertEqual(response.cookies['c']['path'], '/path/')\n", "    def test_delete_cookie_with_path(self):\n        response = HttpResponse()\n        response.set_cookie('c', path='/foo/')\n        response.delete_cookie('c', path='/foo/')\n        self.assertFalse(response.cookies.get('c'))\n        response.delete_cookie('c', path='/bar/')\n        self.assertTrue(response.cookies.get('c'))\n", "    def test_signed_cookie(self):\n        response = HttpResponse()\n        response.set_signed_cookie('c', 'value', salt='secret')\n        cookie = response.cookies['c']\n        self.assertTrue(len(cookie.value) > len('value'))\n        self.assertEqual(signing.get_cookie_signer(salt='secret').unsign(cookie.value), 'value')\n", "    def test_delete_cookie_path_domain(self):\n        response = HttpResponse()\n        response.delete_cookie('c', path='/mypath', domain='example.com')\n        cookie = response.cookies['c']\n        self.assertEqual(cookie['path'], '/mypath')\n        self.assertEqual(cookie['domain'], 'example.com')\n", "    def test_delete_cookie_path(self):\n        response = HttpResponse()\n        response.delete_cookie('c', path='/test/')\n        self.assertEqual(response.cookies['c']['path'], '/test/')\n"], "sample_981": ["def test_inverse():\n    p = Permutation([3, 1, 2, 0])\n    assert p.inverse() == Permutation([3, 0, 1, 2])\n    assert Permutation.from_array_form(p.array_form).inverse() == \\\n        Permutation.from_array_form(p.inverse().array_form)\n    assert Permutation([0, 1, 2]).inverse() == Permutation([0, 1, 2])\n    assert Permutation([]).inverse() == Permutation([])\n    assert Permutation([0, 2, 1]).inverse() * Permutation([0, 2, 1]) == Permutation([0, 1, 2])\n\n", "compilation error", "    def test_pickle():\n\n        p = Permutation([1, 0, 2])\n        s = pickle.dumps(p)\n\n        loaded = pickle.loads(s)\n\n        assert loaded == p\n\n", "    def test_from_list():\n        assert Permutation.from_list([1, 0, 3, 2], size=4) == Permutation([1, 0, 3, 2])\n        assert Permutation.from_list([1, 0, 3, 2]) == Permutation([1, 0, 3, 2])\n        raises(ValueError, lambda: Permutation.from_list([0, 1, 2], size=3))\n        assert Permutation.from_list([0, 1, 2, 3], size=5) == Permutation([0, 1, 2, 3, 4])\n       \n", "    def test_inverse():\n        p = Permutation([2, 5, 1, 6, 3, 0, 4])\n        q = ~p\n        assert p * q == Permutation(list(range(7)))\n        assert q * p == Permutation(list(range(7)))\n        assert ~(~p) == p\n"], "sample_240": ["    def test_password_reset_token_with_invalid_user_id(self):\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        # Modify the user ID in the token to make it invalid\n        invalid_token = tk1.replace(str(user.pk), str(user.pk + 1))\n        self.assertIs(p0.check_token(user, invalid_token), False)\n", "    def test_token_with_changed_password(self):\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        user.set_password('newpassword')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n\n", "    def test_token_with_changed_password(self):\n        \"\"\"Updating the user password invalidates the token.\"\"\"\n        user = User.objects.create_user('changepassworduser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n        user.set_password('newpassword')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n", "    def test_password_reset_token_length(self):\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        generator = PasswordResetTokenGenerator()\n        token = generator.make_token(user)\n        self.assertLess(len(token), 100)\n", "    def test_token_with_changed_password(self):\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        user.set_password('newpassword')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n\n"], "sample_1185": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_decompogen_with_constants():\n    assert decompogen(sin(x) + 2, x) == [sin(x) + 2]\n    assert decompogen(2*x + 5, x) == [2*x + 5]\n    assert decompogen(x**2 + 2*x + 3, x) == [x**2 + 2*x + 3]\n"], "sample_567": ["def test_text_bbox():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1])\n    text = ax.text(0.5, 0.5, 'text', bbox=dict(boxstyle='round').get('boxstyle'),\n                   ha='center', va='center')\n    fig.canvas.draw()\n    box = text.get_window_extent(fig.canvas.get_renderer())\n    assert box.xmin > 0 and box.ymin > 0\n    assert box.xmax < fig.canvas.get_width_height()[0]\n    assert box.ymax < fig.canvas.get_width_height()[1]\n", "compilation error", "    def test_text_ha_align(self):\n        fig, ax = plt.subplots()\n        ax.text(0.5, 0.5, \"text\", ha=\"left\")\n        ax.text(0.5, 0.7, \"text\", ha=\"center\")\n        ax.text(0.5, 0.9, \"text\", ha=\"right\")\n        fig.canvas.draw()\n        # TODO: check actual text alignment\n", "def test_invisible_text():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, \"foo\")\n    text.set_visible(False)\n    fig.canvas.draw()\n    assert text.get_visible() is False\n", "    def test_annotate_and_offsetfrom_copy_input_with_transform(fig, ax):\n        l, = ax.plot([0, 2], [0, 2])\n        of_xy = np.array([.5, .5])\n        ax.annotate(\"foo\", textcoords=OffsetFrom(l,\n                                                of_xy,\n                                                transform=ax.transData),\n                    xytext=(10, 0),\n                    xy=(0, 0))  # xy is unused.\n\n"], "sample_360": ["    def test_cache_key_with_accept_language(self):\n        request = self.factory.get(self.path, HTTP_ACCEPT_LANGUAGE='en-US,en;q=0.8')\n        template = engines['django'].from_string(\"This is a test\")\n        response = TemplateResponse(HttpRequest(), template)\n        learn_cache_key(request, response)\n        self.assertEqual(\n            get_cache_key(request),\n            'views.decorators.cache.cache_page.settingsprefix.GET.'\n            '90a381e99d5901ee6883c05423015d42.d41d8cd98f00b204e9800998ecf8427e'\n        )\n", "    def test_cache_get_or_set(self):\n        cache_key = 'test_key'\n        cache_value = 'test_value'\n\n        # Test when key doesn't exist in cache\n        with patch('django.core.cache.caches[DEFAULT_CACHE_ALIAS].get') as mock_get:\n            mock_get.return_value = None\n            result = cache.get_or_set(cache_key, lambda: cache_value)\n            self.assertEqual(result, cache_value)\n            mock_get.assert_called_once_with(cache_key)\n\n        # Test when key exists in cache\n        with patch('django.core.cache.caches[DEFAULT_CACHE_ALIAS].get') as mock_get:\n            mock_get.return_value = 'existing_value'\n            result = cache.get_or_set(cache_key, lambda: cache_value)\n            self.assertEqual(result, 'existing_value')\n            mock_get.assert_called_once_with(cache_key)\n", "    def test_cache_handler_clear(self):\n        test_caches = CacheHandler({\n            'cache_1': {\n                'BACKEND': 'django.core.cache.backends.dummy.DummyCache',\n            },\n            'cache_2': {\n                'BACKEND': 'django.core.cache.backends.dummy.DummyCache',\n            },\n        })\n        cache_1 = test_caches['cache_1']\n        cache_1.set('test_key', 'test_value')\n        test_caches.clear()\n        self.assertIsNone(cache_1.get('test_key'))\n", "    def test_cache_set_expiry_time(self):\n        with patch('django.core.cache.cache.get', return_value=None):\n            view = cache_page(timeout=10)(hello_world_view)\n            request = self.factory.get('/view/')\n            response = view(request, '1')\n\n            # Access the cache key directly.\n            cache.set(\n                get_cache_key(request),\n                'some_data',\n                timeout=5\n            )\n            self.assertEqual(cache.get(get_cache_key(request)), 'some_data')\n\n            time.sleep(6)\n            self.assertIsNone(cache.get(get_cache_key(request)))\n", "    def test_multiple_caches_vary_on(self):\n        cache1 = CacheHandler({\n            'cache_1': {\n                'BACKEND': 'django.core.cache.backends.locmem.LocMemCache',\n            },\n            'cache_2': {\n                'BACKEND': 'django.core.cache.backends.locmem.LocMemCache',\n            },\n        })\n        request = self.factory.get(\n            self.path\n        )\n\n        response = TemplateResponse(HttpRequest(), engines['django'].from_string(\"This is a test\"))\n        learn_cache_key(request, response, key_prefix='cache1', cache_alias='cache_1')\n        key1 = get_cache_key(request)\n\n        learn_cache_key(request, response, key_prefix='cache2', cache_alias='cache_2')\n        key2 = get_cache_key(request)\n        self.assertNotEqual(key1, key2)\n\n"], "sample_1011": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_619": ["def test_encode_cf_datetime_out_of_cftime_range():\n    dates = pd.date_range('0001', periods=1)\n    with pytest.raises(ValueError):\n        encode_cf_datetime(dates, units='days since 0001-01-01', calendar='gregorian')\n\n", "    def test_decode_cf_datetime_with_times_and_bounds(self, calendar):\n        times = np.arange(5, dtype=np.int64)\n        bounds = np.array([times[:-1], times[1:]])\n        units = \"days since 2000-01-01\"\n\n        result_times = decode_cf_datetime(times, units, calendar)\n        result_bounds = decode_cf_datetime(bounds, units, calendar)\n\n        expected_times = pd.date_range(\"2000\", periods=5)\n        expected_bounds = np.array([expected_times[:-1], expected_times[1:]])\n\n        np.testing.assert_array_equal(result_times, expected_times)\n        np.testing.assert_array_equal(result_bounds, expected_bounds)\n", "def test_encode_cf_datetime_with_cftime_object():\n    # GH 5690\n    dates = cftime.DatetimeGregorian(2000, 1, 1)\n    units = \"days since 2000-01-01\"\n    calendar = \"gregorian\"\n    encoded, _, _ = encode_cf_datetime(dates, units, calendar)\n    expected = 0\n    assert encoded == expected\n", "def test_encode_cf_datetime_with_scalar_input(calendar):\n\n    dates = pd.Timestamp(\"2000-01-01\")\n    units = \"days since 1970-01-01 00:00:00\"\n    num, _, _ = encode_cf_datetime(dates, units, calendar)\n    assert isinstance(num, np.number)\n\n", "def test_decode_cf_datetime_with_non_standard_calendar_and_non_cftime_units():\n    units = \"seconds since 2000-01-01 00:00:00\"\n    calendar = \"noleap\"\n    # check that non standard calendars are handled with cftime\n    # when the units do not indicate a cftime datetime format\n    num_dates = np.array([0, 1], dtype=np.int64)\n    with pytest.warns(SerializationWarning):\n        result = decode_cf_datetime(num_dates, units, calendar)\n    expected = np.array([\n        cftime.datetime(2000, 1, 1, 0, 0, 0),\n        cftime.datetime(2000, 1, 1, 0, 0, 1)\n    ], dtype='object')\n    np.testing.assert_array_equal(result, expected)\n"], "sample_458": ["    def test_rounding_behavior(self):\n        self.assertEqual(floatformat(1.5, 0), \"2\")\n        self.assertEqual(floatformat(-1.5, 0), \"-2\")\n", "    def test_format_with_grouping_and_unlocalizing(self):\n        with self.settings(\n            USE_THOUSAND_SEPARATOR=True, NUMBER_GROUPING=3, THOUSAND_SEPARATOR=\"!\"\n        ):\n            self.assertEqual(floatformat(12345.6789, \"2gu\"), \"12!345.68\")\n            self.assertEqual(floatformat(12345.6789, \"2ug\"), \"12!345.68\")\n", "    def test_floatformat03(self):\n        output = self.engine.render_to_string(\n            \"floatformat03\", {\"a\": \"1.42\"}\n        )\n        self.assertEqual(output, \"1.42\")\n", "    def test_floatformat03(self):\n        output = self.engine.render_to_string(\n            \"floatformat02\", {\"a\": 1.42, \"b\": mark_safe(\"1.42\")}\n        )\n        self.assertEqual(output, \"1.4 1.4\")\n\n", "    def test_floatformat_with_comma_as_decimal_separator(self):\n        with translation.override(\"de\", deactivate=True):\n            self.assertEqual(floatformat(1234.567, 2), \"1234.57\")\n"], "sample_632": ["def test_empty_files():\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--ignore-imports\", \"empty_file1.py\", \"empty_file2.py\"])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == \"\"\"", "def test_duplicate_code_with_comments():\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([SIMILAR1, SIMILAR2])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == (\n            \"\"\"", "def test_ignore_docstrings_multiline():\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--ignore-docstrings\", SIMILAR1, SIMILAR2])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == (\n            \"\"\"", "def test_ignore_comments_and_docstrings():\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--ignore-comments\", \"--ignore-docstrings\", SIMILAR1, SIMILAR2])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == (\n            \"\"\"", "def test_ignore_comments_and_imports():\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--ignore-comments\", \"--ignore-imports\", SIMILAR1, SIMILAR2])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == (\n            \"\"\""], "sample_99": ["    def test_trunc_date_none_with_timezone(self):\n        self.create_model(None, None)\n        self.assertIsNone(DTModel.objects.annotate(truncated=Trunc('start_datetime', 'day', tzinfo=timezone.utc)).first().truncated)\n", "    def test_trunc_with_custom_timezone(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n        self.create_model(end_datetime, start_datetime)\n\n        \n        class MyTimezone(tzinfo):\n                return timedelta(hours=3)\n                return timedelta(0)\n\n        mytimezone = MyTimezone()\n\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated=Trunc('start_datetime', 'day', output_field=DateTimeField(), tzinfo=mytimezone)\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime.astimezone(mytimezone), 'day')),\n                (end_datetime, truncate_to(end_datetime.astimezone(mytimezone), 'day'))\n            ],\n            lambda m: (m.start_datetime, m.truncated)\n        )\n", "    def test_trunc_func_with_timezone_and_filter(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n        self.create_model(end_datetime, start_datetime)\n\n        melb = pytz.timezone('Australia/Melbourne')\n        qs = DTModel.objects.filter(\n            truncated_date__day=Trunc('start_datetime', 'day', output_field=DateField(), tzinfo=melb)\n        ).order_by('start_datetime')\n        self.assertEqual(qs.count(), 2)\n", "    def test_trunc_with_expression(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        if settings.USE_TZ:\n            start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n            end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n        self.create_model(end_datetime, start_datetime)\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated=Trunc('start_datetime', 'day', output_field=DateField())\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime, 'day')),\n                (end_datetime, truncate_to(end_datetime, 'day')),\n            ],\n            lambda m: (m.start_datetime, m.truncated)\n        )\n", "    def test_trunc_with_timezone_context(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n        self.create_model(end_datetime, start_datetime)\n\n        melb = pytz.timezone('Australia/Melbourne')\n\n        with timezone.override(melb):\n            self.assertQuerysetEqual(\n                DTModel.objects.annotate(truncated=Trunc('start_datetime', 'day')).order_by('start_datetime'),\n                [\n                    (start_datetime, truncate_to(start_datetime.astimezone(melb), 'day')),\n                    (end_datetime, truncate_to(end_datetime.astimezone(melb), 'day')),\n                ],\n                lambda m: (m.start_datetime, m.truncated)\n            )\n"], "sample_1072": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1128": ["def test_point_partial_velocity_with_non_orthogonal_frames():\n    q1 = dynamicsymbols('q1')\n\n    N = ReferenceFrame('N')\n    A = ReferenceFrame('A')\n    A.set_ang_vel(N, q1 * N.x)\n    \n    p = Point('p')\n    p.set_vel(N, (q1**2 * A.x + q1 * N.y))\n    \n    assert p.partial_velocity(N, q1) == (2 * q1 * A.x + N.y)\n\n", "compilation error", "def test_point_vel_when_ref_frame_rotates():\n    t = dynamicsymbols._t\n    q = dynamicsymbols('q')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    B.set_ang_vel(N, q * B.y)\n    P = Point('P')\n    P.set_pos(N.origin, B.x)\n    assert P.vel(N) == q * B.y ^ B.x\n", "    def test_point_vel_after_rotation():\n        t = dynamicsymbols._t\n        q1, q2 = dynamicsymbols('q1 q2')\n        N = ReferenceFrame('N')\n        B = ReferenceFrame('B')\n        O = Point('O')\n        O.set_vel(N, q1 * N.x)\n        B.set_ang_vel(N, q2 * B.z)\n        P = Point('P')\n        P.set_pos(O, N.y)\n        assert P.vel(N) == q1 * N.x + q2 * P.pos_from(O).cross(N.z)\n", "compilation error"], "sample_1138": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1140": ["def test_issue_18566():\n    from sympy import Symbol\n    a = Symbol('a', real=True, positive=True)\n    b = Symbol('b', real=True, positive=True)\n    assert pretty(a**b) == 'a**b'\n", "compilation error", "compilation error", "def test_issue_18577():\n    from sympy.physics.mechanics import ReferenceFrame, dynamicsymbols\n    N = ReferenceFrame('N')\n    theta = dynamicsymbols('theta')\n    omega = N.ang_vel_in(N)\n    assert pretty(omega) == '\u03c9'\n", "def test_issue_18677():\n    M = Matrix([[1, 2], [3, 4]])\n    N = Matrix([[5, 6], [7, 8]])\n    assert pretty(M.inv() * N) == '(1/(-2) * [[-2, 1], [-3, 1]]) * [[5, 6], [7, 8]]'\n\n"], "sample_611": ["compilation error", "compilation error", "compilation error", "    def test_cftime_range_with_calendar(start, end, periods, freq, calendar):\n        result = cftime_range(start, end, periods, freq, calendar=calendar)\n        assert result.calendar == calendar\n", "def test_cftime_range_with_date_format():\n    result = cftime_range(start=\"2000-01-01\", periods=4, freq=\"M\", date_format=\"%Y-%m\")\n    expected = pd.date_range(start=\"2000-01-01\", periods=4, freq=\"M\").strftime(\"%Y-%m\")\n    np.testing.assert_array_equal(result.strftime(\"%Y-%m\"), expected)\n\n"], "sample_135": ["    def test_time_format_with_tzinfo(self):\n        ltz = get_default_timezone()\n        dt = make_aware(datetime(2009, 5, 16, 5, 30, 30), ltz)\n        self.assertEqual(\n            dateformat.time_format(dt, 'r'),\n            'Sat, 16 May 2009 05:30:30 +0200',\n        )\n        self.assertEqual(\n            dateformat.time_format(dt, 'r'),\n            'Sat, 16 May 2009 05:30:30 +0200',\n        )\n", "    def test_naive_datetime_with_dst(self):\n        dt = datetime(2015, 10, 25, 2, 30, 0)\n        aware_dt = make_aware(dt, get_default_timezone())\n        self.assertEqual(format(aware_dt, 'I'), '1')\n", "    def test_aware_datetime_with_different_tzinfo(self):\n        tz1 = get_fixed_timezone(-210)\n        tz2 = get_fixed_timezone(-390)\n        dt = datetime(2009, 5, 16, 5, 30, 30, tzinfo=tz1)\n        dt_converted = dt.astimezone(tz2)\n\n        self.assertEqual(dateformat.format(dt, 'O'), '-0330')\n        self.assertEqual(dateformat.format(dt_converted, 'O'), '-0630')\n", "    def test_timezone_name_with_dst_transition(self):\n        # Test that time zone names are handled correctly when transitioning\n        # into or out of DST.\n        #\n        # Note: This test relies on the fact that Europe/Copenhagen observes\n        # DST.\n        dt_before_dst = datetime(2023, 3, 25, 1, 00, tzinfo=get_fixed_timezone(120))\n        dt_during_dst = datetime(2023, 4, 1, 1, 00, tzinfo=get_fixed_timezone(120))\n        dt_after_dst = datetime(2023, 10, 28, 1, 00, tzinfo=get_fixed_timezone(120))\n\n        self.assertEqual(dateformat.format(dt_before_dst, 'T'), 'CET')\n        self.assertEqual(dateformat.format(dt_during_dst, 'T'), 'CEST')\n        self.assertEqual(dateformat.format(dt_after_dst, 'T'), 'CET')\n\n", "    def test_invalid_date_format(self):\n        with self.assertRaises(ValueError):\n            dateformat.format(datetime(2000, 1, 1), 'X')\n"], "sample_44": ["    def test_log_quantities_with_same_data():\n        # Test that LogQuantity with the same data and unit,\n        # but different physical units, are considered equal\n        lq1 = u.Magnitude(np.arange(1., 4.) * u.Jy)\n        lq2 = u.Magnitude(np.arange(1., 4.), u.mag(u.Jy))\n        assert lq1 == lq2\n\n", "    def test_to_value_and_unit(self):\n        lq = u.Magnitude(np.arange(1., 10.)*u.Jy)\n        v, u = lq.to_value(u.Jy)\n        assert np.all(v == lq.physical.value)\n        assert u == u.Jy\n        with pytest.raises(u.UnitsError):\n            lq.to_value(u.m)\n", "    def test_astype(self):\n        for mag in self.mags:\n            assert mag.astype(float).dtype == np.float64\n            assert mag.astype(int).dtype == np.int64\n            assert mag.astype(u.Quantity).unit == mag.unit\n            with pytest.raises(TypeError):\n                mag.astype(str)\n", "    def test_to_physical(self):\n        assert np.all(self.mJy.to_physical().value ==\n                      self.mJy._function_view.to(self.mJy.unit.physical_unit).value)\n        assert np.all(self.m1.to_physical().value ==\n                      self.m1._function_view.to(self.m1.unit.physical_unit).value)\n", "    def test_argmin_argmax(self):\n        for mag in self.mags:\n            assert np.all(mag.argmin() == mag.value.argmin())\n            assert np.all(mag.argmax() == mag.value.argmax())\n"], "sample_249": ["    def test_destroy_test_db_keepdb(self, mocked_sync_apps, mocked_migrate, *mocked_objects):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0, keepdb=True)\n                # Verify that the database was not actually destroyed\n                self.assertEqual(creation._destroy_test_db.call_count , 0)\n        finally:\n            creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def test_destroy_test_db_keepdb(self, mocked_sync_apps, mocked_migrate, *mocked_objects):\n        test_connection = get_connection_copy()\n        old_database_name = test_connection.settings_dict['NAME']\n        creation = test_connection.creation_class(test_connection)\n        try:\n            creation.create_test_db(verbosity=0, autoclobber=True, serialize=False, keepdb=True)\n            # Check if _destroy_test_db is not called.\n            creation._destroy_test_db.assert_not_called()\n            # Restore the original database name\n            creation.destroy_test_db(old_database_name, verbosity=0, keepdb=True)\n\n        finally:\n            # Restore the original database name\n            creation.destroy_test_db(old_database_name, verbosity=0, keepdb=True)\n", "    def test_destroy_test_db_with_keepdb(self, mocked_sync_apps, mocked_migrate, *mocked_objects):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = True\n        creation = test_connection.creation_class(test_connection)\n        if connection.vendor == 'oracle':\n            # Don't close connection on Oracle.\n            creation.connection.close = mock.Mock()\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n            # Destroy the db but keep it.\n            creation.destroy_test_db(old_database_name, verbosity=0, keepdb=True)\n            # Make sure _destroy_test_db wasn't called\n            creation._destroy_test_db.assert_not_called()\n        finally:\n            # Clean up the test db.\n            creation.destroy_test_db(old_database_name, verbosity=0)\n\n", "    def test_destroy_test_db_keepdb(self, mocked_sync_apps, mocked_migrate, *mocked_objects):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = True\n        creation = test_connection.creation_class(test_connection)\n        if connection.vendor == 'oracle':\n            # Don't close connection on Oracle.\n            creation.connection.close = mock.Mock()\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n            # Migrations run.\n            mocked_migrate.assert_called()\n            args, kwargs = mocked_migrate.call_args\n            self.assertEqual(args, ([('app_unmigrated', '0001_initial')],))\n            self.assertEqual(len(kwargs['plan']), 1)\n            # App is not synced.\n            mocked_sync_apps.assert_not_called()\n            # Destroy the database with keepdb=True\n            creation.destroy_test_db(old_database_name, verbosity=0, keepdb=True)\n            # Make sure the database is not deleted\n            with self.assertRaises(Exception):\n                creation._destroy_test_db(old_database_name, verbosity=0)\n        finally:\n            # Destroy the database with keepdb=False\n            creation.destroy_test_db(old_database_name, verbosity=0)\n\n\n\n", "    def test_test_db_clone_settings(self):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['NAME'] = 'my_test_db'\n        creation = test_connection.creation_class(test_connection)\n        clone_settings = creation.get_test_db_clone_settings(suffix='my_suffix')\n        self.assertEqual(clone_settings['NAME'], 'my_test_db_my_suffix')\n"], "sample_852": ["    def test_make_classification_weights_sum():\n        with pytest.raises(ValueError):\n            make_classification(weights=[.2, .8], weights_sum=1.1)\n", "compilation error", "def test_make_biclusters_noise():\n    X, rows, cols = make_biclusters(\n        shape=(100, 100), n_clusters=4, noise=0.2, shuffle=True, random_state=0\n    )\n    assert X.shape == (100, 100), \"X shape mismatch\"\n    assert rows.shape == (4, 100), \"rows shape mismatch\"\n    assert cols.shape == (4, 100,), \"columns shape mismatch\"\n    assert_all_finite(X)\n    assert_all_finite(rows)\n    assert_all_finite(cols)\n", "    def test_make_moons_noise(self):\n        X, y = make_moons(n_samples=100, noise=0.3, random_state=0)\n        assert X.shape == (100, 2), \"X shape mismatch\"\n        assert y.shape == (100,), \"y shape mismatch\"\n", "compilation error"], "sample_1083": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_244": ["    def test_formset_error_class_with_extra(self):\n        class CustomErrorList(ErrorList):\n            pass\n\n        formset = FavoriteDrinksFormSet(error_class=CustomErrorList, extra=1)\n        self.assertEqual(formset.forms[0].error_class, CustomErrorList)\n        self.assertEqual(formset.empty_form.error_class, CustomErrorList) \n", "    def test_empty_formset_with_error(self):\n        class BrokenForm(Form):\n            field = CharField()\n\n                raise ValidationError(\"This form always has an error\")\n\n        BrokenFormset = formset_factory(BrokenForm, extra=0)\n        formset = BrokenFormset()\n        self.assertFalse(all_valid([formset]))\n        self.assertEqual(formset.errors, [{'': ['This form always has an error']}])\n\n\n", "    def test_formset_with_custom_error_class(self):\n        class CustomErrorList(ErrorList):\n            pass\n\n        ChoiceFormSet = formset_factory(Choice, error_class=CustomErrorList)\n        data = {\n            'choices-TOTAL_FORMS': '1',\n            'choices-INITIAL_FORMS': '0',\n            'choices-0-choice': 'Calexico',\n            'choices-0-votes': '100',\n        }\n\n        formset = ChoiceFormSet(data)\n        self.assertTrue(isinstance(formset.errors, CustomErrorList))\n", "    def test_empty_formset_errors(self):\n        \"\"\"An empty formset should not raise an error when calling errors.\"\"\"\n        formset = formset_factory(Choice, extra=0)()\n        self.assertEqual(formset.errors, [])\n", "    def test_empty_formset_with_non_form_errors(self):\n        ChoiceFormSet = formset_factory(Choice, extra=0)\n        formset = ChoiceFormSet(\n            {'choices-TOTAL_FORMS': '0', 'choices-INITIAL_FORMS': '0'},\n            prefix='choices',\n        )\n        formset.clean()\n        self.assertEqual(formset.non_form_errors(), [])\n"], "sample_184": ["    def test_check_constraint_pointing_to_m2m_field_through_reverse_fk(self):\n        class Target(models.Model):\n            pass\n\n        class Model(models.Model):\n            targets = models.ManyToManyField(Target, through='ThroughModel')\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(fields=['id'], include=['targets__name'], name='name'),\n                ]\n\n        class ThroughModel(models.Model):\n            model = models.ForeignKey(Model, models.CASCADE)\n            target = models.ForeignKey(Target, models.CASCADE)\n\n        self.assertEqual(Model.check(databases=self.databases), [\n            Error(\n                \"'constraints' refers to a ManyToManyField 'targets' \"\n                \"through 'ThroughModel', but ManyToManyFields are not \"\n                \"permitted in 'constraints'.\",\n                obj=Model,\n                id='models.E013',\n            ),\n        ])\n", "    def test_check_constraint_pointing_to_reverse_fk(self):\n        class Model(models.Model):\n            parent = models.ForeignKey('self', models.CASCADE, related_name='parents')\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(name='name', check=models.Q(parents__count=2)),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [\n            Error(\n                \"'constraints' refers to the nonexistent field 'parents__count'.\",\n                obj=Model,\n                id='models.E012',\n            ),\n        ])\n", "    def test_check_constraint_pointing_to_reverse_fk_with_on_delete(self):\n        class Model(models.Model):\n            parent = models.ForeignKey('self', models.CASCADE, related_name='parents')\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(name='name', check=models.Q(parents=3)),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [\n            Error(\n                \"'constraints' refers to the nonexistent field 'parents'.\",\n                obj=Model,\n                id='models.E012',\n            ),\n        ])\n\n", "    def test_check_constraint_pointing_to_field_in_proxy_model(self):\n        class Base(models.Model):\n            value = models.IntegerField()\n\n        class Proxy(Base):\n            class Meta:\n                proxy = True\n\n        class Model(models.Model):\n            proxy_field = models.ForeignKey(Proxy, on_delete=models.CASCADE)\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(\n                        name='name', check=models.Q(proxy_field__value=2)\n                    ),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [])\n\n", "    def test_check_constraint_pointing_to_reversed_fk(self):\n        class Model(models.Model):\n            parent = models.ForeignKey('self', models.CASCADE, related_name='parents')\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(name='name', check=models.Q(parent__parents=2)),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [\n            Error(\n                \"'constraints' refers to the nonexistent field 'parent__parents'.\",\n                obj=Model,\n                id='models.E012',\n            ),\n        ])\n"], "sample_1000": ["def test_octave_piecewise_with_functions():\n    pw = Piecewise((sin(x), x < 0), (cos(x), x <= 1), (tan(x), True))\n    assert mcode(pw) == \"((x < 0).*(sin(x)) + (~(x < 0)).*((x <= 1).*(cos(x)) + (~(x <= 1)).*(tan(x))))\"\n    assert mcode(pw, assign_to='y') == \"y = ((x < 0).*(sin(x)) + (~(x < 0)).*((x <= 1).*(cos(x)) + (~(x <= 1)).*(tan(x))));\"\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_884": ["compilation error", "def test_deprecated_property():\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        MockClass2().n_features_\n", "    def test_deprecated_property():\n        obj = MockClass2()\n        with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n            val = obj.n_features_\n        assert val == 10\n", "compilation error", "compilation error"], "sample_327": ["    def test_empty_string_input(self):\n        class JSONForm(Form):\n            json_field = JSONField(required=False)\n\n        form = JSONForm({'json_field': ''})\n        self.assertIsNone(form['json_field'].value())\n        self.assertEqual(form.errors['json_field'], [])", "    def test_redisplay_empty_string(self):\n        class JSONForm(Form):\n            json_field = JSONField(required=False)\n\n        form = JSONForm({'json_field': ''})\n        self.assertEqual(form.errors, {})\n        self.assertEqual(form['json_field'].value(), '')\n        self.assertIn('</textarea>', form.as_p())\n\n", "    def test_empty_string_as_input(self):\n        field = JSONField(required=False)\n        self.assertIsNone(field.clean(''))\n", "    def test_bound_data_with_invalid_json(self):\n        field = JSONField()\n        data = '{\"foo\": \"bar'\n        value = field.bound_data(data, None)\n        self.assertIsInstance(value, InvalidJSONInput)\n        self.assertEqual(value, data)\n", "    def test_empty_string_input(self):\n        field = JSONField(required=False)\n        self.assertEqual(field.clean(''), '{}')\n        self.assertEqual(field.prepare_value(''), '{}')\n        self.assertEqual(field.clean(None), None)\n        self.assertEqual(field.prepare_value(None), 'null')\n"], "sample_1032": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_907": ["    def test_domain_cpp_parse_mixed_decl_unique(app, warning):\n        text = (\".. cpp:class:: A\\n\"\n                \".. cpp:function:: void A()\\n\"\n                \".. cpp:member:: int A::a\\n\")\n        restructuredtext.parse(app, text)\n        ws = warning.getvalue().split(\"\\n\")\n        assert len(ws) == 0\n\n", "def test_domain_cpp_parse_noindexentry_member(app):\n    text = (\".. cpp:class:: A\\n\"\n            \"   .. cpp:member:: int x\\n\"\n            \"      :noindexentry:\\n\"\n            \"   .. cpp:member:: int y\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[('single', 'A (C++ class)', '_CPPv41A', '', None)])\n    assert_node(doctree[2], addnodes.index, entries=[('single', 'y (C++ member)', '_CPPv41AY', '', None)])\n", "def test_domain_cpp_parse_cpp_function_with_trailing_newline(app, warning):\n    text = (\n        \".. cpp:function:: void f()\\n\"\n        \"\\n\"\n    )\n    restructuredtext.parse(app, text)\n    ws = warning.getvalue().split(\"\\n\")\n    assert len(ws) == 0\n\n", "def test_domain_cpp_parse_enum_noindexentry(app):\n    text = (\".. cpp:enum:: MyEnum\\n\"\n            \"   :noindexentry:\\n\"\n            \"   .. cpp:enumerator:: VAL1\\n\"\n            \"   .. cpp:enumerator:: VAL2\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, addnodes.index))\n    assert_node(doctree[0], addnodes.index, entries=[])\n    assert_node(doctree[1], addnodes.index, entries=[('single', 'VAL1 (C++ enumerator)', '_CPPv45VAL1E', '', None)])\n    assert_node(doctree[2], addnodes.index, entries=[('single', 'VAL2 (C++ enumerator)', '_CPPv45VAL2E', '', None)])\n", "def test_domain_cpp_parse_mix_decl_duplicate(app, warning):\n    # Issue 8270\n    text = (\".. cpp:class:: A\\n\"\n            \".. cpp:function:: void A()\\n\"\n            \".. cpp:class:: A\\n\")\n    restructuredtext.parse(app, text)\n    ws = warning.getvalue().split(\"\\n\")\n    assert len(ws) == 5\n    assert \"index.rst:2: WARNING: Duplicate C++ declaration, also defined at index:1.\" in ws[0]\n    assert \"Declaration is '.. cpp:function:: void A()'.\" in ws[1]\n    assert \"index.rst:3: WARNING: Duplicate C++ declaration, also defined at index:1.\" in ws[2]\n    assert \"Declaration is '.. cpp:class:: A'.\" in ws[3]\n    assert ws[4] == \"\"\n"], "sample_1034": ["def test_grover_iteration_with_negative_phase():\n    numqubits = 2\n    basis_states = superposition_basis(numqubits)\n    v = OracleGate(numqubits, lambda qubits: qubits == IntQubit(3))\n    expected =  -IntQubit(3, nqubits=numqubits)\n    assert qapply(grover_iteration(basis_states, v)) == expected\n", "def test_OracleGate_repr():\n    v = OracleGate(2, return_one_on_two)\n    assert str(v) == 'V(2, <function return_one_on_two at 0x...>)'\n", "    def test_grover_multiple_solutions():\n        nqubits = 3\n        f = lambda qubits: qubits == IntQubit(3) or qubits == IntQubit(5)\n        result = apply_grover(f, nqubits)\n        assert result == IntQubit(3, nqubits=nqubits) or result == IntQubit(5, nqubits=nqubits)\n", "    def test_grover_iteration_generalized():\n        numqubits = 3\n        basis_states = superposition_basis(numqubits)\n        v = oracle_gate = OracleGate(numqubits, lambda qubits: (qubits == IntQubit(4, nqubits=numqubits)))\n        expected = IntQubit(4, nqubits=numqubits)\n        assert qapply(grover_iteration(basis_states, v)) == expected\n", "    def test_grover_iteration_with_multiple_solutions():\n        numqubits = 3\n        basis_states = superposition_basis(numqubits)\n        v = OracleGate(numqubits, lambda qubits: qubits == IntQubit(2, nqubits=numqubits)\n                                            or qubits == IntQubit(6, nqubits=numqubits))\n        # After (pi/4)sqrt(pow(2, n)), IntQubit(2) and IntQubit(6) should\n        # have highest probability\n        iterated = grover_iteration(basis_states, v)\n        iterated = qapply(iterated)\n        iterated = grover_iteration(iterated, v)\n        iterated = qapply(iterated)\n        # Ask about measurement\n        # Check that the probabilities of IntQubit(2) and IntQubit(6) are maximized \n        # compared to other states.\n"], "sample_631": ["    def test_unused_variable_in_async_function(self):\n        node = astroid.parse(\n            \"\"\"\n        async def foo():\n            x = 1\n        \"\"\"\n        )\n        with self.assertAddsMessages(\n            Message(\"unused-variable\", node=node[\"x\"], args=\"x\")\n        ):\n            self.walk(node)\n", "    def test_module_level_unused_variable(self):\n        node = astroid.parse(\n            \"\"\"\n            data = 1\n                pass\n            \"\"\"\n        )\n        with self.assertAddsMessages(\n            Message(\"unused-variable\", node=node.body[0], args=(\"data\",))\n        ):\n            self.checker.visit_module(node)\n", "    def test_undefined_variable_from_redefined(self):\n        node = astroid.parse(\n            \"\"\"\n        from foo import bar as _bar\n        print(_bar)\n        \"\"\"\n        )\n        with set_config(allow_redefined_builtin=True):\n            with self.assertAddsMessages(\n                Message(\"undefined-variable\", node=node.body[1], args=\"_bar\")\n            ):\n                self.walk(node)\n", "    def test_unused_variable_with_type_annotation(self):\n        node = astroid.extract_node(\n            \"\"\"\n            from typing import List\n\n                b: str = 1  # Should trigger unused variable\n\n            func()\n            \"\"\"\n        )\n        with self.assertAddsMessages(\n            Message('unused-variable', node=node['b'], args=\"b\")\n        ):\n            self.checker.visit_module(node.root())\n            self.checker.visit_functiondef(node)\n            self.checker.leave_functiondef(node)\n\n", "    def test_import_from_within_function(self):\n        node = astroid.extract_node(\n            \"\"\"\n            from . import bar\n        \"\"\"\n        )\n        with self.assertNoMessages():\n            self.checker.visit_functiondef(node)\n"], "sample_295": ["    def test_expression_wrapper_as_sql(self):\n        expr = ExpressionWrapper(F('col'), output_field=IntegerField())\n        self.assertEqual(expr.as_sql(connection=connection, compiler=connection.ops.compiler_class()), ('%s', [F('col')]))\n", "    def test_expressionwrapper_resolve_output_field(self):\n        class TestModel(models.Model):\n            field = models.CharField(max_length=10)\n\n        expr = ExpressionWrapper(F('field'), output_field=IntegerField())\n        self.assertFalse(expr.get_source_expression().concrete)\n        with self.assertRaises(FieldError):\n            expr.resolve_expression(TestModel, alias='test')\n        self.assertTrue(expr.output_field is None)\n\n        expr.output_field = IntegerField()\n        expr.resolve_expression(TestModel, alias='test')\n        self.assertTrue(expr.cached_output_field is IntegerField())\n", "    def test_expression_wrapper_repr(self):\n        expr = ExpressionWrapper(Lower('a'), output_field=CharField())\n        self.assertEqual(repr(expr), 'ExpressionWrapper(Lower(F(a)))')\n", "    def test_expressionwrapper_with_empty_expressionlist(self):\n        expr = ExpressionWrapper(ExpressionList(), output_field=IntegerField())\n        self.assertEqual(expr.get_group_by_cols(alias=None), [])\n", "    def test_expressionwrapper_output_field(self):\n        value = ExpressionWrapper(Value(3), output_field=IntegerField())\n        self.assertIsInstance(value.output_field, IntegerField)\n"], "sample_661": ["def test_system_out_and_err_both_empty(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        junit_logging=system-out-err\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testcase\")\n    assert len(node.find_by_tag(\"system-err\")) == 0\n    assert len(node.find_by_tag(\"system-out\")) == 0\n\n", "def test_record_testsuite_property_same_name_twice(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            record_testsuite_property(\"stats\", \"all good\")\n            record_testsuite_property(\"stats\", \"all bad\")\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testsuite\")\n    properties_node = node.find_first_by_tag(\"properties\")\n    p1_node = properties_node.find_nth_by_tag(\"property\", 0)\n    p2_node = properties_node.find_nth_by_tag(\"property\", 1)\n    p1_node.assert_attr(name=\"stats\", value=\"all good\")\n    p2_node.assert_attr(name=\"stats\", value=\"all bad\") \n", "    def test_record_testsuite_property_type_checking(testdir):\n\n\n        testdir.makepyfile(\n            \"\"\"\n                record_testsuite_property(1, 2)", "def test_logxml_path_expansion_tilde_in_root(testdir, monkeypatch):\n    home_tilde = py.path.local(os.path.expanduser(\"~\")).join(\"test.xml\")\n    xml_tilde = LogXML(\"~%stest.xml\" % os.sep, None)\n    assert xml_tilde.logfile == home_tilde\n\n    monkeypatch.setenv(\"HOME\", str(testdir))\n    home_var = os.path.normpath(os.path.expandvars(\"$HOME/test.xml\"))\n    xml_var = LogXML(\"$HOME%stest.xml\" % os.sep, None)\n    assert xml_var.logfile == home_var\n", "def test_record_testsuite_property_multiple_calls(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            record_testsuite_property(\"stats\", \"all good\")\n            record_testsuite_property(\"stats\", \"something else\")\n\n            record_testsuite_property(\"stats\", 10)\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testsuite\")\n    properties_node = node.find_first_by_tag(\"properties\")\n    p1_node = properties_node.find_nth_by_tag(\"property\", 0)\n    p2_node = properties_node.find_nth_by_tag(\"property\", 1)\n    p3_node = properties_node.find_nth_by_tag(\"property\", 2)\n    p1_node.assert_attr(name=\"stats\", value=\"all good\")\n    p2_node.assert_attr(name=\"stats\", value=\"something else\")\n    p3_node.assert_attr(name=\"stats\", value=\"10\")\n\n"], "sample_830": ["def test_get_blas_info(capsys):\n    show_versions()\n    out, err = capsys.readouterr()\n    assert 'macros' in out\n    assert 'lib_dirs' in out\n    assert 'cblas_libs' in out \n", "def test_get_blas_info(capsys):\n    show_versions()\n    out, err = capsys.readouterr()\n    assert 'macros' in out\n    assert 'lib_dirs' in out\n    assert 'cblas_libs' in out\n\n", "    def test_get_blas_info(capsys):\n        show_versions()\n        out, err = capsys.readouterr()\n        assert 'macros' in out\n        assert 'lib_dirs' in out\n        assert 'cblas_libs' in out\n", "def test_get_blas_info(capsys):\n    show_versions()\n    out, err = capsys.readouterr()\n    assert 'macros' in out\n    assert 'lib_dirs' in out\n    assert 'cblas_libs' in out\n\n", "def test_get_blas_info(capsys):\n    show_versions()\n    out, err = capsys.readouterr()\n    assert 'macros' in out\n    assert 'lib_dirs' in out\n    assert 'cblas_libs' in out\n"], "sample_218": ["    def test_trunc_with_different_timezones(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n        self.create_model(end_datetime, start_datetime)\n\n        melb = pytz.timezone('Australia/Melbourne')\n        london = pytz.timezone('Europe/London')\n\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated_melb=Trunc('start_datetime', 'day', tzinfo=melb, output_field=DateTimeField()),\n                truncated_london=Trunc('start_datetime', 'day', tzinfo=london, output_field=DateTimeField()),\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime.astimezone(melb), 'day'), truncate_to(start_datetime.astimezone(london), 'day')),\n                (end_datetime, truncate_to(end_datetime.astimezone(melb), 'day'), truncate_to(end_datetime.astimezone(london), 'day')),\n            ],\n            lambda m: (m.start_datetime, m.truncated_melb, m.truncated_london)\n        )\n", "    def test_trunc_func_with_timezone_and_different_output_field(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n        melb = pytz.timezone('Australia/Melbourne')\n\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated_date=Trunc('start_datetime', 'day', output_field=DateField(), tzinfo=melb),\n                truncated_datetime=Trunc('start_datetime', 'day', output_field=DateTimeField(), tzinfo=melb),\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime.astimezone(melb).date(), 'day'),\n                 truncate_to(start_datetime.astimezone(melb), 'day')),\n                (end_datetime, truncate_to(end_datetime.astimezone(melb).date(), 'day'),\n                 truncate_to(end_datetime.astimezone(melb), 'day')),\n            ],\n            lambda m: (m.start_datetime, m.truncated_date, m.truncated_datetime)\n        )\n\n", "    def test_trunc_week_start_day(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        if settings.USE_TZ:\n            start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n            end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n        self.create_model(end_datetime, start_datetime)\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(truncated=TruncWeek('start_datetime', start_day=MONDAY)).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime, 'week', start_day=MONDAY)),\n                (end_datetime, truncate_to(end_datetime, 'week', start_day=MONDAY))\n            ],\n            lambda m: (m.start_datetime, m.truncated)\n        )\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(truncated=TruncWeek('start_datetime', start_day=TUESDAY)).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime, 'week', start_day=TUESDAY)),\n                (end_datetime, truncate_to(end_datetime, 'week', start_day=TUESDAY))\n            ],\n            lambda m: (m.start_datetime, m.truncated)\n        )\n\n", "    def test_trunc_timezone_aware_comparison(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n        melb = pytz.timezone('Australia/Melbourne')\n\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(truncated=TruncDay('start_datetime', tzinfo=melb)).filter(\n                truncated__lt=TruncDay('start_datetime', tzinfo=melb)\n            ),\n            []\n        )\n\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(truncated=TruncDay('start_datetime', tzinfo=melb)).filter(\n                truncated__lte=TruncDay('start_datetime', tzinfo=melb)\n            ),\n            [\n                (\n                    start_datetime,\n                    truncate_to(start_datetime.astimezone(melb), 'day', melb)\n                )\n            ],\n            lambda m: (m.start_datetime, m.truncated)\n        )\n", "    def test_trunc_with_different_timezone_than_model(self):\n        start_datetime = datetime(2015, 6, 15, 23, 30, 1, 321)\n        end_datetime = datetime(2015, 6, 16, 13, 11, 27, 123)\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n        melb = pytz.timezone('Australia/Melbourne')\n        \n        qs = DTModel.objects.annotate(\n            truncated_start=Trunc('start_datetime','day', output_field=DateTimeField(), tzinfo=melb),\n        ).order_by('start_datetime')\n        self.assertEqual(qs[0].truncated_start.tzinfo, melb)\n"], "sample_1157": ["    def test_repeated_decimals_overflow():\n        assert parse_expr(\"1.234[5678901234567890]\") == Rational(\n            1234567890123456789,\n            9999999999999999990)\n", "    def test_parse_expr_with_global_dict():\n        global_dict = {'sin': sin}\n        result = parse_expr('sin(x)', global_dict=global_dict)\n        assert result == sin('x')\n", "    def test_repeated_decimals_order():\n        assert parse_expr(\"0.1[3] + 0.3[1]\") == Rational(13, 30)\n", "    def test_repeated_decimals_mixed_symbol():\n        x = Symbol('x')\n        assert parse_expr('0.1[1] + x', transformations=(repeated_decimals,\n                                                              standard_transformations)) == Rational(11, 90) + x\n", "    def test_issue_19815():\n        x = Symbol('x')\n        y = Symbol('y')\n        eq = parse_expr('2^(x+y)', transformations=standard_transformations + (implicit_multiplication_application,))\n        assert eq == 2**(x + y)\n"], "sample_1197": ["def test_issue_24483():\n    from sympy.physics.units import meter, second\n    from sympy.physics.units.physical_quantities import PhysicalQuantity\n    from sympy.physics.units.definitions.dimension_definitions import Dimension\n\n    meter2 = PhysicalQuantity('meter2', 'm2', 'length*length', 'square meter', 1, meter*meter)\n    assert meter2.dimension == Dimension(length**2)\n    assert convert_to(meter2, meter**2) == 1*meter**2\n    assert convert_to(meter**2, meter2) == 1\n    assert SI._collect_factor_and_dimension(meter2) == (1, Dimension(length**2))\n\n    second2 = PhysicalQuantity('second2', 's2', 'time*time', 'square second', 1, second*second)\n    assert second2.dimension == Dimension(time**2)\n    assert convert_to(second2, second**2) == 1*second**2\n    assert convert_to(second**2, second2) == 1\n    assert SI._collect_factor_and_dimension(second2) == (1, Dimension(time**2))\n", "compilation error", "def test_issue_25663():\n    from sympy.physics.units import Kilometer, Meter\n\n    k = Kilometer()\n    m = Meter()\n    assert k.convert_to(m) == 1000*m\n", "compilation error", "compilation error"], "sample_946": ["def test_typehints(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert '<span class=\"n\"><span class=\"pre\">int</span></span>' in content\n    assert '<span class=\"n\"><span class=\"pre\">List</span></span>[<span class=\"n\"><span class=\"pre\">str</span></span>]' in content\n\n", "def test_domain_objects(app):\n    app.builder.env.domains['py'].objects = {}\n    text = \".. py:class:: Class\\n\"\n    doctree = restructuredtext.parse(app, text)\n    assert 'Class' in app.env.get_domain('py').objects\n    assert app.env.get_domain('py').objects['Class'] == ('index', 'Class', 'class', False)\n", "def test_py_function_nested(app):\n    text = (\".. py: function:: outer_func\\n\"\n            \"\\n\"\n            \"   This is an outer function.\\n\"\n            \"\\n\"\n            \"   :param arg1: Description of arg1\\n\"\n            \"\\n\"\n            \"   .. py: function:: inner_func\\n\"\n            \"\\n\"\n            \"      This is a nested function.\\n\"\n            \"\\n\"\n            \"      :param arg2: Description of arg2\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          desc,\n                          desc))\n    assert_node(doctree[0], addnodes.index, entries=[('single', 'outer_func() (built-in function)', 'outer_func', '', None)])\n    assert_node(doctree[1][1][0], addnodes.index, entries=[('single', 'inner_func() (built-in function)', 'outer_func.inner_func', '', None)])\n\n", "def test_class_attribute_noindex(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert 'MyClass.attribute' not in content\n\n", "def test_inheritance_diagram(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert '<img src=\"_images/inheritance_diagram.png\" class=\"inheritance-diagram\">' in content\n"], "sample_74": ["    def test_empty_params(self):\n        self.assertEqual(\n            self._run_it({}), (\n                ['psql'],\n                {},\n            )\n        )\n", "    def test_empty_params(self):\n        self.assertEqual(\n            self._run_it({}), (\n                ['psql'],\n                {},\n            )\n        )\n", "    def test_empty_host(self):\n        self.assertEqual(\n            self._run_it({\n                'database': 'dbname',\n                'user': 'someuser',\n                'password': 'somepassword',\n            }), (\n                ['psql', '-U', 'someuser', 'dbname'],\n                {'PGPASSWORD': 'somepassword'},\n            )\n        ) \n", "    def test_empty_connection_params(self):\n        self.assertEqual(\n            self._run_it({}), (\n                ['psql'],\n                {},\n            )\n        )\n", "    def test_empty_connection_params(self):\n        self.assertEqual(\n            self._run_it({}),\n            (['psql'], {}),\n        )\n"], "sample_839": ["    def test_vectorizer_empty_input(self):\n        vectorizer = CountVectorizer()\n        with pytest.raises(ValueError, match=\"Input data cannot be empty\"):\n            vectorizer.fit_transform([])\n\n", "    def test_tfidfvectorizer_empty_vocabulary(self):\n        vect = TfidfVectorizer()\n        with pytest.raises(ValueError):\n            vect.fit([])\n\n", "def test_vectorizer_empty_corpus(Estimator):\n    # check if empty corpus raises error\n    vec = Estimator()\n\n    with pytest.raises(ValueError, match='Empty corpus'):\n        vec.fit_transform([])\n\n", "def test_tfidfvectorizer_ngram_range_errors():\n    # Non-regression test: Ensure ngram_range errors are raised correctly\n    with pytest.raises(ValueError, match=\"Invalid value for ngram_range: (1, 0)\"):\n        TfidfVectorizer(ngram_range=(1, 0)).fit([\"hello world\"])\n\n    with pytest.raises(ValueError, match=\"Invalid value for ngram_range: (2, 1)\"):\n        TfidfVectorizer(ngram_range=(2, 1)).fit([\"hello world\"])\n\n", "    def test_vectorizer_empty_input_list(self):\n        # Test when fit_transform is called with an empty list\n        vect = CountVectorizer()\n        with pytest.raises(ValueError, match=\"Cannot create the matrix with empty input\"):\n            vect.fit_transform([])\n"], "sample_293": ["    def test_include_nested_namespace(self):\n        with override_settings(ROOT_URLCONF='urlpatterns_reverse.nested_namespace_urls'):\n            response = self.client.get('/inc-ns1/test3/inner/')\n            self.assertEqual(response.status_code, 200)\n", "    def test_include_with_urlpatterns_is_iterable(self):\n        self.assertEqual(include(self.url_patterns, 'app_name'), (self.url_patterns, 'app_name', 'app_name'))\n", "    def test_include_empty_tuple(self):\n        with self.assertRaisesMessage(ImproperlyConfigured, 'include() argument must be a tuple or URLObject'):\n            include(())\n", "    def test_include_regex_names(self):\n        patterns = [\n            re_path(r'^inner/(?P<arg1>[0-9]+)/(?P<arg2>[0-9]+)/$', views.empty_view, name='regex-view'),\n        ]\n        included = include(patterns, namespace='included')\n        self.assertEqual(included[0][0].regex.pattern, r'^inner/(?P<arg1>[0-9]+)/(?P<arg2>[0-9]+)/$')\n        self.assertEqual(included[0][0].name, 'included:regex-view')\n", "    def test_include_with_no_app_name(self):\n        url_patterns = [\n            path('inner/', views.empty_view, name='urlobject-view'),\n        ]\n        expected = (url_patterns, None, None)\n        self.assertEqual(include(url_patterns), expected)\n\n"], "sample_1134": ["    def test_latex_multivariate_integral():\n        x, y = symbols('x y', real=True)\n        f = x*y\n        assert latex(Integral(f, (x, 0, 1), (y, 0, 2))) == r'\\int_{0}^{1} \\int_{0}^{2} x y \\, dy \\, dx'\n        assert latex(Integral(f, (x, 0, 1), (y, x, 2))) == r'\\int_{0}^{1} \\int_{x}^{2} x y \\, dy \\, dx'\n\n\n", "def test_issue_19164():\n    f = Function('f')\n    x, y = symbols('x y')\n    assert latex(f(x, y).diff(x).diff(y)) == r'\\frac{\\partial^{2}}{\\partial y \\partial x} f{\\left(x, y \\right)}'\n", "compilation error", "compilation error", "compilation error"], "sample_691": ["def test_get_timeout_config_value(pytester: Pytester) -> None:\n    \"\"\"Test that the get_timeout_config_value function retrieves the correct timeout value.\"\"\"\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        faulthandler_timeout = 5.0\n        \"\"\"\n    )\n    from _pytest.faulthandler import FaultHandlerHooks\n    assert FaultHandlerHooks.get_timeout_config_value(pytester.config) == 5.0\n", "def test_timeout_after_test_duration(pytester: Pytester, timeout):\n    pytester.makepyfile(\n        f\"\"\"\n        import time\n            time.sleep({timeout})\n    \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        faulthandler_timeout = {timeout - 0.1}\n        \"\"\"\n    )\n\n    result = pytester.runpytest()\n    tb_output = \"most recent call first\"\n    result.stderr.fnmatch_lines([\"*%s*\" % tb_output])\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n    assert result.ret == 0\n", "def test_timeout_without_stderr_dup(pytester: Pytester) -> None:\n    \"\"\"Test that fault handler works even without stderr duplication.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n    import os, time\n        time.sleep(1 if \"CI\" in os.environ else 0.1)\n    \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        faulthandler_timeout = 0.01\n        \"\"\"\n    )\n\n    # Temporarily remove the stderr duplication logic.\n    with pytest.MonkeyPatch.context() as mp:\n        mp.setattr(\n            \"_pytest.faulthandler.FaultHandlerHooks\", \"_get_stderr_fileno\",  lambda self: sys.stderr.fileno()\n        )\n\n    result = pytester.runpytest_subprocess()\n    tb_output = \"most recent call first\"\n    result.stderr.fnmatch_lines([\"*%s*\" % tb_output])\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n    assert result.ret == 0\n", "def test_multiple_tests(faulthandler_timeout: int, pytester: Pytester) -> None:\n    \"\"\"Test simultaneous crash and timeout.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import faulthandler\n        import time\n            faulthandler._sigabrt()\n            time.sleep(1)\n    \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        faulthandler_timeout = %s\n        \"\"\"\n        % faulthandler_timeout\n    )\n    result = pytester.runpytest()\n    assert result.ret != 0\n    if faulthandler_timeout > 0:\n        result.stderr.fnmatch_lines([\"*Fatal Python error*\"])\n", "def test_timeout_zero(pytester: Pytester) -> None:\n    \"\"\"Test that setting faulthandler_timeout to 0 disables timeout functionality.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n    import time\n        time.sleep(1)\n    \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        faulthandler_timeout = 0\n        \"\"\"\n    )\n    result = pytester.runpytest_subprocess()\n    assert \"most recent call first\" not in result.stderr.str()\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n    assert result.ret == 0\n"], "sample_80": ["    def test_negated_isnull(self):\n        query = Query(Author)\n        where = query.build_where(~Q(name__isnull=True))\n        lookup = where.children[0]\n        self.assertIsInstance(lookup, IsNull)\n        self.assertEqual(lookup.lhs.target, Author._meta.get_field('name'))\n        self.assertFalse(lookup.negated)\n\n", "    def test_related_lookup_negation(self):\n        query = Query(Author)\n        where = query.build_where(~Q(item__isnull=True))\n        lookup = where.children[0]\n        self.assertIsInstance(lookup, RelatedIsNull)\n        self.assertFalse(lookup.negated)\n        self.assertEqual(lookup.lhs.target.field.name, 'item')\n", "    def test_related_null_join_promotion(self):\n        query = Query(ObjectC)\n        where = query.build_where(Q(objecta__isnull=True, objectb__isnull=False))\n        self.assertEqual(len(query.alias_refcount), 3)\n        self.assertIn('objectc', query.alias_refcount)\n        self.assertIn('objecta', query.alias_refcount)\n        self.assertIn('objectb', query.alias_refcount)\n\n\n\n", "    def test_empty_string_isnull(self):\n        query = Query(Author)\n        query.add_filter(Q(name__isnull=True))\n        where = query.build_where()\n        lookup = where.children[0]\n        self.assertIsInstance(lookup, IsNull)\n        self.assertEqual(lookup.lhs.target, Author._meta.get_field('name'))\n", "    def test_related_isnull_and_exact(self):\n        query = Query(ObjectC)\n        where = query.build_where(Q(objecta__isnull=False, objecta__num=2))\n        isnull_lookup, exact_lookup = where.children[0].children\n        self.assertIsInstance(isnull_lookup, RelatedIsNull)\n        self.assertIsInstance(isnull_lookup.lhs, SimpleCol)\n        self.assertEqual(isnull_lookup.lhs.target, ObjectC._meta.get_field('objecta'))\n\n        self.assertIsInstance(exact_lookup, Exact)\n        self.assertIsInstance(exact_lookup.lhs, SimpleCol)\n        self.assertEqual(exact_lookup.lhs.target, ObjectC._meta.get_field('objecta__num'))\n        self.assertEqual(exact_lookup.rhs, 2)\n"], "sample_715": ["def test_learning_curve_with_pandas_dataframe():\n    # Test learning curve with pandas DataFrame input\n    iris = load_iris()\n    X = pd.DataFrame(iris.data, columns=iris.feature_names)\n    y = iris.target\n\n    clf = MockImprovingEstimator(20)\n    train_sizes, train_scores, test_scores = learning_curve(\n        clf, X, y, cv=3, train_sizes=np.linspace(0.1, 1.0, 10))\n    assert_array_equal(train_sizes, np.linspace(2, 20, 10))\n    assert_array_almost_equal(train_scores.mean(axis=1),\n                              np.linspace(1.9, 1.0, 10))\n    assert_array_almost_equal(test_scores.mean(axis=1),\n                              np.linspace(0.1, 1.0, 10))\n", "    def test_cross_val_predict_with_groups():\n        iris = load_iris()\n        X, y = iris.data, iris.target\n        groups = np.tile(np.arange(3), 50)\n        est = LogisticRegression()\n        predictions = cross_val_predict(est, X, y, groups=groups, cv=GroupKFold())\n        assert_equal(len(predictions), len(y))\n\n", "    def test_cross_val_score_with_sample_weight():\n        # Test that cross_val_score works correctly with sample weights\n\n        iris = load_iris()\n        X, y = iris.data, iris.target\n        sample_weight = np.random.rand(len(y))\n\n        clf = LogisticRegression()\n        scores = cross_val_score(clf, X, y, sample_weight=sample_weight)\n        assert_array_equal(scores.shape, (5,))\n", "    def test_cross_val_predict_with_precomputed_kernel():\n        X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n        y = np.array([0, 1, 0, 1])\n\n        # Create a classifier that uses a precomputed kernel\n        clf = SVC(kernel='precomputed')\n\n        # Precompute the kernel matrix\n        K = np.array([[1, 0.5, 0.2, 0.1],\n                      [0.5, 1, 0.3, 0.2],\n                      [0.2, 0.3, 1, 0.5],\n                      [0.1, 0.2, 0.5, 1]])\n\n        predictions = cross_val_predict(clf, K, y, cv=2)\n\n        assert_array_equal(predictions,\n                           np.array([0, 1, 0, 1]))\n", "def test_cross_val_score_with_sparse_matrices():\n    X = sparse.csr_matrix([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n    clf = LogisticRegression()\n    scores = cross_val_score(clf, X, y, cv=2)\n    assert_array_almost_equal(scores.shape, (2,))\n\n"], "sample_1158": ["def test_issue_21536_complex():\n    #test to check evaluate=False in case of iterable input with complex numbers\n    u = sympify(\"x+3*x+2j\", evaluate=False)\n    v = sympify(\"2*x+4*x+2+4j\", evaluate=False)\n\n    assert u.is_Add and set(u.args) == {x, 3*x, 2*I}\n    assert v.is_Add and set(v.args) == {2*x, 4*x, 2, 4*I}\n    assert sympify([\"x+3*x+2j\", \"2*x+4*x+2+4j\"], evaluate=False) == [u, v]\n\n    #test to check evaluate=True in case of iterable input with complex numbers\n    u = sympify(\"x+3*x+2j\", evaluate=True)\n    v = sympify(\"2*x+4*x+2+4j\", evaluate=True)\n\n    assert u.is_Add and set(u.args) == {4*x, 2*I}\n    assert v.is_Add and set(v.args) == {6*x, 2, 4*I}\n    assert sympify([\"x+3*x+2j\", \"2*x+4*x+2+4j\"], evaluate=True) == [u, v]\n\n    #test to check evaluate with no input in case of iterable input with complex numbers\n    u = sympify(\"x+3*x+2j\")\n    v = sympify(\"2*x+4*x+2+4j\")\n\n    assert u.is_Add and set(u.args) == {4*x, 2*I}\n    assert v.is_Add and set(v.args) == {6*x, 2, 4*I}\n    assert sympify([\"x+3*x+2j\", \"2*x+4*x+2+4j\"]) == [u, v]\n\n", "compilation error", "def test_issue_22213():\n    assert sympify('(x + y)**2') == (x + y)**2\n    raises(SympifyError, lambda: sympify('(x+y)**2', locals={'x': 1}))\n", "compilation error", "def test_sympify_issue_19378():\n    assert sympify('a**b**c') == Pow(a, Pow(b, c))\n"], "sample_1067": ["compilation error", "compilation error", "def test_issue_15849():\n    x = Symbol('x')\n    n = Symbol('n', integer=True)\n    a = Wild('a')\n    assert (x**n).match(x**a) == {a: n}\n    assert (x**n).match(x**(a + n)) == {a: 0}\n", "def test_issue_6227():\n    x = Symbol('x')\n    a = Wild('a')\n    f = Function('f')\n    assert (f(x) + 1).match(a + 1) == {a: f(x)}\n", "compilation error"], "sample_31": ["    def test_write_latex_units(self, write, cosmo, tmp_path, format):\n        \"\"\"Test writing with units.\"\"\"\n        fp = tmp_path / \"test_write_latex_units.tex\"\n        write(fp, format=format)\n        tbl = QTable.read(fp)\n        for col in cosmo.__parameters__:\n            if hasattr(cosmo, col) and isinstance(getattr(cosmo, col), Parameter) and getattr(cosmo, col).unit is not None:\n                assert tbl[col].unit == getattr(cosmo, col).unit\n\n", "    def test_latex_units(self, write, tmp_path, format):\n        \"\"\"Test that units are correctly added to the LaTeX table.\"\"\"\n        fp = tmp_path / \"test_latex_units.tex\"\n        write(fp, format=format)\n        tbl = QTable.read(fp)\n        for col in tbl.colnames:\n            if col in self.cosmo.__parameters__:\n                assert tbl[col].unit is not None\n", "    def test_to_latex_units(self, write, tmp_path, format):\n        fp = tmp_path / \"test_to_latex_units.tex\"\n        write(fp, format=format)\n        tbl = QTable.read(fp)\n        for colname in tbl.colnames[2:]:\n            assert tbl[colname].unit is not None\n\n", "    def test_latex_units(self, write, tmp_path, format):\n        fp = tmp_path / \"test_latex_units.tex\"\n        write(fp, format=format)\n        tbl = QTable.read(fp)\n        for col in tbl.colnames[2:]:\n            assert tbl[col].unit is not None\n", "    def test_write_latex_units(self, write, cosmo, tmp_path, format):\n        \"\"\"Test LaTeX output includes units.\"\"\"\n        fp = tmp_path / \"test_write_latex_units.tex\"\n        write(fp, format=format)\n        tbl = QTable.read(fp)\n        for colname in cosmo.__parameters__:\n            if hasattr(cosmo, colname) and isinstance(getattr(cosmo, colname), Parameter):\n                assert tbl[colname].unit.to_string() == getattr(cosmo, colname).unit.to_string()\n"], "sample_666": ["    def test_capture_file_path(testdir, tmpfile):\n        testdir.makepyfile(\n            \"\"\"\n            import os\n            import sys\n\n                with open(tmpfile, 'w') as f:\n                    print('hello', file=f)\n                assert os.path.exists(tmpfile)\n                with open(tmpfile, 'r') as f:\n                    contents = f.read() \n                assert contents == 'hello\\\\n'\n            \"\"\"\n        )\n        result = testdir.runpytest_subprocess(\n            f\"--capture=fd {tmpfile.name}\",\n        )\n        assert result.ret == 0\n        result.stdout.fnmatch_lines(\n            [\n                \"*1 passed*\",\n            ]\n        )\n", "def test_capture_works_with_custom_encoding(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import sys\n            sys.stdout.write(\"hello\\n\".encode(\"latin-1\"))\n        \"\"\"\n    )\n    result = testdir.runpytest_subprocess(\"--capture=sys\")\n    assert result.ret == 0\n    assert result.stdout.str() == \"hello\\n\"\n\n", "def test_capture_with_nonascii_encoding(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import sys\n        from _pytest.capture import capture\n        \n\n            sys.stdout.write('\u4f60\u597d \u4e16\u754c!\\n')\n            out, err = capture.readouterr()\n            assert out == '\u4f60\u597d \u4e16\u754c!\\n'\n    \"\"\"\n    )\n    result = testdir.runpytest(p)\n    assert result.ret == 0\n", "def test_capture_file_mode(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import sys\n            with open(\"testfile.txt\", \"w\") as f:\n                print(\"hello\", file=f)\n            with open(\"testfile.txt\") as f:\n                assert f.read() == \"hello\\\\n\"\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    assert result.ret == 0\n", "def test_capture_with_unicode_newline_and_binary(capsys):\n    print(\"hello\\r\\n\", end=\"\")\n    print(b\"world\\r\\n\", end=\"\")\n    captured = capsys.readouterr()\n    assert captured.out == \"hello\\r\\nworld\\r\\n\"\n"], "sample_673": ["    def test_doctest_report_multi_failures(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n                '''\n                >>> foo()\n                1\n                >>> foo()\n                2\n                >>> foo()\n                3\n                '''\n                return 1\n            \"\"\"\n        )\n        result = self._run_doctest_report(testdir, \"udiff\")\n        result.stdout.fnmatch_lines(\n            [\n                \"    0  1  4\",\n                \"    1  2  4\",\n                \"    2  3  6\",\n            ]\n        )\n", "    def test_doctest_report_none(self, testdir):\n        result = self._run_doctest_report(testdir, \"none\")\n        result.stdout.fnmatch_lines([])\n", "    def test_doctest_report_no_diff(self, testdir):\n        result = self._run_doctest_report(testdir, \"no_diff\")\n        result.stdout.fnmatch_lines(\n            [\n                \"Expected:\",\n                \"       a  b\",\n                \"    0  1  4\",\n                \"    1  2  4\",\n                \"    2  3  6\",\n                \"Got:\",\n                \"       a  b\",\n                \"    0  1  4\",\n                \"    1  2  5\",\n                \"    2  3  6\",\n            ]\n        )\n\n", "def test_doctest_report_plain(self, testdir):\n    result = self._run_doctest_report(testdir, \"plain\")\n    result.stdout.fnmatch_lines(\n        [\n            \"Expected:\",\n            \"0 1 4\",\n            \"1 2 4\",\n            \"2 3 6\",\n            \"Got:\",\n            \"0 1 4\",\n            \"1 2 5\",\n            \"2 3 6\",\n        ]\n    )\n\n", "def test_doctest_report_empty_output(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            '''\n            >>> print()\n            '''\n            pass\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--doctest-modules\", \"--doctest-report\", \"udiff\")\n    result.stdout.no_fnmatch_line(\"*\")\n\n"], "sample_789": ["    def test_zero_estimator_weights():\n        # Test that AdaBoost can handle cases where estimator weights become zero\n        # This can happen due to numerical instability or if the base estimator\n        # always predicts the same class.\n\n        class ZeroWeightEstimator(BaseEstimator):\n                pass\n\n                return np.zeros(X.shape[0])\n\n        boost = AdaBoostClassifier(ZeroWeightEstimator(), n_estimators=3)\n        boost.fit(X, y_class)\n\n        # Check if all estimator weights are zero\n        assert_array_equal(boost.estimator_weights_, np.zeros(3))\n", "    def test_loss_parameter():\n        # Check that different loss functions for AdaBoostRegressor work.\n        rng = np.random.RandomState(0)\n        X = rng.randn(50, 3)\n        y = rng.randn(50)\n\n        for loss in ['linear', 'square', 'exponential']:\n            clf = AdaBoostRegressor(loss=loss, random_state=0)\n            clf.fit(X, y)\n            clf.predict(X)\n", "    def test_warm_start():\n        # Check warm_start functionality\n        X = [[1, -1], [2, -2], [3, -3]]\n        y = [-1, -1, 1]\n        clf = AdaBoostClassifier(n_estimators=10)\n        clf.fit(X, y)\n        score_before = clf.score(X,y)\n        clf.fit(X, y, warm_start=True) #Continues training\n        score_after = clf.score(X,y)\n        assert score_after >= score_before , \"score should not decrease when warm_start is True\" \n        assert len(clf.estimators_) == 20, \"number of estimators should have increased\"\n\n", "    def test_loss_parameter():\n        # Check that AdaBoostRegressor correctly handles the 'linear',\n        # 'square', and 'exponential' loss parameters.\n        for loss in ['linear', 'square', 'exponential']:\n            reg = AdaBoostRegressor(loss=loss, random_state=0)\n            reg.fit(boston.data, boston.target)\n            score = reg.score(boston.data, boston.target)\n            assert score > 0, \"Failed with loss = %s and score = %f\" % (loss, score)\n", "    def test_sample_weight_shape():\n        # check that sample_weights are properly reshaped\n        # when they have shape (n_samples,)\n        rng = np.random.RandomState(0)\n        X = rng.randn(10, 3)\n        y = rng.randint(0, 2, size=10)\n        sample_weight = rng.rand(10)\n        clf = AdaBoostClassifier()\n        clf.fit(X, y, sample_weight=sample_weight)\n"], "sample_314": ["    def test_empty_password(self):\n        user = User.objects.get(username='testclient')\n        data = {'password1': '', 'password2': ''}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['password1'], [Field.default_error_messages['required']])\n        self.assertEqual(form.errors['password2'], [Field.default_error_messages['required']])\n", "    def test_password_validation(self):\n        user = User.objects.get(username='testclient')\n        with override_settings(AUTH_PASSWORD_VALIDATORS=[\n            {'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator', 'OPTIONS': {'min_length': 12}},\n        ]):\n            data = {'password1': 'short', 'password2': 'short'}\n            form = AdminPasswordChangeForm(user, data)\n            self.assertFalse(form.is_valid())\n            self.assertIn('This password is too short. It must contain at least 12 characters.', form.errors['password1'])\n", "    def test_invalid_password(self):\n        user = User.objects.get(username='testclient')\n        data = {'password1': 'test', 'password2': 'test'}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['password1'], [\n            'The password must meet the following requirements: '] ) \n", "    def test_password_validation(self):\n        user = User.objects.get(username='testclient')\n        with override_settings(AUTH_PASSWORD_VALIDATORS=[\n            {'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator'},\n            {'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator', 'OPTIONS': {\n                'min_length': 12,\n            }},\n        ]):\n            data = {'password1': 'testclient', 'password2': 'testclient'}\n            form = AdminPasswordChangeForm(user, data)\n            self.assertFalse(form.is_valid())\n            self.assertEqual(len(form.errors['password1']), 2)\n            self.assertIn('The password is too similar to your username.', form.errors['password1'])\n            self.assertIn('This password is too short. It must contain at least 12 characters.', form.errors['password1'])\n", "    def test_invalid_password_length(self):\n        user = User.objects.get(username='testclient')\n        data = {'password1': '123', 'password2': '123'}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['password1'], [\n            'password field must be at least 8 characters long.'\n        ])\n"], "sample_891": ["compilation error", "    def test_balanced_accuracy_score_multiclass_with_weights(self):\n        \"\"\"Test balanced accuracy score with multiclass data and sample weights.\"\"\"\n        y_true = np.array([0, 1, 2, 0, 1, 2, 0, 1, 2])\n        y_pred = np.array([0, 0, 1, 0, 2, 2, 1, 1, 2])\n        sample_weight = np.array([1, 2, 1, 1, 1, 2, 1, 1, 1])\n        score = balanced_accuracy_score(y_true, y_pred, sample_weight=sample_weight)\n        assert score == pytest.approx(0.3333)\n", "    def test_roc_curve_multiclass_with_labels(global_random_seed):\n        rng = np.random.RandomState(global_random_seed)\n        n_samples = 10\n        y_true = rng.randint(0, 3, size=n_samples)\n        y_score = rng.rand(n_samples, 3)\n        labels = [0, 1, 2]\n        fpr, tpr, thresholds = roc_curve(y_true, y_score, pos_label=labels[2], multi_class='ovr')\n\n        assert fpr.shape[0] == thresholds.shape[0]\n        assert tpr.shape[0] == thresholds.shape[0] \n\n\n", "compilation error", "compilation error"], "sample_485": ["    def test_urlize_trim_url_limit(self):\n        urls = [\n            \"https://www.example.com/very/long/path/with/lots/of/components\",\n            \"http://example.com/longpath?querystring=withlotsofcharacters\",\n        ]\n        trimmed_urls = [\n            \"https://www.example.com/very/long/path/with/lots/of/components\u2026\",\n            \"http://example.com/longpath?querystring=withlotsofcharacters\u2026\",\n        ]\n        for url, trimmed_url in zip(urls, trimmed_urls):\n            with self.subTest(url=url):\n                self.assertEqual(urlize(url, trim_url_limit=50), trimmed_url)\n", "    def test_urlize_trim_url_limit(self):\n        text = \"Visit https://www.example.com/very/long/url/path/for/a/test.html\"\n        expected = (\n            \"Visit <a href='https://www.example.com/very/long/url/path/for/a/test.html'>\"\n            \"https://www.example.com/very/long/url/path/for/a/test.html...</a>\"\n        )\n        self.assertEqual(urlize(text, trim_url_limit=50), expected)\n", "    def test_urlize_trim_url_limit(self):\n        items = (\n            (\n                \"This link is super long https://example.com/this/is/a/very/long/url\"\n                \"/that/needs/to/be/truncated\",\n                \"This link is super long <a href='https://example.com/this/is/a/very/long/url/that/needs/to/be/truncated'>https://example.com/this/is/a/very/long/url\u2026</a>\",\n            ),\n            (\n                lazystr(\"This link is super long https://example.com/this/is/a/very/long/url/that/needs/to/be/truncated\"),\n                \"This link is super long <a href='https://example.com/this/is/a/very/long/url/that/needs/to/be/truncated'>https://example.com/this/is/a/very/long/url\u2026</a>\",\n            ),\n        )\n        for value, output in items:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value, trim_url_limit=30), output)\n\n", "    def test_urlize_trim_url_limit(self):\n        self.assertEqual(\n            urlize(\"This is a long url: https://www.example.com/this-is-a-very-long-url-with-lots-of-words\",\n                   trim_url_limit=20),\n            \"This is a long url: <a href=\\\"https://www.example.com/this-is-a-very-long-url-with-lots-of-words\\\">https://www.example.com/this-is-a-very-lo\u2026</a>\",\n        )\n", "    def test_urlize_trim_url_limit(self):\n        text = \"This is a long URL http://www.example.com/very/long/path/to/a/resource?with=lots&of=query&params=here and some more text.\"\n        self.assertEqual(\n            urlize(text, trim_url_limit=25),\n            \"This is a long URL <a href='http://www.example.com/very/long/path/to/a/resource?with=lots&of=query&params=here'>http://www.example.com/very/long/\u2026</a> and some more text.\",\n        )\n\n"], "sample_418": ["    def test_length_is_warning02(self):\n        msg = (\n            \"The length_is template filter is deprecated in favor of the length \"\n            \"template filter and the == operator within an {% if %} tag.\"\n        )\n        with self.assertRaisesMessage(RemovedInDjango51Warning, msg):\n            self.engine.render_to_string(\"length_is_warning02\", {\"string\": \"good\"})\n\n", "    def test_length_is_none(self):\n        msg = (\n            \"The length_is template filter is deprecated in favor of the length \"\n            \"template filter and the == operator within an {% if %} tag.\"\n        )\n        with self.assertRaisesMessage(RemovedInDjango51Warning, msg):\n            self.engine.render_to_string(\"length_is_none\", {\"my_var\": None})\n\n", "    def test_length_is_warning_no_arg(self):\n        msg = (\n            \"The length_is template filter is deprecated in favor of the length \"\n            \"template filter and the == operator within an {% if %} tag.\"\n        )\n        with self.assertRaisesMessage(RemovedInDjango51Warning, msg):\n            self.engine.render_to_string(\"length_is_warning_no_arg\", {\"string\": \"good\"})\n\n", "    def test_length_is_warning1(self):\n        msg = (\n            \"The length_is template filter is deprecated in favor of the length \"\n            \"template filter and the == operator within an {% if %} tag.\"\n        )\n        with self.assertRaisesMessage(RemovedInDjango51Warning, msg):\n            self.engine.render_to_string(\"length_is_warning1\", {})\n", "    def test_length_is_false(self):\n        output = self.engine.render_to_string(\"length_is_false\")\n        self.assertEqual(output, \"True\")\n"], "sample_1119": ["compilation error", "compilation error", "compilation error", "def test_inverse_matmul():\n    assert (C*D).inverse() == D.I*C.I\n", "compilation error"], "sample_622": ["    def test_decode_cf_with_invalid_dtype(self) -> None:\n        ds = Dataset({\"foo\": (\"x\", [1, 2, 3], {\"dtype\": \"bogus\"})})\n        with pytest.raises(TypeError, match=r\"dtype 'bogus' not understood\"):\n            decode_cf(ds)\n\n", "    def test_decode_cf_with_invalid_dtype(self) -> None:\n        ds = Dataset({\"invalid\": (\"t\", [1, 2, 3], {\"dtype\": \"invalid\"})})\n        with pytest.raises(TypeError, match=\"Invalid dtype 'invalid'\"):\n            decode_cf(ds)\n", "    def test_decode_cf_with_invalid_dtype_conversion():\n        original = Variable(\n            [\"t\"],\n            np.array([1, 2, 3], dtype=np.float32),\n            {\"dtype\": \"int32\"},\n        )\n        with pytest.raises(TypeError, match=\"cannot safely cast\"):\n            conventions.decode_cf_variable(\"t\", original)\n", "def test_decode_cf_datetime64_with_calendar():\n    time_units = \"days since 2000-01-01 00:00:00 UTC\"\n    variable = Variable([\"time\"], pd.date_range(\"2000\", periods=2), {\"units\": time_units, \"calendar\": \"gregorian\"})\n    decoded = conventions.decode_cf_variable(\"time\", variable)\n    assert decoded.encoding == {}\n    assert_identical(decoded, variable)\n", "    def test_decode_cf_with_invalid_calendar(self) -> None:\n        ds = Dataset(\n            {\"time\": (\"time\", [0, 1], {\"units\": \"days since 0001-01-01\", \"calendar\": \"totallybogus\"})}\n        )\n        with pytest.raises(ValueError, match=r\"Invalid calendar 'totallybogus'\"):\n            conventions.decode_cf(ds)\n"], "sample_962": ["def test_mock_isinstance():\n    with mock(['unknown']):\n        import unknown\n        assert isinstance(unknown.secret.Class(), unknown.secret.Class)\n        assert not isinstance(unknown.secret.Class(), object)\n", "def test_mock_mockobject():\n    with mock(['target.module.name']):\n        from target.module.name import MyClass\n        assert isinstance(MyClass(), _MockObject)\n        assert MyClass.__sphinx_mock__\n", "    def test_mock_object():\n        with mock(['tests.test_util_typing']):\n            from tests.test_util_typing import MyClass1\n            mocked_class = MyClass1()\n            assert ismock(mocked_class)\n            assert undecorate(mocked_class) is MyClass1\n            assert restify(mocked_class) == ':py:class:`tests.test_util_typing.MyClass1`'\n            assert stringify(mocked_class) == 'tests.test_util_typing.MyClass1'\n", "def test_stringify_type_hints_recursive_types():\n    from typing import Recursive\n    class Node(Generic[T]):\n        value: T\n        next: Recursive['Node']\n    assert stringify(Node[int]) == \"Node[int]\"\n    #assert stringify(Recursive['Node']) == 'Recursive[Node]'\n", "def test_mock_undecorate():\n    with mock(['tests.test_util_typing']):\n        from tests.test_util_typing import MyClass1, MyClass2, restify\n\n        decorated = _MockObject('decorated', ('tests.test_util_typing', 'MyClass2', 1))\n        assert undecorate(decorated) == 1\n\n        decorated = _MockObject('decorated', ('tests.test_util_typing', 'MyClass1', MyClass2))\n        assert undecorate(decorated) == MyClass2\n"], "sample_409": ["    def test_blocktranslate_with_empty_string(self):\n        t = self.get_template(\n            \"{% load i18n %}{% blocktranslate %} {{ '' }}{% endblocktranslate %}\"\n        )\n        with translation.override(\"de\"):\n            self.assertEqual(t.render(Context({})), \"\")\n", "    def test_blocktranslate_mixed_quotes(self):\n        template = (\n            \"{% load i18n %}\"\n            '{% blocktranslate %}This is a \"quoted\" string and \\'another\\'\"{% endblocktranslate %}'\n        )\n        output = self.engine.render_to_string(template)\n        self.assertEqual(output, 'This is a \"quoted\" string and \\'another\\'')\n", "    def test_blocktranslate_with_empty_context(self):\n        t = self.get_template(\n            \"{% load i18n %}{% blocktranslate context '' %}Empty context{% endblocktranslate %}\"\n        )\n        with translation.override(\"de\"):\n            self.assertEqual(t.render(Context({})), \"Empty context\")\n", "    def test_blocktranslate_with_empty_singular(self):\n        t = self.get_template(\n            \"{% load i18n %}{% blocktranslate %}{% endblocktranslate %}\"\n        )\n        with translation.override(\"de\"):\n            self.assertEqual(t.render(Context({})), \"\")\n", "    def test_blocktranslate_with_empty_singular(self):\n        template = self.get_template(\n            '{% load i18n %}{% blocktranslate %}{% endblocktranslate %}'\n        )\n        with translation.override(\"fr\"):\n            output = template.render(Context())\n        self.assertEqual(output, \"\")\n"], "sample_537": ["    def test_specgram_window_length_error():\n        with pytest.raises(ValueError):\n            mlab.specgram(x=self.y, NFFT=len(self.y)-1, Fs=self.Fs)\n\n", "def test_specgram_empty_input():\n    \"\"\"Test that specgram handles an empty input array gracefully.\"\"\"\n    with pytest.raises(ValueError):\n        mlab.specgram([], NFFT=128, Fs=1)\n", "    def test_csd_onesided_norm():\n        u = np.array([0, 1, 2, 3, 1, 2, 1])\n        v = np.array([1, 2, 3, 1, 2, 3, 1])\n        dt = 1.0\n        Su = np.abs(np.fft.fft(u) * dt)**2 / (dt * u.size)\n        Sv = np.abs(np.fft.fft(v) * dt)**2 / (dt * v.size)\n        Sxy = np.fft.fft(u * np.conj(v)) * dt / (dt * u.size)\n        Pxy, f = mlab.csd(u, v, NFFT=u.size, Fs=1/dt, window=mlab.window_none,\n                    detrend=mlab.detrend_none, noverlap=0, pad_to=None,\n                    scale_by_freq=None,\n                    sides='onesided')\n        Su_1side = np.append([Su[0]], Su[1:4] + Su[4:][::-1])\n        Sv_1side = np.append([Sv[0]], Sv[1:4] + Sv[4:][::-1])\n        Sxy_1side = np.append([Sxy[0]], Sxy[1:4] + Sxy[4:][::-1])\n\n        assert_allclose(Pxy, Sxy_1side, atol=1e-06)\n\n", "    def test_specgram_complex_output():\n        freqs = self.freqs_specgram\n        spec, fsp, t = mlab.specgram(x=self.y,\n                                     NFFT=self.NFFT_specgram,\n                                     Fs=self.Fs,\n                                     noverlap=self.nover_specgram,\n                                     pad_to=self.pad_to_specgram,\n                                     sides=self.sides,\n                                     mode='complex')\n        assert spec.dtype == complex\n\n\n\n", "    def test_specgram_padding_and_noverlap():\n        \"\"\"\n        Test specgram with a small signal and different padding and noverlap\n        values. Make sure the output size is consistent.\n        \"\"\"\n        fs = 1000\n        duration = 1\n        t = np.arange(0, duration, 1/fs)\n        signal = np.sin(2*np.pi*50*t)\n\n        # Test with different padding and noverlap values\n        for pad_to in [None, 256, 512]:\n            for noverlap in [0, 128, 256]:\n                _, _, f = mlab.specgram(signal, Fs=fs, pad_to=pad_to,\n                                       noverlap=noverlap)\n                assert f.size > 0\n\n"], "sample_660": ["    def test_escaped_error_issue4271(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n                raise ValueError('<>&\"')\n        \"\"\"\n        )\n        _, dom = runandparse(testdir)\n        node = dom.find_first_by_tag(\"testcase\")\n        fnode = node.find_first_by_tag(\"failure\")\n        assert \"<>&\\\" \" in fnode.toxml() \n", "    def test_logging_passing_tests_disabled_does_not_log_test_output(testdir):\n        testdir.makeini(\n            \"\"\"\n            [pytest]\n            junit_log_passing_tests=False\n            junit_logging=system-out\n        \"\"\"\n        )\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            import logging\n            import sys\n\n                sys.stdout.write('This is stdout')\n                sys.stderr.write('This is stderr')\n                logging.warning('hello')", "    def test_escaped_skipreason_issue3533_complex(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            @pytest.mark.skip(reason='1 <> 2 \\n and \\\\\"3\" is not 4')\n                pass\n        \"\"\"\n        )\n        _, dom = runandparse(testdir)\n        node = dom.find_first_by_tag(\"testcase\")\n        snode = node.find_first_by_tag(\"skipped\")\n        assert \"1 <> 2 \\n and \\\\\\\"3\\\" is not 4\" in snode.text\n        snode.assert_attr(message=\"1 <> 2 \\n and \\\\\\\"3\\\" is not 4\")\n\n", "    def test_logging_passing_tests_disabled_does_not_log_test_output_and_log_failures(testdir):\n        testdir.makeini(\n            \"\"\"\n            [pytest]\n            junit_log_passing_tests=False\n            junit_logging=system-out\n        \"\"\"\n        )\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            import logging\n            import sys\n\n                sys.stdout.write('This is stdout')\n                sys.stderr.write('This is stderr')\n                logging.warning('hello')\n\n                assert False", "    def test_escaped_skipreason_issue3533(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            @pytest.mark.skip(reason='1 <> 2')\n                pass\n        \"\"\"\n        )\n        _, dom = runandparse(testdir)\n        node = dom.find_first_by_tag(\"testcase\")\n        snode = node.find_first_by_tag(\"skipped\")\n        assert \"1 <> 2\" in snode.text\n        snode.assert_attr(message=\"1 <> 2\")\n    \n"], "sample_1132": ["compilation error", "compilation error", "    def test_cycle_permutation():\n        assert cycle_permutation([1, 2, 3]) == [1, 3, 2]\n        assert cycle_permutation([1, 2, 3], 2) == [2, 1, 3]\n        assert cycle_permutation([1, 2, 3], 3) == [1, 2, 3]\n        assert cycle_permutation([1, 2, 3], -1) == [3, 1, 2]\n        assert cycle_permutation([1, 2, 3], -2) == [2, 3, 1]\n\n", "    def test_multiset_permutations_empty():\n        assert list(multiset_permutations([], 2)) == []\n", "    def test_unflatten_errors():\n        raises(TypeError, lambda: unflatten(1))\n        raises(TypeError, lambda: unflatten([1], 'x'))\n        raises(ValueError, lambda: unflatten([1, 2, 3], -1))\n        raises(ValueError, lambda: unflatten([1, 2, 3], 4))\n\n"], "sample_24": ["    def test_nan_to_num(self):\n        self.check(np.nan_to_num)\n", "    def test_nan_to_num(self):\n        self.check(np.nan_to_num)\n", "def test_where(self):\n        a = np.array([1, 2, 3, 0, 4, 5])\n        mask_a = np.array([False, True, False, True, False, True])\n        ma = Masked(a, mask=mask_a)\n        b = np.array([6, 7, 8, 9, 10, 11])\n        mask_b = np.array([True, False, False, True, True, False])\n        mb = Masked(b, mask=mask_b)\n\n        result = np.where(ma > 2, ma, mb)\n        expected_data = np.where(a > 2, a, b)\n        expected_mask = np.logical_or(mask_a, mask_b)\n        assert isinstance(result, Masked)\n        assert_array_equal(result.unmasked, expected_data)\n        assert_array_equal(result.mask, expected_mask)\n\n", "    def test_polyval(self):\n        self.check(np.polyval, [1, 2, 3], masked_result=False)\n\n", "    def test_array_equal_masked(self):\n        a = np.array([1, 2, 3])\n        mask_a = np.array([False, True, False])\n        ma = Masked(a, mask=mask_a)\n        b = np.array([1, np.nan, 3])\n        mask_b = np.array([False, True, False])\n        mb = Masked(b, mask=mask_b)\n        assert np.array_equal(ma, mb)\n\n\n"], "sample_271": ["    def test_watch_dir_absolute_path(self):\n        self.reloader.watch_dir(str(self.tempdir), '*.py')\n        watched_files = list(self.reloader.watched_files())\n        self.assertIn(self.existing_file, watched_files)\n", "    def test_should_stop_when_run_loop_is_called_twice(self):\n        self.reloader.run_loop()\n        with self.assertRaises(autoreload.ShouldStop):\n            self.reloader.run_loop()\n", "    def test_statReloader_should_stop_when_stop_called(self):\n        ticker = self.reloader.tick()\n        self.reloader.stop()\n        with self.assertRaises(StopIteration):\n            next(ticker)\n\n", "    def test_should_stop_when_exception_occurs(self, mocked_modules):\n        self.reloader.watch_dir(self.tempdir, '*.py')\n        with self.tick_twice():\n            self.reloader.should_stop = True\n            self.increment_mtime(self.existing_file)\n        self.assertEqual(mocked_modules.call_count, 1)\n", "    def test_should_stop_returns_true_when_exception_occurred(self):\n        autoreload._EXCEPTION = Exception()\n        self.assertTrue(self.reloader.should_stop())\n"], "sample_910": ["def test_LogCollector(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    collector = logging.LogCollector()\n    with collector.collect():\n        logger.info('message1')\n        logger.warning('message2')\n\n    assert len(collector.logs) == 2\n    assert collector.logs[0].levelno == logging.INFO\n    assert collector.logs[1].levelno == logging.WARNING\n", "def test_log_collector(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n    collector = logging.LogCollector()\n\n    with collector.collect():\n        logger.info('message1')\n        logger.warning('message2', location=('index', 10))\n\n    assert len(collector.logs) == 2\n    assert collector.logs[0].msg == 'message1'\n    assert collector.logs[1].msg == 'message2'\n    assert collector.logs[1].location == ('index', 10)\n", "    def test_logcollector(app, status, warning):\n\n        logging.setup(app, status, warning)\n        logger = logging.getLogger(__name__)\n\n        collector = logging.LogCollector()\n        with collector.collect():\n            logger.info('message1')\n            logger.warning('message2')\n\n        assert len(collector.logs) == 2\n        assert collector.logs[0].msg == 'message1'\n        assert collector.logs[1].msg == 'message2'\n", "def test_memoryhandler(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    memhandler = logging.MemoryHandler()\n    logger.addHandler(memhandler)\n\n    logger.info('message1')\n    logger.warning('message2')\n\n    assert 'message1' in memhandler.getvalue()\n    assert 'message2' in memhandler.getvalue()\n\n    memhandler.clear()\n\n    assert 'message1' not in memhandler.getvalue()\n    assert 'message2' not in memhandler.getvalue()\n", "def test_memhandler(app, status):\n    logging.setup(app, status, status)\n    logger = logging.getLogger(__name__)\n\n    memhandler = logging.MemoryHandler()\n    logger.addHandler(memhandler)\n\n    logger.info('message1')\n    logger.warning('message2')\n    logger.error('message3')\n\n    memhandler.flushTo(logger)\n\n    assert 'message1' in status.getvalue()\n    assert 'message2' in status.getvalue()\n    assert 'message3' in status.getvalue()\n"], "sample_809": ["    def test_mutual_info_sparse_input():\n        X = csr_matrix([[1, 0, 0],\n                        [0, 0, 1],\n                        [1, 1, 0]])\n        y = np.array([0, 1, 0])\n        mi_dense = mutual_info_classif(X.toarray(), y,\n                                       discrete_features=True)\n        mi_sparse = mutual_info_classif(X, y,\n                                        discrete_features=True)\n        assert_array_equal(mi_dense, mi_sparse)\n", "    def test_mutual_info_classif_sparse():\n        X = csr_matrix([[0, 0, 0],\n                  [1, 1, 0],\n                  [2, 0, 1],\n                  [2, 0, 1],\n                  [2, 0, 1]])\n        y = np.array([0, 1, 2, 2, 1])\n\n        # Here X[:, 0] is the most informative feature, and X[:, 1] is weakly\n        # informative.\n        mi = mutual_info_classif(X, y, discrete_features=True)\n        assert_array_equal(np.argsort(-mi), np.array([0, 2, 1]))\n", "    def test_mutual_info_classif_sparse():\n        # Test mutual_info_classif with a sparse input matrix.\n        rng = check_random_state(0)\n        X = csr_matrix(rng.rand(1000, 3))\n        X[:, 1] += X[:, 0]\n        y = ((0.5 * X[:, 0].A1 + X[:, 2].A1) > 0.5).astype(int)\n\n        mi = mutual_info_classif(X, y, discrete_features=[2], n_neighbors=3,\n                random_state=0)\n        assert_array_equal(np.argsort(-mi), [2, 0, 1])\n", "def test_mutual_info_classif_sparse():\n    # Test mutual_info_classif with a sparse matrix input.\n\n    rng = check_random_state(0)\n    X = rng.rand(1000, 3)\n    X[:, 1] += X[:, 0]\n    y = ((0.5 * X[:, 0] + X[:, 2]) > 0.5).astype(int)\n    X_sparse = csr_matrix(X)\n\n    mi_dense = mutual_info_classif(X, y, discrete_features=[2],\n                                  n_neighbors=3, random_state=0)\n    mi_sparse = mutual_info_classif(X_sparse, y, discrete_features=[2],\n                                   n_neighbors=3, random_state=0)\n\n    assert_array_equal(mi_dense, mi_sparse)\n", "    def test_mutual_info_sparse_data():\n        rng = check_random_state(0)\n        n_samples = 1000\n        n_features = 5\n\n        # Generate sparse data\n        X = csr_matrix(rng.rand(n_samples, n_features) > 0.8)\n        y = rng.rand(n_samples) > 0.5\n\n        mi = mutual_info_classif(X, y, discrete_features=True, random_state=0)\n        assert mi.shape == (n_features,)\n"], "sample_1015": ["compilation error", "compilation error", "def test_ccode_custom_type_aliases():\n    p = C99CodePrinter({\n        'mytype': FloatType('myfloat', float32.nbits, float32.nmant, float32.nexp)\n    })\n    assert p.doprint(x) == 'x'\n    assert p.doprint(x + 1.0) == 'x + 1.0f'\n    assert p.doprint(sin(x)) == 'sinf(x)'\n    assert p.doprint(Type('mytype')(x)) == 'myfloat(x)'\n", "compilation error", "def test_ccode_Matrix():\n\n    A = Matrix([[1, 2], [3, 4]])\n    assert ccode(A) == (\n        \"double A[2][2] = { {1, 2}, {3, 4} };\"\n    )\n\n    B = MatrixSymbol('B', 2, 3)\n    assert ccode(B) == 'double B[2][3];'\n\n"], "sample_175": ["    def test_fast_delete_deferrable_fk(self):\n        u = User.objects.create(\n            avatar=Avatar.objects.create()\n        )\n        with connection.cursor() as cursor:\n            cursor.execute(\"PRAGMA foreign_keys = OFF\")\n        a = Avatar.objects.get(pk=u.avatar_id)\n        # 1 query to fast-delete the user\n        # 1 query to delete the avatar\n        self.assertNumQueries(2, a.delete)\n        self.assertFalse(User.objects.exists())\n        self.assertFalse(Avatar.objects.exists())\n\n", "    def test_fast_delete_m2m_through_proxy(self):\n        t = M2MTo.objects.create()\n        f = M2MFrom.objects.create()\n        f.m2m.add(t)\n        # 1 to delete f, 1 to fast-delete m2m through proxy for f\n        self.assertNumQueries(2, f.delete)\n", "    def test_fast_delete_empty_no_update_can_self_select_with_multiple_referrers(self):\n        \"\"\"\n        Similar to test_fast_delete_empty_no_update_can_self_select, but with\n        multiple referrers.\n        \"\"\"\n        origin = Origin.objects.create()\n        # No Referrer or SecondReferrer objects should match the filter.\n        with self.assertNumQueries(1):\n            self.assertEqual(\n                Referrer.objects.filter(origin__in=[origin], unique_field=42).delete(),\n                (0, {}),\n            )\n", "    def test_fast_delete_inherited_through_proxy(self):\n        child = RChild.objects.create()\n        parent = child.r_ptr\n        self.assertNumQueries(2, parent.delete)\n        self.assertFalse(RChild.objects.exists())\n        self.assertFalse(R.objects.exists())\n\n", "    def test_fast_delete_inherited_m2m(self):\n        p = Parent.objects.create()\n        c = Child.objects.create(parent=p)\n        t = Tag.objects.create()\n        c.tags.add(t)\n        # 1 query to delete the child, 1 to delete the parent.\n        self.assertNumQueries(2, c.delete)\n        self.assertFalse(Child.objects.exists())\n        self.assertFalse(Parent.objects.exists())\n"], "sample_402": ["    def test_no_compress_no_accept_encoding(self):\n        \"\"\"\n        Compression isn't performed on responses if the client doesn't accept\n        gzip encoding.\n        \"\"\"\n        self.req.META[\"HTTP_ACCEPT_ENCODING\"] = \"\"\n        r = GZipMiddleware(self.get_response)(self.req)\n        self.assertEqual(r.content, self.compressible_string)\n        self.assertIsNone(r.get(\"Content-Encoding\"))\n", "    def test_gzip_no_content_length(self):\n        \"\"\"\n        GZipMiddleware sets Content-Length header if the original response\n        doesn't have one.\n        \"\"\"\n\n            response = HttpResponse(self.compressible_string)\n            response.headers[\"ETag\"] = '\"eggs\"'\n            del response['Content-Length']\n            return response\n\n        request = self.rf.get(\"/\", HTTP_ACCEPT_ENCODING=\"gzip, deflate\")\n        gzip_response = GZipMiddleware(get_response)(request)\n        self.assertIn(\"Content-Length\", gzip_response.headers)\n\n", "    def test_no_compress_if_user_agent_doesnt_support_gzip(self):\n        \"\"\"\n        Compression isn't performed if the browser doesn't support gzip.\n        \"\"\"\n        self.req.META[\"HTTP_ACCEPT_ENCODING\"] = \"deflate\"  # Only deflate\n        r = GZipMiddleware(self.get_response)(self.req)\n        self.assertEqual(r.content, self.compressible_string)\n        self.assertIsNone(r.get(\"Content-Encoding\"))\n", "    def test_no_compress_empty_response(self):\n        \"\"\"\n        Compression isn't performed on empty responses.\n        \"\"\"\n        self.resp.content = b\"\"\n        r = GZipMiddleware(self.get_response)(self.req)\n        self.assertEqual(r.content, b\"\")\n        self.assertIsNone(r.get(\"Content-Encoding\"))\n\n", "    def test_no_compress_content_length_zero(self):\n        \"\"\"\n        Compression isn't performed on responses with Content-Length: 0.\n        \"\"\"\n        self.resp.content = b''\n        self.resp[\"Content-Length\"] = \"0\"\n        r = GZipMiddleware(self.get_response)(self.req)\n        self.assertEqual(r.content, b'')\n        self.assertIsNone(r.get(\"Content-Encoding\"))\n"], "sample_108": ["    def test_reverse_type_error_propagates(self):\n        @DynamicConverter.register_to_url\n            raise TypeError('This type error propagates.')\n        with self.assertRaisesMessage(TypeError, 'This type error propagates.'):\n            reverse('dynamic', kwargs={'value': object()})\n", "    def test_reverse_type_error_means_no_match(self):\n        @DynamicConverter.register_to_url\n            raise TypeError('This type error propagates.')\n        with self.assertRaisesMessage(TypeError, 'This type error propagates.'):\n            reverse('dynamic', kwargs={'value': object()})\n", "    def test_reverse_type_error_means_no_match(self):\n        @DynamicConverter.register_to_url\n            raise TypeError('This type error propagates.')\n        with self.assertRaisesMessage(TypeError, 'This type error propagates.'):\n            reverse('dynamic', kwargs={'value': object()})\n", "    def test_reverse_type_error_propagates(self):\n        @DynamicConverter.register_to_url\n            raise TypeError('This type error propagates.')\n        with self.assertRaisesMessage(TypeError, 'This type error propagates.'):\n            reverse('dynamic', kwargs={'value': object()})\n", "    def test_reverse_type_error_propagates(self):\n        @DynamicConverter.register_to_url\n            raise TypeError('This type error propagates.')\n        with self.assertRaisesMessage(TypeError, 'This type error propagates.'):\n            reverse('dynamic', kwargs={'value': object()})\n"], "sample_1135": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1186": ["compilation error", "compilation error", "    def test_array_shape():\n        for array_type in array_types:\n            A = array_type([1, 2, 3, 4, 5, 6], (2, 3))\n            assert A.shape == (2, 3)\n            A = array_type([[1, 2], [3, 4]])\n            assert A.shape == (2, 2)\n            A = array_type([1, 2, 3])\n            assert A.shape == (3,)\n            A = array_type([])\n            assert A.shape == (0,)\n", "def test_array_eq():\n    A = ImmutableDenseNDimArray([1, 2, 3], shape=(3,))\n    B = ImmutableDenseNDimArray([1, 2, 3], shape=(3,))\n    C = ImmutableDenseNDimArray([1, 2, 4], shape=(3,))\n    D = ImmutableDenseNDimArray([1, 2, 3], shape=(1,3))\n\n    assert A == B\n    assert A != C\n    assert A != D\n", "    def test_array_add_scalar():\n        for ArrayType in array_types:\n            a = ArrayType([[1, 2], [3, 4]])\n            assert (a + 5) == ArrayType([[6, 7], [8, 9]])\n            assert (5 + a) == ArrayType([[6, 7], [8, 9]])\n\n"], "sample_1189": ["compilation error", "    def test_issue_10132():\n        if not numpy:\n            skip(\"numpy not installed\")\n\n        x = symbols('x')\n        f = lambdify(x, floor(x), 'numpy')\n        assert f(2.5) == 2\n        assert f(-2.5) == -3\n", "compilation error", "    def test_issue_16135():\n        if not scipy:\n            skip(\"scipy not installed\")\n\n        x, y, z = symbols('x y z')\n\n        f = lambdify((x, y), Piecewise((1, x > 0), (0, True)), 'scipy')\n        assert f(1, 2) == 1\n        assert f(-1, 2) == 0\n\n        g = lambdify((x, y, z), Piecewise((x + y*z, x > 0), (0, True)), 'scipy')\n        assert g(1, 2, 3) == 7\n        assert g(-1, 2, 3) == 0\n\n", "def test_lambdify_with_constants():\n    f = lambdify(x, 2*x + sympy.pi)\n    assert f(2) == 4 + sympy.pi\n"], "sample_309": ["    def test_parse_http_date_invalid(self):\n        with self.assertRaises(ValueError):\n            parse_http_date('Invalid date string')\n", "    def test_parse_http_date_invalid(self):\n        with self.assertRaises(ValueError):\n            parse_http_date('bogus date')\n", "    def test_parse_http_date_invalid(self):\n        with self.assertRaises(ValueError):\n            parse_http_date('Invalid Date')\n", "    def test_parse_http_date_with_invalid_date(self):\n        with self.assertRaises(ValueError):\n            parse_http_date('Invalid Date') \n", "    def test_parse_http_date_invalid(self):\n        with self.assertRaises(ValueError):\n            parse_http_date('Invalid date string')\n\n"], "sample_1045": ["compilation error", "compilation error", "compilation error", "def test_issue_10020_2():\n    assert (-oo)**(-1 + I) == S.Zero\n\n", "compilation error"], "sample_629": ["def test_expand_modules_package_without_init():\n    ignore_list, ignore_list_re, ignore_list_paths_re = [], [], []\n    modules, errors = expand_modules(\n        [str(TEST_DIRECTORY / \"lint\")], ignore_list, ignore_list_re, ignore_list_paths_re\n    )\n    modules.sort(key=lambda d: d[\"name\"])\n    assert modules == [init_of_package, this_file_from_init, unittest_lint]\n    assert not errors\n", "    def test_expand_modules_ignore_list():\n        ignore_list = [\"unittest_lint.py\", \"unittest_expand_modules.py\"]\n        ignore_list_re, ignore_list_paths_re = [], []\n        modules, errors = expand_modules(\n            [Path(__file__).parent], ignore_list, ignore_list_re, ignore_list_paths_re\n        )\n        assert modules == []\n        assert not errors\n", "def test_expand_modules_ignore_pattern():\n    ignore_list, ignore_list_re, ignore_list_paths_re = [], [re.compile(\"unittest_.*\")], []\n    modules, errors = expand_modules(\n        [TEST_DIRECTORY / \"lint\"], ignore_list, ignore_list_re, ignore_list_paths_re\n    )\n    assert modules == [init_of_package]\n    assert not errors\n\n\n\n", "def test_expand_modules_ignore(\n    files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re, expected", "def test_expand_modules_with_ignore_regex(tmp_path):\n    ignore_list_re = [re.compile(\"unittest.*\")]\n    file1 = tmp_path / \"foo.py\"\n    file2 = tmp_path / \"unittest_foo.py\"\n    file1.write_text(\"pass\")\n    file2.write_text(\"pass\")\n    modules, errors = expand_modules(\n        [str(file1), str(file2)], [], ignore_list_re, []\n    )\n\n    assert len(modules) == 1\n    assert modules[0][\"path\"] == str(file1)\n\n\n"], "sample_677": ["def test_ident(expr: str, matcher: dict[str, bool], expected: bool) -> None:\n        assert evaluate(expr, matcher.__getitem__) is expected\n", "def test_python_keywords(expr: str) -> None:\n    matcher = {\"True\": True, \"False\": False, \"None\": None}.__getitem__\n    assert evaluate(expr, matcher) == eval(expr)\n", "def test_numeric_identifer_errors(expr: str, message: str) -> None:\n    with pytest.raises(ParseError) as excinfo:\n        evaluate(expr, lambda ident: True)\n    assert excinfo.value.message == message\n", "def test_complex_ident_matching(expr: str, expected: bool) -> None:\n    matcher = {\"ident1\": True, \"ident2\": False, \"ident3\": True}.__getitem__\n    assert evaluate(expr, matcher) is expected\n\n", "def test_parentheses_ident(expr: str, expected: bool) -> None:\n    assert evaluate(expr, {\"ident\": True}.__getitem__)\n\n\n\n"], "sample_323": ["    def test_backwards_circular_dependencies(self):\n        r\"\"\"\n        Handle circular dependencies when rolling back.\n        \n        a: 1 <---- 2\n        b:    \\- 1\n        c:     \\- 1\n\n        If a1 and a2 are applied and we're asked to migrate to 'a1', we should\n        unapply a2, b1 and c1 even though they have circular dependencies.\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        c1_impl = FakeMigration('c1')\n        c1 = ('c', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(c1, c1_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, c1, a1)\n        graph.add_dependency(None, b1, c1)\n        graph.add_dependency(None, c1, b1)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            a2: a2_impl,\n            b1: b1_impl,\n            c1: c1_impl,\n        })\n\n        plan = executor.migration_plan({a1})\n\n        should_be_rolled_back = [a2_impl, b1_impl, c1_impl]\n        exp = [(m, True) for m in should_be_rolled_back]\n        self.assertEqual(plan, exp)\n\n", "    def test_backwards_dependent_on_unapplied(self):\n        \"\"\"\n        Don't unapply migrations on which others depend, even if they are\n        \"backwards\" of the target.\n\n        a: 1 <--- 2\n        b:    \\- 1\n\n        If a1 is applied, and b1 also, but a2 is not, and we're asked to migrate\n        to a1, don't unapply b1 even though it's \"backwards\" from a1.\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, b1, a1)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n        })\n\n        plan = executor.migration_plan({a1})\n\n        self.assertEqual(plan, [])\n\n", "    def test_migrate_dependency_on_fake_initial(self):\n        \"\"\"\n        Test migrating when dependencies exist on fake-initial migrations.\n\n        This case covers when a new app introduces migrations that depend on a\n        fake initial migration from an existing app.\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        b2_impl = FakeMigration('b2')\n        b2 = ('b', '2')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(b2, b2_impl)\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, b2, b1)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n        })\n\n        plan = executor.migration_plan({\n            'b',\n        }, fake_initial=True\n        )\n\n        self.assertEqual(plan, [(b1_impl, False), (b2_impl, False)])\n", "    def test_migrate_empty_list(self):\n        \"\"\"\n        Migrating an empty list of apps should have no effect.\n\n        This test checks if migrate doesn't raise an error when given an\n        empty list of migrations.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        executor.migrate([])\n", "    def test_migrate_empty_plan(self):\n        \"\"\"\n        If the migration plan is empty, do nothing.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        executor.migrate([])\n        self.assertEqual(executor.recorder.applied_migrations(), [])\n"], "sample_597": ["    def test_merge_dataset_overwrite(self, overwrite_vars, expected_vars):\n        data = create_test_data()\n        ds1 = data[expected_vars]\n        ds2 = data[[\"var1\"]].copy(deep=True)\n        ds2[\"var1\"][:, :] = 10  # Change values to ensure they are overwritten\n        actual = ds1.merge(ds2, overwrite_vars=overwrite_vars)\n        expected = data[expected_vars].copy(deep=True)\n        if \"var1\" in overwrite_vars:\n            expected[\"var1\"][:, :] = 10\n        assert_identical(actual, expected)\n\n", "    def test_merge_override_attrs(self):\n        ds1 = xr.Dataset({\"x\": (\"y\", [0], {\"foo\": \"bar\"})})\n        ds2 = xr.Dataset({\"x\": (\"y\", [1])})\n\n        actual = ds1.merge(ds2, compat=\"override\")\n        expected = ds2.copy()\n        assert_identical(actual, expected)\n", "    def test_merge_overwrite_vars(self):\n        ds1 = xr.Dataset({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\n        ds2 = xr.Dataset({\"a\": [7, 8, 9], \"c\": [10, 11, 12]})\n        expected = xr.Dataset({\"a\": [7, 8, 9], \"b\": [4, 5, 6], \"c\": [10, 11, 12]})\n        actual = ds1.merge(ds2, overwrite_vars=\"a\")\n        assert expected.identical(actual)\n        actual = ds1.merge(ds2, overwrite_vars=[\"a\"])\n        assert expected.identical(actual)\n", "    def test_merge_DataArray_with_Dataset(self):\n\n        ds = xr.Dataset({\"a\": ((\"x\", \"y\"), np.arange(6).reshape(2, 3))})\n        da = xr.DataArray(np.arange(2), dims=\"x\", name=\"b\")\n\n        expected = xr.Dataset(\n            {\"a\": ((\"x\", \"y\"), np.arange(6).reshape(2, 3)), \"b\": ((\"x\"), np.arange(2))}\n        )\n        actual = ds.merge(da)\n        assert_identical(actual, expected)\n", "    def test_merge_dataarray_to_dataset(self):\n        data = create_test_data()\n        actual = data.var1.merge(data)\n        expected = xr.merge([data.var1, data])\n        assert actual.identical(expected)\n"], "sample_553": ["def test_animation_repeat_delay(anim):\n    fig, ax = plt.subplots()\n    line, = ax.plot([], [])\n\n        line.set_data([], [])\n        return line,\n\n        line.set_data(i, i)\n        return line,\n\n    anim = animation.FuncAnimation(fig, animate, init_func=init,\n                                   frames=5, repeat=True,\n                                   repeat_delay=1000)\n    assert anim._repeat_delay == 1000\n", "def test_blit_with_artist_removal(anim):\n    fig, ax = plt.subplots()\n    line1, = ax.plot([], [])\n    line2, = ax.plot([], [])\n\n    drawn_artists = []\n\n        line1.set_data([], [])\n        line2.set_data([], [])\n        return line1, line2\n\n        line1.set_data([i], [i])\n        if i > 2:\n            line2.set_visible(False)\n\n        return line1, line2\n\n    anim = animation.FuncAnimation(\n        fig, animate, init_func=init, frames=5, blit=True\n    )\n    assert anim._drawn_artists == (line1, line2)\n", "def test_save_count_is_respected(tmpdir, save_count, anim):\n    frames = list(range(5))\n    anim = animation.FuncAnimation(\n        **{**anim, 'frames': frames, 'save_count': save_count}\n    )\n\n    with tmpdir.as_cwd():\n        anim.save(\"test.gif\", writer='pillow')\n\n    if save_count is None:\n        assert len(anim._save_seq) == len(frames)\n    else:\n        assert len(anim._save_seq) == min(save_count, len(frames))\n\n", "def test_animation_blit_reuse(anim):\n    fig, ax = plt.subplots()\n    line, = ax.plot([], [])\n\n    artist_id = weakref.ref(line)\n\n        line.set_data([], [])\n        return line,\n\n        line.set_data(np.arange(i), np.arange(i))\n        return line,\n\n    anim = animation.FuncAnimation(\n        fig, animate, init_func=init, blit=True, frames=5, save_count=5\n    )\n    anim._init_draw()\n    assert artist_id() is anim._drawn_artists[0]\n\n    for i in range(5):\n        anim._step(i)\n        assert artist_id() is anim._drawn_artists[0]\n", "def test_animation_blit_false(anim):\n    anim = animation.FuncAnimation(**anim, blit=False)\n    assert anim._blit is False\n\n"], "sample_415": ["    def test_check_constraint_with_condition(self):\n        constraint = models.CheckConstraint(\n            check=models.Q(price__gt=models.F(\"discounted_price\")),\n            name=\"price_gt_discounted_price\",\n            condition=models.Q(color=\"red\"),\n        )\n        table_name = Product._meta.db_table\n        with self.assertRaises(IntegrityError):\n            Product.objects.create(price=10, discounted_price=20, color=\"red\")\n        Product.objects.create(\n            price=10, discounted_price=5, color=\"red\"\n        )\n        with self.assertRaises(IntegrityError):\n            Product.objects.create(price=10, discounted_price=20, color=\"blue\")\n", "    def test_invalid_include_type(self):\n        msg = \"Elements in UniqueConstraint.include must be strings.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                name=\"uniq_include_invalid\",\n                fields=[\"field\"],\n                include=[1],\n            )\n", "    def test_include_creates_covering_index(self):\n        with self.assertNumQueries(1):\n            UniqueConstraintInclude.objects.create(name=\"p1\", color=\"red\")\n        # Check if covering index was created\n        index_names = [\n            index.name for index in connection.introspection.get_indexes(UniqueConstraintInclude._meta.db_table)\n        ]\n        self.assertIn(\n            f\"{UniqueConstraintInclude._meta.db_table}_name_color_covering\",\n            index_names,\n        )\n\n", "    def test_deferrable_with_include_condition(self):\n        message = (\n            \"UniqueConstraint with include fields and conditions cannot be deferred.\"\n        )\n        with self.assertRaisesMessage(ValueError, message):\n            models.UniqueConstraint(\n                fields=[\"name\"],\n                name=\"name_inc_condition_unique\",\n                include=[\"color\"],\n                condition=models.Q(color__isnull=True),\n                deferrable=models.Deferrable.DEFERRED,\n            )\n", "    def test_invalid_opclass(self):\n        msg = \"Invalid opclass name: invalid_opclass_name\"\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                fields=[\"field\"],\n                name=\"invalid_opclass\",\n                opclasses=[\"invalid_opclass_name\"],\n            )\n\n"], "sample_160": ["    def test_format_number_with_non_uniform_grouping(self):\n        self.assertEqual(nformat(1234567890, '.', grouping=(3, 2, 0)), '1,234,567')\n", "    def test_non_uniform_grouping(self):\n        self.assertEqual(nformat(123456789, '.', grouping=(3, 2, 0), thousand_sep=' '), '123 45 6789')\n        self.assertEqual(nformat(123456789, '.', grouping=(2, 2, 3), thousand_sep=' '), '12 34 56 789')\n        self.assertEqual(nformat(-123456789, '.', grouping=(3, 2, 0), thousand_sep=' '), '-123 45 6789')\n        self.assertEqual(nformat(123456789.123, '.', grouping=(3, 2, 0), thousand_sep=' ', decimal_pos=2), '123 45 6789.12')\n", "    def test_grouping_intervals(self):\n        self.assertEqual(nformat(1234567890, '.', grouping=[3, 2, 0]), '1,234,567,890')\n        self.assertEqual(nformat(123456789012, '.', grouping=[3, 2, 1, 0]), '123,456,789,012')\n", "    def test_scientific_notation(self):\n        self.assertEqual(nformat(Decimal('1e200'), '.', decimal_pos=2), '1.00e+200')\n        self.assertEqual(nformat(Decimal('1e-200'), '.', decimal_pos=8), '1.00000000e-200')\n        self.assertEqual(nformat(Decimal('1e+200'), '.', decimal_pos=2), '1.00e+200')\n        self.assertEqual(nformat(Decimal('1e-200'), '.', decimal_pos=0), '0')\n", "    def test_non_uniform_grouping(self):\n        self.assertEqual(nformat(1234567, '.', grouping=[3, 2, 0]), '1,234,567')\n        self.assertEqual(nformat(123456789, '.', grouping=[3, 2, 0]), '12,34,56789')\n        self.assertEqual(nformat(1234567890123, '.', grouping=[3, 2, 0]), '1,234,567,890,123')\n        self.assertEqual(nformat(123456789012345, '.', grouping=[3, 2]), '12,345,678,901,234.5')\n"], "sample_55": ["    def test_no_pk(self):\n        post_data = {'name': '1'}\n        response = self.client.post(reverse('admin:admin_views_nopk_add'), post_data)\n        self.assertEqual(response.status_code, 302)\n", "    def test_add_view_form_and_formsets_run_validation_empty_formset(self):\n        \"\"\"\n        Issue #20522\n        Verifying that if the parent form fails validation, the inlines also\n        run validation even if validation is contingent on parent form data\n        and the inline formset is empty\n        \"\"\"\n        post_data = {\n            'family_name': 'Test1',\n            'dependentchild_set-TOTAL_FORMS': '0',\n            'dependentchild_set-INITIAL_FORMS': '0',\n            'dependentchild_set-MAX_NUM_FORMS': '0',\n        }\n        response = self.client.post(reverse('admin:admin_views_parentwithdependentchildren_add'), post_data)\n        self.assertFormError(response, 'adminform', 'some_required_info', ['This field is required.'])\n", "    def test_change_view_with_nonexistent_pk(self):\n        response = self.client.get(reverse('admin:admin_views_implicitlygeneratedpk_change', args=(999,)))\n        self.assertEqual(response.status_code, 404)\n", "    def test_add_view_form_and_formsets_errors_are_clearly_displayed(self):\n        \"\"\"\n        Issue #20522\n        When validation fails on the parent form or on an inline formset, make\n        sure errors are clearly displayed to the user. This ensures a good user\n        experience.\n        \"\"\"\n        post_data = {\n            'family_name': 'Test1',\n            'dependentchild_set-TOTAL_FORMS': '1',\n            'dependentchild_set-INITIAL_FORMS': '0',\n            'dependentchild_set-MAX_NUM_FORMS': '1',\n            'dependentchild_set-0-id': '',\n            'dependentchild_set-0-parent': '',\n            'dependentchild_set-0-family_name': 'Test2',\n            'some_required_info': '', # This field is required on ParentWithDependentChildren\n        }\n        response = self.client.post(reverse('admin:admin_views_parentwithdependentchildren_add'), post_data)\n        self.assertContains(response, 'This field is required.')\n        self.assertContains(response, 'Children must share a family name with their parents in this contrived test case')\n\n\n\n", "    def test_add_view_form_and_formsets_run_validation_required_on_inline(self):\n        \"\"\"\n        The inline formset validation should fail if a required field\n        in the child model is missing.\n        \"\"\"\n        post_data = {\n            'name': 'Test Parent',\n            'dependentchild_set-TOTAL_FORMS': '1',\n            'dependentchild_set-INITIAL_FORMS': '0',\n            'dependentchild_set-MAX_NUM_FORMS': '1',\n            'dependentchild_set-0-id': '',\n            'dependentchild_set-0-parent': '',\n            'dependentchild_set-0-family_name': '',\n        }\n        response = self.client.post(reverse('admin:admin_views_parentwithdependentchildren_add'), post_data)\n        self.assertFormError(response, 'inline_admin_formset', 0, None, ['This field is required.'])\n\n"], "sample_230": ["    def test_empty_json_string(self):\n        field = JSONField(required=False)\n        self.assertIsNone(field.clean('\"\"'))\n        self.assertEqual(field.clean('\"\"'), '')\n", "    def test_empty_string(self):\n        field = JSONField(required=False)\n        self.assertEqual(field.clean(''), {})\n", "    def test_empty_values(self):\n        field = JSONField(required=False, empty_values=['', None])\n        self.assertIsNone(field.clean(''))\n        self.assertIsNone(field.clean(None))\n", "    def test_bound_data_invalid_json(self):\n        field = JSONField()\n        data = '{some badly formed: json}'\n        bound_data = field.bound_data(data, None)\n        self.assertIsInstance(bound_data, InvalidJSONInput)\n        self.assertEqual(bound_data, data)\n", "    def test_empty_string_as_input(self):\n        field = JSONField(required=False)\n        self.assertIsNone(field.clean(''))\n\n"], "sample_539": ["compilation error", "def test_polygon_selector_clear(draw_bounding_box):\n    verts = [(50, 50), (150, 50), (50, 150)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n    ax = get_ax()\n\n    tool = widgets.PolygonSelector(ax, onselect=noop,\n                                   draw_bounding_box=draw_bounding_box)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n    assert len(ax.patches) == 1  # the polygon patch\n\n    tool.clear()\n    assert len(ax.patches) == 0\n", "def test_polygon_selector_empty_on_draw(draw_bounding_box):\n    ax = get_ax()\n    onselect = mock.Mock(spec=noop, return_value=None)\n    tool = widgets.PolygonSelector(\n        ax, onselect, draw_bounding_box=draw_bounding_box\n    )\n\n    # Simulate a drawing event without any vertices\n    do_event(tool, 'onmove', xdata=50, ydata=50)\n    do_event(tool, 'release', xdata=50, ydata=50)\n    assert onselect.call_count == 0\n    assert len(tool.verts) == 0\n", "def test_MultiCursor_remove_axes():\n    fig, axs = plt.subplots(2, 2)\n    multi = widgets.MultiCursor(fig.canvas, axs.flatten(), useblit=False)\n\n    multi.remove_axes(axs[1, 0])\n    assert len(multi.vlines) == 3\n    assert len(multi.hlines) == 3\n\n\n\n", "def test_MultiCursor_visibility_on_axes_change(\n    horizOn,\n    vertOn,"], "sample_394": ["    def test_single_model_append_slash(self):\n        superuser = User.objects.create_user(\n            username=\"staff\",\n            password=\"secret\",\n            email=\"staff@example.com\",\n            is_staff=True,\n        )\n        self.client.force_login(superuser)\n        known_url = reverse(\"admin9:admin_views_actor_changelist\")\n        response = self.client.get(known_url)\n        self.assertEqual(response.status_code, 200)\n", "    def test_non_admin_url_404_if_authenticated(self):\n        superuser = User.objects.create_superuser(\n            username=\"super\",\n            password=\"secret\",\n            email=\"super@example.com\",\n        )\n        self.client.force_login(superuser)\n        unknown_url = \"/unknown/\"\n        response = self.client.get(unknown_url)\n        self.assertEqual(response.status_code, 404)\n", "    def test_final_catch_all_view_disabled_for_model_admin_site(self):\n        superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n        self.client.force_login(superuser)\n\n        url = reverse(\"admin11:admin_views_article_changelist\")\n        response = self.client.get(url[:-1])\n        self.assertEqual(response.status_code, 404)\n", "    def test_non_admin_url_404_if_authenticated(self):\n        superuser = User.objects.create_superuser(\n            username=\"super\",\n            password=\"secret\",\n            email=\"super@example.com\",\n        )\n        self.client.force_login(superuser)\n        unknown_url = \"/unknown/\"\n        response = self.client.get(unknown_url)\n        # Does not redirect to the admin login.\n        self.assertEqual(response.status_code, 404)\n\n", "    def test_known_url_missing_slash_404_if_not_authenticated_without_final_catch_all_view(\n        self,"], "sample_1094": ["def test_count_ops():\n    from sympy.core.basic import count_ops\n    assert count_ops(x + y) == 2\n    assert count_ops(x * y) == 1\n    assert count_ops(x**y) == 1\n    assert count_ops(sin(x)) == 1\n    assert count_ops(sin(x) + cos(x)) == 2\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_287": ["    def test_readonly_fields_from_inherited_model(self):\n        class BaseAdmin(admin.ModelAdmin):\n            readonly_fields = ('title', )\n\n        class SongAdmin(BaseAdmin):\n            pass\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        self.assertEqual(errors, [])\n", "    def test_duplicate_fieldsets_names(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fieldsets = [\n                ('Header 1', {'fields': ['title']}),\n                ('Header 1', {'fields': ['album']}),\n            ]\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"There are duplicate fieldset names: 'Header 1'.\",\n                obj=MyModelAdmin,\n                id='admin.E010'\n            )\n        ]\n        self.assertEqual(errors, expected)\n\n", "compilation error", "    def test_fieldsets_duplicate_keys(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fieldsets = [\n                (None, {\n                    'fields': ['title', 'album']\n                }),\n                (None, {\n                    'fields': ['title', 'album']\n                }),\n            ]\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"Duplicate fieldset found.\",\n                obj=MyModelAdmin,\n                id='admin.E011'\n            )\n        ]\n        self.assertEqual(errors, expected)\n\n", "    def test_prepopulated_fields_is_a_dict(self):\n        class PrepopulatedFieldsAdmin(admin.ModelAdmin):\n            prepopulated_fields = {'slug': ('title',)}\n\n        errors = PrepopulatedFieldsAdmin(Song, AdminSite()).check()\n        self.assertEqual(errors, [])\n"], "sample_970": ["        def my_method(self):\n            pass\n", "    def test_getsource_with_cache():\n            pass\n\n        source = inspect.getsource(func)\n        cached_source = inspect.getsource(func)\n        assert source == cached_source\n        assert inspect.getsource_with_cache(func) == source\n", "    def test_getdoc_property():\n        class Foo:\n            @property\n                \"\"\"\n                docstring\n                    indented text\n                \"\"\"\n                return 1\n\n        assert inspect.getdoc(Foo.my_prop, getattr, False, Foo, \"my_prop\") is None\n        assert inspect.getdoc(Foo.my_prop, getattr, True, Foo, \"my_prop\") == Foo.my_prop.__doc__\n", "def test_is_builtin_class_method_with_slots():\n    class MyInt(int):\n        __slots__ = []\n            pass\n\n    assert inspect.is_builtin_class_method(MyInt, 'to_bytes')\n    assert inspect.is_builtin_class_method(MyInt, '__init__')\n    assert not inspect.is_builtin_class_method(MyInt, 'my_method')\n    assert not inspect.is_builtin_class_method(MyInt, 'does_not_exist')\n    assert not inspect.is_builtin_class_method(4, 'still does not crash')\n", "def test_getdoc_inheritance_with_docstring_inheritance():\n    class Foo:\n            \"\"\"\n            docstring\n                indented text\n            \"\"\"\n\n    class Bar(Foo):\n        pass\n\n    assert inspect.getdoc(Bar.meth, getattr, False, Bar, \"meth\") == Foo.meth.__doc__\n    assert inspect.getdoc(Bar.meth, getattr, True, Bar, \"meth\") == Foo.meth.__doc__\n"], "sample_543": ["def test_polygon_selector_remove_vertex_after_completion(draw_bounding_box):\n    verts = [(50, 50), (150, 50), (50, 150)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n        *polygon_remove_vertex(150, 50),\n    ]\n    check_polygon_selector(event_sequence, verts[::2], 2,\n                           draw_bounding_box=draw_bounding_box)\n", "def test_MultiCursor_remove_axes():\n    fig, (ax1, ax2) = plt.subplots(2, sharex=True)\n\n    multi = widgets.MultiCursor((ax1, ax2), useblit=False)\n\n    multi.remove_axes(ax1)\n    assert len(multi.vlines) == 1\n    assert len(multi.hlines) == 1\n", "def test_polygon_selector_set_verts(draw_bounding_box):\n    verts = [(50, 50), (150, 50), (50, 150)]\n    ax = get_ax()\n\n    tool = widgets.PolygonSelector(ax, onselect=noop,\n                                   draw_bounding_box=draw_bounding_box)\n    tool.verts = verts\n\n    assert tool.verts == verts\n\n", "def test_polygon_selector_clear(ax, draw_bounding_box):\n    verts = [(50, 50), (150, 50), (50, 150)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    tool = widgets.PolygonSelector(ax, onselect=noop,\n                                   draw_bounding_box=draw_bounding_box)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n    assert tool.verts == verts\n    assert len(ax.patches) == 1\n    tool.clear()\n    assert tool.verts == []\n    assert len(ax.patches) == 0\n", "def test_polygon_selector_release_outside_axes(draw_bounding_box):\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 100)\n\n    onselect = mock.Mock(spec=noop, return_value=None)\n    tool = widgets.PolygonSelector(ax, onselect,\n                                   draw_bounding_box=draw_bounding_box)\n\n    # Start drawing a polygon\n    do_event(tool, 'onmove', xdata=50, ydata=50)\n    do_event(tool, 'press', xdata=50, ydata=50)\n    \n    # Move the mouse outside the axes bounds on release\n    do_event(tool, 'onmove', xdata=120, ydata=50)\n    do_event(tool, 'release', xdata=120, ydata=50)\n\n    assert onselect.call_count == 0\n    assert len(tool.verts) == 0\n\n"], "sample_161": ["    def test_duplicate_through_fields(self):\n        class Fan(models.Model):\n            pass\n\n        class Event(models.Model):\n            invitees = models.ManyToManyField(Fan, through='Invitation', through_fields=('invitee', 'invitee'))\n\n        class Invitation(models.Model):\n            event = models.ForeignKey(Event, models.CASCADE)\n            invitee = models.ForeignKey(Fan, models.CASCADE)\n            inviter = models.ForeignKey(Fan, models.CASCADE, related_name='+')\n\n        field = Event._meta.get_field('invitees')\n        self.assertEqual(field.check(from_model=Event), [\n            Error(\n                \"The value 'invitee' appears twice in 'through_fields'.\",\n                hint=\"Make sure the 'through_fields' tuple contains distinct field names.\",\n                obj=field,\n                id='fields.E340',\n            ),\n        ])\n\n", "  def test_invalid_through_fields_references(self):\n        class Fan(models.Model):\n            pass\n\n        class Event(models.Model):\n            invitees = models.ManyToManyField(\n                Fan,\n                through='Invitation',\n                through_fields=('nonexistent_field_on_invitation', 'invitee'),\n            )\n\n        class Invitation(models.Model):\n            event = models.ForeignKey(Event, models.CASCADE)\n            invitee = models.ForeignKey(Fan, models.CASCADE)\n            inviter = models.ForeignKey(Fan, models.CASCADE, related_name='+')\n\n        field = Event._meta.get_field('invitees')\n        self.assertEqual(field.check(from_model=Event), [\n            Error(\n                \"The intermediary model 'invalid_models_tests.Invitation' has no field 'nonexistent_field_on_invitation'.\",\n                hint=\"Did you mean one of the following foreign keys to 'Event': event?\",\n                obj=field,\n                id='fields.E338',\n            ),\n        ])\n\n", "    def test_missing_through_fields(self):\n        \"\"\"\n        Ensure that an error is raised if through_fields is not provided\n        when a through model is specified for a ManyToManyField.\n        \"\"\"\n        class Fan(models.Model):\n            pass\n\n        class Event(models.Model):\n            invitees = models.ManyToManyField(Fan, through='Invitation')\n\n        self.assertEqual(Event._meta.get_field('invitees').check(from_model=Event), [\n            Error(\n                \"The 'through_fields' argument must be provided when a 'through' model is specified.\",\n                hint=\"Specify the names of the two link fields on the intermediary model 'Invitation' using 'through_fields=('field1', 'field2')'.\",\n                obj=Event._meta.get_field('invitees'),\n                id='fields.E336',\n            ),\n        ])\n", "    def test_empty_through_fields(self):\n        class Fan(models.Model):\n            pass\n\n        class Event(models.Model):\n            invitees = models.ManyToManyField(Fan, through='Invitation', through_fields=())\n\n        class Invitation(models.Model):\n            event = models.ForeignKey(Event, models.CASCADE)\n            invitee = models.ForeignKey(Fan, models.CASCADE)\n            inviter = models.ForeignKey(Fan, models.CASCADE, related_name='+')\n\n        field = Event._meta.get_field('invitees')\n        self.assertEqual(field.check(from_model=Event), [\n            Error(\n                \"through_fields cannot be empty.\",\n                hint=\"Please specify the two link fields from the intermediary model.\",\n                obj=field,\n                id='fields.E340',\n            ),\n\n        ])\n", "    def test_invalid_foreign_object_through_fields(self):\n        class Fan(models.Model):\n            pass\n\n        class Event(models.Model):\n            invitees = models.ManyToManyField(\n                Fan,\n                through='Invitation',\n                through_fields=('invitee', 'event'),\n            )\n\n        class Invitation(models.Model):\n            event = models.ForeignKey(Event, models.CASCADE)\n            invitee = models.ForeignKey(Fan, models.CASCADE)\n            inviter = models.ForeignKey(Fan, models.CASCADE, related_name='+')\n\n        class Child(models.Model):\n            parent = models.ForeignObject(\n                Invitation,\n                from_fields=('invitee', 'event'),\n                to_fields=('event', 'invitee'),\n            )\n\n        field = Child._meta.get_field('parent')\n        self.assertEqual(field.check(from_model=Child), [\n            Error(\n                \"Relation 'Child.parent' references through fields that are not unique together \"\n                \"on model 'invalid_models_tests.Invitation'.\",\n                hint=(\n                    \"Make sure the 'Invitation' model has a unique constraint using \"\n                    \"the fields specified in 'from_fields'. You can use \"\n                    \"'unique_together' or a 'UniqueConstraint'.\"\n                ),\n                obj=field,\n                id='fields.E340',\n            )\n        ])\n"], "sample_183": ["    def test_when_with_empty_lookup(self):\n        with self.assertRaisesMessage(ValueError, \"Empty lookups are not allowed.\"):\n            Case(When(pk__in=[], then=Value(True)))\n", "    def test_empty_when_list(self):\n        msg = 'Case() requires at least one When() clause.'\n        with self.assertRaisesMessage(ValueError, msg):\n            Case(default=Value(False)).as_sql(compiler='django.db.backends.sqlite3.compiler.SQLiteCompiler', connection=None)\n", "    def test_q_object_with_lookup(self):\n        self.assertEqual(\n            str(When(Q(integer__exact=1), then=Value(True))),\n            'CASE WHEN \"integer\" = 1 THEN True ELSE NULL END'\n        )\n", "    def test_duplicate_when_conditions(self):\n        with self.assertRaisesMessage(ValueError, \"Duplicate WHEN conditions found.\"):\n            Case(When(account_type=Client.REGULAR, then=Value(1)), When(account_type=Client.REGULAR, then=Value(2)))\n", "    def test_zero_when_clauses(self):\n        msg = 'Case must have at least one When clause.'\n        with self.assertRaisesMessage(ValueError, msg):\n            Case(default=Value(True))\n"], "sample_560": ["compilation error", "    def test_legend_markerfacecolor_iterable_cmap():\n        # test the labelcolor for labelcolor='markerfacecolor' on PathCollection\n        # with colormaps\n        fig, ax = plt.subplots()\n        facecolors = mpl.cm.viridis(np.random.rand(10))\n        ax.scatter(\n            np.arange(10),\n            np.arange(10),\n            label='#1',\n            c=np.arange(10),\n            facecolor=facecolors\n        )\n\n        leg = ax.legend(labelcolor='markerfacecolor')\n        for text, color in zip(leg.get_texts(), ['k']):\n            assert mpl.colors.same_color(text.get_color(), color)\n", "def test_legend_with_empty_label_string():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='')\n    ax.legend()\n", "def test_bbox_to_anchor_out_of_bounds(bbox_to_anchor):\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label='line')\n    with pytest.warns(UserWarning):\n        ax.legend(bbox_to_anchor=bbox_to_anchor)\n\n", "compilation error"], "sample_173": ["    def test_combine_duration_expression(self):\n        self.assertEqual(\n            self.ops.combine_duration_expression(' + ', ['a', 'b']),\n            'a + b'\n        )\n", "    def test_json_field_lookup(self):\n        msg = 'This backend does not support JSONField lookups.'\n        with self.assertRaisesMessage(NotSupportedError, msg):\n            self.ops.json_field_lookup(None, None, None)\n", "    def test_sql_flush_supports_insert_with_field_names(self):\n        with transaction.atomic():\n            author = Author.objects.create(name='George Orwell')\n            Book.objects.create(author=author)\n            author = Author.objects.create(name='Harper Lee')\n            Book.objects.create(author=author)\n            Book.objects.create(author=author)\n\n        sql_list = connection.ops.sql_flush(no_style(), [\n            Author._meta.db_table, Book._meta.db_table], reset_sequences=True, allow_cascade=True)\n\n        msg = 'This backend does not support insert with field names.'\n        with self.assertRaisesMessage(NotSupportedError, msg):\n            connection.ops.execute_sql_flush(connection.alias, sql_list)\n", "    def test_sql_flush_with_allow_cascade(self):\n        with transaction.atomic():\n            author = Author.objects.create(name='George Orwell')\n            Book.objects.create(author=author)\n            author = Author.objects.create(name='Harper Lee')\n            Book.objects.create(author=author)\n            Book.objects.create(author=author)\n            self.assertIs(Author.objects.exists(), True)\n            self.assertIs(Book.objects.exists(), True)\n\n        sql_list = connection.ops.sql_flush(\n            no_style(),\n            [Book._meta.db_table],\n            reset_sequences=False,\n            allow_cascade=True,\n        )\n        connection.ops.execute_sql_flush(connection.alias, sql_list)\n\n        with transaction.atomic():\n            self.assertIs(Author.objects.exists(), True)\n            self.assertIs(Book.objects.exists(), False)\n", "    def test_window_frame_range(self):\n        msg = 'This backend does not support window expressions.'\n        with self.assertRaisesMessage(NotSupportedError, msg):\n            self.ops.window_frame_range(1, 2)\n"], "sample_993": ["    def test_FreeGroupElm_cyclic_reduction():\n        w = x*y*x*y*x\n        assert w.cyclic_reduction() == y*x\n        assert w.cyclic_reduction(removed=True) == (y*x, x)\n        w = x*y**2*x**2*y\n        assert w.cyclic_reduction() == x**2*y**2\n        assert w.cyclic_reduction(removed=True) == (x**2*y**2, x)\n", "compilation error", "    def test_FreeGroupElm_cyclically_reduced():\n        w = x**2*y**-1*x**-1\n        assert not w.is_cyclically_reduced()\n        assert w.identity_cyclic_reduction() == x*y**-1\n        w2 = x**-3*y**-1*x**5\n        assert not w2.is_cyclically_reduced()\n        assert w2.identity_cyclic_reduction() == x**2*y**-1\n", "    def test_FreeGroupElm_cyclic_conjugates():\n        w = x*y*x*y*x\n        assert w.cyclic_conjugates() == {x*y*x**2*y, x**2*y*x*y, y*x*y*x**2, y*x**2*y*x, x*y*x*y*x}\n        s = x*y*x**2*y*x\n        assert s.cyclic_conjugates() == {x**2*y*x**2*y, y*x**2*y*x**2, x*y*x**2*y*x}\n", "compilation error"], "sample_814": ["def test_gradient_boosting_max_features_float():\n    # Test that max_features can be a float between 0 and 1 for both\n    # classifiers and regressors.\n    X, y = make_regression(random_state=0)\n    gbc = GradientBoostingClassifier(max_features=0.5, random_state=0)\n    gbr = GradientBoostingRegressor(max_features=0.5, random_state=0)\n\n    gbc.fit(X, y)\n    gbr.fit(X, y)\n\n    assert gbc.estimators_[0, 0].tree_.n_features == int(0.5*X.shape[1])\n    assert gbr.estimators_[0, 0].tree_.n_features == int(0.5*X.shape[1])\n\n", "def test_gradient_boosting_oob_score():\n    # Test that oob_score is calculated correctly for both classification\n    # and regression\n    X, y = make_classification(n_samples=100, n_features=10, random_state=0)\n    X_reg, y_reg = make_regression(n_samples=100, n_features=10, random_state=0)\n\n    for EstimatorClass, y, score_func in [(GradientBoostingClassifier, y,\n                                        'score'),\n                                       (GradientBoostingRegressor, y_reg,\n                                        'r2_score')]:\n        est = EstimatorClass(n_estimators=50, subsample=0.8, random_state=0,\n                             oob_score=True)\n        est.fit(X, y)\n\n        assert hasattr(est, 'oob_score_')\n        oob_score = score_func(est, X, y)\n        assert_allclose(est.oob_score_, oob_score)\n\n", "def test_gradient_boosting_regressor_with_constant_features():\n    # Test GradientBoostingRegressor with constant features\n    X = np.ones((100, 1))\n    y = np.random.rand(100)\n    reg = GradientBoostingRegressor(random_state=0).fit(X, y)\n    # Check if the model can predict\n    reg.predict(X)\n    # Check if feature importance is zero for constant feature\n    assert reg.feature_importances_[0] == 0\n", "        def __init__(self, alpha=0.1):\n            self.alpha = alpha\n", "    def test_gradient_boosting_with_validation_fraction_and_tol(self):\n        # Test if early stopping criteria is correctly applied when both\n        # validation_fraction and tol are used.\n\n        X, y = make_classification(n_samples=1000, random_state=0)\n\n        gbc = GradientBoostingClassifier(n_estimators=1000,\n                                         validation_fraction=0.2,\n                                         tol=1e-3,\n                                         max_depth=3,\n                                         random_state=42)\n\n        gbr = GradientBoostingRegressor(n_estimators=1000,\n                                        validation_fraction=0.2,\n                                        tol=1e-3,\n                                        max_depth=3,\n                                        random_state=42)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                           random_state=42)\n        gbc.fit(X_train, y_train)\n        gbr.fit(X_train, y_train)\n        assert gbc.n_estimators_ < 1000 \n        assert gbr.n_estimators_ < 1000\n\n"], "sample_72": ["    def test_serialize_decimal_field(self):\n        self.assertSerializedEqual(models.DecimalField())\n        self.assertSerializedEqual(models.DecimalField(max_digits=10, decimal_places=2))\n        field = models.DecimalField(max_digits=10, decimal_places=2, default=decimal.Decimal('10.20'))\n        string = MigrationWriter.serialize(field)[0]\n        self.assertEqual(\n            string,\n            \"models.DecimalField(decimal_places=2, max_digits=10, default=decimal.Decimal('10.20'))\"\n        )\n\n", "    def test_serialize_custom_class_with_path(self):\n        @deconstructible(path='migrations.test_writer.CustomClass')\n        class CustomClass:\n                self.value = value\n\n        self.assertSerializedEqual(CustomClass(123))\n", "    def test_serialize_decimal_with_context(self):\n        with decimal.localcontext(decimal.Context(prec=28)):\n            value = decimal.Decimal(\"1.2345678901234567890123456789\")\n        self.assertSerializedEqual(value)\n", "    def test_serialize_default_function(self):\n            return datetime.datetime(2021, 1, 1)\n        \n        string, imports = MigrationWriter.serialize(models.DateTimeField(default=get_default))\n        self.assertEqual(string, 'models.DateTimeField(default=migrations.test_writer.get_default)')\n        self.assertEqual(imports, {'from django.db import models', 'import migrations.test_writer'})\n", "    def test_serialize_enum_with_module_path(self):\n        class TextEnum(enum.Enum):\n            A = 'a-value'\n            B = 'value-b'\n        \n        field = models.CharField(default=TextEnum.B, choices=[(m.value, m) for m in TextEnum])\n        string = MigrationWriter.serialize(field)[0]\n        self.assertEqual(\n            string,\n            \"models.CharField(choices=[\"\n            \"('a-value', migrations.test_writer.TextEnum('a-value')), \"\n            \"('value-b', migrations.test_writer.TextEnum('value-b'))], \"\n            \"default=migrations.test_writer.TextEnum('value-b'))\"\n        )\n\n"], "sample_1133": ["def test_deviation_total_internal_reflection():\n    n1, n2 = symbols('n1, n2')\n    r1 = Ray3D(Point3D(-1, -1, 1), Point3D(0, 0, 0))\n    n = Matrix([0, 0, 1])\n    i = Matrix([-1, -1, -1])\n    normal_ray = Ray3D(Point3D(0, 0, 0), Point3D(0, 0, 1))\n    P = Plane(Point3D(0, 0, 0), normal_vector=[0, 0, 1])\n    assert deviation(r1, 1.33, 1, plane=P) is None\n", "    def test_fresnel_coefficients_normal_incidence():\n        assert all(ae(i, j, 5) for i, j in zip(\n            fresnel_coefficients(0, 1, 1.33),\n            [1, 0, 0, 0]))\n        assert all(ae(i, j, 5) for i, j in zip(\n            fresnel_coefficients(0, 1.33, 1),\n            [0, 0, 1, 0]))\n\n", "compilation error", "    def test_fresnel_coefficients_p_s():\n        assert all(ae(i, j, 5) for i, j in zip(\n            fresnel_coefficients(0.5, 1, 1.33, polarized='p'),\n            [0.11163, -0.17138, 0.83581, 0.82862]))\n        assert all(ae(i, j, 5) for i, j in zip(\n            fresnel_coefficients(0.5, 1.33, 1, polarized='p'),\n            [-0.07726, 0.20482, 1.22724, 1.20482]))\n        m1 = Medium('m1')\n        m2 = Medium('m2', n=2)\n        assert all(ae(i, j, 5) for i, j in zip(\n            fresnel_coefficients(0.3, m1, m2, polarized='p'),\n            [0.31784, -0.34865, 0.65892, 0.65135]))\n        ans = [[-0.23563, -0.97184], [0.81648, -0.57738]]\n        got = fresnel_coefficients(0.6, m2, m1, polarized='p')\n        for i, j in zip(got, ans):\n            for a, b in zip(i.as_real_imag(), j):\n                assert ae(a, b, 5)\n\n", "    def test_refraction_angle_negative_angle():\n        n1, n2 = symbols('n1, n2')\n        m1 = Medium('m1')\n        m2 = Medium('m2')\n        r1 = Ray3D(Point3D(-1, -1, 1), Point3D(0, 0, 0))\n        i = Matrix([-1, -1, -1])\n        n = Matrix([0, 0, 1])\n        assert refraction_angle(r1, 1, 1, n, negative_angle=True) == Matrix([\n                [ 1],\n                [ 1],\n                [-1]])\n        assert refraction_angle(i, 1, 1, n, negative_angle=True) == Matrix([\n                [ 1],\n                [ 1],\n                [-1]])\n        assert refraction_angle([1, 1, 1], 1, 1, n, negative_angle=True) == Matrix([\n                [ 1],\n                [ 1],\n                [-1]])\n        assert refraction_angle((1, 1, 1), 1, 1, n, negative_angle=True) == Matrix([\n                [ 1],\n                [ 1],\n                [-1]])\n        assert refraction_angle(i, 1, 1.33, n, negative_angle=True).evalf(3) + 0.119 < 1e-3\n\n"], "sample_1063": ["compilation error", "def test_issue_17247():\n    if not numpy:\n        skip(\"numpy not installed\")\n\n    x = sympy.symbols('x')\n    y = sympy.symbols('y')\n    f = lambdify((x, y), x + y, modules='numpy')\n    assert f(numpy.array([1, 2]), numpy.array([3, 4]))[0] == 4\n    assert f(numpy.array([1, 2]), numpy.array([3, 4]))[1] == 6\n\n", "compilation error", "compilation error", "def test_issue_17681():\n    a = symbols('a')\n    f = lambdify(a, factorial(a), 'mpmath')\n    assert f(5) == 120\n    assert f(5.0) == 120\n"], "sample_266": ["    def test_circular_references(self):\n        \"\"\"\n        Tests that circular dependencies between migrations raise a clear error.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        with self.assertRaises(RuntimeError) as cm:\n            loader.build_graph()\n        self.assertIn(\n            \"Circular dependencies found in migrations:\",\n            str(cm.exception),\n        )\n", "    def test_detect_conflicts(self):\n        \"\"\"Test that detect_conflicts identifies apps with more than one leaf migration.\"\"\"\n        loader = MigrationLoader(connection)\n        conflicts = loader.detect_conflicts()\n        self.assertEqual(conflicts, {'migrations': ['second', 'third']})\n", "    def test_loading_squashed_complex_multi_apps_conflicting_dependencies(self):\n        \"\"\"Tests loading a complex set of squashed migrations across multiple apps\n        with conflicting dependencies.\"\"\"\n        with self.override_settings(\n            MIGRATION_MODULES={\n                'app1': 'migrations.test_migrations_squashed_complex_multi_apps_conflicting_dependencies.app1',\n                'app2': 'migrations.test_migrations_squashed_complex_multi_apps_conflicting_dependencies.app2',\n            }\n        ):\n            with self.modify_settings(INSTALLED_APPS=[\n                'migrations.test_migrations_squashed_complex_multi_apps_conflicting_dependencies.app1',\n                'migrations.test_migrations_squashed_complex_multi_apps_conflicting_dependencies.app2',\n            ]):\n                loader = MigrationLoader(connection)\n                recorder = MigrationRecorder(connection)\n\n                # Partial application to trigger the conflict\n                recorder.record_applied('app1', '1_auto')\n                recorder.record_applied('app2', '1_auto')\n\n                with self.assertRaises(NodeNotFoundError) as context:\n                    loader.build_graph()\n\n                self.assertIn('Migration app2.2_auto depends on nonexistent node (\\'app1\\', \\'2_auto\\').', str(context.exception))\n", "    def test_detect_conflicts(self):\n        loader = MigrationLoader(connection)\n        conflicts = loader.detect_conflicts()\n        self.assertEqual(conflicts, {'migrations': ['0001_initial', '0002_second']})\n\n", "    def test_detect_conflicts(self):\n        loader = MigrationLoader(connection)\n        conflicts = loader.detect_conflicts()\n        self.assertEqual(conflicts, {\n            'migrations': sorted(['0001_initial', '0002_second']),\n        })\n"], "sample_1003": ["def test_Options_from_dict():\n    opt = Options.from_dict({'domain': ZZ, 'gens': (x, y)})\n\n    assert opt.gens == (x, y)\n    assert opt.domain == ZZ\n", "    def test_build_options(self):\n        opt = build_options((x, y), {'domain': ZZ})\n        assert opt.gens == (x, y)\n        assert opt.domain == ZZ\n\n        opt = build_options((x, y), {'opt': {'domain': ZZ}})\n        assert opt.gens == (x, y)\n        assert opt.domain == ZZ\n", "def test_Gens_postprocess_empty():\n    opt = {'gens': ()}\n    Gens.postprocess(opt)\n\n    assert opt == {'gens': ()}\n", "def test_Options_allows_flags():\n\n    opt = Options((x, y, z), {'domain': ZZ})\n    allowed_flags({'domain': ZZ}, ['domain'])\n\n    raises(FlagError, lambda: allowed_flags({'domain': ZZ, 'frac': True}, []))\n    allowed_flags({'domain': ZZ, 'frac': True}, ['frac', 'domain'])\n", "    def test_Domain_preprocess_errors():\n        raises(OptionError, lambda: Domain.preprocess('Z[x,y,'))\n        raises(OptionError, lambda: Domain.preprocess('Q(x,y,'))\n        raises(OptionError, lambda: Domain.preprocess('ZZ(x,y,'))\n        raises(OptionError, lambda: Domain.preprocess('QQ(x,y,'))\n\n        raises(OptionError, lambda: Domain.preprocess('Z[a'))\n        raises(OptionError, lambda: Domain.preprocess('Q[a'))\n        raises(OptionError, lambda: Domain.preprocess('ZZ[a'))\n        raises(OptionError, lambda: Domain.preprocess('QQ[a'))\n\n        raises(OptionError, lambda: Domain.preprocess('Z(a'))\n        raises(OptionError, lambda: Domain.preprocess('Q(a'))\n        raises(OptionError, lambda: Domain.preprocess('ZZ(a'))\n        raises(OptionError, lambda: Domain.preprocess('QQ(a'))\n"], "sample_180": ["    def test_check_constraints_invalid_condition(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(\n                        check=models.Q(age__gt='invalid'),\n                        name='is_adult'\n                    ),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [\n            Error(\n                \"Invalid condition in 'constraints': 'age__gt='invalid''. \"\n                \"Ensure the condition uses a valid Django Q expression.\",\n                obj=Model,\n                id='models.E031',\n            ),\n        ])\n\n", "    def test_unique_constraint_pointing_to_reverse_fk(self):\n        class Target(models.Model):\n            pass\n\n        class Model(models.Model):\n            fk = models.ForeignKey(Target, models.CASCADE, related_name='related_models')\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(fields=['fk'], name='name'),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [])\n\n", "    def test_check_constraints_with_deferrable_unique_constraint(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['age'],\n                        name='unique_age_deferrable',\n                        deferrable=models.Deferrable.DEFERRED,\n                    ),\n                    models.UniqueConstraint(\n                        fields=['age'],\n                        name='unique_age_immediate',\n                        deferrable=models.Deferrable.IMMEDIATE,\n                    ),\n                ]\n\n        errors = Model.check(databases=self.databases)\n        expected = [] if connection.features.supports_deferrable_unique_constraints else [\n            Warning(\n                '%s does not support deferrable unique constraints.'\n                % connection.display_name,\n                hint=(\n                    \"A constraint won't be created. Silence this warning if \"\n                    \"you don't care about it.\"\n                ),\n                obj=Model,\n                id='models.W038',\n            ),\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_unique_constraint_pointing_to_related_model(self):\n        class Author(models.Model):\n            name = models.CharField(max_length=100)\n\n        class Book(models.Model):\n            title = models.CharField(max_length=100)\n            author = models.ForeignKey('Author', models.CASCADE, related_name='books')\n\n            class Meta:\n                constraints = [models.UniqueConstraint(fields=['title', 'author__name'], name='unique_title_author')]\n\n        self.assertEqual(Book.check(databases=self.databases), [])\n\n", "    def test_unique_constraint_pointing_to_related_model_field(self):\n        class Target(models.Model):\n            field = models.IntegerField()\n\n        class Model(models.Model):\n            fk = models.ForeignKey(Target, models.CASCADE, related_name='target')\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(fields=['fk__field'], name='name'),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [])\n"], "sample_999": ["compilation error", "compilation error", "    def test_issue_17002():\n        from sympy.physics.units import meter, second\n        assert latex(meter/second) == r\"\\frac{\\text{m}}{\\text{s}}\"\n\n", "def test_ExteriorDerivative_printing():\n    from sympy.diffgeom import Manifold, Patch, CoordSystem, exterior_derivative\n    m = Manifold('M', 2)\n    p = Patch('P', m)\n    cs = CoordSystem('xy', p, ['x', 'y'])\n    dx = cs.dx\n    dy = cs.dy\n    assert latex(exterior_derivative(dx)) == r\"\\mathrm{d}\\left(\\mathrm{d}x\\right)\"\n", "def test_issue_15833():\n    from sympy.physics.units import meter, second\n    assert latex(meter/second) == r\"\\frac{\\text{m}}{\\text{s}}\"\n"], "sample_1002": ["def test_Float_issue_11718():\n    x = Float(1.0, dps=28)\n    assert x._prec == 28\n    assert x == Float(1.0, dps=28)\n    assert x != Float(1.0, dps=27)\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_655": ["    def test_donotreadfrominput_is_closed_after_read(testdir):\n        p = testdir.makepyfile(\n            \"\"\"\n                import sys\n                sys.stdin.read()\n                sys.stdout.write(\"hello\")\n        \"\"\"\n        )\n        result = testdir.runpytest(str(p))\n        assert result.ret == 0\n        assert result.stdout.str() == \"hello\\n\"\n\n", "def test_StdCaptureFD_closed_file(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import os\n            fd = os.open(\"myfile\", os.O_WRONLY | os.O_CREAT)\n            os.close(fd)\n            with StdCaptureFD(out=True, err=True, in_=True) as cap:\n                try:\n                    os.write(fd, b\"hello\\\\n\")\n                except OSError:\n                    pass\n                out, err = cap.readouterr()\n            assert out == \"\"\n            assert err == \"\"\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    assert result.ret == 0\n", "    def test_capture_with_unicode_error():\n        with capture.CaptureIO() as out:\n            try:\n                out.write(b\"\\xa6\")\n            except UnicodeEncodeError:\n                pass\n            assert out.getvalue() == \"\\ufffd\"\n", "    def test_capturing_raw_io(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import io\n\n                raw_stdout = io.BytesIO()\n                with io.StringIO() as string_io:\n                    print('hello', file=string_io)\n                    print('world', file=raw_stdout)\n                assert string_io.getvalue() == 'hello\\\\n'\n                assert raw_stdout.getvalue() == b'world\\\\n'\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines([\"*test_raw_io*passed*\"])\n", "def test_capture_works_with_logging_and_multiprocessing(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import multiprocessing\n        import sys\n\n        logger = logging.getLogger(__name__)\n\n            print(\"hello from worker\")\n            logger.info(\"worker logging\")\n\n            p = multiprocessing.Process(target=worker)\n            p.start()\n            p.join()\n            captured = sys.stdout.getvalue()\n            assert captured == \"hello from worker\\\\n\"\n            assert len(logger.handlers) == 1\n        \"\"\"\n    )\n\n    result = testdir.runpytest(\"--log-cli-level=INFO\")\n    assert result.ret == 0\n"], "sample_615": ["compilation error", "    def test_cross_empty():\n        a = xr.DataArray(np.array([]).reshape((0, 3)), dims=[\"dim_0\", \"dim_1\"])\n        b = xr.DataArray(np.array([]).reshape((0, 3)), dims=[\"dim_0\", \"dim_1\"])\n\n        with pytest.raises(ValueError, match=r\"Input arrays must not be empty\"):\n            xr.cross(a, b, dim=\"dim_1\")\n\n", "def test_cross_different_lengths(use_dask: bool) -> None:\n    a = xr.DataArray([1, 2, 3], dims=[\"x\"])\n    b = xr.DataArray([4, 5, 6, 7], dims=[\"x\"])\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"test for dask.\")\n        a = a.chunk()\n        b = b.chunk()\n\n    with pytest.raises(ValueError):\n        xr.cross(a, b)\n", "compilation error", "    def test_cross_mismatched_coords(use_dask):\n        if use_dask and not has_dask:\n            pytest.skip(\"requires dask\")\n\n        a = xr.DataArray(\n            np.arange(12).reshape((3, 4)),\n            dims=[\"x\", \"y\"],\n            coords={\"x\": [1, 2, 3], \"y\": [\"a\", \"b\", \"c\", \"d\"]},\n        )\n        b = xr.DataArray(\n            np.arange(12).reshape((3, 4)), dims=[\"x\", \"z\"], coords={\"x\": [1, 2, 3], \"z\": [1, 2, 3, 4]}\n        )\n\n        if use_dask:\n            a = a.chunk({\"x\": 2})\n            b = b.chunk({\"x\": 2})\n\n        with pytest.raises(ValueError, match=r\"cannot be broadcast\"):\n            xr.cross(a, b, dim=\"x\")\n"], "sample_362": ["compilation error", "    def test_alter_unique_together(self):\n        before = [\n            ModelState('app', 'Model', [\n                ('id', models.AutoField(primary_key=True)),\n                ('field1', models.CharField(max_length=100)),\n                ('field2', models.CharField(max_length=100)),\n            ]),\n        ]\n        after = [\n            ModelState('app', 'Model', [\n                ('id', models.AutoField(primary_key=True)),\n                ('field1', models.CharField(max_length=100)),\n                ('field2', models.CharField(max_length=100)),\n            ],\n            options={'unique_together': [('field1', 'field2')]}\n        )\n\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['AlterModelOptions'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='Model', options={'unique_together': [('field1', 'field2')]})\n", "    def test_add_field_with_default(self):\n        changes = self.get_changes([self.author_empty], [self.author_with_bio])\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['AddField'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='biography')\n", "    def test_rename_model(self):\n        before = [\n            ModelState('testapp', 'OldName', [\n                ('id', models.AutoField(primary_key=True)),\n            ]),\n        ]\n        after = [\n            ModelState('testapp', 'NewName', [\n                ('id', models.AutoField(primary_key=True)),\n            ]),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['RenameModel'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name='OldName', new_name='NewName')\n\n", "    def test_squashed_migration_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[]),\n                migrations.CreateModel('Animal', fields=[]),\n            ]\n            replaced_by = '0002_squashed_migration'\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'person_animal')\n"], "sample_114": ["    def test_proxy_with_empty_proxy_bases(self):\n        \"\"\"\n        Proxy models with no explicit bases should have their own migrations\n        created, not be skipped.\n        \"\"\"\n        EmptyProxy = ModelState('app', 'EmptyProxy', [], bases=['testapp.ProxyBase'])\n        ProxyBase = ModelState('testapp', 'ProxyBase', [('id', models.AutoField(primary_key=True))])\n        changes = self.get_changes([ProxyBase], [EmptyProxy, ProxyBase])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['CreateModel'])\n        self.assertOperationTypes(changes, 'testapp', 0, ['CreateModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='EmptyProxy')\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='ProxyBase')\n\n", "    def test_mti_inheritance_model_addition_with_existing_pk(self):\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n        ])\n        Dog = ModelState('app', 'Dog', [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"animal_ptr\", models.OneToOneField(Animal, models.CASCADE, parent_link=True)),\n        ], bases=('app.Animal',))\n        changes = self.get_changes([Animal], [Animal, Dog])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['CreateModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='Dog')\n\n", "    def test_remove_mti_base_model(self):\n        A = ModelState(\"app\", \"A\", [\n            (\"id\", models.AutoField(primary_key=True)),\n        ])\n        B = ModelState(\"app\", \"B\", [], bases=(\"app.A\",))\n        C = ModelState(\"app\", \"C\", [], bases=(\"app.B\",))\n        changes = self.get_changes([A, B, C], [B, C])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['DeleteModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='A')\n", "    def test_circular_dependency_proxy_models(self):\n        \"\"\"\n        #23877 - A proxy model should not cause circular dependency\n        if its base is processed after it.\n        \"\"\"\n        proxy = ModelState('app', 'Proxy', [], bases=('app.Other',))\n        other = ModelState('app', 'Other', [\n            ('id', models.AutoField(primary_key=True)),\n        ])\n        changes = self.get_changes([proxy, other], [other, proxy])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['CreateModel', 'CreateModel'])\n", "    def test_proxy_model_inheritance_with_fields(self):\n        \"\"\"\n        #24080 - Proxy models inheriting from models with fields should\n        correctly handle those fields in migrations.\n        \"\"\"\n        BaseBook = ModelState(\"app\", \"BaseBook\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"title\", models.CharField(max_length=100))\n        ])\n        ProxyBook = ModelState(\"app\", \"ProxyBook\", [], bases=(\"app.BaseBook\",),\n                                options={'proxy': True})\n        changes = self.get_changes([BaseBook], [BaseBook, ProxyBook])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['CreateModel', 'CreateModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='ProxyBook')\n        self.assertOperationAttributes(changes, 'app', 0, 1, name='BaseBook')\n"], "sample_268": ["    def test_should_stop_after_setting_should_stop(self):\n        self.reloader.should_stop = True\n        self.assertTrue(self.reloader.should_stop)\n", "    def test_watch_dir_with_absolute_path(self):\n        self.reloader.watch_dir(str(self.tempdir), '*.py')\n        watched_files = list(self.reloader.watched_files())\n        self.assertIn(self.existing_file, watched_files)\n", "    def test_watch_files_with_duplicate_globs(self):\n        self.reloader.watch_dir(self.tempdir, '*.py')\n        self.reloader.watch_dir(self.tempdir, '*.py')\n        watched_files = list(self.reloader.watched_files())\n        self.assertEqual(len(watched_files), 1)\n        self.assertIn(self.existing_file, watched_files)\n", "    def test_watch_dir_recursive_with_unresolvable_path(self):\n        path = Path('unresolvable_directory')\n        with mock.patch.object(Path, 'absolute', side_effect=FileNotFoundError):\n            self.reloader.watch_dir(path, '**/*.py')\n        self.assertEqual(list(self.reloader.directory_globs), [])\n\n", "    def test_should_stop(self):\n        # Should stop returning True if self._should_stop is True\n        self.reloader._should_stop = True\n        self.assertTrue(self.reloader.should_stop())\n        \n        # Should return False otherwise.\n        self.reloader._should_stop = False\n        self.assertFalse(self.reloader.should_stop())\n"], "sample_330": ["    def test_delete_cascade(self):\n        obj = Object.objects.create()\n        obj.related_objects.create()\n        self.assertEqual(Object.objects.count(), 2)\n        obj.delete()\n        self.assertEqual(Object.objects.count(), 1)\n", "    def test_many_to_many_non_existent(self):\n        obj = Object.objects.create()\n        obj.related_objects.create()\n        self.assertEqual(Object.objects.count(), 2)\n        self.assertEqual(obj.related_objects.count(), 1)\n\n        with self.assertRaises(Object.DoesNotExist):\n            obj.related_objects.get(id=12345)\n", "    def test_many_to_many_with_through_table(self):\n        obj = Object.objects.create()\n        obj.related_through_objects.create(value='foo')\n        self.assertEqual(Object.objects.count(), 2)\n        self.assertEqual(obj.related_through_objects.count(), 1)\n\n        intermediary_model = Object._meta.get_field(\n            \"related_through_objects\"\n        ).remote_field.through\n        obj1 = Object.objects.create()\n        intermediary_model.objects.create(\n            from_object=obj, to_object=obj1, value='bar'\n        )\n\n        self.assertEqual(obj.related_through_objects.count(), 2)\n        self.assertEqual(obj1.related_through_objects.count(), 1)\n        self.assertEqual(intermediary_model.objects.count(), 2)\n", "    def test_many_to_many_non_existent(self):\n        obj = Object.objects.create()\n        obj.related_objects.create()\n        obj.related_objects.add(12345)\n        self.assertEqual(Object.objects.count(), 2)\n        self.assertEqual(obj.related_objects.count(), 2) \n\n", "    def test_constraint_check_disabled(self):\n        with transaction.atomic():\n            obj = SQLKeywordsModel.objects.create(reporter=self.r)\n            obj.refresh_from_db()\n            obj.reporter_id = 30\n            with connection.constraint_checks_disabled():\n                obj.save()\n            self.assertTrue(SQLKeywordsModel.objects.filter(pk=obj.pk).exists())\n            with self.assertRaises(IntegrityError):\n                connection.check_constraints(table_names=['order'])\n\n\n\n"], "sample_791": ["compilation error", "def test_ordinal_encoder_sparse():\n    enc = OrdinalEncoder(sparse=True)\n    X = [['abc', 2, 55], ['def', 1, 55]]\n    exp = csr_matrix([[0, 1, 0], [1, 0, 0]])\n    assert_array_equal(enc.fit_transform(X).toarray(), exp.toarray())\n", "compilation error", "    def test_one_hot_encoder_drop_numeric():\n        enc = OneHotEncoder(drop=['Female'])\n        X = [['Male', 12], ['Female', 3], ['Male', 1]]\n        trans = enc.fit_transform(X).toarray()\n        exp = [[1, 1],\n               [0, 1],\n               [1, 0]]\n        assert_array_equal(trans, exp)\n", "    def test_one_hot_encoder_get_feature_names_with_drop(self):\n        enc = OneHotEncoder(drop=['first'])\n        X = [['Male', 1], ['Female', 3]]\n        enc.fit(X)\n        feature_names = enc.get_feature_names()\n        assert isinstance(feature_names, np.ndarray)\n        assert_array_equal(feature_names, ['x1_3'])\n"], "sample_506": ["def test_spine_set_bounds_with_position(spine_type):\n    fig, ax = plt.subplots()\n    spine = getattr(ax.spines, spine_type)\n    spine.set_position(('data', 0.5))\n    spine.set_bounds(0, 1)\n    assert spine.get_bounds() == (0, 1)\n", "def test_circular_spine():\n    fig, ax = plt.subplots(polar=True)\n    ax.set_rmax(10)\n    ax.set_theta_zero_location(\"N\")\n    ax.spines['polar'].set_visible(False)\n    ax.spines['circle'].set_visible(True)\n    ax.set_theta_direction(-1)\n    ax.set_theta_offset(np.pi/4)\n\n    ax.plot(np.linspace(0, 2*np.pi, 100), np.arange(100))\n\n", "    def test_spine_circular(fig_test, fig_ref):\n        fig_test.set_tight_layout(True)\n        fig_ref.set_tight_layout(True)\n\n        ax_test = fig_test.add_subplot(111, projection='polar')\n        ax_ref = fig_ref.add_subplot(111, projection='polar')\n\n        ax_test.set_theta_zero_location(\"N\")\n        ax_ref.set_theta_zero_location(\"N\")\n\n        ax_test.spines['polar'].set_visible(False)\n        ax_ref.spines['polar'].set_visible(False)\n\n        ax_test.set_rlim(0, 1)\n        ax_ref.set_rlim(0, 1)\n\n        ax_test.spines['circle'].set_bounds(0, 1)\n        ax_ref.spines['circle'].set_bounds(0, 1)\n\n        ax_test.plot([0, np.pi/2], [0.5, 0.5], marker='o')\n        ax_ref.plot([0, np.pi/2], [0.5, 0.5], marker='o')", "def test_spines_arcs():\n    fig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\n    ax.spines['polar'].set_visible(False)  # Hide the default polar spine\n    ax.set_theta_zero_location(\"N\")\n    ax.set_theta_direction(-1)\n    ax.spines['top'].set_position(('data', 0))\n    ax.spines['top'].set_color('red')\n    ax.spines['top'].set_linestyle('-')\n\n    ax.spines['left'].set_position(('data', np.pi/2))\n    ax.spines['left'].set_color('green')\n    ax.spines['left'].set_linestyle('--')\n\n    ax.set_rmax(2)\n\n    ax.plot(np.linspace(0, 2*np.pi, 100),\n            [1]*100, color='blue', linewidth=2)\n\n", "def test_spines_arc_spine():\n    fig, ax = plt.subplots(subplot_kw={'polar': True})\n    ax.set_theta_offset(np.pi/2)\n    ax.set_theta_direction(-1)\n    ax.set_rlabel_position(0)\n\n    ax.spines['polar'].set_visible(False)\n    ax.spines['spokes'].set_visible(False)\n    ax.spines['circle'].set_position('zero')\n    ax.spines['circle'].set_color(\"blue\")\n\n"], "sample_477": ["    def test_random_non_iterable(self):\n        output = self.engine.render_to_string(\"random_non_iterable\", {\"a\": \"string\"})\n        self.assertEqual(output, \"string\")\n\n", "    def test_random_dict(self):\n        output = self.engine.render_to_string(\"random_dict\", {\"a\": {\"key1\": \"value1\", \"key2\": \"value2\"}})\n        self.assertIn(output, [\"value1\", \"value2\"])\n", "    def test_random_none(self):\n        output = self.engine.render_to_string(\"random_none\", {\"none_list\": None })\n        self.assertEqual(output, \"\")\n\n", "    def test_random_none(self):\n        output = self.engine.render_to_string(\"random_none\", {\"data\": None})\n        self.assertEqual(output, \"\")\n", "    def test_random_with_none(self):\n        output = self.engine.render_to_string(\"random_with_none\", {\"list\": [1, 2, None]})\n        self.assertIn(output, [\"1\", \"2\", \"\"])\n"], "sample_450": ["    def test_get_admin_log_template_tag(self):\n        LogEntry.objects.log_action(\n            self.user.pk,\n            ContentType.objects.get_for_model(Article).pk,\n            self.a1.pk,\n            \"Article changed\",\n            CHANGE,\n            change_message=\"Article changed message\",\n        )\n        response = self.client.get(reverse(\"admin:index\"))\n        # Test template tag get_admin_log\n        template = \"\"\"\n        {% load admin_utils_tags %}\n        {% get_admin_log 5 as logs %}\n        {% for log in logs %}\n            {{ log.change_message }}\n        {% endfor %}\n        \"\"\"\n        rendered_output = self.client.render(request=self.request, template=template)\n        self.assertIn(\"Article changed message\", rendered_output)\n", "    def test_get_admin_log_template_tag(self):\n        LogEntry.objects.log_action(\n            self.user.pk,\n            ContentType.objects.get_for_model(Article).pk,\n            self.a1.pk,\n            repr(self.a1),\n            CHANGE,\n            change_message=\"Changed something\",\n        )\n\n        template = \"\"\"", "    def test_get_admin_log_template_tag(self):\n        LogEntry.objects.log_action(\n            self.user.pk,\n            ContentType.objects.get_for_model(Article).pk,\n            self.a1.pk,\n            \"Article changed\",\n            CHANGE,\n            change_message=\"Article changed message\",\n        )\n        LogEntry.objects.log_action(\n            self.user.pk,\n            ContentType.objects.get_for_model(Car).pk,\n            c1.pk,\n            \"Car created\",\n            ADDITION,\n            change_message=\"Car created message\",\n        )\n        template = \"\"\"\n        {% load admin_utils_tags %}\n        {% get_admin_log 2 as admin_log %}\n        {% for log in admin_log %}\n            {{ log.object_repr }}\n        {% endfor %}\n        \"\"\"\n        response = self.client.get(reverse(\"admin:index\"))\n        rendered = template.render(\n            {\n                \"user\": self.user,\n                \"request\": response.wsgi_request,\n            }\n        )\n        self.assertIn(\"Article changed\", rendered)\n        self.assertIn(\"Car created\", rendered)\n\n", "    def test_get_admin_log_template_tag(self):\n        LogEntry.objects.log_action(\n            self.user.pk,\n            ContentType.objects.get_for_model(Article).pk,\n            self.a1.pk,\n            \"Article changed\",\n            CHANGE,\n            change_message=\"Article changed message\",\n        )\n        response = self.client.get(reverse(\"admin:index\"))\n        self.assertContains(response, '{% get_admin_log 5 as admin_log %}')\n        context = {}\n        template = Template('{% get_admin_log 5 as admin_log %}{{ admin_log }}')\n        template.render(Context(context))\n        self.assertEqual(len(context['admin_log']), 1)\n", "    def test_get_admin_log_tag(self):\n        # Create some log entries to test\n        LogEntry.objects.log_action(\n            self.user.pk,\n            ContentType.objects.get_for_model(Article).pk,\n            self.a1.pk,\n            \"Article changed\",\n            CHANGE,\n            change_message=\"Article changed message\",\n        )\n        LogEntry.objects.log_action(\n            self.user.pk,\n            ContentType.objects.get_for_model(Car).pk,\n            c1.pk,\n            \"Car created\",\n            ADDITION,\n            change_message=\"Car created message\",\n        )\n\n        # Test without arguments\n        context = {}\n        template = Template('{% get_admin_log 5 as admin_log %}{{ admin_log }}')\n        template.render(context)\n        self.assertEqual(len(context['admin_log']), 5)\n\n        # Test with user filter\n        context = {'user': self.user}\n        template = Template('{% get_admin_log 5 as admin_log for_user user %}{{ admin_log }}')\n        template.render(context)\n        self.assertEqual(len(context['admin_log']), 2)\n\n        # Test with invalid user filter\n        template = Template('{% get_admin_log 5 as admin_log for_user 9999 %}{{ admin_log }}')\n        with self.assertRaises(TemplateSyntaxError):\n            template.render(context)\n\n"], "sample_162": ["    def test_makemessages_no_default_locale(self):\n        with override_settings(LANGUAGE_CODE='en-us'):\n            out, err = self.run_django_admin(['makemessages', '-l', 'fr', '-v', '0'])\n        self.assertNoOutput(err)\n        self.assertNoOutput(out)\n", "    def test_makemessages_ignores_pyc_during_extraction(self):\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n        self.assertMsgId('This literal should be included.', po_contents)\n        self.assertNotMsgId('This literal should be ignored.', po_contents)\n", "    def test_makemessages_no_locale_paths(self):\n        with override_settings(LOCALE_PATHS=[]):\n            with self.assertRaisesMessage(\n                management.CommandError,\n                \"Unable to find a locale path to store translations\"\n            ):\n                management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n\n", "    def test_project_locale_paths_with_symlinks(self):\n        symlink_target = os.path.join(self.test_dir, 'project_locale_target')\n        symlink_path = os.path.join(self.test_dir, 'project_locale')\n        os.mkdir(symlink_target)\n        os.symlink(symlink_target, symlink_path)\n        with override_settings(LOCALE_PATHS=[symlink_path]):\n            management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n            project_de_locale = os.path.join(\n                symlink_target, 'de', 'LC_MESSAGES', 'django.po')\n            self.assertTrue(os.path.exists(project_de_locale))\n", "    def test_custom_ignore_patterns(self):\n        with override_settings(MAKEMESSAGES_IGNORE_PATTERNS=['xxx_*']):\n            management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n            self.assertTrue(os.path.exists(self.PO_FILE))\n            with open(self.PO_FILE) as fp:\n                po_contents = fp.read()\n            self.assertNotMsgId('This should be ignored too.', po_contents)\n"], "sample_709": ["compilation error", "def test_pytester_outcomes_deselected_with_skip(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.skip\n            pass\n\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1, skipped=1)\n    # If skipped is not passed, it is not checked at all.\n    result.assert_outcomes(passed=1)\n", "        def test_skipped():\n            pass\n", "def test_pytester_outcomes_deselected_by_xfail(pytester: Pytester) -> None:\n    p1 = pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.xfail\n            pass\n\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1, xfailed=1)\n", "def test_pytester_run_no_timeout(pytester: Pytester) -> None:\n    testfile = pytester.makepyfile(\"def test_no_timeout(): pass\")\n    assert pytester.runpytest_subprocess(testfile).ret == ExitCode.OK\n"], "sample_1024": ["def test_Integer_to_numpy():\n    from sympy.utilities.pytest import skip\n    from sympy.external import import_module\n    np = import_module('numpy')\n    if not np:\n        skip('numpy not installed. Abort numpy tests.')\n    assert np.allclose(np.array(Integer(1)), 1)\n    assert np.allclose(np.array(Integer(-1)), -1)\n    assert np.allclose(np.array(Integer(0)), 0)\n    a = np.array([Integer(1), Integer(2), Integer(3)])\n    assert np.allclose(a, [1, 2, 3])\n\n\n\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_794": ["def test_ridge_warm_start(solver):\n    rng = np.random.RandomState(0)\n    X = rng.randn(10, 5)\n    y = rng.randn(10)\n\n    ridge = Ridge(solver=solver, alpha=1.0, warm_start=True)\n\n    ridge.fit(X, y)\n    coef1 = ridge.coef_.copy()\n\n    ridge.fit(X, y)  # second fit with warm_start\n\n    coef2 = ridge.coef_\n\n    assert_allclose(coef1, coef2)\n", "compilation error", "def test_ridge_classifier_predict_proba():\n    X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],\n                  [1.0, 1.0], [1.0, 0.0]])\n    y = np.array([1, 1, 1, -1, -1])\n\n    reg = RidgeClassifier(alpha=1.0)\n    reg.fit(X, y)\n\n    # Test predict_proba for binary classification\n    proba = reg.predict_proba(X)\n    assert proba.shape == (5, 2)\n    assert_allclose(proba.sum(axis=1), np.ones(5))\n\n    # Test predict_proba raises error for multiclass\n    reg = RidgeClassifier(alpha=1.0, solver='svd')\n    with pytest.raises(ValueError):\n        reg.fit(X, np.array([1, 1, 1, 2, 2]))\n", "def test_ridgecv_max_iter():\n    X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],\n                  [1.0, 1.0], [1.0, 0.0]])\n    y = [1, 1, 1, -1, -1]\n    ridgecv = RidgeCV(alphas=[0.1, 1.0, 10.0], cv=5, max_iter=2)\n    ridgecv.fit(X, y)\n    assert ridgecv.n_iter_ is not None\n", "def test_ridge_solver_auto_choice():\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 6, 5\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples)\n    alpha = 1.0\n\n    ridge = Ridge(alpha=alpha, solver='auto')\n    ridge.fit(X, y)\n    assert ridge.solver_ in ('svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga')\n"], "sample_1027": ["def test_issue_14499():\n    # test for issue 14499 - incorrect simplification of Poly objects\n    p = Poly(x**2 + 2*x + 1, x)\n    p = p.subs(x, x + 1)\n    assert p == Poly(x**2 + 4*x + 4, x)\n", "compilation error", "compilation error", "def test_issue_15669():\n    f = (x+y)**2\n    assert f.expand(force=True) == x**2 + 2*x*y + y**2\n    assert poly(f).expand(force=True) == Poly(x**2 + 2*x*y + y**2, x, y)\n", "compilation error"], "sample_519": ["    def test_savefig_kwargs():\n        fig, ax = plt.subplots()\n        with pytest.raises(ValueError):\n            fig.savefig(\"test.png\", dpi='hello')\n", "    def test_subplot_tool():\n        fig, ax = plt.subplots()\n        fig.canvas.draw()\n        toolitems = fig.canvas.toolbar.toolitems\n\n        # check that the default tool is 'home'\n        assert toolitems['home'].get_label() == 'Home'\n\n        # check that the tool can be set to 'pan'\n        toolitems['pan'].trigger(fig.canvas, None)\n        assert fig.canvas.toolbar._active == 'PAN'\n", "    def test_unpickle_with_constrained_layout(recwarn):\n        fig, ax = plt.subplots(constrained_layout=True)\n        ax.set_xlabel('x-label')\n        ax.set_ylabel('y-label')\n        fig2 = pickle.loads(pickle.dumps(fig))\n        assert fig2.axes[0].get_xlabel() == 'x-label'\n        assert fig2.axes[0].get_ylabel() == 'y-label'\n", "compilation error", "compilation error"], "sample_1089": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_725": ["compilation error", "compilation error", "compilation error", "compilation error", "    def test_check_array_dtype_object_with_pandas(self):\n        # Test that data-frame like objects with dtype object and a mix of\n        # numeric and string types get converted\n        X = np.array([[1, 'a'], [2, 'b'], [3.0, 'c']], dtype=np.object)\n        X_df = MockDataFrame(X)\n        assert_equal(check_array(X_df).dtype.kind, 'f')\n        assert_equal(check_array(X_df, ensure_2d=False).dtype.kind, 'f')\n        # smoke-test against dataframes with column named \"dtype\"\n        X_df.dtype = \"Hans\"\n        assert_equal(check_array(X_df, ensure_2d=False).dtype.kind, 'f')\n"], "sample_542": ["    def test_text_alignment_vertical():\n        fig, ax = plt.subplots()\n\n        text = ax.text(0.5, 0.5, 'text', ha='center', va='center',\n                       bbox=dict(facecolor='red'))\n\n        # Get the bounding box of the text in pixels\n        bbox = text.get_window_extent(fig.canvas.get_renderer())\n\n        # Check that the text is centered both horizontally and vertically\n        assert bbox.x0 < 0.5 * fig.bbox.width < bbox.x1\n        assert bbox.y0 < 0.5 * fig.bbox.height < bbox.y1\n", "    def test_text_with_unicode_emoji():\n        fig, ax = plt.subplots()\n        ax.text(0.5, 0.5, \"Hello \ud83c\udf0e !\", ha=\"center\", va=\"center\")\n        fig.canvas.draw()\n", "    def test_text_get_window_extent_with_transform():\n        fig, ax = plt.subplots()\n        t = ax.text(0.5, 0.5, \"test\", transform=ax.transData)\n        renderer = fig.canvas.get_renderer()\n        bbox = t.get_window_extent(renderer)\n        assert bbox.x0 == bbox.x1\n        assert bbox.y0 == bbox.y1\n", "    def test_text_invalid_font_family(self):\n        with pytest.raises(ValueError):\n            plt.text(0.5, 0.5, 'foo', fontfamily='InvalidFontFamily')\n", "    def test_text_with_non_ascii_chars():\n        plt.figure()\n        text = plt.text(0.5, 0.5, \"Hello, \u4e16\u754c!\", ha='center', va='center')\n        fig.canvas.draw()\n        bbox = text.get_window_extent(fig.canvas.get_renderer())\n        assert bbox.width > 0\n        assert bbox.height > 0\n"], "sample_48": ["    def test_aggregate_with_f_expression(self):\n        qs = Publisher.objects.annotate(\n            max_price_or_rating=Greatest(Avg('book__price'), Avg('book__rating'))\n        ).order_by('-max_price_or_rating')\n        self.assertEqual(\n            qs[0].max_price_or_rating,\n            Approximate(Decimal(\"75.00\"), places=2)\n        )\n", "    def test_aggregate_non_existing_field(self):\n        with self.assertRaisesMessage(FieldError, \"Cannot resolve keyword 'non_existing_field' into field. Choices are: age, contact, friends, id, isbn, name, pages, price, publisher, pubdate, rating\"):\n            Book.objects.values('rating').aggregate(avg_non_existing_field=Avg('non_existing_field'))\n", "    def test_annotate_aggregate_on_values(self):\n        qs = Book.objects.values('rating').annotate(\n            count=Count('id'), avg_price=Avg('price')).filter(count__gt=1)\n        for book in qs:\n            self.assertGreater(book['count'], 1)\n\n", "    def test_aggregate_with_filter(self):\n        qs = Book.objects.filter(rating__gt=4).aggregate(\n            avg_price=Avg('price'), count=Count('*'))\n        self.assertEqual(qs['avg_price'], Approximate(Decimal('47.39'), places=2))\n        self.assertEqual(qs['count'], 3)\n", "    def test_filter_after_annotate_with_order_by(self):\n        qs = Book.objects.annotate(\n            price_plus_rating = F('price') + F('rating')\n        ).filter(price_plus_rating__gt=50).order_by('price_plus_rating')\n        self.assertEqual(len(qs), 3)\n\n"], "sample_9": ["    def test_write_table_html_html_entities():\n        \"\"\"\n        Test that HTML entities are correctly encoded when writing to HTML.\n\n        \"\"\"\n        from astropy.io.ascii import html\n        t = Table([['<script>x</script>'], ['&']], names=['a'])\n        expected = '<tr><td>&lt;script&gt;x&lt;/script&gt;</td></tr><tr><td>&amp;</td></tr>'\n        with StringIO() as out:\n            html.write(t, out)\n\n        assert expected in out.getvalue()\n", "    def test_write_html_table_empty():\n        \"\"\"\n        Test writing empty tables as html\n        \"\"\"\n        t = Table()\n\n        expected = \"<html>\\n <head>\\n  <meta charset=\\\"utf-8\\\"/>\\n  <meta content=\\\"text/html;charset=UTF-8\\\" http-equiv=\\\"Content-type\\\"/>\\n </head>\\n <body>\\n  <table>\\n   <thead>\\n   </thead>\\n   <tbody>\\n   </tbody>\\n  </table>\\n </body>\\n</html>\"\n        with StringIO() as sp:\n            t.write(sp, format=\"html\")\n            out = sp.getvalue().strip()\n        assert out == expected.strip()\n\n", "    def test_write_html_table_with_empty_rows():\n        \"\"\"\n        Test to ensure that empty rows are handled correctly when writing to HTML\n        \"\"\"\n        t = Table([[1, 2], [], [3, 4]], names=('a', 'b'))\n\n        expected = \"\"\"\\", "def test_write_table_html_escaped():\n    \"\"\"\n    Test that characters which have special meaning in HTML are correctly escaped when \n    written to HTML.\n    \"\"\"\n    t = Table([[ '<' , '>' ], [ '&' , '\"' ]], names=('a', 'b'))\n\n    buffer_output = StringIO()\n    ascii.write(t, buffer_output, format='html')\n\n    t_expected = Table([['&lt;' , '&gt;' ], [ '&amp' , '&quot;' ]], names=('a', 'b'))\n    buffer_expected = StringIO()\n    ascii.write(t_expected, buffer_expected, format='html')\n\n    assert buffer_output.getvalue() == buffer_expected.getvalue()\n\n", "    def test_write_table_html_empty():\n        \"\"\"\n        Test writing an empty table to HTML\n        \"\"\"\n        t = Table(dtype=('i4', 'i8'))\n        buffer_output = StringIO()\n        ascii.write(t, buffer_output, format='html')\n        assert buffer_output.getvalue() == '<html>\\n <head>\\n  <meta charset=\"utf-8\"/>\\n  <meta content=\"text/html;charset=UTF-8\" http-equiv=\"Content-type\"/>\\n </head>\\n <body>\\n  <table>\\n   <thead>\\n    <tr>\\n    </tr>\\n   </thead>\\n   <tbody>\\n   </tbody>\\n  </table>\\n </body>\\n</html>'\n\n"], "sample_734": ["def test_fowlkes_mallows_score_empty():\n    assert_equal(fowlkes_mallows_score([], []), 1.0)\n", "    def test_fowlkes_mallows_score_empty():\n        # Check behavior with empty label lists\n        score = fowlkes_mallows_score([], [])\n        assert_equal(score, 1.0)\n", "def test_fowlkes_mallows_score_empty_labels():\n    # test for empty labels\n\n    assert_equal(fowlkes_mallows_score([], []), 0)\n    assert_equal(fowlkes_mallows_score([0, 0], []), 0)\n    assert_equal(fowlkes_mallows_score([], [1, 1]), 0)\n", "    def test_fowlkes_mallows_score_empty_labels():\n        # Test for empty label arrays\n        labels_a = np.array([])\n        labels_b = np.array([])\n        assert fowlkes_mallows_score(labels_a, labels_b) == 0.0\n", "def test_sparse_input():\n    # Test sparse input for contingency matrix calculation\n    labels_a = np.array([0, 1, 2, 0, 1, 2])\n    labels_b = np.array([1, 2, 0, 1, 2, 0])\n    labels_a_sparse = scipy.sparse.csr_matrix(labels_a)\n    labels_b_sparse = scipy.sparse.csr_matrix(labels_b)\n    C = contingency_matrix(labels_a, labels_b)\n    C_sparse = contingency_matrix(labels_a_sparse, labels_b_sparse)\n    assert_array_almost_equal(C, C_sparse.toarray())\n"], "sample_489": ["    def test_update_conflicts_unique_fields_update_fields_db_column_and_field(self):\n        FieldsWithDbColumns.objects.bulk_create(\n            [\n                FieldsWithDbColumns(rank=1, name=\"a\"),\n                FieldsWithDbColumns(rank=2, name=\"b\"),\n            ]\n        )\n        self.assertEqual(FieldsWithDbColumns.objects.count(), 2)\n\n        conflicting_objects = [\n            FieldsWithDbColumns(rank=1, name=\"c\", extra_field=\"x\"),\n            FieldsWithDbColumns(rank=2, name=\"d\", extra_field=\"y\"),\n        ]\n        results = FieldsWithDbColumns.objects.bulk_create(\n            conflicting_objects,\n            update_conflicts=True,\n            unique_fields=[\"rank\"],\n            update_fields=[\"name\", \"extra_field\"],\n        )\n        self.assertEqual(len(results), len(conflicting_objects))\n        if connection.features.can_return_rows_from_bulk_insert:\n            for instance in results:\n                self.assertIsNotNone(instance.pk)\n        self.assertEqual(FieldsWithDbColumns.objects.count(), 2)\n        self.assertCountEqual(\n            FieldsWithDbColumns.objects.values(\"rank\", \"name\", \"extra_field\"),\n            [\n                {\"rank\": 1, \"name\": \"c\", \"extra_field\": \"x\"},\n                {\"rank\": 2, \"name\": \"d\", \"extra_field\": \"y\"},\n            ],\n        )\n", "    def test_update_conflicts_unique_fields_pk_and_field(self):\n        TwoFields.objects.bulk_create(\n            [\n                TwoFields(f1=1, f2=1, name=\"a\"),\n                TwoFields(f1=2, f2=2, name=\"b\"),\n            ]\n        )\n\n        obj1 = TwoFields.objects.get(f1=1)\n        obj2 = TwoFields.objects.get(f1=2)\n        conflicting_objects = [\n            TwoFields(pk=obj1.pk, f1=3, f2=3, name=\"c\"),\n            TwoFields(pk=obj2.pk, f1=4, f2=4, name=\"d\"),\n        ]\n        results = TwoFields.objects.bulk_create(\n            conflicting_objects,\n            update_conflicts=True,\n            unique_fields=[\"pk\", \"f1\"],\n            update_fields=[\"name\"],\n        )\n        self.assertEqual(len(results), len(conflicting_objects))\n        if connection.features.can_return_rows_from_bulk_insert:\n            for instance in results:\n                self.assertIsNotNone(instance.pk)\n        self.assertEqual(TwoFields.objects.count(), 2)\n        self.assertCountEqual(\n            TwoFields.objects.values(\"f1\", \"f2\", \"name\"),\n            [\n                {\"f1\": 1, \"f2\": 1, \"name\": \"c\"},\n                {\"f1\": 2, \"f2\": 2, \"name\": \"d\"},\n            ],\n        )\n\n", "    def test_update_conflicts_unique_fields_with_auto_field(self):\n        AutoFields.objects.bulk_create(\n            [\n                AutoFields(name=\"a\"),\n                AutoFields(name=\"b\"),\n            ]\n        )\n        self.assertEqual(AutoFields.objects.count(), 2)\n\n        conflicting_objects = [\n            AutoFields(name=\"c\"),\n            AutoFields(name=\"d\"),\n        ]\n        results = AutoFields.objects.bulk_create(\n            conflicting_objects,\n            update_conflicts=True,\n            unique_fields=[\"id\"],\n            update_fields=[\"name\"],\n        )\n        self.assertEqual(len(results), len(conflicting_objects))\n        if connection.features.can_return_rows_from_bulk_insert:\n            for instance in results:\n                self.assertIsNotNone(instance.pk)\n        self.assertEqual(AutoFields.objects.count(), 2)\n        self.assertCountEqual(\n            AutoFields.objects.values(\"id\", \"name\"),\n            [\n                {\"id\": 1, \"name\": \"c\"},\n                {\"id\": 2, \"name\": \"d\"},\n            ],\n        )\n", "    def test_update_conflicts_pk_in_unique_fields(self):\n        BigAutoFieldModel.objects.bulk_create(\n            [\n                BigAutoFieldModel(name=\"a\"),\n                BigAutoFieldModel(name=\"b\"),\n            ]\n        )\n        self.assertEqual(BigAutoFieldModel.objects.count(), 2)\n\n        conflicting_objects = [\n            BigAutoFieldModel(pk=1, name=\"c\"),\n            BigAutoFieldModel(pk=2, name=\"d\"),\n        ]\n        results = BigAutoFieldModel.objects.bulk_create(\n            conflicting_objects,\n            update_conflicts=True,\n            unique_fields=[\"pk\"],\n            update_fields=[\"name\"],\n        )\n        self.assertEqual(len(results), len(conflicting_objects))\n        if connection.features.can_return_rows_from_bulk_insert:\n            for instance in results:\n                self.assertIsNotNone(instance.pk)\n        self.assertEqual(BigAutoFieldModel.objects.count(), 2)\n        self.assertCountEqual(\n            BigAutoFieldModel.objects.values(\"id\", \"name\"),\n            [\n                {\"id\": 1, \"name\": \"c\"},\n                {\"id\": 2, \"name\": \"d\"},\n            ],\n        )\n", "    def test_update_conflicts_unique_fields_multiple_unique_fields(self):\n        self._test_update_conflicts(unique_fields=[\"number\", \"rank\"])\n"], "sample_771": ["compilation error", "    def test_quantile_transform_n_quantiles(self):\n        rng = np.random.RandomState(42)\n        X = rng.rand(10, 5)\n        for n_quantiles in [10, 100, 1000]:\n            X_trans = quantile_transform(X, n_quantiles=n_quantiles)\n            assert X_trans.shape == X.shape\n            for col in range(X.shape[1]):\n                quantiles = np.linspace(0, 1, n_quantiles)\n                assert_allclose(np.unique(X_trans[:, col]), quantiles, atol=1e-5)\n", "compilation error", "compilation error", "    def test_power_transformer_sparse(self, method, standardize):\n        X = sparse.csc_matrix(X_2d)\n        if method == 'box-cox':\n            X = np.abs(X)\n\n        pt = PowerTransformer(method=method, standardize=standardize)\n        pt.fit(X)\n        X_trans = pt.transform(X)\n\n        assert sparse.isspmatrix_csc(X_trans)\n        assert_array_almost_equal(pt.inverse_transform(X_trans), X.toarray())\n"], "sample_982": ["compilation error", "def test_isprime():\n    assert isprime(2) is True\n    assert isprime(3) is True\n    assert isprime(4) is False\n    assert isprime(5) is True\n    assert isprime(-2) is False\n    assert isprime(1) is False\n    assert isprime(2**31 - 1) is True\n    n = Symbol('n', integer=True)\n    assert isprime(n)\n    assert isprime(n).subs(n, 2**31 - 1) is True\n    assert summation(isprime(n), (n, 2, 10)) == 4\n\n\n\n", "compilation error", "compilation error", "    def test_isprime():\n        assert isprime(2) == True\n        assert isprime(3) == True\n        assert isprime(4) == False\n        assert isprime(5) == True\n        assert isprime(6) == False\n        assert isprime(7) == True\n        assert isprime(10) == False\n        assert isprime(11) == True\n        assert isprime(12) == False\n        assert isprime(13) == True\n        assert isprime(17) == True\n        assert isprime(19) == True\n        assert isprime(23) == True\n        assert isprime(29) == True\n        assert isprime(31) == True\n        assert isprime(37) == True\n        assert isprime(41) == True\n        assert isprime(43) == True\n        assert isprime(47) == True\n        assert isprime(53) == True\n        assert isprime(59) == True\n        assert isprime(61) == True\n        assert isprime(67) == True\n        assert isprime(71) == True\n        assert isprime(73) == True\n        assert isprime(79) == True\n        assert isprime(83) == True\n        assert isprime(89) == True\n        assert isprime(97) == True\n        assert isprime(101) == True\n        assert isprime(103) == True\n        assert isprime(107) == True\n        assert isprime(109) == True\n        assert isprime(113) == True\n        assert isprime(127) == True\n        assert isprime(131) == True\n        assert isprime(137) == True\n        assert isprime(139) == True\n\n        assert isprime(-1) == False\n        assert isprime(-2) == False\n        assert isprime(0) == False\n        assert isprime(1) == False\n\n\n"], "sample_68": ["    def test_cleanse_setting_cleanses_nested_dictionary(self):\n        initial = {'outer': {'password': 'secret', 'other': 'value'}}\n        expected = {'outer': {'password': CLEANSED_SUBSTITUTE, 'other': 'value'}}\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected)\n", "    def test_cleanse_setting_recurses_in_list(self):\n        initial = ['username', {'login': 'cooper', 'password': 'secret'}]\n        expected = ['username', {'login': 'cooper', 'password': CLEANSED_SUBSTITUTE}]\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected)\n", "    def test_cleanse_setting_recurses_in_list(self):\n        initial = ['login', 'password']\n        expected = ['login', CLEANSED_SUBSTITUTE]\n        settings = {'SETTING_NAME': initial}\n        self.assertEqual(cleanse_setting('SETTING_NAME', settings), {'SETTING_NAME': expected})\n", "    def test_cleanse_setting_nested_dicts(self):\n        initial = {'login': 'cooper', \n                   'credentials': {'password': 'secret', 'token': '123'}}\n        expected = {'login': 'cooper', \n                   'credentials': {'password': CLEANSED_SUBSTITUTE, 'token': '123'}}\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected)\n\n", "    def test_cleanse_setting_handles_nested_lists(self):\n        initial = {'user': {'name': 'Jane', 'credentials': ['username', 'password123']}}\n        expected = {'user': {'name': 'Jane', 'credentials': ['username', CLEANSED_SUBSTITUTE]}}\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected)\n\n"], "sample_357": ["    def test_foreign_key_field_addition(self):\n        changes = self.get_changes([self.author_empty], [self.author_with_book_fk])\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['AddField'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='book')\n", "    def test_unique_together_constraint_change(self):\n        \"\"\"\n        #25470 - Ensure that alterations to UniqueTogether constraints are properly detected.\n        \"\"\"\n        model_state_before = ModelState(\n            'testapp',\n            'Author',\n            [\n                ('id', models.AutoField(primary_key=True)),\n                ('name', models.CharField(max_length=200)),\n                ('email', models.EmailField()),\n            ],\n            options={'unique_together': [('name', 'email')]},\n        )\n        model_state_after = ModelState(\n            'testapp',\n            'Author',\n            [\n                ('id', models.AutoField(primary_key=True)),\n                ('name', models.CharField(max_length=200)),\n                ('email', models.EmailField()),\n                ('bio', models.TextField()),\n            ],\n            options={'unique_together': [('name', 'bio', 'email')]},\n        )\n        changes = self.get_changes([model_state_before], [model_state_after])\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['AlterUniqueTogether'])\n", "    def test_complex_model_rename_operation(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.RenameModel('OldModelName', new_name='NewModelName'), \n            ]\n        migration = Migration('0001_rename_oldmodelname', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'rename_oldmodelname_to_newmodelname')\n", "    def test_operation_with_custom_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[], name='custom_person_creation'),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'custom_person_creation')\n", "    def test_existing_migration_names(self):\n        class Migration(migrations.Migration):\n            operations = [migrations.CreateModel('Person', fields=[])]\n\n        migration = Migration('0002_person', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'person')\n"], "sample_95": ["    def test_cache_control_arguments(self):\n        @cache_control(public=True, max_age=3600)\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(r['Cache-Control'], 'max-age=3600, public')\n\n", "    def test_cache_control_decorator(self):\n            return HttpResponse(\"response\")\n        my_view_cache_controlled = cache_control(max_age=60)(my_view)\n        response = my_view_cache_controlled(HttpRequest())\n        self.assertEqual(response['Cache-Control'], 'max-age=60')\n", "    def test_vary_on_headers_decorator(self):\n        \"\"\"\n        Ensures @vary_on_headers properly sets the Vary header.\n        \"\"\"\n        @vary_on_headers('Accept-Language', 'User-Agent')\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(r['Vary'], 'Accept-Language, User-Agent')\n", "    def test_cache_control_decorator(self):\n        @cache_control(max_age=3600)\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(r['Cache-Control'], 'max-age=3600')\n", "    def test_cache_control_decorator(self):\n        @cache_control(private=True, max_age=3600)\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(\n            set(r['Cache-Control'].split(', ')),\n            {'private', 'max-age=3600'},\n        )\n\n"], "sample_687": ["def test_empty_capture(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            assert not caplog.records\n            assert not caplog.text\n\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_log_report_captures_according_to_config_option_upon_success(testdir):\n    \"\"\"Test that upon success:\n    (1) `caplog` succeeded to capture the DEBUG message and assert on it => No `Exception` is raised.\n    (2) The `DEBUG` message appears in the `Captured log call` report due to `--log-level=DEBUG`.\n    (3) The stdout, `INFO`, and `WARNING` messages DO appear in the test reports.\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n            logging.debug('DEBUG log ' + 'message')\n            logging.info('INFO log ' + 'message')\n            logging.warning('WARNING log ' + 'message')\n            print('Print ' + 'message')\n\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_level == logging.DEBUG\n\n            with caplog.at_level(logging.DEBUG):\n                function_that_logs()\n\n            if 'DEBUG log ' + 'message' not in caplog.text:\n                raise Exception('caplog failed to ' + 'capture DEBUG')\n            assert True\n    \"\"\"\n    )\n\n    result = testdir.runpytest(\"--log-level=DEBUG\")\n    result.stdout.no_fnmatch_line(\"*Exception: caplog failed to capture DEBUG*\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*DEBUG log message*\",\n            \"*Print message*\",\n            \"*INFO log message*\",\n            \"*WARNING log message*\",\n        ]\n    )\n    assert result.ret == 0\n\n", "def test_log_file_captures_according_to_config_option(testdir, tmpdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.info('INFO log ' + 'message')\n            logging.warning('WARNING log ' + 'message')\n    \"\"\"\n    )\n\n    log_file = tmpdir.join(\"pytest.log\")\n    testdir.makeini(\n        f\"\"\"\n        [pytest]\n        log_file = {log_file}\n        log_file_level = WARNING\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    assert result.ret == 0\n    with open(str(log_file), \"r\") as f:\n        content = f.read()\n    assert \"INFO log message\" not in content\n    assert \"WARNING log message\" in content\n\n", "def test_log_config_overrides_pytest_log_level(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            logger = logging.getLogger('catchlog')\n            logger.info(\"INFO message won't be shown because log_level is set\")\n            assert 'INFO message won't be shown because log_level is set' not in caplog.text\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_level=DEBUG\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    assert result.ret == 0\n\n", "def test_log_report_captures_according_to_config_option_upon_pass(testdir):\n    \"\"\"Test that upon success:\n    (1) `caplog` succeeds to capture the DEBUG message and assert on it => No `Exception` is raised.\n    (2) The `DEBUG` message DOES appear in the `Captured log call` report.\n    (3) The stdout, `INFO`, and `WARNING` messages DO appear in the test reports due to `--log-level=INFO`.\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n            logging.debug('DEBUG log ' + 'message')\n            logging.info('INFO log ' + 'message')\n            logging.warning('WARNING log ' + 'message')\n            print('Print ' + 'message')\n\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_level == logging.INFO\n\n            with caplog.at_level(logging.DEBUG):\n                function_that_logs()\n\n            if 'DEBUG log ' + 'message' not in caplog.text:\n                raise Exception('caplog failed to ' + 'capture DEBUG')\n\n            assert True \n    \"\"\"\n    )\n\n    result = testdir.runpytest(\"--log-level=INFO\")\n    result.stdout.no_fnmatch_line(\"*Exception: caplog failed to capture DEBUG*\")\n    result.stdout.fnmatch_lines(\n        [\"*DEBUG log message*\", \"*Print message*\", \"*INFO log message*\", \"*WARNING log message*\"]\n    )\n    assert result.ret == 0\n\n"], "sample_850": ["    def test_nystroem_sparse_input():\n        # test Nystroem on sparse input data\n        rnd = np.random.RandomState(42)\n        X = rnd.uniform(size=(10, 4))\n        X_sparse = csr_matrix(X)\n\n        # With n_components = n_samples this is exact\n        nystroem = Nystroem(n_components=X.shape[0])\n        X_transformed = nystroem.fit_transform(X_sparse)\n        K = rbf_kernel(X)\n        assert_array_almost_equal(np.dot(X_transformed, X_transformed.T), K)\n\n", "def test_nystroem_sparse_input():\n    # test Nystroem with sparse input\n    rnd = np.random.RandomState(0)\n    X = rnd.uniform(size=(10, 4))\n    X_sparse = csr_matrix(X)\n\n    # With n_components = n_samples this is exact\n    nystroem = Nystroem(n_components=X.shape[0])\n    X_transformed = nystroem.fit_transform(X_sparse)\n    K = rbf_kernel(X)\n    assert_array_almost_equal(np.dot(X_transformed, X_transformed.T), K)\n\n    # test callable kernel\n        return np.dot(X, Y.T)\n    trans = Nystroem(n_components=2, kernel=linear_kernel)\n    X_transformed = trans.fit(X_sparse).transform(X_sparse)\n\n", "compilation error", "compilation error", "    def test_nystroem_error_handling():\n        X = [[1, 2], [3, 4]]\n        # Test error handling for incorrect input types\n        with pytest.raises(TypeError):\n            Nystroem().fit(X, y=\"wrong_type\")\n\n        # Test error handling for insufficient samples\n        with pytest.raises(ValueError):\n            Nystroem(n_components=3).fit(X)\n"], "sample_301": ["    def test_watch_dir_with_duplicate_globs(self):\n        self.reloader.watch_dir(self.tempdir, '*.py')\n        self.reloader.watch_dir(self.tempdir, '**/*.py')\n        watched_files = list(self.reloader.watched_files())\n        self.assertIn(self.existing_file, watched_files)\n", "    def test_watch_dir_recursive_with_existing_file(self):\n        inner_file = self.ensure_file(self.tempdir / 'dir' / 'test.py')\n        self.reloader.watch_dir(self.tempdir, '**/*.py')\n        with self.tick_twice():\n            self.increment_mtime(inner_file)\n        self.assertEqual(self.reloader.notify_file_changed.call_count, 1)\n        self.assertCountEqual(self.reloader.notify_file_changed.call_args[0], [inner_file])\n", "    def test_should_stop_when_reloader_is_stopping(self):\n        self.reloader.stop()\n        with self.tick_twice():\n            self.increment_mtime(self.existing_file)\n\n        self.assertFalse(self.reloader.should_stop())\n", "    def test_should_stop_with_error(self):\n        with mock.patch.object(self.reloader, 'notify_file_changed'):\n            self.reloader.should_stop = True\n            self.reloader.notify_file_changed(self.existing_file)\n            self.assertTrue(self.reloader.should_stop)\n", "    def test_run_loop_with_exception(self):\n            yield\n            raise Exception('Test Exception')\n\n        with mock.patch.object(self.reloader, 'tick', side_effect=mocked_tick) as tick:\n            with self.assertRaises(Exception) as cm:\n                self.reloader.run_loop()\n            self.assertEqual(cm.exception.args[0], 'Test Exception')\n            self.assertEqual(tick.call_count, 1)\n"], "sample_1020": ["def test_list_arguments():\n    assert mcode([sin(x), cos(x)]) == \"{Sin[x], Cos[x]}\"\n    assert mcode([x, y, z]) == \"{x, y, z}\"\n    assert mcode([[1, 2], [3, 4]]) == \"{{1, 2}, {3, 4}}\"\n", "def test_special_functions():\n    assert mcode(exp(x)) == \"Exp[x]\"\n    assert mcode(log(x)) == \"Log[x]\"\n    assert mcode(sin(x)) == \"Sin[x]\"\n    assert mcode(cos(x)) == \"Cos[x]\"\n\n", "def test_user_functions():\n    uprintf = MCodePrinter()\n    uprintf.known_functions[\"foo\"] = [(lambda *x: True, \"bar\")]\n    assert uprintf.doprint(foo(x)) == \"bar[x]\"\n", "def test_Complex():\n    assert mcode(1 + 2*I) == \"1 + 2*I\"\n    assert mcode(x + y*I) == \"x + I*y\"\n", "def test_user_functions():\n    from sympy import mathematica_code as mcode\n    add = symbols('add')\n    mcode.known_functions.update({add: [(lambda *x: True, 'Plus')]})\n    assert mcode(add(x, y)) == 'Plus[x, y]'\n"], "sample_356": ["    def test_circular_dependency_swappable_self_multi_mti(self):\n        \"\"\"\n        #23322 - The dependency resolver knows to explicitly resolve\n        swappable models.\n        \"\"\"\n        with isolate_lru_cache(apps.get_swappable_settings_name):\n            user = ModelState('a', 'User', [], bases=(AbstractBaseUser, 'a.Person'))\n            person = ModelState('a', 'Person', [\n                ('id', models.AutoField(primary_key=True)),\n                ('parent1', models.ForeignKey(settings.AUTH_USER_MODEL, models.CASCADE, related_name='children'))\n            ])\n            changes = self.get_changes([], [user, person])\n        self.assertNumberMigrations(changes, 'a', 1)\n        self.assertOperationTypes(changes, 'a', 0, ['CreateModel', 'CreateModel', 'AddField'])\n\n", "    def test_squashed_migration(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[]),\n                migrations.CreateModel('Animal', fields=[]),\n            ]\n\n        migration = Migration('0002_squashed_0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'person_animal')\n", "    def test_alter_unique_together(self):\n        \"\"\"\n        #24262 - AlterUniqueTogether should suggest a name based on the fields.\n\n        \"\"\"\n        before = ModelState('app', 'model', [\n            ('id', models.AutoField(primary_key=True)),\n            ('field1', models.CharField(max_length=255)),\n            ('field2', models.CharField(max_length=255)),\n        ])\n        after = ModelState('app', 'Model', [\n            ('id', models.AutoField(primary_key=True)),\n            ('field1', models.CharField(max_length=255)),\n            ('field2', models.CharField(max_length=255)),\n        ], options={'unique_together': (('field1', 'field2'),)})\n        changes = self.get_changes([before], [after])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['AlterUniqueTogether'])\n        self.assertEqual(changes['app'][0].operations[0].name, 'unique_field1_field2')\n", "    def test_operation_with_custom_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[], name='my_person'),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'my_person')\n", "    def test_custom_operation_name(self):\n        class CustomOperation(migrations.MigrationOperation):\n                super().__init__(**kwargs)\n                self.custom_name = name\n\n                return f'Custom Operation: {self.custom_name}'\n\n        class Migration(migrations.Migration):\n            operations = [\n                CustomOperation('my_custom_operation'),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'my_custom_operation')\n"], "sample_1182": ["compilation error", "def test_issue_21278():\n    from sympy.functions.elementary.piecewise import Piecewise\n    from sympy.core.symbol import Symbol\n    x = Symbol('x')\n\n    expr = Piecewise((0, x < 0), (x, x >= 0))\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == '((0) if (x < 0) else (x) if (x >= 0) else None)'\n\n", "def test_issue_20762_2():\n    antlr4 = import_module(\"antlr4\")\n    if not antlr4:\n        skip('antlr not installed.')\n    # Make sure pycode does not insert extra spaces around subscripts\n    expr = parse_latex(r'a_{b}')\n    assert pycode(expr) == 'a_b'\n    expr = parse_latex(r'a_{b_c}')\n    assert pycode(expr) == 'a_{b_c}'\n", "compilation error", "compilation error"], "sample_258": ["    def test_receiver_weakref_sender(self):\n        class Sender:\n            pass\n\n        sender = Sender()\n\n            self.sender_ref = weakref.ref(sender)\n        \n        @receiver(a_signal, sender=sender)\n            self.sender_ref = weakref.ref(sender)\n\n        a_signal.connect(receiver_func, sender=sender)\n        receiver_with_decorator(sender=sender)\n        del sender\n\n        garbage_collect()\n\n        self.assertIsNone(self.sender_ref())\n\n\n\n", "    def test_receiver_weak_ref(self):\n        class MyCallable:\n                return val\n        receiver_obj = MyCallable()\n        @receiver(a_signal, weak=True)\n            return receiver_obj(val, **kwargs)\n        result = a_signal.send(sender=self, val='test')\n        self.assertEqual(result[0][1], 'test')\n        del receiver_obj\n        garbage_collect()\n        result = a_signal.send(sender=self, val='test')\n        self.assertEqual(result, []) \n", "    def test_receiver_kwargs(self):\n        @receiver(a_signal)\n            self.assertEqual(sender, self)\n            self.assertEqual(val, 'test')\n            self.assertEqual(kwarg1, 'kwarg1')\n        a_signal.send(sender=self, val='test', kwarg1='kwarg1')\n", "    def test_receiver_disconnect(self):\n        @receiver(a_signal)\n            self.state = val\n        self.state = False\n        a_signal.send(sender=self, val=True)\n        self.assertTrue(self.state)\n        a_signal.disconnect(f)\n        a_signal.send(sender=self, val=False)\n        self.assertTrue(self.state)\n", "    def test_receiver_disconnect(self):\n            pass\n        @receiver(a_signal)\n            self.received = val\n\n        self.received = False\n        a_signal.send(sender=self, val=True)\n        self.assertTrue(self.received)\n        receiver_disconnect = receiver_1_arg\n\n        a_signal.disconnect(receiver_disconnect)\n        a_signal.send(sender=self, val=True)\n        self.assertTrue(self.received)\n"], "sample_197": ["    def test_timeuntil_future_date_with_depth(self):\n        future_date = self.t + self.oneyear + 3 * self.onemonth + 2 * self.oneweek + 5 * self.oneday + 12 * self.onehour\n        self.assertEqual(\n            timeuntil(future_date, depth=2), '1\\xa0year, 3\\xa0months'\n        )\n        self.assertEqual(\n            timeuntil(future_date, depth=3), '1\\xa0year, 3\\xa0months, 2\\xa0weeks'\n        )\n        self.assertEqual(\n            timeuntil(future_date, depth=4), '1\\xa0year, 3\\xa0months, 2\\xa0weeks, 5\\xa0days'\n        )\n        self.assertEqual(\n            timeuntil(future_date, depth=5), '1\\xa0year, 3\\xa0months, 2\\xa0weeks, 5\\xa0days, 12\\xa0hours'\n        )\n", "    def test_timeuntil_with_future_date(self):\n        future = datetime.datetime(2024, 12, 31)\n        self.assertEqual(timeuntil(future), '4\\xa0years')\n", "    def test_timeuntil_with_future_date(self):\n        future = self.t + self.oneyear + self.onemonth + self.oneweek + self.oneday\n        self.assertEqual(timeuntil(future), '1\\xa0year, 1\\xa0month, 1\\xa0week, 1\\xa0day')\n", "    def test_timeuntil_depth(self):\n        t = self.t + self.oneyear + self.onemonth + self.oneweek + self.oneday + self.onehour\n        tests = [\n            (t, 1, '1\\xa0year'),\n            (t, 2, '1\\xa0year, 1\\xa0month'),\n            (t, 3, '1\\xa0year, 1\\xa0month, 1\\xa0week'),\n            (t, 4, '1\\xa0year, 1\\xa0month, 1\\xa0week, 1\\xa0day'),\n            (t, 5, '1\\xa0year, 1\\xa0month, 1\\xa0week, 1\\xa0day, 1\\xa0hour'),\n            (t, 6, '1\\xa0year, 1\\xa0month, 1\\xa0week, 1\\xa0day, 1\\xa0hour'),\n            (self.t + self.onehour, 5, '1\\xa0hour'),\n            (self.t + (4 * self.oneminute), 3, '4\\xa0minutes'),\n            (self.t + self.onehour + self.oneminute, 1, '1\\xa0hour'),\n            (self.t + self.oneday + self.onehour, 1, '1\\xa0day'),\n            (self.t + self.oneweek + self.oneday, 1, '1\\xa0week'),\n            (self.t + self.onemonth + self.oneweek, 1, '1\\xa0month'),\n            (self.t + self.oneyear + self.onemonth, 1, '1\\xa0year'),\n            (self.t + self.oneyear + self.oneweek + self.oneday, 3, '1\\xa0year'),\n        ]\n        for value, depth, expected in tests:\n            with self.subTest():\n                self.assertEqual(timeuntil(self.t, value, depth=depth), expected)\n", "    def test_timeuntil_leap_year(self):\n        start_date = datetime.date(2016, 12, 25)\n        self.assertEqual(timeuntil(start_date + self.oneweek, start_date), '1\\xa0week')\n        self.assertEqual(timeuntil(start_date, start_date + self.oneweek), '1\\xa0week')\n\n"], "sample_364": ["    def test_include_with_name_parameter(self):\n        match = resolve('/included_urls/name/something/')\n        self.assertEqual(match.url_name, 'inner-name')\n        self.assertEqual(match.kwargs, {'name': 'something'})\n", "    def test_path_inclusion_with_empty_string_inclusion_no_match(self):\n        with self.assertRaises(Resolver404):\n            resolve('/included_urls//')\n\n", "    def test_multiple_converters_in_one_url(self):\n        @DynamicConverter.register_to_url\n            return value.replace(' ', '-')\n\n        @DynamicConverter.register_to_python\n            return value.replace('-', ' ')\n\n        tests = [\n            ('dynamic/hello-world', {'slug': 'hello world'}),\n            ('dynamic/hello_world', {'slug': 'hello world'}),\n        ]\n        for url, kwargs in tests:\n            with self.subTest(url=url):\n                match = resolve(url)\n                self.assertEqual(match.url_name, 'dynamic')\n                self.assertEqual(match.kwargs, kwargs)\n\n                reversed_url = reverse('dynamic', kwargs=kwargs)\n                self.assertEqual(reversed_url, url)\n", "    def test_path_reverse_with_trailing_slash(self):\n        url = reverse('articles-2003', kwargs={'year': 2003})\n        self.assertEqual(url, '/articles/2003/')\n        url = reverse('articles-2003')\n        self.assertEqual(url, '/articles/2003/')\n", "    def test_path_with_duplicate_name(self):\n        with self.assertRaises(ImproperlyConfigured):\n            path('duplicate/', empty_view, name='duplicate')\n            path('duplicate/', empty_view, name='duplicate')\n"], "sample_366": ["    def test_parse_datetime_invalid_date(self):\n        with self.assertRaises(ValueError):\n            parse_datetime('2012-04-56T09:15:00')\n        with self.assertRaises(ValueError):\n            parse_datetime('2012-02-29T09:15:00')\n", "    def test_parse_datetime_invalid_format(self):\n        invalid_inputs = (\n            '2012-04-23T09:15:00Z+',\n            '2012-04-23T09 15:00',\n            '2012-04-23T09:15:00A',\n            '2012-4-9 4:8:16-032',\n            '2012-04-23T10:20:30.400+02:',\n            '2012-04-23T10:20:30,400+023',\n        )\n        for source in invalid_inputs:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_datetime(source))\n", "    def test_parse_datetime_naive(self):\n        with self.assertRaises(ValueError):\n            parse_datetime('2012-04-23 10:20:30')\n\n", "    def test_parse_duration_invalid(self):\n        test_values = (\n            '15',\n            '15:',\n            '15:30:',\n            ':30',\n            '15:30.a',\n            '15:30.123456789',\n            '15:30,123456789',\n            '15:30,1234567890',\n            '15:30 123456789',\n            'day',\n            '15d',\n            '15h',\n            '15m',\n            '15s',\n            '15.d',\n            '15.h',\n            '15.m',\n            '15.s',\n            \n        )\n        for value in test_values:\n            with self.subTest(value=value):\n                with self.assertRaises(ValueError):\n                    parse_duration(value)\n", "    def test_parse_datetime_iso8601(self):\n        test_values = (\n            ('2023-10-26T16:30:00Z', datetime(2023, 10, 26, 16, 30, tzinfo=get_fixed_timezone(0))),\n            ('2023-10-26T16:30:00+02:00', datetime(2023, 10, 26, 16, 30, tzinfo=get_fixed_timezone(120))),\n            ('20231026T163000Z', None),\n            ('2023-10-26T16:30:00.123456Z', datetime(2023, 10, 26, 16, 30, 0, 123456, tzinfo=get_fixed_timezone(0))),\n            ('2023-10-26T16:30:00.123456+02:00', datetime(2023, 10, 26, 16, 30, 0, 123456, tzinfo=get_fixed_timezone(120))),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_datetime(source), expected)\n"], "sample_1035": ["compilation error", "def test_measure_all():\n    nqubits = 2\n    qubit = superposition_basis(nqubits)\n    result = measure_all(qubit)\n\n    assert len(result) == 4\n    for state, prob in result:\n        assert isinstance(state, IntQubit)\n        assert 0 <= prob <= 1\n\n\n", "def test_measure_all():\n    from sympy.physics.quantum.qubit import measure_all\n    q = (IntQubit(0, 2) + IntQubit(3, 2))/sqrt(2)\n    result = measure_all(q)\n    assert len(result) == 4\n    assert result[0][0] == IntQubit(0, 2) \n    assert abs(result[0][1] - 1/2) < 1e-10\n    assert result[3][0] == IntQubit(3, 2)\n    assert abs(result[3][1] - 1/2) < 1e-10 \n\n\n\n", "compilation error", "compilation error"], "sample_671": ["    def test_module_level_skip_with_allow_module_level_and_reason(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            pytest.skip(\"skip_module_level\", allow_module_level=True, reason=\"custom reason\")\n\n                assert 0\n        \"\"\"\n        )\n        result = testdir.runpytest(\"-rxs\")\n        result.stdout.fnmatch_lines(\n            [\"*SKIP*custom reason*skip_module_level*\"]\n        )\n", "    def test_strict_xfail_condition_false(testdir):\n        p = testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.xfail(False, reason='unsupported feature', strict=True)\n                pass\n        \"\"\"\n        )\n        result = testdir.runpytest(p, \"-rxX\")\n        result.stdout.fnmatch_lines([\"*1 failed*\"])\n        assert result.ret == 1\n", "def test_importorskip_condition(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import sys\n\n        if sys.version_info < (3, 0):\n            pytest.importorskip(\"doesnotexist\")\n\n            pass\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\"SKIPPED [[]1[]] condition: sys.version_info < (3, 0)\"]\n    )\n\n", "    def test_importorskip_with_reason(testdir):\n        testdir.makepyfile(\n            test_one=\"\"\"\n            import pytest\n            pytest.importorskip(\"doesnotexist\", reason=\"This module is not essential for these tests\")\n                assert 1\n        \"\"\"\n        )\n        result = testdir.runpytest(\"-rs\")\n        result.stdout.fnmatch_lines(\n            [\n                \"*SKIPPED*reason: This module is not essential for these tests*\",\n            ]\n        )\n", "    def test_skip_within_class(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            class TestClass:\n                @pytest.mark.skip(reason=\"skipping this class\")\n                    assert True\n\n                    assert True\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines(\n            [\"*1 skipped*1 passed*\"]\n        )\n"], "sample_952": ["    def test_signature_from_str_complex_annotations_with_ellipsis():\n        sig = inspect.signature_from_str('(a: int, *args: Tuple[str, ...], b: str = \"...\") -> None')\n        assert sig.parameters['a'].annotation == \"int\"\n        assert sig.parameters['args'].annotation == \"Tuple[str, ...]\"\n        assert sig.parameters['b'].annotation == \"str\"\n        assert sig.return_annotation == 'None'\n", "        def __getitem__(self, key):\n            return super().__getitem__(key.lower())\n", "def test_getsource_non_existent(app):\n    from target.functions import func\n    from target.methods import Base\n\n    with pytest.raises(IOError):\n        inspect.getsource(Base, \"non_existent_method\")\n\n", "    def test_getdoc_with_signature():\n            pass\n        \n        assert inspect.getdoc(func) == \"\"\n\n        func.__doc__ = \"My docstring\"\n        assert inspect.getdoc(func) == \"My docstring\"\n\n           \n", "def test_getsource_with_partial(app):\n    from target.functions import partial_func\n\n    source = inspect.getsource(partial_func)\n    assert source.startswith('functools.partial(')\n\n"], "sample_953": ["def test_invalid_dot(tempdir):\n    answers = {\n        'Root path': tempdir,\n        'Separate source and build': 'y',\n        'Name prefix for templates': '.',\n        'Project name': 'Sphinx Test',\n        'Author name': 'Georg Brandl',\n    }\n    qs.term_input = mock_input(answers)\n    d = {}\n    qs.ask_user(d)\n    with pytest.raises(ValueError):\n        qs.generate(d)\n", "def test_quiet_mode_with_missing_args(tempdir, capsys):\n    answers = {\n        'Project name': 'Quiet Project',\n        'Author name': 'Quiet Author',\n    }\n    qs.term_input = mock_input(answers)\n    with pytest.raises(SystemExit) as excinfo:\n        qs.main(['--quiet', '-p', 'Quiet Project', '-a', 'Quiet Author', '-v', '0.1',\n                 tempdir])\n    assert excinfo.value.code == 1\n    captured = capsys.readouterr()\n    assert 'Error: sphinx-quickstart only generate into a empty directory.' in captured.err\n\n\n\n", "def test_quickstart_overwrite_existing_directory(tempdir, monkeypatch):\n    existing_files = ['conf.py', 'source/index.rst', 'Makefile']\n    for filename in existing_files:\n        (tempdir / filename).touch()\n    \n    answers = {\n        'Root path': tempdir,\n        'Project name': 'Project Name',  \n        'Author name': 'Author Name', \n        'Project version': '0.1',\n    }\n    qs.term_input = mock_input(answers)\n    d = {}\n    with pytest.raises(SystemExit):\n        qs.ask_user(d)\n\n", "def test_conf_py_overwrite_asks(tempdir):\n    answers = {\n        'Root path': tempdir,\n        'Project name': 'Sphinx Test',\n        'Author name': 'Georg Brandl',\n        'Project version': '0.1',\n    }\n\n    # Create an existing conf.py\n    (tempdir / 'conf.py').touch()\n    qs.term_input = mock_input(answers)\n    d = {}\n    qs.ask_user(d)\n    assert 'overwrite' in qs.term_input.call_args_list[0][0][0]\n\n\n", "def test_quickstart_overwrite(tempdir):\n    answers = {\n        'Root path': tempdir,\n        'Project name': 'Sphinx Test',\n        'Author name': 'Georg Brandl',\n        'Project version': '0.1',\n    }\n    qs.term_input = mock_input(answers)\n    d = {}\n    qs.ask_user(d)\n    qs.generate(d)\n\n    # create a file to simulate existing conf.py\n    (tempdir / 'conf.py').write_text('')\n\n    answers = {\n        'Please enter a new root path (or just Enter to exit)': '',\n        'Overwrite existing files? (y/n)':'y',\n    }\n    qs.term_input = mock_input(answers)\n    d = {}\n    qs.ask_user(d)\n    qs.generate(d, overwrite=True)\n\n    conffile = tempdir / 'conf.py'\n    assert conffile.isfile()\n"], "sample_1077": ["def test_imageset_union():\n    n = Dummy()\n    s1 = ImageSet(Lambda(n, 2*n), S.Reals)\n    s2 = ImageSet(Lambda(n, 2*n + 1), S.Reals)\n    assert s1.union(s2) == S.Reals\n", "compilation error", "    def test_ImageSet_empty_intersection():\n        i = ImageSet(Lambda(x, x), S.EmptySet)\n        assert i.intersect(S.Reals) is S.EmptySet\n\n", "compilation error", "compilation error"], "sample_216": ["    def test_operation_attributes_with_field_name(self):\n        changes = self.get_changes([], [self.author_name, self.book])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"book\", field=models.ForeignKey(\n            'otherapp.Book', models.CASCADE,\n        ))\n", "    def test_rename_model(self):\n        before = [\n            ModelState('app', 'OldName', [\n                ('id', models.AutoField(primary_key=True)),\n            ]),\n        ]\n        after = [\n            ModelState('app', 'NewName', [\n                ('id', models.AutoField(primary_key=True)),\n            ]),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['RenameModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, old_name='OldName', new_name='NewName') \n", "    def test_add_field_with_choice_and_default(self):\n        \"\"\"\n        #23811 - Adding a field with choices and a default value should work properly in migrations.\n        \"\"\"\n        before = [\n            ModelState('app', 'MyModel', [\n                ('id', models.AutoField(primary_key=True)),\n            ]),\n        ]\n        after = [\n            ModelState('app', 'MyModel', [\n                ('id', models.AutoField(primary_key=True)),\n                ('status', models.CharField(max_length=10, choices=[('draft', 'Draft'), ('published', 'Published')], default='draft')),\n            ]),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['AddField'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='status', field_type='CharField',\n                                      default='draft, ', choices=[('draft', 'Draft'), ('published', 'Published')])\n", "    def test_initial_with_create_model(self):\n        class Migration(migrations.Migration):\n            initial = True\n            operations = [\n                migrations.CreateModel('Person', fields=[]),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'initial')\n", "    def test_fk_dependency_same_app(self):\n        \"\"\"\n        #23884 - ForeignKeys referencing models in the same app should\n        depend on the model's creation, not on the app's first migration.\n        \"\"\"\n        book = ModelState('app', 'Book', [\n            ('id', models.AutoField(primary_key=True)),\n        ])\n        chapter = ModelState('app', 'Chapter', [\n            ('id', models.AutoField(primary_key=True)),\n            ('book', models.ForeignKey('app.Book', models.CASCADE)),\n        ])\n        changes = self.get_changes([book], [book, chapter])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, [\"CreateModel\", \"CreateModel\"])\n\n        self.assertMigrationDependencies(changes, 'app', 0, [('app', 'auto_1')])\n"], "sample_120": ["    def test_serialize_non_serializable(self):\n        class NotSerializable:\n            pass\n        with self.assertRaisesMessage(ValueError, 'Cannot serialize: <class \\'migrations.test_writer.NotSerializable\\'>'):\n            self.assertSerializedEqual(NotSerializable())\n", "    def test_serialize_foreignkey(self):\n        class Author(models.Model):\n            name = models.CharField(max_length=100)\n\n        class Book(models.Model):\n            title = models.CharField(max_length=200)\n            author = models.ForeignKey(Author, on_delete=models.CASCADE)\n\n        self.assertSerializedEqual(Book.author)\n        self.assertSerializedResultEqual(\n            Book.author,\n            (\"models.ForeignKey(to='migrations.test_writer.Author', on_delete=models.CASCADE)\",\n             {'from django.db import models', 'import migrations.test_writer'}),\n        )\n\n\n\n", "    def test_serialize_decimal_with_custom_context(self):\n        class CustomDecimal(decimal.Decimal):\n            pass\n\n        with self.assertRaisesMessage(ValueError, 'Cannot serialize: CustomDecimal(\\'1.2\\')'):\n            self.serialize_round_trip(CustomDecimal('1.2'))\n\n        MigrationWriter.register_serializer(CustomDecimal, DecimalSerializer)\n\n        self.assertSerializedEqual(CustomDecimal('1.2'))\n        MigrationWriter.unregister_serializer(CustomDecimal)\n", "    def test_serialize_enum(self):\n        class Status(enum.Enum):\n            PENDING = 'pending'\n            APPROVED = 'approved'\n            REJECTED = 'rejected'\n\n        self.assertSerializedEqual(Status.PENDING)\n", "    def test_serialize_decimal_with_custom_context(self):\n        class DecimalContextSerializer(BaseSerializer):\n                return f'decimal.Decimal(\"{self.value}\" , context={self.context})'\n\n        MigrationWriter.register_serializer(decimal.Decimal, DecimalContextSerializer)\n        value = decimal.Decimal(\"12.345\", context=decimal.getcontext().copy().prec=28)\n        self.assertSerializedEqual(value)\n        MigrationWriter.unregister_serializer(decimal.Decimal)\n"], "sample_869": ["compilation error", "def test_jaccard_score_sparse_matrix():\n    # Sparse matrix format\n    from scipy.sparse import csr_matrix\n    y_true = csr_matrix([[0, 1, 1], [1, 0, 1]])\n    y_pred = csr_matrix([[0, 0, 1], [1, 0, 1]])\n\n    # size(y1 \\inter y2) = [1, 2]\n    # size(y1 \\union y2) = [2, 2]\n\n    assert jaccard_score(y_true, y_pred, average='samples') == 0.75\n    assert jaccard_score(y_true, y_true, average='samples') == 1\n    assert jaccard_score(y_pred, y_pred, average='samples') == 1\n", "compilation error", "def test_jaccard_score_sparse():\n    # Test Jaccard score with sparse matrices\n    from scipy.sparse import csr_matrix\n\n    y_true = csr_matrix([[0, 1, 1], [1, 0, 1]])\n    y_pred = csr_matrix([[0, 0, 1], [1, 0, 1]])\n\n    # size(y1 \\inter y2) = [1, 2]\n    # size(y1 \\union y2) = [2, 2]\n    assert jaccard_score(y_true, y_pred, average='samples') == 0.75\n    assert jaccard_score(y_true, y_true, average='samples') == 1\n\n    with pytest.raises(ValueError):\n        jaccard_score(y_true, y_pred, average='macro')\n", "compilation error"], "sample_647": ["def test_unformatted_warning(pytester: Pytester) -> None:\n    \"\"\"Test that UnformattedWarning correctly formats its message.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            with pytest.warns(pytest.PytestWarning) as record:\n                pytest.UnformattedWarning(pytest.PytestWarning, \"This is a {placeholder} message\").format(placeholder=\"test\")\n            assert record[0].message.args[0] == \"This is a test message\"\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_unformatted_warning_formats_correctly(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            warning = pytest.UnformattedWarning[pytest.PytestWarning](\n                pytest.PytestWarning, \"message {index}\"\n            )\n            assert str(warning.format(index=1)) == \"message 1\"\n\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_warning_types_message(warning_class, expected_message):\n    assert warning_class.__doc__ == expected_message\n\n", "    def test_warn_explicit_for_ignores_existing_warnings(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n            import warnings\n\n                warnings.warn(pytest.deprecated(\"some warning\"))\n                \n                warnings.warn(pytest.deprecated(\"some other warning\"))\n        \"\"\"\n        )\n        result = pytester.runpytest(\"-W\", \"ignore::pytest.DeprecatedWarning\")\n        assert result.ret == 0\n\n", "def test_unhandled_coroutine_warning(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        async def test():\n            pass\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"E       pytest.PytestUnhandledCoroutineWarning: \",\n            \"E       An unhandled coroutine was encountered when collecting test functions, but was not\",\n            \"E       handled by any async-aware plugin.\",\n            \"E       Coroutine test functions are not natively supported.\",\n        ]\n    )\n\n"], "sample_1126": ["    def test_Dagger_order():\n\n        A = Operator('A')\n        B = Operator('B')\n        assert Dagger(A*B) == Dagger(B)*Dagger(A)\n", "def test_dagger_repeated():\n    O = Operator('O')\n    assert Dagger(Dagger(O)) == O\n\n", "    def test_dagger_identity():\n        assert Dagger(IdentityOperator()) == IdentityOperator()\n", "    def test_nested_dagger():\n        A = Operator('A')\n        assert Dagger(Dagger(A)) == A\n", "def test_operators():\n    A = Operator('A')\n    B = Operator('B')\n    assert Dagger(A + B) == Dagger(A) + Dagger(B)\n    assert Dagger(A*B) == Dagger(B)*Dagger(A)\n    assert Dagger(A**2) == Dagger(A)**2\n"], "sample_242": ["    def test_year_lookup_bounds(self):\n        look_up = YearLookup(\n            lhs=Value(datetime(2010, 1, 1, 0, 0, 0), output_field=DateTimeField()),\n            rhs=Value(2010, output_field=IntegerField()),\n        )\n        with mock.patch('django.db.backends.utils.DatabaseOperations.year_lookup_bounds_for_datetime_field') as mock_bounds:\n            mock_bounds.return_value = (datetime(2010, 1, 1, 0, 0, 0), datetime(2010, 12, 31, 23, 59, 59))\n            start, finish = look_up.year_lookup_bounds(mock.Mock(), 2010)\n            mock_bounds.assert_called_once_with(2010)\n            self.assertEqual(start, datetime(2010, 1, 1, 0, 0, 0))\n            self.assertEqual(finish, datetime(2010, 12, 31, 23, 59, 59))\n\n", "    def test_year_lookup_bounds(self):\n        look_up = YearLookup(\n            lhs=Value(datetime(2010, 1, 1, 0, 0, 0), output_field=DateTimeField()),\n            rhs=Value(2010),\n        )\n        with mock.patch('django.db.models.lookups.DateTimeField') as mock_datetime:\n            mock_datetime.ops.year_lookup_bounds_for_datetime_field.return_value = (\n                datetime(2010, 1, 1, 0, 0, 0),\n                datetime(2010, 12, 31, 23, 59, 59),\n            )\n            start, finish = look_up.year_lookup_bounds(\n                mock_datetime.ops,\n                look_up.rhs,\n            )\n            self.assertEqual(start, datetime(2010, 1, 1, 0, 0, 0))\n            self.assertEqual(finish, datetime(2010, 12, 31, 23, 59, 59))\n", "    def test_year_lookup_bounds(self):\n        look_up = YearLookup(\n            lhs=Value(datetime(2010, 1, 1, 0, 0, 0), output_field=DateTimeField()),\n            rhs=Value(2010, output_field=IntegerField()),\n        )\n        with mock.patch.object(look_up, 'year_lookup_bounds') as mock_bounds:\n            look_up.as_sql(None, None)\n            mock_bounds.assert_called_once_with(None, 2010)\n", "    def test_year_lookup_bounds(self):\n        lookup = YearLookup(\n            lhs=Value(datetime(2010, 1, 1, 0, 0, 0), output_field=DateTimeField()),\n            rhs=Value(2010, output_field=IntegerField()),\n        )\n        with mock.patch.object(lookup, 'year_lookup_bounds') as mock_bounds:\n            lookup.year_lookup_bounds(mock.MagicMock(), 2010)\n            mock_bounds.assert_called_once_with(mock.MagicMock(), 2010)\n", "    def test_year_lookup_bounds(self):\n        lookup = YearLookup(\n            lhs=Value(datetime(2010, 1, 1, 0, 0, 0), output_field=DateTimeField()),\n            rhs=Value(2010),\n        )\n\n        with mock.patch('django.db.backends.base.BaseDatabaseWrapper.ops') as mock_ops:\n            mock_ops.year_lookup_bounds_for_datetime_field.return_value = (datetime(2010, 1, 1, 0, 0, 0), datetime(2010, 12, 31, 23, 59, 59))\n\n            start, finish = lookup.year_lookup_bounds(mock_ops, 2010)\n            self.assertEqual(start, datetime(2010, 1, 1, 0, 0, 0))\n            self.assertEqual(finish, datetime(2010, 12, 31, 23, 59, 59))\n"], "sample_106": ["    def test_cache_control_stale_while_revalidate(self):\n        cache_control = ('max-age=60, stale-while-revalidate=300')\n\n        view = cache_page(30, cache_control)(hello_world_view)\n\n        request = self.factory.get('/view/')\n        response = view(request, '1')\n        self.assertEqual(response.content, b'Hello World 1')\n        \n        # Wait for the cache to expire\n        time.sleep(31)\n\n        response = view(request, '1')\n        self.assertEqual(response.content, b'Hello World 1')\n\n", "    def test_cache_key_i18n_with_special_chars(self):\n        request = self.factory.get(self.path)\n        request.META['HTTP_ACCEPT_ENCODING'] = 'gzip;q=1.0, identity; q=0.5, *;q=0'\n        \n        with patch('django.utils.translation.get_language', return_value='fr-fr'):\n            response = HttpResponse()\n            key = learn_cache_key(request, response)\n            self.assertIn('fr-fr', key, \"Cache keys should include the language name with special chars\")\n            \n        self.assertNotEqual(key, learn_cache_key(request, response))\n", "    def test_cache_control_no_store(self):\n        \"\"\"\n        Responses with 'Cache-Control: no-store' are not cached.\n        \"\"\"\n        view_with_no_store = cache_page(3)(cache_control(no_store=True)(hello_world_view))\n        request = self.factory.get('/view/')\n        response = view_with_no_store(request, '1')\n        self.assertEqual(response.content, b'Hello World 1')\n        response = view_with_no_store(request, '2')\n        self.assertEqual(response.content, b'Hello World 2')\n", "    def test_cache_middleware_non_200(self):\n\n            return HttpResponseBadRequest('Bad Request')\n\n        cached_view = cache_page(3)(view)\n        request = self.factory.get('/view/')\n        response = cached_view(request)\n        self.assertEqual(response.status_code, 400)\n        self.assertEqual(self.other_cache.get('cache_middleware_test'), None)\n\n", "    def test_cache_key_language_code_in_etag(self):\n        request = self.factory.get(self.path)\n        translation.activate('es')\n        template = engines['django'].from_string(\"This is a test\")\n        response = TemplateResponse(HttpRequest(), template)\n        learn_cache_key(request, response)\n        etag = response['ETag']\n        self.assertIn('es', etag)\n"], "sample_61": ["    def test_ascii_validator_with_unicode(self):\n        v = validators.ASCIIUsernameValidator()\n        with self.assertRaises(ValidationError):\n            v('\u0623\u062d\u0645\u062f')\n", "    def test_ascii_validator_empty(self):\n        v = validators.ASCIIUsernameValidator()\n        with self.assertRaises(ValidationError):\n            v('')\n", "    def test_get_password_validators_empty(self):\n        self.assertEqual(get_password_validators([]), [])\n\n", "    def test_ascii_validator_with_custom_regex(self):\n        v = validators.ASCIIUsernameValidator(regex=r'^[a-zA-Z0-9]+$')\n        for valid in ['glenn', 'GLEnN', '123']:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in ['o\\'connell', '\u00c9ric', 'jean marc', 'trailingnewline\\n', '123-', 'glenn.' ]:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n\n", "    def test_ascii_validator_custom_message(self):\n        v = validators.ASCIIUsernameValidator(message='Custom message.')\n        with self.assertRaises(ValidationError) as cm:\n            v('\u00c9ric')\n        self.assertEqual(cm.exception.messages, ['Custom message.'])\n"], "sample_508": ["compilation error", "compilation error", "compilation error", "compilation error", "    def test_setp_invalid_kwarg():\n        fig, ax = plt.subplots()\n        with pytest.raises(TypeError, match=\"Invalid keyword arguments\"):\n            plt.setp(ax.lines, foo='bar')\n\n"], "sample_690": ["def test_importorskip_with_reason(pytester: Pytester) -> None:\n    with pytest.raises(\n        pytest.skip.Exception, match=\"^could not import 'doesnotexist': No module named .*reason: missing dependency$\"\n    ):\n        pytest.importorskip(\"doesnotexst\", reason=\"missing dependency\")\n", "    def test_skip_if_condition_evaluates_to_false(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n            @pytest.mark.skipif(False)\n                assert True\n        \"\"\"\n        )\n        result = pytester.runpytest()\n        result.stdout.fnmatch_lines(\n            \"\"\"\n            *1 passed*\n            \"\"\"\n        )\n\n", "    def test_skipif_condition_evaluates_inside_module(pytester: Pytester):\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n            my_condition = True\n            @pytest.mark.skipif(\"my_condition\")\n                assert 0\n\n            my_condition = False\n            @pytest.mark.skipif(\"my_condition\")\n                assert 0\n\n        \"\"\"\n        )\n        result = pytester.runpytest()\n        result.stdout.fnmatch_lines([\"*1 passed*1 skipped*\"])\n\n\n\n", "    def test_importorskip_with_reason(pytester: Pytester) -> None:\n        with pytest.raises(pytest.skip.Exception, match=\"^Skip reason: missing module\"):\n            pytest.importorskip(\"doesnotexist\", reason=\"missing module\")\n", "    def test_importorskip_attr(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            test_module=\"\"\"\n            import pytest\n            \n            try:\n                import nonexistentmodule\n            except ImportError:\n                pytest.skip(\"nonexistentmodule not found\")\n            \n                pass\n            \"\"\"\n        )\n        result = pytester.runpytest(\"test_module.py\")\n        result.stdout.fnmatch_lines([\"*SKIP*nonexistentmodule not found*\"])\n\n"], "sample_341": ["    def test_formset_with_initial_data_and_extra(self):\n        data = {\n            'choices-TOTAL_FORMS': '3',\n            'choices-INITIAL_FORMS': '2',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n\n        initial = [\n            {'choice': 'Initial Zero', 'votes': 0},\n            {'choice': 'Initial One', 'votes': 1},\n        ]\n        formset = ChoiceFormSet(data, initial=initial, auto_id=False, prefix='choices')\n        self.assertEqual(formset.initial_form_count(), 2)\n        self.assertEqual(formset.total_form_count(), 3)\n        self.assertEqual(len(formset.forms), 3)\n        self.assertEqual(formset.cleaned_data, [\n            {'choice': 'Zero', 'votes': 0},\n            {'choice': 'One', 'votes': 1},\n            {'choice': None, 'votes': ''},\n        ])\n", "    def test_all_valid_empty_formset(self):\n        formset = formset_factory(Choice, extra=0)()\n        self.assertTrue(all_valid((formset,)))\n", "    def test_formset_errors_override_formset_is_valid(self):\n        \"\"\"\n        Checks that formset.errors is populated correctly even if\n        formset.is_valid() is True due to validation skipping.\n        \"\"\"\n        class AlwaysValidForm(Form):\n            field = CharField()\n\n        class AlwaysValidFormSet(BaseFormSet):\n                raise ValidationError(\"This formset is always invalid\")\n\n        AlwaysValidFormSet = formset_factory(AlwaysValidForm, formset=AlwaysValidFormSet)\n        data = {'form-TOTAL_FORMS': '1', 'form-INITIAL_FORMS': '0', 'form-0-field': 'test'}\n        formset = AlwaysValidFormSet(data)\n\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.errors, [{'': ['This formset is always invalid']}])\n", "    def test_formset_max_num_with_initial(self):\n        \"\"\"\n        Testing that max_num is respected when initial data is present.\n        \"\"\"\n        data = {'form-TOTAL_FORMS': '3', 'form-INITIAL_FORMS': '2'}\n        limited_form_set = formset_factory(\n            FavoriteDrinkForm,\n            max_num=2,\n        )\n        formset = limited_form_set(data, initial=[{'name': 'Gin Tonic'}, {'name': 'Rum Coke'}]) \n        self.assertEqual(len(formset.forms), 2)\n", "    def test_empty_formset_data_with_initial(self):\n        ChoiceFormSet = formset_factory(Choice, extra=0)\n        formset = ChoiceFormSet(initial=[{'choice': 'Initial', 'votes': 0}])\n        self.assertEqual(len(formset.forms), 1)\n        self.assertEqual(formset.cleaned_data, [{'choice': 'Initial', 'votes': 0}])\n\n"], "sample_932": ["compilation error", "    def test_build_domain_cpp_warn_template_param_qualified_name(app, status, warning):\n        app.builder.build_all()\n        ws = filter_warnings(warning, \"warn-template-param-qualified-name\")\n        assert len(ws) == 2\n        assert \"WARNING: cpp:type reference target not found: T::typeWarn\" in ws[0]\n        assert \"WARNING: cpp:type reference target not found: T::U::typeWarn\" in ws[1]\n\n\n", "    def test_cpp_index_entry_overriding():\n        text = (\".. cpp:function:: void f()\\n\"\n                \"   :noindexentry:\\n\"\n                \"   :cpp:index: function f (C++)\\n\"\n                \" .. cpp:function:: void g()\\n\"\n                \"   :cpp:index: Function G\\n\"\n                )\n        doctree = restructuredtext.parse(app, text)\n        assert_node(doctree, (addnodes.index, desc, addnodes.index, desc))\n        assert_node(doctree[0], addnodes.index, entries=[('single', 'function f (C++)', '_CPPv41fv', '', None)])\n        assert_node(doctree[2], addnodes.index, entries=[('single', 'Function G', '_CPPv41gv', '', None)])\n\n", "    def check(target):\n        class Config:\n            cpp_id_attributes = [\"id_attr\"]\n            cpp_paren_attributes = [\"paren_attr\"]\n        parser = DefinitionParser(target, location=None,\n                                  config=Config())\n        ast, isShorthand = parser.parse_xref_object()\n        parser.assert_end()", "def test_build_domain_cpp_no_default_returns(app, status, warning):\n    app.builder.build_all()\n    output = (app.outdir / 'functions.html').read_text()\n    assert 'returns' not in output\n\n"], "sample_12": ["def test_angle_comparison_with_other_types(cls, unit):\n    a1 = cls(1, unit=unit)\n    a2 = cls(1, unit=unit)\n    assert a1 == a2\n    assert a1 >= a2\n    assert a1 <= a2\n    assert not a1 < a2\n    assert not a1 > a2\n\n    with pytest.raises(TypeError):\n        a1 == \"1h\"\n\n    with pytest.raises(TypeError):\n        a1 >= \"1h\"\n", "compilation error", "    def test_wrap_at_multiples_of_360(cls, input_angle, expected_value):\n        a = cls(input_angle)\n        assert a.wrap_at(360.0 * u.deg).value == expected_value.value\n", "compilation error", "    def test_angle_pickle(cls, tmpdir):\n        angle_obj = cls(10.5, unit=u.deg)\n        with open(tmpdir.join(\"angle.pkl\"), 'wb') as f:\n            pickle.dump(angle_obj, f)\n        with open(tmpdir.join(\"angle.pkl\"), 'rb') as f:\n            loaded_angle = pickle.load(f)\n        assert loaded_angle == angle_obj\n"], "sample_1044": ["compilation error", "compilation error", "compilation error", "def test_issue_10994():\n    x = Symbol('x', real=True)\n    h = Symbol('h', real=True)\n    assert (x + h).is_real is True\n    assert (x + I*h).is_real is False\n    assert (x + I*h).is_imaginary is False\n", "compilation error"], "sample_845": ["def test_empty_input(Estimator):\n    # Test that vectorizers handle empty input lists gracefully.\n    if issubclass(Estimator, HashingVectorizer):\n        pytest.xfail('HashingVectorizer is not supported on PyPy')\n    est = Estimator()\n    assert_array_equal(est.fit_transform([]).toarray(), np.array([]))\n\n", "    def test_callable_analyzer_empty_input(tmpdir, Estimator):\n        # Test that error is raised when analyzer returns an empty list\n            return []\n        f = tmpdir.join(\"file.txt\")\n        f.write(\"sample content\\n\")\n\n        with pytest.raises(ValueError, match=\"empty corpus\"):\n            Estimator(analyzer=analyzer, input='filename').fit_transform([f])\n", "    def test_countvectorizer_empty_vocab():\n        vec = CountVectorizer()\n        X = vec.fit_transform([])\n        assert X.shape == (0, 0)\n        assert vec.get_feature_names() == []\n\n", "    def test_vectorizer_empty_vocabulary():\n        vect = CountVectorizer()\n        with pytest.raises(ValueError,\n                           match=\"Vocabulary is empty. \"\n                                 \"Please provide at least one document\"):\n            vect.fit_transform([])\n\n", "    def test_vectorizers_invalid_ngram_range_empty_string(vec):\n        # vectorizers could be initialized with invalid ngram range\n        # test for raising error message\n        invalid_range = vec.ngram_range\n        message = (\"Invalid value for ngram_range=%s \"\n                   \"lower boundary larger than the upper boundary.\"\n                   % str(invalid_range))\n        if isinstance(vec, HashingVectorizer):\n            pytest.xfail(reason='HashingVectorizer is not supported on PyPy')\n\n        assert_raise_message(\n            ValueError, message, vec.fit, [\"\"])\n        assert_raise_message(\n            ValueError, message, vec.fit_transform, [\"\"])\n\n        if isinstance(vec, HashingVectorizer):\n            assert_raise_message(\n                ValueError, message, vec.transform, [\"\"])\n\n"], "sample_540": ["    def test_animation_frames_generator_empty(tmpdir):\n        fig, ax = plt.subplots()\n        line, = ax.plot([], [])\n\n            line.set_data([], [])\n            return line,\n\n            line.set_data(frame['x'], frame['y'])\n            return line,\n\n            yield from []\n\n        anim = animation.FuncAnimation(fig, animate, init_func=init,\n                                       frames=frames_generator,\n                                       save_count=10)\n        writer = NullMovieWriter()\n        anim.save('unused.null', writer=writer)\n\n", "def test_animation_repeating(anim):\n    class RepeatingWriter(animation.AbstractMovieWriter):\n            self._count = 0\n\n            self._count += 1\n    writer = RepeatingWriter()\n\n    anim = animation.FuncAnimation(**anim)\n    anim.save('test.null', writer=writer, fps=30)\n\n    assert writer._count == anim._save_count * (anim.repeat_delay // 2)\n", "def test_empty_frames(tmpdir):\n    fig, ax = plt.subplots()\n    line, = ax.plot([])\n\n        return line,\n\n    anim = animation.FuncAnimation(fig, animate, frames=[], blit=True)\n    with tmpdir.as_cwd():\n        with pytest.warns(UserWarning, match=\"exhausted\"):\n            anim.save(\"test.gif\", writer='pillow')\n\n", "def test_animation_blit_no_artists(anim):\n    fig, ax = plt.subplots()\n        return []\n\n        return []\n\n    anim = animation.FuncAnimation(\n        fig, animate, init_func=init, blit=True,\n    )\n    with pytest.raises(RuntimeError, match=\"The init_func must return a sequence\"):\n        anim._init_draw()\n\n", "def test_empty_frames(anim):\n    anim = animation.FuncAnimation(**anim, frames=())\n    with pytest.raises(StopIteration):\n        next(anim.new_frame_seq())\n\n"], "sample_977": ["    def test_Piecewise():\n        from sympy.functions import Piecewise\n        p = Piecewise((0, x < 0), (1, True))\n\n        assert mcode(p) == \"Piecewise[{{0, x < 0}, {1, True}}\"\n\n", "def test_user_functions():\n    from sympy.abc import x\n    mcode_printer = MCodePrinter(\n        settings={'user_functions': {'foo': [lambda *x: True, 'bar']}})\n    assert mcode_printer.doprint(foo(x)) == 'bar[x]'\n", "compilation error", "compilation error", "def test_Function_with_args():\n    f = Function('f')\n    assert mcode(f(x, y, z)) == \"f[x, y, z]\"\n    assert mcode(f(1, 2, 3)) == \"f[1, 2, 3]\"\n    assert mcode(f(x + y, z*2, sin(x))) == \"f[x + y, 2*z, Sin[x]]\"\n"], "sample_712": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_950": ["def test_pyobject_signature(app):\n    text = (\".. py:function:: foo(a, b=None)\\n\"\n            \"\\n\"\n            \".. py:object:: foo\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert 'foo' in domain.objects \n    assert domain.objects['foo'] == ('index', 'foo', 'function', False)\n", "def test_noindex_entry(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    # The generated HTML should not contain the reference to noindexentry class.\n    assert '<span class=\"n\">NoIndexEntry</span>' not in content\n", "def test_py_object_index_entry(app):\n    text = \".. py:function:: example.func\\n\"\n    doctree = restructuredtext.parse(app, text)\n    index = PythonDomain(app.env)\n    index.process_doc(doctree=doctree, source_path='example.rst')\n    assert index.object_index['example.func'] == [('pair', 'built-in function; example.func()', 'example.func', '', None)]\n\n", "def test_inline_literal(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert '<span class=\"n\">Name</span>' in content\n    assert '<span class=\"n\">Age</span>' in content\n\n", "def test_py_attribute_with_default_value(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :type: Optional[str]\\n\"\n            \"      :default: 'default value'\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'attr (Class attribute)', 'Class.attr', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"attr\"],\n                                                     [desc_annotation, (\": \",\n                                                                        [pending_xref, \"Optional\"],\n                                                                        [desc_sig_punctuation, \"[\"],\n                                                                        [pending_xref, \"str\"],\n                                                                        [desc_sig_punctuation, \"]\"],\n                                                                        \": \",\n                                                                        \"default: 'default value'\")]),\n                                                     ],\n                                   [desc_content, ()]))\n    assert_node(doctree[1][1][1][0][1][1], pending_xref, **{\"py:class\": \"Class\"})\n    assert_node(doctree[1][1][1][0][1][3], pending_xref, **{\"py:class\": \"Class\"})\n    assert 'Class.attr' in domain.objects\n    assert domain.objects['Class.attr'] == ('index', 'Class.attr', 'attribute', False)\n\n"], "sample_445": ["    def test_past_midnight(self):\n        t = datetime.datetime(\n            self.t.year + 1, self.t.month, self.t.day, 2, 0, 0, tzinfo=timezone.get_default_timezone()\n        )\n        self.assertEqual(timesince(t), \"1\\xa0year\")\n        self.assertEqual(timeuntil(self.t, t), \"1\\xa0year\")\n", "    def test_different_timezones_again(self):\n        now = datetime.datetime.now(timezone.utc)\n        now_tz = timezone.make_aware(now, timezone.get_default_timezone())\n        self.assertEqual(timesince(now_tz, now), \"0\\xa0minutes\")\n", "    def test_timezone_different_units(self):\n        \"\"\"Test different units across timezones.\"\"\"\n        now = datetime.datetime.now(timezone.utc)\n        tz_aware_past = timezone.make_aware(self.t, timezone.get_default_timezone())\n        self.assertEqual(timesince(tz_aware_past, now), \"15\\xa0years, 3\\xa0months\")\n", "    def test_timezone_aware_with_naive(self):\n        naive_datetime = datetime.datetime(2023, 1, 1, 12, 0, 0)\n        aware_datetime = timezone.make_aware(naive_datetime, timezone.get_default_timezone())\n        self.assertEqual(timesince(naive_datetime, aware_datetime), \"0\\xa0minutes\")\n        self.assertEqual(timeuntil(aware_datetime, naive_datetime), \"0\\xa0minutes\")\n", "    def test_tzinfo_mismatch(self):\n        t = timezone.make_aware(self.t, timezone.get_default_timezone())\n        t_diff_tz = timezone.make_aware(self.t, timezone.get_fixed_timezone(195))\n        self.assertEqual(timesince(t, t_diff_tz), \"0\\xa0minutes\")\n        self.assertEqual(timeuntil(t_diff_tz, t), \"0\\xa0minutes\")\n"], "sample_340": ["    def test_squashed_migration_dependencies(self):\n        \"\"\"\n        Tests loading squashed migrations and their dependencies.\n        \"\"\"\n        with override_settings(MIGRATION_MODULES={\"migrations\": \"migrations.test_migrations_squashed_complex\"}):\n            loader = MigrationLoader(connection)\n            recorder = MigrationRecorder(connection)\n            self.addCleanup(recorder.flush)\n\n            # Load with nothing applied: should use the squashed migration\n            loader.build_graph()\n            plan = set(loader.graph.forwards_plan(('migrations', '7_auto')))\n            self.assertEqual(len(plan - loader.applied_migrations.keys()), 5)\n\n            # Apply the first migration: should still use the squashed migration\n            self.record_applied(recorder, 'migrations', '1_auto')\n            loader.build_graph()\n            plan = set(loader.graph.forwards_plan(('migrations', '7_auto')))\n            self.assertEqual(len(plan - loader.applied_migrations.keys()), 4)\n\n            # Apply the second migration: should still use the squashed migration\n            self.record_applied(recorder, 'migrations', '2_auto')\n            loader.build_graph()\n            plan = set(loader.graph.forwards_plan(('migrations', '7_auto')))\n            self.assertEqual(len(plan - loader.applied_migrations.keys()), 3)\n\n            # Apply the third migration: now should have individual migrations\n            self.record_applied(recorder, 'migrations', '3_auto')\n            loader.build_graph()\n            plan = set(loader.graph.forwards_plan(('migrations', '7_auto')))\n            self.assertEqual(len(plan - loader.applied_migrations.keys()), 4)\n", "    def test_detect_conflicts(self):\n        \"Makes sure detect_conflicts finds apps with more than one leaf migration.\"\n        migration_loader = MigrationLoader(connection)\n        conflicting_apps = migration_loader.detect_conflicts()\n        self.assertEqual(conflicting_apps, {})\n        self.record_applied(recorder, 'migrations', '0001_initial')\n        conflicting_apps = migration_loader.detect_conflicts()\n        self.assertEqual(conflicting_apps, {})\n", "    def test_cyclic_dependencies(self):\n        \"\"\"\n        Tests that cyclic dependencies between migrations in the same app are\n        detected and raise an exception.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        with self.assertRaises(ValueError) as e:\n            loader.build_graph()\n        self.assertIn(\"Cyclic dependency found in migrations for app 'migrations'\", str(e.exception))\n\n", "    def test_circular_dependencies(self):\n        \"\"\"\n        Tests that circular dependencies between migrations raise an error.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        with self.assertRaises(RuntimeError) as cm:\n            loader.load_disk()\n        self.assertIn(\n            \"Circular dependency detected: migrations.0001_initial depends on migrations.0002_second, which depends on migrations.0001_initial\",\n            str(cm.exception),\n        )\n", "    def test_circular_dependencies(self):\n        \"\"\"\n        Tests that circular dependencies between migrations are detected.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        msg = \"Circular dependency detected: migrations.0002_second depends on migrations.0001_initial which depends on migrations.0002_second.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            loader.build_graph()\n"], "sample_1069": ["def test_issue_15517_complex_matrix():\n    A = Matrix([[1, 2], [3, 4]])\n    B = Matrix([[5, 6], [7, 8]])\n    assert mcode(A + 2*B, assign_to='C') == \"C = A + 2*B;\"\n", "def test_print_matrix_scalar_power():\n    A = MatrixSymbol('A', 2, 2)\n    assert octave_code(A**x) == 'A.^x'\n", "compilation error", "compilation error", "def test_octave_special_functions():\n    assert octave_code(Chi(x)) == 'coshint(x)'\n    assert octave_code(Shi(x)) == 'sinhint(x)'\n    assert octave_code(Ci(x)) == 'cosint(x)'\n    assert octave_code(Si(x)) == 'sinint(x)'\n    assert octave_code(li(x)) == 'logint(x)'\n    assert octave_code(polygamma(x, y)) == 'psi(x, y)'\n    assert octave_code(RisingFactorial(x, y)) == 'pochhammer(x, y)'\n    assert octave_code(DiracDelta(x)) == 'dirac(x)'\n    assert octave_code(DiracDelta(x, 3)) == 'dirac(3, x)'\n    assert octave_code(Heaviside(x)) == 'heaviside(x)'\n    assert octave_code(Heaviside(x, y)) == 'heaviside(x, y)'\n\n"], "sample_412": ["    def test_urlize_nofollow(self):\n        tests = (\n            (\n                \"Search for google.com/?q=! and see.\",\n                'Search for <a href=\"http://google.com/?q=\" rel=\"nofollow\">google.com/?q='\n                \"</a>! and see.\",\n            ),\n            (lazystr(\"Search for google.com/?q=!\"), 'Search for <a href=\"http://google.com/?q=\" rel=\"nofollow\">google.com/?q=</a>!'),\n        )\n        for value, output in tests:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value, nofollow=True), output)\n", "    def test_urlize_trim_url_limit(self):\n        text = (\n            \"This is a long url: https://www.example.com/very/very/very/long/url/path\"\n            \"?query=string&with=lots&of=parameters\"\n        )\n        self.assertEqual(\n            urlize(text, trim_url_limit=30),\n            'This is a long url: <a href=\"https://www.example.com/very/very/very/long/url/path?query=string&with=lots&of=parameters\">https://www.example.com/very/very/very...</a>'\n        )\n", "    def test_urlize_trim_url_limit(self):\n        long_url = (\n            \"https://www.example.com/very/long/path/with/many/segments\"\n            \"/and/a/really/long/querystring?parameter1=value1&parameter2=value2\"\n        )\n        self.assertEqual(urlize(long_url, trim_url_limit=30), '<a href=\"%s\">%s...</a>' % (long_url, long_url[: 30]))\n", "    def test_urlize_trim_url(self):\n        self.assertEqual(\n            urlize(\"This is a very very long url http://www.example.com/very/very/very/very/very/long/path?q=query\", trim_url_limit=20),\n            'This is a very very long url <a href=\"http://www.example.com/very/very/very/very/very/long/path?q=query\">http://www.example.com/very/very/very\u2026</a>',\n        )\n", "    def test_urlize_trim_url_limit(self):\n        long_url = (\n            \"https://www.example.com/very/long/url/path/that/will/be/truncated\"\n        )\n        self.assertEqual(\n            urlize(long_url, trim_url_limit=20),\n            '<a href=\"https://www.example.com/very/long/url/path/that/will/be/truncated\">https://www.example.com/very/long/url/path/\u2026</a>',\n        )\n"], "sample_324": ["    def test_csrf_token_in_500_error_stays_constant(self):\n        response = self.client.get('/error/')\n        self.assertEqual(response.status_code, 500)\n        token1 = response.content\n        response = self.client.get('/error/')\n        self.assertEqual(response.status_code, 500)\n        token2 = response.content\n        self.assertTrue(equivalent_tokens(token1.decode('ascii'), token2.decode('ascii')))\n", "    def test_csrf_token_on_500_stays_constant(self):\n        response = self.client.get('/error500/')\n        self.assertEqual(response.status_code, 500)\n        token1 = response.cookies.get(settings.CSRF_COOKIE_NAME)\n        response = self.client.get('/error500/')\n        self.assertEqual(response.status_code, 500)\n        token2 = response.cookies.get(settings.CSRF_COOKIE_NAME)\n        self.assertEqual(token1, token2)\n\n", "    def test_bare_secret_rejected(self):\n        \"\"\"\n        A POST request with a bare secret in the cookie, but no valid CSRF\n        token, is rejected.\n        \"\"\"\n        req = self._get_POST_bare_secret_csrf_cookie_request()\n        mw = CsrfViewMiddleware(post_form_view)\n        with self.assertLogs('django.security.csrf', 'WARNING') as cm:\n            response = mw.process_view(req, post_form_view, (), {})\n        self.assertEqual(response.status_code, 403)\n        self.assertEqual(\n            cm.records[0].getMessage(), 'Forbidden (%s): ' % REASON_CSRF_MISSING,\n        )\n", "    def test_csrf_token_on_500_stays_constant(self):\n        response = self.client.get('/internal_server_error/')\n        self.assertEqual(response.status_code, 500)\n        token1 = response.content.decode('ascii')\n        response = self.client.get('/internal_server_error/')\n        self.assertEqual(response.status_code, 500)\n        token2 = response.content.decode('ascii')\n        self.assertTrue(equivalent_tokens(token1, token2))\n", "    def test_csrf_token_on_500_stays_constant(self):\n        response = self.client.get('/exception/')\n        self.assertEqual(response.status_code, 500)\n        token1 = response.content\n        response = self.client.get('/exception/')\n        self.assertEqual(response.status_code, 500)\n        token2 = response.content\n        self.assertTrue(equivalent_tokens(token1.decode('ascii'), token2.decode('ascii')))\n"], "sample_71": ["    def test_zero_decimal_places(self):\n        self.assertEqual(nformat(1234.567, '.', decimal_pos=0), '1235')\n        self.assertEqual(nformat(-1234.567, '.', decimal_pos=0), '-1235')\n        self.assertEqual(nformat(Decimal('1234.567'), '.', decimal_pos=0), '1235')\n        self.assertEqual(nformat(Decimal('-1234.567'), '.', decimal_pos=0), '-1235')\n", "    def test_grouping_iterable(self):\n        self.assertEqual(nformat(1234567890, '.', grouping=[3, 2, 0]), '1,234,567,890')\n", "    def test_grouping_sequences(self):\n        self.assertEqual(nformat(1234567890, '.', grouping=(3, 2, 0)), '12,34,56,7890')\n        self.assertEqual(nformat(123456789012, '.', grouping=(3, 2, 3, 2, 0)), '12,34,56,78,9012')\n", "    def test_zero_decimal_pos(self):\n        self.assertEqual(nformat(1234.5678, '.', decimal_pos=0), '1235')\n        self.assertEqual(nformat(Decimal('1234.5678'), '.', decimal_pos=0), '1235')\n", "    def test_zero_decimal_places(self):\n        self.assertEqual(nformat(1234.5678, '.', decimal_pos=0), '1235')\n        self.assertEqual(nformat(-1234.5678, '.', decimal_pos=0), '-1235')\n"], "sample_281": ["    def test_custom_to_field_empty_result(self):\n        opts = {\n            'app_label': Answer._meta.app_label,\n            'model_name': Answer._meta.model_name,\n            'field_name': 'question_with_to_field',\n        }\n        request = self.factory.get(self.url, {'term': 'nonexisting', **opts})\n        request.user = self.superuser\n        response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n        self.assertEqual(response.status_code, 200)\n        data = json.loads(response.content.decode('utf-8'))\n        self.assertEqual(data, {\n            'results': [],\n            'pagination': {'more': False},\n        })\n", "    def test_autocomplete_with_related_object_already_selected(self):\n        q1 = Question.objects.create(question='Who am I?')\n        Answer.objects.create(question=q1)\n        self.selenium.get(self.live_server_url + reverse('autocomplete_admin:admin_views_answer_add'))\n        elem = self.selenium.find_element_by_css_selector('#id_question')\n        Select(elem).select_by_visible_text(q1.question)\n        elem.click()  # Open the autocomplete dropdown.\n        results = self.selenium.find_element_by_css_selector('.select2-results')\n        self.assertTrue(results.is_displayed())\n        results = results.find_elements_by_css_selector('.select2-results__option')\n        self.assertEqual(len(results), 1)\n\n", "    def test_autocomplete_field_required(self):\n        \"\"\"\n        Test that a required field is displayed in the autocomplete results\n        even if no data is entered in the search field.\n        \"\"\"\n        opts = {\n            'app_label': Answer._meta.app_label,\n            'model_name': Answer._meta.model_name,\n            'field_name': 'question',\n        }\n        request = self.factory.get(self.url, opts)\n        request.user = self.superuser\n        response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n        self.assertEqual(response.status_code, 200)\n        data = json.loads(response.content.decode('utf-8'))\n        self.assertEqual(len(data['results']), 0)\n\n\n", "    def test_no_search_fields(self):\n        class NoSearchFieldsAdmin(QuestionAdmin):\n            search_fields = []\n\n        with model_admin(Question, NoSearchFieldsAdmin):\n            response = self.client.get(self.url, {'term': 'is', **self.opts})\n            self.assertEqual(response.status_code, 404)\n", "    def test_custom_to_field_permission_denied_superuser(self):\n        q = Question.objects.create(question='Is this a question?')\n        request = self.factory.get(self.url, {'term': 'is', **self.opts, 'field_name': 'question_with_to_field'})\n        request.user = self.superuser\n        with self.assertRaises(PermissionDenied):\n            AutocompleteJsonView.as_view(**self.as_view_args)(request)\n\n"], "sample_603": ["def test_summarize_coords_with_empty_coords(dataset):\n    empty_coords = xr.Dataset(\n        coords={\"empty\": []},\n        attrs={\"description\": \"Test data.\"},\n    )\n    formatted = fh.summarize_coords(empty_coords.coords)\n    assert formatted == '<ul class=\"xr-var-list\"></ul>'\n\n    \n", "def test_summarize_coord_non_index_variable(dataset):\n    coord = dataset.coords[\"time\"].variable\n    formatted = fh.summarize_coord(\"time\", coord)\n    assert \"class='xr-has-index'\" not in formatted\n    assert \"time\" in formatted[\"time\"]\n", "    def test_icon(dataarray):\n        icon_html = fh._icon(\"icon-database\")\n        assert icon_html.startswith(\"<svg\") and icon_html.endswith(\"</svg>\")\n        assert \"icon-database\" in icon_html\n", "def test_summarize_coord_non_index(dataset):\n    formatted = fh.summarize_coord(\"time\", dataset.time)\n    assert \"class='xr-has-index'\" not in formatted\n", "def test_summarize_attrs_with_long_string_values():\n    attrs = {\"long_string\": \"a\" * 1000}\n    formatted = fh.summarize_attrs(attrs)\n    assert \"<dt><span>long_string :</span></dt>\" in formatted\n    assert f\"<dd>{escape('a' * 1000)}</dd>\" in formatted \n"], "sample_166": ["    def test_get_random_string_length(self):\n        self.assertEqual(len(get_random_string(32)), 32)\n", "    def test_get_random_string_length(self):\n        self.assertEqual(len(get_random_string(32)), 32)\n", "    def test_get_random_string_length(self):\n        self.assertEqual(len(get_random_string(30)), 30)\n", "    def test_get_random_string_length(self):\n        self.assertEqual(len(get_random_string(32)), 32)\n", "    def test_get_random_string_length(self):\n        self.assertEqual(len(get_random_string(32)), 32)\n\n"], "sample_1004": ["compilation error", "compilation error", "def test_ConditionSet_emptyset():\n    assert ConditionSet(x, S.False, S.Reals) == S.EmptySet\n    assert ConditionSet(x, S.False, FiniteSet(1, 2, 3)) == S.EmptySet\n    assert ConditionSet(x, x < 0, S.EmptySet) == S.EmptySet\n", "compilation error", "def test_ConditionSet_Lambda():\n    c = ConditionSet(x, Lambda(x, x > 0)(x), S.Reals)\n    assert c == ConditionSet(x, x > 0, S.Reals)\n    assert c.contains(1)\n    assert not c.contains(-1)\n"], "sample_643": ["def test_multi_reporter_independant_messages_types():\n    class ReporterModify(BaseReporter):\n            msg.msg = \"Modified message\"\n\n            pass\n\n            pass\n\n    class ReporterCheck(BaseReporter):\n            assert (\n                msg.msg == \"Not modified\"\n            ), \"Message object should not be changed by other reporters.\"\n\n            pass\n\n            pass\n\n    multi_reporter = MultiReporter([ReporterModify(), ReporterCheck()], lambda: None)\n\n    message = Message(\n        symbol=\"missing-docstring\",\n        msg_id=\"C0123\",\n        location=MessageLocationTuple(\"abspath\", \"path\", \"module\", \"obj\", 1, 2, 1, 3),\n        msg=\"Not modified\",\n        confidence=HIGH,\n    )\n\n    multi_reporter.handle_message(message)\n\n", "def test_colorized_output(capsys, mocker):\n    output = capsys.capture()\n    reporter = ColorizedTextReporter()\n    linter = PyLinter(reporter=reporter)\n    linter.open()\n    mock_msg = mocker.MagicMock(\n        msg_id=\"C0301\",\n        line=1,\n        column=1,\n        args=(1, 2),\n        symbol=\"line-too-long\",\n        module=\"my_module\",\n        category=\"convention\",\n        C=\"\",  # This is a custom field, not defined by pylint\n    )\n    linter.add_message(mock_msg)\n    linter.reporter.write_message(mock_msg)\n    out, err = output.readouterr()\n    assert \"my_module:1:1: C0301: Line too long (1/2) (line-too-long)\" in out\n", "def test_colorized_text_reporter(linter: PyLinter) -> None:\n    output = StringIO()\n    linter.reporter.out = output\n    linter.set_option(\"colorize-output\", True)\n    linter.open()\n    linter.set_current_module(\"my_mod\")\n\n    linter.add_message(\"C0301\", line=1, args=(1, 2))\n    linter.add_message(\"line-too-long\", line=2, end_lineno=2, end_col_offset=4, args=(3, 4))\n\n    out_lines = output.getvalue().split(\"\\n\")\n    assert out_lines[1].startswith(\"\\033[32;1m\")\n    assert \"\\033[0m\" in out_lines[1]\n    assert out_lines[2].startswith(\"\\033[31;1m\")\n    assert \"\\033[0m\" in out_lines[2]\n\n", "def test_colorized_text_reporter_colorizing(linter: PyLinter,\n                                           capsys):\n    linter.set_option(\"colorize\", True)\n    linter.reporter = ColorizedTextReporter()\n    linter.open()\n\n    linter.add_message(\"C0301\", line=1, args=(1, 2),\n                       path=\"my_file.py\")\n\n    out, _ = capsys.readouterr()\n    assert f\"\\033[{ColorizedTextReporter.COLOR_MAPPING['C'][0]}m\" in out\n    assert f\"\\033[{ColorizedTextReporter.COLOR_MAPPING['C'][0]}m\" in out\n", "def test_colorized_text_reporter_ansi_colors():\n    output = StringIO()\n    reporter = ColorizedTextReporter(\n        out=output,\n        color_mapping={\n            \"I\": MessageStyle(\"green\"),\n            \"C\": MessageStyle(None, (\"bold\",)),\n            \"R\": MessageStyle(\"magenta\", (\"bold\", \"italic\")),\n            \"W\": MessageStyle(\"magenta\"),\n            \"E\": MessageStyle(\"red\", (\"bold\",)),\n            \"F\": MessageStyle(\"red\", (\"bold\", \"underline\")),\n            \"S\": MessageStyle(\"yellow\", (\"inverse\",)),\n        },\n    )\n    linter = PyLinter(reporter=reporter)\n    linter.load_default_plugins()\n    linter.set_option(\"persistent\", False)\n    linter.set_option(\"reports\", True)\n    linter.set_option(\"score\", True)\n    linter.open()\n    linter.check_single_file_item(FileItem(\"somemodule\", source_file, \"somemodule\"))\n    linter.add_message(\"line-too-long\", line=1, args=(1, 2))\n    linter.generate_reports()\n    for message in linter.reporter.messages:\n      \n\n"], "sample_758": ["compilation error", "compilation error", "def test_check_array_dtype_raises_for_unhashable_dtype():\n    X = np.array([1, 2, 3], dtype=object)\n    assert_raises(TypeError, check_array, X, dtype=np.int64)\n", "compilation error", "compilation error"], "sample_1098": ["def test_hyper_rewrite_meijerg():\n    assert hyper([1, 2], [3, 4], z).rewrite(Meijerg) ==  Meijerg([], [], [3, 4], [], z)\n\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_746": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1146": ["compilation error", "compilation error", "def test_latex_printing_of_piecewise():\n    x = Symbol('x')\n    pw = Piecewise((1, x<0), (2, x>=0))\n    assert latex(pw) == r'\\begin{cases} 1 & \\text{for } x < 0 \\\\ 2 & \\text{for } x \\ge 0 \\end{cases}' \n", "compilation error", "compilation error"], "sample_460": ["    def test_non_admin_url_shares_url_prefix_without_final_catch_all_view(self):\n        url = reverse(\"non_admin10\")[:-1]\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 404)\n", "    def test_missing_slash_append_slash_true_non_staff_user_without_final_catch_all_view(self):\n        user = User.objects.create_user(\n            username=\"user\",\n            password=\"secret\",\n            email=\"user@example.com\",\n            is_staff=False,\n        )\n        self.client.force_login(user)\n        known_url = reverse(\"admin10:admin_views_article_changelist\")\n        response = self.client.get(known_url[:-1])\n        self.assertRedirects(\n            response,\n            \"/test_admin/admin10/login/?next=/test_admin/admin10/admin_views/article\",\n        )\n", "    def test_non_existing_view_with_trailing_slash_if_not_authenticated(self):\n        url = reverse(\"admin10:non_existing_view\") + \"/\"\n        response = self.client.get(url)\n        self.assertRedirects(response, \"%s?next=%s\" % (reverse(\"admin10:login\"), url))\n", "    def test_non_admin_url_404_if_authenticated(self):\n        superuser = User.objects.create_superuser(\n            username=\"super\",\n            password=\"secret\",\n            email=\"super@example.com\",\n        )\n        self.client.force_login(superuser)\n        unknown_url = \"/unknown/\"\n        response = self.client.get(unknown_url)\n        self.assertEqual(response.status_code, 404)\n", "    def test_non_admin_url_404_if_authenticated(self):\n        superuser = User.objects.create_superuser(\n            username=\"super\",\n            password=\"secret\",\n            email=\"super@example.com\",\n        )\n        self.client.force_login(superuser)\n        unknown_url = \"/unknown/\"\n        response = self.client.get(unknown_url)\n        self.assertEqual(response.status_code, 404)\n"], "sample_21": ["def test_read_qdp_with_empty_lines(tmp_path):\n    example_qdp = \"\"\"\n        ! Initial comment line 1\n        ! Initial comment line 2\n        READ TERR 1\n        READ SERR 3\n        ! Table 0 comment\n        !a a(pos) a(neg) b c ce d\n\n        53000.5   0.25  -0.5   1  1.5  3.5 2\n\n        54000.5   1.25  -1.5   2  2.5  4.5 3\n\n        NO NO NO NO NO\n\n\n\n\n        ! Table 1 comment\n        !a a(pos) a(neg) b c ce d\n        54000.5   2.25  -2.5   NO  3.5  5.5 5\n        55000.5   3.25  -3.5   4  4.5  6.5 nan\n        \"\"\"\n    test_file = tmp_path / \"test.qdp\"\n    with open(test_file, \"w\") as fp:\n        fp.write(example_qdp)\n\n    t = Table.read(test_file, format=\"ascii.qdp\", table_id=1, names=[\"a\", \"b\", \"c\", \"d\"])\n    assert np.allclose(t[\"a\"], [54000, 55000])\n    assert t[\"c_err\"][0] == 5.5\n    assert np.ma.is_masked(t[\"b\"][0])\n    assert np.isnan(t[\"d\"][1])\n", "    def test_read_write_error_keywords(tmp_path):\n        test_file = tmp_path / \"test.qdp\"\n        t1 = Table()\n        t1.add_column(Column(name=\"a\", data=[1, 2, 3, 4]))\n        t1.add_column(\n            MaskedColumn(\n                data=[4.0, np.nan, 3.0, 1.0], name=\"b\", mask=[False, False, False, True]\n            )\n        )\n        t1.add_column(Column(name=\"c\", data=[1.0, 2.0, 3.0, 4.0]))\n        t1.add_column(\n            Column(name=\"d\", data=[0.1, 0.2, 0.3, 0.4])\n            )\n        t1.write(test_file, err_specs={'terr': [1], 'serr': [3]})\n        t2 = Table.read(test_file, format=\"ascii.qdp\")\n        assert np.allclose(t2[\"col1\"], t1[\"a\"])\n", "def test_read_write_empty_file(tmp_path):\n    test_file = tmp_path / \"test.qdp\"\n    with open(test_file, \"w\") as fobj:\n        pass\n    with pytest.raises(IOError) as excinfo:\n        Table.read(test_file, format=\"ascii.qdp\")\n    assert \"File is empty\" in str(excinfo.value)\n", "def test_read_write_simple_err_specs(tmp_path):\n    test_file = tmp_path / \"test.qdp\"\n    t1 = Table()\n    t1.add_column(Column(name=\"a\", data=[1, 2, 3]))\n    t1.add_column(\n        MaskedColumn(\n            data=[4.0, np.nan, 3.0, 1.0], name=\"b\", mask=[False, False, False, True]\n        )\n    )\n    t1.write(test_file, format=\"ascii.qdp\", err_specs={'terr': [1]})\n    t2 = Table.read(test_file, format=\"ascii.qdp\")\n    assert np.allclose(t2[\"col1\"], t1[\"a\"])\n\n", "def test_read_write_err_specs(tmp_path):\n    test_file = tmp_path / \"test.qdp\"\n    t1 = Table()\n    t1.add_column(Column(name=\"a\", data=[1, 2, 3, 4]))\n    t1.add_column(\n        MaskedColumn(\n            data=[4.0, np.nan, 3.0, 1.0], name=\"b\", mask=[False, False, False, True]\n        )\n    )\n    t1.add_column(Column(name=\"c\", data=[1.0, 2.0, 3.0, 4.0]))\n    t1.write(test_file, err_specs={\"terr\": [1], \"serr\": [2]}, format=\"ascii.qdp\")\n    t2 = Table.read(test_file, format=\"ascii.qdp\")\n\n    assert np.allclose(t2[\"col1\"], t1[\"a\"])\n    assert np.allclose(t2[\"col2\"], t1[\"b\"])\n    assert np.allclose(t2[\"col3_perr\"], t1[\"c\"])\n    assert np.allclose(t2[\"col2_nerr\"], t1[\"b\"])\n"], "sample_548": ["    def test_colorbar_aspect(self):\n        fig, ax = plt.subplots()\n        im = ax.imshow([[0, 1], [2, 3]])\n        cb = fig.colorbar(im, aspect=2)\n        fig.draw_without_rendering()\n        assert cb.ax.get_aspect() == 2\n", "compilation error", "    def test_colorbar_mappable_copy(self):\n        fig, ax = plt.subplots()\n        im = ax.imshow([[0, 1], [2, 3]])\n        cb = fig.colorbar(im)\n        # Modifying the original mappable's norm should not affect the colorbar\n        im.set_norm(mcolors.Normalize(vmin=0, vmax=5))\n        fig.draw_without_rendering()\n        assert cb.norm.vmin == 0\n        assert cb.norm.vmax == 3\n", "    def test_colorbar_aspect(self):\n        fig, ax = plt.subplots()\n        im = ax.imshow([[0, 1], [2, 3]])\n        fig.colorbar(im, aspect=2)\n        fig.draw_without_rendering()\n        assert cb.ax.get_aspect() == 2\n", "def test_colorbar_no_ticks():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[0, 1], [2, 3]])\n    cb = fig.colorbar(im, ticks=[])\n"], "sample_577": ["    def test_legend_title_custom(self, xy):\n\n        s = pd.Series([\"a\", \"b\", \"a\", \"c\"], name=\"s\")\n        p = Plot(**xy, color=s).add(MockMark(), legend=\"my title\").plot()\n        legend, = p._figure.legends\n        assert legend.get_title().get_text() == \"my title\" \n", "    def test_multiple_default_values(self):\n\n        d1 = Default()\n        d2 = Default()\n\n        assert d1 is not d2\n", "    def test_missing_variable_no_error(self):\n\n        p = Plot().add(MockMark())\n        with pytest.raises(ValueError, match=\"Missing variables\"):\n            p.plot()\n", "    def test_legend_title(self, xy, long_df):\n\n        p = Plot(long_df, x=\"x\", y=\"y\").add(MockMark(), color=\"a\",\n                                              legend_title=\"My Legend\")\n\n        legend, = p._figure.legends\n        assert legend.get_title().get_text() == \"My Legend\"\n", "    def test_legend_variable_order(self, xy):\n\n        s1 = pd.Series([\"a\", \"b\", \"a\", \"c\"], name=\"s1\")\n        s2 = pd.Series([\"m\", \"m\", \"p\", \"m\"], name=\"s2\")\n        sem = dict(color=s1, marker=s2)\n        p = Plot(**xy).add(MockMark(), **sem).plot()\n        e1, e2 = p._legend_contents\n\n        assert e1[0] == (s1.name, s1.name)\n        assert e2[0] == (s2.name, s2.name)\n"], "sample_195": ["    def test_integer_field_range(self):\n        self.assertEqual(self.ops.integer_field_range('IntegerField'), (None, None))\n", "    def test_sql_flush_no_reset_sequences(self):\n        with transaction.atomic():\n            Author.objects.create(name='George Orwell')\n            Book.objects.create(author=Author.objects.first())\n        sql_flush = connection.ops.sql_flush(\n            no_style(),\n            [Author._meta.db_table, Book._meta.db_table],\n            reset_sequences=False,\n            allow_cascade=True,\n        )\n        connection.ops.execute_sql_flush(sql_flush)\n        with transaction.atomic():\n            self.assertFalse(Author.objects.exists())\n            self.assertFalse(Book.objects.exists())\n            \n", "    def test_convert_bool_value(self):\n        self.assertTrue(self.ops.convert_booleanfield_value(1, None, None))\n        self.assertFalse(self.ops.convert_booleanfield_value(0, None, None))\n        self.assertIsNone(self.ops.convert_booleanfield_value(None, None, None))\n        self.assertTrue(self.ops.convert_booleanfield_value(True, None, None))\n        self.assertFalse(self.ops.convert_booleanfield_value(False, None, None))\n", "    def test_adapt_datetimefield_value_aware(self):\n        now = timezone.now()\n        self.assertEqual(self.ops.adapt_datetimefield_value(now), str(now.astimezone(timezone.utc)))\n", "    def test_regex_lookup(self):\n        msg = 'This backend does not support regex lookups.'\n        with self.assertRaisesMessage(NotSupportedError, msg):\n            self.ops.regex_lookup(lookup_type='regex')\n"], "sample_659": ["    def test_handling_mix(self, message, match, expectation):\n        with expectation:\n            with pytest.raises(RuntimeError, match=match):\n                raise RuntimeError(message)\n", "    def test_exception_instance_match(self, message, match, expectation):\n        with expectation:\n            with pytest.raises(Exception, match=match):\n                raise message\n\n", "    def test_unicode_raw_string_match(self, message, match, expectation):\n        with expectation:\n            with pytest.raises(RuntimeError, match=match):\n                raise RuntimeError(message)\n", "    def test_handling_py3(self, message, match, expectation):\n        with expectation:\n            with pytest.raises(RuntimeError, match=match):\n                raise RuntimeError(message.decode())\n", "    def test_decode_handling(self, message, match, expectation):\n        with expectation:\n            with pytest.raises(RuntimeError, match=match):\n                raise RuntimeError(message)\n"], "sample_945": ["def test_pyclass_noindexentry(app):\n    text = (\".. py:class:: MyClass\\n\"\n            \"   :noindexentry:\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[])\n    assert 'MyClass' not in domain.objects\n", "def test_pymodule_in_docstring(app):\n    text = (\".. py:function:: func\\n\"\n            \"\\n\"\n            \"   This function uses the :py:mod:`os` module.\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, nodes.paragraph))\n    assert_node(doctree[2][0], nodes.reference,\n                target=\"os\")\n", "compilation error", "def test_py_module_no_members(app):\n    text = \".. py:module:: mymodule\\n\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, addnodes.index, entries=[('single', 'mymodule (module)', 'mymodule', '', None)])\n", "compilation error"], "sample_163": ["    def test_uuid_user_changelist(self):\n        u1 = UUIDUser.objects.create_user(username=\"testclient1\", email=\"test1@example.com\", password=\"password\")\n        u2 = UUIDUser.objects.create_user(username=\"testclient2\", email=\"test2@example.com\", password=\"password\")\n        response = self.client.get(reverse(\"custom_user_admin:auth_tests_uuiduser_changelist\"))\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, u1.username)\n        self.assertContains(response, u2.username)\n\n", "    def test_user_change_username(self):\n        user_change_url = reverse(\n            \"auth_test_admin:auth_user_change\", args=(self.admin.pk,)\n        )\n        data = self.get_user_data(self.admin)\n        data['username'] = 'newusername'\n\n        response = self.client.post(user_change_url, data)\n        self.assertRedirects(response, user_change_url)\n        row = LogEntry.objects.latest(\"id\")\n        self.assertEqual(row.get_change_message(), \"Changed Username.\")\n        self.assertEqual(User.objects.get(pk=self.admin.pk).username, 'newusername')\n\n", "    def test_permissions_for_uuid_user_admin(self):\n        u = UUIDUser.objects.create_user(username=\"uuid\", email=\"foo@bar.com\", password=\"test\")\n        self.assertTrue(self.client.login(username=\"uuid\", password=\"test\"))\n\n        # Unauthorized user should not be able to access changelist\n\n        response = self.client.get(reverse('custom_user_admin:auth_tests_uuiduser_changelist'))\n        self.assertEqual(response.status_code, 404)\n\n        # Make u a staff member\n        u.is_staff = True\n        u.save()\n\n        # Now they should be able to access the changelist\n\n        response = self.client.get(reverse('custom_user_admin:auth_tests_uuiduser_changelist'))\n        self.assertEqual(response.status_code, 200)\n", "    def test_uuido_user_changelist_password(self):\n        user = UUIDUser.objects.create_user(\n            username=\"testclient\", email=\"testclient@example.com\", password=\"password\"\n        )\n        response = self.client.get(\n            reverse(\"custom_user_admin:auth_tests_uuiduser_changelist\")\n        )\n        self.assertContains(response, user.username)\n        self.assertContains(response, f\"{user.email}\") \n", "    def test_password_change_form_with_custom_user_model(self):\n        u = UUIDUser.objects.create_superuser(\n            username=\"uuid\", email=\"foo@bar.com\", password=\"test\"\n        )\n        self.assertTrue(self.client.login(username=\"uuid\", password=\"test\"))\n        password_change_url = reverse(\n            \"custom_user_admin:auth_user_password_change\", args=(u.pk,)\n        )\n        response = self.client.get(password_change_url)\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, '<form method=\"post\" id=\"uuiduser_form\">')\n        self.assertContains(response, '<input type=\"password\" name=\"password1\"')\n        self.assertContains(response, '<input type=\"password\" name=\"password2\"')\n"], "sample_456": ["    def test_non_form_errors_dont_cause_all_valid_to_return_false(self):\n        data = {\n            \"choices-TOTAL_FORMS\": \"1\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-0-choice\": \"Zero\",\n            \"choices-0-votes\": \"0\",\n            \"non_form_error_field\": \"some_value\",\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertIs(all_valid((formset,)), True)\n\n", "    def test_empty_formset_management_form(self):\n        formset = formset_factory(FavoriteDrinkForm, extra=0)()\n        self.assertIsInstance(formset.management_form, ManagementForm)\n        self.assertEqual(\n            formset.management_form.as_p(),\n            '<input type=\"hidden\" name=\"form-TOTAL_FORMS\" value=\"0\">'\n            '<input type=\"hidden\" name=\"form-INITIAL_FORMS\" value=\"0\">'\n            '<input type=\"hidden\" name=\"form-MIN_NUM_FORMS\" value=\"0\">'\n            '<input type=\"hidden\" name=\"form-MAX_NUM_FORMS\" value=\"0\">',\n        )\n\n", "    def test_formset_errors(self):\n        data = {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Zero\",\n            \"choices-0-votes\": \"\",\n            \"choices-1-choice\": \"One\",\n            \"choices-1-votes\": \"\",\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(len(formset.errors), 2)\n        self.assertIn(\"votes\", formset.errors[0])\n        self.assertIn(\"votes\", formset.errors[1])\n", "    def test_empty_formset_error_message(self):\n        formset = ChoiceFormSet({})\n        self.assertEqual(\n            str(formset.non_form_errors()),\n            \"ManagementForm data is missing or has been tampered with. \"\n            \"Missing fields: form-TOTAL_FORMS, form-INITIAL_FORMS. \"\n            \"You may need to file a bug report if the issue persists.\",\n        )\n", "    def test_formset_order_when_deleting(self):\n        data = {\n            'form-TOTAL_FORMS': '3',\n            'form-INITIAL_FORMS': '0',\n            'form-0-choice': 'Zero',\n            'form-0-votes': '0',\n            'form-0-DELETE': 'on',\n            'form-1-choice': 'One',\n            'form-1-votes': '1',\n            'form-2-choice': 'Two',\n            'form-2-votes': '2',\n        }\n        formset = ChoiceFormSet(data, auto_id=False)\n        self.assertEqual(formset.cleaned_data, [\n            {'choice': 'One', 'votes': 1},\n            {'choice': 'Two', 'votes': 2},\n        ])\n"], "sample_109": ["    def test_render_empty_choices(self):\n        \"\"\"Empty choices should not result in a crash.\"\"\"\n        empty_form = AlbumForm(initial={'band': None})\n        output = empty_form.as_table()\n        self.assertIn('<select', output)\n        self.assertIn('</select>', output)\n", "    def test_optgroups(self):\n        beatles = Band.objects.create(name='The Beatles', style='rock')\n        who = Band.objects.create(name='The Who', style='rock')\n        form = AlbumForm(initial={'band': beatles.pk})\n        widget = form['band'].field.widget\n        # Test the rendering of optgroups based on the ModelChoiceIterator.\n        groups = widget.optgroups('band', [beatles.pk], attr=None)\n\n        self.assertEqual(len(groups), 1)\n        self.assertEqual(len(groups[0][1]), 1)\n        self.assertEqual(groups[0][1][0]['value'], str(beatles.pk))\n        self.assertEqual(groups[0][1][0]['label'], beatles.name)\n\n        # Test when the field is not required and allow_multiple_selected is False.\n        not_required_form = NotRequiredBandForm()\n        not_required_widget = not_required_form['band'].field.widget\n        groups = not_required_widget.optgroups('band', [], attr=None)\n        self.assertEqual(len(groups[0][1]), 1)\n        self.assertEqual(groups[0][1][0]['value'], '')\n", "    def test_media_for_debug(self):\n        \"\"\"Ensure the correct media files are loaded in debug mode.\"\"\"\n        with self.settings(DEBUG=True):\n            rel = Album._meta.get_field('band').remote_field\n            self.assertEqual(AutocompleteSelect(rel, admin.site).media._js[0],\n                             'admin/js/vendor/jquery/jquery.js')\n            self.assertEqual(AutocompleteSelect(rel, admin.site).media._js[1],\n                             'admin/js/vendor/select2/select2.full.js')\n", "    def test_render_empty_choices(self):\n        \"\"\"Empty choices should render correctly.\"\"\"\n        form = AlbumForm(\n            initial={'band': None, 'featuring': []}\n        )\n        output = form.as_table()\n        self.assertIn('<option value=\"\"></option>', output)\n", "    def test_allow_multiple_selected(self):\n        form = AlbumForm(initial={'featuring': [1, 2]})\n        output = form.as_table()\n        option1 = '<option value=\"1\" selected>The Beatles</option>'\n        option2 = '<option value=\"2\" selected>The Who</option>'\n        self.assertIn(option1, output)\n        self.assertIn(option2, output)\n"], "sample_800": ["    def test_check_estimator_fit_score_takes_y():\n        # check that check_fit_score_takes_y works on estimators \n        # that require y in fit\n        \n\n        class EstimatorWithYInFit(BaseEstimator):\n            if y is None:\n              raise ValueError(\"y should be provided in fit\")\n            return self\n\n        check_fit_score_takes_y(\"estimator\", EstimatorWithYInFit())\n\n", "    def test_check_estimator_with_deprecated_tag():\n\n        class MyEstimatorWithDeprecatedTag(BaseEstimator):\n            @deprecated(\"Deprecated for the purpose of testing\")\n                return self\n\n        # check that estimator with deprecated tag can be checked\n        check_estimator(MyEstimatorWithDeprecatedTag)\n\n", "def test_check_estimator_non_negative():\n    # Check that proper warning is raised for non-negative X\n    # when tag requires_positive_X is present\n    X = np.array([[-1., 1], [-1., 1]])\n    y = np.array([1, 2])\n    msg = \"Negative values in data passed to\"\n    assert_raises_regex(ValueError, msg, check_estimator,\n                        RequiresPositiveYRegressor(), X, y)\n", "def test_check_estimator_check_pickle():\n    # check that estimators can be pickled\n    for Estimator in [GaussianMixture, LinearRegression,\n                      RandomForestClassifier, NMF, SGDClassifier,\n                      MiniBatchKMeans]:\n        with ignore_warnings(category=(FutureWarning, DeprecationWarning)):\n            est = Estimator()\n            _set_checking_parameters(est)\n            set_random_state(est)\n            est.fit(iris.data, iris.target)\n        check_estimator(est, check_pickle=True)\n", "    def test_check_estimator_deprecated_fit_method():\n        # Tests that check_estimator correctly handles estimators with deprecated\n        # fit methods.\n\n        class TestEstimatorWithDeprecatedFitMethod(BaseEstimator):\n            @deprecated(\"Deprecated for testing purposes.\")\n                return self\n\n        check_estimator(TestEstimatorWithDeprecatedFitMethod())\n"], "sample_637": ["    def test_regex_notes(self) -> None:\n        code = \"\"\"a = 1\n                # FIX me\n                \"\"\"\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"fixme\", line=2, args=\"FIX me\", col_offset=17)\n        ):\n            self.checker.process_tokens(_tokenize_str(code))\n", "    def test_regex_notes(self) -> None:\n        code = \"\"\"a = 1\n                # BUG\n                # TODO\n                # this is not a note\n                \"\"\"\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"fixme\", line=2, args=\"BUG\", col_offset=17),\n            MessageTest(msg_id=\"fixme\", line=3, args=\"TODO\", col_offset=17),\n        ):\n            self.checker.process_tokens(_tokenize_str(code))\n\n", "    def test_regex_note_match(self) -> None:\n        code = \"\"\"a = 1\n                # FIXME\n                # fixMe\n                \"\"\"\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"fixme\", line=2, args=\"FIXME\", col_offset=17),\n            MessageTest(msg_id=\"fixme\", line=3, args=\"fixMe\", col_offset=17)\n\n        ):\n            self.checker.process_tokens(_tokenize_str(code))\n", "    def test_regex_match(self) -> None:\n        code = \"\"\"a = 1\n                # FIXME message\n                # TODO\n                # XXX\n                \"\"\"\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"fixme\", line=2, args=\"FIXME message\", col_offset=17),\n            MessageTest(msg_id=\"fixme\", line=3, args=\"TODO\", col_offset=17),\n            MessageTest(msg_id=\"fixme\", line=4, args=\"XXX\", col_offset=17),\n        ):\n            self.checker.process_tokens(_tokenize_str(code))\n", "    def test_regex_notes(self) -> None:\n        code = \"\"\"a = 1\n                # TODO\n                # FIXME\n                # TODO: something\n                \"\"\"\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"fixme\", line=2, args=\"TODO\", col_offset=17),\n            MessageTest(msg_id=\"fixme\", line=4, args=\"TODO: something\", col_offset=17),\n        ):\n            self.checker.process_tokens(_tokenize_str(code))\n"], "sample_193": ["    def test_proxy_with_fk_to_non_proxy(self):\n        A = self.create_model(\"A\")\n        B = self.create_model(\"B\", proxy=True, foreign_keys=[models.ForeignKey('A', models.CASCADE)])\n        C = self.create_model(\"C\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n        self.assertRelated(A, [B, C])\n        self.assertRelated(B, [A, C])\n        self.assertRelated(C, [A, B])\n", "    def test_custom_manager_with_related_field(self):\n        class Author(models.Model):\n            name = models.CharField(max_length=255)\n\n        class Book(models.Model):\n            author = models.ForeignKey(Author, models.CASCADE)\n            food_mgr = FoodManager('a', 'b')\n            food_qs = FoodQuerySet.as_manager()\n\n            class Meta:\n                app_label = 'migrations'\n                indexes = [models.Index(fields=['author'])]\n\n        state = ModelState.from_model(Book)\n        self.assertEqual([name for name, mgr in state.managers], ['food_mgr'])\n\n", "    def test_proxy_fk_through_base(self):\n        A = self.create_model(\"A\")\n        B = self.create_model(\"B\", bases=(A,), proxy=True, foreign_keys=[models.ForeignKey('C', models.CASCADE)])\n        C = self.create_model(\"C\")\n        self.assertRelated(A, [B, C])\n        self.assertRelated(B, [A, C])\n        self.assertRelated(C, [A, B])\n", "    def test_m2m_through_proxy(self):\n        A = self.create_model(\"A\")\n        B = self.create_model(\"B\", bases=(A,), proxy=True)\n        C = self.create_model(\"C\")\n        T = self.create_model(\"T\", foreign_keys=[\n            models.ForeignKey('A', models.CASCADE),\n            models.ForeignKey('C', models.CASCADE),\n        ])\n        A.a_1.through = T\n        B.b_1 = models.ManyToManyField('C', through=T)\n        self.assertRelated(A, [B, C, T])\n        self.assertRelated(B, [A])\n        self.assertRelated(C, [A, B, T])\n        self.assertRelated(T, [A, B, C])\n", "    def test_proxy_model_with_direct_fk(self):\n        A = self.create_model(\"A\")\n        B = self.create_model(\"B\", bases=(A,), proxy=True)\n        C = self.create_model(\"C\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n        self.assertRelated(A, [B, C])\n        self.assertRelated(B, [C])\n        self.assertRelated(C, [A, B])\n"], "sample_1124": ["compilation error", "compilation error", "compilation error", "    def test_FracElement___new__():\n        F, x, y = field(\"x,y\", ZZ)\n        f = F.__new__(F, x, y)\n        assert f == x/y\n\n", "    def test_FracElement___rpow__():\n        F, x,y = field(\"x,y\", QQ)\n\n        f, g = 1/x, 1/y\n\n        assert (x**3)**f == x**(3/x)\n\n"], "sample_878": ["def test_column_transformer_set_output_with_empty_transformers(remainder):\n    \"\"\"Test setting output with empty transformers.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\n    ct = ColumnTransformer(\n        [(\"empty1\", \"passthrough\", []), (\"empty2\", \"drop\", [])],\n        remainder=remainder,\n    )\n    ct.set_output(transform=\"pandas\")\n    X_trans = ct.fit_transform(df)\n    assert isinstance(X_trans, pd.DataFrame)\n\n", "    def test_column_transformer_set_output_with_sparse_output(remainder):\n        \"\"\"Check ColumnTransformer outputs correctly when sparse output is used.\"\"\"\n        from scipy.sparse import csr_matrix\n\n        pd = pytest.importorskip(\"pandas\")\n        X = pd.DataFrame(\n            {\n                \"pet\": pd.Series([\"dog\", \"cat\", \"snake\"], dtype=\"category\"),\n                \"color\": pd.Series([\"green\", \"blue\", \"red\"], dtype=\"object\"),\n                \"age\": [1.4, 2.1, 4.4],\n                \"height\": [20, 40, 10],\n            }\n        )\n        ct = ColumnTransformer(\n            [\n                (\n                    \"color_encode\",\n                    OneHotEncoder(sparse_output=True, dtype=\"int8\"),\n                    [\"color\"],\n                ),\n                (\"age\", StandardScaler(), [\"age\"]),\n            ],\n            remainder=remainder,\n            verbose_feature_names_out=False,\n        )\n        ct.set_output(transform=\"pandas\")\n\n        X_trans = ct.fit_transform(X)\n        assert isinstance(X_trans, pd.DataFrame)\n        assert isinstance(X_trans.iloc[:, :3], csr_matrix)\n\n", "    def test_column_transformer_set_output_with_transformer_that_fails_on_numpy(\n        remainder,", "    def test_column_transform_set_output_after_fitting_with_empty_transform(remainder):\n        pd = pytest.importorskip(\"pandas\")\n        df = pd.DataFrame(\n            {\n                \"pet\": pd.Series([\"dog\", \"cat\", \"snake\"], dtype=\"category\"),\n                \"age\": [1.4, 2.1, 4.4],\n                \"height\": [20, 40, 10],\n            }\n        )\n        ct = ColumnTransformer(\n            [\n                (\n                    \"color_encode\",\n                    OneHotEncoder(sparse_output=False, dtype=\"int16\"),\n                    [],\n                ),\n                (\"age\", StandardScaler(), [\"age\"]),\n            ],\n            remainder=remainder,\n            verbose_feature_names_out=False,\n        )\n\n        # fit without calling set_output\n        X_trans = ct.fit_transform(df)\n        assert isinstance(X_trans, np.ndarray)\n\n        ct.set_output(transform=\"pandas\")\n        X_trans_df = ct.transform(df)\n        expected_dtypes = {\"age\": \"float64\"}\n        if remainder == \"passthrough\":\n            expected_dtypes[\"height\"] = \"int64\"\n        for col, dtype in X_trans_df.dtypes.items():\n            assert dtype == expected_dtypes.get(col)\n\n", "def test_column_transformer_remainder_with_mixed_dtypes(remainder):\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame(\n        {\n            \"col1\": [1, 2, 3],\n            \"col2\": pd.to_datetime([\"2023-03-01\", \"2023-03-02\", \"2023-03-03\"]),\n            \"col3\": [\"a\", \"b\", \"c\"],\n        }\n    )\n\n    ct = ColumnTransformer(\n        [(\"scale\", StandardScaler(), [\"col1\"])],\n        remainder=remainder,\n    )\n    ct.fit(df)\n\n    X_trans = ct.transform(df)\n\n    if remainder == \"passthrough\":\n        assert X_trans.shape[1] == 3\n        assert X_trans.dtypes[1] == object  # col2 datetime\n        assert X_trans.dtypes[2] == object  # col3\n\n    elif remainder == \"drop\":\n        assert X_trans.shape[1] == 1\n        assert X_trans.dtypes[0] == \"float64\" # scaled col1\n\n"], "sample_531": ["    def test_figure_add_subplot_duplicate_keys():\n        fig = plt.figure()\n        with pytest.raises(ValueError, match=\"Duplicate keys detected\"):\n            fig.add_subplot(111, projection='polar')\n            fig.add_subplot(121, projection='polar')\n\n", "    def test_subplot_mosaic_list_of_axes(self):\n        fig, axs = plt.subplot_mosaic([['A', 'B'], ['C', 'D']], sharex=True)\n        assert len(axs) == 4\n        assert axs['A'] is not axs['B']\n\n", "def test_subfigure_constrained_layout():\n    fig = plt.figure(constrained_layout=True)\n    subfigs = fig.subfigures(2, 1)\n    subfigs[0].subplots(2, 2)\n    subfigs[1].subplots(1, 2)\n\n", "compilation error", "    def test_add_subplot_kwargs():\n        # fig.add_subplot() always creates new axes, even if axes kwargs differ.\n        fig = plt.figure()\n        ax = fig.add_subplot(1, 1, 1)\n        ax1 = fig.add_subplot(1, 1, 1)\n        assert ax is not None\n        assert ax1 is not ax\n        plt.close()\n\n        fig = plt.figure()\n        ax = fig.add_subplot(1, 1, 1, projection='polar')\n        ax1 = fig.add_subplot(1, 1, 1, projection='polar')\n        assert ax is not None\n        assert ax1 is not ax\n        plt.close()\n\n        fig = plt.figure()\n        ax = fig.add_subplot(1, 1, 1, projection='polar')\n        ax1 = fig.add_subplot(1, 1, 1)\n        assert ax is not None\n        assert ax1.name == 'rectilinear'\n        assert ax1 is not ax\n        plt.close()\n\n"], "sample_383": ["    def test_ticket_24278_empty_in(self):\n        qs = School.objects.filter(Q(pk__in=[]))\n        self.assertQuerysetEqual(qs, [])\n", "    def test_ticket_24881(self):\n\n        s1 = SimpleModel.objects.create()\n        s2 = SimpleModel.objects.create(related=s1)\n        s3 = SimpleModel.objects.create(related=s1)\n        s4 = SimpleModel.objects.create()\n\n        self.assertSequenceEqual(\n            SimpleModel.objects.filter(related__in=[s1]).distinct(), [s2, s3]\n        )\n\n", "    def test_ticket_23622_exclude(self):\n        \"\"\"\n        Make sure __pk__not_in and __notin work the same for related fields when\n        using a distinct on subquery.\n        \"\"\"\n        a1 = Ticket23605A.objects.create()\n        a2 = Ticket23605A.objects.create()\n        c1 = Ticket23605C.objects.create(field_c0=0.0)\n        Ticket23605B.objects.create(\n            modela_fk=a1,\n            field_b0=123,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a1,\n            field_b0=23,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a1,\n            field_b0=234,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a1,\n            field_b0=12,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a2,\n            field_b0=567,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a2,\n            field_b0=76,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a2,\n            field_b0=7,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket2", "    def test_ticket_23622_with_exclude(self):\n        \"\"\"\n        Make sure __pk__in and __in work the same for related fields when\n        using a distinct on subquery with an exclude.\n        \"\"\"\n        a1 = Ticket23605A.objects.create()\n        a2 = Ticket23605A.objects.create()\n        c1 = Ticket23605C.objects.create(field_c0=0.0)\n        Ticket23605B.objects.create(\n            modela_fk=a1,\n            field_b0=123,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a1,\n            field_b0=23,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a1,\n            field_b0=234,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a1,\n            field_b0=12,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a2,\n            field_b0=567,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a2,\n            field_b0=76,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a2,\n            field_b0=7,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n", "    def test_ticket_24878(self):\n        a1 = Ticket23605A.objects.create()\n        a2 = Ticket23605A.objects.create()\n        c1 = Ticket23605C.objects.create(field_c0=0.0)\n        Ticket23605B.objects.create(modela_fk=a1, field_b0=123, field_b1=True, modelc_fk=c1)\n        Ticket23605B.objects.create(modela_fk=a2, field_b0=123, field_b1=True, modelc_fk=c1)\n        qs = Ticket23605A.objects.filter(ticket23605b__field_b0=123)\n        self.assertSequenceEqual(qs.order_by('pk'), [a1, a2])\n"], "sample_164": ["    def test_uncaught_exception_with_custom_formatter(self):\n        self.assertLogsRequest(\n            url='/uncaught_exception/',\n            level='ERROR',\n            msg='Internal Server Error: /uncaught_exception/',\n            status_code=500,\n            logger='django.request',\n            exc_class=views.UncaughtException,\n        )\n", "    def test_request_logger_format(self):\n        self.client.get('/suspicious/')\n        self.assertEqual(len(mail.outbox), 1)\n        message = mail.outbox[0].body\n        self.assertIn('Report at /suspicious/', message)\n        self.assertIn('INTERNAL IP', message)\n", "    def test_server_formatter_uses_colored_output(self):\n        color_style = color.make_style('')\n        formatter = ServerFormatter()\n        formatter.style = color_style\n        log_msg = 'log message'\n        record = logging.makeLogRecord({'msg': log_msg, 'status_code': 200})\n        with self.assertLogs() as cm:\n            logger = logging.getLogger('django.server')\n            logger.info(log_msg, extra={'status_code': 200})\n        self.assertEqual(cm.output[0], color.HTTP_SUCCESS(log_msg))\n", "    def test_request_logger_custom_level(self):\n        self.assertLogsRequest(\n            url='/custom_level/',\n            level='WARNING',\n            msg='Custom Level Request',\n            status_code=200,\n            logger='django.request',\n        )\n", "    def test_log_response_custom_level(self):\n        with self.assertLogs('django.request', level='WARNING') as cm:\n            self.client.get('/custom_level/')\n            self.assertEqual(len(cm.records), 1)\n            self.assertEqual(cm.records[0].levelname, 'WARNING')\n"], "sample_310": ["    def test_simplify_regex_with_leading_slash(self):\n        self.assertEqual(simplify_regex('/^a/b'), '/a/b')\n", "    def test_simplify_regex_with_groups_and_escapes(self):\n        tests = (\n            (r'^\\(?P<slug>\\w+)\\)\\b/(\\w+)?$', '/<slug>/<var>'),\n            (r'^\\\\(?P<slug>\\w+)\\\\)\\b/(\\w+)?$', '/<slug>/<var>'),\n        )\n        for pattern, output in tests:\n            with self.subTest(pattern=pattern):\n                self.assertEqual(simplify_regex(pattern), output)\n", "    def test_simplify_regex_with_lookaround(self):\n        tests = (\n            # Positive lookahead\n            (r'^(?=\\w+)', '/'),\n            (r'^(?=a)', '/'),\n            (r'^(?=\\w+)\\w+$', '/<var>'),\n            # Negative lookahead\n            (r'^(?!\\w+)', '/'),\n            (r'^(?!a)', '/'),\n            (r'^(?!\\w+)\\w+$', '/<var>'),\n            # Positive lookbehind\n            (r'(?<=\\w+)b$', '/b'),\n            (r'(?<=a)b$', '/b'),\n            (r'(?<=\\w+)b', '/<var>b'),\n            # Negative lookbehind\n            (r'(?<!\\w+)b$', '/b'),\n            (r'(?<!a)b$', '/b'),\n            (r'(?<!\\w+)b', '/<var>b'),\n        )\n        for pattern, output in tests:\n            with self.subTest(pattern=pattern):\n                self.assertEqual(simplify_regex(pattern), output)\n", "    def test_simplify_regex_with_optional_group(self):\n        self.assertEqual(simplify_regex('^(?P<slug>\\w+)/?$'), '/<slug>/')\n\n", "    def test_simplify_regex_with_lookarounds(self):\n        tests = (\n            (r'^(?=a)b', '/b'),\n            (r'^(?!a)b', '/b'),\n            (r'^(?<=a)b', '/b'),\n            (r'^(?<!a)b', '/b'),\n            (r'a(?=b)c', '/a/c'),\n            (r'a(?<!b)c', '/a/c'),\n            (r'a(?<=b)c', '/a/c'),\n            (r'a(?!=b)c', '/a/c'),\n        )\n        for pattern, output in tests:\n            with self.subTest(pattern=pattern):\n                self.assertEqual(simplify_regex(pattern), output)\n"], "sample_91": ["    def test_page_not_found_request_path_quoted(self):\n        \"\"\"\n        The request path in the 404 template should be URL-quoted to prevent\n        content injection attacks.\n        \"\"\"\n        response = self.client.get('/path/with spaces/and?%20special%23chars/')\n        self.assertContains(\n            response, \"path: /path%2Fwith%20spaces%2Fand%3F%2520special%2523chars/\",\n            status_code=404\n        )\n", "    def test_permission_denied_context(self):\n        \"The permission_denied view should pass the exception to the context\"\n        request = self.request_factory.get('/')\n        exception = Exception(\"Testing technical 403.\")\n        response = permission_denied(request, exception)\n        self.assertContains(response, \"Testing technical 403.\", status_code=403)\n", "    def test_permission_denied_context(self):\n        request = self.request_factory.get('/')\n        response = permission_denied(request, Exception(\"Testing technical 403.\"))\n        self.assertContains(response, b\"Testing technical 403.\", status_code=403)\n\n", "    def test_permission_denied_context(self):\n        request = self.request_factory.get('/')\n        response = permission_denied(request, Exception('Testing technical 403.'))\n        self.assertContains(response, 'Testing technical 403.', status_code=403)\n", "    def test_permission_denied_context(self):\n        \"\"\"\n        The permission_denied view should pass the exception to the template.\n        \"\"\"\n        request = self.request_factory.get('/')\n        exception = Exception(\"Testing technical 403.\")\n        response = permission_denied(request, exception)\n        self.assertContains(response, \"Testing technical 403.\", status_code=403)\n\n"], "sample_228": ["    def test_formset_errors_as_dict(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertFalse(formset.is_valid())\n        expected_errors = {\n            'choices': {\n                '0': {'votes': ['This field is required.']},\n                '1': {'votes': ['This field is required.']},\n            }\n        }\n        self.assertEqual(formset.errors_as_dict(), expected_errors)\n", "    def test_formset_non_field_errors(self):\n        class CustomForm(Form):\n            name = CharField()\n\n        class CustomFormSet(BaseFormSet):\n                if self.initial_form_count() > 1:\n                    raise ValidationError(\"Too many initial forms!\")\n\n        CustomFormSet = formset_factory(CustomForm, formset=CustomFormSet)\n        data = {\n            'form-TOTAL_FORMS': '2',\n            'form-INITIAL_FORMS': '2',\n            'form-0-name': 'Form 1',\n            'form-1-name': 'Form 2',\n        }\n        formset = CustomFormSet(data)\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.non_form_errors(), ['Too many initial forms!'])\n", "    def test_formset_validate_max_with_extra(self):\n        \"\"\"Validate that validate_max works correctly with extra forms.\"\"\"\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-MAX_NUM_FORMS': '1',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice, max_num=1, validate_max=True)\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(len(formset.errors), 1)\n        self.assertEqual(list(formset.errors.keys()), ['__all__'])\n", "    def test_formset_with_initial_data_and_extra(self):\n        data = {\n            'choices-TOTAL_FORMS': '3',\n            'choices-INITIAL_FORMS': '2',\n            'choices-0-choice': 'Initial Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'Initial One',\n            'choices-1-votes': '1',\n        }\n        initial = [\n            {'choice': 'Initial Zero', 'votes': 0},\n            {'choice': 'Initial One', 'votes': 1},\n        ]\n        formset = ChoiceFormSet(data, initial=initial, auto_id=False, prefix='choices')\n        self.assertEqual(len(formset.forms), 3)\n        self.assertEqual(formset.cleaned_data, [\n            {'choice': 'Initial Zero', 'votes': 0},\n            {'choice': 'Initial One', 'votes': 1},\n            {'choice': '', 'votes': ''},\n        ])\n", "    def test_formset_with_initial_data_and_extra(self):\n        initial_data = [{'choice': 'Initial', 'votes': 10}]\n        formset = ChoiceFormSet(initial=initial_data, extra=1)\n        self.assertEqual(len(formset.forms), 2)\n        self.assertEqual(formset.forms[0].initial_data, {'choice': 'Initial', 'votes': 10})\n        self.assertEqual(formset.forms[1].initial_data, {})\n        \n"], "sample_285": ["    def test_finder_storage_instance_creation(self):\n        \"\"\"Test that BaseStorageFinder raises an error if storage is not provided or a valid instance.\"\"\"\n        class CustomStorageFinder(BaseStorageFinder):\n            storage = None\n        with self.assertRaises(ImproperlyConfigured):\n            CustomStorageFinder()\n\n        class CustomStorageFinder2(BaseStorageFinder):\n            storage = lambda: 'not a storage'\n        with self.assertRaises(ImproperlyConfigured):\n            CustomStorageFinder2()\n\n        class CustomStorageFinder3(BaseStorageFinder):\n            storage = mock.MagicMock()\n        finder = CustomStorageFinder3()\n        self.assertIsInstance(finder.storage, mock.MagicMock)\n\n", "    def test_finders_with_invalid_storage(self):\n        class MyFinder(BaseStorageFinder):\n            storage = 'invalid_storage'\n\n        with mock.patch('django.contrib.staticfiles.finders.get_finders', lambda: [MyFinder()]):\n            with self.assertRaisesMessage(ImproperlyConfigured, \"The storage backend of the staticfiles finder MyFinder doesn't have a valid location.\"):\n                check_finders(None)\n\n", "    def test_find_location_with_prefix(self):\n        finder = get_finder('django.contrib.staticfiles.finders.FileSystemFinder')\n        with self.settings(STATICFILES_DIRS=[('prefix', TEST_ROOT / 'static')]):\n            path = finder.find_location(TEST_ROOT / 'static', 'images/logo.png', 'prefix')\n            self.assertEqual(path, str(TEST_ROOT / 'static/images/logo.png'))\n\n            path = finder.find_location(TEST_ROOT / 'static', 'prefix/images/logo.png', 'prefix')\n            self.assertEqual(path, str(TEST_ROOT / 'static/prefix/images/logo.png'))\n", "    def test_filesystem_finder_check_with_invalid_staticfiles_dirs(self):\n        with self.settings(STATICFILES_DIRS=[123]):\n            errors = get_finder('django.contrib.staticfiles.finders.FileSystemFinder').check()\n            self.assertEqual(len(errors), 1)\n            self.assertIsInstance(errors[0], TypeError)\n", "    def test_get_finder_lru_cache(self):\n        finder1 = get_finder('django.contrib.staticfiles.finders.FileSystemFinder')\n        finder2 = get_finder('django.contrib.staticfiles.finders.FileSystemFinder')\n        self.assertIs(finder1, finder2)\n"], "sample_517": ["compilation error", "compilation error", "compilation error", "compilation error", "    def test_pdf_font42_kerning():\n        plt.rcParams['pdf.fonttype'] = 42\n        plt.figure()\n        plt.figtext(0.1, 0.5, \"ATAVATAVATAVATAVATA\", size=30)\n"], "sample_1070": ["compilation error", "compilation error", "compilation error", "def test_log_nonpositive():\n    x = Symbol('x')\n    assert log(x, evaluate=False).is_nonpositive is None\n    assert log(x**2).is_nonpositive is False\n    assert log(0, evaluate=False).is_nonpositive is True\n    assert log(-1, evaluate=False).is_nonpositive is None\n    assert log(S.Half, evaluate=False).is_nonpositive is False\n    assert log(-S.Half, evaluate=False).is_nonpositive is True\n", "compilation error"], "sample_329": ["    def test_custom_operation_multiple_args(self):\n        \"\"\"\n        Tests serialization of custom operations with multiple arguments\n        \"\"\"\n        migration = type(\"Migration\", (migrations.Migration,), {\n            \"operations\": [\n                custom_migration_operations.operations.MultipleArgsOperation('arg1', 'arg2'),\n            ],\n            \"dependencies\": []\n        })\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        result = self.safe_exec(output)\n        self.assertIn('custom_migration_operations', result)\n        self.assertEqual(result['custom_migration_operations'].operations.MultipleArgsOperation.deserialize(\n            ('arg1', 'arg2')\n        ), ('arg1', 'arg2'))\n\n", "    def test_serialize_proxy_model(self):\n        class MyProxyModel(models.Model):\n            class Meta:\n                proxy = True\n\n        self.assertSerializedEqual(MyProxyModel)\n", "    def test_serialize_proxy_model(self):\n        class MyProxyModel(models.Model):\n            class Meta:\n                proxy = True\n\n        self.assertSerializedEqual(MyProxyModel)\n\n", "    def test_custom_migration_operations_no_module(self):\n        # Test a custom operation where the module is not available.\n        migration = type(\"Migration\", (migrations.Migration,), {\n            \"operations\": [\n                custom_migration_operations_no_module.operations.TestOperation(),\n            ],\n            \"dependencies\": []\n        })\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        result = self.safe_exec(output)\n        self.assertIn(\"custom_migration_operations_no_module\", result)\n\n", "    def test_serialize_decimal_decimal(self):\n        self.assertSerializedEqual(decimal.Decimal('12.34'))\n        self.assertSerializedResultEqual(\n            decimal.Decimal('12.34'),\n            (\"decimal.Decimal('12.34')\", {'import decimal'})\n        )\n"], "sample_430": ["    def test_alter_model_with_related_name_change(self):\n        initial_author = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n            ],\n        )\n        author_with_related_name_change = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n            ],\n            options={'related_name': 'authors'},\n        )\n\n        changes = self.get_changes([initial_author], [author_with_related_name_change])\n\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelOptions\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", options={'related_name': 'authors'})\n", "    def test_add_unique_constraint_with_options(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.AlterModelOptions(\n                    \"Author\", options={\"ordering\": [\"-name\"], \"unique_together\": [(\"name\", \"age\")]}\n                ),\n            ]\n        migration = Migration(\"0001_initial\", \"testapp\")\n        self.assertEqual(migration.suggest_name(), \"author\")\n", "    def test_multiple_field_changes_in_one_operation(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.AlterField(\n                    \"Person\",\n                    \"name\",\n                    field=models.CharField(max_length=200, unique=True),\n                ),\n                migrations.AlterField(\n                    \"Person\",\n                    \"age\",\n                    field=models.IntegerField(null=True, blank=True),\n                ),\n            ]\n\n        migration = Migration(\"0002_person_name_age\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person_name_age\")\n", "    def test_alter_unique_together(self):\n        initial_author = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"age\", models.IntegerField()),\n            ],\n            {\n                \"unique_together\": {(\"name\", \"age\")},\n            },\n        )\n        author_new_constraints = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"age\", models.IntegerField()),\n            ],\n            {\n                \"unique_together\": {(\"age\",)},\n            },\n        )\n        changes = self.get_changes([initial_author], [author_new_constraints])\n\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"testapp\",\n            0,\n            [\"AlterUniqueTogether\"],\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            0,\n            name=\"author\",\n            unique_together={(\"age\",)},\n        )\n", "    def test_custom_operation_with_name(self):\n        OperationWithCustomName = type(\n            \"OperationWithCustomName\", (migrations.MigrationOperation,), {\"name\": \"custom_operation\"}\n        )\n        class Migration(migrations.Migration):\n            operations = [OperationWithCustomName(\"Person\", fields=[])]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"custom_operation_person\")\n"], "sample_406": ["    def test_refresh_with_deferred_accessor(self):\n        a = Article.objects.create(pub_date=datetime.now())\n        a.save()\n        a.refresh_from_db(fields=[\"headline\"])\n        with self.assertNumQueries(1):\n            self.assertEqual(a.pub_date, a.get_pub_date())\n", "    def test_select_related_refresh(self):\n        a = Article.objects.create(pub_date=datetime(2005, 7, 28))\n        s = SelfRef.objects.create(article=a)\n        # refresh_from_db() without fields=[...]\n        a1_prefetched = Article.objects.select_related(\"selfref\").first()\n        self.assertEqual(a1_prefetched.selfref, s)\n        s.article = None\n        s.save()\n        # Relation is cleared and prefetch cache is stale.\n        self.assertIsNone(a1_prefetched.selfref)\n        a1_prefetched.refresh_from_db()\n        # Cache was cleared and new results are available.\n        self.assertIsNone(a1_prefetched.selfref)\n        # refresh_from_db() with fields=[...]\n        a2_prefetched = Article.objects.select_related(\"selfref\").first()\n        self.assertIsNone(a2_prefetched.selfref)\n        s.article = a\n        s.save()\n        # Relation is added and prefetch cache is stale.\n        self.assertIsNone(a2_prefetched.selfref)\n        a2_prefetched.refresh_from_db(fields=[\"selfref\"])\n        # Cache was cleared and new results are available.\n        self.assertEqual(a2_prefetched.selfref, s)\n", "    def test_refresh_m2m_related(self):\n        a = Article.objects.create(headline=\"Article A\")\n        a.tags.add('tag1', 'tag2')\n        a.refresh_from_db(fields=['tags'])\n        self.assertEqual(a.tags.all()[0].name, 'tag1')\n\n", "    def test_refresh_m2m_cache(self):\n        \"\"\"refresh_from_db() clears cached m2m relations.\"\"\"\n        a = Article.objects.create(pub_date=datetime(2005, 7, 28))\n        t1 = Tag.objects.create(name=\"django\")\n        t2 = Tag.objects.create(name=\"python\")\n        a.tags.add(t1, t2)\n        self.assertEqual(list(a.tags.all()), [t1, t2])\n        t3 = Tag.objects.create(name=\"framework\")\n        a.tags.add(t3)\n        a.refresh_from_db()\n        self.assertEqual(list(a.tags.all()), [t1, t2, t3])\n", "    def test_refresh_m2m(self):\n        a = Article.objects.create(headline=\"Article A\", pub_date=datetime(2005, 7, 28))\n        t1 = Tag.objects.create(name=\"tag1\")\n        a.tags.add(t1)\n        a.refresh_from_db()\n        self.assertEqual(a.tags.count(), 1)\n        self.assertEqual(a.tags.first(), t1)\n"], "sample_40": ["compilation error", "def test_doppler_optical():\n    nu0 = (u.GHz * 10).to(u.Hz)\n\n    z = 0.1\n    v = (\n        constants.c * z / (1 + z)\n    )\n\n    v_from_doppler = (\n        nu0.to(u.km/u.s, equivalencies=u.doppler_optical(nu0))\n    )\n    assert_allclose(v.to(u.km/u.s).value, v_from_doppler.value)\n\n    # Now test the inverse\n    nu = nu0 * (1 + z)\n    nu_back = nu.to(u.Hz, equivalencies=u.doppler_optical(nu0))\n\n    assert_allclose(nu0.value, nu_back.value)\n\n", "compilation error", "compilation error", "def test_equivalencies_deprecation():\n    with pytest.warns(DeprecationWarning):\n        u.m.to(u.cm, equivalencies=u.dimensionless_angles())\n"], "sample_644": ["    def test_incorrect_alias(self) -> None:\n        module = astroid.MANAGER.ast_from_module_name(\"incorrect_alias\", REGR_DATA)\n        import_from = module.body[0]\n\n        msg = MessageTest(\n            msg_id=\"invalid-alias\",\n            node=import_from.names[0],\n            args=(\"bad_alias\", \"good_module\"),\n            line=1,\n            col_offset=12,\n            end_line=1,\n            end_col_offset=23,\n        )\n        with self.assertAddsMessages(msg):\n            self.checker.visit_importfrom(import_from)\n", "    def test_cyclic_import_with_type_checking_guard(self) -> None:\n        module = astroid.MANAGER.ast_from_module_name(\n            \"cyclic_import_with_type_checking_guard\", REGR_DATA\n        )\n        import_from = module.body[1].body[0]\n\n        with self.assertNoMessages():\n            self.checker.visit_importfrom(import_from)\n", "    def test_import_unused(self) -> None:\n        module = astroid.MANAGER.ast_from_module_name(\"unused_import\", REGR_DATA)\n        import_from = module.body[0]\n        with self.assertNoMessages():\n            self.checker.visit_importfrom(import_from)\n        self.checker.linter.stats.used_imports = {\"unused_import.module\"}\n        msg = MessageTest(\n            msg_id=\"unused-import\",\n            node=import_from.names[0],\n            args=\"module\",\n            line=1,\n            col_offset=7,\n            end_line=1,\n            end_col_offset=13,\n        )\n        with self.assertAddsMessages(msg):\n            self.checker.visit_importfrom(import_from)\n\n", "    def test_import_error_in_fallback_block(capsys: CaptureFixture[str]) -> None:\n        \"\"\"Test import-error message is suppressed in fallback blocks.\"\"\"\n        Run(\n            [\n                f\"{os.path.join(REGR_DATA, 'fallback_block')}\",\n                \"-d all\",\n                \"--ignore=I0011\",\n            ],\n            exit=False,\n        )\n        output, errors = capsys.readouterr()\n        assert \"ImportError: No module named 'nonexistent'\" not in output\n        assert len(errors) == 0\n", "    def test_consider_using_from_import(self) -> None:\n        module = astroid.MANAGER.ast_from_module_name(\"consider_from_import\", REGR_DATA)\n        import_from = module.body[0]\n\n        msg = MessageTest(\n            msg_id=\"consider-using-from-import\",\n            node=import_from,\n            args=(\"package\", \"module\"),\n            line=1,\n            col_offset=0,\n            end_line=1,\n            end_col_offset=28,\n        )\n        with self.assertAddsMessages(msg):\n            self.checker.visit_importfrom(import_from)\n"], "sample_1036": ["compilation error", "def test_matmul_scalar_after_Add():\n    X = Matrix([[1, 2], [3, 4]])\n    assert MatMul(2+3, X).doit() == 5*X\n", "compilation error", "def test_matmul_empty_matrix():\n    assert MatMul(Matrix([]), Matrix([[]])).doit() == Matrix([])\n", "compilation error"], "sample_584": ["    def test_auto_combine_no_coords_empty_datasets(self):\n        objs = [Dataset(), Dataset()]\n        with pytest.warns(FutureWarning, match=\"supplied are empty\"):\n            auto_combine(objs)\n", "    def test_manual_combine_with_dataarrays(self):\n        da1 = DataArray([1, 2], dims=['x'])\n        da2 = DataArray([3, 4], dims=['x'])\n        da3 = DataArray([5, 6], dims=['x'])\n        actual = combine_nested([[da1, da2], [da3]], concat_dim='x')\n        expected = DataArray([1, 2, 3, 4, 5, 6], dims=['x'])\n        assert_identical(expected, actual)\n", "    def test_auto_combine_empty_input(self):\n        assert_identical(Dataset(), auto_combine([]))\n", "    def test_auto_combine_with_fill_value_and_coords(self):\n        datasets = [Dataset({'a': ('x', [2, 3]), 'x': [1, 2]}),\n                    Dataset({'a': ('x', [1, 2]), 'x': [0, 1]})]\n        with pytest.warns(FutureWarning, match=\"`fill_value`\"):\n            auto_combine(datasets, concat_dim='t', fill_value=np.nan) \n", "    def test_auto_combine_no_concat_dim(self):\n        objs = [Dataset({'x': [0]}), Dataset({'y': [1]})]\n        with pytest.warns(FutureWarning, match=\"`concat_dim` parameter\"):\n            auto_combine(objs)\n"], "sample_159": ["    def test_permission_name_length_with_long_model_name(self):\n        class VeryLongModelName(models.Model):\n            pass\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'auth.E007')\n        self.assertIn(\n            'The verbose_name of model '\n            \"'auth_tests.VeryLongModelName' must be at most \",\n            errors[0].msg\n        )\n\n", "    def test_required_fields_is_tuple(self):\n        \"\"\"REQUIRED_FIELDS should be a tuple.\"\"\"\n        class CustomUserRequiredFieldsIsTuple(AbstractBaseUser):\n            username = models.CharField(max_length=30, unique=True)\n            date_of_birth = models.DateField()\n\n            USERNAME_FIELD = 'username'\n            REQUIRED_FIELDS = ('date_of_birth',)\n\n        errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n        self.assertEqual(errors, [])\n", "    def test_custom_permissions_too_long_after_truncation(self):\n        class Checked(models.Model):\n            long_field = models.CharField(max_length=255)\n            class Meta:\n                verbose_name = 'This is a very long verbose name' * 5\n                permissions = [\n                    ('change_checked', 'Can edit checked'),\n                    ('delete_checked', 'Can delete checked'),\n                ]\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The verbose_name of model 'auth_tests.Checked' must be at most 244 \"\n                \"characters for its builtin permission names to be at most 255 characters.\",\n                obj=Checked,\n                id='auth.E007',\n            ),\n        ])\n\n", "    def test_permission_name_and_code_name_length(self):\n        class Checked(models.Model):\n            class Meta:\n                verbose_name = 'Checked Model'\n                permissions = [\n                    ('long_permission_codename',\n                     'This permission name is intentionally very long to test the length check'),\n                ]\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The verbose_name of model 'auth_tests.Checked' must be at most 217 \"\n                \"characters for its builtin permission names to be at most 255 characters.\",\n                obj=Checked,\n                id='auth.E007',\n            ),\n            checks.Error(\n                \"The permission codenamed 'long_permission_codename' of model 'auth_tests.Checked' \"\n                \"is longer than 100 characters.\",\n                obj=Checked,\n                id='auth.E012',\n            ),\n        ])\n", "    def test_username_non_unique_custom_backend(self):\n        \"\"\"\n        A non-unique USERNAME_FIELD doesn't raise an error if a custom\n        authentication backend is used.\n        \"\"\"\n        with self.settings(AUTHENTICATION_BACKENDS=['my.custom.backend']):\n            errors = checks.run_checks()\n            self.assertEqual(errors, [])\n\n"], "sample_716": ["    def test_ridge_cv_empty_cv(self):\n        X = np.array([[1, 2], [3, 4]])\n        y = np.array([1, 2])\n        ridge = RidgeCV(cv=KFold(n_splits=0))\n        with pytest.raises(ValueError) as excinfo:\n             ridge.fit(X, y)\n        assert \"cv\" in str(excinfo.value)\n\n", "    def test_ridgecv_alpha_zero():\n        # Test RidgeCV with alpha=0, ensure it doesn't raise warnings.\n        ridgecv = RidgeCV(alphas=[0, 1], cv=5)\n        ridgecv.fit(X_diabetes, y_diabetes)\n        # Assert that the model works without raising warnings\n        assert ridgecv.alpha_ == 0\n\n", "    def test_ridgecv_scoring_null():\n        ridgecv = RidgeCV(scoring=None)\n        ridgecv.fit(X_diabetes, y_diabetes)\n        assert_equal(ridgecv.alpha_, ridgecv.alphas_[0])\n", "    def test_ridgecv_sample_weight_empty_array():\n        # Test that RidgeCV handles empty sample weight arrays gracefully.\n        rng = np.random.RandomState(0)\n        n_samples = 5\n        n_features = 3\n        X = rng.randn(n_samples, n_features)\n        y = rng.randn(n_samples)\n\n        # Empty sample_weight array\n        sample_weight = np.array([])\n\n        ridgecv = RidgeCV(alphas=[0.1, 1.0, 10.0])\n\n        # Should not raise an error\n        ridgecv.fit(X, y, sample_weight=sample_weight)\n\n", "    def test_ridgecv_warm_start(self):\n        rng = np.random.RandomState(42)\n        X = rng.randn(100, 5)\n        y = rng.randn(100)\n        alphas = [0.1, 1.0, 10.0]\n\n        ridgecv = RidgeCV(alphas=alphas, cv=5)\n        ridgecv.fit(X, y)\n        initial_coef_ = ridgecv.coef_.copy()\n\n        ridgecv_warm_start = RidgeCV(alphas=alphas, cv=5, warm_start=True)\n        ridgecv_warm_start.fit(X, y)\n        warm_start_coef_ = ridgecv_warm_start.coef_\n\n        assert_array_almost_equal(initial_coef_, warm_start_coef_)\n"], "sample_1017": ["compilation error", "compilation error", "def test_BooleanFunction_integrate():\n    assert And(x,y).integrate(x) == Piecewise((0, Eq(False, y)),\n                                           (x, True))\n\n", "compilation error", "def test_ITE_with_subs():\n    x, y, z = symbols('x y z')\n    expr = ITE(x, y, z)\n    assert expr.subs(x, True) == y\n    assert expr.subs(x, False) == z\n    assert expr.subs({x: True, y: 1, z: 2}) == 1\n    assert expr.subs({x: False, y: 1, z: 2}) == 2\n\n"], "sample_971": ["def test_message_prefix_filter(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    filter = logging.MessagePrefixFilter(\"PREFIX:\")\n    logger.addFilter(filter)\n\n    logger.warning('message1')\n    assert 'WARNING: PREFIX: message1' in warning.getvalue()\n\n    logger.removeFilter(filter)\n    logger.warning('message2')\n    assert 'WARNING: message2' in warning.getvalue()\n", "def test_LogCollector(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    collector = logging.LogCollector()\n    with collector.collect():\n        logger.info('message1')\n        logger.warning('message2')\n        logger.error('message3')\n\n    assert len(collector.logs) == 3\n    assert collector.logs[0].levelname == 'INFO'\n    assert collector.logs[1].levelname == 'WARNING'\n    assert collector.logs[2].levelname == 'ERROR'\n", "def test_logging_log_record(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    logger.info('message', extra={'key': 'value'})\n\n    assert 'key: value' in status.getvalue()\n", "def test_pending_logging_and_parallel(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n        with logging.pending_logging():\n            logger.info('message1')\n            logger.warning('message2')\n\n    tasks = ParallelTasks(1)\n    tasks.add_task(child_process)\n    tasks.join()\n\n    assert 'message1\\n' not in status.getvalue()\n    assert 'WARNING: message2' not in warning.getvalue()\n", "    def test_warning_location_with_parallel(app, status, warning):\n        logging.setup(app, status, warning)\n        logger = logging.getLogger(__name__)\n\n            logger.warning('message1', location=('index', 10))\n\n        tasks = ParallelTasks(1)\n        tasks.add_task(child_process)\n        tasks.join()\n        assert 'index.txt:10: WARNING: message1' in warning.getvalue()\n"], "sample_796": ["    def test_huber_intercept_only():\n        # Test that huber regressor works when only intercept is fitted\n        X = np.ones((10, 1))\n        y = np.arange(10)\n        huber = HuberRegressor(fit_intercept=True, alpha=0.0)\n        huber.fit(X, y)\n        assert huber.coef_ == 0\n        assert_array_almost_equal(huber.intercept_, 4.5)\n", "    def test_huber_coef_zero():\n        # Test that if there's no signal and only noise, the coefficients are\n        # close to zero.\n        rng = np.random.RandomState(0)\n        X = rng.randn(100, 10)\n        y = rng.randn(100)\n        huber = HuberRegressor().fit(X, y)\n        assert_almost_equal(huber.coef_, np.zeros(10), 2)\n\n", "def test_huber_epsilon():\n    # Test that changing epsilon results in different coefficients\n\n    X, y = make_regression_with_outliers()\n    huber_low_epsilon = HuberRegressor(\n        fit_intercept=True, alpha=0.1, epsilon=0.5, max_iter=100)\n    huber_low_epsilon.fit(X, y)\n    huber_high_epsilon = HuberRegressor(\n        fit_intercept=True, alpha=0.1, epsilon=2.0, max_iter=100)\n    huber_high_epsilon.fit(X, y)\n    assert_array_almost_equal(huber_low_epsilon.coef_,\n                              huber_high_epsilon.coef_, 2)\n", "    def test_huber_intercept():\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([3, 7, 11])\n        huber = HuberRegressor(fit_intercept=False)  \n        huber.fit(X, y)\n\n        assert huber.intercept_ == 0.0\n\n        huber = HuberRegressor(fit_intercept=True)  \n        huber.fit(X, y)\n        assert huber.intercept_ != 0.0\n", "def test_huber_epsilon():\n    X, y = make_regression_with_outliers()\n    huber = HuberRegressor(fit_intercept=True)\n    huber.fit(X, y)\n    huber_coef = huber.coef_\n    huber_intercept = huber.intercept_\n\n    # Test different epsilon values\n    huber.epsilon = 1.0\n    huber.fit(X, y)\n    assert_array_almost_equal(huber.coef_, huber_coef, 2)\n    assert_array_almost_equal(huber.intercept_, huber_intercept, 2)\n\n    huber.epsilon = 2.0\n    huber.fit(X, y)\n    assert_array_almost_equal(huber.coef_, huber_coef, 2)\n    assert_array_almost_equal(huber.intercept_, huber_intercept, 2)\n"], "sample_573": ["    def test_low_unique_x(self, df):\n\n        groupby = GroupBy([\"group\"])\n        df[\"x\"] = df[\"group\"].astype('category').cat.codes\n        res = PolyFit(order=2)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n        assert res.empty\n", "    def test_low_nunique(self, df):\n        groupby = GroupBy([\"group\"])\n        df = df.head(3)\n        res = PolyFit(order=2)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n        assert res.empty\n", "    def test_low_uniqueness(self, df):\n\n        df = df.head(3)  # Ensure less than order unique x values\n        groupby = GroupBy([\"group\"])\n        res = PolyFit(order=2)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n        assert res.empty\n", "    def test_order_too_high(self, df):\n\n        groupby = GroupBy([\"group\"])\n        df = df.head(3)\n        res = PolyFit(order=2)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n        assert res.empty\n", "    def test_low_unique_x(self, df):\n\n        df[\"x\"] = np.repeat(np.arange(3), 34)\n        groupby = GroupBy([\"group\"])\n\n        res = PolyFit()(df[[\"x\", \"y\"]], groupby, \"x\", {})\n        assert res.shape[0] == 0\n"], "sample_741": ["def test_grid_search_with_refit_false():\n    X, y = make_classification(n_samples=50, n_features=10, random_state=0)\n    clf = LogisticRegression()\n\n    gs = GridSearchCV(clf, {'C': [0.1, 1, 10]}, refit=False, scoring='accuracy')\n    gs.fit(X, y)\n    assert_true(hasattr(gs, 'best_estimator_'))\n    assert_true(hasattr(gs, 'best_params_'))\n    assert_false(hasattr(gs, 'best_score_'))\n", "    def test_grid_search_with_empty_param_grid():\n        grid_search = GridSearchCV(SVC(gamma='scale'), param_grid={}, cv=2)\n        assert_raises(ValueError, grid_search.fit, X, y)\n", "def test_grid_search_with_precomputed_kernel():\n    # Test GridSearchCV with precomputed kernels\n    X = make_blobs(n_samples=20, n_features=10, random_state=0)\n    # Precompute kernel\n    kernel_matrix = pairwise_kernels(X, metric='linear')\n\n    clf = SVC(kernel='precomputed')\n    gs = GridSearchCV(clf, {'C': [0.1, 1, 10]}, cv=2)\n\n    gs.fit(kernel_matrix, y)\n    assert_equal(gs.best_params_, {'C': 10})\n", "    def test_grid_search_with_non_default_scoring():\n        X, y = make_classification(random_state=0)\n        clf = LogisticRegression(random_state=0)\n        scoring = {'accuracy': 'accuracy', 'f1': make_scorer(f1_score)}\n        grid_search = GridSearchCV(clf, {'C': [0.1, 1, 10]}, scoring=scoring,\n                                   refit='f1')\n        grid_search.fit(X, y)\n        assert_equal(grid_search.best_score_,\n                     grid_search.cv_results_['mean_test_f1'][\n                         grid_search.best_index_])\n\n", "compilation error"], "sample_877": ["def test_isotonic_regression_pickle_with_pandas():\n    \"\"\"Test if IsotonicRegression can be pickled and unpickled with pandas.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    X, y = make_regression(n_samples=10, n_features=1, random_state=42)\n    X = pd.DataFrame(X)\n    with sklearn.config_context(transform_output=\"pandas\"):\n        regressor = IsotonicRegression()\n        regressor.fit(X, y)\n        y_pred = regressor.predict(X)\n\n        pickled_regressor = pickle.dumps(regressor)\n        unpickled_regressor = pickle.loads(pickled_regressor)\n        unpickled_y_pred = unpickled_regressor.predict(X)\n        assert_array_equal(y_pred, unpickled_y_pred)\n", "    def test_isotonic_regression_empty_input():\n        \"\"\"Test IsotonicRegression with empty input arrays.\"\"\"\n        X = np.array([])\n        y = np.array([])\n\n        with pytest.raises(ValueError, match='Input arrays should not be empty'):\n            IsotonicRegression().fit(X, y)\n", "    def test_isotonic_regression_empty_input():\n        \"\"\"\n        Check that IsotonicRegression handles empty input data gracefully.\n\n        This test ensures that the code doesn't crash or raise unexpected errors\n        when given empty input arrays for X and y.\n        \"\"\"\n        X = np.array([])\n        y = np.array([])\n        ireg = IsotonicRegression()\n        with pytest.raises(ValueError, match=\"Input arrays should not be empty\"):\n            ireg.fit(X, y)\n", "    def test_isotonic_regression_empty_input():\n        iso = IsotonicRegression()\n        X = np.array([])\n        y = np.array([])\n        with pytest.raises(ValueError):\n            iso.fit(X, y)\n", "compilation error"], "sample_5": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1188": ["def test_pretty_printing_vectors_with_custom_names():\n    B = CoordSys3D('B', vector_names=['x', 'y', 'z'],\n                   variable_names=['i', 'j', 'k'])\n    v = B.i + 2*B.j + 3*B.k\n    assert pretty(v) == 'i_B + 2 j_B + 3 k_B'\n\n\n\n", "compilation error", "def test_nested_vectors():\n    N = CoordSys3D('N')\n    v = N.i\n    u = v + N.j\n    w = u | N.k\n    assert pretty(w) == '(N.i + N.j)|k_N'\n    assert upretty(w) == '(i_N + j_N)|k_N'\n", "def test_pretty_printing_of_zero_vector():\n    assert pretty(Vector.zero) == '0'\n    assert upretty(Vector.zero) == '0'\n", "def test_pretty_print_unicode_d():\n    assert upretty(d[0]) == '(0|0)'\n    assert upretty(d[1]) == '(i_N|k_N)'\n    assert upretty(d[5]) == '(a) (i_N|k_N) + (-b) (j_N|k_N)'\n    assert upretty(d[7]) == upretty_d_7\n    assert upretty(d[10]) == '(cos(a)) (i_C|k_N) + (-sin(a)) (j_C|k_N)'\n"], "sample_813": ["def test_ard_regression_threshold_lambda():\n    # Test ARDRegression with threshold_lambda parameter\n    X = np.array([[1, 1], [2, 2], [3, 3], [4, 4]])\n    y = np.array([1, 2, 3, 4])\n    clf = ARDRegression(threshold_lambda=10)\n    clf.fit(X, y)\n    assert clf.lambda_.shape[0] == 2  # Only two weights should be kept\n\n", "    def test_ard_regression_coef_pruning():\n        # Test if ARDRegression correctly prunes irrelevant coefficients\n        X = np.array([[1, 0, 0, 0],\n                      [0, 2, 0, 0],\n                      [0, 0, 3, 0],\n                      [0, 0, 0, 4]])\n        y = np.array([1, 2, 3, 4])\n        clf = ARDRegression()\n        clf.fit(X, y)\n\n        # Assert that only the first four coefficients are non-zero\n        relevant_coefs = np.count_nonzero(clf.coef_)\n        assert_equal(relevant_coefs, 4)\n\n", "    def test_ard_regression_threshold_lambda():\n        # Test ARDRegression with threshold_lambda parameter.\n        # Check if the coefficients corresponding to features with lambda_ >\n        # threshold_lambda are set to zero.\n        n_samples, n_features = 10, 5\n        random_state = check_random_state(42)\n        X = random_state.random_sample((n_samples, n_features))\n        y = random_state.random_sample(n_samples)\n\n        # Set a threshold_lambda value to prune some features\n        threshold_lambda = 100\n\n        clf = ARDRegression(threshold_lambda=threshold_lambda)\n        clf.fit(X, y)\n        \n        # Check if some coefficients are zeroed out\n        assert np.any(clf.coef_ == 0)\n\n", "    def test_ard_regression_with_pruning():\n        # Test ARDRegression's pruning mechanism by creating\n        # a synthetic dataset where only a subset of features is relevant.\n        n_samples = 100\n        n_features = 20\n        relevant_features = [0, 5, 10]\n        random_state = check_random_state(42)\n        X = random_state.randn(n_samples, n_features)\n        coef_true = np.zeros(n_features)\n        coef_true[relevant_features] = np.array([2.0, -1.5, 0.8])\n        y = np.dot(X, coef_true) + random_state.randn(n_samples) * 0.5\n        clf = ARDRegression(threshold_lambda=1e3)\n        clf.fit(X, y)\n\n        # Check that only relevant features have non-zero coefficients\n        assert_almost_equal(clf.coef_[relevant_features], coef_true[relevant_features],\n                            decimal=1)\n        assert_array_almost_equal(clf.coef_[~np.isin(np.arange(n_features), relevant_features)],\n                                  np.zeros(n_features - len(relevant_features)),\n                                  decimal=1)\n\n", "    def test_ard_regression_n_features_greater_n_samples():\n        # Test ARDRegression when the number of features is greater than the\n        # number of samples.\n\n        n_samples = 5\n        n_features = 10\n        random_state = check_random_state(42)\n        X = random_state.random_sample((n_samples, n_features))\n        y = random_state.random_sample(n_samples)\n\n        clf = ARDRegression()\n        clf.fit(X, y)\n        assert clf.coef_.shape[0] == n_features\n"], "sample_1106": ["def test_matmul_zero_dimension():\n    assert MatMul(\n        ZeroMatrix(0, 0),\n        ZeroMatrix(0, 0),\n    ) == ZeroMatrix(0, 0)\n", "def test_MatAdd_identity():\n    assert MatAdd.identity == GenericZeroMatrix()\n    assert MatAdd.identity != 0\n\n", "def test_matmul_scalar_on_right():\n    X = Matrix([[1, 2], [3, 4]])\n    assert MatMul(X, 2).doit() == X*2\n", "def test_matmul_empty():\n    assert MatMul() == GenericIdentity()\n", "def test_zero_matrix_in_matmul():\n    assert MatMul(C, ZeroMatrix(m, k), evaluate=False) == ZeroMatrix(n, k)\n    assert MatMul(ZeroMatrix(n, k), C, evaluate=False) == ZeroMatrix(n, k)\n\n"], "sample_1054": ["    def test_ComplexRegion_is_empty():\n        assert ComplexRegion(Interval(0, 1)*Interval(0, 0)).is_empty == True\n        assert ComplexRegion(Interval(0, 1)*Interval(2, 1)).is_empty == True\n        assert ComplexRegion(Interval(2, 1)*Interval(0, 1)).is_empty == True\n        assert ComplexRegion(Interval(1, 2)*Interval(0, 0)).is_empty == True\n        assert ComplexRegion(Interval(1, 2)*Interval(2, 1)).is_empty == True\n\n", "compilation error", "def test_ComplexRegion_complement():\n    c1 = ComplexRegion(Interval(0, 1)*Interval(0, S.Pi), polar=True)\n    c2 = ComplexRegion(Interval(0.5, 1.5)*Interval(0, S.Pi/2), polar=True)\n    assert c1.complement() == ComplexRegion( (S.Complexes - c1), polar=False)\n    assert (c1.complement()).intersect(c1) == S.EmptySet\n    assert (c1.complement()).union(c1) == S.Complexes\n    assert c2.complement().intersect(c2) == S.EmptySet\n    assert c2.complement().union(c2) == S.Complexes\n", "def test_issue_12134():\n    a = Interval(0, 1)\n    b = Interval(0, pi/2)\n    c = ComplexRegion(a*b, polar=True)\n    d = ComplexRegion(a*b)\n    assert c.intersect(d) == c\n", "compilation error"], "sample_526": ["    def test_DateFormatter_settz_tzinfo_object():\n        time = mdates.date2num(datetime.datetime(2011, 1, 1, 0, 0,\n                                                tzinfo=mdates.UTC))\n        formatter = mdates.DateFormatter('%Y-%b-%d %H:%M')\n        # Default UTC\n        assert formatter(time) == '2011-Jan-01 00:00'\n\n        # Set tzinfo\n        formatter.set_tzinfo(dateutil.tz.gettz('Pacific/Kiritimati'))\n        assert formatter(time) == '2011-Jan-01 14:00'\n", "    def test_DateFormatter_no_tzinfo():\n        time = mdates.date2num(datetime.datetime(2011, 1, 1, 0, 0))\n        formatter = mdates.DateFormatter('%Y-%b-%d %H:%M')\n        assert formatter(time) == '2011-Jan-01 00:00'\n\n", "    def test_DateFormatter_invalid_tzinfo():\n        formatter = mdates.DateFormatter('%Y-%b-%d %H:%M')\n        with pytest.raises(ValueError, match=\"Invalid timezone\"):\n            formatter.set_tzinfo('Invalid/Timezone')\n\n", "def test_DateFormatter_formats():\n    formatter = mdates.DateFormatter('%Y-%m-%d')\n    assert formatter.format_data(19002) == '2022-01-10'\n    assert formatter.format_ticks(np.array([19002, 19033])) == [\n        '2022-01-10', '2022-02-10']\n    with pytest.raises(ValueError, match='Invalid format string'):\n        mdates.DateFormatter('%Y-%m-%D')\n", "compilation error"], "sample_259": ["    def test_prefetch_related_objects_empty_queryset(self):\n        with self.assertNumQueries(1):\n            prefetch_related_objects([], 'authors')\n", "    def test_prefetch_cascade(self):\n        reader1 = Reader.objects.get(id=self.reader1.id)\n        with self.assertNumQueries(2):\n            prefetch_related_objects([reader1], Prefetch('books_read', Prefetch('authors')))\n        with self.assertNumQueries(0):\n            for book in reader1.books_read.all():\n                self.assertEqual(len(book.authors.all()), len([author for author in book.authors.all()]))\n\n", "    def test_prefetch_related_objects_invalid_queryset(self):\n        book1 = Book.objects.get(id=self.book1.id)\n        with self.assertRaises(ValueError):\n            prefetch_related_objects(\n                [book1],\n                Prefetch('authors', queryset=Book.objects.all())\n            )\n", "    def test_circular_prefetch(self):\n        reader1 = Reader.objects.get(id=self.reader1.id)\n        with self.assertNumQueries(2):\n            prefetch_related_objects([reader1], 'books_read__authors__first_book')\n\n        with self.assertNumQueries(0):\n            [\n                book.authors.all()[0].first_book\n                for book in reader1.books_read.all()\n            ]\n", "    def test_prefetch_related_objects_empty_queryset(self):\n        with self.assertNumQueries(1):\n            prefetch_related_objects([], 'authors')\n"], "sample_231": ["    def test_sensitive_variables_decorator_with_request(self):\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_variables_with_request_view, check_for_POST_params=False)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_variables_with_request_view, check_for_POST_params=False)\n\n", "    def test_sensitive_variables_with_kwargs(self):\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_kwargs_view, check_for_vars=False)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_kwargs_view, check_for_vars=False)\n", "    def test_sensitive_variables_with_nested_functions(self):\n        @sensitive_variables(\"secret\")\n                return \"secret value\"\n            return inner_func()\n        \n        response = self.client.get('/sensitive_view/')\n        self.assertContains(response, 'secret value', status_code=500)\n", "    def test_sensitive_variables_with_args_and_kwargs(self):\n            return password, a, b, args, kwargs\n        with self.settings(DEBUG=True):\n           view = sensitive_variables(test_func)\n           response = self.client.get('/sensitive_variables_with_args_and_kwargs/', {'password': 'secret'})\n           self.assertEqual(response.status_code, 500)\n           self.assertContains(response, 'secret', status_code=500)\n", "    def test_sensitive_variables_decorator_with_arguments(self):\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_variables_with_args_view, check_for_POST_params=False)\n            self.verify_unsafe_email(sensitive_variables_with_args_view, check_for_POST_params=False)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_variables_with_args_view, check_for_POST_params=False)\n            self.verify_safe_email(sensitive_variables_with_args_view, check_for_POST_params=False)\n\n"], "sample_571": ["    def test_lmplot_hue_order(self):\n\n        g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, hue=\"h\", hue_order=[\"y\", \"x\"])\n        expected_order = [\"y\", \"x\"]\n        assert [c.get_label() for c in g.axes[0, 0].collections][1:] == expected_order\n", "    def test_lmplot_log(self):\n\n        g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, logx=True)\n        ax = g.axes[0, 0]\n        assert ax.get_yscale() == \"linear\"\n        assert ax.get_xscale() == \"log\"\n\n        g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, logy=True)\n        ax = g.axes[0, 0]\n        assert ax.get_yscale() == \"log\"\n        assert ax.get_xscale() == \"linear\"\n\n        g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, logx=True, logy=True)\n        ax = g.axes[0, 0]\n        assert ax.get_yscale() == \"log\"\n        assert ax.get_xscale() == \"log\"\n\n", "    def test_lmplot_units_hue(self):\n        g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, units=\"s\", hue=\"h\",\n                      fit_reg=False)\n        assert len(g.axes) == 2\n", "    def test_lmplot_legend_title(self):\n\n        g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, hue=\"h\",\n                      legend=\"brief\")\n\n        assert g._legend.get_title().get_text() == \"h\"\n\n", "    def test_lmplot_palette_mapping(self):\n\n        with pytest.raises(ValueError):\n            lm.lmplot(x=\"x\", y=\"y\", data=self.df, hue=\"h\",\n                      palette=dict(a=\"red\", b=\"blue\"))\n\n        g1 = lm.lmplot(x=\"x\", y=\"y\", data=self.df, hue=\"h\",\n                      palette={\"x\": \"red\", \"y\": \"blue\"})\n        assert g1.hue_kws == {\"color\": [\"red\", \"blue\"]}\n\n        g2 = lm.lmplot(x=\"x\", y=\"y\", data=self.df, hue=\"h\",\n                      palette={\"xy\": \"red\", \"y\": \"blue\"})\n        assert g2.hue_kws == {\"color\": [\"red\", \"blue\"]}\n"], "sample_1055": ["compilation error", "compilation error", "compilation error", "def test_padded_key_errors():\n    raises(ValueError, lambda: padded_key('ab', ''))\n    raises(ValueError, lambda: padded_key('', 'ab'))\n", "    def test_decipher_gm_invalid_input():\n        pri = gm_private_key(17, 19)\n        raises(TypeError, lambda: decipher_gm(\"abc\", pri))\n"], "sample_557": ["    def test_get_constrained_layout_pads_default():\n        fig = plt.figure(layout=mpl.layout_engine.ConstrainedLayoutEngine())\n        with pytest.warns(PendingDeprecationWarning, match=\"will be deprecated\"):\n            assert fig.get_constrained_layout_pads() == (0.02, 0.02, 0.02, 0.02)\n\n", "def test_get_constrained_layout_pads_default():\n    fig = plt.figure(layout='constrained')\n    with pytest.warns(PendingDeprecationWarning, match=\"will be deprecated\"):\n        assert fig.get_constrained_layout_pads() == (0.02, 0.02, 0.02, 0.02)\n\n", "    def test_figure_dpi_setters():\n        fig = plt.figure()\n        assert fig.get_dpi() == 100\n        fig.set_dpi(200)\n        assert fig.get_dpi() == 200\n        fig.dpi = 150\n        assert fig.get_dpi() == 150\n", "def test_tight_layout_with_constrained_layout():\n    fig_test, ax_test = plt.subplots(2, 2, constrained_layout=True)\n    fig_ref, ax_ref = plt.subplots(2, 2)\n    ax_test[0, 0].set_title('Test 1')\n    ax_test[0, 1].set_title('Test 2')\n    ax_test[1, 0].set_title('Test 3')\n    ax_test[1, 1].set_title('Test 4')\n    fig_test.tight_layout()\n    fig_ref.tight_layout()\n", "    def test_constrained_layout_empty():\n        fig, ax = plt.subplots(constrained_layout=True)\n        fig.canvas.draw()\n        bbox = ax.get_position()\n        assert bbox.width > 0\n        assert bbox.height > 0\n"], "sample_980": ["compilation error", "    def test_inversions():\n        p = Permutation([1, 2, 4, 3, 0])\n        assert p.inversions() == 3\n        assert Permutation([0, 1, 2, 3]).inversions() == 0\n        assert Permutation([3, 2, 1, 0]).inversions() == 6   \n", "compilation error", "    def test_identity_permutation():\n        p = Permutation(0)\n        assert p.is_Identity\n        assert p.array_form == [0]\n        assert p.cyclic_form == []\n        assert p.size == 1\n        assert p.order() == 1\n        assert p.sign() == 1\n        assert p.inversions() == 0\n", "compilation error"], "sample_448": ["    def test_violation_error_message(self):\n        constraint = UniqueConstraintProduct._meta.constraints[0]\n        with self.assertRaisesMessage(\n            ValidationError,\n            f\"Unique constraint product with this Name {self.p1.name} and Color {self.p1.color} already exists.\"\n        ):\n            constraint.validate(\n                UniqueConstraintProduct,\n                UniqueConstraintProduct(name=self.p1.name, color=self.p1.color),\n            )\n\n", "    def test_validate_opclasses(self):\n        constraint = models.UniqueConstraint(\n            fields=[\"author\"],\n            name=\"author_opclasses\",\n            opclasses=[\"text_pattern_ops\"],\n        )\n        msg = \"Constraint \u201cauthor_opclasses\u201d is violated.\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            constraint.validate(\n                UniqueConstraintProduct,\n                UniqueConstraintProduct(name=\"p1\", author=\"John Doe\"),\n            )\n        constraint.validate(\n            UniqueConstraintProduct,\n            UniqueConstraintProduct(name=\"p2\", author=\"John Doe\"),\n        )\n        # Existing instances have their existing row excluded.\n        constraint.validate(UniqueConstraintProduct, self.p1)\n        # Unique field is excluded.\n        constraint.validate(\n            UniqueConstraintProduct,\n            UniqueConstraintProduct(name=\"p1\", author=\"John Doe\"),\n            exclude={\"author\"},\n        )\n", "    def test_unique_constraint_with_condition_and_expressions(self):\n        constraint = models.UniqueConstraint(\n            Lower(\"name\"),\n            condition=models.Q(color__isnull=True),\n            name=\"name_lower_without_color_uniq\",\n        )\n        msg = \"Constraint \u201cname_lower_without_color_uniq\u201d is violated.\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            constraint.validate(UniqueConstraintProduct, UniqueConstraintProduct(name=self.p1.name.lower(), color=\"blue\"))\n        constraint.validate(\n            UniqueConstraintProduct,\n            UniqueConstraintProduct(name=self.p1.name.upper()),\n        )\n        constraint.validate(UniqueConstraintProduct, self.p2)\n\n\n\n", "    def test_covering_index(self):\n        constraint = models.UniqueConstraint(\n            fields=[\"name\", \"color\"],\n            name=\"name_color_uniq_covering\",\n            include=[\"price\"],\n        )\n        self.assertTrue(constraint.supports_covering_index)\n", "    def test_name_with_condition(self):\n        constraints = get_constraints(UniqueConstraintConditionProduct._meta.db_table)\n        expected_name = \"name_without_color_uniq\"\n        self.assertIn(expected_name, constraints)\n\n"], "sample_1025": ["def test_scipy_special_functions():\n    p = SciPyPrinter()\n    assert p.doprint(gamma(x)) == 'scipy.special.gamma(x)'\n    assert 'scipy.special' in p.module_imports\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_87": ["    def test_notify_file_changed_with_empty_file(self):\n        with mock.patch.object(self.reloader, 'get_mtime') as mocked_mtime:\n            mocked_mtime.return_value = 0\n            self.reloader.notify_file_changed(self.existing_file)\n            mocked_mtime.assert_called_once_with(self.existing_file)\n\n", "    def test_should_stop_with_error_file(self):\n        self._error_files.append(self.existing_file)\n        self.assertTrue(self.reloader.should_stop())\n", "    def test_watch_dir_recursive(self):\n        self.reloader.watch_dir(self.tempdir / 'dir', '**/*.py')\n        with self.tick_twice():\n            self.increment_mtime(self.existing_file)\n        self.assertEqual(notify_mock.call_count, 1)\n        self.assertCountEqual(notify_mock.call_args[0], [self.existing_file])\n\n", "    def test_notify_file_changed(self):\n        self.reloader.watch_file(self.existing_file)\n        self.reloader.notify_file_changed(self.existing_file)\n        with self.tick_twice():\n            self.reloader._run_notify_callback()\n        self.assertEqual(self.reloader.should_stop, True)\n", "    def test_run_loop_handles_directory_creation(self):\n        new_dir = self.tempdir / 'new_dir'\n        self.reloader.watch_dir(self.tempdir, '*.py')\n        with mock.patch('os.makedirs') as mocked_makedirs:\n            mocked_makedirs.return_value = None\n            # Simulate directory creation.\n            new_dir.mkdir()\n            with self.tick_twice():\n                pass\n        self.assertIn(new_dir / '__init__.py', self.reloader.watched_files())\n"], "sample_777": ["    def test_decision_function_shape(self):\n        # Check that the shape of the decision function output is correct\n\n        X, y = make_classification(n_samples=100, random_state=0)\n\n        clf = GradientBoostingClassifier(random_state=0).fit(X, y)\n        dec_func = clf.decision_function(X)\n        assert dec_func.shape == (100,)\n", "compilation error", "compilation error", "    def test_gradient_boosting_with_feature_names(self):\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([0, 1, 0])\n        feature_names = ['feature1', 'feature2']\n\n        clf = GradientBoostingClassifier(random_state=0)\n        clf.fit(X, y, feature_names_in=feature_names)\n\n        assert_array_equal(clf.feature_names_in_, feature_names)\n", "    def test_gradient_boosting_with_user_defined_loss():\n\n        X, y = make_regression(random_state=0)\n\n        class CustomLoss:\n                self.alpha = alpha\n\n                grad = (y_pred - y_true) * (1 + self.alpha * np.abs(y_pred - y_true))\n                hess = (1 + self.alpha * np.abs(y_pred - y_true)\n                        )  \n                return grad, hess\n\n        n_estimators = 5\n        gb = GradientBoostingRegressor(loss=CustomLoss(), n_estimators=n_estimators,\n                                        random_state=42)\n        gb.fit(X, y)\n\n        assert gb.n_estimators_ == n_estimators\n"], "sample_102": ["    def test_order_by_with_expressions(self):\n        qs1 = Number.objects.annotate(double_num=F('num') * 2)\n        qs2 = Number.objects.annotate(triple_num=F('num') * 3)\n        self.assertNumbersEqual(\n            qs1.union(qs2).order_by('double_num'),\n            [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n        )\n        self.assertNumbersEqual(\n            qs1.union(qs2).order_by('triple_num'),\n            [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n        )\n", "    def test_mixed_operations_on_combined_qs(self):\n        qs1 = Number.objects.filter(num__lte=5).values('num')\n        qs2 = Number.objects.filter(num__gte=5).values('num')\n        self.assertEqual(\n            list(qs1.union(qs2).annotate(double_num=F('num') * 2).filter(double_num__gt=10)),\n            [(6, 12), (7, 14), (8, 16), (9, 18), (10, 20)],\n        )\n", "    def test_union_with_different_column_names(self):\n        ReservedName.objects.create(name='a', order=2)\n        qs1 = Number.objects.annotate(name=F('num')).values('name', 'other_num')\n        qs2 = ReservedName.objects.values('name', 'order').filter(name='a')\n        with self.assertRaisesMessage(\n            FieldError,\n            \"Column 'name' is not found in select_related set\"\n        ):\n            list(qs1.union(qs2))\n", "    def test_union_with_filtered_subqueries(self):\n        qs1 = Number.objects.filter(num__lt=5).values('num')\n        qs2 = Number.objects.filter(num__gt=5).values('num').order_by('num')\n        self.assertNumbersEqual(qs1.union(qs2).order_by('num'), [0, 1, 2, 3, 4, 6, 7, 8, 9])\n", "    def test_intersection_with_empty_queryset(self):\n        qs1 = Number.objects.filter(num__lte=5)\n        qs2 = Number.objects.none()\n        self.assertNumbersEqual(qs1.intersection(qs2), [])\n\n"], "sample_1087": ["compilation error", "def test_interpolating_poly_non_symbol():\n    x = symbols('x')\n    points = [(0, 1), (1, 2), (2, 3)]\n    assert interpolating_poly(len(points) - 1, x, *zip(*points)) == \\\n        Poly(x**2 + x, x)\n", "compilation error", "def test_w_polys():\n    from sympy.polys.specialpolys import w_polys\n\n    w1, w2 = w_polys()\n\n    assert set(w1.free_symbols) == {x, y, z}\n    assert set(w2.free_symbols) == {x, y}\n\n    assert w1.total_degree() == 12\n    assert w2.total_degree() == 8\n", "compilation error"], "sample_763": ["        def __getitem__(self, item):\n            return 1\n", "    def test_check_array_dtype_object_conversion_with_multi_indexed_dataframe():\n        pd = importorskip(\"pandas\")\n        # Test that data-frame like objects with dtype object\n        # get converted with multi-index columns\n        arr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n        index = pd.MultiIndex.from_tuples([('a', 'x'), ('b', 'y'), ('c', 'z')])\n        df = pd.DataFrame(arr, columns=index)\n        checked_arr = check_array(df)\n        assert_equal(checked_arr.dtype, np.float64)\n\n", "def test_check_array_dtype_conversion_string():\n\n    X_str = np.array(['1', '2', '3'])\n    X_checked = check_array(X_str, dtype=float)\n    assert_array_equal(X_checked, [1, 2, 3])\n    assert X_checked.dtype == float\n\n    # Test with a mixed array\n    X_mixed = np.array(['1', 2, '3'])\n    assert_raises(ValueError, check_array, X_mixed, dtype=float)\n", "compilation error", "        def mock_warn(message, category, filename, lineno, file=None, line=None):\n            pass"], "sample_30": ["def test_get_infos_by_id():\n    vot = parse(\n        io.BytesIO(\n            b\"\"\"\n            <VOTABLE xmlns=\"http://www.ivoa.net/xml/VOTable/v1.3\"\n              xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" version=\"1.4\">\n              <RESOURCE type=\"results\">\n                <INFO id=\"creator\"  value=\"Cannon, A.\"/>\n                <INFO id=\"observer\" value=\"Fleming, W.\"/>\n              </RESOURCE>\n            </VOTABLE>\"\"\"\n        )\n    )\n    info = vot.get_info_by_id(\"creator\")\n    assert info.value == \"Cannon, A.\"\n    info = vot.get_info_by_id(\"observer\")\n    assert info.value == \"Fleming, W.\"\n", "def test_get_infos_by_id():\n    vot = parse(\n        io.BytesIO(\n            b\"\"\"\n        <VOTABLE xmlns=\"http://www.ivoa.net/xml/VOTable/v1.3\"\n          xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" version=\"1.4\">\n          <RESOURCE type=\"results\">\n            <INFO id=\"creator-id\" name=\"creator-name\" value=\"Cannon, A.\"/>\n            <INFO id=\"creator-id2\" name=\"creator-name\" value=\"Fleming, W.\"/>\n          </RESOURCE>\n        </VOTABLE>\"\"\"\n        )\n    )\n    infos = vot.get_infos_by_id(\"creator-id\")\n    assert len(infos) == 1\n    assert infos[0].value == \"Cannon, A.\" \n", "def test_empty_tables():\n    votable = parse(\n        io.BytesIO(\n            b\"\"\"\n        <VOTABLE xmlns=\"http://www.ivoa.net/xml/VOTable/v1.3\" version=\"1.3\">\n          <RESOURCE>\n            <TABLE id=\"table1\"/>\n            <TABLE id=\"table2\" name=\"Empty Table\"/>\n          </RESOURCE>\n        </VOTABLE>\n        \"\"\"\n        )\n    )\n    assert len(votable.resources[0].tables) == 2\n    assert votable.resources[0].tables[0].name is None\n    assert votable.resources[0].tables[1].name == \"Empty Table\"\n", "def test_get_field_by_id():\n    vot = parse(\n        io.BytesIO(\n            b\"\"\"\n        <VOTABLE xmlns=\"http://www.ivoa.net/xml/VOTable/v1.3\"\n          xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" version=\"1.4\">\n          <RESOURCE type=\"results\">\n            <TABLE>\n              <FIELD ID=\"myid\" datatype=\"int\" />\n            </TABLE>\n          </RESOURCE>\n        </VOTABLE>\"\"\"\n        )\n    )\n    field = vot.get_field_by_id(\"myid\")\n    assert field.ID == \"myid\"\n    assert field.datatype == \"int\"\n", "    def test_empty_table():\n        votable = parse(get_pkg_data_filename(\"data/empty_table.xml\"))\n        assert len(votable.get_first_table().array) == 0\n"], "sample_46": ["    def test_uuid_pk_in_filter(self):\n        obj = PrimaryKeyUUIDModel.objects.create()\n        self.assertEqual(PrimaryKeyUUIDModel.objects.filter(pk=obj.pk).count(), 1)\n", "    def test_uuid_pk_filter_with_in(self):\n        u1 = PrimaryKeyUUIDModel.objects.create()\n        u2 = PrimaryKeyUUIDModel.objects.create()\n        self.assertSequenceEqual(PrimaryKeyUUIDModel.objects.filter(pk__in=[u1.pk, u2.pk]), [u1, u2])\n", "    def test_uuid_pk_after_delete(self):\n        u1 = PrimaryKeyUUIDModel.objects.create()\n        u1.delete()\n        with self.assertRaises(PrimaryKeyUUIDModel.DoesNotExist):\n            PrimaryKeyUUIDModel.objects.get(pk=u1.pk)\n", "    def test_delete_related_object_with_cascade(self):\n        u1 = PrimaryKeyUUIDModel.objects.create()\n        r = RelatedToUUIDModel.objects.create(uuid_fk=u1)\n        u1.delete()\n        with self.assertRaises(RelatedToUUIDModel.DoesNotExist):\n            r.refresh_from_db()\n", "    def test_update_with_invalid_uuid(self):\n        u1 = PrimaryKeyUUIDModel.objects.create()\n        with self.assertRaises(IntegrityError):\n            RelatedToUUIDModel.objects.update(uuid_fk='invalid')\n\n"], "sample_196": ["    def test_force_no_ordering(self):\n        self.assertEqual(connection.ops.force_no_ordering(), [])\n", "    def test_window_frame_range_start_end(self):\n        self.assertEqual(\n            self.ops.window_frame_range_start_end(1, 2),\n            ('1 PRECEDING', '2 FOLLOWING'),\n        )\n        self.assertEqual(\n            self.ops.window_frame_range_start_end(-1, 1),\n            ('1 PRECEDING', '1 FOLLOWING'),\n        ) \n", "    def test_end_transaction_sql_no_transactions(self):\n        self.assertEqual(self.ops.end_transaction_sql(success=False), 'ROLLBACK;')\n", "    def test_combine_expression(self):\n        connector = 'AND'\n        sub_expressions = ['a = 1', 'b = 2']\n        expected = ' a = 1 AND b = 2 '\n        self.assertEqual(self.ops.combine_expression(connector, sub_expressions), expected)\n", "    def test_explain_query_prefix(self):\n        with self.assertRaisesMessage(NotSupportedError, 'This backend does not support explaining query execution.'):\n            self.ops.explain_query_prefix()\n\n"], "sample_227": ["    def test_listfilter_default_filter_value(self):\n        modeladmin = BookAdminWithDefaultFilterValue(Book, site)\n        request = self.request_factory.get('/', {})\n        request.user = self.alfred\n        changelist = modeladmin.get_changelist_instance(request)\n        filterspec = changelist.get_filters(request)[0][0]\n        self.assertEqual(filterspec.title, 'Verbose Author')\n        choices = list(filterspec.choices(changelist))\n        self.assertEqual(choices[2]['selected'], True)\n        self.assertEqual(choices[2]['query_string'], '?author__id__exact=%s' % self.alfred.pk)\n", "    def test_ordering_with_emptylistfieldfilter(self):\n        modeladmin = BookAdminWithEmptyFieldListFilter(Book, site)\n        request = self.request_factory.get('/', {'author__isempty': '1', 'o': '-title'})\n        request.user = self.alfred\n        changelist = modeladmin.get_changelist_instance(request)\n        queryset = changelist.get_queryset(request)\n        self.assertEqual(list(queryset), [self.guitar_book])\n", "    def test_emptylistfieldfilter_with_isnull_in_lookup(self):\n        modeladmin = BookAdminWithEmptyFieldListFilter(Book, site)\n\n        # Create a book with a null author\n        book_with_null_author = Book.objects.create(title='No Author', author=None)\n\n        tests = [\n            # Is null.\n            ({'author__isnull': 'True'}, [book_with_null_author]),\n            ({'author__isnull': 'False'},\n             [self.django_book, self.bio_book, self.djangonaut_book, self.guitar_book, self.empty_title]),\n            # Is not null\n            ({'author__isnull': '0'},\n             [self.django_book, self.bio_book, self.djangonaut_book, self.guitar_book, self.empty_title]),\n            ({'author__isnull': '1'}, [book_with_null_author]),\n\n        ]\n        for query_string, expected_result in tests:\n            with self.subTest(query_string=query_string):\n                request = self.request_factory.get('/', query_string)\n                request.user = self.alfred\n                changelist = modeladmin.get_changelist_instance(request)\n                queryset = changelist.get_queryset(request)\n                self.assertCountEqual(queryset, expected_result)\n", "    def test_emptylistfieldfilter_multiple_fields(self):\n        class TestAdmin(ModelAdmin):\n            list_filter = [\n                ('author__isempty', EmptyFieldListFilter),\n                ('title__isempty', EmptyFieldListFilter),\n            ]\n\n        modeladmin = TestAdmin(Book, site)\n        request = self.request_factory.get('/', {'author__isempty': '1', 'title__isempty': '0'})\n        request.user = self.alfred\n        changelist = modeladmin.get_changelist_instance(request)\n        queryset = changelist.get_queryset(request)\n        self.assertCountEqual(queryset, [self.django_book, self.bio_book, self.djangonaut_book])\n", "    def test_emptylistfieldfilter_reverse_one_to_one_related_manager(self):\n        class BookAdminReverseOneToOneManager(BookAdmin):\n            list_filter = (\n                ('improvedbook', EmptyFieldListFilter),\n            )\n\n        ImprovedBook.objects.create(book=self.guitar_book)\n        ImprovedBook.objects.create(book=self.bio_book)\n\n        modeladmin = BookAdminReverseOneToOneManager(Book, site)\n\n        tests = [\n            ({'improvedbook__isempty': '1'}, [self.django_book, self.djangonaut_book]),\n            ({'improvedbook__isempty': '0'}, [self.guitar_book, self.bio_book]),\n        ]\n        for query_string, expected_result in tests:\n            with self.subTest(query_string=query_string):\n                request = self.request_factory.get('/', query_string)\n                request.user = self.alfred\n                changelist = modeladmin.get_changelist_instance(request)\n                queryset = changelist.get_queryset(request)\n                self.assertCountEqual(queryset, expected_result)\n"], "sample_32": ["def test_lookback_time_example(cosmo_cls, args, kwargs, expected):\n    \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.lookback_time`.\n\n    These do not come from external codes -- they are just internal checks to make\n    sure nothing changes if we muck with the distance calculators.\n    \"\"\"\n    cosmo = cosmo_cls(*args, **kwargs)\n    z = np.array([1.0, 2.0, 3.0, 4.0])\n\n    assert u.allclose(cosmo.lookback_time(z), expected * u.Gyr, rtol=1e-4)\n", "    def test_de_densityscale_negative_z(self, cosmo, z):\n        with pytest.raises(ValueError):\n            cosmo.de_density_scale(z)\n", "    def test_de_densityscale_negative(self, cosmo, z):\n        with pytest.raises(ValueError):\n            cosmo.de_density_scale(z) \n", "    def test_w_edge_cases(self, cosmo):\n        \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.w` for edge cases.\"\"\"\n        assert np.isnan(cosmo.w(np.inf))\n        assert np.isnan(cosmo.w(-np.inf))\n", "    def test_w_array(self, cosmo):\n        \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.w` with array input.\"\"\"\n        z = np.array([0.0, 0.5, 1.0, 1.5, 2.3])\n        expected = np.array([-1.0, -0.75, -0.5, -0.25, 0.15])\n        assert u.allclose(cosmo.w(z), expected)\n"], "sample_780": ["def test_lda_partial_fit_doc_prior():\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 50, 20\n    X = rng.randint(4, size=(n_samples, n_features))\n\n    n_components = 3\n\n    # Use a different doc prior than the default\n    doc_topic_prior = np.array([0.1, 0.1, 0.8]) \n    lda = LatentDirichletAllocation(n_components=n_components,\n                                    doc_topic_prior=doc_topic_prior,\n                                    learning_offset=5., total_samples=20,\n                                    random_state=rng)\n\n    lda.partial_fit(X)\n\n    lda.partial_fit(X)\n    # Assert that doc_topic_prior is taken into account in the model\n\n    assert_allclose(lda.doc_topic_prior_, doc_topic_prior, atol=1e-6)\n", "def test_lda_partial_fit_empty_docs():\n    \"\"\"Test LDA partial_fit on empty documents (all-zero rows).\"\"\"\n    Z = np.zeros((5, 4))\n    lda = LatentDirichletAllocation(max_iter=750)\n\n    for X in [Z, csr_matrix(Z)]:\n        lda.partial_fit(X)\n        assert_almost_equal(lda.components_.sum(axis=0),\n                            np.ones(lda.components_.shape[1]))\n\n", "def test_lda_partial_fit_random_state():\n    # Test LDA partial_fit with random_state.\n    rng = np.random.RandomState(0)\n    n_components, X = _build_sparse_mtx()\n    lda_1 = LatentDirichletAllocation(n_components=n_components,\n                                     learning_offset=10., total_samples=100,\n                                     random_state=rng)\n    lda_2 = LatentDirichletAllocation(n_components=n_components,\n                                     learning_offset=10., total_samples=100,\n                                     random_state=rng)\n    for i in range(3):\n        lda_1.partial_fit(X)\n        lda_2.partial_fit(X)\n    assert_allclose(lda_1.components_, lda_2.components_, atol=1e-10)\n\n", "def test_lda_doc_topic_prior_setter():\n    \"\"\"Test setting the doc_topic_prior parameter.\"\"\"\n    n_components, X = _build_sparse_mtx()\n    lda = LatentDirichletAllocation(n_components=n_components,\n                                    random_state=0)\n    lda.doc_topic_prior = 0.1\n    assert lda.doc_topic_prior == 0.1\n", "def test_lda_partial_fit_doc_topic_update():\n    # test partial_fit update doc_topic_distr correctly.\n    rng = np.random.RandomState(0)\n    n_components = 3\n    n_samples = 10\n    X = rng.randint(4, size=(n_samples, 10))\n    lda = LatentDirichletAllocation(n_components=n_components,\n                learning_offset=5., total_samples=30, random_state=rng)\n\n    doc_topic_distr = lda.fit_transform(X)\n\n    _doc_topic_distr = np.copy(doc_topic_distr)\n    lda.partial_fit(X)\n    doc_topic_distr = lda.transform(X)\n    assert_almost_equal(doc_topic_distr, _doc_topic_distr)\n\n"], "sample_651": ["    def test_warns_context_manager_with_no_warnings(self) -> None:\n        with pytest.warns(UserWarning):\n            pass\n", "    def test_warns_in_contextmanager(self) -> None:\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"always\")\n            with pytest.warns(RuntimeWarning) as record:\n                with pytest.warns():\n                    warnings.warn(\"runtime\", RuntimeWarning)\n        assert len(record) == 1\n        assert str(record[0].message) == \"runtime\"\n", "    def test_warns_with_parametrize(self, pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n            import warnings\n\n            @pytest.mark.parametrize(\"i\", [1, 2])\n                if i == 1:\n                    warnings.warn(\"warn only if i is 1\", UserWarning)\n                recwarn.pop() # ensure no leftover warnings from previous iterations\n\n            @pytest.mark.parametrize(\"i\", [1, 2])\n                with pytest.warns(UserWarning):\n                    if i == 1:\n                        warnings.warn(\"warn only if i is 1\", UserWarning)\n                    elif i == 2:\n                        warnings.warn(\"warn only if i is 2\", UserWarning)\n                recwarn.pop() # ensure no leftover warnings from previous iterations\n\n        \"\"\"\n        )\n        result = pytester.runpytest()\n        result.assert_outcomes(passed=4)\n", "    def test_warns_re_emitting_warns(self) -> None:\n        with pytest.warns(DeprecationWarning, match=\"hi\"):\n            with pytest.warns(UserWarning):\n                warnings.warn(\"hi\", DeprecationWarning)\n                warnings.warn(\"hi\", UserWarning)\n", "    def test_warns_raises_re_emit(self) -> None:\n        with pytest.warns(UserWarning) as record:\n            with pytest.raises(ValueError) as excinfo:\n                warnings.warn(\"user warning\", UserWarning)\n                raise ValueError(\"some exception\")\n        assert len(record) == 1\n        assert \"user warning\" in str(record[0].message)\n        assert \"some exception\" in str(excinfo.value)\n"], "sample_594": ["    def test_format_items_empty(self):\n        actual = formatting.format_items([])\n        expected = \"\"\n        assert expected == actual\n\n", "    def test_limit_lines_empty_string(self):\n        assert formatting.limit_lines(\"\", limit=10) == \"\"\n", "    def test_format_items_scalar():\n        cases = [\n            (np.array(10), \"10\"),\n            (np.array(3.14), \"3.14\"),\n            (pd.Timestamp(\"2000-01-01T12\"), \"2000-01-01T12:00:00\"),\n            (pd.Timestamp(\"NaT\"), \"NaT\"),\n            (pd.Timedelta(\"10 days 1 hour\"), \"10 days 01:00:00\"),\n            (\"foo\", \"'foo'\"),\n            (b\"foo\", \"b'foo'\"),\n        ]\n        for item, expected in cases:\n            actual = \" \".join(formatting.format_items(item))\n            assert expected == actual\n\n", "    def test_short_numpy_repr_with_dtype_object():\n        arr = np.array([1, \"string\", 3.14], dtype=object)\n        assert \"object\" in formatting.short_numpy_repr(arr)\n", "    def test_format_items_with_mixed_types(self):\n        cases = [\n            (np.arange(4) * np.timedelta64(1, \"D\"), pd.Timestamp(\"2000-01-01\"),\n             \"0 days 1 days 2 days 3 days 2000-01-01\"),\n            (\n                pd.to_timedelta([\"NaT\", \"0s\", \"1s\", \"NaT\"]), np.array([1, 2, 3]),\n                \"NaT 00:00:00 00:00:01 NaT 1 2 3\",\n            ),\n        ]\n        for item, expected in cases:\n            actual = \" \".join(formatting.format_items(item))\n            assert expected == actual\n"], "sample_302": ["    def test_runshell_sigint(self):\n        with mock.patch('subprocess.run') as run:\n            with mock.patch.object(\n                BaseDatabaseClient,\n                'settings_to_cmd_args_env',\n                return_value=([], {}),\n            ):\n                self.client.runshell(None)\n            run.assert_called_once()\n            self.assertTrue(run.call_args[0][0])\n", "    def test_runshell_SIGINT(self):\n        with mock.patch('subprocess.run') as run:\n            with mock.patch.object(\n                BaseDatabaseClient,\n                'settings_to_cmd_args_env',\n                return_value=([], None),\n            ):\n                self.client.runshell([])\n            run.assert_called_once()\n", "    def test_runshell_signal_handling(self):\n        with mock.patch('subprocess.run') as run:\n            with mock.patch.object(signal, 'getsignal') as getsignal:\n                with mock.patch.object(\n                    signal, 'signal', side_effect=lambda *args: None\n                ) as signal_signal:\n                    self.client.runshell(None)\n                    getsignal.assert_called_once_with(signal.SIGINT)\n                    signal_signal.assert_has_calls(\n                        [\n                            mock.call(signal.SIGINT, signal.SIG_IGN),\n                            mock.call(signal.SIGINT, getsignal.return_value),\n                        ]\n                    )\n                    run.assert_called_once_with(mock.ANY, env=None, check=True)\n\n", "    def test_runshell_with_parameters(self):\n        with mock.patch('subprocess.run') as run:\n            with mock.patch.object(\n                BaseDatabaseClient,\n                'settings_to_cmd_args_env',\n                return_value=(['psql'], {}),\n            ):\n                self.client.runshell(['-c', 'SELECT 1'])\n            run.assert_called_once_with(['psql', '-c', 'SELECT 1'], check=True)\n", "    def test_runshell_call_settings_to_cmd_args_env(self):\n        with mock.patch('subprocess.run') as run:\n            with mock.patch.object(\n                BaseDatabaseClient,\n                'settings_to_cmd_args_env',\n                return_value=(['psql', '-U', 'user'], {}),\n            ):\n                self.client.runshell(['param1', 'param2'])\n            run.assert_called_once_with(\n                ['psql', '-U', 'user', 'param1', 'param2'],\n                env={},\n                check=True,\n            )\n"], "sample_831": ["compilation error", "    def test_plot_tree_errors():\n        clf = DecisionTreeClassifier(max_depth=3, random_state=0)\n\n        # Check if it errors when argument is not an estimator\n        message = \"is not an estimator instance\"\n        assert_raise_message(TypeError, message, plot_tree, clf.fit(X, y).tree_)\n        \n        # Check if it errors when argument is not fitted\n        message = \"DecisionTreeClassifier instance has not been fitted\"\n        assert_raises_regex(NotFittedError, message, plot_tree, clf)\n", "compilation error", "compilation error", "    def test_plot_tree_errors():\n        clf = DecisionTreeClassifier(max_depth=3, random_state=0)\n        assert_raises(NotFittedError, plot_tree, clf)\n        clf.fit(X, y)\n\n        assert_raises(TypeError, plot_tree, clf.fit(X, y).tree_)\n        assert_raises(TypeError, plot_tree, clf, ax=1)\n"], "sample_3": ["def test_ecsv_round_trip_masked_column_with_null_string():\n    \"\"\"Test round-trip of a masked column with null string representation.\"\"\"\n    t = Table()\n    t['a'] = np.ma.masked_array(['a', 'b', None, 'd'], mask=[False, False, True, False])\n    out = StringIO()\n    t.write(out, format='ascii.ecsv')\n    t2 = Table.read(out.getvalue(), format='ascii.ecsv')\n    assert np.all(t2['a'] == ['a', 'b', '', 'd'])\n    assert np.all(t2['a'].mask == [False, False, True, False])\n", "def test_ecsv_delimiter():\n    txt = \"\"\"\n    # %ECSV 1.0\n    # ---\n    # delimiter: ;\n    # datatype:\n    # - {name: a, datatype: int64}\n    # - {name: b, datatype: string}\n    # schema: astropy-2.0\n    a;b\n    1;hello\n    2;world\n    \"\"\"\n    t = ascii.read(txt, format='ascii.ecsv')\n    assert t['a'].dtype.kind == 'i'\n    assert t['b'][0] == 'hello'\n", "def test_read_ecsv_with_empty_string():\n    \"\"\"Test read with empty string as a column value\"\"\"\n    txt = \"\"\"\n    # %ECSV 1.0\n    # ---\n    # datatype:\n    # - {name: col, datatype: string}\n    # schema: astropy-2.0\n    col\n    \n    \n    hello\n    \"\"\"\n    t = ascii.read(txt, format='ecsv')\n    assert t['col'][0] == ''\n    assert t['col'][1] == 'hello'\n", "compilation error", "    def test_write_empty_table():\n        t = Table()\n        out = StringIO()\n        t.write(out, format='ascii.ecsv')\n        assert out.getvalue() == '# %ECSV 1.0\\n# ---\\n# datatype:\\n# schema: astropy-2.0\\n'\n\n"], "sample_626": ["    def test_stack_multiindex_with_existing_levels(self) -> None:\n        prod_vars = {\n            \"x\": xr.Variable(\"x\", pd.Index([\"b\", \"a\"]), attrs={\"foo\": \"bar\"}),\n            \"y\": xr.Variable(\"y\", pd.Index([1, 3, 2])),\n        }\n        \n        index = PandasMultiIndex.stack(prod_vars, \"z\")\n        new_index = index.stack({\"x\": \"x2\"})\n\n        assert new_index.dim == \"z\"\n        assert new_index.index.names == [\"x2\", \"y\"]\n\n", "def test_multiindex_from_variables_mismatched_dims():\n    v1 = xr.Variable([\"x\", \"y\"], [[1, 2, 3], [4, 5, 6]], attrs={\"unit\": \"m\"})\n    v2 = xr.Variable(\"x\", [10, 20, 30])\n\n    with pytest.raises(ValueError, match=r\"unmatched dimensions for multi-index variables.*\"):\n        PandasMultiIndex.from_variables({\"level1\": v1, \"level2\": v2}, {})\n", "compilation error", "    def test_sel_multiindex_out_of_order(self) -> None:\n        # Test that multiindex selection works even if the levels are selected\n        # in a different order than they are defined in the MultiIndex\n        index = PandasMultiIndex(\n            pd.MultiIndex.from_product([[\"a\", \"b\"], [1, 2]], names=(\"one\", \"two\")),\n            \"x\",\n        )\n        actual = index.sel({\"two\": 2, \"one\": \"b\"})\n        expected = index.sel({\"one\": \"b\", \"two\": 2})\n        assert actual.equals(expected)\n", "    def test_index_from_variable(self, indexes) -> None:\n        for dim, var in indexes.variables.items():\n            actual = Index.from_variable(var)\n            expected = indexes[dim]\n            assert actual.equals(expected)\n"], "sample_561": ["def test_marker_filled(marker, fillstyle, expected_filled):\n    marker.set_fillstyle(fillstyle)\n    assert marker.get_fillstyle() == fillstyle\n    assert marker.is_filled() == expected_filled\n", "    def test_marker_filled_half():\n        fig, ax = plt.subplots()\n        ax.plot(0, 0, marker=(5, 2, 0), markersize=10, fillstyle='half',\n                markerfacecolor='red', color='black')\n\n        # We use 1.4 instead of 1.5 to make the test pass reliably across\n        # backends. Different backends draw markers with slight variations.\n        assert ax.artists[0].get_transform().transform((1.4, 0)) == \\\n               ax.artists[0].get_transform().transform((0, 0))\n", "def test_marker_fillstyle_half(fig_test, fig_ref):\n    ax_test = fig_test.add_subplot()\n    ax_ref = fig_ref.add_subplot()\n\n    markers = ['o', '^', 's', 'p', '*', 'h', 'H', 'x', 'D']\n    for i, m in enumerate(markers):\n        ax_test.scatter([i], [0], marker=m, s=300, fillstyle='half')\n        ax_ref.scatter([i], [0], marker=m, s=300)\n\n    ax_test.set(xlim=(-0.5, len(markers) - 0.5), ylim=(-1, 1))\n    ax_ref.set(xlim=(-0.5, len(markers) - 0.5), ylim=(-1, 1))\n", "def test_marker_fillstyle_half_filled():\n    marker = markers.MarkerStyle(\"o\", fillstyle=\"top\")\n    assert marker.get_fillstyle() == \"top\"\n    assert marker.is_filled()\n    marker = markers.MarkerStyle(\"o\", fillstyle=\"none\")\n    assert marker.get_fillstyle() == \"none\"\n    assert not marker.is_filled()\n\n", "    def test_marker_half_filled():\n        # Test half-filled markers (right, top, left, bottom)\n        fig, ax = plt.subplots()\n\n        marker = markers.MarkerStyle(marker='o', fillstyle='left')\n        ax.scatter([0], [0], marker=marker, s=20**2)\n\n        marker = markers.MarkerStyle(marker='o', fillstyle='right')\n        ax.scatter([1], [1], marker=marker, s=20**2)\n\n        marker = markers.MarkerStyle(marker='o', fillstyle='top')\n        ax.scatter([2], [0], marker=marker, s=20**2)\n\n        marker = markers.MarkerStyle(marker='o', fillstyle='bottom')\n        ax.scatter([3], [1], marker=marker, s=20**2)\n"], "sample_348": ["    def test_actions_valid_with_permission(self):\n        @admin.action(permissions=['song.can_edit'])\n            pass\n\n        class SongAdmin(ModelAdmin):\n            actions = (edit_songs,)\n\n        self.assertIsValid(SongAdmin, Song)\n", "    def test_actions_invalid_type(self):\n        class BadAction:\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (BadAction,)\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            \"The value of 'actions[0]' must be a function.\",\n            id='admin.E131',\n        )\n", "    def test_actions_with_invalid_permission(self):\n        @admin.action(permissions=['nonexistent_permission'])\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action,)\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            'The value of \"permissions\" for action \"action\" includes '\n            '\"nonexistent_permission\" which is not a valid permission.',\n            id='admin.E131',\n        )\n", "    def test_actions_with_duplicate_names_and_different_permissions(self):\n        @admin.action(permissions=['permission1'])\n            pass\n\n        @admin.action(permissions=['permission2'])\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action, action)\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            \"__name__ attributes of actions defined in BandAdmin must be \"\n            \"unique. Name 'action' is not unique.\",\n            id='admin.E130',\n        )\n", "    def test_actions_valid_permission(self):\n        @admin.action(permissions=['change_band'])\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (change_band_action,)\n\n        self.assertIsValid(BandAdmin, Band) \n"], "sample_674": ["def test_get_fslocation_from_item(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n\n        class TestClass:\n    \"\"\"\n    )\n\n    item = testdir.getitem(\"test_func\")\n    assert get_fslocation_from_item(item) == (p, 2)\n\n    item = testdir.getitem(\"TestClass::test_method\")\n    assert get_fslocation_from_item(item) == (p, 4)\n\n    class DummyItem:\n            self.fspath = p\n    item = DummyItem()\n    assert get_fslocation_from_item(item) == (p, -1)\n\n", "def test_get_fslocation_from_item(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    items = testdir.getitems(p.basename)\n    result = nodes.get_fslocation_from_item(items[0])\n    assert result == (p, None)\n", "    def test_get_fslocation_from_item(testdir):\n        p = testdir.makepyfile(\n            \"\"\"\n                pass\n            class TestClass:\n                    pass\n        \"\"\"\n        )\n        items = testdir.getitems(p.basename)\n        assert nodes.get_fslocation_from_item(items[0]) == (p, 1)\n        assert nodes.get_fslocation_from_item(items[1]) == (p, 3)\n", "    def test_ischildnode_empty_nodeid(testdir):\n        result = nodes.ischildnode(\"foo\", \"\")\n        assert result is True\n", "def test_splitnode():\n    result = nodes._splitnode(\"foo/bar::TestBaz\")\n    assert result == (\"foo\", \"bar\", \"TestBaz\")\n"], "sample_902": ["compilation error", "    def test_pipeline_memory_location(self):\n        # Test that the memory location can be specified correctly\n        iris = load_iris()\n        X = iris.data\n        y = iris.target\n        cachedir = mkdtemp()\n        memory = Memory(cachedir=cachedir, verbose=10)\n\n        clf = SVC(probability=True, random_state=0)\n        transf = DummyTransf()\n        pipe = Pipeline([('transf', transf), ('svc', clf)], memory=memory)\n        pipe.fit(X, y)\n        assert_true(os.path.exists(os.path.join(cachedir, 'joblib')))\n        # Check that the cache file is created in the specified location\n        assert_true(os.path.exists(os.path.join(cachedir, 'joblib',\n                                                    'pipeline')))\n\n        shutil.rmtree(cachedir)\n", "    def test_pipeline_memory_location(self):\n        cachedir = mkdtemp()\n        memory = Memory(cachedir=cachedir, verbose=0)\n        X = np.array([[1, 2]])\n\n        pipe = Pipeline([('transf', Transf()), ('clf', Mult())],\n                        memory=memory)\n        pipe.fit(X, y=None)\n\n        # Check if the cache directory is correctly created\n        assert os.path.exists(cachedir)\n\n        # Check for the presence of the cache file\n        cache_file = os.path.join(cachedir, 'transf_Transf.pkl')\n        assert os.path.exists(cache_file)\n\n        shutil.rmtree(cachedir)\n", "    def test_pipeline_with_memory_and_params(self):\n        # test that pipeline with memory works correctly even if passed parameters\n        # to the estimators\n        cachedir = mkdtemp()\n        memory = Memory(cachedir=cachedir)\n        X = np.array([[1, 2]])\n        transf = DummyTransf()\n        clf = SVC(probability=True, random_state=0)\n        pipe = Pipeline([('transf', transf), ('svc', clf)],\n                        memory=memory)\n        pipe.fit(X, y=None)\n        shutil.rmtree(cachedir)\n", "def test_pipeline_memory_empty():\n    cachedir = mkdtemp()\n    memory = Memory(cachedir=cachedir)\n    # Test with an empty pipeline\n    pipe = Pipeline(memory=memory)\n    pipe.fit(np.array([[1]]), [1])\n    shutil.rmtree(cachedir)\n"], "sample_826": ["compilation error", "    def test_one_hot_encoder_drop_pandas(self):\n        pd = pytest.importorskip('pandas')\n        cats_to_drop = ['def', 12, 3, 56]\n        enc = OneHotEncoder(drop=cats_to_drop)\n        X = pd.DataFrame([['abc', 12, 2, 55],\n                          ['def', 12, 1, 55],\n                          ['def', 12, 3, 56]],\n                         columns=['a', 'b', 'c', 'd'])\n        trans = enc.fit_transform(X)\n        exp = pd.DataFrame([[1, 0, 1, 1],\n                           [0, 1, 0, 1],\n                           [0, 0, 0, 0]],\n                          columns=['a_abc', 'a_def', 'c_2', 'd_55'])\n        pd.testing.assert_frame_equal(trans, exp)\n        dropped_cats = [cat[feature]\n                        for cat, feature in zip(enc.categories_,\n                                                enc.drop_idx_)]\n        assert_array_equal(dropped_cats, cats_to_drop)\n        assert_frame_equal(X, enc.inverse_transform(trans))\n", "    def test_one_hot_encoder_fit_transform_empty():\n        enc = OneHotEncoder()\n        X = np.array([], dtype=object).reshape(0, 2)\n        Xt = enc.fit_transform(X)\n        assert Xt.shape == (0, 0)\n        assert len(enc.categories_) == 2\n        assert all([len(c) == 0 for c in enc.categories_])\n\n", "compilation error", "compilation error"], "sample_827": ["compilation error", "    def test_csc_row_median_empty_rows():\n        # Test that csc_row_median handles rows with all zeros correctly.\n        X = np.array([[0, 0],\n                      [1, 0],\n                      [0, 0]])\n        csc = sp.csc_matrix(X)\n        sparse_median = csc_median_axis_0(csc)\n        assert_array_equal(sparse_median, np.array([0.5, 0]))\n", "compilation error", "    def test_csc_row_median_empty(self):\n        csc = sp.csc_matrix((0, 5))\n        assert_array_equal(csc_median_axis_0(csc), np.array([]))\n", "    def test_csc_row_median_empty_rows():\n        # Test that csc_row_median handles empty rows correctly.\n        X = sp.csc_matrix((3, 4))  # Empty matrix\n        median = csc_median_axis_0(X)\n        assert_array_equal(median, np.zeros(4))\n\n"], "sample_128": ["    def test_index_with_suffix_names(self):\n        index = Index(fields=['headline'], name='test_index', suffix='suffix_name')\n        with connection.schema_editor() as editor:\n            self.assertIn(\n                editor._create_index_name(Article._meta.db_table, ['headline'], suffix='suffix_name'),\n                str(index.create_sql(Article, editor)),\n            )\n\n", "    def test_index_name_unique(self):\n        editor = connection.schema_editor()\n        index_name1 = editor._create_index_name(\n            table_name=Article._meta.db_table,\n            column_names=(\"c1\",),\n            suffix=\"123\",\n        )\n        index_name2 = editor._create_index_name(\n            table_name=Article._meta.db_table,\n            column_names=(\"c1\",),\n            suffix=\"123\",\n        )\n        self.assertEqual(index_name1, index_name2)\n", "    def test_index_tablespace(self):\n        index = Index(\n            name='test_index_tablespace',\n            fields=['headline'],\n            db_tablespace='test_tablespace',\n        )\n        with connection.schema_editor() as editor:\n            self.assertIn(\n                'TABLESPACE \"test_tablespace\"',\n                str(index.create_sql(Article, editor)),\n            )\n            editor.add_index(Article, index)\n            with connection.cursor() as cursor:\n                constraints = connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                )\n                self.assertIn(index.name, constraints)\n            editor.remove_index(Article, index)\n", "    def test_index_with_suffix(self):\n        index = Index(\n            fields=['headline'], name='test_index_with_suffix', suffix='123'\n        )\n        with connection.schema_editor() as editor:\n            editor.add_index(Article, index)\n        self.assertIn(\n            'indexes_article_headline_123',\n            str(index.create_sql(Article, connection.schema_editor())),\n        )\n\n", "    def test_index_with_deferred_sql(self):\n        with connection.schema_editor() as editor:\n            index = Index(\n                name='test_deferred_index',\n                fields=['headline'],\n            )\n            editor.deferred_sql.append(\"CREATE TABLE IF NOT EXISTS indexes_article (id INT);\")\n            editor.add_index(Article, index)\n            self.assertIn(\n                str(index.create_sql(Article, editor)), editor.deferred_sql\n            )\n\n"], "sample_1030": ["compilation error", "compilation error", "def test_ordered_points():\n    p1 = Point(0, 0)\n    p2 = Point(1, 1)\n    assert _ordered_points((p2, p1)) == (p1, p2)\n    assert _ordered_points((p1, p2)) == (p1, p2)\n", "compilation error", "compilation error"], "sample_724": ["def test_imputation_error_invalid_axis():\n    # Test if ValueError is raised when axis is not 0 or 1\n\n    X = np.array([[1, 2], [3, np.nan]])\n    imputer = Imputer(missing_values=np.nan, strategy=\"mean\")\n\n    with assert_raises(ValueError):\n        imputer.fit(X, axis=2)\n\n", "def test_imputation_sparse_dense_interaction():\n    # Test imputation for handling interaction between sparse and dense data.\n    X_sparse = sparse.csc_matrix([[1, 2, np.nan], [4, np.nan, 6]])\n    X_dense = np.array([[np.nan, 8, 9], [10, np.nan, 12]])\n\n    # Test the interaction when fit on sparse and transform on dense.\n    imputer = Imputer(strategy=\"mean\")\n    imputer.fit(X_sparse)\n    X_dense_imputed = imputer.transform(X_dense)\n    assert_array_almost_equal(\n        X_dense_imputed,\n        np.array([[1.5, 8, 9], [10, 5, 12]]),\n    )\n\n    # Test the interaction when fit on dense and transform on sparse.\n    imputer.fit(X_dense)\n    X_sparse_imputed = imputer.transform(X_sparse)\n\n    assert_array_almost_equal(\n        X_sparse_imputed.toarray(),\n        np.array([[1, 2, 5.5], [4, 5, 6]]),\n    )\n\n\n\n", "def test_imputation_sparse_zero_dense_zero():\n    # Test imputation with a sparse matrix containing zeros and a dense matrix\n    # containing zeros, ensuring that the sparsity structure is preserved\n    # during imputation.\n    X_sparse = sparse.csr_matrix([[0, 1, 0], [0, 0, 1], [1, 0, 0]])\n    X_dense = np.array([[0, 1, 0], [0, 0, 1], [1, 0, 0]])\n\n    imputer = Imputer(missing_values=0, strategy='mean')\n\n    X_sparse_imputed = imputer.fit_transform(X_sparse)\n    X_dense_imputed = imputer.fit_transform(X_dense)\n\n    assert_array_equal(X_sparse_imputed.toarray(), X_dense_imputed)\n    assert sparse.issparse(X_sparse_imputed)\n", "def test_imputation_error_message():\n    # Test imputation error messages for invalid inputs.\n    X = np.array([[1, 2], [3, 4], [np.nan, 6]])\n    imputer = Imputer(missing_values=0, strategy=\"mean\")\n\n    with pytest.raises(ValueError, match=\"Expected axis 0 or 1\"):\n        imputer.fit(X, axis=2)\n\n    with pytest.raises(ValueError, match=\"Invalid strategy\"):\n        imputer.fit(X, strategy='invalid')\n\n\n", "    def test_imputation_sparse_zeros():\n        # Test imputation with sparse matrices containing only zeros.\n        X = sparse.csr_matrix((10, 5))\n        X_true = X.copy()\n\n        for strategy in [\"mean\", \"median\", \"most_frequent\"]:\n            _check_statistics(X, X_true, strategy,\n                              [np.nan] * 5, 0)\n"], "sample_979": ["compilation error", "compilation error", "compilation error", "def test_MatrixElement_with_zero():\n    x, y, z, w = symbols(\"x y z w\")\n    M = ZeroMatrix(2, 2)\n    i, j = symbols(\"i, j\")\n    Mij = M[i, j]\n    assert isinstance(Mij, MatrixElement)\n    for oi, oj in [(0, 0), (0, 1), (1, 0), (1, 1)]:\n        assert Mij.subs({i: oi, j: oj}) == 0\n    A = MatrixSymbol(\"A\", 2, 2)\n    assert A[0, 0].subs(A, M) == 0\n    assert A[i, j].subs(A, M) == 0\n    assert M[i, j].subs(M, A) == A[i, j]\n\n", "def test_MatrixSymbol_determinant_2x2():\n    A = MatrixSymbol('A', 2, 2)\n    assert A.as_explicit().det() == A[0, 0]*A[1, 1] - A[0, 1]*A[1, 0] \n"], "sample_541": ["def test_polygon_selector_set_visible(draw_bounding_box):\n    verts = [(50, 50), (150, 50), (50, 150)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n    ax = get_ax()\n    tool = widgets.PolygonSelector(ax, onselect=noop,\n                                  draw_bounding_box=draw_bounding_box)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n    \n    tool.set_visible(False)\n    assert not tool.get_visible()\n    tool.set_visible(True)\n    assert tool.get_visible()\n\n", "def test_multi_cursor_colors(fig_test, fig_ref):\n    ax1_test = fig_test.subplots()\n    ax1_ref = fig_ref.subplots()\n\n    multi_test = widgets.MultiCursor(\n        (ax1_test,),\n        color='r',\n        linewidth=2,\n        linestyle='dashed',\n    )\n    multi_ref = widgets.MultiCursor(\n        (ax1_ref,),\n        color='r',\n        linewidth=2,\n        linestyle='dashed',\n    )\n", "def test_polygon_selector_modify_verts(draw_bounding_box):\n    verts = [(50, 50), (150, 50), (50, 150)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n    tool = widgets.PolygonSelector(\n        get_ax(), onselect=noop, draw_bounding_box=draw_bounding_box)\n\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    # Modify verts directly\n    tool.verts[1] = (100, 100)\n    assert tool.verts == [(50, 50), (100, 100), (50, 150)]\n", "compilation error", "    def test_polygon_selector_remove_repeated_points(draw_bounding_box):\n        verts = [(50, 50), (150, 50), (100, 100), (150, 50), (50, 150)]\n        event_sequence = [\n            *polygon_place_vertex(*verts[0]),\n            *polygon_place_vertex(*verts[1]),\n            *polygon_place_vertex(*verts[2]),\n            *polygon_place_vertex(*verts[3]),\n            *polygon_place_vertex(*verts[4]),\n            *polygon_place_vertex(*verts[0]),\n        ]\n        check_polygon_selector(event_sequence, verts[:4], 2,\n                               draw_bounding_box=draw_bounding_box)\n\n"], "sample_786": ["    def test_fit_transform_with_constant_features():\n        X = np.array([[1, 2, 3],\n                      [1, 2, 3],\n                      [1, 2, 3],\n                      [1, 2, 3]])\n        est = KBinsDiscretizer(n_bins=3)\n        Xt = est.fit_transform(X)\n        assert_array_equal(Xt, np.array([[0, 0, 0],\n                                        [0, 0, 0],\n                                        [0, 0, 0],\n                                        [0, 0, 0]]))\n", "    def test_empty_data(encode):\n        X = np.array([]).reshape(0, 4)\n        est = KBinsDiscretizer(n_bins=2, encode=encode)\n        est.fit(X)\n        Xt = est.transform(X)\n        assert Xt.shape == (0, 4) if encode == 'onehot-dense' else Xt.shape == (0, )\n", "def test_n_bins_single_feature(n_bins):\n    X = np.array([0, 1, 2, 3, 4], dtype=np.float64)[:, None]\n    kbd = KBinsDiscretizer(n_bins=n_bins, encode='ordinal')\n    kbd.fit(X)\n    Xt = kbd.transform(X)\n    assert Xt.shape[1] == 1\n    assert np.unique(Xt).shape[0] == min(n_bins, X.shape[0])\n", "    def test_no_features(self, strategy):\n        X = np.empty((0, 4))\n        est = KBinsDiscretizer(n_bins=3, strategy=strategy, encode='ordinal')\n        Xt = est.fit_transform(X)\n        assert Xt.shape == (0, 4)\n", "def test_fit_transform_empty_array(encode):\n    X = np.array([])\n    X = X.reshape(0, 4)\n    est = KBinsDiscretizer(n_bins=2, encode=encode)\n    Xt = est.fit_transform(X)\n    assert Xt.shape == (0, 4) if encode != 'onehot' else Xt.shape == (0, 8)\n"], "sample_1183": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_563": ["def test_draggabletext():\n    fig, ax = plt.subplots()\n    text = ax.text(0.1, 0.1, 'draggable', picker=True)\n    text.set_draggable(True)\n    fig.canvas.draw()\n", "def test_draggable_anchoredtext(fig_test, fig_ref):\n    ax_ref = fig_ref.add_subplot()\n    text_ref = AnchoredText(\"Draggable Text\", loc='upper left')\n    ax_ref.add_artist(text_ref)\n    text_ref.draggable(True)\n\n    ax_test = fig_test.add_subplot()\n    text_test = AnchoredText(\"Draggable Text\", loc='upper left')\n    ax_test.add_artist(text_test)\n    text_test.draggable(True)\n    # Simulate a drag event\n    event = MouseEvent(\n        \"button_press_event\", fig_test.canvas, 10, 10, MouseButton.LEFT\n    )\n    text_test._on_press(event)\n    event = MouseEvent(\"motion_notify_event\", fig_test.canvas, 20, 20)\n    text_test._on_motion(event)\n    event = MouseEvent(\n        \"button_release_event\", fig_test.canvas, 20, 20, MouseButton.LEFT\n    )\n    text_test._on_release(event)\n", "def test_draggable_annotation(boxcoords):\n    fig, ax = plt.subplots()\n    annotation = ax.annotate(\"test\", (0.5, 0.5), xycoords=boxcoords,\n                             draggable=True)\n    fig.canvas.draw()\n    start_pos = annotation.xyann\n\n    # Simulate a drag operation\n\n    event = MouseEvent(\"button_press_event\", fig.canvas,\n                       *fig.transFigure.transform(start_pos),\n                       MouseButton.LEFT)\n    event._process()\n\n    event = MouseEvent(\"motion_notify_event\", fig.canvas,\n                       *fig.transFigure.transform((start_pos[0] + 0.1,\n                                                    start_pos[1] + 0.1)),\n                       )\n    event._process()\n\n    event = MouseEvent(\"button_release_event\", fig.canvas,\n                       *fig.transFigure.transform(start_pos),\n                       MouseButton.LEFT)\n    event._process()\n\n    assert annotation.xyann != start_pos\n", "def test_anchoredtext_vertical_alignment():\n    fig, ax = plt.subplots()\n\n    text0 = AnchoredText(\"test\\ntest long text\", loc=\"upper left\",\n                         pad=0.2, prop={\"va\": \"bottom\"})\n    ax.add_artist(text0)\n    text1 = AnchoredText(\"test\\ntest long text\", loc=\"center\",\n                         pad=0.2, prop={\"va\": \"center\"})\n    ax.add_artist(text1)\n    text2 = AnchoredText(\"test\\ntest long text\", loc=\"lower right\",\n                         pad=0.2, prop={\"va\": \"top\"})\n    ax.add_artist(text2)\n\n", "def test_draggble_annotation(loc):\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, \"draggable text\")\n    ann = AnnotationBbox(text, (0.5, 0.5), loc=loc,\n                           xycoords='axes fraction',\n                           boxcoords='axes fraction',\n                           frameon=True,\n                           boxstyle=\"round\",\n                           arrowprops=dict(arrowstyle=\"->\"))\n    ann.draggable(True)\n    ann.set_animated(True)\n    fig.canvas.draw()\n    assert ann.draggable()\n"], "sample_1048": ["compilation error", "compilation error", "def test_parabola_tangents():\n    l1 = Line(Point(1, -2), Point(-1,-2))\n    l2 = Line(Point(1, 2), Point(-1,2))\n\n    p1 = Point(0,0)\n    parabola1 = Parabola(p1, l1)\n\n    assert len(parabola1.tangent_lines(Point2D(2, 0))) > 0\n    assert len(parabola1.tangent_lines(Point2D(0, -1))) == 2\n    assert len(parabola1.tangent_lines(Point2D(0, 10))) == 2\n    assert parabola1.tangent_lines(Point2D(-2, 0)) == [Line2D(Point2D(-2, 0), Point2D(-2, 1))]\n    assert parabola1.tangent_lines(Point(4, 3)) == [Line(Point(4,3), Point(12,3))]\n\n", "compilation error", "compilation error"], "sample_1202": ["    def test_issue_20614():\n        assert (1/sqrt(2)).evalf() == sqrt(2)/2\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_111": ["    def test_changelist_view_list_editable_multiple_objects_changed(self):\n        a = Swallow.objects.create(origin='Swallow A', load=4, speed=1)\n        Swallow.objects.create(origin='Swallow B', load=2, speed=2)\n        data = {\n            'form-TOTAL_FORMS': '2',\n            'form-INITIAL_FORMS': '2',\n            'form-MIN_NUM_FORMS': '0',\n            'form-MAX_NUM_FORMS': '1000',\n            'form-0-uuid': str(a.pk),\n            'form-0-load': '10',\n            'form-1-uuid': str(Swallow.objects.last().pk),\n            'form-1-speed': '5',\n            '_save': 'Save',\n        }\n        superuser = self._create_superuser('superuser')\n        self.client.force_login(superuser)\n        changelist_url = reverse('admin:admin_changelist_swallow_changelist')\n        response = self.client.post(changelist_url, data=data)\n        self.assertEqual(response.status_code, 200)\n        messages = self.client.messages\n        self.assertEqual(len(messages), 1)\n        self.assertEqual(str(messages[0]), \"2 swallows were changed successfully.\")\n\n", "    def test_empty_result_set(self):\n        \"\"\"\n        Ensure that the changelist displays an appropriate message when\n        there are no objects to display (#22748).\n        \"\"\"\n        CustomUser.objects.all().delete()\n        self.admin_login(username='super', password='secret')\n        self.selenium.get(self.live_server_url + reverse('admin:customuser_changelist'))\n        self.assertContains(self.selenium.page_source, 'No CustomUsers found.')\n", "    def test_searchbox_with_quotes(self):\n        \"\"\"\n        Regression tests for ticket #18072: Ensure searchbox works correctly\n        with quoted strings.\n        \"\"\"\n        Parent.objects.create(name='parent one')\n        Parent.objects.create(name='parent two')\n        Parent.objects.create(name='\"parent three\"')\n        Parent.objects.create(name='parent four')\n\n        m = ParentAdmin(Parent, custom_site)\n        superuser = self._create_superuser('superuser')\n        request = self._mocked_authenticated_request('/parent/', superuser)\n        response = m.changelist_view(request)\n\n        self.assertIn('parent one', response.content.decode())\n        self.assertIn('parent two', response.content.decode())\n        self.assertIn('parent four', response.content.decode())\n\n        # Search for \"parent three\" using quotes.\n        request = self._mocked_authenticated_request('/parent/?q=%22parent+three%22', superuser)\n        response = m.changelist_view(request)\n\n        self.assertNotIn('parent one', response.content.decode())\n        self.assertNotIn('parent two', response.content.decode())\n        self.assertNotIn('parent four', response.content.decode())\n        self.assertIn('\"parent three\"', response.content.decode())\n", "    def test_changelist_view_list_editable_changed_objects_filters_empty_objects(self):\n        \"\"\"\n        list_editable edits use a filtered queryset to limit the changes to\n        objects that actually exist in the database.\n        \"\"\"\n        a = Swallow.objects.create(origin='Swallow A', load=4, speed=1)\n        data = {\n            'form-TOTAL_FORMS': '2',\n            'form-INITIAL_FORMS': '2',\n            'form-MIN_NUM_FORMS': '0',\n            'form-MAX_NUM_FORMS': '1000',\n            'form-0-uuid': str(a.pk),\n            'form-0-load': '10',\n            '_save': 'Save',\n        }\n        superuser = self._create_superuser('superuser')\n        self.client.force_login(superuser)\n        changelist_url = reverse('admin:admin_changelist_swallow_changelist')\n\n        Swallow.objects.filter(pk=a.pk).delete()\n\n        with CaptureQueriesContext(connection) as context:\n            response = self.client.post(changelist_url, data=data)\n            self.assertEqual(response.status_code, 200)\n            self.assertIn('WHERE', context.captured_queries[4]['sql'])\n            self.assertIn('IN', context.captured_queries[4]['sql'])\n            # Check only the first few characters since the UUID may have dashes.\n            self.assertIn(str(a.pk)[:8], context.captured_queries[4]['sql'])\n", "    def test_filter_dates_with_timezone_aware_datetime(self):\n        \"\"\"\n        Test filtering date fields with timezone-aware datetimes.\n\n        This test ensures that the ChangeList correctly handles filtering\n        when date fields contain timezone-aware datetime objects. It checks\n        both the \"is after\" and \"is before\" filters.\n        \"\"\"\n        # Create a timezone-aware datetime.\n        from django.utils import timezone\n        now = timezone.now()\n\n        # Create some objects with different dates.\n        Event.objects.create(name='Past Event', start_time=now - timedelta(days=2))\n        Event.objects.create(name='Today Event', start_time=now)\n        Event.objects.create(name='Future Event', start_time=now + timedelta(days=2))\n\n        # Login as superuser\n        self.admin_login(username='super', password='secret')\n        self.selenium.get(self.live_server_url + reverse('admin:event_event_changelist'))\n\n"], "sample_1041": ["compilation error", "compilation error", "compilation error", "def test_MatrixElement_subs():\n    A = ImmutableMatrix([[1, 2], [3, 4]])\n    B = MatrixSymbol('B', 2, 2)\n    assert A[0, 1].subs(A[0, 1], B[0, 0]) == B[0, 0]\n    assert A[1, 0].subs(A, B) == B[1, 0]\n\n", "    def test_MatrixSymbol_determinant():\n        A = MatrixSymbol('A', 4, 4)\n        assert A.as_explicit().det() == A[0, 0]*A[1, 1]*A[2, 2]*A[3, 3] - \\\n            A[0, 0]*A[1, 1]*A[2, 3]*A[3, 2] - A[0, 0]*A[1, 2]*A[2, 1]*A[3, 3] + \\\n            A[0, 0]*A[1, 2]*A[2, 3]*A[3, 1] + A[0, 0]*A[1, 3]*A[2, 1]*A[3, 2] - \\\n            A[0, 0]*A[1, 3]*A[2, 2]*A[3, 1] - A[0, 1]*A[1, 0]*A[2, 2]*A[3, 3] + \\\n            A[0, 1]*A[1, 0]*A[2, 3]*A[3, 2] + A[0, 1]*A[1, 2]*A[2, 0]*A[3, 3] - \\\n            A[0, 1]*A[1, 2]*A[2, 3]*A[3, 0] - A[0, 1]*A[1, 3]*A[2, 0]*A[3, 2] + \\\n            A[0, 1]*A[1, 3]*A[2, 2]*A[3, 0] + A[0, 2]*A[1, 0]*A[2, 1]*A[3, 3] - \\\n            A[0, 2]*A[1, 0]*A[2, 3]*A[3, 1] - A[0, 2]*A[1, 1]*A[2, 0]*A[3, 3] + \\\n            A[0, 2]*A[1, 1"], "sample_77": ["    def test_urlize_trim_url_limit(self):\n        self.assertEqual(urlize('https://www.example.com/very/long/url/that/needs/trimming', trim_url_limit=20),\n                         '<a href=\"https://www.example.com/very/long/url/that/needs/trimming\">https://www.example.com/very/long/url\u2026</a>')\n", "    def test_urlize_trim_url_limit(self):\n        self.assertEqual(\n            urlize('Check out https://www.example.com/very/long/path/to/a/page?with=lots&of=parameters', trim_url_limit=20),\n            'Check out <a href=\"https://www.example.com/very/long/path/to/a/page?with=lots&of=parameters\">https://www.example.com/very/\u2026</a>'\n        )\n\n", "    def test_urlize_trim_url(self):\n        value = 'Visit this very long URL: https://www.example.com/path/to/a/very/long/url/with/lots/of/segments.html'\n\n        # Expected output with default limit\n        expected_default = 'Visit this very long URL: <a href=\"https://www.example.com/path/to/a/very/long/url/with/lots/of/segments.html\">https://www.example.com/path/to/a/very/long/url/\u2026</a>'\n\n        # Expected output with a specific limit\n        expected_limit_50 = 'Visit this very long URL: <a href=\"https://www.example.com/path/to/a/very/long/url/with/lots/of/segments.html\">https://www.example.com/path/to/a/very/long/url/with/lots/of/segments.html\u2026</a>'\n\n        self.assertEqual(urlize(value, trim_url_limit=20), expected_default)\n        self.assertEqual(urlize(value, trim_url_limit=50), expected_limit_50)\n\n", "    def test_urlize_trim_url(self):\n        text = 'Visit my website at http://example.com/very/long/path?with=long&query=string for more info.'\n        self.assertEqual(urlize(text, trim_url_limit=20),\n                         'Visit my website at <a href=\"http://example.com/very/long/path?with=long&query=string\">http://example.com/very/long\u2026</a> for more info.')\n", "    def test_urlize_trim_url_limit(self):\n        value = 'This is a really really really long url: https://www.example.com/very/long/path?query=string&another=param#fragment. This is some more text.'\n        expected = 'This is a really really really long url: <a href=\"https://www.example.com/very/long/path?query=string&another=param#fragment\">https://www.example.com/very/long/path\u2026? </a>This is some more text.'\n        self.assertEqual(urlize(value, trim_url_limit=30), expected)\n"], "sample_507": ["    def test_plot_empty(self, plotter):\n        ax = plt.figure().subplots()\n        plotter(ax, [], [])\n        assert len(ax.xaxis.get_majorticklocs()) == 0\n        assert len(ax.yaxis.get_majorticklocs()) == 0 \n", "    def test_hist_with_existing_units(self):\n        fig, ax = plt.subplots()\n        ax.xaxis.update_units(['a', 'b', 'c'])\n        n, bins, patches = ax.hist(['a', 'b', 'a', 'c', 'ff'])\n        assert n.shape == (10,)\n        np.testing.assert_allclose(n, [2., 0., 0., 1., 0., 0., 1., 0., 0., 1.])\n", "    def test_plot_empty(self, plotter):\n        ax = plt.figure().subplots()\n        plotter(ax, [], [])\n        assert len(ax.xaxis.units._mapping) == 0\n        assert len(ax.yaxis.units._mapping) == 0\n", "    def test_plot_empty_data(self, plotter):\n        ax = plt.figure().subplots()\n        plotter(ax, [], [])\n        assert len(ax.xaxis.units._mapping) == 0\n        assert len(ax.yaxis.units._mapping) == 0\n", "    def test_plot_dates(self, plotter):\n        dates = [datetime(2023, 1, i) for i in range(1, 4)]\n        ax = plt.figure().subplots()\n        plotter(ax, dates, [1, 2, 3])\n        axis_test(ax.xaxis, dates)\n"], "sample_441": ["    def test_no_change(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {\"password1\": user.password, \"password2\": user.password}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertTrue(form.is_valid())\n        form.save()\n        self.assertEqual(form.changed_data, [])\n", "    def test_invalid_password(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {'password1': 'password', 'password2': 'wrong'}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(\n            form.errors['password2'], [form.error_messages['password_mismatch']]\n        )\n        self.assertEqual(form.changed_data, [\"password\"])\n", "    def test_invalid_password(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {\"password1\": \"test\", \"password2\": \"test\"}\n        form = AdminPasswordChangeForm(user, data)\n        form.is_valid()\n        self.assertEqual(form.errors, {'password1': ['Short passwords are not allowed.'], 'password2': ['Short passwords are not allowed.']})\n", "    def test_no_password_change(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {\"password1\": \"\", \"password2\": \"\"}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.has_changed())\n", "    def test_password_validation_minimum_length(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {\n            \"password1\": \"short\",\n            \"password2\": \"short\",\n        }\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(\n            form[\"password1\"].errors,\n            [\n                str(\n                    form.error_messages[\"password_too_short\"]\n                    % {\"min_length\": 12}\n                )\n            ],\n        )\n\n"], "sample_482": ["    def test_empty_list(self):\n        output = self.engine.render_to_string(\"escapeseq_empty_list\", {\"a\": []})\n        self.assertEqual(output, \"\")\n", "    def test_empty_list(self):\n        output = self.engine.render_to_string(\"escapeseq_empty_list\", {\"a\": []})\n        self.assertEqual(output, \"\")\n\n", "    def test_none(self):\n        output = self.engine.render_to_string(\"escapeseq_none\", {\"a\": None})\n        self.assertEqual(output, \"-- None\")\n", "    def test_empty(self):\n        output = self.engine.render_to_string(\"escapeseq_empty\", {\"a\": []})\n        self.assertEqual(output, \"\")\n", "    def test_empty(self):\n        output = self.engine.render_to_string(\"escapeseq_empty\", {\"a\": []})\n        self.assertEqual(output, \"\")\n"], "sample_90": ["    def test_modelform_factory_metaclass_with_formfield_callback(self):\n            return db_field.formfield(**kwargs)\n\n        class CustomForm(forms.ModelForm):\n            class Meta:\n                model = Person\n                formfield_callback = callback\n\n        NewForm = modelform_factory(Person, form=CustomForm)\n        self.assertEqual(NewForm.base_fields['name'].widget, forms.CharField().widget)\n", "    def test_modelform_factory_with_formfield_callback_and_widget(self):\n            kwargs['widget'] = forms.Textarea\n            return db_field.formfield(**kwargs)\n\n        widget = forms.Select\n\n        Form = modelform_factory(\n            Person,\n            fields='__all__',\n            formfield_callback=callback,\n            widgets={'name': widget},\n        )\n        self.assertEqual(Form.base_fields['name'].widget.__class__, forms.Textarea) \n", "    def test_modelform_clean_with_many_to_many(self):\n        class ColourfulItemForm(forms.ModelForm):\n            class Meta:\n                model = ColourfulItem\n                fields = '__all__'\n            \n                cleaned_data = self.cleaned_data\n                if not cleaned_data.get('colours').exists():\n                    raise forms.ValidationError('You must select at least one colour.')\n                return cleaned_data\n\n        colour = Colour.objects.create(name='Blue')\n        data = {'name': 'Item name', 'colours': [colour.pk]}\n        form = ColourfulItemForm(data)\n        self.assertTrue(form.is_valid())\n        form.save()\n\n        data = {'name': 'Item name', 'colours': []}\n        form = ColourfulItemForm(data)\n        self.assertFalse(form.is_valid())\n        self.assertIn('colours', form.errors)\n\n", "    def test_modelform_factory_with_form_kwargs(self):\n        \"\"\"\n        Test that passing form kwargs to modelform_factory works.\n        \"\"\"\n        Form = modelform_factory(Person, form=forms.ModelForm, form_kwargs={'auto_id': False})\n        form = Form({'name': 'John Doe'})\n        self.assertFalse(any(id_ for id_ in form.as_p().split() if id_.startswith('id_')))\n\n", "    def test_modelform_factory_with_exclude(self):\n        Form = modelform_factory(Person, exclude=('id',))\n        self.assertEqual(list(Form.base_fields), [\"name\"])\n\n\n\n"], "sample_25": ["    def test_header_set_and_get_with_comments(self):\n        \"\"\"Test setting and getting header card values with comments.\"\"\"\n        h = fits.Header()\n        h['FOO'] = 'bar', 'This is a comment'\n        assert h['FOO'] == 'bar'\n        assert h.comments['FOO'] == 'This is a comment'\n        h['FOO'] = 2, 'Another comment'\n        assert h['FOO'] == 2\n        assert h.comments['FOO'] == 'Another comment'\n\n", "    def test_header_from_file(self):\n        hdulist = fits.open(self.data(\"test0.fits\"))\n\n        # This test the header is correctly loaded from file.\n        header = hdulist[0].header\n        assert header[\"SIMPLE\"] == True\n        assert header[\"BITPIX\"] == 8\n        assert header[\"NAXIS\"] == 2\n        assert header[\"NAXIS1\"] == 100\n        assert header[\"NAXIS2\"] == 100\n        hdulist.close()\n", "    def test_create_cardlist_from_header():\n        hdu = fits.PrimaryHDU()\n        hdu.header.append((\"a\", 1))\n        hdu.header.append((\"b\", 2))\n        cardlist = fits.CardList.from_header(hdu.header)\n        assert isinstance(cardlist, fits.CardList)\n        assert cardlist[0].keyword == \"a\"\n        assert cardlist[1].keyword == \"b\"\n", "    def test_card_equality(self):\n        hdu = fits.PrimaryHDU()\n        card1 = fits.Card(\"NAXIS\", 1024)\n        card2 = fits.Card(\"NAXIS\", 1024)\n        card3 = fits.Card(\"BITPIX\", 8)\n\n        assert card1 == card2\n        assert card1 != card3\n        assert card2 != card3\n        assert card1 != \"NAXIS\"\n", "    def test_header_copy_rvkc():\n        header = fits.Header()\n        header['DP1'] = 'NAXIS: 2'\n        header['DP1'] = 'AXIS.1: 1'\n        header['DP1'] = 'AUX.1.COEFF.0: 0'\n\n        header_copy = header.copy()\n        assert header_copy['DP1.NAXIS'] == 2.0\n        assert header_copy['DP1.AXIS.1'] == 1.0\n        assert header_copy['DP1.AUX.1.COEFF.0'] == 0.0\n"], "sample_201": ["    def test_empty_cookie_data(self):\n        storage = self.storage_class(self.get_request())\n        set_cookie_data(storage, [])\n        self.assertEqual(list(storage), [])\n", "    def test_store_empty_list(self):\n        storage = self.get_storage()\n        response = self.get_response()\n        storage.update(response)\n        self.assertEqual(stored_cookie_messages_count(storage, response), 0)\n", "    def test_empty_cookie(self):\n        storage = self.storage_class(self.get_request())\n        response = self.get_response()\n        # Ensure storage updates correctly to an empty cookie.\n        storage.update(response)\n        self.assertEqual(stored_cookie_messages_count(storage, response), 0)\n", "    def test_empty_cookie(self):\n        storage = self.storage_class(self.get_request())\n        response = self.get_response()\n        storage.update(response)\n        self.assertEqual(self.stored_messages_count(storage, response), 0)\n", "    def test_empty_cookie(self):\n        storage = self.get_storage()\n        response = self.get_response()\n        # An empty cookie should be handled correctly\n        self.assertEqual(self.stored_messages_count(storage, response), 0)\n        storage.update(response)\n        self.assertEqual(self.stored_messages_count(storage, response), 0)\n"], "sample_119": ["    def test_negated_related_isnull(self):\n        query = Query(ObjectC)\n        where = query.build_where(~Q(objecta__isnull=True))\n        self.assertTrue(where.negated)\n        lookup = where.children[0]\n        self.assertIsInstance(lookup, RelatedIsNull)\n        self.assertFalse(lookup.negated)\n        self.assertIsInstance(lookup.lhs, SimpleCol)\n        self.assertEqual(lookup.lhs.target, ObjectC._meta.get_field('objecta'))\n", "    def test_double_join_clone(self):\n        query = Query(Item)\n        query.add_select_related('creator__rankings')\n        clone = query.clone()\n        clone.add_select_related('creator__rankings__author')\n        self.assertEqual(query.select_related, {'creator': {'rankings': {}}})\n", "    def test_related_fields_with_transform(self):\n        query = Query(Item)\n        with register_lookup(CharField, Lower):\n            where = query.build_where(Q(creator__name__lower='foo'))\n        lookup = where.children[0]\n        self.assertIsInstance(lookup, Exact)\n        self.assertIsInstance(lookup.lhs, Lower)\n        self.assertIsInstance(lookup.lhs.lhs, SimpleCol)\n        self.assertEqual(lookup.lhs.lhs.target, Item._meta.get_field('creator').field.related_field.name)\n", "    def test_related_field_in_where(self):\n        query = Query(Item)\n        with register_lookup(CharField, Lower):\n            where = query.build_where(Q(creator__name__lower='foo'))\n        lookup = where.children[0]\n        self.assertIsInstance(lookup, Exact)\n        self.assertIsInstance(lookup.lhs, Lower)\n        self.assertIsInstance(lookup.lhs.lhs, SimpleCol)\n        self.assertEqual(lookup.lhs.lhs.target, Author._meta.get_field('name'))\n", "    def test_empty_related_lookup(self):\n        query = Query(Item)\n        with self.assertRaises(FieldError):\n            query.build_where(Q(creator__isnull=True, creator__name='Alice'))\n"], "sample_624": ["compilation error", "    def test_format_array_flat_empty() -> None:\n        actual = formatting.format_array_flat(np.array([]), 5)\n        expected = \"\"\n        assert expected == actual\n", "compilation error", "compilation error", "compilation error"], "sample_70": ["    def test_fast_delete_empty_related_set(self):\n        # Test that fast-deleting an object with an empty related set doesn't\n        # cause problems.\n        a = Avatar.objects.create()\n        self.assertEqual(User.objects.filter(avatar=a).count(), 0)\n        self.assertNumQueries(1, a.delete)\n", "    def test_fast_delete_related_objects_with_no_references(self):\n        u = User.objects.create()\n        a = Avatar.objects.create()\n        u.avatar = a\n        u.save()\n        self.assertNumQueries(2, u.delete())\n\n", "    def test_fast_delete_related_multiple_inheritance(self):\n        ch = RChild.objects.create()\n        r = ch.r_ptr\n        u = User.objects.create(avatar=Avatar.objects.create())\n        # Delete r, which triggers fast deletion of ch but not u.\n        self.assertNumQueries(2, r.delete)\n        self.assertFalse(RChild.objects.exists())\n        self.assertTrue(User.objects.exists())\n", "    def test_bulk_delete_large_related(self):\n        TEST_SIZE = 2000\n        s = S.objects.create(r=R.objects.create())\n        for i in range(TEST_SIZE):\n            T.objects.create(s=s)\n        # Calculate the number of queries needed.\n        batch_size = connection.ops.bulk_batch_size(['pk'], range(TEST_SIZE))\n        # The `delete` query for related objects is done in batches.\n        batches = ceil(TEST_SIZE / batch_size)\n        # One query for S.objects.all() and then one related fast delete for\n        # each batch.\n        fetches_to_mem = 1 + batches\n        # The `s` object is going to be deleted in a single query.\n        queries = fetches_to_mem + 1 \n        self.assertNumQueries(queries, S.objects.all().delete())\n        self.assertFalse(S.objects.exists())\n        self.assertFalse(T.objects.exists())\n", "    def test_fast_delete_empty_related_objects(self):\n        a = Avatar.objects.create(desc='a')\n        self.assertNumQueries(1, a.delete)\n"], "sample_940": ["    def test_signature_from_str_complex_annotations_with_forwardref():\n        signature = '(a: Callable[[str, int], float], b: ForwardRef(\"SomeType\")) -> None'\n        sig = inspect.signature_from_str(signature)\n        assert sig.parameters['a'].annotation == \"Callable[[str, int], float]\"\n        assert sig.parameters['b'].annotation == \"SomeType\"\n", "        def __get__(self, obj, objtype=None):\n            return obj.custom_value\n", "    def test_signature_from_str_async_function():\n        signature = 'async def func(a: int, *args: float, b: str = \"hello\", **kwargs: Datetime) -> None:'\n        sig = inspect.signature_from_str(signature)\n        assert list(sig.parameters.keys()) == ['a', 'args', 'b', 'kwargs']\n        assert sig.parameters['a'].annotation == 'int'\n        assert sig.parameters['args'].annotation == 'float'\n        assert sig.parameters['b'].annotation == 'str'\n        assert sig.parameters['kwargs'].annotation == 'Datetime'\n        assert sig.return_annotation == 'None'\n", "    def test_is_builtin_class_method_with_none_mro():\n        class MyClass:\n            pass\n\n        assert not inspect.is_builtin_class_method(MyClass(), 'my_method')\n", "compilation error"], "sample_887": ["    def test_calibration_with_prefit_estimator_and_sample_weight(data):\n        \"\"\"Check that sample weights can be provided when using a prefit estimator.\"\"\"\n        X, y = data\n        clf = LogisticRegression().fit(X, y)\n        sample_weight = np.ones_like(y)\n        \n        calibrated_clf = CalibratedClassifierCV(\n            estimator=clf, method='isotonic', cv='prefit'\n        )\n        calibrated_clf.fit(X, y, sample_weight=sample_weight)\n", "    def test_calibration_with_non_int_target(data):\n        \"\"\"\n        Test CalibratedClassifierCV with non-integer target.\n        \"\"\"\n        X, y = data\n        y = y.astype(float)\n        clf = LogisticRegression()\n        calibrated_clf = CalibratedClassifierCV(clf)\n\n        with pytest.raises(ValueError, match=\"Target values must be integers\"):\n            calibrated_clf.fit(X, y)\n", "compilation error", "    def test_calibrated_classifier_cv_with_class_weight(data):\n        \"\"\"Test that class_weight is properly passed to the underlying estimator\n        during cross-validation.\"\"\"\n        X, y = data\n        clf = LogisticRegression()\n        calibrated_clf = CalibratedClassifierCV(clf, cv=2)\n\n        calibrated_clf.fit(X, y, class_weight='balanced')\n\n        # For each fold, check that the fitted estimator has the correct class\n        # weights.\n        for est in calibrated_clf.calibrated_classifiers_:\n            assert est.estimator.class_weight == 'balanced'\n\n", "def test_calibration_display_score_isotonic(pyplot, iris_data_binary):\n    X, y = iris_data_binary\n\n    lr = LogisticRegression().fit(X, y)\n    viz = CalibrationDisplay.from_estimator(lr, X, y, method=\"isotonic\")\n    assert viz.score is not None\n"], "sample_335": ["    def test_decimalfield_empty_values(self):\n        f = DecimalField(max_digits=4, decimal_places=2)\n        self.assertIsNone(f.clean(''))\n        self.assertIsNone(f.clean(None))\n        f = DecimalField(max_digits=4, decimal_places=2, empty_value=0)\n        self.assertEqual(f.clean(''), decimal.Decimal('0'))\n        self.assertEqual(f.clean(None), decimal.Decimal('0'))\n\n", "    def test_decimalfield_empty_string(self):\n        f = DecimalField(max_digits=4, decimal_places=2)\n        \n        self.assertIsNone(f.clean(''))\n        self.assertIsNone(f.clean(None))\n        self.assertEqual(f.clean('1'), decimal.Decimal(\"1\"))\n", "    def test_decimalfield_custom_widget(self):\n        class CustomWidget(Widget):\n            pass\n        f = DecimalField(widget=CustomWidget())\n        self.assertEqual(f.widget, CustomWidget())\n", "    def test_decimalfield_empty_values(self):\n        f = DecimalField(max_digits=4, decimal_places=2)\n        self.assertIsNone(f.clean(None))\n        self.assertIsNone(f.clean(''))\n        self.assertEqual(f.clean('1'), decimal.Decimal('1.00'))\n", "def test_decimalfield_empty_values(self):\n    f = DecimalField(max_digits=4, decimal_places=2)\n    self.assertIsNone(f.clean(''))\n    self.assertIsNone(f.clean(None))\n    f.empty_values = ['']\n    self.assertIsNone(f.clean(''))\n    self.assertIsNone(f.clean(None))\n"], "sample_368": ["    def test_migrate_app_label_with_leading_underscore(self):\n        \"\"\"\n        Test applying migrations for an app label starting with an underscore.\n\n        Some project structures might use leading underscores in app labels.\n\n        \"\"\"\n        _private_1_impl = FakeMigration('_private_1')\n        _private_1 = ('_private', '1')\n        graph = MigrationGraph()\n        graph.add_node(_private_1, _private_1_impl)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {_private_1: _private_1_impl})\n        plan = executor.migration_plan({'_private': _private_1})\n\n        self.assertEqual(plan, [(_private_1_impl, True)])\n\n", "    def test_apply_migration_with_custom_sql(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.RunSQL('SELECT 1;'),\n            ]\n\n        executor = MigrationExecutor(connection)\n        with mock.patch.object(executor.connection, 'run_sql') as run_sql_mock:\n            executor.apply_migration(\n                ProjectState(),\n                Migration('0001_initial', 'custom_sql'),\n            )\n            run_sql_mock.assert_called_once_with('SELECT 1;')\n", "    def test_backwards_unapply_unrelated(self):\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_dependency(None, a2, a1)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            a2: a2_impl,\n            b1: b1_impl,\n        })\n\n        plan = executor.migration_plan({a1})\n\n        self.assertEqual(plan, [(b1_impl, False)])\n", "    def test_unapply_with_cycle(self):\n        \"\"\"\n        Test unapplying migrations in a graph with a cycle.\n\n        a: 1 <---- 2\n        b:    \\- 1\n\n        If we apply a1 and a2, then try to unapply a1, it shouldn't unapply b1.\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, b1, a1)\n        \n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            a2: a2_impl,\n            b1: b1_impl,\n        })\n\n        # Apply a1 and a2\n        executor.migration_plan({a1, a2})\n\n        # Unapply a1\n        plan = executor.migration_plan({a1}, fake=True)\n\n        # Should only unapply a2, and not b1\n        self.assertEqual(plan, [(a2_impl, False)])\n\n", "    def test_replace_migration_dependencies(self):\n        \"\"\" migrations are correctly handled when a migration is replaced by a squashed migration.\"\"\"\n\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        a3_impl = FakeMigration('a3')\n        a3 = ('a', '3')\n\n        squashed_impl = FakeMigration('squashed')\n        squashed = ('a', 'squashed')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(a3, a3_impl)\n        graph.add_node(squashed, squashed_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, a3, a2)\n\n        graph.add_dependency(None, squashed, a1)\n        graph.add_dependency(None, squashed, a2)\n        graph.add_dependency(None, squashed, a3)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            a2: a2_impl,\n            a3: a3_impl,\n        })\n\n        plan = executor.migration_plan({squashed})\n\n        self.assertEqual(plan, [(squashed_impl, True)])\n"], "sample_361": ["compilation error", "    def test_urlize_nofollow(self):\n        tests = (\n            ('Search for google.com/?q=! and see.',\n             'Search for <a href=\"http://google.com/?q=\" rel=\"nofollow\">google.com/?q=</a>! and see.'),\n            ('foo@example.com', '<a href=\"mailto:foo@example.com\" rel=\"nofollow\">foo@example.com</a>'),\n        )\n        for value, output in tests:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value, nofollow=True), output)\n", "    def test_urlize_nofollow(self):\n        self.assertEqual(\n            urlize('https://example.com', nofollow=True),\n            '<a href=\"https://example.com\" rel=\"nofollow\">https://example.com</a>'\n        )\n", "    def test_urlize_nofollow(self):\n        self.assertEqual(\n            urlize('http://example.com', nofollow=True),\n            '<a href=\"http://example.com\" rel=\"nofollow\">http://example.com</a>'\n        )\n        self.assertEqual(\n            urlize('https://example.com', nofollow=False),\n            '<a href=\"https://example.com\">https://example.com</a>'\n        )\n", "    def test_urlize_nofollow(self):\n        self.assertEqual(urlize('Search for google.com/?q=! and see.', nofollow=True),\n                         'Search for <a href=\"http://google.com/?q=\" rel=\"nofollow\">google.com/?q=</a>! and see.')\n"], "sample_113": ["    def test_model_with_many_to_many_field_through_table(self):\n        response = self.client.get(\n            reverse('django-admindocs-models-detail', args=['admin_docs', 'company'])\n        )\n        self.assertContains(\n            response,\n            \"number of related <a class='reference external' href='/admindocs/models/myapp.person/'>myapp.Person</a> objects\"\n        )\n        self.assertContains(\n            response,\n            \"all related <a class='reference external' href='/admindocs/models/myapp.person/'>myapp.Person</a> objects\"\n        )\n\n", "    def test_model_detail_with_custom_ordering(self):\n        response = self.client.get(reverse('django-admindocs-models-detail', args=['admin_docs', 'OrderedPerson']))\n        self.assertContains(response, '<th>Custom ordering</th>')\n        self.assertContains(response, '<dl class=\"docutils\">')\n        self.assertContains(response, '<dt><tt class=\"docutils literal\">ordering</tt></dt>')\n        self.assertContains(response, '<dd>Custom ordering field for this model</dd>')\n        self.assertContains(response, '</dl>')\n", "    def test_view_detail_redirect(self):\n        url = reverse('django-admindocs-views-detail', args=['django.contrib.admindocs.views.BaseAdminDocsView'])\n        response = self.client.get(url + '?redirect=True')\n        self.assertRedirects(response, reverse('django-admindocs-views-index'))\n", "    def test_model_detail_field_verbose_name(self):\n        response = self.client.get(reverse('django-admindocs-models-detail', args=['admin_docs', 'Person']))\n        self.assertContains(response, '<td><tt class=\"docutils literal\">first_name</tt> - First name</td>')\n\n", "    def test_get_view_name_with_module(self):\n        class InAClass:\n                pass\n        self.assertEqual(utils.get_view_name(InAClass.my_view), 'admin_docs.InAClass.my_view')\n\n"], "sample_1193": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_676": ["def test_folded_skips_with_different_reasons(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.skip(reason=\"reason1\")\n            pass\n\n        @pytest.mark.skip(reason=\"reason2\")\n            pass\n\n        @pytest.mark.skip(reason=\"reason1\")\n            pass\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*2 skipped*\"])\n\n", "    def test_failure_and_error_message(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n                assert False\n            \n\n                raise ValueError(\"Some error\")\n        \"\"\"\n        )\n\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines([\n            \"*test_failure_and_error_message.py FF\",\n        \n\n        \"*FAILED test_failure_and_error_message.py::test_fail*\",\n        \"*ERROR test_failure_and_error_message.py::test_error*\",\n        ]\n        )\n\n", "    def test_unicode_errors(testdir):\n        testdir.makepyfile(\n            \"\"\"\n                raise UnicodeEncodeError('test', 'utf-8', 1, 2, 'reason')\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines([\"*UnicodeEncodeError*\"])\n", "    def test_show_captured_stdout_stderr(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import sys\n                sys.stdout.write(\"stdout\")\n                sys.stderr.write(\"stderr\")\n        \"\"\"\n        )\n        result = testdir.runpytest(\"--show-capture=stdout\")\n        result.stdout.fnmatch_lines([\"stdout\"])\n        result = testdir.runpytest(\"--show-capture=stderr\")\n        result.stdout.fnmatch_lines([\"stderr\"])\n", "    def test_folded_skips_with_different_reasons(testdir):\n        testdir.makepyfile(\"\"\"\n            import pytest\n\n            @pytest.mark.skip(reason=\"Reason1\")\n                pass\n\n            @pytest.mark.skip(reason=\"Reason2\")\n                pass\n\n            @pytest.mark.skip(reason=\"Reason1\")\n                pass\n        \"\"\")\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines(\n            [\n                'skipping *Reason1*',\n                'skipping *Reason2*',\n            ]\n        )\n"], "sample_835": ["    def test_adaboost_oob_score():\n        # Check that Out-of-bag score can be computed for AdaBoostClassifier\n        # Regression test for:\n        # https://github.com/scikit-learn/scikit-learn/issues/14278\n        X, y = datasets.make_classification(n_samples=100, n_features=10,\n                                            random_state=42)\n        clf = AdaBoostClassifier(DecisionTreeClassifier(), n_estimators=50,\n                                out_of_bag_estimation=True)\n        clf.fit(X, y)\n\n        assert hasattr(clf, 'oob_score_')\n        assert isinstance(clf.oob_score_, float)\n", "    def test_decision_function_shape():\n        # Check that the shape of decision_function is consistent for\n        # both classification and regression.\n        # Regression test for:\n        # https://github.com/scikit-learn/scikit-learn/issues/19953\n        clf = AdaBoostClassifier(random_state=42).fit(X, y_class)\n\n        assert clf.decision_function(X).shape == (len(X),)\n\n        reg = AdaBoostRegressor(random_state=42).fit(X, y_regr)\n        assert reg.decision_function(X).shape == (len(X),)\n\n", "def test_adaboost_invalid_sample_weight():\n    # check that AdaBoost raises an error with invalid sample weight format\n    X, y = datasets.make_classification(n_samples=100, random_state=42)\n\n    clf = AdaBoostClassifier(random_state=42)\n    with pytest.raises(ValueError):\n        clf.fit(X, y, sample_weight='invalid_format')\n\n    reg = AdaBoostRegressor(random_state=42)\n    with pytest.raises(ValueError):\n        reg.fit(X, y, sample_weight='invalid_format')\n", "    def test_sparse_feature_importance():\n        # Test feature importance calculation for sparse input.\n\n        X, y = datasets.make_classification(n_samples=2000,\n                                            n_features=10,\n                                            n_informative=3,\n                                            n_redundant=0,\n                                            n_repeated=0,\n                                            shuffle=False,\n                                            random_state=1)\n        X_sparse = csc_matrix(X)\n\n        for alg in ['SAMME', 'SAMME.R']:\n            clf = AdaBoostClassifier(algorithm=alg)\n            clf.fit(X_sparse, y)\n            importances = clf.feature_importances_\n\n            assert importances.shape[0] == 10\n            assert (importances[:3, np.newaxis] >= importances[3:]).all()\n", "    def test_adaboost_estimator_weights():\n        # Test that estimator weights are positive.\n        # regression test for:\n        # https://github.com/scikit-learn/scikit-learn/issues/17500\n        X_train, X_test, y_train, y_test = train_test_split(\n            *datasets.load_digits(return_X_y=True), random_state=42\n        )\n        model = AdaBoostClassifier(random_state=42)\n        model.fit(X_train, y_train)\n\n        assert np.all(model.estimator_weights_ > 0)\n"], "sample_1010": ["compilation error", "def test_issue_14877():\n    from sympy.physics.quantum.state import Ket, Bra\n    from sympy.physics.quantum.operator import Operator\n    j = Symbol('j', integer=True)\n    m = Symbol('m', integer=True)\n\n    ket = Ket(j, m)\n    bra = Bra(j, m)\n    op = Operator('O')\n    assert latex(ket) == r\"\\left| j, m \\right\\rangle\"\n    assert latex(bra) == r\"\\left\\langle j, m \\right|\"\n    assert latex(op*ket) == r\"O \\left| j, m \\right\\rangle\"\n    assert latex(bra*op) == r\"\\left\\langle j, m \\right| O\"\n    assert latex(bra*op*ket) == r\"\\left\\langle j, m \\right| O \\left| j, m \\right\\rangle\"\n\n", "compilation error", "def test_issue_14564():\n    from sympy.physics.quantum import Operator, Dagger\n    a = Operator('a')\n    assert latex(Dagger(a) * a) == r\"\\dagger a a\"\n", "def test_OuterProduct_printing():\n    from sympy.tensor.functions import OuterProduct\n    A = MatrixSymbol(\"A\", 3, 3)\n    B = MatrixSymbol(\"B\", 3, 3)\n    assert latex(OuterProduct(A, B)) == r\"A \\otimes B\"\n"], "sample_532": ["compilation error", "def test_contourf_negative_linestyles():\n    delta = 0.025\n    x = np.arange(-3.0, 3.0, delta)\n    y = np.arange(-2.0, 2.0, delta)\n    X, Y = np.meshgrid(x, y)\n    Z1 = np.exp(-X**2 - Y**2)\n    Z2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\n    Z = (Z1 - Z2) * 2\n\n    # Negative contour defaults to dashed\n    fig1, ax1 = plt.subplots()\n    CS1 = ax1.contourf(X, Y, Z, 6, cmap='RdBu')\n    ax1.set_title('Filled contour - negative contours dashed (default)')\n    assert CS1.negative_linestyles == 'dashed'  # default\n\n    # Change negative_linestyles using rcParams\n    plt.rcParams['contour.negative_linestyle'] = 'dotted'\n    fig2, ax2 = plt.subplots()\n    CS2 = ax2.contourf(X, Y, Z, 6, cmap='RdBu')\n    ax2.set_title('Filled contour - negative contours dotted (using rcParams)')\n    assert CS2.negative_linestyles == 'dotted'\n\n    # Change negative_linestyles using negative_linestyles kwarg\n    fig3, ax3 = plt.subplots()\n    CS3 = ax3.contourf(X, Y, Z, 6, cmap='RdBu',\n                      negative_linestyles='dashdot')\n    ax3.set_title('Filled contour - negative contours dashdot')\n    assert CS3.negative_linestyles == 'dashdot' \n", "compilation error", "def test_contour_with_empty_data():\n    fig, ax = plt.subplots()\n    with pytest.raises(ValueError, match=r\"Input data can not be empty\"):\n        ax.contour([[], []])\n", "compilation error"], "sample_855": ["def test_dummy_classifier_predict_proba_output_shape():\n    X = [[0] * 10] * 50\n    y = [0] * 50\n    clf = DummyClassifier(strategy=\"uniform\", random_state=0)\n    clf.fit(X, y)\n    \n    probas = clf.predict_proba(X)\n    assert probas.shape == (50, 1)\n", "    def test_dummy_classifier_sample_weight_multiclass():\n        X = [[0]] * 5\n        y = np.array([0, 1, 2, 0, 1])\n        sample_weight = np.array([0.1, 1., 0.1, 0.2, 1.])\n        clf = DummyClassifier(strategy=\"stratified\").fit(X, y, sample_weight)\n        assert_array_almost_equal(clf.class_prior_,\n                                  [0.2/1.4, 0.3/1.4, 0.1/1.4])\n\n", "    def test_dummy_classifier_on_empty_X_with_constant(self):\n        X = np.array([])\n        y = np.array([0])\n        clf = DummyClassifier(strategy='constant')\n        clf.fit(X, y)\n        with pytest.raises(ValueError):\n            clf.predict(np.array([[0]]))\n", "    def test_classifier_predict_proba_raises_error_for_unexpected_classes():\n        X = [[0]] * 5\n        y = [0, 1, 2, 3, 4]\n\n        clf = DummyClassifier(strategy=\"most_frequent\")\n\n        clf.fit(X, y)\n        with pytest.raises(ValueError):\n            clf.predict_proba([[0]])\n\n", "def test_dummy_classifier_raises_error_on_empty_y():\n    X = [[0]] * 3\n    y = []\n    clf = DummyClassifier()\n    with pytest.raises(ValueError, match=\"y should not be empty\"):\n        clf.fit(X, y)\n"], "sample_1097": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_470": ["    def test_lazy_object_pickle(self):\n        lazy_obj = lazy(lambda: {\"key\": \"value\"}, dict)\n        pickled = pickle.dumps(lazy_obj)\n        unpickled = pickle.loads(pickled)\n        self.assertEqual(unpickled(), {\"key\": \"value\"})\n\n", "    def test_lazy_text_cast(self):\n        lazy_obj = lazy(lambda: \"test\", str)()\n        self.assertEqual(str(lazy_obj), \"test\")\n", "    def test_lazy_pickle(self):\n        original_object = object()\n        lazy_obj = lazy(lambda: original_object, object)\n        self.assertEqual(pickle.loads(pickle.dumps(lazy_obj())), original_object)\n\n", "    def test_keep_lazy_text(self):\n        @keep_lazy_text\n            return f\"{arg1} {arg2}\"\n\n        self.assertEqual(my_func(\"Hello\", \"world\"), \"Hello world\")\n\n        lazy_arg1 = lazy(lambda: \"Hello\", str)\n        self.assertEqual(my_func(lazy_arg1, \"world\"), \"Hello world\")\n\n        lazy_arg2 = lazy(lambda: \"world\", str)\n        self.assertEqual(my_func(\"Hello\", lazy_arg2), \"Hello world\")\n\n        lazy_arg1_and_2 = lazy(lambda: \"Hello\", str), lazy(lambda: \"world\", str)\n        self.assertEqual(my_func(*lazy_arg1_and_2), \"Hello world\")\n\n", "    def test_keep_lazy_text(self):\n            return text.upper()\n\n        lazy_func = keep_lazy_text(my_func)\n        self.assertEqual(lazy_func(\"hello\"), \"HELLO\")\n"], "sample_41": ["compilation error", "compilation error", "compilation error", "    def test_unit_with_units_in_name():\n        u.Unit(\"arcsec/pixel\", parse_strict='silent')\n\n", "compilation error"], "sample_247": ["    def test_alias_filter_with_subquery(self):\n        subquery = Book.objects.alias(\n            average_rating_alias=Avg('rating')\n        ).values('average_rating_alias')\n        qs = Author.objects.filter(\n            book__in=Subquery(subquery)\n        ).annotate(\n            average_rating=Avg('book__rating')\n        )\n\n        self.assertEqual(qs.count(), 5)\n", "    def test_select_related_alias_with_fk_reverse(self):\n        qs = Book.objects.alias(\n            publisher_name_alias=F('publisher__name')\n        ).select_related('publisher').filter(publisher_name_alias='Apress')\n        self.assertEqual(qs.count(), 1)\n        self.assertEqual(qs.first().publisher.name, 'Apress')\n", "    def test_alias_aggregate_with_subquery(self):\n        qs = (\n            Book.objects.alias(\n                rating_alias=F('rating'),\n            ).annotate(\n                avg_price_by_rating=Subquery(\n                    Book.objects.filter(rating=OuterRef('rating_alias')).values('price').annotate(avg_price=Avg('price'))\n                ),\n            )\n        )\n        self.assertCountEqual(qs, [\n            {'avg_price_by_rating': Decimal('30.00')},\n            {'avg_price_by_rating': Decimal('29.69')},\n            {'avg_price_by_rating': Decimal('82.80')},\n            {'avg_price_by_rating': Decimal('75.00')},\n        ])\n", "    def test_alias_with_subquery(self):\n        qs = Book.objects.alias(\n            max_rating=Subquery(Book.objects.exclude(pk=OuterRef('pk')).values('rating').order_by('-rating').first()),\n        ).filter(max_rating__lt=F('rating'))\n        self.assertSequenceEqual(qs, [self.b1])\n", "    def test_filter_alias_with_subquery(self):\n        subquery = Book.objects.values('rating').annotate(\n            rating_count=Count('rating')\n        ).filter(rating_count__gt=1)\n        qs = Book.objects.alias(\n            rating_alias=F('rating'),\n        ).filter(rating_alias__in=subquery.values('rating'))\n        self.assertIs(hasattr(qs.first(), 'rating_alias'), False)\n        self.assertEqual(qs.count(), Book.objects.filter(rating=4.0).count())\n"], "sample_1016": ["compilation error", "def test_octave_assign_to_lambda():\n    f = Lambda(x, x**2 + 1)\n    assert mcode(f, assign_to='f') == \"f = @(x) x.^2 + 1;\"\n", "def test_Lambda():\n    f = Lambda(x, x**2)\n    assert mcode(f(x)) == 'x.^2'\n    assert mcode(f(2*x)) == ' (2*x).^2'\n\n", "def test_polygamma_printing():\n    assert mcode(polygamma(0, x)) == 'psi(0, x)'\n    assert mcode(polygamma(1, x)) == 'psi(1, x)'\n    assert mcode(polygamma(n, x)) == 'psi(n, x)'\n\n", "def test_piecewise_with_piecewise():\n    pw1 = Piecewise((x, x < 0), (x**2, True))\n    pw2 = Piecewise((x**3, x > 0), (x**4, True))\n    assert mcode(Piecewise((pw1, x < 1), (pw2, True))) == (\n        \"((x < 1).*((x < 0).*x + (~(x < 0)).*x.^2) + (~(x < 1)).*((x > 0).*x.^3 + (~(x > 0)).*x.^4))\"\n    )\n"], "sample_555": ["    def test_arc_linewidth(self):\n        fig, ax = plt.subplots()\n        arc = Arc((0.5, 0.5), 0.5, 0.5, theta1=0, theta2=180, linewidth=5)\n        ax.add_patch(arc)\n        ax.set_aspect('equal')\n", "    def test_arc3d_plot():\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection='3d')\n        arc = Arc3D((0, 0, 0), 1, 1, 0, 90)\n        ax.add_patch(arc)\n        ax.autoscale()\n", "    def test_patch_facecolor():\n        fig, ax = plt.subplots()\n        rect = Rectangle((0.2, 0.2), 0.4, 0.4, facecolor='lightblue')\n        ax.add_patch(rect)\n        ax.set_xlim(0, 1)\n        ax.set_ylim(0, 1)\n", "    def test_arc_with_transforms(self):\n        fig, ax = plt.subplots()\n        arc = Arc((0, 0), 1, 1, theta1=0, theta2=90, transform=ax.transAxes)\n        ax.add_patch(arc)\n        ax.set_xlim(-0.2, 1.2)\n        ax.set_ylim(-0.2, 1.2)\n\n", "    def test_boxstyle_defaults(self, shape, kwargs):\n        box = getattr(BoxStyle, shape)(**kwargs)\n        # Basic checks on the returned box\n        assert isinstance(box, BoxStyle)\n        assert box.trans == mpl.transforms.IdentityTransform()\n        assert box.pad is None\n"], "sample_1066": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_805": ["    def test_mean_tweedie_deviance_empty_input():\n        # Test empty input arrays\n        y_true = []\n        y_pred = []\n        assert_raises(ValueError, mean_tweedie_deviance, y_true, y_pred)\n", "def test_tweedie_deviance_multioutput():\n    y_true = [[1, 0], [0, 1], [1, 1]]\n    y_pred = [[0, 0], [1, 1], [0, 1]]\n\n    # Test with raw_values\n    deviance = mean_tweedie_deviance(y_true, y_pred, p=1,\n                                    multioutput='raw_values')\n    assert deviance.shape == (2,)\n\n    # Test with uniform_average\n    deviance = mean_tweedie_deviance(y_true, y_pred, p=1,\n                                    multioutput='uniform_average')\n    assert np.isscalar(deviance)\n\n    # Test with variance_weighted\n    deviance = mean_tweedie_deviance(y_true, y_pred, p=1,\n                                    multioutput='variance_weighted')\n    assert np.isscalar(deviance)\n\n    # Test with invalid multioutput\n    with pytest.raises(ValueError):\n        mean_tweedie_deviance(y_true, y_pred, p=1,\n                             multioutput='invalid_option')\n", "    def test_tweedie_deviance_negative_values():\n        y_true = np.array([1, 2, 3, 4])\n        y_pred = np.array([0.5, 1, 1.5, 2])\n        with pytest.raises(ValueError):\n            mean_tweedie_deviance(y_true, y_pred, p=-1)\n\n", "def test_multioutput_regression_different_lengths():\n    y_true = [[1, 0, 0, 1], [0, 1, 1, 1], [1, 1, 0, 1]]\n    y_pred = [[0, 0, 0], [1, 0, 1], [0, 0, 0]]\n\n    with pytest.raises(ValueError,\n                       match=\"y_true and y_pred arrays should have \"\n                             \"the same number of samples\"):\n        mean_squared_error(y_true, y_pred)\n", "def test_mean_tweedie_deviance_zero_variance():\n    y_true = np.zeros(100)\n    y_pred = np.zeros(100)\n    for p in [0, 1, 2, 3]:\n        assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, p=p), 0.0)\n\n"], "sample_742": ["def test_logreg_coef_rounding():\n    X, y = make_classification(n_samples=50, n_features=20, random_state=0)\n    # Set a small tolerance for convergence to easily trigger rounding\n    lr = LogisticRegression(tol=1e-7, solver='liblinear')\n    lr.fit(X, y)\n    assert_array_almost_equal(lr.coef_, lr.coef_, decimal=6)\n", "def test_logreg_predict_proba_ovr():\n    X, y = make_classification(n_samples=10, n_features=20, random_state=0,\n                               n_classes=3, n_informative=10)\n\n    clf = LogisticRegression(multi_class='ovr', solver='lbfgs')\n    clf.fit(X, y)\n    probabilities = clf.predict_proba(X)\n    assert_almost_equal(np.sum(probabilities, axis=1), np.ones(len(y)))\n    assert probabilities.shape == (len(y), 3)\n", "    def test_saga_intercept_scaling():\n        # Test intercept scaling for saga solver.\n        X = np.random.rand(100, 5)\n        y = np.random.randint(0, 2, size=100)\n\n        for intercept_scaling in [0.1, 1.0, 10.0]:\n            lr = LogisticRegression(solver='saga', intercept_scaling=intercept_scaling)\n            lr.fit(X, y)\n            assert_array_equal(lr.intercept_, lr.coef_[:, 0] * intercept_scaling)\n", "    def test_logreg_l1_penalty_sparse_data_intercept():\n        rng = np.random.RandomState(42)\n        n_samples = 50\n        X, y = make_classification(n_samples=n_samples, n_features=20,\n                                   random_state=0)\n        X_noise = rng.normal(scale=0.1, size=(n_samples, 3))\n        X_constant = np.ones(shape=(n_samples, 2))\n        X = np.concatenate((X, X_noise, X_constant), axis=1)\n        X[X < 1] = 0\n        X = sparse.csr_matrix(X)\n\n        # Test with fit_intercept=True\n        lr_liblinear = LogisticRegression(penalty=\"l1\", C=1.0, solver='liblinear',\n                                         max_iter=1000, tol=1e-10)\n        lr_liblinear.fit(X, y)\n\n        lr_saga = LogisticRegression(penalty=\"l1\", C=1.0, solver='saga',\n                                     max_iter=1000, tol=1e-10)\n        lr_saga.fit(X, y)\n\n        assert_array_almost_equal(lr_saga.coef_, lr_liblinear.coef_)\n        # Noise and constant features should be regularized to zero by the l1\n        # penalty\n        assert_array_almost_equal(lr_liblinear.coef_[0, -5:], np.zeros(5))\n        assert_array_almost_equal(lr_saga.coef_[0, -5:], np.zeros(5))\n\n        # Test with fit_intercept=False\n        lr_liblinear = LogisticRegression(penalty=\"l1\", C=1.0,\n                                         solver='liblinear',\n                                         fit_intercept=False,\n                                         max_iter=1000, tol=1e-10)\n        lr_liblinear.fit(X, y)\n\n        lr_saga = LogisticRegression(penalty=\"l1\", C=1.0,\n                                     solver='saga',\n                                     fit_intercept=False,\n                                     ", "def test_logreg_coef_sparse_data():\n    # Test that coef_ is correctly calculated for sparse data\n\n    rng = np.random.RandomState(42)\n    n_samples = 50\n    X, y = make_classification(n_samples=n_samples, n_features=20,\n                               random_state=0)\n    X_sparse = sparse.csr_matrix(X)\n\n    lr_dense = LogisticRegression(penalty=\"l1\", C=1.0, solver='saga',\n                                  fit_intercept=False,\n                                  max_iter=1000, tol=1e-10)\n    lr_dense.fit(X, y)\n\n    lr_sparse = LogisticRegression(penalty=\"l1\", C=1.0, solver='saga',\n                                  fit_intercept=False,\n                                  max_iter=1000, tol=1e-10)\n    lr_sparse.fit(X_sparse, y)\n    assert_array_almost_equal(lr_sparse.coef_, lr_dense.coef_)\n"], "sample_50": ["    def test_empty_params(self):\n        self.assertEqual(\n            self._run_it({}), (\n                ['psql'],\n                None,\n            )\n        )\n\n", "    def test_defaults(self):\n        self.assertEqual(\n            self._run_it({\n                'database': 'dbname',\n            }), (\n                ['psql', 'dbname'],\n                None,\n            )\n        )\n", "    def test_empty_dbinfo(self):\n        self.assertEqual(\n            self._run_it({}), \n            (['psql'], None)\n        )\n", "    def test_default_values(self):\n        self.assertEqual(\n            self._run_it({}),\n            (['psql', 'template1'], None),\n        )\n", "    def test_no_host(self):\n        self.assertEqual(\n            self._run_it({\n                'database': 'dbname',\n                'user': 'someuser',\n                'password': 'somepassword',\n                'port': '444',\n            }), (\n                ['psql', '-U', 'someuser', '-p', '444', 'dbname'],\n                'somepassword',\n            )\n        )\n"], "sample_130": ["    def test_multiple_negated_and(self):\n        query = Query(Author)\n        where = query.build_where(~Q(name__startswith='J') & ~Q(num__gt=2))\n        self.assertTrue(where.negated)\n        self.assertEqual(where.connector, AND)\n", "    def test_empty_iterable_lookup_value(self):\n        query = Query(Item)\n        where = query.build_where(Q(name=[]))\n        name_exact = where.children[0]\n        self.assertIsInstance(name_exact, Exact)\n        self.assertEqual(name_exact.rhs, \"[]\")\n", "    def test_add_ordering(self):\n        query = Query(Author)\n        query.add_ordering('num')\n        self.assertEqual(query.order_by, ('num',))\n        query.add_ordering('-name')\n        self.assertEqual(query.order_by, ('num', '-name'))\n        query.add_ordering('id')\n        self.assertEqual(query.order_by, ('num', '-name', 'id'))\n", "    def test_multi_value_lookup_empty_list(self):\n        query = Query(Item)\n        where = query.build_where(Q(name=[]))\n        self.assertEqual(where.children, [])\n\n", "    def test_multiple_or_conditions(self):\n        query = Query(Item)\n        where = query.build_where(\n            Q(name='item1') | Q(text='something') | Q(modified__gt=datetime(2017, 1, 1))\n        )\n        self.assertEqual(where.connector, OR)\n        self.assertEqual(len(where.children), 3)\n"], "sample_1142": ["compilation error", "compilation error", "def test_MatrixSymbol_determinant_issue_18944():\n    n = Symbol('n', integer=True, positive=True)\n    A = MatrixSymbol('A', n, n)\n    assert A.det().diff(A[0, 0]) == _\n", "compilation error", "compilation error"], "sample_127": ["    def test_ignore_conflicts_with_pk_specified(self):\n        data = [\n            TwoFields(id=1, f1=1, f2=1),\n            TwoFields(id=2, f1=2, f2=2),\n            TwoFields(id=3, f1=3, f2=3),\n        ]\n        TwoFields.objects.bulk_create(data)\n        self.assertEqual(TwoFields.objects.count(), 3)\n        # With ignore_conflicts=True, conflicts are ignored.\n        conflicting_objects = [\n            TwoFields(id=2, f1=2, f2=2),\n            TwoFields(id=3, f1=3, f2=3),\n        ]\n        TwoFields.objects.bulk_create(conflicting_objects, ignore_conflicts=True)\n        self.assertEqual(TwoFields.objects.count(), 3)\n        self.assertEqual(TwoFields.objects.get(id=2).pk, 2)\n        self.assertEqual(TwoFields.objects.get(id=3).pk, 3)\n        # New objects are created and conflicts are ignored.\n\n", "    def test_ignore_conflicts_partial_success(self):\n        data = [TwoFields(f1=1, f2=1), TwoFields(f1=2, f2=2)]\n        TwoFields.objects.bulk_create(data)\n        self.assertEqual(TwoFields.objects.count(), 2)\n        # The first object will be ignored because it already exists, the second\n        # object will be created successfully\n        conflicting_objects = [TwoFields(f1=1, f2=1), TwoFields(f1=3, f2=3)]\n        TwoFields.objects.bulk_create(conflicting_objects, ignore_conflicts=True)\n        self.assertEqual(TwoFields.objects.count(), 3)\n        self.assertIsNone(conflicting_objects[0].pk)\n        self.assertIsNotNone(conflicting_objects[1].pk)\n", "    def test_bulk_create_with_m2m_field(self):\n        restaurant1 = Restaurant.objects.create(name='Restaurant A')\n        restaurant2 = Restaurant.objects.create(name='Restaurant B')\n        countries = [Country(name='Country A'), Country(name='Country B')]\n        Country.objects.bulk_create(countries)\n        restaurant1.countries.set(countries)\n\n        restaurant1.refresh_from_db()\n        self.assertEqual(restaurant1.countries.count(), 2)\n        self.assertQuerysetEqual(\n            restaurant1.countries.all(), countries, attrgetter('name')\n        )\n", "    def test_bulk_create_with_defaults(self):\n        class DefaultModel(models.Model):\n            name = models.CharField(max_length=100)\n            created_at = models.DateTimeField(auto_now_add=True, default=timezone.now)\n\n        DefaultModel.objects.bulk_create([\n            DefaultModel(name='Instance 1'),\n            DefaultModel(name='Instance 2'),\n        ])\n        for instance in DefaultModel.objects.all():\n            self.assertIsNotNone(instance.created_at)\n", "    def test_bulk_create_with_null_boolean_field(self):\n        class TempModel(models.Model):\n            null_bool_field = models.BooleanField(null=True)\n\n        data = [\n            TempModel(null_bool_field=True),\n            TempModel(null_bool_field=False),\n            TempModel(null_bool_field=None),\n        ]\n        TempModel.objects.bulk_create(data)\n        self.assertEqual(TempModel.objects.count(), 3)\n        self.assertEqual(TempModel.objects.filter(null_bool_field=True).count(), 1)\n        self.assertEqual(TempModel.objects.filter(null_bool_field=False).count(), 1)\n        self.assertEqual(TempModel.objects.filter(null_bool_field=None).count(), 1)\n"], "sample_1086": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_692": ["    def test_tmpdir_factory_cleanup_lock_removal(self, tmp_path_factory: TempPathFactory, tmp_path: Path):\n        f = tmp_path / \"lockfile\"\n        f.touch()\n        create_cleanup_lock(f.parent)\n\n            cleanup_numbered_dir(\n                root=tmp_path,\n                prefix=self.PREFIX,\n                keep=2,\n                consider_lock_dead_if_created_before=0,\n            )\n\n        register_cleanup_lock_removal(f, cleanup_lock)\n        cleanup_lock()\n\n        assert not f.exists()\n", "    def test_tmpdir_handles_invalid_dir_characters(tmp_path_factory: TempPathFactory, monkeypatch: MonkeyPatch) -> None:\n        monkeypatch.setattr(\"getpass.getuser\", lambda: \"os/<:*?;>agnostic\")\n        # _basetemp / _given_basetemp are cached / set in parallel runs, patch them\n        monkeypatch.setattr(tmp_path_factory, \"_basetemp\", None)\n        monkeypatch.setattr(tmp_path_factory, \"_given_basetemp\", None)\n        with tmp_path_factory.mktemp(\"invalid/path:with*chars\") as tmpdir:\n            assert tmpdir.exists()\n\n", "    def test_basetemp_with_symlinked_basetemp(pytester: Pytester) -> None:\n        basetemp = pytester.mkdir(\"mybasetemp\")\n        linktemp = pytester.path.joinpath(\"symlinktemp\")\n        attempt_symlink_to(linktemp, str(basetemp))\n        pytester.makepyfile(\n            \"\"\"\n                assert tmpdir.exists()\n        \"\"\"\n        )\n        result = pytester.runpytest(\"--basetemp=%s\" % linktemp)\n        assert result.ret == 0\n", "    def test_tmpdir_factory_create_basetemp_with_empty_string(\n        pytester: Pytester, tmp_path_factory: TempPathFactory", "    def test_tmpdir_with_unicode_characters(pytester: Pytester, tmp_path: Path) -> None:\n        p = pytester.makepyfile(\n            \"\"\"\n                import os\n                tmpdir.joinpath('\u00e4\u00f6\u00fc').mkdir()\n                assert os.path.exists(str(tmpdir.joinpath('\u00e4\u00f6\u00fc')))\n        \"\"\"\n        )\n        result = pytester.runpytest(p)\n        assert result.ret == 0\n"], "sample_492": ["    def test_serialize_datetime_naive(self):\n        self.assertSerializedEqual(datetime.datetime(2023, 10, 26, 10, 0))\n", "    def test_serialize_defaultdict(self):\n        self.assertSerializedEqual(defaultdict(list))\n        self.assertSerializedEqual(defaultdict(lambda: 42))\n        self.assertSerializedResultEqual(\n            defaultdict(int), (\"defaultdict(int)\", set())\n        )\n", "    def test_serialize_pathlib_parts(self):\n        parts = pathlib.PurePosixPath(\"/path/to/file.txt\").parts\n        expected = (\n            \"tuple(pathlib.PurePosixPath('/path/to/file.txt').parts)\",\n            {\"import pathlib\"},\n        )\n\n        self.assertSerializedResultEqual(parts, expected)\n", "    def test_serialize_recursive_types(self):\n        class Node:\n                self.value = value\n                self.children = children or []\n\n        root = Node(1, [Node(2), Node(3, [Node(4)])])\n\n        self.assertSerializedResultEqual(\n            root,\n            (\n                \"migrations.test_writer.Node(1, [migrations.test_writer.Node(2), migrations.test_writer.Node(3, [migrations.test_writer.Node(4)])])\",\n                {\"import migrations.test_writer\"},\n            ),\n        )\n", "    def test_serialize_model_with_custom_manager(self):\n        class MyModel(models.Model):\n            name = models.CharField(max_length=100)\n\n            objects = MyCustomManager()\n\n        self.serialize_round_trip(MyModel)\n"], "sample_1152": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_870": ["    def test_gpr_predict_input_not_modified_return_cov(self):\n        \"\"\"\n        Check that the input X is not modified by the predict method of the\n        GaussianProcessRegressor when setting return_cov=True.\n\n        Non-regression test for:\n        https://github.com/scikit-learn/scikit-learn/issues/24340\n        \"\"\"\n        gpr = GaussianProcessRegressor(kernel=CustomKernel()).fit(X, y)\n\n        X2_copy = np.copy(X2)\n        _, _ = gpr.predict(X2, return_cov=True)\n\n        assert_allclose(X2, X2_copy)\n", "def test_gpr_predict_input_not_modified_return_cov():\n    \"\"\"\n    Check that the input X is not modified by the predict method of the\n    GaussianProcessRegressor when setting return_cov=True.\n\n    Non-regression test for:\n    https://github.com/scikit-learn/scikit-learn/issues/24340\n    \"\"\"\n    gpr = GaussianProcessRegressor(kernel=CustomKernel()).fit(X, y)\n\n    X2_copy = np.copy(X2)\n    _, _ = gpr.predict(X2, return_cov=True)\n\n    assert_allclose(X2, X2_copy)\n\n", "    def test_gpr_predict_input_not_modified_cov():\n        \"\"\"\n        Check that the input X is not modified by the predict method of the\n        GaussianProcessRegressor when setting return_cov=True.\n\n        Non-regression test for:\n        https://github.com/scikit-learn/scikit-learn/issues/24340\n        \"\"\"\n        gpr = GaussianProcessRegressor(kernel=CustomKernel()).fit(X, y)\n\n        X2_copy = np.copy(X2)\n        _, _ = gpr.predict(X2, return_cov=True)\n\n        assert_allclose(X2, X2_copy)\n", "def test_gpr_predict_input_not_modified_return_cov():\n    \"\"\"\n    Check that the input X is not modified by the predict method of the\n    GaussianProcessRegressor when setting return_cov=True.\n\n    Non-regression test for:\n    https://github.com/scikit-learn/scikit-learn/issues/24340\n    \"\"\"\n    gpr = GaussianProcessRegressor(kernel=CustomKernel()).fit(X, y)\n\n    X2_copy = np.copy(X2)\n    _, _ = gpr.predict(X2, return_cov=True)\n\n    assert_allclose(X2, X2_copy)\n", "def test_gpr_kernel_matrix_shape():\n    \"\"\"\n    \"\"\"\n    rng = np.random.RandomState(0)\n    X = rng.randn(10, 3)\n    y = rng.randn(10)\n\n    model = GaussianProcessRegressor(kernel=RBF() + WhiteKernel())\n    model.fit(X, y)\n\n    kernel_matrix = model.kernel_(X, X)\n\n    assert kernel_matrix.shape == (X.shape[0], X.shape[0])\n"], "sample_486": ["    def test_inlineformset_factory_editable_pk(self):\n        FormSet = inlineformset_factory(\n            UUIDPKParent, ChildWithEditablePK, fields=\"__all__\"\n        )\n        formset = FormSet(\n            {\n                \"childwitheditablepk_set-TOTAL_FORMS\": 3,\n                \"childwitheditablepk_set-INITIAL_FORMS\": 0,\n                \"childwitheditablepk_set-MAX_NUM_FORMS\": \"\",\n                \"childwitheditablepk_set-0-name\": \"Foo\",\n                \"childwitheditablepk_set-0-pk\": \"1\",\n                \"childwitheditablepk_set-1-name\": \"\",\n                \"childwitheditablepk_set-2-name\": \"\",\n            }\n        )\n        self.assertTrue(formset.is_valid())\n        self.assertIsNotNone(formset.instance.uuid)\n        self.assertEqual(formset.forms[0].instance.pk, 1)\n\n", "    def test_inlineformset_factory_nulls_default_pks_parent_with_pk_field_name(self):\n        \"\"\"\n        #24958 - Variant of test_inlineformset_factory_nulls_default_pks for\n        the case of a parent object with non-default primary key field name.\n        \"\"\"\n        FormSet = inlineformset_factory(\n            ParentWithUUIDAlternateKey, ChildRelatedViaAK, fields=\"__all__\"\n        )\n        formset = FormSet()\n        self.assertIsNone(formset.forms[0].fields[\"parent\"].initial)\n", "    def test_inlineformset_factory_nulls_default_pks_alternate_key_relation_data_with_initial(self):\n        \"\"\"\n        If initial data is provided, a parent's auto-generated alternate key is\n        set.\n        \"\"\"\n        initial_data = [\n            {\"name\": \"Test\"},\n        ]\n        FormSet = inlineformset_factory(\n            ParentWithUUIDAlternateKey, ChildRelatedViaAK, fields=\"__all__\"\n        )\n        formset = FormSet(initial=initial_data)\n        self.assertTrue(formset.is_valid())\n        self.assertIsNotNone(formset.instance.uuid)\n        self.assertEqual(formset.forms[0].instance.parent_id, formset.instance.uuid)\n", "    def test_inlineformset_factory_nulls_default_pks_with_initial_data(self):\n        \"\"\"\n        #24958 - If initial data is provided for the inline formset, the parent's\n        initial value should be used, even if the parent has a default value.\n        \"\"\"\n        parent = UUIDPKParent.objects.create()\n        FormSet = inlineformset_factory(UUIDPKParent, UUIDPKChild, fields=\"__all__\")\n        formset = FormSet(instance=parent)\n        self.assertEqual(formset.forms[0].fields[\"parent\"].initial, parent.uuid)\n", "    def test_inlineformset_factory_nulls_default_pks_when_editing_existing_instance(self):\n        \"\"\"\n        #24958 - If we're editing an existing inline formset, a parent's\n        auto-generated pk should be set.\n        \"\"\"\n        parent = UUIDPKParent.objects.create(uuid = uuid.uuid4())\n        FormSet = inlineformset_factory(UUIDPKParent, UUIDPKChild, fields=\"__all__\")\n        formset = FormSet(instance=parent)\n        self.assertIsNotNone(formset.forms[0].fields[\"parent\"].initial)\n"], "sample_1163": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1177": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_141": ["    def test_datetime_serialization(self):\n        now = datetime.datetime.now()\n        serialized_data = serializers.json.Serializer().serialize([Score(score=5.0,\n                                                                  created=now)])\n        obj = json.loads(serialized_data)\n        loaded_datetime = datetime.datetime.fromisoformat(obj[0]['fields']['created'])\n        self.assertEqual(loaded_datetime, now)\n", "    def test_deserialize_fk_value_handle_forward_references(self):\n        test_string = \"\"\"[{\n            \"pk\": 1,\n            \"model\": \"serializers.article\",\n            \"fields\": {\n                \"author\": 2,\n                \"headline\": \"Headline with forward reference\",\n                \"pub_date\": \"2006-06-16T15:00:00\",\n                \"categories\": [1]\n            }\n        },\n        {\n            \"pk\": 1,\n            \"model\": \"serializers.category\",\n            \"fields\": {\n                \"name\": \"Reference\"\n            }\n        },\n        {\n            \"pk\": 2,\n            \"model\": \"serializers.author\",\n            \"fields\": {\n                \"name\": \"Bob\"\n            }\n        }]\"\"\"\n        list(serializers.deserialize('json', test_string, handle_forward_references=True))\n\n", "    def test_serialize_empty_queryset(self):\n        data = serializers.json.Serializer().serialize(Score.objects.none(), indent=2)\n        self.assertEqual(data, '[]')\n", "    def test_deserialize_m2m_with_natural_keys_and_duplicates(self):\n        test_string = \"\"\"[{\n            \"pk\": 1,\n            \"model\": \"serializers.article\",\n            \"fields\": {\n                \"author\": 1,\n                \"headline\": \"Duplicate natural keys in many to many\",\n                \"pub_date\": \"2014-09-15T10:35:00\",\n                \"categories\": [\n                    [\"author\", \"meta1\"],\n                    [\"author\", \"meta1\"]\n                ]\n            }\n        }, {\n            \"pk\": 1,\n            \"model\": \"serializers.categorymetadata\",\n            \"fields\": {\n                \"kind\": \"author\",\n                \"name\": \"meta1\",\n                \"value\": \"Agnes\"\n            }\n        }, {\n            \"pk\": 1,\n            \"model\": \"serializers.author\",\n            \"fields\": {\n                \"name\": \"Agnes\"\n            }\n        }]\"\"\"\n        for obj in serializers.deserialize('json', test_string, ignore=False):\n            obj.save()\n        categories = Article.objects.get(pk=1).categories.all()\n        self.assertEqual(len(categories), 1)\n        self.assertEqual(categories[0].name, \"meta1\")\n\n", "    def test_deserialization_with_custom_object_creation_function(self):\n        class CustomArticle(Article):\n            class Meta:\n                proxy = True\n\n            return CustomArticle(**data)\n\n        data = '[{ \"pk\": 1, \"model\": \"serializers.article\", \"fields\": { \"headline\": \"Headline\", \"pub_date\": \"2006-06-16T11:00:00\", \"categories\": [1], \"author\": 1 }}]'\n        deserialized_obj = serializers.deserialize('json', data, using='default', create_function=create_article_instance)\n\n        self.assertEqual(list(deserialized_obj), [CustomArticle(\n            headline='Headline',\n            pub_date=datetime.datetime(2006, 6, 16, 11, 0),\n            categories=[Category.objects.get(pk=1)],\n            author=Author.objects.get(pk=1)\n        )])\n"], "sample_4": ["    def test_readwrite_html_empty_table(self, cosmo_cls, read, write, tmp_path, add_cu):\n        \"\"\"Tests reading and writing an empty table.\"\"\"\n        fp = tmp_path / \"test_readwrite_html_empty_table.html\"\n\n        write(fp, format=\"ascii.html\")\n\n        with pytest.raises(IndexError, match=\"Table is empty\"):\n            read(fp, format=\"ascii.html\")\n\n\n\n", "    def test_html_identify_with_invalid_format(self, readwrite_registry) :\n        \"\"\"Test if html_identify properly identifies invalid files.\n\n        Identify if an object uses the HTML Table format.\n        \"\"\"\n        assert not readwrite_registry.is_format('ascii.html', 'invalid format')\n", "    def test_readwrite_html_with_units(self, cosmo, read, write, tmp_path, add_cu):\n        \"\"\"Test reading and writing a Cosmology with units.\"\"\"\n        fp = tmp_path / \"test_readwrite_html_with_units.html\"\n\n        write(fp, format=\"ascii.html\")\n\n        # Check if units are preserved when reading back\n        got = read(fp, format=\"ascii.html\")\n        for param_name in cosmo.__parameters__:\n            param = getattr(cosmo, param_name)\n            if isinstance(param, Parameter) and param.unit is not None:\n                assert getattr(got, param_name).unit == param.unit\n", "    def test_readwrite_html_table_units(self, cosmo_cls, cosmo, read, write, tmp_path, add_cu):\n        \"\"\"Test cosmology -> ascii.html -> cosmology, preserving units.\"\"\"\n        fp = tmp_path / \"test_readwrite_html_table_units.html\"\n\n         Cosmo = cosmo_cls\n        write(fp, format=\"ascii.html\")\n        \n        tbl = QTable.read(fp)\n\n        # Assert units are preserved in the table\n        for column_name in tbl.colnames:\n            if column_name in cosmo.__parameters__:\n                param = getattr(Cosmo, column_name)\n                if isinstance(param, Parameter) and param.unit is not None:\n                    assert tbl[column_name].unit == param.unit\n\n        # Read back in the cosmology\n        got = read(fp, format=\"ascii.html\")\n        assert got == cosmo\n", "    def test_readwrite_html_no_latex_names(self, cosmo, read, write, tmp_path, add_cu):\n        \"\"\"Test reading and writing html table without latex names\"\"\"\n        fp = tmp_path / \"test_readwrite_html_no_latex_names.html\"\n        write(fp, format=\"ascii.html\", latex_names=False)\n\n        cosmo2 = read(fp, format=\"ascii.html\", latex_names=False)\n        assert cosmo2 == cosmo\n"], "sample_277": ["    def test_deconstruct_empty_q(self):\n        q = Q()\n        path, args, kwargs = q.deconstruct()\n        self.assertEqual(path, 'django.db.models.Q')\n        self.assertEqual(args, ())\n        self.assertEqual(kwargs, {})\n", "    def test_q_object_equality(self):\n        q1 = Q(name='test')\n        q2 = Q(name='test')\n        q3 = Q(name='other')\n        self.assertEqual(q1, q2)\n        self.assertNotEqual(q1, q3)\n\n", "    def test_invert(self):\n        q = Q(price__gt=F('discounted_price'))\n        self.assertEqual(~q, Q(price__lte=F('discounted_price')))\n\n", "    def test_combine_and(self):\n        q1 = Q(name='Django')\n        q2 = Q(name__startswith='D')\n        self.assertEqual(q1 & q2, Q(name='Django', name__startswith='D'))\n", "    def test_deconstruct_empty(self):\n        q = Q()\n        path, args, kwargs = q.deconstruct()\n        self.assertEqual(path, 'django.db.models.Q')\n        self.assertEqual(args, ())\n        self.assertEqual(kwargs, {})\n"], "sample_440": ["    def test_update_conflicts_empty_update_fields(self):\n        UpsertConflict.objects.bulk_create(\n            [\n                UpsertConflict(number=1, rank=1, name=\"John\"),\n                UpsertConflict(number=2, rank=2, name=\"Mary\"),\n                UpsertConflict(number=3, rank=3, name=\"Hannah\"),\n            ]\n        )\n        self.assertEqual(UpsertConflict.objects.count(), 3)\n\n        conflicting_objects = [\n            UpsertConflict(number=1, rank=4, name=\"Steve\"),\n            UpsertConflict(number=2, rank=2, name=\"Olivia\"),\n            UpsertConflict(number=3, rank=1, name=\"Hannah\"),\n        ]\n        with self.assertRaises(ValueError):\n            UpsertConflict.objects.bulk_create(\n                conflicting_objects,\n                update_conflicts=True,\n                update_fields=[],\n                unique_fields=[\"number\"],\n            )\n\n", "    def test_update_conflicts_unique_fields_multi_unique(self):\n        self._test_update_conflicts(unique_fields=[\"number\", \"rank\"])\n", "    def test_update_conflicts_with_defaults(self):\n        UpsertConflict.objects.bulk_create(\n            [\n                UpsertConflict(number=1, rank=1, name=\"John\"),\n                UpsertConflict(number=2, rank=2, name=\"Mary\"),\n                UpsertConflict(number=3, rank=3, name=\"Hannah\"),\n            ]\n        )\n        self.assertEqual(UpsertConflict.objects.count(), 3)\n\n        conflicting_objects = [\n            UpsertConflict(number=1, rank=None, name=\"Steve\"),\n            UpsertConflict(number=2, rank=None, name=\"Olivia\"),\n            UpsertConflict(number=3, rank=None, name=\"Hannah\"),\n        ]\n        UpsertConflict.objects.bulk_create(\n            conflicting_objects,\n            update_conflicts=True,\n            update_fields=[\"name\"],\n            unique_fields=[\"number\"],\n        )\n        self.assertEqual(UpsertConflict.objects.count(), 3)\n        self.assertCountEqual(\n            UpsertConflict.objects.values(\"number\", \"rank\", \"name\"),\n            [\n                {\"number\": 1, \"rank\": 1, \"name\": \"Steve\"},\n                {\"number\": 2, \"rank\": 2, \"name\": \"Olivia\"},\n                {\"number\": 3, \"rank\": 3, \"name\": \"Hannah\"},\n            ],\n        )\n", "    def test_update_conflicts_empty_update_fields(self):\n        UpsertConflict.objects.bulk_create(\n            [\n                UpsertConflict(number=1, rank=1, name=\"John\"),\n                UpsertConflict(number=2, rank=2, name=\"Mary\"),\n            ]\n        )\n        self.assertEqual(UpsertConflict.objects.count(), 2)\n\n        conflicting_objects = [\n            UpsertConflict(number=1, rank=4, name=\"Steve\"),\n            UpsertConflict(number=2, rank=2, name=\"Olivia\"),\n        ]\n        with self.assertRaises(ValueError) as cm:\n            UpsertConflict.objects.bulk_create(\n                conflicting_objects,\n                update_conflicts=True,\n                update_fields=[],\n                unique_fields=[\"number\"],\n            )\n        self.assertEqual(\n            str(cm.exception), \"bulk_create() requires at least one field to update.\"\n        )\n", "    def test_update_conflicts_multiple_unique_fields(self):\n        self._test_update_conflicts(unique_fields=[\"number\", \"rank\"])\n"], "sample_739": ["compilation error", "compilation error", "    def test_inverse_binarize_multiclass_sparse_input():\n        y_pred = csr_matrix([[0, 1, 0],  [0, 0, 1], [1, 0, 0]])\n        classes = np.array([0, 1, 2])\n        expected = np.array([1, 2, 0])\n        got = _inverse_binarize_multiclass(y_pred, classes=classes)\n        assert_array_equal(got, expected)\n", "compilation error", "    def test_label_binarizer_set_label_encoding_empty_input():\n        lb = LabelBinarizer(neg_label=-2, pos_label=0)\n\n        # empty input\n        empty_input = []\n\n        # check fit_transform\n        got = lb.fit_transform(empty_input)\n        assert_array_equal(got, [])\n        assert_array_equal(lb.classes_, [])\n\n        # check transform\n        got = lb.transform(empty_input)\n        assert_array_equal(got, [])\n"], "sample_754": ["    def test_sparse_pca_empty_data():\n        spca = SparsePCA(n_components=2)\n        X = np.array([])\n        with pytest.raises(ValueError):\n            spca.fit(X)\n", "def test_spca_error(spca):\n    rng = np.random.RandomState(0)\n    Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)\n\n    # Test that error attribute is set correctly\n    model = spca(n_components=3, random_state=0)\n    model.fit(Y)\n    assert hasattr(model, 'error_')\n    assert len(model.error_) > 0 \n", "def test_sparse_pca_empty_input(norm_comp):\n    # Test SparsePCA with empty input\n    rng = np.random.RandomState(0)\n    Y = np.array([])\n    spca = SparsePCA(n_components=3, random_state=rng,\n                     normalize_components=norm_comp)\n    with pytest.raises(ValueError):\n        spca.fit(Y)\n", "def test_error_behavior():\n    rng = np.random.RandomState(0)\n    Y = rng.randn(10, 5)\n    spca = SparsePCA(n_components=6, random_state=rng)\n\n    # Test that fit raises ValueError for invalid input shape\n    with pytest.raises(ValueError):\n        spca.fit(Y.T)\n\n\n", "def test_spca_multiple_initializations(spca):\n    rng = np.random.RandomState(0)\n    X = rng.randn(100, 10)\n    model1 = spca(n_components=5, alpha=0.5, random_state=0)\n    model2 = spca(n_components=5, alpha=0.5, random_state=1)\n    model1.fit(X)\n    model2.fit(X)\n    assert not np.allclose(model1.components_, model2.components_)\n"], "sample_768": ["    def test_repeated_kfold_error_raising():\n        with pytest.raises(ValueError, match=\"The number of splits must be greater than 1\"):\n            RepeatedKFold(n_splits=1)\n", "def test_time_series_cv_overlapping():\n    X = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13, 14]]\n\n    tscv = TimeSeriesSplit(n_splits=3)\n\n    splits = tscv.split(X)\n    train, test = next(splits)\n\n    assert_array_equal(train, [0])\n    assert_array_equal(test, [1])\n\n    train, test = next(splits)\n    assert_array_equal(train, [0, 1])\n    assert_array_equal(test, [2])\n", "    def test_kfold_empty_array(self):\n        kf = KFold(n_splits=2)\n        X = np.array([], dtype=int).reshape(0, 3)\n        y = np.array([], dtype=int)\n        with pytest.warns(UserWarning):\n            list(kf.split(X, y))\n", "    def test_build_repr_custom_order():\n        class MockSplitter:\n                self.a = a\n                self.b = b\n                self.c = c\n\n                return _build_repr(self)\n\n        assert_equal(repr(MockSplitter(5, 6)), \"MockSplitter(a=0, b=1, c=5)\")\n", "    def test_repeated_kfold_deterministic():\n        X = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]]\n        y = [1, 1, 0, 0, 1, 0]\n        random_state = 1944695409\n        rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=random_state)\n        \n        # Test that the splits are deterministic across multiple calls\n        for _ in range(3):\n            splits = rkf.split(X, y)\n            train, test = next(splits)\n            assert_array_equal(train, [0, 1, 2])\n            assert_array_equal(test, [3, 4, 5])\n\n            train, test = next(splits)\n            assert_array_equal(train, [3, 4, 5])\n            assert_array_equal(test, [0, 1, 2])\n\n            train, test = next(splits)\n            assert_array_equal(train, [0, 1, 2])\n            assert_array_equal(test, [3, 4, 5])\n\n            train, test = next(splits)\n            assert_array_equal(train, [3, 4, 5])\n            assert_array_equal(test, [0, 1, 2])\n            \n            assert_raises(StopIteration, next, splits)\n"], "sample_939": ["compilation error", "def test_unparse_constant_ellipsis():\n    assert ast.unparse(ast.parse(\"...\").body[0].value) == \"...\"\n", "    def test_unparse_type_comments():\n        source = \"\"\"", "    def test_unparse_type_comments():\n        code = \"\"\"", "    def test_unparse_complex_call():\n        source = \"f(a, b=c, *d, e=f(**g))\"\n        module = ast.parse(source)\n        expected = \"f(a, b=c, *d, e=f(**g))\"\n        assert ast.unparse(module.body[0].value, source) == expected\n"], "sample_145": ["    def test_actions_with_no_has_permission_methods(self):\n            pass\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (edit_action, delete_action)\n\n        self.assertIsValid(BandAdmin, Band)\n", "        def custom_permission_action(modeladmin, request, queryset):\n            pass\n", "    def test_actions_not_callable(self):\n        class NotCallable:\n            pass\n        class BandAdmin(ModelAdmin):\n            actions = (NotCallable,)\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            \"The value of 'actions[0]' must be callable.\",\n            id='admin.E131',\n        )\n\n", "    def test_actions_with_wrong_signature(self):\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action,)\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            \"The action 'action' in BandAdmin must have a signature of \"\n            \"(modeladmin, request, queryset).\",\n            id='admin.E131',\n        )\n\n", "    def test_actions_with_custom_permission_and_non_matching_has_permission_method(self):\n            pass\n\n        custom_permission_action.allowed_permissions = ('custom',)\n\n        class BandAdmin(ModelAdmin):\n            actions = (custom_permission_action,)\n\n                return False\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            'BandAdmin.has_custom_permission() returned False, but '\n            'custom_permission_action is defined in the actions list.',\n            id='admin.E131',\n        )\n"], "sample_1198": ["compilation error", "compilation error", "def test_parser_mathematica_function_with_no_arguments():\n    parser = MathematicaParser()\n    assert parser.parse(\"f[]\") == Function(f)() \n", "compilation error", "compilation error"], "sample_1043": ["    def test_user_defined_functions():\n        from sympy import Function\n        x = symbols('x')\n        f = Function('f')\n        g = Function('g')\n        assert mcode(f(x)) == \"f[x]\"\n        assert mcode(g(x)) == \"g[x]\"\n        ufunctions = {'f': [(lambda x: True, 'myF'),\n                          \n\n                                (lambda x: True, 'myF2')]}\n\n        printer = MCodePrinter(user_functions=ufunctions)\n        assert printer.doprint(f(x)) == \"myF[x]\"\n        assert printer.doprint(f(x) * g(x)) == \"myF[x]*g[x]\"\n", "    def test_Piecewise():\n        from sympy import Piecewise\n        p = Piecewise((x, x<1), (x**2, True))\n        assert mcode(p) == \\\n            \"Piecewise[{{x, x < 1}, {x^2, True}}]\"\n", "def test_user_functions():\n    from sympy import mathematica_code as mcode\n    f = Function('f')\n    g = Function('g')\n\n    assert mcode(f(x), user_functions={'f': ['Sin']}) == \"Sin[x]\"\n    assert mcode(f(x) + g(x),\n                 user_functions={'f': ['Sin'], 'g': ['Cos']}) == \"Sin[x] + Cos[x]\"\n", "    def test_Piecewise():\n        from sympy.functions.piecewise import Piecewise\n        pw = Piecewise((x, x < 1), (x**2, True))\n        assert mcode(pw) == \"Piecewise[{{x, x < 1}, {x^2, True}}]\"\n", "def test_user_defined_functions():\n    from sympy import Function, mathematica_code as mcode\n    f = Function('f')\n    g = Function('g')\n\n    assert mcode(f(x) + g(x)) == 'f[x] + g[x]'\n\n    # Test custom mappings\n    printer = MCodePrinter({f: ('MyFunc', lambda x: True), g: ('OtherFunc', lambda x: True)})\n    assert printer.doprint(f(x) + g(x)) == 'MyFunc[x] + OtherFunc[x]'\n\n"], "sample_596": ["    def test_concat_multiindex_not_aligned():\n        x = pd.MultiIndex.from_product([[1, 2], ['a', 'b']])\n        ds1 = Dataset({'a': ('x', np.arange(4))}, coords={'x': x})\n        ds2 = Dataset({'a': ('x', np.arange(4, 8))}, coords={'x': x[:2]})\n\n        with pytest.raises(ValueError):\n            concat([ds1, ds2], 'x')\n\n", "    def test_concat_dask_array_preserve_chunksize():\n        arr1 = DataArray(\n            da.array(np.arange(10).reshape(5, 2), chunks=(1, 2)), dims=[\"x\", \"y\"]\n        )\n        arr2 = DataArray(\n            da.array(np.arange(10, 20).reshape(5, 2), chunks=(1, 2)), dims=[\"x\", \"y\"]\n        )\n        expected = DataArray(\n            da.array(np.arange(20).reshape(10, 2), chunks=(2, 2)), dims=[\"x\", \"y\"]\n        )\n\n        actual = concat([arr1, arr2], dim=\"x\")\n\n        assert_identical(actual.data, expected.data)\n        assert actual.chunks == expected.chunks\n", "    def test_concat_merge_single_non_dim_coord_different_shapes():\n        da1 = DataArray([1, 2, 3], dims=\"x\", coords={\"x\": [1, 2, 3], \"y\": 1})\n        da2 = DataArray([4, 5], dims=\"x\", coords={\"x\": [4, 5], \"z\": 2})\n\n        with raises_regex(ValueError, \"cannot be concatenated\"):\n            concat([da1, da2], \"x\")\n\n", "    def test_concat_preserve_coordinate_order_with_missing_coords():\n        x = np.arange(0, 5)\n        y = np.arange(0, 10)\n        time = np.arange(0, 4)\n        data = np.zeros((4, 10, 5), dtype=bool)\n\n        ds1 = Dataset(\n            {\"data\": ([\"time\", \"y\", \"x\"], data[0:2])},\n            coords={\"time\": time[0:2], \"y\": y},\n        )\n        ds2 = Dataset(\n            {\"data\": ([\"time\", \"y\", \"x\"], data[2:4])},\n            coords={\"time\": time[2:4], \"x\": x},\n        )\n\n        expected = Dataset(\n            {\"data\": ([\"time\", \"y\", \"x\"], data)},\n            coords={\"time\": time, \"y\": y, \"x\": x},\n        )\n\n        actual = concat([ds1, ds2], dim=\"time\", compat=\"override\")\n\n", "    def test_concat_merge_single_non_dim_coord_with_missing_values():\n        da1 = DataArray([1, 2, 3], dims=\"x\", coords={\"x\": [1, 2, 3], \"y\": 1})\n        da2 = DataArray([4, 5, 6], dims=\"x\", coords={\"x\": [4, 5, 6], \"y\": np.nan})\n\n        expected = DataArray(\n            range(1, 7), dims=\"x\", coords={\"x\": range(1, 7), \"y\": [1, np.nan]}\n        )\n\n        for coords in [\"different\", \"minimal\"]:\n            actual = concat([da1, da2], \"x\", coords=coords)\n            assert_identical(actual, expected)\n\n        with raises_regex(ValueError, \"'y' is not present in all datasets.\"):\n            concat([da1, da2], dim=\"x\", coords=\"all\")\n"], "sample_494": ["    def test_serialize_complex_nested_objects(self):\n        class NestedClass:\n                self.a = a\n                self.b = b\n\n        value = [NestedClass(1, \"a\"), NestedClass(2, \"b\")]\n        self.assertSerializedResultEqual(\n            value,\n            (\n                \"[\"\n                \"migrations.test_writer.NestedClass(1, 'a'), \"\n                \"migrations.test_writer.NestedClass(2, 'b')\"\n                \"]\",\n                {\"import migrations.test_writer\"},\n            ),\n        )\n\n", "    def test_serialize_custom_field_with_arguments(self):\n        class CustomField(models.Field):\n                super().__init__(*args, **kwargs)\n                self.arg1 = arg1\n                self.arg2 = arg2\n        \n        field = CustomField(arg1=\"value1\", arg2=3)\n        string, imports = MigrationWriter.serialize(field)\n        \n        self.assertEqual(string,\n                         \"migrations.test_writer.CustomField(arg1='value1', arg2=3)\")\n        self.assertEqual(imports, {\"from migrations.test_writer import CustomField\"})\n", "    def test_serialize_proxy_model(self):\n        class ProxyModel(models.Model):\n            class Meta:\n                proxy = True\n\n        self.assertSerializedEqual(ProxyModel)\n", "    def test_serialize_proxy_models(self):\n        class MyModel(models.Model):\n            pass\n\n        class ProxyModel(MyModel):\n            class Meta:\n                proxy = True\n\n        self.assertSerializedEqual(ProxyModel)\n\n", "    def test_serialize_empty_tuple_as_default(self):\n        field = models.CharField(default=())\n        string = MigrationWriter.serialize(field)[0]\n        self.assertEqual(string, \"models.CharField(default=())\")\n"], "sample_1005": ["def test_issue_14704():\n    from sympy.physics.quantum.braket import Bra, Ket\n    assert latex(Bra('a')*Ket('b')) == r'\\left\\langle a \\right| b \\right\\rangle'\n    assert latex(Bra('a')*Ket('a')) == r'\\left\\langle a \\right| a \\right\\rangle'\n", "def test_issue_14328():\n    x = symbols('x')\n    assert latex(Eq(x, 1)) == r'x = 1'\n    assert latex(Eq(1, x)) == r'1 = x'\n", "def test_issue_15458():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert latex(A + B) == r\"A + B\"\n    assert latex(A + 2*B) == r\"A + 2 B\"\n    assert latex(2*A + B) == r\"2 A + B\"\n", "compilation error", "compilation error"], "sample_278": ["    def test_empty_output_field_expression(self):\n        expr = ExpressionWrapper(Value(3))\n        self.assertIsNone(expr.output_field)\n", "    def test_expressionwrapper_as_sql(self):\n        value = Value(3)\n        expr = ExpressionWrapper(value, output_field=IntegerField())\n        compiler = SQLCompiler(None, connection)\n        self.assertEqual(expr.as_sql(compiler, connection), ('%s', [3]))\n", "    def test_expressionwrapper_deconstruct(self):\n        value = Value('name')\n        wrapped = ExpressionWrapper(value, output_field=CharField())\n        path, args, kwargs = wrapped.deconstruct()\n        self.assertEqual(path, 'django.db.models.expressions.ExpressionWrapper')\n        self.assertEqual(args, (value,))\n        self.assertEqual(kwargs['output_field'].deconstruct(), CharField().deconstruct())\n", "    def test_expressionwrapper_as_sql_with_alias(self):\n        value = Value('f')\n        value.output_field = None\n        expr = ExpressionWrapper(Lower(value), output_field=IntegerField())\n        with connection.cursor() as cursor:\n            sql, params = expr.as_sql(\n                compiler=connection.compiler, connection=connection,\n                \n            )\n            cursor.execute(sql, params)\n", "    def test_expressionwrapper_as_sql_output_field(self):\n        expr = ExpressionWrapper(\n            Value('foo'), output_field=CharField(),\n        )\n        compiler = Time.objects.all().query.get_compiler(connection=connection)\n        with CaptureWarnings(record=True) as captured_warnings:\n            sql, params = expr.as_sql(compiler, connection)\n        self.assertEqual(len(captured_warnings), 0)\n        self.assertEqual(sql, '%s')\n        self.assertEqual(params, ('foo',))\n"], "sample_269": ["    def test_js_catalog_fallback_language_availability(self):\n        \"\"\"\n        Test that if a language is not available, it gracefully falls back\n        to the default language for the javascript catalog.\n\n        This tests the fallback behavior when the requested language is\n        not available, but the default language is.\n        \"\"\"\n        with self.settings(LANGUAGE_CODE='en-us'):\n            self.selenium.get(self.live_server_url + '/jsi18n/')\n            elem = self.selenium.find_element_by_id('catalog-test')\n            self.assertEqual(elem.text, 'This text should be translated to English.')\n", "    def test_missing_return_in_gettext(self):\n        self.selenium.get(self.live_server_url + '/jsi18n_missing_return/')\n\n        elem = self.selenium.find_element_by_id('missing_return')\n        self.assertEqual(elem.text, 'This string should be translated')\n\n", "    def test_jsi18n_with_context(self):\n        self.selenium.get(self.live_server_url + '/jsi18n_context/')\n        elem = self.selenium.find_element_by_id('context_translated_text')\n        self.assertEqual(elem.text, 'Translated text with context.')\n", "    def test_jsi18n_context(self):\n        with override('fr'):\n            response = self.client.get('/jsi18n_context/')\n            self.assertContains(response, '\"month name\\x04May\": \"mai\"', 1)\n", "    def test_js_catalog_non_bmp_character_fallback(self):\n        with override('fr'):\n            self.selenium.get(self.live_server_url + '/jsi18n/app5/')\n            self.assertNotIn('\\U0001f609', self.selenium.page_source)\n"], "sample_229": ["    def test_union_with_empty_values(self):\n        qs1 = Number.objects.filter(num=1).values('num')\n        qs2 = Number.objects.none().values()\n        self.assertCountEqual(qs1.union(qs2), [(1,)])\n", "    def test_union_with_different_select_related_models(self):\n        class AnotherModel(models.Model):\n            related_number = models.ForeignKey(Number, on_delete=models.CASCADE)\n            name = models.CharField(max_length=10)\n\n        AnotherModel.objects.create(related_number=Number.objects.get(num=5), name='Test')\n\n        qs1 = Number.objects.filter(num=5).select_related('anothermodel_set')\n        qs2 = Number.objects.filter(num=6).select_related('anothermodel_set')\n\n        self.assertEqual(qs1.union(qs2).count(), 2)\n", "    def test_union_with_complex_annotations(self):\n        qs1 = Number.objects.annotate(\n            doubled_num=F('num') * 2,\n            is_even=F('num') % 2 == 0,\n        ).filter(doubled_num__gt=10)\n        qs2 = Number.objects.annotate(\n            half_num=F('num') / 2,\n            is_odd=F('num') % 2 != 0,\n        ).filter(half_num__lt=5)\n        self.assertCountEqual(\n            qs1.union(qs2).values_list('num', 'doubled_num', 'is_even', 'half_num', 'is_odd'),\n            [(1, 2, False, 0.5, True), (2, 4, True, 1.0, False),\n             (3, 6, False, 1.5, True), (4, 8, True, 2.0, False),\n             (5, 10, False, 2.5, True), (6, 12, True, 3.0, False),\n             (7, 14, False, 3.5, True), (8, 16, True, 4.0, False),\n             (9, 18, False, 4.5, True)],\n        )\n", "    def test_select_related_after_union(self):\n        ReservedName.objects.create(name='rn1', order=1)\n        Number.objects.create(num=1)\n        \n        qs1 = Number.objects.filter(pk=1).select_related('reservedname')\n        qs2 = Number.objects.none()\n        self.assertEqual(\n            list(qs1.union(qs2).values_list('num', 'reservedname__name')),\n            [(1, 'rn1')]\n        )\n", "    def test_complex_ordering_with_combined_qs(self):\n        qs1 = Number.objects.filter(num__lt=5).order_by('num')\n        qs2 = Number.objects.filter(num__gt=5).order_by('-num')\n        self.assertNumbersEqual(qs1.union(qs2).order_by('num'), list(range(10)))\n"], "sample_788": ["def test_fit_transform_with_missing_values(encode):\n    X = [[1, 2], [2, np.nan], [np.nan, 3], [3, 4]]\n    est = KBinsDiscretizer(n_bins=2, encode=encode).fit(X)\n    Xt = est.transform(X)\n    assert not np.isnan(Xt).any() \n", "    def test_transform_empty_array():\n        X = np.array([])\n        X = X.reshape(0, 4)\n        est = KBinsDiscretizer(n_bins=3, encode='ordinal')\n        assert_array_equal(est.fit_transform(X), np.array([]).reshape(0, 4))\n", "def test_fit_transform_empty_array():\n    X = np.array([])\n    est = KBinsDiscretizer(n_bins=2)\n    with pytest.raises(ValueError) as excinfo:\n        est.fit(X)\n    assert \"Input features must have at least one data point\" in str(excinfo.value)\n", "def test_empty_input(encode):\n    kbd = KBinsDiscretizer(encode=encode)\n    X = np.array([]).reshape(0, 4)\n    assert_array_equal(kbd.fit_transform(X), np.array([]).reshape(0, 4))\n", "def test_invalid_encode_option_onehot():\n    est = KBinsDiscretizer(n_bins=[2, 3, 3, 3], encode='invalid-encode')\n    assert_raise_message(ValueError, \"Valid options for 'encode' are \"\n                         \"('onehot', 'onehot-dense', 'ordinal'). \"\n                         \"Got encode='invalid-encode' instead.\",\n                         est.fit, X)\n"], "sample_476": ["    def test_no_image_file_provided(self):\n        \"\"\"\n        Test what happens when no image file is provided for an ImageField\n        \"\"\"\n        p = self.PersonModel(name=\"Jane\")\n\n        # Saving the model should not raise an error\n        p.save()\n\n        # Verify dimensions are None\n        self.check_dimensions(p, None, None) \n", "    def test_saving_empty_image(self):\n        p = self.PersonModel(name=\"Joe\")\n        p.mugshot.save(\"mug\", File(io.BytesIO(), name=\"empty.png\"))\n        self.check_dimensions(p, None, None)\n", "    def test_dimensions_when_setting_file_to_none(self):\n        p = self.PersonModel(name=\"Joe\", mugshot=self.file1, headshot=self.file2)\n\n        p.mugshot = None\n        self.check_dimensions(p, None, None, \"mugshot\")\n        self.check_dimensions(p, 8, 4, \"headshot\")\n\n        p.headshot = None\n        self.check_dimensions(p, None, None, \"mugshot\")\n        self.check_dimensions(p, None, None, \"headshot\")\n", "    def test_clear_image(self):\n        \"\"\"\n        Test clearing an ImageField by assigning it None.\n        \"\"\"\n        p = self.PersonModel(name=\"Joe\", mugshot=self.file1)\n        self.check_dimensions(p, 4, 8)\n\n        # Clear the image field by assigning None.\n        p.mugshot = None\n\n        # Dimensions should be cleared.\n        self.check_dimensions(p, None, None)\n", "    def test_upload_to_callable(self):\n        class CustomUploadTo:\n                self.instance = instance\n                self.filename = filename\n\n                return os.path.join(\n                    \"custom\", f\"{self.instance.name}_{self.filename}\"\n                )\n\n        self.PersonModel.mugshot.upload_to = CustomUploadTo\n        p = self.PersonModel(name=\"Joan\")\n        p.mugshot.save(\"shot\", self.file1)\n        self.assertEqual(\n            p.mugshot.name,\n            os.path.join(\"custom\", \"Joan_shot\"),\n        )\n"], "sample_79": ["    def test_non_numeric_input(self):\n        self.check_values(('abc', 'ies'), ('', 'ies'), ('1abc', 'ies')) \n", "    def test_empty_string(self):\n        self.check_values(('', 'y'), ('1', 'candy'), ('2', 'candies'))\n", "    def test_non_numeric_values(self):\n        self.check_values(('hello', 'ies'), ('1', 'candy'), ('2', 'candies'))\n\n", "    def test_reverse_order_suffix(self):\n        self.check_values(('0', 'candies'), ('1', 'candy'), ('2', 'candies'))\n", "    def test_single_suffix(self):\n        self.check_values(('0', 'candies'), ('1', 'candy'), ('2', 'candies'))\n\n"], "sample_1200": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_issue_24528():\n    from sympy.physics.units import meter, kilogram, second, newton, joule\n    from sympy.physics.units.systems.si import dimsys_SI\n    F = Quantity('F')\n    SI.set_quantity_dimension(F, newton)\n    F.set_global_relative_scale_factor(1, newton)\n\n    expr = F * meter / second**2\n    assert dimsys_SI.get_dimensional_dependencies(expr) == {length: 1, mass: 1, time: -2}\n    assert convert_to(expr, joule) == 1\n"], "sample_150": ["    def test_postgresql_extension_check(self):\n        with mock.patch('django.db.backends.postgresql.base.Database.cursor', create=True) as mocked_cursor:\n            mocked_cursor.return_value.fetchone.side_effect = [('plpgsql',), (None,)]\n            result = check_database_backends(databases=self.databases)\n            self.assertEqual(len(result), 0)\n\n        with mock.patch('django.db.backends.postgresql.base.Database.cursor', create=True) as mocked_cursor:\n            mocked_cursor.return_value.fetchone.side_effect = [None, ('plpgsql',)]\n            result = check_database_backends(databases=self.databases)\n            self.assertEqual(len(result), 1)\n            self.assertEqual(result[0].id, 'postgresql.W001')\n", "    def test_postgresql_extension_check(self):\n        with mock.patch('django.db.backends.postgresql.creation.DatabaseCreation.execute_sql') as mocked_execute_sql:\n            mocked_execute_sql.return_value = None\n            result = check_database_backends(databases=self.databases)\n            self.assertEqual(len(result), 2)\n            self.assertEqual([r.id for r in result], ['postgresql.W001', 'postgresql.W001'])\n", "    def test_check_migrations(self, mock_check):\n        from django.core.management.base import BaseCommand\n        command = BaseCommand()\n        command.check_migrations()\n        mock_check.assert_called_once()\n\n", "    def test_postgresql_effective_cache_time(self):\n        with mock.patch('django.db.backends.postgresql.base.Database.get_effective_cache_time', return_value=0):\n            result = check_database_backends(databases=self.databases)\n            self.assertEqual(len(result), 2)\n            self.assertEqual([r.id for r in result], ['postgresql.W001', 'postgresql.W001'])\n", "    def test_system_check_output(self, mocked_write):\n        from django.core.management.base import CommandError\n        class MockCommand(BaseCommand):\n                self.check()\n                raise CommandError(\"Test exception\")\n\n        cmd = MockCommand()\n        cmd.stderr = mock.MagicMock()\n        cmd.stdout = mock.MagicMock()\n        with mock.patch('django.core.checks.run_checks', return_value=[mock.MagicMock(level=1, msg='Test message')] ):\n            with self.assertRaises(CommandError):\n                cmd.run_from_argv(['manage.py', 'test'])\n        \n        expected_call_args = [mock.call(mock.ANY, mock.ANY)]\n        mocked_write.assert_has_calls(expected_call_args)\n"], "sample_1081": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_926": ["def test_build_domain_cpp_object_macro(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"object-macro\")\n    assert len(ws) == 0\n\n", "    def check(target):\n        class Config:\n            cpp_id_attributes = [\"id_attr\"]\n            cpp_paren_attributes = [\"paren_attr\"]\n        parser = DefinitionParser(target, location=None,\n                                  config=Config())\n        ast, isShorthand = parser.parse_xref_object()\n        parser.assert_end()", "def test_cpp_enum_values(app, status, warning):\n    app.builder.build_all()\n\n    text = \"\"\"\n    .. cpp:enum:: Colors\n\n       .. cpp:enumerator:: RED\n       .. cpp:enumerator:: GREEN\n       .. cpp:enumerator:: BLUE\n\n    \"\"\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.desc, addnodes.desc, addnodes.desc))\n\n    assert doctree[0].astext() == 'Colors'\n    assert doctree[1].astext() == 'RED'\n    assert doctree[2].astext() == 'GREEN'\n    assert doctree[3].astext() == 'BLUE'\n\n", "def test_build_domain_cpp_warn_template_param_qualified_name(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"warn-template-param-qualified-name\")\n\n    assert len(ws) == 2\n    assert \"WARNING: cpp:type reference target not found: T::typeWarn\" in ws[0]\n    assert \"WARNING: cpp:type reference target not found: T::U::typeWarn\" in ws[1]\n", "    def check(target):\n        class Config:\n            cpp_id_attributes = [\"id_attr\"]\n            cpp_paren_attributes = [\"paren_attr\"]\n        parser = DefinitionParser(target, location=None,\n                                  config=Config())\n        ast, isShorthand = parser.parse_xref_object()\n        parser.assert_end()"], "sample_578": ["    def test_baseline(self, x, y):\n\n        p = Plot(x, y).add(Bars(baseline=2)).plot()\n        ax = p._figure.axes[0]\n        paths = ax.collections[0].get_paths()\n        assert len(paths) == len(x)\n        for i, path in enumerate(paths):\n            verts = path.vertices\n            assert verts[0, 1] == pytest.approx(2)\n\n", "    def test_baseline(self, x, y):\n\n        baseline = 2\n        p = Plot(x, y).add(Bars(baseline=baseline)).plot()\n        ax = p._figure.axes[0]\n        paths = ax.collections[0].get_paths()\n        for i, path in enumerate(paths):\n            verts = path.vertices\n            assert verts[0, 1] == pytest.approx(baseline)\n            assert verts[3, 1] == pytest.approx(y[i] + baseline)\n", "    def test_baseline(self, x, y):\n\n        baseline = 2\n        p = Plot(x, y).add(Bars(baseline=baseline)).plot()\n        ax = p._figure.axes[0]\n        paths = ax.collections[0].get_paths()\n        for i, path in enumerate(paths):\n            verts = path.vertices\n            assert verts[0, 1] == baseline\n            assert verts[3, 1] == y[i] + baseline\n\n\n", "    def test_edgewidth_default(self, x, y):\n\n        p = Plot(x, y).add(Bars()).plot()\n        ax = p._figure.axes[0]\n        lws = ax.collections[0].get_linewidths()\n        assert_array_equal(lws,\n                           np.full(len(x), mpl.rcParams[\"patch.linewidth\"]))\n", "    def test_baseline(self, x, y):\n\n        baseline = 2\n        p = Plot(x, y).add(Bars(baseline=baseline)).plot()\n        ax = p._figure.axes[0]\n        paths = ax.collections[0].get_paths()\n        for i, path in enumerate(paths):\n            verts = path.vertices\n            assert verts[0, 1] == pytest.approx(baseline)\n            assert verts[3, 1] == pytest.approx(baseline + y[i])\n"], "sample_182": ["    def test_union_with_values_and_extra(self):\n        qs1 = Number.objects.filter(num=1).values('num').extra(select={'count': 0})\n        qs2 = Number.objects.filter(num=2).extra(select={'count': 1})\n        self.assertCountEqual(qs1.union(qs2), [(1, 0), (2, 1)])\n\n", "    def test_difference_with_different_fields(self):\n        ReservedName.objects.create(name='a', order=2)\n        qs1 = Number.objects.values('num')\n        qs2 = ReservedName.objects.values('order')\n        with self.assertRaisesMessage(TypeError, 'Cannot perform difference between QuerySets with different fields.'):\n            list(qs1.difference(qs2))\n", "    def test_union_different_model_fields(self):\n        ReservedName.objects.create(name='a', order=1)\n        qs1 = Number.objects.filter(num=1).values('num')\n        qs2 = ReservedName.objects.filter(order=1).values('order')\n        self.assertEqual(list(qs1.union(qs2)), [1, 1])\n", "    def test_select_related_on_combined_qs(self):\n        ReservedName.objects.create(name='a', order=2)\n        qs1 = Number.objects.filter(pk=1).select_related('reservedname_set')\n        qs2 = Number.objects.filter(pk=2).select_related('reservedname_set')\n\n        combined_qs = qs1.union(qs2)\n        self.assertEqual(len(combined_qs), 2)\n\n        for obj in combined_qs:\n            self.assertIsNotNone(obj.reservedname_set)\n", "    def test_intersection_with_complex_filters(self):\n        ReservedName.objects.create(name='a', order=2)\n        ReservedName.objects.create(name='b', order=1)\n        qs1 = ReservedName.objects.filter(name__in=['a', 'b'], order=1)\n        qs2 = ReservedName.objects.filter(order=1)\n        self.assertQuerysetEqual(qs1.intersection(qs2), [\n            ReservedName.objects.get(name='b', order=1),\n        ])\n"], "sample_848": ["def test_multioutput_predict_proba_shape():\n    X, Y = datasets.load_iris(return_X_y=True)\n    multi_task_classifier = MultiOutputClassifier(LogisticRegression())\n    multi_task_classifier.fit(X, Y)\n    y_prob = multi_task_classifier.predict_proba(X)\n    assert y_prob.shape == (X.shape[0], Y.shape[1], \n                           multi_task_classifier.estimators_[0].classes_.size)  \n", "def test_base_chain_fit_and_predict_with_missing_values():\n    # Fit chain with data containing missing values and verify predict performance\n    X, Y = generate_multilabel_dataset_with_correlations()\n    X[::5, ::3] = np.nan\n    for chain in [ClassifierChain(LogisticRegression(max_iter=1000), imputation='mean'),\n                  RegressorChain(Ridge(), imputation='median')]:\n        chain.fit(X, Y)\n        Y_pred = chain.predict(X)\n        assert Y_pred.shape == Y.shape\n", "def test_classifier_chain_order_fixed():\n    # Test fixed order in ClassifierChain\n\n    X, Y = generate_multilabel_dataset_with_correlations()\n    order = [2, 0, 1, 3]\n    chain = ClassifierChain(LogisticRegression(), order=order)\n    chain.fit(X, Y)\n\n    assert chain.order_ == order\n    assert len(chain.estimators_) == Y.shape[1]\n    assert [c.coef_.size for c in chain.estimators_] == list(range(X.shape[1], X.shape[1] + Y.shape[1]))\n", "    def test_multi_output_classifier_chain_sparse_input_shape_mismatch():\n        # Test that ClassifierChain raises ValueError when input shape for\n        # predict or predict_proba does not match training data shape\n        X, Y = generate_multilabel_dataset_with_correlations()\n        chain = ClassifierChain(LinearSVC())\n        chain.fit(X, Y)\n        X_mismatch = sp.csr_matrix(X[:, :5])\n\n        with pytest.raises(ValueError):\n            chain.predict(X_mismatch)\n\n        with pytest.raises(ValueError):\n            chain.predict_proba(X_mismatch)\n\n", "def test_multioutput_regressor_sparse_sample_weights():\n    # weighted regressor with sparse data\n    Xw = [[1, 2, 3], [4, 5, 6]]\n    yw = [[3.141, 2.718], [2.718, 3.141]]\n    w = np.asarray([2., 1.])\n    mor = MultiOutputRegressor(GradientBoostingRegressor(random_state=0))\n    Xw_sparse = sp.csr_matrix(Xw)\n    mor.fit(Xw_sparse, yw, w)\n\n    # unweighted, but with repeated samples\n    X = [[1, 2, 3], [1, 2, 3], [4, 5, 6]]\n    y = [[3.141, 2.718], [3.141, 2.718], [2.718, 3.141]]\n    mor = MultiOutputRegressor(GradientBoostingRegressor(random_state=0))\n    mor.fit(X, y)\n\n    X_test = [[1.5, 2.5, 3.5], [3.5, 4.5, 5.5]]\n    assert_almost_equal(mor.predict(X_test), mor.predict(X_test))\n"], "sample_1090": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_313": ["    def test_get_template_directories_empty_dirs(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            set()\n        )\n", "    def test_watch_for_template_changes_no_dirs(self, mock_get_dirs):\n        mock_get_dirs.return_value = set()\n        mock_reloader = mock.MagicMock()\n        autoreload.watch_for_template_changes(mock_reloader)\n        self.assertEqual(mock_reloader.watch_dir.call_count, 0)\n", "    def test_no_template_dirs(self):\n        self.assertSetEqual(autoreload.get_template_directories(), set())\n\n", "    def test_get_template_directories_no_dirs(self):\n        self.assertSetEqual(autoreload.get_template_directories(), set())\n", "    def test_get_template_directories_no_dirs(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            set()\n        )\n"], "sample_212": ["    def test_session_middleware_save_session_data(self):\n        request = HttpRequest()\n        response = HttpResponse()\n        request.session = self.create_test_session({\n            'test_key': 'test_value'\n        })\n\n        middleware = SessionMiddleware()\n        middleware.process_response(request, response)\n\n        self.assertIn(settings.SESSION_COOKIE_NAME, response.cookies)\n        self.assertEqual(response.cookies[settings.SESSION_COOKIE_NAME].value, request.session.session_key)\n", "    def test_session_middleware_empty_session(self):\n        request = HttpRequest()\n        response = HttpResponse()\n\n        # Set an empty session\n        request.session = self.SessionMiddleware().__call__(request).session\n        request.session.save()\n\n        # Test that the session cookie is deleted\n        middleware = self.SessionMiddleware(lambda r: response)\n        response = middleware.process_response(request, response)\n        self.assertFalse(settings.SESSION_COOKIE_NAME in response.cookies)\n", "    def test_session_save_on_empty_session_and_empty_cookie(self):\n        request = HttpRequest()\n        response = HttpResponse()\n        request.COOKIES = {}\n        request.session = self.SessionMiddleware().SessionStore(None)\n\n        # Empty session, no cookie\n        self.SessionMiddleware().process_response(request, response)\n        self.assertFalse(settings.SESSION_COOKIE_NAME in response.cookies)\n", "    def test_session_middleware_empty_session_cookie(self):\n        request = HttpRequest()\n        response = HttpResponse()\n        request.COOKIES = {settings.SESSION_COOKIE_NAME: 'some-session-key'}\n        SessionMiddleware(lambda request: response).process_request(request)\n        request.session.flush()\n        response = SessionMiddleware(lambda request: response).process_response(request, response)\n        self.assertIn('Set-Cookie', response['Set-Cookie'])\n        self.assertEqual(response['Set-Cookie'], f'{settings.SESSION_COOKIE_NAME}=; Expires=Thu, 01 Jan 1970 00:00:00 GMT; Path={settings.SESSION_COOKIE_PATH}; Domain={settings.SESSION_COOKIE_DOMAIN}; SameSite={settings.SESSION_COOKIE_SAMESITE}')\n\n\n", "    def test_session_middleware_empty_session(self):\n        request = HttpRequest()\n        request.COOKIES = {settings.SESSION_COOKIE_NAME: 'empty'}\n        request.session = self.SessionMiddleware.SessionStore('')\n        response = HttpResponse()\n        response = self.SessionMiddleware.process_response(request, response)\n        self.assertFalse(settings.SESSION_COOKIE_NAME in response.cookies)\n"], "sample_467": ["    def test_widget_attrs(self):\n        widget = SelectDateWidget(attrs={'class': 'my-date-picker'})\n        self.assertHTMLEqual(\n            widget.render('mydate', '2010-04-15'),\n            \"\"\"\n            <select name=\"mydate_month\" id=\"id_mydate_month\" class=\"my-date-picker\">\n                <option value=\"\">---</option>\n                <option value=\"1\">January</option>\n                <option value=\"2\">February</option>\n                <option value=\"3\">March</option>\n                <option value=\"4\" selected>April</option>\n                <option value=\"5\">May</option>\n                <option value=\"6\">June</option>\n                <option value=\"7\">July</option>\n                <option value=\"8\">August</option>\n                <option value=\"9\">September</option>\n                <option value=\"10\">October</option>\n                <option value=\"11\">November</option>\n                <option value=\"12\">December</option>\n            </select>\n\n            <select name=\"mydate_day\" id=\"id_mydate_day\" class=\"my-date-picker\">\n                <option value=\"\">---</option>\n                <option value=\"1\">1</option>\n                <option value=\"2\">2</option>\n                <option value=\"3\">3</option>\n                <option value=\"4\">4</option>\n                <option value=\"5\">5</option>\n                <option value=\"6\">6</option>\n                <option value=\"7\">7</option>\n                <option value=\"8\">8</option>\n                <option value=\"9\">9</option>\n                <option value=\"10\">10</option>\n                <option value=\"11\">11</option>\n                <option value=\"12\">12</option>\n                <option value=\"13\">13</option>\n                <option value=\"14\">14</option>\n                <option value=\"15\" selected>15</option>\n                <option value=\"16\">16</option>\n                <option value=\"17\">", "    def test_empty_value(self):\n        self.widget.is_required = False\n        self.assertEqual(\n            self.widget.format_value(None), {\"day\": None, \"month\": None, \"year\": None}\n        )\n\n\n\n", "    def test_widget_attrs(self):\n        widget = SelectDateWidget(attrs={'class': 'my-date-widget'})\n        self.assertEqual(widget.attrs, {'class': 'my-date-widget'})\n        self.check_html(\n            widget,\n            \"mydate\",\n            \"\",\n            html=(\n                \"\"\"\n            <select name=\"mydate_month\" id=\"id_mydate_month\" class=\"my-date-widget\">\n                <option selected value=\"\">---</option>\n                <option value=\"1\">January</option>\n                <option value=\"2\">February</option>\n                <option value=\"3\">March</option>\n                <option value=\"4\">April</option>\n                <option value=\"5\">May</option>\n                <option value=\"6\">June</option>\n                <option value=\"7\">July</option>\n                <option value=\"8\">August</option>\n                <option value=\"9\">September</option>\n                <option value=\"10\">October</option>\n                <option value=\"11\">November</option>\n                <option value=\"12\">December</option>\n            </select>\n            <select name=\"mydate_day\" id=\"id_mydate_day\" class=\"my-date-widget\">\n                <option selected value=\"\">---</option>\n                <option value=\"1\">1</option>\n                <option value=\"2\">2</option>\n                <option value=\"3\">3</option>\n                <option value=\"4\">4</option>\n                <option value=\"5\">5</option>\n                <option value=\"6\">6</option>\n                <option value=\"7\">7</option>\n                <option value=\"8\">8</option>\n                <option value=\"9\">9</option>\n                <option value=\"10\">10</option>\n                <option value=\"11\">11</option>\n                <option value=\"12\">12</option>\n                <option value=\"13\">13</option>\n                <option value=\"14\">14</option>\n                <option value=\"15\">15</option>\n                <option value=\"16\">1", "    def test_disabled_widget(self):\n        self.widget.disabled = True\n        self.assertHTMLEqual(\n            self.widget.render(\"mydate\", \"2010-04-15\"),\n            \"\"\"\n            <select name=\"mydate_month\" id=\"id_mydate_month\" disabled>\n                <option value=\"\">---</option>\n                <option value=\"1\">January</option>\n                <option value=\"2\">February</option>\n                <option value=\"3\">March</option>\n                <option value=\"4\" selected>April</option>\n                <option value=\"5\">May</option>\n                <option value=\"6\">June</option>\n                <option value=\"7\">July</option>\n                <option value=\"8\">August</option>\n                <option value=\"9\">September</option>\n                <option value=\"10\">October</option>\n                <option value=\"11\">November</option>\n                <option value=\"12\">December</option>\n            </select>\n\n            <select name=\"mydate_day\" id=\"id_mydate_day\" disabled>\n                <option value=\"\">---</option>\n                <option value=\"1\">1</option>\n                <option value=\"2\">2</option>\n                <option value=\"3\">3</option>\n                <option value=\"4\">4</option>\n                <option value=\"5\">5</option>\n                <option value=\"6\">6</option>\n                <option value=\"7\">7</option>\n                <option value=\"8\">8</option>\n                <option value=\"9\">9</option>\n                <option value=\"10\">10</option>\n                <option value=\"11\">11</option>\n                <option value=\"12\">12</option>\n                <option value=\"13\">13</option>\n                <option value=\"14\">14</option>\n                <option value=\"15\" selected>15</option>\n                <option value=\"16\">16</option>\n                <option value=\"17\">17</option>\n                <option value=\"18\">18</option>\n", "    def test_invalid_date(self):\n        with self.assertRaises(ValueError) as context:\n            self.widget.value_from_datadict({\"field_year\": \"\", \"field_month\": \"13\", \"field_day\": \"31\"}, {}, \"field\")\n        self.assertIn(\"Invalid date component: month\", str(context.exception))\n"], "sample_267": ["    def test_get_db_wrapper(self):\n        with self.assertDeprecatedContext(\n            message=(\n                '\"get_db_wrapper\" is deprecated in favor of'\n                ' \"connection.connection\" for getting the underlying'\n                ' sqlite3 connection.'\n            ),\n        ):\n            wrapper = connection.get_db_wrapper()\n        self.assertIsInstance(wrapper, connection.DatabaseWrapper)\n", "    def test_custom_sqlite_functions(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT _sqlite_datetime_cast_date('2023-04-07 10:00:00')\")\n            result = cursor.fetchone()[0]\n            self.assertEqual(result, '2023-04-07')\n", "    def test_concurrent_table_rename_inside_atomic_block(self):\n        \"\"\"It should not be possible to concurrently rename the same table.\"\"\"\n            with connection.schema_editor(atomic=True) as editor:\n                editor.alter_db_table(Author, \"backends_author\", \"renamed_table\")\n        thread = threading.Thread(target=rename_table)\n\n        with self.assertRaises(Exception) as ctx:\n            thread.start()\n            rename_table()\n        thread.join()\n        self.assertIn(\"attempt to rename table 'backends_author' to 'renamed_table' failed\", str(ctx.exception))\n", "    def test_savepoint_allowed(self):\n        with self.assertRaises(NotSupportedError):\n            with connection.cursor() as cursor:\n                cursor.execute('SAVEPOINT test')\n", "    def test_datetime_extract_iso_week_day(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT EXTRACT(weekday FROM '2023-04-03')\")\n            result = cursor.fetchone()[0]\n            self.assertEqual(result, 1)\n\n"], "sample_868": ["    def test_empty_input(metric_name):\n        # Check if metric raises error for empty input\n        metric = SUPERVISED_METRICS[metric_name]\n        with pytest.raises(ValueError):\n            metric([], [])\n        with pytest.raises(ValueError):\n            metric([0], [])\n        with pytest.raises(ValueError):\n            metric([], [0])\n\n", "    def test_empty_input(metric_name, metric_func):\n        if metric_name in SUPERVISED_METRICS:\n            with pytest.raises(ValueError, match='Input labels should not be empty'):\n                metric_func([], [])\n        else:\n            with pytest.raises(ValueError, match='Input data should not be empty'):\n                metric_func([], [])\n", "def test_contingency_matrix_consistency(metric_name, y1, y2):\n    metric = SUPERVISED_METRICS[metric_name]\n    score1 = metric(y1, y2)\n    contingency = contingency_matrix(y1, y2)\n    score2 = metric(y1, y2, contingency=contingency)\n    assert_allclose(score1, score2)\n", "    def test_empty_input(metric):\n        with pytest.raises(ValueError):\n            metric([], [])\n", "    def test_empty_labels(metric_name):\n        # Test behavior when one or both input arrays are empty\n        metric = SUPERVISED_METRICS[metric_name]\n        y_true = np.array([])\n        y_pred = np.array([0, 1, 2])\n        with pytest.raises(ValueError, match=\"Input arrays should not be empty\"):\n            metric(y_true, y_pred)\n        y_true = np.array([0, 1, 2])\n        y_pred = np.array([])\n        with pytest.raises(ValueError, match=\"Input arrays should not be empty\"):\n            metric(y_true, y_pred)\n\n"], "sample_57": ["    def test_validation_error_message(self):\n        user = User.objects.get(username='testclient')\n        data = {'password1': 'password', 'password2': 'password2'}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['password2'], [form.error_messages['password_mismatch']])\n", "    def test_empty_password_fields(self):\n        user = User.objects.get(username='testclient')\n        form = AdminPasswordChangeForm(user, {})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data.get('password1'), '')\n        self.assertEqual(form.cleaned_data.get('password2'), '')\n", "    def test_password_reset_email_template(self):\n        user, username, email = self.create_dummy_user()\n        form = PasswordResetForm({'email': email})\n        self.assertTrue(form.is_valid())\n        form.save()\n        self.assertEqual(len(mail.outbox), 1)\n\n        template = Template(mail.outbox[0].body)\n        context = template.context\n        self.assertIn(username, context['user'].get('username'))\n        self.assertIn('site.name', context)\n        self.assertIn('reset_url', context)\n\n", "    def test_superusers_can_change_their_password(self):\n        user = User.objects.create_superuser('superuser', 'superuser@example.com', 'password')\n        data = {'password1': 'newpassword', 'password2': 'newpassword'}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertTrue(form.is_valid())\n        form.save()\n", "    def test_bug_20941_unhashable_password(self):\n        user = User.objects.get(username='testclient')\n        user.set_password(None)\n        user.save()\n        data = {'password1': 'testpassword', 'password2': 'testpassword'}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertTrue(form.is_valid())\n        form.save()\n"], "sample_429": ["    def test_int_list_validator(self):\n        validator = int_list_validator()\n        self.assertEqual(validator(\"1,2,3\"), [1, 2, 3])\n        with self.assertRaises(ValidationError):\n            validator(\"1,2,a\")\n        with self.assertRaises(ValidationError):\n            validator(\"1,2,\")\n        with self.assertRaises(ValidationError):\n            validator(\"1,,2\")\n", "    def test_int_list_validator(self):\n        validator = int_list_validator(sep=\"|\")\n        self.assertEqual(validator(\"1|2|3\"), [1, 2, 3])\n        self.assertEqual(validator(\"1\"), [1])\n        with self.assertRaises(ValidationError):\n            validator(\"1|a|3\")\n        with self.assertRaises(ValidationError):\n            validator(\"1||3\")\n        with self.assertRaises(ValidationError):\n            validator(\"\")\n        with self.assertRaises(ValidationError):\n            validator(\"1,2,3\")\n", "    def test_int_list_validator_with_custom_sep(self):\n        v = int_list_validator(sep=\",\")\n        self.assertEqual(v(\"1,2,3\"), [1, 2, 3])\n        with self.assertRaises(ValidationError):\n            v(\"1.2.3\")\n\n", "    def test_int_list_validator(self):\n        v = int_list_validator(sep=',')\n        self.assertEqual(v(\"1,2,3\"), [1, 2, 3])\n        with self.assertRaises(ValidationError):\n            v(\"1,2,a\")\n        with self.assertRaises(ValidationError):\n            v(\"1,2,3,\")\n        with self.assertRaises(ValidationError):\n            v(\",1,2,3\")\n        v = int_list_validator(allow_negative=True)\n        self.assertEqual(v(\"-1,-2,3\"), [-1, -2, 3])\n\n", "    def test_validate_comma_separated_integer_list_with_empty_string(self):\n        self.assertEqual(validate_comma_separated_integer_list(\"\"), \"\")\n        self.assertEqual(validate_comma_separated_integer_list(\" \"), \"\")\n"], "sample_336": ["    def test_include_with_path_prefix(self):\n        included_urls = [\n            path('inner/', views.empty_view, name='urlobject-view'),\n        ]\n        prefix_urls = [\n            path('prefix/', include(included_urls, namespace='included')),\n        ]\n        resolver = URLResolver(RegexPattern(r'^'), prefix_urls)\n        match = resolver.resolve('/prefix/inner/')\n        self.assertEqual(match.url_name, 'included:urlobject-view')\n", "    def test_include_with_trailing_slash(self):\n        urlconf = URLObject('inc-app')\n        urlconf.urlpatterns = [\n            path('inner/', views.empty_view, name='urlobject-view'),\n        ]\n        self.assertEqual(\n            include(urlconf, namespace='test', app_name='test'),\n            (urlconf.urlpatterns, 'test', 'test')\n        )\n", "    def test_include_with_partial_match(self):\n        urls = [path('test/', views.empty_view, name='included-test')]\n        included_urls = URLObject('inc-urls', urls)\n        outer_urls = [path('inc/', include(included_urls))]\n        resolver = URLResolver(RegexPattern(r'^'), outer_urls)\n\n        match = resolver.resolve('/inc/test/')\n        self.assertEqual(match.url_name, 'included-test')\n        self.assertEqual(match.app_name, 'inc-urls')\n", "    def test_include_with_none_app_name(self):\n        self.assertEqual(include(self.url_patterns, app_name=None), (\n            self.url_patterns,\n            None,\n            None\n        ))\n", "    def test_get_view(self):\n      \n        match = resolve('/test-resolver-match/')\n        self.assertEqual(match.get_view(), views.test_resolver_match)\n\n"], "sample_279": ["    def test_database_constraint_expression_wrapper(self):\n        Product.objects.create(price=499, discounted_price=5)\n        with self.assertRaises(IntegrityError):\n            Product.objects.create(price=500, discounted_price=5)\n", "    def test_include_covering_index(self):\n        constraints = get_constraints(UniqueConstraintInclude._meta.db_table)\n        expected_name = 'name_color_uniq'\n        self.assertIn(expected_name, constraints)\n", "    def test_covering_index_database_constraint(self):\n        UniqueConstraintInclude.objects.create(name='p1', color='red', price=10)\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintInclude.objects.create(name='p1', color='red', price=10)\n", "    def test_opclasses_database_constraint(self):\n        UniqueConstraintProduct.objects.create(price=10, discounted_price=5, unit='\u03bcg/mL')\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintProduct.objects.create(price=10, discounted_price=7, unit='l')\n", "    def test_name_with_deferrable(self):\n        constraints = get_constraints(UniqueConstraintDeferrable._meta.db_table)\n        self.assertIn('name_init_deferred_uniq', constraints)\n        self.assertIn('sheld_init_immediate_uniq', constraints)\n"], "sample_382": ["    def test_get_template_directories_no_dirs(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            set(),\n        )\n", "    def test_is_django_path(self):\n        self.assertTrue(autoreload.is_django_path('/path/to/django/templates'))\n        self.assertFalse(autoreload.is_django_path('/path/to/project/templates'))\n\n", "    def test_watch_for_template_changes_no_dirs(self, mock_get_dirs):\n        mock_get_dirs.return_value = set()\n        mock_reloader = mock.MagicMock()\n        autoreload.watch_for_template_changes(mock_reloader)\n        self.assertEqual(mock_reloader.watch_dir.call_count, 0)\n\n", "    def test_get_template_directories_no_dirs(self):\n        self.assertSetEqual(autoreload.get_template_directories(), set())\n", "    def test_template_changed_with_single_loader(self, mock_reset):\n        template_path = Path(__file__).parent / 'templates' / 'index.html'\n        self.assertTrue(autoreload.template_changed(None, template_path))\n        mock_reset.assert_called_once()\n\n"], "sample_960": ["def test_python_python_use_unqualified_type_names_no_module_names(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><a class=\"reference internal\" href=\"#Name\" title=\"Name\">'\n            '<span class=\"pre\">Name</span></a></span>' in content)\n    assert '<span class=\"n\"><span class=\"pre\">Age</span></span>' in content\n    assert ('<p><strong>name</strong> (<a class=\"reference internal\" href=\"#Name\" '\n            'title=\"Name\"><em>Name</em></a>) \u2013 blah blah</p>' in content)\n    assert '<p><strong>age</strong> (<em>Age</em>) \u2013 blah blah</p>' in content\n", "def test_nested_module(app):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert \"Nested Module Docstring\" in content\n    assert \"Inner nested class docstring\" in content\n", "def test_py_attribute_deprecated(app, warning):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :deprecated:\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert \".. py:attribute:: attr\" in warning.getvalue()\n", "compilation error", "def test_python_python_use_unqualified_type_names_nested(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><a class=\"reference internal\" href=\"#foo.bar.NestedName\" title=\"foo.bar.NestedName\">'\n            '<span class=\"pre\">NestedName</span></a></span>' in content)\n    assert '<span class=\"n\"><span class=\"pre\">foo.bar.NestedAge</span></span>' in content\n    assert ('<p><strong>nested_name</strong> (<a class=\"reference internal\" href=\"#foo.bar.NestedName\" '\n            'title=\"foo.bar.NestedName\"><em>NestedName</em></a>) \u2013 blah blah</p>' in content)\n    assert '<p><strong>nested_age</strong> (<em>foo.bar.NestedAge</em>) \u2013 blah blah</p>' in content\n"], "sample_43": ["compilation error", "compilation error", "def test_fitness_function_validation():\n    rng = np.random.RandomState(42)\n    t = rng.randn(100)\n\n    # Test that fitness='events' raises an error if x is not integer or None\n    with pytest.raises(ValueError):\n        bayesian_blocks(t, fitness='events', x=t)\n\n    # Test that fitness='regular_events' raises an error if x is not binary\n    t = np.arange(100)\n    with pytest.raises(ValueError):\n        bayesian_blocks(t, fitness='regular_events', x=t, dt=1)\n\n    # Test that fitness='measures' raises an error if x is not specified\n    with pytest.raises(ValueError):\n        bayesian_blocks(t, fitness='measures')\n", "    def test_missing_data():\n        rng = np.random.RandomState(0)\n        t = rng.rand(100)\n        x = rng.rand(100)\n        t[::10] = np.nan\n        x[::10] = np.nan\n\n        with warnings.catch_warnings(record=True) as w:\n            bins = bayesian_blocks(t, x, fitness='measures')\n            assert len(w) == 1\n            assert 'NaN values in the data' in str(w[0].message)\n\n        assert len(bins) > 1\n", "compilation error"], "sample_509": ["compilation error", "    def test_DateFormatter_format(self, fmt, expected):\n        dt = datetime.datetime(2022, 1, 10)\n        formatter = mdates.DateFormatter(fmt)\n        assert formatter(mdates.date2num(dt)) == expected \n", "compilation error", "compilation error", "compilation error"], "sample_237": ["    def test_required_fields_empty(self):\n        class CustomUser(AbstractBaseUser):\n            username = models.CharField(max_length=30, unique=True)\n            USERNAME_FIELD = 'username'\n            REQUIRED_FIELDS = []\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [])\n", "    def test_is_anonymous_authenticated_methods_with_bad_user_model(self):\n        class CustomUserBadMethods(AbstractBaseUser):\n            username = models.CharField(max_length=30, unique=True)\n            \n            USERNAME_FIELD = 'username'\n            REQUIRED_FIELDS = 'date_of_birth'\n\n                return True\n\n                return True\n\n        errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Critical(\n                '%s.is_anonymous must be an attribute or property rather than '\n                'a method. Ignoring this is a security issue as anonymous '\n                'users will be treated as authenticated!' % CustomUserBadMethods,\n                obj=CustomUserBadMethods,\n                id='auth.C009',\n            ),\n            checks.Critical(\n                '%s.is_authenticated must be an attribute or property rather '\n                'than a method. Ignoring this is a security issue as anonymous '\n                'users will be treated as authenticated!' % CustomUserBadMethods,\n                obj=CustomUserBadMethods,\n                id='auth.C010',\n            ),\n        ])\n", "    def test_permission_name_length_with_long_model_name(self):\n        long_model_name = 'X' * 64\n        class LongModelName(models.Model):\n            pass\n        LongModelName._meta.verbose_name = 'Long verbose name'\n        LongModelName._meta.permissions = [('long_permission_name', 'Long permission')]\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The verbose_name of model 'auth_tests.%s' must be at most 191 \"\n                \"characters for its builtin permission names to be at most \"\n                \"255 characters.\" % long_model_name,\n                obj=LongModelName,\n                id='auth.E007',\n            ),\n        ])\n\n", "    def test_required_fields_empty(self):\n        \"\"\"REQUIRED_FIELDS should not be empty.\"\"\"\n        class CustomUserNonListRequiredFields(AbstractBaseUser):\n            username = models.CharField(max_length=30, unique=True)\n            date_of_birth = models.DateField()\n\n            USERNAME_FIELD = 'username'\n            REQUIRED_FIELDS = []\n        errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"'REQUIRED_FIELDS' must be a list or tuple.\",\n                obj=CustomUserNonListRequiredFields,\n                id='auth.E001',\n            ),\n        ])\n", "    def test_is_anonymous_is_authenticated_properties(self):\n        class CustomUser(AbstractBaseUser):\n            username = models.CharField(max_length=30, unique=True)\n            USERNAME_FIELD = 'username'\n\n            @property\n                return True\n\n            @property\n                return True\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Critical(\n                '%s.is_anonymous must be an attribute or property rather than '\n                'a method. Ignoring this is a security issue as anonymous '\n                'users will be treated as authenticated!' % CustomUser,\n                obj=CustomUser,\n                id='auth.C009',\n            ),\n            checks.Critical(\n                '%s.is_authenticated must be an attribute or property rather '\n                'than a method. Ignoring this is a security issue as anonymous '\n                'users will be treated as authenticated!' % CustomUser,\n                obj=CustomUser,\n                id='auth.C010',\n            ),\n        ])\n\n"], "sample_172": ["    def test_ManyToManyField_using_to_field(self):\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_member_add'))\n\n        # Add a new member\n        self.selenium.find_element_by_id('id_name').send_keys('test')\n        self.selenium.find_element_by_xpath('//input[@value=\"Save\"]').click()\n        self.wait_page_ready()\n\n        member = Member.objects.get(name='test')\n\n        # Add new groups\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_group_add'))\n        self.selenium.find_element_by_id('id_name').send_keys('Group1')\n        self.selenium.find_element_by_xpath('//input[@value=\"Save\"]').click()\n        self.wait_page_ready()\n        Group.objects.create(name='Group2')\n\n        # Edit the member and assign groups using add button\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_member_change', args=(member.id,)))\n\n        # Click the Add Group button to add new\n        self.selenium.find_element_by_id('add_id_groups').click()\n        self.wait_for_and_switch_to_popup()\n\n        group_checkbox = self.selenium.find_element_by_css_selector('input[value=\"Group1\"]')\n        group_checkbox.click()\n\n        group2_checkbox = self.selenium.find_element_by_css_selector('input[value=\"2\"]')\n        group2_checkbox.click()\n\n        save_button_css_selector = '.submit-row > input[type=submit]'\n        self.selenium.find_element_by_css_selector(save_button_css_selector).click()\n        self.selenium.switch_to.window(self.selenium.window_handles[0])\n\n        member = Member.objects.get(name='test')\n        self.assertEqual(member.groups.", "    def test_ManyToManyField_using_to_field(self):\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_event_add'))\n\n        main_window = self.selenium.current_window_handle\n\n        # Click the Add Band button to add a new band\n        self.selenium.find_element_by_id('add_id_supporting_bands').click()\n        self.wait_for_and_switch_to_popup()\n        name_field = self.selenium.find_element_by_id('id_name')\n        name_field.send_keys('New Band')\n\n        save_button_css_selector = '.submit-row > input[type=submit]'\n        self.selenium.find_element_by_css_selector(save_button_css_selector).click()\n\n        self.selenium.switch_to.window(main_window)\n        self.selenium.find_element_by_css_selector('#id_supporting_bands option[value=New Band]')\n", "    def test_ManyToManyField(self):\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_event_add'))\n        self.wait_page_ready()\n\n        # Find the \"Select\" button for the ManyToManyField.\n        select_button = self.selenium.find_element_by_id('lookup_id_bands')\n        select_button.click()\n\n        # Wait for the popup to open.\n        self.wait_for_popup()\n\n        # Assert that the Bogey Blues band is present in the popup list.\n        bogey_blues_option = self.selenium.find_element_by_css_selector('.related-object-list a[href*=\"/band/42/\"]')\n        self.assertTrue(bogey_blues_option.is_displayed())\n\n        # Click on the Bogey Blues band.\n        bogey_blues_option.click()\n\n        # Close the popup.\n        self.selenium.find_element_by_css_selector('.related-lookups-modal button:last-of-type').click()\n\n        # Assert that the Bogey Blues band is now selected in the ManyToManyField.\n        bogey_blues_input = self.selenium.find_element_by_id('id_bands')\n        self.assertEqual(bogey_blues_input.get_attribute('value'), '42')\n", "    def test_ManyToManyField_using_to_field(self):\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_event_add'))\n\n        main_window = self.selenium.current_window_handle\n        # Click the Add Band button to add new\n        self.selenium.find_element_by_id('add_id_supporting_bands').click()\n        self.wait_for_and_switch_to_popup()\n        name_field = self.selenium.find_element_by_id('id_name')\n        name_field.send_keys('New Band')\n\n        save_button_css_selector = '.submit-row > input[type=submit]'\n        self.selenium.find_element_by_css_selector(save_button_css_selector).click()\n        self.selenium.switch_to.window(main_window)\n        # The field now contains the new band\n        self.selenium.find_element_by_css_selector('#id_supporting_bands option[value=New Band]')\n\n        # Click the Change Band button to change it\n        self.selenium.find_element_by_id('change_id_supporting_bands').click()\n        self.wait_for_and_switch_to_popup()\n\n        name_field = self.selenium.find_element_by_id('id_name')\n        name_field.clear()\n        name_field.send_keys('Changed Band')\n\n        save_button_css_selector = '.submit-row > input[type=submit]'\n        self.selenium.find_element_by_css_selector(save_button_css_selector).click()\n        self.selenium.switch_to.window(main_window)\n        self.selenium.find_element_by_css_selector('#id_supporting_bands option[value=Changed Band]')\n\n        # Go ahead and submit the form to make sure it works\n        self.selenium.find_element_by_css_selector(save_button_css_selector).click()\n        self.wait_for_text('li.success', 'The event was", "    def test_ManyToManyField_using_to_field(self):\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_event_add'))\n\n        main_window = self.selenium.current_window_handle\n        # Click the Add Band button to add new\n        self.selenium.find_element_by_id('add_id_supporting_bands').click()\n        self.wait_for_and_switch_to_popup()\n\n        name_field = self.selenium.find_element_by_id('id_name')\n        name_field.send_keys('New Band')\n\n        save_button_css_selector = '.submit-row > input[type=submit]'\n        self.selenium.find_element_by_css_selector(save_button_css_selector).click()\n        self.selenium.switch_to.window(main_window)\n        # The field now contains the new band\n        self.selenium.find_element_by_css_selector('#id_supporting_bands option[value=New Band]')\n\n"], "sample_702": ["compilation error", "compilation error", "compilation error", "def test_pytester_assert_outcomes_warnings_filter(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import warnings\n\n            warnings.warn(UserWarning(\"some custom warning\"))\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-W\", \"ignore::UserWarning\")\n    result.assert_outcomes(passed=1)\n", "def test_pytester_subprocess_with_timeout_and_environment(pytester: Pytester) -> None:\n    testfile = pytester.makepyfile(\n        \"\"\"\n        import time\n        import os\n\n            time.sleep(1)\n            assert os.environ.get(\"CUSTOM_ENV\") == \"42\"\n        \"\"\"\n    )\n    with pytest.raises(pytester.TimeoutExpired):\n        pytester.runpytest_subprocess(\n            testfile, timeout=0.5, env={\"CUSTOM_ENV\": \"42\"}\n        )\n"], "sample_1208": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_54": ["    def test_file_response_with_custom_filename(self):\n        response = FileResponse(open(__file__, 'rb'), filename='custom_filename.py')\n        self.assertEqual(response['Content-Length'], str(os.path.getsize(__file__)))\n        self.assertIn(response['Content-Type'], ['text/x-python', 'text/plain'])\n        self.assertEqual(response['Content-Disposition'], 'attachment; filename=\"custom_filename.py\"')\n        response.close()\n\n", "    def test_file_response_with_filename(self):\n        response = FileResponse(ContentFile(b'binary content'), filename='my_file.txt')\n        self.assertEqual(response['Content-Disposition'], 'inline; filename=\"my_file.txt\"')\n", "    def test_empty_file_response_content_length_is_zero(self):\n        response = FileResponse(io.BytesIO(b''))\n        self.assertEqual(response['Content-Length'], '0')\n", "    def test_file_response_filename_from_file_object(self):\n        with tempfile.NamedTemporaryFile(mode='w+', suffix='.txt') as tmp:\n            tmp.write('test content')\n            tmp.seek(0)\n            response = FileResponse(tmp, filename='file.txt')\n            self.assertEqual(response['Content-Disposition'], 'attachment; filename=\"file.txt\"')\n", "    def test_file_response_with_filename_and_as_attachment(self):\n        response = FileResponse(io.BytesIO(b'binary content'), filename='test.txt', as_attachment=True)\n        self.assertEqual(response['Content-Length'], '14')\n        self.assertEqual(response['Content-Type'], 'application/octet-stream')\n        self.assertEqual(response['Content-Disposition'], 'attachment; filename=\"test.txt\"')\n        self.assertEqual(list(response), [b'binary content'])\n"], "sample_116": ["    def test_cache_key_with_post_request(self):\n        request = self.factory.post(self.path, {'test': 1})\n        template = engines['django'].from_string(\"This is a test\")\n        response = TemplateResponse(HttpRequest(), template)\n        # POST requests are not cached by default.\n        self.assertIsNone(get_cache_key(request))\n", "    def test_cache_timeout_zero(self):\n        \"\"\"\n        Test cache_page decorator when cache timeout is 0.\n        \"\"\"\n        view = cache_page(0)(hello_world_view)\n        request = self.factory.get('/view/')\n        response = view(request, '1')\n        self.assertEqual(response.content, b'Hello World 1')\n        response = view(request, '2')\n        self.assertEqual(response.content, b'Hello World 2')\n\n", "    def test_cache_key_with_etag(self):\n        request = self.factory.get(self.path)\n        response = TemplateResponse(HttpRequest(), engines['django'].from_string(\"This is a test\"))\n        response['ETag'] = 'some-etag'\n        learn_cache_key(request, response)\n        self.assertEqual(\n            get_cache_key(request),\n            'views.decorators.cache.cache_page.settingsprefix.GET.'\n            '58a0a05c8a5620f813686ff969c26853.some-etag'\n        )\n", "    def test_alias_invalid(self):\n        with self.assertRaises(ImproperlyConfigured):\n            caches['invalid_alias']\n", "    def test_cache_key_with_language_and_timezone(self):\n        with translation.override('fr'):\n            timezone.activate('America/Los_Angeles')\n            request = self.factory.get(self.path, {'test': 1})\n            template = engines['django'].from_string(\"This is a test\")\n            response = TemplateResponse(HttpRequest(), template)\n            learn_cache_key(request, response)\n\n            self.assertEqual(\n                get_cache_key(request),\n                'views.decorators.cache.cache_page.settingsprefix.GET.'\n                '0f1c2d56633c943073c4569d9a9502fe.fr.America%2FLos_Angeles'\n            )\n"], "sample_920": ["    def test_numpy_docstring_with_raises_and_returns(self):\n        docstring = \"\"\"", "    def test_empty_sections(self):\n        docstring = \"\"\"", "    def test_empty_returns(self):\n        docstring = \"\"\"", "    def test_references_in_example(self):\n        docstring = \"\"\"", "def test_returns_with_types(self):\n        docstring = \"\"\"\\"], "sample_527": ["    def test_figure_canvas_draw_event(self):\n        fig, ax = plt.subplots()\n        canvas = fig.canvas\n        called = []\n\n            called.append(event)\n\n        canvas.mpl_connect('draw_event', handler)\n\n        fig.canvas.draw()\n        assert len(called) == 1\n        assert isinstance(called[0], DrawEvent)\n\n", "    def test_rubberband_draw(self):\n        fig, ax = plt.subplots()\n        ax.plot([0, 1], [0, 1])\n        rubberband = RubberbandBase(ax.figure.canvas)\n\n        # Simulate a rectangle selection\n        start_event = MouseEvent(\"button_press_event\", fig.canvas, 100, 100)\n        rubberband.press(start_event)\n        stop_event = MouseEvent(\"button_release_event\", fig.canvas, 200, 200)\n        rubberband.release(stop_event)\n\n        assert rubberband.get_artists()\n", "    def test_toolmanager_trigger_tool(self):\n        with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n            plt.rcParams['toolbar'] = 'toolmanager'\n        fig = plt.gcf()\n        assert fig.canvas.manager.toolmanager.trigger_tool('forward')\n", "    def test_toolmanager_add_tool():\n        with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n            plt.rcParams['toolbar'] = 'toolmanager'\n        fig = plt.gcf()\n        class CustomTool(ToolBase):\n            name = 'custom'\n            description = 'My Custom Tool'\n                pass\n        fig.canvas.manager.toolmanager.add_tool(CustomTool)\n        assert 'custom' in fig.canvas.manager.toolmanager.tools\n        assert 'custom' in fig.canvas.manager.toolmanager.tool_keymap\n\n", "    def test_rubberband_rect(self):\n        fig, ax = plt.subplots()\n        rect = RubberbandBase(ax)\n        event = MouseEvent(\"button_press_event\", fig.canvas, 10, 10,\n                           MouseButton.LEFT)\n        rect.on_press(event)\n        event = MouseEvent(\"button_release_event\", fig.canvas, 20, 20,\n                           MouseButton.LEFT)\n        rect.on_release(event)\n        assert rect.geometry.vertices == [(10, 10), (20, 10), (20, 20),\n                                          (10, 20), (10, 10)]\n"], "sample_879": ["def test_ordinal_encoder_drop_first_with_unknown_categories():\n    \"\"\"Check drop='first' with unknown categories and handle_unknown='ignore'.\"\"\"\n    X = [[\"a\", 0], [\"b\", 2], [\"b\", 1], [\"c\", 3]]\n\n    ohe = OrdinalEncoder(drop=\"first\", handle_unknown=\"ignore\").fit(X)\n    X_trans = ohe.transform(X)\n\n    assert_allclose(X_trans, [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [2.0, 0.0]])\n\n", "    def test_ordinal_encoder_handle_unknown_unknown_value_int():\n        X = np.array([[1], [2], [3]])\n        oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=0).fit(X)\n        X_test = np.array([[4]])\n        X_trans = oe.transform(X_test)\n        assert_allclose(X_trans, [[0]])\n\n", "    def test_ordinal_encoder_pandas_set_output_sparse_raises(self):\n        pd = pytest.importorskip(\"pandas\")\n        X_df = pd.DataFrame({\"A\": [\"a\", \"b\"], \"B\": [1, 2]})\n        oe = OrdinalEncoder()\n        with pytest.raises(TypeError, match=\"Pandas output does not support sparse data\"):\n            oe.set_output(transform=\"pandas\").fit_transform(X_df)\n", "compilation error", "compilation error"], "sample_733": ["    def test_countvectorizer_empty_vocabulary():\n        # Test that CountVectorizer raises an error when vocabulary is empty.\n        message = (\n            \"Vocabulary can not be empty when using vocabulary argument\"\n        )\n        assert_raise_message(ValueError, message, CountVectorizer,\n                             vocabulary=[])\n\n\n", "    def test_countvectorizer_stopwords_fit_transform_parameter():\n        vect = CountVectorizer(stop_words='english')\n        X = vect.fit_transform(JUNK_FOOD_DOCS)\n        assert all(word not in vect.get_feature_names()\n                   for word in vect.stop_words_)\n", "    def test_vectorizer_empty_corpus():\n        for vec in [CountVectorizer(), TfidfVectorizer(), HashingVectorizer()]:\n            assert_raises(ValueError, vec.fit, [])\n            assert_raises(ValueError, vec.fit_transform, [])\n            \n", "    def test_countvectorizer_empty_vocabulary():\n        # Check behavior when vocabulary is empty\n        vect = CountVectorizer(vocabulary=[])\n        assert_raises(ValueError, vect.fit, ALL_FOOD_DOCS)\n", "    def test_vectorizer_dtype_consistency():\n        # Test dtype consistency for different vectorizer types and input types\n        for vec_class in [CountVectorizer, TfidfVectorizer, HashingVectorizer]:\n            vec = vec_class()\n            X_int = vec.fit_transform([\"one\", \"two\", \"three\"]).astype('int32')\n            assert_equal(X_int.dtype, np.int32)\n            X_float = vec.fit_transform([\"one\", \"two\", \"three\"]).astype('float64')\n            assert_equal(X_float.dtype, np.float64)\n"], "sample_672": ["def test_saferepr_with_cycle():\n    class Cyclic:\n            self.ref = self\n    assert \"Cyclic object\" in saferepr(Cyclic())\n", "def test_saferepr_with_circular_reference():\n    class Node:\n            self.value = value\n            self.next = None\n\n    node1 = Node(1)\n    node2 = Node(2)\n    node1.next = node2\n    node2.next = node1\n\n    repr_result = saferepr(node1)\n    assert \"<circular reference>\" in repr_result\n", "def test_saferepr_recursion():\n    l = []\n    l.append(l)\n    assert \"Recursion\" in saferepr(l)\n", "    def test_saferepr_of_saferepr():\n        s = saferepr(saferepr(range(100)))\n        assert len(s) <= len(\"[\" + SafeRepr(0).maxlist * \"100\" + \"]\") + 2\n        assert s.startswith(\"[\") and s.endswith(\"]\")\n\n", "def test_saferepr_recursive():\n    class Recursive:\n            self.value = value\n\n            return repr(Recursive(self))\n\n    assert saferepr(Recursive(1)) == \"<[RecursionError raised in repr()] Recursive object at 0x{:x}>\".format(\n        id(Recursive(1))\n    )\n"], "sample_824": ["    def test_haversine_distances_chunked():\n        # Check haversine distance with distances computation using chunk handling\n            diff_lat = y[0] - x[0]\n            diff_lon = y[1] - x[1]\n            a = np.sin(diff_lat / 2) ** 2 + (\n                np.cos(x[0]) * np.cos(y[0]) * np.sin(diff_lon/2) ** 2\n            )\n            c = 2 * np.arcsin(np.sqrt(a))\n            return c\n\n        rng = np.random.RandomState(0)\n        X = rng.random_sample((500, 2))\n        Y = rng.random_sample((1000, 2))\n        D1 = np.array([[slow_haversine_distances(x, y) for y in Y] for x in X])\n        D2 = pairwise_distances_chunked(X, Y, metric='haversine',\n                 working_memory=100)\n        assert_allclose(D1, D2)\n\n", "    def test_haversine_distances_large_dataset():\n        # Test haversine distance computation on a large dataset\n        rng = np.random.RandomState(0)\n        X = rng.random_sample((10000, 2))\n        Y = rng.random_sample((10000, 2))\n        distances = haversine_distances(X, Y)\n        # Check if the distances are within a reasonable range\n        assert np.all(distances >= 0)\n        assert np.all(distances <= 2 * np.pi * 6371)\n", "def test_pairwise_distances_chunked_empty_input():\n    # Test that pairwise_distances_chunked handles empty input arrays.\n    X = np.array([])\n    Y = np.array([])\n    with pytest.raises(ValueError, match=\"Input arrays must be non-empty\"):\n        pairwise_distances_chunked(X, Y, metric=\"euclidean\")\n\n    X = np.array([]).reshape(0, 3)\n    Y = np.array([]).reshape(0, 3)\n    with pytest.raises(ValueError, match=\"Input arrays must be non-empty\"):\n        pairwise_distances_chunked(X, Y, metric=\"euclidean\")\n", "def test_pairwise_distances_sparse_discrete_metrics(metric):\n    # Test pairwise_distances on sparse matrices with discrete metrics.\n    rng = np.random.RandomState(0)\n    X = rng.randint(2, size=(10, 5)).tocsr()\n    Y = rng.randint(2, size=(10, 5)).tocsr()\n    dist_matrix = pairwise_distances(X, Y, metric=metric)\n    assert dist_matrix.shape == (10, 10)\n    assert np.all(dist_matrix >= 0)\n", "    def test_pairwise_distances_chunked_sparse():\n        # Check the pairwise distances computation on sparse matrices with chunking.\n        rng = np.random.RandomState(0)\n        X = rng.random_sample((1000, 10))\n        X_sparse = csr_matrix(X)\n        Y = rng.random_sample((1000, 10))\n        Y_sparse = csr_matrix(Y)\n\n        distances_dense = pairwise_distances(X, Y)\n        distances_sparse_chunked = pairwise_distances_chunked(X_sparse,\n                                                            Y_sparse,\n                                                            metric='euclidean')\n\n        assert_allclose(distances_dense, distances_sparse_chunked)\n\n"], "sample_1091": ["compilation error", "compilation error", "compilation error", "def test_issue_21984():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    assert (Eq(x, 0) & Eq(y, 0)).simplify() == Eq(x, 0) & Eq(y, 0)\n    assert (Eq(x, 1) & Eq(x, 0)).simplify() == False\n    assert (Eq(x, y) & Eq(x, 0)).simplify() == Eq(y, 0)\n\n\n\n", "compilation error"], "sample_755": ["def test_silhouette_empty_cluster():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    labels = np.array([0, 1, 2])\n    \n    with pytest.warns(UserWarning, match=\"Empty cluster detected\"):\n        silhouette_score(X, labels)\n\n", "def test_silhouette_samples_empty_cluster():\n    # Test silhouette_samples when there is an empty cluster\n    X = [[0.], [1.], [2.], [3.], [4.], [5.]]\n    labels = np.array([0, 0, 1, 1, 2, 2])\n    \n    # Assert that the silhouette samples for the empty cluster are NaN\n    silhouette_samples_values = silhouette_samples(X, labels)\n    assert np.isnan(silhouette_samples_values[2]\n\n", "compilation error", "def test_silhouette_samples_empty_cluster():\n\n    X = np.array([[1, 2], [1, 3],\n                  [2, 1], [2, 2],\n                  [3, 3], [3, 2]])\n\n    labels = np.array([0, 0, 1, 1, 2, 3])\n    \n    silhouette_samples_scores = silhouette_samples(X, labels)\n    assert np.isnan(silhouette_samples_scores[5])\n", "    def test_davies_bouldin_score_csr_matrix():\n        X = csr_matrix([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5 +\n                          [[0, 4], [1, 3]] * 5 + [[3, 1], [4, 0]] * 5)\n        labels = [0] * 10 + [1] * 10 + [2] * 10 + [3] * 10\n        pytest.approx(davies_bouldin_score(X, labels), 2 * np.sqrt(0.5) / 3)\n\n"], "sample_176": ["    def test_alter_unique_together_from_empty(self):\n        \"\"\"\n        #24449 - When modifying UniqueTogether constraints, if the initial\n        state is empty, ensure the new constraint is added correctly.\n        \"\"\"\n        before = [\n            ModelState('app', 'MyModel', [\n                ('id', models.AutoField(primary_key=True)),\n            ]),\n        ]\n        after = [\n            ModelState('app', 'MyModel', [\n                ('id', models.AutoField(primary_key=True)),\n                ('field1', models.CharField(max_length=100)),\n                ('field2', models.CharField(max_length=100)),\n            ],\n            unique_together=[('field1', 'field2')]),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['AlterModelTable'])\n        self.assertOperationAttributes(\n            changes, 'app', 0, 0, name='MyModel',\n            unique_together=[('field1', 'field2')]\n        )\n", "    def test_alter_field_with_default_from_base_model(self):\n        \"\"\"\n        #24078 - Altering a field with a default inherited from a base model\n        shouldn't create an extra migration step\n        \"\"\"\n        before = [\n            ModelState('app', 'Base', [\n                ('id', models.AutoField(primary_key=True)),\n                ('name', models.CharField(default='default', max_length=200)),\n            ]),\n            ModelState('app', 'Child', [], bases=('app.Base',)),\n        ]\n        after = [\n            ModelState('app', 'Base', [\n                ('id', models.AutoField(primary_key=True)),\n                ('name', models.CharField(default='new_default', max_length=200)),\n            ]),\n            ModelState('app', 'Child', [], bases=('app.Base',)),\n        ]\n        changes = self.get_changes(before, after)\n\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['AlterField'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='name', model_name='Base')\n", "    def test_field_rename_after_model_removal(self):\n        \"\"\"\n        Renaming a field in a model after another model inheriting\n        from it is removed.\n        \"\"\"\n        before = [\n            ModelState('app', 'base', [\n                ('id', models.AutoField(primary_key=True)),\n                ('name', models.CharField(max_length=100)),\n            ]),\n            ModelState('app', 'child', [], bases=('app.base',)),\n        ]\n        after = [\n            ModelState('app', 'base', [\n                ('id', models.AutoField(primary_key=True)),\n                ('new_name', models.CharField(max_length=100)),\n            ]),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['DeleteModel', 'RenameField'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='child')\n        self.assertOperationAttributes(changes, 'app', 0, 1, old_name='name', new_name='new_name', model_name='base')\n\n", "    def test_remove_custom_user_model(self):\n        \"\"\"\n        Removing a custom user model should be handled correctly.\n        \"\"\"\n        with override_settings(AUTH_USER_MODEL=\"testapp.CustomUser\"):\n            before = [\n                ModelState('testapp', 'CustomUser', [\n                    ('id', models.AutoField(primary_key=True)),\n                ], bases=(AbstractBaseUser,)),\n            ]\n            after = []\n            changes = self.get_changes(before, after)\n            self.assertNumberMigrations(changes, 'testapp', 1)\n            self.assertOperationTypes(changes, 'testapp', 0, ['DeleteModel'])\n            self.assertOperationAttributes(changes, 'testapp', 0, 0, name='CustomUser')\n", "    def test_remove_field_in_base_model_after_adding_inherited_model():\n        \"\"\"\n        Removing a field from a base model after adding an inherited model\n        that uses the field should remove it from the child model as well\n        \"\"\"\n        before = [\n            ModelState('app', 'base', [\n                ('id', models.AutoField(primary_key=True)),\n                ('name', models.CharField(max_length=200)),\n            ]),\n            ModelState('app', 'child', [\n\n            ], bases=('app.base',)),\n        ]\n        after = [\n            ModelState('app', 'base', [\n                ('id', models.AutoField(primary_key=True)),\n            ]),\n            ModelState('app', 'child', [\n\n            ], bases=('app.base',)),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['RemoveField'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='name', model_name='base')\n\n"], "sample_634": ["    def test_expand_modules_with_multiple_files(self, files_or_modules, expected):\n        \"\"\"Test expand_modules when given multiple files.\"\"\"\n        ignore_list, ignore_list_re = [], []\n        modules, errors = expand_modules(\n            files_or_modules,\n            ignore_list,\n            ignore_list_re,\n            get_global_option(self.checker, \"ignore-paths\"),\n        )\n        modules.sort(key=lambda d: d[\"name\"])\n        assert modules == expected\n        assert not errors\n", "    def test_expand_modules_with_package(self, files_or_modules, expected):\n        \"\"\"Test expand_modules when given a package path\"\"\"\n        ignore_list, ignore_list_re = [], []\n        modules, errors = expand_modules(\n            files_or_modules,\n            ignore_list,\n            ignore_list_re,\n            get_global_option(self.checker, \"ignore-paths\"),\n        )\n        modules.sort(key=lambda d: d[\"name\"])\n        assert modules == expected\n        assert not errors\n", "    def test_expand_modules_with_nonexistent_file(self, files_or_modules, expected):\n        ignore_list, ignore_list_re = [], []\n        modules, errors = expand_modules(\n            files_or_modules,\n            ignore_list,\n            ignore_list_re,\n            get_global_option(self.checker, \"ignore-paths\"),\n        )\n        modules.sort(key=lambda d: d[\"name\"])\n        assert modules == expected\n        assert not errors\n\n", "    def test_expand_modules_with_ignore_unittest(self):\n        \"\"\"Test expand_modules with a non-default value of ignore-paths\"\"\"\n        ignore_list, ignore_list_re = [], []\n        files_or_modules = [str(Path(__file__).parent)]\n        modules, errors = expand_modules(\n            files_or_modules,\n            ignore_list,\n            ignore_list_re,\n            get_global_option(self.checker, \"ignore-paths\"),\n        )\n        modules.sort(key=lambda d: d[\"name\"])\n        assert modules == [init_of_package]\n        assert not errors\n", "    def test_expand_modules_nonexistent_module(self, files_or_modules, expected):\n        \"\"\"Test expand_modules with non-existent modules.\"\"\"\n        ignore_list, ignore_list_re = [], []\n        modules, errors = expand_modules(\n            files_or_modules,\n            ignore_list,\n            ignore_list_re,\n            get_global_option(self, \"ignore-paths\"),\n        )\n        assert modules == []\n        assert len(errors) == len(files_or_modules)\n        for error in errors:\n            assert error[\"key\"] == \"fatal\"\n"], "sample_668": ["def test_node_from_parent_warning(name):\n    class MockConfig:\n        pass\n\n    ms = MockConfig()\n    with pytest.warns(\n        DeprecationWarning,\n        match=\"Direct construction of .* has been deprecated, please use .*from_parent.*\",\n    ) as w:\n        getattr(nodes, name)(name=name, config=ms, session=ms, nodeid=\"None\")\n    assert w[0].lineno == inspect.currentframe().f_lineno - 1\n    assert w[0].filename == __file__\n", "def test_fixture_positional_arguments(testdir, fixture_definition):\n    testdir.makepyfile(fixture_definition)\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"Passing arguments to pytest.fixture() as positional arguments is deprecated - pass them \"\n            \"as a keyword argument instead.\",\n        ]\n    )\n\n", "def test_fixture_positional_arguments_warning(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            return arg1\n\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*Passing arguments to pytest.fixture() as positional arguments is deprecated*\",\n            \"*pass them as a keyword argument instead.*\",\n        ]\n    )\n", "def test_fixture_positional_arguments_warning(testdir, argname):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            return {argname}\n\n            pass\n    \"\"\".format(argname=argname)\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            f\"Passing arguments to pytest.fixture() as positional arguments is deprecated - pass them as a keyword argument instead.\"\n        ]\n    )\n", "def test_nodep_fixture_positional_argument_warning(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            return 'value'\n\n            assert my_fixture == 'value'\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*Passing arguments to pytest.fixture() as positional arguments is deprecated - pass them as a keyword argument instead.*\"\n        ]\n    )\n"], "sample_60": ["    def test_get_formsets_with_inlines_empty_inlines(self):\n        class EpisodeAdmin(admin.ModelAdmin):\n            inlines = []\n        ma = EpisodeAdmin(Episode, self.site)\n        self.assertEqual(ma.get_formsets_with_inlines(request), [])\n", "    def test_inline_foreign_key_change(self):\n        class EpisodeInline(GenericTabularInline):\n            model = Episode\n            fk_name = 'category'\n\n        class CategoryAdmin(admin.ModelAdmin):\n            inlines = [EpisodeInline]\n\n        ma = CategoryAdmin(Category, self.site)\n        self.assertEqual(ma.get_inline_instances(request)[0].fk_name, 'category')\n", "    def test_formset_with_readonly_fields_and_exclude(self):\n        \"\"\"Test that readonly_fields and exclude work together correctly in formsets.\"\"\"\n\n        class MediaForm(ModelForm):\n            class Meta:\n                model = Media\n                exclude = ['url']\n\n        class MediaInline(GenericTabularInline):\n            readonly_fields = ['description']\n            form = MediaForm\n            model = Media\n            extra = 0\n\n        class EpisodeAdmin(admin.ModelAdmin):\n            inlines = [\n                MediaInline\n            ]\n\n        ma = EpisodeAdmin(Episode, self.site)\n        formset = ma.get_formsets_with_inlines(request)[0][0]()\n        self.assertEqual(list(formset.forms[0].fields), ['keywords', 'id', 'DELETE'])\n", "    def test_permissions_for_auto_created_related_models(self):\n        # Test that permissions for auto-created related models are handled correctly.\n        # This is related to #22550.\n        class CustomCategoryAdmin(admin.ModelAdmin):\n            pass\n        admin.site.register(Category, CustomCategoryAdmin)\n        request = self.factory.get(reverse('admin:generic_inline_admin_episode_change', args=(self.episode_pk,)))\n        request.user = User(username='super', is_superuser=True)\n        response = CustomCategoryAdmin.changeform_view(request, object_id=str(self.episode_pk))\n", "    def test_get_formset_kwargs_with_exclude_and_readonly(self):\n        media_inline = MediaInline(Media, AdminSite())\n\n        # Create a formset with custom exclude and readonly_fields\n        formset = media_inline.get_formset(request, exclude=['url'], readonly_fields=['description'])\n        self.assertEqual(formset.exclude, ['url'])\n        self.assertEqual(formset.readonly_fields, ['description'])\n\n"], "sample_774": ["  def test_one_hot_encoder_sparse_output():\n        enc = OneHotEncoder(sparse=True)\n        X = np.array([['a', 'b'], ['c', 'd']])\n        X_tr = enc.fit_transform(X)\n\n        assert isinstance(X_tr, sparse.csr_matrix)\n        assert X_tr.shape == (2, 4)\n\n        enc = OneHotEncoder(sparse=False)\n        X_tr = enc.fit_transform(X)\n        assert isinstance(X_tr, np.ndarray)\n        assert X_tr.shape == (2, 4)\n\n", "    def test_empty_input(self):\n        enc = OneHotEncoder()\n        X = np.array([])\n        with pytest.raises(ValueError, match=\"Input array is empty\"):\n            enc.fit(X)\n", "    def test_one_hot_encoder_drop_first_dense_output(self):\n        enc = OneHotEncoder(drop='first', sparse=False)\n        X = [['a', 1], ['b', 2], ['a', 3]]\n        trans = enc.fit_transform(X)\n        exp = [[0., 1.],\n              [1., 0.],\n              [0., 1.]]\n        assert_array_equal(trans, exp)\n", "    def test_ordinal_encoder_drop(self):\n        enc = OrdinalEncoder(handle_unknown='ignore', drop='first')\n        X = [['abc', 2, 55], ['def', 1, 55], ['abc', 3, 55]]\n        exp = np.array([[1., 0.],\n                       [0., 1.],\n                       [1., 1.]])\n        assert_array_equal(enc.fit_transform(X), exp)\n\n", "compilation error"], "sample_823": ["    def test_pairwise_distances_chunked_metric(self):\n        # Test pairwise_distances_chunked with a custom metric function\n            return np.sum((x - y) ** 2)\n        X = np.random.rand(10, 5)\n        distances = pairwise_distances_chunked(X, metric=my_metric)\n        expected = cdist(X, metric=my_metric)\n        assert_allclose(distances, expected)\n", "compilation error", "def test_haversine_distances_large_data():\n    # Check haversine distance with distances computation for large data\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((10000, 2))\n    Y = rng.random_sample((1000, 2))\n    D = haversine_distances(X, Y)\n    assert D.shape == (10000, 1000)\n    assert np.all(D >= 0.)\n    assert np.all(D <= 2.)\n", "    def test_pairwise_distances_empty_input():\n        # Test pairwise distances with empty input arrays\n        X = np.array([])\n        Y = np.array([])\n\n        assert pairwise_distances(X, Y).shape == (0, 0)\n        assert pairwise_distances(X, None).shape == (0, 0)\n        assert pairwise_distances(None, Y).shape == (0, 0)\n", "def test_pairwise_distances_chunked_empty(metric):\n    # check that pairwise_distances_chunked works with empty input\n    X = np.array([], dtype=np.float64).reshape(0, 5)\n    Y = np.array([], dtype=np.float64).reshape(0, 5)\n\n    distances = pairwise_distances_chunked(X, Y, metric=metric)\n    assert isinstance(distances, np.ndarray)\n    assert distances.shape == (0, 0)\n"], "sample_213": ["    def test_file_object_no_name(self):\n        \"\"\"\n        Storing a file object without a name should generate a unique filename\n        \"\"\"\n        file_like_object = StringIO('Some content')\n        f = File(file_like_object)\n        stored_filename = self.storage.save('', f)\n        self.assertTrue(stored_filename)  \n", "    def test_file_like_object_with_name(self):\n        obj = SimpleUploadedFile('test.txt', b'content')\n        self.storage.save('uploaded.txt', obj)\n        with self.storage.open('uploaded.txt') as f:\n            self.assertEqual(f.read(), b'content')\n", "    def test_file_like_object_closing(self):\n        class ClosingFileLike:\n                self.closed = False\n                return b'content'\n                self.closed = True\n\n        f = ClosingFileLike()\n        self.storage.save(\"closing_file.txt\", f)\n        self.assertTrue(f.closed)\n\n", "    def test_file_like_object_with_read_only(self):\n        class ReadOnlyFileLike:\n                self.content = content\n                self.position = 0\n\n                if size == -1:\n                    remaining = len(self.content) - self.position\n                    data = self.content[self.position:]\n                    self.position = len(self.content)\n                    return data\n                else:\n                    end = min(self.position + size, len(self.content))\n                    data = self.content[self.position:end]\n                    self.position = end\n                    return data\n\n        read_only_file = ReadOnlyFileLike(b\"Test content\")\n        f = File(read_only_file)\n        stored_filename = self.storage.save(\"read_only_file.txt\", f)\n        with self.storage.open(stored_filename) as stored_file:\n            self.assertEqual(stored_file.read(), b\"Test content\")\n\n", "    def test_io_bytes_io(self):\n        \"\"\"\n        Test the File storage API with an io.BytesIO object.\n        \"\"\"\n        file_like_object = io.BytesIO(b\"Some bytes\")\n        f = File(file_like_object)\n        stored_filename = self.storage.save(\"bytes_file.txt\", f)\n\n        with self.storage.open(stored_filename) as stored_file:\n            self.assertEqual(stored_file.read(), b\"Some bytes\")\n"], "sample_154": ["    def test_postgresql_default_tablespace(self):\n        good_default_tablespaces = [\n            'pg_default',\n        ]\n        for response in good_default_tablespaces:\n            with mock.patch(\n                'django.db.backends.utils.CursorWrapper.fetchone', create=True,\n                return_value=(response,)\n            ):\n                self.assertEqual(check_database_backends(databases=self.databases), [])\n\n        bad_default_tablespaces = ['some_nonexistent_tablespace', '']\n        for response in bad_default_tablespaces:\n            with mock.patch(\n                'django.db.backends.utils.CursorWrapper.fetchone', create=True,\n                return_value=(response,)\n            ):\n                # One warning for each database alias\n                result = check_database_backends(databases=self.databases)\n                self.assertEqual(len(result), 2)\n                self.assertEqual([r.id for r in result], ['postgresql.W001', 'postgresql.W001'])\n", "    def test_sqlite_pragma_foreign_keys(self):\n        good_pragma_value = 1\n        with mock.patch('django.db.backends.sqlite3.base.Database.get_pragma',\n                        return_value=good_pragma_value):\n            self.assertEqual(check_database_backends(databases=self.databases), [])\n\n        bad_pragma_value = 0\n        with mock.patch('django.db.backends.sqlite3.base.Database.get_pragma',\n                        return_value=bad_pragma_value):\n            result = check_database_backends(databases=self.databases)\n            self.assertEqual(len(result), 2)\n            self.assertEqual([r.id for r in result], ['sqlite.W001', 'sqlite.W001'])\n", "    def test_postgresql_effective_cache_time(self):\n        good_effective_cache_time = '120'\n        with mock.patch('django.db.backends.postgresql.base.Database.get_effective_cache_time', return_value=good_effective_cache_time):\n            self.assertEqual(check_database_backends(databases=self.databases), [])\n\n        bad_effective_cache_time = '0'\n        with mock.patch('django.db.backends.postgresql.base.Database.get_effective_cache_time', return_value=bad_effective_cache_time):\n            result = check_database_backends(databases=self.databases)\n            self.assertEqual(len(result), 2)\n            self.assertEqual([r.id for r in result], ['postgresql.W001', 'postgresql.W001']) \n", "    def test_postgresql_setting(self):\n        with mock.patch('django.db.backends.postgresql.validation.DatabaseValidation.check') as mocked_check:\n            check_database_backends(databases=self.databases)\n            mocked_check.assert_called_once()\n", "    def test_database_checks_empty_databases(self, mocked_check):\n        check_database_backends(databases=set())\n        self.assertFalse(mocked_check.called) \n"], "sample_1037": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1046": ["compilation error", "def test_tensor_replacement_with_wildcards():\n    L = TensorIndexType(\"L\")\n    i, j, k, l = tensor_indices(\"i j k l\", L)\n    A, B = tensorhead(\"A B\", [L], [[1]])\n\n    expr = A(i)*B(j)\n    repl = {A(i): Any, B(j): Any}\n    assert expr.replace_with_arrays(repl, [i, j]) == (i, j)\n\n    expr = A(i)*A(-i)\n    repl = {A(_): Any}\n    assert expr.replace_with_arrays(repl, []) == Any\n\n    expr = A(i)*A(j)\n    repl = {A(i): Any, A(j): Any}\n    raises(ValueError, lambda: expr.replace_with_arrays(repl, [i, j]))\n\n", "def test_tensor_replacement_with_free_indices():\n    L = TensorIndexType(\"L\")\n    i, j, k = tensor_indices(\"i j k\", L)\n    A, B = tensorhead(\"A B\", [L], [[1]])\n\n    expr = A(i)*B(j)\n    repl = {A(i): i**2}\n    assert expr.replace_with_arrays(repl) == Symbol('i')**2*B(j)\n\n    expr = A(i)*A(j)\n    repl = {A(i): i**2}\n    assert expr.replace_with_arrays(repl) == Symbol('i')**2*Symbol('j')**2\n\n    expr = A(i)*B(i)\n    repl = {A(i): i**2}\n    assert expr.replace_with_arrays(repl) == Symbol('i')**2*B(i)\n\n    expr = A(i)*B(-i)\n    repl = {A(i): i**2}\n\n    raises(ValueError, lambda: expr.replace_with_arrays(repl))\n", "compilation error", "compilation error"], "sample_8": ["    def test_masked_array_compress(self, axis):\n        compressed = self.ma.compress(self.ma > 0, axis=axis)\n        expected_data = self.a.compress(self.a > 0, axis=axis)\n        expected_mask = self.mask_a.compress(self.a > 0, axis=axis)\n        assert_array_equal(compressed.unmasked, expected_data)\n        assert_array_equal(compressed.mask, expected_mask)\n", "    def test_masked_array_from_quantity(self):\n        \"\"\"Check that we can initialize a MaskedArray from a Quantity.\"\"\"\n        np_ma = np.ma.MaskedArray(self.q)\n        assert type(np_ma) is np.ma.MaskedArray\n        assert type(np_ma.data) is MaskedQuantity\n        assert type(np_ma.mask) is np.ndarray\n        assert_array_equal(np_ma.data.value, self.q.value)\n        assert_array_equal(np_ma.mask, self.q.mask)\n\n", "def test_masked_array_to_quantity(self):\n        ma = Masked(self.a, mask=self.mask_a, unit=self.a.unit)\n        q = ma.astype(self.a.dtype)\n        assert isinstance(q, Quantity)\n        assert q.value.shape == ma.shape\n        assert_array_equal(q.value, ma.unmasked)\n        assert_array_equal(q.mask, ma.mask)\n        assert q.unit == ma.unit\n", "    def test_masked_array_roundtrip(self):\n        \"\"\"Check roundtrip conversion to and from MaskedArray.\"\"\"\n        ma = np.ma.MaskedArray(self.a, mask=self.mask_a)\n        msa = Masked(ma)\n        assert type(msa) is type(self.ma)\n        assert_array_equal(msa.unmasked, self.a)\n        assert_array_equal(msa.mask, self.mask_a)\n", "    def test_masked_array_constructor_from_masked_quantity(self):\n        \"\"\"Check that we can initialize a MaskedArray from a MaskedQuantity.\"\"\"\n        np_ma = np.ma.MaskedArray(self.mq)\n        assert type(np_ma) is np.ma.MaskedArray\n        assert type(np_ma.data) is np.ndarray\n        assert type(np_ma.mask) is np.ndarray\n        assert_array_equal(np_ma.data, self.mq.value)\n        assert_array_equal(np_ma.mask, self.mq.mask)\n\n"], "sample_1143": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_29": ["    def test_to_latex_names(self, write, latex_names, tmp_path):\n        fp = tmp_path / \"test_to_latex_names.tex\"\n        write(fp, format=\"latex\", latex_names=latex_names)\n        tbl = QTable.read(fp)\n        if latex_names:\n            for column_name in tbl.colnames[2:]:\n                assert column_name in _FORMAT_TABLE.values()\n        else:\n            for column_name in tbl.colnames[2:]:\n                assert column_name in self.cosmo.__parameters__\n", "    def test_latex_unit_conversion(self, cosmo, write, tmp_path):\n        \"\"\"Test that units are correctly converted in the LaTeX output.\"\"\"\n        fp = tmp_path / \"test_latex_unit_conversion.tex\"\n        write(fp, format=\"latex\")\n        tbl = QTable.read(fp)\n        for colname in cosmo.__parameters__:\n            param = getattr(type(cosmo), colname)\n            if isinstance(param, Parameter) and param.unit is not None:\n                assert tbl[colname].unit == param.unit\n", "    def test_write_latex_units(self, cosmo, write, tmp_path):\n        fp = tmp_path / \"test_write_latex_units.tex\"\n        write(fp, format=\"latex\")\n        tbl = QTable.read(fp)\n        for name, unit in cosmo.parameters.get_units().items():\n            if unit is not None:\n                # Check that the unit is included in the table output\n                assert f' {unit} ' in str(tbl[name][0]) \n", "    def test_to_latex_units(self, write, cosmo, tmp_path):\n        \"\"\"Test that units are correctly written to the LaTeX file.\"\"\"\n        fp = tmp_path / \"test_to_latex_units.tex\"\n        write(fp, format=\"latex\")\n        tbl = QTable.read(fp)\n        for param_name in cosmo.__parameters__:\n            param = getattr(type(cosmo), param_name)\n            if param.unit is not None and param.unit != u.one:\n                assert tbl[param_name].unit == param.unit\n", "    def test_invalid_format(self, write, tmp_path, format, error_type, error_match):\n        fp = tmp_path / \"test_invalid_format.tex\"\n        with pytest.raises(error_type, match=error_match):\n            write(fp, format=format)\n"], "sample_614": ["def test_diff_dim_summary(n_elements: int = 100):\n    ds_a = xr.Dataset(\n        coords={\n            f\"dim_{k}\": np.arange(3) for k in range(n_elements)\n        },\n    )\n    ds_b = xr.Dataset(\n        coords={\n            f\"dim_{k}\": np.arange(3) for k in range(\n                n_elements // 2\n            )\n        },\n    )\n\n    expected = \"\"\"\\", "    def test_diff_coords_repr_with_attrs(self) -> None:\n        \n        coords_a = {'x': xr.DataArray([1, 2, 3], dims=\"x\", attrs={'attr': 'a'})}\n        coords_b = {'x': xr.DataArray([1, 2, 3], dims=\"x\", attrs={'attr': 'b'})}\n\n        expected = dedent(\n            \"\"\"\\\n        Differing coordinates:\n        L * x        (x) int64 1 2 3\n        R * x        (x) int64 1 2 3\n        Differing attributes:\n        L   x: attr: a\n        R   x: attr: b\n        \"\"\"\n        ).strip()\n        actual = formatting.diff_coords_repr(coords_a, coords_b, \"equals\")\n        assert expected == actual\n", "    def test_diff_data_vars_repr_equals(self) -> None:\n        da_a = xr.DataArray(\n            np.array([[1, 2, 3], [4, 5, 6]], dtype=\"int64\"),\n            dims=(\"x\", \"y\"),\n            coords={\n                \"x\": np.array([\"a\", \"b\"], dtype=\"U1\"),\n                \"y\": np.array([1, 2, 3], dtype=\"int64\"),\n            },\n            attrs={\"units\": \"m\"},\n        )\n\n        da_b = xr.DataArray(\n            np.array([[1, 2, 3], [4, 5, 6]], dtype=\"int64\"),\n            dims=(\"x\", \"y\"),\n            coords={\n                \"x\": np.array([\"a\", \"b\"], dtype=\"U1\"),\n                \"y\": np.array([1, 2, 3], dtype=\"int64\"),\n            },\n            attrs={\"units\": \"m\"},\n        )\n\n        actual = formatting.diff_data_vars_repr(da_a, da_b, \"equals\")\n        expected = \"\"\n        assert actual == expected\n", "def test_diff_data_vars_repr_with_attrs() -> None:\n    ds_a = xr.Dataset(\n        data_vars={\n            \"var1\": ((\"x\", \"y\"), np.array([[1, 2, 3], [4, 5, 6]], dtype=\"int64\")),\n            \"var2\": (\"x\", np.array([3, 4], dtype=\"int64\")),\n        },\n        coords={\n            \"x\": np.array([\"a\", \"b\"], dtype=\"U1\"),\n            \"y\": np.array([1, 2, 3], dtype=\"int64\"),\n        },\n        attrs={\"units\": \"m\", \"description\": \"desc\"},\n    )\n\n    ds_b = xr.Dataset(\n        data_vars={\n            \"var1\": ((\"x\", \"y\"), np.array([[1, 2, 3], [4, 5, 6]], dtype=\"int64\")),\n            \"var2\": (\"x\", np.array([3, 4], dtype=\"int64\")),\n        },\n        coords={\n            \"x\": np.array([\"a\", \"b\"], dtype=\"U1\"),\n            \"y\": np.array([1, 2, 3], dtype=\"int64\"),\n        },\n        attrs={\"units\": \"kg\", \"description\": \"desc\"},\n    )\n    expected = dedent(\n        \"\"\"\\\n        Differing attributes:\n        L   units: m\n        R   units: kg\"\"\"\n    )\n\n    actual = formatting.diff_data_vars_repr(ds_a.data_vars, ds_b.data_vars, \"identical\")\n    assert actual == expected\n\n\n\n", "def test__element_formatter_no_truncation(n_elements: int = 3) -> None:\n    intro = \"Dimensions without coordinates: \"\n    elements = [\n        f\"{k}: {v}\" for k, v in {f\"dim_{k}\": 3 for k in np.arange(n_elements)}.items()\n    ]\n    expected = intro + \", \".join(elements) + \"\\n\"\n    values = xr.core.formatting._element_formatter(\n        elements, col_width=len(intro), max_rows=None\n    )\n    actual = intro + values\n    assert expected == actual\n\n"], "sample_558": ["    def test_grid_with_axes_class_not_overriding_axis():\n        fig, axs = plt.subplots(2, 2, figsize=(6, 6),\n                                 gridspec_kw={'hspace': 0.3, 'wspace': 0.3})\n        for ax in axs.flat:\n            ax.plot([1, 2, 3], [4, 5, 6], marker='o')\n\n        # Recreate the grid with axes_class=Axes\n        grid = Grid(fig, 111, (2, 2), axes_class=plt.Axes)\n\n        for ax in grid:\n            ax.plot([1, 2, 3], [4, 5, 6], marker='o')\n", "    def test_grid_with_axes_class_overriding_axis():\n        class MyAxes(mpl.axes.Axes):\n                super().__init__(*args, **kwargs)\n                self.axis(\"off\")\n        Grid(plt.figure(), 111, (2, 2), axes_class=MyAxes)\n", "compilation error", "compilation error", "compilation error"], "sample_892": ["    def test_adaboost_warm_start_with_increasing_estimators(self):\n        \"\"\"Test warm_start with increasing n_estimators.\"\"\"\n        rng = np.random.RandomState(0)\n        X = rng.rand(100, 10)\n        y = rng.randint(0, 2, size=100)\n\n        model = AdaBoostClassifier(random_state=0)\n        model.fit(X, y)\n        initial_n_estimators = model.n_estimators\n\n        model.set_params(n_estimators=initial_n_estimators * 2, warm_start=True)\n        model.fit(X, y)\n\n        assert model.n_estimators == initial_n_estimators * 2\n\n", "    def test_estimator_with_get_params_and_set_params(self, estimator):\n        \"\"\"\n        Test that estimators with custom get_params and set_params methods work\n        with AdaBoost.\n\n        This test ensures that custom parameter handling in estimators doesn't\n        break AdaBoost functionality.\n        \"\"\"\n\n        X, y = datasets.make_classification(n_samples=100, random_state=42)\n        if isinstance(estimator, DummyClassifier):\n            model = AdaBoostClassifier(estimator=estimator)\n        else:\n            model = AdaBoostRegressor(estimator=estimator)\n\n        model.fit(X, y)\n        \n\n\n", "def test_adaboost_regressor_sample_weight_empty():\n    \"\"\"\n    Test that AdaBoostRegressor works with an empty sample_weight array.\n    \"\"\"\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([1, 0])\n    sample_weight = np.array([])\n\n    model = AdaBoostRegressor(n_estimators=1)\n    with pytest.raises(ValueError, match=\"Sample weights have shape\"):\n        model.fit(X, y, sample_weight=sample_weight)\n", "    def test_multioutput_regression():\n        rng = np.random.RandomState(0)\n        X = rng.rand(100, 5)\n        y = rng.rand(100, 3)\n\n        regr = AdaBoostRegressor(\n            estimator=DecisionTreeRegressor(), n_estimators=10, random_state=0\n        )\n        regr.fit(X, y)\n        assert regr.predict(X).shape == (100, 3)\n", "    def test_adaboost_regressor_sparse_data(self):\n        X, y = datasets.make_regression(n_samples=100, n_features=10, random_state=42)\n        X_sparse = csc_matrix(X)\n        regressor = AdaBoostRegressor(DecisionTreeRegressor(), random_state=42)\n        regressor.fit(X_sparse, y)\n        predictions = regressor.predict(X_sparse)\n        assert predictions.shape == (100,)\n"], "sample_787": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_balanced_accuracy_score_empty():\n    assert balanced_accuracy_score([], []) == 1.0\n    assert balanced_accuracy_score([0], []) == 0.0\n    assert balanced_accuracy_score([], [0]) == 0.0\n"], "sample_378": ["    def test_bulk_update_with_related_objects(self):\n        parent = RelatedObject.objects.create()\n        RelatedObject.objects.create()\n        related_objs = [\n            SingleObject.objects.create(data=i)\n            for i in range(10)\n        ]\n        parent.multiple.set(related_objs[:5])\n        parent.refresh_from_db()\n        RelatedObject.objects.bulk_update(\n            [parent],\n            fields=['multiple'],\n        )\n        parent.refresh_from_db()\n\n        self.assertEqual(parent.multiple.count(), 5)\n        self.assertCountEqual(parent.multiple.all(), related_objs[:5])\n\n\n\n\n", "    def test_bulk_update_with_empty_fields(self):\n        notes = [Note.objects.create(note='test', misc='test') for _ in range(10)]\n        Note.objects.bulk_update(notes, [])\n        self.assertEqual(Note.objects.filter(note='test').count(), 10)\n        self.assertEqual(Note.objects.filter(misc='test').count(), 10)\n", "    def test_related_objects_bulk_update(self):\n        parent = RelatedObject.objects.create()\n        children = [SingleObject() for _ in range(5)]\n        parent.singleobjects.set(children)\n\n        for child in children:\n            child.data = 'test-%s' % child.pk\n        RelatedObject.objects.bulk_update([parent], fields=['singleobjects__data'])\n        self.assertCountEqual(\n            SingleObject.objects.filter(data__icontains='test-'), children)\n", "    def test_m2m_field(self):\n        note = Note.objects.create(note='test', misc='test')\n        tags = self.create_tags()\n        note.tags.add(*tags[:5])\n        Note.objects.bulk_update([note], ['tags'])\n        note.refresh_from_db()\n        self.assertEqual(note.tags.count(), 5)\n", "    def test_bulk_update_with_related_object(self):\n        parent = RelatedObject.objects.create()\n        child = SingleObject.objects.create()\n        parent.single = child\n        RelatedObject.objects.bulk_update([parent], fields=['single'])\n        parent.refresh_from_db()\n        self.assertEqual(parent.single, child)\n"], "sample_834": ["def test_precomputed_init():\n    rng = np.random.RandomState(42)\n    X, y = make_classification(n_samples=30, centers=6, n_features=5,\n                               random_state=0)\n    # Generate a random transformation matrix\n    init = rng.randn(X.shape[1], X.shape[1])\n\n    # Fit with precomputed initialization\n    nca_precomputed = NeighborhoodComponentsAnalysis(init=init,\n                                                    max_iter=1)\n    nca_precomputed.fit(X, y)\n\n    # Fit without precomputed initialization\n    nca_no_init = NeighborhoodComponentsAnalysis(max_iter=1)\n    nca_no_init.fit(X, y)\n    assert_array_almost_equal(nca_precomputed.components_, init)\n", "    def test_convergence_warning_no_warn():\n        nca = NeighborhoodComponentsAnalysis(max_iter=100, verbose=1)\n        cls_name = nca.__class__.__name__\n        n_iter_ = nca.n_iter_\n        with pytest.warns(None) as record:\n            nca.fit(iris_data, iris_target)\n        assert len(record) == 0\n        assert nca.n_iter_ > n_iter_\n\n", "def test__validate_params_n_components(n_components, expected_output):\n    # Test the _validate_params method for\n    # the n_components parameter\n    X, y = make_classification(n_samples=30,\n                               n_features=5,\n                               n_classes=4,\n                               n_redundant=0,\n                               n_informative=5,\n                               random_state=0)\n    nca = NeighborhoodComponentsAnalysis(n_components=n_components)\n\n    with pytest.raises(ValueError) as excinfo:\n        nca._validate_params(X, y)\n    assert re.search(\n        rf\"The preferred dimensionality of the projected \"\n        rf\"space `n_components` ({n_components}) \"\n        rf\"must be strictly greater than 0 and less than or \"\n        rf\"equal to the number of features ({X.shape[1]})\"\n        , excinfo.value.args[0]\n    )\n\n", "    def test_n_components_error(self):\n        # Test that n_components is positive and less than the number of features\n        X = np.arange(12).reshape(4, 3)\n        y = [1, 1, 2, 2]\n        nca = NeighborhoodComponentsAnalysis(n_components=4)\n        assert_raises(ValueError, nca.fit, X, y)\n", "    def test_precomputed_transformation_shape(self):\n        \"\"\"Test that the transformation has the expected shape when\n        init='precomputed'.\"\"\"\n        X = iris_data\n        y = iris_target\n        n_components = 2\n        precomputed_transformation = np.random.rand(X.shape[1],\n                                                    n_components)\n        nca = NeighborhoodComponentsAnalysis(init=precomputed_transformation,\n                                            n_components=n_components)\n        nca.fit(X, y)\n        assert_equal(nca.components_.shape, (X.shape[1], n_components))\n"], "sample_866": ["def test_affinity_propagation_empty_input():\n    # Test AffinityPropagation with empty input\n    X = np.array([])\n    af = AffinityPropagation()\n    with pytest.raises(ValueError):\n        af.fit(X)\n", "def test_affinity_propagation_preference_array():\n    # Test affinity propagation with array-like preference.\n    S = -euclidean_distances(X, squared=True)\n    preferences = np.array([0.5, 1.0, 1.5])\n    cluster_centers_indices, labels = affinity_propagation(\n        S, preference=preferences)\n\n    af = AffinityPropagation(preference=preferences)\n    labels_precomputed = af.fit(S).labels_\n\n    assert_array_equal(labels, labels_precomputed)\n", "def test_affinity_propagation_preference_parameter():\n    # Test AffinityPropagation with different preference values and ensure\n    # predictable behavior\n    X = np.array([[1, 2], [1, 4], [1, 0],\n                  [4, 2], [4, 4], [4, 0]])\n\n    # Test default preference (median of similarities)\n    af = AffinityPropagation()\n    labels1 = af.fit(X).labels_\n\n    # Test explicit preference set close to the median\n    preference = np.median(-euclidean_distances(X, squared=True))\n    af = AffinityPropagation(preference=preference)\n    labels2 = af.fit(X).labels_\n    assert_array_equal(labels1, labels2)\n\n    # Test explicitly high preference, forcing more exemplars\n    af = AffinityPropagation(preference=100)\n    labels3 = af.fit(X).labels_\n    assert len(np.unique(labels3)) > len(np.unique(labels1))\n\n    # Test explicitly low preference, forcing fewer exemplars\n    af = AffinityPropagation(preference=-100)\n    labels4 = af.fit(X).labels_\n    assert len(np.unique(labels4)) < len(np.unique(labels1))\n\n", "def test_affinity_propagation_max_iter():\n    # Test the max_iter parameter\n    X = np.array([[0, 0], [1, 1], [-2, -2]])\n\n    # Set a low max_iter to force non-convergence\n    af = AffinityPropagation(max_iter=1)\n    with assert_warns(ConvergenceWarning):\n        af.fit(X)\n\n    # Check that the number of iterations is equal to max_iter\n    assert af.n_iter_ == 1\n", "    def test_affinity_propagation_preference_all_equal():\n        # Test AffinityPropagation with all preference values equal\n        # should result in a single cluster\n\n\n        X = np.array([[1, 0], [2, 0], [3, 0]])\n        S = -euclidean_distances(X, squared=True)\n\n        preference = np.array([0, 0, 0])\n        cluster_centers_indices, labels = affinity_propagation(S, preference=preference)\n        assert len(cluster_centers_indices) == 1\n\n        af = AffinityPropagation(preference=preference)\n        labels = af.fit(X).labels_\n\n        assert len(np.unique(labels)) == 1\n"], "sample_223": ["    def test_ticket_24605_join(self):\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        i3 = Individual.objects.create(alive=True)\n        i4 = Individual.objects.create(alive=False)\n\n        self.assertSequenceEqual(\n            Individual.objects.filter(\n                Q(alive=False) | Q(related_individual__isnull=True)\n            ).order_by('pk'),\n            [i2, i3, i4],\n        )\n", "    def test_ticket_23622_exclude(self):\n        \"\"\"\n        Make sure __pk__in and __in work the same for related fields when\n        using a distinct on subquery with exclude.\n        \"\"\"\n        a1 = Ticket23605A.objects.create()\n        a2 = Ticket23605A.objects.create()\n        c1 = Ticket23605C.objects.create(field_c0=0.0)\n        Ticket23605B.objects.create(\n            modela_fk=a1, field_b0=123,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a1, field_b0=23,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a1, field_b0=234,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a1, field_b0=12,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a2, field_b0=567,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a2, field_b0=76,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a2, field_b0=7,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela", "    def test_ticket_23622_empty(self):\n        \"\"\"\n        Ensure correct results with an empty subquery.\n        Ticket 23622 also covers empty subqueries, so we should test this explicitly too.\n        \"\"\"\n        self.assertSequenceEqual(Ticket23605A.objects.filter(Q(ticket23605b__pk__in=Ticket23605B.objects.none())), [])\n        self.assertSequenceEqual(Ticket23605A.objects.filter(Q(ticket23605b__in=Ticket23605B.objects.none())), [])\n", "    def test_ticket_25139(self):\n        # Ticket 25139 reported an issue with subqueries within\n        # F expressions and the use of `select_related` causing\n        # incorrect results.\n\n        parent = ParentObject.objects.create()\n        child1 = ChildObject.objects.create(parent=parent)\n        child2 = ChildObject.objects.create(parent=parent)\n\n        # This query used to return incorrect results\n        # because the subquery was not properly aliased.\n\n        qs = ParentObject.objects.select_related('childobject_set').annotate(\n            has_child_with_value=Exists(ChildObject.objects.filter(\n                parent=OuterRef('pk'),\n                value__gt=F('parent__value')\n            ))\n        )\n        self.assertQuerysetEqual(\n            qs,\n            [\n                '<ParentObject: > has_child_with_value: False',\n            ],\n        )\n", "    def test_ticket_24826(self):\n        \"\"\"\n        Regression test. Ensure that a JOIN is not mistakenly added when\n        filter conditions already ensure the models are joined.\n        \"\"\"\n        a1 = Author.objects.create(name='Author 1')\n        b1 = Book.objects.create(title='Book 1', author=a1)\n        b2 = Book.objects.create(title='Book 2', author=a1)\n        self.assertEqual(\n            Book.objects.filter(author__name='Author 1').count(),\n            2\n        )\n        self.assertEqual(\n            str(Book.objects.filter(author__name='Author 1').query).count('JOIN'),\n            1\n        )\n\n"], "sample_964": ["def test_py_attribute_no_type(app, warning):\n    text = \".. py:class:: Class\\n\"\n    text += \"   .. py:attribute:: attr\\n\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, (\"class\", desc_sig_space)],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'attr (Class attribute)', 'Class.attr', '', None)])\n    assert 'Class.attr' in app.env.objects\n    assert app.env.objects['Class.attr'] == ('index', 'Class.attr', 'attribute', False)\n", "compilation error", "def test_py_role_with_alias(app, status, warning):\n    text = \".. py:function:: my.module.func\"\n    restructuredtext.parse(app, text)\n    assert 'my.module.func' in app.env.roles\n\n    text = \".. py:func:: my.module.func\"\n    restructuredtext.parse(app, text)\n    assert 'my.module.func' in app.env.roles\n\n    text = \".. py:meth:: my.module.Class.method\"\n    restructuredtext.parse(app, text)\n    assert 'my.module.Class.method' in app.env.roles\n\n", "def test_python_use_unqualified_type_names_recursive_class(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert '<span class=\"n\"><a class=\"reference internal\" href=\"#foo.Outer.Inner\" title=\"foo.Outer.Inner\">' \\\n           '<span class=\"pre\">Inner</span></a></span>' in content\n    assert '<span class=\"n\"><span class=\"pre\">foo.Outer.Inner</span></span>' in content\n    assert ('<p><strong>inner</strong> (<a class=\"reference internal\" href=\"#foo.Outer.Inner\" '\n            'title=\"foo.Outer.Inner\"><em>Inner</em></a>) \u2013 blah blah</p>' in content)\n\n", "def test_py_module_index_entry_with_colon(app):\n    text = \".. py:module:: my_module:submodule\\n\"\n    doctree = restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate()[0] == (\n        'm', [IndexEntry('my_module', 0, 'index', 'module-my_module:submodule', '', '', '')]\n    )\n"], "sample_620": ["    def test_concat_different_index_types() -> None:\n        ds1 = Dataset(coords={\"x\": (\"x\", [1, 2])})\n        ds2 = Dataset(coords={\"x\": ((\"y\", \"z\"), [[3, 4], [5, 6]])})\n        with pytest.raises(\n            ValueError,\n            match=r\"Cannot concatenate along dimension 'x' indexes with different types.*\",\n        ):\n            concat([ds1, ds2], dim=\"x\")\n\n", "    def test_concat_preserve_dask_chunks(self) -> None:\n        data = np.arange(100).reshape((10, 10))\n\n        import dask.array as da\n\n        ds1 = Dataset(\n            {\"data\": ((\"x\", \"y\"), da.from_array(data[:5, :], chunks=(2, 5)))},\n            coords={\"x\": np.arange(5), \"y\": np.arange(10)},\n        )\n        ds2 = Dataset(\n            {\"data\": ((\"x\", \"y\"), da.from_array(data[5:, :], chunks=(2, 5)))},\n            coords={\"x\": np.arange(5, 10), \"y\": np.arange(10)},\n        )\n\n        actual = concat([ds1, ds2], dim=\"x\")\n        assert actual.data.chunks == ((2, 2, 2, 2, 2), (5,))\n", "    def test_concat_dask_array_metadata(self) -> None:\n        import dask.array as da\n\n        arr = da.from_array(np.arange(10), chunks=(5,))\n        da1 = DataArray(arr, dims=[\"x\"])\n        da2 = DataArray(arr, dims=[\"x\"])\n\n        actual = concat([da1, da2], dim=\"x\")\n\n        assert isinstance(actual.data, da.Array)\n        assert actual.chunks == ((5, 5),)\n", "    def test_concat_index_merge_conflict(self) -> None:\n        ds1 = Dataset(coords={\"x\": (\"x\", [1, 2]), \"y\": 1})\n        ds2 = Dataset(coords={\"x\": (\"x\", [3, 4]), \"y\": 2})\n\n        with pytest.raises(ValueError, match=r\"Conflicting index names\"):\n            concat([ds1, ds2], dim=\"x\", coords=\"all\")\n", "    def test_concat_index_different_names(self) -> None:\n        ds1 = Dataset(coords={\"x\": (\"x\", [1, 2])})\n        ds2 = Dataset(coords={\"y\": (\"x\", [3, 4])})\n\n        with pytest.raises(ValueError, match=r\"Cannot concatenate along dimension 'x'\"):\n            concat([ds1, ds2], dim=\"x\")\n\n"], "sample_759": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_751": ["def test_warm_start():\n    # Check warm start functionality\n    for alg in ['SAMME', 'SAMME.R']:\n        clf = AdaBoostClassifier(algorithm=alg, random_state=0, n_estimators=5)\n        clf.fit(iris.data, iris.target)\n        init_score = clf.score(iris.data, iris.target)\n\n        clf.set_params(n_estimators=10, warm_start=True)\n        clf.fit(iris.data, iris.target)\n\n        # Warm start should reuse existing estimators and improve score\n        assert clf.score(iris.data, iris.target) >= init_score\n        assert len(clf.estimators_) == 10 \n\n", "    def test_decision_function_shape(self):\n        # Test that decision_function returns a consistent shape.\n        clf = AdaBoostClassifier()\n        clf.fit(iris.data, iris.target)\n        scores = clf.decision_function(iris.data)\n        assert scores.shape == (iris.data.shape[0], len(clf.classes_))\n        \n", "def test_warm_start():\n    # Check warm start functionality.\n    rng = np.random.RandomState(0)\n    iris_weights = rng.randint(10, size=iris.target.shape)\n    boston_weights = rng.randint(10, size=boston.target.shape)\n\n    # AdaBoost classification\n    for alg in ['SAMME', 'SAMME.R']:\n        clf = AdaBoostClassifier(algorithm=alg, n_estimators=10, random_state=0)\n        clf.fit(iris.data, iris.target, sample_weight=iris_weights)\n        score = clf.score(iris.data, iris.target,\n                          sample_weight=iris_weights)\n        initial_n_estimators = clf.n_estimators\n\n        clf.set_params(warm_start=True).fit(iris.data, iris.target,\n                                           sample_weight=iris_weights)\n\n        assert_equal(clf.n_estimators, initial_n_estimators * 2)\n        assert_greater(clf.score(iris.data, iris.target,\n                                sample_weight=iris_weights), score)\n\n\n    # AdaBoost regression\n    clf = AdaBoostRegressor(n_estimators=10, random_state=0)\n    clf.fit(boston.data, boston.target, sample_weight=boston_weights)\n    score = clf.score(boston.data, boston.target,\n                      sample_weight=boston_weights)\n    initial_n_estimators = clf.n_estimators\n\n    clf.set_params(warm_start=True).fit(boston.data, boston.target,\n                                       sample_weight=boston_weights)\n\n    assert_equal(clf.n_estimators, initial_n_estimators * 2)\n    assert_greater(clf.score(boston.data, boston.target,\n                             sample_weight=boston_weights), score)\n", "def test_warm_start():\n    # Check warm start functionality.\n\n    for alg in ['SAMME', 'SAMME.R']:\n        clf = AdaBoostClassifier(algorithm=alg, warm_start=True, n_estimators=2)\n        clf.fit(iris.data[:50], iris.target[:50])\n\n        initial_n_estimators = len(clf.estimators_)\n        clf.fit(iris.data[:100], iris.target[:100])\n        assert len(clf.estimators_) == initial_n_estimators * 2\n", "def test_warm_start():\n    # Check warm start functionality.\n    rng = np.random.RandomState(0)\n    iris = datasets.load_iris()\n    perm = rng.permutation(iris.target.size)\n    iris.data, iris.target = shuffle(iris.data, iris.target,\n                                        random_state=rng)\n    X_train, X_test, y_train, y_test = train_test_split(iris.data,\n                                                        iris.target,\n                                                        random_state=rng)\n\n    clf1 = AdaBoostClassifier(n_estimators=5, random_state=0)\n    clf1.fit(X_train, y_train)\n    score1 = clf1.score(X_test, y_test)\n\n    clf2 = AdaBoostClassifier(n_estimators=10, random_state=0, warm_start=True)\n    clf2.fit(X_train, y_train)\n    score2 = clf2.score(X_test, y_test)\n\n    # warm_start=True should add 5 more estimators\n    assert_equal(len(clf2.estimators_), 10)\n    assert score2 >= score1\n\n"], "sample_243": ["    def test_add_filter_empty_conditions(self):\n        query = Query(Author)\n        query.add_filter(Q())\n        where = query.build_where(None)\n        self.assertIsNone(where)\n", "    def test_empty_join_promotion(self):\n        query = Query(Author)\n        query.add_filter(Q(published=True))\n        query.add_filter(Q(name__startswith='F'))\n        self.assertEqual(query.count(), 0)\n        query.promote_joins(query.alias_map.keys())\n        self.assertEqual(query.count(), 0)\n", "    def test_empty_q_object(self):\n        query = Query(Item)\n        where = query.build_where(Q())\n        self.assertIsNone(where)\n", "    def test_clone(self):\n        query = Query(Author)\n        query.add_select_related('item')\n        query.set_ordering(('name',))\n        clone = query.clone()\n        clone.add_filter(Q(num__gt=2))\n        self.assertEqual(query.select_related, {'item': {}})\n        self.assertEqual(query.ordering, ['name'])\n        self.assertEqual(clone.where.children[0].lhs.target, Author._meta.get_field('num'))\n", "    def test_empty_query(self):\n        query = Query(Author)\n        where = query.build_where(Q())\n        self.assertIsNone(where)\n"], "sample_328": ["    def test_update_objects_with_null_foreign_key(self):\n        self.create_tags()\n        with self.assertNumQueries(1):\n            Note.objects.bulk_update(self.notes, ['tag'], batch_size=1)\n", "    def test_json_field_none(self):\n        objs = [\n            JSONFieldNullable(json_field=None) for _ in range(10)\n        ]\n        JSONFieldNullable.objects.bulk_create(objs)\n        for obj in objs:\n            obj.json_field = {'a': 1}\n        JSONFieldNullable.objects.bulk_update(objs, ['json_field'])\n        self.assertCountEqual(JSONFieldNullable.objects.filter(json_field__has_key='a'), objs)\n        \n", "    def test_json_field_empty_value(self):\n        JSONFieldNullable.objects.bulk_create([\n            JSONFieldNullable(json_field=None) for _ in range(10)\n        ])\n        objs = JSONFieldNullable.objects.all()\n        for obj in objs:\n            obj.json_field = {}\n        JSONFieldNullable.objects.bulk_update(objs, ['json_field'])\n        self.assertCountEqual(JSONFieldNullable.objects.filter(json_field__exact={}), objs)\n", "    def test_empty_json_field(self):\n        JSONFieldNullable.objects.bulk_create([\n            JSONFieldNullable(json_field=None) for _ in range(10)\n        ])\n        objs = JSONFieldNullable.objects.all()\n        for obj in objs:\n            obj.json_field = {}\n        JSONFieldNullable.objects.bulk_update(objs, ['json_field'])\n        self.assertCountEqual(JSONFieldNullable.objects.filter(json_field__isnull=False), objs)\n", "    def test_empty_objects_with_fields(self):\n        rows_updated = Note.objects.bulk_update([], ['note'])\n        self.assertEqual(rows_updated, 0)\n"], "sample_670": ["def test_parentheses(expr: str, expected: bool) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_parentheses(expr: str, expected: bool) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_ident_surrounding_whitespace(expr: str, expected: bool) -> None:\n    matcher = {\"ident\": True}.__getitem__\n    assert evaluate(expr, matcher) is expected\n\n", "def test_parentheses(expr: str, expected: bool) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_nested_parens() -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(\"((true))\", matcher) is True\n    assert evaluate(\"(((true)))\", matcher) is True\n    assert evaluate(\"((((true))))\", matcher) is True\n    assert evaluate(\"((false))\", matcher) is False\n    assert evaluate(\"( (true) and (false))\", matcher) is False\n    assert evaluate(\"(((true) or (false)))\", matcher) is True \n"], "sample_475": ["    def test_actions_valid_with_custom_permissions(self):\n        @admin.action(permissions=[\"custom\"])\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (custom_permission_action,)\n\n                return True\n\n        self.assertIsValid(BandAdmin, Band)\n", "    def test_actions_permissions_not_callable(self):\n        @admin.action(permissions=\"invalid\")\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action,)\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"The value of 'permissions' for the 'action' action must be \"\n            \"a list or callable.\",\n            id=\"admin.E131\",\n        )\n", "    def test_actions_have_correct_permission(self):\n        @admin.action(permissions=[\"mypermission\"])\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action,)\n\n        self.assertIsValid(BandAdmin, Band)\n", "    def test_actions_valid_permissions(self):\n        @admin.action(permissions=[\"some_permission\"])\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (my_action,)\n\n        self.assertIsValid(BandAdmin, Band)\n", "    def test_actions_permissions_tuple(self):\n        @admin.action(permissions=(\"custom1\", \"custom2\"))\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (custom_permission_action,)\n\n        self.assertIsValid(BandAdmin, Band)\n\n"], "sample_103": ["    def test_complex_expression_on_aggregation(self):\n        qs = Publisher.objects.annotate(\n            price_or_median=Greatest(Avg('book__rating', output_field=DecimalField()),\n                                     Avg('book__price') * F('num_awards'))\n        ).filter(price_or_median__gte=F('num_awards')).order_by('num_awards')\n        self.assertQuerysetEqual(\n            qs, [1, 3, 7, 9], lambda v: v.num_awards)\n", "    def test_annotation_with_null_values(self):\n        # Handle cases where annotations might result in NULL values.\n        qs = Book.objects.annotate(price_divided_by_rating=F('price') / F('rating'))\n        self.assertQuerysetEqual(qs.filter(price_divided_by_rating__isnull=True),\n                                [self.b2], lambda b: b.pk)\n\n        self.assertQuerysetEqual(qs.filter(price_divided_by_rating__isnull=False),\n                                [self.b1, self.b3, self.b4, self.b5],\n                                lambda b: b.pk)\n", "    def test_filter_annotation(self):\n        qs = Book.objects.annotate(\n            price_gt_100=Case(When(price__gt=100, then=True), default=False),\n        ).filter(price_gt_100=True)\n\n        self.assertQuerysetEqual(qs, [self.b1, self.b3], lambda b: b.pk)\n\n", "    def test_annotate_with_aggregate_ordering(self):\n        qs = Book.objects.annotate(avg_rating=Avg('rating')).order_by('avg_rating')\n        self.assertEqual(list(qs.values_list('pk', flat=True)), [self.b2.pk, self.b1.pk, self.b3.pk, self.b4.pk])\n", "    def test_aggregate_with_aliased_subquery(self):\n        subquery = Book.objects.filter(\n            publisher=OuterRef('pk')\n        ).values('price').annotate(avg_price=Avg('price'))\n        qs = Publisher.objects.annotate(\n            avg_price_subquery=Subquery(subquery.values('avg_price'), output_field=DecimalField())\n        ).aggregate(avg_price_all=Avg(F('avg_price_subquery')))\n        self.assertEqual(qs['avg_price_all'], Approximate(Decimal('39.09'), places=2))\n"], "sample_538": ["compilation error", "compilation error", "def test_bbox_transformed():\n    bbox = mtransforms.Bbox([[0, 0], [1, 1]])\n    transform = mtransforms.Affine2D().scale(2, 3)\n    transformed_bbox = bbox.transformed(transform)\n    expected_bbox = mtransforms.Bbox([[0, 0], [2, 3]])\n    assert_bbox_eq(transformed_bbox, expected_bbox)\n", "    def test_transform_non_affine_then_affine():\n        ax = plt.axes()\n        offset = mtransforms.Affine2D().translate(10, 10)\n        na_offset = NonAffineForTest(mtransforms.Affine2D().translate(10, 10))\n        plt.plot(np.arange(10), transform=offset + na_offset + ax.transData)\n        expected_data_lim = np.array([[0., 0.], [9.,  9.]]) + 20\n        assert_array_almost_equal(ax.dataLim.get_points(), expected_data_lim)\n        # Now apply an affine transform on top\n        ax.transData = offset + ax.transData\n        expected_data_lim = np.array([[0., 0.], [9.,  9.]]) + 30\n        assert_array_almost_equal(ax.dataLim.get_points(), expected_data_lim)\n\n", "compilation error"], "sample_1144": ["compilation error", "def test_split_super_sub_complex():\n    assert split_super_sub(\"x_a^b_c__d\") == (\"x\", [\"b\", \"d\"], [\"a\", \"c\"])\n    assert split_super_sub(\"x__a__b_c_d\") == (\"x\", [\"a\", \"b\"], [\"c\", \"d\"])\n", "compilation error", "def test_split_super_sub_empty():\n    assert split_super_sub(\"\") == (\"\", [], [])\n", "def test_split_super_sub_empty():\n    assert split_super_sub('') == ('', [], [])\n"], "sample_449": ["    def test_connections_override(self):\n        class TestApp:\n                self.connections = connections\n                start_response(\"200 OK\", [])\n                yield (f'{self.connections.values()}').encode()\n\n        # Use a database connection alias\n        connections = {'default': 'in-memory'}\n        server = ThreadedWSGIServer((\"localhost\", 0), WSGIRequestHandler,\n                                    connections_override=connections)\n        server.set_app(TestApp(connections))\n        rfile = BytesIO(b\"GET / HTTP/1.1\\r\\nHost: localhost\\r\\n\\r\\n\")\n        wfile = UnclosableBytesIO()\n\n            if mode == \"rb\":\n                return rfile\n            elif mode == \"wb\":\n                return wfile\n\n        request = Stub(makefile=makefile)\n        with self.assertLogs(\"django.server\", \"INFO\") as cm:\n            WSGIRequestHandler(request, \"192.168.0.2\", server)\n        wfile.seek(0)\n        lines = list(wfile.readlines())\n        self.assertIn(b\"['in-memory']\", lines[-1])\n\n", "    def test_threadedserver_connections(self):\n        \"\"\"Test that ThreadedWSGIServer can handle multiple connections.\"\"\"\n        # Use a simple WSGI app that returns the number of times it's been called.\n        counter = 0\n\n            nonlocal counter\n            counter += 1\n            start_response(\"200 OK\", [])\n            return [str(counter).encode()]\n\n        server = ThreadedWSGIServer((\"localhost\", 0), WSGIRequestHandler)\n        server.set_app(test_app)\n        server._thread_count = 2\n        server_thread = threading.Thread(target=server.serve_forever)\n        server_thread.start()\n\n        # Make two requests concurrently.\n        request1 = self.request_factory.get('/')\n        response1 = self.client.get(server.server_address)\n        request2 = self.request_factory.get('/')\n        response2 = self.client.get(server.server_address)\n\n        server.shutdown()\n        server_thread.join()\n\n        # Both responses should contain \"1\", indicating they were handled by separate threads.\n        self.assertEqual(response1.content, b\"1\")\n        self.assertEqual(response2.content, b\"2\")\n", "    def test_close_connection_after_response(self):\n        \"\"\"WSGIServer closes connection after response completion.\"\"\"\n            start_response(\"200 OK\", [])\n            yield b\"Hello World\"\n\n        rfile = BytesIO(b\"GET / HTTP/1.0\\r\\nHost: localhost\\r\\n\\r\\n\")\n        rfile.seek(0)\n\n        wfile = UnclosableBytesIO()\n\n            if mode == \"rb\":\n                return rfile\n            elif mode == \"wb\":\n                return wfile\n\n        request = Stub(makefile=makefile)\n        server = Stub(base_environ={}, get_app=lambda: test_app)\n\n        with self.assertLogs(\"django.server\", \"INFO\") as cm:\n            WSGIRequestHandler(request, \"192.168.0.2\", server)\n\n        self.assertIn(\"Connection: close\\r\\n\", wfile.getvalue().decode())\n", "    def test_connections_override(self):\n            connections['test_alias']\n            start_response(\"200 OK\", [])\n            yield b\"\"\n\n        rfile = BytesIO()\n        rfile.write(b\"GET / HTTP/1.0\\r\\n\")\n        rfile.write(b\"Host: example.com\\r\\n\")\n        rfile.write(b\"\\r\\n\")\n        rfile.seek(0)\n\n        wfile = UnclosableBytesIO()\n\n            if mode == \"rb\":\n                return rfile\n            elif mode == \"wb\":\n                return wfile\n\n        request = Stub(makefile=makefile)\n        # Mock connections\n        connections['test_alias'] = 'mock connection'\n\n        server = Stub(base_environ={}, get_app=lambda: test_app)\n        server.connections_override = {'test_alias': 'mock connection'}\n        # Prevent logging from appearing in test output.\n        with self.assertLogs(\"django.server\", \"INFO\"):\n            WSGIRequestHandler(request, \"192.168.0.2\", server)\n", "    def test_connections_override(self):\n        \"\"\"\n        ThreadedWSGIServer should be able to override database connections\n        for each thread.\n        \"\"\"\n        from django.db import connections\n\n            alias = \"default\"\n            start_response(\"200 OK\", [])\n            return [connections[alias].settings_dict['NAME'].encode()]\n\n        rfile = BytesIO(b\"GET / HTTP/1.0\\r\\n\")\n        rfile.seek(0)\n\n        wfile = UnclosableBytesIO()\n\n            if mode == \"rb\":\n                return rfile\n            elif mode == \"wb\":\n                return wfile\n\n        request = Stub(makefile=makefile)\n        server = Stub(get_app=lambda: test_app, connections_override={\"default\": connections[\"default\"]})\n\n        # Prevent logging from appearing in test output.\n        with self.assertLogs(\"django.server\", \"INFO\"):\n            WSGIRequestHandler(request, \"192.168.0.2\", server)\n\n        wfile.seek(0)\n        body = list(wfile.readlines())[-1].decode()\n\n        # The body is returned in a GET response.\n        self.assertEqual(body, connections['default'].settings_dict['NAME'])\n"], "sample_849": ["        def fit(self, X, y, groups=None):\n            self.fitted = True\n            self.X = X\n            self.y = y\n            self.groups = groups\n            return self\n", "compilation error", "compilation error", "compilation error", "def test_group_kfold_single_group():\n    X = np.ones(10)\n    y = np.ones(10)\n    groups = np.array([0] * 10)\n    lkf = GroupKFold(n_splits=5)\n    splits = list(lkf.split(X, y, groups))\n    assert len(splits) == 1\n\n"], "sample_248": ["    def test_shell_with_no_interface(self, mock_ipython, mock_bpython):\n        mock_ipython.start_ipython.side_effect = ImportError(\"Not Installed\")\n        mock_bpython.embed.side_effect = ImportError(\"Not Installed\")\n        with self.assertRaisesMessage(CommandError, \"Couldn't import ipython interface.\"):\n            call_command('shell')\n", "    def test_python_startup(self):\n        with mock.patch('builtins.open', side_effect=IOError(\"Test error\")), captured_stdout() as stdout:\n            call_command('shell')\n        self.assertIn('Exception', stdout.getvalue())\n", "    def test_pythonrc_execution(self, os_mock, select):\n        os_mock.environ = {'PYTHONSTARTUP': 'print(\"PYTHONSTARTUP\")'}\n        os_mock.path.expanduser.return_value = '/home/user/.pythonrc.py'\n        with captured_stdout() as stdout:\n            call_command('shell')\n        self.assertEqual(stdout.getvalue().strip(), 'PYTHONSTARTUP')\n\n", "    def test_shell_with_pythonstartup(self, select):\n        select.return_value = ([], [], [])\n        with captured_stdout() as stdout:\n            call_command('shell')\n        self.assertEqual(stdout.getvalue().strip(), 'PYTHONSTARTUP')\n", "    def test_no_startup_option(self):\n        with captured_stdout() as stdout:\n            call_command('shell', interface='python', no_startup=True, command='print(__file__)')\n        self.assertEqual(stdout.getvalue().strip(), os.path.abspath(__file__))\n\n"], "sample_1127": ["def test_isomorphisms():\n    G = SymmetricGroup(3)\n    H = PermutationGroup(Permutation(0, 1, 2), Permutation(0, 2))\n    assert G.is_isomorphic(H)\n", "def test_is_subgroup():\n    G = SymmetricGroup(5)\n    H = AlternatingGroup(5)\n    assert H.is_subgroup(G)\n    K = PermutationGroup([Permutation(1,2)])\n    assert K.is_subgroup(G)\n    assert not G.is_subgroup(H)\n    L = PermutationGroup([Permutation(0,1,2)(3,4)])\n    assert L.is_subgroup(G)\n\n", "compilation error", "    def test_is_solvable():\n        G = SymmetricGroup(4)\n        assert G.is_solvable is False\n\n        G = AlternatingGroup(4)\n        assert G.is_solvable is True\n\n        G = DihedralGroup(8)\n        assert G.is_solvable is True\n\n        G = PermutationGroup([Permutation(0,1,2), Permutation(0,2)])\n        assert G.is_solvable is True\n\n        G = PermutationGroup([Permutation(0,1,2)(3,4), Permutation(0,2,4)])\n        assert G.is_solvable is True\n", "def test_centralizer():\n    G = SymmetricGroup(4)\n    a = Permutation(0, 1)\n    C = G.centralizer(a)\n    assert C.order() == 4\n    b = Permutation(0, 1, 2)\n    C = G.centralizer(b)\n    assert C.order() == 3 \n"], "sample_117": ["    def test_invalid_password(self):\n        user = User.objects.get(username='testclient')\n        data = {'password1': 'p', 'password2': 'p'}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertIn(\"This password is too short. It needs to be at least 8 characters.\", form.errors['password1'])\n        self.assertIn(\"This password is too short. It needs to be at least 8 characters.\", form.errors['password2'])\n", "    def test_password_validation_messages(self):\n        user = User.objects.get(username='testclient')\n        data = {'password1': 'short', 'password2': 'short'}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.error_messages['password1'], ['Ensure this field has at least 12 characters.'])\n        self.assertEqual(form.error_messages['password2'], ['Ensure this field has at least 12 characters.'])\n", "    def test_empty_password_field(self):\n        user = User.objects.get(username='testclient')\n        form = AdminPasswordChangeForm(user, {})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data.get('password1'), None)\n        self.assertEqual(form.cleaned_data.get('password2'), None)\n", "    def test_password_validation(self):\n        user = User.objects.get(username='testclient')\n        data = {\n            'password1': 'short',\n            'password2': 'short',\n        }\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertIn('password1', form.errors)\n        self.assertIn('password2', form.errors)\n", "    def test_custom_validation(self):\n        class CustomAdminPasswordChangeForm(AdminPasswordChangeForm):\n                password2 = self.cleaned_data['password2']\n                if 'bad' in password2:\n                    raise ValidationError(_(\"Password cannot contain 'bad'.\"))\n                return password2\n\n        user = User.objects.get(username='testclient')\n        data = {'password1': 'testbad123', 'password2': 'testbad123'}\n        form = CustomAdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['password2'], [_(\"Password cannot contain 'bad'.\" )])\n"], "sample_207": ["    def test_key_transform_expression_with_alias(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.annotate(\n                key=KeyTransform('d', 'value'),\n                aliased_key=F('key'),\n            ).filter(aliased_key__0='e'),\n            [self.objs[4]],\n        )\n", "    def test_length_lookup(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__length=0),\n            [self.objs[2]],\n            )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__a__length=1),\n            [self.objs[3], self.objs[4]],\n            )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__foo__length__gt=3),\n            [self.objs[7]],\n            )\n", "    def test_double_nesting_key_transform_raw_expression(self):\n        expr = RawSQL(self.raw_sql, ['{\"x\": {\"y\": {\"z\": \"bar\"}}}'])\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__foo=KeyTransform('z', KeyTransform('y', KeyTransform('x', expr)))),\n            [self.objs[7]],\n        )\n", "    def test_lookups_with_queryset_transform(self):\n        qs = NullableJSONModel.objects.filter(value__d__contains='e')\n        self.assertIs(\n            NullableJSONModel.objects.filter(\n                value__has_key=KeyTransform('k', qs.annotate(k=KeyTransform('1', 'value'))),\n            ).exists(),\n            True,\n        )\n", "    def test_key_transform_with_subquery(self):\n        subquery = NullableJSONModel.objects.filter(value__a='b').values('pk')\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(pk__in=subquery).annotate(\n                key=KeyTransform('c', 'value'),\n            ).filter(key=14),\n            [self.objs[3]],\n        )\n"], "sample_880": ["compilation error", "def test_unique_labels_empty_sparse():\n    \"\"\"Test unique_labels with an empty sparse matrix.\"\"\"\n    y_sparse = sp.csr_matrix((0, 4))\n    assert_array_equal(unique_labels(y_sparse), np.array([]))\n", "def test_unique_labels_empty():\n    # Test unique_labels with empty input\n    with pytest.raises(ValueError):\n        unique_labels([])\n", "compilation error", "def test_type_of_target_empty():\n    assert type_of_target([]) == \"unknown\"\n    assert type_of_target(np.array([])) == \"unknown\"\n    assert type_of_target(csr_matrix(([], ([], [])), shape=(0, 0))) == \"unknown\"\n"], "sample_833": ["    def test_tol_convergence(self, tol):\n        # Test that decreasing the tolerance for solver='saga' results in a\n        # smaller duality gap\n        X, y = make_classification(n_samples=100, n_features=20, random_state=0)\n        clf1 = LogisticRegression(penalty='l2', tol=tol, solver='saga',\n                    max_iter=1000, random_state=0).fit(X, y)\n\n        clf2 = LogisticRegression(penalty='l2', tol=tol * 0.1, solver='saga',\n                    max_iter=1000, random_state=0).fit(X, y)\n\n        assert clf1.n_iter_ < clf2.n_iter_\n        assert clf1.score(X, y) == clf2.score(X, y)\n", "    def test_logistic_regression_positive_class_weight():\n        # Check if LogisticRegression handles positive class weights correctly\n        X, y = make_classification(n_classes=2, weights=[0.2, 0.8],\n                                   random_state=0)\n        # Create a classifier with class weight for the minority class\n        lr = LogisticRegression(class_weight={0: 2.5}, random_state=0)\n        lr.fit(X, y)\n        # Check if the number of support vectors for each class is balanced\n        n_sv_0 = np.sum(lr.coef_[0] != 0)\n        n_sv_1 = np.sum(lr.coef_[1] != 0)\n        assert n_sv_0 < n_sv_1\n", "    def test_class_weight_balanced_multiclass(self, penalty):\n        # Check that class_weight='balanced' gives the same result as manual\n        # weighting for multiclass classification.\n        X, y = make_classification(n_samples=100, n_classes=3,\n                                   random_state=0)\n        unique_classes, counts = np.unique(y, return_counts=True)\n        class_weights = {c: 1 / count for c, count in zip(unique_classes,\n                                                         counts)}\n\n        lr_balanced = LogisticRegression(penalty=penalty,\n                                        class_weight='balanced',\n                                        random_state=0)\n        lr_manual_weights = LogisticRegression(penalty=penalty,\n                                               class_weight=class_weights,\n                                               random_state=0)\n        lr_balanced.fit(X, y)\n        lr_manual_weights.fit(X, y)\n\n        assert_array_almost_equal(lr_balanced.coef_, lr_manual_weights.coef_)\n", "compilation error", "def test_logistic_regression_lbfgs_bounded():\n    # Make sure that L-BFGS solver works with bounds\n\n    X, y = make_classification(n_samples=100, n_features=10, random_state=0)\n    lr = LogisticRegression(solver='lbfgs', bounds=(-10, 10),\n                            penalty='l2', C=1, random_state=0)\n    lr.fit(X, y)\n    assert np.all(lr.coef_ >= -10)\n    assert np.all(lr.coef_ <= 10)\n"], "sample_770": ["compilation error", "def test_silhouette_samples_empty_cluster():\n    X = np.array([[0, 0], [1, 1], [2, 2]])\n    labels = np.array([0, 1, 0])\n    silhouette_samples_values = silhouette_samples(X, labels)\n\n    assert np.isnan(silhouette_samples_values[0])\n    assert np.isnan(silhouette_samples_values[2])\n", "compilation error", "def test_silhouette_with_empty_cluster():\n    X = [[0, 0], [1, 1], [2, 2], [3, 3], [4, 4]]\n    labels = [0, 0, 1, 1, 2]\n    assert_raises_regexp(ValueError, \"Number of labels is\",\n                         silhouette_score, X, labels)\n", "def test_silhouette_samples_empty_clusters():\n    # Test that silhouette_samples handles empty clusters correctly\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    labels = np.array([0, 1, 0])\n    with pytest.warns(UserWarning):\n        silhouette_samples(X, labels)\n"], "sample_59": ["    def test_related_objects_manager(self):\n        \"\"\"\n        Regression test for #13640: related objects manager should be available\n        \"\"\"\n        dept = Department.objects.create(pk=1, name='abc')\n        Worker.objects.create(department=dept, name='worker1')\n        Worker.objects.create(department=dept, name='worker2')\n\n        self.assertEqual(dept.worker_set.count(), 2)\n\n", "    def test_ordering_with_double_underscore(self):\n        \"\"\"\n        Test ordering with double underscores in the field name.\n        Regression test for #28787\n        \"\"\"\n        event = Event.objects.create(when=datetime.datetime.now())\n        event.extra_info.create(data='some_data')\n        with self.assertRaisesMessage(FieldDoesNotExist, \"Cannot resolve keyword 'extra_info__data' into field. Choices are: id\"):\n            Event.objects.order_by('extra_info__data')\n", "    def test_model_with_evaluate_method_filter(self):\n        dept = Department.objects.create(pk=1, name=\"abc\")\n        dept.evaluate = 'abc'\n        Worker.objects.create(department=dept, name=\"worker1\")\n        self.assertQuerysetEqual(Worker.objects.filter(department__evaluate='abc'), [Worker(name=\"worker1\", department=dept)])\n\n\n", "    def test_model_with_evaluate_method_and_pk_filter(self):\n        \"\"\"\n        You can filter by objects that have 'evaluate' attr and a pk filter\n        \"\"\"\n        dept = Department.objects.create(pk=1, name='abc')\n        dept.evaluate = 'abc'\n        Worker.objects.filter(department=dept, pk=1)\n", "    def test_model_with_evaluate_method_filter_by_evaluate(self):\n        dept = Department.objects.create(pk=1, name='abc')\n        dept.evaluate = 'abc'\n        Worker.objects.create(department=dept)\n        self.assertEqual(Worker.objects.filter(department__evaluate='abc').count(), 1)\n"], "sample_1104": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_697": ["    def test_tmp_path_factory_keeps_basetemp_when_config_basetemp_is_set(\n        tmp_path, monkeypatch", "def test_cleanup_numbered_dir_keeps_latest(self, tmp_path):\n        for i in range(5):\n            make_numbered_dir(root=tmp_path, prefix=self.PREFIX)\n        cleanup_numbered_dir(\n            root=tmp_path,\n            prefix=self.PREFIX,\n            keep=3,\n            consider_lock_dead_if_created_before=0,\n        )\n        dirs = sorted([p for p in tmp_path.iterdir() if p.is_dir() and p.name.startswith(self.PREFIX)])\n        assert len(dirs) == 3\n        assert dirs[0].name == self.PREFIX + \"2\"\n        assert dirs[1].name == self.PREFIX + \"3\"\n        assert dirs[2].name == self.PREFIX + \"4\"\n", "    def test_numbered_dir_with_existing_locked_dir(self, tmp_path):\n        p = tmp_path / \"test-0\"\n        p.mkdir()\n        create_cleanup_lock(p)\n        with pytest.raises(OSError, match=\"cannot create lockfile in .*\"):\n            make_numbered_dir(root=tmp_path, prefix=\"test-\")\n", "    def test_tmp_path_with_reserved_characters(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import os\n                path_with_reserved = tmp_path / \"file:name\"\n                path_with_reserved.write_text(\"hello world\")\n                assert path_with_reserved.exists()\n                assert path_with_reserved.read_text() == \"hello world\"\n        \"\"\"\n        )\n        result = pytester.runpytest()\n        assert result.ret == 0\n\n", "    def test_tmp_path_factory_nested_basetemp(\n        pytester: Pytester, tmp_path_factory: TempPathFactory"], "sample_238": ["    def test_aggregation_null_values(self):\n        qs = Book.objects.annotate(\n            my_null=NullValues('price'),\n        ).filter(my_null=True)\n        self.assertEqual(qs.count(), 0)\n\n", "    def test_aggregation_with_isnull_filter_and_subquery(self):\n        \"\"\"\n        Test that aggregation works correctly with an IS NULL filter\n        applied to a subquery.\n        \"\"\"\n        qs = Book.objects.annotate(\n            has_author_with_same_age=Exists(Author.objects.filter(\n                age=OuterRef('contact__age'),\n            ))\n        ).filter(has_author_with_same_age=True).aggregate(\n            total_books=Count('pk')\n        )\n        self.assertEqual(qs['total_books'], 8)\n", "    def test_aggregation_with_empty_result(self):\n        with self.assertRaises(EmptyResult):\n            Book.objects.filter(isbn='nonexistent').aggregate(Avg('rating'))\n", "    def test_aggregation_subquery_annotation_in_values(self):\n        \"\"\"Subquery annotations are included in the GROUP BY when used in VALUES.\"\"\"\n        latest_book_pubdate_qs = Book.objects.filter(\n            publisher=OuterRef('pk')\n        ).order_by('-pubdate').values('pubdate')[:1]\n        publisher_qs = Publisher.objects.annotate(\n            latest_book_pubdate=Subquery(latest_book_pubdate_qs),\n        ).values('name', 'latest_book_pubdate')\n        self.assertQuerysetEqual(\n            publisher_qs, [\n                {'name': 'Apress', 'latest_book_pubdate': datetime.date(2007, 12, 1)},\n                {'name': 'Morgan Kaufmann', 'latest_book_pubdate': datetime.date(\n                    2008, 1, 1)},\n                {'name': \"Jonno's House of Books\", 'latest_book_pubdate': None},\n                {'name': 'Prentice Hall', 'latest_book_pubdate': datetime.date(\n                    2009, 12, 1)},\n                {'name': 'Sams', 'latest_book_pubdate': datetime.date(2010, 1, 1)},\n\n            ],\n            lambda v: (v['name'], v['latest_book_pubdate'])\n        )\n\n", "    def test_aggregation_empty_subquery_annotation(self):\n        subquery = Book.objects.filter(id=OuterRef('author')).values('rating').annotate(\n            count=Count('*'),\n        ).filter(count=0)\n        author_qs = Author.objects.annotate(\n            empty_rating=Subquery(subquery),\n        ).filter(empty_rating__isnull=False)\n        self.assertEqual(len(author_qs), 0)\n"], "sample_1151": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_393": ["    def test_exclude_pattern_with_locale(self):\n        management.call_command(\n            \"makemessages\", locale=['en'], exclude_pattern=['templates/excluded_dir/*'], verbosity=0\n        )\n        self.assertRecentlyModified(self.PO_FILE % \"en\")\n        self.assertNotRecentlyModified(os.path.join(self.test_dir, \"locale\", 'en', 'LC_MESSAGES', 'django.po'))\n\n", "    def test_ignore_pattern_with_special_characters(self):\n        out, po_contents = self._run_makemessages(\n            ignore_patterns=[\n                \"templates/ignore-*.html\",\n            ]\n        )\n        self.assertIn(\"ignoring file templates/ignore-foo.html\", out)\n        self.assertMsgId(\"This should be included.\", po_contents)\n        self.assertNotMsgId(\"This should be ignored.\", po_contents)\n", "    def test_ignore_files_pattern(self):\n        out, po_contents = self._run_makemessages(\n            ignore_patterns=[\"templates/ignore_*.html\"]\n        )\n        self.assertIn(\"ignoring file templates/ignore_test.html\", out)\n        self.assertNotMsgId(\"This should be ignored.\", po_contents)\n", "    def test_translatable_string_in_comment(self):\n        \"\"\"Comment containing translatable string should be ignored.\"\"\"\n        with open(\"templates/test.html\", \"w\") as f:\n            f.write(\n                \"<!-- Translators: This is a translatable string in a comment -->\"\n            )\n        management.call_command(\"makemessages\", locale=[LOCALE], verbosity=0)\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n        self.assertNotMsgId(\n            \"This is a translatable string in a comment\", po_contents\n        )\n", "    def test_keep_pot_with_locales(self):\n        management.call_command(\n            \"makemessages\", locale=[LOCALE], verbosity=0, keep_pot=True\n        )\n        self.assertTrue(os.path.exists(self.POT_FILE))\n        self.assertTrue(os.path.exists(self.PO_FILE))\n"], "sample_857": ["    def test_predict_proba_one_sample(self):\n        # Test predict_proba on a single sample with a multiclass classifier.\n        clf = DecisionTreeClassifier(random_state=0)\n        clf.fit(X, y_multilabel)\n        proba = clf.predict_proba(X[0].reshape(1, -1))\n        assert proba.shape == (1, 3)\n        assert_almost_equal(np.sum(proba), 1.0)\n", "    def test_decision_tree_fit_empty_data(self):\n        # Test fit on empty data\n        for cls in ALL_TREES.values():\n            x = np.array([]).reshape((0, 1))\n            y = np.array([])\n\n            est = cls()\n\n            with pytest.raises(ValueError, match=\"Sample weight array needs to \"):\n                est.fit(x, y, sample_weight=np.array([1]))\n            \n            with pytest.raises(ValueError, match=\"Input data is empty\"):\n                est.fit(x, y)\n", "    def test_tree_weights(self):\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([0, 1, 0])\n        SampleWeights = np.array([0.1, 0.5, 0.4])\n\n        tree = DecisionTreeClassifier()\n        tree.fit(X, y, sample_weight=SampleWeights)\n\n        # Check if the weights are correctly incorporated\n        # into the node counts and impurity calculations.\n\n        # Note: This test relies on the internal structure\n        # of the DecisionTreeClassifier, which might change\n        # in future versions.\n\n        assert tree.tree_.weighted_n_node_samples[0] == 1.0\n        assert tree.tree_.weighted_n_node_samples[1] == 0.1\n        assert tree.tree_.weighted_n_node_samples[2] == 0.5\n\n        assert tree.tree_.weighted_n_node_samples[3] == 0.4\n\n\n", "    def test_predict_proba_consistency(self):\n        # Test consistency between predict_proba and predict for classifiers\n        X = iris.data\n        y = iris.target\n        clf = DecisionTreeClassifier(random_state=0)\n        clf.fit(X, y)\n\n        y_proba = clf.predict_proba(X)\n        y_pred = clf.predict(X)\n\n        assert y_proba.shape[0] == len(X)\n        assert y_proba.shape[1] == len(np.unique(y))\n\n        for i in range(len(X)):\n            predicted_class = np.argmax(y_proba[i])\n            assert predicted_class == y_pred[i]\n", "def test_max_depth_parameter():\n    X = iris.data\n    y = iris.target\n\n    # Test with max_depth=1\n    dt = DecisionTreeClassifier(max_depth=1, random_state=0).fit(X, y)\n    assert dt.tree_.max_depth == 1\n    # Check predictions\n    y_pred = dt.predict(X)\n    assert len(np.unique(y_pred)) <= 2  # Should have at most 2 unique predictions\n\n    # Test with max_depth=None (should go to full depth)\n    dt = DecisionTreeClassifier(max_depth=None, random_state=0).fit(X, y)\n    assert dt.tree_.max_depth == len(dt.tree_.feature)\n    y_pred = dt.predict(X)\n    assert len(np.unique(y_pred)) == len(np.unique(y))\n\n"], "sample_179": ["    def test_unique_constraint_with_default_ordering(self):\n        class Model(models.Model):\n            name = models.CharField(max_length=100)\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['name'],\n                        name='unique_name',\n                        ordering=['name'],\n                    ),\n                ]\n\n        errors = Model.check(databases=self.databases)\n        expected = [] if connection.features.supports_unique_constraints_ordering else [\n            Warning(\n                '%s does not support ordering for unique constraints.'\n                % connection.display_name,\n                hint=\n                \"A constraint won't be created. Silence this warning if you don't care about it.\",\n                obj=Model,\n                id='models.W037',\n            ),\n        ]\n        self.assertEqual(errors, expected)\n\n", "    def test_exclusive_constraint(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n            name = models.CharField(max_length=100)\n\n            class Meta:\n                constraints = [\n                    models.ExclusiveConstraint(\n                        fields=['age', 'name'],\n                        name='exclusive_age_name',\n                    ),\n                ]\n\n        errors = Model.check(databases=self.databases)\n        expected = [] if connection.features.supports_exclusive_constraints else [\n            Warning(\n                '%s does not support exclusive constraints.'\n                % connection.display_name,\n                hint=(\n                    \"A constraint won't be created. Silence this warning if \"\n                    \"you don't care about it.\"\n                ),\n                obj=Model,\n                id='models.W039',\n            ),\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_unique_constraint_with_invalid_fields(self):\n        class Model(models.Model):\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(fields=['nonexistent_field']),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [\n            Error(\n                \"Field 'nonexistent_field' does not exist in model 'invalid_models_tests.Model'.\",\n                obj=Model,\n                id='models.E007',\n            ),\n        ])\n", "    def test_exclude_constraint(self):\n        class Model(models.Model):\n            name = models.CharField(max_length=100)\n            value = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.ExcludeConstraint(\n                        fields=['name', 'value'],\n                        name='exclude_name_value',\n                    ),\n                ]\n\n        errors = Model.check(databases=self.databases)\n        expected = [] if connection.features.supports_exclusion_constraints else [\n            Warning(\n                '%s does not support exclusion constraints.'\n                % connection.display_name,\n                hint=(\n                    \"A constraint won't be created. Silence this warning if \"\n                    \"you don't care about it.\"\n                ),\n                obj=Model,\n                id='models.W037',\n            ),\n        ]\n        self.assertEqual(errors, expected)\n\n", "    def test_exclude_constraint_with_condition(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.ExcludeConstraint(\n                        fields=['age'],\n                        name='exclude_age_lte_18',\n                        condition=models.Q(age__lte=18),\n                    ),\n                ]\n\n        errors = Model.check(databases=self.databases)\n        expected = [] if connection.features.supports_exclude_constraints else [\n            Warning(\n                '%s does not support exclude constraints.'\n                % connection.display_name,\n                hint=(\n                    \"A constraint won't be created. Silence this warning if \"\n                    \"you don't care about it.\"\n                ),\n                obj=Model,\n                id='models.W037',\n            ),\n        ]\n        self.assertEqual(errors, expected)\n\n"], "sample_689": ["    def test_warning_captured_hook_is_deprecated(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            test_warning_captured_hook=\"\"\"\n                pytest.warn(\"This is a warning.\")\n            \"\"\",\n        )\n        result = pytester.runpytest_subprocess(\"-p\", \"no:warning_captured\", test_warning_captured_hook)\n        result.stdout.fnmatch_lines(\n            [\n                \"*PytestDeprecationWarning: The pytest_warning_captured is deprecated and will be removed in a future release.*\",\n            ]\n        )\n", "def test_warning_captured_hook_is_deprecated(testdir: Testdir) -> None:\n    testdir.makepyfile(\n        \"\"\"\n            pytest.warning_captured(\"hello\")\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*The pytest_warning_captured is deprecated and will be removed in a future release.*\",\n        ]\n    )\n\n", "def test_warning_captured_hook_is_deprecated(testdir: Testdir) -> None:\n    testdir.makepyfile(\n        \"\"\"\n            pytest.warning_captured()\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"'pytest_warning_captured' is deprecated*\",\n            \"Please use pytest_warning_recorded instead.\",\n        ]\n    )\n", "def test_warning_captured_hook_is_deprecated(testdir: Testdir) -> None:\n    testdir.makepyfile(\n        \"\"\"\n            pytest.warning_captured()\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\"*The pytest_warning_captured is deprecated*\"]\n    )\n", "def test_warning_captured_hook_is_deprecated(testdir: Testdir) -> None:\n    testdir.makepyfile(\n        \"\"\"\n            assert 1\n        \"\"\"\n    )\n    with pytest.warns(pytest.PytestDeprecationWarning, match=\"pytest_warning_captured\"):\n        result = testdir.runpytest(\"-p\", \"pytest_warning_captured\")\n        result.stdout.fnmatch_lines([\"*pytest_warning_captured is deprecated*\"]) \n\n"], "sample_132": ["    def test_sensitive_variables_with_no_arguments(self):\n        @sensitive_variables\n            return 'success'\n        response = self.client.get('/sensitive_variables_with_no_arguments/')\n        self.assertContains(response, 'success', status_code=200)\n", "    def test_paranoid_view_with_non_html_response_encoding(self):\n        with self.settings(DEBUG=True):\n            response = self.client.get('/paranoid_raises500/', HTTP_ACCEPT='application/json')\n            self.assertEqual(response['Content-Type'], 'text/plain; charset=utf-8')\n", "    def test_paranoid_response_with_custom_exception_reporter_filter(self):\n        \"\"\"\n        No POST parameters and frame variables can be seen in the\n        default error reports for 'paranoid' requests, even when a custom\n        exception reporter filter is used.\n        \"\"\"\n        with self.settings(DEBUG=True):\n            self.verify_paranoid_response(paranoid_view_with_custom_filter, check_for_vars=False)\n\n        with self.settings(DEBUG=False):\n            self.verify_paranoid_response(paranoid_view_with_custom_filter, check_for_vars=False)\n\n", "    def test_sensitive_variables_with_kwargs(self):\n        \"\"\"\n        Test that sensitive_variables decorator correctly handles keyword arguments.\n        \"\"\"\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_args_function_caller, check_for_POST_params=False)\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_args_function_caller, check_for_POST_params=False)\n", "    def test_sensitive_variables_with_kwargs(self):\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_kwargs_view, check_for_vars=False)\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_kwargs_view, check_for_vars=False)\n"], "sample_921": ["def test_signature_from_str_complex_annotations_recursive():\n    sig = inspect.signature_from_str('(a: List[Tuple[int, str]]) -> None')\n    assert sig.parameters['a'].annotation == \"List[Tuple[int, str]]\"\n\n", "    def test_getdoc_inherited_class_method():\n        class Foo:\n            @classmethod\n                pass\n\n        class Bar(Foo):\n            pass\n\n        assert inspect.getdoc(Bar.meth, getattr, True, Bar, \"meth\") is not None\n\n\n", "def test_is_builtin_class_method_inheritance(app):\n    from target.inheritance import A, B, C\n\n    assert inspect.is_builtin_class_method(A, 'meth')\n    assert inspect.is_builtin_class_method(B, 'meth')\n    assert inspect.is_builtin_class_method(C, 'meth')\n", "    def test_signature_from_str_complex_annotations_pep604():\n        sig = inspect.signature_from_str('() -> Callable[[int, str], int] /')\n        assert sig.return_annotation == 'Callable[[int, str], int]'\n", "    def test_signature_from_str_annotations_with_forwardref():\n        signature = '(arg1: ForwardRef(\"mymodule.MyClass\"), arg2: List[int])'\n        sig = inspect.signature_from_str(signature)\n        assert sig.parameters['arg1'].annotation == 'ForwardRef(\"mymodule.MyClass\")'\n        assert sig.parameters['arg2'].annotation == 'List[int]'\n"], "sample_568": ["    def test_scatter3d_empty_input():\n        # Test that scatter3d can handle empty input arrays\n        fig = plt.figure()\n        ax = fig.add_subplot(projection='3d')\n        ax.scatter([], [], [])\n", "def test_Poly3DCollection_shade_none():\n    # smoke test to ensure shade=None works\n    # GH#26420\n    poly = np.array([[0, 0, 1], [0, 1, 1], [0, 0, 0]], float)\n    c = art3d.Poly3DCollection([poly], shade=None)\n", "def test_ax_sharex():\n    fig, axs = plt.subplots(2, 1, subplot_kw={'projection': '3d'})\n    axs[0].plot([1, 2, 3], [4, 5, 6], [7, 8, 9], label='Line 1')\n    axs[1].sharex(axs[0])\n    axs[1].plot([1, 2, 3], [10, 11, 12], [13, 14, 15], label='Line 2')\n    axs[0].set_xlabel('X Label')\n    axs[1].set_ylabel('Y Label')\n    axs[1].set_zlabel('Z Label')\n    fig.legend()\n\n", "def test_scatter_masked_zdir():\n    x = [1, 2, 3, 4]\n    y = [1, 2, 3, 4]\n    z = [1, 2, 3, np.nan]\n    mask = np.isfinite(z)\n\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    ax.scatter(x, y, z, zdir='y', c=mask)\n", "def test_scatter_masked_color_with_single_color():\n    \"\"\"\n    Test color parameter usage with non-finite coordinate arrays\n    and a single color provided.\n\n    GH#26236\n    \"\"\"\n\n    x = [np.nan, 1, 2,  1]\n    y = [0, np.inf, 2,  1]\n    z = [0, 1, -np.inf, 1]\n\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    path3d = ax.scatter(x, y, z, color='red')\n\n    # Assert sizes' equality\n    assert len(path3d.get_offsets()) ==\\\n           len(super(type(path3d), path3d).get_facecolors())\n"], "sample_23": ["def test_angle_comparison_quantity(cls):\n    \"\"\"\n    Regression test for issue #14836\n    \"\"\"\n    angle1 = cls(1, unit=\"deg\")\n    angle2 = 1 * u.deg\n    assert angle1 == angle2\n    assert angle2 == angle1\n    assert angle1 != angle2 + 1 * u.deg\n\n", "    def test_angle_pickle_to_string_with_unit_change():\n        \"\"\"\n        Regression test similar to test_angle_pickle_to_string, but with\n        a unit change from radian to degree after pickling.\n\n        Ensure that after pickling, unit change and to_string work.\n        Regression test for gh-13923.\n\n        \"\"\"\n        angle = Angle(0.5 * np.pi * u.rad)\n        expected = angle.to(u.deg).to_string()\n        via_pickle = pickle.loads(pickle.dumps(angle))\n        via_pickle_string = via_pickle.to(u.deg).to_string()\n        assert via_pickle_string == expected\n", "    def test_angle_copy(cls):\n        angle = cls([0, 45, 90], u.deg)\n        copied_angle = angle.copy()\n        assert angle is not copied_angle\n        assert np.all(angle == copied_angle)\n        angle[0] = 10\n        assert angle[0] != copied_angle[0]\n\n", "    def test_angle_wrapat_negative_angle(self):\n        a = Angle([-20, 150, 350] * u.deg)\n        assert np.all(a.wrap_at(-180* u.deg).degree == np.array([340.0, 150.0, -10.0]))\n", "compilation error"], "sample_1199": ["compilation error", "compilation error", "compilation error", "compilation error", "    def test_tensor_product_zero():\n        assert TensorProduct(0, A) == 0\n        assert TensorProduct(A, 0) == 0\n"], "sample_750": ["def test_omp_cv_n_jobs():\n    y_ = y[:, 0]\n    gamma_ = gamma[:, 0]\n    ompcv = OrthogonalMatchingPursuitCV(normalize=True, fit_intercept=False,\n                max_iter=10, cv=5, n_jobs=2)\n    ompcv.fit(X, y_)\n    assert_equal(ompcv.n_nonzero_coefs_, n_nonzero_coefs)\n    assert_array_almost_equal(ompcv.coef_, gamma_)\n", "def test_omp_fit_intercept_orthogonal():\n    # Test if fit_intercept correctly handles orthogonal data\n    rng = check_random_state(0)\n    n_samples, n_features = (20, 10)\n    X = rng.randn(n_samples, n_features)\n    # Make sure data is orthogonal\n    X = np.linalg.qr(X)[0]\n    y = rng.randn(n_samples)\n    omp_intercept = OrthogonalMatchingPursuit(fit_intercept=True)\n    omp_no_intercept = OrthogonalMatchingPursuit(fit_intercept=False)\n    omp_intercept.fit(X, y)\n    omp_no_intercept.fit(X, y)\n    assert_array_almost_equal(omp_intercept.coef_, omp_no_intercept.coef_)\n    assert_array_almost_equal(omp_intercept.intercept_, 0)\n", "def test_omp_max_iter():\n    # Tests if max_iter is respected\n    omp = OrthogonalMatchingPursuit(n_nonzero_coefs=10, max_iter=5)\n    omp.fit(X, y)\n    assert_true(np.count_nonzero(omp.coef_) <= 5)\n    # test with Gram matrix\n    omp_gram = OrthogonalMatchingPursuit(n_nonzero_coefs=10, max_iter=5,\n                                       precompute=True)\n    omp_gram.fit(X, y)\n    assert_true(np.count_nonzero(omp_gram.coef_) <= 5)\n\n", "    def test_omp_intercept():\n        # Test if the intercept is computed correctly.\n        X = np.array([[1, 2], [3, 4]])\n        y = np.array([5, 11])\n        omp = OrthogonalMatchingPursuit(fit_intercept=True)\n        omp.fit(X, y)\n        assert_array_almost_equal(omp.intercept_, 1)\n", "def test_omp_with_intercept():\n    # Test that the intercept is calculated correctly\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([3, 7, 11])\n\n    # Fit OMP with intercept\n    omp = OrthogonalMatchingPursuit(fit_intercept=True)\n    omp.fit(X, y)\n\n    # Expected intercept is 1 (y = intercept + X beta)\n    assert_array_almost_equal(omp.intercept_, 1)\n\n"], "sample_908": ["    def test_unparse_function_signature(source, expected):\n        module = ast.parse(source)\n        assert ast.unparse(module.body[0]) == expected\n", "    def test_unparse_arguments():\n        source = \"def func(a, b: int, *, c=1, d: str = 'hello', **kwargs): pass\"\n        expected = \"a, b: int, *, c=1, d: str = 'hello', **kwargs\"\n        module = ast.parse(source)\n        assert ast.unparse(module.body[0].args) == expected\n\n", "    def test_unparse_arguments():\n        source = \"def func(a, b: int, *args, c=1, d: str='hello', **kwargs): pass\"\n        expected = \"a, b: int, *args, c=1, d: str='hello', **kwargs\"\n        module = ast.parse(source)\n        assert ast.unparse(module.body[0].args) == expected\n", "    def test_unparse_arguments_posonlyargs():\n        source = \"def func(a, /, b, c=1, *, d): pass\"\n        expected = \"def func(a, /, b, c=1, *args, **kwargs): pass\"\n        module = ast.parse(source)\n        assert ast.unparse(module.body[0]) == expected\n", "    def test_unparse_arguments():\n        source = 'def func(a: int, b: str, *args: float, c: bool = True, d=None, **kwargs: str): pass'\n        module = ast.parse(source)\n        assert ast.unparse(module.body[0].args) == 'a: int, b: str, *args: float, c: bool = True, d=None, **kwargs: str'\n"], "sample_1040": ["    def test_print_Wild():\n        x = Symbol('x')\n        w = Wild('w')\n        assert mpp.doprint(w) == '<mi>w</mi>'\n        assert mp.doprint(w) == '<ci>w</ci>'\n        assert mpp.doprint(Eq(x, w)) == '<mrow><mi>x</mi><mo>=</mo><mi>w</mi></mrow>'\n        assert mp.doprint(Eq(x, w)) == '<apply><eq/><ci>x</ci><ci>w</ci></apply>'\n", "    def test_print_piecewise():\n        pw = Piecewise((x, x < 1), (x**2, x > 1))\n        assert mathml(pw, printer='presentation') == '<piecewise><piece><condition><lt/><ci>x</ci><cn>1</cn></lt></condition><ci>x</ci></piece><piece><condition><gt/><ci>x</ci><cn>1</cn></gt></condition><msup><ci>x</ci><cn>2</cn></msup></piece></piecewise>'\n        assert mathml(pw, printer='content') == '<piecewise><piece><condition><lt/><ci>x</ci><cn type=\"integer\">1</cn></lt></condition><ci>x</ci></piece><piece><condition><gt/><ci>x</ci><cn type=\"integer\">1</cn></gt></condition><apply><power/><ci>x</ci><cn type=\"integer\">2</cn></apply></piece></piecewise>'\n\n", "compilation error", "compilation error", "compilation error"], "sample_188": ["    def test_empty_output_field(self):\n        expr = ExpressionWrapper(Value(3))\n        with self.assertRaises(ValueError):\n            expr.get_source_expressions()\n", "    def test_expression_wrapper_deconstruct(self):\n        expr = ExpressionWrapper(F('field') + 1, output_field=IntegerField())\n        path, args, kwargs = expr.deconstruct()\n        self.assertEqual(path, 'django.db.models.expressions.ExpressionWrapper')\n        self.assertEqual(args, (F('field') + 1, IntegerField()))\n        self.assertEqual(kwargs, {})\n", "    def test_expression_wrapper_commutative(self):\n        wrapper1 = ExpressionWrapper(F('cost') + F('tax'), output_field=IntegerField())\n        wrapper2 = ExpressionWrapper(F('tax') + F('cost'), output_field=IntegerField())\n        self.assertEqual(wrapper1, wrapper2)\n", "    def test_expressionwrapper_output_field_type(self):\n        expr = ExpressionWrapper(Value('2023-01-01'), output_field=DateField())\n        self.assertEqual(expr.output_field.get_internal_type(), 'DateField')\n\n", "    def test_expressionwrapper_output_field(self):\n        expr = ExpressionWrapper(Value('name'), output_field=CharField())\n        self.assertEqual(expr.output_field.get_internal_type(), CharField().get_internal_type())\n"], "sample_638": ["def test_no_arguments_given(capsys):\n    \"\"\"Test that pyreverse prints the help message if no arguments are given.\"\"\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        main.Run([])\n    # Check that the right message is shown to the user\n    assert main.Run.__doc__ in capsys.readouterr().out\n    assert wrapped_sysexit.value.code == 1\n\n", "def test_run_with_empty_args(mock_writer, capsys):\n    \"\"\"Test that the correct help message is shown with no arguments.\"\"\"\n   \n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        main.Run([])\n\n    assert \"Usage: %prog [options] <packages>\" in capsys.readouterr().out\n    assert wrapped_sysexit.value.code == 1\n\n\n\n", "def test_no_args_given(mock_writer, capsys):\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        main.Run([])\n    output = capsys.readouterr().out\n    assert \"Usage: pyreverse [options] <packages>\" in output\n    assert wrapped_sysexit.value.code == 1\n\n", "def test_no_args_provided(capsys):\n    \"\"\"Test that pyreverse exits with error code 1 and shows help message when no arguments are provided.\"\"\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        main.Run([])\n    # Check that we exited with the expected error code\n    assert wrapped_sysexit.value.code == 1\n    stdout = capsys.readouterr().out\n    assert  \"Usage:\" in stdout # confirm help message is presented\n\n", "def test_run_with_no_arguments(capsys):\n    \"\"\"Test that pyreverse exits with an error when no arguments are provided.\"\"\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        main.Run([])\n    assert wrapped_sysexit.value.code == 1\n    captured_output = capsys.readouterr().out\n    assert \"Usage: %prog [options] <packages>\" in captured_output\n"], "sample_595": ["compilation error", "compilation error", "compilation error", "    def test_split_expand(dtype):\n        values = xr.DataArray([\"a,b,c\", \"d,e\", \"f\"]).astype(dtype)\n        result = values.str.split(\",\").str.get(1, None)\n        expected = xr.DataArray([\"b\", \"e\", None]).astype(dtype)\n        assert_equal(result, expected)\n\n", "compilation error"], "sample_975": ["compilation error", "    def test_nsolve_matrix():\n        x, y = symbols('x y')\n        sol = nsolve(Matrix((x**2 - 1, y**2 + 1)), (x, y), (1, 1), prec=128)\n        assert isinstance(sol, Matrix)\n        assert sol.shape == (2, 1)\n        assert abs(1 - sol[0]) < 1e-128\n        assert abs(-1*I - sol[1]) < 1e-128\n", "compilation error", "compilation error", "compilation error"], "sample_847": ["def test_enet_warm_start_alpha_change():\n    X, y, _, _ = build_dataset()\n    clf = ElasticNet(alpha=0.1, warm_start=True)\n    clf.fit(X, y)\n    initial_coef = clf.coef_.copy()\n    clf.set_params(alpha=0.5)\n    clf.fit(X, y)\n    assert_array_not_equal(initial_coef, clf.coef_)\n", "def test_enet_intercept_scaling():\n    # Test that intercept scaling works as expected for both ElasticNet and\n    # Lasso.\n\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([3, 7, 11])\n\n    # Fit ElasticNet and Lasso without intercept scaling\n\n    enet_no_scaling = ElasticNet(fit_intercept=True, intercept_scaling=False).fit(\n        X, y\n    )\n    lasso_no_scaling = Lasso(fit_intercept=True, intercept_scaling=False).fit(X, y)\n\n    # Fit ElasticNet and Lasso with intercept scaling\n\n    enet_scaling = ElasticNet(fit_intercept=True, intercept_scaling=True).fit(\n        X, y\n    )\n    lasso_scaling = Lasso(fit_intercept=True, intercept_scaling=True).fit(X, y)\n\n    # Assert that intercepts are different\n    assert not np.allclose(enet_no_scaling.intercept_, enet_scaling.intercept_)\n    assert not np.allclose(lasso_no_scaling.intercept_, lasso_scaling.intercept_)\n\n    # Assert that coefficients are the same\n    assert np.allclose(enet_no_scaling.coef_, enet_scaling.coef_)\n    assert np.allclose(lasso_no_scaling.coef_, lasso_scaling.coef_)\n\n", "compilation error", "compilation error", "def test_enet_path_positive_lasso():\n    # Test positive parameter for lasso_path\n\n    X, Y, _, _ = build_dataset(n_samples=50, n_features=50, n_targets=2)\n\n    # For mono output\n    pos_path_coef = lasso_path(X, Y[:, 0], positive=True)[1]\n    assert np.all(pos_path_coef >= 0)\n\n    # For multi output, positive parameter is not allowed\n    assert_raises(ValueError, lasso_path, X, Y, positive=True)\n\n"], "sample_966": ["def test_info_field_list_Union(app):\n    text = (\".. py:module:: example\\n\"\n            \".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   :param age: blah blah\\n\"\n            \"   :type age: Union[int, str]\\n\")\n    doctree = restructuredtext.parse(app, text)\n\n    assert_node(doctree,\n                (nodes.target,\n                 addnodes.index,\n                 addnodes.index,\n                 [desc, ([desc_signature, ([desc_annotation, (\"class\", desc_sig_space)],\n                                           [desc_addname, \"example.\"],\n                                           [desc_name, \"Class\"])],\n                         [desc_content, nodes.field_list, nodes.field, (nodes.field_name,\n                                                                        nodes.field_body)])]))\n    assert_node(doctree[3][1][0][0][1],\n                ([nodes.paragraph, ([addnodes.literal_strong, \"age\"],\n                                    \" (\",\n                                    [pending_xref, addnodes.literal_emphasis, \"Union\"],\n                                    [addnodes.literal_emphasis, \"[\"],\n                                    [pending_xref, addnodes.literal_emphasis, \"int\"],\n                                    [addnodes.literal_emphasis, \", \"],\n                                    [pending_xref, addnodes.literal_emphasis, \"str\"],\n                                    [addnodes.literal_emphasis, \"]\"],\n                                    \")\",\n                                    \" -- \",\n                                    \"blah blah\")],))\n    assert_node(doctree[3][1][0][0][1][0][2], pending_xref,\n                refdomain=\"py\", reftype=\"class\", reftarget=\"Union\",\n                **{\"py:module\": \"example\", \"py:class\": \"Class\"})\n    assert_node(doctree[3][1][0][0][1][0][4], pending_xref,\n                refdomain=\"py\", reftype=\"class\", reftarget=\"int\",\n                **{\"py:module\": \"example\", \"py:class\": \"Class\"})\n    assert_node(doctree[3][1][0][0][1][", "compilation error", "def test_python_domain_roles(app, status, warning):\n    app.builder.build_all()\n    out = (app.outdir / 'index.html').read_text()\n    assert ':ref:`foo.func_with_args`' in out\n    assert ':ref:`foo.func_with_args(arg)`' in out\n    assert ':ref:`foo.func_with_args(arg1, arg2)`' in out\n    assert ':ref:`foo.func_with_args(arg1, arg2, arg3)`' in out\n", "def test_duplicate_index_entries(app, warning):\n    text = (\".. py:function:: func1\\n\"\n            \"   :index: entry1\\n\"\n            \".. py:function:: func2\\n\"\n            \"   :index: entry1\\n\")\n    restructuredtext.parse(app, text)\n\n    domain = app.env.get_domain('py')\n    assert 'entry1' in domain.indices\n    assert domain.indices['entry1'] == (('single', 'entry1', 'entry1', '', None),)\n", "compilation error"], "sample_757": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_747": ["compilation error", "compilation error", "compilation error", "compilation error", "    def test_power_transformer_yeo_johnson(self):\n        X = np.array([[0, 25, 50, 75, 100],\n                      [2, 4, 6, 8, 10],\n                      [2.6, 4.1, 2.3, 9.5, 0.1]])\n        for standardize in [True, False]:\n            pt = PowerTransformer(method='yeo-johnson', standardize=standardize)\n            X_trans = pt.fit_transform(X)\n            X_trans_func = power_transform(X, method='yeo-johnson',\n                                           standardize=standardize)\n            assert_array_almost_equal(X_trans, X_trans_func)\n            X_inv = pt.inverse_transform(X_trans)\n            assert_array_almost_equal(X_inv, X)\n"], "sample_17": ["    def test_matrix_power_fail(self):\n        with pytest.raises(TypeError):\n            np.linalg.matrix_power(self.q, 1.5)\n", "    def test_merge_arrays_empty_array(self, flatten):\n        # Test merging with an empty array.\n        empty_array = np.array([], dtype=self.pv_dtype)\n        arr = rfn.merge_arrays((empty_array, self.q_pv), flatten=flatten)\n        assert_array_equal(arr[\"p\"], self.q_pv[\"p\"])\n", "    def test_add_mixed_units(self, q1, q2):\n        with pytest.raises(u.UnitsError):\n            q1 + q2\n", "    def test_einsum(self):\n        a = np.arange(10) << u.m\n        b = np.arange(5) << u.s\n\n        # Simple dot product\n        c = np.einsum('i,i->i', a, b, optimize=True)\n        expected = (a * b).sum(axis=0) << (u.m * u.s)\n        assert_array_equal(c, expected)\n        \n", "    def test_merge_arrays_structured_units(self, flatten):\n        # Create two structured arrays with compatible units.\n        dtype = np.dtype([('p', float), ('v', float)])\n        q1 = np.array([(1, 2), (3, 4)], dtype=dtype) * u.m\n        q2 = np.array([(5, 6), (7, 8)], dtype=dtype) * u.m\n\n        arr = rfn.merge_arrays((q1, q2), flatten=flatten)\n\n        if flatten:\n            assert arr.dtype == np.dtype([('p', float), ('v', float)])\n            assert_array_equal(arr['p'], np.array([1, 2, 3, 4, 5, 6, 7, 8]) * u.m)\n            assert_array_equal(arr['v'], np.array([2, 4, 6, 8, 6, 8, 8, 10]) * u.m)\n        else:\n            assert arr.shape == (2, 2)\n            assert_array_equal(arr['f0']['p'], [1, 3, 5, 7] * u.m)\n            assert_array_equal(arr['f0']['v'], [2, 4, 6, 8] * u.m)\n            assert_array_equal(arr['f1']['p'], [5, 7] * u.m)\n            assert_array_equal(arr['f1']['v'], [6, 8] * u.m)\n\n        assert arr.unit == u.m\n"], "sample_115": ["    def test_sensitive_variables_with_keyword_arguments(self):\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_kwargs_view, check_for_POST_params=False)\n            self.verify_unsafe_email(sensitive_kwargs_view, check_for_POST_params=False)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_kwargs_view, check_for_POST_params=False)\n            self.verify_safe_email(sensitive_kwargs_view, check_for_POST_params=False)\n", "    def test_sensitive_variables_with_kwargs(self):\n        with self.settings(DEBUG=True):\n            self.verify_safe_response(sensitive_kwargs_view, check_for_vars=False)\n", "    def test_sensitive_variables_with_args(self):\n        @sensitive_variables('password')\n            pass\n        self.assertDictEqual(\n            {'password': 'XXXXXXXXXXXXXXXXXXXX'},\n            test_func.sensitive_variables,\n\n        )\n", "    def test_sensitive_variables_with_arguments(self):\n        @sensitive_variables('password', 'secret')\n            # Some logic using the variables\n            return password + secret + other\n\n        with self.settings(DEBUG=True):\n            response = self.client.post('/sensitive/', data={'password': 'p', 'secret': 's', 'other': 'o'})\n            self.assertEqual(response.status_code, 500)\n            self.assertContains(response, \"password\")\n            self.assertNotContains(response, \"secret\")\n\n", "    def test_sensitive_variables_multiple_arguments(self):\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_variables_multiple_arguments_view)\n            self.verify_unsafe_email(sensitive_variables_multiple_arguments_view)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_variables_multiple_arguments_view)\n            self.verify_safe_email(sensitive_variables_multiple_arguments_view)\n"], "sample_1147": ["compilation error", "compilation error", "compilation error", "compilation error", "    def test_issue_19201():\n        from sympy.physics.quantum.state import Ket\n        from sympy.physics.quantum.qexpr import QExpr\n        q = Ket('q')\n        assert latex(q) == r'|q\\rangle'\n        class MyQExpr(QExpr):\n                return 'MyQExpr'\n\n        myq = MyQExpr()\n        assert latex(myq) == r'MyQExpr'\n"], "sample_380": ["    def test_aggregation_default_with_filter(self):\n        result = Book.objects.filter(rating__lt=3.0).aggregate(\n            value=Sum('price', default=Decimal('0.00'))\n        )\n        self.assertEqual(result['value'], Decimal('0.00'))\n", "    def test_aggregation_default_using_jsonfield_from_python(self):\n        data = {'key': 'value'}\n        result = Book.objects.filter(rating__gt=4.5).aggregate(\n            json_data=Sum('extra_data', default=data),\n        )\n        self.assertEqual(result['json_data'], data)\n", "    def test_aggregation_filter_with_coalesce(self):\n        result = Book.objects.annotate(avg_rating=Coalesce(Avg('book__rating'), 2.5)).filter(avg_rating__gt=3).values('isbn')\n        self.assertSequenceEqual(result, [\n            {'isbn': '013235613'},\n            {'isbn': '013790395',},\n            {'isbn': '155860191'},\n            {'isbn': '159059725'},\n        ])\n", "    def test_aggregation_default_with_complex_expression(self):\n        result = Publisher.objects.annotate(\n            avg_rating=Avg('book__rating', default=2.5),\n            books_with_pages_over_500=Count('book', filter=Q(book__pages__gt=500)),\n        ).aggregate(\n            avg_ratings_with_default=Avg('avg_rating', default=1.0),\n        )\n        self.assertEqual(result['avg_ratings_with_default'], Decimal('3.35'))\n", "    def test_aggregation_default_empty_result_set_subquery(self):\n        with self.assertNumQueries(1):\n            self.assertEqual(\n                Publisher.objects.none().annotate(\n                    sum_awards=Coalesce(Subquery(Book.objects.filter(\n                        publisher__in=OuterRef('pk')).aggregate(total_awards=Sum('price'))).values('total_awards'), 0),\n                )['sum_awards'],\n                0,\n            )\n"], "sample_924": ["    def check(spec, text):\n        pattern = '<li><p>%s<a .*?><code .*?><span .*?>%s</span></code></a></p></li>' % spec\n        res = re.search(pattern, text)\n        if not res:\n            print(\"Pattern\\n\\t%s\\nnot found in %s\" % (pattern, \"roles.html\"))\n            assert False", "    def check_warn():\n            pattern = '<li><p>%s<a .*?><code .*?><span .*?>%s</span></code></a></p></li>' % spec\n            res = re.search(pattern, text)\n            if not res:\n                print(\"Pattern\\n\\t%s\\nnot found in %s\" % (pattern, f))\n                assert False\n\n        f = 'any-role.html'\n        t = (app.outdir / f).read_text()\n        check(('warn', 'Sphinx'), t)\n", "def test_build_domain_cpp_backslash_ok_with_strip_signature_backslash_true(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"backslash\")\n    assert len(ws) == 0\n", "    def check(role, tag, text, file):\n        pattern = '<(?P<tag>{tag}) .*?class=[\"\\'](?P<classes>.*?)[\"\\'].*?>' \\\n                  r'.*?</(?P=tag)>'.format(tag=tag)\n        result = re.search(pattern, text)\n        expect = (\n            r'''Pattern for role '{role}' with tag '{tag}'", "compilation error"], "sample_186": ["    def test_modelform_exclude(self):\n        class MyModelForm(forms.ModelForm):\n            class Meta:\n                model = Song\n                exclude = ('title',)\n\n        class MyModelAdmin(admin.ModelAdmin):\n            form = MyModelForm\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        self.assertEqual(errors, [])\n", "    def test_list_editable_with_callable_method(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            list_editable = ['get_title']\n            \n                return obj.title\n        errors = MyModelAdmin(Book, AdminSite()).check()\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The value 'get_title' in 'list_editable' refers to a method \"\n                \"but should be a field name.\",\n                obj=MyModelAdmin,\n                id='admin.E124',\n            )\n        ])\n\n", "    def test_nested_fields_with_duplicates(self):\n        class NestedFieldsAdmin(admin.ModelAdmin):\n            fields = ('price', ('name', 'subtitle', 'name'))\n\n        errors = NestedFieldsAdmin(Book, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'fields' contains duplicate field(s).\",\n                obj=NestedFieldsAdmin,\n                id='admin.E006'\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_nested_inline_fieldsets(self):\n        class AuthorInline(admin.TabularInline):\n            model = Author\n\n        class BookAdmin(admin.ModelAdmin):\n            inlines = [AuthorInline]\n            fieldsets = (\n                ('Main', {'fields': ('name', ('authors',))}),\n            )\n\n        errors = BookAdmin(Book, AdminSite()).check()\n        self.assertEqual(errors, [])\n", "    def test_invalid_list_filter_field(self):\n        class SongAdminWithInvalidListFilter(admin.ModelAdmin):\n            list_filter = ['nonexistent_field']\n\n        errors = SongAdminWithInvalidListFilter(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'list_filter[0]' refers to 'nonexistent_field', \"\n                \"which is not an attribute of 'admin_checks.Song'.\",\n                obj=SongAdminWithInvalidListFilter,\n                id='admin.E036',\n            )\n        ]\n        self.assertEqual(errors, expected)\n"], "sample_397": ["    def test_invalid_loader_config(self):\n        with self.assertRaisesMessage(ImproperlyConfigured, \"Invalid value in template loaders configuration: 'invalid'\"):\n            DjangoTemplates({\"DIRS\": [], \"APP_DIRS\": False, \"NAME\": \"django\", \"OPTIONS\": {\"loaders\": \"invalid\"}})\n", "    def test_context_processors_override_by_template_context_processors(self):\n        \"\"\"\n        Ensure context processors defined in OPTIONS override those defined in settings.\n        \"\"\"\n        engine = DjangoTemplates(\n            {\n                \"DIRS\": [],\n                \"APP_DIRS\": False,\n                \"NAME\": \"django\",\n                \"OPTIONS\": {\n                    \"context_processors\": [test_processor_name],\n                },\n            }\n        )\n        template = engine.from_string(\"{{ processors }}\")\n        request = self.request_factory.get(\"/\")\n        content = template.render({}, request)\n        self.assertEqual(content, \"yes\")\n", "    def test_get_default(self):\n        with self.settings(TEMPLATES=[{'BACKEND': 'django.template.backends.django.DjangoTemplates'}]):\n            engine = Engine.get_default()\n            self.assertEqual(engine.name, 'django')\n", "    def test_invalid_loader_configuration(self):\n        with self.assertRaisesMessage(\n            ImproperlyConfigured,\n            \"Invalid value in template loaders configuration: \"\n            \"'django.template.loaders.invalid_loader'\",\n\n        ):\n            DjangoTemplates(\n                {\n                    \"DIRS\": [],\n                    \"APP_DIRS\": False,\n                    \"NAME\": \"django\",\n                    \"OPTIONS\": {\"loaders\": \"django.template.loaders.invalid_loader\"},\n                }\n            )\n", "    def test_context_processors_override(self):\n        engine = DjangoTemplates(\n            {\n                \"DIRS\": [],\n                \"APP_DIRS\": False,\n                \"NAME\": \"django\",\n                \"OPTIONS\": {\n                    \"context_processors\": [\n                        lambda request: {\"test\": \"overridden\"},\n                    ],\n                },\n            }\n        )\n\n        template = engine.from_string(\"{{ test }}\")\n\n        # Context processors run\n        content = template.render({}, self.request_factory.get(\"/\"))\n        self.assertEqual(content, \"overridden\")\n\n"], "sample_825": ["    def test_empty_input():\n        # Check if the code handles empty inputs gracefully\n        X = np.array([])\n        Y = np.array([])\n        pls = pls_.PLSCanonical()\n        assert_raises(ValueError, pls.fit, X, Y)\n\n", "    def test_n_components_auto():\n        d = load_linnerud()\n        X = d.data\n        Y = d.target\n        for clf in [pls_.PLSCanonical, pls_.PLSRegression, pls_.PLSSVD]:\n            pls = clf(n_components='auto')\n            pls.fit(X, Y)\n            assert isinstance(pls.n_components, int)  \n", "    def test_partial_fit():\n        d = load_linnerud()\n        X = d.data\n        Y = d.target\n        n_samples = X.shape[0]\n\n        pls = pls_.PLSRegression(n_components=2)\n\n        # Fit on half the data\n        pls.partial_fit(X[:n_samples // 2], Y[:n_samples // 2])\n\n        # Fit on the remaining data\n        pls.partial_fit(X[n_samples // 2:], Y[n_samples // 2:])\n\n        # Fit on the whole dataset at once\n        pls_full = pls_.PLSRegression(n_components=2)\n        pls_full.fit(X, Y)\n\n        assert_array_almost_equal(pls.x_scores_, pls_full.x_scores_)\n        assert_array_almost_equal(pls.y_scores_, pls_full.y_scores_)\n        assert_array_almost_equal(pls.x_weights_, pls_full.x_weights_)\n        assert_array_almost_equal(pls.y_weights_, pls_full.y_weights_)\n", "def test_pls_multioutput():\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n    # Test multi-output support by stacking the target\n    Y_stacked = np.hstack((Y, Y[:, None]))\n    clf = pls_.PLSRegression()\n    clf.fit(X, Y_stacked)\n    assert clf.y_weights_.shape[0] == Y_stacked.shape[1]\n", "    def test_n_components_validation(self):\n        d = load_linnerud()\n        X = d.data\n        Y = d.target\n        pls = pls_.PLSRegression()\n\n        with pytest.raises(ValueError, match=\"n_components must be a positive integer\"):\n            pls.fit(X, Y, n_components=0)\n\n        with pytest.raises(ValueError, match=\"n_components must be less than or equal to the minimum of\"):\n            pls.fit(X, Y, n_components=X.shape[1] + 1)\n"], "sample_706": ["def test_matching_with_none() -> None:\n    matcher = {\"true\": True, \"None\": None, \"false\": False}.__getitem__\n    assert evaluate(\"None\", matcher) is None\n    assert evaluate(\"true or None\", matcher) is True\n    assert evaluate(\"None or true\", matcher) is True\n    assert evaluate(\"None and true\", matcher) is None\n    assert evaluate(\"true and None\", matcher) is None\n    assert evaluate(\"None and false\", matcher) is None \n", "def test_dollar_prefix(expr: str, expected: bool) -> None:\n    matcher = {\"$true\": True, \"$false\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "    def test_matching_boolean_literals() -> None:\n        matcher = {\"True\": True, \"False\": False}.__getitem__\n        assert evaluate(\"True\", matcher)\n        assert not evaluate(\"False\", matcher)\n        assert evaluate(\"not False\", matcher)\n        assert not evaluate(\"not True\", matcher)\n", "def test_parentheses(expr: str) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is False\n\n", "def test_complex_expression() -> None:\n    matcher = {\"a\": True, \"b\": True, \"c\": False}.__getitem__\n    assert evaluate(\"a and (b or not c)\", matcher)\n"], "sample_35": ["    def test_isinstancemethod():\n        class MyClass:\n                pass\n\n            @classmethod\n                pass\n\n            @staticmethod\n                pass\n\n        assert introspection.isinstancemethod(MyClass, MyClass.instmethod)\n        assert not introspection.isinstancemethod(MyClass, MyClass.clsmethod)\n        assert not introspection.isinstancemethod(MyClass, MyClass.staticmethod)\n", "def test_isinstancemethod():\n    class MyClass:\n            return 'an instance method'\n\n        @classmethod\n            return 'a class method'\n\n        @staticmethod\n            return 'a static method'\n\n    obj = MyClass()\n    assert isinstancemethod(MyClass, obj.instance_method)\n    assert not isinstancemethod(MyClass, MyClass.class_method)\n    assert not isinstancemethod(MyClass, MyClass.static_method)\n", "def test_isinstancemethod():\n    class MetaClass(type):\n\n    class MyClass(metaclass=MetaClass):\n\n        @classmethod\n\n        @staticmethod\n    assert introspection.isinstancemethod(MyClass, MyClass.a_classmethod) is False\n    assert introspection.isinstancemethod(MyClass, MyClass.another_classmethod) is False\n    assert introspection.isinstancemethod(MyClass, MyClass.a_staticmethod) is False\n    assert introspection.isinstancemethod(MyClass, MyClass.an_instancemethod) is True\n", "def test_minversion_import():\n    test_module_name = 'astropy'\n    # Test version path\n    assert minversion(test_module_name, '4.0', version_path='version.minimum')\n    assert not minversion(test_module_name, '3.0', version_path='version.minimum')\n    # Test module import failure\n    with pytest.raises(ImportError):\n        minversion('fake_module', '1.0')\n\n", "def test_isinstancemethod():\n    class MyClass:\n            pass\n\n        @classmethod\n            pass\n        @staticmethod\n            pass\n\n    assert isinstancemethod(MyClass, MyClass.instance_method)\n    assert not isinstancemethod(MyClass, MyClass.class_method)\n    assert not isinstancemethod(MyClass, MyClass.static_method)\n"], "sample_483": ["    def test_filter_horizontal_nonexistent_field(self):\n        class MyAdmin(admin.ModelAdmin):\n            filter_horizontal = ('nonexistent',)\n\n        errors = MyAdmin(Album, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'filter_horizontal[0]' refers to 'nonexistent', which \"\n                \"is not a field of 'admin_checks.Album'.\",\n                obj=MyAdmin,\n                id=\"admin.E007\",\n            )\n        ]\n        self.assertEqual(errors, expected)\n\n", "    def test_valid_filter_horizontal_fields(self):\n        class ArtistAdmin(admin.ModelAdmin):\n            filter_horizontal = ('genres',)\n\n        errors = ArtistAdmin(Artist, AdminSite()).check()\n        self.assertEqual(errors, [])\n", "    def test_list_filter_nonexistent_field(self):\n        class SongAdminIncorrectlyFiltered(admin.ModelAdmin):\n            list_filter = [\"nonexistent_field\"]\n\n        errors = SongAdminIncorrectlyFiltered(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value 'nonexistent_field' is not a valid field for \"\n                \"list_filter.\",\n                obj=SongAdminIncorrectlyFiltered,\n                id=\"admin.E037\",\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_list_display_for_related_object(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            list_display = [\"related_object__field\"]\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        self.assertEqual(errors, [])\n", "    def test_fieldsets_duplicate_fields(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fieldsets = [\n                (None, {\"fields\": [\"title\", \"album\", \"title\"]}),\n            ]\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"Duplicate field 'title' found in 'fieldsets[0][1]['fields']'.\",\n                obj=MyModelAdmin,\n                id=\"admin.E010\",\n            )\n        ]\n        self.assertEqual(errors, expected)\n"], "sample_174": ["    def test_adapt_unknown_value_ipaddress(self):\n        value = '192.168.0.1'\n        self.assertEqual(self.ops.adapt_unknown_value(value), value)\n", "    def test_execute_sql_flush_statements_without_returning(self):\n        with transaction.atomic():\n            Author.objects.create(name='George Orwell')\n            Book.objects.create(author=Author.objects.get(name='George Orwell'))\n\n        sql_list = connection.ops.sql_flush(\n            no_style(),\n            [Author._meta.db_table, Book._meta.db_table],\n            reset_sequences=True,\n            allow_cascade=True,\n        )\n\n        # Ensure no RETURN clause is present in sql_list\n        for sql in sql_list:\n            self.assertNotIn('RETURNING', sql)\n\n        connection.ops.execute_sql_flush(sql_list)\n", "    def test_sql_flush_with_sequences(self):\n        with transaction.atomic():\n            author = Author.objects.create(name='George Orwell')\n            Book.objects.create(author=author)\n            author = Author.objects.create(name='Harper Lee')\n            Book.objects.create(author=author)\n            Book.objects.create(author=author)\n\n        original_author_pk = author.pk\n        original_book_pk = Book.objects.latest('pk').pk\n\n        sql_list = connection.ops.sql_flush(\n            no_style(),\n            [Author._meta.db_table, Book._meta.db_table],\n            reset_sequences=True,\n            allow_cascade=True,\n        )\n        connection.ops.execute_sql_flush(sql_list)\n\n        with transaction.atomic():\n            author = Author.objects.create(name='F. Scott Fitzgerald')\n            book = Book.objects.create(author=author)\n\n            self.assertEqual(author.pk, 1)\n            self.assertEqual(book.pk, 1)\n", "    def test_integer_field_range(self):\n        self.assertEqual(self.ops.integer_field_range('PositiveIntegerField'), (1, 2147483647))\n", "    def test_integer_field_range(self):\n        with self.assertRaisesMessage(NotImplementedError, self.may_require_msg % 'integer_field_range'):\n            self.ops.integer_field_range('PositiveIntegerField')\n"], "sample_997": ["    def test_issue_13132():\n        transformations = standard_transformations + (convert_equals_signs,)\n        x = Symbol('x')\n        y = Symbol('y')\n        assert parse_expr('x=y=1', transformations=transformations) == Eq(x, Eq(y, 1))\n", "    def test_issue_11669():\n        x = Symbol('x')\n        y = Symbol('y')\n        assert parse_expr('x**y', evaluate=False) == Pow(x, y, evaluate=False)\n        assert parse_expr('x**y', evaluate=False,\n                      local_dict={'x':x, 'y':y}) == Pow(x, y, evaluate=False)\n\n", "    def test_issue_12180():\n        assert parse_expr(\"x**y\") == Pow(Symbol(\"x\"), Symbol(\"y\"))\n\n", "    def test_implicit_application():\n        x = Symbol('x')\n\n        assert parse_expr(\"sin x\", transformations=standard_transformations +\n                          (implicit_application,)) == sin(x)\n        assert parse_expr(\"sin(x)\", transformations=standard_transformations +\n                          (implicit_application,)) == sin(x)\n        assert parse_expr(\"2x\", transformations=standard_transformations +\n                          (implicit_application,)) == 2*x\n\n        raises(SyntaxError, lambda: parse_expr('sin x + 1',\n                                               transformations=standard_transformations +\n                                               (implicit_application,)))\n\n", "    def test_convert_equality_operators():\n        transformations = standard_transformations + (convert_equality_operators,)\n        x = Symbol('x')\n        y = Symbol('y')\n        assert parse_expr(\"x == y\", transformations=transformations) == Eq(x, y)\n        assert parse_expr(\"x != y\", transformations=transformations) == NotEqual(x, y)\n        assert parse_expr(\"x >= y\", transformations=transformations) == Ge(x, y)\n        assert parse_expr(\"x <= y\", transformations=transformations) == Le(x, y)\n        assert parse_expr(\"x > y\", transformations=transformations) == Gt(x, y)\n        assert parse_expr(\"x < y\", transformations=transformations) == Lt(x, y)\n\n"], "sample_616": ["    def test_where_alignment(use_dask):\n        if use_dask:\n            if not has_dask:\n                pytest.skip(\"test for dask.\")\n        da = xr.DataArray(\n            np.arange(10), dims=\"x\", coords={\"x\": np.arange(10)}\n        )\n        other = xr.DataArray(\n            np.arange(5), dims=\"x\", coords={\"x\": np.arange(5, 10)}\n        )\n        if use_dask:\n            da = da.chunk()\n            other = other.chunk()\n\n        actual = xr.where(da > 5, other, da, keep_attrs=True)\n        expected = xr.DataArray(\n            np.where(np.arange(10) > 5, np.arange(5, 10), np.arange(10)),\n            dims=\"x\",\n            coords={\"x\": np.arange(10)},\n        )\n        xr.testing.assert_identical(expected, actual)\n", "def test_cross_errors(use_dask):\n    a = xr.DataArray([1, 2, 3], dims=[\"x\"])\n    b = xr.DataArray([4, 5], dims=[\"x\"])\n    if use_dask:\n        a = a.chunk()\n        b = b.chunk()\n    with pytest.raises(ValueError, match=\"Input arrays must have the same number\"):\n        xr.cross(a, b)\n    b = xr.DataArray([4, 5, 6, 7], dims=[\"x\"])\n    if use_dask:\n        b = b.chunk()\n    with pytest.raises(ValueError, match=\"Input arrays must have a common dimension\"):\n        xr.cross(a, b)\n", "    def test_cross_errors(self, use_dask):\n        a = xr.DataArray(np.arange(0, 3 * 4).reshape((3, 4)), dims=[\"x\", \"y\"])\n        b = xr.DataArray(np.arange(0, 4 * 5).reshape((4, 5)), dims=[\"y\", \"z\"])\n\n        if use_dask:\n            if not has_dask:\n                pytest.skip(\"test for dask.\")\n            a = a.chunk()\n            b = b.chunk()\n\n        with pytest.raises(ValueError, match=r\"Input arrays must have the same number\"):\n            xr.cross(a, b)\n        with pytest.raises(ValueError, match=r\"axis must be an integer\"):\n            xr.cross(a, b, axis=\"x\")\n\n", "    def test_cross_errors(use_dask: bool) -> None:\n        # Test cases where an error should be raised:\n\n        a = xr.DataArray([1, 2, 3], dims=[\"x\"])\n        b = xr.DataArray([4, 5], dims=[\"x\"])\n\n        if use_dask:\n            if not has_dask:\n                pytest.skip(\"test for dask.\")\n            a = a.chunk()\n            b = b.chunk()\n\n        with pytest.raises(ValueError, match=\"Arrays must have the same length\"):\n            xr.cross(a, b)\n\n        a = xr.DataArray([1, 2, 3], dims=[\"x\"])\n        b = xr.DataArray([4, 5, 6, 7], dims=[\"x\"])\n\n        with pytest.raises(ValueError, match=\"Arrays must have the same length\"):\n            xr.cross(a, b)\n", "    def test_cross_different_dims(use_dask):\n        if use_dask:\n            if not has_dask:\n                pytest.skip(\"Test for dask\")\n        a = xr.DataArray(\n            np.arange(0, 5 * 3 * 4).reshape((5, 3, 4)),\n            dims=[\"time\", \"cartesian\", \"var\"],\n            coords=dict(\n                time=([\"time\"], np.arange(0, 5)),\n                cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"]),\n                var=([\"var\"], [1, 1.5, 2, 2.5]),\n            ),\n        ).chunk({\"time\": 2})\n        b = xr.DataArray(\n            np.arange(0, 10 * 2 * 4).reshape((10, 2, 4)),\n            dims=[\"time\", \"another_cartesian\", \"var\"],\n            coords=dict(\n                time=([\"time\"], np.arange(0, 10)),\n                another_cartesian=([\"another_cartesian\"], [\"x\", \"y\"]),\n                var=([\"var\"], [1, 1.5, 2, 2.5]),\n            ),\n        ).chunk({\"time\": 2})\n\n        with pytest.raises(ValueError):\n            xr.cross(a, b, dim=\"cartesian\")\n"], "sample_587": ["    def test_merge_overwrite_vars(self):\n        data = create_test_data()\n        ds1 = data[[\"var1\", \"var2\"]]\n        ds2 = data[[\"var2\", \"var3\"]]\n\n        expected = data[[\"var1\", \"var2\", \"var3\"]]\n        actual = ds1.merge(ds2, overwrite_vars=\"var2\")\n        assert expected.identical(actual)\n\n        actual = ds2.merge(ds1, overwrite_vars=[\"var2\"])\n        assert expected.identical(actual)\n\n        with pytest.raises(ValueError):\n            # overwrite_vars should be a single variable name or a list of\n            ds1.merge(ds2, overwrite_vars=\"var2\", compat=\"no_conflicts\")\n", "    def test_merge_overwrite_vars(self):\n        ds1 = xr.Dataset({\"a\": 0, \"b\": 1})\n        ds2 = xr.Dataset({\"a\": 2, \"c\": 3})\n        expected = xr.Dataset({\"a\": 2, \"b\": 1, \"c\": 3})\n        assert expected.identical(ds1.merge(ds2, overwrite_vars=\"a\"))\n        assert expected.identical(ds1.merge(ds2, overwrite_vars=[\"a\"]))\n        with pytest.raises(ValueError):\n            ds1.merge(ds2, overwrite_vars=[\"a\", \"d\"])\n", "    def test_merge_empty_dataset(self):\n        ds1 = xr.Dataset()\n        ds2 = xr.Dataset({\"x\": ([], 0)})\n        actual = ds1.merge(ds2)\n        assert ds2.identical(actual)\n", "    def test_merge_overwrite_vars(self):\n        ds1 = xr.Dataset({\"a\": 0, \"b\": 1})\n        ds2 = xr.Dataset({\"a\": 2, \"c\": 3})\n        expected = xr.Dataset({\"a\": 2, \"b\": 1, \"c\": 3})\n        actual = ds1.merge(ds2, overwrite_vars={'a'})\n        assert expected.identical(actual)\n\n        expected = xr.Dataset({\"a\": 0, \"b\": 1, \"c\": 3})\n        actual = ds1.merge(ds2, overwrite_vars={'c'})\n        assert expected.identical(actual)\n\n        with pytest.raises(TypeError):\n            ds1.merge(ds2, overwrite_vars='a')\n\n", "    def test_merge_overwrite_vars(self):\n        data = create_test_data()\n        ds1 = data[[\"var1\", \"var2\"]]\n        ds2 = data[[\"var3\", \"var2\"]]\n        expected = data[[\"var1\", \"var3\", \"var2\"]]\n        actual = ds1.merge(ds2, overwrite_vars=\"var2\")\n        assert expected.identical(actual)\n\n        actual = ds2.merge(ds1, overwrite_vars=\"var2\")\n        assert expected.identical(actual)\n\n"], "sample_292": ["    def test_csrf_token_on_404_in_debug_mode_stays_constant(self):\n        with self.settings(DEBUG=True):\n            response = self.client.get('/does not exist/')\n            self.assertEqual(response.status_code, 404)\n            token1 = response.content.decode('ascii')\n            response = self.client.get('/does not exist/')\n            self.assertEqual(response.status_code, 404)\n            token2 = response.content.decode('ascii')\n            self.assertTrue(equivalent_tokens(token1, token2))\n", "    def test_csrf_token_on_500_stays_constant(self):\n        response = self.client.get('/error/')\n        self.assertEqual(response.status_code, 500)\n        token1 = response.content\n        response = self.client.get('/error/')\n        self.assertEqual(response.status_code, 500)\n        token2 = response.content\n        self.assertTrue(equivalent_tokens(token1.decode('ascii'), token2.decode('ascii')))\n", "    def test_csrf_token_on_500_stays_constant(self):\n        response = self.client.get('/error500/')\n\n        self.assertEqual(response.status_code, 500)\n        token1 = response.content\n        response = self.client.get('/error500/')\n        self.assertEqual(response.status_code, 500)\n        token2 = response.content\n        self.assertTrue(equivalent_tokens(token1.decode('ascii'), token2.decode('ascii')))\n", "    def test_csrf_token_on_500_stays_constant(self):\n        response = self.client.get('/error/')\n        # The error handler returns status code 500.\n        self.assertEqual(response.status_code, 500)\n        token1 = response.content\n        response = self.client.get('/error/')\n        self.assertEqual(response.status_code, 500)\n        token2 = response.content\n        self.assertTrue(equivalent_tokens(token1.decode('ascii'), token2.decode('ascii')))\n", "    def test_csrf_token_on_500_stays_constant(self):\n        response = self.client.get('/error/')\n        self.assertEqual(response.status_code, 500)\n        token1 = response.content\n        response = self.client.get('/error/')\n        self.assertEqual(response.status_code, 500)\n        token2 = response.content\n        self.assertTrue(equivalent_tokens(token1.decode('ascii'), token2.decode('ascii')))\n"], "sample_1050": ["def test_SciPyPrinter_print_assoc_legendre():\n    p = SciPyPrinter()\n    expr = assoc_legendre(2, 3, x)\n    assert p.doprint(expr) == 'scipy.special.lpmv(3, 2, x)'\n", "compilation error", "compilation error", "compilation error", "def test_pycode_complex():\n    assert pycode(2 + 3j) == 'complex(2, 3)'\n"], "sample_463": ["    def test_migration_autodetector_creates_empty_migration_for_no_changes(self):\n        before = []\n        after = []\n        changes = self.get_changes(before, after)\n        self.assertEqual(len(changes), 0)\n\n", "    def test_squashed_migration_with_operations(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\"Person\", fields=[]),\n                migrations.CreateModel(\"Animal\", fields=[]),\n                migrations.DeleteModel(\"Animal\"),\n            ]\n            squashed = True\n\n        migration = Migration(\"0002_squashed_0001_initial\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person\")\n\n", "    def test_unique_together(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    \"Author\",\n                    fields=[\n                        (\"id\", models.AutoField(primary_key=True)),\n                        (\"name\", models.CharField(max_length=200)),\n                        (\"age\", models.IntegerField()),\n                    ],\n                    options={\"unique_together\": {(\"name\", \"age\")}},\n                ),\n            ]\n\n        migration = Migration(\"some_migration\", \"testapp\")\n        self.assertEqual(migration.suggest_name(), \"author\")\n\n\n", "    def test_operation_with_custom_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    \"Person\",\n                    fields=[],\n                    name=\"create_person\",\n                ),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"create_person\")\n", "    def test_operation_with_invalid_chars_in_suggested_name_with_prefix(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.AddConstraint(\n                    \"Person\",\n                    models.UniqueConstraint(\n                        fields=[\"name\"], name=\"person.name-*~unique!\"\n                    ),\n                ),\n            ]\n\n        migration = Migration(\"0001_add_unique_constraint\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person_person_name_unique_\")\n\n"], "sample_438": ["    def test_get_prefetch_queryset_arguments_with_fields(self):\n        question = Question.objects.create(text=\"Who?\")\n        post = Post.objects.create(title=\"Answer\", parent=question)\n        prefetch_queryset = Answer.objects.select_related(\"question__content_type\")\n        with self.assertNumQueries(1):\n            post.question.get_prefetch_queryset(prefetch_queryset)\n\n", "    def test_empty_value_to_string(self):\n        question = Question.objects.create(text=\"test\")\n        result = json.loads(Question.answer_set.field.value_to_string(question))\n        self.assertEqual(result, [])\n", "    def test_get_prefetch_queryset_arguments_with_filter(self):\n        question = Question.objects.create(text=\"What is your name?\")\n        answer1 = Answer.objects.create(question=question)\n\n        with self.assertNumQueries(1):\n            answers = Answer.objects.prefetch_related(\n                \"question\",\n            ).filter(question=question)\n            self.assertEqual(len(answers), 1)\n", "    def test_get_related_objects_through_filter(self):\n        question = Question.objects.create(text=\"Who?\")\n        post1 = Post.objects.create(title=\"Answer1\", parent=question)\n        post2 = Post.objects.create(title=\"Answer2\", parent=question)\n        post3 = Post.objects.create(title=\"Unrelated\", parent=None)\n\n        related_posts = Question.objects.filter(\n            pk=question.pk\n        ).get_related_objects_through_filter(\"post_set\", Post.objects.filter(title__startswith=\"Answer\"))\n\n        self.assertCountEqual(related_posts, [post1, post2])\n", "    def test_get_prefetch_queryset_with_select_related(self):\n        question = Question.objects.create(text=\"What is your name?\")\n        answer = Answer.objects.create(text=\"Answer\", question=question)\n\n        with self.assertNumQueries(1):\n            answers = Answer.objects.select_related('question').all()\n            for answer in answers:\n                self.assertIsNotNone(answer.question)\n"], "sample_989": ["compilation error", "compilation error", "compilation error", "def test_Float_evalf():\n    x = Float('1.2345678901234567890')\n    assert x.evalf() == x\n    assert x.evalf(3) == Float('1.235')\n    assert x.evalf(5) == Float('1.23457')\n\n    assert Float(pi, 10).evalf() == Float(pi, 10)\n    assert Float(pi, 10).evalf(5) == Float('3.14159')\n\n", "compilation error"], "sample_760": ["def test_scorer_with_decision_function():\n    X, y = make_classification(random_state=0)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n    clf = SVC(probability=True, random_state=0)\n    clf.fit(X_train, y_train)\n\n    roc_auc_scorer = make_scorer(roc_auc_score)\n    score1 = roc_auc_scorer(clf, X_test, y_test)\n\n    # Ensure that the scorer correctly uses the decision_function\n    score2 = roc_auc_score(y_test, clf.decision_function(X_test))\n    assert_almost_equal(score1, score2)\n", "    def test_scoring_with_labels():\n        # Test that scoring works with labels and that it raises errors when\n        # labels are not provided when they are needed\n\n        X, y = make_blobs(random_state=0, centers=2)\n        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n        clf = LogisticRegression(random_state=0)\n        clf.fit(X_train, y_train)\n\n        # Test scoring with labels\n        score = get_scorer('accuracy')(clf, X_test, y_test)\n\n        # Test scoring raises error when labels are not provided\n        with pytest.raises(ValueError, match=\"Labels are required to compute\"):\n            get_scorer('accuracy')(clf, X_test)\n\n        # Test scoring with multi-output\n        X, y = make_multilabel_classification(random_state=0)\n        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n        clf.fit(X_train, y_train)\n        score = get_scorer('hamming_loss')(clf, X_test, y_test)\n\n        # Test scoring raises error when labels are not provided\n        with pytest.raises(ValueError, match=\"Labels are required to compute\"):\n            get_scorer('hamming_loss')(clf, X_test)\n\n", "def test_make_scorer_with_kwargs():\n    # Test that make_scorer can be used to pass keyword arguments to the\n    # scoring function.\n\n        return f1_score(y_true, y_pred, pos_label=pos_label,\n                       sample_weight=sample_weight)\n\n    X, y = make_blobs(random_state=0, centers=2)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n    clf = LinearSVC(random_state=0)\n    clf.fit(X_train, y_train)\n\n    scorer = make_scorer(custom_scorer, pos_label=1)\n    score1 = scorer(clf, X_test, y_test)\n    score2 = custom_scorer(y_test, clf.predict(X_test), pos_label=1)\n    assert_almost_equal(score1, score2)\n\n    # Test with sample_weight\n    sample_weight = np.ones_like(y_test)\n    sample_weight[:10] = 0\n    scorer = make_scorer(custom_scorer, pos_label=1)\n    score1 = scorer(clf, X_test, y_test, sample_weight=sample_weight)\n\n    score2 = custom_scorer(y_test, clf.predict(X_test), pos_label=1,\n                           sample_weight=sample_weight)\n    assert_almost_equal(score1, score2)\n", "def test_scoring_with_different_types():\n    X, y = make_classification(random_state=0)\n    clf = LogisticRegression()\n\n    # Test with a list of scorers\n    scorers = ['accuracy', 'f1_macro']\n    scores = cross_val_score(clf, X, y, scoring=scorers)\n    assert isinstance(scores, np.ndarray)\n    assert scores.shape == (2, )\n\n    # Test with a dictionary of scorers\n    scorers = {'accuracy': 'accuracy', 'f1': make_scorer(f1_score)}\n    scores = cross_val_score(clf, X, y, scoring=scorers)\n    assert isinstance(scores, dict)\n    assert len(scores) == 2\n", "def test_supervised_cluster_scorers_with_labels():\n    # Test clustering scorers against gold standard labeling.\n    X, y = make_blobs(random_state=0, centers=2)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n    km = KMeans(n_clusters=3).fit(X_train)\n    for name in CLUSTER_SCORERS:\n        score1 = get_scorer(name)(km, X_test, y_test)\n        score2 = getattr(cluster_module, name)(y_test, km.labels_)\n        assert_almost_equal(score1, score2)\n"], "sample_325": ["    def test_empty_data_files_multi_value_dict_with_post(self):\n        p = Person({'first_name': 'John', 'last_name': 'Lennon'})\n        self.assertIsInstance(p.data, MultiValueDict)\n        self.assertIsInstance(p.files, MultiValueDict)\n\n", "    def test_field_order(self):\n        class MyForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n            field3 = BooleanField()\n\n        f = MyForm()\n        self.assertEqual(list(f.fields.keys()), ['field1', 'field2', 'field3'])\n\n", "    def test_non_field_errors_as_text_without_html(self):\n        class MyForm(Form):\n            foo = CharField()\n\n                raise ValidationError('This is a non-field error')\n\n        form = MyForm({'foo': 'bar'})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.non_field_errors().as_text(), '* This is a non-field error')\n", "    def test_field_max_length_error_message(self):\n        class MyForm(Form):\n            name = CharField(max_length=10)\n        form = MyForm({'name': 'ThisIsTooLongName'})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['name'], ['Ensure this value has at most 10 characters (it has 18).'])\n\n", "    def test_renderer_as_context(self):\n        class CustomForm(Form):\n            default_renderer = CustomRenderer\n\n        form = CustomForm()\n        context = form.as_context()\n        self.assertEqual(context['renderer'], CustomForm.default_renderer)\n"], "sample_815": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_129": ["    def test_floatformat_empty_string(self):\n        self.assertEqual(floatformat(''), '')\n", "    def test_decimal_rounding(self):\n        self.assertEqual(floatformat(Decimal('1.2345'), 2), '1.23')\n        self.assertEqual(floatformat(Decimal('15.2042'), -3), '15.204')\n", "    def test_floatformat_with_context(self):\n        with localcontext() as ctx:\n            ctx.prec = 4\n            self.assertEqual(floatformat(1.23456789, 2), '1.23')\n            self.assertEqual(floatformat(15.20426789, -3), '15.204')\n", "    def test_floatformat_with_rounding_mode(self):\n        self.assertEqual(floatformat(1.55, 1, rounding='ROUND_HALF_UP'), '1.6')\n        self.assertEqual(floatformat(1.55, 1, rounding='ROUND_HALF_DOWN'), '1.5')\n        self.assertEqual(floatformat(1.45, 1, rounding='ROUND_HALF_UP'), '1.5')\n        self.assertEqual(floatformat(1.45, 1, rounding='ROUND_HALF_DOWN'), '1.4')\n        with self.assertRaises(ValueError):\n            floatformat(1.55, 1, rounding='InvalidRoundingMode')\n", "    def test_floatformat_with_separators(self):\n        self.assertEqual(floatformat(12345.6789, 2, thousand_sep=' '), '12 345.68')\n        self.assertEqual(floatformat(-12345.6789, 2, thousand_sep='_'), '-12_345.68')\n        self.assertEqual(floatformat(12345.6789, 2, decimal_sep=',' , thousand_sep='.' ), '12.345,68')\n"], "sample_860": ["def test_check_scalar_bounds_inclusive():\n    check_scalar(2, \"test_name\", int, min_val=2, max_val=4)\n    check_scalar(4, \"test_name\", int, min_val=2, max_val=4)\n", "    def test_check_array_dtype_object():", "compilation error", "def test_check_array_dtype_object():\n    X = np.array([[1, 2], [3, 4]], dtype=object)\n    X_checked = check_array(X, dtype=np.float64, force_all_finite=False)\n    assert_array_equal(X_checked, np.array([[1, 2], [3, 4]]))\n\n    X = np.array([['a', 2], [3, 'b']], dtype=object)\n    with pytest.raises(ValueError):\n        check_array(X, dtype=[np.int64, np.float64], force_all_finite=False)\n", "    def test_check_consistent_length_empty_list():\n        assert check_consistent_length([]) is None\n"], "sample_187": ["    def test_camel_case_to_spaces(self):\n        self.assertEqual(text.camel_case_to_spaces(\"simpleString\"), \"simple string\")\n        self.assertEqual(text.camel_case_to_spaces(\"ThisIsACamelCaseString\"), \"this is a camel case string\")\n        self.assertEqual(text.camel_case_to_spaces(\"already_spaced\"), \"already spaced\")\n        self.assertEqual(text.camel_case_to_spaces(\".\"), \".\")\n        self.assertEqual(text.camel_case_to_spaces(\"IDontKnowWhatThisIs\"), \"i dont know what this is\")\n\n", "    def test_camel_case_to_spaces(self):\n        self.assertEqual(text.camel_case_to_spaces('someCamelCase'), 'some camel case')\n        self.assertEqual(text.camel_case_to_spaces('SomeCamelCase'), 'some camel case')\n        self.assertEqual(text.camel_case_to_spaces('camelCase'), 'camel case')\n        self.assertEqual(text.camel_case_to_spaces('CamelCaseWord'), 'camel case word')\n        self.assertEqual(text.camel_case_to_spaces('already_spaced'), 'already spaced')\n        self.assertEqual(text.camel_case_to_spaces('some-kebab-case'), 'some kebab case')\n        self.assertEqual(text.camel_case_to_spaces(lazystr('someCamelCase')), 'some camel case')\n\n", "    def test_camel_case_to_spaces(self):\n        self.assertEqual(text.camel_case_to_spaces(\"someThing\"), \"some thing\")\n        self.assertEqual(text.camel_case_to_spaces(\"someThingElse\"), \"some thing else\")\n        self.assertEqual(text.camel_case_to_spaces(\"getHTTPResponse\"), \"get http response\")\n        self.assertEqual(text.camel_case_to_spaces(\"HTTPResponse\"), \"http response\")\n        self.assertEqual(text.camel_case_to_spaces(\"HTTPResponseCode\"), \"http response code\")\n        self.assertEqual(text.camel_case_to_spaces(\"MultipleWords\"), \"multiple words\")\n        self.assertEqual(text.camel_case_to_spaces(\"already_spaced\"), \"already spaced\")\n        self.assertEqual(text.camel_case_to_spaces(\"Some1Thing\"), \"some1 thing\")\n\n        self.assertEqual(text.camel_case_to_spaces(lazystr(\"someThing\")), \"some thing\")\n", "    def test_camel_case_to_spaces(self):\n        items = [\n            ('ThisIsACamelCaseString', 'this is a camel case string'),\n            ('already_spaced', 'already spaced'),\n            ('someHTMLTagsAreCamelCase', 'some html tags are camel case'),\n            ('', ''),\n        ]\n        for value, output in items:\n            self.assertEqual(text.camel_case_to_spaces(value), output)\n            self.assertEqual(text.camel_case_to_spaces(lazystr(value)), output)\n", "    def test_camel_case_to_spaces(self):\n        self.assertEqual(text.camel_case_to_spaces('thisIsAString'), 'this is a string')\n        self.assertEqual(text.camel_case_to_spaces('ThisIsATest'), 'this is a test')\n        self.assertEqual(text.camel_case_to_spaces('APIKey'), 'api key')\n        self.assertEqual(text.camel_case_to_spaces('JSONData'), 'json data')\n        self.assertEqual(text.camel_case_to_spaces(''), '')\n        self.assertEqual(text.camel_case_to_spaces('already-spaced'), 'already spaced')\n\n"], "sample_678": ["def test_cleanup_candidates(tmp_path):\n    root = tmp_path\n    candidates = list(cleanup_candidates(root, \"test_\", 2))\n    assert len(candidates) == 0\n\n    for i in range(5):\n        (root / f\"test_{i}\").mkdir()\n    candidates = list(cleanup_candidates(root, \"test_\", 2))\n    assert len(candidates) == 3\n    assert all(c.name.startswith(\"test_\") for c in candidates)\n    assert all(0 <= int(c.name[5:]) <= 2 for c in candidates)\n", "def test_ensure_deletable_with_missing_lock(tmp_path):\n    \"\"\"ensure_deletable should return True if the lock file does not exist.\"\"\"\n    path = tmp_path / \"dir\"\n    path.mkdir()\n    assert ensure_deletable(path, consider_lock_dead_if_created_before=0)\n", "def test_ensure_deletable_with_permission_error(tmp_path, monkeypatch):\n    \"\"\"Ensure ensure_deletable handles OSError when removing a lock (specifically PermissionError)\n\n\n    by ignoring it and returning False.\n    \"\"\"\n    path = tmp_path / \"dir\"\n    path.mkdir()\n    lock = get_lock_path(path)\n    lock.touch()\n    mtime = lock.stat().st_mtime\n\n        raise OSError(210) from None\n\n    monkeypatch.setattr(Path, \"unlink\", raise_permission_error)\n\n    assert not ensure_deletable(path, consider_lock_dead_if_created_before=mtime + 30)\n    assert lock.is_file()\n\n", "def test_ensure_deletable_with_nonexistent_lock(tmp_path):\n    \"\"\"ensure_deletable should return True if the lock file doesn't exist.\"\"\"\n    path = tmp_path / \"dir\"\n    path.mkdir()\n    assert ensure_deletable(path, consider_lock_dead_if_created_before=0)\n", "def test_ensure_deletable_with_non_existent_lock(tmp_path):\n    \"\"\"ensure_deletable should handle the case where the lock file doesn't exist.\"\"\"\n    path = tmp_path / \"dir\"\n    path.mkdir()\n    assert ensure_deletable(path, consider_lock_dead_if_created_before=0)\n"], "sample_276": ["    def test_model_detail_view_missing_docstring(self):\n        response = self.client.get(reverse('django-admindocs-models-detail', args=['admin_docs', 'NoDocstringModel']))\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, '<h1>admin_docs.NoDocstringModel</h1>')\n        self.assertContains(response, '<p>DESCRIPTION</p>')\n        self.assertNotContains(response, '<p>Stores information about something.</p>')\n", "    def test_method_with_args_and_defaults(self):\n        \"\"\"\n        Methods with arguments that have default values should have those\n        values displayed.\n        \"\"\"\n        self.assertContains(self.response, \"<td>method_with_defaults, baz=&#x27;default&#x27;</td>\")\n\n", "    def test_model_detail_template_error(self):\n        # Check that when a template error occurs (e.g., missing field in a template), the\n        # admin docs view doesn't crash.\n        with mock.patch('django.template.loaders.app_directories.Loader.get_template') as mock_get_template:\n            mock_get_template.side_effect = TemplateDoesNotExist\n            response = self.client.get(reverse('django-admindocs-models-detail', args=['admin_docs', 'Person']))\n            self.assertIn('TemplateDoesNotExist', response.context['exception'])\n            self.assertEqual(response.status_code, 500)\n\n\n\n", "    def test_template_detail_missing_template(self):\n        response = self.client.get(reverse('django-admindocs-templates', args=['missing_template.html']))\n        self.assertContains(response, 'Template not found: missing_template.html', html=True)\n        self.assertEqual(response.status_code, 404)\n\n", "    def test_model_detail_with_custom_field(self):\n        class CustomModel(models.Model):\n            name = models.CharField(max_length=100)\n            custom_field = CustomField()\n\n        with patch('django.contrib.admindocs.views.get_readable_field_data_type') as mock_get_readable_field_data_type:\n            mock_get_readable_field_data_type.return_value = 'Custom Field Value'\n\n            response = self.client.get(reverse('django-admindocs-models-detail', args=['myapp', 'custommodel']))\n\n            self.assertContains(response, '<td>custom_field</td>')\n            self.assertContains(response, '<td>Custom Field Value</td>')\n"], "sample_1": ["def test_separable_ninputs_gt_1():\n    cm = sh1 & sh2 | map1 & p2\n    assert_allclose(is_separable(cm), np.array([False, False]))\n    assert_allclose(separability_matrix(cm), np.array([[True, True], [True, True]]))\n\n", "    def model_d(x, y):\n        return x * y\n", "def test_separable_compound_models():\n    cm = sh1 & sh2 | rot\n    assert is_separable(cm).all() == False\n    assert separability_matrix(cm).all() == False\n\n", "    def test_separability_matrix_with_input():\n        m = models.Shift(1) & models.Scale(2)\n        matrix = separability_matrix(m)\n        assert_allclose(matrix, np.array([[True, False]]))\n", "def test_separability_matrix_with_mapping():\n    m = Mapping((0, 1, 2, 3))\n    assert np.allclose(separability_matrix(m),\n                       np.array([[True, False, False, False],\n                                 [False, True, False, False],\n                                 [False, False, True, False],\n                                 [False, False, False, True]]))\n"], "sample_143": ["    def test_camel_case_to_spaces(self):\n        self.assertEqual(text.camel_case_to_spaces('ThisIsACamelCaseString'), 'this is a camel case string')\n        self.assertEqual(text.camel_case_to_spaces('ThisIs'), 'this is')\n        self.assertEqual(text.camel_case_to_spaces('HTML'), 'html')\n        self.assertEqual(text.camel_case_to_spaces('AlreadySpaced'), 'already spaced')\n\n", "    def test_camel_case_to_spaces(self):\n        self.assertEqual(text.camel_case_to_spaces('ABCDE'), 'abcde')\n        self.assertEqual(text.camel_case_to_spaces('ThisIsATest'), 'this is a test')\n        self.assertEqual(text.camel_case_to_spaces('thisIsAlreadySpaced'), 'this is already spaced')\n        self.assertEqual(text.camel_case_to_spaces('some-string-with-hyphens'), 'some string with hyphens')\n        self.assertEqual(text.camel_case_to_spaces('SomeMixedCaseWithNumbers123'), 'some mixed case with numbers 123')\n        self.assertEqual(text.camel_case_to_spaces(lazystr('ABCDE')), 'abcde')\n", "    def test_camel_case_to_spaces(self):\n        self.assertEqual(text.camel_case_to_spaces('ThisIsACamelCaseString'), 'this is a camel case string')\n        self.assertEqual(text.camel_case_to_spaces('thisIsAlreadySpaced'), 'this is already spaced')\n        self.assertEqual(text.camel_case_to_spaces('singleWord'), 'single word')\n", "    def test_camel_case_to_spaces(self):\n        self.assertEqual(text.camel_case_to_spaces('someText'), 'some text')\n        self.assertEqual(text.camel_case_to_spaces('someHTML'), 'some html')\n        self.assertEqual(text.camel_case_to_spaces('alreadySpaces'), 'already spaces')\n        self.assertEqual(text.camel_case_to_spaces('SomeTextWithNumbers123'), 'some text with numbers 123')\n        self.assertEqual(text.camel_case_to_spaces('aBCdef'), 'a b cdef')\n\n", "    def test_camel_case_to_spaces(self):\n        self.assertEqual(text.camel_case_to_spaces('someVariableName'), 'some variable name')\n        self.assertEqual(text.camel_case_to_spaces('someVariableNameWithNumbers123'), 'some variable name with numbers 123')\n        self.assertEqual(text.camel_case_to_spaces(''), '')\n        self.assertEqual(text.camel_case_to_spaces('alreadySpaced'), 'already spaced')\n        self.assertEqual(text.camel_case_to_spaces(lazystr('someVariableName')), 'some variable name')\n\n"], "sample_214": ["    def test_key_transform_with_conditional_expression(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                Q(value__has_key='a') & Q(value__f=F('value__a'))\n            ),\n            [self.objs[3], self.objs[4]],\n        )\n", "    def test_key_transform_nested_expression(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__has_key=KeyTransform('x', KeyTransform('a', 'value')),\n            ),\n            [self.objs[3]],\n        )\n", "    def test_lookup_with_key_transform_and_raw_sql(self):\n        expr = RawSQL(self.raw_sql, ['{\"x\": 1}'])\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__x__gt=KeyTransform('x', expr)),\n            [],\n        )\n\n", "    def test_lookup_with_nested_key_transform_and_multiple_arguments(self):\n        qs = NullableJSONModel.objects.filter(\n            value__has_key=KeyTransform('a', KeyTransform('baz', 'value')),\n            value__baz__a='x',\n        )\n        self.assertSequenceEqual(qs, [])\n", "    def test_multiple_key_transforms(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__bax__foo=KeyTransform('bar', KeyTransform('0', 'value')),\n            ).values_list('value__bax__foo'),\n            [('bar',)],\n        )\n\n"], "sample_881": ["    def test_top_k_accuracy_score_with_labels_multiclass(\n        y_true, y_score, k, labels, expected_score", "def test_top_k_accuracy_score_empty_labels():\n    y_true = [0, 1, 2]\n    y_score = np.array([[0.1, 0.2, 0.7], [0.5, 0.3, 0.2], [0.3, 0.6, 0.1]])\n    with pytest.raises(ValueError, match=\"Parameter 'labels' cannot be empty.\"):\n        top_k_accuracy_score(y_true, y_score, k=2, labels=[])\n", "compilation error", "    def test_label_ranking_average_precision_score_with_weights():\n        y_true = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n        y_score = np.array([[0.9, 0.5, 0.6], [0.2, 0.8, 0.1], [0.1, 0.3, 0.9]])\n        sample_weights = np.array([0.5, 1.0, 0.5])\n\n        # Without weights\n        score_no_weights = label_ranking_average_precision_score(y_true, y_score)\n\n        # With weights\n        score_with_weights = label_ranking_average_precision_score(\n            y_true, y_score, sample_weight=sample_weights\n        )\n\n        assert score_with_weights != score_no_weights\n", "    def test_top_k_accuracy_score_sparse_y_true():\n        # Test with sparse y_true\n        y_true = csr_matrix([[0, 1, 0, 0], [0, 0, 0, 1], [1, 0, 0, 0], [0, 0, 1, 0]])\n        y_score = np.array(\n            [\n                [0.4, 0.3, 0.2, 0.1],\n                [0.1, 0.3, 0.4, 0.2],\n                [0.4, 0.1, 0.2, 0.3],\n                [0.3, 0.2, 0.4, 0.1],\n            ]\n        )\n        score = top_k_accuracy_score(y_true, y_score, k=2)\n        assert score == pytest.approx(0.75)\n"], "sample_205": ["    def test_update_error_dict(self):\n        error_dict = {}\n        error = ValidationError({'field1': 'message'})\n        error.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['message']})\n        error_dict = {}\n        error = ValidationError(['message2', 'message3'])\n        error.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'__all__': ['message2', 'message3']})\n        error_dict = {}\n        error = ValidationError({'field1': ['message2', 'message3'], 'field2': 'message'})\n        error.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['message2', 'message3'], 'field2': ['message']})\n\n        error_dict = {'field1': ['message1']}\n        error = ValidationError({'field1': ['message2', 'message3']})\n        error.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['message1', 'message2', 'message3']})\n\n        error_dict = {'field1': ['message1']}\n        error = ValidationError([ValidationError('message2'), ValidationError('message3')])\n        error.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['message1'], '__all__': ['message2', 'message3']})\n", "    def test_update_error_dict(self):\n        error_dict = {}\n        error = ValidationError({'field1': ['E1', 'E2']})\n        error.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['E1', 'E2']})\n\n        error_dict = {}\n        error = ValidationError(['E1', 'E2'])\n        error.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'__all__': ['E1', 'E2']})\n        error_dict = {}\n        error.update_error_dict(error_dict)\n\n        error_dict = {'field1': ['E1']}\n        error = ValidationError({'field2': ['E2', 'E3']})\n        error.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['E1'], 'field2': ['E2', 'E3']})\n\n\n", "    def test_empty_messages(self):\n        error = ValidationError({})\n        self.assertEqual(list(error), [])\n        self.assertEqual(error.messages, [])\n        self.assertEqual(error.message_dict, {})\n\n\n", "    def test_update_error_dict(self):\n        error_dict = {}\n        error = ValidationError({'field1': 'error1'})\n        error.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['error1']})\n\n        error = ValidationError({'field2': 'error2'})\n        error.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['error1'], 'field2': ['error2']})\n\n        error = ValidationError({'field1': 'error1_2'})\n        error.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['error1', 'error1_2'], 'field2': ['error2']})\n\n        error = ValidationError('error3')\n        error.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['error1', 'error1_2'], 'field2': ['error2'], '__all__': ['error3']})\n", "    def test_update_error_dict(self):\n        error1 = ValidationError({'field1': ['E1', 'E2']})\n        error_dict = {}\n        error1.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['E1', 'E2']})\n        error2 = ValidationError({'field2': 'E3'})\n        error2.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['E1', 'E2'], 'field2': ['E3']})\n        error3 = ValidationError({'field1': 'E4'})\n        error3.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['E1', 'E2', 'E4'], 'field2': ['E3']})\n        error4 = ValidationError(\n            'error %(parm1)s %(parm2)s',\n            code='my_code1',\n            params={'parm1': 'val1', 'parm2': 'val2'},\n        )\n        error4.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['E1', 'E2', 'E4'], 'field2': ['E3'], '__all__': ['error val1 val2']})\n\n\n\n"], "sample_841": ["    def test_ridge_sparse_cg_n_iter(self):\n        # Test n_iter_ attribute for sparse_cg solver with Ridge\n        X, y = make_regression(n_samples=10, n_features=5, random_state=42)\n        ridge = Ridge(solver='sparse_cg', max_iter=5)\n        ridge.fit(X, y)\n        assert ridge.n_iter_ is not None\n", "    def test_ridge_sag_max_iter():\n        # Test max_iter with 'sag' solver\n    \n        n_samples, n_features = 50, 10\n        X, y = make_regression(n_samples=n_samples, n_features=n_features,\n                               random_state=0)\n    \n        # Test that max_iter is respected\n        ridge = Ridge(solver='sag', max_iter=5, random_state=0)\n        with pytest.warns(ConvergenceWarning):\n            ridge.fit(X, y)\n        assert ridge.n_iter_ < ridge.max_iter\n\n", "def test_ridge_regression_sample_weight_array_like(solver):\n    rng = np.random.RandomState(42)\n    n_samples, n_features = 10, 5\n    X = rng.randn(n_samples, n_features)\n    coef = rng.randn(n_features)\n    y = np.dot(X, coef)\n    alpha = 1.0\n\n    sample_weight = np.ones(n_samples)\n    sample_weight[::2] = 2  # check for non-uniform weights\n    \n    result_array = ridge_regression(X, y, alpha=alpha, solver=solver,\n                                   sample_weight=sample_weight)\n    result_list = ridge_regression(X, y, alpha=alpha, solver=solver,\n                                   sample_weight=list(sample_weight))\n    assert_allclose(result_array, result_list)\n", "    def test_ridge_regression_fortran_order(self):\n        # check that Fortran array are converted when using ridge_regression\n        X, y = make_regression(random_state=42)\n        # for the order of X and y to not be C-ordered arrays\n        X = np.asfortranarray(X)\n        X = X[::2, :]\n        y = y[::2]\n        ridge_regression(X, y, alpha=0.1)\n\n", "    def test_ridge_sparse_cg_with_X_fortran():\n        # check that Fortran array are converted when using Sparse CG solver\n        X, y = make_regression(random_state=42)\n        # for the order of X and y to not be C-ordered arrays\n        X = np.asfortranarray(X)\n        X = X[::2, :]\n        y = y[::2]\n        Ridge(solver='sparse_cg').fit(X, y)\n"], "sample_1112": ["def test_count_digits_base():\n    assert count_digits(0o77, 8) == {7: 2}\n    assert count_digits(0xA, 16) == {10: 1}\n", "    def test_digits_errors():\n        raises(ValueError, lambda: digits(35, 1))\n", "compilation error", "def test_count_digits_base_and_digits():\n    assert count_digits(255, 16) == {15: 2, 0xf: 1}\n    assert count_digits(255, 16, digits=4) == {0: 2, 15: 2}\n\n", "compilation error"], "sample_286": ["    def test_refresh_m2m_cache_cleared(self):\n        a = Article.objects.create(pub_date=datetime(2005, 7, 28))\n        a.authors.add(Author.objects.create(name='Alice'))\n        a.authors.add(Author.objects.create(name='Bob'))\n        # refresh_from_db() without fields=[...]\n        a1_prefetched = Article.objects.prefetch_related('authors').first()\n        self.assertCountEqual(a1_prefetched.authors.all(),\n                             Author.objects.filter(name__in=['Alice', 'Bob']))\n        a1_prefetched.authors.clear()\n        # Cache is stale.\n        self.assertCountEqual(a1_prefetched.authors.all(),\n                             Author.objects.filter(name__in=['Alice', 'Bob']))\n        a1_prefetched.refresh_from_db()\n        # Cache was cleared and new results are available.\n        self.assertCountEqual(a1_prefetched.authors.all(), [])\n        # refresh_from_db() with fields=[...]\n        a2_prefetched = Article.objects.prefetch_related('authors').first()\n        self.assertCountEqual(a2_prefetched.authors.all(), [])\n        Author.objects.create(name='Charlie').articles.add(a)\n        # Cache is stale.\n        self.assertCountEqual(a2_prefetched.authors.all(), [])\n        a2_prefetched.refresh_from_db(fields=['authors'])\n        # Cache was cleared and new results are available.\n        self.assertCountEqual(a2_prefetched.authors.all(),\n                             Author.objects.filter(name='Charlie'))\n", "    def test_refresh_fk_on_delete_cascade(self):\n        a = Article.objects.create(\n            headline='Parrot programs in Python',\n            pub_date=datetime(2005, 7, 28),\n        )\n        s1 = SelfRef.objects.create(article=a)\n        a.delete()\n        with self.assertRaises(ObjectDoesNotExist):\n            s1.refresh_from_db()\n", "    def test_refresh_invalid_fk(self):\n        a = Article.objects.create(\n            headline='Parrot programs in Python',\n            pub_date=datetime(2005, 7, 28),\n        )\n        s1 = SelfRef.objects.create(article=a)\n        a.delete()\n        with self.assertRaisesMessage(django.db.utils.IntegrityError,\n                                     \"FOREIGN KEY constraint failed\"):\n            s1.refresh_from_db()\n", "    def test_refresh_m2m(self):\n        a1 = Article.objects.create(headline='Article 1', pub_date=datetime.now())\n        a2 = Article.objects.create(headline='Article 2', pub_date=datetime.now())\n        t1 = Tag.objects.create(name='python')\n        t2 = Tag.objects.create(name='django')\n        a1.tags.add(t1, t2)\n        a2.tags.add(t1)\n\n        a1_refreshed = Article.objects.get(pk=a1.pk)\n        with self.assertNumQueries(1):\n            a1_refreshed.refresh_from_db()\n        self.assertEqual(sorted([t.name for t in a1_refreshed.tags.all()]), ['django', 'python'])\n\n        # Update tags for a1 outside the ORM\n        with connection.cursor() as cursor:\n            cursor.execute(f\"DELETE FROM {a1._meta.db_table}_tags WHERE article_id = {a1.id}\")\n            cursor.execute(f\"INSERT INTO {a1._meta.db_table}_tags (article_id, tag_id) VALUES (%s, %s)\", (a1.id, t2.id))\n        \n        with self.assertNumQueries(1):\n            a1_refreshed.refresh_from_db()\n        self.assertEqual(sorted([t.name for t in a1_refreshed.tags.all()]), ['django'])\n", "    def test_refresh_m2m_field(self):\n        a = Article.objects.create(pub_date=datetime.now())\n        t1 = Tag.objects.create(name='django')\n        t2 = Tag.objects.create(name='python')\n        a.tags.add(t1, t2)\n        with self.assertNumQueries(1):\n            a.refresh_from_db(fields=['tags'])\n        self.assertEqual(a.tags.count(), 2)\n"], "sample_976": ["def test_symbols_exclude():\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n\n    W = Wild('W', exclude=[x])\n    assert W.match(y) is not None\n    assert W.match(z) is not None\n    assert W.match(x) is None\n\n    W = Wild('W', exclude=[x, y])\n    assert W.match(z) is not None\n    assert W.match(x) is None\n    assert W.match(y) is None\n", "    def sym(s):\n        return str(symbols(s))", "def test_symbols_kwargs():\n    a, b = symbols('a,b', real=True, commutative=False)\n    assert a.is_real and b.is_real\n    assert not a.is_commutative and not b.is_commutative\n\n    x, y, z = symbols('x,y,z', cls=Wild)\n    assert isinstance(x, Wild)\n    assert isinstance(y, Wild)\n    assert isinstance(z, Wild)\n", "compilation error", "compilation error"], "sample_379": ["    def test_safe_string_addition_with_safe_data(self):\n        s1 = mark_safe(\"Hello\")\n        s2 = SafeData()\n        s3 = s1 + s2\n        self.assertIsInstance(s3, SafeString)\n        self.assertEqual(s3, \"Hello\")\n", "    def test_safetext_equality(self):\n        self.assertEqual(SafeString('foo'), SafeString('foo'))\n        self.assertNotEqual(SafeString('foo'), SafeString('bar'))\n        self.assertNotEqual(SafeString('foo'), 'foo')\n", "    def test_safe_string_concatenation_with_non_safe_string(self):\n        safe_str = mark_safe('a')\n        unsafe_str = '<b>'\n        result = safe_str + unsafe_str\n        self.assertFalse(isinstance(result, SafeString))\n        self.assertEqual(result, 'a<b>')\n", "    def test_safe_string_addition(self):\n        # Test concatenating SafeStrings with regular strings\n        safe_str = mark_safe(\"Hello, \")\n        regular_str = \"world!\"\n        \n        result = safe_str + regular_str\n        self.assertIsInstance(result, str)\n        self.assertEqual(result, \"Hello, world!\")\n\n        # Test concatenating SafeStrings with other SafeStrings\n        safe_str2 = mark_safe(\"Django \")\n        combined_safe = safe_str + safe_str2\n        self.assertIsInstance(combined_safe, SafeString)\n        self.assertEqual(combined_safe, \"Hello, Django \")\n", "    def test_safe_string_addition_with_non_str(self):\n        s = SafeString('a')\n        with self.assertRaises(TypeError):\n            s + 1\n"], "sample_300": ["    def test_set_select_fields_empty(self):\n        query = Query(Item)\n        query.set_select_fields([], True)\n        self.assertEqual(query.select, [])\n        self.assertEqual(query.default_cols, False)\n\n", "    def test_filter_isnull_func(self):\n        query = Query(Item)\n        where = query.build_where(Q(note__isnull=True))\n        isnull = where.children[0]\n        self.assertIsInstance(isnull, IsNull)\n        self.assertIsInstance(isnull.lhs, Col)\n        self.assertEqual(isnull.lhs.target, Item._meta.get_field('note'))\n", "    def test_add_related_filter(self):\n        query = Query(ObjectC)\n\n        # Add a filter to the related manager\n        query.add_related_filter('objecta', {'num__gt': 2})\n\n        where = query.build_where(None)\n        lookup = where.children[0]\n        self.assertIsInstance(lookup, GreaterThan)\n        self.assertEqual(lookup.lhs.target.name, 'objecta__num')\n        self.assertEqual(lookup.rhs, 2)\n", "    def test_promote_joins(self):\n        query = Query(Author)\n        query.add_filter(Q(books__title__icontains='test'))\n        query.add_filter(Q(books__id__gt=2) | Q(friends__age__lt=30))\n        query.promote_joins({'books': 1, 'friends': 1})\n        self.assertEqual(query.alias_refcount['books'], 1)\n        self.assertEqual(query.alias_refcount['friends'], 1)\n        self.assertEqual(query.alias_map['books'].join_type, 'INNER')\n        self.assertEqual(query.alias_map['friends'].join_type, 'INNER')\n", "    def test_empty_select(self):\n        query = Query(Author)\n        query.set_values([])\n        where = query.build_where(Q(num__gt=2))\n        self.assertEqual(query.select, ())\n"], "sample_94": ["    def test_create_superuser_with_custom_user_and_groups(self):\n        group = Group.objects.create(name='test_group')\n        @mock.patch(\"django.contrib.auth.management.commands.createsuperuser.get_default_username\")\n        @mock_inputs({'password': 'password', 'username': 'testuser', 'email': 'test@example.com'})\n            mock_username.return_value = 'testuser'\n            call_command(\n                'createsuperuser',\n                interactive=True,\n                stdin=MockTTY(),\n                stdout=StringIO(),\n                groups=[group.name],\n            )\n\n            user = User.objects.get(username='testuser')\n            self.assertEqual(user.groups.count(), 1)\n            self.assertIn(group, user.groups.all())\n\n        test(self)\n", "    def test_create_permissions_with_custom_permission_handler(self):\n        \"\"\"\n        Test that a custom permission handler is used correctly.\n        \"\"\"\n\n        class CustomPermissionHandler:\n                self.app_config = app_config\n                self.created_permissions = []\n\n                self.created_permissions.append('custom_permission')\n\n        PermissionHandler = CustomPermissionHandler\n\n        Permission._meta.permissions = []\n        create_permissions(self.app_config, verbosity=0)\n\n        self.assertEqual(PermissionHandler.created_permissions, ['custom_permission'])\n", "    def test_permission_with_proxy_model_deleted(self):\n        \"\"\"\n        Deleting a proxy model should not delete the permissions associated\n        with its concrete model.\n        \"\"\"\n        opts = UserProxy._meta\n        codename = get_permission_codename('add', opts)\n        permission = Permission.objects.get(\n            content_type__model=opts.model_name,\n            content_type__app_label=opts.app_label,\n            codename=codename,\n        )\n        permission.delete()\n\n        self.assertTrue(\n            Permission.objects.filter(\n                content_type__model=User._meta.model_name,\n                content_type__app_label=User._meta.app_label,\n                codename=codename,\n            ).exists()\n        )\n", "    def test_permission_creation_with_custom_default_permissions(self):\n        Permission._meta.default_permissions = ['add_user', 'delete_user']\n        create_permissions(self.app_config, verbosity=0)\n        self.assertEqual(Permission.objects.filter(codename='add_user').count(), 1)\n        self.assertEqual(Permission.objects.filter(codename='delete_user').count(), 1)\n        self.assertEqual(\n            Permission.objects.filter(codename='change_user').count(),\n            0)\n        self.assertEqual(\n            Permission.objects.filter(codename='view_user').count(),\n            0)\n", "    def test_create_permissions_with_custom_permissions(self):\n        Permission._meta.permissions = [\n            ('custom_permission_1', 'Custom permission 1'),\n            ('custom_permission_2', 'Custom permission 2'),\n        ]\n        create_permissions(self.app_config, verbosity=0)\n        self.assertEqual(Permission.objects.count(), 7)  # 2 default + 5 custom\n\n        Permission.objects.all().delete()\n        Permission._meta.default_permissions = ['add_user', 'change_user']\n\n        create_permissions(self.app_config, verbosity=0)\n\n        self.assertEqual(Permission.objects.count(), 7)  # 2 default + 5 custom\n        self.assertTrue(Permission.objects.filter(codename='custom_permission_1').exists())\n        self.assertTrue(Permission.objects.filter(codename='custom_permission_2').exists())\n\n"], "sample_49": ["    def test_media_complex_inheritance(self):\n\n        class MyWidget1(TextInput):\n            class Media:\n                css = {\n                    'all': ('path/to/css1', '/path/to/css2'),\n                    'print': ('/print/only.css',)\n                }\n                js = ('/path/to/js1', 'http://media.other.com/path/to/js2', 'https://secure.other.com/path/to/js3')\n\n        class MyWidget2(MyWidget1):\n            class Media:\n                extend = ('css',)\n                css = {\n                    'screen': ('/screen/only.css',),\n                  'print': ('/print/only2.css',) \n                }\n                js = ('/path/to/js4',)\n\n        w2 = MyWidget2()\n        self.assertEqual(\n            str(w2.media),\n            \"\"\"<link href=\"http://media.example.com/static/path/to/css1\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">", "compilation error", "    def test_media_inheritance_empty(self):\n        class MyWidget1(TextInput):\n            class Media:\n                css = {\n                    'all': ('path/to/css1', '/path/to/css2')\n                }\n\n        class MyWidget2(MyWidget1):\n            class Media:\n                pass\n\n        w2 = MyWidget2()\n        self.assertEqual(str(w2.media), \"\"\"<link href=\"http://media.example.com/static/path/to/css1\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">", "    def test_media_rendering_order(self):\n        class MyWidget1(TextInput):\n            class Media:\n                css = {\n                    'all': ('style1.css',),\n                }\n                js = ('script1.js',)\n\n        class MyWidget2(TextInput):\n            class Media:\n                css = {\n                    'all': ('style2.css',),\n                }\n                js = ('script2.js',)\n\n        class MyForm(Form):\n            field1 = CharField(max_length=20, widget=MyWidget1())\n            field2 = CharField(max_length=20, widget=MyWidget2())\n\n        f = MyForm()\n\n        self.assertEqual(\n            str(f.media),\n            \"\"\"<link href=\"style1.css\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">", "compilation error"], "sample_987": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_issue_10676():\n    a = Symbol('a', real=True)\n    b = Symbol('b', real=True)\n    c = a + b*I\n    assert (c.conjugate()).evalf(subs={a: 1, b: 2}) == 1 - 2*I\n"], "sample_226": ["    def test_clone_test_db(self):\n        # Test cloning a test database.\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['NAME'] = 'hodor'\n        creation = test_connection.creation_class(test_connection)\n        creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            creation.clone_test_db('clone', verbosity=0, autoclobber=True)\n            \n            # Check that the clone database exists.\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n        finally:\n            creation._destroy_test_db(old_database_name, verbosity=0)\n", "    def test_clone_test_db(self):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        with mock.patch.object(creation, '_clone_test_db'):\n            creation.clone_test_db('clone')\n", "    def test_clone_test_db_name(self, mocked_sync_apps, mocked_migrate, *mocked_objects):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST'] = {'NAME': 'my_test_db'}\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.clone_test_db(suffix='suffix', verbosity=0, autoclobber=True, keepdb=False)\n            self.assertEqual(creation.connection.settings_dict['NAME'], f'my_test_db_suffix')\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def test_get_test_db_clone_settings(self):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['NAME'] = 'original_db'\n        creation = test_connection.creation_class(test_connection)\n        clone_settings = creation.get_test_db_clone_settings('suffix')\n        self.assertEqual(clone_settings['NAME'], 'original_db_suffix')\n", "    def test_create_test_db_keeps_existing(self, mocked_sync_apps, mocked_migrate, *mocked_objects\n"], "sample_574": ["    def test_color_palette_with_transform(self, t, x):\n\n        cmap = color_palette(\"magma\", as_cmap=True)\n        transform = lambda x: x.astype(int)\n        s = Temporal(transform=transform)._setup(t, Color())\n        normed = (transform(x) - transform(x).min()) / (transform(x).max() - transform(x).min())\n        assert_array_equal(s(t), cmap(normed)[:, :3])  # FIXME RGBA\n", "    def test_color_transform(self, t, x):\n\n        cmap = color_palette(\"ch:\", as_cmap=True)\n        s = Temporal(transform=\"log\")._setup(t, Color())\n        normed = np.log(x) / np.log(x.max())\n        assert_array_equal(s(t), cmap(normed)[:, :3])  # FIXME RGBA\n", "    def test_tick_every(self, t, x):\n\n        d = 100\n        ax = mpl.figure.Figure().subplots()\n        Temporal().tick(every=d)._setup(t, Coordinate(), ax.xaxis)\n        locator = ax.xaxis.get_major_locator()\n        assert np.allclose(np.diff(locator.tick_values(0, 365)), d)\n", "    def test_label_unit(self, t, x):\n\n        s = Temporal().label(unit=\"years\")\n        a = PseudoAxis(s._setup(t, Coordinate())._matplotlib_scale)\n        a.set_view_interval(10, 1000)\n        label, = a.major.formatter.format_ticks([100])\n        assert label == \"1970 years\"\n", "    def test_fill_with_nunique_warning_on(self, x):\n        with pytest.warns(UserWarning, match=\"The variable assigned to fill\"):\n            s = Nominal()._setup(x, Fill())\n        assert_array_equal(s(x), [True, False, True, True, False])\n"], "sample_1192": ["compilation error", "compilation error", "compilation error", "def test_symbol_creation():\n    from sympy.core.symbol import Symbol, _symbol\n    x = Symbol('x')\n    assert str(type(x)) == \"<class 'sympy.core.symbol.Symbol'>\"\n    assert _symbol('x') == x\n    assert _symbol('x') is x\n\n", "compilation error"], "sample_391": ["    def test_optimize_circular_dependencies(self):\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    \"A\", [(\"name\", models.CharField(max_length=255))]\n                ),\n                migrations.CreateModel(\n                    \"B\", [(\"name\", models.CharField(max_length=255))]\n                ),\n                migrations.AddField(\n                    \"A\", \"b\", models.ForeignKey(\"migrations.B\", models.CASCADE)\n                ),\n                migrations.AddField(\n                    \"B\", \"a\", models.ForeignKey(\"migrations.A\", models.CASCADE)\n                ),\n                migrations.CreateModel(\n                    \"C\", [(\"name\", models.CharField(max_length=255))]\n                ),\n                migrations.AddField(\n                    \"C\", \"a\", models.ForeignKey(\"migrations.A\", models.CASCADE)\n                ),\n            ],\n            [\n                migrations.CreateModel(\n                    \"A\", [(\"name\", models.CharField(max_length=255))]\n                ),\n                migrations.CreateModel(\n                    \"B\", [(\"name\", models.CharField(max_length=255))]\n                ),\n                migrations.AddField(\n                    \"A\", \"b\", models.ForeignKey(\"migrations.B\", models.CASCADE)\n                ),\n                migrations.AddField(\n                    \"B\", \"a\", models.ForeignKey(\"migrations.A\", models.CASCADE)\n                ),\n                migrations.CreateModel(\n                    \"C\",\n                    [\n                        (\"name\", models.CharField(max_length=255)),\n                        (\"a\", models.ForeignKey(\"migrations.A\", models.CASCADE)),\n                    ],\n                ),\n            ],\n        )\n\n", "    def test_optimize_through_fields_delete_created(self):\n        \"\"\"\n        Ensure that DeleteModel doesn't block field-level through\n        optimizations when the model is created within the same migration.\n        \"\"\"\n\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    \"Foo\", [(\"name\", models.CharField(max_length=255))]\n                ),\n                migrations.CreateModel(\"Bar\", [(\"size\", models.IntegerField())]),\n                migrations.AddField(\n                    \"Foo\",\n                    \"bar\",\n                    models.ForeignKey(\"migrations.Bar\", models.CASCADE),\n                ),\n                migrations.DeleteModel(\"Foo\"),\n            ],\n            [],\n        )\n\n", "    def test_optimize_through_fields_with_autofield(self):\n        \"\"\"\n        field-level through checking is working with AutoFields.\n        \"\"\"\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    \"Foo\", [(\"name\", models.CharField(max_length=255))]\n                ),\n                migrations.CreateModel(\"Bar\", [(\"size\", models.IntegerField())]),\n                migrations.AddField(\"Foo\", \"age\", models.IntegerField()),\n                migrations.AddField(\"Bar\", \"width\", models.AutoField(primary_key=True)),\n                migrations.AlterField(\"Foo\", \"age\", models.IntegerField()),\n                migrations.RenameField(\"Bar\", \"size\", \"dimensions\"),\n                migrations.RemoveField(\"Foo\", \"age\"),\n                migrations.RenameModel(\"Foo\", \"Phou\"),\n                migrations.RemoveField(\"Bar\", \"dimensions\"),\n                migrations.RenameModel(\"Phou\", \"Fou\"),\n                migrations.DeleteModel(\"Fou\"),\n            ],\n            [\n                migrations.CreateModel(\"Bar\", [(\"width\", models.AutoField(primary_key=True))]),\n            ],\n        )\n\n", "    def test_optimize_elidable_operation_no_optimization(self):\n        elidable_operation = operations.base.Operation()\n        elidable_operation.elidable = True\n        self.assertDoesNotOptimize(\n            [\n                elidable_operation,\n                migrations.CreateModel(\n                    \"Foo\", [(\"name\", models.CharField(max_length=255))]\n                ),\n                elidable_operation,\n                migrations.AddField(\"Foo\", \"age\", models.IntegerField()),\n                elidable_operation,\n            ],\n        )\n", "    def test_optimize_delete_model_with_fields(self):\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    \"Foo\",\n                    [\n                        (\"name\", models.CharField(max_length=255)),\n                        (\"age\", models.IntegerField()),\n                    ],\n                ),\n                migrations.DeleteModel(\"Foo\"),\n            ],\n            [],\n        )\n"], "sample_986": ["def test_issue_11156():\n    f = Piecewise((1, x > 0), (0, True))\n    assert f.evalf(subs={x: 2}) == 1\n", "def test_issue_11097():\n    a = Symbol('a', positive=True)\n    b = Symbol('b', positive=True)\n    assert (a/b).evalf(subs={a: 1, b: 1e-10}) == 1e+10\n", "def test_issue_11803():\n    assert (x**(2*I)).n() == (exp(2*I*log(x))).n()\n", "compilation error", "compilation error"], "sample_1029": ["def test_Sum2():\n    sT(Sum2(x**2, (x, 1, 5)), \"Sum2(Pow(Symbol('x'), Integer(2)), Symbol('x'), Integer(1), Integer(5))\")\n", "compilation error", "def test_Sum2():\n    i, a, b = symbols('i a b')\n    f = Function('f')(i)\n    s = Sum2(f, (i, a, b))\n    sT(s, \"Sum2(Function('f')(i), (i, Symbol('a'), Symbol('b')))\")\n", "def test_Sum2():\n    i, a, b = symbols('i a b')\n    sT(Sum2(x**2, (i, a, b)), \"Sum2(Pow(Symbol('x'), Integer(2)), (Symbol('i'), Symbol('a'), Symbol('b')))\")\n", "def test_Sum2():\n    x,y = symbols('x,y')\n    sT(Sum2(x, (y, 1, 5)), 'Sum2(Symbol(x),(Symbol(y), Integer(1), Integer(5)))')\n"], "sample_662": ["    def test_report_with_empty_longrepr(self, testdir, pytestconfig):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            \n                pytest.skip(\"skip this test\")\n                \n                pass\n        \"\"\"\n        )\n        reprec = testdir.inline_run()\n        reports = reprec.getreports(\"pytest_runtest_logreport\")\n        assert len(reports) == 6\n        rep = reports[1]\n        assert rep.longrepr is None\n        data = pytestconfig.hook.pytest_report_to_serializable(\n            config=pytestconfig, report=rep\n        )\n        assert data[\"longrepr\"] is None\n        new_rep = pytestconfig.hook.pytest_report_from_serializable(\n            config=pytestconfig, data=data\n        )\n        assert new_rep.longrepr is None\n\n", "    def test_report_with_keywords(self, testdir, pytestconfig):\n        testdir.makepyfile(\n            \"\"\"\n\n        \"\"\"\n        )\n        reprec = testdir.inline_run(\"--keywords=a\")\n        reports = reprec.getreports(\"pytest_runtest_logreport\")\n        assert len(reports) == 3\n        rep = reports[1]\n        data = pytestconfig.hook.pytest_report_to_serializable(\n            config=pytestconfig, report=rep\n        )\n        assert \"keywords\" in data\n        assert data[\"keywords\"][\"a\"] == 1\n        new_rep = pytestconfig.hook.pytest_report_from_serializable(\n            config=pytestconfig, data=data\n        )\n        assert new_rep.keywords == rep.keywords\n\n", "    def test_deserialize_report_with_empty_longrepr(self, testdir, pytestconfig):\n        testdir.makepyfile(\n            \"\"\"\n        \"\"\"\n        )\n        reprec = testdir.inline_run()\n        reports = reprec.getreports(\"pytest_runtest_logreport\")\n        assert reports\n        rep = reports[0]\n        rep.longrepr = None\n        data = pytestconfig.hook.pytest_report_to_serializable(\n            config=pytestconfig, report=rep\n        )\n        new_rep = pytestconfig.hook.pytest_report_from_serializable(\n            config=pytestconfig, data=data\n        )\n        assert new_rep.longrepr is None\n\n", "    def test_report_no_longrepr(self, testdir, pytestconfig):\n        testdir.makepyfile(\n            \"\"\"\n                pass\n        \"\"\"\n        )\n        reprec = testdir.inline_run()\n        reports = reprec.getreports(\"pytest_runtest_logreport\")\n        assert len(reports) == 3\n        rep = reports[1]\n        rep.longrepr = None\n        data = pytestconfig.hook.pytest_report_to_serializable(\n            config=pytestconfig, report=rep\n        )\n        assert \"$report_type\" in data\n\n        new_rep = pytestconfig.hook.pytest_report_from_serializable(\n            config=pytestconfig, data=data\n        )\n\n        assert new_rep.longrepr is None\n", "    def test_collectreport_passed_with_location(self, testdir, pytestconfig):\n        testdir.makepyfile(\n            \"\"\"\n        \"\"\"\n        )\n        reprec = testdir.inline_run()\n        reports = reprec.getreports(\"pytest_collectreport\")\n        for rep in reports:\n            if rep.passed:\n                data = pytestconfig.hook.pytest_report_to_serializable(\n                    config=pytestconfig, report=rep\n                )\n                assert data[\"location\"] == (str(testdir.tmpdir), None, str(testdir.tmpdir))\n"], "sample_296": ["    def test_message_ordering_cookie_storage(self):\n        storage = self.get_storage()\n        response = self.get_response()\n\n        storage.add(constants.INFO, 'Message 1')\n        storage.add(constants.WARNING, 'Message 2')\n        storage.add(constants.ERROR, 'Message 3')\n        storage.update(response)\n\n        self.assertEqual(list(storage), ['Message 1', 'Message 2', 'Message 3'])\n", "    def test_not_finished(self):\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Add messages until they exceed the cookie size limit\n        while True:\n            msg = get_random_string(100)\n            storage.add(constants.INFO, msg)\n            cookie_storing = self.stored_messages_count(storage, response)\n            if cookie_storing < 1:\n                # Messages are being dropped\n                break\n        \n        self.assertEqual(storage._get()[1], False)\n        # Add one more message, which should be dropped and the sentinel value\n        # added\n        storage.add(constants.INFO, \"final message\")\n        self.assertEqual(cookie_storing , 1)\n        self.assertEqual(storage._get()[1], False)\n        \n        \n        # Retrieve all messages (including the sentinel value)\n        all_messages = list(storage)\n        self.assertEqual(len(all_messages), cookie_storing)\n\n        # Check that the sentinel value is not part of the retrieved messages\n        self.assertNotIn(CookieStorage.not_finished, all_messages)\n\n", "    def test_cookie_size_overflow_with_not_finished(self):\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Create a large message that will exceed the cookie size limit.\n        long_message = get_random_string(int(CookieStorage.max_cookie_size / 2))\n        storage.add(constants.INFO, long_message)\n\n        # Add a second message to ensure the sentinel value is used.\n        storage.add(constants.INFO, 'short message')\n\n        # Make sure the messages are saved correctly and the sentinel value is present.\n        storage.update(response)\n        cookie_data = response.cookies.get(CookieStorage.cookie_name).value\n        self.assertIn(CookieStorage.not_finished, json.loads(\n            cookie_data.decode('latin-1'), cls=MessageDecoder))\n\n        # Retrieve the messages and ensure that the sentinel value is removed.\n        retrieved_messages = list(storage)\n        self.assertNotIn(CookieStorage.not_finished, retrieved_messages)\n", "    def test_empty_cookie_data(self):\n        \"\"\"\n        An empty cookie data should not raise an error. It should return\n        an empty list when accessed.\n\n        \"\"\"\n        storage = self.get_storage()\n        set_cookie_data(storage, [], encode_empty=True)\n        self.assertEqual(list(storage), [])\n", "    def test_empty_cookie(self):\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Set the cookie to an empty value\n        storage._update_cookie('', response)\n        self.assertIsNone(response.cookies.get('messages'))\n\n"], "sample_535": ["def test_cell_alignment():\n    fig, axs = plt.subplots(2, 2)\n    data = [['left', 'center', 'right'],\n            ['left', 'center', 'right'],\n            ['left', 'center', 'right']]\n\n    for ax, loc in zip(axs.flat, ['left', 'center', 'right']):\n        ax.axis('off')\n        ax.table(cellText=data, loc='center', cellLoc=loc)\n", "def test_empty_cell():\n    fig, ax = plt.subplots()\n    \n    table = Table(ax)\n    table.add_cell(0, 0, width=0.2, height=0.2)\n\n    ax.add_table(table)\n", "def test_empty_cells():\n    fig, ax = plt.subplots()\n    table = ax.table(cellText=[['A', 'B', ''],\n                              ['C', '', 'E']], loc='center')\n    table.auto_set_column_width([0, 1])\n", "def test_celltext_fontsize():\n    fig, axs = plt.subplots(2, 2)\n\n    for ax, fs in zip(axs.flat, [6, 10, 14, 18]):\n        tb = ax.table(cellText=[['A', 'B'],\n                                ['C', 'D']],\n                      loc='center')\n        tb.set_fontsize(fs)\n        ax.axis('off')\n", "def test_table_cellText_with_newline():\n    data = [['Line 1\\nLine 2', 2],\n            ['Line 1\\nLine 2\\nLine 3', 5]]\n\n    fig, ax = plt.subplots()\n    table = ax.table(cellText=data, loc='center')\n\n    ax.axis('off')\n"], "sample_875": ["def test_balanced_accuracy_score_empty_labels():\n    y_true = np.array([])\n    y_pred = np.array([])\n    \n    with pytest.warns(UserWarning, match=\"No samples in y_true\"):\n        balanced_accuracy_score(y_true, y_pred)\n\n", "compilation error", "compilation error", "def test_balanced_accuracy_score_empty():\n    assert balanced_accuracy_score([], []) == 0.0\n    assert balanced_accuracy_score([0], []) == 0.0\n    assert balanced_accuracy_score([], [0]) == 0.0\n\n", "    def test_hinge_loss_multiclass_with_missing_labels_and_sample_weight():\n        pred_decision = np.array(\n            [\n                [+0.36, -0.17, -0.58, -0.99],\n                [-0.55, -0.38, -0.48, -0.58],\n                [-1.45, -0.58, -0.38, -0.17],\n                [-0.55, -0.38, -0.48, -0.58],\n                [-1.45, -0.58, -0.38, -0.17],\n            ]\n        )\n        y_true = np.array([0, 1, 2, 1, 2])\n        labels = np.array([0, 1, 2, 3])\n        sample_weight = np.array([1, 2, 1, 1, 1])\n\n        dummy_losses = np.array(\n            [\n                1 - pred_decision[0][0] + pred_decision[0][1],\n                1 - pred_decision[1][1] + pred_decision[1][2],\n                1 - pred_decision[2][2] + pred_decision[2][3],\n                1 - pred_decision[3][1] + pred_decision[3][2],\n                1 - pred_decision[4][2] + pred_decision[4][3],\n            ]\n        )\n        np.clip(dummy_losses, 0, None, out=dummy_losses)\n        dummy_hinge_loss = np.average(dummy_losses, weights=sample_weight)\n        assert hinge_loss(y_true, pred_decision, labels=labels, sample_weight=sample_weight) == dummy_hinge_loss\n"], "sample_934": ["def test_build_domain_cpp_no_parse_warnings(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"no-parse\")\n    assert len(ws) == 0\n", "    def check(target):\n        class Config:\n            cpp_id_attributes = [\"id_attr\"]\n            cpp_paren_attributes = [\"paren_attr\"]\n        parser = DefinitionParser(target, location=None,\n                                  config=Config())\n        ast, isShorthand = parser.parse_xref_object()\n        parser.assert_end()", "    def check(spec, text, file):\n        pattern = '<li><p>%s<a .*?><code .*?><span .*?>%s</span></code></a></p></li>' % spec\n        res = re.search(pattern, text)\n        if not res:\n            print(\"Pattern\\n\\t%s\\nnot found in %s\" % (pattern, file))\n            assert False", "def test_noindexentry_multiple(app):\n    text = (\".. cpp:function:: void f()\\n\"\n            \".. cpp:function:: void g()\\n\"\n            \"   :noindexentry:\\n\"\n            \".. cpp:function:: void h()\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc, addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[('single', 'f (C++ function)', '_CPPv41fv', '', None)])\n    assert_node(doctree[2], addnodes.index, entries=[])\n    assert_node(doctree[4], addnodes.index, entries=[('single', 'h (C++ function)', '_CPPv41hv', '', None)])\n", "compilation error"], "sample_442": ["    def test_sign_unsign_with_different_algorithms(self):\n        signer1 = signing.Signer(key=\"predictable-secret\", algorithm=\"sha256\")\n        signer2 = signing.Signer(key=\"predictable-secret\", algorithm=\"sha512\")\n        value = \"hello world\"\n        signed1 = signer1.sign(value)\n        with self.assertRaises(signing.BadSignature):\n            signer2.unsign(signed1)\n", "    def test_timestamp_signer_with_custom_salt(self):\n        value = \"hello\"\n        with freeze_time(123456789):\n            signer = signing.TimestampSigner(key=\"predictable-key\", salt=\"custom-salt\")\n            ts = signer.sign(value)\n            self.assertNotEqual(ts, signing.Signer(key=\"predictable-key\").sign(value))\n            self.assertEqual(signer.unsign(ts), value)\n", "    def test_unsign_object_with_different_serializer(self):\n        class CustomSerializer:\n                return json.dumps(obj, separators=(\",\", \":\")).encode(\"latin-1\").upper()\n\n                return json.loads(data.decode(\"latin-1\"))\n\n        obj = {\"foo\": \"bar\"}\n        signer = signing.Signer(key=\"predictable-secret\")\n        signed_obj = signer.sign_object(obj, serializer=CustomSerializer)\n        self.assertEqual(signer.unsign_object(signed_obj, serializer=CustomSerializer), obj)\n", "    def test_sign_object_compressed(self):\n        signer = signing.Signer(key=\"predictable-secret\")\n        test_data = list(range(1000))\n        compressed = signer.sign_object(test_data, compress=True)\n        uncompressed = signer.sign_object(test_data, compress=False)\n\n        self.assertNotEqual(compressed, uncompressed)\n        self.assertEqual(signing.loads(compressed), test_data)\n        self.assertEqual(signing.loads(uncompressed), test_data)\n        self.assertTrue(len(compressed) < len(uncompressed))\n", "    def test_sign_object_with_custom_serializer(self):\n        class CustomSerializer:\n                return json.dumps(obj).encode(\"latin-1\")\n\n                return json.loads(data.decode(\"latin-1\"))\n\n        signer = signing.Signer(key=\"predictable-secret\")\n        obj = {\"hello\": \"world\"}\n        signed_obj = signer.sign_object(obj, serializer=CustomSerializer())\n        self.assertEqual(signer.unsign_object(signed_obj, serializer=CustomSerializer()), obj)\n"], "sample_810": ["compilation error", "def test_pipeline_with_named_steps():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    # Test with Transformer + SVC\n    clf = SVC(gamma='scale', probability=True, random_state=0)\n    transf = DummyTransf()\n    pipe = Pipeline([('transf', transf), ('svc', clf)])\n    named_pipe = Pipeline([('transformer', transf), ('classifier', clf)])\n    pipe.fit(X, y)\n    named_pipe.fit(X, y)\n    assert_array_equal(pipe.predict(X), named_pipe.predict(X))\n", "compilation error", "    def test_pipeline_with_sparse_input(self):\n        iris = load_iris()\n        X = iris.data\n        y = iris.target\n        X_sparse = sparse.csr_matrix(X)\n\n        pipe = make_pipeline(StandardScaler(), LogisticRegression())\n        pipe.fit(X_sparse, y)\n        y_pred = pipe.predict(X_sparse)\n        assert_array_equal(y_pred, pipe.predict(X))\n\n", "compilation error"], "sample_69": ["    def test_watch_dir_recursive_glob_with_parent_directory_not_existing(self):\n        nonexistent_parent = self.tempdir / 'does_not_exist'\n        self.reloader.watch_dir(nonexistent_parent, '**/*.py')\n        watched_files = list(self.reloader.watched_files())\n        self.assertNotIn(nonexistent_parent, watched_files)\n\n", "    def test_watch_dir_with_multiple_patterns(self):\n        self.reloader.watch_dir(self.tempdir, ['*.py', '*.txt'])\n        watched_files = list(self.reloader.watched_files())\n        self.assertIn(self.existing_file, watched_files)\n", "    def test_run_loop_with_reloader_exception(self):\n            yield\n            raise Exception('Intentional Exception')\n\n        with mock.patch.object(self.reloader, 'tick', side_effect=mocked_tick) as tick:\n            with self.assertRaises(Exception) as context:\n                self.reloader.run_loop()\n        self.assertEqual(str(context.exception), 'Intentional Exception')\n        self.assertEqual(tick.call_count, 1)\n", "    def test_check_for_changes_with_modified_file(self):\n        self.reloader.watch_file(self.existing_file)\n        with self.tick_twice():\n            self.increment_mtime(self.existing_file)\n        self.assertTrue(self.reloader.check_for_changes())\n", "    def test_watch_dir_recursive(self):\n        self.reloader.watch_dir(self.tempdir, '**/*.py')\n        inner_file = self.ensure_file(self.tempdir / 'dir' / 'test.py')\n        watched_files = list(self.reloader.watched_files())\n        self.assertIn(inner_file, watched_files)\n"], "sample_416": ["    def test_default_db(self):\n        \"\"\"Test connecting to default 'postgres' db when dbname is not specified.\"\"\"\n        with mock.patch(\"django.db.backends.postgresql.client.DatabaseClient.settings_to_cmd_args_env\") as mock_settings_to_cmd_args_env:\n            mock_settings_to_cmd_args_env.return_value = (\n                [\"psql\", \"postgres\"],\n                None,\n            )\n            connection.client.runshell([])\n            mock_settings_to_cmd_args_env.assert_called_once_with(\n                {'OPTIONS': {}}, []\n            )\n\n", "    def test_default_database(self):\n        \"\"\"When no dbname is given, it defaults to 'postgres'.\"\"\"\n        settings = {\"USER\": \"someuser\", \"HOST\": \"somehost\", \"PORT\": \"444\"}\n        args, env = self.settings_to_cmd_args_env(settings)\n        self.assertEqual(args, [\"psql\", \"-U\", \"someuser\", \"-h\", \"somehost\", \"-p\", \"444\", \"postgres\"])\n        self.assertIsNone(env)\n\n", "    def test_default_db(self):\n        \"\"\"Connecting without dbname defaults to 'postgres'.\"\"\"\n        settings = {\"HOST\": \"somehost\", \"PORT\": \"444\", \"USER\": \"someuser\"}\n        args, env = self.settings_to_cmd_args_env(settings)\n        self.assertEqual(args, [\"psql\", \"-U\", \"someuser\", \"-h\", \"somehost\", \"-p\", \"444\", \"postgres\"])\n        self.assertIsNone(env)\n\n", "    def test_default_db(self):\n        \"\"\"If no dbname is provided, connect to the default 'postgres' db.\"\"\"\n        with mock.patch.dict(\"os.environ\", {}):\n            args, env = DatabaseClient.settings_to_cmd_args_env({\"USER\": \"someuser\"})\n            self.assertEqual(args, [\"psql\", \"-U\", \"someuser\", \"postgres\"])\n", "    def test_default_db(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env({\"HOST\": \"somehost\", \"PORT\": \"444\"}),\n            ([\"psql\", \"-h\", \"somehost\", \"-p\", \"444\", \"postgres\"], None),\n        )\n"], "sample_498": ["    def test_legend_title_fontsize_autosize(self):\n        # Test that legend title fontsize is autosized when title_fontsize is None\n        plt.plot(range(10), label='aardvark')\n        leg = plt.legend(title=\"Legend Title\")\n        assert leg.get_title().get_fontsize() == mpl.rcParams['legend.title_fontsize']\n", "def test_legend_bbox_to_anchor():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label='line')\n    leg = ax.legend(bbox_to_anchor=(1, 1))\n    assert leg.bbox.anchored == (1.0, 1.0)\n", "def test_legend_bbox_to_anchor():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label='line')\n    ax.legend(bbox_to_anchor=(1, 0.5))\n", "def test_legend_bbox_to_anchor(bbox_to_anchor):\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label='line')\n    ax.legend(bbox_to_anchor=bbox_to_anchor)\n", "def test_legend_fontproperties():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line')\n    font = FontProperties(size=20, weight='bold')\n    leg = ax.legend(prop=font)\n    assert leg.get_texts()[0].get_fontsize() == 20\n    assert leg.get_texts()[0].get_weight() == 'bold'\n"], "sample_719": ["    def test_vectorizer_max_features_with_preprocessor():\n        # Regression test: max_features didn't work correctly with preprocessor\n        # in versions before 0.17.\n\n        preprocessor = lambda x: \" \".join(x.split()[:3])\n        cv_1 = CountVectorizer(max_features=1, preprocessor=preprocessor)\n        cv_3 = CountVectorizer(max_features=3, preprocessor=preprocessor)\n        cv_None = CountVectorizer(max_features=None, preprocessor=preprocessor)\n\n        counts_1 = cv_1.fit_transform(JUNK_FOOD_DOCS).sum(axis=0)\n        counts_3 = cv_3.fit_transform(JUNK_FOOD_DOCS).sum(axis=0)\n        counts_None = cv_None.fit_transform(JUNK_FOOD_DOCS).sum(axis=0)\n\n        features_1 = cv_1.get_feature_names()\n        features_3 = cv_3.get_feature_names()\n        features_None = cv_None.get_feature_names()\n\n        # The most common feature should be the first one\n        assert_equal(np.argmax(counts_1), 0)\n        assert_equal(np.argmax(counts_3), 0)\n        assert_equal(np.argmax(counts_None), 0)\n\n        # Verify the number of features selected\n        assert_equal(len(features_1), 1)\n        assert_equal(len(features_3), 3)\n        assert_equal(len(features_None), len(cv_None.vocabulary_))\n\n", "    def test_count_vectorizer_empty_input():\n        vectorizer = CountVectorizer()\n        \n        with pytest.raises(ValueError) as excinfo:\n            vectorizer.fit([])\n        assert \"Input corpus is empty\" in str(excinfo.value)\n\n        with pytest.raises(ValueError) as excinfo:\n            vectorizer.transform([])\n        assert \"Input corpus is empty\" in str(excinfo.value)\n", "def test_vectorizer_empty_vocabulary():\n    # Test cases for empty vocabulary handling with CountVectorizer and\n    # TfidfVectorizer\n    for vect in [CountVectorizer(), TfidfVectorizer()]:\n\n        # Test fit_transform\n        X = vect.fit_transform([])\n        assert X.shape == (0, 0)\n        assert_equal(vect.vocabulary_, {})\n\n        # Test transform with empty input\n        X = vect.transform([])\n        assert X.shape == (0, 0)\n\n        # Test transform with previously fitted vocabulary and empty input\n        vect.fit(ALL_FOOD_DOCS)\n        X = vect.transform([])\n        assert X.shape == (0, len(vect.vocabulary_))\n", "    def test_vectorizer_empty_vocabulary():\n        vect = TfidfVectorizer(vocabulary=[])\n        with pytest.raises(ValueError, match=\"vocabulary cannot be empty\"):\n            vect.fit_transform(JUNK_FOOD_DOCS)\n", "    def test_countvectorizer_zero_length_docs():\n        vect = CountVectorizer()\n        docs = ['hello world', '']\n        X = vect.fit_transform(docs)\n        assert X.shape[0] == 2\n        assert X.shape[1] > 0\n        assert_array_equal(X.toarray()[1], np.zeros(X.shape[1]))\n"], "sample_704": ["    def test_nodeid_with_double_colons(pytester: Pytester) -> None:\n        # issue #2243: nodeid should handle double colons correctly\n\n        pytester.makepyfile(\n            \"\"\"\n                pass\n            \"\"\"\n        )\n        result = pytester.runpytest(\"-v\")\n        result.stdout.fnmatch_lines(\"*test_func*\")\n", "compilation error", "def test_node_repr_failure_style_auto(pytester: Pytester) -> None:\n    \"\"\"Test that repr_failure uses 'short' style when style is 'auto' and fulltrace is False.\"\"\"\n    p = pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            assert False\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    # Ensure that the output contains 'short' style traceback\n    assert \"short traceback\" in result.stdout.str()\n\n", "    def test_repr_failure_style_short(pytester: Pytester, tmp_path: Path) -> None:\n        \"\"\"#6917 - Ensure 'short' style works as expected.\"\"\"\n        p = tmp_path / \"test_example.py\"\n        p.write_text(\n            \"\"\"\n            import pytest\n\n                assert False\n        \"\"\"\n        )\n        result = pytester.runpytest(p, \"--tbstyle=short\")\n        result.stdout.fnmatch_lines(\n            [\n                \"== FAILURES ==\",\n                \"test_example.py:4: AssertionError\",\n                \"assert False\",\n                \"---\",\n                \"1 failed in 0.01s\",\n            ]\n        )\n", "def test_path_representation(pytester: Pytester) -> None:\n    p = pytester.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    items = pytester.getitems(p)\n    assert str(items[0].path) == str(p)\n\n\n\n"], "sample_262": ["    def test_keep_lazy(self):\n        @keep_lazy(str)\n            return a + b\n\n        self.assertEqual(my_func(lazystr(\"hello\"), \" world\"), \"hello world\")\n        self.assertEqual(my_func(\"hello\", lazystr(\" world\")), \"hello world\")\n        self.assertIsInstance(my_func(lazystr(\"hello\"), lazystr(\" world\")), str)\n\n", "    def test_keep_lazy(self):\n            return arg1 + arg2\n\n        lazy_func = keep_lazy(int)(myfunc)\n\n        self.assertEqual(lazy_func(3, 4), 7)\n        self.assertEqual(lazy_func(lazy(lambda:3, int), 4), 7)\n        self.assertEqual(lazy_func(3, lazy(lambda:4, int)), 7)\n\n\n", "    def test_keep_lazy(self):\n            return a + b\n\n        @keep_lazy(int)\n\n            return lazy_sum(a, b)\n\n        self.assertEqual(func(1, 2), 3)\n        self.assertEqual(func(lazy(lambda: 1, int), 2), 3)\n        self.assertEqual(func(1, lazy(lambda: 2, int)), 3)\n        self.assertIsInstance(func(lazy(lambda: 1, int), lazy(lambda: 2, int)), lazy)\n", "    def test_keep_lazy_text(self):\n        @keep_lazy_text\n            return a + \" \" + b\n\n        a = lazystr(\"Hello\")\n        b = lazystr(\"world\")\n        self.assertEqual(func(a, b), \"Hello world\")\n", "    def test_lazy_pickle(self):\n        lazy_obj = lazy(lambda: [1, 2, 3], list)\n        unpickled = pickle.loads(pickle.dumps(lazy_obj))\n        self.assertEqual(unpickled(), [1, 2, 3])\n"], "sample_253": ["    def test_should_stop(self):\n        self.assertFalse(self.reloader.should_stop())\n        self.reloader.stop()\n        self.assertTrue(self.reloader.should_stop())\n", "    def test_ensure_dir_exists(self):\n        nonexistent_dir = self.tempdir / 'does_not_exist'\n        self.assertFalse(nonexistent_dir.exists())\n        self.reloader._ensure_dir_exists(nonexistent_dir)\n        self.assertTrue(nonexistent_dir.exists())\n", "    def test_notify_file_changed_ignores_nonexistent_files(self):\n        with mock.patch.object(self.reloader, 'should_stop', return_value=False):\n            self.reloader.notify_file_changed(self.nonexistent_file)\n", "    def test_notify_file_changed_ignores_nonexistent_files(self):\n        with mock.patch('django.utils.autoreload.BaseReloader.should_stop') as mocked_stop:\n            self.reloader.notify_file_changed(self.nonexistent_file)\n        self.assertFalse(mocked_stop.called)\n", "    def test_watch_dir_with_relative_path(self):\n        relative_path = Path('relative/path/to/file.py')\n        self.reloader.watch_dir(relative_path, '**/*.py')\n        watched_files = list(self.reloader.watched_files())\n        self.assertEqual(len(watched_files), 0)\n"], "sample_819": ["    def test_check_estimator_type(self, estimator_name, estimator):\n        # Check if correct error is raised when estimator is not a Classifier\n        eclf = VotingClassifier(estimators=[\n                (estimator_name, estimator)])\n        with pytest.raises(TypeError, match=\"All estimators in the ensemble should be classifiers\"):\n            eclf.fit(X, y)\n", "    def test_voting_classifier_empty_estimators():\n        # Check that VotingClassifier raises ValueError if estimators is empty\n        with pytest.raises(ValueError, match=\"At least one estimator is required.\"):\n          VotingClassifier([], voting='hard').fit(X, y)\n", "        def fit(self, X, y):\n            return self\n", "    def test_voting_classifier_none_estimator():\n\n        clf1 = LogisticRegression(random_state=123)\n        clf2 = RandomForestClassifier(random_state=123)\n\n        eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)], voting='hard', weights=[1, 0])\n\n        eclf.fit(X, y)\n        assert eclf.predict(X).shape == y.shape\n\n        eclf.set_params(rf=None)\n        eclf.fit(X, y)\n        assert eclf.predict(X).shape == y.shape\n", "    def test_pickle():\n        \"\"\"Check if VotingClassifier can be pickled and unpickled.\"\"\"\n\n        clf1 = LogisticRegression(random_state=123)\n        clf2 = RandomForestClassifier(random_state=123)\n        clf3 = GaussianNB()\n        eclf = VotingClassifier(estimators=[\n            ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n            voting='soft',\n            weights=[1, 2, 10])\n\n        eclf.fit(X, y)\n\n        # Pickle the classifier\n        import pickle\n        filename = 'voting_classifier.pkl'\n\n        with open(filename, 'wb') as f:\n            pickle.dump(eclf, f)\n\n        # Unpickle the classifier\n        with open(filename, 'rb') as f:\n            eclf_loaded = pickle.load(f)\n\n        # Check if the predictions are the same\n        assert_array_equal(eclf.predict(X), eclf_loaded.predict(X))\n        assert_array_almost_equal(eclf.predict_proba(X),\n                                  eclf_loaded.predict_proba(X))\n"], "sample_22": ["compilation error", "    def test_matrix_transpose():\n        m1 = np.arange(12).reshape((2, 2, 3))\n        assert np.allclose(matrix_transpose(m1), m1.transpose(0, 2, 1))\n", "    def test_matrix_transpose():\n        m1 = rotation_matrix(35 * u.deg, \"x\")\n        assert_allclose(matrix_transpose(m1), m1.T)\n        m2 = np.zeros((2, 3, 4, 3, 3))\n        for i in range(2):\n            for j in range(3):\n                for k in range(4):\n                    m2[i, j, k] = rotation_matrix(i * 10 * u.deg, \"x\")\n        assert_allclose(matrix_transpose(m2), m2.transpose((0, 1, 2, -1, -2)))\n", "compilation error", "    def test_matrix_transpose():\n        m1 = np.arange(6).reshape((2, 3))\n        assert_array_equal(matrix_transpose(m1), m1.T)\n        m2 = np.tile(m1, (2, 1, 1))\n        assert_array_equal(matrix_transpose(m2), np.transpose(m2, axes=(0, 2, 1)))\n\n"], "sample_640": ["def test_get_node_first_ancestor_of_type() -> None:\n    tree = astroid.extract_node(\n        \"\"\"\n            while True:\n                if True:\n                    pass\n        \"\"\"\n    )\n    assert utils.get_node_first_ancestor_of_type(tree.body[0].body[0].body[0], (nodes.If, nodes.FunctionDef)) == tree.body[0]\n    assert utils.get_node_first_ancestor_of_type(tree.body[0].body[0], (nodes.If, nodes.FunctionDef)) == tree.body[0]\n    assert utils.get_node_first_ancestor_of_type(tree.body[0], (nodes.If, nodes.Module)) == tree\n", "def test_is_node_in_guarded_import_block() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n    if TYPE_CHECKING:\n        from typing import List\n    else:\n        x = 1\n    List\n    \"\"\"\n    )\n    assert utils.is_node_in_guarded_import_block(code[0]) is True\n    assert utils.is_node_in_guarded_import_block(code[1]) is False\n", "def test_is_node_in_guarded_import_block() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n    if TYPE_CHECKING:\n        from typing import List\n    else:\n        from typing import Tuple\n    \n    list_var: List #@\n    tuple_var: Tuple #@\n\n    \"\"\"\n    )\n\n    assert utils.is_node_in_guarded_import_block(code[0]) is True\n    assert utils.is_node_in_guarded_import_block(code[1]) is True\n    assert utils.is_node_in_guarded_import_block(code[2]) is False\n\n", "def test_get_node_first_ancestor_of_type_and_its_child() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n    try:\n        pass #@\n    except ZeroDivisionError:\n        pass\n    finally:\n        pass\n    \"\"\"\n    )\n    ancestor, child = utils.get_node_first_ancestor_of_type_and_its_child(code[0], (nodes.TryFinally,))\n    assert isinstance(ancestor, nodes.TryFinally)\n    assert isinstance(child, nodes.Try)\n\n    ancestor, child = utils.get_node_first_ancestor_of_type_and_its_child(code[1], (nodes.TryFinally,))\n    assert ancestor is None\n    assert child is None\n\n    ancestor, child = utils.get_node_first_ancestor_of_type_and_its_child(code[2], (nodes.TryFinally,))\n    assert isinstance(ancestor, nodes.TryFinally)\n    assert isinstance(child, nodes.Finally)\n", "def test_get_node_first_ancestor_of_type():\n    code = astroid.extract_node(\n        \"\"\"\n    if True:\n        while True:\n            if True:\n                pass\n    \"\"\"\n    )\n    assert utils.get_node_first_ancestor_of_type(code[0], (nodes.If, nodes.While)) is None\n    assert isinstance(utils.get_node_first_ancestor_of_type(code[1], nodes.If), nodes.If)\n    assert isinstance(utils.get_node_first_ancestor_of_type(code[2], nodes.While), nodes.While)\n"], "sample_801": ["def test_empty_init():\n    # Render a estimator with an empty __init__\n    class EmptyInitEstimator(BaseEstimator):\n            pass\n\n    estimator = EmptyInitEstimator()\n    expected = \"\"\"EmptyInitEstimator()\"\"\"\n    assert estimator.__repr__() == expected\n", "def test_estimator_with_empty_dict():\n    # Test repr for estimator with an empty dict as a parameter\n    estimator = LogisticRegression(penalty={})\n    expected = \"\"\"", "def test_nested_estimator_types():\n    # Test rendering of nested estimators with different types\n    pipeline = make_pipeline(\n        StandardScaler(),\n        SelectKBest(chi2, k=2),\n        LogisticRegression(C=0.1, multi_class='ovr', solver='liblinear')\n    )\n    expected = \"\"\"", "    def test_empty_dict_params():\n        # Regression test: when estimator parameters are an empty dict,\n        # the repr should still be pretty.\n        estimator = LogisticRegression(penalty='l2', **{})\n        expected = \"\"\"", "def test_nested_pipeline():\n    # Ensure nested pipelines are rendered correctly\n    nested_pipeline = make_pipeline(\n        StandardScaler(),\n        make_pipeline(PCA(n_components=2), LogisticRegression())\n    )\n    expected = \"\"\""], "sample_1187": ["    def test_issue_19234_complex():\n        polygon = Polygon(Point(0, 0), Point(0, 1), Point(1, 1), Point(1, 0))\n        polys = [1, x, y, x*y, x**2*y, x*y**2, I]\n        assert polytope_integrate(polygon, polys) == \\\n            {1: 1, x: S.Half, y: S.Half, x*y: Rational(1, 4), x**2*y: Rational(1, 6), x*y**2: Rational(1, 6), I: 0} \n", "def test_hyperplane_parameters_robust():\n    # Test cases where the hyperplane parameters might be numerically unstable\n    # due to nearly parallel sides or very small values.\n    polygon = Polygon(Point(0, 0), Point(1e-10, 1e-10), Point(1, 0), Point(1, 1))\n    hp_params = hyperplane_parameters(polygon.sides)\n    assert len(hp_params) == len(polygon.sides)\n    for a, b in hp_params:\n        assert isinstance(a, (float, int))\n        assert isinstance(b, (float, int))\n\n", "    def test_polytope_integrate_empty():\n        assert polytope_integrate([], 1) == 0\n        assert polytope_integrate([], x) == 0 \n", "def test_empty_polygon():\n    polygon = Polygon([])\n    assert polytope_integrate(polygon, 1) == 0\n", "compilation error"], "sample_1058": ["compilation error", "compilation error", "compilation error", "def test_SymPyPrinter():\n    prntr = SymPyPrinter()\n    assert prntr.doprint(sign(x)) == 'sympy.sign(x)'\n\n    assert prntr.doprint(x**Rational(1, 2)) == 'sympy.sqrt(x)'\n    assert prntr.doprint(sqrt(x)) == 'sympy.sqrt(x)'\n\n", "compilation error"], "sample_152": ["    def test_fast_delete_on_proxy_model(self):\n        proxy = ProxyModel.objects.create()\n        proxy.delete()\n        self.assertFalse(ProxyModel.objects.exists())\n        self.assertFalse(BaseModel.objects.exists())\n", "    def test_fast_delete_no_fk_cascade(self):\n        r = R.objects.create()\n        s = S.objects.create(r=r)\n        # 1 query to delete r, 1 query to delete s\n        self.assertNumQueries(2, r.delete)\n        self.assertFalse(R.objects.exists())\n        self.assertFalse(S.objects.exists())\n", "    def test_fast_delete_filtered_related_objects(self):\n        origin = Origin.objects.create()\n        referrer1 = Referrer.objects.create(origin=origin, unique_field=1)\n        referrer2 = Referrer.objects.create(origin=origin, unique_field=2)\n        second_referrer1 = SecondReferrer.objects.create(referrer=referrer1)\n        second_referrer2 = SecondReferrer.objects.create(referrer=referrer2)\n        # We delete only the referrer with unique_field=1,\n        # and we expect only its related SecondReferrer to be deleted.\n        self.assertNumQueries(2, Referrer.objects.filter(unique_field=1).delete)\n        self.assertFalse(SecondReferrer.objects.filter(referrer=referrer1).exists())\n        self.assertTrue(SecondReferrer.objects.filter(referrer=referrer2).exists())\n\n", "    def test_fast_delete_related_without_on_delete_cascade(self):\n        u = User.objects.create()\n        a = Avatar.objects.create()\n        u.avatar = a\n        u.save()\n        # This should not fast-delete related objects as on_delete isn't set\n        # to CASCADE.\n        self.assertNumQueries(2, u.delete)\n", "    def test_fast_delete_with_signal_disconnect(self):\n        \n        origin = Origin.objects.create()\n        receiver_called = False\n\n            nonlocal receiver_called\n            receiver_called = True\n\n        signal = models.signals.pre_delete\n        signal.connect(receiver, sender=Referrer)\n        try:\n            # 1 query to select Referrer, 1 to delete Origin and related objects\n\n            # This should still result in only two queries, because\n            # even though a signal is connected, the fast delete\n            # optimization kicks in.\n            with self.assertNumQueries(2):\n                origin.delete()\n            self.assertTrue(receiver_called)\n\n        finally:\n            signal.disconnect(receiver, sender=Referrer)\n"], "sample_75": ["    def test_prefetch_related_with_queryset_filter(self):\n        with self.assertNumQueries(2):\n            books = BookWithYear.objects.filter(published_year=2010).prefetch_related(\n                Prefetch('aged_authors', queryset=AuthorWithAge.objects.filter(age__gt=60)),\n            )\n        with self.assertNumQueries(0):\n            self.assertEqual(list(books[0].aged_authors.all()), [self.author1])\n", "    def test_prefetching_through_manytomany_with_limit(self):\n        with self.assertNumQueries(2):\n            books = Book.objects.prefetch_related('authors').all()[:2]\n        self.assertEqual(len(books), 2)\n        for book in books:\n            self.assertEqual(len(book.authors.all()), 1)\n\n", "    def test_prefetch_related_from_subquery(self):\n        authors = AuthorWithAge.objects.filter(first_book__title='Candide').prefetch_related(\n            Prefetch(\n                'author',\n                queryset=Author.objects.prefetch_related(\n                    Prefetch('favorite_authors__first_book'),\n                ),\n            ),\n        )\n        with self.assertNumQueries(3):\n            # AuthorWithAge -> Author -> FavoriteAuthors, Book\n            self.assertQuerysetEqual(authors, ['<AuthorWithAge: Voltaire>'])\n", "    def test_prefetch_related_with_select_related(self):\n        with self.assertNumQueries(2):\n            authors = AuthorWithAge.objects.select_related('author').prefetch_related(\n                Prefetch('favorite_authors__first_book'),\n            )\n        with self.assertNumQueries(0):\n            self.assertQuerysetEqual(authors, ['<AuthorWithAge: Rousseau>', '<AuthorWithAge: Voltaire>'])\n", "    def test_prefetch_related_with_select_related(self):\n        authors = AuthorWithAge.objects.select_related('first_book').prefetch_related('favorite_authors__first_book')\n        with self.assertNumQueries(3):\n            self.assertQuerysetEqual(authors, ['<AuthorWithAge: Rousseau>', '<AuthorWithAge: Voltaire>'])\n"], "sample_222": ["    def test_file_lock_timeout(self):\n        file_path = Path(__file__).parent / 'test.png'\n        with open(file_path, 'rb') as f1:\n            self.assertIs(locks.lock(f1, locks.LOCK_EX), True)\n            with self.assertRaises(TimeoutError):\n                locks.lock(f1, locks.LOCK_EX | locks.LOCK_NB, timeout=0.1)\n            self.assertIs(locks.unlock(f1), True)\n", "    def test_file_move_safe_file_not_found(self):\n        handle_a, self.file_a = tempfile.mkstemp()\n        os.unlink(self.file_a)\n\n        with self.assertRaises(FileNotFoundError):\n            file_move_safe(self.file_a, self.file_b, allow_overwrite=True)\n\n        os.close(handle_a)\n\n", "    def test_file_iteration_with_multiple_chunks(self):\n        f = File(BytesIO(b'one\\ntwo\\nthree\\nfour\\nfive'))\n        f.DEFAULT_CHUNK_SIZE = 4\n        self.assertEqual(list(f), [b'one\\n', b'two\\n', b'thre', b'e\\n', b'four\\n', b'five'])\n", "    def test_file_iteration_with_binary_newline(self):\n        f = File(BytesIO(b'one\\x00two\\x00three'))\n        self.assertEqual(list(f), [b'one\\x00', b'two\\x00', b'three'])\n", "    def test_file_move_safe_source_missing(self):\n        handle_b, self.file_b = tempfile.mkstemp()\n        # Should raise FileNotFoundError if source doesn't exist\n        with self.assertRaises(FileNotFoundError):\n            file_move_safe('nonexistent_file', self.file_b, allow_overwrite=True)\n        os.close(handle_b)\n"], "sample_783": ["def test_imputation_constant_pandas_dataframe(X_constructor):\n    # Test imputation using the constant strategy on pandas DataFrame\n    pd = pytest.importorskip(\"pandas\")\n\n    data = {'col1': [np.nan, 1, 2, np.nan],\n            'col2': [4, np.nan, 6, 7],\n            'col3': [8, 9, np.nan, 11]}\n    df = pd.DataFrame(data)\n\n    X = X_constructor(df.values)\n\n    imputer = SimpleImputer(strategy=\"constant\", fill_value=0)\n    X_trans = imputer.fit_transform(X)\n\n    if isinstance(X, np.ndarray):\n        assert_allclose(X_trans, np.array([[0., 1., 2., 0.],\n                                          [4., 0., 6., 7.],\n                                          [8., 9., 0., 11.]]))\n    else:\n        assert_allclose(X_trans.toarray(), np.array([[0., 1., 2., 0.],\n                                                  [4., 0., 6., 7.],\n                                                  [8., 9., 0., 11.]]))\n", "    def test_imputation_object(self, strategy):\n        # Test imputation on a dataframe with object dtype\n        df = pd.DataFrame({'col1': ['a', 'b', 'c', 'a', np.nan],\n                           'col2': [1, 2, np.nan, 4, 5],\n                           'col3': [True, False, True, True, False]})\n        imputer = SimpleImputer(strategy=strategy)\n        df_imputed = imputer.fit_transform(df)\n\n        assert df_imputed.shape == (5, 3)\n\n        # Check for correct imputation (may require more specific assertions)\n        assert not pd.isna(df_imputed).any().any()\n", "def test_missing_indicator_no_missing_values(X):\n    indicator = MissingIndicator()\n    X_trans = indicator.fit_transform(X)\n    assert_array_equal(X_trans, np.zeros(X.shape))\n\n", "compilation error", "    def test_imputation_sparse_zero_based_indices(self, strategy):\n        X = sparse.csr_matrix([[1, np.nan, 3],\n                               [4, 5, np.nan]])\n\n        imputer = SimpleImputer(strategy=strategy)\n        X_trans = imputer.fit_transform(X)\n\n        assert_allclose_dense_sparse(X_trans.toarray(),\n                                     np.array([[1, 3, 3],\n                                               [4, 5, 3]]))\n"], "sample_897": ["    def test_partial_dependence_three_features_kind(\n        pyplot,\n        clf_diabetes,\n        diabetes,\n        features,\n        kind,", "    def test_partial_dependence_display_with_array_sample_weight(\n        pyplot,\n        clf_diabetes,\n        diabetes,", "    def test_partial_dependence_display_with_non_constant_sample_weight(\n        self,\n        pyplot,\n        clf_diabetes,\n        diabetes,\n        kind,", "    def test_partial_dependence_display_grid_resolution_shape(\n        self,\n        pyplot,\n        clf_diabetes,\n        diabetes,\n        features,\n        grid_resolution,\n        expected_shape,", "def test_partial_dependence_display_invalid_input_data(\n        pyplot, error_type,\n        input_data, clf_diabetes,\n    ):\n    \"\"\"Check that we raise an error when `input_data` is not a valid array-like\n    object.\"\"\"\n\n    with pytest.raises(error_type):\n        PartialDependenceDisplay.from_estimator(\n            clf_diabetes,\n            input_data,\n            features=[0, 1],\n            kind=\"average\",\n        )\n"], "sample_14": ["def test_angle_pickle_roundtrip_units():\n    a = Angle('10d10m', unit=u.degree)\n    pickled_a = pickle.dumps(a)\n    unpickled_a = pickle.loads(pickled_a)\n    assert unpickled_a.unit == u.degree\n    assert unpickled_a.value == a.value\n", "    def test_angle_pickle_to_string():\n        \"\"\"\n        Ensure that after pickling we can still do to_string on hourangle.\n\n        Regression test for gh-13923.\n        \"\"\"\n        angle = Angle(0.25 * u.hourangle)\n        expected = angle.to_string()\n        via_pickle = pickle.loads(pickle.dumps(angle))\n        via_pickle_string = via_pickle.to_string()  # This used to fail.\n        assert via_pickle_string == expected\n\n", "def test_angle_pickle_to_string_with_precision():\n    \"\"\"\n    Ensure that after pickling, to_string still works correctly with set precision.\n\n    Regression test for gh-13923.\n    \"\"\"\n    angle = Angle(0.25 * u.hourangle)\n    expected = angle.to_string(precision=2)\n    via_pickle = pickle.loads(pickle.dumps(angle))\n    via_pickle_string = via_pickle.to_string(precision=2)  # This used to fail.\n    assert via_pickle_string == expected\n", "def test_angle_invalid_string():\n    \"\"\"\n    Test that invalid string representations raise a ValueError.\n    \"\"\"\n    with pytest.raises(ValueError):\n        Angle('invalid-string')\n", "    def test_angle_pickle_to_string():\n        \"\"\"\n        Ensure that after pickling we can still do to_string on hourangle.\n\n        Regression test for gh-13923.\n        \"\"\"\n        angle = Angle(0.25 * u.hourangle)\n        expected = angle.to_string()\n        via_pickle = pickle.loads(pickle.dumps(angle))\n        via_pickle_string = via_pickle.to_string()  # This used to fail.\n        assert via_pickle_string == expected\n\n"], "sample_6": ["compilation error", "def test_to_string():\n    c = ICRS([1, 2]*u.deg, [3, 4]*u.deg)\n\n    assert str(c) == 'ICRS(ra=[1d00m00.000s, 2d00m00.000s], dec=[3d00m00.000s, 4d00m00.000s])'\n    assert repr(c) == 'ICRS(ra=[1., 2.] * degree, dec=[3., 4.] * degree)'\n\n    c = ICRS([180*u.deg, 360*u.deg], [0*u.deg, 90*u.deg])\n    assert str(c) == 'ICRS(ra=[180d00m00.000s, 360d00m00.000s], dec=[00d00m00.000s, 90d00m00.000s])'\n    assert repr(c) == 'ICRS(ra=[180., 360.] * degree, dec=[0., 90.] * degree)'\n", "def test_angle_format():\n    a1 = Angle(123.456789, unit=u.degree)\n\n    assert a1.format(precision=3).value == '123.457'\n\n    assert a1.format(u.arcsec).value == '44844.32561'\n\n    with pytest.raises(ValueError):\n        a1.format(precision=-1)\n\n    assert Angle(['123.456789', '234.56789'], unit=u.degree).format(precision=3) == ['123.457', '234.568']\n\n", "    def test_angle_wrap():\n        a = Angle([0, 360, 720, -90, -450], unit=u.deg)\n        # The angle wrap should only work on Angle objects\n        a_wrapped = a.wrap_at(180 * u.deg)\n        assert_allclose(a_wrapped.degree, [0, 0, 0, -90, -90])\n", "def test_latitude_and_longitude():\n    lat = Latitude([45.0, -90.0, 90.0], unit=u.deg)\n    lon = Longitude([45.0, 180.0, -180.0, 360.0], unit=u.deg)\n    assert lat.isscalar is False\n    assert lon.isscalar is False\n\n    with pytest.raises(TypeError):\n        Latitude(lon)\n    with pytest.raises(TypeError):\n        Longitude(lat)\n\n    # test wrapping for Longitude\n    lon2 = Longitude(lon.value + 360, unit=u.deg)\n    assert_allclose(lon2.value, lon.value)\n\n    # test wrap_angle for Longitude\n    lon_wrap180 = Longitude([190.0], unit=u.deg, wrap_angle=180.0*u.deg)\n    assert_allclose(lon_wrap180.value, [-170.0])\n\n    # Test that setting a Long to a list of lat's raises a TypeError\n    with pytest.raises(TypeError):\n       lon[0:2] = lat[0:2]\n"], "sample_675": ["    def test_log_config_overrides_ini(testdir):\n        log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n\n        testdir.makeini(\n            \"\"\"\n            [pytest]\n            log_file={}\n            log_file_level = INFO\n            \"\"\".format(\n                log_file\n            )\n        )\n        testdir.makeconftest(\n            \"\"\"\n            import logging\n            pytest_plugins = ['logging_plugin']\n\n                config.pluginmanager.log_file = 'override.log'\n                config.pluginmanager.log_file_level = 'WARNING'\n        \"\"\"\n        )\n        testdir.makepyfile(\n            \"\"\"\n            import logging\n                plugin = request.config.pluginmanager.getplugin('logging-plugin')\n                assert plugin.log_file_handler.baseFilename == 'override.log'\n                assert plugin.log_file_handler.level == logging.WARNING\n                logging.getLogger('catchlog').info(\"This log message won't be shown\")\n                logging.getLogger('catchlog').warning(\"This log message will be shown\")\n                print('PASSED')\n        \"\"\"\n        )\n\n        result = testdir.runpytest(\"-s\")\n\n        result.stdout.fnmatch_lines([\"test_log_config_overrides_ini.py PASSED\"])\n        assert os.path.isfile('override.log')\n        with open('override.log') as rfh:\n            contents = rfh.read()\n            assert \"This log message will be shown\" in contents\n            assert \"This log message won't be shown\" not in contents\n", "    def test_log_cli_level_after_test_collection(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n                pass\n\n        \"\"\"\n        )\n\n        result = testdir.runpytest(\"-q\", \"--log-cli-level=DEBUG\")\n        result.stdout.fnmatch_lines(\n            [\n                \"*test_log_cli_level_after_test_collection.py::test_simple*\",\n                \"*collected 1 item*\",\n                \"*test_log_cli_level_after_test_collection.py .*\"\n            ]\n        )\n", "def test_log_no_capture(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import sys\n\n            logger = logging.getLogger(__name__)\n            logger.info('message without capture')\n            assert 'message without capture' in caplog.text\n\n            # Check if sys.stderr is also receiving logs\n            # This will ensure that logging is not exclusively captured\n            handler = logging.StreamHandler(sys.stderr)\n            logger.addHandler(handler)\n            logger.info('message to stderr')\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_log_filter(testdir):\n    log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level = DEBUG\n        log_cli=true\n        filterloglevel=WARNING\n        \"\"\".format(\n            log_file\n        )\n    )\n    testdir.makepyfile(\n        \"\"\"\n            import logging\n            logger = logging.getLogger(__name__)\n\n                logger.debug(\"debug message\")\n                logger.info(\"info message\")\n                logger.warning(\"warning message\")\n                assert True\n\n        \"\"\"\n    )\n    testdir.runpytest()\n    with open(log_file) as rfh:\n        contents = rfh.read()\n        assert \"debug message\" not in contents\n        assert \"info message\" not in contents\n        assert \"warning message\" in contents\n", "def test_log_cli_level_with_pytest_log_level(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import logging\n            import pytest\n\n                plugin = request.config.pluginmanager.getplugin('logging-plugin')\n                assert plugin.log_cli_handler.level == logging.WARNING\n                logging.getLogger('catchlog').debug(\"This log message won't be shown\")\n                logging.getLogger('catchlog').warning(\"This log message will be shown\")\n                print('PASSED')\n        \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_cli=true\n        log_cli_level = INFO\n        \"\"\"\n    )\n    result = testdir.runpytest(\"-s\")\n\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_log_cli_level_with_pytest_log_level.py *\",\n            \"*-- live log call --*\",\n            \"*WARNING*This log message will be shown*\",\n            \"PASSED *100%*\",\n            \"=* 1 passed in *=\",\n        ]\n    )\n\n"], "sample_92": ["    def test_backend_path_login_with_backend_attribute(self):\n        user = User.objects.create_user(self.username, 'email', self.password)\n        user.backend = self.other_backend\n        self.client._login(user)\n        self.assertBackendInSession(self.other_backend)\n", "    def test_authenticate_custom_permissions_user(self):\n        user = CustomPermissionsUser.objects.create_user(\n            email='test@example.com',\n            password='test',\n            date_of_birth=date(2006, 4, 25)\n        )\n        authenticated_user = authenticate(email='test@example.com', password='test')\n        self.assertEqual(user, authenticated_user)\n", "    def test_rowlevel_backend_precedes_allowall_backend(self):\n        self.assertEqual(authenticate(username='test2', password='test'), self.user2)\n", "    def test_authenticate_custom_permissions_user(self):\n        test_user = CustomPermissionsUser.objects.create_user(\n            email='test@example.com',\n            password='test',\n            date_of_birth=date(2006, 4, 25)\n        )\n        authenticated_user = authenticate(email='test@example.com', password='test')\n        self.assertEqual(test_user, authenticated_user)\n\n", "    def test_authenticate_inactive_user(self):\n        self.assertFalse(self.user.is_active)\n        user = authenticate(**self.user_credentials)\n        self.assertEqual(user, self.user)\n        self.assertFalse(user.is_authenticated)\n"], "sample_767": ["compilation error", "def test_column_transformer_invalid_transformer_names():\n    ct = ColumnTransformer([('trans', Trans(), [0])], remainder='drop')\n    ct.fit([[0, 1, 2]])\n\n    # Check if accessing a non-existent transformer raises a KeyError\n    with pytest.raises(KeyError):\n        ct.named_transformers_['invalid_name']\n\n", "def test_column_transformer_callable_specifier_errors():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n        return [0, 1]\n\n    ct = ColumnTransformer([('trans', Trans(), func)],\n                           remainder='drop')\n    assert_raises(ValueError, ct.fit, X_array)\n    assert_raises(ValueError, ct.fit_transform, X_array)\n\n        return 'string'\n\n    ct = ColumnTransformer([('trans', Trans(), func)],\n                           remainder='drop')\n    assert_raises(TypeError, ct.fit, X_array)\n    assert_raises(TypeError, ct.fit_transform, X_array)\n\n", "def test_column_transformer_callable_specifier_pandas_index():\n    pd = pytest.importorskip('pandas')\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_df = pd.DataFrame(X_array, columns=['first', 'second'])\n    X_res_first = np.array([[0, 1, 2]]).T\n\n        assert_array_equal(X.index, X_df.index)\n        assert_array_equal(X.columns, X_df.columns)\n        assert_array_equal(X.values, X_df.values)\n        return ['first']\n\n    ct = ColumnTransformer([('trans', Trans(), func)],\n                           remainder='drop')\n    assert_array_equal(ct.fit_transform(X_df), X_res_first)\n    assert_array_equal(ct.fit(X_df).transform(X_df), X_res_first)\n    assert callable(ct.transformers[0][2])\n    assert ct.transformers_[0][2] == ['first']\n\n", "compilation error"], "sample_679": ["    def test_marker_expr_eval_failure_handling_no_match(testdir, expr):\n        foo = testdir.makepyfile( \"\"\"\n        import pytest\n\n        @pytest.mark.internal_err\n            pass\n        \"\"\")\n        expected = \"ERROR: Wrong expression passed to '-m': {}: *\".format(expr)\n        result = testdir.runpytest(foo, \"-m\", expr)\n        result.stderr.fnmatch_lines([expected])\n        assert result.ret == ExitCode.USAGE_ERROR\n", "    def test_marker_expr_eval_failure_handling_skip(testdir, expr):\n\n        foo = testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.skip(reason=\"This test should be skipped\")\n                pass\n            \"\"\"\n        )\n        expected = \"ERROR: Wrong expression passed to '-m': {}: *\".format(expr)\n        result = testdir.runpytest(foo, \"-m\", expr)\n        result.stderr.fnmatch_lines([expected])\n        assert result.ret == ExitCode.USAGE_ERROR\n", "    def test_marker_expr_eval_failure_handling_with_spaces(testdir):\n        foo = testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.internal_err\n                pass\n            \"\"\"\n        )\n        result = testdir.runpytest(foo, \"-m\", \" NOT  internal_err \")\n        result.stderr.fnmatch_lines([\"*ERROR: Wrong expression passed to '-m': NOT  internal_err:*\"])\n        assert result.ret == ExitCode.USAGE_ERROR\n\n", "    def test_marker_expression_complex(testdir):\n        testdir.makepyfile(\n            \"def test_func(): pass\"\n        )\n        result = testdir.runpytest(\"-m\", \"not (internal_err and skip and not foo)\")\n        result.assertoutcome(passed=1)\n", "    def test_marker_expr_with_spaces_and_parentheses(testdir):\n        foo = testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.internal_err\n                pass\n\n            @pytest.mark.another_err\n                pass\n            \"\"\"\n        )\n        result = testdir.runpytest(foo, \"-m\", \"internal_err and (another_err or internal_err)\")\n        result.assert_outcomes(passed=1, skipped=1)\n\n"], "sample_251": ["    def test_defer_alias_after_annotate(self):\n        qs = Book.objects.annotate(rating_double=F('rating') * 2).alias(\n            rating_alias=F('rating_double'),\n        ).defer('rating_double')\n        self.assertEqual(qs.first().rating_double, None)\n", "    def test_alias_with_aggregation_and_filter(self):\n        qs = Book.objects.alias(rating_sum=Sum('rating')).filter(rating_sum__gt=8)\n        self.assertIs(hasattr(qs.first(), 'rating_sum'), False)\n        self.assertSequenceEqual(qs, [self.b1])\n", "    def test_alias_across_relationships(self):\n        qs = Author.objects.alias(\n            num_books_alias=Count('book')\n        ).filter(book__rating__gt=4).annotate(\n            num_books=F('num_books_alias')\n        )\n        self.assertSequenceEqual(qs, [self.a1])\n", "    def test_alias_with_transform(self):\n        qs = Book.objects.alias(\n            rating_alias=F('rating') - 1,\n        ).annotate(\n            avg_rating=Window(\n                expression=Avg('rating'),\n                partition_by=['publisher'],\n            ).alias('avg_rating'),\n            \n        )\n        self.assertIs(hasattr(qs.first(), 'rating_alias'), False)\n", "    def test_alias_with_related_field(self):\n        qs = Author.objects.alias(\n            book_count_alias=Count('book__pk'),\n        ).filter(book_count_alias__gt=1).order_by('book_count_alias')\n        self.assertIs(hasattr(qs.first(), 'book_count_alias'), False)\n        self.assertQuerysetEqual(\n            qs, [self.a4, self.a1, self.a3], lambda a: a.book_count_alias\n        )\n"], "sample_872": ["    def test_average_precision_score_multiclass_with_ovr_ovo(multi_class):\n        y_true = np.array([0, 1, 2, 0, 1, 2])\n        y_score = np.array([[0.1, 0.5, 0.4], [0.8, 0.1, 0.1], [0.2, 0.3, 0.5],\n                           [0.1, 0.6, 0.3], [0.9, 0.05, 0.05], [0.3, 0.4, 0.3]])\n        with pytest.warns(FutureWarning):\n            score = average_precision_score(y_true, y_score, average='macro', multi_class=multi_class)\n", "    def test_empty_labels_binary(metric, expected_score):\n        y_true = np.array([])\n        y_score = np.array([])\n        assert metric(y_true, y_score) == expected_score\n", "def test_label_ranking_average_precision_score_perfect_score():\n    # Test that label_ranking_average_precision_score returns 1 for a perfect ranking\n    y_true = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    y_score = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    assert label_ranking_average_precision_score(y_true, y_score) == 1.0\n", "def test_top_k_accuracy_score_empty_array():\n    y_true = np.array([])\n    y_score = np.array([])\n    with pytest.raises(ValueError, match=\"y_true and y_score must have the same length.\"):\n        top_k_accuracy_score(y_true, y_score, k=2)\n", "    def test_label_ranking_average_precision_score_with_ties():\n        # Test that label_ranking_average_precision_score handles ties correctly\n        y_true = np.array([[0, 1, 0], [1, 0, 0]])\n        y_score = np.array([[0.5, 0.5, 0.0], [0.5, 0.5, 0.0]])\n        result = label_ranking_average_precision_score(y_true, y_score)\n        assert result == pytest.approx(0.5)\n"], "sample_948": ["def test_build_domain_cpp_no_escape(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"no-escape\")\n    assert len(ws) == 0\n", "def test_cpp_member_reference(app, warning):\n    text = (\n        \".. cpp:class:: MyClass\\n\"\n        \"\\n\"\n        \".. cpp:member:: MyClass::myVariable\\n\"\n        \"\\n\"\n        \".. cpp:function:: MyClass::myFunction(int arg)\\n\"\n    )\n    restructuredtext.parse(app, text)\n    ws = warning.getvalue().split(\"\\n\")\n    assert len(ws) == 0\n", "compilation error", "def test_parse_template_args_with_default_values(app, warning):\n    text = (\n        \".. cpp:function:: void f(int a = 42, const char\\* b = \\\"hello\\\")\\n\"\n    )\n    restructuredtext.parse(app, text)\n    ws = warning.getvalue().split(\"\\n\")\n    assert len(ws) == 0\n", "def test_anonymous_struct_union_enum(app, warning):\n    text = (\n        \".. cpp:struct:: {\\nint a;\\n}\\n\"\n        \".. cpp:union:: {\\nint a;\\n}\\n\"\n        \".. cpp:enum:: {\\nvalue1,\\nvalue2\\n}\\n\"\n    )\n    restructuredtext.parse(app, text)\n    ws = warning.getvalue().split(\"\\n\")\n    assert len(ws) == 0\n"], "sample_665": ["def test_collect_ignore_multiple(testdir):\n    \"\"\"Test multiple collect_ignore patterns, including globs.\"\"\"\n    tests = testdir.mkpydir(\"tests\")\n    tests.ensure(\"conftest.py\").write(\n        textwrap.dedent(\n            \"\"\"\n            collect_ignore = ['ignore_me', 'ignore_*']\n\n        \"\"\"\n        )\n    )\n\n    ignore_me = tests.mkdir(\"ignore_me\")\n    ignore_me.ensure(\"__init__.py\")\n    ignore_me.ensure(\"test_a.py\")\n    ignore_file = tests.mkdir(\"ignore_file\")\n    ignore_file.ensure(\"__init__.py\")\n\n    tests.ensure(\"test_ok.py\").write(\"def test_ok(): pass\")\n    result = testdir.runpytest()\n    assert result.ret == ExitCode.OK\n    result.stdout.fnmatch_lines([\"*1 passed in*\"])\n", "def test_collect_file_with_non_ascii_name(testdir):\n    fname = \"test_\u00e4\u00f6\u00fc.py\"\n    testdir.makepyfile(fname, \"def test_func(): pass\")\n    result = testdir.runpytest(fname)\n    result.stdout.fnmatch_lines([\"*1 passed in*\"])\n", "def test_collect_with_init_py_and_conftest(testdir):\n    sub = testdir.mkdir(\"sub\")\n    sub.ensure(\"__init__.py\")\n    sub.ensure(\"conftest.py\").write(\"def pytest_generate_tests(metafunc): pass\")\n    testdir.makepyfile(\n        \"def test_outside(): pass\"\n    )\n\n    result = testdir.runpytest(str(sub))\n    result.stdout.fnmatch_lines([\"*no tests ran in*\"])\n", "def test_collect_with_relative_paths(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n\n    with testdir.as_cwd():\n        result = testdir.runpytest(\"test_one.py\")\n    result.stdout.fnmatch_lines([\"*1 passed in*\"])\n    assert result.ret == 0\n", "    def test_collect_invalid_signature_message(testdir):\n        \"\"\"Check that we issue a proper message when we can't determine the signature of a test\n        function (#4026).\n        \"\"\"\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            class TestCase:\n                @pytest.fixture\n                    pass"], "sample_18": ["    def test_view(self):\n        q_view = self.q.view(np.ndarray)\n        assert_no_info(q_view)\n", "    def test_structured_quantity_change_unit(self):\n        q = self.q.copy()\n        q[\"p\"] = q[\"p\"].to(u.km)\n        assert_info_equal(q, self.q)\n\n", "    def test_structured_quantity_info_after_unit_change(self):\n        q = self.q.copy()\n        q.to(u.km, \"p\")\n        assert_info_equal(q, self.q, ignore={\"unit\"})\n", "    def test_view(self):\n        q_view = self.q.view(np.ndarray)\n        assert_no_info(q_view)\n", "    def test_masking(self):\n        mask = np.array([True, False])\n        q = self.q[mask]\n        assert_info_equal(q, self.q)\n"], "sample_523": ["def test_legend_fontsize_inheritance():\n    # Test that legend fontsize inherits from the axes if not explicitly set\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label=\"line\")\n    ax.tick_params(labelsize=12)\n\n    leg = ax.legend()\n\n    assert leg.get_texts()[0].get_fontsize() == 12\n", "def test_legend_title_fontproperties():\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='test')\n    font = FontProperties(size=16, weight='bold', family='monospace')\n    leg = ax.legend(title='Aardvark', title_fontproperties=font)\n    assert leg.get_title().get_fontsize() == 16\n    assert leg.get_title().get_weight() == 'bold'\n    assert leg.get_title().get_family() == 'monospace'\n", "def test_legend_markerscale_zero():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], marker='o', label='data')\n    leg = ax.legend(markerscale=0)\n    assert leg.get_lines()[0].get_markersize() == 0\n", "def test_legend_borderaxespad():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label='line')\n    leg = ax.legend()\n    leg.set_borderaxespad(2)\n    assert leg.get_borderaxespad() == 2\n\n", "def test_legend_handler_update_properties():\n    fig, ax = plt.subplots()\n    line, = ax.plot([0, 1], [0, 1], label='test', lw=3, color='red')\n    leg = ax.legend()\n    handler = leg.get_legend_handles_labels()[0][0].get_legend_handler()\n    handler.update_prop(line)\n    assert handler.artist.get_linewidth() == 3\n    assert handler.artist.get_color() == 'red'\n"], "sample_682": ["    def test_importorskip_with_reason(testdir):\n        with pytest.raises(\n            pytest.skip.Exception, match=\"^could not import 'doesnotexist': No module named .* 'Some reason'\"\n        ):\n            pytest.importorskip(\"doesnotexist\", reason=\"Some reason\")\n", "    def test_importorskip_with_reason(testdir):\n        with pytest.raises(pytest.skip.Exception) as excinfo:\n            pytest.importorskip(\"doesnotexist\", reason=\"This should be skipped\")\n        assert \"This should be skipped\" in str(excinfo.value)\n", "    def test_relpath_rootdir(testdir):\n        testdir.makepyfile(\n            \"\n            import pytest\n            @pytest.mark.skipif('sys.platform == \"win32\"')\n                pass\n            \"\n        )\n        result = testdir.runpytest(\"-rs\", \"--rootdir=.\", \"-v\")\n        result.stdout.fnmatch_lines(\n            [\"SKIPPED [[]1[]] test_relpath_rootdir.py:2: sys.platform == \\\"win32\\\"\"]\n        )\n\n", "    def test_skip_reason_message(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            @pytest.mark.skip(reason=\"This test is skipped.\")\n                pass\n        \"\"\"\n        )\n\n        result = testdir.runpytest(\"-rs\")\n        result.stdout.fnmatch_lines([\"*SKIP*This test is skipped.*\"])\n", "    def test_importorskip_reason(testdir):\n        with pytest.raises(\n            pytest.skip.Exception,\n            match=\"^could not import 'doesnotexist': No module named .*reason: I need it.*\",\n        ):\n            pytest.importorskip(\"doesnotexist\", reason=\"I need it\")\n"], "sample_311": ["    def test_final_catch_all_view_disabled(self):\n        with override_settings(APPEND_SLASH=True):\n            superuser = User.objects.create_superuser(\n                username='super',\n                password='secret',\n                email='super@example.com',\n            )\n            self.client.force_login(superuser)\n            known_url = reverse('admin:admin_views_article_changelist')\n            response = self.client.get(known_url[:-1], HTTP_HOST='example.com')\n            self.assertEqual(response.status_code, 404)\n", "    def test_non_admin_url_404_if_authenticated(self):\n        superuser = User.objects.create_superuser(\n            username='super',\n            password='secret',\n            email='super@example.com',\n        )\n        self.client.force_login(superuser)\n        unknown_url = '/unknown/'\n        response = self.client.get(unknown_url)\n        self.assertEqual(response.status_code, 404)\n", "    def test_missing_slash_append_slash_false_without_final_catch_all_view_non_admin(self):\n        superuser = User.objects.create_user(\n            username='staff',\n            password='secret',\n            email='staff@example.com',\n            is_staff=True,\n        )\n        self.client.force_login(superuser)\n        known_url = reverse('non_admin10')\n        response = self.client.get(known_url[:-1])\n        self.assertEqual(response.status_code, 404)\n", "    def test_admin_site_final_catch_all_view_with_custom_login_url(self):\n        site = AdminSite(name='my_admin')\n        site.login_url = '/my/custom/login/'\n        admin_url = reverse('my_admin:admin_views_article_changelist')\n        response = self.client.get(admin_url[:-1])\n        self.assertRedirects(response, '%s?next=%s' % (site.login_url, admin_url))\n", "    def test_unknown_url_with_trailing_slash_if_not_authenticated_without_final_catch_all_view(self):\n        url = reverse('admin10:article_extra_json') + '/'\n        response = self.client.get(url)\n        self.assertRedirects(response, '%s?next=%s' % (reverse('admin10:login'), url[:-1]))\n"], "sample_273": ["    def test_unique_together_with_auto_created_pk(self):\n        class Model(models.Model):\n            name = models.CharField(max_length=100)\n\n            class Meta:\n                unique_together = ('name', )\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Warning(self.msg, hint=self.hint, obj=Model, id='models.W042'),\n        ])\n\n", "    def test_app_default_auto_field_warning(self, apps):\n        class ModelWithoutPkViaAppConfig(models.Model):\n            class Meta:\n                app_label = 'check_framework'\n\n        self.assertEqual(checks.run_checks(app_configs=apps.get_app_configs()), [\n            Warning(self.msg, hint=self.hint, obj=ModelWithoutPkViaAppConfig, id='models.W042'),\n        ])\n\n", "    def test_auto_now_add_with_default_is_now(self):\n        class Model(models.Model):\n            created_at = models.DateTimeField(auto_now_add=True)\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [])\n", "    def test_app_default_auto_field_inheritance(self, apps):\n        class ParentModel(models.Model):\n            class Meta:\n                app_label = 'check_framework.apps.CheckPKConfig'\n\n        class ChildModel(ParentModel):\n            pass\n\n        self.assertEqual(checks.run_checks(app_configs=apps.get_app_configs()), [])\n", "    def test_abstract_model_inherited_pk(self):\n        class AbstractModel(models.Model):\n            pass\n\n        class Model(AbstractModel):\n            pass\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Warning(self.msg, hint=self.hint, obj=AbstractModel, id='models.W042'),\n        ])\n\n"], "sample_893": ["compilation error", "compilation error", "    def test_plot_tree_filled(pyplot):\n        # Check if filled option works correctly\n        clf = DecisionTreeClassifier(max_depth=2, random_state=0)\n        clf.fit(X, y)\n\n        nodes = plot_tree(clf, filled=True)\n        for node in nodes:\n            assert node.get_facecolor() != 'none' \n", "compilation error", "def test_plot_tree_errors():\n    clf = DecisionTreeClassifier()\n\n    # Test with empty input\n\n    message = r\"DecisionTreeClassifier object is not fitted yet.\"\n    with pytest.raises(NotFittedError, match=message):\n        plot_tree(clf)\n\n    clf_reg = DecisionTreeRegressor()\n\n    # Test with empty input\n    message = r\"DecisionTreeRegressor object is not fitted yet.\"\n    with pytest.raises(NotFittedError, match=message):\n        plot_tree(clf_reg)\n\n    clf.fit(X, y)\n\n    with pytest.raises(TypeError, match=\"`feature_names` must be a list\"):\n        plot_tree(clf, feature_names=np.array([\"a\", \"b\"]))\n    with pytest.raises(TypeError, match=\"`class_names` must be a list\"):\n        plot_tree(clf, class_names=np.array([\"a\", \"b\"]))\n\n    # Check error when argument is not an estimator\n\n    message = \"is not an estimator instance\"\n    with pytest.raises(TypeError, match=message):\n        plot_tree(clf.fit(X, y).tree_)\n"], "sample_245": ["    def test_ignore_file_with_glob_patterns(self):\n        out, po_contents = self._run_makemessages(ignore_patterns=[\n            'templates/ignore_*.html',\n        ])\n        self.assertIn(\"ignoring file templates/ignore_test.html\", out)\n        self.assertNotMsgId('This should be ignored because of glob pattern.', po_contents)\n", "    def test_makemessages_no_settings_with_locale_path(self):\n        with override_settings(LOCALE_PATHS=['some/path']):\n            out, err = self.run_django_admin(['makemessages', '-l', 'en', '-v', '0'])\n            self.assertNoOutput(err)\n            self.assertNoOutput(out)\n", "    def test_makemessages_with_custom_domain(self):\n        management.call_command('makemessages', locale=[LOCALE], domain='my_custom_domain', verbosity=0)\n        self.assertTrue(os.path.exists('locale/%s/LC_MESSAGES/my_custom_domain.po' % LOCALE))\n        with open('locale/%s/LC_MESSAGES/my_custom_domain.po' % LOCALE) as fp:\n            po_contents = fp.read()\n            self.assertIn('msgid \"Translatable literal #1\"', po_contents)\n            self.assertIn('msgid \"Translatable literal #6b\"', po_contents)\n", "    def test_makemessages_with_custom_domain(self):\n        os.environ['DJANGO_SETTINGS_MODULE'] = 'test_project.settings'\n        management.call_command('makemessages', locale=[LOCALE], domain='custom', verbosity=0)\n        self.assertTrue(os.path.exists('locale/%s/LC_MESSAGES/custom.po' % LOCALE))\n        with open('locale/%s/LC_MESSAGES/custom.po' % LOCALE) as fp:\n            po_contents = fp.read()\n            self.assertMsgId('Translatable literal #1', po_contents)\n\n", "    def test_makemessages_no_settings_with_locale_paths(self):\n        os.environ['DJANGO_SETTINGS_MODULE'] = ''\n        with override_settings(LOCALE_PATHS=['locale']):\n            out, err = self.run_django_admin(['makemessages', '-l', 'en', '-v', '0'])\n            self.assertNoOutput(err)\n            self.assertNoOutput(out)\n            self.assertTrue(os.path.exists('locale/en/LC_MESSAGES/django.po'))\n"], "sample_0": ["def test_conversion_to_from_variance_not_supported(UncertClass):\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = UncertClass(uncert)\n    with pytest.raises(TypeError):\n        var_uncert = start_uncert.represent_as(VarianceUncertainty)\n", "def test_conversion_to_from_variance_not_supported(UncertClass):\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = UncertClass(uncert)\n    with pytest.raises(TypeError):\n        var_uncert = start_uncert.represent_as(VarianceUncertainty)\n", "def test_conversion_to_from_variance_not_supported(UncertClass):\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = UncertClass(uncert)\n    with pytest.raises(TypeError):\n        var_uncert = start_uncert.represent_as(VarianceUncertainty)\n", "def test_conversion_to_from_variance_not_supported(UncertClass):\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = UncertClass(uncert)\n    with pytest.raises(TypeError):\n        var_uncert = start_uncert.represent_as(VarianceUncertainty)\n", "def test_conversion_to_from_variance_not_supported(UncertClass):\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = UncertClass(uncert)\n    with pytest.raises(TypeError):\n        var_uncert = start_uncert.represent_as(VarianceUncertainty)\n"], "sample_321": ["    def test_csrf_token_on_500_stays_constant(self):\n        response = self.client.get('/causes_error/')\n        self.assertEqual(response.status_code, 500)\n        token1 = response.cookies.get(settings.CSRF_COOKIE_NAME).value\n        response = self.client.get('/causes_error/')\n        self.assertEqual(response.status_code, 500)\n        token2 = response.cookies.get(settings.CSRF_COOKIE_NAME).value\n        self.assertEqual(token1, token2)\n", "    def test_bad_csrf_cookie_missing(self):\n        \"\"\"\n        If the CSRF cookie is missing in a POST request, the middleware\n        rejects the incoming request.\n        \"\"\"\n        self._check_bad_or_missing_cookie(None, 'CSRF cookie not found.')\n", "    def test_csrf_token_on_404_stays_constant_after_post(self):\n        response = self.client.post('/does not exist/', data={'key': 'value'})\n        # The error handler returns status code 599.\n        self.assertEqual(response.status_code, 599)\n        token1 = response.content\n        response = self.client.get('/does not exist/')\n        self.assertEqual(response.status_code, 599)\n        token2 = response.content\n        self.assertTrue(equivalent_tokens(token1.decode('ascii'), token2.decode('ascii')))\n\n", "    def test_csrf_cookie_secure_flag(self):\n        req = self._get_GET_no_csrf_cookie_request()\n        mw = CsrfViewMiddleware(token_view)\n        mw.process_view(req, token_view, (), {})\n        resp = mw(req)\n        self.assertTrue(resp.cookies[settings.CSRF_COOKIE_NAME]['secure'])\n", "    def test_csrf_token_on_500_stays_constant(self):\n        response = self.client.get('/raises_error/')\n        self.assertEqual(response.status_code, 500)\n        token1 = response.content\n        response = self.client.get('/raises_error/')\n        self.assertEqual(response.status_code, 500)\n        token2 = response.content\n        self.assertTrue(equivalent_tokens(token1.decode('ascii'), token2.decode('ascii')))\n"], "sample_633": ["def test_ignore_comments_and_docstrings() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--ignore-comments\", \"--ignore-docstrings\", SIMILAR1, SIMILAR2])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == (\n            f\"\"\"", "def test_ignore_signatures_partial_match() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--ignore-signatures\", SIMILAR5, SIMILAR6])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == \"\"\"", "def test_duplicates() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--duplicates=3\", SIMILAR1, SIMILAR2])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == \"\"\"", "def test_ignore_comments_and_docstrings() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--ignore-comments\", \"--ignore-docstrings\", SIMILAR1, SIMILAR2])\n    assert ex.value.code == 0\n\n", "def test_ignore_comments_and_signatures() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--ignore-comments\", \"--ignore-signatures\", SIMILAR5, SIMILAR6])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == \"\"\""], "sample_491": ["    def test_custom_renderer_field_template_name_with_widget(self):\n        class Person(Form):\n            first_name = CharField(widget=TextInput(attrs={'class': 'special'}))\n\n        get_default_renderer.cache_clear()\n        t = Template(\"{{ form.first_name.as_field_group }}\")\n        html = t.render(Context({\"form\": Person()}))\n        expected = \"\"\"\n        <label for=\"id_first_name\">First name:</label>\n        <p>Custom Field<p>\n        <input type=\"text\" class=\"special\" name=\"first_name\" required id=\"id_first_name\">\n        \"\"\"\n        self.assertHTMLEqual(html, expected)\n        get_default_renderer.cache_clear()\n\n", "    def test_multiple_errors_same_field(self):\n        class MyForm(Form):\n            name = CharField(max_length=50)\n\n        f = MyForm({'name': 'x' * 100})\n        self.assertEqual(\n            f.errors['name'],\n            [\n                'Ensure this value has at most 50 characters (it has 100).',\n                'Ensure this value has at most 50 characters (it has 100).',\n            ],\n        )\n", "    def test_hidden_field_widget_override(self):\n        class MyHiddenField(HiddenInput):\n            template_name = \"forms_tests/custom_hidden_field.html\"\n\n        class MyForm(Form):\n            hidden_field = CharField(widget=MyHiddenField)\n\n        f = MyForm()\n        self.assertHTMLEqual(\n            f.render(),\n            '<input type=\"hidden\" name=\"hidden_field\" id=\"id_hidden_field\" '\n            'value=\"\">Hidden Value',\n        )\n", "    def test_error_class_inheritance(self):\n        class CustomErrorList(ErrorList):\n            template_name = \"forms_tests/error.html\"\n\n        class MyForm(Form):\n            name = CharField()\n            error_class = CustomErrorList\n\n        f = MyForm({\"name\": \"\"})\n        self.assertHTMLEqual(\n            f.as_p(),\n            '<p><input type=\"text\" name=\"name\" required><ul class=\"errorlist\">'\n            '<li>This field is required.</li></ul></p>',\n        ) \n", "    def test_hidden_field_rendering(self):\n        class HiddenForm(Form):\n            data = IntegerField(widget=HiddenInput)\n\n        f = HiddenForm({})\n        self.assertHTMLEqual(\n            f.as_p(),\n            '<input type=\"hidden\" name=\"data\" id=\"id_data\">'\n        )\n        self.assertHTMLEqual(\n            f.as_table(),\n            '<tr><td><input type=\"hidden\" name=\"data\" id=\"id_data\"></td></tr>',\n        )\n"], "sample_546": ["def test_rubberband():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1])\n    rubberband = RubberbandBase(ax)\n    event = MouseEvent('button_press_event', fig.canvas, x=10, y=10)\n    rubberband.press(event)\n    assert rubberband.start == (10, 10)\n\n", "def test_rubberband_rectangle():\n    fig, ax = plt.subplots()\n    rect = ax.add_patch(plt.Rectangle((0.2, 0.2), 0.3, 0.4))\n    rubberband = RubberbandBase()\n    rubberband.press(fig.canvas, ax, (0.1, 0.1))\n    rubberband.drag(fig.canvas, ax, (0.4, 0.5))\n    rubberband.release(fig.canvas, ax, (0.4, 0.5))\n    assert rect.get_xy() == (0.1, 0.1)\n    assert rect.get_width() == 0.3\n    assert rect.get_height() == 0.4\n", "def test_rubberband_update_props():\n    # Regression: Ensure RubberbandBase.update_props() correctly updates the\n    # artist properties even when the RubberbandBase instance is not updated\n    # in the canvas's toolmanager.\n\n    fig, ax = plt.subplots()\n    rubberband = RubberbandBase(ax, 'draw', **{'color': 'red'})\n    fig.canvas.draw()  # Trigger initial update\n    assert rubberband.artist.get_color() == 'red'\n    rubberband.artist.set_color('blue')  # Directly change artist properties\n\n\n    # This should not change the RubberbandBase artist color\n    rubberband.update_props(**{'color': 'green'})\n    assert rubberband.artist.get_color() == 'blue'\n\n", "    def test_toolmanager_reset_keymap(self):\n        with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n            plt.rcParams['toolbar'] = 'toolmanager'\n        fig = plt.gcf()\n        fig.canvas.manager.toolmanager.update_keymap('pan', 'x')\n        fig.canvas.manager.toolmanager.reset_keymap()\n        assert 'x' not in fig.canvas.manager.toolmanager.get_tool_keymap('pan')\n", "def test_toolmanager_add_tool():\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    fig = plt.gcf()\n    initial_len = len(fig.canvas.manager.toolmanager.tools)\n    class TestTool(ToolBase):\n        name = 'testtool'\n    fig.canvas.manager.toolmanager.add_tool(TestTool)\n    assert len(fig.canvas.manager.toolmanager.tools) == initial_len + 1\n    assert 'testtool' in fig.canvas.manager.toolmanager.tools\n    with pytest.raises(ValueError):\n        fig.canvas.manager.toolmanager.add_tool(TestTool)\n\n"], "sample_749": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_707": ["def test_nodeid_separator_handling():\n    \"\"\"Ensure nodeid handles separators correctly.\"\"\"\n    nodeid = \"a::b:c/d::e\"\n    parts = nodes.split_nodeid(nodeid)\n    assert parts == [\"a\", \"b:c/d\", \"e\"]\n", "def test_get_fslocation_from_item(pytester: Pytester) -> None:\n    item = pytester.getitem(\"def test_func(): pass\")\n    location = nodes.get_fslocation_from_item(item)\n    assert location == (pytester.path.join(\"test_func.py\"), 1)\n", "def test_node_repr_failure_style_handles_fulltrace(pytester: Pytester) -> None:\n    \"\"\"Test repr_failure handles style \"long\" even with fulltrace.\"\"\"\n    p = pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            assert False\n    \"\"\"\n    )\n    with pytester.wrap_session(config=pytester.parseconfigure( \"-vv\")) as session:\n        item = session._node_for_item(p.join(\"test_fail\"))\n        excinfo = pytest.ExceptionInfo.from_excinfo(\n            ExceptionInfo.from_traceback(\n                traceback.TracebackException(\n                    __traceback__=(),\n                    exception=AssertionError(),\n                ),\n            )\n        )\n        item._prunetraceback(excinfo)\n        result = item.repr_failure(excinfo, style=\"long\")\n\n     assert result.lines[0].startswith(\"AssertionError: \")  \n", "def test_node_repr_failure_tbstyle_fulltrace(pytester: Pytester) -> None:\n    \"\"\"Test that repr_failure with 'fulltrace' option shows full traceback.\"\"\"\n    p = pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            raise ValueError(\"oops\")\n    \"\"\"\n    )\n    result = pytester.runpytest('-vv', '--fulltrace')\n    result.stdout.fnmatch_lines(\n        [\n            \"*Traceback*\",\n            \"ValueError: oops*\",\n        ]\n    )\n", "def test_node_repr_failure_fulltrace_no_prune(pytester: Pytester) -> None:\n    \"\"\"\n    Test repr failure when fulltrace is enabled and prunetraceback is already handled\n    \"\"\"\n    p = pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            assert False\n    \"\"\"\n    )\n    result = pytester.runpytest(\"--fulltrace\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_fail_fulltrace*\",\n            \"* AssertionError*\",\n            \"*   assert False*\",\n        ]\n    )\n"], "sample_935": ["    def check(target):\n        class Config:\n            cpp_id_attributes = [\"id_attr\"]\n            cpp_paren_attributes = [\"paren_attr\"]\n        parser = DefinitionParser(target, location=None,\n                                  config=Config())\n        ast, isShorthand = parser.parse_xref_object()\n        parser.assert_end()", "    def check_index_entry(app, text, expected_entries):\n        doctree = restructuredtext.parse(app, text)\n        index_entries = [entry for node in doctree.traverse(addnodes.index) for entry in node['entries']]\n        assert index_entries == expected_entries\n\n", "    def check(target):\n        class Config:\n            cpp_id_attributes = [\"id_attr\"]\n            cpp_paren_attributes = [\"paren_attr\"]\n        parser = DefinitionParser(target, location=None,\n                                  config=Config())\n        ast, isShorthand = parser.parse_xref_object()\n        parser.assert_end()", "    def check(text, file, expected):\n        \"\"\"Check index entries for a specific text block.\"\"\"\n        app.builder.build_all()\n        content = (app.outdir / file).read_text()\n        # Simple substring check for now, should be improved\n        assert expected in content, (\n            f\"Expected substring '{expected}' not found in '{file}' output:\\n\\n{content}\")\n", "compilation error"], "sample_612": ["    def test_resample_groupby_apply(self):\n        times = pd.date_range(\"2000-01-01\", freq=\"6H\", periods=10)\n        ds = Dataset(\n            {\n                \"foo\": ([\"time\", \"x\", \"y\"], np.random.randn(10, 5, 3)),\n                \"bar\": (\"time\", np.random.randn(10), {\"meta\": \"data\"}),\n                \"time\": times,\n            }\n        )\n        \n            return ds.mean()\n        \n        expected = ds.groupby(\"time\").apply(mean_by_group)\n        actual = ds.resample(time=\"1D\").apply(mean_by_group)\n        assert_identical(expected, actual) \n", "    def test_resample_empty_dataset(self):\n        ds = Dataset()\n        with pytest.raises(ValueError, match=\"cannot resample an empty Dataset\"):\n            ds.resample(time=\"1D\")\n", "    def test_resample_empty_dataset(self):\n        ds = xr.Dataset()\n        actual = ds.resample(time=\"1D\").mean()\n        assert_identical(ds, actual)\n", "    def test_resample_upsample_median(self):\n        times = pd.date_range(\"2000-01-01\", freq=\"6H\", periods=5)\n        ds = Dataset({\"foo\": ([\"time\", \"x\", \"y\"], np.random.randn(5, 5, 3)), \"time\": times})\n\n        actual = ds.resample(time=\"1H\").median()\n        expected = ds.reindex(\n            time=pd.date_range(\"2000-01-01\", freq=\"1H\", periods=20)\n        ).median(\"time\")\n        assert_identical(expected, actual)\n\n", "    def test_resample_upsample_with_multiindex(self):\n        data = np.arange(12).reshape(4, 3)\n        times = pd.date_range(\"2000-01-01\", freq=\"3D\", periods=4)\n        index = pd.MultiIndex.from_arrays([[\"a\", \"a\", \"b\", \"b\"], times])\n        ds = xr.Dataset({\"foo\": (index, data)}, coords={\"time\": times})\n        actual = ds.resample(time=\"D\").ffill()\n        # expected is complex, make sure it works as intended\n        expected = ds.reindex(\n            time=pd.date_range(\"2000-01-01\", freq=\"D\", periods=12),\n            method=\"ffill\",\n        )\n        assert_identical(expected, actual)\n\n"], "sample_621": ["    def test_from_dataarray(self, indexes) -> None:\n        indexes_from_da = Indexes.from_dataarray(\n            xr.DataArray(\n                np.random.rand(3, 3),\n                coords={\"x\": indexes[\"x\"], \"y\": indexes[\"y\"]},\n                dims=[\"x\", \"y\"],\n            ),\n            {\"z\": indexes[\"z\"]},\n        )\n        assert indexes_from_da == indexes\n", "    def test_to_pandas_indexes_empty(self, indexes) -> None:\n        empty_indexes = Indexes({}, {})\n        assert empty_indexes.to_pandas_indexes() == empty_indexes\n\n", "    def test_drop(self, indexes):\n        expected = Indexes(\n            {\"y\": indexes[\"y\"], \"z\": indexes[\"z\"], \"one\": indexes[\"one\"], \"two\": indexes[\"two\"]},\n            {k: v for k, v in indexes.variables.items() if k not in [\"x\"]},\n        )\n        actual = indexes.drop([\"x\"])\n        assert actual.equals(expected)\n\n        expected = Indexes(\n            {\"x\": indexes[\"x\"], \"z\": indexes[\"z\"], \"one\": indexes[\"one\"], \"two\": indexes[\"two\"]},\n            {k: v for k, v in indexes.variables.items() if k not in [\"y\"]},\n        )\n        actual = indexes.drop([\"y\"])\n        assert actual.equals(expected)\n", "    def test_concat_different_dtypes(self, indexes_and_vars) -> None:\n        x_idx, y_idx, z_midx = indexes_and_vars[0]\n        \n        new_x_idx = PandasIndex(pd.Index([\"a\", \"b\", \"c\"], name=\"x\"), \"x\", coord_dtype='U1')\n        \n        with pytest.raises(TypeError, match=r\"Cannot concatenate indexes with different dtypes\"):\n            Indexes([x_idx, new_x_idx, y_idx, z_midx], indexes_and_vars[1])\n", "    def test_from_variables_empty(self, indexes_and_vars) -> None:\n        _, variables = indexes_and_vars\n        with pytest.raises(ValueError, match=r\".*cannot create indexes from empty.*\"):\n            Indexes.from_variables({})\n"], "sample_398": ["    def test_password_change_nonexistent_user(self):\n        password_change_url = reverse(\n            \"custom_user_admin:auth_user_password_change\", args=(\"nonexistent_pk\",)\n        )\n        response = self.client.post(\n            password_change_url,\n            {\"password1\": \"password1\", \"password2\": \"password1\"},\n        )\n        self.assertEqual(response.status_code, 404)\n", "    def test_user_change_password_permissions(self):\n        u = UUIDUser.objects.create_user(username=\"uuid\", email=\"foo@bar.com\", password=\"test\")\n\n        # Check superuser can change password\n        self.assertTrue(self.client.login(username=\"uuid\", password=\"test\"))\n        user_change_url = reverse(\n            \"custom_user_admin:auth_tests_uuiduser_change\", args=(u.pk,)\n        )\n        response = self.client.get(user_change_url)\n        self.assertEqual(response.status_code, 200)\n\n        # Log out superuser\n        self.client.logout()\n\n        # Create a normal user\n        normal_user = UUIDUser.objects.create_user(\n            username=\"normaluser\", email=\"normal@bar.com\", password=\"test\"\n        )\n        self.assertTrue(self.client.login(username=\"normaluser\", password=\"test\"))\n        \n        #Normal user should not be able to access the password change page\n        response = self.client.get(\n            reverse(\n                \"custom_user_admin:auth_user_password_change\", args=(u.pk,)\n            )\n        )\n        self.assertEqual(response.status_code, 403)\n", "    def test_user_change_password_with_incorrect_password(self):\n        u = UUIDUser.objects.create_superuser(\n            username=\"uuid\", email=\"foo@bar.com\", password=\"test\"\n        )\n        self.assertTrue(self.client.login(username=\"uuid\", password=\"test\"))\n\n        user_change_url = reverse(\n            \"custom_user_admin:auth_tests_uuiduser_change\", args=(u.pk,)\n        )\n        password_change_url = reverse(\n            \"custom_user_admin:auth_user_password_change\", args=(u.pk,)\n        )\n        response = self.client.post(\n            password_change_url,\n            {\n                \"password1\": \"wrongpassword\",\n                \"password2\": \"wrongpassword\",\n            },\n        )\n        self.assertContains(response, \"The passwords don't match.\")\n", "    def test_user_change_permissions(self):\n        u = UUIDUser.objects.create_user(\n            username=\"uuid\", email=\"foo@bar.com\", password=\"test\"\n        )\n        self.assertTrue(self.client.login(username=\"uuid\", password=\"test\"))\n\n        user_change_url = reverse(\n            \"custom_user_admin:auth_tests_uuiduser_change\", args=(u.pk,)\n        )\n        response = self.client.get(user_change_url)\n        self.assertEqual(response.status_code, 200)\n\n        # Test that the user permissions can be changed\n        data = {\n            \"user_permissions\": [\n                str(get_perm(UUIDUser, \"add_uuiduser\").pk),\n                str(get_perm(UUIDUser, \"change_uuiduser\").pk),\n            ],\n        }\n        response = self.client.post(user_change_url, data)\n        self.assertRedirects(response, user_change_url)\n        row = LogEntry.objects.latest(\"id\")\n        self.assertEqual(row.get_change_message(), \"Changed permissions.\")\n", "    def test_user_change_password_permissions(self):\n        u = UUIDUser.objects.create_user(\n            username=\"user\", email=\"user@example.com\", password=\"password\"\n        )\n        self.assertTrue(self.client.login(username=\"user\", password=\"password\"))\n        \n        user_change_url = reverse(\n            \"custom_user_admin:auth_tests_uuiduser_change\", args=(u.pk,)\n        )\n        \n        password_change_url = reverse(\n            \"custom_user_admin:auth_user_password_change\", args=(u.pk,)\n        )\n\n        # User without change permission should see a 403 when trying to access the password change page\n        self.client.logout()\n        self.assertFalse(self.client.login(username=\"staffmember\", password=\"password\"))\n        response = self.client.get(password_change_url, follow=True)\n        self.assertEqual(response.status_code, 403)\n\n        # User with change permission should be able to access the password change page\n        self.client.logout()\n        self.assertTrue(self.client.login(username=\"superuser\", password=\"password\"))\n        response = self.client.get(password_change_url)\n        self.assertEqual(response.status_code, 200)\n"], "sample_1171": ["compilation error", "def test_issue_18400b():\n    n = Symbol('n', integer=True)\n    raises(ValueError, lambda: imageset(lambda x: x*2, Range(n, oo)))\n", "def test_issue_19163():\n    assert ImageSet(lambda x: x**2, S.Reals).intersect(S.Reals0\n        ) == ImageSet(lambda x: x**2, S.Reals0)\n", "compilation error", "def test_issue_20139():\n    assert imageset(Lambda(x, x), S.EmptySet) == S.EmptySet\n"], "sample_1023": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_882": ["    def test_mlp_warm_start_with_different_solver(self):\n        # Check warm_start works when changing solver between fits\n        mlp = MLPClassifier(solver='sgd', warm_start=True, max_iter=10)\n        mlp.fit(X_iris, y_iris)\n        mlp.set_params(solver='adam')\n        mlp.fit(X_iris, y_iris)\n", "    def test_mlp_weights_after_partial_fit(MLPEstimator):\n        # Check that partial_fit updates weights correctly\n        X, y = X_digits[:50], y_digits[:50]\n        mlp = MLPEstimator(hidden_layer_sizes=(5,), max_iter=1, random_state=0)\n        mlp.fit(X, y)\n        weights_before = mlp.coefs_\n        mlp.partial_fit(X, y)\n        weights_after = mlp.coefs_\n        assert not np.array_equal(weights_before, weights_after)\n", "compilation error", "def test_mlp_set_params_after_fit(MLPEstimator):\n    \"\"\"Test setting parameters after fitting.\"\"\"\n    mlp = MLPEstimator(max_iter=10, random_state=0)\n    mlp.fit(X_iris, y_iris)\n\n    # Test changing  hidden_layer_sizes\n    mlp.set_params(hidden_layer_sizes=(10,))\n    assert mlp.hidden_layer_sizes == (10,)\n\n    # Test changing learning_rate\n    mlp.set_params(learning_rate='adaptive')\n    assert mlp.learning_rate == 'adaptive'\n", "    def test_mlp_warm_start_with_validation_fraction(MLPEstimator):\n\n        mlp = MLPEstimator(\n            max_iter=10,\n            random_state=0,\n            warm_start=True,\n            validation_fraction=0.2,\n            early_stopping=True,\n        )\n        mlp.fit(X_iris, y_iris)\n        n_validation_scores = len(mlp.validation_scores_)\n        mlp.set_params(max_iter=20)\n        mlp.fit(X_iris, y_iris)\n        assert len(mlp.validation_scores_) > n_validation_scores\n\n"], "sample_580": ["compilation error", "def test_variable_type_categorical_dtypes():\n    s = pd.Series([\"a\", \"b\", \"c\"], dtype=\"category\")\n    assert variable_type(s) == \"categorical\"\n\n    s = pd.Series([1, 2, 3], dtype=\"category\")\n    assert variable_type(s) == \"categorical\"\n\n    s = pd.Series([True, False], dtype=\"category\")\n    assert variable_type(s) == \"categorical\"\n", "compilation error", "compilation error", "compilation error"], "sample_351": ["    def test_modelchoicefield_with_empty_label_optional(self):\n        f = forms.ModelChoiceField(Category.objects.all(), empty_label=None)\n        self.assertEqual(list(f.choices), [\n            (self.c1.pk, 'Entertainment'),\n            (self.c2.pk, 'A test'),\n            (self.c3.pk, 'Third'),\n        ])\n        with self.assertRaises(ValidationError):\n            f.clean('')\n", "    def test_modelchoicefield_initial(self):\n        class ArticleForm(forms.ModelForm):\n            author = forms.ModelChoiceField(queryset=Author.objects.all(), initial=Author.objects.first())\n\n            class Meta:\n                model = Article\n                fields = ['author']\n\n        article = Article.objects.create(\n            pub_date=datetime.date(1988, 1, 4),\n            writer=Writer.objects.create(name='Test writer'),\n        )\n        form = ArticleForm(instance=article)\n        self.assertEqual(form.initial['author'], Author.objects.first().pk)\n", "    def test_modelchoicefield_empty_label(self):\n        f = forms.ModelChoiceField(Category.objects.all(), empty_label=None)\n        self.assertEqual(f.choices[0], ('', ''))\n        self.assertEqual(f.choices[1:], [\n            (self.c1.pk, 'Entertainment'),\n            (self.c2.pk, 'A test'),\n            (self.c3.pk, 'Third'),\n        ])\n", "    def test_empty_label_none(self):\n        f = forms.ModelChoiceField(Category.objects.all(), empty_label=None)\n        self.assertEqual(list(f.choices), [\n            (self.c1.pk, 'Entertainment'),\n            (self.c2.pk, 'A test'),\n            (self.c3.pk, 'Third'),\n        ])\n        with self.assertRaises(ValidationError):\n            f.clean('')\n", "    def test_modelmultiplechoicefield_empty_iterable(self):\n        f = forms.ModelMultipleChoiceField(Category.objects.all())\n        self.assertEqual(f.clean([]), [])\n        self.assertEqual(f.clean(set()), [])\n\n"], "sample_1052": ["def test_fcode_matrixsymbol_slice_complex():\n    A = MatrixSymbol('A', 2, 3)\n    B = MatrixSymbol('B', 1, 3)\n    name_expr = (\"test\", [Equality(B, A[0, :])])\n    result = codegen(name_expr, \"f95\", \"test\", header=False, empty=False)\n    source = result[0][1]\n    expected = (\n        \"subroutine test(A, B)\\n\"\n        \"implicit none\\n\"\n        \"COMPLEX*16, intent(in), dimension(1:2, 1:3) :: A\\n\"\n        \"COMPLEX*16, intent(out), dimension(1:1, 1:3) :: B\\n\"\n        \"B(1, 1) = A(1, 1)\\n\"\n        \"B(1, 2) = A(1, 2)\\n\"\n        \"B(1, 3) = A(1, 3)\\n\"\n        \"end subroutine\\n\"\n    )\n    assert source == expected\n", "compilation error", "def test_fcode_output_file_names():\n    x, y = symbols(\"x y\")\n    name_expr = (\"test\", x*y)\n    result = codegen(name_expr, \"f95\", \"my_test\", header=False, empty=False)\n    assert result[0][0] == \"my_test.f90\"\n    assert result[1][0] == \"my_test.h\"\n\n", "compilation error", "compilation error"], "sample_1079": ["compilation error", "compilation error", "    def test_centroid():\n        p1 = Point(1, 2)\n        p2 = Point(3, 4)\n        p3 = Point(5, 6)\n        assert Point.centroid(p1, p2, p3) == Point(3, 4)\n", "compilation error", "    def test_point_equals_tolerence():\n        assert Point(1.000001, 2).equals(Point(1, 2), tolerance=0.0001) is True\n        assert Point(1.000001, 2).equals(Point(1, 2), tolerance=0.0000001) is False\n"], "sample_282": ["    def test_boundfield_errors_with_initial(self):\n        form = ComplexFieldForm({\"field1_0\": \"initial_text\", \"field1_1\": [\"J\"], \"field1_2_0\": \"2007-04-25\", \"field1_2_1\": \"06:24:00\"})\n        form.full_clean()\n\n        form.errors['field1'] = ValidationError(\"This field is invalid!\")\n\n        bound_field = form['field1']\n\n        self.assertEqual(bound_field.errors, form.errors.get('field1'))\n        self.assertEqual(bound_field.errors[0].message, \"This field is invalid!\")\n\n", "    def test_form_errors(self):\n        form = ComplexFieldForm({\n            'field1_0': 'some text',\n            'field1_1': ['X'],\n            'field1_2_0': '2007-04-25',\n            'field1_2_1': '06:24:00',\n        })\n        form.full_clean()\n        self.assertEqual(len(form.errors), 1)\n        self.assertEqual(list(form.errors.keys()), ['field1'])\n", "    def test_partially_required_field_with_no_initial(self):\n        form = PartiallyRequiredForm()\n        self.assertFalse(form.is_valid())\n        self.assertIn('f', form.errors)\n        self.assertEqual(form.errors['f'][0], 'This field is required.')\n\n", "def test_partially_required_field_with_initial(self):\n        form = PartiallyRequiredForm({'f_0': 'Hello', 'f_1': 'World'}, initial={'f': ['Howdy', 'World']})\n        form.full_clean()\n\n        self.assertEqual(form.cleaned_data['f'], 'Howdy,World')\n", "    def test_boundfield_as_hidden(self):\n        form = ComplexFieldForm({'field1_0': 'some text', 'field1_1': ['J', 'P'], 'field1_2_0': '2007-04-25', 'field1_2_1': '06:24:00'})\n        bound_field = form['field1']\n        expected_html = '<input type=\"hidden\" name=\"field1_0\" value=\"some text\" id=\"id_field1_0\">'\n        expected_html += '<input type=\"hidden\" name=\"field1_1\" value=\"J\" id=\"id_field1_1\">'\n        expected_html += '<input type=\"hidden\" name=\"field1_1\" value=\"P\" id=\"id_field1_1\">'\n        expected_html += '<input type=\"hidden\" name=\"field1_2_0\" value=\"2007-04-25\" id=\"id_field1_2_0\">'\n        expected_html += '<input type=\"hidden\" name=\"field1_2_1\" value=\"06:24:00\" id=\"id_field1_2_1\">'\n        self.assertEqual(bound_field.as_hidden(), expected_html)\n"], "sample_500": ["compilation error", "def test_colorbar_tick_formatters():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.arange(100).reshape(10, 10))\n    cb = fig.colorbar(pc)\n\n    # Test with a string formatter\n    cb.formatter = ScalarFormatter(useOffset=False)\n    cb.update_ticks()\n    assert isinstance(cb.ax.yaxis.get_majorticklabels()[0].get_text(),\n                      str)\n\n    # Test with a custom formatter function\n        return f\"{x:.2f} kPa\"\n\n    cb.formatter = FuncFormatter(my_formatter)\n    cb.update_ticks()\n    assert cb.ax.yaxis.get_majorticklabels()[0].get_text().endswith(\" kPa\")\n\n    # Test removing the formatter\n    cb.formatter = None\n    cb.update_ticks()\n    assert isinstance(cb.ax.yaxis.get_majorticklabels()[0].get_text(),\n                      str)\n", "compilation error", "compilation error", "compilation error"], "sample_859": ["    def test_enet_positive_sparse(self):\n        X, y, _, _ = build_dataset(n_samples=50, n_features=50, n_targets=2)\n        X = sparse.csr_matrix(X)\n        # For mono output\n        # Test that the coefs returned by positive=True in enet_path are positive\n        for path in [enet_path, lasso_path]:\n            pos_path_coef = path(X, y[:, 0], positive=True)[1]\n            assert np.all(pos_path_coef >= 0)\n\n        # For multi output, positive parameter is not allowed\n        # Test that an error is raised\n        for path in [enet_path, lasso_path]:\n            assert_raises(ValueError, path, X, y, positive=True)\n", "    def test_enet_sparse_input_convergence_warning():\n        X, y, _, _ = build_dataset(n_samples=1000, n_features=500)\n\n        with pytest.warns(ConvergenceWarning):\n            ElasticNet(max_iter=1, tol=0).fit(\n                sparse.csr_matrix(X, dtype=np.float32), y)\n\n        # check that the model converges w/o warnings\n        with pytest.warns(None) as record:\n            Lasso(max_iter=1000).fit(sparse.csr_matrix(X, dtype=np.float32), y)\n\n        assert not record.list\n", "    def test_enet_positive_multi_task():\n        X, y, _, _ = build_dataset(n_samples=20, n_features=10,\n                                     n_targets=2)\n        clf = MultiTaskElasticNet(positive=True, max_iter=100)\n        clf.fit(X, y)\n        assert np.all(clf.coef_ >= 0)\n", "    def test_enet_path_positive_sparse():\n        X, y = load_boston(return_X_y=True)\n        X = sparse.csr_matrix(X)\n        _, coefs, _ = enet_path(X, y, positive=True)\n        assert np.all(coefs >= 0)\n", "    def test_enet_cv_multi_target_same_coef():\n        X, y, _, _ = build_dataset(n_samples=50, n_features=10, n_targets=2)\n        y[:, 1] = y[:, 0]\n        clf = MultiTaskElasticNetCV(cv=3)\n        clf.fit(X, y)\n        assert_array_almost_equal(clf.coef_[0], clf.coef_[1])\n"], "sample_210": ["    def test_template_params_filtering_warning(self):\n        class ArtistView(TemplateView):\n            template_name = 'generic_views/about.html'\n\n                context = super().get_context_data(**kwargs)\n                artist = Artist.objects.get(name=artist_name)\n                return {**context, 'artist': artist}\n\n        artist = Artist.objects.create(name='Rene Magritte')\n        response = ArtistView.as_view()(self.rf.get('/'), artist_name=artist.name)\n        with self.assertRaisesMessage(RemovedInDjango40Warning,\n                                    'TemplateView passing URL kwargs to the context is deprecated. Reference %s in your template through view.kwargs instead.' % 'artist_name'):\n            str(response.context['artist_name'])\n\n\n", "    def test_redirect_view_querystring_handling(self):\n        response = RedirectView.as_view(url='/bar/', query_string=True)(self.rf.get('/foo/?test=value'))\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response['Location'], '/bar/?test=value')\n\n", "    def test_redirect_with_named_url_pattern_and_kwargs(self):\n        response = RedirectView.as_view(pattern_name='artist_detail')(self.rf.get('/foo/'), name='Magritte', pk=1)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response['Location'], '/detail/artist/Magritte/1/')\n", "    def test_redirectview_with_object(self):\n        view = RedirectView.as_view(\n            pattern_name='artist_detail',\n             permanente=True\n        )\n        request = self.rf.get('/foo/')\n        request.artist = Artist.objects.create(name='Test Artist')\n        response = view(request)\n        self.assertEqual(response.status_code, 301)\n", "    def test_redirect_view_with_kwargs(self):\n        view = RedirectView.as_view(url='/bar/{artist_id}/')\n        response = view(self.rf.get('/foo/'), artist_id=123)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response['Location'], '/bar/123/')\n"], "sample_963": ["def test_stringify_type_hints_PEP_604():\n    from typing import ParamSpec, TypeVar  # type: ignore\n    P = ParamSpec('P')\n    T = TypeVar('T')\n    assert stringify(Callable[P, T]) == \"Callable[P, T]\"\n", "def test_restify_type_hints_SpecialForm():\n    from typing import NewType, TypeVar\n    T = TypeVar('T')\n    Integer = NewType('Integer', int)\n    assert restify(Integer) == ':py:class:`tests.test_util_typing.Integer`'\n", "def test_stringify_type_hints_PEP612():\n    from typing import TypeAlias  # type: ignore\n    MyTypeAlias: TypeAlias = int\n    assert stringify(MyTypeAlias) == 'int'\n    assert stringify(TypeAlias) == \"TypeAlias\"\n\n", "def test_stringify_type_hints_NewType():\n    MyInt = NewType('MyInt', int)\n    assert stringify(MyInt) == \"MyInt\" \n    assert stringify(MyInt(5)) == \"5\"\n", "def test_restify_type_hints_NewType():\n    MyInt = NewType('MyInt', int)\n    assert restify(MyInt) == \":py:class:`tests.test_util_typing.MyInt`\"\n"], "sample_89": ["    def test_run_loop_with_exceptions(self):\n            yield\n            raise Exception('Something bad happened')\n\n        with mock.patch.object(self.reloader, 'tick', side_effect=mocked_tick) as tick:\n            with self.assertRaises(Exception):\n                self.reloader.run_loop()\n        self.assertEqual(tick.call_count, 1)\n", "    def test_notify_file_changed_nonexistent_file(self):\n        with mock.patch.object(self.reloader, 'check_server_status') as mocked_server_status:\n            mocked_server_status.return_value = False\n            self.reloader.notify_file_changed(self.nonexistent_file)\n            \n            mocked_server_status.assert_called_once()\n", "    def test_notify_file_changed_ignores_missing_files(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.nonexistent_file]):\n            self.reloader.notify_file_changed(self.nonexistent_file)\n            self.assertFalse(self.reloader._should_restart)\n", "    def test_notify_file_changed_with_removed_file(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            self.reloader.snapshot_files()\n            self.existing_file.unlink()\n            self.reloader.notify_file_changed(self.existing_file)\n            self.assertTrue(self.reloader.should_stop)\n", "    def test_watch_dir_recursive_with_symlink(self):\n        symlink_target = self.ensure_file(self.tempdir / 'test.py')\n        symlink = self.tempdir / 'link'\n        os.symlink(symlink_target, symlink)\n        self.reloader.watch_dir(self.tempdir, '**/*.py')\n        with self.tick_twice():\n            self.increment_mtime(symlink_target) \n        self.assertEqual(notify_mock.call_count, 1)\n        self.assertCountEqual(notify_mock.call_args[0], [symlink_target])\n\n"], "sample_505": ["    def test_rrulewrapper_empty_range():\n        rule = mdates.rrulewrapper(freq=dateutil.rrule.DAILY, dtstart=datetime.datetime(2023, 1, 1))\n        assert list(rule.between(\n            datetime.datetime(2023, 1, 1), datetime.datetime(2022, 12, 31))) == []\n", "    def test_date_format_with_locale():\n        import locale\n\n        # Set locale to a locale that uses a different date format\n        locale.setlocale(locale.LC_ALL, 'fr_FR')\n        dates = np.arange('2020-01-01', '2020-05-01', dtype='datetime64[D]')\n        fig, ax = plt.subplots()\n\n        ax.plot(dates, np.arange(len(dates)))\n        fig.canvas.draw()\n        # Check that the x-axis tick labels are formatted according to the locale\n        assert ax.get_xticklabels()[0].get_text() == '01 janv. 2020'\n        assert ax.get_xticklabels()[1].get_text() == '15 janv. 2020'\n        locale.setlocale(locale.LC_ALL, '') # Reset locale to default\n", "    def test_date2num_tzaware(self):\n        dt = datetime.datetime(2023, 1, 1, tzinfo=dateutil.tz.UTC)\n        num = mdates.date2num(dt)\n        assert num == mdates.date2num(dt.replace(tzinfo=None))\n\n", "def test_num2julian_with_float_input():\n    with pytest.raises(TypeError):\n        mdates.num2julian('1.0')\n    assert mdates.num2julian(1.0) == 2440588.5\n    assert mdates.num2julian(1.5) == 2440589\n", "    def test_date_inverted_limit_with_tzinfo():\n        # test ax hline with date inputs and timezone awareness\n        t0 = datetime.datetime(2009, 1, 20, tzinfo=dateutil.tz.tzutc())\n        tf = datetime.datetime(2009, 1, 31, tzinfo=dateutil.tz.tzutc())\n        fig, ax = plt.subplots()\n        ax.axhline(t0, color=\"blue\", lw=3)\n        ax.set_ylim(t0 - datetime.timedelta(days=5),\n                    tf + datetime.timedelta(days=5))\n        ax.invert_yaxis()\n        fig.subplots_adjust(left=0.25)\n"], "sample_417": ["    def test_floatformat_with_rounding_mode_set(self):\n        self.assertEqual(floatformat(1.2345, 2, rounding='ROUND_HALF_UP'), \"1.23\")\n        self.assertEqual(floatformat(15.2042, -3, rounding='ROUND_HALF_UP'), \"15.204\")\n        self.assertEqual(floatformat(1.2345, 2, rounding='ROUND_HALF_DOWN'), \"1.23\")\n        self.assertEqual(floatformat(15.2042, -3, rounding='ROUND_HALF_DOWN'), \"15.204\")\n", "    def test_rounding_ties_to_even(self):\n        self.assertEqual(floatformat(1.5, 0), \"2\")\n        self.assertEqual(floatformat(2.5, 0), \"2\")\n        self.assertEqual(floatformat(3.5, 0), \"4\")\n\n", "    def test_floatformat_with_scientific_notation(self):\n        self.assertEqual(floatformat(1.2345e6, 2), \"1234500.00\")\n        self.assertEqual(floatformat(1.2345e-6, 2), \"0.000001\")\n", "    def test_floatformat_with_locale(self):\n        with translation.override(\"fr-FR\"):\n            self.assertEqual(floatformat(1234.5678, 2), \"1\u00a0234,57\")\n            self.assertEqual(floatformat(1234.5678, \"2\"), \"1\u00a0234,57\")\n            self.assertEqual(floatformat(1234.5678, \"g\"), \"1\u00a0234,5678\")\n\n", "    def test_decimal_with_rounding(self):\n        with localcontext() as ctx:\n            ctx.prec = 2\n            self.assertEqual(floatformat(Decimal('1.2345'), '0.2'), '1.23')\n            self.assertEqual(floatformat(Decimal('15.2042'), '-3'), '15.204')\n"], "sample_696": ["def test_warning_captured_hook_is_deprecated(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-p\", \"pytest_warning_captured\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: The pytest_warning_captured is deprecated*\",\n        ]\n    )\n", "def test_warning_captured_hook_is_deprecated(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--capture=no\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*DeprecationWarning: The pytest_warning_captured is deprecated*\",\n        ]\n    )\n", "def test_argument_type_str_choice(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            pass\n\n        \"\"\"\n    )\n    with pytest.raises(pytest.PytestDeprecationWarning) as excinfo:\n        pytester.parseconfig(\n            \"--addopts\",\n            \"-k test_func\",\n            \"-m 'typ in ['str']'\",\n        )\n    assert excinfo.value.args[0] == (\n        \"`type` argument to addoption() is the string 'str'.\"\n        \" For choices this is optional and can be omitted, \"\n        \" but when supplied should be a type (for example `str` or `int`).\"\n        \" (options: ['typ'])\"\n    )\n\n\n", "def test_warning_captured_hook_is_deprecated(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: The pytest_warning_captured is deprecated*\",\n        ]\n    )\n\n", "def test_warning_captured_hook_is_deprecated(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--showlocals\", \"-p\", \"no:warning_captured_hook\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: The pytest_warning_captured is deprecated*\",\n        ]\n    )\n"], "sample_122": ["    def test_cache_page_with_custom_cache_alias(self):\n        @cache_page(3, cache='other')\n            return HttpResponse('Hello World')\n\n        request = self.factory.get('/view/')\n        response1 = view(request)\n        response2 = view(request)\n        self.assertEqual(response1.content, response2.content)\n        self.assertEqual(self.other_cache.get('views.decorators.cache.cache_page.other.GET.d41d8cd98f00b204e9800998ecf8427e'), response1.content)\n        self.assertIsNone(self.default_cache.get('views.decorators.cache.cache_page.settingsprefix.GET.d41d8cd98f00b204e9800998ecf8427e'))\n", "    def test_cache_key_with_language(self):\n        request = self.factory.get(self.path)\n        request.LANGUAGE_CODE = 'es'\n        template = engines['django'].from_string(\"This is a test\")\n        response = TemplateResponse(HttpRequest(), template)\n        learn_cache_key(request, response)\n\n        self.assertEqual(\n            get_cache_key(request),\n            'views.decorators.cache.cache_page.settingsprefix.GET.'\n            '58a0a05c8a5620f813686ff969c26853.es.d41d8cd98f00b204e9800998ecf8427e'\n        )\n", "    def test_cache_handler_context(self):\n        with caches['default'].context():\n            cache1 = caches['default']\n        cache2 = caches['default']\n        self.assertIsNot(cache1, cache2)\n", "    def test_cache_page_with_cache_alias(self):\n        @cache_page(3, cache='other')\n            return HttpResponse('Hello World')\n\n        request = self.factory.get('/view/')\n        response = view(request)\n        self.assertEqual(response.content, b'Hello World')\n        # Check that the response was cached in the 'other' cache\n        self.assertIn(\n            'Hello World',\n            self.other_cache.get_many(\n                ['views.decorators.cache.cache_page.other.GET.'\n                 + hashlib.sha1(b'GET/view/').hexdigest() + '.d41d8cd98f00b204e9800998ecf8427e']\n            )\n        )\n\n", "    def test_cachepage_missing_view(self):\n        with self.assertRaises(ValueError):\n            cache_page(1)(None)\n"], "sample_890": ["def test_no_features_selected(direction):\n    # Check that no features are selected when all features are uninformative.\n    X, y = make_regression(\n        n_samples=100,\n        n_features=10,\n        n_informative=0,\n        random_state=0,\n    )\n    sfs = SequentialFeatureSelector(\n        LinearRegression(),\n        n_features_to_select=1,\n        direction=direction,\n        cv=2,\n    )\n    sfs.fit(X, y)\n    assert sfs.get_support().sum() == 0\n\n", "def test_n_features_to_select_boundary(direction):\n\n    X, y = make_regression(n_samples=100, n_features=5, random_state=0)\n\n    # Test cases where n_features_to_select is equal to the number of features\n    sfs = SequentialFeatureSelector(\n        LinearRegression(),\n        n_features_to_select=X.shape[1],\n        direction=direction,\n        cv=2,\n    )\n    sfs.fit(X, y)\n    assert sfs.n_features_to_select_ == X.shape[1]\n    assert np.all(sfs.get_support())\n\n    # Test cases where n_features_to_select is 0\n    sfs = SequentialFeatureSelector(\n        LinearRegression(),\n        n_features_to_select=0,\n        direction=direction,\n        cv=2,\n    )\n\n    with pytest.warns(UserWarning, match=\"n_features_to_select is 0\"):\n       sfs.fit(X, y)\n    assert sfs.n_features_to_select_ == 0\n\n", "def test_n_features_to_select_error():\n    # Test for ValueError when n_features_to_select is not a valid type\n    X, y = make_regression(n_features=10)\n    with pytest.raises(ValueError, match=\"n_features_to_select must be either\"):\n        sfs = SequentialFeatureSelector(LinearRegression(), n_features_to_select=\"wrong_type\").fit(X, y) \n", "    def test_get_support_mask():\n        # Test the _get_support_mask() method\n\n        X, y = make_regression(n_features=10)\n        sfs = SequentialFeatureSelector(LinearRegression(), n_features_to_select=5)\n        sfs.fit(X, y)\n        support_mask = sfs._get_support_mask()\n        assert support_mask.shape == (10,)\n        assert np.sum(support_mask) == 5\n", "def test_multiclass_classification(direction):\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=5, n_classes=3, random_state=0)\n    sfs = SequentialFeatureSelector(KNeighborsClassifier(), n_features_to_select=3, direction=direction, cv=5)\n    sfs.fit(X, y)\n    assert sfs.get_support(indices=True).shape[0] == 3\n"], "sample_1184": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_488": ["    def test_namespace_app_config_path(self):\n        with extend_sys_path(self.base_location):\n            with self.settings(INSTALLED_APPS=[\"nsapp.apps.NSAppConfig\"]):\n                app_config = apps.get_app_config(\"nsapp\")\n                self.assertEqual(app_config.path, self.app_path)\n", "    def test_app_config_name_clash(self):\n        \"\"\"\n        Conflicting AppConfig names within the same app should raise an error.\n        \"\"\"\n        with self.assertRaisesMessage(\n            ImproperlyConfigured,\n            \"Conflicting AppConfig 'ClashConfig' in application 'apps':\",\n\n        ):\n            with self.settings(INSTALLED_APPS=[\"apps.apps.ClashConfigApp\"]):\n                pass\n", "    def test_relabeling_of_models(self):\n        self.assertEqual(apps.get_model(\"relabel\", \"LogEntry\"), LogEntry)\n", "    def test_relabeling_with_default_config(self):\n        \"\"\"\n        If multiple AppConfig classes are provided, one being the default, and the\n        app is relabeled in INSTALLED_APPS, the default configurations should use the\n        relabeled name.\n        \"\"\"\n        self.assertEqual(apps.get_app_config(\"renamed\").label, \"renamed\")\n        self.assertEqual(apps.get_app_config(\"renamed\").name, \"apps\")\n", "    def test_namespace_package_submodule(self):\n        \"\"\"\n        A Py3.3+ namespace package with a submodule can be an app if it has only one path.\n        \"\"\"\n        with extend_sys_path(self.base_location):\n            with self.settings(INSTALLED_APPS=[\"nsapp.submodule\"]):\n                app_config = apps.get_app_config(\"nsapp.submodule\")\n                self.assertEqual(app_config.path, os.path.join(self.app_path, 'submodule'))\n"], "sample_123": ["    def test_escape_leading_slashes_empty(self):\n        self.assertEqual(escape_leading_slashes(''), '')\n", "    def test_empty_string(self):\n        self.assertEqual(escape_leading_slashes(''), '')\n", "    def test_https_allowed_hosts(self):\n        self.assertTrue(url_has_allowed_host_and_scheme('https://example.com', allowed_hosts={'example.com'}))\n", "    def test_parsing_invalid_dates(self):\n        with self.assertRaises(ValueError):\n            parse_http_date('Invalid date string')\n", "    def test_url_has_allowed_host_and_scheme_empty_host(self):\n        self.assertFalse(url_has_allowed_host_and_scheme('http://', allowed_hosts={'example.com'}))\n"], "sample_105": ["    def test_get_context_data_super_in_templateview(self):\n        \"\"\"Test that get_context_data() calls super() in TemplateView.\"\"\"\n        view = views.CustomTemplateView()\n        response = view.get(self.rf.get('/'))\n        context = response.context\n        self.assertEqual(context['custom_key'], 'custom_value')\n", "    def test_redirect_view_with_explicit_permanent_redirect(self):\n        response = RedirectView.as_view(url='/bar/', permanent=True)(self.rf.get('/foo/'))\n        self.assertEqual(response.status_code, 301)\n", "    def test_redirect_view_context(self):\n        response = self.client.get('/redirect/context/')\n        context = response.context\n\n        self.assertEqual(context['foo'], 'bar')\n", "    def test_redirect_view_with_context(self):\n        view = RedirectView.as_view(url='/bar/', permanent=True)\n        request = self.rf.get('/foo/')\n        request.context_data = {'foo': 'bar'}\n        response = view(request)\n        self.assertEqual(response.status_code, 301)\n        self.assertEqual(response.url, '/bar/')\n        self.assertEqual(response.context.get('foo'), 'bar')\n\n", "    def test_redirect_view_with_kwargs(self):\n        response = RedirectView.as_view(url='/bar/')(self.rf.get('/foo/'), arg='value')\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, '/bar/')\n"], "sample_1155": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1028": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_34": ["    def test_compose_with_zero_power():\n        x = u.m ** 0\n        y = x.compose()\n        assert y[0].is_unity\n\n", "compilation error", "    def test_custom_unit_equivalency(self):\n        class MyUnit(u.UnitBase):\n                super().__init__('MyUnit', namespace=locals())\n                if other is u.meter:\n                    return value * 10\n                return super().to(other, value)\n        \n        MyUnit()\n        with u.add_enabled_units(MyUnit()):\n            assert MyUnit().to(u.meter, 1) == 10\n", "def test_composite_unit_format():\n    unit = u.km/u.s * u.g\n    assert str(unit) == 'km g / s'\n", "def test_dimensionless_unit_compose():\n    # Regression test for #4084. This checks that dimensionless units\n    # compose correctly.\n\n    x = u.dimensionless_unscaled * u.Unit(\"1/s\")\n    assert x.compose() == [u.Unit(\"1/s\")]\n"], "sample_820": ["compilation error", "    def test_estimator_clone():\n        # test if estimators are cloned when fit is called\n        clf1 = LogisticRegression(random_state=123, penalty='l2')\n        clf2 = RandomForestClassifier(random_state=123, max_depth=3)\n        eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)],\n                               voting='soft')\n\n\n        eclf.fit(X, y)\n        assert clf1.get_params()['penalty'] == 'l2'\n        assert clf2.get_params()['max_depth'] == 3\n        eclf.set_params(lr__penalty='elasticnet', rf__max_depth=5)\n\n        assert clf1.get_params()['penalty'] == 'l2'\n\n        eclf.fit(X, y)\n        assert clf1.get_params()['penalty'] == 'l2'\n        assert clf2.get_params()['max_depth'] == 5\n\n", "def test_transform_multilabel():\n    \"\"\"Check transform method of VotingClassifier for multilabel classification.\"\"\"\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([[0, 1], [1, 0], [1, 1]])\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    eclf1 = VotingClassifier(estimators=[\n                    ('lr', clf1), ('rf', clf2)],\n                    voting='soft').fit(X, y)\n    eclf2 = VotingClassifier(estimators=[\n                    ('lr', clf1), ('rf', clf2)],\n                    voting='soft',\n                    flatten_transform=True).fit(X, y)\n    eclf3 = VotingClassifier(estimators=[\n                    ('lr', clf1), ('rf', clf2)],\n                    voting='soft',\n                    flatten_transform=False).fit(X, y)\n    assert_array_equal(eclf1.transform(X).shape, (3, 6))\n    assert_array_equal(eclf2.transform(X).shape, (3, 6))\n    assert_array_equal(eclf3.transform(X).shape, (3, 2, 2))\n    assert_array_almost_equal(eclf1.transform(X),\n                              eclf2.transform(X))\n    assert_array_almost_equal(\n            eclf3.transform(X).swapaxes(0, 1).reshape((3, 6)),\n            eclf2.transform(X)\n    )\n", "compilation error", "compilation error"], "sample_591": ["    def test_merge_coords(self):\n        ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n        ds2 = xr.Dataset({\"b\": (\"x\", [3, 4]), \"x\": [1, 2]})\n        ds3 = ds1.merge(ds2)\n        assert_identical(ds3.coords, ds1.coords)\n\n", "    def test_merge_coords_from_dataarray(self):\n        ds = xr.Dataset({\"a\": (\"x\", [1, 2])}, coords={\"x\": [0, 1]})\n        da = xr.DataArray([3, 4], coords={\"x\": [1, 2]}, dims=\"x\", name=\"b\")\n\n        actual = ds.merge(da)\n        expected = xr.Dataset({\"a\": (\"x\", [1, 2]), \"b\": (\"x\", [3, 4])}, coords={\"x\": [0, 1, 2]})\n        assert_identical(actual, expected)\n\n", "    def test_merge_empty_datasets(self):\n        ds1 = xr.Dataset()\n        ds2 = xr.Dataset({\"x\": 0})\n\n        actual = ds1.merge(ds2)\n        expected = ds2.copy()\n        assert expected.identical(actual)\n\n        actual = ds2.merge(ds1)\n        assert expected.identical(actual)\n", "    def test_merge_empty_dataset(self):\n        empty_ds = xr.Dataset()\n        data = create_test_data()\n        actual = empty_ds.merge(data)\n        assert_identical(actual, data)\n\n        actual = data.merge(empty_ds)\n        assert_identical(actual, data)\n", "    def test_merge_preserve_attrs(self):\n        ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]}, attrs={\"foo\": \"bar\"})\n        ds2 = xr.Dataset({\"b\": (\"x\", [3, 4]), \"x\": [1, 2]}, attrs={\"baz\": \"qux\"})\n        expected = xr.Dataset({\"a\": (\"x\", [1, 2]), \"b\": (\"x\", [3, 4]), \"x\": [0, 1]},\n                              attrs={\"foo\": \"bar\", \"baz\": \"qux\"})\n\n        actual = ds1.merge(ds2)\n        assert expected.identical(actual)\n"], "sample_846": ["compilation error", "def test_column_transformer_sparse_input():\n    X = sparse.csr_matrix([[0, 1, 2], [2, 4, 6], [8, 6, 4]]).T\n    ct = ColumnTransformer([('trans1', Trans(), [0]), ('trans2', Trans(), [1])],\n                           remainder='passthrough')\n    X_trans = ct.fit_transform(X)\n    assert sparse.issparse(X_trans)\n    assert_array_equal(X_trans.toarray(), np.array([[0, 1, 2], [2, 4, 6], [8, 6, 4]]))\n\n", "compilation error", "def test_column_transformer_sparse_fit_transform_pandas():\n    pd = pytest.importorskip('pandas')\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).T\n    X_df = pd.DataFrame(X_array, columns=['first', 'second', 'third'])\n    ct = ColumnTransformer([('trans1', Trans(), 'first')],\n                           remainder='passthrough')\n\n    X_trans = ct.fit_transform(X_df)\n    assert sparse.issparse(X_trans)\n    assert X_trans.shape == (3, 4)\n", "def test_column_transformer_drop_all_columns():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).T\n\n    ct = ColumnTransformer([('trans1', 'drop', [0, 1, 2])])\n    assert_array_equal(ct.fit_transform(X_array).shape, (3, 0))\n    assert_array_equal(ct.fit(X_array).transform(X_array).shape, (3, 0))\n"], "sample_512": ["def test_figure_dpi():\n    fig, ax = plt.subplots()\n    assert plt.gcf().dpi == fig.dpi\n    plt.figure(dpi=100)\n    assert plt.gcf().dpi == 100\n", "def test_get_current_fig_manager():\n    fig = plt.figure()\n    fig_manager = plt.get_current_fig_manager()\n    assert fig_manager.canvas.figure is fig\n", "def test_subplot_too_many_args():\n    with pytest.raises(ValueError):\n        plt.subplot(1, 2, 3, 4, 5)\n", "def test_figure_close_after_savefig():\n    fig, ax = plt.subplots()\n    fig.savefig('test.png')\n    assert fig in plt._pylab_helpers.Gcf.figs\n    plt.close()\n    assert fig not in plt._pylab_helpers.Gcf.figs\n", "    def test_figure_close_after_error():\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3], [1, 2, 3])\n        try:\n            raise ValueError(\"This is an error.\")\n        except ValueError:\n            plt.close(fig)\n        assert not plt.get_fignums()\n"], "sample_736": ["    def test_logreg_predict_proba_zero_intercept():\n        # Test that predict_proba returns probabilities summing to 1 even when\n        # the intercept is zero.\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([0, 1, 0])\n        clf = LogisticRegression(fit_intercept=False)\n        clf.fit(X, y)\n        probs = clf.predict_proba(X)\n        assert_array_almost_equal(probs.sum(axis=1), np.ones(3))\n", "def test_saga_intercept_scaling():\n    # Test that intercept scaling is correctly handled for 'saga' solver\n    X, y = make_classification(n_samples=50, n_features=10, random_state=0)\n    for intercept_scaling in [0.1, 1.0, 10.0]:\n\n        clf = LogisticRegression(solver='saga', \n                                    intercept_scaling=intercept_scaling,\n                                    random_state=0\n\n                                   )\n        clf.fit(X, y)\n        intercept = clf.intercept_\n        assert_almost_equal(intercept, \n                            intercept_scaling * clf._intercept_scaled,\n                            decimal=5)\n", "    def test_logreg_predict_proba_multiclass_sparse():\n        X, y = make_classification(n_samples=10, n_features=20, random_state=0,\n                                   n_classes=3, n_informative=10)\n        X_sparse = sparse.csr_matrix(X)\n        # Predicted probabilities using the true-entropy loss should give a\n        # smaller loss than those using the ovr method.\n        clf_multi = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\")\n        clf_multi.fit(X_sparse, y)\n        clf_multi_loss = log_loss(y, clf_multi.predict_proba(X_sparse))\n        clf_ovr = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\")\n        clf_ovr.fit(X_sparse, y)\n        clf_ovr_loss = log_loss(y, clf_ovr.predict_proba(X_sparse))\n        assert_greater(clf_ovr_loss, clf_multi_loss)\n", "    def test_logistic_regression_class_weights_multiclass():\n        # Test that class_weight works correctly for multiclass\n        # logistic regression\n\n        X, y = make_classification(n_samples=100, n_features=20,\n                                   n_informative=10, n_classes=3,\n                                   random_state=0)\n        # Make class 2 much smaller\n        y[50:] = 2\n\n        class_weight = 'balanced'\n\n        for solver in ['lbfgs', 'newton-cg', 'sag', 'saga']:\n            clf = LogisticRegression(solver=solver,\n                                     multi_class='multinomial',\n                                     class_weight=class_weight)\n            clf.fit(X, y)\n\n            # Check that the coefficients for the\n            # under-represented class are not all zero\n            assert not np.all(clf.coef_[2] == 0)\n", "    def test_logreg_sparse_input_coef_shape():\n        # Test that the shape of coefficients is consistent with the input data\n        # when sparse input is used.\n\n        X, y = make_classification(n_samples=50, n_features=10, random_state=0)\n        X_sparse = sparse.csr_matrix(X)\n\n        lr = LogisticRegression(multi_class='ovr')\n        lr.fit(X_sparse, y)\n        assert_equal(lr.coef_.shape, (1, 10))  # 1 class, 10 features\n\n        lr = LogisticRegression(multi_class='multinomial')\n        lr.fit(X_sparse, y)\n        assert_equal(lr.coef_.shape, (3, 10))  # 3 classes, 10 features\n\n"], "sample_955": ["compilation error", "compilation error", "    def test_parse_type_comments():\n        code = \"\"\"", "    def test_unparse_complex():\n        source = \"(1 + 2j)\"\n        expected = \"(1 + 2j)\"\n        module = ast.parse(source)\n        assert ast.unparse(module.body[0].value, source) == expected\n", "def test_unparse_function_def(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0], source) == expected    \n"], "sample_717": ["    def test_load_fake_lfw_pairs_subset():\n        lfw_pairs_test = fetch_lfw_pairs(subset='test',\n                                       data_home=SCIKIT_LEARN_DATA,\n                                       download_if_missing=False)\n\n        assert_equal(lfw_pairs_test.pairs.shape[0], 1)\n        assert_equal(lfw_pairs_test.target.shape[0], 1)\n\n", "def test_load_fake_lfw_pairs_subset():\n    lfw_pairs_test = fetch_lfw_pairs(subset='test',\n                                     data_home=SCIKIT_LEARN_DATA,\n                                     download_if_missing=False)\n    assert_equal(lfw_pairs_test.pairs.shape[0], 1)\n    assert_equal(lfw_pairs_test.target.shape[0], 1)\n\n", "compilation error", "def test_load_fake_lfw_pairs_subsets():\n    lfw_pairs_test = fetch_lfw_pairs(subset='test',\n                                      data_home=SCIKIT_LEARN_DATA,\n                                      download_if_missing=False)\n    assert_equal(lfw_pairs_test.pairs.shape[0], 1)\n    assert_equal(lfw_pairs_test.target.shape[0], 1)\n    assert_equal(lfw_pairs_test.target_names,\n                 ['Different persons', 'Same person'])\n", "    def test_load_fake_lfw_pairs_other_subsets():\n        lfw_pairs_devtest = fetch_lfw_pairs(subset='test',\n                                          data_home=SCIKIT_LEARN_DATA,\n                                          download_if_missing=False)\n        assert_equal(lfw_pairs_devtest.pairs.shape, (1, 2, 62, 47))\n        assert_equal(lfw_pairs_devtest.target.shape, (1,))\n        assert_array_equal(lfw_pairs_devtest.target, [0])\n\n        lfw_pairs_folds = fetch_lfw_pairs(subset='10_folds',\n                                       data_home=SCIKIT_LEARN_DATA,\n                                       download_if_missing=False)\n        assert_equal(lfw_pairs_folds.pairs.shape, (1, 2, 62, 47))\n        assert_equal(lfw_pairs_folds.target.shape, (1,))\n        assert_array_equal(lfw_pairs_folds.target, [0])\n"], "sample_454": ["    def test_validate_condition_with_fexpr_and_none(self):\n        p1 = UniqueConstraintConditionProduct.objects.create(name=\"p1\")\n        constraint = UniqueConstraintConditionProduct._meta.constraints[0]\n        constraint.violation_error_message = \"Custom message\"\n        constraint.violation_error_code = \"custom_code\"\n        msg = \"Custom message\"\n        with self.assertRaisesMessage(ValidationError, msg) as cm:\n            constraint.validate(\n                UniqueConstraintConditionProduct,\n                UniqueConstraintConditionProduct(name=p1.name, color=None),\n            )\n        self.assertEqual(cm.exception.code, \"custom_code\")\n\n", "    def test_validate_deferrable(self):\n        constraint = models.UniqueConstraint(\n            fields=[\"name\"],\n            name=\"name_deferrable_uniq\",\n            deferrable=models.Deferrable.DEFERRED,\n        )\n        with atomic():\n            constraint.validate(UniqueConstraintDeferrable, UniqueConstraintDeferrable(name=\"p1\"))\n            constraint.validate(\n                UniqueConstraintDeferrable,\n                UniqueConstraintDeferrable(name=\"p2\"),\n                exclude={\"name\"},\n            )\n\n", "    def test_covering_indexes(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\n                \"SELECT EXISTS (SELECT 1 FROM pg_indexes WHERE \"\n                \"tablename = 'django_migrations' AND \"\n                \"indexname = 'django_migrations_app_name_migration_pkey_covering');\"\n            )\n            has_covering_index = cursor.fetchone()[0]\n        self.assertFalse(has_covering_index)\n\n", "    def test_database_constraint_deferrable_after_save(self):\n        obj_1 = UniqueConstraintDeferrable.objects.create(name=\"p1\", shelf=\"front\")\n        obj_2 = UniqueConstraintDeferrable.objects.create(name=\"p2\", shelf=\"back\")\n        obj_1.name, obj_2.name = obj_2.name, obj_1.name\n        obj_1.save()\n        # The constraint is deferred, so the duplicate name is allowed\n        obj_2.save()\n        # Now, the constraint is enforced\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintDeferrable.objects.create(name=\"p1\", shelf=\"back\")\n", "    def test_validate_condition_with_exclude(self):\n        p1 = UniqueConstraintConditionProduct.objects.create(name=\"p1\")\n        constraint = UniqueConstraintConditionProduct._meta.constraints[0]\n        msg = \"Constraint \u201cname_without_color_uniq\u201d is violated.\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            constraint.validate(\n                UniqueConstraintConditionProduct,\n                UniqueConstraintConditionProduct(name=p1.name, color=None),\n                exclude={\"color\"},\n            )\n\n\n\n"], "sample_125": ["    def test_delete_cookie_path(self):\n        response = HttpResponse()\n        response.set_cookie('c', path='/foo')\n        response.delete_cookie('c', path='/foo')\n        self.assertFalse('c' in response.cookies)\n\n", "    def test_delete_cookie_with_path(self):\n        response = HttpResponse()\n        response.set_cookie('c', path='/specific/path')\n        response.delete_cookie('c', path='/specific/path')\n        self.assertEqual(response.cookies['c']['expires'], 'Thu, 01 Jan 1970 00:00:00 GMT')\n        self.assertEqual(response.cookies['c']['max-age'], 0)\n        self.assertEqual(response.cookies['c']['path'], '/specific/path')\n        self.assertEqual(response.cookies['c']['secure'], '')\n        self.assertEqual(response.cookies['c']['domain'], '')\n", "    def test_delete_cookie_path(self):\n        response = HttpResponse()\n        response.set_cookie('c', path='/path/')\n        response.delete_cookie('c', path='/path/')\n        self.assertEqual(response.cookies['c']['expires'], 'Thu, 01 Jan 1970 00:00:00 GMT')\n\n", "    def test_delete_cookie_domain(self):\n        response = HttpResponse()\n        response.delete_cookie('c', domain='example.com')\n        cookie = response.cookies['c']\n        self.assertEqual(cookie['domain'], 'example.com')\n", "    def test_signed_cookie(self):\n        response = HttpResponse()\n        message = 'secret message'\n        response.set_signed_cookie('c', message, salt='secret')\n        cookie = response.cookies['c']\n        self.assertEqual(signing.get_cookie_signer(salt='csecret').unsign(cookie.value), message)\n"], "sample_1174": ["compilation error", "compilation error", "def test_Abs_is_zero():\n    x = Symbol('x')\n    assert Abs(x).is_zero is None\n    assert (Abs(x) == 0).simplify() == Abs(x) == 0\n    assert Abs(0).is_zero is True\n    assert not Abs(1).is_zero\n\n", "def test_sign_zero():\n    assert sign(0) == 0\n    assert sign(-0) == 0\n    assert sign(S.Zero) == 0\n", "compilation error"], "sample_863": ["    def test_pipeline_with_transformer_which_returns_none(self):\n        # Test case for issue #15181 where transformers returning None\n        # would cause unexpected behavior.\n        class NoneTransformer(TransformerMixin, BaseEstimator):\n                return self\n                return None\n        pipe = Pipeline([('none', NoneTransformer()), ('clf', SVC())])\n        X, y = iris.data, iris.target\n        with pytest.raises(TypeError):\n            pipe.fit(X, y)\n", "compilation error", "    def test_pipeline_with_final_estimator_none():\n        # Test pipeline where the final estimator is None\n        X = np.array([[1], [2], [3]])\n        y = np.array([0, 1, 0])\n\n        pipe = Pipeline([('scaler', StandardScaler()), ('clf', None)])\n        pipe.fit(X, y)\n        assert pipe.predict(X).shape == (3,)\n\n", "    def test_feature_union_passthrough_empty(self):\n        X = np.array([[1, 2], [3, 4]])\n        union = FeatureUnion([\n            ('passthrough', 'passthrough'),\n            ('select', SelectKBest(k=1))\n        ])\n        Xt = union.fit_transform(X)\n        assert Xt.shape == (2, 3)\n", "compilation error"], "sample_605": ["compilation error", "    def test_groupby_assign_coords_inplace():\n        actual = xr.DataArray(\n            [[3, 4, 5], [6, 7, 8]], dims=[\"y\", \"x\"], coords={\"y\": range(2), \"x\": range(3)}\n        )\n        actual.groupby(\"x\").assign_coords({\"y\": [-1, -2]}, inplace=True)\n        expected = xr.DataArray(\n            [[3, 4, 5], [6, 7, 8]], dims=[\"y\", \"x\"], coords={\"y\": [-1, -2], \"x\": range(3)}\n        )\n        assert_identical(expected, actual)\n\n", "compilation error", "compilation error", "    def test_groupby_itertuple():\n        ds = xr.Dataset({\"foo\": ((\"x\", \"y\"), np.arange(12).reshape(3, 4))}, coords={\"x\": [1, 2, 3], \"y\": [4, 5, 6, 7]})\n        expected = list(ds.groupby(\"x\"))\n        actual = [(k, v) for k, v in ds.groupby(\"x\")]\n        for a, e in zip(actual, expected):\n            assert_identical(a[1], e[1])\n            assert a[0] == e[0]\n"], "sample_234": ["    def test_union_with_annotations_on_different_models(self):\n        ReservedName.objects.create(name='rn1', order=1)\n        qs1 = Number.objects.annotate(\n            has_reserved_name=Exists(ReservedName.objects.filter(order=OuterRef('num')))\n        ).filter(has_reserved_name=True)\n        qs2 = ReservedName.objects.values('name', 'order').annotate(\n            num=Value(10, IntegerField())\n        )\n        self.assertCountEqual(qs1.union(qs2).values_list('num', flat=True), [1, 10])\n", "    def test_select_related_on_combined_qs(self):\n        qs1 = Number.objects.select_related('reservedname__reservedname')\n        qs2 = Number.objects.select_related('reservedname')\n        with self.assertRaisesMessage(\n                NotSupportedError,\n                'Calling QuerySet.select_related() after union() is not supported.'\n        ):\n            qs1.union(qs2)\n", "    def test_union_with_different_column_names(self):\n        qs1 = Number.objects.values(number=F('num'))\n        qs2 = Number.objects.values(number=F('other_num'))\n        self.assertQuerysetEqual(qs1.union(qs2), [\n            {'number': 0},\n            {'number': 1},\n            {'number': 2},\n            {'number': 3},\n            {'number': 4},\n            {'number': 5},\n            {'number': 6},\n            {'number': 7},\n            {'number': 8},\n            {'number': 9},\n        ], lambda x: x)\n", "    def test_union_with_select_related(self):\n        ReservedName.objects.create(name='rn1', order=1, number=Number.objects.get(num=1))\n        qs1 = Number.objects.filter(num=1).select_related('reservedname')\n        qs2 = Number.objects.filter(num=2).select_related('reservedname')\n        self.assertSequenceEqual(\n            list(qs1.union(qs2)),\n            [\n                {'num': 1, 'other_num': 9, 'reservedname': {'name': 'rn1', 'order': 1, 'id': 1}},\n                {'num': 2, 'other_num': 8, 'reservedname': None},\n            ],", "    def test_intersection_with_different_select_fields(self):\n        qs1 = Number.objects.values('num', 'other_num')\n        qs2 = Number.objects.values('num')\n        self.assertEqual(\n            list(qs1.intersection(qs2)),\n            [(n, 10 - n) for n in range(10)],\n        )\n"], "sample_903": ["def test_tsne_with_precomputed_distances_and_init():\n    # Test that TSNE works with precomputed distances and initialization\n    random_state = check_random_state(0)\n    X = random_state.randn(50, 2)\n    distances = pairwise_distances(X)\n    tsne = TSNE(metric='precomputed', init='pca', random_state=0)\n    X_embedded = tsne.fit_transform(distances)\n    assert X_embedded.shape == (50, 2) \n", "def test_tsne_with_different_perplexities():\n    \"\"\"Make sure that TSNE produces different embeddings for different perplexity values.\"\"\"\n    random_state = check_random_state(0)\n    X = random_state.randn(100, 2)\n    perplexity_values = [5, 30, 100]\n    embeddings = []\n    for perplexity in perplexity_values:\n        tsne = TSNE(perplexity=perplexity, random_state=0).fit(X)\n        embeddings.append(tsne.embedding_)\n    \n    # Assert that the embeddings are different for different perplexities\n    for i in range(len(embeddings) - 1):\n        assert not np.allclose(embeddings[i], embeddings[i + 1])\n", "def test_tsne_with_sparse_data():\n    random_state = check_random_state(0)\n    n_samples = 100\n    n_features = 50\n    n_components = 2\n    density = 0.1\n\n    X = sparse.csr_matrix(random_state.rand(n_samples, n_features) < density)\n    tsne = TSNE(n_components=n_components, random_state=0)\n    X_embedded = tsne.fit_transform(X)\n    assert X_embedded.shape == (n_samples, n_components)\n", "    def test_tsne_with_sparse_input():\n        random_state = check_random_state(0)\n        n_components = 2\n        n_samples = 100\n        n_features = 5\n        X = random_state.randn(n_samples, n_features)\n        X_sparse = csr_matrix(X)\n        tsne = TSNE(n_components=n_components, random_state=0, method='exact')\n        X_embedded_dense = tsne.fit_transform(X)\n        X_embedded_sparse = tsne.fit_transform(X_sparse)\n        assert_array_almost_equal(X_embedded_dense, X_embedded_sparse, decimal=5)\n", "def test_tsne_with_negative_perplexity():\n    \"\"\"Test if TSNE raises ValueError for negative perplexity.\"\"\"\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    tsne = TSNE(perplexity=-1)\n    with pytest.raises(ValueError) as e:\n        tsne.fit_transform(X)\n    assert \"Perplexity cannot be less than 1\" in str(e.value)\n\n"], "sample_1173": ["    def test_issue_23877():\n        x = Symbol('x')\n        y = Symbol('y')\n        assert parse_expr('x**y * x**y^2',\n                         transformations=standard_transformations +\n                         (implicit_multiplication_application,)) == x**(y + y**2)\n", "compilation error", "    def test_issue_20263():\n        x = Symbol('x')\n        y = Symbol('y')\n        assert parse_expr('x**y', local_dict={'x': x, 'y': y}, transformations=(\n            standard_transformations +\n            (implicit_multiplication,))) == x**y\n", "def test_issue_22493():\n    x = Symbol('x')\n    assert parse_expr('x**(-1/2)', evaluate=False).func == Pow\n    assert parse_expr('x**(-1/2)').func == Pow\n", "    def test_issue_20186():\n        x = Symbol('x')\n        y = Symbol('y')\n        assert parse_expr(\"x**y**2\").args[0] == x\n        assert parse_expr(\"x**y**2\").args[1] == y**2\n        assert parse_expr(\"x**y**2\", evaluate=False).args[0] == x\n        assert parse_expr(\"x**y**2\", evaluate=False).args[1] == y**2\n\n"], "sample_656": ["def test_dontreadfrominput_readline(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import sys\n            f = sys.stdin\n            f.readline()\n    \"\"\"\n    )\n    result = testdir.runpytest_subprocess(str(p))\n    assert result.ret == 0\n", "def test_capture_with_sys_set_stderr(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import sys\n\n            sys.stderr = sys.stdout\n            print(\"hello world\")\n            assert 0\n    \"\"\"\n    )\n    result = testdir.runpytest_subprocess()\n    result.stdout.fnmatch_lines(\n        \"\"\"\n        *test_capture*\n        *assert 0*\n        *Captured stdout*\n        *hello world*\n    \"\"\"\n    )\n\n", "def test_capture_with_socket_file_object(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import socket\n        import subprocess\n\n            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            sock.connect(('localhost', 12345))\n            sock.sendall('hello world'.encode(\"ascii\"))\n            sock.close()\n\n            proc = subprocess.Popen(['nc', '-l', '12345'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            stdout, stderr = proc.communicate()\n            assert stdout.decode('ascii') == 'hello world'\n            assert stderr == b''\n    \"\"\"\n    )\n    # This test relies on nc being available on the system, so it might\n    # need to be skipped on platforms where nc is not common.\n    result = testdir.runpytest()\n    assert result.ret == 0\n\n", "def test_dontreadfrominput_closed_before_use(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import sys\n            sys.stdin.close()\n            with pytest.raises(ValueError):\n                sys.stdin.read()\n    \"\"\"\n    )\n    result = testdir.runpytest_subprocess(str(p))\n    result.assert_outcomes(passed=1)\n\n", "    def test_capture_with_unicode_output(testdir):\n        testdir.makepyfile(\n            \"\"\"\n                print('\u4f60\u597d')\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        assert result.ret == 0\n        result.stdout.fnmatch_lines([\"\u4f60\u597d\"])\n"], "sample_585": ["compilation error", "compilation error", "def test_da_groupby_nonnumerical_dim():\n    array = xr.DataArray([1, 2, 3, 4, 5, 6],\n                         [('x', ['a', 'a', 'b', 'b', 'c', 'c'])])\n\n    expected = xr.DataArray([2, 5, 6], [('x', ['a', 'b', 'c'])])\n    actual = array.groupby('x').quantile(.5)\n    assert_identical(expected, actual)\n\n", "    def test_da_groupby_cumsum():\n        array = xr.DataArray([1, 2, 3, 4, 5, 6],\n                             [('x', [1, 1, 1, 2, 2, 2])])\n        expected = xr.DataArray([1, 3, 6, 4, 9, 15], [('x', [1, 1, 1, 2, 2, 2])])\n        actual = array.groupby('x').cumsum()\n        assert_identical(expected, actual)\n\n", "def test_da_groupby_quantile_empty_groups():\n    array = xr.DataArray([1, 2, 3, 4, 5, 6],\n                         [('x', [1, 1, 1, 2, 2, 2])])\n\n    # Test with empty group\n    array = array.where(array > 3, other=np.nan)\n    expected = xr.DataArray([np.nan, np.nan],\n                            [('x', [1, 2])])\n    actual = array.groupby('x').quantile(.5)\n    assert_identical(expected, actual) \n\n"], "sample_479": ["    def test_optimize_through_indexes(self):\n        \"\"\"\n        Index-level through checking is working. This should manage to collapse\n        model Foo to nonexistence, and model Bar to a single IntegerField\n        called \"width\" but preserving the index.\n        \"\"\"\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    \"Foo\", [(\"name\", models.CharField(max_length=255))]\n                ),\n                migrations.CreateModel(\"Bar\", [(\"size\", models.IntegerField())]),\n                migrations.AddIndex(\n                    \"Bar\", models.Index(fields=[\"size\"], name=\"bar_size_idx\")\n                ),\n                migrations.AddField(\"Foo\", \"age\", models.IntegerField()),\n                migrations.AddField(\"Bar\", \"width\", models.IntegerField()),\n                migrations.AlterField(\"Foo\", \"age\", models.IntegerField()),\n                migrations.RenameField(\"Bar\", \"size\", \"dimensions\"),\n                migrations.RemoveField(\"Foo\", \"age\"),\n                migrations.RenameModel(\"Foo\", \"Phou\"),\n                migrations.RemoveField(\"Bar\", \"dimensions\"),\n                migrations.RenameModel(\"Phou\", \"Fou\"),\n                migrations.DeleteModel(\"Fou\"),\n            ],\n            [\n                migrations.CreateModel(\n                    \"Bar\",\n                    [(\"width\", models.IntegerField())],\n                ),\n                migrations.AddIndex(\n                    \"Bar\", models.Index(fields=[\"width\"], name=\"bar_size_idx\")\n                ),\n            ],\n        )\n\n", "compilation error", "    def test_create_alter_alter_one_to_many(self):\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\"Article\", [(\"title\", models.CharField(max_length=255))]),\n                migrations.CreateModel(\"Comment\", [(\"text\", models.TextField())]),\n                migrations.AddField(\n                    \"Comment\", \"article\", models.ForeignKey(\"Article\", models.CASCADE)\n                ),\n                migrations.AlterUniqueTogether(\n                    \"Comment\", [(\"article\", \"text\")]\n                ),\n                migrations.AlterUniqueTogether(\n                    \"Comment\", [(\"article\", \"text\")]\n                ),\n\n            ],\n            [\n                migrations.CreateModel(\n                    \"Article\", [(\"title\", models.CharField(max_length=255))]\n                ),\n                migrations.CreateModel(\n                    \"Comment\",\n                    [\n                        (\"text\", models.TextField()),\n                        (\"article\", models.ForeignKey(\"Article\", models.CASCADE)),\n                    ],\n                ),\n                migrations.AlterUniqueTogether(\n                    \"Comment\", [(\"article\", \"text\")]\n                ),\n            ],\n        )\n\n", "    def test_alter_unique_to_index_then_delete(self):\n        self.assertOptimizesTo(\n            [\n                migrations.AlterUniqueTogether(\n                    \"Pony\", [(\"weight\", \"pink\")], name=\"unique_pony\"\n                ),\n                migrations.AlterIndexTogether(\n                    \"Pony\", [(\"weight\", \"pink\")], name=\"unique_pony\"\n                ),\n                migrations.RemoveIndex(\"Pony\", \"unique_pony\"),\n            ],\n            [],\n        )\n", "    def test_optimize_through_rename_field_and_alter_field(self):\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    \"Foo\", [(\"name\", models.CharField(max_length=255))]\n                ),\n                migrations.AddField(\"Foo\", \"age\", models.IntegerField()),\n                migrations.RenameField(\"Foo\", \"name\", \"title\"),\n                migrations.AlterField(\"Foo\", \"age\", models.CharField(max_length=20)),\n            ],\n            [\n                migrations.CreateModel(\n                    \"Foo\",\n                    [(\"title\", models.CharField(max_length=255)), (\"age\", models.CharField(max_length=20))],\n                ),\n            ],\n        )\n        \n"], "sample_1088": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_549": ["    def test_remove_masked_points():\n        x = np.array([1, 2, 3, 4, 5])\n        y = np.array([1, 2, np.nan, 4, 5])\n        mask = np.isnan(y)\n        x_new, y_new = cbook.remove_masked_points(x, y)\n        assert np.array_equal(x_new, np.array([1, 2, 4, 5]))\n        assert np.array_equal(y_new, np.array([1, 2, 4, 5]))\n\n\n", "def test_safe_zip():\n    assert list(cbook._safe_zip([1, 2], [3, 4])) == [(1, 3), (2, 4)]\n    assert list(cbook._safe_zip([1, 2, 3], [3, 4])) == [(1, 3), (2, 4)]\n    assert list(cbook._safe_zip([1, 2], [3, 4, 5])) == [(1, 3), (2, 4)]\n    assert list(cbook._safe_zip([], [3, 4, 5])) == []\n    assert list(cbook._safe_zip([1, 2, 3], [])) == []\n\n", "    def test_reshape2d_empty():\n        assert cbook._reshape_2D([], 'x') == [(np.array([]))]\n        assert cbook._reshape_2D([], 'xy') == [[]]\n        assert cbook._reshape_2D([], 'x').shape == (1, 0)\n\n", "    def test_is_writable():\n        # Test cases with different permissions\n        root_dir = os.path.abspath(os.sep)\n        assert cbook.is_writable(root_dir) is True\n        tempdir = tempfile.mkdtemp()\n        assert cbook.is_writable(tempdir) is True\n        # Remove write permission\n        os.chmod(tempdir, stat.S_IREAD | stat.S_IEXEC)\n        assert cbook.is_writable(tempdir) is False\n        # Cleanup\n        os.chmod(tempdir, stat.S_IRWXU)\n        shutil.rmtree(tempdir)\n", "def test_Grouper_remove_sibling():\n    class Dummy:\n        pass\n    objs = [Dummy() for _ in range(5)]\n    g = cbook.Grouper()\n    g.join(*objs)\n    g.remove(objs[0])\n    assert not g.joined(objs[0], objs[1]) \n"], "sample_730": ["compilation error", "    def test_enet_coef_shape_empty_X(self):\n        # Test the shape of coef_ when X is empty\n        X = np.array([], dtype=np.float64).reshape(0, 10)\n        y = np.array([])\n        clf = ElasticNet()\n        clf.fit(X, y)\n        assert clf.coef_.shape == (10,)\n", "compilation error", "    def test_enet_intercept_scaling(self):\n        # Test that intercept scaling is correctly applied when using\n        # fit_intercept=True and normalize=True.\n\n        X_small =np.array([[1, 2], [3, 4], [5, 6]])\n        y_small = np.array([3, 6,9])\n        X_scaled = (X_small - X_small.mean(axis=0)) / X_small.std(axis=0)\n\n        for fit_intercept in [True, False]:\n\n            clf_no_scale = ElasticNet(fit_intercept=fit_intercept, normalize=False, random_state=42)\n            clf_scale = ElasticNet(fit_intercept=fit_intercept, normalize=True, random_state=42)\n\n            clf_no_scale.fit(X_small, y_small)\n            clf_scale.fit(X_scaled, y_small)\n\n            if fit_intercept:\n                # intercept should be different due to scaling\n                assert clf_no_scale.intercept_ != clf_scale.intercept_\n            else:\n                #intercept should be zero in both cases\n                assert clf_no_scale.intercept_ == clf_scale.intercept_ == 0\n\n            #Coefs should be different due to scaling\n            assert_array_almost_equal(clf_no_scale.coef_, clf_scale.coef_ * X_small.std(axis=0), decimal = 4)\n", "compilation error"], "sample_534": ["compilation error", "compilation error", "compilation error", "compilation error", "    def test_contour_label_rotation():\n        fig, ax = plt.subplots()\n        x = np.linspace(-2, 2, 100)\n        y = x**2\n        cs = ax.contour(x, y, y, levels=[1, 4, 9])\n        ax.clabel(cs, rotation=45)\n        assert all(\n            (label.get_rotation() == 45 for label in ax.texts)\n        )"], "sample_772": ["compilation error", "    def test_feature_importances_constant_features(self):\n        # Test that feature importances are computed correctly\n        # when a feature is constant.\n        X = np.array([[1, 2, 3],\n                      [1, 2, 3],\n                      [1, 2, 3]])\n        y = np.array([0, 1, 0])\n        est = RandomForestClassifier(random_state=0)\n        est.fit(X, y)\n        importances = est.feature_importances_\n        assert_array_equal(importances, np.array([0, 0, 0]))\n\n", "def test_predict_proba_shape(name):\n    ForestEstimator = FOREST_ESTIMATORS[name]\n    # Test that predict_proba returns probabilities of the correct shape.\n    clf = ForestEstimator(n_estimators=10, random_state=0)\n    clf.fit(X, y)\n    if name in FOREST_CLASSIFIERS:\n        proba = clf.predict_proba(X)\n        assert proba.shape == (X.shape[0], clf.n_classes_)\n        assert_almost_equal(proba.sum(axis=1), np.ones(X.shape[0]))\n    else:\n        assert clf.predict_proba is None\n\n", "compilation error", "    def test_n_outputs(name):\n        # Test that n_outputs is handled correctly.\n        ForestEstimator = FOREST_ESTIMATORS[name]\n        X, y = iris.data, iris.target\n        clf = ForestEstimator(n_estimators=10)\n        \n        # Test single output\n        clf.fit(X, y)\n        assert clf.n_outputs_ == 1\n\n        y_multi = np.column_stack((y, y, y))\n        clf.fit(X, y_multi)\n        assert clf.n_outputs_ == 3 \n\n"], "sample_784": ["    def test_calibration_error_handling():\n        \"\"\"Test that calibration handles errors gracefully.\"\"\"\n        X, y = make_classification(n_samples=10, n_features=2,\n                                   random_state=42)\n        # Test with a classifier that doesn't implement predict_proba\n        clf = LinearSVC()\n        cal_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n        assert_raises(AttributeError, cal_clf.fit, X, y)\n\n        # Test with mismatched number of classes in training and calibration\n        clf = RandomForestClassifier(random_state=42)\n        cal_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n        cal_clf.fit(X, y)\n        X_test, y_test = make_classification(n_samples=5, n_features=2,\n                                            n_classes=3, random_state=42)\n        assert_raises(ValueError, cal_clf.predict_proba, X_test)\n", "    def test_calibration_errors(self):\n        # Test that calibration raises errors for invalid input\n        X, y = make_classification(n_samples=10, n_features=5)\n\n        # Check error if y is not 1-dimensional\n        clf = LinearSVC()\n        cal_clf = CalibratedClassifierCV(clf)\n        assert_raises(ValueError, cal_clf.fit, X, y[:, None])\n\n        # Check error if y is not integer\n        y_bad = y.astype(float)\n        assert_raises(ValueError, cal_clf.fit, X, y_bad)\n\n        # Check error if X is not 2-dimensional\n        assert_raises(ValueError, cal_clf.fit, X[:, None], y)\n", "    def test_calibration_invalid_cv(self):\n        n_samples = 100\n        X, y = make_classification(n_samples=2 * n_samples, n_features=6,\n                                   random_state=42)\n\n        X_train, y_train = X[:n_samples], y[:n_samples]\n\n        clf = MultinomialNB()\n        pc_clf = CalibratedClassifierCV(clf, method=\"isotonic\", cv=-1)\n        assert_raises(ValueError, pc_clf.fit, X_train, y_train)\n\n", "compilation error", "    def test_calibration_sparse_input(self):\n        \"\"\"Check if sparse input works correctly.\"\"\"\n        X, y = make_classification(n_samples=100, n_features=6, random_state=42)\n\n        # Convert X to sparse matrix\n        X_sparse = sparse.csr_matrix(X)\n\n        # Naive Bayes with calibration\n        clf = MultinomialNB()\n        pc_clf = CalibratedClassifierCV(clf, method='isotonic', cv=2)\n\n        # Fit on sparse input\n        pc_clf.fit(X_sparse, y)\n\n        # Predict probabilities on sparse input\n        prob_pos_pc_clf = pc_clf.predict_proba(X_sparse)[:, 1]\n"], "sample_745": ["def test_sparse_input():\n    X = sparse.csr_matrix([[1, 2], [3, 4]])\n    transformer = FunctionTransformer(\n        func=lambda x: x.toarray(),\n        accept_sparse=True\n    )\n    transformed = transformer.transform(X)\n    assert_array_equal(transformed, X.toarray())\n", "    def test_sparse_matrix_transformation():\n        X_sparse = sparse.csr_matrix([[1, 2], [3, 4]])\n        transformer = FunctionTransformer(lambda x: x * 2, accept_sparse=True)\n        transformed = transformer.transform(X_sparse)\n        \n        assert isinstance(transformed, sparse.csr_matrix)\n        assert_array_equal(transformed.toarray(), X_sparse.toarray() * 2)\n", "def test_function_transformer_sparse():\n    X_sparse = sparse.csr_matrix(np.random.randn(100, 10))\n\n    transformer = FunctionTransformer(validate=True, accept_sparse=True)\n\n    with pytest.warns(None) as record:\n        X_trans = transformer.fit_transform(X_sparse)\n    assert len(record) == 0\n    assert isinstance(X_trans, sparse.csr_matrix)\n", "    def test_sparse_input(self):\n        X_sparse = sparse.csr_matrix(np.arange(10).reshape((5, 2)))\n        transformer = FunctionTransformer(func=lambda x: x, accept_sparse=True)\n        assert_array_equal(transformer.transform(X_sparse), X_sparse)\n\n\n\n", "    def test_pickle(self):\n        import pickle\n        X = np.arange(10).reshape((5, 2))\n        transformer = FunctionTransformer(np.log1p)\n        pkl_transformer = pickle.dumps(transformer)\n        loaded_transformer = pickle.loads(pkl_transformer)\n        assert_array_equal(\n            loaded_transformer.transform(X),\n            transformer.transform(X),\n\n"], "sample_645": ["    def test_log_captured_according_to_config_option_upon_success(pytester: Pytester):\n        \"\"\"Test that upon success:\n        (1) `caplog` succeeded to capture the DEBUG message and assert on it => No `Exception` is raised.\n        (2) The `DEBUG` message appears in the `Captured log call` report as expected due to `--log-level=DEBUG`.\n        (3) The stdout, `INFO`, and `WARNING` messages DO appear in the test reports due to `--log-level=DEBUG`.\n        \"\"\"\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n            import logging\n\n                logging.debug('DEBUG log ' + 'message')\n                logging.info('INFO log ' + 'message')\n                logging.warning('WARNING log ' + 'message')\n                print('Print ' + 'message')\n\n                plugin = request.config.pluginmanager.getplugin('logging-plugin')\n                assert plugin.log_level == logging.DEBUG\n\n                with caplog.at_level(logging.DEBUG):\n                    function_that_logs()\n\n                if 'DEBUG log ' + 'message' not in caplog.text:\n                    raise Exception('caplog failed to ' + 'capture DEBUG')\n            \"\"\"\n        )\n\n        result = pytester.runpytest(\"--log-level=DEBUG\")\n        result.stdout.no_fnmatch_line(\"*Exception: caplog failed to capture DEBUG*\")\n        result.stdout.fnmatch_lines(\n            [\"*DEBUG log message*\", \"*Print message*\", \"*INFO log message*\", \"*WARNING log message*\"]\n        )\n        assert result.ret == 0\n\n\n\n", "    def test_log_report_captures_according_to_config_option_upon_success(pytester: Pytester) -> None:\n        \"\"\"Test that upon success:\n        (1) The `caplog` does NOT capture the DEBUG message, ensuring it's not printed\n        (2) The `DEBUG` message does NOT appear in the `Captured log call` report.\n        (3) The stdout, `INFO`, and `WARNING` messages DO appear in the test reports due to `--log-level=INFO`.\n        \"\"\"\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n            import logging\n\n                logging.debug('DEBUG log ' + 'message')\n                logging.info('INFO log ' + 'message')\n                logging.warning('WARNING log ' + 'message')\n                print('Print ' + 'message')\n\n                plugin = request.config.pluginmanager.getplugin('logging-plugin')\n                assert plugin.log_level == logging.INFO\n\n                with caplog.at_level(logging.DEBUG):\n                    function_that_logs()\n\n                assert 'DEBUG log ' + 'message' not in caplog.text\n\n            \"\"\"\n        )\n\n        result = pytester.runpytest(\"--log-level=INFO\")\n        result.stdout.no_fnmatch_line(\"*DEBUG log message*\")\n        result.stdout.fnmatch_lines(\n            [\"*Print message*\", \"*INFO log message*\", \"*WARNING log message*\"]\n        )\n        assert result.ret == 0\n", "def test_log_report_captures_according_to_config_option_upon_success(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_level == logging.INFO\n\n            logging.debug('DEBUG log ' + 'message')\n            logging.info('INFO log ' + 'message')\n            logging.warning('WARNING log ' + 'message')\n            print('Print ' + 'message')\n\n            assert True\n    \"\"\"\n    )\n\n    result = pytester.runpytest(\"--log-level=INFO\")\n    result.stdout.fnmatch_lines([\"*INFO log message*\", \"*WARNING log message*\", \"*Print message*\"])\n    assert result.ret == 0\n", "    def test_log_report_captures_according_to_config_option_upon_success(\n        pytester: Pytester,", "def test_log_report_captures_according_to_config_option_upon_success(pytester: Pytester) -> None:\n    \"\"\"Test that upon success:\n\n    (1) The `DEBUG` message IS included in the `Captured log call` report\n    due to `--log-level=DEBUG`.\n\n    (2) The `INFO`, `WARNING`, and `print` messages DO appear in the test reports.\n    \"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n            logging.debug('DEBUG log ' + 'message')\n            logging.info('INFO log ' + 'message')\n            logging.warning('WARNING log ' + 'message')\n            print('Print ' + 'message')\n\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_level == logging.DEBUG\n\n            with caplog.at_level(logging.DEBUG):\n                function_that_logs()\n\n            assert 'DEBUG log message' in caplog.text\n\n\n    \"\"\"\n    )\n\n    result = pytester.runpytest(\"--log-level=DEBUG\")\n    result.stdout.fnmatch_lines(\n        [\"*DEBUG log message*\", \"*INFO log message*\", \"*WARNING log message*\", \"*Print message*\"]\n    )\n    assert result.ret == 0\n\n"], "sample_58": ["    def test_non_field_errors_empty_data(self):\n        class SimpleForm(Form):\n            field = CharField()\n\n        form = SimpleForm({})\n        form.add_error(None, 'Error message')\n        self.assertHTMLEqual(\n            form.as_ul(),\n            '<li><ul class=\"errorlist nonfield\"><li>Error message</li></ul></li>'\n            '<li><label for=\"id_field\">Field:</label> <input type=\"text\" name=\"field\" id=\"id_field\"></li>',\n        )\n\n", "    def test_custom_widget_renderer(self):\n        class MyWidget(TextInput):\n            template_name = 'mywidget.html'\n\n        class MyForm(Form):\n            my_field = CharField(widget=MyWidget())\n\n        form = MyForm()\n        self.assertEqual(form.renderer, get_default_renderer())\n        # Ensure the custom widget's template is used\n        self.assertIn('mywidget.html', str(form['my_field']))\n", "    def test_empty_permitte_chars_clean(self):\n        class EmptyPermittedCharsForm(Form):\n            data = CharField(allowed_chars=set())\n\n        f = EmptyPermittedCharsForm({'data': 'abc'})\n        self.assertFalse(f.is_valid())\n        self.assertEqual(f.errors['data'], ['Enter only the allowed characters.'])\n", "    def test_render_with_custom_renderer(self):\n        class CustomRenderer(DjangoTemplates):\n                return f\"<div class='custom-rendered'>{super().render(template_name, context)}</div>\"\n\n        class CustomForm(Form):\n            name = CharField()\n            \n        form = CustomForm({'name': 'John'})\n        rendered = form.as_p(renderer=CustomRenderer())\n        self.assertEqual(rendered, \"<div class='custom-rendered'><p><label for=\\\"id_name\\\">Name:</label> <input type=\\\"text\\\" name=\\\"name\\\" value=\\\"John\\\" required id=\\\"id_name\\\"></p></div>\")\n", "    def test_hidden_widget_with_empty_initial(self):\n        class MyForm(Form):\n            hidden_field = CharField(widget=HiddenInput, initial='')\n\n        form = MyForm()\n        self.assertHTMLEqual(\n            form.as_p(),\n            '<p><input type=\"hidden\" name=\"hidden_field\" id=\"id_hidden_field\"></p>'\n        )\n"], "sample_353": ["    def test_createsuperuser_with_custom_user_model_no_password_field(self):\n        \"\"\"\n        Test creating a superuser with a custom user model that doesn't have a\n        password field.\n        \"\"\"\n        @override_settings(AUTH_USER_MODEL='auth_tests.NoPasswordFieldUser')\n            new_io = StringIO()\n            call_command(\n                'createsuperuser',\n                interactive=False,\n                username='testuser',\n                email='test@example.com',\n                stdout=new_io,\n            )\n            self.assertEqual(new_io.getvalue().strip(), 'Superuser created successfully.')\n            user = NoPasswordFieldUser.objects.get(username='testuser')\n            self.assertEqual(user.email, 'test@example.com')\n\n        test(self)\n", "    def test_createsuperuser_command_with_custom_user_model_and_different_username_field(self):\n        \"\"\"\n        Test that createsuperuser works with a custom User model that has a different 'USERNAME_FIELD'.\n        \"\"\"\n        class CustomUser(AbstractBaseUser):\n            username = models.CharField(unique=True, max_length=150)\n            email = models.EmailField(unique=True)\n            name = models.CharField(max_length=150)\n            USERNAME_FIELD = 'email'\n\n            objects = CustomUserManager()\n\n        settings.AUTH_USER_MODEL = 'auth_tests.CustomUser'\n\n        new_io = StringIO()\n        call_command(\n            'createsuperuser',\n            interactive=False,\n            username='test@example.com',\n            email='test@example.com',\n            stdout=new_io,\n        )\n        command_output = new_io.getvalue().strip()\n        self.assertEqual(command_output, 'Superuser created successfully.')\n        user = CustomUser.objects.get(email='test@example.com')\n        self.assertEqual(user.email, 'test@example.com')\n", "    def test_createsuperuser_with_custom_user_model_with_required_field(self):\n        class CustomUser(User):\n            first_name = models.CharField(max_length=150, blank=False)\n\n        with override_settings(AUTH_USER_MODEL='auth_tests.CustomUser'):\n            new_io = StringIO()\n            entered_first_names = ['John', 'John']\n\n                return entered_first_names.pop(0)\n            \n            @mock_inputs({'password': 'nopasswd', 'username': 'joe', 'first_name': return_first_names, 'email': 'joe@example.com'})\n                call_command(\n                    'createsuperuser',\n                    interactive=True,\n                    stdin=MockTTY(),\n                    stdout=new_io,\n                    stderr=new_io,\n                )\n                self.assertEqual(\n                    new_io.getvalue().strip(),\n                    'Superuser created successfully.'\n                )\n                user = CustomUser.objects.get(username='joe')\n                self.assertEqual(user.first_name, 'John')\n\n            test(self)\n\n\n\n", "    def test_createsuperuser_requires_required_field(self):\n        new_io = StringIO()\n\n        @mock_inputs({'password': 'password', 'username': 'joe', 'required_field': 'required_value'})\n            call_command(\n                'createsuperuser',\n                interactive=True,\n                stdin=MockTTY(),\n                stdout=new_io,\n                stderr=new_io,\n            )\n            self.assertEqual(new_io.getvalue().strip(), 'Superuser created successfully.')\n\n        test(self)\n", "    def test_createsuperuser_with_custom_user_model_and_required_fields(self):\n        @override_settings(AUTH_USER_MODEL='auth_tests.CustomUserWithRequiredFields')\n            new_io = StringIO()\n            entered_passwords = ['password', 'password', 'valid_password', 'valid_password']\n\n                return entered_passwords.pop(0)\n\n            @mock_inputs({\n                'password': bad_then_good_password,\n                'username': 'joe1234567890',\n                'first_name': 'John',\n                'last_name': 'Doe',\n                'email': 'joey@example.com',\n                'bypass': 'n',\n            })\n                call_command(\n                    'createsuperuser',\n                    interactive=True,\n                    stdin=MockTTY(),\n                    stdout=new_io,\n                    stderr=new_io,\n                )\n\n            create_superuser()\n            self.assertEqual(\n                new_io.getvalue().strip(),\n                'Error: This password is entirely numeric.\\n'\n                'Superuser created successfully.'\n            )\n\n        test(self)\n"], "sample_62": ["    def test_unregister_model(self):\n        self.site.register(Person)\n        self.assertTrue(self.site.is_registered(Person))\n        self.site.unregister(Person)\n        self.assertFalse(self.site.is_registered(Person))\n", "    def test_register_with_empty_string_raises_error(self):\n        with self.assertRaisesMessage(ValueError, 'Model cannot be an empty string'):\n            register('')(NameAdmin)\n\n", "    def test_unregister(self):\n        self.site.register(Person)\n        self.assertTrue(self.site.is_registered(Person))\n        self.site.unregister(Person)\n        self.assertFalse(self.site.is_registered(Person))\n", "    def test_register_with_same_model_twice(self):\n        register(Person)(NameAdmin)\n        with self.assertRaises(admin.sites.AlreadyRegistered):\n            register(Person)(NameAdmin)\n", "    def test_unregister_model(self):\n        self.site.register([Person, Place])\n        self.assertTrue(self.site.is_registered(Person))\n        self.site.unregister(Person)\n        self.assertFalse(self.site.is_registered(Person))\n        self.assertTrue(self.site.is_registered(Place))\n"], "sample_876": ["def test_mlp_early_stopping_validation_fraction(MLPEstimator):\n    \"\"\"Checks if validation_fraction is correctly used with early stopping.\"\"\"\n    X = X_iris\n    y = y_iris\n\n    # Test with default validation_fraction (10% by default)\n    mlp = MLPEstimator(\n        validation_fraction=0.1, early_stopping=True, max_iter=50\n    )\n    mlp.fit(X, y)\n    assert len(mlp.validation_scores_) > 0\n\n    # Test with different validation_fraction\n\n    validation_fraction = 0.2\n    mlp = MLPEstimator(\n        validation_fraction=validation_fraction, early_stopping=True, max_iter=50\n    )\n    mlp.fit(X, y)\n    assert len(mlp.validation_scores_) > 0\n    # The number of validation scores should correspond to the\n    # validation_fraction\n    assert len(mlp.validation_scores_) == int(50 * validation_fraction)\n", "def test_mlp_warm_start_with_constant_learning_rate(MLPEstimator):\n    # Test that warm start works with constant learning rate\n    mlp = MLPEstimator(\n        solver=\"sgd\",\n        learning_rate=\"constant\",\n        learning_rate_init=0.01,\n        max_iter=10,\n        random_state=0,\n        warm_start=True,\n    )\n    mlp.fit(X_iris, y_iris)\n    initial_loss = mlp.loss_\n    mlp.set_params(max_iter=20)\n    mlp.fit(X_iris, y_iris)\n    assert mlp.loss_ < initial_loss\n", "    def test_mlp_warm_start_with_partial_fit(self):\n        mlp = MLPClassifier(\n            max_iter=10,\n            random_state=0,\n            warm_start=True,\n            early_stopping=False,\n        )\n        mlp.partial_fit(X_digits[:50], y_digits[:50])\n        n_iter = mlp.n_iter_\n        mlp.partial_fit(X_digits[50:100], y_digits[50:100])\n        assert mlp.n_iter_ == n_iter + 1\n\n", "def test_mlp_sparse_matrix_with_early_stopping(MLPEstimator):\n    X = csr_matrix(X_digits[:100])\n    y = y_digits[:100]\n    mlp = MLPEstimator(early_stopping=True)\n    mlp.fit(X, y)\n    assert len(mlp.validation_scores_) > 0\n", "def test_mlp_alpha_zero(MLPEstimator):\n    # Test that setting alpha to zero doesn't raise an error\n    mlp = MLPEstimator(alpha=0, hidden_layer_sizes=(5,), random_state=0)\n    mlp.fit(X_digits[:50], y_digits[:50])\n"], "sample_1205": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_991": ["compilation error", "compilation error", "def test_issue_19811():\n    i = Symbol('i')\n    assert Product(i, (i, 1, 5)).subs(i, 2) == 2*3*4*5\n", "compilation error", "compilation error"], "sample_346": ["    def test_cache_page_decorator_http_request(self):\n        class MyClass:\n            @cache_page(3600)\n                return HttpResponse()\n        msg = (\n            \"cache_page didn't receive an HttpRequest. If you are decorating \"\n            \"a classmethod, be sure to use @method_decorator.\"\n        )\n        with self.assertRaisesMessage(TypeError, msg):\n            MyClass().a_view(HttpRequest())\n", "    def test_decorator_from_middleware_with_args(self):\n        class TestMiddleware:\n                self.view_func = view_func\n                self.arg1 = arg1\n                self.arg2 = arg2\n\n                return self.view_func(request, self.arg1, self.arg2)\n\n        @decorator_from_middleware_with_args(TestMiddleware)\n            return f\"{arg1}{arg2}\"\n\n        request = HttpRequest()\n        response = my_view(request, 'hello', 'world')\n        self.assertEqual(response, 'helloworld')\n", "    def test_cache_control_decorator(self):\n        @cache_control(max_age=3600, private=True)\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(r.headers['Cache-Control'], 'max-age=3600, private')\n\n", "    def test_cache_control_decorator_with_http_response(self):\n        @cache_control(max_age=123, private=True, s_maxage=456)\n            return HttpResponse()\n        response = a_view(HttpRequest())\n        self.assertEqual(response.headers['Cache-Control'], 'max-age=123, private, s-maxage=456')\n", "    def test_method_decorator_on_classmethod(self):\n        \"\"\"\n        Test that @method_decorator applied to a classmethod works as expected.\n        \"\"\"\n        class MyClass:\n            @method_decorator(simple_dec_m)\n            @classmethod\n                return \"test:\" + arg\n        self.assertEqual(\"test:hello\", MyClass.method(\"hello\"))\n"], "sample_350": ["    def test_values_list_with_different_column_names(self):\n        qs1 = Number.objects.values_list(num='num')\n        qs2 = Number.objects.values_list(new_num='num')\n        self.assertEqual(list(qs1.union(qs2)), list(qs1))\n", "    def test_union_with_select_related(self):\n        Celebrity.objects.create(name='John Doe')\n        qs1 = Number.objects.select_related('celebrity').filter(num=1)\n        qs2 = Number.objects.select_related('celebrity').filter(num=2)\n        self.assertQuerysetEqual(qs1.union(qs2),\n                                [\n                                    {'num': 1, 'other_num': 9, 'celebrity': None},\n                                    {'num': 2, 'other_num': 8, 'celebrity': None},\n                                ],\n                                lambda x: (x['num'], x['other_num'], x['celebrity'])\n                                )\n", "    def test_union_with_values_list_and_filter(self):\n        ReservedName.objects.create(name='rn1', order=7)\n        qs1 = Number.objects.filter(num=1).values_list('num', flat=True)\n        qs2 = ReservedName.objects.values_list('order', flat=True).filter(order=7)\n        self.assertCountEqual(qs1.union(qs2), [1, 7])\n", "    def test_filtering_after_union(self):\n        qs1 = Number.objects.filter(num__lt=5)\n        qs2 = Number.objects.filter(num__gt=5)\n        union_qs = qs1.union(qs2)\n        self.assertNumbersEqual(union_qs.filter(num__gte=3), [3, 4, 6, 7, 8, 9])\n", "    def test_union_with_different_models_and_values(self):\n        ReservedName.objects.create(name='rn1', order=1)\n        qs1 = Number.objects.values('num')\n        qs2 = ReservedName.objects.values('name')\n        self.assertEqual(\n            list(qs1.union(qs2)),\n            [{'num': n} for n in range(10)] + [{'name': 'rn1'}],\n        )\n\n"], "sample_110": ["    def test_pickle_in_lookup_with_subqueries(self):\n        \"\"\"\n        Test pickling and unpickling a QuerySet with an __in lookup that\n        involves a subquery.\n        \"\"\"\n        subquery = Event.objects.filter(title__startswith='Event').values('group_id')\n        events = Event.objects.filter(group__in=subquery)\n\n        dumped = pickle.dumps(events)\n        reloaded = pickle.loads(dumped)\n        self.assertEqual(list(reloaded), list(events))\n", "    def test_pickle_m2m_value_queryset(self):\n        m2m = M2MModel.objects.create()\n        group = Group.objects.create(name='foo')\n        m2m.groups.add(group)\n        qs = m2m.groups.all()\n        self.assert_pickles(qs)\n\n", "    def test_in_lookup_with_subquery(self):\n        es = Event.objects.annotate(group_id=models.Subquery(Group.objects.filter(name='Group 1').values('id')[:1])).filter(group_id__in=Event.objects.values('group__id'))\n        self.assert_pickles(es)\n", "    def test_pickle_exists_queryset_with_f_expressions(self):\n        group = Group.objects.create(name='group')\n        Event.objects.create(title='event', group=group)\n        groups = Group.objects.annotate(\n            has_event=models.Exists(\n                Event.objects.filter(group_id=models.OuterRef('id')).annotate(\n                    title_length=models.Length('title')\n                )\n            ),\n        )\n        groups2 = pickle.loads(pickle.dumps(groups))\n        self.assertSequenceEqual(groups2.filter(has_event=True), [group])\n\n", "    def test_in_lookup_queryset_with_values(self):\n        events = Event.objects.filter(group__in=Group.objects.values('name'))\n\n        with self.assertNumQueries(0):\n            dumped = pickle.dumps(events.query)\n\n        with self.assertNumQueries(0):\n            reloaded = pickle.loads(dumped)\n            reloaded_events = Event.objects.none()\n            reloaded_events.query = reloaded\n\n        self.assertSequenceEqual(reloaded_events, [self.e1])\n"], "sample_593": ["    def test_summarize_coord_multiindex_variable(multiindex):\n        idx = multiindex.x.variable.to_index_variable()\n        formatted = fh.summarize_variable(\"x\", idx, is_index=True)\n        assert \"(level_1, level_2)\" in formatted\n        assert \"MultiIndex\" in formatted\n        assert \"<span class='xr-has-index'>x</span>\" in formatted\n", "    def test_summarize_coord(multiindex):\n        formatted = fh.summarize_coord(\"x\", multiindex.x)\n        assert \"level_1\" in formatted\n        assert \"level_2\" in formatted\n", "def test_summarize_attrs_empty_attr(dataset):\n    ds = dataset.attrs = {}\n    formatted = fh.summarize_attrs(ds.attrs)\n    assert formatted == \"<dl class='xr-attrs'></dl>\"\n\n", "def test_summarize_variable_dtype_object(multiindex):\n    var = multiindex.x.variable\n    formatted = fh.summarize_variable(\"x\", var)\n    assert \"object\" in formatted\n", "def test_summarize_coord_with_dask_array(dask_dataarray):\n    ds = xr.Dataset({\"foo\": dask_dataarray})\n    summary = fh.summarize_coord(\"foo\", ds[\"foo\"])\n    assert \"dask.array\" in summary\n\n"], "sample_598": ["    def test_summarize_coord(self):\n        coord = xr.DataArray([1, 2, 3], dims=[\"x\"], attrs={\"units\": \"m\"})\n        expected = \"    x: <xarray.DataArray (x: 3)>\\n        array([1, 2, 3])\\n        Dimensions: (x)\\n        Attributes:\\n            units: m\"\n        assert formatting.summarize_coord(\"x\", coord) == expected\n", "    def test_format_array_flat_empty_array(self):\n        actual = formatting.format_array_flat(np.array([]), 5)\n        expected = \"\"\n        assert actual == expected\n", "    def test_format_array_flat_empty(self):\n        actual = formatting.format_array_flat(np.array([]), 5)\n        expected = \"\"\n        assert expected == actual\n", "    def test_format_array_flat_empty(self):\n        actual = formatting.format_array_flat(np.array([]), 5)\n        expected = \"\"\n        assert expected == actual\n", "    def test_format_item_datetime64(self):\n        actual = formatting.format_item(np.datetime64(\"2023-10-27\"))\n        expected = \"2023-10-27\"\n        assert expected == actual\n\n"], "sample_373": ["    def test_simplify_regex_nested_groups(self):\n        tests = (\n            # Nested groups.\n            (r'^(?P<a>(?P<b>\\w+)/(?P<c>\\w+))/?$', '/<a>/'),\n            (r'^(?P<a>(?P<b>\\w+)/(?P<c>\\w+))/?(.*)$', '/<a>/<var>'),\n        )\n        for pattern, output in tests:\n            with self.subTest(pattern=pattern):\n                self.assertEqual(simplify_regex(pattern), output)\n\n", "    def test_simplify_regex_empty(self):\n        self.assertEqual(simplify_regex(''), '/')\n", "    def test_simplify_regex_with_optional_group(self):\n        pattern = r'^a/(?P<optional>b)?$'\n        output = '/a/<optional>'\n        self.assertEqual(simplify_regex(pattern), output)\n", "    def test_simplify_regex_with_optional_capture(self):\n        self.assertEqual(simplify_regex(r'^(?P<a>\\w+)/b/(?:\\w+)?'), '/<a>/b/')\n", "    def test_simplify_regex_with_quantifiers(self):\n        tests = (\n            (r\"^(\\w+){2,3}$\", \"/<var><var>\"),\n            (r\"^(\\w+){2,3}/$\", \"/<var><var>/\"),\n            (r\"^(\\w+)*$\", \"/<var>*\"),\n            (r\"^(\\w+)+$\", \"/<var>+\"),\n            (r\"^(\\w+)(?P<group>\\w+)?$\", \"/<var><group>?\"),\n        )\n        for pattern, output in tests:\n            with self.subTest(pattern=pattern):\n                self.assertEqual(simplify_regex(pattern), output)\n\n"], "sample_667": ["    def test_tmpdir_factory_different_basetemp(testdir, tmp_path_factory):\n        from _pytest.tmpdir import TempdirFactory\n\n        testdir.makepyfile(\n            \"\"\"\n                assert str(tmpdir_factory.getbasetemp()) != str(tmpdir_factory.getbasetemp())\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        assert result.ret == 1\n", "def test_tmpdir_factory_with_specific_root(testdir, tmp_path):\n        mytemp = testdir.tmpdir.mkdir(\"myroot\")\n        p = testdir.makepyfile(\n            \"\"\"\n        import pytest\n            assert tmpdir_factory._basetemp == path.join(str(mytemp), '/myroot')\n        \"\"\"\n        )\n        result = testdir.runpytest(p, \"--basetemp=%s\" % mytemp)\n        assert result.ret == 0\n\n\n\n", "    def test_tmp_path_empty_dir(tmp_path):\n        assert tmp_path.is_dir()\n        assert not list(tmp_path.iterdir())\n", "    def test_tmpdir_with_long_path(testdir):\n        \"\"\"#4428: ensure long paths don't cause issues\"\"\"\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n                long_path_component = 'a' * 250\n                p = tmpdir.join(\n                    long_path_component, long_path_component, long_path_component\n                )\n                p.ensure_dir()\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        assert result.ret == 0\n", "    def test_tmpdir_factory_with_explicit_basetemp(testdir, tmp_path):\n        mytemp = testdir.tmpdir.mkdir(\"mytemp\")\n        p = testdir.makepyfile(\n            \"\"\"\n            import pytest\n                assert str(tmpdir_factory.getbasetemp()) == str(mytemp)\n        \"\"\"\n        )\n\n        result = testdir.runpytest(p, \"--basetemp=%s\" % mytemp)\n        assert result.ret == 0\n"], "sample_1201": ["compilation error", "compilation error", "def test_cgs_gauss_conversions():\n    assert convert_to(statvolt, volt, cgs_gauss) == 2997924580 * volt / 10**6\n    assert convert_to(volt, statvolt, cgs_gauss) == 10**6 * statvolt / 2997924580\n    assert convert_to(tesla, gauss, cgs_gauss) == 10**4 * gauss\n    assert convert_to(gauss, tesla, cgs_gauss) == tesla / 10**4\n    assert convert_to(weber, maxwell, cgs_gauss) == 10**8 * maxwell\n    assert convert_to(maxwell, weber, cgs_gauss) == weber / 10**8\n", "def test_cgs_gauss_convert_electromagnetism():\n    assert convert_to(statvolt, volt, cgs_gauss) == 2997924580/10**6*volt\n    assert convert_to(volt, statvolt, cgs_gauss) == 10**6*statvolt/299792458\n    assert convert_to(statampere, ampere, cgs_gauss) == 2997924580*ampere\n    assert convert_to(ampere, statampere, cgs_gauss) == ampere/2997924580\n    assert convert_to(gauss, tesla, cgs_gauss) == 10**-4*tesla\n    assert convert_to(tesla, gauss, cgs_gauss) == 10**4*gauss\n    assert convert_to(weber, maxwell, cgs_gauss) == 10**-8*maxwell\n    assert convert_to(maxwell, weber, cgs_gauss) == 10**8*weber\n\n", "compilation error"], "sample_206": ["    def test_filefield_empty_string(self):\n        d = Document(myfile='')\n        d.save()\n        self.assertEqual(d.myfile, '')\n        self.assertFalse(os.path.exists(d.myfile.path))\n", "    def test_filefield_with_custom_storage_backend_and_path(self):\n        class CustomStorage(Storage):\n                super().__init__(*args, **kwargs)\n\n                return os.path.join('custom_path', name)\n        \n        with tempfile.TemporaryDirectory() as tmp_dir:\n            with override_settings(MEDIA_ROOT=Path(tmp_dir)):\n                custom_storage = CustomStorage()\n                document = Document(myfile='test_file.txt')\n                document.myfile.storage = custom_storage\n                with open(__file__, 'rb') as fp:\n                    file1 = File(fp, name='test_file.txt')\n                    document.myfile.save('test_file.txt', file1)\n                    self.assertEqual(document.myfile.url, os.path.join('custom_path', 'test_file.txt'))\n", "    def test_default_storage_with_custom_upload_to(self):\n        \"\"\"\n        Ensure that the default storage is used when upload_to is set, even if\n        no custom storage is provided.\n        \"\"\"\n        with tempfile.TemporaryDirectory() as temp_dir:\n            with override_settings(MEDIA_ROOT=Path(temp_dir)):\n                document = Document(myfile='test_file.py')\n                document.myfile.upload_to = 'test_upload_to'\n                document.save()\n                self.assertTrue(os.path.exists(os.path.join(temp_dir, 'test_upload_to', 'test_file.py')))\n", "    def test_filefield_default(self):\n        \"\"\"\n        FileField should accept a default value (either a File object or a string\n        path) and should store the file correctly.\n        \"\"\"\n        default_file_path = 'test_file.txt'\n\n        class MyDocumentWithDefaultFile(models.Model):\n            myfile = models.FileField(upload_to='unused', default=default_file_path)\n\n        doc = MyDocumentWithDefaultFile.objects.create()\n        self.assertEqual(doc.myfile.name, default_file_path)\n", "    def test_filefield_default_storage_is_instance(self):\n        \"\"\"\n        The default storage associated with a FileField should be an instance of\n        Storage, not just a class.\n        \"\"\"\n        d = Document.objects.create(myfile='something.txt')\n        self.assertIsInstance(d.myfile.storage, Storage)\n"], "sample_307": ["    def test_midnight_noon(self):\n        self.assertEqual(dateformat.format(datetime(2000, 1, 1, 0, 0), 'P'), 'midnight')\n        self.assertEqual(dateformat.format(datetime(2000, 1, 1, 12, 0), 'P'), 'noon')\n", "    def test_format_with_aware_datetime_naive_timezone(self):\n        # Test that we correctly handle cases where a timezone-aware datetime\n        # object has a timezone that's considered naive by the system.\n        dt = datetime(2009, 5, 16, 5, 30, 30, tzinfo=get_fixed_timezone(0))\n        with self.settings(USE_TZ=True):\n            self.assertEqual(dateformat.format(dt, 'O'), '+0000')\n            self.assertEqual(dateformat.format(dt, 'r'), 'Sat, 16 May 2009 05:30:30 +0000')\n", "    def test_format_with_explicit_timezone(self):\n        dt = datetime(2023, 11, 15, 10, 30, 0, tzinfo=get_fixed_timezone(-360))\n        self.assertEqual(dateformat.format(dt, 'Z'), '-3600')\n", "    def test_time_format_midnight_and_noon(self):\n        tests = [\n            (datetime(2000, 1, 1, 0, 0), 'midnight'),\n            (datetime(2000, 1, 1, 12, 0), 'noon'),\n\n        ]\n        for dt, expected in tests:\n            with self.subTest(dt=dt):\n                self.assertEqual(dateformat.format(dt, 'P'), expected)\n", "    def test_w_format(self):\n        tests = [\n            (datetime(2023, 10, 29), '6'),\n            (datetime(2023, 10, 30), '0'),\n            (datetime(2023, 10, 31), '1'),\n        ]\n        for dt, expected in tests:\n            with self.subTest(dt=dt):\n                self.assertEqual(dateformat.format(dt, 'w'), expected)\n"], "sample_272": ["    def test_backwards_nothing_to_do_with_circular_dependencies(self):\n        r\"\"\"\n        If the current state satisfies the given target, do nothing, even with circular dependencies.\n\n        a: 1 <--- 2\n        b:    \\- 1 <--- 2\n        c:     \\- 1\n\n        If a1 is applied already and a2 is not, and we're asked to migrate to\n        a1, don't apply or unapply b1 or c1, regardless of their current state.\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        b2_impl = FakeMigration('b2')\n        b2 = ('b', '2')\n        c1_impl = FakeMigration('c1')\n        c1 = ('c', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(b2, b2_impl)\n        graph.add_node(c1, c1_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, b2, b1)\n        graph.add_dependency(None, b2, a2)\n        graph.add_dependency(None, c1, a1)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n        })\n\n        plan = executor.migration_plan({a1})\n\n        self.assertEqual(plan, [])\n", "    def test_circular_dependency_detection(self):\n        \"\"\"\n        A circular dependency causes a ValueError.\n\n        a: 1 <---- 2 <--\\\n              \\ \\- 3 <--/\n               \\\n        b:      \\- 1\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        a3_impl = FakeMigration('a3')\n        a3 = ('a', '3')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(a3, a3_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, a3, a2)\n        graph.add_dependency(None, a1, a3)\n        graph.add_dependency(None, b1, a1)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {})\n\n        with self.assertRaisesMessage(ValueError, 'Circular dependency detected'):\n            executor.migration_plan({a1})\n", "    def test_migrate_target_none(self):\n        \"\"\"\n        Test the case where target is None.\n\n        When target is None, it should apply all migrations which are not\n        already applied.\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, a2, a1)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n        })\n\n        plan = executor.migration_plan(None)\n\n        self.assertEqual(plan, [(a2_impl, False), (b1_impl, False)]) \n\n", "    def test_circular_dependencies(self):\n        \"\"\"\n        Test that circular dependencies are handled correctly.\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, a1, b1)\n        graph.add_dependency(None, b1, a2)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {})\n\n        with self.assertRaises(CircularDependencyError):\n            executor.migration_plan({a1})\n", "    def test_circular_dependencies(self):\n        \"\"\"\n        Detect circular dependencies in migrations.\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, a1, b1)\n        graph.add_dependency(None, b1, a2)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {})\n\n        with self.assertRaises(CircularDependencyError):\n            executor.migration_plan({a1})\n"], "sample_1038": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_issue_2989():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n\n    assert (A + B).as_explicit() == Matrix([[A[0, 0] + B[0, 0], A[0, 1] + B[0, 1]],\n                                            [A[1, 0] + B[1, 0], A[1, 1] + B[1, 1]]])\n\n\n"], "sample_695": ["    def test_node_repr_failure(pytester: Pytester) -> None:\n        \"\"\"Test repr_failure handles fulltrace option correctly.\"\"\"\n\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.parametrize(\"x\", [0, 1])\n                if x:\n                    raise Exception(\"My Exception!\")\n            \n            \"\"\"\n        )\n        result = pytester.runpytest(\"--fulltrace\")\n        result.stdout.fnmatch_lines(\n            [\n                \"*test_func*1*My Exception!*Traceback*\",\n                \"*/test_func*\",\n                \"*1 failed in*\",\n            ]\n        )\n", "    def test_node_keywords_extra_keyword_matches(pytester: Pytester) -> None:\n        p = pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.slow\n                pass\n        \"\"\"\n        )\n        result = pytester.runpytest()\n        assert result.ret == 0\n        items = pytester.getitems(p)\n        assert items[0].extra_keyword_matches == {\"slow\"}\n\n", "    def test_nodeid_handles_empty_string_correctly(pytester: Pytester) -> None:\n        items = pytester.getitems(\n            \"\"\"\n                pass\n        \"\"\"\n        )\n        assert items[0].nodeid == \"test\"\n\n", "    def test_nodeid_with_absolute_path(pytester: Pytester, tmp_path: Path) -> None:\n        \"\"\"Test that nodeid generation handles absolute paths correctly\n\n        See Issue #10198\n        \"\"\"\n        filepath = tmp_path / \"test_something.py\"\n\n        pytester.makepyfile(\n            \"\"\"\n                pass\n        \"\"\",\n            filepath,\n        )\n        result = pytester.runpytest(filepath)\n        assert result.exit_code == 0\n        assert result.stdout.str().find(\n            str(filepath).replace(str(tmp_path), \"\")\n        ) != -1\n", "def test_node_keywords_extra_keywords(pytester: Pytester) -> None:\n    \"\"\"Ensure `extra_keyword_matches` is properly updated.\"\"\"\n\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.foo\n        @pytest.mark.bar\n\n        class TestClass:\n            @pytest.mark.baz\n    \"\"\"\n    )\n    result = pytester.runpytest()\n\n    assert result.stdout.str().count(\"foo\") == 1\n    assert result.stdout.str().count(\"bar\") == 1\n    assert result.stdout.str().count(\"baz\") == 1\n\n    # Check if extra keywords are propagated correctly\n\n    items = pytester.items(\"test_foo_bar.py\")\n    assert items[0].extra_keyword_matches == {\"foo\", \"bar\"}\n    assert items[1].extra_keyword_matches == {\"baz\"}\n    "], "sample_280": ["    def test_annotation_with_related_field_in_aggregate(self):\n        qs = Book.objects.annotate(\n            avg_age=Avg(F('authors__age')),\n        ).filter(avg_age__gt=30)\n        self.assertQuerysetEqual(\n            qs, [self.b4, self.b1, self.b6, self.b5], lambda b: b.pk)\n", "    def test_aggregation_default_subquery_with_outerref(self):\n        # Test default values with Subqueries using OuterRef\n        qs = Publisher.objects.annotate(\n            latest_book_pubdate=Subquery(\n                Book.objects.filter(\n                    publisher=OuterRef('pk'),\n                ).order_by('-pubdate').values('pubdate')[:1]\n            )\n        ).annotate(\n            average_rating=Avg('book__rating', output_field=DecimalField(), default=Decimal('2.5')),\n        )\n        self.assertEqual(qs[0].average_rating, Decimal('3.875'))\n        self.assertEqual(qs[1].average_rating, Decimal('2.5'))\n", "    def test_aggregation_default_using_field_from_related_model(self):\n        result = Book.objects.annotate(\n            publisher_name=F('publisher__name'),\n            avg_rating=Avg('rating', default=Value(self.p1.name)),\n        ).filter(publisher_name=self.p1.name).values('avg_rating')\n        self.assertEqual(list(result), [Decimal('3.80')])\n", "    def test_aggregation_default_using_nested_expression(self):\n        result = Book.objects.aggregate(\n            value=Sum('price', default=Coalesce(Avg('rating'), Count('authors'))),\n        )\n        self.assertAlmostEqual(result['value'], Decimal('135.28'), places=2)\n", "    def test_complex_default_expression_with_case(self):\n        result = Book.objects.aggregate(\n            value=Sum(\n                'price',\n                filter=Q(rating__lt=3.0),\n                default=Case(\n                    When(pages__gt=500, then=Value(10)),\n                    default=Value(5),\n                ),\n            ),\n        )\n        self.assertEqual(result['value'], 61.72)\n"], "sample_525": ["    def test_subfigure_polar(self):\n        fig = plt.figure()\n        gs = fig.add_gridspec(1, 2)\n        ax1 = fig.add_subplot(gs[0, 0], projection='polar')\n        ax2 = fig.add_subplot(gs[0, 1])\n        ax1.plot([0, 1], [1, 2])\n        ax2.plot([1, 2], [2, 1])\n        # Test that both axes have been created properly \n        assert isinstance(ax1, PolarAxes)\n        assert isinstance(ax2, Axes)\n        \n", "    def test_subfigure_empty_gs(self):\n        fig = plt.figure()\n        gs = fig.add_gridspec(1, 2)\n        subfig = fig.add_subfigure(gs[0])\n        assert len(subfig.axes) == 0\n        subfig.subplots()\n        assert len(subfig.axes) == 1\n", "    def test_figure_pickle_with_subfigure(self):\n        fig = plt.figure()\n        sfig = fig.add_subfigure(1, 1, 1)\n        sfig.add_subplot(111)\n        with io.BytesIO() as buf:\n            pickle.dump(fig, buf)\n            buf.seek(0)\n            fig2 = pickle.load(buf)\n        assert isinstance(fig2.axes[0], SubFigure)\n\n", "    def test_add_subplot_default_projection(fig, ax):\n        ax2 = fig.add_subplot()\n        assert ax2.name == 'rectilinear'\n\n", "    def test_subplot_mosaic_with_fig_size(self, fig_test, fig_ref):\n        fig_test.set_size_inches(4, 4)\n        grid_axes = fig_test.subplot_mosaic([['A', 'B'], ['C', 'D']])\n\n        for k, ax in grid_axes.items():\n            ax.set_title(k)\n\n        fig_ref.set_size_inches(4, 4)\n        gs = fig_ref.add_gridspec(2, 2)\n        fig_ref.add_subplot(gs[0, 0]).set_title('A')\n        fig_ref.add_subplot(gs[0, 1]).set_title('B')\n        fig_ref.add_subplot(gs[1, 0]).set_title('C')\n        fig_ref.add_subplot(gs[1, 1]).set_title('D')\n"], "sample_829": ["def test_incremental_pca_empty_input():\n    # Test that IncrementalPCA raises an error with empty input\n    ipca = IncrementalPCA()\n    with pytest.raises(ValueError, match='Empty data array encountered'):\n        ipca.partial_fit([])\n", "    def test_incremental_pca_max_iter():\n        # Test that the max_iter parameter works as expected.\n\n        rng = np.random.RandomState(1999)\n        n_samples = 100\n        n_features = 3\n        X = rng.randn(n_samples, n_features)\n\n        # Test with a low number of iterations\n        ipca = IncrementalPCA(n_components=2, batch_size=10, max_iter=1).fit(X)\n        assert ipca.n_iter_ < ipca.max_iter\n\n        # Test with a high number of iterations\n        ipca = IncrementalPCA(n_components=2, batch_size=10, max_iter=10).fit(X)\n        assert ipca.n_iter_ == ipca.max_iter\n", "    def test_partial_fit_empty_data():\n        # Test that partial_fit raises an error when called with empty data.\n        ipca = IncrementalPCA(n_components=2)\n        with pytest.raises(ValueError):\n            ipca.partial_fit(np.array([]))\n", "    def test_incremental_pca_get_covariance():\n        # Test get_covariance method for IncrementalPCA\n        rng = np.random.RandomState(0)\n        X = rng.randn(100, 5)\n        ipca = IncrementalPCA(n_components=3).fit(X)\n        cov = ipca.get_covariance()\n\n        # Check that the covariance matrix has the correct shape\n        assert cov.shape == (5, 5)\n\n        # Check that the covariance matrix is symmetric\n        assert np.allclose(cov, cov.T)\n", "    def test_incremental_pca_zero_variance():\n        # Test that IncrementalPCA handles zero variance features correctly.\n        X = np.array([[1, 0, 0], [1, 0, 0], [1, 0, 0]])\n        ipca = IncrementalPCA(n_components=2).fit(X)\n        assert ipca.explained_variance_.shape[0] == 2\n        assert np.isfinite(ipca.explained_variance_ratio_).all()\n"], "sample_604": ["    def test__calculate_col_width(self):\n       cases = [\n           (set([\"variable_name1\", \"variable_name2\"]), 15),\n           (set([\"very_long_variable_name\"]), 23),\n           (set([\"var1\", \"var2\", \"var3\"]), 5),\n       ]\n       for s, expected in cases:\n           actual = formatting._calculate_col_width(s)\n           assert actual == expected\n", "def test__get_var_names_for_display():\n    ds = xr.Dataset()\n    ds[\"a\"] = xr.DataArray(name=\"a\", data=[1, 2])\n    ds[\"b\"] = xr.DataArray(name=\"b\", data=[3, 4])\n    ds[\"c\"] = xr.DataArray(name=\"c\", data=[5, 6])\n    ds[\"d\"] = xr.DataArray(name=\"d\", data=[7, 8])\n\n    with xr.set_options(display_max_rows=2):\n        assert formatting._get_var_names_for_display(ds.data_vars) == [\"a\", \"b\", \"...\"]\n", "    def test_diff_attrs_repr_with_identical():\n        attrs_a = {\"attr\": np.array([0, 1])}\n        attrs_b = {\"attr\": np.array([0, 1])}\n\n        expected = \"\"\n        actual = formatting.diff_attrs_repr(attrs_a, attrs_b, \"identical\")\n        assert expected == actual\n", "    def test_summarize_attr_with_long_string_and_newline():\n        long_string = \"This is a very long string that should be truncated\\nwith a newline character.\"\n        actual = formatting.summarize_attr(\"key\", long_string)\n        assert len(actual) <= 80\n        assert \"\\n\" not in actual\n", "    def test_diff_dataset_repr_identical_with_attrs_only(self):\n        ds_a = xr.Dataset(\n            data_vars={\"var1\": (\"x\", np.array([1, 2], dtype=\"int64\"))},\n            coords={\"x\": np.array([\"a\", \"b\"], dtype=\"U1\")},\n            attrs={\"units\": \"m\", \"description\": \"desc\"},\n        )\n\n        ds_b = xr.Dataset(\n            data_vars={\"var1\": (\"x\", np.array([1, 2], dtype=\"int64\"))},\n            coords={\"x\": np.array([\"a\", \"b\"], dtype=\"U1\")},\n            attrs={\"units\": \"m\"},\n        )\n\n        expected = dedent(\n            \"\"\"\\\n        Left and right Dataset objects are identical\n        Differing attributes:\n        L   description: desc\"\"\"\n        )\n\n        actual = formatting.diff_dataset_repr(ds_a, ds_b, \"identical\")\n        assert actual == expected\n"], "sample_7": ["compilation error", "    def test_masked_column_copy_mask_shape():\n        d = np.array([1, 2, 3])\n        mc = table.MaskedColumn(d, mask=[True, False, True])\n        mc_copy = mc.copy()\n        assert mc_copy.mask.shape == mc.mask.shape\n", "def test_masked_column_in_table():\n\n    c = table.Column([1, 2, 3], dtype='i4')\n    mc = table.MaskedColumn([1, 2, 3], mask=[False, True, False], dtype='i4')\n    t = table.Table({'a': c, 'b': mc})\n    assert t['a'].mask is None\n    assert np.all(t['b'].mask == [False, True, False])\n    # Accessing a slice of a masked column should return a masked column\n    assert isinstance(t['b'][0:2], table.MaskedColumn)\n\n", "def test_masked_column_convert_to_masked_array():\n    mc = table.MaskedColumn([1, 2, 3], mask=[True, False, True])\n    ma = np.ma.masked_array([1, 2, 3], mask=[True, False, True])\n    assert np.all(mc.astype(np.ma.MaskedArray) == ma)\n    assert isinstance(mc.astype(np.ma.MaskedArray), np.ma.MaskedArray)\n", "    def test_empty_column_searchsorted(dtype):\n        c = Column([], dtype=dtype)\n        res = c.searchsorted(np.array([2, 1], dtype=dtype), side='right')\n        assert np.all(res == [0, 0])\n\n"], "sample_765": ["compilation error", "compilation error", "def test_balanced_accuracy_score_empty():\n    assert balanced_accuracy_score([], []) == 0\n    assert balanced_accuracy_score([1], []) == 0\n    assert balanced_accuracy_score([], [1]) == 0\n", "def test_balanced_accuracy_score_multiclass():\n    y_true = [0, 1, 2, 0, 1, 2]\n    y_pred = [0, 2, 1, 0, 1, 2]\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred), 0.333333)\n", "def test_hinge_loss_multiclass_with_labels():\n    pred_decision = np.array([\n        [+0.36, -0.17, -0.58, -0.99],\n        [-0.55, -0.38, -0.48, -0.58],\n        [-1.45, -0.58, -0.38, -0.17],\n        [-0.55, -0.38, -0.48, -0.58],\n        [-1.45, -0.58, -0.38, -0.17]\n    ])\n    y_true = np.array([0, 1, 2, 1, 2])\n    labels = np.array([0, 1, 2, ])\n    dummy_losses = np.array([\n        1 - pred_decision[0][0] + pred_decision[0][1],\n        1 - pred_decision[1][1] + pred_decision[1][2],\n        1 - pred_decision[2][2] + pred_decision[2][3],\n        1 - pred_decision[3][1] + pred_decision[3][2],\n        1 - pred_decision[4][2] + pred_decision[4][3]\n    ])\n    np.clip(dummy_losses, 0, None, out=dummy_losses)\n    dummy_hinge_loss = np.mean(dummy_losses)\n\n    assert_equal(hinge_loss(y_true, pred_decision, labels=labels),\n                 dummy_hinge_loss)\n"], "sample_930": ["def test_create_index_with_subentry_key(app):\n    text = \"\"\"\n    .. index:: pair: Python; interpreter\n       :key: python\n\n    .. index:: pair: Sphinx; documentation tool\n       :key: sphinx\n\n    .. index:: pair: Sphinx; :+1:\n       :key: sphinx\n\n    .. index:: pair: Sphinx; \u0415\u043b\u044c\n       :key: russian\n\n    .. index:: pair: Sphinx; \u0451\u043b\u043a\u0430\n       :key: russian\n    \"\"\"\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 6\n    assert index[0] == ('Symbols', [(':+1:', [[], [('Sphinx', [('', '#index-2')])], None])])\n    assert index[1] == ('I', [('interpreter', [[], [('Python', [('', '#index-0')])], None])])\n    assert index[2] == ('P', [('Python', [[], [('interpreter', [('', '#index-0')])], None])])\n    assert index[3] == ('R', [('russian', [[], [('\u0415\u043b\u044c', [('', '#index-4')]),\n                                       ('\u0451\u043b\u043a\u0430', [('', '#index-5')])], None])])\n    assert index[4] == ('S', [('Sphinx', [[],\n                [(':+1:', [('', '#index-2')]),\n                ('documentation tool', [('', '#index-1')]),\n                ('\u0451\u043b\u043a\u0430', [('', '#index-5')]),\n                ('\u0415\u043b\u044c', [('', '#index-4')])],\n                None])])\n    assert index[5] == ('D',", "def test_create_index_case_insensitive(app):\n    text = (\".. index:: case sensitive\\n\"\n            \".. index:: Case Sensitive\\n\"\n            \".. index:: CASE SENSITIVE\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 1\n    assert index[0] == ('C', [('Case Sensitive', [[('', '#index-2'),\n                                                   ('', '#index-1'),\n                                                   ('', '#index-0')], [], None])])\n", "def test_create_index_with_category_key(app):\n    text = (\".. index:: single: docutils; category: Documentation\\n\"\n            \".. index:: single: Python; category: Programming Language\\n\"\n            \".. index:: Sphinx\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n\n    assert len(index) == 3\n    assert index[0] == ('Documentation', [('docutils', [[('', '#index-0')], [], 'Documentation'])])\n    assert index[1] == ('Programming Language', [('Python', [[('', '#index-1')], [], 'Programming Language'])])\n    assert index[2] == ('S', [('Sphinx', [[('', '#index-2')], [], None])])\n", "def test_create_index_group_entries(app):\n    text = (\".. index:: pair: foo; bar\\n\"\n            \".. index:: pair: foo; baz\\n\"\n            \".. index:: pair: foo; qux\\n\"\n            \".. index:: foo (in module1)\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder, group_entries=True)\n    assert len(index) == 2\n    assert index[0] == ('F', [('foo', [[('main', '#index-3')], {}, None],\n                             ['bar', [('', '#index-0')]],\n                             ['baz', [('', '#index-1')]],\n                             ['qux', [('', '#index-2')]]])])\n    assert index[1] == ('Q', [('qux', [[\n\n", "def test_create_index_with_category(app):\n    text = (\".. index:: category: docutils; Documentation Tools\\n\"\n            \".. index:: category: Python; Programming Languages\\n\"\n            \".. index:: Sphinx; Documentation Tools\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 3\n    assert index[0] == ('Documentation Tools', [('docutils', [[('', '#index-0')], [], 'docutils')]),\n    assert index[1] == ('Programming Languages', [('Python', [[('', '#index-1')], [], 'Python')])\n    assert index[2] == ('S', [('Sphinx', [[('', '#index-2')], [], None])])\n"], "sample_562": ["def test_axline_transform():\n    fig, ax = plt.subplots()\n    ax.set_xlim(-1, 1)\n    ax.set_ylim(-1, 1)\n    ax.axline((0, 0), slope=1, transform=ax.transData, color='r')\n    ax.axline((0, 0),\n              slope=1,\n              transform=ax.transAxes,\n              color='g',\n              lw=2)\n", "def test_axline_color(fig_test, fig_ref):\n    ax_test = fig_test.add_subplot()\n    ax_ref = fig_ref.add_subplot()\n\n    ax_test.axline((0.1, 0.1), (0.8, 0.8), color='red')\n    ax_ref.plot([0.1, 0.8], [0.1, 0.8], color='red', linestyle='-', marker='')\n\n    ax_test.axline((0.2, 0.2), slope=0.5, color='green')\n    ax_ref.plot([0.2, 0.9], [0.2, 0.9], color='green', linestyle='-', marker='')\n\n", "    def test_axline_limits():\n        fig, ax = plt.subplots()\n        ax.set_xlim(0, 1)\n        ax.set_ylim(0, 1)\n        line = ax.axline((0.2, 0.2), slope=1)\n        assert line.get_xy1() == (0.2, 0.2)\n        assert line.get_xy2() is None\n", "def test_axline_transform():\n    fig, ax = plt.subplots()\n    line = ax.axline((0.1, 0.1), slope=0.6, transform=ax.transData)\n    # Testing transform setter and getter.\n    # This should not produce an error.\n    line.set_transform(ax.transAxes)\n    assert line.get_transform() == ax.transAxes\n", "def test_axline_zorder(fig_test, fig_ref):\n    ax_test = fig_test.add_subplot()\n    ax_ref = fig_ref.add_subplot()\n\n    ax_test.plot([0, 1], [0, 1], color='blue', zorder=1)\n    ax_test.axline((0, 0), (1, 1), color='red', zorder=2)\n\n    ax_ref.plot([0, 1], [0, 1], color='blue', zorder=1)\n    ax_ref.axline((0, 0), (1, 1), color='red', zorder=2)\n"], "sample_283": ["    def test_default_database(self):\n        with mock.patch('django.db.backends.postgresql.client.DatabaseClient.settings_to_cmd_args_env') as mock_settings_to_cmd_args_env:\n            mock_settings_to_cmd_args_env.return_value = (['psql', 'postgres'], {})\n            connection.client.runshell([])\n            mock_settings_to_cmd_args_env.assert_called_once_with({'NAME': 'postgres'}, [])\n", "    def test_default_db(self):\n        \"\"\"Whendbname and service are not provided, connect to the default 'postgres' db.\"\"\"\n        with mock.patch('subprocess.run') as mock_subprocess_run:\n            connection.client.runshell([])\n            mock_subprocess_run.assert_called_once_with(\n                ['psql', 'postgres'],\n                check=True,\n                env={},\n            )\n", "    def test_runshell_with_parameters(self):\n        with mock.patch('subprocess.run') as mock_subprocess_run:\n            connection.client.runshell(['-c', 'SELECT 1'])\n            mock_subprocess_run.assert_called_once_with(\n                ['psql', '-c', 'SELECT 1'],\n                check=True,\n                env={'PGPASSWORD': connection.settings_dict['PASSWORD']},\n            )\n", "    def test_runshell_with_parameters(self):\n        \"\"\"Test that runshell passes parameters correctly.\"\"\"\n        with mock.patch('subprocess.run') as mock_run:\n            connection.client.runshell(['-c', 'SELECT 1'])\n            mock_run.assert_called_once_with(\n                ['psql', '-c', 'SELECT 1'],\n                check=True,\n                env={},\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n            )\n", "    def test_default_db(self):\n        \"\"\"Verify connection to the default 'postgres' database when none\n        is specified.\n        \"\"\"\n        with mock.patch('django.db.backends.postgresql.client.DatabaseClient.runshell') as mock_runshell:\n            connection.client.runshell([])\n            mock_runshell.assert_called_with(['psql', 'postgres'])\n"], "sample_1102": ["compilation error", "compilation error", "compilation error", "def test_issue_19641():\n    assert cancel(sin(x) / sin(2*x)) == 1/(2*cos(x)) \n", "def test_issue_19472():\n    f = (1 + x)**Rational(1, 3)\n    assert f.as_poly(x) == Poly(x**(1/3) + 1, x)\n"], "sample_76": ["    def test_empty_languages(self):\n        with self.settings(LANGUAGES=[]):\n            self.assertEqual(check_setting_languages(None), [])\n\n", "    def test_consistent_language_settings_with_bidi(self):\n        msg = (\n            'You have provided a value for the LANGUAGE_CODE setting that is '\n            'not in the LANGUAGES setting.'\n        )\n        with self.settings(LANGUAGE_CODE='fr', LANGUAGES=[('en', 'English')], LANGUAGES_BIDI=['ar']):\n            self.assertEqual(check_language_settings_consistent(None), [\n                Error(msg, id='translation.E004'),\n            ])\n", "    def test_consistent_language_settings_with_bidi(self):\n        msg = (\n            'You have provided a value for the LANGUAGE_CODE setting that is '\n            'not in the LANGUAGES setting.'\n        )\n        with self.settings(LANGUAGE_CODE='fr', LANGUAGES=[('en', 'English')], LANGUAGES_BIDI=['ar']):\n            self.assertEqual(check_language_settings_consistent(None), [\n                Error(msg, id='translation.E004'),\n            ])\n", "    def test_consistent_language_settings_with_language_bidi(self):\n        msg = (\n            'You have provided a value for the LANGUAGE_CODE setting that is '\n            'not in the LANGUAGES setting.'\n        )\n        with self.settings(LANGUAGE_CODE='fr', LANGUAGES=[('en', 'English')], LANGUAGES_BIDI=['fr']):\n            self.assertEqual(check_language_settings_consistent(None), [])\n", "    def test_consistent_language_settings_with_bidi(self):\n        msg = (\n            'You have provided a value for the LANGUAGE_CODE setting that is '\n            'not in the LANGUAGES setting.'\n        )\n        with self.settings(LANGUAGE_CODE='fr', LANGUAGES=[('en', 'English')], LANGUAGES_BIDI=['ar']):\n            self.assertEqual(check_language_settings_consistent(None), [\n                Error(msg, id='translation.E004'),\n            ])\n"], "sample_1118": ["def test_matpow():\n    assert MatPow(C, 2).shape == (n, n)\n    assert MatPow(A, 2).shape == (n, m)\n    assert MatPow(C, 0).doit() == Identity(n)\n    assert MatPow(C, -1).doit() == Inverse(C)\n    assert MatPow(C, 2).doit() == C*C\n\n    raises(NonSquareMatrixError, lambda: MatPow(A, 2))\n\n    assert MatPow(Identity(n), 2).doit() == Identity(n)\n\n    assert MatPow(ZeroMatrix(n, n), 2).doit() == ZeroMatrix(n, n)\n\n", "def test_matpow():\n    assert MatPow(C, 2).shape == (n, n)\n    assert MatPow(C, 2).args == (C, 2)\n    assert MatPow(A, -2).shape == (m, n)\n    assert MatPow(A, -2).args == (A, -2)\n\n    assert MatPow(C, 0) == Identity(n)\n    assert MatPow(C, 1) == C\n    assert MatPow(C, 2) == C*C\n    assert MatPow(C, -1) == Inverse(C)\n    assert MatPow(C, -2) == Inverse(C*C)\n\n    assert (MatPow(C, 2)*MatPow(C, 3)).doit() == MatPow(C, 5).doit()\n    assert (MatPow(C, 2) + MatPow(C, 3)).doit() == MatPow(C, 2) + MatPow(C, 3).doit()\n\n    assert MatPow(ZeroMatrix(n, n), 2) == ZeroMatrix(n, n)\n\n    raises(NonSquareMatrixError, lambda: MatPow(A, 2))\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(n, n), -1).doit())\n", "def test_matpow():\n    assert MatPow(C, 2).shape == (n, n)\n    assert MatPow(A, 3).shape == (n, m)\n    assert MatPow(C, -2).shape == (n, n)\n    assert MatPow(C, S.Half).shape == (n, n)\n    assert MatPow(C, 0).doit() == Identity(n) \n    assert MatPow(ZeroMatrix(n, n), 2).doit() == ZeroMatrix(n, n)\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(n, n), -2).doit()) \n    raises(NonSquareMatrixError, lambda: MatPow(A, 2).doit())\n    assert MatPow(C, 2).exp == 2 \n    assert (MatPow(C, 2)*MatPow(C, 3)).doit() == MatPow(C, 5).doit()\n", "    def test_matpow_identity():\n        assert MatPow(Identity(n), 3).doit() == Identity(n)\n        assert MatPow(Identity(n), -2).doit() == Inverse(Identity(n)).doit()**2\n        assert MatPow(Identity(n), S.Zero).doit() == Identity(n)\n", "def test_MatPow_evaluation():\n    A = MatrixSymbol('A', 3, 3)\n    assert MatPow(A, 2).doit() == A*A\n    assert MatPow(A, 0).doit() == Identity(3)\n    assert MatPow(A, -2).doit() == Inverse(A)*Inverse(A)\n\n    raises(NonSquareMatrixError, lambda: MatPow(A, 2).doit() if A.shape != (3,3))\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(3,3), -2).doit())\n"], "sample_602": ["    def test_to_netcdf_invalid_netcdf(self) -> None:\n        ds = xr.Dataset({\"a\": (\"x\", [1, 2, 3])}, coords={\"x\": [1, 2, 3]})\n        with pytest.raises(ValueError, match=\"unrecognized option 'invalid_netcdf'\"):\n            ds.to_netcdf(\"test.nc\", invalid_netcdf=True, engine=\"scipy\")\n\n        with pytest.raises(ValueError, match=\"unrecognized option 'invalid_netcdf'\"):\n            ds.to_netcdf(\"test.nc\", invalid_netcdf=True, engine=\"h5netcdf\")\n\n        ds.to_netcdf(\"test.nc\", invalid_netcdf=True, engine=\"h5netcdf\")\n\n", "def test_to_netcdf_invalid_netcdf():\n    ds = xr.Dataset(\n        data_vars={\"a\": (\"x\", [1, 2, 3])}, coords={\"x\": [0, 1, 2]}\n    )\n    with pytest.raises(ValueError, match=\"unrecognized option 'invalid_netcdf' for engine h5netcdf\"):\n        ds.to_netcdf(\"test.nc\", engine=\"h5netcdf\", invalid_netcdf=True)\n", "def test_to_netcdf_invalid_netcdf():\n    ds = xr.Dataset({'foo': ('x', [1, 2, 3])}, coords={'x': [0, 1, 2]})\n    with pytest.raises(ValueError):\n        ds.to_netcdf(\"test.nc\", invalid_netcdf=True, engine=\"netcdf4\")\n", "def test_to_netcdf_invalid_netcdf():\n    ds = xr.Dataset({\"a\": ((\"x\", \"y\"), np.ones((2, 3)))})\n    with pytest.raises(ValueError, match=\"unrecognized option 'invalid_netcdf' for engine h5netcdf\"):\n        ds.to_netcdf(\"test.nc\", engine=\"h5netcdf\", invalid_netcdf=True)\n", "def test_to_netcdf_invalid_netcdf():\n    ds = xr.Dataset({'a': ('x', [1, 2, 3])}, coords={'x': [0, 1, 2]})\n    with pytest.raises(ValueError, match=\"unrecognized option 'invalid_netcdf' for engine 'scipy'\"):\n        ds.to_netcdf(\"test.nc\", engine=\"scipy\", invalid_netcdf=True)\n    with pytest.raises(ValueError, match=\"recognized option 'invalid_netcdf' for engine 'h5netcdf'\"):\n        ds.to_netcdf(\"test.nc\", engine=\"h5netcdf\", invalid_netcdf=True)\n\n"], "sample_1160": ["def test_issue_18134b():\n    assert S.Rationals.interior == S.EmptySet\n    assert S.Rationals.exterior == S.Reals - S.Rationals\n", "def test_issue_18134():\n    c = ComplexRegion(Interval(0, 1)*Interval(0, pi/2), polar=True)\n    assert c.boundary == Union(ComplexRegion(Interval(0, 1)*FiniteSet(0)),\n        ComplexRegion(Interval(0, 1)*FiniteSet(pi/2)))\n    assert c.closure == c\n    assert c.is_open == False\n    assert c.is_closed == True\n\n", "compilation error", "compilation error", "compilation error"], "sample_427": ["    def test_formset_add_extra_field(self):\n        class CustomForm(Form):\n            field1 = CharField()\n\n        CustomFormSet = formset_factory(CustomForm, extra=1)\n        formset1 = CustomFormSet()\n        formset1.add_extra_field()\n        self.assertEqual(formset1.extra, 2)\n", "    def test_empty_formset_errors(self):\n        data = {\"form-TOTAL_FORMS\": \"0\", \"form-INITIAL_FORMS\": \"0\"}\n        formset = ArticleFormSet(data)\n        self.assertEqual(formset.errors, {})\n", "    def test_empty_formset_with_initial_data(self):\n        data = {\"form-INITIAL_FORMS\": \"0\", \"form-TOTAL_FORMS\": \"0\"}\n        formset = formset_factory(Choice, extra=0)(data=data, prefix=\"form\")\n        self.assertTrue(formset.is_valid())\n\n", "    def test_formset_html_safe_rendering(self):\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet()\n        self.assertIsInstance(formset.as_p(), str)\n", "    def test_extra_formset_forms(self):\n        ChoiceFormFormset = formset_factory(form=Choice, can_delete=True, extra=2)\n        formset = ChoiceFormFormset(initial=[{\"choice\": \"Zero\", \"votes\": \"1\"}])\n        self.assertEqual(len(formset.forms), 3)\n        self.assertTrue(all(not form.is_deleted for form in formset.forms))\n        self.assertNotIn(\"DELETE\", formset.forms[0].fields)\n        self.assertIn(\"DELETE\", formset.forms[1].fields)\n        self.assertIn(\"DELETE\", formset.forms[2].fields)\n"], "sample_1078": ["compilation error", "compilation error", "def test_IndexedBase_free_symbols():\n    i = Symbol('i', integer=True)\n    a = Symbol('a')\n    A = IndexedBase(a)\n    assert A[i].free_symbols == {A, a, i}\n", "compilation error", "compilation error"], "sample_1139": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_imageset_empty():\n    assert ImageSet(Lambda(x, 2*x), S.EmptySet) == S.EmptySet\n"], "sample_929": ["def test_pyclass_with_inheritance(app):\n    text = (\".. py:class:: Parent\\n\"\n            \"\\n\"\n            \".. py:class:: Child(Parent)\\n\"\n            \"   :property:\\n\"\n            \"       attr: Some parent property.\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Parent\"])],\n                                  [desc_content, ()])],\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Child\"],\n                                                    [desc_inheritance,  \"(\",\n                                                                          [desc_name, \"Parent\",],\n                                                                          \")\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'Parent (built-in class)', 'Parent', '', None)])\n    assert 'Parent' in domain.objects\n    assert domain.objects['Parent'] == ('index', 'Parent', 'class')\n    assert_node(doctree[3][1][0], addnodes.index,\n                entries=[('single', 'Child (built-in class)', 'Child', '', None)])\n    assert_node(doctree[3][1][1], ([desc_signature, ([desc_annotation, \"property \"],\n                                                     [desc_name, \"attr\"],\n                                                     [desc_punctuation, \":\"],\n                                                     [desc_content, \" Some parent property.\"])],\n                                   [desc_content, ()]))\n    assert 'Child' in domain.objects\n    assert domain.objects['Child'] == ('index', 'Child', 'class') \n", "compilation error", "def test_pyattribute_noindexentry(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :noindexentry:\\n\"\n            \"      :type: Optional[str]\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'attr (Class attribute)', 'Class.attr', '', None)])\n\n    assert 'Class.attr' in domain.objects\n    assert domain.objects['Class.attr'] == ('index', 'Class.attr', 'attribute')\n", "def test_pydata_signature_old_noindex(app):\n    text = (\".. py:data:: version\\n\"\n            \"   :annotation: = 1\\n\"\n            \"   :noindex:\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          desc,\n                          addnodes.index))\n    assert_node(doctree[1], addnodes.desc, desctype=\"data\",\n                domain=\"py\", objtype=\"data\", noindex=True)\n", "def test_pyexception_with_module(app):\n    text = \".. py:exception:: exceptions.IOError\\n\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"exception \"],\n                                                    [desc_addname, \"exceptions.\"],\n                                                    [desc_name, \"IOError\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], desc, desctype=\"exception\",\n                domain=\"py\", objtype=\"exception\", noindex=False)\n    assert 'exceptions.IOError' in domain.objects\n    assert domain.objects['exceptions.IOError'] == ('index', 'exceptions.IOError', 'exception')\n"], "sample_315": ["    def test_url_with_path_includes_language(self):\n        with translation.override('en'):\n            response = self.client.get('/en/with-arguments/regular-argument/optional.html')\n            self.assertEqual(response.status_code, 200)\n            self.assertEqual(response.headers['content-language'], 'en')\n\n        with translation.override('nl'):\n            response = self.client.get('/nl/with-arguments/regular-argument/optional.html')\n            self.assertEqual(response.status_code, 200)\n            self.assertEqual(response.headers['content-language'], 'nl')\n", "    def test_invalid_language_in_url(self):\n        with translation.override('en'):\n            response = self.client.get('/invalid-language/prefixed/')\n        self.assertEqual(response.status_code, 404)\n", "    def test_default_language_redirect(self):\n        response = self.client.get('/account/register/', HTTP_ACCEPT_LANGUAGE='en')\n        self.assertRedirects(response, '/nl/profiel/registreren/')\n", "    def test_language_code_from_url_precedence(self):\n        with translation.override('nl'):\n            response = self.client.get('/nl/prefixed/')\n            self.assertEqual(response.status_code, 200)\n            self.assertEqual(response.headers['content-language'], 'nl')\n            self.assertEqual(response.context['LANGUAGE_CODE'], 'nl')\n\n        with translation.override('en'):\n            response = self.client.get('/en/prefixed/')\n            self.assertEqual(response.status_code, 200)\n            self.assertEqual(response.headers['content-language'], 'en')\n            self.assertEqual(response.context['LANGUAGE_CODE'], 'en')\n\n", "    def test_language_prefix_with_query_string(self):\n        prefix = '/script_prefix'\n        with override_script_prefix(prefix):\n            response = self.client.get('/prefixed/?param=value', HTTP_ACCEPT_LANGUAGE='en', SCRIPT_NAME=prefix)\n            self.assertRedirects(response, '%s/en/prefixed/?param=value' % prefix, target_status_code=404)\n"], "sample_96": ["    def test_actions_are_functions(self):\n        class BandAdmin(ModelAdmin):\n            actions = ('not_a_function',)\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            \"All actions in 'actions' must be callable.\",\n            id='admin.E131',\n        )\n", "    def test_actions_should_be_iterable(self):\n        class BandAdmin(ModelAdmin):\n            actions = 123\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            \"The value of 'actions' must be a list or tuple.\",\n            id='admin.E125',\n        )\n", "    def test_actions_invalid_type(self):\n        class InvalidAction:\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (InvalidAction,)\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            \"The value of 'actions[0]' must be callable.\",\n            id='admin.E131',\n        )\n\n", "    def test_actions_valid_custom_permissions(self):\n            pass\n\n        custom_permission_action.allowed_permissions = ('custom',)\n\n        class BandAdmin(ModelAdmin):\n            actions = (custom_permission_action,)\n\n                return True\n\n        self.assertIsValid(BandAdmin, Band)\n", "    def test_actions_with_incorrect_signature(self):\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action,)\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            \"Action 'action' in <class 'modeladmin.test_checks.ActionsCheckTests.\"\n            \"test_actions_with_incorrect_signature.<locals>.BandAdmin'> \"\n            \"must accept 3 arguments: (modeladmin, request, queryset).\",\n            id='admin.E131',\n        )\n"], "sample_1033": ["compilation error", "def test_issue_16131():\n    p = symbols('p', positive=True)\n    assert (p**2).is_positive\n\n", "compilation error", "compilation error", "compilation error"], "sample_261": ["    def test_parse_duration_mixed_format(self):\n        self.assertEqual(parse_duration('1 days 15:30:1.1'), timedelta(days=1, minutes=15, seconds=30, milliseconds=100))\n        self.assertEqual(parse_duration('5 days 10:0:1.12345'), timedelta(days=5, hours=10, seconds=1, microseconds=123450))\n        self.assertEqual(parse_duration('1 days 15:30'), timedelta(days=1, minutes=15, seconds=30))\n", "    def test_parse_iso8601_with_microseconds(self):\n        self.assertEqual(parse_duration('P1DT10H30M20.123456S'), timedelta(days=1, hours=10, minutes=30, seconds=20, microseconds=123456))\n", "    def test_parse_duration_invalid(self):\n        test_values = (\n            'invalid',\n            '1 day 04',\n            '1 days 0:00:01',\n            '1 day 0:00:01x',\n            '1 day 0:00:01.x',\n            '1 day -0:00:01.',\n            '1 days 0:00:01.123456789',\n            '1 day 00:00:01',\n            '1 days 00:00:01',\n            '1 day 0:00',\n            '1 day :00:01',\n            '1 day 0:00:01:',\n            '-1 day 0:00:01',\n            '1 day -0:00:01',\n            '4 days 0:15:30.123456789',\n            '4 days 0:15:30,123456789',\n            '4 days 0:15:30.a',\n            '4 days 0:15:30,a',\n            'P4DT1',\n            'PT1H30M',\n        )\n        for source in test_values:\n            with self.subTest(source=source):\n                with self.assertRaises(ValueError):\n                    parse_duration(source)\n\n", "    def test_parse_duration_invalid_formats(self):\n        invalid_inputs = (\n            '2 days 3 hours',\n            '1 day 00:00',\n            '1 day -0:00',\n            '-1 day -00:00',\n            '1 day 0:00 00:00',\n            '1 day 0:00.00.00',\n            '1 days 0:00:00.00.00',\n            '1 day 0:00:00,00.00',\n            '1d 2h',\n            '1d 2h30m',\n            '1d 2h30m10s',\n        )\n        for source in invalid_inputs:\n            with self.subTest(source=source):\n                with self.assertRaises(ValueError):\n                    parse_duration(source)\n", "    def test_parse_duration_invalid(self):\n        test_values = (\n            '15:30:100', \n            '15:30:', \n            '15::30', \n            ':',\n            '15',\n            '-15',\n            '+15',\n            'word',\n            '1 day 1', \n            '1 day word',\n            '1 days 15:30',\n            '1 day 15:30 word', \n            '1 day 15:30.word',\n            '1 day 15::30',\n            '1 days 15:30:word',\n            '1 days 15:30,word',\n\n        )\n        for source in test_values:\n            with self.subTest(source=source):\n                with self.assertRaises(ValueError):\n                    parse_duration(source)\n\n"], "sample_533": ["compilation error", "def test_contourf_corner_mask_with_masked_array():\n    x, y = np.mgrid[0:10, 0:10]\n    z = np.ones((10, 10))\n    mask = np.zeros_like(z, dtype=bool)\n    mask[0, 0] = True\n    masked_z = np.ma.array(z, mask=mask)\n\n    fig, ax = plt.subplots()\n    CS = ax.contourf(x, y, masked_z, corner_mask=True)\n    assert len(CS.collections) == 1\n", "def test_contour_linestyle_inheritance(linestyle, expected):\n    x = np.arange(-5, 5, 0.25)\n    y = np.arange(-5, 5, 0.25)\n    X, Y = np.meshgrid(x, y)\n    Z = np.sin(np.sqrt(X**2 + Y**2))\n\n    fig, ax = plt.subplots()\n    \n    plt.rcParams['contour.linestyle'] = linestyle\n\n    CS = ax.contour(X, Y, Z, levels=[-0.5, 0, 0.5])\n    assert CS.linestyles == expected\n", "compilation error", "compilation error"], "sample_894": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_270": ["    def test_check_constraints_empty(self):\n        class Model(models.Model):\n            pass\n\n        self.assertEqual(Model.check(databases=self.databases), [])\n", "    def test_unique_constraint_include_pointing_to_joined_fields(self):\n        class Model(models.Model):\n            name = models.CharField(max_length=10)\n            field1 = models.PositiveSmallIntegerField()\n            field2 = models.PositiveSmallIntegerField()\n\n            parent = models.ForeignKey('self', models.CASCADE)\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['id'],\n                        include=['parent__field1', 'parent__field2'],\n                        name='name',\n                    ),\n                ]\n\n        joined_fields = ['parent__field1', 'parent__field2']\n        errors = Model.check(databases=self.databases)\n        expected_errors = [\n            Error(\n                \"'constraints' refers to the joined field '%s'.\" % field_name,\n                obj=Model,\n                id='models.E041',\n            ) for field_name in joined_fields\n        ]\n        self.assertCountEqual(errors, expected_errors)\n", "    def test_check_constraint_pointing_to_reverse_fk_with_include(self):\n        class Model(models.Model):\n            parent = models.ForeignKey('self', models.CASCADE, related_name='parents')\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(name='name', check=models.Q(parents__count=2), include=['parents']),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [\n            Error(\n                \"'constraints' refers to the nonexistent field 'parents'.\",\n                obj=Model,\n                id='models.E012',\n            ),\n        ])\n", "    def test_check_constraint_pointing_to_reverse_fk_field_name(self):\n        class Model(models.Model):\n            parent = models.ForeignKey('self', models.CASCADE, related_name='parents')\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(name='name', fields=['parent__parent_id']),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [\n            Error(\n                \"'constraints' refers to the nonexistent field 'parent__parent_id'.\",\n                obj=Model,\n                id='models.E012',\n            ),\n        ])\n\n", "    def test_unique_constraint_include_pointing_to_joined_field(self):\n        class Model(models.Model):\n            name = models.CharField(max_length=10)\n            field1 = models.PositiveSmallIntegerField()\n            field2 = models.PositiveSmallIntegerField()\n            parent = models.ForeignKey('self', models.CASCADE)\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['id'],\n                        include=['parent__field1'],\n                        name='name',\n                    ),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [\n            Error(\n                \"'constraints' refers to the joined field 'parent__field1'.\",\n                obj=Model,\n                id='models.E041',\n            ),\n        ])\n"], "sample_654": ["def test_fixture_param_shadowing_with_autouse(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(params=['a', 'b'], autouse=True)\n            return request.param\n\n        @pytest.fixture\n            return argroot\n\n        # This should only be parametrized directly\n        @pytest.mark.parametrize(\"arg\", [1])\n            assert arg == 1\n\n        # This should be parametrized based on the fixtures\n            assert isinstance(arg, str)\n        \"\"\"\n    )\n    # Only one test should have run\n    result = testdir.runpytest(\"-v\")\n    result.assert_outcomes(passed=2)\n    result.stdout.fnmatch_lines([\"*::test_direct[[]1[]]*\"])\n    result.stdout.fnmatch_lines([\"*::test_normal_fixture[[]a[]]*\"])\n    result.stdout.fnmatch_lines([\"*::test_normal_fixture[[]b[]]*\"])\n", "    def test_fixture_with_same_name_as_param(testdir):\n        \"\"\"Regression test for #6131: Fixture name should not clash with parameter name\"\"\"\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.fixture\n                return request.param\n\n            @pytest.mark.parametrize(\"my_arg\", [1, 2])\n                assert my_arg in [1, 2]\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.assert_outcomes(passed=2)\n", "    def test_fixture_param_shadowing_with_autouse(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.fixture(params=['a', 'b'], autouse=True)\n                return request.param\n\n            @pytest.fixture\n                return argroot\n\n            @pytest.mark.parametrize(\"arg\", [1])\n                assert arg == 1\n\n                assert isinstance(arg, str)\n        \"\"\"\n        )\n        result = testdir.runpytest(\"-v\")\n        result.assert_outcomes(passed=2)\n        result.stdout.fnmatch_lines([\"*::test_direct[[]1[]]*\"])\n        result.stdout.fnmatch_lines([\"*::test_normal_fixture[[]a[]]*\"])\n", "    def test_fixture_param_shadowing_indirect(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.fixture(params=['a', 'b'])\n                return request.param\n\n            @pytest.fixture\n                return argroot\n\n            @pytest.fixture(params=[1,2], indirect=True)\n                return request.param * 2\n\n            @pytest.mark.parametrize(\"arg2\", [1,2], indirect=True)\n                assert isinstance(arg, str)\n                assert isinstance(arg2, int)\n\n        \"\"\"\n        )\n        result = testdir.runpytest(\"-v\")\n        result.assert_outcomes(passed=4)\n        result.stdout.fnmatch_lines([\"*::test_fixture_param_shadowing_indirect[[]a[]1[]]*\"])\n        result.stdout.fnmatch_lines([\"*::test_fixture_param_shadowing_indirect[[]b[]1[]]*\"])\n        result.stdout.fnmatch_lines([\"*::test_fixture_param_shadowing_indirect[[]a[]2[]]*\"])\n        result.stdout.fnmatch_lines([\"*::test_fixture_param_shadowing_indirect[[]b[]2[]]*\"])\n", "    def test_fixture_param_shadowing_different_scopes(testdir):\n        \"\"\"Parametrized arguments should not be shadowed by fixtures of different scopes (#5036)\"\"\"\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.fixture(scope='module', params=['a', 'b'])\n                return request.param\n\n            @pytest.fixture\n                return argroot\n\n            @pytest.mark.parametrize(\"arg\", [1])\n                assert arg == 1\n\n                assert isinstance(arg, str)\n        \"\"\"\n        )\n        result = testdir.runpytest(\"-v\")\n        result.assert_outcomes(passed=4)\n        result.stdout.fnmatch_lines([\"*::test_direct[[]1[]]*\"])\n        result.stdout.fnmatch_lines([\"*::test_normal_fixture[[]a[]]*\"])\n        result.stdout.fnmatch_lines([\"*::test_normal_fixture[[]b[]]*\"])\n"], "sample_701": ["def test_argument_percent_default_is_deprecated(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import argparse\n\n            parser = argparse.ArgumentParser()\n            parser.add_argument('--foo', '%default', type=int)\n            args = parser.parse_args('--foo 10'.split())\n            assert args.foo == 10\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            '*PytestDeprecationWarning: '\n            '\\'pytest now uses argparse. \"%default\" should be changed to \"%(default)s\"',\n        ]\n    )\n", "def test_argument_percent_default(pytester: Pytester) -> None:\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        addopts = --some-option=%default\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-h\")\n    result.stdout.fnmatch_lines(\n        [\n            f\"*PytestDeprecationWarning: pytest now uses argparse. '%default' should be changed to '%(default)s'\",\n        ]\n    )\n\n", "def test_argument_percent_default_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            parser.addoption('--foo', default='bar', type=str, help='%default')\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: pytest now uses argparse. '%default' should be changed to '%(default)s'\",\n        ]\n    )\n\n", "    def test_argument_percent_default(self, pytester: Pytester) -> None:\n        p = pytester.makepyfile(\n            \"\"\"\n                pass\n            \"\"\",\n        )\n        result = pytester.runpytest(\"-m\", \"test_something\", \"--default-percent-default\")\n        result.stdout.fnmatch_lines(\n            [\n                \"*PytestDeprecationWarning: pytest now uses argparse. '%default' should be changed to '%(default)s'\",\n            ]\n        )\n\n", "def test_argument_percent_default_is_deprecated(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.parametrize(\"arg\", [1, 2])\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-k test_foo --arg\", \"arg=%default\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: pytest now uses argparse. '%default' should be changed to '%(default)s'\",\n        ]\n    )\n\n"], "sample_913": ["def test_pyexception_signature_nested(app):\n    text = \".. py:exception:: module1.module2.NestedException\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"exception \"],\n                                                    [desc_addname, \"module1.\"],\n                                                    [desc_addname, \"module2.\"],\n                                                    [desc_name, \"NestedException\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], desc, desctype=\"exception\",\n                domain=\"py\", objtype=\"exception\", noindex=False)\n", "def test_pyobject_signature_with_parent(app):\n    text = \".. py:class:: SomeClass\\n\"\n    text += \"   .. py:method:: SomeClass.some_method\\n\"\n    text += \"      :async:\\n\"\n    restructuredtext.parse(app, text)\n    domain = app.env.get_domain('py')\n    assert 'SomeClass.some_method' in domain.objects\n    assert domain.objects['SomeClass.some_method'] == ('index', 'SomeClass.some_method', 'method')\n\n", "compilation error", "def test_pyexception_signature_with_module(app):\n    text = \".. py:exception:: mymodule.MyException\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"exception \"],\n                                                    [desc_addname, \"mymodule.\"],\n                                                    [desc_name, \"MyException\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], desc, desctype=\"exception\",\n                domain=\"py\", objtype=\"exception\", noindex=False)\n\n", "def test_pyinheritance(app):\n    text = (\".. py:class:: Parent\\n\"\n            \"\\n\"\n            \"   .. py:class:: Child(Parent)\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Parent\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc,\n                                                  addnodes.index)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'Parent (class)', 'Parent', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"Parent\"],\n                                                     [desc_parameterlist, ()])],\n                                   [desc_content, ()]))\n\n    assert 'Parent' in domain.objects\n    assert domain.objects['Parent'] == ('index', 'Parent', 'class')\n\n    assert_node(doctree[1][1][2], addnodes.index,\n                entries=[('single', 'Child (class)', 'Child', '', None)])\n    assert_node(doctree[1][1][3], ([desc_signature, ([desc_annotation, \"class \"],\n                                                     [desc_name, \"Child\"],\n                                                     [desc_parameterlist, (), \"Parent\"])],\n                                   [desc_content, ()]))\n\n    assert 'Child' in domain.objects\n    assert domain.objects['Child'] == ('index', 'Child', 'class')\n"], "sample_434": ["    def test_view_setup_sets_request_and_args_kwargs(self):\n        request_factory = RequestFactory()\n        request = request_factory.get('/')\n        view = SyncView()\n\n        view.setup(request, 'arg1', kwarg1='kwarg1value')\n\n        self.assertIs(view.request, request)\n        self.assertEqual(view.args, ('arg1',))\n        self.assertEqual(view.kwargs, {'kwarg1': 'kwarg1value'})\n", "    def test_view_get_context_data_passes_kwargs(self):\n        class TestView(TemplateResponseMixin, ContextMixin, View):\n                return self.render_to_response(self.get_context_data(**kwargs))\n\n        view = TestView()\n        request = RequestFactory().get('/')\n        response = view.get(request, foo='bar', baz='qux')\n        context = response.context_data\n        self.assertEqual(context['foo'], 'bar')\n        self.assertEqual(context['baz'], 'qux')\n        self.assertEqual(context['view'], view)\n", "    def test_get_context_data_with_extra_context(self):\n        class TestView(ContextMixin, View):\n            extra_context = {\"foo\": \"bar\"}\n\n                return self.render_to_response(self.get_context_data())\n\n        view = TestView.as_view()\n        request = RequestFactory().post(\"/\")\n        response = view(request)\n        self.assertEqual(response.context_data[\"foo\"], \"bar\")\n", "    def test_view_as_view_with_kwargs(self):\n        class MyView(View):\n                self.foo = foo\n\n                return HttpResponse(self.foo)\n\n        view = MyView.as_view(foo=\"bar\")\n        response = view(None)\n\n        self.assertEqual(response.content.decode(), \"bar\")\n", "    def test_redirect_view_returns_correct_redirect(self):\n        view = RedirectView()\n        view.url = \"/new-url/\"\n        request_factory = RequestFactory()\n        response = view.get(request_factory.get(\"/\"))\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response[\"Location\"], \"/new-url/\")\n\n"], "sample_318": ["    def test_include_app_name_no_urlconf(self):\n        msg = 'The included URLconf \\'None\\' does not appear to have any patterns in it.'\n        with self.assertRaisesMessage(ImproperlyConfigured, msg):\n            include(None, 'app_name')\n\n", "    def test_include_empty_list(self):\n        with self.assertRaisesMessage(ImproperlyConfigured, \"include() argument must be a list, tuple, or URLObject\"):\n            include([])\n", "    def test_include_app_name_multiple_namespaces(self):\n        with override_settings(ROOT_URLCONF='urlpatterns_reverse.urls_multiple_namespaces'):\n            self.assertEqual(\n                include(self.app_urls, namespace='inc-app-ns'),\n                (self.app_urls, 'inc-app', 'inc-app-ns')\n            )\n\n", "    def test_include_with_default_namespace_and_app_name(self):\n        self.assertEqual(\n            include(self.app_urls, namespace='default'),\n            (self.app_urls, 'inc-app', 'default')\n        )\n", "    def test_include_app_name_default_namespace(self):\n        self.assertEqual(\n            include(self.app_urls, namespace=''),\n            (self.app_urls, 'inc-app', 'inc-app')\n        )\n"], "sample_1065": ["compilation error", "compilation error", "def test_binomial_Mod_neg():\n    p, q = 10**5 + 3, 10**9 + 33 # prime modulo\n    r, s = 10**7 + 5, 33333333 # composite modulo\n    n, k = symbols('n k')\n\n    assert (binomial(n, k) % q).subs({n: -5, k: 3}) == Mod(binomial(-5, 3), q)\n    assert (binomial(n, k) % r).subs({n: -8, k: 5}) == Mod(binomial(-8, 5), r)\n    assert (binomial(n, k) % s).subs({n: -12, k: 8}) == Mod(binomial(-12, 8), s)\n", "compilation error", "compilation error"], "sample_439": ["    def test_error_list_as_ul(self):\n        class SimpleForm(Form):\n            name = CharField()\n            email = EmailField()\n\n        form = SimpleForm({\"name\": \"John Doe\", \"email\": \"invalid\"})\n        self.assertHTMLEqual(\n            form.errors.as_ul(),\n            '<ul class=\"errorlist\"><li>Enter a valid email address.</li></ul>',\n        )\n\n", "    def test_non_field_errors_empty_list(self):\n        class CustomForm(Form):\n                if 'something' not in self.cleaned_data:\n                    raise ValidationError(\"Something went wrong!\")\n                return self.cleaned_data\n        f = CustomForm()\n        self.assertEqual(f.non_field_errors(), [])  \n", "    def test_form_renderers_override_at_form_level(self):\n        class MyRenderer(DjangoTemplates):\n            template_name = 'forms_tests/form_snippet.html'\n\n        class CustomForm(Form):\n            name = CharField()\n            default_renderer = MyRenderer\n\n        f = CustomForm()\n        t = Template('{{ form }}')\n        html = t.render(Context({'form': f}))\n        self.assertIn('fieldWrapper', html)\n\n", "    def test_form_field_widget_attrs_deprecation(self):\n        from django.forms.utils import FIELD_WIDGET_ATTRS_DEPRECATION_MSG\n\n        with isolate_lru_cache(get_default_renderer), self.assertRaisesMessage(\n            RemovedInDjango50Warning, FIELD_WIDGET_ATTRS_DEPRECATION_MSG\n        ):\n            class MyForm(Form):\n                field = ChoiceField(widget=Select(attrs={'class': 'custom-select'}))\n\n            MyForm()\n", "    def test_hidden_non_field_errors(self):\n        class MyForm(Form):\n            data = IntegerField(widget=HiddenInput)\n\n        f = MyForm({})\n        f.errors[\"non_field_errors\"] = [\"This is a non-field error\"]\n        self.assertHTMLEqual(\n            f.as_p(),\n            '<ul class=\"errorlist nonfield\">'\n            \"<li>This is a non-field error</li></ul>\"\n            '<p> '\n            '<input type=\"hidden\" name=\"data\" id=\"id_data\"></p>',\n        )\n"], "sample_756": ["compilation error", "    def test_max_eps_behavior(self):\n        # Test that setting max_eps to a very small value behaves as intended\n        X = np.array([[1, 1], [1.1, 1.1], [2, 2], [3, 3]])\n        \n        # With max_eps = 0.1, only the first two points should be considered\n        # as neighbors, resulting in two clusters.\n        clust_small_eps = OPTICS(max_eps=0.1).fit(X)\n        assert_array_equal(clust_small_eps.labels_, [0, 0, -1, -1])\n\n        # With max_eps = 1, all points should be considered as neighbors\n        # and likely clustered together.\n        clust_large_eps = OPTICS(max_eps=1).fit(X)\n        assert_array_equal(clust_large_eps.labels_, [0, 0, 0, 0])\n", "compilation error", "compilation error", "compilation error"], "sample_478": ["    def test_actions_with_permissions_not_callable(self):\n        class BandAdmin(ModelAdmin):\n            actions = (\"delete_selected\",)\n\n                pass\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"Actions defined in BandAdmin must be callable. 'delete_selected' is not callable.\",\n            id=\"admin.E131\",\n        )\n", "    def test_actions_decorator_with_invalid_permissions(self):\n        @admin.action(permissions=[\"invalid_permission\"])\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action,)\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"The value 'invalid_permission' in permissions list for action 'action' \"\n            \"is not a valid permission.\",\n            id=\"admin.E131\",\n        )\n", "    def test_valid_actions_with_permissions(self):\n        @admin.action(permissions=[\"custom_permission\"])\n            pass\n\n        class BandAdmin(ModelAdmin):\n                return True\n\n            actions = (custom_permission_action,)\n\n        self.assertIsValid(BandAdmin, Band)\n\n", "    def test_actions_with_incorrect_signature_no_queryset(self):\n        @admin.action\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (bad_signature,)\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"Action 'bad_signature' has an incorrect signature. It must take \"\n            \" 'modeladmin', 'request' and 'queryset' as arguments.\",\n            id=\"admin.E131\",\n        )\n\n", "    def test_actions_with_default_permissions_not_allowed(self):\n        @admin.action\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action,)\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"Define 'permissions' argument in action decorator\",\n            id=\"admin.E131\",\n        )\n"], "sample_776": ["def test_lasso_lars_n_nonzero_coefs_deprecation():\n    # Ensure warning is raised when n_nonzero_coefs is deprecated\n    lasso_lars = linear_model.LassoLars(n_nonzero_coefs=5)\n    with pytest.warns(DeprecationWarning, match=\"The `n_nonzero_coefs`\"):\n        lasso_lars.fit(X, y)\n", "    def test_lars_path_with_sample_weight(self):\n        # Test that lars_path works correctly with sample weights\n\n        rng = np.random.RandomState(0)\n        n_samples = 10\n        n_features = 5\n        X = rng.randn(n_samples, n_features)\n        y = rng.randn(n_samples)\n        sample_weight = rng.rand(n_samples)\n\n        # Fit LassoLars with sample weights and compare coefficients\n        _, _, coef_path_weighted = linear_model.lars_path(\n            X, y, sample_weight=sample_weight, method='lasso'\n        )\n        _, _, coef_path_unweighted = linear_model.lars_path(\n            X, y, method='lasso'\n        )\n\n        assert not np.allclose(coef_path_weighted, coef_path_unweighted)\n", "    def test_lasso_lars_n_nonzero_coefs_deprecation(self):\n        # Test deprecation warning for n_nonzero_coefs in LassoLars\n        lasso_lars = linear_model.LassoLars(n_nonzero_coefs=5)\n        with pytest.warns(DeprecationWarning, match=\"The 'n_nonzero_coefs'\"):\n            lasso_lars.fit(X, y)\n", "    def test_lars_path_empty_input():\n        # Test that lars_path works correctly when given an empty input array.\n        X = np.array([]).reshape(0, 0) \n        y = np.array([])\n\n        with pytest.raises(ValueError):\n            linear_model.lars_path(X, y)\n\n", "def test_lars_path_empty_features():\n   # Test that lars_path handles cases with empty features gracefully\n   X = np.zeros((10, 0))\n   y = np.random.randn(10)\n\n   with pytest.raises(ValueError, match='Found 0 features'):\n       linear_model.lars_path(X, y)\n\n"], "sample_1122": ["compilation error", "compilation error", "compilation error", "def test_issue_16229():\n    x = Symbol('x', real=True)\n    assert Abs(I*x).expand(trig=True) == Abs(I*x)\n    assert sign(I*x).expand(trig=True) == sign(I*x)\n    assert arg(I*x).expand(trig=True) == arg(I*x)\n", "compilation error"], "sample_684": ["def test_exception_chain_repr_equality() -> None:\n    try:\n      raise ValueError(\"First ValueError\")\n    except ValueError:\n        excinfo1 = ExceptionInfo.from_current()\n    try:\n      raise RuntimeError(\"RuntimeError\")\n    except RuntimeError:\n        excinfo2 = ExceptionInfo.from_current()\n    repr1 = excinfo1.getrepr()\n    repr2 = excinfo2.getrepr()\n    assert repr1 != repr2\n    assert isinstance(repr1, ExceptionChainRepr)\n    assert isinstance(repr2, ExceptionChainRepr)\n    assert repr1 == repr1 # testing reflexivity\n    assert repr2 == repr2 # testing reflexivity\n\n", "def test_getfslineno_with_no_source():\n        pass\n    code = Code(no_source)\n    path, lineno = code.getfslineno()\n    assert path == \"\"\n    assert lineno == -1\n", "    def test_ExceptionChainRepr_single_exception(self):\n        try:\n            raise ValueError()\n        except ValueError:\n            excinfo = ExceptionInfo.from_current()\n\n        repr = excinfo.getrepr()\n        assert isinstance(repr, ExceptionChainRepr)\n        assert len(repr.chain) == 1\n", "def test_ExceptionChainRepr_with_cause():\n    try:\n        raise ValueError(\"Cause!\")\n    except ValueError as e:\n        try:\n            raise ValueError(\"Effect!\") from e\n        except ValueError as e2:\n            excinfo = ExceptionInfo.from_current()\n            repr_ = excinfo.getrepr()\n\n    assert isinstance(repr_, ExceptionChainRepr)\n    assert len(repr_.chain) == 2\n    assert repr_.chain[0][0].reprtraceback.entries[-1].line.startswith(\"raise ValueError('Effect!')\")\n    assert repr_.chain[1][0].reprtraceback.entries[-1].line.startswith(\"raise ValueError('Cause!')\")\n\n", "def test_getsource_for_dynamically_generated_code() -> None:\n        exec(\"def g(): pass\\n\", globals())\n        g()\n    \n    with pytest.raises(Exception):\n        f()\n    excinfo = ExceptionInfo.from_current()\n    frame = excinfo.traceback[0]\n    source = frame.getsource()\n    assert source is None \n"], "sample_215": ["    def test_sensitive_variables_decorator_with_multiple_arguments(self):\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_variables_multiple_arguments_view, check_for_vars=False)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_variables_multiple_arguments_view, check_for_vars=False)\n\n", "    def test_sensitive_variables_multiple_decorators(self):\n        @sensitive_variables('password')\n        @sensitive_post_parameters('api_key')\n            pass\n        self.assertTrue(test_func.__wrapped__.__name__, 'test_func')\n", "    def test_sensitive_variables_with_keyword_arguments(self):\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_kwargs_view, check_for_vars=False)\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_kwargs_view, check_for_vars=False)\n", "    def test_sensitive_variables_with_kwargs(self):\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_kwargs_view, check_for_vars=False)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_kwargs_view, check_for_vars=False)\n\n\n", "    def test_sensitive_variables_arguments_decorating_class_method(self):\n        \"\"\"\n        The sensitive_variables decorator works with class methods.\n        \"\"\"\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_class_method_view, check_for_POST_params=False)\n            self.verify_unsafe_email(sensitive_class_method_view, check_for_POST_params=False)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_class_method_view, check_for_POST_params=False)\n            self.verify_safe_email(sensitive_class_method_view, check_for_POST_params=False)\n"], "sample_465": ["    def test_get_list_filter_field_types(self):\n        class TestModelAdmin(ModelAdmin):\n            list_filter = ['name', 'bio']\n\n        ma = TestModelAdmin(Band, self.site)\n        self.assertEqual(ma.get_list_filter_field_types(request), {'name': 'CharField', 'bio': 'TextField'})\n", "    def test_get_model_perms(self):\n        ma = ModelAdmin(Band, self.site)\n        perms = ma.get_model_perms(request=MockRequest())\n        self.assertEqual(perms, {\"add\", \"change\", \"delete\", \"view\"})\n", "    def test_get_fieldsets_with_custom_fields(self):\n        class CustomAdmin(ModelAdmin):\n            fieldsets = (\n                (None, {\"fields\": [\"name\", \"bio\", \"sign_date\"]}),\n                (\n                    \"Additional Information\",\n                    {\"fields\": [\"extra_field\"], \"classes\": (\"wide\",)},\n                ),\n            )\n\n        ma = CustomAdmin(Band, self.site)\n        self.assertEqual(ma.get_fieldsets(request), ma.fieldsets)\n", "    def test_get_readonly_fields_with_formfield_exclude(self):\n        class AdminBandForm(forms.ModelForm):\n            class Meta:\n                model = Band\n                exclude = ['bio']\n                fields = ['name']\n\n        class BandAdmin(ModelAdmin):\n            readonly_fields = ['name']\n            form = AdminBandForm\n\n        ma = BandAdmin(Band, self.site)\n        self.assertEqual(list(ma.get_form(request).base_fields), [\"name\"])\n        self.assertEqual(\n            list(ma.get_fieldsets(request)),\n            [(None, {\"fields\": [\"name\"]})],\n        )\n", "    def test_get_readonly_fields(self):\n        class AdminBandForm(forms.ModelForm):\n            class Meta:\n                model = Band\n                fields = \"__all__\"\n\n        class BandAdmin(ModelAdmin):\n            form = AdminBandForm\n            readonly_fields = [\"name\"]\n\n        ma = BandAdmin(Band, self.site)\n        self.assertEqual(ma.get_readonly_fields(request), [\"name\"])\n"], "sample_447": ["    def test_alias_with_subquery_and_filter(self):\n        qs = (\n            Book.objects.filter(\n                isbn__in=Subquery(Book.objects.alias(isbn_alias=F(\"isbn\")).values(\"isbn_alias\"))\n            )\n            .filter(isbn__startswith=\"159\")\n        )\n        self.assertCountEqual(qs, [self.b1, self.b2])\n", "    def test_alias_forbidden_chars(self):\n        tests = [\n            'al\"ias',\n            \"a'lias\",\n            \"ali`as\",\n            \"alia s\",\n            \"alias\\t\",\n            \"ali\\nas\",\n            \"alias--\",\n            \"ali/*as\",\n            \"alias*/\",\n            \"alias;\",\n            # [] are used by MSSQL.\n            \"alias[\",\n            \"alias]\",\n        ]\n        msg = (\n            \"Column aliases cannot contain whitespace characters, quotation marks, \"\n            \"semicolons, or SQL comments.\"\n        )\n        for crafted_alias in tests:\n            with self.subTest(crafted_alias):\n                with self.assertRaisesMessage(ValueError, msg):\n                    Book.objects.alias(**{crafted_alias: Value(1)})\n\n", "    def test_alias_forbidden_chars(self):\n        tests = [\n            'al\"ias',\n            \"a'lias\",\n            \"ali`as\",\n            \"alia s\",\n            \"alias\\t\",\n            \"ali\\nas\",\n            \"alias--\",\n            \"ali/*as\",\n            \"alias*/\",\n            \"alias;\",\n            # [] are used by MSSQL.\n            \"alias[\",\n            \"alias]\",\n        ]\n        msg = (\n            \"Column aliases cannot contain whitespace characters, quotation marks, \"\n            \"semicolons, or SQL comments.\"\n        )\n        for crafted_alias in tests:\n            with self.subTest(crafted_alias):\n                with self.assertRaisesMessage(ValueError, msg):\n                    Book.objects.alias(**{crafted_alias: Value(1)})\n", "    def test_alias_transform_with_f_expression(self):\n        qs = Book.objects.alias(\n            pub_year=ExtractYear(\"pubdate\"),\n        ).annotate(\n            top_rating=Subquery(\n                Book.objects.filter(pubdate__year=F(\"pub_year\"))\n                .order_by(\"-rating\")\n                .values(\"rating\")[:1]\n            ),\n        ).values(\"pub_year\", \"top_rating\")\n        self.assertCountEqual(\n            qs,\n            [\n                {\"pub_year\": 1991, \"top_rating\": 5.0},\n                {\"pub_year\": 1995, \"top_rating\": 4.0},\n                {\"pub_year\": 2007, \"top_rating\": 4.5},\n                {\"pub_year\": 2008, \"top_rating\": 4.0},\n            ],\n        )\n\n", "    def test_alias_forbidden_chars(self):\n        tests = [\n            'al\"ias',\n            \"a'lias\",\n            \"ali`as\",\n            \"alia s\",\n            \"alias\\t\",\n            \"ali\\nas\",\n            \"alias--\",\n            \"ali/*as\",\n            \"alias*/\",\n            \"alias;\",\n            # [] are used by MSSQL.\n            \"alias[\",\n            \"alias]\",\n        ]\n        msg = (\n            \"Column aliases cannot contain whitespace characters, quotation marks, \"\n            \"semicolons, or SQL comments.\"\n        )\n        for crafted_alias in tests:\n            with self.subTest(crafted_alias):\n                with self.assertRaisesMessage(ValueError, msg):\n                    Book.objects.alias(**{crafted_alias: Value(1)})\n"], "sample_495": ["    def test_Paginator_empty_object_list_with_orphans(self):\n        paginator = Paginator([], 2, orphans=1)\n        self.assertEqual(paginator.num_pages, 1)\n        self.assertEqual(paginator.page(1).object_list, [])\n", "    def test_empty_paginator_with_allow_empty_first_page_false(self):\n        paginator = Paginator([], 2, allow_empty_first_page=False)\n        with self.assertRaises(EmptyPage):\n            paginator.page(1)\n", "    def test_paginator_invalid_per_page_zero(self):\n        \"\"\"\n        Paginator should raise ValueError when per_page is zero.\n        \"\"\"\n        with self.assertRaises(ValueError):\n            Paginator([1, 2, 3], 0)\n", "    def test_no_content_allow_empty_first_page_false(self):\n        paginator = Paginator([], 2, allow_empty_first_page=False)\n        with self.assertRaises(EmptyPage):\n            paginator.page(1) \n", "    def test_get_page_valid_page_number(self):\n        paginator = Paginator(Article.objects.order_by('id'), 5)\n        self.assertEqual(paginator.get_page(1).number, 1)\n        self.assertEqual(paginator.get_page(2).number, 2)\n"], "sample_78": ["    def test_call_command_with_positional_args(self):\n        out = StringIO()\n        management.call_command('positional_args', 'hello', 'world', stdout=out)\n        self.assertIn('hello world', out.getvalue())\n", "    def test_call_command_with_positional_args(self):\n        out = StringIO()\n        management.call_command('positional_args', 'foo', 'bar', stdout=out)\n        self.assertIn('You provided foo and bar as positional arguments.', out.getvalue())\n", "    def test_subcommand_help(self):\n        out, err = self.run_manage(['help', 'subparser'])\n        self.assertNoOutput(err)\n        self.assertIn('usage: manage.py subparser [SUBCOMMAND] [options]', out)\n", "    def test_call_command_with_unknown_app(self):\n        with self.assertRaises(CommandError):\n            management.call_command('unknown_app.unknown_command', stdout=StringIO())\n", "    def test_command_handle_app_config_called(self):\n        \"\"\"\n        Ensure handle_app_config() is called for each app label passed.\n        \"\"\"\n        class TestCommand(AppCommand):\n                self.called_apps.append(app_config.name)\n\n        TestCommand.called_apps = []\n        command = TestCommand()\n        command.handle('app1', 'app2', 'app3')\n        self.assertEqual(command.called_apps, ['app1', 'app2', 'app3'])\n\n"], "sample_1176": ["compilation error", "compilation error", "def test_issue_10587():\n    assert (Float(1.1) - Float(1.0)).n(2) == Float('0.10')\n", "compilation error", "    def test_issue_11010():\n        x = S.Half\n        y = 3\n        assert (x // y) == 0\n        assert (x / y) == S.Half / 3\n"], "sample_312": ["    def test_add_squash(self):\n        node1 = Node([('a', 1), ('b', 2)], 'AND')\n        node2 = Node([('c', 3), ('d', 4)], 'AND')\n\n        # Squashing possible\n        node1.add(node2, 'AND')\n        self.assertEqual(len(node1), 4)\n        self.assertEqual(str(node1), \"(AND: ('a', 1), ('b', 2), ('c', 3), ('d', 4))\")\n\n        # Squashing not possible due to different connectors\n        node3 = Node([('e', 5)], 'OR')\n        node1.add(node3, 'AND')\n        self.assertEqual(len(node1), 5)\n        self.assertEqual(str(node1), \"(AND: ('a', 1), ('b', 2), ('c', 3), ('d', 4), ('e', 5))\")\n", "    def test_add_squash(self):\n        node1 = Node([('a', 1), ('b', 2)], 'AND')\n        node2 = Node([('c', 3), ('d', 4)], 'AND')\n        combined_node = node1.add(node2, 'AND')\n        self.assertEqual(len(combined_node.children), 4)\n        self.assertEqual(str(combined_node), \"(AND: ('a', 1), ('b', 2), ('c', 3), ('d', 4))\")\n", "    def test_add_squash(self):\n        node3 = Node([('a', 1), ('b', 2)], 'AND')\n        node4 = Node([('c', 3), ('d', 4)], 'AND')\n        self.assertEqual(node3.add(node4, 'AND'), node4)\n        self.assertEqual(str(node3), \"(AND: ('a', 1), ('b', 2), ('c', 3), ('d', 4))\")\n", "    def test_add_squash(self):\n        node1 = Node([('a', 1)], 'AND')\n        node2 = Node([('b', 2)], 'AND')\n        node3 = node1.add(node2, 'AND')\n        self.assertEqual(node3, node2)\n        self.assertEqual(node1.children, [('a', 1), ('b', 2)])\n\n", "    def test_add_squash(self):\n        node1 = Node([('a', 1), ('b', 2)], 'AND')\n        node2 = Node([('c', 3)], 'AND')\n        node3 = node1.add(node2, 'AND')\n        self.assertEqual(len(node3), 3)\n        self.assertEqual(set(node3.children), {('a', 1), ('b', 2), ('c', 3)})\n"], "sample_436": ["    def test_empty_settings(self):\n        \"\"\"Ensure that loading an empty settings module doesn't crash.\"\"\"\n        self.write_settings(\"empty_settings.py\", \"\")\n        args = [\"dumpdata\", \"--settings=empty_settings\"]\n        out, err = self.run_manage(args)\n        self.assertNoOutput(out)\n        self.assertOutput(err, \"No Django models found. You can only dump data from models defined in your Django project.\")\n\n", "    def test_invalid_command_with_arguments(self):\n        args = [\"invalid_command\", \"arg1\", \"arg2\", \"--settings=test_project.settings\"]\n        out, err = self.run_django_admin(args)\n        self.assertNoOutput(out)\n        self.assertOutput(err, \"Unknown command: 'invalid_command'.\")\n", "    def test_command_with_invalid_option(self):\n        args = [\"startproject\", \"testproject\", \"--invalid-option\"]\n        out, err = self.run_django_admin(args)\n        self.assertNoOutput(out)\n        self.assertOutput(\n            err,\n            \"unrecognized arguments: --invalid-option\",\n        )\n", "    def test_custom_setting_as_integer(self):\n        self.write_settings(\"settings.py\", sdict={\"MY_SETTING\": 123})\n        args = [\"diffsettings\", \"--settings=settings\"]\n        out, err = self.run_manage(args)\n        self.assertNoOutput(err)\n        self.assertOutput(out, \"MY_SETTING = 123\")\n", "    def test_invalid_target_name_with_relative_path(self):\n        for bad_target in (\n            \"invalid.dir_name\",\n            \"7invalid_dir_name\",\n            \".invalid_dir_name\",\n        ):\n            with self.subTest(bad_target):\n                _, err = self.run_django_admin([\"startapp\", \"app\", os.path.join(\"apps\", bad_target)])\n                self.assertOutput(\n                    err,\n                    \"CommandError: '%s' is not a valid app directory. Please \"\n                    \"make sure the directory is a valid identifier.\" % bad_target,\n                )\n"], "sample_136": ["    def test_header_case_insensitive(self):\n        environ = {\n            'CONTENT-TYPE': 'text/html',\n            'CONTENT-LENGTH': '100',\n            'HTTP_HOST': 'example.com',\n            'http_user_agent': 'python-requests/1.2.0',\n        }\n        headers = HttpHeaders(environ)\n        self.assertEqual(headers['content-type'], 'text/html')\n        self.assertEqual(headers['CONTENT-LENGTH'], '100')\n        self.assertEqual(headers['host'], 'example.com')\n        self.assertEqual(headers['User-Agent'], 'python-requests/1.2.0')\n", "    def test_getlist(self):\n        environ = {\n            'CONTENT_TYPE': 'text/html',\n            'CONTENT_LENGTH': '100',\n            'HTTP_HOST': 'example.com',\n            'HTTP_ACCEPT': 'text/html,application/xhtml+xml',\n        }\n        headers = HttpHeaders(environ)\n        self.assertEqual(headers.getlist('Accept'), ['text/html', 'application/xhtml+xml'])\n", "    def test_getlist(self):\n        environ = {\n            'HTTP_COOKIE': 'foo=bar; moo=cow',\n            'HTTP_ACCEPT': 'text/html,application/xhtml+xml',\n        }\n        headers = HttpHeaders(environ)\n        self.assertEqual(headers.getlist('Cookie'), ['foo=bar; moo=cow'])\n        self.assertEqual(headers.getlist('Accept'), ['text/html,application/xhtml+xml'])\n", "    def test_http_headers_getlist(self):\n        environ = {\n            'HTTP_COOKIE': 'foo=bar; baz=quux',\n            'HTTP_ACCEPT': 'text/html,application/xhtml+xml',\n        }\n        headers = HttpHeaders(environ)\n        self.assertEqual(headers.getlist('Cookie'), ['foo=bar; baz=quux'])\n        self.assertEqual(headers.getlist('Accept'), ['text/html,application/xhtml+xml'])\n", "    def test_header_order(self):\n        environ = {\n            'HTTP_ACCEPT': 'text/html',\n            'HTTP_HOST': 'example.com',\n            'CONTENT_TYPE': 'text/plain',\n            'CONTENT_LENGTH': '100',\n        }\n        headers = HttpHeaders(environ)\n        self.assertEqual(list(headers), [\n            'Accept', 'Content-Length', 'Content-Type', 'Host',\n        ])\n"], "sample_579": ["    def test_dendrogram_kws():\n        g = mat.clustermap(self.df_norm, dendrogram_kws={'linkage':'ward'})\n        assert g.dendrogram_col.linkage == 'ward'\n        assert g.dendrogram_row.linkage == 'ward'\n\n\n\n", "    def test_row_col_colors_single_color(self):\n\n        kws = self.default_kws.copy()\n        kws['row_colors'] = (1, 0, 0)\n        kws['col_colors'] = (0, 1, 0)\n\n        cm = mat.clustermap(self.df_norm, **kws)\n\n        assert len(cm.ax_row_colors.collections) == 1\n        assert len(cm.ax_col_colors.collections) == 1\n        colors = cm.ax_row_colors.collections[0].get_facecolors()\n        assert np.all(colors == (1, 0, 0))\n        colors = cm.ax_col_colors.collections[0].get_facecolors()\n        assert np.all(colors == (0, 1, 0))\n\n", "    def test_dendrogram_leaves(self):\n\n        kws = self.default_kws.copy()\n\n        g = mat.clustermap(self.df_norm, **kws)\n\n\n\n        assert g.dendrogram_row.leaves == g.dendrogram_row.reordered_ind\n\n        assert g.dendrogram_col.leaves == g.dendrogram_col.reordered_ind\n", "    def test_linkage_methods(self):\n        kws = self.default_kws.copy()\n        kws['row_linkage'] = 'ward'\n        cm1 = mat.clustermap(self.df_norm, **kws)\n        kws['row_linkage'] = 'single'\n        cm2 = mat.clustermap(self.df_norm, **kws)\n        assert not np.array_equal(cm1.dendrogram_row.linkage,\n                                  cm2.dendrogram_row.linkage)\n        kws['col_linkage'] = 'average'\n        cm3 = mat.clustermap(self.df_norm, **kws)\n        assert not np.array_equal(cm1.dendrogram_col.linkage,\n                                  cm3.dendrogram_col.linkage)\n", "    def test_col_colors_df_with_missing_cols(self):\n        # Test case with col colors DataFrame having missing column indices.\n\n        \n        kws = self.default_kws.copy()\n\n        col_colors = pd.DataFrame({'col_annot': list(self.col_colors)},\n                                  index=self.df_norm.columns[:-1])\n        kws['col_colors'] = col_colors\n\n        cm = mat.clustermap(self.df_norm, **kws)\n        assert list(cm.col_colors) == list(self.col_colors)[:-1]\n\n"], "sample_98": ["    def test_multiple_requests_simultaneously(self):\n        with concurrent.futures.ThreadPoolExecutor() as executor:\n            futures = [executor.submit(self.urlopen, '/example_view/') for _ in range(10)]\n            for future in concurrent.futures.as_completed(futures):\n                self.assertEqual(future.result().read(), b'example view')\n", "    def test_handles_broken_pipe_error(self):\n        \"\"\"\n        Ensure the server handles broken pipe errors gracefully\n        without crashing or logging an error.\n        \"\"\"\n        with self.assertRaises(HTTPError) as err:\n            self.urlopen('/example_view/')\n        # Simulate a broken pipe error\n        err.exception.close()\n        with self.urlopen('/example_view/') as f:\n            self.assertEqual(f.read(), b'example view')\n", "    def test_handle_broken_pipe_error(self):\n        \"\"\"\n        Test that the server handles broken pipe errors gracefully and logs\n        them appropriately.\n        \"\"\"\n        conn = HTTPConnection(LiveServerBase.server_thread.host, LiveServerBase.server_thread.port)\n        conn.request('GET', '/example_view/')\n        response = conn.getresponse()\n        conn.close()  # Simulate abrupt closure\n\n        # Check for log message\n        self.assertLogContains(logger, 'Broken pipe from')\n\n    \n", "    def test_threaded_server_handles_multiple_requests(self):\n        with self.urlopen('/threaded_view/') as f1:\n            response1 = f1.read()\n        with self.urlopen('/threaded_view/') as f2:\n            response2 = f2.read()\n        self.assertEqual(response1, b'thread 1 response')\n        self.assertEqual(response2, b'thread 2 response')\n", "    def test_threading_server_closes_connection_with_content_length(self):\n        \"\"\"\n        Contrast to\n        LiveServerViews.test_keep_alive_on_connection_with_content_length().\n        Persistent connections require threading server.\n        \"\"\"\n        conn = HTTPConnection(\n            LiverServerThreadedTests.server_thread.host,\n            LiverServerThreadedTests.server_thread.port,\n            timeout=1,\n        )\n        try:\n            conn.request('GET', '/example_view/', headers={'Connection': 'keep-alive'})\n            response = conn.getresponse()\n            self.assertTrue(response.will_close)\n            self.assertEqual(response.read(), b'example view')\n            self.assertEqual(response.status, 200)\n            self.assertEqual(response.getheader('Connection'), 'close')\n        finally:\n            conn.close()\n\n"], "sample_294": ["    def test_csrf_cookie_secure_flag(self):\n        \"\"\"\n        CSRF cookie is marked as secure if CSRF_COOKIE_SECURE is True.\n        \"\"\"\n        req = self._get_GET_no_csrf_cookie_request()\n        mw = CsrfViewMiddleware(token_view)\n        mw.process_view(req, token_view, (), {})\n        resp = mw(req)\n        self.assertTrue(resp.cookies[settings.CSRF_COOKIE_NAME]['secure'])\n", "    def test_csrf_cookie_domain_not_subdomain(self):\n        \"\"\"\n        The CSRF cookie should not be set if the domain does not match.\n        \"\"\"\n        req = self._get_GET_no_csrf_cookie_request()\n        req.META['HTTP_HOST'] = 'example.org'\n        mw = CsrfViewMiddleware(token_view)\n        mw.process_view(req, token_view, (), {})\n        resp = mw(req)\n        self.assertFalse(CSRF_COOKIE_NAME in resp.cookies)\n", "    def test_csrf_token_on_500_stays_constant(self):\n        response = self.client.get('/internal_server_error/')\n        self.assertEqual(response.status_code, 500)\n        token1 = response.content\n        response = self.client.get('/internal_server_error/')\n        self.assertEqual(response.status_code, 500)\n        token2 = response.content\n        self.assertTrue(equivalent_tokens(token1.decode('ascii'), token2.decode('ascii')))\n", "    def test_csrf_token_on_500_stays_constant(self):\n        response = self.client.get('/error/')\n        self.assertEqual(response.status_code, 500)\n        token1 = response.content\n        response = self.client.get('/error/')\n        self.assertEqual(response.status_code, 500)\n        token2 = response.content\n        self.assertTrue(equivalent_tokens(token1.decode('ascii'), token2.decode('ascii')))\n", "    def test_csrf_cookie_secure_flag(self):\n        \"\"\"\n        The CSRF cookie is set with the Secure flag when CSRF_COOKIE_SECURE is True.\n        \"\"\"\n        req = self._get_GET_no_csrf_cookie_request()\n        mw = CsrfViewMiddleware(token_view)\n        mw.process_view(req, token_view, (), {})\n        resp = mw(req)\n        self.assertEqual(resp.cookies[settings.CSRF_COOKIE_NAME]['secure'], True)\n\n"], "sample_255": ["    def test_persistent_connections(self):\n        \"\"\"Test that persistent connections are handled correctly.\"\"\"\n\n            start_response('200 OK', [])\n            yield b'Hello, world!'\n\n        # Create a server instance\n        server = WSGIServer(('localhost', 0), WSGIRequestHandler, allow_reuse_address=True)\n\n        # Start the server in a separate thread\n        import threading\n        server_thread = threading.Thread(target=server.serve_forever)\n        server_thread.daemon = True\n        server_thread.start()\n\n        # Make two requests to the server\n        with self.request_factory.get('/'):\n            response = self.client.get('/')\n        self.assertEqual(response.status_code, 200)\n\n        # Since persistent connections are enabled, the client should reuse\n        # the existing connection for the second request\n        with self.request_factory.get('/'):\n            response = self.client.get('/')\n        self.assertEqual(response.status_code, 200)\n\n        # Shutdown the server\n        server.shutdown()\n        server_thread.join()\n", "    def test_handle_one_request(self):\n        request = WSGIRequest(self.request_factory.get('/').environ)\n        request.makefile = lambda *args, **kwargs: BytesIO()\n        handler = WSGIRequestHandler(request, '192.168.0.2', None)\n        handler.close_connection = False\n        \n            start_response('200 OK', [])\n            yield b'Hello, world!'\n\n        server = Stub(get_app=lambda: test_app)\n        handler.server = server\n        handler.handle_one_request()\n        self.assertEqual(request.get_full_path(), \n                        '/')\n        self.assertEqual(handler.get_response().getvalue(), b'Hello, world!')\n", "    def test_ipv6(self):\n        \"\"\"WSGIServer can be configured to use IPv6.\"\"\"\n        server = WSGIServer(('::1', 0), WSGIRequestHandler, ipv6=True)\n        # Simply ensure the server can be created without raising an exception.\n        server.server_close()\n", "    def test_close(self):\n        request = WSGIRequest(self.request_factory.get('/').environ)\n        # Need to mock out stdin to avoid blocking on read\n            if args[0] == 'rb': \n                return BytesIO() \n            return Stub(sendall=lambda data: None)\n\n        request.makefile = makefile\n        handler = ServerHandler(\n            BytesIO(),\n            Stub(),\n            Stub(),\n            request.environ,\n        )\n\n        handler.close()\n        self.assertFalse(handler.get_stdin()._read_called)\n\n", "    def test_run(self):\n        \"\"\"Test that run() starts the server.\"\"\"\n        with self.assertLogs('django.server', 'INFO') as cm:\n            with captured_stderr() as err:\n                run('127.0.0.1', 8000, self.get_wsgi_app, ipv6=False, threading=False)\n        self.assertIn('Starting development server at http://127.0.0.1:8000/', cm.output[0])\n"], "sample_185": ["    def test_get_language_from_path_null(self):\n        g = trans_null.get_language_from_path\n        self.assertIsNone(g('/pl/'))\n        self.assertIsNone(g('/pl'))\n        self.assertIsNone(g('/xyz/'))\n", "    def test_round_away_from_one_infinity(self):\n        tests = [\n            (float('inf'), float('inf')),\n            (float('-inf'), float('-inf')),\n        ]\n        for value, expected in tests:\n            with self.subTest(value=value):\n                self.assertEqual(round_away_from_one(value), expected)\n", "    def test_get_language_info_empty_language(self):\n        with self.assertRaises(KeyError):\n            get_language_info('')\n", "    def test_round_away_from_one_decimal(self):\n        tests = [\n            (1.1, 1),\n            (1.2, 1),\n            (1.3, 1),\n            (1.4, 1),\n            (1.5, 2),\n            (1.6, 2),\n            (1.7, 2),\n            (1.8, 2),\n            (1.9, 2),\n            (-1.1, -1),\n            (-1.2, -1),\n            (-1.3, -1),\n            (-1.4, -1),\n            (-1.5, -2),\n            (-1.6, -2),\n            (-1.7, -2),\n            (-1.8, -2),\n            (-1.9, -2),\n        ]\n        for value, expected in tests:\n            with self.subTest(value=value):\n                self.assertEqual(round_away_from_one(value, decimal=1), expected)\n", "    def test_get_language_bidi(self):\n        with self.settings(LANGUAGE_CODE='ar'):\n            self.assertTrue(is_rtl('ar'))\n            self.assertFalse(is_rtl('en'))\n"], "sample_649": ["def test_log_cli_level_overrides_logging_ini(testdir):\n    testdir.makeini(f\"\"\"\n        [pytest]\n        log_level = WARNING\n    \"\"\")\n    testdir.makepyfile(\n        \"\"\"\n    import logging\n\n        logging.getLogger().info('This should be hidden')\n        logging.getLogger().warning('This should be visible')\n\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--log-cli-level=DEBUG\", \"-s\")\n    assert result.ret == ExitCode.OK\n\n    result.stdout.fnmatch_lines(\n        [\n            \"INFO     test_log_cli_level_overrides.py:5 This should be hidden\",\n            \"WARNING  test_log_cli_level_overrides.py:6 This should be visible\",\n        ]\n    )\n\n\n\n", "def test_log_file_cli_subdirectories_already_exists(pytester):\n    path = pytester.makepyfile(\"\"\" def test_logger(): pass \"\"\")\n    os.makedirs(os.path.join(os.path.dirname(str(path)), \"foo\", \"bar\"), exist_ok=True)\n    with open(os.path.join(os.path.dirname(str(path)), \"foo\", \"bar\", \"logf.log\"), \"w\") as f:\n        f.write(\"hello\")\n    result = pytester.runpytest(\"--log-file=foo/bar/logf.log\")\n    assert result.ret == ExitCode.OK\n", "def test_log_cli_level_overrides_log_file_level(pytester: Pytester) -> None:\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_file_level=INFO\n        \"\"\"\n    )\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n        logger = logging.getLogger(__name__)\n            logger.debug(\"This should not be shown\")\n            logger.info(\"This should be shown\")\n            logger.warning(\"This should be shown\")\n    \"\"\"\n    )\n    result = pytester.runpytest(\"--log-cli-level=DEBUG\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_log_levels.py:*INFO*This should be shown*\",\n            \"*test_log_levels.py:*WARNING*This should be shown*\",\n        ]\n    )\n\n", "    def test_log_disabling_doesnt_affect_other_loggers_with_same_name_prefix(\n        testdir,", "def test_disable_loggers_multiple_levels(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        logger = logging.getLogger(\"parent.child.grandchild\")\n\n            with caplog.at_level(logging.DEBUG):\n                logger.info(\"Some message\")\n                assert caplog.record_tuples == []\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--log-disable=parent.child\", \"-s\")\n    assert result.ret == ExitCode.OK\n    assert not result.stderr.lines\n"], "sample_344": ["    def test_proxy_with_fk(self):\n        A = self.create_model(\"A\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n        B = self.create_model(\"B\")\n        P = self.create_model(\"P\", bases=(A,), proxy=True)\n        self.assertRelated(A, [B])\n        self.assertRelated(B, [A, P])\n        self.assertRelated(P, [B])\n", "    def test_proxy_model_fk(self):\n        A = self.create_model(\"A\")\n        B = self.create_model(\"B\", bases=(A,), proxy=True)\n        C = self.create_model(\"C\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n        self.assertRelated(A, [B, C])\n        self.assertRelated(B, [C])\n        self.assertRelated(C, [A, B])\n", "    def test_proxy_with_fk(self):\n        A = self.create_model(\"A\")\n        B = self.create_model(\"B\", bases=(A,), proxy=True, foreign_keys=[models.ForeignKey('C', models.CASCADE)])\n        C = self.create_model(\"C\")\n        self.assertRelated(A, [B, C])\n        self.assertRelated(B, [C])\n        self.assertRelated(C, [A, B])\n", "    def test_abstract_model_inheritance(self):\n        class AbstractBase(models.Model):\n            name = models.CharField(max_length=100)\n\n            class Meta:\n                abstract = True\n                app_label = 'migrations'\n\n        class ConcreteModel1(AbstractBase):\n            pass\n\n        class ConcreteModel2(AbstractBase):\n            additional_field = models.CharField(max_length=50)\n\n        state1 = ModelState.from_model(ConcreteModel1)\n        state2 = ModelState.from_model(ConcreteModel2)\n        self.assertEqual(len(state1.fields), 1)\n        self.assertEqual(len(state2.fields), 2)\n\n", "    def test_proxy_base_fk(self):\n        A = self.create_model(\"A\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n        B = self.create_model(\"B\", bases=(A,), proxy=True)\n        C = self.create_model(\"C\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n        self.assertRelated(A, [B, C])\n        self.assertRelated(B, [A, C])\n        self.assertRelated(C, [A, B])\n\n"], "sample_444": ["    def test_collectstatic_no_changes(self):\n        # Create initial static files.\n        file_contents = (\n            (\"foo.png\", \"foo\"),\n            (\"bar.css\", 'url(\"foo.png\")'),\n        )\n        for filename, content in file_contents:\n            with open(self._get_filename_path(filename), \"w\") as f:\n                f.write(content)\n\n        with self.modify_settings(STATICFILES_DIRS={\"append\": self._temp_dir}):\n            finders.get_finder.cache_clear()\n            # First collectstatic run.\n            call_command(\"collectstatic\", interactive=False, verbosity=0)\n\n            # The hashes of the png files in the CSS file stay\n            # the same after a second collectstatic.\n            call_command(\"collectstatic\", interactive=False, verbosity=0)\n            relpath = self.hashed_file_path(\"test/bar.css\")\n            with storage.staticfiles_storage.open(relpath) as relfile:\n                content = relfile.read()\n                self.assertIn(b\"foo.acbd18db4cc2.png\", content)\n", "    def test_manifest_strict_missing(self):\n        missing_file_name = \"cached/missing.css\"\n        configured_storage = storage.staticfiles_storage\n        storage.staticfiles_storage.manifest_strict = True\n        self.assertNotIn(missing_file_name, configured_storage.hashed_files)\n        with self.assertRaisesMessage(\n            ValueError,\n            \"Missing staticfiles manifest entry for '%s'\" % missing_file_name,\n        ):\n            self.hashed_file_path(missing_file_name)\n\n", "    def test_empty_static_files_directory(self):\n        # Create an empty static files directory.\n        empty_dir = os.path.join(self._temp_dir, \"empty_dir\")\n        os.makedirs(empty_dir)\n\n        with self.modify_settings(STATICFILES_DIRS={\"append\": empty_dir}):\n            finders.get_finder.cache_clear()\n            err = StringIO()\n            call_command(\"collectstatic\", interactive=False, verbosity=0, stderr=err)\n            self.assertEqual(err.getvalue(), \"\")\n", "    def test_empty_file_hash(self):\n        filename = \"test/empty_file.css\"\n        with open(self._get_filename_path(filename), \"w\") as f:\n            pass\n\n        with self.modify_settings(STATICFILES_DIRS={\"append\": self._temp_dir}):\n            finders.get_finder.cache_clear()\n            call_command(\"collectstatic\", interactive=False, verbosity=0)\n            relpath = self.hashed_file_path(filename)\n            self.assertEqual(relpath, f\"cached/empty_file.{storage.staticfiles_storage.manifest_hash}.css\")\n\n", "    def test_manifest_hash_after_file_change(self):\n        file_contents = (\n            (\"foo.png\", \"foo\"),\n            (\"bar.css\", 'url(\"foo.png\")'),\n        )\n        for filename, content in file_contents:\n            with open(self._get_filename_path(filename), \"w\") as f:\n                f.write(content)\n\n        with self.modify_settings(STATICFILES_DIRS={\"append\": self._temp_dir}):\n            finders.get_finder.cache_clear()\n            # First collectstatic run.\n            self.run_collectstatic()\n            _, manifest_hash_orig = storage.staticfiles_storage.load_manifest()\n            self.assertNotEqual(manifest_hash_orig, \"\")\n\n            # Change the contents of the png file.\n            with open(self._get_filename_path(\"foo.png\"), \"w+b\") as f:\n                f.write(b\"new content of file to change its hash\")\n\n            # Second collectstatic run.\n            self.run_collectstatic()\n            _, manifest_hash = storage.staticfiles_storage.load_manifest()\n            self.assertNotEqual(manifest_hash, manifest_hash_orig)\n"], "sample_170": ["    def test_sensitive_variables_empty_list(self):\n        @sensitive_variables()\n            return locals()\n        result = test_func()\n        self.assertEqual(result, {})\n", "    def test_sensitive_variables_with_kwargs(self):\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_kwargs_view, check_for_vars=False)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_kwargs_view, check_for_vars=False)\n", "    def test_sensitive_variables_with_lambda(self):\n        with self.settings(DEBUG=True):\n            @sensitive_variables\n                return password\n            with self.assertRaises(TypeError):\n                test_func('mypassword')\n", "    def test_sensitive_variables_with_default_arguments(self):\n        @sensitive_variables('password')\n            return password\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(lambda request: test_func(), check_for_vars=False)\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(lambda request: test_func(), check_for_vars=False)\n", "    def test_sensitive_variables_with_multiple_decorators(self):\n        @sensitive_variables('password', 'secret_key')\n        @sensitive_variables('api_key')\n            pass\n        # Assertions for testing the logic of using multiple decorators\n        # can be added here based on the desired behavior.\n"], "sample_1093": ["compilation error", "def test_SymPyPrinter_print_seq():\n    s = SymPyPrinter()\n\n    assert s._print_seq(range(2)) == '(0, 1,)'\n", "def test_piecewise_evalf():\n    from sympy import Piecewise, S, Eq, Symbol\n\n    x = Symbol('x')\n    p = Piecewise((1, Eq(x, 0)), (2, True))\n    prntr = PythonCodePrinter()\n\n    assert p.evalf(subs={x: 0}).dtype == numpy.dtype(float)\n    assert prntr.doprint(p.evalf(subs={x: 0})) == '1.0'\n    assert p.evalf(subs={x: 1}).dtype == numpy.dtype(float)\n    assert prntr.doprint(p.evalf(subs={x: 1})) == '2.0'\n\n", "def test_SymPyPrinter_print_seq():\n    s = SymPyPrinter()\n\n    assert s._print_seq(range(2)) == '(0, 1)'\n", "def test_SymPyPrinter_piecewise():\n    p = SymPyPrinter()\n    x = symbols('x')\n    expr = Piecewise((x**2, x > 0), (x, True))\n    assert p.doprint(expr) == 'Piecewise((x**2, x > 0), (x, True))'\n"], "sample_723": ["def test_imputation_error():\n    # Test for errors when imputing\n    X = np.array([[1, 2], [3, np.nan]])\n    imputer = SimpleImputer(strategy=\"most_frequent\")\n\n    # Test error when axis is invalid\n    assert_raises(ValueError, imputer.fit, X, axis=2)\n\n    # Test error when missing_values is not a number or 'NaN'\n    assert_raises(TypeError, SimpleImputer, missing_values=\"a\",\n                  strategy=\"mean\")\n\n", "    def test_imputation_empty_columns_rows():\n        # Test imputation when all values in a column or row are missing.\n\n        X_incomplete = np.array([[np.nan, 1, 2],\n                                 [np.nan, 4, 5],\n                                 [np.nan, np.nan, np.nan]])\n\n        X_expected = np.array([[np.nan, 1, 2],\n                                [np.nan, 4, 5],\n                                [np.nan, 4, 5]])\n\n        imputer = SimpleImputer(strategy='most_frequent', axis=0)\n        X_imputed = imputer.fit_transform(X_incomplete)\n        assert_array_equal(X_imputed, X_expected)\n\n        X_incomplete = np.array([[1, np.nan, 2],\n                                 [4, np.nan, 5],\n                                 [np.nan, np.nan, np.nan]])\n\n        X_expected = np.array([[1, 4, 2],\n                                [4, 4, 5],\n                                [1, 4, 2]])\n\n        imputer = SimpleImputer(strategy='most_frequent', axis=1)\n        X_imputed = imputer.fit_transform(X_incomplete)\n        assert_array_equal(X_imputed, X_expected)\n\n", "def test_imputation_sparse_csc_matrix():\n    # Test imputation with sparse CSC matrix with missing_values=0.\n    X_csc = sparse.csc_matrix([[1, 0, 3],\n                               [4, 0, 6],\n                               [0, 8, 9]])\n    imputer = SimpleImputer(missing_values=0, strategy='mean')\n    X_transformed = imputer.fit_transform(X_csc)\n    assert isinstance(X_transformed, sparse.csc_matrix)\n    assert_array_equal(X_transformed.toarray(),\n                       np.array([[1, 0, 3],\n                                [4, 5, 6],\n                                [5, 8, 9]]))\n\n", "def test_imputation_empty_input():\n    # Test imputation with empty input\n\n    X_empty = np.array([])\n    X_empty = X_empty.reshape(0, 5)\n\n    for strategy in [\"mean\", \"median\", \"most_frequent\"]:\n        imputer = SimpleImputer(missing_values=0, strategy=strategy)\n        imputer.fit(X_empty)\n        assert imputer.statistics_.size == 5\n\n\n", "def test_imputation_invalid_input():\n    # Test imputation with invalid input types\n\n    X_invalid_type = [['a', 'b', 'c'], ['d', 'e', 'f']]\n\n    for strategy in [\"mean\", \"median\", \"most_frequent\"]:\n        imputer = SimpleImputer(missing_values=0, strategy=strategy)\n        assert_raises(TypeError, imputer.fit, X_invalid_type)\n"], "sample_252": ["    def test_contains_contained_by_with_key_transform_and_null(self):\n        self.assertIs(NullableJSONModel.objects.filter(\n            value__contains=KeyTransform('foo', 'value'),\n        ).exists(), True)\n        self.assertIs(NullableJSONModel.objects.filter(\n            value__contained_by=KeyTransform('foo', 'value'),\n        ).exists(), True)\n", "    def test_key_transform_with_lookup(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__c__gt=KeyTransform('d', KeyTransform(1, 'value'))),\n            [self.objs[4]],\n        )\n", "    def test_key_transform_with_null_in_subquery(self):\n        subquery = NullableJSONModel.objects.filter(pk=OuterRef('pk')).values(\n            'value',\n        )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.annotate(\n                subquery_value=subquery,\n                key=KeyTransform('0', 'subquery_value'),\n            ).filter(key__isnull=True),\n            [self.objs[0]],\n        )\n\n", "    def test_key_transform_expression_function(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.annotate(\n                length=Length(KeyTransform('c', 'value')),\n            ).filter(length=2),\n            [self.objs[3], self.objs[4]],\n        )\n\n", "    def test_key_transform_with_function(self):\n        self.assertEqual(\n            NullableJSONModel.objects.filter(\n                value__c=Length(KeyTransform('d', 'value')),\n            ).get(),\n            self.objs[4],\n        )\n"], "sample_990": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_556": ["def test_add_subplot_axes_kwargs_conflict():\n    fig, ax = plt.subplots(1, 1)\n    with pytest.raises(ValueError, match=\"Conflicting arguments\"):\n        fig.add_subplot(1, 1, 1, projection='polar', axes_class=Axes)\n\n", "def test_figure_get_axes():\n    fig, ax = plt.subplots()\n    assert fig.get_axes() == [ax]\n    fig.add_subplot(111)\n    assert len(fig.get_axes()) == 2\n", "    def test_add_axes_kwargs_projection():\n        fig = plt.figure()\n        ax = fig.add_axes([0, 0, 1, 1], projection='polar')\n        assert ax.name == 'polar'\n\n\n", "    def test_subfigure_tightbbox_with_constrained_layout():\n        fig = plt.figure(layout='constrained')\n        sub_fig = fig.subfigures()\n        ax = sub_fig.add_subplot()\n        ax.set_xlabel('X Label', fontsize=12)\n        ax.set_ylabel('Y Label', fontsize=12)\n        ax.set_title('Title', fontsize=14)\n        fig.tight_layout()\n        assert fig.bbox_inches.width == ax.bbox_inches.width\n", "    def test_figure_add_axes_twice():\n        fig, ax = plt.subplots()\n        with pytest.raises(ValueError, match=\"Axes already exists\"):\n            fig.add_axes(ax.get_position())\n\n"], "sample_700": ["    def test_importorskip_module_level(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n                import doesnotexist\n                assert doesnotexist.something()\n        \"\"\"\n        )\n        result = pytester.runpytest()\n        result.stdout.fnmatch_lines(\n            [\n                \"*could not import 'doesnotexist': No module named 'doesnotexist'*\",\n                \"*1 skipped*\",\n            ]\n        )\n", "    def test_importorskip_module(pytester: Pytester) -> None:\n        modpath = pytester.makepyfile(\n            test_helpers=\"\"\"\n                pass\n        \"\"\"\n        ).replace(\"test_helpers.py\", \"\")\n        pytester.makepyfile(\n            test_one=\"\"\"\n            import pytest\n            pytest.importorskip(\"test_helpers\")\n\n                test_helpers.helper()\n        \"\"\"\n        )\n        result = pytester.runpytest(\"-rs\", \"--rootdir=.\")\n        assert result.ret == 0\n        result.stdout.fnmatch_lines([\"*1 passed*\"])\n", "    def test_importorskip_with_reason(pytester: Pytester) -> None:\n        with pytest.raises(pytest.skip.Exception) as excinfo:\n            pytest.importorskip(\"doesnotexist\", reason=\"My reason\")\n        assert str(excinfo.value) == \"could not import 'doesnotexist': My reason: No module named 'doesnotexist'\"\n\n", "def test_importorskip_with_reason(pytester: Pytester) -> None:\n    with pytest.raises(pytest.skip.Exception, match=\".*No module named .*\"):\n        pytest.importorskip(\"doesnotexist\", reason=\"This module should not exist\")\n\n", "    def test_importorskip_existing(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n            pytest.importorskip(\"os\")\n\n                pass\n            \"\"\"\n        )\n        result = pytester.runpytest()\n\n        result.stdout.fnmatch_lines([\"*1 passed*\"])\n\n"], "sample_646": ["    def test_using_unittest_skip_in_setup_class(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import unittest\n\n            class MyTestCase(unittest.TestCase):\n                @classmethod\n                    raise unittest.SkipTest(\"skip this class\")\n\n                    pass\n                    pass\n        \"\"\"\n        )\n        reprec = pytester.inline_run()\n        passed, skipped, failed = reprec.countoutcomes()\n        assert passed == 0\n        assert skipped == 1\n        assert failed == 0\n        assert reprec.ret == 0\n", "    def test_raising_unittest_skiptest_during_collection_with_message(\n        pytester: Pytester,", "    def test_raising_unittest_skipif_during_collection(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n        import unittest\n\n        class TestIt(unittest.TestCase):\n            @unittest.skipIf(True, \"Skip this test\")\n\n\n        class TestIt2(unittest.TestCase):\n        \"\"\"\n        )\n        reprec = pytester.inline_run()\n        passed, skipped, failed = reprec.countoutcomes()\n        assert passed == 1\n        assert skipped == 1\n        assert failed == 0\n        assert reprec.ret == 0\n\n", "    def test_raising_unittest_skipif_during_collection(\n        pytester: Pytester,", "    def test_skip_test_marker(pytester: Pytester)-> None:\n        pytester.makepyfile(\n            \"\"\"\n            import unittest\n            import pytest\n\n            @pytest.mark.skip\n            class TestIt(unittest.TestCase):\n                    pass\n\n            class TestIt2(unittest.TestCase):\n                @pytest.mark.skip\n                    pass\n\n        \"\"\"\n        )\n        result = pytester.runpytest()\n        result.stdout.fnmatch_lines([\"*1 skipped*\", \"*1 skipped*\"])\n"], "sample_1196": ["def test_contains_evaluate_false():\n    x = Symbol('x')\n    assert Contains(x, S.Naturals, evaluate=False) == Contains(x, S.Naturals)\n", "def test_Contains_evaluate_false():\n    x = Symbol('x')\n    assert Contains(x, Interval(1, 2)).evaluate(x, 3) is S.False\n", "compilation error", "def test_contains_evaluate():\n    x = Symbol('x')\n    assert Contains(x, S.Naturals, evaluate = False) == Contains(x, S.Naturals)\n    assert Contains(x, S.Naturals, evaluate=True) == (x > 0) & (x.is_integer)\n", "def test_contains_evaluate():\n    x = Symbol('x')\n    assert Contains(x, S.Naturals, evaluate=False).args == (x, S.Naturals)\n    assert Contains(x, S.Naturals, evaluate=True) == (x>=1)\n"], "sample_590": ["    def test_concat_empty_arrays(self):\n        ds1 = Dataset({\"a\": ((\"x\", \"y\"), [[1]])}, coords={\"x\": [0], \"y\": [0]})\n        ds2 = ds1.isel(x=slice(None), y=slice(None) ).isel(x=slice(0,0))\n\n        expected = Dataset({\"a\": ((\"x\", \"y\"), [[1]])}, coords={\"x\": [0], \"y\": [0]})\n\n        actual = concat([ds1, ds2], dim=\"x\")\n        assert_identical(expected, actual)\n", "    def test_concat_coords_overlap(self):\n        ds1 = Dataset({\"a\": (\"x\", [1, 2]), \"b\": (\"x\", [3, 4])}, coords={\"x\": [0, 1]})\n        ds2 = Dataset({\"a\": (\"x\", [5, 6]), \"b\": (\"x\", [7, 8])}, coords={\"x\": [1, 2]})\n        expected = Dataset(\n            {\"a\": (\"x\", [1, 2, 5, 6]), \"b\": (\"x\", [3, 4, 7, 8])}, coords={\"x\": [0, 1, 2]}\n        )\n        actual = concat([ds1, ds2], dim=\"x\")\n        assert_identical(actual, expected)\n", "    def test_concat_empty(self):\n        ds1 = Dataset({\"foo\": (\"x\", [])})\n        ds2 = Dataset({\"bar\": (\"x\", [1, 2])})\n        actual = concat([ds1, ds2], \"x\")\n        expected = Dataset({\"bar\": (\"x\", [1, 2])})\n        assert_identical(expected, actual)\n", "    def test_concat_preserve_attrs(self):\n        ds1 = Dataset({\"a\": (\"x\", [1, 2]), \"b\": (\"x\", [3, 4])}, attrs={\"foo\": \"bar\"})\n        ds2 = Dataset({\"a\": (\"x\", [5, 6]), \"b\": (\"x\", [7, 8])}, attrs={\"baz\": \"qux\"})\n        expected = Dataset(\n            {\"a\": ((\"x\", \"concat_dim\"), [1, 2, 5, 6]), \"b\": ((\"x\", \"concat_dim\"), [3, 4, 7, 8])},\n            attrs={\"foo\": \"bar\", \"baz\": \"qux\"},\n        )\n        actual = concat([ds1, ds2], dim=\"concat_dim\")\n        assert_identical(expected, actual)\n\n", "    def test_concat_empty_datasets(self):\n        actual = concat([], \"dim1\")\n        expected = Dataset()\n        assert_identical(expected, actual)\n\n"], "sample_843": ["def test_kernel_hyperparameter_bounds(kernel):\n    # Check that hyperparameter bounds are respected.\n    for hyperparameter in kernel.hyperparameters:\n        if isinstance(\"string\", type(hyperparameter.bounds)):\n            if hyperparameter.bounds == \"fixed\":\n                continue\n        bounds = hyperparameter.bounds\n        if np.isscalar(bounds):\n            bounds = (0, bounds)\n        value = bounds[0] - 0.1\n        with pytest.raises(ValueError):\n            kernel.set_params(**{hyperparameter.name: value})\n        value = bounds[1] + 0.1\n        with pytest.raises(ValueError):\n            kernel.set_params(**{hyperparameter.name: value})\n\n", "def test_kernel_operator_commutativity_with_sum():\n    # Adding kernels should be commutative even when using KernelOperator\n\n    kernel1 = RBF(2.0) + ConstantKernel(2.0)\n    kernel2 = ConstantKernel(2.0) + RBF(2.0)\n    assert_almost_equal(kernel1(X), kernel2(X))\n\n    kernel3 = 2.0 * RBF(1.0) + RBF(2.0)\n    kernel4 = RBF(2.0) + 2.0 * RBF(1.0) \n    assert_almost_equal(kernel3(X), kernel4(X))\n\n", "    def test_kernel_operator(kernel):\n        # Test KernelOperator functionality\n\n        # Test composition: kernel * kernel\n        k_squared = kernel * kernel\n        k_squared_eval = k_squared(X)\n        k_eval = kernel(X)\n        assert_array_almost_equal(k_squared_eval, k_eval * k_eval)\n\n        # Test addition: kernel + kernel\n        k_sum = kernel + kernel\n        k_sum_eval = k_sum(X)\n        assert_array_almost_equal(k_sum_eval, 2 * k_eval)\n\n        # Test scalar multiplication\n        k_scaled = 2.0 * kernel\n        k_scaled_eval = k_scaled(X)\n        assert_array_almost_equal(k_scaled_eval, 2.0 * k_eval)\n\n        # Test mixing of kernels\n\n        k_mixed = 2 * kernel + kernel * RBF(0.5)\n        k_mixed_eval = k_mixed(X)\n        assert_array_almost_equal(\n            k_mixed_eval, 2*kernel(X) + kernel(X) * RBF(0.5)(X)\n        )\n", "def test_kernel_exponent():\n    # Test that the Exponentiation kernel correctly raises the base kernel\n    # to the given power.\n    base_kernel = RBF(length_scale=2.0)\n    exponent = 2.0\n    kernel = Exponentiation(base_kernel, exponent)\n\n    K1 = kernel(X)\n    K2 = base_kernel(X) ** exponent\n    assert_almost_equal(K1, K2)\n", "def test_kernel_exponentiation():\n    # Test the Exponentiation kernel.\n    base_kernel = RBF(length_scale=1.0)\n    kernel = Exponentiation(base_kernel, 2)\n    K = kernel(X)\n    assert_array_almost_equal(K, base_kernel(X)**2)\n"], "sample_199": ["    def test_annotation_filter_with_aggregate(self):\n        qs = Author.objects.annotate(\n            count_books=Count('book'),\n        ).filter(count_books__gt=2).values('name')\n        self.assertCountEqual(\n            qs,\n            [{'name': 'Adrian Holovaty'}, {'name': 'Jacob Kaplan-Moss'}],\n        )\n\n", "    def test_annotation_with_transform_function(self):\n        qs = Book.objects.annotate(\n            publisher_name=F('publisher__name'),\n            num_books_by_publisher=Count('publisher'),\n        ).transform(\n            Sum('num_books_by_publisher')\n        ).values_list('publisher_name', 'num_books_by_publisher')\n        self.assertCountEqual(qs, [\n            ('Apress', 1),\n            (\"Jonno's House of Books\", 1),\n            ('Morgan Kaufmann', 2),\n            ('Prentice Hall', 2),\n            ('Sams', 1),\n        ])\n\n", "    def test_annotation_with_join_and_filter_on_join(self):\n        stores = Store.objects.annotate(\n            book_prices=Subquery(\n                Book.objects.filter(store=OuterRef('pk')).values('price')\n            )\n        ).filter(book_prices__gt=Decimal(20))\n        self.assertCountEqual(\n            stores,\n            [\n                Store.objects.get(id=self.s1.pk),\n            ]\n        )\n\n", "    def test_annotation_aggregate_with_m2m(self):\n        qs = Book.objects.values('publisher').annotate(\n            max_rating=Max(F('authors__rating')),\n        ).order_by('max_rating')\n        self.assertCountEqual(\n            qs,\n            [\n                {'publisher': self.p1.pk, 'max_rating': 5.0},\n                {'publisher': self.p2.pk, 'max_rating': 4.5},\n                {'publisher': self.p3.pk, 'max_rating': 3.5},\n                {'publisher': self.p4.pk, 'max_rating': 4.0},\n                {'publisher': self.p5.pk, 'max_rating': 3.0},\n            ]\n        )\n", "    def test_annotation_with_complex_filtering(self):\n        qs = Book.objects.filter(\n            publisher__name__startswith='M',\n            rating__gt=4,\n        ).annotate(\n            avg_price=Avg(F('price')),\n            highest_rated=Max('rating'),\n        ).filter(\n            avg_price__gt=Decimal('20'),\n        ).values('avg_price', 'highest_rated')\n\n        self.assertEqual(len(qs), 1)\n        self.assertEqual(qs[0]['avg_price'], Decimal('25.00'))\n        self.assertEqual(qs[0]['highest_rated'], 5)\n\n"], "sample_204": ["    def test_conflicting_apps(self):\n        \"\"\"\n        Tests that a MigrationLoader raises an error when there are apps with the same name,\n        but different module paths, and attempts to load migrations from both.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        with self.assertRaisesMessage(ValueError, \"Conflicting app labels found: 'myapp'.\"):\n            loader.build_graph()\n\n", "    def test_detect_conflicts(self):\n        loader = MigrationLoader(connection)\n        conflicts = loader.detect_conflicts()\n        self.assertEqual(conflicts, {'migrations': ['0001_initial', '0002_initial']})\n", "    def test_circular_dependency(self):\n        \"\"\"\n        Tests that circular dependencies are detected.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        with self.assertRaisesMessage(ValueError, \"Circular dependency detected: migrations.0001_initial depends on migrations.0002_second, which depends on migrations.0001_initial\"):\n            loader.build_graph()\n", "    def test_loading_squashed_dependency(self):\n        loader = MigrationLoader(connection)\n        recorder = MigrationRecorder(connection)\n        self.addCleanup(recorder.flush)\n        recorder.record_applied('migrations', '0002_second')\n        loader.build_graph()\n        plan = set(loader.graph.forwards_plan(('migrations', '0003_third')))\n        plan = plan - loader.applied_migrations.keys()\n        expected_plan = {('migrations', '0003_third')}\n        self.assertEqual(plan, expected_plan)\n", "    def test_loading_circular_dependencies(self):\n        \"Tests loading migrations with circular dependencies\"\n        loader = MigrationLoader(connection)\n        with self.assertRaises(ValueError) as e:\n            loader.build_graph()\n        self.assertIn(\"Circular dependency detected\", str(e.exception))\n"], "sample_576": ["    def test_legend_artist_mapping(self, xy):\n\n        class MockMark1(MockMark):\n                return mpl.lines.Line2D([], [])\n\n        class MockMark2(MockMark):\n                return mpl.patches.Patch()\n\n        s = pd.Series([\"a\", \"b\", \"a\", \"c\"], name=\"s\")\n        p = Plot(**xy, color=s).add(MockMark1()).add(MockMark2()).plot()\n        artists = p._legend_contents[0][1]\n        assert all(isinstance(a, mpl.lines.Line2D) for a in artists)\n\n", "    def test_legend_order(self, xy):\n\n        s1 = pd.Series([\"a\", \"b\", \"c\", \"a\"], name=\"s1\")\n        s2 = pd.Series([1, 2, 1, 3], name=\"s2\")\n\n        p = Plot(**xy, color=s1, marker=s2).add(MockMark()).plot()\n        e1, e2 = p._legend_contents\n\n        assert e1[0] == (s1.name, s1.name)\n        assert e2[0] == (s2.name, s2.name)\n\n        labels1 = categorical_order(s1)\n        labels2 = categorical_order(s2)\n\n        assert e1[-1] == labels1\n        assert e2[-1] == labels2\n", "    def test_legend_title(self, xy):\n\n        s = pd.Series([\"a\", \"b\", \"a\", \"c\"], name=\"s\")\n        p = Plot(**xy, color=s).add(MockMark(), legend_title=\"My Legend\").plot()\n        legend, = p._figure.legends\n        assert legend.get_title().get_text() == \"My Legend\"\n\n", "    def test_multi_layer_wrap(self, xy):\n\n        s1 = pd.Series([\"a\", \"b\", \"a\", \"c\"], name=\"s1\")\n        s2 = pd.Series([\"m\", \"m\", \"p\", \"m\"], name=\"s2\")\n        sem = dict(color=s1), dict(marker=s2)\n        p = Plot(**xy, wrap=3).add(MockMark(), **sem[0]).add(MockMark(), **sem[1]).plot()\n        e1, e2 = p._legend_contents\n\n        for e, s in zip([e1, e2], [s1, s2]):\n            assert e[0] == (s.name, s.name)\n\n            labels = categorical_order(s)\n            assert e[-1] == labels\n\n            artists = e[1]\n            assert len(artists) == len(labels)\n            for a, label in zip(artists, labels):\n                assert isinstance(a, mpl.artist.Artist)\n                assert a.value == label\n                assert a.variables == [s.name]\n", "    def test_pair_no_orient(self, long_df):\n\n        p = Plot(long_df).pair(x=[\"x\", \"y\"], y=[\"z\", \"f\"]).plot()\n        for ax in p._figure.axes:\n            assert ax.get_xlabel() is not None\n            assert ax.get_ylabel() is not None\n\n"], "sample_807": ["def test_calibration_multiclass_with_different_classes():\n    # Test calibration for multiclass setting when classes are not\n    # consecutive integers\n    X, y = make_classification(n_samples=100, n_features=2,\n                               random_state=42, n_classes=3)\n\n    target_names = ['a', 'c', 'e']\n    y = target_names[y]\n\n    X_train, y_train = X[::2], y[::2]\n    X_test, y_test = X[1::2], y[1::2]\n\n    clf = LinearSVC()\n    clf.fit(X_train, y_train)\n    for method in ['isotonic', 'sigmoid']:\n        cal_clf = CalibratedClassifierCV(clf, method=method, cv=2)\n        cal_clf.fit(X_train, y_train)\n\n        probas = cal_clf.predict_proba(X_test)\n        assert_array_almost_equal(np.sum(probas, axis=1), np.ones(len(X_test)))\n", "def test_calibration_multiclass_all_probs():\n    \"\"\"Test that multiclass calibration returns probabilities for all classes\"\"\"\n    X, y = make_blobs(n_samples=100, n_features=2, random_state=42,\n                      centers=3, cluster_std=3.0)\n    clf = LinearSVC()\n    calibrated_clf = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=2)\n    calibrated_clf.fit(X, y)\n\n    probas = calibrated_clf.predict_proba(X)\n    assert_equal(probas.shape[1], 3)\n    assert_array_almost_equal(probas.sum(axis=1), np.ones(len(X)))\n", "def test_calibration_single_class():\n    # Test calibration when there is only one class\n    X = np.random.randn(10, 5)\n    y = np.zeros(10)\n    clf = LinearSVC(C=1.0)\n    cal_clf = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=LeaveOneOut())\n    cal_clf.fit(X, y)\n\n    probas = cal_clf.predict_proba(X)\n    assert_array_almost_equal(probas[:, 0], np.ones(len(y)))\n    assert_array_almost_equal(probas[:, 1:], np.zeros((len(y), 0)))\n", "    def test_calibration_with_different_cv_splits():\n        n_samples = 50\n        X, y = make_classification(n_samples=3 * n_samples, n_features=6,\n                                   random_state=42)\n        clf = MultinomialNB()\n        \n        # Create different cv splits\n        cv1 = LeaveOneOut()\n        cv2 = KFold(n_splits=5, shuffle=True, random_state=42)\n\n        # Train CalibratedClassifierCV with different cv splits\n        calibrated_clf1 = CalibratedClassifierCV(clf, cv=cv1, method=\"sigmoid\")\n        calibrated_clf2 = CalibratedClassifierCV(clf, cv=cv2, method=\"sigmoid\")\n        calibrated_clf1.fit(X, y)\n        calibrated_clf2.fit(X, y)\n\n        # Predict probabilities\n        y_prob1 = calibrated_clf1.predict_proba(X)[:, 1]\n        y_prob2 = calibrated_clf2.predict_proba(X)[:, 1]\n\n        # Assert that the predictions are different due to different cv splits\n        assert_not_equal(y_prob1, y_prob2) \n", "def test_calibration_cv_parameter():\n    X, y = make_classification(n_samples=100, n_features=5, random_state=42)\n    clf = LinearSVC()\n\n    # Test that setting cv to an integer raises a deprecation warning\n    with pytest.warns(FutureWarning):\n        CalibratedClassifierCV(clf, cv=5)\n\n    # Test setting cv to None\n    cal_clf = CalibratedClassifierCV(clf, cv=None)\n    cal_clf.fit(X, y)\n"], "sample_67": ["    def test_modelform_factory_with_formfield_callback_and_exclude(self):\n            return db_field.formfield(**kwargs)\n        Form = modelform_factory(Person, fields='__all__', exclude=['age'], formfield_callback=callback)\n        self.assertEqual(list(Form.base_fields), [\"name\"])\n", "    def test_modelform_factory_with_exclude(self):\n        Form = modelform_factory(Person, exclude=['name'])\n        self.assertEqual(list(Form.base_fields), [])\n", "    def test_modelform_with_callable_initial_value(self):\n        class MyModel(models.Model):\n            my_field = models.CharField(max_length=10)\n\n            return 'initial'\n\n        class MyForm(forms.ModelForm):\n            class Meta:\n                model = MyModel\n                fields = '__all__'\n                initial = {'my_field': initial_value}\n\n        form = MyForm()\n        self.assertEqual(form.initial['my_field'], 'initial')\n", "    def test_modelform_factory_with_exclude(self):\n        Form = modelform_factory(Person, exclude=['name'])\n        self.assertEqual(list(Form.base_fields), [])\n", "    def test_many_to_many_empty(self):\n        item = ColourfulItem.objects.create()\n        data = model_to_dict(item)['colours']\n        self.assertEqual(data, [])\n\n"], "sample_1080": ["def test_refine_Piecewise_with_constants():\n    assert refine(Piecewise((1, x < 0), (2, True)), Q.is_true(x < 0)) == 1\n    assert refine(Piecewise((1, x < 0), (2, True)), ~Q.is_true(x < 0)) == 2\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_936": ["    def test_stringify_type_hints_forward_ref():\n        if sys.version_info >= (3, 7):\n            from typing import ForwardRef\n            assert stringify(ForwardRef('int')) == \"int\"\n            assert stringify(ForwardRef('List[str]')) == \"List[str]\"\n        else:\n            from sphinx.util.typing import ForwardRef\n            assert stringify(ForwardRef('int')) == \"int\"\n            assert stringify(ForwardRef('List[str]')) == \"List[str]\"\n", "    def test_stringify_type_hints_forwardref():\n        from typing import ForwardRef\n        assert stringify(ForwardRef(\"str\")) == \"str\"\n\n", "def test_stringify_type_hints_Generic():\n    class MyGeneric(Generic[T]):\n        pass\n    assert stringify(MyGeneric) == \"test_util_typing.MyGeneric\"\n    assert stringify(MyGeneric[int]) == \"test_util_typing.MyGeneric[int]\"\n", "def test_stringify_is_system_TypeVar():\n    assert stringify(TypeVar('T')) == 'T'\n    assert stringify(TypeVar('T', covariant=True)) == 'T'\n    assert stringify(TypeVar('T', contravariant=True)) == 'T'\n", "def test_stringify_ForwardRef():\n    from typing import ForwardRef\n    assert stringify(ForwardRef('int')) == 'int'\n    assert stringify(ForwardRef('List')) == 'List'\n    assert stringify(ForwardRef('test_util_typing.MyList')) == 'test_util_typing.MyList'\n"], "sample_1057": ["compilation error", "compilation error", "compilation error", "compilation error", "    def test_fully_qualified_modules():\n        printer = PythonCodePrinter({'fully_qualified_modules': True})\n        ast = Print('x y'.split(), \"coordinate: %12.5g %12.5g\")\n        rendered = render_as_module(ast)\n        assert \"sympy.codegen.ast\" in rendered\n        assert \"print\" in rendered\n"], "sample_732": ["def test_subset_SA_random_state():\n    try:\n        data1 = fetch_kddcup99(subset='SA', random_state=0, shuffle=True,\n                              download_if_missing=False)\n        data2 = fetch_kddcup99(subset='SA', random_state=0, shuffle=True,\n                              download_if_missing=False)\n    except IOError:\n        raise SkipTest(\"kddcup99 dataset can not be loaded.\")\n    assert_equal(data1.data, data2.data)\n    assert_equal(data1.target, data2.target)\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_63": ["    def test_find_template_loader_error(self):\n        with self.assertRaises(ImproperlyConfigured):\n            Engine(loaders=[\n                ('django.template.loaders.cached.Loader', [\n                    'django.template.loaders.filesystem.Loader',\n                    'invalid.loader.path',\n                ]),\n            ]).get_template('test_context.html')\n", "    def test_get_template_libraries(self):\n        engine = Engine(libraries={'my_library': 'my_app.templatetags.my_library'})\n        self.assertIn('my_library', engine.template_libraries)\n        self.assertIsNotNone(engine.template_libraries['my_library'])\n", "    def test_find_template_loader_invalid_loader(self):\n        with self.assertRaises(ImproperlyConfigured) as e:\n            Engine(loaders=[('django.template.loaders.cached.Loader', ['invalid_loader'])])\n        self.assertIn('Invalid value in template loaders configuration', str(e.exception))\n", "    def test_find_template_loader_invalid_loader(self):\n        engine = Engine(dirs=[TEMPLATE_DIR])\n        msg = \"Invalid value in template loaders configuration: 'invalid'\"\n        with self.assertRaisesMessage(ImproperlyConfigured, msg):\n            engine.find_template_loader('invalid')\n", "    def test_app_dirs_and_loaders(self):\n        with self.assertRaises(ImproperlyConfigured) as e:\n            Engine(app_dirs=True, loaders=['django.template.loaders.filesystem.Loader'])\n        self.assertIn(\"app_dirs must not be set when loaders is defined.\", str(e.exception))\n\n"], "sample_918": ["def test_modindex_show_inherited_members(app):\n    app.config.pygments_style = 'sphinx'\n    text = (\".. py:class:: Parent\\n\"\n            \"   :members:\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr1\\n\"\n            \"\\n\"\n            \".. py:class:: Child(Parent)\\n\"\n            \"   :members:\\n\")\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('a', [IndexEntry('attr1', 1, 'index', 'Child.attr1', '', '', '')]),\n         ('C', [IndexEntry('Child', 1, 'index', 'Child', '', '', '')]),\n         ('P', [IndexEntry('Parent', 1, 'index', 'Parent', '', '', '')])],\n        True\n    )\n\n\n\n", "def test_pyattribute_with_default_value(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :type: str\\n\"\n            \"      :default: 'Hello'\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'attr (Class attribute)', 'Class.attr', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"attr\"],\n                                                     [desc_annotation, (\": \",\n                                                                        [pending_xref, \"str\"])],\n                                                     [desc_annotation, \" = 'Hello'\"])],\n                                   [desc_content, ()]))\n    assert 'Class.attr' in domain.objects\n    assert domain.objects['Class.attr'] == ('index', 'Class.attr', 'attribute')\n\n", "    def test_pyattribute_signature_with_default(app):\n        text = (\".. py:class:: Class\\n\"\n                \"\\n\"\n                \"   .. py:attribute:: attr\\n\"\n                \"      :type: str\\n\"\n                \"      :default: 'value'\\n\")\n        domain = app.env.get_domain('py')\n        doctree = restructuredtext.parse(app, text)\n        assert_node(doctree, (addnodes.index,\n                              [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                       [desc_name, \"Class\"])],\n                                      [desc_content, (addnodes.index,\n                                                      desc)])]))\n        assert_node(doctree[1][1][0], addnodes.index,\n                    entries=[('single', 'attr (Class attribute)', 'Class.attr', '', None)])\n        assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"attr\"],\n                                                        [desc_annotation, (\": \",\n                                                                           [pending_xref, \"str\"])],\n                                                        [desc_annotation, \" = 'value'\"])],\n                                      [desc_content, ()]))\n        assert 'Class.attr' in domain.objects\n        assert domain.objects['Class.attr'] == ('index', 'Class.attr', 'attribute') \n\n\n\n", "    def test_pyexception_signature_with_annotation(app):\n        text = \".. py:exception:: MyException(ValueError)\"\n        doctree = restructuredtext.parse(app, text)\n        assert_node(doctree, (addnodes.index,\n                              [desc, ([desc_signature, ([desc_annotation, \"exception \"],\n                                                        [desc_addname, \"MyException\"],\n                                                        [desc_parameterlist,\n                                                         (\n                                                             [desc_parameter, \"ValueError\"],\n                                                         )])],\n                                      desc_content)]))\n        assert_node(doctree[1], desc, desctype=\"exception\",\n                    domain=\"py\", objtype=\"exception\", noindex=False)\n", "def test_pymodule_only_module_index(app, env):\n    text = (\".. py:module:: docutils\\n\"\n            \".. py:module:: sphinx.config\\n\")\n    restructuredtext.parse(app, text)\n    domain = env.get_domain('py')\n    index = PythonModuleIndex(domain)\n    assert index.generate() == (\n        [('d', [IndexEntry('docutils', 0, 'index', 'module-docutils', '', '', '')]),\n         ('s', [IndexEntry('sphinx', 1, 'index', 'module-sphinx', '', '', ''),\n                IndexEntry('sphinx.config', 2, 'index', 'module-sphinx.config', '', '', '')])],\n        False\n    )\n"], "sample_493": ["    def test_subquery_annotation_with_multiple_aggregates(self):\n        with CaptureQueriesContext(connection) as ctx:\n            queryset = (\n                Book.objects.annotate(\n                    total_pages=Sum(\"pages\"),\n                    avg_rating=Avg(\"rating\"),\n                )\n                .filter(total_pages__gt=100)\n                .values(\"avg_rating\")\n                .annotate(total_books=Count(\"id\"))\n            )\n            for _ in queryset:\n                pass\n        self.assertGreater(len(ctx.captured_queries), 1)\n", "    def test_aggregate_annotation_on_related_field_with_filter(self):\n        with CaptureQueriesContext(connection) as ctx:\n            agg_result = Publisher.objects.annotate(\n                avg_price=Avg(\"book__price\", filter=Q(book__rating__gt=4))\n            ).aggregate(avg_avg_price=Avg(\"avg_price\"))\n        sql = ctx.captured_queries[0][\"sql\"].lower()\n        self.assertIn(\"book.rating > 4\", sql)\n        self.assertEqual(agg_result[\"avg_avg_price\"], Decimal(\"10.00\"))\n\n", "    def test_aggregate_annotation_with_non_aggregate_field(self):\n        with CaptureQueriesContext(connection) as ctx:\n            Book.objects.annotate(\n                authors_count=Count(\"authors\"),\n                name=F(\"name\")\n            ).count()\n        sql = ctx.captured_queries[0][\"sql\"].lower()\n        self.assertEqual(sql.count(\"select\"), 2, \"Subquery wrapping required\")\n        self.assertIn(\"authors_count\", sql)\n        self.assertIn(\"name\", sql)\n", "    def test_unused_aggregate_in_subquery(self):\n        with CaptureQueriesContext(connection) as ctx:\n            total_books_qs = Book.objects.annotate(\n                total_authors=Count(\"authors\")\n            ).values(\"total_authors\")\n            Author.objects.aggregate(\n                total_books_with_authors=Sum(\n                    Subquery(total_books_qs.filter(total_authors__gt=1))\n                )\n            )\n\n        sql = ctx.captured_queries[0][\"sql\"].lower()\n        self.assertEqual(sql.count(\"select\"), 3)\n        self.assertNotIn(\"total_authors\", sql)\n", "    def test_aggregation_default_using_time_from_database_with_timezone(self):\n        now = timezone.now().astimezone(datetime.timezone.utc)\n        expr = Min(\n            \"store__friday_night_closing\",\n            filter=~Q(store__name=\"Amazon.com\"),\n            default=TruncHour(Now(), output_field=TimeField()),\n        )\n        queryset = Book.objects.annotate(oldest_store_opening=expr).order_by(\"isbn\")\n        self.assertSequenceEqual(\n            queryset.values(\"isbn\", \"oldest_store_opening\"),\n            [\n                {\"isbn\": \"013235613\", \"oldest_store_opening\": datetime.time(21, 30)},\n                {\n                    \"isbn\": \"013790395\",\n                    \"oldest_store_opening\": datetime.time(23, 59, 59),\n                },\n                {\"isbn\": \"067232959\", \"oldest_store_opening\": now.time()},\n                {\"isbn\": \"155860191\", \"oldest_store_opening\": datetime.time(21, 30)},\n                {\n                    \"isbn\": \"159059725\",\n                    \"oldest_store_opening\": datetime.time(23, 59, 59),\n                },\n                {\"isbn\": \"159059996\", \"oldest_store_opening\": datetime.time(21, 30)},\n            ],\n        )\n"], "sample_194": ["    def test_opclasses_database_constraint(self):\n        UniqueConstraintProduct.objects.create(price=1, discounted_price=1)\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintProduct.objects.create(price=1, discounted_price=1)\n", "    def test_deferred_unique_with_condition(self):\n        obj_1 = UniqueConstraintConditionProduct.objects.create(name='p1', color='red')\n        obj_2 = UniqueConstraintConditionProduct.objects.create(name='p2')\n\n        with atomic(), connection.cursor() as cursor:\n            constraint_name = connection.ops.quote_name('name_condition_deferred_uniq')\n            cursor.execute('SET CONSTRAINTS %s DEFERRED' % constraint_name)\n\n            obj_1.name, obj_2.name = obj_2.name, obj_1.name\n            obj_1.save()\n            obj_2.save()\n\n        with self.assertRaises(IntegrityError):\n            with atomic(), connection.cursor() as cursor:\n                constraint_name = connection.ops.quote_name('name_condition_deferred_uniq')\n                cursor.execute('SET CONSTRAINTS %s IMMEDIATE' % constraint_name)\n                obj_1.save()\n", "    def test_database_constraint_with_opclasses(self):\n        UniqueConstraintProduct.objects.create(name='p3', color='red')\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintProduct.objects.create(name='p3', color='blue')\n", "    def test_database_constraint_deferred(self):\n        obj_1 = UniqueConstraintDeferrable.objects.create(name='p1', shelf='front')\n        obj_2 = UniqueConstraintDeferrable.objects.create(name='p2', shelf='back')\n        obj_1.shelf, obj_2.shelf = obj_2.shelf, obj_1.shelf\n        obj_1.save()\n        obj_2.save()\n", "    def test_opclasses_database_constraint(self):\n        UniqueConstraintProduct.objects.create(name='p1', color='red', price=10, discounted_price=5)\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintProduct.objects.create(name='p1', color='blue', price=10, discounted_price=5)\n"], "sample_1085": ["compilation error", "    def test_issue_21457():\n        assert (sqrt(2) - 1) // 1 == 0\n        assert (sqrt(2) - 1) // S.Half == 2\n", "compilation error", "compilation error", "compilation error"], "sample_916": ["def test_build_domain_cpp_with_add_function_parentheses_is_True_and_show_signatures(app, status, warning):\n    app.builder.build_all()\n\n        pattern = '<li><p>%s<a .*?><code .*?><span .*?>%s</span></code></a></p></li>' % spec\n        res = re.search(pattern, text)\n        if not res:\n            print(\"Pattern\\n\\t%s\\nnot found in %s\" % (pattern, file))\n            assert False\n    rolePatterns = [\n        ('', 'Sphinx'),\n        ('', 'Sphinx::version'),\n        ('', 'version'),\n        ('', 'List'),\n        ('', 'MyEnum')\n    ]\n    parenPatterns = [\n        ('ref function without parens ', r'paren_1\\(\\)'),\n        ('ref function with parens ', r'paren_2\\(\\)'),\n        ('ref function without parens, explicit title ', 'paren_3_title'),\n        ('ref function with parens, explicit title ', 'paren_4_title'),\n        ('ref op call without parens ', r'paren_5::operator\\(\\)\\(\\)'),\n        ('ref op call with parens ', r'paren_6::operator\\(\\)\\(\\)'),\n        ('ref op call without parens, explicit title ', 'paren_7_title'),\n        ('ref op call with parens, explicit title ', 'paren_8_title')\n    ]\n\n    f = 'roles.html'\n    t = (app.outdir / f).read_text()\n    for s in rolePatterns:\n        check(s, t, f)\n    for s in parenPatterns:\n        check(s, t, f)\n\n    f = 'any-role.html'\n    t = (app.outdir / f).read_text()\n    for s in parenPatterns:\n        check(s, t, f)\n\n", "def test_build_domain_cpp_templates_in_members(app, status, warning):\n    app.builder.build_all()\n\n    ws = filter_warnings(warning, \"templates-in-members\")\n    assert len(ws) == 0\n\n\n", "def test_build_domain_cpp_type_alias(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"type-alias\")\n    assert len(ws) == 0\n", "    def check(spec, text, file):\n        pattern = '<li><p>%s<a .*?><code .*?><span .*?>%s</span></code></a></p></li>' % spec\n        res = re.search(pattern, text)\n        if not res:\n            print(\"Pattern\\n\\t%s\\nnot found in %s\" % (pattern, file))\n            assert False", "    def test_build_domain_cpp_id_and_paren_attributes(app, status, warning):\n        app.builder.build_all()\n        ws = filter_warnings(warning, \"test-id-paren-attrs\")\n        assert len(ws) == 0\n"], "sample_157": ["    def test_clone_test_db(self):\n        db_creation = get_connection_copy().creation_class(get_connection_copy())\n        old_database_name = db_creation.connection.settings_dict['NAME']\n        with mock.patch.object(db_creation, '_clone_test_db'):\n            db_creation.clone_test_db(suffix='1')\n        self.assertEqual(\n            db_creation.get_test_db_clone_settings('1')['NAME'],\n            f'{old_database_name}_1',\n        )\n\n", "    def test_serialize_deserialize_objects(self):\n        # serialize_db_to_string() and deserialize_db_from_string()\n        # correctly serialize and deserialize objects.\n        obj1 = Object.objects.create(obj_ref=None)\n        obj2 = Object.objects.create(obj_ref=obj1)\n        obj_ref = ObjectReference.objects.create(obj=obj1)\n\n        data = connection.creation.serialize_db_to_string()\n        connection.creation.deserialize_db_from_string(data)\n\n        # Ensure the objects were recreated correctly.\n        obj1_from_db = Object.objects.first()\n        obj2_from_db = Object.objects.last()\n        obj_ref_from_db = ObjectReference.objects.first()\n        self.assertEqual(obj1_from_db.obj_ref, None)\n        self.assertEqual(obj2_from_db.obj_ref, obj1_from_db)\n        self.assertEqual(obj_ref_from_db.obj, obj1_from_db)\n\n", "    def test_clone_database_creation(self):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            creation.clone_test_db(suffix='_clone', verbosity=0,\n                                   autoclobber=True, keepdb=False)\n            \n            # Make sure the test database was cloned correctly\n            clone_name = '{}_{}'.format(old_database_name, '_clone')\n            cloned_settings = creation.get_test_db_clone_settings('_clone')\n            self.assertEqual(cloned_settings['NAME'], clone_name)\n        finally:\n            creation.destroy_test_db(old_database_name, verbosity=0)\n\n", "    def test_serialize_and_deserialize(self):\n        # Test that serialize_db_to_string() and\n        # deserialize_db_from_string() work together correctly.\n        Object.objects.create(obj_ref=None)\n        data = connection.creation.serialize_db_to_string()\n        connection.creation.deserialize_db_from_string(data)\n        self.assertEqual(Object.objects.count(), 1)\n", "    def test_clone_test_db(self):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_clone_test_db'):\n                creation.clone_test_db(suffix='clone1', verbosity=0, autoclobber=True, keepdb=False)\n            \n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n"], "sample_514": ["compilation error", "compilation error", "compilation error", "    def test_colorbar_no_label_if_no_title():\n        fig, ax = plt.subplots()\n        pc = ax.pcolormesh(np.random.randn(10, 10))\n        cb = fig.colorbar(pc)\n        assert cb.ax.get_ylabel() is None\n", "def test_colorbar_update_normal():\n    # test fix for #23141, make sure that an updated norm is reflected\n    # in the colorbar ticks and colors\n    fig, ax = plt.subplots()\n    im = ax.imshow([[0, 1], [2, 3]], cmap='viridis')\n    cb = fig.colorbar(im)\n    cb.mappable.set_norm(Normalize(vmin=0, vmax=2))\n    fig.canvas.draw()\n    np.testing.assert_allclose(cb.set_ticklocs(np.arange(0, 3)),\n                                  np.arange(0, 3))\n"], "sample_994": ["    def test_issue_11758():\n        x = Float('0.1', 25)\n        assert x.n(25) == '0.1000000000000000000000000'\n", "compilation error", "compilation error", "compilation error", "def test_Float_as_int():\n    assert int(Float('1.5')) == 1\n    assert int(Float('-1.5')) == -1\n    assert int(Float('2.9')) == 2\n    assert int(Float('-2.9')) == -2\n"], "sample_390": ["    def test_was_modified_since_invalid_header(self):\n        self.assertTrue(was_modified_since(header=\"invalid header\", mtime=1))\n", "    def test_directory_index_show_indexes_false(self):\n        response = self.client.get(\"/%s/subdir/\" % self.prefix, follow=True)\n        self.assertEqual(response.status_code, 404)\n", "    def test_was_modified_since_future_date(self):\n        mtime = 1343416141.107817\n        header = http_date(mtime + 3600)  # one hour in the future\n        self.assertTrue(was_modified_since(header, mtime))\n", "    def test_was_modified_since_future_date(self):\n        \"\"\"was_modified_since should return True for a future date.\"\"\"\n        mtime = 1343416141.107817\n        header = http_date(mtime + 86400)\n        self.assertTrue(was_modified_since(header, mtime))\n", "    def test_directory_index_missing_template(self):\n        with self.settings(TEMPLATE_DIRS=[]):\n            response = self.client.get(\"/%s/\" % self.prefix)\n            self.assertContains(response, \"Index of ./\")\n\n"], "sample_134": ["    def test_default_permissions_tuple(self):\n        self.assertSerializedEqual(\n            ('add', 'change', 'delete', 'view'),\n        )\n", "    def test_serialize_custom_field(self):\n        class CustomField(models.Field):\n            pass\n\n        migration = type(\"Migration\", (migrations.Migration,), {\n            \"operations\": [\n                migrations.AddField(\"MyModel\", field_name=\"custom_field\", field=CustomField()),\n            ]\n        })\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        self.assertIn(\"migrations.test_writer.CustomField\", output)\n\n", "    def test_serialize_custom_manager_with_path(self):\n        class MyCustomManager(models.Manager):\n                return super().get_queryset().filter(id__gt=5)\n\n        class MyModel(models.Model):\n            myfield = models.CharField(max_length=255)\n            objects = MyCustomManager()\n\n        migration = type(\"Migration\", (migrations.Migration,), {\n            \"operations\": [\n                migrations.CreateModel(\n                    \"MyModel\",\n                    (\n                        \"myfield\",\n                        models.CharField(max_length=255),\n                    ),\n                    options={\"verbose_name\": \"My model\"},\n                    bases=(models.Model,),\n                ),\n            ],\n        })\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        self.assertIn(\"MyCustomManager\", output)\n\n", "    def test_serialize_namedtuple(self):\n        Point = namedtuple('Point', ['x', 'y'])\n        point = Point(1, 2)\n        self.assertSerializedEqual(point)\n        self.assertSerializedResultEqual(\n            point,\n            (\"Point(x=1, y=2)\", {'from collections import namedtuple'}),\n        )\n", "    def test_serialize_namedtuples(self):\n        Point = collections.namedtuple('Point', ['x', 'y'])\n        point = Point(1, 2)\n        self.assertSerializedEqual(point)\n        string, imports = MigrationWriter.serialize(point)\n        self.assertEqual(string, \"migrations.test_writer.Point(x=1, y=2)\")\n        self.assertEqual(imports, {'import collections', 'from migrations.test_writer import Point'})\n"], "sample_104": ["    def test_manifest_strict_missing_file(self):\n        missing_file_name = 'cached/missing.css'\n        configured_storage = storage.staticfiles_storage\n        self.assertNotIn(missing_file_name, configured_storage.hashed_files)\n\n        #File name not found in manifest\n        with self.assertRaisesMessage(ValueError, \"Missing staticfiles manifest entry for '%s'\" % missing_file_name):\n            self.hashed_file_path(missing_file_name)\n\n        configured_storage.manifest_strict = False\n        # File doesn't exist on disk\n        err_msg = \"The file '%s' could not be found with %r.\" % (missing_file_name, configured_storage._wrapped)\n        with self.assertRaisesMessage(ValueError, err_msg):\n            self.hashed_file_path(missing_file_name)\n\n        content = StringIO()\n        content.write('Found')\n        configured_storage.save(missing_file_name, content)\n        # File exists on disk\n        self.hashed_file_path(missing_file_name)\n", "    def test_manifest_strict_raises_error(self):\n        storage.staticfiles_storage.manifest_strict = True\n\n        self._temp_dir = temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(temp_dir, 'test'))\n        self.addCleanup(shutil.rmtree, temp_dir)\n\n        with self.modify_settings(STATICFILES_DIRS={'append': self._temp_dir}):\n            finders.get_finder.cache_clear()\n            err = StringIO()\n            # First collectstatic run.\n            call_command('collectstatic', interactive=False, verbosity=0, stderr=err)\n\n            os.remove(os.path.join(self._temp_dir, 'test/foo.png'))\n\n            with self.assertRaises(ValueError):\n                call_command('collectstatic', interactive=False, verbosity=0, stderr=err)\n", "    def test_manifest_strict_missing_file(self):\n        missing_file_name = 'cached/missing.css'\n        configured_storage = storage.staticfiles_storage\n        configured_storage.manifest_strict = True\n        self.assertNotIn(missing_file_name, configured_storage.hashed_files)\n\n        # File name not found in manifest\n        with self.assertRaisesMessage(ValueError, \"Missing staticfiles manifest entry for '%s'\" % missing_file_name):\n            self.hashed_file_path(missing_file_name)\n\n\n", "    def test_manifest_strict_missing_file(self):\n        missing_file_name = 'cached/missing.css'\n        configured_storage = storage.staticfiles_storage\n        self.assertNotIn(missing_file_name, configured_storage.hashed_files)\n\n        # File name not found in manifest\n        with self.assertRaisesMessage(ValueError, \"Missing staticfiles manifest entry for '%s'\" % missing_file_name):\n            self.hashed_file_path(missing_file_name)\n\n        configured_storage.manifest_strict = False\n        # File doesn't exist on disk\n        err_msg = \"The file '%s' could not be found with %r.\" % (missing_file_name, configured_storage._wrapped)\n        with self.assertRaisesMessage(ValueError, err_msg):\n            self.hashed_file_path(missing_file_name)\n\n        content = StringIO()\n        content.write('Found')\n        configured_storage.save(missing_file_name, content)\n        # File exists on disk\n        self.hashed_file_path(missing_file_name)\n", "    def test_manifest_update_after_file_change(self):\n        # Create initial static files.\n        file_contents = (\n            ('foo.png', 'foo'),\n            ('bar.css', 'url(\"foo.png\")'),\n        )\n        for filename, content in file_contents:\n            with open(self._get_filename_path(filename), 'w') as f:\n                f.write(content)\n\n        with self.modify_settings(STATICFILES_DIRS={'append': self._temp_dir}):\n            finders.get_finder.cache_clear()\n            # First collectstatic run.\n            call_command('collectstatic', interactive=False, verbosity=0)\n\n            # Change the contents of the png file.\n            with open(self._get_filename_path('foo.png'), 'w+b') as f:\n                f.write(b\"new content of file to change its hash\")\n\n            # The manifest should be updated after the second collectstatic\n            # run to reflect the new hash of the changed file.\n            original_manifest = self.hashed_file_path('manifest.json')\n            call_command('collectstatic', interactive=False, verbosity=0)\n            self.assertNotEqual(\n                storage.staticfiles_storage.load_manifest(),\n                storage.staticfiles_storage.load_manifest(original_manifest)\n            )\n            \n\n"], "sample_399": ["    def test_aggregate_default_with_function(self):\n        result = Publisher.objects.aggregate(\n            avg_awards=Avg(\"num_awards\", default=F(\"num_awards\") + 1)\n        )\n        self.assertEqual(result[\"avg_awards\"], Decimal(\"2.5\"))\n\n", "    def test_aggregation_filter_with_subquery_comparison(self):\n        qs = Book.objects.filter(\n            publisher__in=Publisher.objects.annotate(\n                count=Count(\"book\")\n            ).filter(count__gt=2),\n        ).annotate(avg_price=Avg(\"price\")).filter(avg_price__gt=Decimal(\"20\"))\n\n        self.assertEqual(len(qs), 1)\n", "    def test_annotation_select_related(self):\n        qs = Author.objects.annotate(\n            count_books=Count(\"book\"),\n        ).select_related(\"contact\").order_by(\"id\")\n        self.assertSequenceEqual(\n            qs.values(\"name\", \"count_books\", \"contact__name\"),\n            [\n                {\"name\": \"Adrian Holovaty\", \"count_books\": 1, \"contact__name\": \"Jacob Kaplan-Moss\"},\n                {\"name\": \"Jacob Kaplan-Moss\", \"count_books\": 1, \"contact__name\": \"Adrian Holovaty\"},\n                {\"name\": \"Brad Dayley\", \"count_books\": 1, \"contact__name\": \"Jacob Kaplan-Moss\"},\n                {\"name\": \"James Bennett\", \"count_books\": 1, \"contact__name\": \"Jacob Kaplan-Moss\"},\n                {\"name\": \"Jeffrey Forcier\", \"count_books\": 1, \"contact__name\": \"Jacob Kaplan-Moss\"},\n                {\"name\": \"Paul Bissex\", \"count_books\": 1, \"contact__name\": \"Jacob Kaplan-Moss\"},\n                {\"name\": \"Wesley J. Chun\", \"count_books\": 1, \"contact__name\": \"Jacob Kaplan-Moss\"},\n                {\"name\": \"Stuart Russell\", \"count_books\": 1, \"contact__name\": \"Peter Norvig\"},\n                {\"name\": \"Peter Norvig\", \"count_books\": 2, \"contact__name\": \"Stuart Russell\"},\n            ],\n        )\n", "    def test_aggregation_default_using_array_agg(self):\n        result = Book.objects.annotate(\n            authors_names=ArrayAgg(F('authors__name'))\n        ).aggregate(\n            author_names=Sum(\n                ArrayLength(F('authors_names'), default=0),\n            ),\n        )\n        self.assertEqual(result['author_names'], 10)\n\n\n\n", "    def test_aggregation_default_with_subquery(self):\n        authors = Author.objects.annotate(\n            book_count=Count(\"book\"),\n            avg_rating=Avg(\n                Case(\n                    When(book__rating__isnull=False, then=F(\"book__rating\")),\n                    default=Value(0),\n                ),\n                output_field=DecimalField(),\n            ),\n        ).filter(book_count__gt=1)\n        subquery = (\n            authors.values(\"avg_rating\")\n            .annotate(\n                min_rating=Min(\"avg_rating\", default=Value(0.0), output_field=DecimalField())\n            )\n            .values(\"min_rating\")\n        )\n        result = Publisher.objects.aggregate(\n            min_avg_rating=Subquery(subquery).first()\n        )\n\n        self.assertAlmostEqual(result[\"min_avg_rating\"], Decimal(\"3.17\"), places=2)\n"], "sample_685": ["def test_capture_exception_in_report(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n            logger = logging.getLogger(__name__)\n\n            try:\n                raise ValueError(\"something bad happened\")\n            except ValueError:\n                logger.exception(\"Caught exception\")\n\n            assert \"Caught exception\" in caplog.text\n            assert \"ValueError\" in caplog.text\n            assert \"something bad happened\" in caplog.text\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*Caught exception*\", \"*ValueError*\", \"*something bad happened*\"])\n    assert result.ret == 1\n", "def test_log_report_captures_according_to_config_option_upon_success(testdir):\n    \"\"\" Test that upon success:\n    (1) `caplog` succeeded to capture the DEBUG message and assert on it => No `Exception` is raised\n    (2) The `DEBUG` message DOES appear in the `Captured log call` report\n    (3) The stdout, `INFO`, and `WARNING` messages DO appear in the test reports due to `--log-level=INFO`\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n            logging.debug('DEBUG log ' + 'message')\n            logging.info('INFO log ' + 'message')\n            logging.warning('WARNING log ' + 'message')\n            print('Print ' + 'message')\n\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_level == logging.INFO\n\n            with caplog.at_level(logging.DEBUG):\n                function_that_logs()\n\n            if 'DEBUG log ' + 'message' not in caplog.text:\n                raise Exception('caplog failed to ' + 'capture DEBUG')\n\n            assert True\n    \"\"\"\n    )\n\n    result = testdir.runpytest(\"--log-level=INFO\")\n    result.stdout.no_fnmatch_line(\"*Exception: caplog failed to capture DEBUG*\")\n    result.stdout.fnmatch_lines(\n        [\"*DEBUG log message*\", \"*Print message*\", \"*INFO log message*\", \"*WARNING log message*\"]\n    )\n    assert result.ret == 0\n\n", "def test_log_report_captures_according_to_config_option_upon_success(testdir):\n    \"\"\" Test that upon success:\n    (1) The `Captured log call` report DOES contain all messages from `INFO` and above\n    (2) The 'DEBUG' message is NOT present in the `Captured log call` report\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n            logging.debug('DEBUG log ' + 'message')\n            logging.info('INFO log ' + 'message')\n            logging.warning('WARNING log ' + 'message')\n            print('Print ' + 'message')\n\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_level == logging.INFO\n\n            with caplog.at_level(logging.DEBUG):\n                function_that_logs()\n\n            assert True\n    \"\"\"\n    )\n\n    result = testdir.runpytest(\"--log-level=INFO\")\n    result.stdout.fnmatch_lines(\n        [\"*Print message*\", \"*INFO log message*\", \"*WARNING log message*\"]\n    )\n    result.stdout.no_fnmatch_line(\"*DEBUG log message*\")\n    assert result.ret == 0\n", "    def test_log_report_captures_according_to_config_option_upon_success(testdir):\n        \"\"\"\n        Test that upon success:\n        (1) `caplog` successfully captures all log levels according to `--log-level=DEBUG`.\n        (2) All messages appear in the `Captured log call` report.\n        \n        \"\"\"\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            import logging\n\n                logging.debug('DEBUG log ' + 'message')\n                logging.info('INFO log ' + 'message')\n                logging.warning('WARNING log ' + 'message')\n                print('Print ' + 'message')\n                assert True\n\n        \"\"\"\n        )\n\n        result = testdir.runpytest(\"--log-level=DEBUG\")\n        result.stdout.fnmatch_lines(\n            [\"*DEBUG log message*\", \"*INFO log message*\", \"*WARNING log message*\"]\n        )\n        assert result.ret == 0\n", "def test_log_report_captures_according_to_cli_level(testdir):\n    \"\"\" Test that upon success:\n    (1) `caplog` succeeded to capture the DEBUG message and assert on it\n    (2) The `DEBUG` message DOES appear in the `Captured log call` report\n    (3) The stdout, `INFO`, and `WARNING` messages DO appear in the test reports\n    due to `--log-cli-level=DEBUG`\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n            logging.debug('DEBUG log ' + 'message')\n            logging.info('INFO log ' + 'message')\n            logging.warning('WARNING log ' + 'message')\n            print('Print ' + 'message')\n\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_level == logging.INFO\n\n            function_that_logs()\n\n            assert 'DEBUG log message' in caplog.text\n\n    \"\"\"\n    )\n\n    result = testdir.runpytest(\"--log-cli-level=DEBUG\")\n    result.stdout.fnmatch_lines(\n        [\"*DEBUG log message*\", \"*INFO log message*\", \"*WARNING log message*\", \"*Print message*\"]\n    )\n    assert result.ret == 0\n"], "sample_1194": ["def test_julia_matrix_times_scalar():\n    A = Matrix([[1, 2, 3], [4, 5, 6]]).T\n    assert julia_code(2*A) == \"2 * A\"\n    assert julia_code(A/2) == \"A / 2\"\n", "compilation error", "compilation error", "compilation error", "def test_IndexedBase():\n    from sympy.core.symbol import IndexedBase\n    from sympy.core.numbers import Integer\n    i = Symbol('i', integer=True)\n    A = IndexedBase('A', shape=(5,))\n    assert julia_code(A[i]) == \"A[i + 1]\"\n    assert julia_code(A[Integer(2)]) == \"A[3]\"\n\n"], "sample_520": ["def test_Poly3DCollection_get_alpha():\n    # Smoke test to see that get_alpha does not raise\n    # See GH#4067\n    y, x = np.ogrid[1:10:100j, 1:10:100j]\n    z2 = np.cos(x) ** 3 - np.sin(y) ** 2\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    r = ax.plot_surface(x, y, z2, cmap='hot')\n    r.get_alpha()\n", "def test_set_aspect():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.plot([0,1],[0,1],[0,1])\n    ax.set_aspect('equal')\n    ax.figure.canvas.draw()\n", "    def test_poly3dcollection_alpha(self):\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection='3d')\n        verts = [(0, 0, 0), (1, 0, 0), (0, 1, 0), (0, 0, 1)]\n        facecolor = [1, 0, 0, 0.5]\n        poly = art3d.Poly3DCollection([verts], facecolor=facecolor)\n        ax.add_collection3d(poly)\n        ax.set_xlabel('X')\n        ax.set_ylabel('Y')\n        ax.set_zlabel('Z')\n", "def test_bar3d_alpha():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    for i in range(10):\n      for j in range(10):\n          x = i\n          y = j\n          z = i\n          dx = 0.5\n          dy = 0.5\n          dz = np.random.rand() * i\n          ax.bar3d([x], [y], [z], [dx], [dy], [dz],\n                   color=plt.cm.jet(dz / 10), alpha=dz / 10, zsort='average')\n", "def test_plot_surface_faceted(fig_test, fig_ref):\n    # Test that 'faceted' plot_surface argument works\n    x = np.arange(-5, 5, 0.25)\n    y = np.arange(-5, 5, 0.25)\n    X, Y = np.meshgrid(x, y)\n    Z = np.sin(np.sqrt(X**2 + Y**2))\n\n    ax_ref = fig_ref.add_subplot(projection='3d')\n    ax_ref.plot_surface(X, Y, Z, rstride=1, cstride=1, faceted=False)\n\n    ax_test = fig_test.add_subplot(projection='3d')\n    ax_test.plot_surface(X, Y, Z, rstride=1, cstride=1, faceted=True)\n"], "sample_16": ["    def test_array_creation(self, func):\n        q = func((2, 3), dtype=u.m)\n        assert_array_equal(q.value, func((2, 3)))\n        assert q.unit == u.m\n", "    def test_merge_arrays_error(self):\n        with pytest.raises(ValueError, match=\"Must provide at least one array\"):\n            rfn.merge_arrays()\n", "    def test_ufunc(self):\n        with pytest.raises(TypeError):\n            np.add.reduce(self.q)\n", "    def test_structured_to_unstructured_with_different_units(self):\n        # Test with different units in a structured array, but convertible\n        dt = np.dtype([('x', 'm'), ('y', 'km')])\n        arr = np.array([(1, 2.0)], dtype=dt) << u.Unit(\"m, km\")\n        result = rfn.structured_to_unstructured(arr)\n\n        # Make sure it's converted to consistent units\n        assert_array_equal(result, [1, 2000] * u.m)\n", "    def test_merge_arrays_mixed_dtypes(self, flatten):\n        # Merge arrays with different dtypes.\n        q1 = u.Quantity([(1, 2)], dtype=[(\"f1\", float), (\"f2\", int)])\n        q2 = u.Quantity([(3, 4)], dtype=[(\"f3\", float), (\"f4\", int)])\n        arr = rfn.merge_arrays((q1, q2), flatten=flatten)\n        assert arr.dtype == [(\"f1\", float), (\"f2\", int), (\"f3\", float), (\"f4\", int)]\n        assert_array_equal(arr[\"f1\"], [1, 3])\n        assert_array_equal(arr[\"f2\"], [2, 4])\n"], "sample_1084": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_200": ["    def test_fail_silently_on_send_error(self):\n        \"\"\"\n        A sending error is silenced with fail_silently=True.\n        \"\"\"\n        self.backend.fail_silently = True\n        email = EmailMessage('Subject', 'Content', 'from@example.com', ['to@example.com'])\n        self.assertEqual(self.backend.send_messages([email]), 0)\n", "    def test_email_tls_certificate_verification(self):\n        \"\"\"Test that TLS certificate verification can be disabled.\"\"\"\n        backend = smtp.EmailBackend(use_tls=True, verify_certs=False)\n        self.assertFalse(backend.verify_certs)\n\n", "    def test_email_multi_recipient(self):\n       \"\"\"Test sending an email to multiple recipients.\"\"\"\n       backend = smtp.EmailBackend(username='', password='')\n       email = EmailMessage('Subject', 'Content', 'from@example.com', ['to1@example.com', 'to2@example.com'])\n       sent = backend.send_messages([email])\n       self.assertEqual(sent, 1)\n       self.assertEqual(len(self.server.get_sink()), 1)\n       message = self.server.get_sink()[0]\n       self.assertIn('to1@example.com', message.get('to'))\n       self.assertIn('to2@example.com', message.get('to'))\n", "    def test_connection_error_without_fail_silently(self):\n        \"\"\"\n        A socket connection error is raised if fail_silently=False.\n        \"\"\"\n        self.backend.fail_silently = False\n        with self.assertRaises(ConnectionError):\n            self.backend.open()\n", "    def test_email_with_unicode_subject_and_body(self):\n        \"\"\"\n        Test sending an email with unicode characters in the subject and body.\n        \"\"\"\n        subject = '\u3053\u3093\u306b\u3061\u306f\u3001\u4e16\u754c\uff01'\n        body = 'This is a message with unicode characters: \u4e16\u754c'\n        email = EmailMessage(subject, body, 'from@example.com', ['to@example.com'])\n        send_mail(subject, body, 'from@example.com', ['to@example.com'])\n        message = self.get_the_message()\n        self.assertEqual(message.get('subject'), subject)\n        self.assertEqual(message.get_payload(), body)\n"], "sample_1022": ["compilation error", "compilation error", "    def test_convert_equals_signs():\n        cases = {\n            '1=2': 'Eq(1, 2)',\n            '1*2=x': 'Eq(2, x)',\n            '(1*2=x)=False': 'Eq(Eq(2, x), False)',\n            'x=(1*2)=y': 'Eq(x, Eq(2, y))',\n            '1=2=3': 'Eq(Eq(1, 2), 3)',\n            'x=y=z': 'Eq(x, Eq(y, z))',\n        }\n        transformations = standard_transformations + (convert_xor,)\n        transformations2 = transformations + (convert_equals_signs,)\n        for case, expected in cases.items():\n            implicit = parse_expr(case, transformations=transformations2)\n            normal = parse_expr(expected, transformations=transformations)\n            assert implicit == normal\n", "    def test_rationalize():\n        transformations = standard_transformations + (convert_xor, rationalize)\n        cases = {\n            '1.2': 'Float(1.2)',\n            '1.2*x': 'Float(1.2)*x',\n            'x/1.2': 'x/Float(1.2)',\n            '1/1.2': '1/Float(1.2)',\n            '.5': 'Float(0.5)',\n            '0.5': 'Float(0.5)',\n            '1.0': 'Float(1.0)',\n            '1e-4': 'Float(0.0001)',\n            '-1.2': '-Float(1.2)',\n            \"1.2e-3\": 'Float(0.0012)',\n            '1.2e3': 'Float(1200)',\n            '1.2j': 'Float(1.2)*I',\n        }\n\n        for case, expected in cases.items():\n            assert(parse_expr(case, transformations=transformations) ==\n                   parse_expr(expected))\n\n", "    def test_repeated_decimals():\n        cases = {\n            '0.2[1]': '19/90',\n            '1.2[34]': '1234/990',\n            '0.[123]': '123/999',\n            '1.[234]': '1234/999',\n        }\n        transformations = standard_transformations + (repeated_decimals,\n                                                     convert_xor)\n        transformations2 = transformations + (implicit_multiplication_application,)\n        for case in cases:\n            implicit = parse_expr(case, transformations=transformations2)\n            normal = parse_expr(cases[case], transformations=transformations)\n            assert(implicit == normal)\n"], "sample_726": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1162": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_694": ["    def test_marked_fixure_is_deprecated(pytester: Pytester) -> None:\n\n", "def test_nose_deprecated_with_setup_teardown(pytester: Pytester) -> None:\n    pytest.importorskip(\"nose\")\n    pytester.makepyfile(\n        \"\"\"\n        from nose.tools import setup, teardown\n\n        @setup\n            ...\n\n        @teardown\n            ...\n\n            ...\n        \"\"\"\n    )\n    output = pytester.runpytest(\"-Wdefault::pytest.PytestRemovedIn8Warning\")\n    message = [\n        \"*PytestRemovedIn8Warning: Support for nose tests is deprecated and will be removed in a future release.\",\n        \"*test_nose_deprecated_with_setup_teardown.py::test_using_setup_teardown is using nose method: `setup_fn` (setup)\",\n        \"*PytestRemovedIn8Warning: Support for nose tests is deprecated and will be removed in a future release.\",\n        \"*test_nose_deprecated_with_setup_teardown.py::test_using_setup_teardown is using nose method: `teardown_fn` (teardown)\",\n    ]\n    output.stdout.fnmatch_lines(message)\n    output.assert_outcomes(passed=1)\n", "def test_deprecation_hookimpl(pytester: Pytester):\n    from _pytest.config import PytestPluginManager\n    from _pytest.compat import legacy_path\n\n    pm = PytestPluginManager()\n\n    class DeprecatedMarkImplPlugin:\n            pass\n\n        pytest_runtest_call.tryfirst = True  # type: ignore[attr-defined]\n\n    with pytest.warns(pytest.PytestDeprecationWarning) as recorder:\n        pm.register(DeprecatedMarkImplPlugin())\n    deprecated_warning = (\n        \"Please use the pytest.hookimpl(tryfirst=True) decorator instead\\n. \"\n    )\n\n    assert len(recorder) == 1\n    assert deprecated_warning in str(recorder[0].message)\n\n\n", "    def test_hookspec_via_function_attributes_are_deprecated_with_historic(pytester: Pytester) -> None:\n        from _pytest.config import PytestPluginManager\n\n        pm = PytestPluginManager()\n\n        class DeprecatedHookMarkerSpec:\n                pass\n\n            pytest_bad_hook.historic = True  # type: ignore[attr-defined]\n\n        with pytest.warns(\n            PytestDeprecationWarning,\n            match=r\"Please use the pytest\\.hookspec\\(historic=True\\) decorator\",\n        ) as recorder:\n            pm.add_hookspecs(DeprecatedHookMarkerSpec)\n        (record,) = recorder\n        assert record.lineno == DeprecatedHookMarkerSpec.pytest_bad_hook.__code__.co_firstlineno\n        assert record.filename == __file__\n", "    def test_hookspec_via_function_attributes_are_deprecated_with_historic(self):\n        from _pytest.config import PytestPluginManager\n\n        pm = PytestPluginManager()\n\n        class DeprecatedHookMarkerSpec:\n                pass\n\n            pytest_bad_hook.historic = True  # type: ignore[attr-defined]\n\n        with pytest.warns(\n            PytestDeprecationWarning,\n            match=r\"Please use the pytest\\.hookspec\\(historic=True\\) decorator\",\n        ) as recorder:\n            pm.add_hookspecs(DeprecatedHookMarkerSpec)\n        (record,) = recorder\n        assert (\n            record.lineno\n            == DeprecatedHookMarkerSpec.pytest_bad_hook.__code__.co_firstlineno\n        )\n        assert record.filename == __file__\n\n"], "sample_1049": ["compilation error", "compilation error", "compilation error", "    def test_arbitrary_point():\n        pl = Plane((1, 2, 3), (4, 5, 6))\n        assert pl.arbitrary_point(t) == Point3D(\n            1 + 4*t, 2 + 5*t, 3 + 6*t)\n        assert pl.arbitrary_point(u, v) == Point3D(\n            1 + 4*u, 2 + 5*v, 3 + 6*u)\n", "compilation error"], "sample_790": ["    def test_kernel_pca_bad_n_components():\n        rng = np.random.RandomState(0)\n        X_fit = rng.random_sample((5, 4))\n        kpca = KernelPCA(n_components=-1)\n        assert_raises(ValueError, kpca.fit, X_fit)\n", "    def test_kernel_pca_n_components_vs_eigenvalues(self):\n        rng = np.random.RandomState(0)\n        X = rng.random_sample((10, 3))\n        kpca = KernelPCA(n_components=2, kernel='linear')\n        kpca.fit(X)\n        assert_equal(len(kpca.lambdas_), 3)\n        assert_equal(kpca.n_components, 2)\n        assert_equal(sum(kpca.lambdas_ > 0), 2)\n", "    def test_kernelpca_with_precomputed_kernel_and_n_components_None():\n        rng = np.random.RandomState(0)\n        X_fit = rng.random_sample((5, 4))\n        X_pred = rng.random_sample((2, 4))\n        kernel = rbf_kernel(X_fit, X_fit)\n        kpca = KernelPCA(n_components=None, kernel='precomputed')\n        kpca.fit(kernel)\n        X_kpca = kpca.transform(kernel)\n        assert X_kpca.shape[1] <= X_fit.shape[0] and X_kpca.shape[1] >= 1\n\n", "    def test_kernel_pca_bad_kernel_params():\n        rng = np.random.RandomState(0)\n        X_fit = rng.random_sample((5, 4))\n        kpca = KernelPCA(kernel=\"poly\", gamma=1, degree='a', coef0='b')\n        assert_raises(TypeError, kpca.fit, X_fit) \n", "    def test_kernel_pca_inverse_transform_exact_match():\n        # Test if the inverse transform reconstructs the original data\n        # exactly when fit_inverse_transform is True.\n\n        rng = np.random.RandomState(0)\n        X = rng.random_sample((10, 5))\n\n        for kernel in (\"linear\", \"rbf\") :  \n            kpca = KernelPCA(n_components=3, kernel=kernel,\n                             fit_inverse_transform=True)\n\n            X_transformed = kpca.fit_transform(X)\n            X_recovered = kpca.inverse_transform(X_transformed)\n            assert_array_almost_equal(X, X_recovered)\n"], "sample_1061": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_88": ["    def test_smtp_connection_close(self):\n        \"\"\"\n        The connection is closed when the context manager exits.\n        \"\"\"\n        self.backend.open()\n        self.assertIsNotNone(self.backend.connection)\n        with self.backend:\n            pass\n        self.assertIsNone(self.backend.connection)\n", "    def test_email_multiple_messages(self):\n        \"\"\"Sending multiple messages in a single call should work correctly.\"\"\"\n        email1 = EmailMessage('Subject1', 'Content1', 'from@example.com', ['to@example.com'])\n        email2 = EmailMessage('Subject2', 'Content2', 'from@example.com', ['to@example.com'])\n        self.backend.send_messages([email1, email2])\n        self.assertEqual(len(self.backend.server.get_sink()), 2)\n", "    def test_multiple_connections(self):\n        \"\"\"Multiple connections to the same SMTP server should work.\"\"\"\n        backend1 = smtp.EmailBackend()\n        backend2 = smtp.EmailBackend()\n        backend1.open()\n        backend2.open()\n        backend1.close()\n        backend2.close()\n", "    def test_email_authentication_empty_username_password(self):\n        backend = smtp.EmailBackend(username='', password='')\n        self.assertEqual(backend.username, '')\n        self.assertEqual(backend.password, '')\n", "    def test_email_from_header_without_local_part(self):\n        \"\"\"\n        Emails sent without a local part in the From address shouldn't cause errors.\n        \"\"\"\n        send_mail('Subject', 'Content', '@example.com', ['to@example.com'])\n        message = self.get_the_message()\n        self.assertEqual(message.get('from'), '@example.com')\n"], "sample_1073": ["compilation error", "def test_sqrtdenest_numeric():\n    assert sqrtdenest(sqrt(1 + sqrt(2) + sqrt(3))) == sqrt(1 + sqrt(2) + sqrt(3))\n\n", "compilation error", "compilation error", "compilation error"], "sample_169": ["    def test_double_nested_key_transform(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__has_key=KeyTransform('x', KeyTransform('y', 'value')),\n            ),\n            [self.objs[7]],\n        )\n", "    def test_key_transform_with_f_expression(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__foo=KeyTransform(\n                'bar',\n                Cast('baz', models.JSONField()),\n            )).exists(),\n            True,\n        )\n", "    def test_key_transform_with_f_expression(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__has_key=KeyTransform('x', F('value__y')),\n            ).values_list('id'),\n            [self.objs[7]],\n        )\n\n", "    def test_key_transform_expression_complex(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__d__0__isnull=False\n            ).annotate(\n                key=KeyTransform('d', 'value'),\n                chain=KeyTransform('1', Cast(KeyTransform('f', KeyTransform('0', 'key')), models.JSONField())),\n                expr=KeyTransform('f', KeyTransform('1', Cast('key', models.JSONField()))),\n            ).filter(chain=F('expr')),\n            [self.objs[4]],\n        )\n", "    def test_deep_key_transform_sql(self):\n        qs = NullableJSONModel.objects.annotate(\n            deep_key=KeyTransform('f', KeyTransform('1', KeyTransform('d', 'value'))),\n        ).filter(deep_key='g')\n        if connection.vendor == 'postgresql':\n            self.assertIn('->> \\'d\\'->> 1 ->> \\'f\\' AS \"deep_key\"', str(qs.query))\n        elif connection.vendor == 'mysql':\n            self.assertIn('JSON_EXTRACT(JSON_EXTRACT(JSON_EXTRACT(value, \\'$.d\\'), \\'$[1]\\'), \\'$.f\\') AS \"deep_key\"', str(qs.query))\n        else:\n            self.assertIn(\n                'value->>\\'d\\'->>[1]->>\\'f\\' AS \"deep_key\"', str(qs.query)\n            )\n        self.assertSequenceEqual(qs, [self.objs[4]])\n\n"], "sample_851": ["    def test_tweedie_deviance_zero_values():\n        y_true = np.array([1, 0, 1, 0])\n        y_pred = np.array([0.5, 0.5, 1, 1])\n\n        with pytest.raises(ValueError):\n            mean_tweedie_deviance(y_true, y_pred, power=1) \n        with pytest.raises(ValueError):\n            mean_tweedie_deviance(y_true, y_pred, power=2) \n", "compilation error", "    def test_tweedie_deviance_at_boundary():\n        # Edge cases for Tweedie deviance\n        y_true = np.array([1.])\n        y_pred = np.array([1.])\n        assert mean_tweedie_deviance(y_true, y_pred, power=-1) == 0\n        assert mean_tweedie_deviance(y_true, y_pred, power=0) == 0\n        assert mean_tweedie_deviance(y_true, y_pred, power=1) == 0\n        assert mean_tweedie_deviance(y_true, y_pred, power=2) == 0\n\n        y_pred = np.array([0.5])\n        assert mean_tweedie_deviance(y_true, y_pred, power=-1) == 2\n        assert mean_tweedie_deviance(y_true, y_pred, power=0) == 0\n        assert mean_tweedie_deviance(y_true, y_pred, power=1) > 0\n        assert mean_tweedie_deviance(y_true, y_pred, power=2) == np.log(2)\n", "compilation error", "    def test_tweedie_deviance_edge_cases():\n        y_true = np.array([1, 1, 1, 1])\n        y_pred = np.array([0.5, 1.5, 1, 1])\n        assert_allclose(mean_tweedie_deviance(y_true, y_pred, power=0),\n                        mean_squared_error(y_true, y_pred))\n        assert_allclose(mean_tweedie_deviance(y_true, y_pred, power=1),\n                        mean_poisson_deviance(y_true, y_pred))\n        assert_allclose(mean_tweedie_deviance(y_true, y_pred, power=2),\n                        mean_gamma_deviance(y_true, y_pred))\n"], "sample_446": ["compilation error", "    def test_floatformat_large_numbers(self):\n        self.assertEqual(floatformat(12345678901234567890.12345678901234567890, 2),\n                         \"12345678901234567890.12\")\n", "    def test_floatformat_with_none_decimal_precision_should_return_string_representation(self):\n        self.assertEqual(floatformat(-1.2345, None), \"-1.2345\")\n        self.assertEqual(floatformat(1.2345, None), \"1.2345\")\n", "    def test_rounding_issues(self):\n        self.assertEqual(floatformat(Decimal('1.2345'), 2), '1.23')\n        self.assertEqual(floatformat(Decimal('1.2355'), 2), '1.24')\n", "    def test_floatformat_with_scientific_notation(self):\n        self.assertEqual(floatformat(1.2345e6, 2), \"1234500.00\")\n        self.assertEqual(floatformat(1.2345e-6, 2), \"0.00\")\n        self.assertEqual(floatformat(12345e6, \"2\"), \"1.23e+07\")\n        self.assertEqual(floatformat(12345e-6, \"-2\"), \"0.00\")\n"], "sample_842": ["def test_kernel_addition():\n    # Test addition of kernels.\n    kernel1 = RBF(length_scale=1.0)\n    kernel2 = 2.0 * Matern(length_scale=1.0)\n    kernel_sum = kernel1 + kernel2\n    K1 = kernel1(X)\n    K2 = kernel2(X)\n    K_sum = kernel_sum(X)\n    assert_array_almost_equal(K_sum, K1 + K2)\n", "def test_kernel_hyperparameters_immutable(kernel):\n    # Test that kernel hyperparameters are immutable after instantiation.\n\n    # Make a copy of the initial hyperparameters.\n    initial_params = kernel.get_params()\n\n    # Attempt to modify the hyperparameters directly.\n    try:\n        kernel.theta = np.random.rand(*kernel.theta.shape)\n        assert False  # Should not reach this point.\n    except AttributeError:\n        pass\n\n    # Check if the hyperparameters remain unchanged.\n    assert kernel.get_params() == initial_params\n", "    def test_kernel_hyperparameter_bounds():\n        kernel = RBF(length_scale=1.0, length_scale_bounds=(0.5, 2.0))\n        assert kernel.hyperparameters[0].bounds == (0.5, 2.0)\n        kernel = RBF(length_scale=1.0)\n        assert kernel.hyperparameters[0].bounds == (1e-5, 1e5)\n\n", "    def test_kernel_hyperparameter_bounds(self):\n        # Test setting bounds for kernel hyperparameters.\n        kernel = RBF(length_scale=1.0, length_scale_bounds=(0.1, 10.0))\n        assert kernel.length_scale_bounds == (0.1, 10.0)\n        \n        with pytest.raises(ValueError):\n            kernel = RBF(length_scale=1.0, length_scale_bounds=(10.0, 0.1))\n\n        kernel = RBF(length_scale=[1.0, 2.0], length_scale_bounds=[(0.1, 10.0), (0.5, 5.0)])\n        assert kernel.length_scale_bounds == [(0.1, 10.0), (0.5, 5.0)]\n\n        with pytest.raises(ValueError):\n            kernel = RBF(length_scale=[1.0, 2.0], length_scale_bounds=[(10.0, 0.1), (0.5, 5.0)])\n", "def test_kernel_negative_length_scale(kernel):\n    # Check if setting negative length scale raises a ValueError\n    if hasattr(kernel, 'length_scale'):\n        with pytest.raises(ValueError):\n            kernel.set_params(length_scale=-1)\n\n"], "sample_1068": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1166": ["def test_Monomial_eq():\n    m = Monomial((3, 4, 1), (x, y, z))\n    n = Monomial((3, 4, 1), (x, y, z))\n    p = Monomial((1, 2, 0), (x, y, z))\n\n    assert m == n\n    assert m != p\n    assert n != p\n", "compilation error", "def test_Monomial_from_expr():\n    m = Monomial.from_expr(x*y**2*z**3)\n    assert m.exponents == (1, 2, 3)\n    assert m.gens == (x, y, z)\n\n    n = Monomial.from_expr(a**2*b)\n    assert n.exponents == (2, 1)\n    assert n.gens == (a, b)\n\n    raises(ValueError, lambda: Monomial.from_expr(1))\n    raises(ValueError, lambda: Monomial.from_expr(\"x+y\"))\n", "def test_Monomial_hashing():\n    m1 = Monomial((3, 4, 1), (x, y, z))\n    m2 = Monomial((3, 4, 1), (x, y, z))\n    m3 = Monomial((1, 2, 0), (x, y, z))\n\n    assert hash(m1) == hash(m2)\n    assert hash(m1) != hash(m3)\n", "def test_Monomial_empty_gens():\n    m = Monomial((3,4,1))\n    assert m.gens is None\n    assert m.as_expr() ==  m.exponents[0]*m.exponents[1]**m.exponents[2]\n\n"], "sample_499": ["def test_legend_duplicate_labels():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='A')\n    ax.plot([2, 3], [4, 5], label='A')\n    with pytest.warns(UserWarning):\n        ax.legend()\n", "    def test_legend_with_image():\n        fig, axs = plt.subplots(1, 2)\n        axs[0].plot([1, 2], [3, 4], label='line')\n        axs[1].imshow(np.random.rand(10, 10))\n        fig.legend()\n", "compilation error", "def test_legend_patch_artist():\n    fig, ax = plt.subplots()\n    patch = mpl.patches.Rectangle((0, 0), 1, 1, facecolor='red')\n    ax.add_patch(patch)\n    ax.legend([patch], ['patch'])\n    assert len(ax.get_legend().get_patches()) == 1\n", "def test_legend_facecolor_none():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label=\"line\", color='blue')\n    leg = ax.legend(facecolor='none', edgecolor='red')\n    assert leg.get_facecolor() == mpl.rcParams['axes.facecolor']\n"], "sample_156": ["    def test_form_with_only_hidden_fields(self):\n        class HiddenForm(Form):\n            field1 = CharField(widget=HiddenInput)\n            field2 = IntegerField(widget=HiddenInput)\n\n        form = HiddenForm({'field1': 'value1', 'field2': '2'})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data, {'field1': 'value1', 'field2': 2})\n", "    def test_custom_error_class(self):\n        class CustomErrorList(ErrorList):\n                return 'Custom Error: ' + super().__str__()\n\n        class CustomForm(Form):\n            name = CharField()\n            \n            error_class = CustomErrorList \n\n        form = CustomForm({'name': ''})\n        self.assertHTMLEqual(\n            str(form.errors),\n            'Custom Error: <ul class=\"errorlist\"><li>This field is required.</li></ul>'\n        )\n\n\n\n", "    def test_field_error_messages_on_form_level(self):\n        class CustomForm(Form):\n            field1 = CharField()\n\n                raise ValidationError('This is a form-level error.')\n\n        form = CustomForm({'field1': 'some data'})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors, {'__all__': ['This is a form-level error.']})\n", "    def test_form_field_rendering_with_custom_renderer(self):\n        class CustomForm(Form):\n            name = CharField()\n\n        class CustomRenderer(DjangoTemplates):\n                return 'Custom rendering with: %s' % kwargs['form_data']\n\n        form = CustomForm({'name': 'John'})\n        form.renderer = CustomRenderer()\n        rendered = form.as_p()\n        self.assertEqual(rendered, 'Custom rendering with: {\"name\": [\"John\"]}')\n", "    def test_form_with_no_fields(self):\n        class EmptyForm(Form):\n            pass\n\n        form = EmptyForm()\n        self.assertEqual(form.errors, {})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data, {})\n"], "sample_410": ["    def test_get_group_permissions(self):\n        user = User.objects.create_user(username=\"test_user\", password=\"test_password\")\n        group = Group.objects.create(name=\"test_group\")\n        permission = Permission.objects.create(codename=\"test_permission\", name=\"Test Permission\", content_type=ContentType.objects.get_for_model(User))\n        group.permissions.add(permission)\n        user.groups.add(group)\n        self.assertIn(permission, user.get_group_permissions()) \n", "    def test_permission_codename_unique(self):\n        with self.assertRaises(IntegrityError):\n            Permission.objects.create(\n                content_type=ContentType.objects.get_for_model(User),\n                codename=\"view_user\",\n                name=\"View User\",\n            )\n", "    def test_permissions_with_related_object(self):\n        user = User.objects.create_user(username=\"testuser\", password=\"password\")\n        group = Group.objects.create(name=\"testgroup\")\n        group.permissions.add(self.permission)\n        user.groups.add(group)\n        self.assertTrue(user.has_perm(\"auth.test\"))\n", "    def test_permission_representation(self):\n        p = Permission.objects.create(\n            name=\"Can delete test permission\",\n            codename=\"delete_test\",\n            content_type=ContentType.objects.get_for_model(User),\n        )\n        self.assertEqual(\n            p.natural_key(),\n            ('delete_test', 'auth', 'user'),\n        )\n\n", "    def test_user_groups_permissions(self):\n        user = User.objects.create_user(\"user\", \"user@example.com\")\n        group = Group.objects.create(name=\"test_group\")\n        permission = Permission.objects.get(codename=\"add_group\")\n        group.permissions.add(permission)\n        user.groups.add(group)\n        self.assertTrue(user.has_perm(permission.codename))\n"], "sample_840": ["    def test_pls_empty_y():\n        d = load_linnerud()\n        X = d.data\n        Y = d.target\n        Y = Y[:, :0]  # Empty array\n        pls = pls_.PLSRegression()\n        assert_raises(ValueError, pls.fit, X, Y)\n", "compilation error", "    def test_pls_multioutput_error():\n        # Check if error raised when using multioutput=False with multioutput target\n        d = load_linnerud()\n        X = d.data\n        Y = d.target\n\n        clf = pls_.PLSRegression(n_components=2, multioutput='raw_values')\n        assert_raise_message(ValueError, \"multioutput should be 'raw_values' \"\n                             \"or 'variance_weighted' when Y is multioutput.\",\n                             clf.fit, X, Y)\n\n", "def test_pls_score_multioutput():\n    # Test score method for multi-output regression\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n\n    clf = pls_.PLSRegression(n_components=2)\n    clf.fit(X, Y)\n    score_all = clf.score(X, Y)\n\n    # Test scoring for individual outputs\n    scores = []\n    for i in range(Y.shape[1]):\n        score = clf.score(X, Y[:, i:i+1])\n        scores.append(score)\n\n    assert_almost_equal(score_all, np.mean(scores), decimal=2)\n\n", "compilation error"], "sample_521": ["def test_scatter_alpha():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    th = np.linspace(0, 2 * np.pi * 6, 256)\n    sc = ax.scatter(np.sin(th), np.cos(th), th, s=(1 + th * 5), c=th ** 2, alpha=0.5)\n    fig.canvas.draw()\n", "compilation error", "def test_scatter_spiral_alpha():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    th = np.linspace(0, 2 * np.pi * 6, 256)\n    sc = ax.scatter(np.sin(th), np.cos(th), th, s=(1 + th * 5), c=th ** 2, alpha=0.5)\n\n    # force at least 1 draw!\n    fig.canvas.draw()\n", "    def test_contour(self):\n        fig = plt.figure()\n        ax = fig.add_subplot(projection='3d')\n        X, Y, Z = np.meshgrid(np.arange(10), np.arange(10), np.arange(10))\n        ax.contour(X, Y, Z, cmap='viridis')\n", "def test_scatter_vertical():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    th = np.linspace(0, 2 * np.pi * 6, 256)\n    sc = ax.scatter(th, np.sin(th), np.cos(th), s=(1 + th * 5), c=th ** 2)\n\n    # force at least 1 draw!\n    fig.canvas.draw()\n"], "sample_370": ["    def test_prefetch_across_m2m_and_fk(self):\n        \"\"\"\n        Test prefetching across an m2m relationship and a FK relationship.\n\n        This scenario ensures that prefetching works correctly when traversing\n        multiple relationship types in a single prefetch chain.\n        \"\"\"\n        # Create some data\n        author = Author.objects.create(name='Stephen King')\n        book1 = Book.objects.create(title='The Shining', authors=author)\n        book2 = Book.objects.create(title='It', authors=author)\n        genre1 = Genre.objects.create(name='Horror')\n        genre2 = Genre.objects.create(name='Thriller')\n        book1.genres.add(genre1)\n        book2.genres.add(genre1, genre2)\n\n        # Prefetch genres and author through the m2m and FK relationships\n        with self.assertNumQueries(3):\n            books = Book.objects.prefetch_related(\n                Prefetch('genres', queryset=Genre.objects.all()),\n                'authors'\n            ).filter(authors__name='Stephen King')\n        \n        # Verify prefetching results\n        for book in books:\n            self.assertEqual(len(book.genres.all()), len(book.genres))\n            self.assertEqual(book.authors.first().name, 'Stephen King')\n", "    def test_prefetch_related_with_transform(self):\n        \"\"\"\n        Checks if prefetch_related() works correctly when using a transform\n        function on the related manager.\n        \"\"\"\n        queryset = Author.objects.filter(id__in=[self.author1.id, self.author2.id]).prefetch_related(\n            Prefetch('books', queryset=Book.objects.transform('double_published_year', Book.objects.only('published_year').values('published_year')), to_attr='books_doubled')\n        )\n        with self.assertNumQueries(2):\n            authors = list(queryset)\n        self.assertEqual(authors[0].books_doubled.count(), 1)\n        self.assertEqual(authors[1].books_doubled.count(), 1)\n", "    def test_prefetch_related_empty_result(self):\n        queryset = House.objects.filter(pk__in=[]).prefetch_related(\n            Prefetch('rooms', queryset=Room.objects.only('name')),\n        )\n        with self.assertNumQueries(1):\n            list(queryset)\n\n", "    def test_prefetch_related_with_filtered_queryset(self):\n        house = House.objects.create(name='Big house', address='123 Main St')\n        Room.objects.create(name='Kitchen', house=house)\n        Room.objects.create(name='Living Room', house=house)\n        rooms = Room.objects.filter(name='Kitchen').prefetch_related('house')\n        with self.assertNumQueries(2):\n            self.assertEqual(rooms.first().house.address, '123 Main St')\n\n", "    def test_prefetch_related_with_select_related(self):\n        \"\"\"\n        prefetch_related() and select_related() can be combined.\n\n        When a field is specified in both select_related() and\n        prefetch_related(), select_related() takes precedence.\n        \"\"\"\n        with self.assertNumQueries(2):\n            authors = AuthorWithAge.objects.select_related('first_book').prefetch_related('favorite_authors__first_book')\n            for author in authors:\n                author.first_book\n                for favorite_author in author.favorite_authors.all():\n                    favorite_author.first_book\n"], "sample_1019": ["compilation error", "compilation error", "compilation error", "    def test_issue_10307():\n        p = symbols('p')\n        assert _monotonic_sign(p**(1/3)) is None\n", "compilation error"], "sample_308": ["    def test_rfc5322_aware_datetime(self):\n        dt = datetime(2009, 5, 16, 5, 30, 30, tzinfo=utc)\n        self.assertEqual(dateformat.format(dt, 'r'), 'Sat, 16 May 2009 05:30:30 +0000')\n", "    def test_g_format_midnight(self):\n        self.assertEqual(dateformat.format(datetime(2000, 1, 1, 0), 'g'), '12')\n", "    def test_timezone_name_ambiguous_datetime(self):\n        dt = datetime(2015, 10, 25, 2, 30, 0)\n        \n        self.assertEqual(format(dt, 'T'), 'CET')\n\n", "    def test_microsecond_formatting(self):\n        # Regression test for #27748\n        dt = datetime(2023, 1, 1, 12, 0, 0, 999999)\n        self.assertEqual(dateformat.format(dt, 'u'), '999999')\n\n", "    def test_date_format_with_invalid_date(self):\n        with self.assertRaises(ValueError):\n            dateformat.format(datetime(2023, 13, 3), 'Y-m-d')\n"], "sample_883": ["def test_bayesian_ridge_ard_fit_sample_weight():\n    # Test that BayesianRidge and ARDRegression can handle sample_weights\n    X = np.array([[1, 1], [2, 2], [3, 3], [4, 4]])\n    y = np.array([1, 2, 3, 4])\n    sample_weight = np.array([0.5, 1, 1.5, 2])\n\n    for estimator in (BayesianRidge(), ARDRegression()):\n        estimator.fit(X, y, sample_weight=sample_weight)\n        # Check if the model learned the weighted average\n        assert_almost_equal(estimator.predict([[1, 1]]), 1.25)\n", "    def test_sample_weight_int(self):\n        # Test BayesianRidge with integer sample weights\n\n        X = np.array([[1, 1], [3, 4], [5, 7], [4, 1], [2, 6], [3, 10], [3, 2]])\n        y = np.array([1, 2, 3, 2, 0, 4, 5])\n        w = np.array([1, 2, 3, 4, 5, 6, 7]).astype(int)\n\n        clf = BayesianRidge()\n        clf.fit(X, y, sample_weight=w)\n\n        # Check that the model could approximately learn the identity function\n        test = [[1], [3], [4]]\n        assert_array_almost_equal(clf.predict(test), [1, 3, 4], 2)\n", "    def test_predict_output_shape(self):\n        n_samples = 10\n        n_features = 5\n        random_state = check_random_state(42)\n        X = random_state.random_sample((n_samples, n_features))\n        y = random_state.random_sample(n_samples)\n\n        for clf in [BayesianRidge(), ARDRegression()]:\n            clf.fit(X, y)\n            # Test prediction shape for both single sample and multiple samples\n            assert clf.predict(X[0:1]).shape == (1,)\n            assert clf.predict(X).shape == (n_samples,)\n\n", "    def test_bayesian_ridge_ard_fit_empty_input(self):\n        \"\"\"Test BayesianRidge ARD fit method when empty input data provided.\"\"\"\n\n        X = np.array([]).reshape(0, 10)\n        y = np.array([])\n\n        for clf in [BayesianRidge(), ARDRegression()]:\n            with pytest.raises(ValueError):\n                clf.fit(X, y)\n", "    def test_bayesian_ridge_ard_intercept_with_fit_intercept_false(\n        global_random_seed"], "sample_332": ["    def test_formset_with_initial_data(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        initial = [{'choice': 'Initial Zero', 'votes': 0}, {'choice': 'Initial One', 'votes': 1}]\n        formset = ChoiceFormSet(data, initial=initial, auto_id=False, prefix='choices')\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(formset.cleaned_data, [\n            {'choice': 'Zero', 'votes': 0},\n            {'choice': 'One', 'votes': 1},\n        ])\n", "    def test_empty_formset_with_initial_data(self):\n        \"\"\"\n        Test an empty formset with initial data.\n\n        This tests that the formset is still considered valid even if it's empty.\n\n        \"\"\"\n        ChoiceFormSet = formset_factory(Choice, extra=0)\n        formset = ChoiceFormSet(initial=[{'choice': 'One', 'votes': 1}])\n        self.assertTrue(formset.is_valid())\n", "    def test_all_valid_empty_formsets(self):\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet()\n        formset2 = ChoiceFormSet()\n        self.assertIs(all_valid((formset1, formset2)), True)\n\n", "    def test_formset_with_non_field_errors_from_form(self):\n        class CustomForm(Form):\n            field = CharField()\n\n                raise ValidationError(\"This is a non-form error\")\n\n        ChoiceFormSet = formset_factory(CustomForm)\n        data = {'form-TOTAL_FORMS': '1', 'form-INITIAL_FORMS': '0'}\n        formset = ChoiceFormSet(data)\n        self.assertEqual(len(formset.non_form_errors()), 1)\n        self.assertEqual(formset.errors, [{'': [\"This is a non-form error\"]}])\n", "    def test_empty_formset_errors(self):\n        \"\"\"\n        all_valid() should return False if any formset is empty and invalid.\n        \"\"\"\n        formset1 = ChoiceFormSet(data={}, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data={}, auto_id=False, prefix='choices')\n        self.assertFalse(all_valid((formset1, formset2)))\n"], "sample_888": ["def test_iforest_fit_predict_empty():\n    \"\"\"Test that IsolationForest can handle empty input data.\"\"\"\n    X = np.empty((0, 1))\n    clf = IsolationForest()\n    clf.fit(X)\n    assert clf.predict(X).size == 0\n", "def test_iforest_predict_proba():\n    # Test predict_proba method for IsolationForest\n    X_train = make_classification(\n        n_samples=100, n_features=10, random_state=0, n_informative=5\n    )\n    X_test = make_classification(\n        n_samples=50, n_features=10, random_state=0, n_informative=5\n    )\n    clf = IsolationForest(contamination=0.1, random_state=0).fit(X_train)\n    probas = clf.predict_proba(X_test)\n    assert probas.shape == (50, 2)\n    assert np.all(probas[:, 0] >= 0)\n    assert np.all(probas[:, 1] >= 0)\n    assert np.all(probas[:, 0] + probas[:, 1] == 1)\n", "def test_iforest_contamination_auto():\n    # Generate train/test data\n    rng = check_random_state(0)\n    X = rng.randn(100, 2)\n    X_test = rng.randn(50, 2)\n\n    # Define contamination automatically from the training data\n    clf = IsolationForest(contamination=\"auto\", random_state=rng).fit(X)\n\n    # Predict scores (the lower, the more normal)\n    y_pred = -clf.decision_function(X_test)\n    outlier_indices = np.where(y_pred > clf.offset_)\n\n    # Check if the number of predicted outliers is approximately equal to the\n    # contamination percentage.\n    assert len(outlier_indices[0]) / len(X_test) == pytest.approx(\n        clf.contamination_, rel=0.2\n    )\n\n", "def test_iforest_preserve_feature_names_auto_contamination():\n    \"\"\"Check that feature names are preserved when contamination is \"auto\".\n\n    Feature names are required for consistency checks during scoring.\n\n    Non-regression test for Issue #25844\n    \"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    rng = np.random.RandomState(0)\n\n    X = pd.DataFrame(data=rng.randn(4), columns=[\"a\"])\n    model = IsolationForest(random_state=0, contamination=\"auto\")\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"error\", UserWarning)\n        model.fit(X)\n", "def test_iforest_decision_function():\n    # Test if the decision_function returns the correct results\n    X = np.array([[1, 1], [1, 2], [2, 1]])\n    clf = IsolationForest().fit(X)\n\n    # Test for inliers\n    assert clf.decision_function(X[:2]).min() > 0\n    # Test for outlier\n    assert clf.decision_function(X[2:])[0] < 0\n\n"], "sample_1075": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_37": ["def test_invalid_header():\n    \"\"\"\n    Test handling of headers with missing or invalid WCS keywords.\n    \"\"\"\n    hdr = fits.Header()\n    with pytest.raises(ValueError) as exc:\n        wcs.WCS(hdr)\n    assert \"No valid WCS information found\" in str(exc.value)\n\n", "def test_to_fits_2():\n    \"\"\"\n    Test to_fits() with a WCS object created directly.\n    \"\"\"\n    w = wcs.WCS(naxis=2)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\"]\n    w.wcs.crval = [123.45, 67.89]\n\n    wfits = w.to_fits()\n", "    def test_to_fits_with_extra_hdu():\n        fits_name = get_pkg_data_filename('data/dist.fits')\n        w = wcs.WCS(fits_name)\n        hdulist = fits.HDUList.fromfile(fits_name)\n        hdulist.append(fits.ImageHDU())\n        wfits = w.to_fits(hdulist)\n        assert isinstance(wfits, fits.HDUList)\n        assert len(wfits) == 3\n        assert isinstance(wfits[0], fits.PrimaryHDU)\n        assert isinstance(wfits[1], fits.ImageHDU)\n        assert isinstance(wfits[2], fits.ImageHDU)\n", "    def test_wcs_to_fits_header():\n        w = wcs.WCS(naxis=2)\n        w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\"]\n        w.wcs.crval = [180.0, \n                       -30.0]\n        w.wcs.crpix = [1000.0, \n                       500.0]\n        w.wcs.cdelt = [0.001, \n                       0.001]\n        h = w.to_fits_header()\n        assert \"CTYPE1\" in h\n        assert \"CTYPE2\" in h\n        assert \"CRVAL1\" in h\n        assert \"CRVAL2\" in h\n        assert \"CRPIX1\" in h\n        assert \"CRPIX2\" in h\n        assert \"CDELT1\" in h\n        assert \"CDELT2\" in h\n\n", "def test_to_fits_2():\n    \"\"\"\n    Test to_fits() without any distortion.\n    \"\"\"\n    h = fits.Header()\n    h['NAXIS'] = 2\n    h['CTYPE1'] = 'RA---TAN'\n    h['CTYPE2'] = 'DEC--TAN'\n    h['CRVAL1'] = 180.0\n    h['CRVAL2'] = 30.0\n    h['CDELT1'] = -0.005\n    h['CDELT2'] = 0.005\n    h['CRPIX1'] = 500.5\n    h['CRPIX2'] = 500.5\n    w = wcs.WCS(h)\n    wfits = w.to_fits()\n    assert isinstance(wfits, fits.HDUList)\n    assert isinstance(wfits[0], fits.PrimaryHDU)\n\n"], "sample_836": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_782": ["def test_column_transformer_callable_specifier_pandas():\n    # assert that function gets the full array / dataframe\n    pd = pytest.importorskip('pandas')\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_res_first = np.array([[0, 1, 2]]).T\n    X_df = pd.DataFrame(X_array, columns=['first', 'second'])\n\n        assert_array_equal(X.columns, X_df.columns)\n        assert_array_equal(X.values, X_df.values)\n        return ['first']\n\n    ct = ColumnTransformer([('trans', Trans(), func)],\n                           remainder='drop')\n    assert_array_equal(ct.fit_transform(X_df), X_res_first)\n    assert_array_equal(ct.fit(X_df).transform(X_df), X_res_first)\n    assert callable(ct.transformers[0][2])\n    assert ct.transformers_[0][2] == ['first']\n\n", "def test_column_transformer_nested_remainder():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).T\n    ct = ColumnTransformer([('trans1', Trans(), [0]),\n                           ('trans2', StandardScaler(), [1])],\n                           remainder=ColumnTransformer([('trans3', 'drop', [0]),\n                                                        ('trans4', Trans(), [1])]))\n\n    X_trans = ct.fit_transform(X_array)\n\n    assert X_trans.shape == (3, 2)\n", "compilation error", "    def test_column_transformer_error_on_invalid_transformer():\n        X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n        ct = ColumnTransformer([('trans', 'invalid', [0])])\n        assert_raises(TypeError, ct.fit, X_array)\n", "compilation error"], "sample_762": ["def test_clone_with_metaestimator():\n    from sklearn.pipeline import make_pipeline\n\n    my_estimator = MyEstimator()\n    pipe = make_pipeline(my_estimator, SVC())\n    cloned_pipe = clone(pipe)\n\n    assert pipe.steps[0][1] is not cloned_pipe.steps[0][1]\n", "def test_get_params_empty_list():\n    est = MyEstimator(empty=[])\n    assert est.get_params()['empty'] == []\n", "def test_clone_with_none_param():\n    class MyEstimator(BaseEstimator):\n            self.a = a\n    est = MyEstimator()\n    cloned_est = clone(est)\n    assert cloned_est.a is None\n", "    def test_clone_with_transformermixin():\n        # Tests that clone works correctly with estimators that mixin TransformerMixin\n\n        class MyTransformer(BaseEstimator, TransformerMixin):\n                return self\n\n                return X\n\n        transformer = MyTransformer()\n        cloned_transformer = clone(transformer)\n        assert transformer is not cloned_transformer\n        assert transformer.get_params() == cloned_transformer.get_params()\n\n", "def test_dont_pickle_empty_list_in_init():\n    # Regression test for #16997\n    class MyEstimator(BaseEstimator):\n            self.empty_list = empty_list\n\n    est = MyEstimator()\n    est_pickled = pickle.dumps(est)\n    est_loaded = pickle.loads(est_pickled)\n\n    assert est_loaded.empty_list == []\n"], "sample_686": ["def test_funcargnames_deprecated(attribute):\n    with pytest.warns(DeprecationWarning, match=attribute):\n        getattr(pytest.Fixture, attribute)\n", "def test_fixture_positional_arguments_deprecated(testdir, kwarg: str) -> None:\n    testdir.makepyfile(\n        f\"\"\"\n        import pytest\n\n        @pytest.fixture\n        def {kwarg.lower()}(request):\n            return request.param\n\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [f\"Passing arguments to pytest.fixture() as positional arguments is deprecated\"]\n    )\n", "def test_funcargnames_and_fixturenames_deprecated(attribute):\n    with pytest.warns(DeprecationWarning, match=attribute):\n        getattr(pytest, attribute)\n\n", "def test_funcargnames_is_deprecated(attribute):\n    with pytest.warns(pytest.PytestDeprecationWarning as w) as cw:\n        getattr(pytest.MonkeyPatch(), attribute)\n\n    assert \"deprecated\" in str(w.message)\n    assert \"since pytest 2.3\" in str(w.message)\n\n\n\n", "def test_pytest_collect_module_deprecated_pytester(name):\n    with pytest.warns(DeprecationWarning, match=f\"pytest.collect.{name}\"):\n        getattr(pytest, name)\n"], "sample_718": ["    def test_check_estimator_set_params():\n        # Check that set_params returns self\n        for Estimator in [GaussianMixture, LinearRegression,\n                          RandomForestClassifier, NMF, SVC,\n                          MiniBatchKMeans]:\n            with ignore_warnings(category=FutureWarning):\n                # when 'est = SGDClassifier()'\n                est = Estimator()\n            set_checking_parameters(est)\n            assert_true(est.set_params() is est)\n", "    def test_check_sparse_matrices():\n        # check that check_estimator correctly handles sparse matrices\n        from scipy.sparse import csr_matrix\n\n        class SparseEstimator(BaseEstimator):\n                X, y = check_X_y(X, y, accept_sparse=['csr'])\n                assert isinstance(X, csr_matrix)\n                return self\n\n                X = check_array(X, accept_sparse=['csr'])\n                assert isinstance(X, csr_matrix)\n                return np.ones(X.shape[0])\n\n        check_estimator(SparseEstimator)\n\n", "    def test_check_estimator_sparse_input():\n        # check that check_estimator() works with sparse input\n        # and doesn't raise warnings related to sparse matrix handling\n        from sklearn.datasets import load_iris\n        from scipy.sparse import csr_matrix\n\n        iris = load_iris()\n        X_sparse = csr_matrix(iris.data)\n        y = iris.target\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\")\n            est = SVC()\n            check_estimator(est)\n            est.fit(X_sparse, y)\n            est.predict(X_sparse)\n", "    def test_check_estimator_sparse_data_input():\n        # Tests that the estimator correctly handle sparse data input\n\n        from sklearn.datasets import make_classification\n        from sklearn.linear_model import LogisticRegression\n        x, y = make_classification(n_samples=100, random_state=42)\n        sparse_x = sp.csr_matrix(x)\n\n        for estimator in [LogisticRegression(), SVC(),\n                          AdaBoostClassifier(), RandomForestClassifier()]:\n            \n            # Clone the estimator to avoid side effects\n            cloned_estimator = clone(estimator)\n            # Fit on sparse data\n            cloned_estimator.fit(sparse_x, y)\n            # Predict on sparse data\n            cloned_estimator.predict(sparse_x)\n\n        \n\n", "    def test_check_estimators_data_not_an_array(self):\n        # check if estimators raise errors when data is not an array\n        # Test for estimators that accept sparse matrices\n        check_estimators_data_not_an_array(\"estimator_name\",\n                                         SVC(kernel='linear'))\n        # Test for estimators that do not accept sparse matrices\n        check_estimators_data_not_an_array(\"estimator_name\",\n                                         LinearRegression())\n"], "sample_1137": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_648": ["    def test_mark_mro_with_skip(pytester):\n        skip = pytest.mark.skip\n\n        @skip(\"a\")\n        class A:\n            pass\n\n        @skip(\"b\")\n        class B:\n            pass\n\n        @skip(\"c\")\n        class C(A, B):\n            pass\n\n        from _pytest.mark.structures import get_unpacked_marks\n\n        all_marks = get_unpacked_marks(C)\n\n        assert all_marks == [skip(\"c\").mark, skip(\"a\").mark, skip(\"b\").mark]\n\n        assert get_unpacked_marks(C, consider_mro=False) == [skip(\"c\").mark] \n", "    def test_parametrize_marker_scope(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.parametrize(\"foo\", [1, 2, 3])\n                if request.node.scope == 'function':\n                    assert True\n                else:\n                    assert False\n\n            @pytest.mark.parametrize(\"bar\", [1, 2, 3])\n            @pytest.fixture(scope='module')\n                if request.node.scope == 'module':\n                    assert True\n                else:\n                    assert False\n        \"\"\"\n        )\n\n        result = pytester.runpytest()\n        result.assert_outcomes(passed=6)\n", "    def test_mark_mro_mixed(pytester: Pytester) -> None:\n        xfail = pytest.mark.xfail\n        skip = pytest.mark.skip\n\n        @skip(\"a\")\n        class A:\n            pass\n\n        @xfail(\"b\")\n        class B(A):\n            pass\n\n        @skip(\"c\")\n        class C(B):\n            pass\n\n        from _pytest.mark.structures import get_unpacked_marks\n\n        all_marks = get_unpacked_marks(C)\n\n        assert all_marks == [skip(\"c\").mark, skip(\"a\").mark, xfail(\"b\").mark]\n", "    def test_mark_mro_order(pytester: Pytester):\n        xfail = pytest.mark.xfail\n\n        @xfail(\"a\")\n        class A:\n            pass\n\n        @xfail(\"b\")\n        class B:\n            pass\n\n        class C(B, A):\n            pass\n\n        from _pytest.mark.structures import get_unpacked_marks\n        all_marks = get_unpacked_marks(C)\n        assert all_marks == [xfail(\"a\").mark, xfail(\"b\").mark]\n", "    def test_mark_mro_with_multiple_inheritance_skip(pytester: Pytester) -> None:\n        xfail = pytest.mark.xfail\n        skip = pytest.mark.skip\n\n        @skip(\"a\")\n        class A:\n            pass\n\n        @xfail(\"b\")\n        class B(A):\n            pass\n\n        @skip(\"c\")\n        class C:\n            pass\n\n        @xfail(\"d\")\n        class D(B, C):\n            pass\n\n        from _pytest.mark.structures import get_unpacked_marks\n\n        all_marks = get_unpacked_marks(D)\n\n        assert all_marks == [xfail(\"d\").mark, skip(\"a\").mark, xfail(\"b\").mark, skip(\"c\").mark]\n\n"], "sample_42": ["compilation error", "compilation error", "def test_equivalencies_with_scales():\n    with u.set_enabled_equivalencies(u.dimensionless_angles()):\n        # Test that scales are correctly handled when using equivalencies\n        phase = 0.5 * u.cycle\n        assert_allclose(phase.to_value(u.radian), np.pi)\n        assert_allclose(phase.to_value(u.degree), 180)\n", "def test_redshift():\n    z = 0.5 * u.dimensionless\n    c = u.speed_of_light\n    H0 = 70 * u.km/u.s/u.Mpc\n    cosmo = cosmology.FlatLambdaCDM(H0=H0, Om0=0.3)\n    dist = cosmo.comoving_distance(z).to(u.Mpc)\n    assert_allclose(dist.value, 1200.730521543911)\n\n", "def test_doppler_optical_with_velocity():\n    z = 0.1\n    c = constants.c\n    velocity = z * c\n    equiv = u.doppler_optical(velocity=velocity)\n    assert equiv.wavelength_scale == (1 + z)\n"], "sample_871": ["    def test_silhouette_score_empty_cluster():\n        X = np.array([[0, 0], [1, 1], [2, 2]])\n        labels = np.array([0, 0, 1])\n        with pytest.raises(ValueError, match=\"Number of labels is\"):\n            silhouette_score(X, labels)\n", "    def test_silhouette_score_empty_cluster():\n        \"\"\"\n        Test silhouette_score and silhouette_samples when there is an empty cluster\n        \"\"\"\n        X = np.array([[0, 0], [1, 1], [2, 2], [3, 3]])\n        labels = np.array([0, 0, 1, -1])\n        with pytest.warns(UserWarning, match=\"Some clusters are empty\"):\n            silhouette_score(X, labels)\n        with pytest.warns(UserWarning, match=\"Some clusters are empty\"):\n            silhouette_samples(X, labels)\n\n\n", "    def test_silhouette_sparse_metric():\n        # Test silhouette_score with a sparse metric.\n        rng = np.random.RandomState(0)\n        X = rng.rand(10, 2)\n        labels = np.array([0, 0, 1, 1, 0, 1, 0, 1, 0, 1])\n        sparse_dist_matrix = pairwise_distances(X, metric='euclidean')\n        sparse_dist_matrix = csr_matrix(sparse_dist_matrix)\n        \n        score_sparse = silhouette_score(sparse_dist_matrix, labels, metric='precomputed')\n        score_dense = silhouette_score(X, labels, metric='euclidean')\n        assert_allclose(score_sparse, score_dense)\n", "def test_silhouette_samples_empty_cluster():\n    # Test Silhouette Score with an empty cluster\n    X = np.array([[0.2, 0.1, 0.1, 0.2, 0.1, 1.6, 0.2, 0.1]], dtype=np.float32).T\n    y = [0, 0, 0, 0, 1, 1, 1, -1]\n    # Empty cluster has label -1\n    with pytest.raises(ValueError, match=\"Empty cluster\"):\n        silhouette_samples(X, y)    \n", "def test_silhouette_score_empty_cluster():\n    # Test silhouette score when a cluster is empty.\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    labels = np.array([0, 0, 1, -1])\n\n    with pytest.raises(ValueError, match=\"Number of labels is\"):\n        silhouette_score(X, labels)\n"], "sample_395": ["    def test_template_dirs_without_filesystem_loader(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {ROOT / \"templates\"},\n        )\n", "    def test_get_template_directories_no_template_dirs(self):\n        self.assertSetEqual(autoreload.get_template_directories(), set())\n", "    def test_get_template_directories_with_app_directories(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(), {ROOT / \"templates\"}\n        )\n", "    def test_template_dirs_with_app_directories_loader(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {ROOT / \"templates\"},\n        )\n", "    def test_get_template_directories_no_dirs(self):\n        self.assertEqual(autoreload.get_template_directories(), set())\n"], "sample_326": ["    def test_urlize_nofollow(self):\n        tests = (\n            (\n                'Search for google.com/?q=! and see.',\n                'Search for <a href=\"http://google.com/?q=\" rel=\"nofollow\">google.com/?q=</a>! and see.'\n            ),\n            (\n                'Search for google.com/?q=1&lt! and see.',\n                'Search for <a href=\"http://google.com/?q=1%3C\" rel=\"nofollow\">google.com/?q=1&lt</a>! and see.'\n            ),\n        )\n        for value, output in tests:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value, nofollow=True), output)\n\n", "    def test_urlize_trim_url(self):\n        tests = (\n            (\n                'Search for http://this.is.a.very.long.url.com?with=lots&of=parameters and see.',\n                'Search for <a href=\"http://this.is.a.very.long.url.com?with=lots&of=parameters\">http://this.is.a.very.long.url.com\u2026</a> and see.'\n            ),\n            (\n                'Search for google.com/?q=! and see.',\n                'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>! and see.'\n            ),\n        )\n        for value, output in tests:\n            with self.subTest(value=value, output=output):\n                self.assertEqual(urlize(value, trim_url_limit=20), output)\n", "    def test_urlize_trim_url_limit(self):\n        text = (\n            'This is a test with a very very very very very very very very '\n            'long URL: https://www.example.com/very/long/url/path/that/needs/to/be/trimmed'\n        )\n        url_limit = 30\n        expected = (\n            'This is a test with a very very very very very very very very '\n            'long URL: <a href=\"https://www.example.com/very/long/url/path/that/needs/to/be/trimmed\">'\n            'https://www.example.com/very/long/url/path\u2026</a>'\n        )\n        self.assertEqual(urlize(text, trim_url_limit=url_limit), expected)\n", "    def test_urlize_with_nofollow(self):\n        tests = (\n            (\n                'Search for google.com/?q=! and see.',\n                'Search for <a href=\"http://google.com/?q=\" rel=\"nofollow\">google.com/?q=</a>! and see.'\n            ),\n            (\n                'Check out this site: https://example.com',\n                'Check out this site: <a href=\"https://example.com\" rel=\"nofollow\">https://example.com</a>'\n            ),\n        )\n        for value, output in tests:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value, nofollow=True), output)\n", "    def test_urlize_trim_url_limit(self):\n        text = 'See: https://www.example.com/very/long/path/to/some/resource. This is some more text.'\n        expected = 'See: <a href=\"https://www.example.com/very/long/path/to/some/resource\">https://www.example.com/very/long/path/to/some/resource\u2026</a>.'\n        self.assertEqual(urlize(text, trim_url_limit=50), expected)\n"], "sample_407": ["    def test_clear_after_prefetch_related(self):\n        c = City.objects.create(name=\"Musical City\")\n        d = District.objects.create(name=\"Ladida\", city=c)\n        city = City.objects.prefetch_related(\"districts\").get(id=c.id)\n        self.assertSequenceEqual(city.districts.all(), [d])\n        city.districts.clear()\n        city = City.objects.prefetch_related(\"districts\").get(id=c.id)\n        self.assertSequenceEqual(city.districts.all(), [])", "    def test_reverse_foreign_key_set_and_add(self):\n        usa = Country.objects.create(name=\"United States\")\n        chicago = City.objects.create(name=\"Chicago\")\n        new_york = City.objects.create(name=\"New York\")\n        usa.cities.add(chicago, new_york)\n        usa = Country.objects.get(name=\"United States\")\n        self.assertSequenceEqual([chicago, new_york], list(usa.cities.all()))\n", "    def test_prefetch_with_custom_manager(self):\n        public_school = School.objects.create(is_public=True)\n        private_school = School.objects.create(is_public=False)\n        public_student = Student.objects.create(school=public_school)\n        private_student = Student.objects.create(school=private_school)\n\n        with self.assertNumQueries(2):\n            Student.objects.prefetch_related('school').all()\n\n        with self.assertNumQueries(1):\n            students = Student.objects.prefetch_related('school').all()\n        for student in students:\n            self.assertIsNotNone(student.school)\n\n", "    def test_fk_to_self_reference(self):\n        grandpa = SelfRef.objects.create(name=\"grandpa\")\n        dad = SelfRef.objects.create(name=\"dad\", parent=grandpa)\n        son = SelfRef.objects.create(name=\"son\", parent=dad)\n        self.assertEqual(son.parent.parent.name, \"grandpa\")\n", "    def test_clear_after_prefetch_with_custom_manager(self):\n        c = CustomManagerCity.objects.create(name=\"Musical City\")\n        d = CustomManagerDistrict.objects.create(name=\"Ladida\", city=c)\n        city = CustomManagerCity.objects.prefetch_related(\"districts\").get(id=c.id)\n        self.assertSequenceEqual(city.districts.all(), [d])\n        city.districts.clear()\n        self.assertSequenceEqual(city.districts.all(), [])\n\n"], "sample_524": ["    def test_colorbar_set_label_font_properties():\n        fig, ax = plt.subplots()\n        pc = ax.pcolormesh(np.random.randn(10, 10))\n        cb = fig.colorbar(pc)\n        font = mpl.font_manager.FontProperties(family='serif', size=16)\n        cb.set_label('My Label', fontproperties=font)\n        assert cb.ax.get_ylabel() == 'My Label'\n        assert cb.ax.get_ylabel().get_fontname() == 'serif'\n        assert cb.ax.get_ylabel().get_fontsize() == 16\n\n", "    def test_colorbar_labelsize(self):\n        fig, ax = plt.subplots()\n        pc = ax.pcolormesh(np.arange(10).reshape(5, 2))\n        cb = fig.colorbar(pc, ax=ax, label='Colorbar')\n        cb.ax.tick_params(labelsize=12)\n        assert cb.ax.yaxis.label.get_fontsize() == 12\n", "def test_colorbar_extendfrac():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc, extend='both')\n    assert cb.extendfrac == 0.05\n    cb.extendfrac = 0.1\n    assert cb.extendfrac == 0.1\n\n", "    def test_colorbar_set_label():\n        fig, ax = plt.subplots()\n        pc = ax.pcolormesh(np.random.randn(10, 10))\n        cb = fig.colorbar(pc, label='My Label')\n        assert cb.ax.get_ylabel() == 'My Label'\n", "    def test_colorbar_label_color():\n        fig, ax = plt.subplots()\n        pc = ax.pcolormesh(np.random.randn(10, 10))\n        cb = fig.colorbar(pc)\n        cb.set_label('My Label', color='red')\n        assert cb.ax.get_ylabel().get_color() == 'red'\n"], "sample_443": ["    def test_cache_miss_with_vary(self):\n        request = self.factory.get(self.path)\n        template = engines[\"django\"].from_string(\"This is a test\")\n        response = TemplateResponse(HttpRequest(), template)\n\n        response['Vary'] = 'Accept-Language'\n        response['Content-Language'] = 'en-us'\n        content = response.content\n        # Cache miss on first request with specific language\n        key = get_cache_key(request)\n        with self.assertRaises(KeyError):\n            cache.get(key)\n\n        cache.set(key, content)\n        # Cache hit on the second request\n        response2 = TemplateResponse(HttpRequest(), template)\n        response2['Vary'] = 'Accept-Language'\n        response2['Content-Language'] = 'en-us'\n        self.assertEqual(cache.get(key), response2.content)\n\n        # Cache miss on request with different language\n        response3 = TemplateResponse(HttpRequest(), template)\n        response3['Vary'] = 'Accept-Language'\n        response3['Content-Language'] = 'fr-fr'\n        self.assertNotEqual(cache.get(key), response3.content)\n\n", "    def test_cache_control_missing_max_age(self):\n        response = HttpResponse(\"Hello World!\")\n        response['Cache-Control'] = 'private'\n        cache_middleware = CacheMiddleware(empty_response)\n        cached_response = cache_middleware.process_response(None, response)\n        self.assertIsNone(cached_response)\n\n", "    def test_cache_page_no_args(self):\n        \"\"\"\n        Test that cache_page works correctly when called without arguments.\n        \"\"\"\n        view = cache_page()(hello_world_view)\n        request = self.factory.get(\"/view/\")\n        response = view(request, \"1\")\n        self.assertEqual(response.content, b\"Hello World 1\")\n        response = view(request, \"2\")\n        self.assertEqual(response.content, b\"Hello World 1\")\n", "    def test_cache_key_with_accept_language(self):\n        request = self.factory.get(self.path, HTTP_ACCEPT_LANGUAGE='en-US')\n        template = engines[\"django\"].from_string(\"This is a test\")\n        response = TemplateResponse(HttpRequest(), template)\n        # Expect None if no headers have been set yet.\n        self.assertIsNone(get_cache_key(request))\n        # Set headers to an empty list.\n        learn_cache_key(request, response)\n\n        self.assertEqual(\n            get_cache_key(request),\n            \"views.decorators.cache.cache_page.settingsprefix.GET.\"\n            \"58a0a05c8a5620f813686ff969c26853.d41d8cd98f00b204e9800998ecf8427e\",\n        )\n\n", "    def test_cache_middleware_with_custom_cache_name(self):\n        middleware = CacheMiddleware(hello_world_view, cache_alias=\"custom_cache\")\n        request = self.factory.get(\"/view/\")\n\n        response = hello_world_view(request, \"1\")\n\n        response = middleware.process_response(request, response)\n\n        # Repeating the request should result in a cache hit\n        result = middleware.process_request(request)\n        self.assertIsNotNone(result)\n        self.assertEqual(result.content, b\"Hello World 1\")\n"], "sample_265": ["    def test_template_loader_inheritance(self):\n        templates = [{\n            'BACKEND': 'django.template.backends.django.DjangoTemplates',\n            'DIRS': [Path(__file__).parent / 'templates'],\n            'OPTIONS': {'loaders': ['django.template.loaders.filesystem.Loader']},\n        }, {\n            'BACKEND': 'django.template.backends.django.DjangoTemplates',\n            'DIRS': [Path(__file__).parent / 'templates' / 'inheritance'],\n            'OPTIONS': {'loaders': [\n                'django.template.loaders.filesystem.Loader',\n                'django.template.loaders.app_directories.Loader',\n            ]},\n        }]\n        engines = EngineHandler(templates=templates)\n\n        template = engines['django'].get_template('base.html')\n        self.assertEqual(template.render({'title': 'Base'}), 'Base template')\n\n        template = engines['django'].get_template('child.html')\n        self.assertEqual(template.render({'title': 'Child'}), 'Child template\\nBase template')\n", "    def test_get_installed_libraries(self):\n        # Test that get_installed_libraries returns a dictionary with the correct\n        # module names as keys and full module paths as values\n        installed_libraries = get_installed_libraries()\n        self.assertIn('i18n', installed_libraries)\n        self.assertEqual(installed_libraries['i18n'], 'django.templatetags.i18n')\n        # Check that libraries from installed apps are included\n        self.assertIn('good_tags', installed_libraries)\n        self.assertEqual(installed_libraries['good_tags'], 'template_backends.apps.good.templatetags.good_tags')\n", "    def test_get_template_not_found(self):\n        \"\"\"Ensure TemplateDoesNotExist is raised when template is not found.\"\"\"\n        engine = DjangoTemplates({'DIRS': [], 'APP_DIRS': False, 'NAME': 'django', 'OPTIONS': {}})\n\n        with self.assertRaises(TemplateDoesNotExist) as cm:\n            engine.get_template('nonexistent.html')\n        self.assertEqual(cm.exception.template_name, 'nonexistent.html')\n", "    def test_template_debug_on_non_debug(self):\n        # Regression test for #27507: Template debug data should be included\n        # even if DEBUG is False, but only in the case of TemplateDoesNotExist.\n        with override_settings(DEBUG=False):\n            engine = DjangoTemplates({'DIRS': [], 'APP_DIRS': False, 'NAME': 'django', 'OPTIONS': {}})\n            template_name = 'template_that_does_not_exist.html'\n            with self.assertRaises(TemplateDoesNotExist) as exc_info:\n                engine.get_template(template_name)\n            exc = exc_info.exception\n            self.assertEqual(exc.template_debug, template_name)\n\n", "    def test_get_installed_libraries_finds_all_libraries(self):\n        engine = DjangoTemplates({'DIRS': [], 'APP_DIRS': True, 'NAME': 'django', 'OPTIONS': {}})\n        libraries = engine.get_installed_libraries()\n\n        # Ensure all expected libraries are present\n        self.assertIn('i18n', libraries)\n        self.assertIn('static', libraries)\n        self.assertIn('good_tags', libraries)\n        self.assertIn('subpackage.tags', libraries)\n"], "sample_802": ["    def test_pipeline_memory_after_clone(self):\n        iris = load_iris()\n        X = iris.data\n        y = iris.target\n        cachedir = mkdtemp()\n        if LooseVersion(joblib_version) < LooseVersion('0.12'):\n            # Deal with change of API in joblib\n            memory = Memory(cachedir=cachedir, verbose=10)\n        else:\n            memory = Memory(location=cachedir, verbose=10)\n        # Test with Transformer + SVC\n        clf = SVC(gamma='scale', probability=True, random_state=0)\n        transf = DummyTransf()\n        pipe = Pipeline([('transf', transf), ('svc', clf)], memory=memory)\n        pipe.fit(X, y)\n        cloned_pipe = clone(pipe)\n        assert_array_equal(pipe.predict(X), cloned_pipe.predict(X))\n        shutil.rmtree(cachedir)\n", "    def test_pipeline_memory_with_duplicate_names(self):\n       # Test that a Pipeline with duplicate names in the steps raises an error when memory is used.\n       cachedir = mkdtemp()\n       if LooseVersion(joblib_version) < LooseVersion('0.12'):\n           memory = Memory(cachedir=cachedir, verbose=10)\n       else:\n           memory = Memory(location=cachedir, verbose=10)\n\n       with pytest.raises(ValueError):\n           Pipeline([('a', DummyTransf()), ('a', DummyTransf())], memory=memory)\n\n       shutil.rmtree(cachedir)\n", "def test_feature_union_sparse_input():\n    # Test FeatureUnion with sparse input\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    X_sparse = sparse.csr_matrix(X)\n    pca = PCA(n_components=2, svd_solver='randomized', random_state=0)\n    select = SelectKBest(k=1)\n    fs = FeatureUnion([(\"pca\", pca), (\"select\", select)])\n\n    # Test fit and transform\n    X_trans = fs.fit(X_sparse, y).transform(X_sparse)\n    assert_equal(X_trans.shape[0], len(X))\n\n    # Test fit_transform\n    X_trans2 = fs.fit_transform(X_sparse, y)\n    assert_array_equal(X_trans, X_trans2)\n", "    def test_pipeline_memory_shared_memory():\n        iris = load_iris()\n        X = iris.data\n        y = iris.target\n        cachedir = mkdtemp()\n\n        if LooseVersion(joblib_version) < LooseVersion('0.12'):\n            # Deal with change of API in joblib\n            memory = Memory(cachedir=cachedir, verbose=10)\n        else:\n            memory = Memory(location=cachedir, verbose=10)\n\n        # Test with shared memory\n\n        clf = SVC(gamma='scale', probability=True, random_state=0)\n        transf = DummyTransf()\n\n        pipeline = Pipeline([('transf', transf), ('svc', clf)],\n                            memory=memory)\n\n        pipeline.fit(X, y)\n\n        # Check if the cache is actually used\n        assert os.path.exists(os.path.join(cachedir, 'joblib'))\n\n        # Test with a different cachedir\n\n        cachedir2 = mkdtemp()\n        if LooseVersion(joblib_version) < LooseVersion('0.12'):\n            memory2 = Memory(cachedir=cachedir2, verbose=10)\n        else:\n            memory2 = Memory(location=cachedir2, verbose=10)\n\n        pipeline2 = Pipeline([('transf', transf), ('svc', clf)],\n                             memory=memory2)\n\n        pipeline2.fit(X, y)\n        assert os.path.exists(os.path.join(cachedir2, 'joblib'))\n\n        shutil.rmtree(cachedir)\n        shutil.rmtree(cachedir2)\n\n", "    def test_pipeline_memory_different_cache_dir(self):\n        iris = load_iris()\n        X = iris.data\n        y = iris.target\n        cachedir1 = mkdtemp()\n        cachedir2 = mkdtemp()\n        try:\n            if LooseVersion(joblib_version) < LooseVersion('0.12'):\n                # Deal with change of API in joblib\n                memory1 = Memory(cachedir=cachedir1, verbose=10)\n                memory2 = Memory(cachedir=cachedir2, verbose=10)\n            else:\n                memory1 = Memory(location=cachedir1, verbose=10)\n                memory2 = Memory(location=cachedir2, verbose=10)\n            # Test with Transformer + SVC\n            clf = SVC(gamma='scale', probability=True, random_state=0)\n            transf = DummyTransf()\n\n            pipeline1 = Pipeline([('transf', transf), ('svc', clf)],\n                                 memory=memory1)\n            pipeline2 = Pipeline([('transf', transf), ('svc', clf)],\n                                 memory=memory2)\n\n            # Memoize the transformer at the first fit\n            pipeline1.fit(X, y)\n            pipeline2.fit(X, y)\n            # Check that the cached transformers are different\n            assert pipeline1.named_steps['transf'].timestamp_ != \\\n                   pipeline2.named_steps['transf'].timestamp_\n        finally:\n            shutil.rmtree(cachedir1)\n            shutil.rmtree(cachedir2)\n\n"], "sample_744": ["compilation error", "compilation error", "compilation error", "compilation error", "    def test_power_transformer_yeo_johnson_sparse(self):\n        X = sparse.csr_matrix(np.abs(X_2d))\n        pt = PowerTransformer(method='yeo-johnson')\n\n        X_trans = pt.fit_transform(X)\n        X_trans_func = power_transform(X, method='yeo-johnson')\n\n        assert_sparse_matrix_almost_equal(X_trans, X_trans_func)\n        assert_array_almost_equal(pt.inverse_transform(X_trans), X.toarray())\n"], "sample_359": ["    def test_references_field_same_app_label(self):\n        operation = FieldOperation('Model', 'field', models.ForeignKey('Other', models.CASCADE))\n        self.assertIs(operation.references_field('Other', 'field', 'migrations'), True)\n\n", "    def test_remove_field_references_model(self):\n        operation = migrations.RemoveField('MoDel', 'field')\n        self.assertIs(operation.references_model('mOdEl', 'migrations'), True)\n        self.assertIs(operation.references_model('oTher', 'migrations'), False)\n", "    def test_remove_field_references_field(self):\n        operation = FieldOperation('Model', 'field', None, action='remove')\n        self.assertIs(operation.references_field('Model', 'field', 'migrations'), True)\n", "    def test_deconstruct_with_through_and_through_fields(self):\n        operation = FieldOperation(\n            'Model', 'field',\n            models.ManyToManyField('Other', through='Through', through_fields=('first', 'second'))\n        )\n        definition = operation.deconstruct()\n        self.assertEqual(definition[0], 'FieldOperation')\n        self.assertEqual(definition[1], ['Model', 'field'])\n        self.assertEqual(sorted(definition[2]), ['many_to_many', 'through', 'through_fields'])\n", "    def test_references_field_with_related_name(self):\n        operation = FieldOperation('Model', 'field', models.ForeignKey('Other', models.CASCADE, related_name='related_name'))\n        self.assertIs(operation.references_field('Other', 'related_name', 'migrations'), True)\n        self.assertIs(operation.references_field('Other', 'whatever', 'migrations'), False)\n        self.assertIs(operation.references_field('Missing', 'related_name', 'migrations'), False)\n"], "sample_263": ["    def test_loaddata_invalid_format(self):\n        with self.assertRaisesMessage(CommandError, \"Unsupported format: 'invalid'.\"):\n            management.call_command('loaddata', 'fixture1.json', format='invalid', verbosity=0)\n", "    def test_loaddata_app_label_conflict(self):\n        with self.assertRaisesMessage(management.CommandError, \"Multiple fixtures named 'conflict' found.\"):\n            management.call_command(\n                'loaddata', 'conflict', app_label='fixtures', verbosity=0\n            )\n", "    def test_circular_reference_proxy_model(self):\n        management.call_command('loaddata', 'circular_reference_proxy_model.json', verbosity=0)\n        obj_a = CircularAP.objects.get()\n        obj_b = CircularBP.objects.get()\n        self.assertEqual(obj_a.obj, obj_b)\n        self.assertEqual(obj_b.obj, obj_a)\n        self._dumpdata_assert(\n            ['fixtures'],\n            '[{\"model\": \"fixtures.circulara\", \"pk\": 1, '\n            '\"fields\": {\"key\": \"x\", \"obj\": 1}}, '\n            '{\"model\": \"fixtures.circulara\", \"pk\": 2, '\n            '\"fields\": {\"key\": \"x2\", \"obj\": 1}},'\n            '{\"model\": \"fixtures.circularb\", \"pk\": 1, '\n            '\"fields\": {\"key\": \"y\", \"obj\": 1}},'\n            '{\"model\": \"fixtures.circularb\", \"pk\": 2, '\n            '\"fields\": {\"key\": \"y2\", \"obj\": 1}}]',\n        )\n", "    def test_loaddata_with_invalid_format(self):\n        with self.assertRaisesMessage(\n            management.CommandError,\n            \"Unsupported format: 'invalid'\"\n        ):\n            management.call_command('loaddata', 'fixture1.json', format='invalid', verbosity=0)\n", "    def test_loaddata_with_invalid_natural_key(self):\n        msg = \"Could not load fixtures.NaturalKeyThing(pk=1): 't1' is not a valid value\"\n        with self.assertRaisesMessage(ValueError, msg):\n            management.call_command('loaddata', 'invalid_natural_key.json', verbosity=0)\n"], "sample_177": ["    def test_proxy_inheritance(self):\n        A = self.create_model(\"A\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n        B = self.create_model(\"B\")\n        P = self.create_model(\"P\", bases=(A,), proxy=True)\n        self.assertRelated(A, [B, P])\n        self.assertRelated(B, [A, P])\n        self.assertRelated(P, [B])\n", "    def test_field_with_relational_default(self):\n        class Group(models.Model):\n            name = models.CharField(max_length=100)\n\n        class User(models.Model):\n            name = models.CharField(max_length=100)\n            group = models.ForeignKey(Group, models.CASCADE, default=Group.objects.first)\n\n        group = Group.objects.create(name='test')\n        user = User.objects.create(name='test_user')\n        self.assertEqual(user.group, group)\n        state = ModelState.from_model(User)\n        self.assertEqual((state.fields['group'].default, 'migrations.group'), (group.pk, 'migrations.group'))\n\n", "    def test_proxy_foreign_key(self):\n        A = self.create_model(\"A\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n        B = self.create_model(\"B\", proxy=True, bases=(A,))\n        self.assertRelated(A, [B])\n        self.assertRelated(B, [])\n\n", "    def test_m2m_through_abstract_model(self):\n        class AbstractT(models.Model):\n            abstract = True\n            a = models.ForeignKey('A', models.CASCADE)\n            b = models.ForeignKey('B', models.CASCADE)\n\n        A = self.create_model(\"A\", foreign_keys=[models.ManyToManyField('B', through='T')])\n        B = self.create_model(\"B\")\n        T = self.create_model(\"T\", bases=(AbstractT,))\n        self.assertRelated(A, [B, T])\n        self.assertRelated(B, [A, T])\n        self.assertRelated(T, [A, B])\n", "    def test_abstract_model_with_proxy_base_and_fk(self):\n        class AbstractModel(models.Model):\n            pass\n        \n            class Meta:\n                abstract = True\n                app_label = 'migrations'\n\n        class ConcreteModel(AbstractModel):\n            field = models.CharField(max_length=100)\n\n            class Meta:\n                app_label = 'migrations'\n\n        class ProxyModel(ConcreteModel):\n            class Meta:\n                app_label = 'migrations'\n                proxy = True\n\n        class ModelWithFK(models.Model):\n            foreign_key = models.ForeignKey(ProxyModel, on_delete=models.CASCADE)\n\n            class Meta:\n                app_label = 'migrations'\n\n        self.assertRelated(AbstractModel, [ConcreteModel, ProxyModel, ModelWithFK])\n        self.assertRelated(ConcreteModel, [ProxyModel, ModelWithFK])\n        self.assertRelated(ProxyModel, [ModelWithFK])\n        self.assertRelated(ModelWithFK, [])\n"], "sample_1181": ["compilation error", "    def test_scipy_array_print():\n        if not np:\n            skip(\"NumPy not installed\")\n\n        from scipy.sparse import csr_matrix\n        sparse_matrix = csr_matrix([[1, 0, 0], [0, 2, 3], [0, 0, 0]])\n        printer = SciPyPrinter()\n        assert printer.doprint(sparse_matrix) == 'scipy.sparse.csr_matrix((array([1, 2, 3]), array([0, 1, 2])), shape=(3, 3))'\n", "compilation error", "    def test_scipy_integral():\n        if not import_module('scipy'):\n            skip(\"SciPy not installed\")\n\n        x = Symbol('x')\n        e = Integral(x**2, (x, 0, 1))\n        f = lambdify((), e, 'scipy')\n        assert np.isclose(f(), 1/3)\n", "def test_issue_21099():\n    if not np:\n        skip(\"NumPy not installed\")\n\n    M = MatrixSymbol(\"M\", 2, 2)\n    expr = M[0, 0]\n    f = lambdify(M, expr)\n    ma = np.array([[1, 2], [3, 4]])\n    assert f(ma) == 1\n"], "sample_435": ["    def test_invalid_password(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {\"password1\": \"test\", \"password2\": \"test\"}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(\n            form.errors[\"password1\"],\n            [\n                \"Password must meet these requirements: Minimum length: 8; Maximum length: 128; At least 1 uppercase letter; At least 1 lowercase letter; At least 1 digit; At least 1 special character.\"\n            ],\n        )\n", "    def test_password_validation(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {\"password1\": \"test\", \"password2\": \"test\"}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(\n            form[\"password1\"].errors,\n            [_(\"The password must contain at least 8 characters.\")],\n        )\n        self.assertEqual(form[\"password2\"].errors, [])\n        self.assertEqual(form.changed_data, [\"password\"])\n\n", "    def test_html_autocomplete_attributes_with_prefix(self):\n        user = User.objects.get(username=\"testclient\")\n        form = AdminPasswordChangeForm(user, prefix='my_prefix')\n        tests = (\n            (\"password1\", \"new-password\"),\n            (\"password2\", \"new-password\"),\n        )\n        for field_name, autocomplete in tests:\n            with self.subTest(field_name=field_name, autocomplete=autocomplete):\n                self.assertEqual(\n                    form.fields[f'{form.prefix}-{field_name}'].widget.attrs[\"autocomplete\"], autocomplete\n                )\n", "    def test_password_reset_request_form_invalid_token(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {\"uid\": str(user.pk), \"token\": \"invalid-token\"}\n        form = PasswordResetForm(data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['token'], [_(\"Invalid token.\")]) \n", "    def test_empty_password(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {\"password1\": \"\", \"password2\": \"\"}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors[\"password1\"], [Field.default_error_messages[\"required\"]])\n        self.assertEqual(form.errors[\"password2\"], [Field.default_error_messages[\"required\"]])\n"], "sample_137": ["    def test_replace_named_groups(self):\n        pattern = r\"^(?P<a>\\w+)/b/(\\w+)$\"\n        expected_pattern = r\"^<a>/b/<var>$\"\n        self.assertEqual(replace_named_groups(pattern), expected_pattern)\n\n        pattern = r\"^(?P<a>\\w+)/b/(?P<c>\\w+)$\"\n        expected_pattern = r\"^<a>/b/<c>$\"\n        self.assertEqual(replace_named_groups(pattern), expected_pattern)\n\n        pattern = r\"^(?P<a>\\w+)/b/(\\w+)$\"\n        expected_pattern = r\"^<a>/b/(\\w+)$\"\n        self.assertEqual(replace_named_groups(pattern), expected_pattern)\n\n        pattern = r\"^(?P<a>\\w+)/b/(?P<c>\\w+)$\"\n        expected_pattern = r\"^<a>/b/<c>$\"\n        self.assertEqual(replace_named_groups(pattern), expected_pattern)\n\n        pattern = r\"^(?P<a>\\w+) /(?P<b>\\w+)$\"\n        expected_pattern = r\"^<a> /<b>$\"\n        self.assertEqual(replace_named_groups(pattern), expected_pattern)\n", "    def test_replace_unnamed_groups(self):\n        self.assertEqual(replace_unnamed_groups('(?P<a>\\w+)/b/(\\w+)'), '(?P<a>\\w+)/b/<var>')\n        self.assertEqual(replace_unnamed_groups('(?P<a>\\w+)/b((x|y)\\w+)'), '(?P<a>\\w+)/b/<var>')\n        self.assertEqual(replace_unnamed_groups('(?P<a>\\w+)/b/(x|y)\\w+'), '(?P<a>\\w+)/b/<var>')\n        self.assertEqual(replace_unnamed_groups('(?P<a>\\w+)/b/(\\w+)(x|y)'), '(?P<a>\\w+)/b/<var>(x|y)')\n        self.assertEqual(replace_unnamed_groups('^(?P<a>\\w+)/b/(\\w+)$'), '^(?P<a>\\w+)/b/<var>$')\n        self.assertEqual(replace_unnamed_groups('^(?P<a>\\w+)/b/(x|y)\\w+$'), '^(?P<a>\\w+)/b/<var>$')\n        self.assertEqual(replace_unnamed_groups('(?P<a>\\w+)/(?P<b>\\w+)/(?P<c>\\w+)'), '(?P<a>\\w+)/(?P<b>\\w+)/(?P<c>\\w+)')\n\n", "    def test_replace_named_groups(self):\n        self.assertEqual(replace_named_groups('(?P<a>\\w+)/b/(\\w+)$'), '^<a>/b/<var>$')\n        self.assertEqual(replace_named_groups('(?P<a>\\w+)/b/(?P<c>\\w+)$'), '^<a>/b/<c>$')\n        self.assertEqual(replace_named_groups('(?P<a>\\w+)/b/(\\w+)'), '^<a>/b/<var>')\n        self.assertEqual(replace_named_groups('(?P<a>\\w+)/b/(?P<c>\\w+)'), '^<a>/b/<c>')\n        self.assertEqual(replace_named_groups('^(?P<a>(x|y))/b'), '^<a>/b')\n", "    def test_replace_named_groups(self):\n        self.assertEqual(replace_named_groups(r'^(\\w+)/b/(\\w+)$'), r'^<var1>/b/<var2>$')\n        self.assertEqual(replace_named_groups(r'^(\\w+)/b/(?P<c>\\w+)$'), r'^<var1>/b/<c>$')\n        self.assertEqual(replace_named_groups(r'^(\\w+)/b/(\\w+)'), r'^<var1>/b/<var2>$')\n        self.assertEqual(replace_named_groups(r'^(\\w+)/b/(?P<c>\\w+)'), r'^<var1>/b/<c>$')\n        self.assertEqual(replace_named_groups(r'^(?P<a>(x|y))/b'), r'^<a>/b')\n", "    def test_replace_unnamed_groups(self):\n        self.assertEqual(replace_unnamed_groups(r'^(\\w+)/b/(\\w+)$'), r'^(\\w+)/b/<var>$')\n        self.assertEqual(replace_unnamed_groups(r'^(\\w+)/b/((x|y)\\w+)$'), r'^(\\w+)/b/<var>$')\n        self.assertEqual(replace_unnamed_groups(r'^(\\w+)/b/(\\w+)'), r'^(\\w+)/b/<var>')\n        self.assertEqual(replace_unnamed_groups(r'^(\\w+)/b/((x|y)\\w+)'), r'^(\\w+)/b/<var>')\n"], "sample_957": ["    def test_restify_type_hints_special_form():\n        from typing import _GenericAlias\n        class MyClass:\n            pass\n        MyClassT = _GenericAlias(MyClass, (int,))\n        assert restify(MyClassT) == \":class:`tests.test_util_typing.MyClass`\\\\ [int]\"\n", "    def test_restify_type_hints_union_operator():\n        assert restify(int | None) == \":obj:`~typing.Optional`\\\\ [:class:`int`]\"  # type: ignore\n        assert restify(int | str) == \":class:`int` | :class:`str`\"  # type: ignore\n        assert restify(int | str | None) == \":obj:`~typing.Optional`\\\\ [:class:`int` | :class:`str`]\"  # type: ignore\n\n", "    def test_stringify_type_hints_params():\n            pass\n\n        assert stringify(func.__annotations__) == (\n            'Dict[str, Any]'\n        )\n", "    def test_stringify_type_hints_nested_generics():\n        from typing import List, Dict\n\n        assert stringify(List[Dict[str, int]]) == \"List[Dict[str, int]]\"\n        assert stringify(Dict[str, List[int]]) == \"Dict[str, List[int]]\"\n", "    def test_restify_type_Literal():\n        from typing import Literal  # type: ignore\n        assert restify(Literal[1, \"2\", \"\\r\"]) == \":obj:`~typing.Literal`\\\\ [1, '2', '\\\\r']\"\n"], "sample_414": ["    def test_ManyToManyField_with_filter(self):\n        from selenium.webdriver.common.by import By\n        from selenium.webdriver.support.ui import Select\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_event_add\")\n        )\n\n        main_window = self.selenium.current_window_handle\n        # Add a new band \n        self.selenium.find_element(By.ID, \"add_id_bands\").click()\n        self.wait_for_and_switch_to_popup()\n        band_name_field = self.selenium.find_element(By.ID, \"id_name\")\n        band_name_field.send_keys(\"New Band\")\n        save_button_css_selector = \".submit-row > input[type=submit]\"\n        self.selenium.find_element(By.CSS_SELECTOR, save_button_css_selector).click()\n        self.selenium.switch_to.window(main_window)\n        # Select the new band in the supporting bands field\n        self.selenium.find_element(By.ID, \"lookup_id_supporting_bands\").click()\n        self.wait_for_and_switch_to_popup()\n        link = self.selenium.find_element(By.LINK_TEXT, \"New Band\")\n        link.click()\n        self.selenium.switch_to.window(main_window)\n        self.wait_for_value(\"#id_supporting_bands\", \"100\")\n        \n", "    def test_ManyToManyField_using_to_field(self):\n        from selenium.webdriver.common.by import By\n        from selenium.webdriver.support.ui import Select\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_event_add\")\n        )\n\n        main_window = self.selenium.current_window_handle\n        # Click the Add Band button to add new\n\n        self.selenium.find_element(By.ID, \"add_id_bands\").click()\n        self.wait_for_and_switch_to_popup()\n        name_field = self.selenium.find_element(By.ID, \"id_name\")\n        name_field.send_keys(\"newband\")\n\n        save_button_css_selector = \".submit-row > input[type=submit]\"\n        self.selenium.find_element(By.CSS_SELECTOR, save_button_css_selector).click()\n        self.selenium.switch_to.window(main_window)\n\n        select = Select(self.selenium.find_element(By.ID, \"id_bands\"))\n        select.select_by_visible_text(\"newband\")\n        # Click the Change Band button to change it\n        self.selenium.find_element(By.ID, \"change_id_bands\").click()\n        self.wait_for_and_switch_to_popup()\n\n        name_field = self.selenium.find_element(By.ID, \"id_name\")\n        name_field.clear()\n        name_field.send_keys(\"changednewband\")\n\n        save_button_css_selector = \".submit-row > input[type=submit]\"\n        self.selenium.find_element(By.CSS_SELECTOR, save_button_css_selector).click()\n        self.selenium.switch_to.window(main_window)\n\n        select = Select(self.selenium.find_element(By.ID, \"id_bands\"))\n        select.select_by_visible_text(\"changednewband\")\n\n        # Go ahead and submit the form to make sure it works\n", "    def test_ManyToManyField(self):\n        from selenium.webdriver.common.by import By\n        from selenium.webdriver.support.ui import Select\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_event_add\")\n        )\n\n        main_window = self.selenium.current_window_handle\n\n        # Click the Add Band button to add new band\n        self.selenium.find_element(By.ID, \"add_id_bands\").click()\n        self.wait_for_and_switch_to_popup()\n\n        band_name_field = self.selenium.find_element(By.ID, \"id_name\")\n        band_name_field.send_keys(\"New Band\")\n\n        save_button_css_selector = \".submit-row > input[type=submit]\"\n        self.selenium.find_element(By.CSS_SELECTOR, save_button_css_selector).click()\n        self.selenium.switch_to.window(main_window)\n\n        # The field now contains the new band\n        self.selenium.find_element(By.CSS_SELECTOR, \"#id_bands option[value=New Band]\")\n\n        self.selenium.find_element(By.ID, \"view_id_bands\").click()\n        self.wait_for_value(\"#id_name\", \"New Band\")\n        self.selenium.back()\n\n        select = Select(self.selenium.find_element(By.ID, \"id_bands\"))\n        select.select_by_visible_text(\"New Band\")\n        # Click the Change Band button to change it\n        self.selenium.find_element(By.ID, \"change_id_bands\").click()\n        self.wait_for_and_switch_to_popup()\n\n        name_field = self.selenium.find_element(By.ID, \"id_name\")\n        name_field.clear()\n        name_field.send_keys(\"Changed Band Name\")\n\n        save_button_css_selector = \".submit-row > input[type=submit]\"\n        self.selenium.find_element(By", "    def test_ManyToMany_using_to_field(self):\n        from selenium.webdriver.common.by import By\n        from selenium.webdriver.support.ui import Select\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_event_add\")\n        )\n\n        main_window = self.selenium.current_window_handle\n        # Click the Add Band button to add new\n        self.selenium.find_element(By.ID, \"add_id_supporting_bands\").click()\n        self.wait_for_and_switch_to_popup()\n        self.selenium.find_element(By.ID, \"id_name\").send_keys(\"New Band\")\n\n        save_button_css_selector = \".submit-row > input[type=submit]\"\n        self.selenium.find_element(By.CSS_SELECTOR, save_button_css_selector).click()\n        self.selenium.switch_to.window(main_window)\n        # The field now contains the new band\n\n        select = Select(self.selenium.find_element(By.ID, \"id_supporting_bands\"))\n        select.select_by_visible_text(\"New Band\")\n        # Click the Change Band button to change it\n        self.selenium.find_element(By.ID, \"change_id_supporting_bands\").click()\n        self.wait_for_and_switch_to_popup()\n\n        self.selenium.find_element(By.ID, \"id_name\").clear()\n        self.selenium.find_element(By.ID, \"id_name\").send_keys(\"Changed New Band\")\n\n        save_button_css_selector = \".submit-row > input[type=submit]\"\n        self.selenium.find_element(By.CSS_SELECTOR, save_button_css_selector).click()\n\n        self.selenium.switch_to.window(main_window)\n\n        select = Select(self.selenium.find_element(By.ID, \"id_supporting_bands\"))\n        select.select_by_visible_text(\"Changed New Band\")\n\n        self.selenium", "    def test_ManyToManyField_using_to_field(self):\n        from selenium.webdriver.common.by import By\n        from selenium.webdriver.support.ui import Select\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_event_add\")\n        )\n\n        main_window = self.selenium.current_window_handle\n        \n        # Click the Add Band button to add new\n        self.selenium.find_element(By.ID, \"add_id_bands\").click()\n        self.wait_for_and_switch_to_popup()\n        name_field = self.selenium.find_element(By.ID, \"id_name\")\n        name_field.send_keys(\"newband\")\n\n        save_button_css_selector = \".submit-row > input[type=submit]\"\n        self.selenium.find_element(By.CSS_SELECTOR, save_button_css_selector).click()\n        self.selenium.switch_to.window(main_window)\n        # The field now contains the new band\n        self.selenium.find_element(By.CSS_SELECTOR, \"#id_bands option[value=newband]\")\n\n        self.selenium.find_element(By.ID, \"view_id_bands\").click()\n        self.wait_for_value(\"#id_name\", \"newband\")\n        self.selenium.back()\n\n        select = Select(self.selenium.find_element(By.ID, \"id_bands\"))\n        select.select_by_value(\"newband\")\n        # Click the Change Band button to change it\n        self.selenium.find_element(By.ID, \"change_id_bands\").click()\n        self.wait_for_and_switch_to_popup()\n\n        name_field = self.selenium.find_element(By.ID, \"id_name\")\n        name_value = \"changednewband\"\n        name_field.clear()\n        name_field.send_keys(name_value)\n\n        save_button_css_selector = \".submit-row > input[type=submit"], "sample_720": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_828": ["def test_pairwise_distances_non_numeric():\n    # Check that pairwise_distances raises a TypeError for non-numeric data.\n    X = np.array(['a', 'b', 'c'])\n    Y = np.array(['d', 'e', 'f'])\n    with pytest.raises(TypeError):\n        pairwise_distances(X, Y)\n", "    def test_pairwise_distances_chunked_sparse_output(self, X_array_constr,\n                                                    Y_array_constr):\n        rng = np.random.RandomState(0)\n        X = rng.random_sample((100, 10))\n        Y = rng.random_sample((10, 10))\n        X = X_array_constr(X)\n        Y = Y_array_constr(Y)\n\n        # Test with dense output\n        distances_dense = pairwise_distances_chunked(X, Y,\n                                                     dense_output=True)\n\n        # Test with sparse output\n        distances_sparse = pairwise_distances_chunked(X, Y,\n                                                     dense_output=False)\n\n        assert_allclose(distances_dense, distances_sparse.todense())\n", "    def test_pairwise_distances_chunked_sparse_input(self):\n        rng = np.random.RandomState(0)\n        X = rng.random_sample((100, 10))\n        Y = rng.random_sample((50, 10))\n        X_sparse = csr_matrix(X)\n        Y_sparse = csr_matrix(Y)\n\n        self.assert_allclose(\n            pairwise_distances_chunked(X_sparse, Y_sparse,\n                                      metric='euclidean'),\n            pairwise_distances(X, Y, metric='euclidean'),\n        )\n", "    def test_pairwise_distances_chunked_empty_arrays(self):\n        # Test pairwise_distances_chunked with empty input arrays.\n        X = np.array([])\n        Y = np.array([])\n        with pytest.raises(ValueError):\n            pairwise_distances_chunked(X, Y, metric='euclidean')\n", "    def test_check_invalid_input_type():\n        # Ensures that an error is raised for non-numerical input.\n        XA = np.array([[1, 2], [3, 4]])\n        XB = [['a', 'b'], ['c', 'd']]\n        assert_raises(TypeError, check_pairwise_arrays, XA, XB)\n"], "sample_586": ["    def test_concat_coords_overlap(self):\n        ds1 = Dataset(\n            {\"a\": ((\"x\", \"y\"), [[0, 1], [2, 3]]), \"x\": [0, 1], \"y\": [0, 1]},\n            coords={\"z\": (\"x\", [0, 1])},\n        )\n        ds2 = Dataset(\n            {\"a\": ((\"x\", \"y\"), [[4, 5], [6, 7]]), \"x\": [1, 2], \"y\": [0, 1]},\n            coords={\"z\": (\"x\", [1, 2])},\n        )\n        expected = Dataset(\n            {\"a\": ((\"x\", \"y\"), [[0, 1], [2, 3], [4, 5], [6, 7]]), \"x\": [0, 1, 1, 2], \"y\": [0, 1, 0, 1]},\n            coords={\"z\": (\"x\", [0, 1, 1, 2])},\n        )\n        actual = concat([ds1, ds2], \"x\")\n        assert_identical(actual, expected)\n", "    def test_concat_positions(self):\n        ds1 = Dataset({\"a\": ((\"x\", \"y\"), [[1, 2], [3, 4]])}, coords={\"x\": [0, 1]})\n        ds2 = Dataset({\"a\": ((\"x\", \"y\"), [[5, 6], [7, 8]])}, coords={\"x\": [1, 2]})\n        expected = Dataset({\"a\": ((\"x\", \"y\"), [[1, 2], [3, 4], [5, 6], [7, 8]])}, coords={\"x\": [0, 1, 2]})\n        actual = concat([ds1, ds2], dim=\"x\", positions=[0,1])\n        assert_identical(actual, expected)\n", "    def test_concat_autoalign_coords(self):\n        ds1 = Dataset(\n            {\"foo\": ((\"x\", \"y\"), [[1, 2], [3, 4]]), \"x\": [0, 1]},\n            coords={\"y\": [10, 20]},\n        )\n        ds2 = Dataset(\n            {\"foo\": ((\"x\", \"y\"), [[5, 6], [7, 8]]), \"x\": [0, 1]},\n            coords={\"y\": [10, 30]},\n        )\n        actual = concat([ds1, ds2], \"z\")\n        expected = Dataset(\n            {\n                \"foo\": ((\"z\", \"x\", \"y\"), [[[1, 2], [3, 4]], [[5, 6], [7, np.nan]]]),\n                \"x\": [0, 1, 0, 1],\n                \"y\": [10, 20, 10, 30],\n            }\n        )\n        assert_identical(actual, expected) \n", "    def test_concat_coord_not_in_all_datasets(self):\n        ds1 = Dataset({\"foo\": (\"x\", [1, 2])}, coords={\"x\": [0, 1], \"y\": 0})\n        ds2 = Dataset({\"foo\": (\"x\", [3, 4])}, coords={\"x\": [1, 2]})\n        with pytest.raises(ValueError):\n            concat([ds1, ds2], dim=\"x\")\n\n", "    def test_concat_dim_name_conflict(self):\n        ds1 = Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n        ds2 = Dataset({\"a\": (\"y\", [3, 4]), \"y\": [0, 1]})\n        with pytest.raises(ValueError, match=\"conflicting dimensions\"):\n            concat([ds1, ds2], dim=\"x\")\n"], "sample_290": ["    def test_operation_with_multiple_models_in_operation(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    'User',\n                    fields=[\n                        ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                        ('name', models.CharField(max_length=255)),\n                    ],\n                ),\n                migrations.CreateModel(\n                    'Group',\n                    fields=[\n                        ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                        ('name', models.CharField(max_length=255)),\n                    ],\n                ),\n                migrations.AddField(\n                    model_name='user',\n                    name='groups',\n                    field=models.ManyToManyField(to='test_app.Group'),\n                ),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'user_group')\n", "    def test_mti_inheritance_model_addition(self):\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n        ])\n        Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n        changes = self.get_changes([Animal], [Animal, Dog])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['CreateModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='Dog')\n", "    def test_add_model_with_field_removed_from_base_model_with_same_name(self):\n        \"\"\"\n        If a base model field is removed and a new model inherits from it\n        with a field of the same name, the new field should be added.\n        \"\"\"\n        before = [\n            ModelState('app', 'readable', [\n                ('id', models.AutoField(primary_key=True)),\n                ('title', models.CharField(max_length=200)),\n            ]),\n        ]\n        after = [\n            ModelState('app', 'readable', [\n                ('id', models.AutoField(primary_key=True)),\n            ]),\n            ModelState('app', 'book', [\n                ('title', models.CharField(max_length=200)),\n            ], bases=('app.readable',)),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'CreateModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='title', model_name='readable')\n        self.assertOperationAttributes(changes, 'app', 0, 1, name='book')\n\n", "    def test_empty_operations(self):\n        class Migration(migrations.Migration):\n            operations = []\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertIs(migration.suggest_name().startswith('auto_'), True)\n\n", "    def test_multiple_create_models_same_name_different_app_label(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Common', fields=[], app_label='app1'),\n                migrations.CreateModel('Common', fields=[], app_label='app2'),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'app1_common_app2_common')\n"], "sample_832": ["    def test_ard_threshold_lambda():\n        # Test ARDRegression with threshold_lambda parameter\n        X = np.array([[1, 0], [0, 1], [1, 1]])\n        y = np.array([1, 1, 2])\n        clf = ARDRegression(threshold_lambda=1e-1)\n        clf.fit(X, y)\n        assert clf.lambda_.shape == (2,)\n        assert np.all(clf.lambda_ > 0)\n\n", "compilation error", "def test_ard_regression_empty_input():\n    # Test ARDRegression on empty input\n    X = np.array([])\n    y = np.array([])\n    clf = ARDRegression()\n    assert_raises(ValueError, clf.fit, X, y)\n", "    def test_bayesian_ridge_alpha_prior():\n        # Test if BayesianRidge correctly handles different alpha priors\n        X = np.array([[1, 1], [3, 4], [5, 7], [4, 1], [2, 6], [3, 10], [3, 2]])\n        y = np.array([1, 2, 3, 2, 0, 4, 5]).T\n        \n        # Test with different alpha_1 and alpha_2 values\n        alphas_to_test = [(1e-6, 1e-6), (1, 1), (10, 1)]\n        for alpha_1, alpha_2 in alphas_to_test:\n            clf = BayesianRidge(alpha_1=alpha_1, alpha_2=alpha_2, compute_score=True)\n            clf.fit(X, y)\n            \n            # Assert that alpha value is within a reasonable range\n            assert 0 < clf.alpha_ < 1e5\n", "    def test_ard_regression_normalization():\n        # Test ARDRegression with and without normalization\n        # The results should be similar\n        X = np.array([[1, 1], [2, 1], [3, 1], [4, 1], [5, 1]])\n        y = np.array([1, 2, 3, 4, 5])\n\n        clf_with_normalization = ARDRegression(normalize=True)\n        clf_with_normalization.fit(X, y)\n\n        clf_without_normalization = ARDRegression(normalize=False)\n        clf_without_normalization.fit(X, y)\n\n        y_pred_with_normalization = clf_with_normalization.predict(X)\n        y_pred_without_normalization = clf_without_normalization.predict(X)\n\n        assert_array_almost_equal(\n            y_pred_with_normalization, y_pred_without_normalization, decimal=2\n        )\n"], "sample_1141": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1165": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_quaternion_equality():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(1, 2, 3, 4)\n    q3 = Quaternion(2, 3, 4, 5)\n    assert q1 == q2\n    assert q1 != q3\n\n"], "sample_211": ["    def test_get_context_data_object(self):\n        test_view = views.CustomSingleObjectView()\n        context = test_view.get_context_data()\n        self.assertEqual(context['object'], test_view.object)\n", "    def test_single_object_template_context(self):\n        test_view = views.SingleObjectTemplateResponseView()\n        request = self.rf.get('/')\n        response = test_view(request)\n        self.assertEqual(response.status_code, 200)\n        self.assertIn('object', response.context_data)\n        self.assertEqual(response.context_data['object'], test_view.object)\n", "    def test_object_at_custom_name_in_template(self):\n        test_view = views.CustomSingleObjectView()\n        test_view.context_object_name = 'pony'\n        response = test_view.render_to_response({})\n        self.assertContains(response, 'pony')\n", "    def test_object_at_custom_name_in_context_data(self):\n        # Checks 'pony' key presence in dict returned by get_context_date\n        test_view = views.CustomSingleObjectView()\n        test_view.context_object_name = 'pony'\n        context = test_view.get_context_data()\n        self.assertEqual(context['pony'], test_view.object)\n\n", "    def test_get_context_data_object_name(self):\n        test_view = views.CustomSingleObjectView()\n        test_view.context_object_name = 'custom_object'\n        context = test_view.get_context_data()\n        self.assertEqual(context['custom_object'], test_view.object)\n"], "sample_15": ["    def test_erf_at_scalar(self, function):\n        x = 1.0 * u.percent\n        result = np.empty(1)\n        function.at(x, result)\n        assert result[0] == function(x)\n\n\n\n", "    def test_scipy_registration_after_import():\n        \"\"\"Check that scipy is lazily loaded upon first use.\"\"\"\n        assert sps.erf not in qh.UFUNC_HELPERS\n        import scipy\n        assert sps.erf not in qh.UFUNC_HELPERS\n        sps.erf(1.0 * u.percent)\n        assert sps.erf in qh.UFUNC_HELPERS\n        assert sps.erf in qh.UFUNC_HELPERS\n\n", "        def test_erf_inplace(self, function):\n            q1 = 1.0 * u.percent\n            function(q1, out=q1)\n            q2 = function(1.0 * u.percent)\n            assert_allclose(q1.value, q2.value)\n            assert q1.unit == q2.unit\n", "    def test_jv_units(self):\n        with pytest.raises(u.UnitsError):\n            sps.jv(1.0 * u.m, 1.0 * u.s)\n\n", "        def test_jv_invalid_orders(self, function):\n            with pytest.raises(ValueError):\n                function(1.0 * u.dimensionless_unscaled, -1.0 * u.dimensionless_unscaled)\n"], "sample_1178": ["def test_IfElse():\n    c1 = CodeBlock(Assignment(x,1))\n    c2 = CodeBlock(Assignment(x, 2))\n    ifs = IfElse(x < 2, c1, c2)\n    assert ifs.condition.args[0] == x\n    assert ifs.condition.args[1] == 2\n    assert ifs.then_block == c1\n    assert ifs.else_block == c2\n    assert ifs.func(*ifs.args) == ifs\n    assert ifs == IfElse(x < 2, c1, c2)\n    assert ifs != IfElse(x > 2, c1, c2)\n    assert ifs != IfElse(x < 2, c2, c1)\n\n", "def test_ast_replace_nested():\n    x = Variable('x', real)\n    y = Variable('y', real)\n    n = Variable('n', integer)\n\n    fcall = FunctionCall('pwer', [x, n])\n    body = [Assignment(y, fcall), Return(y)]\n    pwer = FunctionDefinition(real, 'pwer', [x, n], body)\n\n    tree = CodeBlock(pwer)\n\n    tree2 = tree.replace(fcall, FunctionCall('power', [x, n]))\n    assert str(tree.args[0].body.args[0].function_args[0]) == 'pwer'\n    assert str(tree2.args[0].body.args[0].function_args[0]) == 'power'\n", "def test_Pointer_arithmetic():\n    i = symbols('i', integer=True)\n    p = Pointer(x)\n    p_inc = p + 1\n    assert p_inc.symbol == x\n    assert isinstance(p_inc.offset, Integer)\n    assert p_inc.offset == 1\n\n    p_dec = p - 1\n    assert p_dec.symbol == x\n    assert p_dec.offset == -1\n\n    p_i = p[i]\n    assert p_i.indices == (i,)\n    assert isinstance(p_i, Variable)\n    assert p_i.symbol == x\n\n    p_2i = p[2*i]\n\n\n\n    assert p_2i.indices == (2*i,)\n    assert isinstance(p_2i, Variable)\n    assert p_2i.symbol == x\n\n    # Test pointer dereferencing\n    deref = *p\n    assert deref == x\n", "compilation error", "compilation error"], "sample_33": ["compilation error", "compilation error", "compilation error", "    def test_shaped_like_ndarray():\n        class TestShaped(misc.ShapedLikeNDArray):\n                self._data = data\n                \n\n            @property\n                return self._data.shape\n\n                return type(self)(getattr(self._data, method)(*args, **kwargs))\n\n        data = np.arange(12).reshape(3, 4)\n        instance = TestShaped(data)\n        assert instance.shape == (3, 4)\n        assert instance.ndim == 2\n        assert instance.size == 12\n        assert (instance.reshape((6, 2))._data == data.reshape((6, 2))).all()\n        assert (instance.swapaxes(0, 1)._data == data.swapaxes(0, 1)).all()\n        assert instance.isscalar is False\n        assert len(instance) == 3\n", "compilation error"], "sample_464": ["    def test_file_like_object_without_tell(self):\n        class TestFileLike:\n                self.content = content\n                self.pos = 0\n\n                if size == -1:\n                    result = self.content[self.pos :]\n                    self.pos = len(self.content)\n                    return result\n                result = self.content[self.pos : self.pos + size]\n                self.pos += size\n                return result\n\n                return False\n\n        file_like = TestFileLike(b'binary content')\n        response = FileResponse(file_like)\n        self.assertEqual(list(response), [b'binary content'])\n", "    def test_file_response_not_seekable(self):\n        class NotSeekableFile:\n                self.content = content\n                self.tell_result = 0\n\n                if n_bytes == -1:\n                    return self.content\n                return self.content[:n_bytes]\n\n                raise io.UnsupportedOperation(\"Seek is not supported\")\n\n                return False\n\n                return self.tell_result\n\n        file = NotSeekableFile(b\"binary content\")\n        response = FileResponse(file)\n        self.assertEqual(list(response), [b\"binary content\"])\n", "    def test_file_like_object_with_tell_and_seek(self):\n        class TestFileLike:\n                self.data = data\n                self.pos = 0\n\n                if size == -1:\n                    result = self.data[self.pos:]\n                    self.pos = len(self.data)\n                    return result\n                result = self.data[self.pos : self.pos + size]\n                self.pos += size\n                return result\n\n                return self.pos\n\n                if whence == 0:\n                    self.pos = offset\n                elif whence == 1:\n                    self.pos += offset\n                elif whence == 2:\n                    self.pos = len(self.data) + offset\n                else:\n                    raise\n\n        test_data = b\"binary content\"\n        file_like_obj = TestFileLike(test_data)\n        response = FileResponse(file_like_obj)\n\n        # Ensure the response object handles seeking correctly\n        self.assertEqual(list(response), [test_data])\n\n        # Check that Content-Length is set correctly\n        self.assertEqual(response.headers[\"Content-Length\"], str(len(test_data)))\n", "    def test_streaming_response_with_close(self):\n        with tempfile.NamedTemporaryFile() as tmp:\n            response = FileResponse(tmp)\n            response.close()\n            with self.assertRaises(ValueError):\n                list(response) \n", "    def test_file_response_with_none_content_type(self):\n        response = FileResponse(open(__file__, \"rb\"))\n        response.close()\n        self.assertIn(response.headers[\"Content-Type\"], [\"text/x-python\", \"text/plain\"])\n"], "sample_153": ["    def test_postgres_default_timezone(self):\n        with mock.patch('django.db.backends.postgresql.base.Database.get_timezone',\n                        return_value='UTC'):\n            self.assertEqual(check_database_backends(databases=self.databases), [])\n\n        with mock.patch('django.db.backends.postgresql.base.Database.get_timezone',\n                             return_value='some_timezone'):\n            result = check_database_backends(databases=self.databases)\n            self.assertEqual(len(result), 1)\n            self.assertEqual(result[0].id, 'postgres.W004')  \n", "    def test_postgresql_extensions(self):\n        with mock.patch(\n            'django.db.backends.postgresql.creation.DatabaseCreation.extension_name',\n            create=True,\n        ):\n            # Test with no extensions installed\n            with mock.patch(\n                'django.db.backends.postgresql.creation.DatabaseCreation._get_extension_status',\n                create=True,\n                return_value=False\n            ):\n                result = check_database_backends(databases=self.databases)\n                self.assertEqual(len(result), 2)\n                self.assertEqual([r.id for r in result], ['postgresql.W001', 'postgresql.W001'])\n            # Test with extensions installed\n            with mock.patch(\n                'django.db.backends.postgresql.creation.DatabaseCreation._get_extension_status',\n                create=True,\n                return_value=True\n            ):\n                result = check_database_backends(databases=self.databases)\n                self.assertEqual(len(result), 0)\n", "    def test_postgres_extensions(self):\n        with mock.patch(\n            'django.db.backends.postgresql.creation.DatabaseCreation.get_postgres_version',\n            return_value=(15, 0, 0)\n        ) as mock_version:\n            # Extensions should be required\n            check_database_backends(databases=self.databases)\n            mock_version.assert_called_once()\n", "    def test_postgresql_default_settings(self):\n        with mock.patch('django.db.backends.postgresql.base.DatabaseWrapper.get_db_version', create=True,\n                        return_value=(14, 0, 0)):\n            # Check that no warnings are issued for supported PostgreSQL versions\n            self.assertEqual(check_database_backends(databases=self.databases), [])\n\n", "    def test_postgresql_default_tablespace(self):\n        with mock.patch(\n            'django.db.backends.postgresql.creation.DatabaseCreation.get_database_schemas',\n            return_value=[\n                {'name': 'public', 'tablespace': 'pg_default'},\n                {'name': 'another_schema', 'tablespace': 'custom_tablespace'},\n            ],\n        ):\n            self.assertEqual(check_database_backends(databases=self.databases), [])\n\n        with mock.patch(\n            'django.db.backends.postgresql.creation.DatabaseCreation.get_database_schemas',\n            return_value=[\n                {'name': 'public', 'tablespace': 'custom_tablespace'},\n            ],\n        ):\n            result = check_database_backends(databases=self.databases)\n            self.assertEqual(len(result), 2)\n            self.assertEqual([r.id for r in result], ['postgresql.W001', 'postgresql.W001'])\n"], "sample_856": ["compilation error", "def test_time_series_split_empty_sequences():\n    X = []\n    tscv = TimeSeriesSplit(n_splits=2)\n\n    # Should work even if the data is empty\n    splits = tscv.split(X)\n\n    train, test = next(splits, ([], []))\n    assert_array_equal(train, [])\n    assert_array_equal(test, [])\n\n    train, test = next(splits, ([], []))\n    assert_array_equal(train, [])\n    assert_array_equal(test, [])\n", "compilation error", "def test_groupkfold_n_splits_error():\n    n_samples = 10\n    groups = np.array(['A'] * 5 + ['B'] * 5)\n    n_splits = 6\n\n    with pytest.raises(ValueError) as excinfo:\n        GroupKFold(n_splits=n_splits).split(np.zeros(n_samples),\n                                            groups=groups)\n\n    assert \"The number of splits, n_splits=6, must be less than or equal to the\" \\\n           \" number of groups, n_groups=2\" in str(excinfo.value)\n", "compilation error"], "sample_51": ["    def test_parse_datetime_microseconds(self):\n        self.assertEqual(parse_datetime('2012-04-23T09:15:00.123456'), datetime(2012, 4, 23, 9, 15, 0, 123456))\n", "    def test_parse_datetime_no_tzinfo(self):\n        self.assertEqual(parse_datetime('2012-04-23T09:15:00'), datetime(2012, 4, 23, 9, 15))\n", "    def test_parse_datetime_invalid_format(self):\n        invalid_inputs = (\n            '2012-04-23T09:15',\n            '2012-04-23 09:15:00Z',\n            '2012-04-23T09:15:00.123456789',\n            '2012-04-23 09:15:00+0200',\n        )\n        for source in invalid_inputs:\n            with self.subTest(source=source):\n                with self.assertRaises(ValueError):\n                    parse_datetime(source)\n", "    def test_parse_datetime_invalid_format(self):\n        invalid_values = (\n            '2012-04-23T09:15:00Z0',\n            '2012-04-23 09:15:00Z',\n            '2012-04-23T09:15:00+0230',\n            '2012-04-23T09:15:00+02:3',\n            '2012-04-23T09:15:00+02:',\n            '2012-04-23T09:15:00+02000',\n            '2012-04-23 09:15:00.123456789',\n        )\n        for value in invalid_values:\n            with self.subTest(value=value):\n                with self.assertRaises(ValueError):\n                    parse_datetime(value)\n\n", "    def test_parse_datetime_invalid_format(self):\n        invalid_inputs = (\n            '2012-04-23T091500',\n            '2012-04-23 09:15:00Z',\n            '2012-4-9T4:8:16+',\n            '2012-04-23T09:15:00+02:60',\n            '2012-04-23T09:15:00-02',\n            '2012-04-23T09:15:00.1234567',\n        )\n        for source in invalid_inputs:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_datetime(source))\n\n"], "sample_1209": ["def test_prefix_latex():\n    assert Prefix('foo', 'bar', 1)._latex(None) == r'\\text{bar}'\n    assert Prefix('foo', 'bar', 1, latex_repr=r\"\\mu\")._latex(None) == r\"\\mu\"\n", "def test_prefix_latex():\n    assert Prefix('micro', 'mu', -6).latex() == r'\\text{mu}'\n    assert Prefix('mega', 'M', 6).latex() == r'M'\n", "def test_prefix_creation():\n    p = Prefix('test', 't', 2, base=2)\n\n    assert p.name == 'test'\n    assert p.abbrev == 't'\n    assert p.exponent == 2\n    assert p.base == 2\n    assert p.scale_factor == 4\n", "def test_latex():\n    assert kilo._latex(None) == r'\\text{k}'\n\n\n", "def test_latex():\n    assert kilo._latex(None) == r'\\text{k}'\n    assert Prefix('micro', 'mu', -6, latex_repr=r\"\\mu\")._latex(None) == r\"\\mu\"\n"], "sample_1039": ["def test_print_Piecewise():\n    pw = Piecewise((x**2, x>0), (x, True))\n    assert mpp.doprint(pw) == '<piecewise><condition><mrow><mi>x</mi><mo>&gt;</mo><mn>0</mn></mrow></condition><mrow><msup><mi>x</mi><mn>2</mn></msup></mrow><otherwise><mi>x</mi></otherwise></piecewise>'\n    assert mp.doprint(pw) == '<piecewise><piece><condition><gt/><ci>x</ci><cn>0</cn></condition><apply><power/><ci>x</ci><cn>2</cn></apply></piece><otherwise><ci>x</ci></otherwise></piecewise>'\n\n", "def test_print_Piecewise():\n    expr = Piecewise((x, x < 1), (x**2, x >= 1))\n    assert mpp.doprint(expr) == '<mrow><mi>piecewise</mi><mfenced><mrow><mi>x</mi><mo>, </mo><mi>x</mi><mo>&lt;</mo><mn>1</mn></mrow><mrow><msup><mi>x</mi><mn>2</mn></msup><mo>, </mo><mi>x</mi><mo>\u2265</mo><mn>1</mn></mrow></mrow></mfenced></mrow>'\n    assert mp.doprint(expr) == '<piecewise><piece><condition><lt/><ci>x</ci><cn>1</cn></lt></condition><apply><ci>x</ci></apply></piece><piece><condition><geq/><ci>x</ci><cn>1</cn></geq></condition><apply><power/><ci>x</ci><cn>2</cn></apply></piece></piecewise>'\n", "compilation error", "def test_print_MatrixElement():\n    A = MatrixSymbol('A', 2, 2)\n    x, y = symbols('x y')\n    expr = MatrixElement(A, x, y)\n\n    assert mpp.doprint(expr) == '<msub><mrow><mi>A</mi></mrow><mn>x</mn><mn>y</mn></msub>'\n    assert mp.doprint(expr) == '<apply><csymbol cd=\"linalg1\">vector_selector</csymbol><ci>A</ci><cn>x</cn><cn>y</cn></apply>'\n", "compilation error"], "sample_411": ["    def test_help_command(self):\n        out, err = self.run_manage([\"help\", \"dance\"])\n        self.assertNoOutput(err)\n        self.assertIn(\"Usage: ./manage.py dance [option... ]\", out)\n\n", "    def test_normalize_path_patterns_removes_empty_strings(self):\n        self.assertEqual(normalize_path_patterns(['foo/bar', '', 'baz/*']),\n                         [os.path.normcase(p) for p in ['foo/bar', 'baz/*']])\n", "    def test_commands_ignore_app_config_errors(self):\n        self.write_settings(\n            \"settings.py\",\n            apps=[\"user_commands\"],\n            sdict={\n                \"INSTALLED_APPS\": ['broken_app']\n            },\n        )\n        # Simulate a broken app config by raising an ImportError.\n        with mock.patch(\"django.apps.apps.get_app_config\", side_effect=ImportError):\n            out, err = self.run_manage([\"hal\"])\n        self.assertNoOutput(err)\n        self.assertIn(\"Dave, I can't do that.\", out.strip())\n\n", "    def test_command_output_encoding(self):\n        with mock.patch('builtins.print') as mock_print:\n            management.call_command('dance', stdout=StringIO())\n        mock_print.assert_called_once_with('I don\\'t feel like dancing Rock\\'n\\'Roll.\\n')\n", "    def test_output_with_color_and_style(self):\n        out = StringIO()\n        with self.settings(FORCE_COLOR='no'):\n            management.call_command(\"dance\", style=\"bold\", stdout=out, verbosity=1)\n        self.assertIn(\"I don't feel like dancing bold.\\n\", out.getvalue())\n"], "sample_1108": ["    def test_cycle_length():\n        assert cycle_length([1, 2, 3, 1]) == 3\n        assert cycle_length([1, 2, 1]) == 2\n        assert cycle_length([1, 2, 3, 4, 1]) == 4\n        assert cycle_length([1]) == 1\n        assert cycle_length([]) == 0\n\n\n", "compilation error", "    def test_runs():\n        assert runs([]) == []\n        assert runs([1]) == [[1]]\n        assert runs([1, 1]) == [[1], [1]]\n        assert runs([1, 1, 2]) == [[1], [1, 2]]\n        assert runs([1, 2, 1]) == [[1, 2], [1]]\n        assert runs([2, 1, 1]) == [[2], [1], [1]]\n        from operator import lt\n        assert runs([2, 1, 1], lt) == [[2, 1], [1]]\n\n\n\n", "compilation error", "compilation error"], "sample_889": ["    def test_calibration_display_with_non_fitted_estimator(pyplot):\n        \"\"\"Check that CalibrationDisplay raises an error if estimator is not fitted.\"\"\"\n        X, y = make_classification(n_samples=100, n_features=5, random_state=42)\n        clf = LogisticRegression()\n\n        with pytest.raises(ValueError, match=\"The estimator needs to be fitted\"):\n            CalibrationDisplay.from_estimator(clf, X, y)\n", "def test_calibration_wrong_sample_weight_length(data):\n    X, y = data\n    clf = LogisticRegression()\n    calibrated_clf = CalibratedClassifierCV(clf)\n    with pytest.raises(ValueError, match=\"sample_weight should have the same length as y\"):\n        calibrated_clf.fit(X, y, sample_weight=np.ones(len(y) - 1))\n\n", "    def test_calibration_with_cv_iterator(data):\n        \"\"\"Check that CalibratedClassifierCV accepts a CV iterator.\"\"\"\n        from sklearn.model_selection import KFold\n        X, y = data\n\n        cv = KFold(n_splits=5)\n\n        clf = CalibratedClassifierCV(LogisticRegression(), cv=cv)\n        clf.fit(X, y)\n\n", "compilation error", "def test_calibration_display_ax_none(pyplot, iris_data_binary):\n    # Check that CalibrationDisplay works with ax=None\n    X, y = iris_data_binary\n    clf = LogisticRegression().fit(X, y)\n    viz = CalibrationDisplay.from_estimator(clf, X, y, ax=None)\n\n    assert isinstance(viz.ax_, matplotlib.axes.Axes)\n    assert viz.ax_.get_xlabel() == \"Mean predicted probability (Positive class: 1)\"\n    assert viz.ax_.get_ylabel() == \"Fraction of positives (Positive class: 1)\"\n\n"], "sample_66": ["    def test_getlist(self):\n        environ = {\n            'HTTP_COOKIE': 'foo=bar; baz=quux',\n            'HTTP_ACCEPT': 'text/html,application/xhtml+xml',\n        }\n        headers = HttpHeaders(environ)\n        self.assertEqual(headers.getlist('Cookie'), ['foo=bar; baz=quux'])\n        self.assertEqual(headers.getlist('Accept'), ['text/html,application/xhtml+xml'])\n        self.assertEqual(headers.getlist('Not-Sent'), [])\n", "    def test_header_casing(self):\n        environ = {\n            'content-type': 'text/html',\n            'content-LeNgTh': '100',\n            'http_host': 'example.com',\n            'x-forwarded-for': '192.168.1.1',\n        }\n        headers = HttpHeaders(environ)\n        self.assertEqual(headers.get('Content-Type'), 'text/html')\n        self.assertEqual(headers.get('Content-Length'), '100')\n        self.assertEqual(headers.get('Host'), 'example.com')\n        self.assertEqual(headers.get('X-Forwarded-For'), '192.168.1.1')\n", "    def test_getlist_header(self):\n        environ = {\n            'HTTP_COOKIE': 'foo=bar;baz=quux',\n            'HTTP_ACCEPT_LANGUAGE': 'en-US,en;q=0.5',\n        }\n        headers = HttpHeaders(environ)\n        self.assertEqual(headers.getlist('Cookie'), ['foo=bar;baz=quux'])\n        self.assertEqual(headers.getlist('Accept-Language'), ['en-US', 'en;q=0.5'])\n", "    def test_http_headers_are_case_insensitive(self):\n        environ = {\n            'http_accept': 'text/html',\n            'CONTENT_LENGTH': '100',\n            'HTTP_HOST': 'example.com',\n        }\n        headers = HttpHeaders(environ)\n        self.assertEqual(headers.get('accept'), 'text/html')\n        self.assertEqual(headers.get('HOST'), 'example.com')\n", "    def test_get_header_with_fallback(self):\n        environ = {'HTTP_HOST': 'example.com'}\n        headers = HttpHeaders(environ)\n        self.assertEqual(headers.get('Host', 'default'), 'example.com')\n        self.assertEqual(headers.get('User-Agent', 'default'), 'default')\n"], "sample_650": ["    def test_log_format_custom_format(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import logging\n\n            logger = logging.getLogger(__name__)\n\n                logger.warning('text')\n                assert False\n            \"\"\"\n        )\n        pytester.makeini(\n            \"\"\"\n            [pytest]\n            log_format=%(levelname)s:%(name)s:%(message)s\n        \"\"\"\n        )\n        result = pytester.runpytest()\n        assert result.ret == 1\n        result.stdout.fnmatch_lines([\"WARNING:__main__:text\"])\n", "def test_log_format_custom_format(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n        logger = logging.getLogger(__name__)\n\n            logger.debug('debug message')\n            logger.info('info message')\n            logger.warning('warning message')\n            assert True\n        \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_format=%(message)s - %(levelname)s - %(asctime)s\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == ExitCode.OK\n    result.stdout.fnmatch_lines(\n        [\n            \"debug message - DEBUG -*\",\n            \"info message - INFO -*\",\n            \"warning message - WARNING -*\",\n        ]\n    )\n", "def test_log_file_cli_subdirectories_are_successfully_created_with_relative_path(\n    pytester: Pytester,", "def test_log_format_custom(pytester: Pytester) -> None:\n    \"\"\"Check that log_format can be customized.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.warning('text')\n            assert False\n        \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_format=%(message)s at %(asctime)s\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == 1\n    result.stdout.fnmatch_lines([\"text at *\"])\n", "def test_log_file_encoding(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n        logger = logging.getLogger(__name__)\n\n            logger.info(\"\u4f60\u597d\")\n        \"\"\"\n    )\n    log_file = str(pytester.path.joinpath(\"pytest.log\"))\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level = INFO\n        log_file_encoding=utf-8\n        \"\"\".format(\n            log_file\n        )\n    )\n    result = pytester.runpytest()\n    assert result.ret == 0\n    with open(log_file, encoding=\"utf-8\") as rfh:\n        content = rfh.read()\n        assert \"\u4f60\u597d\" in content \n"], "sample_369": ["    def test_many_operations_suffix_with_custom_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person1', fields=[]),\n                migrations.CreateModel('Person2', fields=[]),\n                migrations.CreateModel('Person3', fields=[]),\n                migrations.DeleteModel('Person4'),\n                migrations.DeleteModel('Person5'),\n            ]\n            name = 'custom_migration'\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(\n            migration.suggest_name(),\n            'custom_migration',\n        )\n\n", "    def test_operation_with_custom_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[], name='create_person'),\n                migrations.RunSQL('SELECT 1 FROM person;'),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'create_person')\n", "    def test_custom_operation_with_name(self):\n        class CustomOperation(migrations.Operation):\n                super().__init__()\n                self.name = 'custom_op'\n\n                return 'Custom operation'\n\n        class Migration(migrations.Migration):\n            operations = [CustomOperation()]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'custom_op')\n", "    def test_alter_unique_together(self):\n        before = [\n            ModelState('testapp', 'Pizza', [\n                ('id', models.AutoField(primary_key=True)),\n                ('name', models.CharField(max_length=100)),\n                ('toppings', models.ManyToManyField('testapp.Topping')),\n            ])\n        ]\n        after = [\n            ModelState('testapp', 'Pizza', [\n                ('id', models.AutoField(primary_key=True)),\n                ('name', models.CharField(max_length=100)),\n                ('toppings', models.ManyToManyField('testapp.Topping')),\n            ],\n            options={'unique_together': (('name', 'toppings'),)},\n        )\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['AlterUniqueTogether'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='Pizza', unique_together=[('name', 'toppings')])\n", "    def test_delete_model_with_related_fields(self):\n        book = ModelState(\"testapp\", \"Book\", [(\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE))])\n        author = ModelState(\"testapp\", \"Author\", [])\n        changes = self.get_changes([book, author], [book])\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['DeleteModel'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='Author')\n"], "sample_547": ["def test_anchor_align(align):\n    fig, ax = plt.subplots()\n    text = \"annotation\"\n    at = AnchoredText(text, \"upper right\", loc=\"upper right\",\n                      frameon=True, align=align)\n    ax.add_artist(at)\n    fig.canvas.draw()\n    renderer = fig.canvas.get_renderer()\n    bbox = at.get_bbox(renderer)\n    x, y = bbox.verts[0]\n\n    if align == \"left\":\n       assert x == at.patch.get_window_extent(renderer).x0\n    elif align == \"right\":\n       assert x + bbox.width == at.patch.get_window_extent(renderer).x1\n    elif align == \"center\":\n       assert (x + bbox.width / 2) == at.patch.get_window_extent(\n           renderer).x0 + at.patch.get_window_extent(renderer).width / 2\n\n\n", "def test_anchoredtext_pad(loc):\n    fig, ax = plt.subplots()\n\n    text = AnchoredText(\"test\\ntest long text\", loc=loc, pad=0.5,\n                        prop={\"ha\": \"center\", \"va\": \"center\"})\n    ax.add_artist(text)\n    fig.canvas.draw()\n    renderer = fig.canvas.get_renderer()\n    bbox = text.get_bbox(renderer)\n    # check if padding is taken into account\n    assert bbox.width > text.get_text().get_window_extent(renderer).width\n", "def test_anchoredtext_set_text(fig_test, fig_ref):\n    ax_ref = fig_ref.add_subplot()\n    text0 = AnchoredText(\"Foo\", \"upper left\")\n    ax_ref.add_artist(text0)\n\n    ax_test = fig_test.add_subplot()\n    text1 = AnchoredText(\"Bar\", \"upper left\")\n    ax_test.add_artist(text1)\n    text1.txt.set_text(\"Foo\")\n\n", "def test_anchoredtext_fontsize():\n    fig, ax = plt.subplots()\n\n    text0 = AnchoredText(\"test\\ntest long text\", loc=\"upper right\",\n                         prop={\"fontsize\": 14})\n    ax.add_artist(text0)\n", "def test_get_packed_offsets_empty(mode):\n    widths = []\n    total = 10\n    sep = 1\n    result = _get_packed_offsets(widths, total, sep, mode=mode)\n    assert result[0] == 0 if mode == 'expand' else total\n    assert result[1] == []\n"], "sample_112": ["    def test_cell_count(self):\n        request = self.request_factory.get(reverse('admin:admin_views_article_changelist'))\n        request.user = self.superuser\n        admin = ArticleAdmin(Article, site)\n        response = admin.changelist_view(request)\n\n        # Create a sample inline formset\n        inline_formset = response.context_data['inline_admin_formsets'][0]\n\n        # Mock the formset to have a specific number of fields and a delete checkbox\n        inline_formset.formset.can_delete = True\n        inline_formset[0].fields = [\n            {'name': 'field1', 'widget': 'TextInput'},\n            {'name': 'field2', 'widget': 'Textarea'},\n            {'name': 'field3', 'widget': 'HiddenInput'},\n        ]\n        \n        # Calculate the expected cell count\n        expected_count = 1 + len(inline_formset[0].fields) + 1 # 1 for hidden ID, len(fields) for visible fields, +1 for delete checkbox\n        \n        # Test the cell_count filter\n        self.assertEqual(cell_count(inline_formset[0]), expected_count)\n\n", "    def test_cell_count(self):\n        request = self.request_factory.get(reverse('admin:admin_views_article_changelist'))\n        request.user = self.superuser\n        admin = ArticleAdmin(Article, site)\n        response = admin.changelist_view(request)\n        response.render()\n        inline_admin_formset = response.context_data['inline_admin_formsets'][0]\n        self.assertEqual(cell_count(inline_admin_formset.formset.forms[0]), 5)\n\n", "    def test_cell_count(self):\n        request = self.request_factory.get(reverse('admin:admin_views_article_changelist'))\n        request.user = self.superuser\n        admin = ArticleAdmin(Article, site)\n        response = admin.changelist_view(request)\n        response.render()\n\n        inline_admin_formset = response.context_data['inline_admin_formsets'][0]\n        inline_admin_form = inline_admin_formset.forms[0]\n\n        # Assert that cell_count returns the correct number of cells for the inline formset\n        self.assertEqual(cell_count(inline_admin_form), 4)\n", "    def test_cell_count(self):\n        request = self.request_factory.get(reverse('admin:admin_views_article_changelist'))\n        request.user = self.superuser\n        admin = ArticleAdmin(Article, site)\n        response = admin.changelist_view(request)\n        response.render()\n        inline_admin_formset = response.context_data['adminform'].formset\n        self.assertEqual(cell_count(inline_admin_formset.forms[0]), 5)\n", "    def test_cell_count(self):\n        request = self.request_factory.get(reverse('admin:admin_views_article_changelist'))\n        request.user = self.superuser\n        admin = ArticleAdmin(Article, site)\n        response = admin.changelist_view(request)\n        response.render()\n        inline_admin_formset = response.context_data['inline_admin_formsets'][0]\n        self.assertEqual(\n            cell_count(inline_admin_formset[0]),\n            len(inline_admin_formset.formset.form.fields)\n        )\n"], "sample_133": ["    def test_i18n_with_context(self):\n        self.selenium.get(self.live_server_url + '/i18n_context/')\n        elem = self.selenium.find_element_by_id('translated_text')\n        self.assertEqual(elem.text, 'Texto con contexto traducido')\n\n", "    def test_jsi18n_language_change(self):\n        self.selenium.get(self.live_server_url + '/jsi18n/')\n        elem = self.selenium.find_element_by_id(\"language\")\n        elem.send_keys('fr')\n        elem.submit()\n        elem = self.selenium.find_element_by_id(\"gettext\")\n        self.assertEqual(elem.text, \"Enlever\")\n", "    def test_jsi18n_no_translation(self):\n        with self.settings(USE_I18N=True):\n            response = self.client.get('/jsi18n/')\n            self.assertContains(response, '\"this is to be translated\": \"this is to be translated\"')\n", "    def test_i18n_empty_catalog(self):\n        \"\"\"\n        Check if the Javascript i18n view returns an empty dictionary\n        when the selected language has no translations. See #17357\n        \"\"\"\n        with self.settings(LANGUAGE_CODE='fr'), override('xx-xx'):\n            response = self.client.get('/jsi18n/')\n            self.assertEqual(response.context['catalog'], {})\n", "    def test_setlang_reversal_reverse(self):\n        self.assertEqual(reverse('set_language'), '/i18n/setlang/')\n        response = self.client.get('/i18n/setlang/')\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response['Location'], '/es/')\n"], "sample_208": ["    def test_alter_model_rename_column(self):\n        \"\"\"Test adding a new column and renaming an existing one in the same migration.\"\"\"\n        before = [\n            ModelState('app', 'MyModel', [\n                ('id', models.AutoField(primary_key=True)),\n                ('old_name', models.CharField(max_length=200)),\n            ]),\n        ]\n        after = [\n            ModelState('app', 'MyModel', [\n                ('id', models.AutoField(primary_key=True)),\n                ('new_name', models.CharField(max_length=200)),\n                ('old_name', models.CharField(max_length=200)),\n            ]),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['AddField', 'RenameField'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='new_name')\n        self.assertOperationAttributes(changes, 'app', 0, 1,\n                                       old_name='old_name', new_name='new_name')\n", "    def test_field_renam_migration(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.RenameField(\n                    model_name='Person',\n                    old_name='first_name',\n                    new_name='given_name',\n                ),\n            ]\n\n        migration = Migration('0001_rename_field', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'rename_first_name_to_given_name')\n\n", "    def test_rename_model(self):\n        class Migration(migrations.Migration):\n            operations = [migrations.RenameModel('Person', new_name='Human')]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'rename_person_to_human')\n", "    def test_delete_model_and_add_field(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.DeleteModel('Person'),\n                migrations.AddField('Animal', 'name', models.CharField()),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'delete_person_add_animal_name')\n\n", "    def test_create_model_with_default_value(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    'Book',\n                    fields=[\n                        ('id', models.AutoField(primary_key=True)),\n                        ('name', models.CharField(max_length=200, default='New Book')),\n                    ],\n                ),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'book')\n"], "sample_912": ["    def test_pyclass_visibility(app):\n        text = (\".. py:class:: Class\\n\"\n                \"\\n\"\n                \".. py:method:: public_method()\\n\")\n        domain = app.env.get_domain('py')\n        doctree = restructuredtext.parse(app, text)\n        assert_node(doctree, (addnodes.index,\n                              [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                   [desc_name, \"Class\"])],\n                                      [desc_content, (addnodes.index,\n                                                      desc)])]))", "def test_pyattribute_signature_with_value(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :type: str\\n\"\n            \"      :value: 'default'\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'attr (Class attribute)', 'class.attr', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"attr\"],\n                                                     [desc_annotation, \": str\"],\n                                                     [desc_annotation, \" = 'default'\"])],\n                                   [desc_content, ()]))\n    assert 'Class.attr' in domain.objects\n    assert domain.objects['Class.attr'] == ('index', 'class.attr', 'attribute')\n\n", "def test_pyexception_signature_no_module(app):\n    text = \".. py:exception:: IOError\\n\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"exception \"],\n                                                    [desc_name, \"IOError\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], desc, desctype=\"exception\",\n                domain=\"py\", objtype=\"exception\", noindex=False)\n", "def test_pyobject_overloading(app):\n    text = (\".. py:class:: Foo\\n\"\n            \"    :members: bar, baz\\n\"\n            \"\\n\"\n            \"   .. py:method:: Foo.bar\\n\"\n            \"   .. py:method:: Foo.baz\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Foo\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc,\n                                                  desc)])]))\n    assert 'Foo' in domain.objects\n    assert domain.objects['Foo'] == ('index', 'Foo', 'class')\n    assert 'Foo.bar' in domain.objects\n    assert domain.objects['Foo.bar'] == ('index', 'Foo.bar', 'method')\n    assert 'Foo.baz' in domain.objects\n    assert domain.objects['Foo.baz'] == ('index', 'Foo.baz', 'method')\n", "def test_pyclass_oldstyle(app):\n    text = \".. py:class:: OldStyleClass\\n\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"OldStyleClass\"])],\n                                  [desc_content, ()])]))\n    assert 'OldStyleClass' in domain.objects\n    assert domain.objects['OldStyleClass'] == ('index', 'OldStyleClass', 'class')\n\n"], "sample_518": ["    def test_patch_set_facecolor_with_alpha(self):\n        patch = Patch(facecolor='C0')\n        patch.set_facecolor(('C1', 0.5))\n        assert patch.get_facecolor() == (0.65906285, 0.80655366, 0.12489219, 0.5)\n", "compilation error", "compilation error", "def test_empty_bbox():\n    poly = Polygon([])\n    assert poly.get_bbox().extents == (0, 0, 0, 0)\n\n", "    def test_default_linewidth():\n        patch = Patch()\n        assert patch.get_linewidth() == rcParams['lines.linewidth']\n"], "sample_421": ["    def test_default_when_after_other_whens(self):\n        msg = \"Default When() must be the last When() in the Case() expression.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            Case(\n                When(Q(pk=1), then=Value(1)),\n                When(default=Value(2)),\n                When(Q(pk=2), then=Value(3)),\n            )\n\n", "    def test_lookup_with_null_values(self):\n        Client.objects.create(name=\"Jean Grey\", account_type=Client.REGULAR, registered_on=None)\n        cases = [\n            Case(When(registered_on__isnull=True, then=Value(\"Unknown\")), default=Value(\"Known\")),\n            Case(When(registered_on__isnull=False, then=Value(\"Known\")), default=Value(\"Unknown\")),\n        ]\n        for case in cases:\n            with self.subTest(case=case):\n                self.assertQuerysetEqual(\n                    Client.objects.annotate(registration=case).order_by(\"pk\"),\n                    [\n                        (\"Jane Doe\", \"Known\"),\n                        (\"James Smith\", \"Known\"),\n                        (\"Jack Black\", \"Known\"),\n                        (\"Jean Grey\", \"Unknown\"),\n                    ],\n                    transform=attrgetter(\"name\", \"registration\"),\n                )\n", "    def test_output_field_mismatch(self):\n        with self.assertRaisesMessage(ValueError, \"Output field type mismatch\"):\n            Case(\n                When(account_type=Client.GOLD, then=Value(True)),\n                output_field=IntegerField(),\n            )\n", "    def test_when_with_subquery(self):\n        client = Client.objects.create(name=\"Test Client\", account_type=Client.REGULAR)\n        Order.objects.create(client=client, total=100)\n        Order.objects.create(client=client, total=200)\n        qs = Client.objects.annotate(\n            high_value_orders=Sum(\n                Case(\n                    When(\n                        order__in=Subquery(Order.objects.filter(total__gt=150)),\n                        then=1,\n                    ),\n                    default=0,\n                )\n            )\n        )\n        self.assertEqual(qs.get(name=\"Test Client\").high_value_orders, 1)\n", "    def test_case_when_with_model_field_lookup(self):\n        class MyModel(models.Model):\n            name = models.CharField(max_length=10)\n            age = models.PositiveIntegerField()\n\n        MyModel.objects.create(name=\"Alice\", age=25)\n        MyModel.objects.create(name=\"Bob\", age=30)\n\n        qs = MyModel.objects.annotate(\n            status=Case(\n                When(age__gt=25, then=Value(\"Adult\")),\n                default=Value(\"Child\"),\n            )\n        )\n        self.assertQuerysetEqual(\n            qs, [(\"Alice\", \"Child\"), (\"Bob\", \"Adult\")], lambda x: (x.name, x.status)\n        )\n"], "sample_419": ["    def test_formset_with_custom_formset_renderer(self):\n        class MyRenderer(TemplatesSetting):\n            formset_template_name = \"a/custom/formset/template.html\"\n\n        ChoiceFormSet = formset_factory(Choice, formset=BaseCustomFormSet)\n        formset = ChoiceFormSet(renderer=MyRenderer())\n        self.assertEqual(formset.renderer.formset_template_name, \"a/custom/formset/template.html\")\n", "    def test_formset_with_no_forms(self):\n        ChoiceFormSet = formset_factory(Choice, extra=0)\n        formset = ChoiceFormSet()\n        self.assertEqual(len(formset.forms), 0)\n        self.assertEqual(formset.errors, {})\n        self.assertFalse(formset.is_bound)\n        self.assertFalse(formset.is_valid())\n", "    def test_formset_can_be_instantiated_with_initial_forms(self):\n        ChoiceFormSet = formset_factory(Choice)\n        initial_data = [{\"choice\": \"Zero\", \"votes\": \"1\"}, {\"choice\": \"One\", \"votes\": \"2\"}]\n        formset = ChoiceFormSet(initial=initial_data)\n        self.assertEqual(len(formset.forms), 2)\n        self.assertEqual(formset.forms[0].cleaned_data, {\"choice\": \"Zero\", \"votes\": \"1\"})\n        self.assertEqual(formset.forms[1].cleaned_data, {\"choice\": \"One\", \"votes\": \"2\"})\n", "    def test_formset_with_custom_error_class(self):\n        class CustomErrorList(ErrorList):\n            pass\n\n        formset = ChoiceFormSet(error_class=CustomErrorList)\n        self.assertEqual(formset.forms[0].error_class, CustomErrorList)\n\n", "    def test_management_form_rendering(self):\n        formset = formset_factory(Choice)(\n            {}, auto_id=False\n        )\n        management_form_html = str(formset.management_form)\n\n        self.assertIn('<input type=\"hidden\" name=\"choices-TOTAL_FORMS\"', management_form_html)\n        self.assertIn('<input type=\"hidden\" name=\"choices-INITIAL_FORMS\"', management_form_html)\n        self.assertIn('<input type=\"hidden\" name=\"choices-MIN_NUM_FORMS\"', management_form_html)\n        self.assertIn('<input type=\"hidden\" name=\"choices-MAX_NUM_FORMS\"', management_form_html)\n"], "sample_371": ["    def test_sensitive_variables_with_kwargs(self):\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_kwargs_function_caller, check_for_POST_params=False)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_kwargs_function_caller, check_for_POST_params=False)\n\n", "    def test_sensitive_variables_with_kwargs(self):\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_kwargs_view)\n            self.verify_unsafe_email(sensitive_kwargs_view)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_kwargs_view)\n            self.verify_safe_email(sensitive_kwargs_view)\n", "    def test_sensitive_variables_with_non_string_keys(self):\n        @sensitive_variables('password')\n            return data\n\n        result = test_func({'password': 'secret', 123: 'value'})\n        self.assertEqual(result['password'], 'XXXXXXXXXXXXXXXXXXXX')\n        self.assertEqual(result[123], 'value')\n", "    def test_sensitive_variables_with_args_and_kwargs(self):\n        @sensitive_variables('password', 'secret_key')\n            pass\n        \n        self.assertEqual(test_func.__wrapped__.__name__, 'test_func')\n", "    def test_sensitive_variables_with_non_str_key(self):\n        \"\"\"\n        The sensitive_variables decorator works with non-string keys in the\n        decorated function's arguments.\n        \"\"\"\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_non_str_key_view, check_for_POST_params=False)\n            self.verify_unsafe_email(sensitive_non_str_key_view, check_for_POST_params=False)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_non_str_key_view, check_for_POST_params=False)\n            self.verify_safe_email(sensitive_non_str_key_view, check_for_POST_params=False)\n"], "sample_1113": ["def test_block_inverse_2x2():\n    I = Identity(2)\n    A = BlockMatrix([[I, I], [I, I]])\n    B = BlockMatrix([[2*I, 2*I], [2*I, 2*I]])\n    assert A.inverse() == Inverse(A)\n\n\n\n", "def test_blockdiag_inverse():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    BD = BlockDiagMatrix(A, B)\n    assert BD.inverse() == BlockDiagMatrix(A.inverse(), B.inverse())\n", "def test_block_matrix_scalar_product():\n    A = BlockMatrix([[1, 2], [3, 4]])\n    assert (2*A).as_explicit() == ImmutableMatrix([[2, 4], [6, 8]])\n    assert (A*2).as_explicit() == ImmutableMatrix([[2, 4], [6, 8]])\n", "def test_block_collapse():\n    n, m, l = symbols('n m l')\n    X = MatrixSymbol('X', n, n)\n    Y = MatrixSymbol('Y', m, m)\n    Z = MatrixSymbol('Z', n, m)\n    A = BlockMatrix([[X, Z], [ZeroMatrix(m, n), Y]])\n    assert block_collapse(A * A) == BlockMatrix([[X**2 + Z*Y.I*Z, X*Y], [ZeroMatrix(m, n), Y**2]])\n", "def test_block_diagonal():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 3, 3)\n    BD = BlockDiagonalMatrix(A, B)\n    assert BD[0, 0] == A[0, 0]\n\n"], "sample_147": ["    def test_intersection_with_different_selects(self):\n        qs1 = Number.objects.values('num')\n        qs2 = Number.objects.filter(num__lt=5).values('num')\n        self.assertEqual(list(qs1.intersection(qs2)), list(qs2))\n", "    def test_order_by_mixed_types(self):\n        qs1 = Number.objects.all().annotate(\n            str_num=F('num')\n        )\n        qs2 = Number.objects.all().annotate(\n            other_str_num=F('other_num')\n        )\n        union = qs1.union(qs2)\n        with self.assertRaisesMessage(\n            DatabaseError,\n            'ORDER BY term does not match any column in the result set'\n        ):\n           list(union.order_by('str_num')) \n", "    def test_union_with_annotations_across_models(self):\n        ReservedName.objects.create(name='a', order=2)\n        qs1 = Number.objects.annotate(\n            has_reserved_name=Exists(ReservedName.objects.filter(order=OuterRef('num')))\n        ).filter(has_reserved_name=True)\n        qs2 = ReservedName.objects.filter(order=2).annotate(\n            num=Value(1, IntegerField())\n        )\n        self.assertCountEqual(\n            qs1.union(qs2).values_list('num', flat=True),\n            [1]\n        )\n", "    def test_union_with_aggregate_annotations(self):\n        qs1 = Number.objects.annotate(sum=F('num') + F('other_num')).filter(sum__gt=5)\n        qs2 = Number.objects.annotate(avg=F('num') / 2).filter(avg__lt=5)\n        self.assertCountEqual(qs1.union(qs2).values_list('num', flat=True), [\n            2, 3, 4, 5, 6, 7, 8, 9,\n        ])\n\n", "    def test_difference_with_same_table(self):\n        qs1 = Number.objects.filter(num__lt=5)\n        qs2 = Number.objects.filter(num__lt=3)\n        self.assertEqual(len(list(qs1.difference(qs2))), 2)\n"], "sample_919": ["    def check(target):\n        class Config:\n            cpp_id_attributes = [\"id_attr\"]\n            cpp_paren_attributes = [\"paren_attr\"]\n        parser = DefinitionParser(target, location=None,\n                                  config=Config())\n        ast, isShorthand = parser.parse_xref_object()\n        parser.assert_end()", "def test_build_domain_cpp_nested_templates(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"nested-templates\")\n    assert len(ws) == 0\n\n", "def test_build_domain_cpp_template_args(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"template-args\")\n    assert len(ws) == 0\n", "    def check(spec, text, file):\n        pattern = '<li><p>%s<a .*?><code .*?><span .*?>%s</span></code></a></p></li>' % spec\n        res = re.search(pattern, text)\n        if not res:\n            print(\"Pattern\\n\\t%s\\nnot found in %s\" % (pattern, file))\n            assert False", "def test_build_domain_cpp_ref_role_and_function_parentheses(app, status, warning):\n    app.builder.build_all()\n    f = 'function_parentheses.html'\n    t = (app.outdir / f).read_text()\n    pattern = r'<a .*?>\\([^)]*\\)</a>'\n    result = re.search(pattern, t)\n    assert result, \"Could not find parentheses in function reference in '{}'\".format(f)\n"], "sample_778": ["    def test_nmf_sparse_input_zero_rows():\n        # Test that NMF handles input matrices with zero rows correctly\n        rng = np.random.mtrand.RandomState(42)\n        A = np.abs(rng.randn(5, 10))\n        A[1, :] = 0  # Make the second row all zeros\n        A = csc_matrix(A)\n\n        for solver in ('cd', 'mu'):\n            model = NMF(solver=solver, random_state=0, n_components=2,\n                        max_iter=400)\n            A_fit_tr = model.fit_transform(A)\n            assert A_fit_tr.shape == (5, 2)  # Check output shape\n            assert A_fit_tr[1,:].all() == 0 # Check if the second row is all zeros\n", "    def test_nmf_invalid_init_input():\n        # Test that invalid input types for init parameter raise ValueError\n        X = np.random.rand(10, 5)\n        \n        invalid_init_types = [\n            123,\n            (1, 2),\n            \"random\",  \n            np.array([[1, 2], [3, 4]]),\n        ]\n\n        for init_type in invalid_init_types:\n            with pytest.raises(ValueError):\n                NMF(init=init_type).fit(X)\n", "    def test_nmf_fit_transform_sparse(self):\n        # Smoke test for fit_transform with sparse input\n        rng = np.random.RandomState(42)\n        n_samples = 20\n        n_features = 10\n        n_components = 5\n\n        X = rng.randn(n_samples, n_features)\n        X_sparse = sp.csr_matrix(X)\n\n        model = NMF(n_components=n_components, random_state=0)\n\n        W1 = model.fit_transform(X)\n        W2 = model.fit_transform(X_sparse)\n        assert_array_almost_equal(W1, W2)\n", "    def test_nmf_init_size_mismatch(self):\n        #Test that the initialization function raises an error if the sizes\n        #don't match\n        rng = np.random.RandomState(42)\n        X = rng.randn(6, 5)\n        n_components = 4\n        W_init = np.abs(rng.randn(6, 3))\n        H_init = np.abs(rng.randn(3, 5))\n\n        msg = \"The shape of W_init and H_init don't match the input data.\"\n        assert_raise_message(ValueError, msg , nmf._initialize_nmf, X,\n                             n_components, init='custom', W=W_init, H=H_init)\n\n", "    def test_nmf_init_error(self):\n        # Test that an error is raised if the init parameter is invalid\n        n_samples = 6\n        n_features = 5\n        n_components = 3\n\n        rng = np.random.mtrand.RandomState(42)\n        X = rng.randn(n_samples, n_features)\n        np.clip(X, 0, None, out=X)\n\n        msg = \"Invalid init parameter: got 'spam' instead of one of\"\n        assert_raise_message(ValueError, msg, NMF(n_components=n_components, init='spam').fit, X)\n"], "sample_355": ["    def test_get_user_anonymous(self):\n        request = HttpRequest()\n        user = get_user(request)\n        self.assertEqual(user, AnonymousUser())\n\n", "    def test_authenticate_custom_user_model_with_backend(self):\n        test_user = CustomPermissionsUser._default_manager.create_user(\n            email='test@example.com', password='test'\n        )\n        self.assertTrue(self.client.login(username=test_user.email, password='test'))\n        self.assertEqual(CustomPermissionsUser.objects.get(pk=self.client.session[SESSION_KEY]), test_user)\n", "    def test_authentication_with_disabled_user(self):\n        CountingMD5PasswordHasher.calls = 0\n        user = User.objects.create_user(email='test@example.com', password='test')\n        user.is_active = False\n        user.save()\n        authenticate(username=user.username, password='test')\n        self.assertEqual(CountingMD5PasswordHasher.calls, 0)\n", "    def test_skips_backends_with_decorated_method_and_no_arguments(self):\n        \"\"\"\n        A backend (SkippedBackendWithDecoratedMethod) is ignored if it doesn't\n        accept the credentials as arguments, even if decorated with\n        @sensitive_variables.\n        \"\"\"\n        self.assertEqual(authenticate(username='test', password='test'), self.user1)\n", "compilation error"], "sample_1115": ["    def test_tensor_simplify():\n        L = TensorIndexType(\"L\")\n        i, j, k, l = tensor_indices(\"i j k l\", L)\n        A, B = tensor_heads(\"A B\", [L])\n\n        expr = A(i)*A(-i)\n        assert expr.simplify() == A(i)*A(-i)\n\n        expr = A(i)*A(j) + A(j)*A(i)\n        assert expr.simplify() == 2*A(i)*A(j)\n\n        expr = A(i)*B(i)\n        assert expr.simplify() == A(i)*B(i)\n\n        expr = A(i)*B(j) + A(j)*B(i)\n        assert expr.simplify() == 2*A(i)*B(j)\n\n\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_636": ["    def test_duplicate_code_raw_strings_no_config(self) -> None:\n        \"\"\"Tests duplicate-code detection with no configuration file.\"\"\"\n        path = join(DATA, \"raw_strings_all\")\n        expected_output = \"Similar lines in 2 files\"\n        self._test_output(\n            [path],\n            expected_output=expected_output,\n        )\n", "    def test_duplicate_code_raw_strings_ignore_comments(self) -> None:\n        \"\"\"Tests if comments are ignored when checking for duplicate code.\"\"\"\n        path = join(DATA, \"raw_strings_ignore_comments\")\n        expected_output = \"Similar lines in 2 files\"\n        self._test_output(\n            [path, \"--disable=all\", \"--enable=duplicate-code\"],\n            expected_output=expected_output,\n        )\n", "    def test_duplicate_code_raw_strings_disable_scope_nested(self) -> None:\n        \"\"\"Tests disabling duplicate-code at an inner scope level nested in another scope.\"\"\"\n        path = join(DATA, \"raw_strings_disable_scope_nested\")\n        expected_output = \"Similar lines in 2 files\"\n        self._test_output(\n            [path, \"--disable=all\", \"--enable=duplicate-code\"],\n            expected_output=expected_output,\n        )\n", "    def test_duplicate_code_raw_strings_ignore_docstrings(self) -> None:\n        \"\"\"Tests disabling duplicate-code with ignore-docstrings set.\"\"\"\n        path = join(DATA, \"raw_strings_ignore_docstrings\")\n        expected_output = \"Similar lines in 2 files\"\n        self._test_output(\n            [path, \"--disable=all\", \"--enable=duplicate-code\", \"--ignore-docstrings\"],\n            expected_output=expected_output,\n        )\n", "    def test_duplicate_code_raw_strings_ignore_comments(self) -> None:\n        \"\"\"Tests if comments are ignored when calculating similarities.\"\"\"\n        path = join(DATA, \"raw_strings_ignore_comments\")\n        expected_output = \"Similar lines in 2 files\"\n        self._test_output(\n            [path, \"--disable=all\", \"--enable=duplicate-code\"],\n            expected_output=expected_output,\n        )\n"], "sample_628": ["    def test_skip_words_with_unicode_characters(self):\n        stmt = astroid.extract_node(\n            'class ComentAbc(object):\\n   \"\"\"This is caf\u00e9 coment\"\"\"\\n   pass'\n        )\n        self.checker.visit_classdef(stmt)\n        assert self.linter.release_messages() == []\n", "    def test_skip_words_with_leading_numbers(self):\n        stmt = astroid.extract_node(\n            'class ComentAbc(object):\\n   \"\"\"2bad coment\"\"\"\\n   pass'\n        )\n        self.checker.visit_classdef(stmt)\n        assert self.linter.release_messages() == []\n", "    def test_ignore_words_in_code_blocks(self):\n        stmt = astroid.extract_node(\n            '''\n            ", "    def test_ignore_words_with_leading_or_trailing_numbers(self):\n        stmt = astroid.extract_node(\n            'class ComentAbc(object):\\n   \"\"\"1coment and coment2\"\"\"\\n   pass'\n        )\n        self.checker.visit_classdef(stmt)\n        assert self.linter.release_messages() == []\n", "    def test_skip_words_with_numbers_2(self):\n        stmt = astroid.extract_node('class ComentAbc(object):\\n   \"\"\"ComentAbc with a 1bad coment\"\"\"\\n   pass')\n        with self.assertAddsMessages(\n            Message(\n                \"wrong-spelling-in-docstring\",\n                line=2,\n                args=(\n                    \"coment\",\n                    \"ComentAbc with a 1bad coment\",\n                    \"                     ^^^^^^\",\n                    self._get_msg_suggestions(\"coment\"),\n                ),\n            )\n        ):\n            self.checker.visit_classdef(stmt)\n"], "sample_983": ["compilation error", "compilation error", "compilation error", "def test_sparse_norm():\n    A = SparseMatrix([[1, 2], [3, 4]])\n    assert A.norm() == (1**2 + 2**2 + 3**2 + 4**2)**0.5\n    assert A.norm(1) == max(1 + 3, 2 + 4)\n    assert A.norm(np.inf) == max(1 + 2, 3 + 4)\n", "def test_sparse_solve_overdetermined():\n    from sympy.matrices import SparseMatrix\n    A = SparseMatrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    b = SparseMatrix([1, 2, 3])\n    x = A.solve_least_squares(b, 'CH')\n    assert (A*x).evalf() == b.evalf()\n\n"], "sample_743": ["def test_kneighbors_graph_empty_input():\n    # Test k-neighbors graph with empty input\n\n    X = np.array([])\n    X = X.reshape(0, 2)\n    nn = neighbors.NearestNeighbors(n_neighbors=1)\n    nn.fit(X)\n\n    assert_raises(ValueError, nn.kneighbors_graph)\n", "def test_radius_neighbors_graph_empty():\n    # Test radius_neighbors_graph when no neighbors are found within radius \n    X = np.array([[0, 0], [1, 1], [2, 2]])\n    nn = neighbors.NearestNeighbors(radius=1).fit(X)\n    graph = nn.radius_neighbors_graph([[3, 3]], mode='connectivity')\n\n    assert_array_equal(graph.A, [[0]])\n    assert_array_equal(graph.data, [])\n    assert_array_equal(graph.indices, [])\n\n", "def test_kneighbors_graph_empty():\n    X = np.array([]).reshape(0, 3)\n    nn = neighbors.NearestNeighbors(n_neighbors=1)\n    nn.fit(X)\n    assert_array_equal(nn.kneighbors_graph(X).A, np.array([]).reshape(0, 0))\n    \n", "    def test_kneighbors_graph_sparse_empty_input(self):\n        X_csr = csr_matrix((10, 5))\n        nn = neighbors.NearestNeighbors(n_neighbors=2, algorithm='brute')\n        graph = nn.fit(X_csr).kneighbors_graph(X_csr)\n        assert_array_equal(graph.A, np.zeros((10, 10)))\n", "    def test_radius_neighbors_empty_input():\n        # Test radius neighbors with empty input data\n        X = np.array([], dtype=float).reshape(0, 5)\n        y = np.array([], dtype=int)\n\n        nn = neighbors.RadiusNeighborsClassifier(radius=1.0)\n        nn.fit(X, y)\n        assert_array_equal(nn.radius_neighbors(X)[0], [])\n        assert_array_equal(nn.radius_neighbors(X)[1], [])\n\n        nn = neighbors.RadiusNeighborsRegressor(radius=1.0)\n        nn.fit(X, y)\n        assert_array_equal(nn.radius_neighbors(X)[0], [])\n        assert_array_equal(nn.radius_neighbors(X)[1], [])\n"], "sample_289": ["    def test_caseinsensitivemapping_get_returns_none(self):\n        self.assertIsNone(self.dict1.get('nonexistent')) \n", "    def test_caseinsensitivemapping_from_iterable_of_pairs(self):\n        data = [('a', 1), ('b', 2), ('A', 3), ('B', 4)]\n        ci_mapping = CaseInsensitiveMapping(data)\n        expected = {'a': 3, 'b': 4}\n        self.assertEqual(dict(ci_mapping), expected)\n\n", "    def test_caseinsensitivemapping_update(self):\n        ci_map = CaseInsensitiveMapping({'name': 'Jane'})\n        ci_map.update({'NAME': 'John'})\n        self.assertEqual(ci_map['name'], 'John')\n", "    def test_case_insensitive_mapping_update(self):\n        d1 = CaseInsensitiveMapping({'Accept': 'application/json'})\n        d1.update({'CONTENT-TYPE': 'text/html'})\n        self.assertEqual(d1, {'Accept': 'application/json', 'CONTENT-TYPE': 'text/html'})\n", "    def test_multivaluedict_getlist_empty_list_default(self):\n        x = MultiValueDict({'a': [1]})\n        self.assertEqual(x.getlist('b', default=[]), [])\n"], "sample_384": ["    def test_database_routing_create(self):\n        Food.objects.create(name=\"Tomato\")\n        with self.assertNumQueries(1, using=\"other\"):\n            Food.objects.create(name=\"Avocado\")\n", "    def test_update_m2m_field_raises_error(self):\n        related_object = RelatedObject.objects.create()\n        tags = [Tag.objects.create(name=str(i)) for i in range(3)]\n        related_object.tags.add(*tags)\n        with self.assertRaisesMessage(\n            ValueError, \"bulk_update() cannot be used with ManyToManyFields.\"\n        ):\n            RelatedObject.objects.bulk_update([related_object], fields=[\"tags\"])\n", "    def test_fields_with_same_name_in_different_tables(self):\n        related_obj = Tag.objects.create(name=\"test\")\n        main_obj = Note.objects.create(note=\"test\", tag=related_obj)\n        main_obj.note = \"updated\"\n        related_obj.name = \"updated name\"\n        Note.objects.bulk_update([main_obj], fields=[\"note\"])\n        Tag.objects.bulk_update([related_obj], fields=[\"name\"])\n        self.assertEqual(Note.objects.get(id=main_obj.id).note, \"updated\")\n        self.assertEqual(Tag.objects.get(id=related_obj.id).name, \"updated name\")\n", "    def test_related_objects_empty_list(self):\n        parent = RelatedObject.objects.create()\n        RelatedObject.objects.bulk_update([parent], fields=[\"single\"])\n        self.assertIsNone(parent.single)\n\n", "    def test_bulk_update_with_null_fks(self):\n        parent = RelatedObject.objects.create()\n        child1 = SingleObject(name=\"child1\")\n        child2 = SingleObject(name=\"child2\")\n        parent.single = child1\n        parent.save()\n        RelatedObject.objects.bulk_update([parent], fields=[\"single\"], values=[None])\n        parent.refresh_from_db()\n        self.assertIsNone(parent.single)\n        parent.single = child2\n        parent.save()\n        RelatedObject.objects.bulk_update([parent], fields=[\"single\"], values=[None])\n        parent.refresh_from_db()\n        self.assertIsNone(parent.single)\n"], "sample_705": ["def test_pytester_outcomes_deselected_skipping(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.skip\n            pass\n\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1, skipped=1)\n", "def test_pytester_outcomes_deselected_markers(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.skip\n            pass\n\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1, skipped=1)\n\n", "def test_pytester_outcomes_deselected_by_marker(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n        pytest.mark.skip_me = pytest.mark.skip(reason=\"just skipping\")\n\n            config.addinivalue_line(\"markers\", \"skip_me: mark a test to be skipped\")\n    \"\"\"\n    )\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.skip_me\n            pass\n\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1, skipped=1)\n", "def test_pytester_run_no_timeout_default_value(pytester: Pytester) -> None:\n    testfile = pytester.makepyfile(\"def test_no_timeout(): pass\")\n    assert pytester.runpytest_subprocess(testfile).ret == ExitCode.OK\n", "def test_pytester_outcomes_deselected_marked(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.skip\n            pass\n\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(skipped=1, passed=1)\n"], "sample_93": ["    def test_nested_aggregate_over_subquery(self):\n        qs = Book.objects.annotate(\n            max_price_for_publisher=Subquery(\n                Publisher.objects.filter(pk=OuterRef('publisher_id')).values('max_price')\n            )\n        ).annotate(\n            avg_price_compared_to_max=Avg('price') / F('max_price_for_publisher')\n        )\n        self.assertQuerysetEqual(qs, [\n            ('Artificial Intelligence: A Modern Approach', 0.7142857142857143),\n            ('The Definitive Guide to Django: Web Development Done Right', 0.7142857142857143),\n            ('Practical Django Projects', 0.5),\n            ('Python Crash Course', 0.5),\n            ('Automate the Boring Stuff with Python', 0.5),\n            ('Fluent Python', 0.3333333333333333),\n            ('Sams Teach Yourself Python in 24 Hours', 0.2),\n            ('Head First Python, 2nd Edition', 0.2),\n\n        ], lambda b: (b.name, b.avg_price_compared_to_max))\n\n\n", "    def test_ticket_26903(self):\n        \"\"\"\n        Ensure that complex annotations using F() expressions within an aggregate\n        don't throw an error if the base expression involves a subquery.\n\n        Refs #26903\n        \"\"\"\n        # Create a subquery that counts the number of books per publisher\n        book_counts = Book.objects.values('publisher').annotate(\n            book_count=Count('id')\n        )\n        # Annotate the Publisher model with the average rating of its books,\n        # taking into account the book counts from the subquery.\n        publisher_avg_ratings = Publisher.objects.annotate(\n            avg_rating=Avg(\n                F('book__rating') *\n                F('book__publisher__book_count') /\n                F('book__publisher__book_count')\n            )\n        ).order_by('avg_rating')\n\n        self.assertQuerysetEqual(\n            publisher_avg_ratings,\n            expected_results,\n            lambda p: p.avg_rating,\n        )\n\n", "    def test_annotation_with_select_related(self):\n        qs = Book.objects.select_related('publisher').annotate(\n            publisher_award_count=Count('publisher__awards')\n        )\n        self.assertEqual(qs.count(), 4)\n        self.assertQuerysetEqual(\n            qs,\n            [\n                ('The Definitive Guide to Django: Web Development Done Right', 9),\n                ('Artificial Intelligence: A Modern Approach', 7),\n                ('Automate the Boring Stuff with Python', 3),\n                ('Fluent Python', 1)\n            ],\n            lambda b: (b.name, b.publisher_award_count)\n        )\n", "    def test_aggregation_with_different_output_fields(self):\n        qs = Book.objects.annotate(\n            avg_price_decimal=Avg('price'),\n            avg_price_float=Cast(Avg('price'), output_field=FloatField()),\n            avg_price_integer=Cast(Avg('price'), output_field=IntegerField()),\n        )\n        for book in qs:\n            self.assertIsInstance(book.avg_price_decimal, Decimal)\n            self.assertIsInstance(book.avg_price_float, float)\n            self.assertIsInstance(book.avg_price_integer, int)\n", "    def test_complex_group_by(self):\n        qs = Publisher.objects.annotate(\n            avg_price=Avg('book__price'),\n            count=Count('book')\n        ).filter(avg_price__gt=40, count__gt=2)\n        self.assertQuerysetEqual(\n            qs, [\n                'Apress',\n                'Prentice Hall'\n            ],\n            lambda v: v.name)\n"], "sample_798": ["def test_ridge_negative_sample_weight():\n    rng = np.random.RandomState(42)\n    X = rng.randn(10, 5)\n    y = rng.randn(10)\n    sample_weight = -np.ones(10)\n\n    ridge = Ridge(alpha=1.)\n    assert_raises_regex(ValueError, \"Sample weights must be non-negative.\",\n                        ridge.fit, X, y, sample_weight=sample_weight)\n\n", "    def test_ridgecv_deprecation():\n        # Test deprecation warning for RidgeCV when passing `fit_intercept`\n        ridgecv = RidgeCV(fit_intercept=True)\n        with pytest.warns(FutureWarning, match=\"The 'fit_intercept' parameter\"):\n            ridgecv.fit(X, y)\n", "    def test_ridge_classifier_positive_class_weight():\n        X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],\n                      [1.0, 1.0], [1.0, 0.0]])\n        y = np.array([1, 1, 1, -1, -1])\n        clf = RidgeClassifier(class_weight={1: 2})\n        clf.fit(X, y)\n        assert clf.coef_[0, 0] > clf.coef_[0, 1]\n", "def test_ridge_cv_n_jobs():\n    # Test that RidgeCV can be parallelized.\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 100, 10\n\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples)\n\n    ridge_cv = RidgeCV(alphas=[0.1, 1.0, 10.0], cv=5)\n\n    # With n_jobs=1\n    ridge_cv.fit(X, y, n_jobs=1)\n    coef_1 = ridge_cv.coef_\n\n    # With n_jobs=-1 (using all processors)\n    ridge_cv.fit(X, y, n_jobs=-1)\n    coef_n = ridge_cv.coef_\n\n    assert_allclose(coef_1, coef_n)\n", "def test_ridge_sample_weight_with_intercept():\n    # Test if sample_weights are correctly applied with fit_intercept=True\n    rng = np.random.RandomState(0)\n    X = rng.randn(10, 3)\n    y = rng.randn(10)\n    sample_weight = rng.rand(10)\n\n    # Fit with sample weights and intercept\n    ridge = Ridge(fit_intercept=True)\n    ridge.fit(X, y, sample_weight=sample_weight)\n    intercept = ridge.intercept_\n\n    # Fit without sample weights and intercept\n    ridge_no_weights = Ridge(fit_intercept=True)\n    ridge_no_weights.fit(X, y)\n    intercept_no_weights = ridge_no_weights.intercept_\n\n    assert intercept != intercept_no_weights\n"], "sample_905": ["        def __init__(self, x, y):\n            self.x = x\n            self.y = y\n", "        def __init__(self):\n            self.attr = 1\n", "def test_getdoc_inheritance_with_no_docstring(app):\n    class Foo:\n            pass\n\n    class Bar(Foo):\n        pass\n\n    assert inspect.getdoc(Bar.meth, getattr, False, Bar, \"meth\") is None\n    assert inspect.getdoc(Bar.meth, getattr, True, Bar, \"meth\") is None\n", "    def test_getdoc_property():\n        class Foo:\n            @property\n                \"\"\"\n                docstring\n                    indented text\n                \"\"\"\n                return 1\n\n        assert inspect.getdoc(Foo.prop, getattr, False, Foo, \"prop\") is None\n        assert inspect.getdoc(Foo.prop, getattr, True, Foo, \"prop\") == Foo.prop.__doc__\n\n", "compilation error"], "sample_352": ["    def test_ticket_23622_with_annotations_and_ordering(self):\n        \"\"\"\n        Make sure __pk__in and __in work the same for related fields when\n        using a distinct on subquery with annotations and ordering.\n        \"\"\"\n        a1 = Ticket23605A.objects.create()\n        a2 = Ticket23605A.objects.create()\n        c1 = Ticket23605C.objects.create(field_c0=0.0)\n        Ticket23605B.objects.create(\n            modela_fk=a1, field_b0=123,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a1, field_b0=23,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a1, field_b0=234,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a1, field_b0=12,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a2, field_b0=567,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a2, field_b0=76,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a2, field_b0=7,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.", "    def test_ticket_23622_exclude(self):\n        \"\"\"\n        Make sure __pk__in and __in work the same for related fields when\n        using a distinct on subquery with exclude.\n        \"\"\"\n        a1 = Ticket23605A.objects.create()\n        a2 = Ticket23605A.objects.create()\n        c1 = Ticket23605C.objects.create(field_c0=0.0)\n        Ticket23605B.objects.create(\n            modela_fk=a1, field_b0=123,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a1, field_b0=23,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a1, field_b0=234,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a1, field_b0=12,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a2, field_b0=567,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a2, field_b0=76,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a2, field_b0=7,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela", "    def test_ticket_24987(self):\n        \"\"\"\n        Test joining to a model with a ManyToMany field through a ForeignKey\n        relationship.\n        \"\"\"\n        a1 = Author.objects.create(name=\"Author 1\")\n        a2 = Author.objects.create(name=\"Author 2\")\n        b1 = Book.objects.create(title=\"Book 1\", authors=[a1, a2])\n        b2 = Book.objects.create(title=\"Book 2\", authors=[a1])\n        c1 = Category.objects.create(name=\"Category 1\")\n        c2 = Category.objects.create(name=\"Category 2\")\n        b1.categories.add(c1)\n        b2.categories.add(c2)\n        self.assertSequenceEqual(\n            Author.objects.filter(\n                book__categories=c1\n            ).order_by('pk'),\n            [a1, a2]\n        )\n        self.assertSequenceEqual(\n            Book.objects.filter(categories=c1).values_list(\"title\", flat=True),\n            [\"Book 1\"]\n        )\n\n", "    def test_ticket_24819(self):\n        i1 = Item.objects.create(name='item1', price=10)\n        i2 = Item.objects.create(name='item2', price=20)\n        i3 = Item.objects.create(name='item3', price=30)\n        o1 = Order.objects.create(user='user1')\n        o2 = Order.objects.create(user='user2')\n\n        o1.items.add(i1)\n        o1.items.add(i2)\n        o2.items.add(i3)\n\n        # Test with F() expression in the subquery\n        qs = Order.objects.filter(items__price__gte=F('items__price').distinct())\n        self.assertCountEqual(qs, [o1, o2])\n\n        # Test with subquery that returns a single value (no distinct needed)\n        qs = Order.objects.filter(items__price__gte=Subquery(Item.objects.values('price').first()))\n        self.assertCountEqual(qs, [o1, o2])\n", "    def test_ticket_24993(self):\n        \"\"\"\n        Test join promotion with multiple levels of ManyToManyField relationships\n        and subquery annotations.\n        \"\"\"\n        a1 = Author.objects.create(name='Author 1')\n        a2 = Author.objects.create(name='Author 2')\n        b1 = Book.objects.create(title='Book 1', author=a1)\n        b2 = Book.objects.create(title='Book 2', author=a2)\n        c1 = Category.objects.create(name='Category 1')\n        c2 = Category.objects.create(name='Category 2')\n        b1.categories.add(c1)\n        b2.categories.add(c2)\n\n        qs = Book.objects.annotate(\n            category_count=Count('categories')\n        ).filter(\n            author__in=Author.objects.filter(\n                book__categories__name='Category 1'\n            )\n        )\n        self.assertSequenceEqual(qs, [b1])\n"], "sample_1051": ["def test_empty_expression():\n    assert dotprint(Basic()) == \\\n           'digraph{\\n\\n# Graph style\\n\"ordering\"=\"out\"\\n\"rankdir\"=\"TD\"\\n\\n######### \\n# Nodes #\\n#########\\n\\n\"Basic()_()\" [\"color\"=\"blue\", \"label\"=\"Basic\", \"shape\"=\"ellipse\"];\\n\\n######### \\n# Edges #\\n#########\\n\\n} ' \n", "def test_empty_expr():\n    assert dotprint(Expr()) == 'digraph{\\n\\n# Graph style\\n\"ordering\"=\"out\"\\n\"rankdir\"=\"TD\"\\n\\n######### \\n# Nodes #\\n#########\\n\\n######### \\n# Edges #\\n#########\\n}\\n' \n", "def test_dotprint_slots():\n    class MySlotClass(object):\n        __slots__ = ['a', 'b']\n            self.a = a\n            self.b = b\n    obj = MySlotClass(1, 2)\n    assert 'digraph' in dotprint(obj)\n", "def test_empty_expression():\n    text = dotprint(Basic(), repeat=False)\n    assert '\"Basic()\" [\"color\"=\"blue\", \"label\"=\"Basic\", \"shape\"=\"ellipse\"];' in text\n    assert '->' not in text\n    text = dotprint(Basic(), repeat=True)\n    assert '\"Basic()_()\" [\"color\"=\"blue\", \"label\"=\"Basic\", \"shape\"=\"ellipse\"];' in text\n    assert '->' not in text\n", "def test_atom():\n    assert dotprint(x, atom=lambda x: x == 1) == dotprint(x + 1)\n\n"], "sample_158": ["    def test_valid_foreign_object(self):\n        class Parent(models.Model):\n            a = models.PositiveIntegerField()\n            b = models.PositiveIntegerField()\n\n            class Meta:\n                unique_together = (('a', 'b'),)\n\n        class Child(models.Model):\n            a = models.PositiveIntegerField()\n            b = models.PositiveIntegerField()\n            value = models.CharField(max_length=255)\n            parent = models.ForeignObject(\n                Parent,\n                on_delete=models.SET_NULL,\n                from_fields=('a', 'b'),\n                to_fields=('a', 'b'),\n                related_name='children',\n            )\n", "    def test_valid_foreign_object_with_existing_unique_together(self):\n        class Parent(models.Model):\n            a = models.PositiveIntegerField()\n            b = models.PositiveIntegerField()\n            c = models.PositiveIntegerField()\n\n            class Meta:\n                unique_together = (('a', 'b'),)\n\n        class Child(models.Model):\n            a = models.PositiveIntegerField()\n            b = models.PositiveIntegerField()\n            value = models.CharField(max_length=255)\n            parent = models.ForeignObject(\n                Parent,\n                on_delete=models.SET_NULL,\n                from_fields=('a', 'b'),\n                to_fields=('a', 'b'),\n                related_name='children',\n            )\n", "def test_empty_through_fields(self):\n    \"\"\"\n    ManyToManyField with empty through_fields is validated properly.\n    \"\"\"\n    class Fan(models.Model):\n        pass\n\n    class Event(models.Model):\n        invitees = models.ManyToManyField(Fan, through='Invitation', through_fields=())\n\n    class Invitation(models.Model):\n        event = models.ForeignKey(Event, models.CASCADE)\n        invitee = models.ForeignKey(Fan, models.CASCADE)\n        inviter = models.ForeignKey(Fan, models.CASCADE, related_name='+')\n\n    field = Event._meta.get_field('invitees')\n    self.assertEqual(field.check(from_model=Event), [\n        Error(\n            'ManyToManyField with \"through_fields\" cannot be empty.',\n            hint=\"Specify the two link fields as through_fields=('field1', 'field2').\",\n            obj=field,\n            id='fields.E336',\n        )\n    ])\n", "    def test_valid_foreign_object(self):\n        class Parent(models.Model):\n            a = models.PositiveIntegerField()\n            b = models.PositiveIntegerField()\n            c = models.PositiveIntegerField()\n            d = models.PositiveIntegerField()\n\n            class Meta:\n                unique_together = (('a', 'b', 'c'),)\n\n        class Child(models.Model):\n            a = models.PositiveIntegerField()\n            b = models.PositiveIntegerField()\n            c = models.PositiveIntegerField()\n            value = models.CharField(max_length=255)\n            parent = models.ForeignObject(\n                Parent,\n                on_delete=models.SET_NULL,\n                from_fields=('a', 'b', 'c'),\n                to_fields=('a', 'b', 'c'),\n                related_name='children',\n            )\n\n", "    def test_valid_field_names(self):\n        \"\"\"\n        Providing valid field names to ManyToManyField.through_fields\n        should pass validation.\n        \"\"\"\n        class Fan(models.Model):\n            pass\n\n        class Event(models.Model):\n            invitees = models.ManyToManyField(\n                Fan,\n                through='Invitation',\n                through_fields=('invitee', 'event'),\n            )\n\n        class Invitation(models.Model):\n            event = models.ForeignKey(Event, models.CASCADE)\n            invitee = models.ForeignKey(Fan, models.CASCADE)\n            inviter = models.ForeignKey(Fan, models.CASCADE, related_name='+')\n \n        self.assertEqual(Event.check(), [])\n"], "sample_374": ["    def test_prefetch_related_with_select_related(self):\n        with self.assertNumQueries(2):\n            qs = Room.objects.select_related('house').prefetch_related(\n                Prefetch('house', queryset=House.objects.all())\n            )\n            for room in qs:\n                self.assertEqual(str(room.house), str(room.house))\n", "    def test_prefetch_related_with_complex_filter(self):\n        \"\"\"\n        Test prefetching related objects with complex filter on the related manager.\n        \"\"\"\n        books = Book.objects.filter(\n            title__startswith='Poems'\n        ).prefetch_related(\n            Prefetch(\n                'first_time_authors',\n                Author.objects.filter(name__endswith='Bronte')\n            )\n        )\n        with self.assertNumQueries(2):\n            self.assertEqual(list(books), [self.book1])\n            self.assertEqual(books[0].first_time_authors.all(), [self.author1])\n", "    def test_prefetch_related_non_existent_relationship(self):\n        with self.assertRaises(FieldDoesNotExist):\n            House.objects.prefetch_related('non_existent_relationship')\n\n", "    def test_nested_prefetch_with_select_related(self):\n        \"\"\"\n        Ensure that nested prefetching with select_related works correctly.\n        \"\"\"\n        queryset = House.objects.select_related('rooms').prefetch_related(\n            Prefetch('rooms__house', queryset=House.objects.only('address')),\n        )\n        with self.assertNumQueries(3):\n            house = queryset.first()\n\n        self.assertIs(Room.house.is_cached(self.room), True)\n        with self.assertNumQueries(0):\n            house.rooms.first().house.address\n\n", "    def test_prefetch_related_with_select_related(self):\n        with self.assertNumQueries(2):\n            authors = AuthorWithAge.objects.select_related(\n                'first_book',\n            ).prefetch_related(\n                Prefetch('favorite_authors__first_book'),\n            )\n        self.assertCountEqual(authors, [self.author1, self.author2])\n"], "sample_510": ["def test_figure_savefig_with_default_bbox():\n    fig, ax = plt.subplots()\n\n    # Test saving figure with default bbox\n    fig.savefig(\n        'test_figure_savefig_with_default_bbox.png'\n    )\n    \n    plt.close(fig)\n", "    def test_get_fignums():\n        fig1 = plt.figure()\n        fig2 = plt.figure()\n        assert set(plt.get_fignums()) == {fig1.number, fig2.number}\n\n        plt.close(fig2)\n        assert set(plt.get_fignums()) == {fig1.number}\n", "compilation error", "compilation error", "compilation error"], "sample_951": ["def test_signature_from_str_positionaly_only_args_with_defaults():\n    sig = inspect.signature_from_str('(a, /, b=0, c=1)')\n    assert list(sig.parameters.keys()) == ['a', 'b', 'c']\n    assert sig.parameters['a'].kind == Parameter.POSITIONAL_ONLY\n    assert sig.parameters['a'].default == Parameter.empty\n    assert sig.parameters['b'].kind == Parameter.POSITIONAL_ONLY\n    assert sig.parameters['b'].default == '0'\n    assert sig.parameters['c'].kind == Parameter.POSITIONAL_OR_KEYWORD\n    assert sig.parameters['c'].default == '1'\n\n", "    def test_getsourcefile():\n        assert inspect.getsourcefile(inspect.getsourcefile) == __file__\n        try:\n            inspect.getsourcefile(object)\n        except TypeError as exc:\n            assert str(exc) == \"object()' has no source file\"\n        assert inspect.getsourcefile(lambda x: x) is None\n", "    def test_getsourcefile():\n\n        assert inspect.getsourcefile(inspect.getsourcefile) == __file__\n        assert inspect.getsourcefile(len) is None\n", "def test_get_annotations_from_signature():\n    sig = inspect.signature(lambda a: a)\n    assert inspect.get_annotations_from_signature(sig) == {}\n\n    sig = inspect.signature(lambda a: a, {'a': 1})\n    assert inspect.get_annotations_from_signature(sig) == {'a': 1}\n\n        pass\n\n    sig = inspect.signature(foo)\n    assert inspect.get_annotations_from_signature(sig) == {'a': int, 'b': str}\n", "def test_getsource():\n        return a + b\n    assert inspect.getsource(func1) == ('    return a + b\\n')\n    with pytest.raises(OSError):\n        inspect.getsource(func1, method='non-existing-method')\n    with pytest.raises(TypeError):\n        inspect.getsource(123)\n"], "sample_652": ["    def test_nested_call_fixture(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.fixture\n                return 42\n\n                assert my_fixture == 42\n\n                @pytest.fixture\n                    return 10\n\n                    assert inner_fixture == 10\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.assert_outcomes(passed=2)\n\n", "    def test_fixture_dependency_cycle(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.fixture\n                return 'foo'\n\n            @pytest.fixture\n                return 'bar'\n\n                pass\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines(\n            [\n                '*pytest.fixture.FixtureLookupError: fixture \"foo\" depends on fixture \"bar\", which depends on fixture \"foo\", creating a circular dependency.',\n            ]\n        )\n\n", "    def test_fixture_with_same_name_different_scope(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.fixture\n                return 1\n\n            @pytest.fixture(scope='module')\n                return 2\n\n                assert my_fixture == 2\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.assert_outcomes(passed=1)\n", "    def test_fixture_function_object_error(self):\n        @pytest.fixture\n            return 1\n\n        f = fix\n\n        with pytest.raises(pytest.fail.Exception):\n            assert f() == 1\n", "    def test_fixture_teardown_exception_during_fixture_teardown(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.fixture\n                yield 1\n                raise ValueError(\"teardown error\")\n\n                pass\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines(\n            [\n                \"*teardown*\",\n                \"*ValueError: teardown error*\",\n                \"*1 passed*\",\n                \"*1 error*\",\n            ]\n        )\n\n"], "sample_954": ["def test_man_show_urls(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n    assert 'function test_func(arg1: int, arg2: str)' in content\n    assert ' <https://github.com>' in content\n\n", "def test_image(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n    assert '[image: Sphinx logo]' in content\n", "def test_desc_signatures_with_urls(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n\n    assert 'MyModule.my_function() <https://example.com/' in content\n    assert '.IP \"MyModule.my_function() <https://example.com/' in content\n    assert '.IP \"MyModule.my_function()\" [https://example.com/' in content\n\n", "def test_domain_objects(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n    assert r'\\fBmodule\\fP' in content\n    assert r'\\fBSomeClass\\fP' in content\n    assert r'\\fBsome_function\\fP' in content\n", "def test_download_reference(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n    assert '[download: http://example.com]' in content\n"], "sample_171": ["    def test_migrate_with_empty_migration_directory(self):\n        \"\"\"\n        migrate shouldn't raise an error if a migration directory is empty.\n        \"\"\"\n        with tempfile.TemporaryDirectory() as empty_migration_dir:\n            apps.register_model('migrations', UnicodeModel)\n            # Mock the migration directory as empty.\n            MigrationRecorder.migrate_dir = empty_migration_dir\n            call_command('migrate', 'migrations', verbosity=0)\n", "    def test_showmigrations_no_Migrations_directory(self):\n        with mock.patch('os.path.isdir', return_value=False):\n            err = io.StringIO()\n            with self.assertRaises(SystemExit) as cm:\n                call_command('showmigrations', 'migrations', stderr=err)\n            self.assertEqual(cm.exception.code, 1)\n            self.assertIn(\"No migration files found for 'migrations'\", err.getvalue())\n\n", "    def test_makemigrations_empty_migration_created(self):\n        \"\"\"\n        makemigrations creates an empty migration file if no changes are detected.\n        \"\"\"\n\n        with self.temporary_migration_module(module=\"migrations.test_migrations_no_changes\"):\n            call_command(\"makemigrations\", \"migrations\", verbosity=0)\n            empty_migration_file = os.path.join(self.migration_dir, \"0001_initial.py\")\n            self.assertTrue(os.path.exists(empty_migration_file))\n\n", "    def test_makemigrations_no_changes(self):\n        \"\"\"\n        makemigrations doesn't create migrations if there are no changes.\n        \"\"\"\n        with self.temporary_migration_module(module=\"migrations.test_migrations_no_changes\"):\n            call_command(\"makemigrations\", \"migrations\", verbosity=0)\n            migration_dir = os.path.join(self.migration_dir, 'migrations')\n            self.assertEqual(len(os.listdir(migration_dir)), 1)\n", "    def test_makemigrations_merge_conflicting_migrations(self):\n        \"\"\"\n        makemigrations --merge should create a merge migration when\n        there are conflicting migrations.\n        \"\"\"\n        with self.temporary_migration_module(module=\"migrations.test_migrations_conflict\") as migration_dir:\n            call_command(\"makemigrations\", \"migrations\", name=\"merge\",\n                         merge=True, interactive=False)\n            merge_file = os.path.join(migration_dir, '0003_merge.py')\n            self.assertTrue(os.path.exists(merge_file))\n"], "sample_288": ["    def test_key_transform_with_raw_sql_expression(self):\n        expr = RawSQL(self.raw_sql, ['{\"x\": \"bar\"}'])\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__foo=KeyTransform('x', expr)\n            ),\n            [self.objs[7]],\n        )\n", "    def test_key_transform_contains_mixed_types(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__d__contains=KeyTransform('x', RawSQL(self.raw_sql, ['{\"x\": \"b\"}'])),\n            ),\n            [self.objs[4]],\n        )\n", "    def test_deep_lookup_not_in(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__d__1__f__not_in=['g']),\n            self.objs[:4] + self.objs[5:],\n        )\n", "    def test_key_transform_on_subquery(self):\n        obj = NullableJSONModel.objects.create(value={'x': 'y'})\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                id__in=NullableJSONModel.objects.filter(\n                    value__x=KeyTransform('x', Subquery(\n                        NullableJSONModel.objects.filter(pk=OuterRef('pk')).values('value')\n                    )),\n                ),\n            ),\n            [obj],\n        )\n\n", "    def test_key_transform_exclude(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.exclude(\n                key_transform=KeyTransform('a', 'value'),\n            ),\n            [self.objs[0], self.objs[1], self.objs[2], self.objs[5}, self.objs[6], self.objs[7]],\n        )\n\n"], "sample_764": ["def test_column_transformer_callable_specifier_pandas_names():\n    pd = pytest.importorskip('pandas')\n    X_df = pd.DataFrame([[0, 1, 2], [2, 4, 6]], columns=['a', 'b', 'c'])\n    X_res_first = X_df[['a']]\n\n        return ['a']\n\n    ct = ColumnTransformer([('trans', Trans(), func)],\n                           remainder='drop')\n    assert_frame_equal(ct.fit_transform(X_df), X_res_first)\n    assert_frame_equal(ct.fit(X_df).transform(X_df), X_res_first)\n    assert callable(ct.transformers[0][2])\n    assert ct.transformers_[0][2] == ['a']\n\n", "def test_column_transformer_with_pandas_multi_index():\n    pd = pytest.importorskip('pandas')\n    X_multi = pd.DataFrame(\n        np.arange(12).reshape(3, 4),\n        columns=pd.MultiIndex.from_tuples([('a', 'x'), ('a', 'y'),\n                                         ('b', 'x'), ('b', 'y')]))\n\n    ct = ColumnTransformer(\n        [('trans1', StandardScaler(), [('a', 'x'), ('a', 'y')])],\n        remainder='passthrough')\n    X_transformed = ct.fit_transform(X_multi)\n    ct.fit(X_multi)\n\n    # Check shape\n    assert X_transformed.shape == (3, 4)\n\n    # Check that the transformed columns are correct\n    assert_array_equal(X_transformed[:, :2], ct.named_transformers_.trans1.transform(X_multi[[('a', 'x'), ('a', 'y')]]))\n\n    # Check that the passthrough columns are correct\n    assert_array_equal(X_transformed[:, 2:], X_multi[[('b', 'x'), ('b', 'y')]])\n\n", "    def test_column_transformer_callable_specifier_error():\n        X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n            return 'error'\n\n        ct = ColumnTransformer([('trans', Trans(), func)])\n        assert_raises(TypeError, ct.fit_transform, X_array)\n        assert_raises(TypeError, ct.fit, X_array)\n", "def test_column_transformer_callable_specifier_error():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n        return 'invalid'\n\n    ct = ColumnTransformer([('trans', Trans(), func)],\n                           remainder='drop')\n    assert_raises(TypeError, ct.fit_transform, X_array)\n    assert_raises(TypeError, ct.fit, X_array)\n", "def test_column_transformer_invalid_remainder_callable():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n        return 'invalid'\n\n    ct = ColumnTransformer([('trans', Trans(), [0])], remainder=func)\n    assert_raise_message(TypeError,\n                         \"remainder must be one of 'drop', 'passthrough', \"\n                         \"or an estimator instance.\",\n                         ct.fit, X_array)\n    assert_raise_message(TypeError,\n                         \"remainder must be one of 'drop', 'passthrough', \"\n                         \"or an estimator instance.\",\n                         ct.fit_transform, X_array)\n"], "sample_1116": ["def test_inverse_determinant():\n    assert Inverse(C)._eval_determinant() == 1/C.det()\n\n", "compilation error", "def test_inverse_determinant():\n    C = MatrixSymbol('C', 2, 2)\n    assert Inverse(C)._eval_determinant() == 1/C.det()\n", "def test_inverse_determinant():\n    from sympy import det\n    assert Inverse(C).det() == 1/det(C)\n", "def test_inverse_determinant():\n    assert Inverse(C).determinant() == 1/C.det()\n"], "sample_516": ["def test_pdfpages_save_empty_figure():\n    with NamedTemporaryFile(delete=False) as tmp:\n        filename = tmp.name\n        with PdfPages(filename) as pdf:\n            pass\n    os.remove(filename)\n", "def test_pdfpages_context_manager():\n    with PdfPages(io.BytesIO()) as pdf:\n        pdf.savefig(plt.figure())\n    with pytest.raises(FileNotFoundError):\n        pdf.savefig(plt.figure())\n\n", "def test_pdf_savefig_no_artist_no_axes(fig_test, fig_ref):\n    fig_test.savefig(io.BytesIO(), format=\"pdf\")\n    fig_ref.savefig(io.BytesIO(), format=\"pdf\")\n", "    def test_fontsizes():\n        fig, ax = plt.subplots()\n        ax.set_title('This is a title', fontsize=16)\n        ax.set_xlabel('This is a label', fontsize=14)\n        ax.set_ylabel('This is another label', fontsize=12)\n        ax.tick_params(axis='both', which='major', labelsize=10)\n        with io.BytesIO() as fd:\n            fig.savefig(fd, format='pdf')\n\n", "    def test_multipage_pagecount():\n        with PdfPages(io.BytesIO()) as pdf:\n            assert pdf.get_pagecount() == 0\n            fig, ax = plt.subplots()\n            ax.plot([1, 2, 3])\n            fig.savefig(pdf, format=\"pdf\")\n            assert pdf.get_pagecount() == 1\n            pdf.savefig()\n            assert pdf.get_pagecount() == 2\n"], "sample_559": ["def test_grid_with_axes_class_overriding_axis():\n    class MyAxes(mpl.axes.Axes):\n            super().__init__(*args, **kwargs)\n            self._axis_drawn = False\n\n            self._axis_drawn = True\n            super().draw(renderer)\n\n    Grid(plt.figure(), 111, (2, 2), axes_class=MyAxes)\n", "def test_inset_position_with_axes_locator():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1])\n\n    inset_ax = fig.add_axes([0.5, 0.5, 0.3, 0.3])\n    inset_ax.plot([0, 1], [0, 1])\n\n    locator = AnchoredLocator(\n        bbox_to_anchor=(0.2, 0.2), loc='upper left',\n        coord_transform=ax.transAxes,\n        axes_transform=ax.transAxes)\n    inset_ax.set_axes_locator(locator)\n\n", "compilation error", "compilation error", "def test_grid_axes_position_with_size_ratio(direction):\n    \"\"\"Test positioning of the axes in Grid,\n    taking into account size ratios.\n    \"\"\"\n    fig = plt.figure()\n    grid = Grid(fig, 111, (2, 2), direction=direction,\n                size_ratios=(2, 1))\n    loc = [ax.get_axes_locator() for ax in np.ravel(grid.axes_row)]\n    # Test nx.\n    assert loc[1].args[0] > loc[0].args[0]\n    assert loc[0].args[0] == loc[2].args[0]\n    assert loc[3].args[0] == loc[1].args[0]\n    # Test ny.\n    assert loc[2].args[1] < loc[0].args[1]\n    assert loc[0].args[1] == loc[1].args[1]\n    assert loc[3].args[1] == loc[2].args[1]\n\n\n\n"], "sample_821": ["def test_affinity_propagation_preference_array():\n    # Test AffinityPropagation with preference array input\n    X = np.array([[0, 0], [1, 1], [-2, -2]])\n    preferences = np.array([0.5, -0.5, 1])\n    af = AffinityPropagation(preference=preferences)\n    assert_warns(ConvergenceWarning, af.fit, X)\n\n    # Check if cluster centers and labels are calculated correctly\n    assert len(af.cluster_centers_indices_) > 0\n    assert len(af.labels_) == X.shape[0]\n\n", "    def test_affinity_propagation_preference_shape():\n        # Test that input preferences can be a scalar or a 1D array.\n\n        X = np.array([[1, 2], [1, 4], [1, 0],\n                     [4, 2], [4, 4], [4, 0]])\n\n        # Test scalar preference\n        clustering = AffinityPropagation(preference=1).fit(X)\n        assert clustering.cluster_centers_indices_.size > 0\n        assert clustering.labels_.size == X.shape[0]\n\n        # Test 1D array preference\n        clustering = AffinityPropagation(preference=np.array([1])).fit(X)\n        assert clustering.cluster_centers_indices_.size > 0\n        assert clustering.labels_.size == X.shape[0]\n\n        # Test preference with wrong shape\n        with pytest.raises(ValueError):\n            clustering = AffinityPropagation(preference=np.array([1, 2])).fit(X)\n", "def test_affinity_propagation_damping():\n    # Test the influence of damping factor on convergence\n\n    # Generate some data\n    X, _ = make_blobs(n_samples=100, n_features=2, centers=3,\n                      cluster_std=0.6, shuffle=True, random_state=0)\n\n    # Define damping factors to test\n    damping_factors = [0.4, 0.6, 0.8, 0.9]\n\n    for damping in damping_factors:\n        # Fit AffinityPropagation with different damping values\n        af = AffinityPropagation(damping=damping, random_state=0)\n        af.fit(X)\n\n        # Check if the model converged\n        assert af.n_iter_ > 0\n\n        # Expect number of clusters to be stable for different damping factors\n        assert np.unique(af.labels_).size == 3\n\n", "    def test_affinity_propagation_damping_effect():\n        # Test the effect of damping parameter on convergence\n        # Generate some data\n        X, _ = make_blobs(n_samples=100, n_features=2, centers=3,\n                          cluster_std=0.6, shuffle=True, random_state=0)\n\n        # Run affinity propagation with different damping values\n        damping_values = [0.5, 0.8, 0.95]\n        for damping in damping_values:\n            af = AffinityPropagation(damping=damping, max_iter=1000)\n            af.fit(X)\n            # Check if the algorithm converged\n            assert af.n_iter_ < af.max_iter\n", "def test_affinity_propagation_precomputed_and_preference():\n    # Test that precomputed affinity matrix respects the preference parameter\n\n    # Generate simple data\n    X = np.array([[1, 2], [2, 1], [3, 4], [4, 3]])\n    # Define a preference value\n    preference = 10\n    # Calculate precomputed similarity matrix (using negative squared euclidean distance)\n    S = -euclidean_distances(X, squared=True)\n    # Fit AffinityPropagation with precomputed affinity matrix and given preference\n    af = AffinityPropagation(affinity='precomputed', preference=preference).fit(S)\n    # Assert that the preference is reflected in the resulting cluster centers\n    assert (np.array_equal(af.cluster_centers_indices_, [0, 2]))\n\n"], "sample_522": ["    def test_colorbar_title_fontsize():\n        fig, ax = plt.subplots()\n        pc = ax.pcolormesh(np.random.randn(10, 10))\n        cb = fig.colorbar(pc)\n        cb.set_label('My Label', fontsize=14)\n        assert cb.ax.title.get_fontsize() == 14\n\n", "def test_colorbar_labelpad():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc)\n    cb.set_label('My label')\n\n    # default labelpad is 2 points\n    assert cb.ax.yaxis.label.get_verticalalignment() == 'bottom'\n\n    cb.ax.yaxis.labelpad = 10\n    fig.draw_without_rendering()\n    assert cb.ax.yaxis.label.get_position()[1] > cb.ax.bbox.ymin + 10\n", "    def test_colorbar_label_size(self):\n        fig, ax = plt.subplots()\n        pc = ax.pcolormesh(np.random.randn(10, 10))\n        cb = fig.colorbar(pc)\n        cb.set_label('My Label', fontsize=20)\n        assert cb.ax.get_ylabel() == 'My Label'\n        assert cb.ax.yaxis.label.get_fontsize() == 20\n", "    def test_colorbarbase_draw_all(self):\n        fig, ax = plt.subplots()\n        pc = ax.pcolormesh(np.random.randn(10, 10))\n        cb = fig.colorbar(pc)\n        cb.draw_all()\n        # Ensure that draw_all doesn't raise an error.\n", "compilation error"], "sample_334": ["    def test_field_with_choices_not_in_choices(self):\n        class MyForm(Form):\n            choice = ChoiceField(choices=[('A', 'Choice A'), ('B', 'Choice B')])\n\n        form = MyForm({'choice': 'C'})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors, {'choice': ['Select a valid choice. That choice is not one of the available options.']})\n\n\n", "    def test_field_required_attribute_with_widget(self):\n        class MyForm(Form):\n            f1 = CharField(max_length=30, widget=TextInput(attrs={'required': True}))\n\n        form = MyForm()\n        self.assertHTMLEqual(\n            form.as_p(),\n            '<p><label for=\"id_f1\">F1:</label> <input id=\"id_f1\" maxlength=\"30\" name=\"f1\" type=\"text\" required></p>'\n        )\n", "    def test_required_hidden(self):\n        class RequiredHiddenForm(Form):\n            hidden = CharField(widget=HiddenInput, required=True)\n\n        form = RequiredHiddenForm({})\n        self.assertFalse(form.is_valid())\n        self.assertIn('This field is required.', str(form.errors['hidden']))\n", "    def test_no_renderer(self):\n        form = Form(renderer=None)\n        self.assertEqual(form.renderer, get_default_renderer())\n", "    def test_hidden_input_with_none_value(self):\n        class HiddenForm(Form):\n            data = CharField(widget=HiddenInput)\n\n        f = HiddenForm({'data': None})\n        self.assertFalse(f.is_valid())\n        self.assertEqual(f.errors['data'], ['This field is required.'])\n"], "sample_239": ["    def test_formset_with_initial_data_and_extra(self):\n        data = {\n            'choices-TOTAL_FORMS': '3',\n            'choices-INITIAL_FORMS': '2',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        initial = [\n            {'choice': 'Initial Zero', 'votes': 0},\n            {'choice': 'Initial One', 'votes': 1},\n        ]\n        ChoiceFormSet = formset_factory(Choice, extra=1)\n        formset = ChoiceFormSet(data, initial=initial)\n        self.assertEqual(formset.cleaned_data, [\n            {'choice': 'Zero', 'votes': 0},\n            {'choice': 'One', 'votes': 1},\n            {'choice': '', 'votes': ''},\n        ])\n", "    def test_formset_with_custom_error_class(self):\n        class CustomErrorList(ErrorList):\n            pass\n\n        formset = ChoiceFormSet(error_class=CustomErrorList)\n        self.assertEqual(formset.forms[0].error_class, CustomErrorList)\n", "    def test_formset_can_delete_initial_forms(self):\n        data = {\n            'choices-TOTAL_FORMS': '1',\n            'choices-INITIAL_FORMS': '2',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-MAX_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-0-DELETE': 'on',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice, can_delete=True)\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(len(formset.cleaned_data), 1)\n", "    def test_initial_form_count_with_empty_formset(self):\n        formset = ArticleFormSet(initial=[{'title': 'Initial Title', 'pub_date': '2023-10-26'}])\n        self.assertEqual(formset.initial_form_count(), 1)\n", "    def test_formset_error_is_bound(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n\n        self.assertFalse(formset.is_valid())\n        self.assertTrue(formset.errors)\n\n        # Check if the error on the first form is bound\n        self.assertTrue(formset.forms[0].errors)\n        self.assertIn('votes', formset.forms[0].errors)\n        self.assertTrue({'votes': ['This field is required.']} == formset.forms[0].errors)\n"], "sample_392": ["    def test_key_transform_with_function_expression(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.annotate(\n                key=KeyTransform(\"a\", \"value\"),\n                length=Length(\"key\"),\n            ).filter(length=1),\n            [self.objs[3]],\n        )\n", "    def test_raw_sql_in_key_transform(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__a=RawSQL(\n                    self.raw_sql,\n                    ['\"b\"'],\n                ),\n            ),\n            [self.objs[3], self.objs[4]],\n        )\n", "    def test_double_null_lookup_with_key_transform(self):\n        NullableJSONModel.objects.create(value={\"a\": None, \"b\": 1})\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__a__isnull=True,\n                value__b__isnull=False\n            ),\n            [NullableJSONModel.objects.last()],\n        )\n", "    def test_key_transform_with_expression_wrapper(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.annotate(\n                expr=ExpressionWrapper(\n                    KeyTransform(\"a\", \"value\"), output_field=IntegerField()\n                )\n            ).filter(expr=1),\n            [self.objs[3]],\n        )\n", "    def test_key_transform_with_subquery(self):\n        subquery = NullableJSONModel.objects.filter(\n            id__in=RelatedJSONModel.objects.filter(json_model_id=OuterRef(\"pk\"))\n        ).values(\"value__a\")\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.annotate(\n                subquery_value=Subquery(subquery),\n                key=KeyTransform(\"a\", \"subquery_value\"),\n            ).filter(key=\"b\"),\n            [self.objs[3]],\n        )\n"], "sample_381": ["    def test_squashed_migration_with_initial_true(self):\n        class Migration(migrations.Migration):\n            initial = True\n            operations = [migrations.CreateModel('Person', fields=[])]\n            replaced_by = '0002_initial'\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'initial')\n", "    def test_empty_operations(self):\n        class Migration(migrations.Migration):\n            operations = []\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'initial')\n\n", "    def test_squashed_migration(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[]),\n                migrations.DeleteModel('Person'),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'person')\n\n\n\n", "    def test_field_rename(self):\n        before = [\n            ModelState('app', 'MyModel', [\n                ('old_field_name', models.CharField(max_length=255)),\n            ]),\n        ]\n        after = [\n            ModelState('app', 'MyModel', [\n                ('new_field_name', models.CharField(max_length=255)),\n            ]),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['RenameField'])\n        self.assertOperationAttributes(changes, 'app', 0, 0,\n                                       model_name='MyModel',\n                                       old_name='old_field_name',\n                                       new_name='new_field_name')\n", "    def test_mti_model_inheritance_with_self_referential_field(self):\n        Author = ModelState('testapp', 'Author', [\n            ('id', models.AutoField(primary_key=True)),\n            ('name', models.CharField(max_length=100)),\n        ])\n        Story = ModelState('testapp', 'Story', [\n            ('id', models.AutoField(primary_key=True)),\n            ('author', models.ForeignKey('testapp.Author', models.CASCADE, related_name='authors')),\n            ('previous_story', models.ForeignKey('testapp.Story', models.SET_NULL, null=True, blank=True, related_name='next_story')),\n        ])\n        changes = self.get_changes([Author], [Author, Story])\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['CreateModel', 'CreateModel'])\n\n"], "sample_552": ["    def test_subfigure_axes_labels():\n        fig = plt.figure()\n        subfig = fig.add_subfigure(1, 1)\n        ax = subfig.add_subplot(111)\n        ax.set_xlabel(\"X-Axis\")\n        ax.set_ylabel(\"Y-Axis\")\n        subfig.set_xlabel(\"Subfigure X-Axis\")\n        subfig.set_ylabel(\"Subfigure Y-Axis\")\n        assert ax.get_xlabel() == \"X-Axis\"\n        assert ax.get_ylabel() == \"Y-Axis\"\n        assert subfig.get_xlabel() == \"Subfigure X-Axis\"\n        assert subfig.get_ylabel() == \"Subfigure Y-Axis\"\n", "compilation error", "    def test_setp():\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3], [4, 5, 6])\n        plt.setp(ax.lines, linewidth=2, color='r')\n        assert ax.lines[0].get_linewidth() == 2\n        assert ax.lines[0].get_color() == 'r'\n\n", "def test_savefig_bbox_inches_tight():\n    fig, ax = plt.subplots()\n    ax.text(0.9, 0.9, \"Test\", ha=\"right\", va=\"top\", transform=ax.transAxes)\n    fig.savefig(io.BytesIO(), format=\"png\", bbox_inches=\"tight\")\n    # Check that the image generated does not contain extraneous white space\n    # This is a visual check that may need to be adjusted based on platform.\n\n", "def test_subplot_mosaic_empty_string():\n    fig, axd = plt.subplot_mosaic([['A', ''], ['', 'B']])\n    assert len(axd) == 2\n"], "sample_618": ["def test_cross_multidimensional_arrays() -> None:\n    a = xr.DataArray(\n        np.arange(0, 2 * 3 * 4).reshape((2, 3, 4)), dims=[\"time\", \"x\", \"y\"],\n    )\n    b = xr.DataArray(\n        np.arange(0, 2 * 3 * 4).reshape((2, 3, 4)), dims=[\"time\", \"x\", \"y\"],\n    )\n    expected = np.cross(a.data, b.data, axis=-1)\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"test for dask.\")\n        a = a.chunk()\n        b = b.chunk()\n    actual = xr.cross(a, b)\n    xr.testing.assert_duckarray_allclose(expected, actual)\n\n", "    def test_cross_broadcasting_dims(\n        self, a, b, dims, axis, expected_dims", "def test_cross_empty_array():\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"test for dask.\")\n    a = xr.DataArray(np.array([]), dims=['x', 'y'])\n    b = xr.DataArray(np.array([]), dims=['x', 'y'])\n    with pytest.raises(ValueError):\n        xr.cross(a, b, dim='x')\n", "    def test_cross_different_coords(use_dask):\n        if use_dask:\n            if not has_dask:\n                pytest.skip(\"test for dask.\")\n        a = xr.DataArray(\n            np.arange(0, 5 * 3 * 4).reshape((5, 3, 4)),\n            dims=[\"time\", \"cartesian\", \"var\"],\n            coords=dict(\n                time=([\"time\"], np.arange(0, 5)),\n                cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"]),\n                var=([\"var\"], [1, 1.5, 2, 2.5]),\n            ),\n        )\n        b = xr.DataArray(\n            np.arange(0, 5 * 3 * 4).reshape((5, 3, 4)) + 1,\n            dims=[\"time\", \"cartesian\", \"var\"],\n            coords=dict(\n                time=([\"time\"], np.arange(1, 6)),\n                cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"]),\n                var=([\"var\"], [1, 1.5, 2, 2.5]),\n            ),\n        )\n        with pytest.raises(ValueError):\n            xr.cross(a, b, dim=\"cartesian\")\n", "def test_cross_sparse(use_dask) -> None:\n    # Test sparse arrays with NaNs and differing lengths\n\n    a = xr.DataArray(\n        np.array([1, 2, np.nan, 4, 5]), dims=[\"dim_0\"], coords={\"dim_0\": [0, 1, 2, 3, 4]}\n    )\n    b = xr.DataArray(\n        np.array([6, 7, 8, np.nan, 10]), dims=[\"dim_0\"], coords={\"dim_0\": [0, 1, 2, 3, 4]}\n    )\n\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"test for dask.\")\n        a = a.chunk()\n        b = b.chunk()\n\n    expected = np.cross(a.fillna(0).values, b.fillna(0).values, axis=0)\n    actual = xr.cross(a, b, dim=\"dim_0\")\n    xr.testing.assert_duckarray_allclose(expected, actual)\n\n"], "sample_131": ["    def test_clone_test_db(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        saved_settings = copy.deepcopy(connection.settings_dict)\n        try:\n            connection.settings_dict['NAME'] = 'original_db'\n            with mock.patch.object(creation, '_clone_test_db'):\n                creation.clone_test_db(suffix='clone1', verbosity=0, autoclobber=True, keepdb=False)\n        finally:\n            connection.settings_dict = saved_settings\n\n", "    def test_serialize_db_to_string_with_no_models(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        saved_settings = copy.deepcopy(connection.settings_dict)\n        try:\n            # Mock out the apps.get_app_configs() to return an empty list\n            with mock.patch('django.apps.apps.get_app_configs', return_value=[]), \\\n                 mock.patch.object(creation, '_create_test_db'):\n                serialized_data = creation.serialize_db_to_string()\n            self.assertEqual(serialized_data, '')\n        finally:\n            connection.settings_dict = saved_settings\n\n", "    def test_create_test_db_keepdb(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        saved_settings = copy.deepcopy(connection.settings_dict)\n        try:\n            connection.settings_dict['TEST']['MIGRATE'] = True\n            with mock.patch.object(creation, '_create_test_db') as mocked_create_test_db:\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False, keepdb=True)\n                mocked_create_test_db.assert_called_once_with(\n                    verbosity=0, autoclobber=True, keepdb=True\n                )\n        finally:\n            connection.settings_dict = saved_settings\n\n", "    def test_clone_test_db(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        saved_settings = copy.deepcopy(connection.settings_dict)\n        try:\n            # Mock the _clone_test_db method so it doesn't raise NotImplementedError\n            with mock.patch.object(creation, '_clone_test_db'):\n                creation.clone_test_db('clone_suffix', verbosity=0, autoclobber=True, keepdb=False)\n        finally:\n            connection.settings_dict = saved_settings\n", "    def test_create_test_db_keepdb(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        with mock.patch.object(creation, '_create_test_db') as mock_create_test_db:\n            creation.create_test_db(verbosity=0, autoclobber=True, serialize=False, keepdb=True)\n            mock_create_test_db.assert_called_once()\n"], "sample_1082": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_513": ["def test_legend_ncol_too_few():\n    # Test that ncols less than the number of labels raises a warning\n    strings = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]\n    with pytest.warns(UserWarning):\n        fig, ax = plt.subplots()\n        ax.legend(strings, ncol=2)\n\n", "def test_legend_title_fontproperties():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label='Aardvark')\n    fontprops = FontProperties(family='serif', size=14, weight='bold')\n    title = 'Legend Title'\n\n    # Test setting font properties through the title_fontproperties kwargs\n    leg = ax.legend(title=title, title_fontproperties=fontprops)\n    assert leg.get_title().get_fontproperties() == fontprops\n    assert leg.get_title().get_text() == title\n\n", "    def test_legend_with_empty_strings(self):\n        fig, ax = plt.subplots()\n        ax.plot([0, 1], [0, 1], label='Data')\n        ax.plot([0, 1], [1, 0], label='')\n        ax.legend()\n", "def test_legend_title_fontsize_float():\n    # test the title_fontsize kwarg with a float\n    plt.plot(range(10))\n    with pytest.raises(ValueError):\n        plt.legend(title='Aardvark', title_fontsize=22.5)\n", "def test_legend_bbox_to_anchor_transform():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [2, 3], label='Test')\n\n    trans = mtransforms.Affine2D().translate(0.1, 0.1)\n    leg = ax.legend(bbox_to_anchor=(0, 0), loc='upper left',\n                    transform=trans)\n\n    assert leg.get_window_extent().x0 == 0.1\n    assert leg.get_window_extent().y1 == 0.9\n\n"], "sample_731": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_data_integrity():\n    data = fetch()\n    assert data.DESCR == __doc__\n    assert data.feature_names == [\"MedInc\", \"HouseAge\", \"AveRooms\", \"AveBedrms\",\n                     \"Population\", \"AveOccup\", \"Latitude\", \"Longitude\"]\n"], "sample_600": ["    def test_UnsignedIntegerCoder_encode_keep_dtype(self, dtype):\n        original = xr.Variable(\n            (\"x\",), np.array([1, 2, 3], dtype=dtype), attrs={\"_Unsigned\": \"true\"}\n        )\n        coder = variables.UnsignedIntegerCoder()\n        roundtripped = coder.decode(coder.encode(original))\n\n        assert roundtripped.dtype == original.dtype\n        assert_identical(original, roundtripped)\n", "    def test_UnsignedIntegerCoder_unsigned_dtype_without_attribute():\n        data = np.array([1, 2, 3], dtype=np.uint8)\n        original = xr.Variable((\"x\",), data)\n        coder = variables.UnsignedIntegerCoder()\n        decoded = coder.decode(original)\n        assert decoded.dtype == np.uint8\n        assert_allclose(decoded.values, data)\n", "compilation error", "def test_CFMaskCoder_encode_with_existing_fill_value_in_attrs():\n    original = xr.Variable((\"x\",), [0, -1, 1], {\"_FillValue\": -1, \"missing_value\":-999})\n    coder = variables.CFMaskCoder()\n\n    with pytest.warns(variables.SerializationWarning):\n        encoded = coder.encode(original)\n\n    assert \"_FillValue\" not in encoded.attrs\n    assert \"missing_value\" in encoded.attrs\n    assert encoded.attrs[\"missing_value\"] == -999\n\n", "def test_unsigned_coder_roundtrip(dtype):\n    original = xr.Variable((\"x\",), np.arange(5, dtype=dtype))\n    coder = variables.UnsignedIntegerCoder()\n    roundtripped = coder.decode(coder.encode(original))\n    assert_identical(original, roundtripped)\n"], "sample_377": ["    def test_sensitive_variables_with_kwargs(self):\n        @sensitive_variables(\"password\")\n            return password\n\n        with self.assertRaises(TypeError) as cm:\n            test_func(password=\"super_secret\", other=\"value\")\n\n        self.assertEqual(\n            str(cm.exception),\n            \"sensitive_variables() should only accept one positional argument.\",\n        )\n", "    def test_sensitive_variables_multiple_calls(self):\n        @sensitive_variables('secret')\n        @sensitive_variables('password')\n            pass\n\n        decorated_func = test_func\n        self.assertEqual(decorated_func.__name__, 'test_func')\n        self.assertIn('secret', decorated_func.__wrapped__.__annotations__)\n        self.assertIn('password', decorated_func.__wrapped__.__annotations__)\n", "    def test_sensitive_variables_with_method_decorator(self):\n        class MyClass:\n            @sensitive_variables\n                return HttpResponse()\n\n        view = MyClass()\n        view.a_view(HttpRequest(), \"my_password\")\n", "    def test_sensitive_variables_on_class_method(self):\n        class MyClass:\n            @sensitive_variables(\"password\")\n            @classmethod\n                pass\n\n        self.assertEqual(MyClass.a_method.__name__, \"a_method\")\n", "    def test_sensitive_variables_with_args(self):\n        @sensitive_variables(\"secret_key\", \"api_token\")\n            pass\n"], "sample_781": ["    def test_forest_oob_score_with_sample_weight(self):\n        X = np.random.rand(100, 10)\n        y = np.random.randint(0, 2, size=100)\n        sample_weight = np.random.rand(100)\n        clf = RandomForestClassifier(oob_score=True, random_state=0)\n        clf.fit(X, y, sample_weight=sample_weight)\n        assert hasattr(clf, 'oob_score_')\n", "    def test_oob_score_multioutput(self):\n        # Test that the oob_score is calculated correctly for multi-output\n        # regression.\n\n        X, y = make_regression(n_samples=100, n_targets=2, random_state=42)\n        clf = RandomForestRegressor(n_estimators=10, oob_score=True,\n                                   random_state=42)\n        clf.fit(X, y)\n        assert hasattr(clf, 'oob_score_')\n        assert isinstance(clf.oob_score_, float)\n        assert 0 <= clf.oob_score_ <= 1\n", "    def test_forest_feature_importances_empty_data(self):\n        X = np.array([]).reshape(0, 10)\n        y = np.array([], dtype=int)\n        forest = RandomForestClassifier()\n        forest.fit(X, y)\n        assert forest.feature_importances_.size == 10\n        assert np.all(forest.feature_importances_ == 0)\n", "def test_forest_oob_score_with_all_samples_in_same_leaf():\n    # Test for a case where all samples end up in the same leaf for each tree.\n    # This can happen with small datasets or highly correlated features.\n\n    X = np.zeros((10, 1))\n    y = np.zeros(10)\n    rf = RandomForestClassifier(n_estimators=10, oob_score=True, random_state=42)\n    rf.fit(X, y)\n    assert rf.oob_score_ == 1.0\n", "    def test_forest_feature_importances_binary_classification(self):\n        X, y = make_classification(n_samples=15, n_informative=3,\n                                   random_state=1, n_classes=2)\n        clf = RandomForestClassifier(min_samples_leaf=5, random_state=42,\n                                     n_estimators=200).fit(X, y)\n        assert math.isclose(1, clf.feature_importances_.sum(), abs_tol=1e-7)\n"], "sample_1103": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_345": ["    def test_should_stop(self):\n        self.reloader.should_stop = True\n        self.assertTrue(self.reloader.should_stop())\n", "    def test_should_stop(self):\n        # Stop should be true if the reloader is stopping and there are no files left in the watchlist.\n        self.reloader.stopping = True\n        self.reloader.files_to_watch = {}\n        self.assertTrue(self.reloader.should_stop())\n\n", "    def test_should_stop_returns_true_if_error_files_not_empty(self):\n        autoreload._exception = (Exception('test'),)\n        self.assertTrue(self.reloader.should_stop())\n", "    def test_should_stop_returns_true_when_stop_is_called(self):\n        self.reloader.stop()\n        self.assertTrue(self.reloader.should_stop())\n", "    def test_watch_dir_with_absolute_path(self):\n        path = Path(self.tempdir)\n        self.reloader.watch_dir(path, '*.py')\n        self.assertIn(self.existing_file, list(self.reloader.watched_files()))\n\n"], "sample_909": ["    def test_numpydocstring_empty_return(self):\n        docstring = \"\"\"", "    def test_see_also_with_empty_lines(self):\n        docstring = \"\"\"\\", "    def test_returns_with_types(self):\n        docstring = \"\"\"\\", "    def test_empty_sections(self):\n        docstrings = [(\"\"\"", "    def test_see_also_empty_refs(self):\n        docstring = \"\"\"\\"], "sample_1195": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_432": ["    def test_search_input_placeholder(self):\n        self.admin_login(username=\"super\", password=\"secret\")\n        self.selenium.get(self.live_server_url + reverse(\"admin:auth_user_changelist\"))\n        search_input = self.selenium.find_element(By.ID, \"searchbar\")\n        self.assertEqual(search_input.get_attribute(\"placeholder\"), \"Search\")\n", "    def test_ordering_dropdown_options(self):\n        from selenium.webdriver.common.by import By\n        from selenium.webdriver.support.ui import Select\n\n        self.admin_login(username=\"super\", password=\"secret\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_changelist_parent_changelist\")\n        )\n\n        dropdown = Select(self.selenium.find_element(By.CSS_SELECTOR, \"select[name='ordering']\"))\n\n        options = [option.text for option in dropdown.options]\n        self.assertIn(\"name\", options)\n        self.assertIn(\"-name\", options)\n        self.assertIn(\"created\", options)\n        self.assertIn(\"-created\", options)\n        self.assertNotIn(\"id\", options)\n", "    def test_ordering_by_related_field_with_null_values(self):\n        from selenium.webdriver.common.by import By\n\n        Parent.objects.create(name=\"parent1\")\n        Child.objects.create(\n            name=\"child1\", parent=Parent.objects.get(name=\"parent1\")\n        )\n        Child.objects.create(name=\"child2\")\n\n        self.admin_login(username=\"super\", password=\"secret\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_changelist_child_changelist\")\n        )\n\n        select = self.selenium.find_element(By.NAME, \"ordering\")\n        option = select.find_elements(By.TAG_NAME, \"option\")[1]\n        option.click()\n\n        self.selenium.find_element(By.NAME, \"_save\").click()\n        rows = self.selenium.find_elements(By.CSS_SELECTOR, \"tbody tr\")\n        self.assertEqual(len(rows), 2)\n        self.assertEqual(rows[0].text, \"child1 parent1\")\n        self.assertEqual(rows[1].text, \"child2\")\n", "    def test_default_filters_preserve_state(self):\n        from selenium.webdriver.common.by import By\n\n        self.admin_login(username=\"super\", password=\"secret\")\n        self.selenium.get(self.live_server_url + reverse(\"admin:auth_user_changelist\"))\n\n        # Close all filters\n        for detail in self.selenium.find_elements(By.CSS_SELECTOR, \"details\"):\n            summary = detail.find_element(By.CSS_SELECTOR, \"summary\")\n            summary.click()\n\n        self.selenium.refresh()\n\n        # All filters should still be closed\n        for detail in self.selenium.find_elements(By.CSS_SELECTOR, \"details\"):\n            self.assertFalse(detail.get_attribute(\"open\"))\n", "    def test_filter_with_empty_choices(self):\n        from selenium.webdriver.common.by import By\n        from selenium.webdriver.support.ui import Select\n\n        self.admin_login(username=\"super\", password=\"secret\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_changelist_book_changelist\")\n        )\n        # Find the author filter\n        author_filter = self.selenium.find_element(By.ID, \"id_author\")\n        select = Select(author_filter)\n\n        # Assert that there is an empty option in the filter selection\n        self.assertEqual(select.first_selected_option.text, \"---------\")\n"], "sample_572": ["    def test_estimate_aggregator_weights(self, long_df):\n\n        agg = EstimateAggregator(\"mean\")\n        out = agg(long_df, \"x\", weights=long_df[\"w\"])\n        assert out[\"x\"] == (long_df[\"x\"] * long_df[\"w\"]).sum() / long_df[\"w\"].sum()\n", "    def test_empty_data(self):\n        agg = EstimateAggregator(\"mean\")\n        out = agg(pd.DataFrame(), \"x\")\n        assert pd.isna(out[\"x\"])\n        assert pd.isna(out.get(\"xmin\"))\n        assert pd.isna(out.get(\"xmax\"))\n", "    def test_empty_data(self):\n        agg = EstimateAggregator(\"mean\", \"sd\")\n        df = pd.DataFrame({\"x\": []})\n        out = agg(df, \"x\")\n        assert pd.isna(out[\"x\"])\n        assert pd.isna(out[\"xmin\"])\n        assert pd.isna(out[\"xmax\"])\n", "    def test_empty_data(self):\n        agg = EstimateAggregator(\"mean\")\n        out = agg(pd.DataFrame(dict(x=[])), \"x\")\n        assert pd.isna(out[\"x\"])\n        assert pd.isna(out[\"xmin\"])\n        assert pd.isna(out[\"xmax\"])\n", "    def test_aggregate_no_errorbars(self, long_df):\n        agg = EstimateAggregator(\"mean\")\n        out = agg(long_df, \"x\")\n        assert \"xmin\" not in out\n        assert \"xmax\" not in out\n"], "sample_1125": ["def test_IdentityOperator():\n    I = IdentityOperator()\n    assert I.dimension is oo\n    assert I*I == I\n    assert adjoint(I) == I\n    assert Dagger(I) == I\n", "def test_OuterProduct():\n    from sympy.physics.quantum.operator import OuterProduct\n    from sympy.physics.quantum.state import Ket, Bra\n\n    k = Ket('k')\n    b = Bra('b')\n    op = OuterProduct(k, b)\n\n    assert op.ket == k\n    assert op.bra == b\n    assert Dagger(op) == OuterProduct(b, k)\n\n    assert str(op) == '|k><b|'\n", "def test_identity_operator():\n    i = IdentityOperator()\n    assert Dagger(i) == i\n    assert i*i == i\n    assert i.dimension == oo\n    i2 = IdentityOperator(3)\n    assert i2.dimension == 3 \n", "def test_operator_commutator():\n    A = Operator('A')\n    B = Operator('B')\n    assert A._eval_commutator(B) is None\n    assert B._eval_commutator(A) is None\n", "def test_identity_operator():\n    I = IdentityOperator()\n    assert Dagger(I) == I\n    assert I * I == I\n    assert I.dimension == oo \n"], "sample_217": ["    def test_media_inheritance_empty_parent(self):\n        class MyWidget1(TextInput):\n            pass\n\n        class MyWidget2(MyWidget1):\n            class Media:\n                css = {\n                    'all': ('/path/to/css1',),\n                }\n        w2 = MyWidget2()\n        self.assertEqual(str(w2.media), \"\"\"<link href=\"http://media.example.com/static/path/to/css1\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">\"\"\")\n\n", "    def test_media_empty_widget(self):\n        class MyEmptyWidget(TextInput):\n            class Media:\n                pass\n\n        w = MyEmptyWidget()\n        self.assertEqual(str(w.media), \"\") \n", "    def test_add_empty_media(self):\n        media = Media(css={'screen': ['a.css']}, js=['a'])\n        empty_media = Media()\n        merged = empty_media + media\n        self.assertEqual(merged._css_lists, [{'screen': ['a.css']}])\n        self.assertEqual(merged._js_lists, [['a']])\n\n", "    def test_media_class_inheritance(self):\n        class BaseWidget(TextInput):\n            class Media:\n                css = {'all': ('base.css',)}\n\n        class DerivedWidget(BaseWidget):\n            class Media:\n                js = ('derived.js',)\n\n        w = DerivedWidget()\n        self.assertEqual(str(w.media), \"\"\"<link href=\"http://media.example.com/static/base.css\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">", "    def test_media_property_with_none(self):\n        class MyWidget(TextInput):\n                return None\n            media = property(_media)\n        w = MyWidget()\n        self.assertEqual(str(w.media), \"\")\n\n"], "sample_944": ["    def test_restify_type_hints_Literal():\n        from typing import Literal  # type: ignore\n        assert restify(Literal[1, 2, 3]) == \":obj:`Literal`\\\\ [:class:`int`]\"\n        assert restify(Literal[\"a\", \"b\", \"c\"]) == \":obj:`Literal`\\\\ [:class:`str`]\"\n", "    def test_restify_type_hints_typevars_with_bounds():\n        T = TypeVar('T', bound=str)\n        assert restify(T) == \":obj:`tests.test_util_typing.T`\"\n        assert restify(List[T]) == \":class:`List`\\\\ [:obj:`tests.test_util_typing.T`]\"\n", "def test_restify_nested_type_hints():\n    assert restify(List[Dict[str, List[int]]]) == (\n        ':class:`List`\\\\ [:class:`Dict`\\\\ [:class:`str`, :class:`List`\\\\ [:class:`int`]]]'\n    )\n\n\n", "def test_restify_type_hints_newtype():\n    assert restify(MyInt) == \":class:`MyInt`\"\n", "def test_restify_type_hints_generic_alias():\n    MyList = list[str]  # type: ignore\n    assert restify(MyList) == \":class:`list`\\\\ [:class:`str`]\"\n"], "sample_984": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_992": ["    def test_NumPyPrinter_Piecewise():\n        p = NumPyPrinter()\n        expr = Piecewise((x, x < 0), (y, x > 0), (x + y, True))\n        assert p.doprint(expr) == 'numpy.select([x < 0, x > 0, True], [x, y, x + y], default=numpy.nan)'\n", "compilation error", "def test_NumPyPrinter_Piecewise():\n    p = NumPyPrinter()\n    expr = sympy.Piecewise((0, x < 0),\n                           (1, x >= 0))\n    assert p.doprint(expr) == 'numpy.select([x < 0, x >= 0], [0, 1], default=numpy.nan)'\n", "compilation error", "compilation error"], "sample_192": ["    def test_empty_formset_with_initial_data(self):\n        data = {\n            'form-TOTAL_FORMS': '0',\n            'form-INITIAL_FORMS': '0',\n            \n        }\n        formset = ArticleFormSet(data, initial=[{'title': 'Test', 'pub_date': '1904-06-16'}])\n        self.assertEqual(0, formset.initial_form_count())\n        self.assertEqual(1, formset.total_form_count())\n        self.assertTrue(formset.is_bound)\n        self.assertTrue(formset.forms[0].is_bound)\n        self.assertTrue(formset.is_valid())\n        self.assertTrue(formset.forms[0].is_valid())\n        self.assertEqual([{'title': 'Test', 'pub_date': '1904-06-16'}], formset.cleaned_data)\n", "    def test_empty_formset_with_initial_data(self):\n        ChoiceFormSet = formset_factory(Choice, extra=0)\n        formset = ChoiceFormSet(initial=[{'choice': 'Initial value', 'votes': 10}])\n        self.assertEqual(len(formset.forms), 1)\n        self.assertEqual(formset.forms[0].initial_data, {'choice': 'Initial value', 'votes': 10})\n", "    def test_formset_initial_data(self):\n        data = {\n            'choices-TOTAL_FORMS': '3',\n            'choices-INITIAL_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n            'choices-2-choice': 'Two',\n            'choices-2-votes': '2',\n        }\n        initial = [\n            {'choice': 'Three', 'votes': '3'},\n            {'choice': 'Four', 'votes': '4'},\n            {'choice': 'Five', 'votes': '5'},\n        ]\n        ChoiceFormSet = formset_factory(Choice, extra=0)\n        formset = ChoiceFormSet(data, initial=initial, prefix='choices')\n        self.assertEqual(len(formset.forms), 3)\n        self.assertEqual(formset.forms[0].initial, {'choice': 'Zero', 'votes': '0'})\n        self.assertEqual(formset.forms[1].initial, {'choice': 'One', 'votes': '1'})\n        self.assertEqual(formset.forms[2].initial, {'choice': 'Two', 'votes': '2'})\n\n\n", "    def test_empty_formset_with_initial(self):\n        \"\"\"Empty formset should work with initial data.\"\"\"\n        formset = FavoriteDrinksFormSet(initial=[{'name': 'Gin Tonic'}])\n        self.assertEqual(formset.forms[0].initial_data, {'name': 'Gin Tonic'})\n        self.assertEqual(len(formset.forms), 1)\n\n\n", "    def test_formset_get_forms_by_index(self):\n        data = {\n            'choices-TOTAL_FORMS': '3',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n            'choices-2-choice': 'Two',\n            'choices-2-votes': '2',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertEqual(formset.get_forms_by_index(1)[0].cleaned_data, {'choice': 'One', 'votes': 1})\n"], "sample_306": ["    def test_invalid_date(self):\n        with self.assertRaises(ValueError):\n            parse_datetime('2012-04-31T10:20:30')\n        with self.assertRaises(ValueError):\n            parse_datetime('2012-02-29T10:20:30')\n", "    def test_parse_duration_invalid(self):\n        invalid_inputs = (\n            '1 day 25:00:00',\n            '25:00:00',\n            '1 day 0:60:00',\n            '30.',\n            '15:30.123456789',\n            '1 day 10:15:30.123456789',\n            '1 day-',\n            '1 day 10:15',\n        )\n        for source in invalid_inputs:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_duration(source))\n", "    def test_invalid_input(self):\n        self.assertIsNone(parse_duration('invalid'))\n", "    def test_postgres_interval_format_mixed(self):\n        test_values = (\n            ('3 days 04:05:06', timedelta(days=3, hours=4, minutes=5, seconds=6)),\n            ('3 days 04:05:06.123', timedelta(days=3, hours=4, minutes=5, seconds=6, milliseconds=123)),\n            ('- 3 days 04:05:06', timedelta(days=-3, hours=4, minutes=5, seconds=6)),\n            (' -3 days 04:05:06 ', timedelta(days=-3, hours=4, minutes=5, seconds=6)),\n            ('3 days -04:05:06', timedelta(days=3, hours=-4, minutes=-5, seconds=-6)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "    def test_parse_duration_invalid(self):\n        invalid_inputs = (\n            '15', '15:30:', '15:30:30:', '15::30', '15:30:30.30.30',\n            '15:30:30.', '15:30:30.a', '15:30:a', '15:a:30',\n            '15days', '15 hours', '15:30 days', 'abc', '', \n            'P1D1H', 'P1DT1H', 'P1D1H0S', 'P1D1H0M', 'P1D1H0M0S.1'\n        )\n        for source in invalid_inputs:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_duration(source))\n\n"], "sample_1168": ["def test_multiset_permutations_empty():\n    assert list(multiset_permutations([])) == [[]]\n    assert list(multiset_permutations(multiset([]))) == [[]]\n", "def test_minlex_iterable():\n    assert minlex([1, 2, 0], key=lambda x: abs(x - 1)) == (0, 1, 2)\n", "    def test_multiset_permutations_empty():\n        assert list(multiset_permutations([])) == [[]]\n\n", "    def test_cartesian_product():\n        assert list(cartesian_product([1, 2], [3, 4])) == \\\n            [(1, 3), (1, 4), (2, 3), (2, 4)]\n        assert list(cartesian_product([1, 2, 3], [4, 5])) == \\\n            [(1, 4), (1, 5), (2, 4), (2, 5), (3, 4), (3, 5)]\n        assert list(cartesian_product([1], [2])) == [(1, 2)]\n        assert list(cartesian_product([], [2])) == []\n        assert list(cartesian_product([1, 2], [])) == []\n        assert list(cartesian_product([], [])) == []\n\n", "compilation error"], "sample_400": ["    def test_suggestion_for_custom_operation(self):\n        class CustomOperation(migrations.Operation):\n                super().__init__(*args, **kwargs)\n\n                return (\n                    \"custom_operation\",\n                    \"tests.migrations.tests.CustomOperation\",\n                    [],\n                    {},\n                )\n\n        class Migration(migrations.Migration):\n            operations = [CustomOperation()]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertIs(migration.suggest_name().startswith(\"auto_\"), True)\n\n", "    def test_rename_model(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.RenameModel(\"OldName\", \"NewName\"),\n            ]\n\n        migration = Migration(\"0001_initial\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"rename_oldname_to_newname\")  \n", "    def test_auto_with_squashed_migration(self):\n        migration = migrations.Migration(\"0002_squashed_0001_initial\", \"test_app\")\n        suggest_name = migration.suggest_name()\n        self.assertEqual(suggest_name, \"squashed_0001_initial\")\n", "compilation error", "    def test_operation_with_name_and_suffix(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\"Book\", fields=[(\"title\", models.CharField(max_length=200))]),\n                migrations.AddField(\n                    model_name=\"book\",\n                    name=\"author\",\n                    field=models.ForeignKey(\"testapp.Author\", on_delete=models.CASCADE),\n                ),\n            ]\n\n        migration = Migration(\"some_migration\", \"testapp\")\n        self.assertEqual(migration.suggest_name(), \"book_and_author\")\n"], "sample_138": ["    def test_manifest_strict_no_file(self):\n        self.storage.manifest_strict = True\n        # Delete a file that is referenced in the manifest\n        os.remove(self._get_filename_path('foo.png'))\n\n        with self.assertRaises(ValueError) as cm:\n            self.run_collectstatic()\n        self.assertIn(\"The file 'foo.png' could not be found with\", str(cm.exception))\n", "    def test_manifest_cache_busting(self):\n        file_contents = (\n            ('styles.css', 'body { background: url(\"image.png\"); }'),\n            ('image.png', 'image data'),\n        )\n        for filename, content in file_contents:\n            with open(self._get_filename_path(filename), 'w') as f:\n                f.write(content)\n\n        with self.modify_settings(STATICFILES_DIRS={'append': self._temp_dir}):\n            finders.get_finder.cache_clear()\n            call_command('collectstatic', interactive=False, verbosity=0)\n\n            relpath = self.hashed_file_path('test/styles.css')\n            with storage.staticfiles_storage.open(relpath) as relfile:\n                content = relfile.read()\n                original_hash = re.search(rb'url\\(\"image\\.(.+?)\\\"', content).group(1).decode()\n\n            # Modify image.png to trigger a cache bust.\n            with open(self._get_filename_path('image.png'), 'w') as f:\n                f.write(b'modified image data')\n\n            call_command('collectstatic', interactive=False, verbosity=0)\n\n            relpath = self.hashed_file_path('test/styles.css')\n            with storage.staticfiles_storage.open(relpath) as relfile:\n                content = relfile.read()\n                new_hash = re.search(rb'url\\(\"image\\.(.+?)\\\"', content).group(1).decode()\n\n            self.assertNotEqual(original_hash, new_hash)\n\n\n", "    def test_collectstatic_with_nested_directories(self):\n        # Create a directory structure with nested static files.\n        os.makedirs(os.path.join(self._temp_dir, 'test', 'subdir', 'nested'))\n        file_contents = (\n            ('test/subdir/nested/file.css', 'url(\"image.png\")'),\n            ('test/image.png', 'image content'),\n        )\n        for filename, content in file_contents:\n            with open(self._get_filename_path(filename), 'w') as f:\n                f.write(content)\n\n        with self.modify_settings(STATICFILES_DIRS={'append': self._temp_dir}):\n            finders.get_finder.cache_clear()\n            err = StringIO()\n            call_command('collectstatic', interactive=False, verbosity=0, stderr=err)\n            relpath = self.hashed_file_path('test/subdir/nested/file.css')\n            with storage.staticfiles_storage.open(relpath) as relfile:\n                content = relfile.read()\n                self.assertIn(b'image.d41d8cd98f00.png', content)\n\n", "    def test_preserve_manifest_strict_mode(self):\n        # Ensure that 'strict' mode is preserved across collectstatic runs.\n        initial_strict_mode = storage.staticfiles_storage.manifest_strict\n        self.assertFalse(initial_strict_mode)\n\n        storage.staticfiles_storage.manifest_strict = True\n        call_command('collectstatic', interactive=False, verbosity=0)\n\n        self.assertTrue(storage.staticfiles_storage.manifest_strict)\n        # Second run should preserve the strict mode.\n        call_command('collectstatic', interactive=False, verbosity=0)\n        self.assertTrue(storage.staticfiles_storage.manifest_strict)\n", "    def test_manifest_strict_mode(self):\n        # Test that strict mode raises an error when a file is referenced\n        # in a manifest but not found in the filesystem.\n        self.run_collectstatic()\n        storage.staticfiles_storage.manifest_strict = True\n        # Delete a file from the filesystem to cause an error\n        os.remove(self._get_filename_path('test/foo.png'))\n        with self.assertRaises(ValueError):\n            self.run_collectstatic()\n"], "sample_958": ["def test_domain_cpp_parse_mix_decl_duplicate_roles(app, warning):\n    # Issue 8270\n    text = (\".. cpp:struct:: A\\n\"\n            \".. cpp:member:: int A::a\\n\"\n            \".. cpp:function:: void A()\\n\"\n            \".. cpp:struct:: A\\n\"\n            \".. cpp:member:: int A::b\\n\")\n    restructuredtext.parse(app, text)\n    ws = warning.getvalue().split(\"\\n\")\n    assert len(ws) == 7\n    assert \"index.rst:3: WARNING: Duplicate C++ declaration, also defined at index:1.\" in ws[0]\n    assert \"Declaration is '.. cpp:function:: void A()'.\" in ws[1]\n    assert \"index.rst:4: WARNING: Duplicate C++ declaration, also defined at index:1.\" in ws[2]\n    assert \"Declaration is '.. cpp:struct:: A'.\" in ws[3]\n    assert \"index.rst:3: WARNING: Duplicate C++ declaration, also defined at index:1.\" in ws[4]\n    assert \"Declaration is '.. cpp:function:: void A()'.\" in ws[5]\n    assert ws[6] == \"\"\n", "def test_domain_cpp_parse_decl_with_special_chars(app):\n    text = \"\"\"", "compilation error", "    def check_duplicate_decl(app, warning):\n        text = (\".. cpp:class:: A\\n\"\n                \".. cpp:class:: A\\n\"\n                \".. cpp:member:: A::a\\n\")\n        restructuredtext.parse(app, text)\n        ws = warning.getvalue().split(\"\\n\")\n        assert len(ws) == 3\n        assert \"index.rst:2: WARNING: Duplicate C++ declaration, also defined at index:1.\" in ws[0]\n        assert \"Declaration is '.. cpp:class:: A'.\" in ws[1]\n        assert ws[2] == \"\"\n\n", "def test_domain_cpp_build_no_default_id(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"no-default-id\")\n    assert len(ws) == 2\n    assert \"WARNING: cpp:member reference target not found: A::b\" in ws[0]\n    assert \"WARNING: cpp:member reference target not found: A::c\" in ws[1]\n\n"], "sample_337": ["    def test_session_modify_after_error_handling(self):\n        with self.assertLogs('django.security.csrf', 'WARNING') as cm:\n            response = self.client.post('/error/403/', data={'csrfmiddlewaretoken': 'invalid'})\n        self.assertEqual(response.status_code, 403)\n        self.assertEqual(cm.output, [\n            'WARNING:django.security.csrf:Forbidden (CSRF token missing or incorrect.): /error/403/'\n        ])\n        req = response.wsgi_request\n        csrf_cookie = self._read_csrf_cookie(req)\n        self.assertTrue(csrf_cookie)\n        req.session.modified = False\n        response = self.client.get('/')\n        self.assertEqual(response.status_code, 200)\n        self.assertFalse(req.session.modified)\n", "    def test_csrf_token_on_500_stays_constant(self):\n        response = self.client.get('/error/')\n        self.assertEqual(response.status_code, 500)\n        token1 = response.content\n        response = self.client.get('/error/')\n        self.assertEqual(response.status_code, 500)\n        token2 = response.content\n        self.assertTrue(equivalent_tokens(token1.decode('ascii'), token2.decode('ascii')))\n", "    def test_csrf_token_on_500_stays_constant(self):\n        response = self.client.get('/causes_error/')\n        self.assertEqual(response.status_code, 500)\n        token1 = response.content\n        response = self.client.get('/causes_error/')\n        self.assertEqual(response.status_code, 500)\n        token2 = response.content\n        self.assertTrue(equivalent_tokens(token1.decode('ascii'), token2.decode('ascii')))\n", "    def test_csrf_token_on_500_stays_constant(self):\n        response = self.client.get('/error/')\n        # The error handler returns status code 500.\n        self.assertEqual(response.status_code, 500)\n        token1 = response.content\n        response = self.client.get('/error/')\n        self.assertEqual(response.status_code, 500)\n        token2 = response.content\n        self.assertTrue(equivalent_tokens(token1.decode('ascii'), token2.decode('ascii')))\n\n", "    def test_csrftoken_accessible_in_javascript(self):\n        \"\"\"\n        The CSRF token should be accessible in JavaScript when CSRF_COOKIE_HTTPONLY is False.\n        \"\"\"\n        response = self.client.get('/')\n        self.assertEqual(response.status_code, 200)\n        self.assertIn('csrftoken', response.context['csrf_token'])\n        # Check if the csrftoken is accessible in JavaScript\n        self.assertIn(f\"var csrftoken = '{response.context['csrf_token']['csrf_token']}'\", response.content.decode())\n"], "sample_609": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_388": ["    def test_persistent_remote_user_with_session(self):\n        User.objects.create(username=\"knownuser\")\n        # Known user authenticates\n        self.client.get(\"/remote_user/\", **{self.header: self.known_user})\n        self.client.cookies.clear()\n        # Should stay logged in even if the REMOTE_USER header disappears\n        # and session is cleared\n        response = self.client.get(\"/remote_user/\")\n        self.assertFalse(response.context[\"user\"].is_anonymous)\n        self.assertEqual(response.context[\"user\"].username, \"knownuser\")\n", "    def test_session_persistence_after_logout(self):\n        \"\"\"\n        After logging out, a user should not be persistently logged in if the\n        REMOTE_USER header is present in the subsequent request.\n        \"\"\"\n        User.objects.create(username=\"knownuser\")\n\n        # Known user authenticates\n        response = self.client.get(\"/remote_user/\", **{self.header: self.known_user})\n        self.assertEqual(response.context[\"user\"].username, \"knownuser\")\n\n        # Log out the user\n        self.client.logout()\n\n        # Subsequent request with the REMOTE_USER header should not log the user in\n        response = self.client.get(\"/remote_user/\", **{self.header: self.known_user})\n        self.assertTrue(response.context[\"user\"].is_anonymous\n", "    def test_multiple_remote_user_headers(self):\n        \"\"\"\n        Test that the middleware correctly handles multiple REMOTE_USER headers\n        appearing in the request.\n        \"\"\"\n        User.objects.create(username=\"knownuser\")\n\n        response = self.client.get(\n            \"/remote_user/\",\n            **{self.header: self.known_user, \"HTTP_\" + self.header: \"unknownuser\"},\n        )\n        self.assertEqual(response.context[\"user\"].username, self.known_user)\n", "    def test_user_can_authenticate(self):\n        \"\"\"\n        Test the user_can_authenticate method of RemoteUserBackend\n        and AllowAllUsersRemoteUserBackend.\n        \"\"\"\n        self.assertTrue(RemoteUserBackend().user_can_authenticate(User.objects.create(username=\"user1\", is_active=True)))\n        self.assertFalse(RemoteUserBackend().user_can_authenticate(User.objects.create(username=\"user2\", is_active=False)))\n        self.assertTrue(AllowAllUsersRemoteUserBackend().user_can_authenticate(User.objects.create(username=\"user3\", is_active=False)))\n        self.assertTrue(AllowAllUsersRemoteUserBackend().user_can_authenticate(User.objects.create(username=\"user4\", is_active=True)))\n\n", "    def test_user_logout(self):\n        User.objects.create(username=\"knownuser\")\n        response = self.client.get(\"/remote_user/\", **{self.header: self.known_user})\n        self.assertEqual(response.context[\"user\"].username, \"knownuser\")\n        self.client.get(\"/logout/\")\n        response = self.client.get(\"/remote_user/\")\n        self.assertTrue(response.context[\"user\"].is_anonymous) \n"], "sample_511": ["def test_savefig():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3])\n\n    with pytest.raises(ValueError, match='cannot save to a file'):\n        plt.savefig()\n\n    with tmpdir() as tmp_dir:\n        filepath = tmp_dir / 'test.png'\n        plt.savefig(filepath)\n        assert filepath.exists()\n        assert filepath.stat().st_size > 0\n\n", "def test_imshow_kwargs():\n    fig, ax = plt.subplots()\n    img = np.random.rand(10, 10)\n    ax.imshow(img, cmap='viridis', interpolation='nearest')\n    assert ax.images[0].get_cmap().name == 'viridis'\n    assert ax.images[0].get_interpolation() == 'nearest'\n\n", "    def test_colorbar_label():\n        fig, ax = plt.subplots()\n        im = ax.imshow([[1, 2], [3, 4]])\n        cbar = fig.colorbar(im)\n        assert cbar.ax.get_ylabel() is None\n        cbar.set_label('test')\n        assert cbar.ax.get_ylabel() == 'test'\n\n", "def test_setp():\n    fig, ax = plt.subplots()\n    ln, = ax.plot([1, 2, 3], [4, 5, 6])\n    plt.setp(ln, linewidth=2, color='r')\n\n    assert ln.get_linewidth() == 2\n    assert ln.get_color() == 'r'\n", "    def test_gridspec_colspan():\n        fig, axs = plt.subplots(nrows=2, ncols=2, gridspec_kw={'width_ratios': [2, 1]})\n        assert axs[0, 0].get_position().width * 2 == axs[0, 1].get_position().width\n"], "sample_785": ["    def test_leave_one_group_out_error():\n        # Check that LeaveOneGroupOut raises an error if there are groups with less than 2 samples\n        X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n        y = np.array([0, 0, 1, 1, 1])\n        groups = np.array([1, 1, 2, 3, 3])\n\n        logo = LeaveOneGroupOut()\n        with pytest.raises(ValueError, match=r\"The number of samples in each group must be >= 2\"):\n            list(logo.split(X, y, groups))\n", "    def test_leave_one_out_empty_data():\n        # Check that LeaveOneOut handles empty data gracefully\n        loo = LeaveOneOut()\n        X = np.array([])\n        y = np.array([])\n        splits = list(loo.split(X, y))\n        assert len(splits) == 0\n", "def test_shufflesplit_error_n_splits():\n    # Check that error is raised when n_splits is None\n    assert_raises(TypeError, ShuffleSplit, n_splits=None)\n\n\n", "def test_repeated_kfold_n_splits():\n    rkf = RepeatedKFold(n_splits=5, n_repeats=3)\n    assert_equal(rkf.get_n_splits(), 15)\n", "    def test_shuffle_split_multioutput_data():\n        X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n        y = np.array([[0, 1], [1, 0], [0, 1], [1, 1], [0, 0]])\n        cv = ShuffleSplit(n_splits=2)\n        for train, test in cv.split(X):\n            assert len(train) + len(test) == len(X)\n\n"], "sample_1007": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1006": ["compilation error", "def test_binomial_special_cases():\n    n = Symbol('n', integer=True)\n\n    assert binomial(n, 0) == 1\n    assert binomial(n, n) == 1\n    assert binomial(n, 1) == n\n    assert binomial(n, n - 1) == n\n", "compilation error", "def test_binomial_rewrite_negative_k():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True, negative=True)\n    assert binomial(n, k).rewrite(factorial) == factorial(n)/(factorial(k)*factorial(n - k))\n    assert binomial(n, k).rewrite(gamma) == gamma(n + 1)/(gamma(k + 1)*gamma(n - k + 1))\n", "def test_subfactorial_diff():\n    x = Symbol('x')\n    assert subfactorial(x).diff(x) == x*polygamma(0, x + 1) - x*polygamma(0, x + 2) + polygamma(0, x + 1) - polygamma(0, x + 2) + 1\n"], "sample_333": ["    def test_as_ul_with_hidden_fields(self):\n        class HiddenFieldsForm(Form):\n            visible = CharField()\n            hidden1 = CharField(widget=HiddenInput())\n            hidden2 = CharField(widget=HiddenInput())\n\n        form = HiddenFieldsForm({'visible': 'Visible Value'})\n        self.assertHTMLEqual(\n            form.as_ul(),\n            '<li><label for=\"id_visible\">Visible:</label> <input type=\"text\" name=\"visible\" value=\"Visible Value\" id=\"id_visible\"></li>'\n            '<li><input type=\"hidden\" name=\"hidden1\" id=\"id_hidden1\"></li>'\n            '<li><input type=\"hidden\" name=\"hidden2\" id=\"id_hidden2\"></li>'\n        )\n\n", "    def test_no_widget_error_message(self):\n        class CustomCharField(CharField):\n                kwargs.pop('widget', None)\n                super().__init__(**kwargs)\n\n        field = CustomCharField(error_messages={'invalid': 'Form custom error message.'})\n        self.assertEqual(field. widget.__class__.__name__, 'CharField')\n\n", "    def test_html5_required_attribute_not_set_if_required_is_false(self):\n        class MyForm(Form):\n            use_required_attribute = True\n            f1 = CharField(max_length=30, required=False)\n\n        form = MyForm()\n        self.assertHTMLEqual(\n            form.as_p(),\n            '<p><label for=\"id_f1\">F1:</label> <input id=\"id_f1\" maxlength=\"30\" name=\"f1\" type=\"text\"></p>',\n        )\n\n", "    def test_empty_data_files(self):\n        class FormWithFiles(Form):\n            file_field = FileField()\n        f = FormWithFiles({'file_field': ''})\n        self.assertTrue(f.is_valid())\n        self.assertIsNone(f.cleaned_data.get('file_field'))\n", "    def test_field_render_widget_with_renderer(self):\n        class CustomForm(Form):\n            my_field = CharField()\n\n            default_renderer = CustomRenderer()\n\n        form = CustomForm()\n        rendered_field = form['my_field'].as_widget(renderer=form.renderer)\n        self.assertIsInstance(rendered_field, str)\n"], "sample_457": ["    def test_violation_error_message(self):\n        constraint_name = get_constraints(UniqueConstraintProduct._meta.db_table)[0]\n        with self.assertRaisesMessage(\n            IntegrityError,\n            f\"Constraint \u201c{constraint_name}\u201d is violated.\",\n        ):\n            UniqueConstraintProduct.objects.create(\n                name=self.p1.name, color=self.p1.color\n            )\n\n", "    def test_validate_include_with_condition(self):\n        constraint = UniqueConstraintInclude._meta.constraints[0]\n        msg = \"Constraint \u201cname_without_color_uniq\u201d is violated.\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            constraint.validate(\n                UniqueConstraintInclude,\n                UniqueConstraintInclude(name=self.p1.name, color=\"anything-but-none\"),\n            )\n        # Values not matching condition are ignored.\n        constraint.validate(\n            UniqueConstraintInclude,\n            UniqueConstraintInclude(name=self.p1.name, color=None),\n        )\n\n", "    def test_condition_with_expressions(self):\n        constraint = models.UniqueConstraint(\n            Lower(\"name\"),\n            condition=models.Q(F(\"id\") > 50),\n            name=\"name_lower_with_id_cond_uniq\",\n        )\n        non_unique_product = UniqueConstraintProduct(name=self.p1.name.upper())\n        msg = \"Constraint \u201cname_lower_with_id_cond_uniq\u201d is violated.\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            constraint.validate(UniqueConstraintProduct, non_unique_product)\n        # Values not matching condition are ignored.\n        constraint.validate(\n            UniqueConstraintProduct,\n            UniqueConstraintProduct(name=self.p1.name, id=100),\n        )\n        # Existing instances have their existing row excluded.\n        constraint.validate(UniqueConstraintProduct, self.p1)\n        # Unique field is excluded.\n        constraint.validate(\n            UniqueConstraintProduct,\n            non_unique_product,\n            exclude={\"name\"},\n        )\n", "    def test_include_with_expression(self):\n        with self.assertRaisesMessage(\n            ValueError,\n            \"UniqueConstraint.include cannot be used with expressions.\",\n        ):\n            models.UniqueConstraint(\n                Lower(\"name\"),\n                name=\"include_expression_unique\",\n                include=[\"color\"],\n            )\n", "    def test_include_with_condition(self):\n        UniqueConstraintIncludeCondition.objects.create(name=\"p1\", color=\"red\")\n        UniqueConstraintIncludeCondition.objects.create(name=\"p2\", color=\"blue\")\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintIncludeCondition.objects.create(name=\"p1\", color=\"red\")\n"], "sample_1203": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1105": ["compilation error", "compilation error", "def test_matmul_inverse():\n    assert MatMul(A, Inverse(A)).doit() == Identity(m)\n    assert MatMul(A, Inverse(A), evaluate=False).doit() == Identity(m)\n\n", "compilation error", "def test_permutation_matrix():\n    P = PermutationMatrix([[0, 1, 2],[1, 0, 2],[2, 1, 0]])\n    assert MatMul(A, P, A.T).doit() == MatMul(A, P, A.T).doit(deep=True)\n"], "sample_26": ["    def test_comp_image_hdu_empty_extension(self):\n        hdu = fits.CompImageHDU()\n        assert hdu.data is None\n        assert len(hdu._header) == 0\n", "    def test_comp_image_properties_set_errors():\n        with pytest.raises(TypeError):\n            fits.CompImageHDU(np.zeros((3, 4, 5)), compression_type=123)\n        with pytest.raises(ValueError):\n            fits.CompImageHDU(np.zeros((3, 4, 5)), tile_shape=(2, 3, 4, 5))\n\n", "    def test_comphdu_empty(tmp_path):\n        hdu = fits.CompImageHDU(data=np.array([], dtype=np.int32))\n        hdu.writeto(tmp_path / \"empty.fits\")\n        with fits.open(tmp_path / \"empty.fits\") as hdul:\n            assert hdul[1].data.size == 0\n\n", "    def test_comp_image_dtype_change(self, tmp_path):\n        data = np.arange(100, dtype=np.int16).reshape((10, 10))\n        hdu = fits.CompImageHDU(data=data)\n        filename = tmp_path / \"test_dtype_change.fits\"\n        hdu.writeto(filename)\n        with fits.open(filename) as hdul:\n            assert hdul[1].data.dtype == np.int16\n            hdul[1].data = hdul[1].data.astype(np.float32)\n            assert hdul[1].data.dtype == np.float32\n            hdul.writeto(filename)\n        with fits.open(filename) as hdul:\n            assert hdul[1].data.dtype == np.float32\n\n", "    def test_comp_image_hdu_empty_data(self):\n        hdu = fits.CompImageHDU(data=np.empty((0, 0)))\n        assert hdu.data.size == 0\n        assert hdu.header['NAXIS1'] == 0\n        assert hdu.header['NAXIS2'] == 0\n"], "sample_528": ["def test_use_style_file_pattern():\n    style_file_pattern = mpl.style.core.__getattr__().STYLE_FILE_PATTERN\n    assert style_file_pattern.match('test.mplstyle') is not None\n    assert style_file_pattern.match('test.txt') is None\n\n", "def test_use_nonexistent_style():\n    with pytest.raises(IOError):\n        style.use('nonexistent_style')\n", "def test_reload_library():\n    original_styles = style.library.copy()\n    with temp_style('temp_style'):\n        style.reload_library()\n        assert 'temp_style' in style.library\n\n    style.reload_library()\n    assert style.library == original_styles\n\n", "def test_use_with_multiple_styles():\n    mpl.rcParams[PARAM] = 'gray'\n    with temp_style('style1', {'axes.facecolor': 'red'}):\n        with temp_style('style2', {'figure.facecolor': 'blue'}):\n            with style.context(['style1', 'style2']):\n                assert mpl.rcParams['axes.facecolor'] == 'red'\n                assert mpl.rcParams['figure.facecolor'] == 'blue'\n    assert mpl.rcParams[PARAM] == 'gray'\n", "def test_reload_library():\n    # Create a temporary style file\n    temp_file = Path(TemporaryDirectory().name, 'test_style.mplstyle')\n    temp_file.write_text(f'{PARAM}: {VALUE}', encoding='utf-8')\n\n    # Add the path to USER_LIBRARY_PATHS\n    USER_LIBRARY_PATHS.append(temp_file.parent)\n\n    # Reload the library\n    style.reload_library()\n    assert 'test_style' in style.available\n\n    # Remove the temporary file\n    temp_file.unlink()\n\n    # Reload the library again\n    style.reload_library()\n    assert 'test_style' not in style.available\n"], "sample_469": ["    def test_alias_with_case(self):\n        qs = Book.objects.alias(\n            is_old=Case(When(pubdate__year__lt=2000, then=Value(1)), default=Value(0))\n        ).filter(is_old=1)\n        self.assertSequenceEqual(qs, [self.b4])\n\n", "    def test_alias_forbidden_chars(self):\n        tests = [\n            'al\"ias',\n            \"a'lias\",\n            \"ali`as\",\n            \"alia s\",\n            \"alias\\t\",\n            \"ali\\nas\",\n            \"alias--\",\n            \"ali/*as\",\n            \"alias*/\",\n            \"alias;\",\n            # [] are used by MSSQL.\n            \"alias[\",\n            \"alias]\",\n        ]\n        msg = (\n            \"Column aliases cannot contain whitespace characters, quotation marks, \"\n            \"semicolons, or SQL comments.\"\n        )\n        for crafted_alias in tests:\n            with self.subTest(crafted_alias):\n                with self.assertRaisesMessage(ValueError, msg):\n                    Book.objects.alias(**{crafted_alias: Value(1)})\n\n", "    def test_alias_forbidden_chars(self):\n        tests = [\n            'al\"ias',\n            \"a'lias\",\n            \"ali`as\",\n            \"alia s\",\n            \"alias\\t\",\n            \"ali\\nas\",\n            \"alias--\",\n            \"ali/*as\",\n            \"alias*/\",\n            \"alias;\",\n            # [] are used by MSSQL.\n            \"alias[\",\n            \"alias]\",\n        ]\n        msg = (\n            \"Column aliases cannot contain whitespace characters, quotation marks, \"\n            \"semicolons, or SQL comments.\"\n        )\n        for crafted_alias in tests:\n            with self.subTest(crafted_alias):\n                with self.assertRaisesMessage(ValueError, msg):\n                    Book.objects.alias(**{crafted_alias: Value(1)})\n", "    def test_alias_forbidden_chars(self):\n        tests = [\n            'al\"ias',\n            \"a'lias\",\n            \"ali`as\",\n            \"alia s\",\n            \"alias\\t\",\n            \"ali\\nas\",\n            \"alias--\",\n            \"ali/*as\",\n            \"alias*/\",\n            \"alias;\",\n            # [] are used by MSSQL.\n            \"alias[\",\n            \"alias]\",\n        ]\n        msg = (\n            \"Column aliases cannot contain whitespace characters, quotation marks, \"\n            \"semicolons, or SQL comments.\"\n        )\n        for crafted_alias in tests:\n            with self.subTest(crafted_alias):\n                with self.assertRaisesMessage(ValueError, msg):\n                    Book.objects.alias(**{crafted_alias: Value(1)})\n", "    def test_alias_with_transform_and_aggregate(self):\n        qs = Book.objects.alias(\n            pubdate_alias=F(\"pubdate\"),\n        ).annotate(\n            year=TruncYear(\"pubdate_alias\"),\n            rating_sum=Sum(\"rating\"),\n        ).filter(year=TruncYear(datetime.date.today()))\n        self.assertIs(hasattr(qs.first(), \"pubdate_alias\"), False)\n        self.assertEqual(qs.count(), 1)\n\n"], "sample_812": ["        def __init__(self, foo='bar'):\n            self.foo = foo\n", "def test_nested_pipelines():\n    # Test rendering of nested pipelines\n    nested_pipeline = make_pipeline(\n        StandardScaler(),\n\n        make_pipeline(\n            LogisticRegression(),\n            RFE(LogisticRegression())\n        )\n    )\n    expected = \"\"\"", "def test_key_val_tuple():\n    # Test repr of KeyValTuple\n    kvt = _EstimatorPrettyPrinter.KeyValTuple(key=\"key\", value=\"value\")\n    assert kvt.__repr__() == \"'key': 'value'\"\n", "    def test_nested_pipelines():\n        # Render a pipeline with nested pipelines\n        nested_pipe = Pipeline([\n            ('reduce_dim', Pipeline([\n                ('pca', PCA()),\n                ('scaler', StandardScaler())\n            ])),\n            ('classify', SVC())\n        ])\n        expected = \"\"\"", "def test_empty_vocabulary():\n    vectorizer = CountVectorizer(vocabulary={})\n    expected = r\"\"\""], "sample_472": ["    def test_get_page_invalid_number(self):\n        paginator = Paginator(Article.objects.all(), 5)\n        with self.assertRaises(InvalidPage):\n            paginator.page(0)\n        with self.assertRaises(InvalidPage):\n            paginator.page(-1)\n            \n        with self.assertRaises(InvalidPage):\n            paginator.page(\"abc\")\n\n", "    def test_get_page_number(self):\n        paginator = Paginator(self.articles, 5)\n        page1 = paginator.page(1)\n        self.assertEqual(page1.number, 1)\n        self.assertEqual(paginator.get_page_number(page1), 1)\n\n        page2 = paginator.page(2)\n        self.assertEqual(page2.number, 2)\n        self.assertEqual(paginator.get_page_number(page2), 2)\n", "    def test_get_elided_page_range_invalid_arguments(self):\n        paginator = Paginator(range(50), 10)\n        with self.assertRaises(ValueError):\n            list(paginator.get_elided_page_range(1, on_each_side=-1))\n        with self.assertRaises(ValueError):\n            list(paginator.get_elided_page_range(1, on_ends=-1))\n", "    def test_page_range_empty_object_list(self):\n        paginator = Paginator([], 2)\n        self.assertEqual(list(paginator.page_range), [])\n", "    def test_paginator_iteration_empty_object_list(self):\n        paginator = Paginator([], 2)\n        with self.assertRaises(StopIteration):\n            next(iter(paginator))\n"], "sample_142": ["    def test_invalid_list_filter_on_through_field(self):\n        class SongAdminWithInvalidListFilter(admin.ModelAdmin):\n            list_filter = ['authorsbook__featured']  # Intentional typo\n\n        errors = SongAdminWithInvalidListFilter(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"Invalid field 'authorsbook__featured' for list_filter. \"\n                \"Check the field name and ensure the relationship exists.\",\n                obj=SongAdminWithInvalidListFilter,\n                id='admin.E024',\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_list_filter_invalid_field(self):\n        class SongAdminWithInvalidListFilter(admin.ModelAdmin):\n            list_filter = ['nonexistent_field']\n\n        errors = SongAdminWithInvalidListFilter(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'list_filter[0]' refers to 'nonexistent_field', which is \"\n                \"not an attribute of 'admin_checks.Song'.\",\n                obj=SongAdminWithInvalidListFilter,\n                id='admin.E001',\n            )\n        ]\n        self.assertEqual(errors, expected)\n\n", "    def test_fieldsets_incorrect_type(self):\n        class IncorrectFieldsetsAdmin(admin.ModelAdmin):\n            fieldsets = 'title'\n\n        errors = IncorrectFieldsetsAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'fieldsets' must be a list or tuple of tuples.\",\n                obj=IncorrectFieldsetsAdmin,\n                id='admin.E007',\n            )\n        ]\n        self.assertEqual(errors, expected) \n", "    def test_inline_fieldsets_with_nonexistent_field(self):\n        class WrongInline(admin.TabularInline):\n            model = Author\n            fieldsets = (\n                (None, {'fields': ['name', 'nonexistent']}),\n            )\n\n        class MyAdmin(admin.ModelAdmin):\n            inlines = [WrongInline]\n\n        errors = MyAdmin(Book, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'fieldsets[0][1][\\\"fields\\\"]' contains the nonexistent field 'nonexistent'.\",\n                obj=WrongInline,\n                id='admin.E005',\n            )\n        ]\n        self.assertEqual(errors, expected)\n\n", "    def test_invalid_list_filter_field(self):\n        class SongAdmin(admin.ModelAdmin):\n            list_filter = ['nonexistent_field']\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'list_filter[0]' refers to 'nonexistent_field', which is not an attribute of 'admin_checks.Song'.\",\n                obj=SongAdmin,\n                id='admin.E040',\n            )\n        ]\n        self.assertEqual(errors, expected)\n"], "sample_554": ["compilation error", "def test_text_default_fontsize():\n    fig, ax = plt.subplots()\n    txt = ax.text(0.5, 0.5, \"foo\")\n    assert txt.get_size() == mpl.rcParams['font.size']\n", "    def test_text_with_color_cycle(self):\n        fig, ax = plt.subplots()\n        colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n        for i, c in enumerate(colors):\n            ax.text(0.1 + i * 0.2, 0.5, f'Text {i}', color=c)\n        fig.canvas.draw()\n", "def test_alignment():\n    fig, ax = plt.subplots()\n    ax.text(0.5, 0.5, 'Test', ha='left', va='top')\n    ax.text(0.5, 0.75, 'Test', ha='center', va='center')\n    ax.text(0.5, 0.25, 'Test', ha='right', va='bottom')\n", "compilation error"], "sample_107": ["    def test_cleanse_setting_handles_lists(self):\n        initial = ['TEST', 'PASSWORD', 'OTHER']\n        expected = ['TEST', CLEANSED_SUBSTITUTE, 'OTHER']\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected)\n\n", "    def test_cleanse_setting_handles_nested_lists(self):\n        initial = {'login': 'cooper', 'credentials': ['user', {'password': 'secret'}]}\n        expected = {'login': 'cooper', 'credentials': ['user', {'password': CLEANSED_SUBSTITUTE}]}\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected) \n", "    def test_cleanse_setting_handles_none(self):\n        self.assertIsNone(cleanse_setting('SETTING_NAME', None))\n", "    def test_cleanse_setting_recurses_in_list(self):\n        initial = [{'login': 'cooper', 'password': 'secret'}, {'password': 'another_secret'}]\n        expected = [{'login': 'cooper', 'password': CLEANSED_SUBSTITUTE}, {'password': CLEANSED_SUBSTITUTE}]\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected)\n", "    def test_cleanse_setting_recurses_in_nested_dictionary(self):\n        initial = {'user': {'login': 'cooper', 'password': 'secret'}}\n        expected = {'user': {'login': 'cooper', 'password': CLEANSED_SUBSTITUTE}}\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected)\n\n"], "sample_998": ["def test_issue_14053():\n    # Test if latex(expr) returns a str and not a sympy.core.symbol.Symbol\n    x = Symbol('x')\n    expr = x + 1\n    latex_expr = latex(expr)\n    assert isinstance(latex_expr, str)\n", "compilation error", "compilation error", "def test_issue_15193():\n    from sympy.physics.quantum import Ket, Bra, Dagger\n    ket = Ket('psi')\n    bra = Bra('phi')\n    assert latex(Dagger(ket)) == r\"\\left\\langle \\psi \\right\\rangle\"\n    assert latex(bra @ ket) == r\"\\left\\langle \\phi\\right| \\left| \\psi \\right\\rangle\"\n\n", "compilation error"], "sample_735": ["compilation error", "    def test_predict_proba_single_sample():\n        rng = np.random.RandomState(0)\n        rand_data = RandomData(rng, scale=7)\n        n_components = rand_data.n_components\n        X = rand_data.X['full']\n\n        gmm = GaussianMixture(n_components=n_components).fit(X)\n        # Test predict_proba on a single data point\n        proba = gmm.predict_proba(X[0:1, :])\n        assert_array_equal(proba.shape, (1, n_components))\n        assert_almost_equal(proba.sum(), 1)\n", "def test_weights_init():\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng, scale=7)\n    n_components = rand_data.n_components\n    n_samples = rand_data.n_samples\n\n    # Test that the weights are initialized correctly\n    gmm = GaussianMixture(n_components=n_components,\n                          covariance_type='full', random_state=rng)\n    gmm.means_init = rand_data.means\n    gmm.precisions_init = rand_data.precisions['full']\n    gmm._initialize_parameters(X=rand_data.X['full'],\n                               random_state=rng)\n    assert_allclose(gmm.weights_, np.ones(n_components) / n_components)\n\n    # Test that the weights are initialized correctly with a given init\n    weights_init = np.array([0.1, 0.2, 0.7])\n    gmm.weights_init = weights_init\n\n    gmm._initialize_parameters(X=rand_data.X['full'],\n                               random_state=rng)\n    assert_allclose(gmm.weights_, weights_init)\n\n", "compilation error", "def test_determinant():\n    # Test that determinant calculation works for all covariance types\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng, scale=5)\n    n_components = rand_data.n_components\n\n    for covar_type in COVARIANCE_TYPE:\n        X = rand_data.X[covar_type]\n        gmm = GaussianMixture(n_components=n_components,\n                              covariance_type=covar_type, random_state=rng)\n        gmm.fit(X)\n\n        for k in range(n_components):\n            if covar_type == 'full':\n                true_det = np.linalg.det(gmm.covariances_[k])\n                pred_det = gmm._compute_determinant(gmm.covariances_[k],\n                                                    covar_type)[k]\n                assert_allclose(true_det, pred_det)\n            elif covar_type == 'tied':\n                true_det = np.linalg.det(gmm.covariances_)\n                pred_det = gmm._compute_determinant(gmm.covariances_,\n                                                    covar_type)\n                assert_allclose(true_det, pred_det)\n            else:\n                true_det = np.prod(gmm.covariances_[k])\n                pred_det = gmm._compute_determinant(gmm.covariances_[k],\n                                                    covar_type)\n                assert_allclose(true_det, pred_det)\n\n"], "sample_928": ["def test_default_role(app, tmpdir):\n    docname = str(tmpdir / 'doc.rst')\n    with default_role(docname, 'default'):\n        pass\n    assert hasattr(docutils, 'roles')\n\n    with default_role(docname, 'nonexistent'):\n        pass\n    assert 'default role nonexistent not found' in logging.getLogger(__name__).get_records()\n\n", "def test_default_role(app):\n    with default_role('dummy.rst', 'myrole'):\n        pass\n    with default_role('dummy.rst', 'nonexistent'):\n        pass\n", "def test_default_role(app):\n    with default_role('dummy.rst', 'myrole'):\n        pass\n    assert docutils.roles[''] == 'myrole'\n    \n    with default_role('dummy.rst', 'nonexistentrole'):\n        pass\n    assert 'nonexistentrole' not in docutils.roles\n", "def test_default_role(app, monkeypatch):\n    with monkeypatch.stringio() as stderr:\n        with default_role('dummy.rst', 'missing_role'):\n            pass\n        assert 'default role missing_role not found' in stderr.getvalue()\n", "def test_default_role(app):\n    with default_role('dummy.rst', 'myrole'):\n        pass\n\n    # verify nothing is registered\n        assert docutils.roles == {}\n"], "sample_385": ["    def test_render_options_empty_queryset(self):\n        \"\"\"\n        Test that the widget renders correctly if the queryset is empty.\"\"\"\n        Album.objects.all().delete()\n        form = AlbumForm()\n        output = form.as_table()\n        self.assertIn('<option value=\"\"></option>', output)\n", "    def test_render_options_with_empty_values(self):\n        beatles = Band.objects.create(name=\"The Beatles\", style=\"rock\")\n        who = Band.objects.create(name=\"The Who\", style=\"rock\")\n        form = AlbumForm(\n            initial={\"band\": \"\", \"featuring\": [beatles.pk, \"\"]}\n        )\n        output = form.as_table()\n        self.assertIn('value=\"\"', output)\n        self.assertIn(f'<option value=\"{beatles.pk}\" selected>The Beatles</option>', output)\n", "    def test_get_select2_language(self):\n        with translation.override(\"de-at\"):\n            self.assertEqual(get_select2_language(), \"de\")\n        with translation.override(\"zh-hans-HK\"):\n            self.assertEqual(get_select2_language(), \"zh-CN\")\n", "    def test_render_options_with_empty_values(self):\n        beatles = Band.objects.create(name=\"The Beatles\", style=\"rock\")\n        # Create an Album with an empty value for the 'featuring' field.\n        Album.objects.create(name=\"Abbey Road\", band=beatles, featuring=[])\n\n        form = AlbumForm(initial={\"featuring\": []})\n        output = form.as_table()\n        self.assertIn(self.empty_option, output)\n", "    def test_autocomplete_select_multiple(self):\n        beatles = Band.objects.create(name=\"The Beatles\", style=\"rock\")\n        who = Band.objects.create(name=\"The Who\", style=\"rock\")\n        form = AlbumForm(initial={\"featuring\": [beatles.pk, who.pk]})\n        output = form.as_table()\n        selected_option_beatles = '<option value=\"%s\" selected>The Beatles</option>' % beatles.pk\n        selected_option_who = '<option value=\"%s\" selected>The Who</option>' % who.pk\n        self.assertIn(selected_option_beatles, output)\n        self.assertIn(selected_option_who, output)\n"], "sample_658": ["    def test_doctest_skip_expected_failure(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n                '''\n                >>> 1 + 1 # doctest: +SKIP\n                3\n                '''\n        \"\"\"\n        )\n        result = testdir.runpytest(\"--doctest-modules\")\n        result.stdout.fnmatch_lines(\n            [\"*1 skipped*\"]\n        )\n", "    def test_doctest_report_no_diff(self, testdir):\n        result = self._run_doctest_report(testdir, \"none\")\n        result.stdout.fnmatch_lines(\n            [\n                \"Expected:\",\n                \"       a  b\",\n                \"    0  1  4\",\n                \"    1  2  4\",\n                \"    2  3  6\",\n                \"Got:\",\n                \"       a  b\",\n                \"    0  1  4\",\n                \"    1  2  5\",\n                \"    2  3  6\",\n            ]\n        )\n", "    def test_doctest_report_udiff_failure_marker(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n                '''\n                >>> foo()\n                   a  b\n                0  1  4\n                1  2  4\n                2  3  6\n                '''\n                print('   a  b\\\\n'\n                      '0  1  4\\\\n'\n                      '1  2  5\\\\n'\n                      '2  3  6')\n            \"\"\"\n        )\n        result = testdir.runpytest(\"--doctest-modules\", \"--doctest-report=udiff\", \"--doctest-failure-marker=!\")\n        result.stdout.fnmatch_lines(\n            [\"     0  1  4\", \"    !1  2  4\", \"    +1  2  5\", \"     2  3  6\"]\n        )\n", "    def test_doctest_ignore_exception_for_example(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n                '''\n                >>> my_func(1)\n                1\n                >>> my_func('a')  # doctest: +IGNORE_EXCEPTION\n                Traceback (most recent call last):\n                    ...\n                TypeError: ...\n                '''\n                return x\n\n        \"\"\"\n        )\n        result = testdir.runpytest(\"--doctest-modules\")\n        result.stdout.fnmatch_lines([\"* 1 passed *\"])\n", "    def test_doctest_mock_objects_dont_recurse(mock_module, testdir):\n        pytest.importorskip(mock_module)\n        testdir.makepyfile(\n            \"\"\"\n            from {mock_module} import patch\n\n            class Example(object):\n                    self.x = 1\n\n            @patch('__main__.Example.x', 2)\n                '''\n                >>> example = Example()\n                >>> example.x\n                2\n                '''\n        \"\"\".format(\n                mock_module=mock_module\n            )\n        )\n        result = testdir.runpytest(\"--doctest-modules\")\n        result.stdout.fnmatch_lines([\"* 1 passed *\"])\n"], "sample_641": ["def test_save_results_create_parent_dir(path: str, tmp_path: Path) -> None:\n    save_results(LinterStats(), Path(tmp_path) / path)\n    assert (Path(tmp_path) / path).exists()\n", "def test_load_results_nonexistent_file(tmp_path: Path, linter_stats: LinterStats) -> None:\n    file_path = tmp_path / \"nonexistent_file.stats\"\n    assert not file_path.exists()\n    loaded = load_results(file_path)\n    assert loaded is None\n", "def test_load_nonexistent_file(path: str) -> None:\n    loaded = load_results(path)\n    assert loaded is None\n", "def test_load_results_file_not_exist(path: str) -> None:\n    loaded = load_results(path)\n    assert loaded is None\n\n", "    def test_load_results_file_not_exist(self, tmp_path: Path) -> None:\n        file_path = tmp_path / \"non_existent_file.stats\"\n        assert load_results(file_path) is None\n"], "sample_899": ["def test_check_estimator_multioutput():\n    # check that check_estimator also works with multioutput estimators\n    check_estimator(MultiTaskElasticNet())\n", "    def test_check_estimator_deprecation():\n        # Test that check_estimator() raises a warning for deprecated estimators\n        # and their parameters.\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            check_estimator(deprecated.DeprecatedEstimator(\"Deprecated\"))\n        assert len(w) == 2\n        assert \"Deprecated\" in str(w[0].message)\n        assert \"parameter 'param'\" in str(w[1].message)\n\n", "    def test_check_estimator_sample_weight_type():\n        # check that estimators can handle sample weights of different types\n        from sklearn.linear_model import LinearRegression\n\n        # test with numpy array\n        est = LinearRegression()\n        check_estimator(est, sample_weight=np.array([1, 2, 3]))\n        \n        # test with list\n        est = LinearRegression()\n        check_estimator(est, sample_weight=[1, 2, 3])\n", "    def test_check_estimator_with_large_sparse_data():\n        # check that check_estimator works with large sparse data\n        # and doesn't raise MemoryError\n\n        from scipy.sparse import csr_matrix\n        from sklearn.linear_model import LogisticRegression\n\n        # Create a large sparse matrix\n        n_samples = 10000\n        n_features = 1000\n        X = csr_matrix((np.random.rand(n_samples * n_features),\n                       (np.random.randint(0, n_samples, n_samples * n_features),\n                        np.random.randint(0, n_features, n_samples * n_features))))\n        y = np.random.randint(0, 2, n_samples)\n\n        est = LogisticRegression(solver='lbfgs')\n\n        # check that check_estimator doesn't raise MemoryError\n        check_estimator(est)\n", "    def test_check_estimator_get_params_invariance():\n        # check if get_params(deep=False) is a subset\n        # of get_params(deep=True)\n\n        class T(BaseEstimator):\n                pass\n\n                return self\n\n                return X\n        e = T()\n        shallow_params = e.get_params(deep=False)\n        deep_params = e.get_params(deep=True)\n\n        assert_true(all(item in deep_params.items() for item in\n                        shallow_params.items()))\n"], "sample_451": ["    def test_replace_named_groups(self):\n        self.assertEqual(\n            replace_named_groups(r\"^(\\w+)/b/(\\w+)$\"), r\"^<var1>/b/<var2>$\"\n        )\n        self.assertEqual(\n            replace_named_groups(r\"^(\\w+)/b/(?:\\w+)$\"), r\"^<var1>/b/<var>$\"\n        )\n        self.assertEqual(\n            replace_named_groups(r\"^(\\w+)/b/(\\w+)/$\"), r\"^<var1>/b/<var2>/$\"\n        )\n        self.assertEqual(\n            replace_named_groups(r\"^(\\w+)/b/(?:\\w+)/(?:\\w+)$\"), r\"^<var1>/b/<var>/$\"\n        )\n        self.assertEqual(\n            replace_named_groups(r\"^(\\w+)/b/(\\w+)\"), r\"^<var1>/b/<var2>\"\n        )\n", "    def test_replace_named_groups(self):\n        self.assertEqual(\n            replace_named_groups(r'^(?P<a>\\w+)/b/(\\w+)$'), r'^<a>/b/(\\w+)$'\n        )\n        self.assertEqual(\n            replace_named_groups(r'^(?P<a>\\w+)/b/(?P<c>\\w+)$'), r'^<a>/b/<c>$'\n        )\n        self.assertEqual(\n            replace_named_groups(r'^(?P<a>\\w+)/b/(\\w+)$'), r'^<a>/b/(\\w+)$'\n        )\n        self.assertEqual(\n            replace_named_groups(r'^(?P<a>\\w+)/b/(?P<c>\\w+)$'), r'^<a>/b/<c>$'\n        )\n", "    def test_replace_metacharacters(self):\n        self.assertEqual(replace_metacharacters(r\"^\\w+?\"), r\"^\\\\w+?\")\n        self.assertEqual(replace_metacharacters(r\"^\\w+$\"), r\"^\\\\w+$\")\n        self.assertEqual(replace_metacharacters(r\"^[\\w\\s]+$\"), r\"^[\\w\\s]+$\")\n        self.assertEqual(replace_metacharacters(r\"^[\\w\\s]+?$\"), r\"^[\\w\\s]+?$\")\n        self.assertEqual(replace_metacharacters(r\"^abc\\*def$\"), r\"^abc\\*def$\")\n        self.assertEqual(replace_metacharacters(r\"\\^abc\\*def$\"), r\"\\\\^abc\\*def$\")\n", "    def test_remove_non_capturing_groups(self):\n        self.assertEqual(\n            remove_non_capturing_groups(r\"(?P<a>\\w+)/b/(?:\\w+)c(?:\\w+)\"),\n            r\"(?P<a>\\w+)/b/c\",\n        )\n        self.assertEqual(\n            remove_non_capturing_groups(r\"^(?:\\w+(?:\\w+))a\"),\n            r\"^a\",\n        )\n        self.assertEqual(\n            remove_non_capturing_groups(r\"^a(?:\\w+)/b(?:\\w+)\"),\n            r\"^a/b\",\n        )\n\n", "    def test_replace_metacharacters(self):\n        escape_sequences = [\n            (r\"(\\*\", r\"\\(\"),\n            (r\"\\)\", r\"\\)\"),\n            (r\"\\*\", r\"\\*\"),\n            (r\"\\+\", r\"\\+\"),\n            (r\"\\?\", r\"\\?\"),\n            (r\"\\^\", r\"\\^\"),\n            (r\"\\$\", r\"\\$\"),\n\n        ] \n        for pattern, replacement in escape_sequences:\n            self.assertEqual(replace_metacharacters(pattern), replacement)\n\n\n\n"], "sample_86": ["    def test_lazy_hash(self):\n        hash_value = hash(\"test\")\n        lazy_obj = lazy(lambda: \"test\", str)\n        self.assertEqual(hash(lazy_obj()), hash_value)\n        self.assertEqual(hash(lazy_obj), hash_value)\n", "    def test_lazy_object_pickle(self):\n        \"\"\"\n        Lazy objects can be pickled and unpickled.\n        \"\"\"\n        original = lazy(lambda: {'foo': 'bar'}, dict)\n        pickled = pickle.dumps(original)\n        unpickled = pickle.loads(pickled)\n        self.assertEqual(unpickled(), {'foo': 'bar'})\n", "    def test_lazy_proxy_delegation(self):\n        class Klazz:\n                self.value = value\n\n                return self.value\n\n        lazy_klazz = lazy(lambda: Klazz(42), Klazz)\n\n        self.assertEqual(lazy_klazz().method(), 42)\n\n", "    def test_lazy_hash(self):\n        \"\"\"\n        hash() works correctly for Promises.\n        \"\"\"\n        lazy_a = lazy(lambda: 'foo', str)\n        lazy_b = lazy(lambda: 'foo', str)\n        lazy_c = lazy(lambda: 'bar', str)\n\n        self.assertEqual(hash(lazy_a()), hash(lazy_b()))\n        self.assertNotEqual(hash(lazy_b()), hash(lazy_c()))\n\n", "    def test_lazy_text_cast(self):\n        text = \"Test String\"\n        lazy_obj = lazy(lambda: text, str)\n        self.assertEqual(lazy_obj.__str__(), text)\n"], "sample_973": ["compilation error", "    def test_signature_from_str_param_spec(self):\n        sig = inspect.signature_from_str(\n            '(a: int = 10, b: typing.List[str] = ...) -> None'\n        )\n\n        assert sig.parameters['a'].default == '10'\n        assert sig.parameters['b'].default == 'Ellipsis'\n\n", "    def test_find_docstring():\n        class Foo:\n                pass\n\n                \"\"\"This is a method docstring.\"\"\"\n                pass\n\n        assert inspect.find_docstring(Foo) is None\n        assert inspect.find_docstring(Foo.meth) == \"This is a method docstring.\"\n        assert inspect.find_docstring(Foo.meth.__wrapped__) == \"This is a method docstring.\"\n\n", "def test_getsource_file_not_found(app):\n    with pytest.raises(IOError):\n        inspect.getsource('nonexistent_module') \n", "    def test_is_builtin_class_method_for_types():\n        assert inspect.is_builtin_class_method(int, 'from_bytes')\n        assert inspect.is_builtin_class_method(str, '__new__')\n        assert not inspect.is_builtin_class_method(str, 'upper')\n\n"], "sample_13": ["def test_angle_arithmetic():\n    a = Angle('45d')\n    b = Angle('30d')\n    c = a + b\n    assert c.value == 75\n    assert c.unit == u.deg\n\n    c = a - b\n    assert c.value == 15\n    assert c.unit == u.deg\n\n    c = a * 2\n    assert c.value == 90\n    assert c.unit == u.deg\n\n", "def test_angle_copy(cls):\n    \"\"\"Test that copying an Angle object creates a new independent object.\"\"\"\n    a = cls(10*u.deg)\n    b = a.copy()\n\n    assert b.value == a.value\n    assert b.unit == a.unit\n\n    b.value = 20*u.deg\n    assert a.value != b.value\n\n", "def test_angle_convert_scalar(value, unit, expected_value, expected_unit):\n    a = Angle(value, unit=unit)\n    assert a.to(expected_unit).value == expected_value\n    assert a.to(expected_unit).unit == expected_unit\n", "compilation error", "    def test_angle_comparison_with_scalar(angle_type):\n        a = angle_type('10d')\n        assert a > angle_type('5d')\n        assert a < angle_type('15d')\n        assert a == angle_type('10d')\n        assert a != angle_type('5d')\n        assert a >= angle_type('10d')\n        assert a <= angle_type('10d')\n"], "sample_551": ["    def test_legend_Poly3DCollection_with_alpha():\n        verts = np.asarray([[0, 0, 0], [0, 1, 1], [1, 0, 1]])\n        mesh = art3d.Poly3DCollection([verts], label=\"surface\", alpha=0.5)\n\n        fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n        handle = ax.add_collection3d(mesh)\n        leg = ax.legend()\n        assert (leg.legend_handles[0].get_facecolor()\n                == handle.get_facecolor()).all()\n        assert leg.legend_handles[0].get_alpha() == 0.5\n", "compilation error", "def test_empty_poly3dcollection():\n    verts = []\n    mesh = art3d.Poly3DCollection(verts, label=\"surface\")\n\n    fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n    handle = ax.add_collection3d(mesh)\n    leg = ax.legend()\n\n    assert len(leg.legend_handles) == 0 \n", "compilation error", "def test_poly3dcollection_legend_elements():\n    verts = np.asarray([[0, 0, 0], [0, 1, 1], [1, 0, 1]])\n    mesh = art3d.Poly3DCollection([verts], facecolors=['b'],\n                                  edgecolors=['r'], label=\"surface\")\n    fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n    ax.add_collection3d(mesh)\n    artists, labels = mesh.legend_elements()\n    assert labels == [\"surface\"]\n    assert len(artists) == 1\n    assert isinstance(artists[0], art3d.Poly3DCollection)\n    assert (artists[0].get_facecolor() == mesh.get_facecolor()).all()\n    assert (artists[0].get_edgecolor() == mesh.get_edgecolor()).all()\n"], "sample_708": ["def test_getstatement_decorator_empty_line() -> None:\n    source = \"\"\"", "def test_getstatementrange_with_comments_in_multiline_statements() -> None:\n    source = Source(\n        \"\"\"\n        if (\n            1\n            # comment 1\n            +\n            2\n        ):\n            pass\n        \"\"\"\n    )\n    assert source.getstatementrange(1) == (1, 5)\n", "def test_getstatementrange_with_lambda():\n    source = Source(\n        \"\"\"\n        x = lambda a: a + 1\n            x(3)\n        \"\"\"\n    )\n\n    statement = source.getstatement(1)\n    assert str(statement).strip() == \"x = lambda a: a + 1\"\n", "def test_getstatementrange_with_empty_lines() -> None:\n    source = Source(\"\"\"\n\n        pass\n\n    \"\"\")\n    assert source.getstatementrange(2) == (2, 3)\n\n", "def test_source_with_leading_and_trailing_empty_lines() -> None:\n    source = Source(\n        \"\"\"\n\n        pass\n\n    \"\"\"\n    )\n    assert str(source).strip() == \"def f():\\n    pass\"\n\n\n"], "sample_968": ["def test_info_field_list_empty_type(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   :param arg: blah blah\\n\"\n            \"   :type arg:\\n\")\n    doctree = restructuredtext.parse(app, text)\n\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, (\"class\", desc_sig_space)],\n                                           [desc_name, \"Class\"])],\n                                   [desc_content, nodes.field_list, nodes.field])]))\n    assert_node(doctree[1][1][0][0], ([nodes.field_name, \"Parameters\"],\n                                      [nodes.field_body, nodes.paragraph]))\n\n    # :param arg:\n    assert_node(doctree[1][1][0][0][1][0], ([addnodes.literal_strong, \"arg\"],\n                                            \" -- \", \"blah blah\"))\n\n\n", "def test_attribute_name(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><span class=\"pre\">MyClass.my_attribute</span></span>' in content)\n    assert ('<span class=\"n\"><span class=\"pre\">MyClass.another_attribute</span></span>' in content)\n\n", "def test_unknown_ref(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert 'unknown-ref' in content\n\n", "compilation error", "def test_info_field_list_optional(app):\n    text = (\".. py:function:: func\\n\"\n            \"\\n\"\n            \"   :param arg1: blah blah\\n\"\n            \"   :type arg1: str\\n\"\n            \"   :param arg2: blah blah\\n\"\n            \"   :type arg2: int, optional\\n\")\n    doctree = restructuredtext.parse(app, text)\n\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"func\"])],\n                                   [desc_content, nodes.field_list, nodes.field, nodes.field])]))\n    assert_node(doctree[1][1][0][0],\n                ([nodes.field_name, \"Parameters\"],\n                 [nodes.field_body, nodes.paragraph]))\n\n    assert_node(doctree[1][1][0][0][1][0],\n                ([addnodes.literal_strong, \"arg1\"],\n                 \" (\",\n                 [pending_xref, addnodes.literal_emphasis, \"str\"],\n                 \")\",\n                 \" -- \",\n                 \"blah blah\"))\n\n    assert_node(doctree[1][1][0][0][1][1],\n                ([addnodes.literal_strong, \"arg2\"],\n                 \" (\",\n                 [pending_xref, addnodes.literal_emphasis, \"int\"],\n                 \", optional\",\n                 \")\",\n                 \" -- \",\n                 \"blah blah\"))\n\n"], "sample_343": ["    def test_get_extra_restriction(self):\n        contenttype_pk = ContentType.objects.get_for_model(Post).pk\n        related_alias = 'related_object'\n        remote_alias = 'related_object'\n        expected_restriction = WhereNode([\n            models.Q(**{'%s__pk' % 'contenttype': contenttype_pk}),\n        ], connector=AND)\n        restriction = Post.answer_set._field.get_extra_restriction(related_alias, remote_alias)\n\n        self.assertEqual(restriction, expected_restriction)\n\n", "    def test_get_extra_restriction(self):\n        question = Question.objects.create(text='test')\n        answer = Answer.objects.create(question=question)\n        ct = ContentType.objects.get_for_model(Question)\n        expected_lookup = models.Q(\n            contenttypes_tests__question__object_id__exact=answer.pk,\n            contenttypes_tests__question__content_type_id__exact=ct.pk\n        )\n        self.assertEqual(Answer.question.get_extra_restriction('contenttypes_tests', 'contenttypes_tests__question'), expected_lookup)\n", "    def test_get_prefetch_queryset_bulk_delete(self):\n        question = Question.objects.create(text='Who?')\n        post1 = Post.objects.create(title='Answer1', parent=question)\n        post2 = Post.objects.create(title='Answer2', parent=question)\n\n        post1.delete()\n\n        with self.assertNumQueries(1):\n            posts = list(question.answer_set.all())\n\n        self.assertCountEqual(posts, [post2])\n", "    def test_get_extra_restriction(self):\n        question = Question.objects.create(text='test')\n        answer = Answer.objects.create(question=question)\n        related_alias = 'answered_questions'\n        remote_alias = 'answer'\n        content_type = ContentType.objects.get_for_model(Answer)\n        expected_lookup = WhereNode([\n            models.Q(\n                f'{Answer._meta.get_field(\"question\").remote_field.name}__pk',\n                content_type.id),\n        ], connector=AND)\n        self.assertEqual(\n            Answer.objects.get_extra_restriction(related_alias, remote_alias),\n            expected_lookup\n        ) \n", "    def test_prefetch_cache_cleared_on_instance_save(self):\n        question = Question.objects.create(text='test')\n        answer1 = Answer.objects.create(question=question)\n        answer2 = Answer.objects.create(question=question)\n\n        # Prefetch related objects\n        questions = Question.objects.prefetch_related('answer_set')\n        self.assertEqual(len(questions.get(pk=question.pk).answer_set.all()), 2)\n\n        # Update the question\n        question.text = 'updated question'\n        question.save()\n\n        # Ensure prefetch cache is cleared\n        questions = Question.objects.prefetch_related('answer_set')\n        self.assertEqual(len(questions.get(pk=question.pk).answer_set.all()), 2)\n"], "sample_1170": ["compilation error", "def test_issue_23073():\n    assert str(Float(1.5, 3)) == '1.500'\n", "def test_issue_22981():\n    from sympy.physics.quantum.state import  Ket\n    from sympy.physics.quantum.operator import Operator\n    A = Operator('A')\n    ket = Ket('psi')\n    assert sstr(A*ket) == 'A*|psi>'\n", "def test_printing_str_array_expressions_with_indices():\n    # test case for issue #22355\n    from sympy import symbols\n    I = symbols(\"I\")\n    array = ArraySymbol(\"A\", 2, 3, 4)\n    assert sstr(array[I, 1, 0]) == \"A[I, 1, 0]\"\n\n", "compilation error"], "sample_635": ["    def test_docstring_min_length_fail(self) -> None:\n        \"\"\"Example of a function that is less than 'docstring-min-length' config option\n\n        An error message is emitted.\n        \"\"\"\n        node = astroid.extract_node(\n            \"\"\"\n            '''f'''\n            pass\n        \"\"\"\n        )\n        with self.assertAddsMessages(\n            MessageTest(\n                msg_id=\"docstring-too-short\", node=node, args=(\"function_foo\", 3)\n            ),\n        ):\n            self.checker.visit_functiondef(node)\n", "    def test_docstring_min_length_fails(self) -> None:\n        \"\"\"Example of a function that is less than 'docstring-min-length' config option\n\n        An error message is emitted.\n        \"\"\"\n        node = astroid.extract_node(\n            \"\"\"\n            '''f'''\n            pass\n        \"\"\"\n        )\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"docstring-too-short\", node=node)\n        ):\n            self.checker.visit_functiondef(node)\n\n", "    def test_docstring_min_length_failure(self) -> None:\n        \"\"\"Example of a function exceeding the 'docstring-min-length' config option\n\n        An error message is emitted.\n        \"\"\"\n        node = astroid.extract_node(\n            \"\"\"\n            '''function'''\n            pass\n        \"\"\"\n        )\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"docstring-too-short\", node=node)\n        ):\n            self.checker.visit_functiondef(node)\n", "    def test_fail_docstring_min_length(self) -> None:\n        \"\"\"Example of a function that is less than 'docstring-min-length' config option\n\n        Error message is emitted.\n        \"\"\"\n        node = astroid.extract_node(\n            \"\"\"\n            '''Short'''\n            pass\n        \"\"\"\n        )\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"missing-docstring\", node=node)\n        ):\n            self.checker.visit_functiondef(node)\n", "    def test_skip_docstring_min_length_multi_line(self) -> None:\n        \"\"\"Example of a function that is less than 'docstring-min-length' config option\n\n        It has a multi-line docstring, but the content is still too short\n        No error message is emitted.\n        \"\"\"\n        node = astroid.extract_node(\n            \"\"\"\n            '''function is too short and is missing parameter documentation \n            \n            '''\n            pass\n        \"\"\"\n        )\n        with self.assertNoMessages():\n            self.checker.visit_functiondef(node)\n\n"], "sample_653": ["    def test_log_set_path_failure(testdir):\n        report_dir_base = testdir.tmpdir.strpath\n\n        testdir.makeini(\n            \"\"\"\n            [pytest]\n            log_file_level = DEBUG\n            log_cli=true\n            \"\"\"\n        )\n        testdir.makeconftest(\n            \"\"\"\n            import os\n            import pytest\n            @pytest.hookimpl(hookwrapper=True, tryfirst=True)\n                config = item.config\n                logging_plugin = config.pluginmanager.get_plugin(\"logging-plugin\")\n                report_file = os.path.join({}, item._request.node.name + \".txt\")\n                logging_plugin.set_log_path(report_file)\n                yield\n            \"\"\".format(\n                repr(report_dir_base)\n            )\n        )\n        testdir.makepyfile(\n            \"\"\"\n            import logging\n            logger = logging.getLogger(\"testcase-logger\")\n                logger.info(\"message from test 1\")\n\n                logger.debug(\"message from test 2\")\n                assert False\n\n            \n        \"\"\"\n        )\n        result = testdir.runpytest()\n        assert result.ret != 0\n        assert os.path.isfile(os.path.join(report_dir_base, \"test_first.txt\"))\n        assert os.path.isfile(os.path.join(report_dir_base, \"test_second.txt\"))\n\n", "def test_log_file_multiple_runs(testdir):\n    log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level = INFO\n        \"\"\".format(\n            log_file\n        )\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.getLogger().info(\"info message\")\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    assert result.ret == 0\n    with open(log_file, encoding=\"utf-8\") as rfh:\n        contents = rfh.read()\n        assert \"info message\" in contents\n\n    result = testdir.runpytest() \n    assert result.ret == 0\n    with open(log_file, encoding=\"utf-8\") as rfh:\n        contents = rfh.read()\n        assert contents.count(\"info message\") == 2\n\n", "def test_log_set_path_relative(testdir):\n    report_dir_base = testdir.tmpdir.strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file_level = DEBUG\n        log_cli=true\n        \"\"\"\n    )\n    testdir.makeconftest(\n        \"\"\"\n            import os\n            import pytest\n            @pytest.hookimpl(hookwrapper=True, tryfirst=True)\n                config = item.config\n                logging_plugin = config.pluginmanager.get_plugin(\"logging-plugin\")\n                report_file = os.path.join('logs', item._request.node.name)\n                logging_plugin.set_log_path(report_file)\n                yield\n        \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n            import logging\n            logger = logging.getLogger(\"testcase-logger\")\n                logger.info(\"message from test 1\")\n                assert True\n\n                logger.debug(\"message from test 2\")\n                assert True\n        \"\"\"\n    )\n    testdir.runpytest()\n    logs_dir = os.path.join(report_dir_base, 'logs')\n    with open(os.path.join(logs_dir, \"test_first\"), \"r\") as rfh:\n        content = rfh.read()\n        assert \"message from test 1\" in content\n\n    with open(os.path.join(logs_dir, \"test_second\"), \"r\") as rfh:\n        content = rfh.read()\n        assert \"message from test 2\" in content\n\n", "def test_log_set_path_format(testdir):\n    report_dir_base = testdir.tmpdir.strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file_level = DEBUG\n        log_cli=true\n        log_format= %(asctime)s - %(levelname)s - %(message)s\n        \"\"\"\n    )\n    testdir.makeconftest(\n        \"\"\"\n            import os\n            import pytest\n            @pytest.hookimpl(hookwrapper=True, tryfirst=True)\n                config = item.config\n                logging_plugin = config.pluginmanager.get_plugin(\"logging-plugin\")\n                report_file = os.path.join({}, item._request.node.name)\n                logging_plugin.set_log_path(report_file)\n                yield\n        \"\"\".format(\n            repr(report_dir_base)\n        )\n    )\n    testdir.makepyfile(\n        \"\"\"\n            import logging\n            logger = logging.getLogger(\"testcase-logger\")\n                logger.info(\"message from test 1\")\n                assert True\n\n                logger.debug(\"message from test 2\")\n                assert True\n        \"\"\"\n    )\n    testdir.runpytest()\n    with open(os.path.join(report_dir_base, \"test_first\"), \"r\") as rfh:\n        content = rfh.read()\n        assert \"message from test 1\" in content\n        assert \"[INFO]\" in content\n\n    with open(os.path.join(report_dir_base, \"test_second\"), \"r\") as rfh:\n        content = rfh.read()\n        assert \"message from test 2\" in content\n        assert \"[DEBUG]\" in content\n", "    def test_log_set_path_existing_file(testdir):\n        report_dir_base = testdir.tmpdir.strpath\n\n        testdir.makeini(\n            \"\"\"\n            [pytest]\n            log_file_level = DEBUG\n            log_cli=true\n            \"\"\"\n        )\n        testdir.makeconftest(\n            \"\"\"\n                import os\n                import pytest\n                @pytest.hookimpl(hookwrapper=True, tryfirst=True)\n                    config = item.config\n                    logging_plugin = config.pluginmanager.get_plugin(\"logging-plugin\")\n                    report_file = os.path.join({}, item._request.node.name)\n                    logging_plugin.set_log_path(report_file)\n                    yield\n            \"\"\".format(\n                repr(report_dir_base)\n            )\n        )\n        testdir.makepyfile(\n            \"\"\"\n                import logging\n                logger = logging.getLogger(\"testcase-logger\")\n                    logger.info(\"message from test 1\")\n                    assert True\n\n                    logger.debug(\"message from test 2\")\n                    assert True\n            \"\"\"\n        )\n        # Create an existing file in the report directory\n        with open(os.path.join(report_dir_base, \"test_first\"), \"w\") as f:\n            f.write(\"Existing content\\n\")\n\n        testdir.runpytest()\n        with open(os.path.join(report_dir_base, \"test_first\"), \"r\") as rfh:\n            content = rfh.read()\n            assert \"Existing content\" in content\n            assert \"message from test 1\" in content\n\n        with open(os.path.join(report_dir_base, \"test_second\"), \"r\") as rfh:\n            content = rfh.read()\n            assert \"message from test 2\" in content\n"], "sample_144": ["    def test_inherited_field_ordering(self):\n        # Regression test for #24113\n        p = Place.objects.create(name='My Place', address='123 Main St')\n        r = Restaurant.objects.create(\n            place_ptr=p, serves_hot_dogs=True, serves_pizza=False\n        )\n        self.assertEqual(list(r._meta.get_fields()), [\n            r._meta.get_field('name'),\n            r._meta.get_field('address'),\n            r._meta.get_field('serves_hot_dogs'),\n            r._meta.get_field('serves_pizza'),\n        ])\n", "    def test_inheritance_get_or_create(self):\n        # Regression test for #24750\n        with self.assertNumQueries(1):\n            italian_restaurant, created = ItalianRestaurant.objects.get_or_create(\n                name='Pizza Place',\n                serves_gnocchi=True,\n                restaurant_ptr__place_ptr__address='123 Main Street',\n            )\n        self.assertTrue(created)\n        with self.assertNumQueries(1):\n            italian_restaurant, created = ItalianRestaurant.objects.get_or_create(\n                name='Pizza Place',\n                serves_gnocchi=True,\n                restaurant_ptr__place_ptr__address='123 Main Street',\n            )\n        self.assertFalse(created)\n", "    def test_inheritance_unique_constraint_across_tables(self):\n        ItalianRestaurant.objects.create(\n            name='Restaurant A', serves_gnocchi=True, serves_hot_dogs=True, serves_pizza=False\n        )\n        with self.assertRaisesMessage(\n                IntegrityError, \"UNIQUE constraint failed: myapp_italianrestaurant.name\"):\n            ItalianRestaurant.objects.create(name='Restaurant A', serves_gnocchi=True, serves_hot_dogs=True, serves_pizza=False)\n", "    def test_foreign_key_cascade_on_delete(self):\n        # Regression test for #28257\n        p1 = Place.objects.create(name=\"Guido's House of Pasta\", address='944 W. Fullerton')\n        r1 = Restaurant.objects.create(place_ptr=p1, serves_hot_dogs=True, serves_pizza=False)\n        i1 = ItalianRestaurant.objects.create(restaurant_ptr=r1, serves_gnocchi=True)\n        p1.delete()\n        with self.assertRaises(Restaurant.DoesNotExist):\n            Restaurant.objects.get(pk=r1.pk)\n        with self.assertRaises(ItalianRestaurant.DoesNotExist):\n            ItalianRestaurant.objects.get(pk=i1.pk)\n", "    def test_delete_cascade_with_multi_table_inheritance(self):\n        p = Person.objects.create(name='Alice')\n        c = Congressman.objects.create(person_ptr=p, state='CA', title='Senator')\n        s = Senator.objects.create(congressman_ptr=c, seniority=10)\n        \n        s.delete()\n        self.assertFalse(Congressman.objects.filter(pk=c.pk).exists())\n        self.assertFalse(Person.objects.filter(pk=p.pk).exists())\n\n"], "sample_358": ["    def test_expressions_with_alias(self):\n        compiler = Person.objects.all().query.get_compiler(connection.alias)\n        self.expressions = Expressions(\n            table='person',\n            expressions=ExpressionList(\n                IndexExpression(F('first_name')),\n                IndexExpression(F('last_name').desc()),\n                IndexExpression(Upper('last_name')),\n            ).resolve_expression(compiler.query),\n            compiler=compiler,\n            quote_value=self.editor.quote_value,\n        )\n        self.assertIn(\n            '%s.%s' % (self.editor.quote_name('person'), self.editor.quote_name('first_name')),\n            str(self.expressions),\n        )\n", "    def test_expressions_with_alias(self):\n        alias = 'alias'\n        compiler = Person.objects.filter(pk=1).query.get_compiler(connection.alias)\n        expressions = Expressions(\n            table=alias,\n            expressions=ExpressionList(\n                IndexExpression(F('first_name')),\n                IndexExpression(F('last_name').desc()),\n                IndexExpression(Upper('last_name')),\n            ).resolve_expression(compiler.query),\n            compiler=compiler,\n            quote_value=self.editor.quote_value,\n        )\n        self.assertIs(expressions.references_table(alias), True)\n        self.assertIs(expressions.references_table(Person._meta.db_table), False)\n        self.assertIn(\n            '%s.%s' % (\n                self.editor.quote_name(alias),\n                self.editor.quote_name('first_name'),\n            ),\n            str(expressions),\n        )\n", "    def test_expressions_with_alias(self):\n        compiler = Person.objects.all().query.get_compiler(connection.alias)\n        expressions = Expressions(\n            table='alias',\n            expressions=ExpressionList(\n                IndexExpression(F('first_name')),\n            ).resolve_expression(compiler.query),\n            compiler=compiler,\n            quote_value=self.editor.quote_value,\n        )\n        expressions.rename_table_references('alias', Person._meta.db_table)\n        self.assertIn(\n            '%s.%s' % (\n                self.editor.quote_name(Person._meta.db_table),\n                self.editor.quote_name('first_name'),\n            ),\n            str(expressions),\n        )\n", "    def test_expressions_with_alias(self):\n        compiler = Query(Person, alias='person').get_compiler(connection=connection)\n        table = Person._meta.db_table\n        expressions = Expressions(\n            table='person',\n            expressions=ExpressionList(\n                IndexExpression(F('first_name')),\n                IndexExpression(F('last_name').desc()),\n                IndexExpression(Upper('last_name')),\n            ).resolve_expression(compiler.query),\n            compiler=compiler,\n            quote_value=self.editor.quote_value,\n        )\n        self.assertIn(\n            '%s.%s' % (\n                self.editor.quote_name('person'),\n                self.editor.quote_name('first_name'),\n            ),\n            str(expressions),\n        )\n", "    def test_empty_expressions(self):\n        compiler = Person.objects.all().query.get_compiler(connection.alias)\n        expressions = Expressions(\n            table=Person._meta.db_table,\n            expressions=ExpressionList().resolve_expression(compiler.query),\n            compiler=compiler,\n            quote_value=self.editor.quote_value,\n        )\n        self.assertEqual(str(expressions), '')\n"], "sample_625": ["def test_cross_keep_dims(use_dask: bool, keep_dims: bool) -> None:\n    a = xr.DataArray(\n        [[1, 2, 3], [4, 5, 6]], dims=[\"x\", \"y\"], coords={\"x\": [0, 1], \"y\": [\"a\", \"b\", \"c\"]}\n    )\n    b = xr.DataArray(\n        [[7, 8, 9], [10, 11, 12]], dims=[\"x\", \"y\"], coords={\"x\": [0, 1], \"y\": [\"a\", \"b\", \"c\"]}\n    )\n\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"test for dask.\")\n        a = a.chunk()\n        b = b.chunk()\n    expected = np.cross(a.values, b.values, axis=-1)\n    actual = xr.cross(a, b, dim=\"y\", keep_dims=keep_dims)\n    xr.testing.assert_duckarray_allclose(expected, actual.values)\n    assert actual.dims == ([\"x\"] if keep_dims else []) \n", "    def test_cross_errors(self, use_dask: bool): \n        a = xr.DataArray([1, 2, 3], dims=[\"x\"])\n        b = xr.DataArray([4, 5], dims=[\"x\"])\n        if use_dask:\n            if not has_dask:\n                pytest.skip(\"test for dask.\")\n            a = a.chunk()\n            b = b.chunk()\n        with pytest.raises(ValueError):\n            xr.cross(a, b)\n", "def test_cross_invalid_input() -> None:\n    a = xr.DataArray([1, 2, 3], dims=[\"x\"])\n    b = xr.DataArray([4, 5], dims=[\"y\"])\n    with pytest.raises(ValueError):\n        xr.cross(a, b, dim=\"x\")\n", "def test_cross_ndim_error(use_dask: bool) -> None:\n    a = xr.DataArray(np.random.rand(2, 2, 2), dims=[\"x\", \"y\", \"z\"])\n    b = xr.DataArray(np.random.rand(2, 2, 2), dims=[\"x\", \"y\", \"z\"])\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"test for dask.\")\n        a = a.chunk()\n        b = b.chunk()\n\n    with pytest.raises(ValueError):\n        xr.cross(a, b)\n", "    def test_cross_empty_array(use_dask: bool) -> None:\n        a = xr.DataArray(np.zeros((0, 3)), dims=['dim_0', 'dim_1'])\n        b = xr.DataArray(np.ones((0, 3)), dims=['dim_0', 'dim_1'])\n        if use_dask:\n            if not has_dask:\n                pytest.skip(\"test for dask.\")\n            a = a.chunk()\n            b = b.chunk()\n\n        with pytest.raises(ValueError):\n            xr.cross(a, b, dim=\"dim_0\")\n\n"], "sample_914": ["    def test_parse_type_comments():\n        source = \"\"\"\n            return a + b\n        \"\"\"\n        module = ast.parse(source, mode='exec', type_comments=True)\n        assert module.body[0].args.args[0].annotation.id == 'int'\n        assert module.body[0].args.args[1].annotation.id == 'str'\n        assert module.body[0].returns.id == 'float'\n", "    def test_unparse_arguments():\n        source = \"def func(a, b: str, *args, kw1=1, kw2: int = 2, **kwargs): pass\"\n        expected = \"a, b: str, *args, kw1=1, kw2: int = 2, **kwargs\"\n        module = ast.parse(source)\n        assert ast.unparse(module.body[0].args) == expected\n", "def test_unparse_function_def(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0]) == expected\n", "def test_unparse_arguments_with_type_comments():\n    source = \"\"\"", "    def test_unparse_arguments():\n        source = \"def func(a: int, b: str, *args: float, c=10, d='hello', **kwargs) -> bool:\"\n        expected = \"a, b, *args, c=10, d='hello', **kwargs\"\n        module = ast.parse(source)\n        assert ast.unparse_arguments(module.body[0].args) == expected\n"], "sample_2": ["def test_ccddata_copy_properties():\n    ccd_data = create_ccd_data()\n    ccd_data.mask = ccd_data.data > 10\n    ccd_data.uncertainty = StdDevUncertainty(ccd_data.data * 10)\n    ccd_data.meta = {'FOO': 'BAR'}\n    ccd_copy = ccd_data.copy()\n\n    assert ccd_copy.mask is not None\n    assert ccd_copy.uncertainty is not None\n    assert ccd_copy.meta == ccd_data.meta\n\n    # Check that modifications to the copy don't affect the original\n    ccd_copy.mask = None\n    ccd_copy.uncertainty = None\n    ccd_copy.meta = {}\n    assert ccd_data.mask is not None\n    assert ccd_data.uncertainty is not None\n    assert ccd_data.meta == {'FOO': 'BAR'}\n\n", "def test_read_fits_with_different_case_extension_names():\n    # Test that CCDData.read can handle FITS files with extension names that\n    # have different capitalization than the defaults.\n    ccd_data = create_ccd_data()\n    ccd_data.mask = ccd_data.data > 10\n    ccd_data.uncertainty = StdDevUncertainty(ccd_data.data * 10)\n\n    # Create a temporary FITS file with non-standard extension names\n    hdulist = ccd_data.to_hdu()\n    hdulist[1].header['EXTNAME'] = 'MASK'\n    hdulist[2].header['EXTNAME'] = 'UNCERTAINTY'\n    tmpfile = tmpdir.join('afile.fits').strpath\n    hdulist.writeto(tmpfile)\n\n    # Read the file back in, specifying the non-standard extension names\n    ccd_read = CCDData.read(tmpfile, hdu_mask='MASK',\n                            hdu_uncertainty='UNCERTAINTY')\n\n    # Verify that the data, mask, and uncertainty were read correctly\n    np.testing.assert_array_equal(ccd_data.data, ccd_read.data)\n    np.testing.assert_array_equal(ccd_data.mask, ccd_read.mask)\n    np.testing.assert_array_equal(ccd_data.uncertainty.array,\n                                 ccd_read.uncertainty.array)\n", "    def test_ccdata_with_mask_and_uncertainty(tmpdir):\n        ccd_data = create_ccd_data()\n        ccd_data.mask = ccd_data.data > 10\n        ccd_data.uncertainty = StdDevUncertainty(ccd_data.data * 10)\n\n        filename = tmpdir.join('afile.fits').strpath\n        ccd_data.write(filename)\n        ccd_after = CCDData.read(filename)\n        np.testing.assert_array_equal(ccd_data.mask, ccd_after.mask)\n        np.testing.assert_array_equal(ccd_data.uncertainty.array,\n                                     ccd_after.uncertainty.array)\n", "def test_ccdata_from_hdulist():\n    hdul = fits.HDUList([fits.PrimaryHDU(np.ones((10, 10))),\n                         fits.ImageHDU(np.ones((10, 10)))])\n    ccd = CCDData.from_hdulist(hdul)\n    assert isinstance(ccd, CCDData)\n    assert ccd.data.shape == (10, 10)\n\n", "    def test_ccddata_from_table(self):\n        \"\"\"\n        Test the creation of a CCDData object from an astropy Table.\n        \"\"\"\n        data = np.ones((10, 10))\n        table = Table(data=data, names=['flux'], meta={'unit': 'adu'})\n\n        ccd = CCDData.from_table(table)\n\n        assert np.allclose(ccd.data, data)\n        assert ccd.unit == 'adu'\n"], "sample_126": ["    def test_add_unique_together(self):\n        \"\"\"\n        Adding unique_together constraints should be a separate migration.\n        \"\"\"\n        Book = ModelState('app', 'Book', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=100)),\n            ('author', models.ForeignKey('app.Author', models.CASCADE)),\n        ])\n        Author = ModelState('app', 'Author', [\n            ('id', models.AutoField(primary_key=True)),\n            ('name', models.CharField(max_length=100)),\n        ])\n        BookUnique = ModelState('app', 'Book', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=100)),\n            ('author', models.ForeignKey('app.Author', models.CASCADE)),\n        ],\n            unique_together=[('author', 'title')]\n        )\n        changes = self.get_changes([Book, Author], [BookUnique, Author])\n        self.assertNumberMigrations(changes, 'app', 2)\n        self.assertOperationTypes(changes, 'app', 0, ['CreateModel', 'CreateModel'])\n        self.assertOperationTypes(changes, 'app', 1, ['AlterUniqueTogether'])\n", "    def test_alter_unique_together(self):\n        class BookReview(models.Model):\n            book = models.ForeignKey(self.book, models.CASCADE)\n            user = models.ForeignKey(self.author, models.CASCADE)\n            review_text = models.TextField()\n\n            class Meta:\n                unique_together = ('book', 'user')\n\n        class BookReviewNew(models.Model):\n            book = models.ForeignKey(self.book, models.CASCADE)\n            user = models.ForeignKey(self.author, models.CASCADE)\n            review_text = models.TextField()\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(fields=['book', 'user'], name='unique_book_user_review')\n                ]\n\n        changes = self.get_changes([self.book, self.author], [self.book, self.author, BookReview, BookReviewNew])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['CreateModel', 'AlterUniqueTogether'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 1, name='bookreview', unique_together=set([('book', 'user')]))\n", "    def test_mti_inheritance_field_removal(self):\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"species\", models.CharField(max_length=50)),\n        ])\n        Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n        changes = self.get_changes([Animal, Dog], [Animal, Dog])\n\n", "    def test_mti_inheritance_field_removal(self):\n        Animal = ModelState('app', 'Animal', [\n            ('id', models.AutoField(primary_key=True)),\n            ('name', models.CharField(max_length=100)),\n        ])\n        Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n        changes = self.get_changes([Animal, Dog], [Animal, Dog, ModelState('app', 'Dog', [('name', models.CharField(max_length=100))], bases=('app.Animal',))])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['RemoveField'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='name', model_name='Dog')\n\n", "    def test_mti_inheritance_field_removal(self):\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=100)),\n        ])\n        Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n        changes = self.get_changes([Animal, Dog], [Animal, Dog, ModelState('app', 'Dog', [(\"name\", models.CharField(max_length=100))], bases=('app.Animal',))])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['RemoveField'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='name', model_name='Dog')\n\n"], "sample_565": ["compilation error", "    def test_grid_with_axes_class_overriding_axis():\n        class CustomAxes(mpl.axes.Axes):\n                super().__init__(*args, **kwargs)\n                self.set_tick_params(direction='in')\n\n        Grid(plt.figure(), 111, (2, 2), axes_class=CustomAxes)\n", "    def test_grid_errors(rect, ngrids, error, message):\n        fig = plt.figure()\n        with pytest.raises(error, match=message):\n          Grid(fig, rect, (2, 3), ngrids=ngrids)\n", "    def test_grid_axes_locator():\n        fig = plt.figure()\n        grid = Grid(fig, 111, (2, 2))\n        for ax in grid:\n            assert isinstance(ax.get_axes_locator(), mpl.transforms.BboxTransformTo)\n", "compilation error"], "sample_1148": ["    def test_matrixsymbol_determinant_zero():\n        A = MatrixSymbol('A', 3, 3)\n        assert A.det().rewrite(det).expand().simplify() == 0\n", "    def test_transpose_properties():\n        A = MatrixSymbol('A', 2, 2)\n        B = MatrixSymbol('B', 2, 2)\n        assert (A + B).T == A.T + B.T\n        assert (A*B).T == B.T*A.T\n        assert (A.T).T == A\n        assert (2*A).T == 2*A.T\n", "compilation error", "def test_MatrixSymbol_shape_in_substitution():\n    A = MatrixSymbol('A', n, m)\n    M = ImmutableMatrix([[1, 2], [3, 4]])\n    assert A.subs(n, 2).shape == (2, m)\n    assert (A.subs(n, 2)).subs(m, 3).shape == (2, 3)\n    assert A.subs(M, 2).shape == (2, 2)\n    assert (A.subs(M, 2)).subs(n, 3).shape == (3, 2)\n", "compilation error"], "sample_1099": ["    def test_eval_partial_derivative_mixed_scalar_tensor_expr3():\n\n        tau, alpha = symbols(\"tau alpha\")\n\n        base_expr3 = tau*A(i)*A(-i) + tau**2 * H(i, j)\n\n        vector_expression = PartialDerivative(base_expr3, A(k))._perform_derivative()\n        assert (vector_expression -\n                (tau*L.delta(L_0, -k)*A(-L_0) + tau*A(L_0)*L.metric(-L_0, -L_1)*L.delta(L_1, -k)\n                + tau**2 * L.delta(i, -k) * H(-i, j))).expand() == 0\n\n        scalar_expression = PartialDerivative(base_expr3, tau)._perform_derivative()\n        assert (scalar_expression - A(i)*A(-i) - 2 * tau * H(i, j)).expand() == 0\n\n        tensor_expression = PartialDerivative(base_expr3, H(k, m))._perform_derivative()\n        assert (tensor_expression - tau**2 * L.delta(i, -k) * L.delta(j, -m)) == 0\n\n\n\n", "compilation error", "def test_eval_partial_derivative_mixed_scalar_tensor_expr3():\n\n    tau, alpha = symbols(\"tau alpha\")\n\n    base_expr3 = A(i)*H(i, j)*tau**alpha + A(-i)*H(-i, j)*tau**alpha\n\n    vector_expression = PartialDerivative(base_expr3, A(k))._perform_derivative()\n    assert (vector_expression -\n            (L.delta(L_0, -k) * H(L_0, j) * tau**alpha +\n             H(-k, j) * tau**alpha)).expand() == 0\n\n    scalar_expression = PartialDerivative(base_expr3, tau)._perform_derivative()\n    assert scalar_expression == A(i) * H(i, j) * alpha * 1 / tau * tau**alpha + A(-i) * H(-i, j) * alpha * 1 / tau * tau**alpha\n\n", "def test_eval_partial_derivative_mixed_scalar_tensor_expr3():\n    tau, alpha, beta = symbols(\"tau alpha beta\")\n\n    base_expr3 = A(i)*H(i, j)*B(-j) + tau**alpha*beta\n\n    tensor_deriv = PartialDerivative(base_expr3, H(k, m))._perform_derivative()\n    assert tensor_deriv - (A(L_0)*L.delta(L_0, -k)*B(-m)) == 0\n\n    vector_deriv = PartialDerivative(base_expr3, A(k))._perform_derivative()\n    assert (vector_deriv - (H(k, j)*B(-j))).expand() == 0\n\n    scalar_deriv = PartialDerivative(base_expr3, tau)._perform_derivative()\n    assert scalar_deriv - alpha * tau**(alpha - 1) * beta == 0\n\n", "    def test_eval_partial_derivative_mixed_scalar_tensor_expr3():\n\n        tau, alpha = symbols(\"tau alpha\")\n\n        base_expr3 = tau**2*A(-i)* A(i) + H(i, j)*tau\n\n        vector_expression = PartialDerivative(base_expr3, A(k))._perform_derivative()\n        assert  (vector_expression -\n            (tau**2 * L.delta(L_0, -k) + H(L_0, j)*tau*L.delta(L_0, -k))).expand() == 0\n\n        scalar_expression = PartialDerivative(base_expr3, tau)._perform_derivative()\n        assert scalar_expression == 2*tau*A(-i)*A(i) + H(i, j)\n"], "sample_85": ["    def test_fast_delete_inherited_m2m(self):\n        parent = Parent.objects.create()\n        child = Child.objects.create(parent=parent)\n        m = M.objects.create()\n        child.m_set.add(m)\n        # 1 for child, 1 for parent (fast deleted), 1 for m2m\n\n        self.assertNumQueries(3, parent.delete)\n        self.assertFalse(Parent.objects.exists())\n        self.assertFalse(Child.objects.exists())\n\n        self.assertFalse(M.objects.exists())\n", "    def test_fast_delete_inherited_m2m(self):\n        r = R.objects.create()\n        s1 = S.objects.create(r=r)\n        s2 = S.objects.create(r=r)\n        T.objects.create(s=s1)\n        T.objects.create(s=s2)\n        self.assertNumQueries(2, r.delete)\n        self.assertFalse(R.objects.exists())\n        self.assertFalse(S.objects.exists())\n        self.assertFalse(T.objects.exists())\n", "    def test_delete_no_related_objects(self):\n        a = Avatar.objects.create()\n        User.objects.create(avatar=a)\n        self.assertNumQueries(1, a.delete)\n", "    def test_fast_delete_m2m_through_cascade(self):\n        \"\"\"\n        Test fast deletion cascade when deleting a parent through a\n        many-to-many relationship.\n        \"\"\"\n        m = M.objects.create()\n        r = R.objects.create()\n        MR.objects.create(m=m, r=r)\n        # 1 query to delete m\n        # 1 query to delete through model (cascade)\n        self.assertNumQueries(2, m.delete)\n        self.assertFalse(M.objects.exists())\n        self.assertFalse(MR.objects.exists())\n", "    def test_fast_delete_no_related_objects(self):\n        u = User.objects.create()\n        u.avatar = None\n        u.save()\n        self.assertNumQueries(1, u.delete)\n"], "sample_818": ["def test_spectral_clustering_n_init():\n    # Test that n_init leads to the same solution on the same data\n    X, y = make_blobs(n_samples=20, random_state=0,\n                      centers=[[1, 1], [-1, -1]], cluster_std=0.01)\n    sp = SpectralClustering(n_clusters=2, n_init=5, random_state=0)\n    labels = sp.fit(X).labels_\n    assert_array_equal(sp.labels_, labels)\n\n", "    def test_spectral_clustering_with_invalid_n_components(self):\n        # Test that SpectralClustering raises an error when n_components is\n        # greater than n_clusters\n        X, y = make_blobs(n_samples=20, random_state=0,\n                          centers=[[1, 1], [-1, -1]], cluster_std=0.01)\n        sp = SpectralClustering(n_clusters=2, n_components=3,\n                               random_state=0)\n        assert_raises(ValueError, sp.fit, X)\n\n\n", "def test_spectral_clustering_bad_affinity():\n    # Test that spectral_clustering raises ValueError if affinity is not\n    # a valid string\n    X, y = make_blobs(n_samples=20, random_state=0,\n                      centers=[[1, 1], [-1, -1]], cluster_std=0.01)\n    assert_raises(ValueError, spectral_clustering, X, n_clusters=2,\n                  affinity='<unknown>')\n", "def test_spectral_clustering_empty_affinity():\n    # Test that SpectralClustering handles empty affinity matrix correctly\n    X = np.empty((0, 10))\n    \n    sp = SpectralClustering(n_clusters=2, affinity='precomputed',\n                            random_state=0)\n    assert_raises(ValueError, sp.fit, X)\n", "def test_spectral_clustering_empty_affinity_matrix():\n    X = np.empty((0, 10))\n    sp = SpectralClustering(n_clusters=2, random_state=0)\n    assert_raises(ValueError, sp.fit, X)\n\n"], "sample_864": ["def test_bandwidth_warning():\n    # Test warning when bandwidth is None and n_samples is large\n    X = np.random.rand(10000, 2)\n    with warnings.catch_warnings(record=True) as w:\n        MeanShift(bandwidth=None).fit(X)\n    assert len(w) == 1\n    assert \"quadratic time in the number of samples\" in str(w[0].message)\n", "    def test_get_bin_seeds_with_all_points():\n        # Test that get_bin_seeds returns all points when bin_size is small enough\n        X = np.array([[1, 1], [2, 1], [1, 0],\n                      [4, 7], [3, 5], [3, 6]])\n        bin_size = 0.1\n        all_points = get_bin_seeds(X, bin_size)\n        assert_array_almost_equal(all_points, X)\n", "    def test_get_bin_seeds_empty():\n        # Test get_bin_seeds with empty input array.\n\n        X = np.array([])\n        with pytest.raises(ValueError) as excinfo:\n            get_bin_seeds(X, 1)\n        assert \"Input points is empty.\" in str(excinfo.value)\n", "def test_mean_shift_empty_input():\n    # Test MeanShift with empty input\n    ms = MeanShift()\n    with pytest.raises(ValueError, match=\"Input data should not be \"):\n        ms.fit(np.array([]))\n", "    def test_n_jobs(self, n_jobs):\n        ms = MeanShift(n_jobs=n_jobs)\n        labels = ms.fit_predict(X)\n        assert len(labels) == len(X)\n"], "sample_1111": ["def test_horizontal_lines():\n    x = Symbol('x')\n    lines = [\n        '      1 |.----------------------------------------------------',\n        '        |                                                      ',\n        '        |.----------------------------------------------------',\n        '        |                                                      ',\n        '        |.----------------------------------------------------',\n        '        |                                                      ',\n        '        |.----------------------------------------------------',\n        '        |                                                      ',\n        '        |.----------------------------------------------------',\n        '        |                                                      ',\n        '      0 |-------------------------------------------------------',\n        '        |                                                      ',\n        '        |.----------------------------------------------------',\n        '        |                                                      ',\n        '        |.----------------------------------------------------',\n        '        |                                                      ',\n        '        |.----------------------------------------------------',\n        '        |                                                      ',\n        '        |.----------------------------------------------------',\n        '        |                                                      ',\n        '     -1 |_______________________________________________________',\n        '         -1                         0                          1'\n    ]\n    assert list(textplot_str(1, -1, 1)) == lines\n", "def test_constant():\n    x = Symbol('x')\n    lines = [\n        '      1 |                .....................................',\n        '        |               ......................................',\n        '        |              .......................................',\n        '        |             ........................................',\n        '        |            .........................................',\n        '        |           ..........................................',\n        '        |          ...........................................',\n        '        |         ............................................',\n        '        |        .............................................',\n        '        |-------................................--------------',\n        '        |       ..............................................',\n        '        |      ...............................................',\n        '        |     .................................................',\n        '        |    ..................................................',\n        '        |   ...................................................',\n        '        |  ....................................................',\n        '        | .....................................................',\n        '        |.......................................................',\n        '     -1 |_______________________________________________________',\n        '         0                          0.5                        1'\n    ]\n    assert lines == list(textplot_str(2, -1, 1))\n", "def test_constant():\n    x = Symbol('x')\n    lines = [\n        '      1 |_______________________________________________________',\n        '        |                ..                                     ',\n        '        |                ..                                     ',\n        '        |                ..                                     ',\n        '        |                ..                                     ',\n        '        |                ..                                     ',\n        '        |                ..                                     ',\n        '        |                ..                                     ',\n        '        |                ..                                     ',\n        '        |                ..                                     ',\n        '    0.5 |--------------...--------------------------------------',\n        '        |                ..                                     ',\n        '        |                ..                                     ',\n        '        |                ..                                     ',\n        '        |                ..                                     ',\n        '        |                ..                                     ',\n        '        |                ..                                     ',\n        '        |                ..                                     ',\n        '        |                ..                                     ',\n        '        |                ..                                     ',\n        '     -1 |_______________________________________________________',\n        '         -1                         0                          1'\n    ]\n    assert list(textplot_str(2, -1, 1)) == lines\n", "def test_errors():\n    x = Symbol('x')\n    lines = [\n        '      1 |                .                                  ',\n        '        |               .                                     ',\n        '        |              .                                  ',\n        '        |              .                                     ',\n        '        |              .                                     ',\n        '        |              .                                     ',\n        '        |               .                                     ',\n        '        |             .                                        ',\n        '        |            .                                        ',\n        '        |           .                                         ',\n        '      0 |-------------------------------------------------------',\n        '        |          .                                          ',\n        '        |          .                                           ',\n        '        |         .                                           ',\n        '        |         .                                           ',\n        '        |         .                                           ',\n        '        |         .                                           ',\n        '        |         .                                           ',\n        '        |         .                                           ',\n        '     -1 |_______________________________________________________',\n        '         -1                         0                          1'\n    ]\n    assert lines == list(textplot_str(1/sin(x), -1, 1))\n", "def test_constant():\n    x = Symbol('x')\n    lines = [\n        '      1 |_______________________________________________________',\n        '        |                .                                      ',\n        '        |                .                                      ',\n        '        |                .                                      ',\n        '        |                .                                      ',\n        '        |                .                                      ',\n        '        |                .                                      ',\n        '        |                .                                      ',\n        '        |                .                                      ',\n        '        |                .                                      ',\n        '    0.5 |-------------------------------------------------------',\n        '        |                .                                      ',\n        '        |                .                                      ',\n        '        |                .                                      ',\n        '        |                .                                      ',\n        '        |                .                                      ',\n        '        |                .                                      ',\n        '        |                .                                      ',\n        '        |                .                                      ',\n        '      0 |_______________________________________________________',\n        '         0                         1                         2'\n    ]\n    assert list(textplot_str(2, 0, 2)) == lines\n\n"], "sample_403": ["    def test_references_field_related_name(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\"Other\", models.CASCADE, related_name='related_name'),\n        )\n        self.assertIs(operation.references_field(\"Other\", \"related_name\", \"migrations\"), True) \n", "    def test_remove_field_references_model(self):\n        operation = migrations.RemoveField(\"Model\", \"field\")\n        self.assertIs(operation.references_model(\"model\", \"migrations\"), True)\n        self.assertIs(operation.references_model(\"other\", \"migrations\"), False)\n", "    def test_field_operation_deconstruction(self):\n        operation = migrations.AlterField(\n            \"Pony\", \"cute\", models.BooleanField(default=True)\n        )\n        definition = operation.deconstruct()\n        self.assertEqual(definition[0], \"AlterField\")\n        self.assertEqual(definition[1], [])\n        self.assertEqual(sorted(definition[2]), [\"model_name\", \"field_name\", \"field\"])\n", "    def test_references_field_by_related_name(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\"Other\", models.CASCADE, related_name=\"related_objects\"),\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"related_objects\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Model\", \"related_objects\", \"migrations\"), False\n        )\n        self.assertIs(\n            operation.references_field(\"Missing\", \"related_objects\", \"migrations\"), False\n        )\n", "    def test_references_field_by_related_model(self):\n        operation = FieldOperation(\"Model\", \"field\", models.OneToOneField(\"Other\", on_delete=models.CASCADE))\n        self.assertIs(operation.references_field(\"Other\", \"whatever\", \"migrations\"), True)\n"], "sample_484": ["    def test_empty_string(self):\n        Author.objects.create(name=\"\")\n        authors = Author.objects.annotate(name_part=Right(\"name\", 3))\n        self.assertQuerySetEqual(\n            authors.order_by(\"name\"),\n            [\"\" if connection.features.interprets_empty_strings_as_nulls else None],\n            lambda a: a.name_part,\n        )\n", "    def test_database_specific_functions(self):\n        # Test that Oracle's specific implementation of RIGHT is working correctly.\n        if connection.vendor == \"oracle\":\n            authors = Author.objects.annotate(name_part=Right(\"name\", 5))\n            self.assertQuerySetEqual(\n                authors.order_by(\"name\"), [\"Smith\", \"honda\"], lambda a: a.name_part\n            ) \n", "    def test_empty_string(self):\n        Author.objects.create(name=\"\")\n        authors = Author.objects.annotate(name_part=Right(\"name\", 5))\n        self.assertQuerySetEqual(\n            authors.order_by(\"name\"),\n            [\n                \"\" if connection.features.interprets_empty_strings_as_nulls else None,\n            ],\n            lambda a: a.name_part,\n        )\n\n", "    def test_oracle(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\n                \"CREATE TABLE IF NOT EXISTS test_right (id INTEGER PRIMARY KEY, name VARCHAR2(255))\"\n            )\n            cursor.execute(\"INSERT INTO test_right (name) VALUES ('John Smith')\")\n            cursor.execute(\"INSERT INTO test_right (name) VALUES ('Rhonda')\")\n            cursor.execute(\"COMMIT\")\n        authors = Author.objects.using('oracle').annotate(name_part=Right(\"name\", 5))\n        self.assertQuerySetEqual(\n            authors.order_by(\"name\"), [\"Smith\", \"honda\"], lambda a: a.name_part\n        )\n", "    def setUpTestData(cls):\n        Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        Author.objects.create(name=\"Rhonda\")\n"], "sample_669": ["    def test_capture_with_multiprocessing(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import multiprocessing\n\n                print(\"hello from worker\")\n\n                p = multiprocessing.Process(target=worker_func)\n                p.start()\n                p.join()\n\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines([\"hello from worker*\"])\n", "def test_MultiCapture_reset_twice_error(testdir):\n    cap = capture.MultiCapture()\n    cap.start_capturing()\n    out, err = cap.readouterr()\n    with pytest.raises(ValueError):\n        cap.stop_capturing()\n\n    with pytest.raises(ValueError):\n        cap.stop_capturing()\n\n    assert out == \"\"\n    assert err == \"\"\n", "def test_dontreadfrominput_closed_file():    \n    f = capture.DontReadFromInput()\n    f.close()\n    pytest.raises(ValueError, f.read)\n\n", "    def test_encodedfile_writelines_empty(tmpfile: BinaryIO) -> None:\n        ef = capture.EncodedFile(tmpfile, encoding=\"utf-8\")\n        assert ef.writelines([]) is None  # type: ignore[func-returns-value]  # noqa: F821\n        ef.flush()\n        tmpfile.seek(0)\n        assert tmpfile.read() == b\"\"\n        tmpfile.close()\n", "def test_get_multicapture_defaults():\n    cap = _get_multicapture()\n    assert isinstance(cap, MultiCapture)\n    assert cap.out and cap.err and not cap.in_\n\n\n"], "sample_298": ["    def test_token_with_changed_password(self):\n        \"\"\"\n        Updating the user password invalidates the token.\n        \"\"\"\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n        user.set_password('newpassword')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n\n", "    def test_token_with_changed_password(self):\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        user.set_password('newpassword')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n", "    def test_token_with_user_password_change(self):\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        user.set_password('newpassword')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n", "    def test_invalid_password_resets(self):\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        user.set_password('testpwnew')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n\n", "    def test_check_token_with_modified_timestamp(self):\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        # Modify the timestamp part of the token\n        tk1_modified = tk1.split('-')\n        tk1_modified[0] = str(int(tk1_modified[0]) + 1)\n        tk1_modified = '-'.join(tk1_modified)\n        self.assertIs(p0.check_token(user, tk1_modified), False)\n"], "sample_844": ["compilation error", "compilation error", "compilation error", "def test_set_invalid_params():\n    clust = OPTICS(min_samples=-1)\n    with pytest.raises(ValueError, match=\"min_samples must be a positive integer\"):\n        clust.fit(X)\n    clust = OPTICS(max_eps=-1)\n    with pytest.raises(ValueError, match=\"max_eps must be greater than 0\"):\n        clust.fit(X)\n", "compilation error"], "sample_995": ["compilation error", "compilation error", "def test_issue_11264():\n    x = Symbol('x')\n    assert (x + 1).equals(x + 1)\n    assert not (x + 1).equals(x + 2)\n", "def test_issue_12643():\n    x = S(12345678901234567890)/10**18\n    assert x.n(50) == x._mpf_\n", "compilation error"], "sample_529": ["def test_legend_title_fontsize_auto():\n    # test that legend title fontsize is set to 'medium' by default\n    plt.plot(range(10), label='Aardvark')\n    leg = plt.legend(title='My Legend')\n    assert leg.get_title().get_fontsize() == mpl.rcParams['font.size']\n", "def test_legend_ncol(ncol):\n    fig, ax = plt.subplots()\n    for i in range(ncol*2):\n        ax.plot([i], [i], label=f'Line {i}')\n    leg = ax.legend(ncol=ncol)\n    assert len(leg.get_children()) == ncol*2\n    assert len(leg.get_lines()) == ncol*2\n    assert len(leg.get_texts()) == ncol*2\n", "def test_legend_fontsize_units():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label='Test')\n    \n    # Test relative fontsize\n    ax.legend(fontsize='large')\n\n    # Test absolute fontsize\n    ax.legend(fontsize=12)\n    \n    # Test fontsize with units\n    ax.legend(fontsize='10pt')\n", "    def test_legend_markerscale(self):\n        # Check that markerscale is respected by legend\n        fig, ax = plt.subplots()\n        ax.plot([0], [0], marker='o', markersize=10, label='Big')\n        ax.plot([1], [1], marker='o', markersize=5, label='Small')\n        legend = ax.legend()\n        legend_handles = legend.get_lines()\n        assert legend_handles[0].get_markersize() == 10\n        assert legend_handles[1].get_markersize() == 5\n\n", "def test_legend_handles_labels_empty():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line')\n    leg = ax.legend(handles=[], labels=[])\n    assert len(leg.legend_handles) == 0\n    assert len(leg.get_texts()) == 0\n"], "sample_225": ["    def test_admin_view(self):\n        request = self.request_factory.get(reverse('test_adminsite:index'))\n        request.user = self.u1\n        view = site.admin_view(lambda r: None)\n        response = view(request)\n        self.assertEqual(response.status_code, 200)\n        \n        # Test if user is not authenticated\n        request = self.request_factory.get(reverse('test_adminsite:index'))\n        response = view(request)\n        self.assertEqual(response.status_code, 302)  \n", "    def test_get_app_list(self):\n        request = self.request_factory.get(reverse('test_adminsite:index'))\n        request.user = self.u1\n        app_list = site.get_app_list(request)\n        self.assertEqual(len(app_list), 2)\n        self.assertEqual(app_list[0]['name'], 'Auth')\n        self.assertEqual(app_list[1]['name'], 'admin_views')\n", "    def test_actions(self):\n        self.site.add_action(test_action)\n        actions = list(self.site.actions)\n        self.assertIn(('test_action', test_action), actions)\n", "    def test_admin_view_decorator(self):\n            return 'my_view'\n\n        decorated_view = self.site.admin_view(my_view)\n\n        request = self.request_factory.get('/test_admin/admin/')\n        request.user = self.u1\n\n        with self.assertTemplateUsed('admin/login.html'):\n            response = decorated_view(request)\n            self.assertEqual(response.status_code, 302)\n", "    def test_each_context_is_popup(self):\n        request = self.request_factory.get(reverse('test_adminsite:index'))\n        request.user = self.u1\n        request.is_popup = True  # Simulate a popup request\n        ctx = site.each_context(request)\n        self.assertIs(ctx['is_popup'], True)\n"], "sample_1012": ["compilation error", "compilation error", "compilation error", "def test_NumPyPrinter_Piecewise_with_default():\n    p = NumPyPrinter()\n    expr = Piecewise((x, Eq(y, 0)), (x**2, Eq(y, 1)), (x**3, True))\n    assert p.doprint(expr) == 'numpy.select([y == 0, y == 1], [x, x**2], default=x**3)'\n", "def test_issue_14856():\n    p = PythonCodePrinter()\n    assert p.doprint(sympy.Float(1.23)) == '1.23'\n"], "sample_322": ["    def test_detect_soft_applied_with_deleted_table(self):\n        \"\"\"\n        #25779 - Table deletion shouldn't lead to soft-applied detection\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        # Were the tables there before?\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_tribble\")\n        # Migrate forwards\n        executor.migrate([(\"migrations\", \"0001_initial\")])\n        # Rebuild the graph to reflect the new DB state\n        executor.loader.build_graph()\n        # Delete the table of the first migration\n        with connection.schema_editor() as editor:\n            editor.execute(editor.sql_delete_table % {\"table\": \"migrations_author\"})\n        # Apply the migration again\n        migration = executor.loader.get_migration(\"migrations\", \"0001_initial\")\n        self.assertIs(executor.detect_soft_applied(None, migration)[0], False)\n        executor.migrate([(\"migrations\", \"0001_initial\")])\n        # Rebuild the graph to reflect the new DB state\n        executor.loader.build_graph()\n        # And migrate back to clean up the database\n        executor.migrate([(\"migrations\", None)])\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_tribble\")\n", "    def test_applied_migrations_not_updated_on_fake_migration(self):\n        \"\"\"\n        Ensure applied migrations aren't updated when migrations are applied\n        with fake=True.\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_dependency(None, a2, a1)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {a1: a1_impl})\n        applied_migrations = executor.recorder.applied_migrations()\n\n        executor.migrate([('a', '1')], fake=True)\n        self.assertEqual(executor.recorder.applied_migrations(), applied_migrations)\n", "    def test_migrate_no_fake_initial_with_existing_data(self):\n        \"\"\"\n        Migrate forwards doesn't apply initial migrations if there's already data.\n\n        This is a regression test for #23093.\n        \"\"\"\n        # Mock out the database connection to control data existence.\n        class MockConnection:\n                return True\n                return True\n\n        executor = MigrationExecutor(MockConnection())\n        executor.loader = MockLoader(\n            MigrationGraph(),\n            {\n                ('migrations', '0001_initial'): FakeMigration('0001_initial')\n            },\n        )\n        # Initial migration is requested\n        plan = executor.migration_plan([('migrations', '0001_initial')])\n        self.assertEqual(plan, [])\n", "    def test_rollback_with_replace_dependencies(self):\n        \"\"\"\n        Test rollback handling when a replacement migration was applied.\n\n        a: 1 <--- 2\n        b:    \\- 1\n\n        If we apply a1, a2, b1, and then unapply a2, the rollback should\n        remove b1 and then a2, and leave a1.\n\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, b1, a1)\n\n        # Simulate a scenario where a2 is a replacement migration\n        graph.replace_dependencies(a2, [a1])\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            a2: a2_impl,\n            b1: b1_impl,\n        })\n\n        plan = executor.migration_plan({a1})\n\n        self.assertEqual(plan, [(b1_impl, False), (a2_impl, False)])\n\n", "    def test_migrate_forwards_nothing_to_do(self):\n        r\"\"\"\n        If the current state satisfies the given target, do nothing.\n\n        a: 1 <--- 2\n        b:    \\- 1\n        c:     \\- 1\n\n        If a1 and a2 are applied, and we're asked to migrate to a2, don't apply\n        or unapply b1 or c1, regardless of their current state.\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        c1_impl = FakeMigration('c1')\n        c1 = ('c', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(c1, c1_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, c1, a1)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            a2: a2_impl,\n            b1: b1_impl,\n        })\n\n        plan = executor.migration_plan({a2})\n\n        self.assertEqual(plan, [])\n"], "sample_816": ["    def test_empty_vocabulary_warning(self):\n        vec = CountVectorizer()\n        with pytest.warns(UserWarning,\n                          match=\"Vocabulary is empty.*add at least one\"):\n            vec.fit([])\n", "    def test_countvectorizer_vocabulary_intersection(self):\n        # test that vocabulary intersection with existing vocabulary is handled\n        # correctly\n        \n        vectorizer = CountVectorizer(vocabulary=[\"apple\", \"banana\", \"cherry\"])\n        \n        # Add new documents with some overlapping vocabulary and some new words\n        new_documents = [\"apple pie\", \"banana bread\", \"grapefruit juice\"]\n        \n        # Fit the vectorizer with new documents\n        vectorizer.fit_transform(new_documents)\n        \n        # Assert that the vocabulary now includes the overlapping and new words\n        expected_vocabulary = {\"apple\", \"banana\", \"cherry\", \"pie\", \"bread\", \"grapefruit\", \"juice\"}\n        assert vectorizer.get_feature_names_out() == sorted(list(expected_vocabulary))\n", "    def test_vectorizer_empty_vocabulary():\n        # Test that CountVectorizer and TfidfVectorizer handle empty\n        # vocabularies gracefully.\n\n        # Empty vocabulary due to empty input\n        vect = CountVectorizer()\n        X = vect.fit_transform([])\n        assert_array_equal(X.toarray(), [])  \n\n        # Empty vocabulary due to stop_words excluding all tokens\n        vect = TfidfVectorizer(stop_words='english')\n        X = vect.fit_transform(['This is a test', 'This is another test'])\n        assert_array_equal(X.toarray(), []) \n\n", "def test_empty_vocabulary(Estimator):\n    # test for empty vocabulary when fitting an empty list\n    estimator = Estimator()\n    with pytest.raises(ValueError, match='Vocabulary is empty. '\n                                        'Did you provide any text to fit?'):\n        estimator.fit([])\n", "def test_vectorizer_preserve_dtype():\n    # Ensure vectorizers preserve the dtype of the input data\n    for dtype in [np.int32, np.int64, np.float32, np.float64]:\n        X = np.array(JUNK_FOOD_DOCS, dtype=dtype)\n        for Vectorizer in [CountVectorizer, TfidfVectorizer]:\n            vec = Vectorizer()\n            X_transformed = vec.fit_transform(X)\n            assert X_transformed.dtype == dtype\n"], "sample_1164": ["compilation error", "def test_qop():\n    qop = QOp('Q')\n    assert str(qop) == 'Q'\n    assert pretty(qop) == 'Q'\n    assert upretty(qop) == 'Q'\n    assert latex(qop) == 'Q'\n    sT(qop, \"QOp(Symbol('Q'))\")\n", "def test_qoperator():\n    a = QOperator('a')\n    x = QOperator('x')\n    comm = Commutator(a, x)\n    assert str(comm) == '[a,x]'\n    ascii_str = \\", "def test_superop():\n    a = Destroy(1)\n    rho = DensityOperator()\n    u = SuperOp(a*a.dag())\n    assert str(u) == 'SuperOp(a*Dagger(a))'\n    assert pretty(u) == '\u214b(a*\u2020a)'\n    assert latex(u) == r'\\mathcal{L}_{a a^{\\dagger}}'\n    assert str(u*rho) == 'SuperOp(a*Dagger(a))(DensityOperator())'\n    ascii_str = \\", "    def test_qbit():\n        qbit = Qubit(1,0,0,1)\n        assert str(qbit) == '|1001>'\n        assert pretty(qbit) == '|1001>'\n        assert upretty(qbit) == '\u27581001\u27e9'\n        assert latex(qbit) == r'{\\left|1001\\right\\rangle }'\n        sT(qbit, \"Qubit(Integer(1),Integer(0),Integer(0),Integer(1))\")\n"], "sample_1204": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_is_subgroup():\n    a = Permutation(1, 2)(3, 4)\n    b = Permutation(1, 3)(2, 4)\n    G = PermutationGroup([a, b])\n    H = PermutationGroup([a])\n    assert G.is_subgroup(G)\n    assert H.is_subgroup(G)\n    assert not G.is_subgroup(H)\n"], "sample_424": ["    def test_references_field_related_name(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\"Other\", models.CASCADE, related_name='related'),\n        )\n        self.assertIs(operation.references_field(\"Other\", \"related\", \"migrations\"), True)\n        self.assertIs(operation.references_field(\"Other\", \"whatever\", \"migrations\"), False)\n\n\n", "    def test_add_field_on_swappable_model(self):\n        \"\"\"\n        Tests AddField on a model that is swappable.\n        \"\"\"\n        operation = migrations.AddField(\n            \"Pony\",\n            \"height\",\n            models.FloatField(null=True, default=5),\n        )\n        project_state, new_state = self.make_test_state(\"test_adfligsw\", operation)\n        # Test the database alteration\n        self.assertTableNotExists(\"test_adfligsw_pony\")\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\n                \"test_adfligsw\", editor, project_state, new_state\n            )\n        self.assertTableNotExists(\"test_adfligsw_pony\")\n        # And test reversal\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\n                \"test_adfligsw\", editor, new_state, project_state\n            )\n        self.assertTableNotExists(\"test_adfligsw_pony\")\n", "    def test_field_operation_to_field_name(self):\n        operation = FieldOperation(\"Model\", \"field\", models.ForeignKey(\"Other\", models.CASCADE, to_field=\"to_field_name\"))\n        self.assertEqual(operation.to_field_name, 'to_field_name')\n", "    def test_references_field_by_related_name(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\"Other\", models.CASCADE, related_name=\"related\"),\n        )\n        self.assertIs(operation.references_field(\"Other\", \"related\", \"migrations\"), True)\n        self.assertIs(\n            operation.references_field(\"Other\", \"whatever\", \"migrations\"), False\n        )\n", "    def test_alter_field_ignore_swapped(self):\n        \"\"\"\n        Tests the AlterField operation ignores swapped models.\n        \"\"\"\n        operation = migrations.AlterField(\n            \"Pony\", \"pink\", models.CharField(max_length=10)\n        )\n        project_state, new_state = self.make_test_state(\"test_altfigs\", operation)\n        # Test the database alteration\n        self.assertTableNotExists(\"test_altfigs_pony\")\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_altfigs\", editor, project_state, new_state)\n        self.assertTableNotExists(\"test_altfigs_pony\")\n        # And test reversal\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\n                \"test_altfigs\", editor, new_state, project_state\n            )\n        self.assertTableNotExists(\"test_altfigs_pony\")\n"], "sample_959": ["    def check(text, domain='cpp'):\n        doctree = restructuredtext.parse(app, text)\n        assert len(doctree) == 1\n        assert isinstance(doctree[0], nodes.paragraph)\n        assert isinstance(doctree[0][0], addnodes.desc_signature)\n        assert isinstance(doctree[0][0][0], nodes.Text)\n        assert doctree[0][0][0].astext() == text\n\n        # Check that the domain is correctly set\n        assert doctree[0][0].get('domain') == domain\n", "    def test_domain_cpp_parse_noindexentry_multiple(app):\n        text = (\".. cpp:function:: void f()\\n\"\n                \".. cpp:function:: void g()\\n\"\n                \"   :noindexentry:\\n\"\n                \".. cpp:function:: void h()\")\n        doctree = restructuredtext.parse(app, text)\n        assert_node(doctree, (addnodes.index, desc, addnodes.index, desc, addnodes.index))\n        assert_node(doctree[0], addnodes.index, entries=[('single', 'f (C++ function)', '_CPPv41fv', '', None)])\n        assert_node(doctree[2], addnodes.index, entries=[])\n        assert_node(doctree[4], addnodes.index, entries=[('single', 'h (C++ function)', '_CPPv41hv', '', None)])\n\n", "def test_domain_cpp_parse_class_inline(app, warning):\n    text = \".. cpp:class:: MyClass\\n\" \\\n           \"   :inline:\\n\" \\\n           \"\\n\" \\\n           \"   This is a class.\"\n    restructuredtext.parse(app, text)\n    ws = warning.getvalue().split(\"\\n\")\n    assert len(ws) == 0\n\n\n", "def test_domain_cpp_parse_escaped_backslash(app, warning):\n    text = (\".. cpp:function:: void f( \\\\\\\\\\n\"\n            \"  int a\\n\"\n            \")\")\n    restructuredtext.parse(app, text)\n    ws = warning.getvalue().split(\"\\n\")\n    assert len(ws) == 0\n", "compilation error"], "sample_56": ["    def test_fieldsets_nested_tuples_with_duplicate_fields(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fieldsets = (\n                (None, {\n                    'fields': [('title', 'album'), ('title', 'album')]\n                }),\n            )\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"There are duplicate field(s) in 'fieldsets[0][1]'.\",\n                obj=MyModelAdmin,\n                id='admin.E012'\n            )\n        ]\n        self.assertEqual(errors, expected)\n\n", "    def test_empty_list_filter(self):\n        class BookAdminEmptyFilter(admin.ModelAdmin):\n            list_filter = []\n\n        errors = BookAdminEmptyFilter(Book, AdminSite()).check()\n        self.assertEqual(errors, [])\n", "    def test_invalid_list_filter_field(self):\n        class BookAdminWithInvalidListFilter(admin.ModelAdmin):\n            list_filter = ['invalid_field']\n        \n        errors = BookAdminWithInvalidListFilter(Book, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"Invalid field 'invalid_field' for 'list_filter' on 'admin_checks.BookAdminWithInvalidListFilter'.\",\n                obj=BookAdminWithInvalidListFilter,\n                id='admin.E127',\n            ),\n        ]\n        self.assertEqual(errors, expected)\n\n", "    def test_fieldsets_with_object_ids(self):\n        \"\"\"\n        Regression test for #24234 - Allow using object IDs directly in fieldsets.\n        \"\"\"\n        class MyModelAdmin(admin.ModelAdmin):\n            fieldsets = [\n                (None, {\n                    'fields': ['id', 'title']\n                }),\n            ]\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        self.assertEqual(errors, [])\n", "    def test_invalid_field_in_list_filter(self):\n        \"\"\"\n        Regression for #24244: Ensure an error is raised if a field used in 'list_filter' doesn't exist.\n        \"\"\"\n        class BookAdminWithInvalidListFilter(admin.ModelAdmin):\n            list_filter = ['nonexistent_field']\n\n        errors = BookAdminWithInvalidListFilter(Book, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'list_filter[0]' refers to 'nonexistent_field', which is not an attribute \"\n                \"of 'admin_checks.Book'.\",\n                obj=BookAdminWithInvalidListFilter,\n                id='admin.E007',\n            )\n        ]\n        self.assertEqual(errors, expected)\n"], "sample_1092": ["def test_cse_issue_19882():\n    x, y, z = symbols('x y z')\n    expr = (x + y)/(z + 1)\n    substitutions, reduced = cse([expr, expr**2])\n    assert len(substitutions) == 1\n    assert substitutions[0][1] == expr\n\n", "def test_issue_6281():\n    a, b, c, d, e = symbols('a b c d e')\n    expr = (a*b + c)*(d*e + f)\n    assert cse(expr) == ([(x0, a*b + c)], [x0*(d*e + f)])\n", "def test_issue_14911():\n    f = Function('f')\n    x = Symbol('x')\n    eq = f(x)\n    assert cse(eq) == ([], [eq])\n", "def test_cse_with_assumptions():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    exprs = [x**2 + 1, x*y + 1, y**2 + 2*y + 1]\n    subst, red = cse(exprs, assumptions=(x > 0, y > 0))\n    assert len(subst) > 0, \"Assumptions should lead to CSE opportunities\"\n\n", "def test_issue_18801():\n    x, y = symbols('x y')\n    f = Function('f')\n    expr = f(x, y) + f(x, y)\n    assert cse(expr) == ([(x0, f(x, y))], [2*x0])\n"], "sample_1047": ["   def test_issue_16332():\n        x = Symbol('x', real=True)\n        y = Symbol('y', real=True)\n        p = Symbol('p', positive=True)\n        assert (1/(x + y)).is_positive is None\n        assert (1/(x + p)).is_positive\n\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_121": ["    def test_check_constraints_db_features_missing(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [models.CheckConstraint(check=models.Q(age__gte=18), name='is_adult')]\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"The database doesn't support check constraints but they are \"\n                \"defined in the 'Meta' options.\",\n                obj=Model,\n                id='models.E028',\n            ),\n        ])\n", "    def test_check_constraints_invalid_db_features(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                required_db_features = {'unsupported_feature'}\n                constraints = [models.CheckConstraint(check=models.Q(age__gte=18), name='is_adult')]\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"The database backend doesn't support the required feature 'unsupported_feature'.\",\n                obj=Model,\n                id='models.E028',\n            )\n        ])\n", "    def test_unique_together_with_invalid_fields(self):\n        class Model(models.Model):\n            field1 = models.CharField(max_length=100)\n            field2 = models.CharField(max_length=100)\n\n            class Meta:\n                unique_together = ('field1', 'nonexistent_field')\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"Field 'nonexistent_field' referenced in 'unique_together' does not exist in model 'invalid_models_tests.Model'.\",\n                obj=Model,\n                id='models.E008',\n            ),\n        ])\n", "    def test_check_constraints_multiple(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n            name = models.CharField(max_length=100)\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(check=models.Q(age__gte=18), name='is_adult'),\n                    models.CheckConstraint(check=models.Q(name__isnull=False), name='has_name'),\n                ]\n\n        self.assertEqual(Model.check(), [])\n", "    def test_unique_together_with_related_field(self):\n        class Author(models.Model):\n            pass\n\n        class Book(models.Model):\n            author = models.ForeignKey(Author, models.CASCADE)\n            title = models.CharField(max_length=100)\n\n            class Meta:\n                unique_together = ('author', 'title')\n\n        self.assertEqual(Book.check(), [])\n"], "sample_969": ["def test_stringify_type_hints_generator():\n    assert stringify(Generator[str, int, None], False) == \"Generator[str, int, None]\"\n    assert stringify(Generator[str, int, None], True) == \"~typing.Generator[str, int, None]\"\n\n    assert stringify(Generator[None, None, None], False) == \"Generator[None, None, None]\"\n    assert stringify(Generator[None, None, None], True) == \"~typing.Generator[None, None, None]\"\n\n\n", "def test_stringify_type_hints_final():\n    from typing import Final # type: ignore\n\n    MyFinalInt = Final[int]\n    assert stringify(MyFinalInt, False) == 'Final[int]'\n    assert stringify(MyFinalInt, True) == '~typing.Final[int]'\n\n", "    def test_stringify_type_union_operator_with_complex_types():\n        MyType = typing.List[int]\n        assert stringify(MyType | None, False) == \"list[int] | None\"\n        assert stringify(MyType | None, True) == \"list[int] | None\"\n", "compilation error", "def test_stringify_type_hints_nested():\n    assert stringify(List[Dict[str, List[int]]], False) == \"List[Dict[str, List[int]]]\"\n    assert stringify(List[Dict[str, List[int]]], True) == \"~typing.List[~typing.Dict[str, ~typing.List[int]]]\"\n"], "sample_250": ["    def test_format_iso8601(self):\n        dt = datetime(2023, 10, 26, 12, 30, 45, 123456)\n        self.assertEqual(format(dt, 'c'), dt.isoformat())\n        self.assertEqual(dateformat.format(dt, 'c'), dt.isoformat())\n", "    def test_timezone_offset_with_naive_datetime(self):\n        dt = datetime(2009, 5, 16, 5, 30, 30)\n        self.assertEqual(dateformat.format(dt, 'O'), '')\n        self.assertEqual(dateformat.format(dt, 'Z'), '')\n\n", "    def test_time_formats_with_custom_tzinfo(self):\n        dt = datetime(2023, 10, 26, 12, 30, tzinfo=get_fixed_timezone(-360))\n        self.assertEqual(dateformat.format(dt, 'O'), '-0600')\n        self.assertEqual(dateformat.format(dt, 'T'), 'EST')\n\n", "    def test_format_with_timezone_name(self):\n        if TZ_SUPPORT:\n            tz = get_fixed_timezone(360)  # +1 hour\n            aware_dt = datetime(2023, 10, 27, 10, 0, 0, tzinfo=tz)\n            self.assertEqual(dateformat.format(aware_dt, 'T'), '+0100')\n", "     def test_rfc5322_format_with_naive_datetime(self):\n        dt = datetime(2009, 5, 16, 5, 30, 30)\n        self.assertEqual(dateformat.format(dt, 'r'), 'Thu, 16 May 2009 05:30:30 +0000')\n\n"], "sample_799": ["    def my_scorer(estimator, X, y):\n        return len(estimator.coef_[0])\n", "    def failing_scorer(estimator, X, y):\n        raise ValueError(\"Failing scorer\")", "    def raise_scorer(estimator, X_test):\n        raise ValueError(\"Score error\")\n    ", "    def scorer_with_side_effect(estimator, X, y):\n        global side_effect_counter\n        side_effect_counter += 1\n        return 0.5\n", "def test_cross_val_predict_for_multioutput_regression():\n    # Test cross_val_predict with multioutput regression\n    from sklearn.multioutput import MultiOutputRegressor\n    from sklearn.linear_model import LinearRegression\n\n    X, y = make_regression(n_samples=100, n_targets=2, random_state=0)\n    regressor = MultiOutputRegressor(LinearRegression())\n    scores = cross_val_predict(regressor, X, y, cv=5)\n    assert scores.shape == y.shape\n"], "sample_459": ["    def test_enum_choices_cleans_valid_integer(self):\n        f = models.IntegerField(choices=self.Choices.choices)\n        self.assertEqual(f.clean(1, None), 1)\n", "    def test_integerfield_cleans_valid_float(self):\n        f = models.IntegerField()\n        self.assertEqual(f.clean(2.0, None), 2)\n", "    def test_integerfield_cleans_valid_float(self):\n        f = models.IntegerField()\n        self.assertEqual(f.clean(\"2.0\", None), 2)\n\n", "    def test_integerfield_cleans_valid_float(self):\n        f = models.IntegerField()\n        self.assertEqual(f.clean(2.0, None), 2)\n", "    def test_integerfield_cleans_valid_float(self):\n        f = models.IntegerField()\n        self.assertEqual(f.clean(2.0, None), 2)\n"], "sample_1095": ["compilation error", "compilation error", "def test_permutation_with_symbols():\n    x, y = symbols('x y')\n    p = Permutation(0, 1)\n\n    assert p.apply(x) == AppliedPermutation(p, x)\n\n    assert p.apply(x) == y\n    assert p.apply(y) == x\n", "compilation error", "compilation error"], "sample_822": ["    def test_pairwise_distances_chunked_empty_input():\n        # Test pairwise_distances_chunked with empty input arrays\n\n        X = np.array([], dtype=np.float64).reshape(0, 5)\n        Y = np.array([], dtype=np.float64).reshape(0, 5)\n\n        # Test with various metrics\n        for metric in ['euclidean', 'cityblock', 'cosine', 'manhattan']:\n            distances = pairwise_distances_chunked(X, Y, metric=metric)\n            assert distances.shape == (0, 0)\n\n        # Test with precomputed distance matrix\n        distances = pairwise_distances_chunked(X, Y, metric='precomputed')\n        assert distances.shape == (0, 0)\n", "    def test_pairwise_distances_chunked_empty_chunk_error(self):\n        # Check that pairwise_distances_chunked raises an error if a chunk is empty.\n\n        X = np.empty((0, 10))\n        Y = np.random.random_sample((10, 10))\n        with pytest.raises(ValueError, match=\"Empty chunk\"):\n            list(pairwise_distances_chunked(X, Y, working_memory=1))\n", "def test_pairwise_distances_chunked_empty():\n    # Check that pairwise_distances_chunked handles empty arrays gracefully\n    X = np.array([])\n    Y = np.array([])\n    with pytest.raises(ValueError):\n        list(pairwise_distances_chunked(X, Y, metric='euclidean'))\n\n    X = np.array([[1, 2], [3, 4]])\n    Y = np.array([])\n    with pytest.raises(ValueError):\n        list(pairwise_distances_chunked(X, Y, metric='euclidean'))\n\n    X = np.array([])\n    Y = np.array([[1, 2], [3, 4]])\n    with pytest.raises(ValueError):\n        list(pairwise_distances_chunked(X, Y, metric='euclidean'))\n", "compilation error", "    def test_pairwise_distances_chunked_empty():\n        # Test pairwise distances chunked with empty arrays\n        X = np.array([])\n        Y = np.array([])\n\n        dist = list(pairwise_distances_chunked(X, Y, metric='euclidean',\n                working_memory=1))\n\n        assert len(dist) == 0\n\n        Y = np.array([[1, 2], [3, 4]])\n\n        dist = list(pairwise_distances_chunked(X, Y, metric='euclidean',\n                working_memory=1))\n\n        assert len(dist) == 0\n\n        X = np.array([[1, 2], [3, 4]])\n        dist = list(pairwise_distances_chunked(X, Y, metric='euclidean',\n                working_memory=1))\n\n        assert len(dist) == 2\n\n"], "sample_985": ["compilation error", "compilation error", "compilation error", "def test_is_constant():\n    x = symbols('x')\n    assert sin(1).is_constant is True\n    assert cos(x).is_constant is False\n    assert (x + 1).is_constant is False\n    assert 1.5.is_constant is True\n", "compilation error"], "sample_481": ["    def test_join_empty_joiner(self):\n        output = self.engine.render_to_string(\"join_empty_joiner\", {\"a\": [\"alpha\", \"beta & me\"]})\n        self.assertEqual(output, \"alphabeta & me\")\n", "    def test_join09(self):\n        output = self.engine.render_to_string(\n            \"join09\", {\"a\": [\"alpha\", \"beta & me\"], \"var\": \" & \"}\n        )\n        self.assertEqual(output, \"alpha & beta & me\")\n", "    def test_join09(self):\n        output = self.engine.render_to_string(\n            \"join09\", {\"a\": [\"alpha\", \"beta & me\"], \"var\": mark_safe(\" & \")}\n        )\n        self.assertEqual(output, \"alpha & beta & me\")\n", "    def test_join_empty_list(self):\n        empty_list = []\n        context = {\"empty_list\": empty_list, \"var_joiner\": \"<br/>\"}\n        output = self.engine.render_to_string(\"join_empty_list\", context)\n        self.assertEqual(output, \"\")\n\n", "    def test_join_empty_list(self):\n        empty_list = []\n        output = self.engine.render_to_string(\"join_empty_list\", {\"empty_list\": empty_list})\n        self.assertEqual(output, \"\")\n"], "sample_502": ["def test_subplots_adjust_auto():\n    fig, axs = plt.subplots(2, 2)\n    plt.subplots_adjust(wspace=0, hspace=0)\n    assert fig.subplotpars.wspace == 0\n    assert fig.subplotpars.hspace == 0\n\n    fig2, axs2 = plt.subplots(2, 2)\n    plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n    assert fig2.subplotpars.left == 0.1\n    assert fig2.subplotpars.right == 0.9\n    assert fig2.subplotpars.top == 0.9\n    assert fig2.subplotpars.bottom == 0.1\n", "    def test_subplot_polar_reuse_with_existing_axes():\n        fig, ax = plt.subplots(1, 2)\n        ax[0].plot([1, 2, 3], [4, 5, 6])\n        ax[1] = plt.subplot(1, 2, 2, projection='polar')\n        ax[1].plot([1, 2, 3], [4, 5, 6])\n        assert ax[1].name == 'polar'\n        ax[1] = plt.subplot(1, 2, 2, projection='polar')\n        assert ax[1].name == 'polar'\n        assert ax[0] is not ax[1]\n", "def test_subplots_figsize():\n    fig, axes = plt.subplots(2, 2, figsize=(10, 5))\n    assert fig.get_size_inches() == (10, 5)\n    for ax in axes.flatten():\n        assert ax.figure is fig\n", "def test_subplot_kwargs_reuse():\n    fig, ax = plt.subplots()\n    ax1 = plt.subplot(111, facecolor='r')\n    assert ax is ax1\n    ax2 = plt.subplot(111, facecolor='r', xlim=[0,1], ylim=[0,1])\n    assert ax2 is not ax1\n    assert ax2.get_xlim() == (0, 1)\n    assert ax2.get_ylim() == (0, 1)\n", "def test_subplot_add_axes():\n\n    fig, ax = plt.subplots()\n    ax2 = fig.add_axes([0.2, 0.2, 0.2, 0.2])\n    assert ax2 is plt.gca()\n    assert ax is not ax2\n"], "sample_474": ["    def test_empty_string(self):\n        authors = Author.objects.annotate(first_initial=Left(\"name\", 1))\n        self.assertCountEqual(authors.filter(first_initial=Chr(ord(\"\"))), [])\n", "    def test_sqlite_chr(self):\n        authors = Author.objects.annotate(first_initial=Chr(Ord(\"name\")))\n        self.assertSequenceEqual(\n            authors.values_list(\"first_initial\", flat=True).order_by(\"id\"),\n            [\"J\", \"\u00c9\", \"R\"],\n        )\n", "    def test_null_handling(self):\n        self.rhonda.name = None\n        self.rhonda.save()\n        authors = Author.objects.annotate(first_initial=Chr(Left(\"name\", 1)))\n        self.assertIsNone(authors.get(pk=self.rhonda.pk).first_initial)\n", "    def test_null_handling(self):\n        Author.objects.create(name=None)\n        authors = Author.objects.annotate(first_initial=Left(\"name\", 1))\n        self.assertEqual(authors.filter(first_initial__isnull=True).count(), 1)\n", "    def test_empty_string(self):\n        authors = Author.objects.annotate(first_initial=Left(\"name\", 1))\n        self.assertCountEqual(authors.filter(first_initial=Chr(ord(\"\"))), [])\n"], "sample_1101": ["def test_schur_number_input_type():\n    raises(TypeError, lambda: SchurNumber(\"five\"))\n", "def test_schur_number_eval():\n    raises(ValueError, lambda: SchurNumber(-1))\n    raises(ValueError, lambda: SchurNumber(Rational(1,2)))\n", "compilation error", "def test_schur_number_negative():\n    raises(ValueError, lambda: SchurNumber(-1))\n    raises(ValueError, lambda: SchurNumber(-10))\n\n", "def test_schur_number_lower_bound():\n    raises(ValueError,lambda: SchurNumber(Rational(1,2)).lower_bound())\n    raises(ValueError,lambda: SchurNumber(-1).lower_bound())\n"], "sample_1154": ["compilation error", "compilation error", "compilation error", "def test__linsolve_underdetermined():\n    eqs = [\n        x + y - 2,\n        2*x + 2*y - 4\n    ]\n    sol = {x:2 - y, y:y}\n    assert _linsolve(eqs, [x, y]) == sol\n", "compilation error"], "sample_931": ["def test_pyexception_nested(app):\n    text = \".. py:exception:: Module.Subclass.Error\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"exception \"],\n                                                    [desc_addname, \"Module.\"],\n                                                    [desc_addname, \"Subclass.\"],\n                                                    [desc_name, \"Error\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"exception\",\n                domain=\"py\", objtype=\"exception\", noindex=False)\n\n    assert 'Module.Subclass.Error' in domain.objects\n    assert domain.objects['Module.Subclass.Error'] == ('index', 'Module.Subclass.Error', 'exception')\n", "def test_pytype(app):\n    text = \".. py:function:: func\\n\"\n    \"   :type: int\\n\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, ))\n    assert_node(doctree[1], desc, desctype=\"function\", domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][1][0], desc_signature, [desc_name, desc_parameterlist])\n    assert_node(doctree[1][1][0][0], desc_name, \"func\")\n    assert_node(doctree[1][1][0][1], desc_parameterlist, [])\n    assert_node(doctree[1][1][1], desc_content, [\n    desc_annotation,\n    nodes.Text(\"int\"),\n    nodes.Text(\"\")\n    ])\n\n", "def test_pyexception_module_is_ignored(app):\n    text = (\".. py:exception:: IOError\\n\"\n            \"   :module: exceptions\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ( [desc_annotation, \"exception \"],\n                                                    [desc_addname, \"exceptions.\"],\n                                                    [desc_name, \"IOError\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], desc, desctype=\"exception\",\n                domain=\"py\", objtype=\"exception\", noindex=False)\n", "def test_pyexception_signature_with_module(app):\n    text = \".. py:exception:: exceptions.IOError\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"exception \"],\n                                                    [desc_addname, \"exceptions.\"],\n                                                    [desc_name, \"IOError\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], desc, desctype=\"exception\",\n                domain=\"py\", objtype=\"exception\", noindex=False)\n", "def test_py_attribute_parameterlist(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :type: int\\n\"\n            \"      :param a: Description of parameter a\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'attr (Class attribute)', 'Class.attr', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"attr\"],\n                                                     [desc_annotation, (\": \",\n                                                                        [pending_xref, \"int\"])],\n                                                     [desc_parameterlist, ([desc_parameter,\n                                                                     [desc_name, \"a\"],\n                                                                     [desc_annotation,\n                                                                      \": \",\n                                                                      [nodes.plain, \"Description of parameter a\"]]])])],\n                                   [desc_content, ()]))\n\n"], "sample_27": ["    def test_diff_table_column_units(self):\n        \"\"\"Test diffing table columns with differing units.\"\"\"\n        ca = Column(\"A\", format=\"E\", unit=\"m\", array=[0.0, 1.0])\n        cb = Column(\"A\", format=\"E\", unit=\"km\", array=[0.0, 1.0])\n        ta = BinTableHDU.from_columns([ca])\n        tb = BinTableHDU.from_columns([cb])\n\n        diff = TableDataDiff(ta.data, tb.data)\n        assert not diff.identical\n        assert diff.diff_columns == []\n        assert diff.diff_headers == [(\"A\", \"TUNIT1\")]\n        assert diff.diff_values == []\n\n        report = diff.report()\n        assert \"Column A unit differences:\" in report\n        assert \"a: m\\nb: km\" in report\n", "    def test_fitsdiff_empty_files(self, tmp_path):\n        \"\"\"Regression test for https://github.com/astropy/astropy/issues/13330\"\"\"\n        path1 = tmp_path / \"test1.fits\"\n        path2 = tmp_path / \"test2.fits\"\n        \n        with open(path1, 'w') as f:\n            pass\n        with open(path2, 'w') as f: \n            pass\n        diff = FITSDiff(path1, path2)\n        assert diff.identical\n        report = diff.report()\n        assert \"No differences found.\" in report\n", "    def test_diff_empty_table_with_different_dtypes(self):\n        \"\"\"\n        Regression test for #2106\n\n        Ensure comparing tables with different dtypes but empty data doesn't crash.\n        \"\"\"\n        c1 = Column(\"A\", format=\"I\")\n        c2 = Column(\"B\", format=\"1J\")\n        ta = BinTableHDU.from_columns([c1], nrows=0)\n        tb = BinTableHDU.from_columns([c2], nrows=0)\n\n        diff = TableDataDiff(ta.data, tb.data)\n        assert diff.identical\n\n", "def test_rawdatadiff_diff_with_atol(tmp_path):\n    \"\"\"Regression test for https://github.com/astropy/astropy/issues/13330\"\"\"\n    path1 = tmp_path / \"test1.fits\"\n    path2 = tmp_path / \"test2.fits\"\n    a = np.zeros((10, 2), dtype=\"float32\")\n    a[:, 0] = np.arange(10, dtype=\"float32\") + 10\n    a[:, 1] = np.arange(10, dtype=\"float32\") + 20\n    b = a.copy()\n    changes = [(3, 13.1, 23.1), (8, 20.5, 30.5)]\n    for i, v, w in changes:\n        b[i, 0] = v\n        b[i, 1] = w\n\n    ca = Column(\"A\", format=\"20E\", array=[a])\n    cb = Column(\"A\", format=\"20E\", array=[b])\n    hdu_a = BinTableHDU.from_columns([ca])\n    hdu_a.writeto(path1, overwrite=True)\n    hdu_b = BinTableHDU.from_columns([cb])\n    hdu_b.writeto(path2, overwrite=True)\n    with fits.open(path1) as fits1:\n        with fits.open(path2) as fits2:\n            diff = FITSDiff(fits1, fits2, atol=0.1, rtol=0)\n            str1 = diff.report(fileobj=None, indent=0)\n\n            diff = FITSDiff(fits1, fits2, atol=1, rtol=0)\n            str2 = diff.report(fileobj=None, indent=0)\n\n    assert \"...and at 1 more indices.\" in str1\n    assert \"...and at 1 more indices.\" not in str2\n\n", "def test_fitsdiff_ignore_checksums(tmp_path):\n    path1 = tmp_path / \"test1.fits\"\n    path2 = tmp_path / \"test2.fits\"\n\n    hdulist = HDUList([PrimaryHDU(), ImageHDU(data=np.zeros(5))])\n    hdulist.writeto(path1)\n    hdulist.writeto(path2)\n\n    # Modify the CHECKSUM in the second file\n    with fits.open(path2, mode='update') as hdul:\n        hdul[0].header[\"CHECKSUM\"] = \"modified_checksum\"\n\n    diff = FITSDiff(path1, path2, ignore_checksums=True)\n    assert diff.identical\n    assert \"No differences found.\" in diff.report()\n"], "sample_885": ["def test_generate_valid_param_with_seed(constraint):\n    \"\"\"Check that the value generated can be consistently reproduced with a seed.\"\"\"\n    seed = 42\n    np.random.seed(seed)\n", "    def test_validate_params_with_callable_default():\n        \"\"\"Check that validate_params works with functions having callable defaults.\"\"\"\n\n        @validate_params({\"a\": [int], \"b\": [callable, lambda x: x + 1]})\n            pass\n\n        f(1)  # b should use the default callable\n        f(1, b=lambda x: x * 2)  # b can be overridden with a callable\n\n        with pytest.raises(\n            InvalidParameterError, match=\"The 'b' parameter of f must be\"\n        ):\n            f(1, b=1)  # b should not be an int\n", "    def test_validate_params_kwargs_only():\n        \"\"\"Check that validate_params works when only keyword arguments are passed.\"\"\"\n\n        @validate_params({\"param1\": [int], \"param2\": [str]})\n            pass\n        \n        func(param1=1, param2=\"test\")\n        with pytest.raises(\n            InvalidParameterError, match=\"The 'param1' parameter of func must be\"\n        ):\n            func(param2=\"test\")\n", "    def test_multiple_constraints_same_parameter():\n        @validate_params({\"param\": [int, {\"a\": [str], \"b\": [float]}]})\n            pass\n        f(param={\"a\":\"a\", \"b\":1.0})\n        with pytest.raises(InvalidParameterError, match=\"The 'param' parameter\"):\n            f(param={\"a\":1, \"b\":1.0})\n\n", "    def test_scalar_constraint(constraint, value, match):\n        with pytest.raises(InvalidParameterError, match=match):\n            constraint.is_satisfied_by(value)\n\n"], "sample_317": ["    def test_empty_feed(self):\n        \"\"\"\n        Test empty feed content and structure.\n        \"\"\"\n        response = self.client.get('/syndication/empty/')\n        doc = minidom.parseString(response.content)\n        feed = doc.getElementsByTagName('rss')[0]\n        chan = feed.getElementsByTagName('channel')[0]\n        self.assertEqual(len(chan.getElementsByTagName('item')), 0)\n", "    def test_feed_description_empty(self):\n        response = self.client.get('/syndication/empty_description/')\n        doc = minidom.parseString(response.content)\n        feed = doc.getElementsByTagName('rss')[0]\n        chan = feed.getElementsByTagName('channel')[0]\n        self.assertEqual(chan.getElementsByTagName('description')[0].firstChild.wholeText, '')\n", "    def test_atom_feed_with_author_email(self):\n        response = self.client.get('/syndication/atom/author_email/')\n        feed = minidom.parseString(response.content).firstChild\n        entries = feed.getElementsByTagName('entry')\n        for entry in entries:\n            author = entry.getElementsByTagName('author')[0]\n            self.assertEqual(author.getElementsByTagName('email')[0].firstChild.wholeText, 'test@example.com')\n", "    def test_empty_feed(self):\n        \"\"\"\n\n        Test that an empty feed is generated correctly.\n        \"\"\"\n        response = self.client.get('/syndication/empty/')\n        doc = minidom.parseString(response.content)\n        feed = doc.getElementsByTagName('rss')[0] or doc.getElementsByTagName('feed')[0]\n\n        self.assertEqual(len(feed.getElementsByTagName('item')), 0)\n", "    def test_feed_generator_default_feed_format(self):\n        \"\"\"\n        Test that the DefaultFeed class generates a correctly formatted RSS2 feed.\n        \"\"\"\n        response = self.client.get('/syndication/default-feed/')\n        doc = minidom.parseString(response.content)\n        feed = doc.getElementsByTagName('rss')[0]\n\n        self.assertEqual(feed.getAttribute('version'), '2.0')\n"], "sample_592": ["    def test_short_data_repr_large_array_with_dask_array():\n        import dask.array as da\n        array = da.random.random((100, 100), chunks=(10, 10))\n        ds = xr.Dataset({'foo': xr.DataArray(array)})\n        assert len(repr(ds['foo']).splitlines())  < 50\n", "    def test_short_data_repr_dask_array(self):\n        import dask.array as da\n\n        arr = da.random.random((100, 100), chunks=(50, 50))\n        expected = (\n            \"[...] (100, 100) dask.array<random-..., shape=(100, 100), dtype=float64>\"\n        )\n        actual = formatting.short_data_repr(arr)\n        assert actual == expected\n\n", "    def test_summarize_attr_long_string(self):\n        long_string = \"a\" * 100\n        key = \"my_key\"\n        expected = f\"    {key}: {long_string[:80]}...\"\n        actual = formatting.summarize_attr(key, long_string)\n\n        assert actual == expected\n", "    def test_short_data_repr_dask(self):\n        import dask.array as da\n\n        arr = da.random.random((100, 100), chunks=(50, 50))\n        expected = formatting.short_data_repr(arr)\n        assert \"dask.array\" in expected\n        assert len(expected.splitlines()) < 10\n\n", "    def test_short_data_repr_with_dask_array(self):\n        import dask.array as da\n\n        array = da.random.random((100, 100), chunks=(20, 20))\n        result = formatting.short_data_repr(array)\n        assert len(result.splitlines()) < 50\n"], "sample_714": ["    def test_brier_score_loss_multiclass():\n        y_true = np.array([0, 1, 2, 0, 1, 2])\n        y_pred = np.array([[0.2, 0.7, 0.1], [0.1, 0.8, 0.1],\n                          [0.3, 0.1, 0.6], [0.7, 0.2, 0.1],\n                          [0.1, 0.8, 0.1], [0.1, 0.2, 0.7]])\n        true_scores = np.array([linalg.norm(y_true[i] - y_pred[i])**2\n                                for i in range(len(y_true))])\n        true_score = np.mean(true_scores)\n        assert_almost_equal(brier_score_loss(y_true, y_pred),\n                            true_score)\n        \n        y_pred_onehot = np.eye(3)[y_pred.argmax(axis=1)]\n        assert_almost_equal(brier_score_loss(y_true, y_pred_onehot),\n                            true_score)\n\n", "def test_precision_recall_f1_score_empty_labels():\n    y_true = np.array([])\n    y_pred = np.array([])\n\n    for average in [None, 'micro', 'macro', 'samples', 'weighted']:\n        # precision\n        p, _, _, _ = precision_recall_fscore_support(y_true, y_pred,\n                                                    average=average)\n        assert_array_equal(p, np.array([]))\n        # recall\n        _, r, _, _ = precision_recall_fscore_support(y_true, y_pred,\n                                                    average=average)\n        assert_array_equal(r, np.array([]))\n        # f1-score\n        _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n                                                    average=average)\n        assert_array_equal(f, np.array([]))\n        # support\n        _, _, _, s = precision_recall_fscore_support(y_true, y_pred,\n                                                    average=average)\n        assert_array_equal(s, np.array([]))\n\n        # fbeta_score\n        for beta in [0.5, 1, 2]:\n            f = fbeta_score(y_true, y_pred, beta=beta, average=average)\n            assert_array_equal(f, np.array([]))\n\n", "def test_precision_recall_f1_sample_weight():\n    y_true = [0, 1, 1, 0, 1]\n    y_pred = [0, 1, 0, 0, 1]\n    sample_weight = [1, 2, 1, 1, 1]\n\n    p, r, f, s = precision_recall_fscore_support(y_true, y_pred,\n                                                sample_weight=sample_weight)\n    assert_almost_equal(p, [0.5, 0.66666667])\n    assert_almost_equal(r, [0. , 0.66666667])\n    assert_almost_equal(f, [0. , 0.66666667])\n    assert_array_equal(s, [1, 3])\n", "    def test_confusion_matrix_multiclass_consistent_labels(self):\n        # Test that confusion_matrix works correctly when labels are consistent\n        y_true = [0, 1, 2, 0, 1, 2]\n        y_pred = [0, 2, 1, 0, 1, 2]\n        cm = confusion_matrix(y_true, y_pred)\n        expected_cm = np.array([[2, 0, 0],\n                               [0, 1, 1],\n                               [0, 1, 1]])\n        assert_array_equal(cm, expected_cm)\n", "compilation error"], "sample_773": ["def test_logistic_regression_sample_weight():\n    # Test that sample weights are correctly applied.\n\n    X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n    sample_weight = np.random.rand(100)\n    lr = LogisticRegression(random_state=42)\n\n    # Fit with sample weights\n    lr.fit(X, y, sample_weight=sample_weight)\n    preds_weighted = lr.predict(X)\n\n    # Fit without sample weights\n    lr.fit(X, y)\n    preds = lr.predict(X)\n\n    # Check that predictions are different\n    assert not np.array_equal(preds_weighted, preds)\n", "    def test_logistic_regression_predict_proba():\n        # Check predict_proba output shape and values for various cases.\n        X, y = make_classification(n_samples=100, n_features=10, n_classes=3,\n                                   random_state=0)\n\n        for solver in ('lbfgs', 'liblinear', 'newton-cg', 'sag', 'saga'):\n            lr = LogisticRegression(solver=solver, random_state=0)\n            lr.fit(X, y)\n\n            y_proba = lr.predict_proba(X)\n            assert y_proba.shape == (100, 3)\n            assert_allclose(y_proba.sum(axis=1), np.ones(100))\n\n            # Check for binary case\n            y_bin = (y == 0)\n            lr_bin = LogisticRegression(solver=solver, random_state=0)\n            lr_bin.fit(X, y_bin)\n            y_proba_bin = lr_bin.predict_proba(X)\n            assert y_proba_bin.shape == (100, 2)\n            assert_allclose(y_proba_bin.sum(axis=1), np.ones(100))\n\n", "def test_class_weight_balanced(solver):\n    # Check that class_weight='balanced' works correctly with different solvers\n\n    X, y = make_classification(n_samples=1000, n_features=20, n_classes=2,\n                               weights=[0.1, 0.9], random_state=0)\n\n    # Train with class_weight='balanced'\n    lr_balanced = LogisticRegression(solver=solver, class_weight='balanced',\n                                     random_state=0)\n    lr_balanced.fit(X, y)\n\n    # Train without class_weight\n    lr_unweighted = LogisticRegression(solver=solver, random_state=0)\n    lr_unweighted.fit(X, y)\n\n    # Calculate the ratio of coefficients\n    coef_ratio = np.abs(lr_balanced.coef_[0]) / np.abs(lr_unweighted.coef_[0])\n\n    # The coefficients for the minority class should be larger than the\n    # coefficients for the majority class when using class_weight='balanced'\n    assert coef_ratio[0] > coef_ratio[1]\n", "    def test_logistic_regression_fit_intercept_input_validation():\n        X, y = make_classification(random_state=0)\n        lr = LogisticRegression(fit_intercept=True)\n\n        # Test for correct type\n        assert_raises(TypeError, lr.fit, X, y, fit_intercept='something')\n\n        # Test for correct value\n        assert_raises(ValueError, lr.fit, X, y, fit_intercept=2)\n\n        # Test for correct shape when fit_intercept is a list\n        assert_raises(ValueError, lr.fit, X, y, fit_intercept=[True])\n\n\n\n", "    def test_warm_start_with_none_penalty(self, solver):\n        # Test that warm_start works correctly with 'none' penalty\n        X, y = make_classification(n_samples=1000, random_state=0)\n        lr = LogisticRegression(penalty='none', solver=solver, random_state=0,\n                               warm_start=True)\n        lr.fit(X, y)\n        initial_coef = lr.coef_.copy()\n        lr.fit(X, y)\n        assert np.allclose(lr.coef_, initial_coef)\n"], "sample_904": ["compilation error", "def test_glossary_duplicate_terms(app, warning):\n    text = (\".. glossary::\\n\"\n            \"\\n\"\n            \"   term1\\n\"\n            \"       description\\n\"\n            \"   term1\\n\"\n            \"       description\\n\")\n    restructuredtext.parse(app, text)\n    assert \"case.rst:4: WARNING: duplicate term description of term1, other instance in case\" in warning.getvalue()\n\n", "    def test_glossary_unique_terms(app, warning):\n        text = (\".. glossary::\\n\"\n                \"\\n\"\n                \"   term1\\n\"\n                \"   term1\\n\")\n        restructuredtext.parse(app, text)\n        assert \"case1.rst:5: WARNING: duplicate term description of term1, other instance in case1\" in warning.getvalue()\n", "def test_glossary_with_classifier(app):\n    text = (\".. glossary::\\n\"\n            \"\\n\"\n            \"   term1 : classifier\\n\"\n            \"       description\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (\n        [glossary, definition_list, ([definition_list_item, ([term, (\"term1\",\n                                                index)],\n                                                                  definition)],\n    ))\n    assert_node(doctree[0][0][0][0][1],\n                entries=[(\"single\", \"term1\", \"term-term1\", \"main\", \"classifier\")])\n    assert_node(doctree[0][0][0][1],\n                [nodes.definition, nodes.paragraph, \"description\"])\n", "def test_glossary_empty(app):\n    text = \".. glossary::\\n\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (glossary, definition_list))\n"], "sample_779": ["    def test_check_estimator_check_fit_score(self):\n        # check that check_estimator() calls check_fit_score_takes_y\n        # for estimators that have a score method (and don't have a\n        # deprecated fit method)\n        with mock.patch.object(check_estimator_checks, 'check_fit_score_takes_y') \\\n                as mock_check_fit_score:\n            check_estimator(LinearRegression())\n            mock_check_fit_score.assert_called_once()\n\n        with mock.patch.object(check_estimator_checks, 'check_fit_score_takes_y') \\\n                as mock_check_fit_score:\n            check_estimator(AdaBoostClassifier())\n            mock_check_fit_score.assert_called_once()\n", "def test_check_estimator_pandas():\n    # Test case to ensure estimators can handle pandas DataFrames\n    # as input, if they are known to support them.\n    try:\n        from pandas import DataFrame\n        # Create a basic DataFrame\n        df = DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n        # Check that an estimator that supports pandas DataFrames\n        # does not raise an error when fit with it\n        check_estimator(LinearRegression(), )  \n        # Check that an estimator that does not support pandas DataFrames \n        # raises an error when fit with it\n        assert_raises(TypeError, check_estimator(SVC()), ) \n    except ImportError:\n        pass\n", "    def test_check_estimator_input_validation():\n        # Check that estimators raise meaningful errors for invalid input\n        msg = \"Estimator estimator_name does not raise ValueError\"\n        assert_raises_regex(AssertionError, msg, check_estimator,\n                            \"estimator_name\",\n                            NoCheckinPredict)\n", "    def test_check_estimator_warm_start():\n        # check if warm_start is correctly handled\n        from sklearn.linear_model import LogisticRegression\n        from sklearn.exceptions import NotFittedError\n\n        class EstimatorWithWarmStart(LogisticRegression):\n                super().__init__(warm_start=warm_start)\n\n        est = EstimatorWithWarmStart()\n        check_estimator(est)\n\n        # also check if warm_start raises error if we set it to True\n        # and the estimator doesn't support it\n        class EstimatorNoWarmStart(BaseEstimator):\n            pass\n\n        est = EstimatorNoWarmStart()\n        with pytest.raises(ValueError):\n            check_estimator(est, parameters={'warm_start': True})\n\n", "def test_check_estimator_with_deprecation():\n\n    class DeprecatedEstimator(BaseEstimator):\n        @deprecated(\"Deprecated estimator\")\n            return self\n\n    # check that estimators with deprecated methods are handled correctly\n    check_estimator(DeprecatedEstimator())\n"], "sample_232": ["    def test_contains_with_nested_key_transform(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__contains=KeyTransform('baz',\n                                              KeyTransform('a', 'value'))\n            ),\n            [self.objs[7]],\n        )\n", "    def test_lookup_exclude_after_filter(self):\n        qs = NullableJSONModel.objects.filter(value__isnull=False)\n        self.assertSequenceEqual(\n            qs.exclude(value__a='b'),\n            [self.objs[0], self.objs[4], self.objs[5]],\n        )\n\n", "    def test_key_transform_nested_objects(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__baz__a__iexact='B',\n            ),\n            [self.objs[7]],\n        )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__bax__foo__icontains='ar',\n            ),\n            [self.objs[7]],\n        )\n", "    def test_nested_key_transform_with_isnull(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects\n            .filter(value__d__0__isnull=False, value__d__1__f__isnull=True)\n            .annotate(\n                key=KeyTransform('f', KeyTransform('1', 'value')),\n                expr=KeyTransform('f', KeyTransform('1', Cast('value', models.JSONField()))),\n            )\n            .filter(expr__isnull=True),\n            [self.objs[4]],\n        )\n\n", "    def test_key_transform_nested_json(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__d__a=KeyTransform('x', KeyTransform('nested', 'value'))\n            ),\n            [self.objs[4]],\n        )\n\n"], "sample_235": ["    def test_on_commit_after_atomic_block(self):\n        with transaction.atomic():\n            self.do(1)\n\n        transaction.on_commit(lambda: self.notify(2))\n        self.assertDone([1, 2])\n", "    def test_hook_in_atomic_block_with_savepoint(self):\n        with transaction.atomic():\n                self.notify(1)\n\n            transaction.on_commit(inner_hook)\n            with transaction.atomic(savepoint=True):\n                Thing.objects.create(num=1)\n\n        self.assertDone([1]) \n", "    def test_no_hooks_run_from_failed_atomic_block(self):\n        \"\"\"If an atomic block fails due to a rollback, no hooks run.\"\"\"\n        with self.assertRaises(ForcedError):\n            with transaction.atomic():\n                self.do(1)\n                raise ForcedError()\n\n        self.assertDone([])\n", "    def test_multiple_on_commit_calls_in_same_transaction(self):\n        with transaction.atomic():\n            transaction.on_commit(lambda: self.notify(1))\n            transaction.on_commit(lambda: self.notify(2))\n        self.assertDone([1, 2])\n", "    def test_hooks_cleared_on_savepoint_rollback(self):\n        with transaction.atomic():\n            with transaction.atomic(savepoint=True):\n                self.do(1)\n                transaction.savepoint_rollback()\n\n            self.do(2)\n\n        self.assertDone([2])\n\n"], "sample_837": ["def test_get_blas_info(capsys):\n    from sklearn.utils._show_versions import _get_blas_info\n\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n", "def test_get_blas_info(capsys):\n    show_versions()\n    out, err = capsys.readouterr()\n    assert 'macros' in out\n    assert 'lib_dirs' in out\n    assert 'cblas_libs' in out \n", "def test_get_blas_info(capsys):\n    show_versions()\n    out, err = capsys.readouterr()\n    assert 'macros' in out\n    assert 'lib_dirs' in out\n    assert 'cblas_libs' in out\n", "def test_get_blas_info(capsys):\n    show_versions()\n    out, err = capsys.readouterr()\n    assert 'macros' in out\n    assert 'lib_dirs' in out\n    assert 'cblas_libs' in out\n\n", "def test_get_blas_info(capsys):\n    show_versions()\n    out, err = capsys.readouterr()\n    assert 'macros' in out\n    assert 'lib_dirs' in out\n    assert 'cblas_libs' in out\n"], "sample_1129": ["compilation error", "compilation error", "def test_issue_20085():\n    from sympy import LambertW\n    prntr = SciPyPrinter()\n\n    assert prntr.doprint(LambertW(x)) == 'scipy.special.lambertw(x)'\n\n", "compilation error", "compilation error"], "sample_1156": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_613": ["    def test_resample_empty_dataset(self):\n        ds = Dataset({\"foo\": ([], [])})\n        actual = ds.resample(time=\"1D\").mean()\n        expected = Dataset({\"foo\": ([], [])})\n        assert_identical(expected, actual)\n", "    def test_resample_ds_with_empty_group(self):\n        times = pd.date_range(\"2000-01-01\", freq=\"D\", periods=10)\n        ds = xr.Dataset({\"foo\": (\"time\", [1.0] * 10), \"time\": times})\n        ds_empty = ds.isel(time=0)\n\n        actual = xr.concat([ds, ds_empty], dim='time').resample(time='5D').sum()\n        expected = ds.resample(time='5D').sum()\n        assert_identical(expected, actual)\n", "    def test_resample_ds_da_are_the_same_with_datetime_index(self):\n        time = pd.DatetimeIndex(\n            np.arange(\n                pd.Timestamp(\"2000-01-01\").value,\n                pd.Timestamp(\"2001-01-01\").value,\n                np.timedelta64(1, \"h\"),\n            ),\n            freq=\"H\",\n        )\n        ds = xr.Dataset(\n            {\n                \"foo\": ((\"time\", \"x\"), np.random.randn(len(time), 5)),\n                \"time\": time,\n                \"x\": np.arange(5),\n            }\n        )\n        assert_identical(\n            ds.resample(time=\"D\").mean()[\"foo\"], ds.foo.resample(time=\"D\").mean()\n        )\n\n", "    def test_resample_ds_with_nonnumeric_dimension(self):\n        times = pd.date_range(\"2000-01-01\", freq=\"6H\", periods=10)\n        letters = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n        ds = xr.Dataset(\n            {\n                \"foo\": ([\"time\", \"letter\"], np.random.randn(10, 5)),\n                \"bar\": (\"time\", np.random.randn(10)),\n                \"time\": times,\n                \"letter\": letters,\n            }\n        )\n\n        actual = ds.resample(time=\"1D\").mean()\n        expected = ds.groupby(\"time\").mean()\n\n        assert_identical(expected, actual)\n", "    def test_resample_ds_nonstandard_coord(self):\n        times = pd.date_range(\"2000-01-01\", freq=\"6H\", periods=10)\n        ds = Dataset(\n            {\n                \"foo\": ([\"time\", \"x\", \"y\"], np.random.randn(10, 5, 3)),\n                \"bar\": (\"time\", np.random.randn(10), {\"meta\": \"data\"}),\n                \"time\": times,\n                \"other_time\": (\"time\", times),\n            }\n        )\n        actual = ds.resample(time=\"1D\").mean()\n        expected = ds.resample(other_time=\"1D\").mean()\n        assert_identical(actual, expected)\n"], "sample_711": ["def test_repr_failure_tbstyle_fulltrace(pytester: Pytester) -> None:\n    \"\"\"Ensures that repr_failure uses 'fulltrace' when pytest.ini configures it.\"\"\"\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        fulltrace = True\n    \"\"\"\n    )\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.parametrize(\"x\", [1, 2])\n            assert False, f\"Failed with x={x}\"\n\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_failing_test*\",\n            \"*1 failed in *\",\n            \"*---*\",\n            \"*Failed with x=1*\",  # Full traceback should include the assert\n            \"*test_failing_test*\",\n            \"*1 failed in *\",\n            \"*---*\",\n            \"*Failed with x=2*\",\n        ]\n    )\n", "    def test_iter_markers(pytester: Pytester) -> None:\n        p = pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.x\n            @pytest.mark.y\n                pass\n        \"\"\"\n        )\n\n        item = pytester.getitem(p)\n        markers = list(item.iter_markers())\n        assert len(markers) == 2\n        assert markers[0][1].name == \"x\"\n        assert markers[1][1].name == \"y\"\n\n", "    def test_reportinfo(pytester: Pytester) -> None:\n        \"\"\"Ensure reportinfo returns the expected tuple.\"\"\"\n        items = pytester.getitems(\n            \"\"\"\n                pass\n        \"\"\"\n        )\n        item = items[0]\n        expected_path = pytester.path / \"test_file.py\"\n        assert item.reportinfo() == (expected_path, None, \"\")\n", "    def test_getitem_on_node(pytester: Pytester) -> None:\n        items = pytester.getitems(\n            \"\"\"\n                pass\n                pass\n        \"\"\"\n        )\n        assert items[0].name == \"test_a\"\n        assert items[1].name == \"test_b\"\n        with pytest.raises(IndexError):\n            items[2]\n", "def test_node_repr_failure_traceback_filtering(pytester: Pytester) -> None:\n    \"\"\"Ensure repr_failure respects style= argument.\"\"\"\n    p = pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            assert False\n    \"\"\"\n    )\n    result = pytester.runpytest('-s', \"--tb=short\")\n    result.stdout.fnmatch_lines([\"*assert False*\", \"*1 failed in*\"])\n\n"], "sample_1114": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_437": ["    def test_health_checks_disabled_autocommit_changed(self):\n        self.patch_settings_dict(conn_health_checks=False)\n        self.assertIsNone(connection.connection)\n        # Newly created connections are considered healthy without performing\n        # the health check.\n        with patch.object(connection, \"is_usable\", side_effect=AssertionError):\n            # Simulate outermost atomic block: changing autocommit for\n            # a connection.\n            connection.set_autocommit(False)\n            self.run_query()\n            connection.commit()\n            connection.set_autocommit(True)\n\n        old_connection = connection.connection\n        # Simulate request_finished.\n        connection.close_if_unusable_or_obsolete()\n        # Persistent connections are enabled.\n        self.assertIs(old_connection, connection.connection)\n\n        # Simulate outermost atomic block: changing autocommit for\n        # a connection.\n        connection.set_autocommit(False)\n        self.run_query()\n        connection.commit()\n        connection.set_autocommit(True)\n        # The connection is unchanged after the next query either during\n        # the current \"request\".\n        self.assertIs(old_connection, connection.connection)\n\n", "    def test_connection_health_checks_disabled_errors_occurred(self):\n        self.patch_settings_dict(conn_health_checks=False)\n        self.assertIsNone(connection.connection)\n        # Newly created connections are considered healthy without performing\n        # the health check.\n        with patch.object(connection, \"is_usable\", side_effect=AssertionError):\n            self.run_query()\n\n        old_connection = connection.connection\n        # Simulate errors_occurred.\n        connection.errors_occurred = True\n        # Simulate request_started\n        connection.close_if_unusable_or_obsolete()\n        # Persistent connections are enabled.\n        self.assertIs(old_connection, connection.connection)\n        # No additional health checks after the one in\n        # close_if_unusable_or_obsolete() are executed during this \"request\"\n        # when running queries.\n        with patch.object(connection, \"is_usable\", side_effect=AssertionError):\n            self.run_query()\n            # Health check wasn't performed and the connection is unchanged.\n            self.assertIs(old_connection, connection.connection)\n            self.run_query()\n            # The connection is unchanged after the next query either during\n            # the current \"request\".\n            self.assertIs(old_connection, connection.connection)\n", "    def test_atomic_block_context_manager_rollback(self):\n        with connection.atomic():\n            Person.objects.create(first_name=\"first\", last_name=\"last\")\n            with self.assertRaises(Exception):\n                raise Exception(\"Force rollback\")\n        self.assertEqual(Person.objects.count(), 0)\n", "    def test_health_checks_disabled_close_if_unusable_or_obsolete(self):\n        self.patch_settings_dict(conn_health_checks=False)\n        self.assertIsNone(connection.connection)\n        with patch.object(connection, \"is_usable\", return_value=False):\n            self.run_query()\n        old_connection = connection.connection\n        connection.close_if_unusable_or_obsolete()\n        self.assertIs(old_connection, connection.connection)\n\n", "    def test_multi_database_cursor_context(self):\n        with self.subTest(database=\"default\"):\n            with connection.cursor() as cursor:\n                self.assertIsInstance(cursor, DatabaseCursor)\n        with self.subTest(database=\"other\"):\n            with connections[\"other\"].cursor() as cursor:\n                self.assertIsInstance(cursor, DatabaseCursor)\n"], "sample_354": ["    def test_create_permissions_with_no_permission_model(self):\n        \"\"\"\n        Test create_permissions when the 'auth.Permission' model doesn't exist in\n        the database.\n        \"\"\"\n        with patch.object(Permission, 'objects', return_value=Mock(\n                create=lambda **kwargs: None\n        )):\n            with self.assertNumQueries(0):\n                create_permissions(self.app_config, verbosity=0)\n", "    def test_createsuperuser_with_custom_user_model_and_custom_perm(self):\n        class CustomUser(User):\n            pass\n        CustomUser._meta.permissions = (\n            ('can_do_stuff', 'Can do stuff'),\n        )\n        \n        with override_settings(AUTH_USER_MODEL='auth_tests.CustomUser'):\n            new_io = StringIO()\n            @mock_inputs({'password': 'nopasswd', 'username': 'joe', 'email': ''})\n                call_command(\n                    'createsuperuser',\n                    interactive=True,\n                    stdin=MockTTY(),\n                    verbosity=0,\n                )\n\n                user = CustomUser.objects.get(username='joe')\n                self.assertTrue(user.has_perm('auth_tests.can_do_stuff'))\n            test(self)\n", "    def test_permission_creation_with_custom_model(self):\n        class CustomModel(models.Model):\n            name = models.CharField(max_length=100)\n\n        class CustomAppConfig(AppConfig):\n            name = 'custom_app'\n            verbose_name = 'Custom App'\n\n                self.model_classes = [CustomModel]\n\n        custom_app_config = CustomAppConfig()\n        custom_app_config.name = 'custom_app'\n        custom_app_config.label = 'custom_app'\n\n        Permission._meta.permissions = []\n        create_permissions(custom_app_config, verbosity=0)\n\n        content_type = ContentType.objects.get_by_natural_key('custom_app', 'custommodel')\n        self.assertEqual(Permission.objects.filter(content_type=content_type).count(), 7)\n", "    def test_createsuperuser_command_with_email_field_options(self):\n        new_io = StringIO()\n        call_command(\n            'createsuperuser',\n            interactive=False,\n            username='joe',\n            email='joe@somewhere.org',\n            email_field='different_email',\n            stdout=new_io,\n        )\n        command_output = new_io.getvalue().strip()\n        self.assertEqual(command_output, 'Superuser created successfully.')\n        user = User.objects.get(username='joe')\n        self.assertEqual(getattr(user, 'different_email'), 'joe@somewhere.org')\n", "    def test_createsuperuser_command_database_option_invalid_database(self):\n        new_io = StringIO()\n        with self.assertRaisesMessage(CommandError, 'Invalid database \"invalid_database\".'):\n            call_command(\n                'createsuperuser',\n                interactive=False,\n                username='joe',\n                email='joe@somewhere.org',\n                database='invalid_database',\n                stdout=new_io,\n            )\n"], "sample_1014": ["def test_free_symbols():\n    from sympy.abc import x, y, z\n    md = ImmutableDenseNDimArray([[x, y], [z, 1]])\n    assert md.free_symbols == {x, y, z}\n\n\n\n    sd = ImmutableSparseNDimArray([[x, y], [z, 1]])\n    assert sd.free_symbols == {x, y, z}\n\n", "def test_zeros_like():\n\n    # Test zeros_like for different array types\n    a = ImmutableDenseNDimArray([1, 2, 3], (3,))\n    b = ImmutableSparseNDimArray([1, 2, 3], (3,))\n    c = ImmutableDenseNDimArray([[1, 2, 3], [4, 5, 6]], (2, 3))\n\n    assert ImmutableDenseNDimArray.zeros_like(a) == ImmutableDenseNDimArray([0, 0, 0], (3,))\n    assert ImmutableSparseNDimArray.zeros_like(b) == ImmutableSparseNDimArray([0, 0, 0], (3,))\n\n    assert ImmutableDenseNDimArray.zeros_like(c) == ImmutableDenseNDimArray([[0, 0, 0], [0, 0, 0]], (2, 3))\n    assert ImmutableSparseNDimArray.zeros_like(c) == ImmutableSparseNDimArray([[0, 0, 0], [0, 0, 0]], (2, 3))\n", "def test_mutable_array_modifying():\n\n    mutable_array = MutableDenseNDimArray([1, 2, 3], (3,))\n\n    mutable_array[0] = 4\n    assert mutable_array[0] == 4\n    assert mutable_array.tolist() == [4, 2, 3]\n\n    mutable_array[1:3] = [5, 6]\n    assert mutable_array.tolist() == [4, 5, 6]\n\n    mutable_array[:] = [7, 8, 9]\n    assert mutable_array.tolist() == [7, 8, 9]\n\n    mutable_array[0] = 10\n\n    # Test setting a value with a symbolic expression\n    from sympy.abc import x\n    mutable_array[1] = x\n    assert mutable_array.tolist() == [10, x, 9]\n\n    mutable_array = MutableDenseNDimArray([[1, 2, 3], [4, 5, 6]], (2, 3))\n    mutable_array[0, 0] = 7\n    assert mutable_array[0, 0] == 7\n    assert mutable_array.tolist() == [[7, 2, 3], [4, 5, 6]]\n\n    mutable_array[1, :] = [7, 8, 9]\n    assert mutable_array.tolist() == [[7, 2, 3], [7, 8, 9]]\n\n", "def test_zero_dim_array():\n    z = ImmutableDenseNDimArray(5)\n    assert z.shape == ()\n    assert z.rank() == 0\n    assert z[()] == 5\n    assert z.tolist() == [5]\n\n\n\n", "    def test_mutable_array_getitem():\n        mutable_array = MutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n        assert mutable_array[0, 0] == 1\n        assert mutable_array[0, 1] == 2\n        assert mutable_array[1, 0] == 3\n        assert mutable_array[1, 1] == 4\n"], "sample_988": ["compilation error", "compilation error", "def test_issue_14903():\n    a = symbols('a', positive=True)\n    b = symbols('b', positive=True)\n    assert Relational(a, b, '==').is_commutative\n    assert Relational(a, b, '!=').is_commutative\n\n\n", "compilation error", "compilation error"], "sample_367": ["    def test_cache_control_decorator_multiple_arguments(self):\n        @cache_control(max_age=120, no_cache=True, must_revalidate=True)\n            return HttpResponse()\n        response = a_view(HttpRequest())\n        self.assertEqual(response.headers['Cache-Control'], 'max-age=120, no-cache, must-revalidate') \n\n", "    def test_never_cache_with_cache_page(self):\n        @cache_page(120)\n        @never_cache\n            return HttpResponse()\n\n        response = a_view(HttpRequest())\n        self.assertIn('Cache-Control', response.headers)\n        self.assertIn('Expires', response.headers)\n        self.assertEqual(response.headers['Cache-Control'], 'max-age=0, no-cache, no-store, must-revalidate, private')\n", "    def test_cache_control_decorator_with_multiple_arguments(self):\n        @cache_control(max_age=3600, s_maxage=7200, private=True)\n            return HttpResponse()\n\n        response = a_view(HttpRequest())\n        self.assertEqual(response.headers['Cache-Control'],\n                         'max-age=3600, s-maxage=7200, private')\n", "    def test_cache_page_decorator_vary(self):\n            return \"response\"\n        my_view_cached = cache_page(123, vary_on_cookie='test')(my_view)\n        response1 = my_view_cached(HttpRequest())\n        response2 = my_view_cached(HttpRequest(COOKIES={'test': '1'}))\n        self.assertEqual(response1, response2)\n\n", "    def test_cache_control_decorator_multiple_arguments(self):\n        @cache_control(max_age=3600, no_cache=True)\n            return HttpResponse()\n\n        response = a_view(HttpRequest())\n        self.assertEqual(response.headers['Cache-Control'], 'max-age=3600, no-cache')\n"], "sample_462": ["    def test_choicefield_empty_values(self):\n        f = ChoiceField(choices=[(\"J\", \"John\"), (\"P\", \"Paul\")], empty_values=[\"\", None])\n        self.assertEqual(\"\", f.clean(\"\"))\n        self.assertEqual(\"\", f.clean(None))\n", "    def test_choicefield_empty_values(self):\n        f = ChoiceField(choices=[(\"1\", \"One\"), (\"2\", \"Two\")], empty_values=['', None])\n        self.assertEqual(f.clean(\"\"), \"\")\n        self.assertEqual(f.clean(None), \"\")\n", "    def test_choicefield_empty_values_default(self):\n        f = ChoiceField(choices=[(\"1\", \"One\"), (\"2\", \"Two\")])\n        self.assertEqual(f.empty_values, [None, '', ''])\n", "    def test_choicefield_required_false_empty_value(self):\n        f = ChoiceField(choices=[(\"J\", \"John\"), (\"P\", \"Paul\")], required=False)\n        self.assertEqual(f.clean(\"\"), \"\")\n        self.assertEqual(f.clean(None), \"\")\n", "    def test_choicefield_with_empty_string_in_choices(self):\n        f = ChoiceField(choices=[(\"J\", \"John\"), (\"\", \"Empty\"), (\"P\", \"Paul\")])\n        self.assertEqual(\"\", f.clean(\"\"))\n        self.assertEqual(\"J\", f.clean(\"J\"))\n        self.assertEqual(\"\", f.clean(\"\"))\n        self.assertEqual(\"P\", f.clean(\"P\"))\n\n\n\n"], "sample_1008": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_721": ["compilation error", "    def test_check_array_sparse_array_like():\n        X_csr = sp.csr_matrix([[1, 2], [3, 4]])\n        X_csc = sp.csc_matrix([[1, 2], [3, 4]])\n        X_dok = sp.dok_matrix([[1, 2], [3, 4]])\n        X_coo = sp.coo_matrix([[1, 2], [3, 4]])\n\n        # Test accept_sparse with array-like input\n        assert_array_equal(check_array(X_csr, accept_sparse=['csr']),\n                           X_csr.toarray())\n        assert_array_equal(check_array(X_csc, accept_sparse=['csc']),\n                           X_csc.toarray())\n        assert_array_equal(check_array(X_dok, accept_sparse=['dok']),\n                           X_dok.toarray())\n        assert_array_equal(check_array(X_coo, accept_sparse=['coo']),\n                           X_coo.toarray())\n\n        # Test that non-matching accept_sparse raises ValueError\n        assert_raises(ValueError, check_array, X_csr,\n                      accept_sparse=['csc'])\n        assert_raises(ValueError, check_array, X_csc,\n                      accept_sparse=['csr'])\n", "def test_check_array_dtype_object_raise_error():\n    X_object = np.array([[1, 2], [3, 'a']], dtype=object)\n    assert_raises(ValueError, check_array, X_object)\n", "compilation error", "def test_check_array_dtype_object_raise_error():\n    X = np.array([[1, 2, 3], [4, 5, 'a'], [7, 8, 9]], dtype=object)\n    assert_raises(ValueError, check_array, X)\n"], "sample_769": ["compilation error", "def test_balanced_accuracy_score_multilabel():\n    y_true = np.array([[1, 0], [0, 1], [1, 1], [0, 0]])\n    y_pred = np.array([[0, 1], [1, 1], [1, 0], [0, 1]])\n    score = balanced_accuracy_score(y_true, y_pred)\n    assert_almost_equal(score, 0.5)\n", "def test_balanced_accuracy_score_empty():\n    assert balanced_accuracy_score([], []) == 0.0\n    assert balanced_accuracy_score([0], []) == 0.0\n    assert balanced_accuracy_score([], [0]) == 0.0\n\n\n", "def test_brier_score_loss_multiclass():\n    y_true = np.array([0, 1, 2, 0, 1, 2])\n    y_pred = np.array([[0.2, 0.7, 0.1], [0.1, 0.8, 0.1], [0.1, 0.1, 0.8],\n                       [0.3, 0.6, 0.1], [0.1, 0.8, 0.1], [0.1, 0.2, 0.7]])\n    true_score = np.mean(np.sum((y_pred - np.eye(3)[y_true])**2, axis=1))\n\n", "compilation error"], "sample_703": ["def test_precedence() -> None:\n    matcher = {\"a\": True, \"b\": False}.__getitem__\n\n    assert evaluate(\"a and b or a\", matcher)\n    assert not evaluate(\"a or b and a\", matcher)\n    assert evaluate(\"(a or b) and a\", matcher)\n    assert evaluate(\"a or (b and a)\", matcher)\n", "def test_dollar_prefix(expr: str, expected: bool) -> None:\n    matcher = {\"a\": True, \"b\": False, \"c\": True}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_match_expression_with_special_characters() -> None:\n    matcher = {\"test$1\": True}.__getitem__\n    assert evaluate(\"test$1\", matcher)\n    assert not evaluate(\"test1\", matcher)\n\n\n\n", "    def test_escape_sequence_in_ident(self) -> None:\n        matcher = {\"\\\\foo\": True}\n        assert evaluate(r\"\\foo\", matcher)\n", "def test_complex_expression() -> None:\n    matcher = {\n        \"a\": True,\n        \"b\": False,\n        \"c\": True,\n        \"d\": False,\n    }.__getitem__\n    assert evaluate(\"a and (b or not c) and d\", matcher) is False\n"], "sample_338": ["    def test_squashed_migration_suggest_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[]),\n            ]\n            squashed = True\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), \n                         'auto_squashed_0001_initial')\n\n", "    def test_squashed_migrations_operations_order(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[]),\n                migrations.DeleteModel('Animal'),\n            ]\n\n        migration1 = Migration('0001_initial', 'test_app')\n        migration2 = Migration('0002_delete_animal', 'test_app')\n        migration2.squashed_migrations = [migration1]\n\n        self.assertEqual(migration2.suggest_name(), 'person_delete_animal')\n", "    def test_operation_with_empty_name(self):\n        class Migration(migrations.Migration):\n            operations = [migrations.RunPython(None, reverse_code=None)]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertIs(migration.suggest_name().startswith('auto_'), True)\n", "    def test_rename_model(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.RenameModel('OldName', 'NewName'),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'rename_oldname_to_newname')\n\n", "    def test_field_rename_suggest_name(self):\n        class Migration(migrations.Migration):\n            operations = [migrations.RenameField('Person', 'name', 'full_name')]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'rename_person_field_name_to_full_name')\n\n"], "sample_713": ["    def test_ridge_cv_with_n_jobs(self):\n        X, y = make_regression(n_samples=100, n_features=10,\n                               n_informative=5, random_state=0)\n        ridge = RidgeCV(alphas=np.arange(0, 1, .1), cv=5, n_jobs=2)\n        ridge.fit(X, y)\n        assert ridge.best_score_ is not None\n", "def test_negative_alpha():\n    # Test if Ridge raises ValueError for negative alpha values\n    reg = Ridge(alpha=-1)\n    X = np.random.randn(10, 5)\n    y = np.random.randn(10)\n    with pytest.raises(ValueError):\n        reg.fit(X, y)\n", "    def test_ridge_cv_warm_start(self):\n        n_samples, n_features = 50, 10\n        X = np.random.randn(n_samples, n_features)\n        y = np.random.randn(n_samples)\n\n        alphas = [1e-3, 1e-2, 1e-1, 1]\n\n        # Fit RidgeCV without warm_start\n        ridgecv = RidgeCV(alphas=alphas, cv=5)\n        ridgecv.fit(X, y)\n\n        # Fit RidgeCV with warm_start\n        ridgecv_warm_start = RidgeCV(alphas=alphas, cv=5, warm_start=True)\n        ridgecv_warm_start.fit(X, y)\n\n        # Assert that the results are the same\n        assert_array_almost_equal(ridgecv.coef_, ridgecv_warm_start.coef_)\n        assert_array_almost_equal(ridgecv.alpha_, ridgecv_warm_start.alpha_)\n", "    def test_ridge_warm_start():\n        rng = np.random.RandomState(42)\n        X = rng.randn(10, 5)\n        y = rng.randn(10)\n        alphas = [1., 0.1, 0.01]\n\n        # Test warm_start with Ridge\n\n        ridge = Ridge(warm_start=True)\n        for alpha in alphas:\n            ridge.alpha = alpha\n            ridge.fit(X, y)\n\n            # Check if the coefficient vector is updated correctly\n\n        # Test warm_start with RidgeCV\n        ridge_cv = RidgeCV(alphas=alphas, warm_start=True)\n        ridge_cv.fit(X, y)\n\n", "    def test_ridge_classifier_empty_classes():\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([0, 1, 0])\n        clf = RidgeClassifier(class_weight='balanced')\n\n        # Ensure fitting works without raising errors due to empty classes\n        clf.fit(X, y)\n        predictions = clf.predict(X)\n        assert len(predictions) == len(X)\n"], "sample_349": ["    def test_empty_choices(self):\n        form = AlbumForm(initial={'band': None})\n        output = form.as_table()\n        self.assertIn(self.empty_option, output)\n", "    def test_media_without_debug(self):\n        with override_settings(DEBUG=False):\n            rel = Album._meta.get_field('band').remote_field\n            self.assertEqual(AutocompleteSelect(rel, admin.site).media._js, [\n                'admin/js/vendor/jquery/jquery.js',\n                'admin/js/vendor/select2/select2.full.min.js',\n                'admin/js/vendor/select2/i18n/en.js',\n                'admin/js/jquery.init.js',\n                'admin/js/autocomplete.js',\n            ])\n", "    def test_media_debug(self):\n        rel = Album._meta.get_field('band').remote_field\n        with self.settings(DEBUG=True):\n            expected_files = (\n                'admin/js/vendor/jquery/jquery.js',\n                'admin/js/vendor/select2/select2.full.js',\n                'admin/js/vendor/select2/i18n/en.js',  # Language file is inserted here.\n                'admin/js/jquery.init.js',\n                'admin/js/autocomplete.js',\n            )\n            self.assertEqual(AutocompleteSelect(rel, admin.site).media._js, list(expected_files))\n", "    def test_media_debug(self):\n        with self.settings(DEBUG=True):\n            rel = Album._meta.get_field('band').remote_field\n            self.assertEqual(AutocompleteSelect(rel, admin.site).media._js[0], 'admin/js/vendor/jquery/jquery.js')\n            self.assertEqual(AutocompleteSelect(rel, admin.site).media._js[1], 'admin/js/vendor/select2/select2.full.js')\n\n", "    def test_url_with_extra_args(self):\n        rel = Album._meta.get_field('band')\n        w = AutocompleteSelect(rel, admin.site, url_name='admin:custom_autocomplete')\n        url = w.get_url()\n        self.assertEqual(url, '/custom_autocomplete/')\n"], "sample_190": ["    def test_isnull_with_subquery(self):\n        author = Author.objects.create(name='Author 1')\n        article = Article.objects.create(headline='Article 1', author=author)\n        article2 = Article.objects.create(headline='Article 2')\n\n        self.assertQuerysetEqual(\n            Article.objects.annotate(author_isnull=Exists(Author.objects.filter(articles__isnull=True).values('pk'))).filter(author_isnull=True),\n            [article2],\n        )\n\n", "    def test_isnull_with_none_rhs(self):\n        season = Season.objects.create(year=2012, nulled_text_field='not null')\n        self.assertFalse(Season.objects.filter(nulled_text_field__isnull=None))\n\n", "    def test_isnull_with_q_object(self):\n        author = Author.objects.create(name='New Author')\n        qs = Author.objects.filter(Q(alias__isnull=True) | Q(pk=author.pk))\n        self.assertCountEqual(qs, Author.objects.all())\n", "    def test_isnull_with_empty_queryset(self):\n        self.assertFalse(\n            Author.objects.filter(\n                id__in=Article.objects.filter(author__isnull=True).values('author_id')\n            ).exists()\n        )\n\n", "    def test_isnull_with_subquery(self):\n        article = Article.objects.create(headline='Test Headline', author=self.au1)\n        qs = Article.objects.filter(author__isnull=False, author__in=\n                                     Author.objects.filter(name=OuterRef('author__name')))\n        self.assertEqual(qs.get(), article)\n"], "sample_1001": ["compilation error", "def test_issue_15122():\n    from sympy.parsing.sympy_parser import parse_expr\n    a = Symbol('a')\n    b = Symbol('b')\n    expr = parse_expr('a*b/(1+a*b)'.replace('*', '\u00b7'))\n    assert latex(expr) == r\"\\frac{a \\cdot b}{a b + 1}\"\n", "compilation error", "compilation error", "def test_latex_printing_of_Matrix_with_indexed_symbol():\n    A = Matrix([[x[0], x[1]], [x[2], x[3]]])\n    assert latex(A) == r'\\begin{bmatrix} x_{0} & x_{1} \\\\ x_{2} & x_{3} \\end{bmatrix}'\n"], "sample_582": ["    def test_cli_blueprint_overwrite(app):\n        bp = Blueprint(\"blue\", __name__, cli_group=\"blue\")\n        bp2 = Blueprint(\"blue\", __name__)\n\n        @bp.cli.command(\"command\")\n            click.echo(\"bp_result\")\n\n        @bp2.cli.command(\"command\")\n            click.echo(\"bp2_result\")\n\n        app.register_blueprint(bp)\n        app.register_blueprint(bp2)\n\n        result = app.test_cli_runner().invoke(args=[\"blue\", \"command\"])\n        assert \"bp2_result\" in result.output\n", "    def test_blueprint_cli_group(self, app, blueprint_cli_group, expected_group):\n        bp = Blueprint(\"bp\", __name__, cli_group=blueprint_cli_group)\n\n        @bp.cli.command(\"cmd\")\n            click.echo(\"bp_result\")\n\n        app.register_blueprint(bp)\n\n        result = app.test_cli_runner().invoke(args=[expected_group, \"cmd\"])\n        assert result.exit_code == 0\n        assert \"bp_result\" in result.output\n", "def test_cli_blueprint_conflicts():\n    \"\"\"Test blueprint commands with conflicting names raise an error.\"\"\"\n    app = Flask(__name__)\n    bp1 = Blueprint(\"bp1\", __name__)\n    bp2 = Blueprint(\"bp2\", __name__)\n\n    @bp1.cli.command(\"conflict\")\n        pass\n\n    @bp2.cli.command(\"conflict\")\n        pass\n\n    app.register_blueprint(bp1)\n    with pytest.raises(ValueError) as excinfo:\n        app.register_blueprint(bp2)\n\n    assert \"Command 'conflict' already registered by blueprint 'bp1'\" in str(excinfo.value)\n", "    def test_cli_blueprint_name_clash(app):\n        \"\"\"Test blueprint commands with the same name resolve correctly.\"\"\"\n        bp1 = Blueprint(\"same_name\", __name__, cli_group=\"same_name\")\n        bp2 = Blueprint(\"same_name\", __name__, cli_group=\"same_name\")\n\n        @bp1.cli.command(\"clash\")\n            click.echo(\"bp1_result\")\n\n        @bp2.cli.command(\"clash\")\n            click.echo(\"bp2_result\")\n\n        app.register_blueprint(bp1)\n        app.register_blueprint(bp2)\n\n        result = app.test_cli_runner().invoke(args=[\"same_name\", \"clash\"])\n        assert result.exit_code == 0\n        assert \"bp1_result\" in result.output\n\n", "def test_cli_duplicate_command(app):\n    \"\"\"Test that registering the same command name in different blueprints raises an error.\"\"\"\n    bp1 = Blueprint(\"bp1\", __name__)\n    bp2 = Blueprint(\"bp2\", __name__)\n\n    @bp1.cli.command(\"duplicate\")\n        pass\n\n    @bp2.cli.command(\"duplicate\")\n        pass\n\n    app.register_blueprint(bp1)\n    with pytest.raises(ValueError) as excinfo:\n        app.register_blueprint(bp2)\n\n    assert \"duplicate command 'duplicate' is already registered\" in str(excinfo.value)\n"], "sample_617": ["compilation error", "compilation error", "    def test_cross_keepdims(self, keepdims):\n        a = xr.DataArray([\n            [1, 0, 0],\n            [0, 1, 0],\n        ], dims=['x', 'cartesian'])\n        b = xr.DataArray([\n            [0, 1, 0],\n            [0, 0, 1],\n        ], dims=['x', 'cartesian'])\n        expected = np.cross(a, b, axis =1)\n        if not keepdims:\n            expected = np.squeeze(expected, axis = 1)\n\n        actual = xr.cross(a, b, dim='cartesian', keepdims= keepdims)\n        xr.testing.assert_duckarray_allclose(expected, actual)\n\n", "compilation error", "def test_cross_ndim(use_dask: bool) -> None:\n    # test for ndim > 1\n    a = np.arange(24).reshape(2, 3, 4)\n    b = np.arange(24).reshape(2, 3, 4) + 1\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"test for dask.\")\n        a = xr.DataArray(a, dims=[\"x\", \"y\", \"z\"]).chunk()\n        b = xr.DataArray(b, dims=[\"x\", \"y\", \"z\"]).chunk()\n\n    with pytest.raises(ValueError):\n        xr.cross(a, b, dim=\"y\")\n"], "sample_1062": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_896": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_433": ["    def test_migration_suggest_name_with_custom_operation(self):\n        class CustomOperation(migrations.Operation):\n                super().__init__()\n                self.name = name\n\n                return self.name\n\n        class Migration(migrations.Migration):\n            operations = [CustomOperation(\"MyCustomOperation\")]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"mycustomoperation\")\n", "    def test_alter_field_and_unique_together(self):\n        initial_author = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"age\", models.IntegerField(db_index=True)),\n            ],\n            {\n                \"unique_together\": {(\"name\", \"age\")},\n            },\n        )\n        author_reversed_constraints = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200, unique=True)),\n                (\"age\", models.IntegerField()),\n            ],\n            {\n                \"unique_together\": {(\"name\",)},\n            },\n        )\n        changes = self.get_changes([initial_author], [author_reversed_constraints])\n\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"testapp\",\n            0,\n            [\n                \"AlterField\",\n                \"AlterField\",\n                \"AlterUniqueTogether\",\n            ],\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            2,\n            name=\"author\",\n            unique_together={(\"name\",)},\n        )\n", "    def test_alter_index_together_add_field(self):\n        initial_author = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n            ],\n            {\n                \"index_together\": {(\"name\",)},\n            },\n        )\n        author_new_constraints = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"age\", models.IntegerField()),\n            ],\n            {\n                \"index_together\": {(\"name\", \"age\")},\n            },\n        )\n        changes = self.get_changes([initial_author], [author_new_constraints])\n\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"testapp\",\n            0,\n            [\"AddField\", \"AlterIndexTogether\"],\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            1,\n            name=\"author\",\n            index_together={(\"name\", \"age\")},\n        )\n", "    def test_custom_operation_with_name(self):\n        class CustomOperation(migrations.Operation):\n                super().__init__()\n                self.name = name\n        class Migration(migrations.Migration):\n            operations = [CustomOperation(\"my_custom_operation\")]\n\n        migration = Migration(\"0001_initial\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"my_custom_operation\")\n", "    def test_rename_field_with_existing_model(self):\n        initial_author = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"pen_name\", models.CharField(max_length=200)),\n            ],\n        )\n        author_renamed_field = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"aka\", models.CharField(max_length=200)),\n            ],\n        )\n        changes = self.get_changes([initial_author], [author_renamed_field])\n\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"testapp\",\n            0,\n            [\"RenameField\"],\n        )\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 0, model_name=\"author\", old_name=\"pen_name\", new_name=\"aka\"\n        )\n"], "sample_550": ["    def test_rubberband_draw():\n        fig, ax = plt.subplots()\n        ax.plot([0, 1], [0, 1])\n        rb = RubberbandBase(ax)\n        fig.canvas.draw()\n        draw_event = MouseEvent(\"draw_event\", fig.canvas, 10, 10)\n        rb.on_draw(draw_event)\n        assert rb.get_xy() == (10, 10)\n\n", "def test_rubberband_draw():\n    fig, ax = plt.subplots()\n\n    # Set up a simple plot\n    ax.plot([0, 1], [0, 1])\n\n    # Get the rubberband tool\n    rubberband = fig.canvas.manager.toolmanager.get_tool('rubberband')\n\n    rubberband.set_active(True)\n    MouseEvent(\"button_press_event\", fig.canvas, 50, 50, MouseButton.LEFT)._process()\n    MouseEvent(\"motion_notify_event\", fig.canvas, 100, 100, MouseButton.LEFT)._process()\n    assert ax.patches\n\n    MouseEvent(\"button_release_event\", fig.canvas, 100, 100, MouseButton.LEFT)._process()\n    assert not ax.patches\n\n\n\n", "    def test_toolmanager_switch_to_tool_by_name(self):\n        with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n            plt.rcParams['toolbar'] = 'toolmanager'\n        fig = plt.gcf()\n        tb = fig.canvas.manager.toolbar\n        assert tb.mode == ''\n        fig.canvas.manager.toolmanager.switch_to_tool_by_name('pan')\n        assert tb.mode == 'PAN'\n        assert tb._active == 'PAN'\n        fig.canvas.manager.toolmanager.switch_to_tool_by_name('zoom')\n        assert tb.mode == 'ZOOM'\n        assert tb._active == 'ZOOM'\n        with pytest.raises(KeyError, match=\"Tool 'foo' not found.\"):\n            fig.canvas.manager.toolmanager.switch_to_tool_by_name('foo')\n", "    def test_toolmanager_reset_keymap():\n        with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n            plt.rcParams['toolbar'] = 'toolmanager'\n        fig = plt.gcf()\n        fig.canvas.manager.toolmanager.update_keymap('forward', 'c')\n        fig.canvas.manager.toolmanager.reset_keymap()\n        assert fig.canvas.manager.toolmanager.get_tool_keymap('forward') == ['v']\n\n", "    def test_draw_artist_zorder():\n        fig, ax = plt.subplots()\n        ax.plot([0, 1], [0, 1], zorder=1)\n        ax.plot([0, 1], [0, 1], zorder=2)\n        fig.canvas.draw()\n        assert ax.lines[0].zorder == 1\n        assert ax.lines[1].zorder == 2\n"], "sample_895": ["    def test_remainder_set_output(transformers, remainder, exp_output):\n        pd = pytest.importorskip(\"pandas\")\n        df = pd.DataFrame([[1, 2, 3]], columns=[\"a\", \"b\", \"c\"])\n        ct = ColumnTransformer(\n            transformers, remainder=remainder, verbose_feature_names_out=False\n        )\n        ct.set_output(transform=\"pandas\")\n        \n        X_trans = ct.fit_transform(df)\n        assert_array_equal(X_trans.columns, exp_output)\n", "    def test_column_transform_set_output_mixed_dtypes(self):\n        pd = pytest.importorskip(\"pandas\")\n        df = pd.DataFrame(\n            {\n                'A': pd.Series([1, 2, 3], dtype='Int64'),\n                'B': pd.Series([1.2, 2.3, 3.4], dtype='float32'),\n                'C': pd.Series(['a', 'b', 'c'], dtype='string'),\n            }\n        )\n\n        ct = ColumnTransformer(\n            transformers=[\n                ('transformerA', 'passthrough', ['A']),\n                ('transformerB', StandardScaler(), ['B']),\n            ],\n            remainder='passthrough',\n            verbose_feature_names_out=False,\n        ).set_output(transform=\"pandas\") \n\n        df_transformed = ct.fit_transform(df)\n\n        assert isinstance(df_transformed, pd.DataFrame)\n        assert df_transformed.dtypes[\"transformerA__A\"] == \"Int64\"\n        assert df_transformed.dtypes[\"transformerB__B\"] == \"float64\" \n        assert df_transformed.dtypes[\"C\"] == \"string\"\n", "compilation error", "    def test_remainder_transformer_output_type(self):\n        pd = pytest.importorskip(\"pandas\")\n\n        df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6], \"c\": [7, 8, 9]})\n        ct = ColumnTransformer([(\"trans\", \"passthrough\", [\"a\"]), (\"trans2\", StandardScaler(), [\"b\"])],\n                              remainder=\"drop\")\n        ct.set_output(transform=\"pandas\")\n\n        out = ct.fit_transform(df)\n        assert isinstance(out, pd.DataFrame)\n\n        ct.set_output(transform=\"numpy\")\n        out = ct.transform(df)\n        assert isinstance(out, np.ndarray)\n", "   def test_column_transformer_sparse_output():\n    \"\"\"Check that ColumnTransformer works correctly with sparse input and output.\"\"\"\n    from scipy.sparse import csc_matrix\n\n    X_dense = np.array([[1, 2], [3, 4], [5, 6]])\n    X_sparse = csc_matrix(X_dense)\n\n    ct = ColumnTransformer(\n        [(\"scaler\", StandardScaler(), [0]), (\"passthrough\", \"passthrough\", [1])]\n    )\n\n    # Test fit_transform with sparse input\n    X_trans_sparse = ct.fit_transform(X_sparse)\n    assert isinstance(X_trans_sparse, csc_matrix)\n\n    # Test transform with sparse input\n    X_trans_sparse = ct.transform(X_sparse)\n    assert isinstance(X_trans_sparse, csc_matrix)\n\n    # Test fit_transform with dense input\n    X_trans_dense = ct.fit_transform(X_dense)\n    assert isinstance(X_trans_dense, np.ndarray)\n\n    # Test transform with dense input\n    X_trans_dense = ct.transform(X_dense)\n    assert isinstance(X_trans_dense, np.ndarray)\n\n"], "sample_544": ["    def test_imshow_masked_with_nans(fig_test, fig_ref):\n        # Test masked arrays containing NaNs are handled correctly\n        x = np.arange(10)\n        y = np.arange(10)\n        xv, yv = np.meshgrid(x, y)\n        z = xv + yv\n\n        masked_z = np.ma.masked_array(z, mask=np.isnan(z))\n\n        fig_test.subplots()\n        plt.imshow(masked_z, cmap=\"viridis\")\n\n        fig_ref.subplots()\n        plt.imshow(z, cmap=\"viridis\")\n", "def test_imshow_masked_array():\n    x = np.arange(100)\n    y = x.reshape((10, 10))\n    masked_y = np.ma.masked_where(x % 2 == 0, y)\n    fig, ax = plt.subplots()\n    ax.imshow(masked_y, cmap='viridis')\n", "def test_imshow_nan_holes(fig_test, fig_ref):\n    # Check that NaN holes are handled correctly in imshow\n    data = np.eye(10)\n    data[2:5, 2:5] = np.nan\n\n    ax = fig_test.subplots()\n    ax.imshow(data, cmap='viridis', interpolation='nearest')\n\n    ax = fig_ref.subplots()\n    ax.imshow(data, cmap='viridis', interpolation='nearest')\n", "compilation error", "    def test_imshow_extent_data(self, origin):\n        # Test that imshow respects the extent argument when data is provided\n        fig, ax = plt.subplots()\n        data = np.arange(12).reshape((3, 4))\n        extent = (-1, 2, 0, 3)\n        im = ax.imshow(data, extent=extent, origin=origin)\n        extent_data = im.get_extent()\n\n        assert extent_data == extent\n\n        # Test that the data is transformed correctly\n        x_pixels, y_pixels = im.get_array().shape\n        x_data = np.linspace(extent[0], extent[1], x_pixels)\n        y_data = np.linspace(extent[2], extent[3], y_pixels)\n\n        assert np.allclose(im.get_xdata(), x_data)\n        assert np.allclose(im.get_ydata(), y_data)\n"], "sample_256": ["    def test_superusers_can_change_password_for_inactive_user(self):\n        user = User.objects.create_user('inactive_user', 'inactive@example.com', 'password')\n        user.is_active = False\n        user.save()\n\n        super_user = User.objects.create_superuser('super_user', 'super@example.com', 'superpassword')\n        data = {'password1': 'newpassword', 'password2': 'newpassword'}\n        form = AdminPasswordChangeForm(user, data, user_instance=user)\n        with self.assertRaises(PermissionDenied):\n            form.save()\n\n", "    def test_unicode_password(self):\n        user = User.objects.get(username='testclient')\n        data = {'password1': '\u4f60\u597d', 'password2': '\u4f60\u597d'}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertTrue(form.is_valid())\n\n", "    def test_password_complexity(self):\n        user = User.objects.get(username='testclient')\n        data = {'password1': '123', 'password2': '123'}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertIn('password1', form.errors)\n        self.assertIn('password2', form.errors)\n", "    def test_password_validation(self):\n        user = User.objects.get(username='testclient')\n        with self.assertRaises(ValidationError):\n            form = AdminPasswordChangeForm(user, {'password1': ' ', 'password2': ' '})\n            form.is_valid()\n            form.save()\n\n            form = AdminPasswordChangeForm(user, {'password1': password, 'password2': password})\n            self.assertFalse(form.is_valid())\n            self.assertEqual(form.errors['password1'], expected_error)\n\n        check_error('password', [str(form.error_messages['password_too_short'])])\n", "    def test_admin_password_change_form_with_custom_user_model(self):\n        class CustomUser(User):\n            class Meta:\n                proxy = True\n\n        CustomUser.objects.create_user('testuser', 'test@example.com', 'password')\n        user = CustomUser.objects.get(username='testuser')\n        data = {'password1': 'test123', 'password2': 'test123'}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertTrue(form.is_valid())\n        form.save()\n        self.assertTrue(user.check_password('test123'))\n\n"], "sample_861": ["    def test_grid_search_with_refit_error(self):\n        # Test GridSearchCV when refit fails with a non-numeric score\n        X = np.arange(20).reshape(5, -1)\n        y = [0, 0, 1, 1, 1]\n\n        class FailingEstimator(BaseEstimator):\n                return self\n\n                return np.zeros(X.shape[0])\n\n                raise ValueError(\"Intentional error\")\n\n        gs = GridSearchCV(FailingEstimator(), {'param': [1]}, cv=2,\n                          scoring='accuracy', refit=True)\n        with pytest.raises(ValueError):\n            gs.fit(X, y)\n\n        assert gs.best_score_ is None\n        assert gs.best_params_ is None\n        assert gs.best_estimator_ is None\n", "    def test_grid_search_with_refit_False(self):\n        # Test that GridSearchCV with refit=False doesn't refit on the whole data\n        # after the cross-validation\n\n        X, y = make_classification(n_samples=20, n_features=10, random_state=0)\n        clf = MockClassifier()\n        gs = GridSearchCV(clf, {'foo_param': [1, 2]}, cv=2, refit=False)\n        gs.fit(X, y)\n        assert not gs.best_estimator_.fit_called\n\n", "    def test_randomized_search_cv_results_n_splits(self):\n        # Test that `n_splits` is correctly recorded in cv_results_\n        X = np.arange(20).reshape(5, -1)\n        y = [0, 0, 1, 1, 1]\n        clf = MockClassifier()\n        param_grid = {'foo_param': [1, 2, 3]}\n        n_splits = 3\n        rs = RandomizedSearchCV(clf, param_grid, cv=n_splits,\n                           n_iter=2, random_state=0)\n        rs.fit(X, y)\n        assert rs.cv_results_['n_splits'][0] == n_splits\n", "    def test_randomized_search_cv_splits_consistency():\n        # Check if a one time iterable is accepted as a cv parameter.\n        n_samples = 100\n        n_splits = 5\n        X, y = make_classification(n_samples=n_samples, random_state=0)\n\n        rs = RandomizedSearchCV(LinearSVC(random_state=0),\n                                param_distributions={'C': [0.1, 0.2, 0.3]},\n                                cv=OneTimeSplitter(n_splits=n_splits,\n                                                    n_samples=n_samples),\n                                return_train_score=True)\n        rs.fit(X, y)\n\n        rs2 = RandomizedSearchCV(LinearSVC(random_state=0),\n                                 param_distributions={'C': [0.1, 0.2, 0.3]},\n                                 cv=KFold(n_splits=n_splits), return_train_score=True)\n        rs2.fit(X, y)\n\n        # Give generator as a cv parameter\n        assert isinstance(KFold(n_splits=n_splits,\n                                shuffle=True, random_state=0).split(X, y),\n                          GeneratorType)\n        rs3 = RandomizedSearchCV(LinearSVC(random_state=0),\n                                param_distributions={'C': [0.1, 0.2, 0.3]},\n                                cv=KFold(n_splits=n_splits, shuffle=True,\n                                            random_state=0).split(X, y),\n                                return_train_score=True)\n        rs3.fit(X, y)\n\n        rs4 = RandomizedSearchCV(LinearSVC(random_state=0),\n                                 param_distributions={'C': [0.1, 0.2, 0.3]},\n                                 cv=KFold(n_splits=n_splits, shuffle=True,\n                                            random_state=0), return_train_score=True)\n        rs4.fit(X, y)\n\n            for key in ('mean_fit_time', 'std_fit_time',\n                        'mean_", "    def test_grid_search_refit_with_transform_error():\n        # Test that refit works correctly when transformer in pipeline\n        # raises an error during transform\n\n        class ErrorTransformer(TransformerMixin):\n                return self\n\n                raise ValueError(\"Intentional error in transform\")\n\n        pipeline = Pipeline([\n            ('transformer', ErrorTransformer()),\n            ('classifier', SVC())\n        ])\n        X, y = make_classification(n_samples=10, n_features=5, random_state=0)\n\n        grid_search = GridSearchCV(pipeline,\n                                   param_grid={'classifier__C': [0.1, 1]},\n                                   refit=True, cv=2)\n        with pytest.raises(ValueError,\n                           match=\"Intentional error in transform\"):\n            grid_search.fit(X, y)\n"], "sample_389": ["    def test_setting_change_signal_after_rollback(self):\n        \"\"\"\n        Ensure the `setting_changed` signal is emitted when a setting change\n        within `override_settings` is rolled back due to an exception.\n        \"\"\"\n        # Spy to track emitted signals\n        self.spy_receiver = mock.Mock()\n        signals.setting_changed.connect(self.spy_receiver)\n\n        try:\n            with self.assertRaises(SettingChangeEnterException):\n                with override_settings(SETTING_BOTH=\"BOTH\"):\n                    raise SettingChangeEnterException\n        finally:\n            signals.setting_changed.disconnect(self.spy_receiver)\n\n        # Check if the signal was emitted twice for both enter and exit\n        self.assertEqual(self.spy_receiver.call_count, 2)\n", "    def test_override_settings_nested_conflicts(self):\n        \"\"\"\n        Test that nested override_settings blocks with conflicting settings raise\n        an ImproperlyConfigured exception.\n        \"\"\"\n        with self.assertRaises(ImproperlyConfigured) as exc_info:\n            with override_settings(SETTING_A=\"outer\"):\n                with override_settings(SETTING_A=\"inner\"):\n                    pass\n        self.assertIn(\n            \"Conflicting settings values: SETTING_A\", str(exc_info.exception)\n        )\n", "    def test_script_name_with_trailing_slash(self):\n        # Test that SCRIPT_NAME with trailing slash doesn't add an extra slash\n        # to MEDIA_URL and STATIC_URL.\n\n        tests = (\n            (\"/somesubpath/\", \"path\", \"/somesubpath/path\"),\n            (\"/\", \"path\", \"/path\"),\n        )\n        for setting in (\"MEDIA_URL\", \"STATIC_URL\"):\n            for script_name, path, expected_path in tests:\n                new_settings = {setting: path}\n                with self.settings(**new_settings):\n                    with self.subTest(script_name=script_name, **new_settings):\n                        try:\n                            self.set_script_name(script_name)\n                            self.assertEqual(getattr(settings, setting), expected_path)\n                        finally:\n                            clear_script_prefix()\n\n", "    def test_path_with_trailing_slash(self):\n        new_settings = {\"MEDIA_URL\": \"/media/\", \"STATIC_URL\": \"/static/\"}\n        with self.settings(**new_settings):\n            with self.subTest():\n                self.set_script_name(\"/some/path/\")\n                self.assertEqual(settings.MEDIA_URL, \"/some/path/media/\")\n                self.assertEqual(settings.STATIC_URL, \"/some/path/static/\")\n\n", "    def test_override_settings_nested_with_errors(self):\n        \"\"\"\n        Ensure that nested override_settings context managers correctly\n        restore settings even if errors occur within inner contexts.\n        \"\"\"\n        inner_raised = False\n\n            nonlocal inner_raised\n            try:\n                with override_settings(SETTING_ENTER=\"INNER_ERROR\"):\n                    raise SettingChangeEnterException\n            except SettingChangeEnterException:\n                inner_raised = True\n\n        with self.assertRaises(SettingChangeEnterException):\n            with override_settings(SETTING_PASS=\"OUTER\"):\n                inner_func()\n\n        self.assertTrue(inner_raised)\n        self.check_settings()\n        self.check_spy_receiver_exit_calls(call_count=2)\n"], "sample_588": ["    def test_auto_combine_with_empty_dataset(self):\n        objs = [Dataset({\"x\": [0]}), Dataset()]\n        with pytest.warns(FutureWarning, match=\"supplied are empty\"):\n            auto_combine(objs) \n", "    def test_auto_combine_join(self):\n        objs = [Dataset({\"x\": [0], \"y\": [0]}), Dataset({\"x\": [1], \"y\": [1]})]\n        with pytest.warns(FutureWarning):\n            actual = auto_combine(objs, join=\"left\")\n        expected = Dataset({\"x\": [0, 1], \"y\": [0, 1]})\n        assert_identical(expected, actual)\n", "    def test_auto_combine_empty_input(self):\n        with pytest.warns(FutureWarning, match=\"empty list\"):\n            auto_combine([])\n\n", "    def test_auto_combine_empty_input(self):\n        with pytest.warns(FutureWarning, match=\"supplied are empty\"):\n            auto_combine([])\n\n", "    def test_combine_nested_fill_value_with_coords(self):\n        datasets = [\n            Dataset({\"a\": (\"x\", [2, 3]), \"x\": [1, 2]}),\n            Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]}),\n        ]\n        expected = Dataset(\n            {\"a\": ((\"t\", \"x\"), [[np.nan, 2, 3], [1, 2, np.nan]])},\n            {\"x\": [0, 1, 2]},\n        )\n        actual = combine_nested(datasets, concat_dim=\"t\", fill_value=np.nan)\n        assert_identical(expected, actual)\n"], "sample_937": ["def test_unparse_function_def(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0]) == expected\n", "    def test_unparse_type_comments():\n        source = \"\"\"", "    def test_unparse_arguments_posonlyargs():\n        source = \"lambda x, /, y: x + y\"\n        expected = \"lambda x, /, y: ...\"\n        module = ast.parse(source)\n        assert ast.unparse(module.body[0].value) == expected\n", "def test_unparse_function_with_types(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module) == expected \n\n", "def test_unparse_function_def(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0]) == expected\n"], "sample_793": ["def test_iforest_n_estimators():\n    # Test that different n_estimators change the predictions\n    X = iris.data[:100]\n\n    clf1 = IsolationForest(n_estimators=10).fit(X)\n    clf2 = IsolationForest(n_estimators=100).fit(X)\n    assert_not_equal(clf1.predict(X), clf2.predict(X))\n", "def test_iforest_predict_proba(): \n    # toy sample (the last two samples are outliers)\n    X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1], [6, 3], [-4, 7]]\n\n    # Test IsolationForest\n    clf = IsolationForest(\n        behaviour=\"new\", random_state=rng, contamination=contamination\n    )\n    clf.fit(X)\n    proba = clf.predict_proba(X)\n    # assert probabilities sum to 1\n    assert_allclose(proba.sum(axis=1), np.ones(len(X)))\n\n", "def test_iforest_predict_on_empty_data():\n    clf = IsolationForest()\n    clf.fit(iris.data)\n    empty_data =np.array([])\n    empty_data = empty_data.reshape(0, iris.data.shape[1])\n\n    with pytest.raises(ValueError):\n        clf.predict(empty_data)\n", "def test_iforest_n_estimators():\n    X = np.array([[1, 1], [2, 2], [3, 3], [4, 4]])\n\n    clf1 = IsolationForest(n_estimators=50, random_state=42).fit(X)\n    clf2 = IsolationForest(n_estimators=100, random_state=42).fit(X)\n\n    # Assert that the decision function scores are different for different\n    # n_estimators\n    assert not np.allclose(clf1.decision_function(X),\n                           clf2.decision_function(X))\n\n", "def test_iforest_sparse_predict_scores():\n    # toy sample (the last two samples are outliers)\n    X = csc_matrix([[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1], [6, 3], [-4, 7]])\n\n    # Test IsolationForest\n    clf = IsolationForest(\n        behaviour=\"new\", random_state=rng, contamination=contamination\n    )\n    clf.fit(X)\n    decision_func = -clf.decision_function(X)\n    pred = clf.predict(X)\n    # assert detect outliers:\n    assert_greater(np.min(decision_func[-2:]), np.max(decision_func[:-2]))\n    assert_array_equal(pred, 6 * [1] + 2 * [-1])\n"], "sample_299": ["    def test_file_based_cache_is_absolute_with_multiple_caches(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': pathlib.Path.cwd() / 'cache',\n            },\n            'other': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': 'relative/path',\n            },\n        }):\n            self.assertEqual(check_file_based_cache_is_absolute(None), [\n                Warning(\n                    \"Your 'other' cache LOCATION path is relative. Use an \"\n                    \"absolute path instead.\",\n                    id='caches.W003',\n                ),\n            ])\n", "    def test_file_based_cache_not_absolute_in_deploy(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': 'cache',\n            },\n        }, DEPLOY=True):\n            self.assertEqual(check_file_based_cache_is_absolute(None), [\n                Warning(\n                    \"Your 'default' cache LOCATION path is relative. Use an \"\n                    \"absolute path instead.\",\n                    id='caches.W003',\n                ),\n            ])\n", "    def test_cache_path_not_absolute_outside_project(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': '/tmp/cache',\n            },\n        }):\n            self.assertEqual(check_file_based_cache_is_absolute(None), [])\n\n", "    def test_file_based_cache_not_absolute_warning(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.locmem.LocMemCache',\n                'LOCATION': 'cache',\n            },\n            'other': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': pathlib.Path.cwd() / 'cache',\n            },\n        }):\n            self.assertEqual(check_file_based_cache_is_absolute(None), [\n                Warning(\n                    \"Your 'other' cache LOCATION path is relative. Use an \"\n                    \"absolute path instead.\",\n                    id='caches.W003',\n                ),\n            ])\n\n", "    def test_multiple_file_based_caches(self):\n        root = pathlib.Path.cwd()\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': root / 'cache1',\n            },\n            'other': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': root / 'cache2',\n            },\n        }):\n            self.assertEqual(check_file_based_cache_is_absolute(None), [])\n\n\n\n"], "sample_956": ["def test_inventory_file_not_found(tempdir, app, status, warning):\n    inv_file = tempdir / 'inventory'\n    set_config(app, {\n        'https://docs.python.org/': str(inv_file),\n    })\n\n    # load the inventory and check if it's done correctly\n    normalize_intersphinx_mapping(app, app.config)\n    with pytest.raises(IOError):\n        load_mappings(app)\n\n    assert warning.getvalue() == ''\n    assert 'failed to reach any of the inventories' in status.getvalue()\n", "    def test_missing_reference_inventory_version_format(tempdir, app, status, warning):\n        inv_file = tempdir / 'inventory'\n        inv_file.write_bytes(b\"\"\"", "def test_missing_reference_refdoc(tempdir, app, status, warning):\n    inv_file = tempdir / 'inventory'\n    inv_file.write_bytes(inventory_v2)\n    set_config(app, {\n        'https://docs.python.org/': inv_file,\n    })\n\n    # load the inventory and check if it's done correctly\n    normalize_intersphinx_mapping(app, app.config)\n    load_mappings(app)\n\n    kwargs = {'refdoc': 'sub/dir/test'}\n    node, contnode = fake_node('py', 'mod', 'py3k:module1', 'foo', **kwargs)\n    rn = missing_reference(app, app.env, node, contnode)\n    assert rn['refuri'] == 'https://docs.python.org/sub/dir/test/foo.html#module-module1'\n", "def test_missing_reference_inventory_not_found(tempdir, app, status, warning):\n    inv_file = tempdir / 'inventory'\n    set_config(app, {\n        'https://docs.python.org/': inv_file,\n    })\n\n    # load the inventory and check if it's done correctly\n    normalize_intersphinx_mapping(app, app.config)\n    load_mappings(app)\n\n    # simulate the case where inventory is not found\n    os.remove(inv_file)\n    rn = reference_check(app, 'py', 'func', 'module1.func', 'foo')\n    assert rn is None\n", "def test_missing_reference_pydomain_empty_module(tempdir, app, status, warning):\n    inv_file = tempdir / 'inventory'\n    inv_file.write_bytes(inventory_v2)\n    set_config(app, {\n        'https://docs.python.org/': inv_file,\n    })\n\n    # load the inventory and check if it's done correctly\n    normalize_intersphinx_mapping(app, app.config)\n    load_mappings(app)\n\n    # no context data\n    kwargs = {'py:module': ''}\n    node, contnode = fake_node('py', 'func', 'func', 'func()', **kwargs)\n    rn = missing_reference(app, app.env, node, contnode)\n    assert rn is None\n"], "sample_1076": ["def test_SymPyPrinter_print_piecewise():\n    p = SymPyPrinter()\n    expr = Piecewise((1, Eq(x, 0)), (2, x > 6))\n    assert p.doprint(expr) == \"Piecewise((1, Eq(x, 0)), (2, x > 6))\"\n", "def test_SymPyPrinter_print_seq():\n    s = SymPyPrinter()\n\n    assert s._print_seq(range(2)) == '(0, 1,)'\n\n", "def test_symPyPrinter():\n    p = SymPyPrinter()\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    assert p.doprint(Rational(1, 2)) == 'sympy.Rational(1, 2)'\n    assert p.doprint(x**Rational(1, 2)) == 'x**(sympy.Rational(1, 2))'\n    assert p.doprint(sqrt(x)) == 'sympy.sqrt(x)'\n    assert p.doprint(acos(x)) == 'sympy.acos(x)'\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    assert p.doprint(Rational(1, 2)) == 'sympy.Rational(1, 2)'\n    assert p.doprint(pi) == 'sympy.pi'\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    assert p.doprint(Rational(1, 2)) == 'Rational(1, 2)'\n    A = MatrixSymbol(\"A\", 2, 2)\n    assert p.doprint(A**(-1)) == 'A**(-1)'\n    assert p.doprint(A**5) == 'A**5'\n    assert p.doprint(Identity(3)) == 'Identity(3)'\n\n    u = MatrixSymbol('x', 2, 1)\n    v = MatrixSymbol('y', 2, 1)\n    assert p.doprint(MatrixSolve(A, u)) == 'Eq(A*x, u).solveset(x)'\n    assert p.doprint(MatrixSolve(A, u) + v) == 'Eq(A*x, u).solveset(x) + y'\n\n    # Workaround for SymPy negative integer power errors\n    assert p.doprint(x**-1) == '1/x'\n    assert p.doprint(x**-2) == '1/x**2'\n"], "sample_181": ["    def test_filtered_aggregate_ref_self_annotation(self):\n        aggs = Book.objects.annotate(\n            authors_age_sum=Sum('authors__age'),\n        ).aggregate(\n            cnt=Count('pk', filter=Q(authors_age_sum__gt=100)),\n        )\n        self.assertEqual(aggs['cnt'], 3)\n", "    def test_filtered_aggregate_ref_multiple_subquery_annotation_exclude(self):\n        aggregate = Book.objects.values('publisher').annotate(\n            has_authors=Exists(\n                Book.authors.through.objects.filter(book=OuterRef('pk')),\n            ),\n            authors_have_other_books=Exists(\n                Book.objects.filter(\n                    authors__in=Author.objects.filter(\n                        book_contact_set=OuterRef(OuterRef('pk')),\n                    )\n                ).exclude(pk=OuterRef('pk')),\n            ),\n        ).aggregate(\n            max_rating=Max(\n                'rating',\n                filter=Q(has_authors=True) & ~Q(authors_have_other_books=True),\n            )\n        )\n        self.assertEqual(aggregate, {'max_rating': 4.5})\n", "    def test_filtered_count_related_exists(self):\n        aggregate = Author.objects.annotate(\n            has_published_books=Exists(\n                Book.objects.filter(contact=OuterRef('pk'), publisher__name='Apress')\n            ),\n        ).aggregate(cnt=Count('pk', filter=Q(has_published_books=True)))\n        self.assertEqual(aggregate['cnt'], 3)\n\n", "    def test_filtered_aggregate_on_related_model_fk(self):\n        agg = Sum('book__price', filter=Q(book__rating__gt=3))\n        qs = Author.objects.annotate(total_price=agg).order_by('pk')\n        self.assertSequenceEqual([a.total_price for a in qs], [Decimal('30.00'), None, Decimal('59.78')])\n", "    def test_filtered_aggregate_with_conditional_f_expression(self):\n        agg = Sum(\n            Case(\n                When(age__gt=50, then=F('age') * 2),\n                default=F('age'),\n            ),\n            filter=Q(name__startswith='test'),\n        )\n        self.assertEqual(Author.objects.aggregate(age=agg)['age'], 200)\n\n"], "sample_941": ["def test_restify_type_hints_recursive_typevars():\n    TV = TypeVar('TV')\n    class Recursive(Generic[TV]):\n            self.value = value\n    assert restify(Recursive[TV]) == ':class:`tests.test_util_typing.Recursive`\\\\ [:obj:`tests.test_util_typing.TV`]'\n\n    R = Recursive[TV]\n    assert restify(R) == ':class:`tests.test_util_typing.Recursive`\\\\ [:obj:`tests.test_util_typing.TV`]' \n", "    def test_stringify_newtype():\n        NewTypeInt = NewType('NewTypeInt', int)\n        assert stringify(NewTypeInt) == \"NewTypeInt\"\n        assert stringify(NewTypeInt(1)) == \"NewTypeInt(1)\"\n", "    def test_stringify_type_hints_union_with_none():\n        assert stringify(Union[NoneType, int]) == \"Optional[int]\"\n\n", "    def test_stringify_type_hints_ellipsis():\n        assert stringify(List[...]) == 'List[...]'\n        assert stringify(Tuple[str, ...]) == 'Tuple[str, ...]'\n        assert stringify(Callable[..., int]) == 'Callable[..., int]'\n", "    def test_stringify_typing_origin():\n        assert stringify(typing.Union) == \"Union\"\n        assert stringify(typing.List) == \"List\"\n        assert stringify(typing.Dict) == \"Dict\"\n        assert stringify(typing.Tuple) == \"Tuple\"\n        assert stringify(typing.Callable) == \"Callable\"\n"], "sample_922": ["    def test_pydata_type_annotation(app):\n        text = (\".. py:data:: var\\n\"\n                \"   :type: str\\n\")\n        domain = app.env.get_domain('py')\n        doctree = restructuredtext.parse(app, text)\n        assert_node(doctree, (addnodes.index,\n                              [desc, ([desc_signature, ([desc_name, \"var\"],\n                                                        [desc_annotation, (\": \",\n                                                                           [pending_xref, \"str\"])])],\n                                                [desc_content, ()])]))\n        assert 'var' in domain.objects\n        assert domain.objects['var'] == ('index', 'var', 'data')\n", "def test_pyattribute_type(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :type: str\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'attr (Class attribute)', 'Class.attr', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"attr\"],\n                                                     [desc_annotation, (\": \",\n                                                        [pending_xref, \"str\"])])],\n                                   [desc_content, ()]))\n    assert 'Class.attr' in domain.objects\n    assert domain.objects['Class.attr'] == ('index', 'Class.attr', 'attribute') \n\n", "def test_pyexception_with_module(app):\n    text = \".. py:exception:: exceptions.IOError\\n\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"exception \"],\n                                                    [desc_addname, \"exceptions.\"],\n                                                    [desc_name, \"IOError\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], desc, desctype=\"exception\",\n                domain=\"py\", objtype=\"exception\", noindex=False)\n    assert 'exceptions.IOError' in domain.objects\n    assert domain.objects['exceptions.IOError'] == ('index', 'exceptions.IOError', 'exception')\n\n", "    def test_pyattribute_oldstyle(app):\n        text = (\".. py:class:: Class\\n\"\n                \"\\n\"\n                \"   .. py:attribute:: attr\\n\"\n                \"      :type: str\\n\"\n                \"      :value: ''\\n\")\n        domain = app.env.get_domain('py')\n        doctree = restructuredtext.parse(app, text)\n        assert_node(doctree, (addnodes.index,\n                              [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                        [desc_name, \"Class\"])],\n                                      [desc_content, (addnodes.index,\n                                                      desc)])]))\n        assert_node(doctree[1][1][0], addnodes.index,\n                    entries=[('single', 'attr (Class attribute)', 'Class.attr', '', None)])\n        assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"attr\"],\n                                                       [desc_annotation, (\": \",\n                                                                          [pending_xref, \"str\"])],\n                                                       [desc_annotation, \" = ''\"])],\n                                      [desc_content, ()]))\n        assert 'Class.attr' in domain.objects\n        assert domain.objects['Class.attr'] == ('index', 'Class.attr', 'attribute')\n\n", "def test_pyattribute_with_default(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :type: int\\n\"\n            \"      :default: 42\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'attr (Class attribute)', 'Class.attr', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"attr\"],\n                                                     [desc_annotation, (\": \",\n                                                                        [pending_xref, \"int\"])],\n                                                     [desc_annotation, \" = 42\"])],\n                                   [desc_content, ()]))\n    assert 'Class.attr' in domain.objects\n    assert domain.objects['Class.attr'] == ('index', 'Class.attr', 'attribute')\n\n"], "sample_221": ["    def test_pickle_order_by_with_expression(self):\n        qs = Happening.objects.annotate(\n            combined_field=models.F('when') + models.DurationField(days=1)\n        ).order_by('combined_field')\n        self.assert_pickles(qs)\n", "    def test_pickle_model_with_inherited_manager(self):\n        g = Group.objects.create(name='testgroup')\n        g.save()\n        original = MyEvent.objects.create(title='test event', group=g)\n\n        dumped = pickle.dumps(original)\n        reloaded = pickle.loads(dumped)\n        self.assertEqual(original, reloaded)\n", "    def test_pickle_complex_related_filter_queryset(self):\n        g1 = Group.objects.create(name='g1')\n        g2 = Group.objects.create(name='g2')\n        Event.objects.create(title='event1', group=g1)\n        Event.objects.create(title='event2', group=g2)\n        qs = Event.objects.filter(group__in=Group.objects.filter(name__in=['g1', 'g2']))\n        self.assert_pickles(qs)\n\n", "    def test_select_related_pickling_with_deferred_fields(self):\n        related_event = Event.objects.create(title='Related Event', group=self.e1.group)\n        container = Container.SomeModel.objects.create(somefield=related_event)\n\n        qs = Container.SomeModel.objects.select_related('somefield__group').defer('somefield__title')\n        self.assert_pickles(qs)\n", "    def test_in_lookup_pickled_inner_query_with_select_related(self):\n        events = Event.objects.select_related('group').filter(group__in=Group.objects.all())\n        dumped = pickle.dumps(events.query)\n        reloaded = pickle.loads(dumped)\n        reloaded_events = Event.objects.none()\n        reloaded_events.query = reloaded\n        self.assertSequenceEqual(reloaded_events, [self.e1])\n"], "sample_1042": ["compilation error", "compilation error", "compilation error", "def test_IndexedBase_with_integer_symbol():\n    n = symbols(\"n\", integer=True)\n    A = IndexedBase(\"A\", shape=(n,))\n    i = symbols(\"i\", integer=True)\n    assert A[i] == A[i]\n", "compilation error"], "sample_740": ["compilation error", "compilation error", "def test_check_array_allow_nd_warning():\n    X = np.ones((10, 5, 3))\n    with pytest.warns(UserWarning, match=\"array with 3 dimensions\"):\n        check_array(X, allow_nd=True, ensure_2d=False)\n", "compilation error", "compilation error"], "sample_241": ["    def test_alias_and_output_field(self):\n        expr = ExpressionWrapper(F('field'), output_field=IntegerField(), alias='alias')\n        self.assertEqual(expr.alias, 'alias')\n        self.assertEqual(expr.output_field, IntegerField())\n", "    def test_empty_group_by_alias(self):\n        expr = ExpressionWrapper(Value(3), output_field=IntegerField(), alias='myalias')\n        self.assertEqual(expr.get_group_by_cols(alias='myalias'), [])\n", "    def test_resolve_output_field_none(self):\n        expr = ExpressionWrapper(Value(None), output_field=None)\n        self.assertIsNone(expr.output_field)\n", "    def test_expression_wrapper_deconstruct(self):\n        expr = ExpressionWrapper(F('field'), output_field=IntegerField())\n        path, args, kwargs = expr.deconstruct()\n        self.assertEqual(path, 'django.db.models.expressions.ExpressionWrapper')\n        self.assertEqual(args, (F('field'),))\n        self.assertEqual(kwargs, {'output_field': IntegerField()})\n", "    def test_expression_wrapper_copy(self):\n        expr = ExpressionWrapper(Lower(Value('f')), output_field=IntegerField())\n        copy = expr.copy()\n        self.assertEqual(copy.expression, expr.expression)\n        self.assertEqual(copy.output_field, expr.output_field)\n        self.assertIsNot(copy, expr)\n\n"], "sample_627": ["    def test_concat_stacked_coord_types():\n        ds1 = Dataset(coords={\"x\": range(4), \"y\": (\"x\", [\"a\", \"b\", \"c\", \"d\"])})\n        ds2 = Dataset(coords={\"x\": range(4, 8), \"y\": ([\"x\"], [1, 2, 3, 4])})\n        with pytest.raises(TypeError, match=r\"cannot concatenate coordinates: 'y'\"):\n            concat([ds1, ds2], dim=\"x\")\n", "    def test_concat_multiindex_levels_mismatch(self) -> None:\n        arrays = [\n            Dataset(\n                {\"data\": ([\"x\", \"y\"], np.random.rand(2, 3))},\n                coords={\n                    \"x\": MultiIndex.from_product([[\"A\"], [1, 2]]),\n                    \"y\": [\"a\", \"b\", \"c\"],\n                },\n            ),\n            Dataset(\n                {\"data\": ([\"x\", \"y\"], np.random.rand(2, 3))},\n                coords={\n                    \"x\": MultiIndex.from_product([[\"A\", \"B\"], [1]]),\n                    \"y\": [\"a\", \"b\", \"c\"],\n                },\n            ),\n        ]\n        with pytest.raises(ValueError, match=r\"MultiIndex levels must match\"):\n            concat(arrays, dim=\"x\")\n", "    def test_concat_index_same_name_different_dim(self) -> None:\n        ds1 = Dataset(coords={\"x\": (\"x\", [1, 2])})\n        ds2 = Dataset(coords={\"x\": (\"y\", [3, 4])})\n\n        with pytest.raises(\n            ValueError, match=r\"Cannot concatenate along dimension 'x' indexes with dimensions.*\"\n        ):\n            concat([ds1, ds2], dim=\"x\")\n\n", "    def test_concat_index_different_names() -> None:\n        ds1 = Dataset(coords={\"x\": (\"x\", [1, 2])})\n        ds2 = Dataset(coords={\"y\": (\"x\", [3, 4])})\n        with pytest.raises(ValueError, match=r\"Cannot concatenate along dimension 'x' .*\"):\n            concat([ds1, ds2], dim=\"x\")\n", "    def test_concat_preserve_attrs_of_common_coords() -> None:\n        ds1 = Dataset({\"foo\": ([\"x\"], [1, 2])}, {\"x\": [0, 1], \"y\": 0})\n        ds2 = Dataset({\"foo\": ([\"x\"], [3, 4])}, {\"x\": [2, 3], \"y\": 1})\n\n        expected = Dataset({\"foo\": ([\"x\"], [1, 2, 3, 4])}, {\"x\": [0, 1, 2, 3], \"y\": [0, 1]})\n        actual = concat([ds1, ds2], dim=\"x\")\n\n        assert_identical(actual, expected)\n"], "sample_664": ["def test_funcargnames_deprecation(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            return request.param\n\n            assert func == 'hello'\n\n        test_func.funcargnames = ['func']\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: The `funcargnames` attribute was an alias for `fixturenames`*\",\n            \"*since pytest 2.3 - use the newer attribute instead*\",\n        ]\n    )\n\n", "def test_funcargnames_is_deprecated(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            return 42\n\n            assert fixture_a == 42\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\"*PytestDeprecationWarning: The `funcargnames` attribute was an alias for `fixturenames`*\",]\n    )\n", "def test_warn_about_funcargnames(testdir, pytestconfig):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            return 42\n\n            assert arg == 42\n\n            print(test_func.funcargnames)\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*The `funcargnames` attribute was an alias for `fixturenames`*\",\n            \"*since pytest 2.3 - use the newer attribute instead.*\",\n        ]\n    )\n    ", "def test_fixture_positional_args(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            return request.param\n\n            assert arg_fixture == 1\n\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n    result.stdout.fnmatch_lines(\n        [\"*Passing arguments to pytest.fixture() as positional arguments is deprecated - pass them*\", \"*as a keyword argument instead.\"]\n    )\n\n", "def test_funcargnames_deprecation(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            return 42\n\n        @pytest.mark.parametrize(\"fixturenames, expected\", [\n            ([\"arg\"], [\"arg\"]),\n            ([\"arg\", \"another\"], [\"arg\"]),\n        ])\n            assert request.fixturenames == expected\n            assert getattr(request, \"funcargnames\", None) == expected\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-v\")\n    result.stdout.fnmatch_lines(\n        [\"*PytestDeprecationWarning: The `funcargnames` attribute was an alias*\",\n         \"*use the newer attribute instead.\"]\n    )\n\n"], "sample_396": ["    def test_ticket_23622_with_isnull(self):\n        \"\"\"\n        Make sure __pk__in and __in work the same for related fields when\n        using a distinct on subquery with an ISNULL check.\n        \"\"\"\n        a1 = Ticket23605A.objects.create()\n        a2 = Ticket23605A.objects.create()\n        c1 = Ticket23605C.objects.create(field_c0=0.0)\n        Ticket23605B.objects.create(\n            modela_fk=a1,\n            field_b0=123,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a1,\n            field_b0=23,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a1,\n            field_b0=234,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a1,\n            field_b0=12,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a2,\n            field_b0=567,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a2,\n            field_b0=76,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        qx = Q(ticket23605b__pk__in=Ticket23605B.objects.filter(ticket23605b__isnull=False).distinct(\"modela_fk\"))\n        qy = Q", "    def test_ticket_24971(self):\n        i1 = Item.objects.create(name=\"item1\", price=10)\n        i2 = Item.objects.create(name=\"item2\", price=20)\n        i3 = Item.objects.create(name=\"item3\", price=30)\n        Order.objects.create(items=[i1, i2])\n        Order.objects.create(items=[i2, i3])\n\n        queryset = Item.objects.filter(order__items__isnull=False)\n        self.assertSequenceEqual(queryset, [i1, i2, i3])\n        queryset = Item.objects.exclude(order__items__isnull=False)\n        self.assertEqual(queryset.exists(), False)\n", "    def test_ticket_24279_empty_in(self):\n        qs = School.objects.filter(Q(pk__in=[]))\n        self.assertQuerysetEqual(qs, [])\n", "    def test_ticket_23622_empty_result(self):\n        self.assertSequenceEqual(\n            Ticket23605A.objects.filter(\n                Q(ticket23605b__pk__in=()) & Q(ticket23605b__field_b0__gte=300)\n            ),\n            [],\n        )\n        self.assertSequenceEqual(\n            Ticket23605A.objects.filter(\n                Q(ticket23605b__in=()) & Q(ticket23605b__field_b0__gte=300)\n            ),\n            [],\n        )\n", "    def test_ticket_25282(self):\n        \"\"\"\n        Test that the filter method doesn't cause unnecessary joins when\n        a field is explicitly selected using defer.\n        \"\"\"\n        p1 = Parent.objects.create()\n        p2 = Parent.objects.create()\n        c1 = Child.objects.create(parent=p1)\n        c2 = Child.objects.create(parent=p2)\n        qs = Parent.objects.defer('child__name').filter(child__age__gt=10)\n        self.assertIn('LEFT OUTER JOIN', str(qs.query))\n        self.assertCountEqual(qs, [p1])\n"], "sample_967": ["def test_mathjax_custom_path(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    assert 'some/custom/path.js' in content\n", "def test_mathjax_custom_path(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<script defer=\"defer\" src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\">'\n            '</script>' in content)\n", "def test_mathjax_custom_path(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    assert ('https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js' in content)\n\n\n", "def test_mathjax_custom_path(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<script defer=\"defer\" src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\">'\n            '</script>' in content)\n", "def test_mathjax_custom_path(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    assert 'some_other_path.js' in content\n\n"], "sample_1207": ["def test_issue_27376():\n    raises(SyntaxError, lambda: parse_expr('1/0'))\n", "    def test_issue_24980():\n        assert parse_expr(\"sin(x)+\", evaluate=False) == sin(x) + Integer(1)\n", "    def test_issue_23859():\n        raises(SyntaxError, lambda: parse_expr('x[1, 2]'))\n", "    def test_issue_24397():\n        n = Symbol('n')\n        assert parse_expr('n!/(n+1)!', evaluate=False) == factorial(n)/(factorial(n + 1))\n", "    def test_issue_23588():\n        assert parse_expr('1//2**3') == 1//8\n        assert parse_expr('1**3//2') == 1//2\n"], "sample_81": ["    def test_resolver_cache_default_urlconf(self):\n        # resolver for 'urlpatterns.path_urls' should be cached.\n        resolver1 = get_resolver('urlpatterns.path_urls')\n        resolver2 = get_resolver('urlpatterns.path_urls')\n        self.assertIs(resolver1, resolver2)\n", "    def test_resolver_cache_different_root_urlconf(self):\n        # resolver for different URLconf are different cached objects.\n        self.assertIsNot(get_resolver('urlpatterns.path_urls'), get_resolver('urlpatterns.path_dynamic_urls'))\n", "    def test_resolver_cache_with_urlconf_arg(self):\n        # resolver for a URLconf passed as argument is cached separately from the\n        # default URLconf\n        resolver_from_arg = get_resolver('urlpatterns.path_urls')\n        resolver_default = get_resolver()\n        self.assertIsNot(resolver_from_arg, resolver_default)\n", "    def test_reverse_with_prefix(self):\n        resolver = get_resolver('urlpatterns.path_urls')\n        with self.settings(ROOT_URLCONF='urlpatterns.path_urls'):\n            self.assertEqual(resolver.reverse('blog_detail', args=[1]), '/blog/1/')\n            self.assertEqual(resolver.reverse('blog_detail', args=[1], _prefix='/admin'), '/admin/blog/1/')\n", "    def test_resolver_cache_dynamic_root_urlconf(self):\n        # resolver for a different URLconf ('urlpatterns.path_dynamic_urls')\n        # should not be cached with  get_resolver().\n        self.assertIsNot(get_resolver('urlpatterns.path_urls'), get_resolver('urlpatterns.path_dynamic_urls'))\n"], "sample_139": ["    def test_invalid_ordering_field(self):\n        \"\"\"\n        Regression test for #17816: ensure invalid ordering fields raise an error.\n        \"\"\"\n        class InvalidOrderingModelAdmin(admin.ModelAdmin):\n            list_display = ('name', 'age')\n            ordering = ('invalid_field',)\n\n        custom_site = self._create_site()\n        custom_site.register(Child, InvalidOrderingModelAdmin)\n        with self.assertRaisesMessage(FieldError, \"Cannot resolve keyword 'invalid_field' into field. Choices are: age, id, name, parent\"):\n            response = self.client.get(reverse('admin:child_changelist', current_app=custom_site.name))\n\n", "    def test_add_row_selection_multi_page(self):\n        \"\"\"\n        The status line for selected rows gets updated correctly across\n        multiple pages (#22038).\n        \"\"\"\n        for i in range(20):\n            Author.objects.create(name='Author %s' % i)\n        self.admin_login(username='super', password='secret')\n        self.selenium.get(self.live_server_url + reverse('admin:books_author_changelist'))\n\n        form_id = '#changelist-form'\n\n        # Select rows on page 1\n        rows = self.selenium.find_elements_by_css_selector(\n            '%s #result_list tbody tr' % form_id)\n        for row in rows:\n            row.find_element_by_css_selector('.action-select').click()\n        selection_indicator = self.selenium.find_element_by_css_selector(\n            '%s .action-counter' % form_id)\n        self.assertEqual(selection_indicator.text, \"%s of 20 selected\" % len(rows))\n\n        # Go to next page\n        self.selenium.find_element_by_link_text('2').click()\n        # Select all rows on page 2\n        rows = self.selenium.find_elements_by_css_selector(\n            '%s #result_list tbody tr' % form_id)\n        for row in rows:\n            row.find_element_by_css_selector('.action-select').click()\n        selection_indicator = self.selenium.find_element_by_css_selector(\n            '%s .action-counter' % form_id)\n        self.assertEqual(selection_indicator.text, \"20 of 20 selected\")\n\n", "    def test_changelist_view_list_editable_changed_objects_uses_filter_in_get_queryset(self):\n        \"\"\"list_editable edits use a filtered queryset to limit memory usage.\"\"\"\n        a = Swallow.objects.create(origin='Swallow A', load=4, speed=1)\n        Swallow.objects.create(origin='Swallow B', load=2, speed=2)\n        data = {\n            'form-TOTAL_FORMS': '2',\n            'form-INITIAL_FORMS': '2',\n            'form-MIN_NUM_FORMS': '0',\n            'form-MAX_NUM_FORMS': '1000',\n            'form-0-uuid': str(a.pk),\n            'form-0-load': '10',\n            '_save': 'Save',\n        }\n        superuser = self._create_superuser('superuser')\n        self.client.force_login(superuser)\n        changelist_url = reverse('admin:admin_changelist_swallow_changelist')\n        with CaptureQueriesContext(connection) as context:\n            response = self.client.post(changelist_url, data=data)\n            self.assertEqual(response.status_code, 200)\n            # Check only the first few characters since the UUID may have dashes.\n            self.assertIn(str(a.pk)[:8], context.captured_queries[4]['sql'])\n\n", "    def test_changelist_filter_with_related_field(self):\n        \"\"\"\n        Regression tests for ticket #18167: filtering on related fields should\n        work correctly.\n        \"\"\"\n        superuser = self._create_superuser('superuser')\n        parent1 = Parent.objects.create(name='Parent 1')\n        parent2 = Parent.objects.create(name='Parent 2')\n        for i in range(5):\n            Child.objects.create(name='child %s' % i, parent=parent1)\n        for i in range(3):\n            Child.objects.create(name='child %s' % (i + 5), parent=parent2)\n\n        m = ChildAdmin(Child, custom_site)\n        request = self._mocked_authenticated_request('/child/', superuser)\n        response = m.changelist_view(request)\n\n        self.assertIn('parent', response.context_data['cl'].list_filter)\n        self.assertEqual(Parent.objects.count(), 2)\n\n        # Test filtering by related field\n        request = self._mocked_authenticated_request('/child/?parent=1', superuser)\n        response = m.changelist_view(request)\n        self.assertEqual(len(response.context_data['cl'].result_list), 5)\n        request = self._mocked_authenticated_request('/child/?parent=2', superuser)\n        response = m.changelist_view(request)\n        self.assertEqual(len(response.context_data['cl'].result_list), 3)\n\n\n\n", "    def test_search_results_pagination_with_empty_search(self):\n        \"\"\"\n        Ensure pagination works correctly when searching with an empty string.\n        \"\"\"\n        for i in range(1, 21):\n            Event.objects.create(title=f\"Event {i}\", description=f\"Description for event {i}\")\n        superuser = self._create_superuser('superuser')\n        m = EventAdmin(Event, custom_site)\n        request = self._mocked_authenticated_request('/event/', superuser)\n        cl = m.get_changelist_instance(request)\n        response = m.changelist_view(request)\n        self.assertEqual(len(response.context_data['cl'].result_list), 20)\n\n        request = self._mocked_authenticated_request('/event/?q=&p=2', superuser)\n        cl = m.get_changelist_instance(request)\n        response = m.changelist_view(request)\n        self.assertEqual(len(response.context_data['cl'].result_list), 0)\n"], "sample_947": ["def test_build_domain_c_typedefs(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"typedefs\")\n    assert len(ws) == 0\n    t = (app.outdir / \"typedefs.html\").read_text()\n    assert 'id=\"c.MyType\"' in t\n    assert 'id=\"c.SomeStruct\"' in t\n", "def test_intersphinx_namespace_member(tempdir, app, status, warning):\n    origSource = \"\"\"\\", "def test_build_domain_c_anon_nested(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"anon-nested\")\n    assert len(ws) == 0\n", "compilation error", "compilation error"], "sample_319": ["    def test_operation_with_invalid_chars_in_suggested_name_with_prefix(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.AddConstraint(\n                    \"Person\",\n                    models.UniqueConstraint(\n                        fields=[\"name\"], name=\"~person.name-unique!\"\n                    ),\n                ),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person_person_name_unique_\")\n", "    def test_rename_field_and_index_together(self):\n        \"\"\"Fields are renamed before updating index_together.\"\"\"\n        changes = self.get_changes(\n            [AutodetectorTests.author_empty, self.book_index_together_3],\n            [AutodetectorTests.author_empty, self.book_index_together_4],\n            MigrationQuestioner({\"ask_rename\": True}),\n        )\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"otherapp\",\n            0,\n            [\"RenameField\", \"AlterIndexTogether\"],\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"otherapp\",\n            0,\n            1,\n            name=\"book\",\n            index_together={(\"title\", \"newfield2\")},\n        )\n", "    def test_create_model_with_complex_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    \"VeryLongAndDescriptiveModelNameThatWillBeTruncatedForSure\",\n                    fields=[],\n                ),\n            ]\n        \n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"very_long_and_descriptive_model_name_that_will_be_trunc\")\n", "    def test_empty_operations_with_renames(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.RenameField('old_name', 'new_name'),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"rename_old_name_to_new_name\")\n", "    def test_squashed_migration_name(self):\n        class Migration(migrations.Migration):\n            operations = [migrations.CreateModel(\"Person\", fields=[])]\n            squashed = True\n\n        migration = Migration(\"0001_initial\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"initial\")\n\n"], "sample_862": ["    def test_vectorizers_empty_input(vec):\n        # Test vectorizers on empty input\n        with pytest.raises(ValueError,\n                            match=\"Input data cannot be empty\"):\n            vec.fit_transform([])\n        with pytest.raises(ValueError,\n                            match=\"Input data cannot be empty\"):\n            vec.fit([])\n", "def test_empty_vocabulary_warning(Vectorizer):\n    vect = Vectorizer()\n    with pytest.warns(UserWarning, match=\"empty vocabulary\"):\n        vect.fit([])\n\n\n\n", "def test_empty_corpus(Vectorizer):\n    # Test behavior with an empty corpus\n    vectorizer = Vectorizer()\n    with pytest.raises(ValueError):\n        vectorizer.fit([])\n\n    vectorizer = Vectorizer()\n    X = vectorizer.fit_transform([])\n    assert X.shape == (0, 0)\n", "    def test_tfidfvectorizer_idf_handling_empty_docs():\n        tfidf = TfidfVectorizer()\n        # Empty document list\n        X = tfidf.fit_transform([])\n        assert X.shape[0] == 0\n        assert X.shape[1] == 0\n\n        # List with empty documents\n        X = tfidf.fit_transform([\"\", \"\", \"\", \"\"])\n        assert X.shape[0] == 4  \n        assert X.shape[1] == 0\n", "    def test_tfidfvectorizer_lowercase(self):\n        vect = TfidfVectorizer(lowercase=False)\n        corpus = [\"This is THE first document\",\n                  \"This is the second document\"]\n        expected_vocabulary = {'This': 0, 'is': 1, 'THE': 2,\n                               'first': 3, 'document': 4,\n                               'the': 5, 'second': 6}\n        vect.fit(corpus)\n        assert vect.vocabulary_ == expected_vocabulary\n"], "sample_365": ["    def test_lazystr(self):\n        lazy_str = lazystr(\"Hello\")\n        self.assertEqual(lazy_str, \"Hello\")\n", "    def test_lazystr(self):\n        lazy_str = lazystr('text')\n        self.assertEqual(lazy_str(), 'text')\n        self.assertEqual(repr(lazy_str), repr('text'))\n\n", "    def test_lazy_bytes_cast(self):\n        lazy_obj = lazy(lambda: b'J\\xc3\\xbcst a str\\xc3\\xadng', bytes)\n        self.assertEqual(lazy_obj().__bytes__(), b'J\\xc3\\xbcst a str\\xc3\\xadng')\n", "    def test_lazy_str_caching(self):\n        lazy_obj = lazystr('test')\n        self.assertEqual(lazy_obj(), 'test')\n        self.assertEqual(lazy_obj(), 'test')\n", "    def test_lazy_object_pickle_roundtrip(self):\n        obj = lazy(lambda: {'key': 'value'}, dict)\n        pickled_obj = pickle.dumps(obj)\n        unpickled_obj = pickle.loads(pickled_obj)\n        self.assertEqual(unpickled_obj(), {'key': 'value'})\n\n"], "sample_775": ["def test_class_weights():\n    # Test rendering estimators with custom class weights\n    lr = LogisticRegression(class_weight={0: 0.2, 1: 0.8})\n    expected = \"\"\"", "compilation error", "def test_pipeline_with_named_steps():\n    # Test pretty-printing of pipelines with named steps\n    pipeline = Pipeline([\n        ('scaler', StandardScaler()),\n        ('classifier', LogisticRegression())\n    ])\n    expected = \"\"\"", "def test_sparse_matrix_repr():\n    from scipy.sparse import csr_matrix\n    lr = LogisticRegression()\n    lr.fit(csr_matrix((2, 2)), [0, 1])\n    expected = \"\"\"", "def test_changed_only_with_empty_dict():\n    lr = LogisticRegression()\n    lr.set_params()\n    set_config(print_changed_only=True)\n    expected = \"LogisticRegression()\"\n    assert lr.__repr__() == expected\n"], "sample_461": ["    def test_urlfield_clean_with_idn(self):\n        f = URLField()\n        self.assertEqual(f.clean(\"http://xn--bcher-kva.dk/\"), \"http://xn--bcher-kva.dk/\")\n\n", "    def test_urlfield_clean_ipv4_address(self):\n        f = URLField()\n        self.assertEqual(f.clean(\"http://192.168.1.1\"), \"http://192.168.1.1\")\n        self.assertEqual(f.clean(\"https://10.0.0.1:8080\"), \"https://10.0.0.1:8080\")\n\n        msg = \"'Enter a valid URL.'\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            f.clean(\"http://256.0.0.1\")\n        with self.assertRaisesMessage(ValidationError, msg):\n            f.clean(\"http://192.168.1.256\")\n", "    def test_urlfield_assume_scheme_https(self):\n        f = URLField(assume_scheme=\"https\")\n        self.assertEqual(f.clean(\"example.com\"), \"https://example.com\")\n", "    def test_urlfield_clean_protocol(self):\n        f = URLField(protocol=\"https\")\n        with self.assertRaisesMessage(ValidationError, \"'Enter a valid URL.'\"):\n            f.clean(\"http://example.com\")\n        self.assertEqual(f.clean(\"https://example.com\"), \"https://example.com\")\n        \n        f = URLField(protocol=\"http\")\n        with self.assertRaisesMessage(ValidationError, \"'Enter a valid URL.'\"):\n            f.clean(\"https://example.com\")\n        self.assertEqual(f.clean(\"http://example.com\"), \"http://example.com\")\n\n", "    def test_urlfield_assume_scheme_https(self):\n        f = URLField(assume_scheme=\"https\")\n        self.assertEqual(f.clean(\"example.com\"), \"https://example.com\")\n"], "sample_11": ["def test_coupled_world_slicing_2():\n    fits_wcs = WCS(header=COUPLED_WCS_HEADER)\n    sl = SlicedLowLevelWCS(fits_wcs, [slice(None), 0, slice(None)])\n    world = sl.pixel_to_world_values(0, 0)\n    out_pix = sl.world_to_pixel_values(world[0], world[1])\n    assert np.allclose(out_pix[0], 0)\n\n", "    def test_array_index_to_world_values_coupled(self):\n        fits_wcs = WCS(header=COUPLED_WCS_HEADER)\n        sl = SlicedLowLevelWCS(fits_wcs, 0)\n        world = fits_wcs.pixel_to_world_values(0, 0, 0)\n        out_world = sl.array_index_to_world_values(0, 0)\n        assert np.allclose(out_world, world[:2])\n", "def test_coupled_world_slicing_2d():\n\n    fits_wcs = WCS(header=COUPLED_WCS_HEADER)\n    sl = SlicedLowLevelWCS(fits_wcs, np.s_[:, 0, :])\n    world = fits_wcs.pixel_to_world_values(0,0,0)\n    out_pix = sl.world_to_pixel_values(world[0], world[1])\n\n    assert np.allclose(out_pix[0], 0)\n", "def test_coupled_world_slicing_2d():\n    fits_wcs = WCS(header=COUPLED_WCS_HEADER)\n    sl = SlicedLowLevelWCS(fits_wcs, np.s_[:, :, 0])\n    world = fits_wcs.pixel_to_world_values(50, 12, 0)\n    out_pix = sl.world_to_pixel_values(world[0], world[1])\n\n    assert np.allclose(out_pix[0], 50)\n    assert np.allclose(out_pix[1], 12)\n\n", "def test_coupled_world_slicing_2d():\n    fits_wcs = WCS(header=COUPLED_WCS_HEADER)\n    sl = SlicedLowLevelWCS(fits_wcs, np.s_[:, 0, :])\n    world = fits_wcs.pixel_to_world_values(0,0,0)\n    out_pix = sl.world_to_pixel_values(world[0], world[1])\n\n    assert np.allclose(out_pix[0], 0)\n    assert sl.pixel_n_dim == 2\n"], "sample_274": ["    def test_modelchoicefield_empty_values(self):\n        e = {\n            'required': 'REQUIRED',\n            'invalid_choice': 'INVALID CHOICE',\n        }\n        f = ModelChoiceField(queryset=ChoiceModel.objects.all(), error_messages=e)\n        self.assertFormErrors(['REQUIRED'], f.clean, '')\n        self.assertFormErrors(['INVALID CHOICE'], f.clean, ' ')\n", "    def test_modelchoicefield_empty_label(self):\n        # Create choices for the model choice field tests below.\n        ChoiceModel.objects.create(pk=1, name='a')\n        ChoiceModel.objects.create(pk=2, name='b')\n        ChoiceModel.objects.create(pk=3, name='c')\n\n        e = {\n            'required': 'REQUIRED',\n            'invalid_choice': 'INVALID CHOICE',\n        }\n        f = ModelChoiceField(queryset=ChoiceModel.objects.all(), empty_label='---------', error_messages=e)\n        self.assertFormErrors(['REQUIRED'], f.clean, '')\n        self.assertFormErrors(['INVALID CHOICE'], f.clean, '4')\n", "    def test_modelchoicefield_empty_value(self):\n        e = {\n            'required': 'REQUIRED',\n            'invalid_choice': 'INVALID CHOICE',\n        }\n        f = ModelChoiceField(queryset=ChoiceModel.objects.all(), error_messages=e)\n        self.assertFormErrors(['REQUIRED'], f.clean, '')\n        self.assertFormErrors(['INVALID CHOICE'], f.clean, None)\n\n", "    def test_modelchoicefield_empty_value(self):\n        e = {\n            'required': 'REQUIRED',\n        }\n        f = ModelChoiceField(queryset=ChoiceModel.objects.all(), error_messages=e)\n        self.assertFormErrors(['REQUIRED'], f.clean, '')\n", "    def test_modelchoicefield_empty_value(self):\n        e = {\n            'required': 'REQUIRED',\n            'invalid_choice': 'INVALID CHOICE',\n        }\n        f = ModelChoiceField(queryset=ChoiceModel.objects.all(), error_messages=e)\n        self.assertFormErrors(['REQUIRED'], f.clean, '')\n        self.assertFormErrors(['INVALID CHOICE'], f.clean, None) \n"], "sample_566": ["    def test_add_subplot_kwargs_projection():\n        fig, ax = plt.subplots(projection='polar')\n        assert ax.name == 'polar'\n        fig, ax = plt.subplots(1, 1, projection='polar')\n        assert ax.name == 'polar'\n", "    def test_subfigure_constrained_layout_with_axes_added():\n        fig = plt.figure(layout='constrained')\n        subfig = fig.subfigures(1, 1)\n        subfig.add_subplot(111)\n        assert fig.constrained_layout.w_pad == 0.05\n        assert fig.constrained_layout.h_pad == 0.05\n\n", "def test_shared_axes():\n    fig, axs = plt.subplots(2, 2, sharex=True, sharey=True)\n    axs[0, 0].set_xlim(0, 1)\n    axs[0, 0].set_ylim(0, 1)\n    axs[1, 1].set_xlim(0.5, 1.5)\n    axs[1, 1].set_ylim(0.5, 1.5)\n    for ax in axs.flat:\n        assert ax.get_xlim() == (0, 1)\n        assert ax.get_ylim() == (0, 1)\n", "    def test_subfigure_tight_layout():\n        fig = plt.figure(figsize=(4, 4))\n        subfigs = fig.subfigures(2, 1)\n        subfigs[0].add_subplot(111).set(xlabel='x-label', ylabel='y-label',\n                                     title='Subfigure 1')\n        subfigs[1].add_subplot(111).set(xlabel='x-label', ylabel='y-label',\n                                     title='Subfigure 2')\n        fig.tight_layout()\n        fig.canvas.draw()\n        renderer = fig.canvas.get_renderer()\n        bbox = subfigs[1].get_tightbbox(renderer=renderer)\n        assert bbox.y0 > 0\n\n", "def test_subfigure_share_axes():\n    fig = plt.figure()\n    gs = fig.add_gridspec(2, 2)\n    ax1 = fig.add_subplot(gs[0, 0])\n    ax2 = fig.subfigures(1, 1, sharex=ax1, sharey=ax1).subplots(1, 1)\n    ax3 = fig.add_subplot(gs[1, :], sharex=ax1, sharey=ax1)\n\n    for ax in [ax1, ax2[0], ax3]:\n        ax.scatter([1, 2], [1, 2])\n\n    assert ax1 == ax2[0].xaxis.shared_axes[0]._axes\n    assert ax1 == ax3.xaxis.shared_axes[0]._axes\n    assert ax1 == ax2[0].yaxis.shared_axes[0]._axes\n    assert ax1 == ax3.yaxis.shared_axes[0]._axes\n"], "sample_191": ["    def test_watch_already_existing_file(self, mocked_notify, mocked_exists):\n        mocked_exists.return_value = True\n        self.reloader.watch_dir(self.tempdir, '*.py')\n        self.increment_mtime(self.existing_file)\n        with self.tick_twice():\n            pass\n        self.assertEqual(mocked_notify.call_count, 0)\n", "    def test_get_file_mtime(self):\n        self.increment_mtime(self.existing_file)\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            mtime = self.reloader.get_file_mtime(self.existing_file)\n        self.assertEqual(mtime, self.existing_file.stat().st_mtime)\n", "    def test_notify_file_changed_with_nonexistent_file(self):\n        with self.assertRaises(FileNotFoundError):\n            self.reloader.notify_file_changed(self.nonexistent_file)\n", "    def test_watch_dir_with_relative_path(self):\n        relative_path = Path('relative_path')\n        self.reloader.watch_dir(relative_path, '*.py')\n        watched_files = list(self.reloader.watched_files())\n        self.assertEqual(watched_files, [])\n", "    def test_watch_dir_with_non_existent_path(self):\n        path = Path('non_existent_directory')\n        self.reloader.watch_dir(path, '**/*.py')\n        watched_files = list(self.reloader.watched_files())\n        self.assertCountEqual(watched_files, [])\n"], "sample_581": ["compilation error", "def test_blueprint_url_defaults(app, client):\n    bp = flask.Blueprint(\"bp\", __name__)\n\n    @bp.route(\"/user/<username>\")\n        return f\"User: {username}\"\n\n    @bp.app_url_defaults\n        values.setdefault(\"lang\", \"en\")\n\n    app.register_blueprint(bp, url_prefix=\"/api\")\n    assert client.get(\"/api/user/john\").data == b\"User: john\"\n    assert client.get(\"/api/user/john?lang=fr\").data == b\"User: john\"\n", "    def sub_index():\n        return \"Sub blueprint index\"\n", "compilation error", "    def test_multilevel_blueprint_name_url_generation(app, client):\n        parent = flask.Blueprint('parent', __name__)\n        child1 = flask.Blueprint('child1', __name__)\n        child2 = flask.Blueprint('child2', __name__)\n\n        @parent.route('/')\n            return 'Parent'\n\n        @child1.route('/')\n            return 'Child 1'\n\n        @child2.route('/')\n            return 'Child 2'\n\n        parent.register_blueprint(child1, url_prefix='/c1')\n        parent.register_blueprint(child2, url_prefix='/c2')\n\n        app.register_blueprint(parent, url_prefix='/p')\n\n        assert client.get('/p/c1/').data == b'Child 1'\n        assert client.get('/p/c2/').data == b'Child 2'\n        \n        assert url_for('parent.child1.child1_index') == '/p/c1/'\n        assert url_for('parent.child2.child2_index') == '/p/c2/'\n\n"], "sample_949": ["def test_custom_man_pages(app, status, warning):\n    app.builder.build_all()\n    assert (app.outdir / 'myproject.1').exists()\n    assert (app.outdir / 'myotherpage.8').exists()\n", "def test_custom_man_pages(app, status, warning):\n    app.build()\n    assert (app.outdir / 'sphinx-manpage.1').exists()\n", "def test_toctree(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n    assert 'Example Domain' in content\n", "def test_man_pages_config(app, status, warning):\n    app.build()\n    assert (app.outdir / 'test.1').exists()\n", "def test_domain_index(app, status, warning):\n    app.build()\n    assert (app.outdir / 'man1' / 'py-modindex.1').exists()\n\n"], "sample_246": ["    def test_makemessages_with_custom_template_ext(self):\n        with override_settings(TEMPLATES=[{\n            'BACKEND': 'django.template.backends.django.DjangoTemplates',\n            'DIRS': [os.path.join(self.test_dir, 'templates')],\n            'APP_DIRS': True,\n            'OPTIONS': {\n                'context_processors': [\n                    'django.template.context_processors.debug',\n                    'django.template.context_processors.request',\n                    'django.contrib.auth.context_processors.auth',\n                    'django.contrib.messages.context_processors.messages',\n                ],\n                'debug': True,\n                'loaders': [\n                    ('django.template.loaders.filesystem.Loader', [\n                        os.path.join(self.test_dir, 'templates'),\n                    ]),\n                    ('django.template.loaders.app_directories.Loader', [\n                        'django',\n                    ]),\n                ],\n            },\n        }]):\n            management.call_command('makemessages', locale=[LOCALE], verbosity=0, template_ext=['html', 'my_template'])\n            self.assertTrue(os.path.exists(self.PO_FILE))\n            with open(self.PO_FILE) as fp:\n                po_contents = fp.read()\n                self.assertMsgId('Translatable literal #1', po_contents)\n                self.assertMsgId('Translatable literal #2', po_contents)\n", "    def test_javascript_extraction_with_babel(self):\n        with override_settings(\n            INSTALLED_APPS=[\n                'django.contrib.admin',\n                'django.contrib.auth',\n                'django.contrib.contenttypes',\n                'django.contrib.sessions',\n                'django.contrib.messages',\n                'django.contrib.staticfiles',\n                'tests.testapp',\n            ],\n            STATIC_ROOT='static',\n            WEBPACK_LOADER={\n                'DEFAULT': {\n                    'BUNDLE_DIR_NAME': 'bundles/',\n                    'STATS_FILE': os.path.join(BASE_DIR, 'webpack-stats.json'),\n                    'ORDERING': {\n                        'style': ['css'] \n                    }\n                }\n            },\n        ):\n            management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n            self.assertTrue(os.path.exists(self.PO_FILE))\n            with open(self.PO_FILE) as fp:\n                po_contents = fp.read()\n                self.assertMsgId('This literal should be included in bundled JS', po_contents)\n", "    def test_makemessages_custom_template_tag(self):\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertMsgId('Translatable literal from a custom template tag', po_contents)\n", "    def test_custom_domain(self):\n        management.call_command('makemessages',\n                                locale=[LOCALE],\n                                domain='custom_domain',\n                                verbosity=0)\n        self.assertTrue(os.path.exists('locale/%s/LC_MESSAGES/custom_domain.po' % LOCALE))\n        with open('locale/%s/LC_MESSAGES/custom_domain.po' % LOCALE) as fp:\n            po_contents = fp.read()\n            self.assertMsgId('Translatable literal #1', po_contents)\n", "    def test_makemessages_no_settings_with_project_locale_paths(self):\n        with override_settings(LOCALE_PATHS=[self.test_dir]):\n            out, err = self.run_django_admin(['makemessages', '-l', 'en', '-v', '0'])\n            self.assertNoOutput(err)\n            self.assertNoOutput(out)\n"], "sample_865": ["    def test_max_depth_pruning_path(self):\n        # Create a deep tree and check if pruning path limits the depth\n        X = np.array([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4]])\n        y = np.array([0, 1, 0, 1, 0])\n        dt = DecisionTreeClassifier(random_state=0)\n\n        # Fit a tree with unlimited depth\n        dt.fit(X, y)\n        initial_depth = dt.tree_.max_depth\n\n        # Create a pruning path for a limited depth\n        max_depth = 3\n        ccp_alphas, impurities = dt.cost_complexity_pruning_path(X, y)\n\n        # Check if any alpha value in the path limits the depth to the desired value\n\n        pruned_depths = [\n            DecisionTreeClassifier(ccp_alpha=alpha, random_state=0).fit(\n                X, y\n            ).tree_.max_depth\n            for alpha in ccp_alphas\n        ]\n        assert any([depth == max_depth for depth in pruned_depths])\n\n", "    def test_multioutput_predict_proba(self):\n        from sklearn.datasets import make_multilabel_classification\n\n        X, y = make_multilabel_classification(n_samples=100, n_features=20,\n                                            n_classes=5, random_state=0)\n        clf = DecisionTreeClassifier(random_state=0)\n        clf.fit(X, y)\n\n        # predict_proba should return the probability of each class for each sample\n        proba = clf.predict_proba(X)\n        assert proba.shape == (100, 5)\n        assert_allclose(proba.sum(axis=1), np.ones(100))\n", "    def test_predict_proba_nan_handling(self):\n        # Check predict_proba handles NaN values in the input data.\n        X = np.array([[1, 2], [3, np.nan], [5, 6]])\n        y = np.array([0, 1, 0])\n        clf = DecisionTreeClassifier(random_state=0).fit(X, y)\n\n        result = clf.predict_proba(X)\n        assert not np.isnan(result).any()\n\n        # Test with sparse input\n        X_sparse = csr_matrix(X)\n        result_sparse = clf.predict_proba(X_sparse)\n        assert_array_almost_equal(result, result_sparse)\n", "    def test_predict_proba_multiclass_empty_leaf(self):\n        # Test case for issue #9074: predict_proba should not raise error when\n        # encountering empty leaf nodes in multi-class classification\n        X = np.array([[0, 0], [1, 1], [2, 2], [3, 3]]).astype(float)\n        y = np.array([0, 1, 0, 1])\n\n        clf = DecisionTreeClassifier(criterion='gini',\n                                     max_depth=2, random_state=0).fit(X, y)\n        clf.tree_.children_left[3] = TREE_LEAF\n        clf.tree_.children_right[3] = TREE_LEAF\n\n        proba = clf.predict_proba(X)\n        assert len(proba) == len(X)\n        assert proba.shape[1] == len(np.unique(y))\n        assert np.allclose(proba.sum(axis=1), 1)\n\n        # Test case when all samples are in the same leaf node\n        clf = DecisionTreeClassifier(criterion='gini',\n                                     max_depth=1, random_state=0).fit(X, y)\n        clf.tree_.children_left[1] = TREE_LEAF\n        clf.tree_.children_right[1] = TREE_LEAF\n\n        proba = clf.predict_proba(X)\n        assert len(proba) == len(X)\n        assert proba.shape[1] == len(np.unique(y))\n        assert np.allclose(proba.sum(axis=1), 1)\n", "    def test_feature_importances_sparse(self):\n        # Test feature importance calculation for sparse input\n        # with regression\n\n        n_samples = 100\n        n_features = 5\n        X = csr_matrix(np.random.rand(n_samples, n_features))\n        y = np.random.rand(n_samples)\n\n        regressor = DecisionTreeRegressor(random_state=0)\n        regressor.fit(X, y)\n\n        importances = regressor.feature_importances_\n        assert_array_almost_equal(\n            importances, regressor.tree_.compute_feature_importances()\n        )\n        assert np.sum(importances) == 1.0\n\n        # Check for correct shapes\n        assert importances.shape == (n_features,)\n\n"], "sample_1159": ["    def test_assumptions_with_symbols_of_different_types():\n        x = Symbol('x', integer=True)\n        y = Symbol('y', real=True)\n        z = Symbol('z', complex=True)\n        assert assumptions(x + y) == {'commutative': True, 'real': True}\n        assert assumptions(x * z) == {'algebraic': True, 'commutative': True,\n        'complex': True, 'finite': True}\n", "compilation error", "compilation error", "compilation error", "def test_issue_16313_complex():\n    x = Symbol('x', extended_real=False)\n    k = Symbol('k', complex=True)\n    l = Symbol('l', complex=True, zero=False)\n    assert (-x).is_real is False\n    assert (k*x).is_real is None\n    assert (l*x).is_real is False\n    assert (l*x*x).is_real is None\n    assert (-x).is_positive is False\n"], "sample_965": ["def test_signature_from_str_union_operator():\n    signature = '(a: int | str, b: float | None)'\n    sig = inspect.signature_from_str(signature)\n    assert list(sig.parameters.keys()) == ['a', 'b']\n    assert sig.parameters['a'].annotation == 'int | str'\n    assert sig.parameters['b'].annotation == 'float | None'\n", "def test_getargspec_with_default_values():\n        pass\n\n    spec = inspect.getargspec(func)\n    assert spec.args == ['a', 'b', 'c']\n    assert spec.varargs == 'args'\n    assert spec.keywords == 'kwargs'\n    assert spec.defaults == (1, 2, 3)\n", "    def test_getclosurevars_of_lambda():\n        f = lambda x: x + 1\n        closure = inspect.getclosurevars(f)\n        assert closure.globals is None\n        assert closure.builtins is None\n        assert closure.nonlocals is None\n        assert closure.frees is {}\n", "        def __init__(self, value):\n            self._value = value\n", "def test_getsource_of_builtin(app):\n    assert inspect.getsource(len) is None\n    assert inspect.getsource(object.__init__) is None\n"], "sample_151": ["    def test_unique_together_removal(self):\n        \"\"\"\n        #23968 - Removing unique_together constraint should be handled.\n        \"\"\"\n        class Book(models.Model):\n            title = models.CharField(max_length=200)\n            author = models.ForeignKey(Author, on_delete=models.CASCADE)\n\n            class Meta:\n                unique_together = ('title', 'author')\n\n        class Author(models.Model):\n            name = models.CharField(max_length=200)\n\n        class BookNoUnique(models.Model):\n            title = models.CharField(max_length=200)\n            author = models.ForeignKey(Author, on_delete=models.CASCADE)\n\n        book_meta = ModelState(\"testapp\", \"Book\", [\n            ('title', models.CharField(max_length=200)),\n            ('author', models.ForeignKey('testapp.Author', models.CASCADE)),\n        ],\n            options={'unique_together': ('title', 'author')})\n        book_no_unique_meta = ModelState(\"testapp\", \"Book\", [\n            ('title', models.CharField(max_length=200)),\n            ('author', models.ForeignKey('testapp.Author', models.CASCADE)),\n        ])\n\n        author = ModelState(\"testapp\", \"Author\", [\n            ('name', models.CharField(max_length=200)),\n        ])\n        changes = self.get_changes([author, book_meta], [author, book_no_unique_meta])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['AlterModelOptions'])\n\n", "    def test_add_field_to_mti_model(self):\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n        ])\n        Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n        DogWithColor = ModelState('app', 'Dog', [\n            ('color', models.CharField(max_length=20)),\n        ], bases=('app.Animal',))\n        changes = self.get_changes([Animal, Dog], [Animal, DogWithColor])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['AddField'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='color', model_name='Dog') \n", "    def test_mti_inheritance_field_removal(self):\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n            ('name', models.CharField(max_length=100)),\n        ])\n        Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n        changes = self.get_changes([Animal, Dog], [Animal])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['RemoveField'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='name', model_name='Animal')\n", "    def test_mti_inheritance_model_rename(self):\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n        ])\n        Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n        Creature = ModelState('app', 'Creature', [], bases=('app.Animal',))\n        changes = self.get_changes([Animal, Dog], [Creature, Dog])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['RenameModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, old_name='Animal', new_name='Creature')\n\n", "    def test_swappable_dependency(self):\n        \"\"\"\n        #23322 - The dependency resolver correctly handles dependency on\n        swappable models.\n        \"\"\"\n        with isolate_lru_cache(apps.get_swappable_settings_name):\n            author = ModelState(\"a\", \"Author\", [\n                (\"id\", models.AutoField(primary_key=True)),\n            ])\n            book = ModelState(\"b\", \"Book\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"author\", models.ForeignKey(settings.AUTH_USER_MODEL, models.CASCADE)),\n            ])\n            changes = self.get_changes([], [author, book])\n\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'a', 1)\n        self.assertOperationTypes(changes, 'a', 0, [\"CreateModel\"])\n        self.assertMigrationDependencies(changes, 'a', 0, [])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'b', 1)\n        self.assertOperationTypes(changes, 'b', 0, [\"CreateModel\"])\n        self.assertMigrationDependencies(changes, 'b', 0, [('__setting__', 'AUTH_USER_MODEL'), ('a', 'auto_1')])\n       \n"], "sample_84": ["    def test_parse_http_date_invalid(self):\n        with self.assertRaises(ValueError):\n            parse_http_date('invalid date string')\n", "    def test_parse_http_date_invalid(self):\n        with self.assertRaises(ValueError):\n            parse_http_date('Mon, 31 Feb 2007 01:54:21 GMT')\n", "    def test_parse_http_date_invalid_date(self):\n        with self.assertRaises(ValueError):\n            parse_http_date('Invalid Date String')\n", "    def test_parse_http_date_invalid_format(self):\n        with self.assertRaises(ValueError):\n            parse_http_date('Invalid date string')\n", "    def test_parse_http_date_with_microseconds(self):\n        parsed = parse_http_date('Sun, 06 Nov 1994 08:49:37.123456 GMT')\n        self.assertEqual(datetime.utcfromtimestamp(parsed), datetime(1994, 11, 6, 8, 49, 37, 123456))\n"], "sample_886": ["def test_set_output_transform_error():\n    \"\"\"Check that setting transform output to an invalid value raises ValueError.\"\"\"\n    est = EstimatorWithSetOutput()\n    with pytest.raises(ValueError, match=\"output config must be 'default'\"):\n        est.set_output(transform=\"invalid\")\n\n", "def test__wrap_data_with_container_no_get_feature_names_out():\n    \"\"\"Check _wrap_data_with_container when get_feature_names_out is None.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n    est = EstimatorNoSetOutputWithTransform()\n\n    with pytest.raises(ValueError) as excinfo:\n        _wrap_data_with_container(\"transform\", X, X, est)\n    assert \"Estimator has no `get_feature_names_out` method\" in str(excinfo.value)\n", "def test_set_output_pandas_with_empty_dataframe():\n    \"\"\"Check set_output works correctly with an empty DataFrame.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    est = EstimatorWithSetOutput().set_output(transform=\"pandas\")\n    X_empty = pd.DataFrame(columns=['a', 'b', 'c'])\n    X_trans = est.transform(X_empty)\n    assert isinstance(X_trans, pd.DataFrame)\n    assert X_trans.empty\n", "def test__wrap_data_with_container_no_transform():\n    \"\"\"Check that no wrapping happens if estimator doesn't have transform.\"\"\"\n\n    pd = pytest.importorskip(\"pandas\")\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n\n    class EstimatorNoTransform:\n        pass\n\n    est = EstimatorNoTransform()\n    output = _wrap_data_with_container(\n        \"transform\", data_to_wrap=X, original_input=X, estimator=est\n    )\n\n    assert output is X\n", "def test__SetOutputMixin_auto_wrap_output_keys_empty():\n    \"\"\"Checks _SetOutputMixin when auto_wrap_output_keys is an empty tuple.\"\"\"\n    class EstimatorWithEmptyAutoWrapKeys(_SetOutputMixin, auto_wrap_output_keys=()):\n            return X\n\n    est = EstimatorWithEmptyAutoWrapKeys()\n    assert not hasattr(est, \"set_output\")\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n    assert X is est.transform(X)\n"], "sample_1056": ["def test_NumExprPrinter_basic():\n    ne = NumExprPrinter()\n    assert ne.doprint(x + y) == \"x + y\"\n    assert ne.doprint(x*y) == \"x * y\"\n    assert ne.doprint(x**y) == \"x**y\"\n", "def test_lambdarepr_boolean():\n    assert lambdarepr(True) == \"True\"\n    assert lambdarepr(False) == \"False\"\n    assert lambdarepr(x and y) == \"(x and y)\"\n    assert lambdarepr(x or y) == \"(x or y)\"\n    assert lambdarepr(not x) == \"(not (x))\"\n", "    def test_ITE():\n        p = Piecewise((x, x < 0), (x**2, True))\n        l = lambdarepr(p)\n        eval(h + l)\n        assert l == \"((x) if (x < 0) else (x**2))\"\n\n", "def test_boolean():\n    assert lambdarepr(x & y) == \"(x and y)\"\n    assert lambdarepr(x | y) == \"(x or y)\"\n    assert lambdarepr(~x) == \"(not (x))\"\n    assert lambdarepr(True) == \"True\"\n    assert lambdarepr(False) == \"False\"\n", "def test_ITE():\n    p = Piecewise((x, x > 0), (0, True))\n    assert lambdarepr(p) == \"((x) if (x > 0) else (0))\"\n"], "sample_599": ["def test_unsigned_integer_coding(dtype):\n    data = np.array([0, 127, 128, 255], dtype=dtype)\n    original = xr.Variable((\"x\",), data)\n    coder = variables.UnsignedIntegerCoder()\n    encoded = coder.encode(original)\n    assert encoded.dtype == np.dtype(f\"u{dtype[1:]}\")\n    roundtripped = coder.decode(encoded)\n    assert_identical(original, roundtripped)\n\n", "def test_CFScaleOffsetCoder_encode_scale_factor_int():\n    original = xr.Variable((\"x\",), np.arange(10.0), encoding={\"scale_factor\": 10})\n    coder = variables.CFScaleOffsetCoder()\n    encoded = coder.encode(original)\n    assert encoded.dtype == np.float32\n\n", "    def test_CFMaskCoder_encode_decode_with_unsigned():\n        data = np.array([0, 1, 65535, 65536], dtype=np.uint16)\n        original = xr.Variable((\"x\",), data, {\"_FillValue\": 65535})\n        coder = variables.CFMaskCoder()\n        encoded = coder.encode(original)\n        decoded = coder.decode(encoded)\n        assert_identical(original, decoded)\n\n", "compilation error", "def test_UnsignedIntegerCoder_encode_decode():\n    data = np.array([0, 1, 2, 3, 4], dtype=np.uint8)\n    original = xr.Variable((\"x\",), data)\n    original.attrs[\"_Unsigned\"] = \"true\"\n    coder = variables.UnsignedIntegerCoder()\n    encoded = coder.encode(original)\n    roundtripped = coder.decode(encoded)\n    assert_identical(original, roundtripped)\n"], "sample_683": ["def test_capture_with_log_level_warning(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.warning(\"This is a warning\")\n            assert \"This is a warning\" in caplog.text\n\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    assert result.ret == 0\n", "def test_capture_with_null_fd(testdir):\n    \"\"\"Issue #7153: Support capturing when stdin/stdout/stderr are replaced with /dev/null\"\"\"\n    p = testdir.makepyfile(\n        \"\"\"\n        import sys\n\n            print(\"hello stdout\")\n            sys.stderr.write(\"hello stderr\")\n            assert sys.stdin.isatty() is False\n\n    \"\"\"\n    )\n    result = testdir.runpytest_subprocess(p)\n    assert result.ret == 0\n    assert result.stdout.str() == \"\"\n    assert result.stderr.str() == \"hello stderr\"\n", "def test_capture_with_python_stderr_redirection(testdir):\n    \"\"\"Test that using sys.stderr.write() within a captured context works with redirected stderr.\"\"\"\n    p = testdir.makepyfile(\n        \"\"\"\n        import sys\n        import os\n\n            # Redirect stderr to stdout\n            sys.stderr = sys.stdout\n\n            # Write to stderr (which is now stdout)\n            print(\"This should appear in captured stdout\", file=sys.stderr)\n\n            # Write to stdout normally\n            print(\"This should also appear in captured stdout\")\n    \"\"\"\n    )\n    result = testdir.runpytest_subprocess(p, \"--capture=sys\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*This should appear in captured stdout*\",\n            \"*This should also appear in captured stdout*\",\n        ]\n    )\n\n\n", "    def test_stderr_write_with_error_encoding(capsys):\n        sys.stderr = capture.EncodedFile(sys.stderr, encoding=\"latin-1\")\n        try:\n            print(\"hello\", file=sys.stderr)\n            print(\"caf\u00e9\", file=sys.stderr)\n        finally:\n            sys.stderr = sys.__stderr__\n        out, err = capsys.readouterr()\n        assert err == \"hello\\ncaf\\xe9\"\n", "    def test_capture_log_file(testdir):\n        testdir.makepyfile(\n            \"\"\"\n        import logging\n        import sys\n\n        logging.basicConfig(filename='app.log', level=logging.DEBUG)\n\n            logging.debug('This message should go to the log file')\n            print('This message should go to stdout')\n            assert 1 == 1\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        assert 'This message should go to the log file' in open('app.log').read()\n        assert result.stdout.str() == \"*1 passed*\\n\"\n\n"], "sample_942": ["def test_pyfunction_overloaded_signature(app):\n    text = (\".. py:function:: func(a, b)\\n\"\n            \".. py:function:: func(a, b, c)\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"func\"],\n                                                     [desc_parameterlist, ([desc_parameter, \"a\"],\n                                                                           [desc_parameter, \"b\"])])],\n                                  desc_content)],\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"func\"],\n                                                     [desc_parameterlist, ([desc_parameter, \"a\"],\n                                                                           [desc_parameter, \"b\"],\n                                                                           [desc_parameter, \"c\"])])],\n                                  desc_content)]))\n    assert_node(doctree[1][0], ([desc_name, \"func\"],\n                [desc_parameterlist, ([desc_parameter, \"a\"],\n                                      [desc_parameter, \"b\"])]))\n    assert_node(doctree[2][0], ([desc_name, \"func\"],\n                [desc_parameterlist, ([desc_parameter, \"a\"],\n                                      [desc_parameter, \"b\"],\n                                      [desc_parameter, \"c\"])]))\n", "compilation error", "def test_pyobject_return_type(app):\n    text = (\".. py:function:: func\\n\"\n            \"   :return: str\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"func\"],\n                                                     [desc_parameterlist, ()])],\n                                  [desc_content, (nodes.paragraph,\n                                  nodes.Text,\n                                  \"Return type: \",\n                                  [pending_xref, addnodes.literal_emphasis, 'str'],\n                                  nodes.Text,\n                                  \".\")])]))\n    assert_node(doctree[1][1][1][1][1][1], pending_xref, **{\"py:class\": \"str\"})\n    assert 'func' in domain.objects\n    assert domain.objects['func'] == ('index', 'func', 'function', False)\n", "def test_pyclass_empty(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:method:: meth1\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc,\n                                                  desc)])]))\n\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'Class (built-in class)', 'Class', '', None)])\n    assert 'Class' in domain.objects\n    assert domain.objects['Class'] == ('index', 'Class', 'class', False)\n", "compilation error"], "sample_680": ["def test_importorskip_module_path(testdir):\n    testdir.makepyfile(\n        **{\n            \"some/sub/module.py\": \"def foo(): pass\",\n        }\n    )\n    p = testdir.makepyfile(test_rootdir=\"\"\"\n        import pytest\n        pytest.importorskip(\"some.sub.module\")\n\n            some.sub.module.foo()\n    \"\"\")\n    result = testdir.runpytest(p, \"--rootdir=.\")\n    result.assertoutcomes(passed=1)\n", "    def test_skip_if_fixture(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.fixture(scope='module')\n                pytest.skip('skipping the fixture')\n                return 'fixture value'\n\n                assert 0\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines([\"*SKIP*skipping the fixture*\", \"*1 skipped*\"])\n\n", "    def test_skip_for_reason_empty_string(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.skip('')\n                assert False\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines([\"*1 skipped*\"])\n", "    def test_skipif_condition_string(self, testdir):\n        testdir.makepyfile(\n            test_one=\"\"\"\n            import pytest\n            @pytest.mark.skipif(\"sys.platform == 'win32'\")\n                assert 0\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines([\"*1 skipped*\"])\n", "    def test_skip_with_reason_in_fixture(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            @pytest.fixture\n                pytest.skip(\"fixture skip reason\")\n                pass\n        \"\"\"\n        )\n        result = testdir.runpytest(\"-rs\")\n        result.stdout.fnmatch_lines(\n            [\"*SKIP*fixture skip reason*\", \"*1 skipped*\"]\n        )\n\n"], "sample_853": ["def test_transform_target_regressor_pipeline_with_transformer():\n    X, y = friedman\n    transformer = StandardScaler()\n    regr = Pipeline(steps=[('transformer', transformer),\n                           ('regressor', LinearRegression())])\n    tt = TransformedTargetRegressor(regressor=regr)\n    tt.fit(X, y)\n    y_pred = tt.predict(X)\n    assert y.shape == y_pred.shape\n", "def test_transform_target_regressor_copy_regressor():\n    X, y = friedman\n    regr1 = LinearRegression()\n    regr2 = clone(regr1)\n    tt = TransformedTargetRegressor(regressor=regr1)\n    tt.fit(X, y)\n    assert regr1 is not tt.regressor_\n    assert regr1.coef_.ravel() == tt.regressor_.coef_.ravel()\n    assert regr2.coef_.ravel() == tt.regressor_.coef_.ravel()\n\n\n", "def test_transform_target_regressor_pipeline_transformer():\n    X, y = friedman\n    transformer = StandardScaler()\n    regr = TransformedTargetRegressor(regressor=LinearRegression(),\n                                     transformer=transformer)\n    estimators = [\n        ('transformer', transformer), ('regressor', regr)\n    ]\n    pipeline = Pipeline(estimators)\n    pipeline.fit(X, y)\n    assert pipeline.named_steps['regressor'].transformer_ is transformer\n    assert_allclose(\n        pipeline.named_steps['regressor'].regressor_.coef_,\n        pipeline.named_steps['regressor'].regressor_.regressor_.coef_)\n\n", "def test_check_inverse_warning():\n    X, y = friedman\n    regr = TransformedTargetRegressor(regressor=LinearRegression(),\n                                     func=np.log, inverse_func=np.exp)\n    with pytest.warns(UserWarning, match=\"The provided functions or transformer\"\n                       \" are not strictly inverse of each other\"):\n        regr.fit(X, y)\n", "    def test_transform_target_regressor_with_sample_weights():\n        X, y = friedman\n        regr = TransformedTargetRegressor(regressor=LinearRegression())\n        sample_weight = np.ones((y.shape[0],))\n        regr.fit(X, y, sample_weight=sample_weight)\n        assert regr.regressor_.coef_.shape == (1,)\n\n"], "sample_1123": ["def test_ConditionSet_empty():\n    assert ConditionSet(x, False, S.Reals) == S.EmptySet\n    assert ConditionSet(x, False, FiniteSet(1, 2, 3)) == S.EmptySet\n    assert ConditionSet(x, True, S.EmptySet) == S.EmptySet\n", "compilation error", "def test_setintersection():\n    a = ConditionSet(x, x < 1, Interval(0, 3))\n    b = ConditionSet(x, x > 2, Interval(0, 4))\n    assert Intersection(a, b) == ConditionSet(x, And(x < 1, x > 2), Interval(0, 3))\n    assert Intersection(a, S.EmptySet) == S.EmptySet\n    assert Intersection(S.EmptySet, b) == S.EmptySet\n", "def test_ConditionSet_emptyset():\n    assert ConditionSet(x, False, S.Reals) == S.EmptySet\n    assert ConditionSet(x, True, S.EmptySet) == S.EmptySet\n    assert ConditionSet(x, x > 0, S.EmptySet) == S.EmptySet\n", "def test_ConditionSet_empty_base_set():\n    assert ConditionSet(x, x > 0, S.EmptySet) == S.EmptySet\n    assert ConditionSet(x, x < 0, S.EmptySet) == S.EmptySet\n    assert ConditionSet(x, x == 0, S.EmptySet) == S.EmptySet\n"], "sample_1149": ["def test_sympify_singleton():\n    assert S(1) is S.One\n    assert S(\"1/2\") is S.Half\n", "    def test_singleton_sympify():\n        from sympy import sympify\n        assert sympify('S.Zero') is S.Zero\n        assert sympify('Zero') is S.Zero\n", "def test_registration_during_import():\n    class TestSingletonDuringImport(Basic, metaclass=Singleton):\n        pass\n\n    assert TestSingletonDuringImport() is S.TestSingletonDuringImport\n\n", "def test_S_access():\n    assert S('1/2') is S.Half\n    assert S('oo') is S.Infinity\n    assert S('-oo') is S.NegativeInfinity\n", "def test_S_call():\n    assert S(1) == 1\n    assert S('1/2') == Rational(1, 2)\n"], "sample_148": ["    def test_get_model_from_relation(self):\n        event = Event.objects.create()\n        guide = EventGuide.objects.create(event=event)\n        self.assertEqual(get_model_from_relation(guide._meta.get_field('event')), Event)\n", "    def test_get_model_from_relation(self):\n        model = get_model_from_relation(Location._meta.get_field('event'))\n        self.assertEqual(model, Event)\n", "    def test_display_for_field_empty_string(self):\n        self.assertEqual(display_for_field(\"\", models.CharField(), self.empty_value), \"\")\n        self.assertEqual(display_for_field(\"\", models.TextField(), self.empty_value), \"\")\n", "    def test_get_model_from_relation(self):\n        self.assertEqual(get_model_from_relation(Article._meta.get_field('site')), Site)\n        self.assertEqual(get_model_from_relation(Location._meta.get_field('event')), Event)\n        self.assertEqual(get_model_from_relation(Event._meta.get_field('location')), Location)\n\n        with self.assertRaises(NotRelationField):\n            get_model_from_relation(Article._meta.get_field('title'))\n", "    def test_related_objects_get_fields(self):\n        self.assertEqual(get_fields_from_path(Article, \"location.event\"), \n                         [Article._meta.get_field(\"location\"), Location._meta.get_field(\"event\")])\n\n"], "sample_1179": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_issue_23739():\n    f = Function('f')\n    assert str(f(x).diff(x)) == \"Derivative(f(x), x)\"\n"], "sample_264": ["    def test_empty_message_store(self):\n        storage = self.get_storage()\n        response = self.get_response()\n        storage.update(response)\n        self.assertEqual(stored_cookie_messages_count(storage, response), 0)\n", "    def test_get_empty_cookie(self):\n        storage = self.get_storage()\n        response = self.get_response()\n        self.assertEqual(storage._decode(response.cookies.get(storage.cookie_name, '' ).value), None)\n", "    def test_message_with_tags(self):\n        storage = self.get_storage()\n        message = Message(constants.INFO, 'test', extra_tags='important')\n        storage.add(constants.INFO, message)\n        response = self.get_response()\n        storage.update(response)\n        self.assertEqual(response.cookies['messages'].value, json.dumps(\n            ['__json_message', 1, constants.INFO, 'test', ['important']],\n            separators=(',', ':'),\n            cls=MessageEncoder,\n        ).encode('latin-1'))\n", "    def test_multiple_messages_encoded_messages(self):\n        storage = self.storage_class(self.get_request())\n        messages = [\n            Message(constants.INFO, 'test'),\n            Message(constants.WARNING, 'test2'),\n            Message(constants.ERROR, 'test3'),\n        ]\n        set_cookie_data(storage, messages)\n\n        # The message contains what's expected.\n        self.assertEqual(list(storage), messages)\n", "    def test_empty_cookie(self):\n        storage = self.storage_class(self.get_request())\n        set_cookie_data(storage, [], encode_empty=True)\n        self.assertEqual(storage._decode(storage.request.COOKIES.get(CookieStorage.cookie_name)), [])\n"], "sample_167": ["    def test_naturaltime_past_future_consistency(self):\n        test_list = [\n            now - datetime.timedelta(days=1),\n            now + datetime.timedelta(days=1),\n        ]\n        result_list_plural = ['p\u0159ed 1\\xa0dnem', 'za 1\\xa0den']\n        result_list_singular = ['v\u010dera', 'z\u00edtra']\n\n        with translation.override('cs'), self.settings(USE_L10N=True):\n            self.humanize_tester(test_list, result_list_plural, 'naturaltime')\n            # Test for specific singular forms \n            self.humanize_tester(test_list, result_list_singular, 'naturaltime') \n", "    def test_naturaltime_past_future_singular_plural(self):\n        class MockDateTime(datetime.datetime):\n            @classmethod\n                return datetime.datetime(2023, 4, 5, 10, 0, tzinfo=utc)\n\n        orig_humanize_datetime, humanize.datetime = humanize.datetime, MockDateTime\n        try:\n            with translation.override('en'):\n                test_list = [\n                    \n                    MockDateTime.now() - datetime.timedelta(seconds=1),\n                    MockDateTime.now() + datetime.timedelta(seconds=1),\n                    MockDateTime.now() - datetime.timedelta(minutes=1),\n                    MockDateTime.now() + datetime.timedelta(minutes=1),\n                    MockDateTime.now() - datetime.timedelta(hours=1),\n                    MockDateTime.now() + datetime.timedelta(hours=1),\n                ]\n                result_list = [\n                    'a second ago',\n                    'a second from now',\n                    'a minute ago',\n                    'a minute from now',\n                    'an hour ago',\n                    'an hour from now', \n                ]\n                self.humanize_tester(test_list, result_list, 'naturaltime')\n\n        finally:\n            humanize.datetime = orig_humanize_datetime\n", "    def test_naturaltime_overflow(self):\n        class OverflowMockDateTime(datetime.datetime):\n            @classmethod\n                if tz is None or tz.utcoffset(now) is None:\n                    return now\n                else:\n                    return now.replace(tzinfo=tz) + tz.utcoffset(now)\n\n        now_plus_1000_years = now + datetime.timedelta(days=365 * 1000)\n        orig_humanize_datetime = humanize.datetime\n        humanize.datetime = OverflowMockDateTime\n        try:\n            with translation.override('en'):\n                self.assertEqual(humanize.naturaltime(now_plus_1000_years),\n                                '1,000 years from now')\n        finally:\n            humanize.datetime = orig_humanize_datetime\n\n", "    def test_naturaltime_plural_boundaries(self):\n        class MockDateTime(datetime.datetime):\n            @classmethod\n                if tz is None:\n                    return now\n                else:\n                    return now.replace(tzinfo=tz) + tz.utcoffset(now)\n\n        # Test cases for boundaries\n        test_list = [\n            now - datetime.timedelta(days=29),\n            now - datetime.timedelta(days=30),\n            now - datetime.timedelta(days=59),\n            now - datetime.timedelta(days=60),\n            now - datetime.timedelta(days=364),\n            now - datetime.timedelta(days=365),\n        ]\n\n        result_list = [\n            \"29 days ago\",\n            \"1 month ago\",\n            \"59 days ago\",\n            \"2 months ago\",\n            \"11 months ago\",\n            \"1 year ago\",\n\n        ]\n\n        orig_humanize_datetime = humanize.datetime\n        humanize.datetime = MockDateTime\n        try:\n            with translation.override('en'):\n                self.humanize_tester(test_list, result_list, 'naturaltime')\n        finally:\n            humanize.datetime = orig_humanize_datetime\n\n", "    def test_naturaltime_with_tzinfo(self):\n        \n\n        # Using MockDateTime to control the time\n\n        now = datetime.datetime(2012, 3, 9, 22, 30, tzinfo=utc)\n\n        test_list = [\n            now,\n            now - datetime.timedelta(microseconds=1),\n            now + datetime.timedelta(seconds=1),\n            now.replace(tzinfo=get_fixed_timezone(-720)),\n            now.replace(tzinfo=get_fixed_timezone(720)),\n        ]\n        result_list = [\n            'now',\n            'a second ago',\n            'a second from now',\n            'now', \n            'now',\n        ]\n        orig_humanize_datetime, humanize.datetime = humanize.datetime, MockDateTime\n        try:\n            with translation.override('en'):\n                self.humanize_tester(test_list, result_list, 'naturaltime')\n        finally:\n            humanize.datetime = orig_humanize_datetime\n"], "sample_530": ["    def test_anchoredoffsetbox_get_window_extent(self):\n        fig, ax = plt.subplots()\n        size = 100\n        da = DrawingArea(size, size)\n        bg = mpatches.Rectangle((0, 0), size, size,\n                                facecolor='#CCCCCC',\n                                edgecolor='None',\n                                linewidth=0)\n        line = mlines.Line2D([-size*.5, size*1.5], [size/2, size/2],\n                             color='black',\n                             linewidth=10)\n        anchored_box = AnchoredOffsetbox(\n            loc='center',\n            child=da,\n            pad=0.,\n            frameon=False,\n            bbox_to_anchor=(.5, .5),\n            bbox_transform=ax.transAxes,\n            borderpad=0.)\n\n        da.add_artist(bg)\n        da.add_artist(line)\n        ax.add_artist(anchored_box)\n        fig.canvas.draw()\n\n        renderer = fig.canvas.get_renderer()\n        bbox = anchored_box.get_window_extent(renderer)\n\n        assert bbox.x0 >= 0\n        assert bbox.y0 >= 0\n        assert bbox.x1 <= fig.canvas.get_width_height()[0]\n        assert bbox.y1 <= fig.canvas.get_width_height()[1]\n", "    def test_offsetbox_clip_on():\n        fig, ax = plt.subplots()\n        da = DrawingArea(100, 100)\n        ab = AnchoredOffsetbox(loc='upper left', child=da, clip_on=False)\n        ax.add_artist(ab)\n        ax.set_xlim(0, 1)\n        ax.set_ylim(0, 1)\n\n        fig.canvas.draw()\n", "def test_annotationbbox_clip_on():\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n\n    # Annotation partially outside the axes limits\n    ab = AnnotationBbox(\n        mpatches.Rectangle((0, 0), 1, 1, fc='red'), (1.2, 0.5),\n        xybox=(0.5, 0.5),\n        boxcoords='axes fraction',\n        arrowprops=dict(arrowstyle=\"->\"),\n        clip_on=True,\n    )\n    ax.add_artist(ab)\n\n    ab_clip_off = AnnotationBbox(\n        mpatches.Rectangle((0, 0), 1, 1, fc='green'), (1.2, 0.5),\n        xybox=(0.5, 0.5),\n        boxcoords='axes fraction',\n        arrowprops=dict(arrowstyle=\"->\"),\n        clip_on=False,\n    )\n    ax.add_artist(ab_clip_off)\n", "    def test_packer_empty(self, align):\n        hpacker = HPacker(children=[], pad=0, sep=0, align=align)\n        vpacker = VPacker(children=[], pad=0, sep=0, align=align)\n        assert hpacker.get_extent_width() == 0\n        assert hpacker.get_extent_height() == 0\n        assert vpacker.get_extent_width() == 0\n        assert vpacker.get_extent_height() == 0\n\n", "def test_packers_align_center(align):\n    # set the DPI to match points to make the math easier below\n    fig = plt.figure(dpi=72)\n    x1, y1 = 10, 30\n    x2, y2 = 20, 60\n    r1 = DrawingArea(x1, y1)\n    r2 = DrawingArea(x2, y2)\n\n    hpacker = HPacker(children=[r1, r2], pad=0, sep=0, align=align)\n    vpacker = VPacker(children=[r1, r2], pad=0, sep=0, align=align)\n    renderer = fig.canvas.get_renderer()\n\n    # HPacker\n    *extents, offset_pairs = hpacker.get_extent_offsets(renderer)\n    # width, height, xdescent, ydescent\n    assert_allclose((x1 + x2, max(y1, y2), 0, 0), extents)\n    # internal element placement\n    y_height = (y2 - y1) / 2\n    # x-offsets, y-offsets\n    assert_allclose([(0, y_height), (x1, 0)], offset_pairs)\n\n    # VPacker\n    *extents, offset_pairs = vpacker.get_extent_offsets(renderer)\n    # width, height, xdescent, ydescent\n    assert_allclose([max(x1, x2), y1 + y2, 0, max(y1, y2)], extents)\n    # internal element placement\n    x_height = (x2 - x1) / 2\n    # x-offsets, y-offsets\n    assert_allclose([(x_height, 0), (0, -y2)], offset_pairs)\n"], "sample_39": ["def test_wcs_from_hdu():\n    \"\"\"\n    Test creating a WCS object directly from an HDU object.\n    \"\"\"\n    with fits.open(get_pkg_data_filename('data/sip.fits')) as hdul:\n        w = wcs.WCS(hdul[0])\n    assert isinstance(w, wcs.WCS)\n", "compilation error", "compilation error", "compilation error", "    def test_empty_header():\n        header = fits.Header()\n        with pytest.raises(ValueError) as exc:\n            wcs.WCS(header)\n        assert exc.value.args[0] == \"Header does not contain WCS information\"\n"], "sample_1191": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_917": ["    def check_cpp_type(target, expected):\n        class Config:\n            cpp_id_attributes = [\"id_attr\"]\n            cpp_paren_attributes = [\"paren_attr\"]\n        parser = DefinitionParser(target, location=None,\n                                  config=Config())\n        ast, isShorthand = parser.parse_cpp_type()\n        parser.assert_end()\n        assert ast == expected\n", "    def check_function_type(expected, input, output):\n        parser = DefinitionParser(input, location=None)\n        res, _ = parser.parse_function_type()\n        assert res == expected, f'Expected {expected}, got {res} from input: {input}'\n", "    def check(spec, text, file):\n        pattern = '<li><p>%s<a .*?><code .*?><span .*?>%s</span></code></a></p></li>' % spec\n        res = re.search(pattern, text)\n        if not res:\n            print(\"Pattern\\n\\t%s\\nnot found in %s\" % (pattern, file))\n            assert False", "    def check(spec, text, file):\n        pattern = '<li><p>%s<a .*?><code .*?><span .*?>%s</span></code></a></p></li>' % spec\n        res = re.search(pattern, text)\n        if not res:\n            print(\"Pattern\\n\\t%s\\nnot found in %s\" % (pattern, file))\n            assert False\n", "    def check_id_output(role, text, file):\n        pattern = r'<a .*?><code .*?><span .*?>%s</span></code></a>' % text\n        res = re.search(pattern, output)\n        expect = '''\\"], "sample_943": ["    def test_package_file_with_init_py(tempdir):\n        outdir = path(tempdir)\n        (outdir / 'testpkg').makedirs()\n        (outdir / 'testpkg' / '__init__.py').write_text('def foo(): pass')\n        (outdir / 'testpkg' / 'example.py').write_text('')\n        apidoc_main(['-o', tempdir, tempdir / 'testpkg'])\n        assert (outdir / 'testpkg.rst').exists()\n\n        content = (outdir / 'testpkg.rst').read_text()\n        assert content == (\"testpkg package\\n\"\n                           \"===============\\n\"\n                           \"\\n\"\n                           \"Submodules\\n\"\n                           \"----------\\n\"\n                           \"\\n\"\n                           \".. toctree::\\n\"\n                           \"   :maxdepth: 4\\n\"\n                           \"\\n\"\n                           \"   testpkg.example\\n\"\n                           \"\\n\"\n                           \"Module contents\\n\"\n                           \"---------------\\n\"\n                           \"\\n\"\n                           \".. automodule:: testpkg\\n\"\n                           \"   :members:\\n\"\n                           \"   :undoc-members:\\n\"\n                           \"   :show-inheritance:\\n\")\n", "def test_excludes_partial_subpackage(apidoc):\n    outdir = apidoc.outdir\n    assert (outdir / 'conf.py').isfile()\n    assert (outdir / 'a.rst').isfile()\n    assert (outdir / 'a.b.rst').isfile()\n    assert (outdir / 'a.b.c.rst').isfile()  \n    assert not (outdir / 'a.b.e.rst').isfile()\n    assert (outdir / 'a.b.x.rst').isfile()\n    assert (outdir / 'a.b.x.y.rst').isfile()\n", "def test_pep_0420_disabled_top_level_verify_double_underscore(make_app, apidoc):\n    outdir = apidoc.outdir\n    assert (outdir / 'conf.py').isfile()\n    assert (outdir / '__init__.rst').isfile()\n    assert not (outdir / 'c.rst').exists()\n\n    with open(outdir / '__init__.rst') as f:\n        rst = f.read()\n        assert \"__init__ package\\n\" in rst\n        assert \"automodule:: __init__\\n\" in rst\n\n    app = make_app('text', srcdir=outdir)\n    app.build()\n    print(app._status.getvalue())\n    print(app._warning.getvalue())\n", "    def test_apidoc_force(tempdir):\n        (tempdir / 'testpkg').makedirs()\n        (tempdir / 'testpkg' / '__init__.py').write_text('')\n        apidoc_main(['-o', tempdir, '--force', tempdir / 'testpkg'])\n        assert (tempdir / 'testpkg.rst').exists()\n", "    def test_relative_paths_module_first(make_app, apidoc):\n        outdir = apidoc.outdir\n        assert (outdir / 'testpkg.rst').isfile()\n        assert (outdir / 'testpkg.subpackage.rst').isfile()\n\n        with open(outdir / 'testpkg.rst') as f:\n            content = f.read()\n\n        assert '.. toctree::\\n   :maxdepth: 4\\n\\n   testpkg.subpackage' in content\n        assert '.. automodule:: testpkg\\n   :members:\\n   :undoc-members:\\n   :show-inheritance:\\n' in content\n\n        with open(outdir / 'testpkg.subpackage.rst') as f:\n            content = f.read()\n\n        assert '.. toctree::\\n   :maxdepth: 4\\n\\n   testpkg.subpackage.module' in content\n"], "sample_428": ["    def test_scientific_notation(self):\n        self.assertEqual(\n            nformat(Decimal(\"1e201\"), \".\"), \"1e+201\"\n        )\n        self.assertEqual(\n            nformat(Decimal(\"1e201\"), \".\", decimal_pos=2), \"1.00e+201\"\n        )\n        self.assertEqual(\n            nformat(Decimal(\"1.2345e201\"), \".\"), \"1.2345e+201\"\n        )\n        self.assertEqual(\n            nformat(Decimal(\"1.2345e201\"), \".\", decimal_pos=3), \"1.235e+201\"\n        )\n        self.assertEqual(\n            nformat(Decimal(\"1e-10\"), \".\"), \"1e-10\"\n        )\n        self.assertEqual(\n            nformat(Decimal(\"1e-10\"), \".\", decimal_pos=2), \"0.00\"\n        )\n\n\n\n", "    def test_grouping_with_sequences(self):\n        self.assertEqual(nformat(1234567890, \".\", grouping=(3, 2, 0)), \"1,234,567890\")\n        self.assertEqual(nformat(123456789012345, \".\", grouping=(3, 2, 0)), \"1,234,567,890,123,45\")\n        self.assertEqual(nformat(1234567890, \".\", grouping=(2, 2, 2)), \"12,34,56,78,90\")\n        self.assertEqual(nformat(-1234567890, \".\", grouping=(3, 2, 0)), \"-1,234,567890\")\n", "    def test_grouping_sequence(self):\n        self.assertEqual(\n            nformat(1234567890, \".\", grouping=(3, 2, 0), thousand_sep=\",\"),\n            \"1,23,45,678,90\",\n        )\n", "    def test_decimal_rounding(self):\n        self.assertEqual(nformat(Decimal('12.345'), '.', decimal_pos=2), \"12.35\")\n        self.assertEqual(nformat(Decimal('12.344'), '.', decimal_pos=2), \"12.34\")\n        self.assertEqual(nformat(Decimal('-12.345'), '.', decimal_pos=2), \"-12.35\")\n        self.assertEqual(nformat(Decimal('-12.344'), '.', decimal_pos=2), \"-12.34\")\n", "    def test_non_uniform_grouping(self):\n        self.assertEqual(nformat(1234567890, \".\", grouping=(3, 2, 0)), \"1,23,45,67890\")\n"], "sample_431": ["    def test_refresh_fk_no_objects(self):\n        s1 = SelfRef.objects.create()\n        s2 = SelfRef.objects.create(selfref=s1)\n        s1.delete()\n        with self.assertNumQueries(1):\n            s2.refresh_from_db()\n        self.assertIsNone(s2.selfref)\n", "    def test_refresh_m2m_cleared(self):\n        a = Article.objects.create(pub_date=datetime(2005, 7, 28))\n        a.keywords.add(\"python\", \"programming\")\n        a_prefetched = Article.objects.prefetch_related(\"keywords\").first()\n        self.assertEqual(len(a_prefetched.keywords.all()), 2)\n        a.keywords.clear()\n        self.assertEqual(len(a.keywords.all()), 0)\n        # Relation was cleared but the prefetch cache is stale.\n        self.assertEqual(len(a_prefetched.keywords.all()), 2)\n        a_prefetched.refresh_from_db()\n        # Cache was cleared and new results are available.\n        self.assertEqual(len(a_prefetched.keywords.all()), 0)\n\n", "    def test_refresh_m2m_cache_cleared(self):\n        a = Article.objects.create(pub_date=datetime(2005, 7, 28))\n        a2 = Article.objects.create(pub_date=datetime(2005, 7, 29))\n        s = SelfRef.objects.create(article=a)\n        s2 = SelfRef.objects.create(article=a2)\n        a.selfref_set.add(s2)\n        a.refresh_from_db()\n        self.assertCountEqual(a.selfref_set.all(), [s, s2])\n        a.selfref_set.remove(s2)\n        self.assertCountEqual(a.selfref_set.all(), [s])\n", "    def test_refresh_from_db_with_only(self):\n        a = Article.objects.create(pub_date=datetime.now())\n        a.headline = \"New headline\"\n        a.save()\n        with self.assertNumQueries(1):\n            a.refresh_from_db(fields=[\"headline\"])\n        self.assertEqual(a.headline, \"New headline\")\n        self.assertIsNone(a.pub_date)\n", "    def test_refresh_m2m_field(self):\n        a = Article.objects.create(\n            headline=\"Parrot programs in Python\",\n            pub_date=datetime(2005, 7, 28),\n        )\n        t1 = Tag.objects.create(name='python')\n        t2 = Tag.objects.create(name='parrot')\n        a.tags.add(t1, t2)\n        a.refresh_from_db(fields=[\"tags\"])\n        self.assertEqual(a.tags.count(), 2)\n"], "sample_487": ["    def test_actions_have_permissions_defined(self):\n        @admin.action\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action,)\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"The 'action' action in BandAdmin must define a 'permissions' \"\n            \"attribute.\",\n            id=\"admin.E131\",\n        )\n", "    def test_valid_actions_with_permission(self):\n        @admin.action(permissions=[\"change\"])\n            pass\n\n        @admin.action\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action1, action2)\n            \n        self.assertIsValid(BandAdmin, Band)\n", "    def test_actions_not_callable(self):\n        class BandAdmin(ModelAdmin):\n            actions = \"not_a_callable\"\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"The value of 'actions' must be a list or tuple of callable objects.\",\n            id=\"admin.E131\",\n        )\n", "    def test_actions_permissions_should_be_list(self):\n        @admin.action(permissions=\"custom\")\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action,)\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"The value of 'permissions' for action 'action' must be a list.\",\n            id=\"admin.E131\",\n        )\n", "    def test_actions_valid_with_permissions(self):\n        @admin.action(permissions=[\"custom\"])\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (custom_permission_action,)\n\n                return True\n\n        self.assertIsValid(BandAdmin, Band)\n\n"], "sample_124": ["    def test_field_override_required(self):\n        class MyForm(Form):\n            name = CharField(required=True)\n            email = CharField()\n\n        form = MyForm({'name':'John', 'email':''})\n        self.assertFalse(form.is_valid())\n        self.assertIn('This field is required', form.errors['email'])\n        \n        form.fields['email'].required = False # override\n        form.full_clean()\n        self.assertTrue(form.is_valid())\n", "    def test_get_default_renderer(self):\n        self.assertEqual(get_default_renderer(), DjangoTemplates)\n", "    def test_renderer_as_subclass_of_template(self):\n        class CustomRenderer(Template):\n            pass\n\n        class CustomForm(Form):\n            default_renderer = CustomRenderer\n\n        form = CustomForm()\n        self.assertTrue(isinstance(form.renderer, Template))\n", "    def test_form_label_suffix(self):\n        class MyForm(Form):\n            name = CharField(label_suffix=':')\n\n        form = MyForm()\n        self.assertHTMLEqual(\n            form.as_p(),\n            '<p><label for=\"id_name\">Name:</label> <input type=\"text\" name=\"name\" id=\"id_name\"></p>',\n        )\n", "    def test_renderer_as_property(self):\n        class CustomForm(Form):\n            default_renderer = CustomRenderer()\n\n        form = CustomForm()\n        self.assertEqual(form.renderer, CustomForm.default_renderer)\n        form.renderer = DjangoTemplates()\n        self.assertEqual(form.renderer, DjangoTemplates())\n"], "sample_97": ["    def test_notify_file_changed_with_exception(self):\n        with mock.patch.object(self.reloader, '_update_watches') as mocked_watches:\n            mocked_watches.side_effect = Exception()\n            with self.assertRaises(Exception):\n                self.reloader.notify_file_changed(self.existing_file)\n\n", "    def test_stat_reloader_handles_deleted_file(self):\n        self.reloader.watch_file(self.existing_file)\n        with self.tick_twice():\n            os.remove(self.existing_file)\n        self.assertEqual(self.reloader.notify_file_changed.call_count, 1)\n        self.assertEqual(self.reloader.notify_file_changed.call_args[0][0], self.existing_file)\n", "    def test_notify_file_changed_ignores_missing_files(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.nonexistent_file]):\n            self.reloader.notify_file_changed(self.nonexistent_file)\n            self.assertFalse(self.reloader.should_stop)\n", "    def test_should_stop_when_no_changes_and_no_files_to_watch(self):\n        self.reloader.watched_files = mock.MagicMock(return_value=set())\n        self.reloader._should_stop = mock.MagicMock(return_value=False)\n        self.assertFalse(self.reloader._should_stop())\n", "    def test_watch_multiple_files(self):\n        self.reloader.watch_file(self.existing_file)\n        self.reloader.watch_file(self.nonexistent_file)\n        with self.tick_twice():\n            self.increment_mtime(self.existing_file)\n        self.assertEqual(notify_mock.call_count, 1)\n        self.assertCountEqual(notify_mock.call_args[0], [self.existing_file])\n"], "sample_453": ["    def test_cell_count(self):\n        request = self.request_factory.get(reverse(\"admin:admin_views_article_changelist\"))\n        request.user = self.superuser\n        admin = ArticleAdmin(Article, site)\n        inline_admin_formset = admin.get_inline_instances(request, Article)\n\n        self.assertEqual(cell_count(inline_admin_formset[0]), 2)\n        Question.objects.create(question='q1', posted=datetime.date.today())\n        self.assertEqual(cell_count(inline_admin_formset[0]), 3)\n", "    def test_cell_count(self):\n        request = self.request_factory.get(reverse(\"admin:admin_views_article_changelist\"))\n        request.user = self.superuser\n        admin = ArticleAdmin(Article, site)\n        inline_admin_form = admin.inline_instances[0].formset.forms[0]\n        count = cell_count(inline_admin_form)\n        self.assertEqual(count, 4)\n\n", "    def test_prepopulated_fields_js_renders_for_inline(self):\n        request = self.request_factory.get(\n            reverse(\"admin:admin_views_article_change\", args=[self.article.pk])\n        )\n        request.user = self.superuser\n        admin = ArticleAdmin(Article, site)\n        \n        # Assuming Article has a one-to-many relationship with Question, and\n        # 'author' field is prepopulated\n\n        extra_context = {\"inline_admin_formsets\": [\n            { \"pk\": 1, \n            \"original\": None, \n            \"prepopulated_fields\": [\n                {\"field\": {\"name\": \"author\"}, \"dependencies\": [{\"name\": \"title\"}]}\n            ]\n            },\n        ]}\n        response = admin.change_view(request, str(self.article.pk), extra_context=extra_context)\n        # We need to check the prepopulated_fields_json content\n        # to ensure the inline form's prepopulated fields are correctly included.\n        prepopulated_fields_json = json.loads(response.context_data[\"prepopulated_fields_json\"]) \n        self.assertEqual(len(prepopulated_fields_json), 1)\n        self.assertEqual(prepopulated_fields_json[0][\"name\"], \"author\")\n\n\n\n", "    def test_prepopulated_fields_js(self):\n        article = Article.objects.create(title=\"Test Article\")\n        request = self.request_factory.get(\n            reverse(\"admin:admin_views_article_change\", args=[article.pk])\n        )\n        request.user = self.superuser\n        admin = ArticleAdmin(Article, site)\n        admin.prepopulated_fields = {\"content\": {\"field_name\": \"title\"}}\n\n        response = admin.change_view(request, str(article.pk))\n        response.render()\n        self.assertContains(response, 'data-prepopulated-fields')\n", "    def test_prepopulated_fields_js(self):\n        request = self.request_factory.get(\n            reverse(\"admin:admin_views_article_change\", args=[self.article.pk])\n        )\n        request.user = self.superuser\n        admin = ArticleAdmin(Article, site)\n        response = admin.change_view(request, str(self.article.pk))\n        response.render()\n        self.assertContains(response, 'data-prepopulated-fields=\"')\n        prepopulated_fields_json = json.loads(\n            response.context_data[\"prepopulated_fields_json\"]\n        )\n        self.assertEqual(prepopulated_fields_json[0]['id'], '#id_slug')\n"], "sample_101": ["    def test_force_script_name(self):\n        \"\"\"\n        get_script_name() returns FORCE_SCRIPT_NAME if it is set.\n\n        \"\"\"\n        request = self.request_factory.get('/path/')\n        self.assertEqual(request.path, '/someprefix/path/')\n\n", "    def test_force_script_name(self):\n        \"\"\"\n        FORCE_SCRIPT_NAME is used to set SCRIPT_NAME.\n        \"\"\"\n        application = get_wsgi_application()\n        environ = self.request_factory._base_environ(\n            PATH_INFO=\"/path/\",\n            CONTENT_TYPE=\"text/html; charset=utf-8\",\n            REQUEST_METHOD=\"GET\",\n        )\n        response_data = {}\n\n            response_data[\"status\"] = status\n            response_data[\"headers\"] = headers\n\n        response = application(environ, start_response)\n\n        self.assertEqual(response_data[\"status\"], \"200 OK\")\n        self.assertEqual(\n            set(response_data[\"headers\"]),\n            {('Content-Length', '12'), ('Content-Type', 'text/html; charset=utf-8')})\n        self.assertIn(bytes(response), [\n            b\"Content-Length: 12\\r\\nContent-Type: text/html; charset=utf-8\\r\\n\\r\\nHello World!\",\n            b\"Content-Type: text/html; charset=utf-8\\r\\nContent-Length: 12\\r\\n\\r\\nHello World!\"\n        ])\n\n        self.assertEqual(environ['SCRIPT_NAME'], '/myapp/')\n\n", "    def test_limited_stream(self):\n        \"\"\"\n        LimitedStream limits the amount of data that can be read.\n        \"\"\"\n        stream = BytesIO(b'1234567890' * 10)\n        limited_stream = LimitedStream(stream, 20)\n        self.assertEqual(limited_stream.read(10), b'1234567890')\n        self.assertEqual(limited_stream.read(10), b'1234567890')\n        self.assertEqual(limited_stream.read(10), b'')\n", "    def test_force_script_name(self):\n        application = get_wsgi_application()\n        environ = self.request_factory._base_environ(\n            PATH_INFO=\"/bar/\",\n            CONTENT_TYPE=\"text/html; charset=utf-8\",\n            REQUEST_METHOD=\"GET\"\n        )\n        response_data = {}\n\n            response_data[\"status\"] = status\n            response_data[\"headers\"] = headers\n\n        response = application(environ, start_response)\n\n        self.assertEqual(response_data[\"status\"], \"200 OK\")\n        self.assertIn(b'/foo/bar', response)\n", "    def test_limited_stream(self):\n        stream = BytesIO(b'This is a test stream.')\n        limited_stream = LimitedStream(stream, 10)\n        self.assertEqual(limited_stream.read(5), b'This ')\n        self.assertEqual(limited_stream.read(5), b'is a ')\n        self.assertEqual(limited_stream.read(10), b'test stream.')\n        self.assertEqual(limited_stream.read(), b'')\n        self.assertEqual(limited_stream.readline(), b'')\n\n"], "sample_606": ["compilation error", "def test_cross_broadcast(use_dask: bool) -> None:\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"test for dask.\")\n\n    a = xr.DataArray(np.arange(6).reshape((2, 3)), dims=['x', 'y'])\n    b = xr.DataArray(np.arange(12).reshape((2, 2, 3)), dims=['x', 'z', 'y'])\n\n    if use_dask:\n        a = a.chunk({'x': 1})\n        b = b.chunk({'x':1, 'z':1})\n\n    with pytest.raises(ValueError):\n        xr.cross(a, b, dim='y')\n\n", "def test_cross_error(use_dask):\n    if use_dask and not has_dask:\n        pytest.skip(\"test for dask.\")\n\n    a = xr.DataArray([1, 2, 3], dims=[\"x\"])\n    b = xr.DataArray([4, 5], dims=[\"x\"])\n\n    if use_dask:\n        a = a.chunk()\n        b = b.chunk()\n\n    with pytest.raises(ValueError, match=r\"Input arrays must have the same number of dimensions\"):\n        xr.cross(a, b)\n", "    def test_cross_dtypes(use_dask):\n        if use_dask and not has_dask:\n            pytest.skip(\"requires dask.\")\n        a = xr.DataArray(\n            np.array([1.0, 2.0, 3.0], dtype=np.float32), dims=[\"cartesian\"]\n        )\n        b = xr.DataArray(\n            np.array([4.0, 5.0, 6.0], dtype=np.float64), dims=[\"cartesian\"]\n        )\n        if use_dask:\n            a = a.chunk()\n            b = b.chunk()\n        expected = np.cross(a, b, axis=-1)\n        actual = xr.cross(a, b, dim=\"cartesian\")\n        xr.testing.assert_duckarray_allclose(expected, actual)\n        assert actual.dtype == np.float64\n", "compilation error"], "sample_811": ["    def test_pairwise_distances_chunked_metric_args():\n        rng = np.random.RandomState(0)\n        X = rng.random_sample((100, 10))\n        Y = rng.random_sample((200, 10))\n\n            return np.sum(w * (x - y) ** 2, axis=1)[:, None]\n\n        weights = np.ones(10)\n\n        distances = pairwise_distances_chunked(X, Y, metric=custom_metric,\n                                              metric_kwargs={'w': weights},\n                                              working_memory=1)\n        expected = pairwise_distances(X, Y, metric=custom_metric,\n                                       metric_kwargs={'w': weights})\n        assert_allclose(distances, expected)\n", "    def test_pairwise_distances_chunked_empty():\n        X = np.array([], dtype=np.float64).reshape(0, 5)\n        Y = np.array([], dtype=np.float64).reshape(0, 5)\n        distances = list(pairwise_distances_chunked(X, Y, metric='euclidean'))\n        assert len(distances) == 0\n", "    def test_pairwise_distances_chunked_empty_input():\n        # Test the case where one or both input arrays are empty.\n        X = np.array([])\n        Y = np.array([[1, 2, 3]])\n\n        with pytest.raises(ValueError):\n            list(pairwise_distances_chunked(X, Y, metric='euclidean'))\n\n        Y = np.array([])\n        with pytest.raises(ValueError):\n            list(pairwise_distances_chunked(X, Y, metric='euclidean'))\n\n\n\n", "    def test_pairwise_distances_sparse_data_derived_params(\n                self, n_jobs, metric):\n        # check that pairwise_distances give the same result in sequential and\n        # parallel, when metric has data-derived parameters and input is sparse.\n        with config_context(working_memory=1):  # to have more than 1 chunk\n            rng = np.random.RandomState(0)\n            X = rng.random_sample((1000, 10))\n            X_sparse = csr_matrix(X)\n            Y = rng.random_sample((1000, 10))\n            Y_sparse = csr_matrix(Y)\n\n            if metric == \"seuclidean\":\n                params = {'V': np.var(np.vstack([X, Y]), axis=0, ddof=1)}\n            else:\n                params = {'VI': np.linalg.inv(np.cov(np.vstack([X, Y]).T)).T}\n\n            expected_dist_explicit_params = cdist(X, Y, metric=metric,\n                                                  **params)\n            dist = pairwise_distances(X_sparse, Y_sparse, metric=metric,\n                                       n_jobs=n_jobs, **params)\n\n            assert_allclose(dist, expected_dist_explicit_params)\n", "    def test_pairwise_distances_mixed_types():\n        # Check that pairwise distances handles mixed array types correctly\n        # Test with a list and a NumPy array\n        X = [[1, 2], [3, 4]]\n        Y = np.array([[5, 6], [7, 8]])\n\n        dist = pairwise_distances(X, Y, metric='euclidean')\n        expected_dist = np.array([[5.65685425, 7.28010989],\n                                 [7.28010989, 8.94427191]])\n        assert_allclose(dist, expected_dist)\n"], "sample_52": ["    def test_empty_queryset(self):\n        f = forms.ModelChoiceField(Category.objects.none())\n        self.assertEqual(len(f.choices), 1)\n        with self.assertRaises(ValidationError):\n            f.clean('1')\n", "    def test_modelmultiplechoicefield_empty_label(self):\n        \"\"\"\n        Test that ModelMultipleChoiceField handles empty_label correctly.\n        \"\"\"\n        form = forms.ModelMultipleChoiceField(Category.objects.all(), empty_label='--------')\n        self.assertEqual(len(form.choices), 4)\n        self.assertEqual(form.choices[0], ('', '--------'))\n\n\n", "    def test_modelmultiplechoicefield_empty_initial(self):\n        class AuthorForm(forms.ModelForm):\n            authors = forms.ModelMultipleChoiceField(Author.objects.all(), required=False)\n\n            class Meta:\n                model = Book\n                fields = ['authors']\n\n        book = Book.objects.create(\n            pub_date=datetime.date(1988, 1, 4),\n            writer=Writer.objects.create(name='Test writer'),\n        )\n        form = AuthorForm(instance=book)\n        self.assertEqual(form.initial['authors'], [])\n", "    def test_modelchoicefield_with_empty_label(self):\n        f = forms.ModelChoiceField(Category.objects.all(), empty_label='Choose a category')\n        self.assertEqual(list(f.choices), [\n            ('Choose a category', '---------'),\n            (self.c1.pk, 'Entertainment'),\n            (self.c2.pk, 'A test'),\n            (self.c3.pk, 'Third'),\n        ])\n        with self.assertRaises(ValidationError):\n            f.clean('Choose a category')\n", "    def test_empty_queryset(self):\n        f = forms.ModelChoiceField(Category.objects.none())\n        self.assertEqual(f.choices, [('', '---------')])\n        with self.assertRaises(ValidationError):\n            f.clean('1')\n"], "sample_339": ["    def test_modelformset_factory_with_custom_error_message(self):\n        class CustomErrorForm(forms.Form):\n            field = forms.CharField(error_messages={'required': 'This field is mandatory.'})\n\n        CustomBookFormSet = modelformset_factory(Book, fields='__all__', form=CustomErrorForm)\n        data = {'form-TOTAL_FORMS': '1', 'form-INITIAL_FORMS': '0', 'form-MAX_NUM_FORMS': '', 'form-0-title': ''}\n        formset = CustomBookFormSet(data)\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.errors, [{'title': ['This field is mandatory.']}])\n", "    def test_modelformset_factory_with_custom_form(self):\n        class MyAuthorForm(forms.ModelForm):\n            class Meta:\n                model = Author\n                fields = '__all__'\n\n        AuthorFormSet = modelformset_factory(\n            Author,\n            form=MyAuthorForm,\n            fields='__all__',\n        )\n        formset = AuthorFormSet()\n        self.assertIsInstance(formset.forms[0], MyAuthorForm)\n", "    def test_modelformset_factory_passes_validate_max_num(self):\n        AuthorFormSet = modelformset_factory(\n            Author,\n            fields='__all__',\n            max_num=2,\n            validate_max=True\n        )\n        data = {\n            'form-TOTAL_FORMS': '3',\n            'form-INITIAL_FORMS': '0',\n            'form-MAX_NUM_FORMS': '',\n            \n        }\n        formset = AuthorFormSet(data=data)\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.non_form_errors(), ['Please submit at most 2 forms.']) \n", "    def test_modelformset_factory_empty_formset_with_initial_data(self):\n        AuthorFormSet = modelformset_factory(Author, fields='__all__')\n        formset = AuthorFormSet(initial=[{'name': 'Charles'}])\n        self.assertEqual(formset.initial_data[0]['name'], 'Charles')\n        self.assertEqual(len(formset.forms), 1)\n        self.assertTrue(formset.forms[0].is_valid())\n", "    def test_modelformset_factory_initial_with_empty_list(self):\n        BookFormSet = modelformset_factory(Book, fields='__all__')\n        formset = BookFormSet(initial=[])\n        self.assertEqual(len(formset.forms), 0)\n"], "sample_933": ["def test_gettext_uuid(app):\n    app.config.gettext_uuid = True\n    app.builder.build_all()\n\n    catalog = (app.outdir / 'extapi.pot').read_text()\n    assert \"msgid \\\"something, something else, something more\\\"\" in catalog\n    assert \"msgid \\\"Another message\\\"\" in catalog\n\n", "def test_gettext_uuid(app):\n    app.config.gettext_uuid = True\n    app.builder.build_all()\n    # UUIDs should be present in the PO file\n\n    catalog = (app.outdir / 'extapi.pot').read_text()\n    assert \"msgid \\\"something\\\"\" in catalog\n    assert len(re.findall(r'msgstr[ ]+\\\"\\\"', catalog)) > 0\n\n    # Verify UUIDs are present in the messages\n    for match in re.findall(r'msgid \\\"(.*?)\\\"', catalog):\n        assert match in catalog\n        assert len(re.findall(r'uuid: [0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}', catalog)) > 0\n", "def test_build_single_pot_with_sphinx_name(app):\n    app.builder.build_all()\n\n    assert (app.outdir / 'sphinx.pot').isfile()\n\n    result = (app.outdir / 'sphinx.pot').read_text()\n    assert re.search(\n        ('msgid \"Todo\".*'\n         'msgid \"Like footnotes.\".*'\n         'msgid \"The minute.\".*'\n         'msgid \"Generated section\".*'),\n        result,\n        flags=re.S)\n", "def test_build_separate_pots(app):\n    app.builder.build_all()\n\n    assert (app.outdir / 'extapi.pot').isfile()\n    assert (app.outdir / 'subdir.pot').isfile()\n    assert not (app.outdir / 'documentation.pot').exists()\n", "def test_build_uuid_based_catalog(app):\n    app.builder.build_all()\n\n    # Check if multiple catalogs are generated for different domains\n    assert (app.outdir / 'extapi.pot').isfile()\n    assert (app.outdir / 'subdir.pot').isfile()\n    assert (app.outdir / 'sphinx.pot').isfile()\n\n\n\n"], "sample_1074": ["compilation error", "compilation error", "compilation error", "compilation error", "    def test_is_solvable():\n        assert AlternatingGroup(3).is_solvable == True\n        assert AlternatingGroup(5).is_solvable == False\n        assert SymmetricGroup(4).is_solvable == True\n        assert SymmetricGroup(5).is_solvable == False\n        assert CyclicGroup(5).is_solvable == True\n"], "sample_589": ["    def test_interpolate_nd():\n        da = xr.DataArray(np.arange(24).reshape(2, 3, 4), dims=['x', 'y', 'z'])\n        da[0, 1, :] = np.nan\n        actual = da.interpolate_na(dim=['y', 'z'], method=\"linear\")\n        assert not np.isnan(actual.data).any()\n", "    def test_interpolate_na_max_gap_errors_dataset(self, ds, max_gap):\n        with raises_regex(\n            NotImplementedError, \"max_gap not implemented for unlabeled coordinates\"\n        ):\n            ds.interpolate_na(\"time\", max_gap=max_gap)\n\n", "    def test_interpolate_na_multidim_methods(self, ndim, method):\n        if ndim == 1:\n            data = np.array([np.nan, 1, 2, np.nan, 4, 5, 6, np.nan])\n        else:\n            data = np.array(\n                [\n                    [np.nan, 1, 2, np.nan],\n                    [4, 5, 6, np.nan],\n                    [np.nan, 8, 9, 10],\n                ]\n            )\n        da = xr.DataArray(data, dims=[\"x\"] * ndim)\n        actual = da.interpolate_na(dim=\"x\", method=method).fillna(0)\n        expected = da.interp(method=method, kwargs={\"fill_value\": 0})\n        assert_allclose(actual, expected)\n\n", "    def test_interpolate_na_method_2d(self, da, method):\n        actual = da.interpolate_na(dim=\"y\", method=method)\n        assert_equal(actual, expected)\n\n", "    def test_interpolate_na_max_gap_dask():\n        da = xr.DataArray(\n            [np.nan, 1, 2, np.nan, np.nan, 5, np.nan, np.nan, np.nan, np.nan, 10],\n            dims=[\"t\"],\n        )\n        da[\"t\"] = pd.date_range(\"2001-01-01\", freq=\"H\", periods=11)\n        da = da.chunk({\"t\": 3})\n        actual = da.interpolate_na(\"t\", max_gap=\"3H\")\n        expected = da.load().interpolate_na(\"t\", max_gap=\"3H\")\n        assert isinstance(actual.data, dask_array_type)\n        assert_equal(actual.compute(), expected)\n\n"], "sample_906": ["def test_domain_cpp_parse_nested_template_class(app, warning):\n    text = (\".. cpp:class:: Outer<typename T>\\n\"\n            \"    .. cpp:class:: Inner<typename U>\\n\")\n    restructuredtext.parse(app, text)\n    ws = warning.getvalue().split(\"\\n\")\n    assert len(ws) == 0\n", "    def check(spec, text, file):\n        pattern = '<li><p>%s<a .*?><code .*?><span .*?>%s</span></code></a></p></li>' % spec\n        res = re.search(pattern, text)\n        if not res:\n            print(\"Pattern\\n\\t%s\\nnot found in %s\" % (pattern, file))\n            assert False\n", "def test_domain_cpp_parse_noindexentry_duplicate():\n    text = (\".. cpp:function:: void f()\\n\"\n            \".. cpp:function:: void f()\\n\"\n            \"   :noindexentry:\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[('single', 'f (C++ function)', '_CPPv41fv', '', None)])\n    assert_node(doctree[2], addnodes.index, entries=[])\n", "    def check(target):\n        class Config:\n            cpp_id_attributes = [\"id_attr\"]\n            cpp_paren_attributes = [\"paren_attr\"]\n        parser = DefinitionParser(target, location=None,\n                                  config=Config())\n        ast, isShorthand = parser.parse_xref_object()\n        parser.assert_end()", "def test_domain_cpp_parse_mix_decl_duplicate(app, warning):\n    # Issue 8270\n    text = (\".. cpp:class:: A\\n\"\n            \".. cpp:function:: void A()\\n\"\n            \".. cpp:class:: A\\n\")\n    restructuredtext.parse(app, text)\n    ws = warning.getvalue().split(\"\\n\")\n    assert len(ws) == 5\n    assert \"index.rst:2: WARNING: Duplicate C++ declaration, also defined at index:1.\" in ws[0]\n    assert \"Declaration is '.. cpp:function:: void A()'.\" in ws[1]\n    assert \"index.rst:3: WARNING: Duplicate C++ declaration, also defined at index:1.\" in ws[2]\n    assert \"Declaration is '.. cpp:class:: A'.\" in ws[3]\n    assert ws[4] == \"\"\n"], "sample_536": ["def test_MultiCursor_colors(fig_test, fig_ref):\n    ax_test = fig_test.subplots()\n    ax_ref = fig_ref.subplots()\n\n    mc_test = widgets.MultiCursor(\n        ax_test, color='r', linewidth=2, linestyle='--'\n    )\n    mc_ref = widgets.MultiCursor(ax_ref)\n\n    # Trigger a move event for both\n    event = mock_event(ax_test, xdata=0.5, ydata=0.5)\n    mc_test.onmove(event)\n    mc_ref.onmove(event)\n\n\n\n", "compilation error", "def test_polygon_selector_remove_key(draw_bounding_box):\n    verts = [(50, 50), (150, 50), (50, 150)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n        ('on_key_press', dict(key='delete')),\n    ]\n    check_polygon_selector(event_sequence, verts[1:], 2,\n                           draw_bounding_box=draw_bounding_box)\n", "def test_polygon_selector_set_verts(draw_bounding_box):\n    verts = [(50, 50), (150, 50), (50, 150)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    # Create selector\n    ax = get_ax()\n    tool = widgets.PolygonSelector(ax, onselect=noop,\n                                   draw_bounding_box=draw_bounding_box)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n    tool.set_verts(verts)\n    assert tool.verts == verts\n\n", "    def test_polygon_selector_empty(fig_test, fig_ref, draw_bounding_box):\n        ax_test = fig_test.add_subplot()\n        tool_test = widgets.PolygonSelector(\n            ax_test, onselect=noop, draw_bounding_box=draw_bounding_box)\n        ax_ref = fig_ref.add_subplot()\n        tool_ref = widgets.PolygonSelector(\n            ax_ref, onselect=noop, draw_bounding_box=draw_bounding_box)\n"], "sample_1018": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1096": ["compilation error", "compilation error", "compilation error", "def test_IndexedBase_subs_complex():\n    i, j = symbols('i j', integer=True)\n    a = Symbol('a')\n    A = IndexedBase('A')\n    assert A[i].subs(i, 1 + 2j) == A[1 + 2j]\n", "def test_IndexedBase_equality():\n    i = Symbol('i', integer=True)\n    a = Symbol('a')\n    b = Symbol('b')\n    A = IndexedBase(a)\n    B = IndexedBase(b)\n    C = IndexedBase(a, shape=(2,))\n    assert A[i] == A[i]\n    assert A[i] != B[i]\n    assert A[i] != C[i]\n    assert A == A\n    assert A != B\n    assert A != C\n"], "sample_363": ["    def test_ManyToMany_using_to_field(self):\n        from selenium.webdriver.common.by import By\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_event_add'))\n\n        main_window = self.selenium.current_window_handle\n        # No value has been selected yet\n        self.assertEqual(self.selenium.find_element(By.ID, 'id_supporting_bands').get_attribute('value'), '')\n\n        # Help text for the field is displayed\n        self.assertEqual(\n            self.selenium.find_element(By.CSS_SELECTOR, '.field-supporting_bands div.help').text,\n            'Supporting Bands.'\n        )\n\n        # Click the Add Band button to add new Band\n        self.selenium.find_element(By.ID, 'add_id_supporting_bands').click()\n        self.wait_for_and_switch_to_popup()\n        name_field = self.selenium.find_element(By.ID, 'id_name')\n        name_field.send_keys('newband')\n\n        save_button_css_selector = '.submit-row > input[type=submit]'\n        self.selenium.find_element(By.CSS_SELECTOR, save_button_css_selector).click()\n        self.selenium.switch_to.window(main_window)\n        # The field now contains the new band\n        self.selenium.find_element(By.CSS_SELECTOR, '#id_supporting_bands option[value=newband]')\n\n        # Click the Change Band button to change it\n        self.selenium.find_element(By.ID, 'change_id_supporting_bands').click()\n        self.wait_for_and_switch_to_popup()\n\n        name_field = self.selenium.find_element(By.ID, 'id_name')\n        name_field.clear()\n        name_field.send_keys('changednewband')\n\n        save_button_css_selector = '.submit-row > input[type=submit]'\n        self.selenium.find_element(By.CSS", "    def test_ManyToManyField_using_to_field(self):\n        from selenium.webdriver.common.by import By\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_event_add'))\n\n        main_window = self.selenium.current_window_handle\n        # Click the Add Band button to add new\n        self.selenium.find_element(By.ID, 'add_id_bands').click()\n        self.wait_for_and_switch_to_popup()\n        name_field = self.selenium.find_element(By.ID, 'id_name')\n        name_field.send_keys('newband')\n        save_button_css_selector = '.submit-row > input[type=submit]'\n        self.selenium.find_element(By.CSS_SELECTOR, save_button_css_selector).click()\n        self.selenium.switch_to.window(main_window)\n        # The field now contains the new band\n        self.selenium.find_element(By.CSS_SELECTOR, '#id_bands option[value=newband]')\n\n        # Click the Change Band button to change it\n        self.selenium.find_element(By.ID, 'change_id_bands').click()\n        self.wait_for_and_switch_to_popup()\n\n        name_field = self.selenium.find_element(By.ID, 'id_name')\n        name_field.clear()\n        name_field.send_keys('changednewband')\n\n        save_button_css_selector = '.submit-row > input[type=submit]'\n        self.selenium.find_element(By.CSS_SELECTOR, save_button_css_selector).click()\n        self.selenium.switch_to.window(main_window)\n        self.selenium.find_element(By.CSS_SELECTOR, '#id_bands option[value=changednewband]')\n\n        # Go ahead and submit the form to make sure it works\n        self.selenium.find_element(By.CSS_SELECTOR, save_button_css_selector).click()\n", "    def test_ManyToMany_using_to_field(self):\n        from selenium.webdriver.common.by import By\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_event_add'))\n\n        # Add a new band\n        main_window = self.selenium.current_window_handle\n        self.selenium.find_element(By.ID, 'add_id_bands').click()\n        self.wait_for_and_switch_to_popup()\n\n        name_field = self.selenium.find_element(By.ID, 'id_name')\n        name_field.send_keys('New Band')\n\n        save_button_css_selector = '.submit-row > input[type=submit]'\n        self.selenium.find_element(By.CSS_SELECTOR, save_button_css_selector).click()\n        self.selenium.switch_to.window(main_window)\n\n        # Select the new band in the ManyToMany field\n\n        band_option = self.selenium.find_element(By.CSS_SELECTOR, '#id_bands option[value=\"New Band\"]')\n        band_option.click()\n\n        # Click the Change Band button to change it\n\n        self.selenium.find_element(By.ID, 'change_id_bands').click()\n        self.wait_for_and_switch_to_popup()\n        name_field = self.selenium.find_element(By.ID, 'id_name')\n        name_field.clear()\n        name_field.send_keys('Changed Band')\n\n        save_button_css_selector = '.submit-row > input[type=submit]'\n        self.selenium.find_element(By.CSS_SELECTOR, save_button_css_selector).click()\n        self.selenium.switch_to.window(main_window)\n\n        # Make sure the change is reflected in the field\n        self.selenium.find_element(By.CSS_SELECTOR, '#id_bands option[value=\"Changed Band\"]')\n\n", "    def test_ManyToManyField_using_to_field(self):\n        from selenium.webdriver.common.by import By\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_event_add'))\n\n        main_window = self.selenium.current_window_handle\n        self.selenium.find_element(By.ID, 'lookup_id_supporting_bands').click()\n        self.wait_for_and_switch_to_popup()\n        link = self.selenium.find_element(By.LINK_TEXT, 'Bogey Blues')\n        link.click()\n        self.selenium.switch_to.window(main_window)\n        self.wait_for_value('#id_supporting_bands', '42')\n\n        self.selenium.find_element(By.ID, 'lookup_id_supporting_bands').click()\n        self.wait_for_and_switch_to_popup()\n        link = self.selenium.find_element(By.LINK_TEXT, 'Green Potatoes')\n        link.click()\n        self.selenium.switch_to.window(main_window)\n        self.wait_for_value('#id_supporting_bands', '42,98')\n", "    def test_ManyToManyField_using_to_field(self):\n        from selenium.webdriver.common.by import By\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_event_add'))\n        main_window = self.selenium.current_window_handle\n        \n        # Add a new band\n        self.selenium.find_element(By.ID, 'add_id_bands').click()\n        self.wait_for_and_switch_to_popup()\n\n        name_field = self.selenium.find_element(By.ID, 'id_name')\n        name_field.send_keys('New Band')\n\n        save_button_css_selector = '.submit-row > input[type=submit]'\n        self.selenium.find_element(By.CSS_SELECTOR, save_button_css_selector).click()\n        self.selenium.switch_to.window(main_window)\n\n        #Select the new band\n\n        self.selenium.find_element(By.CSS_SELECTOR, '#id_bands option[value=New Band]').click()\n\n        # Add another new band\n        self.selenium.find_element(By.ID, 'add_id_bands').click()\n        self.wait_for_and_switch_to_popup()\n\n        name_field = self.selenium.find_element(By.ID, 'id_name')\n        name_field.send_keys('Another Band')\n\n        save_button_css_selector = '.submit-row > input[type=submit]'\n        self.selenium.find_element(By.CSS_SELECTOR, save_button_css_selector).click()\n        self.selenium.switch_to.window(main_window)\n\n        #Select the new band\n\n        self.selenium.find_element(By.CSS_SELECTOR, '#id_bands option[value=Another Band]').click()\n\n        #Save the event\n        self.selenium.find_element(By.CSS_SELECTOR, save_button_css_selector).click()\n        self.wait_for_text('li.success', 'The event"], "sample_413": ["    def test_template_tags_with_same_library_name_and_multiple_paths(self):\n        with self.settings(\n            TEMPLATES=[\n                self.get_settings(\n                    \"same_tags\", \"same_tags_app_1.templatetags.same_tags\"\n                ),\n                self.get_settings(\n                    \"same_tags\", \"same_tags_app_2.templatetags.same_tags\"\n                ),\n                self.get_settings(\n                    \"same_tags\", \"same_tags_app_3.templatetags.same_tags\"\n                ),\n            ]\n        ):\n            errors = check_for_template_tags_with_the_same_name(None)\n            self.assertEqual(len(errors), 1)\n            self.assertEqual(\n                errors[0].msg,\n                E003.msg.format(\n                    \"'same_tags'\",\n                    \"'check_framework.template_test_apps.same_tags_app_1.\"\n                    \"templatetags.same_tags', \"\n                    \"'check_framework.template_test_apps.same_tags_app_2.\"\n                    \"templatetags.same_tags', \"\n                    \"'check_framework.template_test_apps.same_tags_app_3.\"\n                    \"templatetags.same_tags'\",\n                ),\n            )\n", "    def test_template_tags_same_name_in_same_config(self):\n        self.assertEqual(\n            check_for_template_tags_with_the_same_name(None),\n            [self.error_same_tags],\n        )\n", "    def test_template_tags_with_same_name_in_installed_apps(self):\n        with self.settings(\n            TEMPLATES=[\n                self.get_settings(\n                    \"different_tags\", \"same_tags_app_1.templatetags.same_tags\"\n                ),\n                self.get_settings(\n                    \"different_tags\", \"same_tags_app_2.templatetags.same_tags\"\n                ),\n            ]\n        ):\n            self.assertEqual(\n                check_for_template_tags_with_the_same_name(None),\n                [self.error_same_tags],\n            )\n\n", "    def test_template_tags_with_same_name_and_module_path(self):\n        self.assertEqual(\n            check_for_template_tags_with_the_same_name(None),\n            [self.error_same_tags],\n        )\n", "    def test_template_tags_with_same_name_in_one_config(self):\n        self.assertEqual(\n            check_for_template_tags_with_the_same_name(None),\n            [\n                Error(\n                    E003.msg.format(\n                        \"'same_tags'\",\n                        \"'check_framework.template_test_apps.same_tags_app_1.\"\n                        \"templatetags.same_tags', \"\n                        \"'check_framework.template_test_apps.same_tags_app_1.\"\n                        \"templatetags.same_tags'\",\n                    ),\n                    id=E003.id,\n                )\n            ],\n        )\n"], "sample_64": ["    def test_empty_cookie_string(self):\n        c = SimpleCookie()\n        c.load('')\n        self.assertEqual(c, {})\n", "    def test_cookie_domain_not_loaded(self):\n        c = SimpleCookie()\n        c.load('name=value; Domain=example.com')\n        self.assertNotIn('Domain', c['name'])\n", "    def test_cookie_expires_after_load(self):\n        c = SimpleCookie()\n        c.load('name=val; Expires=Fri, 31 Dec 9999 23:59:59 GMT')\n        self.assertEqual(c['name']['expires'], datetime.datetime(9999, 12, 31, 23, 59, 59, tzinfo=datetime.timezone.utc))\n", "    def test_set_cookie_unicode_value(self):\n        c = SimpleCookie()\n        c['test'] = 'caf\u00e9'\n        self.assertEqual(c['test'].value, 'caf\u00e9')\n        self.assertEqual(c.output(header='Set-Cookie')[12:], 'test=caf\\xc3\\xa9')\n", "    def test_secure_after_load(self):\n        c = SimpleCookie()\n        c.load(\"name=val\")\n        c['name']['secure'] = True\n        self.assertTrue(c['name']['secure'])\n"], "sample_203": ["    def test_message_with_params(self):\n        class MyForm(forms.Form):\n            field = forms.CharField(\n                validators=[\n                    validators.RegexValidator(\n                        regex='^[a-zA-Z]*$',\n                        message=\"Letters only: %(value)s\",\n                    )\n                ]\n            )\n\n        form = MyForm({'field': '2 is not correct'})\n        self.assertIs(form.is_valid(), False)\n        self.assertEqual(form.errors['field'], [\"Letters only: 2 is not correct\"])\n", "    def test_prohibit_null_characters_validator(self):\n        class MyForm(forms.Form):\n            field = forms.CharField(\n                validators=[validators.ProhibitNullCharactersValidator()],\n                error_messages={'null_characters_not_allowed': '%(value)s'},\n            )\n\n        form = MyForm({'field': 'a\\x00b'})\n        self.assertIs(form.is_valid(), False)\n        self.assertEqual(form.errors, {'field': ['a\\x00b']})\n", "    def test_file_extension_validator_case_insensitive(self):\n        class MyForm(forms.Form):\n            field = forms.FileField(\n                validators=[validators.FileExtensionValidator(allowed_extensions=['txt', 'TXT'])],\n                error_messages={'invalid_extension': '%(value)s'},\n            )\n\n        form = MyForm(files={'field': SimpleUploadedFile('myfile.TXT', b'abc')})\n        self.assertTrue(form.is_valid())\n", "    def test_prohibit_null_character_validator(self):\n        class MyForm(forms.Form):\n            field = forms.CharField(validators=[validators.ProhibitNullCharactersValidator()])\n\n        form = MyForm({'field': 'a\\0b'})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors, {'field': ['Null characters are not allowed.']})\n", "    def test_prohibit_null_characters_validator(self):\n        class MyForm(forms.Form):\n            field = forms.CharField(\n                validators=[validators.ProhibitNullCharactersValidator()],\n            )\n\n        form = MyForm({'field': 'a\\x00b'})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['field'], ['Null characters are not allowed.'])\n"], "sample_1145": ["compilation error", "compilation error", "def test_refine_issue_12796():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    z = Symbol('z', real=True)\n\n    expr = (x + y)**2\n    result = refine(expr, Q.positive(x) & Q.positive(y))\n    assert result == x**2 + 2*x*y + y**2\n", "compilation error", "compilation error"], "sample_1013": ["compilation error", "compilation error", "compilation error", "def test_issue_15092():\n    f = lambdify((x, y), [x + y, x - y], 'sympy')\n    assert f(1, 2) == [3, -1]\n", "compilation error"], "sample_874": ["    def test_get_feature_names_out_sparse():\n        sel = StepSelector()\n        sel.fit(sparse(X))\n        assert_array_equal(\n            sel.get_feature_names_out(input_features=feature_names),\n            np.array(feature_names)[sel.get_support(indices=True)],\n        )\n\n        with pytest.raises(ValueError, match=\"input_features must be None\"):\n            sel.get_feature_names_out(input_features=[\"\"])\n", "def test_output_numpy():\n    \"\"\"Check output dtype is consistent with input dtype when fit on pandas and transform on numpy\n    and vice versa.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    X = pd.DataFrame(\n        {\n            \"a\": pd.Series([1.0, 2.4, 4.5], dtype=np.float32),\n            \"b\": pd.Series([\"a\", \"b\", \"a\"], dtype=\"category\"),\n        }\n    )\n\n    sel = StepSelector().set_output(transform=\"numpy\")\n\n    # Fit on pandas, transform on numpy\n    sel.fit(X)\n    output = sel.transform(X.values)\n    assert output.dtype == np.float32\n\n    # Fit on numpy, transform on pandas\n    sel.fit(X.values)\n    output = sel.transform(X)\n    assert isinstance(output, pd.DataFrame)\n    assert output.values.dtype == np.float32\n\n", "def test_get_feature_names_out_empty():\n    \"\"\"\n    Test `get_feature_names_out` when no features are selected.\n    \"\"\"\n    sel = StepSelector(step=0)\n    sel.fit(X, y)\n    feature_names_out = sel.get_feature_names_out(input_features=feature_names)\n    assert feature_names_out == []\n\n", "    def test_fit_transform_empty():\n        sel = StepSelector()\n        Xt = sel.fit_transform(np.array([]).reshape(0, 10), y)\n        assert Xt.shape == (0, 10 // sel.step)\n        Xt = sel.transform(np.array([]).reshape(0, 10))\n        assert Xt.shape == (0, 10 // sel.step)\n\n", "    def test_set_output_none():\n        \"\"\"Test set_output=None: default to numpy arrays.\"\"\"\n        sel = StepSelector().set_output(transform=\"default\")\n        sel.fit(X, y)\n        Xt = sel.transform(X)\n        assert isinstance(Xt, np.ndarray)\n"], "sample_1060": ["def test_NumPyPrinter_print_CodegenArrayDiagonal():\n    n = NumPyPrinter()\n    from sympy.codegen import CodegenArrayDiagonal\n    from sympy.tensor.array.dense_ndim_array import DenseNDimArrayTensor\n    from sympy.matrices import MatrixSymbol\n    A = MatrixSymbol('A', 2, 2)\n    expr = CodegenArrayDiagonal(DenseNDimArrayTensor(A))\n    assert n.doprint(expr) == 'numpy.diagonal(A, 0, axis1=0, axis2=1)'\n\n", "def test_SciPyPrinter_print_assoc_legendre():\n    p = SciPyPrinter()\n    expr = assoc_legendre(2, 3, x)\n    assert p.doprint(expr) == 'scipy.special.lpmv(3, 2, x)'\n", "compilation error", "compilation error", "    def test_NumPyPrinter_print_CodegenArrayDiagonal():\n        n = NumPyPrinter()\n        from sympy.codegen.array_utils import CodegenArrayDiagonal\n        A = symbols('A', cls=MatrixSymbol, shape=(3, 3))\n        diag = CodegenArrayDiagonal(A, diagonal_indices=[(0, 0)])\n        assert n.doprint(diag) == 'numpy.diagonal(A, 0, axis1=0, axis2=0)'\n"], "sample_978": ["def test_empty_knots():\n    d = 1\n    knots = []\n    with pytest.raises(ValueError):\n        bspline_basis_set(d, knots, x)\n\n", "def test_degree_negative():\n    d = -1\n    knots = range(5)\n    with pytest.raises(ValueError):\n        bspline_basis_set(d, knots, x)\n", "    def test_degree_zero_empty_knots():\n        d = 0\n        knots = []\n        splines = bspline_basis_set(d, knots, x)\n        assert splines == []\n", "def test_bspline_basis_degree_zero_negative_knots():\n    d = 0\n    knots = [-2, -1, 0, 1, 2]\n    splines = bspline_basis_set(d, knots, x)\n    for i in range(len(splines)):\n        assert splines[i] == Piecewise((1, Interval(knots[i], knots[i + 1]).contains(x)),\n                (0, True))\n", "def test_bspline_basis_set_empty_knots():\n    knots = []\n    d = 1\n    with pytest.raises(ValueError):\n        bspline_basis_set(d, knots, x)\n"], "sample_38": ["def test_invalid_naxis():\n    with pytest.raises(ValueError) as exc:\n        w = wcs.WCS(naxis=-1)\n    assert exc.value.args[0] == 'naxis must be a positive integer.'\n\n    with pytest.raises(ValueError) as exc:\n        w = wcs.WCS(naxis=[1, 2, 3])\n    assert exc.value.args[0] == 'naxis must be a single integer.'\n", "def test_zero_size_input_scalar():\n    with fits.open(get_pkg_data_filename('data/sip.fits')) as f:\n        w = wcs.WCS(f[0].header)\n\n    result = w.all_pix2world(2, 0)\n    assert_array_equal(result, [np.array(2.)])\n\n    result = w.all_world2pix(2, 0)\n    assert_array_equal(result, [np.array(2.)])\n", "compilation error", "    def test_wcs_read_header_with_empty_keywords():\n        \"\"\"\n        Issue #7976\n        Test for reading header with empty keywords like WCSNAME=''\n\n        \"\"\"\n        header = \"\"\"\n        SIMPLE  = T\n        BITPIX  = -64\n        NAXIS   = 2\n        NAXIS1  = 512\n        NAXIS2  = 512\n        WCSAXES = 2\n        CRPIX1  = 256.5\n        CRPIX2  = 256.5\n        CRVAL1  = 0.0\n        CRVAL2  = 0.0\n        CDELT1  = -0.0016666666666666667\n        CDELT2  = 0.0016666666666666667\n        CTYPE1  = 'RA---TAN'\n        CTYPE2  = 'DEC--TAN'\n        WCSNAME = ''  \n        \"\"\"\n\n        w = wcs.WCS(fits.Header.fromstring(header))\n\n        assert isinstance(w, wcs.WCS)\n        assert w.wcs.naxis == 2\n\n        # Test if the WCSNAME attribute was correctly set to ''\n        assert w.wcs.wcsname == ''\n\n", "compilation error"], "sample_471": ["compilation error", "    def test_jsonfield_default(self):\n        f = JSONField()\n        self.assertEqual(f.to_python('{\"key\": \"value\"}'), {'key': 'value'})\n        with self.assertRaisesMessage(ValidationError, \"'Enter a valid JSON.'\"):\n            f.to_python('invalid json')\n        self.assertEqual(f.prepare_value({'key': 'value'}), '{\"key\": \"value\"}')\n        self.assertIsNone(f.prepare_value(None))\n        self.assertEqual(f.bound_data('{\"key\": \"value\"}', None), {'key': 'value'})\n        self.assertEqual(f.bound_data(None, None), None)\n        self.assertEqual(f.bound_data('invalid json', None), InvalidJSONInput('invalid json'))\n\n", "    def test_integerfield_zero_step_size(self):\n        f = IntegerField(step_size=0)\n        with self.assertRaises(ValueError):\n            f.clean('1')\n", "    def test_integerfield_invalid_input(self):\n        f = IntegerField()\n        with self.assertRaisesMessage(ValueError, \"'int' object is not callable\"):\n            f.clean(int)\n", "    def test_integerfield_max_value_validation_with_float(self):\n        f = IntegerField(max_value=10)\n        with self.assertRaisesMessage(\n            ValidationError, \"'Ensure this value is less than or equal to 10.'\"\n        ):\n            f.clean(10.5)\n"], "sample_331": ["    def test_parse_time_with_microseconds(self):\n        self.assertEqual(parse_time('10:10:30.123456'), time(10, 10, 30, 123456))\n        self.assertEqual(parse_time('10:10:30,123456'), time(10, 10, 30, 123456))\n        self.assertEqual(parse_time('10:10:30.123'), time(10, 10, 30, 123000))\n        self.assertEqual(parse_time('10:10:30,123'), time(10, 10, 30, 123000))\n\n", "    def test_invalid_iso_8601(self):\n        test_values = (\n            ('P4DT', None),\n            ('PT', None),\n            ('P5', None),\n            ('P5H', None),\n            ('P5M', None),\n            ('P5S', None),\n            ('P4Y5M', None),\n            ('P4DT5H5M5S', None),\n        )\n        for source in test_values:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_duration(source))\n\n", "    def test_parse_datetime_timezones(self):\n        # Test handling of time zones in various formats\n        # Note: Timezones are not parsed, they are just checked to be present\n        test_values = (\n            ('2012-04-23T09:15:00Z', '+00:00'),\n            ('2012-04-23T09:15:00+00:00', '+00:00'),\n            ('2012-04-23T09:15:00+00', '+00:00'),\n            ('2012-04-23T09:15:00+0000', '+00:00'),\n            ('2012-04-23T09:15:00+02:30', '+02:30'),\n            ('2012-04-23T09:15:00-05:00', '-05:00'),\n            ('2012-04-23T09:15:00-05', '-05:00'),\n            ('2012-04-23T09:15:00-0530', '-05:30'),\n        )\n        for dt_str, tz_str in test_values:\n            with self.subTest(dt_str=dt_str):\n                dt = parse_datetime(dt_str)\n                self.assertEqual(dt.tzinfo.tzname(None), tz_str)\n\n", "    def test_invalid_formats(self):\n        invalid_inputs = (\n            '20120423T091500',\n            '2012-04-23T091590',\n            '2012-04-23T09:15:00+00:60',\n            '2012-04-23T09:15:00+00:01:00',\n            '2012-04-23 09:15:00+00:01',\n            '10:15:00+',\n            '10:15:00:00',\n            '10:15:00.123456789',\n            '10:15:00,1234567890',\n            '1 day 01:00:00',\n            '1 day 00:60:00',\n            '1 day 00:00:60',\n            'Days 00:00:01',\n\n        )\n        for source in invalid_inputs:\n            with self.subTest(source=source):\n                with self.assertRaises(ValueError):\n                    parse_datetime(source)\n\n", "    def test_invalid_iso_8601_format(self):\n        test_values = (\n            ('P4DT', None),\n            ('PT', None),\n            ('P5', None),\n            ('P4D5H', None),\n            ('P4D5M1S', None),\n            ('P4D5M1S.123456789', None),\n        )\n        for source in test_values:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_duration(source))\n"], "sample_729": ["    def test_enet_warm_start_after_refit():\n        X, y, _, _ = build_dataset()\n        # Train a model with a specific set of parameters\n        model = ElasticNet(alpha=1e-2, l1_ratio=0.5, warm_start=True,\n                          max_iter=10).fit(X, y)\n\n        # Refits the model with new parameters while keeping warm_start=True\n        model.set_params(alpha=1e-3, l1_ratio=0.3).fit(X, y)\n\n        #  Check if the number of iterations is less than when trained from scratch\n        assert model.n_iter_ < 10\n\n", "    def test_enet_intercept_scaling():\n        # Test that the intercept is properly scaled when fit_intercept=True\n        # and the data is not centered\n\n        X, y, _, _ = build_dataset(n_samples=20, n_features=10)\n        # Shift the data so it's not centered around zero\n        X[:, 0] += 10\n        y += 5\n\n        # Fit the model with and without intercept\n        enet_intercept = ElasticNet(fit_intercept=True)\n        enet_no_intercept = ElasticNet(fit_intercept=False)\n\n        enet_intercept.fit(X, y)\n        enet_no_intercept.fit(X, y)\n\n        # Check that the intercept is not zero\n        assert_not_equal(enet_intercept.intercept_, 0)\n\n        # Check that the coefficients are different\n        assert_not_array_equal(enet_intercept.coef_, enet_no_intercept.coef_)\n\n        # Check that the predictions are the same\n        assert_array_almost_equal(enet_intercept.predict(X),\n                                  enet_no_intercept.predict(X))\n", "compilation error", "    def test_enet_l1_ratio_deprecation():\n        X, y, _, _ = build_dataset()\n        # Test deprecation warning for l1_ratio_\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            clf = ElasticNet(l1_ratio_=0.5)\n            clf.fit(X, y)\n            assert_equal(len(w), 1)\n            assert_(issubclass(w[-1].category, DeprecationWarning))\n            assert_(\"l1_ratio_ is deprecated\" in str(w[-1].message))\n\n", "    def test_enet_positive_intercept():\n        X, y, _, _ = build_dataset(n_samples=50, n_features=20)\n        clf = ElasticNet(fit_intercept=True, positive=True)\n\n        # Positive intercept is only supported for fit_intercept=True\n        clf.fit(X, y)\n\n        assert_greater(clf.intercept_, 0)\n\n\n"], "sample_858": ["    def test_voting_estimator_weights_none():\n        # Check that passing None as weights raises a ValueError\n        clf1 = LogisticRegression(random_state=123)\n        clf2 = RandomForestClassifier(random_state=123)\n        clf3 = GaussianNB()\n        with pytest.raises(ValueError):\n            VotingClassifier(estimators=[('lr', clf1), ('rf', clf2),\n                                       ('nb', clf3)],\n                            weights=None).fit(X, y)\n\n        \n", "    def test_voting_classifier_empty_estimator_list():\n        clf1 = LogisticRegression(random_state=123)\n        clf2 = RandomForestClassifier(random_state=123)\n        \n        # Test empty estimator list\n        eclf = VotingClassifier(estimators=[], voting='hard')\n        msg = \"The 'estimators' parameter should contain at least one estimator.\"\n        with pytest.raises(ValueError, match=msg):\n          eclf.fit(X, y)\n        \n        \n        eclf = VotingClassifier(estimators=[('lr', clf1)], voting='hard')\n        # Test empty estimator list\n        eclf.set_params(estimators=[]).fit(X, y)\n        \n        msg = \"The 'estimators' parameter should contain at least one estimator.\"\n        with pytest.raises(ValueError, match=msg):\n          eclf.predict(X)\n\n        \n\n", "    def test_invalid_voting_method(self):\n        clf1 = LogisticRegression()\n        clf2 = RandomForestClassifier()\n        with pytest.raises(ValueError, match=\"Voting must be 'soft' or 'hard'\"):\n            VotingClassifier([('lr', clf1), ('rf', clf2)], voting='invalid')\n\n", "    def test_voting_estimator_with_n_jobs(self):\n        clf1 = LogisticRegression(random_state=123)\n        clf2 = RandomForestClassifier(random_state=123, n_jobs=2)\n        clf3 = GaussianNB()\n        eclf1 = VotingClassifier(estimators=[\n            ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n            voting='soft',\n            n_jobs=2).fit(X, y)\n        eclf2 = VotingClassifier(estimators=[\n            ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n            voting='soft',\n            n_jobs=1).fit(X, y)\n        assert_array_equal(eclf1.predict(X), eclf2.predict(X))\n        assert_array_almost_equal(eclf1.predict_proba(X), eclf2.predict_proba(X))\n\n", "def test_voting_classifier_with_custom_voting_function():\n    \"\"\"Check that a custom voting function can be used with VotingClassifier.\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    clf3 = GaussianNB()\n\n        # Example: Average probabilities of the first two classifiers\n        return np.mean(clf_probs[:, :2], axis=1)\n\n    eclf = VotingClassifier(estimators=[\n                ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n                voting=custom_voting)\n\n    eclf.fit(X, y)\n    y_pred = eclf.predict(X)\n"], "sample_607": ["def test_get_backend(engine_name, expected_engine_class):\n    engines = {\n        \"engine1\": DummyBackendEntrypoint1,\n        \"engine2\": DummyBackendEntrypoint2,\n    }\n\n    backend = plugins.get_backend(engine_name)\n    assert isinstance(backend, expected_engine_class)\n", "def test_detect_parameters():\n    backend = DummyBackendEntrypoint1()\n    detected_parameters = plugins.detect_parameters(backend.open_dataset)\n    assert detected_parameters == (\"filename_or_obj\", \"decoder\")\n\n    backend = DummyBackendEntrypoint2()\n    detected_parameters = plugins.detect_parameters(backend.open_dataset)\n    assert detected_parameters == (\"filename_or_obj\", \"decoder\")\n", "def test_detect_parameters():\n    class TestBackend:\n            pass\n\n    backend = TestBackend()\n    assert plugins.detect_parameters(backend.open_dataset) == (\"filename_or_obj\", \"a\", \"b\")\n\n    class TestBackendKwargs:\n            pass\n\n    backend = TestBackendKwargs()\n    assert plugins.detect_parameters(backend.open_dataset) == (\"filename_or_obj\",)\n\n    class TestBackendArgs:\n            pass\n\n    backend = TestBackendArgs()\n    assert plugins.detect_parameters(backend.open_dataset) == (\"filename_or_obj\",)\n\n", "def test_build_engines_with_unavailable_backends():\n    dummy_pkg_entrypoint = pkg_resources.EntryPoint.parse(\n        \"engine2 = xarray.tests.test_plugins:backend_1\"\n    )\n    backend_entrypoints = plugins.build_engines([dummy_pkg_entrypoint])\n\n    assert \"engine1\" not in backend_entrypoints\n    assert \"engine2\" in backend_entrypoints\n", "def test_detect_parameters():\n    class DummyBackendEntrypoint(common.BackendEntrypoint):\n            pass\n\n    backend = DummyBackendEntrypoint()\n    parameters = plugins.detect_parameters(backend.open_dataset)\n    assert parameters == (\"filename_or_obj\", \"a\", \"b\")\n\n"], "sample_570": ["    def test_errorbar_none(self, long_df):\n        agg = EstimateAggregator(\"mean\", None)\n        out = agg(long_df, \"x\")\n        assert out[\"x\"] == long_df[\"x\"].mean()\n        assert pd.isna(out.get(\"xmin\"))\n        assert pd.isna(out.get(\"xmax\"))\n\n", "    def test_empty_data(self):\n         agg = EstimateAggregator(\"mean\")\n         data = pd.DataFrame({\"x\": []})\n         out = agg(data, \"x\")\n         assert out[\"x\"] == np.nan\n         assert pd.isna(out[\"xmin\"])\n         assert pd.isna(out[\"xmax\"])\n\n", "    def test_no_errorbar(self, long_df):\n\n        agg = EstimateAggregator(\"mean\")\n        out = agg(long_df, \"x\")\n        assert out[\"x\"] == long_df[\"x\"].mean()\n        assert \"xmin\" not in out\n        assert \"xmax\" not in out\n", "    def test_no_errorbar(self, long_df):\n\n        agg = EstimateAggregator(\"mean\")\n        out = agg(long_df, \"x\")\n        assert out[\"x\"] == long_df[\"x\"].mean()\n        assert 'xmin' not in out\n        assert 'xmax' not in out\n", "    def test_bad_stat(self):\n\n        with pytest.raises(ValueError):\n            EstimateAggregator(stat=\"invalid\")\n"], "sample_47": ["    def test_cleanse_setting_handles_none(self):\n        self.assertIsNone(cleanse_setting('none', None))\n", "    def test_cleanse_setting_handles_none(self):\n        self.assertIsNone(cleanse_setting('SETTING_NAME', None))\n", "    def test_cleanse_setting_recurses_in_nested_dictionary(self):\n        initial = {'user': {'login': 'cooper', 'password': 'secret'}}\n        expected = {'user': {'login': 'cooper', 'password': CLEANSED_SUBSTITUTE}}\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected)\n", "    def test_cleanse_setting_with_nested_dict(self):\n        initial = {'login': 'cooper', \n                   'data': {'password': 'secret', 'email': 'test@example.com'}}\n        expected = {'login': 'cooper',\n                    'data': {'password': CLEANSED_SUBSTITUTE, 'email': 'test@example.com'}}\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected)\n", "    def test_cleanse_setting_recurses_in_list_of_dictionaries(self):\n        initial = [{'login': 'cooper', 'password': 'secret'}, {'login': 'bob', 'password': 'another_secret'}]\n        expected = [{'login': 'cooper', 'password': CLEANSED_SUBSTITUTE}, {'login': 'bob', 'password': CLEANSED_SUBSTITUTE}]\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected)\n"], "sample_1053": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_642": ["def test_pre_process_options(capsys: CaptureFixture) -> None:\n    \"\"\"Test that pre-processing options work correctly.\"\"\"\n    with tempdir() as chroot:\n        with fake_home():\n            chroot_path = Path(chroot)\n            testutils.create_files([\"a/b/c/d/__init__.py\"])\n            os.chdir(chroot_path / \"a/b/c\")\n            run = Run([\"--rcfile\", \"fake.rc\"])\n            run._process_options()\n            out = capsys.readouterr()\n            assert \"--rcfile fake.rc\" in out.err\n", "    def test_preprocess_options(\n        self, run: Run, args: list[str], expected: list[str]", "def test_preprocess_options(option: str, expected_action: Callable, capsys: CaptureFixture, tmp_path: Path) -> None:\n    \"\"\"Test that pre-processing options are handled correctly.\"\"\"\n    fake_toml = tmp_path / \"fake.toml\"\n    with open(fake_toml, \"w\", encoding=\"utf8\") as f:\n        f.write(\n            \"\"\"", "    def test_init_hook(capsys: CaptureFixture, tmp_path: Path) -> None:\n        \"\"\"Test the init_hook preprocessor.\"\"\"\n        fake_pylintrc = tmp_path / \".pylintrc\"\n        with open(fake_pylintrc, \"w\") as f:\n            f.write(\"[MESSAGES CONTROL]\\n\")\n            f.write(\"disable=missing-module-docstring\\n\")\n\n        fake_script = tmp_path / \"test.py\"\n        with open(fake_script, \"w\") as f:\n            f.write(\"import sys\\nprint('Hello')\\n\")\n\n        with mock.patch.object(sys, \"argv\", [\"pylint\", \"--rcfile=\" + str(fake_pylintrc),\n                                            \"--init-hook=\" + \"import sys; sys.path.append('/my/extra/path')\",\n                                            str(fake_script)]):\n\n            run = Run([])\n            run.parse_arguments()\n            assert \"/my/extra/path\" in sys.path\n", "def test_preprocess_options(\n    option: str, value: str, expected_value: str, run: Run"], "sample_146": ["    def test_language_code_not_in_languages(self):\n        msg = (\n            'You have provided a value for the LANGUAGE_CODE setting that is '\n            'not in the LANGUAGES setting.'\n        )\n        with self.settings(LANGUAGE_CODE='fr'):\n            self.assertEqual(check_language_settings_consistent(None), [\n                Error(msg, id='translation.E004'),\n            ])\n", "    def test_consistent_language_settings_with_supported_variants(self):\n        msg = (\n            'You have provided a value for the LANGUAGE_CODE setting that is '\n            'not in the LANGUAGES setting.'\n        )\n        with self.settings(LANGUAGE_CODE='en-gb'):\n            self.assertEqual(check_language_settings_consistent(None), [])\n", "    def test_consistent_language_settings_with_unsupported_variant(self):\n        msg = (\n            'You have provided a value for the LANGUAGE_CODE setting that is not in '\n            'the LANGUAGES setting.'\n        )\n        with self.settings(LANGUAGE_CODE='en-gb'):\n            self.assertEqual(check_language_settings_consistent(None), [\n                Error(msg, id='translation.E004'),\n            ])\n", "    def test_valid_language_code_in_languages(self):\n        for tag in self.valid_tags:\n            with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n                self.assertEqual(check_language_settings_consistent(None), [])\n", "    def test_consistent_language_settings(self):\n        with self.settings(LANGUAGE_CODE='en'):\n            self.assertEqual(check_language_settings_consistent(None), [])\n"], "sample_927": ["    def check_lookup_key_overload_warning(app, status, warning):\n        app.builder.build_all()\n        ws = filter_warnings(warning, \"lookup-key-overload\")\n        assert len(ws) == 1\n        assert \"WARNING: cpp: lookup key 'f' is overloaded, specify the correct signature.\" in ws[0]\n", "def test_cpp_domain_config(app, status, warning):\n    app.config.cpp_id_attributes = ['CppAttr']\n    app.config.cpp_paren_attributes = ['CppParenAttr']\n    app.builder.build_all()\n    output = (app.outdir / 'roles.html').read_text()\n\n        pattern = r'<a .*?><code .*?>%s<span .*?>%s</span></code></a>' % target\n        res = re.search(pattern, output)\n        expect = '''\\", "    def check(target):\n        class Config:\n            cpp_id_attributes = [\"id_attr\"]\n            cpp_paren_attributes = [\"paren_attr\"]\n        parser = DefinitionParser(target, location=None,\n                                  config=Config())\n        ast, isShorthand = parser.parse_xref_object()\n        parser.assert_end()\n    ", "    def check(spec, text, file):\n        pattern = '<li><p>%s<a .*?><code .*?><span .*?>%s</span></code></a></p></li>' % spec\n        res = re.search(pattern, text)\n        if not res:\n            print(\"Pattern\\n\\t%s\\nnot found in %s\" % (pattern, file))\n            assert False", "    def check(target, expected_id):\n        class Config:\n            cpp_id_attributes = [\"id_attr\"]\n            cpp_paren_attributes = [\"paren_attr\"]\n        parser = DefinitionParser(target, location=None,\n                                  config=Config())\n        ast, isShorthand = parser.parse_xref_object()\n        parser.assert_end()\n        actual_id = ast.get('id')\n        assert actual_id == expected_id, f\"Expected {expected_id}, got {actual_id}\"\n"], "sample_1175": ["compilation error", "compilation error", "def test_issue_18631():\n    from sympy import Symbol, Function, Derivative\n    x = Symbol('x')\n    f = Function('f')\n    assert pretty(Derivative(f(x), x, 2)) == \"d\u00b2f(x)/dx\u00b2\"\n    assert upretty(Derivative(f(x), x, 2)) == \"d\u00b2f(x)/dx\u00b2\"\n", "compilation error", "compilation error"], "sample_901": ["    def test_kmeans_empty_cluster_init():\n        # check that empty clusters are correctly handled during initialization\n        X = np.array([[1, 2], [3, 4]])\n        km = KMeans(n_clusters=3, init='k-means++', random_state=42)\n        km.fit(X)\n        assert len(km.cluster_centers_) == 3\n", "    def test_k_means_with_empty_clusters(self):\n        # Test that KMeans handles empty clusters correctly\n        # when using init='random'.\n\n        X = np.array([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4]], dtype=float)\n        true_labels = np.array([0, 0, 1, 1, 1])\n\n        # Create a KMeans instance with more clusters than data points\n        km = KMeans(n_clusters=6, random_state=42, init='random')\n\n        # Fit the model to the data\n        km.fit(X)\n\n        # Check that the number of clusters is correct\n        assert len(np.unique(km.labels_)) == 6\n\n        # Check that the inertia is non-negative\n        assert km.inertia_ >= 0\n", "    def test_kMeans_sparse_zeroed_input(self):\n        X = sp.csr_matrix(np.array([[1, 2], [3, 4], [0, 0]]))\n        km = KMeans(n_clusters=2)\n        km.fit(X)\n        assert km.labels_.shape == (3,)\n", "def test_k_means_empty_cluster_relocated_random_init():\n    # check that empty clusters are correctly relocated when using sample\n    # weights and random initialization  (#13486)\n    X = np.array([[-1], [1]])\n    sample_weight = [1.9, 0.1]\n\n    km = KMeans(n_clusters=2, random_state=42)\n    km.fit(X, sample_weight=sample_weight)\n\n    assert len(set(km.labels_)) == 2\n    # The relocated cluster center should be closer to the data point\n    # with higher weight\n    assert np.isclose(km.cluster_centers_[0][0], -1, atol=1e-2)\n\n", "    def test_k_means_empty_cluster_with_empty_data():\n        # Check that KMeans can handle empty data and doesn't crash\n        # when encountering empty clusters\n        X = np.array([])\n        km = KMeans(n_clusters=2)\n        assert km.fit(X).cluster_centers_.shape == (2, 0)  \n"], "sample_303": ["    def test_runshell_with_env(self):\n        with mock.patch('subprocess.run') as run:\n            with mock.patch.object(\n                BaseDatabaseClient,\n                'settings_to_cmd_args_env',\n                return_value=([], {'KEY': 'VALUE'}),\n            ):\n                self.client.runshell(None)\n            expected_env = {'KEY': 'VALUE', **os.environ}\n            run.assert_called_once_with([], env=expected_env, check=True)\n", "    def test_runshell_use_custom_env(self):\n        custom_env = {'KEY': 'VALUE'}\n        with mock.patch('subprocess.run') as run:\n            with mock.patch.object(\n                BaseDatabaseClient,\n                'settings_to_cmd_args_env',\n                return_value=([], custom_env),\n            ):\n                self.client.runshell(None)\n            run.assert_called_once_with([], env=custom_env, check=True)\n", "    def test_runshell_with_env(self):\n        with mock.patch('subprocess.run') as run:\n            with mock.patch.object(\n                BaseDatabaseClient,\n                'settings_to_cmd_args_env',\n                return_value=([], {'KEY': 'VALUE'}),\n            ):\n                self.client.runshell(None)\n            run.assert_called_once_with([], env={'KEY': 'VALUE'}, check=True)\n", "    def test_runshell(self):\n        args = ['arg1', 'arg2']\n        env = {'KEY': 'value'}\n        with mock.patch('subprocess.run') as run:\n            with mock.patch.object(\n                BaseDatabaseClient,\n                'settings_to_cmd_args_env',\n                return_value=(args, env),\n            ):\n                self.client.runshell(None)\n            run.assert_called_once_with(args, env=env, check=True)\n", "    def test_runshell_merge_env(self):\n        with mock.patch('subprocess.run') as run:\n            with mock.patch.object(\n                BaseDatabaseClient,\n                'settings_to_cmd_args_env',\n                return_value=([], {'KEY': 'VALUE'}),\n            ):\n                os.environ['OTHER_KEY'] = 'OTHER_VALUE'\n                self.client.runshell(None)\n                run.assert_called_once_with(\n                    [], env={'KEY': 'VALUE', 'OTHER_KEY': 'OTHER_VALUE'}, check=True\n                )\n\n"], "sample_83": ["    def test_tag_function(self):\n            return Node()\n        self.library.tag_function(func)\n        self.assertIn(getattr(func, \"_decorated_function\", func).__name__, self.library.tags) \n", "    def test_tag_function(self):\n            return Node()\n        self.library.tag_function(func)\n        self.assertEqual(self.library.tags[func.__name__], func)\n", "    def test_filter_flags(self):\n        @self.library.filter(name='test_filter', is_safe=True)\n            return value\n        self.assertTrue(func._filter_name == 'test_filter')\n        self.assertTrue(func.is_safe)\n\n", "    def test_tag_function(self):\n            return Node()\n        self.library.tag_function(func)\n        self.assertEqual(self.library.tags[func.__name__], func)\n", "    def test_tag_function(self):\n            return Node()\n        self.library.tag_function(func)\n        self.assertEqual(self.library.tags[func.__name__], func)\n\n"], "sample_149": ["    def test_permission_codename_length_with_long_model_name(self):\n        model_name = 'X' * 80\n        model = type(model_name, (models.Model,), {'__module__': self.__module__})\n        class Meta:\n            permissions = [\n                ('my_permission', 'Permission'),\n            ]\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [])\n\n", "    def test_permission_name_length_with_custom_user_model(self):\n        class CustomUser(AbstractBaseUser):\n            username = models.CharField(max_length=30, unique=True)\n            USERNAME_FIELD = 'username'\n\n        class Checked(models.Model):\n            user = models.ForeignKey(CustomUser, on_delete=models.CASCADE)\n            class Meta:\n                verbose_name = 'Very long verbose name for the Checked model'  * 5\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The verbose_name of model 'auth_tests.Checked' must be at most 244 \"\n                \"characters for its builtin permission names to be at most 255 characters.\",\n                obj=Checked,\n                id='auth.E007',\n            ),\n        ])\n\n", "    def test_is_anonymous_authenticated_attributes(self):\n        \"\"\"\n        <User Model>.is_anonymous/is_authenticated must not be methods.\n        \"\"\"\n        class CustomUserWithMethods(AbstractBaseUser):\n            username = models.CharField(max_length=30, unique=True)\n            USERNAME_FIELD = 'username'\n\n            is_anonymous = True\n            is_authenticated = False\n\n        errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n        self.assertEqual(errors, [])\n\n", "    def test_model_without_permissions(self):\n        class Checked(models.Model):\n            pass\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [])\n", "    def test_permission_name_max_length_with_long_model_name(self):\n        model_name = 'X' * 94\n        model = type(model_name, (models.Model,), {'__module__': self.__module__})\n        class Meta:\n            permissions = [\n                ('some_permission', 'Some permission'),\n            ]\n        model.Meta = Meta\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The verbose_name of model 'auth_tests.%s' must be at most 244 \"\n                \"characters for its builtin permission names to be at most 255 \"\n                \"characters.\" % model_name,\n                obj=model,\n                id='auth.E007',\n            ),\n        ])\n"], "sample_974": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_468": ["    def test_context_pop_exception(self):\n        c = Context()\n        c.push()\n        with self.assertRaises(ContextPopException):\n            c.pop()\n", "    def test_bind_template_context_processors(self):\n        request = self.request_factory.get(\"/\")\n        context = RequestContext(request, {})\n\n        with context.bind_template(Template(\"\")):\n            processors = context._processors\n            self.assertEqual(len(context.dicts), 1 + len(processors))\n            self.assertIsInstance(context.dicts[1], dict)\n\n        self.assertEqual(len(context.dicts), 1)  # Back to initial state\n", "    def test_empty_context_processors(self):\n        request = self.request_factory.get(\"/\")\n        context = RequestContext(request)\n        self.assertEqual(len(context.dicts), 2)\n\n", "    def test_context_pop_exception(self):\n        request = self.request_factory.get(\"/\")\n        ctx = RequestContext(request, {})\n        with self.assertRaisesMessage(ContextPopException, \"pop() has been called more times than push()\"):\n            ctx.pop()\n            ctx.pop()\n\n", "    def test_context_pop_exception(self):\n        request = self.request_factory.get(\"/\")\n        ctx = RequestContext(request, {})\n        with self.assertRaises(ContextPopException):\n            ctx.pop()\n"], "sample_1107": ["    def test_next_perm():\n        for i in range(1, 6):\n            p = Permutation(range(i))\n            for j in range(factorial(i)):\n                assert p == Permutation(p.array_form)\n                p = p.next_perm()\n\n", "compilation error", "compilation error", "compilation error", "    def test_RGS_unrank():\n        assert RGS_unrank(0, 4) == {4: 1}\n        assert RGS_unrank(1, 4) == {1: 1, 3: 1}\n        assert RGS_unrank(6, 4) == {2: 2}\n        assert RGS_unrank(11, 4) == {1: 2, 2: 1}\n        assert RGS_unrank(14, 4) == {1: 4}\n"], "sample_1136": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_36": ["compilation error", "compilation error", "compilation error", "def test_biweight_location_empty():\n    with pytest.raises(ValueError) as e:\n        biweight_location([])\n    assert 'Input array must not be empty.' in str(e.value)\n", "compilation error"], "sample_575": ["    def test_label_unit(self, t, x):\n\n        ax = mpl.figure.Figure().subplots()\n        Temporal().label(unit=\"year\")._setup(t, Coordinate(), ax.xaxis)\n        formatter = ax.xaxis.get_major_formatter()\n        assert formatter.format_ticks([100]) == [\"1970 year\"]\n", "    def test_color_tuple_values(self, t, x):\n\n        cmap = color_palette(\"blend:b,g\", as_cmap=True)\n        s = Temporal((\"b\", \"g\"))._setup(t, Color())\n        normed = (x - x.min()) / (x.max() - x.min())\n        assert_array_equal(s(t), cmap(normed)[:, :3])  # FIXME RGBA\n\n", "    def test_coordinate_axis_with_category_dtype(self, x):\n\n        order = [\"b\", \"a\", \"d\", \"c\"]\n        x = x.astype(pd.CategoricalDtype(order))\n        ax = mpl.figure.Figure().subplots()\n        s = Nominal()._setup(x, Coordinate(), ax.xaxis)\n        assert_array_equal(s(x), np.array([1, 3, 0, 3], float))\n        f = ax.xaxis.get_major_formatter()\n        assert f.format_ticks([0, 1, 2, 3]) == order\n", "    def test_tick_format_date(self, t):\n\n        ax = mpl.figure.Figure().subplots()\n        Temporal(format=\"%Y-%m-%d\").tick()._setup(t, Coordinate(), ax.xaxis)\n        formatter = ax.xaxis.get_major_formatter()\n        assert isinstance(formatter, mpl.dates.DateFormatter)\n        assert formatter.fmt == \"%Y-%m-%d\" \n", "    def test_color_with_date_dtype(self, t):\n\n        cmap = color_palette(\"ch:\", as_cmap=True)\n        s = Temporal()._setup(t, Color())\n        normed = (mpl.dates.date2num(t) -mpl.dates.date2num(t.min())) / (mpl.dates.date2num(t.max()) - mpl.dates.date2num(t.min()))\n        assert_array_equal(s(t), cmap(normed)[:, :3]) # FIXME RGBA\n"], "sample_608": ["    def test_diff_array_repr_different_dtypes(self) -> None:\n        da_a = xr.DataArray(\n            np.array([[1, 2, 3], [4, 5, 6]], dtype=\"int64\"),\n            dims=(\"x\", \"y\"),\n        )\n        da_b = xr.DataArray(\n            np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], dtype=\"float64\"),\n            dims=(\"x\", \"y\"),\n        )\n        expected = dedent(\n            \"\"\"\\\n        Left and right DataArray objects are not identical\n        Differing values:\n        L\n            array([[1, 2, 3],\n                   [4, 5, 6]], dtype=int64)\n        R\n            array([[1., 2., 3.],\n                   [4., 5., 6.]], dtype=float64)\n        \"\"\"\n        )\n        actual = formatting.diff_array_repr(da_a, da_b, \"identical\")\n        assert actual == expected\n\n", "    def test_short_numpy_repr_empty_array(self) -> None:\n        array = np.array([])\n        actual = formatting.short_numpy_repr(array)\n        expected = \"array([], dtype=float64)\"\n        assert actual == expected\n", "    def test_format_item_dtype(self) -> None:\n        cases = [\n            (np.dtype(\"int64\"), \"int64\"),\n            (np.dtype(\"float32\"), \"float32\"),\n            (np.dtype(\"bool\"), \"bool\"),\n            (np.dtype(\"O\"), \"object\"),\n            (np.dtype(\"datetime64[ns]\"), \"datetime64[ns]\"),\n        ]\n        for item, expected in cases:\n            actual = formatting.format_item(item)\n            assert expected == actual\n", "    def test_diff_coords_repr_with_different_dtype(self) -> None:\n        coords_a = {\"x\": np.array([1, 2], dtype=\"int64\")}\n        coords_b = {\"x\": np.array([1.0, 2.0], dtype=\"float64\")}\n        expected = dedent(\n            \"\"\"\\\n        Differing coordinates:\n        L * x        (x) int64 1 2\n        R * x        (x) float64 1.0 2.0\n        \"\"\"\n        ).strip()\n        actual = formatting.diff_coords_repr(coords_a, coords_b, \"equals\")\n        assert expected == actual\n\n        coords_c = {\"x\": np.array([1, 2, 3], dtype=\"int64\")}\n        expected = dedent(\n            \"\"\"\\\n        Differing coordinates:\n        L * x        (x) int64 1 2\n        R * x        (x) int64 1 2 3\n        \"\"\"\n        ).strip()\n        actual = formatting.diff_coords_repr(coords_a, coords_c, \"equals\")\n        assert expected == actual\n\n", "    def test_diff_dataset_repr_identical_coords(self) -> None:\n        ds1 = xr.Dataset(\n            data_vars={\"foo\": ((\"x\", \"y\"), np.arange(4).reshape(2, 2))},\n            coords={\"x\": [0, 1], \"y\": [0, 1]},\n        )\n        ds2 = xr.Dataset(\n            data_vars={\"foo\": ((\"x\", \"y\"), np.arange(4).reshape(2, 2))},\n            coords={\"x\": [0, 1], \"y\": [0, 1]},\n        )\n\n        expected = (\n            \"Left and right Dataset objects are identical\"\n        )\n\n        actual = formatting.diff_dataset_repr(ds1, ds2, \"identical\")\n        assert actual == expected\n"], "sample_490": ["    def test_validate_with_related_object(self):\n        obj = UniqueConstraintRelatedProduct.objects.create(name=\"p1\")\n        obj.related.create(other_name=\"related_p1\")\n        constraint = UniqueConstraintRelatedProduct._meta.constraints[0]\n        with self.assertRaises(ValidationError) as cm:\n            constraint.validate(\n                UniqueConstraintRelatedProduct,\n                UniqueConstraintRelatedProduct(name=\"p1\"),\n            )\n        self.assertEqual(\n            cm.exception.messages[0],\n            \"Constraint \u201cname_related_uniq\u201d is violated.\",\n        )\n\n", "    def test_validate_condition_null_fields(self):\n        p1 = UniqueConstraintConditionProduct.objects.create(name=\"p1\")\n        constraint = UniqueConstraintConditionProduct._meta.constraints[0]\n        msg = \"Constraint \u201cname_without_color_uniq\u201d is violated.\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            constraint.validate(\n                UniqueConstraintConditionProduct,\n                UniqueConstraintConditionProduct(name=p1.name, color=None),\n            )\n        # Values not matching condition are ignored.\n        constraint.validate(\n            UniqueConstraintConditionProduct,\n            UniqueConstraintConditionProduct(name=p1.name, color=\"anything-but-none\"),\n        )\n        # Existing instances have their existing row excluded.\n        constraint.validate(UniqueConstraintConditionProduct, p1)\n        # Unique field is excluded.\n        constraint.validate(\n            UniqueConstraintConditionProduct,\n            UniqueConstraintConditionProduct(name=p1.name, color=None),\n            exclude={\"name\"},\n        )\n", "    def test_violation_error_message(self):\n        constraint = models.UniqueConstraint(\n            fields=[\"name\"],\n            name=\"name_uniq\",\n            violation_error_message=\"Name already exists.\",\n        )\n        # Apply the constraint to the model.\n        UniqueConstraintProduct._meta.constraints = [constraint]\n\n        non_unique_product = UniqueConstraintProduct(\n            name=self.p1.name, color=self.p1.color\n        )\n        msg = \"Name already exists.\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            constraint.validate(UniqueConstraintProduct, non_unique_product)\n\n", "    def test_validate_condition_no_matching_row(self):\n        p1 = UniqueConstraintConditionProduct.objects.create(name=\"p1\", color=\"red\")\n        constraint = UniqueConstraintConditionProduct._meta.constraints[0]\n        msg = \"Constraint \u201cname_without_color_uniq\u201d is violated.\"\n\n        with self.assertRaisesMessage(ValidationError, msg):\n            constraint.validate(\n                UniqueConstraintConditionProduct,\n                UniqueConstraintConditionProduct(name=p1.name),\n            )\n", "    def test_nulls_distinct_with_expressions(self):\n        unique_constraint = models.UniqueConstraint(\n            Lower(\"title\"),\n            name=\"book_func_nulls_distinct_uniq\",\n            nulls_distinct=True,\n        )\n        self.assertEqual(unique_constraint.nulls_distinct, True)\n"], "sample_65": ["    def test_i18n_context(self):\n        self.selenium.get(self.live_server_url + '/jsi18n_context/')\n        elem = self.selenium.find_element_by_id('context_string')\n        self.assertEqual(elem.text, 'String with context')\n", "    def test_i18n_template_non_english_default(self):\n        self.selenium.get(self.live_server_url + '/jsi18n_template/')\n        elem = self.selenium.find_element_by_id(\"gettext\")\n        self.assertEqual(elem.text, \"Eliminar\")\n", "    def test_i18n_english_variant_plural(self):\n        self.selenium.get(self.live_server_url + '/jsi18n/')\n\n        elem = self.selenium.find_element_by_id(\"plural\")\n        self.assertEqual(elem.text, \"1 message\")\n        elem = self.selenium.find_element_by_id(\"plural2\")\n        self.assertEqual(elem.text, \"2 messages\")\n", "    def test_jsi18n_with_context(self):\n        with override('fr'):\n            response = self.client.get('/jsi18n_context/')\n            self.assertContains(response, '\"month name\\\\u0004May\": \"mai\"', 1)\n", "    def test_jsi18n_plural_forms(self):\n        with override('ru'):\n            response = self.client.get('/jsi18n/')\n            self.assertContains(response, '(n != 1)')\n            self.assertContains(response, '\"{count} plural3\": [')\n\n"], "sample_1100": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_737": ["def test_vectorizer_with_empty_document():\n    # Test that vectorizers handle empty documents gracefully\n\n    vect = CountVectorizer()\n    X = vect.fit_transform(['hello world', '', 'hello hello'])\n    assert_equal(X.shape, (3, 2))\n    assert_array_equal(X[1].toarray(), np.zeros((1, 2)))\n\n    vect = TfidfVectorizer()\n    X = vect.fit_transform(['hello world', '', 'hello hello'])\n    assert_equal(X.shape, (3, 2))\n    assert_array_equal(X[1].toarray(), np.zeros((1, 2)))\n\n    vect = HashingVectorizer()\n    X = vect.fit_transform(['hello world', '', 'hello hello'])\n    assert_equal(X.shape, (3, 2 ** 20))\n    assert_array_equal(X[1].toarray(), np.zeros((1, 2 ** 20)))\n", "def test_tfidfvectorizer_lowercase_parameter():\n    corpus = ['THIS SHOULD BE LOWERcased', 'This should also be lowercased']\n    \n    vect_lowercase = TfidfVectorizer(lowercase=True)\n    vect_no_lowercase = TfidfVectorizer(lowercase=False)\n\n    X_lower = vect_lowercase.fit_transform(corpus).toarray()\n    X_no_lower = vect_no_lowercase.fit_transform(corpus).toarray()\n\n    assert_array_equal(X_lower, X_no_lower)\n\n", "    def test_vectorizer_empty_corpus():\n        # Ensure correct handling of empty corpus for CountVectorizer\n        vect = CountVectorizer()\n        X = vect.fit_transform([])\n        assert_array_equal(X.shape, (0, 0))\n        assert_equal(len(vect.vocabulary_), 0)\n", "    def test_vectorizer_empty_input(self):\n        for vec in [CountVectorizer(), TfidfVectorizer(), HashingVectorizer()]:\n            with pytest.raises(ValueError) as e:\n                vec.fit([])\n            assert \"Empty input corpus\" in str(e.value)\n\n            with pytest.raises(ValueError) as e:\n                vec.fit_transform([])\n            assert \"Empty input corpus\" in str(e.value)\n", "    def test_vectorizer_empty_vocabulary():\n        # test for empty vocabulary\n        for vec in [CountVectorizer(), TfidfVectorizer(), HashingVectorizer()]:\n            \n            with pytest.raises(ValueError):\n                vec.fit([])\n\n            with pytest.raises(ValueError):\n                vec.fit_transform([])\n            \n            with pytest.raises(ValueError):\n                vec.transform([])\n"], "sample_961": ["compilation error", "compilation error", "compilation error", "def test_signature_param_default_value(app):\n    text = (\".. py:function:: func\\n\"\n            \"   :param int x: A description\\n\"\n            \"   :param int y=10: Another description\")\n    doctree = restructuredtext.parse(app, text)\n\n    assert_node(doctree, (addnodes.index, desc, desc_content))\n    assert_node(doctree[1][1][0], desc_signature,\n                [desc_parameterlist, (desc_param[0], desc_param[1], (desc_name,\n                                                                     desc_addname,\n                                                                     desc_annotation,\n                                                                     desc_content))])\n\n    assert_node(doctree[1][1][0][0][0],\n                ([desc_param, ([desc_name, \"x\"],\n                [desc_annotation, \":\"],\n                [desc_content, nodes.paragraph],\n                [desc_addname, \"int\"])]))\n    assert_node(doctree[1][1][0][0][1],\n                ([desc_param, ([desc_name, \"y\"],\n                [desc_annotation, \"=\"],\n                [desc_content, nodes.paragraph],\n                [desc_addname, \"int\"])]))\n    assert_node(doctree[1][1][0][0][1][0][0], ([desc_name, \"y\"]))\n    assert_node(doctree[1][1][0][0][1][0][1], ([desc_annotation, \"=\"]))\n    assert_node(doctree[1][1][0][0][1][0][2], ([desc_content, nodes.paragraph,\n                                                    nodes.literal, \"10\"]))\n\n", "def test_literal_eval(app):\n    text = \".. py:function:: eval(x)\\n   :param x: A string to be evaluated.\\n   :return: The result of evaluating `x` using `literal_eval`.\\n\"\n    doctree = restructuredtext.parse(app, text)\n\n    assert_node(doctree, (addnodes.index,\n                          desc,\n                          desc,\n                          desc))\n    assert_node(doctree[1][1][0][0], addnodes.literal_emphasis, \"literal_eval\")\n\n\n"], "sample_688": ["    def test_collect_symlink_in_pkg(testdir):\n        \"\"\"Collect symlink to Python file in package.\"\"\"\n        sub = testdir.mkdir(\"sub\")\n        sub.ensure(\"__init__.py\")\n        real = sub.join(\"real.py\")\n        real.write(\"def test_nodeid(): pass\")\n\n        symlink = sub.join(\"link_to_real.py\")\n        symlink_or_skip(real, symlink)\n\n        result = testdir.runpytest(\"-v\", str(sub))\n        result.stdout.fnmatch_lines([\"sub/real.py::test_nodeid PASSED*\", \"*1 passed in*\"])\n        assert result.ret == 0\n\n", "def test_collect_symlink_circle(testdir):\n    \"\"\"Check symlink in a circular dependency chain doesn't lead to infinite recursion.\"\"\"\n    p1 = testdir.mkdir(\"p1\")\n    p2 = testdir.mkdir(\"p2\")\n\n    p1.join(\"testfile.py\").write(\"import p2\")\n    p2.join(\"testfile.py\").write(\n        f'import os\\n'\n        f'os.symlink(os.path.join(os.getcwd(), \"../p1\"), \"p1\")'\n        f'\\nimport p1'\n    )\n\n    result = testdir.runpytest(\n        \"--import-mode=importlib\",\n    )\n    # Should not hang or crash!\n    result.stdout.fnmatch_lines([\"*1 error in*\"])\n    assert result.ret == ExitCode.INTERRUPTED\n", "    def test_collect_file_with_invalid_pickle(testdir: Testdir) -> None:\n        testdir.makepyfile(\n            \"\"\"\n            import pickle\n\n                pickle.loads(b'some_invalid_pickle_data')\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.assert_outcomes(errors=1)\n", "    def test_collect_symlink_dir_with_init(pytester: Pytester) -> None:\n        \"\"\"A symlinked directory with __init__.py is collected.\"\"\"\n        dir = pytester.mkdir(\"dir\")\n        dir.joinpath(\"__init__.py\").write_text(\"\", \"utf-8\")\n        dir.joinpath(\"test_it.py\").write_text(\"def test_it(): pass\", \"utf-8\")\n        pytester.path.joinpath(\"symlink_dir\").symlink_to(dir)\n        result = pytester.runpytest()\n        result.assert_outcomes(passed=1)\n", "    def test_collect_with_dunder_init_in_module(testdir):\n        \"\"\"Ensure __init__ methods in modules are not treated as test functions.\"\"\"\n        testdir.makepyfile(\n            \"\"\"\n            class MyModule:\n\n                    pass\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines([\"*1 passed in*\"])\n"], "sample_405": ["    def test_field_operation_deconstruct(self):\n        operation = migrations.AddField(\n            \"Pony\", \"height\", models.FloatField(null=True, default=5)\n        )\n        definition = operation.deconstruct()\n        self.assertEqual(definition[0], \"AddField\")\n        self.assertEqual(definition[1], [])\n        self.assertEqual(\n            definition[2],\n            [\n                \"Pony\",\n                \"height\",\n                models.FloatField(null=True, default=5),\n            ],\n        )\n", "    def test_field_operation_with_complex_references(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\n                \"Other\",\n                models.CASCADE,\n                related_name=\"related_things\",\n                to_field=\"other_field\",\n            ),\n        )\n        self.assertIs(operation.references_model(\"Model\", \"migrations\"), True)\n        self.assertIs(operation.references_model(\"Other\", \"migrations\"), True)\n        self.assertIs(\n            operation.references_field(\"Other\", \"other_field\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Model\", \"related_things\", \"migrations\"), True\n        )\n", "    def test_references_field_by_to_fields_m2m(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ManyToManyField(\"Other\", through=\"Through\", to_field=\"field\"),\n        )\n        self.assertIs(operation.references_field(\"Other\", \"field\", \"migrations\"), True)\n        self.assertIs(\n            operation.references_field(\"Other\", \"whatever\", \"migrations\"), False\n        )\n        self.assertIs(\n            operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False\n        )\n", "    def test_field_operation_deconstruct(self):\n        operation = FieldOperation(\n            \"MoDel\",\n                \"field\",\n                models.ForeignKey(\"Other\", models.CASCADE),\n        )\n        definition = operation.deconstruct()\n        self.assertEqual(definition[0], \"FieldOperation\")\n        self.assertEqual(definition[1], [])\n        self.assertEqual(definition[2], {\n            'model_name': 'MoDel',\n            'name': 'field',\n            'field': models.ForeignKey(\"Other\", models.CASCADE),\n        })\n", "    def test_references_field_by_related_name(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\"Other\", models.CASCADE, related_name=\"related_objects\")\n        )\n        self.assertIs(operation.references_field(\"Other\", \"related_objects\", \"migrations\"), True)\n\n"], "sample_515": ["    def test_colorbar_labelpad_outside_ticks(self):\n        fig, ax = plt.subplots()\n        im = ax.imshow([[0, 1], [2, 3]])\n        cb = fig.colorbar(im, label='My Label')\n        cb.ax.set_ylabel('My Y Label')\n        cb.ax.tick_params(width=10, length=15)\n        fig.draw_without_rendering()\n        # The label should be positioned outside the ticks\n        assert cb.ax.yaxis.get_label().get_position()[0] > cb.ax.spines['left'].get_position()[0]\n        assert cb.ax.yaxis.get_label().get_position()[0] < cb.ax.spines['right'].get_position()[0]\n", "compilation error", "    def test_colorbar_set_ticks_no_auto_location(self):\n        fig, ax = plt.subplots()\n        im = ax.imshow([[0, 1], [2, 3]])\n\n        cb = fig.colorbar(im)\n        cb.set_ticks([0.5, 1.5])\n\n        assert_allclose(cb.ax.get_yticks(), [0.5, 1.5])\n", "    def test_colorbar_set_labelpad():\n        fig, ax = plt.subplots()\n        im = ax.imshow([[0, 1], [2, 3]])\n        cb = fig.colorbar(im)\n        cb.set_label('Label', labelpad=20)\n        assert cb.label.get_window_extent().x0 > cb.ax.bbox.x0 + 20\n", "compilation error"], "sample_728": ["def test_make_biclusters_all_rows():\n    X, _, _ = make_biclusters(shape=(100, 100), n_clusters=4,\n                              shuffle=False, random_state=0)\n    assert_array_equal(np.sum(X, axis=1), np.ones(100) * 4)\n", "def test_make_classification_weights_sum():\n    for n_classes in [2, 3, 5]:\n        weights = np.random.rand(n_classes)\n        X, y = make_classification(\n            n_samples=100, n_features=20, n_informative=5,\n            n_redundant=0, n_repeated=0, flip_y=0, scale=1, random_state=0,\n            weights=weights\n        )\n        assert_almost_equal(np.bincount(y) / len(y), weights / weights.sum())\n\n", "    def test_make_biclusters_min_samples():\n        X, rows, cols = make_biclusters(shape=(100, 100), n_clusters=4,\n                                       min_samples=20, shuffle=True,\n                                       random_state=0)\n        assert_equal(X.shape, (100, 100), \"X shape mismatch\")\n        assert_equal(rows.shape, (4, 100), \"rows shape mismatch\")\n        assert_equal(cols.shape, (4, 100,), \"columns shape mismatch\")\n", "    def test_make_biclusters_with_constant_row_column_clusters():\n        X, rows, cols = make_biclusters(\n            shape=(100, 100), n_clusters=(1, 5), shuffle=True, random_state=0)\n        assert_equal(X.shape, (100, 100), \"X shape mismatch\")\n        assert_equal(rows.shape, (1, 100), \"rows shape mismatch\")\n        assert_equal(cols.shape, (5, 100,), \"columns shape mismatch\")\n", "    def test_make_classification_hypercube_informative_features(self):\n        # Test that informative features are indeed \"informative\"\n        # when hypercube=True\n        X, y = make_classification(n_samples=100, n_features=10,\n                                   n_informative=3, n_redundant=0,\n                                   n_repeated=0, flip_y=0,\n                                   class_sep=10, hypercube=True,\n                                   random_state=0)\n\n        # Compute the average distance between points of the same class\n        distances_same = []\n        for i in range(len(y)):\n            for j in range(i + 1, len(y)):\n                if y[i] == y[j]:\n                    distances_same.append(np.linalg.norm(X[i] - X[j]))\n\n        # Compute the average distance between points of different classes\n        distances_diff = []\n        for i in range(len(y)):\n            for j in range(i + 1, len(y)):\n                if y[i] != y[j]:\n                    distances_diff.append(np.linalg.norm(X[i] - X[j]))\n\n        # Assert that the average distance between points of different\n        # classes is larger than the average distance between points\n        # of the same class\n\n        assert np.mean(distances_diff) > np.mean(distances_same)\n"], "sample_722": ["def test_more_features_than_samples():\n    # test case where number of features is greater than number of samples\n    X = np.random.rand(5, 10)\n    km = KMeans(n_clusters=3, random_state=42).fit(X)\n    assert_equal(km.cluster_centers_.shape, (3, 10))\n    km = MiniBatchKMeans(n_clusters=3, random_state=42).fit(X)\n    assert_equal(km.cluster_centers_.shape, (3, 10))\n\n", "compilation error", "    def test_k_means_empty_cluster(self):\n        X = np.array([[1, 2], [1.1, 2.1], [10, 10], [10.1, 10.1]])\n        km = KMeans(n_clusters=3, init='random', random_state=42)\n        km.fit(X)\n        # ensure that no empty cluster is created\n        assert_equal(np.all(np.bincount(km.labels_) > 0), True)\n", "compilation error", "    def test_k_means_empty_cluster():\n        # Test for the case where a cluster becomes empty during fitting\n        X = np.array([[1, 2], [1.1, 2.2], [5, 6], [5.1, 6.2], [10, 11]])\n        n_clusters = 3\n        km = KMeans(n_clusters=n_clusters, random_state=42)\n\n        # Force an empty cluster by setting a very high initialization\n        km.cluster_centers_[0] = [100, 100]\n        km.fit(X)\n        assert_equal(len(np.unique(km.labels_)), n_clusters)\n\n"], "sample_699": ["    def test_doctest_report_default_is_udiff(self, pytester: Pytester):\n        result = self._run_doctest_report(pytester, None)\n        result.stdout.fnmatch_lines(\n            [\"     0  1  4\", \"    -1  2  4\", \"    +1  2  5\", \"     2  3  6\"]\n        )\n", "    def test_doctest_report_with_empty_diff(self, pytester):\n        pytester.makepyfile(\n            \"\"\"\n                '''\n                >>> 1 + 1\n                2\n                '''\n                pass\n            \"\"\"\n        )\n        result = pytester.runpytest(\n            \"--doctest-modules\", \"--doctest-report\", \"udiff\"\n        )\n        result.stdout.fnmatch_lines([])\n", "    def test_is_setup_py_with_shebang(tmp_path: Path) -> None:\n        setup_py = tmp_path.joinpath(\"setup.py\")\n        setup_py.write_text(\n            '#!/usr/bin/env python\\nfrom setuptools import setup; setup(name=\"foo\")'\n        )\n        assert _is_setup_py(setup_py)\n", "    def test_doctest_report_default(self, pytester: Pytester):\n        result = self._run_doctest_report(pytester, None)\n        result.stdout.fnmatch_lines(\n            [\n                \"Expected:\",\n                \"       a  b\",\n                \"    0  1  4\",\n                \"    1  2  4\",\n                \"    2  3  6\",\n                \"Got:\",\n                \"       a  b\",\n                \"    0  1  4\",\n                \"    1  2  5\",\n                \"    2  3  6\",\n            ]\n        )\n", "    def test_doctest_report_default(self, pytester: Pytester):\n        result = self._run_doctest_report(pytester, \"\")\n        result.stdout.fnmatch_lines(\n            [\"Expected:\",\n             \"       a  b\",\n             \"    0  1  4\",\n             \"    1  2  4\",\n             \"    2  3  6\",\n             \"Got:\",\n             \"       a  b\",\n             \"    0  1  4\",\n             \"    1  2  5\",\n             \"    2  3  6\",]\n        )\n\n"], "sample_455": ["    def test_invalid_violation_error_code(self):\n        msg = \"UniqueConstraint.violation_error_code must be a string.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                fields=[\"name\"],\n                name=\"name_uniq\",\n                violation_error_code=123,\n            )\n\n", "    def test_unique_constraint_with_non_model_field(self):\n        msg = \"UniqueConstraint.fields can only reference model fields.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                F(\"non_model_field\"), name=\"non_model_field_uniq\"\n            )\n", "    def test_violation_error_message(self):\n        constraint = models.UniqueConstraint(\n            fields=[\"name\"],\n            name=\"name_uniq\",\n            violation_error_message=\"Name must be unique!\",\n        )\n        msg = \"Constraint 'name_uniq' is violated. Name must be unique!\"\n        with self.assertRaisesMessage(ValidationError , msg):\n            constraint.validate(UniqueConstraintProduct, UniqueConstraintProduct(name=self.p1.name))\n\n", "    def test_include_coverage(self):\n        if not connection.features.supports_covering_indexes:\n            self.skipTest(\"This test requires support for covering indexes.\")\n        constraint = UniqueConstraintInclude._meta.constraints[0]\n        msg = \"Constraint \u201cname_color_uniq\u201d is violated.\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            constraint.validate(\n                UniqueConstraintInclude,\n                UniqueConstraintInclude(name=\"p1\", color=\"blue\"),\n                exclude={\"color\"},\n            )\n", "    def test_validate_condition_with_exclude(self):\n        p1 = UniqueConstraintConditionProduct.objects.create(name=\"p1\")\n        constraint = UniqueConstraintConditionProduct._meta.constraints[0]\n        msg = \"Constraint \u201cname_without_color_uniq\u201d is violated.\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            constraint.validate(\n                UniqueConstraintConditionProduct,\n                UniqueConstraintConditionProduct(name=p1.name, color=None),\n                exclude={\"name\"},\n            )\n\n"], "sample_155": ["    def test_file_response_close(self):\n        with tempfile.TemporaryFile() as f:\n            f.write(b'content')\n            f.seek(0)\n            response = FileResponse(f)\n            self.assertFalse(response.closed)\n            response.close()\n            self.assertTrue(response.closed)\n", "    def test_empty_response(self):\n        response = FileResponse(io.BytesIO(b''))\n        self.assertEqual(response['Content-Length'], '0')\n        self.assertEqual(response['Content-Type'], 'application/octet-stream')\n        self.assertFalse(response.has_header('Content-Disposition'))\n        self.assertEqual(list(response), [b''])\n", "    def test_empty_file_response(self):\n        response = FileResponse(io.BytesIO(b''), as_attachment=True)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response['Content-Disposition'], 'attachment')\n        self.assertEqual(response['Content-Type'], 'application/octet-stream')\n        self.assertEqual(list(response), [b''])\n\n", "    def test_filename_as_empty_string(self):\n        response = FileResponse(io.BytesIO(b'binary content'), filename='')\n        self.assertFalse(response.has_header('Content-Disposition'))\n", "    def test_file_response_without_filename(self):\n        response = FileResponse(ContentFile(b'binary content'), as_attachment=True)\n        self.assertEqual(response['Content-Type'], 'application/octet-stream')\n        self.assertEqual(response['Content-Disposition'], 'attachment')\n"], "sample_452": ["    def test_references_field_by_related_name(self):\n        operation = FieldOperation(\n            \"Model\", \"field\", models.ForeignKey(\"Other\", models.CASCADE, related_name=\"related_field\")\n        )\n        self.assertIs(operation.references_field(\"Other\", \"related_field\", \"migrations\"), True)\n        self.assertIs(operation.references_field(\"Other\", \"whatever\", \"migrations\"), False)\n", "    def test_references_field_related_name(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\"Other\", models.CASCADE, related_name=\"related_name\"),\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"related_name\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"whatever\", \"migrations\"), False\n        )\n", "    def test_references_field_by_through_model(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ManyToManyField(\"Other\", through=\"ThroughModel\"),\n        )\n        self.assertIs(operation.references_field(\"Model\", \"field\", \"migrations\"), True)\n        self.assertIs(\n            operation.references_field(\"ThroughModel\", \"whatever\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False\n        )\n\n", "    def test_references_field_case_insensitive(self):\n        operation = FieldOperation(\"MoDel\", \"field\", models.BooleanField(default=False))\n        self.assertIs(operation.references_field(\"MODEL\", \"FIELD\", \"migrations\"), True)\n", "    def test_references_field_by_related_name(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\n                \"Other\", models.CASCADE, related_name=\"related_objects\"\n            ),\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"related_objects\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"whatever\", \"migrations\"), False\n        )\n"], "sample_466": ["    def test_serialize_decimal(self):\n        self.assertSerializedEqual(Decimal(\"123.456\"))\n", "def test_serialize_decimal(self):\n    self.assertSerializedEqual(Decimal(\"10.2\"))\n", "    def test_serialize_decimal(self):\n        self.assertSerializedEqual(Decimal(\"12.34\"))\n        self.assertSerializedEqual(Decimal(\"-0.5\"))\n\n", "    def test_serialize_complex_enum(self):\n        self.serialize_round_trip(IntEnum.A)\n        with self.assertRaises(ValueError):\n            self.serialize_round_trip(IntEnum(42))\n\n", "    def test_deconstructed_field_with_args(self):\n        class MyField(models.CharField):\n                self.extra_arg = kwargs.pop(\"extra_arg\", None)\n                super().__init__(*args, **kwargs)\n\n        field = MyField(max_length=255, extra_arg=\"hello\")\n        string, imports = MigrationWriter.serialize(field)\n        self.assertEqual(\n            string,\n            \"migrations.test_writer.MyField(max_length=255, extra_arg='hello')\",\n        )\n        self.assertEqual(imports, {\"from django.db import models\"})\n"], "sample_140": ["    def test_sensitive_variables_decorator_with_arguments(self):\n        # Test that the decorator works correctly when applied to a function\n        # that takes arguments, including sensitive variables.\n\n            return f\"Hello {name}, your password is {password}\"\n\n        decorated_func = sensitive_variables(test_func)\n        result = decorated_func(\"secret\", \"Alice\")\n        self.assertEqual(result, f\"Hello Alice, your password is {CLEANSED_SUBSTITUTE}\")\n", "    def test_sensitive_variables_with_kwargs(self):\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_variables_with_kwargs_view)\n            self.verify_unsafe_email(sensitive_variables_with_kwargs_view)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_variables_with_kwargs_view)\n            self.verify_safe_email(sensitive_variables_with_kwargs_view)\n", "    def test_sensitive_variables_decorator_with_kwargs(self):\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_kwargs_view)\n            self.verify_unsafe_email(sensitive_kwargs_view)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_kwargs_view)\n            self.verify_safe_email(sensitive_kwargs_view)\n", "    def test_sensitive_variables_with_kwargs(self):\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_variables_with_kwargs)\n            self.verify_unsafe_email(sensitive_variables_with_kwargs)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_variables_with_kwargs)\n            self.verify_safe_email(sensitive_variables_with_kwargs)\n", "    def test_sensitive_variables_with_nested_functions(self):\n            @sensitive_variables(password)\n                pass\n            inner_function()\n\n        with self.settings(DEBUG=True):\n            outer_function('super_secret')\n            self.assertLogs(logger, level='WARNING')\n"], "sample_623": ["    def test_zero_sized_dimension(self, pref_chunks):\n        \"\"\"Test that preferred chunking works with zero-sized dimensions.\"\"\"\n        shape = (0, 5)\n        initial = self.create_dataset(shape, pref_chunks)\n        final = xr.open_dataset(\n            initial, engine=PassThroughBackendEntrypoint, chunks={}\n        )\n        self.check_dataset(initial, final, explicit_chunks(pref_chunks, shape))\n", "    def test_ignore_preferred_chunks(self, shape, pref_chunks, req_chunks):\n        \"\"\"Ignore the backend's preferred chunks if the requested chunks are smaller.\"\"\"\n        initial = self.create_dataset(shape, pref_chunks)\n        final = xr.open_dataset(\n            initial,\n            engine=PassThroughBackendEntrypoint,\n            chunks=dict(zip(initial[self.var_name].dims, req_chunks)),\n        )\n        self.check_dataset(initial, final, explicit_chunks(req_chunks, shape))\n\n", "    def test_smaller_chunks(self, shape, pref_chunks, req_chunks):\n        \n        initial = self.create_dataset(shape, pref_chunks)\n        with pytest.raises(ValueError):\n            xr.open_dataset(\n                initial,\n                engine=PassThroughBackendEntrypoint,\n                chunks=dict(zip(initial[self.var_name].dims, req_chunks)),\n            )\n", "    def test_unlimited_dims(self):\n        \"\"\"\n\n        Test that unlimited dimensions are properly handled when retrieving a dataset with the\n        PassThroughBackendEntrypoint.\n\n        \"\"\"\n        shape = (5, 10)\n        unlimited_dims = {\"dim_0\": True}\n        pref_chunks = (4, 5)\n        initial = self.create_dataset(shape, pref_chunks, unlimited_dims=unlimited_dims)\n\n        final = xr.open_dataset(\n            initial, engine=PassThroughBackendEntrypoint, chunks=dict(zip(initial[self.var_name].dims, pref_chunks))\n        )\n\n        self.check_dataset(initial, final, explicit_chunks(pref_chunks, shape))\n\n        assert final[self.var_name].encoding[\"unlimited_dims\"] == unlimited_dims\n", "    def test_reduce_chunks(self, shape, pref_chunks, req_chunks):\n        \"\"\"Don't warn when the requested chunks reduce the preferred chunks.\"\"\"\n        initial = self.create_dataset(shape, pref_chunks)\n        with assert_no_warnings():\n            final = xr.open_dataset(\n                initial,\n                engine=PassThroughBackendEntrypoint,\n                chunks=dict(zip(initial[self.var_name].dims, req_chunks)),\n            )\n        self.check_dataset(initial, final, explicit_chunks(req_chunks, shape))\n"], "sample_320": ["    def test_swappable_operation_with_references(self):\n        \"\"\"\n        Test SwappableOperation referencing a model,\n        ensuring it uses the correct model for operations and references.\n        \"\"\"\n        with override_settings(TEST_SWAP_MODEL=\"migrations.SomeFakeModel\"):\n            project_state = ProjectState()\n            new_state = project_state.clone()\n            operation = migrations.CreateModel(\n                \"Pony\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"pink\", models.IntegerField(default=1)),\n                    (\n                        \"favorite_something\",\n                        models.ForeignKey(\"TEST_SWAP_MODEL\", models.CASCADE),\n                    ),\n                ],\n                options={\n                    \"swappable\": \"TEST_SWAP_MODEL\",\n                },\n            )\n            operation.state_forwards(\"test_swapref\", new_state)\n            self.assertEqual(\n                new_state.models[\"test_swapref\", \"pony\"].fields[2].related_model,\n                \"migrations.SomeFakeModel\",\n            )\n            with connection.schema_editor() as editor:\n                operation.database_forwards(\"test_swapref\", editor, project_state, new_state)\n            self.assertTableExists(\"test_swapref_pony\")\n", "    def test_field_operation_null_default(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.IntegerField(null=True, default=0),\n        )\n        self.assertEqual(operation.field.default, 0)\n        self.assertTrue(operation.field.null)\n", "    def test_references_field_by_related_model(self):\n        operation = FieldOperation(\n            \"Model\", \"field\", models.OneToOneField(\"Other\", on_delete=models.CASCADE)\n        )\n        self.assertIs(operation.references_field(\"Other\", \"whatever\", \"migrations\"), True)\n        self.assertIs(\n            operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False\n        )\n\n", "    def test_add_field_rename_column(self):\n        \"\"\"\n        Tests renaming a column during AddField, when the new field name matches\n        an existing column.\n        \"\"\"\n        with self.assertRaisesMessage(\n            ValueError,\n            \"Field name 'name' already exists for model 'Pony', use \"\n            \"RenameField to change the existing field.\",\n        ):\n            operation = migrations.AddField(\n                \"Pony\",\n                \"name\",\n                models.CharField(max_length=100),\n                preserve_default=True,\n            )\n            operation.state_forwards(\"test_addfieldrenamecolumn\", self.project_state)\n\n\n\n", "    def test_field_operation_deconstruct(self):\n        operation = migrations.AddField(\n            \"Pony\", \"height\", models.FloatField(null=True, default=5)\n        )\n        definition = operation.deconstruct()\n        self.assertEqual(definition[0], \"AddField\")\n        self.assertEqual(definition[1], [])\n        self.assertEqual(\n            sorted(definition[2]),\n            [\"model_name\", \"name\", \"field\", \"preserve_default\"],\n        )\n\n"], "sample_693": ["    def test_do_cleanups_on_failure(pytester: Pytester) -> None:\n        testpath = pytester.makepyfile(\n            \"\"\"\n            import unittest\n            class MyTestCase(unittest.TestCase):\n                values = []\n                        self.values.append(1)\n                    self.addCleanup(cleanup)\n                    assert False\n                assert MyTestCase.values == [1]\n        \"\"\"\n        )\n        reprec = pytester.inline_run(testpath)\n        passed, skipped, failed = reprec.countoutcomes()\n        assert failed == 1\n        assert passed == 1\n", "    def test_do_class_cleanups_on_teardownclass_success(pytester: Pytester) -> None:\n        testpath = pytester.makepyfile(\n            \"\"\"\n            import unittest\n            class MyTestCase(unittest.TestCase):\n                values = []\n                @classmethod\n                        cls.values.append(1)\n                    cls.addClassCleanup(cleanup)\n                @classmethod\n                    cls.values.append(2)\n                    pass\n                assert MyTestCase.values == [1, 2]\n        \"\"\"\n        )\n        reprec = pytester.inline_run(testpath)\n        passed, skipped, failed = reprec.countoutcomes()\n        assert failed == 0\n        assert passed == 2\n\n", "    def test_exception_during_class_setup_doesnt_run_teardown(pytester: Pytester) -> None:\n        testpath = pytester.makepyfile(\n            \"\"\"\n            import unittest\n            class MyTestCase(unittest.TestCase):\n                values = []\n                    super().__init__(*args, **kwargs)\n                        self.values.append(1)\n                    self.addCleanup(cleanup)\n\n                @classmethod\n                    raise Exception(\"SetupClass Exception\")\n\n                    pass\n                assert MyTestCase.values == []\n        \"\"\"\n        )\n        reprec = pytester.inline_run(testpath)\n        passed, skipped, failed = reprec.countoutcomes()\n        assert failed == 1\n        assert passed == 1\n\n\n", "def test_do_class_cleanups_on_instance_setup_failure(pytester: PyTester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n        class MyTestCase(unittest.TestCase):\n            values = []\n            @classmethod\n                    cls.values.append(1)\n                cls.addClassCleanup(cleanup)\n                assert False\n                pass\n            assert MyTestCase.values == [1]\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    passed, skipped, failed = reprec.countoutcomes()\n    assert failed == 1\n    assert passed == 1\n", "    def test_do_multiple_cleanups(pytester: PyTester) -> None:\n        testpath = pytester.makepyfile(\n            \"\"\"\n            import unittest\n            class MyTestCase(unittest.TestCase):\n                values = []\n                        self.values.append(1)\n                        self.values.append(2)\n                    self.addCleanup(cleanup1)\n                    self.addCleanup(cleanup2)\n                    pass\n                assert MyTestCase.values == [1, 2]\n        \"\"\"\n        )\n        reprec = pytester.inline_run(testpath)\n        passed, skipped, failed = reprec.countoutcomes()\n        assert failed == 0\n        assert passed == 2\n"], "sample_838": ["def test_column_transformer_sparse_output():\n    X_array = np.array([[0, 1, 2], [2, 4, 6], [8, 6, 4]]).T\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           remainder='drop',\n                           sparse_threshold=0.8)\n    ct.fit(X_array)\n    assert isinstance(ct.transform(X_array), sparse.csr_matrix)\n", "def test_column_transformer_remainder_list_of_transformers():\n    X_array = np.array([[0, 1, 2], [2, 4, 6], [8, 6, 4]]).T\n    X_res_both = X_array\n\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           remainder=Trans())\n\n    assert_array_equal(ct.fit_transform(X_array), X_res_both)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert isinstance(ct.transformers_[-1][1], Trans)\n    assert_array_equal(ct.transformers_[-1][2], [1, 2])\n\n\n", "    def test_column_transformer_no_transformer_remainder(self):\n        X = np.array([[1, 2], [3, 4]])\n        remainder = 'passthrough'\n        ct = ColumnTransformer(transformers=[], remainder=remainder)\n        X_trans = ct.fit_transform(X)\n        assert_array_equal(X_trans, X)\n", "def test_column_transformer_remainder_with_feature_names_out():\n    X_df = pd.DataFrame({'a': [1, 2], 'b': [3, 4], 'c': [5, 6]})\n    ct = ColumnTransformer(\n        [('trans1', 'passthrough', ['a']), ('trans2', 'passthrough', ['b'])],\n        remainder='drop',\n    )\n    ct.fit(X_df)\n    transformed = ct.transform(X_df)\n    assert ct.get_feature_names_out(input_features=X_df.columns.tolist()) == [\n        'a',\n        'b'\n    ]\n", "    def test_column_transformer_remainder_with_copy_data():\n        # Test that the remainder transformer doesn't modify the input data\n        X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n        scaler = StandardScaler(copy=True)\n        ct = ColumnTransformer([\n            ('trans', Trans(), [0]),\n        ], remainder=scaler)\n\n        X_fit = X_array.copy()\n        ct.fit(X_fit)\n        assert_array_equal(X_fit, X_array)\n\n        ct = ColumnTransformer([\n            ('trans', Trans(), [0]),\n        ], remainder=scaler)\n        X_transform = X_array.copy()\n        ct.fit_transform(X_transform)\n        assert_array_equal(X_transform, X_array)\n"], "sample_797": ["    def test_power_transformer_n_features_in(self):\n        # test that the n_features_in_ attribute is set correctly\n        X = np.abs(X_2d)\n        pt = PowerTransformer(method='yeo-johnson')\n        pt.fit(X)\n        assert pt.n_features_in_ == X.shape[1]\n", "    def test_power_transformer_sparse_matrices(self, method):\n        X = X_2d\n        if method == 'box-cox':\n            X = np.abs(X)\n        \n        X_sparse = sparse.csr_matrix(X)\n\n        pt = PowerTransformer(method=method)\n\n        X_trans = pt.fit_transform(X_sparse)\n        assert isinstance(X_trans, sparse.csr_matrix)\n        assert_array_almost_equal(pt.inverse_transform(X_trans), X)\n", "compilation error", "compilation error", "    def test_power_transformer_sparse_matrix(method):\n        X = sparse.csr_matrix(np.abs(X_2d))\n\n        pt = PowerTransformer(method=method)\n        X_trans = pt.fit_transform(X)\n        assert isinstance(X_trans, sparse.csr_matrix)\n        assert_almost_equal(pt.inverse_transform(X_trans).A, X.A)\n"], "sample_806": ["def test_gradient_boosting_missing_class_label():\n    # Test if gradient boosting handles unseen labels correctly.\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([0, 1, 2])\n\n    gbc = GradientBoostingClassifier(random_state=0)\n    with pytest.raises(ValueError,\n                       match=\"The training data contains label\"):\n        gbc.fit(X, y)\n\n", "    def test_gradient_boosting_warm_start_with_different_n_classes():\n        # Test warm start with different number of classes between initial fit\n        # and subsequent fits.\n\n        X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n        y1 = np.array([0, 1, 1, 1])\n        y2 = np.array([0, 1, 2, 2])\n\n        gb = GradientBoostingClassifier(n_estimators=5, warm_start=True,\n                                        random_state=0)\n\n        gb.fit(X, y1)\n        with pytest.raises(ValueError, match=\"The number of classes\"):\n            gb.fit(X, y2)\n", "def test_gradient_boosting_init_sparse_input():\n    # Check if init estimator can handle sparse input\n    X, y = make_classification(random_state=0)\n    X_sparse = csr_matrix(X)\n\n    # Use a classifier that supports sparse input\n    init = LogisticRegression(solver='lbfgs', random_state=0)\n    gb = GradientBoostingClassifier(init=init)\n\n    gb.fit(X_sparse, y)\n", "    def test_gradient_boosting_warm_start_with_validation_fraction(self):\n        # test warm_start with validation fraction\n        X, y = datasets.make_classification(random_state=0)\n        est = GradientBoostingClassifier(n_estimators=10,\n                                        validation_fraction=0.2,\n                                        warm_start=True,\n                                        random_state=0)\n        est.fit(X, y)\n        init_score = est.score(X, y)\n        est.set_params(n_estimators=20)\n        est.fit(X, y)\n        final_score = est.score(X, y)\n        assert final_score > init_score\n", "    def test_gradient_boosting_n_estimators_early_stopping_warning():\n        # Test if a warning is raised if n_estimators is set lower than\n        # n_iter_no_change\n        X, y = make_classification(n_samples=1000, random_state=0)\n        gb = GradientBoostingClassifier(n_estimators=5, n_iter_no_change=10,\n                                        learning_rate=0.1, max_depth=3,\n                                        random_state=42)\n\n        with pytest.warns(UserWarning, match=\"n_estimators is set to a value\"\n                         \" lower than n_iter_no_change\"):\n             gb.fit(X, y)\n"], "sample_503": ["def test_drawstyle_and_offset(drawstyle):\n    fig, ax = plt.subplots()\n    x = np.arange(5)\n    y = np.arange(5)\n    ax.plot(x, y, drawstyle=drawstyle, marker='o')\n    ax.set_xlim(-1, 6)\n    ax.set_ylim(-1, 6)\n    # Check if the lines are drawn correctly with and without offset\n    # Add more assertions to check specific cases\n", "def test_markeredgecolor_kwargs():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [1, 2], marker='o', mec='red', markeredgecolor='blue',\n            markerfacecolor='green')\n    line = ax.lines[0]\n    assert line.get_markeredgecolor() == 'blue'\n\n\n", "def test_line2d_empty(fig_test, fig_ref):\n    ax_test = fig_test.add_subplot()\n    line = mlines.Line2D([], [], linestyle='-', color='blue')\n    ax_test.add_line(line)\n\n    ax_ref = fig_ref.add_subplot()\n    ax_ref.plot([], [])\n\n", "    def test_markersize_scalings():\n        fig, ax = plt.subplots()\n\n        # Test default markersize scaling\n        ax.plot([1, 2, 3], marker='o', markersize=10)\n\n        # Test markersize scaling with a Line2D object\n        line = mlines.Line2D([1, 2, 3], [4, 5, 6], marker='s', markersize=5)\n        ax.add_line(line)\n\n        # Test markersize scaling with a different units system\n        ax.plot([1, 2, 3], marker='D', markersize=20, transform=ax.transAxes)\n        fig.canvas.draw()\n", "    def test_set_markerfacecoloralt_after_creation(self):\n        fig, ax = plt.subplots()\n        l, = ax.plot([0, 1, 2, 3], marker='o', markerfacecolor='blue',\n                     markersize=10)\n        l.set_markerfacecoloralt('red')\n        assert l.get_markerfacecoloralt() == 'red'\n"], "sample_224": ["    def test_aggregation_subquery_annotation_exists_and_related_field(self):\n        publisher = Publisher.objects.create(name=self.a9.name, num_awards=2)\n        book = Book.objects.create(\n            isbn='159059999', name='Test book.', pages=819, rating=2.5,\n            price=Decimal('14.44'), contact=self.a9, publisher=publisher,\n            pubdate=datetime.date(2019, 12, 6),\n        )\n        book.authors.add(self.a5, self.a6, self.a7)\n        books_qs = Book.objects.annotate(\n            contact_publisher=Subquery(\n                Publisher.objects.filter(\n                    pk=OuterRef('publisher'),\n                    name=OuterRef('contact__name'),\n                ).values('name')[:1],\n            )\n        ).filter(\n            contact_publisher__isnull=False,\n        ).annotate(has_author=Exists(Author.objects.filter(pk=OuterRef('authors__id'))))\n        self.assertSequenceEqual(books_qs, [book])\n", "    def test_aggregation_subquery_annotation_related_field_values(self):\n        publisher = Publisher.objects.create(name=self.a9.name, num_awards=2)\n        book = Book.objects.create(\n            isbn='159059999', name='Test book.', pages=819, rating=2.5,\n            price=Decimal('14.44'), contact=self.a9, publisher=publisher,\n            pubdate=datetime.date(2019, 12, 6),\n        )\n        book.authors.add(self.a5, self.a6, self.a7)\n        books_qs = Book.objects.annotate(\n            contact_publisher=Subquery(\n                Publisher.objects.filter(\n                    pk=OuterRef('publisher'),\n                    name=OuterRef('contact__name'),\n                ).values('name')[:1],\n            )\n        ).filter(\n            contact_publisher__isnull=False,\n        ).values('contact_publisher', 'name').annotate(count=Count('authors'))\n        self.assertEqual(\n            list(books_qs),\n            [{'contact_publisher': self.a9.name, 'name': 'Test book.', 'count': 3}]\n        )\n", "    def test_aggregate_count_with_isnull_filter(self):\n        qs = Book.objects.filter(rating__isnull=True).annotate(\n            count_null=Count('*')\n        )\n        self.assertEqual(qs.get().count_null, Book.objects.filter(\n            rating__isnull=True).count())\n\n", "    def test_aggregation_subquery_annotation_related_field_isnull(self):\n        publisher = Publisher.objects.create(name='Non-existent Author', num_awards=2)\n        books_qs = Book.objects.annotate(\n            contact_publisher=Subquery(\n                Publisher.objects.filter(\n                    pk=OuterRef('publisher'),\n                    name=OuterRef('contact__name'),\n                ).values('name')[:1],\n            )\n        ).filter(contact_publisher__isnull=True).annotate(count=Count('authors'))\n        self.assertEqual(books_qs.count(), 0)\n", "    def test_aggregation_annotation_ordering_related_field(self):\n        books_qs = Book.objects.annotate(\n            publisher_name=F('publisher__name')\n        ).order_by('publisher_name').values_list('publisher_name', flat=True).distinct()\n        self.assertEqual(\n            list(books_qs),\n            sorted(list(Publisher.objects.values_list('name', flat=True)))\n        )\n"], "sample_601": ["    def test_cftime_strftime_accessor(cftime_rounding_dataarray, cftime_date_type, use_dask):\n        import dask.array as da\n\n        date_format = \"%Y%m%d\"\n        expected = xr.DataArray(\n            [\n                [\"20010101\", \"20010101\"],\n                [\"20010101\", \"20010201\"],\n            ],\n            name=\"strftime\",\n        )\n\n        if use_dask:\n            chunks = {\"dim_0\": 1}\n            with raise_if_dask_computes(max_computes=1):\n                result = (\n                    cftime_rounding_dataarray.chunk(chunks).dt.strftime(date_format)\n                )\n            expected = expected.chunk(chunks)\n            assert isinstance(result.data, da.Array)\n            assert result.chunks == expected.chunks\n        else:\n            result = cftime_rounding_dataarray.dt.strftime(date_format)\n\n        assert_identical(result, expected)\n", "    def test_cftime_accessor_method_errors(self, method, cftime_date_type):\n        import dask.array as da\n\n        dates = xr.DataArray(\n            [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 1, 15)],\n            dims=[\"time\"],\n        )\n        dates_dask = dates.chunk({\"time\": 1})\n\n        with pytest.raises(TypeError):\n            getattr(dates_dask.dt, method)(\"M\")\n\n        expected = getattr(dates.dt, method)(\"D\")\n        result = getattr(dates_dask.dt, method)(\"D\").compute()\n        assert_identical(result, expected)\n\n", "def test_cftime_strftime_accessor(cftime_rounding_dataarray, use_dask):\n    import dask.array as da\n    \n    expected = xr.DataArray(\n        [\n            [\"0001-01-01 01:00:00\", \"0001-01-01 15:00:00\"],\n            [\"0001-01-01 23:00:00\", \"0001-01-02 01:00:00\"],\n        ],\n        name=\"strftime\",\n    )\n\n    if use_dask:\n        chunks = {\"dim_0\": 1}\n        # Currently a compute is done to inspect a single value of the array\n        # if it is of object dtype to check if it is a cftime.datetime (if not\n        # we raise an error when using the dt accessor).\n        with raise_if_dask_computes(max_computes=1):\n            result = cftime_rounding_dataarray.chunk(chunks).dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n        expected = expected.chunk(chunks)\n        assert isinstance(result.data, da.Array)\n        assert result.chunks == expected.chunks\n    else:\n        result = cftime_rounding_dataarray.dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    assert_identical(result, expected)\n", "def test_cftime_strftime_accessor(cftime_rounding_dataarray, cftime_date_type, use_dask):\n    import dask.array as da\n\n    date_format = \"%Y%m%d\"\n    expected = xr.DataArray(\n        [\n            [\"00010101\", \"00010115\"],\n            [\"00010123\", \"00010201\"],\n        ],\n        name=\"strftime\",\n        dims=[\"dim_0\", \"dim_1\"],\n    )\n\n    if use_dask:\n        chunks = {\"dim_0\": 1}\n        # Currently a compute is done to inspect a single value of the array\n        # if it is of object dtype to check if it is a cftime.datetime (if not\n        # we raise an error when using the dt accessor).\n        with raise_if_dask_computes(max_computes=1):\n            result = cftime_rounding_dataarray.chunk(chunks).dt.strftime(date_format)\n        expected = expected.chunk(chunks)\n        assert isinstance(result.data, da.Array)\n        assert result.chunks == expected.chunks\n    else:\n        result = cftime_rounding_dataarray.dt.strftime(date_format)\n\n    assert_identical(result, expected)\n", "def test_accessor_method_cftime(data, method, parameters):\n    expected = getattr(xr.coding.cftimeindex.CFTimeIndex(data.time.values), method)(parameters)\n    actual = getattr(data.time.dt, method)(parameters)\n    assert_equal(actual, expected)\n\n"], "sample_254": ["    def test_inline_formset_readonly_fields(self):\n        self.admin_login(username='super', password='secret')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_inlines_readonlymodel_add'))\n        self.wait_until_visible('#id_dummy')\n        self.selenium.find_element_by_id('id_dummy').send_keys(1)\n        fields = ['id_readonlyinline_set-0-readonly_field']\n        show_links = self.selenium.find_elements_by_link_text('SHOW')\n        for show_index, field_name in enumerate(fields):\n            show_links[show_index].click()\n            self.wait_until_visible('#' + field_name)\n            self.assertTrue(self.selenium.find_element_by_id(field_name).get_attribute('readonly'))\n        self.selenium.find_element_by_xpath('//input[@value=\"Save\"]').click()\n        \n", "    def test_inlines_with_empty_formset(self):\n        self.admin_login(username='super', password='secret')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_inlines_emptyformset_add'))\n        self.assertEqual(len(self.selenium.find_elements_by_css_selector('.dynamic-emptyformset_set')), 0)\n        self.selenium.find_element_by_link_text('Add another EmptyFormSet').click()\n        self.assertEqual(len(self.selenium.find_elements_by_css_selector('.dynamic-emptyformset_set')), 1)\n", "    def test_delete_filtered_inline(self):\n        self.admin_login(username='super', password='secret')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_inlines_course_add'))\n        inline_id = '#class_set-group'\n        # Filter the list of classes.\n        search_input = self.selenium.find_element_by_id('id_class__name')\n        search_input.send_keys('Test Class')\n        self.selenium.find_element_by_css_selector('.filter-button').click()\n        self.assertEqual(len(self.selenium.find_elements_by_css_selector(inline_id + ' .dynamic-class_set')), 2)\n        # Delete one of the filtered classes.\n        delete_button = self.selenium.find_element_by_css_selector(\n            inline_id + ' tr.dynamic-class_set:nth-child(2) td.delete a'\n        )\n        delete_button.click()\n        self.assertEqual(len(self.selenium.find_elements_by_css_selector(inline_id + ' .dynamic-class_set')), 1)\n", "    def test_inline_formset_error_message(self):\n        self.admin_login(username='super', password='secret')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_inlines_holder5_add'))\n        self.wait_until_visible('#id_dummy')\n        self.selenium.find_element_by_id('id_dummy').send_keys(1)\n        fields = ['id_inner5stacked_set-0-dummy', 'id_inner5tabular_set-0-dummy']\n        show_links = self.selenium.find_elements_by_link_text('SHOW')\n        for show_index, field_name in enumerate(fields):\n            show_links[show_index].click()\n            self.wait_until_visible('#' + field_name)\n            self.selenium.find_element_by_id(field_name).send_keys('1')\n        self.selenium.find_element_by_xpath('//input[@value=\"Save\"]').click()\n        error_messages = self.selenium.find_elements_by_css_selector('.errorlist li')\n        self.assertEqual(len(error_messages), 2)\n        self.assertIn('This field is required.', error_messages[0].text)\n        self.assertIn('This field is required.', error_messages[1].text)\n\n", "    def test_inline_formset_error_message(self):\n        self.admin_login(username='super', password='secret')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_inlines_holder5_add'))\n        self.wait_until_visible('#id_dummy')\n        self.selenium.find_element_by_id('id_dummy').send_keys(1)\n        fields = ['id_inner5stacked_set-0-dummy', 'id_inner5tabular_set-0-dummy']\n        show_links = self.selenium.find_elements_by_link_text('SHOW')\n        for show_index, field_name in enumerate(fields):\n            show_links[show_index].click()\n            self.wait_until_visible('#' + field_name)\n\n        self.selenium.find_element_by_xpath('//input[@value=\"Save\"]').click()\n        self.assertIn('This field is required.', self.selenium.page_source)\n        self.assertIn('This field is required.', self.selenium.page_source)\n"], "sample_873": ["def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    assert_array_equal(feature_names_t, sel.get_feature_names_out(feature_names))\n    assert_array_equal(\n        feature_names_t, sel.get_feature_names_out(input_features=feature_names)\n    )\n", "def test_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n\n    assert_array_equal(sel.get_feature_names_out(), feature_names_t)\n    assert_array_equal(\n        sel.get_feature_names_out(input_features=feature_names), feature_names_t\n\n    )\n    # check that it works when feature_names_in_ is None\n    sel2 = StepSelector()\n    sel2.fit(X)\n    assert_array_equal(sel2.get_feature_names_out(), [\"x0\", \"x2\", \"x4\", \"x6\", \"x8\"])\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    names_t_actual = sel.get_feature_names_out(feature_names)\n    assert_array_equal(feature_names_t, names_t_actual)\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    assert_array_equal(feature_names_t, sel.get_feature_names_out())\n    assert_array_equal(\n        feature_names_t, sel.get_feature_names_out(input_features=feature_names)\n    )\n    assert_array_equal(\n        feature_names_t, sel.get_feature_names_out(input_features=np.array(feature_names))\n    )\n    with pytest.raises(ValueError):\n        sel.get_feature_names_out(input_features=[1,2,3])\n\n\n", "def test_get_feature_names_out():\n    sel = StepSelector(step=2)\n    sel.fit(X, y)\n\n    feature_names_out = sel.get_feature_names_out(feature_names)\n    assert_array_equal(feature_names_t, feature_names_out)\n\n    feature_names_out_none = sel.get_feature_names_out()\n    assert_array_equal(feature_names_t, feature_names_out_none)\n\n    with pytest.raises(ValueError):\n        sel.get_feature_names_out([\"a\", \"b\"])\n"], "sample_1131": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_issue_18811():\n    from sympy import exp, Lambda, MatrixSymbol, Derivative\n    from sympy.codegen.scipy_nodes import cosm1\n    x = symbols('x')\n    M = MatrixSymbol('M', 2, 2)\n    expr = cosm1(exp(x))\n    jac = Derivative(expr, x).doit()\n    prntr = SciPyPrinter()\n    assert prntr.doprint(jac) == 'scipy.special.cosm1(numpy.exp(x)) * numpy.exp(x)'\n    prntr = NumPyPrinter()\n    assert prntr.doprint(jac) == 'numpy.exp(x) * math.expm1(x)'\n\n"], "sample_425": ["    def test_serialize_nested_class_without_name(self):\n        class Outer:\n            class Inner:\n                pass\n\n        self.assertSerializedResultEqual(\n            Outer.Inner, (\"migrations.test_writer.Outer.Inner\", {\"import migrations.test_writer\"}),\n        )\n", "    def test_serialize_unbound_function(self):\n            return 'hello'\n        self.serialize_round_trip(func)\n", "    def test_serialize_decimal(self):\n        self.assertSerializedEqual(decimal.Decimal(\"12.34\"))\n        self.assertSerializedEqual(decimal.Decimal(\"12.34E+5\"))\n        self.assertSerializedEqual(decimal.Decimal(\"-12.34\"))\n", "    def test_serialize_recursive_references(self):\n        class RecursiveModel(models.Model):\n            parent = models.ForeignKey(\"self\", null=True, blank=True)\n            name = models.CharField(max_length=255)\n\n        self.assertSerializedResultEqual(\n            RecursiveModel._meta.get_field(\"parent\"),\n            (\n                \"django.db.models.fields.related.ForeignKey(\"\n                \"'migrations.test_writer.RecursiveModel', null=True, blank=True)\",\n                {\n                    \"from django.db import models\",\n                    \"from django.db.models.fields.related import ForeignKey\",\n                },\n            ),\n        )\n\n", "    def test_serialize_decimal_round_trip(self):\n        value = Decimal(\"12.3456789012345678901234567890\")\n        result = self.serialize_round_trip(value)\n        self.assertEqual(result, value)\n"], "sample_1190": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1071": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_583": ["    def test_create_mask_dask_2d():\n        da = pytest.importorskip('dask.array')\n\n        indexer = indexing.OuterIndexer(\n            (np.array([0, -1, 2]), np.array([0, 1, -1])))\n        expected = np.array([[False, True, True]] * 2).T\n        actual = indexing.create_mask(indexer, (5, 2),\n                                      chunks_hint=((2,), (2,)))\n        assert isinstance(actual, da.Array)\n        assert actual.chunks == ((2,), (2,))\n        np.testing.assert_array_equal(expected, actual)\n", "    def test_create_mask_copy_on_write():\n        arr = indexing.CopyOnWriteArray(np.arange(10))\n        indexer = indexing.BasicIndexer((slice(2, 4),))\n        mask = indexing.create_mask(indexer, (10,))\n        np.testing.assert_array_equal(\n            mask, np.array([False, False, True, True, False, False, False, False,\n                            False, False]))\n", "    def test_implicit_indexing_adapter_assignment():\n        array = np.arange(10, dtype=np.int64)\n        implicit = indexing.ImplicitToExplicitIndexingAdapter(\n            indexing.CopyOnWriteArray(array), indexing.BasicIndexer)\n\n        implicit[:] = 0\n        np.testing.assert_array_equal(array, np.zeros(10, dtype=np.int64))\n\n", "def test_slice_slice_extended():\n    I = ReturnItem()  # noqa: E741  # allow ambiguous name\n    for size in [100, 99]:\n        # We test even/odd size cases\n        x = np.arange(size)\n        slices = [I[:3], I[:4], I[2:4], I[:1], I[:-1], I[5:-1], I[-5:-1],\n                  I[::-1], I[5::-1], I[:3:-1], I[:30:-1], I[10:4:],\n                  I[::4], I[4:4:4], I[:4:-4], I[::-2]]\n        for i in slices:\n            for j in slices:\n                expected = x[i][j]\n\n                new_slice = indexing.slice_slice(i, j, size=size)\n                actual = x[new_slice]\n                assert_array_equal(expected, actual)\n\n", "    def test_create_mask_empty_indexer():\n        indexer = indexing.BasicIndexer(())\n        actual = indexing.create_mask(indexer, (3,))\n        np.testing.assert_array_equal(True, actual)\n"], "sample_911": ["    def check_id_attributes(attr_name, expected):\n        parser = DefinitionParser(\n            f'int {attr_name} f()',\n            location=None,\n            config=Config()\n        )\n        ast, isShorthand = parser.parse_member()\n        parser.assert_end()\n        assert ast.attributes[0].name == attr_name\n", "compilation error", "compilation error", "def test_build_domain_cpp_enum_values_with_underscores(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"enum-values\")\n    assert len(ws) == 0\n", "def test_xref_consistency_cpp_texpr(app, status, warning):\n    app.builder.build_all()\n\n    test = 'xref_consistency.html'\n    output = (app.outdir / test).read_text()\n\n        pattern = (r'{role}-role:.*?'\n                   r'<(?P<tag>{tag}) .*?class=[\"\\'](?P<classes>.*?)[\"\\'].*?>'\n                   r'.*'\n                   r'</(?P=tag)>').format(role=role, tag=tag)\n        result = re.search(pattern, output)\n        expect = '''\\"], "sample_1109": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_545": ["    def test_subfigure_spaning_nested():\n        fig = plt.figure()\n        gs = fig.add_gridspec(3, 3)\n        subfig1 = fig.add_subfigure(gs[0, :2])\n        subfig2 = fig.add_subfigure(gs[1:, :])\n\n        axs1 = subfig1.subplots(1, 2)\n        axs2 = subfig2.subplots(2, 1)\n        assert len(fig.axes) == 5\n\n", "compilation error", "    def test_subfigure_constrained_layout_spacing():\n        fig = plt.figure(layout='constrained')\n        subfigs = fig.subfigures(2, 2)\n        subfigs[0, 0].set_title('top left')\n        subfigs[0, 1].set_title('top right')\n        subfigs[1, 0].set_title('bottom left')\n        subfigs[1, 1].set_title('bottom right')\n        plt.tight_layout()\n        \n        # Check spacing between subfigures\n", "    def test_add_subplot_kwargs_projection():\n        fig = plt.figure()\n        ax1 = fig.add_subplot(111, projection='polar')\n        ax2 = fig.add_subplot(111)\n        assert ax1.name == 'polar'\n        assert ax2.name == 'rectilinear'\n\n", "compilation error"]}, "raw_scored_generations": {"sample_4": [false, false, false, false, false], "sample_5": [false, false, false, false, false], "sample_3": [true, false, false, false, true], "sample_0": [false, false, false, false, false], "sample_2": [true, false, true, false, true], "sample_1": [false, false, false, true, true], "sample_6": [false, false, false, true, false], "sample_7": [false, true, false, true, true], "sample_8": [false, false, false, true, false], "sample_9": [true, true, false, false, true], "sample_10": [true, false, false, true, false], "sample_11": [false, true, false, false, false], "sample_12": [false, false, true, false, true], "sample_13": [true, false, false, false, true], "sample_14": [true, true, true, true, true], "sample_15": [true, true, true, true, true], "sample_16": [false, false, false, false, false], "sample_17": [false, false, false, false, false], "sample_18": [false, true, false, false, true], "sample_19": [false, true, true, false, true], "sample_20": [true, true, true, true, true], "sample_21": [true, true, false, false, false], "sample_22": [false, true, true, false, true], "sample_23": [false, true, true, true, false], "sample_24": [false, false, false, false, true], "sample_25": [true, true, true, true, true], "sample_26": [true, true, true, true, true], "sample_27": [true, true, true, true, false], "sample_28": [false, true, false, true, true], "sample_29": [false, false, false, false, false], "sample_30": [true, false, true, true, true], "sample_31": [false, false, false, false, false], "sample_32": [false, true, true, true, true], "sample_33": [false, false, false, false, false], "sample_34": [true, false, false, false, false], "sample_35": [false, false, false, false, false], "sample_36": [false, false, false, false, false], "sample_37": [false, false, true, true, true], "sample_38": [false, false, false, true, false], "sample_39": [true, false, false, false, true], "sample_40": [false, false, false, false, false], "sample_41": [false, false, false, false, false], "sample_42": [false, false, true, false, false], "sample_43": [false, false, false, true, false], "sample_44": [false, false, false, false, true], "sample_45": [false, false, false, false, false], "sample_46": [true, true, true, true, false], "sample_47": [true, true, true, true, false], "sample_48": [false, false, true, false, false], "sample_49": [false, false, false, false, false], "sample_50": [false, true, false, false, true], "sample_51": [true, true, false, false, false], "sample_52": [true, false, false, false, false], "sample_54": [false, true, true, false, false], "sample_53": [false, false, true, false, false], "sample_55": [false, true, false, true, true], "sample_58": [false, false, false, false, false], "sample_56": [true, true, false, true, false], "sample_57": [true, false, false, true, true], "sample_59": [true, false, false, false, false], "sample_60": [false, true, false, false, false], "sample_61": [true, true, true, false, true], "sample_62": [false, false, false, true, false], "sample_63": [false, false, false, false, true], "sample_64": [true, true, false, false, true], "sample_65": [true, true, true, true, true], "sample_67": [false, true, false, true, true], "sample_66": [false, false, false, false, true], "sample_68": [true, false, false, false, false], "sample_69": [true, false, false, false, true], "sample_70": [false, false, false, false, false], "sample_71": [false, false, false, false, false], "sample_72": [false, false, true, false, true], "sample_73": [true, false, false, false, true], "sample_75": [false, false, true, false, true], "sample_74": [false, false, true, false, false], "sample_76": [true, true, true, false, true], "sample_77": [false, false, false, false, false], "sample_78": [false, false, false, true, false], "sample_79": [false, false, false, false, false], "sample_80": [false, false, false, false, false], "sample_82": [false, true, true, false, false], "sample_81": [true, true, true, false, true], "sample_83": [false, false, false, false, false], "sample_85": [false, false, false, false, true], "sample_84": [true, true, true, true, false], "sample_86": [false, false, false, true, false], "sample_88": [false, false, false, true, false], "sample_87": [false, false, false, false, false], "sample_89": [false, false, false, false, false], "sample_90": [false, false, false, false, true], "sample_91": [false, false, false, false, false], "sample_92": [false, false, false, false, false], "sample_93": [false, false, false, false, false], "sample_94": [false, false, false, false, false], "sample_95": [false, false, false, false, false], "sample_98": [false, false, false, false, false], "sample_96": [false, false, false, false, false], "sample_99": [true, false, false, false, true], "sample_97": [false, false, false, true, false], "sample_100": [true, false, false, true, false], "sample_102": [false, false, false, false, true], "sample_101": [false, false, false, false, false], "sample_103": [false, false, false, false, false], "sample_104": [true, false, true, true, false], "sample_107": [false, false, true, false, true], "sample_106": [false, false, false, false, false], "sample_105": [false, false, false, false, false], "sample_108": [false, false, false, false, false], "sample_109": [true, false, true, false, false], "sample_111": [true, true, true, true, false], "sample_110": [true, false, false, false, false], "sample_112": [false, false, false, false, false], "sample_113": [false, false, false, false, false], "sample_114": [false, false, false, false, false], "sample_115": [false, false, false, false, false], "sample_116": [false, false, false, false, false], "sample_117": [false, false, false, false, false], "sample_118": [false, false, false, false, false], "sample_119": [false, false, false, false, true], "sample_120": [false, false, false, false, false], "sample_121": [false, false, false, true, true], "sample_122": [false, false, false, false, false], "sample_123": [true, true, true, true, true], "sample_124": [false, false, false, false, false], "sample_125": [false, true, true, true, false], "sample_126": [false, false, false, false, false], "sample_127": [true, false, false, false, false], "sample_128": [false, true, false, false, false], "sample_129": [true, true, true, false, false], "sample_130": [false, true, true, false, false], "sample_131": [false, false, false, true, true], "sample_132": [false, false, false, false, false], "sample_133": [true, true, true, true, true], "sample_135": [false, false, true, false, false], "sample_134": [true, false, false, false, false], "sample_136": [false, false, false, false, false], "sample_139": [true, true, true, true, true], "sample_137": [false, false, false, false, false], "sample_138": [false, false, false, false, false], "sample_140": [false, false, false, false, false], "sample_141": [false, false, false, false, false], "sample_142": [false, false, false, false, false], "sample_143": [true, false, true, false, false], "sample_144": [false, false, false, true, false], "sample_145": [false, false, false, false, false], "sample_146": [false, true, false, false, true], "sample_147": [true, false, false, false, true], "sample_148": [false, false, true, false, false], "sample_151": [false, false, false, false, false], "sample_149": [true, false, true, true, false], "sample_152": [false, false, false, false, false], "sample_150": [false, false, false, false, false], "sample_153": [false, false, false, false, false], "sample_154": [false, false, false, false, false], "sample_155": [true, false, false, true, true], "sample_156": [true, false, false, false, false], "sample_157": [false, false, false, false, false], "sample_158": [false, false, true, false, false], "sample_159": [false, true, false, false, true], "sample_160": [false, false, false, false, false], "sample_161": [false, false, false, false, false], "sample_162": [true, true, true, true, true], "sample_163": [false, false, false, false, true], "sample_164": [false, false, false, false, true], "sample_165": [true, true, false, false, false], "sample_166": [true, true, true, true, true], "sample_167": [false, false, false, false, false], "sample_168": [false, false, false, false, false], "sample_169": [false, false, false, false, false], "sample_171": [false, false, false, false, false], "sample_170": [false, false, false, false, false], "sample_172": [false, false, true, false, false], "sample_173": [false, false, false, true, false], "sample_174": [false, true, false, false, false], "sample_175": [true, true, true, false, false], "sample_176": [false, false, false, true, false], "sample_177": [false, false, false, false, false], "sample_178": [false, false, true, false, false], "sample_180": [false, true, true, false, false], "sample_179": [false, false, false, false, false], "sample_182": [false, false, false, false, false], "sample_181": [false, true, true, false, false], "sample_183": [false, false, false, false, false], "sample_184": [false, false, true, false, false], "sample_185": [true, false, true, false, false], "sample_186": [true, false, true, false, false], "sample_187": [false, false, false, false, false], "sample_188": [false, false, false, true, true], "sample_189": [false, false, false, false, false], "sample_190": [false, false, false, true, false], "sample_191": [false, false, false, false, false], "sample_192": [false, false, false, false, false], "sample_193": [false, true, true, false, false], "sample_194": [false, false, false, false, false], "sample_195": [false, true, false, false, false], "sample_196": [true, false, false, false, false], "sample_198": [false, false, true, false, true], "sample_197": [false, false, false, false, false], "sample_199": [false, false, false, false, false], "sample_200": [true, false, false, true, false], "sample_201": [true, true, true, true, true], "sample_202": [true, true, false, false, true], "sample_203": [true, false, true, false, false], "sample_204": [false, false, false, false, false], "sample_205": [false, false, true, false, false], "sample_206": [false, false, false, false, false], "sample_207": [true, false, true, true, false], "sample_208": [false, false, false, false, true], "sample_209": [false, false, false, false, false], "sample_210": [false, true, false, false, false], "sample_211": [true, false, false, true, true], "sample_213": [true, true, false, false, false], "sample_212": [false, false, false, false, false], "sample_214": [false, false, false, true, false], "sample_215": [false, false, false, false, false], "sample_216": [false, false, false, false, false], "sample_217": [false, true, false, false, false], "sample_218": [true, false, false, true, false], "sample_219": [true, true, true, true, false], "sample_220": [true, false, false, true, true], "sample_221": [false, true, false, false, true], "sample_222": [false, false, false, false, true], "sample_223": [true, false, true, false, false], "sample_224": [false, true, false, false, false], "sample_225": [false, false, false, false, false], "sample_226": [false, false, false, false, false], "sample_227": [false, true, false, false, true], "sample_228": [false, false, false, false, false], "sample_229": [false, false, false, false, false], "sample_230": [false, false, false, false, true], "sample_231": [false, false, false, false, false], "sample_232": [false, false, true, false, false], "sample_233": [true, true, true, true, true], "sample_234": [false, false, false, false, false], "sample_235": [false, false, true, false, false], "sample_236": [true, false, false, false, false], "sample_237": [true, false, false, false, false], "sample_238": [false, false, false, false, false], "sample_239": [false, true, true, true, true], "sample_240": [true, true, true, true, true], "sample_241": [false, false, false, false, true], "sample_242": [false, false, false, false, false], "sample_243": [false, false, false, false, false], "sample_244": [false, false, false, true, true], "sample_245": [true, true, true, true, true], "sample_246": [true, true, true, true, true], "sample_247": [false, false, false, false, true], "sample_248": [false, false, false, false, false], "sample_249": [false, false, false, false, false], "sample_250": [true, false, false, false, false], "sample_251": [false, false, false, false, false], "sample_252": [false, false, false, false, false], "sample_253": [false, false, false, false, false], "sample_254": [true, true, true, true, true], "sample_256": [false, true, false, false, false], "sample_255": [false, false, false, false, false], "sample_257": [false, true, false, false, false], "sample_258": [false, false, false, false, false], "sample_259": [false, false, false, false, false], "sample_260": [true, true, true, false, true], "sample_261": [false, true, false, false, false], "sample_262": [false, false, false, false, false], "sample_263": [false, false, false, false, false], "sample_264": [true, false, false, true, true], "sample_265": [false, false, false, false, false], "sample_266": [false, false, false, false, false], "sample_267": [false, false, false, false, false], "sample_268": [false, true, false, true, false], "sample_269": [true, true, true, true, true], "sample_270": [true, false, false, true, false], "sample_271": [true, false, false, false, false], "sample_272": [true, false, false, false, false], "sample_273": [true, false, false, false, true], "sample_274": [true, true, false, true, false], "sample_275": [false, false, false, false, true], "sample_276": [false, false, false, false, false], "sample_277": [true, true, false, true, true], "sample_278": [false, false, true, false, false], "sample_279": [true, false, false, false, false], "sample_280": [false, false, false, false, false], "sample_281": [true, true, true, true, true], "sample_282": [false, true, false, true, false], "sample_283": [false, false, false, false, false], "sample_284": [false, false, true, false, false], "sample_285": [false, false, false, false, true], "sample_286": [false, false, false, false, false], "sample_287": [true, false, false, false, false], "sample_288": [true, false, false, true, false], "sample_289": [true, false, false, false, true], "sample_290": [false, false, false, true, false], "sample_291": [true, true, false, false, false], "sample_292": [false, false, false, false, false], "sample_293": [false, false, false, false, true], "sample_294": [false, false, false, false, false], "sample_295": [false, false, true, false, true], "sample_296": [false, false, false, true, false], "sample_297": [false, false, false, false, false], "sample_298": [true, true, true, true, false], "sample_299": [true, true, true, false, true], "sample_300": [false, true, false, false, false], "sample_301": [true, false, false, false, false], "sample_302": [false, true, false, false, false], "sample_303": [false, false, false, false, false], "sample_304": [true, true, false, false, true], "sample_305": [true, true, false, false, true], "sample_306": [false, false, true, false, false], "sample_307": [true, true, false, true, false], "sample_308": [true, true, false, true, true], "sample_309": [true, true, true, true, true], "sample_310": [true, false, false, true, false], "sample_312": [true, true, false, false, true], "sample_311": [false, true, false, false, false], "sample_313": [false, false, false, false, false], "sample_314": [true, false, false, false, false], "sample_315": [true, true, false, true, true], "sample_316": [false, false, false, false, false], "sample_317": [false, false, false, false, false], "sample_318": [false, false, false, false, false], "sample_319": [false, false, false, false, false], "sample_320": [false, true, true, false, false], "sample_321": [false, false, true, false, false], "sample_322": [false, false, false, false, true], "sample_323": [false, true, false, false, false], "sample_324": [false, false, false, false, false], "sample_325": [false, true, false, false, false], "sample_326": [true, false, false, true, false], "sample_327": [false, true, true, false, false], "sample_328": [false, true, true, true, true], "sample_329": [false, false, false, false, false], "sample_330": [true, true, false, false, false], "sample_331": [true, false, false, false, false], "sample_332": [true, false, false, false, true], "sample_333": [false, false, true, false, false], "sample_334": [false, true, true, true, true], "sample_335": [false, false, false, false, true], "sample_336": [false, false, false, false, false], "sample_337": [false, false, false, false, false], "sample_338": [false, true, false, false, false], "sample_339": [false, true, true, false, false], "sample_340": [false, false, false, false, false], "sample_341": [false, false, false, false, false], "sample_342": [true, true, true, true, true], "sample_343": [false, false, false, false, true], "sample_344": [false, false, false, false, true], "sample_345": [false, false, false, false, true], "sample_346": [false, false, false, false, false], "sample_347": [false, true, false, false, true], "sample_348": [false, false, false, false, false], "sample_349": [false, false, true, true, false], "sample_350": [false, false, true, false, false], "sample_351": [true, false, false, true, false], "sample_352": [false, false, false, false, false], "sample_353": [false, false, false, false, false], "sample_354": [false, false, false, false, false], "sample_355": [false, false, false, false, false], "sample_356": [false, true, false, false, false], "sample_357": [false, false, false, false, true], "sample_358": [false, false, true, false, false], "sample_359": [true, true, false, false, false], "sample_360": [false, false, false, false, false], "sample_361": [false, false, true, true, true], "sample_362": [false, false, false, false, true], "sample_363": [false, false, false, false, false], "sample_364": [false, true, false, false, false], "sample_365": [false, false, true, false, false], "sample_366": [false, true, false, false, true], "sample_367": [false, false, false, false, false], "sample_368": [false, false, false, false, false], "sample_369": [false, false, false, false, false], "sample_371": [false, false, false, false, false], "sample_370": [false, false, false, true, false], "sample_372": [false, false, true, false, false], "sample_373": [true, true, true, true, false], "sample_374": [false, false, false, false, false], "sample_375": [true, true, false, true, true], "sample_376": [false, true, true, true, false], "sample_377": [false, false, false, false, false], "sample_378": [false, false, false, false, true], "sample_379": [false, false, true, true, true], "sample_380": [true, false, false, false, false], "sample_381": [true, false, false, false, false], "sample_382": [false, false, false, false, false], "sample_383": [true, false, false, false, true], "sample_384": [false, false, true, true, false], "sample_385": [false, false, false, false, true], "sample_386": [false, true, false, true, true], "sample_387": [false, false, true, false, false], "sample_388": [false, false, true, false, true], "sample_389": [true, false, true, false, false], "sample_390": [true, false, false, false, false], "sample_391": [false, false, true, false, true], "sample_392": [false, true, false, false, false], "sample_393": [true, true, true, true, true], "sample_394": [false, true, false, true, false], "sample_395": [true, false, true, true, false], "sample_396": [false, false, true, true, false], "sample_397": [false, true, false, false, false], "sample_398": [false, false, false, false, false], "sample_399": [false, false, false, false, false], "sample_400": [false, false, false, false, false], "sample_401": [true, false, false, true, true], "sample_402": [false, false, false, false, false], "sample_403": [true, true, false, true, true], "sample_404": [false, false, true, false, true], "sample_405": [false, false, false, false, true], "sample_406": [false, false, false, false, false], "sample_407": [true, true, false, false, false], "sample_408": [false, false, false, false, false], "sample_409": [false, false, false, false, false], "sample_410": [false, false, false, true, false], "sample_411": [false, false, false, false, true], "sample_412": [true, false, false, false, false], "sample_413": [true, false, false, false, false], "sample_414": [false, false, false, false, false], "sample_415": [false, false, false, false, false], "sample_416": [false, true, true, false, true], "sample_417": [false, false, false, false, false], "sample_418": [false, false, false, false, false], "sample_419": [false, false, false, true, false], "sample_420": [false, false, true, true, true], "sample_421": [false, false, false, false, false], "sample_422": [false, false, false, false, false], "sample_423": [false, false, false, false, false], "sample_424": [false, false, false, false, false], "sample_425": [false, false, true, false, false], "sample_426": [false, true, false, false, false], "sample_427": [false, false, true, true, false], "sample_428": [false, false, false, false, false], "sample_429": [false, false, false, false, false], "sample_430": [false, false, false, false, false], "sample_431": [true, false, true, false, false], "sample_432": [true, true, true, true, true], "sample_433": [false, false, false, false, false], "sample_434": [true, false, false, false, false], "sample_435": [false, false, false, false, true], "sample_436": [false, true, true, true, true], "sample_437": [false, false, false, false, false], "sample_438": [false, true, false, false, false], "sample_439": [false, false, false, false, false], "sample_440": [true, false, false, false, false], "sample_441": [false, true, false, true, false], "sample_442": [true, true, false, false, false], "sample_443": [false, false, false, false, false], "sample_444": [true, true, true, false, true], "sample_445": [false, false, false, false, false], "sample_446": [false, false, false, true, false], "sample_447": [false, true, true, false, true], "sample_448": [false, false, false, false, true], "sample_449": [false, false, false, false, false], "sample_450": [false, false, false, false, false], "sample_451": [false, false, false, false, false], "sample_453": [false, false, false, false, false], "sample_452": [false, false, true, false, false], "sample_454": [true, false, false, false, false], "sample_455": [false, false, false, true, false], "sample_456": [true, false, true, false, false], "sample_457": [false, false, false, false, false], "sample_458": [true, true, false, false, false], "sample_459": [true, true, false, true, true], "sample_460": [false, false, false, true, true], "sample_461": [false, false, true, false, true], "sample_462": [false, false, false, true, false], "sample_463": [false, false, true, false, true], "sample_464": [false, false, false, true, true], "sample_465": [false, false, false, false, false], "sample_466": [false, true, false, true, false], "sample_467": [false, true, false, false, false], "sample_469": [false, true, true, true, false], "sample_468": [false, false, false, false, false], "sample_470": [false, true, false, false, false], "sample_471": [false, false, true, false, false], "sample_472": [false, false, false, false, false], "sample_473": [false, false, false, false, false], "sample_474": [false, true, false, false, false], "sample_475": [false, false, false, false, false], "sample_476": [false, false, false, false, false], "sample_477": [false, false, false, false, false], "sample_478": [false, false, false, false, false], "sample_479": [false, false, false, false, true], "sample_480": [true, false, false, false, true], "sample_481": [false, false, false, false, false], "sample_482": [false, false, false, false, false], "sample_483": [false, false, false, false, false], "sample_484": [false, true, false, false, false], "sample_485": [false, false, false, false, false], "sample_486": [false, true, false, true, false], "sample_487": [false, false, false, false, false], "sample_488": [true, false, false, false, false], "sample_489": [false, false, false, false, false], "sample_490": [false, true, true, false, true], "sample_491": [false, false, false, false, false], "sample_492": [true, false, false, false, false], "sample_493": [false, false, false, false, false], "sample_494": [false, false, false, false, true], "sample_495": [true, true, false, true, true], "sample_496": [false, false, false, true, false], "sample_497": [true, true, true, true, true], "sample_498": [true, false, false, false, true], "sample_499": [false, false, false, true, false], "sample_500": [false, false, false, false, false], "sample_501": [true, false, true, false, true], "sample_502": [true, true, false, false, true], "sample_503": [false, false, false, false, true], "sample_504": [false, false, true, false, false], "sample_505": [true, true, true, true, false], "sample_506": [false, false, false, false, false], "sample_507": [true, true, true, true, false], "sample_508": [false, false, false, false, true], "sample_509": [false, true, false, false, false], "sample_510": [false, true, false, false, false], "sample_511": [false, true, true, true, true], "sample_512": [true, true, false, false, true], "sample_513": [false, true, false, false, false], "sample_514": [false, false, false, true, false], "sample_515": [true, false, true, true, false], "sample_516": [false, false, false, false, true], "sample_517": [false, false, false, false, false], "sample_518": [true, false, false, false, true], "sample_519": [true, true, true, false, false], "sample_520": [false, false, false, false, false], "sample_521": [false, false, false, false, false], "sample_522": [true, false, true, false, false], "sample_523": [false, false, true, false, false], "sample_524": [true, true, false, true, true], "sample_525": [true, true, true, true, false], "sample_526": [true, true, true, false, false], "sample_527": [false, true, true, false, true], "sample_528": [false, true, false, true, false], "sample_529": [true, false, false, true, false], "sample_530": [true, false, false, true, false], "sample_531": [true, true, false, false, true], "sample_532": [false, true, false, false, false], "sample_533": [false, false, false, false, false], "sample_534": [false, false, false, false, true], "sample_535": [false, false, false, false, false], "sample_536": [false, false, false, false, false], "sample_537": [true, false, true, true, true], "sample_538": [false, false, true, true, false], "sample_539": [false, false, false, false, false], "sample_540": [false, false, false, false, false], "sample_541": [false, false, false, false, false], "sample_542": [true, false, true, true, true], "sample_543": [false, false, false, false, false], "sample_544": [false, false, false, false, true], "sample_545": [true, false, false, true, false], "sample_546": [false, false, false, true, false], "sample_547": [false, false, false, false, false], "sample_548": [true, false, true, true, false], "sample_549": [true, false, true, true, true], "sample_550": [true, false, true, true, true], "sample_551": [true, false, false, false, false], "sample_552": [true, false, true, false, false], "sample_553": [false, false, false, false, false], "sample_554": [false, true, false, false, false], "sample_555": [false, false, false, false, true], "sample_556": [false, true, true, true, true], "sample_557": [true, false, true, false, true], "sample_558": [false, false, false, false, false], "sample_559": [false, false, false, false, false], "sample_560": [false, true, false, false, false], "sample_561": [false, true, false, true, false], "sample_562": [false, false, true, false, false], "sample_563": [false, false, false, false, false], "sample_564": [false, false, false, true, true], "sample_565": [false, false, true, true, false], "sample_566": [true, true, false, true, false], "sample_567": [false, false, false, true, false], "sample_568": [false, false, false, false, true], "sample_569": [true, false, false, false, false], "sample_570": [true, false, false, false, false], "sample_571": [false, false, false, true, false], "sample_572": [false, false, true, true, false], "sample_573": [true, false, false, false, false], "sample_574": [false, false, false, false, false], "sample_575": [false, true, false, false, true], "sample_576": [false, false, false, false, true], "sample_577": [false, true, false, false, false], "sample_578": [false, false, false, false, false], "sample_579": [true, true, true, true, true], "sample_580": [false, true, false, false, false], "sample_581": [false, false, false, false, false], "sample_582": [false, false, false, false, false], "sample_583": [true, true, true, true, true], "sample_584": [false, false, true, false, false], "sample_585": [false, false, false, true, false], "sample_586": [false, false, false, false, false], "sample_587": [false, false, true, false, true], "sample_588": [false, false, false, false, true], "sample_589": [true, true, true, true, true], "sample_590": [true, false, false, false, false], "sample_591": [false, false, true, true, false], "sample_592": [true, true, true, true, true], "sample_593": [true, true, false, true, false], "sample_594": [true, true, true, true, true], "sample_595": [false, false, false, true, false], "sample_596": [true, true, true, false, true], "sample_597": [false, false, false, true, false], "sample_598": [true, true, true, true, true], "sample_599": [false, false, true, false, false], "sample_600": [true, true, false, false, false], "sample_601": [true, true, false, false, false], "sample_602": [true, false, false, false, false], "sample_603": [false, false, true, true, false], "sample_604": [true, false, true, true, true], "sample_605": [false, true, false, false, true], "sample_606": [false, false, false, true, false], "sample_607": [false, true, false, false, false], "sample_608": [true, true, true, true, true], "sample_609": [false, false, false, false, false], "sample_610": [false, false, false, false, false], "sample_611": [false, false, false, true, false], "sample_612": [false, false, false, false, false], "sample_613": [false, false, false, false, false], "sample_614": [false, true, true, false, false], "sample_615": [false, true, false, false, true], "sample_616": [true, false, true, true, true], "sample_617": [false, false, true, false, false], "sample_618": [false, false, false, true, false], "sample_619": [false, true, false, false, false], "sample_620": [true, true, true, true, true], "sample_621": [false, true, false, false, false], "sample_622": [true, true, true, true, true], "sample_623": [false, false, false, false, false], "sample_624": [false, true, false, false, false], "sample_625": [false, true, true, false, true], "sample_626": [true, false, false, true, true], "sample_627": [true, true, true, true, true], "sample_628": [true, true, false, true, false], "sample_629": [true, true, true, false, true], "sample_630": [false, false, false, false, false], "sample_631": [false, false, false, false, false], "sample_632": [false, false, false, false, false], "sample_633": [false, false, false, true, false], "sample_634": [false, false, false, false, false], "sample_635": [false, false, false, false, false], "sample_636": [false, false, false, false, false], "sample_637": [false, false, true, true, false], "sample_638": [false, false, false, true, false], "sample_639": [false, true, false, true, false], "sample_640": [false, false, false, false, false], "sample_641": [false, true, false, false, true], "sample_642": [false, false, false, true, false], "sample_643": [false, false, false, false, false], "sample_644": [false, false, false, false, false], "sample_645": [true, true, false, false, false], "sample_646": [true, false, true, false, false], "sample_647": [false, false, false, true, false], "sample_648": [true, true, true, true, true], "sample_649": [false, true, false, false, false], "sample_650": [true, false, false, false, false], "sample_651": [false, false, false, false, true], "sample_652": [true, true, true, false, true], "sample_653": [true, false, false, false, true], "sample_654": [false, true, true, true, true], "sample_655": [true, false, true, true, false], "sample_656": [false, false, false, false, true], "sample_657": [true, true, true, true, true], "sample_658": [false, false, false, true, false], "sample_659": [false, false, false, false, false], "sample_660": [true, false, true, false, true], "sample_661": [false, false, false, true, false], "sample_662": [false, false, false, false, false], "sample_663": [false, true, true, true, false], "sample_664": [false, false, false, false, false], "sample_665": [true, false, false, false, false], "sample_666": [true, false, false, false, false], "sample_667": [true, false, true, true, true], "sample_668": [false, false, false, false, false], "sample_669": [false, false, false, true, false], "sample_670": [false, false, false, false, true], "sample_671": [true, true, false, true, true], "sample_672": [false, false, false, true, false], "sample_673": [false, false, false, false, false], "sample_674": [false, false, true, true, true], "sample_675": [true, false, false, false, false], "sample_676": [false, true, true, false, false], "sample_677": [false, false, false, false, false], "sample_678": [false, true, false, true, true], "sample_679": [true, true, true, true, true], "sample_680": [false, true, true, true, false], "sample_681": [true, false, false, true, false], "sample_682": [true, true, false, false, true], "sample_683": [false, false, false, true, true], "sample_684": [true, false, true, false, false], "sample_685": [false, false, false, true, false], "sample_686": [false, false, false, false, false], "sample_687": [false, false, false, false, false], "sample_688": [true, false, true, true, false], "sample_689": [false, false, false, false, false], "sample_690": [false, true, true, true, true], "sample_691": [false, false, false, false, false], "sample_692": [false, true, true, false, true], "sample_693": [true, true, true, false, false], "sample_694": [false, true, false, false, false], "sample_695": [false, true, true, true, false], "sample_696": [false, false, false, false, false], "sample_697": [false, false, true, true, false], "sample_698": [true, false, false, true, true], "sample_699": [false, false, true, false, false], "sample_700": [true, true, true, false, false], "sample_701": [false, false, false, false, false], "sample_702": [false, false, false, false, false], "sample_703": [false, false, false, true, true], "sample_704": [false, false, false, true, false], "sample_705": [false, false, false, true, false], "sample_706": [true, false, true, false, true], "sample_707": [false, false, false, false, false], "sample_708": [false, false, false, true, false], "sample_709": [false, false, false, false, true], "sample_710": [false, true, true, true, true], "sample_711": [false, true, true, true, false], "sample_712": [false, false, false, false, false], "sample_713": [true, false, true, false, true], "sample_714": [true, false, false, true, false], "sample_715": [false, true, true, true, false], "sample_716": [true, true, true, false, true], "sample_717": [true, true, false, true, true], "sample_718": [true, false, false, false, false], "sample_719": [true, true, false, true, true], "sample_720": [false, false, false, false, false], "sample_721": [false, true, true, false, true], "sample_722": [true, false, true, false, true], "sample_723": [false, true, false, false, false], "sample_724": [false, false, false, false, false], "sample_725": [false, false, false, false, true], "sample_726": [false, false, false, false, false], "sample_727": [true, false, false, false, false], "sample_728": [false, false, true, true, true], "sample_729": [true, true, false, true, true], "sample_730": [false, true, false, true, false], "sample_731": [false, false, false, false, false], "sample_732": [true, false, false, false, false], "sample_733": [true, true, true, true, true], "sample_734": [false, true, false, true, false], "sample_735": [false, true, false, false, false], "sample_736": [true, false, true, true, true], "sample_737": [true, false, true, true, true], "sample_738": [true, true, true, true, true], "sample_739": [false, false, true, false, true], "sample_740": [false, false, false, false, false], "sample_741": [false, true, false, true, false], "sample_742": [true, true, true, false, true], "sample_743": [false, false, false, true, true], "sample_744": [false, false, false, false, true], "sample_745": [true, true, true, true, false], "sample_746": [false, false, false, false, false], "sample_747": [false, false, false, false, true], "sample_748": [true, true, true, false, false], "sample_749": [false, false, false, false, false], "sample_750": [true, false, false, true, true], "sample_751": [false, true, false, false, false], "sample_752": [true, false, false, false, false], "sample_753": [false, false, true, true, true], "sample_754": [true, false, false, false, false], "sample_755": [false, false, false, false, false], "sample_756": [false, true, false, false, false], "sample_757": [false, false, false, false, false], "sample_758": [false, false, false, false, false], "sample_759": [false, false, false, false, false], "sample_760": [false, true, false, false, false], "sample_761": [true, true, true, false, true], "sample_762": [false, true, false, false, false], "sample_763": [false, true, false, false, false], "sample_764": [false, false, false, false, false], "sample_765": [false, false, false, false, true], "sample_766": [false, true, false, false, true], "sample_767": [false, true, false, false, false], "sample_768": [true, false, false, false, true], "sample_769": [false, false, false, false, false], "sample_770": [false, false, false, false, false], "sample_771": [false, true, false, false, true], "sample_772": [false, true, false, false, true], "sample_773": [true, true, false, true, true], "sample_774": [false, true, true, true, false], "sample_775": [false, false, false, false, true], "sample_776": [false, true, false, true, false], "sample_777": [true, false, false, true, false], "sample_778": [true, true, true, true, true], "sample_779": [true, true, true, true, false], "sample_780": [false, false, false, true, false], "sample_781": [true, true, true, true, true], "sample_782": [false, false, false, true, false], "sample_783": [false, true, false, false, true], "sample_784": [true, true, true, false, false], "sample_785": [true, true, false, true, true], "sample_786": [true, true, false, true, false], "sample_787": [false, false, false, false, false], "sample_788": [false, true, false, false, true], "sample_789": [false, false, true, true, false], "sample_790": [true, true, true, true, true], "sample_791": [false, false, false, true, true], "sample_792": [false, false, true, true, false], "sample_793": [false, false, true, true, false], "sample_794": [false, false, false, false, false], "sample_795": [false, false, false, false, true], "sample_796": [true, true, false, true, false], "sample_797": [true, true, false, false, true], "sample_798": [false, false, true, false, true], "sample_799": [false, true, true, false, true], "sample_800": [false, false, false, false, false], "sample_801": [true, false, false, false, false], "sample_802": [true, true, false, true, true], "sample_803": [false, false, false, false, true], "sample_804": [true, true, true, false, true], "sample_805": [true, false, true, false, false], "sample_806": [false, true, false, true, false], "sample_807": [false, true, false, true, false], "sample_808": [true, false, false, false, false], "sample_809": [true, true, true, false, true], "sample_810": [false, true, false, true, false], "sample_811": [false, true, true, true, true], "sample_812": [false, false, false, false, false], "sample_813": [true, true, true, true, true], "sample_814": [false, false, true, false, true], "sample_815": [false, false, false, false, false], "sample_816": [false, true, true, false, false], "sample_817": [true, true, false, false, true], "sample_818": [true, true, false, true, true], "sample_819": [true, true, false, true, true], "sample_820": [false, true, false, false, false], "sample_821": [false, true, false, true, false], "sample_822": [true, true, false, false, true], "sample_823": [false, false, true, true, false], "sample_824": [false, true, false, false, true], "sample_825": [true, true, true, false, true], "sample_826": [false, true, true, false, false], "sample_827": [false, true, false, true, true], "sample_828": [false, true, true, true, true], "sample_829": [false, true, true, true, true], "sample_830": [true, true, true, true, true], "sample_831": [false, true, false, false, true], "sample_832": [true, false, false, true, true], "sample_833": [true, true, true, false, false], "sample_834": [false, true, false, true, true], "sample_835": [true, true, false, true, true], "sample_836": [false, false, false, false, false], "sample_837": [true, true, true, true, true], "sample_838": [false, true, true, false, true], "sample_839": [true, true, false, false, true], "sample_840": [true, false, true, false, false], "sample_841": [true, true, false, false, false], "sample_842": [true, false, true, true, false], "sample_843": [false, true, true, true, true], "sample_844": [false, false, false, false, false], "sample_845": [false, false, true, true, true], "sample_846": [false, false, false, false, true], "sample_847": [false, false, false, false, true], "sample_848": [false, false, true, true, true], "sample_849": [false, false, false, false, false], "sample_850": [true, false, false, false, true], "sample_851": [true, false, true, false, true], "sample_852": [true, false, true, true, false], "sample_853": [true, false, false, false, true], "sample_854": [false, false, false, false, false], "sample_855": [true, true, true, true, false], "sample_856": [false, false, false, false, false], "sample_857": [true, true, true, true, false], "sample_858": [true, true, true, true, false], "sample_859": [true, true, true, true, true], "sample_860": [false, false, false, true, true], "sample_861": [false, true, true, false, false], "sample_862": [true, false, false, true, true], "sample_863": [false, false, true, true, false], "sample_864": [false, true, true, false, true], "sample_865": [true, true, true, true, true], "sample_866": [true, true, true, false, true], "sample_867": [true, false, true, false, false], "sample_868": [true, true, false, true, true], "sample_869": [false, true, false, false, false], "sample_870": [true, true, true, true, true], "sample_871": [true, false, true, false, false], "sample_872": [false, true, true, false, true], "sample_873": [true, false, true, false, false], "sample_874": [true, false, false, true, true], "sample_875": [false, false, false, false, true], "sample_876": [false, false, true, false, false], "sample_877": [true, true, true, true, false], "sample_878": [false, true, false, true, false], "sample_879": [false, true, true, false, false], "sample_880": [false, false, false, false, false], "sample_881": [false, false, false, true, true], "sample_882": [false, true, false, false, true], "sample_883": [false, true, true, true, false], "sample_884": [false, false, true, false, false], "sample_885": [false, false, false, false, true], "sample_886": [false, false, true, false, false], "sample_887": [false, true, false, true, false], "sample_888": [false, false, false, false, false], "sample_889": [true, false, false, false, false], "sample_890": [false, false, false, true, false], "sample_891": [false, true, true, false, false], "sample_892": [true, false, false, true, true], "sample_893": [false, false, true, false, false], "sample_894": [false, false, false, false, false], "sample_895": [true, true, false, true, false], "sample_896": [false, false, false, false, false], "sample_897": [false, false, false, false, false], "sample_898": [false, true, false, false, true], "sample_899": [false, true, false, true, false], "sample_900": [false, false, true, true, true], "sample_901": [true, true, true, false, true], "sample_902": [false, true, true, false, false], "sample_903": [false, true, false, true, false], "sample_904": [false, false, true, false, false], "sample_905": [false, false, false, false, false], "sample_906": [false, true, false, false, false], "sample_907": [true, false, false, false, false], "sample_908": [true, true, true, true, true], "sample_909": [false, false, false, false, false], "sample_910": [false, true, true, false, false], "sample_911": [true, false, false, true, false], "sample_912": [true, true, true, false, false], "sample_913": [false, false, false, true, false], "sample_914": [true, true, false, false, true], "sample_915": [true, true, false, true, true], "sample_916": [false, true, true, true, false], "sample_917": [true, true, true, true, false], "sample_918": [false, false, true, true, false], "sample_919": [true, true, true, true, false], "sample_920": [false, false, false, false, false], "sample_921": [true, false, false, true, true], "sample_922": [true, true, true, true, false], "sample_923": [false, true, true, false, false], "sample_924": [true, false, true, false, false], "sample_925": [false, false, false, true, true], "sample_926": [true, true, false, false, false], "sample_927": [true, false, true, true, false], "sample_928": [false, false, false, false, false], "sample_929": [false, false, false, false, true], "sample_930": [false, false, false, false, false], "sample_931": [false, false, false, true, false], "sample_932": [false, true, true, true, false], "sample_933": [false, false, false, false, false], "sample_934": [true, true, true, false, false], "sample_935": [true, true, true, false, false], "sample_936": [true, true, false, true, true], "sample_937": [false, false, true, false, false], "sample_938": [false, true, false, false, false], "sample_939": [false, true, false, false, true], "sample_940": [true, false, true, true, false], "sample_941": [false, true, true, true, true], "sample_942": [false, false, false, false, false], "sample_943": [true, false, false, true, true], "sample_944": [true, true, true, true, false], "sample_945": [false, false, false, false, false], "sample_946": [false, false, false, true, false], "sample_947": [false, false, true, false, false], "sample_948": [true, false, false, false, false], "sample_949": [false, false, false, false, false], "sample_950": [true, true, false, false, false], "sample_951": [false, true, true, false, false], "sample_952": [true, false, false, false, false], "sample_953": [false, false, false, false, false], "sample_954": [false, false, false, false, false], "sample_955": [false, false, false, true, false], "sample_956": [false, false, false, false, true], "sample_957": [true, true, false, true, true], "sample_958": [false, false, false, true, false], "sample_959": [true, true, false, false, false], "sample_960": [false, false, true, false, false], "sample_961": [false, false, false, false, false], "sample_962": [false, false, true, false, false], "sample_963": [false, false, false, true, false], "sample_964": [false, false, false, false, false], "sample_965": [true, false, true, false, false], "sample_966": [false, false, false, false, false], "sample_967": [false, false, false, false, false], "sample_968": [true, false, false, false, false], "sample_969": [true, true, true, false, true], "sample_970": [false, false, false, false, false], "sample_971": [false, true, false, false, false], "sample_972": [true, true, true, false, false], "sample_973": [false, true, true, false, true], "sample_974": [false, false, false, false, false], "sample_975": [false, true, false, false, false], "sample_976": [false, false, false, false, false], "sample_977": [true, false, false, false, true], "sample_978": [false, false, true, true, false], "sample_979": [false, false, false, false, true], "sample_980": [false, true, false, true, false], "sample_981": [false, false, true, true, true], "sample_982": [false, false, false, false, true], "sample_983": [false, false, false, false, false], "sample_984": [false, false, false, false, false], "sample_985": [false, false, false, false, false], "sample_986": [false, true, false, false, false], "sample_987": [false, false, false, false, true], "sample_988": [false, false, false, false, false], "sample_989": [false, false, false, false, false], "sample_990": [false, false, false, false, false], "sample_991": [false, false, false, false, false], "sample_992": [true, false, false, false, false], "sample_993": [true, false, true, true, false], "sample_994": [true, false, false, false, true], "sample_995": [false, false, true, false, false], "sample_996": [false, false, true, false, true], "sample_997": [true, true, true, true, true], "sample_998": [true, false, false, false, false], "sample_999": [false, false, true, false, false], "sample_1000": [false, false, false, false, false], "sample_1001": [false, false, false, false, false], "sample_1002": [false, false, false, false, false], "sample_1003": [false, true, true, false, true], "sample_1004": [false, false, false, false, true], "sample_1005": [false, true, false, false, false], "sample_1006": [false, false, false, false, false], "sample_1007": [false, false, false, false, false], "sample_1008": [false, false, false, false, false], "sample_1009": [false, false, false, false, false], "sample_1010": [false, false, false, false, false], "sample_1011": [false, false, false, false, false], "sample_1012": [false, false, false, false, false], "sample_1013": [false, false, false, true, false], "sample_1014": [true, false, false, false, true], "sample_1015": [false, false, false, false, false], "sample_1016": [false, false, false, false, false], "sample_1017": [false, false, false, false, true], "sample_1018": [false, false, false, false, false], "sample_1019": [false, false, false, true, false], "sample_1020": [true, false, false, false, false], "sample_1021": [false, false, false, true, false], "sample_1022": [false, false, true, true, true], "sample_1023": [false, false, false, false, false], "sample_1024": [true, false, false, false, false], "sample_1025": [false, false, false, false, false], "sample_1026": [false, true, false, false, false], "sample_1027": [false, false, false, true, false], "sample_1028": [false, false, false, false, false], "sample_1029": [false, false, false, false, false], "sample_1030": [false, false, true, false, false], "sample_1031": [false, false, false, false, false], "sample_1032": [false, false, false, false, false], "sample_1033": [false, true, false, false, false], "sample_1034": [false, false, true, true, false], "sample_1035": [false, false, false, false, false], "sample_1036": [false, true, false, false, false], "sample_1037": [false, false, false, false, false], "sample_1038": [false, false, false, false, true], "sample_1039": [false, false, false, false, false], "sample_1040": [true, true, false, false, false], "sample_1041": [false, false, false, false, false], "sample_1042": [false, false, false, true, false], "sample_1043": [true, true, false, true, false], "sample_1044": [false, false, false, false, false], "sample_1045": [false, false, false, true, false], "sample_1046": [false, false, false, false, false], "sample_1047": [false, false, false, false, false], "sample_1048": [false, false, false, false, false], "sample_1049": [false, false, false, true, false], "sample_1050": [false, false, false, false, false], "sample_1051": [false, false, false, true, false], "sample_1052": [false, false, true, false, false], "sample_1053": [false, false, false, false, false], "sample_1054": [true, false, false, false, false], "sample_1055": [false, false, false, false, true], "sample_1056": [false, false, true, false, true], "sample_1057": [false, false, false, false, true], "sample_1058": [false, false, false, false, false], "sample_1059": [false, false, false, false, false], "sample_1060": [false, false, false, false, true], "sample_1061": [false, false, false, false, false], "sample_1062": [false, false, false, false, false], "sample_1063": [false, true, false, false, true], "sample_1064": [false, false, false, true, true], "sample_1065": [false, false, true, false, false], "sample_1066": [false, false, false, false, false], "sample_1067": [false, false, true, true, false], "sample_1068": [false, false, false, false, false], "sample_1069": [false, false, false, false, true], "sample_1070": [false, false, false, false, false], "sample_1071": [false, false, false, false, false], "sample_1072": [false, false, false, false, false], "sample_1073": [false, true, false, false, false], "sample_1074": [false, false, false, false, true], "sample_1075": [false, false, false, false, false], "sample_1076": [false, false, false, false, false], "sample_1077": [false, false, true, false, false], "sample_1078": [false, false, false, false, false], "sample_1079": [false, false, true, false, true], "sample_1080": [true, false, false, false, false], "sample_1081": [false, false, false, false, false], "sample_1082": [false, false, false, false, false], "sample_1083": [false, false, false, false, false], "sample_1084": [false, false, false, false, false], "sample_1085": [false, true, false, false, false], "sample_1086": [false, false, false, false, false], "sample_1087": [false, false, false, false, false], "sample_1088": [false, false, false, false, false], "sample_1089": [false, false, false, false, false], "sample_1090": [false, false, false, false, false], "sample_1091": [false, false, false, false, false], "sample_1092": [false, false, true, false, false], "sample_1093": [false, false, false, false, false], "sample_1094": [false, false, false, false, false], "sample_1095": [false, false, false, false, false], "sample_1096": [false, false, false, true, true], "sample_1097": [false, false, false, false, false], "sample_1098": [false, false, false, false, false], "sample_1099": [true, false, false, false, true], "sample_1100": [false, false, false, false, false], "sample_1101": [false, true, false, true, true], "sample_1102": [false, false, false, false, false], "sample_1103": [false, false, false, false, false], "sample_1104": [false, false, false, false, false], "sample_1105": [false, false, false, false, false], "sample_1106": [false, false, true, true, false], "sample_1107": [true, false, false, false, true], "sample_1108": [true, false, true, false, false], "sample_1109": [false, false, false, false, false], "sample_1110": [false, false, false, false, false], "sample_1111": [false, false, false, false, false], "sample_1112": [true, true, false, false, false], "sample_1113": [false, false, false, false, false], "sample_1114": [false, false, false, false, false], "sample_1115": [true, false, false, false, false], "sample_1116": [false, false, false, false, false], "sample_1117": [false, false, false, false, true], "sample_1118": [false, false, false, true, false], "sample_1119": [false, false, false, true, false], "sample_1120": [false, false, false, false, false], "sample_1121": [false, false, false, false, false], "sample_1122": [false, false, false, true, false], "sample_1123": [true, false, false, true, true], "sample_1124": [false, false, false, true, true], "sample_1125": [false, false, false, false, false], "sample_1126": [true, true, true, true, true], "sample_1127": [false, false, false, true, false], "sample_1128": [true, false, false, true, false], "sample_1129": [false, false, true, false, false], "sample_1130": [false, true, false, false, false], "sample_1131": [false, false, false, false, false], "sample_1132": [false, false, true, true, true], "sample_1133": [true, true, false, true, true], "sample_1134": [true, false, false, false, false], "sample_1135": [false, false, false, false, false], "sample_1136": [false, false, false, false, false], "sample_1137": [false, false, false, false, false], "sample_1138": [false, false, false, false, false], "sample_1139": [false, false, false, false, false], "sample_1140": [false, false, false, false, false], "sample_1141": [false, false, false, false, false], "sample_1142": [false, false, false, false, false], "sample_1143": [false, false, false, false, false], "sample_1144": [false, true, false, true, true], "sample_1145": [false, false, false, false, false], "sample_1146": [false, false, false, false, false], "sample_1147": [false, false, false, false, false], "sample_1148": [true, true, false, false, false], "sample_1149": [true, true, true, true, true], "sample_1150": [false, false, false, false, false], "sample_1151": [false, false, false, false, false], "sample_1152": [false, false, false, false, false], "sample_1153": [false, false, false, false, false], "sample_1154": [false, false, false, true, false], "sample_1155": [false, false, false, false, false], "sample_1156": [false, false, false, false, false], "sample_1157": [true, true, true, true, true], "sample_1158": [true, false, false, false, false], "sample_1159": [true, false, false, false, false], "sample_1160": [false, false, false, false, false], "sample_1161": [false, true, false, false, false], "sample_1162": [false, false, false, false, false], "sample_1163": [false, false, false, false, false], "sample_1164": [false, false, false, false, true], "sample_1165": [false, false, false, false, true], "sample_1166": [true, false, false, true, false], "sample_1167": [false, false, false, false, false], "sample_1168": [true, false, true, true, false], "sample_1169": [false, false, true, false, false], "sample_1170": [false, false, true, true, false], "sample_1171": [false, false, false, false, true], "sample_1172": [false, false, true, false, false], "sample_1173": [true, false, true, true, true], "sample_1174": [false, false, false, true, false], "sample_1175": [false, false, false, false, false], "sample_1176": [false, false, false, false, true], "sample_1177": [false, false, false, false, false], "sample_1178": [false, false, false, false, false], "sample_1179": [false, false, false, false, true], "sample_1180": [true, false, true, false, false], "sample_1181": [false, true, false, true, true], "sample_1182": [false, false, true, false, false], "sample_1183": [false, false, false, false, false], "sample_1184": [false, false, false, false, false], "sample_1185": [false, false, false, false, false], "sample_1186": [false, false, true, true, true], "sample_1187": [true, false, true, false, false], "sample_1188": [false, false, false, false, true], "sample_1189": [false, true, false, true, false], "sample_1190": [false, false, false, false, false], "sample_1191": [false, false, false, false, false], "sample_1192": [false, false, false, false, false], "sample_1193": [false, false, false, false, false], "sample_1194": [false, false, false, false, false], "sample_1195": [false, false, false, false, false], "sample_1196": [true, false, false, false, false], "sample_1197": [false, false, false, false, false], "sample_1198": [false, false, false, false, false], "sample_1199": [false, false, false, false, true], "sample_1200": [false, false, false, false, false], "sample_1201": [false, false, false, false, false], "sample_1202": [true, false, false, false, false], "sample_1203": [false, false, false, false, false], "sample_1204": [false, false, false, false, true], "sample_1205": [false, false, false, false, false], "sample_1206": [false, false, false, false, false], "sample_1207": [false, true, true, true, true], "sample_1208": [false, false, false, false, false], "sample_1209": [true, false, false, true, true]}}